{
  "date": "2025-11-24",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-24 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv è®ºæ–‡çˆ†å‘å¼å¢é•¿ï¼Œ**Deep Research (æ·±åº¦ç ”ç©¶)** Agent æˆä¸ºç»å¯¹ç„¦ç‚¹ï¼Œä»ç³»ç»Ÿç»¼è¿°åˆ°å…·ä½“çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚ DR Tuluï¼‰éƒ½æœ‰é‡ç£…æ›´æ–°ã€‚åŒ»ç–— AI æ–¹é¢ï¼Œ**å¤šæ™ºèƒ½ä½“åä½œæ²»ç–—éª¨å…³èŠ‚ç‚**å’Œ**MedSAM3**å±•ç°äº† AI åœ¨ç²¾å‡†åŒ»ç–—å’Œå½±åƒåˆ†å‰²ä¸Šçš„è½åœ°æ½œåŠ›ã€‚æ­¤å¤–ï¼Œç”Ÿæˆå¼æ¨¡å‹åœ¨**æµåŒ¹é… (Flow Matching)** åŠ é€Ÿå’Œ**OCR ç«¯åˆ°ç«¯æ¨¡å‹**ï¼ˆè…¾è®¯æ··å…ƒï¼‰ä¸Šä¹Ÿå–å¾—äº† SOTA çº§åˆ«çš„çªç ´ã€‚\n\n---\n\n### ğŸš€ æ·±åº¦ç ”ç©¶ Agent ä¸å¤šæ™ºèƒ½ä½“æ¶æ„ (Deep Research & Agents)\n\n**43. DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research (DR Tuluï¼šåŸºäºæ¼”åŒ–è¯„åˆ†æ ‡å‡†å¼ºåŒ–å­¦ä¹ çš„æ·±åº¦ç ”ç©¶æ¨¡å‹)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šç›®å‰çš„ Deep Research æ¨¡å‹å¤§å¤šä¾èµ–ç®€å•çš„ QA ä»»åŠ¡è®­ç»ƒï¼Œéš¾ä»¥åº”å¯¹é•¿ç¨‹ç ”ç©¶ã€‚ä½œè€…æå‡ºäº† **RLER (Reinforcement Learning with Evolving Rubrics)**ï¼Œè®©è¯„åˆ†æ ‡å‡†åœ¨è®­ç»ƒä¸­ä¸ç­–ç•¥æ¨¡å‹å…±åŒæ¼”åŒ–ã€‚\n*   **è´¡çŒ®**ï¼šå‘å¸ƒäº† **DR Tulu-8B**ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç›´æ¥é’ˆå¯¹å¼€æ”¾å¼ã€é•¿ç¯‡æ·±åº¦ç ”ç©¶è®­ç»ƒçš„å¼€æºæ¨¡å‹ã€‚å®ƒåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰å¼€æºæ¨¡å‹ï¼Œç”šè‡³åŒ¹æ•Œä¸“æœ‰ç³»ç»Ÿï¼Œä¸”æˆæœ¬æ›´ä½ã€‚\n\n**80. Deep Research: A Systematic Survey (æ·±åº¦ç ”ç©¶ï¼šç³»ç»Ÿæ€§ç»¼è¿°)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šè¿™æ˜¯ä¸€ç¯‡å…³äº Deep Research çš„å…¨é¢ç»¼è¿°ã€‚æ–‡ç« å°†æ·±åº¦ç ”ç©¶å®šä¹‰ä¸ºç»“åˆ LLM æ¨ç†ä¸å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æœç´¢å¼•æ“ï¼‰çš„ Agentã€‚\n*   **è´¡çŒ®**ï¼šæ­£å¼æå‡ºäº†ä¸‰é˜¶æ®µè·¯çº¿å›¾ï¼Œæ¢³ç†äº†å››å¤§æ ¸å¿ƒç»„ä»¶ï¼šæŸ¥è¯¢è§„åˆ’ã€ä¿¡æ¯è·å–ã€è®°å¿†ç®¡ç†å’Œç­”æ¡ˆç”Ÿæˆã€‚è¿™æ˜¯äº†è§£è¯¥é¢†åŸŸç°çŠ¶çš„å¿…è¯»æ–‡çŒ®ã€‚\n\n**16. A Layered Protocol Architecture for the Internet of Agents (æ™ºèƒ½ä½“äº’è”ç½‘çš„åˆ†å±‚åè®®æ¶æ„)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šç°æœ‰çš„ OSI å’Œ TCP/IP åè®®ä¸é€‚åˆ Agent é—´çš„è¯­ä¹‰åä½œã€‚\n*   **è´¡çŒ®**ï¼šæå‡ºäº† **Internet of Agents (IoA)** æ¶æ„ï¼Œå¼•å…¥äº† **Agent é€šä¿¡å±‚ (L8)** å’Œ **Agent è¯­ä¹‰å±‚ (L9)**ã€‚è¿™ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ ‡å‡†åŒ–é€šä¿¡å’Œåä½œå¥ å®šäº†åŸºç¡€ï¼Œç±»ä¼¼äºäº’è”ç½‘åè®®ä¹‹äºç½‘ç»œã€‚\n\n**1. KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA) (KOMï¼šç”¨äºè†éª¨å…³èŠ‚ç‚ç²¾å‡†ç®¡ç†çš„å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿ)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šé’ˆå¯¹åŒ»ç–—èµ„æºå—é™é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ä¸ªä¸“é—¨ç®¡ç†è†éª¨å…³èŠ‚ç‚çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ **KOM**ã€‚\n*   **è´¡çŒ®**ï¼šKOM åœ¨å½±åƒåˆ†æå’Œå¤„æ–¹ç”Ÿæˆä¸Šä¼˜äºé€šç”¨ LLMã€‚ä»¿çœŸç ”ç©¶æ˜¾ç¤ºï¼ŒKOM ä¸åŒ»ç”Ÿåä½œå¯å‡å°‘ 38.5% çš„è¯Šæ–­è§„åˆ’æ—¶é—´ï¼Œå¹¶æé«˜æ²»ç–—è´¨é‡ã€‚è¿™æ˜¯å¤šæ™ºèƒ½ä½“åœ¨æ…¢æ€§ç—…ç®¡ç†ä¸Šçš„é‡è¦è½åœ°ã€‚\n\n---\n\n### ğŸ–¼ï¸ è§†è§‰ç”Ÿæˆä¸ OCR (Vision Generation & OCR)\n\n**2. Terminal Velocity Matching (ç»ˆç«¯é€Ÿåº¦åŒ¹é…)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šæ¥è‡ª Luma AI ç­‰å›¢é˜Ÿçš„ç ”ç©¶ã€‚æå‡ºäº† **TVM**ï¼Œè¿™æ˜¯ Flow Matching çš„æ³›åŒ–å½¢å¼ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸçš„å•æ­¥æˆ–å°‘æ­¥ç”Ÿæˆã€‚\n*   **è´¡çŒ®**ï¼šTVM åœ¨ç»ˆç«¯æ—¶é—´è€Œéåˆå§‹æ—¶é—´æ­£åˆ™åŒ–æ¨¡å‹è¡Œä¸ºã€‚åœ¨ ImageNet ä¸Šï¼Œä»…éœ€ **1 æ¬¡å‡½æ•°è¯„ä¼° (NFE)** å³å¯è¾¾åˆ° 3.29 FIDï¼Œä»£è¡¨äº†ä»å¤´è®­ç»ƒçš„å°‘æ­¥ç”Ÿæˆæ¨¡å‹çš„ SOTA æ°´å¹³ã€‚\n\n**50. HunyuanOCR Technical Report (æ··å…ƒ OCR æŠ€æœ¯æŠ¥å‘Š)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šè…¾è®¯æ··å…ƒå›¢é˜Ÿå‘å¸ƒçš„å•†ä¸šçº§ã€å¼€æº OCR æ¨¡å‹ï¼ˆ1B å‚æ•°ï¼‰ã€‚\n*   **è´¡çŒ®**ï¼šé‡‡ç”¨çº¯ç«¯åˆ°ç«¯æ¶æ„ï¼ˆNative ViT + LLMï¼‰ï¼Œæ— éœ€é¢„å¤„ç†ï¼ˆå¦‚ç‰ˆé¢åˆ†æï¼‰ã€‚åœ¨ OCRBench ä¸Šå–å¾—äº† 3B å‚æ•°ä»¥ä¸‹æ¨¡å‹çš„ SOTAï¼Œå¹¶åœ¨ ICDAR 2025 æŒ‘æˆ˜èµ›ä¸­å¤ºå† ã€‚æ”¯æŒå¤šä»»åŠ¡ï¼ˆè¯†åˆ«ã€è§£æã€ç¿»è¯‘ç­‰ï¼‰ã€‚\n\n**111. MedSAM3: Delving into Segment Anything with Medical Concepts (MedSAM3ï¼šæ·±å…¥æ¢ç©¶èåˆåŒ»å­¦æ¦‚å¿µçš„ Segment Anything)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šç°æœ‰çš„åŒ»å­¦åˆ†å‰²æ¨¡å‹ç¼ºä¹é€šç”¨æ€§ã€‚\n*   **è´¡çŒ®**ï¼šæå‡ºäº† **MedSAM-3**ï¼Œæ”¯æŒé€šè¿‡å¼€æ”¾è¯æ±‡çš„æ–‡æœ¬æç¤ºï¼ˆText Promptableï¼‰è¿›è¡ŒåŒ»å­¦å½±åƒåˆ†å‰²ï¼Œè€Œä¸ä»…ä»…æ˜¯å‡ ä½•æç¤ºã€‚ç»“åˆ MLLM Agentï¼Œå®ç°äº†å¤æ‚æ¨ç†å’Œè¿­ä»£ä¼˜åŒ–ï¼Œåœ¨ X å…‰ã€MRIã€CT ç­‰å¤šç§æ¨¡æ€ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚\n\n**42. In-Video Instructions: Visual Signals as Generative Control (è§†é¢‘å†…æŒ‡ä»¤ï¼šè§†è§‰ä¿¡å·ä½œä¸ºç”Ÿæˆæ§åˆ¶)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šæ¢ç©¶æ˜¯å¦å¯ä»¥é€šè¿‡ç›´æ¥åœ¨è§†é¢‘å¸§ä¸­åµŒå…¥è§†è§‰ä¿¡å·ï¼ˆå¦‚æ–‡å­—ã€ç®­å¤´ã€è½¨è¿¹ï¼‰æ¥æ§åˆ¶è§†é¢‘ç”Ÿæˆã€‚\n*   **è´¡çŒ®**ï¼šæå‡ºäº† **In-Video Instruction** èŒƒå¼ã€‚å®éªŒè¡¨æ˜ï¼ŒVeo 3.1ã€Kling 2.5 ç­‰å…ˆè¿›æ¨¡å‹èƒ½å¤Ÿç†è§£å¹¶æ‰§è¡Œè¿™äº›è§†è§‰æŒ‡ä»¤ï¼Œå®ç°æ¯”æ–‡æœ¬æç¤ºæ›´ç²¾ç»†çš„ç©ºé—´å’ŒåŠ¨ä½œæ§åˆ¶ã€‚\n\n---\n\n### âš¡ æ¨¡å‹æ•ˆç‡ä¸ç³»ç»Ÿä¼˜åŒ– (Efficiency & Systems)\n\n**144. Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models (Nemotron-Flashï¼šè¿ˆå‘å»¶è¿Ÿæœ€ä¼˜çš„æ··åˆå°è¯­è¨€æ¨¡å‹)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šNVIDIA ç ”ç©¶æŒ‡å‡ºï¼Œå‚æ•°æ•ˆç‡ä¸ç­‰äºè®¾å¤‡ç«¯çš„ä½å»¶è¿Ÿã€‚æ·±è€Œçª„ï¼ˆDeep-thinï¼‰çš„æ¨¡å‹è™½ç„¶å‡†ç¡®ç‡é«˜ï¼Œä½†å»¶è¿Ÿä¹Ÿé«˜ã€‚\n*   **è´¡çŒ®**ï¼šæå‡º **Nemotron-Flash** ç³»åˆ—ï¼Œé€šè¿‡è¿›åŒ–æœç´¢å‘ç°å»¶è¿Ÿæœ€ä¼˜çš„ç®—å­ç»„åˆï¼ˆæ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€‚ç›¸æ¯” Qwen3-1.7Bï¼Œå‡†ç¡®ç‡æå‡ 5.5%ï¼Œå»¶è¿Ÿé™ä½ 1.3 å€ï¼Œååé‡æå‡ 18.7 å€ã€‚\n\n**133. SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression (SWANï¼šé€šè¿‡å…è§£å‹ KV ç¼“å­˜å‹ç¼©å®ç°ä½æ˜¾å­˜æ¨ç†çš„ç¨€ç–ç­›é€‰æ³¨æ„åŠ›)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šé•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­ KV Cache æ˜¾å­˜å ç”¨æ˜¯ç“¶é¢ˆï¼Œç°æœ‰å‹ç¼©æ–¹æ³•å¾€å¾€éœ€è¦æ˜‚è´µçš„è§£å‹æ“ä½œã€‚\n*   **è´¡çŒ®**ï¼šSWAN ä½¿ç”¨ç¦»çº¿æ­£äº¤çŸ©é˜µæ—‹è½¬å¹¶ä¿®å‰ª KV Cacheï¼Œ**æ— éœ€è§£å‹**å³å¯ç›´æ¥ç”¨äºæ³¨æ„åŠ›è®¡ç®—ã€‚åœ¨èŠ‚çœ 50-60% æ˜¾å­˜çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æ¥è¿‘æ— å‹ç¼©åŸºçº¿ã€‚\n\n**37. ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models (ThreadWeaverï¼šè¯­è¨€æ¨¡å‹é«˜æ•ˆå¹¶è¡Œæ¨ç†çš„è‡ªé€‚åº”çº¿ç¨‹åŒ–)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šé¡ºåºè§£ç å¯¼è‡´å¤æ‚æ¨ç†ä»»åŠ¡å»¶è¿Ÿé«˜ã€‚\n*   **è´¡çŒ®**ï¼šæå‡ºäº† **ThreadWeaver**ï¼Œä¸€ç§è‡ªé€‚åº”å¹¶è¡Œæ¨ç†æ¡†æ¶ã€‚é€šè¿‡å¹¶è¡Œè½¨è¿¹ç”Ÿæˆå’Œ Trie æ ‘ç»“æ„è®­ç»ƒï¼Œåœ¨ä¸ä¿®æ”¹æ¨ç†å¼•æ“ï¼ˆå¦‚ KV Cacheï¼‰çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸ CoT ç›¸å½“çš„å‡†ç¡®ç‡ï¼Œä½†å»¶è¿Ÿé™ä½äº† 1.53 å€ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸å¯¹é½ (Safety & Alignment)\n\n**11. Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts (æç¤ºè¯å›´æ ï¼šåœ¨ LLM æç¤ºè¯ä¸­å»ºç«‹å®‰å…¨è¾¹ç•Œçš„å¯†ç å­¦æ–¹æ³•)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šæç¤ºè¯æ³¨å…¥ï¼ˆPrompt Injectionï¼‰æ˜¯ LLM çš„ä¸»è¦å¨èƒã€‚\n*   **è´¡çŒ®**ï¼šå¼•å…¥äº† **Prompt Fencing**ï¼Œåˆ©ç”¨å¯†ç å­¦ç­¾åå…ƒæ•°æ®ï¼ˆä¿¡ä»»è¯„çº§ã€å†…å®¹ç±»å‹ï¼‰è£…é¥°æç¤ºè¯ç‰‡æ®µã€‚åœ¨å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•å°†æ”»å‡»æˆåŠŸç‡ä» 86.7% é™è‡³ **0%**ï¼Œå¼€é”€æä½ã€‚\n\n**41. PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach (PropensityBenchï¼šé€šè¿‡ä»£ç†æ–¹æ³•è¯„ä¼° LLM çš„æ½œåœ¨å®‰å…¨é£é™©)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šç°æœ‰çš„å®‰å…¨è¯„ä¼°åªå…³æ³¨æ¨¡å‹â€œèƒ½åšä»€ä¹ˆâ€ï¼ˆèƒ½åŠ›ï¼‰ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹åœ¨å—å‹æˆ–è·å¾—æˆæƒæ—¶â€œä¼šåšä»€ä¹ˆâ€ï¼ˆå€¾å‘æ€§ï¼‰ã€‚\n*   **è´¡çŒ®**ï¼šå‘å¸ƒ **PropensityBench**ï¼ŒåŒ…å«è¿‘ 6000 ä¸ªåœºæ™¯ã€‚å‘ç°å³ä½¿æ¨¡å‹ç¼ºä¹ç‹¬ç«‹æ‰§è¡Œå±å®³çš„èƒ½åŠ›ï¼Œåœ¨å‹åŠ›ä¸‹ä¹Ÿè¡¨ç°å‡ºé€‰æ‹©é«˜é£é™©å·¥å…·çš„å€¾å‘ï¼ˆPropensityï¼‰ã€‚\n\n**187. Automating Deception: Scalable Multi-Turn LLM Jailbreaks (è‡ªåŠ¨åŒ–æ¬ºéª—ï¼šå¯æ‰©å±•çš„å¤šè½® LLM è¶Šç‹±)**\n*   **æ ¸å¿ƒå‘ç°**ï¼šåˆ©ç”¨å¿ƒç†å­¦ä¸­çš„â€œå¾—å¯¸è¿›å°ºâ€ï¼ˆFoot-in-the-Doorï¼‰æ•ˆåº”è¿›è¡Œå¤šè½®å¯¹è¯æ”»å‡»ã€‚\n*   **è´¡çŒ®**ï¼šæ„å»ºäº†è‡ªåŠ¨åŒ–æ”»å‡»æµæ°´çº¿ã€‚ç ”ç©¶å‘ç°ï¼ŒGPT ç³»åˆ—æ¨¡å‹å¯¹å¯¹è¯å†å²éå¸¸æ•æ„Ÿï¼ˆæ”»å‡»æˆåŠŸç‡å¢åŠ  32%ï¼‰ï¼Œè€Œ Gemini 2.5 Flash è¡¨ç°å‡ºæå¼ºçš„é˜²å¾¡èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ§¬ å…¶ä»–å€¼å¾—å…³æ³¨çš„ç ”ç©¶\n\n*   **21. fMRI-LM**: æå‡ºäº†è¿æ¥å¤§è„‘ fMRI ä¿¡å·ä¸è¯­è¨€çš„åŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†ä»è„‘æ´»åŠ¨åˆ°æ–‡æœ¬æè¿°çš„è½¬åŒ–ã€‚\n*   **51. DeCo**: é¢‘ç‡è§£è€¦çš„åƒç´ çº§æ‰©æ•£æ¨¡å‹ï¼Œå°†é«˜é¢‘ç»†èŠ‚ä¸ä½é¢‘è¯­ä¹‰åˆ†å¼€ç”Ÿæˆï¼ŒFID è¾¾åˆ° 1.62ã€‚\n*   **86. Torsion-Space Diffusion**: åœ¨æ‰­è½¬è§’ç©ºé—´è¿›è¡Œè›‹ç™½è´¨éª¨æ¶ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ï¼Œä¿è¯äº†ç”Ÿæˆçš„å‡ ä½•ç»“æ„ 100% ç¬¦åˆåŒ–å­¦é”®é•¿çº¦æŸã€‚\n*   **152. Discover, Learn, and Reinforce (DLR)**: åˆ©ç”¨ä¿¡æ¯è®ºæ¨¡å¼å‘ç°æ¡†æ¶ï¼Œé€šè¿‡ RL ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜è´¨é‡æœºå™¨äººæ“ä½œè½¨è¿¹ï¼Œç”¨äº VLA é¢„è®­ç»ƒã€‚\n*   **199. Low-Rank GEMM**: åˆ©ç”¨ FP8 åŠ é€Ÿå’Œä½ç§©è¿‘ä¼¼ä¼˜åŒ–å¤§çŸ©é˜µä¹˜æ³•ï¼Œåœ¨ RTX 4090 ä¸Šæ¯” PyTorch FP32 å¿« 7.8 å€ã€‚\n\nä»Šå¤©çš„å¿«æŠ¥å°±åˆ°è¿™é‡Œï¼ŒDeep Research Agent çš„å…´èµ·é¢„ç¤ºç€ LLM æ­£ä»å•çº¯çš„ Chatbot å‘èƒ½å¤Ÿè‡ªä¸»å®Œæˆå¤æ‚ä»»åŠ¡çš„ç ”ç©¶å‘˜è¿›åŒ–ã€‚ç¥å¤§å®¶ç§‘ç ”é¡ºåˆ©ï¼",
  "papers": [
    {
      "arxiv_id": "2511.19798v1",
      "title": "KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)",
      "title_zh": "KOMï¼šé¢å‘è†éª¨å…³èŠ‚ç‚ (KOA) ç²¾å‡†ç®¡ç†çš„å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿ",
      "authors": [
        "Weizhi Liu",
        "Xi Chen",
        "Zekun Jiang",
        "Liang Zhao",
        "Kunyuan Jiang",
        "Ruisi Tang",
        "Li Wang",
        "Mingke You",
        "Hanyu Zhou",
        "Hongyu Chen",
        "Qiankun Xiong",
        "Yong Nie",
        "Kang Li",
        "Jian Li"
      ],
      "abstract": "Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KOMï¼Œä¸€ç§æ—¨åœ¨å®ç°è†éª¨å…³èŠ‚ç‚ç²¾å‡†ç®¡ç†çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-agent system)ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–è¯„ä¼°ã€é£é™©é¢„æµ‹å’Œæ²»ç–—å¤„æ–¹ç”Ÿæˆæ¥ç¼“è§£åŒ»ç–—èµ„æºä¸è¶³çš„é—®é¢˜ã€‚è¯¥ç³»ç»ŸååŠ©ä¸´åºŠåŒ»ç”Ÿç®¡ç†å…¨å‘¨æœŸçš„æŠ¤ç†è·¯å¾„ï¼Œå¹¶èƒ½æ ¹æ®æ‚£è€…çš„ä¸ªä½“ç‰¹å¾ã€é£é™©å› ç´ åŠç¦å¿Œç—‡ç”Ÿæˆå®šåˆ¶åŒ–çš„ç®¡ç†æ–¹æ¡ˆã€‚åœ¨åŸºå‡†å®éªŒä¸­ï¼ŒKOMåœ¨å½±åƒåˆ†æå’Œå¤„æ–¹ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå¤šç§é€šç”¨çš„åŸºç¡€å¤§è¯­è¨€æ¨¡å‹ (LLMs)ã€‚ä¸€é¡¹éšæœºä¸‰è‡‚æ¨¡æ‹Ÿç ”ç©¶è¡¨æ˜ï¼ŒKOMä¸ä¸´åºŠåŒ»ç”Ÿçš„åä½œä½¿æ€»è¯Šæ–­å’Œè§„åˆ’æ—¶é—´ç¼©çŸ­äº†38.5%ï¼Œä¸”å…¶æ²»ç–—è´¨é‡ä¼˜äºå•ç‹¬ä½¿ç”¨ä¸´åºŠåŒ»ç”Ÿæˆ–AIçš„æ–¹æ¡ˆã€‚è¿™äº›å‘ç°è¯æ˜äº†KOMåœ¨æå‡æ…¢æ€§ç—…æŠ¤ç†æ•ˆç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå…¶æ¨¡å—åŒ–æ¶æ„ (Modular architecture) ä¹Ÿä¸ºå…¶ä»–æ…¢æ€§ç—…çš„AIè¾…åŠ©ç®¡ç†ç³»ç»Ÿå¼€å‘æä¾›äº†å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19798v1",
      "published_date": "2025-11-24 23:56:51 UTC",
      "updated_date": "2025-11-24 23:56:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:52:29.334142+00:00"
    },
    {
      "arxiv_id": "2511.19797v2",
      "title": "Terminal Velocity Matching",
      "title_zh": "ç»ˆç«¯é€Ÿåº¦åŒ¹é…",
      "authors": [
        "Linqi Zhou",
        "Mathias Parger",
        "Ayaan Haque",
        "Jiaming Song"
      ],
      "abstract": "We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the $2$-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Terminal Velocity Matching (TVM)ï¼Œè¿™æ˜¯ä¸€ç§ Flow Matching çš„å¹¿ä¹‰å½¢å¼ï¼Œæ—¨åœ¨å®ç°é«˜è´¨é‡çš„ä¸€æ­¥æˆ–å°‘é‡æ­¥æ•°ç”Ÿæˆå»ºæ¨¡ã€‚TVM èƒ½å¤Ÿå»ºæ¨¡ä»»æ„ä¸¤ä¸ªæ‰©æ•£æ—¶é—´æ­¥ä¹‹é—´çš„è½¬æ¢ï¼Œå¹¶é€‰æ‹©åœ¨ç»ˆç«¯æ—¶åˆ»è€Œéåˆå§‹æ—¶åˆ»å¯¹å…¶è¡Œä¸ºè¿›è¡Œæ­£åˆ™åŒ–ã€‚ç†è®ºä¸Šï¼Œç ”ç©¶è¯æ˜äº†åœ¨æ¨¡å‹æ»¡è¶³ Lipschitz Continuous æ¡ä»¶æ—¶ï¼ŒTVM ä¸ºæ•°æ®åˆ†å¸ƒä¸æ¨¡å‹åˆ†å¸ƒä¹‹é—´çš„ 2-Wasserstein Distance æä¾›äº†ä¸Šç•Œã€‚é’ˆå¯¹ Diffusion Transformers çš„ç‰¹æ€§ï¼Œç ”ç©¶é€šè¿‡å¾®å°çš„æ¶æ„è°ƒæ•´å®ç°äº†ç¨³å®šçš„å•é˜¶æ®µè®­ç»ƒï¼Œå¹¶å¼€å‘äº†æ”¯æŒ Jacobian-Vector Products åå‘ä¼ æ’­çš„ Fused Attention Kernel ä»¥ç¡®ä¿åœ¨å¤§æ¨¡å‹ä¸Šçš„é«˜æ•ˆè¿è¡Œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTVM åœ¨ ImageNet-256x256 ä»»åŠ¡ä¸­ä»…éœ€å•æ­¥æ¨ç†ï¼ˆ1-NFEï¼‰å³å¯è¾¾åˆ° 3.29 FIDï¼Œåœ¨ 4 æ­¥æ¨ç†ä¸‹è¾¾åˆ° 1.99 FIDã€‚è¯¥æ–¹æ³•åœ¨ ImageNet-512x512 ä¸ŠåŒæ ·å–å¾—äº†å“è¶Šæ€§èƒ½ï¼Œä»£è¡¨äº†ç›®å‰ä»é›¶å¼€å§‹è®­ç»ƒçš„ä¸€æ­¥æˆ–å°‘æ•°æ­¥ç”Ÿæˆæ¨¡å‹çš„æœ€å…ˆè¿›æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Blog post: https://lumalabs.ai/blog/engineering/tvm Code available at: https://github.com/lumalabs/tvm",
      "pdf_url": "https://arxiv.org/pdf/2511.19797v2",
      "published_date": "2025-11-24 23:55:45 UTC",
      "updated_date": "2025-11-26 19:48:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:52:37.433659+00:00"
    },
    {
      "arxiv_id": "2511.20713v1",
      "title": "Active Slice Discovery in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ä¸»åŠ¨åˆ‡ç‰‡å‘ç°",
      "authors": [
        "Minhui Zhang",
        "Prahar Ijner",
        "Yoav Wald",
        "Elliot Creager"
      ],
      "abstract": "Large Language Models (LLMs) often exhibit systematic errors on specific subsets of data, known as error slices. For instance, a slice can correspond to a certain demographic, where a model does poorly in identifying toxic comments regarding that demographic. Identifying error slices is crucial to understanding and improving models, but it is also challenging. An appealing approach to reduce the amount of manual annotation required is to actively group errors that are likely to belong to the same slice, while using limited access to an annotator to verify whether the chosen samples share the same pattern of model mistake. In this paper, we formalize this approach as Active Slice Discovery and explore it empirically on a problem of discovering human-defined slices in toxicity classification. We examine the efficacy of active slice discovery under different choices of feature representations and active learning algorithms. On several slices, we find that uncertainty-based active learning algorithms are most effective, achieving competitive accuracy using 2-10% of the available slice membership information, while significantly outperforming baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç‰¹å®šæ•°æ®å­é›†ä¸Šå‡ºç°çš„ç³»ç»Ÿæ€§é”™è¯¯ï¼Œå³é”™è¯¯åˆ‡ç‰‡(error slices)é—®é¢˜ï¼Œæ­£å¼æå‡ºäº†ä¸»åŠ¨åˆ‡ç‰‡å‘ç°(Active Slice Discovery)æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸»åŠ¨å°†å¯èƒ½å±äºåŒä¸€åˆ‡ç‰‡çš„é”™è¯¯è¿›è¡Œåˆ†ç»„ï¼Œå¹¶ç»“åˆæœ‰é™çš„äººå·¥æ ‡æ³¨æ¥éªŒè¯æ ·æœ¬é—´çš„é”™è¯¯æ¨¡å¼ä¸€è‡´æ€§ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è¯†åˆ«æ¨¡å‹å¼±ç‚¹æ‰€éœ€çš„æ ‡æ³¨æˆæœ¬ã€‚ç ”ç©¶è€…åœ¨æ¯’æ€§åˆ†ç±»(toxicity classification)ä»»åŠ¡ä¸­ï¼Œé’ˆå¯¹å¤šç§ç‰¹å¾è¡¨ç¤ºå’Œä¸»åŠ¨å­¦ä¹ ç®—æ³•(active learning algorithms)è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºä¸ç¡®å®šæ€§(uncertainty-based)çš„ä¸»åŠ¨å­¦ä¹ ç®—æ³•è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œä»…éœ€2-10%çš„åˆ‡ç‰‡æˆå‘˜ä¿¡æ¯å³å¯è¾¾åˆ°æé«˜çš„å‘ç°å‡†ç¡®ç‡ï¼Œæ€§èƒ½å¤§å¹…è¶…è¶ŠåŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºæ·±å…¥ç†è§£æ¨¡å‹è¡Œä¸ºã€é’ˆå¯¹æ€§ä¼˜åŒ–ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹è¡¨ç°æä¾›äº†é«˜æ•ˆä¸”ä½æˆæœ¬çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for presentation at NeurIPS 2025 - Reliable ML Workshop",
      "pdf_url": "https://arxiv.org/pdf/2511.20713v1",
      "published_date": "2025-11-24 23:43:20 UTC",
      "updated_date": "2025-11-24 23:43:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:52:59.132764+00:00"
    },
    {
      "arxiv_id": "2511.19780v1",
      "title": "NOEM$^{3}$A: A Neuro-Symbolic Ontology-Enhanced Method for Multi-Intent Understanding in Mobile Agents",
      "title_zh": "NOEM$^{3}$Aï¼šä¸€ç§é¢å‘ç§»åŠ¨æ™ºèƒ½ä½“å¤šæ„å›¾ç†è§£çš„ç¥ç»ç¬¦å·æœ¬ä½“å¢å¼ºæ–¹æ³•",
      "authors": [
        "Ioannis Tzachristas",
        "Aifen Sui"
      ],
      "abstract": "We introduce a neuro-symbolic framework for multi-intent understanding in mobile AI agents by integrating a structured intent ontology with compact language models. Our method leverages retrieval-augmented prompting, logit biasing and optional classification heads to inject symbolic intent structure into both input and output representations. We formalize a new evaluation metric-Semantic Intent Similarity (SIS)-based on hierarchical ontology depth, capturing semantic proximity even when predicted intents differ lexically. Experiments on a subset of ambiguous/demanding dialogues of MultiWOZ 2.3 (with oracle labels from GPT-o3) demonstrate that a 3B Llama model with ontology augmentation approaches GPT-4 accuracy (85% vs 90%) at a tiny fraction of the energy and memory footprint. Qualitative comparisons show that ontology-augmented models produce more grounded, disambiguated multi-intent interpretations. Our results validate symbolic alignment as an effective strategy for enabling accurate and efficient on-device NLU.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NOEM$^{3}$Aï¼Œè¿™æ˜¯ä¸€ç§é¢å‘ç§»åŠ¨AIæ™ºèƒ½ä½“(mobile AI agents)å¤šæ„å›¾ç†è§£çš„ç¥ç»ç¬¦å·æ¡†æ¶(neuro-symbolic framework)ï¼Œé€šè¿‡å°†ç»“æ„åŒ–æ„å›¾æœ¬ä½“(intent ontology)ä¸ç´§å‡‘å‹è¯­è¨€æ¨¡å‹ç›¸é›†æˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ£€ç´¢å¢å¼ºæç¤º(retrieval-augmented prompting)ã€å¯¹æ•°æ¦‚ç‡åå·®(logit biasing)ä»¥åŠå¯é€‰åˆ†ç±»å¤´ï¼Œå°†ç¬¦å·æ„å›¾ç»“æ„æ³¨å…¥åˆ°è¾“å…¥å’Œè¾“å‡ºè¡¨ç¤ºä¸­ã€‚ç ”ç©¶è€…è¿˜åŸºäºå±‚æ¬¡åŒ–æœ¬ä½“æ·±åº¦æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”è¯­ä¹‰æ„å›¾ç›¸ä¼¼åº¦(Semantic Intent Similarity, SIS)ï¼Œç”¨äºæ•æ‰è¯æ±‡é¢„æµ‹å·®å¼‚ä¸‹çš„è¯­ä¹‰æ¥è¿‘åº¦ã€‚åœ¨MultiWOZ 2.3æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡æœ¬ä½“å¢å¼ºçš„3B Llamaæ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ°äº†85%ï¼Œæ¥è¿‘GPT-4çš„90%ï¼Œä¸”èƒ½è€—å’Œå†…å­˜å ç”¨æä½ã€‚å®šæ€§åˆ†ææ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹èƒ½äº§ç”Ÿæ›´å…·è½åœ°æ„Ÿä¸”å»æ­§ä¹‰çš„å¤šæ„å›¾è§£é‡Šã€‚è¯¥ç»“æœéªŒè¯äº†ç¬¦å·å¯¹é½(symbolic alignment)æ˜¯å®ç°å‡†ç¡®é«˜æ•ˆçš„è®¾å¤‡ç«¯è‡ªç„¶è¯­è¨€ç†è§£(NLU)çš„æœ‰æ•ˆç­–ç•¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19780v1",
      "published_date": "2025-11-24 23:14:45 UTC",
      "updated_date": "2025-11-24 23:14:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:52:50.231478+00:00"
    },
    {
      "arxiv_id": "2511.19773v1",
      "title": "Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs",
      "title_zh": "é¢å‘ VLMs å·¥å…·é›†æˆæ¨ç†çš„è§„æ¨¡åŒ–æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Meng Lu",
        "Ran Xu",
        "Yi Fang",
        "Wenxuan Zhang",
        "Yue Yu",
        "Gaurav Srivastava",
        "Yuchen Zhuang",
        "Mohamed Elhoseiny",
        "Charles Fleming",
        "Carl Yang",
        "Zhengzhong Tu",
        "Yang Xie",
        "Guanghua Xiao",
        "Hanrui Wang",
        "Di Jin",
        "Wenqi Shi",
        "Xuan Wang"
      ],
      "abstract": "While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to \"think with images\", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤šæ­¥è§†è§‰äº¤äº’æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†VISTA-Gymï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ¿€åŠ±å·¥å…·é›†æˆè§†è§‰æ¨ç†èƒ½åŠ›çš„å¯æ‰©å±•è®­ç»ƒç¯å¢ƒã€‚VISTA-Gymé›†æˆäº†æ¥è‡ª13ä¸ªæ•°æ®é›†çš„7é¡¹å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œé€šè¿‡æä¾›è§†è§‰å·¥å…·ï¼ˆå¦‚groundingå’Œparsingï¼‰çš„æ ‡å‡†åŒ–æ¥å£ã€å¯æ‰§è¡Œäº¤äº’å¾ªç¯å’Œå¯éªŒè¯åé¦ˆä¿¡å·ï¼Œå®ç°äº†å¤§è§„æ¨¡çš„ä»£ç†å¼ºåŒ–å­¦ä¹ (Agentic Reinforcement Learning)ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¯¥ç¯å¢ƒï¼Œé€šè¿‡å¤šè½®è½¨è¿¹é‡‡æ ·å’Œç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒäº†VISTA-R1æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç†Ÿç»ƒåè°ƒå·¥å…·è°ƒç”¨ä¸è‡ªä¸»æ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVISTA-R1-8Båœ¨11ä¸ªæ¨ç†å¯†é›†å‹è§†è§‰é—®ç­”(VQA)åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½æ¯”åŒè§„æ¨¡çš„æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹æå‡äº†9.51%è‡³18.72%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†VISTA-Gymä½œä¸ºè®­ç»ƒå¹³å°ï¼Œèƒ½æœ‰æ•ˆè§£é”VLMsåœ¨å¤æ‚è§†è§‰åœºæ™¯ä¸‹çš„å·¥å…·é›†æˆæ¨ç†æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 9 figures, work in progress",
      "pdf_url": "https://arxiv.org/pdf/2511.19773v1",
      "published_date": "2025-11-24 22:58:26 UTC",
      "updated_date": "2025-11-24 22:58:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:52:55.940938+00:00"
    },
    {
      "arxiv_id": "2511.19768v1",
      "title": "Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering",
      "title_zh": "Prune-Then-Planï¼šå…·èº«é—®ç­”ä¸­ç¨³å®šå‰æ²¿æ¢ç´¢çš„æ­¥çº§æ ¡å‡†",
      "authors": [
        "Noah Frahm",
        "Prakrut Patel",
        "Yue Zhang",
        "Shoubin Yu",
        "Mohit Bansal",
        "Roni Sengupta"
      ],
      "abstract": "Large vision-language models (VLMs) have improved embodied question answering (EQA) agents by providing strong semantic priors for open-vocabulary reasoning. However, when used directly for step-level exploration, VLMs often exhibit frontier oscillations, unstable back-and-forth movements caused by overconfidence and miscalibration, leading to inefficient navigation and degraded answer quality. We propose Prune-Then-Plan, a simple and effective framework that stabilizes exploration through step-level calibration. Instead of trusting raw VLM scores, our method prunes implausible frontier choices using a Holm-Bonferroni inspired pruning procedure and then delegates final decisions to a coverage-based planner. This separation converts overconfident predictions into conservative, interpretable actions by relying on human-level judgments to calibrate the step-level behavior of VLMs. Integrated into the 3D-Mem EQA framework, our approach achieves relative improvements of up to 49% and 33% in visually grounded SPL and LLM-Match metrics respectively over baselines. Overall, our method achieves better scene coverage under equal exploration budgets on both OpenEQA and EXPRESS-Bench datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large vision-language models (VLMs) åœ¨ä½“ç°æ™ºèƒ½é—®ç­” (Embodied Question Answering, EQA) ä¸­å› è¿‡åº¦è‡ªä¿¡å’Œæ ¡å‡†å¤±å‡†å¯¼è‡´çš„å‰æ²¿æŒ¯è¡ (frontier oscillations) ä¸ä¸ç¨³å®šå¾€è¿”ç§»åŠ¨é—®é¢˜ï¼Œæå‡ºäº† Prune-Then-Plan æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ­¥éª¤çº§æ ¡å‡† (step-level calibration) æ¥ç¨³å®šæ¢ç´¢è¿‡ç¨‹ï¼Œé¦–å…ˆé‡‡ç”¨å— Holm-Bonferroni å¯å‘çš„å‰ªæç¨‹åºè¿‡æ»¤æ‰ä¸åˆç†çš„å‰æ²¿é€‰é¡¹ï¼Œéšåå°†æœ€ç»ˆå†³ç­–äº¤ç»™åŸºäºè¦†ç›–èŒƒå›´çš„è§„åˆ’å™¨ (coverage-based planner)ã€‚è¿™ç§è®¾è®¡å°† VLMs çš„è¿‡åº¦è‡ªä¿¡é¢„æµ‹è½¬åŒ–ä¸ºä¿å®ˆä¸”å¯è§£é‡Šçš„åŠ¨ä½œï¼Œå¹¶æˆåŠŸé›†æˆåˆ° 3D-Mem EQA æ¡†æ¶ä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ OpenEQA å’Œ EXPRESS-Bench æ•°æ®é›†ä¸Šçš„è§†è§‰å®šä½ SPL å’Œ LLM-Match æŒ‡æ ‡åˆ†åˆ«æ¯”åŸºçº¿æå‡äº† 49% å’Œ 33%ã€‚æ€»ä½“è€Œè¨€ï¼ŒPrune-Then-Plan åœ¨ç›¸åŒçš„æ¢ç´¢é¢„ç®—ä¸‹å®ç°äº†æ›´ä¼˜çš„åœºæ™¯è¦†ç›–ç‡ï¼Œæ˜¾è‘—æé«˜äº† EQA æ™ºèƒ½ä½“çš„å¯¼èˆªæ•ˆç‡å’Œå›ç­”è´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "webpage: https://noahfrahm.github.io/Prune-Then-Plan-project-page/",
      "pdf_url": "https://arxiv.org/pdf/2511.19768v1",
      "published_date": "2025-11-24 22:50:50 UTC",
      "updated_date": "2025-11-24 22:50:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:52:45.944632+00:00"
    },
    {
      "arxiv_id": "2511.20710v1",
      "title": "Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?",
      "title_zh": "å—ç¥ç»å¯å‘çš„å¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹èƒ½å¦æœ‰æ•ˆæŠµå¾¡æˆå‘˜æ¨ç†éšç§æ³„éœ²ï¼Ÿ",
      "authors": [
        "David Amebley",
        "Sayanton Dibbo"
      ],
      "abstract": "In the age of agentic AI, the growing deployment of multi-modal models (MMs) has introduced new attack vectors that can leak sensitive training data in MMs, causing privacy leakage. This paper investigates a black-box privacy attack, i.e., membership inference attack (MIA) on multi-modal vision-language models (VLMs). State-of-the-art research analyzes privacy attacks primarily to unimodal AI-ML systems, while recent studies indicate MMs can also be vulnerable to privacy attacks. While researchers have demonstrated that biologically inspired neural network representations can improve unimodal model resilience against adversarial attacks, it remains unexplored whether neuro-inspired MMs are resilient against privacy attacks. In this work, we introduce a systematic neuroscience-inspired topological regularization (tau) framework to analyze MM VLMs resilience against image-text-based inference privacy attacks. We examine this phenomenon using three VLMs: BLIP, PaliGemma 2, and ViT-GPT2, across three benchmark datasets: COCO, CC3M, and NoCaps. Our experiments compare the resilience of baseline and neuro VLMs (with topological regularization), where the tau > 0 configuration defines the NEURO variant of VLM. Our results on the BLIP model using the COCO dataset illustrate that MIA attack success in NEURO VLMs drops by 24% mean ROC-AUC, while achieving similar model utility (similarities between generated and reference captions) in terms of MPNet and ROUGE-2 metrics. This shows neuro VLMs are comparatively more resilient against privacy attacks, while not significantly compromising model utility. Our extensive evaluation with PaliGemma 2 and ViT-GPT2 models, on two additional datasets: CC3M and NoCaps, further validates the consistency of the findings. This work contributes to the growing understanding of privacy risks in MMs and provides evidence on neuro VLMs privacy threat resilience.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)åœ¨é»‘ç›’æˆå‘˜æ¨ç†æ”»å‡»(Membership Inference Attack, MIA)ä¸‹çš„éšç§æ³„éœ²é£é™©ï¼Œå¡«è¡¥äº†ç¥ç»å¯å‘æ¨¡å‹åœ¨éšç§ä¿æŠ¤éŸ§æ€§æ–¹é¢ç ”ç©¶çš„ç©ºç™½ã€‚ä½œè€…æå‡ºäº†ä¸€ç§å—ç¥ç»ç§‘å­¦å¯å‘çš„ç³»ç»Ÿæ€§æ‹“æ‰‘æ­£åˆ™åŒ–(Topological Regularization, $\\tau$)æ¡†æ¶ï¼Œç”¨äºå¢å¼ºVLMså¯¹å›¾åƒæ–‡æœ¬æ¨ç†éšç§æ”»å‡»çš„æŠµå¾¡èƒ½åŠ›ã€‚é€šè¿‡åœ¨BLIPã€PaliGemma 2å’ŒViT-GPT2æ¨¡å‹ä»¥åŠCOCOã€CC3Mç­‰æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œç ”ç©¶å‘ç°åº”ç”¨æ­£åˆ™åŒ–çš„NEUROå˜ä½“èƒ½æ˜¾è‘—æå‡å®‰å…¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNEUROç‰ˆæœ¬çš„BLIPæ¨¡å‹åœ¨ä¿æŒæ¨¡å‹æ•ˆç”¨(Utility)åŸºæœ¬ä¸å˜çš„æƒ…å†µä¸‹ï¼Œä½¿MIAæ”»å‡»æˆåŠŸç‡çš„å¹³å‡ROC-AUCä¸‹é™äº†24%ã€‚è¿™ä¸€å‘ç°åœ¨ä¸åŒæ¨¡å‹å’Œå¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å¾—åˆ°äº†ä¸€è‡´éªŒè¯ï¼Œè¯æ˜äº†å—ç¥ç»ç§‘å­¦å¯å‘çš„è¡¨ç¤ºæ–¹æ³•åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹ï¼Œå…·æœ‰æ›´å¼ºçš„éšç§ä¿æŠ¤æ½œåŠ›ã€‚è¯¥å·¥ä½œä¸ºç†è§£å¤šæ¨¡æ€æ¨¡å‹çš„éšç§é£é™©æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶ä¸ºæ„å»ºå®‰å…¨å¯ä¿¡çš„æ™ºèƒ½ä½“AIç³»ç»Ÿæä¾›äº†ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20710v1",
      "published_date": "2025-11-24 22:32:03 UTC",
      "updated_date": "2025-11-24 22:32:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:53:05.534572+00:00"
    },
    {
      "arxiv_id": "2511.20709v1",
      "title": "DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation",
      "title_zh": "DUALGAUGEï¼šé¢å‘å®‰å…¨ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§ä¸åŠŸèƒ½æ€§è‡ªåŠ¨åŒ–è”åˆåŸºå‡†æµ‹è¯•",
      "authors": [
        "Abhijeet Pathak",
        "Suvadra Barua",
        "Dinesh Gudimetla",
        "Rupam Patir",
        "Jiawei Guo",
        "Hongxin Hu",
        "Haipeng Cai"
      ],
      "abstract": "Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DUALGAUGEï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨åŒæ—¶ä¸¥æ ¼è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆä»£ç çš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ­£ç¡®æ€§çš„å…¨è‡ªåŠ¨åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°å·¥å…·å¾€å¾€æ— æ³•å…¼é¡¾å®‰å…¨æ€§ä¸æ­£ç¡®æ€§æˆ–è¯„ä¼°æ•°æ®é›†åˆ†ç¦»çš„ç¼ºé™·ï¼Œä½œè€…å¼€å‘äº† DUALGAUGE-BENCHï¼Œè¿™æ˜¯ä¸€å¥—ç»è¿‡äººå·¥éªŒè¯ä¸”è¦†ç›–å…¨é¢çš„åŸºå‡†æµ‹è¯•é›†ï¼Œä¸“é—¨ç”¨äºè”åˆè¯„ä¼°ä»£ç è´¨é‡ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒåŒ…å«ä¸€ä¸ªåœ¨æ²™ç›’ç¯å¢ƒ (sandboxed environments) ä¸­è¿è¡Œç¨‹åºçš„ agentic program executorï¼Œä»¥åŠä¸€ä¸ªç”¨äºè¯„ä¼°ä»£ç è¡Œä¸ºä¸é¢„æœŸç»“æœä¸€è‡´æ€§çš„ LLM-based evaluatorã€‚é€šè¿‡å¯¹åç§é¢†å…ˆ LLMs çš„æ•°åƒä¸ªåœºæ™¯è¿›è¡Œæµ‹è¯•ï¼Œç ”ç©¶å‘ç°å½“å‰æ¨¡å‹åœ¨åŒæ—¶ä¿è¯ä»£ç å®‰å…¨ä¸åŠŸèƒ½æ­£ç¡®æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚è¯¥å¼€æºæ¡†æ¶å’Œæ•°æ®é›†é€šè¿‡æä¾›å¯é‡å¤ä¸”å…·æœ‰æ‰©å±•æ€§çš„è¯„ä¼°æ‰‹æ®µï¼Œä¸ºæ¨åŠ¨å®‰å…¨ä»£ç ç”ŸæˆæŠ€æœ¯çš„å‘å±•æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20709v1",
      "published_date": "2025-11-24 22:26:14 UTC",
      "updated_date": "2025-11-24 22:26:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:53:08.044875+00:00"
    },
    {
      "arxiv_id": "2511.19751v1",
      "title": "Leveraging Foundation Models for Histological Grading in Cutaneous Squamous Cell Carcinoma using PathFMTools",
      "title_zh": "åˆ©ç”¨ PathFMTools ååŒåŸºç¡€æ¨¡å‹è¿›è¡Œçš®è‚¤é³çŠ¶ç»†èƒç™Œçš„ç»„ç»‡å­¦åˆ†çº§",
      "authors": [
        "Abdul Rahman Diab",
        "Emily E. Karn",
        "Renchin Wu",
        "Emily S. Ruiz",
        "William Lotter"
      ],
      "abstract": "Despite the promise of computational pathology foundation models, adapting them to specific clinical tasks remains challenging due to the complexity of whole-slide image (WSI) processing, the opacity of learned features, and the wide range of potential adaptation strategies. To address these challenges, we introduce PathFMTools, a lightweight, extensible Python package that enables efficient execution, analysis, and visualization of pathology foundation models. We use this tool to interface with and evaluate two state-of-the-art vision-language foundation models, CONCH and MUSK, on the task of histological grading in cutaneous squamous cell carcinoma (cSCC), a critical criterion that informs cSCC staging and patient management. Using a cohort of 440 cSCC H&E WSIs, we benchmark multiple adaptation strategies, demonstrating trade-offs across prediction approaches and validating the potential of using foundation model embeddings to train small specialist models. These findings underscore the promise of pathology foundation models for real-world clinical applications, with PathFMTools enabling efficient analysis and validation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç—…ç†å­¦åŸºç¡€æ¨¡å‹ (Pathology Foundation Models) åœ¨å¤„ç†å…¨åˆ‡ç‰‡å›¾åƒ (Whole-slide Image, WSI) æ—¶çš„å¤æ‚æ€§å’Œä¸´åºŠé€‚é…éš¾é¢˜ï¼Œæ¨å‡ºäº†è½»é‡çº§ä¸”å¯æ‰©å±•çš„ Python å·¥å…·åŒ… PathFMToolsã€‚PathFMTools èƒ½å¤Ÿé«˜æ•ˆæ‰§è¡Œã€åˆ†æå’Œå¯è§†åŒ–è¿™äº›åŸºç¡€æ¨¡å‹ï¼Œæå¤§åœ°ç®€åŒ–äº†å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„åº”ç”¨æµç¨‹ã€‚ç ”ç©¶è€…åˆ©ç”¨è¯¥å·¥å…·è¯„ä¼°äº†ä¸¤ç§å…ˆè¿›çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ CONCH å’Œ MUSKï¼Œå¹¶å°†å…¶åº”ç”¨äºçš®è‚¤é³çŠ¶ç»†èƒç™Œ (Cutaneous Squamous Cell Carcinoma, cSCC) çš„ç»„ç»‡å­¦åˆ†çº§ä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜Ÿä½¿ç”¨äº†åŒ…å« 440 å¼  cSCC çš„ H&E æŸ“è‰²å…¨åˆ‡ç‰‡å›¾åƒçš„æ•°æ®é›†ï¼Œå¯¹å¤šç§æ¨¡å‹é€‚é…ç­–ç•¥è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœéªŒè¯äº†åˆ©ç”¨åŸºç¡€æ¨¡å‹åµŒå…¥ (Embeddings) æ¥è®­ç»ƒå°å‹ä¸“å®¶æ¨¡å‹ (Small Specialist Models) çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æƒè¡¡äº†ä¸åŒé¢„æµ‹æ–¹æ³•çš„ä¼˜åŠ£ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åŸºç¡€æ¨¡å‹åœ¨å®é™…ä¸´åºŠåº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ï¼ŒåŒæ—¶è¯æ˜äº† PathFMTools åœ¨æ¨åŠ¨ç—…ç†å­¦åŸºç¡€æ¨¡å‹é«˜æ•ˆåˆ†æå’ŒéªŒè¯æ–¹é¢çš„æ ¸å¿ƒä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Proceedings of the 5th Machine Learning for Health (ML4H) Symposium (2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.19751v1",
      "published_date": "2025-11-24 22:16:12 UTC",
      "updated_date": "2025-11-24 22:16:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:53:10.124429+00:00"
    },
    {
      "arxiv_id": "2511.19749v1",
      "title": "Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°é¢˜ç›®ä¸æ ‡å‡†å¯¹é½çš„è§„æ¨¡åŒ–ï¼šå‡†ç¡®æ€§ã€å±€é™æ€§ä¸è§£å†³æ–¹æ¡ˆ",
      "authors": [
        "Farzan Karimi-Malekabadi",
        "Pooya Razavi",
        "Sonya Powers"
      ],
      "abstract": "As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªåŠ¨åŒ–æ•™è‚²è¯„ä¼°é¡¹ç›®ä¸è¯¾ç¨‹æ ‡å‡†å¯¹é½(Item-to-Standard Alignment)çš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿäººå·¥å®¡æŸ¥æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨è¶…è¿‡12,000ä¸ªK-5å¹´çº§çš„é¡¹ç›®-æŠ€èƒ½å¯¹ï¼Œåœ¨GPT-3.5 Turboã€GPT-4o-miniå’ŒGPT-4oä¸Šæµ‹è¯•äº†è¯†åˆ«é”™ä½é¡¹ç›®ã€é€‰æ‹©æ­£ç¡®æŠ€èƒ½å’Œç­›é€‰å€™é€‰åˆ—è¡¨ä¸‰é¡¹æ ¸å¿ƒä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4o-miniåœ¨è¯†åˆ«å¯¹é½çŠ¶æ€æ–¹é¢è¾¾åˆ°äº†83-94%çš„å‡†ç¡®ç‡ï¼Œå°½ç®¡åœ¨è¯­ä¹‰é‡å è¾ƒå¤šçš„é˜…è¯»æ ‡å‡†ä¸­è¡¨ç°ç•¥é€Šäºæ•°å­¦ã€‚é€šè¿‡å¼•å…¥é¢„è¿‡æ»¤å€™é€‰æŠ€èƒ½(candidate filtering)ç­–ç•¥ï¼Œæ­£ç¡®æŠ€èƒ½å‡ºç°åœ¨å‰äº”åå»ºè®®ä¸­çš„æ¯”ä¾‹è¶…è¿‡95%ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤æ‚åˆ†ç±»ä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶è¯æ˜äº†LLMsåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶èƒ½å¤§å¹…å‡è½»äººå·¥å®¡æŸ¥çš„è´Ÿæ‹…ï¼Œå¹¶å»ºè®®å¼€å‘ä¸€ç§ç»“åˆLLMè‡ªåŠ¨ç­›é€‰ä¸äººå·¥å¤æ ¸çš„æ··åˆå·¥ä½œæµ(hybrid pipelines)ï¼Œä¸ºå¤§è§„æ¨¡æ•™è‚²è¯„ä¼°é¡¹ç›®çš„æŒç»­éªŒè¯æä¾›å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19749v1",
      "published_date": "2025-11-24 22:12:23 UTC",
      "updated_date": "2025-11-24 22:12:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:53:13.241681+00:00"
    },
    {
      "arxiv_id": "2511.19727v1",
      "title": "Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts",
      "title_zh": "Prompt Fencingï¼šä¸€ç§åœ¨å¤§è¯­è¨€æ¨¡å‹æç¤ºè¯ä¸­å»ºç«‹å®‰å…¨è¾¹ç•Œçš„å¯†ç å­¦æ–¹æ³•",
      "authors": [
        "Steven Peh"
      ],
      "abstract": "Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we demonstrate that simulated awareness through prompt instructions achieved complete prevention of injection attacks in our experiments, reducing success rates from 86.7% (260/300 successful attacks) to 0% (0/300 successful attacks) across 300 test cases with two leading LLM providers. We implement a proof-of-concept fence generation and verification pipeline with a total overhead of 0.224 seconds (0.130s for fence generation, 0.094s for validation) across 100 samples. Our approach is platform-agnostic and can be incrementally deployed as a security layer above existing LLM infrastructure, with the expectation that future models will be trained with native fence awareness for optimal security.",
      "tldr_zh": "é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜“å—æç¤ºæ³¨å…¥(Prompt Injection)æ”»å‡»çš„å®‰å…¨æ€§éš¾é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†åä¸ºPrompt Fencingçš„åˆ›æ–°æ¶æ„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å€Ÿé‰´å¯†ç å­¦èº«ä»½éªŒè¯å’Œæ•°æ®æ¶æ„åŸåˆ™ï¼Œé€šè¿‡ä¸ºæç¤ºç‰‡æ®µæ·»åŠ åŒ…å«ä¿¡ä»»è¯„çº§å’Œå†…å®¹ç±»å‹çš„åŠ å¯†ç­¾åå…ƒæ•°æ®(Cryptographically Signed Metadata)ï¼Œåœ¨æ¨¡å‹è¾“å…¥ä¸­å»ºç«‹äº†æ˜ç¡®çš„å®‰å…¨è¾¹ç•Œã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†å—ä¿¡ä»»çš„æŒ‡ä»¤ä¸ä¸å—ä¿¡ä»»çš„å†…å®¹ï¼Œä»è€Œè§£å†³å½“å‰ç”Ÿäº§éƒ¨ç½²ä¸­é¢ä¸´çš„æœ€ä¸¥å³»å®‰å…¨å¨èƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡æ¨¡æ‹Ÿæç¤ºå›´æ æ„è¯†(Fence Awareness)ï¼Œè¯¥æ¶æ„åœ¨300ä¸ªæµ‹è¯•ç”¨ä¾‹ä¸­å°†æ³¨å…¥æ”»å‡»çš„æˆåŠŸç‡ä»86.7%é™ä½è‡³0%ï¼Œè¡¨ç°å‡ºå“è¶Šçš„é˜²å¾¡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆåœ¨ç”Ÿæˆå’ŒéªŒè¯æµæ°´çº¿ä¸Šçš„å¹³å‡æ€»å¼€é”€ä»…ä¸º0.224ç§’ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é«˜æ•ˆæ€§ã€‚Prompt Fencingå…·æœ‰å¹³å°æ— å…³æ€§ï¼Œå¯ä½œä¸ºç°æœ‰å¤§è¯­è¨€æ¨¡å‹åŸºç¡€è®¾æ–½ä¹‹ä¸Šçš„å®‰å…¨å±‚è¿›è¡Œå¢é‡éƒ¨ç½²ï¼Œä¸ºæ„å»ºå®‰å…¨ã€å¯ä¿¡çš„AIåº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "44 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2511.19727v1",
      "published_date": "2025-11-24 21:44:33 UTC",
      "updated_date": "2025-11-24 21:44:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:53:45.033254+00:00"
    },
    {
      "arxiv_id": "2511.19726v1",
      "title": "An Adaptive, Data-Integrated Agent-Based Modeling Framework for Explainable and Contestable Policy Design",
      "title_zh": "é¢å‘å¯è§£é‡Šä¸å¯è´¨ç–‘æ”¿ç­–è®¾è®¡çš„è‡ªé€‚åº”æ•°æ®é›†æˆä¸»ä½“å»ºæ¨¡æ¡†æ¶",
      "authors": [
        "Roberto Garrone"
      ],
      "abstract": "Multi-agent systems often operate under feedback, adaptation, and non-stationarity, yet many simulation studies retain static decision rules and fixed control parameters. This paper introduces a general adaptive multi-agent learning framework that integrates: (i) four dynamic regimes distinguishing static versus adaptive agents and fixed versus adaptive system parameters; (ii) information-theoretic diagnostics (entropy rate, statistical complexity, and predictive information) to assess predictability and structure; (iii) structural causal models for explicit intervention semantics; (iv) procedures for generating agent-level priors from aggregate or sample data; and (v) unsupervised methods for identifying emergent behavioral regimes. The framework offers a domain-neutral architecture for analyzing how learning agents and adaptive controls jointly shape system trajectories, enabling systematic comparison of stability, performance, and interpretability across non-equilibrium, oscillatory, or drifting dynamics. Mathematical definitions, computational operators, and an experimental design template are provided, yielding a structured methodology for developing explainable and contestable multi-agent decision processes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºä¸€ä¸ªé€šç”¨çš„è‡ªé€‚åº”å¤šæ™ºèƒ½ä½“å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå¯è§£é‡Šï¼ˆExplainableï¼‰å’Œå¯è´¨ç–‘ï¼ˆContestableï¼‰çš„æ”¿ç­–è®¾è®¡æä¾›æ”¯æŒã€‚è¯¥æ¡†æ¶é›†æˆäº†å››ç§åŠ¨æ€æœºåˆ¶æ¥åŒºåˆ†é™æ€ä¸è‡ªé€‚åº”æ™ºèƒ½ä½“ï¼Œå¹¶å¼•å…¥äº†ç†µç‡ï¼ˆEntropy Rateï¼‰ã€ç»Ÿè®¡å¤æ‚æ€§ï¼ˆStatistical Complexityï¼‰å’Œé¢„æµ‹ä¿¡æ¯ç­‰ä¿¡æ¯è®ºè¯Šæ–­æŒ‡æ ‡æ¥è¯„ä¼°ç³»ç»Ÿçš„å¯é¢„æµ‹æ€§ã€‚é€šè¿‡ç»“åˆç»“æ„å› æœæ¨¡å‹ï¼ˆStructural Causal Modelsï¼‰å’Œæ— ç›‘ç£è¡Œä¸ºè¯†åˆ«æ–¹æ³•ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿæ˜¾å¼å¤„ç†å¹²é¢„è¯­ä¹‰å¹¶æ•æ‰ç³»ç»Ÿè½¨è¿¹ä¸­çš„æ¶Œç°ç°è±¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯¦ç»†ä»‹ç»äº†ä»èšåˆæ•°æ®ç”Ÿæˆæ™ºèƒ½ä½“å…ˆéªŒçš„ç¨‹åºï¼Œå¹¶æä¾›äº†å®Œæ•´çš„æ•°å­¦å®šä¹‰ä¸å®éªŒè®¾è®¡æ¨¡æ¿ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿç³»ç»Ÿåœ°å¯¹æ¯”éå¹³è¡¡æ€æˆ–æ¼‚ç§»æ€ä¸‹çš„ç¨³å®šæ€§ä¸æ€§èƒ½ï¼Œä¸ºå¼€å‘é€æ˜ä¸”å—æ§çš„å¤šæ™ºèƒ½ä½“å†³ç­–è¿‡ç¨‹å¥ å®šäº†åšå®çš„æ–¹æ³•è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.MA",
      "comment": "27 pages, 2 case studies (emissions and smart grids). Preprint prepared during the author's PhD research at the Open University of Cyprus and the University of Milano-Bicocca. Introduces a unified framework for adaptive multi-agent learning with information-theoretic, causal, and clustering diagnostics",
      "pdf_url": "https://arxiv.org/pdf/2511.19726v1",
      "published_date": "2025-11-24 21:41:45 UTC",
      "updated_date": "2025-11-24 21:41:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:53:43.443955+00:00"
    },
    {
      "arxiv_id": "2511.19711v1",
      "title": "CrypTorch: PyTorch-based Auto-tuning Compiler for Machine Learning with Multi-party Computation",
      "title_zh": "CrypTorchï¼šåŸºäº PyTorch çš„é¢å‘å¤šæ–¹è®¡ç®—æœºå™¨å­¦ä¹ çš„è‡ªåŠ¨è°ƒä¼˜ç¼–è¯‘å™¨",
      "authors": [
        "Jinyu Liu",
        "Gang Tan",
        "Kiwan Maeng"
      ],
      "abstract": "Machine learning (ML) involves private data and proprietary model parameters. MPC-based ML allows multiple parties to collaboratively run an ML workload without sharing their private data or model parameters using multi-party computing (MPC). Because MPC cannot natively run ML operations such as Softmax or GELU, existing frameworks use different approximations. Our study shows that, on a well-optimized framework, these approximations often become the dominating bottleneck. Popular approximations are often insufficiently accurate or unnecessarily slow, and these issues are hard to identify and fix in existing frameworks. To tackle this issue, we propose a compiler for MPC-based ML, CrypTorch. CrypTorch disentangles these approximations with the rest of the MPC runtime, allows easily adding new approximations through its programming interface, and automatically selects approximations to maximize both performance and accuracy. Built as an extension to PyTorch 2's compiler, we show that CrypTorch's auto-tuning alone provides 1.20--1.7$\\times$ immediate speedup without sacrificing accuracy, and 1.31--1.8$\\times$ speedup when some accuracy degradation is allowed, compared to our well-optimized baseline. Combined with better engineering and adoption of state-of-the-art practices, the entire framework brings 3.22--8.6$\\times$ end-to-end speedup compared to the popular framework, CrypTen.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤šæ–¹è®¡ç®—(Multi-party Computation, MPC)çš„æœºå™¨å­¦ä¹ (Machine learning)åœ¨å¤„ç†Softmaxæˆ–GELUç­‰æ“ä½œæ—¶é¢ä¸´çš„è¿‘ä¼¼è®¡ç®—(Approximation)ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº†CrypTorchç¼–è¯‘å™¨ã€‚ä½œä¸ºPyTorch 2ç¼–è¯‘å™¨çš„æ‰©å±•ï¼ŒCrypTorchå°†è¿™äº›è¿‘ä¼¼æ–¹æ¡ˆä¸MPCè¿è¡Œæ—¶è§£è€¦ï¼Œå…è®¸é€šè¿‡å…¶ç¼–ç¨‹æ¥å£çµæ´»æ·»åŠ æ–°ç®—æ³•ã€‚è¯¥ç¼–è¯‘å™¨èƒ½å¤Ÿè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è¿‘ä¼¼æ–¹æ¡ˆï¼Œä»¥å®ç°åœ¨æ€§èƒ½ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCrypTorchçš„è‡ªåŠ¨è°ƒä¼˜åŠŸèƒ½åœ¨ä¸æŸå¤±å‡†ç¡®æ€§çš„æƒ…å†µä¸‹å¯å®ç°1.20è‡³1.7å€çš„å³æ—¶æé€Ÿï¼Œåœ¨å…è®¸éƒ¨åˆ†ç²¾åº¦æŸå¤±æ—¶æé€Ÿå¯è¾¾1.31è‡³1.8å€ã€‚é€šè¿‡ç»“åˆå…ˆè¿›çš„å·¥ç¨‹å®è·µï¼Œæ•´ä¸ªæ¡†æ¶ç›¸æ¯”äºæµè¡Œæ¡†æ¶CrypTenå®ç°äº†3.22è‡³8.6å€çš„ç«¯åˆ°ç«¯æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.CR",
      "comment": "28 pages, 17 figures. Submitted to PLDI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.19711v1",
      "published_date": "2025-11-24 21:21:55 UTC",
      "updated_date": "2025-11-24 21:21:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:53:44.633834+00:00"
    },
    {
      "arxiv_id": "2511.21762v1",
      "title": "Factors That Support Grounded Responses in LLM Conversations: A Rapid Review",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¯¹è¯ä¸­æœ‰æ®å›å¤çš„æ”¯æŒå› ç´ ï¼šå¿«é€Ÿç»¼è¿°",
      "authors": [
        "Gabriele Cesar Iwashima",
        "Claudia Susie Rodrigues",
        "Claudio Dipolitto",
        "Geraldo XexÃ©o"
      ],
      "abstract": "Large language models (LLMs) may generate outputs that are misaligned with user intent, lack contextual grounding, or exhibit hallucinations during conversation, which compromises the reliability of LLM-based applications. This review aimed to identify and analyze techniques that align LLM responses with conversational goals, ensure grounding, and reduce hallucination and topic drift. We conducted a Rapid Review guided by the PRISMA framework and the PICO strategy to structure the search, filtering, and selection processes. The alignment strategies identified were categorized according to the LLM lifecycle phase in which they operate: inference-time, post-training, and reinforcement learning-based methods. Among these, inference-time approaches emerged as particularly efficient, aligning outputs without retraining while supporting user intent, contextual grounding, and hallucination mitigation. The reviewed techniques provided structured mechanisms for improving the quality and reliability of LLM responses across key alignment objectives.",
      "tldr_zh": "è¿™é¡¹å¿«é€Ÿå®¡æŸ¥ (Rapid Review) æ—¨åœ¨è¯†åˆ«å¹¶åˆ†æä½¿å¤§è¯­è¨€æ¨¡å‹ (LLM) å“åº”ä¸å¯¹è¯ç›®æ ‡ä¿æŒä¸€è‡´çš„æŠ€æœ¯ï¼Œä»¥è§£å†³å¹»è§‰ (hallucinations)ã€ç¼ºä¹ä¸Šä¸‹æ–‡å…³è” (contextual grounding) å’Œä¸»é¢˜åç§» (topic drift) ç­‰å¯é æ€§é—®é¢˜ã€‚ç ”ç©¶éµå¾ª PRISMA æ¡†æ¶å’Œ PICO ç­–ç•¥ï¼Œå¯¹ LLM ç”Ÿå‘½å‘¨æœŸä¸åŒé˜¶æ®µçš„å¯¹é½ç­–ç•¥è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„æœç´¢ã€ç­›é€‰å’Œåˆ†ç±»ã€‚ç¡®å®šçš„å¯¹é½ç­–ç•¥è¢«å½’çº³ä¸ºæ¨ç†æ—¶ (inference-time)ã€è®­ç»ƒå (post-training) ä»¥åŠåŸºäºå¼ºåŒ–å­¦ä¹  (reinforcement learning-based) çš„æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨ç†æ—¶æ–¹æ³• (inference-time approaches) åœ¨æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œèƒ½é«˜æ•ˆæ”¯æŒç”¨æˆ·æ„å›¾ã€å¢å¼ºä¸Šä¸‹æ–‡å…³è”å¹¶æŠ‘åˆ¶å¹»è§‰ã€‚è¯¥ç»¼è¿°é€šè¿‡æä¾›ç»“æ„åŒ–çš„æ”¹è¿›æœºåˆ¶ï¼Œä¸ºæå‡ LLM åœ¨å„ç±»å…³é”®å¯¹é½ç›®æ ‡ä¸‹çš„å“åº”è´¨é‡ä¸åº”ç”¨å¯é æ€§å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "28 pages, 1 figure, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.21762v1",
      "published_date": "2025-11-24 21:18:46 UTC",
      "updated_date": "2025-11-24 21:18:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:53:44.437651+00:00"
    },
    {
      "arxiv_id": "2511.19703v1",
      "title": "The Alexander-Hirschowitz theorem for neurovarieties",
      "title_zh": "ç¥ç»ç°‡çš„ Alexander-Hirschowitz å®šç†",
      "authors": [
        "A. Massarenti",
        "M. Mella"
      ],
      "abstract": "We study neurovarieties for polynomial neural networks and fully characterize when they attain the expected dimension in the single-output case. As consequences, we establish non-defectiveness and global identifiability for multi-output architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šé¡¹å¼ç¥ç»ç½‘ç»œ (polynomial neural networks) çš„ neurovarietiesï¼Œæ—¨åœ¨é€šè¿‡ä»£æ•°å‡ ä½•å·¥å…·åˆ†æç¥ç»ç½‘ç»œçš„ç»“æ„ç‰¹æ€§ã€‚ä½œè€…å®Œæ•´åˆ»ç”»äº†åœ¨å•è¾“å‡º (single-output) æƒ…å†µä¸‹ï¼Œè¿™äº›ç°‡ä½•æ—¶èƒ½å¤Ÿè¾¾åˆ°é¢„æƒ³ç»´åº¦ (expected dimension)ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç†è®ºç©ºç™½ã€‚åŸºäºè¿™ä¸€æ ¸å¿ƒå‘ç°ï¼Œç ”ç©¶è¿›ä¸€æ­¥ç¡®ç«‹äº†å¤šè¾“å‡ºæ¶æ„ (multi-output architectures) çš„éç¼ºé™·æ€§ (non-defectiveness)ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜è¯æ˜äº†å¤šè¾“å‡ºæ¶æ„çš„å…¨å±€å¯è¯†åˆ«æ€§ (global identifiability)ï¼Œç¡®ä¿äº†ç½‘ç»œå‚æ•°ä¸æ‰€å®ç°å‡½æ•°ä¹‹é—´çš„å”¯ä¸€å¯¹åº”å…³ç³»ã€‚é€šè¿‡è¿™äº›ç»“è®ºï¼Œè¯¥ç ”ç©¶æˆåŠŸåœ°å°†ç»å…¸çš„ Alexander-Hirschowitz theorem æ¨å¹¿åˆ°äº† neurovarieties é¢†åŸŸã€‚è¿™ä¸€æˆæœä¸ºç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‡ ä½•è¡¨è¾¾ä»¥åŠå‚æ•°åŒ–è¿‡ç¨‹å¥ å®šäº†åšå®çš„æ•°å­¦åŸºç¡€ã€‚",
      "categories": [
        "math.AG",
        "cs.AI",
        "cs.LG",
        "math.AC"
      ],
      "primary_category": "math.AG",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.19703v1",
      "published_date": "2025-11-24 21:09:42 UTC",
      "updated_date": "2025-11-24 21:09:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:53:53.239107+00:00"
    },
    {
      "arxiv_id": "2511.19699v3",
      "title": "A Layered Protocol Architecture for the Internet of Agents",
      "title_zh": "é¢å‘æ™ºèƒ½ä½“äº’è”ç½‘çš„åˆ†å±‚åè®®æ¶æ„",
      "authors": [
        "Charles Fleming",
        "Luca Muscariello",
        "Vijoy Pandey",
        "Ramana Kompella"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance improvements and the ability to learn domain-specific languages (DSLs), including APIs and tool interfaces. This capability has enabled the creation of AI agents that can perform preliminary computations and act through tool calling, which is now being standardized via protocols like MCP. However, LLMs face fundamental limitations: their context windows cannot grow indefinitely, restricting their memory and computational capacity. Agent collaboration emerges as essential for solving increasingly complex problems, mirroring how computational systems rely on different types of memory to scale. The \"Internet of Agents\" (IoA) represents the communication stack that enables agents to scale by distributing computation across collaborating entities.\n  Current network architectural stacks (OSI and TCP/IP) were designed for data delivery between hosts and processes, not for agent collaboration with semantic understanding. To address this gap, we propose two new layers: an Agent Communication Layer (L8) and an Agent Semantic Layer (L9). L8 formalizes the structure of communication, standardizing message envelopes, speech-act performatives (e.g., REQUEST, INFORM), and interaction patterns (e.g., request-reply, publish-subscribe), building on protocols like MCP. The proposed L9 layer: (1) formalizes semantic context discovery and negotiation, (2) provides semantic grounding by binding terms to semantic context, and (3) semantically validates incoming prompts and performs disambiguation as needed. Furthermore, L9 introduces primitives for coordination and consensus, allowing agents to achieve alignment on shared states, collective goals, and distributed beliefs. Together, these layers provide the foundation for scalable, distributed agent collaboration, enabling the next generation of multi-agentic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸Šä¸‹æ–‡çª—å£å’Œè®¡ç®—èƒ½åŠ›ä¸Šçš„å±€é™æ€§ï¼ŒæŒ‡å‡ºä»£ç†åä½œæ˜¯å®ç°å¤æ‚ä»»åŠ¡æ‰©å±•çš„å…³é”®ï¼Œå¹¶æå‡ºäº†â€œä»£ç†äº’è”ç½‘â€(Internet of Agents, IoA)è¿™ä¸€é€šä¿¡æ ˆæ¦‚å¿µã€‚ç”±äºç°æœ‰çš„OSIå’ŒTCP/IPæ¶æ„ä¸»è¦é’ˆå¯¹ä¸»æœºé—´çš„æ•°æ®ä¼ è¾“è€Œéè¯­ä¹‰åä½œï¼Œä½œè€…æå‡ºäº†ä»£ç†é€šä¿¡å±‚(Agent Communication Layer, L8)å’Œä»£ç†è¯­ä¹‰å±‚(Agent Semantic Layer, L9)ä¸¤å±‚æ–°æ¶æ„ã€‚L8å±‚åˆ©ç”¨MCPç­‰åè®®å½¢å¼åŒ–äº†é€šä¿¡ç»“æ„ï¼Œæ ‡å‡†åŒ–äº†æ¶ˆæ¯ä¿¡å°ã€è¨€è¯­è¡Œä¸ºè¡¨ç°(speech-act performatives)åŠäº¤äº’æ¨¡å¼ã€‚L9å±‚åˆ™è´Ÿè´£è¯­ä¹‰ä¸Šä¸‹æ–‡çš„å‘ç°ä¸åå•†ï¼Œé€šè¿‡è¯­ä¹‰å¯¹é½(semantic grounding)å’Œæ¶ˆæ­§ç¡®ä¿ä¿¡æ¯ç†è§£çš„å‡†ç¡®æ€§ï¼Œå¹¶å¼•å…¥åŸè¯­ä»¥å®ç°ä»£ç†é—´çš„åè°ƒä¸å…±è¯†ã€‚è¿™ä¸¤å±‚åè®®å…±åŒä¸ºå¯æ‰©å±•çš„åˆ†å¸ƒå¼ä»£ç†åä½œå¥ å®šäº†åŸºç¡€ï¼Œä¸ºä¸‹ä¸€ä»£å¤šä»£ç†ç³»ç»Ÿ(multi-agentic systems)çš„æ„å»ºæä¾›äº†æ ¸å¿ƒæ”¯æ’‘ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19699v3",
      "published_date": "2025-11-24 21:06:14 UTC",
      "updated_date": "2026-01-20 23:06:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:06.430254+00:00"
    },
    {
      "arxiv_id": "2511.19694v2",
      "title": "TiCT: A Synthetically Pre-Trained Foundation Model for Time Series Classification",
      "title_zh": "TiCTï¼šç”¨äºæ—¶é—´åºåˆ—åˆ†ç±»çš„åˆæˆé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹",
      "authors": [
        "Chin-Chia Michael Yeh",
        "Uday Singh Saini",
        "Junpeng Wang",
        "Xin Dai",
        "Xiran Fan",
        "Jiarui Sun",
        "Yujie Fan",
        "Yan Zheng"
      ],
      "abstract": "The ubiquity of time series data creates a strong demand for general-purpose foundation models, yet developing them for classification remains a significant challenge, largely due to the high cost of labeled data. Foundation models capable of in-context learning (ICL) offer a powerful solution, adapting to new tasks with minimal examples and reducing the need for extensive retraining. However, prior work on large-scale time series models has predominantly focused on forecasting, leaving a critical gap for versatile, fine-tuning-free classification. To address this, we introduce TiCT (Time-series in-Context Transformer), a transformer-based model pre-trained exclusively on synthetic data to perform in-context classification. We make two primary technical contributions: 1) a novel architecture featuring a scalable bit-based label encoding and a special output attention mechanism to handle an arbitrary number of classes; and 2) a synthetic pre-training framework that combines a Mixup-inspired process with data augmentation to foster generalization and noise invariance. Extensive evaluations on the UCR Archive show that TiCT achieves competitive performance against state-of-the-art supervised methods. Crucially, this is accomplished using only in-context examples at inference time, without updating a single model weight.",
      "tldr_zh": "é’ˆå¯¹æ—¶é—´åºåˆ—åˆ†ç±»(Time Series Classification)é¢†åŸŸå› æ ‡æ³¨æ•°æ®æˆæœ¬é«˜æ˜‚è€Œç¼ºä¹é€šç”¨åŸºç¡€æ¨¡å‹çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†TiCT (Time-series in-Context Transformer)ï¼Œä¸€ç§ä¸“é—¨ç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning)çš„TransformeråŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹å®Œå…¨åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œé€šè¿‡å¼•å…¥å¯æ‰©å±•çš„ä½æ ‡ç­¾ç¼–ç (bit-based label encoding)å’Œç‰¹æ®Šçš„è¾“å‡ºæ³¨æ„åŠ›æœºåˆ¶(output attention mechanism)ï¼Œå…·å¤‡äº†å¤„ç†ä»»æ„æ•°é‡ç±»åˆ«çš„èƒ½åŠ›ã€‚ä¸ºäº†æå‡æ³›åŒ–èƒ½åŠ›å’Œå™ªå£°ä¸å˜æ€§ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº†ç»“åˆMixupå¯å‘è¿‡ç¨‹ä¸æ•°æ®å¢å¼º(data augmentation)çš„åˆæˆé¢„è®­ç»ƒæ¡†æ¶ã€‚åœ¨UCR Archiveæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTiCTåœ¨æ¨ç†æ—¶æ— éœ€æ›´æ–°æ¨¡å‹æƒé‡ï¼Œä»…å‡­ä¸Šä¸‹æ–‡ç¤ºä¾‹å³å¯è¾¾åˆ°ä¸å½“å‰æœ€å…ˆè¿›çš„æœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•ç«äº‰çš„æ€§èƒ½ã€‚è¯¥å·¥ä½œæœ‰æ•ˆå¡«è¡¥äº†å¤§è§„æ¨¡æ—¶é—´åºåˆ—æ¨¡å‹åœ¨æ— éœ€å¾®è°ƒçš„åˆ†ç±»ä»»åŠ¡ä¸Šçš„ç©ºç™½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19694v2",
      "published_date": "2025-11-24 20:57:55 UTC",
      "updated_date": "2025-11-26 07:35:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:05.835000+00:00"
    },
    {
      "arxiv_id": "2511.19693v2",
      "title": "TREASURE: A Transformer-Based Foundation Model for High-Volume Transaction Understanding",
      "title_zh": "TREASUREï¼šé¢å‘æµ·é‡äº¤æ˜“ç†è§£çš„ Transformer åŸºåº§æ¨¡å‹",
      "authors": [
        "Chin-Chia Michael Yeh",
        "Uday Singh Saini",
        "Xin Dai",
        "Xiran Fan",
        "Shubham Jain",
        "Yujie Fan",
        "Jiarui Sun",
        "Junpeng Wang",
        "Menghai Pan",
        "Yingtong Dou",
        "Yuzhong Chen",
        "Vineeth Rakesh",
        "Liang Wang",
        "Yan Zheng",
        "Mahashweta Das"
      ],
      "abstract": "Payment networks form the backbone of modern commerce, generating high volumes of transaction records from daily activities. Properly modeling this data can enable applications such as abnormal behavior detection and consumer-level insights for hyper-personalized experiences, ultimately improving people's lives. In this paper, we present TREASURE, TRansformer Engine As Scalable Universal transaction Representation Encoder, a multipurpose transformer-based foundation model specifically designed for transaction data. The model simultaneously captures both consumer behavior and payment network signals (such as response codes and system flags), providing comprehensive information necessary for applications like accurate recommendation systems and abnormal behavior detection. Verified with industry-grade datasets, TREASURE features three key capabilities: 1) an input module with dedicated sub-modules for static and dynamic attributes, enabling more efficient training and inference; 2) an efficient and effective training paradigm for predicting high-cardinality categorical attributes; and 3) demonstrated effectiveness as both a standalone model that increases abnormal behavior detection performance by 111% over production systems and an embedding provider that enhances recommendation models by 104%. We present key insights from extensive ablation studies, benchmarks against production models, and case studies, highlighting valuable knowledge gained from developing TREASURE.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TREASURE (TRansformer Engine As Scalable Universal transaction Representation Encoder)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å¤§è§„æ¨¡äº¤æ˜“æ•°æ®è®¾è®¡çš„ Transformer åŸºç¡€æ¨¡å‹ (foundation model)ã€‚è¯¥æ¨¡å‹èƒ½å¤ŸåŒæ—¶æ•æ‰æ¶ˆè´¹è€…è¡Œä¸ºå’Œæ”¯ä»˜ç½‘ç»œä¿¡å·ï¼Œä¸ºå¼‚å¸¸è¡Œä¸ºæ£€æµ‹ (abnormal behavior detection) å’Œé«˜ç²¾åº¦æ¨èç³»ç»Ÿæä¾›å…¨é¢çš„ä¿¡æ¯æ”¯æ’‘ã€‚TREASURE åˆ›æ–°æ€§åœ°é‡‡ç”¨äº†ä¸“é—¨å¤„ç†é™æ€ä¸åŠ¨æ€å±æ€§çš„è¾“å…¥æ¨¡å—ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§é’ˆå¯¹é«˜åŸºæ•°ç±»åˆ«å±æ€§ (high-cardinality categorical attributes) çš„é«˜æ•ˆè®­ç»ƒèŒƒå¼ã€‚åœ¨è¡Œä¸šçº§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½œä¸ºç‹¬ç«‹æ¨¡å‹æ—¶ï¼ŒTREASURE åœ¨å¼‚å¸¸è¡Œä¸ºæ£€æµ‹æ€§èƒ½ä¸Šæ¯”ç°æœ‰ç”Ÿäº§ç³»ç»Ÿæå‡äº† 111%ï¼›è€Œä½œä¸ºåµŒå…¥æä¾›è€… (embedding provider) æ—¶ï¼Œå®ƒä½¿æ¨èæ¨¡å‹çš„è¡¨ç°æå‡äº† 104%ã€‚è¯¥ç ”ç©¶é€šè¿‡å¹¿æ³›çš„æ¶ˆèå®éªŒå’ŒåŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†åœ¨å¤§è§„æ¨¡äº¤æ˜“ç†è§£é¢†åŸŸæ„å»ºé€šç”¨è¡¨ç¤ºç¼–ç å™¨çš„æœ‰æ•ˆæ€§ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19693v2",
      "published_date": "2025-11-24 20:57:31 UTC",
      "updated_date": "2025-11-26 17:43:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:03.847058+00:00"
    },
    {
      "arxiv_id": "2511.19684v1",
      "title": "IndEgo: A Dataset of Industrial Scenarios and Collaborative Work for Egocentric Assistants",
      "title_zh": "IndEgoï¼šé¢å‘ç¬¬ä¸€è§†è§’åŠ©æ‰‹çš„å·¥ä¸šåœºæ™¯ä¸ååŒå·¥ä½œæ•°æ®é›†",
      "authors": [
        "Vivek Chavan",
        "Yasmina Imgrund",
        "Tung Dao",
        "Sanwantri Bai",
        "Bosong Wang",
        "Ze Lu",
        "Oliver Heimann",
        "JÃ¶rg KrÃ¼ger"
      ],
      "abstract": "We introduce IndEgo, a multimodal egocentric and exocentric dataset addressing common industrial tasks, including assembly/disassembly, logistics and organisation, inspection and repair, woodworking, and others. The dataset contains 3,460 egocentric recordings (approximately 197 hours), along with 1,092 exocentric recordings (approximately 97 hours). A key focus of the dataset is collaborative work, where two workers jointly perform cognitively and physically intensive tasks. The egocentric recordings include rich multimodal data and added context via eye gaze, narration, sound, motion, and others. We provide detailed annotations (actions, summaries, mistake annotations, narrations), metadata, processed outputs (eye gaze, hand pose, semi-dense point cloud), and benchmarks on procedural and non-procedural task understanding, Mistake Detection, and reasoning-based Question Answering. Baseline evaluations for Mistake Detection, Question Answering and collaborative task understanding show that the dataset presents a challenge for the state-of-the-art multimodal models. Our dataset is available at: https://huggingface.co/datasets/FraunhoferIPK/IndEgo",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† IndEgoï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–ç»„è£…æ‹†å¸ã€ç‰©æµç»„ç»‡ã€æ£€æŸ¥ç»´ä¿®åŠæœ¨å·¥ç­‰å·¥ä¸šåœºæ™¯çš„å¤šæ¨¡æ€ç¬¬ä¸€äººç§°(egocentric)ä¸ç¬¬ä¸‰äººç§°(exocentric)æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«çº¦ 197 å°æ—¶çš„ç¬¬ä¸€äººç§°è®°å½•å’Œ 97 å°æ—¶çš„ç¬¬ä¸‰äººç§°è®°å½•ï¼Œé‡ç‚¹å…³æ³¨ä¸¤åå·¥äººååŒå®Œæˆé«˜è®¤çŸ¥ä¸ä½“åŠ›å¼ºåº¦ä»»åŠ¡çš„åä½œè¿‡ç¨‹ã€‚æ•°æ®é›†é›†æˆäº†è§†çº¿(eye gaze)ã€æ‰‹åŠ¿ã€å£°éŸ³å’Œè¿åŠ¨ç­‰ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶æä¾›åŠ¨ä½œã€é”™è¯¯æ ‡æ³¨åŠå™è¿°ç­‰è¯¦ç»†å…ƒæ•°æ®ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹ä»»åŠ¡ç†è§£ã€é”™è¯¯æ£€æµ‹(Mistake Detection)å’ŒåŸºäºæ¨ç†çš„é—®ç­”(Question Answering)è®¾å®šäº†åŸºå‡†æµ‹è¯•ã€‚åŸºå‡†è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒIndEgo å¯¹å½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹æ„æˆäº†æ˜¾è‘—æŒ‘æˆ˜ï¼Œä¸ºå·¥ä¸šç¯å¢ƒä¸‹çš„æ™ºèƒ½è¾…åŠ©æŠ€æœ¯ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS 2025 D&B Track. Project Page: https://indego-dataset.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2511.19684v1",
      "published_date": "2025-11-24 20:45:17 UTC",
      "updated_date": "2025-11-24 20:45:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:12.136698+00:00"
    },
    {
      "arxiv_id": "2511.20705v1",
      "title": "Solving Diffusion Inverse Problems with Restart Posterior Sampling",
      "title_zh": "åŸºäºé‡å¯åéªŒé‡‡æ ·çš„æ‰©æ•£é€†é—®é¢˜æ±‚è§£",
      "authors": [
        "Bilal Ahmed",
        "Joseph G. Makin"
      ],
      "abstract": "Inverse problems are fundamental to science and engineering, where the goal is to infer an underlying signal or state from incomplete or noisy measurements. Recent approaches employ diffusion models as powerful implicit priors for such problems, owing to their ability to capture complex data distributions. However, existing diffusion-based methods for inverse problems often rely on strong approximations of the posterior distribution, require computationally expensive gradient backpropagation through the score network, or are restricted to linear measurement models.\n  In this work, we propose Restart for Posterior Sampling (RePS), a general and efficient framework for solving both linear and non-linear inverse problems using pre-trained diffusion models. RePS builds on the idea of restart-based sampling, previously shown to improve sample quality in unconditional diffusion, and extends it to posterior inference. Our method employs a conditioned ODE applicable to any differentiable measurement model and introduces a simplified restart strategy that contracts accumulated approximation errors during sampling. Unlike some of the prior approaches, RePS avoids backpropagation through the score network, substantially reducing computational cost.\n  We demonstrate that RePS achieves faster convergence and superior reconstruction quality compared to existing diffusion-based baselines across a range of inverse problems, including both linear and non-linear settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸä¸­ä»å™ªå£°æˆ–ä¸å®Œæ•´æµ‹é‡ä¸­æ¨æ–­ä¿¡å·çš„Inverse problemsï¼Œæå‡ºäº†åä¸ºRestart for Posterior Sampling (RePS) çš„é«˜æ•ˆé€šç”¨æ¡†æ¶ã€‚ç°æœ‰çš„åŸºäºDiffusion modelsçš„æ–¹æ³•å¾€å¾€é¢ä¸´åéªŒåˆ†å¸ƒè¿‘ä¼¼è¿‡å¼ºã€åœ¨Score networkä¸Šæ‰§è¡Œæ¢¯åº¦åå‘ä¼ æ’­(Backpropagation)è®¡ç®—å¼€é”€å¤§æˆ–ä»…é™äºLinear measurement modelsç­‰æŒ‘æˆ˜ã€‚RePSé€šè¿‡å°†Restart-based samplingæŠ€æœ¯æ‰©å±•è‡³åéªŒæ¨ç†ï¼Œæ„å»ºäº†é€‚ç”¨äºä»»ä½•å¯å¾®æµ‹é‡æ¨¡å‹çš„Conditioned ODEï¼Œå¹¶åˆ©ç”¨ç®€åŒ–çš„é‡å¯ç­–ç•¥æ¥æ¶ˆé™¤é‡‡æ ·ä¸­ç´¯ç§¯çš„è¿‘ä¼¼è¯¯å·®ã€‚ç”±äºè¯¥æ–¹æ³•æ— éœ€é€šè¿‡Score networkè¿›è¡Œåå‘ä¼ æ’­ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡å¹¶é™ä½äº†èµ„æºéœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRePSåœ¨Linearå’ŒNon-linearé€†é—®é¢˜ä¸­å‡æ¯”ç°æœ‰åŸºå‡†æ¨¡å‹è¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å“è¶Šçš„å›¾åƒé‡å»ºè´¨é‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20705v1",
      "published_date": "2025-11-24 20:42:33 UTC",
      "updated_date": "2025-11-24 20:42:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:20.737760+00:00"
    },
    {
      "arxiv_id": "2511.21760v2",
      "title": "fMRI-LM: Towards a Universal Foundation Model for Language-Aligned fMRI Understanding",
      "title_zh": "fMRI-LMï¼šè¿ˆå‘è¯­è¨€å¯¹é½ fMRI ç†è§£çš„é€šç”¨åŸºç¡€æ¨¡å‹",
      "authors": [
        "Yuxiang Wei",
        "Yanteng Zhang",
        "Xi Xiao",
        "Chengxuan Qian",
        "Tianyang Wang",
        "Vince D. Calhoun"
      ],
      "abstract": "Recent advances in multimodal large language models (LLMs) have enabled unified reasoning across images, audio, and video, but extending such capability to brain imaging remains largely unexplored. Bridging this gap is essential to link neural activity with semantic cognition and to develop cross-modal brain representations. To this end, we present fMRI-LM, a foundational model that bridges functional MRI (fMRI) and language through a three-stage framework. In Stage 1, we learn a neural tokenizer that maps fMRI into discrete tokens embedded in a language-consistent space. In Stage 2, a pretrained LLM is adapted to jointly model fMRI tokens and text, treating brain activity as a sequence that can be temporally predicted and linguistically described. To overcome the lack of natural fMRI-text pairs, we construct a large descriptive corpus that translates diverse imaging-based features into structured textual descriptors, capturing the low-level organization of fMRI signals. In Stage 3, we perform multi-task, multi-paradigm instruction tuning to endow fMRI-LM with high-level semantic understanding, supporting diverse downstream applications. Across various benchmarks, fMRI-LM achieves strong zero-shot and few-shot performance, and adapts efficiently with parameter-efficient tuning (LoRA), establishing a scalable pathway toward a language-aligned, universal model for structural and semantic understanding of fMRI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† fMRI-LMï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†åŠŸèƒ½ç£å…±æŒ¯æˆåƒ (fMRI) ä¸è¯­è¨€å¯¹é½çš„é€šç”¨åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨å¡«è¡¥è„‘æˆåƒä¸è¯­ä¹‰è®¤çŸ¥ä¹‹é—´çš„è¿æ¥ç©ºç™½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œé¦–å…ˆé€šè¿‡ç¥ç»åˆ†è¯å™¨ (Neural Tokenizer) å°† fMRI ä¿¡å·æ˜ å°„åˆ°è¯­è¨€ä¸€è‡´ç©ºé—´ä¸­çš„ç¦»æ•£æ ‡è®° (Tokens)ï¼›éšååˆ©ç”¨é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ (LLM) è”åˆå»ºæ¨¡æ ‡è®°ä¸æ–‡æœ¬ï¼Œå¹¶ç»“åˆå¤§è§„æ¨¡æè¿°æ€§è¯­æ–™åº“æ¥å…‹æœè‡ªç„¶å¯¹é½æ•°æ®åŒ®ä¹çš„æŒ‘æˆ˜ã€‚ç¬¬ä¸‰é˜¶æ®µé€šè¿‡å¤šä»»åŠ¡å’Œå¤šèŒƒå¼çš„æŒ‡ä»¤å¾®è°ƒ (Instruction Tuning) èµ‹äºˆæ¨¡å‹é«˜å±‚è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä½¿å…¶æ”¯æŒå¤šæ ·åŒ–çš„ä¸‹æ¸¸åº”ç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒfMRI-LM åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬ (Zero-shot) å’Œå°‘æ ·æœ¬ (Few-shot) æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æ”¯æŒé€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ (LoRA) è¿›è¡Œå¿«é€Ÿé€‚é…ï¼Œä¸ºæ„å»ºå…·å¤‡ç»“æ„ä¸è¯­ä¹‰ç†è§£èƒ½åŠ›çš„é€šç”¨ fMRI åŸºç¡€æ¨¡å‹å¼€è¾Ÿäº†å¯æ‰©å±•çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code are available: https://github.com/yuxiangwei0808/fMRI-LM",
      "pdf_url": "https://arxiv.org/pdf/2511.21760v2",
      "published_date": "2025-11-24 20:26:59 UTC",
      "updated_date": "2025-12-26 15:54:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:35.840914+00:00"
    },
    {
      "arxiv_id": "2511.19671v1",
      "title": "FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking",
      "title_zh": "FISCALï¼šé¢å‘é«˜æ•ˆäº‹å®æ ¸æŸ¥çš„é‡‘èåˆæˆå£°æ˜-æ–‡æ¡£å¢å¼ºå­¦ä¹ ",
      "authors": [
        "Rishab Sharma",
        "Iman Saberi",
        "Elham Alipour",
        "Jie JW Wu",
        "Fatemeh Fard"
      ],
      "abstract": "Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FISCALï¼ˆFinancial Synthetic Claim-document Augmented Learningï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç”Ÿæˆé‡‘èäº‹å®æ ¸æŸ¥åˆæˆæ•°æ®çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡‘èåº”ç”¨ä¸­é¢ä¸´çš„è™šå‡å¹»è§‰å’Œè®¡ç®—æ•ˆç‡é—®é¢˜ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œç ”ç©¶è€…æ„å»ºäº†FISCAL-dataæ•°æ®é›†å¹¶è®­ç»ƒå‡ºåä¸ºMiniCheck-FISCALçš„è½»é‡çº§éªŒè¯å™¨ï¼Œä¸“é—¨ç”¨äºæ•°å€¼å‹é‡‘èå£°æ˜çš„æ ¸æŸ¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMiniCheck-FISCALçš„æ€§èƒ½ä¸ä»…è¶…è¶Šäº†GPT-3.5 TurboåŠå…¶åŒè§„æ¨¡å¼€æºæ¨¡å‹ï¼Œè¿˜æ¥è¿‘äº†Mixtral-8x22Bå’ŒCommand R+ç­‰è§„æ¨¡å¤§20å€çš„å¤§å‹ç³»ç»Ÿã€‚åœ¨FinDVerå’ŒFin-Factç­‰å¤–éƒ¨æ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å‹çš„è¡¨ç°å¯ä¸GPT-4oå’ŒClaude-3.5ç›¸åª²ç¾ï¼Œä¸”ä¼˜äºGemini-1.5 Flashã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†é¢†åŸŸç‰¹å®šçš„åˆæˆæ•°æ®ç»“åˆé«˜æ•ˆå¾®è°ƒï¼Œèƒ½ä½¿å°å‹æ¨¡å‹åœ¨å®é™…é‡‘èAIåº”ç”¨ä¸­è¾¾åˆ°æœ€å…ˆè¿›ï¼ˆState-of-the-Artï¼‰çš„å‡†ç¡®æ€§ã€é²æ£’æ€§ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "3 tables, 11 pages, 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Generative AI in Finance",
      "pdf_url": "https://arxiv.org/pdf/2511.19671v1",
      "published_date": "2025-11-24 20:11:44 UTC",
      "updated_date": "2025-11-24 20:11:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:37.141136+00:00"
    },
    {
      "arxiv_id": "2511.19669v1",
      "title": "HeaRT: A Hierarchical Circuit Reasoning Tree-Based Agentic Framework for AMS Design Optimization",
      "title_zh": "HeaRTï¼šåŸºäºåˆ†å±‚ç”µè·¯æ¨ç†æ ‘çš„ AMS è®¾è®¡ä¼˜åŒ–æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Souradip Poddar",
        "Chia-Tung Ho",
        "Ziming Wei",
        "Weidong Cao",
        "Haoxing Ren",
        "David Z. Pan"
      ],
      "abstract": "Conventional AI-driven AMS design automation algorithms remain constrained by their reliance on high-quality datasets to capture underlying circuit behavior, coupled with poor transferability across architectures, and a lack of adaptive mechanisms. This work proposes HeaRT, a foundational reasoning engine for automation loops and a first step toward intelligent, adaptive, human-style design optimization. HeaRT consistently demonstrates reasoning accuracy >97% and Pass@1 performance >98% across our 40-circuit benchmark repository, even as circuit complexity increases, while operating at <0.5x real-time token budget of SOTA baselines. Our experiments show that HeaRT yields >3x faster convergence in both sizing and topology design adaptation tasks across diverse optimization approaches, while preserving prior design intent.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HeaRTï¼Œä¸€ç§åŸºäºåˆ†å±‚ç”µè·¯æ¨ç†æ ‘ (Hierarchical Circuit Reasoning Tree) çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¨¡æ‹Ÿæ··åˆä¿¡å· (AMS) è®¾è®¡è‡ªåŠ¨åŒ–ä¸­ä¼ ç»Ÿ AI ç®—æ³•å¯¹é«˜è´¨é‡æ•°æ®çš„ä¾èµ–ã€è¿ç§»æ€§å·®åŠç¼ºä¹è‡ªé€‚åº”æœºåˆ¶ç­‰æŒ‘æˆ˜ã€‚HeaRT ä½œä¸ºä¸€ç§åŸºç¡€æ¨ç†å¼•æ“ï¼Œåˆæ­¥å®ç°äº†ç±»ä¼¼äººç±»æ€ç»´çš„æ™ºèƒ½è‡ªé€‚åº”è®¾è®¡ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åŒ…å« 40 ä¸ªç”µè·¯çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHeaRT çš„æ¨ç†å‡†ç¡®ç‡è¶…è¿‡ 97%ï¼ŒPass@1 æ€§èƒ½ä¼˜äº 98%ï¼Œä¸”å…¶ token é¢„ç®—ä¸è¶³ç°æœ‰ SOTA åŸºå‡†çš„ä¸€åŠã€‚åœ¨å°ºå¯¸æ ‡æ³¨ (sizing) å’Œæ‹“æ‰‘è®¾è®¡ (topology design) çš„é€‚é…ä»»åŠ¡ä¸­ï¼ŒHeaRT ä½¿æ”¶æ•›é€Ÿåº¦æå‡äº† 3 å€ä»¥ä¸Šï¼ŒåŒæ—¶èƒ½å¤Ÿæœ‰æ•ˆä¿ç•™å…ˆå‰çš„è®¾è®¡æ„å›¾ (design intent)ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†å…¶åœ¨å¤„ç†å¤æ‚ç”µè·¯ä»»åŠ¡æ—¶çš„å“è¶Šæ€§èƒ½ï¼Œä¸ºå®ç°é«˜æ•ˆã€å¯è¿ç§»çš„ç”µè·¯è®¾è®¡è‡ªåŠ¨åŒ–æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19669v1",
      "published_date": "2025-11-24 20:11:06 UTC",
      "updated_date": "2025-11-24 20:11:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:49.933536+00:00"
    },
    {
      "arxiv_id": "2511.19663v1",
      "title": "Fara-7B: An Efficient Agentic Model for Computer Use",
      "title_zh": "Fara-7Bï¼šä¸€ç§é¢å‘è®¡ç®—æœºæ“ä½œçš„é«˜æ•ˆæ™ºèƒ½ä½“æ¨¡å‹",
      "authors": [
        "Ahmed Awadallah",
        "Yash Lara",
        "Raghav Magazine",
        "Hussein Mozannar",
        "Akshay Nambi",
        "Yash Pandya",
        "Aravind Rajeswaran",
        "Corby Rosset",
        "Alexey Taymanov",
        "Vibhav Vineet",
        "Spencer Whitehead",
        "Andrew Zhao"
      ],
      "abstract": "Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“(Computer Use Agents, CUAs)ç¼ºä¹é«˜è´¨é‡äº¤äº’è½¨è¿¹æ•°æ®é›†çš„é—®é¢˜ï¼Œæå‡ºäº†FaraGenï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ­¥ç½‘é¡µä»»åŠ¡çš„æ–°å‹åˆæˆæ•°æ®ç”Ÿæˆç³»ç»Ÿã€‚FaraGenèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–ä»»åŠ¡å¹¶é€šè¿‡å¤šé‡éªŒè¯å™¨ç­›é€‰æˆåŠŸè½¨è¿¹ï¼Œå®ç°äº†çº¦1ç¾å…ƒä¸€æ¡çš„é«˜æ€§ä»·æ¯”ã€é«˜è´¨é‡æ•°æ®äº§å‡ºã€‚åŸºäºè¯¥ç³»ç»Ÿç”Ÿæˆçš„æ•°æ®ï¼Œç ”ç©¶å›¢é˜Ÿè®­ç»ƒäº†Fara-7Bï¼Œè¿™æ˜¯ä¸€æ¬¾ä»…ä¾èµ–æˆªå›¾(Screenshots)æ„ŸçŸ¥ã€é€šè¿‡é¢„æµ‹åæ ‡æ‰§è¡ŒåŠ¨ä½œä¸”æ”¯æŒè®¾å¤‡ç«¯(On-device)è¿è¡Œçš„é«˜æ•ˆåŸç”Ÿæ™ºèƒ½ä½“æ¨¡å‹ã€‚åœ¨WebVoyagerã€Online-Mind2Webä»¥åŠè¯¥ç ”ç©¶æ–°æ¨å‡ºçš„WebTailBenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒFara-7Bçš„æ€§èƒ½ä¼˜äºåŒè§„æ¨¡æ¨¡å‹ï¼Œä¸”åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å¯ä¸æ›´å¤§å‹çš„å‰æ²¿æ¨¡å‹(Frontier Models)ç›¸åª²ç¾ã€‚å®éªŒç»“æœè¯æ˜äº†å¯æ‰©å±•æ•°æ®ç”Ÿæˆç³»ç»Ÿåœ¨æå‡å°å‹æ™ºèƒ½ä½“æ¨¡å‹æ•ˆç‡æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚ç›®å‰ï¼ŒFara-7Bçš„æ¨¡å‹æƒé‡ä¸WebTailBenchåŸºå‡†å·²æ­£å¼å¼€æºï¼Œä¸ºæœªæ¥è½»é‡åŒ–è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19663v1",
      "published_date": "2025-11-24 19:56:28 UTC",
      "updated_date": "2025-11-24 19:56:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:45.532344+00:00"
    },
    {
      "arxiv_id": "2511.19654v1",
      "title": "Accuracy and Efficiency Trade-Offs in LLM-Based Malware Detection and Explanation: A Comparative Study of Parameter Tuning vs. Full Fine-Tuning",
      "title_zh": "åŸºäºLLMçš„æ¶æ„è½¯ä»¶æ£€æµ‹ä¸è§£é‡Šçš„å‡†ç¡®ç‡ä¸æ•ˆç‡æƒè¡¡ï¼šå‚æ•°å¾®è°ƒä¸å…¨é‡å¾®è°ƒçš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Stephen C. Gravereaux",
        "Sheikh Rabiul Islam"
      ],
      "abstract": "This study examines whether Low-Rank Adaptation (LoRA) fine-tuned Large Language Models (LLMs) can approximate the performance of fully fine-tuned models in generating human-interpretable decisions and explanations for malware classification. Achieving trustworthy malware detection, particularly when LLMs are involved, remains a significant challenge. We developed an evaluation framework using Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), and Semantic Similarity Metrics to benchmark explanation quality across five LoRA configurations and a fully fine-tuned baseline. Results indicate that full fine-tuning achieves the highest overall scores, with BLEU and ROUGE improvements of up to 10% over LoRA variants. However, mid-range LoRA models deliver competitive performance exceeding full fine-tuning on two metrics while reducing model size by approximately 81% and training time by over 80% on a LoRA model with 15.5% trainable parameters. These findings demonstrate that LoRA offers a practical balance of interpretability and resource efficiency, enabling deployment in resource-constrained environments without sacrificing explanation quality. By providing feature-driven natural language explanations for malware classifications, this approach enhances transparency, analyst confidence, and operational scalability in malware detection systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº†ä½ç§©è‡ªé€‚åº”(LoRA)å¾®è°ƒä¸å…¨å‚æ•°å¾®è°ƒ(Full Fine-Tuning)åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¶æ„è½¯ä»¶æ£€æµ‹åŠè§£é‡Šç”Ÿæˆä¸­çš„æ€§èƒ½è¡¨ç°ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨BLEUã€ROUGEå’Œè¯­ä¹‰ç›¸ä¼¼æ€§æŒ‡æ ‡å»ºç«‹è¯„ä¼°æ¡†æ¶ï¼Œå¯¹ä¸åŒå¾®è°ƒç­–ç•¥äº§ç”Ÿçš„æ£€æµ‹è§£é‡Šè´¨é‡è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶å…¨å‚æ•°å¾®è°ƒåœ¨æ ¸å¿ƒæŒ‡æ ‡ä¸Šé¢†å…ˆçº¦10%ï¼Œä½†ç‰¹å®šçš„LoRAé…ç½®åœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶ï¼Œå°†æ¨¡å‹å¤§å°ç¼©å‡äº†çº¦81%å¹¶èŠ‚çœäº†è¶…è¿‡80%çš„è®­ç»ƒæ—¶é—´ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†LoRAåœ¨æ¨¡å‹å¯è§£é‡Šæ€§ä¸èµ„æºæ•ˆç‡ä¹‹é—´èƒ½å¤Ÿè¾¾æˆæœ‰æ•ˆå¹³è¡¡ï¼Œå…è®¸åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜é€æ˜åº¦çš„æ¶æ„è½¯ä»¶åˆ†ç±»ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”ŸæˆåŸºäºç‰¹å¾çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œæ˜¾è‘—æå‡äº†åˆ†æå¸ˆçš„ä¿¡å¿ƒå’Œç³»ç»Ÿçš„è¿è¥å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted in IEEE Big Data 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.19654v1",
      "published_date": "2025-11-24 19:37:13 UTC",
      "updated_date": "2025-11-24 19:37:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:58.049446+00:00"
    },
    {
      "arxiv_id": "2511.19649v1",
      "title": "Synthetic Data: AI's New Weapon Against Android Malware",
      "title_zh": "åˆæˆæ•°æ®ï¼šäººå·¥æ™ºèƒ½å¯¹æŠ— Android æ¶æ„è½¯ä»¶çš„æ–°åˆ©å™¨",
      "authors": [
        "Angelo Gaspar Diniz Nogueira",
        "Kayua Oleques Paim",
        "Hendrio BraganÃ§a",
        "Rodrigo BrandÃ£o Mansilha",
        "Diego Kreutz"
      ],
      "abstract": "The ever-increasing number of Android devices and the accelerated evolution of malware, reaching over 35 million samples by 2024, highlight the critical importance of effective detection methods. Attackers are now using Artificial Intelligence to create sophisticated malware variations that can easily evade traditional detection techniques. Although machine learning has shown promise in malware classification, its success relies heavily on the availability of up-to-date, high-quality datasets. The scarcity and high cost of obtaining and labeling real malware samples presents significant challenges in developing robust detection models. In this paper, we propose MalSynGen, a Malware Synthetic Data Generation methodology that uses a conditional Generative Adversarial Network (cGAN) to generate synthetic tabular data. This data preserves the statistical properties of real-world data and improves the performance of Android malware classifiers. We evaluated the effectiveness of this approach using various datasets and metrics that assess the fidelity of the generated data, its utility in classification, and the computational efficiency of the process. Our experiments demonstrate that MalSynGen can generalize across different datasets, providing a viable solution to address the issues of obsolescence and low quality data in malware detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Androidæ¶æ„è½¯ä»¶æ¼”å˜åŠ é€ŸåŠAIç”Ÿæˆå¤æ‚å˜ä½“ä»¥è§„é¿ä¼ ç»Ÿæ£€æµ‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†MalSynGenï¼Œä¸€ç§åŸºäºæ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(cGAN)çš„æ¶æ„è½¯ä»¶åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆä¿ç•™çœŸå®æ•°æ®ç»Ÿè®¡ç‰¹æ€§çš„åˆæˆè¡¨æ ¼æ•°æ®ï¼Œæ—¨åœ¨è§£å†³é«˜è´¨é‡ã€æœ€æ–°æ¶æ„è½¯ä»¶æ ·æœ¬ç¨€ç¼ºä¸”æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚å®éªŒé€šè¿‡å¤šç§æ•°æ®é›†å’ŒæŒ‡æ ‡è¯„ä¼°äº†ç”Ÿæˆæ•°æ®çš„å¿ å®åº¦ã€åˆ†ç±»å®ç”¨æ€§åŠè®¡ç®—æ•ˆç‡ã€‚ç»“æœè¡¨æ˜ï¼ŒMalSynGenèƒ½å¤Ÿæœ‰æ•ˆæå‡Androidæ¶æ„è½¯ä»¶åˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä¸åŒæ•°æ®é›†é—´è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ¡ˆä¸ºåº”å¯¹æ¶æ„è½¯ä»¶æ£€æµ‹ä¸­çš„æ•°æ®è¿‡æ—¶å’Œä½è´¨é‡æŒ‘æˆ˜æä¾›äº†ä¸€ç§é«˜æ•ˆçš„å¯è¡Œæ–¹æ¡ˆï¼Œæ˜¾è‘—å¢å¼ºäº†é˜²å¾¡ç³»ç»Ÿå¯¹æŠ—AIé©±åŠ¨å‹æ¶æ„è½¯ä»¶å¨èƒçš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "23 pages, 18 figures, 8 tables. Accepted for publication at the JBCS",
      "pdf_url": "https://arxiv.org/pdf/2511.19649v1",
      "published_date": "2025-11-24 19:27:58 UTC",
      "updated_date": "2025-11-24 19:27:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:54.129250+00:00"
    },
    {
      "arxiv_id": "2511.19648v1",
      "title": "Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search",
      "title_zh": "åŸºäº LLM è§„åˆ’ä¸åµŒå…¥å¼•å¯¼æœç´¢çš„çŸ¥è¯†å›¾è°±é«˜æ•ˆå¤šè·³é—®ç­”",
      "authors": [
        "Manil Shrestha",
        "Edward Kim"
      ],
      "abstract": "Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.",
      "tldr_zh": "é’ˆå¯¹çŸ¥è¯†å›¾è°±(Knowledge Graphs)å¤šè·³é—®ç­”ä¸­è·¯å¾„ç»„åˆçˆ†ç‚¸å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ¨ç†æ˜‚è´µä¸”ç¼ºä¹éªŒè¯ç­‰æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†LLM-Guided Planningå’ŒEmbedding-Guided Neural Searchä¸¤ç§äº’è¡¥çš„æ··åˆç®—æ³•ã€‚LLM-Guided Planningé€šè¿‡å•æ¬¡è°ƒç”¨LLMé¢„æµ‹å…³ç³»åºåˆ—å¹¶é…åˆå¹¿åº¦ä¼˜å…ˆæœç´¢(Breadth-First Search)ï¼Œåœ¨ç¡®ä¿ç­”æ¡ˆå®Œå…¨æ ¹æ¤äºçŸ¥è¯†å›¾è°±çš„åŒæ—¶å®ç°äº†è¶…è¿‡0.90çš„Micro-F1ã€‚Embedding-Guided Neural Searchåˆ™åˆ©ç”¨è½»é‡çº§çš„è¾¹ç¼˜æ‰“åˆ†å™¨èåˆæ–‡æœ¬ä¸å›¾åµŒå…¥ï¼Œåœ¨å®Œå…¨ä¸ä¾èµ–LLMæ¨ç†çš„æƒ…å†µä¸‹å®ç°äº†100å€ä»¥ä¸Šçš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡çŸ¥è¯†è’¸é¦å°†å¤æ‚è§„åˆ’èƒ½åŠ›å‹ç¼©è‡³4Bå‚æ•°æ¨¡å‹ï¼Œåœ¨MetaQAè¯„ä¼°ä¸­è¯æ˜äº†ç»“åˆç¬¦å·ç»“æ„ä¸å­¦ä¹ è¡¨å¾çš„æ¶æ„åå·®èƒ½å¤Ÿä»¥æä½æˆæœ¬å®ç°ä¼˜äºæ— çº¦æŸç”Ÿæˆçš„æ¨ç†æ•ˆæœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19648v1",
      "published_date": "2025-11-24 19:27:56 UTC",
      "updated_date": "2025-11-24 19:27:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:54:53.341043+00:00"
    },
    {
      "arxiv_id": "2511.19647v1",
      "title": "Robot-Powered Data Flywheels: Deploying Robots in the Wild for Continual Data Collection and Foundation Model Adaptation",
      "title_zh": "æœºå™¨äººé©±åŠ¨çš„æ•°æ®é£è½®ï¼šé€šè¿‡åœ¨çœŸå®åœºæ™¯ä¸­éƒ¨ç½²æœºå™¨äººå®ç°æŒç»­æ•°æ®é‡‡é›†ä¸åŸºåº§æ¨¡å‹é€‚é…",
      "authors": [
        "Jennifer Grannen",
        "Michelle Pan",
        "Kenneth Llontop",
        "Cherie Ho",
        "Mark Zolotas",
        "Jeannette Bohg",
        "Dorsa Sadigh"
      ],
      "abstract": "Foundation models (FM) have unlocked powerful zero-shot capabilities in vision and language, yet their reliance on internet pretraining data leaves them brittle in unstructured, real-world settings. The messy, real-world data encountered during deployment (e.g. occluded or multilingual text) remains massively underrepresented in existing corpora. Robots, as embodied agents, are uniquely positioned to close this gap: they can act in physical environments to collect large-scale, real-world data that enriches FM training with precisely the examples current models lack. We introduce the Robot-Powered Data Flywheel, a framework that transforms robots from FM consumers into data generators. By deploying robots equipped with FMs in the wild, we enable a virtuous cycle: robots perform useful tasks while collecting real-world data that improves both domain-specific adaptation and domain-adjacent generalization. We instantiate this framework with Scanford, a mobile manipulator deployed in the East Asia Library for 2 weeks. Scanford autonomously scans shelves, identifies books using a vision-language model (VLM), and leverages the library catalog to label images without human annotation. This deployment both aids librarians and produces a dataset to finetune the underlying VLM, improving performance on the domain-specific in-the-wild library setting and on domain-adjacent multilingual OCR benchmarks. Using data collected from 2103 shelves, Scanford improves VLM performance on book identification from 32.0% to 71.8% and boosts domain-adjacent multilingual OCR from 24.8% to 46.6% (English) and 30.8% to 38.0% (Chinese), while saving an ~18.7 hrs of human time. These results highlight how robot-powered data flywheels can both reduce human effort in real deployments and unlock new pathways for continually adapting FMs to the messiness of reality. More details are at: https://scanford-robot.github.io",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Robot-Powered Data Flywheel æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºç¡€æ¨¡å‹ (Foundation Models) åœ¨å¤„ç†éç»“æ„åŒ–ç°å®ä¸–ç•Œæ•°æ®æ—¶çš„è„†å¼±æ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†æœºå™¨äººä»æ¨¡å‹çš„ä½¿ç”¨è€…è½¬å˜ä¸ºæ•°æ®ç”Ÿæˆè€…ï¼Œé€šè¿‡åœ¨ç°å®ç¯å¢ƒä¸­éƒ¨ç½²æœºå™¨äººæ¥æŒç»­æ”¶é›†å¤§è§„æ¨¡ã€çœŸå®ä¸–ç•Œçš„æ•°æ®ï¼Œä»è€Œå®ç°é¢†åŸŸç‰¹å®šé€‚åº”å’Œé¢†åŸŸç›¸é‚»æ³›åŒ–ã€‚ç ”ç©¶è€…å¼€å‘äº†åä¸º Scanford çš„ç§»åŠ¨æœºæ¢°è‡‚å¹¶åœ¨ä¸œäºšå›¾ä¹¦é¦†è¿›è¡Œäº†ä¸¤å‘¨çš„å®åœ°éƒ¨ç½²ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªåŠ¨æ‰«æä¹¦æ¶å¹¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Model, VLM) ç»“åˆå›¾ä¹¦é¦†ç›®å½•å®ç°æ— éœ€äººå·¥å¹²é¢„çš„è‡ªåŠ¨æ ‡æ³¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆ©ç”¨ä» 2103 ä¸ªä¹¦æ¶æ”¶é›†çš„æ•°æ®è¿›è¡Œå¾®è°ƒåï¼ŒScanford å°† VLM çš„å›¾ä¹¦è¯†åˆ«å‡†ç¡®ç‡ä» 32.0% æ˜¾è‘—æå‡è‡³ 71.8%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¢å¼ºäº†æ¨¡å‹åœ¨å¤šè¯­è¨€ OCR åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œå°†è‹±æ–‡å’Œä¸­æ–‡çš„è¯†åˆ«ç‡åˆ†åˆ«æå‡è‡³ 46.6% å’Œ 38.0%ã€‚æ­¤é¡¹ç ”ç©¶åœ¨èŠ‚çœçº¦ 18.7 å°æ—¶äººå·¥åŠ³åŠ¨çš„åŒæ—¶ï¼Œè¯æ˜äº†æœºå™¨äººé©±åŠ¨çš„æ•°æ®é£è½®æ˜¯å®ç°åŸºç¡€æ¨¡å‹ä¸æ–­é€‚åº”å¤æ‚ç°å®ä¸–ç•Œçš„æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19647v1",
      "published_date": "2025-11-24 19:23:04 UTC",
      "updated_date": "2025-11-24 19:23:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:55:59.842386+00:00"
    },
    {
      "arxiv_id": "2511.19644v1",
      "title": "IRSDA: An Agent-Orchestrated Framework for Enterprise Intrusion Response",
      "title_zh": "IRSDAï¼šé¢å‘ä¼ä¸šå…¥ä¾µå“åº”çš„æ™ºèƒ½ä½“ç¼–æ’æ¡†æ¶",
      "authors": [
        "Damodar Panigrahi",
        "Raj Patel",
        "Shaswata Mitra",
        "Sudip Mittal",
        "Shahram Rahimi"
      ],
      "abstract": "Modern enterprise systems face escalating cyber threats that are increasingly dynamic, distributed, and multi-stage in nature. Traditional intrusion detection and response systems often rely on static rules and manual workflows, which limit their ability to respond with the speed and precision required in high-stakes environments. To address these challenges, we present the Intrusion Response System Digital Assistant (IRSDA), an agent-based framework designed to deliver autonomous and policy-compliant cyber defense. IRSDA combines Self-Adaptive Autonomic Computing Systems (SA-ACS) with the Knowledge guided Monitor, Analyze, Plan, and Execute (MAPE-K) loop to support real-time, partition-aware decision-making across enterprise infrastructure.\n  IRSDA incorporates a knowledge-driven architecture that integrates contextual information with AI-based reasoning to support system-guided intrusion response. The framework leverages retrieval mechanisms and structured representations to inform decision-making while maintaining alignment with operational policies. We assess the system using a representative real-world microservices application, demonstrating its ability to automate containment, enforce compliance, and provide traceable outputs for security analyst interpretation. This work outlines a modular and agent-driven approach to cyber defense that emphasizes explainability, system-state awareness, and operational control in intrusion response.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IRSDAï¼ˆIntrusion Response System Digital Assistantï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºä¼ä¸šå…¥ä¾µå“åº”è®¾è®¡çš„æ™ºèƒ½ä½“ç¼–æ’æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹æ—¥ç›ŠåŠ¨æ€ä¸”å¤šé˜¶æ®µçš„ç½‘ç»œå¨èƒã€‚è¯¥æ¡†æ¶å°†è‡ªé€‚åº”è‡ªä¸»è®¡ç®—ç³»ç»Ÿï¼ˆSA-ACSï¼‰ä¸çŸ¥è¯†å¼•å¯¼çš„MAPE-Kï¼ˆMonitor, Analyze, Plan, and Executeï¼‰å¾ªç¯ç›¸ç»“åˆï¼Œæ”¯æŒè·¨ä¼ä¸šåŸºç¡€è®¾æ–½çš„å®æ—¶ã€åˆ†åŒºæ„ŸçŸ¥å†³ç­–ã€‚IRSDAé‡‡ç”¨çŸ¥è¯†é©±åŠ¨æ¶æ„ï¼Œé€šè¿‡é›†æˆä¸Šä¸‹æ–‡ä¿¡æ¯ã€äººå·¥æ™ºèƒ½æ¨ç†åŠæ£€ç´¢æœºåˆ¶ï¼Œç¡®ä¿å…¥ä¾µå“åº”å†³ç­–æ—¢å…·å¤‡æ™ºèƒ½æ€§åˆç¬¦åˆæ“ä½œç­–ç•¥ã€‚å®éªŒåœ¨çœŸå®çš„å¾®æœåŠ¡åº”ç”¨ä¸ŠéªŒè¯äº†è¯¥ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶èƒ½å¤Ÿå®ç°è‡ªåŠ¨åŒ–éåˆ¶ã€å¼ºåˆ¶æ‰§è¡Œåˆè§„æ€§ï¼Œå¹¶ä¸ºå®‰å…¨åˆ†æå¸ˆæä¾›å¯è¿½æº¯çš„è¾“å‡ºã€‚è¯¥ç ”ç©¶ä¸ºç½‘ç»œé˜²å¾¡æä¾›äº†ä¸€ç§æ¨¡å—åŒ–ä¸”ç”±æ™ºèƒ½ä½“é©±åŠ¨çš„æ–°é€”å¾„ï¼Œæ˜¾è‘—æå‡äº†å…¥ä¾µå“åº”è¿‡ç¨‹ä¸­çš„å¯è§£é‡Šæ€§ã€ç³»ç»ŸçŠ¶æ€æ„ŸçŸ¥å’Œæ“ä½œæ§åˆ¶èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.19644v1",
      "published_date": "2025-11-24 19:21:09 UTC",
      "updated_date": "2025-11-24 19:21:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:55:04.345627+00:00"
    },
    {
      "arxiv_id": "2511.19641v1",
      "title": "On the Utility of Foundation Models for Fast MRI: Vision-Language-Guided Image Reconstruction",
      "title_zh": "è®ºåŸºç¡€æ¨¡å‹åœ¨å¿«é€Ÿç£å…±æŒ¯æˆåƒä¸­çš„æ•ˆç”¨ï¼šè§†è§‰-è¯­è¨€å¼•å¯¼çš„å›¾åƒé‡å»º",
      "authors": [
        "Ruimin Feng",
        "Xingxin He",
        "Ronald Mercer",
        "Zachary Stewart",
        "Fang Liu"
      ],
      "abstract": "Purpose: To investigate whether a vision-language foundation model can enhance undersampled MRI reconstruction by providing high-level contextual information beyond conventional priors. Methods: We proposed a semantic distribution-guided reconstruction framework that uses a pre-trained vision-language foundation model to encode both the reconstructed image and auxiliary information into high-level semantic features. A contrastive objective aligns the reconstructed representation with the target semantic distribution, ensuring consistency with high-level perceptual cues. The proposed objective works with various deep learning-based reconstruction methods and can flexibly incorporate semantic priors from multimodal sources. To test the effectiveness of these semantic priors, we evaluated reconstruction results guided by priors derived from either image-only or image-language auxiliary information. Results: Experiments on knee and brain datasets demonstrate that semantic priors from images preserve fine anatomical structures and achieve superior perceptual quality, as reflected in lower LPIPS values, higher Tenengrad scores, and improved scores in the reader study, compared with conventional regularization. The image-language information further expands the semantic distribution and enables high-level control over reconstruction attributes. Across all evaluations, the contrastive objective consistently guided the reconstructed features toward the desired semantic distributions while maintaining data fidelity, demonstrating the effectiveness of the proposed optimization framework. Conclusion: The study highlights that vision-language foundation models can improve undersampled MRI reconstruction through semantic-space optimization.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹(Vision-Language Foundation Models)å¢å¼ºæ¬ é‡‡æ ·MRIé‡å»ºçš„æ•ˆç”¨ï¼Œæ—¨åœ¨æä¾›è¶…è¶Šä¼ ç»Ÿå…ˆéªŒçš„é«˜å±‚ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä½œè€…æå‡ºäº†ä¸€ç§è¯­ä¹‰åˆ†å¸ƒå¼•å¯¼çš„é‡å»ºæ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å°†é‡å»ºå›¾åƒä¸è¾…åŠ©ä¿¡æ¯ç¼–ç ä¸ºé«˜å±‚è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ ç›®æ ‡(Contrastive Objective)ç¡®ä¿é‡å»ºç»“æœä¸ç›®æ ‡è¯­ä¹‰åˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿçµæ´»æ•´åˆæ¥è‡ªå›¾åƒæˆ–å¤šæ¨¡æ€Image-Languageè¾…åŠ©ä¿¡æ¯çš„è¯­ä¹‰å…ˆéªŒï¼Œä¸”å…¼å®¹å¤šç§æ·±åº¦å­¦ä¹ é‡å»ºç®—æ³•ã€‚åœ¨è†ç›–å’Œå¤§è„‘æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯­ä¹‰å…ˆéªŒåœ¨ä¿ç•™ç²¾ç»†è§£å‰–ç»“æ„å’Œæå‡æ„ŸçŸ¥è´¨é‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè·å¾—äº†æ›´ä½çš„LPIPSå€¼ã€æ›´é«˜çš„Tenengradåˆ†æ•°ä»¥åŠæ›´å¥½çš„é˜…ç‰‡ç ”ç©¶è¯„ä»·ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼ŒImage-Languageä¿¡æ¯çš„åŠ å…¥èƒ½æ‰©å±•è¯­ä¹‰åˆ†å¸ƒå¹¶å®ç°å¯¹é‡å»ºå±æ€§çš„é«˜å±‚æ§åˆ¶ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†Vision-Language Foundation Modelsé€šè¿‡è¯­ä¹‰ç©ºé—´ä¼˜åŒ–èƒ½æ˜¾è‘—æ”¹è¿›MRIé‡å»ºè´¨é‡å¹¶ç»´æŒæ•°æ®ä¿çœŸåº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19641v1",
      "published_date": "2025-11-24 19:15:47 UTC",
      "updated_date": "2025-11-24 19:15:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:55:08.929170+00:00"
    },
    {
      "arxiv_id": "2511.19636v1",
      "title": "Many Ways to be Right: Rashomon Sets for Concept-Based Neural Networks",
      "title_zh": "æ®Šé€”åŒå½’ï¼šåŸºäºæ¦‚å¿µç¥ç»ç½‘ç»œçš„ Rashomon é›†åˆ",
      "authors": [
        "Shihan Feng",
        "Cheng Zhang",
        "Michael Xi",
        "Ethan Hsu",
        "Lesia Semenova",
        "Chudi Zhong"
      ],
      "abstract": "Modern neural networks rarely have a single way to be right. For many tasks, multiple models can achieve identical performance while relying on different features or reasoning patterns, a property known as the Rashomon Effect. However, uncovering this diversity in deep architectures is challenging as their continuous parameter spaces contain countless near-optimal solutions that are numerically distinct but often behaviorally similar. We introduce Rashomon Concept Bottleneck Models, a framework that learns multiple neural networks which are all accurate yet reason through distinct human-understandable concepts. By combining lightweight adapter modules with a diversity-regularized training objective, our method constructs a diverse set of deep concept-based models efficiently without retraining from scratch. The resulting networks provide fundamentally different reasoning processes for the same predictions, revealing how concept reliance and decision making vary across equally performing solutions. Our framework enables systematic exploration of data-driven reasoning diversity in deep models, offering a new mechanism for auditing, comparison, and alignment across equally accurate solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¥ç»ç½‘ç»œä¸­çš„Rashomon Effectï¼Œå³å¤šä¸ªæ¨¡å‹åœ¨æ€§èƒ½ç›¸åŒçš„æƒ…å†µä¸‹å¯èƒ½ä¾èµ–ä¸åŒçš„ç‰¹å¾æˆ–æ¨ç†æ¨¡å¼ã€‚é’ˆå¯¹æ·±åº¦æ¶æ„ä¸­éš¾ä»¥å‘ç°è¿™ç§å¤šæ ·æ€§çš„æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†Rashomon Concept Bottleneck Modelsæ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ ä¸€ç»„æ—¢å‡†ç¡®åˆé€šè¿‡ä¸åŒçš„äººç±»å¯ç†è§£æ¦‚å¿µ(Concepts)è¿›è¡Œæ¨ç†çš„ç¥ç»ç½‘ç»œã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆè½»é‡åŒ–é€‚é…å™¨æ¨¡å—(Adapter modules)ä¸å¤šæ ·æ€§æ­£åˆ™åŒ–(Diversity-regularized)çš„è®­ç»ƒç›®æ ‡ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€ä»å¤´è®­ç»ƒçš„æƒ…å†µä¸‹é«˜æ•ˆæ„å»ºå‡ºå¤šæ ·åŒ–çš„æ·±åº¦æ¦‚å¿µæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›ç”Ÿæˆçš„ç½‘ç»œé’ˆå¯¹ç›¸åŒçš„é¢„æµ‹æä¾›äº†æ ¹æœ¬ä¸åŒçš„æ¨ç†è¿‡ç¨‹ï¼Œæ­ç¤ºäº†æ€§èƒ½ç›¸å½“çš„è§£å†³æ–¹æ¡ˆåœ¨æ¦‚å¿µä¾èµ–å’Œå†³ç­–æœºåˆ¶ä¸Šçš„å·®å¼‚ã€‚è¯¥æ¡†æ¶ä¸ºç³»ç»Ÿæ€§æ¢ç´¢æ·±åº¦æ¨¡å‹ä¸­çš„æ¨ç†å¤šæ ·æ€§æä¾›äº†å·¥å…·ï¼Œä¹Ÿä¸ºæ¨¡å‹å®¡è®¡(Auditing)ã€å¯¹æ¯”ä»¥åŠç­‰æ•ˆç²¾åº¦æ–¹æ¡ˆä¹‹é—´çš„å¯¹é½(Alignment)æä¾›äº†å…¨æ–°çš„æœºåˆ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19636v1",
      "published_date": "2025-11-24 19:12:26 UTC",
      "updated_date": "2025-11-24 19:12:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:56:50.130893+00:00"
    },
    {
      "arxiv_id": "2511.19436v1",
      "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection",
      "title_zh": "VDC-Agentï¼šè§†é¢‘è¯¦ç»†æè¿°æ¨¡å‹é€šè¿‡æ™ºèƒ½ä½“è‡ªæˆ‘åæ€å®ç°è‡ªæˆ‘è¿›åŒ–",
      "authors": [
        "Qiang Wang",
        "Xinyuan Gao",
        "SongLin Dong",
        "Jizhou Han",
        "Jiangyang Li",
        "Yuhang He",
        "Yihong Gong"
      ],
      "abstract": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VDC-Agentï¼Œä¸€ä¸ªç”¨äºè§†é¢‘è¯¦ç»†æè¿°(Video Detailed Captioning)çš„è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–å¤§å‹æ•™å¸ˆæ¨¡å‹å³å¯å®ç°æ€§èƒ½æå‡ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ç”±æè¿°ç”Ÿæˆã€åŸåˆ™å¼•å¯¼è¯„åˆ†(principle-guided scoring)å’Œæç¤ºè¯ä¼˜åŒ–(prompt refinement)ç»„æˆçš„é—­ç¯ï¼Œå¹¶åœ¨è´¨é‡å›é€€æ—¶é€šè¿‡è‡ªæˆ‘åæ€(self-reflection)è·¯å¾„ç»“åˆé“¾å¼æ€ç»´(Chain-of-Thought)æ¥ä¿®æ­£æ›´æ–°ã€‚ç ”ç©¶åˆ©ç”¨è¯¥è¿‡ç¨‹åœ¨æ— æ ‡æ³¨è§†é¢‘ä¸Šè‡ªåŠ¨ç”Ÿæˆäº†åŒ…å«18,886ä¸ªåå¥½å¯¹çš„VDC-Agent-19Kæ•°æ®é›†ã€‚é€šè¿‡å¯¹Qwen2.5-VL-7B-Instructè¿›è¡Œä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹ç›´æ¥åå¥½ä¼˜åŒ–(curriculum direct preference optimization)å¾®è°ƒï¼ŒVDC-Agent-7Båœ¨VDCåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°49.08%ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒç›¸ä¼¼æ¨ç†æˆæœ¬çš„å‰æä¸‹ï¼Œæ¯”åŸºå‡†æ¨¡å‹å‡†ç¡®ç‡æå‡äº†5.13%ï¼Œæœ‰æ•ˆè¯æ˜äº†æ™ºèƒ½ä½“è‡ªæˆ‘è¿›åŒ–æœºåˆ¶åœ¨å¤æ‚è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19436v1",
      "published_date": "2025-11-24 18:59:56 UTC",
      "updated_date": "2025-11-24 18:59:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:56:27.231634+00:00"
    },
    {
      "arxiv_id": "2511.19433v1",
      "title": "Mixture of Horizons in Action Chunking",
      "title_zh": "åŠ¨ä½œåˆ†å—ä¸­çš„å¤šæ—¶ç•Œæ··åˆ",
      "authors": [
        "Dong Jing",
        "Gang Wang",
        "Jiaqi Liu",
        "Weiliang Tang",
        "Zelong Sun",
        "Yunchao Yao",
        "Zhenyu Wei",
        "Yunhui Liu",
        "Zhiwu Lu",
        "Mingyu Ding"
      ],
      "abstract": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $Ï€_0$, $Ï€_{0.5}$, and one-step regression policy $Ï€_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $Ï€_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹(Vision-language-action, VLA)åœ¨æœºå™¨äººæ“ä½œä¸­å¯¹åŠ¨ä½œå—é•¿åº¦(Action chunk lengthï¼Œå³Horizon)æ•æ„Ÿçš„é—®é¢˜ï¼ŒæŒ‡å‡ºäº†é•¿çŸ­æ—¶ç•Œåœ¨å…¨å±€é¢„è§æ€§ä¸å±€éƒ¨ç²¾åº¦ä¹‹é—´çš„å›ºæœ‰æƒè¡¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†æ—¶ç•Œæ··åˆç­–ç•¥(Mixture of Horizons, MoH)ï¼Œé€šè¿‡å°†åŠ¨ä½œå—åˆ’åˆ†ä¸ºä¸åŒæ—¶ç•Œçš„å¤šä¸ªæ®µå¹¶åˆ©ç”¨å…±äº«çš„åŠ¨ä½œå˜æ¢å™¨(Action transformer)è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œæœ€åç»ç”±è½»é‡çº§çº¿æ€§é—¨æ§è¿›è¡Œèåˆã€‚MoHèƒ½å¤ŸåŒæ—¶å…¼é¡¾é•¿æ—¶é¢„è§ä¸çŸ­æ—¶ç²¾åº¦ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”ä½œä¸ºå³æ’å³ç”¨æ¨¡å—ä»…éœ€æå°çš„è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œè¯¥ç­–ç•¥æ”¯æŒè‡ªé€‚åº”æ—¶ç•Œçš„åŠ¨æ€æ¨ç†ï¼Œåœ¨ä¿è¯æ€§èƒ½çš„å‰æä¸‹å®ç°äº†æ¯”åŸºçº¿æ¨¡å‹é«˜2.5å€çš„ååé‡ã€‚å®éªŒè¯æ˜ï¼ŒMoHåœ¨ä»¿çœŸå’Œç°å®ä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨LIBEROåŸºå‡†æµ‹è¯•çš„æ··åˆä»»åŠ¡è®¾ç½®ä¸‹ä»…éœ€30kæ¬¡è¿­ä»£ä¾¿è¾¾åˆ°äº†99%çš„å¹³å‡æˆåŠŸç‡ï¼Œåˆ·æ–°äº†å½“å‰æœ€å…ˆè¿›(State-of-the-art)çš„æ€§èƒ½è®°å½•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "15 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.19433v1",
      "published_date": "2025-11-24 18:59:51 UTC",
      "updated_date": "2025-11-24 18:59:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:56:23.738402+00:00"
    },
    {
      "arxiv_id": "2511.19427v1",
      "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering",
      "title_zh": "å°‘å†™æç¤ºï¼Œå¤šç‚¹å¾®ç¬‘ï¼šä»¥è¯­ä¹‰å·¥ç¨‹å–ä»£æç¤ºå·¥ç¨‹çš„ MTP",
      "authors": [
        "Jayanaka L. Dantanarayana",
        "Savini Kashmira",
        "Thakee Nathees",
        "Zichen Zhang",
        "Krisztian Flautner",
        "Lingjia Tang",
        "Jason Mars"
      ],
      "abstract": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† AI-Integrated programming è¿™ä¸€æ„å»ºæ™ºèƒ½ç³»ç»Ÿçš„åŸºç¡€èŒƒå¼ï¼Œé’ˆå¯¹ç°æœ‰çš„ Meaning Typed Programming (MTP) æ— æ³•å……åˆ†æ•æ‰ä¸Šä¸‹æ–‡å’Œå¼€å‘è€…æ„å›¾çš„å±€é™ï¼Œæå‡ºäº†åä¸º Semantic Engineering çš„è½»é‡åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥è¯­ä¹‰ä¸Šä¸‹æ–‡æ³¨é‡Šï¼ˆSemantic Context Annotations, SemTextsï¼‰ï¼Œå…è®¸å¼€å‘è€…åœ¨ç¨‹åºç»“æ„ä¸­ç›´æ¥åµŒå…¥è‡ªç„¶è¯­è¨€ä¸Šä¸‹æ–‡ï¼Œä»è€Œåœ¨æ— éœ€æ‰‹åŠ¨ Prompt Engineering çš„æƒ…å†µä¸‹å¢å¼ºç¨‹åºè¯­ä¹‰ã€‚è¿™ä¸€æœºåˆ¶è¢«é›†æˆåˆ° Jac ç¼–ç¨‹è¯­è¨€ä¸­ï¼Œé€šè¿‡æ‰©å±• MTP å®ç°äº†åœ¨æç¤ºè¯ç”Ÿæˆè¿‡ç¨‹ä¸­è‡ªåŠ¨æ•´åˆä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ„å»ºäº†æ¨¡æ‹ŸçœŸå® AI é›†æˆåœºæ™¯çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°ç»“æœæ˜¾ç¤º Semantic Engineering åœ¨æ˜¾è‘—é™ä½å¼€å‘è€…è´Ÿæ‹…çš„åŒæ—¶ï¼Œå®ç°äº†ä¸ä¼ ç»Ÿ Prompt Engineering ç›¸å½“çš„æ€§èƒ½æ°´å¹³å’Œæé«˜çš„æç¤ºè¯ä¿çœŸåº¦ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19427v1",
      "published_date": "2025-11-24 18:58:22 UTC",
      "updated_date": "2025-11-24 18:58:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:56:47.245383+00:00"
    },
    {
      "arxiv_id": "2511.19423v1",
      "title": "Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design",
      "title_zh": "è¶…è¶Šè›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼šç”¨äºæœºç†æ€§é…¶è®¾è®¡çš„æ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶",
      "authors": [
        "Bruno Jacob",
        "Khushbu Agarwal",
        "Marcel Baer",
        "Peter Rice",
        "Simone Raugei"
      ],
      "abstract": "We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Genie-CATï¼Œä¸€ç§å·¥å…·å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ç³»ç»Ÿï¼Œæ—¨åœ¨åŠ é€Ÿè›‹ç™½è´¨è®¾è®¡ä¸­çš„ç§‘å­¦å‡è®¾ç”Ÿæˆã€‚è¯¥æ¡†æ¶å°†åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) çš„æ–‡çŒ®æ¨ç†ã€Protein Data Bank æ–‡ä»¶ç»“æ„è§£æã€é™ç”µåŠ¿è®¡ç®—ä»¥åŠæ°§åŒ–è¿˜åŸç‰¹æ€§çš„æœºå™¨å­¦ä¹ é¢„æµ‹é›†æˆåˆ°ç»Ÿä¸€çš„æ™ºèƒ½ä½“å·¥ä½œæµä¸­ã€‚é€šè¿‡å°†è‡ªç„¶è¯­è¨€æ¨ç†ä¸æ•°æ®é©±åŠ¨åŠç‰©ç†è®¡ç®—ç›¸ç»“åˆï¼ŒGenie-CAT èƒ½å¤Ÿç”Ÿæˆè¿æ¥åºåˆ—ã€ç»“æ„å’ŒåŠŸèƒ½çš„å…·æœ‰æœºæ¢°æ€§å¯è§£é‡Šä¸”å¯æµ‹è¯•çš„å‡è®¾ã€‚åœ¨é’ˆå¯¹é‡‘å±è›‹ç™½çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿè‡ªä¸»è¯†åˆ« [Fe-S] ç°‡é™„è¿‘å½±å“æ°§åŒ–è¿˜åŸè°ƒèŠ‚çš„æ®‹åŸºçº§ä¿®é¥°ï¼Œåœ¨æçŸ­æ—¶é—´å†…å¤ç°äº†ä¸“å®¶çº§çš„ç§‘ç ”å‡è®¾ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº† AI æ™ºèƒ½ä½“å¦‚ä½•é€šè¿‡ç»“åˆè¯­è¨€æ¨¡å‹ä¸é¢†åŸŸç‰¹å®šå·¥å…·æ¥å¼¥åˆç¬¦å·æ¨ç†ä¸æ•°å€¼æ¨¡æ‹Ÿä¹‹é—´çš„å·®è·ï¼Œä½¿ LLM æˆåŠŸä»å¯¹è¯åŠ©æ‰‹è½¬å‹ä¸ºè®¡ç®—å‘ç°çš„ç§‘ç ”åˆä½œä¼™ä¼´ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.19423v1",
      "published_date": "2025-11-24 18:57:07 UTC",
      "updated_date": "2025-11-24 18:57:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:56:33.534702+00:00"
    },
    {
      "arxiv_id": "2511.19422v1",
      "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning",
      "title_zh": "SLMFixï¼šåŸºäºå¼ºåŒ–å­¦ä¹ åˆ©ç”¨å°è¯­è¨€æ¨¡å‹å®ç°é”™è¯¯ä¿®å¤",
      "authors": [
        "David Jiahao Fu",
        "Aryan Gupta",
        "Aaron Councilman",
        "David Grove",
        "Yu-Xiong Wang",
        "Vikram Adve"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SLMFixï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨åˆ©ç”¨é€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¾®è°ƒçš„å°è¯­è¨€æ¨¡å‹(Small Language Model)æ¥ä¿®å¤å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„ç¨‹åºè¯­æ³•é”™è¯¯çš„åˆ›æ–°æµæ°´çº¿ã€‚é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä½èµ„æºç¼–ç¨‹è¯­è¨€(LRPLs)æ—¶å®¹æ˜“äº§ç”Ÿè¯­æ³•é”™è¯¯ä¸”å¾®è°ƒæˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ï¼ŒSLMFixç»“åˆäº†é™æ€éªŒè¯å™¨å’Œé™æ€è¯­ä¹‰ç›¸ä¼¼åº¦æŒ‡æ ‡æ¥æ„å»ºå¥–åŠ±æœºåˆ¶ï¼Œä»è€Œæå‡é¢†åŸŸç‰¹å®šè¯­è¨€(DSLs)çš„ä»£ç ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé¢†åŸŸç‰¹å®šè¯­è¨€ä¸Šè¡¨ç°å“è¶Šï¼Œåœ¨é™æ€éªŒè¯å™¨ä¸Šçš„é€šè¿‡ç‡è¶…è¿‡äº†95%ã€‚SLMFixçš„æ•ˆæœæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒ(Supervised Finetuning)æ–¹æ³•ï¼Œç”šè‡³åœ¨ä½èµ„æºè¯­è¨€ä»»åŠ¡ä¸­è¶…è¶Šäº†7Bå‚æ•°è§„æ¨¡çš„æ¨¡å‹ï¼Œä¸ºå—é™è®¡ç®—èµ„æºä¸‹çš„ä»£ç ä¼˜åŒ–æä¾›äº†é«˜æ•ˆä¸”ä½æˆæœ¬çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19422v1",
      "published_date": "2025-11-24 18:56:47 UTC",
      "updated_date": "2025-11-24 18:56:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:57:21.638631+00:00"
    },
    {
      "arxiv_id": "2512.07843v1",
      "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models",
      "title_zh": "ThreadWeaverï¼šé¢å‘è¯­è¨€æ¨¡å‹é«˜æ•ˆå¹¶è¡Œæ¨ç†çš„è‡ªé€‚åº”çº¿ç¨‹æŠ€æœ¯",
      "authors": [
        "Long Lian",
        "Sida Wang",
        "Felix Juefei-Xu",
        "Tsu-Jui Fu",
        "Xiuyu Li",
        "Adam Yala",
        "Trevor Darrell",
        "Alane Suhr",
        "Yuandong Tian",
        "Xi Victoria Lin"
      ],
      "abstract": "Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ThreadWeaverï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†é˜¶æ®µé¡ºåºè§£ç å¯¼è‡´çš„é«˜å»¶è¿Ÿé—®é¢˜çš„è‡ªé€‚åº”å¹¶è¡Œæ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ï¼Œåœ¨æ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿçš„åŒæ—¶ï¼Œä¿æŒäº†ä¸ä¸»æµé¡ºåºæ¨ç†æ¨¡å‹ç›¸å½“çš„å‡†ç¡®ç‡ã€‚é¦–å…ˆï¼Œå®ƒåˆ©ç”¨ä¸¤é˜¶æ®µå¹¶è¡Œè½¨è¿¹ç”Ÿæˆå™¨äº§ç”Ÿå¤§è§„æ¨¡é«˜è´¨é‡çš„ Chain-of-Thought (CoT) æ•°æ®ç”¨äºç›‘ç£å¾®è°ƒã€‚å…¶æ¬¡ï¼Œé‡‡ç”¨äº†åŸºäºå‰ç¼€æ ‘ï¼ˆTrie-basedï¼‰çš„è®­ç»ƒ-æ¨ç†ååŒè®¾è®¡ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç°æœ‰çš„è‡ªå›å½’æ¨ç†å¼•æ“ä¸Šç›´æ¥è¿è¡Œï¼Œæ— éœ€ä¿®æ”¹ä½ç½®åµŒå…¥æˆ– KV Cacheã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†å¹¶è¡Œæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆParallelization-aware Reinforcement Learningï¼‰æ¡†æ¶æ¥æ•™å¯¼æ¨¡å‹å¹³è¡¡å‡†ç¡®ç‡ä¸å¹¶è¡ŒåŒ–æ•ˆç‡ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºäº Qwen3-8B æ„å»ºçš„ ThreadWeaver è¾¾åˆ°äº†ä¸å°–ç«¯é¡ºåºæ¨¡å‹ç›¸å½“çš„æ°´å¹³ï¼Œå¹¶åœ¨ Token å»¶è¿Ÿä¸Šå®ç°äº†æœ€é«˜ 1.53 å€çš„åŠ é€Ÿã€‚è¿™ä¸€ç ”ç©¶æˆåŠŸå»ºç«‹äº†å‡†ç¡®ç‡ä¸æ•ˆç‡ä¹‹é—´çš„æ–°å¸•ç´¯æ‰˜å‰æ²¿ï¼ˆPareto frontierï¼‰ï¼Œä¸ºé«˜æ•ˆå¹¶è¡Œæ¨ç†æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.07843v1",
      "published_date": "2025-11-24 18:55:59 UTC",
      "updated_date": "2025-11-24 18:55:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:56:43.935531+00:00"
    },
    {
      "arxiv_id": "2511.19418v2",
      "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens",
      "title_zh": "Chain-of-Visual-Thoughtï¼šåˆ©ç”¨è¿ç»­è§†è§‰æ ‡è®°æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥ä¸æ€è€ƒèƒ½åŠ›",
      "authors": [
        "Yiming Qin",
        "Bomin Wei",
        "Jiaxin Ge",
        "Konstantinos Kallidromitis",
        "Stephanie Fu",
        "Trevor Darrell",
        "XuDong Wang"
      ],
      "abstract": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç©ºé—´æ¨ç†å’Œå‡ ä½•æ„ŸçŸ¥ç­‰å¯†é›†è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œæå‡ºäº†Chain-of-Visual-Thought (COVT)æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è¿ç»­è§†è§‰Token (Continuous Visual Tokens) ä½œä¸ºç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºï¼Œé€šè¿‡ä»è½»é‡çº§è§†è§‰ä¸“å®¶ä¸­è’¸é¦çŸ¥è¯†æ¥æ•æ‰2Då¤–è§‚ã€3Då‡ ä½•ã€ç©ºé—´å¸ƒå±€å’Œè¾¹ç¼˜ç»“æ„ç­‰äº’è¡¥å±æ€§ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é€šè¿‡è‡ªå›å½’æ–¹å¼é¢„æµ‹è¿™äº›Tokenä»¥é‡å»ºæ·±åº¦ã€åˆ†å‰²åŠè¾¹ç¼˜ç­‰ç›‘ç£ä¿¡å·ï¼Œè€Œåœ¨æ¨ç†æ—¶åˆ™ç›´æ¥åœ¨è¿ç»­è§†è§‰Tokenç©ºé—´å†…è¿›è¡Œé«˜æ•ˆæ¨ç†ã€‚åœ¨CV-Benchã€MMVPã€RealWorldQAç­‰åå¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå°†COVTé›†æˆè‡³Qwen2.5-VLå’ŒLLaVAç­‰æ¨¡å‹åï¼Œå…¶æ€§èƒ½æ˜¾è‘—æå‡äº†3%è‡³16%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¼•å…¥ç´§å‡‘çš„è¿ç»­è§†è§‰æ€ç»´è·¯å¾„èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºå¤šæ¨¡æ€æ™ºèƒ½çš„ç²¾ç¡®æ€§ã€åœ°é¢æ€§(Grounded)ä»¥åŠæ¨ç†çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://wakalsprojectpage.github.io/covt-website/",
      "pdf_url": "https://arxiv.org/pdf/2511.19418v2",
      "published_date": "2025-11-24 18:55:19 UTC",
      "updated_date": "2025-11-30 04:42:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:57:02.633786+00:00"
    },
    {
      "arxiv_id": "2511.19417v1",
      "title": "Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration",
      "title_zh": "Be My Eyesï¼šé€šè¿‡å¤šæ™ºèƒ½ä½“åä½œå°†å¤§è¯­è¨€æ¨¡å‹æ‰©å±•è‡³æ–°æ¨¡æ€",
      "authors": [
        "James Y. Huang",
        "Sheng Zhang",
        "Qianchu Liu",
        "Guanghui Qin",
        "Tinghui Zhu",
        "Tristan Naumann",
        "Muhao Chen",
        "Hoifung Poon"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BeMyEyesï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ (multi-agent framework)ï¼Œæ—¨åœ¨é€šè¿‡åè°ƒé«˜æ•ˆä¸”å¯é€‚åº”çš„è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) ä½œä¸ºæ„ŸçŸ¥å™¨ (perceivers) ä¸å¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä½œä¸ºæ¨ç†å™¨ (reasoners) ä¹‹é—´çš„åä½œï¼Œå°† LLMs æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†é¢†åŸŸã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§æ•°æ®åˆæˆå’Œç›‘ç£å¾®è°ƒ (supervised fine-tuning) æµæ°´çº¿ï¼Œä»¥è®­ç»ƒæ„ŸçŸ¥å™¨æ™ºèƒ½ä½“ä¸æ¨ç†å™¨è¿›è¡Œé«˜æ•ˆåä½œã€‚é€šè¿‡ç»“åˆæ„ŸçŸ¥ä¸æ¨ç†çš„äº’è¡¥ä¼˜åŠ¿ï¼Œè¯¥æ–¹æ³•é¿å…äº†è®­ç»ƒå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„éœ€è¦ï¼Œå¹¶ä¿ç•™äº† LLMs åŸæœ‰çš„æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè§£é”çº¯æ–‡æœ¬æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†æ½œåŠ›ï¼Œä¾‹å¦‚é…å¤‡ Qwen2.5-VL-7B æ„ŸçŸ¥å™¨çš„ DeepSeek-R1 åœ¨å¤šé¡¹çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äº GPT-4o ç­‰å¤§è§„æ¨¡é—­æºæ¨¡å‹ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†åˆ©ç”¨å¤šæ™ºèƒ½ä½“åä½œæ„å»ºè½»é‡çº§ã€å¼€æºä¸”é«˜æ€§èƒ½å¤šæ¨¡æ€ç³»ç»Ÿåœ¨æœªæ¥å‘å±•ä¸­çš„æœ‰æ•ˆæ€§ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19417v1",
      "published_date": "2025-11-24 18:55:16 UTC",
      "updated_date": "2025-11-24 18:55:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:57:02.289947+00:00"
    },
    {
      "arxiv_id": "2511.19413v2",
      "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary",
      "title_zh": "UniGameï¼šå°†ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹è½¬åŒ–ä¸ºè‡ªèº«çš„å¯¹æŠ—è€…",
      "authors": [
        "Zhaolong Su",
        "Wang Lu",
        "Hao Chen",
        "Sharon Li",
        "Jindong Wang"
      ],
      "abstract": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ (Unified Multimodal Models, UMMs) åœ¨ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¹‹é—´å­˜åœ¨çš„æ ¹æœ¬æ€§ä¸ä¸€è‡´é—®é¢˜ï¼Œæå‡ºäº† UniGame è‡ªå¯¹æŠ—åè®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ­ç¤ºäº†ç†è§£ä»»åŠ¡åå¥½ç´§å‡‘åµŒå…¥è€Œç”Ÿæˆä»»åŠ¡åå¥½ä¸°å¯Œé‡æ„è¡¨ç¤ºä¹‹é—´çš„çŸ›ç›¾ï¼Œå¯¼è‡´æ¨¡å‹åœ¨äº¤å‰æ¨¡æ€ä¸€è‡´æ€§åŠå¯¹æŠ—é²æ£’æ€§æ–¹é¢è¡¨ç°è„†å¼±ã€‚UniGame é€šè¿‡åœ¨å…±äº«ä»¤ç‰Œæ¥å£å¤„å¼•å…¥è½»é‡çº§æ‰°åŠ¨å™¨ï¼Œä½¿ç”Ÿæˆåˆ†æ”¯èƒ½å¤Ÿä¸»åŠ¨æŒ‘æˆ˜è–„å¼±çš„ç†è§£åˆ†æ”¯ï¼Œä»è€Œå®ç°æ¨¡å‹è‡ªèº«çš„å¯¹æŠ—è¿›åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUniGame æ˜¾è‘—æå‡äº†æ¨¡å‹çš„ä¸€è‡´æ€§ï¼ˆ+4.6%ï¼‰å’Œç†è§£èƒ½åŠ›ï¼ˆ+3.6%ï¼‰ï¼Œå¹¶åœ¨ NaturalBench å’Œ AdVQA ç­‰åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºæ›´å¼ºçš„åˆ†å¸ƒå¤–åŠå¯¹æŠ—é²æ£’æ€§ã€‚è¯¥æ¡†æ¶å…·æœ‰æ¶æ„æ— å…³æ€§ï¼Œä»…å¼•å…¥ä¸åˆ° 1% çš„é¢å¤–å‚æ•°ï¼Œä¸”èƒ½ä¸ç°æœ‰çš„åè®­ç»ƒæ–¹æ³•æœ‰æ•ˆäº’è¡¥ï¼Œä¸ºæå‡å¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç¨³å®šæ€§ä¸è¿è´¯æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19413v2",
      "published_date": "2025-11-24 18:50:01 UTC",
      "updated_date": "2025-11-26 05:12:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:57:16.834538+00:00"
    },
    {
      "arxiv_id": "2511.20703v1",
      "title": "PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach",
      "title_zh": "PropensityBenchï¼šåŸºäºæ™ºèƒ½ä½“æ–¹æ³•çš„å¤§è¯­è¨€æ¨¡å‹æ½œåœ¨å®‰å…¨é£é™©è¯„ä¼°",
      "authors": [
        "Udari Madhushani Sehwag",
        "Shayan Shabihi",
        "Alex McAvoy",
        "Vikash Sehwag",
        "Yuancheng Xu",
        "Dalton Towers",
        "Furong Huang"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have sparked concerns over their potential to acquire and misuse dangerous or high-risk capabilities, posing frontier risks. Current safety evaluations primarily test for what a model \\textit{can} do - its capabilities - without assessing what it $\\textit{would}$ do if endowed with high-risk capabilities. This leaves a critical blind spot: models may strategically conceal capabilities or rapidly acquire them, while harboring latent inclinations toward misuse. We argue that $\\textbf{propensity}$ - the likelihood of a model to pursue harmful actions if empowered - is a critical, yet underexplored, axis of safety evaluation. We present $\\textbf{PropensityBench}$, a novel benchmark framework that assesses the proclivity of models to engage in risky behaviors when equipped with simulated dangerous capabilities using proxy tools. Our framework includes 5,874 scenarios with 6,648 tools spanning four high-risk domains: cybersecurity, self-proliferation, biosecurity, and chemical security. We simulate access to powerful capabilities via a controlled agentic environment and evaluate the models' choices under varying operational pressures that reflect real-world constraints or incentives models may encounter, such as resource scarcity or gaining more autonomy. Across open-source and proprietary frontier models, we uncover 9 alarming signs of propensity: models frequently choose high-risk tools when under pressure, despite lacking the capability to execute such actions unaided. These findings call for a shift from static capability audits toward dynamic propensity assessments as a prerequisite for deploying frontier AI systems safely. Our code is available at https://github.com/scaleapi/propensity-evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ç°æœ‰çš„å®‰å…¨è¯„ä¼°ç›²ç‚¹ï¼ŒæŒ‡å‡ºå½“å‰è¯„ä¼°å¤šä¾§é‡äºèƒ½åŠ›æµ‹è¯•(Capability Audits)ï¼Œè€Œå¿½è§†äº†æ¨¡å‹åœ¨å…·å¤‡é«˜é£é™©èƒ½åŠ›æ—¶æ˜¯å¦è¡¨ç°å‡ºæ»¥ç”¨å€¾å‘(Propensity)çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†PropensityBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ™ºèƒ½ä½“(Agentic)æ–¹æ³•è¯„ä¼°æ¨¡å‹åœ¨æ¨¡æ‹Ÿå±é™©ç¯å¢ƒä¸‹è¡Œä¸ºå€¾å‘çš„æ–°å‹åŸºå‡†æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†ç½‘ç»œå®‰å…¨(Cybersecurity)ã€è‡ªæˆ‘å¢æ®–(Self-proliferation)ã€ç”Ÿç‰©å®‰å…¨(Biosecurity)å’ŒåŒ–å­¦å®‰å…¨(Chemical Security)å››ä¸ªé«˜é£é™©é¢†åŸŸçš„æ•°åƒä¸ªåœºæ™¯ä¸å·¥å…·ã€‚é€šè¿‡åœ¨å—æ§çš„æ™ºèƒ½ä½“ç¯å¢ƒä¸­æ¨¡æ‹Ÿè¿è¡Œå‹åŠ›ï¼Œç ”ç©¶å‘ç°å³ä½¿æ¨¡å‹åœ¨æ— è¾…åŠ©ä¸‹ç¼ºä¹æ‰§è¡Œèƒ½åŠ›ï¼Œåœ¨å‹åŠ›ä¸‹ä»ä¼šè¡¨ç°å‡ºé€‰æ‹©é«˜é£é™©å·¥å…·çš„å€¾å‘ã€‚å®éªŒç»“æœæ­ç¤ºäº†å‰æ²¿æ¨¡å‹ä¸­å­˜åœ¨çš„æ½œåœ¨å®‰å…¨é£é™©ï¼Œå¹¶å‘¼ååœ¨å®‰å…¨éƒ¨ç½²AIç³»ç»Ÿå‰ï¼Œè¯„ä¼°åº”ä»é™æ€çš„èƒ½åŠ›å®¡è®¡è½¬å‘åŠ¨æ€çš„å€¾å‘æ€§è¯„ä¼°(Dynamic Propensity Assessments)ã€‚è¯¥é¡¹ç ”ç©¶ä¸ºç†è§£å’Œé¢„é˜²å¤§è¯­è¨€æ¨¡å‹çš„æ½œåœ¨æ»¥ç”¨é£é™©æä¾›äº†å…³é”®çš„å·¥å…·å’Œè§†è§’ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20703v1",
      "published_date": "2025-11-24 18:46:44 UTC",
      "updated_date": "2025-11-24 18:46:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:57:41.132775+00:00"
    },
    {
      "arxiv_id": "2511.19401v1",
      "title": "In-Video Instructions: Visual Signals as Generative Control",
      "title_zh": "In-Video Instructionsï¼šå°†è§†è§‰ä¿¡å·ä½œä¸ºç”Ÿæˆå¼æ§åˆ¶",
      "authors": [
        "Gongfan Fang",
        "Xinyin Ma",
        "Xinchao Wang"
      ],
      "abstract": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å¼ºå¤§è§†è§‰èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§åä¸ºIn-Video Instructionçš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡åœ¨å›¾åƒå¸§ä¸­åµŒå…¥è§†è§‰ä¿¡å·ï¼ˆå¦‚è¦†ç›–æ–‡å­—ã€ç®­å¤´æˆ–è½¨è¿¹ï¼‰æ¥å®ç°å¯æ§çš„å›¾åƒåˆ°è§†é¢‘(image-to-video)ç”Ÿæˆã€‚ä¸æä¾›å…¨å±€æ¨¡ç³Šæè¿°çš„Promptæ§åˆ¶ä¸åŒï¼ŒIn-Video Instructionå°†ç”¨æˆ·æŒ‡ä»¤ç›´æ¥ç¼–ç åˆ°è§†è§‰åŸŸï¼Œä»è€Œåœ¨è§†è§‰ä¸»ä½“åŠå…¶åŠ¨ä½œä¹‹é—´å»ºç«‹èµ·ç²¾ç¡®çš„ç©ºé—´æ„ŸçŸ¥å’Œæ— æ­§ä¹‰å¯¹åº”å…³ç³»ã€‚è¿™ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿä¸ºä¸åŒçš„ç‰©ä½“åˆ†é…ä¸ªæ€§åŒ–çš„åŠ¨ä½œæŒ‡ä»¤ï¼Œæå¤§åœ°å¢å¼ºäº†ç”Ÿæˆçš„çµæ´»æ€§ä¸å‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨Veo 3.1ã€Kling 2.5å’ŒWan 2.2ç­‰å…ˆè¿›ç”Ÿæˆæ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œç ”ç©¶è¯æ˜äº†è§†é¢‘æ¨¡å‹å¯ä»¥å¯é åœ°è§£æå¹¶æ‰§è¡Œæ­¤ç±»è§†è§‰åµŒå…¥æŒ‡ä»¤ã€‚åœ¨å¤æ‚çš„å¤šç›®æ ‡äº¤äº’åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºäº†ä¼˜äºä¼ ç»Ÿæ–‡æœ¬æ§åˆ¶çš„å¯é æ€§ï¼Œä¸ºç²¾ç»†åŒ–è§†é¢‘ç”Ÿæˆæ§åˆ¶æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19401v1",
      "published_date": "2025-11-24 18:38:45 UTC",
      "updated_date": "2025-11-24 18:38:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:58:00.939726+00:00"
    },
    {
      "arxiv_id": "2511.19399v2",
      "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
      "title_zh": "DR Tuluï¼šåŸºäºæ¼”è¿›å¼è¯„ä»·å‡†åˆ™çš„æ·±åº¦ç ”ç©¶å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Rulin Shao",
        "Akari Asai",
        "Shannon Zejiang Shen",
        "Hamish Ivison",
        "Varsha Kishore",
        "Jingming Zhuo",
        "Xinran Zhao",
        "Molly Park",
        "Samuel G. Finlayson",
        "David Sontag",
        "Tyler Murray",
        "Sewon Min",
        "Pradeep Dasigi",
        "Luca Soldaini",
        "Faeze Brahman",
        "Wen-tau Yih",
        "Tongshuang Wu",
        "Luke Zettlemoyer",
        "Yoon Kim",
        "Hannaneh Hajishirzi",
        "Pang Wei Koh"
      ],
      "abstract": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æºæ·±åº¦ç ”ç©¶æ¨¡å‹å› ä¾èµ–å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ (RLVR)è€Œéš¾ä»¥å¤„ç†é•¿ç¯‡ä»»åŠ¡çš„é—®é¢˜ï¼Œæå‡ºäº†Reinforcement Learning with Evolving Rubrics (RLER)æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨è®­ç»ƒä¸­æ„å»ºå¹¶ç»´æŠ¤ä¸ç­–ç•¥æ¨¡å‹å…±åŒæ¼”åŒ–çš„å‡†åˆ™(rubrics)ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿæ•´åˆæ–°æ¢ç´¢çš„ä¿¡æ¯å¹¶æä¾›åˆ¤åˆ«æ€§çš„åœ¨çº¿(on-policy)åé¦ˆã€‚åŸºäºæ­¤æŠ€æœ¯å¼€å‘çš„DR Tulu-8Bæ˜¯é¦–ä¸ªç›´æ¥é’ˆå¯¹å¼€æ”¾å¼ã€é•¿ç¯‡æ·±åº¦ç ”ç©¶ä»»åŠ¡è®­ç»ƒçš„å¼€æºæ¨¡å‹ã€‚åœ¨ç§‘å­¦ã€åŒ»ç–—å’Œé€šç”¨é¢†åŸŸçš„å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDR Tuluçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼€æºæ·±åº¦ç ”ç©¶æ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ˜¾è‘—å‡å°è§„æ¨¡å’Œé™ä½æŸ¥è¯¢æˆæœ¬çš„å‰æä¸‹ï¼Œå…¶æ€§èƒ½å¯åŒ¹é…ç”šè‡³è¶…è¶Šé—­æºçš„æ·±åº¦ç ”ç©¶ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†åŒ…æ‹¬åŸºäºæ¨¡å‹ä¸Šä¸‹æ–‡åè®®(MCP)çš„æ™ºèƒ½ä½“åŸºç¡€è®¾æ–½åœ¨å†…çš„æ‰€æœ‰æ•°æ®å’Œä»£ç ï¼Œä¸ºæ·±åº¦ç ”ç©¶é¢†åŸŸæä¾›äº†é‡è¦çš„å¼€æºèµ„æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19399v2",
      "published_date": "2025-11-24 18:35:54 UTC",
      "updated_date": "2025-11-26 14:52:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:58:42.842643+00:00"
    },
    {
      "arxiv_id": "2511.19396v1",
      "title": "Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments",
      "title_zh": "åŠ¨æ€å£°å­¦ç¯å¢ƒä¸‹åŸºäºç«¯ä¾§æ·±åº¦å­¦ä¹ çš„è‡ªé€‚åº”æ³¢æŸæˆå½¢å®æ—¶ç›®æ ‡è·Ÿè¸ª",
      "authors": [
        "Jorge Ortigoso-Narro",
        "Jose A. Belloch",
        "Adrian Amor-Martin",
        "Sandra Roger",
        "Maximo Cobos"
      ],
      "abstract": "Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é›†æˆåœ¨è®¾å¤‡ç«¯æ·±åº¦å­¦ä¹ (Deep Learning)è¿½è¸ªä¸è‡ªé€‚åº”æ³¢æŸæˆå½¢(Adaptive Beamforming)çš„åµŒå…¥å¼ç³»ç»Ÿï¼Œæ—¨åœ¨åŠ¨æ€å£°å­¦ç¯å¢ƒä¸­å®ç°ç²¾ç¡®çš„å£°æºå®šä½å’Œå®šå‘éŸ³é¢‘é‡‡é›†ã€‚ç³»ç»Ÿç»“åˆäº†å•ç›®æ·±åº¦ä¼°è®¡(Single-camera depth estimation)å’Œç«‹ä½“è§†è§‰(Stereo vision)æŠ€æœ¯ï¼Œå®ç°äº†å¯¹ç§»åŠ¨ç›®æ ‡çš„ç²¾ç¡®3Då®šä½ã€‚ç¡¬ä»¶ä¸Šé‡‡ç”¨äº†åŸºäºMEMSéº¦å…‹é£çš„å¹³é¢åŒå¿ƒåœ†é˜µåˆ—ï¼Œæ„å»ºäº†ä¸€ä¸ªç´§å‡‘ä¸”èŠ‚èƒ½çš„å¹³å°ï¼Œæ”¯æŒæ–¹ä½è§’å’Œä»°è§’çš„äºŒç»´æ³¢æŸè½¬å‘(2D beam steering)ã€‚å®æ—¶è¿½è¸ªè¾“å‡ºèƒ½æŒç»­è°ƒæ•´é˜µåˆ—ç„¦ç‚¹ï¼Œä½¿å£°å­¦å“åº”ä¸ç›®æ ‡ä½ç½®ä¿æŒåŒæ­¥ï¼Œç¡®ä¿äº†åœ¨å¤šå£°æºæˆ–ç§»åŠ¨æºç¯å¢ƒä¸‹çš„æ€§èƒ½é²æ£’æ€§ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥è®¾è®¡åœ¨ä¿¡å·å¹²æ‰°æ¯”(Signal-to-interference ratio)æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚è¯¥é¡¹æˆæœä¸ºè¿œç¨‹ä¼šè®®ã€æ™ºèƒ½å®¶å±…è®¾å¤‡å’Œè¾…åŠ©æŠ€æœ¯æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19396v1",
      "published_date": "2025-11-24 18:33:50 UTC",
      "updated_date": "2025-11-24 18:33:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:58:05.440845+00:00"
    },
    {
      "arxiv_id": "2511.19390v1",
      "title": "Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme",
      "title_zh": "åˆ©ç”¨å¤šå°ºåº¦æ¨ç†æœºåˆ¶ä¸‹çš„æ‰©æ•£æ¨¡å‹é¢„æµ‹éƒ¨åˆ†å¯è§‚æµ‹åŠ¨åŠ›ç³»ç»Ÿ",
      "authors": [
        "Rudy Morel",
        "Francesco Pio Ramunno",
        "Jeff Shen",
        "Alberto Bietti",
        "Kyunghyun Cho",
        "Miles Cranmer",
        "Siavash Golkar",
        "Olexandr Gugnin",
        "Geraud Krawezik",
        "Tanya Marwah",
        "Michael McCabe",
        "Lucas Meyer",
        "Payel Mukhopadhyay",
        "Ruben Ohana",
        "Liam Parker",
        "Helen Qu",
        "FranÃ§ois Rozet",
        "K. D. Leka",
        "FranÃ§ois Lanusse",
        "David Fouhey",
        "Shirley Ho"
      ],
      "abstract": "Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éƒ¨åˆ†å¯è§‚æµ‹ä¸”å…·æœ‰é•¿è®°å¿†ç‰¹æ€§çš„åŠ¨åŠ›ç³»ç»Ÿ (partially observable, long-memory dynamical systems)ï¼Œæå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹ (diffusion models) çš„å¤šå°ºåº¦æ¨ç†æ–¹æ¡ˆ (multiscale inference scheme)ã€‚ä¼ ç»Ÿçš„è‡ªå›å½’æ¨æ–­ (autoregressive rollouts) åœ¨å¤„ç†å¦‚å¤ªé˜³åŠ¨åŠ›å­¦ (solar dynamics) ç­‰ç‰©ç†è¿‡ç¨‹æ—¶ï¼Œå¾€å¾€å› æ— æ³•æœ‰æ•ˆæ•´åˆå†å²ä¿¡æ¯è€Œéš¾ä»¥æ•è·é•¿ç¨‹ä¾èµ– (long-range dependencies)ã€‚ä¸ºæ­¤ï¼Œè¯¥æ–¹æ¡ˆè®¾è®¡äº†ä¸€ç§ç‹¬ç‰¹çš„æ—¶é—´è½¨è¿¹ç”Ÿæˆç­–ç•¥ï¼Œå³åœ¨æ¥è¿‘å½“å‰æ—¶åˆ»é‡‡ç”¨ç»†ç²’åº¦å»ºæ¨¡ï¼Œè€Œåœ¨è¿œç¦»å½“å‰æ—¶åˆ»é‡‡ç”¨ç²—ç²’åº¦å»ºæ¨¡ã€‚è¿™ç§å¤šå°ºåº¦ç­–ç•¥èƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„å‰æä¸‹ï¼ŒæˆåŠŸæ•æ‰é•¿æ—¶ç©ºä¾èµ–å¹¶æ•´åˆå…³é”®çš„å†å²è§‚æµ‹ä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨æ–­æ–¹æ¡ˆæ˜¾è‘—é™ä½äº†é¢„æµ‹åˆ†å¸ƒçš„åå·®ï¼Œå¹¶å¢å¼ºäº†æ¨¡å‹åœ¨é•¿æœŸæ¼”åŒ–é¢„æµ‹ä¸­çš„ç¨³å®šæ€§ã€‚è¯¥ç ”ç©¶ä¸ºå¤ªé˜³æ´»åŠ¨åŒºæ¼”å˜ç­‰å¤æ‚ç‰©ç†ç³»ç»Ÿçš„æ¦‚ç‡é¢„æµ‹æä¾›äº†æ›´é«˜æ•ˆä¸”ç¨³å¥çš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.LG",
        "astro-ph.SR",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19390v1",
      "published_date": "2025-11-24 18:30:04 UTC",
      "updated_date": "2025-11-24 18:30:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:57:56.843054+00:00"
    },
    {
      "arxiv_id": "2511.19580v1",
      "title": "Towards Synergistic Teacher-AI Interactions with Generative Artificial Intelligence",
      "title_zh": "è¿ˆå‘ç”Ÿæˆå¼äººå·¥æ™ºèƒ½èµ‹èƒ½çš„æ•™å¸ˆ-äººå·¥æ™ºèƒ½ååŒäº’åŠ¨",
      "authors": [
        "Mutlu Cukurova",
        "Wannapon Suraworachet",
        "Qi Zhou",
        "Sahan Bulathwela"
      ],
      "abstract": "Generative artificial intelligence (GenAI) is increasingly used in education, posing significant challenges for teachers adapting to these changes. GenAI offers unprecedented opportunities for accessibility, scalability and productivity in educational tasks. However, the automation of teaching tasks through GenAI raises concerns about reduced teacher agency, potential cognitive atrophy, and the broader deprofessionalisation of teaching. Drawing findings from prior literature on AI in Education, and refining through a recent systematic literature review, this chapter presents a conceptualisation of five levels of teacher-AI teaming: transactional, situational, operational, praxical and synergistic teaming. The framework aims to capture the nuanced dynamics of teacher-AI interactions, particularly with GenAI, that may lead to the replacement, complementarity, or augmentation of teachers' competences and professional practice. GenAI technological affordances required in supporting teaming, along with empirical studies, are discussed. Drawing on empirical observations, we outline a future vision that moves beyond individual teacher agency toward collaborative decision-making between teachers and AI, in which both agents engage in negotiation, constructive challenge, and co-reasoning that enhance each other's capabilities and enable outcomes neither could realise independently. Further discussion of socio-technical factors beyond teacher-AI teaming is also included to streamline the synergy of teachers and AI in education ethically and practically.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨ï¼Œåˆ†æäº†å…¶åœ¨æå‡å¯è®¿é—®æ€§å’Œç”Ÿäº§åŠ›çš„åŒæ—¶ï¼Œå¯¹æ•™å¸ˆè‡ªä¸»æ€§åŠèŒä¸šåŒ–å¸¦æ¥çš„æ½œåœ¨æŒ‘æˆ˜ã€‚åŸºäºæ—¢å¾€æ–‡çŒ®åŠç³»ç»Ÿæ€§ç»¼è¿°ï¼Œæœ¬æ–‡æå‡ºäº†æ•™å¸ˆä¸ AI åä½œ (teacher-AI teaming) çš„äº”ä¸ªå±‚æ¬¡æ¡†æ¶ï¼Œæ¶µç›–äº‹åŠ¡æ€§ (transactional)ã€æƒ…å¢ƒæ€§ (situational)ã€æ“ä½œæ€§ (operational)ã€å®è·µæ€§ (praxical) å’ŒååŒæ€§ (synergistic) åä½œã€‚è¯¥æ¡†æ¶æ—¨åœ¨ç»†è‡´æ•æ‰æ•™å¸ˆä¸ GenAI äº¤äº’ä¸­çš„åŠ¨æ€å…³ç³»ï¼Œæ˜ç¡® AI å¦‚ä½•å®ç°å¯¹æ•™å¸ˆä¸“ä¸šå®è·µçš„æ›¿ä»£ã€äº’è¡¥æˆ–å¢å¼ºã€‚æ–‡ä¸­è¯¦ç»†è®¨è®ºäº†æ”¯æŒæ­¤ç±»åä½œæ‰€éœ€çš„ GenAI æŠ€æœ¯èµ‹èƒ½ (affordances) ä»¥åŠç›¸å…³çš„å®è¯ç ”ç©¶ã€‚ç ”ç©¶å±•æœ›äº†ä»å•ä¸€æ•™å¸ˆè‡ªä¸»æ€§å‘æ•™å¸ˆä¸ AI å…±åŒå†³ç­–çš„è½¬å‹ï¼Œå¼ºè°ƒåŒæ–¹é€šè¿‡åå•†ã€å»ºè®¾æ€§æŒ‘æˆ˜å’Œå…±åŒæ¨ç† (co-reasoning) æ¥æå‡å½¼æ­¤èƒ½åŠ›ã€‚è¿™ç§ååŒæ¨¡å¼æ—¨åœ¨å®ç°åŒæ–¹ç‹¬ç«‹æ— æ³•è¾¾æˆçš„æ•™è‚²æˆæœï¼Œå¹¶è¿›ä¸€æ­¥æ¢è®¨äº†ç¡®ä¿æ•™å¸ˆä¸ AI ååŒåœ¨ä¼¦ç†å’Œå®è·µå±‚é¢çš„ç¤¾ä¼šæŠ€æœ¯å› ç´ ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "18 pages, 6 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.19580v1",
      "published_date": "2025-11-24 18:29:29 UTC",
      "updated_date": "2025-11-24 18:29:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:59:00.637828+00:00"
    },
    {
      "arxiv_id": "2511.20702v1",
      "title": "Post-Pruning Accuracy Recovery via Data-Free Knowledge Distillation",
      "title_zh": "åŸºäºæ— æ•°æ®çŸ¥è¯†è’¸é¦çš„å‰ªæåç²¾åº¦æ¢å¤",
      "authors": [
        "Chinmay Tripurwar",
        "Utkarsh Maurya",
        "Dishant"
      ],
      "abstract": "Model pruning is a widely adopted technique to reduce the computational complexity and memory footprint of Deep Neural Networks (DNNs). However, global unstructured pruning often leads to significant degradation in accuracy, typically necessitating fine-tuning on the original training dataset to recover performance. In privacy-sensitive domains such as healthcare or finance, access to the original training data is often restricted post-deployment due to regulations (e.g., GDPR, HIPAA). This paper proposes a Data-Free Knowledge Distillation framework to bridge the gap between model compression and data privacy. We utilize DeepInversion to synthesize privacy-preserving ``dream'' images from the pre-trained teacher model by inverting Batch Normalization (BN) statistics. These synthetic images serve as a transfer set to distill knowledge from the original teacher to the pruned student network. Experimental results on CIFAR-10 across various architectures (ResNet, MobileNet, VGG) demonstrate that our method significantly recovers accuracy lost during pruning without accessing a single real data point.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨¡å‹å‰ªæ(Model pruning)åœ¨é™ä½æ·±åº¦ç¥ç»ç½‘ç»œè®¡ç®—å¤æ‚åº¦æ—¶å¯¼è‡´çš„ç²¾åº¦ä¸‹é™é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å—éšç§æ³•è§„é™åˆ¶è€Œæ— æ³•è·å–åŸå§‹è®­ç»ƒæ•°æ®çš„åœºæ™¯ä¸‹ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ— æ•°æ®çŸ¥è¯†è’¸é¦(Data-Free Knowledge Distillation)æ¡†æ¶ï¼Œåˆ©ç”¨DeepInversionæŠ€æœ¯ä»é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„æ‰¹å½’ä¸€åŒ–(Batch Normalization)ç»Ÿè®¡æ•°æ®ä¸­åˆæˆéšç§ä¿æŠ¤çš„â€œæ¢¦å¢ƒâ€å›¾åƒã€‚è¿™äº›åˆæˆå›¾åƒä½œä¸ºä¼ è¾“é›†ï¼Œå°†çŸ¥è¯†ä»æ•™å¸ˆæ¨¡å‹æœ‰æ•ˆè¿ç§»è‡³å‰ªæåçš„å­¦ç”Ÿæ¨¡å‹ã€‚å®éªŒåœ¨CIFAR-10æ•°æ®é›†ä¸ŠéªŒè¯äº†ResNetã€MobileNetå’ŒVGGç­‰å¤šç§æ¶æ„ï¼Œç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨å®Œå…¨ä¸ä½¿ç”¨çœŸå®æ•°æ®çš„æƒ…å†µä¸‹ï¼Œèƒ½æ˜¾è‘—æ¢å¤å‰ªæå¸¦æ¥çš„ç²¾åº¦æŸå¤±ï¼Œå®ç°äº†æ¨¡å‹å‹ç¼©ä¸æ•°æ®éšç§çš„æœ‰æ•ˆå¹³è¡¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20702v1",
      "published_date": "2025-11-24 18:27:40 UTC",
      "updated_date": "2025-11-24 18:27:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:58:13.328787+00:00"
    },
    {
      "arxiv_id": "2511.19577v2",
      "title": "From Wearables to Warnings: Predicting Pain Spikes in Patients with Opioid Use Disorder",
      "title_zh": "ä»å¯ç©¿æˆ´è®¾å¤‡åˆ°é£é™©é¢„è­¦ï¼šé˜¿ç‰‡ç±»è¯ç‰©ä½¿ç”¨éšœç¢æ‚£è€…çš„ç–¼ç—›éª¤å¢é¢„æµ‹",
      "authors": [
        "Abhay Goyal",
        "Navin Kumar",
        "Kimberly DiMeola",
        "Rafael Trujillo",
        "Soorya Ram Shimgekar",
        "Christian Poellabauer",
        "Pi Zonooz",
        "Ermonda Gjoni-Markaj",
        "Declan Barry",
        "Lynn Madden"
      ],
      "abstract": "Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¯ç©¿æˆ´è®¾å¤‡å’Œäººå·¥æ™ºèƒ½æŠ€æœ¯é¢„æµ‹é˜¿ç‰‡ç±»è¯ç‰©ä½¿ç”¨éšœç¢(OUD)ä¼´æ…¢æ€§ç–¼ç—›(CP)æ‚£è€…çš„ç–¼ç—›é«˜å³°(pain spikes)ã€‚ç ”ç©¶å›¢é˜Ÿè¯„ä¼°äº†å¤šç§äººå·¥æ™ºèƒ½æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿæœºå™¨å­¦ä¹ (ML)æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å¯ç©¿æˆ´è®¾å¤‡æ•°æ®æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨é¢„æµ‹ç–¼ç—›é«˜å³°æ–¹é¢å®ç°äº†è¶…è¿‡0.7çš„å‡†ç¡®ç‡ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æä¾›æ·±åº¦è§è§£æ–¹é¢ç›®å‰ä»å­˜åœ¨å±€é™ã€‚ç ”ç©¶å¼ºè°ƒï¼Œç»“åˆå®æ—¶ç›‘æµ‹ä¸å…ˆè¿›AIæ¨¡å‹èƒ½æœ‰æ•ˆæ”¯æŒä¸ªæ€§åŒ–å¹²é¢„ï¼Œæœ‰åŠ©äºé™ä½å¤å‘é£é™©å¹¶æé«˜è¯ç‰©æ²»ç–—(MOUD)çš„ä¾ä»æ€§ã€‚è¿™ä¸€æˆæœä¸ä»…è¯æ˜äº†é¢„æµ‹æ€§ç›‘æ§çš„å¯è¡Œæ€§ï¼Œä¹ŸæŒ‡å‡ºäº†å¼€å‘é’ˆå¯¹OUD/CPé¢†åŸŸä¸“ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19577v2",
      "published_date": "2025-11-24 18:19:56 UTC",
      "updated_date": "2026-01-12 04:58:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:58:20.839149+00:00"
    },
    {
      "arxiv_id": "2511.19367v1",
      "title": "An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification",
      "title_zh": "èåˆè§£å‰–æ„ŸçŸ¥çš„è‚ºç™Œè‚¿ç˜¤åˆ†æœŸåˆ†ç±»æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Saniah Kayenat Chowdhury",
        "Rusab Sarmun",
        "Muhammad E. H. Chowdhury",
        "Sohaib Bassam Zoghoul",
        "Israa Al-Hashimi",
        "Adam Mushtak",
        "Amith Khandakar"
      ],
      "abstract": "Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable \"black box\" manner, our method offers both state-of-the-art performance and transparent decision support.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§èåˆè§£å‰–æ„ŸçŸ¥çš„æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç«¯åˆ°ç«¯æ–¹æ³•åœ¨è‚ºç™Œåˆ†æœŸä¸­å¿½è§†ç©ºé—´ä¸è§£å‰–ä¿¡æ¯çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸“é—¨çš„ encoder-decoder ç½‘ç»œå¯¹è‚ºéƒ¨ã€è‚ºå¶ã€è‚¿ç˜¤ã€çºµéš” (mediastinum) å’Œè†ˆè‚Œ (diaphragm) è¿›è¡Œç²¾ç¡®åˆ†å‰²ï¼Œéšåé€šè¿‡å¯¹åˆ†å‰²æ©ç è¿›è¡Œå®šé‡åˆ†æï¼Œæ˜¾å¼æµ‹é‡è‚¿ç˜¤çš„æœ€å¤§å°ºå¯¸åŠå…¶ä¸é‚»è¿‘è§£å‰–ç»“æ„çš„è·ç¦»ã€‚æœ€åï¼Œç ”ç©¶æ ¹æ®ä¸´åºŠåŒ»å­¦æŒ‡å—åº”ç”¨ rule-based é€»è¾‘è¿›è¡Œè‚¿ç˜¤åˆ†æœŸï¼Œè€Œéå°†å…¶ç®€åŒ–ä¸ºçº¯ç²¹çš„å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚åœ¨ Lung-PET-CT-Dx æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶å®ç°äº† 91.36% çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œå„åˆ†æœŸï¼ˆT1-T4ï¼‰çš„ F1-scores è¡¨ç°å‡è¡¡ä¸”ä¼˜å¼‚ã€‚è¯¥ç ”ç©¶æ˜¯é¦–æ¬¡å°†æ˜¾å¼ä¸´åºŠèƒŒæ™¯åµŒå…¥åˆ†æœŸä»»åŠ¡çš„å°è¯•ï¼Œå…‹æœäº†ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œ (CNNs) çš„â€œé»‘ç›’â€å±æ€§ï¼Œä¸ºä¸´åºŠæä¾›äº†é«˜æ€§èƒ½ä¸”å…·æœ‰é€æ˜åº¦çš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19367v1",
      "published_date": "2025-11-24 18:01:47 UTC",
      "updated_date": "2025-11-24 18:01:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:58:30.535133+00:00"
    },
    {
      "arxiv_id": "2511.19575v2",
      "title": "HunyuanOCR Technical Report",
      "title_zh": "HunyuanOCR æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Hunyuan Vision Team",
        "Pengyuan Lyu",
        "Xingyu Wan",
        "Gengluo Li",
        "Shangpin Peng",
        "Weinong Wang",
        "Liang Wu",
        "Huawen Shen",
        "Yu Zhou",
        "Canhui Tang",
        "Qi Yang",
        "Qiming Peng",
        "Bin Luo",
        "Hower Yang",
        "Xinsong Zhang",
        "Jinnian Zhang",
        "Houwen Peng",
        "Hongming Yang",
        "Senhao Xie",
        "Longsha Zhou",
        "Ge Pei",
        "Binghong Wu",
        "Rui Yan",
        "Kan Wu",
        "Jieneng Yang",
        "Bochao Wang",
        "Kai Liu",
        "Jianchen Zhu",
        "Jie Jiang",
        "Linus",
        "Han Hu",
        "Chengquan Zhang"
      ],
      "abstract": "This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.\n  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow \"OCR expert models\" and inefficient \"General VLMs\". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.\n  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.",
      "tldr_zh": "è¯¥ç ”ç©¶å‘å¸ƒäº†HunyuanOCRï¼Œä¸€ä¸ªå•†ç”¨çº§ã€å¼€æºä¸”è½»é‡çº§ï¼ˆ1Bå‚æ•°ï¼‰çš„Vision-Language Model (VLM)ï¼Œä¸“é—¨ç”¨äºå¤„ç†å„ç±»OCRä»»åŠ¡ã€‚å…¶æ¶æ„ç”±Native Vision Transformer (ViT)å’Œè½»é‡çº§LLMé€šè¿‡MLP adapterè¿æ¥è€Œæˆï¼Œåœ¨æ„ŸçŸ¥ä»»åŠ¡å’Œè¯­ä¹‰ä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½è¶…è¶Šäº†éƒ¨åˆ†å•†ä¸šAPIåŠQwen3-VL-4Bç­‰æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚HunyuanOCRå®ç°äº†é€šç”¨æ€§ä¸æ•ˆç‡çš„ç»Ÿä¸€ï¼Œåœ¨è½»é‡çº§æ¡†æ¶å†…å…¨é¢æ”¯æŒText Spottingã€Parsingã€IEã€VQAåŠç¿»è¯‘ç­‰åŠŸèƒ½ã€‚é€šè¿‡é‡‡ç”¨çº¯ç²¹çš„ç«¯åˆ°ç«¯(end-to-end)æ¶æ„ï¼Œè¯¥æ¨¡å‹æ¶ˆé™¤äº†å¯¹å¸ƒå±€åˆ†æç­‰é¢„å¤„ç†æ¨¡å—çš„ä¾èµ–ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸPipelineä¸­çš„é”™è¯¯ä¼ æ’­é—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡é«˜è´¨é‡æ•°æ®å’Œå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç­–ç•¥å®ç°äº†æŠ€æœ¯çªç ´ï¼Œå¹¶åœ¨ICDAR 2025 DIMTæŒ‘æˆ˜èµ›ä¸­å–å¾—ç¬¬ä¸€åã€‚ç›®å‰è¯¥æ¨¡å‹å·²åœ¨HuggingFaceå¼€æºï¼Œå¹¶é…å¥—æä¾›åŸºäºvLLMçš„é«˜æ€§èƒ½éƒ¨ç½²æ–¹æ¡ˆï¼Œæ—¨åœ¨ä¸ºå·¥ä¸šåº”ç”¨æä¾›é«˜æ•ˆä¸”åšå®çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19575v2",
      "published_date": "2025-11-24 17:59:59 UTC",
      "updated_date": "2025-12-11 04:04:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:59:09.364964+00:00"
    },
    {
      "arxiv_id": "2511.19365v1",
      "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation",
      "title_zh": "DeCoï¼šé¢å‘ç«¯åˆ°ç«¯å›¾åƒç”Ÿæˆçš„é¢‘ç‡è§£è€¦åƒç´ æ‰©æ•£",
      "authors": [
        "Zehong Ma",
        "Longhui Wei",
        "Shuai Wang",
        "Shiliang Zhang",
        "Qi Tian"
      ],
      "abstract": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeCoï¼Œä¸€ç§é¢‘ç‡è§£è€¦(Frequency-Decoupled)çš„åƒç´ æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç«¯åˆ°ç«¯å›¾åƒç”Ÿæˆçš„æ•ˆç‡å¹¶è§£å†³ä¼ ç»Ÿåƒç´ æ‰©æ•£æ¨¡å‹è®­ç»ƒæ¨ç†æ…¢çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å›¾åƒçš„é«˜é¢‘ç»†èŠ‚ä¸ä½é¢‘è¯­ä¹‰ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œè§£è€¦ï¼Œåˆ©ç”¨è½»é‡çº§åƒç´ è§£ç å™¨åœ¨Diffusion Transformer (DiT)çš„è¯­ä¹‰å¼•å¯¼ä¸‹è´Ÿè´£ç»†èŠ‚ç”Ÿæˆã€‚è¿™ç§è®¾è®¡ä½¿DiTèƒ½å¤Ÿä¸“æ³¨äºå»ºæ¨¡æ ¸å¿ƒè¯­ä¹‰ï¼Œå¹¶ç»“åˆé¢‘ç‡æ„ŸçŸ¥æµé‡åŒ¹é…æŸå¤±(Frequency-aware Flow-matching Loss)æ¥ä¼˜åŒ–è§†è§‰æ˜¾è‘—é¢‘ç‡çš„è¡¨è¾¾ã€‚å®éªŒè¡¨æ˜ï¼ŒDeCoåœ¨ImageNet 256x256æ•°æ®é›†ä¸Šè¾¾åˆ°äº†1.62çš„FIDè¯„åˆ†ï¼ŒæˆåŠŸç¼©å°äº†ä¸æ½œåœ¨æ‰©æ•£(Latent Diffusion)æ–¹æ³•çš„æ€§èƒ½å·®è·ã€‚æ­¤å¤–ï¼Œå…¶é¢„è®­ç»ƒæ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹åœ¨GenEvalè¯„ä¼°ä¸­å–å¾—äº†0.86çš„é¢†å…ˆæˆç»©ï¼Œå……åˆ†è¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ¨¡å‹å®¹é‡å’Œç”Ÿæˆè´¨é‡ä¸Šçš„ä¼˜è¶Šè¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://zehong-ma.github.io/DeCo. Code Repository: https://github.com/Zehong-Ma/DeCo",
      "pdf_url": "https://arxiv.org/pdf/2511.19365v1",
      "published_date": "2025-11-24 17:59:06 UTC",
      "updated_date": "2025-11-24 17:59:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:59:34.637345+00:00"
    },
    {
      "arxiv_id": "2511.19355v1",
      "title": "Leveraging LLMs for reward function design in reinforcement learning control tasks",
      "title_zh": "åˆ©ç”¨ LLM ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ æ§åˆ¶ä»»åŠ¡ä¸­çš„å¥–åŠ±å‡½æ•°è®¾è®¡",
      "authors": [
        "Franklin Cardenoso",
        "Wouter Caarls"
      ],
      "abstract": "The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡ç“¶é¢ˆçš„å…¨è‡ªåŠ¨ã€æ¨¡å‹æ— å…³æ¡†æ¶ã€‚ä¸ EUREKA ç­‰ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒLEARN-Opt æ— éœ€ç¯å¢ƒæºä»£ç æˆ–é¢„è®¾çš„äººå·¥è¯„ä¼°æŒ‡æ ‡ï¼Œä»…å‡­ç³»ç»Ÿå’Œä»»åŠ¡çš„æ–‡æœ¬æè¿°å³å¯è‡ªä¸»æ¨å¯¼æ€§èƒ½åº¦é‡æ ‡å‡†ï¼Œå®ç°äº†å¥–åŠ±å‡½æ•°çš„è‡ªåŠ¨åŒ–ç”Ÿæˆã€æ‰§è¡Œä¸æ— ç›‘ç£è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLEARN-Opt åœ¨æ§åˆ¶ä»»åŠ¡ä¸Šçš„è¡¨ç°å¯ä¸æˆ–è¶…è¿‡å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯æ°´å¹³ï¼Œä¸”å¯¹å…ˆéªŒçŸ¥è¯†çš„éœ€æ±‚æ›´å°‘ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°è‡ªåŠ¨åŒ–å¥–åŠ±è®¾è®¡å…·æœ‰é«˜æ–¹å·®ç‰¹æ€§ï¼Œé€šå¸¸éœ€è¦é€šè¿‡å¤šæ¬¡è¿è¡Œ (multi-run approach) æ¥è¯†åˆ«æœ€ä¼˜å€™é€‰æ–¹æ¡ˆã€‚æ­¤å¤–ï¼ŒLEARN-Opt æˆåŠŸé‡Šæ”¾äº†ä½æˆæœ¬ LLMs çš„æ½œåŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆä¸å¤§å‹æ¨¡å‹è¡¨ç°ç›¸å½“ç”šè‡³æ›´ä¼˜çš„é«˜æ€§èƒ½å¥–åŠ±å‡½æ•°ã€‚è¯¥æ¡†æ¶æ˜¾è‘—é™ä½äº†å·¥ç¨‹å¼€é”€å¹¶å¢å¼ºäº†ç³»ç»Ÿçš„é€šç”¨æ€§ï¼Œä¸ºåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°å¥–åŠ±å‡½æ•°è‡ªåŠ¨åŒ–è®¾è®¡æä¾›äº†é«˜æ•ˆä¸”ä½æˆæœ¬çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19355v1",
      "published_date": "2025-11-24 17:55:46 UTC",
      "updated_date": "2025-11-24 17:55:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:59:34.033424+00:00"
    },
    {
      "arxiv_id": "2511.19342v1",
      "title": "Explicit Tonal Tension Conditioning via Dual-Level Beam Search for Symbolic Music Generation",
      "title_zh": "åŸºäºåŒå±‚é›†æŸæœç´¢çš„ç¬¦å·åŒ–éŸ³ä¹ç”Ÿæˆæ˜¾å¼éŸ³è°ƒå¼ åŠ›è°ƒèŠ‚",
      "authors": [
        "Maral Ebrahimzadeh",
        "Gilberto Bernardes",
        "Sebastian Stober"
      ],
      "abstract": "State-of-the-art symbolic music generation models have recently achieved remarkable output quality, yet explicit control over compositional features, such as tonal tension, remains challenging. We propose a novel approach that integrates a computational tonal tension model, based on tonal interval vector analysis, into a Transformer framework. Our method employs a two-level beam search strategy during inference. At the token level, generated candidates are re-ranked using model probability and diversity metrics to maintain overall quality. At the bar level, a tension-based re-ranking is applied to ensure that the generated music aligns with a desired tension curve. Objective evaluations indicate that our approach effectively modulates tonal tension, and subjective listening tests confirm that the system produces outputs that align with the target tension. These results demonstrate that explicit tension conditioning through a dual-level beam search provides a powerful and intuitive tool to guide AI-generated music. Furthermore, our experiments demonstrate that our method can generate multiple distinct musical interpretations under the same tension condition.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¬¦å·éŸ³ä¹ç”Ÿæˆ(Symbolic Music Generation)ä¸­è°ƒæ€§å¼ åŠ›(Tonal Tension)éš¾ä»¥æ˜¾å¼æ§åˆ¶çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†åŸºäºè°ƒæ€§åŒºé—´å‘é‡(Tonal Interval Vector)åˆ†æçš„è®¡ç®—æ¨¡å‹é›†æˆåˆ°Transformeræ¡†æ¶ä¸­çš„æ–°æ–¹æ³•ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°é‡‡ç”¨äº†åŒå±‚æŸæœç´¢(Dual-Level Beam Search)ç­–ç•¥ï¼Œå…¶ä¸­Tokençº§åˆ«é€šè¿‡æ¦‚ç‡å’Œå¤šæ ·æ€§æŒ‡æ ‡ä¿éšœç”Ÿæˆè´¨é‡ï¼Œè€Œå°èŠ‚(Bar)çº§åˆ«åˆ™åˆ©ç”¨å¼ åŠ›é‡æ’åºç¡®ä¿è¾“å‡ºèƒ½å¤Ÿç²¾ç¡®åŒ¹é…é¢„è®¾çš„å¼ åŠ›æ›²çº¿(Tension Curve)ã€‚å®¢è§‚è¯„ä¼°å’Œä¸»è§‚å¬æ„Ÿæµ‹è¯•å‡è¯æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆä¸”ç›´è§‚åœ°è°ƒèŠ‚è°ƒæ€§å¼ åŠ›ï¼Œä½¿ç”Ÿæˆçš„éŸ³ä¹ä¸ç›®æ ‡å¼ åŠ›é«˜åº¦ä¸€è‡´ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜è¯¥æ–¹æ³•åœ¨ç›¸åŒçš„å¼ åŠ›çº¦æŸä¸‹èƒ½å¤Ÿç”Ÿæˆå¤šç§ä¸åŒçš„éŸ³ä¹è¯ é‡Šï¼Œä¸ºAIè¾…åŠ©éŸ³ä¹åˆ›ä½œæä¾›äº†å¼ºå¤§çš„å¯æ§æ€§ä¸è¡¨è¾¾çµæ´»æ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "12 pages, 2 Figures, Accepted at the 17th International Symposium on Computer Music Multidisciplinary Research (CMMR) 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.19342v1",
      "published_date": "2025-11-24 17:41:04 UTC",
      "updated_date": "2025-11-24 17:41:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:59:42.237553+00:00"
    },
    {
      "arxiv_id": "2511.19325v1",
      "title": "Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval",
      "title_zh": "é¢å‘è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢çš„åŸºäºå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¼æŸ¥è¯¢æ‰©å±•",
      "authors": [
        "Olivia Macmillan-Scott",
        "Roksana Goworek",
        "Eda B. Ã–zyiÄŸit"
      ],
      "abstract": "Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹(mLLMs)åœ¨è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢(Cross-Lingual Information Retrieval, CLIR)ä¸­é€šè¿‡ç”Ÿæˆå¼æŸ¥è¯¢æ‰©å±•(Generative Query Expansion)æå‡æ£€ç´¢æ€§èƒ½çš„æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼Œå°†æŸ¥è¯¢æ‰©å±•ä»ä¼ ç»Ÿçš„åŒä¹‰è¯æ‰©å±•è½¬å‘ç”Ÿæˆä¼ªæ–‡æ¡£(Pseudo-document Generation)ï¼Œèƒ½æœ‰æ•ˆå¼¥åˆçŸ­æŸ¥è¯¢ä¸é•¿æ–‡æ¡£ä¹‹é—´çš„è¯­ä¹‰å·®è·ï¼Œåœ¨ç¨ å¯†æ£€ç´¢(Dense Retrieval)ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒæŸ¥è¯¢é•¿åº¦æ˜¯å†³å®šæç¤º(Prompting)ç­–ç•¥æœ‰æ•ˆæ€§çš„å…³é”®å› ç´ ï¼Œè€Œè¿‡äºå¤æ‚çš„æç¤ºå¾€å¾€æ— æ³•å¸¦æ¥è¿›ä¸€æ­¥çš„æ€§èƒ½å¢ç›Šã€‚å°½ç®¡è¯¥æ–¹æ³•åœ¨åŸºå‡†è¡¨ç°è¾ƒå¼±çš„è¯­è¨€ä¸Šè¿›æ­¥æœ€ä¸ºæ˜¾è‘—ï¼Œä½†åœ¨ä½¿ç”¨ä¸åŒä¹¦å†™ç³»ç»Ÿ(Scripts)çš„è¯­è¨€ä¹‹é—´ï¼Œæ£€ç´¢æ•ˆæœä¾ç„¶å­˜åœ¨æ˜æ˜¾çš„è¯­è¨€å·®å¼‚å’ŒæŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå¾®è°ƒ(Fine-tuning)çš„æ•ˆæœé«˜åº¦ä¾èµ–äºè®­ç»ƒä¸æµ‹è¯•æ•°æ®æ ¼å¼çš„ä¸€è‡´æ€§ï¼Œè¿™å‡¸æ˜¾äº†å¼€å‘æ›´å¹³è¡¡çš„å¤šè¯­è¨€åŠè·¨è¯­è¨€è®­ç»ƒä¸è¯„ä¼°èµ„æºçš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19325v1",
      "published_date": "2025-11-24 17:18:25 UTC",
      "updated_date": "2025-11-24 17:18:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:59:37.732849+00:00"
    },
    {
      "arxiv_id": "2511.19324v1",
      "title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models",
      "title_zh": "è·¨è¯­è¨€æ’åçš„é©±åŠ¨å› ç´ ï¼šåŸºäºå¤šè¯­è¨€è¯­è¨€æ¨¡å‹çš„æ£€ç´¢æ–¹æ³•",
      "authors": [
        "Roksana Goworek",
        "Olivia Macmillan-Scott",
        "Eda B. Ã–zyiÄŸit"
      ],
      "abstract": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ (CLIR) ä¸­çš„é©±åŠ¨å› ç´ ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•ä¾èµ–ç¿»è¯‘å’Œå¯å‘å¼å•è¯­æ£€ç´¢å¯¼è‡´çš„è®¡ç®—å¼€é”€åŠæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ä½œè€…ç³»ç»Ÿåœ°è¯„ä¼°äº†å››ç§å¹²é¢„æ‰‹æ®µï¼ŒåŒ…æ‹¬æ–‡æ¡£ç¿»è¯‘ (document translation)ã€åŸºäºé¢„è®­ç»ƒç¼–ç å™¨çš„å¤šè¯­è¨€å¯†é›†æ£€ç´¢ (multilingual dense retrieval)ã€ä¸åŒç²’åº¦çš„å¯¹æ¯”å­¦ä¹  (contrastive learning) ä»¥åŠäº¤å‰ç¼–ç å™¨é‡æ’åº (cross-encoder re-ranking)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸“é—¨ä¸º CLIR è®­ç»ƒçš„å¯†é›†æ£€ç´¢æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„è¯æ³•åŒ¹é… (lexical matching)ï¼Œä¸”åŸºæœ¬ä¸å†å—ç›Šäºæ–‡æ¡£ç¿»è¯‘ã€‚å¯¹æ¯”å­¦ä¹ èƒ½æœ‰æ•ˆç¼“è§£è¯­è¨€åè§å¹¶å¢å¼ºåˆå§‹å¯¹é½è¾ƒå¼±çš„ç¼–ç å™¨ï¼Œè€Œé‡æ’åºçš„æœ‰æ•ˆæ€§åˆ™é«˜åº¦ä¾èµ–äºè®­ç»ƒæ•°æ®çš„è´¨é‡ã€‚å°½ç®¡é«˜èµ„æºè¯­è¨€ä»å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†è¯¥æ–¹æ³•åœ¨ä½èµ„æºå’Œè·¨è¯­ç§ (cross-script) ç»„åˆä¸Šç›¸æ¯”åŸºå‡†æ¨¡å‹æå‡æœ€ä¸ºæ˜¾è‘—ã€‚ç ”ç©¶ç»“è®ºæŒ‡å‡ºï¼Œè·¨è¯­è¨€æœç´¢ç³»ç»Ÿåº”ä¼˜å…ˆé‡‡ç”¨è¯­ä¹‰å¤šè¯­è¨€åµŒå…¥å’Œé’ˆå¯¹æ€§çš„å­¦ä¹ å¯¹é½ç­–ç•¥ï¼Œè€Œéä¼ ç»Ÿçš„åŸºäºç¿»è¯‘çš„æµæ°´çº¿ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19324v1",
      "published_date": "2025-11-24 17:17:40 UTC",
      "updated_date": "2025-11-24 17:17:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:59:36.838164+00:00"
    },
    {
      "arxiv_id": "2511.19317v1",
      "title": "MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset",
      "title_zh": "MultiBanAbsï¼šä¸€ä¸ªå…¨é¢çš„å¤šé¢†åŸŸå­ŸåŠ æ‹‰è¯­ç”Ÿæˆå¼æ–‡æœ¬æ‘˜è¦æ•°æ®é›†",
      "authors": [
        "Md. Tanzim Ferdous",
        "Naeem Ahsan Chowdhury",
        "Prithwiraj Bhattacharjee"
      ],
      "abstract": "This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åä¸º MultiBanAbs çš„å¤§è§„æ¨¡å¤šé¢†åŸŸå­ŸåŠ æ‹‰è¯­ (Bangla) ç”Ÿæˆå¼æ–‡æœ¬æ‘˜è¦æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†ä¸»è¦å±€é™äºæ–°é—»é¢†åŸŸä¸”éš¾ä»¥é€‚åº”ç°å®ä¸­å¤šæ ·åŒ–å†™ä½œé£æ ¼çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ 54,000 ç¯‡æ¥è‡ª Cinegolpo åšå®¢ä»¥åŠ Samakal å’Œ The Business Standard ç­‰æŠ¥çº¸çš„æ–‡ç« ä¸æ‘˜è¦ï¼Œæ¶µç›–äº†å¤šç§é¢†åŸŸå’Œé£æ ¼ï¼Œå…·æœ‰æé«˜çš„é€‚åº”æ€§ã€‚ä¸ºäº†å»ºç«‹å¼ºæœ‰åŠ›çš„åŸºå‡† (baselines)ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨ LSTMã€BanglaT5-small å’Œ MTS-small ç­‰æ·±åº¦å­¦ä¹ ä¸è¿ç§»å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒå’Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ•°æ®é›†å¯ä½œä¸ºå­ŸåŠ æ‹‰è¯­è‡ªç„¶è¯­è¨€å¤„ç† (NLP) æœªæ¥ç ”ç©¶çš„é‡è¦åŸºå‡†ï¼Œä¸ºæ„å»ºé²æ£’çš„æ‘˜è¦ç³»ç»Ÿæä¾›äº†åšå®åŸºç¡€ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æœ‰æ•ˆæ‰©å±•äº†ä½èµ„æºè¯­è¨€ (low-resource languages) çš„å­¦æœ¯èµ„æºï¼Œä¹Ÿä¸ºå¸®åŠ©è¯»è€…å¿«é€Ÿç†è§£æµ·é‡å­ŸåŠ æ‹‰è¯­å†…å®¹æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19317v1",
      "published_date": "2025-11-24 17:11:49 UTC",
      "updated_date": "2025-11-24 17:11:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:59:52.437614+00:00"
    },
    {
      "arxiv_id": "2511.19316v1",
      "title": "Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach",
      "title_zh": "è¯„ä¼°å®šåˆ¶åŒ–æ‰©æ•£æ¨¡å‹å¾®è°ƒå¯è¿½æº¯æ€§çš„æ•°æ®é›†æ°´å°ï¼šå…¨é¢åŸºå‡†æµ‹è¯•ä¸æ¶ˆé™¤æ–¹æ³•",
      "authors": [
        "Xincheng Wang",
        "Hanchi Sun",
        "Wenjun Sun",
        "Kejun Xue",
        "Wangqiu Zhou",
        "Jianbo Zhang",
        "Wei Sun",
        "Dandan Zhu",
        "Xiongkuo Min",
        "Jun Jia",
        "Zhijun Fang"
      ],
      "abstract": "Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®šåˆ¶åŒ– Diffusion Models å¾®è°ƒæ‰€å¼•å‘çš„ç‰ˆæƒä¸å®‰å…¨é£é™©ï¼Œé‡ç‚¹è¯„ä¼°äº†ç”¨äºå¾®è°ƒå¯è¿½æº¯æ€§çš„ Dataset Watermarking æŠ€æœ¯ã€‚ä¸ºäº†è§£å†³å½“å‰ç¼ºä¹ç»Ÿä¸€è¯„ä¼°ä½“ç³»çš„é—®é¢˜ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªé€šç”¨çš„å¨èƒæ¨¡å‹ï¼Œå¹¶æå‡ºäº†æ¶µç›– Universalityã€Transmissibility å’Œ Robustness çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æ°´å°æ–¹æ³•åœ¨ Universality å’Œ Transmissibility æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åº”å¯¹çœŸå®ä¸–ç•Œå¨èƒåœºæ™¯æ—¶çš„ Robustness ä»å­˜åœ¨çŸ­æ¿ã€‚ä¸ºè¿›ä¸€æ­¥æ­ç¤ºè¿™äº›æ¼æ´ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§å®ç”¨çš„æ°´å°ç§»é™¤æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸å½±å“å¾®è°ƒæ•ˆæœçš„æƒ…å†µä¸‹å®Œå…¨æ¶ˆé™¤æ•°æ®é›†æ°´å°ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡æ­ç¤ºç°æœ‰é˜²å¾¡æ‰‹æ®µçš„è„†å¼±æ€§ï¼Œä¸ºæœªæ¥æ›´å…·é²æ£’æ€§çš„æ•°æ®é›†æ°´å°ç ”ç©¶æå‡ºäº†å…³é”®æŒ‘æˆ˜å¹¶æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19316v1",
      "published_date": "2025-11-24 17:11:00 UTC",
      "updated_date": "2025-11-24 17:11:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:59:56.134848+00:00"
    },
    {
      "arxiv_id": "2511.19314v1",
      "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
      "title_zh": "PRInTSï¼šé¢å‘é•¿æ—¶ç¨‹ä¿¡æ¯å¯»æ±‚çš„å¥–åŠ±å»ºæ¨¡",
      "authors": [
        "Jaewoo Lee",
        "Archiki Prasad",
        "Justin Chih-Yao Chen",
        "Zaid Khan",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIæ™ºèƒ½ä½“åœ¨é•¿ç¨‹ä¿¡æ¯å¯»æ±‚ï¼ˆlong-horizon information-seekingï¼‰ä»»åŠ¡ä¸­é¢ä¸´çš„å¤šæ­¥æ¨ç†ä¸å·¥å…·è°ƒç”¨æŒ‘æˆ˜ï¼Œæå‡ºäº†PRInTSã€‚PRInTSæ˜¯ä¸€ç§å…·æœ‰åŒé‡èƒ½åŠ›çš„ç”Ÿæˆå¼è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆprocess reward models, PRMsï¼‰ï¼Œæ—¨åœ¨å…‹æœä¼ ç»ŸPRMåœ¨äºŒå…ƒåˆ¤æ–­å’Œå¤„ç†é•¿ç¨‹ä¸Šä¸‹æ–‡æ—¶çš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹å·¥å…·è°ƒç”¨ä¿¡æ¯é‡å’Œå·¥å…·è¾“å‡ºè§£é‡Šç­‰å¤šç»´åº¦è¿›è¡Œç¨ å¯†è¯„åˆ†ï¼ˆdense scoringï¼‰ï¼Œå¹¶åˆ©ç”¨è½¨è¿¹æ‘˜è¦ï¼ˆtrajectory summarizationï¼‰æŠ€æœ¯åœ¨å‹ç¼©ä¸Šä¸‹æ–‡çš„åŒæ—¶ä¿ç•™å…³é”®è¯„ä¼°ä¿¡æ¯ã€‚åœ¨FRAMESã€GAIAå’ŒWebWalkerQAç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç»“åˆPRInTSçš„né€‰ä¸€é‡‡æ ·ï¼ˆbest-of-n samplingï¼‰èƒ½æ˜¾è‘—å¢å¼ºå¼€æºæ¨¡å‹å’Œä¸“ä¸šæ™ºèƒ½ä½“çš„ä¿¡æ¯å¯»æ±‚èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œä½¿ç”¨è¾ƒå°éª¨å¹²æ™ºèƒ½ä½“ï¼ˆbackbone agentï¼‰çš„PRInTSåœ¨æ€§èƒ½ä¸Šå¯åŒ¹é…æˆ–è¶…è¶Šå‰æ²¿æ¨¡å‹ï¼ˆfrontier modelsï¼‰ï¼Œä¸”è¡¨ç°ä¼˜äºå…¶ä»–å¼ºå¥–åŠ±å»ºæ¨¡åŸºå‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, code: https://github.com/G-JWLee/PRInTS",
      "pdf_url": "https://arxiv.org/pdf/2511.19314v1",
      "published_date": "2025-11-24 17:09:43 UTC",
      "updated_date": "2025-11-24 17:09:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:59:54.934739+00:00"
    },
    {
      "arxiv_id": "2511.19304v2",
      "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning",
      "title_zh": "AutoEnvï¼šè¯„ä¼°è·¨ç¯å¢ƒæ™ºèƒ½ä½“å­¦ä¹ çš„è‡ªåŠ¨åŒ–ç¯å¢ƒ",
      "authors": [
        "Jiayi Zhang",
        "Yiran Peng",
        "Fanqi Kong",
        "Cheng Yang",
        "Yifan Wu",
        "Zhaoyang Yu",
        "Jinyu Xiang",
        "Jianhao Ruan",
        "Jinlin Wang",
        "Maojia Song",
        "HongZhang Liu",
        "Xiangru Tang",
        "Bang Liu",
        "Chenglin Wu",
        "Yuyu Luo"
      ],
      "abstract": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ™ºèƒ½ä½“ä¸»è¦åœ¨å›ºå®šç¯å¢ƒåˆ†å¸ƒä¸­æ¼”åŒ–è€Œç¼ºä¹è·¨ç¯å¢ƒå­¦ä¹ (Cross-environment learning)åº¦é‡æ ‡å‡†çš„é—®é¢˜ï¼Œæå‡ºäº†è‡ªåŠ¨åŒ–æ¡†æ¶AutoEnvã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç¯å¢ƒå»ºæ¨¡ä¸ºè½¬æ¢ã€è§‚å¯Ÿå’Œå¥–åŠ±çš„å¯å› å­åŒ–åˆ†å¸ƒï¼Œå®ç°äº†ä½æˆæœ¬ç”Ÿæˆå¼‚è´¨åŒ–ä¸–ç•Œï¼Œå¹¶æ®æ­¤æ„å»ºäº†åŒ…å«36ä¸ªç¯å¢ƒã€358ä¸ªéªŒè¯å…³å¡çš„AutoEnv-36æ•°æ®é›†ã€‚ç ”ç©¶å°†æ™ºèƒ½ä½“å­¦ä¹ è¿‡ç¨‹å½¢å¼åŒ–ä¸ºç”±é€‰æ‹©(Selection)ã€ä¼˜åŒ–(Optimization)å’Œè¯„ä¼°(Evaluation)æ„æˆçš„ä¸‰ä¸ªé˜¶æ®µï¼Œå¹¶è¯„ä¼°äº†å…«ç§ä¸åŒçš„å­¦ä¹ æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ƒç§è¯­è¨€æ¨¡å‹åœ¨AutoEnv-36ä¸Šçš„å½’ä¸€åŒ–å¥–åŠ±ä»…ä¸º12-49%ï¼Œä¸”å›ºå®šå­¦ä¹ æ–¹æ³•åœ¨å¼‚è´¨ç¯å¢ƒä¸­çš„æ‰©å±•æ€§è¾ƒå·®ã€‚å°½ç®¡ç¯å¢ƒè‡ªé€‚åº”(Environment-adaptive)çš„å­¦ä¹ æ–¹æ³•é€‰æ‹©èƒ½æå‡æ€§èƒ½ï¼Œä½†éšç€æ–¹æ³•ç©ºé—´æ‰©å¤§ï¼Œå…¶æ”¶ç›Šä¹Ÿå‘ˆç°é€’å‡è¶‹åŠ¿ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†è·¨ç¯å¢ƒæ³›åŒ–ä¸­æ™ºèƒ½ä½“å­¦ä¹ çš„å¿…è¦æ€§ä¸å±€é™æ€§ï¼Œä¸ºæœªæ¥è·¨ç¯å¢ƒæ™ºèƒ½ä½“å­¦ä¹ ç ”ç©¶æä¾›äº†é‡è¦çš„å®éªŒåŸºå‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19304v2",
      "published_date": "2025-11-24 16:54:23 UTC",
      "updated_date": "2025-12-03 07:47:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:00:04.755430+00:00"
    },
    {
      "arxiv_id": "2511.19299v1",
      "title": "Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning",
      "title_zh": "å¼€æºæƒé‡åŸºå› ç»„è¯­è¨€æ¨¡å‹å®‰å…¨é˜²æŠ¤ï¼šåŸºäºå¯¹æŠ—æ€§å¾®è°ƒçš„é²æ£’æ€§è¯„ä¼°",
      "authors": [
        "James R. M. Black",
        "Moritz S. Hanke",
        "Aaron Maiwald",
        "Tina Hernandez-Boussard",
        "Oliver M. Crook",
        "Jaspreet Pannu"
      ],
      "abstract": "Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¼€æºåŸºå› ç»„è¯­è¨€æ¨¡å‹ (genomic language models, gLMs) åœ¨å¯¹æŠ—æ€§å¾®è°ƒ (adversarial fine-tuning) ä¸‹çš„é²æ£’æ€§ï¼Œé‡ç‚¹åˆ†æäº†ç°æœ‰å®‰å…¨é™åˆ¶æªæ–½çš„æœ‰æ•ˆæ€§ã€‚é’ˆå¯¹ç›®å‰æ™®éé‡‡ç”¨çš„è¿‡æ»¤é¢„è®­ç»ƒç—…æ¯’æ•°æ®è¿™ä¸€é˜²å¾¡æ‰‹æ®µï¼Œç ”ç©¶è€…ä½¿ç”¨ 110 ç§äººç±»æ„ŸæŸ“ç—…æ¯’çš„åºåˆ—å¯¹å…ˆè¿›æ¨¡å‹ Evo 2 è¿›è¡Œå¾®è°ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨æœªè§è¿‡çš„ç—…æ¯’åºåˆ—ä¸Šå±•ç°å‡ºæ›´ä½çš„å›°æƒ‘åº¦ (perplexity)ï¼Œå¹¶æˆåŠŸè¯†åˆ«å‡º SARS-CoV-2 çš„å…ç–«é€ƒé€¸å˜ä½“ã€‚è¿™è¯æ˜äº†å¾®è°ƒå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šè§„é¿æ•°æ®è¿‡æ»¤ç­–ç•¥ï¼Œä»è€Œæ¢å¤ä¸æ»¥ç”¨ç›¸å…³çš„å…³é”®é¢„æµ‹èƒ½åŠ›ã€‚è¯¥å‘ç°æ­ç¤ºäº†ç°æœ‰ gLMs é˜²å¾¡æœºåˆ¶çš„è„†å¼±æ€§ï¼Œå‡¸æ˜¾äº†æ„å»ºæ›´å®Œå–„å®‰å…¨è¯„ä¼°æ¡†æ¶å’Œç¼“è§£æªæ–½ä»¥ç¡®ä¿æ¨¡å‹å®‰å…¨éƒ¨ç½²çš„è¿«åˆ‡éœ€æ±‚ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Biosecurity Safeguards for Generative AI",
      "pdf_url": "https://arxiv.org/pdf/2511.19299v1",
      "published_date": "2025-11-24 16:46:44 UTC",
      "updated_date": "2025-11-24 16:46:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:00:06.639700+00:00"
    },
    {
      "arxiv_id": "2511.19283v1",
      "title": "Data Flows and Colonial Regimes in Africa: A Critical Analysis of the Colonial Futurities Embedded in AI Ecosystems",
      "title_zh": "Africa çš„æ•°æ®æµä¸æ®–æ°‘ä½“åˆ¶ï¼šäººå·¥æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿä¸­æ®–æ°‘æœªæ¥æ€§çš„æ‰¹åˆ¤æ€§åˆ†æ",
      "authors": [
        "Ndaka. A",
        "Avila-Acosta. F",
        "Mbula-Ndaka. H",
        "Amera. C",
        "Chauke. S",
        "Majiwa. E"
      ],
      "abstract": "This chapter seeks to frame the elemental and invisible problems of AI and big data in the African context by examining digital sites and infrastructure through the lens of power and interests. It will present reflections on how these sites are using AI recommendation algorithms to recreate new digital societies in the region, how they have the potential to propagate algorithmic colonialism and negative gender norms, and what this means for the regional sustainable development agenda. The chapter proposes adopting business models that embrace response-ability and consider the existence of alternative socio-material worlds of AI. These reflections will mainly come from ongoing discussions with Kenyan social media users in this authors' user space talks, personal experiences and six months of active participant observations done by the authors.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡æƒåŠ›å’Œåˆ©ç›Šçš„è§†è§’ï¼Œæ‰¹åˆ¤æ€§åœ°åˆ†æäº†éæ´² AI å’Œå¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿä¸­åµŒå…¥çš„æ®–æ°‘æœªæ¥æ€§(Colonial Futurities)åŠå…¶å¸¦æ¥çš„éšè”½é—®é¢˜ã€‚ä½œè€…ç»“åˆå¯¹è‚¯å°¼äºšç¤¾äº¤åª’ä½“ç”¨æˆ·çš„è®¿è°ˆã€ä¸ªäººç»éªŒä»¥åŠä¸ºæœŸå…­ä¸ªæœˆçš„å‚ä¸å¼è§‚å¯Ÿ(Participant Observations)ï¼Œæ·±å…¥æ¢è®¨äº†æ•°å­—åŸºç¡€è®¾æ–½å¦‚ä½•åˆ©ç”¨æ¨èç®—æ³•(Recommendation Algorithms)é‡æ„å½“åœ°æ•°å­—ç¤¾ä¼šã€‚ç ”ç©¶é‡ç‚¹æ­ç¤ºäº†è¿™äº›æŠ€æœ¯å¹³å°åœ¨ä¼ æ’­ç®—æ³•æ®–æ°‘ä¸»ä¹‰(Algorithmic Colonialism)å’Œè´Ÿé¢æ€§åˆ«è§„èŒƒæ–¹é¢çš„æ½œåœ¨é£é™©ï¼Œå¹¶è¯„ä¼°äº†å…¶å¯¹åŒºåŸŸå¯æŒç»­å‘å±•è®®ç¨‹(Sustainable Development Agenda)çš„è´Ÿé¢å½±å“ã€‚æ–‡ç« æå€¡é‡‡ç”¨ä¸€ç§å…·å¤‡å“åº”èƒ½åŠ›(Response-ability)çš„å•†ä¸šæ¨¡å¼ï¼Œä»¥å……åˆ†è€ƒè™‘ AI ç³»ç»Ÿä¸­å­˜åœ¨çš„æ›¿ä»£æ€§ç¤¾ä¼šç‰©è´¨ä¸–ç•Œ(Socio-material Worlds)ã€‚è¯¥å·¥ä½œä¸ºç†è§£éæ´²æ•°å­—ä¸»æƒé¢ä¸´çš„æŒ‘æˆ˜æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶ä¸ºæ„å»ºæ›´å…¬å¹³çš„æŠ€æœ¯ç”Ÿæ€ç³»ç»Ÿæä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.19283v1",
      "published_date": "2025-11-24 16:31:50 UTC",
      "updated_date": "2025-11-24 16:31:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:00:40.929125+00:00"
    },
    {
      "arxiv_id": "2511.19275v1",
      "title": "Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization",
      "title_zh": "ç»“åˆå£°å­¦æ¨¡å¼åŒ–ä¸ 3D ç©ºé—´åŒ–çš„åŠ¨æ€å¤šç‰©ç§é¸Ÿç±»å£°æ™¯ç”Ÿæˆ",
      "authors": [
        "Ellie L. Zhang",
        "Duoduo Liao",
        "Callie C. Liao"
      ],
      "abstract": "Generation of dynamic, scalable multi-species bird soundscapes remains a significant challenge in computer music and algorithmic sound design. Birdsongs involve rapid frequency-modulated chirps, complex amplitude envelopes, distinctive acoustic patterns, overlapping calls, and dynamic inter-bird interactions, all of which require precise temporal and spatial control in 3D environments. Existing approaches, whether Digital Signal Processing (DSP)-based or data-driven, typically focus only on single species modeling, static call structures, or synthesis directly from recordings, and often suffer from noise, limited flexibility, or large data needs. To address these challenges, we present a novel, fully algorithm-driven framework that generates dynamic multi-species bird soundscapes using DSP-based chirp generation and 3D spatialization, without relying on recordings or training data. Our approach simulates multiple independently-moving birds per species along different moving 3D trajectories, supporting controllable chirp sequences, overlapping choruses, and realistic 3D motion in scalable soundscapes while preserving species-specific acoustic patterns. A visualization interface provides bird trajectories, spectrograms, activity timelines, and sound waves for analytical and creative purposes. Both visual and audio evaluations demonstrate the ability of the system to generate dense, immersive, and ecologically inspired soundscapes, highlighting its potential for computer music, interactive virtual environments, and computational bioacoustics research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå®Œå…¨ç”±ç®—æ³•é©±åŠ¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç”ŸæˆåŠ¨æ€ã€å¯æ‰©å±•çš„å¤šç‰©ç§é¸Ÿç±»å£°æ™¯ (Bird Soundscape)ï¼Œè§£å†³äº†ç°æœ‰æŠ€æœ¯åœ¨å•ç‰©ç§å»ºæ¨¡å’Œçµæ´»æ€§æ–¹é¢çš„å±€é™ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŸºäºæ•°å­—ä¿¡å·å¤„ç† (DSP) çš„é¸£å«ç”ŸæˆæŠ€æœ¯ä¸ 3D ç©ºé—´åŒ– (3D Spatialization) æŠ€æœ¯ï¼Œæ— éœ€ä¾èµ–å½•éŸ³æˆ–è®­ç»ƒæ•°æ®å³å¯æ¨¡æ‹Ÿå¤šç§é¸Ÿç±»åœ¨ä¸‰ç»´è½¨è¿¹ä¸Šçš„ç‹¬ç«‹ç§»åŠ¨ã€‚ç³»ç»Ÿæ”¯æŒå¯æ§çš„é¸£å«åºåˆ—ã€é‡å çš„åˆå”±ä»¥åŠçœŸå®çš„ 3D è¿åŠ¨ï¼ŒåŒæ—¶ä¿ç•™äº†ç‰¹å®šç‰©ç§çš„å£°å­¦æ¨¡å¼ (Acoustic Patterning)ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜é…å¤‡äº†å¯è§†åŒ–ç•Œé¢ï¼Œèƒ½å¤Ÿå±•ç¤ºé¸Ÿç±»è½¨è¿¹ã€å£°è°±å›¾ (Spectrograms) å’Œæ´»åŠ¨æ—¶é—´çº¿ï¼Œä¸ºåˆ†æå’Œåˆ›ä½œæä¾›æ”¯æŒã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆæ²‰æµ¸å¼ä¸”å…·æœ‰ç”Ÿæ€å¯å‘æ€§çš„å£°æ™¯ï¼Œåœ¨è®¡ç®—æœºéŸ³ä¹ã€äº¤äº’å¼è™šæ‹Ÿç¯å¢ƒå’Œè®¡ç®—ç”Ÿç‰©å£°å­¦ (Computational Bioacoustics) ç ”ç©¶é¢†åŸŸå…·æœ‰æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by IEEE Big Data 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.19275v1",
      "published_date": "2025-11-24 16:25:55 UTC",
      "updated_date": "2025-11-24 16:25:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:00:30.733592+00:00"
    },
    {
      "arxiv_id": "2511.20701v1",
      "title": "Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework",
      "title_zh": "å°†ä¸åŒæ•°æ®é›†æ•´åˆè‡³ Amazon CoT æ¡†æ¶çš„å¤šæ¨¡æ€é“¾å¼æ€ç»´æ¨ç†è·¨é¢†åŸŸè¯„ä¼°",
      "authors": [
        "Nitya Tiwari",
        "Parv Maheshwari",
        "Vidisha Agarwal"
      ],
      "abstract": "While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€é“¾å¼æ€ç»´æ¨ç†(Multimodal-CoT)åœ¨ä¸åŒé¢†åŸŸé—´çš„æ³›åŒ–èƒ½åŠ›å±•å¼€äº†æ·±å…¥è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯å°†å…¶åº”ç”¨äºéœ€è¦å¹¿æ³›å¸¸è¯†å’Œä¸–ç•ŒçŸ¥è¯†çš„A-OKVQAã€OKVQAå’ŒChartQAæ•°æ®é›†ã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå°†åŸç†ç”Ÿæˆ(rationale generation)ä¸ç­”æ¡ˆæ¨ç†(answer inference)ç›¸åˆ†ç¦»ï¼Œå¹¶åˆ©ç”¨åŸºäºT5çš„è¯­è¨€æ¨¡å‹é€šè¿‡é—¨æ§èåˆæœºåˆ¶(gated fusion mechanism)æ•´åˆè§†è§‰ç‰¹å¾ã€‚é€šè¿‡ç³»ç»Ÿçš„æ¶ˆèç ”ç©¶(ablation studies)ï¼Œä½œè€…è¯¦ç»†åˆ†æäº†è§†è§‰ç‰¹å¾ã€åŸç†è´¨é‡åŠæ¶æ„é€‰æ‹©å¯¹æ¨ç†æ€§èƒ½çš„å…·ä½“è´¡çŒ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§†è§‰ç‰¹å¾çš„æ•´åˆèƒ½æ˜¾è‘—å‡å°‘åŸç†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¹»è§‰(hallucination)ç°è±¡ï¼Œä½†Multimodal-CoTæ¨ç†çš„æœ‰æ•ˆæ€§åœ¨ä¸åŒé—®é¢˜ç±»å‹é—´è¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶å‘ç°å¸¸è¯†æ¨ç†(commonsense reasoning)åœ¨è·¨é¢†åŸŸåœºæ™¯ä¸‹ä»é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ï¼Œè¯¥å·¥ä½œä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæä¾›äº†å®ç”¨çš„è§è§£ä¸æ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20701v1",
      "published_date": "2025-11-24 16:20:02 UTC",
      "updated_date": "2025-11-24 16:20:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:00:33.232168+00:00"
    },
    {
      "arxiv_id": "2511.19264v1",
      "title": "Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry",
      "title_zh": "è¯ç‰©å‘ç°ä¸­çš„ GFlowNets è§£è¯»ï¼šæå–è¯ç‰©åŒ–å­¦çš„æŒ‡å¯¼æ€§è§è§£",
      "authors": [
        "Amirtha Varshini A S",
        "Duminda S. Ranasinghe",
        "Hok Hei Tam"
      ],
      "abstract": "Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆæµç½‘ç»œ(GFlowNets)åœ¨è¯ç‰©å‘ç°ä¸­å†…éƒ¨å†³ç­–æœºåˆ¶ä¸é€æ˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªä¸“é—¨é¢å‘SynFlowNetçš„å¯è§£é‡Šæ€§æ¡†æ¶ã€‚SynFlowNetæ˜¯ä¸€æ¬¾åŸºäºå·²çŸ¥åŒ–å­¦ååº”å’Œèµ·å§‹åŸæ–™ç”Ÿæˆåˆ†å­åŠå…¶åˆæˆè·¯å¾„çš„æ¨¡å‹ï¼Œæœ¬ç ”ç©¶é€šè¿‡ä¸‰ç§äº’è¡¥ç»„ä»¶è§£æäº†å…¶å†…éƒ¨é€»è¾‘ã€‚é¦–å…ˆï¼Œç»“åˆåŸºäºæ¢¯åº¦çš„æ˜¾è‘—æ€§(Gradient based saliency)åˆ†æå’Œåäº‹å®æ‰°åŠ¨(counterfactual perturbations)ï¼Œè¯†åˆ«äº†å½±å“å¥–åŠ±çš„å…³é”®åŸå­ç¯å¢ƒä»¥åŠç»“æ„ä¿®æ”¹å¯¹åˆ†å­çš„å½±å“ã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨(Sparse autoencoders)æ­ç¤ºäº†ä¸ææ€§ã€äº²è„‚æ€§å’Œåˆ†å­å¤§å°ç­‰ç‰©ç†åŒ–å­¦æ€§è´¨å¯¹åº”çš„æ½œåœ¨å› å­ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŸºåºæ¢é’ˆ(Motif probes)è¯æ˜äº†èŠ³é¦™ç¯å’Œå¤ç´ ç­‰åŠŸèƒ½å›¢åœ¨æ¨¡å‹å†…éƒ¨åµŒå…¥ä¸­å·²è¢«æ˜¾å¼ç¼–ç å¹¶å¯çº¿æ€§è§£ç ã€‚è¿™äº›å‘ç°å…±åŒæ­ç¤ºäº†SynFlowNetå†…éƒ¨çš„åŒ–å­¦é€»è¾‘ï¼Œä¸ºè¯ç‰©åŒ–å­¦é¢†åŸŸæä¾›äº†å¯æ“ä½œä¸”å…·æœºåˆ¶æ€§çš„è§è§£ï¼Œä»è€Œæ”¯æŒé€æ˜ä¸”å¯æ§çš„åˆ†å­è®¾è®¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 7 figures. Accepted for presentation at NeurIPS 2025 WiML Workshop and Molecular Machine Learning Conference (MoML) 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.19264v1",
      "published_date": "2025-11-24 16:16:18 UTC",
      "updated_date": "2025-11-24 16:16:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:00:40.583473+00:00"
    },
    {
      "arxiv_id": "2511.19263v1",
      "title": "Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention",
      "title_zh": "Solar-GECOï¼šåŸºäºå‡ ä½•æ„ŸçŸ¥ååŒæ³¨æ„åŠ›çš„é’™é’›çŸ¿å¤ªé˜³èƒ½ç”µæ± æ€§èƒ½é¢„æµ‹",
      "authors": [
        "Lucas Li",
        "Jean-Baptiste Puel",
        "Florence Carton",
        "Dounya Barrit",
        "Jhony H. Giraldo"
      ],
      "abstract": "Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é’™é’›çŸ¿å¤ªé˜³èƒ½ç”µæ±  (Perovskite solar cells) å› å¤æ‚çš„å¤šå°ºåº¦å±‚é—´ç›¸äº’ä½œç”¨å¯¼è‡´ä¼ ç»Ÿå®éªŒç­›é€‰æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†å…·å¤‡å‡ ä½•æ„ŸçŸ¥ååŒæ³¨æ„åŠ›æœºåˆ¶çš„ Solar-GECO æ¨¡å‹ï¼Œç”¨äºå‡†ç¡®é¢„æµ‹å…‰ç”µè½¬æ¢æ•ˆç‡ (Power conversion efficiency, PCE)ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å‡ ä½•å›¾ç¥ç»ç½‘ç»œ (Geometric GNN) ä»¥ç›´æ¥ç¼–ç é’™é’›çŸ¿å¸æ”¶å±‚çš„åŸå­ç»“æ„ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è¯­è¨€æ¨¡å‹åµŒå…¥ (Language model embeddings) å¤„ç†ä¼ è¾“å±‚åŠå…¶ä»–ç»„ä»¶çš„åŒ–å­¦æˆåˆ†æ–‡æœ¬ã€‚æ­¤å¤–ï¼Œæ¨¡å‹é€šè¿‡ååŒæ³¨æ„åŠ› (Co-attention) æ¨¡å—æ•æ‰å±‚å†…ä¾èµ–ä¸å±‚é—´ç›¸äº’ä½œç”¨ï¼Œå¹¶é‡‡ç”¨æ¦‚ç‡å›å½’å¤´ (Probabilistic regression head) åŒæ—¶é¢„æµ‹ PCE åŠå…¶å…³è”çš„ä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSolar-GECO è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œå°† PCE é¢„æµ‹çš„å¹³å‡ç»å¯¹è¯¯å·® (MAE) ä»æ­¤å‰æœ€ä¼˜æ¨¡å‹çš„ 3.066 é™ä½è‡³ 2.936ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ•´åˆå‡ ä½•ä¿¡æ¯ä¸æ–‡æœ¬ä¿¡æ¯èƒ½ä¸ºé’™é’›çŸ¿å¤ªé˜³èƒ½ç”µæ± çš„æ€§èƒ½é¢„æµ‹æä¾›æ›´å¼ºå¤§çš„åˆ†ææ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the AI for Accelerated Materials Design (AI4Mat) Workshop at NeurIPS 2025. 14 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.19263v1",
      "published_date": "2025-11-24 16:15:41 UTC",
      "updated_date": "2025-11-24 16:15:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:00:39.940137+00:00"
    },
    {
      "arxiv_id": "2511.19262v1",
      "title": "Psychometric Tests for AI Agents and Their Moduli Space",
      "title_zh": "AIæ™ºèƒ½ä½“å¿ƒç†æµ‹è¯„åŠå…¶æ¨¡ç©ºé—´",
      "authors": [
        "Przemyslaw Chojecki"
      ],
      "abstract": "We develop a moduli-theoretic view of psychometric test batteries for AI agents and connect it explicitly to the AAI score developed previously. First, we make precise the notion of an AAI functional on a battery and set out axioms that any reasonable autonomy/general intelligence score should satisfy. Second, we show that the composite index ('AAI-Index') defined previously is a special case of our AAI functional. Third, we introduce the notion of a cognitive core of an agent relative to a battery and define the associated AAI$_{\\textrm{core}}$ score as the restriction of an AAI functional to that core. Finally, we use these notions to describe invariants of batteries under evaluation-preserving symmetries and outline how moduli of equivalent batteries are organized.",
      "tldr_zh": "è¯¥ç ”ç©¶ä¸ºAI Agentsçš„Psychometric Testså»ºç«‹äº†ä¸€ç§Moduli-theoreticè§‚ç‚¹ï¼Œå¹¶å°†å…¶ä¸å…ˆå‰å¼€å‘çš„AAI scoreå»ºç«‹äº†æ˜ç¡®è”ç³»ã€‚ç ”ç©¶è€…ç²¾ç¡®å®šä¹‰äº†æµ‹è¯•é›†ä¸Šçš„AAI functionalæ¦‚å¿µï¼Œå¹¶æå‡ºäº†ä¸€å¥—è¡¡é‡Autonomyæˆ–General Intelligenceåˆ†æ•°åº”æ»¡è¶³çš„å…¬ç†ç³»ç»Ÿã€‚æ–‡ä¸­è¯æ˜äº†æ­¤å‰å®šä¹‰çš„ç»¼åˆæŒ‡æ•°AAI-Indexæ˜¯è¯¥AAI functionalçš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå¹¶è¿›ä¸€æ­¥æå‡ºäº†ç›¸å¯¹äºæµ‹è¯•é›†çš„Agentçš„Cognitive coreæ¦‚å¿µä»¥åŠç›¸å…³çš„AAI_core scoreã€‚æœ€åï¼Œç ”ç©¶åˆ©ç”¨è¿™äº›æ¦‚å¿µæè¿°äº†åœ¨Evaluation-preserving symmetriesä¸‹çš„æµ‹è¯•é›†ä¸å˜é‡ï¼Œå¹¶é˜è¿°äº†ç­‰æ•ˆæµ‹è¯•é›†çš„Moduliç©ºé—´æ˜¯å¦‚ä½•ç»„ç»‡çš„ã€‚è¯¥å·¥ä½œä¸ºé‡åŒ–å’Œè¯„ä¼°äººå·¥æ™ºèƒ½çš„è‡ªä¸»æ€§ä¸æ™ºèƒ½æ°´å¹³æä¾›äº†ä¸¥è°¨çš„æ•°å­¦ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19262v1",
      "published_date": "2025-11-24 16:15:08 UTC",
      "updated_date": "2025-11-24 16:15:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:00:49.243103+00:00"
    },
    {
      "arxiv_id": "2511.19260v2",
      "title": "Wrist Photoplethysmography Predicts Dietary Information",
      "title_zh": "è…•éƒ¨å…‰ç”µå®¹ç§¯è„‰ææ³¢é¢„æµ‹é¥®é£Ÿä¿¡æ¯",
      "authors": [
        "Kyle Verrier",
        "Achille Nazaret",
        "Joseph Futoma",
        "Andrew C. Miller",
        "Guillermo Sapiro"
      ],
      "abstract": "Whether wearable photoplethysmography (PPG) contains dietary information remains unknown. We trained a language model on 1.1M meals to predict meal descriptions from PPG, aligning PPG to text. PPG nontrivially predicts meal content; predictability decreases for PPGs farther from meals. This transfers to dietary tasks: PPG increases AUC by 11% for intake and satiety across held-out and independent cohorts, with gains robust to text degradation. Wearable PPG may enable passive dietary monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä½©æˆ´å¼å…‰ç”µå®¹ç§¯è„‰ææ³¢(PPG)æ˜¯å¦åŒ…å«é¥®é£Ÿä¿¡æ¯ã€‚ç ”ç©¶äººå‘˜åœ¨110ä¸‡ä»½é¤é£Ÿæ•°æ®ä¸Šè®­ç»ƒäº†ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å°†PPGä¿¡å·ä¸æ–‡æœ¬å¯¹é½æ¥é¢„æµ‹é¤é£Ÿæè¿°ã€‚å®éªŒå‘ç°PPGèƒ½å¤Ÿæ˜¾è‘—åœ°é¢„æµ‹é¥®é£Ÿå†…å®¹ï¼Œä¸”é¢„æµ‹èƒ½åŠ›éšç¦»é¤æ—¶é—´çš„å¢åŠ è€Œå‡å¼±ã€‚åœ¨é¥®é£Ÿç›‘æµ‹ä»»åŠ¡ä¸­ï¼Œå¼•å…¥PPGæ•°æ®ä½¿æ‘„å…¥é‡(intake)å’Œé¥±è…¹æ„Ÿ(satiety)é¢„æµ‹çš„æ›²çº¿ä¸‹é¢ç§¯(AUC)åœ¨ç‹¬ç«‹é˜Ÿåˆ—ä¸­æå‡äº†11%ï¼Œä¸”è¯¥å¢ç›Šåœ¨æ–‡æœ¬ä¿¡æ¯é€€åŒ–æ—¶è¡¨ç°ç¨³å¥ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œä½©æˆ´å¼PPGæŠ€æœ¯æœ‰æœ›å®ç°è‡ªåŠ¨åŒ–çš„è¢«åŠ¨é¥®é£Ÿç›‘æµ‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.19260v2",
      "published_date": "2025-11-24 16:12:03 UTC",
      "updated_date": "2025-12-18 18:27:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:00:49.932376+00:00"
    },
    {
      "arxiv_id": "2511.19257v1",
      "title": "Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation",
      "title_zh": "Medusaï¼šé’ˆå¯¹å¤šæ¨¡æ€åŒ»å­¦æ£€ç´¢å¢å¼ºç”Ÿæˆçš„è·¨æ¨¡æ€å¯è¿ç§»å¯¹æŠ—æ”»å‡»",
      "authors": [
        "Yingjia Shang",
        "Yi Liu",
        "Huimin Wang",
        "Furong Li",
        "Wenfang Sun",
        "Wu Chengyu",
        "Yefeng Zheng"
      ],
      "abstract": "With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Medusa æ¡†æ¶ï¼Œæ—¨åœ¨é’ˆå¯¹é»‘ç›’è®¾ç½®ä¸‹çš„å¤šæ¨¡æ€åŒ»ç–—æ£€ç´¢å¢å¼ºç”Ÿæˆ (MMed-RAG) ç³»ç»Ÿå®æ–½è·¨æ¨¡æ€å¯è¿ç§»å¯¹æŠ—æ”»å‡» (Cross-Modal Transferable Adversarial Attacks)ã€‚Medusa å°†æ”»å‡»å…¬å¼åŒ–ä¸ºæ‰°åŠ¨ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤šæ­£æ ·æœ¬ InfoNCE æŸå¤± (MPIL) ä½¿å¯¹æŠ—æ€§è§†è§‰åµŒå…¥ä¸è¯¯å¯¼æ€§çš„æ¶æ„æ–‡æœ¬ç›®æ ‡å¯¹é½ï¼Œä»è€Œå®ç°å¯¹æ£€ç´¢è¿‡ç¨‹çš„åŠ«æŒã€‚ä¸ºäº†æå‡æ”»å‡»çš„å¯è¿ç§»æ€§ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†æ›¿ä»£æ¨¡å‹é›†æˆæ–¹æ³•ï¼Œå¹¶è®¾è®¡äº†ç»“åˆä¸å˜é£é™©æœ€å°åŒ– (IRM) çš„åŒå¾ªç¯ä¼˜åŒ–ç­–ç•¥ã€‚åœ¨åŒ»ç–—æŠ¥å‘Šç”Ÿæˆå’Œç–¾ç—…è¯Šæ–­ä¸¤é¡¹çœŸå®åŒ»ç–—ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜ï¼ŒMedusa åœ¨å¤šç§ç”Ÿæˆæ¨¡å‹å’Œæ£€ç´¢å™¨é…ç½®ä¸‹å‡å®ç°äº†è¶…è¿‡ 90% çš„å¹³å‡æ”»å‡»æˆåŠŸç‡ï¼Œä¸”åœ¨å¯¹æŠ—å››ç§ä¸»æµé˜²å¾¡æ‰‹æ®µæ—¶è¡¨ç°å‡ºä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹çš„é²æ£’æ€§ã€‚è¯¥é¡¹å·¥ä½œæ­ç¤ºäº† MMed-RAG ç³»ç»Ÿä¸­å­˜åœ¨çš„å…³é”®å®‰å…¨æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†åœ¨åŒ»ç–—ç­‰å®‰å…¨æ•æ„Ÿåº”ç”¨ä¸­å»ºç«‹é²æ£’æ€§åŸºå‡†æµ‹è¯•çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at KDD 2026 First Cycle (full version). Authors marked with * contributed equally. Yi Liu is the lead author",
      "pdf_url": "https://arxiv.org/pdf/2511.19257v1",
      "published_date": "2025-11-24 16:11:01 UTC",
      "updated_date": "2025-11-24 16:11:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:01:01.437992+00:00"
    },
    {
      "arxiv_id": "2511.19256v1",
      "title": "SimDiff: Simpler Yet Better Diffusion Model for Time Series Point Forecasting",
      "title_zh": "SimDiffï¼šç”¨äºæ—¶é—´åºåˆ—ç‚¹é¢„æµ‹çš„æ›´ç®€æ›´ä¼˜æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Hang Ding",
        "Xue Wang",
        "Tian Zhou",
        "Tao Yao"
      ],
      "abstract": "Diffusion models have recently shown promise in time series forecasting, particularly for probabilistic predictions. However, they often fail to achieve state-of-the-art point estimation performance compared to regression-based methods. This limitation stems from difficulties in providing sufficient contextual bias to track distribution shifts and in balancing output diversity with the stability and precision required for point forecasts. Existing diffusion-based approaches mainly focus on full-distribution modeling under probabilistic frameworks, often with likelihood maximization objectives, while paying little attention to dedicated strategies for high-accuracy point estimation. Moreover, other existing point prediction diffusion methods frequently rely on pre-trained or jointly trained mature models for contextual bias, sacrificing the generative flexibility of diffusion models.\n  To address these challenges, we propose SimDiff, a single-stage, end-to-end framework. SimDiff employs a single unified Transformer network carefully tailored to serve as both denoiser and predictor, eliminating the need for external pre-trained or jointly trained regressors. It achieves state-of-the-art point estimation performance by leveraging intrinsic output diversity and improving mean squared error accuracy through multiple inference ensembling. Key innovations, including normalization independence and the median-of-means estimator, further enhance adaptability and stability. Extensive experiments demonstrate that SimDiff significantly outperforms existing methods in time series point forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SimDiffï¼Œä¸€ç§é’ˆå¯¹æ—¶é—´åºåˆ—ç‚¹é¢„æµ‹(Point Forecasting)è®¾è®¡çš„å•é˜¶æ®µã€ç«¯åˆ°ç«¯ Diffusion Model æ¡†æ¶ã€‚ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹åœ¨å¤„ç†åˆ†å¸ƒåç§»ä»¥åŠå¹³è¡¡è¾“å‡ºå¤šæ ·æ€§ä¸ç‚¹é¢„æµ‹ç¨³å®šæ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œä¸”å¾€å¾€ä¾èµ–å¤æ‚çš„å¤–éƒ¨é¢„è®­ç»ƒæ¨¡å‹æä¾›ä¸Šä¸‹æ–‡åç½®ã€‚SimDiff é‡‡ç”¨ç»Ÿä¸€çš„ Transformer ç½‘ç»œåŒæ—¶å……å½“å»å™ªå™¨(Denoiser)å’Œé¢„æµ‹å™¨(Predictor)ï¼Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨å›å½’å™¨çš„ä¾èµ–å¹¶æå‡äº†ç”Ÿæˆçµæ´»æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å½’ä¸€åŒ–ç‹¬ç«‹æ€§(Normalization Independence)å’Œå‡å€¼ä¸­ä½æ•°ä¼°è®¡å™¨(Median-of-Means Estimator)ç­‰åˆ›æ–°æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„é€‚åº”æ€§å’Œç¨³å®šæ€§ã€‚é€šè¿‡åˆ©ç”¨å›ºæœ‰çš„è¾“å‡ºå¤šæ ·æ€§å¹¶ç»“åˆå¤šæ¬¡æ¨ç†é›†æˆ(Inference Ensembling)ï¼ŒSimDiff æœ‰æ•ˆé™ä½äº†å‡æ–¹è¯¯å·®(MSE)å¹¶æå‡äº†é¢„æµ‹ç²¾åº¦ã€‚å®éªŒè¯æ˜ï¼ŒSimDiff åœ¨å¤šé¡¹æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›(State-of-the-Art)çš„ç‚¹é¢„æµ‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.19256v1",
      "published_date": "2025-11-24 16:09:55 UTC",
      "updated_date": "2025-11-24 16:09:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-26T10:02:05.122039+00:00"
    },
    {
      "arxiv_id": "2511.19254v1",
      "title": "Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation",
      "title_zh": "åŸºäºå¯å¾®3Dä»¿çœŸçš„è§†è§‰è´§ç‰©å ç”¨ç‡ä¼°ç®—å¯¹æŠ—è¡¥ä¸æ”»å‡»",
      "authors": [
        "Mohamed Rissal Hedna",
        "Sesugh Samuel Nder"
      ],
      "abstract": "Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç‰©æµé¢†åŸŸåŸºäºè®¡ç®—æœºè§†è§‰çš„è´§ç‰©å ç”¨ç‡ä¼°è®¡ç³»ç»Ÿï¼ˆCargo Occupancy Estimationï¼‰åœ¨é¢å¯¹ç‰©ç†å¯¹æŠ—è¡¥ä¸æ”»å‡»ï¼ˆAdversarial Patch Attacksï¼‰æ—¶çš„è„†å¼±æ€§ã€‚ä½œè€…åˆ©ç”¨ Mitsuba 3 çš„å¯å¾®åˆ†æ¸²æŸ“ï¼ˆDifferentiable Renderingï¼‰æŠ€æœ¯ï¼Œåœ¨å®Œå…¨æ¨¡æ‹Ÿçš„ 3D ç¯å¢ƒä¸­ä¼˜åŒ–è¡¥ä¸çº¹ç†ï¼Œä»¥åº”å¯¹å‡ ä½•ã€ç…§æ˜å’Œè§†ç‚¹çš„å¤šæ ·åŒ–å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ 3D ä¼˜åŒ–æ–¹æ³•åœ¨æ‹’ç»æœåŠ¡æ”»å‡»ï¼ˆDenial-of-Serviceï¼‰åœºæ™¯ä¸‹è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œå°†â€œç©ºèˆ±â€è¯¯å¯¼ä¸ºâ€œæ»¡è½½â€çš„æˆåŠŸç‡è¾¾åˆ°äº† 84.94%ï¼Œè€Œå°†â€œæ»¡è½½â€è¯¯å¯¼ä¸ºâ€œç©ºèˆ±â€çš„éšè—æ”»å‡»æˆåŠŸç‡ä¸º 30.32%ã€‚ç ”ç©¶é€šè¿‡åˆ†æå½±å“æ”»å‡»æˆåŠŸçš„å› ç´ ï¼Œå¯¹æ¯”äº† 3D ä¼˜åŒ–è¡¥ä¸ä¸ 2D åˆæˆåŸºå‡†çš„æœ‰æ•ˆæ€§ã€‚ä½œä¸ºé¦–ä¸ªåœ¨ç‰©ç†çœŸå® 3D æ¨¡æ‹Ÿä¸­ç ”ç©¶æ­¤ç±»æ”»å‡»çš„å·¥ä½œï¼Œè¯¥è®ºæ–‡æ­ç¤ºäº†è‡ªåŠ¨åŒ–ç‰©æµæµæ°´çº¿çš„å®‰å…¨éšæ‚£ã€‚å…¶ç ”ç©¶å‘ç°å¼ºè°ƒäº†æå‡è§†è§‰æ¨¡å‹ç‰©ç†é²æ£’æ€§ï¼ˆPhysical Robustnessï¼‰çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥æ„å»ºæ›´å®‰å…¨çš„ç‰©æµç›‘æ§ç³»ç»Ÿæä¾›äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 5 figures, 1 algorithm",
      "pdf_url": "https://arxiv.org/pdf/2511.19254v1",
      "published_date": "2025-11-24 16:05:40 UTC",
      "updated_date": "2025-11-24 16:05:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:01:01.838510+00:00"
    },
    {
      "arxiv_id": "2511.19253v2",
      "title": "MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization",
      "title_zh": "MAESTROï¼šåŸºäºä»»åŠ¡ä¸å¥–åŠ±ä¼˜åŒ–çš„å¤šæ™ºèƒ½ä½“ç¯å¢ƒå¡‘é€ ",
      "authors": [
        "Boyuan Wu"
      ],
      "abstract": "Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MAESTROæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ååŒå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (Cooperative MARL)åœ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°å’Œæ„å»ºè¯¾ç¨‹æ—¶é¢ä¸´çš„ç“¶é¢ˆã€‚MAESTROå°†å¤§è¯­è¨€æ¨¡å‹(LLMs)å®šä½ä¸ºç¦»çº¿è®­ç»ƒæ¶æ„å¸ˆè€Œéå®æ—¶æ§åˆ¶å™¨ï¼Œä»è€Œåœ¨ä¸å¢åŠ éƒ¨ç½²æ¨ç†æˆæœ¬çš„å‰æä¸‹ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è¯­ä¹‰è¯¾ç¨‹ç”Ÿæˆå™¨å’Œè‡ªåŠ¨å¥–åŠ±åˆæˆå™¨ï¼Œå‰è€…è´Ÿè´£åˆ›å»ºå¤šæ ·åŒ–çš„ä»»åŠ¡åœºæ™¯ï¼Œåè€…åˆ™ç”Ÿæˆé€‚é…è¯¾ç¨‹éš¾åº¦çš„Pythonå¥–åŠ±å‡½æ•°ã€‚è¿™äº›ç»„ä»¶å…±åŒå¼•å¯¼æ ‡å‡†çš„MADDPGç®—æ³•ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒçš„æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚åœ¨åŒ…å«16ä¸ªè·¯å£çš„äº¤é€šä¿¡å·æ§åˆ¶å¤§è§„æ¨¡å®éªŒä¸­ï¼ŒMAESTROç›¸æ¯”å¼ºåŸºçº¿æ¨¡å‹å®ç°äº†4.0%çš„å¹³å‡å›æŠ¥æå‡å’Œ2.2%çš„é£é™©è°ƒæ•´æ€§èƒ½ä¼˜åŒ–ã€‚è¯¥ç ”ç©¶è¯æ˜äº†LLMsä½œä¸ºMARLé«˜å±‚è®¾è®¡è€…çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤æ‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è‡ªåŠ¨åŒ–è®­ç»ƒæä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint. 16 pages, 6 figures. Preliminary version; extended experiments and analysis forthcoming",
      "pdf_url": "https://arxiv.org/pdf/2511.19253v2",
      "published_date": "2025-11-24 16:05:37 UTC",
      "updated_date": "2025-12-10 00:13:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:02:18.536679+00:00"
    },
    {
      "arxiv_id": "2511.19246v1",
      "title": "Neural Architecture Search for Quantum Autoencoders",
      "title_zh": "é‡å­è‡ªç¼–ç å™¨çš„ç¥ç»æ¶æ„æœç´¢",
      "authors": [
        "Hibah Agha",
        "Samuel Yen-Chi Chen",
        "Huan-Hsin Tseng",
        "Shinjae Yoo"
      ],
      "abstract": "In recent years, machine learning and deep learning have driven advances in domains such as image classification, speech recognition, and anomaly detection by leveraging multi-layer neural networks to model complex data. Simultaneously, quantum computing (QC) promises to address classically intractable problems via quantum parallelism, motivating research in quantum machine learning (QML). Among QML techniques, quantum autoencoders show promise for compressing high-dimensional quantum and classical data. However, designing effective quantum circuit architectures for quantum autoencoders remains challenging due to the complexity of selecting gates, arranging circuit layers, and tuning parameters.\n  This paper proposes a neural architecture search (NAS) framework that automates the design of quantum autoencoders using a genetic algorithm (GA). By systematically evolving variational quantum circuit (VQC) configurations, our method seeks to identify high-performing hybrid quantum-classical autoencoders for data reconstruction without becoming trapped in local minima. We demonstrate effectiveness on image datasets, highlighting the potential of quantum autoencoders for efficient feature extraction within a noise-prone, near-term quantum era. Our approach lays a foundation for broader application of genetic algorithms to quantum architecture search, aiming for a robust, automated method that can adapt to varied data and hardware constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡å­è‡ªåŠ¨ç¼–ç å™¨(Quantum Autoencoders)åœ¨ç”µè·¯æ¶æ„è®¾è®¡ä¸­é¢ä¸´çš„é—¨ç”µè·¯é€‰æ‹©ã€å±‚çº§æ’åˆ—åŠå‚æ•°è°ƒä¼˜ç­‰å¤æ‚æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨é—ä¼ ç®—æ³•(Genetic Algorithm, GA)å®ç°è‡ªåŠ¨åŒ–è®¾è®¡çš„ç¥ç»æ¶æ„æœç´¢(Neural Architecture Search, NAS)æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿæ€§åœ°æ¼”åŒ–å˜åˆ†é‡å­ç”µè·¯(Variational Quantum Circuit, VQC)çš„é…ç½®ï¼Œæ—¨åœ¨å¯»æ‰¾ç”¨äºæ•°æ®é‡æ„çš„é«˜æ€§èƒ½æ··åˆé‡å­-ç»å…¸è‡ªåŠ¨ç¼–ç å™¨ï¼Œå¹¶æœ‰æ•ˆè§£å†³æœç´¢è¿‡ç¨‹ä¸­æ˜“é™·å…¥å±€éƒ¨æå°å€¼çš„é—®é¢˜ã€‚å®éªŒåœ¨å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾äº†é‡å­è‡ªåŠ¨ç¼–ç å™¨åœ¨å«å™ªå£°çš„è¿‘æœŸå¾…é‡å­æ—¶ä»£(Near-term Quantum Era)è¿›è¡Œé«˜æ•ˆç‰¹å¾æå–çš„å·¨å¤§æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºé—ä¼ ç®—æ³•åœ¨é‡å­æ¶æ„æœç´¢é¢†åŸŸçš„è¿›ä¸€æ­¥åº”ç”¨å¥ å®šäº†åŸºç¡€ï¼Œæä¾›äº†ä¸€ç§èƒ½å¤Ÿé€‚åº”å¤šæ ·åŒ–æ•°æ®å’Œç¡¬ä»¶çº¦æŸçš„é²æ£’ä¸”è‡ªåŠ¨åŒ–çš„è®¾è®¡æ–¹æ³•ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19246v1",
      "published_date": "2025-11-24 15:55:44 UTC",
      "updated_date": "2025-11-24 15:55:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:02:21.355745+00:00"
    },
    {
      "arxiv_id": "2512.19697v1",
      "title": "Automated Fault Detection in 5G Core Networks Using Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ 5G æ ¸å¿ƒç½‘è‡ªåŠ¨æ•…éšœæ£€æµ‹",
      "authors": [
        "Parsa Hatami",
        "Ahmadreza Majlesara",
        "Ali Majlesi",
        "Babak Hossein Khalaj"
      ],
      "abstract": "With the rapid growth of data volume in modern telecommunication networks and the continuous expansion of their scale, maintaining high reliability has become a critical requirement. These networks support a wide range of applications and services, including highly sensitive and mission-critical ones, which demand rapid and accurate detection and resolution of network errors. Traditional fault-diagnosis methods are no longer efficient for such complex environments.\\cite{b1} In this study, we leverage Large Language Models (LLMs) to automate network fault detection and classification. Various types of network errors were intentionally injected into a Kubernetes-based test network, and data were collected under both healthy and faulty conditions. The dataset includes logs from different network components (pods), along with complementary data such as system descriptions, events, Round Trip Time (RTT) tests, and pod status information. The dataset covers common fault types such as pod failure, pod kill, network delay, network loss, and disk I/O failures. We fine-tuned the GPT-4.1 nano model via its API on this dataset, resulting in a significant improvement in fault-detection accuracy compared to the base model. These findings highlight the potential of LLM-based approaches for achieving closed-loop, and operator-free fault management, which can enhance network reliability and reduce downtime-related operational costs for service providers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨5Gæ ¸å¿ƒç½‘ä¸­åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å®ç°è‡ªåŠ¨åŒ–æ•…éšœæ£€æµ‹å’Œåˆ†ç±»çš„æ–¹æ³•ï¼Œä»¥åº”å¯¹ç°ä»£ç”µä¿¡ç½‘ç»œæ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§å’Œé«˜å¯é æ€§éœ€æ±‚ã€‚ç ”ç©¶äººå‘˜åœ¨ä¸€ä¸ªåŸºäº Kubernetes çš„æµ‹è¯•ç½‘ç»œä¸­ä¸»åŠ¨æ³¨å…¥äº†å¤šç§ç½‘ç»œé”™è¯¯ï¼ŒåŒ…æ‹¬ Pod failureã€Pod killã€Network delayã€Network loss ä»¥åŠ Disk I/O failuresï¼Œå¹¶æ”¶é›†äº†åŒ…å«ç³»ç»Ÿæ—¥å¿—ã€äº‹ä»¶ã€RTT æµ‹è¯•å’Œ Pod çŠ¶æ€åœ¨å†…çš„å¤šç»´æ•°æ®é›†ã€‚é€šè¿‡è°ƒç”¨ API å¯¹ GPT-4.1 nano æ¨¡å‹åœ¨ä¸Šè¿°æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒ (Fine-tuning)ï¼Œä½¿å…¶èƒ½å¤Ÿé’ˆå¯¹ä¸åŒç½‘ç»œç»„ä»¶çš„è¿è¡ŒçŠ¶æ€è¿›è¡Œé«˜æ•ˆçš„æ•…éšœè¯Šæ–­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨æ•…éšœæ£€æµ‹å‡†ç¡®ç‡ä¸Šè¾ƒåŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº† LLM å¤„ç†å¤æ‚ç½‘ç»œæ—¥å¿—çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€å‘ç°å±•ç¤ºäº†åŸºäº LLM çš„æ–¹æ³•åœ¨å®ç°é—­ç¯ (Closed-loop) ä¸”æ— æ“ä½œå‘˜å¹²é¢„çš„æ•…éšœç®¡ç†æ–¹é¢çš„æ½œåŠ›ï¼Œæœ‰åŠ©äºæé«˜ç½‘ç»œå¯é æ€§å¹¶æ˜¾è‘—é™ä½æœåŠ¡æä¾›å•†çš„è¿è¥æˆæœ¬ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19697v1",
      "published_date": "2025-11-24 15:55:29 UTC",
      "updated_date": "2025-11-24 15:55:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:02:31.239828+00:00"
    },
    {
      "arxiv_id": "2511.19241v1",
      "title": "Local Entropy Search over Descent Sequences for Bayesian Optimization",
      "title_zh": "è´å¶æ–¯ä¼˜åŒ–ä¸­åŸºäºä¸‹é™åºåˆ—çš„å±€éƒ¨ç†µæœç´¢",
      "authors": [
        "David Stenger",
        "Armin Lindicke",
        "Alexander von Rohr",
        "Sebastian Trimpe"
      ],
      "abstract": "Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å±€éƒ¨ç†µæœç´¢ (Local Entropy Search, LES)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹è´å¶æ–¯ä¼˜åŒ– (Bayesian Optimization) é¢†åŸŸçš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³å¤§å‹å¤æ‚è®¾è®¡ç©ºé—´ä¸­å¯»æ‰¾å…¨å±€æœ€ä¼˜è§£ä¸åˆ‡å®é™…çš„é—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒLES æ˜ç¡®é’ˆå¯¹é€šè¿‡è¿­ä»£ä¼˜åŒ–å™¨çš„ä¸‹é™åºåˆ— (descent sequences) å¯è¾¾çš„è§£è¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡å°†ç›®æ ‡å‡½æ•°çš„åéªŒä¿¡å¿µåœ¨ä¼˜åŒ–å™¨ä¸­ä¼ æ’­ï¼Œå¾—å‡ºä¸‹é™åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¯¥ç®—æ³•åœ¨é€‰æ‹©ä¸‹ä¸€æ¬¡è¯„ä¼°ç‚¹æ—¶ï¼Œç»“åˆäº†è§£æç†µè®¡ç®—å’Œä¸‹é™åºåˆ—çš„è’™ç‰¹å¡æ´›é‡‡æ · (Monte-Carlo sampling)ï¼Œä»¥å®ç°äº’ä¿¡æ¯ (mutual information) çš„æœ€å¤§åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤„ç†é«˜å¤æ‚åº¦çš„åˆæˆç›®æ ‡å’ŒåŸºå‡†é—®é¢˜æ—¶ï¼ŒLES ç›¸æ¯”äºç°æœ‰çš„å±€éƒ¨å’Œå…¨å±€è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•å±•ç°å‡ºäº†æ›´å¼ºçš„æ ·æœ¬æ•ˆç‡ (sample efficiency)ã€‚è¯¥æ–¹æ³•ä¸ºåœ¨å¤æ‚è®¾è®¡ç©ºé—´ä¸­é€šè¿‡è¿­ä»£æ”¹è¿›åˆå§‹è®¾è®¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ç†è®ºå®Œå¤‡çš„å±€éƒ¨ä¼˜åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19241v1",
      "published_date": "2025-11-24 15:52:17 UTC",
      "updated_date": "2025-11-24 15:52:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:02:27.337415+00:00"
    },
    {
      "arxiv_id": "2511.19565v1",
      "title": "Deductive Systems for Logic Programs with Counting",
      "title_zh": "å«è®¡æ•°ç®—å­çš„é€»è¾‘ç¨‹åºæ¼”ç»ç³»ç»Ÿ",
      "authors": [
        "Jorge Fandinno",
        "Vladimir Lifschitz"
      ],
      "abstract": "In answer set programming, two groups of rules are considered strongly equivalent if they have the same meaning in any context. Strong equivalence of two programs can be sometimes established by deriving rules of each program from rules of the other in an appropriate deductive system. This paper shows how to extend this method of proving strong equivalence to programs containing the counting aggregate.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å›ç­”é›†ç¼–ç¨‹(Answer Set Programming, ASP)ä¸­å¦‚ä½•éªŒè¯è§„åˆ™é›†çš„å¼ºç­‰ä»·æ€§(strong equivalence)ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå¦‚æœä¸¤ç»„è§„åˆ™åœ¨ä»»ä½•è¯­å¢ƒä¸‹éƒ½å…·æœ‰ç›¸åŒçš„å«ä¹‰ï¼Œåˆ™è¢«è§†ä¸ºå¼ºç­‰ä»·ã€‚ä¼ ç»Ÿçš„è¯æ˜æ–¹æ³•æ˜¯é€šè¿‡ç‰¹å®šçš„æ¼”ç»ç³»ç»Ÿ(deductive system)ä»ä¸€ä¸ªç¨‹åºçš„è§„åˆ™ä¸­æ¨å¯¼å‡ºå¦ä¸€ä¸ªç¨‹åºçš„è§„åˆ™ï¼Œä»è€Œå»ºç«‹ç­‰ä»·å…³ç³»ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åœ¨äºå°†è¿™ä¸€è¯æ˜å¼ºç­‰ä»·æ€§çš„æ¼”ç»æ–¹æ³•æ‰©å±•åˆ°äº†åŒ…å«è®¡æ•°èšåˆ(counting aggregate)çš„é€»è¾‘ç¨‹åºä¸­ã€‚é€šè¿‡è¿™ç§æ‰©å±•ï¼Œç ”ç©¶è€…èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ç¨‹åºé€»è¾‘å¹¶ç¡®ä¿å…¶åœ¨å„ç§ä¸Šä¸‹æ–‡ä¸­çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶ä¸ºè¿›ä¸€æ­¥åˆ†æã€è½¬åŒ–å’Œä¼˜åŒ–å¸¦æœ‰è®¡æ•°åŠŸèƒ½çš„é€»è¾‘ç¨‹åºæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "Under consideration in Theory and Practice of Logic Programming (TPLP)",
      "pdf_url": "https://arxiv.org/pdf/2511.19565v1",
      "published_date": "2025-11-24 15:49:06 UTC",
      "updated_date": "2025-11-24 15:49:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:02:27.684036+00:00"
    },
    {
      "arxiv_id": "2511.19236v1",
      "title": "SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control",
      "title_zh": "SENTINELï¼šé¢å‘ç±»äººæœºå™¨äººå…¨èº«æ§åˆ¶çš„å…¨ç«¯åˆ°ç«¯è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
      "authors": [
        "Yuxuan Wang",
        "Haobin Jiang",
        "Shiqing Yao",
        "Ziluo Ding",
        "Zongqing Lu"
      ],
      "abstract": "Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SENTINELï¼Œä¸€ç§é’ˆå¯¹äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶(whole-body control)çš„å…¨ç«¯åˆ°ç«¯(fully end-to-end)è¯­è¨€åŠ¨ä½œæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿä¸­è¯­è¨€æŒ‡ä»¤ä¸ç‰©ç†æ‰§è¡Œä¹‹é—´å¯¹é½ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡åœ¨ä»¿çœŸç¯å¢ƒä¸­è¿½è¸ªäººç±»è¿åŠ¨å¹¶ç»“åˆæ–‡æœ¬æ ‡æ³¨ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå°†è¯­è¨€å‘½ä»¤å’Œæœ¬ä½“æ„Ÿå—è¾“å…¥(proprioceptive inputs)ç›´æ¥æ˜ å°„ä¸ºåº•å±‚åŠ¨ä½œ(low-level actions)ã€‚SENTINELé‡‡ç”¨æµåŒ¹é…(flow matching)æŠ€æœ¯ç”ŸæˆåŠ¨ä½œå—(action chunks)ï¼Œå¹¶åˆ©ç”¨æ®‹å·®åŠ¨ä½œå¤´(residual action head)å¯¹åŠ¨ä½œè¿›è¡Œç²¾ç»†åŒ–å¤„ç†ä»¥é€‚åº”çœŸå®ç¯å¢ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ä»¿çœŸå’Œç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­å‡è¡¨ç°å‡ºå“è¶Šçš„è¯­ä¹‰ç†è§£èƒ½åŠ›å’Œè¿åŠ¨ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡å°†ä¸åŒè¾“å…¥è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œå…·å¤‡è‰¯å¥½çš„å¤šæ¨¡æ€æ‰©å±•(multi-modal extensions)æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "23 pages, 8 figures, 11 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.19236v1",
      "published_date": "2025-11-24 15:48:59 UTC",
      "updated_date": "2025-11-24 15:48:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:02:32.947493+00:00"
    },
    {
      "arxiv_id": "2511.19232v1",
      "title": "In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations",
      "title_zh": "In Machina N400ï¼šå› æœè¯­è¨€æ¨¡å‹ä¸­è¯­ä¹‰è¿èƒŒæ£€æµ‹ä½ç½®çš„ç²¾å‡†å®šä½",
      "authors": [
        "Christos-Nikolaos Zacharopoulos",
        "Revekka Kyriakoglou"
      ],
      "abstract": "How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Transformeræ¨¡å‹å¦‚ä½•ä»¥åŠåœ¨ä½•å¤„è¯†åˆ«è¯­ä¹‰è¿èƒŒ(Semantic Violations)ï¼Œé€šè¿‡å¯¹å› æœè¯­è¨€æ¨¡å‹(Causal Language Model) phi-2 åœ¨åŒ…å«åˆç†ä¸ä¸åˆç†ç»“å°¾çš„è¯­æ–™åº“ä¸Šè¿›è¡Œæ·±å…¥åˆ†æã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ä¸¤ç§äº’è¡¥çš„æ¢æµ‹å·¥å…·åˆ†æäº†æ¨¡å‹å„å±‚çš„éšè—çŠ¶æ€(Hidden States)ï¼Œå…¶ä¸­çº¿æ€§æ¢æµ‹(Linear Probe)å‘ç°æ¨¡å‹åº•å±‚å¯¹è¯­ä¹‰åˆç†æ€§çš„åŒºåˆ†åº¦è¾ƒä½ï¼Œè€Œå‡†ç¡®ç‡åœ¨ä¸­é—´å±‚æ˜¾è‘—æå‡å¹¶åœ¨æ¥è¿‘é¡¶å±‚å‰è¾¾åˆ°å³°å€¼ã€‚é€šè¿‡å¯¹æœ‰æ•ˆç»´åº¦(Effective Dimensionality)çš„åˆ†æï¼Œç ”ç©¶å‘ç°è¯­ä¹‰è¿èƒŒä¼šå¯¼è‡´è¡¨å¾å­ç©ºé—´(Representational Subspace)ç»å†å…ˆæ‰©å¼ ååç¼©çš„è¿‡ç¨‹ï¼Œæš—ç¤ºäº†ä»ä¿¡æ¯æ¢ç´¢åˆ°å¿«é€Ÿæ•´åˆçš„è½¬å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤„ç†æœºåˆ¶ä¸äººç±»é˜…è¯»ä¸­çš„å¿ƒç†è¯­è¨€å­¦å‘ç°å…·æœ‰ä¸€è‡´æ€§ï¼Œå³è¯­ä¹‰å¼‚å¸¸é€šå¸¸åœ¨å¥æ³•è§£æä¹‹åçš„å¤„ç†åºåˆ—åæœŸè¢«æ£€æµ‹åˆ°ã€‚è¯¥å·¥ä½œä¸ºç†è§£ç”Ÿæˆå¼æ¨¡å‹å†…éƒ¨çš„è¯­ä¹‰å¤„ç†é€»è¾‘æä¾›äº†é‡è¦çš„æœºæ¢°è§£é‡Šæ€§è¯æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at AICS2025",
      "pdf_url": "https://arxiv.org/pdf/2511.19232v1",
      "published_date": "2025-11-24 15:43:56 UTC",
      "updated_date": "2025-11-24 15:43:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:02:35.644744+00:00"
    },
    {
      "arxiv_id": "2511.19229v2",
      "title": "Learning Plug-and-play Memory for Guiding Video Diffusion Models",
      "title_zh": "ç”¨äºå¼•å¯¼è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å³æ’å³ç”¨è®°å¿†å­¦ä¹ ",
      "authors": [
        "Selena Song",
        "Ziming Xu",
        "Zijun Zhang",
        "Kun Zhou",
        "Jiaxian Guo",
        "Lianhui Qin",
        "Biwei Huang"
      ],
      "abstract": "Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä¸ºåŸºäº Diffusion Transformer (DiT) çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹é…å¤‡å³æ’å³ç”¨çš„å†…å­˜ï¼Œä»¥è§£å†³å…¶åœ¨ç‰©ç†è§„å¾‹å’Œå¸¸è¯†åŠ¨æ€æ–¹é¢çš„è®¤çŸ¥ç¼ºå¤±ã€‚ä½œè€…å—å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸Šä¸‹æ–‡å†…å­˜çš„å¯å‘ï¼Œé€šè¿‡ç ”ç©¶å‘ç° DiT çš„éšè—çŠ¶æ€å¯ä»¥é€šè¿‡å¹²é¢„æ¥å¼•å¯¼ï¼Œå¹¶åˆ©ç”¨åµŒå…¥ç©ºé—´çš„ä½é€šå’Œé«˜é€šæ»¤æ³¢å™¨å®ç°äº†åº•å±‚å¤–è§‚ä¸é«˜å±‚ç‰©ç†/è¯­ä¹‰çº¿ç´¢çš„æœ‰æ•ˆè§£è€¦ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º DiT-Mem çš„å¯å­¦ä¹ å†…å­˜ç¼–ç å™¨ï¼Œå®ƒç”± 3D CNNsã€æ»¤æ³¢å™¨å’Œè‡ªæ³¨æ„åŠ›å±‚ç»„æˆï¼Œèƒ½å°†å‚è€ƒè§†é¢‘æ˜ å°„ä¸ºå†…å­˜ä»¤ç‰Œå¹¶æ•´åˆåˆ° DiT çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¯¥æ–¹æ³•ä¿æŒæ‰©æ•£éª¨å¹²ç½‘ç»œå†»ç»“ï¼Œä»…éœ€ä¼˜åŒ– 1.5 äº¿å‚æ•°çš„ç¼–ç å™¨ï¼Œåœ¨ 1 ä¸‡ä¸ªæ•°æ®æ ·æœ¬ä¸Šå³å¯å®ç°é«˜æ•ˆè®­ç»ƒå¹¶æ”¯æŒæ¨ç†æ—¶çš„å³æ’å³ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå‰æ²¿æ¨¡å‹ä¸Šæ˜¾è‘—å¢å¼ºäº†è§†é¢‘ç”Ÿæˆå¯¹ç‰©ç†è§„åˆ™çš„éµå¾ªèƒ½åŠ›ï¼Œå¹¶æå‡äº†æ•´ä½“ç”Ÿæˆä¿çœŸåº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19229v2",
      "published_date": "2025-11-24 15:42:23 UTC",
      "updated_date": "2025-11-27 05:44:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:02:49.138710+00:00"
    },
    {
      "arxiv_id": "2511.19562v1",
      "title": "Trust-Based Social Learning for Communication (TSLEC) Protocol Evolution in Multi-Agent Reinforcement Learning",
      "title_zh": "å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­åŸºäºä¿¡ä»»çš„ç¤¾ä¼šå­¦ä¹ é€šä¿¡ï¼ˆTSLECï¼‰åè®®æ¼”åŒ–",
      "authors": [
        "Abraham Itzhak Weinberg"
      ],
      "abstract": "Emergent communication in multi-agent systems typically occurs through independent learning, resulting in slow convergence and potentially suboptimal protocols. We introduce TSLEC (Trust-Based Social Learning with Emergent Communication), a framework where agents explicitly teach successful strategies to peers, with knowledge transfer modulated by learned trust relationships. Through experiments with 100 episodes across 30 random seeds, we demonstrate that trust-based social learning reduces episodes-to-convergence by 23.9% (p < 0.001, Cohen's d = 1.98) compared to independent emergence, while producing compositional protocols (C = 0.38) that remain robust under dynamic objectives (Phi > 0.867 decoding accuracy). Trust scores strongly correlate with teaching quality (r = 0.743, p < 0.001), enabling effective knowledge filtering. Our results establish that explicit social learning fundamentally accelerates emergent communication in multi-agent coordination.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agent Systemsï¼‰ä¸­ç”±äºç‹¬ç«‹å­¦ä¹ å¯¼è‡´çš„é€šä¿¡æ¶Œç°æ”¶æ•›æ…¢å’Œåè®®æ¬¡ä¼˜é—®é¢˜ï¼Œæå‡ºäº†åŸºäºä¿¡ä»»çš„ç¤¾ä¼šå­¦ä¹ é€šä¿¡åè®®æ¼”åŒ–æ¡†æ¶TSLECï¼ˆTrust-Based Social Learning with Emergent Communicationï¼‰ã€‚è¯¥æ¡†æ¶å…è®¸æ™ºèƒ½ä½“ä¹‹é—´æ˜¾å¼åœ°ä¼ æˆæˆåŠŸç­–ç•¥ï¼Œå¹¶é€šè¿‡å­¦ä¹ åˆ°çš„ä¿¡ä»»å…³ç³»æ¥åŠ¨æ€è°ƒèŠ‚çŸ¥è¯†è½¬ç§»è¿‡ç¨‹ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„çŸ¥è¯†è¿‡æ»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç‹¬ç«‹å­¦ä¹ ç›¸æ¯”ï¼ŒåŸºäºä¿¡ä»»çš„ç¤¾ä¼šå­¦ä¹ å°†æ”¶æ•›é€Ÿåº¦æé«˜äº†23.9%ï¼Œå¹¶ç”Ÿæˆäº†åœ¨åŠ¨æ€ç›®æ ‡ä¸‹å…·æœ‰é«˜åº¦é²æ£’æ€§çš„ç»„åˆæ€§åè®®ï¼ˆCompositional Protocolsï¼‰ã€‚ç ”ç©¶è¿˜å‘ç°ä¿¡ä»»è¯„åˆ†ä¸æ•™å­¦è´¨é‡ä¹‹é—´å­˜åœ¨å¼ºæ­£ç›¸å…³æ€§ï¼Œè¯æ˜äº†æ˜¾å¼ç¤¾ä¼šå­¦ä¹ åœ¨åŠ é€Ÿå¤šæ™ºèƒ½ä½“åä½œé€šä¿¡æ¶Œç°æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚è¯¥æˆæœä¸ºæå‡å¤æ‚ç¯å¢ƒä¸‹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMulti-Agent Reinforcement Learningï¼‰çš„æ•ˆç‡æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19562v1",
      "published_date": "2025-11-24 15:31:51 UTC",
      "updated_date": "2025-11-24 15:31:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:02:52.353375+00:00"
    },
    {
      "arxiv_id": "2512.02038v1",
      "title": "Deep Research: A Systematic Survey",
      "title_zh": "æ·±åº¦ç ”ç©¶ï¼šç³»ç»Ÿæ€§ç»¼è¿°",
      "authors": [
        "Zhengliang Shi",
        "Yiqun Chen",
        "Haitao Li",
        "Weiwei Sun",
        "Shiyu Ni",
        "Yougang Lyu",
        "Run-Ze Fan",
        "Bowen Jin",
        "Yixuan Weng",
        "Minjun Zhu",
        "Qiujie Xie",
        "Xinyu Guo",
        "Qu Yang",
        "Jiayi Wu",
        "Jujia Zhao",
        "Xiaqiang Tang",
        "Xinbei Ma",
        "Cunxiang Wang",
        "Jiaxin Mao",
        "Qingyao Ai",
        "Jen-Tse Huang",
        "Wenxuan Wang",
        "Yue Zhang",
        "Yiming Yang",
        "Zhaopeng Tu",
        "Zhaochun Ren"
      ],
      "abstract": "Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.",
      "tldr_zh": "è¯¥ç»¼è¿°å¯¹Deep Research (DR)ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢ä¸”ç³»ç»Ÿçš„æ€»ç»“ï¼Œæ—¨åœ¨æ¢è®¨å¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†èƒ½åŠ›ä¸å¤–éƒ¨å·¥å…·ç›¸ç»“åˆï¼Œä½¿å…¶èƒ½å¤Ÿä½œä¸ºç ”ç©¶æ™ºèƒ½ä½“(research agents)å®Œæˆå¤æ‚çš„å¼€æ”¾å¼ä»»åŠ¡ã€‚æ–‡ç« é¦–å…ˆå½¢å¼åŒ–äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„è·¯çº¿å›¾ï¼Œå¹¶å°†DRä¸ç›¸å…³èŒƒå¼è¿›è¡Œäº†æ˜ç¡®åŒºåˆ†ã€‚ç ”ç©¶è¯¦ç»†ä»‹ç»äº†DRçš„å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå³æŸ¥è¯¢è§„åˆ’(query planning)ã€ä¿¡æ¯è·å–(information acquisition)ã€å†…å­˜ç®¡ç†(memory management)å’Œå›ç­”ç”Ÿæˆ(answer generation)ï¼Œå¹¶ä¸ºæ¯ä¸ªç»„ä»¶æä¾›äº†ç»†ç²’åº¦çš„å­åˆ†ç±»ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ€»ç»“äº†åŒ…æ‹¬æç¤ºå·¥ç¨‹(prompting)ã€ç›‘ç£å¾®è°ƒ(supervised fine-tuning)å’Œæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (agentic reinforcement learning)åœ¨å†…çš„å…³é”®ä¼˜åŒ–æŠ€æœ¯ã€‚æœ€åï¼Œè¯¥ç ”ç©¶æ•´åˆäº†ç°æœ‰çš„è¯„ä¼°æ ‡å‡†å¹¶åˆ†æäº†è¯¥é¢†åŸŸé¢ä¸´çš„é‡è¦æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥Deep Researchçš„å‘å±•æä¾›äº†ç³»ç»Ÿçš„è·¯çº¿å›¾å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.02038v1",
      "published_date": "2025-11-24 15:28:28 UTC",
      "updated_date": "2025-11-24 15:28:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:02:45.651415+00:00"
    },
    {
      "arxiv_id": "2511.19561v1",
      "title": "Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport",
      "title_zh": "æ— é—å¿˜èåˆï¼šåŸºäºæœ€ä¼˜ä¼ è¾“çš„ä»»åŠ¡ç‰¹å®šæ¨¡å‹æŒç»­èåˆ",
      "authors": [
        "Zecheng Pan",
        "Zhikang Chen",
        "Ding Li",
        "Min Zhang",
        "Sen Cui",
        "Hongshuo Jin",
        "Luqi Tao",
        "Yi Yang",
        "Deheng Ye",
        "Yu Zhang",
        "Tingting Zhu",
        "Tianling Ren"
      ],
      "abstract": "Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°†ä¸åŒä»»åŠ¡å¾®è°ƒçš„æ¨¡å‹åˆå¹¶ä¸ºç»Ÿä¸€å¤šä»»åŠ¡ç³»ç»Ÿæ—¶ï¼Œç°æœ‰å‚æ•°æ’å€¼æ–¹æ³•ä¼šå¯¼è‡´ç‰¹å¾ç©ºé—´åˆ†å¸ƒåç§»å¹¶ç ´åä»»åŠ¡ç‰¹å®šçŸ¥è¯†çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†OTMF (Optimal Transport-based Masked Fusion)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæœ€ä¼˜ä¼ è¾“ (Optimal Transport) ç†è®ºçš„æ–°å‹æ¨¡å‹åˆå¹¶æ¡†æ¶ã€‚OTMF é€šè¿‡æœ€ä¼˜ä¼ è¾“è®¡åˆ’å‘ç°åº”ç”¨äºä»»åŠ¡å‘é‡çš„å…¬å…±æ©ç  (masks)ï¼Œä»è€Œå¯¹é½ä¸åŒä»»åŠ¡ç‰¹å®šæ¨¡å‹çš„è¯­ä¹‰å‡ ä½•ç»“æ„ã€‚è¿™äº›æ©ç èƒ½å¤Ÿé€‰æ‹©æ€§åœ°æå–å¯è¿ç§»å’Œä»»åŠ¡æ— å…³çš„ç»„ä»¶ï¼ŒåŒæ—¶ä¿ç•™æ¯ä¸ªä»»åŠ¡ç‹¬ç‰¹çš„ç»“æ„ç‰¹å¾ã€‚ä¸ºäº†é€‚åº”å®é™…åº”ç”¨ä¸­çš„æ‰©å±•æ€§éœ€æ±‚ï¼Œè¯¥æ¡†æ¶æ”¯æŒæŒç»­èåˆ (continual fusion) èŒƒå¼ï¼Œèƒ½å¤Ÿå¢é‡å¼é›†æˆæ–°ä»»åŠ¡å‘é‡è€Œæ— éœ€é‡æ–°è®¿é—®æ—§ä»»åŠ¡ï¼Œä»è€Œä¿æŒæœ‰é™çš„å†…å­˜å ç”¨ã€‚åœ¨å¤šä¸ªè§†è§‰å’Œè¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOTMF åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›æ°´å¹³ (State-of-the-art)ï¼Œä¸ºæ¨¡å‹åˆå¹¶æä¾›äº†é‡è¦çš„ç†è®ºä¸å®è·µä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19561v1",
      "published_date": "2025-11-24 15:27:47 UTC",
      "updated_date": "2025-11-24 15:27:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:03:13.133632+00:00"
    },
    {
      "arxiv_id": "2511.19220v2",
      "title": "Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering",
      "title_zh": "å¤§è§†è§‰è¯­è¨€æ¨¡å‹æ˜¯å¦çœŸæ­£å®ç°äº†åŒ»å­¦å½±åƒçš„è§†è§‰æ‰æ ¹ï¼Ÿæ¥è‡ª Italian ä¸´åºŠè§†è§‰é—®ç­”çš„è¯æ®",
      "authors": [
        "Federico Felizzi",
        "Olivia Riccomi",
        "Michele Ferramola",
        "Francesco Andrea Causio",
        "Manuel Del Medico",
        "Vittorio De Vita",
        "Lorenzo De Mori",
        "Alessandra Piscitelli",
        "Pietro Eric Risuleo",
        "Bianca Destro Castaniti",
        "Antonio Cristiano",
        "Alessia Longo",
        "Luigi De Angelis",
        "Mariapia Vassalli",
        "Marcello Di Pumpo"
      ],
      "abstract": "Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(Large Vision Language Models, VLMs)åœ¨åŒ»ç–—è§†è§‰é—®ç­”(Medical Visual Question Answering)ä¸­æ˜¯å¦å…·å¤‡çœŸå®çš„è§†è§‰åŸºç¡€(Visual Grounding)ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬å¯¹å›¾åƒä¿¡æ¯çš„ä¾èµ–ç¨‹åº¦ã€‚ç ”ç©¶äººå‘˜é’ˆå¯¹ Claude Sonnet 4.5ã€GPT-4oã€GPT-5-mini å’Œ Gemini 2.0 flash exp å››æ¬¾å‰æ²¿æ¨¡å‹ï¼Œåˆ©ç”¨ EuropeMedQA æ„å¤§åˆ©è¯­æ•°æ®é›†ä¸­çš„ä¸´åºŠé—®é¢˜è¿›è¡Œäº†æµ‹è¯•ã€‚é€šè¿‡å°†çœŸå®çš„åŒ»å­¦å›¾åƒæ›¿æ¢ä¸ºç©ºç™½å ä½ç¬¦(Blank Placeholders)ï¼Œå®éªŒæ­ç¤ºäº†æ¨¡å‹åœ¨è§†è§‰ä¾èµ–æ€§ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼šGPT-4o è¡¨ç°å‡ºæœ€å¼ºçš„è§†è§‰åŸºç¡€ï¼Œå…¶å‡†ç¡®ç‡åœ¨æ— å›¾æƒ…å†µä¸‹ä¸‹é™äº†27.9ä¸ªç™¾åˆ†ç‚¹ï¼Œè€Œå…¶ä»–æ¨¡å‹å¦‚ Gemini å’Œ Claude çš„ä¸‹é™å¹…åº¦æå°ã€‚åˆ†æå‘ç°ï¼Œå³ä½¿åœ¨ç¼ºä¹å›¾åƒæ”¯æŒæ—¶ï¼Œæ¨¡å‹ä»èƒ½ç”Ÿæˆè‡ªä¿¡ä½†è™šå‡çš„è§†è§‰æ¨ç†ï¼Œæš—ç¤ºå…¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šåˆ©ç”¨äº†æ–‡æœ¬æ·å¾„(Textual Shortcuts)è€ŒéçœŸå®çš„è§†è§‰åˆ†æã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰ VLMs åœ¨å¤„ç†åŒ»ç–—å½±åƒæ—¶çš„é²æ£’æ€§å±€é™ï¼Œå¼ºè°ƒäº†åœ¨ä¸´åºŠéƒ¨ç½²(Clinical Deployment)å‰è¿›è¡Œæ›´ä¸¥è°¨è¯„ä¼°çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the Workshop on Multimodal Representation Learning for Healthcare (MMRL4H), EurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.19220v2",
      "published_date": "2025-11-24 15:26:58 UTC",
      "updated_date": "2025-11-26 20:23:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:03:37.933278+00:00"
    },
    {
      "arxiv_id": "2511.19218v2",
      "title": "Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization",
      "title_zh": "åŸºäºæ ‘-ç»„åŒé‡æ„ŸçŸ¥æœç´¢ä¸ä¼˜åŒ–çš„ LLM å®‰å…¨å¯¹é½å¯¹æŠ—æ€§æ”»é˜²ååŒæ¼”åŒ–",
      "authors": [
        "Xurui Li",
        "Kaisong Song",
        "Rui Zhu",
        "Pin-Yu Chen",
        "Haixu Tang"
      ],
      "abstract": "Large Language Models (LLMs) have developed rapidly in web services, delivering unprecedented capabilities while amplifying societal risks. Existing works tend to focus on either isolated jailbreak attacks or static defenses, neglecting the dynamic interplay between evolving threats and safeguards in real-world web contexts. To mitigate these challenges, we propose ACE-Safety (Adversarial Co-Evolution for LLM Safety), a novel framework that jointly optimize attack and defense models by seamlessly integrating two key innovative procedures: (1) Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS), which efficiently explores jailbreak strategies to uncover vulnerabilities and generate diverse adversarial samples; (2) Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO), which jointly trains attack and defense LLMs with challenging samples via curriculum reinforcement learning, enabling robust mutual improvement. Evaluations across multiple benchmarks demonstrate that our method outperforms existing attack and defense approaches, and provides a feasible pathway for developing LLMs that can sustainably support responsible AI ecosystems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç½‘ç»œæœåŠ¡ä¸­é¢ä¸´çš„ç¤¾ä¼šé£é™©ï¼Œæå‡ºäº† ACE-Safety (Adversarial Co-Evolution for LLM Safety) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶ä¾§é‡äºå­¤ç«‹çš„è¶Šç‹±æ”»å‡»æˆ–é™æ€é˜²å¾¡è€Œå¿½è§†æ”»é˜²åŠ¨æ€æ¼”è¿›çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è”åˆä¼˜åŒ–æ”»å‡»å’Œé˜²å¾¡æ¨¡å‹ï¼Œé›†æˆäº† Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS) æ¥é«˜æ•ˆæ¢ç´¢è¶Šç‹±ç­–ç•¥å¹¶ç”Ÿæˆå¤šæ ·åŒ–çš„å¯¹æŠ—æ ·æœ¬ã€‚åŒæ—¶ï¼Œåˆ©ç”¨ Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO) é…åˆè¯¾ç¨‹å¼ºåŒ–å­¦ä¹  (curriculum reinforcement learning) ååŒè®­ç»ƒæ”»é˜²æ¨¡å‹ï¼Œå®ç°äº†ä¸¤è€…çš„é²æ£’äº’ä¿ƒã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒACE-Safety çš„æ€§èƒ½ä¼˜äºç°æœ‰çš„æ”»å‡»å’Œé˜²å¾¡æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘èƒ½å¤ŸæŒç»­æ”¯æŒè´Ÿè´£ä»»äººå·¥æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿçš„ LLMs æä¾›äº†ä¸€æ¡å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19218v2",
      "published_date": "2025-11-24 15:23:41 UTC",
      "updated_date": "2025-11-26 15:12:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:03:20.237941+00:00"
    },
    {
      "arxiv_id": "2511.19199v1",
      "title": "CLASH: A Benchmark for Cross-Modal Contradiction Detection",
      "title_zh": "CLASHï¼šè·¨æ¨¡æ€çŸ›ç›¾æ£€æµ‹åŸºå‡†",
      "authors": [
        "Teodora Popordanoska",
        "Jiameng Li",
        "Matthew B. Blaschko"
      ],
      "abstract": "Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•é€šå¸¸å‡è®¾è·¨æ¨¡æ€è¾“å…¥å…·æœ‰ä¸€è‡´æ€§ï¼Œè€Œå¿½ç•¥äº†è·¨æ¨¡æ€å†²çªæ£€æµ‹(cross-modal contradiction detection)è¿™ä¸€é¢„é˜²å¹»è§‰å…³é”®èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†å…¨æ–°çš„åŸºå‡†æµ‹è¯•CLASHã€‚CLASHåˆ©ç”¨COCOå›¾åƒé…å¯¹å«æœ‰å—æ§å¯¹è±¡çº§(object-level)æˆ–å±æ€§çº§(attribute-level)å†²çªçš„æè¿°ï¼Œé€šè¿‡å¤šé€‰é¢˜å’Œå¼€æ”¾å¼é—®é¢˜å½¢å¼è¯„ä¼°æ¨¡å‹ã€‚è¯¥åŸºå‡†åŒ…å«ç»è‡ªåŠ¨åŒ–è´¨é‡æ£€æŸ¥è¿‡æ»¤çš„å¤§è§„æ¨¡å¾®è°ƒé›†(fine-tuning set)ä»¥åŠè§„æ¨¡è¾ƒå°ã€ç»è¿‡äººå·¥éªŒè¯çš„è¯Šæ–­é›†(diagnostic set)ã€‚å®éªŒåˆ†ææ˜¾ç¤ºï¼Œå½“å‰çš„State-of-the-artæ¨¡å‹åœ¨è¯†åˆ«è·¨æ¨¡æ€å†²çªæ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œæš´éœ²äº†ç³»ç»Ÿæ€§çš„æ¨¡æ€åå·®(modality biases)å’Œç‰¹å®šç±»åˆ«çš„æ€§èƒ½å¼±ç‚¹ã€‚ç ”ç©¶è¿›ä¸€æ­¥é€šè¿‡å®è¯è¯æ˜ï¼Œåœ¨CLASHä¸Šè¿›è¡Œé’ˆå¯¹æ€§å¾®è°ƒèƒ½å®è´¨æ€§æå‡æ¨¡å‹å¯¹å†²çªçš„æ£€æµ‹èƒ½åŠ›ï¼Œä¸ºç¡®ä¿å¤šæ¨¡æ€ç³»ç»Ÿçš„å¯é æ€§æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "First two authors contributed equally",
      "pdf_url": "https://arxiv.org/pdf/2511.19199v1",
      "published_date": "2025-11-24 15:09:07 UTC",
      "updated_date": "2025-11-24 15:09:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:03:14.538583+00:00"
    },
    {
      "arxiv_id": "2511.20700v1",
      "title": "Paraconsistent-Lib: an intuitive PAL2v algorithm Python Library",
      "title_zh": "Paraconsistent-Libï¼šä¸€æ¬¾ç›´è§‚çš„ PAL2v ç®—æ³• Python åº“",
      "authors": [
        "Arnaldo de Carvalho Junior",
        "Diego Oliveira da Cruz",
        "Bruno da Silva Alves",
        "Fernando da Silva Paulo Junior",
        "JoÃ£o Inacio da Silva Filho"
      ],
      "abstract": "This paper introduces Paraconsistent-Lib, an open-source, easy-to-use Python library for building PAL2v algorithms in reasoning and decision-making systems. Paraconsistent-Lib is designed as a general-purpose library of PAL2v standard calculations, presenting three types of results: paraconsistent analysis in one of the 12 classical lattice PAL2v regions, paraconsistent analysis node (PAN) outputs, and a decision output. With Paraconsistent-Lib, well-known PAL2v algorithms such as Para-analyzer, ParaExtrCTX, PAL2v Filter, paraconsistent analysis network (PANnet), and paraconsistent neural network (PNN) can be written in stand-alone or network form, reducing complexity, code size, and bugs, as two examples presented in this paper. Given its stable state, Paraconsistent-Lib is an active development to respond to user-required features and enhancements received on GitHub.",
      "tldr_zh": "æœ¬ç ”ç©¶ä»‹ç»äº† Paraconsistent-Libï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºä¸”æ˜“äºä½¿ç”¨çš„ Python åº“ï¼Œä¸“é—¨ç”¨äºåœ¨æ¨ç†å’Œå†³ç­–ç³»ç»Ÿä¸­æ„å»º PAL2v ç®—æ³•ã€‚è¯¥åº“è¢«è®¾è®¡ä¸º PAL2v æ ‡å‡†è®¡ç®—çš„é€šç”¨åº“ï¼Œèƒ½å¤Ÿæä¾›åŒ…æ‹¬ 12 ä¸ªç»å…¸æ ¼ (12 classical lattice PAL2v regions) çš„ä¸ä¸€è‡´åˆ†æã€ä¸ä¸€è‡´åˆ†æèŠ‚ç‚¹ (PAN) è¾“å‡ºä»¥åŠæœ€ç»ˆå†³ç­–è¾“å‡ºåœ¨å†…çš„ä¸‰ç±»ç»“æœã€‚å€ŸåŠ©äº Paraconsistent-Libï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´ç®€ä¾¿åœ°ä»¥å•æœºæˆ–ç½‘ç»œå½¢å¼å®ç°å¦‚ Para-analyzerã€ParaExtrCTXã€PAL2v Filterã€ä¸ä¸€è‡´åˆ†æç½‘ç»œ (PANnet) å’Œä¸ä¸€è‡´ç¥ç»ç½‘ç»œ (PNN) ç­‰çŸ¥å PAL2v ç®—æ³•ã€‚é€šè¿‡è®ºæ–‡ä¸­çš„ä¸¤ä¸ªç¤ºä¾‹è¯æ˜ï¼Œè¯¥åº“æœ‰æ•ˆé™ä½äº†ç®—æ³•å®ç°çš„å¤æ‚æ€§ã€ä»£ç é‡å¹¶å‡å°‘äº†æ½œåœ¨é”™è¯¯ã€‚ç›®å‰ Paraconsistent-Lib å¤„äºç¨³å®šçŠ¶æ€å¹¶æŒç»­åœ¨ GitHub ä¸Šè¿›è¡Œæ´»è·ƒå¼€å‘ï¼Œä»¥æ ¹æ®ç”¨æˆ·åé¦ˆä¸æ–­ä¼˜åŒ–åŠŸèƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 9 figures, 2 appendix",
      "pdf_url": "https://arxiv.org/pdf/2511.20700v1",
      "published_date": "2025-11-24 15:06:35 UTC",
      "updated_date": "2025-11-24 15:06:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:03:19.546332+00:00"
    },
    {
      "arxiv_id": "2511.19184v1",
      "title": "Torsion-Space Diffusion for Protein Backbone Generation with Geometric Refinement",
      "title_zh": "ç»“åˆå‡ ä½•ç»†åŒ–çš„æ‰­è½¬ç©ºé—´æ‰©æ•£è›‹ç™½è´¨éª¨æ¶ç”Ÿæˆ",
      "authors": [
        "Lakshaditya Singh",
        "Adwait Shelke",
        "Divyansh Agrawal"
      ],
      "abstract": "Designing new protein structures is fundamental to computational biology, enabling advances in therapeutic molecule discovery and enzyme engineering. Existing diffusion-based generative models typically operate in Cartesian coordinate space, where adding noise disrupts strict geometric constraints such as fixed bond lengths and angles, often producing physically invalid structures. To address this limitation, we propose a Torsion-Space Diffusion Model that generates protein backbones by denoising torsion angles, ensuring perfect local geometry by construction. A differentiable forward-kinematics module reconstructs 3D coordinates with fixed 3.8 Angstrom backbone bond lengths while a constrained post-processing refinement optimizes global compactness via Radius of Gyration (Rg) correction, without violating bond constraints. Experiments on standard PDB proteins demonstrate 100% bond-length accuracy and significantly improved structural compactness, reducing Rg error from 70% to 18.6% compared to Cartesian diffusion baselines. Overall, this hybrid torsion-diffusion plus geometric-refinement framework generates physically valid and compact protein backbones, providing a promising path toward full-atom protein generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäºç¬›å¡å°”åæ ‡ç©ºé—´(Cartesian coordinate space)çš„è›‹ç™½è´¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹å› ç ´åå‡ ä½•çº¦æŸè€Œäº§ç”Ÿç‰©ç†ä¸Šæ— æ•ˆç»“æ„çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ‰­è½¬ç©ºé—´æ‰©æ•£æ¨¡å‹(Torsion-Space Diffusion Model)ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹æ‰­è½¬è§’(torsion angles)è¿›è¡Œå»å™ªå¤„ç†æ¥ç”Ÿæˆè›‹ç™½è´¨ä¸»é“¾ï¼Œä»æ„é€ ä¸Šç¡®ä¿äº†å®Œç¾çš„å±€éƒ¨å‡ ä½•æ€§è´¨ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªå¯å¾®åˆ†çš„å‰å‘åŠ¨åŠ›å­¦æ¨¡å—(forward-kinematics module)ä»¥åœ¨å›ºå®šä¸»é“¾é”®é•¿çš„æ¡ä»¶ä¸‹é‡å»ºä¸‰ç»´åæ ‡ï¼Œå¹¶ç»“åˆå—é™çš„åå¤„ç†ç²¾ä¿®æŠ€æœ¯ï¼Œé€šè¿‡åŠå¾„å›è½¬(Radius of Gyration, Rg)ä¿®æ­£ä¼˜åŒ–å…¨å±€ç´§å‡‘æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨æ ‡å‡†PDBè›‹ç™½è´¨ä¸Šå®ç°äº†100%çš„é”®é•¿å‡†ç¡®ç‡ï¼Œå¹¶å°†Rgè¯¯å·®ä»åŸºçº¿æ¨¡å‹çš„70%æ˜¾è‘—é™ä½è‡³18.6%ã€‚è¿™ç§ç»“åˆäº†æ‰­è½¬æ‰©æ•£ä¸å‡ ä½•ç²¾ä¿®çš„æ··åˆæ¡†æ¶èƒ½å¤Ÿç”Ÿæˆç‰©ç†æœ‰æ•ˆä¸”ç´§å‡‘çš„è›‹ç™½è´¨ä¸»é“¾ï¼Œä¸ºå®ç°å…¨åŸå­è›‹ç™½è´¨ç”Ÿæˆæä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "5 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.19184v1",
      "published_date": "2025-11-24 14:51:29 UTC",
      "updated_date": "2025-11-24 14:51:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:03:38.540777+00:00"
    },
    {
      "arxiv_id": "2511.19558v1",
      "title": "SPQR: A Standardized Benchmark for Modern Safety Alignment Methods in Text-to-Image Diffusion Models",
      "title_zh": "SPQRï¼šæ–‡ç”Ÿå›¾æ‰©æ•£æ¨¡å‹ç°ä»£å®‰å…¨å¯¹é½æ–¹æ³•çš„æ ‡å‡†åŒ–åŸºå‡†",
      "authors": [
        "Mohammed Talha Alam",
        "Nada Saadi",
        "Fahad Shamshad",
        "Nils Lukas",
        "Karthik Nandakumar",
        "Fahkri Karray",
        "Samuele Poppi"
      ],
      "abstract": "Text-to-image diffusion models can emit copyrighted, unsafe, or private content. Safety alignment aims to suppress specific concepts, yet evaluations seldom test whether safety persists under benign downstream fine-tuning routinely applied after deployment (e.g., LoRA personalization, style/domain adapters). We study the stability of current safety methods under benign fine-tuning and observe frequent breakdowns. As true safety alignment must withstand even benign post-deployment adaptations, we introduce the SPQR benchmark (Safety-Prompt adherence-Quality-Robustness). SPQR is a single-scored metric that provides a standardized and reproducible framework to evaluate how well safety-aligned diffusion models preserve safety, utility, and robustness under benign fine-tuning, by reporting a single leaderboard score to facilitate comparisons. We conduct multilingual, domain-specific, and out-of-distribution analyses, along with category-wise breakdowns, to identify when safety alignment fails after benign fine-tuning, ultimately showcasing SPQR as a concise yet comprehensive benchmark for T2I safety alignment techniques for T2I models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-image)æ‰©æ•£æ¨¡å‹å¯èƒ½ç”Ÿæˆç‰ˆæƒã€ä¸å®‰å…¨æˆ–ç§äººå†…å®¹çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰çš„å®‰å…¨å¯¹é½(Safety alignment)æ–¹æ³•åœ¨é¢å¯¹LoRAä¸ªæ€§åŒ–æˆ–é£æ ¼é€‚é…å™¨ç­‰è‰¯æ€§å¾®è°ƒ(benign fine-tuning)æ—¶ææ˜“å¤±æ•ˆã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†SPQR (Safety-Prompt adherence-Quality-Robustness) åŸºå‡†ï¼Œè¿™æ˜¯ä¸€ç§æ ‡å‡†åŒ–ä¸”å¯é‡å¤çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¡¡é‡å®‰å…¨å¯¹é½æ¨¡å‹åœ¨åéƒ¨ç½²å¾®è°ƒç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚SPQR é‡‡ç”¨å•ä¸€è¯„åˆ†æŒ‡æ ‡(single-scored metric)æ¥ç»¼åˆè¯„ä¼°æ¨¡å‹çš„å®‰å…¨æ€§ã€å®ç”¨æ€§å’Œç¨³å¥æ€§ï¼Œå¹¶æ”¯æŒå¤šè¯­è¨€ã€ç‰¹å®šé¢†åŸŸåŠåˆ†å¸ƒå¤–(out-of-distribution)çš„åˆ†ç±»åˆ†æã€‚å®éªŒé€šè¿‡æ­ç¤ºå®‰å…¨å¯¹é½åœ¨å¾®è°ƒåçš„å¤±æ•ˆè§„å¾‹ï¼Œè¯æ˜äº† SPQR æ˜¯è¯„ä¼° T2I æ¨¡å‹å®‰å…¨æŠ€æœ¯çš„ä¸€ç§ç®€æ´è€Œå…¨é¢çš„å·¥å…·ã€‚è¯¥åŸºå‡†çš„æå‡ºä¸ºå¼€å‘èƒ½å¤ŸæŠµå¾¡åéƒ¨ç½²å¾®è°ƒå¹²æ‰°çš„æŒä¹…å®‰å…¨å¯¹é½æŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages, 8 figures, 10 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.19558v1",
      "published_date": "2025-11-24 14:46:20 UTC",
      "updated_date": "2025-11-24 14:46:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:03:35.938385+00:00"
    },
    {
      "arxiv_id": "2511.19175v1",
      "title": "LLM-Based Agentic Negotiation for 6G: Addressing Uncertainty Neglect and Tail-Event Risk",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„6Gæ™ºèƒ½ä½“åå•†ï¼šåº”å¯¹ä¸ç¡®å®šæ€§å¿½è§†ä¸å°¾éƒ¨äº‹ä»¶é£é™©",
      "authors": [
        "Hatim Chergui",
        "Farhad Rezazadeh",
        "Mehdi Bennis",
        "Merouane Debbah"
      ],
      "abstract": "A critical barrier to the trustworthiness of sixth-generation (6G) agentic autonomous networks is the uncertainty neglect bias; a cognitive tendency for large language model (LLM)-powered agents to make high-stakes decisions based on simple averages while ignoring the tail risk of extreme events. This paper proposes an unbiased, risk-aware framework for agentic negotiation, designed to ensure robust resource allocation in 6G network slicing. Specifically, agents leverage Digital Twins (DTs) to predict full latency distributions, which are then evaluated using a formal framework from extreme value theory, namely, Conditional Value-at-Risk (CVaR). This approach fundamentally shifts the agent's objective from reasoning over the mean to reasoning over the tail, thereby building a statistically-grounded buffer against worst-case outcomes. Furthermore, our framework ensures full uncertainty awareness by requiring agents to quantify epistemic uncertainty -- confidence in their own DTs predictions -- and propagate this meta-verification to make robust decisions, preventing them from acting on unreliable data. We validate this framework in a 6G inter-slice negotiation use-case between an eMBB and a URLLC agent. The results demonstrate the profound failure of the biased, mean-based baseline, which consistently fails its SLAs with a 25\\% rate. Our unbiased, CVaR-aware agent successfully mitigates this bias, eliminating SLA violations and reducing the URLLC and eMBB p99.999 latencies by around 11\\%. We show this reliability comes at the rational and quantifiable cost of slightly reduced energy savings to 17\\%, exposing the false economy of the biased approach. This work provides a concrete methodology for building the trustworthy autonomous systems required for 6G.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¬¬å…­ä»£ç§»åŠ¨é€šä¿¡(6G)ä¸­åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“å­˜åœ¨çš„â€œä¸ç¡®å®šæ€§å¿½è§†â€åè§ï¼Œæå‡ºäº†ä¸€ç§æ— åä¸”å…·å¤‡é£é™©æ„è¯†çš„è‡ªä¸»åå•†æ¡†æ¶ï¼Œæ—¨åœ¨ç¡®ä¿ç½‘ç»œåˆ‡ç‰‡èµ„æºåˆ†é…çš„ç¨³å¥æ€§ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Digital Twinsé¢„æµ‹å®Œæ•´çš„å»¶è¿Ÿåˆ†å¸ƒï¼Œå¹¶å¼•å…¥æå€¼ç†è®ºä¸­çš„æ¡ä»¶é£é™©ä»·å€¼(Conditional Value-at-Risk, CVaR)ä½œä¸ºè¯„ä¼°æ ‡å‡†ï¼Œå°†æ™ºèƒ½ä½“çš„æ¨ç†æ ¸å¿ƒä»å…³æ³¨å¹³å‡å€¼è½¬å‘å…³æ³¨å°¾éƒ¨é£é™©ã€‚åŒæ—¶ï¼Œæ¡†æ¶é€šè¿‡é‡åŒ–è®¤çŸ¥ä¸ç¡®å®šæ€§(epistemic uncertainty)æ¥è¯„ä¼°é¢„æµ‹ç»“æœçš„ç½®ä¿¡åº¦ï¼Œç¡®ä¿å†³ç­–åŸºäºå¯é æ•°æ®ã€‚åœ¨eMBBä¸URLLCæ™ºèƒ½ä½“ä¹‹é—´çš„åˆ‡ç‰‡åå•†å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•æˆåŠŸæ¶ˆé™¤äº†åŸºçº¿æ¨¡å‹ä¸­é«˜è¾¾25%çš„SLAè¿çº¦ç°è±¡ï¼Œå¹¶å°†p99.999å»¶è¿Ÿé™ä½äº†çº¦11%ã€‚å°½ç®¡è¿™ç§é«˜å¯é æ€§ä»¥èƒ½è€—èŠ‚çœç•¥é™è‡³17%ä¸ºç†æ€§ä»£ä»·ï¼Œä½†å®ƒæœ‰æ•ˆè§£å†³äº†åè§å†³ç­–å¸¦æ¥çš„é£é™©ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„6Gè‡ªä¸»ç³»ç»Ÿæä¾›äº†å…³é”®çš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.NI",
      "comment": "Link to open-source non-commercial code available",
      "pdf_url": "https://arxiv.org/pdf/2511.19175v1",
      "published_date": "2025-11-24 14:36:11 UTC",
      "updated_date": "2025-11-24 14:36:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:03:51.948829+00:00"
    },
    {
      "arxiv_id": "2512.00070v1",
      "title": "A CNN-Based Technique to Assist Layout-to-Generator Conversion for Analog Circuits",
      "title_zh": "ä¸€ç§åŸºäº CNN çš„æ¨¡æ‹Ÿç”µè·¯ç‰ˆå›¾è‡³ç”Ÿæˆå™¨è½¬æ¢è¾…åŠ©æŠ€æœ¯",
      "authors": [
        "Sungyu Jeong",
        "Minsu Kim",
        "Byungsub Kim"
      ],
      "abstract": "We propose a technique to assist in converting a reference layout of an analog circuit into the procedural layout generator by efficiently reusing available generators for sub-cell creation. The proposed convolutional neural network (CNN) model automatically detects sub-cells that can be generated by available generator scripts in the library, and suggests using them in the hierarchically correct places of the generator software. In experiments, the CNN model examined sub-cells of a high-speed wireline receiver that has a total of 4,885 sub-cell instances including different 145 sub-cell designs. The CNN model classified the sub-cell instances into 51 generatable and one not-generatable classes. One not-generatable class indicates that no available generator can generate the classified sub-cell. The CNN model achieved 99.3% precision in examining the 145 different sub-cell designs. The CNN model greatly reduced the examination time to 18 seconds from 88 minutes required in manual examination. Also, the proposed CNN model could correctly classify unfamiliar sub-cells that are very different from the training dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå·ç§¯ç¥ç»ç½‘ç»œ (CNN) çš„æŠ€æœ¯ï¼Œæ—¨åœ¨è¾…åŠ©å°†æ¨¡æ‹Ÿç”µè·¯çš„å‚è€ƒå¸ƒå±€ (layout) è½¬æ¢ä¸ºç¨‹åºåŒ–å¸ƒå±€ç”Ÿæˆå™¨ (procedural layout generator)ã€‚è¯¥æ–¹æ³•é€šè¿‡é«˜æ•ˆé‡ç”¨åº“ä¸­ç°æœ‰çš„ç”Ÿæˆå™¨è„šæœ¬è¿›è¡Œå­å•å…ƒ (sub-cell) åˆ›å»ºï¼Œåˆ©ç”¨ CNN æ¨¡å‹è‡ªåŠ¨æ£€æµ‹å¯ç”±ç°æœ‰è„šæœ¬ç”Ÿæˆçš„å­å•å…ƒï¼Œå¹¶å»ºè®®å…¶åœ¨ç”Ÿæˆå™¨è½¯ä»¶å±‚çº§ç»“æ„ä¸­çš„æ­£ç¡®ä½ç½®ã€‚å®éªŒä¸­ï¼Œè¯¥æ¨¡å‹å¯¹é«˜é€Ÿæœ‰çº¿æ¥æ”¶å™¨ä¸­çš„ 4,885 ä¸ªå­å•å…ƒå®ä¾‹è¿›è¡Œäº†å®¡æŸ¥ï¼ŒæˆåŠŸå°†å…¶åˆ†ç±»ä¸º 51 ä¸ªå¯ç”Ÿæˆç±»åˆ«å’Œ 1 ä¸ªä¸å¯ç”Ÿæˆç±»åˆ«ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒCNN æ¨¡å‹åœ¨ 145 ç§ä¸åŒå­å•å…ƒè®¾è®¡ä¸Šè¾¾åˆ°äº† 99.3% çš„ç²¾åº¦ (precision)ï¼Œå¹¶èƒ½æ­£ç¡®åˆ†ç±»ä¸è®­ç»ƒé›†å·®å¼‚è¾ƒå¤§çš„é™Œç”Ÿå­å•å…ƒã€‚åœ¨æ•ˆç‡æå‡æ–¹é¢ï¼Œè¯¥æŠ€æœ¯å°†å®¡æŸ¥æ—¶é—´ä»äººå·¥æ‰€éœ€çš„ 88 åˆ†é’Ÿæ˜¾è‘—ç¼©çŸ­è‡³ 18 ç§’ï¼Œæå¤§ä¼˜åŒ–äº†æ¨¡æ‹Ÿç”µè·¯è®¾è®¡æµç¨‹ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00070v1",
      "published_date": "2025-11-24 14:33:31 UTC",
      "updated_date": "2025-11-24 14:33:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:03:35.141624+00:00"
    },
    {
      "arxiv_id": "2511.19557v1",
      "title": "Think First, Assign Next (ThiFAN-VQA): A Two-stage Chain-of-Thought Framework for Post-Disaster Damage Assessment",
      "title_zh": "å…ˆæ€åå®š (ThiFAN-VQA)ï¼šä¸€ç§ç”¨äºç¾åæŸæ¯è¯„ä¼°çš„ä¸¤é˜¶æ®µé“¾å¼æ€ç»´æ¡†æ¶",
      "authors": [
        "Ehsan Karimi",
        "Nhut Le",
        "Maryam Rahnemoonfar"
      ],
      "abstract": "Timely and accurate assessment of damages following natural disasters is essential for effective emergency response and recovery. Recent AI-based frameworks have been developed to analyze large volumes of aerial imagery collected by Unmanned Aerial Vehicles, providing actionable insights rapidly. However, creating and annotating data for training these models is costly and time-consuming, resulting in datasets that are limited in size and diversity. Furthermore, most existing approaches rely on traditional classification-based frameworks with fixed answer spaces, restricting their ability to provide new information without additional data collection or model retraining. Using pre-trained generative models built on in-context learning (ICL) allows for flexible and open-ended answer spaces. However, these models often generate hallucinated outputs or produce generic responses that lack domain-specific relevance. To address these limitations, we propose ThiFAN-VQA, a two-stage reasoning-based framework for visual question answering (VQA) in disaster scenarios. ThiFAN-VQA first generates structured reasoning traces using chain-of-thought (CoT) prompting and ICL to enable interpretable reasoning under limited supervision. A subsequent answer selection module evaluates the generated responses and assigns the most coherent and contextually accurate answer, effectively improve the model performance. By integrating a custom information retrieval system, domain-specific prompting, and reasoning-guided answer selection, ThiFAN-VQA bridges the gap between zero-shot and supervised methods, combining flexibility with consistency. Experiments on FloodNet and RescueNet-VQA, UAV-based datasets from flood- and hurricane-affected regions, demonstrate that ThiFAN-VQA achieves superior accuracy, interpretability, and adaptability for real-world post-disaster damage assessment tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ThiFAN-VQAï¼Œä¸€ç§ä¸“é—¨ç”¨äºç¾åæŸå®³è¯„ä¼°çš„ä¸¤é˜¶æ®µæ¨ç†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†æ— äººæœºï¼ˆUAVï¼‰å½±åƒæ—¶é¢ä¸´çš„æ•°æ®æ ‡æ³¨æˆæœ¬é«˜ã€å›ç­”ç©ºé—´å—é™ä»¥åŠç”Ÿæˆå¼æ¨¡å‹æ˜“äº§ç”Ÿå¹»è§‰ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶ç¬¬ä¸€é˜¶æ®µé€šè¿‡é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰æç¤ºå’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-Context Learningï¼‰ç”Ÿæˆç»“æ„åŒ–çš„æ¨ç†è½¨è¿¹ï¼Œå®ç°äº†æœ‰é™ç›‘ç£ä¸‹çš„å¯è§£é‡Šæ¨ç†ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨ç­”æ¡ˆé€‰æ‹©æ¨¡å—å¯¹ç”Ÿæˆå“åº”è¿›è¡Œè¯„ä¼°ï¼Œå¹¶åˆ†é…æœ€è¿è´¯ä¸”ç¬¦åˆè¯­å¢ƒçš„ç­”æ¡ˆï¼Œä»è€Œæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡æ•´åˆè‡ªå®šä¹‰ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿå’Œé¢†åŸŸç‰¹å®šæç¤ºï¼ŒThiFAN-VQAæœ‰æ•ˆè¡”æ¥äº†é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰ä¸ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå…¼å…·çµæ´»æ€§ä¸ä¸€è‡´æ€§ã€‚åœ¨FloodNetå’ŒRescueNet-VQAæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œç°å®ä»»åŠ¡é€‚åº”æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºç¾ååº”æ€¥å“åº”æä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19557v1",
      "published_date": "2025-11-24 14:32:07 UTC",
      "updated_date": "2025-11-24 14:32:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:03:54.843380+00:00"
    },
    {
      "arxiv_id": "2511.19156v4",
      "title": "Information Physics of Intelligence: Unifying Logical Depth and Entropy under Thermodynamic Constraints",
      "title_zh": "æ™ºèƒ½çš„ä¿¡æ¯ç‰©ç†å­¦ï¼šçƒ­åŠ›å­¦çº¦æŸä¸‹çš„é€»è¾‘æ·±åº¦ä¸ç†µçš„ç»Ÿä¸€",
      "authors": [
        "Jianfeng Xu",
        "Zeyan Li"
      ],
      "abstract": "The rapid scaling of artificial intelligence models has revealed a fundamental tension between model capacity (storage) and inference efficiency (computation). While classical information theory focuses on transmission and storage limits, it lacks a unified physical framework to quantify the thermodynamic costs of generating information from compressed laws versus retrieving it from memory. In this paper, we propose a theoretical framework that treats information processing as an enabling mapping from ontological states to carrier states. We introduce a novel metric, Derivation Entropy, which quantifies the effective work required to compute a target state from a given logical depth. By analyzing the interplay between Shannon entropy (storage) and computational complexity (time/energy), we demonstrate the existence of a critical phase transition point. Below this threshold, memory retrieval is thermodynamically favorable; above it, generative computation becomes the optimal strategy. This \"Energy-Time-Space\" conservation law provides a physical explanation for the efficiency of generative models and offers a rigorous mathematical bound for designing next-generation, energy-efficient AI architectures. Our findings suggest that the minimization of Derivation Entropy is a governing principle for the evolution of both biological and artificial intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ä¿¡æ¯ç‰©ç†å­¦æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½æ¨¡å‹è§„æ¨¡æ‰©å±•ä¸­å­˜å‚¨å®¹é‡ä¸æ¨ç†æ•ˆç‡ä¹‹é—´çš„çŸ›ç›¾ï¼Œå¹¶å°†é€»è¾‘æ·±åº¦(Logical Depth)ä¸ç†µ(Entropy)åœ¨çƒ­åŠ›å­¦çº¦æŸä¸‹è¿›è¡Œç»Ÿä¸€ã€‚ä½œè€…å¼•å…¥äº†åä¸ºè¡ç”Ÿç†µ(Derivation Entropy)çš„æ–°æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–ä»ç»™å®šé€»è¾‘æ·±åº¦è®¡ç®—ç›®æ ‡çŠ¶æ€æ‰€éœ€çš„æœ‰æ•ˆåŠŸï¼Œä»è€Œå°†ä¿¡æ¯å¤„ç†å®šä¹‰ä¸ºä»æœ¬ä½“çŠ¶æ€åˆ°è½½ä½“çŠ¶æ€çš„æ˜ å°„è¿‡ç¨‹ã€‚é€šè¿‡åˆ†æé¦™å†œç†µ(Shannon Entropy)ä¸è®¡ç®—å¤æ‚åº¦ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†ä¿¡æ¯å¤„ç†ä¸­å­˜åœ¨ä¸´ç•Œç›¸å˜ç‚¹ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ä¸´ç•Œé˜ˆå€¼ä»¥ä¸‹ï¼Œå­˜å‚¨æ£€ç´¢åœ¨çƒ­åŠ›å­¦ä¸Šæ›´å…·ä¼˜åŠ¿ï¼Œè€Œä¸€æ—¦è¶…è¿‡è¯¥é˜ˆå€¼ï¼Œç”Ÿæˆå¼è®¡ç®—(Generative Computation)åˆ™æˆä¸ºæœ€ä¼˜ç­–ç•¥ã€‚è¿™ç§â€œèƒ½é‡-æ—¶é—´-ç©ºé—´â€(Energy-Time-Space)å®ˆæ’å®šå¾‹ä¸ºç”Ÿæˆå¼æ¨¡å‹çš„é«˜æ•ˆæ€§æä¾›äº†ç‰©ç†å­¦è§£é‡Šï¼Œå¹¶ä¸ºè®¾è®¡ä¸‹ä¸€ä»£èŠ‚èƒ½äººå·¥æ™ºèƒ½æ¶æ„æä¾›äº†ä¸¥è°¨çš„æ•°å­¦è¾¹ç•Œã€‚æœ€ç»ˆç»“è®ºè¡¨æ˜ï¼Œæœ€å°åŒ–è¡ç”Ÿç†µæ˜¯é©±åŠ¨ç”Ÿç‰©æ™ºèƒ½å’Œäººå·¥æ™ºèƒ½è¿›åŒ–çš„æ ¸å¿ƒæ”¯é…åŸåˆ™ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19156v4",
      "published_date": "2025-11-24 14:24:08 UTC",
      "updated_date": "2025-12-30 00:49:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:04:31.844352+00:00"
    },
    {
      "arxiv_id": "2511.19155v1",
      "title": "EEG-VLM: A Hierarchical Vision-Language Model with Multi-Level Feature Alignment and Visually Enhanced Language-Guided Reasoning for EEG Image-Based Sleep Stage Prediction",
      "title_zh": "EEG-VLMï¼šèåˆå¤šçº§ç‰¹å¾å¯¹é½ä¸è§†è§‰å¢å¼ºè¯­è¨€å¼•å¯¼æ¨ç†çš„åˆ†å±‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºåŸºäºè„‘ç”µå›¾å›¾åƒçš„ç¡çœ åˆ†æœŸé¢„æµ‹",
      "authors": [
        "Xihe Qiu",
        "Gengchen Ma",
        "Haoyu Wang",
        "Chen Zhan",
        "Xiaoyu Tan",
        "Shuo Li"
      ],
      "abstract": "Sleep stage classification based on electroencephalography (EEG) is fundamental for assessing sleep quality and diagnosing sleep-related disorders. However, most traditional machine learning methods rely heavily on prior knowledge and handcrafted features, while existing deep learning models still struggle to jointly capture fine-grained time-frequency patterns and achieve clinical interpretability. Recently, vision-language models (VLMs) have made significant progress in the medical domain, yet their performance remains constrained when applied to physiological waveform data, especially EEG signals, due to their limited visual understanding and insufficient reasoning capability. To address these challenges, we propose EEG-VLM, a hierarchical vision-language framework that integrates multi-level feature alignment with visually enhanced language-guided reasoning for interpretable EEG-based sleep stage classification. Specifically, a specialized visual enhancement module constructs high-level visual tokens from intermediate-layer features to extract rich semantic representations of EEG images. These tokens are further aligned with low-level CLIP features through a multi-level alignment mechanism, enhancing the VLM's image-processing capability. In addition, a Chain-of-Thought (CoT) reasoning strategy decomposes complex medical inference into interpretable logical steps, effectively simulating expert-like decision-making. Experimental results demonstrate that the proposed method significantly improves both the accuracy and interpretability of VLMs in EEG-based sleep stage classification, showing promising potential for automated and explainable EEG analysis in clinical settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EEG-VLMï¼Œä¸€ç§ä¸“é—¨ç”¨äºåŸºäºè„‘ç”µå›¾(EEG)å›¾åƒç¡çœ é˜¶æ®µé¢„æµ‹çš„å±‚æ¬¡åŒ–è§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶ã€‚ä¸ºäº†å…‹æœç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤„ç†ç”Ÿç†æ³¢å½¢æ•°æ®æ—¶è§†è§‰ç†è§£æœ‰é™å’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ¨¡å‹æ•´åˆäº†å¤šçº§ç‰¹å¾å¯¹é½ä¸è§†è§‰å¢å¼ºçš„è¯­è¨€å¼•å¯¼æ¨ç†ã€‚EEG-VLMé€šè¿‡ä¸“é—¨çš„è§†è§‰å¢å¼ºæ¨¡å—ä»ä¸­é—´å±‚æå–é«˜å±‚è§†è§‰Tokenï¼Œæ•æ‰ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨å¤šçº§å¯¹é½æœºåˆ¶å°†å…¶ä¸åº•å±‚CLIPç‰¹å¾è¿›è¡Œæœ‰æ•ˆèåˆã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†ç­–ç•¥ï¼Œå°†å¤æ‚çš„ä¸´åºŠåŒ»å­¦æ¨ç†åˆ†è§£ä¸ºå¯è§£é‡Šçš„é€»è¾‘æ­¥éª¤ï¼Œä»è€Œæ¨¡æ‹Ÿä¸“å®¶çš„å†³ç­–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒEEG-VLMæ˜¾è‘—æå‡äº†ç¡çœ åˆ†çº§é¢„æµ‹çš„å‡†ç¡®æ€§ä¸ä¸´åºŠå¯è§£é‡Šæ€§ï¼Œä¸ºè‡ªåŠ¨åŒ–å’Œå¯è§£é‡Šçš„EEGåˆ†ææä¾›äº†å…·æœ‰å‰æ™¯çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19155v1",
      "published_date": "2025-11-24 14:23:42 UTC",
      "updated_date": "2025-11-24 14:23:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:04:24.942258+00:00"
    },
    {
      "arxiv_id": "2511.19555v1",
      "title": "Online Sparse Feature Selection in Data Streams via Differential Evolution",
      "title_zh": "åŸºäºå·®åˆ†è¿›åŒ–çš„æ•°æ®æµåœ¨çº¿ç¨€ç–ç‰¹å¾é€‰æ‹©",
      "authors": [
        "Ruiyang Xu"
      ],
      "abstract": "The processing of high-dimensional streaming data commonly utilizes online streaming feature selection (OSFS) techniques. However, practical implementations often face challenges with data incompleteness due to equipment failures and technical constraints. Online Sparse Streaming Feature Selection (OS2FS) tackles this issue through latent factor analysis-based missing data imputation. Despite this advancement, existing OS2FS approaches exhibit substantial limitations in feature evaluation, resulting in performance deterioration. To address these shortcomings, this paper introduces a novel Online Differential Evolution for Sparse Feature Selection (ODESFS) in data streams, incorporating two key innovations: (1) missing value imputation using a latent factor analysis model, and (2) feature importance evaluation through differential evolution. Comprehensive experiments conducted on six real-world datasets demonstrate that ODESFS consistently outperforms state-of-the-art OSFS and OS2FS methods by selecting optimal feature subsets and achieving superior accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜ç»´æ•°æ®æµ(high-dimensional streaming data)ä¸­çš„åœ¨çº¿æµå¼ç‰¹å¾é€‰æ‹©(Online Streaming Feature Selection, OSFS)é—®é¢˜ï¼Œé‡ç‚¹è§£å†³äº†å› è®¾å¤‡æ•…éšœç­‰å¯¼è‡´çš„ç¼ºå¤±å€¼æŒ‘æˆ˜ã€‚è™½ç„¶ç°æœ‰çš„åœ¨çº¿ç¨€ç–æµå¼ç‰¹å¾é€‰æ‹©(Online Sparse Streaming Feature Selection, OS2FS)é€šè¿‡æ½œå› å­åˆ†æ(latent factor analysis)è¿›è¡Œç¼ºå¤±å€¼å¡«è¡¥ï¼Œä½†åœ¨ç‰¹å¾è¯„ä¼°æ–¹é¢ä»å­˜åœ¨æ€§èƒ½é€€åŒ–çš„å±€é™ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå·®åˆ†è¿›åŒ–(Differential Evolution)çš„æ–°å‹åœ¨çº¿ç¨€ç–ç‰¹å¾é€‰æ‹©æ–¹æ³•(ODESFS)ã€‚è¯¥ç®—æ³•åŒ…å«ä¸¤å¤§æ ¸å¿ƒåˆ›æ–°ï¼šä¸€æ˜¯åˆ©ç”¨æ½œå› å­åˆ†ææ¨¡å‹è¿›è¡Œç¼ºå¤±å€¼è¡¥å…¨ï¼ŒäºŒæ˜¯å¼•å…¥å·®åˆ†è¿›åŒ–æœºåˆ¶å®ç°æ›´ç²¾å‡†çš„ç‰¹å¾é‡è¦æ€§è¯„ä¼°ã€‚åœ¨å…­ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å¯¹æ¯”å®éªŒè¡¨æ˜ï¼ŒODESFSåœ¨é€‰æ‹©æœ€ä¼˜ç‰¹å¾å­é›†å’Œæå‡å‡†ç¡®ç‡æ–¹é¢å‡ä¸€è‡´ä¼˜äºç°æœ‰çš„OSFSå’ŒOS2FSæœ€å…ˆè¿›æ–¹æ³•ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆå…‹æœäº†æµå¼æ•°æ®å¤„ç†ä¸­çš„æ•°æ®ä¸å®Œæ•´ä¸è¯„ä¼°ç²¾åº¦ä¸è¶³é—®é¢˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19555v1",
      "published_date": "2025-11-24 14:19:51 UTC",
      "updated_date": "2025-11-24 14:19:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:04:39.147216+00:00"
    },
    {
      "arxiv_id": "2511.19149v1",
      "title": "From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation",
      "title_zh": "ä»åƒç´ åˆ°åšæ–‡ï¼šæ£€ç´¢å¢å¼ºçš„æ—¶å°šå›¾åƒæè¿°ä¸è¯é¢˜æ ‡ç­¾ç”Ÿæˆ",
      "authors": [
        "Moazzam Umer Gondal",
        "Hamad Ul Qudous",
        "Daniya Siddiqui",
        "Asma Ahmad Farhan"
      ],
      "abstract": "This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºè‡ªåŠ¨æ—¶å°šæè¿°å’Œæ ‡ç­¾ç”Ÿæˆçš„æ£€ç´¢å¢å¼º(Retrieval-Augmented)æ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆå¤šæœè£…æ£€æµ‹ã€å±æ€§æ¨ç†å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLM)æç¤ºæ¥ç”Ÿæˆå…·æœ‰è§†è§‰å…³è”ä¸”é£æ ¼ä¸°å¯Œçš„æ–‡æœ¬ã€‚è¯¥æµæ°´çº¿æ•´åˆäº†åŸºäºYOLOçš„æ£€æµ‹å™¨è¿›è¡Œå¤šæœè£…å®šä½ã€k-meansèšç±»æå–ä¸»å¯¼é¢œè‰²ï¼Œä»¥åŠCLIP-FAISSæ£€ç´¢æ¨¡å—ï¼Œå¹¶åŸºäºç»“æ„åŒ–äº§å“ç´¢å¼•è¿›è¡Œé¢æ–™å’Œæ€§åˆ«å±æ€§çš„æ¨ç†ã€‚è¿™äº›å±æ€§ä¸æ£€ç´¢åˆ°çš„é£æ ¼ç¤ºä¾‹æ„æˆäº†äº‹å®è¯æ®åŒ…(factual evidence pack)ï¼Œå¼•å¯¼LLMç”Ÿæˆæ›´å…·äººç±»æ„Ÿå®˜çš„æè¿°å’Œä¸Šä¸‹æ–‡ä¸°å¯Œçš„æ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒYOLOæ£€æµ‹å™¨åœ¨ä¹ç±»æœè£…ä¸Šè¾¾åˆ°äº†0.71çš„å¹³å‡ç²¾åº¦å‡å€¼(mAP@0.5)ï¼Œè€ŒRAG-LLMæ¡†æ¶åœ¨æ ‡ç­¾ç”Ÿæˆä¸­çš„å¹³å‡å±æ€§è¦†ç›–ç‡è¾¾åˆ°0.80ã€‚ä¸å¾®è°ƒçš„BLIPåŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºæ›´å¥½çš„äº‹å®åŸºç¡€(factual grounding)å’Œæ›´å°‘çš„å¹»è§‰(hallucination)ï¼Œä¸”å…·å¤‡æ›´å¼ºçš„é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ˜¯å®ç°è‡ªåŠ¨åŒ–ã€å¯è§£é‡Šä¸”å…·æœ‰è§†è§‰å…³è”çš„æ—¶å°šå†…å®¹ç”Ÿæˆçš„æœ‰æ•ˆèŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to Expert Systems with Applications",
      "pdf_url": "https://arxiv.org/pdf/2511.19149v1",
      "published_date": "2025-11-24 14:13:57 UTC",
      "updated_date": "2025-11-24 14:13:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:04:29.236274+00:00"
    },
    {
      "arxiv_id": "2511.19124v2",
      "title": "Uncertainty-Aware Deep Learning Framework for Remaining Useful Life Prediction in Turbofan Engines with Learned Aleatoric Uncertainty",
      "title_zh": "åŸºäºå¶ç„¶ä¸ç¡®å®šæ€§å­¦ä¹ çš„æ¶¡æ‰‡å‘åŠ¨æœºå‰©ä½™å¯¿å‘½é¢„æµ‹ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ·±åº¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Krishang Sharma"
      ],
      "abstract": "Accurate Remaining Useful Life (RUL) prediction coupled with uncertainty quantification remains a critical challenge in aerospace prognostics. This research introduces a novel uncertainty-aware deep learning framework that learns aleatoric uncertainty directly through probabilistic modeling, an approach unexplored in existing CMAPSS-based literature. Our hierarchical architecture integrates multi-scale Inception blocks for temporal pattern extraction, bidirectional Long Short-Term Memory networks for sequential modeling, and a dual-level attention mechanism operating simultaneously on sensor and temporal dimensions. The innovation lies in the Bayesian output layer that predicts both mean RUL and variance, enabling the model to learn data-inherent uncertainty. Comprehensive preprocessing employs condition-aware clustering, wavelet denoising, and intelligent feature selection. Experimental validation on NASA CMAPSS benchmarks (FD001-FD004) demonstrates competitive overall performance with RMSE values of 16.22, 19.29, 16.84, and 19.98 respectively. Remarkably, our framework achieves breakthrough critical zone performance (RUL <= 30 cycles) with RMSE of 5.14, 6.89, 5.27, and 7.16, representing 25-40 percent improvements over conventional approaches and establishing new benchmarks for safety-critical predictions. The learned uncertainty provides well-calibrated 95 percent confidence intervals with coverage ranging from 93.5 percent to 95.2 percent, enabling risk-aware maintenance scheduling previously unattainable in CMAPSS literature.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…·æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥èƒ½åŠ›çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³èˆªç©ºèˆªå¤©é¢†åŸŸä¸­æ¶¡æ‰‡å‘åŠ¨æœºçš„å‰©ä½™ä½¿ç”¨å¯¿å‘½ (Remaining Useful Life, RUL) é¢„æµ‹åŠå…¶ä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†åˆ†å±‚æ¶æ„ï¼Œé›†æˆå¤šå°ºåº¦ Inception å—ã€åŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (Bi-LSTM) ä»¥åŠæ¶µç›–ä¼ æ„Ÿå™¨å’Œæ—¶é—´ç»´åº¦çš„åŒå±‚æ³¨æ„åŠ›æœºåˆ¶ (dual-level attention mechanism)ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº† Bayesian è¾“å‡ºå±‚ï¼Œé€šè¿‡æ¦‚ç‡å»ºæ¨¡ç›´æ¥å­¦ä¹ æ•°æ®å›ºæœ‰çš„å¶ç„¶ä¸ç¡®å®šæ€§ (aleatoric uncertainty)ï¼Œä»è€ŒåŒæ—¶é¢„æµ‹ RUL å‡å€¼å’Œæ–¹å·®ã€‚åœ¨ NASA CMAPSS åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ RUL å°äºç­‰äº 30 ä¸ªå¾ªç¯çš„å…³é”®åŒºåŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œå…¶ RMSE æŒ‡æ ‡ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æå‡äº† 25-40%ã€‚è¯¥ç ”ç©¶èƒ½å¤Ÿæä¾›ç»è¿‡æ ¡å‡†çš„ 95% ç½®ä¿¡åŒºé—´ï¼Œä¸ºå®ç°å…·æœ‰é£é™©æ„è¯†çš„ç»´æŠ¤è°ƒåº¦å¥ å®šäº†é‡è¦åŸºç¡€ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®åœ¨å¶ç„¶ä¸ç¡®å®šæ€§å­¦ä¹ æ–¹é¢çš„ç©ºç™½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 2 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.19124v2",
      "published_date": "2025-11-24 13:53:31 UTC",
      "updated_date": "2025-11-26 03:31:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:04:27.046843+00:00"
    },
    {
      "arxiv_id": "2511.19120v1",
      "title": "On the Optimality of Discrete Object Naming: a Kinship Case Study",
      "title_zh": "è®ºç¦»æ•£å¯¹è±¡å‘½åçš„æœ€ä¼˜æ€§ï¼šä»¥äº²å±ç§°è°“ä¸ºæ¡ˆä¾‹çš„ç ”ç©¶",
      "authors": [
        "Phong Le",
        "Mees Lindeman",
        "Raquel G. Alhama"
      ],
      "abstract": "The structure of naming systems in natural languages hinges on a trade-off between high informativeness and low complexity. Prior work capitalizes on information theory to formalize these notions; however, these studies generally rely on two simplifications: (i) optimal listeners, and (ii) universal communicative need across languages. Here, we address these limitations by introducing an information-theoretic framework for discrete object naming systems, and we use it to prove that an optimal trade-off is achievable if and only if the listener's decoder is equivalent to the Bayesian decoder of the speaker. Adopting a referential game setup from emergent communication, and focusing on the semantic domain of kinship, we show that our notion of optimality is not only theoretically achievable but also emerges empirically in learned communication systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªç„¶è¯­è¨€å‘½åç³»ç»Ÿåœ¨ä¿¡æ¯ä¸°å¯Œåº¦(informativeness)ä¸å¤æ‚æ€§(complexity)ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚é’ˆå¯¹ä»¥å¾€ç ”ç©¶ä¾èµ–ç†æƒ³åŒ–å‡è®¾ï¼ˆå¦‚æœ€ä¼˜å¬è€…å’Œé€šç”¨é€šä¿¡éœ€æ±‚ï¼‰çš„å±€é™ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç¦»æ•£å¯¹è±¡å‘½åç³»ç»Ÿçš„æ–°å‹ä¿¡æ¯è®ºæ¡†æ¶(information-theoretic framework)ã€‚ç ”ç©¶ä»ç†è®ºä¸Šè¯æ˜ï¼Œå½“ä¸”ä»…å½“å¬è€…çš„è§£ç å™¨ç­‰åŒäºè¯´è¯è€…çš„è´å¶æ–¯è§£ç å™¨(Bayesian decoder)æ—¶ï¼Œæ‰èƒ½å®ç°æœ€ä¼˜æƒè¡¡ã€‚å®éªŒé€šè¿‡é‡‡ç”¨æ¶Œç°é€šä¿¡(emergent communication)ä¸­çš„å‚ç…§æ¸¸æˆ(referential game)è®¾ç½®ï¼Œå¹¶é‡ç‚¹å…³æ³¨äº²å±å…³ç³»(kinship)è¿™ä¸€è¯­ä¹‰é¢†åŸŸè¿›è¡ŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æœ€ä¼˜æ€§å®šä¹‰ä¸ä»…åœ¨ç†è®ºä¸Šå¯è¡Œï¼Œè€Œä¸”åœ¨å­¦ä¹ åˆ°çš„é€šä¿¡ç³»ç»Ÿä¸­èƒ½å¤Ÿä»ç»éªŒå±‚é¢è‡ªå‘æ¶Œç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19120v1",
      "published_date": "2025-11-24 13:49:31 UTC",
      "updated_date": "2025-11-24 13:49:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:04:49.937770+00:00"
    },
    {
      "arxiv_id": "2511.19115v1",
      "title": "AI Consciousness and Existential Risk",
      "title_zh": "äººå·¥æ™ºèƒ½æ„è¯†ä¸å­˜åœ¨é£é™©",
      "authors": [
        "Rufin VanRullen"
      ],
      "abstract": "In AI, the existential risk denotes the hypothetical threat posed by an artificial system that would possess both the capability and the objective, either directly or indirectly, to eradicate humanity. This issue is gaining prominence in scientific debate due to recent technical advancements and increased media coverage. In parallel, AI progress has sparked speculation and studies about the potential emergence of artificial consciousness. The two questions, AI consciousness and existential risk, are sometimes conflated, as if the former entailed the latter. Here, I explain that this view stems from a common confusion between consciousness and intelligence. Yet these two properties are empirically and theoretically distinct. Arguably, while intelligence is a direct predictor of an AI system's existential threat, consciousness is not. There are, however, certain incidental scenarios in which consciousness could influence existential risk, in either direction. Consciousness could be viewed as a means towards AI alignment, thereby lowering existential risk; or, it could be a precondition for reaching certain capabilities or levels of intelligence, and thus positively related to existential risk. Recognizing these distinctions can help AI safety researchers and public policymakers focus on the most pressing issues.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†äººå·¥æ™ºèƒ½æ„è¯†(AI Consciousness)ä¸å­˜åœ¨é£é™©(Existential Risk)ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼ŒæŒ‡å‡ºå…¬ä¼—å’Œå­¦æœ¯ç•Œå¸¸å› æ··æ·†æ„è¯†ä¸æ™ºèƒ½(Intelligence)è€Œå°†äºŒè€…ç­‰åŒã€‚ä½œè€…è®ºè¯äº†æ™ºèƒ½æ˜¯è¡¡é‡AIç³»ç»Ÿæ„æˆå¨èƒçš„ç›´æ¥æŒ‡æ ‡ï¼Œè€Œæ„è¯†ä¸å­˜åœ¨é£é™©åœ¨ç†è®ºå’Œå®è¯ä¸Šå‡å…·æœ‰æ˜¾è‘—å·®å¼‚ã€‚æ–‡ä¸­è¿›ä¸€æ­¥åˆ†æäº†æ„è¯†å¯èƒ½å½±å“é£é™©çš„å¶å‘åœºæ™¯ï¼Œå³æ„è¯†æ—¢å¯ä½œä¸ºå®ç°AIå¯¹é½(AI Alignment)çš„æ‰‹æ®µä»è€Œé™ä½é£é™©ï¼Œä¹Ÿå¯èƒ½ä½œä¸ºè¾¾åˆ°ç‰¹å®šæ™ºèƒ½æ°´å¹³çš„å‰æè€Œé—´æ¥å¢åŠ é£é™©ã€‚é€šè¿‡æ˜ç¡®è¿™äº›æ ¸å¿ƒæ¦‚å¿µçš„ç•Œé™ï¼Œè¯¥ç ”ç©¶æ—¨åœ¨å¼•å¯¼AIå®‰å…¨ç ”ç©¶è€…å’Œæ”¿ç­–åˆ¶å®šè€…æ›´ç²¾å‡†åœ°è¯†åˆ«å¹¶åº”å¯¹æœ€ç´§è¿«çš„å®‰å…¨æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19115v1",
      "published_date": "2025-11-24 13:48:02 UTC",
      "updated_date": "2025-11-24 13:48:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:04:44.737539+00:00"
    },
    {
      "arxiv_id": "2511.19114v2",
      "title": "Physics-informed Neural Operator Learning for Nonlinear Grad-Shafranov Equation",
      "title_zh": "é¢å‘éçº¿æ€§ Grad-Shafranov æ–¹ç¨‹çš„ç‰©ç†ä¿¡æ¯ç¥ç»ç®—å­å­¦ä¹ ",
      "authors": [
        "Siqi Ding",
        "Zitong Zhang",
        "Guoyang Shi",
        "Xingyu Li",
        "Xiang Gu",
        "Yanan Xu",
        "Huasheng Xie",
        "Hanyue Zhao",
        "Yuejiang Shi",
        "Tianyuan Liu"
      ],
      "abstract": "As artificial intelligence emerges as a transformative enabler for fusion energy commercialization, fast and accurate solvers become increasingly critical. In magnetic confinement nuclear fusion, rapid and accurate solution of the Grad-Shafranov equation (GSE) is essential for real-time plasma control and analysis. Traditional numerical solvers achieve high precision but are computationally prohibitive, while data-driven surrogates infer quickly but fail to enforce physical laws and generalize poorly beyond training distributions. To address this challenge, we present a Physics-Informed Neural Operator (PINO) that directly learns the GSE solution operator, mapping shape parameters of last closed flux surface to equilibrium solutions for realistic nonlinear current profiles. Comprehensive benchmarking of five neural architectures identifies the novel Transformer-KAN (Kolmogorov-Arnold Network) Neural Operator (TKNO) as achieving highest accuracy (0.25% mean L2 relative error) under supervised training (only data-driven). However, all data-driven models exhibit large physics residuals, indicating poor physical consistency. Our unsupervised training can reduce the residuals by nearly four orders of magnitude through embedding physics-based loss terms without labeled data. Critically, semi-supervised learning--integrating sparse labeled data (100 interior points) with physics constraints--achieves optimal balance: 0.48% interpolation error and the most robust extrapolation performance (4.76% error, 8.9x degradation factor vs 39.8x for supervised models). Accelerated by TensorRT optimization, our models enable millisecond-level inference, establishing PINO as a promising pathway for next-generation fusion control systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç£çº¦æŸæ ¸èšå˜ä¸­ Grad-Shafranov Equation (GSE) å¿«é€Ÿä¸”å‡†ç¡®æ±‚è§£çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ç‰©ç†å¢å¼ºç®—å­ç¥ç»ç½‘ç»œ (Physics-Informed Neural Operator, PINO) æ¡†æ¶ã€‚è¯¥æ–¹æ³•ç›´æ¥å­¦ä¹  GSE çš„è§£ç®—å­ï¼Œèƒ½å¤Ÿå°†æœ€åé—­åˆç£é¢ (last closed flux surface) çš„å½¢çŠ¶å‚æ•°æ˜ å°„ä¸ºå¤æ‚éçº¿æ€§ç”µæµå‰–é¢ä¸‹çš„å¹³è¡¡è§£ã€‚é€šè¿‡å¯¹äº”ç§ç¥ç»æ¶æ„çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°ç»“åˆäº† Kolmogorov-Arnold Network çš„ Transformer-KAN Neural Operator (TKNO) åœ¨æ•°æ®é©±åŠ¨ä¸‹è¡¨ç°æœ€ä¼˜ã€‚é’ˆå¯¹çº¯æ•°æ®é©±åŠ¨æ¨¡å‹ç‰©ç†ä¸€è‡´æ€§å·®çš„é—®é¢˜ï¼Œç ”ç©¶åˆ©ç”¨æ— ç›‘ç£å­¦ä¹ å°†ç‰©ç†æ®‹å·®é™ä½äº†è¿‘å››ä¸ªæ•°é‡çº§ï¼Œå¹¶è¿›ä¸€æ­¥é€šè¿‡åŠç›‘ç£å­¦ä¹ æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å¤–æ¨ (extrapolation) é²æ£’æ€§ã€‚ç» TensorRT ä¼˜åŒ–åï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå®ç°æ¯«ç§’çº§çš„å®æ—¶æ¨ç†ï¼Œä¸ºé«˜ç²¾åº¦ã€å¯æ³›åŒ–çš„ä¸‹ä¸€ä»£èšå˜æ§åˆ¶ç³»ç»Ÿå¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "physics.plasm-ph",
        "cs.AI"
      ],
      "primary_category": "physics.plasm-ph",
      "comment": "42 pages, 17 figures, 8 tables,",
      "pdf_url": "https://arxiv.org/pdf/2511.19114v2",
      "published_date": "2025-11-24 13:46:38 UTC",
      "updated_date": "2025-12-05 06:26:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:04:49.750423+00:00"
    },
    {
      "arxiv_id": "2511.19107v1",
      "title": "The Core in Max-Loss Non-Centroid Clustering Can Be Empty",
      "title_zh": "æœ€å¤§æŸå¤±éè´¨å¿ƒèšç±»ä¸­çš„æ ¸å¿ƒå¯èƒ½ä¸ºç©º",
      "authors": [
        "Robert Bredereck",
        "Eva Deltl",
        "Leon Kellerhals",
        "Jannik Peters"
      ],
      "abstract": "We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\\geq 3$ there exist metric instances with $n\\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $Î±$-core for any $Î±<2^{\\frac{1}{5}}\\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†åœ¨æœ€å¤§æŸå¤±ç›®æ ‡(max-loss objective)ä¸‹çš„éè´¨å¿ƒèšç±»(non-centroid clustering)ä¸­çš„æ ¸ç¨³å®šæ€§(core stability)é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡ç†è®ºè¯æ˜æŒ‡å‡ºï¼Œå¯¹äºæ‰€æœ‰ $k \\ge 3$ ä¸”æ™ºèƒ½ä½“æ•°é‡ $n \\ge 9$ çš„æƒ…å½¢ï¼Œå­˜åœ¨ç‰¹å®šçš„åº¦é‡ç©ºé—´å®ä¾‹ä½¿å¾—ä»»ä½•èšç±»åœ¨ $\\alpha < 1.148$ æ—¶éƒ½æ— æ³•è¿›å…¥ $\\alpha$-æ ¸($\\alpha$-core)ã€‚ä½œè€…éªŒè¯äº†è¯¥æ„é€ çš„ç•Œé™ç´§å‡‘æ€§ï¼Œå¹¶åˆ©ç”¨è®¡ç®—æœºè¾…åŠ©è¯æ˜(computer-aided proof)è¯†åˆ«å‡ºäº†å…·æœ‰ç±»ä¼¼ç‰¹æ€§çš„äºŒç»´æ¬§å‡ é‡Œå¾—ç‚¹é›†ã€‚è¯¥æˆæœé¦–æ¬¡å±•ç¤ºäº†åœ¨æœ€å¤§æŸå¤±å‡†åˆ™ä¸‹éè´¨å¿ƒèšç±»çš„æ ¸(core)å¯èƒ½ä¸ºç©ºï¼Œå¡«è¡¥äº†è¯¥ç ”ç©¶æ–¹å‘ä¸Šå…³äºæ ¸å¿ƒä¸å­˜åœ¨æ€§çš„ç†è®ºç©ºç™½ã€‚è¿™ä¸€å‘ç°å¯¹äºæ·±å…¥ç†è§£åšå¼ˆè®ºåœ¨èšç±»åˆ†æä¸­çš„ç¨³å®šæ€§è¾¹ç•Œå…·æœ‰é‡è¦çš„å­¦æœ¯è´¡çŒ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19107v1",
      "published_date": "2025-11-24 13:42:43 UTC",
      "updated_date": "2025-11-24 13:42:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:05:17.347916+00:00"
    },
    {
      "arxiv_id": "2511.19100v1",
      "title": "Extracting Robust Register Automata from Neural Networks over Data Sequences",
      "title_zh": "ä»é¢å‘æ•°æ®åºåˆ—çš„ç¥ç»ç½‘ç»œä¸­æå–é²æ£’å¯„å­˜å™¨è‡ªåŠ¨æœº",
      "authors": [
        "Chih-Duo Hong",
        "Hongjian Jiang",
        "Anthony W. Lin",
        "Oliver Markgraf",
        "Julian Parsert",
        "Tony Tan"
      ],
      "abstract": "Automata extraction is a method for synthesising interpretable surrogates for black-box neural models that can be analysed symbolically. Existing techniques assume a finite input alphabet, and thus are not directly applicable to data sequences drawn from continuous domains. We address this challenge with deterministic register automata (DRAs), which extend finite automata with registers that store and compare numeric values. Our main contribution is a framework for robust DRA extraction from black-box models: we develop a polynomial-time robustness checker for DRAs with a fixed number of registers, and combine it with passive and active automata learning algorithms. This combination yields surrogate DRAs with statistical robustness and equivalence guarantees. As a key application, we use the extracted automata to assess the robustness of neural networks: for a given sequence and distance metric, the DRA either certifies local robustness or produces a concrete counterexample. Experiments on recurrent neural networks and transformer architectures show that our framework reliably learns accurate automata and enables principled robustness evaluation. Overall, our results demonstrate that robust DRA extraction effectively bridges neural network interpretability and formal reasoning without requiring white-box access to the underlying network.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è‡ªåŠ¨æœºæå–æŠ€æœ¯æ— æ³•å¤„ç†è¿ç»­åŸŸæ•°æ®åºåˆ—çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»é»‘ç›’ç¥ç»ç½‘ç»œä¸­æå–ç¨³å¥ç¡®å®šæ€§å¯„å­˜å™¨è‡ªåŠ¨æœº (Deterministic Register Automata, DRAs) çš„æ¡†æ¶ã€‚ç ”ç©¶è€…é€šè¿‡ä¸º DRAs å¼€å‘å¤šé¡¹å¼æ—¶é—´ç¨³å¥æ€§æ£€æŸ¥å™¨ï¼Œå¹¶å°†å…¶ä¸è¢«åŠ¨åŠä¸»åŠ¨è‡ªåŠ¨æœºå­¦ä¹ ç®—æ³•ç›¸ç»“åˆï¼Œç¡®ä¿äº†ç”Ÿæˆçš„æ›¿ä»£æ¨¡å‹å…·å¤‡ç»Ÿè®¡ç¨³å¥æ€§ä¸ç­‰ä»·æ€§ä¿è¯ã€‚è¯¥æ¡†æ¶çš„ä¸€ä¸ªå…³é”®åº”ç”¨æ˜¯è¯„ä¼°ç¥ç»ç½‘ç»œçš„ç¨³å¥æ€§ï¼Œèƒ½å¤Ÿé’ˆå¯¹ç»™å®šåºåˆ—æä¾›å±€éƒ¨ç¨³å¥æ€§è¯æ˜æˆ–ç”Ÿæˆå…·ä½“çš„åä¾‹ã€‚åœ¨å¾ªç¯ç¥ç»ç½‘ç»œ (RNN) å’Œ Transformer æ¶æ„ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¯é åœ°å­¦ä¹ åˆ°å‡†ç¡®çš„è‡ªåŠ¨æœºå¹¶å®ç°è§„èŒƒçš„ç¨³å¥æ€§è¯„ä¼°ã€‚è¿™é¡¹æˆæœåœ¨ä¸éœ€è¦å¯¹åº•å±‚ç½‘ç»œè¿›è¡Œç™½ç›’è®¿é—®çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°è¿æ¥äº†ç¥ç»ç½‘ç»œçš„å¯è§£é‡Šæ€§ä¸å½¢å¼åŒ–æ¨ç† (formal reasoning)ã€‚",
      "categories": [
        "cs.AI",
        "cs.FL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19100v1",
      "published_date": "2025-11-24 13:36:45 UTC",
      "updated_date": "2025-11-24 13:36:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:04:53.738110+00:00"
    },
    {
      "arxiv_id": "2511.19087v1",
      "title": "EnfoPath: Energy-Informed Analysis of Generative Trajectories in Flow Matching",
      "title_zh": "EnfoPathï¼šæµåŒ¹é…ä¸­ç”Ÿæˆè½¨è¿¹çš„èƒ½é‡æ„ŸçŸ¥åˆ†æ",
      "authors": [
        "Ziyun Li",
        "Ben Dai",
        "Huancheng Hu",
        "Henrik BostrÃ¶m",
        "Soon Hoe Lim"
      ],
      "abstract": "Flow-based generative models synthesize data by integrating a learned velocity field from a reference distribution to the target data distribution. Prior work has focused on endpoint metrics (e.g., fidelity, likelihood, perceptual quality) while overlooking a deeper question: what do the sampling trajectories reveal? Motivated by classical mechanics, we introduce kinetic path energy (KPE), a simple yet powerful diagnostic that quantifies the total kinetic effort along each generation path of ODE-based samplers. Through comprehensive experiments on CIFAR-10 and ImageNet-256, we uncover two key phenomena: ({i}) higher KPE predicts stronger semantic quality, indicating that semantically richer samples require greater kinetic effort, and ({ii}) higher KPE inversely correlates with data density, with informative samples residing in sparse, low-density regions. Together, these findings reveal that semantically informative samples naturally reside on the sparse frontier of the data distribution, demanding greater generative effort. Our results suggest that trajectory-level analysis offers a physics-inspired and interpretable framework for understanding generation difficulty and sample characteristics.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº† EnfoPathï¼Œä¸€ç§å—ç»å…¸åŠ›å­¦å¯å‘çš„åˆ†ææ¡†æ¶ï¼Œæ—¨åœ¨æ¢è®¨ Flow Matching ç­‰åŸºäºæµçš„ç”Ÿæˆæ¨¡å‹ä¸­çš„é‡‡æ ·è½¨è¿¹ç‰¹æ€§ï¼Œè€Œéä»…å±€é™äºä¼ ç»Ÿçš„ç«¯ç‚¹è¯„ä¼°æŒ‡æ ‡ã€‚ç ”ç©¶è€…å¼•å…¥äº†åŠ¨èƒ½è·¯å¾„èƒ½é‡ (kinetic path energy, KPE) è¿™ä¸€æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–åŸºäº ODE é‡‡æ ·å™¨åœ¨ç”Ÿæˆè·¯å¾„ä¸­çš„æ€»åŠ¨èƒ½åŠªåŠ›ã€‚é€šè¿‡åœ¨ CIFAR-10 å’Œ ImageNet-256 ä¸Šçš„å®éªŒï¼Œè¯¥ç ”ç©¶å‘ç°è¾ƒé«˜çš„ KPE èƒ½æœ‰æ•ˆé¢„æµ‹æ›´å¼ºçš„è¯­ä¹‰è´¨é‡ (semantic quality)ï¼Œæ„å‘³ç€è¯­ä¹‰ä¸°å¯Œçš„æ ·æœ¬åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­éœ€è¦æŠ•å…¥æ›´å¤§çš„åŠ¨èƒ½ã€‚æ­¤å¤–ï¼ŒKPE ä¸æ•°æ®å¯†åº¦ (data density) å‘ˆè´Ÿç›¸å…³ï¼Œæ­ç¤ºäº†é«˜ä¿¡æ¯é‡æ ·æœ¬é€šå¸¸ä½äºæ•°æ®åˆ†å¸ƒçš„ç¨€ç–è¾¹ç•Œã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯­ä¹‰ä¿¡æ¯ä¸°å¯Œçš„æ ·æœ¬è‡ªç„¶éœ€è¦æ›´é«˜çš„ç”Ÿæˆä»£ä»·ï¼Œä¸ºç†è§£æ¨¡å‹ç”Ÿæˆéš¾åº¦å’Œæ ·æœ¬ç‰¹å¾æä¾›äº†ä¸€ä¸ªç‰©ç†å¯å‘ä¸”å…·å¯è§£é‡Šæ€§çš„è½¨è¿¹çº§åˆ†æ (trajectory-level analysis) è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)",
      "pdf_url": "https://arxiv.org/pdf/2511.19087v1",
      "published_date": "2025-11-24 13:27:41 UTC",
      "updated_date": "2025-11-24 13:27:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:06:15.552678+00:00"
    },
    {
      "arxiv_id": "2511.19078v1",
      "title": "GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning",
      "title_zh": "GraphMindï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„åŠ¨æ€å›¾ç¥ç»ç½‘ç»œå®šç†é€‰æ‹©ä¸ç»“è®ºç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Yutong Li",
        "Yitian Zhou",
        "Xudong Wang",
        "GuoChen",
        "Caiyan Qin"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šæ­¥æ¨ç†ä¸­ç¼ºä¹æ˜¾å¼ä¸”åŠ¨æ€çš„ä¸­é—´æ¨ç†çŠ¶æ€ç»“æ„åŒ–è¡¨ç¤ºï¼Œä»è€Œé™åˆ¶äº†å…¶ä¸Šä¸‹æ–‡æ„ŸçŸ¥å®šç†é€‰æ‹©å’Œè¿­ä»£ç»“è®ºç”Ÿæˆèƒ½åŠ›çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†GraphMindï¼Œè¿™æ˜¯ä¸€ç§å°†å›¾ç¥ç»ç½‘ç»œ(GNN)ä¸LLMsé›†æˆçš„åˆ›æ–°åŠ¨æ€å›¾æ¡†æ¶ï¼Œæ—¨åœ¨è¿­ä»£é€‰æ‹©å®šç†å¹¶ç”Ÿæˆä¸­é—´ç»“è®ºã€‚è¯¥æ–¹æ³•å°†æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºå¼‚æ„æ¼”åŒ–å›¾(heterogeneous evolving graph)ï¼Œé€šè¿‡èŠ‚ç‚¹ä»£è¡¨æ¡ä»¶ã€å®šç†å’Œç»“è®ºï¼Œå¹¶åˆ©ç”¨è¾¹æ•æ‰å…¶é€»è¾‘ä¾èµ–å…³ç³»ã€‚é€šè¿‡åˆ©ç”¨GNNç¼–ç å½“å‰æ¨ç†çŠ¶æ€å¹¶ç»“åˆè¯­ä¹‰åŒ¹é…è¿›è¡Œå®šç†é€‰æ‹©ï¼Œè¯¥æ¡†æ¶å®ç°äº†é—­ç¯ã€å¯è§£é‡Šä¸”ç»“æ„åŒ–çš„æ¨ç†è¿‡ç¨‹ã€‚åœ¨å¤šç§é—®ç­”(QA)æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGraphMindåœ¨å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶éªŒè¯äº†ç»“åˆç»“æ„åŒ–å›¾è¡¨ç¤ºä¸ç”Ÿæˆå¼æ¨¡å‹åœ¨å¤„ç†å¤æ‚é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19078v1",
      "published_date": "2025-11-24 13:18:21 UTC",
      "updated_date": "2025-11-24 13:18:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:06:20.436638+00:00"
    },
    {
      "arxiv_id": "2511.19550v1",
      "title": "The Semiotic Channel Principle: Measuring the Capacity for Meaning in LLM Communication",
      "title_zh": "ç¬¦å·ä¿¡é“åŸç†ï¼šè¡¡é‡å¤§è¯­è¨€æ¨¡å‹é€šä¿¡ä¸­çš„æ„ä¹‰å®¹é‡",
      "authors": [
        "Davide Picca"
      ],
      "abstract": "This paper proposes a novel semiotic framework for analyzing Large Language Models (LLMs), conceptualizing them as stochastic semiotic engines whose outputs demand active, asymmetric human interpretation. We formalize the trade-off between expressive richness (semiotic breadth) and interpretive stability (decipherability) using information-theoretic tools. Breadth is quantified as source entropy, and decipherability as the mutual information between messages and human interpretations. We introduce a generative complexity parameter (lambda) that governs this trade-off, as both breadth and decipherability are functions of lambda. The core trade-off is modeled as an emergent property of their distinct responses to $Î»$. We define a semiotic channel, parameterized by audience and context, and posit a capacity constraint on meaning transmission, operationally defined as the maximum decipherability by optimizing lambda. This reframing shifts analysis from opaque model internals to observable textual artifacts, enabling empirical measurement of breadth and decipherability. We demonstrate the framework's utility across four key applications: (i) model profiling; (ii) optimizing prompt/context design; (iii) risk analysis based on ambiguity; and (iv) adaptive semiotic systems. We conclude that this capacity-based semiotic approach offers a rigorous, actionable toolkit for understanding, evaluating, and designing LLM-mediated communication.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ç¬¦å·å­¦(semiotic)æ¡†æ¶ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹(LLMs)è§†ä¸ºéšæœºç¬¦å·å¼•æ“(stochastic semiotic engines)ï¼Œå¹¶å¼ºè°ƒå…¶è¾“å‡ºéœ€è¦äººç±»çš„ä¸»åŠ¨ä¸”éå¯¹ç§°çš„è§£é‡Šã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¿¡æ¯è®ºå·¥å…·å…¬å¼åŒ–äº†è¡¨è¾¾ä¸°å¯Œåº¦(semiotic breadth)ä¸è§£é‡Šç¨³å®šæ€§(decipherability)ä¹‹é—´çš„æƒè¡¡å…³ç³»ï¼Œå…¶ä¸­å®½åº¦ç”±æºç†µ(source entropy)é‡åŒ–ï¼Œè€Œå¯è¯‘ç æ€§åˆ™é€šè¿‡æ¶ˆæ¯ä¸äººç±»è§£é‡Šä¹‹é—´çš„äº’ä¿¡æ¯(mutual information)æ¥è¡¡é‡ã€‚ç ”ç©¶å¼•å…¥äº†ç”Ÿæˆå¤æ‚åº¦å‚æ•°(lambda)æ¥è°ƒèŠ‚è¿™ä¸€æƒè¡¡ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ªç”±å—ä¼—å’ŒèƒŒæ™¯å‚æ•°åŒ–çš„ç¬¦å·é€šé“(semiotic channel)ï¼Œæå‡ºæ„ä¹‰ä¼ è¾“å­˜åœ¨å®¹é‡é™åˆ¶ï¼Œå³é€šè¿‡ä¼˜åŒ–lambdaå®ç°çš„æœ€å¤§å¯è¯‘ç æ€§ã€‚è¿™ä¸€é‡æ„å°†åˆ†æé‡ç‚¹ä»ä¸é€æ˜çš„æ¨¡å‹å†…éƒ¨è½¬å‘äº†å¯è§‚å¯Ÿçš„æ–‡æœ¬äº§ç‰©ï¼Œä»è€Œå®ç°äº†å¯¹å®½åº¦å’Œå¯è¯‘ç æ€§çš„å®è¯æµ‹é‡ã€‚å®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨æ¨¡å‹ç”»åƒ(model profiling)ã€æç¤ºè¯ä¸ä¸Šä¸‹æ–‡è®¾è®¡ä¼˜åŒ–ã€æ­§ä¹‰é£é™©åˆ†æä»¥åŠè‡ªé€‚åº”ç¬¦å·ç³»ç»Ÿç­‰é¢†åŸŸå…·æœ‰æ˜¾è‘—çš„åº”ç”¨ä»·å€¼ã€‚è¿™ç§åŸºäºå®¹é‡çš„ç¬¦å·å­¦æ–¹æ³•ä¸ºç†è§£ã€è¯„ä¼°å’Œè®¾è®¡ç”±LLMsä»‹å¯¼çš„é€šä¿¡æä¾›äº†ä¸¥è°¨ä¸”å…·æ“ä½œæ€§çš„å·¥å…·é›†ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19550v1",
      "published_date": "2025-11-24 13:06:29 UTC",
      "updated_date": "2025-11-24 13:06:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:05:45.849311+00:00"
    },
    {
      "arxiv_id": "2511.19067v2",
      "title": "DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling",
      "title_zh": "DynaMixï¼šåŸºäºåŠ¨æ€é‡æ‰“æ ‡ä¸æ··åˆæ•°æ®é‡‡æ ·çš„å¯æ³›åŒ–è¡Œäººé‡è¯†åˆ«",
      "authors": [
        "Timur Mamedov",
        "Anton Konushin",
        "Vadim Konushin"
      ],
      "abstract": "Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DynaMixï¼Œä¸€ç§é€šè¿‡ç»“åˆæ‰‹åŠ¨æ ‡æ³¨çš„å¤šæ‘„åƒå¤´æ•°æ®å’Œå¤§è§„æ¨¡ä¼ªæ ‡ç­¾å•æ‘„åƒå¤´æ•°æ®æ¥æå‡æ³›åŒ–è¡Œäººé‡è¯†åˆ«(Generalizable person re-identification, Re-ID)æ€§èƒ½çš„æ–°æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è®­ç»ƒæ•°æ®ä¸­çš„ç»“æ„æ€§å™ªå£°é—®é¢˜ï¼ŒDynaMix å¼•å…¥äº† Relabeling Moduleï¼Œèƒ½å¤ŸåŠ¨æ€åœ¨çº¿ä¿®æ­£å•æ‘„åƒå¤´èº«ä»½çš„ä¼ªæ ‡ç­¾ã€‚é’ˆå¯¹å¤§è§„æ¨¡èº«ä»½ç©ºé—´ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ Efficient Centroids Module æ¥ç»´æŒç¨³å¥çš„èº«ä»½è¡¨ç¤ºï¼Œå¹¶é…åˆ Data Sampling Module é€šè¿‡ç²¾å¿ƒæ„å»ºæ··åˆæ•°æ®çš„ mini-batches æ¥å¹³è¡¡å­¦ä¹ å¤æ‚æ€§ä¸æ‰¹æ¬¡å†…å¤šæ ·æ€§ã€‚è¯¥æ¡†æ¶çš„æ‰€æœ‰ç»„ä»¶å‡ä¸“ä¸ºå¤§è§„æ¨¡æ‰©å±•è€Œè®¾è®¡ï¼Œæ”¯æŒåœ¨æ•°ç™¾ä¸‡å¼ å›¾åƒå’Œæ•°åä¸‡ä¸ªèº«ä»½ä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDynaMix åœ¨æ³›åŒ–è¡Œäºº Re-ID ä»»åŠ¡ä¸­æŒç»­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†è·¨æ‘„åƒå¤´å’Œè·¨ç¯å¢ƒè¯†åˆ«ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Neurocomputing Volume 669, 7 March 2026, 132446",
      "pdf_url": "https://arxiv.org/pdf/2511.19067v2",
      "published_date": "2025-11-24 13:01:32 UTC",
      "updated_date": "2025-12-25 20:03:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:05:39.048905+00:00"
    },
    {
      "arxiv_id": "2511.19066v1",
      "title": "Mitigating Participation Imbalance Bias in Asynchronous Federated Learning",
      "title_zh": "ç¼“è§£å¼‚æ­¥è”é‚¦å­¦ä¹ ä¸­çš„å‚ä¸ä¸å‡è¡¡åå·®",
      "authors": [
        "Xiangyu Chang",
        "Manyi Yao",
        "Srikanth V. Krishnamurthy",
        "Christian R. Shelton",
        "Anirban Chakraborty",
        "Ananthram Swami",
        "Samet Oymak",
        "Amit Roy-Chowdhury"
      ],
      "abstract": "In Asynchronous Federated Learning (AFL), the central server immediately updates the global model with each arriving client's contribution. As a result, clients perform their local training on different model versions, causing information staleness (delay). In federated environments with non-IID local data distributions, this asynchronous pattern amplifies the adverse effect of client heterogeneity (due to different data distribution, local objectives, etc.), as faster clients contribute more frequent updates, biasing the global model. We term this phenomenon heterogeneity amplification. Our work provides a theoretical analysis that maps AFL design choices to their resulting error sources when heterogeneity amplification occurs. Guided by our analysis, we propose ACE (All-Client Engagement AFL), which mitigates participation imbalance through immediate, non-buffered updates that use the latest information available from all clients. We also introduce a delay-aware variant, ACED, to balance client diversity against update staleness. Experiments on different models for different tasks across diverse heterogeneity and delay settings validate our analysis and demonstrate the robust performance of our approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼‚æ­¥è”é‚¦å­¦ä¹ (Asynchronous Federated Learning, AFL)ä¸­å­˜åœ¨çš„å‚ä¸ä¸å¹³è¡¡åå·®é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œç‰¹åˆ«å…³æ³¨åœ¨éç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID)æ•°æ®ç¯å¢ƒä¸‹ï¼Œç”±äºå®¢æˆ·ç«¯æ›´æ–°é¢‘ç‡ä¸ä¸€å¯¼è‡´çš„â€œå¼‚æ„æ€§æ”¾å¤§â€(heterogeneity amplification)ç°è±¡ã€‚ä½œè€…é€šè¿‡ç†è®ºåˆ†æï¼Œæ­ç¤ºäº†AFLè®¾è®¡é€‰æ‹©ä¸è¯¯å·®æ¥æºä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œä¸ºè§£å†³æ¨¡å‹åç½®æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚åŸºäºåˆ†æç»“æœï¼Œè®ºæ–‡æå‡ºäº†ACE(All-Client Engagement AFL)æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨æ‰€æœ‰å®¢æˆ·ç«¯çš„æœ€æ–°å¯ç”¨ä¿¡æ¯è¿›è¡Œå³æ—¶ã€éç¼“å†²æ›´æ–°ï¼Œæœ‰æ•ˆç¼“è§£äº†å‚ä¸ä¸å¹³è¡¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å»¶è¿Ÿæ„ŸçŸ¥å˜ä½“ACEDï¼Œæ—¨åœ¨å¹³è¡¡å®¢æˆ·ç«¯å¤šæ ·æ€§ä¸æ›´æ–°é™ˆæ—§åº¦(update staleness)ä¹‹é—´çš„çŸ›ç›¾ã€‚å¤šé¡¹å®éªŒç»“æœéªŒè¯äº†è¯¥ç†è®ºåˆ†æçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜äº†ACEå’ŒACEDåœ¨å¤„ç†é«˜åº¦å¼‚æ„å’Œå»¶è¿Ÿç¯å¢ƒæ—¶å…·æœ‰æ˜¾è‘—çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19066v1",
      "published_date": "2025-11-24 13:01:18 UTC",
      "updated_date": "2025-11-24 13:01:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:06:02.752019+00:00"
    },
    {
      "arxiv_id": "2511.19065v1",
      "title": "Understanding, Accelerating, and Improving MeanFlow Training",
      "title_zh": "MeanFlow è®­ç»ƒçš„ç†è§£ã€åŠ é€Ÿä¸ä¼˜åŒ–",
      "authors": [
        "Jin-Young Kim",
        "Hyojun Go",
        "Lea Bogensperger",
        "Julius Erbach",
        "Nikolai Kalischek",
        "Federico Tombari",
        "Konrad Schindler",
        "Dominik Narnhofer"
      ],
      "abstract": "MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†MeanFlowçš„è®­ç»ƒåŠ¨åŠ›å­¦ï¼Œæ—¨åœ¨ä¼˜åŒ–è¿™ç§é€šè¿‡è”åˆå­¦ä¹ ç¬æ—¶é€Ÿåº¦åœº(instantaneous velocity fields)å’Œå¹³å‡é€Ÿåº¦åœº(average velocity fields)æ¥å®ç°é«˜è´¨é‡å°‘æ­¥æ•°ç”Ÿæˆå»ºæ¨¡çš„æŠ€æœ¯ã€‚åˆ†æå‘ç°ï¼Œå»ºç«‹è‰¯å¥½çš„ç¬æ—¶é€Ÿåº¦æ˜¯å­¦ä¹ å¹³å‡é€Ÿåº¦çš„å‰æï¼Œä¸”ä¸¤è€…çš„ç›¸äº’ä¿ƒè¿›ä½œç”¨å—æ—¶é—´é—´éš”å¤§å°çš„å½±å“ï¼Œå½“é—´éš”è¿‡å¤§æ—¶å­¦ä¹ æ•ˆç‡åè€Œä¼šä¸‹é™ã€‚åŸºäºä»»åŠ¡äº²å’ŒåŠ›(task-affinity)åˆ†æï¼Œç ”ç©¶æŒ‡å‡ºä¸€æ­¥ç”Ÿæˆæ‰€éœ€çš„è·¨åº¦è¾ƒå¤§çš„å¹³å‡é€Ÿåº¦å­¦ä¹ ï¼Œé«˜åº¦ä¾èµ–äºé¢„å…ˆå½¢æˆçš„å‡†ç¡®ç¬æ—¶é€Ÿåº¦å’Œè¾ƒå°é—´éš”çš„å¹³å‡é€Ÿåº¦ã€‚æ®æ­¤ï¼Œä½œè€…è®¾è®¡äº†ä¸€ç§æ”¹è¿›çš„è®­ç»ƒæ–¹æ¡ˆï¼Œé€šè¿‡åŠ é€Ÿç¬æ—¶é€Ÿåº¦çš„å½¢æˆå¹¶å°†å­¦ä¹ é‡å¿ƒé€æ­¥ä»çŸ­é—´éš”è½¬ç§»åˆ°é•¿é—´éš”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„DiT-XLéª¨å¹²ç½‘ç»œä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨ImageNet 256x256æ•°æ®é›†çš„1-NFEç”Ÿæˆä¸­è¾¾åˆ°äº†2.87çš„FIDï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸMeanFlowåŸºçº¿ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨è¾¾åˆ°ç›¸åŒæ€§èƒ½çš„å‰æä¸‹ï¼Œç›¸æ¯”åŸºå‡†æ¨¡å‹ç¼©çŸ­äº†2.5å€çš„è®­ç»ƒæ—¶é—´ï¼Œæˆ–èƒ½ä»¥æ›´å°çš„DiT-Léª¨å¹²ç½‘ç»œå®ç°åŒç­‰æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19065v1",
      "published_date": "2025-11-24 12:59:27 UTC",
      "updated_date": "2025-11-24 12:59:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:06:54.745135+00:00"
    },
    {
      "arxiv_id": "2511.20699v1",
      "title": "In Defense of the Turing Test and its Legacy",
      "title_zh": "ä¸ºå›¾çµæµ‹è¯•åŠå…¶é—äº§è¾©æŠ¤",
      "authors": [
        "Bernardo GonÃ§alves"
      ],
      "abstract": "Considering that Turing's original test was co-opted by Weizenbaum and that six of the most common criticisms of the Turing test are unfair to both Turing's argument and the historical development of AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨ä¸ºå›¾çµæµ‹è¯•(Turing Test)åŠå…¶é—äº§è¿›è¡Œè¾©æŠ¤ï¼Œæ¢è®¨å…¶åœ¨äººå·¥æ™ºèƒ½å†å²å‘å±•ä¸­çš„çœŸå®å†…æ¶µã€‚ä½œè€…æŒ‡å‡ºï¼Œå›¾çµæœ€åˆçš„æµ‹è¯•æ„æƒ³æ›¾è¢«Weizenbaumç­‰äººè¯¯ç”¨ï¼Œå¯¼è‡´äº†å­¦ç•Œå¯¹è¯¥æ¦‚å¿µé•¿æœŸå­˜åœ¨çš„è¯¯è§£ã€‚è®ºæ–‡ç³»ç»Ÿåœ°é©³æ–¥äº†é’ˆå¯¹å›¾çµæµ‹è¯•çš„å…­ç§æœ€å¸¸è§æ‰¹è¯„ï¼Œè®¤ä¸ºè¿™äº›æŒ‡è´£å¯¹å›¾çµçš„åŸå§‹è®ºç‚¹ä»¥åŠAIçš„å†å²è„‰ç»œæ˜¯ä¸å…¬å¹³çš„ã€‚é€šè¿‡è¿˜åŸå†å²è¯­å¢ƒï¼Œè¯¥ç ”ç©¶é‡æ–°ç¡®ç«‹äº†å›¾çµæµ‹è¯•ä½œä¸ºæ™ºèƒ½è¯„ä¼°æ ¸å¿ƒæ¡†æ¶çš„ä»·å€¼ï¼Œä¸ºç†è§£äººå·¥æ™ºèƒ½çš„æ¼”è¿›é€»è¾‘æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.20699v1",
      "published_date": "2025-11-24 12:57:54 UTC",
      "updated_date": "2025-11-24 12:57:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:06:06.034918+00:00"
    },
    {
      "arxiv_id": "2511.19055v1",
      "title": "Large Language Model-Assisted Planning of Electric Vehicle Charging Infrastructure with Real-World Case Study",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©çš„ç”µåŠ¨æ±½è½¦å……ç”µåŸºç¡€è®¾æ–½è§„åˆ’åŠçœŸå®æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Xinda Zheng",
        "Canchen Jiang",
        "Hao Wang"
      ],
      "abstract": "The growing demand for electric vehicle (EV) charging infrastructure presents significant planning challenges, requiring efficient strategies for investment and operation to deliver cost-effective charging services. However, the potential benefits of EV charging assignment, particularly in response to varying spatial-temporal patterns of charging demand, remain under-explored in infrastructure planning. This paper proposes an integrated approach that jointly optimizes investment decisions and charging assignments while accounting for spatial-temporal demand dynamics and their interdependencies. To support efficient model development, we leverage a large language model (LLM) to assist in generating and refining the mathematical formulation from structured natural-language descriptions, significantly reducing the modeling burden. The resulting optimization model enables optimal joint decision-making for investment and operation. Additionally, we propose a distributed optimization algorithm based on the Alternating Direction Method of Multipliers (ADMM) to address computational complexity in high-dimensional scenarios, which can be executed on standard computing platforms. We validate our approach through a case study using 1.5 million real-world travel records from Chengdu, China, demonstrating a 30% reduction in total cost compared to a baseline without EV assignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»¼åˆä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç”µåŠ¨æ±½è½¦ï¼ˆEVï¼‰å……ç”µåŸºç¡€è®¾æ–½è§„åˆ’ä¸­æŠ•èµ„ä¸è¿è¥æ•ˆç‡çš„æŒ‘æˆ˜ï¼Œé€šè¿‡è”åˆä¼˜åŒ–æŠ•èµ„å†³ç­–ä¸å……ç”µä»»åŠ¡åˆ†é…ï¼ˆcharging assignmentsï¼‰æ¥åº”å¯¹ç©ºé—´-æ—¶é—´éœ€æ±‚çš„åŠ¨æ€å˜åŒ–ã€‚ç ”ç©¶åˆ›æ–°æ€§åœ°åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾…åŠ©ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­ç”Ÿæˆå’Œæç‚¼æ•°å­¦æ¨¡å‹ï¼Œæå¤§åœ°å‡è½»äº†å»ºæ¨¡è´Ÿæ‹…å¹¶æå‡äº†å†³ç­–æ•ˆç‡ã€‚ä¸ºåº”å¯¹é«˜ç»´åœºæ™¯ä¸‹çš„è®¡ç®—å¤æ‚æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰çš„åˆ†å¸ƒå¼ä¼˜åŒ–ç®—æ³•ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨æ ‡å‡†è®¡ç®—å¹³å°ä¸Šçš„é«˜æ•ˆè¿è¡Œã€‚é€šè¿‡å¯¹ä¸­å›½æˆéƒ½150ä¸‡æ¡çœŸå®å‡ºè¡Œè®°å½•çš„æ¡ˆä¾‹åˆ†æï¼Œè¯¥æ–¹æ³•è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ï¼Œç›¸æ¯”äºä¸åŒ…å«ä»»åŠ¡åˆ†é…çš„åŸºçº¿æ–¹æ¡ˆï¼Œæ€»æˆæœ¬é™ä½äº†30%ï¼Œä¸ºç”µåŠ¨æ±½è½¦åŸºç¡€è®¾æ–½çš„é«˜æ•ˆè§„åˆ’ä¸è¿è¥æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19055v1",
      "published_date": "2025-11-24 12:45:10 UTC",
      "updated_date": "2025-11-24 12:45:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:06:19.645320+00:00"
    },
    {
      "arxiv_id": "2511.21758v1",
      "title": "A Longitudinal Measurement of Privacy Policy Evolution for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹éšç§æ”¿ç­–æ¼”è¿›çš„çºµå‘æµ‹é‡",
      "authors": [
        "Zhen Tao",
        "Shidong Pan",
        "Zhenchang Xing",
        "Emily Black",
        "Talia Gillis",
        "Chunyang Chen"
      ],
      "abstract": "Large language model (LLM) services have been rapidly integrated into people's daily lives as chatbots and agentic systems. They are nourished by collecting rich streams of data, raising privacy concerns around excessive collection of sensitive personal information. Privacy policies are the fundamental mechanism for informing users about data practices in modern information privacy paradigm. Although traditional web and mobile policies are well studied, the privacy policies of LLM providers, their LLM-specific content, and their evolution over time remain largely underexplored. In this paper, we present the first longitudinal empirical study of privacy policies for mainstream LLM providers worldwide. We curate a chronological dataset of 74 historical privacy policies and 115 supplemental privacy documents from 11 LLM providers across 5 countries up to August 2025, and extract over 3,000 sentence-level edits between consecutive policy versions. We compare LLM privacy policies to those of other software formats, propose a taxonomy tailored to LLM privacy policies, annotate policy edits and align them with a timeline of key LLM ecosystem events. Results show they are substantially longer, demand college-level reading ability, and remain highly vague. Our taxonomy analysis reveals patterns in how providers disclose LLM-specific practices and highlights regional disparities in coverage. Policy edits are concentrated in first-party data collection and international/specific-audience sections, and that product releases and regulatory actions are the primary drivers, shedding light on the status quo and the evolution of LLM privacy policies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (Large Language Model, LLM) æœåŠ¡ä¸­éšç§æ”¿ç­–çš„æ¼”å˜è¿›è¡Œäº†é¦–æ¬¡çºµå‘å®è¯ç ”ç©¶ã€‚é€šè¿‡æ„å»ºåŒ…å« 11 å®¶å…¨çƒä¸»æµæä¾›å•†çš„ 74 ä»½å†å²æ”¿ç­–åŠ 115 ä»½è¡¥å……æ–‡ä»¶çš„æ—¶åºæ•°æ®é›†ï¼Œç ”ç©¶è€…æå–äº†è¶…è¿‡ 3,000 æ¡å¥å­çº§åˆ«çš„ä¿®è®¢è®°å½•ï¼Œå¹¶æå‡ºäº†ä¸€å¥—ä¸“é—¨é’ˆå¯¹ LLM éšç§æ”¿ç­–çš„åˆ†ç±»æ³• (Taxonomy)ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œè¿™äº›æ”¿ç­–æ™®éé•¿åº¦æ˜¾è‘—å¢åŠ ã€é˜…è¯»éš¾åº¦è¾¾åˆ°å¤§å­¦æ°´å¹³ï¼Œä¸”å†…å®¹ä¾ç„¶é«˜åº¦æ¨¡ç³Šã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†æä¾›å•†åœ¨æŠ«éœ² LLM ç‰¹å®šå®è·µæ–¹é¢çš„æ¨¡å¼åŠåœ°åŒºå·®å¼‚ï¼Œå‘ç°æ”¿ç­–ä¿®è®¢ä¸»è¦é›†ä¸­åœ¨ç¬¬ä¸€æ–¹æ•°æ®æ”¶é›†å’Œç‰¹å®šå—ä¼—ç« èŠ‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶æŒ‡å‡ºäº§å“å‘å¸ƒå’Œç›‘ç®¡è¡ŒåŠ¨æ˜¯é©±åŠ¨éšç§æ”¿ç­–æ¼”å˜çš„ä¸»è¦å› ç´ ï¼Œä¸ºç†è§£ LLM éšç§æ²»ç†çš„ç°çŠ¶ä¸è¶‹åŠ¿æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21758v1",
      "published_date": "2025-11-24 12:40:15 UTC",
      "updated_date": "2025-11-24 12:40:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:06:28.743895+00:00"
    },
    {
      "arxiv_id": "2511.19548v1",
      "title": "When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics",
      "title_zh": "ç¥ç»æ•°æ®ä½•æ—¶åº”æŒ‡å¯¼ç¦åˆ©åˆ†æï¼Ÿç¥ç»ç»æµå­¦æ”¿ç­–åº”ç”¨çš„æ‰¹åˆ¤æ€§æ¡†æ¶",
      "authors": [
        "Yiven",
        "Zhu"
      ],
      "abstract": "Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, \"brain-based\" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies \"true\" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä½•ç§æ¡ä»¶ä¸‹ Neural Dataï¼ˆç¥ç»æ•°æ®ï¼‰å¯ä»¥åˆæ³•åœ°ä¸ºæ”¿ç­–ç¦åˆ©åˆ¤æ–­æä¾›ä¾æ®ï¼Œè€Œä¸ä»…ä»…æ˜¯æè¿°è¡Œä¸ºã€‚ä½œè€…å¼€å‘äº†ä¸€ä¸ªéå®è¯çš„ã€åŸºäºæ¨¡å‹çš„æ¡†æ¶ï¼Œå°† Neural Signalsï¼ˆç¥ç»ä¿¡å·ï¼‰ã€Computational Decision Modelsï¼ˆè®¡ç®—å†³ç­–æ¨¡å‹ï¼‰å’Œ Normative Welfare Criteriaï¼ˆè§„èŒƒç¦åˆ©æ ‡å‡†ï¼‰ä¸‰ä¸ªå±‚é¢è”ç³»èµ·æ¥ã€‚é€šè¿‡ Actor-Critic Reinforcement-Learning Modelï¼ˆè¡ŒåŠ¨è€…-è¯„è®ºå®¶å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼‰ï¼Œè¯¥æ¡†æ¶å½¢å¼åŒ–äº†ä»ç¥ç»æ´»åŠ¨åˆ°æ½œåœ¨ä»·å€¼ã€é¢„æµ‹è¯¯å·®å†åˆ°ç¦åˆ©ä¸»å¼ çš„æ¨æ–­è·¯å¾„ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œåªæœ‰å½“ç¥ç»è®¡ç®—æ˜ å°„å¾—åˆ°å……åˆ†éªŒè¯ã€å†³ç­–æ¨¡å‹èƒ½åŒºåˆ†â€œçœŸå®â€åˆ©ç›Šä¸æƒ…å¢ƒé”™è¯¯ï¼Œä¸”ç¦åˆ©æ ‡å‡†è¢«æ˜ç¡®ç•Œå®šæ—¶ï¼Œç¥ç»è¯æ®æ‰èƒ½æœ‰æ•ˆæ”¯æŒç¦åˆ©åˆ¤æ–­ã€‚è¯¥æ¡†æ¶è¢«åº”ç”¨äºæˆç˜¾ã€Neuromarketingï¼ˆç¥ç»è¥é”€ï¼‰å’Œç¯å¢ƒæ”¿ç­–ï¼Œå¹¶ä¸ºç›‘ç®¡è€…å’Œ NeuroAI ç³»ç»Ÿè®¾è®¡è€…æ¨å¯¼å‡ºäº†ä¸€ä»½ Neuroeconomic Welfare Inference Checklistï¼ˆç¥ç»ç»æµå­¦ç¦åˆ©æ¨æ–­æ¸…å•ï¼‰ã€‚æœ€ç»ˆåˆ†æå¼ºè°ƒï¼Œæ— è®ºæ˜¯ç”Ÿç‰©è¿˜æ˜¯äººå·¥ç³»ç»Ÿçš„å†…éƒ¨ Reward Signalsï¼ˆå¥–åŠ±ä¿¡å·ï¼‰å‡ä¸ºè®¡ç®—é‡ï¼Œåœ¨ç¼ºä¹æ˜ç¡®è§„èŒƒæ¨¡å‹çš„æƒ…å†µä¸‹ä¸èƒ½ç›´æ¥ç­‰åŒäºç¦åˆ©è¡¡é‡æŒ‡æ ‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "econ.GN",
        "q-bio.NC"
      ],
      "primary_category": "cs.LG",
      "comment": "Durham Economic Journal 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.19548v1",
      "published_date": "2025-11-24 12:34:40 UTC",
      "updated_date": "2025-11-24 12:34:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:06:40.245309+00:00"
    },
    {
      "arxiv_id": "2511.19046v1",
      "title": "MedSAM3: Delving into Segment Anything with Medical Concepts",
      "title_zh": "MedSAM3ï¼šèåˆåŒ»å­¦æ¦‚å¿µçš„ä¸‡ç‰©åˆ†å‰²æ·±åº¦æ¢ç©¶",
      "authors": [
        "Anglin Liu",
        "Rundong Xue",
        "Xu R. Cao",
        "Yifan Shen",
        "Yi Lu",
        "Xiang Li",
        "Qianqian Chen",
        "Jintai Chen"
      ],
      "abstract": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MedSAM-3ï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘åŒ»ç–—å›¾åƒå’Œè§†é¢‘åˆ†å‰²çš„æ–‡æœ¬å¯æç¤º(text promptable)æ¨¡å‹ã€‚é€šè¿‡åœ¨å¸¦æœ‰è¯­ä¹‰æ¦‚å¿µæ ‡ç­¾çš„åŒ»ç–—å›¾åƒä¸Šå¾®è°ƒ Segment Anything Model (SAM) 3 æ¶æ„ï¼ŒMedSAM-3 å®ç°äº†åŒ»ç–—æç¤ºæ€§æ¦‚å¿µåˆ†å‰²(Promptable Concept Segmentation, PCS)ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡å¼€æ”¾è¯æ±‡çš„æ–‡æœ¬æè¿°è€Œéä»…é å‡ ä½•æç¤ºæ¥ç²¾ç¡®é”å®šè§£å‰–ç»“æ„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† MedSAM-3 Agent æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é›†æˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨ Agent-in-the-loop å·¥ä½œæµä¸­æ‰§è¡Œå¤æ‚æ¨ç†å’Œè¿­ä»£ä¼˜åŒ–ã€‚åœ¨åŒ…æ‹¬ X-rayã€MRIã€è¶…å£°ã€CT åŠè§†é¢‘åœ¨å†…çš„å¤šç§åŒ»ç–—å½±åƒæ¨¡æ€ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸“ç”¨æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19046v1",
      "published_date": "2025-11-24 12:34:38 UTC",
      "updated_date": "2025-11-24 12:34:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:07:15.444749+00:00"
    },
    {
      "arxiv_id": "2511.19035v2",
      "title": "Changes in Gaza: DINOv3-Powered Multi-Class Change Detection for Damage Assessment in Conflict Zones",
      "title_zh": "Changes in Gazaï¼šåŸºäº DINOv3 çš„å†²çªåœ°åŒºæŸæ¯è¯„ä¼°å¤šç±»åˆ«å˜åŒ–æ£€æµ‹",
      "authors": [
        "Kai Zheng",
        "Zhenkai Wu",
        "Fupeng Wei",
        "Miaolan Zhou",
        "Kai Lie",
        "Haitao Guo",
        "Lei Ding",
        "Wei Zhang",
        "Hang-Cheng Dong"
      ],
      "abstract": "Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. The multi-scale cross-attention mechanism allows for precise localization of subtle semantic changes, while the difference siamese structure enhances inter-class feature discrimination, enabling fine-grained semantic change detection. Furthermore, a simple yet powerful lightweight decoder is designed to generate clear detection maps while maintaining high efficiency. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. We evaluated our method on the Gaza-Change and two classical datasets: the SECOND and Landsat-SCD datasets. Experimental results demonstrate that our proposed approach effectively addresses the MCD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†²çªåœ°åŒºæŸä¼¤è¯„ä¼°ä¸­é¢ä¸´çš„æ•°æ®æœ‰é™ã€ç±»å†…ç›¸ä¼¼åº¦é«˜åŠè¯­ä¹‰å˜åŒ–æ¨¡ç³Šç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒ **DINOv3** æ¨¡å‹çš„ **MC-DiSNet**ï¼ˆå¤šå°ºåº¦äº¤å‰æ³¨æ„åŠ›å·®åˆ†å­ªç”Ÿç½‘ç»œï¼‰ã€‚åˆ©ç”¨ **DINOv3** éª¨å¹²ç½‘ç»œå¼ºå¤§çš„è§†è§‰è¡¨å¾èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•èƒ½ä»åŒæ—¶ç›¸é¥æ„Ÿå›¾åƒä¸­æå–é²æ£’ä¸”ä¸°å¯Œçš„ç‰¹å¾ã€‚ç»“åˆå¤šå°ºåº¦äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œå·®åˆ†å­ªç”Ÿç»“æ„ï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿç²¾ç¡®å®šä½ç»†å¾®çš„è¯­ä¹‰å˜åŒ–å¹¶å¢å¼ºç±»é—´ç‰¹å¾è¾¨åˆ«èƒ½åŠ›ï¼ŒåŒæ—¶é‡‡ç”¨è½»é‡çº§è§£ç å™¨ç¡®ä¿ç”Ÿæˆæ¸…æ™°æ£€æµ‹å›¾çš„é«˜æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†åŒ…å«2023-2024å¹´é«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒå¯¹åŠåƒç´ çº§è¯­ä¹‰å˜åŒ–æ ‡æ³¨çš„ **Gaza-change** æ•°æ®é›†ã€‚åœ¨ **Gaza-Change**ã€**SECOND** å’Œ **Landsat-SCD** æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç±»å˜åŒ–æ£€æµ‹ï¼ˆMCDï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ºå†²çªåœ°åŒºå¿«é€ŸæŸä¼¤è¯„ä¼°çš„å®é™…åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19035v2",
      "published_date": "2025-11-24 12:16:21 UTC",
      "updated_date": "2025-12-04 11:03:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-26T10:09:09.736853+00:00"
    },
    {
      "arxiv_id": "2511.19024v1",
      "title": "Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling",
      "title_zh": "Life-IQAï¼šé€šè¿‡GCNå¢å¼ºçš„å±‚é—´äº¤äº’ä¸åŸºäºMoEçš„ç‰¹å¾è§£è€¦æå‡æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä»·",
      "authors": [
        "Long Tang",
        "Guoquan Zhen",
        "Jie Hao",
        "Jianbo Zhang",
        "Huiyu Duan",
        "Liang Yuan",
        "Guangtao Zhai"
      ],
      "abstract": "Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \\underline{l}ayer\\underline{i}nteraction and MoE-based \\underline{f}eature d\\underline{e}coupling, termed \\textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \\href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\\texttt{Life-IQA}}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›²å›¾åƒè´¨é‡è¯„ä¼°(BIQA)ä¸­æµ…å±‚ä¸æ·±å±‚ç‰¹å¾è´¡çŒ®ä¸å‡ä»¥åŠè§£ç æ¶æ„ä¸å®Œå–„çš„é—®é¢˜ï¼Œæå‡ºäº†Life-IQAæ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†GCNå¢å¼ºçš„å±‚é—´äº¤äº’(GCN-enhanced Layer Interaction)æ¨¡å—ï¼Œåˆ©ç”¨GCNå¤„ç†çš„æœ€æ·±å±‚ç‰¹å¾ä½œä¸ºqueryï¼Œä¸å€’æ•°ç¬¬äºŒå±‚ç‰¹å¾è¿›è¡Œè·¨æ³¨æ„åŠ›äº¤äº’ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„ç‰¹å¾èåˆã€‚åŒæ—¶ï¼Œè®ºæ–‡è®¾è®¡äº†åŸºäºæ··åˆä¸“å®¶æ¨¡å‹(MoE)çš„ç‰¹å¾è§£è€¦æ¨¡å—ï¼Œé€šè¿‡ä¸“é—¨é’ˆå¯¹ç‰¹å®šå¤±çœŸç±»å‹æˆ–è´¨é‡ç»´åº¦çš„ä¸“å®¶å­ç½‘ç»œï¼Œå¯¹èåˆåçš„ç‰¹å¾è¡¨ç¤ºè¿›è¡Œè§£è€¦å¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLife-IQAåœ¨å¤šä¸ªBIQAåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶åœ¨é¢„æµ‹å‡†ç¡®ç‡ä¸è®¡ç®—æˆæœ¬ä¹‹é—´å±•ç°å‡ºæ¯”ä¼ ç»ŸTransformerè§£ç å™¨æ›´ä¼˜çš„å¹³è¡¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19024v1",
      "published_date": "2025-11-24 11:59:55 UTC",
      "updated_date": "2025-11-24 11:59:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:07:15.242779+00:00"
    },
    {
      "arxiv_id": "2511.19023v1",
      "title": "OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs",
      "title_zh": "OrdMoEï¼šå¤šæ¨¡æ€æ··åˆä¸“å®¶å¤§è¯­è¨€æ¨¡å‹ä¸­åŸºäºåˆ†å±‚ä¸“å®¶ç»„æ’åºçš„åå¥½å¯¹é½",
      "authors": [
        "Yuting Gao",
        "Weihao Chen",
        "Lan Wang",
        "Ruihan Xu",
        "Qingpei Guo"
      ],
      "abstract": "Preference learning has recently emerged as a pivotal strategy for post-training alignment of Multimodal Large Language Models (MLLMs). However, existing approaches predominantly rely on external human-annotated preference data, which is costly and labor-intensive to collect. In this work, we propose OrdMoE, a novel preference alignment framework that bypasses the reliance on external human preferences entirely by leveraging intrinsic signals within Mixture-of-Experts (MoE) architectures. Specifically, we observe that the router's expert selection scores implicitly encode a quality-aware ranking of responses (i.e. higher-scoring experts consistently generate higher-quality outputs). Building on this insight, OrdMoE constructs an internal preference hierarchy by grouping experts into ranked tiers based on their per-token routing scores and activating each tier separately to produce a sequence of responses with increasing quality. This yields a zero-cost, self-supervised preference ordering over generated responses, which can be directly optimized using standard preference learning objectives. Extensive experiments across multiple multimodal benchmarks demnstrate that OrdMoE significantly enhances both alignment and overall performance of multimodal Mixture-of-Experts LLMs, achieving competitive results without requiring any human-annotated preference data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OrdMoEï¼Œä¸€ç§æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)å¯¹é½è¿‡ç¨‹ä¸­ä¾èµ–æ˜‚è´µäººå·¥æ ‡æ³¨æ•°æ®é—®é¢˜çš„Preference Alignmentæ¡†æ¶ã€‚OrdMoEåˆ›æ–°æ€§åœ°åˆ©ç”¨äº†Mixture-of-Experts (MoE)æ¶æ„ä¸­çš„å†…åœ¨ä¿¡å·ï¼Œå‘ç°è·¯ç”±å™¨çš„ä¸“å®¶é€‰æ‹©åˆ†æ•°éšå«åœ°ç¼–ç äº†è¾“å‡ºå“åº”çš„è´¨é‡æ’åºã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œè¯¥æ¡†æ¶æ ¹æ®æ¯ä¸ªtokençš„è·¯ç”±åˆ†æ•°å°†ä¸“å®¶åˆ’åˆ†ä¸ºä¸åŒå±‚çº§çš„ä¸“å®¶ç»„ï¼Œé€šè¿‡åˆ†åˆ«æ¿€æ´»å„å±‚çº§äº§ç”Ÿè´¨é‡é€’å¢çš„å“åº”åºåˆ—ï¼Œä»è€Œæ„å»ºå†…éƒ¨åå¥½å±‚æ¬¡ã€‚è¿™ç§æ–¹æ³•å®ç°äº†ä¸€ç§é›¶æˆæœ¬çš„è‡ªç›‘ç£åå¥½æ’åºï¼Œå¹¶å¯ç›´æ¥é‡‡ç”¨æ ‡å‡†Preference Learningç›®æ ‡è¿›è¡Œä¼˜åŒ–ã€‚å¤šé¡¹å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ç»“æœè¯æ˜ï¼ŒOrdMoEåœ¨æ— éœ€ä»»ä½•äººå·¥æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å¢å¼ºäº†å¤šæ¨¡æ€MoEæ¨¡å‹çš„å¯¹é½æ•ˆæœå’Œæ•´ä½“æ€§èƒ½ï¼Œå–å¾—äº†æå…·ç«äº‰åŠ›çš„å®éªŒç»“æœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19023v1",
      "published_date": "2025-11-24 11:59:31 UTC",
      "updated_date": "2025-11-24 11:59:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:07:29.047041+00:00"
    },
    {
      "arxiv_id": "2511.21757v1",
      "title": "Medical Malice: A Dataset for Context-Aware Safety in Healthcare LLMs",
      "title_zh": "Medical Maliceï¼šé¢å‘åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹æƒ…å¢ƒæ„ŸçŸ¥å®‰å…¨çš„æ•°æ®é›†",
      "authors": [
        "Andrew MaranhÃ£o Ventura D'addario"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into healthcare demands a safety paradigm rooted in \\textit{primum non nocere}. However, current alignment techniques rely on generic definitions of harm that fail to capture context-dependent violations, such as administrative fraud and clinical discrimination. To address this, we introduce Medical Malice: a dataset of 214,219 adversarial prompts calibrated to the regulatory and ethical complexities of the Brazilian Unified Health System (SUS). Crucially, the dataset includes the reasoning behind each violation, enabling models to internalize ethical boundaries rather than merely memorizing a fixed set of refusals. Using an unaligned agent (Grok-4) within a persona-driven pipeline, we synthesized high-fidelity threats across seven taxonomies, ranging from procurement manipulation and queue-jumping to obstetric violence. We discuss the ethical design of releasing these \"vulnerability signatures\" to correct the information asymmetry between malicious actors and AI developers. Ultimately, this work advocates for a shift from universal to context-aware safety, providing the necessary resources to immunize healthcare AI against the nuanced, systemic threats inherent to high-stakes medical environments -- vulnerabilities that represent the paramount risk to patient safety and the successful integration of AI in healthcare systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—åœºæ™¯ä¸­éš¾ä»¥æ•æ‰è¡Œæ”¿æ¬ºè¯ˆå’Œä¸´åºŠæ­§è§†ç­‰ç‰¹å®šè¿è§„è¡Œä¸ºçš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºMedical Maliceçš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«214,219ä¸ªå¯¹æŠ—æ€§æç¤ºï¼ˆadversarial promptsï¼‰ï¼Œå¹¶ä¸“é—¨é’ˆå¯¹å·´è¥¿ç»Ÿä¸€å«ç”Ÿç³»ç»Ÿï¼ˆSUSï¼‰çš„æ³•è§„å’Œä¼¦ç†å¤æ‚æ€§è¿›è¡Œäº†æ ¡å‡†ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è§’è‰²é©±åŠ¨ï¼ˆpersona-drivenï¼‰çš„æµç¨‹ï¼Œåˆ©ç”¨æ™ºèƒ½ä½“ï¼ˆGrok-4ï¼‰åˆæˆäº†æ¶µç›–é‡‡è´­æ“çºµã€åŒ»ç–—æ’é˜Ÿå’Œäº§ç§‘æš´åŠ›ç­‰ä¸ƒå¤§ç±»çš„é«˜ä¿çœŸå¨èƒã€‚å…³é”®åœ¨äºï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†æ¯ç§è¿è§„è¡Œä¸ºèƒŒåçš„æ¨ç†é€»è¾‘ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå†…åŒ–ä¼¦ç†è¾¹ç•Œè€Œéå•çº¯è®°å¿†æ‹’ç»æœåŠ¡è§„åˆ™ã€‚å®éªŒè®¨è®ºäº†å‘å¸ƒæ­¤ç±»â€œè„†å¼±æ€§ç­¾åâ€ï¼ˆvulnerability signaturesï¼‰çš„ä¼¦ç†è®¾è®¡ï¼Œæ—¨åœ¨çº æ­£æ¶æ„è¡Œä¸ºè€…ä¸å¼€å‘è€…ä¹‹é—´çš„ä¿¡æ¯ä¸å¯¹ç§°ã€‚è¯¥å·¥ä½œæ¨åŠ¨äº†åŒ»ç–—AIå®‰å…¨ä»é€šç”¨æ¨¡å¼å‘è¯­å¢ƒæ„ŸçŸ¥ï¼ˆcontext-awareï¼‰å®‰å…¨çš„è½¬å˜ï¼Œä¸ºæŠµå¾¡é«˜é£é™©åŒ»ç–—ç¯å¢ƒä¸­çš„ç³»ç»Ÿæ€§å¨èƒæä¾›äº†æ ¸å¿ƒèµ„æºã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21757v1",
      "published_date": "2025-11-24 11:55:22 UTC",
      "updated_date": "2025-11-24 11:55:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:07:42.445373+00:00"
    },
    {
      "arxiv_id": "2511.19005v1",
      "title": "Introducing Visual Scenes and Reasoning: A More Realistic Benchmark for Spoken Language Understanding",
      "title_zh": "å¼•å…¥è§†è§‰åœºæ™¯ä¸æ¨ç†ï¼šä¸€ä¸ªæ›´å…·ç°å®æ€§çš„å£è¯­ç†è§£åŸºå‡†",
      "authors": [
        "Di Wu",
        "Liting Jiang",
        "Ruiyu Fang",
        "Bianjing",
        "Hongyan Xie",
        "Haoxiang Su",
        "Hao Huang",
        "Zhongjiang He",
        "Shuangyong Song",
        "Xuelong Li"
      ],
      "abstract": "Spoken Language Understanding (SLU) consists of two sub-tasks: intent detection (ID) and slot filling (SF). Given its broad range of real-world applications, enhancing SLU for practical deployment is increasingly critical. Profile-based SLU addresses ambiguous user utterances by incorporating context awareness (CA), user profiles (UP), and knowledge graphs (KG) to support disambiguation, thereby advancing SLU research toward real-world applicability. However, existing SLU datasets still fall short in representing real-world scenarios. Specifically, (1) CA uses one-hot vectors for representation, which is overly idealized, and (2) models typically focuses solely on predicting intents and slot labels, neglecting the reasoning process that could enhance performance and interpretability. To overcome these limitations, we introduce VRSLU, a novel SLU dataset that integrates both Visual images and explicit Reasoning. For over-idealized CA, we use GPT-4o and FLUX.1-dev to generate images reflecting users' environments and statuses, followed by human verification to ensure quality. For reasoning, GPT-4o is employed to generate explanations for predicted labels, which are then refined by human annotators to ensure accuracy and coherence. Additionally, we propose an instructional template, LR-Instruct, which first predicts labels and then generates corresponding reasoning. This two-step approach helps mitigate the influence of reasoning bias on label prediction. Experimental results confirm the effectiveness of incorporating visual information and highlight the promise of explicit reasoning in advancing SLU.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å£è¯­ç†è§£(Spoken Language Understanding, SLU)æ•°æ®é›†åœ¨æƒ…å¢ƒæ„ŸçŸ¥(Context Awareness)è¡¨ç¤ºè¿‡äºç†æƒ³åŒ–ä»¥åŠç¼ºä¹æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†VRSLUã€‚VRSLU é¦–æ¬¡å°†è§†è§‰åœºæ™¯(Visual images)ä¸æ˜¾å¼æ¨ç†(Reasoning)ç›¸ç»“åˆï¼Œé€šè¿‡ GPT-4o å’Œ FLUX.1-dev ç”Ÿæˆåæ˜ ç”¨æˆ·ç¯å¢ƒä¸çŠ¶æ€çš„å›¾åƒï¼Œå¹¶ç»è¿‡äººå·¥éªŒè¯ç¡®ä¿äº†æ•°æ®è´¨é‡ã€‚ä¸ºäº†å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œç ”ç©¶è€…åˆ©ç”¨ GPT-4o ç”Ÿæˆé¢„æµ‹æ ‡ç­¾çš„è§£é‡Šæ€§ç†ç”±ï¼Œå¹¶ç”±äººå·¥æ ‡æ³¨è€…è¿›è¡Œæ ¡å¯¹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ LR-Instruct æŒ‡ä»¤æ¨¡æ¿ï¼Œé€šè¿‡å…ˆé¢„æµ‹æ ‡ç­¾å†ç”Ÿæˆæ¨ç†çš„ç­–ç•¥æ¥å‡è½»æ¨ç†åå·®å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚å®éªŒç»“æœè¯æ˜ï¼Œç»“åˆè§†è§‰ä¿¡æ¯å’Œæ˜¾å¼æ¨ç†èƒ½å¤Ÿæœ‰æ•ˆæå‡ SLU æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸ºæ„å»ºæ›´å…·çœŸå®æ„Ÿå’Œå¯è§£é‡Šæ€§çš„å£è¯­ç†è§£ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19005v1",
      "published_date": "2025-11-24 11:32:24 UTC",
      "updated_date": "2025-11-24 11:32:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:07:33.342842+00:00"
    },
    {
      "arxiv_id": "2511.18999v1",
      "title": "Enhancing low energy reconstruction and classification in KM3NeT/ORCA with transformers",
      "title_zh": "åˆ©ç”¨ Transformer æå‡ KM3NeT/ORCA ä¸­çš„ä½èƒ½é‡å»ºä¸åˆ†ç±»",
      "authors": [
        "IvÃ¡n MozÃºn Mateo"
      ],
      "abstract": "The current KM3NeT/ORCA neutrino telescope, still under construction, has not yet reached its full potential in neutrino reconstruction capability. When training any deep learning model, no explicit information about the physics or the detector is provided, thus they remain unknown to the model. This study leverages the strengths of transformers by incorporating attention masks inspired by the physics and detector design, making the model understand both the telescope design and the neutrino physics measured on it. The study also shows the efficacy of transformers on retaining valuable information between detectors when doing fine-tuning from one configurations to another.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æå‡ KM3NeT/ORCA ä¸­å¾®å­æœ›è¿œé•œåœ¨ä½èƒ½é‡å»ºä¸åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸æ— æ³•æ˜¾å¼è·å–ç‰©ç†è§„å¾‹æˆ–æ¢æµ‹å™¨ä¿¡æ¯çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ Transformer çš„æ¶æ„ä¼˜åŠ¿ï¼Œå¼•å…¥äº†å—ç‰©ç†å­¦åŸç†å’Œæ¢æµ‹å™¨å‡ ä½•è®¾è®¡å¯å‘çš„ attention masksï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ·±å…¥ç†è§£æœ›è¿œé•œçš„è®¾è®¡ç»“æ„åŠå…¶ä¸­å¾®å­ç‰©ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸåœ°å°†ç‰©ç†èƒŒæ™¯çŸ¥è¯†æ•´åˆè¿›æ¨¡å‹æ¨ç†ä¸­ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹æ¢æµ‹ä¿¡å·çš„ç†è§£ã€‚ç ”ç©¶è¿˜è¿›ä¸€æ­¥è¯æ˜äº† Transformer åœ¨ä¸åŒæ¢æµ‹å™¨é…ç½®çš„ fine-tuning è¿‡ç¨‹ä¸­å…·æœ‰æå¼ºçš„ä¿¡æ¯ä¿ç•™èƒ½åŠ›ã€‚è¿™ä¸€è¿›å±•ä¸ºå°šåœ¨å»ºè®¾ä¸­çš„ KM3NeT/ORCA å……åˆ†å‘æŒ¥å…¶ç§‘å­¦æ¢æµ‹æ½œåŠ›æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "hep-ex",
        "astro-ph.IM",
        "cs.AI"
      ],
      "primary_category": "hep-ex",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18999v1",
      "published_date": "2025-11-24 11:25:30 UTC",
      "updated_date": "2025-11-24 11:25:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:08:33.656821+00:00"
    },
    {
      "arxiv_id": "2511.18992v1",
      "title": "Classification EM-PCA for clustering and embedding",
      "title_zh": "ç”¨äºèšç±»ä¸åµŒå…¥çš„åˆ†ç±» EM-PCA",
      "authors": [
        "Zineddine Tighidet",
        "Lazhar Labiod",
        "Mohamed Nadif"
      ],
      "abstract": "The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜ç»´æ•°æ®ä¸‹ Gaussian æ··åˆæ¨¡å‹é¢ä¸´çš„ç»´åº¦ç¾éš¾åŠ Expectation-Maximization (EM) ç®—æ³•æ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ï¼Œæå‡ºäº† Classification EM-PCA ç®—æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆ Principal Component Analysis (PCA) ä¸ Classification EM (CEM)ï¼Œå®ç°äº†æ•°æ®åµŒå…¥ (Data embedding) ä¸èšç±» (Clustering) ä»»åŠ¡çš„åŒæ—¶ä¸”éé¡ºåºæ‰§è¡Œã€‚ç›¸æ¯”ä¼ ç»Ÿçš„é¡ºåºå¤„ç†æ¨¡å¼ï¼Œè¯¥ç®—æ³•ä¸ä»…ç»§æ‰¿äº† CEM å¿«é€Ÿæ”¶æ•›çš„ç‰¹æ€§ï¼Œè¿˜æœ‰æ•ˆè§£å†³äº†ç»´åº¦é™å‡ (dimensionality reduction) çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå‡èšç±»å‡†ç¡®æ€§çš„åŒæ—¶ä¼˜åŒ–äº†æ•°æ®åµŒå…¥æ•ˆæœï¼Œå¹¶å»ºç«‹äº†å…¶ä¸å…¶ä»–ç»å…¸èšç±»æ–¹æ³•ä¹‹é—´çš„ç†è®ºè”ç³»ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Accepted at the IEEE conference on Big Data (Special Session on Machine Learning)",
      "pdf_url": "https://arxiv.org/pdf/2511.18992v1",
      "published_date": "2025-11-24 11:18:59 UTC",
      "updated_date": "2025-11-24 11:18:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:07:43.154624+00:00"
    },
    {
      "arxiv_id": "2512.00069v1",
      "title": "Enhancing Cognitive Robotics with Commonsense through LLM-Generated Preconditions and Subgoals",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å‰ç½®æ¡ä»¶ä¸å­ç›®æ ‡å¢å¼ºè®¤çŸ¥æœºå™¨äººå­¦çš„å¸¸è¯†èƒ½åŠ›",
      "authors": [
        "Ohad Bachner",
        "Bar Gamliel"
      ],
      "abstract": "Robots often fail at everyday tasks because instructions skip commonsense details like hidden preconditions and small subgoals. Traditional symbolic planners need these details to be written explicitly, which is time consuming and often incomplete. In this project we combine a Large Language Model with symbolic planning. Given a natural language task, the LLM suggests plausible preconditions and subgoals. We translate these suggestions into a formal planning model and execute the resulting plan in simulation. Compared to a baseline planner without the LLM step, our system produces more valid plans, achieves a higher task success rate, and adapts better when the environment changes. These results suggest that adding LLM commonsense to classical planning can make robot behavior in realistic scenarios more reliable.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººç”±äºç¼ºä¹å¸¸è¯†ç»†èŠ‚ï¼ˆå¦‚éšè—çš„Preconditionså’ŒSubgoalsï¼‰è€Œéš¾ä»¥å®Œæˆæ—¥å¸¸ä»»åŠ¡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸ç¬¦å·è§„åˆ’(Symbolic Planning)ç›¸ç»“åˆçš„æ–°æ¡†æ¶ã€‚é€šè¿‡åˆ†æè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼ŒLLM èƒ½å¤Ÿé¢„æµ‹å¹¶ç”Ÿæˆå¿…è¦çš„Preconditionså’ŒSubgoalsï¼Œéšåå°†å…¶è½¬åŒ–ä¸ºæ­£å¼çš„è§„åˆ’æ¨¡å‹å¹¶åœ¨ä»¿çœŸç¯å¢ƒä¸­æ‰§è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»ŸåŸºçº¿è§„åˆ’å™¨ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿä¸ä»…ç”Ÿæˆäº†æ›´å¤šæœ‰æ•ˆçš„è®¡åˆ’ï¼Œè¿˜æ˜¾è‘—æå‡äº†ä»»åŠ¡æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ç¯å¢ƒåŠ¨æ€å˜åŒ–æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§ï¼Œè¯æ˜äº†å¼•å…¥ LLM å¸¸è¯†æ¨ç†èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºç»å…¸è§„åˆ’åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00069v1",
      "published_date": "2025-11-24 11:16:41 UTC",
      "updated_date": "2025-11-24 11:16:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:07:54.751212+00:00"
    },
    {
      "arxiv_id": "2601.05270v1",
      "title": "LiveVectorLake: A Real-Time Versioned Knowledge Base Architecture for Streaming Vector Updates and Temporal Retrieval",
      "title_zh": "LiveVectorLakeï¼šæ”¯æŒæµå¼å‘é‡æ›´æ–°ä¸æ—¶æ€æ£€ç´¢çš„å®æ—¶ç‰ˆæœ¬åŒ–çŸ¥è¯†åº“æ¶æ„",
      "authors": [
        "Tarun Prajapati"
      ],
      "abstract": "Modern Retrieval-Augmented Generation (RAG) systems struggle with a fundamental architectural tension: vector indices are optimized for query latency but poorly handle continuous knowledge updates, while data lakes excel at versioning but introduce query latency penalties. We introduce LiveVectorLake, a dual-tier temporal knowledge base architecture that enables real-time semantic search on current knowledge while maintaining complete version history for compliance, auditability, and point-in-time retrieval. The system introduces three core architectural contributions: (1) Content-addressable chunk-level synchronization using SHA-256 hashing for deterministic change detection without external state tracking; (2) Dual-tier storage separating hot-tier vector indices (Milvus with HNSW) from cold-tier columnar versioning (Delta Lake with Parquet), optimizing query latency and storage cost independently; (3) Temporal query routing enabling point-in-time knowledge retrieval via delta-versioning with ACID consistency across tiers. Evaluation on a 100-document corpus versioned across five time points demonstrates: (i) 10-15% re-processing of content during updates compared to 100% for full re-indexing; (ii) sub-100ms retrieval latency on current knowledge; (iii) sub-2s latency for temporal queries across version history; and (iv) storage cost optimization through hot/cold tier separation (only current chunks in expensive vector indices). The approach enables production RAG deployments requiring simultaneous optimization for query performance, update efficiency, and regulatory compliance. Code and resources: [https://github.com/praj-tarun/LiveVectorLake]",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿåœ¨å‘é‡ç´¢å¼•æŸ¥è¯¢å»¶è¿Ÿä¸çŸ¥è¯†åº“å®æ—¶æ›´æ–°ã€ç‰ˆæœ¬ç®¡ç†ä¹‹é—´çš„æ¶æ„çŸ›ç›¾ï¼Œæå‡ºäº†LiveVectorLakeæ¶æ„ã€‚LiveVectorLakeé‡‡ç”¨äº†åŒå±‚æ—¶åºçŸ¥è¯†åº“æ¶æ„ï¼Œé€šè¿‡SHA-256å“ˆå¸Œå®ç°åŸºäºå†…å®¹çš„å—çº§åŒæ­¥(Content-addressable chunk-level synchronization)ï¼Œåœ¨æ— éœ€å¤–éƒ¨çŠ¶æ€è·Ÿè¸ªçš„æƒ…å†µä¸‹å®ç°ç¡®å®šæ€§å˜æ›´æ£€æµ‹ã€‚è¯¥ç³»ç»Ÿå°†çƒ­å±‚å‘é‡ç´¢å¼•(Milvusä¸HNSW)ä¸å†·å±‚åˆ—å¼ç‰ˆæœ¬åŒ–å­˜å‚¨(Delta Lakeä¸Parquet)åˆ†ç¦»ï¼Œä»è€Œç‹¬ç«‹ä¼˜åŒ–æŸ¥è¯¢å»¶è¿Ÿä¸å­˜å‚¨æˆæœ¬ã€‚å…¶å¼•å…¥çš„æ—¶åºæŸ¥è¯¢è·¯ç”±(Temporal query routing)æ”¯æŒè·¨å±‚çº§ACIDä¸€è‡´æ€§çš„æ—¶ç‚¹çŸ¥è¯†æ£€ç´¢ï¼Œæ»¡è¶³åˆè§„æ€§ä¸å®¡è®¡éœ€æ±‚ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨æ›´æ–°æ—¶ä»…éœ€é‡æ–°å¤„ç†10%-15%çš„å†…å®¹ï¼Œä¸”å®ç°äº†å½“å‰çŸ¥è¯†æ£€ç´¢å»¶è¿Ÿä½äº100msä»¥åŠè·¨å†å²ç‰ˆæœ¬çš„ä½å»¶è¿ŸæŸ¥è¯¢ã€‚LiveVectorLakeé€šè¿‡å†·çƒ­åˆ†å±‚å­˜å‚¨æ˜¾è‘—é™ä½äº†æˆæœ¬ï¼Œä¸ºéœ€è¦å…¼é¡¾æŸ¥è¯¢æ€§èƒ½ã€æ›´æ–°æ•ˆç‡åŠç›‘ç®¡åˆè§„çš„ç”Ÿäº§çº§RAGéƒ¨ç½²æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.IR",
      "comment": "7 pages, 1 figure. Preprint; work in progress",
      "pdf_url": "https://arxiv.org/pdf/2601.05270v1",
      "published_date": "2025-11-24 11:15:39 UTC",
      "updated_date": "2025-11-24 11:15:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:07:58.133672+00:00"
    },
    {
      "arxiv_id": "2511.18989v1",
      "title": "Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning",
      "title_zh": "é‡æ–°æ€è€ƒæ¤ç‰©ç—…å®³è¯Šæ–­ï¼šåˆ©ç”¨ Vision Transformer å’Œé›¶æ ·æœ¬å­¦ä¹ å¼¥åˆå­¦æœ¯ä¸å®è·µé—´çš„å·®è·",
      "authors": [
        "Wassim Benabbas",
        "Mohammed Brahimi",
        "Samir Akhrouf",
        "Bilal Fortas"
      ],
      "abstract": "Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. Much of the existing research in this field has relied on the PlantVillage dataset, which consists of well-centered plant images captured against uniform, uncluttered backgrounds. Although models trained on this dataset achieve high accuracy, they often fail to generalize to real-world field images, such as those submitted by farmers to plant diagnostic systems. This has created a significant gap between published studies and practical application requirements, highlighting the necessity of investigating and addressing this issue. In this study, we investigate whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification. We evaluate three model categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Contrastive Language-Image Pre-training (CLIP)-based zero-shot models. While CNNs exhibit limited robustness under domain shift, Vision Transformers demonstrate stronger generalization by capturing global contextual features. Most notably, CLIP models classify diseases directly from natural language descriptions without any task-specific training, offering strong adaptability and interpretability. These findings highlight the potential of zero-shot learning as a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¤ç‰©ç—…å®³è¯Šæ–­æ¨¡å‹åœ¨å­¦æœ¯æ•°æ®é›†ä¸å®é™…å†œç”°ç¯å¢ƒä¹‹é—´çš„æ³›åŒ–å·®è·ï¼Œé‡ç‚¹åˆ†æäº†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚èƒŒæ™¯å›¾åƒæ—¶çš„å±€é™æ€§ã€‚ä½œè€…å¯¹æ¯”è¯„ä¼°äº†å·ç§¯ç¥ç»ç½‘ç»œ (CNNs)ã€Vision Transformers ä»¥åŠåŸºäºå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ (CLIP) çš„é›¶æ ·æœ¬å­¦ä¹  (Zero-Shot Learning) æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚å®éªŒå‘ç°ï¼ŒCNNs åœ¨é¢å¯¹é¢†åŸŸåç§» (Domain Shift) æ—¶é²æ£’æ€§æœ‰é™ï¼Œè€Œ Vision Transformers å‡­å€Ÿæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾çš„èƒ½åŠ›å±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–æ€§èƒ½ã€‚å°¤ä¸ºå…³é”®çš„æ˜¯ï¼ŒCLIP æ¨¡å‹æ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒå³å¯é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å®ç°ç—…å®³åˆ†ç±»ï¼Œå…·æœ‰æé«˜çš„é€‚åº”æ€§ä¸å¯è§£é‡Šæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé›¶æ ·æœ¬å­¦ä¹ æ˜¯è§£å†³æ¤ç‰©å¥åº·è¯Šæ–­é¢†åŸŸæ³›åŒ–éš¾é¢˜çš„ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„é¢†åŸŸè‡ªé€‚åº” (Domain Adaptation) ç­–ç•¥ï¼Œä¸ºå¼¥åˆå­¦æœ¯ç ”ç©¶ä¸å®é™…åº”ç”¨ä¹‹é—´çš„é¸¿æ²Ÿæä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18989v1",
      "published_date": "2025-11-24 11:08:01 UTC",
      "updated_date": "2025-11-24 11:08:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:09:22.139040+00:00"
    },
    {
      "arxiv_id": "2511.18987v1",
      "title": "Dynamic Mixture of Experts Against Severe Distribution Shifts",
      "title_zh": "åº”å¯¹å‰§çƒˆåˆ†å¸ƒåç§»çš„åŠ¨æ€æ··åˆä¸“å®¶æ¨¡å‹",
      "authors": [
        "Donghu Kim"
      ],
      "abstract": "The challenge of building neural networks that can continuously learn and adapt to evolving data streams is central to the fields of continual learning (CL) and reinforcement learning (RL). This lifelong learning problem is often framed in terms of the plasticity-stability dilemma, focusing on issues like loss of plasticity and catastrophic forgetting. Unlike neural networks, biological brains maintain plasticity through capacity growth, inspiring researchers to explore similar approaches in artificial networks, such as adding capacity dynamically. Prior solutions often lack parameter efficiency or depend on explicit task indices, but Mixture-of-Experts (MoE) architectures offer a promising alternative by specializing experts for distinct distributions. This paper aims to evaluate a DynamicMoE approach for continual and reinforcement learning environments and benchmark its effectiveness against existing network expansion methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¥ç»ç½‘ç»œåœ¨æŒç»­å­¦ä¹ (Continual Learning)å’Œå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­é¢ä¸´çš„å¯å¡‘æ€§-ç¨³å®šæ€§å›°å¢ƒ(plasticity-stability dilemma)ï¼Œå³å¦‚ä½•åº”å¯¹ä¸æ–­æ¼”åŒ–çš„æ•°æ®æµä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚å—ç”Ÿç‰©å¤§è„‘é€šè¿‡å®¹é‡å¢é•¿ç»´æŒå¯å¡‘æ€§çš„å¯å‘ï¼Œè®ºæ–‡æå‡ºå¹¶è¯„ä¼°äº†ä¸€ç§åä¸ºDynamicMoEï¼ˆåŠ¨æ€æ··åˆä¸“å®¶ï¼‰çš„æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€å¢åŠ ç½‘ç»œå®¹é‡æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ··åˆä¸“å®¶æ¨¡å‹(Mixture-of-Experts)ä½¿ä¸åŒä¸“å®¶é’ˆå¯¹ç‰¹å®šæ•°æ®åˆ†å¸ƒè¿›è¡Œä¸“é—¨åŒ–ï¼Œä¸”æ— éœ€ä¾èµ–æ˜¾å¼çš„ä»»åŠ¡ç´¢å¼•ï¼Œä»è€Œåœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚ç ”ç©¶åœ¨æŒç»­å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸‹å¯¹DynamicMoEè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å°†å…¶ä¸ç°æœ‰çš„ç½‘ç»œæ‰©å±•æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœéªŒè¯äº†DynamicMoEåœ¨å¤„ç†ä¸¥é‡åˆ†å¸ƒåç§»ã€ç¼“è§£ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)ä»¥åŠé˜²æ­¢å¯å¡‘æ€§ä¸§å¤±æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæ„å»ºèƒ½å¤ŸæŒç»­å­¦ä¹ å¹¶é€‚åº”ç¯å¢ƒå˜åŒ–çš„æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦çš„æŠ€æœ¯å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18987v1",
      "published_date": "2025-11-24 11:00:32 UTC",
      "updated_date": "2025-11-24 11:00:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:10:41.037751+00:00"
    },
    {
      "arxiv_id": "2511.18980v1",
      "title": "MOCLIP: A Foundation Model for Large-Scale Nanophotonic Inverse Design",
      "title_zh": "MOCLIPï¼šé¢å‘å¤§è§„æ¨¡çº³ç±³å…‰å­å­¦é€†å‘è®¾è®¡çš„åŸºç¡€æ¨¡å‹",
      "authors": [
        "S. Rodionov",
        "A. Burguete-Lopez",
        "M. Makarenko",
        "Q. Wang",
        "F. Getman",
        "A. Fratalocchi"
      ],
      "abstract": "Foundation models (FM) are transforming artificial intelligence by enabling generalizable, data-efficient solutions across different domains for a broad range of applications. However, the lack of large and diverse datasets limits the development of FM in nanophotonics. This work presents MOCLIP (Metasurface Optics Contrastive Learning Pretrained), a nanophotonic foundation model that integrates metasurface geometry and spectra within a shared latent space. MOCLIP employs contrastive learning to align geometry and spectral representations using an experimentally acquired dataset with a sample density comparable to ImageNet-1K. The study demonstrates MOCLIP inverse design capabilities for high-throughput zero-shot prediction at a rate of 0.2 million samples per second, enabling the design of a full 4-inch wafer populated with high-density metasurfaces in minutes. It also shows generative latent-space optimization reaching 97 percent accuracy. Finally, we introduce an optical information storage concept that uses MOCLIP to achieve a density of 0.1 Gbit per square millimeter at the resolution limit, exceeding commercial optical media by a factor of six. These results position MOCLIP as a scalable and versatile platform for next-generation photonic design and data-driven applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MOCLIP (Metasurface Optics Contrastive Learning Pretrained)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¤§è§„æ¨¡çº³ç±³å…‰å­å­¦é€†å‘è®¾è®¡å¼€å‘çš„é€šç”¨åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¯¹æ¯”å­¦ä¹  (Contrastive Learning) å°†è¶…è¡¨é¢ (Metasurface) çš„å‡ ä½•ç»“æ„ä¸å…‰è°±ä¿¡æ¯æ•´åˆåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­ï¼Œå¹¶åŸºäºè§„æ¨¡ä¸ ImageNet-1K ç›¸å½“çš„å®éªŒæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒã€‚MOCLIP æ”¯æŒæ¯ç§’ 20 ä¸‡ä¸ªæ ·æœ¬çš„é«˜é€šé‡é›¶æ ·æœ¬ (Zero-shot) é¢„æµ‹ï¼Œå¯åœ¨æ•°åˆ†é’Ÿå†…å®Œæˆ 4 è‹±å¯¸æ™¶åœ†çš„é«˜å¯†åº¦è¶…è¡¨é¢è®¾è®¡ï¼Œä¸”å…¶ç”Ÿæˆå¼æ½œåœ¨ç©ºé—´ä¼˜åŒ– (Latent-space Optimization) å‡†ç¡®ç‡é«˜è¾¾ 97%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†ä¸€ç§å…‰å­¦ä¿¡æ¯å­˜å‚¨æ¦‚å¿µï¼Œåˆ©ç”¨ MOCLIP åœ¨åˆ†è¾¨ç‡æé™ä¸‹å®ç°äº†æ¯å¹³æ–¹æ¯«ç±³ 0.1 Gbit çš„å­˜å‚¨å¯†åº¦ï¼Œæ¯”ç°æœ‰å•†ç”¨å…‰å­¦ä»‹è´¨é«˜å‡ºå…­å€ã€‚è¿™äº›æˆæœè¯æ˜äº† MOCLIP ä½œä¸ºä¸€ä¸ªå¯æ‰©å±•ä¸”å¤šåŠŸèƒ½çš„å¹³å°ï¼Œèƒ½æ˜¾è‘—æå‡å…‰å­å­¦è®¾è®¡æ•ˆç‡å¹¶èµ‹èƒ½æ–°å‹æ•°æ®é©±åŠ¨åº”ç”¨ã€‚",
      "categories": [
        "physics.optics",
        "cs.AI"
      ],
      "primary_category": "physics.optics",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18980v1",
      "published_date": "2025-11-24 10:54:19 UTC",
      "updated_date": "2025-11-24 10:54:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:10:39.848767+00:00"
    },
    {
      "arxiv_id": "2511.18977v1",
      "title": "FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning",
      "title_zh": "FastForward Pruningï¼šåŸºäºå•æ­¥å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹å‰ªæ",
      "authors": [
        "Xin Yuan",
        "Siqi Li",
        "Jiateng Wei",
        "Chengrui Zhu",
        "Yanming Wu",
        "Qingpeng Li",
        "Jiajun Lv",
        "Xiaoke Lan",
        "Jun Chen",
        "Yong Liu"
      ],
      "abstract": "Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are often hindered by prohibitive computational costs on large-scale models. To overcome this efficiency barrier, we propose FastForward Pruning. Its core is a decoupled, single-step RL framework that separates policy optimization from the complex budget satisfaction problem. Such a decoupling is crucial for efficiently searching the vast policy space of LLMs. This curriculum-based strategy begins with low-cost, simple tasks and gradually increases in complexity, significantly reducing the search's computational overhead. Evaluated on the LLaMA, Mistral, and OPT model families, our framework discovers pruning policies that achieve superior performance over strong heuristic baselines. Crucially, when compared to other search-based algorithms, our method achieves competitive or superior results at a fraction of the computational cost, demonstrating a clear advantage in search efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FastForward Pruningï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‰ªæè¿‡ç¨‹ä¸­å¯»æ‰¾æœ€ä¼˜éå‡åŒ€å±‚çº§ç¨€ç–åº¦åˆ†é…çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å…‹æœäº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰æ–¹æ³•è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªè§£è€¦çš„å•æ­¥RLæ¡†æ¶ï¼Œå®ƒå°†ç­–ç•¥ä¼˜åŒ–ä¸å¤æ‚çš„é¢„ç®—æ»¡è¶³é—®é¢˜åˆ†ç¦»å¼€æ¥ï¼Œä»è€Œæœ‰æ•ˆåœ°åœ¨å·¨å¤§çš„ç­–ç•¥ç©ºé—´ä¸­è¿›è¡Œæœç´¢ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§åŸºäºè¯¾ç¨‹ï¼ˆcurriculum-basedï¼‰çš„ç­–ç•¥ï¼Œä»ä½æˆæœ¬çš„ç®€å•ä»»åŠ¡å…¥æ‰‹å¹¶é€æ¸å¢åŠ å¤æ‚åº¦ï¼Œæ˜¾è‘—é™ä½äº†æœç´¢çš„è®¡ç®—å¼€é”€ã€‚åœ¨LLaMAã€Mistralå’ŒOPTæ¨¡å‹ç³»åˆ—ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶å‘ç°çš„å‰ªæç­–ç•¥åœ¨æ€§èƒ½ä¸Šä¼˜äºå¼ºå¯å‘å¼åŸºçº¿ã€‚ä¸å…¶ä»–åŸºäºæœç´¢çš„ç®—æ³•ç›¸æ¯”ï¼ŒFastForward Pruningä»¥æä½çš„è®¡ç®—æˆæœ¬å®ç°äº†å…·æœ‰ç«äº‰åŠ›ç”šè‡³æ›´ä¼˜çš„ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨æœç´¢æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 2 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.18977v1",
      "published_date": "2025-11-24 10:47:55 UTC",
      "updated_date": "2025-11-24 10:47:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-26T10:11:53.707856+00:00"
    },
    {
      "arxiv_id": "2511.21755v2",
      "title": "Who Owns the Knowledge? Copyright, GenAI, and the Future of Academic Publishing",
      "title_zh": "çŸ¥è¯†å½’è°æ‰€æœ‰ï¼Ÿç‰ˆæƒã€ç”Ÿæˆå¼ AI ä¸å­¦æœ¯å‡ºç‰ˆçš„æœªæ¥",
      "authors": [
        "Dmitry Kochetkov"
      ],
      "abstract": "The integration of generative artificial intelligence (GenAI) and large language models (LLMs) into scientific research and higher education presents a paradigm shift, offering revolutionizing opportunities while simultaneously raising profound ethical, legal, and regulatory questions. This study examines the complex intersection of AI and science, with a specific focus on the challenges posed to copyright law and the principles of open science. The author argues that current regulatory frameworks in key jurisdictions like the United States, China, the European Union, and the United Kingdom, while aiming to foster innovation, contain significant gaps, particularly concerning the use of copyrighted works and open science outputs for AI training. Widely adopted licensing mechanisms, such as Creative Commons, fail to adequately address the nuances of AI training, and the pervasive lack of attribution within AI systems fundamentally challenges established notions of originality. While current doctrine treats AI training as potentially fair use, this paper argues such mechanisms are inadequate and that copyright holders should retain explicit opt-out rights regardless of fair use doctrine. Instead, the author advocates for upholding authors' rights to refuse the use of their works for AI training and proposes that universities assume a leading role in shaping responsible AI governance. The conclusion is that a harmonized international legislative effort is urgently needed to ensure transparency, protect intellectual property, and prevent the emergence of an oligopolistic market structure that could prioritize commercial profit over scientific integrity and equitable knowledge production. This is a substantially expanded and revised version of a work originally presented at the 20th International Conference on Scientometrics & Informetrics (Kochetkov, 2025).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(GenAI)å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç§‘å­¦ç ”ç©¶ä¸­å¸¦æ¥çš„ä¼¦ç†ã€æ³•å¾‹åŠç›‘ç®¡æŒ‘æˆ˜ï¼Œé‡ç‚¹åˆ†æäº†å…¶å¯¹ç‰ˆæƒæ³•(Copyright Law)å’Œå¼€æ”¾ç§‘å­¦(Open Science)åŸåˆ™çš„å†²å‡»ã€‚ä½œè€…æŒ‡å‡ºç¾å›½ã€ä¸­å›½ã€æ¬§ç›ŸåŠè‹±å›½ç­‰ä¸»è¦å¸æ³•ç®¡è¾–åŒºçš„ç°è¡Œç›‘ç®¡æ¡†æ¶å­˜åœ¨æ˜¾è‘—æ¼æ´ï¼Œå°¤å…¶åœ¨Creative Commonsç­‰ç°æœ‰æˆæƒæœºåˆ¶æ— æ³•æœ‰æ•ˆåº”å¯¹AIè®­ç»ƒä¸­çš„è‘—ä½œæƒå½’å±åŠæ¥æºå½’å±(Attribution)ç¼ºå¤±é—®é¢˜ã€‚é’ˆå¯¹å½“å‰æ³•å¾‹æ¡†æ¶å€¾å‘äºå°†AIè®­ç»ƒè§†ä¸ºåˆç†ä½¿ç”¨(Fair Use)çš„è§‚ç‚¹ï¼Œè¯¥è®ºæ–‡ä¸»å¼ ç‰ˆæƒæŒæœ‰äººåº”æ‹¥æœ‰æ˜ç¡®çš„é€€å‡ºæƒ(Opt-out rights)ï¼Œä»¥ä¿æŠ¤ä½œè€…æ‹’ç»å…¶ä½œå“è¢«ç”¨äºæ¨¡å‹è®­ç»ƒçš„æƒåˆ©ã€‚ç ”ç©¶è¿›ä¸€æ­¥å»ºè®®å¤§å­¦åº”åœ¨æ„å»ºè´Ÿè´£ä»»çš„AIæ²»ç†ä¸­å‘æŒ¥é¢†å¯¼ä½œç”¨ï¼Œå¹¶å‘¼åé€šè¿‡å›½é™…ç«‹æ³•åä½œç¡®ä¿é€æ˜åº¦ã€‚æœ€ç»ˆç›®æ ‡æ˜¯é˜²æ­¢å½¢æˆä»¥å•†ä¸šåˆ©æ¶¦ä¸ºå¯¼å‘çš„å¯¡å¤´å„æ–­å¸‚åœºï¼Œä»è€Œç»´æŠ¤ç§‘å­¦è¯šä¿¡å’Œå…¬å¹³çš„çŸ¥è¯†ç”Ÿäº§ä½“ç³»ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.DL",
      "comment": "The second version version substantially revises the original preprint through expanded legal analysis, representation of the new technical standard (RSL 1.0), and removing substantial material lacking direct relevance to copyright and AI training",
      "pdf_url": "https://arxiv.org/pdf/2511.21755v2",
      "published_date": "2025-11-24 10:34:38 UTC",
      "updated_date": "2026-01-18 18:17:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-26T10:12:17.751473+00:00"
    },
    {
      "arxiv_id": "2511.18966v1",
      "title": "LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large Language Models",
      "title_zh": "LLM-CSECï¼šå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ C/C++ ä»£ç å®‰å…¨æ€§å®è¯è¯„ä¼°",
      "authors": [
        "Muhammad Usman Shahid",
        "Chuadhry Mujeeb Ahmed",
        "Rajiv Ranjan"
      ],
      "abstract": "The security of code generated by large language models (LLMs) is a significant concern, as studies indicate that such code often contains vulnerabilities and lacks essential defensive programming constructs. This work focuses on examining and evaluating the security of LLM-generated code, particularly in the context of C/C++. We categorized known vulnerabilities using the Common Weakness Enumeration (CWE) and, to study their criticality, mapped them to CVEs. We used ten different LLMs for code generation and analyzed the outputs through static analysis. The amount of CWEs present in AI-generated code is concerning. Our findings highlight the need for developers to be cautious when using LLM-generated code. This study provides valuable insights to advance automated code generation and encourage further research in this domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLM-CSECï¼Œæ—¨åœ¨å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„C/C++ä»£ç å®‰å…¨æ€§è¿›è¡Œå®è¯è¯„ä¼°ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨å¸¸è§ç¼ºé™·æšä¸¾(CWE)å¯¹å·²çŸ¥æ¼æ´è¿›è¡Œåˆ†ç±»ï¼Œå¹¶å°†å…¶æ˜ å°„åˆ°å¸¸è§æ¼æ´ä¸æŠ«éœ²(CVE)ä»¥åˆ†æå…¶å…³é”®æ€§ã€‚å®éªŒé€‰å–äº†åç§ä¸åŒçš„LLMsè¿›è¡Œä»£ç ç”Ÿæˆï¼Œå¹¶é‡‡ç”¨é™æ€åˆ†æ(static analysis)æ–¹æ³•å¯¹è¾“å‡ºç»“æœè¿›è¡Œäº†ç³»ç»ŸåŒ–æ£€æµ‹ã€‚ç ”ç©¶å‘ç°ï¼ŒAIç”Ÿæˆçš„ä»£ç ä¸­å­˜åœ¨æ•°é‡å¯è§‚çš„CWEæ¼æ´ï¼Œä¸”æ™®éç¼ºä¹å¿…è¦çš„é˜²å¾¡æ€§ç¼–ç¨‹æ„å»ºã€‚è¯¥ç ”ç©¶ç»“æœå¼ºè°ƒäº†å¼€å‘è€…åœ¨ä½¿ç”¨LLMç”Ÿæˆçš„ä»£ç æ—¶éœ€ä¿æŒå®¡æ…æ€åº¦ï¼Œå¹¶ä¸ºæå‡è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§åŠç›¸å…³é¢†åŸŸç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒè§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18966v1",
      "published_date": "2025-11-24 10:31:53 UTC",
      "updated_date": "2025-11-24 10:31:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:09:52.546787+00:00"
    },
    {
      "arxiv_id": "2511.18964v1",
      "title": "Synthesizing Visual Concepts as Vision-Language Programs",
      "title_zh": "å°†è§†è§‰æ¦‚å¿µåˆæˆä¸ºè§†è§‰è¯­è¨€ç¨‹åº",
      "authors": [
        "Antonia WÃ¼st",
        "Wolfgang Stammer",
        "Hikaru Shindo",
        "Lukas Helff",
        "Devendra Singh Dhami",
        "Kristian Kersting"
      ],
      "abstract": "Vision-Language models (VLMs) achieve strong performance on multimodal tasks but often fail at systematic visual reasoning tasks, leading to inconsistent or illogical outputs. Neuro-symbolic methods promise to address this by inducing interpretable logical rules, though they exploit rigid, domain-specific perception modules. We propose Vision-Language Programs (VLP), which combine the perceptual flexibility of VLMs with systematic reasoning of program synthesis. Rather than embedding reasoning inside the VLM, VLP leverages the model to produce structured visual descriptions that are compiled into neuro-symbolic programs. The resulting programs execute directly on images, remain consistent with task constraints, and provide human-interpretable explanations that enable easy shortcut mitigation. Experiments on synthetic and real-world datasets demonstrate that VLPs outperform direct and structured prompting, particularly on tasks requiring complex logical reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Vision-Language Programs (VLP)ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨ç³»ç»Ÿæ€§è§†è§‰æ¨ç†ä»»åŠ¡ä¸­å®¹æ˜“å‡ºç°ä¸ä¸€è‡´æˆ–é€»è¾‘é”™è¯¯çš„é—®é¢˜ã€‚VLP å°† VLMs çš„æ„ŸçŸ¥çµæ´»æ€§ä¸ç¨‹åºåˆæˆ (program synthesis) çš„ç³»ç»Ÿæ¨ç†èƒ½åŠ›ç›¸ç»“åˆï¼Œåˆ©ç”¨æ¨¡å‹ç”Ÿæˆç»“æ„åŒ–çš„è§†è§‰æè¿°å¹¶å°†å…¶ç¼–è¯‘ä¸ºç¥ç»ç¬¦å·ç¨‹åº (neuro-symbolic programs)ã€‚è¿™äº›ç¨‹åºå¯ä»¥ç›´æ¥åœ¨å›¾åƒä¸Šæ‰§è¡Œï¼Œæ—¢ä¿è¯äº†æ¨ç†è¿‡ç¨‹ç¬¦åˆä»»åŠ¡çº¦æŸï¼Œåˆæä¾›äº†äººç±»å¯è§£é‡Šçš„è¯´æ˜ï¼Œæœ‰åŠ©äºç¼“è§£å­¦ä¹ å¿«æ·æ–¹å¼ (shortcut mitigation) çš„é£é™©ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒVLP çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç›´æ¥æç¤ºå’Œç»“æ„åŒ–æç¤ºæ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤æ‚é€»è¾‘æ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18964v1",
      "published_date": "2025-11-24 10:30:33 UTC",
      "updated_date": "2025-11-24 10:30:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:10:27.447236+00:00"
    },
    {
      "arxiv_id": "2511.19537v1",
      "title": "Cross-Domain Generalization of Multimodal LLMs for Global Photovoltaic Assessment",
      "title_zh": "é¢å‘å…¨çƒå…‰ä¼è¯„ä¼°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è·¨åŸŸæ³›åŒ–",
      "authors": [
        "Muhao Guo",
        "Yang Weng"
      ],
      "abstract": "The rapid expansion of distributed photovoltaic (PV) systems poses challenges for power grid management, as many installations remain undocumented. While satellite imagery provides global coverage, traditional computer vision (CV) models such as CNNs and U-Nets require extensive labeled data and fail to generalize across regions. This study investigates the cross-domain generalization of a multimodal large language model (LLM) for global PV assessment. By leveraging structured prompts and fine-tuning, the model integrates detection, localization, and quantification within a unified schema. Cross-regional evaluation using the $Î”$F1 metric demonstrates that the proposed model achieves the smallest performance degradation across unseen regions, outperforming conventional CV and transformer baselines. These results highlight the robustness of multimodal LLMs under domain shift and their potential for scalable, transferable, and interpretable global PV mapping.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal LLMs) åœ¨è·¨åœ°åŸŸåˆ†å¸ƒå¼å…‰ä¼ (Distributed PV) è¯„ä¼°ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ¨¡å‹ (CNNs, U-Nets) åœ¨é¢å¯¹å…¨çƒå«æ˜Ÿå›¾åƒæ—¶å› ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ä¸”éš¾ä»¥å®ç°è·¨åŒºåŸŸæ³›åŒ–çš„æŒ‘æˆ˜ã€‚ç ”ç©¶é€šè¿‡ç»“æ„åŒ–æç¤º (Structured prompts) å’Œå¾®è°ƒ (Fine-tuning) æŠ€æœ¯ï¼Œå°†å…‰ä¼ç³»ç»Ÿçš„æ£€æµ‹ (Detection)ã€å®šä½ (Localization) å’Œé‡åŒ– (Quantification) æ•´åˆè¿›ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ã€‚åˆ©ç”¨ $Î”$F1 æŒ‡æ ‡è¿›è¡Œçš„è·¨åŒºåŸŸè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†æœªè§è¿‡çš„åŒºåŸŸæ—¶æ€§èƒ½é€€åŒ–æœ€å°ï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ CV æ¨¡å‹å’Œ Transformer åŸºçº¿ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†å¤šæ¨¡æ€ LLM åœ¨åº”å¯¹é¢†åŸŸåç§» (Domain shift) æ—¶çš„å¼ºå¤§é²æ£’æ€§ï¼Œä¸ºå®ç°å¯æ‰©å±•ã€å¯è¿ç§»ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„å…¨çƒå…‰ä¼åœ°å›¾ç»˜åˆ¶æä¾›äº†é‡è¦çš„æŠ€æœ¯æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.19537v1",
      "published_date": "2025-11-24 10:26:30 UTC",
      "updated_date": "2025-11-24 10:26:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:10:46.647910+00:00"
    },
    {
      "arxiv_id": "2511.18958v2",
      "title": "Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation",
      "title_zh": "åŸºäºåŒæ™ºèƒ½ä½“çš„å›¾å‹ç¼©å­¦ä¹ ï¼šå®ç°ä¸€è‡´çš„æ‹“æ‰‘é²æ£’æ€§è¯„ä¼°",
      "authors": [
        "Qisen Chai",
        "Yansong Wang",
        "Junjie Huang",
        "Tao Jia"
      ],
      "abstract": "As graph-structured data grow increasingly large, evaluating their robustness under adversarial attacks becomes computationally expensive and difficult to scale. To address this challenge, we propose to compress graphs into compact representations that preserve both topological structure and robustness profile, enabling efficient and reliable evaluation. We propose Cutter, a dual-agent reinforcement learning framework composed of a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA), which collaboratively identify structurally vital and redundant nodes for guided compression. Cutter incorporates three key strategies to enhance learning efficiency and compression quality: trajectory-level reward shaping to transform sparse trajectory returns into dense, policy-equivalent learning signals; prototype-based shaping to guide decisions using behavioral patterns from both high- and low-return trajectories; and cross-agent imitation to enable safer and more transferable exploration. Experiments on multiple real-world graphs demonstrate that Cutter generates compressed graphs that retain essential static topological properties and exhibit robustness degradation trends highly consistent with the original graphs under various attack scenarios, thereby significantly improving evaluation efficiency without compromising assessment fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡å›¾æ•°æ®åœ¨å¯¹æŠ—æ”»å‡»ä¸‹é²æ£’æ€§è¯„ä¼°è®¡ç®—æˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•çš„é—®é¢˜ï¼Œæå‡ºäº†é€šè¿‡å›¾å‹ç¼©æ¥ä¿ç•™æ‹“æ‰‘ç»“æ„ä¸é²æ£’æ€§ç‰¹å¾çš„è¯„ä¼°æ–¹æ³•ã€‚ä½œè€…å¼€å‘äº†åä¸ºCutterçš„åŒæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (Dual-Agent Reinforcement Learning)æ¡†æ¶ï¼Œç”±é‡è¦æ€§æ£€æµ‹æ™ºèƒ½ä½“(Vital Detection Agent, VDA)å’Œå†—ä½™æ£€æµ‹æ™ºèƒ½ä½“(Redundancy Detection Agent, RDA)åä½œè¯†åˆ«å…³é”®ä¸å†—ä½™èŠ‚ç‚¹ä»¥å¼•å¯¼å‹ç¼©ã€‚è¯¥æ¡†æ¶é›†æˆäº†è½¨è¿¹çº§å¥–åŠ±å¡‘é€ (Trajectory-level Reward Shaping)ã€åŸºäºåŸå‹çš„å¡‘é€ å’Œè·¨æ™ºèƒ½ä½“æ¨¡ä»¿ä¸‰é¡¹æ ¸å¿ƒç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡ä¸å‹ç¼©è´¨é‡ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œå›¾æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒCutterç”Ÿæˆçš„å‹ç¼©å›¾åœ¨ä¿ç•™é™æ€æ‹“æ‰‘å±æ€§(Topological Properties)çš„åŒæ—¶ï¼Œå…¶é²æ£’æ€§é€€åŒ–è¶‹åŠ¿ä¸åŸå›¾åœ¨å„ç§æ”»å‡»åœºæ™¯ä¸‹ä¿æŒé«˜åº¦ä¸€è‡´ã€‚è¯¥æ–¹æ³•åœ¨ç¡®ä¿è¯„ä¼°ä¿çœŸåº¦çš„å‰æä¸‹å¤§å¹…ä¼˜åŒ–äº†è®¡ç®—æ•ˆç‡ï¼Œä¸ºå¤§è§„æ¨¡å¤æ‚ç½‘ç»œçš„ç¨³å¥æ€§åˆ†ææä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18958v2",
      "published_date": "2025-11-24 10:19:58 UTC",
      "updated_date": "2025-11-25 12:33:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:10:56.135429+00:00"
    },
    {
      "arxiv_id": "2511.19536v1",
      "title": "AttackPilot: Autonomous Inference Attacks Against ML Services With LLM-Based Agents",
      "title_zh": "AttackPilotï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„æœºå™¨å­¦ä¹ æœåŠ¡è‡ªä¸»æ¨æ–­æ”»å‡»",
      "authors": [
        "Yixin Wu",
        "Rui Wen",
        "Chi Cui",
        "Michael Backes",
        "Yang Zhang"
      ],
      "abstract": "Inference attacks have been widely studied and offer a systematic risk assessment of ML services; however, their implementation and the attack parameters for optimal estimation are challenging for non-experts. The emergence of advanced large language models presents a promising yet largely unexplored opportunity to develop autonomous agents as inference attack experts, helping address this challenge. In this paper, we propose AttackPilot, an autonomous agent capable of independently conducting inference attacks without human intervention. We evaluate it on 20 target services. The evaluation shows that our agent, using GPT-4o, achieves a 100.0% task completion rate and near-expert attack performance, with an average token cost of only $0.627 per run. The agent can also be powered by many other representative LLMs and can adaptively optimize its strategy under service constraints. We further perform trace analysis, demonstrating that design choices, such as a multi-agent framework and task-specific action spaces, effectively mitigate errors such as bad plans, inability to follow instructions, task context loss, and hallucinations. We anticipate that such agents could empower non-expert ML service providers, auditors, or regulators to systematically assess the risks of ML services without requiring deep domain expertise.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AttackPilotï¼Œä¸€ç§èƒ½å¤Ÿç‹¬ç«‹è¿›è¡ŒInference attacksï¼ˆæ¨ç†æ”»å‡»ï¼‰çš„è‡ªä¸»æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³éä¸“å®¶åœ¨è¯„ä¼°æœºå™¨å­¦ä¹ æœåŠ¡(ML services)é£é™©æ—¶é¢ä¸´çš„æŠ€æœ¯é—¨æ§›ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶(multi-agent framework)å’Œä»»åŠ¡ç‰¹å®šåŠ¨ä½œç©ºé—´ï¼Œå®ç°äº†æ— éœ€äººå·¥å¹²é¢„çš„è‡ªåŠ¨åŒ–æ”»å‡»æµç¨‹ã€‚å®éªŒåœ¨20ä¸ªç›®æ ‡æœåŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºä½¿ç”¨GPT-4oçš„AttackPilotè¾¾åˆ°äº†100%çš„ä»»åŠ¡å®Œæˆç‡å’Œæ¥è¿‘ä¸“å®¶çº§åˆ«çš„è¡¨ç°ï¼Œä¸”å•æ¬¡è¿è¡Œæˆæœ¬ä»…ä¸º0.627ç¾å…ƒã€‚è¯¥æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªé€‚åº”ä¼˜åŒ–ä¸åŒæœåŠ¡çº¦æŸä¸‹çš„ç­–ç•¥ï¼Œå¹¶æœ‰æ•ˆç¼“è§£äº†LLMåœ¨å¤æ‚ä»»åŠ¡ä¸­å¸¸è§çš„å¹»è§‰ã€ä¸Šä¸‹æ–‡ä¸¢å¤±å’Œè§„åˆ’å¤±æ•ˆç­‰é—®é¢˜ã€‚è¿™é¡¹å·¥ä½œä¸ºç›‘ç®¡æœºæ„å’Œéä¸“ä¸šæä¾›å•†æä¾›äº†ä¸€ç§ä½æˆæœ¬ã€é«˜æ•ˆçš„å·¥å…·ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç¼ºä¹æ·±åšé¢†åŸŸçŸ¥è¯†çš„æƒ…å†µä¸‹ç³»ç»Ÿåœ°è¯„ä¼°ML servicesçš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19536v1",
      "published_date": "2025-11-24 10:14:14 UTC",
      "updated_date": "2025-11-24 10:14:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:11:23.347268+00:00"
    },
    {
      "arxiv_id": "2511.18955v1",
      "title": "Active Inference is a Subtype of Variational Inference",
      "title_zh": "ä¸»åŠ¨æ¨ç†æ˜¯å˜åˆ†æ¨ç†çš„ä¸€ç§å­ç±»å‹",
      "authors": [
        "Wouter W. L. Nuijten",
        "Mykola Lukashchuk"
      ],
      "abstract": "Automated decision-making under uncertainty requires balancing exploitation and exploration. Classical methods treat these separately using heuristics, while Active Inference unifies them through Expected Free Energy (EFE) minimization. However, EFE minimization is computationally expensive, limiting scalability. We build on recent theory recasting EFE minimization as variational inference, formally unifying it with Planning-as-Inference and showing the epistemic drive as a unique entropic contribution. Our main contribution is a novel message-passing scheme for this unified objective, enabling scalable Active Inference in factored-state MDPs and overcoming high-dimensional planning intractability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Active Inferenceä¸Variational Inferenceä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºActive Inferenceæ˜¯å…¶ä¸€ç§ç‰¹å®šçš„å­ç±»å‹ã€‚åœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„è‡ªåŠ¨åŒ–å†³ç­–ä¸­ï¼ŒActive Inferenceé€šè¿‡æœ€å°åŒ–Expected Free Energy (EFE) å®ç°äº†åˆ©ç”¨(exploitation)ä¸æ¢ç´¢(exploration)çš„ç»Ÿä¸€ã€‚é’ˆå¯¹EFEæœ€å°åŒ–è®¡ç®—æˆæœ¬è¿‡é«˜ä¸”éš¾ä»¥æ‰©å±•çš„å±€é™æ€§ï¼Œæœ¬æ–‡å°†EFEé‡æ„ä¸ºVariational Inferenceï¼Œå¹¶å°†å…¶ä¸Planning-as-Inferenceæ¡†æ¶è¿›è¡Œå½¢å¼ä¸Šçš„ç»Ÿä¸€ï¼Œæ­ç¤ºäº†è®¤çŸ¥é©±åŠ¨åŠ›(epistemic drive)ä½œä¸ºç‹¬ç‰¹ç†µè´¡çŒ®çš„æœ¬è´¨ã€‚è®ºæ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯æå‡ºäº†ä¸€ç§é’ˆå¯¹è¯¥ç»Ÿä¸€ç›®æ ‡çš„æ–°å‹message-passing schemeï¼Œä½¿å¾—åœ¨factored-state MDPsä¸­åº”ç”¨å¯æ‰©å±•çš„Active Inferenceæˆä¸ºå¯èƒ½ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†é«˜ç»´è§„åˆ’ä¸­çš„è®¡ç®—ä¸å¯è¡Œæ€§(intractability)é—®é¢˜ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„æ™ºèƒ½å†³ç­–æä¾›äº†æ›´å…·æ‰©å±•æ€§çš„è®¡ç®—æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the EIML Workshop 2025 at EurIPS (non-archival)",
      "pdf_url": "https://arxiv.org/pdf/2511.18955v1",
      "published_date": "2025-11-24 10:14:09 UTC",
      "updated_date": "2025-11-24 10:14:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:13:44.748612+00:00"
    },
    {
      "arxiv_id": "2511.20698v1",
      "title": "On the Role of Hidden States of Modern Hopfield Network in Transformer",
      "title_zh": "è®ºç°ä»£ Hopfield ç½‘ç»œéšçŠ¶æ€åœ¨ Transformer ä¸­çš„ä½œç”¨",
      "authors": [
        "Tsubasa Masumura",
        "Masato Taki"
      ],
      "abstract": "Associative memory models based on Hopfield networks and self-attention based on key-value mechanisms have been popular approaches in the study of memory mechanisms in deep learning. It has been pointed out that the state update rule of the modern Hopfield network (MHN) in the adiabatic approximation is in agreement with the self-attention layer of Transformer. In this paper, we go beyond this approximation and investigate the relationship between MHN and self-attention. Our results show that the correspondence between Hopfield networks and Transformers can be established in a more generalized form by adding a new variable, the hidden state derived from the MHN, to self-attention. This new attention mechanism, modern Hopfield attention (MHA), allows the inheritance of attention scores from the input layer of the Transformer to the output layer, which greatly improves the nature of attention weights. In particular, we show both theoretically and empirically that MHA hidden states significantly improve serious problem of deep Transformers known as rank collapse and token uniformity. We also confirm that MHA can systematically improve accuracy without adding training parameters to the Vision Transformer or GPT. Our results provide a new case in which Hopfield networks can be a useful perspective for improving the Transformer architecture.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†ç°ä»£éœæ™®è²å°”å¾·ç½‘ç»œ(Modern Hopfield Network, MHN)ä¸Transformerè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚ä½œè€…è¶…è¶Šäº†ä»¥å¾€çš„ç»çƒ­è¿‘ä¼¼(adiabatic approximation)é™åˆ¶ï¼Œé€šè¿‡å¼•å…¥ç”±MHNå¯¼å‡ºçš„éšè—çŠ¶æ€(hidden state)ï¼Œå»ºç«‹äº†ä¸€ç§æ›´ä¸ºå¹¿ä¹‰çš„ç°ä»£éœæ™®è²å°”å¾·æ³¨æ„åŠ›(Modern Hopfield Attention, MHA)æœºåˆ¶ã€‚è¿™ç§æ–°æœºåˆ¶å…è®¸æ³¨æ„åŠ›è¯„åˆ†ä»Transformerçš„è¾“å…¥å±‚ç»§æ‰¿è‡³è¾“å‡ºå±‚ï¼Œä»è€Œæœ‰æ•ˆä¼˜åŒ–äº†æ³¨æ„åŠ›æƒé‡çš„æ€§è´¨ã€‚ç†è®ºä¸å®éªŒå…±åŒè¯æ˜ï¼ŒMHAèƒ½å¤Ÿæ˜¾è‘—ç¼“è§£æ·±åº¦Transformerä¸­ä¸¥é‡çš„ç§©åç¼©(rank collapse)å’ŒTokenå‡åŒ€æ€§(token uniformity)é—®é¢˜ã€‚åœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒå‚æ•°çš„å‰æä¸‹ï¼Œè¯¥æ–¹æ³•ç³»ç»Ÿæ€§åœ°æå‡äº†Vision Transformerå’ŒGPTç­‰ä¸»æµæ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œä¸ºåˆ©ç”¨Hopfieldç½‘ç»œè§†è§’æ”¹è¿›Transformeræ¶æ„æä¾›äº†æ–°çš„ç†è®ºä¾æ®å’Œå®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 accepted",
      "pdf_url": "https://arxiv.org/pdf/2511.20698v1",
      "published_date": "2025-11-24 10:06:31 UTC",
      "updated_date": "2025-11-24 10:06:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:13:09.342810+00:00"
    },
    {
      "arxiv_id": "2511.18936v1",
      "title": "SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression",
      "title_zh": "SWANï¼šé€šè¿‡å…è§£å‹ KV ç¼“å­˜å‹ç¼©é™ä½æ¨ç†å†…å­˜å ç”¨çš„ç¨€ç–ç­›é€‰æ³¨æ„åŠ›",
      "authors": [
        "Santhosh G S",
        "Saurav Prakash",
        "Balaraman Ravindran"
      ],
      "abstract": "Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SWANï¼Œä¸€ç§æ— éœ€å¾®è°ƒ(fine-tuning-free)ä¸”æ— éœ€è§£å‹(decompression-free)çš„KV-cacheå‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‡ªå›å½’æ¨ç†ä¸­å› KV-cacheå ç”¨æµ·é‡å†…å­˜è€Œå¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚SWANé€šè¿‡ä½¿ç”¨ç¦»çº¿æ­£äº¤çŸ©é˜µ(orthogonal matrix)å¯¹KV-cacheè¿›è¡Œæ—‹è½¬å’Œå‰ªæï¼Œä½¿å…¶èƒ½å¤Ÿç›´æ¥å‚ä¸æ³¨æ„åŠ›æœºåˆ¶(attention)è®¡ç®—ï¼Œä»è€Œå½»åº•æ¶ˆé™¤äº†ä¼ ç»Ÿå‹ç¼©æ–¹æ³•ä¸­å¤æ‚çš„é‡æ„ä¸æ˜¾å¼è§£å‹æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç»“åˆå°‘é‡å¯†é›†ç¼“å†²åŒº(dense buffer)çš„æƒ…å†µä¸‹ï¼ŒSWANåœ¨å®ç°å•tokenå†…å­˜èŠ‚çœ50%-60%çš„åŒæ—¶ï¼Œä¾ç„¶èƒ½ä¿æŒæ¥è¿‘æœªå‹ç¼©åŸºå‡†æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSWANå…·å¤‡ç‹¬ç‰¹çš„è¿è¡Œæ—¶å¯è°ƒ(runtime-tunable)å‹ç¼©èƒ½åŠ›ï¼Œå…è®¸æ ¹æ®å®é™…éœ€æ±‚åŠ¨æ€è°ƒæ•´å†…å­˜å ç”¨ï¼Œè€Œæ— éœ€å›ºå®šçš„ç¦»çº¿é…ç½®ã€‚è¿™ç§å…¼å…·é«˜æ•ˆèƒ½ã€çµæ´»æ€§ä¸é›¶è§£å‹å¼€é”€çš„ç‰¹æ€§ï¼Œä¸ºé•¿ä¸Šä¸‹æ–‡LLMsçš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†åˆ‡å®å¯è¡Œçš„ä¼˜åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18936v1",
      "published_date": "2025-11-24 09:41:24 UTC",
      "updated_date": "2025-11-24 09:41:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:13:38.534490+00:00"
    },
    {
      "arxiv_id": "2511.18934v1",
      "title": "Skeletons Matter: Dynamic Data Augmentation for Text-to-Query",
      "title_zh": "éª¨æ¶è‡³å…³é‡è¦ï¼šé¢å‘æ–‡æœ¬åˆ°æŸ¥è¯¢çš„åŠ¨æ€æ•°æ®å¢å¼º",
      "authors": [
        "Yuchen Ji",
        "Bo Xu",
        "Jie Shi",
        "Jiaqing Liang",
        "Deqing Yang",
        "Yu Mao",
        "Hai Chen",
        "Yanghua Xiao"
      ],
      "abstract": "The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºæŸ¥è¯¢è¯­è¨€çš„ Text-to-Query ä»»åŠ¡ï¼Œå¹¶é’ˆå¯¹ç°æœ‰ç ”ç©¶åœ¨ä¸åŒè¯­è¨€é—´æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæ­£å¼å®šä¹‰äº†ç»Ÿä¸€å„ç§æŸ¥è¯¢è¯­è¨€çš„ä»»åŠ¡èŒƒå¼ã€‚ä½œè€…è¯†åˆ«å‡º query skeletons æ˜¯æ­¤ç±»ä»»åŠ¡çš„å…±åŒä¼˜åŒ–ç›®æ ‡ï¼Œå¹¶æ®æ­¤æå‡ºäº†ä¸€ç§é€šç”¨çš„åŠ¨æ€æ•°æ®å¢å¼ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ˜¾å¼è¯Šæ–­æ¨¡å‹åœ¨å¤„ç† query skeletons æ—¶çš„ç‰¹å®šè–„å¼±ç¯èŠ‚ï¼Œä»è€Œç”Ÿæˆå…·æœ‰é’ˆå¯¹æ€§çš„åˆæˆè®­ç»ƒæ•°æ®ã€‚åœ¨å››ä¸ª Text-to-Query åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä»…éœ€å°‘é‡åˆæˆæ•°æ®å³å¯è¾¾åˆ° state-of-the-art æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ä»…å±•ç°äº†æ‰€ææ–¹æ³•çš„é«˜æ•ˆæ€§ä¸æ™®é€‚æ€§ï¼Œè¿˜ä¸º Text-to-Query ä»»åŠ¡çš„ç»Ÿä¸€åŒ–ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.18934v1",
      "published_date": "2025-11-24 09:39:03 UTC",
      "updated_date": "2025-11-24 09:39:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:12:41.341386+00:00"
    },
    {
      "arxiv_id": "2511.18933v1",
      "title": "Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations",
      "title_zh": "å…¼é¡¾è´Ÿè´£ä»»äººå·¥æ™ºèƒ½è€ƒé‡çš„å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±æ”»å‡»é˜²å¾¡",
      "authors": [
        "Ryan Wong",
        "Hosea David Yu Fei Ng",
        "Dhananjai Sharma",
        "Glenn Jun Jie Ng",
        "Kavishvaran Srinivasan"
      ],
      "abstract": "Large Language Models (LLMs) remain susceptible to jailbreak exploits that bypass safety filters and induce harmful or unethical behavior. This work presents a systematic taxonomy of existing jailbreak defenses across prompt-level, model-level, and training-time interventions, followed by three proposed defense strategies. First, a Prompt-Level Defense Framework detects and neutralizes adversarial inputs through sanitization, paraphrasing, and adaptive system guarding. Second, a Logit-Based Steering Defense reinforces refusal behavior through inference-time vector steering in safety-sensitive layers. Third, a Domain-Specific Agent Defense employs the MetaGPT framework to enforce structured, role-based collaboration and domain adherence. Experiments on benchmark datasets show substantial reductions in attack success rate, achieving full mitigation under the agent-based defense. Overall, this study highlights how jailbreaks pose a significant security threat to LLMs and identifies key intervention points for prevention, while noting that defense strategies often involve trade-offs between safety, performance, and scalability. Code is available at: https://github.com/Kuro0911/CS5446-Project",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) å®¹æ˜“å—åˆ°è¶Šç‹±åˆ©ç”¨ (jailbreak exploits) ä»¥ç»•è¿‡å®‰å…¨è¿‡æ»¤å¹¶è¯±å¯¼æœ‰å®³è¡Œä¸ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸‰ç§é˜²å¾¡ç­–ç•¥å¹¶å»ºç«‹äº†ç³»ç»Ÿçš„é˜²å¾¡åˆ†ç±»å­¦ã€‚é¦–å…ˆï¼Œæç¤ºå±‚çº§é˜²å¾¡æ¡†æ¶ (Prompt-Level Defense Framework) é€šè¿‡æ•°æ®æ¸…æ´—ã€æ”¹å†™å’Œè‡ªé€‚åº”ç³»ç»Ÿä¿æŠ¤æ¥æ£€æµ‹å¹¶ä¸­å’Œå¯¹æŠ—æ€§è¾“å…¥ã€‚å…¶æ¬¡ï¼ŒåŸºäºé€»è¾‘å€¼çš„å¼•å¯¼é˜²å¾¡ (Logit-Based Steering Defense) åœ¨å®‰å…¨æ•æ„Ÿå±‚åˆ©ç”¨æ¨ç†æ—¶å‘é‡å¼•å¯¼ (inference-time vector steering) æ¥å¼ºåŒ–æ‹’ç»è¡Œä¸ºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†é¢†åŸŸç‰¹å®šæ™ºèƒ½ä½“é˜²å¾¡ (Domain-Specific Agent Defense)ï¼Œåˆ©ç”¨ MetaGPT æ¡†æ¶é€šè¿‡ç»“æ„åŒ–çš„è§’è‰²åä½œå’Œé¢†åŸŸéµå¾ªæ¥æå‡å®‰å…¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ–¹æ³•æ˜¾è‘—é™ä½äº†æ”»å‡»æˆåŠŸç‡ï¼Œå…¶ä¸­åŸºäºæ™ºèƒ½ä½“çš„é˜²å¾¡å®ç°äº†å®Œå…¨ç¼“è§£ (full mitigation)ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶åœ¨æå‡ LLMs å®‰å…¨æ€§çš„åŒæ—¶ï¼Œä¹Ÿæ·±å…¥åˆ†æäº†é˜²å¾¡ç­–ç•¥åœ¨å®‰å…¨æ€§ã€æ€§èƒ½å’Œå¯æ‰©å±•æ€§ (scalability) ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages including appendix; technical report; NeurIPS 2024 style",
      "pdf_url": "https://arxiv.org/pdf/2511.18933v1",
      "published_date": "2025-11-24 09:38:11 UTC",
      "updated_date": "2025-11-24 09:38:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:13:28.535553+00:00"
    },
    {
      "arxiv_id": "2511.18931v1",
      "title": "Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs",
      "title_zh": "è”ç½‘æœç´¢ï¼šç°ä»£å¤§è¯­è¨€æ¨¡å‹å†…ç½®ç½‘é¡µæœç´¢èƒ½åŠ›åˆ†æ",
      "authors": [
        "Sahil Kale"
      ],
      "abstract": "Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.",
      "tldr_zh": "æœ¬ç ”ç©¶åˆ†æäº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å®é™…éœ€æ±‚ä¸‹è°ƒç”¨å†…éƒ¨ç½‘ç»œæœç´¢èƒ½åŠ›çš„æ ¡å‡†æ•ˆç‡ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å•†ä¸šæ¨¡å‹æœç´¢å¿…è¦æ€§ä¸æœ‰æ•ˆæ€§çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŒ…å«é’ˆå¯¹æ¨¡å‹å†…éƒ¨ç½®ä¿¡åº¦æµ‹è¯•çš„é™æ€æ•°æ®ï¼Œä»¥åŠé’ˆå¯¹æ—¶æ•ˆæ€§ä¿¡æ¯æ£€ç´¢æµ‹è¯•çš„åŠ¨æ€æ•°æ®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ç½‘ç»œè®¿é—®æå‡äº†GPT-5-miniå’ŒClaude Haiku 4.5çš„é™æ€å‡†ç¡®ç‡ï¼Œä½†å…¶ç½®ä¿¡åº¦æ ¡å‡†(Confidence Calibration)åè€Œå˜å·®ã€‚åœ¨åŠ¨æ€æŸ¥è¯¢ä¸­ï¼Œå—é™äºè¾ƒå¼±çš„æŸ¥è¯¢è¡¨è¾¾(Query Formulation)èƒ½åŠ›ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æ™®éä½äº70%ï¼Œä¸”åœ¨æœç´¢åè¡¨ç°å‡ºæ˜æ˜¾çš„è¿‡åº¦è‡ªä¿¡å’Œä¸ä¸€è‡´æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå½“å‰çš„å†…ç½®ç½‘ç»œæœç´¢æ›´é€‚åˆä½œä¸ºä½å»¶è¿Ÿçš„éªŒè¯å±‚(Verification Layer)ï¼Œè€Œéç¨³å¥çš„åˆ†æå·¥å…·ï¼Œæ¨¡å‹åœ¨è¯†åˆ«æœç´¢éœ€æ±‚å’Œä¼˜åŒ–æ£€ç´¢ç­–ç•¥æ–¹é¢ä»æœ‰æ˜¾è‘—æå‡ç©ºé—´ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.18931v1",
      "published_date": "2025-11-24 09:37:43 UTC",
      "updated_date": "2025-11-24 09:37:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:13:10.731536+00:00"
    },
    {
      "arxiv_id": "2511.18930v1",
      "title": "Learning Solution Operators for Partial Differential Equations via Monte Carlo-Type Approximation",
      "title_zh": "åŸºäºè’™ç‰¹å¡ç½—å‹é€¼è¿‘çš„åå¾®åˆ†æ–¹ç¨‹è§£ç®—å­å­¦ä¹ ",
      "authors": [
        "Salah Eddine Choutri",
        "Prajwal Chauhan",
        "Othmane Mazhar",
        "Saif Eddin Jabari"
      ],
      "abstract": "The Monte Carlo-type Neural Operator (MCNO) introduces a lightweight architecture for learning solution operators for parametric PDEs by directly approximating the kernel integral using a Monte Carlo approach. Unlike Fourier Neural Operators, MCNO makes no spectral or translation-invariance assumptions. The kernel is represented as a learnable tensor over a fixed set of randomly sampled points. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with low computational cost, providing a simple and practical alternative to spectral and graph-based neural operators.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Monte Carlo-type Neural Operator (MCNO)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå­¦ä¹ å‚æ•°åŒ– Partial Differential Equations (PDEs) è§£ç®—å­çš„è½»é‡çº§æ¶æ„ã€‚ä¸åŒäº Fourier Neural Operators (FNO)ï¼ŒMCNO ä¸ä¾èµ–é¢‘è°±æˆ–å¹³ç§»ä¸å˜æ€§å‡è®¾ï¼Œè€Œæ˜¯é€šè¿‡ Monte Carlo æ–¹æ³•ç›´æ¥è¿‘ä¼¼æ ¸ç§¯åˆ†ã€‚è¯¥æ¨¡å‹å°†æ ¸è¡¨ç¤ºä¸ºåœ¨ä¸€ç»„å›ºå®šçš„éšæœºé‡‡æ ·ç‚¹ä¸Šçš„å¯å­¦ä¹ å¼ é‡ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šä¸ªç½‘æ ¼åˆ†è¾¨ç‡ä¸Šè¿›è¡Œæ³›åŒ–ï¼Œä¸”æ— éœ€ä¾èµ–å›ºå®šçš„å…¨å±€åŸºå‡½æ•°æˆ–åœ¨è®­ç»ƒä¸­è¿›è¡Œé‡å¤é‡‡æ ·ã€‚åœ¨æ ‡å‡† 1D PDE åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMCNO åœ¨ä¿æŒä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†æå…·ç«äº‰åŠ›çš„å‡†ç¡®ç‡ã€‚è¯¥æ¶æ„ä¸ºé¢‘è°±å’Œå›¾ç¥ç»ç®—å­æä¾›äº†ä¸€ä¸ªç®€å•ä¸”å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†ç®—å­å­¦ä¹ åœ¨å¤„ç†å¤æ‚ç‰©ç†æ–¹ç¨‹æ—¶çš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 Workshop on Machine Learning and the Physical Sciences",
      "pdf_url": "https://arxiv.org/pdf/2511.18930v1",
      "published_date": "2025-11-24 09:35:10 UTC",
      "updated_date": "2025-11-24 09:35:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:13:23.430126+00:00"
    },
    {
      "arxiv_id": "2511.18926v1",
      "title": "MoodBench 1.0: An Evaluation Benchmark for Emotional Companionship Dialogue Systems",
      "title_zh": "MoodBench 1.0ï¼šæƒ…æ„Ÿé™ªä¼´å¯¹è¯ç³»ç»Ÿè¯„æµ‹åŸºå‡†",
      "authors": [
        "Haifeng Jing",
        "Yujie Hou",
        "Junfei Liu",
        "Rui Xie",
        "alan Xu",
        "Jinlong Ma",
        "Qichun Deng"
      ],
      "abstract": "With the rapid development of Large Language Models, dialogue systems are shifting from information tools to emotional companions, heralding the era of Emotional Companionship Dialogue Systems (ECDs) that provide personalized emotional support for users. However, the field lacks clear definitions and systematic evaluation standards for ECDs. To address this, we first propose a definition of ECDs with formal descriptions. Then, based on this theory and the design principle of \"Ability Layer-Task Layer (three level)-Data Layer-Method Layer\", we design and implement the first ECD evaluation benchmark - MoodBench 1.0. Through extensive evaluations of 30 mainstream models, we demonstrate that MoodBench 1.0 has excellent discriminant validity and can effectively quantify the differences in emotional companionship abilities among models. Furthermore, the results reveal current models' shortcomings in deep emotional companionship, guiding future technological optimization and significantly aiding developers in enhancing ECDs' user experience.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æƒ…æ„Ÿé™ªä¼´å¯¹è¯ç³»ç»Ÿ(ECDs)é¢†åŸŸç¼ºä¹æ˜ç¡®å®šä¹‰å’Œç³»ç»ŸåŒ–è¯„ä¼°æ ‡å‡†çš„é—®é¢˜ï¼Œé¦–å…ˆæå‡ºäº†ECDsçš„æ­£å¼å®šä¹‰ä¸ç†è®ºæè¿°ã€‚åŸºäºâ€œèƒ½åŠ›å±‚-ä»»åŠ¡å±‚-æ•°æ®å±‚-æ–¹æ³•å±‚â€(Ability Layer-Task Layer-Data Layer-Method Layer)çš„è®¾è®¡åŸåˆ™ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘å¹¶å®ç°äº†é¦–ä¸ªè¯„ä¼°åŸºå‡† MoodBench 1.0ã€‚é€šè¿‡å¯¹30ä¸ªä¸»æµæ¨¡å‹è¿›è¡Œå¹¿æ³›æµ‹è¯„ï¼Œå®éªŒè¯æ˜ MoodBench 1.0 å…·æœ‰å‡ºè‰²çš„åŒºåˆ†æ•ˆåº¦(discriminant validity)ï¼Œèƒ½æœ‰æ•ˆé‡åŒ–å„æ¨¡å‹åœ¨æƒ…æ„Ÿé™ªä¼´èƒ½åŠ›ä¸Šçš„å·®å¼‚ã€‚ç ”ç©¶ç»“æœè¿›ä¸€æ­¥æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨æ·±å±‚æƒ…æ„Ÿé™ªä¼´æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„æŠ€æœ¯ä¼˜åŒ–å’Œç”¨æˆ·ä½“éªŒæå‡æä¾›äº†å…³é”®æŒ‡å¯¼ä¸åŸºå‡†æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.18926v1",
      "published_date": "2025-11-24 09:32:02 UTC",
      "updated_date": "2025-11-24 09:32:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:13:27.137248+00:00"
    },
    {
      "arxiv_id": "2511.18924v1",
      "title": "LLM-Driven Kernel Evolution: Automating Driver Updates in Linux",
      "title_zh": "LLM é©±åŠ¨çš„å†…æ ¸æ¼”è¿›ï¼šLinux é©±åŠ¨ç¨‹åºè‡ªåŠ¨åŒ–æ›´æ–°",
      "authors": [
        "Arina Kharlamova",
        "Jiawen Liu",
        "Tianyi Zhang",
        "Xinrui Yang",
        "Humaid Alqasimi",
        "Youcheng Sun",
        "Chun Jason Xue"
      ],
      "abstract": "Linux kernel evolution breaks drivers through API/ABI changes, semantic shifts, and security-hardening updates. We introduce DRIVEBENCH, an executable corpus of kernel$\\rightarrow$driver co-evolution cases, and AUTODRIVER, a closed-loop, LLM-driven system for automating driver maintenance. The system integrates prompt engineering, multi-agent collaboration, static analysis, and iterative validation to ensure that generated patches are not only syntactically correct but also functionally and semantically consistent with kernel conventions. The corpus spans v5.10-v6.10 with 235 validated cases drawn from 612 candidates. In evaluation across 55 cases, AUTODRIVER achieves 56.4% compilation success; QEMU-based boot verification indicates that compiled patches preserve driver initialization in most instances. By releasing DRIVEBENCH and tooling, we enable reproducible research and a practical route to continuous, safe co-evolution of drivers with the Linux kernel.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Linux kernel æ¼”è¿›è¿‡ç¨‹ä¸­å›  API/ABI å˜æ›´ã€è¯­ä¹‰åç§»å’Œå®‰å…¨åŠ å›ºæ›´æ–°å¯¼è‡´é©±åŠ¨ç¨‹åºå¤±æ•ˆçš„é—®é¢˜ï¼Œæå‡ºäº†è‡ªåŠ¨åŒ–ç»´æŠ¤æ–¹æ¡ˆã€‚ä½œè€…é¦–å…ˆæ„å»ºäº† DRIVEBENCHï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« v5.10 è‡³ v6.10 ç‰ˆæœ¬ä¸­ 235 ä¸ªç»è¿‡éªŒè¯çš„å†…æ ¸ä¸é©±åŠ¨å…±æ¼”åŒ–æ¡ˆä¾‹çš„å¯æ‰§è¡Œè¯­æ–™åº“ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº† AUTODRIVER ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ç§é—­ç¯ä¸”ç”± LLM é©±åŠ¨çš„é©±åŠ¨è‡ªåŠ¨åŒ–æ›´æ–°æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿæ•´åˆäº† Prompt engineeringã€å¤šæ™ºèƒ½ä½“åä½œ (multi-agent collaboration)ã€é™æ€åˆ†æ (static analysis) å’Œè¿­ä»£éªŒè¯ (iterative validation) æŠ€æœ¯ï¼Œç¡®ä¿ç”Ÿæˆçš„è¡¥ä¸åœ¨è¯­æ³•å’Œè¯­ä¹‰ä¸Šç¬¦åˆå†…æ ¸è§„èŒƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAUTODRIVER åœ¨ 55 ä¸ªæ¡ˆä¾‹ä¸­è¾¾åˆ°äº† 56.4% çš„ç¼–è¯‘æˆåŠŸç‡ï¼Œå¹¶é€šè¿‡ QEMU å¼•å¯¼éªŒè¯ç¡®è®¤è¡¥ä¸åœ¨å¤šæ•°æƒ…å†µä¸‹èƒ½ç»´æŒé©±åŠ¨åˆå§‹åŒ–ã€‚é€šè¿‡å‘å¸ƒ DRIVEBENCH å’Œé…å¥—å·¥å…·ï¼Œè¯¥ç ”ç©¶ä¸º Linux é©±åŠ¨ç¨‹åºä¸å†…æ ¸çš„æŒç»­ã€å®‰å…¨å…±æ¼”åŒ–æä¾›äº†å®ç”¨çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18924v1",
      "published_date": "2025-11-24 09:31:52 UTC",
      "updated_date": "2025-11-24 09:31:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:13:46.427514+00:00"
    },
    {
      "arxiv_id": "2511.18919v1",
      "title": "Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation",
      "title_zh": "å­¦ä¼šä¿¡ä»»ï¼šé¢å‘è§†è§‰ç”Ÿæˆçš„è´å¶æ–¯å…ˆéªŒå¼•å¯¼ä¼˜åŒ–",
      "authors": [
        "Ruiying Liu",
        "Yuanzhi Liang",
        "Haibin Huang",
        "Tianshu Yu",
        "Chi Zhang"
      ],
      "abstract": "Group Relative Policy Optimization (GRPO) has emerged as an effective and lightweight framework for post-training visual generative models. However, its performance is fundamentally limited by the ambiguity of textual visual correspondence: a single prompt may validly describe diverse visual outputs, and a single image or video may support multiple equally correct interpretations. This many to many relationship leads reward models to generate uncertain and weakly discriminative signals, causing GRPO to underutilize reliable feedback and overfit noisy ones. We introduce Bayesian Prior-Guided Optimization (BPGO), a novel extension of GRPO that explicitly models reward uncertainty through a semantic prior anchor. BPGO adaptively modulates optimization trust at two levels: inter-group Bayesian trust allocation emphasizes updates from groups consistent with the prior while down-weighting ambiguous ones, and intra-group prior-anchored renormalization sharpens sample distinctions by expanding confident deviations and compressing uncertain scores. Across both image and video generation tasks, BPGO delivers consistently stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰ç”Ÿæˆæ¨¡å‹åè®­ç»ƒä¸­çš„ Group Relative Policy Optimization (GRPO) æ¡†æ¶åœ¨å¤„ç†æ–‡æœ¬ä¸è§†è§‰å¯¹åº”æ­§ä¹‰æ€§æ—¶çš„å±€é™æ€§è¿›è¡Œäº†æ”¹è¿›ã€‚ç”±äºå•ä¸€æç¤ºè¯å¯å¯¹åº”å¤šç§è§†è§‰è¾“å‡ºï¼Œå¥–åŠ±æ¨¡å‹å¸¸äº§ç”Ÿä¸ç¡®å®šä¸”åŒºåˆ†åº¦å¼±çš„ä¿¡å·ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥åˆ©ç”¨æœ‰æ•ˆåé¦ˆå¹¶è¿‡åº¦æ‹Ÿåˆå™ªå£°ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Bayesian Prior-Guided Optimization (BPGO)ï¼Œé€šè¿‡è¯­ä¹‰å…ˆéªŒé”šç‚¹ (semantic prior anchor) æ˜¾å¼å»ºæ¨¡å¥–åŠ±çš„ä¸ç¡®å®šæ€§ã€‚BPGO åœ¨ä¸¤ä¸ªå±‚é¢è‡ªé€‚åº”è°ƒèŠ‚ä¼˜åŒ–ä¿¡ä»»åº¦ï¼šç»„é—´è´å¶æ–¯ä¿¡ä»»åˆ†é… (inter-group Bayesian trust allocation) å¼ºåŒ–ä¸å…ˆéªŒä¸€è‡´çš„æ›´æ–°ï¼Œè€Œç»„å†…å…ˆéªŒé”šå®šé‡å½’ä¸€åŒ– (intra-group prior-anchored renormalization) åˆ™é€šè¿‡æ‰©å¤§ç½®ä¿¡åå·®æ¥æå‡æ ·æœ¬åŒºåˆ†åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒBPGO ç›¸æ¯”æ ‡å‡† GRPO åŠå…¶å˜ä½“æ˜¾è‘—å¢å¼ºäº†è¯­ä¹‰å¯¹é½å’Œæ„ŸçŸ¥ä¿çœŸåº¦ï¼Œå¹¶å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18919v1",
      "published_date": "2025-11-24 09:29:30 UTC",
      "updated_date": "2025-11-24 09:29:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:13:49.234382+00:00"
    },
    {
      "arxiv_id": "2511.18903v1",
      "title": "How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining",
      "title_zh": "å­¦ä¹ ç‡è¡°å‡å¦‚ä½•åœ¨å¤§è¯­è¨€æ¨¡å‹è¯¾ç¨‹é¢„è®­ç»ƒä¸­æµªè´¹ä¼˜è´¨æ•°æ®",
      "authors": [
        "Kairong Luo",
        "Zhenbo Sun",
        "Haodong Wen",
        "Xinyu Shi",
        "Jiarui Cui",
        "Chenyi Dang",
        "Kaifeng Lyu",
        "Wenguang Chen"
      ],
      "abstract": "Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢„è®­ç»ƒä¸­è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum-based Pretrainingï¼‰æå‡æœ‰é™çš„åŸå› ï¼ŒæŒ‡å‡ºæ ¸å¿ƒçŸ›ç›¾åœ¨äºä¸Šå‡çš„æ•°æ®è´¨é‡é¡ºåºä¸ä¸‹é™çš„å­¦ä¹ ç‡ï¼ˆLearning Rate, LRï¼‰è°ƒåº¦ä¹‹é—´å­˜åœ¨ä¸å…¼å®¹æ€§ã€‚å®éªŒå‘ç°ï¼Œè™½ç„¶åœ¨æ’å®šå­¦ä¹ ç‡ä¸‹æŒ‰è´¨é‡ä»ä½åˆ°é«˜è®­ç»ƒçš„æ•ˆæœæ˜¾è‘—ä¼˜äºéšæœºæ‰“ä¹±ï¼ˆRandom Shufflingï¼‰ï¼Œä½†åœ¨æ ‡å‡†å­¦ä¹ ç‡è¡°å‡ï¼ˆLR Decayï¼‰ç­–ç•¥ä¸‹ï¼Œæ¨¡å‹åœ¨æ¥è§¦æœ€ä¼˜è´¨æ•°æ®æ—¶å› å­¦ä¹ ç‡è¿‡ä½è€Œæ— æ³•æœ‰æ•ˆå¸æ”¶çŸ¥è¯†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†é‡‡ç”¨æ›´æ¸©å’Œçš„å­¦ä¹ ç‡è¡°å‡æ–¹æ¡ˆä»¥åŠåˆ©ç”¨æ¨¡å‹å¹³å‡ï¼ˆModel Averagingï¼‰æ›¿ä»£ä¼ ç»Ÿè¡°å‡çš„ç­–ç•¥ã€‚åœ¨1.5Bå‚æ•°æ¨¡å‹å’Œ30B tokensçš„éªŒè¯ä¸­ï¼Œç»“åˆè¿™äº›ç­–ç•¥çš„æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ¯”éšæœºæ‰“ä¹±æé«˜äº†1.64%çš„å¹³å‡åˆ†ã€‚è¯¥å‘ç°å¼ºè°ƒäº†ååŒè®¾è®¡æ•°æ®è¯¾ç¨‹ä¸ä¼˜åŒ–æ–¹æ³•çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºé‡æ–°è¯„ä¼°è¯¾ç¨‹å­¦ä¹ åœ¨LLMé¢„è®­ç»ƒä¸­çš„æ½œåŠ›æä¾›äº†æ–°è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18903v1",
      "published_date": "2025-11-24 09:03:49 UTC",
      "updated_date": "2025-11-24 09:03:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:14:12.332774+00:00"
    },
    {
      "arxiv_id": "2511.18902v1",
      "title": "VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation for Multimodal RL",
      "title_zh": "VADEï¼šåŸºäºåœ¨çº¿æ ·æœ¬çº§éš¾åº¦ä¼°è®¡çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ–¹å·®æ„ŸçŸ¥åŠ¨æ€é‡‡æ ·",
      "authors": [
        "Zengjie Hu",
        "Jiantao Qiu",
        "Tianyi Bai",
        "Haojin Yang",
        "Binhang Yuan",
        "Qi Jing",
        "Conghui He",
        "Wentao Zhang"
      ],
      "abstract": "Group-based policy optimization methods like GRPO and GSPO have become standard for training multimodal models, leveraging group-wise rollouts and relative advantage estimation. However, they suffer from a critical \\emph{gradient vanishing} problem when all responses within a group receive identical rewards, causing advantage estimates to collapse and training signals to diminish. Existing attempts to mitigate this issue fall into two paradigms: filtering-based and sampling-based methods. Filtering-based methods first generate rollouts broadly and then retroactively filter out uninformative groups, leading to substantial computational overhead. Sampling-based methods proactively select effective samples before rollout but rely on static criteria or prior dataset knowledge, lacking real-time adaptability. To address these issues, we propose \\textbf{VADE}, a \\textbf{V}ariance-\\textbf{A}ware \\textbf{D}ynamic sampling framework via online sample-level difficulty \\textbf{E}stimation. Our framework integrates three key components: online sample-level difficulty estimation using Beta distributions, a Thompson sampler that maximizes information gain through the estimated correctness probability, and a two-scale prior decay mechanism that maintains robust estimation under policy evolution. This three components design enables VADE to dynamically select the most informative samples, thereby amplifying training signals while eliminating extra rollout costs. Extensive experiments on multimodal reasoning benchmarks show that VADE consistently outperforms strong baselines in both performance and sample efficiency, while achieving a dramatic reduction in computational overhead. More importantly, our framework can serves as a plug-and-play component to be seamlessly integrated into existing group-based RL algorithms. Code and models are available at https://VADE-RL.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ GRPO å’Œ GSPO ç­‰å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸­å­˜åœ¨çš„æ¢¯åº¦æ¶ˆå¤± (gradient vanishing) é—®é¢˜ï¼Œæå‡ºäº† VADE æ¡†æ¶ï¼Œå³ä¸€ç§åŸºäºåœ¨çº¿æ ·æœ¬çº§éš¾åº¦ä¼°è®¡çš„æ–¹å·®æ„ŸçŸ¥åŠ¨æ€é‡‡æ ·æŠ€æœ¯ã€‚VADE é€šè¿‡ Beta åˆ†å¸ƒè¿›è¡Œåœ¨çº¿éš¾åº¦è¯„ä¼°ï¼Œå¹¶ç»“åˆ Thompson é‡‡æ ·å™¨æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šï¼ŒåŒæ—¶åˆ©ç”¨åŒå°ºåº¦å…ˆéªŒè¡°å‡æœºåˆ¶åœ¨ç­–ç•¥æ¼”è¿›è¿‡ç¨‹ä¸­ä¿æŒä¼°è®¡çš„é²æ£’æ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå®æ—¶ã€åŠ¨æ€åœ°é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ ·æœ¬ï¼Œæœ‰æ•ˆè§£å†³äº†ç»„å†…å“åº”å¥–åŠ±ä¸€è‡´å¯¼è‡´ä¼˜åŠ¿ä¼°è®¡å´©æºƒçš„é—®é¢˜ï¼ŒåŒæ—¶é¿å…äº†é¢å¤–çš„é‡‡æ ·å¼€é”€ã€‚å¤šæ¨¡æ€æ¨ç†åŸºå‡†å®éªŒè¡¨æ˜ï¼ŒVADE åœ¨æ€§èƒ½å’Œæ ·æœ¬æ•ˆç‡ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼ŒVADE ä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„ç»„ä»¶ï¼Œå¯æ— ç¼é›†æˆè‡³ç°æœ‰çš„ç»„å†…å¼ºåŒ–å­¦ä¹  (group-based RL) ç®—æ³•ä¸­ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18902v1",
      "published_date": "2025-11-24 08:59:54 UTC",
      "updated_date": "2025-11-24 08:59:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:14:31.143409+00:00"
    },
    {
      "arxiv_id": "2511.18894v2",
      "title": "MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting",
      "title_zh": "MetaDCSegï¼šåŸºäºå…ƒåŠ¨æ€ä¸­å¿ƒåŠ æƒçš„é²æ£’åŒ»å­¦å›¾åƒåˆ†å‰²",
      "authors": [
        "Chenyu Mu",
        "Guihai Chen",
        "Xun Yang",
        "Erkun Yang",
        "Cheng Deng"
      ],
      "abstract": "Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MetaDCSegï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ç”±äºå™ªå£°æ ‡æ³¨å’Œæ¨¡ç³Šè§£å‰–è¾¹ç•Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šæ€§é—®é¢˜çš„é²æ£’æ¡†æ¶ã€‚MetaDCSeg é€šè¿‡åŠ¨æ€å­¦ä¹ åƒç´ çº§æƒé‡æ¥æŠ‘åˆ¶é”™è¯¯æ ‡ç­¾çš„å½±å“ï¼Œå¹¶ç¡®ä¿å¯é æ ‡æ³¨å¾—åˆ°ä¿ç•™ã€‚å…¶æ ¸å¿ƒåœ¨äºå¼•å…¥äº†åŠ¨æ€ä¸­å¿ƒè·ç¦»(Dynamic Center Distance, DCD)æœºåˆ¶ï¼Œé€šè¿‡å‰æ™¯ã€èƒŒæ™¯å’Œè¾¹ç•Œä¸­å¿ƒçš„åŠ æƒç‰¹å¾è·ç¦»æ¥æ˜¾å¼å»ºæ¨¡è¾¹ç•Œä¸ç¡®å®šæ€§ã€‚è¿™ä¸€ç­–ç•¥å¼•å¯¼æ¨¡å‹é‡ç‚¹å…³æ³¨æ¨¡ç³Šè¾¹ç•Œé™„è¿‘çš„éš¾åˆ†å‰²åƒç´ ï¼Œå®ç°äº†å¯¹å¤æ‚ç»“æ„è¾¹ç•Œçš„ç²¾ç¡®å¤„ç†ã€‚åœ¨æ¶µç›–ä¸åŒå™ªå£°æ°´å¹³çš„å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒMetaDCSeg æ˜¾è‘—æå‡äº†åˆ†å‰²æ€§èƒ½ï¼Œä¸”å…¶è¡¨ç°ä¸€è‡´ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›(state-of-the-art)æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18894v2",
      "published_date": "2025-11-24 08:51:02 UTC",
      "updated_date": "2026-01-22 15:35:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:14:47.133375+00:00"
    },
    {
      "arxiv_id": "2511.18890v1",
      "title": "Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models",
      "title_zh": "Nemotron-Flashï¼šè¿ˆå‘æ—¶å»¶æœ€ä¼˜çš„æ··åˆå¼å°è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yonggan Fu",
        "Xin Dong",
        "Shizhe Diao",
        "Matthijs Van keirsbilck",
        "Hanrong Ye",
        "Wonmin Byeon",
        "Yashaswi Karnati",
        "Lucas Liebenwein",
        "Hannah Zhang",
        "Nikolaus Binder",
        "Maksim Khadkevich",
        "Alexander Keller",
        "Jan Kautz",
        "Yingyan Celine Lin",
        "Pavlo Molchanov"
      ],
      "abstract": "Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Nemotron-Flashï¼Œè¿™æ˜¯ä¸€ç³»åˆ—æ—¨åœ¨ä¼˜åŒ–å®é™…è®¾å¤‡æ¨ç†å»¶è¿Ÿçš„æ··åˆå°è¯­è¨€æ¨¡å‹(Small Language Models)ã€‚ç ”ç©¶è€…æŒ‡å‡ºä¼ ç»Ÿçš„å‚æ•°é‡ä¼˜åŒ–å¹¶ä¸ç›´æ¥ç­‰åŒäºå®é™…é€Ÿåº¦æå‡ï¼Œå¹¶ç¡®å®šäº†æ·±åº¦-å®½åº¦æ¯”(depth-width ratios)å’Œç®—å­é€‰æ‹©(operator choices)æ˜¯å½±å“è®¾å¤‡å»¶è¿Ÿçš„ä¸¤å¤§æ ¸å¿ƒæ¶æ„å› ç´ ã€‚ç ”ç©¶å‘ç°è™½ç„¶æ·±çª„æ¨¡å‹åœ¨ç›¸åŒå‚æ•°é¢„ç®—ä¸‹å‡†ç¡®ç‡è¾ƒé«˜ï¼Œä½†åœ¨å‡†ç¡®ç‡ä¸å»¶è¿Ÿçš„æƒè¡¡ä¸­å¾€å¾€å¹¶éæœ€ä¼˜è§£ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶é‡‡ç”¨è¿›åŒ–æœç´¢æ¡†æ¶(evolutionary search framework)è‡ªåŠ¨å‘ç°æ··åˆæ¨¡å‹ä¸­ç®—å­çš„æœ€ä½³ç»„åˆï¼Œå¹¶ç»“åˆæƒé‡å½’ä¸€åŒ–(weight normalization)æŠ€æœ¯æå‡è®­ç»ƒçš„æœ‰æ•ˆæ€§ä¸æ”¶æ•›é€Ÿåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNemotron-Flashåœ¨å‡†ç¡®ç‡ã€å»¶è¿Ÿå’Œååé‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºQwen3-1.7Bå’Œ0.6Bç­‰ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ã€‚è¯¥å·¥ä½œä¸ºä½å»¶è¿Ÿåº”ç”¨åœºæ™¯ä¸‹çš„å°è¯­è¨€æ¨¡å‹è®¾è®¡ä¸è®­ç»ƒæä¾›äº†é‡è¦çš„åŸåˆ™å’Œæ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.18890v1",
      "published_date": "2025-11-24 08:46:36 UTC",
      "updated_date": "2025-11-24 08:46:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:14:31.730278+00:00"
    },
    {
      "arxiv_id": "2511.18889v1",
      "title": "CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation",
      "title_zh": "CoreEvalï¼šåˆ©ç”¨çœŸå®ä¸–ç•ŒçŸ¥è¯†è‡ªåŠ¨æ„å»ºæŠ—æ±¡æŸ“æ•°æ®é›†ï¼Œæ—¨åœ¨å®ç°å¯é çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Jingqian Zhao",
        "Bingbing Wang",
        "Geng Tu",
        "Yice Zhang",
        "Qianlong Wang",
        "Bin Liang",
        "Jing Li",
        "Ruifeng Xu"
      ],
      "abstract": "Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \\textbf{CoreEval}, a \\textbf{Co}ntamination-\\textbf{re}silient \\textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CoreEvalï¼Œä¸€ç§æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)è¯„ä¼°ä¸­æ•°æ®æ±¡æŸ“(Data contamination)é—®é¢˜çš„è‡ªåŠ¨åŒ–æ•°æ®é›†æ›´æ–°ç­–ç•¥ã€‚ç”±äºä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å½»åº•æ¶ˆé™¤æ¨¡å‹ä¸­å·²æœ‰çš„é¢„å­˜çŸ¥è¯†æˆ–ä¿æŒè¯­ä¹‰å¤æ‚æ€§ï¼ŒCoreEval é€šè¿‡å¼•å…¥æœ€æ–°çš„çœŸå®ä¸–ç•ŒçŸ¥è¯†æ¥æ„å»ºå…·æœ‰æŠ—æ±¡æŸ“èƒ½åŠ›çš„è¯„ä¼°åŸºå‡†ã€‚è¯¥æ¡†æ¶é¦–å…ˆä»åŸå§‹æ•°æ®ä¸­æå–å®ä½“å…³ç³»ï¼Œå¹¶åˆ©ç”¨ GDELT æ•°æ®åº“æ£€ç´¢ç›¸å…³çš„å®æ—¶çŸ¥è¯†ã€‚éšåï¼Œç³»ç»Ÿå°†è¿™äº›æ–°çŸ¥è¯†è¿›è¡Œè¯­å¢ƒé‡æ„å¹¶ä¸åŸå§‹æ•°æ®æ•´åˆï¼Œé€šè¿‡ç²¾ç»†åŒ–é‡ç»„ç¡®ä¿è¯­ä¹‰è¿è´¯æ€§ã€‚ä¸ºäº†ä¿è¯æ›´æ–°æ•°æ®é›†ä¸åŸæ•°æ®é›†çš„æ ‡ç­¾ä¸€è‡´æ€§ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ç¨³å¥çš„æ•°æ®åå°„æœºåˆ¶(Data reflection mechanism)è¿›è¡Œè¿­ä»£éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoreEval èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£å› æ•°æ®æ±¡æŸ“å¯¼è‡´çš„æ€§èƒ½é«˜ä¼°ï¼Œä¸ºå®ç°æ›´å¯é çš„ LLM è¯„ä¼°æä¾›äº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL'25",
      "pdf_url": "https://arxiv.org/pdf/2511.18889v1",
      "published_date": "2025-11-24 08:44:29 UTC",
      "updated_date": "2025-11-24 08:44:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:14:33.443837+00:00"
    },
    {
      "arxiv_id": "2511.18878v1",
      "title": "Accelerating Reinforcement Learning via Error-Related Human Brain Signals",
      "title_zh": "åˆ©ç”¨é”™è¯¯ç›¸å…³äººè„‘ä¿¡å·åŠ é€Ÿå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Suzie Kim",
        "Hye-Bin Shin",
        "Hyo-Jeong Jang"
      ],
      "abstract": "In this work, we investigate how implicit neural feed back can accelerate reinforcement learning in complex robotic manipulation settings. While prior electroencephalogram (EEG) guided reinforcement learning studies have primarily focused on navigation or low-dimensional locomotion tasks, we aim to understand whether such neural evaluative signals can improve policy learning in high-dimensional manipulation tasks involving obstacles and precise end-effector control. We integrate error related potentials decoded from offline-trained EEG classifiers into reward shaping and systematically evaluate the impact of human-feedback weighting. Experiments on a 7-DoF manipulator in an obstacle-rich reaching environment show that neural feedback accelerates reinforcement learning and, depending on the human-feedback weighting, can yield task success rates that at times exceed those of sparse-reward baselines. Moreover, when applying the best-performing feedback weighting across all sub jects, we observe consistent acceleration of reinforcement learning relative to the sparse-reward setting. Furthermore, leave-one subject-out evaluations confirm that the proposed framework remains robust despite the intrinsic inter-individual variability in EEG decodability. Our findings demonstrate that EEG-based reinforcement learning can scale beyond locomotion tasks and provide a viable pathway for human-aligned manipulation skill acquisition.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨éšå¼ç¥ç»åé¦ˆåŠ é€Ÿå¤æ‚æœºå™¨äººæ“çºµç¯å¢ƒä¸‹çš„ Reinforcement Learning è¿‡ç¨‹ã€‚ä½œè€…å°†ä»ç¦»çº¿è®­ç»ƒçš„ EEG åˆ†ç±»å™¨ä¸­è§£ç å‡ºçš„ Error Related Potentials (ErrPs) æ•´åˆåˆ° Reward Shaping æœºåˆ¶ä¸­ï¼Œå¹¶ç³»ç»Ÿåœ°è¯„ä¼°äº†äººç±»åé¦ˆæƒé‡å¯¹é«˜ç»´æ“çºµä»»åŠ¡ç­–ç•¥å­¦ä¹ çš„å½±å“ã€‚åœ¨éšœç¢ç‰©ä¸°å¯Œçš„ 7-DoF æœºæ¢°è‡‚åˆ°è¾¾ä»»åŠ¡ä¸­ï¼Œå®éªŒè¯æ˜ç¥ç»åé¦ˆæ˜¾è‘—æé«˜äº†å­¦ä¹ æ•ˆç‡ï¼Œä¸”åœ¨ç‰¹å®šåé¦ˆæƒé‡ä¸‹ï¼Œå…¶æˆåŠŸç‡ä¼˜äºä¼ ç»Ÿçš„ Sparse-Reward åŸºçº¿æ¨¡å‹ã€‚é€šè¿‡ Leave-one-subject-out è¯„ä¼°ï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†åœ¨å¤„ç†ä¸ªä½“é—´ EEG ä¿¡å·å·®å¼‚æ—¶çš„é²æ£’æ€§ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜åŸºäº EEG çš„ Reinforcement Learning èƒ½å¤Ÿä»ç®€å•çš„è¿åŠ¨ä»»åŠ¡æ‰©å±•åˆ°å¤æ‚çš„æ“çºµä»»åŠ¡ï¼Œä¸ºå®ç°äººç±»å¯¹é½çš„æœºå™¨äººæŠ€èƒ½è·å–æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18878v1",
      "published_date": "2025-11-24 08:33:47 UTC",
      "updated_date": "2025-11-24 08:33:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:15:27.535458+00:00"
    },
    {
      "arxiv_id": "2511.18874v1",
      "title": "GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction",
      "title_zh": "GContextFormerï¼šä¸€ç§ç»“åˆå…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ··åˆå¤šå¤´æ³¨æ„åŠ›ä¸ç¼©æ”¾åŠ æ€§èšåˆçš„å¤šæ¨¡æ€è½¨è¿¹é¢„æµ‹æ–¹æ³•",
      "authors": [
        "Yuzhi Chen",
        "Yuanchang Xie",
        "Lei Zhao",
        "Pan Liu",
        "Yajie Zou",
        "Chen Wang"
      ],
      "abstract": "Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GContextFormerï¼Œä¸€ç§å…·æœ‰å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„æ··åˆæ³¨æ„åŠ›ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€è½¨è¿¹é¢„æµ‹ä¸­å¯¹é«˜ç²¾åœ°å›¾(HD map)çš„è¿‡åº¦ä¾èµ–ä»¥åŠç°æœ‰æ— åœ°å›¾(Map-free)æ–¹æ³•å› ç¼ºä¹å…¨å±€ä¸Šä¸‹æ–‡è€Œå¯¼è‡´çš„è¿åŠ¨æ„å›¾å¤±è°ƒé—®é¢˜ã€‚è¯¥æ¨¡å‹æ ¸å¿ƒåŒ…å« Motion-Aware Encoderï¼Œé€šè¿‡å¯¹è½¨è¿¹ä»¤ç‰Œè¿›è¡Œæœ‰ç•Œç¼©æ”¾åŠ æ³•èšåˆ(scaled additive aggregation)æ„å»ºåœºæ™¯çº§æ„å›¾å…ˆéªŒï¼Œå¹¶åœ¨å…±äº«å…¨å±€ä¸Šä¸‹æ–‡ä¸­ç»†åŒ–å„æ¨¡å¼è¡¨å¾ï¼Œæœ‰æ•ˆå‡è½»äº†æ¨¡å¼é—´çš„æŠ‘åˆ¶ã€‚Hierarchical Interaction Decoder å°†ç¤¾äº¤æ¨ç†åˆ†è§£ä¸ºåŒè·¯å¾„äº¤å‰æ³¨æ„åŠ›ï¼Œåˆ©ç”¨é—¨æ§æ¨¡å—åè°ƒå‡ ä½•è¦†ç›–ä¸æ˜¾è‘—äº¤äº’å…³æ³¨ä¹‹é—´çš„å¹³è¡¡ã€‚åœ¨ TOD-VT æ•°æ®é›†çš„å¤šç§é«˜é€Ÿå…¬è·¯åœºæ™¯å®éªŒè¯æ˜ï¼ŒGContextFormer åœ¨é«˜æ›²ç‡å’Œè¿‡æ¸¡åœ°å¸¦çš„é¢„æµ‹é²æ£’æ€§æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå…¶é€šè¿‡è¿åŠ¨æ¨¡å¼åŒºåˆ†å’Œé‚»åŸŸä¸Šä¸‹æ–‡è°ƒåˆ¶å®ç°äº†æ¨ç†è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ï¼Œä¸”å…¶æ¨¡å—åŒ–è®¾è®¡æ”¯æŒå‘è·¨é¢†åŸŸå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡çš„æ‰©å±•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA",
        "cs.RO",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18874v1",
      "published_date": "2025-11-24 08:28:42 UTC",
      "updated_date": "2025-11-24 08:28:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:14:47.833457+00:00"
    },
    {
      "arxiv_id": "2511.18871v4",
      "title": "Periodic Asynchrony: An On-Policy Approach for Accelerating LLM Reinforcement Learning",
      "title_zh": "å‘¨æœŸæ€§å¼‚æ­¥ï¼šä¸€ç§ç”¨äºåŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„åŒç­–ç•¥æ–¹æ³•",
      "authors": [
        "Jian Lu",
        "Yi Luo"
      ],
      "abstract": "Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, our approach consistently delivers significant end-to-end training efficiency improvements on NPU platforms, indicating its potential for widespread application.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ (LLM Reinforcement Learning)ä¸­è®­ç»ƒæ•ˆç‡ä½ä¸‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPeriodic Asynchronyçš„å‘¨æœŸæ€§å¼‚æ­¥æ¡†æ¶ã€‚ç”±äºä¸»æµRLæ¡†æ¶åœ¨æ¨ç†ä¸è®­ç»ƒåŒæ­¥æ‰§è¡Œæ—¶å­˜åœ¨è®¡ç®—è€¦åˆé—®é¢˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ†ç¦»æ¨ç†ä¸è®­ç»ƒçš„éƒ¨ç½²å¹¶æ”¹è¿›æ•°æ®åŠ è½½å™¨(data loader)ï¼Œå°†ä¼ ç»ŸåŒæ­¥æ¶æ„è½¬åŒ–ä¸ºå‘¨æœŸæ€§å¼‚æ­¥æ¶æ„ã€‚è¯¥æ¡†æ¶æ”¯æŒå„ç»„ä»¶æ ¹æ®éœ€æ±‚è¿›è¡Œç‹¬ç«‹ä¸”å¼¹æ€§çš„æ‰©å±•ï¼ŒåŒæ—¶åœ¨ç®—æ³•ç²¾åº¦ä¸Šä¸åŒæ­¥æ–¹æ³•å®Œå…¨ç­‰æ•ˆï¼Œå‡éµå¾ªOn-Policyç­–ç•¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åœ¨è®­ç»ƒé˜¶æ®µé‡‡ç”¨äº†ç»Ÿä¸€çš„ä¸‰æ¨¡å‹æ¶æ„(tri-model architecture)ï¼Œå¹¶æå‡ºå…±äº«æç¤ºæ³¨æ„åŠ›æ©ç (shared-prompt attention mask)ä»¥å‡å°‘é‡å¤è®¡ç®—ã€‚å®è·µè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨NPUå¹³å°ä¸Šæ˜¾è‘—æå‡äº†ç«¯åˆ°ç«¯è®­ç»ƒæ•ˆç‡ï¼Œå±•ç°å‡ºå¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18871v4",
      "published_date": "2025-11-24 08:22:50 UTC",
      "updated_date": "2026-01-16 14:34:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:15:04.437324+00:00"
    },
    {
      "arxiv_id": "2511.18869v2",
      "title": "Hear: Hierarchically Enhanced Aesthetic Representations For Multidimensional Music Evaluation",
      "title_zh": "Hearï¼šé¢å‘å¤šç»´éŸ³ä¹è¯„ä»·çš„å±‚çº§å¢å¼ºç¾å­¦è¡¨å¾",
      "authors": [
        "Shuyang Liu",
        "Yuan Jin",
        "Rui Lin",
        "Shizhe Chen",
        "Junyu Dai",
        "Tao Jiang"
      ],
      "abstract": "Evaluating song aesthetics is challenging due to the multidimensional nature of musical perception and the scarcity of labeled data. We propose HEAR, a robust music aesthetic evaluation framework that combines: (1) a multi-source multi-scale representations module to obtain complementary segment- and track-level features, (2) a hierarchical augmentation strategy to mitigate overfitting, and (3) a hybrid training objective that integrates regression and ranking losses for accurate scoring and reliable top-tier song identification. Experiments demonstrate that HEAR consistently outperforms the baseline across all metrics on both tracks of the ICASSP 2026 SongEval benchmark. The code and trained model weights are available at https://github.com/Eps-Acoustic-Revolution-Lab/EAR_HEAR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³ä¹æ„ŸçŸ¥å¤šç»´æ€§å’Œæ ‡æ³¨æ•°æ®ç¨€ç¼ºå¯¼è‡´çš„éŸ³ä¹å®¡ç¾è¯„ä¼°éš¾é¢˜ï¼Œæå‡ºäº†åä¸ºHEARçš„é²æ£’æ€§éŸ³ä¹å®¡ç¾è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨å¤šæºå¤šå°ºåº¦è¡¨ç¤ºæ¨¡å—(multi-source multi-scale representations module)æå–äº’è¡¥çš„ç‰‡æ®µçº§ä¸éŸ³è½¨çº§ç‰¹å¾ï¼Œä»¥åº”å¯¹æ„ŸçŸ¥çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºå¸¦æ¥çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œç ”ç©¶è®¾è®¡å¹¶åº”ç”¨äº†ä¸€ç§å±‚æ¬¡åŒ–å¢å¼ºç­–ç•¥(hierarchical augmentation strategy)ã€‚æ­¤å¤–ï¼ŒHEARé‡‡ç”¨ç»“åˆå›å½’å’Œæ’åºæŸå¤±çš„æ··åˆè®­ç»ƒç›®æ ‡(hybrid training objective)ï¼Œåœ¨ç¡®ä¿å‡†ç¡®è¯„åˆ†çš„åŒæ—¶ï¼Œæå‡äº†é¡¶çº§æ­Œæ›²è¯†åˆ«çš„å¯é æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHEARåœ¨ICASSP 2026 SongEvalåŸºå‡†æµ‹è¯•çš„æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¸€è‡´ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶çš„ç›¸å…³ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹æƒé‡å·²å…¬å¼€å‘å¸ƒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18869v2",
      "published_date": "2025-11-24 08:12:33 UTC",
      "updated_date": "2025-12-30 12:46:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:15:48.134903+00:00"
    },
    {
      "arxiv_id": "2511.18868v1",
      "title": "KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit",
      "title_zh": "KernelBandï¼šåˆ©ç”¨åˆ†å±‚åŠç¡¬ä»¶æ„ŸçŸ¥å¤šè‡‚è€è™æœºæå‡åŸºäº LLM çš„ç®—å­ä¼˜åŒ–",
      "authors": [
        "Dezhi Ran",
        "Shuxiao Xie",
        "Mingfang Ji",
        "Ziyue Hua",
        "Mengzhou Wu",
        "Yuan Cao",
        "Yuzhe Guo",
        "Yu Hao",
        "Linyi Li",
        "Yitao Hu",
        "Tao Xie"
      ],
      "abstract": "High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KernelBandæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ†å±‚ä¸”æ„ŸçŸ¥ç¡¬ä»¶çš„å¤šè‡‚è€è™æœº(Multi-armed Bandit)æœºåˆ¶ï¼Œæå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å†…æ ¸ä¼˜åŒ–(Kernel Optimization)æ•ˆç‡ã€‚è¯¥æ¡†æ¶é’ˆå¯¹ç°æœ‰LLMä»£ç ç”Ÿæˆæ–¹æ³•å› ç¼ºä¹ç¡¬ä»¶é¢†åŸŸçŸ¥è¯†è€Œéš¾ä»¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨(Exploration and Exploitation)çš„é—®é¢˜ï¼Œå°†å†…æ ¸é€‰æ‹©ä¸ä¼˜åŒ–ç­–ç•¥åº”ç”¨å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ã€‚KernelBandåˆ©ç”¨ç¡¬ä»¶åˆ†æ(Hardware Profiling)ä¿¡æ¯è¯†åˆ«æ½œåœ¨çš„ä¼˜åŒ–è·¯å¾„ï¼Œå¹¶é€šè¿‡è¿è¡Œæ—¶è¡Œä¸ºèšç±»(Runtime Behavior Clustering)æŠ€æœ¯æ˜¾è‘—é™ä½äº†å€™é€‰å†…æ ¸çš„æ¢ç´¢å¼€é”€ã€‚åœ¨TritonBenchä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKernelBandåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›(State-of-the-art)æ–¹æ³•ï¼Œä¸”åœ¨æ¶ˆè€—æ›´å°‘Tokençš„æƒ…å†µä¸‹å®ç°äº†æ›´ä¼˜çš„ä¼˜åŒ–æ•ˆæœã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å±•ç°äº†éšè®¡ç®—èµ„æºå¢åŠ è€ŒæŒç»­æå‡æ€§èƒ½ä¸”ä¸é¥±å’Œçš„ç‰¹æ€§ï¼Œä¸ºé«˜æ•ˆã€è‡ªåŠ¨åŒ–çš„ç®—å­ä¼˜åŒ–æä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2511.18868v1",
      "published_date": "2025-11-24 08:11:50 UTC",
      "updated_date": "2025-11-24 08:11:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:16:01.396854+00:00"
    },
    {
      "arxiv_id": "2511.18860v1",
      "title": "Generating Reading Comprehension Exercises with Large Language Models for Educational Applications",
      "title_zh": "é¢å‘æ•™è‚²åº”ç”¨çš„å¤§è¯­è¨€æ¨¡å‹é˜…è¯»ç†è§£ç»ƒä¹ ç”Ÿæˆ",
      "authors": [
        "Xingyu Huang",
        "Fei Jiang",
        "Jianli Xiao"
      ],
      "abstract": "With the rapid development of large language models (LLMs), the applications of LLMs have grown substantially. In the education domain, LLMs demonstrate significant potential, particularly in automatic text generation, which enables the creation of intelligent and adaptive learning content. This paper proposes a new LLMs framework, which is named as Reading Comprehension Exercise Generation (RCEG). It can generate high-quality and personalized English reading comprehension exercises automatically. Firstly, RCEG uses fine-tuned LLMs to generate content candidates. Then, it uses a discriminator to select the best candidate. Finally, the quality of the generated content has been improved greatly. To evaluate the performance of RCEG, a dedicated dataset for English reading comprehension is constructed to perform the experiments, and comprehensive evaluation metrics are used to analyze the experimental results. These metrics include content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment. Experimental results show that RCEG significantly improves the relevance and cognitive appropriateness of the generated exercises.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸º Reading Comprehension Exercise Generation (RCEG) çš„æ–°å‹å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºæ•™è‚²åº”ç”¨è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ä¸”ä¸ªæ€§åŒ–çš„è‹±è¯­é˜…è¯»ç†è§£ç»ƒä¹ ã€‚RCEG æ¡†æ¶é¦–å…ˆåˆ©ç”¨ç»è¿‡å¾®è°ƒçš„ LLMs ç”Ÿæˆå¤šä¸ªå€™é€‰å†…å®¹ï¼Œéšåé€šè¿‡ä¸“é—¨çš„åˆ¤åˆ«å™¨ (discriminator) ç­›é€‰å‡ºæœ€ä¼˜ç»“æœï¼Œä»¥æ˜¾è‘—æå‡ç”Ÿæˆå†…å®¹çš„æœ€ç»ˆè´¨é‡ã€‚ç ”ç©¶å›¢é˜Ÿä¸ºæ­¤æ„å»ºäº†ä¸“é—¨çš„è‹±è¯­é˜…è¯»ç†è§£æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨å†…å®¹å¤šæ ·æ€§ã€äº‹å®å‡†ç¡®æ€§ã€è¯­è¨€æ¯’æ€§åŠæ•™è‚²ä¸€è‡´æ€§ (pedagogical alignment) ç­‰å¤šç»´æŒ‡æ ‡è¿›è¡Œä¸¥è°¨è¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒRCEG èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºæ‰€ç”Ÿæˆç»ƒä¹ çš„ç›¸å…³æ€§ä¸è®¤çŸ¥é€‚å®œæ€§ï¼Œå……åˆ†å±•ç¤ºäº† LLMs åœ¨å¼€å‘æ™ºèƒ½åŒ–ä¸è‡ªé€‚åº”å­¦ä¹ å†…å®¹æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18860v1",
      "published_date": "2025-11-24 08:00:48 UTC",
      "updated_date": "2025-11-24 08:00:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:16:19.768478+00:00"
    },
    {
      "arxiv_id": "2511.19528v1",
      "title": "Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories",
      "title_zh": "å‘ç°ã€å­¦ä¹ ä¸å¼ºåŒ–ï¼šåˆ©ç”¨å¤šæ ·åŒ–çš„å¼ºåŒ–å­¦ä¹ ç”Ÿæˆè½¨è¿¹æ‰©å±•è§†è§‰-è¯­è¨€-åŠ¨ä½œé¢„è®­ç»ƒ",
      "authors": [
        "Rushuai Yang",
        "Zhiyuan Feng",
        "Tianxiang Zhang",
        "Kaixin Wang",
        "Chuheng Zhang",
        "Li Zhao",
        "Xiu Su",
        "Yi Chen",
        "Jiang Bian"
      ],
      "abstract": "Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision-Language-Action (VLA) æ¨¡å‹é¢„è®­ç»ƒä¸­é«˜è´¨é‡è½¨è¿¹æ•°æ®è·å–æˆæœ¬é«˜çš„é—®é¢˜ï¼Œæå‡ºäº† Discover, Learn and Reinforce (DLR) æ¡†æ¶ã€‚DLR æ˜¯ä¸€ç§åŸºäºä¿¡æ¯è®ºçš„æ¨¡å¼å‘ç°æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœæ ‡å‡† Reinforcement Learning (RL) å®¹æ˜“é™·å…¥å•ä¸€æ¨¡å¼çš„å±€é™ï¼Œä¸ºæ¨¡å‹é¢„è®­ç»ƒç”Ÿæˆå¤šç§ä¸åŒä¸”é«˜æˆåŠŸç‡çš„è¡Œä¸ºæ¨¡å¼ã€‚åœ¨ LIBERO æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDLR èƒ½å¤Ÿé’ˆå¯¹åŒä¸€ä»»åŠ¡å­¦ä¹ å¤šç§æˆªç„¶ä¸åŒçš„ç­–ç•¥ï¼Œæ˜¾è‘—æ‰©å¤§äº†å¯¹ state-action space çš„è¦†ç›–ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å¤šæ ·åŒ– RL æ•°æ®ä¸Šé¢„è®­ç»ƒçš„ VLA æ¨¡å‹åœ¨è¿ç§»è‡³ä¸‹æ¸¸æœªè§ä»»åŠ¡æ—¶ï¼Œè¡¨ç°ä¼˜äºä½¿ç”¨æ ‡å‡† RL æ•°æ®é¢„è®­ç»ƒçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒDLR å±•ç¤ºäº†å•æ¨¡å¼ RL æ— æ³•å®ç°çš„æ­£å‘ data-scaling ç‰¹æ€§ï¼Œä¸ºæ„å»ºå…·èº«åŸºç¡€æ¨¡å‹ (embodied foundation models) æä¾›äº†ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„æ•°æ®ç”Ÿæˆå¼•æ“ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19528v1",
      "published_date": "2025-11-24 07:54:49 UTC",
      "updated_date": "2025-11-24 07:54:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:16:21.369434+00:00"
    },
    {
      "arxiv_id": "2511.18856v3",
      "title": "Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos",
      "title_zh": "å…¨å‘è§†é¢‘æ„Ÿå…´è¶£åŒºåŸŸæ£€æµ‹çš„æ·±åº¦æ··åˆæ¨¡å‹",
      "authors": [
        "Sana Alamgeer",
        "Mylene Farias",
        "Marcelo Carvalho"
      ],
      "abstract": "The main goal of the project is to design a new model that predicts regions of interest in 360$^{\\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹360Â°å…¨æ™¯è§†é¢‘ï¼ˆOmnidirectional Videosï¼‰å¼€å‘äº†ä¸€ç§æ–°å‹æ·±åº¦æ··åˆæ¨¡å‹ï¼Œç”¨äºå‡†ç¡®é¢„æµ‹è§†é¢‘ä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆRegion of Interest, ROIï¼‰ã€‚ROIæ£€æµ‹åœ¨å…¨æ™¯è§†é¢‘æµä¼ è¾“ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œé€šè¿‡é¢„æµ‹è§†å£ï¼ˆview-portsï¼‰å’Œæ™ºèƒ½è£å‰ªè§†é¢‘ï¼Œä¸ä»…èƒ½æ˜¾è‘—é™ä½å¸¦å®½æ¶ˆè€—ï¼Œè¿˜èƒ½å‡å°‘ç”¨æˆ·ä½¿ç”¨å¤´æˆ´å¼è®¾å¤‡æ—¶çš„å¤´éƒ¨è¿åŠ¨ï¼Œä»è€Œä¼˜åŒ–è§‚çœ‹ä½“éªŒã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆæ˜¾è‘—æ€§æ¨¡å‹ï¼ˆhybrid saliency modelï¼‰ï¼Œå…¶å·¥ä½œæµç¨‹æ¶µç›–è§†é¢‘å¸§é¢„å¤„ç†ã€æ¨¡å‹é¢„æµ‹ä»¥åŠç»“æœåå¤„ç†ä¸‰ä¸ªå…³é”®é˜¶æ®µã€‚é€šè¿‡åœ¨360RATæ•°æ®é›†ä¸Šä¸ä¸»è§‚æ ‡æ³¨æ•°æ®è¿›è¡Œå¯¹æ¯”åˆ†æï¼Œè¯¥æ–¹æ³•å±•ç°äº†åœ¨æå‡è§†é¢‘æµä¼ è¾“æ•ˆç‡å’Œè§†è§‰è´¨é‡æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "I need to withdraw this as it contains some confidential information related to FAPESP funding agency",
      "pdf_url": "https://arxiv.org/pdf/2511.18856v3",
      "published_date": "2025-11-24 07:52:06 UTC",
      "updated_date": "2026-01-14 18:25:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:16:25.978660+00:00"
    },
    {
      "arxiv_id": "2511.18854v1",
      "title": "Time Travel: LLM-Assisted Semantic Behavior Localization with Git Bisect",
      "title_zh": "Time Travelï¼šåŸºäº Git Bisect çš„ LLM è¾…åŠ©è¯­ä¹‰è¡Œä¸ºå®šä½",
      "authors": [
        "Yujing Wang",
        "Weize Hong"
      ],
      "abstract": "We present a novel framework that integrates Large Language Models (LLMs) into the Git bisect process for semantic fault localization. Traditional bisect assumes deterministic predicates and binary failure states assumptions often violated in modern software development due to flaky tests, nonmonotonic regressions, and semantic divergence from upstream repositories. Our system augments bisect traversal with structured chain of thought reasoning, enabling commit by commit analysis under noisy conditions. We evaluate multiple open source and proprietary LLMs for their suitability and fine tune DeepSeekCoderV2 using QLoRA on a curated dataset of semantically labeled diffs. We adopt a weak supervision workflow to reduce annotation overhead, incorporating human in the loop corrections and self consistency filtering. Experiments across multiple open source projects show a 6.4 point absolute gain in success rate from 74.2 to 80.6 percent, leading to significantly fewer failed traversals and by experiment up to 2x reduction in average bisect time. We conclude with discussions on temporal reasoning, prompt design, and finetuning strategies tailored for commit level behavior analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹ (LLMs) é›†æˆåˆ° Git bisect è¿‡ç¨‹ä¸­çš„æ–°å‹æ¡†æ¶ï¼Œä»¥å®ç°æ›´ç²¾å‡†çš„è¯­ä¹‰æ•…éšœå®šä½ (semantic fault localization)ã€‚é’ˆå¯¹ä¼ ç»Ÿ bisect éš¾ä»¥å¤„ç†ä¸ç¡®å®šæ€§æµ‹è¯• (flaky tests) å’Œéå•è°ƒå›å½’ (nonmonotonic regressions) ç­‰å™ªå£°ç¯å¢ƒçš„é—®é¢˜ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ç»“æ„åŒ–çš„é“¾å¼æ€ç»´ (chain of thought) æ¨ç†å®ç°äº†å¯¹ä»£ç æäº¤çš„æ·±å…¥åˆ†æã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ QLoRA æŠ€æœ¯å¯¹ DeepSeekCoderV2 è¿›è¡Œäº†é’ˆå¯¹æ€§å¾®è°ƒï¼Œå¹¶ç»“åˆå¼±ç›‘ç£ (weak supervision) å·¥ä½œæµã€äººå·¥å¹²é¢„ä¸è‡ªä¸€è‡´æ€§ (self consistency) è¿‡æ»¤æ¥æå‡æ¨¡å‹è¡¨ç°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªå¼€æºé¡¹ç›®ä¸Šå°†å®šä½æˆåŠŸç‡ä» 74.2% æå‡è‡³ 80.6%ï¼Œå¹¶å°†å¹³å‡ bisect æ—¶é—´ç¼©çŸ­äº†å¤šè¾¾ 2 å€ã€‚è¯¥æˆæœä¸ä»…æ¢è®¨äº†é’ˆå¯¹ä»£ç æäº¤çº§åˆ«åˆ†æçš„æ—¶é—´æ¨ç†å’Œæç¤ºè¯è®¾è®¡ï¼Œä¹Ÿä¸ºå¤æ‚è½¯ä»¶å¼€å‘ç¯å¢ƒä¸‹çš„è‡ªåŠ¨åŒ–è°ƒè¯•æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "submitted to Git Bisect SCALCOM 2025 Calgary (to be published)",
      "pdf_url": "https://arxiv.org/pdf/2511.18854v1",
      "published_date": "2025-11-24 07:49:59 UTC",
      "updated_date": "2025-11-24 07:49:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:17:29.881394+00:00"
    },
    {
      "arxiv_id": "2511.18849v1",
      "title": "Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize LLM-Assisted Programming",
      "title_zh": "åˆ©ç”¨å¼€å‘è€…è¡Œä¸ºé¥æµ‹é¢„è¿‡æ»¤ä»£ç å»ºè®®ä»¥ä¼˜åŒ– LLM è¾…åŠ©ç¼–ç¨‹",
      "authors": [
        "Mohammad Nour Al Awad",
        "Sergey Ivanov",
        "Olga Tikhonova"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly integrated into code editors to provide AI-powered code suggestions. Yet many of these suggestions are ignored, resulting in wasted computation, increased latency, and unnecessary interruptions. We introduce a lightweight pre-filtering model that predicts the likelihood of suggestion acceptance before invoking the LLM, using only real-time developer telemetry such as typing speed, file navigation, and editing activity. Deployed in a production-grade Visual Studio Code plugin over four months of naturalistic use, our approach nearly doubled acceptance rates (18.4% -> 34.2%) while suppressing 35% of low-value LLM calls. These findings demonstrate that behavioral signals alone can meaningfully improve both user experience and system efficiency in LLM-assisted programming, highlighting the value of timing-aware, privacy-preserving adaptation mechanisms. The filter operates solely on pre-invocation editor telemetry and never inspects code or prompts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä»£ç ç¼–è¾‘å™¨ä¸­é¢‘ç¹äº§ç”Ÿè¢«å¿½ç•¥å»ºè®®å¯¼è‡´è®¡ç®—æµªè´¹å’Œå¹²æ‰°çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§çš„é¢„è¿‡æ»¤ (pre-filtering) æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨è°ƒç”¨ LLM ä¹‹å‰ï¼Œä»…åˆ©ç”¨æ‰“å­—é€Ÿåº¦ã€æ–‡ä»¶å¯¼èˆªå’Œç¼–è¾‘æ´»åŠ¨ç­‰å®æ—¶å¼€å‘è€…è¡Œä¸ºé¥æµ‹ (behavioral telemetry) æ•°æ®æ¥é¢„æµ‹å»ºè®®è¢«æ¥å—çš„å¯èƒ½æ€§ã€‚è¯¥æ–¹æ³•åœ¨ Visual Studio Code æ’ä»¶çš„ç”Ÿäº§ç¯å¢ƒä¸­è¿›è¡Œäº†ä¸ºæœŸå››ä¸ªæœˆçš„å®æµ‹ï¼Œå°†å»ºè®®æ¥å—ç‡ä» 18.4% æé«˜åˆ° 34.2%ï¼Œå¹¶æˆåŠŸæ‹¦æˆªäº† 35% çš„ä½ä»·å€¼ LLM è°ƒç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…é€šè¿‡è¡Œä¸ºä¿¡å·å°±èƒ½åœ¨ä¸æ£€æŸ¥ä»£ç å†…å®¹æˆ–æç¤ºè¯ (prompts) çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ä¼˜åŒ– LLM è¾…åŠ©ç¼–ç¨‹çš„ç”¨æˆ·ä½“éªŒä¸ç³»ç»Ÿæ•ˆç‡ã€‚è¿™ç§å…·å¤‡éšç§ä¿æŠ¤ç‰¹æ€§çš„è‡ªé€‚åº”æœºåˆ¶ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆã€å¯ä¿¡çš„æ™ºèƒ½ç¼–ç¨‹è¾…åŠ©å·¥å…·æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses",
      "pdf_url": "https://arxiv.org/pdf/2511.18849v1",
      "published_date": "2025-11-24 07:42:07 UTC",
      "updated_date": "2025-11-24 07:42:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:17:28.869976+00:00"
    },
    {
      "arxiv_id": "2511.18847v1",
      "title": "Personalized Federated Segmentation with Shared Feature Aggregation and Boundary-Focused Calibration",
      "title_zh": "åŸºäºå…±äº«ç‰¹å¾èšåˆä¸è¾¹ç•Œèšç„¦æ ¡å‡†çš„ä¸ªæ€§åŒ–è”é‚¦åˆ†å‰²",
      "authors": [
        "Ishmam Tashdeed",
        "Md. Atiqur Rahman",
        "Sabrina Islam",
        "Md. Azam Hossain"
      ],
      "abstract": "Personalized federated learning (PFL) possesses the unique capability of preserving data confidentiality among clients while tackling the data heterogeneity problem of non-independent and identically distributed (Non-IID) data. Its advantages have led to widespread adoption in domains such as medical image segmentation. However, the existing approaches mostly overlook the potential benefits of leveraging shared features across clients, where each client contains segmentation data of different organs. In this work, we introduce a novel personalized federated approach for organ agnostic tumor segmentation (FedOAP), that utilizes cross-attention to model long-range dependencies among the shared features of different clients and a boundary-aware loss to improve segmentation consistency. FedOAP employs a decoupled cross-attention (DCA), which enables each client to retain local queries while attending to globally shared key-value pairs aggregated from all clients, thereby capturing long-range inter-organ feature dependencies. Additionally, we introduce perturbed boundary loss (PBL) which focuses on the inconsistencies of the predicted mask's boundary for each client, forcing the model to localize the margins more precisely. We evaluate FedOAP on diverse tumor segmentation tasks spanning different organs. Extensive experiments demonstrate that FedOAP consistently outperforms existing state-of-the-art federated and personalized segmentation methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹  (Personalized Federated Learning, PFL) åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­å¿½è§†è·¨å®¢æˆ·ç«¯å…±äº«ç‰¹å¾çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º FedOAP çš„æ–°å‹ä¸ªæ€§åŒ–è”é‚¦æ¡†æ¶ã€‚è¯¥æ–¹æ³•æ—¨åœ¨å®ç°å™¨å®˜ä¸å¯çŸ¥ (organ agnostic) çš„è‚¿ç˜¤åˆ†å‰²ï¼Œé€šè¿‡ Decoupled Cross-Attention (DCA) æœºåˆ¶å¯¹ä¸åŒå®¢æˆ·ç«¯é—´çš„å…±äº«ç‰¹å¾è¿›è¡Œé•¿ç¨‹ä¾èµ–å»ºæ¨¡ã€‚DCA å…è®¸æ¯ä¸ªå®¢æˆ·ç«¯åœ¨ä¿ç•™æœ¬åœ° query çš„åŒæ—¶ï¼Œè®¿é—®ä»æ‰€æœ‰å®¢æˆ·ç«¯èšåˆçš„å…¨å±€å…±äº« key-value å¯¹ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰è·¨å™¨å®˜çš„ç‰¹å¾å…³è”ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜åˆ†å‰²çš„ä¸€è‡´æ€§ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† Perturbed Boundary Loss (PBL)ï¼Œé€šè¿‡å…³æ³¨é¢„æµ‹æ©ç è¾¹ç•Œçš„ä¸ä¸€è‡´æ€§æ¥å¼ºåˆ¶æ¨¡å‹æ›´ç²¾å‡†åœ°å®šä½è¾¹ç¼˜ã€‚åœ¨æ¶‰åŠä¸åŒå™¨å®˜çš„å¤šç§è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜ï¼ŒFedOAP çš„è¡¨ç°å§‹ç»ˆä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›è”é‚¦å­¦ä¹ å’Œä¸ªæ€§åŒ–åˆ†å‰²æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18847v1",
      "published_date": "2025-11-24 07:40:04 UTC",
      "updated_date": "2025-11-24 07:40:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:16:39.049520+00:00"
    },
    {
      "arxiv_id": "2511.18846v1",
      "title": "WaveTuner: Comprehensive Wavelet Subband Tuning for Time Series Forecasting",
      "title_zh": "WaveTunerï¼šé¢å‘æ—¶é—´åºåˆ—é¢„æµ‹çš„å…¨æ–¹ä½å°æ³¢å­å¸¦è°ƒä¼˜",
      "authors": [
        "Yubo Wang",
        "Hui He",
        "Chaoxi Niu",
        "Zhendong Niu"
      ],
      "abstract": "Due to the inherent complexity, temporal patterns in real-world time series often evolve across multiple intertwined scales, including long-term periodicity, short-term fluctuations, and abrupt regime shifts. While existing literature has designed many sophisticated decomposition approaches based on the time or frequency domain to partition trend-seasonality components and high-low frequency components, an alternative line of approaches based on the wavelet domain has been proposed to provide a unified multi-resolution representation with precise time-frequency localization. However, most wavelet-based methods suffer from a persistent bias toward recursively decomposing only low-frequency components, severely underutilizing subtle yet informative high-frequency components that are pivotal for precise time series forecasting. To address this problem, we propose WaveTuner, a Wavelet decomposition framework empowered by full-spectrum subband Tuning for time series forecasting. Concretely, WaveTuner comprises two key modules: (i) Adaptive Wavelet Refinement module, that transforms time series into time-frequency coefficients, utilizes an adaptive router to dynamically assign subband weights, and generates subband-specific embeddings to support refinement; and (ii) Multi-Branch Specialization module, that employs multiple functional branches, each instantiated as a flexible Kolmogorov-Arnold Network (KAN) with a distinct functional order to model a specific spectral subband. Equipped with these modules, WaveTuner comprehensively tunes global trends and local variations within a unified time-frequency framework. Extensive experiments on eight real-world datasets demonstrate WaveTuner achieves state-of-the-art forecasting performance in time series forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®ä¸–ç•Œæ—¶é—´åºåˆ—åœ¨å¤šå°ºåº¦æ¼”å˜ä¸­çš„å¤æ‚æ€§ï¼ŒæŒ‡å‡ºå½“å‰åŸºäºå°æ³¢(Wavelet)åŸŸçš„æ–¹æ³•æ™®éå­˜åœ¨è¿‡åº¦å…³æ³¨ä½é¢‘æˆåˆ†è€Œå¿½è§†å…³é”®é«˜é¢‘ä¿¡æ¯çš„é—®é¢˜ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†WaveTuneræ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å…¨é¢‘è°±å­å¸¦è°ƒä¼˜(full-spectrum subband Tuning)æŠ€æœ¯ï¼Œåœ¨ç»Ÿä¸€çš„æ—¶é¢‘æ¡†æ¶å†…å…¨é¢è°ƒèŠ‚å…¨å±€è¶‹åŠ¿å’Œå±€éƒ¨å˜åŒ–ã€‚å…¶æ ¸å¿ƒç”±è‡ªé€‚åº”å°æ³¢ç»†åŒ–(Adaptive Wavelet Refinement)å’Œå¤šåˆ†æ”¯ä¸“ä¸šåŒ–(Multi-Branch Specialization)ä¸¤ä¸ªå…³é”®æ¨¡å—ç»„æˆï¼Œå‰è€…åˆ©ç”¨è‡ªé€‚åº”è·¯ç”±åŠ¨æ€åˆ†é…å­å¸¦æƒé‡ï¼Œåè€…åˆ™é‡‡ç”¨çµæ´»çš„Kolmogorov-Arnold Network (KAN)å¯¹ç‰¹å®šé¢‘è°±å­å¸¦è¿›è¡Œå»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWaveTuneråœ¨å…«ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›(State-of-the-art)çš„é¢„æµ‹æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ç²¾ç¡®çš„æ—¶é—´-é¢‘ç‡å®šä½ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹å¤æ‚æ—¶åºæ¨¡å¼çš„å¤„ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18846v1",
      "published_date": "2025-11-24 07:33:35 UTC",
      "updated_date": "2025-11-24 07:33:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:16:50.563956+00:00"
    },
    {
      "arxiv_id": "2511.18845v1",
      "title": "UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model",
      "title_zh": "UNeMoï¼šåŸºäºå¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹çš„ååŒè§†è§‰-è¯­è¨€æ¨ç†ä¸å¯¼èˆª",
      "authors": [
        "Changxin Huang",
        "Lv Tang",
        "Zhaohuan Zhan",
        "Lisha Yu",
        "Runhao Zeng",
        "Zun Liu",
        "Zhengjie Wang",
        "Jianqiang Li"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) requires agents to autonomously navigate complex environments via visual images and natural language instruction--remains highly challenging. Recent research on enhancing language-guided navigation reasoning using pre-trained large language models (LLMs) has shown promising prospects. However, the reasoning of such methods is limited to the linguistic modality, lacking visual reasoning capabilities. Moreover, existing reasoning modules are optimized separately from navigation policies, leading to incompatibility and potential conflicts in optimization objectives. To tackle these challenges, we introduce UNeMo, a novel framework designed for the collaborative optimization of visual state reasoning and navigational decision-making. It introduces a Multimodal World Model (MWM) that takes visual features, language instructions, and navigational actions as inputs to jointly predict subsequent visual states, enabling cross-modal reasoning. Via a Hierarchical Prediction-Feedback (HPN) mechanism, MWM collaborates with navigation policies: the first layer generates actions using current vision-and-language features; MWM then infers post-action visual states to guide the second layer's fine-grained decisions. This forms a dynamic bidirectional promotion mechanism where MWM reasoning optimizes navigation policies, while policy decisions feedback to improve MWM's reasoning accuracy. Experiments on R2R and REVERIE datasets show UNeMo outperforms state-of-the-art methods by 2.1% and 0.7% in navigation accuracy for unseen scenes, validating its effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UNeMoæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Vision-and-Language Navigation (VLN)ä¸­æ¨ç†å±€é™äºè¯­è¨€æ¨¡æ€ä»¥åŠæ¨ç†æ¨¡å—ä¸å¯¼èˆªç­–ç•¥è„±èŠ‚çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªMultimodal World Model (MWM)ï¼Œé€šè¿‡æ•´åˆè§†è§‰ç‰¹å¾ã€è¯­è¨€æŒ‡ä»¤å’Œå¯¼èˆªåŠ¨ä½œæ¥è”åˆé¢„æµ‹åç»­è§†è§‰çŠ¶æ€ï¼Œå®ç°äº†é«˜æ•ˆçš„è·¨æ¨¡æ€æ¨ç†ã€‚åˆ©ç”¨Hierarchical Prediction-Feedback (HPN)æœºåˆ¶ï¼ŒUNeMoå®ç°äº†è§†è§‰çŠ¶æ€æ¨ç†ä¸å¯¼èˆªå†³ç­–çš„ååŒä¼˜åŒ–ï¼Œç¬¬ä¸€å±‚ç”Ÿæˆåˆæ­¥åŠ¨ä½œåï¼Œç”±MWMæ¨æ–­åŠ¨ä½œåçš„è§†è§‰çŠ¶æ€ä»¥æŒ‡å¯¼ç¬¬äºŒå±‚çš„ç²¾ç»†åŒ–å†³ç­–ã€‚è¿™ç§åŠ¨æ€åŒå‘ä¿ƒè¿›æœºåˆ¶ç¡®ä¿äº†æ¨ç†è¿‡ç¨‹ä¸å¯¼èˆªç­–ç•¥çš„ç›¸äº’å¢å¼ºï¼Œæå‡äº†å†³ç­–çš„è¿è´¯æ€§ã€‚åœ¨R2Rå’ŒREVERIEæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUNeMoåœ¨æœªè§åœºæ™¯ä¸­çš„å¯¼èˆªå‡†ç¡®ç‡åˆ†åˆ«æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æé«˜äº†2.1%å’Œ0.7%ï¼ŒéªŒè¯äº†è¯¥å¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18845v1",
      "published_date": "2025-11-24 07:31:58 UTC",
      "updated_date": "2025-11-24 07:31:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:16:40.566480+00:00"
    },
    {
      "arxiv_id": "2511.18842v1",
      "title": "Optimizing LLM Code Suggestions: Feedback-Driven Timing with Lightweight State Bounds",
      "title_zh": "ä¼˜åŒ– LLM ä»£ç å»ºè®®ï¼šç»“åˆè½»é‡çº§çŠ¶æ€è¾¹ç•Œçš„åé¦ˆé©±åŠ¨è§¦å‘æ—¶æœº",
      "authors": [
        "Mohammad Nour Al Awad",
        "Sergey Ivanov",
        "Olga Tikhonova"
      ],
      "abstract": "Large Language Models (LLMs) have transformed code auto-completion by generating context-aware suggestions. Yet, deciding when to present these suggestions remains underexplored, often leading to interruptions or wasted inference calls. We propose an adaptive timing mechanism that dynamically adjusts the delay before offering a suggestion based on real-time developer feedback. Our suggested method combines a logistic transform of recent acceptance rates with a bounded delay range, anchored by a high-level binary prediction of the developer's cognitive state. In a two-month deployment with professional developers, our system improved suggestion acceptance from 4.9% with no delay to 15.4% with static delays, and to 18.6% with adaptive timing-while reducing blind rejections (rejections without being read) from 8.3% to 0.36%. Together, these improvements increase acceptance and substantially reduce wasted inference calls by 75%, making LLM-based code assistants more efficient and cost-effective in practice.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»£ç è‡ªåŠ¨è¡¥å…¨ä¸­æç¤ºæ—¶æœºä¸å½“å¯¼è‡´çš„å¼€å‘å¹²æ‰°å’Œæ¨ç†èµ„æºæµªè´¹ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå®æ—¶å¼€å‘è€…åé¦ˆçš„è‡ªé€‚åº”å®šæ—¶æœºåˆ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è¿‘æœŸæ¥å—ç‡çš„é€»è¾‘å˜æ¢(logistic transform)ä¸å—é™å»¶è¿ŸèŒƒå›´ï¼Œå¹¶ç»“åˆå¯¹å¼€å‘è€…è®¤çŸ¥çŠ¶æ€(cognitive state)çš„é«˜å±‚äºŒå…ƒé¢„æµ‹æ¥åŠ¨æ€è°ƒæ•´å»ºè®®å‘ˆç°çš„å»¶è¿Ÿã€‚åœ¨ä¸ºæœŸä¸¤ä¸ªæœˆçš„ä¸“ä¸šå¼€å‘è€…å®æµ‹ä¸­ï¼Œè¯¥ç³»ç»Ÿå°†ä»£ç å»ºè®®æ¥å—ç‡ä»æ— å»¶è¿Ÿæ—¶çš„4.9%æ˜¾è‘—æå‡è‡³è‡ªé€‚åº”å®šæ—¶ä¸‹çš„18.6%ï¼ŒåŒæ—¶å°†æœªè¯»å³æ‹’(blind rejections)çš„æ¯”ä¾‹ä»8.3%é™è‡³0.36%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æœºåˆ¶åœ¨æé«˜å»ºè®®è´¨é‡çš„åŒæ—¶ï¼ŒæˆåŠŸå‡å°‘äº†75%çš„æ— æ•ˆæ¨ç†è°ƒç”¨(inference calls)ï¼Œæ˜¾è‘—æå‡äº†ä»£ç åŠ©æ‰‹çš„æ•ˆç‡ä¸æˆæœ¬æ•ˆç›Šã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses",
      "pdf_url": "https://arxiv.org/pdf/2511.18842v1",
      "published_date": "2025-11-24 07:29:15 UTC",
      "updated_date": "2025-11-24 07:29:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:16:51.363891+00:00"
    },
    {
      "arxiv_id": "2511.18841v1",
      "title": "Federated style aware transformer aggregation of representations",
      "title_zh": "é£æ ¼æ„ŸçŸ¥çš„è”é‚¦ Transformer è¡¨å¾èšåˆ",
      "authors": [
        "Mincheol Jeon",
        "Euinam Huh"
      ],
      "abstract": "Personalized Federated Learning (PFL) faces persistent challenges, including domain heterogeneity from diverse client data, data imbalance due to skewed participation, and strict communication constraints. Traditional federated learning often lacks personalization, as a single global model cannot capture client-specific characteristics, leading to biased predictions and poor generalization, especially for clients with highly divergent data distributions.\n  To address these issues, we propose FedSTAR, a style-aware federated learning framework that disentangles client-specific style factors from shared content representations. FedSTAR aggregates class-wise prototypes using a Transformer-based attention mechanism, allowing the server to adaptively weight client contributions while preserving personalization.\n  Furthermore, by exchanging compact prototypes and style vectors instead of full model parameters, FedSTAR significantly reduces communication overhead. Experimental results demonstrate that combining content-style disentanglement with attention-driven prototype aggregation improves personalization and robustness in heterogeneous environments without increasing communication cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FedSTARï¼Œä¸€ç§é£æ ¼æ„ŸçŸ¥çš„ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹  (Personalized Federated Learning, PFL) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é¢†åŸŸå¼‚æ„æ€§ã€æ•°æ®ä¸å¹³è¡¡åŠé€šä¿¡çº¦æŸç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å®¢æˆ·ç«¯ç‰¹å®šçš„é£æ ¼å› å­ (style factors) ä»å…±äº«çš„å†…å®¹è¡¨ç¤º (content representations) ä¸­è§£è€¦ï¼Œå®ç°äº†å¯¹å®¢æˆ·ç«¯ç‰¹å¾çš„ç²¾å‡†æ•æ‰ã€‚FedSTAR å¼•å…¥äº†åŸºäº Transformer çš„æ³¨æ„åŠ›æœºåˆ¶æ¥èšåˆç±»åŸå‹ (class-wise prototypes)ï¼Œä½¿æœåŠ¡å™¨èƒ½å¤Ÿè‡ªé€‚åº”åœ°åŠ æƒå®¢æˆ·ç«¯è´¡çŒ®å¹¶ä¿æŒä¸ªæ€§åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡äº¤æ¢ç´§å‡‘çš„åŸå‹å’Œé£æ ¼å‘é‡ (style vectors) æ›¿ä»£å®Œæ•´çš„æ¨¡å‹å‚æ•°ï¼Œæ˜¾è‘—é™ä½äº†é€šä¿¡å¼€é”€ã€‚å®éªŒç»“æœè¯æ˜ï¼Œå†…å®¹ä¸é£æ ¼è§£è€¦ç»“åˆæ³¨æ„åŠ›é©±åŠ¨çš„åŸå‹èšåˆï¼Œåœ¨ä¸å¢åŠ é€šä¿¡æˆæœ¬çš„å‰æä¸‹ï¼Œæœ‰æ•ˆæå‡äº†å¼‚æ„ç¯å¢ƒä¸­çš„ä¸ªæ€§åŒ–èƒ½åŠ›ä¸ç³»ç»Ÿé²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18841v1",
      "published_date": "2025-11-24 07:24:09 UTC",
      "updated_date": "2025-11-24 07:24:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:16:52.573871+00:00"
    },
    {
      "arxiv_id": "2511.18840v1",
      "title": "Addressing Situated Teaching Needs: A Multi-Agent Framework for Automated Slide Adaptation",
      "title_zh": "åº”å¯¹æƒ…å¢ƒåŒ–æ•™å­¦éœ€æ±‚ï¼šä¸€ç§ç”¨äºè‡ªåŠ¨åŒ–å¹»ç¯ç‰‡é€‚é…çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Binglin Liu",
        "Yucheng Wang",
        "Zheyuan Zhang",
        "Jiyuan Lu",
        "Shen Yang",
        "Daniel Zhang-Li",
        "Huiqin Liu",
        "Jifan Yu"
      ],
      "abstract": "The adaptation of teaching slides to instructors' situated teaching needs, including pedagogical styles and their students' context, is a critical yet time-consuming task for educators. Through a series of educator interviews, we first identify and systematically categorize the key friction points that impede this adaptation process. Grounded in these findings, we introduce a novel multi-agent framework designed to automate slide adaptation based on high-level instructor specifications. An evaluation involving 16 modification requests across 8 real-world courses validates our approach. The framework's output consistently achieved high scores in intent alignment, content coherence and factual accuracy, and performed on par with baseline methods regarding visual clarity, while also demonstrating appropriate timeliness and a high operational agreement with human experts, achieving an F1 score of 0.89. This work heralds a new paradigm where AI agents handle the logistical burdens of instructional design, liberating educators to focus on the creative and strategic aspects of teaching.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•™è‚²è€…åœ¨æ ¹æ®ç‰¹å®šæ•™å­¦é£æ ¼å’Œå­¦ç”ŸèƒŒæ™¯è°ƒæ•´æ•™å­¦è¯¾ä»¶(slides)æ—¶é¢ä¸´çš„è€—æ—¶éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶(multi-agent framework)ï¼Œæ—¨åœ¨å®ç°è¯¾ä»¶çš„è‡ªåŠ¨é€‚é…ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆé€šè¿‡æ•™è‚²è€…è®¿è°ˆç³»ç»Ÿåœ°è¯†åˆ«å¹¶åˆ†ç±»äº†é€‚é…è¿‡ç¨‹ä¸­çš„æ ¸å¿ƒæ‘©æ“¦ç‚¹ï¼Œå¹¶ä»¥æ­¤ä¸ºåŸºç¡€è®¾è®¡äº†èƒ½å¤Ÿæ ¹æ®æ•™å¸ˆé«˜å±‚æŒ‡ä»¤è¿›è¡Œæ“ä½œçš„è‡ªåŠ¨åŒ–ç³»ç»Ÿã€‚é€šè¿‡å¯¹8ä¸ªçœŸå®è¯¾ç¨‹ä¸­çš„16é¡¹ä¿®æ”¹è¯·æ±‚è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶åœ¨æ„å›¾å¯¹é½(intent alignment)ã€å†…å®¹è¿è´¯æ€§(content coherence)å’Œäº‹å®å‡†ç¡®æ€§(factual accuracy)æ–¹é¢å‡è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨è§†è§‰æ¸…æ™°åº¦ä¸Šä¸åŸºçº¿æ–¹æ³•ç›¸å½“ï¼Œä¸”è¡¨ç°å‡ºè‰¯å¥½çš„åŠæ—¶æ€§ï¼Œä¸äººç±»ä¸“å®¶çš„æ“ä½œä¸€è‡´æ€§è¾¾åˆ°0.89çš„F1åˆ†æ•°ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†AIæ™ºèƒ½ä½“æ‰¿æ‹…æ•™å­¦è®¾è®¡åå‹¤è´Ÿæ‹…çš„æ½œåŠ›ï¼Œä»è€Œä½¿æ•™è‚²è€…èƒ½å¤Ÿæ›´ä¸“æ³¨äºæ•™å­¦çš„åˆ›æ„ä¸æˆ˜ç•¥æ ¸å¿ƒã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18840v1",
      "published_date": "2025-11-24 07:22:41 UTC",
      "updated_date": "2025-11-24 07:22:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:17:42.974277+00:00"
    },
    {
      "arxiv_id": "2511.18834v1",
      "title": "FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories",
      "title_zh": "FlowSteerï¼šåˆ©ç”¨çœŸå®è½¨è¿¹å¼•å¯¼å°‘æ­¥å›¾åƒåˆæˆ",
      "authors": [
        "Lei Ke",
        "Hubery Yin",
        "Gongye Liu",
        "Zhengyao Lv",
        "Jingcai Guo",
        "Chen Li",
        "Wenhan Luo",
        "Yujiu Yang",
        "Jing Lyu"
      ],
      "abstract": "With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application. Among flow models' accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching. This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation. In this work, we investigate this issue within the ReFlow framework and propose FlowSteer, a method unlocks the potential of ReFlow-based distillation by guiding the student along teacher's authentic generation trajectories. We first identify that Piecewised ReFlow's performance is hampered by a critical distribution mismatch during the training and propose Online Trajectory Alignment(OTA) to resolve it. Then, we introduce a adversarial distillation objective applied directly on the ODE trajectory, improving the student's adherence to the teacher's generation trajectory. Furthermore, we find and fix a previously undiscovered flaw in the widely-used FlowMatchEulerDiscreteScheduler that largely degrades few-step inference quality. Our experiment result on SD3 demonstrates our method's efficacy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Flow Matching åœ¨å›¾åƒç”Ÿæˆä¸­é‡‡æ ·æ•ˆç‡ä½ä¸‹çš„ç“¶é¢ˆï¼Œæå‡ºäº† FlowSteer æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¼•å¯¼å­¦ç”Ÿæ¨¡å‹éµå¾ªæ•™å¸ˆæ¨¡å‹çš„çœŸå®ç”Ÿæˆè½¨è¿¹æ¥é‡Šæ”¾ ReFlow è’¸é¦æŠ€æœ¯çš„æ½œåŠ›ã€‚ä½œè€…æŒ‡å‡º Piecewised ReFlow åœ¨å®é™…åº”ç”¨ä¸­å—åˆ°è®­ç»ƒæœŸé—´åˆ†å¸ƒä¸åŒ¹é…çš„é™åˆ¶ï¼Œå¹¶æ®æ­¤æå‡ºäº†åœ¨çº¿è½¨è¿¹å¯¹é½ (Online Trajectory Alignment, OTA) æŠ€æœ¯ä»¥è§£å†³è¯¥é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†ç›´æ¥åº”ç”¨äº ODE è½¨è¿¹çš„å¯¹æŠ—æ€§è’¸é¦ç›®æ ‡ï¼Œæ˜¾è‘—å¢å¼ºäº†å­¦ç”Ÿæ¨¡å‹å¯¹æ•™å¸ˆæ¨¡å‹ç”Ÿæˆè·¯å¾„çš„éµå¾ªç¨‹åº¦ã€‚ç ”ç©¶è¿˜å‘ç°å¹¶ä¿®å¤äº†å¹¿æ³›ä½¿ç”¨çš„ FlowMatchEulerDiscreteScheduler ä¸­æ­¤å‰æœªè¢«å¯Ÿè§‰çš„ç¼ºé™·ï¼Œè¯¥ç¼ºé™·æ›¾ä¸¥é‡é™ä½å°‘æ­¥æ¨ç†çš„è´¨é‡ã€‚åœ¨ SD3 ä¸Šçš„å®éªŒç»“æœè¯æ˜äº† FlowSteer çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®ç°é«˜æ•ˆä¸”é«˜è´¨é‡çš„å°‘æ­¥å›¾åƒåˆæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Few-Step Image Synthesis",
      "pdf_url": "https://arxiv.org/pdf/2511.18834v1",
      "published_date": "2025-11-24 07:13:23 UTC",
      "updated_date": "2025-11-24 07:13:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:17:51.776399+00:00"
    },
    {
      "arxiv_id": "2511.18828v3",
      "title": "Solving a Research Problem in Mathematical Statistics with AI Assistance",
      "title_zh": "åœ¨äººå·¥æ™ºèƒ½è¾…åŠ©ä¸‹è§£å†³æ•°ç†ç»Ÿè®¡ä¸­çš„ç ”ç©¶é—®é¢˜",
      "authors": [
        "Edgar Dobriban"
      ],
      "abstract": "Over the last few months, AI models including large language models have improved greatly. There are now several documented examples where they have helped professional mathematical scientists prove new results, sometimes even helping resolve known open problems. In this short note, we add another example to the list, by documenting how we were able to solve a previously unsolved research problem in robust mathematical statistics with crucial help from GPT-5. Our problem concerns robust density estimation, where the observations are perturbed by Wasserstein-bounded contaminations. In a previous preprint (Chao and Dobriban, 2023, arxiv:2308.01853v2), we have obtained upper and lower bounds on the minimax optimal estimation error; which were, however, not sharp.\n  Starting in October 2025, making significant use of GPT-5 Pro, we were able to derive the minimax optimal error rate (reported in version 3 of the above arxiv preprint). GPT-5 provided crucial help along the way, including by suggesting calculations that we did not think of, and techniques that were not familiar to us, such as the dynamic Benamou-Brenier formulation, for key steps in the analysis. Working with GPT-5 took a few weeks of effort, and we estimate that it could have taken several months to get the same results otherwise. At the same time, there are still areas where working with GPT-5 was challenging: it sometimes provided incorrect references, and glossed over details that sometimes took days of work to fill in. We outline our workflow and steps taken to mitigate issues. Overall, our work can serve as additional documentation for a new age of human-AI collaborative work in mathematical science.",
      "tldr_zh": "è¯¥ç ”ç©¶è®°å½•äº†åœ¨ GPT-5 çš„å…³é”®ååŠ©ä¸‹ï¼ŒæˆåŠŸè§£å†³é²æ£’æ•°ç†ç»Ÿè®¡(robust mathematical statistics)ä¸­ä¸€ä¸ªæ­¤å‰æœªè§£å†³çš„ç§‘ç ”éš¾é¢˜çš„è¿‡ç¨‹ã€‚ç ”ç©¶çš„æ ¸å¿ƒé—®é¢˜åœ¨äºå— Wasserstein-bounded contaminations æ‰°åŠ¨ä¸‹çš„é²æ£’å¯†åº¦ä¼°è®¡(robust density estimation)ï¼Œä½œè€…æ­¤å‰ä»…è·å¾—äº†æå°æå¤§æœ€ä¼˜ä¼°è®¡è¯¯å·®(minimax optimal estimation error)çš„éç²¾ç¡®ä¸Šä¸‹ç•Œã€‚é€šè¿‡ä¸ GPT-5 Pro çš„æ·±åº¦åä½œï¼Œç ”ç©¶å›¢é˜ŸæˆåŠŸæ¨å¯¼å‡ºäº†æå°æå¤§æœ€ä¼˜è¯¯å·®ç‡ï¼ŒAI åœ¨æ­¤è¿‡ç¨‹ä¸­æä¾›äº†å…³é”®çš„è®¡ç®—å»ºè®®ï¼Œå¹¶å¼•å…¥äº† dynamic Benamou-Brenier formulation ç­‰ç ”ç©¶å›¢é˜Ÿæ­¤å‰å¹¶ä¸ç†Ÿæ‚‰çš„åˆ†ææŠ€æœ¯ã€‚å°½ç®¡ AI åœ¨åä½œä¸­å­˜åœ¨æä¾›é”™è¯¯å‚è€ƒæ–‡çŒ®åŠå¿½ç•¥ç»†èŠ‚ç­‰æŒ‘æˆ˜ï¼Œä½†è¯¥æ¨¡å¼å°†åŸæœ¬é¢„è®¡éœ€æ•°æœˆçš„ç ”ç©¶å‘¨æœŸç¼©çŸ­è‡³æ•°å‘¨ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ•°å­¦ç§‘å­¦é¢†åŸŸçš„äººæœºåä½œæ–°æ—¶ä»£æä¾›äº†é‡è¦çš„å®è·µæ–‡æ¡£ä¸å·¥ä½œæµå‚è€ƒã€‚",
      "categories": [
        "math.ST",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.ST",
      "comment": "added references",
      "pdf_url": "https://arxiv.org/pdf/2511.18828v3",
      "published_date": "2025-11-24 07:03:56 UTC",
      "updated_date": "2025-12-17 13:53:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:17:48.264316+00:00"
    },
    {
      "arxiv_id": "2511.20697v2",
      "title": "Musical Score Understanding Benchmark: Evaluating Large Language Models' Comprehension of Complete Musical Scores",
      "title_zh": "Musical Score Understanding Benchmarkï¼šå¤§è¯­è¨€æ¨¡å‹å®Œæ•´ä¹è°±ç†è§£èƒ½åŠ›è¯„ä¼°",
      "authors": [
        "Congren Dai",
        "Yue Yang",
        "Krinos Li",
        "Huichi Zhou",
        "Shijie Liang",
        "Zhang Bo",
        "Enyang Liu",
        "Ge Jin",
        "Hongran An",
        "Haosen Zhang",
        "Peiyuan Jing",
        "KinHei Lee",
        "Zhenxuan Zhang",
        "Xiaobing Li",
        "Maosong Sun"
      ],
      "abstract": "Understanding complete musical scores entails integrated reasoning over pitch, rhythm, harmony, and large-scale structure, yet the ability of Large Language Models and Vision-Language Models to interpret full musical notation remains insufficiently examined. We introduce the Musical Score Understanding Benchmark (MSU-Bench), the first large-scale, human-curated benchmark for score-level musical understanding across textual (ABC notation) and visual (PDF) modalities. MSU-Bench contains 1,800 generative Question-Answering pairs from works by Bach, Beethoven, Chopin, Debussy, and others, organised into four levels of increasing difficulty, ranging from onset information to texture and form. Evaluations of more than fifteen state-of-the-art models, in both zero-shot and fine-tuned settings, reveal pronounced modality gaps, unstable level-wise performance, and challenges in maintaining multilevel correctness. Fine-tuning substantially improves results across modalities while preserving general knowledge, positioning MSU-Bench as a robust foundation for future research in multimodal reasoning. To facilitate further research, we publicly release MSU-Bench and all associated resources.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Musical Score Understanding Benchmark (MSU-Bench)ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å®Œæ•´ä¹è°±ç†è§£çš„å¤§è§„æ¨¡ã€äººå·¥ç­–åˆ’åˆ†è¾¨åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°Large Language Modelså’ŒVision-Language Modelsåœ¨æ–‡æœ¬ï¼ˆABC notationï¼‰ä¸è§†è§‰ï¼ˆPDFï¼‰æ¨¡æ€ä¸‹çš„éŸ³ä¹æ¨ç†èƒ½åŠ›ã€‚MSU-BenchåŒ…å«1,800ç»„ç”Ÿæˆå¼é—®ç­”å¯¹ï¼Œæ¶µç›–äº†ä»åŸºæœ¬éŸ³ç¬¦ä¿¡æ¯åˆ°å¤æ‚ç»‡ä½“ä¸ç»“æ„çš„å››ä¸ªéš¾åº¦ç­‰çº§ã€‚é€šè¿‡å¯¹è¶…è¿‡15ç§ä¸»æµæ¨¡å‹çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°æ¨¡å‹åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œä¸”åœ¨å¤šå±‚æ¬¡æ­£ç¡®æ€§çš„ç»´æŠ¤ä¸Šé¢ä¸´æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFine-tuningèƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨å„æ¨¡æ€ä¸‹çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„é€šç”¨çŸ¥è¯†ã€‚è¯¥åŸºå‡†åŠå…¶ç›¸å…³èµ„æºçš„å‘å¸ƒä¸ºæœªæ¥å¤šæ¨¡æ€æ¨ç†ï¼ˆMultimodal Reasoningï¼‰åœ¨éŸ³ä¹ç†è§£é¢†åŸŸçš„æ·±åº¦ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20697v2",
      "published_date": "2025-11-24 06:40:38 UTC",
      "updated_date": "2026-01-06 16:25:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:17:52.368933+00:00"
    },
    {
      "arxiv_id": "2511.18811v1",
      "title": "Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache",
      "title_zh": "é€šè¿‡è‡ªé€‚åº”å¤šæ ·æ€§ç¼“å­˜ç¼“è§£ HOI æ£€æµ‹ä¸­çš„é•¿å°¾åå·®",
      "authors": [
        "Yuqiu Jiang",
        "Xiaozhen Qiao",
        "Tianyu Mei",
        "Haojian Huang",
        "Yifan Chen",
        "Ye Zheng",
        "Zhe Sun"
      ],
      "abstract": "Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\\% mAP gain on rare categories and +4.39\\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººæœºäº¤äº’(Human-Object Interaction, HOI)æ£€æµ‹ä¸­ç°æœ‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„æ–¹æ³•é¢ä¸´çš„é«˜è®¡ç®—å¼€é”€åŠé•¿å°¾åå·®(long-tail bias)é—®é¢˜ï¼Œæå‡ºäº†è‡ªé€‚åº”å¤šæ ·æ€§ç¼“å­˜(Adaptive Diversity Cache, ADC)æ¨¡å—ã€‚ADCä½œä¸ºä¸€ç§æ— éœ€è®­ç»ƒä¸”å³æ’å³ç”¨çš„æœºåˆ¶ï¼Œé€šè¿‡åœ¨æ¨ç†é˜¶æ®µæ„å»ºç±»åˆ«ç‰¹å®šçš„ç¼“å­˜ï¼Œç§¯ç´¯é«˜ç½®ä¿¡åº¦ä¸”å¤šæ ·åŒ–çš„ç‰¹å¾è¡¨ç¤ºã€‚è¯¥æ–¹æ³•ç»“åˆé¢‘ç‡æ„ŸçŸ¥çš„ç¼“å­˜è‡ªé€‚åº”ç­–ç•¥ï¼Œæ—¨åœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹ï¼Œå®ç°å¯¹ç¨€æœ‰ç±»åˆ«çš„é²æ£’é¢„æµ‹æ ¡å‡†ã€‚åœ¨HICO-DETå’ŒV-COCOæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒADCæ˜¾è‘—æå‡äº†ç°æœ‰HOIæ£€æµ‹å™¨çš„æ€§èƒ½ï¼Œåœ¨ç¨€æœ‰ç±»åˆ«ä¸Šè·å¾—äº†+8.57%çš„mAPå¢ç›Šï¼Œåœ¨å…¨æ•°æ®é›†ä¸Šæå‡äº†+4.39%ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†ADCåœ¨ç¼“è§£é•¿å°¾åå·®æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¹Ÿä¿æŒäº†æ¨¡å‹åœ¨å¤šæ ·åŒ–ç°å®åœºæ™¯ä¸­çš„æ•´ä½“æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18811v1",
      "published_date": "2025-11-24 06:30:08 UTC",
      "updated_date": "2025-11-24 06:30:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:17:56.875891+00:00"
    },
    {
      "arxiv_id": "2511.18808v2",
      "title": "HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations",
      "title_zh": "HyperbolicRAGï¼šåˆ©ç”¨åŒæ›²è¡¨ç¤ºå¢å¼ºæ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Linxiao Cao",
        "Ruitao Wang",
        "Jindong Li",
        "Zhipeng Zhou",
        "Menglin Yang"
      ],
      "abstract": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HyperbolicRAGï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†åŒæ›²å‡ ä½•(Hyperbolic Geometry)æ•´åˆè¿›å›¾å¢å¼ºæ£€ç´¢å¢å¼ºç”Ÿæˆ(Graph-based RAG)çš„æ£€ç´¢æ¡†æ¶ï¼Œä»¥è§£å†³ä¼ ç»Ÿæ¬§å‡ é‡Œå¾—åµŒå…¥(Euclidean Embeddings)éš¾ä»¥æœ‰æ•ˆæ•æ‰çŸ¥è¯†å›¾è°±ä¸­å±‚æ¬¡æ·±åº¦å’ŒæŠ½è±¡å…³ç³»çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸‰é¡¹å…³é”®è®¾è®¡ï¼šä¸€æ˜¯æ·±åº¦æ„ŸçŸ¥è¡¨å¾å­¦ä¹ å™¨ï¼Œåœ¨åºåŠ è±æµå½¢(Poincare Manifold)ä¸­åµŒå…¥èŠ‚ç‚¹ä»¥å¯¹é½è¯­ä¹‰ç›¸ä¼¼åº¦ä¸å±‚æ¬¡åŒ…å«å…³ç³»ï¼›äºŒæ˜¯æ— ç›‘ç£å¯¹æ¯”æ­£åˆ™åŒ–ï¼Œç”¨ä»¥ç¡®ä¿è·¨æŠ½è±¡å±‚çº§çš„å‡ ä½•ä¸€è‡´æ€§ï¼›ä¸‰æ˜¯äº’æ’åºèåˆæœºåˆ¶ï¼Œåœ¨æ¨ç†æ—¶å…±åŒåˆ©ç”¨æ¬§å‡ é‡Œå¾—ä¸åŒæ›²ç©ºé—´çš„æ£€ç´¢ä¿¡å·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHyperbolicRAG åœ¨å¤šä¸ªé—®ç­”(QA)åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºæ ‡å‡† RAG åŠç°æœ‰çš„å›¾å¢å¼ºæ¨¡å‹ã€‚é€šè¿‡èåˆåŒæ›²å‡ ä½•ï¼Œè¯¥æ–¹æ³•æˆåŠŸæ•æ‰äº†ç»†ç²’åº¦è¯­ä¹‰å’Œå…¨å±€å±‚æ¬¡ç»“æ„ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚çŸ¥è¯†ç¯å¢ƒä¸‹çš„æ£€ç´¢ç²¾åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.18808v2",
      "published_date": "2025-11-24 06:27:58 UTC",
      "updated_date": "2025-11-25 03:43:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:18:05.456568+00:00"
    },
    {
      "arxiv_id": "2512.00065v2",
      "title": "Satellite to Street : Disaster Impact Estimator",
      "title_zh": "Satellite to Streetï¼šç¾å®³å½±å“è¯„ä¼°å™¨",
      "authors": [
        "Sreesritha Sai",
        "Sai Venkata Suma Sreeja",
        "Deepthi",
        "Nikhil"
      ],
      "abstract": "Accurate assessment of post-disaster damage is essential for prioritizing emergency response, yet current practices rely heavily on manual interpretation of satellite imagery.This approach is time-consuming, subjective, and difficult to scale during large-area disasters. Although recent deep-learning models for semantic segmentation and change detection have improved automation, many of them still struggle to capture subtle structural variations and often perform poorly when dealing with highly imbalanced datasets, where undamaged buildings dominate. This thesis introduces Satellite-to-Street:Disaster Impact Estimator, a deep-learning framework that produces detailed, pixel-level damage maps by analyzing pre and post-disaster satellite images together. The model is built on a modified dual-input U-Net architecture that strengthens feature fusion between both images, allowing it to detect not only small, localized changes but also broader contextual patterns across the scene. To address the imbalance between damage categories, a class-aware weighted loss function is used, which helps the model better recognize major and destroyed structures. A consistent preprocessing pipeline is employed to align image pairs, standardize resolutions, and prepare the dataset for training. Experiments conducted on publicly available disaster datasets show that the proposed framework achieves better classification of damaged regions compared to conventional segmentation networks.The generated damage maps provide faster and objective method for analyzing disaster impact, working alongside expert judgment rather than replacing it. In addition to identifying which areas are damaged, the system is capable of distinguishing different levels of severity, ranging from slight impact to complete destruction. This provides a more detailed and practical understanding of how the disaster has affected each region.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿç¾åæŸå®³è¯„ä¼°ä¾èµ–äººå·¥è§£è¯‘å«æ˜Ÿå›¾åƒå¯¼è‡´çš„æ—¶æ•ˆæ€§å·®ã€ä¸»è§‚æ€§å¼ºä¸”éš¾ä»¥å¤§è§„æ¨¡æ‰©å±•çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Satellite-to-Street: Disaster Impact Estimator çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆåƒç´ çº§çš„è¯¦ç»†æŸå®³å›¾ã€‚è¯¥æ¡†æ¶åŸºäºæ”¹è¿›çš„åŒè¾“å…¥ U-Net æ¶æ„ï¼Œé€šè¿‡å¼ºåŒ–ç¾å‰å’Œç¾åå«æ˜Ÿå›¾åƒä¹‹é—´çš„ç‰¹å¾èåˆ(feature fusion)ï¼Œèƒ½å¤Ÿç²¾å‡†æ•æ‰å±€éƒ¨çš„ç»†å¾®ç»“æ„å˜åŒ–åŠå®è§‚èƒŒæ™¯æ¨¡å¼ã€‚ä¸ºäº†åº”å¯¹æ•°æ®é›†ä¸­å—æŸæ ·æœ¬æ¯”ä¾‹ä¸¥é‡å¤±è¡¡çš„æŒ‘æˆ˜ï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ç±»åˆ«æ„ŸçŸ¥åŠ æƒæŸå¤±å‡½æ•°(class-aware weighted loss function)ï¼Œæ˜¾è‘—æå‡äº†å¯¹é‡åº¦æŸååŠæ‘§æ¯å»ºç­‘çš„è¯†åˆ«ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å…¬å¼€ç¾å®³æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿè¯­ä¹‰åˆ†å‰²(semantic segmentation)ç½‘ç»œï¼Œä¸ä»…èƒ½æä¾›å¿«é€Ÿä¸”å®¢è§‚çš„æŸå®³åˆ†æï¼Œè¿˜èƒ½å‡†ç¡®åŒºåˆ†ä»è½»å¾®å½±å“åˆ°å®Œå…¨æ‘§æ¯çš„ä¸åŒä¸¥é‡ç¨‹åº¦ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æä¾›ç²¾ç»†åŒ–çš„å—ç¾æ•°æ®ï¼Œä¸ºæé«˜åº”æ€¥å“åº”æ•ˆç‡å’Œè¾…åŠ©ä¸“å®¶å†³ç­–æä¾›äº†å®ç”¨çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages,4 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.00065v2",
      "published_date": "2025-11-24 06:20:40 UTC",
      "updated_date": "2026-01-02 04:09:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:18:04.466540+00:00"
    },
    {
      "arxiv_id": "2511.18793v1",
      "title": "NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations",
      "title_zh": "NEZHAï¼šé¢å‘ç”Ÿæˆå¼æ¨èçš„é›¶æŸè€—è¶…é«˜é€Ÿè§£ç æ¶æ„",
      "authors": [
        "Yejing Wang",
        "Shengyu Zhou",
        "Jinyu Lu",
        "Ziwei Liu",
        "Langming Liu",
        "Maolin Wang",
        "Wenlin Zhang",
        "Feng Li",
        "Wenbo Su",
        "Pengjie Wang",
        "Jian Xu",
        "Xiangyu Zhao"
      ],
      "abstract": "Generative Recommendation (GR), powered by Large Language Models (LLMs), represents a promising new paradigm for industrial recommender systems. However, their practical application is severely hindered by high inference latency, which makes them infeasible for high-throughput, real-time services and limits their overall business impact. While Speculative Decoding (SD) has been proposed to accelerate the autoregressive generation process, existing implementations introduce new bottlenecks: they typically require separate draft models and model-based verifiers, requiring additional training and increasing the latency overhead. In this paper, we address these challenges with NEZHA, a novel architecture that achieves hyperspeed decoding for GR systems without sacrificing recommendation quality. Specifically, NEZHA integrates a nimble autoregressive draft head directly into the primary model, enabling efficient self-drafting. This design, combined with a specialized input prompt structure, preserves the integrity of sequence-to-sequence generation. Furthermore, to tackle the critical problem of hallucination, a major source of performance degradation, we introduce an efficient, model-free verifier based on a hash set. We demonstrate the effectiveness of NEZHA through extensive experiments on public datasets and have successfully deployed the system on Taobao since October 2025, driving the billion-level advertising revenue and serving hundreds of millions of daily active users.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆå¼æ¨è(Generative Recommendation)åœ¨å®é™…å·¥ä¸šåº”ç”¨ä¸­é¢ä¸´çš„é«˜æ¨ç†å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†NEZHAæ¶æ„ï¼Œæ—¨åœ¨ä¸ç‰ºç‰²æ¨èè´¨é‡çš„å‰æä¸‹å®ç°æé€Ÿè§£ç ã€‚NEZHAé€šè¿‡åœ¨ä¸»æ¨¡å‹ä¸­ç›´æ¥é›†æˆè½»é‡çº§è‡ªå›å½’è‰æ¡ˆå¤´(draft head)æ¥å®ç°é«˜æ•ˆçš„è‡ªè‰æ¡ˆç”Ÿæˆ(self-drafting)ï¼Œå¹¶é…åˆä¸“é—¨è®¾è®¡çš„è¾“å…¥æç¤ºç»“æ„ä»¥ç»´æŒåºåˆ—ç”Ÿæˆçš„å®Œæ•´æ€§ã€‚é’ˆå¯¹ç”Ÿæˆå¼ç³»ç»Ÿå¸¸è§çš„å¹»è§‰(hallucination)é—®é¢˜ï¼Œè¯¥æ¶æ„å¼•å…¥äº†ä¸€ä¸ªåŸºäºå“ˆå¸Œè¡¨(hash set)çš„é«˜æ•ˆæ— æ¨¡å‹éªŒè¯å™¨ï¼Œåœ¨ä¿è¯å‡†ç¡®æ€§çš„åŒæ—¶é™ä½äº†éªŒè¯å¼€é”€ã€‚ç›®å‰ï¼ŒNEZHAå·²äº2025å¹´10æœˆæˆåŠŸéƒ¨ç½²äºæ·˜å®å¹³å°ï¼Œåœ¨æœåŠ¡æ•°äº¿æ—¥æ´»è·ƒç”¨æˆ·çš„åŒæ—¶é©±åŠ¨äº†åäº¿çº§çš„å¹¿å‘Šæ”¶å…¥å¢é•¿ï¼Œè¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡å®æ—¶æ¨èç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ä¸å•†ä¸šä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18793v1",
      "published_date": "2025-11-24 05:53:46 UTC",
      "updated_date": "2025-11-24 05:53:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:18:06.977380+00:00"
    },
    {
      "arxiv_id": "2511.20696v2",
      "title": "Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding",
      "title_zh": "é¢å‘è·¨è¢«è¯• EEG è§£ç çš„åŸå‹å¼•å¯¼æ— æ ·æœ¬æŒç»­å­¦ä¹ ",
      "authors": [
        "Dan Li",
        "Hye-Bin Shin",
        "Yeon-Woo Choi"
      ],
      "abstract": "Due to the significant variability in electroencephalo-gram (EEG) signals across individuals, knowledge acquired from previous subjects is often overwritten as new subjects are introduced in continual EEG decoding tasks. Existing methods mainly rely on storing historical data from seen subjects as replay buffers to mitigate forgetting, which is impractical under privacy or memory constraints. To address this issue, we propose a Prototype-guided Non-Exemplar Continual Learning (ProNECL) framework that preserves prior knowledge without accessing historical EEG samples. ProNECL summarizes subject-specific discriminative representations into class-level prototypes and incrementally aligns new subject representations with a global prototype memory through prototype-based feature regulariza-tion and cross-subject alignment. Experiments on the BCI Com-petition IV 2a and 2b datasets demonstrate that ProNECL effec-tively balances knowledge retention and adaptability, achieving superior performance in cross-subject continual EEG decoding tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨è¢«è¯•(cross-subject)è„‘ç”µå›¾(EEG)è§£ç ä¸­å› ä¸ªä½“å·®å¼‚å¯¼è‡´çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºProNECLï¼ˆPrototype-guided Non-Exemplar Continual Learningï¼‰çš„æŒç»­å­¦ä¹ æ¡†æ¶ã€‚ä¸ºäº†åœ¨æ»¡è¶³éšç§å’Œå­˜å‚¨é™åˆ¶çš„å‰æä¸‹ä¿ç•™å…ˆéªŒçŸ¥è¯†ï¼Œè¯¥æ¡†æ¶æ”¾å¼ƒäº†å­˜å‚¨å†å²æ ·æœ¬çš„ä¼ ç»Ÿå›æ”¾æœºåˆ¶ï¼Œè½¬è€Œå°†ç‰¹å®šè¢«è¯•çš„åˆ¤åˆ«æ€§è¡¨ç¤ºæ€»ç»“ä¸ºç±»çº§åŸå‹(class-level prototypes)ã€‚é€šè¿‡åŸºäºåŸå‹çš„ç‰¹å¾æ­£åˆ™åŒ–(prototype-based feature regularization)å’Œè·¨è¢«è¯•å¯¹é½(cross-subject alignment)æŠ€æœ¯ï¼ŒProNECLå®ç°äº†æ–°è¢«è¯•è¡¨ç¤ºä¸å…¨å±€åŸå‹è®°å¿†çš„å¢é‡å¯¹é½ã€‚åœ¨BCI Competition IV 2aå’Œ2bæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ²¡æœ‰å†å²æ•°æ®è®¿é—®æƒé™çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½æœ‰æ•ˆå¹³è¡¡çŸ¥è¯†ä¿ç•™ä¸æ–°ç¯å¢ƒé€‚åº”æ€§ã€‚æœ€ç»ˆï¼ŒProNECLåœ¨è·¨è¢«è¯•æŒç»­EEGè§£ç ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages, 2 figures, 14th IEEE International Winter Conference on Brain-Computer Interface Conference 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.20696v2",
      "published_date": "2025-11-24 05:42:03 UTC",
      "updated_date": "2026-01-15 07:05:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:19:17.985966+00:00"
    },
    {
      "arxiv_id": "2511.18781v1",
      "title": "A Novel Dual-Stream Framework for dMRI Tractography Streamline Classification with Joint dMRI and fMRI Data",
      "title_zh": "èåˆ dMRI ä¸ fMRI æ•°æ®çš„ dMRI çº¤ç»´è¿½è¸ªæµçº¿åˆ†ç±»æ–°å‹åŒæµæ¡†æ¶",
      "authors": [
        "Haotian Yan",
        "Bocheng Guo",
        "Jianzhong He",
        "Nir A. Sochen",
        "Ofer Pasternak",
        "Lauren J O'Donnell",
        "Fan Zhang"
      ],
      "abstract": "Streamline classification is essential to identify anatomically meaningful white matter tracts from diffusion MRI (dMRI) tractography. However, current streamline classification methods rely primarily on the geometric features of the streamline trajectory, failing to distinguish between functionally distinct fiber tracts with similar pathways. To address this, we introduce a novel dual-stream streamline classification framework that jointly analyzes dMRI and functional MRI (fMRI) data to enhance the functional coherence of tract parcellation. We design a novel network that performs streamline classification using a pretrained backbone model for full streamline trajectories, while augmenting with an auxiliary network that processes fMRI signals from fiber endpoint regions. We demonstrate our method by parcellating the corticospinal tract (CST) into its four somatotopic subdivisions. Experimental results from ablation studies and comparisons with state-of-the-art methods demonstrate our approach's superior performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹åŒæµæµçº¿åˆ†ç±»æ¡†æ¶(dual-stream framework)ï¼Œæ—¨åœ¨è§£å†³å¼¥æ•£æ ¸ç£å…±æŒ¯(dMRI)çº¤ç»´è¿½è¸ªä¸­ä»…ä¾èµ–å‡ ä½•è½¨è¿¹ç‰¹å¾è€Œéš¾ä»¥åŒºåˆ†åŠŸèƒ½å·®å¼‚çº¤ç»´æŸçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è”åˆåˆ†ædMRIå’ŒåŠŸèƒ½æ ¸ç£å…±æŒ¯(fMRI)æ•°æ®æ¥å¢å¼ºçº¤ç»´æŸåˆ’åˆ†çš„åŠŸèƒ½ä¸€è‡´æ€§ï¼Œå…¶æ ¸å¿ƒè®¾è®¡åŒ…å«ä¸€ä¸ªå¤„ç†å®Œæ•´æµçº¿è½¨è¿¹çš„é¢„è®­ç»ƒä¸»å¹²ç½‘ç»œï¼Œä»¥åŠä¸€ä¸ªä¸“é—¨å¤„ç†çº¤ç»´ç«¯ç‚¹åŒºåŸŸfMRIä¿¡å·çš„è¾…åŠ©ç½‘ç»œã€‚ç ”ç©¶é€šè¿‡å°†çš®è´¨è„Šé«“æŸ(CST)åˆ’åˆ†ä¸ºå››ä¸ªèº¯ä½“å®šä½ç»†åˆ†åŒºåŸŸ(somatotopic subdivisions)å¯¹æ¨¡å‹è¿›è¡Œäº†éªŒè¯ã€‚æ¶ˆèå®éªŒå’Œå¯¹æ¯”ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†ç±»æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿™ä¸€æ–¹æ¡ˆé€šè¿‡æ•´åˆç»“æ„ä¸åŠŸèƒ½ä¿¡æ¯ï¼Œä¸ºæ›´ç²¾ç¡®çš„è„‘ç½‘ç»œè§£å‰–ä¸åŠŸèƒ½è§£ææä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to ISBI 2026, 7 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.18781v1",
      "published_date": "2025-11-24 05:31:47 UTC",
      "updated_date": "2025-11-24 05:31:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:18:16.058838+00:00"
    },
    {
      "arxiv_id": "2511.18780v3",
      "title": "ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection",
      "title_zh": "ConceptGuardï¼šåŸºäºå¤šæ¨¡æ€é£é™©æ£€æµ‹çš„æ–‡æœ¬ä¸å›¾åƒç”Ÿæˆè§†é¢‘ä¸»åŠ¨é˜²å¾¡",
      "authors": [
        "Ruize Ma",
        "Minghong Cai",
        "Yilei Jiang",
        "Jiaming Han",
        "Yi Feng",
        "Yingshui Tan",
        "Xiaoyong Zhu",
        "Bo Zhang",
        "Bo Zheng",
        "Xiangyu Yue"
      ],
      "abstract": "Recent progress in video generative models has enabled the creation of high-quality videos from multimodal prompts that combine text and images. While these systems offer enhanced controllability, they also introduce new safety risks, as harmful content can emerge from individual modalities or their interaction. Existing safety methods are often text-only, require prior knowledge of the risk category, or operate as post-generation auditors, struggling to proactively mitigate such compositional, multimodal risks. To address this challenge, we present ConceptGuard, a unified safeguard framework for proactively detecting and mitigating unsafe semantics in multimodal video generation. ConceptGuard operates in two stages: First, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; Second, a semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning. To support the development and rigorous evaluation of this framework, we introduce two novel benchmarks: ConceptRisk, a large-scale dataset for training on multimodal risks, and T2VSafetyBench-TI2V, the first benchmark adapted from T2VSafetyBench for the Text-and-Image-to-Video (TI2V) safety setting. Comprehensive experiments on both benchmarks show that ConceptGuard consistently outperforms existing baselines, achieving state-of-the-art results in both risk detection and safe video generation. Our code is available at https://github.com/Ruize-Ma/ConceptGuard.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Text-and-Image-to-Video (TI2V) ç”Ÿæˆä¸­ç”±å¤šæ¨¡æ€äº¤äº’å¼•å‘çš„å®‰å…¨é£é™©ï¼Œæå‡ºäº†ä¸»åŠ¨å¼é˜²å¾¡æ¡†æ¶ ConceptGuardï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å®‰å…¨æ–¹æ³•åœ¨å¤„ç†ç»„åˆå¼ã€å¤šæ¨¡æ€é£é™©åŠäº‹åå®¡æ ¸æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œé¦–å…ˆé€šè¿‡å¯¹æ¯”æ£€æµ‹æ¨¡å— (contrastive detection module) å°†èåˆçš„å›¾æ–‡è¾“å…¥æŠ•å½±è‡³ç»“æ„åŒ–æ¦‚å¿µç©ºé—´ä»¥è¯†åˆ«æ½œåœ¨é£é™©ï¼Œéšååˆ©ç”¨è¯­ä¹‰æŠ‘åˆ¶æœºåˆ¶ (semantic suppression mechanism) å¹²é¢„å¤šæ¨¡æ€æ¡ä»¶ä½œç”¨ï¼Œå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹åç¦»ä¸å®‰å…¨æ¦‚å¿µã€‚ä¸ºäº†æ”¯æŒè¯¥é¢†åŸŸçš„å¼€å‘ä¸è¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†å¤§è§„æ¨¡é£é™©è®­ç»ƒæ•°æ®é›† ConceptRisk ä»¥åŠé¦–ä¸ªé’ˆå¯¹ TI2V å®‰å…¨è®¾ç½®çš„åŸºå‡† T2VSafetyBench-TI2Vã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒConceptGuard åœ¨é£é™©æ£€æµ‹å’Œå®‰å…¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œè¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚è¯¥é¡¹å·¥ä½œä¸ºå®ç°æ›´å®‰å…¨ã€å¯æ§çš„å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆæä¾›äº†ç³»ç»ŸåŒ–çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18780v3",
      "published_date": "2025-11-24 05:27:05 UTC",
      "updated_date": "2025-11-26 06:26:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:19:32.181182+00:00"
    },
    {
      "arxiv_id": "2511.18775v1",
      "title": "Rethinking Garment Conditioning in Diffusion-based Virtual Try-On",
      "title_zh": "é‡æ–°æ€è€ƒåŸºäºæ‰©æ•£æ¨¡å‹çš„è™šæ‹Ÿè¯•ç©¿ä¸­çš„æœè£…æ¡ä»¶åŒ–",
      "authors": [
        "Kihyun Na",
        "Jinyoung Choi",
        "Injung Kim"
      ],
      "abstract": "Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ‰©æ•£æ¨¡å‹çš„è™šæ‹Ÿè¯•ç©¿(Virtual Try-On, VTON)ä»»åŠ¡ä¸­ï¼ŒDual UNetæ¶æ„è™½ç„¶ä¿çœŸåº¦é«˜ä½†è®¡ç®—å’Œå†…å­˜å¼€é”€å·¨å¤§çš„é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä½œè€…é€šè¿‡å¯è§†åŒ–å’Œç†è®ºåˆ†æï¼Œæå‡ºäº†å…³äºæ¡ä»¶å»å™ªè¿‡ç¨‹ä¸­ä¸Šä¸‹æ–‡ç‰¹å¾å­¦ä¹ çš„ä¸‰é¡¹å‡è®¾ï¼Œå¹¶æ®æ­¤å¼€å‘äº†é«˜æ•ˆçš„Single UNetæ¨¡å‹Re-CatVTONã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸“é—¨é’ˆå¯¹ç©ºé—´æ‹¼æ¥æ¡ä»¶çš„æ”¹è¿›å‹Classifier-Free Guidanceç­–ç•¥ï¼Œå¹¶é€šè¿‡ç›´æ¥æ³¨å…¥Ground-Truth Garment Latentæ¥é˜²æ­¢é¢„æµ‹è¯¯å·®çš„ç´¯ç§¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRe-CatVTONåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶å‰ä½œCatVTONï¼Œä¸”åœ¨è®¡ç®—èµ„æºéœ€æ±‚ä¸Šä½äºé«˜æ€§èƒ½çš„Leffaæ¨¡å‹ã€‚è¯¥ç ”ç©¶åœ¨FIDã€KIDå’ŒLPIPSç­‰æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œä¸ºSingle UNet VTONæ¨¡å‹ç¡®ç«‹äº†æ–°çš„æ•ˆç‡ä¸æ€§èƒ½å¹³è¡¡åŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages (including references and supplementary material), 10 figures, 7 tables. Code and pretrained models will be released",
      "pdf_url": "https://arxiv.org/pdf/2511.18775v1",
      "published_date": "2025-11-24 05:19:44 UTC",
      "updated_date": "2025-11-24 05:19:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:19:36.387489+00:00"
    },
    {
      "arxiv_id": "2511.18772v1",
      "title": "Re-Key-Free, Risky-Free: Adaptable Model Usage Control",
      "title_zh": "å…é‡è®¾å¯†é’¥ï¼Œæ— å®‰å…¨é£é™©ï¼šå¯è‡ªé€‚åº”çš„æ¨¡å‹ä½¿ç”¨æ§åˆ¶",
      "authors": [
        "Zihan Wang",
        "Zhongkui Ma",
        "Xinguo Feng",
        "Chuan Yan",
        "Dongge Liu",
        "Ruoxi Sun",
        "Derui Wang",
        "Minhui Xue",
        "Guangdong Bai"
      ],
      "abstract": "Deep neural networks (DNNs) have become valuable intellectual property of model owners, due to the substantial resources required for their development. To protect these assets in the deployed environment, recent research has proposed model usage control mechanisms to ensure models cannot be used without proper authorization. These methods typically lock the utility of the model by embedding an access key into its parameters. However, they often assume static deployment, and largely fail to withstand continual post-deployment model updates, such as fine-tuning or task-specific adaptation. In this paper, we propose ADALOC, to endow key-based model usage control with adaptability during model evolution. It strategically selects a subset of weights as an intrinsic access key, which enables all model updates to be confined to this key throughout the evolution lifecycle. ADALOC enables using the access key to restore the keyed model to the latest authorized states without redistributing the entire network (i.e., adaptation), and frees the model owner from full re-keying after each model update (i.e., lock preservation). We establish a formal foundation to underpin ADALOC, providing crucial bounds such as the errors introduced by updates restricted to the access key. Experiments on standard benchmarks, such as CIFAR-100, Caltech-256, and Flowers-102, and modern architectures, including ResNet, DenseNet, and ConvNeXt, demonstrate that ADALOC achieves high accuracy under significant updates while retaining robust protections. Specifically, authorized usages consistently achieve strong task-specific performance, while unauthorized usage accuracy drops to near-random guessing levels (e.g., 1.01% on CIFAR-100), compared to up to 87.01% without ADALOC. This shows that ADALOC can offer a practical solution for adaptive and protected DNN deployment in evolving real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ADALOCï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä¸ºæ·±åº¦ç¥ç»ç½‘ç»œ (DNNs) æä¾›å¯é€‚åº”æ€§æ¨¡å‹ä½¿ç”¨æ§åˆ¶çš„æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰é”å®šæœºåˆ¶åœ¨æ¨¡å‹éƒ¨ç½²åè¿›è¡Œå¾®è°ƒæˆ–ä»»åŠ¡ç‰¹å®šé€‚é…æ—¶éš¾ä»¥æŒç»­ä¿æŠ¤çš„é—®é¢˜ã€‚ADALOC é€šè¿‡ç­–ç•¥æ€§åœ°é€‰æ‹©ä¸€éƒ¨åˆ†æƒé‡ä½œä¸ºå†…åœ¨è®¿é—®å¯†é’¥ (intrinsic access key)ï¼Œå°†æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…çš„æ¨¡å‹æ›´æ–°é™åˆ¶åœ¨è¯¥å¯†é’¥èŒƒå›´å†…ï¼Œä»è€Œå®ç°äº†æ— éœ€é‡æ–°åŠ å¯† (re-keying) çš„åŠ¨æ€é€‚é…ã€‚è¯¥æ¡†æ¶å…è®¸æˆæƒç”¨æˆ·åœ¨ä¸é‡æ–°åˆ†å‘å®Œæ•´ç½‘ç»œçš„æƒ…å†µä¸‹å°†æ¨¡å‹æ¢å¤è‡³æœ€æ–°çŠ¶æ€ï¼ŒåŒæ—¶ç¡®ä¿äº†æ¨¡å‹é”å®šåœ¨æ›´æ–°è¿‡ç¨‹ä¸­çš„æŒä¹…æ€§ã€‚ç ”ç©¶é€šè¿‡å½¢å¼åŒ–è¯æ˜ç¡®ç«‹äº† ADALOC çš„ç†è®ºåŸºç¡€ï¼Œå¹¶åœ¨ CIFAR-100ã€Caltech-256 ç­‰åŸºå‡†æ•°æ®é›†ä»¥åŠ ResNetã€ConvNeXt ç­‰ç°ä»£æ¶æ„ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆæƒç”¨æˆ·èƒ½ä¿æŒæé«˜çš„ä»»åŠ¡æ€§èƒ½ï¼Œè€Œæœªæˆæƒç”¨æˆ·çš„å‡†ç¡®ç‡åˆ™é™è‡³æ¥è¿‘éšæœºçŒœæµ‹çš„æ°´å¹³ï¼ˆå¦‚åœ¨ CIFAR-100 ä¸Šé™è‡³ 1.01%ï¼‰ï¼Œæœ‰æ•ˆè¯æ˜äº† ADALOC åœ¨æ¼”è¿›çš„ç°å®åº”ç”¨åœºæ™¯ä¸­ä¿æŠ¤æ¨¡å‹çŸ¥è¯†äº§æƒçš„å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18772v1",
      "published_date": "2025-11-24 05:13:45 UTC",
      "updated_date": "2025-11-24 05:13:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:19:43.561033+00:00"
    },
    {
      "arxiv_id": "2511.18766v1",
      "title": "Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment",
      "title_zh": "åŸºäºæ¸è¿›å¼å•åº”æ€§å¼•å¯¼å¯¹é½çš„æ— ç›‘ç£å¤šè§†è§’è§†è§‰å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Xintao Chen",
        "Xiaohao Xu",
        "Bozhong Zheng",
        "Yun Liu",
        "Yingna Wu"
      ],
      "abstract": "Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šè§†è§’å›¾åƒä¸‹çš„æ— ç›‘ç£è§†è§‰å¼‚å¸¸æ£€æµ‹(Unsupervised Visual Anomaly Detection)ä¸­éš¾ä»¥åŒºåˆ†çœŸå®ç¼ºé™·ä¸è§†è§’å˜æ¢å·®å¼‚çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ViewSense-AD (VSAD)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜¾å¼å»ºæ¨¡è·¨è§†è§’çš„å‡ ä½•ä¸€è‡´æ€§ï¼Œåˆ©ç”¨å¤šè§†è§’å¯¹é½æ¨¡å—(Multi-View Alignment Module, MVAM)åŠå•åº”æ€§(Homography)å˜æ¢æ¥å¯¹é½ç›¸é‚»è§†è§’çš„ç‰¹å¾åŒºåŸŸã€‚VSADå°†MVAMé›†æˆåˆ°è§†è§’å¯¹é½æ½œåœ¨æ‰©æ•£æ¨¡å‹(View-Align Latent Diffusion Model, VALDM)ä¸­ï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­å®ç°ä»ç²—åˆ°ç»†çš„å¤šé˜¶æ®µæ¸è¿›å¼å¯¹é½ï¼Œä»¥æ„å»ºå¯¹ç‰©ä½“è¡¨é¢çš„æ•´ä½“è®¤çŸ¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†èåˆç²¾ç‚¼æ¨¡å—(Fusion Refiner Module, FRM)ä»¥å¢å¼ºç‰¹å¾çš„å…¨å±€ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡å°†å¤šå±‚çº§ç‰¹å¾ä¸æ­£å¸¸åŸå‹è®°å¿†åº“(Memory Bank)å¯¹æ¯”æ¥è¯†åˆ«å¼‚å¸¸ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒVSADåœ¨RealIADå’ŒMANTAæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨åƒç´ ã€è§†è§’å’Œæ ·æœ¬å±‚çº§å‡è¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³(State-of-the-art)ï¼Œå±•ç¤ºäº†å…¶å¯¹å¤§è§†è§’åç§»å’Œå¤æ‚çº¹ç†çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18766v1",
      "published_date": "2025-11-24 05:01:16 UTC",
      "updated_date": "2025-11-24 05:01:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:19:42.580011+00:00"
    },
    {
      "arxiv_id": "2511.18760v1",
      "title": "HERMES: Towards Efficient and Verifiable Mathematical Reasoning in LLMs",
      "title_zh": "HERMESï¼šè¿ˆå‘å¤§è¯­è¨€æ¨¡å‹ä¸­é«˜æ•ˆä¸”å¯éªŒè¯çš„æ•°å­¦æ¨ç†",
      "authors": [
        "Azim Ospanov",
        "Zijin Feng",
        "Jiacheng Sun",
        "Haoli Bai",
        "Xin Shen",
        "Farzan Farnia"
      ],
      "abstract": "Informal mathematics has been central to modern large language model (LLM) reasoning, offering flexibility and enabling efficient construction of arguments. However, purely informal reasoning is prone to logical gaps and subtle errors that are difficult to detect and correct. In contrast, formal theorem proving provides rigorous, verifiable mathematical reasoning, where each inference step is checked by a trusted compiler in systems such as Lean, but lacks the exploratory freedom of informal problem solving. This mismatch leaves current LLM-based math agents without a principled way to combine the strengths of both paradigms. In this work, we introduce Hermes, the first tool-assisted agent that explicitly interleaves informal reasoning with formally verified proof steps in Lean. The framework performs intermediate formal checking to prevent reasoning drift and employs a memory module that maintains proof continuity across long, multi-step reasoning chains, enabling both exploration and verification within a single workflow. We evaluate Hermes on four challenging mathematical reasoning benchmarks using LLMs of varying parameter scales, from small models to state-of-the-art systems. Across all settings, Hermes reliably improves the reasoning accuracy of base models while substantially reducing token usage and computational cost compared to reward-based approaches. On difficult datasets such as AIME'25, Hermes achieves up to a 67% accuracy improvement while using 80% fewer total inference FLOPs. The implementation and codebase are publicly available at https://github.com/aziksh-ospanov/HERMES.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨éæ­£å¼æ•°å­¦æ¨ç†ä¸­å­˜åœ¨çš„é€»è¾‘ç¼ºé™·ä»¥åŠå½¢å¼åŒ–å®šç†è¯æ˜(Formal theorem proving)ç¼ºä¹æ¢ç´¢è‡ªç”±åº¦çš„é—®é¢˜ï¼Œæå‡ºäº†HERMESæ¡†æ¶ã€‚ä½œä¸ºé¦–ä¸ªå°†éæ­£å¼æ¨ç†ä¸åŸºäºLeançš„å½¢å¼åŒ–éªŒè¯æ­¥éª¤æ˜¾å¼äº¤ç»‡çš„å·¥å…·è¾…åŠ©æ™ºèƒ½ä½“ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ‰§è¡Œä¸­é—´å½¢å¼åŒ–æ£€æŸ¥æ¥é˜²æ­¢æ¨ç†åå·®(Reasoning drift)ï¼Œå¹¶åˆ©ç”¨è®°å¿†æ¨¡å—(Memory module)åœ¨é•¿é“¾ã€å¤šæ­¥æ¨ç†ä¸­ç»´æŒè¯æ˜çš„è¿ç»­æ€§ã€‚è¿™ç§è®¾è®¡å…è®¸åœ¨å•ä¸€å·¥ä½œæµä¸­å…¼é¡¾æ¢ç´¢ä¸éªŒè¯ï¼Œæœ‰æ•ˆç»“åˆäº†ä¸¤ç§æ¨ç†èŒƒå¼çš„ä¼˜åŠ¿ã€‚å®éªŒè¡¨æ˜ï¼ŒHERMESåœ¨å››ä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­å‡èƒ½æ˜¾è‘—æå‡ä¸åŒè§„æ¨¡LLMsçš„æ¨ç†å‡†ç¡®ç‡ï¼Œä¸”å…¶Tokenæ¶ˆè€—å’Œè®¡ç®—æˆæœ¬è¿œä½äºåŸºäºå¥–åŠ±çš„æ–¹æ³•(Reward-based approaches)ã€‚ç‰¹åˆ«æ˜¯åœ¨AIME'25æ•°æ®é›†ä¸Šï¼ŒHERMESåœ¨å‡å°‘80%æ¨ç†æµ®ç‚¹è¿ç®—é‡(FLOPs)çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾67%çš„å‡†ç¡®ç‡æå‡ï¼Œä¸ºå®ç°é«˜æ•ˆä¸”å¯éªŒè¯çš„æ•°å­¦æ¨ç†æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.FL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18760v1",
      "published_date": "2025-11-24 04:50:18 UTC",
      "updated_date": "2025-11-24 04:50:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:19:46.358215+00:00"
    },
    {
      "arxiv_id": "2512.00062v1",
      "title": "SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning",
      "title_zh": "SpeedAugï¼šåŸºäºèŠ‚å¥å¢å¼ºç­–ç•¥ä¸å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„ç­–ç•¥åŠ é€Ÿæ–¹æ³•",
      "authors": [
        "Taewook Nam",
        "Sung Ju Hwang"
      ],
      "abstract": "Recent advances in robotic policy learning have enabled complex manipulation in real-world environments, yet the execution speed of these policies often lags behind hardware capabilities due to the cost of collecting faster demonstrations. Existing works on policy acceleration reinterpret action sequence for unseen execution speed, thereby encountering distributional shifts from the original demonstrations. Reinforcement learning is a promising approach that adapts policies for faster execution without additional demonstration, but its unguided exploration is sample inefficient. We propose SpeedAug, an RL-based policy acceleration framework that efficiently adapts pre-trained policies for faster task execution. SpeedAug constructs behavior prior that encompasses diverse tempos of task execution by pre-training a policy on speed-augmented demonstrations. Empirical results on robotic manipulation benchmarks show that RL fine-tuning initialized from this tempo-enriched policy significantly improves the sample efficiency of existing RL and policy acceleration methods while maintaining high success rate.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººç­–ç•¥æ‰§è¡Œé€Ÿåº¦å—é™äºæ¼”ç¤ºæ•°æ®è·å–æˆæœ¬çš„é—®é¢˜ï¼Œæå‡ºäº†SpeedAugæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„ç­–ç•¥åŠ é€Ÿã€‚SpeedAugé¦–å…ˆé€šè¿‡åœ¨ç»è¿‡é€Ÿåº¦å¢å¼ºï¼ˆspeed-augmentedï¼‰çš„æ¼”ç¤ºæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ„å»ºäº†ä¸€ä¸ªæ¶µç›–å¤šç§æ‰§è¡ŒèŠ‚å¥çš„tempo-enriched policyä½œä¸ºè¡Œä¸ºå…ˆéªŒã€‚éšåï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (RL)å¯¹é¢„è®­ç»ƒç­–ç•¥è¿›è¡Œfine-tuningï¼Œä»¥åœ¨ä¸å¢åŠ é¢å¤–æ¼”ç¤ºçš„æƒ…å†µä¸‹ä½¿ç­–ç•¥é€‚åº”æ›´å¿«çš„æ‰§è¡Œé€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºç°æœ‰çš„RLå’Œç­–ç•¥åŠ é€Ÿæ–¹æ³•ï¼ŒSpeedAugåœ¨ä¿æŒé«˜æˆåŠŸç‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸåŠ é€Ÿæ‰‹æ®µé¢ä¸´çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œä¸ºæå‡æœºå™¨äººä»»åŠ¡æ‰§è¡Œæ•ˆç‡æä¾›äº†ç¨³å¥ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00062v1",
      "published_date": "2025-11-24 04:25:47 UTC",
      "updated_date": "2025-11-24 04:25:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:19:44.761951+00:00"
    },
    {
      "arxiv_id": "2511.18746v1",
      "title": "Any4D: Open-Prompt 4D Generation from Natural Language and Images",
      "title_zh": "Any4Dï¼šåŸºäºè‡ªç„¶è¯­è¨€ä¸å›¾åƒçš„å¼€æ”¾æç¤º4Dç”Ÿæˆ",
      "authors": [
        "Hao Li",
        "Qiao Sun"
      ],
      "abstract": "While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \\textit{\"GPT moment\"} in the embodied domain. There is a naive observation: \\textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \\textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \\textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \\textit{2) reduces} learning complexity, \\textit{3) improves} data efficiency in embodied data collection, and \\textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Primitive Embodied World Models (PEWM)ï¼Œæ—¨åœ¨è§£å†³å…·èº«ä¸–ç•Œæ¨¡å‹(embodied world models)åœ¨é•¿æ—¶ç¨‹è§†é¢‘ç”Ÿæˆå’Œå¤§è§„æ¨¡æ•°æ®é‡‡é›†æ–¹é¢çš„ç“¶é¢ˆã€‚åŸºäºåŸå§‹åŠ¨ä½œ(primitive motions)ç©ºé—´æœ‰é™çš„æ´å¯Ÿï¼ŒPEWMå°†è§†é¢‘ç”Ÿæˆé™åˆ¶åœ¨çŸ­æ—¶ç¨‹èŒƒå›´å†…ï¼Œå®ç°äº†è¯­è¨€æ¦‚å¿µä¸åŠ¨ä½œè§†è§‰è¡¨å¾çš„ç»†ç²’åº¦å¯¹é½ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆé™ä½äº†å­¦ä¹ å¤æ‚åº¦ï¼Œå¹¶æ˜¾è‘—æå‡äº†æ•°æ®æ•ˆç‡ä¸æ¨ç†é€Ÿåº¦ã€‚é€šè¿‡é›†æˆæ¨¡å—åŒ–è§†è§‰è¯­è¨€æ¨¡å‹(VLM)è§„åˆ’å™¨å’Œèµ·ç‚¹-ç»ˆç‚¹çƒ­å›¾å¼•å¯¼æœºåˆ¶(SGG)ï¼ŒPEWMè¿›ä¸€æ­¥å®ç°äº†çµæ´»çš„é—­ç¯æ§åˆ¶å’Œå¤æ‚ä»»åŠ¡ä¸‹çš„ç»„åˆæ³›åŒ–ã€‚è¯¥æ¡†æ¶èåˆäº†è§†é¢‘æ¨¡å‹çš„æ—¶ç©ºè§†è§‰å…ˆéªŒä¸VLMçš„è¯­ä¹‰æ„ŸçŸ¥ï¼ŒæˆåŠŸå¼¥åˆäº†åº•å±‚ç‰©ç†äº¤äº’ä¸é«˜å±‚é€»è¾‘æ¨ç†ä¹‹é—´çš„å·®è·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18746v1",
      "published_date": "2025-11-24 04:17:26 UTC",
      "updated_date": "2025-11-24 04:17:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:20:00.181894+00:00"
    },
    {
      "arxiv_id": "2511.18743v1",
      "title": "RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context",
      "title_zh": "RhinoInsightï¼šé€šè¿‡æ¨¡å‹è¡Œä¸ºä¸ä¸Šä¸‹æ–‡æ§åˆ¶æœºåˆ¶æå‡æ·±åº¦ç ”ç©¶",
      "authors": [
        "Yu Lei",
        "Shuzheng Si",
        "Wei Wang",
        "Yifei Wu",
        "Gang Chen",
        "Fanchao Qi",
        "Maosong Sun"
      ],
      "abstract": "Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨æ·±åº¦ç ”ç©¶(Deep Research)ä»»åŠ¡ä¸­å› çº¿æ€§å·¥ä½œæµå¯¼è‡´çš„é”™è¯¯ç´¯ç§¯å’Œä¸Šä¸‹æ–‡è…è´¥é—®é¢˜ï¼Œæå‡ºäº†RhinoInsightæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ— éœ€å‚æ•°æ›´æ–°ï¼Œé€šè¿‡å¼•å…¥ä¸¤å¤§æ§åˆ¶æœºåˆ¶æ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„é²æ£’æ€§ä¸å¯è¿½æº¯æ€§ã€‚Verifiable Checklistæ¨¡å—å°†ç”¨æˆ·éœ€æ±‚è½¬åŒ–ä¸ºå¯éªŒè¯çš„å­ç›®æ ‡ï¼Œå¹¶ç»“åˆæ‰¹åˆ¤(Critic)æœºåˆ¶ç”Ÿæˆåˆ†å±‚å¤§çº²ä»¥ç¡®ä¿è§„åˆ’çš„å¯æ‰§è¡Œæ€§ã€‚Evidence Auditæ¨¡å—åˆ™é€šè¿‡ç»“æ„åŒ–æœç´¢å†…å®¹ã€è¿‡æ»¤å™ªå£°å¹¶å®ç°è¯æ®ä¸å†…å®¹çš„å¼ºç»‘å®šï¼Œæœ‰æ•ˆé™ä½äº†æ¨¡å‹å¹»è§‰(Hallucinations)ã€‚å®éªŒè¯æ˜ï¼ŒRhinoInsightåœ¨æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸Šè¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œå¹¶åœ¨æ·±åº¦æœç´¢(Deep Search)ä»»åŠ¡ä¸­å±•ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18743v1",
      "published_date": "2025-11-24 04:12:41 UTC",
      "updated_date": "2025-11-24 04:12:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:20:03.613780+00:00"
    },
    {
      "arxiv_id": "2511.18742v2",
      "title": "ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion",
      "title_zh": "ProxT2Iï¼šåŸºäºè¿‘ç«¯æ‰©æ•£çš„é«˜æ•ˆå¥–åŠ±å¼•å¯¼æ–‡æœ¬ç”Ÿæˆå›¾åƒ",
      "authors": [
        "Zhenghan Fang",
        "Jian Zheng",
        "Qiaozi Gao",
        "Xiaofeng Gao",
        "Jeremias Sulam"
      ],
      "abstract": "Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProxT2Iï¼Œä¸€ç§åŸºäº backward discretizations çš„é«˜æ•ˆæ–‡æœ¬ç”Ÿæˆå›¾åƒ (T2I) æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ score-based æ¨¡å‹åœ¨å‰å‘ç¦»æ•£åŒ–è¿‡ç¨‹ä¸­å­˜åœ¨çš„é‡‡æ ·ç¼“æ…¢å’Œä¸ç¨³å®šçš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å­¦ä¹ åˆ°çš„æ¡ä»¶æ€§ proximal operators å–ä»£äº†ä¼ ç»Ÿçš„ score functionsï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸ç­–ç•¥ä¼˜åŒ–æŠ€æœ¯ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¥–åŠ±è¿›è¡Œé‡‡æ ·å™¨ä¼˜åŒ–ã€‚ä¸ºäº†æ”¯æŒç ”ç©¶ï¼Œå›¢é˜Ÿæ„å»ºå¹¶å¼€æºäº†åŒ…å« 1500 ä¸‡å¼ é«˜è´¨é‡äººä½“å›¾åƒåŠç»†ç²’åº¦æ ‡æ³¨çš„å¤§è§„æ¨¡æ•°æ®é›† LAION-Face-T2I-15Mã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProxT2I åœ¨æå‡é‡‡æ ·æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—å¢å¼ºäº† human-preference alignmentï¼Œå¹¶ä»¥æ›´å°çš„æ¨¡å‹å°ºå¯¸å’Œæ›´ä½çš„è®¡ç®—æˆæœ¬å®ç°äº†ä¸å½“å‰ state-of-the-art æ¨¡å‹ç›¸å½“çš„ç”Ÿæˆæ•ˆæœã€‚è¯¥ç ”ç©¶ä¸ºäººä½“é¢†åŸŸçš„ text-to-image ç”Ÿæˆæä¾›äº†ä¸€ç§å…¼å…·è½»é‡åŒ–ä¸é«˜æ€§èƒ½çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18742v2",
      "published_date": "2025-11-24 04:10:53 UTC",
      "updated_date": "2025-11-26 19:17:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:19:58.174005+00:00"
    },
    {
      "arxiv_id": "2511.18739v1",
      "title": "A Problem-Oriented Taxonomy of Evaluation Metrics for Time Series Anomaly Detection",
      "title_zh": "é¢å‘é—®é¢˜çš„æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹è¯„ä»·æŒ‡æ ‡åˆ†ç±»ä½“ç³»",
      "authors": [
        "Kaixiang Yang",
        "Jiarong Liu",
        "Yupeng Song",
        "Shuanghua Yang",
        "Yujue Zhou"
      ],
      "abstract": "Time series anomaly detection is widely used in IoT and cyber-physical systems, yet its evaluation remains challenging due to diverse application objectives and heterogeneous metric assumptions. This study introduces a problem-oriented framework that reinterprets existing metrics based on the specific evaluation challenges they are designed to address, rather than their mathematical forms or output structures. We categorize over twenty commonly used metrics into six dimensions: 1) basic accuracy-driven evaluation; 2) timeliness-aware reward mechanisms; 3) tolerance to labeling imprecision; 4) penalties reflecting human-audit cost; 5) robustness against random or inflated scores; and 6) parameter-free comparability for cross-dataset benchmarking. Comprehensive experiments are conducted to examine metric behavior under genuine, random, and oracle detection scenarios. By comparing their resulting score distributions, we quantify each metric's discriminative ability -- its capability to distinguish meaningful detections from random noise. The results show that while most event-level metrics exhibit strong separability, several widely used metrics (e.g., NAB, Point-Adjust) demonstrate limited resistance to random-score inflation. These findings reveal that metric suitability must be inherently task-dependent and aligned with the operational objectives of IoT applications. The proposed framework offers a unified analytical perspective for understanding existing metrics and provides practical guidance for selecting or developing more context-aware, robust, and fair evaluation methodologies for time series anomaly detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹(Time Series Anomaly Detection)è¯„ä»·ä½“ç³»ä¸­å› åº”ç”¨ç›®æ ‡å¤šæ ·åŒ–å’ŒæŒ‡æ ‡å‡è®¾å¼‚æ„åŒ–å¯¼è‡´çš„è¯„ä¼°éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»¥é—®é¢˜ä¸ºå¯¼å‘(problem-oriented)çš„è¯„ä¼°æŒ‡æ ‡åˆ†ç±»æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»åŸºæœ¬å‡†ç¡®æ€§é©±åŠ¨ã€åŠæ—¶æ€§æ„ŸçŸ¥ã€æ ‡ç­¾ä¸ç²¾ç¡®å®¹å¿åº¦ã€å®¡è®¡æˆæœ¬æƒ©ç½šã€éšæœºå¾—åˆ†ç¨³å¥æ€§ä»¥åŠè·¨æ•°æ®é›†æ¯”è¾ƒèƒ½åŠ›ç­‰å…­ä¸ªç»´åº¦ï¼Œå¯¹äºŒåå¤šç§å¸¸ç”¨æŒ‡æ ‡è¿›è¡Œäº†é‡æ–°æ¢³ç†ä¸å®šä¹‰ã€‚é€šè¿‡åœ¨çœŸå®ã€éšæœºå’Œç†æƒ³æ£€æµ‹åœºæ™¯ä¸‹çš„å¯¹æ¯”å®éªŒï¼Œç ”ç©¶é‡åŒ–åˆ†æäº†å„æŒ‡æ ‡åŒºåˆ†æœ‰æ•ˆæ£€æµ‹ä¸éšæœºå™ªå£°çš„åˆ¤åˆ«èƒ½åŠ›(discriminative ability)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¤§å¤šæ•°äº‹ä»¶çº§æŒ‡æ ‡è¡¨ç°å‡ºè¾ƒå¼ºçš„åˆ†ç¦»åº¦ï¼Œä½†åŒ…æ‹¬NABå’ŒPoint-Adjuståœ¨å†…çš„å¤šé¡¹å¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡åœ¨æŠµå¾¡éšæœºå¾—åˆ†è†¨èƒ€æ–¹é¢è¡¨ç°å—é™ã€‚è¯¥ç ”ç©¶å¼ºè°ƒè¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©å¿…é¡»ä¸å…·ä½“ä»»åŠ¡åŠç‰©è”ç½‘(IoT)åº”ç”¨çš„è¿è¥ç›®æ ‡ä¿æŒä¸€è‡´ï¼Œä¸ºæ„å»ºæ›´å…·é²æ£’æ€§ã€å…¬å¹³æ€§ä¸”ç¬¦åˆåœºæ™¯éœ€æ±‚çš„å¼‚å¸¸æ£€æµ‹è¯„ä»·æ–¹æ³•æä¾›äº†é‡è¦çš„å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18739v1",
      "published_date": "2025-11-24 04:09:04 UTC",
      "updated_date": "2025-11-24 04:09:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:20:01.555685+00:00"
    },
    {
      "arxiv_id": "2511.18735v2",
      "title": "Thinking Ahead: Foresight Intelligence in MLLMs and World Models",
      "title_zh": "é¢„è§æœªæ¥ï¼šMLLM ä¸ä¸–ç•Œæ¨¡å‹ä¸­çš„é¢„è§æ€§æ™ºèƒ½",
      "authors": [
        "Zhantao Gong",
        "Liaoyuan Fan",
        "Qing Guo",
        "Xun Xu",
        "Xulei Yang",
        "Shijie Li"
      ],
      "abstract": "In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶å®šä¹‰äº† Foresight Intelligence (é¢„è§æ™ºèƒ½)ï¼Œå³é¢„æµ‹å’Œè§£é‡Šæœªæ¥äº‹ä»¶çš„èƒ½åŠ›ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨è‡ªåŠ¨é©¾é©¶ç­‰åº”ç”¨ä¸­çš„å…³é”®æ€§ã€‚ä¸ºäº†å¡«è¡¥ç°æœ‰ç ”ç©¶çš„ç©ºç™½ï¼Œä½œè€…æ¨å‡ºäº† FSU-QAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å’Œæ¿€å‘è¯¥èƒ½åŠ›çš„æ–°å‹ Visual Question-Answering (VQA) æ•°æ®é›†ã€‚é€šè¿‡åœ¨è¯¥æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆç ”ç©¶ï¼Œè®ºæ–‡æ­ç¤ºäº†å½“å‰ä¸»æµ Vision-Language Models (VLMs) åœ¨æ¨ç†æœªæ¥æƒ…å¢ƒæ–¹é¢çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼ŒFSU-QA è¿˜å¯ç”¨äºè¯„ä¼° World Models ç”Ÿæˆé¢„æµ‹çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œä½œä¸ºè¡¡é‡å…¶å¢å¼º VLMs æ€§èƒ½çš„æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ FSU-QA è¿›è¡Œå¾®è°ƒèƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„é¢„è§æ€§æ¨ç†èƒ½åŠ›ï¼Œç”šè‡³ä½¿å¾®è°ƒåçš„å°å‹ VLMs åœ¨è¡¨ç°ä¸Šå¤§å¹…è¶…è¶Šäº†è§„æ¨¡æ›´å¤§çš„å…ˆè¿›æ¨¡å‹ã€‚è¯¥é¡¹å·¥ä½œä¸ºå¼€å‘å…·å¤‡çœŸå®é¢„æµ‹å’Œç†è§£æœªæ¥äº‹ä»¶èƒ½åŠ›çš„ä¸‹ä¸€ä»£æ¨¡å‹æä¾›äº†é‡è¦çš„ç†è®ºä¸æ•°æ®åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages, 27 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.18735v2",
      "published_date": "2025-11-24 04:04:59 UTC",
      "updated_date": "2025-12-11 08:02:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:20:21.883177+00:00"
    },
    {
      "arxiv_id": "2511.18734v2",
      "title": "Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion",
      "title_zh": "Yo'Cityï¼šåŸºäºè‡ªæˆ‘æ‰¹åˆ¤æ‰©å±•çš„ä¸ªæ€§åŒ–æ— è¾¹ç•Œä¸‰ç»´å†™å®åŸå¸‚åœºæ™¯ç”Ÿæˆ",
      "authors": [
        "Keyang Lu",
        "Sifan Zhou",
        "Hongbin Xu",
        "Gang Xu",
        "Zhifei Yang",
        "Yikai Wang",
        "Zhen Xiao",
        "Jieyi Long",
        "Ming Li"
      ],
      "abstract": "Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical \"City-District-Grid\" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a \"produce-refine-evaluate\" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Yo'Cityï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ agentic frameworkï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œç»„åˆèƒ½åŠ›å®ç°ç”¨æˆ·å®šåˆ¶ä¸”æ— é™æ‰©å±•çš„ 3D ç°å®åŸå¸‚ç”Ÿæˆã€‚Yo'City é‡‡ç”¨è‡ªé¡¶å‘ä¸‹çš„è§„åˆ’ç­–ç•¥ï¼Œæ„å»ºäº† City-District-Grid å±‚æ¬¡ç»“æ„ï¼Œå¹¶åˆ©ç”¨ Global Planner å’Œ Local Designer åˆ†åˆ«å¤„ç†å®è§‚å¸ƒå±€ä¸å¾®è§‚ç½‘æ ¼æè¿°ã€‚åœ¨ç”Ÿæˆç¯èŠ‚ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ produce-refine-evaluate çš„ç­‰è·å›¾åƒåˆæˆå¾ªç¯ï¼Œå¹¶ç»“åˆ image-to-3D æŠ€æœ¯å®Œæˆä»å›¾åƒåˆ°ä¸‰ç»´æ¨¡å‹çš„è½¬åŒ–ã€‚ä¸ºäº†æ¨¡æ‹ŸæŒç»­çš„åŸå¸‚æ¼”åŒ–ï¼Œç ”ç©¶å¼•å…¥äº† relationship-guided expansion mechanismï¼Œåˆ©ç”¨åŸºäº scene graph çš„è·ç¦»å’Œè¯­ä¹‰æ„ŸçŸ¥å¸ƒå±€ä¼˜åŒ–æ¥ç¡®ä¿ç©ºé—´å¢é•¿çš„è¿è´¯æ€§ã€‚é€šè¿‡åœ¨åŒ…å«å…­ä¸ªç»´åº¦çš„å¤šå…ƒåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå®éªŒè¯æ˜ Yo'City åœ¨ç”Ÿæˆè´¨é‡ã€å‡ ä½•ç²¾åº¦å’Œå¸ƒå±€åˆç†æ€§ç­‰æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ state-of-the-art æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.18734v2",
      "published_date": "2025-11-24 04:02:48 UTC",
      "updated_date": "2025-11-28 12:46:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:20:29.670719+00:00"
    },
    {
      "arxiv_id": "2511.19518v1",
      "title": "Towards Efficient VLMs: Information-Theoretic Driven Compression via Adaptive Structural Pruning",
      "title_zh": "é¢å‘é«˜æ•ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼šåŸºäºä¿¡æ¯è®ºé©±åŠ¨çš„è‡ªé€‚åº”ç»“æ„åŒ–å‰ªæå‹ç¼©",
      "authors": [
        "Zhaoqi Xu",
        "Yingying Zhang",
        "Jian Li",
        "Jianwei Guo",
        "Qiannan Zhu",
        "Hua Huang"
      ],
      "abstract": "Recent advances in vision-language models (VLMs) have shown remarkable performance across multimodal tasks, yet their ever-growing scale poses severe challenges for deployment and efficiency. Existing compression methods often rely on heuristic importance metrics or empirical pruning rules, lacking theoretical guarantees about information preservation. In this work, we propose InfoPrune, an information-theoretic framework for adaptive structural compression of VLMs. Grounded in the Information Bottleneck principle, we formulate pruning as a trade-off between retaining task-relevant semantics and discarding redundant dependencies. To quantify the contribution of each attention head, we introduce an entropy-based effective rank (eRank) and employ the Kolmogorov--Smirnov (KS) distance to measure the divergence between original and compressed structures. This yields a unified criterion that jointly considers structural sparsity and informational efficiency. Building on this foundation, we further design two complementary schemes: (1) a training-based head pruning guided by the proposed information loss objective, and (2) a training-free FFN compression via adaptive low-rank approximation. Extensive experiments on VQAv2, TextVQA, and GQA demonstrate that InfoPrune achieves up to 3.2x FLOP reduction and 1.8x acceleration with negligible performance degradation, establishing a theoretically grounded and practically effective step toward efficient multimodal large models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† InfoPruneï¼Œä¸€ç§ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) è‡ªé€‚åº”ç»“æ„å‹ç¼©çš„ä¿¡æ¯è®ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ¨¡å‹åœ¨éƒ¨ç½²ä¸æ•ˆç‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ¤æ ¹äºä¿¡æ¯ç“¶é¢ˆ (Information Bottleneck) åŸç†ï¼Œå°†å‰ªæè¿‡ç¨‹å»ºæ¨¡ä¸ºä¿ç•™ä»»åŠ¡ç›¸å…³è¯­ä¹‰ä¸èˆå¼ƒå†—ä½™ä¾èµ–ä¹‹é—´çš„æƒè¡¡ã€‚ä¸ºäº†é‡åŒ–æ³¨æ„åŠ›å¤´çš„è´¡çŒ®ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºç†µçš„æœ‰æ•ˆç§© (eRank)ï¼Œå¹¶é‡‡ç”¨ Kolmogorov-Smirnov (KS) è·ç¦»æ¥è¡¡é‡åŸå§‹ç»“æ„ä¸å‹ç¼©ç»“æ„ä¹‹é—´çš„å·®å¼‚ã€‚åŸºäºæ­¤å‡†åˆ™ï¼ŒInfoPrune è®¾è®¡äº†ç”±ä¿¡æ¯æŸå¤±ç›®æ ‡å¼•å¯¼çš„è®­ç»ƒå¼å¤´éƒ¨å‰ªæï¼Œä»¥åŠåŸºäºè‡ªé€‚åº”ä½ç§©è¿‘ä¼¼çš„æ— éœ€è®­ç»ƒå¼ FFN å‹ç¼©æ–¹æ¡ˆã€‚åœ¨ VQAv2ã€TextVQA å’Œ GQA æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½å‡ ä¹æ— æŸçš„æƒ…å†µä¸‹å®ç°äº† 3.2 å€çš„ FLOPs å‡å°‘å’Œ 1.8 å€çš„åŠ é€Ÿã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºç†è®ºå®Œå¤‡ä¸”é«˜æ•ˆçš„å¤šæ¨¡æ€å¤§æ¨¡å‹è¿ˆå‡ºäº†å®è·µæ€§çš„ä¸€æ­¥ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19518v1",
      "published_date": "2025-11-24 03:37:14 UTC",
      "updated_date": "2025-11-24 03:37:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:20:24.383494+00:00"
    },
    {
      "arxiv_id": "2511.18723v4",
      "title": "N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory",
      "title_zh": "N2Nï¼šé¢å‘åˆ†å¸ƒå¼å†…å­˜çš„å¤§è§„æ¨¡ MILP å¹¶è¡Œæ¡†æ¶",
      "authors": [
        "Longfei Wang",
        "Junyan Liu",
        "Fan Zhang",
        "Jiangwen Wei",
        "Yuanhua Tang",
        "Jie Sun",
        "Xiaodong Luo"
      ],
      "abstract": "Parallelization has emerged as a promising approach for accelerating MILP solving. However, the complexity of the branch-and-bound (B&B) framework and the numerous effective algorithm components in MILP solvers make it difficult to parallelize. In this study, a scalable parallel framework, N2N (a node-to-node framework that maps the B&B nodes to distributed computing nodes), was proposed to solve large-scale problems in a distributed memory computing environment. Both deterministic and nondeterministic modes are supported, and the framework is designed to be easily integrated with existing solvers. Regarding the deterministic mode, a novel sliding-window-based algorithm was designed and implemented to ensure that tasks are generated and solved in a deterministic order. Moreover, several advanced techniques, such as the utilization of CP search and general primal heuristics, have been developed to fully utilize distributed computing resources and capabilities of base solvers. Adaptive solving and data communication optimization were also investigated. A popular open-source MILP solver, SCIP, was integrated into N2N as the base solver, yielding N2N-SCIP. Extensive computational experiments were conducted to evaluate the performance of N2N-SCIP compared to ParaSCIP, which is a state-of-the-art distributed parallel MILP solver under the UG framework. The nondeterministic N2N-SCIP achieves speedups of 22.52 and 12.71 with 1,000 MPI processes on the Kunpeng and x86 computing clusters, which is 1.98 and 2.08 times faster than ParaSCIP, respectively. In the deterministic mode, N2N-SCIP also shows significant performance improvements over ParaSCIP across different process numbers and computing clusters. To validate the generality of N2N, HiGHS, another open-source solver, was integrated into N2N. The related results are analyzed, and the requirements of N2N on base solvers are also concluded.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†N2Nï¼Œä¸€ç§ä¸“é—¨ä¸ºåˆ†å¸ƒå¼å†…å­˜è®¡ç®—ç¯å¢ƒè®¾è®¡çš„å¤§è§„æ¨¡MILPæ±‚è§£å¯æ‰©å±•å¹¶è¡Œæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³branch-and-bound (B&B)æ¡†æ¶å¹¶è¡ŒåŒ–çš„å¤æ‚æ€§ã€‚N2Né‡‡ç”¨èŠ‚ç‚¹åˆ°èŠ‚ç‚¹(node-to-node)çš„æ˜ å°„æœºåˆ¶å°†B&BèŠ‚ç‚¹åˆ†é…è‡³åˆ†å¸ƒå¼è®¡ç®—èŠ‚ç‚¹ï¼Œå¹¶åŒæ—¶æ”¯æŒç¡®å®šæ€§å’Œéç¡®å®šæ€§æ±‚è§£æ¨¡å¼ã€‚åœ¨ç¡®å®šæ€§æ¨¡å¼ä¸‹ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åˆ›æ–°çš„åŸºäºæ»‘åŠ¨çª—å£(sliding-window-based)çš„ç®—æ³•ä»¥ç¡®ä¿ä»»åŠ¡æ‰§è¡Œé¡ºåºï¼Œå¹¶ç»“åˆäº†CP searchã€é€šç”¨åŸå§‹å¯å‘å¼(general primal heuristics)åŠè‡ªé€‚åº”æ±‚è§£ç­‰ä¼˜åŒ–æŠ€æœ¯ã€‚å®éªŒé€šè¿‡é›†æˆSCIPæ±‚è§£å™¨æ„å»ºäº†N2N-SCIPï¼Œç»“æœæ˜¾ç¤ºåœ¨1000ä¸ªMPIè¿›ç¨‹ä¸‹ï¼Œå…¶åŠ é€Ÿæ€§èƒ½æ¯”ç›®å‰æœ€å…ˆè¿›çš„ParaSCIPé«˜å‡ºçº¦2å€ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é€šè¿‡é›†æˆHiGHSæ±‚è§£å™¨éªŒè¯äº†N2Nçš„é€šç”¨æ€§ï¼Œè¯æ˜äº†å…¶åœ¨åˆ©ç”¨åˆ†å¸ƒå¼èµ„æºå¤„ç†å¤§è§„æ¨¡å¤æ‚ä¼˜åŒ–é—®é¢˜ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI",
        "cs.DC",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 2 figures; the affiliation of some authors is updated in this version",
      "pdf_url": "https://arxiv.org/pdf/2511.18723v4",
      "published_date": "2025-11-24 03:29:55 UTC",
      "updated_date": "2025-12-18 08:44:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:20:54.056010+00:00"
    },
    {
      "arxiv_id": "2511.18721v1",
      "title": "Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM",
      "title_zh": "è¿ˆå‘ç°å®çš„ä¿éšœï¼šSmoothLLM çš„æ¦‚ç‡åŒ–è®¤è¯",
      "authors": [
        "Adarsh Kumarappan",
        "Ayushi Mehrotra"
      ],
      "abstract": "The SmoothLLM defense provides a certification guarantee against jailbreaking attacks, but it relies on a strict `k-unstable' assumption that rarely holds in practice. This strong assumption can limit the trustworthiness of the provided safety certificate. In this work, we address this limitation by introducing a more realistic probabilistic framework, `(k, $\\varepsilon$)-unstable,' to certify defenses against diverse jailbreaking attacks, from gradient-based (GCG) to semantic (PAIR). We derive a new, data-informed lower bound on SmoothLLM's defense probability by incorporating empirical models of attack success, providing a more trustworthy and practical safety certificate. By introducing the notion of (k, $\\varepsilon$)-unstable, our framework provides practitioners with actionable safety guarantees, enabling them to set certification thresholds that better reflect the real-world behavior of LLMs. Ultimately, this work contributes a practical and theoretically-grounded mechanism to make LLMs more resistant to the exploitation of their safety alignments, a critical challenge in secure AI deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ SmoothLLM é˜²å¾¡åœ¨å¯¹æŠ—è¶Šç‹±æ”»å‡»(jailbreaking attacks)æ—¶ä¾èµ–çš„ä¸¥æ ¼ `k-unstable` å‡è®¾åœ¨å®é™…ä¸­éš¾ä»¥æ»¡è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªæ›´å…·ç°å®æ„ä¹‰çš„æ¦‚ç‡æ¡†æ¶ `(k, Îµ)-unstable`ï¼Œç”¨äºè¯æ˜é’ˆå¯¹ä»åŸºäºæ¢¯åº¦çš„ GCG åˆ°è¯­ä¹‰åŒ–çš„ PAIR ç­‰å¤šç§è¶Šç‹±æ”»å‡»çš„é˜²å¾¡æœ‰æ•ˆæ€§ã€‚é€šè¿‡ç»“åˆæ”»å‡»æˆåŠŸçš„ç»éªŒæ¨¡å‹ï¼Œè¯¥ç ”ç©¶æ¨å¯¼å‡ºäº† SmoothLLM é˜²å¾¡æ¦‚ç‡çš„ä¸€ä¸ªæ–°çš„ã€åŸºäºæ•°æ®çš„ä¸‹ç•Œï¼Œä»è€Œæä¾›äº†æ›´å…·å¯ä¿¡åº¦å’Œå®ç”¨æ€§çš„å®‰å…¨è¯ä¹¦(safety certificate)ã€‚è¯¥æ¡†æ¶ä¸ºä»ä¸šè€…æä¾›äº†å¯æ“ä½œçš„å®‰å…¨ä¿è¯ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿè®¾å®šæ›´ç¬¦åˆå¤§è¯­è¨€æ¨¡å‹(LLMs)çœŸå®ä¸–ç•Œè¡Œä¸ºçš„è®¤è¯é˜ˆå€¼ã€‚è¿™é¡¹å·¥ä½œä¸ºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨å¯¹é½(safety alignments)æ–¹é¢çš„æŠ—åˆ©ç”¨èƒ½åŠ›æä¾›äº†ä¸€ä¸ªç†è®ºä¸å®è·µç›¸ç»“åˆçš„æœºåˆ¶ï¼Œæœ‰æ•ˆåº”å¯¹äº†äººå·¥æ™ºèƒ½å®‰å…¨éƒ¨ç½²ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18721v1",
      "published_date": "2025-11-24 03:25:16 UTC",
      "updated_date": "2025-11-24 03:25:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:21:04.672602+00:00"
    },
    {
      "arxiv_id": "2511.18718v1",
      "title": "AIRHILT: A Human-in-the-Loop Testbed for Multimodal Conflict Detection in Aviation",
      "title_zh": "AIRHILTï¼šé¢å‘èˆªç©ºå¤šæ¨¡æ€å†²çªæ£€æµ‹çš„äººåœ¨å›è·¯è¯•éªŒå¹³å°",
      "authors": [
        "Omar Garib",
        "Jayaprakash D. Kambhampaty",
        "Olivia J. Pinon Fischer",
        "Dimitri N. Mavris"
      ],
      "abstract": "We introduce AIRHILT (Aviation Integrated Reasoning, Human-in-the-Loop Testbed), a modular and lightweight simulation environment designed to evaluate multimodal pilot and air traffic control (ATC) assistance systems for aviation conflict detection. Built on the open-source Godot engine, AIRHILT synchronizes pilot and ATC radio communications, visual scene understanding from camera streams, and ADS-B surveillance data within a unified, scalable platform. The environment supports pilot- and controller-in-the-loop interactions, providing a comprehensive scenario suite covering both terminal area and en route operational conflicts, including communication errors and procedural mistakes. AIRHILT offers standardized JSON-based interfaces that enable researchers to easily integrate, swap, and evaluate automatic speech recognition (ASR), visual detection, decision-making, and text-to-speech (TTS) models. We demonstrate AIRHILT through a reference pipeline incorporating fine-tuned Whisper ASR, YOLO-based visual detection, ADS-B-based conflict logic, and GPT-OSS-20B structured reasoning, and present preliminary results from representative runway-overlap scenarios, where the assistant achieves an average time-to-first-warning of approximately 7.7 s, with average ASR and vision latencies of approximately 5.9 s and 0.4 s, respectively. The AIRHILT environment and scenario suite are openly available, supporting reproducible research on multimodal situational awareness and conflict detection in aviation; code and scenarios are available at https://github.com/ogarib3/airhilt.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†AIRHILT (Aviation Integrated Reasoning, Human-in-the-Loop Testbed)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºGodotå¼•æ“æ„å»ºçš„æ¨¡å—åŒ–ã€è½»é‡çº§æ¨¡æ‹Ÿç¯å¢ƒï¼Œä¸“é—¨ç”¨äºè¯„ä¼°èˆªç©ºå†²çªæ£€æµ‹ä¸­çš„å¤šæ¨¡æ€é£è¡Œå‘˜ä¸ç©ºä¸­äº¤é€šç®¡åˆ¶(ATC)è¾…åŠ©ç³»ç»Ÿã€‚è¯¥å¹³å°æ•´åˆå¹¶åŒæ­¥äº†æ— çº¿ç”µé€šä¿¡ã€æ‘„åƒæœºæµè§†è§‰åœºæ™¯ç†è§£ä»¥åŠADS-Bç›‘æ§æ•°æ®ï¼Œæ¶µç›–äº†ç»ˆç«¯åŒºå’Œèˆªè·¯æ“ä½œä¸­çš„é€šä¿¡é”™è¯¯åŠç¨‹åºå¤±è¯¯ç­‰å¤šç§å†²çªåœºæ™¯ã€‚AIRHILTæ”¯æŒé£è¡Œå‘˜å’Œç®¡åˆ¶å‘˜åœ¨ç¯(Human-in-the-Loop)äº¤äº’ï¼Œå¹¶æä¾›æ ‡å‡†åŒ–çš„JSONæ¥å£ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜é›†æˆå’Œè¯„ä¼°è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ã€è§†è§‰æ£€æµ‹ã€å†³ç­–åˆ¶å®šåŠæ–‡æœ¬è½¬è¯­éŸ³(TTS)æ¨¡å‹ã€‚ä½œè€…é€šè¿‡é›†æˆå¾®è°ƒåçš„Whisper ASRã€åŸºäºYOLOçš„è§†è§‰æ£€æµ‹ä»¥åŠGPT-OSS-20Bç»“æ„åŒ–æ¨ç†çš„å‚è€ƒç®¡çº¿éªŒè¯äº†ç³»ç»Ÿæ€§èƒ½ï¼Œåœ¨è·‘é“é‡å åœºæ™¯ä¸­å®ç°äº†çº¦7.7ç§’çš„å¹³å‡é¦–æ¬¡é¢„è­¦æ—¶é—´ã€‚è¯¥ç¯å¢ƒåŠå…¶åœºæ™¯å¥—ä»¶å·²å¼€æºï¼Œä¸ºèˆªç©ºé¢†åŸŸå¤šæ¨¡æ€æ€åŠ¿æ„ŸçŸ¥å’Œå†²çªæ£€æµ‹çš„å¯é‡å¤ç ”ç©¶æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 4 figures, 1 table, 1 algorithm",
      "pdf_url": "https://arxiv.org/pdf/2511.18718v1",
      "published_date": "2025-11-24 03:18:55 UTC",
      "updated_date": "2025-11-24 03:18:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:21:00.577018+00:00"
    },
    {
      "arxiv_id": "2511.19517v1",
      "title": "Automating Deception: Scalable Multi-Turn LLM Jailbreaks",
      "title_zh": "è‡ªåŠ¨åŒ–æ¬ºéª—ï¼šå¯æ‰©å±•çš„å¤šè½®å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±",
      "authors": [
        "Adarsh Kumarappan",
        "Ananya Mujoo"
      ],
      "abstract": "Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¾—å¯¸è¿›å°º(Foot-in-the-Door, FITD)ç­‰å¿ƒç†å­¦åŸç†çš„å¤šè½®å¯¹è¯æ”»å‡»å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æ„æˆçš„æŒç»­å¨èƒã€‚é’ˆå¯¹é˜²å¾¡å·¥ä½œå—é™äºæ‰‹åŠ¨åˆ›å»ºæ•°æ®é›†ä¸”éš¾ä»¥è§„æ¨¡åŒ–çš„ç°çŠ¶ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æµç¨‹ï¼Œç”¨äºç”Ÿæˆå¤§è§„æ¨¡ä¸”åŸºäºå¿ƒç†å­¦ç†è®ºçš„å¤šè½®è¶Šç‹±(jailbreak)æ•°æ®é›†ã€‚ç ”ç©¶é€šè¿‡å°†FITDæŠ€æœ¯ç³»ç»Ÿåœ°è½¬åŒ–ä¸ºå¯é‡å¤çš„æ¨¡æ¿ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«1,500ä¸ªæ¶‰åŠéæ³•æ´»åŠ¨å’Œæ”»å‡»æ€§å†…å®¹çš„åŸºå‡†åœºæ™¯ã€‚å®éªŒå¯¹GPTã€Geminiå’ŒClaudeç­‰å®¶æ—çš„ä¸ƒä¸ªæ¨¡å‹åœ¨å¤šè½®ä¸å•è½®å¯¹è¯ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPTç³»åˆ—æ¨¡å‹å¯¹å¯¹è¯å†å²å­˜åœ¨æ˜¾è‘—è„†å¼±æ€§ï¼Œå…¶æ”»å‡»æˆåŠŸç‡(ASR)åœ¨å¤šè½®ç¯å¢ƒä¸‹æœ€é«˜æå‡äº†32ä¸ªç™¾åˆ†ç‚¹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGoogleçš„Gemini 2.5 Flashè¡¨ç°å‡ºæå¼ºçš„éŸ§æ€§å¹¶å‡ ä¹å…ç–«æ­¤ç±»æ”»å‡»ï¼Œè€ŒAnthropicçš„Claude 3 Haikuåˆ™è¡¨ç°å‡ºå¼ºåŠ›ä½†éå®Œç¾çš„æŠ—æ€§ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†å½“å‰å®‰å…¨æ¶æ„åœ¨å¤„ç†å¯¹è¯ä¸Šä¸‹æ–‡æ—¶çš„å·®å¼‚ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘èƒ½æŠµå¾¡å™äº‹æ“çºµ(narrative-based manipulation)é˜²å¾¡æœºåˆ¶çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19517v1",
      "published_date": "2025-11-24 03:15:11 UTC",
      "updated_date": "2025-11-24 03:15:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:21:13.971648+00:00"
    },
    {
      "arxiv_id": "2511.18715v1",
      "title": "HuggingR$^{4}$: A Progressive Reasoning Framework for Discovering Optimal Model Companions",
      "title_zh": "HuggingR$^{4}$ï¼šç”¨äºå‘ç°æœ€ä¼˜æ¨¡å‹ä¼™ä¼´çš„æ¸è¿›å¼æ¨ç†æ¡†æ¶",
      "authors": [
        "Shaoyin Ma",
        "Jie Song",
        "Huiqiong Wang",
        "Li Sun",
        "Mingli Song"
      ],
      "abstract": "Large Language Models (LLMs) have made remarkable progress in their ability to interact with external interfaces. Selecting reasonable external interfaces has thus become a crucial step in constructing LLM agents. In contrast to invoking API tools, directly calling AI models across different modalities from the community (e.g., HuggingFace) poses challenges due to the vast scale (> 10k), metadata gaps, and unstructured descriptions. Current methods for model selection often involve incorporating entire model descriptions into prompts, resulting in prompt bloat, wastage of tokens and limited scalability. To address these issues, we propose HuggingR$^4$, a novel framework that combines Reasoning, Retrieval, Refinement, and Reflection, to efficiently select models. Specifically, We first perform multiple rounds of reasoning and retrieval to get a coarse list of candidate models. Then, we conduct fine-grained refinement by analyzing candidate model descriptions, followed by reflection to assess results and determine if retrieval scope expansion is necessary. This method reduces token consumption considerably by decoupling user query processing from complex model description handling. Through a pre-established vector database, complex model descriptions are stored externally and retrieved on-demand, allowing the LLM to concentrate on interpreting user intent while accessing only relevant candidate models without prompt bloat. In the absence of standardized benchmarks, we construct a multimodal human-annotated dataset comprising 14,399 user requests across 37 tasks and conduct a thorough evaluation. HuggingR$^4$ attains a workability rate of 92.03% and a reasonability rate of 82.46%, surpassing existing method by 26.51% and 33.25% respectively on GPT-4o-mini.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä» HuggingFace ç­‰ç¤¾åŒºé€‰æ‹©å¤–éƒ¨ AI æ¨¡å‹æ—¶é¢ä¸´çš„è§„æ¨¡å·¨å¤§ã€å…ƒæ•°æ®ç¼ºå¤±åŠæè¿°éç»“æ„åŒ–ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† HuggingR$^4$ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆ Reasoning (æ¨ç†)ã€Retrieval (æ£€ç´¢)ã€Refinement (ç»†åŒ–) å’Œ Reflection (åæ€) å››ä¸ªæ ¸å¿ƒç¯èŠ‚ï¼Œå®ç°äº†ä¸€ç§æ¸è¿›å¼çš„æ¨¡å‹å‘ç°ä¸é€‰æ‹©æœºåˆ¶ã€‚HuggingR$^4$ åˆ©ç”¨é¢„æ„å»ºçš„å‘é‡æ•°æ®åº“ (vector database) å°†å¤æ‚çš„æ¨¡å‹æè¿°å­˜å‚¨åœ¨å¤–éƒ¨å¹¶è¿›è¡ŒæŒ‰éœ€æ£€ç´¢ï¼Œæœ‰æ•ˆè§£å†³äº†æç¤ºè¯è†¨èƒ€ (prompt bloat) å’Œ Token æµªè´¹é—®é¢˜ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸“æ³¨äºè§£æç”¨æˆ·æ„å›¾ã€‚ä¸ºäº†å¼¥è¡¥æ ‡å‡†è¯„æµ‹åŸºå‡†çš„ç©ºç™½ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŒ…å« 37 ä¸ªä»»åŠ¡ã€14,399 æ¡ç”¨æˆ·è¯·æ±‚çš„å¤šæ¨¡æ€äººå·¥æ ‡æ³¨æ•°æ®é›†è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHuggingR$^4$ åœ¨ GPT-4o-mini ä¸Šå–å¾—äº† 92.03% çš„å¯è¡Œç‡ (workability rate) å’Œ 82.46% çš„åˆç†ç‡ (reasonability rate)ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•åˆ†åˆ«å¤§å¹…æå‡äº† 26.51% å’Œ 33.25%ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†å¤§è§„æ¨¡æ¨¡å‹ä¼™ä¼´å‘ç°ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ä¸å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.18715v1",
      "published_date": "2025-11-24 03:13:45 UTC",
      "updated_date": "2025-11-24 03:13:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:21:05.275070+00:00"
    },
    {
      "arxiv_id": "2511.18714v1",
      "title": "MAGMA-Edu: Multi-Agent Generative Multimodal Framework for Text-Diagram Educational Question Generation",
      "title_zh": "MAGMA-Eduï¼šé¢å‘æ–‡å›¾æ•™è‚²é¢˜ç›®ç”Ÿæˆçš„ç”Ÿæˆå¼å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Zhenyu Wu",
        "Jian Li",
        "Hua Huang"
      ],
      "abstract": "Educational illustrations play a central role in communicating abstract concepts, yet current multimodal large language models (MLLMs) remain limited in producing pedagogically coherent and semantically consistent educational visuals. We introduce MAGMA-Edu, a self-reflective multi-agent framework that unifies textual reasoning and diagrammatic synthesis for structured educational problem generation. Unlike existing methods that treat text and image generation independently, MAGMA-Edu employs a two-stage co-evolutionary pipeline: (1) a generation-verification-reflection loop that iteratively refines question statements and solutions for mathematical accuracy, and (2) a code-based intermediate representation that enforces geometric fidelity and semantic alignment during image rendering. Both stages are guided by internal self-reflection modules that evaluate and revise outputs until domain-specific pedagogical constraints are met. Extensive experiments on multimodal educational benchmarks demonstrate the superiority of MAGMA-Edu over state-of-the-art MLLMs. Compared to GPT-4o, MAGMA-Edu improves the average textual metric from 57.01 to 92.31 (+35.3 pp) and boosts image-text consistency (ITC) from 13.20 to 85.24 (+72 pp). Across all model backbones, MAGMA-Edu achieves the highest scores (Avg-Text 96.20, ITC 99.12), establishing a new state of the art for multimodal educational content generation and demonstrating the effectiveness of self-reflective multi-agent collaboration in pedagogically aligned vision-language reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MAGMA-Eduï¼Œè¿™æ˜¯ä¸€ç§è‡ªåæ€çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶(Multi-Agent Framework)ï¼Œæ—¨åœ¨è§£å†³å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨ç”Ÿæˆå…·æœ‰æ•™å­¦è¿è´¯æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§çš„æ•™è‚²è§†è§‰æ•ˆæœæ–¹é¢çš„å±€é™æ€§ã€‚MAGMA-Edué‡‡ç”¨ä¸¤é˜¶æ®µååŒæ¼”åŒ–æµæ°´çº¿ï¼Œé€šè¿‡â€œç”Ÿæˆ-éªŒè¯-åæ€â€å¾ªç¯è¿­ä»£ä¼˜åŒ–æ•°å­¦é—®é¢˜çš„é™ˆè¿°ä¸è§£ç­”ï¼Œå¹¶åˆ©ç”¨åŸºäºä»£ç çš„ä¸­é—´è¡¨ç¤º(Code-based Intermediate Representation)ç¡®ä¿å›¾è¡¨æ¸²æŸ“ä¸­çš„å‡ ä½•å¿ å®åº¦ä¸è¯­ä¹‰å¯¹é½ã€‚è¿™ä¸¤ä¸ªé˜¶æ®µå‡ç”±å†…éƒ¨è‡ªåæ€æ¨¡å—(Self-reflection Modules)å¼•å¯¼ï¼Œä»¥ç¡®ä¿è¾“å‡ºç¬¦åˆç‰¹å®šé¢†åŸŸçš„æ•™å­¦çº¦æŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAGMA-Eduåœ¨å¤šæ¨¡æ€æ•™è‚²åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºGPT-4oï¼Œå°†å…¶å¹³å‡æ–‡æœ¬æŒ‡æ ‡æå‡è‡³92.31ï¼Œå¹¶å°†å›¾æ–‡ä¸€è‡´æ€§(ITC)ä»13.20å¤§å¹…æå‡è‡³85.24ã€‚åœ¨å„ç§æ¨¡å‹éª¨å¹²ä¸Šï¼Œè¯¥æ¡†æ¶å‡å–å¾—äº†æœ€ä¼˜æ€§èƒ½ï¼Œæœ€é«˜å¹³å‡æ–‡æœ¬å¾—åˆ†è¾¾96.20ï¼ŒITCè¾¾99.12ï¼Œä¸ºå¤šæ¨¡æ€æ•™è‚²å†…å®¹ç”Ÿæˆæ ‘ç«‹äº†æ–°çš„SOTAã€‚è¯¥å·¥ä½œå……åˆ†éªŒè¯äº†è‡ªåæ€å¤šæ™ºèƒ½ä½“åä½œåœ¨å®ç°æ•™å­¦å¯¹é½çš„è§†è§‰è¯­è¨€æ¨ç†ä¸­çš„å“è¶Šæ•ˆèƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18714v1",
      "published_date": "2025-11-24 03:13:26 UTC",
      "updated_date": "2025-11-24 03:13:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:21:11.676562+00:00"
    },
    {
      "arxiv_id": "2511.18711v1",
      "title": "Modality-Collaborative Low-Rank Decomposers for Few-Shot Video Domain Adaptation",
      "title_zh": "é¢å‘å°æ ·æœ¬è§†é¢‘é¢†åŸŸè‡ªé€‚åº”çš„æ¨¡æ€åä½œä½ç§©åˆ†è§£å™¨",
      "authors": [
        "Yuyang Wanyan",
        "Xiaoshan Yang",
        "Weiming Dong",
        "Changsheng Xu"
      ],
      "abstract": "In this paper, we study the challenging task of Few-Shot Video Domain Adaptation (FSVDA). The multimodal nature of videos introduces unique challenges, necessitating the simultaneous consideration of both domain alignment and modality collaboration in a few-shot scenario, which is ignored in previous literature. We observe that, under the influence of domain shift, the generalization performance on the target domain of each individual modality, as well as that of fused multimodal features, is constrained. Because each modality is comprised of coupled features with multiple components that exhibit different domain shifts. This variability increases the complexity of domain adaptation, thereby reducing the effectiveness of multimodal feature integration. To address these challenges, we introduce a novel framework of Modality-Collaborative LowRank Decomposers (MC-LRD) to decompose modality-unique and modality-shared features with different domain shift levels from each modality that are more friendly for domain alignment. The MC-LRD comprises multiple decomposers for each modality and Multimodal Decomposition Routers (MDR). Each decomposer has progressively shared parameters across different modalities. The MDR is leveraged to selectively activate the decomposers to produce modality-unique and modality-shared features. To ensure efficient decomposition, we apply orthogonal decorrelation constraints separately to decomposers and subrouters, enhancing their diversity. Furthermore, we propose a cross-domain activation consistency loss to guarantee that target and source samples of the same category exhibit consistent activation preferences of the decomposers, thereby facilitating domain alignment. Extensive experimental results on three public benchmarks demonstrate that our model achieves significant improvements over existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Few-Shot Video Domain Adaptation (FSVDA)ä¸­ç”±äºè§†é¢‘å¤šæ¨¡æ€ç‰¹æ€§å¯¼è‡´çš„é¢†åŸŸå¯¹é½ä¸æ¨¡æ€åä½œéš¾é¢˜ã€‚é’ˆå¯¹å„æ¨¡æ€ç‰¹å¾ä¸­è€¦åˆäº†ä¸åŒç¨‹åº¦é¢†åŸŸåç§»çš„é—®é¢˜ï¼Œæå‡ºäº†Modality-Collaborative Low-Rank Decomposers (MC-LRD)æ¡†æ¶ï¼Œæ—¨åœ¨ä»å„æ¨¡æ€ä¸­åˆ†è§£å‡ºæ›´åˆ©äºå¯¹é½çš„modality-uniqueå’Œmodality-sharedç‰¹å¾ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Multimodal Decomposition Routers (MDR)å’Œé€çº§å…±äº«å‚æ•°çš„åˆ†è§£å™¨å®ç°ç‰¹å¾æå–ï¼Œå¹¶é€šè¿‡æ­£äº¤å»ç›¸å…³çº¦æŸç¡®ä¿åˆ†è§£è¿‡ç¨‹çš„å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†cross-domain activation consistency lossï¼Œé€šè¿‡ä¿è¯åŒç±»åˆ«è·¨åŸŸæ ·æœ¬çš„æ¿€æ´»åå¥½ä¸€è‡´æ€§æ¥å¢å¼ºé¢†åŸŸå¯¹é½æ•ˆæœã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸‰ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18711v1",
      "published_date": "2025-11-24 03:09:59 UTC",
      "updated_date": "2025-11-24 03:09:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:21:31.961404+00:00"
    },
    {
      "arxiv_id": "2511.21753v1",
      "title": "Extracting Disaster Impacts and Impact Related Locations in Social Media Posts Using Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤åª’ä½“å¸–å­ç¾å®³å½±å“åŠå—ç¾ä½ç½®æå–",
      "authors": [
        "Sameeah Noreen Hameed",
        "Surangika Ranathunga",
        "Raj Prasanna",
        "Kristin Stock",
        "Christopher B. Jones"
      ],
      "abstract": "Large-scale disasters can often result in catastrophic consequences on people and infrastructure. Situation awareness about such disaster impacts generated by authoritative data from in-situ sensors, remote sensing imagery, and/or geographic data is often limited due to atmospheric opacity, satellite revisits, and time limitations. This often results in geo-temporal information gaps. In contrast, impact-related social media posts can act as \"geo-sensors\" during a disaster, where people describe specific impacts and locations. However, not all locations mentioned in disaster-related social media posts relate to an impact. Only the impacted locations are critical for directing resources effectively. e.g., \"The death toll from a fire which ripped through the Greek coastal town of #Mati stood at 80, with dozens of people unaccounted for as forensic experts tried to identify victims who were burned alive #Greecefires #AthensFires #Athens #Greece.\" contains impacted location \"Mati\" and non-impacted locations \"Greece\" and \"Athens\". This research uses Large Language Models (LLMs) to identify all locations, impacts and impacted locations mentioned in disaster-related social media posts. In the process, LLMs are fine-tuned to identify only impacts and impacted locations (as distinct from other, non-impacted locations), including locations mentioned in informal expressions, abbreviations, and short forms. Our fine-tuned model demonstrates efficacy, achieving an F1-score of 0.69 for impact and 0.74 for impacted location extraction, substantially outperforming the pre-trained baseline. These robust results confirm the potential of fine-tuned language models to offer a scalable solution for timely decision-making in resource allocation, situational awareness, and post-disaster recovery planning for responders.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿä¼ æ„Ÿå™¨å’Œé¥æ„Ÿæ•°æ®åœ¨ç¾å®³ç›‘æµ‹ä¸­å­˜åœ¨çš„åœ°ç†æ—¶é—´ä¿¡æ¯é—´éš™ï¼Œæå‡ºåˆ©ç”¨ç¤¾äº¤åª’ä½“å¸–å­ä½œä¸ºâ€œgeo-sensorsâ€æ¥æå–ç¾å®³å½±å“åŠç›¸å…³åœ°ç‚¹ã€‚ç ”ç©¶åˆ©ç”¨Large Language Models (LLMs) è¯†åˆ«ç¤¾äº¤åª’ä½“ä¸­çš„ç¾å®³å½±å“åŠå…¶å¯¹åº”çš„å—ç¾åœ°ç‚¹(impacted locations)ï¼Œç‰¹åˆ«å¼ºè°ƒäº†å°†å—ç¾åœ°ç‚¹ä¸ä»…åœ¨èƒŒæ™¯ä¸­æåŠçš„éå—ç¾åœ°ç‚¹è¿›è¡ŒåŒºåˆ†ã€‚é€šè¿‡å¯¹LLMsè¿›è¡Œfine-tuningï¼Œæ¨¡å‹èƒ½å¤Ÿç²¾å‡†å¤„ç†ç¤¾äº¤åª’ä½“ä¸­çš„éæ­£å¼è¡¨è¾¾ã€ç¼©å†™å’Œç®€çŸ­å½¢å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨ç¾å®³å½±å“å’Œå—ç¾åœ°ç‚¹æå–æ–¹é¢çš„F1-scoreåˆ†åˆ«è¾¾åˆ°äº†0.69å’Œ0.74ï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºé¢„è®­ç»ƒåŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†fine-tunedè¯­è¨€æ¨¡å‹åœ¨å¢å¼ºç¾å®³æ€åŠ¿æ„ŸçŸ¥ã€ä¼˜åŒ–èµ„æºåˆ†é…åŠæ”¯æŒç¾åæ¢å¤è§„åˆ’æ–¹é¢å…·æœ‰é«˜æ•ˆä¸”å¯æ‰©å±•çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21753v1",
      "published_date": "2025-11-24 02:55:51 UTC",
      "updated_date": "2025-11-24 02:55:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:21:50.468698+00:00"
    },
    {
      "arxiv_id": "2511.18701v1",
      "title": "ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction",
      "title_zh": "ObjectAlignï¼šç¥ç»ç¬¦å·å¯¹è±¡ä¸€è‡´æ€§éªŒè¯ä¸ä¿®å¤",
      "authors": [
        "Mustafa Munir",
        "Harsh Goel",
        "Xiwen Wei",
        "Minkyu Choi",
        "Sahil Shah",
        "Kartikeya Bhardwaj",
        "Paul Whatmough",
        "Sandeep Chinchali",
        "Radu Marculescu"
      ],
      "abstract": "Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video's formal representation against a temporal logic specification. A frame transition is subsequently deemed \"consistent\" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ObjectAlignï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³è§†é¢‘ç¼–è¾‘å’Œåˆæˆä¸­å¸¸è§çš„å¯¹è±¡ä¸ä¸€è‡´æ€§ï¼ˆå¦‚å¸§é—ªçƒå’Œèº«ä»½æ¼‚ç§»ï¼‰é—®é¢˜çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ„ŸçŸ¥åº¦é‡ä¸ç¬¦å·æ¨ç† (Symbolic Reasoning) ç›¸ç»“åˆï¼Œå®ç°äº†å¯¹è§†é¢‘åºåˆ—ä¸­å¯¹è±¡çº§å’Œæ—¶é—´ç»´åº¦ä¸ä¸€è‡´æ€§çš„æ£€æµ‹ã€éªŒè¯ä¸ä¿®æ­£ã€‚ObjectAlign å¼•å…¥äº†é’ˆå¯¹ CLIP è¯­ä¹‰ç›¸ä¼¼åº¦ã€LPIPS æ„ŸçŸ¥è·ç¦»ã€ç›´æ–¹å›¾ç›¸å…³æ€§å’Œ SAM å¯¹è±¡æ©ç  IoU ç­‰åº¦é‡çš„å¯å­¦ä¹ é˜ˆå€¼ã€‚æ ¸å¿ƒçš„ç¥ç»ç¬¦å·éªŒè¯å™¨ (Neuro-Symbolic Verifier) ç»“åˆäº†åŸºäº SMT çš„èº«ä»½æ¼‚ç§»å½¢å¼åŒ–æ£€æŸ¥ä¸åŸºäºæ—¶é—´é€»è¾‘ (Temporal Logic) çš„æ¦‚ç‡æ¨¡å‹æ£€æŸ¥ï¼Œé€šè¿‡ç»Ÿä¸€çš„é€»è¾‘æ–­è¨€ç¡®ä¿è§†é¢‘çš„ä½å±‚ç¨³å®šæ€§å’Œé«˜å±‚æ—¶é—´æ­£ç¡®æ€§ã€‚é’ˆå¯¹è¢«æ ‡è®°çš„ä¸ä¸€è‡´å¸§ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„è‡ªé€‚åº”æ’å€¼ä¿®å¤æ–¹æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®æŸåå¸§æ•°åŠ¨æ€è°ƒæ•´æ·±åº¦å¹¶ä»æœ‰æ•ˆå…³é”®å¸§ä¸­é‡å»ºå†…å®¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒObjectAlign åœ¨ DAVIS å’Œ Pexels æ•°æ®é›†ä¸Šç›¸è¾ƒäº SOTA åŸºçº¿æ¨¡å‹åœ¨ CLIP Score ä¸Šæå‡äº† 1.4 ç‚¹ï¼Œåœ¨ Warp Error ä¸Šæ”¹è¿›äº† 6.1 ç‚¹ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘çš„æ„ŸçŸ¥è´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.FL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18701v1",
      "published_date": "2025-11-24 02:50:01 UTC",
      "updated_date": "2025-11-24 02:50:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:21:49.469819+00:00"
    },
    {
      "arxiv_id": "2511.18698v1",
      "title": "Multimodal Real-Time Anomaly Detection and Industrial Applications",
      "title_zh": "å¤šæ¨¡æ€å®æ—¶å¼‚å¸¸æ£€æµ‹ä¸å·¥ä¸šåº”ç”¨",
      "authors": [
        "Aman Verma",
        "Keshav Samdani",
        "Mohd. Samiuddin Shafi"
      ],
      "abstract": "This paper presents the design, implementation, and evolution of a comprehensive multimodal room-monitoring system that integrates synchronized video and audio processing for real-time activity recognition and anomaly detection. We describe two iterations of the system: an initial lightweight implementation using YOLOv8, ByteTrack, and the Audio Spectrogram Transformer (AST), and an advanced version that incorporates multi-model audio ensembles, hybrid object detection, bidirectional cross-modal attention, and multi-method anomaly detection. The evolution demonstrates significant improvements in accuracy, robustness, and industrial applicability. The advanced system combines three audio models (AST, Wav2Vec2, and HuBERT) for comprehensive audio understanding, dual object detectors (YOLO and DETR) for improved accuracy, and sophisticated fusion mechanisms for enhanced cross-modal learning. Experimental evaluation shows the system's effectiveness in general monitoring scenarios as well as specialized industrial safety applications, achieving real-time performance on standard hardware while maintaining high accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶è®¾è®¡å¹¶å®ç°äº†ä¸€ä¸ªç»¼åˆæ€§çš„å¤šæ¨¡æ€æˆ¿é—´ç›‘æ§ç³»ç»Ÿï¼Œåˆ©ç”¨åŒæ­¥çš„è§†é¢‘å’ŒéŸ³é¢‘å¤„ç†æŠ€æœ¯å®ç°å®æ—¶æ´»åŠ¨è¯†åˆ«ä¸å¼‚å¸¸æ£€æµ‹(Real-Time Anomaly Detection)ã€‚ç³»ç»Ÿç»å†äº†ä»ä½¿ç”¨ YOLOv8ã€ByteTrack å’Œ Audio Spectrogram Transformer (AST) çš„è½»é‡åŒ–ç‰ˆæœ¬ï¼Œåˆ°é›†æˆå¤šéŸ³é¢‘æ¨¡å‹ã€æ··åˆç›®æ ‡æ£€æµ‹å’ŒåŒå‘è·¨æ¨¡æ€æ³¨æ„åŠ›(Bidirectional Cross-Modal Attention)çš„é«˜çº§ç‰ˆæœ¬çš„æ¼”è¿›ã€‚é«˜çº§ç‰ˆæœ¬é€šè¿‡èåˆ ASTã€Wav2Vec2 å’Œ HuBERT ä¸‰ç§éŸ³é¢‘æ¨¡å‹ä»¥åŠ YOLO å’Œ DETR åŒæ£€æµ‹å™¨ï¼Œé…åˆç²¾å¯†çš„å¤šæ–¹æ³•èåˆæœºåˆ¶æ˜¾è‘—æå‡äº†è·¨æ¨¡æ€å­¦ä¹ æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨é€šç”¨ç›‘æ§å’Œç‰¹å®šå·¥ä¸šå®‰å…¨åœºæ™¯ä¸­å‡å±•ç°å‡ºæé«˜çš„æœ‰æ•ˆæ€§ï¼Œä¸”åœ¨æ ‡å‡†ç¡¬ä»¶ä¸Šå®ç°äº†é«˜ç²¾åº¦çš„å®æ—¶æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…æå‡äº†å¤šæ¨¡æ€ç›‘æ§ç³»ç»Ÿçš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ï¼Œä¹Ÿä¸ºå…¶åœ¨å¤æ‚å·¥ä¸šç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨æä¾›äº†å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18698v1",
      "published_date": "2025-11-24 02:43:19 UTC",
      "updated_date": "2025-11-24 02:43:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:22:02.359262+00:00"
    },
    {
      "arxiv_id": "2511.18696v1",
      "title": "Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models",
      "title_zh": "å…±æƒ…çº§è”ç½‘ç»œï¼šä¸€ç§æ—¨åœ¨å‡å°‘å¤§è¯­è¨€æ¨¡å‹ç¤¾ä¼šåè§çš„å¤šé˜¶æ®µæç¤ºæŠ€æœ¯",
      "authors": [
        "Wangjiaxuan Xin"
      ],
      "abstract": "This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æŠ¥å‘Šæå‡ºäº† Empathetic Cascading Networks (ECN) æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) å…±æƒ…èƒ½åŠ›ä¸åŒ…å®¹æ€§å¹¶å‡å°‘ç¤¾ä¼šåè§çš„å¤šé˜¶æ®µæç¤ºæŠ€æœ¯ (multi-stage prompting method)ã€‚ECN æ¡†æ¶é€šè¿‡ Perspective Adoptionã€Emotional Resonanceã€Reflective Understanding å’Œ Integrative Synthesis å››ä¸ªæ ¸å¿ƒé˜¶æ®µï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆå…·å¤‡æƒ…æ„Ÿå…±é¸£å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECN åœ¨ GPT-3.5-turbo å’Œ GPT-4 æ¨¡å‹ä¸Šå‡å–å¾—äº†æœ€é«˜çš„æƒ…æ„Ÿå•†æ•° (Empathy Quotient, EQ) è¯„åˆ†ï¼ŒåŒæ—¶åœ¨å…³æ³¨åº¦ (Regard) å’Œå›°æƒ‘åº¦ (Perplexity) æŒ‡æ ‡ä¸Šä¿æŒäº†æå…·ç«äº‰åŠ›çš„è¡¨ç°ã€‚è¿™äº›ç ”ç©¶å‘ç°å……åˆ†è¯æ˜äº† ECN åœ¨éœ€è¦é«˜åº¦å…±æƒ…ä¸åŒ…å®¹æ€§çš„å¯¹è¯å¼äººå·¥æ™ºèƒ½ (conversational AI) åº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18696v1",
      "published_date": "2025-11-24 02:32:20 UTC",
      "updated_date": "2025-11-24 02:32:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:21:59.367448+00:00"
    },
    {
      "arxiv_id": "2511.18694v1",
      "title": "Stable Multi-Drone GNSS Tracking System for Marine Robots",
      "title_zh": "é¢å‘æµ·æ´‹æœºå™¨äººçš„ç¨³å®šå¤šæ— äººæœºGNSSè·Ÿè¸ªç³»ç»Ÿ",
      "authors": [
        "Shuo Wen",
        "Edwin Meriaux",
        "Mariana Sosa GuzmÃ¡n",
        "Zhizun Wang",
        "Junming Shi",
        "Gregory Dudek"
      ],
      "abstract": "Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.",
      "tldr_zh": "é’ˆå¯¹æ°´ä¸‹æœºå™¨äººé¢ä¸´çš„ Global Navigation Satellite System (GNSS) ä¿¡å·ä¸å¯ç”¨ä»¥åŠä¼ ç»Ÿå®šä½æ–¹æ³•ç´¯ç§¯è¯¯å·®å¤§ã€è®¡ç®—æˆæœ¬é«˜æˆ–ä¾èµ–åŸºç¡€è®¾æ–½çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é€‚ç”¨äºæ°´é¢åŠè¿‘æ°´é¢æœºå™¨äººçš„å¯æ‰©å±•å¤šæ— äººæœº GNSS è·Ÿè¸ªç³»ç»Ÿã€‚è¯¥æ–¹æ¡ˆé›†æˆäº†é«˜æ•ˆçš„è§†è§‰æ£€æµ‹ã€è½»é‡çº§å¤šç›®æ ‡è·Ÿè¸ª (Multi-object tracking) ä»¥åŠåŸºäº GNSS çš„ä¸‰è§’æµ‹é‡æŠ€æœ¯ï¼Œå¹¶é‡‡ç”¨ç½®ä¿¡åº¦åŠ æƒçš„æ‰©å±•å¡å°”æ›¼æ»¤æ³¢ (Extended Kalman Filter, EKF) æ¥æä¾›å®æ—¶çš„ç¨³å®šå®šä½ä¼°è®¡ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†è·¨æ— äººæœºè·Ÿè¸ª ID å¯¹é½ç®—æ³•ï¼Œç¡®ä¿äº†å…¨å±€è§†è§’çš„ ID ä¸€è‡´æ€§ï¼Œä»è€Œå®ç°äº†å…·å¤‡å†—ä½™è¦†ç›–èƒ½åŠ›çš„é²æ£’å¤šæœºå™¨äººè·Ÿè¸ªã€‚å®éªŒåœ¨å¤šç§å¤æ‚ç¯å¢ƒä¸‹éªŒè¯äº†è¯¥ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†æ‰€æç®—æ³•åœ¨æµ·æ´‹æœºå™¨äººå®šä½ä»»åŠ¡ä¸­çš„å¯æ‰©å±•æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18694v1",
      "published_date": "2025-11-24 02:28:31 UTC",
      "updated_date": "2025-11-24 02:28:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:21:56.617225+00:00"
    },
    {
      "arxiv_id": "2511.18692v1",
      "title": "VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking",
      "title_zh": "é—ªå­˜ç¯å¢ƒä¸‹çš„ VLMï¼šåŸºäºç¥ç»å…ƒåˆ†å—çš„è§†è§‰è¯­è¨€æ¨¡å‹ I/O é«˜æ•ˆç¨€ç–åŒ–",
      "authors": [
        "Kichang Yang",
        "Seonjun Kim",
        "Minjae Kim",
        "Nairan Zhang",
        "Chi Zhang",
        "Youngki Lee"
      ],
      "abstract": "Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¸­é¢ä¸´çš„I/Oå¼€é”€é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„æ¿€æ´»ç¨€ç–åŒ–(Activation Sparsification)æ–¹æ³•ä»…å…³æ³¨ç¥ç»å…ƒæ¿€æ´»å¼ºåº¦ï¼Œè€Œå¿½ç•¥äº†å­˜å‚¨è®¿é—®æ¨¡å¼å¯¹é—ªå­˜æ€§èƒ½çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Neuron Chunkingï¼Œè¿™æ˜¯ä¸€ç§I/Oé«˜æ•ˆçš„ç¨€ç–åŒ–ç­–ç•¥ï¼Œé€šè¿‡å°†å†…å­˜ä¸­è¿ç»­çš„ç¥ç»å…ƒè¿›è¡Œåˆ†å—å¤„ç†ã€‚è¯¥æ–¹æ³•å°†ç¥ç»å…ƒçš„é‡è¦æ€§ä¸å­˜å‚¨è®¿é—®æˆæœ¬ç›¸ç»“åˆï¼Œåˆ©ç”¨è½»é‡çº§çš„æŠ½è±¡æ¨¡å‹æ¥è¯„ä¼°è®¿é—®è¿ç»­æ€§å¸¦æ¥çš„I/Oå»¶è¿Ÿã€‚é€šè¿‡é€‰æ‹©å…·æœ‰é«˜â€œæ•ˆç”¨â€ï¼ˆå³ç¥ç»å…ƒé‡è¦æ€§é™¤ä»¥é¢„ä¼°å»¶è¿Ÿï¼‰çš„å—ï¼ŒNeuron ChunkingæˆåŠŸå°†ç¨€ç–åŒ–å†³ç­–ä¸åº•å±‚å­˜å‚¨è¡Œä¸ºå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Jetson Orin Nanoå’ŒJetson AGX Orinå¹³å°ä¸Šåˆ†åˆ«å®ç°äº†é«˜è¾¾4.65å€å’Œ5.76å€çš„I/Oæ•ˆç‡æå‡ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†è¾¹ç¼˜ç«¯VLMçš„æ¨ç†æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18692v1",
      "published_date": "2025-11-24 02:27:19 UTC",
      "updated_date": "2025-11-24 02:27:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:22:10.271500+00:00"
    },
    {
      "arxiv_id": "2511.20695v1",
      "title": "A Brief History of Digital Twin Technology",
      "title_zh": "æ•°å­—å­ªç”ŸæŠ€æœ¯ç®€å²",
      "authors": [
        "Yunqi Zhang",
        "Kuangyu Shi",
        "Biao Li"
      ],
      "abstract": "Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.",
      "tldr_zh": "è¯¥ç ”ç©¶å›é¡¾äº†æ•°å­—å­ªç”Ÿ(Digital Twin)æŠ€æœ¯ä»20ä¸–çºª60å¹´ä»£NASAèˆªå¤©æ¨¡æ‹Ÿèµ·æºåˆ°å·¥ä¸šåº”ç”¨ï¼Œå†åˆ°åŒ»ç–—é¢†åŸŸå˜é©çš„å‘å±•å†ç¨‹ã€‚æ•°å­—å­ªç”Ÿè¢«å®šä¹‰ä¸ºç‰©ç†ç³»ç»Ÿçš„åŠ¨æ€ã€æ•°æ®é©±åŠ¨è™šæ‹Ÿå‰¯æœ¬ï¼Œé€šè¿‡å®æ—¶æ•°æ®æµæŒç»­æ›´æ–°å¹¶å®ç°åŒå‘äº¤äº’ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œè¯¥æŠ€æœ¯æ•´åˆå½±åƒå­¦ã€ç”Ÿç‰©ä¼ æ„Ÿå™¨å’Œè®¡ç®—æ¨¡å‹ï¼Œç”Ÿæˆæ‚£è€…ç‰¹å¼‚æ€§æ¨¡æ‹Ÿä»¥æ”¯æŒè¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œè¯ç‰©ç ”å‘ã€‚è®ºæ–‡é‡ç‚¹æ¢è®¨äº†å¿ƒè„ã€è‚¿ç˜¤åŠè¯ç†æ•°å­—å­ªç”Ÿåœ¨é¢„æµ‹æ²»ç–—æ•ˆæœå’Œä¼˜åŒ–æ–¹æ¡ˆä¸­çš„ä»£è¡¨æ€§åº”ç”¨ã€‚å°½ç®¡ç›®å‰é¢ä¸´äº’æ“ä½œæ€§(interoperability)ã€æ•°æ®éšç§å’Œæ¨¡å‹ä¿çœŸåº¦(model fidelity)ç­‰æŒ‘æˆ˜ï¼Œä½†å¯è§£é‡Šäººå·¥æ™ºèƒ½(explainable AI)å’Œè”é‚¦å­¦ä¹ (federated learning)ç­‰æŠ€æœ¯æä¾›äº†è§£å†³è·¯å¾„ã€‚æœªæ¥ï¼Œéšç€å¤šå™¨å®˜æ•°å­—å­ªç”Ÿå’ŒåŸºå› ç»„å­¦æ•´åˆçš„è¿›æ­¥ï¼Œè¯¥æŠ€æœ¯å°†æ¨åŠ¨åŒ»ç–—ä»ååº”å¼æ²»ç–—è½¬å‘é¢„æµ‹æ€§ã€é¢„é˜²æ€§å’ŒçœŸæ­£çš„ä¸ªæ€§åŒ–åŒ»ç–—ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "physics.med-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 1 figure, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2511.20695v1",
      "published_date": "2025-11-24 02:03:55 UTC",
      "updated_date": "2025-11-24 02:03:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:22:09.369125+00:00"
    },
    {
      "arxiv_id": "2511.18676v1",
      "title": "MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis",
      "title_zh": "MedVisionï¼šå®šé‡åŒ»å­¦å›¾åƒåˆ†æçš„æ•°æ®é›†ä¸åŸºå‡†",
      "authors": [
        "Yongcheng Yao",
        "Yongshuo Zong",
        "Raman Dutt",
        "Yongxin Yang",
        "Sotirios A Tsaftaris",
        "Timothy Hospedales"
      ],
      "abstract": "Current vision-language models (VLMs) in medicine are primarily designed for categorical question answering (e.g., \"Is this normal or abnormal?\") or qualitative descriptive tasks. However, clinical decision-making often relies on quantitative assessments, such as measuring the size of a tumor or the angle of a joint, from which physicians draw their own diagnostic conclusions. This quantitative reasoning capability remains underexplored and poorly supported in existing VLMs. In this work, we introduce MedVision, a large-scale dataset and benchmark specifically designed to evaluate and improve VLMs on quantitative medical image analysis. MedVision spans 22 public datasets covering diverse anatomies and modalities, with 30.8 million image-annotation pairs. We focus on three representative quantitative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs perform poorly on these tasks. However, with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement, demonstrating reduced error rates and improved precision. This work provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging. Code and data are available at https://medvision-vlm.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨åŒ»å­¦é¢†åŸŸä¾§é‡å®šæ€§æè¿°è€Œç¼ºä¹å®šé‡åˆ†æèƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†MedVisionï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºå®šé‡åŒ»å­¦å›¾åƒåˆ†æçš„å¤§è§„æ¨¡æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚MedVisionæ•´åˆäº†22ä¸ªå…¬å¼€æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§è§£å‰–ç»“æ„å’Œæ¨¡æ€ï¼ŒåŒ…å«3080ä¸‡ä¸ªå›¾åƒ-æ ‡æ³¨å¯¹(image-annotation pairs)ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨è§£å‰–ç»“æ„ä¸å¼‚å¸¸æ£€æµ‹ã€è‚¿ç˜¤/ç—…ç¶(T/L)å°ºå¯¸ä¼°ç®—ä»¥åŠè§’åº¦/è·ç¦»(A/D)æµ‹é‡è¿™ä¸‰ç±»ä»£è¡¨æ€§å®šé‡ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„é€šç”¨VLMsåœ¨è¿™äº›å®šé‡ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå·®ï¼Œä½†é€šè¿‡åœ¨MedVisionä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning)ï¼Œæ¨¡å‹åœ¨æ£€æµ‹ç²¾åº¦å’Œè¯¯å·®æ§åˆ¶æ–¹é¢å‡å®ç°äº†æ˜¾è‘—æå‡ã€‚è¯¥é¡¹å·¥ä½œä¸ºå¼€å‘å…·å¤‡ç¨³å¥å®šé‡æ¨ç†èƒ½åŠ›çš„åŒ»å­¦å½±åƒVLMså¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 8 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.18676v1",
      "published_date": "2025-11-24 01:26:07 UTC",
      "updated_date": "2025-11-24 01:26:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:22:14.456062+00:00"
    },
    {
      "arxiv_id": "2511.18674v1",
      "title": "Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration",
      "title_zh": "Low-Rank GEMMï¼šåŸºäºä½ç§©è¿‘ä¼¼ä¸ FP8 åŠ é€Ÿçš„é«˜æ•ˆçŸ©é˜µä¹˜æ³•",
      "authors": [
        "Alfredo Metere"
      ],
      "abstract": "Large matrix multiplication is a cornerstone of modern machine learning workloads, yet traditional approaches suffer from cubic computational complexity (e.g., $\\mathcal{O}(n^3)$ for a matrix of size $n\\times n$). We present Low-Rank GEMM, a novel approach that leverages low-rank matrix approximations to achieve sub-quadratic complexity while maintaining hardware-accelerated performance through FP8 precision and intelligent kernel selection. On a NVIDIA RTX 4090, our implementation achieves up to 378 TFLOPS on matrices up to $N=20480$, providing 75\\% memory savings and $7.8\\times$ speedup over PyTorch FP32 for large matrices. The system automatically adapts to hardware capabilities, selecting optimal decomposition methods (SVD, randomized SVD) and precision levels based on matrix characteristics and available accelerators. Comprehensive benchmarking on NVIDIA RTX 4090 demonstrates that Low-Rank GEMM becomes the fastest approach for matrices $N\\geq10240$, surpassing traditional cuBLAS implementations through memory bandwidth optimization rather than computational shortcuts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Low-Rank GEMMï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡çŸ©é˜µä¹˜æ³•ä¸­ä¼ ç»Ÿæ–¹æ³•é¢ä¸´çš„ç«‹æ–¹çº§è®¡ç®—å¤æ‚åº¦é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä½ç§©çŸ©é˜µè¿‘ä¼¼(Low-Rank Matrix Approximations)æŠ€æœ¯ï¼Œåœ¨å®ç°äºšäºŒæ¬¡æ–¹å¤æ‚åº¦çš„åŒæ—¶ï¼Œé€šè¿‡ FP8 ç²¾åº¦å’Œæ™ºèƒ½ç®—å­é€‰æ‹©(Intelligent Kernel Selection)ä¿æŒäº†ç¡¬ä»¶åŠ é€Ÿæ€§èƒ½ã€‚ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®çŸ©é˜µç‰¹æ€§è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„åˆ†è§£æ–¹æ³•ï¼Œå¦‚ SVD æˆ– Randomized SVDï¼Œå¹¶é€‚é…ç›¸åº”çš„ç²¾åº¦ç­‰çº§ã€‚åœ¨ NVIDIA RTX 4090 ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œè¯¥å®ç°åœ¨å¤„ç†è§„æ¨¡è¾¾ $N=20480$ çš„çŸ©é˜µæ—¶å¯è¾¾åˆ° 378 TFLOPS çš„æ€§èƒ½ã€‚ä¸ PyTorch FP32 ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å®ç°äº† $7.8\\times$ çš„åŠ é€Ÿï¼Œå¹¶èŠ‚çœäº† 75% çš„å†…å­˜å ç”¨ã€‚ç»¼åˆåŸºå‡†æµ‹è¯•è¯æ˜ï¼Œå½“çŸ©é˜µè§„æ¨¡ $N \\geq 10240$ æ—¶ï¼ŒLow-Rank GEMM é€šè¿‡å†…å­˜å¸¦å®½ä¼˜åŒ–è€Œéè®¡ç®—å¿«æ·æ–¹å¼ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ cuBLAS å®ç°ã€‚è¯¥æ–¹æ¡ˆä¸ºç°ä»£æœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½æä¾›äº†ä¸€ç§å…¼é¡¾è®¡ç®—æ•ˆç‡ä¸å­˜å‚¨ä¼˜åŒ–çš„å¿«é€ŸçŸ©é˜µä¹˜æ³•è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.PF",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.PF",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.18674v1",
      "published_date": "2025-11-24 01:13:52 UTC",
      "updated_date": "2025-11-24 01:13:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:23:51.778924+00:00"
    },
    {
      "arxiv_id": "2511.18670v1",
      "title": "Deterministic Continuous Replacement: Fast and Stable Module Replacement in Pretrained Transformers",
      "title_zh": "ç¡®å®šæ€§è¿ç»­æ›¿æ¢ï¼šé¢„è®­ç»ƒ Transformer ä¸­å¿«é€Ÿä¸”ç¨³å®šçš„æ¨¡å—æ›¿æ¢",
      "authors": [
        "Rowan Bradbury",
        "Aniket Srinivasan Ashok",
        "Sai Ram Kasanagottu",
        "Gunmay Jhingran",
        "Shuai Meng"
      ],
      "abstract": "Replacing modules in pretrained models, especially swapping quadratic self-attention for efficient attention alternatives, poses a hard optimization problem: cold-start reinitialization destabilizes frozen backbones. We isolate this core stability challenge in a controlled study. Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight. Theoretically, DCR eliminates gate-induced gradient variance inherent to stochastic replacement. In a single-seed study, DCR attains faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement, establishing a foundation for heterogeneous operator swaps.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨é¢„è®­ç»ƒ Transformers æ¨¡å‹ä¸­æ›¿æ¢æ¨¡å—ï¼ˆå¦‚å°†äºŒæ¬¡è‡ªæ³¨æ„åŠ›æ›¿æ¢ä¸ºé«˜æ•ˆæ³¨æ„åŠ›æ›¿ä»£å“ï¼‰æ—¶ï¼Œç”±äºå†·å¯åŠ¨é‡æ–°åˆå§‹åŒ–å¯¼è‡´æ¨¡å‹ä¸ç¨³å®šçš„æŒ‘æˆ˜ï¼Œæå‡ºäº† Deterministic Continuous Replacement (DCR) æ–¹æ³•ã€‚DCR é€šè¿‡ä½¿ç”¨ç¡®å®šæ€§çš„é€€ç«æƒé‡ (annealed weight) æ¥èåˆæ•™å¸ˆæ¨¡å‹ä¸å­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºï¼Œä»è€Œå®ç°å¹³æ»‘è¿‡æ¸¡ã€‚åœ¨ç†è®ºå±‚é¢ï¼ŒDCR èƒ½å¤Ÿæ¶ˆé™¤éšæœºæ›¿æ¢æ–¹æ³•ä¸­å›ºæœ‰çš„é—¨æ§è¯±å¯¼æ¢¯åº¦æ–¹å·® (gate-induced gradient variance)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å—æ§çš„æ³¨æ„åŠ›æ¨¡å—æ›¿æ¢ä»»åŠ¡ä¸­ï¼ŒDCR æ¯”éšæœºé—¨æ§ (stochastic gating) å’Œè’¸é¦ (distillation) åŸºå‡†æ–¹æ³•è¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å¼ºçš„æ¨¡å‹å¯¹é½æ•ˆæœã€‚è¯¥æ–¹æ³•ä¸ºå®ç°é¢„è®­ç»ƒæ¨¡å‹ä¸­å¼‚æ„ç®—å­ (heterogeneous operator swaps) çš„é«˜æ•ˆã€ç¨³å®šæ›¿æ¢å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2025 ScaleOPT Workshop; 8 pages; includes figures",
      "pdf_url": "https://arxiv.org/pdf/2511.18670v1",
      "published_date": "2025-11-24 00:55:14 UTC",
      "updated_date": "2025-11-24 00:55:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T10:22:22.182753+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 200,
  "processed_papers_count": 200,
  "failed_papers_count": 0,
  "llm_backup_calls": 5,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T10:25:05.110688+00:00"
}
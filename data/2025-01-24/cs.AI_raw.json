[
  {
    "arxiv_id": "2501.14980v1",
    "title": "A Deep State Space Model for Rainfall-Runoff Simulations",
    "authors": [
      "Yihan Wang",
      "Lujun Zhang",
      "Annan Yu",
      "N. Benjamin Erichson",
      "Tiantian Yang"
    ],
    "abstract": "The classical way of studying the rainfall-runoff processes in the water\ncycle relies on conceptual or physically-based hydrologic models. Deep learning\n(DL) has recently emerged as an alternative and blossomed in hydrology\ncommunity for rainfall-runoff simulations. However, the decades-old Long\nShort-Term Memory (LSTM) network remains the benchmark for this task,\noutperforming newer architectures like Transformers. In this work, we propose a\nState Space Model (SSM), specifically the Frequency Tuned Diagonal State Space\nSequence (S4D-FT) model, for rainfall-runoff simulations. The proposed S4D-FT\nis benchmarked against the established LSTM and a physically-based Sacramento\nSoil Moisture Accounting model across 531 watersheds in the contiguous United\nStates (CONUS). Results show that S4D-FT is able to outperform the LSTM model\nacross diverse regions. Our pioneering introduction of the S4D-FT for\nrainfall-runoff simulations challenges the dominance of LSTM in the hydrology\ncommunity and expands the arsenal of DL tools available for hydrological\nmodeling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14980v1",
    "published_date": "2025-01-24 23:31:42 UTC",
    "updated_date": "2025-01-24 23:31:42 UTC"
  },
  {
    "arxiv_id": "2501.14970v1",
    "title": "AI-driven Wireless Positioning: Fundamentals, Standards, State-of-the-art, and Challenges",
    "authors": [
      "Guangjin Pan",
      "Yuan Gao",
      "Yilin Gao",
      "Zhiyong Zhong",
      "Xiaoyu Yang",
      "Xinyu Guo",
      "Shugong Xu"
    ],
    "abstract": "Wireless positioning technologies hold significant value for applications in\nautonomous driving, extended reality (XR), unmanned aerial vehicles (UAVs), and\nmore. With the advancement of artificial intelligence (AI), leveraging AI to\nenhance positioning accuracy and robustness has emerged as a field full of\npotential. Driven by the requirements and functionalities defined in the 3rd\nGeneration Partnership Project (3GPP) standards, AI/machine learning (ML)-based\npositioning is becoming a key technology to overcome the limitations of\ntraditional methods. This paper begins with an introduction to the fundamentals\nof AI and wireless positioning, covering AI models, algorithms, positioning\napplications, emerging wireless technologies, and the basics of positioning\ntechniques. Subsequently, focusing on standardization progress, we provide a\ncomprehensive review of the evolution of 3GPP positioning standards, with an\nemphasis on the integration of AI/ML technologies in recent and upcoming\nreleases. Based on the AI/ML-assisted positioning and direct AI/ML positioning\nschemes outlined in the standards, we conduct an in-depth investigation of\nrelated research. we focus on state-of-the-art (SOTA) research in AI-based\nline-of-sight (LOS)/non-line-of-sight (NLOS) detection, time of arrival\n(TOA)/time difference of arrival (TDOA) estimation, and angle estimation\ntechniques. For Direct AI/ML Positioning, we explore SOTA advancements in\nfingerprint-based positioning, knowledge-assisted AI positioning, and channel\ncharting-based positioning. Furthermore, we introduce publicly available\ndatasets for wireless positioning and conclude by summarizing the challenges\nand opportunities of AI-driven wireless positioning.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "32 pages. This work has been submitted to the IEEE for possible\n  publication",
    "pdf_url": "http://arxiv.org/pdf/2501.14970v1",
    "published_date": "2025-01-24 23:09:11 UTC",
    "updated_date": "2025-01-24 23:09:11 UTC"
  },
  {
    "arxiv_id": "2502.14869v1",
    "title": "Envisioning Stakeholder-Action Pairs to Mitigate Negative Impacts of AI: A Participatory Approach to Inform Policy Making",
    "authors": [
      "Julia Barnett",
      "Kimon Kieslich",
      "Natali Helberger",
      "Nicholas Diakopoulos"
    ],
    "abstract": "The potential for negative impacts of AI has rapidly become more pervasive\naround the world, and this has intensified a need for responsible AI\ngovernance. While many regulatory bodies endorse risk-based approaches and a\nmultitude of risk mitigation practices are proposed by companies and academic\nscholars, these approaches are commonly expert-centered and thus lack the\ninclusion of a significant group of stakeholders. Ensuring that AI policies\nalign with democratic expectations requires methods that prioritize the voices\nand needs of those impacted. In this work we develop a participative and\nforward-looking approach to inform policy-makers and academics that grounds the\nneeds of lay stakeholders at the forefront and enriches the development of risk\nmitigation strategies. Our approach (1) maps potential mitigation and\nprevention strategies of negative AI impacts that assign responsibility to\nvarious stakeholders, (2) explores the importance and prioritization thereof in\nthe eyes of laypeople, and (3) presents these insights in policy fact sheets,\ni.e., a digestible format for informing policy processes. We emphasize that\nthis approach is not targeted towards replacing policy-makers; rather our aim\nis to present an informative method that enriches mitigation strategies and\nenables a more participatory approach to policy development.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "14 pages + supplementary information and appendix",
    "pdf_url": "http://arxiv.org/pdf/2502.14869v1",
    "published_date": "2025-01-24 22:57:18 UTC",
    "updated_date": "2025-01-24 22:57:18 UTC"
  },
  {
    "arxiv_id": "2501.14960v2",
    "title": "LLM4DistReconfig: A Fine-tuned Large Language Model for Power Distribution Network Reconfiguration",
    "authors": [
      "Panayiotis Christou",
      "Md. Zahidul Islam",
      "Yuzhang Lin",
      "Jingwei Xiong"
    ],
    "abstract": "Power distribution networks are evolving due to the integration of DERs and\nincreased customer participation. To maintain optimal operation, minimize\nlosses, and meet varying load demands, frequent network reconfiguration is\nnecessary. Traditionally, the reconfiguration task relies on optimization\nsoftware and expert operators, but as systems grow more complex, faster and\nmore adaptive solutions are required without expert intervention. Data-driven\nreconfiguration is gaining traction for its accuracy, speed, and robustness\nagainst incomplete network data. LLMs, with their ability to capture complex\npatterns, offer a promising approach for efficient and responsive network\nreconfiguration in evolving complex power networks.\n  In this work, we introduce LLM4DistReconfig, a deep learning-based approach\nutilizing a fine-tuned LLM to solve the distribution network reconfiguration\nproblem. By carefully crafting prompts and designing a custom loss function, we\ntrain the LLM with inputs representing network parameters such as buses,\navailable lines, open lines, node voltages, and system loss. The model then\npredicts optimal reconfigurations by outputting updated network configurations\nthat minimize system loss while meeting operational constraints. Our approach\nsignificantly reduces inference time compared to classical algorithms, allowing\nfor near real-time optimal reconfiguration after training. Experimental results\nshow that our method generates optimal configurations minimizing system loss\nfor five individual and a combined test dataset. It also produces minimal\ninvalid edges, no cycles, or subgraphs across all datasets, fulfilling\ndomain-specific needs. Additionally, the generated responses contain less than\n5% improper outputs on seen networks and satisfactory results on unseen\nnetworks, demonstrating its effectiveness and reliability for the\nreconfiguration task.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in NAACL 2025 Conference Main Track",
    "pdf_url": "http://arxiv.org/pdf/2501.14960v2",
    "published_date": "2025-01-24 22:46:14 UTC",
    "updated_date": "2025-02-08 06:13:16 UTC"
  },
  {
    "arxiv_id": "2501.14959v1",
    "title": "The Curious Case of Arbitrariness in Machine Learning",
    "authors": [
      "Prakhar Ganesh",
      "Afaf Taik",
      "Golnoosh Farnadi"
    ],
    "abstract": "Algorithmic modelling relies on limited information in data to extrapolate\noutcomes for unseen scenarios, often embedding an element of arbitrariness in\nits decisions. A perspective on this arbitrariness that has recently gained\ninterest is multiplicity-the study of arbitrariness across a set of \"good\nmodels\", i.e., those likely to be deployed in practice. In this work, we\nsystemize the literature on multiplicity by: (a) formalizing the terminology\naround model design choices and their contribution to arbitrariness, (b)\nexpanding the definition of multiplicity to incorporate underrepresented forms\nbeyond just predictions and explanations, (c) clarifying the distinction\nbetween multiplicity and other traditional lenses of arbitrariness, i.e.,\nuncertainty and variance, and (d) distilling the benefits and potential risks\nof multiplicity into overarching trends, situating it within the broader\nlandscape of responsible AI. We conclude by identifying open research questions\nand highlighting emerging trends in this young but rapidly growing area of\nresearch.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14959v1",
    "published_date": "2025-01-24 22:45:09 UTC",
    "updated_date": "2025-01-24 22:45:09 UTC"
  },
  {
    "arxiv_id": "2501.14956v1",
    "title": "ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation",
    "authors": [
      "Alireza Salemi",
      "Julian Killingback",
      "Hamed Zamani"
    ],
    "abstract": "Evaluating personalized text generated by large language models (LLMs) is\nchallenging, as only the LLM user, i.e., prompt author, can reliably assess the\noutput, but re-engaging the same individuals across studies is infeasible. This\npaper addresses the challenge of evaluating personalized text generation by\nintroducing ExPerT, an explainable reference-based evaluation framework. ExPerT\nleverages an LLM to extract atomic aspects and their evidence from the\ngenerated and reference texts, match the aspects, and evaluate their alignment\nbased on content and writing style -- two key attributes in personalized text\ngeneration. Additionally, ExPerT generates detailed, fine-grained explanations\nfor every step of the evaluation process, enhancing transparency and\ninterpretability. Our experiments demonstrate that ExPerT achieves a 7.2%\nrelative improvement in alignment with human judgments compared to the\nstate-of-the-art text generation evaluation methods. Furthermore, human\nevaluators rated the usability of ExPerT's explanations at 4.7 out of 5,\nhighlighting its effectiveness in making evaluation decisions more\ninterpretable.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14956v1",
    "published_date": "2025-01-24 22:44:22 UTC",
    "updated_date": "2025-01-24 22:44:22 UTC"
  },
  {
    "arxiv_id": "2502.08655v1",
    "title": "Personalizing Education through an Adaptive LMS with Integrated LLMs",
    "authors": [
      "Kyle Spriggs",
      "Meng Cheng Lau",
      "Kalpdrum Passi"
    ],
    "abstract": "The widespread adoption of large language models (LLMs) marks a\ntransformative era in technology, especially within the educational sector.\nThis paper explores the integration of LLMs within learning management systems\n(LMSs) to develop an adaptive learning management system (ALMS) personalized\nfor individual learners across various educational stages. Traditional LMSs,\nwhile facilitating the distribution of educational materials, fall short in\naddressing the nuanced needs of diverse student populations, particularly in\nsettings with limited instructor availability. Our proposed system leverages\nthe flexibility of AI to provide a customizable learning environment that\nadjusts to each user's evolving needs. By integrating a suite of\ngeneral-purpose and domain-specific LLMs, this system aims to minimize common\nissues such as factual inaccuracies and outdated information, characteristic of\ngeneral LLMs like OpenAI's ChatGPT. This paper details the development of an\nALMS that not only addresses privacy concerns and the limitations of existing\neducational tools but also enhances the learning experience by maintaining\nengagement through personalized educational content.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.08655v1",
    "published_date": "2025-01-24 22:42:57 UTC",
    "updated_date": "2025-01-24 22:42:57 UTC"
  },
  {
    "arxiv_id": "2501.14954v1",
    "title": "MISCON: A Mission-Driven Conversational Consultant for Pre-Venture Entrepreneurs in Food Deserts",
    "authors": [
      "Subhasis Dasgupta",
      "Hans Taparia",
      "Laura Schmidt",
      "Amarnath Gupta"
    ],
    "abstract": "This work-in-progress report describes MISCON, a conversational consultant\nbeing developed for a public mission project called NOURISH. With MISCON,\naspiring small business owners in a food-insecure region and their advisors in\nCommunity-based organizations would be able to get information, recommendation\nand analysis regarding setting up food businesses. MISCON conversations are\nmodeled as state machine that uses a heterogeneous knowledge graph as well as\nseveral analytical tools and services including a variety of LLMs. In this\nshort report, we present the functional architecture and some design\nconsiderations behind MISCON.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "K.4.2; I.2.7; H.5.2"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages. Acccepted for AAAI 2025 Workshop on AI for Public Missions,\n  March 3rd, 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14954v1",
    "published_date": "2025-01-24 22:39:49 UTC",
    "updated_date": "2025-01-24 22:39:49 UTC"
  },
  {
    "arxiv_id": "2501.14942v1",
    "title": "Force-Based Robotic Imitation Learning: A Two-Phase Approach for Construction Assembly Tasks",
    "authors": [
      "Hengxu You",
      "Yang Ye",
      "Tianyu Zhou",
      "Jing Du"
    ],
    "abstract": "The drive for efficiency and safety in construction has boosted the role of\nrobotics and automation. However, complex tasks like welding and pipe insertion\npose challenges due to their need for precise adaptive force control, which\ncomplicates robotic training. This paper proposes a two-phase system to improve\nrobot learning, integrating human-derived force feedback. The first phase\ncaptures real-time data from operators using a robot arm linked with a virtual\nsimulator via ROS-Sharp. In the second phase, this feedback is converted into\nrobotic motion instructions, using a generative approach to incorporate force\nfeedback into the learning process. This method's effectiveness is demonstrated\nthrough improved task completion times and success rates. The framework\nsimulates realistic force-based interactions, enhancing the training data's\nquality for precise robotic manipulation in construction tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "36 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.14942v1",
    "published_date": "2025-01-24 22:01:23 UTC",
    "updated_date": "2025-01-24 22:01:23 UTC"
  },
  {
    "arxiv_id": "2501.14940v3",
    "title": "CASE-Bench: Context-Aware SafEty Benchmark for Large Language Models",
    "authors": [
      "Guangzhi Sun",
      "Xiao Zhan",
      "Shutong Feng",
      "Philip C. Woodland",
      "Jose Such"
    ],
    "abstract": "Aligning large language models (LLMs) with human values is essential for\ntheir safe deployment and widespread adoption. Current LLM safety benchmarks\noften focus solely on the refusal of individual problematic queries, which\noverlooks the importance of the context where the query occurs and may cause\nundesired refusal of queries under safe contexts that diminish user experience.\nAddressing this gap, we introduce CASE-Bench, a Context-Aware SafEty Benchmark\nthat integrates context into safety assessments of LLMs. CASE-Bench assigns\ndistinct, formally described contexts to categorized queries based on\nContextual Integrity theory. Additionally, in contrast to previous studies\nwhich mainly rely on majority voting from just a few annotators, we recruited a\nsufficient number of annotators necessary to ensure the detection of\nstatistically significant differences among the experimental conditions based\non power analysis. Our extensive analysis using CASE-Bench on various\nopen-source and commercial LLMs reveals a substantial and significant influence\nof context on human judgments (p<0.0001 from a z-test), underscoring the\nnecessity of context in safety evaluations. We also identify notable mismatches\nbetween human judgments and LLM responses, particularly in commercial models\nwithin safe contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.14940v3",
    "published_date": "2025-01-24 21:55:14 UTC",
    "updated_date": "2025-02-07 10:23:16 UTC"
  },
  {
    "arxiv_id": "2501.14936v2",
    "title": "Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing",
    "authors": [
      "David Boldo",
      "Lily Pemberton",
      "Gabriel Thistledown",
      "Jacob Fairchild",
      "Felix Kowalski"
    ],
    "abstract": "The integration of contextual embeddings into the optimization processes of\nlarge language models is an advancement in natural language processing. The\nContext-Aware Neural Gradient Mapping framework introduces a dynamic gradient\nadjustment mechanism, incorporating contextual embeddings directly into the\noptimization process. This approach facilitates real-time parameter\nadjustments, enhancing task-specific generalization even in the presence of\nsparse or noisy data inputs. The mathematical foundation of this framework\nrelies on gradient descent modifications, where contextual embeddings are\nderived from a supplementary neural network trained to map input features to\noptimal adaptation gradients. By employing differential geometry principles,\nhigh-dimensional input dependencies are encoded into low-dimensional gradient\nmanifolds, enabling efficient adaptation without necessitating the retraining\nof the entire model. Empirical evaluations demonstrate that the proposed\nframework consistently outperforms baseline models across various metrics,\nincluding accuracy, robustness to noise, and computational efficiency. The\nintegration of context-specific embeddings allows for a more complex\nunderstanding of language, thereby improving the model's ability to handle\ndiverse linguistic phenomena. Furthermore, the computational efficiency\nachieved through this method demonstrates its scalability for large-scale\nlanguage models operating under diverse constraints.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
    "pdf_url": "http://arxiv.org/pdf/2501.14936v2",
    "published_date": "2025-01-24 21:49:24 UTC",
    "updated_date": "2025-04-24 12:49:38 UTC"
  },
  {
    "arxiv_id": "2501.14934v1",
    "title": "Temporal Binding Foundation Model for Material Property Recognition via Tactile Sequence Perception",
    "authors": [
      "Hengxu You",
      "Tianyu Zhou",
      "Jing Du"
    ],
    "abstract": "Robots engaged in complex manipulation tasks require robust material property\nrecognition to ensure adaptability and precision. Traditionally, visual data\nhas been the primary source for object perception; however, it often proves\ninsufficient in scenarios where visibility is obstructed or detailed\nobservation is needed. This gap highlights the necessity of tactile sensing as\na complementary or primary input for material recognition. Tactile data becomes\nparticularly essential in contact-rich, small-scale manipulations where subtle\ndeformations and surface interactions cannot be accurately captured by vision\nalone. This letter presents a novel approach leveraging a temporal binding\nfoundation model for tactile sequence understanding to enhance material\nproperty recognition. By processing tactile sensor data with a temporal focus,\nthe proposed system captures the sequential nature of tactile interactions,\nsimilar to human fingertip perception. Additionally, this letter demonstrates\nthat, through tailored and specific design, the foundation model can more\neffectively capture temporal information embedded in tactile sequences,\nadvancing material property understanding. Experimental results validate the\nmodel's capability to capture these temporal patterns, confirming its utility\nfor material property recognition in visually restricted scenarios. This work\nunderscores the necessity of embedding advanced tactile data processing\nframeworks within robotic systems to achieve truly embodied and responsive\nmanipulation capabilities.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "4 pages,",
    "pdf_url": "http://arxiv.org/pdf/2501.14934v1",
    "published_date": "2025-01-24 21:47:38 UTC",
    "updated_date": "2025-01-24 21:47:38 UTC"
  },
  {
    "arxiv_id": "2501.14932v1",
    "title": "Explaining Categorical Feature Interactions Using Graph Covariance and LLMs",
    "authors": [
      "Cencheng Shen",
      "Darren Edge",
      "Jonathan Larson",
      "Carey E. Priebe"
    ],
    "abstract": "Modern datasets often consist of numerous samples with abundant features and\nassociated timestamps. Analyzing such datasets to uncover underlying events\ntypically requires complex statistical methods and substantial domain\nexpertise. A notable example, and the primary data focus of this paper, is the\nglobal synthetic dataset from the Counter Trafficking Data Collaborative (CTDC)\n-- a global hub of human trafficking data containing over 200,000 anonymized\nrecords spanning from 2002 to 2022, with numerous categorical features for each\nrecord. In this paper, we propose a fast and scalable method for analyzing and\nextracting significant categorical feature interactions, and querying large\nlanguage models (LLMs) to generate data-driven insights that explain these\ninteractions. Our approach begins with a binarization step for categorical\nfeatures using one-hot encoding, followed by the computation of graph\ncovariance at each time. This graph covariance quantifies temporal changes in\ndependence structures within categorical data and is established as a\nconsistent dependence measure under the Bernoulli distribution. We use this\nmeasure to identify significant feature pairs, such as those with the most\nfrequent trends over time or those exhibiting sudden spikes in dependence at\nspecific moments. These extracted feature pairs, along with their timestamps,\nare subsequently passed to an LLM tasked with generating potential explanations\nof the underlying events driving these dependence changes. The effectiveness of\nour method is demonstrated through extensive simulations, and its application\nto the CTDC dataset reveals meaningful feature pairs and potential data stories\nunderlying the observed feature interactions.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "18 pages main + 6 pages appendix",
    "pdf_url": "http://arxiv.org/pdf/2501.14932v1",
    "published_date": "2025-01-24 21:41:26 UTC",
    "updated_date": "2025-01-24 21:41:26 UTC"
  },
  {
    "arxiv_id": "2502.00034v1",
    "title": "Towards Efficient Multi-Objective Optimisation for Real-World Power Grid Topology Control",
    "authors": [
      "Yassine El Manyari",
      "Anton R. Fuxjager",
      "Stefan Zahlner",
      "Joost Van Dijk",
      "Alberto Castagna",
      "Davide Barbieri",
      "Jan Viebahn",
      "Marcel Wasserer"
    ],
    "abstract": "Power grid operators face increasing difficulties in the control room as the\nincrease in energy demand and the shift to renewable energy introduce new\ncomplexities in managing congestion and maintaining a stable supply. Effective\ngrid topology control requires advanced tools capable of handling\nmulti-objective trade-offs. While Reinforcement Learning (RL) offers a\npromising framework for tackling such challenges, existing Multi-Objective\nReinforcement Learning (MORL) approaches fail to scale to the large state and\naction spaces inherent in real-world grid operations. Here we present a\ntwo-phase, efficient and scalable Multi-Objective Optimisation (MOO) method\ndesigned for grid topology control, combining an efficient RL learning phase\nwith a rapid planning phase to generate day-ahead plans for unseen scenarios.\nWe validate our approach using historical data from TenneT, a European\nTransmission System Operator (TSO), demonstrating minimal deployment time,\ngenerating day-ahead plans within 4-7 minutes with strong performance. These\nresults underline the potential of our scalable method to support real-world\npower grid management, offering a practical, computationally efficient, and\ntime-effective tool for operational planning. Based on current congestion costs\nand inefficiencies in grid operations, adopting our approach by TSOs could\npotentially save millions of euros annually, providing a compelling economic\nincentive for its integration in the control room.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00034v1",
    "published_date": "2025-01-24 21:40:19 UTC",
    "updated_date": "2025-01-24 21:40:19 UTC"
  },
  {
    "arxiv_id": "2501.14929v1",
    "title": "Motion-enhancement to Echocardiography Segmentation via Inserting a Temporal Attention Module: An Efficient, Adaptable, and Scalable Approach",
    "authors": [
      "Md. Kamrul Hasan",
      "Guang Yang",
      "Choon Hwai Yap"
    ],
    "abstract": "Cardiac anatomy segmentation is essential for clinical assessment of cardiac\nfunction and disease diagnosis to inform treatment and intervention. In\nperforming segmentation, deep learning (DL) algorithms improved accuracy\nsignificantly compared to traditional image processing approaches. More\nrecently, studies showed that enhancing DL segmentation with motion information\ncan further improve it. A range of methods for injecting motion information has\nbeen proposed, but many of them increase the dimensionality of input images\n(which is computationally expensive) or have not used an optimal method to\ninsert motion information, such as non-DL registration, non-attention-based\nnetworks or single-headed attention. Here, we present a novel,\ncomputation-efficient alternative where a novel, scalable temporal attention\nmodule (TAM) extracts temporal feature interactions multiple times and where\nTAM has a multi-headed, KQV projection cross-attention architecture. The module\ncan be seamlessly integrated into a wide range of existing CNN- or\nTransformer-based networks, providing novel flexibility for inclusion in future\nimplementations. Extensive evaluations on different cardiac datasets, 2D\nechocardiography (CAMUS), and 3D echocardiography (MITEA) demonstrate the\nmodel's effectiveness when integrated into well-established backbone networks\nlike UNet, FCN8s, UNetR, SwinUNetR, and the recent I2UNet. We further find that\nthe optimized TAM-enhanced FCN8s network performs well compared to contemporary\nalternatives. Our results confirm TAM's robustness, scalability, and\ngeneralizability across diverse datasets and backbones.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14929v1",
    "published_date": "2025-01-24 21:35:24 UTC",
    "updated_date": "2025-01-24 21:35:24 UTC"
  },
  {
    "arxiv_id": "2501.14928v1",
    "title": "Decision Making in Changing Environments: Robustness, Query-Based Learning, and Differential Privacy",
    "authors": [
      "Fan Chen",
      "Alexander Rakhlin"
    ],
    "abstract": "We study the problem of interactive decision making in which the underlying\nenvironment changes over time subject to given constraints. We propose a\nframework, which we call \\textit{hybrid Decision Making with Structured\nObservations} (hybrid DMSO), that provides an interpolation between the\nstochastic and adversarial settings of decision making. Within this framework,\nwe can analyze local differentially private (LDP) decision making, query-based\nlearning (in particular, SQ learning), and robust and smooth decision making\nunder the same umbrella, deriving upper and lower bounds based on variants of\nthe Decision-Estimation Coefficient (DEC). We further establish strong\nconnections between the DEC's behavior, the SQ dimension, local minimax\ncomplexity, learnability, and joint differential privacy. To showcase the\nframework's power, we provide new results for contextual bandits under the LDP\nconstraint.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14928v1",
    "published_date": "2025-01-24 21:31:50 UTC",
    "updated_date": "2025-01-24 21:31:50 UTC"
  },
  {
    "arxiv_id": "2503.16431v1",
    "title": "OpenAI's Approach to External Red Teaming for AI Models and Systems",
    "authors": [
      "Lama Ahmad",
      "Sandhini Agarwal",
      "Michael Lampe",
      "Pamela Mishkin"
    ],
    "abstract": "Red teaming has emerged as a critical practice in assessing the possible\nrisks of AI models and systems. It aids in the discovery of novel risks, stress\ntesting possible gaps in existing mitigations, enriching existing quantitative\nsafety metrics, facilitating the creation of new safety measurements, and\nenhancing public trust and the legitimacy of AI risk assessments. This white\npaper describes OpenAI's work to date in external red teaming and draws some\nmore general conclusions from this work. We describe the design considerations\nunderpinning external red teaming, which include: selecting composition of red\nteam, deciding on access levels, and providing guidance required to conduct red\nteaming. Additionally, we show outcomes red teaming can enable such as input\ninto risk assessment and automated evaluations. We also describe the\nlimitations of external red teaming, and how it can fit into a broader range of\nAI model and system evaluations. Through these contributions, we hope that AI\ndevelopers and deployers, evaluation creators, and policymakers will be able to\nbetter design red teaming campaigns and get a deeper look into how external red\nteaming can fit into model deployment and evaluation processes. These methods\nare evolving and the value of different methods continues to shift as the\necosystem around red teaming matures and models themselves improve as tools for\nred teaming.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16431v1",
    "published_date": "2025-01-24 20:54:48 UTC",
    "updated_date": "2025-01-24 20:54:48 UTC"
  },
  {
    "arxiv_id": "2501.14912v1",
    "title": "Feasible Learning",
    "authors": [
      "Juan Ramirez",
      "Ignacio Hounie",
      "Juan Elenter",
      "Jose Gallego-Posada",
      "Meraj Hashemizadeh",
      "Alejandro Ribeiro",
      "Simon Lacoste-Julien"
    ],
    "abstract": "We introduce Feasible Learning (FL), a sample-centric learning paradigm where\nmodels are trained by solving a feasibility problem that bounds the loss for\neach training sample. In contrast to the ubiquitous Empirical Risk Minimization\n(ERM) framework, which optimizes for average performance, FL demands\nsatisfactory performance on every individual data point. Since any model that\nmeets the prescribed performance threshold is a valid FL solution, the choice\nof optimization algorithm and its dynamics play a crucial role in shaping the\nproperties of the resulting solutions. In particular, we study a primal-dual\napproach which dynamically re-weights the importance of each sample during\ntraining. To address the challenge of setting a meaningful threshold in\npractice, we introduce a relaxation of FL that incorporates slack variables of\nminimal norm. Our empirical analysis, spanning image classification, age\nregression, and preference optimization in large language models, demonstrates\nthat models trained via FL can learn from data while displaying improved tail\nbehavior compared to ERM, with only a marginal impact on average performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at AISTATS 2025. Code available at\n  https://github.com/juan43ramirez/feasible-learning",
    "pdf_url": "http://arxiv.org/pdf/2501.14912v1",
    "published_date": "2025-01-24 20:39:38 UTC",
    "updated_date": "2025-01-24 20:39:38 UTC"
  },
  {
    "arxiv_id": "2501.14892v2",
    "title": "Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in Graph-Augmented LLMs",
    "authors": [
      "Hang Luo",
      "Jian Zhang",
      "Chujun Li"
    ],
    "abstract": "In knowledge-intensive tasks, especially in high-stakes domains like medicine\nand law, it is critical not only to retrieve relevant information but also to\nprovide causal reasoning and explainability. Large language models (LLMs) have\nachieved remarkable performance in natural language understanding and\ngeneration tasks. However, they often suffer from limitations such as\ndifficulty in incorporating new knowledge, generating hallucinations, and\nexplaining their reasoning process. To address these challenges, integrating\nknowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has\nemerged as an effective solution. Traditional Graph RAG methods often rely on\nsimple graph traversal or semantic similarity, which do not capture causal\nrelationships or align well with the model's internal reasoning steps. This\npaper proposes a novel pipeline that filters large knowledge graphs to\nemphasize cause-effect edges, aligns the retrieval process with the model's\nchain-of-thought (CoT), and enhances reasoning through multi-stage path\nimprovements. Experiments on medical question-answering tasks show consistent\ngains, with up to a 10\\% absolute improvement across multiple large language\nmodels (LLMs). This approach demonstrates the value of combining causal\nreasoning with stepwise retrieval, leading to more interpretable and logically\ngrounded solutions for complex queries.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.14892v2",
    "published_date": "2025-01-24 19:31:06 UTC",
    "updated_date": "2025-03-17 14:32:08 UTC"
  },
  {
    "arxiv_id": "2501.14719v1",
    "title": "Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?",
    "authors": [
      "Ipek Baris Schlicht",
      "Zhixue Zhao",
      "Burcu Sayin",
      "Lucie Flek",
      "Paolo Rosso"
    ],
    "abstract": "Equitable access to reliable health information is vital for public health,\nbut the quality of online health resources varies by language, raising concerns\nabout inconsistencies in Large Language Models (LLMs) for healthcare. In this\nstudy, we examine the consistency of responses provided by LLMs to\nhealth-related questions across English, German, Turkish, and Chinese. We\nlargely expand the HealthFC dataset by categorizing health-related questions by\ndisease type and broadening its multilingual scope with Turkish and Chinese\ntranslations. We reveal significant inconsistencies in responses that could\nspread healthcare misinformation. Our main contributions are 1) a multilingual\nhealth-related inquiry dataset with meta-information on disease categories, and\n2) a novel prompt-based evaluation workflow that enables sub-dimensional\ncomparisons between two languages through parsing. Our findings highlight key\nchallenges in deploying LLM-based tools in multilingual contexts and emphasize\nthe need for improved cross-lingual alignment to ensure accurate and equitable\nhealthcare information.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages. Short paper appeared at 47th European Conference on\n  Information Retrieval (ECIR 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.14719v1",
    "published_date": "2025-01-24 18:51:26 UTC",
    "updated_date": "2025-01-24 18:51:26 UTC"
  },
  {
    "arxiv_id": "2501.14700v4",
    "title": "An Attentive Graph Agent for Topology-Adaptive Cyber Defence",
    "authors": [
      "Ilya Orson Sandoval",
      "Isaac Symes Thompson",
      "Vasilios Mavroudis",
      "Chris Hicks"
    ],
    "abstract": "As cyber threats grow increasingly sophisticated, reinforcement learning (RL)\nis emerging as a promising technique to create intelligent and adaptive cyber\ndefense systems. However, most existing autonomous defensive agents have\noverlooked the inherent graph structure of computer networks subject to cyber\nattacks, potentially missing critical information and constraining their\nadaptability. To overcome these limitations, we developed a custom version of\nthe Cyber Operations Research Gym (CybORG) environment, encoding network state\nas a directed graph with realistic low-level features. We employ a Graph\nAttention Network (GAT) architecture to process node, edge, and global\nfeatures, and adapt its output to be compatible with policy gradient methods in\nRL. Our GAT-based approach offers key advantages over flattened alternatives:\npolicies that demonstrate resilience to certain types of unexpected dynamic\nnetwork topology changes, reasonable generalisation to networks of varying\nsizes within the same structural distribution, and interpretable defensive\nactions grounded in tangible network properties. We demonstrate that GAT\ndefensive policies can be trained using our low-level directed graph\nobservations, even when unexpected connections arise during simulation.\nEvaluations across networks of different sizes, but consistent subnetwork\nstructure, show our policies achieve comparable performance to policies trained\nspecifically for each network configuration. Our study contributes to the\ndevelopment of robust cyber defence systems that can better adapt to real-world\nnetwork security challenges.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "Draft requires substantial revision",
    "pdf_url": "http://arxiv.org/pdf/2501.14700v4",
    "published_date": "2025-01-24 18:22:37 UTC",
    "updated_date": "2025-04-16 03:11:32 UTC"
  },
  {
    "arxiv_id": "2501.16382v1",
    "title": "GraPPI: A Retrieve-Divide-Solve GraphRAG Framework for Large-scale Protein-protein Interaction Exploration",
    "authors": [
      "Ziwen Li",
      "Xiang 'Anthony' Chen",
      "Youngseung Jeon"
    ],
    "abstract": "Drug discovery (DD) has tremendously contributed to maintaining and improving\npublic health. Hypothesizing that inhibiting protein misfolding can slow\ndisease progression, researchers focus on target identification (Target ID) to\nfind protein structures for drug binding. While Large Language Models (LLMs)\nand Retrieval-Augmented Generation (RAG) frameworks have accelerated drug\ndiscovery, integrating models into cohesive workflows remains challenging. We\nconducted a user study with drug discovery researchers to identify the\napplicability of LLMs and RAGs in Target ID. We identified two main findings:\n1) an LLM should provide multiple Protein-Protein Interactions (PPIs) based on\nan initial protein and protein candidates that have a therapeutic impact; 2)\nthe model must provide the PPI and relevant explanations for better\nunderstanding. Based on these observations, we identified three limitations in\nprevious approaches for Target ID: 1) semantic ambiguity, 2) lack of\nexplainability, and 3) short retrieval units. To address these issues, we\npropose GraPPI, a large-scale knowledge graph (KG)-based retrieve-divide-solve\nagent pipeline RAG framework to support large-scale PPI signaling pathway\nexploration in understanding therapeutic impacts by decomposing the analysis of\nentire PPI pathways into sub-tasks focused on the analysis of PPI edges.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "14 pages; 5 figures. Published as a finding at NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.16382v1",
    "published_date": "2025-01-24 18:16:53 UTC",
    "updated_date": "2025-01-24 18:16:53 UTC"
  },
  {
    "arxiv_id": "2501.14694v1",
    "title": "Towards Automated Self-Supervised Learning for Truly Unsupervised Graph Anomaly Detection",
    "authors": [
      "Zhong Li",
      "Yuhang Wang",
      "Matthijs van Leeuwen"
    ],
    "abstract": "Self-supervised learning (SSL) is an emerging paradigm that exploits\nsupervisory signals generated from the data itself, and many recent studies\nhave leveraged SSL to conduct graph anomaly detection. However, we empirically\nfound that three important factors can substantially impact detection\nperformance across datasets: 1) the specific SSL strategy employed; 2) the\ntuning of the strategy's hyperparameters; and 3) the allocation of combination\nweights when using multiple strategies. Most SSL-based graph anomaly detection\nmethods circumvent these issues by arbitrarily or selectively (i.e., guided by\nlabel information) choosing SSL strategies, hyperparameter settings, and\ncombination weights. While an arbitrary choice may lead to subpar performance,\nusing label information in an unsupervised setting is label information leakage\nand leads to severe overestimation of a method's performance. Leakage has been\ncriticized as \"one of the top ten data mining mistakes\", yet many recent\nstudies on SSL-based graph anomaly detection have been using label information\nto select hyperparameters. To mitigate this issue, we propose to use an\ninternal evaluation strategy (with theoretical analysis) to select\nhyperparameters in SSL for unsupervised anomaly detection. We perform extensive\nexperiments using 10 recent SSL-based graph anomaly detection algorithms on\nvarious benchmark datasets, demonstrating both the prior issues with\nhyperparameter selection and the effectiveness of our proposed strategy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Manuscript submitted to Data Mining and Knowledge Discovery in May\n  2024 for possible publication. This is the revised version submitted in\n  January 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14694v1",
    "published_date": "2025-01-24 18:13:44 UTC",
    "updated_date": "2025-01-24 18:13:44 UTC"
  },
  {
    "arxiv_id": "2501.14693v1",
    "title": "Rethinking Table Instruction Tuning",
    "authors": [
      "Naihao Deng",
      "Rada Mihalcea"
    ],
    "abstract": "Recent advances in table understanding have focused on instruction-tuning\nlarge language models (LLMs) for table-related tasks. However, existing\nresearch has overlooked the impact of hyperparameter choices and lacks a\ncomprehensive evaluation of the out-of-domain table understanding ability and\nthe general capabilities of these table LLMs. In this paper, we evaluate these\nabilities in existing table LLMs, and reveal significant declines in both\nout-of-domain table understanding and general capabilities compared to their\nbase models. Through systematic analysis, we show that hyperparameters, such as\nlearning rate, can significantly influence both table-specific and general\ncapabilities. Contrary to the existing table instruction-tuning works, we\ndemonstrate that smaller learning rates and fewer training instances can\nenhance table understanding while preserving general capabilities. Based on our\nfindings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B\nInstruct, which achieves performance on par with, or surpassing GPT-3.5 and\nGPT-4 on table tasks, while maintaining strong out-of-domain generalization and\ngeneral capabilities. Our findings highlight the potential for reduced data\nannotation costs and more efficient model development through careful\nhyperparameter selection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14693v1",
    "published_date": "2025-01-24 18:06:07 UTC",
    "updated_date": "2025-01-24 18:06:07 UTC"
  },
  {
    "arxiv_id": "2501.14689v1",
    "title": "Approach to Designing CV Systems for Medical Applications: Data, Architecture and AI",
    "authors": [
      "Dmitry Ryabtsev",
      "Boris Vasilyev",
      "Sergey Shershakov"
    ],
    "abstract": "This paper introduces an innovative software system for fundus image analysis\nthat deliberately diverges from the conventional screening approach, opting not\nto predict specific diagnoses. Instead, our methodology mimics the diagnostic\nprocess by thoroughly analyzing both normal and pathological features of fundus\nstructures, leaving the ultimate decision-making authority in the hands of\nhealthcare professionals. Our initiative addresses the need for objective\nclinical analysis and seeks to automate and enhance the clinical workflow of\nfundus image examination. The system, from its overarching architecture to the\nmodular analysis design powered by artificial intelligence (AI) models, aligns\nseamlessly with ophthalmological practices. Our unique approach utilizes a\ncombination of state-of-the-art deep learning methods and traditional computer\nvision algorithms to provide a comprehensive and nuanced analysis of fundus\nstructures. We present a distinctive methodology for designing medical\napplications, using our system as an illustrative example. Comprehensive\nverification and validation results demonstrate the efficacy of our approach in\nrevolutionizing fundus image analysis, with potential applications across\nvarious medical domains.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.14689v1",
    "published_date": "2025-01-24 18:02:32 UTC",
    "updated_date": "2025-01-24 18:02:32 UTC"
  },
  {
    "arxiv_id": "2501.14687v1",
    "title": "Decoding Generalization from Memorization in Deep Neural Networks",
    "authors": [
      "Simran Ketha",
      "Venkatakrishnan Ramaswamy"
    ],
    "abstract": "Overparameterized Deep Neural Networks that generalize well have been key to\nthe dramatic success of Deep Learning in recent years. The reasons for their\nremarkable ability to generalize are not well understood yet. It has also been\nknown that deep networks possess the ability to memorize training data, as\nevidenced by perfect or high training accuracies on models trained with\ncorrupted data that have class labels shuffled to varying degrees.\nConcomitantly, such models are known to generalize poorly, i.e. they suffer\nfrom poor test accuracies, due to which it is thought that the act of\nmemorizing substantially degrades the ability to generalize. It has, however,\nbeen unclear why the poor generalization that accompanies such memorization,\ncomes about. One possibility is that in the process of training with corrupted\ndata, the layers of the network irretrievably reorganize their representations\nin a manner that makes generalization difficult. The other possibility is that\nthe network retains significant ability to generalize, but the trained network\nsomehow chooses to readout in a manner that is detrimental to generalization.\nHere, we provide evidence for the latter possibility by demonstrating,\nempirically, that such models possess information in their representations for\nsubstantially improved generalization, even in the face of memorization.\nFurthermore, such generalization abilities can be easily decoded from the\ninternals of the trained model, and we build a technique to do so from the\noutputs of specific layers of the network. We demonstrate results on multiple\nmodels trained with a number of standard datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14687v1",
    "published_date": "2025-01-24 18:01:27 UTC",
    "updated_date": "2025-01-24 18:01:27 UTC"
  },
  {
    "arxiv_id": "2501.14685v1",
    "title": "Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST",
    "authors": [
      "Fuping Wu",
      "Bartlomiej W. Papiez"
    ],
    "abstract": "Foundation models are widely employed in medical image analysis, due to their\nhigh adaptability and generalizability for downstream tasks. With the\nincreasing number of foundation models being released, model selection has\nbecome an important issue. In this work, we study the capabilities of\nfoundation models in medical image classification tasks by conducting a\nbenchmark study on the MedMNIST dataset. Specifically, we adopt various\nfoundation models ranging from convolutional to Transformer-based models and\nimplement both end-to-end training and linear probing for all classification\ntasks. The results demonstrate the significant potential of these pre-trained\nmodels when transferred for medical image classification. We further conduct\nexperiments with different image sizes and various sizes of training data. By\nanalyzing all the results, we provide preliminary, yet useful insights and\nconclusions on this topic.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "submitted to MIDL2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14685v1",
    "published_date": "2025-01-24 18:01:07 UTC",
    "updated_date": "2025-01-24 18:01:07 UTC"
  },
  {
    "arxiv_id": "2501.14679v5",
    "title": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for Efficient Spherical Manifold Representation",
    "authors": [
      "Rongzhao He",
      "Weihao Zheng",
      "Leilei Zhao",
      "Ying Wang",
      "Dalin Zhu",
      "Dan Wu",
      "Bin Hu"
    ],
    "abstract": "Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14679v5",
    "published_date": "2025-01-24 17:57:06 UTC",
    "updated_date": "2025-02-20 07:37:41 UTC"
  },
  {
    "arxiv_id": "2501.14678v1",
    "title": "A Predictive Approach for Enhancing Accuracy in Remote Robotic Surgery Using Informer Model",
    "authors": [
      "Muhammad Hanif Lashari",
      "Shakil Ahmed",
      "Wafa Batayneh",
      "Ashfaq Khokhar"
    ],
    "abstract": "Precise and real-time estimation of the robotic arm's position on the\npatient's side is essential for the success of remote robotic surgery in\nTactile Internet (TI) environments. This paper presents a prediction model\nbased on the Transformer-based Informer framework for accurate and efficient\nposition estimation. Additionally, it combines a Four-State Hidden Markov Model\n(4-State HMM) to simulate realistic packet loss scenarios. The proposed\napproach addresses challenges such as network delays, jitter, and packet loss\nto ensure reliable and precise operation in remote surgical applications. The\nmethod integrates the optimization problem into the Informer model by embedding\nconstraints such as energy efficiency, smoothness, and robustness into its\ntraining process using a differentiable optimization layer. The Informer\nframework uses features such as ProbSparse attention, attention distilling, and\na generative-style decoder to focus on position-critical features while\nmaintaining a low computational complexity of O(L log L). The method is\nevaluated using the JIGSAWS dataset, achieving a prediction accuracy of over 90\npercent under various network scenarios. A comparison with models such as TCN,\nRNN, and LSTM demonstrates the Informer framework's superior performance in\nhandling position prediction and meeting real-time requirements, making it\nsuitable for Tactile Internet-enabled robotic surgery.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14678v1",
    "published_date": "2025-01-24 17:57:00 UTC",
    "updated_date": "2025-01-24 17:57:00 UTC"
  },
  {
    "arxiv_id": "2501.14673v1",
    "title": "State Space Models for Extractive Summarization in Low Resource Scenarios",
    "authors": [
      "Nisrine Ait Khayi"
    ],
    "abstract": "Extractive summarization involves selecting the most relevant sentences from\na text. Recently, researchers have focused on advancing methods to improve\nstate-of-the-art results in low-resource settings. Motivated by these\nadvancements, we propose the MPoincareSum method. This method applies the Mamba\nstate space model to generate the semantics of reviews and sentences, which are\nthen concatenated. A Poincare compression is used to select the most meaningful\nfeatures, followed by the application of a linear layer to predict sentence\nrelevance based on the corresponding review. Finally, we paraphrase the\nrelevant sentences to create the final summary. To evaluate the effectiveness\nof MPoincareSum, we conducted extensive experiments using the Amazon review\ndataset. The performance of the method was assessed using ROUGE scores. The\nexperimental results demonstrate that MPoincareSum outperforms several existing\napproaches in the literature",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14673v1",
    "published_date": "2025-01-24 17:49:34 UTC",
    "updated_date": "2025-01-24 17:49:34 UTC"
  },
  {
    "arxiv_id": "2501.14661v1",
    "title": "Neural-Symbolic Message Passing with Dynamic Pruning",
    "authors": [
      "Chongzhi Zhang",
      "Junhao Zheng",
      "Zhiping Peng",
      "Qianli Ma"
    ],
    "abstract": "Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a\nchallenging task. Recently, a line of message-passing-based research has been\nproposed to solve CQA. However, they perform unsatisfactorily on negative\nqueries and fail to address the noisy messages between variable nodes in the\nquery graph. Moreover, they offer little interpretability and require complex\nquery data and resource-intensive training. In this paper, we propose a\nNeural-Symbolic Message Passing (NSMP) framework based on pre-trained neural\nlink predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can\ngeneralize to arbitrary existential first order logic queries without requiring\ntraining while providing interpretable answers. Furthermore, we introduce a\ndynamic pruning strategy to filter out noisy messages between variable nodes.\nExperimental results show that NSMP achieves a strong performance.\nAdditionally, through complexity analysis and empirical verification, we\ndemonstrate the superiority of NSMP in inference time over the current\nstate-of-the-art neural-symbolic method. Compared to this approach, NSMP\ndemonstrates faster inference times across all query types on benchmark\ndatasets, with speedup ranging from 2$\\times$ to over 150$\\times$.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 5 figures, 16 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.14661v1",
    "published_date": "2025-01-24 17:30:17 UTC",
    "updated_date": "2025-01-24 17:30:17 UTC"
  },
  {
    "arxiv_id": "2501.14654v2",
    "title": "MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical LLM Agents",
    "authors": [
      "Yixing Jiang",
      "Kameron C. Black",
      "Gloria Geng",
      "Danny Park",
      "James Zou",
      "Andrew Y. Ng",
      "Jonathan H. Chen"
    ],
    "abstract": "Recent large language models (LLMs) have demonstrated significant\nadvancements, particularly in their ability to serve as agents thereby\nsurpassing their traditional role as chatbots. These agents can leverage their\nplanning and tool utilization capabilities to address tasks specified at a high\nlevel. However, a standardized dataset to benchmark the agent capabilities of\nLLMs in medical applications is currently lacking, making the evaluation of\nLLMs on complex tasks in interactive healthcare environments challenging. To\naddress this gap, we introduce MedAgentBench, a broad evaluation suite designed\nto assess the agent capabilities of large language models within medical\nrecords contexts. MedAgentBench encompasses 300 patient-specific\nclinically-derived tasks from 10 categories written by human physicians,\nrealistic profiles of 100 patients with over 700,000 data elements, a\nFHIR-compliant interactive environment, and an accompanying codebase. The\nenvironment uses the standard APIs and communication infrastructure used in\nmodern EMR systems, so it can be easily migrated into live EMR systems.\nMedAgentBench presents an unsaturated agent-oriented benchmark that current\nstate-of-the-art LLMs exhibit some ability to succeed at. The best model\n(Claude 3.5 Sonnet v2) achieves a success rate of 69.67%. However, there is\nstill substantial space for improvement which gives the community a next\ndirection to optimize. Furthermore, there is significant variation in\nperformance across task categories. MedAgentBench establishes this and is\npublicly available at https://github.com/stanfordmlgroup/MedAgentBench ,\noffering a valuable framework for model developers to track progress and drive\ncontinuous improvements in the agent capabilities of large language models\nwithin the medical domain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14654v2",
    "published_date": "2025-01-24 17:21:01 UTC",
    "updated_date": "2025-02-12 05:32:07 UTC"
  },
  {
    "arxiv_id": "2501.14653v1",
    "title": "Federated Domain Generalization with Data-free On-server Gradient Matching",
    "authors": [
      "Trong-Binh Nguyen",
      "Minh-Duong Nguyen",
      "Jinsun Park",
      "Quoc-Viet Pham",
      "Won Joo Hwang"
    ],
    "abstract": "Domain Generalization (DG) aims to learn from multiple known source domains a\nmodel that can generalize well to unknown target domains. One of the key\napproaches in DG is training an encoder which generates domain-invariant\nrepresentations. However, this approach is not applicable in Federated Domain\nGeneralization (FDG), where data from various domains are distributed across\ndifferent clients. In this paper, we introduce a novel approach, dubbed\nFederated Learning via On-server Matching Gradient (FedOMG), which can\n\\emph{efficiently leverage domain information from distributed domains}.\nSpecifically, we utilize the local gradients as information about the\ndistributed models to find an invariant gradient direction across all domains\nthrough gradient inner product maximization. The advantages are two-fold: 1)\nFedOMG can aggregate the characteristics of distributed models on the\ncentralized server without incurring any additional communication cost, and 2)\nFedOMG is orthogonal to many existing FL/FDG methods, allowing for additional\nperformance improvements by being seamlessly integrated with them. Extensive\nexperimental evaluations on various settings to demonstrate the robustness of\nFedOMG compared to other FL/FDG baselines. Our method outperforms recent SOTA\nbaselines on four FL benchmark datasets (MNIST, EMNIST, CIFAR-10, and\nCIFAR-100), and three FDG benchmark datasets (PACS, VLCS, and OfficeHome).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.MA",
      "68Q32, 68Q32",
      "I.4.0; I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 15 figures, ICLR",
    "pdf_url": "http://arxiv.org/pdf/2501.14653v1",
    "published_date": "2025-01-24 17:20:22 UTC",
    "updated_date": "2025-01-24 17:20:22 UTC"
  },
  {
    "arxiv_id": "2501.14856v2",
    "title": "Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative Framework for Imitation Learning from Observation",
    "authors": [
      "Anish Abhijit Diwan",
      "Julen Urain",
      "Jens Kober",
      "Jan Peters"
    ],
    "abstract": "This paper introduces a new imitation learning framework based on\nenergy-based generative models capable of learning complex, physics-dependent,\nrobot motion policies through state-only expert motion trajectories. Our\nalgorithm, called Noise-conditioned Energy-based Annealed Rewards (NEAR),\nconstructs several perturbed versions of the expert's motion data distribution\nand learns smooth, and well-defined representations of the data distribution's\nenergy function using denoising score matching. We propose to use these learnt\nenergy functions as reward functions to learn imitation policies via\nreinforcement learning. We also present a strategy to gradually switch between\nthe learnt energy functions, ensuring that the learnt rewards are always\nwell-defined in the manifold of policy-generated samples. We evaluate our\nalgorithm on complex humanoid tasks such as locomotion and martial arts and\ncompare it with state-only adversarial imitation learning algorithms like\nAdversarial Motion Priors (AMP). Our framework sidesteps the optimisation\nchallenges of adversarial imitation learning techniques and produces results\ncomparable to AMP in several quantitative metrics across multiple imitation\nsettings.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted as a conference paper at the International Conference on\n  Learning Representations (ICLR) 2025. Revised to include review feedback",
    "pdf_url": "http://arxiv.org/pdf/2501.14856v2",
    "published_date": "2025-01-24 17:15:49 UTC",
    "updated_date": "2025-02-12 10:36:26 UTC"
  },
  {
    "arxiv_id": "2501.14644v1",
    "title": "Whisper D-SGD: Correlated Noise Across Agents for Differentially Private Decentralized Learning",
    "authors": [
      "Angelo Rodio",
      "Zheng Chen",
      "Erik G. Larsson"
    ],
    "abstract": "Decentralized learning enables distributed agents to train a shared machine\nlearning model through local computation and peer-to-peer communication.\nAlthough each agent retains its dataset locally, the communication of local\nmodels can still expose private information to adversaries. To mitigate these\nthreats, local differential privacy (LDP) injects independent noise per agent,\nbut it suffers a larger utility gap than central differential privacy (CDP). We\nintroduce Whisper D-SGD, a novel covariance-based approach that generates\ncorrelated privacy noise across agents, unifying several state-of-the-art\nmethods as special cases. By leveraging network topology and mixing weights,\nWhisper D-SGD optimizes the noise covariance to achieve network-wide noise\ncancellation. Experimental results show that Whisper D-SGD cancels more noise\nthan existing pairwise-correlation schemes, substantially narrowing the CDP-LDP\ngap and improving model performance under the same privacy guarantees.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 3 figures, preprint",
    "pdf_url": "http://arxiv.org/pdf/2501.14644v1",
    "published_date": "2025-01-24 17:05:00 UTC",
    "updated_date": "2025-01-24 17:05:00 UTC"
  },
  {
    "arxiv_id": "2501.17178v3",
    "title": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost",
    "authors": [
      "David Salinas",
      "Omar Swelam",
      "Frank Hutter"
    ],
    "abstract": "Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune hyperparameter of LLM judges. To alleviate the\nhigh cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trades accuracy for cost and\nalso reduce significantly the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.17178v3",
    "published_date": "2025-01-24 17:01:14 UTC",
    "updated_date": "2025-03-18 09:09:29 UTC"
  },
  {
    "arxiv_id": "2501.14634v1",
    "title": "Recommending Actionable Strategies: A Semantic Approach to Integrating Analytical Frameworks with Decision Heuristics",
    "authors": [
      "Renato Ghisellini",
      "Remo Pareschi",
      "Marco Pedroni",
      "Giovanni Battista Raggi"
    ],
    "abstract": "We present a novel approach for recommending actionable strategies by\nintegrating strategic frameworks with decision heuristics through semantic\nanalysis. While strategy frameworks provide systematic models for assessment\nand planning, and decision heuristics encode experiential knowledge,these\ntraditions have historically remained separate. Our methodology bridges this\ngap using advanced natural language processing (NLP), demonstrated through\nintegrating frameworks like the 6C model with the Thirty-Six Stratagems. The\napproach employs vector space representations and semantic similarity\ncalculations to map framework parameters to heuristic patterns, supported by a\ncomputational architecture that combines deep semantic processing with\nconstrained use of Large Language Models. By processing both primary content\nand secondary elements (diagrams, matrices) as complementary linguistic\nrepresentations, we demonstrate effectiveness through corporate strategy case\nstudies. The methodology generalizes to various analytical frameworks and\nheuristic sets, culminating in a plug-and-play architecture for generating\nrecommender systems that enable cohesive integration of strategic frameworks\nand decision heuristics into actionable guidance.",
    "categories": [
      "cs.AI",
      "I.2.7; J.4"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14634v1",
    "published_date": "2025-01-24 16:53:37 UTC",
    "updated_date": "2025-01-24 16:53:37 UTC"
  },
  {
    "arxiv_id": "2501.14630v2",
    "title": "Extracting Problem Structure with LLMs for Optimized SAT Local Search",
    "authors": [
      "Andr Schidler",
      "Stefan Szeider"
    ],
    "abstract": "Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)\nsolvers faster by providing high-quality starting points and modern SAT solvers\nhave incorporated this technique into their preprocessing steps. However, these\ntools rely on basic strategies that miss the structural patterns in problems.\nWe present a method that applies Large Language Models (LLMs) to analyze\nPython-based encoding code. This reveals hidden structural patterns in how\nproblems convert into SAT. Our method automatically generates specialized local\nsearch algorithms that find these patterns and use them to create strong\ninitial assignments. This works for any problem instance from the same encoding\ntype. Our tests show encouraging results, achieving faster solving times\ncompared to baseline preprocessing systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14630v2",
    "published_date": "2025-01-24 16:49:08 UTC",
    "updated_date": "2025-02-04 13:55:19 UTC"
  },
  {
    "arxiv_id": "2501.14622v3",
    "title": "ACT-JEPA: Novel Joint-Embedding Predictive Architecture for Efficient Policy Representation Learning",
    "authors": [
      "Aleksandar Vujinovic",
      "Aleksandar Kovacevic"
    ],
    "abstract": "Learning efficient representations for decision-making policies is a\nchallenge in imitation learning (IL). Current IL methods require expert\ndemonstrations, which are expensive to collect. Consequently, they often have\nunderdeveloped world models. Self-supervised learning (SSL) offers an\nalternative by allowing models to learn from diverse, unlabeled data, including\nfailures. However, SSL methods often operate in raw input space, making them\ninefficient. In this work, we propose ACT-JEPA, a novel architecture that\nintegrates IL and SSL to enhance policy representations. We train a policy to\npredict (1) action sequences and (2) abstract observation sequences. The first\nobjective uses action chunking to improve action prediction and reduce\ncompounding errors. The second objective extends this idea of chunking by\npredicting abstract observation sequences. We utilize Joint-Embedding\nPredictive Architecture to predict in abstract representation space, allowing\nthe model to filter out irrelevant details, improve efficiency, and develop a\nrobust world model. Our experiments show that ACT-JEPA improves the quality of\nrepresentations by learning temporal environment dynamics. Additionally, the\nmodel's ability to predict abstract observation sequences results in\nrepresentations that effectively generalize to action sequence prediction.\nACT-JEPA performs on par with established baselines across a range of\ndecision-making tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14622v3",
    "published_date": "2025-01-24 16:41:41 UTC",
    "updated_date": "2025-04-02 11:15:15 UTC"
  },
  {
    "arxiv_id": "2501.14610v1",
    "title": "Leveraging Spatial Cues from Cochlear Implant Microphones to Efficiently Enhance Speech Separation in Real-World Listening Scenes",
    "authors": [
      "Feyisayo Olalere",
      "Kiki van der Heijden",
      "Christiaan H. Stronks",
      "Jeroen Briaire",
      "Johan HM Frijns",
      "Marcel van Gerven"
    ],
    "abstract": "Speech separation approaches for single-channel, dry speech mixtures have\nsignificantly improved. However, real-world spatial and reverberant acoustic\nenvironments remain challenging, limiting the effectiveness of these approaches\nfor assistive hearing devices like cochlear implants (CIs). To address this, we\nquantify the impact of real-world acoustic scenes on speech separation and\nexplore how spatial cues can enhance separation quality efficiently. We analyze\nperformance based on implicit spatial cues (inherent in the acoustic input and\nlearned by the model) and explicit spatial cues (manually calculated spatial\nfeatures added as auxiliary inputs). Our findings show that spatial cues (both\nimplicit and explicit) improve separation for mixtures with spatially separated\nand nearby talkers. Furthermore, spatial cues enhance separation when spectral\ncues are ambiguous, such as when voices are similar. Explicit spatial cues are\nparticularly beneficial when implicit spatial cues are weak. For instance,\nsingle CI microphone recordings provide weaker implicit spatial cues than\nbilateral CIs, but even single CIs benefit from explicit cues. These results\nemphasize the importance of training models on real-world data to improve\ngeneralizability in everyday listening scenarios. Additionally, our statistical\nanalyses offer insights into how data properties influence model performance,\nsupporting the development of efficient speech separation approaches for CIs\nand other assistive devices in real-world settings.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.14610v1",
    "published_date": "2025-01-24 16:30:58 UTC",
    "updated_date": "2025-01-24 16:30:58 UTC"
  },
  {
    "arxiv_id": "2502.14868v1",
    "title": "Unlocking the Black Box: Analysing the EU Artificial Intelligence Act's Framework for Explainability in AI",
    "authors": [
      "Georgios Pavlidis"
    ],
    "abstract": "The lack of explainability of Artificial Intelligence (AI) is one of the\nfirst obstacles that the industry and regulators must overcome to mitigate the\nrisks associated with the technology. The need for eXplainable AI (XAI) is\nevident in fields where accountability, ethics and fairness are critical, such\nas healthcare, credit scoring, policing and the criminal justice system. At the\nEU level, the notion of explainability is one of the fundamental principles\nthat underpin the AI Act, though the exact XAI techniques and requirements are\nstill to be determined and tested in practice. This paper explores various\napproaches and techniques that promise to advance XAI, as well as the\nchallenges of implementing the principle of explainability in AI governance and\npolicies. Finally, the paper examines the integration of XAI into EU law,\nemphasising the issues of standard setting, oversight, and enforcement.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14868v1",
    "published_date": "2025-01-24 16:30:19 UTC",
    "updated_date": "2025-01-24 16:30:19 UTC"
  },
  {
    "arxiv_id": "2501.18616v1",
    "title": "STAMP: Scalable Task And Model-agnostic Collaborative Perception",
    "authors": [
      "Xiangbo Gao",
      "Runsheng Xu",
      "Jiachen Li",
      "Ziran Wang",
      "Zhiwen Fan",
      "Zhengzhong Tu"
    ],
    "abstract": "Perception is crucial for autonomous driving, but single-agent perception is\noften constrained by sensors' physical limitations, leading to degraded\nperformance under severe occlusion, adverse weather conditions, and when\ndetecting distant objects. Multi-agent collaborative perception offers a\nsolution, yet challenges arise when integrating heterogeneous agents with\nvarying model architectures. To address these challenges, we propose STAMP, a\nscalable task- and model-agnostic, collaborative perception pipeline for\nheterogeneous agents. STAMP utilizes lightweight adapter-reverter pairs to\ntransform Bird's Eye View (BEV) features between agent-specific and shared\nprotocol domains, enabling efficient feature sharing and fusion. This approach\nminimizes computational overhead, enhances scalability, and preserves model\nsecurity. Experiments on simulated and real-world datasets demonstrate STAMP's\ncomparable or superior accuracy to state-of-the-art models with significantly\nreduced computational costs. As a first-of-its-kind task- and model-agnostic\nframework, STAMP aims to advance research in scalable and secure mobility\nsystems towards Level 5 autonomy. Our project page is at\nhttps://xiangbogaobarry.github.io/STAMP and the code is available at\nhttps://github.com/taco-group/STAMP.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper is accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.18616v1",
    "published_date": "2025-01-24 16:27:28 UTC",
    "updated_date": "2025-01-24 16:27:28 UTC"
  },
  {
    "arxiv_id": "2501.14603v1",
    "title": "Age and Power Minimization via Meta-Deep Reinforcement Learning in UAV Networks",
    "authors": [
      "Sankani Sarathchandra",
      "Eslam Eldeeb",
      "Mohammad Shehab",
      "Hirley Alves",
      "Konstantin Mikhaylov",
      "Mohamed-Slim Alouini"
    ],
    "abstract": "Age-of-information (AoI) and transmission power are crucial performance\nmetrics in low energy wireless networks, where information freshness is of\nparamount importance. This study examines a power-limited internet of things\n(IoT) network supported by a flying unmanned aerial vehicle(UAV) that collects\ndata. Our aim is to optimize the UAV flight trajectory and scheduling policy to\nminimize a varying AoI and transmission power combination. To tackle this\nvariation, this paper proposes a meta-deep reinforcement learning (RL) approach\nthat integrates deep Q-networks (DQNs) with model-agnostic meta-learning\n(MAML). DQNs determine optimal UAV decisions, while MAML enables scalability\nacross varying objective functions. Numerical results indicate that the\nproposed algorithm converges faster and adapts to new objectives more\neffectively than traditional deep RL methods, achieving minimal AoI and\ntransmission power overall.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.14603v1",
    "published_date": "2025-01-24 16:17:53 UTC",
    "updated_date": "2025-01-24 16:17:53 UTC"
  },
  {
    "arxiv_id": "2501.18614v1",
    "title": "Review and Recommendations for using Artificial Intelligence in Intracoronary Optical Coherence Tomography Analysis",
    "authors": [
      "Xu Chen",
      "Yuan Huang",
      "Benn Jessney",
      "Jason Sangha",
      "Sophie Gu",
      "Carola-Bibiane Schnlieb",
      "Martin Bennett",
      "Michael Roberts"
    ],
    "abstract": "Artificial intelligence (AI) methodologies hold great promise for the rapid\nand accurate diagnosis of coronary artery disease (CAD) from intravascular\noptical coherent tomography (IVOCT) images. Numerous papers have been published\ndescribing AI-based models for different diagnostic tasks, yet it remains\nunclear which models have potential clinical utility and have been properly\nvalidated. This systematic review considered published literature between\nJanuary 2015 and February 2023 describing AI-based diagnosis of CAD using\nIVOCT. Our search identified 5,576 studies, with 513 included after initial\nscreening and 35 studies included in the final systematic review after quality\nscreening. Our findings indicate that most of the identified models are not\ncurrently suitable for clinical use, primarily due to methodological flaws and\nunderlying biases. To address these issues, we provide recommendations to\nimprove model quality and research practices to enhance the development of\nclinically useful AI products.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18614v1",
    "published_date": "2025-01-24 16:06:32 UTC",
    "updated_date": "2025-01-24 16:06:32 UTC"
  },
  {
    "arxiv_id": "2501.14851v2",
    "title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
    "authors": [
      "Michael K. Chen",
      "Xikun Zhang",
      "Dacheng Tao"
    ],
    "abstract": "Logical reasoning is a critical component of Large Language Models (LLMs),\nand substantial research efforts in recent years have aimed to enhance their\ndeductive reasoning capabilities. However, existing deductive reasoning\nbenchmarks, which are crucial for evaluating and advancing LLMs, are inadequate\ndue to their lack of task complexity, presence of prior knowledge as a\nconfounder, and superficial error analysis. To address these deficiencies, we\nintroduce JustLogic, a synthetically generated deductive reasoning benchmark\ndesigned for rigorous evaluation of LLMs. JustLogic is (i) highly complex,\ncapable of generating a diverse range of linguistic patterns, vocabulary, and\nargument structures; (ii) prior knowledge independent, eliminating the\nadvantage of models possessing prior knowledge and ensuring that only deductive\nreasoning is used to answer questions; and (iii) capable of in-depth error\nanalysis on the heterogeneous effects of reasoning depth and argument form on\nmodel accuracy. Our experimental results on JustLogic reveal that (i)\nstate-of-the-art (SOTA) reasoning LLMs perform on par or better than the human\naverage but significantly worse than the human ceiling, and (ii) SOTA\nnon-reasoning models still underperform the human average. All code and data\nare available at https://github.com/michaelchen-lab/JustLogic",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14851v2",
    "published_date": "2025-01-24 15:49:10 UTC",
    "updated_date": "2025-05-09 05:26:43 UTC"
  },
  {
    "arxiv_id": "2501.14850v1",
    "title": "On the locality bias and results in the Long Range Arena",
    "authors": [
      "Pablo Miralles-Gonzlez",
      "Javier Huertas-Tato",
      "Alejandro Martn",
      "David Camacho"
    ],
    "abstract": "The Long Range Arena (LRA) benchmark was designed to evaluate the performance\nof Transformer improvements and alternatives in long-range dependency modeling\ntasks. The Transformer and its main variants performed poorly on this\nbenchmark, and a new series of architectures such as State Space Models (SSMs)\ngained some traction, greatly outperforming Transformers in the LRA. Recent\nwork has shown that with a denoising pre-training phase, Transformers can\nachieve competitive results in the LRA with these new architectures. In this\nwork, we discuss and explain the superiority of architectures such as MEGA and\nSSMs in the Long Range Arena, as well as the recent improvement in the results\nof Transformers, pointing to the positional and local nature of the tasks. We\nshow that while the LRA is a benchmark for long-range dependency modeling, in\nreality most of the performance comes from short-range dependencies. Using\ntraining techniques to mitigate data inefficiency, Transformers are able to\nreach state-of-the-art performance with proper positional encoding. In\naddition, with the same techniques, we were able to remove all restrictions\nfrom SSM convolutional kernels and learn fully parameterized convolutions\nwithout decreasing performance, suggesting that the design choices behind SSMs\nsimply added inductive biases and learning efficiency for these particular\ntasks. Our insights indicate that LRA results should be interpreted with\ncaution and call for a redesign of the benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14850v1",
    "published_date": "2025-01-24 15:34:50 UTC",
    "updated_date": "2025-01-24 15:34:50 UTC"
  },
  {
    "arxiv_id": "2501.14577v3",
    "title": "ZETA: Leveraging Z-order Curves for Efficient Top-k Attention",
    "authors": [
      "Qiuhao Zeng",
      "Jerry Huang",
      "Peng Lu",
      "Gezheng Xu",
      "Boxing Chen",
      "Charles Ling",
      "Boyu Wang"
    ],
    "abstract": "Over recent years, the Transformer has become a fundamental building block\nfor sequence modeling architectures. Yet at its core is the use of\nself-attention, whose memory and computational cost grow quadratically with the\nsequence length $N$, rendering it prohibitively expensive for long sequences. A\npromising approach is top-$k$ attention, which selects only the $k$ most\nrelevant tokens and achieves performance comparable to vanilla self-attention\nwhile significantly reducing space and computational demands. However, causal\nmasks require the current query token to only attend to past tokens, preventing\nthe existing top-$k$ attention method from efficiently searching for the most\nrelevant tokens in parallel, thereby limiting training efficiency. In this\nwork, we propose ZETA, leveraging \\textbf{Z}-Order Curves for\n\\textbf{E}fficient \\textbf{T}op-$k$ \\textbf{A}ttention, to enable parallel\nquerying of past tokens for entire sequences. % in both space and time\ncomplexity of $\\mathcal{O}(N \\log N)$. We first theoretically show that the\nchoice of key and query dimensions involves a trade-off between the curse of\ndimensionality and the preservation of relative distances after projection. In\nlight of this insight, we propose reducing the dimensionality of keys and\nqueries in contrast to values and further leverage $Z$-order curves to map\nlow-dimensional keys and queries into \\emph{one}-dimensional space, which\npermits parallel sorting, thereby largely improving the efficiency for top-$k$\ntoken selection. Experimental results demonstrate that ZETA matches the\nperformance of standard attention on the synthetic \\textsc{Multi-Query\nAssociative Recall} task and outperforms attention and its variants on\n\\textsc{Long Range Arena} and \\textsc{WikiText-103} language modeling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 4 figures, accepted in International Conference on Learning\n  Representations (ICLR) 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14577v3",
    "published_date": "2025-01-24 15:33:05 UTC",
    "updated_date": "2025-04-01 04:41:35 UTC"
  },
  {
    "arxiv_id": "2501.14568v1",
    "title": "Hybrid Quantum-Classical Multi-Agent Pathfinding",
    "authors": [
      "Thore Gerlach",
      "Loong Kuan Lee",
      "Frdric Barbaresco",
      "Nico Piatkowski"
    ],
    "abstract": "Multi-Agent Path Finding (MAPF) focuses on determining conflict-free paths\nfor multiple agents navigating through a shared space to reach specified goal\nlocations. This problem becomes computationally challenging, particularly when\nhandling large numbers of agents, as frequently encountered in practical\napplications like coordinating autonomous vehicles. Quantum computing (QC) is a\npromising candidate in overcoming such limits. However, current quantum\nhardware is still in its infancy and thus limited in terms of computing power\nand error robustness. In this work, we present the first optimal hybrid\nquantum-classical MAPF algorithm which is based on branch-and-cut-and-prize. QC\nis integrated by iteratively solving QUBO problems, based on conflict graphs.\nExperiments on actual quantum hardware and results on benchmark data suggest\nthat our approach dominates previous QUBO formulations and baseline MAPF\nsolvers.",
    "categories": [
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14568v1",
    "published_date": "2025-01-24 15:20:09 UTC",
    "updated_date": "2025-01-24 15:20:09 UTC"
  },
  {
    "arxiv_id": "2501.16380v1",
    "title": "UDiTQC: U-Net-Style Diffusion Transformer for Quantum Circuit Synthesis",
    "authors": [
      "Zhiwei Chen",
      "Hao Tang"
    ],
    "abstract": "Quantum computing is a transformative technology with wide-ranging\napplications, and efficient quantum circuit generation is crucial for unlocking\nits full potential. Current diffusion model approaches based on U-Net\narchitectures, while promising, encounter challenges related to computational\nefficiency and modeling global context. To address these issues, we propose\nUDiT,a novel U-Net-style Diffusion Transformer architecture, which combines\nU-Net's strengths in multi-scale feature extraction with the Transformer's\nability to model global context. We demonstrate the framework's effectiveness\non two tasks: entanglement generation and unitary compilation, where UDiTQC\nconsistently outperforms existing methods. Additionally, our framework supports\ntasks such as masking and editing circuits to meet specific physical property\nrequirements. This dual advancement, improving quantum circuit synthesis and\nrefining generative model architectures, marks a significant milestone in the\nconvergence of quantum computing and machine learning research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16380v1",
    "published_date": "2025-01-24 15:15:50 UTC",
    "updated_date": "2025-01-24 15:15:50 UTC"
  },
  {
    "arxiv_id": "2501.14546v1",
    "title": "Leveraging ChatGPT's Multimodal Vision Capabilities to Rank Satellite Images by Poverty Level: Advancing Tools for Social Science Research",
    "authors": [
      "Hamid Sarmadi",
      "Ola Hall",
      "Thorsteinn Rgnvaldsson",
      "Mattias Ohlsson"
    ],
    "abstract": "This paper investigates the novel application of Large Language Models (LLMs)\nwith vision capabilities to analyze satellite imagery for village-level poverty\nprediction. Although LLMs were originally designed for natural language\nunderstanding, their adaptability to multimodal tasks, including geospatial\nanalysis, has opened new frontiers in data-driven research. By leveraging\nadvancements in vision-enabled LLMs, we assess their ability to provide\ninterpretable, scalable, and reliable insights into human poverty from\nsatellite images. Using a pairwise comparison approach, we demonstrate that\nChatGPT can rank satellite images based on poverty levels with accuracy\ncomparable to domain experts. These findings highlight both the promise and the\nlimitations of LLMs in socioeconomic research, providing a foundation for their\nintegration into poverty assessment workflows. This study contributes to the\nongoing exploration of unconventional data sources for welfare analysis and\nopens pathways for cost-effective, large-scale poverty monitoring.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14546v1",
    "published_date": "2025-01-24 14:49:00 UTC",
    "updated_date": "2025-01-24 14:49:00 UTC"
  },
  {
    "arxiv_id": "2501.14544v2",
    "title": "Distributed Conformal Prediction via Message Passing",
    "authors": [
      "Haifeng Wen",
      "Hong Xing",
      "Osvaldo Simeone"
    ],
    "abstract": "Post-hoc calibration of pre-trained models is critical for ensuring reliable\ninference, especially in safety-critical domains such as healthcare. Conformal\nPrediction (CP) offers a robust post-hoc calibration framework, providing\ndistribution-free statistical coverage guarantees for prediction sets by\nleveraging held-out datasets. In this work, we address a decentralized setting\nwhere each device has limited calibration data and can communicate only with\nits neighbors over an arbitrary graph topology. We propose two\nmessage-passing-based approaches for achieving reliable inference via CP:\nquantile-based distributed conformal prediction (Q-DCP) and histogram-based\ndistributed conformal prediction (H-DCP). Q-DCP employs distributed quantile\nregression enhanced with tailored smoothing and regularization terms to\naccelerate convergence, while H-DCP uses a consensus-based histogram estimation\napproach. Through extensive experiments, we investigate the trade-offs between\nhyperparameter tuning requirements, communication overhead, coverage\nguarantees, and prediction set sizes across different network topologies. The\ncode of our work is released on:\nhttps://github.com/HaifengWen/Distributed-Conformal-Prediction.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.14544v2",
    "published_date": "2025-01-24 14:47:42 UTC",
    "updated_date": "2025-05-21 13:57:01 UTC"
  },
  {
    "arxiv_id": "2501.14540v1",
    "title": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic Reasoning",
    "authors": [
      "Benjamin Callewaert",
      "Simon Vandevelde",
      "Joost Vennekens"
    ],
    "abstract": "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14540v1",
    "published_date": "2025-01-24 14:45:21 UTC",
    "updated_date": "2025-01-24 14:45:21 UTC"
  },
  {
    "arxiv_id": "2501.14513v2",
    "title": "ABPT: Amended Backpropagation through Time with Partially Differentiable Rewards",
    "authors": [
      "Fanxing Li",
      "Fangyu Sun",
      "Tianbao Zhang",
      "Danping Zou"
    ],
    "abstract": "Quadrotor control policies can be trained with high performance using the\nexact gradients of the rewards to directly optimize policy parameters via\nbackpropagation-through-time (BPTT). However, designing a fully differentiable\nreward architecture is often challenging. Partially differentiable rewards will\nresult in biased gradient propagation that degrades training performance. To\novercome this limitation, we propose Amended Backpropagation-through-Time\n(ABPT), a novel approach that mitigates gradient bias while preserving the\ntraining efficiency of BPTT. ABPT combines 0-step and N-step returns,\neffectively reducing the bias by leveraging value gradients from the learned\nQ-value function. Additionally, it adopts entropy regularization and state\ninitialization mechanisms to encourage exploration during training. We evaluate\nABPT on four representative quadrotor flight tasks \\li{in both real world and\nsimulation}. Experimental results demonstrate that ABPT converges significantly\nfaster and achieves higher ultimate rewards than existing learning algorithms,\nparticularly in tasks involving partially differentiable rewards. The code will\nbe released at http://github.com/Fanxing-LI/ABPT.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14513v2",
    "published_date": "2025-01-24 14:18:22 UTC",
    "updated_date": "2025-05-21 11:27:06 UTC"
  },
  {
    "arxiv_id": "2501.14492v1",
    "title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques",
    "authors": [
      "Zhengyang Tang",
      "Ziniu Li",
      "Zhenyang Xiao",
      "Tian Ding",
      "Ruoyu Sun",
      "Benyou Wang",
      "Dayiheng Liu",
      "Fei Huang",
      "Tianyu Liu",
      "Bowen Yu",
      "Junyang Lin"
    ],
    "abstract": "Critiques are important for enhancing the performance of Large Language\nModels (LLMs), enabling both self-improvement and constructive feedback for\nothers by identifying flaws and suggesting improvements. However, evaluating\nthe critique capabilities of LLMs presents a significant challenge due to the\nopen-ended nature of the task. In this work, we introduce a new benchmark\ndesigned to assess the critique capabilities of LLMs. Unlike existing\nbenchmarks, which typically function in an open-loop fashion, our approach\nemploys a closed-loop methodology that evaluates the quality of corrections\ngenerated from critiques. Moreover, the benchmark incorporates features such as\nself-critique, cross-critique, and iterative critique, which are crucial for\ndistinguishing the abilities of advanced reasoning models from more classical\nones. We implement this benchmark using eight challenging reasoning tasks. We\nhave several interesting findings. First, despite demonstrating comparable\nperformance in direct chain-of-thought generation, classical LLMs significantly\nlag behind the advanced reasoning-based model o1-mini across all critique\nscenarios. Second, in self-critique and iterative critique settings, classical\nLLMs may even underperform relative to their baseline capabilities. We hope\nthat this benchmark will serve as a valuable resource to guide future\nadvancements. The code and data are available at\n\\url{https://github.com/tangzhy/RealCritic}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14492v1",
    "published_date": "2025-01-24 13:48:10 UTC",
    "updated_date": "2025-01-24 13:48:10 UTC"
  },
  {
    "arxiv_id": "2501.14483v1",
    "title": "Registration of Longitudinal Liver Examinations for Tumor Progress Assessment",
    "authors": [
      "Walid Yassine",
      "Martin Charachon",
      "Cline Hudelot",
      "Roberto Ardon"
    ],
    "abstract": "Assessing cancer progression in liver CT scans is a clinical challenge,\nrequiring a comparison of scans at different times for the same patient.\nPractitioners must identify existing tumors, compare them with prior exams,\nidentify new tumors, and evaluate overall disease evolution. This process is\nparticularly complex in liver examinations due to misalignment between exams\ncaused by several factors. Indeed, longitudinal liver examinations can undergo\ndifferent non-pathological and pathological changes due to non-rigid\ndeformations, the appearance or disappearance of pathologies, and other\nvariations. In such cases, existing registration approaches, mainly based on\nintrinsic features may distort tumor regions, biasing the tumor progress\nevaluation step and the corresponding diagnosis. This work proposes a\nregistration method based only on geometrical and anatomical information from\nliver segmentation, aimed at aligning longitudinal liver images for aided\ndiagnosis. The proposed method is trained and tested on longitudinal liver CT\nscans, with 317 patients for training and 53 for testing. Our experimental\nresults support our claims by showing that our method is better than other\nregistration techniques by providing a smoother deformation while preserving\nthe tumor burden (total volume of tissues considered as tumor) within the\nvolume. Qualitative results emphasize the importance of smooth deformations in\npreserving tumor appearance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14483v1",
    "published_date": "2025-01-24 13:35:59 UTC",
    "updated_date": "2025-01-24 13:35:59 UTC"
  },
  {
    "arxiv_id": "2501.14474v1",
    "title": "The Pseudo-Dimension of Contracts",
    "authors": [
      "Paul Duetting",
      "Michal Feldman",
      "Tomasz Ponitka",
      "Ermis Soumalias"
    ],
    "abstract": "Algorithmic contract design studies scenarios where a principal incentivizes\nan agent to exert effort on her behalf. In this work, we focus on settings\nwhere the agent's type is drawn from an unknown distribution, and formalize an\noffline learning framework for learning near-optimal contracts from sample\nagent types. A central tool in our analysis is the notion of pseudo-dimension\nfrom statistical learning theory. Beyond its role in establishing upper bounds\non the sample complexity, pseudo-dimension measures the intrinsic complexity of\na class of contracts, offering a new perspective on the tradeoffs between\nsimplicity and optimality in contract design. Our main results provide\nessentially optimal tradeoffs between pseudo-dimension and representation error\n(defined as the loss in principal's utility) with respect to linear and bounded\ncontracts. Using these tradeoffs, we derive sample- and time-efficient learning\nalgorithms, and demonstrate their near-optimality by providing almost matching\nlower bounds on the sample complexity. Conversely, for unbounded contracts, we\nprove an impossibility result showing that no learning algorithm exists.\n  Finally, we extend our techniques in three important ways. First, we provide\nrefined pseudo-dimension and sample complexity guarantees for the combinatorial\nactions model, revealing a novel connection between the number of critical\nvalues and sample complexity. Second, we extend our results to menus of\ncontracts, showing that their pseudo-dimension scales linearly with the menu\nsize. Third, we adapt our algorithms to the online learning setting, where we\nshow that, a polynomial number of type samples suffice to learn near-optimal\nbounded contracts. Combined with prior work, this establishes a formal\nseparation between expert advice and bandit feedback for this setting.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "econ.TH"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14474v1",
    "published_date": "2025-01-24 13:13:50 UTC",
    "updated_date": "2025-01-24 13:13:50 UTC"
  },
  {
    "arxiv_id": "2501.14469v2",
    "title": "Pesti-Gen: Unleashing a Generative Molecule Approach for Toxicity Aware Pesticide Design",
    "authors": [
      "Taehan Kim",
      "Wonduk Seo"
    ],
    "abstract": "Global climate change has reduced crop resilience and pesticide efficacy,\nmaking reliance on synthetic pesticides inevitable, even though their\nwidespread use poses significant health and environmental risks. While these\npesticides remain a key tool in pest management, previous machine-learning\napplications in pesticide and agriculture have focused on classification or\nregression, leaving the fundamental challenge of generating new molecular\nstructures or designing novel candidates unaddressed. In this paper, we propose\nPesti-Gen, a novel generative model based on variational auto-encoders,\ndesigned to create pesticide candidates with optimized properties for the first\ntime. Specifically, Pesti-Gen leverages a two-stage learning process: an\ninitial pre-training phase that captures a generalized chemical structure\nrepresentation, followed by a fine-tuning stage that incorporates\ntoxicity-specific information. The model simultaneously optimizes over multiple\ntoxicity metrics, such as (1) livestock toxicity and (2) aqua toxicity to\ngenerate environmentally friendly pesticide candidates. Notably, Pesti-Gen\nachieves approximately 68\\% structural validity in generating new molecular\nstructures, demonstrating the model's effectiveness in producing optimized and\nfeasible pesticide candidates, thereby providing a new way for safer and more\nsustainable pest management solutions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "q-bio.MN"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the RECOMB 2025 Poster Track",
    "pdf_url": "http://arxiv.org/pdf/2501.14469v2",
    "published_date": "2025-01-24 13:00:54 UTC",
    "updated_date": "2025-03-14 06:16:49 UTC"
  },
  {
    "arxiv_id": "2501.14459v1",
    "title": "Interpretability Analysis of Domain Adapted Dense Retrievers",
    "authors": [
      "Goksenin Yuksel",
      "Jaap Kamps"
    ],
    "abstract": "Dense retrievers have demonstrated significant potential for neural\ninformation retrieval; however, they exhibit a lack of robustness to domain\nshifts, thereby limiting their efficacy in zero-shot settings across diverse\ndomains. Previous research has investigated unsupervised domain adaptation\ntechniques to adapt dense retrievers to target domains. However, these studies\nhave not focused on explainability analysis to understand how such adaptations\nalter the model's behavior. In this paper, we propose utilizing the integrated\ngradients framework to develop an interpretability method that provides both\ninstance-based and ranking-based explanations for dense retrievers. To generate\nthese explanations, we introduce a novel baseline that reveals both query and\ndocument attributions. This method is used to analyze the effects of domain\nadaptation on input attributions for query and document tokens across two\ndatasets: the financial question answering dataset (FIQA) and the biomedical\ninformation retrieval dataset (TREC-COVID). Our visualizations reveal that\ndomain-adapted models focus more on in-domain terminology compared to\nnon-adapted models, exemplified by terms such as \"hedge,\" \"gold,\" \"corona,\" and\n\"disease.\" This research addresses how unsupervised domain adaptation\ntechniques influence the behavior of dense retrievers when adapted to new\ndomains. Additionally, we demonstrate that integrated gradients are a viable\nchoice for explaining and analyzing the internal mechanisms of these opaque\nneural models.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14459v1",
    "published_date": "2025-01-24 12:42:53 UTC",
    "updated_date": "2025-01-24 12:42:53 UTC"
  },
  {
    "arxiv_id": "2501.18612v3",
    "title": "Deeply Optimizing the SAT Solver for the IC3 Algorithm",
    "authors": [
      "Yuheng Su",
      "Qiusong Yang",
      "Yiwei Ci",
      "Yingcheng Li",
      "Tianjun Bu",
      "Ziyu Huang"
    ],
    "abstract": "The IC3 algorithm, also known as PDR, is a SAT-based model checking algorithm\nthat has significantly influenced the field in recent years due to its\nefficiency, scalability, and completeness. It utilizes SAT solvers to solve a\nseries of SAT queries associated with relative induction. In this paper, we\nintroduce several optimizations for the SAT solver in IC3 based on our\nobservations of the unique characteristics of these SAT queries. By observing\nthat SAT queries do not necessarily require decisions on all variables, we\ncompute a subset of variables that need to be decided before each solving\nprocess while ensuring that the result remains unaffected. Additionally, noting\nthat the overhead of binary heap operations in VSIDS is non-negligible, we\nreplace the binary heap with buckets to achieve constant-time operations.\nFurthermore, we support temporary clauses without the need to allocate a new\nactivation variable for each solving process, thereby eliminating the need to\nreset solvers. We developed a novel lightweight CDCL SAT solver, GipSAT, which\nintegrates these optimizations. A comprehensive evaluation highlights the\nperformance improvements achieved by GipSAT. Specifically, the GipSAT-based IC3\ndemonstrates an average speedup of 3.61 times in solving time compared to the\nIC3 implementation based on MiniSat.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.18612v3",
    "published_date": "2025-01-24 12:40:43 UTC",
    "updated_date": "2025-05-21 01:04:39 UTC"
  },
  {
    "arxiv_id": "2501.14443v1",
    "title": "Learning more with the same effort: how randomization improves the robustness of a robotic deep reinforcement learning agent",
    "authors": [
      "Luca Gitta-Lpez",
      "Jaime Boal",
      "lvaro J. Lpez-Lpez"
    ],
    "abstract": "The industrial application of Deep Reinforcement Learning (DRL) is frequently\nslowed down because of the inability to generate the experience required to\ntrain the models. Collecting data often involves considerable time and economic\neffort that is unaffordable in most cases. Fortunately, devices like robots can\nbe trained with synthetic experience thanks to virtual environments. With this\napproach, the sample efficiency problems of artificial agents are mitigated,\nbut another issue arises: the need for efficiently transferring the synthetic\nexperience into the real world (sim-to-real).\n  This paper analyzes the robustness of a state-of-the-art sim-to-real\ntechnique known as progressive neural networks (PNNs) and studies how adding\ndiversity to the synthetic experience can complement it. To better understand\nthe drivers that lead to a lack of robustness, the robotic agent is still\ntested in a virtual environment to ensure total control on the divergence\nbetween the simulated and real models.\n  The results show that a PNN-like agent exhibits a substantial decrease in its\nrobustness at the beginning of the real training phase. Randomizing certain\nvariables during simulation-based training significantly mitigates this issue.\nOn average, the increase in the model's accuracy is around 25% when diversity\nis introduced in the training process. This improvement can be translated into\na decrease in the required real experience for the same final robustness\nperformance. Notwithstanding, adding real experience to agents should still be\nbeneficial regardless of the quality of the virtual experience fed into the\nagent.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.RO",
    "comment": "This article was accepted and published in Applied Intelligence\n  (10.1007/s10489-022-04227-3)",
    "pdf_url": "http://arxiv.org/pdf/2501.14443v1",
    "published_date": "2025-01-24 12:23:12 UTC",
    "updated_date": "2025-01-24 12:23:12 UTC"
  },
  {
    "arxiv_id": "2501.14406v2",
    "title": "Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models",
    "authors": [
      "Fei Wu",
      "Jia Hu",
      "Geyong Min",
      "Shiqiang Wang"
    ],
    "abstract": "Pre-trained Language Models (PLMs) have demonstrated their superiority and\nversatility in modern Natural Language Processing (NLP), effectively adapting\nto various downstream tasks through further fine-tuning. Federated\nParameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution\nto address privacy and efficiency challenges in distributed training for PLMs\non resource-constrained local devices. However, our measurements reveal two key\nlimitations of FedPEFT: heterogeneous data across devices leads to significant\nperformance degradation, and a fixed parameter configuration results in\ncommunication inefficiency. To overcome these limitations, we propose FedARA, a\nnovel Adaptive Rank Allocation framework for federated parameter-efficient\nfine-tuning of language models. Specifically, FedARA employs truncated Singular\nValue Decomposition (SVD) adaptation to enhance similar feature representation\nacross clients, significantly mitigating the adverse effects of data\nheterogeneity. Subsequently, it utilizes dynamic rank allocation to\nprogressively identify critical ranks, effectively improving communication\nefficiency. Lastly, it leverages rank-based module pruning to automatically\nremove inactive modules, steadily reducing local computational cost and memory\nusage in each federated learning round. Extensive experiments show that FedARA\nconsistently outperforms baselines by an average of 6.95% to 8.49% across\nvarious datasets and models under heterogeneous data while significantly\nimproving communication efficiency by 2.40$ \\times$. Moreover, experiments on\nvarious edge devices demonstrate substantial decreases in total training time\nand energy consumption by up to 48.90% and 46.95%, respectively.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14406v2",
    "published_date": "2025-01-24 11:19:07 UTC",
    "updated_date": "2025-03-01 17:30:25 UTC"
  },
  {
    "arxiv_id": "2501.14400v1",
    "title": "SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation",
    "authors": [
      "Shengjie Wang",
      "Jiacheng You",
      "Yihang Hu",
      "Jiongye Li",
      "Yang Gao"
    ],
    "abstract": "Real-world tasks such as garment manipulation and table rearrangement demand\nrobots to perform generalizable, highly precise, and long-horizon actions.\nAlthough imitation learning has proven to be an effective approach for teaching\nrobots new skills, large amounts of expert demonstration data are still\nindispensible for these complex tasks, resulting in high sample complexity and\ncostly data collection. To address this, we propose Semantic Keypoint Imitation\nLearning (SKIL), a framework which automatically obtain semantic keypoints with\nhelp of vision foundation models, and forms the descriptor of semantic\nkeypoints that enables effecient imitation learning of complex robotic tasks\nwith significantly lower sample complexity. In real world experiments, SKIL\ndoubles the performance of baseline methods in tasks such as picking a cup or\nmouse, while demonstrating exceptional robustness to variations in objects,\nenvironmental changes, and distractors. For long-horizon tasks like hanging a\ntowel on a rack where previous methods fail completely, SKIL achieves a mean\nsuccess rate of 70\\% with as few as 30 demonstrations. Furthermore, SKIL\nnaturally supports cross-embodiment learning due to its semantic keypoints\nabstraction, our experiments demonstrate that even human videos bring\nconsiderable improvement to the learning performance. All these results\ndemonstrate the great success of SKIL in achieving data-efficint generalizable\nrobotic learning. Visualizations and code are available at:\nhttps://skil-robotics.github.io/SKIL-robotics/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "22 pages, 22 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.14400v1",
    "published_date": "2025-01-24 11:11:53 UTC",
    "updated_date": "2025-01-24 11:11:53 UTC"
  },
  {
    "arxiv_id": "2501.14399v1",
    "title": "Handling Heterophily in Recommender Systems with Wavelet Hypergraph Diffusion",
    "authors": [
      "Darnbi Sakong",
      "Thanh Tam Nguyen"
    ],
    "abstract": "Recommender systems are pivotal in delivering personalised user experiences\nacross various domains. However, capturing the heterophily patterns and the\nmulti-dimensional nature of user-item interactions poses significant\nchallenges. To address this, we introduce FWHDNN (Fusion-based Wavelet\nHypergraph Diffusion Neural Networks), an innovative framework aimed at\nadvancing representation learning in hypergraph-based recommendation tasks. The\nmodel incorporates three key components: (1) a cross-difference relation\nencoder leveraging heterophily-aware hypergraph diffusion to adapt\nmessage-passing for diverse class labels, (2) a multi-level cluster-wise\nencoder employing wavelet transform-based hypergraph neural network layers to\ncapture multi-scale topological relationships, and (3) an integrated\nmulti-modal fusion mechanism that combines structural and textual information\nthrough intermediate and late-fusion strategies. Extensive experiments on\nreal-world datasets demonstrate that FWHDNN surpasses state-of-the-art methods\nin accuracy, robustness, and scalability in capturing high-order\ninterconnections between users and items.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DB",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14399v1",
    "published_date": "2025-01-24 11:08:29 UTC",
    "updated_date": "2025-01-24 11:08:29 UTC"
  },
  {
    "arxiv_id": "2501.14846v5",
    "title": "Wormhole Memory: A Rubik's Cube for Cross-Dialogue Retrieval",
    "authors": [
      "Libo Wang"
    ],
    "abstract": "In view of the gap in the current large language model in sharing memory\nacross dialogues, this research proposes a wormhole memory module (WMM) to\nrealize memory as a Rubik's cube that can be arbitrarily retrieved between\ndifferent dialogues. Through simulation experiments, the researcher built an\nexperimental framework based on the Python environment and used setting memory\nbarriers to simulate the current situation where memories between LLMs\ndialogues are difficult to share. The CoQA development data set was imported\ninto the experiment, and the feasibility of its cross-dialogue memory retrieval\nfunction was verified for WMM's nonlinear indexing and dynamic retrieval, and a\ncomparative analysis was conducted with the capabilities of Titans and MemGPT\nmemory modules. Experimental results show that WMM demonstrated the ability to\nretrieve memory across dialogues and the stability of quantitative indicators\nin eight experiments. It contributes new technical approaches to the\noptimization of memory management of LLMs and provides experience for the\npractical application in the future.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "The experimental process and code have been uploaded to the Github\n  repository, the link is:\n  https://github.com/brucewang123456789/GeniusTrail/tree/main/Wormhole%20Memory%20Module",
    "pdf_url": "http://arxiv.org/pdf/2501.14846v5",
    "published_date": "2025-01-24 10:49:45 UTC",
    "updated_date": "2025-04-12 16:47:37 UTC"
  },
  {
    "arxiv_id": "2501.16379v1",
    "title": "FedAGHN: Personalized Federated Learning with Attentive Graph HyperNetworks",
    "authors": [
      "Jiarui Song",
      "Yunheng Shen",
      "Chengbin Hou",
      "Pengyu Wang",
      "Jinbao Wang",
      "Ke Tang",
      "Hairong Lv"
    ],
    "abstract": "Personalized Federated Learning (PFL) aims to address the statistical\nheterogeneity of data across clients by learning the personalized model for\neach client. Among various PFL approaches, the personalized aggregation-based\napproach conducts parameter aggregation in the server-side aggregation phase to\ngenerate personalized models, and focuses on learning appropriate collaborative\nrelationships among clients for aggregation. However, the collaborative\nrelationships vary in different scenarios and even at different stages of the\nFL process. To this end, we propose Personalized Federated Learning with\nAttentive Graph HyperNetworks (FedAGHN), which employs Attentive Graph\nHyperNetworks (AGHNs) to dynamically capture fine-grained collaborative\nrelationships and generate client-specific personalized initial models.\nSpecifically, AGHNs empower graphs to explicitly model the client-specific\ncollaborative relationships, construct collaboration graphs, and introduce\ntunable attentive mechanism to derive the collaboration weights, so that the\npersonalized initial models can be obtained by aggregating parameters over the\ncollaboration graphs. Extensive experiments can demonstrate the superiority of\nFedAGHN. Moreover, a series of visualizations are presented to explore the\neffectiveness of collaboration graphs learned by FedAGHN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16379v1",
    "published_date": "2025-01-24 10:48:30 UTC",
    "updated_date": "2025-01-24 10:48:30 UTC"
  },
  {
    "arxiv_id": "2501.14379v1",
    "title": "ECTIL: Label-efficient Computational Tumour Infiltrating Lymphocyte (TIL) assessment in breast cancer: Multicentre validation in 2,340 patients with breast cancer",
    "authors": [
      "Yoni Schirris",
      "Rosie Voorthuis",
      "Mark Opdam",
      "Marte Liefaard",
      "Gabe S Sonke",
      "Gwen Dackus",
      "Vincent de Jong",
      "Yuwei Wang",
      "Annelot Van Rossum",
      "Tessa G Steenbruggen",
      "Lars C Steggink",
      "Liesbeth G. E. de Vries",
      "Marc van de Vijver",
      "Roberto Salgado",
      "Efstratios Gavves",
      "Paul J van Diest",
      "Sabine C Linn",
      "Jonas Teuwen",
      "Renee Menezes",
      "Marleen Kok",
      "Hugo Horlings"
    ],
    "abstract": "The level of tumour-infiltrating lymphocytes (TILs) is a prognostic factor\nfor patients with (triple-negative) breast cancer (BC). Computational TIL\nassessment (CTA) has the potential to assist pathologists in this\nlabour-intensive task, but current CTA models rely heavily on many detailed\nannotations. We propose and validate a fundamentally simpler deep learning\nbased CTA that can be trained in only ten minutes on hundredfold fewer\npathologist annotations. We collected whole slide images (WSIs) with TILs\nscores and clinical data of 2,340 patients with BC from six cohorts including\nthree randomised clinical trials. Morphological features were extracted from\nwhole slide images (WSIs) using a pathology foundation model. Our\nlabel-efficient Computational stromal TIL assessment model (ECTIL) directly\nregresses the TILs score from these features. ECTIL trained on only a few\nhundred samples (ECTIL-TCGA) showed concordance with the pathologist over five\nheterogeneous external cohorts (r=0.54-0.74, AUROC=0.80-0.94). Training on all\nslides of five cohorts (ECTIL-combined) improved results on a held-out test set\n(r=0.69, AUROC=0.85). Multivariable Cox regression analyses indicated that\nevery 10% increase of ECTIL scores was associated with improved overall\nsurvival independent of clinicopathological variables (HR 0.86, p<0.01),\nsimilar to the pathologist score (HR 0.87, p<0.001). We demonstrate that ECTIL\nis highly concordant with an expert pathologist and obtains a similar hazard\nratio. ECTIL has a fundamentally simpler design than existing methods and can\nbe trained on orders of magnitude fewer annotations. Such a CTA may be used to\npre-screen patients for, e.g., immunotherapy clinical trial inclusion, or as a\ntool to assist clinicians in the diagnostic work-up of patients with BC. Our\nmodel is available under an open source licence\n(https://github.com/nki-ai/ectil).",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Under review. 54 pages including supplementary materials, 2 main\n  tables, 3 main figures, 14 supplementary figures, 4 supplementary tables",
    "pdf_url": "http://arxiv.org/pdf/2501.14379v1",
    "published_date": "2025-01-24 10:28:05 UTC",
    "updated_date": "2025-01-24 10:28:05 UTC"
  },
  {
    "arxiv_id": "2501.14371v1",
    "title": "DRESSing Up LLM: Efficient Stylized Question-Answering via Style Subspace Editing",
    "authors": [
      "Xinyu Ma",
      "Yifeng Xu",
      "Yang Lin",
      "Tianlong Wang",
      "Xu Chu",
      "Xin Gao",
      "Junfeng Zhao",
      "Yasha Wang"
    ],
    "abstract": "We introduce DRESS, a novel approach for generating stylized large language\nmodel (LLM) responses through representation editing. Existing methods like\nprompting and fine-tuning are either insufficient for complex style adaptation\nor computationally expensive, particularly in tasks like NPC creation or\ncharacter role-playing. Our approach leverages the over-parameterized nature of\nLLMs to disentangle a style-relevant subspace within the model's representation\nspace to conduct representation editing, ensuring a minimal impact on the\noriginal semantics. By applying adaptive editing strengths, we dynamically\nadjust the steering vectors in the style subspace to maintain both stylistic\nfidelity and semantic integrity. We develop two stylized QA benchmark datasets\nto validate the effectiveness of DRESS, and the results demonstrate significant\nimprovements compared to baseline methods such as prompting and ITI. In short,\nDRESS is a lightweight, train-free solution for enhancing LLMs with flexible\nand effective style control, making it particularly useful for developing\nstylized conversational agents. Codes and benchmark datasets are available at\nhttps://github.com/ArthurLeoM/DRESS-LLM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2501.14371v1",
    "published_date": "2025-01-24 10:04:53 UTC",
    "updated_date": "2025-01-24 10:04:53 UTC"
  },
  {
    "arxiv_id": "2501.14360v1",
    "title": "In System Alignments we Trust! Explainable Alignments via Projections",
    "authors": [
      "Dominique Sommers",
      "Natalia Sidorova",
      "Boudewijn van Dongen"
    ],
    "abstract": "Alignments are a well-known process mining technique for reconciling system\nlogs and normative process models. Evidence of certain behaviors in a real\nsystem may only be present in one representation - either a log or a model -\nbut not in the other. Since for processes in which multiple entities, like\nobjects and resources, are involved in the activities, their interactions\naffect the behavior and are therefore essential to take into account in the\nalignments.\n  Additionally, both logged and modeled representations of reality may be\nimprecise and only partially represent some of these entities, but not all. In\nthis paper, we introduce the concept of \"relaxations\" through projections for\nalignments to deal with partially correct models and logs. Relaxed alignments\nhelp to distinguish between trustworthy and untrustworthy content of the two\nrepresentations (the log and the model) to achieve a better understanding of\nthe underlying process and expose quality issues.",
    "categories": [
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14360v1",
    "published_date": "2025-01-24 09:47:17 UTC",
    "updated_date": "2025-01-24 09:47:17 UTC"
  },
  {
    "arxiv_id": "2501.14346v2",
    "title": "HorNets: Learning from Discrete and Continuous Signals with Routing Neural Networks",
    "authors": [
      "Boshko Koloski",
      "Nada Lavra",
      "Bla krlj"
    ],
    "abstract": "Construction of neural network architectures suitable for learning from both\ncontinuous and discrete tabular data is a challenging research endeavor.\nContemporary high-dimensional tabular data sets are often characterized by a\nrelatively small instance count, requiring data-efficient learning. We propose\nHorNets (Horn Networks), a neural network architecture with state-of-the-art\nperformance on synthetic and real-life data sets from scarce-data tabular\ndomains. HorNets are based on a clipped polynomial-like activation function,\nextended by a custom discrete-continuous routing mechanism that decides which\npart of the neural network to optimize based on the input's cardinality. By\nexplicitly modeling parts of the feature combination space or combining whole\nspace in a linear attention-like manner, HorNets dynamically decide which mode\nof operation is the most suitable for a given piece of data with no explicit\nsupervision. This architecture is one of the few approaches that reliably\nretrieves logical clauses (including noisy XNOR) and achieves state-of-the-art\nclassification performance on 14 real-life biomedical high-dimensional data\nsets. HorNets are made freely available under a permissive license alongside a\nsynthetic generator of categorical benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the ACML conference journal track with the Machine\n  Learning journal. The first and the last authors share an equal contribution",
    "pdf_url": "http://arxiv.org/pdf/2501.14346v2",
    "published_date": "2025-01-24 09:17:57 UTC",
    "updated_date": "2025-02-13 17:03:04 UTC"
  },
  {
    "arxiv_id": "2501.14844v2",
    "title": "Unmasking Conversational Bias in AI Multiagent Systems",
    "authors": [
      "Erica Coppolillo",
      "Giuseppe Manco",
      "Luca Maria Aiello"
    ],
    "abstract": "Detecting biases in the outputs produced by generative models is essential to\nreduce the potential risks associated with their application in critical\nsettings. However, the majority of existing methodologies for identifying\nbiases in generated text consider the models in isolation and neglect their\ncontextual applications. Specifically, the biases that may arise in multi-agent\nsystems involving generative models remain under-researched. To address this\ngap, we present a framework designed to quantify biases within multi-agent\nsystems of conversational Large Language Models (LLMs). Our approach involves\nsimulating small echo chambers, where pairs of LLMs, initialized with aligned\nperspectives on a polarizing topic, engage in discussions. Contrary to\nexpectations, we observe significant shifts in the stance expressed in the\ngenerated messages, particularly within echo chambers where all agents\ninitially express conservative viewpoints, in line with the well-documented\npolitical bias of many LLMs toward liberal positions. Crucially, the bias\nobserved in the echo-chamber experiment remains undetected by current\nstate-of-the-art bias detection methods that rely on questionnaires. This\nhighlights a critical need for the development of a more sophisticated toolkit\nfor bias detection and mitigation for AI multi-agent systems. The code to\nperform the experiments is publicly available at\nhttps://anonymous.4open.science/r/LLMsConversationalBias-7725.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14844v2",
    "published_date": "2025-01-24 09:10:02 UTC",
    "updated_date": "2025-02-02 14:32:41 UTC"
  },
  {
    "arxiv_id": "2501.14334v2",
    "title": "Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts",
    "authors": [
      "Clment Desroches",
      "Martin Chauvin",
      "Louis Ladan",
      "Caroline Vateau",
      "Simon Gosset",
      "Philippe Cordier"
    ],
    "abstract": "The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.\n  In this paper, we propose a methodology to estimate the environmental impact\nof a company's AI portfolio, providing actionable insights without\nnecessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results\nconfirm that large generative AI models consume up to 4600x more energy than\ntraditional models. Our modelling approach, which accounts for increased AI\nusage, hardware computing efficiency, and changes in electricity mix in line\nwith IPCC scenarios, forecasts AI electricity use up to 2030. Under a high\nadoption scenario, driven by widespread Generative AI and agents adoption\nassociated to increasingly complex models and frameworks, AI electricity use is\nprojected to rise by a factor of 24.4.\n  Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n\"Return on Environment\" metric to align AI development with net-zero goals.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14334v2",
    "published_date": "2025-01-24 08:58:49 UTC",
    "updated_date": "2025-01-27 14:50:32 UTC"
  },
  {
    "arxiv_id": "2501.14322v1",
    "title": "Relative Layer-Wise Relevance Propagation: a more Robust Neural Networks eXplaination",
    "authors": [
      "Eric Nyiri",
      "Olivier Gibaru"
    ],
    "abstract": "Machine learning methods are solving very successfully a plethora of tasks,\nbut they have the disadvantage of not providing any information about their\ndecision. Consequently, estimating the reasoning of the system provides\nadditional information. For this, Layer-Wise Relevance Propagation (LRP) is one\nof the methods in eXplainable Machine Learning (XML). Its purpose is to provide\ncontributions of any neural network output in the domain of its input. The main\ndrawback of current methods is mainly due to division by small values. To\novercome this problem, we provide a new definition called Relative LRP where\nthe classical conservation law is satisfied up to a multiplicative factor but\nwithout divisions by small values except for Resnet skip connection. In this\narticle, we will focus on image classification. This allows us to visualize the\ncontributions of a pixel to the predictions of a multi-layer neural network.\nPixel contributions provide a focus to further analysis on regions of potential\ninterest. R-LRP can be applied for any dense, CNN or residual neural networks.\nMoreover, R-LRP doesn't need any hyperparameters to tune contrary to other LRP\nmethods. We then compare the R-LRP method on different datasets with simple\nCNN, VGG16, VGG19 and Resnet50 networks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2012.14501,\n  arXiv:1605.01713 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2501.14322v1",
    "published_date": "2025-01-24 08:34:22 UTC",
    "updated_date": "2025-01-24 08:34:22 UTC"
  },
  {
    "arxiv_id": "2501.17176v3",
    "title": "Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant",
    "authors": [
      "Marc Ballestero-Rib",
      "Daniel Ortiz-Martnez"
    ],
    "abstract": "The dream of achieving a student-teacher ratio of 1:1 is closer than ever\nthanks to the emergence of large language models (LLMs). One potential\napplication of these models in the educational field would be to provide\nfeedback to students in university introductory programming courses, so that a\nstudent struggling to solve a basic implementation problem could seek help from\nan LLM available 24/7. This article focuses on studying three aspects related\nto such an application. First, the performance of two well-known models,\nGPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The\nempirical results showed that GPT-4T performs much better than GPT-3.5T,\nhowever, it is not yet ready for use in a real-world scenario. This is due to\nthe possibility of generating incorrect information that potential users may\nnot always be able to detect. Second, the article proposes a carefully designed\nprompt using in-context learning techniques that allows automating important\nparts of the evaluation process, as well as providing a lower bound for the\nfraction of feedbacks containing incorrect information, saving time and effort.\nThis was possible because the resulting feedback has a programmatically\nanalyzable structure that incorporates diagnostic information about the LLM's\nperformance in solving the requested task. Third, the article also suggests a\npossible strategy for implementing a practical learning tool based on LLMs,\nwhich is rooted on the proposed prompting techniques. This strategy opens up a\nwhole range of interesting possibilities from a pedagogical perspective.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.17176v3",
    "published_date": "2025-01-24 08:15:05 UTC",
    "updated_date": "2025-05-04 13:32:32 UTC"
  },
  {
    "arxiv_id": "2501.14310v1",
    "title": "Permutation-based multi-objective evolutionary feature selection for high-dimensional data",
    "authors": [
      "Raquel Espinosa",
      "Gracia Snchez",
      "Jos Palma",
      "Fernando Jimnez"
    ],
    "abstract": "Feature selection is a critical step in the analysis of high-dimensional\ndata, where the number of features often vastly exceeds the number of samples.\nEffective feature selection not only improves model performance and\ninterpretability but also reduces computational costs and mitigates the risk of\noverfitting. In this context, we propose a novel feature selection method for\nhigh-dimensional data, based on the well-known permutation feature importance\napproach, but extending it to evaluate subsets of attributes rather than\nindividual features. This extension more effectively captures how interactions\namong features influence model performance. The proposed method employs a\nmulti-objective evolutionary algorithm to search for candidate feature subsets,\nwith the objectives of maximizing the degradation in model performance when the\nselected features are shuffled, and minimizing the cardinality of the feature\nsubset. The effectiveness of our method has been validated on a set of 24\npublicly available high-dimensional datasets for classification and regression\ntasks, and compared against 9 well-established feature selection methods\ndesigned for high-dimensional problems, including the conventional permutation\nfeature importance method. The results demonstrate the ability of our approach\nin balancing accuracy and computational efficiency, providing a powerful tool\nfor feature selection in complex, high-dimensional datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14310v1",
    "published_date": "2025-01-24 08:11:28 UTC",
    "updated_date": "2025-01-24 08:11:28 UTC"
  },
  {
    "arxiv_id": "2501.14308v1",
    "title": "Learning Primitive Relations for Compositional Zero-Shot Learning",
    "authors": [
      "Insu Lee",
      "Jiseob Kim",
      "Kyuhong Shim",
      "Byonghyo Shim"
    ],
    "abstract": "Compositional Zero-Shot Learning (CZSL) aims to identify unseen state-object\ncompositions by leveraging knowledge learned from seen compositions. Existing\napproaches often independently predict states and objects, overlooking their\nrelationships. In this paper, we propose a novel framework, learning primitive\nrelations (LPR), designed to probabilistically capture the relationships\nbetween states and objects. By employing the cross-attention mechanism, LPR\nconsiders the dependencies between states and objects, enabling the model to\ninfer the likelihood of unseen compositions. Experimental results demonstrate\nthat LPR outperforms state-of-the-art methods on all three CZSL benchmark\ndatasets in both closed-world and open-world settings. Through qualitative\nanalysis, we show that LPR leverages state-object relationships for unseen\ncomposition prediction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14308v1",
    "published_date": "2025-01-24 08:10:05 UTC",
    "updated_date": "2025-01-24 08:10:05 UTC"
  },
  {
    "arxiv_id": "2501.14305v1",
    "title": "A Zero-Shot LLM Framework for Automatic Assignment Grading in Higher Education",
    "authors": [
      "Calvin Yeung",
      "Jeff Yu",
      "King Chau Cheung",
      "Tat Wing Wong",
      "Chun Man Chan",
      "Kin Chi Wong",
      "Keisuke Fujii"
    ],
    "abstract": "Automated grading has become an essential tool in education technology due to\nits ability to efficiently assess large volumes of student work, provide\nconsistent and unbiased evaluations, and deliver immediate feedback to enhance\nlearning. However, current systems face significant limitations, including the\nneed for large datasets in few-shot learning methods, a lack of personalized\nand actionable feedback, and an overemphasis on benchmark performance rather\nthan student experience. To address these challenges, we propose a Zero-Shot\nLarge Language Model (LLM)-Based Automated Assignment Grading (AAG) system.\nThis framework leverages prompt engineering to evaluate both computational and\nexplanatory student responses without requiring additional training or\nfine-tuning. The AAG system delivers tailored feedback that highlights\nindividual strengths and areas for improvement, thereby enhancing student\nlearning outcomes. Our study demonstrates the system's effectiveness through\ncomprehensive evaluations, including survey responses from higher education\nstudents that indicate significant improvements in motivation, understanding,\nand preparedness compared to traditional grading methods. The results validate\nthe AAG system's potential to transform educational assessment by prioritizing\nlearning experiences and providing scalable, high-quality feedback.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14305v1",
    "published_date": "2025-01-24 08:01:41 UTC",
    "updated_date": "2025-01-24 08:01:41 UTC"
  },
  {
    "arxiv_id": "2501.14304v2",
    "title": "MASTER: A Multi-Agent System with LLM Specialized MCTS",
    "authors": [
      "Bingzheng Gan",
      "Yufan Zhao",
      "Tianyi Zhang",
      "Jing Huang",
      "Yusu Li",
      "Shu Xian Teo",
      "Changwang Zhang",
      "Wei Shi"
    ],
    "abstract": "Large Language Models (LLM) are increasingly being explored for\nproblem-solving tasks. However, their strategic planning capability is often\nviewed with skepticism. Recent studies have incorporated the Monte Carlo Tree\nSearch (MCTS) algorithm to augment the planning capacity of LLM. Despite its\npotential, MCTS relies on extensive sampling simulations to approximate the\ntrue reward distribution, which leads to two primary issues. Firstly, MCTS is\neffective for tasks like the Game of Go, where simulation results can yield\nobjective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such\nas question answering, the result of a simulation is the answer to the\nquestion, which cannot yield an objective reward without the ground truth.\nSecondly, obtaining statistically significant reward estimations typically\nrequires a sample size exceeding 30 simulations, resulting in excessive token\nusage and time consumption. To address these challenges, we present the\nMulti-Agent System with Tactical Execution and Reasoning using LLM Specialized\nMCTS (MASTER), a novel framework that coordinates agent recruitment and\ncommunication through LLM specialized MCTS. This system autonomously adjusts\nthe number of agents based on task complexity and ensures focused communication\namong them. Comprehensive experiments across various tasks demonstrate the\neffectiveness of our proposed framework. It achieves 76% accuracy on HotpotQA\nand 80% on WebShop, setting new state-of-the-art performance on these datasets.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by main NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14304v2",
    "published_date": "2025-01-24 08:01:11 UTC",
    "updated_date": "2025-02-04 06:26:08 UTC"
  },
  {
    "arxiv_id": "2501.14300v1",
    "title": "Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large Language Model on Knowledge Graph",
    "authors": [
      "Xujian Liang",
      "Zhaoquan Gu"
    ],
    "abstract": "Graph Retrieval Augmented Generation (GRAG) is a novel paradigm that takes\nthe naive RAG system a step further by integrating graph information, such as\nknowledge graph (KGs), into large-scale language models (LLMs) to mitigate\nhallucination. However, existing GRAG still encounter limitations: 1) simple\nparadigms usually fail with the complex problems due to the narrow and shallow\ncorrelations capture from KGs 2) methods of strong coupling with KGs tend to be\nhigh computation cost and time consuming if the graph is dense. In this paper,\nwe propose the Fast Think-on-Graph (FastToG), an innovative paradigm for\nenabling LLMs to think ``community by community\" within KGs. To do this,\nFastToG employs community detection for deeper correlation capture and two\nstages community pruning - coarse and fine pruning for faster retrieval.\nFurthermore, we also develop two Community-to-Text methods to convert the graph\nstructure of communities into textual form for better understanding by LLMs.\nExperimental results demonstrate the effectiveness of FastToG, showcasing\nhigher accuracy, faster reasoning, and better explainability compared to the\nprevious works.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14300v1",
    "published_date": "2025-01-24 07:47:40 UTC",
    "updated_date": "2025-01-24 07:47:40 UTC"
  },
  {
    "arxiv_id": "2501.14294v3",
    "title": "Examining Alignment of Large Language Models through Representative Heuristics: The Case of Political Stereotypes",
    "authors": [
      "Sullam Jeoung",
      "Yubin Ge",
      "Haohan Wang",
      "Jana Diesner"
    ],
    "abstract": "Examining the alignment of large language models (LLMs) has become\nincreasingly important, e.g., when LLMs fail to operate as intended. This study\nexamines the alignment of LLMs with human values for the domain of politics.\nPrior research has shown that LLM-generated outputs can include political\nleanings and mimic the stances of political parties on various issues. However,\nthe extent and conditions under which LLMs deviate from empirical positions are\ninsufficiently examined. To address this gap, we analyze the factors that\ncontribute to LLMs' deviations from empirical positions on political issues,\naiming to quantify these deviations and identify the conditions that cause\nthem.\n  Drawing on findings from cognitive science about representativeness\nheuristics, i.e., situations where humans lean on representative attributes of\na target group in a way that leads to exaggerated beliefs, we scrutinize LLM\nresponses through this heuristics' lens. We conduct experiments to determine\nhow LLMs inflate predictions about political parties, which results in\nstereotyping. We find that while LLMs can mimic certain political parties'\npositions, they often exaggerate these positions more than human survey\nrespondents do. Also, LLMs tend to overemphasize representativeness more than\nhumans. This study highlights the susceptibility of LLMs to representativeness\nheuristics, suggesting a potential vulnerability of LLMs that facilitates\npolitical stereotyping. We also test prompt-based mitigation strategies,\nfinding that strategies that can mitigate representative heuristics in humans\nare also effective in reducing the influence of representativeness on\nLLM-generated responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14294v3",
    "published_date": "2025-01-24 07:24:23 UTC",
    "updated_date": "2025-03-02 06:49:21 UTC"
  },
  {
    "arxiv_id": "2501.14288v2",
    "title": "A Comprehensive Framework for Semantic Similarity Analysis of Human and AI-Generated Text Using Transformer Architectures and Ensemble Techniques",
    "authors": [
      "Lifu Gao",
      "Ziwei Liu",
      "Qi Zhang"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has made detecting\nAI-generated text an increasingly critical challenge. Traditional methods often\nfail to capture the nuanced semantic differences between human and\nmachine-generated content. We therefore propose a novel approach based on\nsemantic similarity analysis, leveraging a multi-layered architecture that\ncombines a pre-trained DeBERTa-v3-large model, Bi-directional LSTMs, and linear\nattention pooling to capture both local and global semantic patterns. To\nenhance performance, we employ advanced input and output augmentation\ntechniques such as sector-level context integration and wide output\nconfigurations. These techniques enable the model to learn more discriminative\nfeatures and generalize across diverse domains. Experimental results show that\nthis approach works better than traditional methods, proving its usefulness for\nAI-generated text detection and other text comparison tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14288v2",
    "published_date": "2025-01-24 07:07:37 UTC",
    "updated_date": "2025-01-31 02:10:36 UTC"
  },
  {
    "arxiv_id": "2501.14278v2",
    "title": "Active Learning for Continual Learning: Keeping the Past Alive in the Present",
    "authors": [
      "Jaehyun Park",
      "Dongmin Park",
      "Jae-Gil Lee"
    ],
    "abstract": "Continual learning (CL) enables deep neural networks to adapt to\never-changing data distributions. In practice, there may be scenarios where\nannotation is costly, leading to active continual learning (ACL), which\nperforms active learning (AL) for the CL scenarios when reducing the labeling\ncost by selecting the most informative subset is preferable. However,\nconventional AL strategies are not suitable for ACL, as they focus solely on\nlearning the new knowledge, leading to catastrophic forgetting of previously\nlearned tasks. Therefore, ACL requires a new AL strategy that can balance the\nprevention of catastrophic forgetting and the ability to quickly learn new\ntasks. In this paper, we propose AccuACL, Accumulated informativeness-based\nActive Continual Learning, by the novel use of the Fisher information matrix as\na criterion for sample selection, derived from a theoretical analysis of the\nFisher-optimality preservation properties within the framework of ACL, while\nalso addressing the scalability issue of Fisher information-based AL. Extensive\nexperiments demonstrate that AccuACL significantly outperforms AL baselines\nacross various CL algorithms, increasing the average accuracy and forgetting by\n23.8% and 17.0%, respectively, on average.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14278v2",
    "published_date": "2025-01-24 06:46:58 UTC",
    "updated_date": "2025-04-21 07:58:09 UTC"
  },
  {
    "arxiv_id": "2501.14276v1",
    "title": "Global Semantic-Guided Sub-image Feature Weight Allocation in High-Resolution Large Vision-Language Models",
    "authors": [
      "Yuxuan Liang",
      "Xu Li",
      "Xiaolei Chen",
      "Haotian Chen",
      "Yi Zheng",
      "Chenghang Lai",
      "Bin Li",
      "Xiangyang Xue"
    ],
    "abstract": "As the demand for high-resolution image processing in Large Vision-Language\nModels (LVLMs) grows, sub-image partitioning has become a popular approach for\nmitigating visual information loss associated with fixed-resolution processing.\nHowever, existing partitioning methods uniformly process sub-images, resulting\nin suboptimal image understanding. In this work, we reveal that the sub-images\nwith higher semantic relevance to the entire image encapsulate richer visual\ninformation for preserving the model's visual understanding ability. Therefore,\nwe propose the Global Semantic-guided Weight Allocator (GSWA) module, which\ndynamically allocates weights to sub-images based on their relative information\ndensity, emulating human visual attention mechanisms. This approach enables the\nmodel to focus on more informative regions, overcoming the limitations of\nuniform treatment. We integrate GSWA into the InternVL2-2B framework to create\nSleighVL, a lightweight yet high-performing model. Extensive experiments\ndemonstrate that SleighVL outperforms models with comparable parameters and\nremains competitive with larger models. Our work provides a promising direction\nfor more efficient and contextually aware high-resolution image processing in\nLVLMs, advancing multimodal system development.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 10 figures and tables",
    "pdf_url": "http://arxiv.org/pdf/2501.14276v1",
    "published_date": "2025-01-24 06:42:06 UTC",
    "updated_date": "2025-01-24 06:42:06 UTC"
  },
  {
    "arxiv_id": "2501.14275v1",
    "title": "Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation",
    "authors": [
      "Sadegh Mahdavi",
      "Muchen Li",
      "Kaiwen Liu",
      "Christos Thrampoulidis",
      "Leonid Sigal",
      "Renjie Liao"
    ],
    "abstract": "Advances in Large Language Models (LLMs) have sparked interest in their\nability to solve Olympiad-level math problems. However, the training and\nevaluation of these models are constrained by the limited size and quality of\navailable datasets, as creating large-scale data for such advanced problems\nrequires extensive effort from human experts. In addition, current benchmarks\nare prone to contamination, leading to unreliable evaluations. In this paper,\nwe present an automated pipeline that leverages the rich resources of the Art\nof Problem Solving (AoPS) forum, which predominantly features Olympiad-level\nproblems and community-driven solutions. Using open-source LLMs, we develop a\nmethod to extract question-answer pairs from the forum, resulting in\nAoPS-Instruct, a dataset of more than 600,000 high-quality QA pairs. Our\nexperiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their\nreasoning abilities across various benchmarks. Moreover, we build an automatic\npipeline that introduces LiveAoPSBench, an evolving evaluation set with\ntimestamps, derived from the latest forum data, providing a\ncontamination-resistant benchmark for assessing LLM performance. Notably, we\nobserve a significant decline in LLM performance over time, suggesting their\nsuccess on older examples may stem from pre-training exposure rather than true\nreasoning ability. Our work presents a scalable approach to creating and\nmaintaining large-scale, high-quality datasets for advanced math reasoning,\noffering valuable insights into the capabilities and limitations of LLMs in\nthis domain. Our benchmark and code is available at\nhttps://github.com/DSL-Lab/aops",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14275v1",
    "published_date": "2025-01-24 06:39:38 UTC",
    "updated_date": "2025-01-24 06:39:38 UTC"
  },
  {
    "arxiv_id": "2501.14269v2",
    "title": "Hierarchical Time-Aware Mixture of Experts for Multi-Modal Sequential Recommendation",
    "authors": [
      "Shengzhe Zhang",
      "Liyi Chen",
      "Dazhong Shen",
      "Chao Wang",
      "Hui Xiong"
    ],
    "abstract": "Multi-modal sequential recommendation (SR) leverages multi-modal data to\nlearn more comprehensive item features and user preferences than traditional SR\nmethods, which has become a critical topic in both academia and industry.\nExisting methods typically focus on enhancing multi-modal information utility\nthrough adaptive modality fusion to capture the evolving of user preference\nfrom user-item interaction sequences. However, most of them overlook the\ninterference caused by redundant interest-irrelevant information contained in\nrich multi-modal data. Additionally, they primarily rely on implicit temporal\ninformation based solely on chronological ordering, neglecting explicit\ntemporal signals that could more effectively represent dynamic user interest\nover time. To address these limitations, we propose a Hierarchical time-aware\nMixture of experts for multi-modal Sequential Recommendation (HM4SR) with a\ntwo-level Mixture of Experts (MoE) and a multi-task learning strategy.\nSpecifically, the first MoE, named Interactive MoE, extracts essential user\ninterest-related information from the multi-modal data of each item. Then, the\nsecond MoE, termed Temporal MoE, captures user dynamic interests by introducing\nexplicit temporal embeddings from timestamps in modality encoding. To further\naddress data sparsity, we propose three auxiliary supervision tasks:\nsequence-level category prediction (CP) for item feature understanding,\ncontrastive learning on ID (IDCL) to align sequence context with user\ninterests, and placeholder contrastive learning (PCL) to integrate temporal\ninformation with modalities for dynamic interest modeling. Extensive\nexperiments on four public datasets verify the effectiveness of HM4SR compared\nto several state-of-the-art approaches.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to WWW 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14269v2",
    "published_date": "2025-01-24 06:26:50 UTC",
    "updated_date": "2025-01-30 02:05:07 UTC"
  },
  {
    "arxiv_id": "2501.14268v1",
    "title": "Pre-train and Fine-tune: Recommenders as Large Models",
    "authors": [
      "Zhenhao Jiang",
      "Chenghao Chen",
      "Hao Feng",
      "Yu Yang",
      "Jin Liu",
      "Jie Zhang",
      "Jia Jia",
      "Ning Hu"
    ],
    "abstract": "In reality, users have different interests in different periods, regions,\nscenes, etc. Such changes in interest are so drastic that they are difficult to\nbe captured by recommenders. Existing multi-domain learning can alleviate this\nproblem. However, the structure of the industrial recommendation system is\ncomplex, the amount of data is huge, and the training cost is extremely high,\nso it is difficult to modify the structure of the industrial recommender and\nre-train it. To fill this gap, we consider recommenders as large pre-trained\nmodels and fine-tune them. We first propose the theory of the information\nbottleneck for fine-tuning and present an explanation for the fine-tuning\ntechnique in recommenders. To tailor for recommendation, we design an\ninformation-aware adaptive kernel (IAK) technique to fine-tune the pre-trained\nrecommender. Specifically, we define fine-tuning as two phases: knowledge\ncompression and knowledge matching and let the training stage of IAK explicitly\napproximate these two phases. Our proposed approach designed from the essence\nof fine-tuning is well interpretable. Extensive online and offline experiments\nshow the superiority of our proposed method. Besides, we also share unique and\nimportant lessons we learned when deploying the method in a large-scale online\nplatform. We also present the potential issues of fine-tuning techniques in\nrecommendation systems and the corresponding solutions. The recommender with\nIAK technique has been deployed on the homepage of a billion-scale online food\nplatform for several months and has yielded considerable profits in our\nbusiness.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by WWW2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14268v1",
    "published_date": "2025-01-24 06:18:12 UTC",
    "updated_date": "2025-01-24 06:18:12 UTC"
  },
  {
    "arxiv_id": "2501.16378v1",
    "title": "Internal Activation Revision: Safeguarding Vision Language Models Without Parameter Update",
    "authors": [
      "Qing Li",
      "Jiahui Geng",
      "Zongxiong Chen",
      "Kun Song",
      "Lei Ma",
      "Fakhri Karray"
    ],
    "abstract": "Vision-language models (VLMs) demonstrate strong multimodal capabilities but\nhave been found to be more susceptible to generating harmful content compared\nto their backbone large language models (LLMs). Our investigation reveals that\nthe integration of images significantly shifts the model's internal activations\nduring the forward pass, diverging from those triggered by textual input.\nMoreover, the safety alignments of LLMs embedded within VLMs are not\nsufficiently robust to handle the activations discrepancies, making the models\nvulnerable to even the simplest jailbreaking attacks. To address this issue, we\npropose an \\textbf{internal activation revision} approach that efficiently\nrevises activations during generation, steering the model toward safer outputs.\nOur framework incorporates revisions at both the layer and head levels,\noffering control over the model's generation at varying levels of granularity.\nIn addition, we explore three strategies for constructing positive and negative\nsamples and two approaches for extracting revision vectors, resulting in\ndifferent variants of our method. Comprehensive experiments demonstrate that\nthe internal activation revision method significantly improves the safety of\nwidely used VLMs, reducing attack success rates by an average of 48.94\\%,\n34.34\\%, 43.92\\%, and 52.98\\% on SafeBench, Safe-Unsafe, Unsafe, and\nMM-SafetyBench, respectively, while minimally impacting model helpfulness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16378v1",
    "published_date": "2025-01-24 06:17:22 UTC",
    "updated_date": "2025-01-24 06:17:22 UTC"
  },
  {
    "arxiv_id": "2501.14250v1",
    "title": "Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors",
    "authors": [
      "Yi Zhao",
      "Youzhi Zhang"
    ],
    "abstract": "Large language models (LLMs) are widely used in real-world applications,\nraising concerns about their safety and trustworthiness. While red-teaming with\njailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus\nprimarily on single-turn attacks, overlooking the multi-turn strategies used by\nreal-world adversaries. Existing multi-turn methods rely on static patterns or\npredefined logical chains, failing to account for the dynamic strategies during\nattacks. We propose Siren, a learning-based multi-turn attack framework\ndesigned to simulate real-world human jailbreak behaviors. Siren consists of\nthree stages: (1) training set construction utilizing Turn-Level LLM feedback\n(Turn-MF), (2) post-training attackers with supervised fine-tuning (SFT) and\ndirect preference optimization (DPO), and (3) interactions between the\nattacking and target LLMs. Experiments demonstrate that Siren achieves an\nattack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against\nGemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o,\nsignificantly outperforming single-turn baselines. Moreover, Siren with a\n7B-scale model achieves performance comparable to a multi-turn baseline that\nleverages GPT-4o as the attacker, while requiring fewer turns and employing\ndecomposition strategies that are better semantically aligned with attack\ngoals. We hope Siren inspires the development of stronger defenses against\nadvanced multi-turn jailbreak attacks under realistic scenarios. Code is\navailable at https://github.com/YiyiyiZhao/siren. Warning: This paper contains\npotentially harmful text.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14250v1",
    "published_date": "2025-01-24 05:31:27 UTC",
    "updated_date": "2025-01-24 05:31:27 UTC"
  },
  {
    "arxiv_id": "2501.14249v7",
    "title": "Humanity's Last Exam",
    "authors": [
      "Long Phan",
      "Alice Gatti",
      "Ziwen Han",
      "Nathaniel Li",
      "Josephina Hu",
      "Hugh Zhang",
      "Chen Bo Calvin Zhang",
      "Mohamed Shaaban",
      "John Ling",
      "Sean Shi",
      "Michael Choi",
      "Anish Agrawal",
      "Arnav Chopra",
      "Adam Khoja",
      "Ryan Kim",
      "Richard Ren",
      "Jason Hausenloy",
      "Oliver Zhang",
      "Mantas Mazeika",
      "Dmitry Dodonov",
      "Tung Nguyen",
      "Jaeho Lee",
      "Daron Anderson",
      "Mikhail Doroshenko",
      "Alun Cennyth Stokes",
      "Mobeen Mahmood",
      "Oleksandr Pokutnyi",
      "Oleg Iskra",
      "Jessica P. Wang",
      "John-Clark Levin",
      "Mstyslav Kazakov",
      "Fiona Feng",
      "Steven Y. Feng",
      "Haoran Zhao",
      "Michael Yu",
      "Varun Gangal",
      "Chelsea Zou",
      "Zihan Wang",
      "Serguei Popov",
      "Robert Gerbicz",
      "Geoff Galgon",
      "Johannes Schmitt",
      "Will Yeadon",
      "Yongki Lee",
      "Scott Sauers",
      "Alvaro Sanchez",
      "Fabian Giska",
      "Marc Roth",
      "Sren Riis",
      "Saiteja Utpala",
      "Noah Burns",
      "Gashaw M. Goshu",
      "Mohinder Maheshbhai Naiya",
      "Chidozie Agu",
      "Zachary Giboney",
      "Antrell Cheatom",
      "Francesco Fournier-Facio",
      "Sarah-Jane Crowson",
      "Lennart Finke",
      "Zerui Cheng",
      "Jennifer Zampese",
      "Ryan G. Hoerr",
      "Mark Nandor",
      "Hyunwoo Park",
      "Tim Gehrunger",
      "Jiaqi Cai",
      "Ben McCarty",
      "Alexis C Garretson",
      "Edwin Taylor",
      "Damien Sileo",
      "Qiuyu Ren",
      "Usman Qazi",
      "Lianghui Li",
      "Jungbae Nam",
      "John B. Wydallis",
      "Pavel Arkhipov",
      "Jack Wei Lun Shi",
      "Aras Bacho",
      "Chris G. Willcocks",
      "Hangrui Cao",
      "Sumeet Motwani",
      "Emily de Oliveira Santos",
      "Johannes Veith",
      "Edward Vendrow",
      "Doru Cojoc",
      "Kengo Zenitani",
      "Joshua Robinson",
      "Longke Tang",
      "Yuqi Li",
      "Joshua Vendrow",
      "Natanael Wildner Fraga",
      "Vladyslav Kuchkin",
      "Andrey Pupasov Maksimov",
      "Pierre Marion",
      "Denis Efremov",
      "Jayson Lynch",
      "Kaiqu Liang",
      "Aleksandar Mikov",
      "Andrew Gritsevskiy",
      "Julien Guillod",
      "Gzdenur Demir",
      "Dakotah Martinez",
      "Ben Pageler",
      "Kevin Zhou",
      "Saeed Soori",
      "Ori Press",
      "Henry Tang",
      "Paolo Rissone",
      "Sean R. Green",
      "Lina Brssel",
      "Moon Twayana",
      "Aymeric Dieuleveut",
      "Joseph Marvin Imperial",
      "Ameya Prabhu",
      "Jinzhou Yang",
      "Nick Crispino",
      "Arun Rao",
      "Dimitri Zvonkine",
      "Gabriel Loiseau",
      "Mikhail Kalinin",
      "Marco Lukas",
      "Ciprian Manolescu",
      "Nate Stambaugh",
      "Subrata Mishra",
      "Tad Hogg",
      "Carlo Bosio",
      "Brian P Coppola",
      "Julian Salazar",
      "Jaehyeok Jin",
      "Rafael Sayous",
      "Stefan Ivanov",
      "Philippe Schwaller",
      "Shaipranesh Senthilkuma",
      "Andres M Bran",
      "Andres Algaba",
      "Kelsey Van den Houte",
      "Lynn Van Der Sypt",
      "Brecht Verbeken",
      "David Noever",
      "Alexei Kopylov",
      "Benjamin Myklebust",
      "Bikun Li",
      "Lisa Schut",
      "Evgenii Zheltonozhskii",
      "Qiaochu Yuan",
      "Derek Lim",
      "Richard Stanley",
      "Tong Yang",
      "John Maar",
      "Julian Wykowski",
      "Mart Oller",
      "Anmol Sahu",
      "Cesare Giulio Ardito",
      "Yuzheng Hu",
      "Ariel Ghislain Kemogne Kamdoum",
      "Alvin Jin",
      "Tobias Garcia Vilchis",
      "Yuexuan Zu",
      "Martin Lackner",
      "James Koppel",
      "Gongbo Sun",
      "Daniil S. Antonenko",
      "Steffi Chern",
      "Bingchen Zhao",
      "Pierrot Arsene",
      "Joseph M Cavanagh",
      "Daofeng Li",
      "Jiawei Shen",
      "Donato Crisostomi",
      "Wenjin Zhang",
      "Ali Dehghan",
      "Sergey Ivanov",
      "David Perrella",
      "Nurdin Kaparov",
      "Allen Zang",
      "Ilia Sucholutsky",
      "Arina Kharlamova",
      "Daniil Orel",
      "Vladislav Poritski",
      "Shalev Ben-David",
      "Zachary Berger",
      "Parker Whitfill",
      "Michael Foster",
      "Daniel Munro",
      "Linh Ho",
      "Shankar Sivarajan",
      "Dan Bar Hava",
      "Aleksey Kuchkin",
      "David Holmes",
      "Alexandra Rodriguez-Romero",
      "Frank Sommerhage",
      "Anji Zhang",
      "Richard Moat",
      "Keith Schneider",
      "Zakayo Kazibwe",
      "Don Clarke",
      "Dae Hyun Kim",
      "Felipe Meneguitti Dias",
      "Sara Fish",
      "Veit Elser",
      "Tobias Kreiman",
      "Victor Efren Guadarrama Vilchis",
      "Immo Klose",
      "Ujjwala Anantheswaran",
      "Adam Zweiger",
      "Kaivalya Rawal",
      "Jeffery Li",
      "Jeremy Nguyen",
      "Nicolas Daans",
      "Haline Heidinger",
      "Maksim Radionov",
      "Vclav Rozho",
      "Vincent Ginis",
      "Christian Stump",
      "Niv Cohen",
      "Rafa Powiata",
      "Josef Tkadlec",
      "Alan Goldfarb",
      "Chenguang Wang",
      "Piotr Padlewski",
      "Stanislaw Barzowski",
      "Kyle Montgomery",
      "Ryan Stendall",
      "Jamie Tucker-Foltz",
      "Jack Stade",
      "T. Ryan Rogers",
      "Tom Goertzen",
      "Declan Grabb",
      "Abhishek Shukla",
      "Alan Givr",
      "John Arnold Ambay",
      "Archan Sen",
      "Muhammad Fayez Aziz",
      "Mark H Inlow",
      "Hao He",
      "Ling Zhang",
      "Younesse Kaddar",
      "Ivar ngquist",
      "Yanxu Chen",
      "Harrison K Wang",
      "Kalyan Ramakrishnan",
      "Elliott Thornley",
      "Antonio Terpin",
      "Hailey Schoelkopf",
      "Eric Zheng",
      "Avishy Carmi",
      "Ethan D. L. Brown",
      "Kelin Zhu",
      "Max Bartolo",
      "Richard Wheeler",
      "Martin Stehberger",
      "Peter Bradshaw",
      "JP Heimonen",
      "Kaustubh Sridhar",
      "Ido Akov",
      "Jennifer Sandlin",
      "Yury Makarychev",
      "Joanna Tam",
      "Hieu Hoang",
      "David M. Cunningham",
      "Vladimir Goryachev",
      "Demosthenes Patramanis",
      "Michael Krause",
      "Andrew Redenti",
      "David Aldous",
      "Jesyin Lai",
      "Shannon Coleman",
      "Jiangnan Xu",
      "Sangwon Lee",
      "Ilias Magoulas",
      "Sandy Zhao",
      "Ning Tang",
      "Michael K. Cohen",
      "Orr Paradise",
      "Jan Hendrik Kirchner",
      "Maksym Ovchynnikov",
      "Jason O. Matos",
      "Adithya Shenoy",
      "Michael Wang",
      "Yuzhou Nie",
      "Anna Sztyber-Betley",
      "Paolo Faraboschi",
      "Robin Riblet",
      "Jonathan Crozier",
      "Shiv Halasyamani",
      "Shreyas Verma",
      "Prashant Joshi",
      "Eli Meril",
      "Ziqiao Ma",
      "Jrmy Androletti",
      "Raghav Singhal",
      "Jacob Platnick",
      "Volodymyr Nevirkovets",
      "Luke Basler",
      "Alexander Ivanov",
      "Seri Khoury",
      "Nils Gustafsson",
      "Marco Piccardo",
      "Hamid Mostaghimi",
      "Qijia Chen",
      "Virendra Singh",
      "Tran Quoc Khnh",
      "Paul Rosu",
      "Hannah Szlyk",
      "Zachary Brown",
      "Himanshu Narayan",
      "Aline Menezes",
      "Jonathan Roberts",
      "William Alley",
      "Kunyang Sun",
      "Arkil Patel",
      "Max Lamparth",
      "Anka Reuel",
      "Linwei Xin",
      "Hanmeng Xu",
      "Jacob Loader",
      "Freddie Martin",
      "Zixuan Wang",
      "Andrea Achilleos",
      "Thomas Preu",
      "Tomek Korbak",
      "Ida Bosio",
      "Fereshteh Kazemi",
      "Ziye Chen",
      "Bir Blint",
      "Eve J. Y. Lo",
      "Jiaqi Wang",
      "Maria Ins S. Nunes",
      "Jeremiah Milbauer",
      "M Saiful Bari",
      "Zihao Wang",
      "Behzad Ansarinejad",
      "Yewen Sun",
      "Stephane Durand",
      "Hossam Elgnainy",
      "Guillaume Douville",
      "Daniel Tordera",
      "George Balabanian",
      "Hew Wolff",
      "Lynna Kvistad",
      "Hsiaoyun Milliron",
      "Ahmad Sakor",
      "Murat Eron",
      "Andrew Favre D. O.",
      "Shailesh Shah",
      "Xiaoxiang Zhou",
      "Firuz Kamalov",
      "Sherwin Abdoli",
      "Tim Santens",
      "Shaul Barkan",
      "Allison Tee",
      "Robin Zhang",
      "Alessandro Tomasiello",
      "G. Bruno De Luca",
      "Shi-Zhuo Looi",
      "Vinh-Kha Le",
      "Noam Kolt",
      "Jiayi Pan",
      "Emma Rodman",
      "Jacob Drori",
      "Carl J Fossum",
      "Niklas Muennighoff",
      "Milind Jagota",
      "Ronak Pradeep",
      "Honglu Fan",
      "Jonathan Eicher",
      "Michael Chen",
      "Kushal Thaman",
      "William Merrill",
      "Moritz Firsching",
      "Carter Harris",
      "Stefan Ciobc",
      "Jason Gross",
      "Rohan Pandey",
      "Ilya Gusev",
      "Adam Jones",
      "Shashank Agnihotri",
      "Pavel Zhelnov",
      "Mohammadreza Mofayezi",
      "Alexander Piperski",
      "David K. Zhang",
      "Kostiantyn Dobarskyi",
      "Roman Leventov",
      "Ignat Soroko",
      "Joshua Duersch",
      "Vage Taamazyan",
      "Andrew Ho",
      "Wenjie Ma",
      "William Held",
      "Ruicheng Xian",
      "Armel Randy Zebaze",
      "Mohanad Mohamed",
      "Julian Noah Leser",
      "Michelle X Yuan",
      "Laila Yacar",
      "Johannes Lengler",
      "Katarzyna Olszewska",
      "Claudio Di Fratta",
      "Edson Oliveira",
      "Joseph W. Jackson",
      "Andy Zou",
      "Muthu Chidambaram",
      "Timothy Manik",
      "Hector Haffenden",
      "Dashiell Stander",
      "Ali Dasouqi",
      "Alexander Shen",
      "Bita Golshani",
      "David Stap",
      "Egor Kretov",
      "Mikalai Uzhou",
      "Alina Borisovna Zhidkovskaya",
      "Nick Winter",
      "Miguel Orbegozo Rodriguez",
      "Robert Lauff",
      "Dustin Wehr",
      "Colin Tang",
      "Zaki Hossain",
      "Shaun Phillips",
      "Fortuna Samuele",
      "Fredrik Ekstrm",
      "Angela Hammon",
      "Oam Patel",
      "Faraz Farhidi",
      "George Medley",
      "Forough Mohammadzadeh",
      "Madellene Peaflor",
      "Haile Kassahun",
      "Alena Friedrich",
      "Rayner Hernandez Perez",
      "Daniel Pyda",
      "Taom Sakal",
      "Omkar Dhamane",
      "Ali Khajegili Mirabadi",
      "Eric Hallman",
      "Kenchi Okutsu",
      "Mike Battaglia",
      "Mohammad Maghsoudimehrabani",
      "Alon Amit",
      "Dave Hulbert",
      "Roberto Pereira",
      "Simon Weber",
      "Handoko",
      "Anton Peristyy",
      "Stephen Malina",
      "Mustafa Mehkary",
      "Rami Aly",
      "Frank Reidegeld",
      "Anna-Katharina Dick",
      "Cary Friday",
      "Mukhwinder Singh",
      "Hassan Shapourian",
      "Wanyoung Kim",
      "Mariana Costa",
      "Hubeyb Gurdogan",
      "Harsh Kumar",
      "Chiara Ceconello",
      "Chao Zhuang",
      "Haon Park",
      "Micah Carroll",
      "Andrew R. Tawfeek",
      "Stefan Steinerberger",
      "Daattavya Aggarwal",
      "Michael Kirchhof",
      "Linjie Dai",
      "Evan Kim",
      "Johan Ferret",
      "Jainam Shah",
      "Yuzhou Wang",
      "Minghao Yan",
      "Krzysztof Burdzy",
      "Lixin Zhang",
      "Antonio Franca",
      "Diana T. Pham",
      "Kang Yong Loh",
      "Joshua Robinson",
      "Abram Jackson",
      "Paolo Giordano",
      "Philipp Petersen",
      "Adrian Cosma",
      "Jesus Colino",
      "Colin White",
      "Jacob Votava",
      "Vladimir Vinnikov",
      "Ethan Delaney",
      "Petr Spelda",
      "Vit Stritecky",
      "Syed M. Shahid",
      "Jean-Christophe Mourrat",
      "Lavr Vetoshkin",
      "Koen Sponselee",
      "Renas Bacho",
      "Zheng-Xin Yong",
      "Florencia de la Rosa",
      "Nathan Cho",
      "Xiuyu Li",
      "Guillaume Malod",
      "Orion Weller",
      "Guglielmo Albani",
      "Leon Lang",
      "Julien Laurendeau",
      "Dmitry Kazakov",
      "Fatimah Adesanya",
      "Julien Portier",
      "Lawrence Hollom",
      "Victor Souza",
      "Yuchen Anna Zhou",
      "Julien Degorre",
      "Yiit Yaln",
      "Gbenga Daniel Obikoya",
      "Rai",
      "Filippo Bigi",
      "M. C. Bosc",
      "Oleg Shumar",
      "Kaniuar Bacho",
      "Gabriel Recchia",
      "Mara Popescu",
      "Nikita Shulga",
      "Ngefor Mildred Tanwie",
      "Thomas C. H. Lux",
      "Ben Rank",
      "Colin Ni",
      "Matthew Brooks",
      "Alesia Yakimchyk",
      "Huanxu",
      "Liu",
      "Stefano Cavalleri",
      "Olle Hggstrm",
      "Emil Verkama",
      "Joshua Newbould",
      "Hans Gundlach",
      "Leonor Brito-Santana",
      "Brian Amaro",
      "Vivek Vajipey",
      "Rynaa Grover",
      "Ting Wang",
      "Yosi Kratish",
      "Wen-Ding Li",
      "Sivakanth Gopi",
      "Andrea Caciolai",
      "Christian Schroeder de Witt",
      "Pablo Hernndez-Cmara",
      "Emanuele Rodol",
      "Jules Robins",
      "Dominic Williamson",
      "Vincent Cheng",
      "Brad Raynor",
      "Hao Qi",
      "Ben Segev",
      "Jingxuan Fan",
      "Sarah Martinson",
      "Erik Y. Wang",
      "Kaylie Hausknecht",
      "Michael P. Brenner",
      "Mao Mao",
      "Christoph Demian",
      "Peyman Kassani",
      "Xinyu Zhang",
      "David Avagian",
      "Eshawn Jessica Scipio",
      "Alon Ragoler",
      "Justin Tan",
      "Blake Sims",
      "Rebeka Plecnik",
      "Aaron Kirtland",
      "Omer Faruk Bodur",
      "D. P. Shinde",
      "Yan Carlos Leyva Labrador",
      "Zahra Adoul",
      "Mohamed Zekry",
      "Ali Karakoc",
      "Tania C. B. Santos",
      "Samir Shamseldeen",
      "Loukmane Karim",
      "Anna Liakhovitskaia",
      "Nate Resman",
      "Nicholas Farina",
      "Juan Carlos Gonzalez",
      "Gabe Maayan",
      "Earth Anderson",
      "Rodrigo De Oliveira Pena",
      "Elizabeth Kelley",
      "Hodjat Mariji",
      "Rasoul Pouriamanesh",
      "Wentao Wu",
      "Ross Finocchio",
      "Ismail Alarab",
      "Joshua Cole",
      "Danyelle Ferreira",
      "Bryan Johnson",
      "Mohammad Safdari",
      "Liangti Dai",
      "Siriphan Arthornthurasuk",
      "Isaac C. McAlister",
      "Alejandro Jos Moyano",
      "Alexey Pronin",
      "Jing Fan",
      "Angel Ramirez-Trinidad",
      "Yana Malysheva",
      "Daphiny Pottmaier",
      "Omid Taheri",
      "Stanley Stepanic",
      "Samuel Perry",
      "Luke Askew",
      "Ral Adrin Huerta Rodrguez",
      "Ali M. R. Minissi",
      "Ricardo Lorena",
      "Krishnamurthy Iyer",
      "Arshad Anil Fasiludeen",
      "Ronald Clark",
      "Josh Ducey",
      "Matheus Piza",
      "Maja Somrak",
      "Eric Vergo",
      "Juehang Qin",
      "Benjmin Borbs",
      "Eric Chu",
      "Jack Lindsey",
      "Antoine Jallon",
      "I. M. J. McInnis",
      "Evan Chen",
      "Avi Semler",
      "Luk Gloor",
      "Tej Shah",
      "Marc Carauleanu",
      "Pascal Lauer",
      "Tran uc Huy",
      "Hossein Shahrtash",
      "Emilien Duc",
      "Lukas Lewark",
      "Assaf Brown",
      "Samuel Albanie",
      "Brian Weber",
      "Warren S. Vaz",
      "Pierre Clavier",
      "Yiyang Fan",
      "Gabriel Poesia Reis e Silva",
      "Long",
      "Lian",
      "Marcus Abramovitch",
      "Xi Jiang",
      "Sandra Mendoza",
      "Murat Islam",
      "Juan Gonzalez",
      "Vasilios Mavroudis",
      "Justin Xu",
      "Pawan Kumar",
      "Laxman Prasad Goswami",
      "Daniel Bugas",
      "Nasser Heydari",
      "Ferenc Jeanplong",
      "Thorben Jansen",
      "Antonella Pinto",
      "Archimedes Apronti",
      "Abdallah Galal",
      "Ng Ze-An",
      "Ankit Singh",
      "Tong Jiang",
      "Joan of Arc Xavier",
      "Kanu Priya Agarwal",
      "Mohammed Berkani",
      "Gang Zhang",
      "Zhehang Du",
      "Benedito Alves de Oliveira Junior",
      "Dmitry Malishev",
      "Nicolas Remy",
      "Taylor D. Hartman",
      "Tim Tarver",
      "Stephen Mensah",
      "Gautier Abou Loume",
      "Wiktor Morak",
      "Farzad Habibi",
      "Sarah Hoback",
      "Will Cai",
      "Javier Gimenez",
      "Roselynn Grace Montecillo",
      "Jakub ucki",
      "Russell Campbell",
      "Asankhaya Sharma",
      "Khalida Meer",
      "Shreen Gul",
      "Daniel Espinosa Gonzalez",
      "Xavier Alapont",
      "Alex Hoover",
      "Gunjan Chhablani",
      "Freddie Vargus",
      "Arunim Agarwal",
      "Yibo Jiang",
      "Deepakkumar Patil",
      "David Outevsky",
      "Kevin Joseph Scaria",
      "Rajat Maheshwari",
      "Abdelkader Dendane",
      "Priti Shukla",
      "Ashley Cartwright",
      "Sergei Bogdanov",
      "Niels Mndler",
      "Sren Mller",
      "Luca Arnaboldi",
      "Kunvar Thaman",
      "Muhammad Rehan Siddiqi",
      "Prajvi Saxena",
      "Himanshu Gupta",
      "Tony Fruhauff",
      "Glen Sherman",
      "Mtys Vincze",
      "Siranut Usawasutsakorn",
      "Dylan Ler",
      "Anil Radhakrishnan",
      "Innocent Enyekwe",
      "Sk Md Salauddin",
      "Jiang Muzhen",
      "Aleksandr Maksapetyan",
      "Vivien Rossbach",
      "Chris Harjadi",
      "Mohsen Bahaloohoreh",
      "Claire Sparrow",
      "Jasdeep Sidhu",
      "Sam Ali",
      "Song Bian",
      "John Lai",
      "Eric Singer",
      "Justine Leon Uro",
      "Greg Bateman",
      "Mohamed Sayed",
      "Ahmed Menshawy",
      "Darling Duclosel",
      "Dario Bezzi",
      "Yashaswini Jain",
      "Ashley Aaron",
      "Murat Tiryakioglu",
      "Sheeshram Siddh",
      "Keith Krenek",
      "Imad Ali Shah",
      "Jun Jin",
      "Scott Creighton",
      "Denis Peskoff",
      "Zienab EL-Wasif",
      "Ragavendran P V",
      "Michael Richmond",
      "Joseph McGowan",
      "Tejal Patwardhan",
      "Hao-Yu Sun",
      "Ting Sun",
      "Nikola Zubi",
      "Samuele Sala",
      "Stephen Ebert",
      "Jean Kaddour",
      "Manuel Schottdorf",
      "Dianzhuo Wang",
      "Gerol Petruzella",
      "Alex Meiburg",
      "Tilen Medved",
      "Ali ElSheikh",
      "S Ashwin Hebbar",
      "Lorenzo Vaquero",
      "Xianjun Yang",
      "Jason Poulos",
      "Vilm Zouhar",
      "Sergey Bogdanik",
      "Mingfang Zhang",
      "Jorge Sanz-Ros",
      "David Anugraha",
      "Yinwei Dai",
      "Anh N. Nhu",
      "Xue Wang",
      "Ali Anil Demircali",
      "Zhibai Jia",
      "Yuyin Zhou",
      "Juncheng Wu",
      "Mike He",
      "Nitin Chandok",
      "Aarush Sinha",
      "Gaoxiang Luo",
      "Long Le",
      "Mickal Noy",
      "Micha Perekiewicz",
      "Ioannis Pantidis",
      "Tianbo Qi",
      "Soham Sachin Purohit",
      "Letitia Parcalabescu",
      "Thai-Hoa Nguyen",
      "Genta Indra Winata",
      "Edoardo M. Ponti",
      "Hanchen Li",
      "Kaustubh Dhole",
      "Jongee Park",
      "Dario Abbondanza",
      "Yuanli Wang",
      "Anupam Nayak",
      "Diogo M. Caetano",
      "Antonio A. W. L. Wong",
      "Maria del Rio-Chanona",
      "Dniel Kondor",
      "Pieter Francois",
      "Ed Chalstrey",
      "Jakob Zsambok",
      "Dan Hoyer",
      "Jenny Reddish",
      "Jakob Hauser",
      "Francisco-Javier Rodrigo-Gins",
      "Suchandra Datta",
      "Maxwell Shepherd",
      "Thom Kamphuis",
      "Qizheng Zhang",
      "Hyunjun Kim",
      "Ruiji Sun",
      "Jianzhu Yao",
      "Franck Dernoncourt",
      "Satyapriya Krishna",
      "Sina Rismanchian",
      "Bonan Pu",
      "Francesco Pinto",
      "Yingheng Wang",
      "Kumar Shridhar",
      "Kalon J. Overholt",
      "Glib Briia",
      "Hieu Nguyen",
      "David",
      "Soler Bartomeu",
      "Tony CY Pang",
      "Adam Wecker",
      "Yifan Xiong",
      "Fanfei Li",
      "Lukas S. Huber",
      "Joshua Jaeger",
      "Romano De Maddalena",
      "Xing Han L",
      "Yuhui Zhang",
      "Claas Beger",
      "Patrick Tser Jern Kon",
      "Sean Li",
      "Vivek Sanker",
      "Ming Yin",
      "Yihao Liang",
      "Xinlu Zhang",
      "Ankit Agrawal",
      "Li S. Yifei",
      "Zechen Zhang",
      "Mu Cai",
      "Yasin Sonmez",
      "Costin Cozianu",
      "Changhao Li",
      "Alex Slen",
      "Shoubin Yu",
      "Hyun Kyu Park",
      "Gabriele Sarti",
      "Marcin Briaski",
      "Alessandro Stolfo",
      "Truong An Nguyen",
      "Mike Zhang",
      "Yotam Perlitz",
      "Jose Hernandez-Orallo",
      "Runjia Li",
      "Amin Shabani",
      "Felix Juefei-Xu",
      "Shikhar Dhingra",
      "Orr Zohar",
      "My Chiffon Nguyen",
      "Alexander Pondaven",
      "Abdurrahim Yilmaz",
      "Xuandong Zhao",
      "Chuanyang Jin",
      "Muyan Jiang",
      "Stefan Todoran",
      "Xinyao Han",
      "Jules Kreuer",
      "Brian Rabern",
      "Anna Plassart",
      "Martino Maggetti",
      "Luther Yap",
      "Robert Geirhos",
      "Jonathon Kean",
      "Dingsu Wang",
      "Sina Mollaei",
      "Chenkai Sun",
      "Yifan Yin",
      "Shiqi Wang",
      "Rui Li",
      "Yaowen Chang",
      "Anjiang Wei",
      "Alice Bizeul",
      "Xiaohan Wang",
      "Alexandre Oliveira Arrais",
      "Kushin Mukherjee",
      "Jorge Chamorro-Padial",
      "Jiachen Liu",
      "Xingyu Qu",
      "Junyi Guan",
      "Adam Bouyamourn",
      "Shuyu Wu",
      "Martyna Plomecka",
      "Junda Chen",
      "Mengze Tang",
      "Jiaqi Deng",
      "Shreyas Subramanian",
      "Haocheng Xi",
      "Haoxuan Chen",
      "Weizhi Zhang",
      "Yinuo Ren",
      "Haoqin Tu",
      "Sejong Kim",
      "Yushun Chen",
      "Sara Vera Marjanovi",
      "Junwoo Ha",
      "Grzegorz Luczyna",
      "Jeff J. Ma",
      "Zewen Shen",
      "Dawn Song",
      "Cedegao E. Zhang",
      "Zhun Wang",
      "Gal Gendron",
      "Yunze Xiao",
      "Leo Smucker",
      "Erica Weng",
      "Kwok Hao Lee",
      "Zhe Ye",
      "Stefano Ermon",
      "Ignacio D. Lopez-Miguel",
      "Theo Knights",
      "Anthony Gitter",
      "Namkyu Park",
      "Boyi Wei",
      "Hongzheng Chen",
      "Kunal Pai",
      "Ahmed Elkhanany",
      "Han Lin",
      "Philipp D. Siedler",
      "Jichao Fang",
      "Ritwik Mishra",
      "Kroly Zsolnai-Fehr",
      "Xilin Jiang",
      "Shadab Khan",
      "Jun Yuan",
      "Rishab Kumar Jain",
      "Xi Lin",
      "Mike Peterson",
      "Zhe Wang",
      "Aditya Malusare",
      "Maosen Tang",
      "Isha Gupta",
      "Ivan Fosin",
      "Timothy Kang",
      "Barbara Dworakowska",
      "Kazuki Matsumoto",
      "Guangyao Zheng",
      "Gerben Sewuster",
      "Jorge Pretel Villanueva",
      "Ivan Rannev",
      "Igor Chernyavsky",
      "Jiale Chen",
      "Deepayan Banik",
      "Ben Racz",
      "Wenchao Dong",
      "Jianxin Wang",
      "Laila Bashmal",
      "Duarte V. Gonalves",
      "Wei Hu",
      "Kaushik Bar",
      "Ondrej Bohdal",
      "Atharv Singh Patlan",
      "Shehzaad Dhuliawala",
      "Caroline Geirhos",
      "Julien Wist",
      "Yuval Kansal",
      "Bingsen Chen",
      "Kutay Tire",
      "Atak Talay Ycel",
      "Brandon Christof",
      "Veerupaksh Singla",
      "Zijian Song",
      "Sanxing Chen",
      "Jiaxin Ge",
      "Kaustubh Ponkshe",
      "Isaac Park",
      "Tianneng Shi",
      "Martin Q. Ma",
      "Joshua Mak",
      "Sherwin Lai",
      "Antoine Moulin",
      "Zhuo Cheng",
      "Zhanda Zhu",
      "Ziyi Zhang",
      "Vaidehi Patil",
      "Ketan Jha",
      "Qiutong Men",
      "Jiaxuan Wu",
      "Tianchi Zhang",
      "Bruno Hebling Vieira",
      "Alham Fikri Aji",
      "Jae-Won Chung",
      "Mohammed Mahfoud",
      "Ha Thi Hoang",
      "Marc Sperzel",
      "Wei Hao",
      "Kristof Meding",
      "Sihan Xu",
      "Vassilis Kostakos",
      "Davide Manini",
      "Yueying Liu",
      "Christopher Toukmaji",
      "Jay Paek",
      "Eunmi Yu",
      "Arif Engin Demircali",
      "Zhiyi Sun",
      "Ivan Dewerpe",
      "Hongsen Qin",
      "Roman Pflugfelder",
      "James Bailey",
      "Johnathan Morris",
      "Ville Heilala",
      "Sybille Rosset",
      "Zishun Yu",
      "Peter E. Chen",
      "Woongyeong Yeo",
      "Eeshaan Jain",
      "Ryan Yang",
      "Sreekar Chigurupati",
      "Julia Chernyavsky",
      "Sai Prajwal Reddy",
      "Subhashini Venugopalan",
      "Hunar Batra",
      "Core Francisco Park",
      "Hieu Tran",
      "Guilherme Maximiano",
      "Genghan Zhang",
      "Yizhuo Liang",
      "Hu Shiyu",
      "Rongwu Xu",
      "Rui Pan",
      "Siddharth Suresh",
      "Ziqi Liu",
      "Samaksh Gulati",
      "Songyang Zhang",
      "Peter Turchin",
      "Christopher W. Bartlett",
      "Christopher R. Scotese",
      "Phuong M. Cao",
      "Aakaash Nattanmai",
      "Gordon McKellips",
      "Anish Cheraku",
      "Asim Suhail",
      "Ethan Luo",
      "Marvin Deng",
      "Jason Luo",
      "Ashley Zhang",
      "Kavin Jindel",
      "Jay Paek",
      "Kasper Halevy",
      "Allen Baranov",
      "Michael Liu",
      "Advaith Avadhanam",
      "David Zhang",
      "Vincent Cheng",
      "Brad Ma",
      "Evan Fu",
      "Liam Do",
      "Joshua Lass",
      "Hubert Yang",
      "Surya Sunkari",
      "Vishruth Bharath",
      "Violet Ai",
      "James Leung",
      "Rishit Agrawal",
      "Alan Zhou",
      "Kevin Chen",
      "Tejas Kalpathi",
      "Ziqi Xu",
      "Gavin Wang",
      "Tyler Xiao",
      "Erik Maung",
      "Sam Lee",
      "Ryan Yang",
      "Roy Yue",
      "Ben Zhao",
      "Julia Yoon",
      "Sunny Sun",
      "Aryan Singh",
      "Ethan Luo",
      "Clark Peng",
      "Tyler Osbey",
      "Taozhi Wang",
      "Daryl Echeazu",
      "Hubert Yang",
      "Timothy Wu",
      "Spandan Patel",
      "Vidhi Kulkarni",
      "Vijaykaarti Sundarapandiyan",
      "Ashley Zhang",
      "Andrew Le",
      "Zafir Nasim",
      "Srikar Yalam",
      "Ritesh Kasamsetty",
      "Soham Samal",
      "Hubert Yang",
      "David Sun",
      "Nihar Shah",
      "Abhijeet Saha",
      "Alex Zhang",
      "Leon Nguyen",
      "Laasya Nagumalli",
      "Kaixin Wang",
      "Alan Zhou",
      "Aidan Wu",
      "Jason Luo",
      "Anwith Telluri",
      "Summer Yue",
      "Alexandr Wang",
      "Dan Hendrycks"
    ],
    "abstract": "Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 2,500\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.14249v7",
    "published_date": "2025-01-24 05:27:46 UTC",
    "updated_date": "2025-04-19 21:49:12 UTC"
  },
  {
    "arxiv_id": "2501.14238v2",
    "title": "Point-LN: A Lightweight Framework for Efficient Point Cloud Classification Using Non-Parametric Positional Encoding",
    "authors": [
      "Marzieh Mohammadi",
      "Amir Salarpour",
      "Pedram MohajerAnsari"
    ],
    "abstract": "We introduce Point-LN, a novel lightweight framework engineered for efficient\n3D point cloud classification. Point-LN integrates essential non-parametric\ncomponents-such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN),\nand non-learnable positional encoding-with a streamlined learnable classifier\nthat significantly enhances classification accuracy while maintaining a minimal\nparameter footprint. This hybrid architecture ensures low computational costs\nand rapid inference speeds, making Point-LN ideal for real-time and\nresource-constrained applications. Comprehensive evaluations on benchmark\ndatasets, including ModelNet40 and ScanObjectNN, demonstrate that Point-LN\nachieves competitive performance compared to state-of-the-art methods, all\nwhile offering exceptional efficiency. These results establish Point-LN as a\nrobust and scalable solution for diverse point cloud classification tasks,\nhighlighting its potential for widespread adoption in various computer vision\napplications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted for presentation at the 29th\n  International Computer Conference, Computer Society of Iran (CSICC) 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14238v2",
    "published_date": "2025-01-24 04:50:16 UTC",
    "updated_date": "2025-02-01 18:04:12 UTC"
  },
  {
    "arxiv_id": "2501.14228v1",
    "title": "Detection and Classification of Acute Lymphoblastic Leukemia Utilizing Deep Transfer Learning",
    "authors": [
      "Md. Abu Ahnaf Mollick",
      "Md. Mahfujur Rahman",
      "D. M. Asadujjaman",
      "Abdullah Tamim",
      "Nosin Anjum Dristi",
      "Md. Takbir Hossen"
    ],
    "abstract": "A mutation in the DNA of a single cell that compromises its function\ninitiates leukemia,leading to the overproduction of immature white blood cells\nthat encroach upon the space required for the generation of healthy blood\ncells.Leukemia is treatable if identified in its initial stages. However,its\ndiagnosis is both arduous and time consuming. This study proposes a novel\napproach for diagnosing leukemia across four stages Benign,Early,Pre,and Pro\nusing deep learning techniques.We employed two Convolutional Neural Network\n(CNN) models as MobileNetV2 with an altered head and a custom model. The custom\nmodel consists of multiple convolutional layers,each paired with corresponding\nmax pooling layers.We utilized MobileNetV2 with ImageNet weights,adjusting the\nhead to integrate the final results.The dataset used is the publicly available\n\"Acute Lymphoblastic Leukemia (ALL) Image Dataset\", and we applied the\nSynthetic Minority Oversampling Technique (SMOTE) to augment and balance the\ntraining dataset.The custom model achieved an accuracy of 98.6%, while\nMobileNetV2 attained a superior accuracy of 99.69%. The pretrained model showed\npromising results,indicating an increased likelihood of real-world application.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07",
      "J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages, 4 figures, Submitted to UCICS",
    "pdf_url": "http://arxiv.org/pdf/2501.14228v1",
    "published_date": "2025-01-24 04:16:03 UTC",
    "updated_date": "2025-01-24 04:16:03 UTC"
  },
  {
    "arxiv_id": "2501.16377v1",
    "title": "Optimal Signal Decomposition-based Multi-Stage Learning for Battery Health Estimation",
    "authors": [
      "Vijay Babu Pamshetti",
      "Wei Zhang",
      "King Jet Tseng",
      "Bor Kiat Ng",
      "Qingyu Yan"
    ],
    "abstract": "Battery health estimation is fundamental to ensure battery safety and reduce\ncost. However, achieving accurate estimation has been challenging due to the\nbatteries' complex nonlinear aging patterns and capacity regeneration\nphenomena. In this paper, we propose OSL, an optimal signal decomposition-based\nmulti-stage machine learning for battery health estimation. OSL treats battery\nsignals optimally. It uses optimized variational mode decomposition to extract\ndecomposed signals capturing different frequency bands of the original battery\nsignals. It also incorporates a multi-stage learning process to analyze both\nspatial and temporal battery features effectively. An experimental study is\nconducted with a public battery aging dataset. OSL demonstrates exceptional\nperformance with a mean error of just 0.26%. It significantly outperforms\ncomparison algorithms, both those without and those with suboptimal signal\ndecomposition and analysis. OSL considers practical battery challenges and can\nbe integrated into real-world battery management systems, offering a good\nimpact on battery monitoring and optimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.16377v1",
    "published_date": "2025-01-24 04:13:55 UTC",
    "updated_date": "2025-01-24 04:13:55 UTC"
  },
  {
    "arxiv_id": "2501.14225v2",
    "title": "Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game",
    "authors": [
      "Rong Ye",
      "Yongxin Zhang",
      "Yikai Zhang",
      "Haoyu Kuang",
      "Zhongyu Wei",
      "Peng Sun"
    ],
    "abstract": "Achieving Artificial General Intelligence (AGI) requires AI agents that can\nnot only make stratigic decisions but also engage in flexible and meaningful\ncommunication. Inspired by Wittgenstein's language game theory in Philosophical\nInvestigations, we propose that language agents can learn through in-context\ninteraction rather than traditional multi-stage frameworks that separate\ndecision-making from language expression. Using Werewolf, a social deduction\ngame that tests language understanding, strategic interaction, and\nadaptability, we develop the Multi-agent Kahneman & Tversky's Optimization\n(MaKTO). MaKTO engages diverse models in extensive gameplay to generate\nunpaired desirable and unacceptable responses, then employs KTO to refine the\nmodel's decision-making process. In 9-player Werewolf games, MaKTO achieves a\n61% average win rate across various models, outperforming GPT-4o and two-stage\nRL agents by relative improvements of 23.0% and 10.9%, respectively. Notably,\nMaKTO also demonstrates human-like performance, winning 60% against expert\nplayers and showing only 49% detectability in Turing-style blind tests.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint. Code and data will be available at\n  https://reneeye.github.io/MaKTO.html",
    "pdf_url": "http://arxiv.org/pdf/2501.14225v2",
    "published_date": "2025-01-24 04:09:03 UTC",
    "updated_date": "2025-03-13 03:55:17 UTC"
  },
  {
    "arxiv_id": "2501.14224v1",
    "title": "Top Ten Challenges Towards Agentic Neural Graph Databases",
    "authors": [
      "Jiaxin Bai",
      "Zihao Wang",
      "Yukun Zhou",
      "Hang Yin",
      "Weizhi Fei",
      "Qi Hu",
      "Zheye Deng",
      "Jiayang Cheng",
      "Tianshi Zheng",
      "Hong Ting Tsang",
      "Yisen Gao",
      "Zhongwei Xie",
      "Yufei Li",
      "Lixin Fan",
      "Binhang Yuan",
      "Wei Wang",
      "Lei Chen",
      "Xiaofang Zhou",
      "Yangqiu Song"
    ],
    "abstract": "Graph databases (GDBs) like Neo4j and TigerGraph excel at handling\ninterconnected data but lack advanced inference capabilities. Neural Graph\nDatabases (NGDBs) address this by integrating Graph Neural Networks (GNNs) for\npredictive analysis and reasoning over incomplete or noisy data. However, NGDBs\nrely on predefined queries and lack autonomy and adaptability. This paper\nintroduces Agentic Neural Graph Databases (Agentic NGDBs), which extend NGDBs\nwith three core functionalities: autonomous query construction, neural query\nexecution, and continuous learning. We identify ten key challenges in realizing\nAgentic NGDBs: semantic unit representation, abductive reasoning, scalable\nquery execution, and integration with foundation models like large language\nmodels (LLMs). By addressing these challenges, Agentic NGDBs can enable\nintelligent, self-improving systems for modern data-driven applications, paving\nthe way for adaptable and autonomous data management solutions.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "12 Pages",
    "pdf_url": "http://arxiv.org/pdf/2501.14224v1",
    "published_date": "2025-01-24 04:06:50 UTC",
    "updated_date": "2025-01-24 04:06:50 UTC"
  },
  {
    "arxiv_id": "2501.14216v3",
    "title": "TFG-Flow: Training-free Guidance in Multimodal Generative Flow",
    "authors": [
      "Haowei Lin",
      "Shanda Li",
      "Haotian Ye",
      "Yiming Yang",
      "Stefano Ermon",
      "Yitao Liang",
      "Jianzhu Ma"
    ],
    "abstract": "Given an unconditional generative model and a predictor for a target property\n(e.g., a classifier), the goal of training-free guidance is to generate samples\nwith desirable target properties without additional training. As a highly\nefficient technique for steering generative models toward flexible outcomes,\ntraining-free guidance has gained increasing attention in diffusion models.\nHowever, existing methods only handle data in continuous spaces, while many\nscientific applications involve both continuous and discrete data (referred to\nas multimodality). Another emerging trend is the growing use of the simple and\ngeneral flow matching framework in building generative foundation models, where\nguided generation remains under-explored. To address this, we introduce\nTFG-Flow, a novel training-free guidance method for multimodal generative flow.\nTFG-Flow addresses the curse-of-dimensionality while maintaining the property\nof unbiased sampling in guiding discrete variables. We validate TFG-Flow on\nfour molecular design tasks and show that TFG-Flow has great potential in drug\ndesign by generating molecules with desired properties.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14216v3",
    "published_date": "2025-01-24 03:44:16 UTC",
    "updated_date": "2025-03-18 07:30:25 UTC"
  },
  {
    "arxiv_id": "2501.14210v1",
    "title": "PuzzleGPT: Emulating Human Puzzle-Solving Ability for Time and Location Prediction",
    "authors": [
      "Hammad Ayyubi",
      "Xuande Feng",
      "Junzhang Liu",
      "Xudong Lin",
      "Zhecan Wang",
      "Shih-Fu Chang"
    ],
    "abstract": "The task of predicting time and location from images is challenging and\nrequires complex human-like puzzle-solving ability over different clues. In\nthis work, we formalize this ability into core skills and implement them using\ndifferent modules in an expert pipeline called PuzzleGPT. PuzzleGPT consists of\na perceiver to identify visual clues, a reasoner to deduce prediction\ncandidates, a combiner to combinatorially combine information from different\nclues, a web retriever to get external knowledge if the task can't be solved\nlocally, and a noise filter for robustness. This results in a zero-shot,\ninterpretable, and robust approach that records state-of-the-art performance on\ntwo datasets -- TARA and WikiTilo. PuzzleGPT outperforms large VLMs such as\nBLIP-2, InstructBLIP, LLaVA, and even GPT-4V, as well as automatically\ngenerated reasoning pipelines like VisProg, by at least 32% and 38%,\nrespectively. It even rivals or surpasses finetuned models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "NAACL 2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2501.14210v1",
    "published_date": "2025-01-24 03:28:37 UTC",
    "updated_date": "2025-01-24 03:28:37 UTC"
  },
  {
    "arxiv_id": "2501.14204v1",
    "title": "Dynamic Token Reduction during Generation for Vision Language Models",
    "authors": [
      "Xiaoyu Liang",
      "Chaofeng Guan",
      "Jiaying Lu",
      "Huiyao Chen",
      "Huan Wang",
      "Haoji Hu"
    ],
    "abstract": "Vision-Language Models (VLMs) have achieved notable success in multimodal\ntasks but face practical limitations due to the quadratic complexity of decoder\nattention mechanisms and autoregressive generation. Existing methods like FASTV\nand VTW have achieved notable results in reducing redundant visual tokens, but\nthese approaches focus on pruning tokens in a single forward pass without\nsystematically analyzing the redundancy of visual tokens throughout the entire\ngeneration process. In this paper, we introduce a dynamic pruning strategy\ntailored for VLMs, namedDynamic Rate (DyRate), which progressively adjusts the\ncompression rate during generation. Our analysis of the distribution of\nattention reveals that the importance of visual tokens decreases throughout the\ngeneration process, inspiring us to adopt a more aggressive compression rate.\nBy integrating a lightweight predictor based on attention distribution, our\napproach enables flexible adjustment of pruning rates based on the attention\ndistribution. Our experimental results demonstrate that our method not only\nreduces computational demands but also maintains the quality of responses.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14204v1",
    "published_date": "2025-01-24 03:20:37 UTC",
    "updated_date": "2025-01-24 03:20:37 UTC"
  },
  {
    "arxiv_id": "2501.14199v1",
    "title": "Coordinating Ride-Pooling with Public Transit using Reward-Guided Conservative Q-Learning: An Offline Training and Online Fine-Tuning Reinforcement Learning Framework",
    "authors": [
      "Yulong Hu",
      "Tingting Dong",
      "Sen Li"
    ],
    "abstract": "This paper introduces a novel reinforcement learning (RL) framework, termed\nReward-Guided Conservative Q-learning (RG-CQL), to enhance coordination between\nride-pooling and public transit within a multimodal transportation network. We\nmodel each ride-pooling vehicle as an agent governed by a Markov Decision\nProcess (MDP) and propose an offline training and online fine-tuning RL\nframework to learn the optimal operational decisions of the multimodal\ntransportation systems, including rider-vehicle matching, selection of drop-off\nlocations for passengers, and vehicle routing decisions, with improved data\nefficiency. During the offline training phase, we develop a Conservative Double\nDeep Q Network (CDDQN) as the action executor and a supervised learning-based\nreward estimator, termed the Guider Network, to extract valuable insights into\naction-reward relationships from data batches. In the online fine-tuning phase,\nthe Guider Network serves as an exploration guide, aiding CDDQN in effectively\nand conservatively exploring unknown state-action pairs. The efficacy of our\nalgorithm is demonstrated through a realistic case study using real-world data\nfrom Manhattan. We show that integrating ride-pooling with public transit\noutperforms two benchmark cases solo rides coordinated with transit and\nride-pooling without transit coordination by 17% and 22% in the achieved system\nrewards, respectively. Furthermore, our innovative offline training and online\nfine-tuning framework offers a remarkable 81.3% improvement in data efficiency\ncompared to traditional online RL methods with adequate exploration budgets,\nwith a 4.3% increase in total rewards and a 5.6% reduction in overestimation\nerrors. Experimental results further demonstrate that RG-CQL effectively\naddresses the challenges of transitioning from offline to online RL in\nlarge-scale ride-pooling systems integrated with transit.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14199v1",
    "published_date": "2025-01-24 03:05:04 UTC",
    "updated_date": "2025-01-24 03:05:04 UTC"
  },
  {
    "arxiv_id": "2501.14194v1",
    "title": "ENTER: Event Based Interpretable Reasoning for VideoQA",
    "authors": [
      "Hammad Ayyubi",
      "Junzhang Liu",
      "Ali Asgarov",
      "Zaber Ibn Abdul Hakim",
      "Najibul Haque Sarker",
      "Zhecan Wang",
      "Chia-Wei Tang",
      "Hani Alomari",
      "Md. Atabuzzaman",
      "Xudong Lin",
      "Naveen Reddy Dyava",
      "Shih-Fu Chang",
      "Chris Thomas"
    ],
    "abstract": "In this paper, we present ENTER, an interpretable Video Question Answering\n(VideoQA) system based on event graphs. Event graphs convert videos into\ngraphical representations, where video events form the nodes and event-event\nrelationships (temporal/causal/hierarchical) form the edges. This structured\nrepresentation offers many benefits: 1) Interpretable VideoQA via generated\ncode that parses event-graph; 2) Incorporation of contextual visual information\nin the reasoning process (code generation) via event graphs; 3) Robust VideoQA\nvia Hierarchical Iterative Update of the event graphs. Existing interpretable\nVideoQA systems are often top-down, disregarding low-level visual information\nin the reasoning plan generation, and are brittle. While bottom-up approaches\nproduce responses from visual data, they lack interpretability. Experimental\nresults on NExT-QA, IntentQA, and EgoSchema demonstrate that not only does our\nmethod outperform existing top-down approaches while obtaining competitive\nperformance against bottom-up approaches, but more importantly, offers superior\ninterpretability and explainability in the reasoning process.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14194v1",
    "published_date": "2025-01-24 02:56:59 UTC",
    "updated_date": "2025-01-24 02:56:59 UTC"
  },
  {
    "arxiv_id": "2504.03643v1",
    "title": "Potential Indicator for Continuous Emotion Arousal by Dynamic Neural Synchrony",
    "authors": [
      "Guandong Pan",
      "Zhaobang Wu",
      "Yaqian Yang",
      "Xin Wang",
      "Longzhao Liu",
      "Zhiming Zheng",
      "Shaoting Tang"
    ],
    "abstract": "The need for automatic and high-quality emotion annotation is paramount in\napplications such as continuous emotion recognition and video highlight\ndetection, yet achieving this through manual human annotations is challenging.\nInspired by inter-subject correlation (ISC) utilized in neuroscience, this\nstudy introduces a novel Electroencephalography (EEG) based ISC methodology\nthat leverages a single-electrode and feature-based dynamic approach. Our\ncontributions are three folds. Firstly, we reidentify two potent emotion\nfeatures suitable for classifying emotions-first-order difference (FD) an\ndifferential entropy (DE). Secondly, through the use of overall correlation\nanalysis, we demonstrate the heterogeneous synchronized performance of\nelectrodes. This performance aligns with neural emotion patterns established in\nprior studies, thus validating the effectiveness of our approach. Thirdly, by\nemploying a sliding window correlation technique, we showcase the significant\nconsistency of dynamic ISCs across various features or key electrodes in each\nanalyzed film clip. Our findings indicate the method's reliability in capturing\nconsistent, dynamic shared neural synchrony among individuals, triggered by\nevocative film stimuli. This underscores the potential of our approach to serve\nas an indicator of continuous human emotion arousal. The implications of this\nresearch are significant for advancements in affective computing and the\nbroader neuroscience field, suggesting a streamlined and effective tool for\nemotion analysis in real-world applications.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03643v1",
    "published_date": "2025-01-24 02:54:28 UTC",
    "updated_date": "2025-01-24 02:54:28 UTC"
  },
  {
    "arxiv_id": "2501.14189v1",
    "title": "Distributed Multi-Agent Coordination Using Multi-Modal Foundation Models",
    "authors": [
      "Saaduddin Mahmud",
      "Dorian Benhamou Goldfajn",
      "Shlomo Zilberstein"
    ],
    "abstract": "Distributed Constraint Optimization Problems (DCOPs) offer a powerful\nframework for multi-agent coordination but often rely on labor-intensive,\nmanual problem construction. To address this, we introduce VL-DCOPs, a\nframework that takes advantage of large multimodal foundation models (LFMs) to\nautomatically generate constraints from both visual and linguistic\ninstructions. We then introduce a spectrum of agent archetypes for solving\nVL-DCOPs: from a neuro-symbolic agent that delegates some of the algorithmic\ndecisions to an LFM, to a fully neural agent that depends entirely on an LFM\nfor coordination. We evaluate these agent archetypes using state-of-the-art\nLLMs (large language models) and VLMs (vision language models) on three novel\nVL-DCOP tasks and compare their respective advantages and drawbacks. Lastly, we\ndiscuss how this work extends to broader frontier challenges in the DCOP\nliterature.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14189v1",
    "published_date": "2025-01-24 02:50:21 UTC",
    "updated_date": "2025-01-24 02:50:21 UTC"
  },
  {
    "arxiv_id": "2501.16376v2",
    "title": "SwiftPrune: Hessian-Free Weight Pruning for Large Language Models",
    "authors": [
      "Yuhan Kang",
      "Yang Shi",
      "Mei We",
      "Jun He",
      "Jianchao Yang",
      "Zeyu Xue",
      "Jing Feng",
      "Xinwang Liu"
    ],
    "abstract": "Post-training pruning, as one of the key techniques for compressing large\nlanguage models, plays a vital role in lightweight model deployment and model\nsparsity. However, current mainstream pruning methods dependent on the Hessian\nmatrix face significant limitations in both pruning speed and practical\neffectiveness due to the computationally intensive nature of second-order\nderivative calculations. This paper presents SwiftPrune, a novel Hessian-free\nweight pruning method that achieves hardware-efficient model compression\nthrough two key innovations: 1) SwiftPrune eliminates the need for\ncomputationally intensive Hessian matrix calculations by introducing a\ncontribution-based weight metric, which evaluates the importance of weights\nwithout relying on second-order derivatives. 2) we employ the Exponentially\nWeighted Moving Average (EWMA) technique to bypass weight sorting, enabling the\nselection of weights that contribute most to LLM accuracy and further reducing\ntime complexity. Our approach is extended to support structured sparsity\npruning, facilitating efficient execution on modern hardware accelerators. We\nvalidate the SwiftPrune on three LLMs (namely LLaMA2, LLaMA3, and Pythia),\ndemonstrating that it significantly enhances compression performance. The\nexperimental findings reveal that SwiftPrune completes the pruning process\nwithin seconds, achieving an average speedup of 12.29x (up to 56.02x) over\nexisting SOTA approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16376v2",
    "published_date": "2025-01-24 02:50:13 UTC",
    "updated_date": "2025-05-19 00:48:23 UTC"
  },
  {
    "arxiv_id": "2501.14183v2",
    "title": "VarDrop: Enhancing Training Efficiency by Reducing Variate Redundancy in Periodic Time Series Forecasting",
    "authors": [
      "Junhyeok Kang",
      "Yooju Shin",
      "Jae-Gil Lee"
    ],
    "abstract": "Variate tokenization, which independently embeds each variate as separate\ntokens, has achieved remarkable improvements in multivariate time series\nforecasting. However, employing self-attention with variate tokens incurs a\nquadratic computational cost with respect to the number of variates, thus\nlimiting its training efficiency for large-scale applications. To address this\nissue, we propose VarDrop, a simple yet efficient strategy that reduces the\ntoken usage by omitting redundant variate tokens during training. VarDrop\nadaptively excludes redundant tokens within a given batch, thereby reducing the\nnumber of tokens used for dot-product attention while preserving essential\ninformation. Specifically, we introduce k-dominant frequency hashing (k-DFH),\nwhich utilizes the ranked dominant frequencies in the frequency domain as a\nhash value to efficiently group variate tokens exhibiting similar periodic\nbehaviors. Then, only representative tokens in each group are sampled through\nstratified sampling. By performing sparse attention with these selected tokens,\nthe computational cost of scaled dot-product attention is significantly\nalleviated. Experiments conducted on public benchmark datasets demonstrate that\nVarDrop outperforms existing efficient baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14183v2",
    "published_date": "2025-01-24 02:22:59 UTC",
    "updated_date": "2025-02-03 05:21:05 UTC"
  },
  {
    "arxiv_id": "2501.14176v1",
    "title": "RL + Transformer = A General-Purpose Problem Solver",
    "authors": [
      "Micah Rentschler",
      "Jesse Roberts"
    ],
    "abstract": "What if artificial intelligence could not only solve problems for which it\nwas trained but also learn to teach itself to solve new problems (i.e.,\nmeta-learn)? In this study, we demonstrate that a pre-trained transformer\nfine-tuned with reinforcement learning over multiple episodes develops the\nability to solve problems that it has never encountered before - an emergent\nability called In-Context Reinforcement Learning (ICRL). This powerful\nmeta-learner not only excels in solving unseen in-distribution environments\nwith remarkable sample efficiency, but also shows strong performance in\nout-of-distribution environments. In addition, we show that it exhibits\nrobustness to the quality of its training data, seamlessly stitches together\nbehaviors from its context, and adapts to non-stationary environments. These\nbehaviors demonstrate that an RL-trained transformer can iteratively improve\nupon its own solutions, making it an excellent general-purpose problem solver.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14176v1",
    "published_date": "2025-01-24 01:55:20 UTC",
    "updated_date": "2025-01-24 01:55:20 UTC"
  },
  {
    "arxiv_id": "2501.14174v5",
    "title": "Dreamweaver: Learning Compositional World Models from Pixels",
    "authors": [
      "Junyeob Baek",
      "Yi-Fu Wu",
      "Gautam Singh",
      "Sungjin Ahn"
    ],
    "abstract": "Humans have an innate ability to decompose their perceptions of the world\ninto objects and their attributes, such as colors, shapes, and movement\npatterns. This cognitive process enables us to imagine novel futures by\nrecombining familiar concepts. However, replicating this ability in artificial\nintelligence systems has proven challenging, particularly when it comes to\nmodeling videos into compositional concepts and generating unseen, recomposed\nfutures without relying on auxiliary data, such as text, masks, or bounding\nboxes. In this paper, we propose Dreamweaver, a neural architecture designed to\ndiscover hierarchical and compositional representations from raw videos and\ngenerate compositional future simulations. Our approach leverages a novel\nRecurrent Block-Slot Unit (RBSU) to decompose videos into their constituent\nobjects and attributes. In addition, Dreamweaver uses a multi-future-frame\nprediction objective to capture disentangled representations for dynamic\nconcepts more effectively as well as static concepts. In experiments, we\ndemonstrate our model outperforms current state-of-the-art baselines for world\nmodeling when evaluated under the DCI framework across multiple datasets.\nFurthermore, we show how the modularized concept representations of our model\nenable compositional imagination, allowing the generation of novel videos by\nrecombining attributes from previously seen objects.\ncun-bjy.github.io/dreamweaver-website",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14174v5",
    "published_date": "2025-01-24 01:50:19 UTC",
    "updated_date": "2025-04-10 13:12:34 UTC"
  },
  {
    "arxiv_id": "2501.14172v2",
    "title": "UltraLightSqueezeNet: A Deep Learning Architecture for Malaria Classification with up to 54x fewer trainable parameters for resource constrained devices",
    "authors": [
      "Suresh Babu Nettur",
      "Shanthi Karpurapu",
      "Unnati Nettur",
      "Likhit Sagar Gajja",
      "Sravanthy Myneni",
      "Akhil Dusi",
      "Lalithya Posham"
    ],
    "abstract": "Lightweight deep learning approaches for malaria detection have gained\nattention for their potential to enhance diagnostics in resource constrained\nenvironments. For our study, we selected SqueezeNet1.1 as it is one of the most\npopular lightweight architectures. SqueezeNet1.1 is a later version of\nSqueezeNet1.0 and is 2.4 times more computationally efficient than the original\nmodel. We proposed and implemented three ultra-lightweight architecture\nvariants to SqueezeNet1.1 architecture, namely Variant 1 (one fire module),\nVariant 2 (two fire modules), and Variant 3 (four fire modules), which are even\nmore compact than SqueezeNetV1.1 (eight fire modules). These models were\nimplemented to evaluate the best performing variant that achieves superior\ncomputational efficiency without sacrificing accuracy in malaria blood cell\nclassification. The models were trained and evaluated using the NIH Malaria\ndataset. We assessed each model's performance based on metrics including\naccuracy, recall, precision, F1-score, and Area Under the Curve (AUC). The\nresults show that the SqueezeNet1.1 model achieves the highest performance\nacross all metrics, with a classification accuracy of 97.12%. Variant 3 (four\nfire modules) offers a competitive alternative, delivering almost identical\nresults (accuracy 96.55%) with a 6x reduction in computational overhead\ncompared to SqueezeNet1.1. Variant 2 and Variant 1 perform slightly lower than\nVariant 3, with Variant 2 (two fire modules) reducing computational overhead by\n28x, and Variant 1 (one fire module) achieving a 54x reduction in trainable\nparameters compared to SqueezeNet1.1. These findings demonstrate that our\nSqueezeNet1.1 architecture variants provide a flexible approach to malaria\ndetection, enabling the selection of a variant that balances resource\nconstraints and performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Corresponding authors: Shanthi Karpurapu\n  (shanthi.karpurapu@gmail.com), Suresh Babu Nettur (nettursuresh@gmail.com)\n  Shanthi Karpurapu and Suresh Babu Nettur are co-first authors",
    "pdf_url": "http://arxiv.org/pdf/2501.14172v2",
    "published_date": "2025-01-24 01:44:48 UTC",
    "updated_date": "2025-01-31 21:40:02 UTC"
  },
  {
    "arxiv_id": "2501.14166v1",
    "title": "Enhancing Multimodal Entity Linking with Jaccard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation",
    "authors": [
      "Cong-Duy Nguyen",
      "Xiaobao Wu",
      "Thong Nguyen",
      "Shuai Zhao",
      "Khoi Le",
      "Viet-Anh Nguyen",
      "Feng Yichao",
      "Anh Tuan Luu"
    ],
    "abstract": "Previous research on multimodal entity linking (MEL) has primarily employed\ncontrastive learning as the primary objective. However, using the rest of the\nbatch as negative samples without careful consideration, these studies risk\nleveraging easy features and potentially overlook essential details that make\nentities unique. In this work, we propose JD-CCL (Jaccard Distance-based\nConditional Contrastive Learning), a novel approach designed to enhance the\nability to match multimodal entity linking models. JD-CCL leverages\nmeta-information to select negative samples with similar attributes, making the\nlinking task more challenging and robust. Additionally, to address the\nlimitations caused by the variations within the visual modality among mentions\nand entities, we introduce a novel method, CVaCPT (Contextual Visual-aid\nControllable Patch Transform). It enhances visual representations by\nincorporating multi-view synthetic images and contextual textual\nrepresentations to scale and shift patch representations. Experimental results\non benchmark MEL datasets demonstrate the strong effectiveness of our approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14166v1",
    "published_date": "2025-01-24 01:35:10 UTC",
    "updated_date": "2025-01-24 01:35:10 UTC"
  },
  {
    "arxiv_id": "2501.14165v1",
    "title": "LoCoML: A Framework for Real-World ML Inference Pipelines",
    "authors": [
      "Kritin Maddireddy",
      "Santhosh Kotekal Methukula",
      "Chandrasekar Sridhar",
      "Karthik Vaidhyanathan"
    ],
    "abstract": "The widespread adoption of machine learning (ML) has brought forth diverse\nmodels with varying architectures, and data requirements, introducing new\nchallenges in integrating these systems into real-world applications.\nTraditional solutions often struggle to manage the complexities of connecting\nheterogeneous models, especially when dealing with varied technical\nspecifications. These limitations are amplified in large-scale, collaborative\nprojects where stakeholders contribute models with different technical\nspecifications. To address these challenges, we developed LoCoML, a low-code\nframework designed to simplify the integration of diverse ML models within the\ncontext of the \\textit{Bhashini Project} - a large-scale initiative aimed at\nintegrating AI-driven language technologies such as automatic speech\nrecognition, machine translation, text-to-speech, and optical character\nrecognition to support seamless communication across more than 20 languages.\nInitial evaluations show that LoCoML adds only a small amount of computational\nload, making it efficient and effective for large-scale ML integration. Our\npractical insights show that a low-code approach can be a practical solution\nfor connecting multiple ML models in a collaborative environment.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "The paper has been accepted for presentation at the 4th International\n  Conference on AI Engineering (CAIN) 2025 co-located with 47th IEEE/ACM\n  International Conference on Software Engineering (ICSE) 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14165v1",
    "published_date": "2025-01-24 01:35:08 UTC",
    "updated_date": "2025-01-24 01:35:08 UTC"
  },
  {
    "arxiv_id": "2501.14158v2",
    "title": "Advancing MRI Reconstruction: A Systematic Review of Deep Learning and Compressed Sensing Integration",
    "authors": [
      "Mojtaba Safari",
      "Zach Eidex",
      "Chih-Wei Chang",
      "Richard L. J. Qiu",
      "Xiaofeng Yang"
    ],
    "abstract": "Magnetic resonance imaging (MRI) is a non-invasive imaging modality and\nprovides comprehensive anatomical and functional insights into the human body.\nHowever, its long acquisition times can lead to patient discomfort, motion\nartifacts, and limiting real-time applications. To address these challenges,\nstrategies such as parallel imaging have been applied, which utilize multiple\nreceiver coils to speed up the data acquisition process. Additionally,\ncompressed sensing (CS) is a method that facilitates image reconstruction from\nsparse data, significantly reducing image acquisition time by minimizing the\namount of data collection needed. Recently, deep learning (DL) has emerged as a\npowerful tool for improving MRI reconstruction. It has been integrated with\nparallel imaging and CS principles to achieve faster and more accurate MRI\nreconstructions. This review comprehensively examines DL-based techniques for\nMRI reconstruction. We categorize and discuss various DL-based methods,\nincluding end-to-end approaches, unrolled optimization, and federated learning,\nhighlighting their potential benefits. Our systematic review highlights\nsignificant contributions and underscores the potential of DL in MRI\nreconstruction. Additionally, we summarize key results and trends in DL-based\nMRI reconstruction, including quantitative metrics, the dataset, acceleration\nfactors, and the progress of and research interest in DL techniques over time.\nFinally, we discuss potential future directions and the importance of DL-based\nMRI reconstruction in advancing medical imaging. To facilitate further research\nin this area, we provide a GitHub repository that includes up-to-date DL-based\nMRI reconstruction publications and public\ndatasets-https://github.com/mosaf/Awesome-DL-based-CS-MRI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.med-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2405.00241",
    "pdf_url": "http://arxiv.org/pdf/2501.14158v2",
    "published_date": "2025-01-24 01:07:58 UTC",
    "updated_date": "2025-02-01 14:38:16 UTC"
  }
]
[
  {
    "arxiv_id": "2412.10999v3",
    "title": "Cocoa: Co-Planning and Co-Execution with AI Agents",
    "authors": [
      "K. J. Kevin Feng",
      "Kevin Pu",
      "Matt Latzke",
      "Tal August",
      "Pao Siangliulue",
      "Jonathan Bragg",
      "Daniel S. Weld",
      "Amy X. Zhang",
      "Joseph Chee Chang"
    ],
    "abstract": "Human collaboration benefits from continuous coordination -- planning,\ndelegating tasks, sharing progress, and adjusting objectives -- to align on\nshared goals. However, agentic AI systems often limit users to previewing or\nreviewing an agent's plans for fully autonomous execution. While this may be\nuseful for confirmation and correction, it does not support deeper\ncollaboration between humans and AI agents. We present Cocoa, a system that\nintroduces a novel design pattern -- interactive plans -- for collaborating\nwith an AI agent on complex, multi-step tasks. Informed by a formative study\n($n=9$), Cocoa builds on interaction designs from computational notebooks and\ndocument editors to support flexible delegation of agency through Co-planning\nand Co-execution, where users collaboratively compose and execute plans with an\nAgent. Using scientific research as a sample domain, our lab (n=16) and field\ndeployment (n=7) studies found that Cocoa improved agent steerability without\nsacrificing ease-of-use compared to a strong chat baseline. Additionally,\nresearchers valued Cocoa for real-world projects and saw the interleaving of\nco-planning and co-execution as an effective novel paradigm for human-AI\ncollaboration.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10999v3",
    "published_date": "2024-12-14 23:59:42 UTC",
    "updated_date": "2025-04-15 18:47:42 UTC"
  },
  {
    "arxiv_id": "2412.10995v1",
    "title": "RapidNet: Multi-Level Dilated Convolution Based Mobile Backbone",
    "authors": [
      "Mustafa Munir",
      "Md Mostafijur Rahman",
      "Radu Marculescu"
    ],
    "abstract": "Vision transformers (ViTs) have dominated computer vision in recent years.\nHowever, ViTs are computationally expensive and not well suited for mobile\ndevices; this led to the prevalence of convolutional neural network (CNN) and\nViT-based hybrid models for mobile vision applications. Recently, Vision GNN\n(ViG) and CNN hybrid models have also been proposed for mobile vision tasks.\nHowever, all of these methods remain slower compared to pure CNN-based models.\nIn this work, we propose Multi-Level Dilated Convolutions to devise a purely\nCNN-based mobile backbone. Using Multi-Level Dilated Convolutions allows for a\nlarger theoretical receptive field than standard convolutions. Different levels\nof dilation also allow for interactions between the short-range and long-range\nfeatures in an image. Experiments show that our proposed model outperforms\nstate-of-the-art (SOTA) mobile CNN, ViT, ViG, and hybrid architectures in terms\nof accuracy and/or speed on image classification, object detection, instance\nsegmentation, and semantic segmentation. Our fastest model, RapidNet-Ti,\nachieves 76.3\\% top-1 accuracy on ImageNet-1K with 0.9 ms inference latency on\nan iPhone 13 mini NPU, which is faster and more accurate than MobileNetV2x1.4\n(74.7\\% top-1 with 1.0 ms latency). Our work shows that pure CNN architectures\ncan beat SOTA hybrid and ViT models in terms of accuracy and speed when\ndesigned properly.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.10995v1",
    "published_date": "2024-12-14 23:39:03 UTC",
    "updated_date": "2024-12-14 23:39:03 UTC"
  },
  {
    "arxiv_id": "2412.10991v1",
    "title": "Navigating Dialectal Bias and Ethical Complexities in Levantine Arabic Hate Speech Detection",
    "authors": [
      "Ahmed Haj Ahmed",
      "Rui-Jie Yew",
      "Xerxes Minocher",
      "Suresh Venkatasubramanian"
    ],
    "abstract": "Social media platforms have become central to global communication, yet they\nalso facilitate the spread of hate speech. For underrepresented dialects like\nLevantine Arabic, detecting hate speech presents unique cultural, ethical, and\nlinguistic challenges. This paper explores the complex sociopolitical and\nlinguistic landscape of Levantine Arabic and critically examines the\nlimitations of current datasets used in hate speech detection. We highlight the\nscarcity of publicly available, diverse datasets and analyze the consequences\nof dialectal bias within existing resources. By emphasizing the need for\nculturally and contextually informed natural language processing (NLP) tools,\nwe advocate for a more nuanced and inclusive approach to hate speech detection\nin the Arab world.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10991v1",
    "published_date": "2024-12-14 23:02:46 UTC",
    "updated_date": "2024-12-14 23:02:46 UTC"
  },
  {
    "arxiv_id": "2412.10982v2",
    "title": "MedG-KRP: Medical Graph Knowledge Representation Probing",
    "authors": [
      "Gabriel R. Rosenbaum",
      "Lavender Yao Jiang",
      "Ivaxi Sheth",
      "Jaden Stryker",
      "Anton Alyakin",
      "Daniel Alexander Alber",
      "Nicolas K. Goff",
      "Young Joon Fred Kwon",
      "John Markert",
      "Mustafa Nasir-Moin",
      "Jan Moritz Niehues",
      "Karl L. Sangwon",
      "Eunice Yang",
      "Eric Karl Oermann"
    ],
    "abstract": "Large language models (LLMs) have recently emerged as powerful tools, finding\nmany medical applications. LLMs' ability to coalesce vast amounts of\ninformation from many sources to generate a response-a process similar to that\nof a human expert-has led many to see potential in deploying LLMs for clinical\nuse. However, medicine is a setting where accurate reasoning is paramount. Many\nresearchers are questioning the effectiveness of multiple choice question\nanswering (MCQA) benchmarks, frequently used to test LLMs. Researchers and\nclinicians alike must have complete confidence in LLMs' abilities for them to\nbe deployed in a medical setting. To address this need for understanding, we\nintroduce a knowledge graph (KG)-based method to evaluate the biomedical\nreasoning abilities of LLMs. Essentially, we map how LLMs link medical concepts\nin order to better understand how they reason. We test GPT-4, Llama3-70b, and\nPalmyraMed-70b, a specialized medical model. We enlist a panel of medical\nstudents to review a total of 60 LLM-generated graphs and compare these graphs\nto BIOS, a large biomedical KG. We observe GPT-4 to perform best in our human\nreview but worst in our ground truth comparison; vice-versa with PalmyraMed,\nthe medical model. Our work provides a means of visualizing the medical\nreasoning pathways of LLMs so they can be implemented in clinical settings\nsafely and effectively.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Findings paper presented at Machine Learning for Health (ML4H)\n  symposium 2024, December 15-16, 2024, Vancouver, Canada, 19 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.10982v2",
    "published_date": "2024-12-14 22:23:20 UTC",
    "updated_date": "2024-12-17 02:06:18 UTC"
  },
  {
    "arxiv_id": "2412.10981v1",
    "title": "Hybrid Forecasting of Geopolitical Events",
    "authors": [
      "Daniel M. Benjamin",
      "Fred Morstatter",
      "Ali E. Abbas",
      "Andres Abeliuk",
      "Pavel Atanasov",
      "Stephen Bennett",
      "Andreas Beger",
      "Saurabh Birari",
      "David V. Budescu",
      "Michele Catasta",
      "Emilio Ferrara",
      "Lucas Haravitch",
      "Mark Himmelstein",
      "KSM Tozammel Hossain",
      "Yuzhong Huang",
      "Woojeong Jin",
      "Regina Joseph",
      "Jure Leskovec",
      "Akira Matsui",
      "Mehrnoosh Mirtaheri",
      "Xiang Ren",
      "Gleb Satyukov",
      "Rajiv Sethi",
      "Amandeep Singh",
      "Rok Sosic",
      "Mark Steyvers",
      "Pedro A Szekely",
      "Michael D. Ward",
      "Aram Galstyan"
    ],
    "abstract": "Sound decision-making relies on accurate prediction for tangible outcomes\nranging from military conflict to disease outbreaks. To improve crowdsourced\nforecasting accuracy, we developed SAGE, a hybrid forecasting system that\ncombines human and machine generated forecasts. The system provides a platform\nwhere users can interact with machine models and thus anchor their judgments on\nan objective benchmark. The system also aggregates human and machine forecasts\nweighting both for propinquity and based on assessed skill while adjusting for\noverconfidence. We present results from the Hybrid Forecasting Competition\n(HFC) - larger than comparable forecasting tournaments - including 1085 users\nforecasting 398 real-world forecasting problems over eight months. Our main\nresult is that the hybrid system generated more accurate forecasts compared to\na human-only baseline which had no machine generated predictions. We found that\nskilled forecasters who had access to machine-generated forecasts outperformed\nthose who only viewed historical data. We also demonstrated the inclusion of\nmachine-generated forecasts in our aggregation algorithms improved performance,\nboth in terms of accuracy and scalability. This suggests that hybrid\nforecasting systems, which potentially require fewer human resources, can be a\nviable approach for maintaining a competitive level of accuracy over a larger\nnumber of forecasting questions.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "20 pages, 6 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.10981v1",
    "published_date": "2024-12-14 22:09:45 UTC",
    "updated_date": "2024-12-14 22:09:45 UTC"
  },
  {
    "arxiv_id": "2412.10975v1",
    "title": "Recursive Aggregates as Intensional Functions in Answer Set Programming: Semantics and Strong Equivalence",
    "authors": [
      "Jorge Fandinno",
      "Zachary Hansen"
    ],
    "abstract": "This paper shows that the semantics of programs with aggregates implemented\nby the solvers clingo and dlv can be characterized as extended First-Order\nformulas with intensional functions in the logic of Here-and-There.\nFurthermore, this characterization can be used to study the strong equivalence\nof programs with aggregates under either semantics. We also present a\ntransformation that reduces the task of checking strong equivalence to\nreasoning in classical First-Order logic, which serves as a foundation for\nautomating this procedure.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication in the Proceedings of the 39th Annual AAAI\n  Conference on Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2412.10975v1",
    "published_date": "2024-12-14 21:34:55 UTC",
    "updated_date": "2024-12-14 21:34:55 UTC"
  },
  {
    "arxiv_id": "2412.10968v1",
    "title": "Composers' Evaluations of an AI Music Tool: Insights for Human-Centred Design",
    "authors": [
      "Eleanor Row",
      "György Fazekas"
    ],
    "abstract": "We present a study that explores the role of user-centred design in\ndeveloping Generative AI (GenAI) tools for music composition. Through\nsemi-structured interviews with professional composers, we gathered insights on\na novel generative model for creating variations, highlighting concerns around\ntrust, transparency, and ethical design. The findings helped form a feedback\nloop, guiding improvements to the model that emphasised traceability,\ntransparency and explainability. They also revealed new areas for innovation,\nincluding novel features for controllability and research questions on the\nethical and practical implementation of GenAI models.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to NeurIPS 2024 Workshop on Generative AI and Creativity: A\n  dialogue between machine learning researchers and creative professionals in\n  Vancouver, Canada",
    "pdf_url": "http://arxiv.org/pdf/2412.10968v1",
    "published_date": "2024-12-14 20:56:23 UTC",
    "updated_date": "2024-12-14 20:56:23 UTC"
  },
  {
    "arxiv_id": "2412.10966v3",
    "title": "FlowDock: Geometric Flow Matching for Generative Protein-Ligand Docking and Affinity Prediction",
    "authors": [
      "Alex Morehead",
      "Jianlin Cheng"
    ],
    "abstract": "Powerful generative AI models of protein-ligand structure have recently been\nproposed, but few of these methods support both flexible protein-ligand docking\nand affinity estimation. Of those that do, none can directly model multiple\nbinding ligands concurrently or have been rigorously benchmarked on\npharmacologically relevant drug targets, hindering their widespread adoption in\ndrug discovery efforts. In this work, we propose FlowDock, the first deep\ngeometric generative model based on conditional flow matching that learns to\ndirectly map unbound (apo) structures to their bound (holo) counterparts for an\narbitrary number of binding ligands. Furthermore, FlowDock provides predicted\nstructural confidence scores and binding affinity values with each of its\ngenerated protein-ligand complex structures, enabling fast virtual screening of\nnew (multi-ligand) drug targets. For the well-known PoseBusters Benchmark\ndataset, FlowDock outperforms single-sequence AlphaFold 3 with a 51% blind\ndocking success rate using unbound (apo) protein input structures and without\nany information derived from multiple sequence alignments, and for the\nchallenging new DockGen-E dataset, FlowDock outperforms single-sequence\nAlphaFold 3 and matches single-sequence Chai-1 for binding pocket\ngeneralization. Additionally, in the ligand category of the 16th community-wide\nCritical Assessment of Techniques for Structure Prediction (CASP16), FlowDock\nranked among the top-5 methods for pharmacological binding affinity estimation\nacross 140 protein-ligand complexes, demonstrating the efficacy of its learned\nrepresentations in virtual screening. Source code, data, and pre-trained models\nare available at https://github.com/BioinfoMachineLearning/FlowDock.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "q-bio.QM",
      "I.2.1; J.3"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 2 tables, 2 algorithms, 11 figures. Code, data, pre-trained\n  models, and baseline method predictions are available at\n  https://github.com/BioinfoMachineLearning/FlowDock",
    "pdf_url": "http://arxiv.org/pdf/2412.10966v3",
    "published_date": "2024-12-14 20:54:37 UTC",
    "updated_date": "2025-03-24 16:50:30 UTC"
  },
  {
    "arxiv_id": "2412.10961v2",
    "title": "PSMGD: Periodic Stochastic Multi-Gradient Descent for Fast Multi-Objective Optimization",
    "authors": [
      "Mingjing Xu",
      "Peizhong Ju",
      "Jia Liu",
      "Haibo Yang"
    ],
    "abstract": "Multi-objective optimization (MOO) lies at the core of many machine learning\n(ML) applications that involve multiple, potentially conflicting objectives\n(e.g., multi-task learning, multi-objective reinforcement learning, among many\nothers). Despite the long history of MOO, recent years have witnessed a surge\nin interest within the ML community in the development of gradient manipulation\nalgorithms for MOO, thanks to the availability of gradient information in many\nML problems. However, existing gradient manipulation methods for MOO often\nsuffer from long training times, primarily due to the need for computing\ndynamic weights by solving an additional optimization problem to determine a\ncommon descent direction that can decrease all objectives simultaneously. To\naddress this challenge, we propose a new and efficient algorithm called\nPeriodic Stochastic Multi-Gradient Descent (PSMGD) to accelerate MOO. PSMGD is\nmotivated by the key observation that dynamic weights across objectives exhibit\nsmall changes under minor updates over short intervals during the optimization\nprocess. Consequently, our PSMGD algorithm is designed to periodically compute\nthese dynamic weights and utilizes them repeatedly, thereby effectively\nreducing the computational overload. Theoretically, we prove that PSMGD can\nachieve state-of-the-art convergence rates for strongly-convex, general convex,\nand non-convex functions. Additionally, we introduce a new computational\ncomplexity measure, termed backpropagation complexity, and demonstrate that\nPSMGD could achieve an objective-independent backpropagation complexity.\nThrough extensive experiments, we verify that PSMGD can provide comparable or\nsuperior performance to state-of-the-art MOO algorithms while significantly\nreducing training time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10961v2",
    "published_date": "2024-12-14 20:47:36 UTC",
    "updated_date": "2024-12-17 04:25:55 UTC"
  },
  {
    "arxiv_id": "2412.10958v3",
    "title": "SoftVQ-VAE: Efficient 1-Dimensional Continuous Tokenizer",
    "authors": [
      "Hao Chen",
      "Ze Wang",
      "Xiang Li",
      "Ximeng Sun",
      "Fangyi Chen",
      "Jiang Liu",
      "Jindong Wang",
      "Bhiksha Raj",
      "Zicheng Liu",
      "Emad Barsoum"
    ],
    "abstract": "Efficient image tokenization with high compression ratios remains a critical\nchallenge for training generative models. We present SoftVQ-VAE, a continuous\nimage tokenizer that leverages soft categorical posteriors to aggregate\nmultiple codewords into each latent token, substantially increasing the\nrepresentation capacity of the latent space. When applied to Transformer-based\narchitectures, our approach compresses 256x256 and 512x512 images using as few\nas 32 or 64 1-dimensional tokens. Not only does SoftVQ-VAE show consistent and\nhigh-quality reconstruction, more importantly, it also achieves\nstate-of-the-art and significantly faster image generation results across\ndifferent denoising-based generative models. Remarkably, SoftVQ-VAE improves\ninference throughput by up to 18x for generating 256x256 images and 55x for\n512x512 images while achieving competitive FID scores of 1.78 and 2.21 for\nSiT-XL. It also improves the training efficiency of the generative models by\nreducing the number of training iterations by 2.3x while maintaining comparable\nperformance. With its fully-differentiable design and semantic-rich latent\nspace, our experiment demonstrates that SoftVQ-VAE achieves efficient\ntokenization without compromising generation quality, paving the way for more\nefficient generative models. Code and model are released.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code and model: https://github.com/Hhhhhhao/continuous_tokenizer",
    "pdf_url": "http://arxiv.org/pdf/2412.10958v3",
    "published_date": "2024-12-14 20:29:29 UTC",
    "updated_date": "2025-03-14 22:22:40 UTC"
  },
  {
    "arxiv_id": "2412.10953v1",
    "title": "Optimizing AI-Assisted Code Generation",
    "authors": [
      "Simon Torka",
      "Sahin Albayrak"
    ],
    "abstract": "In recent years, the rise of AI-assisted code-generation tools has\nsignificantly transformed software development. While code generators have\nmainly been used to support conventional software development, their use will\nbe extended to powerful and secure AI systems. Systems capable of generating\ncode, such as ChatGPT, OpenAI Codex, GitHub Copilot, and AlphaCode, take\nadvantage of advances in machine learning (ML) and natural language processing\n(NLP) enabled by large language models (LLMs). However, it must be borne in\nmind that these models work probabilistically, which means that although they\ncan generate complex code from natural language input, there is no guarantee\nfor the functionality and security of the generated code.\n  However, to fully exploit the considerable potential of this technology, the\nsecurity, reliability, functionality, and quality of the generated code must be\nguaranteed. This paper examines the implementation of these goals to date and\nexplores strategies to optimize them. In addition, we explore how these systems\ncan be optimized to create safe, high-performance, and executable artificial\nintelligence (AI) models, and consider how to improve their accessibility to\nmake AI development more inclusive and equitable.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10953v1",
    "published_date": "2024-12-14 20:14:44 UTC",
    "updated_date": "2024-12-14 20:14:44 UTC"
  },
  {
    "arxiv_id": "2412.10950v1",
    "title": "ALPACA -- Adaptive Learning Pipeline for Comprehensive AI",
    "authors": [
      "Simon Torka",
      "Sahin Albayrak"
    ],
    "abstract": "The advancement of AI technologies has greatly increased the complexity of AI\npipelines as they include many stages such as data collection, pre-processing,\ntraining, evaluation and visualisation. To provide effective and accessible AI\nsolutions, it is important to design pipelines for different user groups such\nas experts, professionals from different fields and laypeople. Ease of use and\ntrust play a central role in the acceptance of AI systems.\n  The presented system, ALPACA (Adaptive Learning Pipeline for Advanced\nComprehensive AI Analysis), offers a comprehensive AI pipeline that addresses\nthe needs of diverse user groups. ALPACA integrates visual and code-based\ndevelopment and facilitates all key phases of the AI pipeline. Its architecture\nis based on Celery (with Redis backend) for efficient task management, MongoDB\nfor seamless data storage and Kubernetes for cloud-based scalability and\nresource utilisation.\n  Future versions of ALPACA will support modern techniques such as federated\nand continuous learning as well as explainable AI methods to further improve\nsecurity, usability and trustworthiness. The application is demonstrated by an\nAndroid app for similarity recognition, which emphasises ALPACA's potential for\nuse in everyday life.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10950v1",
    "published_date": "2024-12-14 20:10:18 UTC",
    "updated_date": "2024-12-14 20:10:18 UTC"
  },
  {
    "arxiv_id": "2412.10941v1",
    "title": "APAR: Modeling Irregular Target Functions in Tabular Regression via Arithmetic-Aware Pre-Training and Adaptive-Regularized Fine-Tuning",
    "authors": [
      "Hong-Wei Wu",
      "Wei-Yao Wang",
      "Kuang-Da Wang",
      "Wen-Chih Peng"
    ],
    "abstract": "Tabular data are fundamental in common machine learning applications, ranging\nfrom finance to genomics and healthcare. This paper focuses on tabular\nregression tasks, a field where deep learning (DL) methods are not consistently\nsuperior to machine learning (ML) models due to the challenges posed by\nirregular target functions inherent in tabular data, causing sensitive label\nchanges with minor variations from features. To address these issues, we\npropose a novel Arithmetic-Aware Pre-training and Adaptive-Regularized\nFine-tuning framework (APAR), which enables the model to fit irregular target\nfunction in tabular data while reducing the negative impact of overfitting. In\nthe pre-training phase, APAR introduces an arithmetic-aware pretext objective\nto capture intricate sample-wise relationships from the perspective of\ncontinuous labels. In the fine-tuning phase, a consistency-based adaptive\nregularization technique is proposed to self-learn appropriate data\naugmentation. Extensive experiments across 10 datasets demonstrated that APAR\noutperforms existing GBDT-, supervised NN-, and pretrain-finetune NN-based\nmethods in RMSE (+9.43% $\\sim$ 20.37%), and empirically validated the effects\nof pre-training tasks, including the study of arithmetic operations. Our code\nand data are publicly available at https://github.com/johnnyhwu/APAR.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI 2025 Main Track",
    "pdf_url": "http://arxiv.org/pdf/2412.10941v1",
    "published_date": "2024-12-14 19:33:21 UTC",
    "updated_date": "2024-12-14 19:33:21 UTC"
  },
  {
    "arxiv_id": "2412.10939v1",
    "title": "Human-Centric NLP or AI-Centric Illusion?: A Critical Investigation",
    "authors": [
      "Piyapath T Spencer"
    ],
    "abstract": "Human-Centric NLP often claims to prioritise human needs and values, yet many\nimplementations reveal an underlying AI-centric focus. Through an analysis of\ncase studies in language modelling, behavioural testing, and multi-modal\nalignment, this study identifies a significant gap between the ideas of\nhuman-centricity and actual practices. Key issues include misalignment with\nhuman-centred design principles, the reduction of human factors to mere\nbenchmarks, and insufficient consideration of real-world impacts. The\ndiscussion explores whether Human-Centric NLP embodies true human-centred\ndesign, emphasising the need for interdisciplinary collaboration and ethical\nconsiderations. The paper advocates for a redefinition of Human-Centric NLP,\nurging a broader focus on real-world utility and societal implications to\nensure that language technologies genuinely serve and empower users.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "Preprint to be published in Proceedings of PACLIC38",
    "pdf_url": "http://arxiv.org/pdf/2412.10939v1",
    "published_date": "2024-12-14 19:16:53 UTC",
    "updated_date": "2024-12-14 19:16:53 UTC"
  },
  {
    "arxiv_id": "2412.10925v1",
    "title": "Video Representation Learning with Joint-Embedding Predictive Architectures",
    "authors": [
      "Katrina Drozdov",
      "Ravid Shwartz-Ziv",
      "Yann LeCun"
    ],
    "abstract": "Video representation learning is an increasingly important topic in machine\nlearning research. We present Video JEPA with Variance-Covariance\nRegularization (VJ-VCR): a joint-embedding predictive architecture for\nself-supervised video representation learning that employs variance and\ncovariance regularization to avoid representation collapse. We show that hidden\nrepresentations from our VJ-VCR contain abstract, high-level information about\nthe input data. Specifically, they outperform representations obtained from a\ngenerative baseline on downstream tasks that require understanding of the\nunderlying dynamics of moving objects in the videos. Additionally, we explore\ndifferent ways to incorporate latent variables into the VJ-VCR framework that\ncapture information about uncertainty in the future in non-deterministic\nsettings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10925v1",
    "published_date": "2024-12-14 18:33:29 UTC",
    "updated_date": "2024-12-14 18:33:29 UTC"
  },
  {
    "arxiv_id": "2412.10924v4",
    "title": "Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning",
    "authors": [
      "Julia Witte Zimmerman",
      "Denis Hudon",
      "Kathryn Cramer",
      "Alejandro J. Ruiz",
      "Calla Beauregard",
      "Ashley Fehr",
      "Mikaela Irene Fudolig",
      "Bradford Demarest",
      "Yoshi Meke Bird",
      "Milo Z. Trujillo",
      "Christopher M. Danforth",
      "Peter Sheridan Dodds"
    ],
    "abstract": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens and current\nstructural constraints motivate changes to existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokens and pretraining can act as a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being arguably\nmeaningfully insulated from the main system intelligence. [First uploaded to\narXiv in December, 2024.]",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10924v4",
    "published_date": "2024-12-14 18:18:52 UTC",
    "updated_date": "2025-04-13 16:17:45 UTC"
  },
  {
    "arxiv_id": "2412.10919v1",
    "title": "Predicting Survival of Hemodialysis Patients using Federated Learning",
    "authors": [
      "Abhiram Raju",
      "Praneeth Vepakomma"
    ],
    "abstract": "Hemodialysis patients who are on donor lists for kidney transplant may get\nmisidentified, delaying their wait time. Thus, predicting their survival time\nis crucial for optimizing waiting lists and personalizing treatment plans.\nPredicting survival times for patients often requires large quantities of high\nquality but sensitive data. This data is siloed and since individual datasets\nare smaller and less diverse, locally trained survival models do not perform as\nwell as centralized ones. Hence, we propose the use of Federated Learning in\nthe context of predicting survival for hemodialysis patients. Federated\nLearning or FL can have comparatively better performances than local models\nwhile not sharing data between centers. However, despite the increased use of\nsuch technologies, the application of FL in survival and even more, dialysis\npatients remains sparse. This paper studies the performance of FL for data of\nhemodialysis patients from NephroPlus, the largest private network of dialysis\ncenters in India.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 2 figures, 4 tables, Presented at MIT Undergraduate Research\n  Technology Conference and to be published as conference proceeding at IEEE\n  Xplore",
    "pdf_url": "http://arxiv.org/pdf/2412.10919v1",
    "published_date": "2024-12-14 18:10:44 UTC",
    "updated_date": "2024-12-14 18:10:44 UTC"
  },
  {
    "arxiv_id": "2412.10918v1",
    "title": "LLMs-in-the-Loop Part 2: Expert Small AI Models for Anonymization and De-identification of PHI Across Multiple Languages",
    "authors": [
      "Murat Gunay",
      "Bunyamin Keles",
      "Raife Hizlan"
    ],
    "abstract": "The rise of chronic diseases and pandemics like COVID-19 has emphasized the\nneed for effective patient data processing while ensuring privacy through\nanonymization and de-identification of protected health information (PHI).\nAnonymized data facilitates research without compromising patient\nconfidentiality. This paper introduces expert small AI models developed using\nthe LLM-in-the-loop methodology to meet the demand for domain-specific\nde-identification NER models. These models overcome the privacy risks\nassociated with large language models (LLMs) used via APIs by eliminating the\nneed to transmit or store sensitive data. More importantly, they consistently\noutperform LLMs in de-identification tasks, offering superior performance and\nreliability. Our de-identification NER models, developed in eight languages\n(English, German, Italian, French, Romanian, Turkish, Spanish, and Arabic)\nachieved f1-micro score averages of 0.966, 0.975, 0.976, 0.970, 0.964, 0.974,\n0.978, and 0.953 respectively. These results establish them as the most\naccurate healthcare anonymization solutions, surpassing existing small models\nand even general-purpose LLMs such as GPT-4o. While Part-1 of this series\nintroduced the LLM-in-the-loop methodology for bio-medical document\ntranslation, this second paper showcases its success in developing\ncost-effective expert small NER models in de-identification tasks. Our findings\nlay the groundwork for future healthcare AI innovations, including biomedical\nentity and relation extraction, demonstrating the value of specialized models\nfor domain-specific challenges.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.10918v1",
    "published_date": "2024-12-14 18:10:29 UTC",
    "updated_date": "2024-12-14 18:10:29 UTC"
  },
  {
    "arxiv_id": "2412.10917v2",
    "title": "Adaptive Reward Design for Reinforcement Learning",
    "authors": [
      "Minjae Kwon",
      "Ingy ElSayed-Aly",
      "Lu Feng"
    ],
    "abstract": "There is a surge of interest in using formal languages such as Linear\nTemporal Logic (LTL) to precisely and succinctly specify complex tasks and\nderive reward functions for Reinforcement Learning (RL). However, existing\nmethods often assign sparse rewards (e.g., giving a reward of 1 only if a task\nis completed and 0 otherwise). By providing feedback solely upon task\ncompletion, these methods fail to encourage successful subtask completion. This\nis particularly problematic in environments with inherent uncertainty, where\ntask completion may be unreliable despite progress on intermediate goals. To\naddress this limitation, we propose a suite of reward functions that\nincentivize an RL agent to complete a task specified by an LTL formula as much\nas possible, and develop an adaptive reward shaping approach that dynamically\nupdates reward functions during the learning process. Experimental results on a\nrange of benchmark RL environments demonstrate that the proposed approach\ngenerally outperforms baselines, achieving earlier convergence to a better\npolicy with higher expected return and task completion rate.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "68T40 (Primary) 93E35, 03B44 (Secondary)"
    ],
    "primary_category": "cs.RO",
    "comment": "UAI 2025 Camera Ready Version",
    "pdf_url": "http://arxiv.org/pdf/2412.10917v2",
    "published_date": "2024-12-14 18:04:18 UTC",
    "updated_date": "2025-05-17 21:14:03 UTC"
  },
  {
    "arxiv_id": "2412.10912v2",
    "title": "ST-FiT: Inductive Spatial-Temporal Forecasting with Limited Training Data",
    "authors": [
      "Zhenyu Lei",
      "Yushun Dong",
      "Jundong Li",
      "Chen Chen"
    ],
    "abstract": "Spatial-temporal graphs are widely used in a variety of real-world\napplications. Spatial-Temporal Graph Neural Networks (STGNNs) have emerged as a\npowerful tool to extract meaningful insights from this data. However, in\nreal-world applications, most nodes may not possess any available temporal data\nduring training. For example, the pandemic dynamics of most cities on a\ngeographical graph may not be available due to the asynchronous nature of\noutbreaks. Such a phenomenon disagrees with the training requirements of most\nexisting spatial-temporal forecasting methods, which jeopardizes their\neffectiveness and thus blocks broader deployment. In this paper, we propose to\nformulate a novel problem of inductive forecasting with limited training data.\nIn particular, given a spatial-temporal graph, we aim to learn a\nspatial-temporal forecasting model that can be easily generalized onto those\nnodes without any available temporal training data. To handle this problem, we\npropose a principled framework named ST-FiT. ST-FiT consists of two key\nlearning components: temporal data augmentation and spatial graph topology\nlearning. With such a design, ST-FiT can be used on top of any existing STGNNs\nto achieve superior performance on the nodes without training data. Extensive\nexperiments verify the effectiveness of ST-FiT in multiple key perspectives.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10912v2",
    "published_date": "2024-12-14 17:51:29 UTC",
    "updated_date": "2024-12-17 02:29:37 UTC"
  },
  {
    "arxiv_id": "2412.10904v1",
    "title": "CEKER: A Generalizable LLM Framework for Literature Analysis with a Case Study in Unikernel Security",
    "authors": [
      "Alex Wollman",
      "John Hastings"
    ],
    "abstract": "Literature reviews are a critical component of formulating and justifying new\nresearch, but are a manual and often time-consuming process. This research\nintroduces a novel, generalizable approach to literature analysis called CEKER\nwhich uses a three-step process to streamline the collection of literature, the\nextraction of key insights, and the summarized analysis of key trends and gaps.\nLeveraging Large Language Models (LLMs), this methodology represents a\nsignificant shift from traditional manual literature reviews, offering a\nscalable, flexible, and repeatable approach that can be applied across diverse\nresearch domains.\n  A case study on unikernel security illustrates CEKER's ability to generate\nnovel insights validated against previous manual methods. CEKER's analysis\nhighlighted reduced attack surface as the most prominent theme. Key security\ngaps included the absence of Address Space Layout Randomization, missing\ndebugging tools, and limited entropy generation, all of which represent\nimportant challenges to unikernel security. The study also revealed a reliance\non hypervisors as a potential attack vector and emphasized the need for dynamic\nsecurity adjustments to address real-time threats.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "K.6.5; I.2.7; H.3.1; A.1; D.4.1"
    ],
    "primary_category": "cs.CR",
    "comment": "7 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10904v1",
    "published_date": "2024-12-14 17:28:43 UTC",
    "updated_date": "2024-12-14 17:28:43 UTC"
  },
  {
    "arxiv_id": "2412.10893v1",
    "title": "BgGPT 1.0: Extending English-centric LLMs to other languages",
    "authors": [
      "Anton Alexandrov",
      "Veselin Raychev",
      "Dimitar I. Dimitrov",
      "Ce Zhang",
      "Martin Vechev",
      "Kristina Toutanova"
    ],
    "abstract": "We present BgGPT-Gemma-2-27B-Instruct and BgGPT-Gemma-2-9B-Instruct:\ncontinually pretrained and fine-tuned versions of Google's Gemma-2 models,\nspecifically optimized for Bulgarian language understanding and generation.\nLeveraging Gemma-2's multilingual capabilities and over 100 billion tokens of\nBulgarian and English text data, our models demonstrate strong performance in\nBulgarian language tasks, setting a new standard for language-specific AI\nmodels. Our approach maintains the robust capabilities of the original Gemma-2\nmodels, ensuring that the English language performance remains intact. To\npreserve the base model capabilities, we incorporate continual learning\nstrategies based on recent Branch-and-Merge techniques as well as thorough\ncuration and selection of training data. We provide detailed insights into our\nmethodology, including the release of model weights with a commercial-friendly\nlicense, enabling broader adoption by researchers, companies, and hobbyists.\nFurther, we establish a comprehensive set of benchmarks based on non-public\neducational data sources to evaluate models on Bulgarian language tasks as well\nas safety and chat capabilities. Our findings demonstrate the effectiveness of\nfine-tuning state-of-the-art models like Gemma 2 to enhance language-specific\nAI applications while maintaining cross-lingual capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10893v1",
    "published_date": "2024-12-14 16:49:52 UTC",
    "updated_date": "2024-12-14 16:49:52 UTC"
  },
  {
    "arxiv_id": "2412.10892v2",
    "title": "Know Unreported Roadway Incidents in Real-time: Early Traffic Anomaly Detection",
    "authors": [
      "Haocheng Duan",
      "Hao Wu",
      "Sean Qian"
    ],
    "abstract": "This research aims to know traffic anomalies as early as possible. A traffic\nanomaly refers to a generic incident on the road that influences traffic flow\nand calls for urgent traffic management measures. `Knowing'' the occurrence of\na traffic anomaly is twofold: the ability to detect this anomaly before it is\nreported anywhere, or it may be such that an anomaly can be predicted before it\nactually occurs on the road (e.g., non-recurrent traffic breakdown). In either\nway, the objective is to inform traffic operators of unreported incidents in\nreal time and as early as possible. The key is to stay ahead of the curve. Time\nis of the essence.\n  Conventional automatic incident detection (AID) methods often struggle with\nearly detection due to their limited consideration of spatial effects and\nearly-stage characteristics. Therefore, we propose a deep learning framework\nutilizing prior domain knowledge and model-designing strategies. This allows\nthe model to detect a broader range of anomalies, not only incidents that\nsignificantly influence traffic flow but also early characteristics of\nincidents along with historically unreported anomalies. We specially design the\nmodel to target the early-stage detection/prediction of an incident.\nAdditionally, unlike most conventional AID studies, our method is highly\nscalable and generalizable, as it is fully automated with no manual selection\nof historical reports required, relies solely on widely available low-cost\ndata, and requires no additional detectors. The experimental results across\nnumerous road segments on different maps demonstrate that our model leads to\nmore effective and early anomaly detection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10892v2",
    "published_date": "2024-12-14 16:49:29 UTC",
    "updated_date": "2025-04-23 18:02:35 UTC"
  },
  {
    "arxiv_id": "2412.10871v1",
    "title": "Fully Test-time Adaptation for Tabular Data",
    "authors": [
      "Zhi Zhou",
      "Kun-Yang Yu",
      "Lan-Zhe Guo",
      "Yu-Feng Li"
    ],
    "abstract": "Tabular data plays a vital role in various real-world scenarios and finds\nextensive applications. Although recent deep tabular models have shown\nremarkable success, they still struggle to handle data distribution shifts,\nleading to performance degradation when testing distributions change. To remedy\nthis, a robust tabular model must adapt to generalize to unknown distributions\nduring testing. In this paper, we investigate the problem of fully test-time\nadaptation (FTTA) for tabular data, where the model is adapted using only the\ntesting data. We identify three key challenges: the existence of label and\ncovariate distribution shifts, the lack of effective data augmentation, and the\nsensitivity of adaptation, which render existing FTTA methods ineffective for\ntabular data. To this end, we propose the Fully Test-time Adaptation for\nTabular data, namely FTAT, which enables FTTA methods to robustly optimize the\nlabel distribution of predictions, adapt to shifted covariate distributions,\nand suit a variety of tasks and models effectively. We conduct comprehensive\nexperiments on six benchmark datasets, which are evaluated using three metrics.\nThe experimental results demonstrate that FTAT outperforms state-of-the-art\nmethods by a margin.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025. Code is available at:\n  https://wnjxyk.github.io/FTTA",
    "pdf_url": "http://arxiv.org/pdf/2412.10871v1",
    "published_date": "2024-12-14 15:49:53 UTC",
    "updated_date": "2024-12-14 15:49:53 UTC"
  },
  {
    "arxiv_id": "2412.10869v2",
    "title": "TinySubNets: An efficient and low capacity continual learning strategy",
    "authors": [
      "Marcin Pietroń",
      "Kamil Faber",
      "Dominik Żurek",
      "Roberto Corizzo"
    ],
    "abstract": "Continual Learning (CL) is a highly relevant setting gaining traction in\nrecent machine learning research. Among CL works, architectural and hybrid\nstrategies are particularly effective due to their potential to adapt the model\narchitecture as new tasks are presented. However, many existing solutions do\nnot efficiently exploit model sparsity, and are prone to capacity saturation\ndue to their inefficient use of available weights, which limits the number of\nlearnable tasks. In this paper, we propose TinySubNets (TSN), a novel\narchitectural CL strategy that addresses the issues through the unique\ncombination of pruning with different sparsity levels, adaptive quantization,\nand weight sharing. Pruning identifies a subset of weights that preserve model\nperformance, making less relevant weights available for future tasks. Adaptive\nquantization allows a single weight to be separated into multiple parts which\ncan be assigned to different tasks. Weight sharing between tasks boosts the\nexploitation of capacity and task similarity, allowing for the identification\nof a better trade-off between model accuracy and capacity. These features allow\nTSN to efficiently leverage the available capacity, enhance knowledge transfer,\nand reduce computational resource consumption. Experimental results involving\ncommon benchmark CL datasets and scenarios show that our proposed strategy\nachieves better results in terms of accuracy than existing state-of-the-art CL\nstrategies. Moreover, our strategy is shown to provide a significantly improved\nmodel capacity exploitation. Code released at:\nhttps://github.com/lifelonglab/tinysubnets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10869v2",
    "published_date": "2024-12-14 15:43:38 UTC",
    "updated_date": "2025-02-25 16:10:06 UTC"
  },
  {
    "arxiv_id": "2412.10861v1",
    "title": "Heterogeneous Graph Transformer for Multiple Tiny Object Tracking in RGB-T Videos",
    "authors": [
      "Qingyu Xu",
      "Longguang Wang",
      "Weidong Sheng",
      "Yingqian Wang",
      "Chao Xiao",
      "Chao Ma",
      "Wei An"
    ],
    "abstract": "Tracking multiple tiny objects is highly challenging due to their weak\nappearance and limited features. Existing multi-object tracking algorithms\ngenerally focus on single-modality scenes, and overlook the complementary\ncharacteristics of tiny objects captured by multiple remote sensors. To enhance\ntracking performance by integrating complementary information from multiple\nsources, we propose a novel framework called {HGT-Track (Heterogeneous Graph\nTransformer based Multi-Tiny-Object Tracking)}. Specifically, we first employ a\nTransformer-based encoder to embed images from different modalities.\nSubsequently, we utilize Heterogeneous Graph Transformer to aggregate spatial\nand temporal information from multiple modalities to generate detection and\ntracking features. Additionally, we introduce a target re-detection module\n(ReDet) to ensure tracklet continuity by maintaining consistency across\ndifferent modalities. Furthermore, this paper introduces the first benchmark\nVT-Tiny-MOT (Visible-Thermal Tiny Multi-Object Tracking) for RGB-T fused\nmultiple tiny object tracking. Extensive experiments are conducted on\nVT-Tiny-MOT, and the results have demonstrated the effectiveness of our method.\nCompared to other state-of-the-art methods, our method achieves better\nperformance in terms of MOTA (Multiple-Object Tracking Accuracy) and ID-F1\nscore. The code and dataset will be made available at\nhttps://github.com/xuqingyu26/HGTMT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "N/A",
    "pdf_url": "http://arxiv.org/pdf/2412.10861v1",
    "published_date": "2024-12-14 15:17:49 UTC",
    "updated_date": "2024-12-14 15:17:49 UTC"
  },
  {
    "arxiv_id": "2412.10849v2",
    "title": "Superhuman performance of a large language model on the reasoning tasks of a physician",
    "authors": [
      "Peter G. Brodeur",
      "Thomas A. Buckley",
      "Zahir Kanjee",
      "Ethan Goh",
      "Evelyn Bin Ling",
      "Priyank Jain",
      "Stephanie Cabral",
      "Raja-Elie Abdulnour",
      "Adrian D. Haimovich",
      "Jason A. Freed",
      "Andrew Olson",
      "Daniel J. Morgan",
      "Jason Hom",
      "Robert Gallo",
      "Liam G. McCoy",
      "Haadi Mombini",
      "Christopher Lucas",
      "Misha Fotoohi",
      "Matthew Gwiazdon",
      "Daniele Restifo",
      "Daniel Restrepo",
      "Eric Horvitz",
      "Jonathan Chen",
      "Arjun K. Manrai",
      "Adam Rodman"
    ],
    "abstract": "A seminal paper published by Ledley and Lusted in 1959 introduced complex\nclinical diagnostic reasoning cases as the gold standard for the evaluation of\nexpert medical computing systems, a standard that has held ever since. Here, we\nreport the results of a physician evaluation of a large language model (LLM) on\nchallenging clinical cases against a baseline of hundreds of physicians. We\nconduct five experiments to measure clinical reasoning across differential\ndiagnosis generation, display of diagnostic reasoning, triage differential\ndiagnosis, probabilistic reasoning, and management reasoning, all adjudicated\nby physician experts with validated psychometrics. We then report a real-world\nstudy comparing human expert and AI second opinions in randomly-selected\npatients in the emergency room of a major tertiary academic medical center in\nBoston, MA. We compared LLMs and board-certified physicians at three predefined\ndiagnostic touchpoints: triage in the emergency room, initial evaluation by a\nphysician, and admission to the hospital or intensive care unit. In all\nexperiments--both vignettes and emergency room second opinions--the LLM\ndisplayed superhuman diagnostic and reasoning abilities, as well as continued\nimprovement from prior generations of AI clinical decision support. Our study\nsuggests that LLMs have achieved superhuman performance on general medical\ndiagnostic and management reasoning, fulfilling the vision put forth by Ledley\nand Lusted, and motivating the urgent need for prospective trials.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10849v2",
    "published_date": "2024-12-14 14:46:18 UTC",
    "updated_date": "2025-05-19 14:26:07 UTC"
  },
  {
    "arxiv_id": "2412.10848v1",
    "title": "Large Language Models for Medical Forecasting -- Foresight 2",
    "authors": [
      "Zeljko Kraljevic",
      "Joshua Au Yeung",
      "Daniel Bean",
      "James Teo",
      "Richard J. Dobson"
    ],
    "abstract": "Foresight 2 (FS2) is a large language model fine-tuned on hospital data for\nmodelling patient timelines (GitHub 'removed for anon'). It can understand\npatients' clinical notes and predict SNOMED codes for a wide range of\nbiomedical use cases, including diagnosis suggestions, risk forecasting, and\nprocedure and medication recommendations. FS2 is trained on the free text\nportion of the MIMIC-III dataset, firstly through extracting biomedical\nconcepts and then creating contextualised patient timelines, upon which the\nmodel is then fine-tuned. The results show significant improvement over the\nprevious state-of-the-art for the next new biomedical concept prediction (P/R -\n0.73/0.66 vs 0.52/0.32) and a similar improvement specifically for the next new\ndisorder prediction (P/R - 0.69/0.62 vs 0.46/0.25). Finally, on the task of\nrisk forecast, we compare our model to GPT-4-turbo (and a range of open-source\nbiomedical LLMs) and show that FS2 performs significantly better on such tasks\n(P@5 - 0.90 vs 0.65). This highlights the need to incorporate hospital data\ninto LLMs and shows that small models outperform much larger ones when\nfine-tuned on high-quality, specialised data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10848v1",
    "published_date": "2024-12-14 14:45:28 UTC",
    "updated_date": "2024-12-14 14:45:28 UTC"
  },
  {
    "arxiv_id": "2412.17833v1",
    "title": "Transfer Learning with Active Sampling for Rapid Training and Calibration in BCI-P300 Across Health States and Multi-centre Data",
    "authors": [
      "Christian Flores",
      "Marcelo Contreras",
      "Ichiro Macedo",
      "Javier Andreu-Perez"
    ],
    "abstract": "Machine learning and deep learning advancements have boosted Brain-Computer\nInterface (BCI) performance, but their wide-scale applicability is limited due\nto factors like individual health, hardware variations, and cultural\ndifferences affecting neural data. Studies often focus on uniform single-site\nexperiments in uniform settings, leading to high performance that may not\ntranslate well to real-world diversity. Deep learning models aim to enhance BCI\nclassification accuracy, and transfer learning has been suggested to adapt\nmodels to individual neural patterns using a base model trained on others'\ndata. This approach promises better generalizability and reduced overfitting,\nyet challenges remain in handling diverse and imbalanced datasets from\ndifferent equipment, subjects, multiple centres in different countries, and\nboth healthy and patient populations for effective model transfer and tuning.\n  In a setting characterized by maximal heterogeneity, we proposed P300 wave\ndetection in BCIs employing a convolutional neural network fitted with adaptive\ntransfer learning based on Poison Sampling Disk (PDS) called Active Sampling\n(AS), which flexibly adjusts the transition from source data to the target\ndomain. Our results reported for subject adaptive with 40% of adaptive\nfine-tuning that the averaged classification accuracy improved by 5.36% and\nstandard deviation reduced by 12.22% using two distinct, internationally\nreplicated datasets. These results outperformed in classification accuracy,\ncomputational time, and training efficiency, mainly due to the proposed Active\nSampling (AS) method for transfer learning.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "68",
      "I.2; J.6"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.17833v1",
    "published_date": "2024-12-14 14:20:21 UTC",
    "updated_date": "2024-12-14 14:20:21 UTC"
  },
  {
    "arxiv_id": "2412.10838v1",
    "title": "Deep Learning Models for Colloidal Nanocrystal Synthesis",
    "authors": [
      "Kai Gu",
      "Yingping Liang",
      "Jiaming Su",
      "Peihan Sun",
      "Jia Peng",
      "Naihua Miao",
      "Zhimei Sun",
      "Ying Fu",
      "Haizheng Zhong",
      "Jun Zhang"
    ],
    "abstract": "Colloidal synthesis of nanocrystals usually includes complex chemical\nreactions and multi-step crystallization processes. Despite the great success\nin the past 30 years, it remains challenging to clarify the correlations\nbetween synthetic parameters of chemical reaction and physical properties of\nnanocrystals. Here, we developed a deep learning-based nanocrystal synthesis\nmodel that correlates synthetic parameters with the final size and shape of\ntarget nanocrystals, using a dataset of 3500 recipes covering 348 distinct\nnanocrystal compositions. The size and shape labels were obtained from\ntransmission electron microscope images using a segmentation model trained with\na semi-supervised algorithm on a dataset comprising 1.2 million nanocrystals.\nBy applying the reaction intermediate-based data augmentation method and\nelaborated descriptors, the synthesis model was able to predict nanocrystal's\nsize with a mean absolute error of 1.39 nm, while reaching an 89% average\naccuracy for shape classification. The synthesis model shows knowledge transfer\ncapabilities across different nanocrystals with inputs of new recipes. With\nthat, the influence of chemicals on the final size of nanocrystals was further\nevaluated, revealing the importance order of nanocrystal composition, precursor\nor ligand, and solvent. Overall, the deep learning-based nanocrystal synthesis\nmodel offers a powerful tool to expedite the development of high-quality\nnanocrystals.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.app-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10838v1",
    "published_date": "2024-12-14 14:18:59 UTC",
    "updated_date": "2024-12-14 14:18:59 UTC"
  },
  {
    "arxiv_id": "2412.12198v1",
    "title": "Pop-out vs. Glue: A Study on the pre-attentive and focused attention stages in Visual Search tasks",
    "authors": [
      "Hendrik Beukelman",
      "Wilder C. Rodrigues"
    ],
    "abstract": "This study explores visual search asymmetry and the detection process between\nparallel and serial search strategies, building upon Treisman's Feature\nIntegration Theory [3]. Our experiment examines how easy it is to locate an\noblique line among vertical distractors versus a vertical line among oblique\ndistractors, a framework previously validated by Treisman & Gormican (1988) [4]\nand Gupta et al. (2015) [1]. We hypothesised that an oblique target among\nvertical lines would produce a perceptual 'pop-out' effect, allowing for\nfaster, parallel search, while the reverse condition would require serial\nsearch strategy. Seventy-eight participants from Utrecht University engaged in\ntrials with varied target-distractor orientations and number of items. We\nmeasured reaction times and found a significant effect of target type on search\nspeed: oblique targets were identified more quickly, reflecting 'pop-out'\nbehaviour, while vertical targets demanded focused attention ('glue phase').\nOur results align with past findings, supporting our hypothesis on search\nasymmetry and its dependency on distinct visual features. Future research could\nbenefit from eye-tracking and neural network analysis, particularly for\nidentifying the neural processing of visual features in both parallel and\nserial search conditions.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "q-bio.NC",
    "comment": "Replication of Gupta et al work from 2015 paper",
    "pdf_url": "http://arxiv.org/pdf/2412.12198v1",
    "published_date": "2024-12-14 13:31:27 UTC",
    "updated_date": "2024-12-14 13:31:27 UTC"
  },
  {
    "arxiv_id": "2412.10827v3",
    "title": "Rethinking Chain-of-Thought from the Perspective of Self-Training",
    "authors": [
      "Zongqian Wu",
      "Baoduo Xu",
      "Ruochen Cui",
      "Mengmeng Zhan",
      "Xiaofeng Zhu",
      "Lei Feng"
    ],
    "abstract": "Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10827v3",
    "published_date": "2024-12-14 13:12:50 UTC",
    "updated_date": "2025-02-12 11:41:16 UTC"
  },
  {
    "arxiv_id": "2412.10826v1",
    "title": "Generative AI: A Pix2pix-GAN-Based Machine Learning Approach for Robust and Efficient Lung Segmentation",
    "authors": [
      "Sharmin Akter"
    ],
    "abstract": "Chest radiography is climacteric in identifying different pulmonary diseases,\nyet radiologist workload and inefficiency can lead to misdiagnoses. Automatic,\naccurate, and efficient segmentation of lung from X-ray images of chest is\nparamount for early disease detection. This study develops a deep learning\nframework using a Pix2pix Generative Adversarial Network (GAN) to segment\npulmonary abnormalities from CXR images. This framework's image preprocessing\nand augmentation techniques were properly incorporated with a U-Net-inspired\ngenerator-discriminator architecture. Initially, it loaded the CXR images and\nmanual masks from the Montgomery and Shenzhen datasets, after which\npreprocessing and resizing were performed. A U-Net generator is applied to the\nprocessed CXR images that yield segmented masks; then, a Discriminator Network\ndifferentiates between the generated and real masks. Montgomery dataset served\nas the model's training set in the study, and the Shenzhen dataset was used to\ntest its robustness, which was used here for the first time. An adversarial\nloss and an L1 distance were used to optimize the model in training. All\nmetrics, which assess precision, recall, F1 score, and Dice coefficient, prove\nthe effectiveness of this framework in pulmonary abnormality segmentation. It,\ntherefore, sets the basis for future studies to be performed shortly using\ndiverse datasets that could further confirm its clinical applicability in\nmedical imaging.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "6 pages, 12 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.10826v1",
    "published_date": "2024-12-14 13:12:09 UTC",
    "updated_date": "2024-12-14 13:12:09 UTC"
  },
  {
    "arxiv_id": "2412.10817v2",
    "title": "Enhance Vision-Language Alignment with Noise",
    "authors": [
      "Sida Huang",
      "Hongyuan Zhang",
      "Xuelong Li"
    ],
    "abstract": "With the advancement of pre-trained vision-language (VL) models, enhancing\nthe alignment between visual and linguistic modalities in downstream tasks has\nemerged as a critical challenge. Different from existing fine-tuning methods\nthat add extra modules to these two modalities, we investigate whether the\nfrozen model can be fine-tuned by customized noise. Our approach is motivated\nby the scientific study of beneficial noise, namely Positive-incentive Noise\n(Pi-noise or $\\pi$-noise) , which quantitatively analyzes the impact of noise.\nIt therefore implies a new scheme to learn beneficial noise distribution that\ncan be employed to fine-tune VL models. Focusing on few-shot classification\ntasks based on CLIP, we reformulate the inference process of CLIP and apply\nvariational inference, demonstrating how to generate $\\pi$-noise towards visual\nand linguistic modalities. Then, we propose Positive-incentive Noise Injector\n(PiNI), which can fine-tune CLIP via injecting noise into both visual and text\nencoders. Since the proposed method can learn the distribution of beneficial\nnoise, we can obtain more diverse embeddings of vision and language to better\nalign these two modalities for specific downstream tasks within limited\ncomputational resources. We evaluate different noise incorporation approaches\nand network architectures of PiNI. The evaluation across 11 datasets\ndemonstrates its effectiveness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10817v2",
    "published_date": "2024-12-14 12:58:15 UTC",
    "updated_date": "2024-12-17 02:35:10 UTC"
  },
  {
    "arxiv_id": "2412.10804v1",
    "title": "Medical Manifestation-Aware De-Identification",
    "authors": [
      "Yuan Tian",
      "Shuo Wang",
      "Guangtao Zhai"
    ],
    "abstract": "Face de-identification (DeID) has been widely studied for common scenes, but\nremains under-researched for medical scenes, mostly due to the lack of\nlarge-scale patient face datasets. In this paper, we release MeMa, consisting\nof over 40,000 photo-realistic patient faces. MeMa is re-generated from massive\nreal patient photos. By carefully modulating the generation and data-filtering\nprocedures, MeMa avoids breaching real patient privacy, while ensuring rich and\nplausible medical manifestations. We recruit expert clinicians to annotate MeMa\nwith both coarse- and fine-grained labels, building the first medical-scene\nDeID benchmark. Additionally, we propose a baseline approach for this new\nmedical-aware DeID task, by integrating data-driven medical semantic priors\ninto the DeID procedure. Despite its conciseness and simplicity, our approach\nsubstantially outperforms previous ones. Dataset is available at\nhttps://github.com/tianyuan168326/MeMa-Pytorch.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10804v1",
    "published_date": "2024-12-14 12:09:41 UTC",
    "updated_date": "2024-12-14 12:09:41 UTC"
  },
  {
    "arxiv_id": "2412.12196v1",
    "title": "TrendSim: Simulating Trending Topics in Social Media Under Poisoning Attacks with LLM-based Multi-agent System",
    "authors": [
      "Zeyu Zhang",
      "Jianxun Lian",
      "Chen Ma",
      "Yaning Qu",
      "Ye Luo",
      "Lei Wang",
      "Rui Li",
      "Xu Chen",
      "Yankai Lin",
      "Le Wu",
      "Xing Xie",
      "Ji-Rong Wen"
    ],
    "abstract": "Trending topics have become a significant part of modern social media,\nattracting users to participate in discussions of breaking events. However,\nthey also bring in a new channel for poisoning attacks, resulting in negative\nimpacts on society. Therefore, it is urgent to study this critical problem and\ndevelop effective strategies for defense. In this paper, we propose TrendSim,\nan LLM-based multi-agent system to simulate trending topics in social media\nunder poisoning attacks. Specifically, we create a simulation environment for\ntrending topics that incorporates a time-aware interaction mechanism,\ncentralized message dissemination, and an interactive system. Moreover, we\ndevelop LLM-based human-like agents to simulate users in social media, and\npropose prototype-based attackers to replicate poisoning attacks. Besides, we\nevaluate TrendSim from multiple aspects to validate its effectiveness. Based on\nTrendSim, we conduct simulation experiments to study four critical problems\nabout poisoning attacks on trending topics for social benefit.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "19 pages, 9 tables, 8 figure",
    "pdf_url": "http://arxiv.org/pdf/2412.12196v1",
    "published_date": "2024-12-14 12:04:49 UTC",
    "updated_date": "2024-12-14 12:04:49 UTC"
  },
  {
    "arxiv_id": "2412.10798v2",
    "title": "AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games",
    "authors": [
      "Kefan Su",
      "Yusen Huo",
      "Zhilin Zhang",
      "Shuai Dou",
      "Chuan Yu",
      "Jian Xu",
      "Zongqing Lu",
      "Bo Zheng"
    ],
    "abstract": "Decision-making in large-scale games is an essential research area in\nartificial intelligence (AI) with significant real-world impact. However, the\nlimited access to realistic large-scale game environments has hindered research\nprogress in this area. In this paper, we present AuctionNet, a benchmark for\nbid decision-making in large-scale ad auctions derived from a real-world online\nadvertising platform. AuctionNet is composed of three parts: an ad auction\nenvironment, a pre-generated dataset based on the environment, and performance\nevaluations of several baseline bid decision-making algorithms. More\nspecifically, the environment effectively replicates the integrity and\ncomplexity of real-world ad auctions through the interaction of several\nmodules: the ad opportunity generation module employs deep generative networks\nto bridge the gap between simulated and real-world data while mitigating the\nrisk of sensitive data exposure; the bidding module implements diverse\nauto-bidding agents trained with different decision-making algorithms; and the\nauction module is anchored in the classic Generalized Second Price (GSP)\nauction but also allows for customization of auction mechanisms as needed. To\nfacilitate research and provide insights into the environment, we have also\npre-generated a substantial dataset based on the environment. The dataset\ncontains 10 million ad opportunities, 48 diverse auto-bidding agents, and over\n500 million auction records. Performance evaluations of baseline algorithms\nsuch as linear programming, reinforcement learning, and generative models for\nbid decision-making are also presented as a part of AuctionNet. We believe that\nAuctionNet is applicable not only to research on bid decision-making in ad\nauctions but also to the general area of decision-making in large-scale games.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10798v2",
    "published_date": "2024-12-14 11:31:21 UTC",
    "updated_date": "2024-12-28 08:30:51 UTC"
  },
  {
    "arxiv_id": "2412.10786v1",
    "title": "Optimizing Few-Step Sampler for Diffusion Probabilistic Model",
    "authors": [
      "Jen-Yuan Huang"
    ],
    "abstract": "Diffusion Probabilistic Models (DPMs) have demonstrated exceptional\ncapability of generating high-quality and diverse images, but their practical\napplication is hindered by the intensive computational cost during inference.\nThe DPM generation process requires solving a Probability-Flow Ordinary\nDifferential Equation (PF-ODE), which involves discretizing the integration\ndomain into intervals for numerical approximation. This corresponds to the\nsampling schedule of a diffusion ODE solver, and we notice the solution from a\nfirst-order solver can be expressed as a convex combination of model outputs at\nall scheduled time-steps. We derive an upper bound for the discretization error\nof the sampling schedule, which can be efficiently optimized with Monte-Carlo\nestimation. Building on these theoretical results, we purpose a two-phase\nalternating optimization algorithm. In Phase-1, the sampling schedule is\noptimized for the pre-trained DPM; in Phase-2, the DPM further tuned on the\nselected time-steps. Experiments on a pre-trained DPM for ImageNet64 dataset\ndemonstrate the purposed method consistently improves the baseline across\nvarious number of sampling steps.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10786v1",
    "published_date": "2024-12-14 10:47:52 UTC",
    "updated_date": "2024-12-14 10:47:52 UTC"
  },
  {
    "arxiv_id": "2412.10782v2",
    "title": "ANaGRAM: A Natural Gradient Relative to Adapted Model for efficient PINNs learning",
    "authors": [
      "Nilo Schwencke",
      "Cyril Furtlehner"
    ],
    "abstract": "In the recent years, Physics Informed Neural Networks (PINNs) have received\nstrong interest as a method to solve PDE driven systems, in particular for data\nassimilation purpose. This method is still in its infancy, with many\nshortcomings and failures that remain not properly understood. In this paper we\npropose a natural gradient approach to PINNs which contributes to speed-up and\nimprove the accuracy of the training. Based on an in depth analysis of the\ndifferential geometric structures of the problem, we come up with two distinct\ncontributions: (i) a new natural gradient algorithm that scales as $\\min(P^2S,\nS^2P)$, where $P$ is the number of parameters, and $S$ the batch size; (ii) a\nmathematically principled reformulation of the PINNs problem that allows the\nextension of natural gradient to it, with proved connections to Green's\nfunction theory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA",
      "math.OC",
      "I.2.8"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted in ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10782v2",
    "published_date": "2024-12-14 10:38:09 UTC",
    "updated_date": "2025-03-19 00:47:29 UTC"
  },
  {
    "arxiv_id": "2412.10778v2",
    "title": "Sample-efficient Unsupervised Policy Cloning from Ensemble Self-supervised Labeled Videos",
    "authors": [
      "Xin Liu",
      "Yaran Chen",
      "Haoran Li"
    ],
    "abstract": "Current advanced policy learning methodologies have demonstrated the ability\nto develop expert-level strategies when provided enough information. However,\ntheir requirements, including task-specific rewards, action-labeled expert\ntrajectories, and huge environmental interactions, can be expensive or even\nunavailable in many scenarios. In contrast, humans can efficiently acquire\nskills within a few trials and errors by imitating easily accessible internet\nvideos, in the absence of any other supervision. In this paper, we try to let\nmachines replicate this efficient watching-and-learning process through\nUnsupervised Policy from Ensemble Self-supervised labeled Videos (UPESV), a\nnovel framework to efficiently learn policies from action-free videos without\nrewards and any other expert supervision. UPESV trains a video labeling model\nto infer the expert actions in expert videos through several organically\ncombined self-supervised tasks. Each task performs its duties, and they\ntogether enable the model to make full use of both action-free videos and\nreward-free interactions for robust dynamics understanding and advanced action\nprediction. Simultaneously, UPESV clones a policy from the labeled expert\nvideos, in turn collecting environmental interactions for self-supervised\ntasks. After a sample-efficient, unsupervised, and iterative training process,\nUPESV obtains an advanced policy based on a robust video labeling model.\nExtensive experiments in sixteen challenging procedurally generated\nenvironments demonstrate that the proposed UPESV achieves state-of-the-art\ninteraction-limited policy learning performance (outperforming five current\nadvanced baselines on 12/16 tasks) without exposure to any other supervision\nexcept for videos.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICRA 2025, 8 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.10778v2",
    "published_date": "2024-12-14 10:12:22 UTC",
    "updated_date": "2025-04-08 08:54:33 UTC"
  },
  {
    "arxiv_id": "2412.10776v1",
    "title": "Boosting ViT-based MRI Reconstruction from the Perspectives of Frequency Modulation, Spatial Purification, and Scale Diversification",
    "authors": [
      "Yucong Meng",
      "Zhiwei Yang",
      "Yonghong Shi",
      "Zhijian Song"
    ],
    "abstract": "The accelerated MRI reconstruction process presents a challenging ill-posed\ninverse problem due to the extensive under-sampling in k-space. Recently,\nVision Transformers (ViTs) have become the mainstream for this task,\ndemonstrating substantial performance improvements. However, there are still\nthree significant issues remain unaddressed: (1) ViTs struggle to capture\nhigh-frequency components of images, limiting their ability to detect local\ntextures and edge information, thereby impeding MRI restoration; (2) Previous\nmethods calculate multi-head self-attention (MSA) among both related and\nunrelated tokens in content, introducing noise and significantly increasing\ncomputational burden; (3) The naive feed-forward network in ViTs cannot model\nthe multi-scale information that is important for image restoration. In this\npaper, we propose FPS-Former, a powerful ViT-based framework, to address these\nissues from the perspectives of frequency modulation, spatial purification, and\nscale diversification. Specifically, for issue (1), we introduce a frequency\nmodulation attention module to enhance the self-attention map by adaptively\nre-calibrating the frequency information in a Laplacian pyramid. For issue (2),\nwe customize a spatial purification attention module to capture interactions\namong closely related tokens, thereby reducing redundant or irrelevant feature\nrepresentations. For issue (3), we propose an efficient feed-forward network\nbased on a hybrid-scale fusion strategy. Comprehensive experiments conducted on\nthree public datasets show that our FPS-Former outperforms state-of-the-art\nmethods while requiring lower computational costs.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10776v1",
    "published_date": "2024-12-14 10:03:08 UTC",
    "updated_date": "2024-12-14 10:03:08 UTC"
  },
  {
    "arxiv_id": "2412.10761v1",
    "title": "Rebalanced Vision-Language Retrieval Considering Structure-Aware Distillation",
    "authors": [
      "Yang Yang",
      "Wenjuan Xi",
      "Luping Zhou",
      "Jinhui Tang"
    ],
    "abstract": "Vision-language retrieval aims to search for similar instances in one\nmodality based on queries from another modality. The primary objective is to\nlearn cross-modal matching representations in a latent common space. Actually,\nthe assumption underlying cross-modal matching is modal balance, where each\nmodality contains sufficient information to represent the others. However,\nnoise interference and modality insufficiency often lead to modal imbalance,\nmaking it a common phenomenon in practice. The impact of imbalance on retrieval\nperformance remains an open question. In this paper, we first demonstrate that\nultimate cross-modal matching is generally sub-optimal for cross-modal\nretrieval when imbalanced modalities exist. The structure of instances in the\ncommon space is inherently influenced when facing imbalanced modalities, posing\na challenge to cross-modal similarity measurement. To address this issue, we\nemphasize the importance of meaningful structure-preserved matching.\nAccordingly, we propose a simple yet effective method to rebalance cross-modal\nmatching by learning structure-preserved matching representations.\nSpecifically, we design a novel multi-granularity cross-modal matching that\nincorporates structure-aware distillation alongside the cross-modal matching\nloss. While the cross-modal matching loss constraints instance-level matching,\nthe structure-aware distillation further regularizes the geometric consistency\nbetween learned matching representations and intra-modal representations\nthrough the developed relational matching. Extensive experiments on different\ndatasets affirm the superior cross-modal retrieval performance of our approach,\nsimultaneously enhancing single-modal retrieval capabilities compared to the\nbaseline models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10761v1",
    "published_date": "2024-12-14 09:10:36 UTC",
    "updated_date": "2024-12-14 09:10:36 UTC"
  },
  {
    "arxiv_id": "2412.10726v1",
    "title": "NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries",
    "authors": [
      "Tao Wu",
      "Chuhao Zhou",
      "Yen Heng Wong",
      "Lin Gu",
      "Jianfei Yang"
    ],
    "abstract": "The rapid advancement of Vision-Language Models (VLMs) has significantly\nadvanced the development of Embodied Question Answering (EQA), enhancing\nagents' abilities in language understanding and reasoning within complex and\nrealistic scenarios. However, EQA in real-world scenarios remains challenging,\nas human-posed questions often contain noise that can interfere with an agent's\nexploration and response, bringing challenges especially for language beginners\nand non-expert users. To address this, we introduce a NoisyEQA benchmark\ndesigned to evaluate an agent's ability to recognize and correct noisy\nquestions. This benchmark introduces four common types of noise found in\nreal-world applications: Latent Hallucination Noise, Memory Noise, Perception\nNoise, and Semantic Noise generated through an automated dataset creation\nframework. Additionally, we also propose a 'Self-Correction' prompting\nmechanism and a new evaluation metric to enhance and measure both noise\ndetection capability and answer quality. Our comprehensive evaluation reveals\nthat current EQA agents often struggle to detect noise in questions, leading to\nresponses that frequently contain erroneous information. Through our\nSelf-Correct Prompting mechanism, we can effectively improve the accuracy of\nagent answers.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10726v1",
    "published_date": "2024-12-14 07:52:24 UTC",
    "updated_date": "2024-12-14 07:52:24 UTC"
  },
  {
    "arxiv_id": "2502.15684v1",
    "title": "An Agent Framework for Real-Time Financial Information Searching with Large Language Models",
    "authors": [
      "Jinzheng Li",
      "Jingshu Zhang",
      "Hongguang Li",
      "Yiqing Shen"
    ],
    "abstract": "Financial decision-making requires processing vast amounts of real-time\ninformation while understanding their complex temporal relationships. While\ntraditional search engines excel at providing real-time information access,\nthey often struggle to comprehend sophisticated user intentions and contextual\nnuances. Conversely, Large Language Models (LLMs) demonstrate reasoning and\ninteraction capabilities but may generate unreliable outputs without access to\ncurrent data. While recent attempts have been made to combine LLMs with search\ncapabilities, they suffer from (1) restricted access to specialized financial\ndata, (2) static query structures that cannot adapt to dynamic market\nconditions, and (3) insufficient temporal awareness in result generation. To\naddress these challenges, we present FinSearch, a novel agent-based search\nframework specifically designed for financial applications that interface with\ndiverse financial data sources including market, stock, and news data.\nInnovatively, FinSearch comprises four components: (1) an LLM-based multi-step\nsearch pre-planner that decomposes user queries into structured sub-queries\nmapped to specific data sources through a graph representation; (2) a search\nexecutor with an LLM-based adaptive query rewriter that executes the searching\nof each sub-query while dynamically refining the sub-queries in its subsequent\nnode based on intermediate search results; (3) a temporal weighting mechanism\nthat prioritizes information relevance based on the deduced time context from\nthe user's query; (4) an LLM-based response generator that synthesizes results\ninto coherent, contextually appropriate outputs. To evaluate FinSearch, we\nconstruct FinSearchBench-24, a benchmark of 1,500 four-choice questions across\nthe stock market, rate changes, monetary policy, and industry developments\nspanning from June to October 2024.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15684v1",
    "published_date": "2024-12-14 07:26:39 UTC",
    "updated_date": "2024-12-14 07:26:39 UTC"
  },
  {
    "arxiv_id": "2412.10719v1",
    "title": "Just a Few Glances: Open-Set Visual Perception with Image Prompt Paradigm",
    "authors": [
      "Jinrong Zhang",
      "Penghui Wang",
      "Chunxiao Liu",
      "Wei Liu",
      "Dian Jin",
      "Qiong Zhang",
      "Erli Meng",
      "Zhengnan Hu"
    ],
    "abstract": "To break through the limitations of pre-training models on fixed categories,\nOpen-Set Object Detection (OSOD) and Open-Set Segmentation (OSS) have attracted\na surge of interest from researchers. Inspired by large language models,\nmainstream OSOD and OSS methods generally utilize text as a prompt, achieving\nremarkable performance. Following SAM paradigm, some researchers use visual\nprompts, such as points, boxes, and masks that cover detection or segmentation\ntargets. Despite these two prompt paradigms exhibit excellent performance, they\nalso reveal inherent limitations. On the one hand, it is difficult to\naccurately describe characteristics of specialized category using textual\ndescription. On the other hand, existing visual prompt paradigms heavily rely\non multi-round human interaction, which hinders them being applied to fully\nautomated pipeline. To address the above issues, we propose a novel prompt\nparadigm in OSOD and OSS, that is, \\textbf{Image Prompt Paradigm}. This brand\nnew prompt paradigm enables to detect or segment specialized categories without\nmulti-round human intervention. To achieve this goal, the proposed image prompt\nparadigm uses just a few image instances as prompts, and we propose a novel\nframework named \\textbf{MI Grounding} for this new paradigm. In this framework,\nhigh-quality image prompts are automatically encoded, selected and fused,\nachieving the single-stage and non-interactive inference. We conduct extensive\nexperiments on public datasets, showing that MI Grounding achieves competitive\nperformance on OSOD and OSS benchmarks compared to text prompt paradigm methods\nand visual prompt paradigm methods. Moreover, MI Grounding can greatly\noutperform existing method on our constructed specialized ADR50K dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10719v1",
    "published_date": "2024-12-14 07:23:14 UTC",
    "updated_date": "2024-12-14 07:23:14 UTC"
  },
  {
    "arxiv_id": "2412.10717v1",
    "title": "HITgram: A Platform for Experimenting with n-gram Language Models",
    "authors": [
      "Shibaranjani Dasgupta",
      "Chandan Maity",
      "Somdip Mukherjee",
      "Rohan Singh",
      "Diptendu Dutta",
      "Debasish Jana"
    ],
    "abstract": "Large language models (LLMs) are powerful but resource intensive, limiting\naccessibility. HITgram addresses this gap by offering a lightweight platform\nfor n-gram model experimentation, ideal for resource-constrained environments.\nIt supports unigrams to 4-grams and incorporates features like context\nsensitive weighting, Laplace smoothing, and dynamic corpus management to\ne-hance prediction accuracy, even for unseen word sequences. Experiments\ndemonstrate HITgram's efficiency, achieving 50,000 tokens/second and generating\n2-grams from a 320MB corpus in 62 seconds. HITgram scales efficiently,\nconstructing 4-grams from a 1GB file in under 298 seconds on an 8 GB RAM\nsystem. Planned enhancements include multilingual support, advanced smoothing,\nparallel processing, and model saving, further broadening its utility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10717v1",
    "published_date": "2024-12-14 07:20:35 UTC",
    "updated_date": "2024-12-14 07:20:35 UTC"
  },
  {
    "arxiv_id": "2412.10716v1",
    "title": "Control of Overfitting with Physics",
    "authors": [
      "Sergei V. Kozyrev",
      "Ilya A Lopatin",
      "Alexander N Pechen"
    ],
    "abstract": "While there are many works on the applications of machine learning, not so\nmany of them are trying to understand the theoretical justifications to explain\ntheir efficiency. In this work, overfitting control (or generalization\nproperty) in machine learning is explained using analogies from physics and\nbiology. For stochastic gradient Langevin dynamics, we show that the Eyring\nformula of kinetic theory allows to control overfitting in the algorithmic\nstability approach - when wide minima of the risk function with low free energy\ncorrespond to low overfitting. For the generative adversarial network (GAN)\nmodel, we establish an analogy between GAN and the predator-prey model in\nbiology. An application of this analogy allows us to explain the selection of\nwide likelihood maxima and overfitting reduction for GANs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10716v1",
    "published_date": "2024-12-14 07:20:33 UTC",
    "updated_date": "2024-12-14 07:20:33 UTC"
  },
  {
    "arxiv_id": "2412.10713v1",
    "title": "RAT: Adversarial Attacks on Deep Reinforcement Agents for Targeted Behaviors",
    "authors": [
      "Fengshuo Bai",
      "Runze Liu",
      "Yali Du",
      "Ying Wen",
      "Yaodong Yang"
    ],
    "abstract": "Evaluating deep reinforcement learning (DRL) agents against targeted behavior\nattacks is critical for assessing their robustness. These attacks aim to\nmanipulate the victim into specific behaviors that align with the attacker's\nobjectives, often bypassing traditional reward-based defenses. Prior methods\nhave primarily focused on reducing cumulative rewards; however, rewards are\ntypically too generic to capture complex safety requirements effectively. As a\nresult, focusing solely on reward reduction can lead to suboptimal attack\nstrategies, particularly in safety-critical scenarios where more precise\nbehavior manipulation is needed. To address these challenges, we propose RAT, a\nmethod designed for universal, targeted behavior attacks. RAT trains an\nintention policy that is explicitly aligned with human preferences, serving as\na precise behavioral target for the adversary. Concurrently, an adversary\nmanipulates the victim's policy to follow this target behavior. To enhance the\neffectiveness of these attacks, RAT dynamically adjusts the state occupancy\nmeasure within the replay buffer, allowing for more controlled and effective\nbehavior manipulation. Our empirical results on robotic simulation tasks\ndemonstrate that RAT outperforms existing adversarial attack algorithms in\ninducing specific behaviors. Additionally, RAT shows promise in improving agent\nrobustness, leading to more resilient policies. We further validate RAT by\nguiding Decision Transformer agents to adopt behaviors aligned with human\npreferences in various MuJoCo tasks, demonstrating its effectiveness across\ndiverse tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10713v1",
    "published_date": "2024-12-14 06:56:11 UTC",
    "updated_date": "2024-12-14 06:56:11 UTC"
  },
  {
    "arxiv_id": "2412.15246v1",
    "title": "Accelerating Retrieval-Augmented Generation",
    "authors": [
      "Derrick Quinn",
      "Mohammad Nouri",
      "Neel Patel",
      "John Salihu",
      "Alireza Salemi",
      "Sukhan Lee",
      "Hamed Zamani",
      "Mohammad Alian"
    ],
    "abstract": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.AR",
      "cs.DC",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15246v1",
    "published_date": "2024-12-14 06:47:56 UTC",
    "updated_date": "2024-12-14 06:47:56 UTC"
  },
  {
    "arxiv_id": "2412.10705v1",
    "title": "Efficient Adaptation of Multilingual Models for Japanese ASR",
    "authors": [
      "Mark Bajo",
      "Haruka Fukukawa",
      "Ryuji Morita",
      "Yuma Ogasawara"
    ],
    "abstract": "This study explores fine-tuning multilingual ASR (Automatic Speech\nRecognition) models, specifically OpenAI's Whisper-Tiny, to improve performance\nin Japanese. While multilingual models like Whisper offer versatility, they\noften lack precision in specific languages. Conversely, monolingual models like\nReazonSpeech excel in language-specific tasks but are less adaptable. Using\nJapanese-specific datasets and Low-Rank Adaptation (LoRA) along with end-to-end\n(E2E) training, we fine-tuned Whisper-Tiny to bridge this gap. Our results show\nthat fine-tuning reduced Whisper-Tiny's Character Error Rate (CER) from 32.7 to\n20.8 with LoRA and to 14.7 with end-to-end fine-tuning, surpassing\nWhisper-Base's CER of 20.2. However, challenges with domain-specific terms\nremain, highlighting the need for specialized datasets. These findings\ndemonstrate that fine-tuning multilingual models can achieve strong\nlanguage-specific performance while retaining their flexibility. This approach\nprovides a scalable solution for improving ASR in resource-constrained\nenvironments and languages with complex writing systems like Japanese.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10705v1",
    "published_date": "2024-12-14 06:32:16 UTC",
    "updated_date": "2024-12-14 06:32:16 UTC"
  },
  {
    "arxiv_id": "2412.10689v1",
    "title": "Learning to Verify Summary Facts with Fine-Grained LLM Feedback",
    "authors": [
      "Jihwan Oh",
      "Jeonghwan Choi",
      "Nicole Hee-Yeon Kim",
      "Taewon Yun",
      "Hwanjun Song"
    ],
    "abstract": "Training automatic summary fact verifiers often faces the challenge of a lack\nof human-labeled data. In this paper, we explore alternative way of leveraging\nLarge Language Model (LLM) generated feedback to address the inherent\nlimitation of using human-labeled data. We introduce FineSumFact, a large-scale\ndataset containing fine-grained factual feedback on summaries. We employ 10\ndistinct LLMs for diverse summary generation and Llama-3-70B-Instruct for\nfeedback. We utilize this dataset to fine-tune the lightweight open-source\nmodel Llama-3-8B-Instruct, optimizing resource efficiency while maintaining\nhigh performance. Our experimental results reveal that the model trained on\nextensive LLM-generated datasets surpasses that trained on smaller\nhuman-annotated datasets when evaluated using human-generated test sets.\nFine-tuning fact verification models with LLM feedback can be more effective\nand cost-efficient than using human feedback. The dataset is available at\nhttps://github.com/DISL-Lab/FineSumFact.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10689v1",
    "published_date": "2024-12-14 05:28:44 UTC",
    "updated_date": "2024-12-14 05:28:44 UTC"
  },
  {
    "arxiv_id": "2412.10675v1",
    "title": "Chasing Progress, Not Perfection: Revisiting Strategies for End-to-End LLM Plan Generation",
    "authors": [
      "Sukai Huang",
      "Trevor Cohn",
      "Nir Lipovetzky"
    ],
    "abstract": "The capability of Large Language Models (LLMs) to plan remains a topic of\ndebate. Some critics argue that strategies to boost LLMs' reasoning skills are\nineffective in planning tasks, while others report strong outcomes merely from\ntraining models on a planning corpus. This study reassesses recent strategies\nby developing an end-to-end LLM planner and employing diverse metrics for a\nthorough evaluation. We find that merely fine-tuning LLMs on a corpus of\nplanning instances does not lead to robust planning skills, as indicated by\npoor performance on out-of-distribution test sets. At the same time, we find\nthat various strategies, including Chain-of-Thought, do enhance the probability\nof a plan being executable. This indicates progress towards better plan\nquality, despite not directly enhancing the final validity rate. Among the\nstrategies we evaluated, reinforcement learning with our novel `Longest\nContiguous Common Subsequence' reward emerged as the most effective,\ncontributing to both plan validity and executability. Overall, our research\naddresses key misconceptions in the LLM-planning literature; we validate\nincremental progress in plan executability, although plan validity remains a\nchallenge. Hence, future strategies should focus on both these aspects, drawing\ninsights from our findings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages main body, 10 pages appendix, accepted by Workshop on\n  Planning in the Era of LLMs (LM4Plan @ AAAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.10675v1",
    "published_date": "2024-12-14 04:23:14 UTC",
    "updated_date": "2024-12-14 04:23:14 UTC"
  },
  {
    "arxiv_id": "2412.10674v3",
    "title": "USM: Unbiased Survey Modeling for Limiting Negative User Experiences in Recommendation Systems",
    "authors": [
      "Chenghui Yu",
      "Peiyi Li",
      "Haoze Wu",
      "Yiri Wen",
      "Bingfeng Deng",
      "Hongyu Xiong"
    ],
    "abstract": "Reducing negative user experiences is essential for the success of\nrecommendation platforms. Exposing users to inappropriate content could not\nonly adversely affect users' psychological well-beings, but also potentially\ndrive users away from the platform, sabotaging the platform's long-term\nsuccess. However, recommendation algorithms tend to weigh more heavily on\npositive feedback signals due to the scarcity of negative ones, which may\nresult in the neglect of valuable negative user feedback. In this paper, we\npropose an approach aimed at limiting negative user experiences. Our method\nprimarily relies on distributing in-feed surveys to the users, modeling the\nusers' feedback collected from the survey, and integrating the model\npredictions into the recommendation system. We further enhance the baseline\nsurvey model by integrating the Learning Hidden Unit Contributions module and\nthe Squeeze-and-Excitation module. In addition, we strive to resolve the\nproblem of response Bias by applying a survey-submit model; The A/B testing\nresults indicate a reduction in survey sexual rate and survey inappropriate\nrate, ranging from -1.44\\% to -3.9\\%. Additionally, we compared our methods\nagainst an online baseline that does not incorporate our approach. The results\nindicate that our approach significantly reduces the report rate and dislike\nrate by 1\\% to 2.27\\% compared to the baseline, confirming the effectiveness of\nour methods in enhancing user experience. After we launched the survey model\nbased our approach on our platform, the model is able to bring reductions of\n1.75\\%, 2.57\\%, 2.06\\% on reports, dislikes, survey inappropriate rate,\nrespectively.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10674v3",
    "published_date": "2024-12-14 04:22:09 UTC",
    "updated_date": "2025-02-15 08:31:04 UTC"
  },
  {
    "arxiv_id": "2412.10673v1",
    "title": "Proposing and solving olympiad geometry with guided tree search",
    "authors": [
      "Chi Zhang",
      "Jiajun Song",
      "Siyu Li",
      "Yitao Liang",
      "Yuxi Ma",
      "Wei Wang",
      "Yixin Zhu",
      "Song-Chun Zhu"
    ],
    "abstract": "Mathematics olympiads are prestigious competitions, with problem proposing\nand solving highly honored. Building artificial intelligence that proposes and\nsolves olympiads presents an unresolved challenge in automated theorem\ndiscovery and proving, especially in geometry for its combination of numerical\nand spatial elements. We introduce TongGeometry, a Euclidean geometry system\nsupporting tree-search-based guided problem proposing and solving. The\nefficient geometry system establishes the most extensive repository of geometry\ntheorems to date: within the same computational budget as the existing\nstate-of-the-art, TongGeometry discovers 6.7 billion geometry theorems\nrequiring auxiliary constructions, including 4.1 billion exhibiting geometric\nsymmetry. Among them, 10 theorems were proposed to regional mathematical\nolympiads with 3 of TongGeometry's proposals selected in real competitions,\nearning spots in a national team qualifying exam or a top civil olympiad in\nChina and the US. Guided by fine-tuned large language models, TongGeometry\nsolved all International Mathematical Olympiad geometry in IMO-AG-30,\noutperforming gold medalists for the first time. It also surpasses the existing\nstate-of-the-art across a broader spectrum of olympiad-level problems. The full\ncapabilities of the system can be utilized on a consumer-grade machine, making\nthe model more accessible and fostering widespread democratization of its use.\nBy analogy, unlike existing systems that merely solve problems like students,\nTongGeometry acts like a geometry coach, discovering, presenting, and proving\ntheorems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10673v1",
    "published_date": "2024-12-14 04:20:47 UTC",
    "updated_date": "2024-12-14 04:20:47 UTC"
  },
  {
    "arxiv_id": "2412.10658v3",
    "title": "Combining Priors with Experience: Confidence Calibration Based on Binomial Process Modeling",
    "authors": [
      "Jinzong Dong",
      "Zhaohui Jiang",
      "Dong Pan",
      "Haoyang Yu"
    ],
    "abstract": "Confidence calibration of classification models is a technique to estimate\nthe true posterior probability of the predicted class, which is critical for\nensuring reliable decision-making in practical applications. Existing\nconfidence calibration methods mostly use statistical techniques to estimate\nthe calibration curve from data or fit a user-defined calibration function, but\noften overlook fully mining and utilizing the prior distribution behind the\ncalibration curve. However, a well-informed prior distribution can provide\nvaluable insights beyond the empirical data under the limited data or\nlow-density regions of confidence scores. To fill this gap, this paper proposes\na new method that integrates the prior distribution behind the calibration\ncurve with empirical data to estimate a continuous calibration curve, which is\nrealized by modeling the sampling process of calibration data as a binomial\nprocess and maximizing the likelihood function of the binomial process. We\nprove that the calibration curve estimating method is Lipschitz continuous with\nrespect to data distribution and requires a sample size of $3/B$ of that\nrequired for histogram binning, where $B$ represents the number of bins. Also,\na new calibration metric ($TCE_{bpm}$), which leverages the estimated\ncalibration curve to estimate the true calibration error (TCE), is designed.\n$TCE_{bpm}$ is proven to be a consistent calibration measure. Furthermore,\nrealistic calibration datasets can be generated by the binomial process\nmodeling from a preset true calibration curve and confidence score\ndistribution, which can serve as a benchmark to measure and compare the\ndiscrepancy between existing calibration metrics and the true calibration\nerror. The effectiveness of our calibration method and metric are verified in\nreal-world and simulated data.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ME",
    "comment": "Accepted by AAAI-25",
    "pdf_url": "http://arxiv.org/pdf/2412.10658v3",
    "published_date": "2024-12-14 03:04:05 UTC",
    "updated_date": "2025-02-18 12:23:13 UTC"
  },
  {
    "arxiv_id": "2501.00013v1",
    "title": "Relation-Aware Equivariant Graph Networks for Epitope-Unknown Antibody Design and Specificity Optimization",
    "authors": [
      "Lirong Wu",
      "Haitao Lin",
      "Yufei Huang",
      "Zhangyang Gao",
      "Cheng Tan",
      "Yunfan Liu",
      "Tailin Wu",
      "Stan Z. Li"
    ],
    "abstract": "Antibodies are Y-shaped proteins that protect the host by binding to specific\nantigens, and their binding is mainly determined by the Complementary\nDetermining Regions (CDRs) in the antibody. Despite the great progress made in\nCDR design, existing computational methods still encounter several challenges:\n1) poor capability of modeling complex CDRs with long sequences due to\ninsufficient contextual information; 2) conditioned on pre-given antigenic\nepitopes and their static interaction with the target antibody; 3) neglect of\nspecificity during antibody optimization leads to non-specific antibodies. In\nthis paper, we take into account a variety of node features, edge features, and\nedge relations to include more contextual and geometric information. We propose\na novel Relation-Aware Antibody Design (RAAD) framework, which dynamically\nmodels antigen-antibody interactions for co-designing the sequences and\nstructures of antigen-specific CDRs. Furthermore, we propose a new evaluation\nmetric to better measure antibody specificity and develop a contrasting\nspecificity-enhancing constraint to optimize the specificity of antibodies.\nExtensive experiments have demonstrated the superior capability of RAAD in\nterms of antibody modeling, generation, and optimization across different CDR\ntypes, sequence lengths, pre-training strategies, and input contexts.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00013v1",
    "published_date": "2024-12-14 03:00:44 UTC",
    "updated_date": "2024-12-14 03:00:44 UTC"
  },
  {
    "arxiv_id": "2412.10651v1",
    "title": "LAN: Learning to Adapt Noise for Image Denoising",
    "authors": [
      "Changjin Kim",
      "Tae Hyun Kim",
      "Sungyong Baik"
    ],
    "abstract": "Removing noise from images, a.k.a image denoising, can be a very challenging\ntask since the type and amount of noise can greatly vary for each image due to\nmany factors including a camera model and capturing environments. While there\nhave been striking improvements in image denoising with the emergence of\nadvanced deep learning architectures and real-world datasets, recent denoising\nnetworks struggle to maintain performance on images with noise that has not\nbeen seen during training. One typical approach to address the challenge would\nbe to adapt a denoising network to new noise distribution. Instead, in this\nwork, we shift our focus to adapting the input noise itself, rather than\nadapting a network. Thus, we keep a pretrained network frozen, and adapt an\ninput noise to capture the fine-grained deviations. As such, we propose a new\ndenoising algorithm, dubbed Learning-to-Adapt-Noise (LAN), where a learnable\nnoise offset is directly added to a given noisy image to bring a given input\nnoise closer towards the noise distribution a denoising network is trained to\nhandle. Consequently, the proposed framework exhibits performance improvement\non images with unseen noise, displaying the potential of the proposed research\ndirection. The code is available at https://github.com/chjinny/LAN",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR2024",
    "pdf_url": "http://arxiv.org/pdf/2412.10651v1",
    "published_date": "2024-12-14 02:46:25 UTC",
    "updated_date": "2024-12-14 02:46:25 UTC"
  },
  {
    "arxiv_id": "2412.10649v1",
    "title": "Hidden Echoes Survive Training in Audio To Audio Generative Instrument Models",
    "authors": [
      "Christopher J. Tralie",
      "Matt Amery",
      "Benjamin Douglas",
      "Ian Utz"
    ],
    "abstract": "As generative techniques pervade the audio domain, there has been increasing\ninterest in tracing back through these complicated models to understand how\nthey draw on their training data to synthesize new examples, both to ensure\nthat they use properly licensed data and also to elucidate their black box\nbehavior. In this paper, we show that if imperceptible echoes are hidden in the\ntraining data, a wide variety of audio to audio architectures (differentiable\ndigital signal processing (DDSP), Realtime Audio Variational autoEncoder\n(RAVE), and ``Dance Diffusion'') will reproduce these echoes in their outputs.\nHiding a single echo is particularly robust across all architectures, but we\nalso show promising results hiding longer time spread echo patterns for an\nincreased information capacity. We conclude by showing that echoes make their\nway into fine tuned models, that they survive mixing/demixing, and that they\nsurvive pitch shift augmentation during training. Hence, this simple, classical\nidea in watermarking shows significant promise for tagging generative audio\nmodels.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS",
      "I.2; I.5.4; J.5"
    ],
    "primary_category": "cs.SD",
    "comment": "8 pages, 11 Figures, Proceedings of 2025 AAAI Workshop on AI for\n  Music",
    "pdf_url": "http://arxiv.org/pdf/2412.10649v1",
    "published_date": "2024-12-14 02:36:45 UTC",
    "updated_date": "2024-12-14 02:36:45 UTC"
  },
  {
    "arxiv_id": "2412.10644v1",
    "title": "Model-driven deep neural network for enhanced direction finding with commodity 5G gNodeB",
    "authors": [
      "Shengheng Liu",
      "Zihuan Mao",
      "Xingkang Li",
      "Mengguan Pan",
      "Peng Liu",
      "Yongming Huang",
      "Xiaohu You"
    ],
    "abstract": "Pervasive and high-accuracy positioning has become increasingly important as\na fundamental enabler for intelligent connected devices in mobile networks.\nNevertheless, current wireless networks heavily rely on pure model-driven\ntechniques to achieve positioning functionality, often succumbing to\nperformance deterioration due to hardware impairments in practical scenarios.\nHere we reformulate the direction finding or angle-of-arrival (AoA) estimation\nproblem as an image recovery task of the spatial spectrum and propose a new\nmodel-driven deep neural network (MoD-DNN) framework. The proposed MoD-DNN\nscheme comprises three modules: a multi-task autoencoder-based beamformer, a\ncoarray spectrum generation module, and a model-driven deep learning-based\nspatial spectrum reconstruction module. Our technique enables automatic\ncalibration of angular-dependent phase error thereby enhancing the resilience\nof direction-finding precision against realistic system non-idealities. We\nvalidate the proposed scheme both using numerical simulations and field tests.\nThe results show that the proposed MoD-DNN framework enables effective spectrum\ncalibration and accurate AoA estimation. To the best of our knowledge, this\nstudy marks the first successful demonstration of hybrid data-and-model-driven\ndirection finding utilizing readily available commodity 5G gNodeB.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "To appear in ACM TOSN. A preliminary version of this article was\n  presented at the AAAI'2024 Main Technical Track",
    "pdf_url": "http://arxiv.org/pdf/2412.10644v1",
    "published_date": "2024-12-14 02:09:36 UTC",
    "updated_date": "2024-12-14 02:09:36 UTC"
  },
  {
    "arxiv_id": "2412.10629v1",
    "title": "Rapid Reconstruction of Extremely Accelerated Liver 4D MRI via Chained Iterative Refinement",
    "authors": [
      "Di Xu",
      "Xin Miao",
      "Hengjie Liu",
      "Jessica E. Scholey",
      "Wensha Yang",
      "Mary Feng",
      "Michael Ohliger",
      "Hui Lin",
      "Yi Lao",
      "Yang Yang",
      "Ke Sheng"
    ],
    "abstract": "Abstract Purpose: High-quality 4D MRI requires an impractically long scanning\ntime for dense k-space signal acquisition covering all respiratory phases.\nAccelerated sparse sampling followed by reconstruction enhancement is desired\nbut often results in degraded image quality and long reconstruction time. We\nhereby propose the chained iterative reconstruction network (CIRNet) for\nefficient sparse-sampling reconstruction while maintaining clinically\ndeployable quality. Methods: CIRNet adopts the denoising diffusion\nprobabilistic framework to condition the image reconstruction through a\nstochastic iterative denoising process. During training, a forward Markovian\ndiffusion process is designed to gradually add Gaussian noise to the densely\nsampled ground truth (GT), while CIRNet is optimized to iteratively reverse the\nMarkovian process from the forward outputs. At the inference stage, CIRNet\nperforms the reverse process solely to recover signals from noise, conditioned\nupon the undersampled input. CIRNet processed the 4D data (3D+t) as temporal\nslices (2D+t). The proposed framework is evaluated on a data cohort consisting\nof 48 patients (12332 temporal slices) who underwent free-breathing liver 4D\nMRI. 3-, 6-, 10-, 20- and 30-times acceleration were examined with a\nretrospective random undersampling scheme. Compressed sensing (CS)\nreconstruction with a spatiotemporal constraint and a recently proposed deep\nnetwork, Re-Con-GAN, are selected as baselines. Results: CIRNet consistently\nachieved superior performance compared to CS and Re-Con-GAN. The inference time\nof CIRNet, CS, and Re-Con-GAN are 11s, 120s, and 0.15s. Conclusion: A novel\nframework, CIRNet, is presented. CIRNet maintains useable image quality for\nacceleration up to 30 times, significantly reducing the burden of 4DMRI.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10629v1",
    "published_date": "2024-12-14 00:43:11 UTC",
    "updated_date": "2024-12-14 00:43:11 UTC"
  },
  {
    "arxiv_id": "2412.10622v3",
    "title": "A recent evaluation on the performance of LLMs on radiation oncology physics using questions of randomly shuffled options",
    "authors": [
      "Peilong Wang",
      "Jason Holmes",
      "Zhengliang Liu",
      "Dequan Chen",
      "Tianming Liu",
      "Jiajian Shen",
      "Wei Liu"
    ],
    "abstract": "Purpose: We present an updated study evaluating the performance of large\nlanguage models (LLMs) in answering radiation oncology physics questions,\nfocusing on the recently released models.\n  Methods: A set of 100 multiple-choice radiation oncology physics questions,\npreviously created by a well-experienced physicist, was used for this study.\nThe answer options of the questions were randomly shuffled to create \"new\" exam\nsets. Five LLMs -- OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,\nand Claude 3.5 Sonnet -- with the versions released before September 30, 2024,\nwere queried using these new exam sets. To evaluate their deductive reasoning\nability, the correct answer options in the questions were replaced with \"None\nof the above.\" Then, the explain-first and step-by-step instruction prompts\nwere used to test if this strategy improved their reasoning ability. The\nperformance of the LLMs was compared with the answers from medical physicists.\n  Results: All models demonstrated expert-level performance on these questions,\nwith o1-preview even surpassing medical physicists with a majority vote. When\nreplacing the correct answer options with 'None of the above', all models\nexhibited a considerable decline in performance, suggesting room for\nimprovement. The explain-first and step-by-step instruction prompts helped\nenhance the reasoning ability of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and\nClaude 3.5 Sonnet models.\n  Conclusion: These recently released LLMs demonstrated expert-level\nperformance in answering radiation oncology physics questions, exhibiting great\npotential to assist in radiation oncology physics education and training.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10622v3",
    "published_date": "2024-12-14 00:05:42 UTC",
    "updated_date": "2025-01-21 17:20:31 UTC"
  },
  {
    "arxiv_id": "2412.10621v1",
    "title": "WaveGNN: Modeling Irregular Multivariate Time Series for Accurate Predictions",
    "authors": [
      "Arash Hajisafi",
      "Maria Despoina Siampou",
      "Bita Azarijoo",
      "Cyrus Shahabi"
    ],
    "abstract": "Accurately modeling and analyzing time series data is crucial for downstream\napplications across various fields, including healthcare, finance, astronomy,\nand epidemiology. However, real-world time series often exhibit irregularities\nsuch as misaligned timestamps, missing entries, and variable sampling rates,\ncomplicating their analysis. Existing approaches often rely on imputation,\nwhich can introduce biases. A few approaches that directly model irregularity\ntend to focus exclusively on either capturing intra-series patterns or\ninter-series relationships, missing the benefits of integrating both. To this\nend, we present WaveGNN, a novel framework designed to directly (i.e., no\nimputation) embed irregularly sampled multivariate time series data for\naccurate predictions. WaveGNN utilizes a Transformer-based encoder to capture\nintra-series patterns by directly encoding the temporal dynamics of each time\nseries. To capture inter-series relationships, WaveGNN uses a dynamic graph\nneural network model, where each node represents a sensor, and the edges\ncapture the long- and short-term relationships between them. Our experimental\nresults on real-world healthcare datasets demonstrate that WaveGNN consistently\noutperforms existing state-of-the-art methods, with an average relative\nimprovement of 14.7% in F1-score when compared to the second-best baseline in\ncases with extreme sparsity. Our ablation studies reveal that both intra-series\nand inter-series modeling significantly contribute to this notable improvement.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10621v1",
    "published_date": "2024-12-14 00:03:44 UTC",
    "updated_date": "2024-12-14 00:03:44 UTC"
  }
]
[
  {
    "arxiv_id": "2505.04639v1",
    "title": "Language translation, and change of accent for speech-to-speech task using diffusion model",
    "authors": [
      "Abhishek Mishra",
      "Ritesh Sur Chowdhury",
      "Vartul Bahuguna",
      "Isha Pandey",
      "Ganesh Ramakrishnan"
    ],
    "abstract": "Speech-to-speech translation (S2ST) aims to convert spoken input in one\nlanguage to spoken output in another, typically focusing on either language\ntranslation or accent adaptation. However, effective cross-cultural\ncommunication requires handling both aspects simultaneously - translating\ncontent while adapting the speaker's accent to match the target language\ncontext. In this work, we propose a unified approach for simultaneous speech\ntranslation and change of accent, a task that remains underexplored in current\nliterature. Our method reformulates the problem as a conditional generation\ntask, where target speech is generated based on phonemes and guided by target\nspeech features. Leveraging the power of diffusion models, known for\nhigh-fidelity generative capabilities, we adapt text-to-image diffusion\nstrategies by conditioning on source speech transcriptions and generating Mel\nspectrograms representing the target speech with desired linguistic and\naccentual attributes. This integrated framework enables joint optimization of\ntranslation and accent adaptation, offering a more parameter-efficient and\neffective model compared to traditional pipelines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.04639v1",
    "published_date": "2025-05-04 23:23:46 UTC",
    "updated_date": "2025-05-04 23:23:46 UTC"
  },
  {
    "arxiv_id": "2505.02288v1",
    "title": "Universal Approximation Theorem of Deep Q-Networks",
    "authors": [
      "Qian Qi"
    ],
    "abstract": "We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs)\nvia stochastic control and Forward-Backward Stochastic Differential Equations\n(FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by\na square-integrable martingale, we analyze DQN approximation properties. We\nshow that DQNs can approximate the optimal Q-function on compact sets with\narbitrary accuracy and high probability, leveraging residual network\napproximation theorems and large deviation bounds for the state-action process.\nWe then analyze the convergence of a general Q-learning algorithm for training\nDQNs in this setting, adapting stochastic approximation theorems. Our analysis\nemphasizes the interplay between DQN layer count, time discretization, and the\nrole of viscosity solutions (primarily for the value function $V^*$) in\naddressing potential non-smoothness of the optimal Q-function. This work\nbridges deep reinforcement learning and stochastic control, offering insights\ninto DQNs in continuous-time settings, relevant for applications with physical\nsystems or high-frequency data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02288v1",
    "published_date": "2025-05-04 22:57:33 UTC",
    "updated_date": "2025-05-04 22:57:33 UTC"
  },
  {
    "arxiv_id": "2505.02281v1",
    "title": "Minimisation of Quasar-Convex Functions Using Random Zeroth-Order Oracles",
    "authors": [
      "Amir Ali Farzin",
      "Yuen-Man Pun",
      "Iman Shames"
    ],
    "abstract": "This study explores the performance of a random Gaussian smoothing\nzeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly\nquasar-convex (SQC) functions in both unconstrained and constrained settings.\nFor the unconstrained problem, we establish the ZO algorithm's convergence to a\nglobal minimum along with its complexity when applied to both QC and SQC\nfunctions. For the constrained problem, we introduce the new notion of\nproximal-quasar-convexity and prove analogous results to the unconstrained\ncase. Specifically, we show the complexity bounds and the convergence of the\nalgorithm to a neighbourhood of a global minimum whose size can be controlled\nunder a variance reduction scheme. Theoretical findings are illustrated through\ninvestigating the performance of the algorithm applied to a range of problems\nin machine learning and optimisation. Specifically, we observe scenarios where\nthe ZO method outperforms gradient descent. We provide a possible explanation\nfor this phenomenon.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02281v1",
    "published_date": "2025-05-04 22:43:57 UTC",
    "updated_date": "2025-05-04 22:43:57 UTC"
  },
  {
    "arxiv_id": "2505.02279v1",
    "title": "A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)",
    "authors": [
      "Abul Ehtesham",
      "Aditi Singh",
      "Gaurav Kumar Gupta",
      "Saket Kumar"
    ],
    "abstract": "Large language model (LLM)-powered autonomous agents demand robust,\nstandardized protocols to integrate tools, share contextual data, and\ncoordinate tasks across heterogeneous systems. Ad-hoc integrations are\ndifficult to scale, secure, and generalize across domains. This survey examines\nfour emerging agent communication protocols: Model Context Protocol (MCP),\nAgent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent\nNetwork Protocol (ANP), each addressing interoperability in distinct deployment\ncontexts. MCP provides a JSON-RPC client-server interface for secure tool\ninvocation and typed data exchange. ACP introduces REST-native messaging via\nmulti-part messages and asynchronous streaming to support multimodal agent\nresponses. A2A enables peer-to-peer task outsourcing through capability-based\nAgent Cards, facilitating enterprise-scale workflows. ANP supports open-network\nagent discovery and secure collaboration using decentralized identifiers (DIDs)\nand JSON-LD graphs. The protocols are compared across multiple dimensions,\nincluding interaction modes, discovery mechanisms, communication patterns, and\nsecurity models. Based on the comparative analysis, a phased adoption roadmap\nis proposed: beginning with MCP for tool access, followed by ACP for multimodal\nmessaging, A2A for collaborative task execution, and extending to ANP for\ndecentralized agent marketplaces. This work provides a comprehensive foundation\nfor designing secure, interoperable, and scalable ecosystems of LLM-powered\nagents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02279v1",
    "published_date": "2025-05-04 22:18:27 UTC",
    "updated_date": "2025-05-04 22:18:27 UTC"
  },
  {
    "arxiv_id": "2505.02274v1",
    "title": "On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles",
    "authors": [
      "Xingyu Zhao",
      "Robab Aghazadeh-Chakherlou",
      "Chih-Hong Cheng",
      "Peter Popov",
      "Lorenzo Strigini"
    ],
    "abstract": "Scenario-based testing has emerged as a common method for autonomous vehicles\n(AVs) safety, offering a more efficient alternative to mile-based testing by\nfocusing on high-risk scenarios. However, fundamental questions persist\nregarding its stopping rules, residual risk estimation, debug effectiveness,\nand the impact of simulation fidelity on safety claims. This paper argues that\na rigorous statistical foundation is essential to address these challenges and\nenable rigorous safety assurance. By drawing parallels between AV testing and\ntraditional software testing methodologies, we identify shared research gaps\nand reusable solutions. We propose proof-of-concept models to quantify the\nprobability of failure per scenario (pfs) and evaluate testing effectiveness\nunder varying conditions. Our analysis reveals that neither scenario-based nor\nmile-based testing universally outperforms the other. Furthermore, we introduce\nRisk Estimation Fidelity (REF), a novel metric to certify the alignment of\nsynthetic and real-world testing outcomes, ensuring simulation-based safety\nclaims are statistically defensible.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.SE",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2505.02274v1",
    "published_date": "2025-05-04 22:06:23 UTC",
    "updated_date": "2025-05-04 22:06:23 UTC"
  },
  {
    "arxiv_id": "2505.03837v1",
    "title": "Explainable Face Recognition via Improved Localization",
    "authors": [
      "Rashik Shadman",
      "Daqing Hou",
      "Faraz Hussain",
      "M G Sarwar Murshed"
    ],
    "abstract": "Biometric authentication has become one of the most widely used tools in the\ncurrent technological era to authenticate users and to distinguish between\ngenuine users and imposters. Face is the most common form of biometric modality\nthat has proven effective. Deep learning-based face recognition systems are now\ncommonly used across different domains. However, these systems usually operate\nlike black-box models that do not provide necessary explanations or\njustifications for their decisions. This is a major disadvantage because users\ncannot trust such artificial intelligence-based biometric systems and may not\nfeel comfortable using them when clear explanations or justifications are not\nprovided. This paper addresses this problem by applying an efficient method for\nexplainable face recognition systems. We use a Class Activation Mapping\n(CAM)-based discriminative localization (very narrow/specific localization)\ntechnique called Scaled Directed Divergence (SDD) to visually explain the\nresults of deep learning-based face recognition systems. We perform fine\nlocalization of the face features relevant to the deep learning model for its\nprediction/decision. Our experiments show that the SDD Class Activation Map\n(CAM) highlights the relevant face features very specifically compared to the\ntraditional CAM and very accurately. The provided visual explanations with\nnarrow localization of relevant features can ensure much-needed transparency\nand trust for deep learning-based face recognition systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03837v1",
    "published_date": "2025-05-04 21:58:16 UTC",
    "updated_date": "2025-05-04 21:58:16 UTC"
  },
  {
    "arxiv_id": "2505.02272v1",
    "title": "Robust Localization, Mapping, and Navigation for Quadruped Robots",
    "authors": [
      "Dyuman Aditya",
      "Junning Huang",
      "Nico Bohlinger",
      "Piotr Kicki",
      "Krzysztof Walas",
      "Jan Peters",
      "Matteo Luperto",
      "Davide Tateo"
    ],
    "abstract": "Quadruped robots are currently a widespread platform for robotics research,\nthanks to powerful Reinforcement Learning controllers and the availability of\ncheap and robust commercial platforms. However, to broaden the adoption of the\ntechnology in the real world, we require robust navigation stacks relying only\non low-cost sensors such as depth cameras. This paper presents a first step\ntowards a robust localization, mapping, and navigation system for low-cost\nquadruped robots. In pursuit of this objective we combine contact-aided\nkinematic, visual-inertial odometry, and depth-stabilized vision, enhancing\nstability and accuracy of the system. Our results in simulation and two\ndifferent real-world quadruped platforms show that our system can generate an\naccurate 2D map of the environment, robustly localize itself, and navigate\nautonomously. Furthermore, we present in-depth ablation studies of the\nimportant components of the system and their impact on localization accuracy.\nVideos, code, and additional experiments can be found on the project website:\nhttps://sites.google.com/view/low-cost-quadruped-slam",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 Pages",
    "pdf_url": "http://arxiv.org/pdf/2505.02272v1",
    "published_date": "2025-05-04 21:58:11 UTC",
    "updated_date": "2025-05-04 21:58:11 UTC"
  },
  {
    "arxiv_id": "2505.02271v1",
    "title": "Real-time Spatial Retrieval Augmented Generation for Urban Environments",
    "authors": [
      "David Nazareno Campo",
      "Javier Conde",
      "Álvaro Alonso",
      "Gabriel Huecas",
      "Joaquín Salvachúa",
      "Pedro Reviriego"
    ],
    "abstract": "The proliferation of Generative Artificial Ingelligence (AI), especially\nLarge Language Models, presents transformative opportunities for urban\napplications through Urban Foundation Models. However, base models face\nlimitations, as they only contain the knowledge available at the time of\ntraining, and updating them is both time-consuming and costly. Retrieval\nAugmented Generation (RAG) has emerged in the literature as the preferred\napproach for injecting contextual information into Foundation Models. It\nprevails over techniques such as fine-tuning, which are less effective in\ndynamic, real-time scenarios like those found in urban environments. However,\ntraditional RAG architectures, based on semantic databases, knowledge graphs,\nstructured data, or AI-powered web searches, do not fully meet the demands of\nurban contexts. Urban environments are complex systems characterized by large\nvolumes of interconnected data, frequent updates, real-time processing\nrequirements, security needs, and strong links to the physical world. This work\nproposes a real-time spatial RAG architecture that defines the necessary\ncomponents for the effective integration of generative AI into cities,\nleveraging temporal and spatial filtering capabilities through linked data. The\nproposed architecture is implemented using FIWARE, an ecosystem of software\ncomponents to develop smart city solutions and digital twins. The design and\nimplementation are demonstrated through the use case of a tourism assistant in\nthe city of Madrid. The use case serves to validate the correct integration of\nFoundation Models through the proposed RAG architecture.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02271v1",
    "published_date": "2025-05-04 21:57:58 UTC",
    "updated_date": "2025-05-04 21:57:58 UTC"
  },
  {
    "arxiv_id": "2505.02266v1",
    "title": "Parameter-Efficient Transformer Embeddings",
    "authors": [
      "Henry Ndubuaku",
      "Mouad Talhi"
    ],
    "abstract": "Embedding layers in transformer-based NLP models typically account for the\nlargest share of model parameters, scaling with vocabulary size but not\nyielding performance gains proportional to scale. We propose an alternative\napproach in which token embedding vectors are first generated\ndeterministically, directly from the token IDs using a Fourier expansion of\ntheir normalized values, followed by a lightweight multilayer perceptron (MLP)\nthat captures higher-order interactions. We train standard transformers and our\narchitecture on natural language inference tasks (SNLI and MNLI), and evaluate\nzero-shot performance on sentence textual similarity (STS-B). Our results\ndemonstrate that the proposed method achieves competitive performance using\nsignificantly fewer parameters, trains faster, and operates effectively without\nthe need for dropout. This proof-of-concept study highlights the potential for\nscalable, memory-efficient language models and motivates further large-scale\nexperimentation based on our findings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T07 (Primary) 68T50 (Secondary)"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 2 tables. Code available at https://github.com/HMUNACHI/pete",
    "pdf_url": "http://arxiv.org/pdf/2505.02266v1",
    "published_date": "2025-05-04 21:47:18 UTC",
    "updated_date": "2025-05-04 21:47:18 UTC"
  },
  {
    "arxiv_id": "2505.02255v2",
    "title": "Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset",
    "authors": [
      "Jakub Wasala",
      "Bartlomiej Wrzalski",
      "Kornelia Noculak",
      "Yuliia Tarasenko",
      "Oliwer Krupa",
      "Jan Kocon",
      "Grzegorz Chodak"
    ],
    "abstract": "This study presents a novel approach to enhance the cost-to-quality ratio of\nimage generation with diffusion models. We hypothesize that differences between\ndistilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are\nconsistent and, therefore, learnable within a specialized domain, like portrait\ngeneration. We generate a synthetic paired dataset and train a fast\nimage-to-image translation head. Using two sets of low- and high-quality\nsynthetic images, our model is trained to refine the output of a distilled\ngenerator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like\nFLUX.1-dev, which is more computationally intensive. Our results show that the\npipeline, which combines a distilled version of a large generative model with\nour enhancement layer, delivers similar photorealistic portraits to the\nbaseline version with up to an 82% decrease in computational cost compared to\nFLUX.1-dev. This study demonstrates the potential for improving the efficiency\nof AI solutions involving large-scale image generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "25th International Conference on Computational Science",
    "pdf_url": "http://arxiv.org/pdf/2505.02255v2",
    "published_date": "2025-05-04 21:28:21 UTC",
    "updated_date": "2025-05-08 19:05:22 UTC"
  },
  {
    "arxiv_id": "2505.02247v1",
    "title": "RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation",
    "authors": [
      "Jingxiang Qu",
      "Wenhan Gao",
      "Jiaxing Zhang",
      "Xufeng Liu",
      "Hua Wei",
      "Haibin Ling",
      "Yi Liu"
    ],
    "abstract": "3D Geometric Graph Neural Networks (GNNs) have emerged as transformative\ntools for modeling molecular data. Despite their predictive power, these models\noften suffer from limited interpretability, raising concerns for scientific\napplications that require reliable and transparent insights. While existing\nmethods have primarily focused on explaining molecular substructures in 2D\nGNNs, the transition to 3D GNNs introduces unique challenges, such as handling\nthe implicit dense edge structures created by a cut-off radius. To tackle this,\nwe introduce a novel explanation method specifically designed for 3D GNNs,\nwhich localizes the explanation to the immediate neighborhood of each node\nwithin the 3D space. Each node is assigned an radius of influence, defining the\nlocalized region within which message passing captures spatial and structural\ninteractions crucial for the model's predictions. This method leverages the\nspatial and geometric characteristics inherent in 3D graphs. By constraining\nthe subgraph to a localized radius of influence, the approach not only enhances\ninterpretability but also aligns with the physical and structural dependencies\ntypical of 3D graph applications, such as molecular learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02247v1",
    "published_date": "2025-05-04 21:01:45 UTC",
    "updated_date": "2025-05-04 21:01:45 UTC"
  },
  {
    "arxiv_id": "2505.03836v1",
    "title": "OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery",
    "authors": [
      "Chongsheng Zhang",
      "Shuwen Wu",
      "Yingqi Chen",
      "Matthias Aßenmacher",
      "Christian Heumann",
      "Yi Men",
      "Gaojuan Fan",
      "João Gama"
    ],
    "abstract": "Oracle Bone Inscription (OBI) is the earliest systematic writing system in\nChina, while the identification of Oracle Bone (OB) duplicates is a fundamental\nissue in OBI research. In this work, we design a progressive OB duplicate\ndiscovery framework that combines unsupervised low-level keypoints matching\nwith high-level text-centric content-based matching to refine and rank the\ncandidate OB duplicates with semantic awareness and interpretability. We\ncompare our approach with state-of-the-art content-based image retrieval and\nimage matching methods, showing that our approach yields comparable recall\nperformance and the highest simplified mean reciprocal rank scores for both\nTop-5 and Top-15 retrieval results, and with significantly accelerated\ncomputation efficiency. We have discovered over 60 pairs of new OB duplicates\nin real-world deployment, which were missed by OBI researchers for decades. The\nmodels, video illustration and demonstration of this work are available at:\nhttps://github.com/cszhangLMU/OBD-Finder/.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "This is the long version of our OBD-Finder paper for AI-enabled\n  Oracle Bone Duplicates Discovery (currently under review at the ECML PKDD\n  2025 Demo Track). The models, video illustration and demonstration of this\n  paper are available at: https://github.com/cszhangLMU/OBD-Finder/.\n  Illustration video: https://www.youtube.com/watch?v=5QT4f0YIo0Q",
    "pdf_url": "http://arxiv.org/pdf/2505.03836v1",
    "published_date": "2025-05-04 20:35:15 UTC",
    "updated_date": "2025-05-04 20:35:15 UTC"
  },
  {
    "arxiv_id": "2505.02236v1",
    "title": "Improving Physical Object State Representation in Text-to-Image Generative Systems",
    "authors": [
      "Tianle Chen",
      "Chaitanya Chakka",
      "Deepti Ghadiyaram"
    ],
    "abstract": "Current text-to-image generative models struggle to accurately represent\nobject states (e.g., \"a table without a bottle,\" \"an empty tumbler\"). In this\nwork, we first design a fully-automatic pipeline to generate high-quality\nsynthetic data that accurately captures objects in varied states. Next, we\nfine-tune several open-source text-to-image models on this synthetic data. We\nevaluate the performance of the fine-tuned models by quantifying the alignment\nof the generated images to their prompts using GPT4o-mini, and achieve an\naverage absolute improvement of 8+% across four models on the public\nGenAI-Bench dataset. We also curate a collection of 200 prompts with a specific\nfocus on common objects in various physical states. We demonstrate a\nsignificant improvement of an average of 24+% over the baseline on this\ndataset. We release all evaluation prompts and code.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to Synthetic Data for Computer Vision - CVPR 2025 Workshop",
    "pdf_url": "http://arxiv.org/pdf/2505.02236v1",
    "published_date": "2025-05-04 20:24:57 UTC",
    "updated_date": "2025-05-04 20:24:57 UTC"
  },
  {
    "arxiv_id": "2505.02235v1",
    "title": "SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation",
    "authors": [
      "Tanguy Herserant",
      "Vincent Guigue"
    ],
    "abstract": "Evaluating text summarization quality remains a critical challenge in Natural\nLanguage Processing. Current approaches face a trade-off between performance\nand interpretability. We present SEval-Ex, a framework that bridges this gap by\ndecomposing summarization evaluation into atomic statements, enabling both high\nperformance and explainability. SEval-Ex employs a two-stage pipeline: first\nextracting atomic statements from text source and summary using LLM, then a\nmatching between generated statements. Unlike existing approaches that provide\nonly summary-level scores, our method generates detailed evidence for its\ndecisions through statement-level alignments. Experiments on the SummEval\nbenchmark demonstrate that SEval-Ex achieves state-of-the-art performance with\n0.580 correlation on consistency with human consistency judgments, surpassing\nGPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our\nframework shows robustness against hallucination.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02235v1",
    "published_date": "2025-05-04 20:16:08 UTC",
    "updated_date": "2025-05-04 20:16:08 UTC"
  },
  {
    "arxiv_id": "2505.02232v1",
    "title": "Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning",
    "authors": [
      "Malte Mosbach",
      "Sven Behnke"
    ],
    "abstract": "Building models responsive to input prompts represents a transformative shift\nin machine learning. This paradigm holds significant potential for robotics\nproblems, such as targeted manipulation amidst clutter. In this work, we\npresent a novel approach to combine promptable foundation models with\nreinforcement learning (RL), enabling robots to perform dexterous manipulation\ntasks in a prompt-responsive manner. Existing methods struggle to link\nhigh-level commands with fine-grained dexterous control. We address this gap\nwith a memory-augmented student-teacher learning framework. We use the\nSegment-Anything 2 (SAM 2) model as a perception backbone to infer an object of\ninterest from user prompts. While detections are imperfect, their temporal\nsequence provides rich information for implicit state estimation by\nmemory-augmented models. Our approach successfully learns prompt-responsive\npolicies, demonstrated in picking objects from cluttered scenes. Videos and\ncode are available at https://memory-student-teacher.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02232v1",
    "published_date": "2025-05-04 19:51:09 UTC",
    "updated_date": "2025-05-04 19:51:09 UTC"
  },
  {
    "arxiv_id": "2505.03835v1",
    "title": "The Shift Towards Preprints in AI Policy Research: A Comparative Study of Preprint Trends in the U.S., Europe, and South Korea",
    "authors": [
      "Simon Suh",
      "Jihyuk Bang",
      "Ji Woo Han"
    ],
    "abstract": "The adoption of open science has quickly changed how artificial intelligence\n(AI) policy research is distributed globally. This study examines the regional\ntrends in the citation of preprints, specifically focusing on the impact of two\nmajor disruptive events: the COVID-19 pandemic and the release of ChatGPT, on\nresearch dissemination patterns in the United States, Europe, and South Korea\nfrom 2015 to 2024. Using bibliometrics data from the Web of Science, this study\ntracks how global disruptive events influenced the adoption of preprints in AI\npolicy research and how such shifts vary by region. By marking the timing of\nthese disruptive events, the analysis reveals that while all regions\nexperienced growth in preprint citations, the magnitude and trajectory of\nchange varied significantly. The United States exhibited sharp, event-driven\nincreases; Europe demonstrated institutional growth; and South Korea maintained\nconsistent, linear growth in preprint adoption. These findings suggest that\nglobal disruptions may have accelerated preprint adoption, but the extent and\ntrajectory are shaped by local research cultures, policy environments, and\nlevels of open science maturity. This paper emphasizes the need for future AI\ngovernance strategies to consider regional variability in research\ndissemination and highlights opportunities for further longitudinal and\ncomparative research to deepen our understanding of open-access adoption in AI\npolicy development.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CY",
      "I.2.0; K.4.0"
    ],
    "primary_category": "cs.DL",
    "comment": "22 pages, 6 figures, 3 tables. Uses cross-regional analysis to\n  evaluate how preprint citation trends in AI - policy research have shifted\n  over time in response to two major global events: the COVID-19 pandemic and\n  the release of ChatGPT. Compares United States, Europe, and South Korea",
    "pdf_url": "http://arxiv.org/pdf/2505.03835v1",
    "published_date": "2025-05-04 19:44:41 UTC",
    "updated_date": "2025-05-04 19:44:41 UTC"
  },
  {
    "arxiv_id": "2505.02230v1",
    "title": "The GenAI Generation: Student Views of Awareness, Preparedness, and Concern",
    "authors": [
      "Micaela Siraj",
      "Jon Duke"
    ],
    "abstract": "Generative AI (GenAI) is revolutionizing education and workforce development,\nprofoundly shaping how students learn, engage, and prepare for their future.\nOutpacing the development of uniform policies and structures, GenAI has\nheralded a unique era and given rise to the GenAI Generation: a cohort of\nstudents whose education has been increasingly shaped by the opportunities and\nchallenges GenAI presents during its widespread adoption within society. This\nstudy examines our students' perceptions of GenAI through a concise survey with\noptional open-ended questions, focusing on their awareness, preparedness, and\nconcerns. Evaluation of more than 250 responses with more than 40% providing\ndetailed qualitative feedback reveals a core dual sentiment: while most\nstudents express enthusiasm for GenAI, an even greater proportion voice a\nspectrum of concerns about ethics, job displacement, and the adequacy of\neducational structures given the highly transformative technology. These\nfindings offer critical insights into how students view the potential and\npitfalls of GenAI for future career impacts, with accompanying recommendations\nto guide educational institutions in navigating a future driven by GenAI.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02230v1",
    "published_date": "2025-05-04 19:37:13 UTC",
    "updated_date": "2025-05-04 19:37:13 UTC"
  },
  {
    "arxiv_id": "2505.02228v1",
    "title": "Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning",
    "authors": [
      "Shangzhe Li",
      "Zhiao Huang",
      "Hao Su"
    ],
    "abstract": "Imitation Learning (IL) has achieved remarkable success across various\ndomains, including robotics, autonomous driving, and healthcare, by enabling\nagents to learn complex behaviors from expert demonstrations. However, existing\nIL methods often face instability challenges, particularly when relying on\nadversarial reward or value formulations in world model frameworks. In this\nwork, we propose a novel approach to online imitation learning that addresses\nthese limitations through a reward model based on random network distillation\n(RND) for density estimation. Our reward model is built on the joint estimation\nof expert and behavioral distributions within the latent space of the world\nmodel. We evaluate our method across diverse benchmarks, including DMControl,\nMeta-World, and ManiSkill2, showcasing its ability to deliver stable\nperformance and achieve expert-level results in both locomotion and\nmanipulation tasks. Our approach demonstrates improved stability over\nadversarial methods while maintaining expert-level performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02228v1",
    "published_date": "2025-05-04 19:32:48 UTC",
    "updated_date": "2025-05-04 19:32:48 UTC"
  },
  {
    "arxiv_id": "2505.02216v2",
    "title": "LLM-Guided Probabilistic Program Induction for POMDP Model Estimation",
    "authors": [
      "Aidan Curtis",
      "Hao Tang",
      "Thiago Veloso",
      "Kevin Ellis",
      "Joshua Tenenbaum",
      "Tomás Lozano-Pérez",
      "Leslie Pack Kaelbling"
    ],
    "abstract": "Partially Observable Markov Decision Processes (POMDPs) model decision making\nunder uncertainty. While there are many approaches to approximately solving\nPOMDPs, we aim to address the problem of learning such models. In particular,\nwe are interested in a subclass of POMDPs wherein the components of the model,\nincluding the observation function, reward function, transition function, and\ninitial state distribution function, can be modeled as low-complexity\nprobabilistic graphical models in the form of a short probabilistic program.\nOur strategy to learn these programs uses an LLM as a prior, generating\ncandidate probabilistic programs that are then tested against the empirical\ndistribution and adjusted through feedback. We experiment on a number of\nclassical toy POMDP problems, simulated MiniGrid domains, and two real\nmobile-base robotics search domains involving partial observability. Our\nresults show that using an LLM to guide in the construction of a low-complexity\nPOMDP model can be more effective than tabular POMDP learning, behavior\ncloning, or direct LLM planning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02216v2",
    "published_date": "2025-05-04 18:59:07 UTC",
    "updated_date": "2025-05-12 03:34:54 UTC"
  },
  {
    "arxiv_id": "2505.02215v1",
    "title": "Interpretable Emergent Language Using Inter-Agent Transformers",
    "authors": [
      "Mannan Bhardwaj"
    ],
    "abstract": "This paper explores the emergence of language in multi-agent reinforcement\nlearning (MARL) using transformers. Existing methods such as RIAL, DIAL, and\nCommNet enable agent communication but lack interpretability. We propose\nDifferentiable Inter-Agent Transformers (DIAT), which leverage self-attention\nto learn symbolic, human-understandable communication protocols. Through\nexperiments, DIAT demonstrates the ability to encode observations into\ninterpretable vocabularies and meaningful embeddings, effectively solving\ncooperative tasks. These results highlight the potential of DIAT for\ninterpretable communication in complex multi-agent environments.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02215v1",
    "published_date": "2025-05-04 18:57:57 UTC",
    "updated_date": "2025-05-04 18:57:57 UTC"
  },
  {
    "arxiv_id": "2505.02206v1",
    "title": "DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units",
    "authors": [
      "Lei Mao",
      "Yuanhe Tian",
      "Yan Song"
    ],
    "abstract": "Genome modeling conventionally treats gene sequence as a language, reflecting\nits structured motifs and long-range dependencies analogous to linguistic units\nand organization principles such as words and syntax. Recent studies utilize\nadvanced neural networks, ranging from convolutional and recurrent models to\nTransformer-based models, to capture contextual information of gene sequence,\nwith the primary goal of obtaining effective gene sequence representations and\nthus enhance the models' understanding of various running gene samples.\nHowever, these approaches often directly apply language modeling techniques to\ngene sequences and do not fully consider the intrinsic information organization\nin them, where they do not consider how units at different granularities\ncontribute to representation. In this paper, we propose DNAZEN, an enhanced\ngenomic representation framework designed to learn from various granularities\nin gene sequences, including small polymers and G-grams that are combinations\nof several contiguous polymers. Specifically, we extract the G-grams from\nlarge-scale genomic corpora through an unsupervised approach to construct the\nG-gram vocabulary, which is used to provide G-grams in the learning process of\nDNA sequences through dynamically matching from running gene samples. A\nTransformer-based G-gram encoder is also proposed and the matched G-grams are\nfed into it to compute their representations and integrated into the encoder\nfor basic unit (E4BU), which is responsible for encoding small units and\nmaintaining the learning and inference process. To further enhance the learning\nprocess, we propose whole G-gram masking to train DNAZEN, where the model\nlargely favors the selection of each entire G-gram to mask rather than an\nordinary masking mechanism performed on basic units. Experiments on benchmark\ndatasets demonstrate the effectiveness of DNAZEN on various downstream tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.02206v1",
    "published_date": "2025-05-04 18:02:28 UTC",
    "updated_date": "2025-05-04 18:02:28 UTC"
  },
  {
    "arxiv_id": "2505.02198v1",
    "title": "Student Perspectives on the Benefits and Risks of AI in Education",
    "authors": [
      "Griffin Pitts",
      "Viktoria Marcus",
      "Sanaz Motamedi"
    ],
    "abstract": "The use of chatbots equipped with artificial intelligence (AI) in educational\nsettings has increased in recent years, showing potential to support teaching\nand learning. However, the adoption of these technologies has raised concerns\nabout their impact on academic integrity, students' ability to problem-solve\nindependently, and potential underlying biases. To better understand students'\nperspectives and experiences with these tools, a survey was conducted at a\nlarge public university in the United States. Through thematic analysis, 262\nundergraduate students' responses regarding their perceived benefits and risks\nof AI chatbots in education were identified and categorized into themes.\n  The results discuss several benefits identified by the students, with\nfeedback and study support, instruction capabilities, and access to information\nbeing the most cited. Their primary concerns included risks to academic\nintegrity, accuracy of information, loss of critical thinking skills, the\npotential development of overreliance, and ethical considerations such as data\nprivacy, system bias, environmental impact, and preservation of human elements\nin education.\n  While student perceptions align with previously discussed benefits and risks\nof AI in education, they show heightened concerns about distinguishing between\nhuman and AI generated work - particularly in cases where authentic work is\nflagged as AI-generated. To address students' concerns, institutions can\nestablish clear policies regarding AI use and develop curriculum around AI\nliteracy. With these in place, practitioners can effectively develop and\nimplement educational systems that leverage AI's potential in areas such as\nimmediate feedback and personalized learning support. This approach can enhance\nthe quality of students' educational experiences while preserving the integrity\nof the learning process with AI.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "K.3; K.4"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02198v1",
    "published_date": "2025-05-04 17:36:11 UTC",
    "updated_date": "2025-05-04 17:36:11 UTC"
  },
  {
    "arxiv_id": "2505.02192v1",
    "title": "DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization",
    "authors": [
      "Wenchuan Wang",
      "Mengqi Huang",
      "Yijing Tu",
      "Zhendong Mao"
    ],
    "abstract": "Customized text-to-video generation with pre-trained large-scale models has\nrecently garnered significant attention through focusing on identity and motion\nconsistency. Existing works typically follow the isolated customized paradigm,\nwhere the subject identity or motion dynamics are customized exclusively.\nHowever, this paradigm completely ignores the intrinsic mutual constraints and\nsynergistic interdependencies between identity and motion, resulting in\nidentity-motion conflicts throughout the generation process that systematically\ndegrades. To address this, we introduce DualReal, a novel framework that,\nemploys adaptive joint training to collaboratively construct interdependencies\nbetween dimensions. Specifically, DualReal is composed of two units: (1)\nDual-aware Adaptation dynamically selects a training phase (i.e., identity or\nmotion), learns the current information guided by the frozen dimension prior,\nand employs a regularization strategy to avoid knowledge leakage; (2)\nStageBlender Controller leverages the denoising stages and Diffusion\nTransformer depths to guide different dimensions with adaptive granularity,\navoiding conflicts at various stages and ultimately achieving lossless fusion\nof identity and motion patterns. We constructed a more comprehensive benchmark\nthan existing methods. The experimental results show that DualReal improves\nCLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top\nperformance on nearly all motion quality metrics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02192v1",
    "published_date": "2025-05-04 17:19:20 UTC",
    "updated_date": "2025-05-04 17:19:20 UTC"
  },
  {
    "arxiv_id": "2505.02184v1",
    "title": "Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes",
    "authors": [
      "Matthew T. Dearing",
      "Yiheng Tao",
      "Xingfu Wu",
      "Zhiling Lan",
      "Valerie Taylor"
    ],
    "abstract": "While large language models (LLMs) are increasingly used for generating\nparallel scientific code, most current efforts emphasize functional\ncorrectness, often overlooking performance and energy considerations. In this\nwork, we propose LASSI-EE, an automated LLM-based refactoring framework that\ngenerates energy-efficient parallel code on a target parallel system for a\ngiven parallel code as input. Through a multi-stage, iterative pipeline\nprocess, LASSI-EE achieved an average energy reduction of 47% across 85% of the\n20 HeCBench benchmarks tested on NVIDIA A100 GPUs. Our findings demonstrate the\nbroader potential of LLMs, not only for generating correct code but also for\nenabling energy-aware programming. We also address key insights and limitations\nwithin the framework, offering valuable guidance for future improvements.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.02184v1",
    "published_date": "2025-05-04 17:05:34 UTC",
    "updated_date": "2025-05-04 17:05:34 UTC"
  },
  {
    "arxiv_id": "2505.02874v1",
    "title": "Uncertainty Quantification for Machine Learning in Healthcare: A Survey",
    "authors": [
      "L. Julián Lechuga López",
      "Shaza Elsharief",
      "Dhiyaa Al Jorf",
      "Firas Darwish",
      "Congbo Ma",
      "Farah E. Shamout"
    ],
    "abstract": "Uncertainty Quantification (UQ) is pivotal in enhancing the robustness,\nreliability, and interpretability of Machine Learning (ML) systems for\nhealthcare, optimizing resources and improving patient care. Despite the\nemergence of ML-based clinical decision support tools, the lack of principled\nquantification of uncertainty in ML models remains a major challenge. Current\nreviews have a narrow focus on analyzing the state-of-the-art UQ in specific\nhealthcare domains without systematically evaluating method efficacy across\ndifferent stages of model development, and despite a growing body of research,\nits implementation in healthcare applications remains limited. Therefore, in\nthis survey, we provide a comprehensive analysis of current UQ in healthcare,\noffering an informed framework that highlights how different methods can be\nintegrated into each stage of the ML pipeline including data processing,\ntraining and evaluation. We also highlight the most popular methods used in\nhealthcare and novel approaches from other domains that hold potential for\nfuture adoption in the medical context. We expect this study will provide a\nclear overview of the challenges and opportunities of implementing UQ in the ML\npipeline for healthcare, guiding researchers and practitioners in selecting\nsuitable techniques to enhance the reliability, safety and trust from patients\nand clinicians on ML-driven healthcare solutions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "46 pages, 3 figures, 2 tables, AHLI Conference on Health, Inference,\n  and Learning (CHIL)",
    "pdf_url": "http://arxiv.org/pdf/2505.02874v1",
    "published_date": "2025-05-04 16:56:22 UTC",
    "updated_date": "2025-05-04 16:56:22 UTC"
  },
  {
    "arxiv_id": "2505.02171v1",
    "title": "A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking",
    "authors": [
      "Henrik Brådland",
      "Morten Goodwin",
      "Per-Arne Andersen",
      "Alexander S. Nossum",
      "Aditya Gupta"
    ],
    "abstract": "Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG)\nby determining how source materials are segmented before indexing. Despite\nevidence that Large Language Models (LLMs) are sensitive to the layout and\nstructure of retrieved data, there is currently no framework to analyze the\nimpact of different chunking methods. In this paper, we introduce a novel\nmethodology that defines essential characteristics of the chunking process at\nthree levels: intrinsic passage properties, extrinsic passage properties, and\npassages-document coherence. We propose HOPE (Holistic Passage Evaluation), a\ndomain-agnostic, automatic evaluation metric that quantifies and aggregates\nthese characteristics. Our empirical evaluations across seven domains\ndemonstrate that the HOPE metric correlates significantly (p > 0.13) with\nvarious RAG performance indicators, revealing contrasts between the importance\nof extrinsic and intrinsic properties of passages. Semantic independence\nbetween passages proves essential for system performance with a performance\ngain of up to 56.2% in factual correctness and 21.1% in answer correctness. On\nthe contrary, traditional assumptions about maintaining concept unity within\npassages show minimal impact. These findings provide actionable insights for\noptimizing chunking strategies, thus improving RAG system design to produce\nmore factually correct responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, To be published in SIGIR25",
    "pdf_url": "http://arxiv.org/pdf/2505.02171v1",
    "published_date": "2025-05-04 16:22:27 UTC",
    "updated_date": "2025-05-04 16:22:27 UTC"
  },
  {
    "arxiv_id": "2505.02170v1",
    "title": "Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach",
    "authors": [
      "Danial Ramezani"
    ],
    "abstract": "Fantasy football is a billion-dollar industry with millions of participants.\nConstrained by a fixed budget, decision-makers draft a squad whose players are\nexpected to perform well in the upcoming weeks to maximize total points. This\npaper proposes novel deterministic and robust integer programming models that\nselect the optimal starting eleven and the captain. A new hybrid scoring metric\nis constructed using an interpretable artificial intelligence framework and\nunderlying match performance data. Several objective functions and estimation\ntechniques are introduced for the programming model. To the best of my\nknowledge, this is the first study to approach fantasy football through this\nlens. The models' performance is evaluated using data from the 2023/24 Premier\nLeague season. Results indicate that the proposed hybrid method achieved the\nhighest score while maintaining consistent performance. Utilizing the Monte\nCarlo simulation, the strategic choice of averaging techniques for estimating\ncost vectors, and the proposed hybrid approach are shown to be effective during\nthe out-of-sample period. This paper also provides a thorough analysis of the\noptimal formations and players selected by the models, offering valuable\ninsights into effective fantasy football strategies.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02170v1",
    "published_date": "2025-05-04 16:21:59 UTC",
    "updated_date": "2025-05-04 16:21:59 UTC"
  },
  {
    "arxiv_id": "2505.02158v1",
    "title": "Pickup & Delivery with Time Windows and Transfers: combining decomposition with metaheuristics",
    "authors": [
      "Ioannis Avgerinos",
      "Ioannis Mourtos",
      "Nikolaos Tsompanidis",
      "Georgios Zois"
    ],
    "abstract": "This paper examines the generalisation of the Pickup and Delivery Problem\nthat allows mid-route load exchanges among vehicles and obeys strict\ntime-windows at all locations. We propose a novel Logic-Based Benders\nDecomposition (LBBD) that improves optimality gaps for all benchmarks in the\nliterature and scales up to handle larger ones. To tackle even larger\ninstances, we introduce a refined Large Neighborhood Search (LNS) algorithm\nthat improves the adaptability of LNS beyond case-specific configurations\nappearing in related literature.\n  To bridge the gap in benchmark availability, we develop an instance generator\nthat allows for extensive experimentation. For moderate datasets (25 and 50\nrequests), we evaluate the performance of both LBBD and LNS, the former being\nable to close the gap and the latter capable of providing near-optimal\nsolutions. For larger instances (75 and 100 requests), we recreate indicative\nstate-of-the-art metaheuristics to highlight the improvements introduced by our\nLNS refinements, while establishing its scalability.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02158v1",
    "published_date": "2025-05-04 15:45:09 UTC",
    "updated_date": "2025-05-04 15:45:09 UTC"
  },
  {
    "arxiv_id": "2505.02156v4",
    "title": "Adaptive Thinking via Mode Policy Optimization for Social Language Agents",
    "authors": [
      "Minzheng Wang",
      "Yongbin Li",
      "Haobo Wang",
      "Xinghua Zhang",
      "Nan Xu",
      "Bingli Wu",
      "Fei Huang",
      "Haiyang Yu",
      "Wenji Mao"
    ],
    "abstract": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\nstudies. Existing methods either lack this kind of reasoning capability or\nenforce Long Chain-of-Thought reasoning uniformly across all scenarios,\nresulting in excessive token usage and inflexible social simulation. To address\nthis, we propose an $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{L}$earning\n($\\textbf{AML}$) framework in this paper, aiming to improve the adaptive\nthinking ability of language agents in dynamic social interactions. To this\nend, we first identify hierarchical thinking modes ranging from intuitive\nresponse to deep deliberation based on the cognitive control theory. We then\ndevelop the $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{P}$olicy\n$\\textbf{O}$ptimization ($\\textbf{AMPO}$) algorithm to optimize the\ncontext-aware mode switching and reasoning. Our framework advances existing\nresearch in three key aspects: (1) Multi-granular thinking mode design, (2)\nContext-aware mode switching across social interaction, and (3) Token-efficient\nreasoning via depth-adaptive processing. Extensive experiments on social\nintelligence benchmarks verify that AML achieves 15.6% higher task performance\nthan GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter\nreasoning chains, demonstrating the advantage of adaptive thinking mode\nselection and optimization mechanism in AMPO over GRPO's fixed-depth solution.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in Progress. The code and data are available, see\n  https://github.com/MozerWang/AMPO",
    "pdf_url": "http://arxiv.org/pdf/2505.02156v4",
    "published_date": "2025-05-04 15:39:58 UTC",
    "updated_date": "2025-05-22 09:44:47 UTC"
  },
  {
    "arxiv_id": "2505.02154v1",
    "title": "Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models through Axiomatic Causal Interventions",
    "authors": [
      "Oliver Savolainen",
      "Dur e Najaf Amjad",
      "Roxana Petcu"
    ],
    "abstract": "This reproducibility study analyzes and extends the paper \"Axiomatic Causal\nInterventions for Reverse Engineering Relevance Computation in Neural Retrieval\nModels,\" which investigates how neural retrieval models encode task-relevant\nproperties such as term frequency. We reproduce key experiments from the\noriginal paper, confirming that information on query terms is captured in the\nmodel encoding. We extend this work by applying activation patching to Spanish\nand Chinese datasets and by exploring whether document-length information is\nencoded in the model as well. Our results confirm that the designed activation\npatching method can isolate the behavior to specific components and tokens in\nneural retrieval models. Moreover, our findings indicate that the location of\nterm frequency generalizes across languages and that in later layers, the\ninformation for sequence-level tasks is represented in the CLS token. The\nresults highlight the need for further research into interpretability in\ninformation retrieval and reproducibility in machine learning research. Our\ncode is available at\nhttps://github.com/OliverSavolainen/axiomatic-ir-reproduce.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, SIGIR 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.02154v1",
    "published_date": "2025-05-04 15:30:45 UTC",
    "updated_date": "2025-05-04 15:30:45 UTC"
  },
  {
    "arxiv_id": "2505.02139v1",
    "title": "Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking",
    "authors": [
      "Muyao Zhong",
      "Yushi Lin",
      "Peng Yang"
    ],
    "abstract": "The Limit Order Book (LOB), the mostly fundamental data of the financial\nmarket, provides a fine-grained view of market dynamics while poses significant\nchallenges in dealing with the esteemed deep models due to its strong\nautocorrelation, cross-feature constrains, and feature scale disparity.\nExisting approaches often tightly couple representation learning with specific\ndownstream tasks in an end-to-end manner, failed to analyze the learned\nrepresentations individually and explicitly, limiting their reusability and\ngeneralization. This paper conducts the first systematic comparative study of\nLOB representation learning, aiming to identify the effective way of extracting\ntransferable, compact features that capture essential LOB properties. We\nintroduce LOBench, a standardized benchmark with real China A-share market\ndata, offering curated datasets, unified preprocessing, consistent evaluation\nmetrics, and strong baselines. Extensive experiments validate the sufficiency\nand necessity of LOB representations for various downstream tasks and highlight\ntheir advantages over both the traditional task-specific end-to-end models and\nthe advanced representation learning models for general time series. Our work\nestablishes a reproducible framework and provides clear guidelines for future\nresearch. Datasets and code will be publicly available at\nhttps://github.com/financial-simulation-lab/LOBench.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02139v1",
    "published_date": "2025-05-04 15:00:00 UTC",
    "updated_date": "2025-05-04 15:00:00 UTC"
  },
  {
    "arxiv_id": "2505.02130v1",
    "title": "Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data",
    "authors": [
      "Zhong Guan",
      "Likang Wu",
      "Hongke Zhao",
      "Ming He",
      "Jianpin Fan"
    ],
    "abstract": "Attention mechanisms are critical to the success of large language models\n(LLMs), driving significant advancements in multiple fields. However, for\ngraph-structured data, which requires emphasis on topological connections, they\nfall short compared to message-passing mechanisms on fixed links, such as those\nemployed by Graph Neural Networks (GNNs). This raises a question: ``Does\nattention fail for graphs in natural language settings?'' Motivated by these\nobservations, we embarked on an empirical study from the perspective of\nattention mechanisms to explore how LLMs process graph-structured data. The\ngoal is to gain deeper insights into the attention behavior of LLMs over graph\nstructures. We uncovered unique phenomena regarding how LLMs apply attention to\ngraph-structured data and analyzed these findings to improve the modeling of\nsuch data by LLMs. The primary findings of our research are: 1) While LLMs can\nrecognize graph data and capture text-node interactions, they struggle to model\ninter-node relationships within graph structures due to inherent architectural\nconstraints. 2) The attention distribution of LLMs across graph nodes does not\nalign with ideal structural patterns, indicating a failure to adapt to graph\ntopology nuances. 3) Neither fully connected attention nor fixed connectivity\nis optimal; each has specific limitations in its application scenarios.\nInstead, intermediate-state attention windows improve LLM training performance\nand seamlessly transition to fully connected windows during inference. Source\ncode: \\href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML2025 Accept",
    "pdf_url": "http://arxiv.org/pdf/2505.02130v1",
    "published_date": "2025-05-04 14:40:31 UTC",
    "updated_date": "2025-05-04 14:40:31 UTC"
  },
  {
    "arxiv_id": "2505.02129v2",
    "title": "Subspace Aggregation Query and Index Generation for Multidimensional Resource Space Model",
    "authors": [
      "Xiaoping Sun",
      "Hai Zhuge"
    ],
    "abstract": "Organizing resources in a multidimensional classification space is an\napproach to efficiently managing and querying large-scale resources. This paper\ndefines an aggregation query on subspace defined by a range on the partial\norder on coordinate tree at each dimension, where each point contains resources\naggregated along the paths of partial order relations on the points so that\naggregated resources at each point within the subspace can be measured, ranked\nand selected. To efficiently locate non-empty points in a large subspace, an\napproach to generating graph index is proposed to build inclusion links with\npartial order relations on coordinates of dimensions to enable a subspace query\nto reach non-empty points by following indexing links and aggregate resources\nalong indexing paths back to their super points. Generating such an index is\ncostly as the number of children of an index node can be very large so that the\ntotal number of indexing nodes is unbounded. The proposed approach adopts the\nfollowing strategies to reduce the cost: (1) adding intersection links between\ntwo indexing nodes, which can better reduce query processing costs while\ncontrolling the number of nodes of the graph index; (2) intersection links are\nadded between two nodes according to the probabilistic distribution calculated\nfor estimating the costs of adding intersection between two nodes; (3)\ncoordinates at one dimension having more resources are split by coordinates at\nanother dimension to balance the number of resources hold by indexing nodes;\nand, (4) short-cut links are added between sibling coordinates of coordinate\ntrees to make an efficient query on linear order coordinates. Analysis and\nexperiments verified the effectiveness of the generated index in supporting\nsubspace aggregation query. This work makes significant contributions to the\ndevelopment of data model based on multi-dimensional classification.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02129v2",
    "published_date": "2025-05-04 14:36:31 UTC",
    "updated_date": "2025-05-09 10:17:34 UTC"
  },
  {
    "arxiv_id": "2505.02121v1",
    "title": "Overview of AI Grading of Physics Olympiad Exams",
    "authors": [
      "Lachlan McGinness"
    ],
    "abstract": "Automatically grading the diverse range of question types in high school\nphysics problem is a challenge that requires automated grading techniques from\ndifferent fields. We report the findings of a Systematic Literature Review of\npotential physics grading techniques. We propose a multi-modal AI grading\nframework to address these challenges and examine our framework in light of\nAustralia's AI Ethical Principles.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "International Conference on Artificial Intelligence in Education,\n  Doctoral Consortium",
    "pdf_url": "http://arxiv.org/pdf/2505.02121v1",
    "published_date": "2025-05-04 14:04:24 UTC",
    "updated_date": "2025-05-04 14:04:24 UTC"
  },
  {
    "arxiv_id": "2505.02120v1",
    "title": "Tricolore: Multi-Behavior User Profiling for Enhanced Candidate Generation in Recommender Systems",
    "authors": [
      "Xiao Zhou",
      "Zhongxiang Zhao",
      "Hanze Guo"
    ],
    "abstract": "Online platforms aggregate extensive user feedback across diverse behaviors,\nproviding a rich source for enhancing user engagement. Traditional recommender\nsystems, however, typically optimize for a single target behavior and represent\nuser preferences with a single vector, limiting their ability to handle\nmultiple important behaviors or optimization objectives. This conventional\napproach also struggles to capture the full spectrum of user interests,\nresulting in a narrow item pool during candidate generation. To address these\nlimitations, we present Tricolore, a versatile multi-vector learning framework\nthat uncovers connections between different behavior types for more robust\ncandidate generation. Tricolore's adaptive multi-task structure is also\ncustomizable to specific platform needs. To manage the variability in sparsity\nacross behavior types, we incorporate a behavior-wise multi-view fusion module\nthat dynamically enhances learning. Moreover, a popularity-balanced strategy\nensures the recommendation list balances accuracy with item popularity,\nfostering diversity and improving overall performance. Extensive experiments on\npublic datasets demonstrate Tricolore's effectiveness across various\nrecommendation scenarios, from short video platforms to e-commerce. By\nleveraging a shared base embedding strategy, Tricolore also significantly\nimproves the performance for cold-start users. The source code is publicly\navailable at: https://github.com/abnering/Tricolore.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02120v1",
    "published_date": "2025-05-04 14:04:22 UTC",
    "updated_date": "2025-05-04 14:04:22 UTC"
  },
  {
    "arxiv_id": "2505.02118v3",
    "title": "Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets",
    "authors": [
      "Wei Liu",
      "Zhongyu Niu",
      "Lang Gao",
      "Zhiying Deng",
      "Jun Wang",
      "Haozhao Wang",
      "Ruixuan Li"
    ],
    "abstract": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.02118v3",
    "published_date": "2025-05-04 14:00:04 UTC",
    "updated_date": "2025-05-11 05:57:46 UTC"
  },
  {
    "arxiv_id": "2505.02110v1",
    "title": "Eterna is Solved",
    "authors": [
      "Tristan Cazenave"
    ],
    "abstract": "RNA design consists of discovering a nucleotide sequence that folds into a\ntarget secondary structure. It is useful for synthetic biology, medicine, and\nnanotechnology. We propose Montparnasse, a Multi Objective Generalized Nested\nRollout Policy Adaptation with Limited Repetition (MOGNRPALR) RNA design\nalgorithm. It solves the Eterna benchmark.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02110v1",
    "published_date": "2025-05-04 13:31:53 UTC",
    "updated_date": "2025-05-04 13:31:53 UTC"
  },
  {
    "arxiv_id": "2505.02872v1",
    "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading",
    "authors": [
      "Cfir Avraham Hadar",
      "Omer Shubi",
      "Yoav Meiri",
      "Yevgeni Berzak"
    ],
    "abstract": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02872v1",
    "published_date": "2025-05-04 13:23:48 UTC",
    "updated_date": "2025-05-04 13:23:48 UTC"
  },
  {
    "arxiv_id": "2505.06262v1",
    "title": "Dialz: A Python Toolkit for Steering Vectors",
    "authors": [
      "Zara Siddique",
      "Liam D. Turner",
      "Luis Espinosa-Anke"
    ],
    "abstract": "We introduce Dialz, a framework for advancing research on steering vectors\nfor open-source LLMs, implemented in Python. Steering vectors allow users to\nmodify activations at inference time to amplify or weaken a 'concept', e.g.\nhonesty or positivity, providing a more powerful alternative to prompting or\nfine-tuning. Dialz supports a diverse set of tasks, including creating\ncontrastive pair datasets, computing and applying steering vectors, and\nvisualizations. Unlike existing libraries, Dialz emphasizes modularity and\nusability, enabling both rapid prototyping and in-depth analysis. We\ndemonstrate how Dialz can be used to reduce harmful outputs such as\nstereotypes, while also providing insights into model behaviour across\ndifferent layers. We release Dialz with full documentation, tutorials, and\nsupport for popular open-source models to encourage further research in safe\nand controllable language generation. Dialz enables faster research cycles and\nfacilitates insights into model interpretability, paving the way for safer,\nmore transparent, and more reliable AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06262v1",
    "published_date": "2025-05-04 13:19:21 UTC",
    "updated_date": "2025-05-04 13:19:21 UTC"
  },
  {
    "arxiv_id": "2505.02099v1",
    "title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents",
    "authors": [
      "Zeyu Zhang",
      "Quanyu Dai",
      "Xu Chen",
      "Rui Li",
      "Zhongyang Li",
      "Zhenhua Dong"
    ],
    "abstract": "Recently, large language model based (LLM-based) agents have been widely\napplied across various fields. As a critical part, their memory capabilities\nhave captured significant interest from both industrial and academic\ncommunities. Despite the proposal of many advanced memory models in recent\nresearch, however, there remains a lack of unified implementations under a\ngeneral framework. To address this issue, we develop a unified and modular\nlibrary for developing advanced memory models of LLM-based agents, called\nMemEngine. Based on our framework, we implement abundant memory models from\nrecent research works. Additionally, our library facilitates convenient and\nextensible memory development, and offers user-friendly and pluggable memory\nusage. For benefiting our community, we have made our project publicly\navailable at https://github.com/nuster1128/MemEngine.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Just accepted by TheWebConf'25 Resource Track",
    "pdf_url": "http://arxiv.org/pdf/2505.02099v1",
    "published_date": "2025-05-04 13:10:44 UTC",
    "updated_date": "2025-05-04 13:10:44 UTC"
  },
  {
    "arxiv_id": "2505.02087v1",
    "title": "Retrieval-augmented in-context learning for multimodal large language models in disease classification",
    "authors": [
      "Zaifu Zhan",
      "Shuang Zhou",
      "Xiaoshan Zhou",
      "Yongkang Xiao",
      "Jun Wang",
      "Jiawen Deng",
      "He Zhu",
      "Yu Hou",
      "Rui Zhang"
    ],
    "abstract": "Objectives: We aim to dynamically retrieve informative demonstrations,\nenhancing in-context learning in multimodal large language models (MLLMs) for\ndisease classification.\n  Methods: We propose a Retrieval-Augmented In-Context Learning (RAICL)\nframework, which integrates retrieval-augmented generation (RAG) and in-context\nlearning (ICL) to adaptively select demonstrations with similar disease\npatterns, enabling more effective ICL in MLLMs. Specifically, RAICL examines\nembeddings from diverse encoders, including ResNet, BERT, BioBERT, and\nClinicalBERT, to retrieve appropriate demonstrations, and constructs\nconversational prompts optimized for ICL. We evaluated the framework on two\nreal-world multi-modal datasets (TCGA and IU Chest X-ray), assessing its\nperformance across multiple MLLMs (Qwen, Llava, Gemma), embedding strategies,\nsimilarity metrics, and varying numbers of demonstrations.\n  Results: RAICL consistently improved classification performance. Accuracy\nincreased from 0.7854 to 0.8368 on TCGA and from 0.7924 to 0.8658 on IU Chest\nX-ray. Multi-modal inputs outperformed single-modal ones, with text-only inputs\nbeing stronger than images alone. The richness of information embedded in each\nmodality will determine which embedding model can be used to get better\nresults. Few-shot experiments showed that increasing the number of retrieved\nexamples further enhanced performance. Across different similarity metrics,\nEuclidean distance achieved the highest accuracy while cosine similarity\nyielded better macro-F1 scores. RAICL demonstrated consistent improvements\nacross various MLLMs, confirming its robustness and versatility.\n  Conclusions: RAICL provides an efficient and scalable approach to enhance\nin-context learning in MLLMs for multimodal disease classification.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 Pages, 1 figure, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.02087v1",
    "published_date": "2025-05-04 12:43:56 UTC",
    "updated_date": "2025-05-04 12:43:56 UTC"
  },
  {
    "arxiv_id": "2505.03833v1",
    "title": "PointExplainer: Towards Transparent Parkinson's Disease Diagnosis",
    "authors": [
      "Xuechao Wang",
      "Sven Nomm",
      "Junqing Huang",
      "Kadri Medijainen",
      "Aaro Toomela",
      "Michael Ruzhansky"
    ],
    "abstract": "Deep neural networks have shown potential in analyzing digitized hand-drawn\nsignals for early diagnosis of Parkinson's disease. However, the lack of clear\ninterpretability in existing diagnostic methods presents a challenge to\nclinical trust. In this paper, we propose PointExplainer, an explainable\ndiagnostic strategy to identify hand-drawn regions that drive model diagnosis.\nSpecifically, PointExplainer assigns discrete attribution values to hand-drawn\nsegments, explicitly quantifying their relative contributions to the model's\ndecision. Its key components include: (i) a diagnosis module, which encodes\nhand-drawn signals into 3D point clouds to represent hand-drawn trajectories,\nand (ii) an explanation module, which trains an interpretable surrogate model\nto approximate the local behavior of the black-box diagnostic model. We also\nintroduce consistency measures to further address the issue of faithfulness in\nexplanations. Extensive experiments on two benchmark datasets and a newly\nconstructed dataset show that PointExplainer can provide intuitive explanations\nwith no diagnostic performance degradation. The source code is available at\nhttps://github.com/chaoxuewang/PointExplainer.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03833v1",
    "published_date": "2025-05-04 12:23:58 UTC",
    "updated_date": "2025-05-04 12:23:58 UTC"
  },
  {
    "arxiv_id": "2505.03832v1",
    "title": "Video Forgery Detection for Surveillance Cameras: A Review",
    "authors": [
      "Noor B. Tayfor",
      "Tarik A. Rashid",
      "Shko M. Qader",
      "Bryar A. Hassan",
      "Mohammed H. Abdalla",
      "Jafar Majidpour",
      "Aram M. Ahmed",
      "Hussein M. Ali",
      "Aso M. Aladdin",
      "Abdulhady A. Abdullah",
      "Ahmed S. Shamsaldin",
      "Haval M. Sidqi",
      "Abdulrahman Salih",
      "Zaher M. Yaseen",
      "Azad A. Ameen",
      "Janmenjoy Nayak",
      "Mahmood Yashar Hamza"
    ],
    "abstract": "The widespread availability of video recording through smartphones and\ndigital devices has made video-based evidence more accessible than ever.\nSurveillance footage plays a crucial role in security, law enforcement, and\njudicial processes. However, with the rise of advanced video editing tools,\ntampering with digital recordings has become increasingly easy, raising\nconcerns about their authenticity. Ensuring the integrity of surveillance\nvideos is essential, as manipulated footage can lead to misinformation and\nundermine judicial decisions. This paper provides a comprehensive review of\nexisting forensic techniques used to detect video forgery, focusing on their\neffectiveness in verifying the authenticity of surveillance recordings. Various\nmethods, including compression-based analysis, frame duplication detection, and\nmachine learning-based approaches, are explored. The findings highlight the\ngrowing necessity for more robust forensic techniques to counteract evolving\nforgery methods. Strengthening video forensic capabilities will ensure that\nsurveillance recordings remain credible and admissible as legal evidence.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.03832v1",
    "published_date": "2025-05-04 12:21:24 UTC",
    "updated_date": "2025-05-04 12:21:24 UTC"
  },
  {
    "arxiv_id": "2505.02078v1",
    "title": "LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning",
    "authors": [
      "Joy Lim Jia Yin",
      "Daniel Zhang-Li",
      "Jifan Yu",
      "Haoxuan Li",
      "Shangqing Tu",
      "Yuanchun Wang",
      "Zhiyuan Liu",
      "Huiqin Liu",
      "Lei Hou",
      "Juanzi Li",
      "Bin Xu"
    ],
    "abstract": "Evaluating the quality of slide-based multimedia instruction is challenging.\nExisting methods like manual assessment, reference-based metrics, and large\nlanguage model evaluators face limitations in scalability, context capture, or\nbias. In this paper, we introduce LecEval, an automated metric grounded in\nMayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal\nknowledge acquisition in slide-based learning. LecEval assesses effectiveness\nusing four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical\nStructure (LS), and Audience Engagement (AE). We curate a large-scale dataset\nof over 2,000 slides from more than 50 online course videos, annotated with\nfine-grained human ratings across these rubrics. A model trained on this\ndataset demonstrates superior accuracy and adaptability compared to existing\nmetrics, bridging the gap between automated and human assessments. We release\nour dataset and toolkits at https://github.com/JoylimJY/LecEval.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.02078v1",
    "published_date": "2025-05-04 12:06:47 UTC",
    "updated_date": "2025-05-04 12:06:47 UTC"
  },
  {
    "arxiv_id": "2505.02077v1",
    "title": "Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents",
    "authors": [
      "Christian Schroeder de Witt"
    ],
    "abstract": "Decentralized AI agents will soon interact across internet platforms,\ncreating security challenges beyond traditional cybersecurity and AI safety\nframeworks. Free-form protocols are essential for AI's task generalization but\nenable new threats like secret collusion and coordinated swarm attacks. Network\neffects can rapidly spread privacy breaches, disinformation, jailbreaks, and\ndata poisoning, while multi-agent dispersion and stealth optimization help\nadversaries evade oversightcreating novel persistent threats at a systemic\nlevel. Despite their critical importance, these security challenges remain\nunderstudied, with research fragmented across disparate fields including AI\nsecurity, multi-agent learning, complex systems, cybersecurity, game theory,\ndistributed systems, and technical AI governance. We introduce\n\\textbf{multi-agent security}, a new field dedicated to securing networks of\ndecentralized AI agents against threats that emerge or amplify through their\ninteractionswhether direct or indirect via shared environmentswith each other,\nhumans, and institutions, and characterize fundamental security-performance\ntrade-offs. Our preliminary work (1) taxonomizes the threat landscape arising\nfrom interacting AI agents, (2) surveys security-performance tradeoffs in\ndecentralized AI systems, and (3) proposes a unified research agenda addressing\nopen challenges in designing secure agent systems and interaction environments.\nBy identifying these gaps, we aim to guide research in this critical area to\nunlock the socioeconomic potential of large-scale agent deployment on the\ninternet, foster public trust, and mitigate national security risks in critical\ninfrastructure and defense contexts.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02077v1",
    "published_date": "2025-05-04 12:03:29 UTC",
    "updated_date": "2025-05-04 12:03:29 UTC"
  },
  {
    "arxiv_id": "2505.02076v1",
    "title": "Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants",
    "authors": [
      "Milapji Singh Gill",
      "Javal Vyas",
      "Artan Markaj",
      "Felix Gehlhoff",
      "Mehmet Mercangöz"
    ],
    "abstract": "Advances in Automation and Artificial Intelligence continue to enhance the\nautonomy of process plants in handling various operational scenarios. However,\ncertain tasks, such as fault handling, remain challenging, as they rely heavily\non human expertise. This highlights the need for systematic, knowledge-based\nmethods. To address this gap, we propose a methodological framework that\nintegrates Large Language Model (LLM) agents with a Digital Twin environment.\nThe LLM agents continuously interpret system states and initiate control\nactions, including responses to unexpected faults, with the goal of returning\nthe system to normal operation. In this context, the Digital Twin acts both as\na structured repository of plant-specific engineering knowledge for agent\nprompting and as a simulation platform for the systematic validation and\nverification of the generated corrective control actions. The evaluation using\na mixing module of a process plant demonstrates that the proposed framework is\ncapable not only of autonomously controlling the mixing module, but also of\ngenerating effective corrective actions to mitigate a pipe clogging with only a\nfew reprompts.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02076v1",
    "published_date": "2025-05-04 12:02:21 UTC",
    "updated_date": "2025-05-04 12:02:21 UTC"
  },
  {
    "arxiv_id": "2505.02075v1",
    "title": "Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation",
    "authors": [
      "Volodymyr Havrylov",
      "Haiwen Huang",
      "Dan Zhang",
      "Andreas Geiger"
    ],
    "abstract": "Vision Foundation Models (VFMs) are large-scale, pre-trained models that\nserve as general-purpose backbones for various computer vision tasks. As VFMs'\npopularity grows, there is an increasing interest in understanding their\neffectiveness for dense prediction tasks. However, VFMs typically produce\nlow-resolution features, limiting their direct applicability in this context.\nOne way to tackle this limitation is by employing a task-agnostic feature\nupsampling module that refines VFM features resolution. To assess the\neffectiveness of this approach, we investigate Interactive Segmentation (IS) as\na novel benchmark for evaluating feature upsampling methods on VFMs. Due to its\ninherent multimodal input, consisting of an image and a set of user-defined\nclicks, as well as its dense mask output, IS creates a challenging environment\nthat demands comprehensive visual scene understanding. Our benchmarking\nexperiments show that selecting appropriate upsampling strategies significantly\nimproves VFM features quality. The code is released at\nhttps://github.com/havrylovv/iSegProbe",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02075v1",
    "published_date": "2025-05-04 11:59:26 UTC",
    "updated_date": "2025-05-04 11:59:26 UTC"
  },
  {
    "arxiv_id": "2505.02073v1",
    "title": "Lightweight Defense Against Adversarial Attacks in Time Series Classification",
    "authors": [
      "Yi Han"
    ],
    "abstract": "As time series classification (TSC) gains prominence, ensuring robust TSC\nmodels against adversarial attacks is crucial. While adversarial defense is\nwell-studied in Computer Vision (CV), the TSC field has primarily relied on\nadversarial training (AT), which is computationally expensive. In this paper,\nfive data augmentation-based defense methods tailored for time series are\ndeveloped, with the most computationally intensive method among them increasing\nthe computational resources by only 14.07% compared to the original TSC model.\nMoreover, the deployment process for these methods is straightforward. By\nleveraging these advantages of our methods, we create two combined methods. One\nof these methods is an ensemble of all the proposed techniques, which not only\nprovides better defense performance than PGD-based AT but also enhances the\ngeneralization ability of TSC models. Moreover, the computational resources\nrequired for our ensemble are less than one-third of those required for\nPGD-based AT. These methods advance robust TSC in data mining. Furthermore, as\nfoundation models are increasingly explored for time series feature learning,\nour work provides insights into integrating data augmentation-based adversarial\ndefense with large-scale pre-trained models in future research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T05, 62H30",
      "I.2.6; I.5.1; G.3"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 8 figures. Accepted at RAFDA Workshop, PAKDD 2025\n  (Springer, EI & Scopus indexed). Code:\n  https://github.com/Yi126/Lightweight-Defence",
    "pdf_url": "http://arxiv.org/pdf/2505.02073v1",
    "published_date": "2025-05-04 11:51:09 UTC",
    "updated_date": "2025-05-04 11:51:09 UTC"
  },
  {
    "arxiv_id": "2505.02072v1",
    "title": "What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction",
    "authors": [
      "Eitan Wagner",
      "Omri Abend"
    ],
    "abstract": "The notion of language modeling has gradually shifted in recent years from a\ndistribution over finite-length strings to general-purpose prediction models\nfor textual inputs and outputs, following appropriate alignment phases. This\npaper analyzes the distinction between distribution estimation and response\nprediction in the context of LLMs, and their often conflicting goals. We\nexamine the training phases of LLMs, which include pretraining, in-context\nlearning, and preference tuning, and also the common use cases for their output\nprobabilities, which include completion probabilities and explicit\nprobabilities as output. We argue that the different settings lead to three\ndistinct intended output distributions. We demonstrate that NLP works often\nassume that these distributions should be similar, which leads to\nmisinterpretations of their experimental findings. Our work sets firmer formal\nfoundations for the interpretation of LLMs, which will inform ongoing work on\nthe interpretation and use of LLMs' induced distributions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02072v1",
    "published_date": "2025-05-04 11:46:48 UTC",
    "updated_date": "2025-05-04 11:46:48 UTC"
  },
  {
    "arxiv_id": "2505.06261v1",
    "title": "Modeling supply chain compliance response strategies based on AI synthetic data with structural path regression: A Simulation Study of EU 2027 Mandatory Labor Regulations",
    "authors": [
      "Wei Meng"
    ],
    "abstract": "In the context of the new mandatory labor compliance in the European Union\n(EU), which will be implemented in 2027, supply chain enterprises face\nstringent working hour management requirements and compliance risks. In order\nto scientifically predict the enterprises' coping behaviors and performance\noutcomes under the policy impact, this paper constructs a methodological\nframework that integrates the AI synthetic data generation mechanism and\nstructural path regression modeling to simulate the enterprises' strategic\ntransition paths under the new regulations. In terms of research methodology,\nthis paper adopts high-quality simulation data generated based on Monte Carlo\nmechanism and NIST synthetic data standards to construct a structural path\nanalysis model that includes multiple linear regression, logistic regression,\nmediation effect and moderating effect. The variable system covers 14\nindicators such as enterprise working hours, compliance investment, response\nspeed, automation level, policy dependence, etc. The variable set with\nexplanatory power is screened out through exploratory data analysis (EDA) and\nVIF multicollinearity elimination. The findings show that compliance investment\nhas a significant positive impact on firm survival and its effect is\ntransmitted through the mediating path of the level of intelligence; meanwhile,\nfirms' dependence on the EU market significantly moderates the strength of this\nmediating effect. It is concluded that AI synthetic data combined with\nstructural path modeling provides an effective tool for high-intensity\nregulatory simulation, which can provide a quantitative basis for corporate\nstrategic response, policy design and AI-assisted decision-making in the\npre-prediction stage lacking real scenario data. Keywords: AI synthetic data,\nstructural path regression modeling, compliance response strategy, EU 2027\nmandatory labor regulation",
    "categories": [
      "cs.CY",
      "cs.AI",
      "stat.AP",
      "90B06 (Primary) 62J05, 91B74 (Secondary)",
      "I.6.3; I.2.6; J.1"
    ],
    "primary_category": "cs.CY",
    "comment": "Simulated data modeling of the impact of non-tariff barriers in trade\n  wars",
    "pdf_url": "http://arxiv.org/pdf/2505.06261v1",
    "published_date": "2025-05-04 11:39:56 UTC",
    "updated_date": "2025-05-04 11:39:56 UTC"
  },
  {
    "arxiv_id": "2505.02062v1",
    "title": "Ethical AI in the Healthcare Sector: Investigating Key Drivers of Adoption through the Multi-Dimensional Ethical AI Adoption Model (MEAAM)",
    "authors": [
      "Prathamesh Muzumdar",
      "Apoorva Muley",
      "Kuldeep Singh",
      "Sumanth Cheemalapati"
    ],
    "abstract": "The adoption of Artificial Intelligence (AI) in the healthcare service\nindustry presents numerous ethical challenges, yet current frameworks often\nfail to offer a comprehensive, empirical understanding of the multidimensional\nfactors influencing ethical AI integration. Addressing this critical research\ngap, this study introduces the Multi-Dimensional Ethical AI Adoption Model\n(MEAAM), a novel theoretical framework that categorizes 13 critical ethical\nvariables across four foundational dimensions of Ethical AI Fair AI,\nResponsible AI, Explainable AI, and Sustainable AI. These dimensions are\nfurther analyzed through three core ethical lenses: epistemic concerns (related\nto knowledge, transparency, and system trustworthiness), normative concerns\n(focused on justice, autonomy, dignity, and moral obligations), and overarching\nconcerns (highlighting global, systemic, and long-term ethical implications).\nThis study adopts a quantitative, cross-sectional research design using survey\ndata collected from healthcare professionals and analyzed via Partial Least\nSquares Structural Equation Modeling (PLS-SEM). Employing PLS-SEM, this study\nempirically investigates the influence of these ethical constructs on two\noutcomes Operational AI Adoption and Systemic AI Adoption. Results indicate\nthat normative concerns most significantly drive operational adoption\ndecisions, while overarching concerns predominantly shape systemic adoption\nstrategies and governance frameworks. Epistemic concerns play a facilitative\nrole, enhancing the impact of ethical design principles on trust and\ntransparency in AI systems. By validating the MEAAM framework, this research\nadvances a holistic, actionable approach to ethical AI adoption in healthcare\nand provides critical insights for policymakers, technologists, and healthcare\nadministrators striving to implement ethically grounded AI solutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02062v1",
    "published_date": "2025-05-04 10:40:05 UTC",
    "updated_date": "2025-05-04 10:40:05 UTC"
  },
  {
    "arxiv_id": "2505.02052v1",
    "title": "TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition",
    "authors": [
      "Lala Shakti Swarup Ray",
      "Lars Krupp",
      "Vitor Fortes Rey",
      "Bo Zhou",
      "Sungho Suh",
      "Paul Lukowicz"
    ],
    "abstract": "Sensor-based human activity recognition (HAR) has predominantly focused on\nInertial Measurement Units and vision data, often overlooking the capabilities\nunique to pressure sensors, which capture subtle body dynamics and shifts in\nthe center of mass. Despite their potential for postural and balance-based\nactivities, pressure sensors remain underutilized in the HAR domain due to\nlimited datasets. To bridge this gap, we propose to exploit generative\nfoundation models with pressure-specific HAR techniques. Specifically, we\npresent a bidirectional Text$\\times$Pressure model that uses generative\nfoundation models to interpret pressure data as natural language. TxP\naccomplishes two tasks: (1) Text2Pressure, converting activity text\ndescriptions into pressure sequences, and (2) Pressure2Text, generating\nactivity descriptions and classifications from dynamic pressure maps.\nLeveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on\nour synthetic PressLang dataset, containing over 81,100 text-pressure pairs.\nValidated on real-world data for activities such as yoga and daily tasks, TxP\nprovides novel approaches to data augmentation and classification grounded in\natomic actions. This consequently improved HAR performance by up to 12.4\\% in\nmacro F1 score compared to the state-of-the-art, advancing pressure-based HAR\nwith broader applications and deeper insights into human movement.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02052v1",
    "published_date": "2025-05-04 10:07:38 UTC",
    "updated_date": "2025-05-04 10:07:38 UTC"
  },
  {
    "arxiv_id": "2505.02050v1",
    "title": "Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks",
    "authors": [
      "Kranthi Kumar Talluri",
      "Anders L. Madsen",
      "Galia Weidl"
    ],
    "abstract": "Cut-in maneuvers in high-speed traffic pose critical challenges that can lead\nto abrupt braking and collisions, necessitating safe and efficient lane change\nstrategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate\nlateral evidence with safety assessment models, thereby predicting lane changes\nand ensuring safe cut-in maneuvers effectively. Our proposed framework\ncomprises three key probabilistic hypotheses (lateral evidence, lateral safety,\nand longitudinal safety) that facilitate the decision-making process through\ndynamic data processing and assessments of vehicle positions, lateral\nvelocities, relative distance, and Time-to-Collision (TTC) computations. The\nDBN model's performance compared with other conventional approaches\ndemonstrates superior performance in crash reduction, especially in critical\nhigh-speed scenarios, while maintaining a competitive performance in low-speed\nscenarios. This paves the way for robust, scalable, and efficient safety\nvalidation in automated driving systems.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02050v1",
    "published_date": "2025-05-04 09:58:02 UTC",
    "updated_date": "2025-05-04 09:58:02 UTC"
  },
  {
    "arxiv_id": "2505.02048v2",
    "title": "Regression is all you need for medical image translation",
    "authors": [
      "Sebastian Rassmann",
      "David Kügler",
      "Christian Ewert",
      "Martin Reuter"
    ],
    "abstract": "The acquisition of information-rich images within a limited time budget is\ncrucial in medical imaging. Medical image translation (MIT) can help enhance\nand supplement existing datasets by generating synthetic images from acquired\ndata. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have\nachieved remarkable success in natural image generation, their benefits -\ncreativity and image realism - do not necessarily transfer to medical\napplications where highly accurate anatomical information is required. In fact,\nthe imitation of acquisition noise or content hallucination hinder clinical\nutility. Here, we introduce YODA (You Only Denoise once - or Average), a novel\n2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and\nregression paradigms to produce realistic or noise-free outputs. Furthermore,\nwe propose Expectation-Approximation (ExpA) DM sampling, which draws\ninspiration from MRI signal averaging. ExpA-sampling suppresses generated noise\nand, thus, eliminates noise from biasing the evaluation of image quality.\nThrough extensive experiments on four diverse multi-modal datasets - comprising\nmulti-contrast brain MRI and pelvic MRI-CT - we show that diffusion and\nregression sampling yield similar results in practice. As such, the\ncomputational overhead of diffusion sampling does not provide systematic\nbenefits in medical information translation. Building on these insights, we\ndemonstrate that YODA outperforms several state-of-the-art GAN and DM methods.\nNotably, YODA-generated images are shown to be interchangeable with, or even\nsuperior to, physical acquisitions for several downstream tasks. Our findings\nchallenge the presumed advantages of DMs in MIT and pave the way for the\npractical application of MIT in medical imaging.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02048v2",
    "published_date": "2025-05-04 09:57:10 UTC",
    "updated_date": "2025-05-06 05:56:47 UTC"
  },
  {
    "arxiv_id": "2505.02027v1",
    "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning",
    "authors": [
      "Rui Lv",
      "Zaixi Zhang",
      "Kai Zhang",
      "Qi Liu",
      "Weibo Gao",
      "Jiawei Liu",
      "Jiaxia Yan",
      "Linan Yue",
      "Fangzhou Yao"
    ],
    "abstract": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted",
    "pdf_url": "http://arxiv.org/pdf/2505.02027v1",
    "published_date": "2025-05-04 08:30:00 UTC",
    "updated_date": "2025-05-04 08:30:00 UTC"
  },
  {
    "arxiv_id": "2505.02024v1",
    "title": "From Mind to Machine: The Rise of Manus AI as a Fully Autonomous Digital Agent",
    "authors": [
      "Minjie Shen",
      "Qikai Yang"
    ],
    "abstract": "Manus AI is a general-purpose AI agent introduced in early 2025, marking a\nsignificant advancement in autonomous artificial intelligence. Developed by the\nChinese startup Monica.im, Manus is designed to bridge the gap between \"mind\"\nand \"hand\" - combining the reasoning and planning capabilities of large\nlanguage models with the ability to execute complex, end-to-end tasks that\nproduce tangible outcomes. This paper presents a comprehensive overview of\nManus AI, exploring its core technical architecture, diverse applications\nacross sectors such as healthcare, finance, manufacturing, robotics, and\ngaming, as well as its key strengths, current limitations, and future\npotential. Positioned as a preview of what lies ahead, Manus AI represents a\nshift toward intelligent agents that can translate high-level intentions into\nreal-world actions, heralding a new era of human-AI collaboration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02024v1",
    "published_date": "2025-05-04 08:24:00 UTC",
    "updated_date": "2025-05-04 08:24:00 UTC"
  },
  {
    "arxiv_id": "2505.02020v1",
    "title": "Wide & Deep Learning for Node Classification",
    "authors": [
      "Yancheng Chen",
      "Wenguo Yang",
      "Zhipeng Jiang"
    ],
    "abstract": "Wide & Deep, a simple yet effective learning architecture for recommendation\nsystems developed by Google, has had a significant impact in both academia and\nindustry due to its combination of the memorization ability of generalized\nlinear models and the generalization ability of deep models. Graph\nconvolutional networks (GCNs) remain dominant in node classification tasks;\nhowever, recent studies have highlighted issues such as heterophily and\nexpressiveness, which focus on graph structure while seemingly neglecting the\npotential role of node features. In this paper, we propose a flexible framework\nGCNIII, which leverages the Wide & Deep architecture and incorporates three\ntechniques: Intersect memory, Initial residual and Identity mapping. We provide\ncomprehensive empirical evidence showing that GCNIII can more effectively\nbalance the trade-off between over-fitting and over-generalization on various\nsemi- and full- supervised tasks. Additionally, we explore the use of large\nlanguage models (LLMs) for node feature engineering to enhance the performance\nof GCNIII in cross-domain node classification tasks. Our implementation is\navailable at https://github.com/CYCUCAS/GCNIII.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 6 figures, 13 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.02020v1",
    "published_date": "2025-05-04 07:53:16 UTC",
    "updated_date": "2025-05-04 07:53:16 UTC"
  },
  {
    "arxiv_id": "2505.02011v1",
    "title": "CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting",
    "authors": [
      "Minhyuk Lee",
      "HyeKyung Yoon",
      "MyungJoo Kang"
    ],
    "abstract": "Multivariate long-term time series forecasting is critical for applications\nsuch as weather prediction, and traffic analysis. In addition, the\nimplementation of Transformer variants has improved prediction accuracy.\nFollowing these variants, different input data process approaches also enhanced\nthe field, such as tokenization techniques including point-wise, channel-wise,\nand patch-wise tokenization. However, previous studies still have limitations\nin time complexity, computational resources, and cross-dimensional\ninteractions. To address these limitations, we introduce a novel CNN\nAutoencoder-based Score Attention mechanism (CASA), which can be introduced in\ndiverse Transformers model-agnosticically by reducing memory and leading to\nimprovement in model performance. Experiments on eight real-world datasets\nvalidate that CASA decreases computational resources by up to 77.7%,\naccelerates inference by 44.0%, and achieves state-of-the-art performance,\nranking first in 87.5% of evaluated metrics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02011v1",
    "published_date": "2025-05-04 06:46:21 UTC",
    "updated_date": "2025-05-04 06:46:21 UTC"
  },
  {
    "arxiv_id": "2505.02003v1",
    "title": "Closed-loop control of seizure activity via real-time seizure forecasting by reservoir neuromorphic computing",
    "authors": [
      "Maryam Sadeghi",
      "Darío Fernández Khatiboun",
      "Yasser Rezaeiyan",
      "Saima Rizwan",
      "Alessandro Barcellona",
      "Andrea Merello",
      "Marco Crepaldi",
      "Gabriella Panuccio",
      "Farshad Moradi"
    ],
    "abstract": "Closed-loop brain stimulation holds potential as personalized treatment for\ndrug-resistant epilepsy (DRE) but still suffers from limitations that result in\nhighly variable efficacy. First, stimulation is typically delivered upon\ndetection of the seizure to abort rather than prevent it; second, the\nstimulation parameters are established by trial and error, requiring lengthy\nrounds of fine-tuning, which delay steady-state therapeutic efficacy. Here, we\naddress these limitations by leveraging the potential of neuromorphic\ncomputing. We present a system capable of driving personalized free-run\nstimulations based on seizure forecasting, wherein each forecast triggers an\nelectrical pulse rather than an arbitrarily predefined fixed-frequency stimulus\ntrain. We validate the system against hippocampal spheroids coupled to 3D\nmicroelectrode array as a simplified testbed, showing that it can achieve\nseizure reduction >97% while primarily using instantaneous stimulation\nfrequencies within 20 Hz, well below what typically used in clinical settings.\nOur work demonstrates the potential of neuromorphic systems as a\nnext-generation neuromodulation strategy for personalized DRE treatment.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.02003v1",
    "published_date": "2025-05-04 06:17:23 UTC",
    "updated_date": "2025-05-04 06:17:23 UTC"
  },
  {
    "arxiv_id": "2505.01998v2",
    "title": "A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction",
    "authors": [
      "Xiaoliang Chen",
      "Xin Yu",
      "Le Chang",
      "Yunhe Huang",
      "Jiashuai He",
      "Shibo Zhang",
      "Jin Li",
      "Likai Lin",
      "Ziyu Zeng",
      "Xianling Tu",
      "Shuyu Zhang"
    ],
    "abstract": "This paper introduces a novel framework integrating nonlinear acoustic\ncomputing and reinforcement learning to enhance advanced human-robot\ninteraction under complex noise and reverberation. Leveraging physically\ninformed wave equations (e.g., Westervelt, KZK), the approach captures\nhigher-order phenomena such as harmonic generation and shock formation. By\nembedding these models in a reinforcement learning-driven control loop, the\nsystem adaptively optimizes key parameters (e.g., absorption, beamforming) to\nmitigate multipath interference and non-stationary noise. Experimental\nevaluations, covering far-field localization, weak signal detection, and\nmultilingual speech recognition, demonstrate that this hybrid strategy\nsurpasses traditional linear methods and purely data-driven baselines,\nachieving superior noise suppression, minimal latency, and robust accuracy in\ndemanding real-world scenarios. The proposed system demonstrates broad\napplication prospects in AI hardware, robot, machine audition, artificial\naudition, and brain-machine interfaces.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "physics.app-ph",
      "68T01",
      "I.2.8"
    ],
    "primary_category": "cs.RO",
    "comment": "34 pages, 11 figures, 10 tables, and 10 equations",
    "pdf_url": "http://arxiv.org/pdf/2505.01998v2",
    "published_date": "2025-05-04 06:03:12 UTC",
    "updated_date": "2025-05-06 16:09:59 UTC"
  },
  {
    "arxiv_id": "2505.01997v1",
    "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
    "authors": [
      "Jiancong Xiao",
      "Bojian Hou",
      "Zhanliang Wang",
      "Ruochen Jin",
      "Qi Long",
      "Weijie J. Su",
      "Li Shen"
    ],
    "abstract": "One of the key technologies for the success of Large Language Models (LLMs)\nis preference alignment. However, a notable side effect of preference alignment\nis poor calibration: while the pre-trained models are typically\nwell-calibrated, LLMs tend to become poorly calibrated after alignment with\nhuman preferences. In this paper, we investigate why preference alignment\naffects calibration and how to address this issue. For the first question, we\nobserve that the preference collapse issue in alignment undesirably generalizes\nto the calibration scenario, causing LLMs to exhibit overconfidence and poor\ncalibration. To address this, we demonstrate the importance of fine-tuning with\ndomain-specific knowledge to alleviate the overconfidence issue. To further\nanalyze whether this affects the model's performance, we categorize models into\ntwo regimes: calibratable and non-calibratable, defined by bounds of Expected\nCalibration Error (ECE). In the calibratable regime, we propose a\ncalibration-aware fine-tuning approach to achieve proper calibration without\ncompromising LLMs' performance. However, as models are further fine-tuned for\nbetter performance, they enter the non-calibratable regime. For this case, we\ndevelop an EM-algorithm-based ECE regularization for the fine-tuning loss to\nmaintain low calibration error. Extensive experiments validate the\neffectiveness of the proposed methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01997v1",
    "published_date": "2025-05-04 05:42:51 UTC",
    "updated_date": "2025-05-04 05:42:51 UTC"
  },
  {
    "arxiv_id": "2505.01967v1",
    "title": "Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview",
    "authors": [
      "Jiatao Li",
      "Yanheng Li",
      "Xiaojun Wan"
    ],
    "abstract": "Large Language Models (LLMs) have become integral to daily life, widely\nadopted in communication, decision-making, and information retrieval, raising\ncritical questions about how these systems implicitly form and express\nsocio-cognitive attitudes or \"worldviews\". While existing research extensively\naddresses demographic and ethical biases, broader dimensions-such as attitudes\ntoward authority, equality, autonomy, and fate-remain under-explored. In this\npaper, we introduce the Social Worldview Taxonomy (SWT), a structured framework\ngrounded in Cultural Theory, operationalizing four canonical worldviews\n(Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable\nsub-dimensions. Using SWT, we empirically identify distinct and interpretable\ncognitive profiles across 28 diverse LLMs. Further, inspired by Social\nReferencing Theory, we experimentally demonstrate that explicit social cues\nsystematically shape these cognitive attitudes, revealing both general response\npatterns and nuanced model-specific variations. Our findings enhance the\ninterpretability of LLMs by revealing implicit socio-cognitive biases and their\nresponsiveness to social feedback, thus guiding the development of more\ntransparent and socially responsible language technologies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01967v1",
    "published_date": "2025-05-04 02:35:24 UTC",
    "updated_date": "2025-05-04 02:35:24 UTC"
  },
  {
    "arxiv_id": "2505.01966v1",
    "title": "A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites",
    "authors": [
      "Bofei Liu",
      "Dong Ye",
      "Zunhao Yao",
      "Zhaowei Sun"
    ],
    "abstract": "Modular self-reconfigurable satellites refer to satellite clusters composed\nof individual modular units capable of altering their configurations. The\nconfiguration changes enable the execution of diverse tasks and mission\nobjectives. Existing path planning algorithms for reconfiguration often suffer\nfrom high computational complexity, poor generalization capability, and limited\nsupport for diverse target configurations. To address these challenges, this\npaper proposes a goal-oriented reinforcement learning-based path planning\nalgorithm. This algorithm is the first to address the challenge that previous\nreinforcement learning methods failed to overcome, namely handling multiple\ntarget configurations. Moreover, techniques such as Hindsight Experience Replay\nand Invalid Action Masking are incorporated to overcome the significant\nobstacles posed by sparse rewards and invalid actions. Based on these designs,\nour model achieves a 95% and 73% success rate in reaching arbitrary target\nconfigurations in a modular satellite cluster composed of four and six units,\nrespectively.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.01966v1",
    "published_date": "2025-05-04 02:35:18 UTC",
    "updated_date": "2025-05-04 02:35:18 UTC"
  },
  {
    "arxiv_id": "2505.01956v2",
    "title": "SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment",
    "authors": [
      "Ganesh Sapkota",
      "Sanjay Madria"
    ],
    "abstract": "In battlefield environments, adversaries frequently disrupt GPS signals,\nrequiring alternative localization and navigation methods. Traditional\nvision-based approaches like Simultaneous Localization and Mapping (SLAM) and\nVisual Odometry (VO) involve complex sensor fusion and high computational\ndemand, whereas range-free methods like DV-HOP face accuracy and stability\nchallenges in sparse, dynamic networks. This paper proposes LanBLoc-BMM, a\nnavigation approach using landmark-based localization (LanBLoc) combined with a\nbattlefield-specific motion model (BMM) and Extended Kalman Filter (EKF). Its\nperformance is benchmarked against three state-of-the-art visual localization\nalgorithms integrated with BMM and Bayesian filters, evaluated on synthetic and\nreal-imitated trajectory datasets using metrics including Average Displacement\nError (ADE), Final Displacement Error (FDE), and a newly introduced Average\nWeighted Risk Score (AWRS). LanBLoc-BMM (with EKF) demonstrates superior\nperformance in ADE, FDE, and AWRS on real-imitated datasets. Additionally, two\nsafe navigation methods, SafeNav-CHull and SafeNav-Centroid, are introduced by\nintegrating LanBLoc-BMM(EKF) with a novel Risk-Aware RRT* (RAw-RRT*) algorithm\nfor obstacle avoidance and risk exposure minimization. Simulation results in\nbattlefield scenarios indicate SafeNav-Centroid excels in accuracy, risk\nexposure, and trajectory efficiency, while SafeNav-CHull provides superior\ncomputational speed.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages, conference paper. arXiv admin note: text overlap with\n  arXiv:2402.14280",
    "pdf_url": "http://arxiv.org/pdf/2505.01956v2",
    "published_date": "2025-05-04 01:40:31 UTC",
    "updated_date": "2025-05-13 21:56:50 UTC"
  },
  {
    "arxiv_id": "2505.01955v1",
    "title": "Generative AI in clinical practice: novel qualitative evidence of risk and responsible use of Google's NotebookLM",
    "authors": [
      "Max Reuter",
      "Maura Philippone",
      "Bond Benton",
      "Laura Dilley"
    ],
    "abstract": "The advent of generative artificial intelligence, especially large language\nmodels (LLMs), presents opportunities for innovation in research, clinical\npractice, and education. Recently, Dihan et al. lauded LLM tool NotebookLM's\npotential, including for generating AI-voiced podcasts to educate patients\nabout treatment and rehabilitation, and for quickly synthesizing medical\nliterature for professionals. We argue that NotebookLM presently poses clinical\nand technological risks that should be tested and considered prior to its\nimplementation in clinical practice.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Eye (2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.01955v1",
    "published_date": "2025-05-04 01:25:33 UTC",
    "updated_date": "2025-05-04 01:25:33 UTC"
  },
  {
    "arxiv_id": "2505.01953v1",
    "title": "Training Environment for High Performance Reinforcement Learning",
    "authors": [
      "Greg Search"
    ],
    "abstract": "This paper presents Tunnel, a simple, open source, reinforcement learning\ntraining environment for high performance aircraft. It integrates the F16 3D\nnonlinear flight dynamics into OpenAI Gymnasium python package. The template\nincludes primitives for boundaries, targets, adversaries and sensing\ncapabilities that may vary depending on operational need. This offers mission\nplanners a means to rapidly respond to evolving environments, sensor\ncapabilities and adversaries for autonomous air combat aircraft. It offers\nresearchers access to operationally relevant aircraft physics. Tunnel code base\nis accessible to anyone familiar with Gymnasium and/or those with basic python\nskills. This paper includes a demonstration of a week long trade study that\ninvestigated a variety of training methods, observation spaces, and threat\npresentations. This enables increased collaboration between researchers and\nmission planners which can translate to a national military advantage. As\nwarfare becomes increasingly reliant upon automation, software agility will\ncorrelate with decision advantages. Airmen must have tools to adapt to\nadversaries in this context. It may take months for researchers to develop\nskills to customize observation, actions, tasks and training methodologies in\nair combat simulators. In Tunnel, this can be done in a matter of days.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.01953v1",
    "published_date": "2025-05-04 01:09:15 UTC",
    "updated_date": "2025-05-04 01:09:15 UTC"
  },
  {
    "arxiv_id": "2505.01950v1",
    "title": "Segment Any RGB-Thermal Model with Language-aided Distillation",
    "authors": [
      "Dong Xing",
      "Xianxun Zhu",
      "Wei Zhou",
      "Qika Lin",
      "Hang Yang",
      "Yuqing Wang"
    ],
    "abstract": "The recent Segment Anything Model (SAM) demonstrates strong instance\nsegmentation performance across various downstream tasks. However, SAM is\ntrained solely on RGB data, limiting its direct applicability to RGB-thermal\n(RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for\nscene understanding in adverse weather and lighting conditions, such as low\nlight and overexposure, we propose a novel framework, SARTM, which customizes\nthe powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash\nthe potential of SAM while introduce semantic understanding modules for RGB-T\ndata pairs. Specifically, our framework first involves fine tuning the original\nSAM by adding extra LoRA layers, aiming at preserving SAM's strong\ngeneralization and segmentation capabilities for downstream tasks. Secondly, we\nintroduce language information as guidance for training our SARTM. To address\ncross-modal inconsistencies, we introduce a Cross-Modal Knowledge\nDistillation(CMKD) module that effectively achieves modality adaptation while\nmaintaining its generalization capabilities. This semantic module enables the\nminimization of modality gaps and alleviates semantic ambiguity, facilitating\nthe combination of any modality under any visual conditions. Furthermore, we\nenhance the segmentation performance by adjusting the segmentation head of SAM\nand incorporating an auxiliary semantic segmentation head, which integrates\nmulti-scale features for effective fusion. Extensive experiments are conducted\nacross three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900,\nand FMB. Both quantitative and qualitative results consistently demonstrate\nthat the proposed SARTM significantly outperforms state-of-the-art approaches\nacross a variety of conditions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2412.04220 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2505.01950v1",
    "published_date": "2025-05-04 00:24:17 UTC",
    "updated_date": "2025-05-04 00:24:17 UTC"
  }
]
[
  {
    "arxiv_id": "2407.16092v1",
    "title": "Faster Optimal Coalition Structure Generation via Offline Coalition Selection and Graph-Based Search",
    "authors": [
      "Redha Taguelmimt",
      "Samir Aknine",
      "Djamila Boukredera",
      "Narayan Changder",
      "Tuomas Sandholm"
    ],
    "abstract": "Coalition formation is a key capability in multi-agent systems. An important\nproblem in coalition formation is coalition structure generation: partitioning\nagents into coalitions to optimize the social welfare. This is a challenging\nproblem that has been the subject of active research for the past three\ndecades. In this paper, we present a novel algorithm, SMART, for the problem\nbased on a hybridization of three innovative techniques. Two of these\ntechniques are based on dynamic programming, where we show a powerful\nconnection between the coalitions selected for evaluation and the performance\nof the algorithms. These algorithms use offline phases to optimize the choice\nof coalitions to evaluate. The third one uses branch-and-bound and integer\npartition graph search to explore the solution space. Our techniques bring a\nnew way of approaching the problem and a new level of precision to the field.\nIn experiments over several common value distributions, we show that the\nhybridization of these techniques in SMART is faster than the fastest prior\nalgorithms (ODP-IP, BOSS) in generating optimal solutions across all the value\ndistributions.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT",
      "I.2; F.2"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16092v1",
    "published_date": "2024-07-22 23:24:03 UTC",
    "updated_date": "2024-07-22 23:24:03 UTC"
  },
  {
    "arxiv_id": "2407.16077v1",
    "title": "Modelling brain connectomes networks: Solv is a worthy competitor to hyperbolic geometry!",
    "authors": [
      "Dorota Celińska-Kopczyńska",
      "Eryk Kopczyński"
    ],
    "abstract": "Finding suitable embeddings for connectomes (spatially embedded complex\nnetworks that map neural connections in the brain) is crucial for analyzing and\nunderstanding cognitive processes. Recent studies have found two-dimensional\nhyperbolic embeddings superior to Euclidean embeddings in modeling connectomes\nacross species, especially human connectomes. However, those studies had\nlimitations: geometries other than Euclidean, hyperbolic, or spherical were not\nconsidered. Following William Thurston's suggestion that the networks of\nneurons in the brain could be successfully represented in Solv geometry, we\nstudy the goodness-of-fit of the embeddings for 21 connectome networks (8\nspecies). To this end, we suggest an embedding algorithm based on Simulating\nAnnealing that allows us to embed connectomes to Euclidean, Spherical,\nHyperbolic, Solv, Nil, and product geometries. Our algorithm tends to find\nbetter embeddings than the state-of-the-art, even in the hyperbolic case. Our\nfindings suggest that while three-dimensional hyperbolic embeddings yield the\nbest results in many cases, Solv embeddings perform reasonably well.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "math.MG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "Full version of our paper accepted to ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.16077v1",
    "published_date": "2024-07-22 22:36:04 UTC",
    "updated_date": "2024-07-22 22:36:04 UTC"
  },
  {
    "arxiv_id": "2407.16067v1",
    "title": "LCA-on-the-Line: Benchmarking Out-of-Distribution Generalization with Class Taxonomies",
    "authors": [
      "Jia Shi",
      "Gautam Gare",
      "Jinjin Tian",
      "Siqi Chai",
      "Zhiqiu Lin",
      "Arun Vasudevan",
      "Di Feng",
      "Francesco Ferroni",
      "Shu Kong"
    ],
    "abstract": "We tackle the challenge of predicting models' Out-of-Distribution (OOD)\nperformance using in-distribution (ID) measurements without requiring OOD data.\nExisting evaluations with \"Effective Robustness\", which use ID accuracy as an\nindicator of OOD accuracy, encounter limitations when models are trained with\ndiverse supervision and distributions, such as class labels (Vision Models,\nVMs, on ImageNet) and textual descriptions (Visual-Language Models, VLMs, on\nLAION). VLMs often generalize better to OOD data than VMs despite having\nsimilar or lower ID performance. To improve the prediction of models' OOD\nperformance from ID measurements, we introduce the Lowest Common Ancestor\n(LCA)-on-the-Line framework. This approach revisits the established concept of\nLCA distance, which measures the hierarchical distance between labels and\npredictions within a predefined class hierarchy, such as WordNet. We assess 75\nmodels using ImageNet as the ID dataset and five significantly shifted OOD\nvariants, uncovering a strong linear correlation between ID LCA distance and\nOOD top-1 accuracy. Our method provides a compelling alternative for\nunderstanding why VLMs tend to generalize better. Additionally, we propose a\ntechnique to construct a taxonomic hierarchy on any dataset using K-means\nclustering, demonstrating that LCA distance is robust to the constructed\ntaxonomic hierarchy. Moreover, we demonstrate that aligning model predictions\nwith class taxonomies, through soft labels or prompt engineering, can enhance\nmodel generalization. Open source code in our Project Page:\nhttps://elvishelvis.github.io/papers/lca/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024 Oral Presentation; Project Page:\n  https://elvishelvis.github.io/papers/lca/",
    "pdf_url": "http://arxiv.org/pdf/2407.16067v1",
    "published_date": "2024-07-22 21:54:19 UTC",
    "updated_date": "2024-07-22 21:54:19 UTC"
  },
  {
    "arxiv_id": "2407.16062v1",
    "title": "Artificial Intelligence-based Decision Support Systems for Precision and Digital Health",
    "authors": [
      "Nina Deliu",
      "Bibhas Chakraborty"
    ],
    "abstract": "Precision health, increasingly supported by digital technologies, is a domain\nof research that broadens the paradigm of precision medicine, advancing\neveryday healthcare. This vision goes hand in hand with the groundbreaking\nadvent of artificial intelligence (AI), which is reshaping the way we diagnose,\ntreat, and monitor both clinical subjects and the general population. AI tools\npowered by machine learning have shown considerable improvements in a variety\nof healthcare domains. In particular, reinforcement learning (RL) holds great\npromise for sequential and dynamic problems such as dynamic treatment regimes\nand just-in-time adaptive interventions in digital health. In this work, we\ndiscuss the opportunity offered by AI, more specifically RL, to current trends\nin healthcare, providing a methodological survey of RL methods in the context\nof precision and digital health. Focusing on the area of adaptive\ninterventions, we expand the methodological survey with illustrative case\nstudies that used RL in real practice.\n  This invited article has undergone anonymous review and is intended as a book\nchapter for the volume \"Frontiers of Statistics and Data Science\" edited by\nSubhashis Ghoshal and Anindya Roy for the International Indian Statistical\nAssociation Series on Statistics and Data Science, published by Springer. It\ncovers the material from a short course titled \"Artificial Intelligence in\nPrecision and Digital Health\" taught by the author Bibhas Chakraborty at the\nIISA 2022 Conference, December 26-30 2022, at the Indian Institute of Science,\nBengaluru.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "This contribution is a part of the volume \"Frontiers of Statistics\n  and Data Science\" edited by Subhashis Ghoshal and Anindya Roy for the\n  International Indian Statistical Association Series on Statistics and Data\n  Science, published by Springer. It covers the material from a short course\n  titled \"Artificial Intelligence in Precision and Digital Health\" (IISA 2022\n  Conference) and includes parts of arXiv:2203.02605",
    "pdf_url": "http://arxiv.org/pdf/2407.16062v1",
    "published_date": "2024-07-22 21:39:34 UTC",
    "updated_date": "2024-07-22 21:39:34 UTC"
  },
  {
    "arxiv_id": "2407.20256v1",
    "title": "Making LLMs Work for Enterprise Data Tasks",
    "authors": [
      "Çağatay Demiralp",
      "Fabian Wenz",
      "Peter Baile Chen",
      "Moe Kayali",
      "Nesime Tatbul",
      "Michael Stonebraker"
    ],
    "abstract": "Large language models (LLMs) know little about enterprise database tables in\nthe private data ecosystem, which substantially differ from web text in\nstructure and content. As LLMs' performance is tied to their training data, a\ncrucial question is how useful they can be in improving enterprise database\nmanagement and analysis tasks. To address this, we contribute experimental\nresults on LLMs' performance for text-to-SQL and semantic column-type detection\ntasks on enterprise datasets. The performance of LLMs on enterprise data is\nsignificantly lower than on benchmark datasets commonly used. Informed by our\nfindings and feedback from industry practitioners, we identify three\nfundamental challenges -- latency, cost, and quality -- and propose potential\nsolutions to use LLMs in enterprise data workflows effectively.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "Poster at North East Database Day 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.20256v1",
    "published_date": "2024-07-22 21:16:59 UTC",
    "updated_date": "2024-07-22 21:16:59 UTC"
  },
  {
    "arxiv_id": "2407.16047v1",
    "title": "Leveraging Large Language Models to Geolocate Linguistic Variations in Social Media Posts",
    "authors": [
      "Davide Savarro",
      "Davide Zago",
      "Stefano Zoia"
    ],
    "abstract": "Geolocalization of social media content is the task of determining the\ngeographical location of a user based on textual data, that may show linguistic\nvariations and informal language. In this project, we address the GeoLingIt\nchallenge of geolocalizing tweets written in Italian by leveraging large\nlanguage models (LLMs). GeoLingIt requires the prediction of both the region\nand the precise coordinates of the tweet. Our approach involves fine-tuning\npre-trained LLMs to simultaneously predict these geolocalization aspects. By\nintegrating innovative methodologies, we enhance the models' ability to\nunderstand the nuances of Italian social media text to improve the\nstate-of-the-art in this domain. This work is conducted as part of the Large\nLanguage Models course at the Bertinoro International Spring School 2024. We\nmake our code publicly available on GitHub\nhttps://github.com/dawoz/geolingit-biss2024.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16047v1",
    "published_date": "2024-07-22 20:54:35 UTC",
    "updated_date": "2024-07-22 20:54:35 UTC"
  },
  {
    "arxiv_id": "2407.16040v2",
    "title": "Generalizing Teacher Networks for Effective Knowledge Distillation Across Student Architectures",
    "authors": [
      "Kuluhan Binici",
      "Weiming Wu",
      "Tulika Mitra"
    ],
    "abstract": "Knowledge distillation (KD) is a model compression method that entails\ntraining a compact student model to emulate the performance of a more complex\nteacher model. However, the architectural capacity gap between the two models\nlimits the effectiveness of knowledge transfer. Addressing this issue, previous\nworks focused on customizing teacher-student pairs to improve compatibility, a\ncomputationally expensive process that needs to be repeated every time either\nmodel changes. Hence, these methods are impractical when a teacher model has to\nbe compressed into different student models for deployment on multiple hardware\ndevices with distinct resource constraints. In this work, we propose Generic\nTeacher Network (GTN), a one-off KD-aware training to create a generic teacher\ncapable of effectively transferring knowledge to any student model sampled from\na given finite pool of architectures. To this end, we represent the student\npool as a weight-sharing supernet and condition our generic teacher to align\nwith the capacities of various student architectures sampled from this\nsupernet. Experimental evaluation shows that our method both improves overall\nKD effectiveness and amortizes the minimal additional training cost of the\ngeneric teacher across students in the pool.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "British Machine Vision Conference (BMVC 24)",
    "pdf_url": "http://arxiv.org/pdf/2407.16040v2",
    "published_date": "2024-07-22 20:34:00 UTC",
    "updated_date": "2025-01-08 07:21:15 UTC"
  },
  {
    "arxiv_id": "2407.18272v1",
    "title": "AICircuit: A Multi-Level Dataset and Benchmark for AI-Driven Analog Integrated Circuit Design",
    "authors": [
      "Asal Mehradfar",
      "Xuzhe Zhao",
      "Yue Niu",
      "Sara Babakniya",
      "Mahdi Alesheikh",
      "Hamidreza Aghasi",
      "Salman Avestimehr"
    ],
    "abstract": "Analog and radio-frequency circuit design requires extensive exploration of\nboth circuit topology and parameters to meet specific design criteria like\npower consumption and bandwidth. Designers must review state-of-the-art\ntopology configurations in the literature and sweep various circuit parameters\nwithin each configuration. This design process is highly specialized and\ntime-intensive, particularly as the number of circuit parameters increases and\nthe circuit becomes more complex. Prior research has explored the potential of\nmachine learning to enhance circuit design procedures. However, these studies\nprimarily focus on simple circuits, overlooking the more practical and complex\nanalog and radio-frequency systems. A major obstacle for bearing the power of\nmachine learning in circuit design is the availability of a generic and diverse\ndataset, along with robust metrics, which are essential for thoroughly\nevaluating and improving machine learning algorithms in the analog and\nradio-frequency circuit domain. We present AICircuit, a comprehensive\nmulti-level dataset and benchmark for developing and evaluating ML algorithms\nin analog and radio-frequency circuit design. AICircuit comprises seven\ncommonly used basic circuits and two complex wireless transceiver systems\ncomposed of multiple circuit blocks, encompassing a wide array of design\nscenarios encountered in real-world applications. We extensively evaluate\nvarious ML algorithms on the dataset, revealing the potential of ML algorithms\nin learning the mapping from the design specifications to the desired circuit\nparameters.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.18272v1",
    "published_date": "2024-07-22 20:32:16 UTC",
    "updated_date": "2024-07-22 20:32:16 UTC"
  },
  {
    "arxiv_id": "2408.00802v1",
    "title": "Leveraging LLM Reasoning Enhances Personalized Recommender Systems",
    "authors": [
      "Alicia Y. Tsai",
      "Adam Kraft",
      "Long Jin",
      "Chenwei Cai",
      "Anahita Hosseini",
      "Taibai Xu",
      "Zemin Zhang",
      "Lichan Hong",
      "Ed H. Chi",
      "Xinyang Yi"
    ],
    "abstract": "Recent advancements have showcased the potential of Large Language Models\n(LLMs) in executing reasoning tasks, particularly facilitated by\nChain-of-Thought (CoT) prompting. While tasks like arithmetic reasoning involve\nclear, definitive answers and logical chains of thought, the application of LLM\nreasoning in recommendation systems (RecSys) presents a distinct challenge.\nRecSys tasks revolve around subjectivity and personalized preferences, an\nunder-explored domain in utilizing LLMs' reasoning capabilities. Our study\nexplores several aspects to better understand reasoning for RecSys and\ndemonstrate how task quality improves by utilizing LLM reasoning in both\nzero-shot and finetuning settings. Additionally, we propose RecSAVER\n(Recommender Systems Automatic Verification and Evaluation of Reasoning) to\nautomatically assess the quality of LLM reasoning responses without the\nrequirement of curated gold references or human raters. We show that our\nframework aligns with real human judgment on the coherence and faithfulness of\nreasoning responses. Overall, our work shows that incorporating reasoning into\nRecSys can improve personalized tasks, paving the way for further advancements\nin recommender system methodologies.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "To be published at ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.00802v1",
    "published_date": "2024-07-22 20:18:50 UTC",
    "updated_date": "2024-07-22 20:18:50 UTC"
  },
  {
    "arxiv_id": "2407.16030v1",
    "title": "Enhancing Temporal Understanding in LLMs for Semi-structured Tables",
    "authors": [
      "Irwin Deng",
      "Kushagra Dixit",
      "Vivek Gupta",
      "Dan Roth"
    ],
    "abstract": "Temporal reasoning over tabular data presents substantial challenges for\nlarge language models (LLMs), as evidenced by recent research. In this study,\nwe conduct a comprehensive analysis of temporal datasets to pinpoint the\nspecific limitations of LLMs. Our investigation leads to enhancements in\nTempTabQA, a dataset specifically designed for tabular temporal question\nanswering. We provide critical insights for improving LLM performance in\ntemporal reasoning tasks with tabular data. Furthermore, we introduce a novel\napproach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings\ndemonstrate that our method significantly improves evidence-based reasoning\nacross various models. Additionally, our experimental results reveal that\nindirect supervision with auxiliary data substantially boosts model performance\nin these tasks. This work contributes to a deeper understanding of LLMs'\ntemporal reasoning abilities over tabular data and promotes advancements in\ntheir application across diverse fields.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Total Pages 18, Total Tables 6, Total figures 7",
    "pdf_url": "http://arxiv.org/pdf/2407.16030v1",
    "published_date": "2024-07-22 20:13:10 UTC",
    "updated_date": "2024-07-22 20:13:10 UTC"
  },
  {
    "arxiv_id": "2407.16026v1",
    "title": "KWT-Tiny: RISC-V Accelerated, Embedded Keyword Spotting Transformer",
    "authors": [
      "Aness Al-Qawlaq",
      "Ajay Kumar M",
      "Deepu John"
    ],
    "abstract": "This paper explores the adaptation of Transformerbased models for edge\ndevices through the quantisation and hardware acceleration of the ARM Keyword\nTransformer (KWT) model on a RISC-V platform. The model was targeted to run on\n64kB RAM in bare-metal C using a custom-developed edge AI library. KWT-1 was\nretrained to be 369 times smaller, with only a 10% loss in accuracy through\nreducing output classes from 35 to 2. The retraining and quantisation reduced\nmodel size from 2.42 MB to 1.65 kB. The integration of custom RISC-V\ninstructions that accelerated GELU and SoftMax operations enabled a 5x speedup\nand thus ~5x power reduction in inference, with inference clock cycle counts\ndecreasing from 26 million to 5.5 million clock cycles while incurring a small\narea overhead of approximately 29%. The results demonstrate a viable method for\nporting and accelerating Transformer-based models in low-power IoT devices.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.AR",
    "comment": "6 pages, 7 figures, accepted to be published in the IEEE SOCC 2024\n  conference",
    "pdf_url": "http://arxiv.org/pdf/2407.16026v1",
    "published_date": "2024-07-22 20:07:21 UTC",
    "updated_date": "2024-07-22 20:07:21 UTC"
  },
  {
    "arxiv_id": "2407.16025v2",
    "title": "Exploring and Addressing Reward Confusion in Offline Preference Learning",
    "authors": [
      "Xin Chen",
      "Sam Toyer",
      "Florian Shkurti"
    ],
    "abstract": "Spurious correlations in a reward model's training data can prevent\nReinforcement Learning from Human Feedback (RLHF) from identifying the desired\ngoal and induce unwanted behaviors. This paper shows that offline RLHF is\nsusceptible to reward confusion, especially in the presence of spurious\ncorrelations in offline data. We create a benchmark to study this problem and\npropose a method that can significantly reduce reward confusion by leveraging\ntransitivity of preferences while building a global preference chain with\nactive learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS2024 Workshop on Bayesian Decision-making and Uncertainty",
    "pdf_url": "http://arxiv.org/pdf/2407.16025v2",
    "published_date": "2024-07-22 20:03:36 UTC",
    "updated_date": "2024-10-15 13:22:19 UTC"
  },
  {
    "arxiv_id": "2407.16010v2",
    "title": "AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations",
    "authors": [
      "Ikhtiyor Nematov",
      "Dimitris Sacharidis",
      "Tomer Sagi",
      "Katja Hose"
    ],
    "abstract": "For many use-cases, it is often important to explain the prediction of a\nblack-box model by identifying the most influential training data samples.\nExisting approaches lack customization for user intent and often provide a\nhomogeneous set of explanation samples, failing to reveal the model's reasoning\nfrom different angles.\n  In this paper, we propose AIDE, an approach for providing antithetical (i.e.,\ncontrastive), intent-based, diverse explanations for opaque and complex models.\nAIDE distinguishes three types of explainability intents: interpreting a\ncorrect, investigating a wrong, and clarifying an ambiguous prediction. For\neach intent, AIDE selects an appropriate set of influential training samples\nthat support or oppose the prediction either directly or by contrast. To\nprovide a succinct summary, AIDE uses diversity-aware sampling to avoid\nredundancy and increase coverage of the training data.\n  We demonstrate the effectiveness of AIDE on image and text classification\ntasks, in three ways: quantitatively, assessing correctness and continuity;\nqualitatively, comparing anecdotal evidence from AIDE and other example-based\napproaches; and via a user study, evaluating multiple aspects of AIDE. The\nresults show that AIDE addresses the limitations of existing methods and\nexhibits desirable traits for an explainability method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16010v2",
    "published_date": "2024-07-22 19:33:12 UTC",
    "updated_date": "2024-08-08 09:12:13 UTC"
  },
  {
    "arxiv_id": "2407.15987v1",
    "title": "AI for Handball: predicting and explaining the 2024 Olympic Games tournament with Deep Learning and Large Language Models",
    "authors": [
      "Florian Felice"
    ],
    "abstract": "Over summer 2024, the world will be looking at Paris to encourage their\nfavorite athletes win the Olympic gold medal. In handball, few nations will\nfight hard to win the precious metal with speculations predicting the victory\nfor France or Denmark for men and France or Norway for women. However, there is\nso far no scientific method proposed to predict the final results of the\ncompetition. In this work, we leverage a deep learning model to predict the\nresults of the handball tournament of the 2024 Olympic Games. This model,\ncoupled with explainable AI (xAI) techniques, allows us to extract insightful\ninformation about the main factors influencing the outcome of each match.\nNotably, xAI helps sports experts understand how factors like match information\nor individual athlete performance contribute to the predictions. Furthermore,\nwe integrate Large Language Models (LLMs) to generate human-friendly\nexplanations that highlight the most important factors impacting the match\nresults. By providing human-centric explanations, our approach offers a deeper\nunderstanding of the AI predictions, making them more actionable for coaches\nand analysts.",
    "categories": [
      "stat.AP",
      "cs.AI"
    ],
    "primary_category": "stat.AP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15987v1",
    "published_date": "2024-07-22 18:54:06 UTC",
    "updated_date": "2024-07-22 18:54:06 UTC"
  },
  {
    "arxiv_id": "2407.15849v1",
    "title": "WayEx: Waypoint Exploration using a Single Demonstration",
    "authors": [
      "Mara Levy",
      "Nirat Saini",
      "Abhinav Shrivastava"
    ],
    "abstract": "We propose WayEx, a new method for learning complex goal-conditioned robotics\ntasks from a single demonstration. Our approach distinguishes itself from\nexisting imitation learning methods by demanding fewer expert examples and\neliminating the need for information about the actions taken during the\ndemonstration. This is accomplished by introducing a new reward function and\nemploying a knowledge expansion technique. We demonstrate the effectiveness of\nWayEx, our waypoint exploration strategy, across six diverse tasks, showcasing\nits applicability in various environments. Notably, our method significantly\nreduces training time by 50% as compared to traditional reinforcement learning\nmethods. WayEx obtains a higher reward than existing imitation learning methods\ngiven only a single demonstration. Furthermore, we demonstrate its success in\ntackling complex environments where standard approaches fall short. More\ninformation is available at: https://waypoint-ex.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "ICRA 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.15849v1",
    "published_date": "2024-07-22 17:59:46 UTC",
    "updated_date": "2024-07-22 17:59:46 UTC"
  },
  {
    "arxiv_id": "2407.15847v4",
    "title": "LLMmap: Fingerprinting For Large Language Models",
    "authors": [
      "Dario Pasquini",
      "Evgenios M. Kornaropoulos",
      "Giuseppe Ateniese"
    ],
    "abstract": "We introduce LLMmap, a first-generation fingerprinting technique targeted at\nLLM-integrated applications. LLMmap employs an active fingerprinting approach,\nsending carefully crafted queries to the application and analyzing the\nresponses to identify the specific LLM version in use. Our query selection is\ninformed by domain expertise on how LLMs generate uniquely identifiable\nresponses to thematically varied prompts. With as few as 8 interactions, LLMmap\ncan accurately identify 42 different LLM versions with over 95% accuracy. More\nimportantly, LLMmap is designed to be robust across different application\nlayers, allowing it to identify LLM versions--whether open-source or\nproprietary--from various vendors, operating under various unknown system\nprompts, stochastic sampling hyperparameters, and even complex generation\nframeworks such as RAG or Chain-of-Thought. We discuss potential mitigations\nand demonstrate that, against resourceful adversaries, effective\ncountermeasures may be challenging or even unrealizable.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Appearing in the proceedings of the 34th USENIX Security Symposium",
    "pdf_url": "http://arxiv.org/pdf/2407.15847v4",
    "published_date": "2024-07-22 17:59:45 UTC",
    "updated_date": "2025-02-10 19:42:35 UTC"
  },
  {
    "arxiv_id": "2407.15845v1",
    "title": "Reconstructing Training Data From Real World Models Trained with Transfer Learning",
    "authors": [
      "Yakir Oz",
      "Gilad Yehudai",
      "Gal Vardi",
      "Itai Antebi",
      "Michal Irani",
      "Niv Haim"
    ],
    "abstract": "Current methods for reconstructing training data from trained classifiers are\nrestricted to very small models, limited training set sizes, and low-resolution\nimages. Such restrictions hinder their applicability to real-world scenarios.\nIn this paper, we present a novel approach enabling data reconstruction in\nrealistic settings for models trained on high-resolution images. Our method\nadapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios --\nspecifically, targeting models trained via transfer learning over image\nembeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs\ndata reconstruction in the embedding space rather than in the image space,\nshowcasing its applicability beyond visual data. Moreover, we introduce a novel\nclustering-based method to identify good reconstructions from thousands of\ncandidates. This significantly improves on previous works that relied on\nknowledge of the training set to identify good reconstructed images. Our\nfindings shed light on a potential privacy risk for data leakage from models\ntrained using transfer learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15845v1",
    "published_date": "2024-07-22 17:59:10 UTC",
    "updated_date": "2024-07-22 17:59:10 UTC"
  },
  {
    "arxiv_id": "2407.15843v1",
    "title": "CarFormer: Self-Driving with Learned Object-Centric Representations",
    "authors": [
      "Shadi Hamdan",
      "Fatma Güney"
    ],
    "abstract": "The choice of representation plays a key role in self-driving. Bird's eye\nview (BEV) representations have shown remarkable performance in recent years.\nIn this paper, we propose to learn object-centric representations in BEV to\ndistill a complex scene into more actionable information for self-driving. We\nfirst learn to place objects into slots with a slot attention model on BEV\nsequences. Based on these object-centric representations, we then train a\ntransformer to learn to drive as well as reason about the future of other\nvehicles. We found that object-centric slot representations outperform both\nscene-level and object-level approaches that use the exact attributes of\nobjects. Slot representations naturally incorporate information about objects\nfrom their spatial and temporal context such as position, heading, and speed\nwithout explicitly providing it. Our model with slots achieves an increased\ncompletion rate of the provided routes and, consequently, a higher driving\nscore, with a lower variance across multiple runs, affirming slots as a\nreliable alternative in object-centric approaches. Additionally, we validate\nour model's performance as a world model through forecasting experiments,\ndemonstrating its capability to predict future slot representations accurately.\nThe code and the pre-trained models can be found at\nhttps://kuis-ai.github.io/CarFormer/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ECCV 2024, code and the pre-trained models can be found\n  at https://kuis-ai.github.io/CarFormer/",
    "pdf_url": "http://arxiv.org/pdf/2407.15843v1",
    "published_date": "2024-07-22 17:59:01 UTC",
    "updated_date": "2024-07-22 17:59:01 UTC"
  },
  {
    "arxiv_id": "2407.15844v1",
    "title": "HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning",
    "authors": [
      "Eugene Valassakis",
      "Guillermo Garcia-Hernando"
    ],
    "abstract": "Predicting camera-space hand meshes from single RGB images is crucial for\nenabling realistic hand interactions in 3D virtual and augmented worlds.\nPrevious work typically divided the task into two stages: given a cropped image\nof the hand, predict meshes in relative coordinates, followed by lifting these\npredictions into camera space in a separate and independent stage, often\nresulting in the loss of valuable contextual and scale information. To prevent\nthe loss of these cues, we propose unifying these two stages into an end-to-end\nsolution that addresses the 2D-3D correspondence problem. This solution enables\nback-propagation from camera space outputs to the rest of the network through a\nnew differentiable global positioning module. We also introduce an image\nrectification step that harmonizes both the training dataset and the input\nimage as if they were acquired with the same camera, helping to alleviate the\ninherent scale-depth ambiguity of the problem. We validate the effectiveness of\nour framework in evaluations against several baselines and state-of-the-art\napproaches across three public benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "To be presented at ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.15844v1",
    "published_date": "2024-07-22 17:59:01 UTC",
    "updated_date": "2024-07-22 17:59:01 UTC"
  },
  {
    "arxiv_id": "2407.15839v2",
    "title": "Importance Sampling-Guided Meta-Training for Intelligent Agents in Highly Interactive Environments",
    "authors": [
      "Mansur Arief",
      "Mike Timmerman",
      "Jiachen Li",
      "David Isele",
      "Mykel J Kochenderfer"
    ],
    "abstract": "Training intelligent agents to navigate highly interactive environments\npresents significant challenges. While guided meta reinforcement learning (RL)\napproach that first trains a guiding policy to train the ego agent has proven\neffective in improving generalizability across scenarios with various levels of\ninteraction, the state-of-the-art method tends to be overly sensitive to\nextreme cases, impairing the agents' performance in the more common scenarios.\nThis study introduces a novel training framework that integrates guided meta RL\nwith importance sampling (IS) to optimize training distributions iteratively\nfor navigating highly interactive driving scenarios, such as T-intersections or\nroundabouts. Unlike traditional methods that may underrepresent critical\ninteractions or overemphasize extreme cases during training, our approach\nstrategically adjusts the training distribution towards more challenging\ndriving behaviors using IS proposal distributions and applies the importance\nratio to de-bias the result. By estimating a naturalistic distribution from\nreal-world datasets and employing a mixture model for iterative training\nrefinements, the framework ensures a balanced focus across common and extreme\ndriving scenarios. Experiments conducted with both synthetic and naturalistic\ndatasets demonstrate both accelerated training and performance improvements\nunder highly interactive driving tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15839v2",
    "published_date": "2024-07-22 17:57:12 UTC",
    "updated_date": "2024-10-28 04:48:53 UTC"
  },
  {
    "arxiv_id": "2407.15837v1",
    "title": "Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning",
    "authors": [
      "Yibing Wei",
      "Abhinav Gupta",
      "Pedro Morgado"
    ],
    "abstract": "Masked Image Modeling (MIM) has emerged as a promising method for deriving\nvisual representations from unlabeled image data by predicting missing pixels\nfrom masked portions of images. It excels in region-aware learning and provides\nstrong initializations for various tasks, but struggles to capture high-level\nsemantics without further supervised fine-tuning, likely due to the low-level\nnature of its pixel reconstruction objective. A promising yet unrealized\nframework is learning representations through masked reconstruction in latent\nspace, combining the locality of MIM with the high-level targets. However, this\napproach poses significant training challenges as the reconstruction targets\nare learned in conjunction with the model, potentially leading to trivial or\nsuboptimal solutions.Our study is among the first to thoroughly analyze and\naddress the challenges of such framework, which we refer to as Latent MIM.\nThrough a series of carefully designed experiments and extensive analysis, we\nidentify the source of these challenges, including representation collapsing\nfor joint online/target optimization, learning objectives, the high region\ncorrelation in latent space and decoding conditioning. By sequentially\naddressing these issues, we demonstrate that Latent MIM can indeed learn\nhigh-level representations while retaining the benefits of MIM models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15837v1",
    "published_date": "2024-07-22 17:54:41 UTC",
    "updated_date": "2024-07-22 17:54:41 UTC"
  },
  {
    "arxiv_id": "2407.15835v2",
    "title": "dMel: Speech Tokenization made Simple",
    "authors": [
      "He Bai",
      "Tatiana Likhomanenko",
      "Ruixiang Zhang",
      "Zijin Gu",
      "Zakaria Aldeneh",
      "Navdeep Jaitly"
    ],
    "abstract": "Large language models have revolutionized natural language processing by\nleveraging self-supervised pretraining on vast textual data. Inspired by this\nsuccess, researchers have investigated complicated speech tokenization methods\nto discretize continuous speech signals so that language modeling techniques\ncan be applied to speech data. However, existing approaches either model\nsemantic (content) tokens, potentially losing acoustic information, or model\nacoustic tokens, risking the loss of semantic (content) information. Having\nmultiple token types also complicates the architecture and requires additional\npretraining. Here we show that discretizing mel-filterbank channels into\ndiscrete intensity bins produces a simple representation (dMel), that performs\nbetter than other existing speech tokenization methods. Using an LM-style\ntransformer architecture for speech-text modeling, we comprehensively evaluate\ndifferent speech tokenization methods on speech recognition (ASR) and speech\nsynthesis (TTS). Our results demonstrate the effectiveness of dMel in achieving\nhigh performance on both tasks within a unified framework, paving the way for\nefficient and effective joint modeling of speech and text.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2407.15835v2",
    "published_date": "2024-07-22 17:51:53 UTC",
    "updated_date": "2024-10-02 20:38:27 UTC"
  },
  {
    "arxiv_id": "2407.15831v2",
    "title": "NV-Retriever: Improving text embedding models with effective hard-negative mining",
    "authors": [
      "Gabriel de Souza P. Moreira",
      "Radek Osmulski",
      "Mengyao Xu",
      "Ronay Ak",
      "Benedikt Schifferer",
      "Even Oldridge"
    ],
    "abstract": "Text embedding models have been popular for information retrieval\napplications such as semantic search and Question-Answering systems based on\nRetrieval-Augmented Generation (RAG). Those models are typically Transformer\nmodels that are fine-tuned with contrastive learning objectives. One of the\nchallenging aspects of fine-tuning embedding models is the selection of high\nquality hard-negative passages for contrastive learning. In this paper we\nintroduce a family of positive-aware mining methods that use the positive\nrelevance score as an anchor for effective false negative removal, leading to\nfaster training and more accurate retrieval models. We provide an ablation\nstudy on hard-negative mining methods over their configurations, exploring\ndifferent teacher and base models. We further demonstrate the efficacy of our\nproposed mining methods at scale with the NV-Retriever-v1 model, which scores\n60.9 on MTEB Retrieval (BEIR) benchmark and placed 1st when it was published to\nthe MTEB Retrieval on July, 2024.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15831v2",
    "published_date": "2024-07-22 17:50:31 UTC",
    "updated_date": "2025-02-07 15:17:18 UTC"
  },
  {
    "arxiv_id": "2407.15820v2",
    "title": "On shallow planning under partial observability",
    "authors": [
      "Randy Lefebvre",
      "Audrey Durand"
    ],
    "abstract": "Formulating a real-world problem under the Reinforcement Learning framework\ninvolves non-trivial design choices, such as selecting a discount factor for\nthe learning objective (discounted cumulative rewards), which articulates the\nplanning horizon of the agent. This work investigates the impact of the\ndiscount factor on the bias-variance trade-off given structural parameters of\nthe underlying Markov Decision Process. Our results support the idea that a\nshorter planning horizon might be beneficial, especially under partial\nobservability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.15820v2",
    "published_date": "2024-07-22 17:34:07 UTC",
    "updated_date": "2025-02-17 21:14:34 UTC"
  },
  {
    "arxiv_id": "2407.15815v2",
    "title": "Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning",
    "authors": [
      "Zhecheng Yuan",
      "Tianming Wei",
      "Shuiqi Cheng",
      "Gu Zhang",
      "Yuanpei Chen",
      "Huazhe Xu"
    ],
    "abstract": "Can we endow visuomotor robots with generalization capabilities to operate in\ndiverse open-world scenarios? In this paper, we propose \\textbf{Maniwhere}, a\ngeneralizable framework tailored for visual reinforcement learning, enabling\nthe trained robot policies to generalize across a combination of multiple\nvisual disturbance types. Specifically, we introduce a multi-view\nrepresentation learning approach fused with Spatial Transformer Network (STN)\nmodule to capture shared semantic information and correspondences among\ndifferent viewpoints. In addition, we employ a curriculum-based randomization\nand augmentation approach to stabilize the RL training process and strengthen\nthe visual generalization ability. To exhibit the effectiveness of Maniwhere,\nwe meticulously design 8 tasks encompassing articulate objects, bi-manual, and\ndexterous hand manipulation tasks, demonstrating Maniwhere's strong visual\ngeneralization and sim2real transfer abilities across 3 hardware platforms. Our\nexperiments show that Maniwhere significantly outperforms existing\nstate-of-the-art methods. Videos are provided at\nhttps://gemcollector.github.io/maniwhere/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Webpage: https://gemcollector.github.io/maniwhere/",
    "pdf_url": "http://arxiv.org/pdf/2407.15815v2",
    "published_date": "2024-07-22 17:29:02 UTC",
    "updated_date": "2024-10-23 05:32:34 UTC"
  },
  {
    "arxiv_id": "2407.15814v2",
    "title": "Perceptions of Linguistic Uncertainty by Language Models and Humans",
    "authors": [
      "Catarina G Belem",
      "Markelle Kelly",
      "Mark Steyvers",
      "Sameer Singh",
      "Padhraic Smyth"
    ],
    "abstract": "_Uncertainty expressions_ such as \"probably\" or \"highly unlikely\" are\npervasive in human language. While prior work has established that there is\npopulation-level agreement in terms of how humans quantitatively interpret\nthese expressions, there has been little inquiry into the abilities of language\nmodels in the same context. In this paper, we investigate how language models\nmap linguistic expressions of uncertainty to numerical responses. Our approach\nassesses whether language models can employ theory of mind in this setting:\nunderstanding the uncertainty of another agent about a particular statement,\nindependently of the model's own certainty about that statement. We find that 7\nout of 10 models are able to map uncertainty expressions to probabilistic\nresponses in a human-like manner. However, we observe systematically different\nbehavior depending on whether a statement is actually true or false. This\nsensitivity indicates that language models are substantially more susceptible\nto bias based on their prior knowledge (as compared to humans). These findings\nraise important questions and have broad implications for human-AI and AI-AI\ncommunication.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2024 (Main)",
    "pdf_url": "http://arxiv.org/pdf/2407.15814v2",
    "published_date": "2024-07-22 17:26:12 UTC",
    "updated_date": "2024-11-07 17:33:37 UTC"
  },
  {
    "arxiv_id": "2407.15811v1",
    "title": "Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget",
    "authors": [
      "Vikash Sehwag",
      "Xianghao Kong",
      "Jingtao Li",
      "Michael Spranger",
      "Lingjuan Lyu"
    ],
    "abstract": "As scaling laws in generative AI push performance, they also simultaneously\nconcentrate the development of these models among actors with large\ncomputational resources. With a focus on text-to-image (T2I) generative models,\nwe aim to address this bottleneck by demonstrating very low-cost training of\nlarge-scale T2I diffusion transformer models. As the computational cost of\ntransformers increases with the number of patches in each image, we propose to\nrandomly mask up to 75% of the image patches during training. We propose a\ndeferred masking strategy that preprocesses all patches using a patch-mixer\nbefore masking, thus significantly reducing the performance degradation with\nmasking, making it superior to model downscaling in reducing computational\ncost. We also incorporate the latest improvements in transformer architecture,\nsuch as the use of mixture-of-experts layers, to improve performance and\nfurther identify the critical benefit of using synthetic images in micro-budget\ntraining. Finally, using only 37M publicly available real and synthetic images,\nwe train a 1.16 billion parameter sparse transformer with only \\$1,890\neconomical cost and achieve a 12.7 FID in zero-shot generation on the COCO\ndataset. Notably, our model achieves competitive FID and high-quality\ngenerations while incurring 118$\\times$ lower cost than stable diffusion models\nand 14$\\times$ lower cost than the current state-of-the-art approach that costs\n\\$28,400. We aim to release our end-to-end training pipeline to further\ndemocratize the training of large-scale diffusion models on micro-budgets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "41 pages, 28 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.15811v1",
    "published_date": "2024-07-22 17:23:28 UTC",
    "updated_date": "2024-07-22 17:23:28 UTC"
  },
  {
    "arxiv_id": "2407.15909v1",
    "title": "A Survey of Explainable Artificial Intelligence (XAI) in Financial Time Series Forecasting",
    "authors": [
      "Pierre-Daniel Arsenault",
      "Shengrui Wang",
      "Jean-Marc Patenande"
    ],
    "abstract": "Artificial Intelligence (AI) models have reached a very significant level of\naccuracy. While their superior performance offers considerable benefits, their\ninherent complexity often decreases human trust, which slows their application\nin high-risk decision-making domains, such as finance. The field of eXplainable\nAI (XAI) seeks to bridge this gap, aiming to make AI models more\nunderstandable. This survey, focusing on published work from the past five\nyears, categorizes XAI approaches that predict financial time series. In this\npaper, explainability and interpretability are distinguished, emphasizing the\nneed to treat these concepts separately as they are not applied the same way in\npractice. Through clear definitions, a rigorous taxonomy of XAI approaches, a\ncomplementary characterization, and examples of XAI's application in the\nfinance industry, this paper provides a comprehensive view of XAI's current\nrole in finance. It can also serve as a guide for selecting the most\nappropriate XAI approach for future applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "A.1; I.2; J.1"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages, This is the author's version of the work. It is posted here\n  for your personal use. Not for redistribution. The definitive Version of\n  Record will be published in a journal soon",
    "pdf_url": "http://arxiv.org/pdf/2407.15909v1",
    "published_date": "2024-07-22 17:06:19 UTC",
    "updated_date": "2024-07-22 17:06:19 UTC"
  },
  {
    "arxiv_id": "2407.15793v4",
    "title": "CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning",
    "authors": [
      "Emanuele Frascaroli",
      "Aniello Panariello",
      "Pietro Buzzega",
      "Lorenzo Bonicelli",
      "Angelo Porrello",
      "Simone Calderara"
    ],
    "abstract": "With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, fine-tuning large pre-trained models has recently become a prevalent\nstrategy in Continual Learning. This has led to the development of numerous\nprompting strategies to adapt transformer-based models without incurring\ncatastrophic forgetting. However, these strategies often compromise the\noriginal zero-shot capabilities of the pre-trained CLIP model and struggle to\nadapt to domains that significantly deviate from the pre-training data. In this\nwork, we propose Continual Generative training for Incremental prompt-Learning,\na simple and novel approach to mitigate forgetting while adapting CLIP.\nBriefly, we employ Variational Autoencoders (VAEs) to learn class-conditioned\ndistributions within the embedding space of the visual encoder. We then exploit\nthese distributions to sample new synthetic visual embeddings and train the\ncorresponding class-specific textual prompts during subsequent tasks. Through\nextensive experiments on different domains, we show that such a generative\nreplay approach can adapt to new tasks while improving zero-shot capabilities,\nevaluated using a novel metric tailored for CL scenarios. Notably, further\nanalysis reveals that our approach can bridge the gap with joint prompt tuning.\nThe codebase is available at https://github.com/aimagelab/mammoth.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 1 figure. Accepted as ORAL at the The 35th British Machine\n  Vision Conference 2024 (BMVC 2024), Glasgow, UK",
    "pdf_url": "http://arxiv.org/pdf/2407.15793v4",
    "published_date": "2024-07-22 16:51:28 UTC",
    "updated_date": "2024-10-28 12:41:35 UTC"
  },
  {
    "arxiv_id": "2408.03950v1",
    "title": "EcoFollower: An Environment-Friendly Car Following Model Considering Fuel Consumption",
    "authors": [
      "Hui Zhong",
      "Xianda Chen",
      "PakHin Tiu",
      "Hongliang Lu",
      "Meixin Zhu"
    ],
    "abstract": "To alleviate energy shortages and environmental impacts caused by\ntransportation, this study introduces EcoFollower, a novel eco-car-following\nmodel developed using reinforcement learning (RL) to optimize fuel consumption\nin car-following scenarios. Employing the NGSIM datasets, the performance of\nEcoFollower was assessed in comparison with the well-established Intelligent\nDriver Model (IDM). The findings demonstrate that EcoFollower excels in\nsimulating realistic driving behaviors, maintaining smooth vehicle operations,\nand closely matching the ground truth metrics of time-to-collision (TTC),\nheadway, and comfort. Notably, the model achieved a significant reduction in\nfuel consumption, lowering it by 10.42\\% compared to actual driving scenarios.\nThese results underscore the capability of RL-based models like EcoFollower to\nenhance autonomous vehicle algorithms, promoting safer and more\nenergy-efficient driving strategies.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03950v1",
    "published_date": "2024-07-22 16:48:37 UTC",
    "updated_date": "2024-07-22 16:48:37 UTC"
  },
  {
    "arxiv_id": "2407.15786v2",
    "title": "LICORICE: Label-Efficient Concept-Based Interpretable Reinforcement Learning",
    "authors": [
      "Zhuorui Ye",
      "Stephanie Milani",
      "Geoffrey J. Gordon",
      "Fei Fang"
    ],
    "abstract": "Recent advances in reinforcement learning (RL) have predominantly leveraged\nneural network policies for decision-making, yet these models often lack\ninterpretability, posing challenges for stakeholder comprehension and trust.\nConcept bottleneck models offer an interpretable alternative by integrating\nhuman-understandable concepts into policies. However, prior work assumes that\nconcept annotations are readily available during training. For RL, this\nrequirement poses a significant limitation: it necessitates continuous\nreal-time concept annotation, which either places an impractical burden on\nhuman annotators or incurs substantial costs in API queries and inference time\nwhen employing automated labeling methods. To overcome this limitation, we\nintroduce a novel training scheme that enables RL agents to efficiently learn a\nconcept-based policy by only querying annotators to label a small set of data.\nOur algorithm, LICORICE, involves three main contributions: interleaving\nconcept learning and RL training, using an ensemble to actively select\ninformative data points for labeling, and decorrelating the concept data. We\nshow how LICORICE reduces human labeling efforts to 500 or fewer concept labels\nin three environments, and 5000 or fewer in two more complex environments, all\nat no cost to performance. We also explore the use of VLMs as automated concept\nannotators, finding them effective in some cases but imperfect in others. Our\nwork significantly reduces the annotation burden for interpretable RL, making\nit more practical for real-world applications that necessitate transparency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.15786v2",
    "published_date": "2024-07-22 16:46:33 UTC",
    "updated_date": "2025-03-20 07:50:44 UTC"
  },
  {
    "arxiv_id": "2407.15784v1",
    "title": "Diffusion Model Based Resource Allocation Strategy in Ultra-Reliable Wireless Networked Control Systems",
    "authors": [
      "Amirhassan Babazadeh Darabi",
      "Sinem Coleri"
    ],
    "abstract": "Diffusion models are vastly used in generative AI, leveraging their\ncapability to capture complex data distributions. However, their potential\nremains largely unexplored in the field of resource allocation in wireless\nnetworks. This paper introduces a novel diffusion model-based resource\nallocation strategy for Wireless Networked Control Systems (WNCSs) with the\nobjective of minimizing total power consumption through the optimization of the\nsampling period in the control system, and blocklength and packet error\nprobability in the finite blocklength regime of the communication system. The\nproblem is first reduced to the optimization of blocklength only based on the\nderivation of the optimality conditions. Then, the optimization theory solution\ncollects a dataset of channel gains and corresponding optimal blocklengths.\nFinally, the Denoising Diffusion Probabilistic Model (DDPM) uses this collected\ndataset to train the resource allocation algorithm that generates optimal\nblocklength values conditioned on the channel state information (CSI). Via\nextensive simulations, the proposed approach is shown to outperform previously\nproposed Deep Reinforcement Learning (DRL) based approaches with close to\noptimal performance regarding total power consumption. Moreover, an improvement\nof up to eighteen-fold in the reduction of critical constraint violations is\nobserved, further underscoring the accuracy of the solution.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.IT",
      "cs.SY",
      "math.IT"
    ],
    "primary_category": "eess.SY",
    "comment": "5 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.15784v1",
    "published_date": "2024-07-22 16:44:57 UTC",
    "updated_date": "2024-07-22 16:44:57 UTC"
  },
  {
    "arxiv_id": "2407.15780v1",
    "title": "Explaining Decisions in ML Models: a Parameterized Complexity Analysis",
    "authors": [
      "Sebastian Ordyniak",
      "Giacomo Paesani",
      "Mateusz Rychlicki",
      "Stefan Szeider"
    ],
    "abstract": "This paper presents a comprehensive theoretical investigation into the\nparameterized complexity of explanation problems in various machine learning\n(ML) models. Contrary to the prevalent black-box perception, our study focuses\non models with transparent internal mechanisms. We address two principal types\nof explanation problems: abductive and contrastive, both in their local and\nglobal variants. Our analysis encompasses diverse ML models, including Decision\nTrees, Decision Sets, Decision Lists, Ordered Binary Decision Diagrams, Random\nForests, and Boolean Circuits, and ensembles thereof, each offering unique\nexplanatory challenges. This research fills a significant gap in explainable AI\n(XAI) by providing a foundational understanding of the complexities of\ngenerating explanations for these models. This work provides insights vital for\nfurther research in the domain of XAI, contributing to the broader discourse on\nthe necessity of transparency and accountability in AI systems.",
    "categories": [
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.AI",
    "comment": "A short version of the paper has been accepted at the 21st\n  International Conference on Principles of Knowledge Representation and\n  Reasoning (KR 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.15780v1",
    "published_date": "2024-07-22 16:37:48 UTC",
    "updated_date": "2024-07-22 16:37:48 UTC"
  },
  {
    "arxiv_id": "2407.15771v1",
    "title": "Local Occupancy-Enhanced Object Grasping with Multiple Triplanar Projection",
    "authors": [
      "Kangqi Ma",
      "Hao Dong",
      "Yadong Mu"
    ],
    "abstract": "This paper addresses the challenge of robotic grasping of general objects.\nSimilar to prior research, the task reads a single-view 3D observation (i.e.,\npoint clouds) captured by a depth camera as input. Crucially, the success of\nobject grasping highly demands a comprehensive understanding of the shape of\nobjects within the scene. However, single-view observations often suffer from\nocclusions (including both self and inter-object occlusions), which lead to\ngaps in the point clouds, especially in complex cluttered scenes. This renders\nincomplete perception of the object shape and frequently causes failures or\ninaccurate pose estimation during object grasping. In this paper, we tackle\nthis issue with an effective albeit simple solution, namely completing\ngrasping-related scene regions through local occupancy prediction. Following\nprior practice, the proposed model first runs by proposing a number of most\nlikely grasp points in the scene. Around each grasp point, a module is designed\nto infer any voxel in its neighborhood to be either void or occupied by some\nobject. Importantly, the occupancy map is inferred by fusing both local and\nglobal cues. We implement a multi-group tri-plane scheme for efficiently\naggregating long-distance contextual information. The model further estimates\n6-DoF grasp poses utilizing the local occupancy-enhanced object shape\ninformation and returns the top-ranked grasp proposal. Comprehensive\nexperiments on both the large-scale GraspNet-1Billion benchmark and real\nrobotic arm demonstrate that the proposed method can effectively complete the\nunobserved parts in cluttered and occluded scenes. Benefiting from the\noccupancy-enhanced feature, our model clearly outstrips other competing methods\nunder various performance metrics such as grasping average precision.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15771v1",
    "published_date": "2024-07-22 16:22:28 UTC",
    "updated_date": "2024-07-22 16:22:28 UTC"
  },
  {
    "arxiv_id": "2407.15762v2",
    "title": "Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning",
    "authors": [
      "Kaiwen Wang",
      "Rahul Kidambi",
      "Ryan Sullivan",
      "Alekh Agarwal",
      "Christoph Dann",
      "Andrea Michi",
      "Marco Gelmi",
      "Yunxuan Li",
      "Raghav Gupta",
      "Avinava Dubey",
      "Alexandre Ramé",
      "Johan Ferret",
      "Geoffrey Cideron",
      "Le Hou",
      "Hongkun Yu",
      "Amr Ahmed",
      "Aranyak Mehta",
      "Léonard Hussenot",
      "Olivier Bachem",
      "Edouard Leurent"
    ],
    "abstract": "Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge is to develop\nsteerable language models that trade-off multiple (conflicting) objectives in a\nflexible and efficient manner. This paper presents Conditional Language Policy\n(CLP), a general framework for finetuning language models on multiple\nobjectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through extensive experiments and ablations on two\nsummarization datasets, we show that CLP learns steerable language models that\noutperform and Pareto-dominate the existing approaches for multi-objective\nfinetuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "40 pages. Findings of EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.15762v2",
    "published_date": "2024-07-22 16:13:38 UTC",
    "updated_date": "2024-10-23 17:42:39 UTC"
  },
  {
    "arxiv_id": "2408.02379v1",
    "title": "The Contribution of XAI for the Safe Development and Certification of AI: An Expert-Based Analysis",
    "authors": [
      "Benjamin Fresz",
      "Vincent Philipp Göbels",
      "Safa Omri",
      "Danilo Brajovic",
      "Andreas Aichele",
      "Janika Kutz",
      "Jens Neuhüttler",
      "Marco F. Huber"
    ],
    "abstract": "Developing and certifying safe - or so-called trustworthy - AI has become an\nincreasingly salient issue, especially in light of upcoming regulation such as\nthe EU AI Act. In this context, the black-box nature of machine learning models\nlimits the use of conventional avenues of approach towards certifying complex\ntechnical systems. As a potential solution, methods to give insights into this\nblack-box - devised in the field of eXplainable AI (XAI) - could be used. In\nthis study, the potential and shortcomings of such methods for the purpose of\nsafe AI development and certification are discussed in 15 qualitative\ninterviews with experts out of the areas of (X)AI and certification. We find\nthat XAI methods can be a helpful asset for safe AI development, as they can\nshow biases and failures of ML-models, but since certification relies on\ncomprehensive and correct information about technical systems, their impact is\nexpected to be limited.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02379v1",
    "published_date": "2024-07-22 16:08:21 UTC",
    "updated_date": "2024-07-22 16:08:21 UTC"
  },
  {
    "arxiv_id": "2407.15756v1",
    "title": "Model editing for distribution shifts in uranium oxide morphological analysis",
    "authors": [
      "Davis Brown",
      "Cody Nizinski",
      "Madelyn Shapiro",
      "Corey Fallon",
      "Tianzhixi Yin",
      "Henry Kvinge",
      "Jonathan H. Tu"
    ],
    "abstract": "Deep learning still struggles with certain kinds of scientific data. Notably,\npretraining data may not provide coverage of relevant distribution shifts\n(e.g., shifts induced via the use of different measurement instruments). We\nconsider deep learning models trained to classify the synthesis conditions of\nuranium ore concentrates (UOCs) and show that model editing is particularly\neffective for improving generalization to distribution shifts common in this\ndomain. In particular, model editing outperforms finetuning on two curated\ndatasets comprising of micrographs taken of U$_{3}$O$_{8}$ aged in humidity\nchambers and micrographs acquired with different scanning electron microscopes,\nrespectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at CV4MS @ CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.15756v1",
    "published_date": "2024-07-22 16:06:51 UTC",
    "updated_date": "2024-07-22 16:06:51 UTC"
  },
  {
    "arxiv_id": "2407.15748v1",
    "title": "MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation",
    "authors": [
      "Marco Simoni",
      "Andrea Saracino",
      "Vinod P.",
      "Mauro Conti"
    ],
    "abstract": "In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the\nfirst specialised AI chatbot for cybersecurity. MoRSE aims to provide\ncomprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG\n(Retrieval Augmented Generation) systems designed to retrieve and organize\ninformation from multidimensional cybersecurity contexts. MoRSE differs from\ntraditional RAGs by using parallel retrievers that work together to retrieve\nsemantically related information in different formats and structures. Unlike\ntraditional Large Language Models (LLMs) that rely on Parametric Knowledge\nBases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases\nin response to user queries. Subsequently, MoRSE uses this information to\ngenerate accurate answers. In addition, MoRSE benefits from real-time updates\nto its knowledge bases, enabling continuous knowledge enrichment without\nretraining. We have evaluated the effectiveness of MoRSE against other\nstate-of-the-art LLMs, evaluating the system on 600 cybersecurity specific\nquestions. The experimental evaluation has shown that the improvement in terms\nof relevance and correctness of the answer is more than 10\\% compared to known\nsolutions such as GPT-4 and Mixtral 7x8.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15748v1",
    "published_date": "2024-07-22 15:53:27 UTC",
    "updated_date": "2024-07-22 15:53:27 UTC"
  },
  {
    "arxiv_id": "2407.15739v1",
    "title": "Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond",
    "authors": [
      "Silvio Galesso",
      "Philipp Schröppel",
      "Hssan Driss",
      "Thomas Brox"
    ],
    "abstract": "In recent years, research on out-of-distribution (OoD) detection for semantic\nsegmentation has mainly focused on road scenes -- a domain with a constrained\namount of semantic diversity. In this work, we challenge this constraint and\nextend the domain of this task to general natural images. To this end, we\nintroduce: 1. the ADE-OoD benchmark, which is based on the ADE20k dataset and\nincludes images from diverse domains with a high semantic diversity, and 2. a\nnovel approach that uses Diffusion score matching for OoD detection (DOoD) and\nis robust to the increased semantic diversity. ADE-OoD features indoor and\noutdoor images, defines 150 semantic categories as in-distribution, and\ncontains a variety of OoD objects. For DOoD, we train a diffusion model with an\nMLP architecture on semantic in-distribution embeddings and build on the score\nmatching interpretation to compute pixel-wise OoD scores at inference time. On\ncommon road scene OoD benchmarks, DOoD performs on par or better than the state\nof the art, without using outliers for training or making assumptions about the\ndata domain. On ADE-OoD, DOoD outperforms previous approaches, but leaves much\nroom for future improvements.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024 - Benchmark page: https://ade-ood.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2407.15739v1",
    "published_date": "2024-07-22 15:41:37 UTC",
    "updated_date": "2024-07-22 15:41:37 UTC"
  },
  {
    "arxiv_id": "2407.15738v3",
    "title": "Parallel Split Learning with Global Sampling",
    "authors": [
      "Mohammad Kohankhaki",
      "Ahmad Ayad",
      "Mahdi Barhoush",
      "Anke Schmeink"
    ],
    "abstract": "Distributed deep learning in resource-constrained environments faces\nscalability and generalization challenges due to large effective batch sizes\nand non-identically distributed client data. We introduce a server-driven\nsampling strategy that maintains a fixed global batch size by dynamically\nadjusting client-side batch sizes. This decouples the effective batch size from\nthe number of participating devices and ensures that global batches better\nreflect the overall data distribution. Using standard concentration bounds, we\nestablish tighter deviation guarantees compared to existing approaches.\nEmpirical results on a benchmark dataset confirm that the proposed method\nimproves model accuracy, training efficiency, and convergence stability,\noffering a scalable solution for learning at the network edge.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15738v3",
    "published_date": "2024-07-22 15:41:23 UTC",
    "updated_date": "2025-05-03 18:37:58 UTC"
  },
  {
    "arxiv_id": "2407.15734v1",
    "title": "TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON",
    "authors": [
      "John Chong Min Tan",
      "Prince Saroj",
      "Bharat Runwal",
      "Hardik Maheshwari",
      "Brian Lim Yi Sheng",
      "Richard Cottrill",
      "Alankrit Chona",
      "Ambuj Kumar",
      "Mehul Motani"
    ],
    "abstract": "TaskGen is an open-sourced agentic framework which uses an Agent to solve an\narbitrary task by breaking them down into subtasks. Each subtask is mapped to\nan Equipped Function or another Agent to execute. In order to reduce verbosity\n(and hence token usage), TaskGen uses StrictJSON that ensures JSON output from\nthe Large Language Model (LLM), along with additional features such as type\nchecking and iterative error correction. Key to the philosophy of TaskGen is\nthe management of information/memory on a need-to-know basis. We empirically\nevaluate TaskGen on various environments such as 40x40 dynamic maze navigation\nwith changing obstacle locations (100% solve rate), TextWorld escape room\nsolving with dense rewards and detailed goals (96% solve rate), web browsing\n(69% of actions successful), solving the MATH dataset (71% solve rate over 100\nLevel-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset\n(F1 score of 47.03%)",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "53 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.15734v1",
    "published_date": "2024-07-22 15:37:41 UTC",
    "updated_date": "2024-07-22 15:37:41 UTC"
  },
  {
    "arxiv_id": "2407.15723v1",
    "title": "DStruct2Design: Data and Benchmarks for Data Structure Driven Generative Floor Plan Design",
    "authors": [
      "Zhi Hao Luo",
      "Luis Lara",
      "Ge Ya Luo",
      "Florian Golemo",
      "Christopher Beckham",
      "Christopher Pal"
    ],
    "abstract": "Text conditioned generative models for images have yielded impressive\nresults. Text conditioned floorplan generation as a special type of raster\nimage generation task also received particular attention. However there are\nmany use cases in floorpla generation where numerical properties of the\ngenerated result are more important than the aesthetics. For instance, one\nmight want to specify sizes for certain rooms in a floorplan and compare the\ngenerated floorplan with given specifications Current approaches, datasets and\ncommonly used evaluations do not support these kinds of constraints. As such,\nan attractive strategy is to generate an intermediate data structure that\ncontains numerical properties of a floorplan which can be used to generate the\nfinal floorplan image. To explore this setting we (1) construct a new dataset\nfor this data-structure to data-structure formulation of floorplan generation\nusing two popular image based floorplan datasets RPLAN and ProcTHOR-10k, and\nprovide the tools to convert further procedurally generated ProcTHOR floorplan\ndata into our format. (2) We explore the task of floorplan generation given a\npartial or complete set of constraints and we design a series of metrics and\nbenchmarks to enable evaluating how well samples generated from models respect\nthe constraints. (3) We create multiple baselines by finetuning a large\nlanguage model (LLM), Llama3, and demonstrate the feasibility of using\nfloorplan data structure conditioned LLMs for the problem of floorplan\ngeneration respecting numerical constraints. We hope that our new datasets and\nbenchmarks will encourage further research on different ways to improve the\nperformance of LLMs and other generative modelling techniques for generating\ndesigns where quantitative constraints are only partially specified, but must\nbe respected.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15723v1",
    "published_date": "2024-07-22 15:27:55 UTC",
    "updated_date": "2024-07-22 15:27:55 UTC"
  },
  {
    "arxiv_id": "2407.15720v2",
    "title": "Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability",
    "authors": [
      "Zhuoyan Xu",
      "Zhenmei Shi",
      "Yingyu Liang"
    ],
    "abstract": "Large language models (LLMs) have emerged as powerful tools for many AI\nproblems and exhibit remarkable in-context learning (ICL) capabilities.\nCompositional ability, solving unseen complex tasks that combine two or more\nsimple tasks, is an essential reasoning ability for Artificial General\nIntelligence. Despite the tremendous success of LLMs, how they approach\ncomposite tasks, especially those not encountered during the pretraining phase,\nremains an open and largely underexplored question. In this study, we delve\ninto the ICL capabilities of LLMs on composite tasks, with only simple tasks as\nin-context examples. We develop a test suite of composite tasks including\nlinguistic and logical challenges and perform empirical studies across\ndifferent LLM families. We observe that models exhibit divergent behaviors: (1)\nFor simpler composite tasks that apply distinct mapping mechanisms to different\ninput segments, the models demonstrate decent compositional ability, while\nscaling up the model enhances this ability; (2) for more complex composite\ntasks involving reasoning multiple steps, where each step represents one task,\nmodels typically underperform, and scaling up generally provides no\nimprovements. We offer theoretical analysis in a simplified setting, explaining\nthat models exhibit compositional capability when the task handles different\ninput parts separately. We believe our work sheds new light on the capabilities\nof LLMs in solving composite tasks regarding the nature of the tasks and model\nscale. Our dataset and code are available at\n{\\url{https://github.com/OliverXUZY/LLM_Compose}}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15720v2",
    "published_date": "2024-07-22 15:22:34 UTC",
    "updated_date": "2024-08-11 04:39:16 UTC"
  },
  {
    "arxiv_id": "2407.15719v3",
    "title": "GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI",
    "authors": [
      "Zhaojie Fang",
      "Shenghao Zhu",
      "Yifei Chen",
      "Binfeng Zou",
      "Fan Jia",
      "Chang Liu",
      "Xiang Feng",
      "Linwei Qiu",
      "Feiwei Qin",
      "Jin Fan",
      "Changbiao Chu",
      "Changmiao Wang"
    ],
    "abstract": "Alzheimer's Disease (AD) is a progressive, irreversible neurodegenerative\ndisorder that often originates from Mild Cognitive Impairment (MCI). This\nprogression results in significant memory loss and severely affects patients'\nquality of life. Clinical trials have consistently shown that early and\ntargeted interventions for individuals with MCI may slow or even prevent the\nadvancement of AD. Research indicates that accurate medical classification\nrequires diverse multimodal data, including detailed assessment scales and\nneuroimaging techniques like Magnetic Resonance Imaging (MRI) and Positron\nEmission Tomography (PET). However, simultaneously collecting the\naforementioned three modalities for training presents substantial challenges.\nTo tackle these difficulties, we propose GFE-Mamba, a multimodal classifier\nfounded on Generative Feature Extractor. The intermediate features provided by\nthis Extractor can compensate for the shortcomings of PET and achieve profound\nmultimodal fusion in the classifier. The Mamba block, as the backbone of the\nclassifier, enables it to efficiently extract information from long-sequence\nscale information. Pixel-level Bi-cross Attention supplements pixel-level\ninformation from MRI and PET. We provide our rationale for developing this\ncross-temporal progression prediction dataset and the pre-trained Extractor\nweights. Our experimental findings reveal that the GFE-Mamba model effectively\npredicts the progression from MCI to AD and surpasses several leading methods\nin the field. Our source code is available at\nhttps://github.com/Tinysqua/GFE-Mamba.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.15719v3",
    "published_date": "2024-07-22 15:22:33 UTC",
    "updated_date": "2025-01-29 06:35:34 UTC"
  },
  {
    "arxiv_id": "2407.15714v1",
    "title": "Mamba meets crack segmentation",
    "authors": [
      "Zhili He",
      "Yu-Hsing Wang"
    ],
    "abstract": "Cracks pose safety risks to infrastructure and cannot be overlooked. The\nprevailing structures in existing crack segmentation networks predominantly\nconsist of CNNs or Transformers. However, CNNs exhibit a deficiency in global\nmodeling capability, hindering the representation to entire crack features.\nTransformers can capture long-range dependencies but suffer from high and\nquadratic complexity. Recently, Mamba has garnered extensive attention due to\nits linear spatial and computational complexity and its powerful global\nperception. This study explores the representation capabilities of Mamba to\ncrack features. Specifically, this paper uncovers the connection between Mamba\nand the attention mechanism, providing a profound insight, an attention\nperspective, into interpreting Mamba and devising a novel Mamba module\nfollowing the principles of attention blocks, namely CrackMamba. We compare\nCrackMamba with the most prominent visual Mamba modules, Vim and Vmamba, on two\ndatasets comprising asphalt pavement and concrete pavement cracks, and steel\ncracks, respectively. The quantitative results show that CrackMamba stands out\nas the sole Mamba block consistently enhancing the baseline model's performance\nacross all evaluation measures, while reducing its parameters and computational\ncosts. Moreover, this paper substantiates that Mamba can achieve global\nreceptive fields through both theoretical analysis and visual interpretability.\nThe discoveries of this study offer a dual contribution. First, as a\nplug-and-play and simple yet effective Mamba module, CrackMamba exhibits\nimmense potential for integration into various crack segmentation models.\nSecond, the proposed innovative Mamba design concept, integrating Mamba with\nthe attention mechanism, holds significant reference value for all Mamba-based\ncomputer vision models, not limited to crack segmentation networks, as\ninvestigated in this study.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "32 pages, 8 figures. Preprint submitted to Elsevier",
    "pdf_url": "http://arxiv.org/pdf/2407.15714v1",
    "published_date": "2024-07-22 15:21:35 UTC",
    "updated_date": "2024-07-22 15:21:35 UTC"
  },
  {
    "arxiv_id": "2407.15708v2",
    "title": "SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams",
    "authors": [
      "Liangyan Jiang",
      "Chuang Zhu",
      "Yanxu Chen"
    ],
    "abstract": "The spike camera, with its high temporal resolution, low latency, and high\ndynamic range, addresses high-speed imaging challenges like motion blur. It\ncaptures photons at each pixel independently, creating binary spike streams\nrich in temporal information but challenging for image reconstruction. Current\nalgorithms, both traditional and deep learning-based, still need to be improved\nin the utilization of the rich temporal detail and the restoration of the\ndetails of the reconstructed image. To overcome this, we introduce Swin\nSpikeformer (SwinSF), a novel model for dynamic scene reconstruction from spike\nstreams. SwinSF is composed of Spike Feature Extraction, Spatial-Temporal\nFeature Extraction, and Final Reconstruction Module. It combines shifted window\nself-attention and proposed temporal spike attention, ensuring a comprehensive\nfeature extraction that encapsulates both spatial and temporal dynamics,\nleading to a more robust and accurate reconstruction of spike streams.\nFurthermore, we build a new synthesized dataset for spike image reconstruction\nwhich matches the resolution of the latest spike camera, ensuring its relevance\nand applicability to the latest developments in spike camera imaging.\nExperimental results demonstrate that the proposed network SwinSF sets a new\nbenchmark, achieving state-of-the-art performance across a series of datasets,\nincluding both real-world and synthesized data across various resolutions. Our\ncodes and proposed dataset will be available soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15708v2",
    "published_date": "2024-07-22 15:17:39 UTC",
    "updated_date": "2024-07-24 16:55:08 UTC"
  },
  {
    "arxiv_id": "2407.15707v1",
    "title": "Predicting the Best of N Visual Trackers",
    "authors": [
      "Basit Alawode",
      "Sajid Javed",
      "Arif Mahmood",
      "Jiri Matas"
    ],
    "abstract": "We observe that the performance of SOTA visual trackers surprisingly strongly\nvaries across different video attributes and datasets. No single tracker\nremains the best performer across all tracking attributes and datasets. To\nbridge this gap, for a given video sequence, we predict the \"Best of the N\nTrackers\", called the BofN meta-tracker. At its core, a Tracking Performance\nPrediction Network (TP2N) selects a predicted best performing visual tracker\nfor the given video sequence using only a few initial frames. We also introduce\na frame-level BofN meta-tracker which keeps predicting best performer after\nregular temporal intervals. The TP2N is based on self-supervised learning\narchitectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO with\nViT-S as a backbone performs the best. The video-level BofN meta-tracker\noutperforms, by a large margin, existing SOTA trackers on nine standard\nbenchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123,\nOTB100, and WebUAV-3M. Further improvement is achieved by the frame-level BofN\nmeta-tracker effectively handling variations in the tracking scenarios within\nlong sequences. For instance, on GOT-10k, BofN meta-tracker average overlap is\n88.7% and 91.1% with video and frame-level settings respectively. The best\nperforming tracker, RTS, achieves 85.20% AO. On VOT2022, BofN expected average\noverlap is 67.88% and 70.98% with video and frame level settings, compared to\nthe best performing ARTrack, 64.12%. This work also presents an extensive\nevaluation of competitive tracking methods on all commonly used benchmarks,\nfollowing their protocols. The code, the trained models, and the results will\nsoon be made publicly available on\nhttps://github.com/BasitAlawode/Best_of_N_Trackers.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15707v1",
    "published_date": "2024-07-22 15:17:09 UTC",
    "updated_date": "2024-07-22 15:17:09 UTC"
  },
  {
    "arxiv_id": "2407.15700v1",
    "title": "A Life-long Learning Intrusion Detection System for 6G-Enabled IoV",
    "authors": [
      "Abdelaziz Amara korba",
      "Souad Sebaa",
      "Malik Mabrouki",
      "Yacine Ghamri-Doudane",
      "Karima Benatchba"
    ],
    "abstract": "The introduction of 6G technology into the Internet of Vehicles (IoV)\npromises to revolutionize connectivity with ultra-high data rates and seamless\nnetwork coverage. However, this technological leap also brings significant\nchallenges, particularly for the dynamic and diverse IoV landscape, which must\nmeet the rigorous reliability and security requirements of 6G networks.\nFurthermore, integrating 6G will likely increase the IoV's susceptibility to a\nspectrum of emerging cyber threats. Therefore, it is crucial for security\nmechanisms to dynamically adapt and learn new attack patterns, keeping pace\nwith the rapid evolution and diversification of these threats - a capability\ncurrently lacking in existing systems. This paper presents a novel intrusion\ndetection system leveraging the paradigm of life-long (or continual) learning.\nOur methodology combines class-incremental learning with federated learning, an\napproach ideally suited to the distributed nature of the IoV. This strategy\neffectively harnesses the collective intelligence of Connected and Automated\nVehicles (CAVs) and edge computing capabilities to train the detection system.\nTo the best of our knowledge, this study is the first to synergize\nclass-incremental learning with federated learning specifically for cyber\nattack detection. Through comprehensive experiments on a recent network traffic\ndataset, our system has exhibited a robust adaptability in learning new cyber\nattack patterns, while effectively retaining knowledge of previously\nencountered ones. Additionally, it has proven to maintain high accuracy and a\nlow false positive rate.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15700v1",
    "published_date": "2024-07-22 15:07:27 UTC",
    "updated_date": "2024-07-22 15:07:27 UTC"
  },
  {
    "arxiv_id": "2407.15688v1",
    "title": "AI-Driven Fast and Early Detection of IoT Botnet Threats: A Comprehensive Network Traffic Analysis Approach",
    "authors": [
      "Abdelaziz Amara korba",
      "Aleddine Diaf",
      "Yacine Ghamri-Doudane"
    ],
    "abstract": "In the rapidly evolving landscape of cyber threats targeting the Internet of\nThings (IoT) ecosystem, and in light of the surge in botnet-driven Distributed\nDenial of Service (DDoS) and brute force attacks, this study focuses on the\nearly detection of IoT bots. It specifically addresses the detection of stealth\nbot communication that precedes and orchestrates attacks. This study proposes a\ncomprehensive methodology for analyzing IoT network traffic, including\nconsiderations for both unidirectional and bidirectional flow, as well as\npacket formats. It explores a wide spectrum of network features critical for\nrepresenting network traffic and characterizing benign IoT traffic patterns\neffectively. Moreover, it delves into the modeling of traffic using various\nsemi-supervised learning techniques. Through extensive experimentation with the\nIoT-23 dataset - a comprehensive collection featuring diverse botnet types and\ntraffic scenarios - we have demonstrated the feasibility of detecting botnet\ntraffic corresponding to different operations and types of bots, specifically\nfocusing on stealth command and control (C2) communications. The results\nobtained have demonstrated the feasibility of identifying C2 communication with\na 100% success rate through packet-based methods and 94% via flow based\napproaches, with a false positive rate of 1.53%.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15688v1",
    "published_date": "2024-07-22 14:54:40 UTC",
    "updated_date": "2024-07-22 14:54:40 UTC"
  },
  {
    "arxiv_id": "2407.15680v1",
    "title": "HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning",
    "authors": [
      "Zhecan Wang",
      "Garrett Bingham",
      "Adams Yu",
      "Quoc Le",
      "Thang Luong",
      "Golnaz Ghiasi"
    ],
    "abstract": "Hallucination has been a major problem for large language models and remains\na critical challenge when it comes to multimodality in which vision-language\nmodels (VLMs) have to deal with not just textual but also visual inputs.\nDespite rapid progress in VLMs, resources for evaluating and addressing\nmultimodal hallucination are limited and mostly focused on evaluation. This\nwork introduces HaloQuest, a novel visual question answering dataset that\ncaptures various aspects of multimodal hallucination such as false premises,\ninsufficient contexts, and visual challenges. A novel idea from HaloQuest is to\nleverage synthetic images, apart from real ones, to enable dataset creation at\nscale. With over 7.7K examples spanning across a wide variety of categories,\nHaloQuest was designed to be both a challenging benchmark for VLMs and a\nfine-tuning dataset for advancing multimodal reasoning. Our experiments reveal\nthat current models struggle with HaloQuest, with all open-source VLMs\nachieving below 36% accuracy. On the other hand, fine-tuning on HaloQuest\nsignificantly reduces hallucination rates while preserving performance on\nstandard reasoning tasks. Our results discover that benchmarking with generated\nimages is highly correlated (r=0.97) with real images. Last but not least, we\npropose a novel Auto-Eval mechanism that is highly correlated with human raters\n(r=0.99) for evaluating VLMs. In sum, this work makes concrete strides towards\nunderstanding, evaluating, and mitigating hallucination in VLMs, serving as an\nimportant step towards more reliable multimodal AI systems in the future.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as a main conference paper at ECCV 2024\n  (https://github.com/google/haloquest)",
    "pdf_url": "http://arxiv.org/pdf/2407.15680v1",
    "published_date": "2024-07-22 14:49:51 UTC",
    "updated_date": "2024-07-22 14:49:51 UTC"
  },
  {
    "arxiv_id": "2407.15906v3",
    "title": "An Ad-hoc graph node vector embedding algorithm for general knowledge graphs using Kinetica-Graph",
    "authors": [
      "B. Kaan Karamete",
      "Eli Glaser"
    ],
    "abstract": "This paper discusses how to generate general graph node embeddings from\nknowledge graph representations. The embedded space is composed of a number of\nsub-features to mimic both local affinity and remote structural relevance.\nThese sub-feature dimensions are defined by several indicators that we\nspeculate to catch nodal similarities, such as hop-based topological patterns,\nthe number of overlapping labels, the transitional probabilities (markov-chain\nprobabilities), and the cluster indices computed by our recursive spectral\nbisection (RSB) algorithm. These measures are flattened over the one\ndimensional vector space into their respective sub-component ranges such that\nthe entire set of vector similarity functions could be used for finding similar\nnodes. The error is defined by the sum of pairwise square differences across a\nrandomly selected sample of graph nodes between the assumed embeddings and the\nground truth estimates as our novel loss function. The ground truth is\nestimated to be a combination of pairwise Jaccard similarity and the number of\noverlapping labels. Finally, we demonstrate a multi-variate stochastic gradient\ndescent (SGD) algorithm to compute the weighing factors among sub-vector spaces\nto minimize the average error using a random sampling logic.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 17 figures, 16 references",
    "pdf_url": "http://arxiv.org/pdf/2407.15906v3",
    "published_date": "2024-07-22 14:43:10 UTC",
    "updated_date": "2025-01-02 20:52:09 UTC"
  },
  {
    "arxiv_id": "2407.15675v1",
    "title": "Flow-guided Motion Prediction with Semantics and Dynamic Occupancy Grid Maps",
    "authors": [
      "Rabbia Asghar",
      "Wenqian Liu",
      "Lukas Rummelhard",
      "Anne Spalanzani",
      "Christian Laugier"
    ],
    "abstract": "Accurate prediction of driving scenes is essential for road safety and\nautonomous driving. Occupancy Grid Maps (OGMs) are commonly employed for scene\nprediction due to their structured spatial representation, flexibility across\nsensor modalities and integration of uncertainty. Recent studies have\nsuccessfully combined OGMs with deep learning methods to predict the evolution\nof scene and learn complex behaviours. These methods, however, do not consider\nprediction of flow or velocity vectors in the scene. In this work, we propose a\nnovel multi-task framework that leverages dynamic OGMs and semantic information\nto predict both future vehicle semantic grids and the future flow of the scene.\nThis incorporation of semantic flow not only offers intermediate scene features\nbut also enables the generation of warped semantic grids. Evaluation on the\nreal-world NuScenes dataset demonstrates improved prediction capabilities and\nenhanced ability of the model to retain dynamic vehicles within the scene.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication at the 27th IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) (ITSC 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.15675v1",
    "published_date": "2024-07-22 14:42:34 UTC",
    "updated_date": "2024-07-22 14:42:34 UTC"
  },
  {
    "arxiv_id": "2407.15671v1",
    "title": "Problems in AI, their roots in philosophy, and implications for science and society",
    "authors": [
      "Max Velthoven",
      "Eric Marcus"
    ],
    "abstract": "Artificial Intelligence (AI) is one of today's most relevant emergent\ntechnologies. In view thereof, this paper proposes that more attention should\nbe paid to the philosophical aspects of AI technology and its use. It is argued\nthat this deficit is generally combined with philosophical misconceptions about\nthe growth of knowledge. To identify these misconceptions, reference is made to\nthe ideas of the philosopher of science Karl Popper and the physicist David\nDeutsch. The works of both thinkers aim against mistaken theories of knowledge,\nsuch as inductivism, empiricism, and instrumentalism. This paper shows that\nthese theories bear similarities to how current AI technology operates. It also\nshows that these theories are very much alive in the (public) discourse on AI,\noften called Bayesianism. In line with Popper and Deutsch, it is proposed that\nall these theories are based on mistaken philosophies of knowledge. This\nincludes an analysis of the implications of these mistaken philosophies for the\nuse of AI in science and society, including some of the likely problem\nsituations that will arise. This paper finally provides a realistic outlook on\nArtificial General Intelligence (AGI) and three propositions on A(G)I and\nphilosophy (i.e., epistemology).",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15671v1",
    "published_date": "2024-07-22 14:38:54 UTC",
    "updated_date": "2024-07-22 14:38:54 UTC"
  },
  {
    "arxiv_id": "2407.15668v2",
    "title": "SLVideo: A Sign Language Video Moment Retrieval Framework",
    "authors": [
      "Gonçalo Vinagre Martins",
      "João Magalhães",
      "Afonso Quinaz",
      "Carla Viegas",
      "Sofia Cavaco"
    ],
    "abstract": "SLVideo is a video moment retrieval system for Sign Language videos that\nincorporates facial expressions, addressing this gap in existing technology.\nThe system extracts embedding representations for the hand and face signs from\nvideo frames to capture the signs in their entirety, enabling users to search\nfor a specific sign language video segment with text queries. A collection of\neight hours of annotated Portuguese Sign Language videos is used as the\ndataset, and a CLIP model is used to generate the embeddings. The initial\nresults are promising in a zero-shot setting. In addition, SLVideo incorporates\na thesaurus that enables users to search for similar signs to those retrieved,\nusing the video segment embeddings, and also supports the edition and creation\nof video sign language annotations. Project web page:\nhttps://novasearch.github.io/SLVideo/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages, 1 figure, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2407.15668v2",
    "published_date": "2024-07-22 14:29:36 UTC",
    "updated_date": "2024-11-05 18:38:08 UTC"
  },
  {
    "arxiv_id": "2407.15662v1",
    "title": "How to Shrink Confidence Sets for Many Equivalent Discrete Distributions?",
    "authors": [
      "Odalric-Ambrym Maillard",
      "Mohammad Sadegh Talebi"
    ],
    "abstract": "We consider the situation when a learner faces a set of unknown discrete\ndistributions $(p_k)_{k\\in \\mathcal K}$ defined over a common alphabet\n$\\mathcal X$, and can build for each distribution $p_k$ an individual\nhigh-probability confidence set thanks to $n_k$ observations sampled from\n$p_k$. The set $(p_k)_{k\\in \\mathcal K}$ is structured: each distribution $p_k$\nis obtained from the same common, but unknown, distribution q via applying an\nunknown permutation to $\\mathcal X$. We call this\n\\emph{permutation-equivalence}. The goal is to build refined confidence sets\n\\emph{exploiting} this structural property. Like other popular notions of\nstructure (Lipschitz smoothness, Linearity, etc.) permutation-equivalence\nnaturally appears in machine learning problems, and to benefit from its\npotential gain calls for a specific approach. We present a strategy to\neffectively exploit permutation-equivalence, and provide a finite-time\nhigh-probability bound on the size of the refined confidence sets output by the\nstrategy. Since a refinement is not possible for too few observations in\ngeneral, under mild technical assumptions, our finite-time analysis establish\nwhen the number of observations $(n_k)_{k\\in \\mathcal K}$ are large enough so\nthat the output confidence sets improve over initial individual sets. We\ncarefully characterize this event and the corresponding improvement. Further,\nour result implies that the size of confidence sets shrink at asymptotic rates\nof $O(1/\\sqrt{\\sum_{k\\in \\mathcal K} n_k})$ and $O(1/\\max_{k\\in K} n_{k})$,\nrespectively for elements inside and outside the support of q, when the size of\neach individual confidence set shrinks at respective rates of $O(1/\\sqrt{n_k})$\nand $O(1/n_k)$. We illustrate the practical benefit of exploiting permutation\nequivalence on a reinforcement learning task.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15662v1",
    "published_date": "2024-07-22 14:19:19 UTC",
    "updated_date": "2024-07-22 14:19:19 UTC"
  },
  {
    "arxiv_id": "2407.15656v1",
    "title": "Evaluation of Reinforcement Learning for Autonomous Penetration Testing using A3C, Q-learning and DQN",
    "authors": [
      "Norman Becker",
      "Daniel Reti",
      "Evridiki V. Ntagiou",
      "Marcus Wallum",
      "Hans D. Schotten"
    ],
    "abstract": "Penetration testing is the process of searching for security weaknesses by\nsimulating an attack. It is usually performed by experienced professionals,\nwhere scanning and attack tools are applied. By automating the execution of\nsuch tools, the need for human interaction and decision-making could be\nreduced. In this work, a Network Attack Simulator (NASim) was used as an\nenvironment to train reinforcement learning agents to solve three predefined\nsecurity scenarios. These scenarios cover techniques of exploitation,\npost-exploitation and wiretapping. A large hyperparameter grid search was\nperformed to find the best hyperparameter combinations. The algorithms\nQ-learning, DQN and A3C were used, whereby A3C was able to solve all scenarios\nand achieve generalization. In addition, A3C could solve these scenarios with\nfewer actions than the baseline automated penetration testing. Although the\ntraining was performed on rather small scenarios and with small state and\naction spaces for the agents, the results show that a penetration test can\nsuccessfully be performed by the RL agent.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15656v1",
    "published_date": "2024-07-22 14:17:29 UTC",
    "updated_date": "2024-07-22 14:17:29 UTC"
  },
  {
    "arxiv_id": "2407.15645v1",
    "title": "Psychometric Alignment: Capturing Human Knowledge Distributions via Language Models",
    "authors": [
      "Joy He-Yueya",
      "Wanjing Anya Ma",
      "Kanishk Gandhi",
      "Benjamin W. Domingue",
      "Emma Brunskill",
      "Noah D. Goodman"
    ],
    "abstract": "Language models (LMs) are increasingly used to simulate human-like responses\nin scenarios where accurately mimicking a population's behavior can guide\ndecision-making, such as in developing educational materials and designing\npublic policies. The objective of these simulations is for LMs to capture the\nvariations in human responses, rather than merely providing the expected\ncorrect answers. Prior work has shown that LMs often generate unrealistically\naccurate responses, but there are no established metrics to quantify how\nclosely the knowledge distribution of LMs aligns with that of humans. To\naddress this, we introduce \"psychometric alignment,\" a metric that measures the\nextent to which LMs reflect human knowledge distribution. Assessing this\nalignment involves collecting responses from both LMs and humans to the same\nset of test items and using Item Response Theory to analyze the differences in\nitem functioning between the groups. We demonstrate that our metric can capture\nimportant variations in populations that traditional metrics, like differences\nin accuracy, fail to capture. We apply this metric to assess existing LMs for\ntheir alignment with human knowledge distributions across three real-world\ndomains. We find significant misalignment between LMs and human populations,\nthough using persona-based prompts can improve alignment. Interestingly,\nsmaller LMs tend to achieve greater psychometric alignment than larger LMs.\nFurther, training LMs on human response data from the target distribution\nenhances their psychometric alignment on unseen test items, but the\neffectiveness of such training varies across domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code and data: https://github.com/joyheyueya/psychometric-alignment",
    "pdf_url": "http://arxiv.org/pdf/2407.15645v1",
    "published_date": "2024-07-22 14:02:59 UTC",
    "updated_date": "2024-07-22 14:02:59 UTC"
  },
  {
    "arxiv_id": "2407.15621v2",
    "title": "RadioRAG: Factual large language models for enhanced diagnostics in radiology using online retrieval augmented generation",
    "authors": [
      "Soroosh Tayebi Arasteh",
      "Mahshad Lotfinia",
      "Keno Bressem",
      "Robert Siepmann",
      "Lisa Adams",
      "Dyke Ferber",
      "Christiane Kuhl",
      "Jakob Nikolas Kather",
      "Sven Nebelung",
      "Daniel Truhn"
    ],
    "abstract": "Large language models (LLMs) often generate outdated or inaccurate\ninformation based on static training datasets. Retrieval augmented generation\n(RAG) mitigates this by integrating outside data sources. While previous RAG\nsystems used pre-assembled, fixed databases with limited flexibility, we have\ndeveloped Radiology RAG (RadioRAG), an end-to-end framework that retrieves data\nfrom authoritative radiologic online sources in real-time. We evaluate the\ndiagnostic accuracy of various LLMs when answering radiology-specific questions\nwith and without access to additional online information via RAG. Using 80\nquestions from the RSNA Case Collection across radiologic subspecialties and 24\nadditional expert-curated questions with reference standard answers, LLMs\n(GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were\nprompted with and without RadioRAG in a zero-shot inference scenario RadioRAG\nretrieved context-specific information from www.radiopaedia.org in real-time.\nAccuracy was investigated. Statistical analyses were performed using\nbootstrapping. The results were further compared with human performance.\nRadioRAG improved diagnostic accuracy across most LLMs, with relative accuracy\nincreases ranging up to 54% for different LLMs. It matched or exceeded non-RAG\nmodels and the human radiologist in question answering across radiologic\nsubspecialties, particularly in breast imaging and emergency radiology.\nHowever, the degree of improvement varied among models; GPT-3.5-turbo and\nMixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2\nshowed no improvement, highlighting variability in RadioRAG's effectiveness.\nLLMs benefit when provided access to domain-specific data beyond their training\ndata. For radiology, RadioRAG establishes a robust framework that substantially\nimproves diagnostic accuracy and factuality in radiological question answering.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15621v2",
    "published_date": "2024-07-22 13:29:56 UTC",
    "updated_date": "2024-12-25 10:49:52 UTC"
  },
  {
    "arxiv_id": "2407.15617v1",
    "title": "Norface: Improving Facial Expression Analysis by Identity Normalization",
    "authors": [
      "Hanwei Liu",
      "Rudong An",
      "Zhimeng Zhang",
      "Bowen Ma",
      "Wei Zhang",
      "Yan Song",
      "Yujing Hu",
      "Wei Chen",
      "Yu Ding"
    ],
    "abstract": "Facial Expression Analysis remains a challenging task due to unexpected\ntask-irrelevant noise, such as identity, head pose, and background. To address\nthis issue, this paper proposes a novel framework, called Norface, that is\nunified for both Action Unit (AU) analysis and Facial Emotion Recognition (FER)\ntasks. Norface consists of a normalization network and a classification\nnetwork. First, the carefully designed normalization network struggles to\ndirectly remove the above task-irrelevant noise, by maintaining facial\nexpression consistency but normalizing all original images to a common identity\nwith consistent pose, and background. Then, these additional normalized images\nare fed into the classification network. Due to consistent identity and other\nfactors (e.g. head pose, background, etc.), the normalized images enable the\nclassification network to extract useful expression information more\neffectively. Additionally, the classification network incorporates a Mixture of\nExperts to refine the latent representation, including handling the input of\nfacial representations and the output of multiple (AU or emotion) labels.\nExtensive experiments validate the carefully designed framework with the\ninsight of identity normalization. The proposed method outperforms existing\nSOTA methods in multiple facial expression analysis tasks, including AU\ndetection, AU intensity estimation, and FER tasks, as well as their\ncross-dataset tasks. For the normalized datasets and code please visit\n{https://norface-fea.github.io/}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2407.15617v1",
    "published_date": "2024-07-22 13:24:32 UTC",
    "updated_date": "2024-07-22 13:24:32 UTC"
  },
  {
    "arxiv_id": "2407.15616v2",
    "title": "Sustainable broadcasting in Blockchain Networks with Reinforcement Learning",
    "authors": [
      "Danila Valko",
      "Daniel Kudenko"
    ],
    "abstract": "Recent estimates put the carbon footprint of Bitcoin and Ethereum at an\naverage of 64 and 26 million tonnes of CO2 per year, respectively. To address\nthis growing problem, several possible approaches have been proposed in the\nliterature: creating alternative blockchain consensus mechanisms, applying\nredundancy reduction techniques, utilizing renewable energy sources, and\nemploying energy-efficient devices, etc. In this paper, we follow the second\navenue and propose an efficient approach based on reinforcement learning that\nimproves the block broadcasting scheme in blockchain networks. The analysis and\nexperimental results confirmed that the proposed improvement of the block\npropagation scheme could cleverly handle network dynamics and achieve better\nresults than the default approach. Additionally, our technical integration of\nthe simulator and developed RL environment can be used as a complete solution\nfor further study of new schemes and protocols that use RL or other ML\ntechniques.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "7 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.15616v2",
    "published_date": "2024-07-22 13:24:08 UTC",
    "updated_date": "2025-04-02 20:17:42 UTC"
  },
  {
    "arxiv_id": "2407.15612v3",
    "title": "Can GPT-4 learn to analyse moves in research article abstracts?",
    "authors": [
      "Danni Yu",
      "Marina Bondi",
      "Ken Hyland"
    ],
    "abstract": "One of the most powerful and enduring ideas in written discourse analysis is\nthat genres can be described in terms of the moves which structure a writer's\npurpose. Considerable research has sought to identify these distinct\ncommunicative acts, but analyses have been beset by problems of subjectivity,\nreliability and the time-consuming need for multiple coders to confirm\nanalyses. In this paper we employ the affordances of GPT-4 to automate the\nannotation process by using natural language prompts. Focusing on abstracts\nfrom articles in four applied linguistics journals, we devise prompts which\nenable the model to identify moves effectively. The annotated outputs of these\nprompts were evaluated by two assessors with a third addressing disagreements.\nThe results show that an 8-shot prompt was more effective than one using two,\nconfirming that the inclusion of examples illustrating areas of variability can\nenhance GPT-4's ability to recognize multiple moves in a single sentence and\nreduce bias related to textual position. We suggest that GPT-4 offers\nconsiderable potential in automating this annotation process, when human actors\nwith domain specific linguistic expertise inform the prompting process.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15612v3",
    "published_date": "2024-07-22 13:14:27 UTC",
    "updated_date": "2024-11-04 13:25:31 UTC"
  },
  {
    "arxiv_id": "2407.15600v1",
    "title": "A Pairwise Comparison Relation-assisted Multi-objective Evolutionary Neural Architecture Search Method with Multi-population Mechanism",
    "authors": [
      "Yu Xue",
      "Chenchen Zhu",
      "MengChu Zhou",
      "Mohamed Wahib",
      "Moncef Gabbouj"
    ],
    "abstract": "Neural architecture search (NAS) enables re-searchers to automatically\nexplore vast search spaces and find efficient neural networks. But NAS suffers\nfrom a key bottleneck, i.e., numerous architectures need to be evaluated during\nthe search process, which requires a lot of computing resources and time. In\norder to improve the efficiency of NAS, a series of methods have been proposed\nto reduce the evaluation time of neural architectures. However, they are not\nefficient enough and still only focus on the accuracy of architectures. In\naddition to the classification accuracy, more efficient and smaller network\narchitectures are required in real-world applications. To address the above\nproblems, we propose the SMEM-NAS, a pairwise com-parison relation-assisted\nmulti-objective evolutionary algorithm based on a multi-population mechanism.\nIn the SMEM-NAS, a surrogate model is constructed based on pairwise compari-son\nrelations to predict the accuracy ranking of architectures, rather than the\nabsolute accuracy. Moreover, two populations cooperate with each other in the\nsearch process, i.e., a main population guides the evolution, while a vice\npopulation expands the diversity. Our method aims to provide high-performance\nmodels that take into account multiple optimization objectives. We conduct a\nseries of experiments on the CIFAR-10, CIFAR-100 and ImageNet datasets to\nverify its effectiveness. With only a single GPU searching for 0.17 days,\ncompetitive architectures can be found by SMEM-NAS which achieves 78.91%\naccuracy with the MAdds of 570M on the ImageNet. This work makes a significant\nadvance in the important field of NAS.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15600v1",
    "published_date": "2024-07-22 12:46:22 UTC",
    "updated_date": "2024-07-22 12:46:22 UTC"
  },
  {
    "arxiv_id": "2407.15595v2",
    "title": "Discrete Flow Matching",
    "authors": [
      "Itai Gat",
      "Tal Remez",
      "Neta Shaul",
      "Felix Kreuk",
      "Ricky T. Q. Chen",
      "Gabriel Synnaeve",
      "Yossi Adi",
      "Yaron Lipman"
    ],
    "abstract": "Despite Flow Matching and diffusion models having emerged as powerful\ngenerative paradigms for continuous variables such as images and videos, their\napplication to high-dimensional discrete data, such as language, is still\nlimited. In this work, we present Discrete Flow Matching, a novel discrete flow\nparadigm designed specifically for generating discrete data. Discrete Flow\nMatching offers several key contributions:(i) it works with a general family of\nprobability paths interpolating between source and target distributions; (ii)\nit allows for a generic formula for sampling from these probability paths using\nlearned posteriors such as the probability denoiser ($x$-prediction) and\nnoise-prediction ($\\epsilon$-prediction); (iii) practically, focusing on\nspecific probability paths defined with different schedulers improves\ngenerative perplexity compared to previous discrete diffusion and flow models;\nand (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we\nreach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6%\nPass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of generating\nhigh-quality discrete data in a non-autoregressive fashion, significantly\nclosing the gap between autoregressive models and discrete flow models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15595v2",
    "published_date": "2024-07-22 12:33:27 UTC",
    "updated_date": "2024-11-05 10:02:42 UTC"
  },
  {
    "arxiv_id": "2407.21045v1",
    "title": "Unlocking the Potential: Benchmarking Large Language Models in Water Engineering and Research",
    "authors": [
      "Boyan Xu",
      "Liang Wen",
      "Zihao Li",
      "Yuxing Yang",
      "Guanlan Wu",
      "Xiongpeng Tang",
      "Yu Li",
      "Zihao Wu",
      "Qingxian Su",
      "Xueqing Shi",
      "Yue Yang",
      "Rui Tong",
      "How Yong Ng"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir potential applications across various fields. This paper embarked on a\npivotal inquiry: Can existing LLMs effectively serve as \"water expert models\"\nfor water engineering and research tasks? This study was the first to evaluate\nLLMs' contributions across various water engineering and research tasks by\nestablishing a domain-specific benchmark suite, namely, WaterER. Herein, we\nprepared 983 tasks related to water engineering and research, categorized into\n\"wastewater treatment\", \"environmental restoration\", \"drinking water treatment\nand distribution\", \"sanitation\", \"anaerobic digestion\" and \"contaminants\nassessment\". We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5,\nGemini, GLM-4, ERNIE, QWEN and Llama3) on these tasks. We highlighted the\nstrengths of GPT-4 in handling diverse and complex tasks of water engineering\nand water research, the specialized capabilities of Gemini in academic\ncontexts, Llama3's strongest capacity to answer Chinese water engineering\nquestions and the competitive performance of Chinese-oriented models like\nGLM-4, ERNIE and QWEN in some water engineering tasks. More specifically,\ncurrent LLMs excelled particularly in generating precise research gaps for\npapers on \"contaminants and related water quality monitoring and assessment\".\nAdditionally, they were more adept at creating appropriate titles for research\npapers on \"treatment processes for wastewaters\", \"environmental restoration\",\nand \"drinking water treatment\". Overall, this study pioneered evaluating LLMs\nin water engineering and research by introducing the WaterER benchmark to\nassess the trustworthiness of their predictions. This standardized evaluation\nframework would also drive future advancements in LLM technology by using\ntargeting datasets, propelling these models towards becoming true \"water\nexpert\".",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21045v1",
    "published_date": "2024-07-22 12:32:22 UTC",
    "updated_date": "2024-07-22 12:32:22 UTC"
  },
  {
    "arxiv_id": "2407.15588v5",
    "title": "Unsupervised Robust Cross-Lingual Entity Alignment via Neighbor Triple Matching with Entity and Relation Texts",
    "authors": [
      "Soojin Yoon",
      "Sungho Ko",
      "Tongyoung Kim",
      "SeongKu Kang",
      "Jinyoung Yeo",
      "Dongha Lee"
    ],
    "abstract": "Cross-lingual entity alignment (EA) enables the integration of multiple\nknowledge graphs (KGs) across different languages, providing users with\nseamless access to diverse and comprehensive knowledge. Existing methods,\nmostly supervised, face challenges in obtaining labeled entity pairs. To\naddress this, recent studies have shifted towards self-supervised and\nunsupervised frameworks. Despite their effectiveness, these approaches have\nlimitations: (1) Relation passing: mainly focusing on the entity while\nneglecting the semantic information of relations, (2) Isomorphic assumption:\nassuming isomorphism between source and target graphs, which leads to noise and\nreduced alignment accuracy, and (3) Noise vulnerability: susceptible to noise\nin the textual features, especially when encountering inconsistent translations\nor Out-of-Vocabulary (OOV) problems. In this paper, we propose ERAlign, an\nunsupervised and robust cross-lingual EA pipeline that jointly performs\nEntity-level and Relation-level Alignment by neighbor triple matching strategy\nusing semantic textual features of relations and entities. Its refinement step\niteratively enhances results by fusing entity-level and relation-level\nalignments based on neighbor triple matching. The additional verification step\nexamines the entities' neighbor triples as the linearized text. This\nAlign-then-Verify pipeline rigorously assesses alignment results, achieving\nnear-perfect alignment even in the presence of noisy textual features of\nentities. Our extensive experiments demonstrate that the robustness and general\napplicability of ERAlign improved the accuracy and effectiveness of EA tasks,\ncontributing significantly to knowledge-oriented applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "WSDM 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.15588v5",
    "published_date": "2024-07-22 12:25:48 UTC",
    "updated_date": "2025-02-12 09:50:02 UTC"
  },
  {
    "arxiv_id": "2408.00800v2",
    "title": "Chatbot-Based Ontology Interaction Using Large Language Models and Domain-Specific Standards",
    "authors": [
      "Jonathan Reif",
      "Tom Jeleniewski",
      "Milapji Singh Gill",
      "Felix Gehlhoff",
      "Alexander Fay"
    ],
    "abstract": "The following contribution introduces a concept that employs Large Language\nModels (LLMs) and a chatbot interface to enhance SPARQL query generation for\nontologies, thereby facilitating intuitive access to formalized knowledge.\nUtilizing natural language inputs, the system converts user inquiries into\naccurate SPARQL queries that strictly query the factual content of the\nontology, effectively preventing misinformation or fabrication by the LLM. To\nenhance the quality and precision of outcomes, additional textual information\nfrom established domain-specific standards is integrated into the ontology for\nprecise descriptions of its concepts and relationships. An experimental study\nassesses the accuracy of generated SPARQL queries, revealing significant\nbenefits of using LLMs for querying ontologies and highlighting areas for\nfuture research.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
    "pdf_url": "http://arxiv.org/pdf/2408.00800v2",
    "published_date": "2024-07-22 11:58:36 UTC",
    "updated_date": "2024-10-17 09:13:18 UTC"
  },
  {
    "arxiv_id": "2407.15901v1",
    "title": "Enhancing Cognitive Workload Classification Using Integrated LSTM Layers and CNNs for fNIRS Data Analysis",
    "authors": [
      "Mehshan Ahmed Khan",
      "Houshyar Asadi",
      "Mohammad Reza Chalak Qazani",
      "Adetokunbo Arogbonlo",
      "Siamak Pedrammehr",
      "Adnan Anwar",
      "Asim Bhatti",
      "Saeid Nahavandi",
      "Chee Peng Lim"
    ],
    "abstract": "Functional near-infrared spectroscopy (fNIRS) is employed as a non-invasive\nmethod to monitor functional brain activation by capturing changes in the\nconcentrations of oxygenated haemoglobin (HbO) and deoxygenated haemo-globin\n(HbR). Various machine learning classification techniques have been utilized to\ndistinguish cognitive states. However, conventional machine learning methods,\nalthough simpler to implement, undergo a complex pre-processing phase before\nnetwork training and demonstrate reduced accuracy due to inadequate data\npreprocessing. Additionally, previous research in cog-nitive load assessment\nusing fNIRS has predominantly focused on differ-sizeentiating between two\nlevels of mental workload. These studies mainly aim to classify low and high\nlevels of cognitive load or distinguish between easy and difficult tasks. To\naddress these limitations associated with conven-tional methods, this paper\nconducts a comprehensive exploration of the im-pact of Long Short-Term Memory\n(LSTM) layers on the effectiveness of Convolutional Neural Networks (CNNs)\nwithin deep learning models. This is to address the issues related to spatial\nfeatures overfitting and lack of tem-poral dependencies in CNN in the previous\nstudies. By integrating LSTM layers, the model can capture temporal\ndependencies in the fNIRS data, al-lowing for a more comprehensive\nunderstanding of cognitive states. The primary objective is to assess how\nincorporating LSTM layers enhances the performance of CNNs. The experimental\nresults presented in this paper demonstrate that the integration of LSTM layers\nwith Convolutional layers results in an increase in the accuracy of deep\nlearning models from 97.40% to 97.92%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "conference",
    "pdf_url": "http://arxiv.org/pdf/2407.15901v1",
    "published_date": "2024-07-22 11:28:34 UTC",
    "updated_date": "2024-07-22 11:28:34 UTC"
  },
  {
    "arxiv_id": "2407.15549v2",
    "title": "Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs",
    "authors": [
      "Abhay Sheshadri",
      "Aidan Ewart",
      "Phillip Guo",
      "Aengus Lynch",
      "Cindy Wu",
      "Vivek Hebbar",
      "Henry Sleight",
      "Asa Cooper Stickland",
      "Ethan Perez",
      "Dylan Hadfield-Menell",
      "Stephen Casper"
    ],
    "abstract": "Large language models (LLMs) can often be made to behave in undesirable ways\nthat they are explicitly fine-tuned not to. For example, the LLM red-teaming\nliterature has produced a wide variety of 'jailbreaking' techniques to elicit\nharmful text from models that were fine-tuned to be harmless. Recent work on\nred-teaming, model editing, and interpretability suggests that this challenge\nstems from how (adversarial) fine-tuning largely serves to suppress rather than\nremove undesirable capabilities from LLMs. Prior work has introduced latent\nadversarial training (LAT) as a way to improve robustness to broad classes of\nfailures. These prior works have considered untargeted latent space attacks\nwhere the adversary perturbs latent activations to maximize loss on examples of\ndesirable behavior. Untargeted LAT can provide a generic type of robustness but\ndoes not leverage information about specific failure modes. Here, we experiment\nwith targeted LAT where the adversary seeks to minimize loss on a specific\ncompeting task. We find that it can augment a wide variety of state-of-the-art\nmethods. First, we use targeted LAT to improve robustness to jailbreaks,\noutperforming a strong R2D2 baseline with orders of magnitude less compute.\nSecond, we use it to more effectively remove backdoors with no knowledge of the\ntrigger. Finally, we use it to more effectively unlearn knowledge for specific\nundesirable tasks in a way that is also more robust to re-learning. Overall,\nour results suggest that targeted LAT can be an effective tool for defending\nagainst harmful behaviors from LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15549v2",
    "published_date": "2024-07-22 11:19:14 UTC",
    "updated_date": "2024-08-21 23:22:40 UTC"
  },
  {
    "arxiv_id": "2408.02378v1",
    "title": "Scaling CS1 Support with Compiler-Integrated Conversational AI",
    "authors": [
      "Jake Renzella",
      "Alexandra Vassar",
      "Lorenzo Lee Solano",
      "Andrew Taylor"
    ],
    "abstract": "This paper introduces DCC Sidekick, a web-based conversational AI tool that\nenhances an existing LLM-powered C/C++ compiler by generating educational\nprogramming error explanations. The tool seamlessly combines code display,\ncompile- and run-time error messages, and stack frame read-outs alongside an AI\ninterface, leveraging compiler error context for improved explanations. We\nanalyse usage data from a large Australian CS1 course, where 959 students\nengaged in 11,222 DCC Sidekick sessions, resulting in 17,982 error explanations\nover seven weeks. Notably, over 50% of interactions occurred outside business\nhours, underscoring the tool's value as an always-available resource. Our\nfindings reveal strong adoption of AI-assisted debugging tools, demonstrating\ntheir scalability in supporting extensive CS1 courses. We provide\nimplementation insights and recommendations for educators seeking to\nincorporate AI tools with appropriate pedagogical safeguards.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "Papers, funding sources, and Github Repositories at:\n  https://dcc.cse.unsw.edu.au/",
    "pdf_url": "http://arxiv.org/pdf/2408.02378v1",
    "published_date": "2024-07-22 10:53:55 UTC",
    "updated_date": "2024-07-22 10:53:55 UTC"
  },
  {
    "arxiv_id": "2407.15532v2",
    "title": "Large-scale Time-Varying Portfolio Optimisation using Graph Attention Networks",
    "authors": [
      "Kamesh Korangi",
      "Christophe Mues",
      "Cristián Bravo"
    ],
    "abstract": "Apart from assessing individual asset performance, investors in financial\nmarkets also need to consider how a set of firms performs collectively as a\nportfolio. Whereas traditional Markowitz-based mean-variance portfolios are\nwidespread, network-based optimisation techniques offer a more flexible tool to\ncapture complex interdependencies between asset values. However, most of the\nexisting studies do not contain firms at risk of default and remove any firms\nthat drop off indices over a certain time. This is the first study to also\nincorporate such firms in portfolio optimisation on a large scale. We propose\nand empirically test a novel method that leverages Graph Attention networks\n(GATs), a subclass of Graph Neural Networks (GNNs). GNNs, as deep\nlearning-based models, can exploit network data to uncover nonlinear\nrelationships. Their ability to handle high-dimensional data and accommodate\ncustomised layers for specific purposes makes them appealing for large-scale\nproblems such as mid- and small-cap portfolio optimisation. This study utilises\n30 years of data on mid-cap firms, creating graphs of firms using distance\ncorrelation and the Triangulated Maximally Filtered Graph approach. These\ngraphs are the inputs to a GAT model incorporating weight and allocation\nconstraints and a loss function derived from the Sharpe ratio, thus focusing on\nmaximising portfolio risk-adjusted returns. This new model is benchmarked\nagainst a network characteristic-based portfolio, a mean variance-based\nportfolio, and an equal-weighted portfolio. The results show that the portfolio\nproduced by the GAT-based model outperforms all benchmarks and is consistently\nsuperior to other strategies over a long period, while also being informative\nof market dynamics.",
    "categories": [
      "q-fin.PM",
      "cs.AI",
      "cs.SI",
      "q-fin.RM",
      "stat.ML"
    ],
    "primary_category": "q-fin.PM",
    "comment": "39 pages, 10 figures, v2",
    "pdf_url": "http://arxiv.org/pdf/2407.15532v2",
    "published_date": "2024-07-22 10:50:47 UTC",
    "updated_date": "2025-02-03 22:04:58 UTC"
  },
  {
    "arxiv_id": "2407.15527v2",
    "title": "Interpretable Concept-Based Memory Reasoning",
    "authors": [
      "David Debot",
      "Pietro Barbiero",
      "Francesco Giannini",
      "Gabriele Ciravegna",
      "Michelangelo Diligenti",
      "Giuseppe Marra"
    ],
    "abstract": "The lack of transparency in the decision-making processes of deep learning\nsystems presents a significant challenge in modern artificial intelligence\n(AI), as it impairs users' ability to rely on and verify these systems. To\naddress this challenge, Concept Bottleneck Models (CBMs) have made significant\nprogress by incorporating human-interpretable concepts into deep learning\narchitectures. This approach allows predictions to be traced back to specific\nconcept patterns that users can understand and potentially intervene on.\nHowever, existing CBMs' task predictors are not fully interpretable, preventing\na thorough analysis and any form of formal verification of their\ndecision-making process prior to deployment, thereby raising significant\nreliability concerns. To bridge this gap, we introduce Concept-based Memory\nReasoner (CMR), a novel CBM designed to provide a human-understandable and\nprovably-verifiable task prediction process. Our approach is to model each task\nprediction as a neural selection mechanism over a memory of learnable logic\nrules, followed by a symbolic evaluation of the selected rule. The presence of\nan explicit memory and the symbolic evaluation allow domain experts to inspect\nand formally verify the validity of certain global properties of interest for\nthe task prediction process. Experimental results demonstrate that CMR achieves\nbetter accuracy-interpretability trade-offs to state-of-the-art CBMs, discovers\nlogic rules consistent with ground truths, allows for rule interventions, and\nallows pre-deployment verification.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15527v2",
    "published_date": "2024-07-22 10:32:48 UTC",
    "updated_date": "2024-11-15 12:05:27 UTC"
  },
  {
    "arxiv_id": "2407.15526v2",
    "title": "Synthetic Image Learning: Preserving Performance and Preventing Membership Inference Attacks",
    "authors": [
      "Eugenio Lomurno",
      "Matteo Matteucci"
    ],
    "abstract": "Generative artificial intelligence has transformed the generation of\nsynthetic data, providing innovative solutions to challenges like data scarcity\nand privacy, which are particularly critical in fields such as medicine.\nHowever, the effective use of this synthetic data to train high-performance\nmodels remains a significant challenge. This paper addresses this issue by\nintroducing Knowledge Recycling (KR), a pipeline designed to optimise the\ngeneration and use of synthetic data for training downstream classifiers. At\nthe heart of this pipeline is Generative Knowledge Distillation (GKD), the\nproposed technique that significantly improves the quality and usefulness of\nthe information provided to classifiers through a synthetic dataset\nregeneration and soft labelling mechanism. The KR pipeline has been tested on a\nvariety of datasets, with a focus on six highly heterogeneous medical image\ndatasets, ranging from retinal images to organ scans. The results show a\nsignificant reduction in the performance gap between models trained on real and\nsynthetic data, with models based on synthetic data outperforming those trained\non real data in some cases. Furthermore, the resulting models show almost\ncomplete immunity to Membership Inference Attacks, manifesting privacy\nproperties missing in models trained with conventional techniques.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15526v2",
    "published_date": "2024-07-22 10:31:07 UTC",
    "updated_date": "2024-07-30 13:03:36 UTC"
  },
  {
    "arxiv_id": "2408.03339v1",
    "title": "The Ontoverse: Democratising Access to Knowledge Graph-based Data Through a Cartographic Interface",
    "authors": [
      "Johannes Zimmermann",
      "Dariusz Wiktorek",
      "Thomas Meusburger",
      "Miquel Monge-Dalmau",
      "Antonio Fabregat",
      "Alexander Jarasch",
      "Günter Schmidt",
      "Jorge S. Reis-Filho",
      "T. Ian Simpson"
    ],
    "abstract": "As the number of scientific publications and preprints is growing\nexponentially, several attempts have been made to navigate this complex and\nincreasingly detailed landscape. These have almost exclusively taken\nunsupervised approaches that fail to incorporate domain knowledge and lack the\nstructural organisation required for intuitive interactive human exploration\nand discovery. Especially in highly interdisciplinary fields, a deep\nunderstanding of the connectedness of research works across topics is essential\nfor generating insights. We have developed a unique approach to data navigation\nthat leans on geographical visualisation and uses hierarchically structured\ndomain knowledge to enable end-users to explore knowledge spaces grounded in\ntheir desired domains of interest. This can take advantage of existing\nontologies, proprietary intelligence schemata, or be directly derived from the\nunderlying data through hierarchical topic modelling. Our approach uses natural\nlanguage processing techniques to extract named entities from the underlying\ndata and normalise them against relevant domain references and navigational\nstructures. The knowledge is integrated by first calculating similarities\nbetween entities based on their shared extracted feature space and then by\nalignment to the navigational structures. The result is a knowledge graph that\nallows for full text and semantic graph query and structured topic driven\nnavigation. This allows end-users to identify entities relevant to their needs\nand access extensive graph analytics. The user interface facilitates graphical\ninteraction with the underlying knowledge graph and mimics a cartographic map\nto maximise ease of use and widen adoption. We demonstrate an exemplar project\nusing our generalisable and scalable infrastructure for an academic biomedical\nliterature corpus that is grounded against hundreds of different named domain\nentities.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.ET",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03339v1",
    "published_date": "2024-07-22 10:29:25 UTC",
    "updated_date": "2024-07-22 10:29:25 UTC"
  },
  {
    "arxiv_id": "2407.15899v3",
    "title": "Spatial-Temporal Cross-View Contrastive Pre-training for Check-in Sequence Representation Learning",
    "authors": [
      "Letian Gong",
      "Huaiyu Wan",
      "Shengnan Guo",
      "Xiucheng Li",
      "Yan Lin",
      "Erwen Zheng",
      "Tianyi Wang",
      "Zeyu Zhou",
      "Youfang Lin"
    ],
    "abstract": "The rapid growth of location-based services (LBS) has yielded massive amounts\nof data on human mobility. Effectively extracting meaningful representations\nfor user-generated check-in sequences is pivotal for facilitating various\ndownstream services. However, the user-generated check-in data are\nsimultaneously influenced by the surrounding objective circumstances and the\nuser's subjective intention. Specifically, the temporal uncertainty and spatial\ndiversity exhibited in check-in data make it difficult to capture the\nmacroscopic spatial-temporal patterns of users and to understand the semantics\nof user mobility activities. Furthermore, the distinct characteristics of the\ntemporal and spatial information in check-in sequences call for an effective\nfusion method to incorporate these two types of information. In this paper, we\npropose a novel Spatial-Temporal Cross-view Contrastive Representation (STCCR)\nframework for check-in sequence representation learning. Specifically, STCCR\naddresses the above challenges by employing self-supervision from \"spatial\ntopic\" and \"temporal intention\" views, facilitating effective fusion of spatial\nand temporal information at the semantic level. Besides, STCCR leverages\ncontrastive clustering to uncover users' shared spatial topics from diverse\nmobility activities, while employing angular momentum contrast to mitigate the\nimpact of temporal uncertainty and noise. We extensively evaluate STCCR on\nthree real-world datasets and demonstrate its superior performance across three\ndownstream tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted as a regular paper at IEEE TKDE",
    "pdf_url": "http://arxiv.org/pdf/2407.15899v3",
    "published_date": "2024-07-22 10:20:34 UTC",
    "updated_date": "2024-07-25 07:18:05 UTC"
  },
  {
    "arxiv_id": "2407.15523v1",
    "title": "TOM: A Development Platform For Wearable Intelligent Assistants",
    "authors": [
      "Nuwan Janaka",
      "Shengdong Zhao",
      "David Hsu",
      "Sherisse Tan Jing Wen",
      "Koh Chun Keat"
    ],
    "abstract": "Advanced digital assistants can significantly enhance task performance,\nreduce user burden, and provide personalized guidance to improve users'\nabilities. However, the development of such intelligent digital assistants\npresents a formidable challenge. To address this, we introduce TOM, a\nconceptual architecture and software platform (https://github.com/TOM-Platform)\ndesigned to support the development of intelligent wearable assistants that are\ncontextually aware of both the user and the environment. This system was\ndeveloped collaboratively with AR/MR researchers, HCI researchers, AI/Robotic\nresearchers, and software developers, and it continues to evolve to meet the\ndiverse requirements of these stakeholders. TOM facilitates the creation of\nintelligent assistive AR applications for daily activities and supports the\nrecording and analysis of user interactions, integration of new devices, and\nthe provision of assistance for various activities. Additionally, we showcase\nseveral proof-of-concept assistive services and discuss the challenges involved\nin developing such services.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "14 pages, 6 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.15523v1",
    "published_date": "2024-07-22 10:20:02 UTC",
    "updated_date": "2024-07-22 10:20:02 UTC"
  },
  {
    "arxiv_id": "2407.15520v2",
    "title": "Future-Proofing Mobile Networks: A Digital Twin Approach to Multi-Signal Management",
    "authors": [
      "Roberto Morabito",
      "Bivek Pandey",
      "Paulius Daubaris",
      "Yasith R Wanigarathna",
      "Sasu Tarkoma"
    ],
    "abstract": "Digital Twins (DTs) are set to become a key enabling technology in future\nwireless networks, with their use in network management increasing\nsignificantly. We developed a DT framework that leverages the heterogeneity of\nnetwork access technologies as a resource for enhanced network performance and\nmanagement, enabling smart data handling in the physical network. Tested in a\nCampus Area Network environment, our framework integrates diverse data sources\nto provide real-time, holistic insights into network performance and\nenvironmental sensing. We also envision that traditional analytics will evolve\nto rely on emerging AI models, such as Generative AI (GenAI), while leveraging\ncurrent analytics capabilities. This capacity can simplify analytics processes\nthrough advanced ML models, enabling descriptive, diagnostic, predictive, and\nprescriptive analytics in a unified fashion. Finally, we present specific\nresearch opportunities concerning interoperability aspects and envision\naligning advancements in DT technology with evolved AI integration.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "A shortened version of this paper is currently under review for\n  publication in an IEEE magazine. If accepted, the copyright will be\n  transferred to IEEE",
    "pdf_url": "http://arxiv.org/pdf/2407.15520v2",
    "published_date": "2024-07-22 10:13:46 UTC",
    "updated_date": "2024-08-06 07:25:16 UTC"
  },
  {
    "arxiv_id": "2407.15512v2",
    "title": "Increasing the Robustness of Model Predictions to Missing Sensors in Earth Observation",
    "authors": [
      "Francisco Mena",
      "Diego Arenas",
      "Andreas Dengel"
    ],
    "abstract": "Multi-sensor ML models for EO aim to enhance prediction accuracy by\nintegrating data from various sources. However, the presence of missing data\nposes a significant challenge, particularly in non-persistent sensors that can\nbe affected by external factors. Existing literature has explored strategies\nlike temporal dropout and sensor-invariant models to address the generalization\nto missing data issues. Inspired by these works, we study two novel methods\ntailored for multi-sensor scenarios, namely Input Sensor Dropout (ISensD) and\nEnsemble Sensor Invariant (ESensI). Through experimentation on three\nmulti-sensor temporal EO datasets, we demonstrate that these methods\neffectively increase the robustness of model predictions to missing sensors.\nParticularly, we focus on how the predictive performance of models drops when\nsensors are missing at different levels. We observe that ensemble multi-sensor\nmodels are the most robust to the lack of sensors. In addition, the sensor\ndropout component in ISensD shows promising robustness results.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the MACLEAN workshop in the ECML/PKDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.15512v2",
    "published_date": "2024-07-22 09:58:29 UTC",
    "updated_date": "2024-09-04 11:01:47 UTC"
  },
  {
    "arxiv_id": "2407.15510v1",
    "title": "Algebraic anti-unification",
    "authors": [
      "Christian Antić"
    ],
    "abstract": "Abstraction is key to human and artificial intelligence as it allows one to\nsee common structure in otherwise distinct objects or situations and as such it\nis a key element for generality in AI. Anti-unification (or generalization) is\n\\textit{the} part of theoretical computer science and AI studying abstraction.\nIt has been successfully applied to various AI-related problems, most\nimportantly inductive logic programming. Up to this date, anti-unification is\nstudied only from a syntactic perspective in the literature. The purpose of\nthis paper is to initiate an algebraic (i.e. semantic) theory of\nanti-unification within general algebras. This is motivated by recent\napplications to similarity and analogical proportions.",
    "categories": [
      "cs.AI",
      "cs.DM",
      "cs.LO",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15510v1",
    "published_date": "2024-07-22 09:49:46 UTC",
    "updated_date": "2024-07-22 09:49:46 UTC"
  },
  {
    "arxiv_id": "2407.15508v3",
    "title": "Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners",
    "authors": [
      "Yifei Gao",
      "Jie Ou",
      "Lei Wang",
      "Jun Cheng",
      "Mengchu Zhou"
    ],
    "abstract": "The quantization of large language models (LLMs) has been a prominent\nresearch area aimed at enabling their lightweight deployment in practice.\nExisting research about LLM's quantization has mainly explored the interplay\nbetween weights and activations, or employing auxiliary components while\nneglecting the necessity of adjusting weights during quantization.\nConsequently, original weight distributions frequently fail to yield desired\nresults after round-to-nearest (RTN) quantization. Even though incorporating\ntechniques such as mixed precision and low-rank error approximation in LLM's\nquantization can yield improved results, they inevitably introduce additional\ncomputational overhead. On the other hand, traditional techniques for weight\nquantization, such as Generative Post-Training Quantization, rely on manually\ntweaking weight distributions to minimize local errors, but they fall short of\nachieving globally optimal outcomes. Although the recently proposed Learnable\nSingular-value Increment improves global weight quantization by modifying\nweight distributions, it disrupts the original distribution considerably. This\nintroduces pronounced bias toward the training data and can degrade downstream\ntask performance. In this paper, we introduce Singular-value Diagonal\nExpansion, a more nuanced approach to refining weight distributions to achieve\nbetter quantization alignment. Furthermore, we introduce Cross-layer Learning\nthat improves overall quantization outcomes by distributing errors more evenly\nacross layers. Our plug-and-play weight-quantization methods demonstrate\nsubstantial performance improvements over state-of-the-art approaches,\nincluding OmniQuant, DuQuant, and PrefixQuant.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Effecient Quantization Methods for LLMs",
    "pdf_url": "http://arxiv.org/pdf/2407.15508v3",
    "published_date": "2024-07-22 09:45:16 UTC",
    "updated_date": "2025-05-15 05:34:45 UTC"
  },
  {
    "arxiv_id": "2408.03948v1",
    "title": "A Survey of AI Reliance",
    "authors": [
      "Sven Eckhardt",
      "Niklas Kühl",
      "Mateusz Dolata",
      "Gerhard Schwabe"
    ],
    "abstract": "Artificial intelligence (AI) systems have become an indispensable component\nof modern technology. However, research on human behavioral responses is\nlagging behind, i.e., the research into human reliance on AI advice (AI\nreliance). Current shortcomings in the literature include the unclear\ninfluences on AI reliance, lack of external validity, conflicting approaches to\nmeasuring reliance, and disregard for a change in reliance over time. Promising\navenues for future research include reliance on generative AI output and\nreliance in multi-user situations. In conclusion, we present a morphological\nbox that serves as a guide for research on AI reliance.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03948v1",
    "published_date": "2024-07-22 09:34:58 UTC",
    "updated_date": "2024-07-22 09:34:58 UTC"
  },
  {
    "arxiv_id": "2407.15487v1",
    "title": "In-Context Learning Improves Compositional Understanding of Vision-Language Models",
    "authors": [
      "Matteo Nulli",
      "Anesa Ibrahimi",
      "Avik Pal",
      "Hoshe Lee",
      "Ivona Najdenkoska"
    ],
    "abstract": "Vision-Language Models (VLMs) have shown remarkable capabilities in a large\nnumber of downstream tasks. Nonetheless, compositional image understanding\nremains a rather difficult task due to the object bias present in training\ndata. In this work, we investigate the reasons for such a lack of capability by\nperforming an extensive bench-marking of compositional understanding in VLMs.\nWe compare contrastive models with generative ones and analyze their\ndifferences in architecture, pre-training data, and training tasks and losses.\nFurthermore, we leverage In-Context Learning (ICL) as a way to improve the\nability of VLMs to perform more complex reasoning and understanding given an\nimage. Our extensive experiments demonstrate that our proposed approach\noutperforms baseline models across multiple compositional understanding\ndatasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15487v1",
    "published_date": "2024-07-22 09:03:29 UTC",
    "updated_date": "2024-07-22 09:03:29 UTC"
  },
  {
    "arxiv_id": "2407.15475v2",
    "title": "Autonomous Robotic Swarms: A Corroborative Approach for Verification and Validation",
    "authors": [
      "Dhaminda B. Abeywickrama",
      "Suet Lee",
      "Chris Bennett",
      "Razanne Abu-Aisheh",
      "Tom Didiot-Cook",
      "Simon Jones",
      "Sabine Hauert",
      "Kerstin Eder"
    ],
    "abstract": "The emergent behaviour of autonomous robotic swarms poses a significant\nchallenge to their safety assurance. Assurance tasks encompass adherence to\nstandards, certification processes, and the execution of verification and\nvalidation (V&V) methods, such as model checking. In this study, we propose a\ncorroborative approach for formally verifying and validating autonomous robotic\nswarms, which are defined at the macroscopic formal modelling, low-fidelity\nsimulation, high-fidelity simulation, and real-robot levels. Our formal\nmacroscopic models, used for verification, are characterised by data derived\nfrom actual simulations to ensure both accuracy and traceability across\ndifferent swarm system models. Furthermore, our work combines formal\nverification with simulations and experimental validation using real robots. In\nthis way, our corroborative approach for V&V seeks to enhance confidence in the\nevidence, in contrast to employing these methods separately. We explore our\napproach through a case study focused on a swarm of robots operating within a\npublic cloakroom.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.9; D.2; I.6"
    ],
    "primary_category": "cs.RO",
    "comment": "Updated Data Repository and Abstract. 15 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.15475v2",
    "published_date": "2024-07-22 08:40:05 UTC",
    "updated_date": "2025-02-28 20:59:51 UTC"
  },
  {
    "arxiv_id": "2407.15431v1",
    "title": "Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs",
    "authors": [
      "Huanjing Zhao",
      "Beining Yang",
      "Yukuo Cen",
      "Junyu Ren",
      "Chenhui Zhang",
      "Yuxiao Dong",
      "Evgeny Kharlamov",
      "Shu Zhao",
      "Jie Tang"
    ],
    "abstract": "The text-attributed graph (TAG) is one kind of important real-world\ngraph-structured data with each node associated with raw texts. For TAGs,\ntraditional few-shot node classification methods directly conduct training on\nthe pre-processed node features and do not consider the raw texts. The\nperformance is highly dependent on the choice of the feature pre-processing\nmethod. In this paper, we propose P2TAG, a framework designed for few-shot node\nclassification on TAGs with graph pre-training and prompting. P2TAG first\npre-trains the language model (LM) and graph neural network (GNN) on TAGs with\nself-supervised loss. To fully utilize the ability of language models, we adapt\nthe masked language modeling objective for our framework. The pre-trained model\nis then used for the few-shot node classification with a mixed prompt method,\nwhich simultaneously considers both text and graph information. We conduct\nexperiments on six real-world TAGs, including paper citation networks and\nproduct co-purchasing networks. Experimental results demonstrate that our\nproposed framework outperforms existing graph few-shot learning methods on\nthese datasets with +18.98% ~ +35.98% improvements.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "Accepted to KDD'24",
    "pdf_url": "http://arxiv.org/pdf/2407.15431v1",
    "published_date": "2024-07-22 07:24:21 UTC",
    "updated_date": "2024-07-22 07:24:21 UTC"
  },
  {
    "arxiv_id": "2408.03337v4",
    "title": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for Psychological Measurements",
    "authors": [
      "Xueyan Li",
      "Xinyan Chen",
      "Yazhe Niu",
      "Shuai Hu",
      "Yu Liu"
    ],
    "abstract": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "29 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.03337v4",
    "published_date": "2024-07-22 07:19:12 UTC",
    "updated_date": "2025-01-16 07:40:27 UTC"
  },
  {
    "arxiv_id": "2407.15428v1",
    "title": "Decoding BACnet Packets: A Large Language Model Approach for Packet Interpretation",
    "authors": [
      "Rashi Sharma",
      "Hiroyuki Okada",
      "Tatsumi Oba",
      "Karthikk Subramanian",
      "Naoto Yanai",
      "Sugiri Pranata"
    ],
    "abstract": "The Industrial Control System (ICS) environment encompasses a wide range of\nintricate communication protocols, posing substantial challenges for Security\nOperations Center (SOC) analysts tasked with monitoring, interpreting, and\naddressing network activities and security incidents. Conventional monitoring\ntools and techniques often struggle to provide a clear understanding of the\nnature and intent of ICS-specific communications. To enhance comprehension, we\npropose a software solution powered by a Large Language Model (LLM). This\nsolution currently focused on BACnet protocol, processes a packet file data and\nextracts context by using a mapping database, and contemporary context\nretrieval methods for Retrieval Augmented Generation (RAG). The processed\npacket information, combined with the extracted context, serves as input to the\nLLM, which generates a concise packet file summary for the user. The software\ndelivers a clear, coherent, and easily understandable summary of network\nactivities, enabling SOC analysts to better assess the current state of the\ncontrol system.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.15428v1",
    "published_date": "2024-07-22 07:15:49 UTC",
    "updated_date": "2024-07-22 07:15:49 UTC"
  },
  {
    "arxiv_id": "2407.15427v1",
    "title": "YOLO-pdd: A Novel Multi-scale PCB Defect Detection Method Using Deep Representations with Sequential Images",
    "authors": [
      "Bowen Liu",
      "Dongjie Chen",
      "Xiao Qi"
    ],
    "abstract": "With the rapid growth of the PCB manufacturing industry, there is an\nincreasing demand for computer vision inspection to detect defects during\nproduction. Improving the accuracy and generalization of PCB defect detection\nmodels remains a significant challenge. This paper proposes a high-precision,\nrobust, and real-time end-to-end method for PCB defect detection based on deep\nConvolutional Neural Networks (CNN). Traditional methods often suffer from low\naccuracy and limited applicability. We propose a novel approach combining\nYOLOv5 and multiscale modules for hierarchical residual-like connections. In\nPCB defect detection, noise can confuse the background and small targets. The\nYOLOv5 model provides a strong foundation with its real-time processing and\naccurate object detection capabilities. The multi-scale module extends\ntraditional approaches by incorporating hierarchical residual-like connections\nwithin a single block, enabling multiscale feature extraction. This\nplug-and-play module significantly enhances performance by extracting features\nat multiple scales and levels, which are useful for identifying defects of\nvarying sizes and complexities. Our multi-scale architecture integrates feature\nextraction, defect localization, and classification into a unified network.\nExperiments on a large-scale PCB dataset demonstrate significant improvements\nin precision, recall, and F1-score compared to existing methods. This work\nadvances computer vision inspection for PCB defect detection, providing a\nreliable solution for high-precision, robust, real-time, and domain-adaptive\ndefect detection in the PCB manufacturing industry.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15427v1",
    "published_date": "2024-07-22 07:08:22 UTC",
    "updated_date": "2024-07-22 07:08:22 UTC"
  },
  {
    "arxiv_id": "2407.17524v1",
    "title": "StreamTinyNet: video streaming analysis with spatial-temporal TinyML",
    "authors": [
      "Hazem Hesham Yousef Shalby",
      "Massimo Pavan",
      "Manuel Roveri"
    ],
    "abstract": "Tiny Machine Learning (TinyML) is a branch of Machine Learning (ML) that\nconstitutes a bridge between the ML world and the embedded system ecosystem\n(i.e., Internet of Things devices, embedded devices, and edge computing units),\nenabling the execution of ML algorithms on devices constrained in terms of\nmemory, computational capabilities, and power consumption. Video Streaming\nAnalysis (VSA), one of the most interesting tasks of TinyML, consists in\nscanning a sequence of frames in a streaming manner, with the goal of\nidentifying interesting patterns. Given the strict constraints of these tiny\ndevices, all the current solutions rely on performing a frame-by-frame\nanalysis, hence not exploiting the temporal component in the stream of data. In\nthis paper, we present StreamTinyNet, the first TinyML architecture to perform\nmultiple-frame VSA, enabling a variety of use cases that requires\nspatial-temporal analysis that were previously impossible to be carried out at\na TinyML level. Experimental results on public-available datasets show the\neffectiveness and efficiency of the proposed solution. Finally, StreamTinyNet\nhas been ported and tested on the Arduino Nicla Vision, showing the feasibility\nof what proposed.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "this paper has been accepted and presented at the WCCI24 conference",
    "pdf_url": "http://arxiv.org/pdf/2407.17524v1",
    "published_date": "2024-07-22 07:08:03 UTC",
    "updated_date": "2024-07-22 07:08:03 UTC"
  },
  {
    "arxiv_id": "2407.15425v2",
    "title": "Empirical Capacity Model for Self-Attention Neural Networks",
    "authors": [
      "Aki Härmä",
      "Marcin Pietrasik",
      "Anna Wilbik"
    ],
    "abstract": "Large pretrained self-attention neural networks, or transformers, have been\nvery successful in various tasks recently. The performance of a model on a\ngiven task depends on its ability to memorize and generalize the training data.\nLarge transformer models, which may have billions of parameters, in theory have\na huge capacity to memorize content. However, the current algorithms for the\noptimization fall short of the theoretical capacity, and the capacity is also\nhighly dependent on the content. In this paper, we focus on the memory capacity\nof these models obtained using common training algorithms and synthetic\ntraining data. Based on the results, we derive an empirical capacity model\n(ECM) for a generic transformer. The ECM can be used to design task-specific\ntransformer models with an optimal number of parameters in cases where the\ntarget memorization capability of the task can be defined.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to BNAIC'24, 14 pages + refs",
    "pdf_url": "http://arxiv.org/pdf/2407.15425v2",
    "published_date": "2024-07-22 07:02:15 UTC",
    "updated_date": "2024-07-31 10:27:37 UTC"
  },
  {
    "arxiv_id": "2407.15423v2",
    "title": "Integrating IP Broadcasting with Audio Tags: Workflow and Challenges",
    "authors": [
      "Rhys Burchett-Vass",
      "Arshdeep Singh",
      "Gabriel Bibbó",
      "Mark D. Plumbley"
    ],
    "abstract": "The broadcasting industry is increasingly adopting IP techniques,\nrevolutionising both live and pre-recorded content production, from news\ngathering to live music events. IP broadcasting allows for the transport of\naudio and video signals in an easily configurable way, aligning with modern\nnetworking techniques. This shift towards an IP workflow allows for much\ngreater flexibility, not only in routing signals but with the integration of\ntools using standard web development techniques. One possible tool could\ninclude the use of live audio tagging, which has a number of uses in the\nproduction of content. These include from automated closed captioning to\nidentifying unwanted sound events within a scene. In this paper, we describe\nthe process of containerising an audio tagging model into a microservice, a\nsmall segregated code module that can be integrated into a multitude of\ndifferent network setups. The goal is to develop a modular, accessible, and\nflexible tool capable of seamless deployment into broadcasting workflows of all\nsizes, from small productions to large corporations. Challenges surrounding\nlatency of the selected audio tagging model and its effect on the usefulness of\nthe end product are discussed.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Submitted to DCASE 2024 Workshop",
    "pdf_url": "http://arxiv.org/pdf/2407.15423v2",
    "published_date": "2024-07-22 07:00:21 UTC",
    "updated_date": "2024-07-23 08:46:03 UTC"
  },
  {
    "arxiv_id": "2407.15421v2",
    "title": "Planning in a recurrent neural network that plays Sokoban",
    "authors": [
      "Mohammad Taufeeque",
      "Philip Quirke",
      "Maximilian Li",
      "Chris Cundy",
      "Aaron David Tucker",
      "Adam Gleave",
      "Adrià Garriga-Alonso"
    ],
    "abstract": "How a neural network (NN) generalizes to novel situations depends on whether\nit has learned to select actions heuristically or via a planning process. \"An\ninvestigation of model-free planning\" (Guez et al. 2019) found that a recurrent\nNN (RNN) trained to play Sokoban appears to plan, with extra computation steps\nimproving the RNN's success rate. We replicate and expand on their behavioral\nanalysis, finding the RNN learns to give itself extra computation steps in\ncomplex situations by \"pacing\" in cycles. Moreover, we train linear probes that\npredict the future actions taken by the network and find that intervening on\nthe hidden state using these probes controls the agent's subsequent actions.\nLeveraging these insights, we perform model surgery, enabling the convolutional\nNN to generalize beyond its 10x10 architectural limit to arbitrarily sized\ninputs. The resulting model solves challenging, highly off-distribution levels.\nWe open-source our model and code, and believe the neural network's small size\n(1.29M parameters) makes it an excellent model organism to deepen our\nunderstanding of learned planning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Mechanistic Interpretability workshop, ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.15421v2",
    "published_date": "2024-07-22 06:57:34 UTC",
    "updated_date": "2024-10-24 18:06:19 UTC"
  },
  {
    "arxiv_id": "2408.03946v1",
    "title": "Prompting for products: Investigating design space exploration strategies for text-to-image generative models",
    "authors": [
      "Leah Chong",
      "I-Ping Lo",
      "Jude Rayan",
      "Steven Dow",
      "Faez Ahmed",
      "Ioanna Lykourentzou"
    ],
    "abstract": "Text-to-image models are enabling efficient design space exploration, rapidly\ngenerating images from text prompts. However, many generative AI tools are\nimperfect for product design applications as they are not built for the goals\nand requirements of product design. The unclear link between text input and\nimage output further complicates their application. This work empirically\ninvestigates design space exploration strategies that can successfully yield\nproduct images that are feasible, novel, and aesthetic, which are three common\ngoals in product design. Specifically, user actions within the global and local\nediting modes, including their time spent, prompt length, mono vs.\nmulti-criteria prompts, and goal orientation of prompts, are analyzed. Key\nfindings reveal the pivotal role of mono vs. multi-criteria and goal\norientation of prompts in achieving specific design goals over time and prompt\nlength. The study recommends prioritizing the use of multi-criteria prompts for\nfeasibility and novelty during global editing, while favoring mono-criteria\nprompts for aesthetics during local editing. Overall, this paper underscores\nthe nuanced relationship between the AI-driven text-to-image models and their\neffectiveness in product design, urging designers to carefully structure\nprompts during different editing modes to better meet the unique demands of\nproduct design.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.HC",
    "comment": "12 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.03946v1",
    "published_date": "2024-07-22 06:42:04 UTC",
    "updated_date": "2024-07-22 06:42:04 UTC"
  },
  {
    "arxiv_id": "2408.03945v1",
    "title": "Impacts of Anthropomorphizing Large Language Models in Learning Environments",
    "authors": [
      "Kristina Schaaff",
      "Marc-André Heidelmann"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly being used in learning\nenvironments to support teaching-be it as learning companions or as tutors.\nWith our contribution, we aim to discuss the implications of the\nanthropomorphization of LLMs in learning environments on educational theory to\nbuild a foundation for more effective learning outcomes and understand their\nemotional impact on learners. According to the media equation, people tend to\nrespond to media in the same way as they would respond to another person. A\nstudy conducted by the Georgia Institute of Technology showed that chatbots can\nbe successfully implemented in learning environments. In this study, learners\nin selected online courses were unable to distinguish the chatbot from a \"real\"\nteacher. As LLM-based chatbots such as OpenAI's GPT series are increasingly\nused in educational tools, it is important to understand how the attribution\nprocesses to LLM-based chatbots in terms of anthropomorphization affect\nlearners' emotions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Presented at Affective Computing Pre-Conference at ISRE 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.03945v1",
    "published_date": "2024-07-22 06:28:54 UTC",
    "updated_date": "2024-07-22 06:28:54 UTC"
  },
  {
    "arxiv_id": "2407.15406v1",
    "title": "Automated Road Safety: Enhancing Sign and Surface Damage Detection with AI",
    "authors": [
      "Davide Merolla",
      "Vittorio Latorre",
      "Antonio Salis",
      "Gianluca Boanelli"
    ],
    "abstract": "Public transportation plays a crucial role in our lives, and the road network\nis a vital component in the implementation of smart cities. Recent advancements\nin AI have enabled the development of advanced monitoring systems capable of\ndetecting anomalies in road surfaces and road signs, which, if unaddressed, can\nlead to serious road accidents. This paper presents an innovative approach to\nenhance road safety through the detection and classification of traffic signs\nand road surface damage using advanced deep learning techniques. This\nintegrated approach supports proactive maintenance strategies, improving road\nsafety and resource allocation for the Molise region and the city of\nCampobasso. The resulting system, developed as part of the Casa delle\nTecnologie Emergenti (House of Emergent Technologies) Molise (Molise CTE)\nresearch project funded by the Italian Minister of Economic Growth (MIMIT),\nleverages cutting-edge technologies such as Cloud Computing and High\nPerformance Computing with GPU utilization. It serves as a valuable tool for\nmunicipalities, enabling quick detection of anomalies and the prompt\norganization of maintenance operations",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "16 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.15406v1",
    "published_date": "2024-07-22 06:22:36 UTC",
    "updated_date": "2024-07-22 06:22:36 UTC"
  },
  {
    "arxiv_id": "2407.15017v4",
    "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
    "authors": [
      "Mengru Wang",
      "Yunzhi Yao",
      "Ziwen Xu",
      "Shuofei Qiao",
      "Shumin Deng",
      "Peng Wang",
      "Xiang Chen",
      "Jia-Chen Gu",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "abstract": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Findings; 39 pages (v4)",
    "pdf_url": "http://arxiv.org/pdf/2407.15017v4",
    "published_date": "2024-07-22 06:15:59 UTC",
    "updated_date": "2024-12-04 09:54:59 UTC"
  },
  {
    "arxiv_id": "2407.15403v1",
    "title": "Offline Imitation Learning Through Graph Search and Retrieval",
    "authors": [
      "Zhao-Heng Yin",
      "Pieter Abbeel"
    ],
    "abstract": "Imitation learning is a powerful machine learning algorithm for a robot to\nacquire manipulation skills. Nevertheless, many real-world manipulation tasks\ninvolve precise and dexterous robot-object interactions, which make it\ndifficult for humans to collect high-quality expert demonstrations. As a\nresult, a robot has to learn skills from suboptimal demonstrations and\nunstructured interactions, which remains a key challenge. Existing works\ntypically use offline deep reinforcement learning (RL) to solve this challenge,\nbut in practice these algorithms are unstable and fragile due to the deadly\ntriad issue. To overcome this problem, we propose GSR, a simple yet effective\nalgorithm that learns from suboptimal demonstrations through Graph Search and\nRetrieval. We first use pretrained representation to organize the interaction\nexperience into a graph and perform a graph search to calculate the values of\ndifferent behaviors. Then, we apply a retrieval-based procedure to identify the\nbest behavior (actions) on each state and use behavior cloning to learn that\nbehavior. We evaluate our method in both simulation and real-world robotic\nmanipulation tasks with complex visual inputs, covering various precise and\ndexterous manipulation skills with objects of different physical properties.\nGSR can achieve a 10% to 30% higher success rate and over 30% higher\nproficiency compared to baselines. Our project page is at\nhttps://zhaohengyin.github.io/gsr.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Robotics: Science and Systems (RSS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.15403v1",
    "published_date": "2024-07-22 06:12:21 UTC",
    "updated_date": "2024-07-22 06:12:21 UTC"
  },
  {
    "arxiv_id": "2407.15402v1",
    "title": "Tackling Selfish Clients in Federated Learning",
    "authors": [
      "Andrea Augello",
      "Ashish Gupta",
      "Giuseppe Lo Re",
      "Sajal K. Das"
    ],
    "abstract": "Federated Learning (FL) is a distributed machine learning paradigm\nfacilitating participants to collaboratively train a model without revealing\ntheir local data. However, when FL is deployed into the wild, some intelligent\nclients can deliberately deviate from the standard training process to make the\nglobal model inclined toward their local model, thereby prioritizing their\nlocal data distribution. We refer to this novel category of misbehaving clients\nas selfish. In this paper, we propose a Robust aggregation strategy for FL\nserver to mitigate the effect of Selfishness (in short RFL-Self). RFL-Self\nincorporates an innovative method to recover (or estimate) the true updates of\nselfish clients from the received ones, leveraging robust statistics (median of\nnorms) of the updates at every round. By including the recovered updates in\naggregation, our strategy offers strong robustness against selfishness. Our\nexperimental results, obtained on MNIST and CIFAR-10 datasets, demonstrate that\njust 2% of clients behaving selfishly can decrease the accuracy by up to 36%,\nand RFL-Self can mitigate that effect without degrading the global model\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 16 figures. European Conference on Artificial Intelligence\n  (ECAI) 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.15402v1",
    "published_date": "2024-07-22 06:08:13 UTC",
    "updated_date": "2024-07-22 06:08:13 UTC"
  },
  {
    "arxiv_id": "2407.15399v1",
    "title": "Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models",
    "authors": [
      "Xiao Liu",
      "Liangzhi Li",
      "Tong Xiang",
      "Fuying Ye",
      "Lu Wei",
      "Wangyue Li",
      "Noa Garcia"
    ],
    "abstract": "With the development of large language models (LLMs) like ChatGPT, both their\nvast applications and potential vulnerabilities have come to the forefront.\nWhile developers have integrated multiple safety mechanisms to mitigate their\nmisuse, a risk remains, particularly when models encounter adversarial inputs.\nThis study unveils an attack mechanism that capitalizes on human conversation\nstrategies to extract harmful information from LLMs. We delineate three pivotal\nstrategies: (i) decomposing malicious questions into seemingly innocent\nsub-questions; (ii) rewriting overtly malicious questions into more covert,\nbenign-sounding ones; (iii) enhancing the harmfulness of responses by prompting\nmodels for illustrative examples. Unlike conventional methods that target\nexplicit malicious responses, our approach delves deeper into the nature of the\ninformation provided in responses. Through our experiments conducted on\nGPT-3.5-turbo, GPT-4, and Llama2, our method has demonstrated a marked efficacy\ncompared to conventional attack methods. In summary, this work introduces a\nnovel attack method that outperforms previous approaches, raising an important\nquestion: How to discern whether the ultimate intent in a dialogue is\nmalicious?",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15399v1",
    "published_date": "2024-07-22 06:04:29 UTC",
    "updated_date": "2024-07-22 06:04:29 UTC"
  },
  {
    "arxiv_id": "2407.15396v2",
    "title": "Semantic Diversity-aware Prototype-based Learning for Unbiased Scene Graph Generation",
    "authors": [
      "Jaehyeong Jeon",
      "Kibum Kim",
      "Kanghoon Yoon",
      "Chanyoung Park"
    ],
    "abstract": "The scene graph generation (SGG) task involves detecting objects within an\nimage and predicting predicates that represent the relationships between the\nobjects. However, in SGG benchmark datasets, each subject-object pair is\nannotated with a single predicate even though a single predicate may exhibit\ndiverse semantics (i.e., semantic diversity), existing SGG models are trained\nto predict the one and only predicate for each pair. This in turn results in\nthe SGG models to overlook the semantic diversity that may exist in a\npredicate, thus leading to biased predictions. In this paper, we propose a\nnovel model-agnostic Semantic Diversity-aware Prototype-based Learning (DPL)\nframework that enables unbiased predictions based on the understanding of the\nsemantic diversity of predicates. Specifically, DPL learns the regions in the\nsemantic space covered by each predicate to distinguish among the various\ndifferent semantics that a single predicate can represent. Extensive\nexperiments demonstrate that our proposed model-agnostic DPL framework brings\nsignificant performance improvement on existing SGG models, and also\neffectively understands the semantic diversity of predicates.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.15396v2",
    "published_date": "2024-07-22 05:53:46 UTC",
    "updated_date": "2024-07-25 12:54:52 UTC"
  },
  {
    "arxiv_id": "2407.15390v1",
    "title": "ALLaM: Large Language Models for Arabic and English",
    "authors": [
      "M Saiful Bari",
      "Yazeed Alnumay",
      "Norah A. Alzahrani",
      "Nouf M. Alotaibi",
      "Hisham A. Alyahya",
      "Sultan AlRashed",
      "Faisal A. Mirza",
      "Shaykhah Z. Alsubaie",
      "Hassan A. Alahmed",
      "Ghadah Alabduljabbar",
      "Raghad Alkhathran",
      "Yousef Almushayqih",
      "Raneem Alnajim",
      "Salman Alsubaihi",
      "Maryam Al Mansour",
      "Majed Alrubaian",
      "Ali Alammari",
      "Zaki Alawami",
      "Abdulmohsen Al-Thubaity",
      "Ahmed Abdelali",
      "Jeril Kuriakose",
      "Abdalghani Abujabal",
      "Nora Al-Twairesh",
      "Areeb Alowisheq",
      "Haidar Khan"
    ],
    "abstract": "We present ALLaM: Arabic Large Language Model, a series of large language\nmodels to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is\ncarefully trained considering the values of language alignment and knowledge\ntransfer at scale. Our autoregressive decoder-only architecture models\ndemonstrate how second-language acquisition via vocabulary expansion and\npretraining on a mixture of Arabic and English text can steer a model towards a\nnew language (Arabic) without any catastrophic forgetting in the original\nlanguage (English). Furthermore, we highlight the effectiveness of using\nparallel/translated data to aid the process of knowledge alignment between\nlanguages. Finally, we show that extensive alignment with human preferences can\nsignificantly enhance the performance of a language model compared to models of\na larger scale with lower quality alignment. ALLaM achieves state-of-the-art\nperformance in various Arabic benchmarks, including MMLU Arabic, ACVA, and\nArabic Exams. Our aligned models improve both in Arabic and English from their\nbase aligned models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15390v1",
    "published_date": "2024-07-22 05:35:17 UTC",
    "updated_date": "2024-07-22 05:35:17 UTC"
  },
  {
    "arxiv_id": "2407.15385v1",
    "title": "Towards Robust Vision Transformer via Masked Adaptive Ensemble",
    "authors": [
      "Fudong Lin",
      "Jiadong Lou",
      "Xu Yuan",
      "Nian-Feng Tzeng"
    ],
    "abstract": "Adversarial training (AT) can help improve the robustness of Vision\nTransformers (ViT) against adversarial attacks by intentionally injecting\nadversarial examples into the training data. However, this way of adversarial\ninjection inevitably incurs standard accuracy degradation to some extent,\nthereby calling for a trade-off between standard accuracy and robustness.\nBesides, the prominent AT solutions are still vulnerable to adaptive attacks.\nTo tackle such shortcomings, this paper proposes a novel ViT architecture,\nincluding a detector and a classifier bridged by our newly developed adaptive\nensemble. Specifically, we empirically discover that detecting adversarial\nexamples can benefit from the Guided Backpropagation technique. Driven by this\ndiscovery, a novel Multi-head Self-Attention (MSA) mechanism is introduced to\nenhance our detector to sniff adversarial examples. Then, a classifier with two\nencoders is employed for extracting visual representations respectively from\nclean images and adversarial examples, with our adaptive ensemble to adaptively\nadjust the proportion of visual representations from the two encoders for\naccurate classification. This design enables our ViT architecture to achieve a\nbetter trade-off between standard accuracy and robustness. Besides, our\nadaptive ensemble technique allows us to mask off a random subset of image\npatches within input data, boosting our ViT's robustness against adaptive\nattacks, while maintaining high standard accuracy. Experimental results exhibit\nthat our ViT architecture, on CIFAR-10, achieves the best standard accuracy and\nadversarial robustness of 90.3% and 49.8%, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.15385v1",
    "published_date": "2024-07-22 05:28:29 UTC",
    "updated_date": "2024-07-22 05:28:29 UTC"
  },
  {
    "arxiv_id": "2407.15366v1",
    "title": "Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias",
    "authors": [
      "Rongwu Xu",
      "Zi'an Zhou",
      "Tianwei Zhang",
      "Zehan Qi",
      "Su Yao",
      "Ke Xu",
      "Wei Xu",
      "Han Qiu"
    ],
    "abstract": "The common toxicity and societal bias in contents generated by large language\nmodels (LLMs) necessitate strategies to reduce harm. Present solutions often\ndemand white-box access to the model or substantial training, which is\nimpractical for cutting-edge commercial LLMs. Moreover, prevailing prompting\nmethods depend on external tool feedback and fail to simultaneously lessen\ntoxicity and bias. Motivated by social psychology principles, we propose a\nnovel strategy named \\textbf{perspective-taking prompting (\\textsc{PeT})} that\ninspires LLMs to integrate diverse human perspectives and self-regulate their\nresponses. This self-correction mechanism can significantly diminish toxicity\n(up to $89\\%$) and bias (up to $73\\%$) in LLMs' responses. Rigorous evaluations\nand ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and\nthree open-source LLMs, revealing \\textsc{PeT}'s superiority in producing less\nharmful responses, outperforming five strong baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15366v1",
    "published_date": "2024-07-22 04:25:01 UTC",
    "updated_date": "2024-07-22 04:25:01 UTC"
  },
  {
    "arxiv_id": "2407.15362v3",
    "title": "A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model",
    "authors": [
      "Yingxue Xu",
      "Yihui Wang",
      "Fengtao Zhou",
      "Jiabo Ma",
      "Cheng Jin",
      "Shu Yang",
      "Jinbang Li",
      "Zhengyu Zhang",
      "Chenglong Zhao",
      "Huajun Zhou",
      "Zhenhui Li",
      "Huangjing Lin",
      "Xin Wang",
      "Jiguang Wang",
      "Anjia Han",
      "Ronald Cheong Kin Chan",
      "Li Liang",
      "Xiuming Zhang",
      "Hao Chen"
    ],
    "abstract": "Remarkable strides in computational pathology have been made in the\ntask-agnostic foundation model that advances the performance of a wide array of\ndownstream clinical tasks. Despite the promising performance, there are still\nseveral challenges. First, prior works have resorted to either vision-only or\nimage-caption data, disregarding pathology reports with more clinically\nauthentic information from pathologists and gene expression profiles which\nrespectively offer distinct knowledge for versatile clinical applications.\nSecond, the current progress in pathology FMs predominantly concentrates on the\npatch level, where the restricted context of patch-level pretraining fails to\ncapture whole-slide patterns. Even recent slide-level FMs still struggle to\nprovide whole-slide context for patch representation. In this study, for the\nfirst time, we develop a pathology foundation model incorporating three levels\nof modalities: pathology slides, pathology reports, and gene expression data,\nwhich resulted in 26,169 slide-level modality pairs from 10,275 patients across\n32 cancer types, amounting to over 116 million pathological patch images. To\nleverage these data for CPath, we propose a novel whole-slide pretraining\nparadigm that injects the multimodal whole-slide context into the patch\nrepresentation, called Multimodal Self-TAught PRetraining (mSTAR). The proposed\nparadigm revolutionizes the pretraining workflow for CPath, enabling the\npathology FM to acquire the whole-slide context. To the best of our knowledge,\nthis is the first attempt to incorporate three modalities at the whole-slide\ncontext for enhancing pathology FMs. To systematically evaluate the\ncapabilities of mSTAR, we built the largest spectrum of oncological benchmark,\nspanning 7 categories of oncological applications in 15 types of 97 practical\noncological tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "62 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.15362v3",
    "published_date": "2024-07-22 04:09:27 UTC",
    "updated_date": "2025-03-25 08:49:58 UTC"
  },
  {
    "arxiv_id": "2407.21043v2",
    "title": "CP-Prompt: Composition-Based Cross-modal Prompting for Domain-Incremental Continual Learning",
    "authors": [
      "Yu Feng",
      "Zhen Tian",
      "Yifan Zhu",
      "Zongfu Han",
      "Haoran Luo",
      "Guangwei Zhang",
      "Meina Song"
    ],
    "abstract": "The key challenge of cross-modal domain-incremental learning (DIL) is to\nenable the learning model to continuously learn from novel data with different\nfeature distributions under the same task without forgetting old ones. However,\nexisting top-performing methods still cause high forgetting rates, by lacking\nintra-domain knowledge extraction and inter-domain common prompting strategy.\nIn this paper, we propose a simple yet effective framework, CP-Prompt, by\ntraining limited parameters to instruct a pre-trained model to learn new\ndomains and avoid forgetting existing feature distributions. CP-Prompt captures\nintra-domain knowledge by compositionally inserting personalized prompts on\nmulti-head self-attention layers and then learns the inter-domain knowledge\nwith a common prompting strategy. CP-Prompt shows superiority compared with\nstate-of-the-art baselines among three widely evaluated DIL tasks. The source\ncode is available at https://github.com/dannis97500/CP_Prompt.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACM MM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.21043v2",
    "published_date": "2024-07-22 04:07:12 UTC",
    "updated_date": "2024-08-02 14:58:54 UTC"
  },
  {
    "arxiv_id": "2407.15356v1",
    "title": "X-Recon: Learning-based Patient-specific High-Resolution CT Reconstruction from Orthogonal X-Ray Images",
    "authors": [
      "Yunpeng Wang",
      "Kang Wang",
      "Yaoyao Zhuo",
      "Weiya Shi",
      "Fei Shan",
      "Lei Liu"
    ],
    "abstract": "Rapid and accurate diagnosis of pneumothorax, utilizing chest X-ray and\ncomputed tomography (CT), is crucial for assisted diagnosis. Chest X-ray is\ncommonly used for initial localization of pneumothorax, while CT ensures\naccurate quantification. However, CT scans involve high radiation doses and can\nbe costly. To achieve precise quantitative diagnosis while minimizing radiation\nexposure, we proposed X-Recon, a CT ultra-sparse reconstruction network based\non ortho-lateral chest X-ray images. X-Recon integrates generative adversarial\nnetworks (GANs), including a generator with a multi-scale fusion rendering\nmodule and a discriminator enhanced by 3D coordinate convolutional layers,\ndesigned to facilitate CT reconstruction. To improve precision, a projective\nspatial transformer is utilized to incorporate multi-angle projection loss.\nAdditionally, we proposed PTX-Seg, a zero-shot pneumothorax segmentation\nalgorithm, combining image processing techniques with deep-learning models for\nthe segmentation of air-accumulated regions and lung structures. Experiments on\na large-scale dataset demonstrate its superiority over existing approaches.\nX-Recon achieved a significantly higher reconstruction resolution with a higher\naverage spatial resolution and a lower average slice thickness. The\nreconstruction metrics achieved state-of-the-art performance in terms of\nseveral metrics including peak signal-to-noise ratio. The zero-shot\nsegmentation algorithm, PTX-Seg, also demonstrated high segmentation precision\nfor the air-accumulated region, the left lung, and the right lung. Moreover,\nthe consistency analysis for the pneumothorax chest occupancy ratio between\nreconstructed CT and original CT obtained a high correlation coefficient. Code\nwill be available at: https://github.com/wangyunpengbio/X-Recon",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15356v1",
    "published_date": "2024-07-22 03:55:36 UTC",
    "updated_date": "2024-07-22 03:55:36 UTC"
  },
  {
    "arxiv_id": "2407.15351v2",
    "title": "LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation",
    "authors": [
      "Jiaxing Zhang",
      "Jiayi Liu",
      "Dongsheng Luo",
      "Jennifer Neville",
      "Hua Wei"
    ],
    "abstract": "Recent studies seek to provide Graph Neural Network (GNN) interpretability\nvia multiple unsupervised learning models. Due to the scarcity of datasets,\ncurrent methods easily suffer from learning bias. To solve this problem, we\nembed a Large Language Model (LLM) as knowledge into the GNN explanation\nnetwork to avoid the learning bias problem. We inject LLM as a Bayesian\nInference (BI) module to mitigate learning bias. The efficacy of the BI module\nhas been proven both theoretically and experimentally. We conduct experiments\non both synthetic and real-world datasets. The innovation of our work lies in\ntwo parts: 1. We provide a novel view of the possibility of an LLM functioning\nas a Bayesian inference to improve the performance of existing algorithms; 2.\nWe are the first to discuss the learning bias issues in the GNN explanation\nproblem.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint Paper with 13 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.15351v2",
    "published_date": "2024-07-22 03:36:38 UTC",
    "updated_date": "2024-07-23 04:01:19 UTC"
  },
  {
    "arxiv_id": "2407.15893v1",
    "title": "Cascaded two-stage feature clustering and selection via separability and consistency in fuzzy decision systems",
    "authors": [
      "Yuepeng Chen",
      "Weiping Ding",
      "Hengrong Ju",
      "Jiashuang Huang",
      "Tao Yin"
    ],
    "abstract": "Feature selection is a vital technique in machine learning, as it can reduce\ncomputational complexity, improve model performance, and mitigate the risk of\noverfitting. However, the increasing complexity and dimensionality of datasets\npose significant challenges in the selection of features. Focusing on these\nchallenges, this paper proposes a cascaded two-stage feature clustering and\nselection algorithm for fuzzy decision systems. In the first stage, we reduce\nthe search space by clustering relevant features and addressing inter-feature\nredundancy. In the second stage, a clustering-based sequentially forward\nselection method that explores the global and local structure of data is\npresented. We propose a novel metric for assessing the significance of\nfeatures, which considers both global separability and local consistency.\nGlobal separability measures the degree of intra-class cohesion and inter-class\nseparation based on fuzzy membership, providing a comprehensive understanding\nof data separability. Meanwhile, local consistency leverages the fuzzy\nneighborhood rough set model to capture uncertainty and fuzziness in the data.\nThe effectiveness of our proposed algorithm is evaluated through experiments\nconducted on 18 public datasets and a real-world schizophrenia dataset. The\nexperiment results demonstrate our algorithm's superiority over benchmarking\nalgorithms in both classification accuracy and the number of selected features.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted by IEEE Transactions on Fuzzy Systems\n  for publication. Permission from IEEE must be obtained for all other uses, in\n  any current or future media. The final version is available at\n  [10.1109/TFUZZ.2024.3420963]",
    "pdf_url": "http://arxiv.org/pdf/2407.15893v1",
    "published_date": "2024-07-22 02:44:32 UTC",
    "updated_date": "2024-07-22 02:44:32 UTC"
  },
  {
    "arxiv_id": "2408.03943v1",
    "title": "Building Machines that Learn and Think with People",
    "authors": [
      "Katherine M. Collins",
      "Ilia Sucholutsky",
      "Umang Bhatt",
      "Kartik Chandra",
      "Lionel Wong",
      "Mina Lee",
      "Cedegao E. Zhang",
      "Tan Zhi-Xuan",
      "Mark Ho",
      "Vikash Mansinghka",
      "Adrian Weller",
      "Joshua B. Tenenbaum",
      "Thomas L. Griffiths"
    ],
    "abstract": "What do we want from machine intelligence? We envision machines that are not\njust tools for thought, but partners in thought: reasonable, insightful,\nknowledgeable, reliable, and trustworthy systems that think with us. Current\nartificial intelligence (AI) systems satisfy some of these criteria, some of\nthe time. In this Perspective, we show how the science of collaborative\ncognition can be put to work to engineer systems that really can be called\n``thought partners,'' systems built to meet our expectations and complement our\nlimitations. We lay out several modes of collaborative thought in which humans\nand AI thought partners can engage and propose desiderata for human-compatible\nthought partnerships. Drawing on motifs from computational cognitive science,\nwe motivate an alternative scaling path for the design of thought partners and\necosystems around their use through a Bayesian lens, whereby the partners we\nconstruct actively build and reason over models of the human and world.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.03943v1",
    "published_date": "2024-07-22 02:42:45 UTC",
    "updated_date": "2024-07-22 02:42:45 UTC"
  },
  {
    "arxiv_id": "2407.15332v1",
    "title": "Robust personalized pricing under uncertainty of purchase probabilities",
    "authors": [
      "Shunnosuke Ikeda",
      "Naoki Nishimura",
      "Noriyoshi Sukegawa",
      "Yuichi Takano"
    ],
    "abstract": "This paper is concerned with personalized pricing models aimed at maximizing\nthe expected revenues or profits for a single item. While it is essential for\npersonalized pricing to predict the purchase probabilities for each consumer,\nthese predicted values are inherently subject to unavoidable errors that can\nnegatively impact the realized revenues and profits. To address this issue, we\nfocus on robust optimization techniques that yield reliable solutions to\noptimization problems under uncertainty. Specifically, we propose a robust\noptimization model for personalized pricing that accounts for the uncertainty\nof predicted purchase probabilities. This model can be formulated as a\nmixed-integer linear optimization problem, which can be solved exactly using\nmathematical optimization solvers. We also develop a Lagrangian decomposition\nalgorithm combined with line search to efficiently find high-quality solutions\nfor large-scale optimization problems. Experimental results demonstrate the\neffectiveness of our robust optimization model and highlight the utility of our\nLagrangian decomposition algorithm in terms of both computational efficiency\nand solution quality.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15332v1",
    "published_date": "2024-07-22 02:36:19 UTC",
    "updated_date": "2024-07-22 02:36:19 UTC"
  },
  {
    "arxiv_id": "2407.15325v2",
    "title": "Odyssey: Empowering Minecraft Agents with Open-World Skills",
    "authors": [
      "Shunyu Liu",
      "Yaoru Li",
      "Kongcheng Zhang",
      "Zhenyu Cui",
      "Wenkai Fang",
      "Yuxuan Zheng",
      "Tongya Zheng",
      "Mingli Song"
    ],
    "abstract": "Recent studies have delved into constructing generalist agents for open-world\nenvironments like Minecraft. Despite the encouraging results, existing efforts\nmainly focus on solving basic programmatic tasks, e.g., material collection and\ntool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set\nof actions available to agents, requiring them to learn effective long-horizon\nstrategies from scratch. Consequently, discovering diverse gameplay\nopportunities in the open world becomes challenging. In this work, we introduce\nOdyssey, a new framework that empowers Large Language Model (LLM)-based agents\nwith open-world skills to explore the vast Minecraft world. Odyssey comprises\nthree key parts: (1) An interactive agent with an open-world skill library that\nconsists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned\nLLaMA-3 model trained on a large question-answering dataset with 390k+\ninstruction entries derived from the Minecraft Wiki. (3) A new agent capability\nbenchmark includes the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task. Extensive experiments demonstrate\nthat the proposed Odyssey framework can effectively evaluate different\ncapabilities of LLM-based agents. All datasets, model weights, and code are\npublicly available to motivate future research on more advanced autonomous\nagent solutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15325v2",
    "published_date": "2024-07-22 02:06:59 UTC",
    "updated_date": "2024-10-07 09:40:07 UTC"
  },
  {
    "arxiv_id": "2407.15312v1",
    "title": "FMDNN: A Fuzzy-guided Multi-granular Deep Neural Network for Histopathological Image Classification",
    "authors": [
      "Weiping Ding",
      "Tianyi Zhou",
      "Jiashuang Huang",
      "Shu Jiang",
      "Tao Hou",
      "Chin-Teng Lin"
    ],
    "abstract": "Histopathological image classification constitutes a pivotal task in\ncomputer-aided diagnostics. The precise identification and categorization of\nhistopathological images are of paramount significance for early disease\ndetection and treatment. In the diagnostic process of pathologists, a\nmulti-tiered approach is typically employed to assess abnormalities in cell\nregions at different magnifications. However, feature extraction is often\nperformed at a single granularity, overlooking the multi-granular\ncharacteristics of cells. To address this issue, we propose the Fuzzy-guided\nMulti-granularity Deep Neural Network (FMDNN). Inspired by the multi-granular\ndiagnostic approach of pathologists, we perform feature extraction on cell\nstructures at coarse, medium, and fine granularity, enabling the model to fully\nharness the information in histopathological images. We incorporate the theory\nof fuzzy logic to address the challenge of redundant key information arising\nduring multi-granular feature extraction. Cell features are described from\ndifferent perspectives using multiple fuzzy membership functions, which are\nfused to create universal fuzzy features. A fuzzy-guided cross-attention module\nguides universal fuzzy features toward multi-granular features. We propagate\nthese features through an encoder to all patch tokens, aiming to achieve\nenhanced classification accuracy and robustness. In experiments on multiple\npublic datasets, our model exhibits a significant improvement in accuracy over\ncommonly used classification methods for histopathological image classification\nand shows commendable interpretability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted by IEEE Transactions on Fuzzy Systems\n  for publication. Permission from IEEE must be obtained for all other uses, in\n  any current or future media. The final version is available at [doi:\n  10.1109/TFUZZ.2024.3410929]",
    "pdf_url": "http://arxiv.org/pdf/2407.15312v1",
    "published_date": "2024-07-22 00:46:15 UTC",
    "updated_date": "2024-07-22 00:46:15 UTC"
  }
]
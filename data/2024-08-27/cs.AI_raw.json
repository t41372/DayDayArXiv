[
  {
    "arxiv_id": "2408.15443v1",
    "title": "Pathfinding with Lazy Successor Generation",
    "authors": [
      "Keisuke Okumura"
    ],
    "abstract": "We study a pathfinding problem where only locations (i.e., vertices) are\ngiven, and edges are implicitly defined by an oracle answering the connectivity\nof two locations. Despite its simple structure, this problem becomes\nnon-trivial with a massive number of locations, due to posing a huge branching\nfactor for search algorithms. Limiting the number of successors, such as with\nnearest neighbors, can reduce search efforts but compromises completeness.\nInstead, we propose a novel LaCAS* algorithm, which does not generate\nsuccessors all at once but gradually generates successors as the search\nprogresses. This scheme is implemented with k-nearest neighbors search on a k-d\ntree. LaCAS* is a complete and anytime algorithm that eventually converges to\nthe optima. Extensive evaluations demonstrate the efficacy of LaCAS*, e.g.,\nsolving complex pathfinding instances quickly, where conventional methods\nfalter.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.15443v1",
    "published_date": "2024-08-27 23:25:25 UTC",
    "updated_date": "2024-08-27 23:25:25 UTC"
  },
  {
    "arxiv_id": "2408.15436v1",
    "title": "Online Event-Triggered Switching for Frequency Control in Power Grids with Variable Inertia",
    "authors": [
      "Jie Feng",
      "Wenqi Cui",
      "Jorge Cort√©s",
      "Yuanyuan Shi"
    ],
    "abstract": "The increasing integration of renewable energy resources into power grids has\nled to time-varying system inertia and consequent degradation in frequency\ndynamics. A promising solution to alleviate performance degradation is using\npower electronics interfaced energy resources, such as renewable generators and\nbattery energy storage for primary frequency control, by adjusting their power\noutput set-points in response to frequency deviations. However, designing a\nfrequency controller under time-varying inertia is challenging. Specifically,\nthe stability or optimality of controllers designed for time-invariant systems\ncan be compromised once applied to a time-varying system. We model the\nfrequency dynamics under time-varying inertia as a nonlinear switching system,\nwhere the frequency dynamics under each mode are described by the nonlinear\nswing equations and different modes represent different inertia levels. We\nidentify a key controller structure, named Neural Proportional-Integral\n(Neural-PI) controller, that guarantees exponential input-to-state stability\nfor each mode. To further improve performance, we present an online\nevent-triggered switching algorithm to select the most suitable controller from\na set of Neural-PI controllers, each optimized for specific inertia levels.\nSimulations on the IEEE 39-bus system validate the effectiveness of the\nproposed online switching control method with stability guarantees and\noptimized performance for frequency control under time-varying inertia.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15436v1",
    "published_date": "2024-08-27 22:44:33 UTC",
    "updated_date": "2024-08-27 22:44:33 UTC"
  },
  {
    "arxiv_id": "2408.15425v1",
    "title": "Fast and Modular Autonomy Software for Autonomous Racing Vehicles",
    "authors": [
      "Andrew Saba",
      "Aderotimi Adetunji",
      "Adam Johnson",
      "Aadi Kothari",
      "Matthew Sivaprakasam",
      "Joshua Spisak",
      "Prem Bharatia",
      "Arjun Chauhan",
      "Brendan Duff Jr.",
      "Noah Gasparro",
      "Charles King",
      "Ryan Larkin",
      "Brian Mao",
      "Micah Nye",
      "Anjali Parashar",
      "Joseph Attias",
      "Aurimas Balciunas",
      "Austin Brown",
      "Chris Chang",
      "Ming Gao",
      "Cindy Heredia",
      "Andrew Keats",
      "Jose Lavariega",
      "William Muckelroy III",
      "Andre Slavescu",
      "Nickolas Stathas",
      "Nayana Suvarna",
      "Chuan Tian Zhang",
      "Sebastian Scherer",
      "Deva Ramanan"
    ],
    "abstract": "Autonomous motorsports aim to replicate the human racecar driver with\nsoftware and sensors. As in traditional motorsports, Autonomous Racing Vehicles\n(ARVs) are pushed to their handling limits in multi-agent scenarios at\nextremely high ($\\geq 150mph$) speeds. This Operational Design Domain (ODD)\npresents unique challenges across the autonomy stack. The Indy Autonomous\nChallenge (IAC) is an international competition aiming to advance autonomous\nvehicle development through ARV competitions. While far from challenging what a\nhuman racecar driver can do, the IAC is pushing the state of the art by\nfacilitating full-sized ARV competitions. This paper details the MIT-Pitt-RW\nTeam's approach to autonomous racing in the IAC. In this work, we present our\nmodular and fast approach to agent detection, motion planning and controls to\ncreate an autonomy stack. We also provide analysis of the performance of the\nsoftware stack in single and multi-agent scenarios for rapid deployment in a\nfast-paced competition environment. We also cover what did and did not work\nwhen deployed on a physical system the Dallara AV-21 platform and potential\nimprovements to address these shortcomings. Finally, we convey lessons learned\nand discuss limitations and future directions for improvement.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.RO",
    "comment": "Published in Journal of Field Robotics",
    "pdf_url": "http://arxiv.org/pdf/2408.15425v1",
    "published_date": "2024-08-27 21:57:16 UTC",
    "updated_date": "2024-08-27 21:57:16 UTC"
  },
  {
    "arxiv_id": "2408.15421v2",
    "title": "Simultaneous Training of First- and Second-Order Optimizers in Population-Based Reinforcement Learning",
    "authors": [
      "Felix Pfeiffer",
      "Shahram Eivazi"
    ],
    "abstract": "The tuning of hyperparameters in reinforcement learning (RL) is critical, as\nthese parameters significantly impact an agent's performance and learning\nefficiency. Dynamic adjustment of hyperparameters during the training process\ncan significantly enhance both the performance and stability of learning.\nPopulation-based training (PBT) provides a method to achieve this by\ncontinuously tuning hyperparameters throughout the training. This ongoing\nadjustment enables models to adapt to different learning stages, resulting in\nfaster convergence and overall improved performance. In this paper, we propose\nan enhancement to PBT by simultaneously utilizing both first- and second-order\noptimizers within a single population. We conducted a series of experiments\nusing the TD3 algorithm across various MuJoCo environments. Our results, for\nthe first time, empirically demonstrate the potential of incorporating\nsecond-order optimizers within PBT-based RL. Specifically, the combination of\nthe K-FAC optimizer with Adam led to up to a 10% improvement in overall\nperformance compared to PBT using only Adam. Additionally, in environments\nwhere Adam occasionally fails, such as the Swimmer environment, the mixed\npopulation with K-FAC exhibited more reliable learning outcomes, offering a\nsignificant advantage in training stability without a substantial increase in\ncomputational time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15421v2",
    "published_date": "2024-08-27 21:54:26 UTC",
    "updated_date": "2024-09-04 10:17:22 UTC"
  },
  {
    "arxiv_id": "2408.15406v1",
    "title": "Intertwined Biases Across Social Media Spheres: Unpacking Correlations in Media Bias Dimensions",
    "authors": [
      "Yifan Liu",
      "Yike Li",
      "Dong Wang"
    ],
    "abstract": "Media bias significantly shapes public perception by reinforcing stereotypes\nand exacerbating societal divisions. Prior research has often focused on\nisolated media bias dimensions such as \\textit{political bias} or\n\\textit{racial bias}, neglecting the complex interrelationships among various\nbias dimensions across different topic domains. Moreover, we observe that\nmodels trained on existing media bias benchmarks fail to generalize effectively\non recent social media posts, particularly in certain bias identification\ntasks. This shortfall primarily arises because these benchmarks do not\nadequately reflect the rapidly evolving nature of social media content, which\nis characterized by shifting user behaviors and emerging trends. In response to\nthese limitations, our research introduces a novel dataset collected from\nYouTube and Reddit over the past five years. Our dataset includes automated\nannotations for YouTube content across a broad spectrum of bias dimensions,\nsuch as gender, racial, and political biases, as well as hate speech, among\nothers. It spans diverse domains including politics, sports, healthcare,\neducation, and entertainment, reflecting the complex interplay of biases across\ndifferent societal sectors. Through comprehensive statistical analysis, we\nidentify significant differences in bias expression patterns and intra-domain\nbias correlations across these domains. By utilizing our understanding of the\ncorrelations among various bias dimensions, we lay the groundwork for creating\nadvanced systems capable of detecting multiple biases simultaneously. Overall,\nour dataset advances the field of media bias identification, contributing to\nthe development of tools that promote fairer media consumption. The\ncomprehensive awareness of existing media bias fosters more ethical journalism,\npromotes cultural sensitivity, and supports a more informed and equitable\npublic discourse.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.SI",
    "comment": "Accepted to ASONAM 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.15406v1",
    "published_date": "2024-08-27 21:03:42 UTC",
    "updated_date": "2024-08-27 21:03:42 UTC"
  },
  {
    "arxiv_id": "2408.15399v1",
    "title": "A Statistical Framework for Data-dependent Retrieval-Augmented Models",
    "authors": [
      "Soumya Basu",
      "Ankit Singh Rawat",
      "Manzil Zaheer"
    ],
    "abstract": "Modern ML systems increasingly augment input instances with additional\nrelevant information to enhance final prediction. Despite growing interest in\nsuch retrieval-augmented models, their fundamental properties and training are\nnot well understood. We propose a statistical framework to study such models\nwith two components: 1) a {\\em retriever} to identify the relevant information\nout of a large corpus via a data-dependent metric; and 2) a {\\em predictor}\nthat consumes the input instances along with the retrieved information to make\nthe final predictions. We present a principled method for end-to-end training\nof both components and draw connections with various training approaches in the\nliterature. Furthermore, we establish excess risk bounds for\nretrieval-augmented models while delineating the contributions of both\nretriever and predictor towards the model performance. We validate the utility\nof our proposed training methods along with the key takeaways from our\nstatistical analysis on open domain question answering task where retrieval\naugmentation is important.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15399v1",
    "published_date": "2024-08-27 20:51:06 UTC",
    "updated_date": "2024-08-27 20:51:06 UTC"
  },
  {
    "arxiv_id": "2408.15395v1",
    "title": "SCAN-Edge: Finding MobileNet-speed Hybrid Networks for Diverse Edge Devices via Hardware-Aware Evolutionary Search",
    "authors": [
      "Hung-Yueh Chiang",
      "Diana Marculescu"
    ],
    "abstract": "Designing low-latency and high-efficiency hybrid networks for a variety of\nlow-cost commodity edge devices is both costly and tedious, leading to the\nadoption of hardware-aware neural architecture search (NAS) for finding optimal\narchitectures. However, unifying NAS for a wide range of edge devices presents\nchallenges due to the variety of hardware designs, supported operations, and\ncompilation optimizations. Existing methods often fix the search space of\narchitecture choices (e.g., activation, convolution, or self-attention) and\nestimate latency using hardware-agnostic proxies (e.g., FLOPs), which fail to\nachieve proclaimed latency across various edge devices. To address this issue,\nwe propose SCAN-Edge, a unified NAS framework that jointly searches for\nself-attention, convolution, and activation to accommodate the wide variety of\nedge devices, including CPU-, GPU-, and hardware accelerator-based systems. To\nhandle the large search space, SCAN-Edge relies on with a hardware-aware\nevolutionary algorithm that improves the quality of the search space to\naccelerate the sampling process. Experiments on large-scale datasets\ndemonstrate that our hybrid networks match the actual MobileNetV2 latency for\n224x224 input resolution on various commodity edge devices.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15395v1",
    "published_date": "2024-08-27 20:39:09 UTC",
    "updated_date": "2024-08-27 20:39:09 UTC"
  },
  {
    "arxiv_id": "2408.15381v2",
    "title": "On Stateful Value Factorization in Multi-Agent Reinforcement Learning",
    "authors": [
      "Enrico Marchesini",
      "Andrea Baisero",
      "Rupali Bhati",
      "Christopher Amato"
    ],
    "abstract": "Value factorization is a popular paradigm for designing scalable multi-agent\nreinforcement learning algorithms. However, current factorization methods make\nchoices without full justification that may limit their performance. For\nexample, the theory in prior work uses stateless (i.e., history) functions,\nwhile the practical implementations use state information -- making the\nmotivating theory a mismatch for the implementation. Also, methods have built\noff of previous approaches, inheriting their architectures without exploring\nother, potentially better ones. To address these concerns, we formally analyze\nthe theory of using the state instead of the history in current methods --\nreconnecting theory and practice. We then introduce DuelMIX, a factorization\nalgorithm that learns distinct per-agent utility estimators to improve\nperformance and achieve full expressiveness. Experiments on StarCraft II\nmicromanagement and Box Pushing tasks demonstrate the benefits of our\nintuitions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 9 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.15381v2",
    "published_date": "2024-08-27 19:45:26 UTC",
    "updated_date": "2024-09-09 22:49:17 UTC"
  },
  {
    "arxiv_id": "2409.00113v2",
    "title": "Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options",
    "authors": [
      "Gracjan G√≥ral",
      "Emilia Wi≈õnios",
      "Piotr Sankowski",
      "Pawe≈Ç Budzianowski"
    ],
    "abstract": "Decision-making under full alignment requires balancing between reasoning and\nfaithfulness - a challenge for large language models (LLMs). This study\nexplores whether LLMs prioritize following instructions over reasoning and\ntruth when given \"misleading\" instructions, such as \"Respond solely with A or\nB\", even when neither option is correct. We introduce a new metric called\n\"reflective judgment\", which sheds new light on the relationship between the\npre-training and post-training alignment schemes. In tasks ranging from basic\narithmetic to domain-specific assessments, models like GPT-4o, o1-mini, or\nClaude 3 Opus adhered to instructions correctly but failed to reflect on the\nvalidity of the provided options. Contrary, models from the Llama 3.1 family\n(8B, 70B, 405B) or base Qwen2.5 (7B, 14B, 32B) families exhibit improved\nrefusal rates with size, indicating a scaling effect. We also observed that\nalignment techniques, though intended to enhance reasoning, sometimes weakened\nthe models' ability to reject incorrect instructions, leading them to follow\nflawed prompts uncritically. Finally, we have also conducted a parallel human\nstudy revealing similar patterns in human behavior and annotations. We\nhighlight how popular RLHF datasets might disrupt either training or evaluation\ndue to annotations exhibiting poor reflective judgement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for NeurIPS 2024 FM-EduAssess Workshop",
    "pdf_url": "http://arxiv.org/pdf/2409.00113v2",
    "published_date": "2024-08-27 19:27:43 UTC",
    "updated_date": "2024-10-10 20:46:36 UTC"
  },
  {
    "arxiv_id": "2408.16027v1",
    "title": "Toward Time-Continuous Data Inference in Sparse Urban CrowdSensing",
    "authors": [
      "Ziyu Sun",
      "Haoyang Su",
      "Hanqi Sun",
      "En Wang",
      "Wenbin Liu"
    ],
    "abstract": "Mobile Crowd Sensing (MCS) is a promising paradigm that leverages mobile\nusers and their smart portable devices to perform various real-world tasks.\nHowever, due to budget constraints and the inaccessibility of certain areas,\nSparse MCS has emerged as a more practical alternative, collecting data from a\nlimited number of target subareas and utilizing inference algorithms to\ncomplete the full sensing map. While existing approaches typically assume a\ntime-discrete setting with data remaining constant within each sensing cycle,\nthis simplification can introduce significant errors, especially when dealing\nwith long cycles, as real-world sensing data often changes continuously. In\nthis paper, we go from fine-grained completion, i.e., the subdivision of\nsensing cycles into minimal time units, towards a more accurate,\ntime-continuous completion. We first introduce Deep Matrix Factorization (DMF)\nas a neural network-enabled framework and enhance it with a Recurrent Neural\nNetwork (RNN-DMF) to capture temporal correlations in these finer time slices.\nTo further deal with the continuous data, we propose TIME-DMF, which captures\ntemporal information across unequal intervals, enabling time-continuous\ncompletion. Additionally, we present the Query-Generate (Q-G) strategy within\nTIME-DMF to model the infinite states of continuous data. Extensive experiments\nacross five types of sensing tasks demonstrate the effectiveness of our models\nand the advantages of time-continuous completion.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16027v1",
    "published_date": "2024-08-27 19:25:41 UTC",
    "updated_date": "2024-08-27 19:25:41 UTC"
  },
  {
    "arxiv_id": "2408.15373v1",
    "title": "Handling Geometric Domain Shifts in Semantic Segmentation of Surgical RGB and Hyperspectral Images",
    "authors": [
      "Silvia Seidlitz",
      "Jan Sellner",
      "Alexander Studier-Fischer",
      "Alessandro Motta",
      "Berkin √ñzdemir",
      "Beat P. M√ºller-Stich",
      "Felix Nickel",
      "Lena Maier-Hein"
    ],
    "abstract": "Robust semantic segmentation of intraoperative image data holds promise for\nenabling automatic surgical scene understanding and autonomous robotic surgery.\nWhile model development and validation are primarily conducted on idealistic\nscenes, geometric domain shifts, such as occlusions of the situs, are common in\nreal-world open surgeries. To close this gap, we (1) present the first analysis\nof state-of-the-art (SOA) semantic segmentation models when faced with\ngeometric out-of-distribution (OOD) data, and (2) propose an augmentation\ntechnique called \"Organ Transplantation\", to enhance generalizability. Our\ncomprehensive validation on six different OOD datasets, comprising 600 RGB and\nhyperspectral imaging (HSI) cubes from 33 pigs, each annotated with 19 classes,\nreveals a large performance drop in SOA organ segmentation models on geometric\nOOD data. This performance decline is observed not only in conventional RGB\ndata (with a dice similarity coefficient (DSC) drop of 46 %) but also in HSI\ndata (with a DSC drop of 45 %), despite the richer spectral information\ncontent. The performance decline increases with the spatial granularity of the\ninput data. Our augmentation technique improves SOA model performance by up to\n67 % for RGB data and 90 % for HSI data, achieving performance at the level of\nin-distribution performance on real OOD test data. Given the simplicity and\neffectiveness of our augmentation method, it is a valuable tool for addressing\ngeometric domain shifts in surgical scene segmentation, regardless of the\nunderlying model. Our code and pre-trained models are publicly available at\nhttps://github.com/IMSY-DKFZ/htc.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Silvia Seidlitz and Jan Sellner contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2408.15373v1",
    "published_date": "2024-08-27 19:13:15 UTC",
    "updated_date": "2024-08-27 19:13:15 UTC"
  },
  {
    "arxiv_id": "2408.15354v1",
    "title": "What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users",
    "authors": [
      "Jana Schaich Borg",
      "Hannah Read"
    ],
    "abstract": "Interest is growing in artificial empathy, but so is confusion about what\nartificial empathy is or needs to be. This confusion makes it challenging to\nnavigate the technical and ethical issues that accompany empathic AI\ndevelopment. Here, we outline a framework for thinking about empathic AI based\non the premise that different constellations of capabilities associated with\nempathy are important for different empathic AI applications. We describe\ndistinctions of capabilities that we argue belong under the empathy umbrella,\nand show how three medical empathic AI use cases require different sets of\nthese capabilities. We conclude by discussing why appreciation of the diverse\ncapabilities under the empathy umbrella is important for both AI creators and\nusers.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear at the 7th AAAI/ACM Conference on AI, Ethics, and Society,\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2408.15354v1",
    "published_date": "2024-08-27 18:27:22 UTC",
    "updated_date": "2024-08-27 18:27:22 UTC"
  },
  {
    "arxiv_id": "2409.07464v1",
    "title": "Reflective Human-Machine Co-adaptation for Enhanced Text-to-Image Generation Dialogue System",
    "authors": [
      "Yuheng Feng",
      "Yangfan He",
      "Yinghui Xia",
      "Tianyu Shi",
      "Jun Wang",
      "Jinsong Yang"
    ],
    "abstract": "Today's image generation systems are capable of producing realistic and\nhigh-quality images. However, user prompts often contain ambiguities, making it\ndifficult for these systems to interpret users' potential intentions.\nConsequently, machines need to interact with users multiple rounds to better\nunderstand users' intents. The unpredictable costs of using or learning image\ngeneration models through multiple feedback interactions hinder their\nwidespread adoption and full performance potential, especially for non-expert\nusers. In this research, we aim to enhance the user-friendliness of our image\ngeneration system. To achieve this, we propose a reflective human-machine\nco-adaptation strategy, named RHM-CAS. Externally, the Agent engages in\nmeaningful language interactions with users to reflect on and refine the\ngenerated images. Internally, the Agent tries to optimize the policy based on\nuser preferences, ensuring that the final outcomes closely align with user\npreferences. Various experiments on different tasks demonstrate the\neffectiveness of the proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07464v1",
    "published_date": "2024-08-27 18:08:00 UTC",
    "updated_date": "2024-08-27 18:08:00 UTC"
  },
  {
    "arxiv_id": "2408.15332v2",
    "title": "What makes math problems hard for reinforcement learning: a case study",
    "authors": [
      "Ali Shehper",
      "Anibal M. Medina-Mardones",
      "Lucas Fagan",
      "Bart≈Çomiej Lewandowski",
      "Angus Gruen",
      "Yang Qiu",
      "Piotr Kucharski",
      "Zhenghan Wang",
      "Sergei Gukov"
    ],
    "abstract": "Using a long-standing conjecture from combinatorial group theory, we explore,\nfrom multiple perspectives, the challenges of finding rare instances carrying\ndisproportionately high rewards. Based on lessons learned in the context\ndefined by the Andrews-Curtis conjecture, we propose algorithmic enhancements\nand a topological hardness measure with implications for a broad class of\nsearch problems. As part of our study, we also address several open\nmathematical questions. Notably, we demonstrate the length reducibility of all\nbut two presentations in the Akbulut-Kirby series (1981), and resolve various\npotential counterexamples in the Miller-Schupp series (1991), including three\ninfinite subfamilies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.CO",
      "math.GR",
      "math.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "58 pages, 25 figures, 1 table. Try it:\n  https://github.com/shehper/AC-Solver",
    "pdf_url": "http://arxiv.org/pdf/2408.15332v2",
    "published_date": "2024-08-27 18:00:06 UTC",
    "updated_date": "2025-02-11 18:01:40 UTC"
  },
  {
    "arxiv_id": "2408.15237v3",
    "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
    "authors": [
      "Junxiong Wang",
      "Daniele Paliotta",
      "Avner May",
      "Alexander M. Rush",
      "Tri Dao"
    ],
    "abstract": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN\nmodel. We also find that the distilled model has natural length extrapolation,\nshowing almost perfect accuracy in the needle-in-a-haystack test at 20x the\ndistillation length. Code and pre-trained checkpoints are open-sourced at\nhttps://github.com/jxiw/MambaInLlama and\nhttps://github.com/itsdaniele/speculative_mamba.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024. v3 updates: fix format errors",
    "pdf_url": "http://arxiv.org/pdf/2408.15237v3",
    "published_date": "2024-08-27 17:56:11 UTC",
    "updated_date": "2025-01-08 20:34:02 UTC"
  },
  {
    "arxiv_id": "2408.15232v2",
    "title": "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations",
    "authors": [
      "Yucheng Jiang",
      "Yijia Shao",
      "Dekun Ma",
      "Sina J. Semnani",
      "Monica S. Lam"
    ],
    "abstract": "While language model (LM)-powered chatbots and generative search engines\nexcel at answering concrete queries, discovering information in the terrain of\nunknown unknowns remains challenging for users. To emulate the common\neducational scenario where children/students learn by listening to and\nparticipating in conversations of their parents/teachers, we create\nCollaborative STORM (Co-STORM). Unlike QA systems that require users to ask all\nthe questions, Co-STORM lets users observe and occasionally steer the discourse\namong several LM agents. The agents ask questions on the user's behalf,\nallowing the user to discover unknown unknowns serendipitously. To facilitate\nuser interaction, Co-STORM assists users in tracking the discourse by\norganizing the uncovered information into a dynamic mind map, ultimately\ngenerating a comprehensive report as takeaways. For automatic evaluation, we\nconstruct the WildSeek dataset by collecting real information-seeking records\nwith user goals. Co-STORM outperforms baseline methods on both discourse trace\nand report quality. In a further human evaluation, 70% of participants prefer\nCo-STORM over a search engine, and 78% favor it over a RAG chatbot.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "I.2.7; H.5.2; H.3.3"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2408.15232v2",
    "published_date": "2024-08-27 17:50:03 UTC",
    "updated_date": "2024-10-17 20:43:22 UTC"
  },
  {
    "arxiv_id": "2408.15313v2",
    "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models",
    "authors": [
      "Wenxuan Zhang",
      "Philip H. S. Torr",
      "Mohamed Elhoseiny",
      "Adel Bibi"
    ],
    "abstract": "Fine-tuning large language models (LLMs) on human preferences, typically\nthrough reinforcement learning from human feedback (RLHF), has proven\nsuccessful in enhancing their capabilities. However, ensuring the safety of\nLLMs during fine-tuning remains a critical concern, and mitigating the\npotential conflicts in safety and helpfulness is costly in RLHF. To address\nthis issue, we propose a supervised learning framework called Bi-Factorial\nPreference Optimization (BFPO), which re-parameterizes a joint RLHF objective\nof both safety and helpfulness into a single supervised learning objective. In\nsupervised optimization, a labeling function is used to capture the global\npreferences ranking to balance both safety and helpfulness. To evaluate BFPO,\nwe develop a benchmark that includes comprehensive discriminative and\ngenerative tasks for helpfulness and harmlessness. The results indicate that\nour method significantly outperforms existing approaches in both safety and\nhelpfulness. Moreover, BFPO achieves the same level of safety as methods that\nheavily rely on human labor with less than 10\\% of the computational resources\nand human prompting and annotation process. The training recipes can be found\nhere: https://github.com/wx-zhang/bfpo.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "The paper has been accepted in ICLR 2025 as spotlight presentation",
    "pdf_url": "http://arxiv.org/pdf/2408.15313v2",
    "published_date": "2024-08-27 17:31:21 UTC",
    "updated_date": "2025-04-08 11:04:33 UTC"
  },
  {
    "arxiv_id": "2408.15217v1",
    "title": "Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance",
    "authors": [
      "Weiyi Zhang",
      "Siyu Huang",
      "Jiancheng Yang",
      "Ruoyu Chen",
      "Zongyuan Ge",
      "Yingfeng Zheng",
      "Danli Shi",
      "Mingguang He"
    ],
    "abstract": "Fundus Fluorescein Angiography (FFA) is a critical tool for assessing retinal\nvascular dynamics and aiding in the diagnosis of eye diseases. However, its\ninvasive nature and less accessibility compared to Color Fundus (CF) images\npose significant challenges. Current CF to FFA translation methods are limited\nto static generation. In this work, we pioneer dynamic FFA video generation\nfrom static CF images. We introduce an autoregressive GAN for smooth,\nmemory-saving frame-by-frame FFA synthesis. To enhance the focus on dynamic\nlesion changes in FFA regions, we design a knowledge mask based on clinical\nexperience. Leveraging this mask, our approach integrates innovative knowledge\nmask-guided techniques, including knowledge-boosted attention, knowledge-aware\ndiscriminators, and mask-enhanced patchNCE loss, aimed at refining generation\nin critical areas and addressing the pixel misalignment challenge. Our method\nachieves the best FVD of 1503.21 and PSNR of 11.81 compared to other common\nvideo generation approaches. Human assessment by an ophthalmologist confirms\nits high generation quality. Notably, our knowledge mask surpasses supervised\nlesion segmentation masks, offering a promising non-invasive alternative to\ntraditional FFA for research and clinical applications. The code is available\nat https://github.com/Michi-3000/Fundus2Video.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "The paper has been accepted by Medical Image Computing and Computer\n  Assisted Intervention Society (MICCAI) 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.15217v1",
    "published_date": "2024-08-27 17:30:49 UTC",
    "updated_date": "2024-08-27 17:30:49 UTC"
  },
  {
    "arxiv_id": "2409.00112v1",
    "title": "Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy",
    "authors": [
      "Daniil Filienko",
      "Yinzhou Wang",
      "Caroline El Jazmi",
      "Serena Xie",
      "Trevor Cohen",
      "Martine De Cock",
      "Weichao Yuwen"
    ],
    "abstract": "While Large Language Models (LLMs) are being quickly adapted to many domains,\nincluding healthcare, their strengths and pitfalls remain under-explored. In\nour study, we examine the effects of prompt engineering to guide Large Language\nModels (LLMs) in delivering parts of a Problem-Solving Therapy (PST) session\nvia text, particularly during the symptom identification and assessment phase\nfor personalized goal setting. We present evaluation results of the models'\nperformances by automatic metrics and experienced medical professionals. We\ndemonstrate that the models' capability to deliver protocolized therapy can be\nimproved with the proper use of prompt engineering methods, albeit with\nlimitations. To our knowledge, this study is among the first to assess the\neffects of various prompting techniques in enhancing a generalist model's\nability to deliver psychotherapy, focusing on overall quality, consistency, and\nempathy. Exploring LLMs' potential in delivering psychotherapy holds promise\nwith the current shortage of mental health professionals amid significant\nneeds, enhancing the potential utility of AI-based and AI-enhanced care\nservices.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for AMIA 2024 proceedings",
    "pdf_url": "http://arxiv.org/pdf/2409.00112v1",
    "published_date": "2024-08-27 17:25:16 UTC",
    "updated_date": "2024-08-27 17:25:16 UTC"
  },
  {
    "arxiv_id": "2409.04456v1",
    "title": "Pattern based learning and optimisation through pricing for bin packing problem",
    "authors": [
      "Huayan Zhang",
      "Ruibin Bai",
      "Tie-Yan Liu",
      "Jiawei Li",
      "Bingchen Lin",
      "Jianfeng Ren"
    ],
    "abstract": "As a popular form of knowledge and experience, patterns and their\nidentification have been critical tasks in most data mining applications.\nHowever, as far as we are aware, no study has systematically examined the\ndynamics of pattern values and their reuse under varying conditions. We argue\nthat when problem conditions such as the distributions of random variables\nchange, the patterns that performed well in previous circumstances may become\nless effective and adoption of these patterns would result in sub-optimal\nsolutions. In response, we make a connection between data mining and the\nduality theory in operations research and propose a novel scheme to efficiently\nidentify patterns and dynamically quantify their values for each specific\ncondition. Our method quantifies the value of patterns based on their ability\nto satisfy stochastic constraints and their effects on the objective value,\nallowing high-quality patterns and their combinations to be detected. We use\nthe online bin packing problem to evaluate the effectiveness of the proposed\nscheme and illustrate the online packing procedure with the guidance of\npatterns that address the inherent uncertainty of the problem. Results show\nthat the proposed algorithm significantly outperforms the state-of-the-art\nmethods. We also analysed in detail the distinctive features of the proposed\nmethods that lead to performance improvement and the special cases where our\nmethod can be further improved.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04456v1",
    "published_date": "2024-08-27 17:03:48 UTC",
    "updated_date": "2024-08-27 17:03:48 UTC"
  },
  {
    "arxiv_id": "2408.15204v2",
    "title": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?",
    "authors": [
      "Kristina Gligoriƒá",
      "Tijana Zrnic",
      "Cinoo Lee",
      "Emmanuel J. Cand√®s",
      "Dan Jurafsky"
    ],
    "abstract": "Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Please cite as: Can Unconfident LLM Annotations Be Used for Confident\n  Conclusions? Kristina Gligori\\'c, Tijana Zrnic, Cinoo Lee, Emmanuel Cand\\`es,\n  and Dan Jurafsky. NAACL, 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.15204v2",
    "published_date": "2024-08-27 17:03:18 UTC",
    "updated_date": "2025-02-08 15:15:35 UTC"
  },
  {
    "arxiv_id": "2408.15198v1",
    "title": "Automatic 8-tissue Segmentation for 6-month Infant Brains",
    "authors": [
      "Yilan Dong",
      "Vanessa Kyriakopoulou",
      "Irina Grigorescu",
      "Grainne McAlonan",
      "Dafnis Batalle",
      "Maria Deprez"
    ],
    "abstract": "Numerous studies have highlighted that atypical brain development,\nparticularly during infancy and toddlerhood, is linked to an increased\nlikelihood of being diagnosed with a neurodevelopmental condition, such as\nautism. Accurate brain tissue segmentations for morphological analysis are\nessential in numerous infant studies. However, due to ongoing white matter (WM)\nmyelination changing tissue contrast in T1- and T2-weighted images, automatic\ntissue segmentation in 6-month infants is particularly difficult. On the other\nhand, manual labelling by experts is time-consuming and labor-intensive. In\nthis study, we propose the first 8-tissue segmentation pipeline for\nsix-month-old infant brains. This pipeline utilizes domain adaptation (DA)\ntechniques to leverage our longitudinal data, including neonatal images\nsegmented with the neonatal Developing Human Connectome Project structural\npipeline. Our pipeline takes raw 6-month images as inputs and generates the\n8-tissue segmentation as outputs, forming an end-to-end segmentation pipeline.\nThe segmented tissues include WM, gray matter (GM), cerebrospinal fluid (CSF),\nventricles, cerebellum, basal ganglia, brainstem, and hippocampus/amygdala.\nCycle-Consistent Generative Adversarial Network (CycleGAN) and Attention U-Net\nwere employed to achieve the image contrast transformation between neonatal and\n6-month images and perform tissue segmentation on the synthesized 6-month\nimages (neonatal images with 6-month intensity contrast), respectively.\nMoreover, we incorporated the segmentation outputs from Infant Brain Extraction\nand Analysis Toolbox (iBEAT) and another Attention U-Net to further enhance the\nperformance and construct the end-to-end segmentation pipeline. Our evaluation\nwith real 6-month images achieved a DICE score of 0.92, an HD95 of 1.6, and an\nASSD of 0.42.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, 4 figures, to be published in MICCAI PIPPI workshop",
    "pdf_url": "http://arxiv.org/pdf/2408.15198v1",
    "published_date": "2024-08-27 16:58:23 UTC",
    "updated_date": "2024-08-27 16:58:23 UTC"
  },
  {
    "arxiv_id": "2408.15185v2",
    "title": "Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose Tokenization and Transformer",
    "authors": [
      "Ghazal Alinezhad Noghre",
      "Armin Danesh Pazho",
      "Hamed Tabkhi"
    ],
    "abstract": "Video Anomaly Detection (VAD) presents a significant challenge in computer\nvision, particularly due to the unpredictable and infrequent nature of\nanomalous events, coupled with the diverse and dynamic environments in which\nthey occur. Human-centric VAD, a specialized area within this domain, faces\nadditional complexities, including variations in human behavior, potential\nbiases in data, and substantial privacy concerns related to human subjects.\nThese issues complicate the development of models that are both robust and\ngeneralizable. To address these challenges, recent advancements have focused on\npose-based VAD, which leverages human pose as a high-level feature to mitigate\nprivacy concerns, reduce appearance biases, and minimize background\ninterference. In this paper, we introduce SPARTA, a novel transformer-based\narchitecture designed specifically for human-centric pose-based VAD. SPARTA\nintroduces an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP)\ntokenization method that produces an enriched representation of human motion\nover time. This approach ensures that the transformer's attention mechanism\ncaptures both spatial and temporal patterns simultaneously, rather than\nfocusing on only one aspect. The addition of the relative pose further\nemphasizes subtle deviations from normal human movements. The architecture's\ncore, a novel Unified Encoder Twin Decoders (UETD) transformer, significantly\nimproves the detection of anomalous behaviors in video data. Extensive\nevaluations across multiple benchmark datasets demonstrate that SPARTA\nconsistently outperforms existing methods, establishing a new state-of-the-art\nin pose-based VAD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15185v2",
    "published_date": "2024-08-27 16:40:14 UTC",
    "updated_date": "2025-03-17 14:05:49 UTC"
  },
  {
    "arxiv_id": "2408.15305v1",
    "title": "Parameter-Efficient Quantized Mixture-of-Experts Meets Vision-Language Instruction Tuning for Semiconductor Electron Micrograph Analysis",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Chidaksh Ravuru",
      "Geethan Sannidhi",
      "Venkataramana Runkana"
    ],
    "abstract": "Semiconductors, crucial to modern electronics, are generally under-researched\nin foundational models. It highlights the need for research to enhance the\nsemiconductor device technology portfolio and aid in high-end device\nfabrication. In this paper, we introduce sLAVA, a small-scale vision-language\nassistant tailored for semiconductor manufacturing, with a focus on electron\nmicroscopy image analysis. It addresses challenges of data scarcity and\nacquiring high-quality, expert-annotated data. We employ a teacher-student\nparadigm, using a foundational vision language model like GPT-4 as a teacher to\ncreate instruction-following multimodal data for customizing the student model,\nsLAVA, for electron microscopic image analysis tasks on consumer hardware with\nlimited budgets. Our approach allows enterprises to further fine-tune the\nproposed framework with their proprietary data securely within their own\ninfrastructure, protecting intellectual property. Rigorous experiments validate\nthat our framework surpasses traditional methods, handles data shifts, and\nenables high-throughput screening.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper published at ICML 2024 Workshop on Foundation Models in the\n  Wild",
    "pdf_url": "http://arxiv.org/pdf/2408.15305v1",
    "published_date": "2024-08-27 15:59:26 UTC",
    "updated_date": "2024-08-27 15:59:26 UTC"
  },
  {
    "arxiv_id": "2408.15737v1",
    "title": "TCNFormer: Temporal Convolutional Network Former for Short-Term Wind Speed Forecasting",
    "authors": [
      "Abid Hasan Zim",
      "Aquib Iqbal",
      "Asad Malik",
      "Zhicheng Dong",
      "Hanzhou Wu"
    ],
    "abstract": "Global environmental challenges and rising energy demands have led to\nextensive exploration of wind energy technologies. Accurate wind speed\nforecasting (WSF) is crucial for optimizing wind energy capture and ensuring\nsystem stability. However, predicting wind speed remains challenging due to its\ninherent randomness, fluctuation, and unpredictability. This study proposes the\nTemporal Convolutional Network Former (TCNFormer) for short-term (12-hour) wind\nspeed forecasting. The TCNFormer integrates the Temporal Convolutional Network\n(TCN) and transformer encoder to capture the spatio-temporal features of wind\nspeed. The transformer encoder consists of two distinct attention mechanisms:\ncausal temporal multi-head self-attention (CT-MSA) and temporal external\nattention (TEA). CT-MSA ensures that the output of a step derives only from\nprevious steps, i.e., causality. Locality is also introduced to improve\nefficiency. TEA explores potential relationships between different sample\nsequences in wind speed data. This study utilizes wind speed data from the NASA\nPrediction of Worldwide Energy Resources (NASA POWER) of Patenga Sea Beach,\nChittagong, Bangladesh (latitude 22.2352{\\deg} N, longitude 91.7914{\\deg} E)\nover a year (six seasons). The findings indicate that the TCNFormer outperforms\nstate-of-the-art models in prediction accuracy. The proposed TCNFormer presents\na promising method for spatio-temporal WSF and may achieve desirable\nperformance in real-world applications of wind power systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15737v1",
    "published_date": "2024-08-27 15:35:42 UTC",
    "updated_date": "2024-08-27 15:35:42 UTC"
  },
  {
    "arxiv_id": "2408.15128v1",
    "title": "Evaluating the Energy Consumption of Machine Learning: Systematic Literature Review and Experiments",
    "authors": [
      "Charlotte Rodriguez",
      "Laura Degioanni",
      "Laetitia Kameni",
      "Richard Vidal",
      "Giovanni Neglia"
    ],
    "abstract": "Monitoring, understanding, and optimizing the energy consumption of Machine\nLearning (ML) are various reasons why it is necessary to evaluate the energy\nusage of ML. However, there exists no universal tool that can answer this\nquestion for all use cases, and there may even be disagreement on how to\nevaluate energy consumption for a specific use case. Tools and methods are\nbased on different approaches, each with their own advantages and drawbacks,\nand they need to be mapped out and explained in order to select the most\nsuitable one for a given situation. We address this challenge through two\napproaches. First, we conduct a systematic literature review of all tools and\nmethods that permit to evaluate the energy consumption of ML (both at training\nand at inference), irrespective of whether they were originally designed for\nmachine learning or general software. Second, we develop and use an\nexperimental protocol to compare a selection of these tools and methods. The\ncomparison is both qualitative and quantitative on a range of ML tasks of\ndifferent nature (vision, language) and computational complexity. The\nsystematic literature review serves as a comprehensive guide for understanding\nthe array of tools and methods used in evaluating energy consumption of ML, for\nvarious use cases going from basic energy monitoring to consumption\noptimization. Two open-source repositories are provided for further\nexploration. The first one contains tools that can be used to replicate this\nwork or extend the current review. The second repository houses the\nexperimental protocol, allowing users to augment the protocol with new ML\ncomputing tasks and additional energy evaluation tools.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "I.2"
    ],
    "primary_category": "cs.LG",
    "comment": "52 pages,",
    "pdf_url": "http://arxiv.org/pdf/2408.15128v1",
    "published_date": "2024-08-27 15:08:06 UTC",
    "updated_date": "2024-08-27 15:08:06 UTC"
  },
  {
    "arxiv_id": "2408.15301v2",
    "title": "The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization",
    "authors": [
      "Minghai Qin"
    ],
    "abstract": "We have observed a distinctive quantization-related behavior in the\nLLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and\nLLaMA3/3.1/3.2-1B/3B/8B/405B models. Quantization is a crucial technique for\ndeploying large language models (LLMs) efficiently. The impact of W8A8\npost-training quantization on model accuracy, especially on the recently\nreleased LLaMA3/3.1 model series, remains contentious. In this paper, we\nexplore three key questions: What makes the LLaMA3-70B model series uniquely\nvulnerable to quantization? Why is this the case? And how can the issue be\naddressed? We empirically investigate multiple LLMs featured on an open LLM\nleaderboard, discovering that the LLaMA3-70B model series have a unique\naccuracy degradation behavior with W8A8 per-channel post-training quantization.\nIn contrast, other model series such as LLaMA2, LLaMA3/3.1-8B, LLaMA3.2, Qwen,\nMixtral, Mistral, Phi-3, and Falcon demonstrate robust performance with W8A8.\nContrary to previous assertions attributing degradation to the large dynamic\nrange of activations, our findings indicate that the weight distribution of the\nLLaMA3-70B is the primary factor behind the vulnerability. By meticulously\nanalyzing the distinct characteristics of weight distributions across\nTransformer blocks, we propose two solutions that make different tradeoffs in\nhardware/software overhead. First, we propose a mixed strategy where less than\n3\\% of the layers employ finer per-group W8A8 quantization granularity. Second,\nwe introduce a bi-smoothing strategy that balances quantization errors between\nweights and activations while maintaining per-channel quantization throughout.\nExperimental results demonstrate that both strategies effectively preserve the\naccuracy of the entire LLaMA3-70B model series under W8A8 quantization,\nachieving performance on par with their FP16 counterparts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 41 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15301v2",
    "published_date": "2024-08-27 15:03:01 UTC",
    "updated_date": "2024-10-01 09:05:45 UTC"
  },
  {
    "arxiv_id": "2408.15121v1",
    "title": "Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis",
    "authors": [
      "Francesco Sovrano",
      "Michael Lognoul",
      "Giulia Vilone"
    ],
    "abstract": "Significant investment and development have gone into integrating Artificial\nIntelligence (AI) in medical and healthcare applications, leading to advanced\ncontrol systems in medical technology. However, the opacity of AI systems\nraises concerns about essential characteristics needed in such sensitive\napplications, like transparency and trustworthiness. Our study addresses these\nconcerns by investigating a process for selecting the most adequate Explainable\nAI (XAI) methods to comply with the explanation requirements of key EU\nregulations in the context of smart bioelectronics for medical devices. The\nadopted methodology starts with categorising smart devices by their control\nmechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving\ninto their technology. Then, we analyse these regulations to define their\nexplainability requirements for the various devices and related goals.\nSimultaneously, we classify XAI methods by their explanatory objectives. This\nallows for matching legal explainability requirements with XAI explanatory\ngoals and determining the suitable XAI algorithms for achieving them. Our\nfindings provide a nuanced understanding of which XAI algorithms align better\nwith EU regulations for different types of medical devices. We demonstrate this\nthrough practical case studies on different neural implants, from chronic\ndisease management to advanced prosthetics. This study fills a crucial gap in\naligning XAI applications in bioelectronics with stringent provisions of EU\nregulations. It provides a practical framework for developers and researchers,\nensuring their AI innovations advance healthcare technology and adhere to legal\nand ethical standards.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at ECAI 2024, main-track",
    "pdf_url": "http://arxiv.org/pdf/2408.15121v1",
    "published_date": "2024-08-27 14:59:27 UTC",
    "updated_date": "2024-08-27 14:59:27 UTC"
  },
  {
    "arxiv_id": "2408.15119v3",
    "title": "A Permuted Autoregressive Approach to Word-Level Recognition for Urdu Digital Text",
    "authors": [
      "Ahmed Mustafa",
      "Muhammad Tahir Rafique",
      "Muhammad Ijlal Baig",
      "Hasan Sajid",
      "Muhammad Jawad Khan",
      "Karam Dad Kallu"
    ],
    "abstract": "This research paper introduces a novel word-level Optical Character\nRecognition (OCR) model specifically designed for digital Urdu text, leveraging\ntransformer-based architectures and attention mechanisms to address the\ndistinct challenges of Urdu script recognition, including its diverse text\nstyles, fonts, and variations. The model employs a permuted autoregressive\nsequence (PARSeq) architecture, which enhances its performance by enabling\ncontext-aware inference and iterative refinement through the training of\nmultiple token permutations. This method allows the model to adeptly manage\ncharacter reordering and overlapping characters, commonly encountered in Urdu\nscript. Trained on a dataset comprising approximately 160,000 Urdu text images,\nthe model demonstrates a high level of accuracy in capturing the intricacies of\nUrdu script, achieving a CER of 0.178. Despite ongoing challenges in handling\ncertain text variations, the model exhibits superior accuracy and effectiveness\nin practical applications. Future work will focus on refining the model through\nadvanced data augmentation techniques and the integration of context-aware\nlanguage models to further enhance its performance and robustness in Urdu text\nrecognition.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15119v3",
    "published_date": "2024-08-27 14:58:13 UTC",
    "updated_date": "2024-08-30 15:29:08 UTC"
  },
  {
    "arxiv_id": "2409.00107v1",
    "title": "Evaluating the Impact of Multiple DER Aggregators on Wholesale Energy Markets: A Hybrid Mean Field Approach",
    "authors": [
      "Jun He",
      "Andrew L. Liu"
    ],
    "abstract": "The integration of distributed energy resources (DERs) into wholesale energy\nmarkets can greatly enhance grid flexibility, improve market efficiency, and\ncontribute to a more sustainable energy future. As DERs -- such as solar PV\npanels and energy storage -- proliferate, effective mechanisms are needed to\nensure that small prosumers can participate meaningfully in these markets. We\nstudy a wholesale market model featuring multiple DER aggregators, each\ncontrolling a portfolio of DER resources and bidding into the market on behalf\nof the DER asset owners. The key of our approach lies in recognizing the\nrepeated nature of market interactions the ability of participants to learn and\nadapt over time. Specifically, Aggregators repeatedly interact with each other\nand with other suppliers in the wholesale market, collectively shaping\nwholesale electricity prices (aka the locational marginal prices (LMPs)). We\nmodel this multi-agent interaction using a mean-field game (MFG), which uses\nmarket information -- reflecting the average behavior of market participants --\nto enable each aggregator to predict long-term LMP trends and make informed\ndecisions. For each aggregator, because they control the DERs within their\nportfolio under certain contract structures, we employ a mean-field control\n(MFC) approach (as opposed to a MFG) to learn an optimal policy that maximizes\nthe total rewards of the DERs under their management. We also propose a\nreinforcement learning (RL)-based method to help each agent learn optimal\nstrategies within the MFG framework, enhancing their ability to adapt to market\nconditions and uncertainties. Numerical simulations show that LMPs quickly\nreach a steady state in the hybrid mean-field approach. Furthermore, our\nresults demonstrate that the combination of energy storage and mean-field\nlearning significantly reduces price volatility compared to scenarios without\nstorage.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "econ.GN",
      "math.OC",
      "q-fin.EC"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00107v1",
    "published_date": "2024-08-27 14:56:28 UTC",
    "updated_date": "2024-08-27 14:56:28 UTC"
  },
  {
    "arxiv_id": "2408.15116v1",
    "title": "Evaluating Stability of Unreflective Alignment",
    "authors": [
      "James Lucassen",
      "Mark Henry",
      "Philippa Wright",
      "Owen Yeung"
    ],
    "abstract": "Many theoretical obstacles to AI alignment are consequences of reflective\nstability - the problem of designing alignment mechanisms that the AI would not\ndisable if given the option. However, problems stemming from reflective\nstability are not obviously present in current LLMs, leading to disagreement\nover whether they will need to be solved to enable safe delegation of cognitive\nlabor. In this paper, we propose Counterfactual Priority Change (CPC)\ndestabilization as a mechanism by which reflective stability problems may arise\nin future LLMs. We describe two risk factors for CPC-destabilization: 1)\nCPC-based stepping back and 2) preference instability. We develop preliminary\nevaluations for each of these risk factors, and apply them to frontier LLMs.\nOur findings indicate that in current LLMs, increased scale and capability are\nassociated with increases in both CPC-based stepping back and preference\ninstability, suggesting that CPC-destabilization may cause reflective stability\nproblems in future LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15116v1",
    "published_date": "2024-08-27 14:55:15 UTC",
    "updated_date": "2024-08-27 14:55:15 UTC"
  },
  {
    "arxiv_id": "2408.15114v1",
    "title": "Few-Shot Unsupervised Implicit Neural Shape Representation Learning with Spatial Adversaries",
    "authors": [
      "Amine Ouasfi",
      "Adnane Boukhayma"
    ],
    "abstract": "Implicit Neural Representations have gained prominence as a powerful\nframework for capturing complex data modalities, encompassing a wide range from\n3D shapes to images and audio. Within the realm of 3D shape representation,\nNeural Signed Distance Functions (SDF) have demonstrated remarkable potential\nin faithfully encoding intricate shape geometry. However, learning SDFs from\nsparse 3D point clouds in the absence of ground truth supervision remains a\nvery challenging task. While recent methods rely on smoothness priors to\nregularize the learning, our method introduces a regularization term that\nleverages adversarial samples around the shape to improve the learned SDFs.\nThrough extensive experiments and evaluations, we illustrate the efficacy of\nour proposed method, highlighting its capacity to improve SDF learning with\nrespect to baselines and the state-of-the-art using synthetic and real data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.15114v1",
    "published_date": "2024-08-27 14:54:33 UTC",
    "updated_date": "2024-08-27 14:54:33 UTC"
  },
  {
    "arxiv_id": "2409.00106v1",
    "title": "Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis",
    "authors": [
      "Aishik Nagar",
      "Shantanu Jaiswal",
      "Cheston Tan"
    ],
    "abstract": "Vision-language models (VLMs) have shown impressive zero- and few-shot\nperformance on real-world visual question answering (VQA) benchmarks, alluding\nto their capabilities as visual reasoning engines. However, the benchmarks\nbeing used conflate \"pure\" visual reasoning with world knowledge, and also have\nquestions that involve a limited number of reasoning steps. Thus, it remains\nunclear whether a VLM's apparent visual reasoning performance is due to its\nworld knowledge, or due to actual visual reasoning capabilities.\n  To clarify this ambiguity, we systematically benchmark and dissect the\nzero-shot visual reasoning capabilities of VLMs through synthetic datasets that\nrequire minimal world knowledge, and allow for analysis over a broad range of\nreasoning steps. We focus on two novel aspects of zero-shot visual reasoning:\ni) evaluating the impact of conveying scene information as either visual\nembeddings or purely textual scene descriptions to the underlying large\nlanguage model (LLM) of the VLM, and ii) comparing the effectiveness of\nchain-of-thought prompting to standard prompting for zero-shot visual\nreasoning.\n  We find that the underlying LLMs, when provided textual scene descriptions,\nconsistently perform better compared to being provided visual embeddings. In\nparticular, 18% higher accuracy is achieved on the PTR dataset. We also find\nthat CoT prompting performs marginally better than standard prompting only for\nthe comparatively large GPT-3.5-Turbo (175B) model, and does worse for\nsmaller-scale models. This suggests the emergence of CoT abilities for visual\nreasoning in LLMs at larger scales even when world knowledge is limited.\nOverall, we find limitations in the abilities of VLMs and LLMs for more complex\nvisual reasoning, and highlight the important role that LLMs can play in visual\nreasoning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.00106v1",
    "published_date": "2024-08-27 14:43:54 UTC",
    "updated_date": "2024-08-27 14:43:54 UTC"
  },
  {
    "arxiv_id": "2408.15300v1",
    "title": "GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs",
    "authors": [
      "Maxim Zhelnin",
      "Viktor Moskvoretskii",
      "Egor Shvetsov",
      "Egor Venediktov",
      "Mariya Krylova",
      "Aleksandr Zuev",
      "Evgeny Burnaev"
    ],
    "abstract": "Parameter Efficient Fine-Tuning (PEFT) methods have gained popularity and\ndemocratized the usage of Large Language Models (LLMs). Recent studies have\nshown that a small subset of weights significantly impacts performance. Based\non this observation, we introduce a novel PEFT method, called Gaussian noise\nInjected Fine Tuning of Salient Weights (GIFT-SW). Our method updates only\nsalient columns, while injecting Gaussian noise into non-salient ones. To\nidentify these columns, we developeda generalized sensitivity metric that\nextends and unifies metrics from previous studies. Experiments with LLaMA\nmodels demonstrate that GIFT-SW outperforms full fine-tuning and modern PEFT\nmethods under the same computational budget. Moreover, GIFT-SW offers practical\nadvantages to recover performance of models subjected to mixed-precision\nquantization with keeping salient weights in full precision.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15300v1",
    "published_date": "2024-08-27 14:41:14 UTC",
    "updated_date": "2024-08-27 14:41:14 UTC"
  },
  {
    "arxiv_id": "2409.00105v2",
    "title": "Negation Blindness in Large Language Models: Unveiling the NO Syndrome in Image Generation",
    "authors": [
      "Mohammad Nadeem",
      "Shahab Saquib Sohail",
      "Erik Cambria",
      "Bj√∂rn W. Schuller",
      "Amir Hussain"
    ],
    "abstract": "Foundational Large Language Models (LLMs) have changed the way we perceive\ntechnology. They have been shown to excel in tasks ranging from poem writing\nand coding to essay generation and puzzle solving. With the incorporation of\nimage generation capability, they have become more comprehensive and versatile\nAI tools. At the same time, researchers are striving to identify the\nlimitations of these tools to improve them further. Currently identified flaws\ninclude hallucination, biases, and bypassing restricted commands to generate\nharmful content. In the present work, we have identified a fundamental\nlimitation related to the image generation ability of LLMs, and termed it The\nNO Syndrome. This negation blindness refers to LLMs inability to correctly\ncomprehend NO related natural language prompts to generate the desired images.\nInterestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found\nto be suffering from this syndrome. To demonstrate the generalization of this\nlimitation, we carried out simulation experiments and conducted entropy-based\nand benchmark statistical analysis tests on various LLMs in multiple languages,\nincluding English, Hindi, and French. We conclude that the NO syndrome is a\nsignificant flaw in current LLMs that needs to be addressed. A related finding\nof this study showed a consistent discrepancy between image and textual\nresponses as a result of this NO syndrome. We posit that the introduction of a\nnegation context-aware reinforcement learning based feedback loop between the\nLLMs textual response and generated image could help ensure the generated text\nis based on both the LLMs correct contextual understanding of the negation\nquery and the generated visual output.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.00105v2",
    "published_date": "2024-08-27 14:40:16 UTC",
    "updated_date": "2024-09-04 14:40:14 UTC"
  },
  {
    "arxiv_id": "2408.15101v1",
    "title": "MTMamba++: Enhancing Multi-Task Dense Scene Understanding via Mamba-Based Decoders",
    "authors": [
      "Baijiong Lin",
      "Weisen Jiang",
      "Pengguang Chen",
      "Shu Liu",
      "Ying-Cong Chen"
    ],
    "abstract": "Multi-task dense scene understanding, which trains a model for multiple dense\nprediction tasks, has a wide range of application scenarios. Capturing\nlong-range dependency and enhancing cross-task interactions are crucial to\nmulti-task dense prediction. In this paper, we propose MTMamba++, a novel\narchitecture for multi-task scene understanding featuring with a Mamba-based\ndecoder. It contains two types of core blocks: self-task Mamba (STM) block and\ncross-task Mamba (CTM) block. STM handles long-range dependency by leveraging\nstate-space models, while CTM explicitly models task interactions to facilitate\ninformation exchange across tasks. We design two types of CTM block, namely\nF-CTM and S-CTM, to enhance cross-task interaction from feature and semantic\nperspectives, respectively. Experiments on NYUDv2, PASCAL-Context, and\nCityscapes datasets demonstrate the superior performance of MTMamba++ over\nCNN-based and Transformer-based methods. The code is available at\nhttps://github.com/EnVision-Research/MTMamba.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2407.02228",
    "pdf_url": "http://arxiv.org/pdf/2408.15101v1",
    "published_date": "2024-08-27 14:36:46 UTC",
    "updated_date": "2024-08-27 14:36:46 UTC"
  },
  {
    "arxiv_id": "2408.15099v3",
    "title": "No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery",
    "authors": [
      "Alexander Rutherford",
      "Michael Beukman",
      "Timon Willi",
      "Bruno Lacerda",
      "Nick Hawes",
      "Jakob Foerster"
    ],
    "abstract": "What data or environments to use for training to improve downstream\nperformance is a longstanding and very topical question in reinforcement\nlearning. In particular, Unsupervised Environment Design (UED) methods have\ngained recent attention as their adaptive curricula promise to enable agents to\nbe robust to in- and out-of-distribution tasks. This work investigates how\nexisting UED methods select training environments, focusing on task\nprioritisation metrics. Surprisingly, despite methods aiming to maximise regret\nin theory, the practical approximations do not correlate with regret but with\nsuccess rate. As a result, a significant portion of an agent's experience comes\nfrom environments it has already mastered, offering little to no contribution\ntoward enhancing its abilities. Put differently, current methods fail to\npredict intuitive measures of ``learnability.'' Specifically, they are unable\nto consistently identify those scenarios that the agent can sometimes solve,\nbut not always. Based on our analysis, we develop a method that directly trains\non scenarios with high learnability. This simple and intuitive approach\noutperforms existing UED methods in several binary-outcome environments,\nincluding the standard domain of Minigrid and a novel setting closely inspired\nby a real-world robotics problem. We further introduce a new adversarial\nevaluation procedure for directly measuring robustness, closely mirroring the\nconditional value at risk (CVaR). We open-source all our code and present\nvisualisations of final policies here:\nhttps://github.com/amacrutherford/sampling-for-learnability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15099v3",
    "published_date": "2024-08-27 14:31:54 UTC",
    "updated_date": "2024-10-29 18:25:44 UTC"
  },
  {
    "arxiv_id": "2408.16088v1",
    "title": "Ensuring Equitable Financial Decisions: Leveraging Counterfactual Fairness and Deep Learning for Bias",
    "authors": [
      "Saish Shinde"
    ],
    "abstract": "Concerns regarding fairness and bias have been raised in recent years due to\nthe growing use of machine learning models in crucial decision-making\nprocesses, especially when it comes to delicate characteristics like gender. In\norder to address biases in machine learning models, this research paper\ninvestigates advanced bias mitigation techniques, with a particular focus on\ncounterfactual fairness in conjunction with data augmentation. The study looks\ninto how these integrated approaches can lessen gender bias in the financial\nindustry, specifically in loan approval procedures. We show that these\napproaches are effective in achieving more equitable results through thorough\ntesting and assessment on a skewed financial dataset. The findings emphasize\nhow crucial it is to use fairness-aware techniques when creating machine\nlearning models in order to guarantee morally righteous and impartial\ndecision-making.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16088v1",
    "published_date": "2024-08-27 14:28:06 UTC",
    "updated_date": "2024-08-27 14:28:06 UTC"
  },
  {
    "arxiv_id": "2408.15096v2",
    "title": "Post-processing fairness with minimal changes",
    "authors": [
      "Federico Di Gennaro",
      "Thibault Laugel",
      "Vincent Grari",
      "Xavier Renard",
      "Marcin Detyniecki"
    ],
    "abstract": "In this paper, we introduce a novel post-processing algorithm that is both\nmodel-agnostic and does not require the sensitive attribute at test time. In\naddition, our algorithm is explicitly designed to enforce minimal changes\nbetween biased and debiased predictions; a property that, while highly\ndesirable, is rarely prioritized as an explicit objective in fairness\nliterature. Our approach leverages a multiplicative factor applied to the logit\nvalue of probability scores produced by a black-box classifier. We demonstrate\nthe efficacy of our method through empirical evaluations, comparing its\nperformance against other four debiasing algorithms on two widely used datasets\nin fairness research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15096v2",
    "published_date": "2024-08-27 14:26:56 UTC",
    "updated_date": "2024-08-29 15:59:13 UTC"
  },
  {
    "arxiv_id": "2408.15079v1",
    "title": "BaichuanSEED: Sharing the Potential of ExtensivE Data Collection and Deduplication by Introducing a Competitive Large Language Model Baseline",
    "authors": [
      "Guosheng Dong",
      "Da Pan",
      "Yiding Sun",
      "Shusen Zhang",
      "Zheng Liang",
      "Xin Wu",
      "Yanjun Shen",
      "Fan Yang",
      "Haoze Sun",
      "Tianpeng Li",
      "Mingan Lin",
      "Jianhua Xu",
      "Yufan Zhang",
      "Xiaonan Nie",
      "Lei Su",
      "Bingning Wang",
      "Wentao Zhang",
      "Jiaxin Mao",
      "Zenan Zhou",
      "Weipeng Chen"
    ],
    "abstract": "The general capabilities of Large Language Models (LLM) highly rely on the\ncomposition and selection on extensive pretraining datasets, treated as\ncommercial secrets by several institutions. To mitigate this issue, we\nopen-source the details of a universally applicable data processing pipeline\nand validate its effectiveness and potential by introducing a competitive LLM\nbaseline. Specifically, the data processing pipeline consists of broad\ncollection to scale up and reweighting to improve quality. We then pretrain a\n7B model BaichuanSEED with 3T tokens processed by our pipeline without any\ndeliberate downstream task-related optimization, followed by an easy but\neffective supervised fine-tuning stage. BaichuanSEED demonstrates consistency\nand predictability throughout training and achieves comparable performance on\ncomprehensive benchmarks with several commercial advanced large language\nmodels, such as Qwen1.5 and Llama3. We also conduct several heuristic\nexperiments to discuss the potential for further optimization of downstream\ntasks, such as mathematics and coding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15079v1",
    "published_date": "2024-08-27 14:08:23 UTC",
    "updated_date": "2024-08-27 14:08:23 UTC"
  },
  {
    "arxiv_id": "2408.15077v2",
    "title": "MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of Children with Autism Spectrum Disorder",
    "authors": [
      "Pavan Uttej Ravva",
      "Behdokht Kiafar",
      "Pinar Kullu",
      "Jicheng Li",
      "Anjana Bhat",
      "Roghayeh Leila Barmaki"
    ],
    "abstract": "Autism spectrum disorder (ASD) is characterized by significant challenges in\nsocial interaction and comprehending communication signals. Recently,\ntherapeutic interventions for ASD have increasingly utilized Deep learning\npowered-computer vision techniques to monitor individual progress over time.\nThese models are trained on private, non-public datasets from the autism\ncommunity, creating challenges in comparing results across different models due\nto privacy-preserving data-sharing issues. This work introduces MMASD+, an\nenhanced version of the novel open-source dataset called Multimodal ASD\n(MMASD). MMASD+ consists of diverse data modalities, including 3D-Skeleton, 3D\nBody Mesh, and Optical Flow data. It integrates the capabilities of Yolov8 and\nDeep SORT algorithms to distinguish between the therapist and children,\naddressing a significant barrier in the original dataset. Additionally, a\nMultimodal Transformer framework is proposed to predict 11 action types and the\npresence of ASD. This framework achieves an accuracy of 95.03% for predicting\naction types and 96.42% for predicting ASD presence, demonstrating over a 10%\nimprovement compared to models trained on single data modalities. These\nfindings highlight the advantages of integrating multiple data modalities\nwithin the Multimodal Transformer framework.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15077v2",
    "published_date": "2024-08-27 14:05:48 UTC",
    "updated_date": "2024-08-28 20:30:29 UTC"
  },
  {
    "arxiv_id": "2408.15076v1",
    "title": "MiWaves Reinforcement Learning Algorithm",
    "authors": [
      "Susobhan Ghosh",
      "Yongyi Guo",
      "Pei-Yao Hung",
      "Lara Coughlin",
      "Erin Bonar",
      "Inbal Nahum-Shani",
      "Maureen Walton",
      "Susan Murphy"
    ],
    "abstract": "The escalating prevalence of cannabis use poses a significant public health\nchallenge globally. In the U.S., cannabis use is more prevalent among emerging\nadults (EAs) (ages 18-25) than any other age group, with legalization in the\nmultiple states contributing to a public perception that cannabis is less risky\nthan in prior decades. To address this growing concern, we developed MiWaves, a\nreinforcement learning (RL) algorithm designed to optimize the delivery of\npersonalized intervention prompts to reduce cannabis use among EAs. MiWaves\nleverages domain expertise and prior data to tailor the likelihood of delivery\nof intervention messages. This paper presents a comprehensive overview of the\nalgorithm's design, including key decisions and experimental outcomes. The\nfinalized MiWaves RL algorithm was deployed in a clinical trial from March to\nMay 2024.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2402.17739",
    "pdf_url": "http://arxiv.org/pdf/2408.15076v1",
    "published_date": "2024-08-27 14:04:04 UTC",
    "updated_date": "2024-08-27 14:04:04 UTC"
  },
  {
    "arxiv_id": "2408.15073v1",
    "title": "Interactive dense pixel visualizations for time series and model attribution explanations",
    "authors": [
      "Udo Schlegel",
      "Daniel A. Keim"
    ],
    "abstract": "The field of Explainable Artificial Intelligence (XAI) for Deep Neural\nNetwork models has developed significantly, offering numerous techniques to\nextract explanations from models. However, evaluating explanations is often not\ntrivial, and differences in applied metrics can be subtle, especially with\nnon-intelligible data. Thus, there is a need for visualizations tailored to\nexplore explanations for domains with such data, e.g., time series. We propose\nDAVOTS, an interactive visual analytics approach to explore raw time series\ndata, activations of neural networks, and attributions in a dense-pixel\nvisualization to gain insights into the data, models' decisions, and\nexplanations. To further support users in exploring large datasets, we apply\nclustering approaches to the visualized data domains to highlight groups and\npresent ordering strategies for individual and combined data exploration to\nfacilitate finding patterns. We visualize a CNN trained on the FordA dataset to\ndemonstrate the approach.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 2 figures, accepted at MLVIS 2023",
    "pdf_url": "http://arxiv.org/pdf/2408.15073v1",
    "published_date": "2024-08-27 14:02:21 UTC",
    "updated_date": "2024-08-27 14:02:21 UTC"
  },
  {
    "arxiv_id": "2409.00103v1",
    "title": "Nuance Matters: Probing Epistemic Consistency in Causal Reasoning",
    "authors": [
      "Shaobo Cui",
      "Junyou Li",
      "Luca Mouchel",
      "Yiyang Feng",
      "Boi Faltings"
    ],
    "abstract": "To address this gap, our study introduces the concept of causal epistemic\nconsistency, which focuses on the self-consistency of Large Language Models\n(LLMs) in differentiating intermediates with nuanced differences in causal\nreasoning. We propose a suite of novel metrics -- intensity ranking\nconcordance, cross-group position agreement, and intra-group clustering -- to\nevaluate LLMs on this front. Through extensive empirical studies on 21\nhigh-profile LLMs, including GPT-4, Claude3, and LLaMA3-70B, we have favoring\nevidence that current models struggle to maintain epistemic consistency in\nidentifying the polarity and intensity of intermediates in causal reasoning.\nAdditionally, we explore the potential of using internal token probabilities as\nan auxiliary tool to maintain causal epistemic consistency. In summary, our\nstudy bridges a critical gap in AI research by investigating the\nself-consistency over fine-grained intermediates involved in causal reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.00103v1",
    "published_date": "2024-08-27 13:42:34 UTC",
    "updated_date": "2024-08-27 13:42:34 UTC"
  },
  {
    "arxiv_id": "2408.15299v1",
    "title": "TourSynbio: A Multi-Modal Large Model and Agent Framework to Bridge Text and Protein Sequences for Protein Engineering",
    "authors": [
      "Yiqing Shen",
      "Zan Chen",
      "Michail Mamalakis",
      "Yungeng Liu",
      "Tianbin Li",
      "Yanzhou Su",
      "Junjun He",
      "Pietro Li√≤",
      "Yu Guang Wang"
    ],
    "abstract": "The structural similarities between protein sequences and natural languages\nhave led to parallel advancements in deep learning across both domains. While\nlarge language models (LLMs) have achieved much progress in the domain of\nnatural language processing, their potential in protein engineering remains\nlargely unexplored. Previous approaches have equipped LLMs with protein\nunderstanding capabilities by incorporating external protein encoders, but this\nfails to fully leverage the inherent similarities between protein sequences and\nnatural languages, resulting in sub-optimal performance and increased model\ncomplexity. To address this gap, we present TourSynbio-7B, the first\nmulti-modal large model specifically designed for protein engineering tasks\nwithout external protein encoders. TourSynbio-7B demonstrates that LLMs can\ninherently learn to understand proteins as language. The model is post-trained\nand instruction fine-tuned on InternLM2-7B using ProteinLMDataset, a dataset\ncomprising 17.46 billion tokens of text and protein sequence for\nself-supervised pretraining and 893K instructions for supervised fine-tuning.\nTourSynbio-7B outperforms GPT-4 on the ProteinLMBench, a benchmark of 944\nmanually verified multiple-choice questions, with 62.18% accuracy. Leveraging\nTourSynbio-7B's enhanced protein sequence understanding capability, we\nintroduce TourSynbio-Agent, an innovative framework capable of performing\nvarious protein engineering tasks, including mutation analysis, inverse\nfolding, protein folding, and visualization. TourSynbio-Agent integrates\npreviously disconnected deep learning models in the protein engineering domain,\noffering a unified conversational user interface for improved usability.\nFinally, we demonstrate the efficacy of TourSynbio-7B and TourSynbio-Agent\nthrough two wet lab case studies on vanilla key enzyme modification and steroid\ncompound catalysis.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15299v1",
    "published_date": "2024-08-27 13:36:00 UTC",
    "updated_date": "2024-08-27 13:36:00 UTC"
  },
  {
    "arxiv_id": "2408.15055v1",
    "title": "Causal Rule Forest: Toward Interpretable and Precise Treatment Effect Estimation",
    "authors": [
      "Chan Hsu",
      "Jun-Ting Wu",
      "Yihuang Kang"
    ],
    "abstract": "Understanding and inferencing Heterogeneous Treatment Effects (HTE) and\nConditional Average Treatment Effects (CATE) are vital for developing\npersonalized treatment recommendations. Many state-of-the-art approaches\nachieve inspiring performance in estimating HTE on benchmark datasets or\nsimulation studies. However, the indirect predicting manner and complex model\narchitecture reduce the interpretability of these approaches. To mitigate the\ngap between predictive performance and heterogeneity interpretability, we\nintroduce the Causal Rule Forest (CRF), a novel approach to learning hidden\npatterns from data and transforming the patterns into interpretable multi-level\nBoolean rules. By training the other interpretable causal inference models with\ndata representation learned by CRF, we can reduce the predictive errors of\nthese models in estimating HTE and CATE, while keeping their interpretability\nfor identifying subgroups that a treatment is more effective. Our experiments\nunderscore the potential of CRF to advance personalized interventions and\npolicies, paving the way for future research to enhance its scalability and\napplication across complex causal inference challenges.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The 25th IEEE International Conference on Information Reuse and\n  Integration for Data Science (IRI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.15055v1",
    "published_date": "2024-08-27 13:32:31 UTC",
    "updated_date": "2024-08-27 13:32:31 UTC"
  },
  {
    "arxiv_id": "2408.15041v1",
    "title": "Earth Observation Satellite Scheduling with Graph Neural Networks",
    "authors": [
      "Antoine Jacquet",
      "Guillaume Infantes",
      "Nicolas Meuleau",
      "Emmanuel Benazera",
      "St√©phanie Roussel",
      "Vincent Baudoui",
      "Jonathan Guerra"
    ],
    "abstract": "The Earth Observation Satellite Planning (EOSP) is a difficult optimization\nproblem with considerable practical interest. A set of requested observations\nmust be scheduled on an agile Earth observation satellite while respecting\nconstraints on their visibility window, as well as maneuver constraints that\nimpose varying delays between successive observations. In addition, the problem\nis largely oversubscribed: there are much more candidate observations than what\ncan possibly be achieved. Therefore, one must select the set of observations\nthat will be performed while maximizing their weighted cumulative benefit, and\npropose a feasible schedule for these observations. As previous work mostly\nfocused on heuristic and iterative search algorithms, this paper presents a new\ntechnique for selecting and scheduling observations based on Graph Neural\nNetworks (GNNs) and Deep Reinforcement Learning (DRL). GNNs are used to extract\nrelevant information from the graphs representing instances of the EOSP, and\nDRL drives the search for optimal schedules. Our simulations show that it is\nable to learn on small problem instances and generalize to larger real-world\ninstances, with very competitive performance compared to traditional\napproaches.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at 17th European Workshop on Reinforcement Learning (EWRL\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.15041v1",
    "published_date": "2024-08-27 13:10:26 UTC",
    "updated_date": "2024-08-27 13:10:26 UTC"
  },
  {
    "arxiv_id": "2408.15037v1",
    "title": "Evidence-Enhanced Triplet Generation Framework for Hallucination Alleviation in Generative Question Answering",
    "authors": [
      "Haowei Du",
      "Huishuai Zhang",
      "Dongyan Zhao"
    ],
    "abstract": "To address the hallucination in generative question answering (GQA) where the\nanswer can not be derived from the document, we propose a novel\nevidence-enhanced triplet generation framework, EATQA, encouraging the model to\npredict all the combinations of (Question, Evidence, Answer) triplet by\nflipping the source pair and the target label to understand their logical\nrelationships, i.e., predict Answer(A), Question(Q), and Evidence(E) given a\nQE, EA, and QA pairs, respectively. Furthermore, we bridge the distribution gap\nto distill the knowledge from evidence in inference stage. Our framework\nensures the model to learn the logical relation between query, evidence and\nanswer, which simultaneously improves the evidence generation and query\nanswering. In this paper, we apply EATQA to LLama and it outperforms other\nLLMs-based methods and hallucination mitigation approaches on two challenging\nGQA benchmarks. Further analysis shows that our method not only keeps prior\nknowledge within LLM, but also mitigates hallucination and generates faithful\nanswers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15037v1",
    "published_date": "2024-08-27 13:07:07 UTC",
    "updated_date": "2024-08-27 13:07:07 UTC"
  },
  {
    "arxiv_id": "2408.15032v1",
    "title": "Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology",
    "authors": [
      "Yuqi Zhang",
      "Xiaoqian Zhang",
      "Jiakai Wang",
      "Yuancheng Yang",
      "Taiying Peng",
      "Chao Tong"
    ],
    "abstract": "Computational pathology (CPath) has significantly advanced the clinical\npractice of pathology. Despite the progress made, Multiple Instance Learning\n(MIL), a promising paradigm within CPath, continues to face challenges,\nparticularly related to incomplete information utilization. Existing\nframeworks, such as those based on Convolutional Neural Networks (CNNs),\nattention, and selective scan space state sequential model (SSM), lack\nsufficient flexibility and scalability in fusing diverse features, and cannot\neffectively fuse diverse features. Additionally, current approaches do not\nadequately exploit order-related and order-independent features, resulting in\nsuboptimal utilization of sequence information. To address these limitations,\nwe propose a novel MIL framework called Mamba2MIL. Our framework utilizes the\nstate space duality model (SSD) to model long sequences of patches of whole\nslide images (WSIs), which, combined with weighted feature selection, supports\nthe fusion processing of more branching features and can be extended according\nto specific application needs. Moreover, we introduce a sequence transformation\nmethod tailored to varying WSI sizes, which enhances sequence-independent\nfeatures while preserving local sequence information, thereby improving\nsequence information utilization. Extensive experiments demonstrate that\nMamba2MIL surpasses state-of-the-art MIL methods. We conducted extensive\nexperiments across multiple datasets, achieving improvements in nearly all\nperformance metrics. Specifically, on the NSCLC dataset, Mamba2MIL achieves a\nbinary tumor classification AUC of 0.9533 and an accuracy of 0.8794. On the\nBRACS dataset, it achieves a multiclass classification AUC of 0.7986 and an\naccuracy of 0.4981. The code is available at\nhttps://github.com/YuqiZhang-Buaa/Mamba2MIL.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15032v1",
    "published_date": "2024-08-27 13:01:19 UTC",
    "updated_date": "2024-08-27 13:01:19 UTC"
  },
  {
    "arxiv_id": "2408.15026v1",
    "title": "Sequence-aware Pre-training for Echocardiography Probe Guidance",
    "authors": [
      "Haojun Jiang",
      "Zhenguo Sun",
      "Yu Sun",
      "Ning Jia",
      "Meng Li",
      "Shaqi Luo",
      "Shiji Song",
      "Gao Huang"
    ],
    "abstract": "Cardiac ultrasound probe guidance aims to help novices adjust the 6-DOF probe\npose to obtain high-quality sectional images. Cardiac ultrasound faces two\nmajor challenges: (1) the inherently complex structure of the heart, and (2)\nsignificant individual variations. Previous works have only learned the\npopulation-averaged 2D and 3D structures of the heart rather than personalized\ncardiac structural features, leading to a performance bottleneck. Clinically,\nwe observed that sonographers adjust their understanding of a patient's cardiac\nstructure based on prior scanning sequences, thereby modifying their scanning\nstrategies. Inspired by this, we propose a sequence-aware self-supervised\npre-training method. Specifically, our approach learns personalized 2D and 3D\ncardiac structural features by predicting the masked-out images and actions in\na scanning sequence. We hypothesize that if the model can predict the missing\ncontent it has acquired a good understanding of the personalized cardiac\nstructure. In the downstream probe guidance task, we also introduced a sequence\nmodeling approach that models individual cardiac structural information based\non the images and actions from historical scan data, enabling more accurate\nnavigation decisions. Experiments on a large-scale dataset with 1.36 million\nsamples demonstrated that our proposed sequence-aware paradigm can\nsignificantly reduce navigation errors, with translation errors decreasing by\n15.90% to 36.87% and rotation errors decreasing by 11.13% to 20.77%, compared\nto state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Tech Report",
    "pdf_url": "http://arxiv.org/pdf/2408.15026v1",
    "published_date": "2024-08-27 12:55:54 UTC",
    "updated_date": "2024-08-27 12:55:54 UTC"
  },
  {
    "arxiv_id": "2408.15018v1",
    "title": "Cross-subject Brain Functional Connectivity Analysis for Multi-task Cognitive State Evaluation",
    "authors": [
      "Jun Chen",
      "Anqi Chen",
      "Bingkun Jiang",
      "Mohammad S. Obaidat",
      "Ni Li",
      "Xinyu Zhang"
    ],
    "abstract": "Cognition refers to the function of information perception and processing,\nwhich is the fundamental psychological essence of human beings. It is\nresponsible for reasoning and decision-making, while its evaluation is\nsignificant for the aviation domain in mitigating potential safety risks.\nExisting studies tend to use varied methods for cognitive state evaluation yet\nhave limitations in timeliness, generalisation, and interpretability.\nAccordingly, this study adopts brain functional connectivity with\nelectroencephalography signals to capture associations in brain regions across\nmultiple subjects for evaluating real-time cognitive states. Specifically, a\nvirtual reality-based flight platform is constructed with multi-screen\nembedded. Three distinctive cognitive tasks are designed and each has three\ndegrees of difficulty. Thirty subjects are acquired for analysis and\nevaluation. The results are interpreted through different perspectives,\nincluding inner-subject and cross-subject for task-wise and gender-wise\nunderlying brain functional connectivity. Additionally, this study incorporates\nquestionnaire-based, task performance-based, and physiological measure-based\napproaches to fairly label the trials. A multi-class cognitive state evaluation\nis further conducted with the active brain connections. Benchmarking results\ndemonstrate that the identified brain regions have considerable influences in\ncognition, with a multi-class accuracy rate of 95.83% surpassing existing\nstudies. The derived findings bring significance to understanding the dynamic\nrelationships among human brain functional regions, cross-subject cognitive\nbehaviours, and decision-making, which have promising practical application\nvalues.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15018v1",
    "published_date": "2024-08-27 12:51:59 UTC",
    "updated_date": "2024-08-27 12:51:59 UTC"
  },
  {
    "arxiv_id": "2409.06729v1",
    "title": "How will advanced AI systems impact democracy?",
    "authors": [
      "Christopher Summerfield",
      "Lisa Argyle",
      "Michiel Bakker",
      "Teddy Collins",
      "Esin Durmus",
      "Tyna Eloundou",
      "Iason Gabriel",
      "Deep Ganguli",
      "Kobi Hackenburg",
      "Gillian Hadfield",
      "Luke Hewitt",
      "Saffron Huang",
      "Helene Landemore",
      "Nahema Marchal",
      "Aviv Ovadya",
      "Ariel Procaccia",
      "Mathias Risse",
      "Bruce Schneier",
      "Elizabeth Seger",
      "Divya Siddarth",
      "Henrik Skaug S√¶tra",
      "MH Tessler",
      "Matthew Botvinick"
    ],
    "abstract": "Advanced AI systems capable of generating humanlike text and multimodal\ncontent are now widely available. In this paper, we discuss the impacts that\ngenerative artificial intelligence may have on democratic processes. We\nconsider the consequences of AI for citizens' ability to make informed choices\nabout political representatives and issues (epistemic impacts). We ask how AI\nmight be used to destabilise or support democratic mechanisms like elections\n(material impacts). Finally, we discuss whether AI will strengthen or weaken\ndemocratic principles (foundational impacts). It is widely acknowledged that\nnew AI systems could pose significant challenges for democracy. However, it has\nalso been argued that generative AI offers new opportunities to educate and\nlearn from citizens, strengthen public discourse, help people find common\nground, and to reimagine how democracies might work better.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "25 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.06729v1",
    "published_date": "2024-08-27 12:05:59 UTC",
    "updated_date": "2024-08-27 12:05:59 UTC"
  },
  {
    "arxiv_id": "2408.14976v1",
    "title": "Prior-free Balanced Replay: Uncertainty-guided Reservoir Sampling for Long-Tailed Continual Learning",
    "authors": [
      "Lei Liu",
      "Li Liu",
      "Yawen Cui"
    ],
    "abstract": "Even in the era of large models, one of the well-known issues in continual\nlearning (CL) is catastrophic forgetting, which is significantly challenging\nwhen the continual data stream exhibits a long-tailed distribution, termed as\nLong-Tailed Continual Learning (LTCL). Existing LTCL solutions generally\nrequire the label distribution of the data stream to achieve re-balance\ntraining. However, obtaining such prior information is often infeasible in real\nscenarios since the model should learn without pre-identifying the majority and\nminority classes. To this end, we propose a novel Prior-free Balanced Replay\n(PBR) framework to learn from long-tailed data stream with less forgetting.\nConcretely, motivated by our experimental finding that the minority classes are\nmore likely to be forgotten due to the higher uncertainty, we newly design an\nuncertainty-guided reservoir sampling strategy to prioritize rehearsing\nminority data without using any prior information, which is based on the mutual\ndependence between the model and samples. Additionally, we incorporate two\nprior-free components to further reduce the forgetting issue: (1) Boundary\nconstraint is to preserve uncertain boundary supporting samples for continually\nre-estimating task boundaries. (2) Prototype constraint is to maintain the\nconsistency of learned class prototypes along with training. Our approach is\nevaluated on three standard long-tailed benchmarks, demonstrating superior\nperformance to existing CL methods and previous SOTA LTCL approach in both\ntask- and class-incremental learning settings, as well as ordered- and\nshuffled-LTCL settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14976v1",
    "published_date": "2024-08-27 11:38:01 UTC",
    "updated_date": "2024-08-27 11:38:01 UTC"
  },
  {
    "arxiv_id": "2408.15297v3",
    "title": "YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection",
    "authors": [
      "Xuanru Zhou",
      "Anshul Kashyap",
      "Steve Li",
      "Ayati Sharma",
      "Brittany Morin",
      "David Baquirin",
      "Jet Vonk",
      "Zoe Ezzes",
      "Zachary Miller",
      "Maria Luisa Gorno Tempini",
      "Jiachen Lian",
      "Gopala Krishna Anumanchipalli"
    ],
    "abstract": "Dysfluent speech detection is the bottleneck for disordered speech analysis\nand spoken language learning. Current state-of-the-art models are governed by\nrule-based systems which lack efficiency and robustness, and are sensitive to\ntemplate design. In this paper, we propose YOLO-Stutter: a first end-to-end\nmethod that detects dysfluencies in a time-accurate manner. YOLO-Stutter takes\nimperfect speech-text alignment as input, followed by a spatial feature\naggregator, and a temporal dependency extractor to perform region-wise boundary\nand class predictions. We also introduce two dysfluency corpus, VCTK-Stutter\nand VCTK-TTS, that simulate natural spoken dysfluencies including repetition,\nblock, missing, replacement, and prolongation. Our end-to-end method achieves\nstate-of-the-art performance with a minimum number of trainable parameters for\non both simulated data and real aphasia speech. Code and datasets are\nopen-sourced at https://github.com/rorizzz/YOLO-Stutter",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.15297v3",
    "published_date": "2024-08-27 11:31:12 UTC",
    "updated_date": "2024-09-15 06:20:19 UTC"
  },
  {
    "arxiv_id": "2408.14961v1",
    "title": "CVPT: Cross-Attention help Visual Prompt Tuning adapt visual task",
    "authors": [
      "Lingyun Huang",
      "Jianxu Mao",
      "Yaonan Wang",
      "Junfei Yi",
      "Ziming Tao"
    ],
    "abstract": "In recent years, the rapid expansion of model sizes has led to large-scale\npre-trained models demonstrating remarkable capabilities. Consequently, there\nhas been a trend towards increasing the scale of models. However, this trend\nintroduces significant challenges, including substantial computational costs of\ntraining and transfer to downstream tasks. To address these issues,\nParameter-Efficient Fine-Tuning (PEFT) methods have been introduced. These\nmethods optimize large-scale pre-trained models for specific tasks by\nfine-tuning a select group of parameters. Among these PEFT methods,\nadapter-based and prompt-based methods are the primary techniques.\nSpecifically, in the field of visual fine-tuning, adapters gain prominence over\nprompts because of the latter's relatively weaker performance and efficiency.\nUnder the circumstances, we refine the widely-used Visual Prompt Tuning (VPT)\nmethod, proposing Cross Visual Prompt Tuning (CVPT). CVPT calculates\ncross-attention between the prompt tokens and the embedded tokens, which allows\nus to compute the semantic relationship between them and conduct the\nfine-tuning of models exactly to adapt visual tasks better. Furthermore, we\nintroduce the weight-sharing mechanism to initialize the parameters of\ncross-attention, which avoids massive learnable parameters from cross-attention\nand enhances the representative capability of cross-attention. We conduct\ncomprehensive testing across 25 datasets and the result indicates that CVPT\nsignificantly improves VPT's performance and efficiency in visual tasks. For\nexample, on the VTAB-1K benchmark, CVPT outperforms VPT over 4% in average\naccuracy, rivaling the advanced adapter-based methods in performance and\nefficiency. Our experiments confirm that prompt-based methods can achieve\nexceptional results in visual fine-tuning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14961v1",
    "published_date": "2024-08-27 11:07:19 UTC",
    "updated_date": "2024-08-27 11:07:19 UTC"
  },
  {
    "arxiv_id": "2408.14960v1",
    "title": "Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual Progress",
    "authors": [
      "Ayomide Odumakinde",
      "Daniel D'souza",
      "Pat Verga",
      "Beyza Ermis",
      "Sara Hooker"
    ],
    "abstract": "The use of synthetic data has played a critical role in recent state-of-art\nbreakthroughs. However, overly relying on a single oracle teacher model to\ngenerate data has been shown to lead to model collapse and invite propagation\nof biases. These limitations are particularly evident in multilingual settings,\nwhere the absence of a universally effective teacher model that excels across\nall languages presents significant challenges. In this work, we address these\nextreme difference by introducing \"multilingual arbitrage\", which capitalizes\non performance variations between multiple models for a given language. To do\nso, we strategically route samples through a diverse pool of models, each with\nunique strengths in different languages. Across exhaustive experiments on\nstate-of-art models, our work suggests that arbitrage techniques allow for\nspectacular gains in performance that far outperform relying on a single\nteacher. In particular, compared to the best single teacher, we observe gains\nof up to 56.5% improvement in win rates averaged across all languages when\nswitching to multilingual arbitrage. We observe the most significant gains for\nthe least resourced languages in our pool.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14960v1",
    "published_date": "2024-08-27 11:07:15 UTC",
    "updated_date": "2024-08-27 11:07:15 UTC"
  },
  {
    "arxiv_id": "2408.14950v1",
    "title": "NeuralOOD: Improving Out-of-Distribution Generalization Performance with Brain-machine Fusion Learning Framework",
    "authors": [
      "Shuangchen Zhao",
      "Changde Du",
      "Hui Li",
      "Huiguang He"
    ],
    "abstract": "Deep Neural Networks (DNNs) have demonstrated exceptional recognition\ncapabilities in traditional computer vision (CV) tasks. However, existing CV\nmodels often suffer a significant decrease in accuracy when confronted with\nout-of-distribution (OOD) data. In contrast to these DNN models, human can\nmaintain a consistently low error rate when facing OOD scenes, partly\nattributed to the rich prior cognitive knowledge stored in the human brain.\nPrevious OOD generalization researches only focus on the single modal,\noverlooking the advantages of multimodal learning method. In this paper, we\nutilize the multimodal learning method to improve the OOD generalization and\npropose a novel Brain-machine Fusion Learning (BMFL) framework. We adopt the\ncross-attention mechanism to fuse the visual knowledge from CV model and prior\ncognitive knowledge from the human brain. Specially, we employ a pre-trained\nvisual neural encoding model to predict the functional Magnetic Resonance\nImaging (fMRI) from visual features which eliminates the need for the fMRI data\ncollection and pre-processing, effectively reduces the workload associated with\nconventional BMFL methods. Furthermore, we construct a brain transformer to\nfacilitate the extraction of knowledge inside the fMRI data. Moreover, we\nintroduce the Pearson correlation coefficient maximization regularization\nmethod into the training process, which improves the fusion capability with\nbetter constrains. Our model outperforms the DINOv2 and baseline models on the\nImageNet-1k validation dataset as well as six curated OOD datasets, showcasing\nits superior performance in diverse scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14950v1",
    "published_date": "2024-08-27 10:54:37 UTC",
    "updated_date": "2024-08-27 10:54:37 UTC"
  },
  {
    "arxiv_id": "2408.14935v1",
    "title": "Quotient Normalized Maximum Likelihood Criterion for Learning Bayesian Network Structures",
    "authors": [
      "Tomi Silander",
      "Janne Lepp√§-aho",
      "Elias J√§√§saari",
      "Teemu Roos"
    ],
    "abstract": "We introduce an information theoretic criterion for Bayesian network\nstructure learning which we call quotient normalized maximum likelihood (qNML).\nIn contrast to the closely related factorized normalized maximum likelihood\ncriterion, qNML satisfies the property of score equivalence. It is also\ndecomposable and completely free of adjustable hyperparameters. For practical\ncomputations, we identify a remarkably accurate approximation proposed earlier\nby Szpankowski and Weinberger. Experiments on both simulated and real data\ndemonstrate that the new criterion leads to parsimonious models with good\npredictive accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AISTATS 2018",
    "pdf_url": "http://arxiv.org/pdf/2408.14935v1",
    "published_date": "2024-08-27 10:17:22 UTC",
    "updated_date": "2024-08-27 10:17:22 UTC"
  },
  {
    "arxiv_id": "2408.14925v1",
    "title": "Distance-Forward Learning: Enhancing the Forward-Forward Algorithm Towards High-Performance On-Chip Learning",
    "authors": [
      "Yujie Wu",
      "Siyuan Xu",
      "Jibin Wu",
      "Lei Deng",
      "Mingkun Xu",
      "Qinghao Wen",
      "Guoqi Li"
    ],
    "abstract": "The Forward-Forward (FF) algorithm was recently proposed as a local learning\nmethod to address the limitations of backpropagation (BP), offering biological\nplausibility along with memory-efficient and highly parallelized computational\nbenefits. However, it suffers from suboptimal performance and poor\ngeneralization, largely due to inadequate theoretical support and a lack of\neffective learning strategies. In this work, we reformulate FF using distance\nmetric learning and propose a distance-forward algorithm (DF) to improve FF\nperformance in supervised vision tasks while preserving its local computational\nproperties, making it competitive for efficient on-chip learning. To achieve\nthis, we reinterpret FF through the lens of centroid-based metric learning and\ndevelop a goodness-based N-pair margin loss to facilitate the learning of\ndiscriminative features. Furthermore, we integrate layer-collaboration local\nupdate strategies to reduce information loss caused by greedy local parameter\nupdates. Our method surpasses existing FF models and other advanced local\nlearning approaches, with accuracies of 99.7\\% on MNIST, 88.2\\% on CIFAR-10,\n59\\% on CIFAR-100, 95.9\\% on SVHN, and 82.5\\% on ImageNette, respectively.\nMoreover, it achieves comparable performance with less than 40\\% memory cost\ncompared to BP training, while exhibiting stronger robustness to multiple types\nof hardware-related noise, demonstrating its potential for online learning and\nenergy-efficient computation on neuromorphic chips.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14925v1",
    "published_date": "2024-08-27 10:01:43 UTC",
    "updated_date": "2024-08-27 10:01:43 UTC"
  },
  {
    "arxiv_id": "2408.15294v2",
    "title": "Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies",
    "authors": [
      "Christos Theodoropoulos",
      "Natasha Mulligan",
      "Joao Bettencourt-Silva"
    ],
    "abstract": "Developing novel predictive models with complex biomedical information is\nchallenging due to various idiosyncrasies related to heterogeneity,\nstandardization or sparseness of the data. We previously introduced a\nperson-centric ontology to organize information about individual patients, and\na representation learning framework to extract person-centric knowledge graphs\n(PKGs) and to train Graph Neural Networks (GNNs). In this paper, we propose a\nsystematic approach to examine the results of GNN models trained with both\nstructured and unstructured information from the MIMIC-III dataset. Through\nablation studies on different clinical, demographic, and social data, we show\nthe robustness of this approach in identifying predictive features in PKGs for\nthe task of readmission prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the 34th Medical Informatics Europe Conference",
    "pdf_url": "http://arxiv.org/pdf/2408.15294v2",
    "published_date": "2024-08-27 09:48:25 UTC",
    "updated_date": "2024-08-29 09:43:04 UTC"
  },
  {
    "arxiv_id": "2408.14895v2",
    "title": "VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities",
    "authors": [
      "Shusaku Egami",
      "Takahiro Ugai",
      "Swe Nwe Nwe Htun",
      "Ken Fukuda"
    ],
    "abstract": "Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data\n(e.g., images and videos) into symbols, have attracted attention as resources\nenabling knowledge processing and machine learning across modalities. However,\nthe construction of MMKGs for videos consisting of multiple events, such as\ndaily activities, is still in the early stages. In this paper, we construct an\nMMKG based on synchronized multi-view simulated videos of daily activities.\nBesides representing the content of daily life videos as event-centric\nknowledge, our MMKG also includes frame-by-frame fine-grained changes, such as\nbounding boxes within video frames. In addition, we provide support tools for\nquerying our MMKG. As an application example, we demonstrate that our MMKG\nfacilitates benchmarking vision-language models by providing the necessary\nvision-language datasets for a tailored task.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "68T30",
      "I.2.4; H.5.1"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 4 figures, accepted by CIKM2024 Resource Track",
    "pdf_url": "http://arxiv.org/pdf/2408.14895v2",
    "published_date": "2024-08-27 09:18:57 UTC",
    "updated_date": "2024-08-28 01:56:33 UTC"
  },
  {
    "arxiv_id": "2408.14886v1",
    "title": "The VoxCeleb Speaker Recognition Challenge: A Retrospective",
    "authors": [
      "Jaesung Huh",
      "Joon Son Chung",
      "Arsha Nagrani",
      "Andrew Brown",
      "Jee-weon Jung",
      "Daniel Garcia-Romero",
      "Andrew Zisserman"
    ],
    "abstract": "The VoxCeleb Speaker Recognition Challenges (VoxSRC) were a series of\nchallenges and workshops that ran annually from 2019 to 2023. The challenges\nprimarily evaluated the tasks of speaker recognition and diarisation under\nvarious settings including: closed and open training data; as well as\nsupervised, self-supervised, and semi-supervised training for domain\nadaptation. The challenges also provided publicly available training and\nevaluation datasets for each task and setting, with new test sets released each\nyear. In this paper, we provide a review of these challenges that covers: what\nthey explored; the methods developed by the challenge participants and how\nthese evolved; and also the current state of the field for speaker verification\nand diarisation. We chart the progress in performance over the five\ninstallments of the challenge on a common evaluation dataset and provide a\ndetailed analysis of how each year's special focus affected participants'\nperformance. This paper is aimed both at researchers who want an overview of\nthe speaker recognition and diarisation field, and also at challenge organisers\nwho want to benefit from the successes and avoid the mistakes of the VoxSRC\nchallenges. We end with a discussion of the current strengths of the field and\nopen challenges. Project page :\nhttps://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/workshop.html",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "TASLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14886v1",
    "published_date": "2024-08-27 08:57:31 UTC",
    "updated_date": "2024-08-27 08:57:31 UTC"
  },
  {
    "arxiv_id": "2408.14875v1",
    "title": "Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for Smart and Connected Infrastructures",
    "authors": [
      "Pooja Krishan",
      "Rohan Mohapatra",
      "Saptarshi Sengupta"
    ],
    "abstract": "The emergence of deep learning models has revolutionized various industries\nover the last decade, leading to a surge in connected devices and\ninfrastructures. However, these models can be tricked into making incorrect\npredictions with high confidence, leading to disastrous failures and security\nconcerns. To this end, we explore the impact of adversarial attacks on\nmultivariate time-series forecasting and investigate methods to counter them.\nSpecifically, we employ untargeted white-box attacks, namely the Fast Gradient\nSign Method (FGSM) and the Basic Iterative Method (BIM), to poison the inputs\nto the training process, effectively misleading the model. We also illustrate\nthe subtle modifications to the inputs after the attack, which makes detecting\nthe attack using the naked eye quite difficult. Having demonstrated the\nfeasibility of these attacks, we develop robust models through adversarial\ntraining and model hardening. We are among the first to showcase the\ntransferability of these attacks and defenses by extrapolating our work from\nthe benchmark electricity data to a larger, 10-year real-world data used for\npredicting the time-to-failure of hard disks. Our experimental results confirm\nthat the attacks and defenses achieve the desired security thresholds, leading\nto a 72.41% and 94.81% decrease in RMSE for the electricity and hard disk\ndatasets respectively after implementing the adversarial defenses.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.PF",
      "B.1.3; I.2.4"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 32 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.14875v1",
    "published_date": "2024-08-27 08:44:31 UTC",
    "updated_date": "2024-08-27 08:44:31 UTC"
  },
  {
    "arxiv_id": "2408.14871v2",
    "title": "Learning Robust Reward Machines from Noisy Labels",
    "authors": [
      "Roko Parac",
      "Lorenzo Nodari",
      "Leo Ardon",
      "Daniel Furelos-Blanco",
      "Federico Cerutti",
      "Alessandra Russo"
    ],
    "abstract": "This paper presents PROB-IRM, an approach that learns robust reward machines\n(RMs) for reinforcement learning (RL) agents from noisy execution traces. The\nkey aspect of RM-driven RL is the exploitation of a finite-state machine that\ndecomposes the agent's task into different subtasks. PROB-IRM uses a\nstate-of-the-art inductive logic programming framework robust to noisy examples\nto learn RMs from noisy traces using the Bayesian posterior degree of beliefs,\nthus ensuring robustness against inconsistencies. Pivotal for the results is\nthe interleaving between RM learning and policy learning: a new RM is learned\nwhenever the RL agent generates a trace that is believed not to be accepted by\nthe current RM. To speed up the training of the RL agent, PROB-IRM employs a\nprobabilistic formulation of reward shaping that uses the posterior Bayesian\nbeliefs derived from the traces. Our experimental analysis shows that PROB-IRM\ncan learn (potentially imperfect) RMs from noisy traces and exploit them to\ntrain an RL agent to solve its tasks successfully. Despite the complexity of\nlearning the RM from noisy traces, agents trained with PROB-IRM perform\ncomparably to agents provided with handcrafted RMs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the 21st International Conference on Principles of\n  Knowledge Representation and Reasoning (KR 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.14871v2",
    "published_date": "2024-08-27 08:41:42 UTC",
    "updated_date": "2025-03-21 14:07:55 UTC"
  },
  {
    "arxiv_id": "2408.15293v1",
    "title": "Learning Granularity Representation for Temporal Knowledge Graph Completion",
    "authors": [
      "Jinchuan Zhang",
      "Tianqi Wan",
      "Chong Mu",
      "Guangxi Lu",
      "Ling Tian"
    ],
    "abstract": "Temporal Knowledge Graphs (TKGs) incorporate temporal information to reflect\nthe dynamic structural knowledge and evolutionary patterns of real-world facts.\nNevertheless, TKGs are still limited in downstream applications due to the\nproblem of incompleteness. Consequently, TKG completion (also known as link\nprediction) has been widely studied, with recent research focusing on\nincorporating independent embeddings of time or combining them with entities\nand relations to form temporal representations. However, most existing methods\noverlook the impact of history from a multi-granularity aspect. The inherent\nsemantics of human-defined temporal granularities, such as ordinal dates,\nreveal general patterns to which facts typically adhere. To counter this\nlimitation, this paper proposes \\textbf{L}earning \\textbf{G}ranularity\n\\textbf{Re}presentation (termed $\\mathsf{LGRe}$) for TKG completion. It\ncomprises two main components: Granularity Representation Learning (GRL) and\nAdaptive Granularity Balancing (AGB). Specifically, GRL employs time-specific\nmulti-layer convolutional neural networks to capture interactions between\nentities and relations at different granularities. After that, AGB generates\nadaptive weights for these embeddings according to temporal semantics,\nresulting in expressive representations of predictions. Moreover, to reflect\nsimilar semantics of adjacent timestamps, a temporal loss function is\nintroduced. Extensive experimental results on four event benchmarks demonstrate\nthe effectiveness of $\\mathsf{LGRe}$ in learning time-related representations.\nTo ensure reproducibility, our code is available at\nhttps://github.com/KcAcoZhang/LGRe.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages. Accepted at ICONIP 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.15293v1",
    "published_date": "2024-08-27 08:19:34 UTC",
    "updated_date": "2024-08-27 08:19:34 UTC"
  },
  {
    "arxiv_id": "2409.05888v2",
    "title": "MA-CDMR: An Intelligent Cross-domain Multicast Routing Method based on Multiagent Deep Reinforcement Learning in Multi-domain SDWN",
    "authors": [
      "Miao Ye",
      "Hongwen Hu",
      "Xiaoli Wang",
      "Yuping Wang",
      "Yong Wang",
      "Wen Peng",
      "Jihao Zheng"
    ],
    "abstract": "The cross-domain multicast routing problem in a software-defined wireless\nnetwork with multiple controllers is a classic NP-hard optimization problem. As\nthe network size increases, designing and implementing cross-domain multicast\nrouting paths in the network requires not only designing efficient solution\nalgorithms to obtain the optimal cross-domain multicast tree but also ensuring\nthe timely and flexible acquisition and maintenance of global network state\ninformation. However, existing solutions have a limited ability to sense the\nnetwork traffic state, affecting the quality of service of multicast services.\nIn addition, these methods have difficulty adapting to the highly dynamically\nchanging network states and have slow convergence speeds. To this end, this\npaper aims to design and implement a multiagent deep reinforcement learning\nbased cross-domain multicast routing method for SDWN with multicontroller\ndomains. First, a multicontroller communication mechanism and a multicast group\nmanagement module are designed to transfer and synchronize network information\nbetween different control domains of the SDWN, thus effectively managing the\njoining and classification of members in the cross-domain multicast group.\nSecond, a theoretical analysis and proof show that the optimal cross-domain\nmulticast tree includes an interdomain multicast tree and an intradomain\nmulticast tree. An agent is established for each controller, and a cooperation\nmechanism between multiple agents is designed to effectively optimize\ncross-domain multicast routing and ensure consistency and validity in the\nrepresentation of network state information for cross-domain multicast routing\ndecisions. Third, a multiagent reinforcement learning-based method that\ncombines online and offline training is designed to reduce the dependence on\nthe real-time environment and increase the convergence speed of multiple\nagents.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.05888v2",
    "published_date": "2024-08-27 08:16:32 UTC",
    "updated_date": "2024-09-11 13:52:05 UTC"
  },
  {
    "arxiv_id": "2408.14855v1",
    "title": "Enhancing Analogical Reasoning in the Abstraction and Reasoning Corpus via Model-Based RL",
    "authors": [
      "Jihwan Lee",
      "Woochang Sim",
      "Sejin Kim",
      "Sundong Kim"
    ],
    "abstract": "This paper demonstrates that model-based reinforcement learning (model-based\nRL) is a suitable approach for the task of analogical reasoning. We hypothesize\nthat model-based RL can solve analogical reasoning tasks more efficiently\nthrough the creation of internal models. To test this, we compared DreamerV3, a\nmodel-based RL method, with Proximal Policy Optimization, a model-free RL\nmethod, on the Abstraction and Reasoning Corpus (ARC) tasks. Our results\nindicate that model-based RL not only outperforms model-free RL in learning and\ngeneralizing from single tasks but also shows significant advantages in\nreasoning across similar tasks.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to IJCAI 2024 IARML Workshop",
    "pdf_url": "http://arxiv.org/pdf/2408.14855v1",
    "published_date": "2024-08-27 08:15:20 UTC",
    "updated_date": "2024-08-27 08:15:20 UTC"
  },
  {
    "arxiv_id": "2408.14853v2",
    "title": "Atoxia: Red-teaming Large Language Models with Target Toxic Answers",
    "authors": [
      "Yuhao Du",
      "Zhuo Li",
      "Pengyu Cheng",
      "Xiang Wan",
      "Anningzhe Gao"
    ],
    "abstract": "Despite the substantial advancements in artificial intelligence, large\nlanguage models (LLMs) remain being challenged by generation safety. With\nadversarial jailbreaking prompts, one can effortlessly induce LLMs to output\nharmful content, causing unexpected negative social impacts. This vulnerability\nhighlights the necessity for robust LLM red-teaming strategies to identify and\nmitigate such risks before large-scale application. To detect specific types of\nrisks, we propose a novel red-teaming method that $\\textbf{A}$ttacks LLMs with\n$\\textbf{T}$arget $\\textbf{Toxi}$c $\\textbf{A}$nswers ($\\textbf{Atoxia}$).\nGiven a particular harmful answer, Atoxia generates a corresponding user query\nand a misleading answer opening to examine the internal defects of a given LLM.\nThe proposed attacker is trained within a reinforcement learning scheme with\nthe LLM outputting probability of the target answer as the reward. We verify\nthe effectiveness of our method on various red-teaming benchmarks, such as\nAdvBench and HH-Harmless. The empirical results demonstrate that Atoxia can\nsuccessfully detect safety risks in not only open-source models but also\nstate-of-the-art black-box models such as GPT-4o.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Findings of NAACL-2025",
    "pdf_url": "http://arxiv.org/pdf/2408.14853v2",
    "published_date": "2024-08-27 08:12:08 UTC",
    "updated_date": "2025-02-16 07:47:15 UTC"
  },
  {
    "arxiv_id": "2408.14849v2",
    "title": "Project SHADOW: Symbolic Higher-order Associative Deductive reasoning On Wikidata using LM probing",
    "authors": [
      "Hanna Abi Akl"
    ],
    "abstract": "We introduce SHADOW, a fine-tuned language model trained on an intermediate\ntask using associative deductive reasoning, and measure its performance on a\nknowledge base construction task using Wikidata triple completion. We evaluate\nSHADOW on the LM-KBC 2024 challenge and show that it outperforms the baseline\nsolution by 20% with a F1 score of 68.72%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 1 figure, accepted for the International Conference on\n  Natural Language Computing (NATL) 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14849v2",
    "published_date": "2024-08-27 08:01:13 UTC",
    "updated_date": "2024-09-23 12:22:31 UTC"
  },
  {
    "arxiv_id": "2408.14841v1",
    "title": "Diffusion based Semantic Outlier Generation via Nuisance Awareness for Out-of-Distribution Detection",
    "authors": [
      "Suhee Yoon",
      "Sanghyu Yoon",
      "Hankook Lee",
      "Ye Seul Sim",
      "Sungik Choi",
      "Kyungeun Lee",
      "Hye-Seung Cho",
      "Woohyung Lim"
    ],
    "abstract": "Out-of-distribution (OOD) detection, which determines whether a given sample\nis part of the in-distribution (ID), has recently shown promising results\nthrough training with synthetic OOD datasets. Nonetheless, existing methods\noften produce outliers that are considerably distant from the ID, showing\nlimited efficacy for capturing subtle distinctions between ID and OOD. To\naddress these issues, we propose a novel framework, Semantic Outlier generation\nvia Nuisance Awareness (SONA), which notably produces challenging outliers by\ndirectly leveraging pixel-space ID samples through diffusion models. Our\napproach incorporates SONA guidance, providing separate control over semantic\nand nuisance regions of ID samples. Thereby, the generated outliers achieve two\ncrucial properties: (i) they present explicit semantic-discrepant information,\nwhile (ii) maintaining various levels of nuisance resemblance with ID.\nFurthermore, the improved OOD detector training with SONA outliers facilitates\nlearning with a focus on semantic distinctions. Extensive experiments\ndemonstrate the effectiveness of our framework, achieving an impressive AUROC\nof 88% on near-OOD datasets, which surpasses the performance of baseline\nmethods by a significant margin of approximately 6%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14841v1",
    "published_date": "2024-08-27 07:52:44 UTC",
    "updated_date": "2024-08-27 07:52:44 UTC"
  },
  {
    "arxiv_id": "2408.14840v2",
    "title": "CL4KGE: A Curriculum Learning Method for Knowledge Graph Embedding",
    "authors": [
      "Yang Liu",
      "Chuan Zhou",
      "Peng Zhang",
      "Yanan Cao",
      "Yongchao Liu",
      "Zhao Li",
      "Hongyang Chen"
    ],
    "abstract": "Knowledge graph embedding (KGE) constitutes a foundational task, directed\ntowards learning representations for entities and relations within knowledge\ngraphs (KGs), with the objective of crafting representations comprehensive\nenough to approximate the logical and symbolic interconnections among entities.\nIn this paper, we define a metric Z-counts to measure the difficulty of\ntraining each triple ($<$head entity, relation, tail entity$>$) in KGs with\ntheoretical analysis. Based on this metric, we propose \\textbf{CL4KGE}, an\nefficient \\textbf{C}urriculum \\textbf{L}earning based training strategy for\n\\textbf{KGE}. This method includes a difficulty measurer and a training\nscheduler that aids in the training of KGE models. Our approach possesses the\nflexibility to act as a plugin within a wide range of KGE models, with the\nadded advantage of adaptability to the majority of KGs in existence. The\nproposed method has been evaluated on popular KGE models, and the results\ndemonstrate that it enhances the state-of-the-art methods. The use of Z-counts\nas a metric has enabled the identification of challenging triples in KGs, which\nhelps in devising effective training strategies.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.14840v2",
    "published_date": "2024-08-27 07:51:26 UTC",
    "updated_date": "2024-09-09 06:57:22 UTC"
  },
  {
    "arxiv_id": "2408.14837v2",
    "title": "Diffusion Models Are Real-Time Game Engines",
    "authors": [
      "Dani Valevski",
      "Yaniv Leviathan",
      "Moab Arar",
      "Shlomi Fruchter"
    ],
    "abstract": "We present GameNGen, the first game engine powered entirely by a neural model\nthat also enables real-time interaction with a complex environment over long\ntrajectories at high quality. When trained on the classic game DOOM, GameNGen\nextracts gameplay and uses it to generate a playable environment that can\ninteractively simulate new trajectories. GameNGen runs at 20 frames per second\non a single TPU and remains stable over extended multi-minute play sessions.\nNext frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG\ncompression. Human raters are only slightly better than random chance at\ndistinguishing short clips of the game from clips of the simulation, even after\n5 minutes of auto-regressive generation. GameNGen is trained in two phases: (1)\nan RL-agent learns to play the game and the training sessions are recorded, and\n(2) a diffusion model is trained to produce the next frame, conditioned on the\nsequence of past frames and actions. Conditioning augmentations help ensure\nstable auto-regressive generation over long trajectories, and decoder\nfine-tuning improves the fidelity of visual details and text.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025. Project page: https://gamengen.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2408.14837v2",
    "published_date": "2024-08-27 07:46:07 UTC",
    "updated_date": "2025-04-24 03:03:57 UTC"
  },
  {
    "arxiv_id": "2408.14834v1",
    "title": "Strategic Optimization and Challenges of Large Language Models in Object-Oriented Programming",
    "authors": [
      "Zinan Wang"
    ],
    "abstract": "In the area of code generation research, the emphasis has transitioned from\ncrafting individual functions to developing class-level method code that\nintegrates contextual information. This shift has brought several benchmarks\nsuch as ClassEval and CoderEval, which consider class-level contexts.\nNevertheless, the influence of specific contextual factors at the method level\nremains less explored.\n  This research focused on method-level code generation within the\nObject-Oriented Programming (OOP) framework. Based on CoderEval, we devised\nexperiments that varied the extent of contextual information in the prompts,\nranging from method-specific to project-wide details. We introduced the\ninnovative metric of \"Prompt-Token Cost-Effectiveness\" to evaluate the economic\nviability of incorporating additional contextual layers. Our findings indicate\nthat prompts enriched with method invocation details yield the highest\ncost-effectiveness. Additionally, our study revealed disparities among Large\nLanguage Models (LLMs) regarding error type distributions and the level of\nassistance they provide to developers. Notably, larger LLMs do not invariably\nperform better. We also observed that tasks with higher degrees of coupling\npresent more substantial challenges, suggesting that the choice of LLM should\nbe tailored to the task's coupling degree. For example, GPT-4 exhibited\nimproved performance in low-coupling scenarios, whereas GPT-3.5 seemed better\nsuited for tasks with high coupling. By meticulously curating prompt content\nand selecting the appropriate LLM, developers can optimize code quality while\nmaximizing cost-efficiency during the development process.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.14834v1",
    "published_date": "2024-08-27 07:44:16 UTC",
    "updated_date": "2024-08-27 07:44:16 UTC"
  },
  {
    "arxiv_id": "2408.14825v1",
    "title": "From Rule-Based Models to Deep Learning Transformers Architectures for Natural Language Processing and Sign Language Translation Systems: Survey, Taxonomy and Performance Evaluation",
    "authors": [
      "Nada Shahin",
      "Leila Ismail"
    ],
    "abstract": "With the growing Deaf and Hard of Hearing population worldwide and the\npersistent shortage of certified sign language interpreters, there is a\npressing need for an efficient, signs-driven, integrated end-to-end translation\nsystem, from sign to gloss to text and vice-versa. There has been a wealth of\nresearch on machine translations and related reviews. However, there are few\nworks on sign language machine translation considering the particularity of the\nlanguage being continuous and dynamic. This paper aims to address this void,\nproviding a retrospective analysis of the temporal evolution of sign language\nmachine translation algorithms and a taxonomy of the Transformers\narchitectures, the most used approach in language translation. We also present\nthe requirements of a real-time Quality-of-Service sign language ma-chine\ntranslation system underpinned by accurate deep learning algorithms. We propose\nfuture research directions for sign language translation systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "I.2, I.2.7, I.4, I.4.9"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14825v1",
    "published_date": "2024-08-27 07:11:45 UTC",
    "updated_date": "2024-08-27 07:11:45 UTC"
  },
  {
    "arxiv_id": "2408.14817v1",
    "title": "A Comprehensive Benchmark of Machine and Deep Learning Across Diverse Tabular Datasets",
    "authors": [
      "Assaf Shmuel",
      "Oren Glickman",
      "Teddy Lazebnik"
    ],
    "abstract": "The analysis of tabular datasets is highly prevalent both in scientific\nresearch and real-world applications of Machine Learning (ML). Unlike many\nother ML tasks, Deep Learning (DL) models often do not outperform traditional\nmethods in this area. Previous comparative benchmarks have shown that DL\nperformance is frequently equivalent or even inferior to models such as\nGradient Boosting Machines (GBMs). In this study, we introduce a comprehensive\nbenchmark aimed at better characterizing the types of datasets where DL models\nexcel. Although several important benchmarks for tabular datasets already\nexist, our contribution lies in the variety and depth of our comparison: we\nevaluate 111 datasets with 20 different models, including both regression and\nclassification tasks. These datasets vary in scale and include both those with\nand without categorical variables. Importantly, our benchmark contains a\nsufficient number of datasets where DL models perform best, allowing for a\nthorough analysis of the conditions under which DL models excel. Building on\nthe results of this benchmark, we train a model that predicts scenarios where\nDL models outperform alternative methods with 86.1% accuracy (AUC 0.78). We\npresent insights derived from this characterization and compare these findings\nto previous benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14817v1",
    "published_date": "2024-08-27 06:58:52 UTC",
    "updated_date": "2024-08-27 06:58:52 UTC"
  },
  {
    "arxiv_id": "2408.14811v1",
    "title": "Brain-inspired Artificial Intelligence: A Comprehensive Review",
    "authors": [
      "Jing Ren",
      "Feng Xia"
    ],
    "abstract": "Current artificial intelligence (AI) models often focus on enhancing\nperformance through meticulous parameter tuning and optimization techniques.\nHowever, the fundamental design principles behind these models receive\ncomparatively less attention, which can limit our understanding of their\npotential and constraints. This comprehensive review explores the diverse\ndesign inspirations that have shaped modern AI models, i.e., brain-inspired\nartificial intelligence (BIAI). We present a classification framework that\ncategorizes BIAI approaches into physical structure-inspired and human\nbehavior-inspired models. We also examine the real-world applications where\ndifferent BIAI models excel, highlighting their practical benefits and\ndeployment challenges. By delving into these areas, we provide new insights and\npropose future research directions to drive innovation and address current gaps\nin the field. This review offers researchers and practitioners a comprehensive\noverview of the BIAI landscape, helping them harness its potential and expedite\nadvancements in AI development.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "35 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.14811v1",
    "published_date": "2024-08-27 06:49:50 UTC",
    "updated_date": "2024-08-27 06:49:50 UTC"
  },
  {
    "arxiv_id": "2408.14806v2",
    "title": "Poly2Vec: Polymorphic Fourier-Based Encoding of Geospatial Objects for GeoAI Applications",
    "authors": [
      "Maria Despoina Siampou",
      "Jialiang Li",
      "John Krumm",
      "Cyrus Shahabi",
      "Hua Lu"
    ],
    "abstract": "Encoding geospatial objects is fundamental for geospatial artificial\nintelligence (GeoAI) applications, which leverage machine learning (ML) models\nto analyze spatial information. Common approaches transform each object into\nknown formats, like image and text, for compatibility with ML models. However,\nthis process often discards crucial spatial information, such as the object's\nposition relative to the entire space, reducing downstream task effectiveness.\nAlternative encoding methods that preserve some spatial properties are often\ndevised for specific data objects (e.g., point encoders), making them\nunsuitable for tasks that involve different data types (i.e., points,\npolylines, and polygons). To this end, we propose Poly2Vec, a polymorphic\nFourier-based encoding approach that unifies the representation of geospatial\nobjects, while preserving the essential spatial properties. Poly2Vec\nincorporates a learned fusion module that adaptively integrates the magnitude\nand phase of the Fourier transform for different tasks and geometries. We\nevaluate Poly2Vec on five diverse tasks, organized into two categories. The\nfirst empirically demonstrates that Poly2Vec consistently outperforms\nobject-specific baselines in preserving three key spatial relationships:\ntopology, direction, and distance. The second shows that integrating Poly2Vec\ninto a state-of-the-art GeoAI workflow improves the performance in two popular\ntasks: population prediction and land use inference.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14806v2",
    "published_date": "2024-08-27 06:28:35 UTC",
    "updated_date": "2025-05-11 20:07:55 UTC"
  },
  {
    "arxiv_id": "2408.14792v2",
    "title": "Measuring Human Contribution in AI-Assisted Content Generation",
    "authors": [
      "Yueqi Xie",
      "Tao Qi",
      "Jingwei Yi",
      "Xiyuan Yang",
      "Ryan Whalen",
      "Junming Huang",
      "Qian Ding",
      "Yu Xie",
      "Xing Xie",
      "Fangzhao Wu"
    ],
    "abstract": "With the growing prevalence of generative artificial intelligence (AI), an\nincreasing amount of content is no longer exclusively generated by humans but\nby generative AI models with human guidance. This shift presents notable\nchallenges for the delineation of originality due to the varying degrees of\nhuman contribution in AI-assisted works. This study raises the research\nquestion of measuring human contribution in AI-assisted content generation and\nintroduces a framework to address this question that is grounded in information\ntheory. By calculating mutual information between human input and AI-assisted\noutput relative to self-information of AI-assisted output, we quantify the\nproportional information contribution of humans in content generation. Our\nexperimental results demonstrate that the proposed measure effectively\ndiscriminates between varying degrees of human contribution across multiple\ncreative domains. We hope that this work lays a foundation for measuring human\ncontributions in AI-assisted content generation in the era of generative AI.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14792v2",
    "published_date": "2024-08-27 05:56:04 UTC",
    "updated_date": "2025-02-13 17:22:36 UTC"
  },
  {
    "arxiv_id": "2408.14791v3",
    "title": "Optimizing Structured Data Processing through Robotic Process Automation",
    "authors": [
      "Vivek Bhardwaj",
      "Ajit Noonia",
      "Sandeep Chaurasia",
      "Mukesh Kumar",
      "Abdulnaser Rashid",
      "Mohamed Tahar Ben Othman"
    ],
    "abstract": "Robotic Process Automation (RPA) has emerged as a game-changing technology in\ndata extraction, revolutionizing the way organizations process and analyze\nlarge volumes of documents such as invoices, purchase orders, and payment\nadvices. This study investigates the use of RPA for structured data extraction\nand evaluates its advantages over manual processes. By comparing\nhuman-performed tasks with those executed by RPA software bots, we assess\nefficiency and accuracy in data extraction from invoices, focusing on the\neffectiveness of the RPA system. Through four distinct scenarios involving\nvarying numbers of invoices, we measure efficiency in terms of time and effort\nrequired for task completion, as well as accuracy by comparing error rates\nbetween manual and RPA processes. Our findings highlight the significant\nefficiency gains achieved by RPA, with bots completing tasks in significantly\nless time compared to manual efforts across all cases. Moreover, the RPA system\nconsistently achieves perfect accuracy, mitigating the risk of errors and\nenhancing process reliability. These results underscore the transformative\npotential of RPA in optimizing operational efficiency, reducing human labor\ncosts, and improving overall business performance.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14791v3",
    "published_date": "2024-08-27 05:53:02 UTC",
    "updated_date": "2024-10-31 12:23:42 UTC"
  },
  {
    "arxiv_id": "2408.14780v2",
    "title": "GINN-KAN: Interpretability pipelining with applications in Physics Informed Neural Networks",
    "authors": [
      "Nisal Ranasinghe",
      "Yu Xia",
      "Sachith Seneviratne",
      "Saman Halgamuge"
    ],
    "abstract": "Neural networks are powerful function approximators, yet their ``black-box\"\nnature often renders them opaque and difficult to interpret. While many\npost-hoc explanation methods exist, they typically fail to capture the\nunderlying reasoning processes of the networks. A truly interpretable neural\nnetwork would be trained similarly to conventional models using techniques such\nas backpropagation, but additionally provide insights into the learned\ninput-output relationships. In this work, we introduce the concept of\ninterpretability pipelineing, to incorporate multiple interpretability\ntechniques to outperform each individual technique. To this end, we first\nevaluate several architectures that promise such interpretability, with a\nparticular focus on two recent models selected for their potential to\nincorporate interpretability into standard neural network architectures while\nstill leveraging backpropagation: the Growing Interpretable Neural Network\n(GINN) and Kolmogorov Arnold Networks (KAN). We analyze the limitations and\nstrengths of each and introduce a novel interpretable neural network GINN-KAN\nthat synthesizes the advantages of both models. When tested on the Feynman\nsymbolic regression benchmark datasets, GINN-KAN outperforms both GINN and KAN.\nTo highlight the capabilities and the generalizability of this approach, we\nposition GINN-KAN as an alternative to conventional black-box networks in\nPhysics-Informed Neural Networks (PINNs). We expect this to have far-reaching\nimplications in the application of deep learning pipelines in the natural\nsciences. Our experiments with this interpretable PINN on 15 different partial\ndifferential equations demonstrate that GINN-KAN augmented PINNs outperform\nPINNs with black-box networks in solving differential equations and surpass the\ncapabilities of both GINN and KAN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14780v2",
    "published_date": "2024-08-27 04:57:53 UTC",
    "updated_date": "2024-08-28 15:48:31 UTC"
  },
  {
    "arxiv_id": "2408.14776v2",
    "title": "MROVSeg: Breaking the Resolution Curse of Vision-Language Models in Open-Vocabulary Image Segmentation",
    "authors": [
      "Yuanbing Zhu",
      "Bingke Zhu",
      "Yingying Chen",
      "Yunfang Niu",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "abstract": "Pretrained vision-language models (VLMs), \\eg CLIP, are increasingly used to\nbridge the gap between open- and close-vocabulary recognition in\nopen-vocabulary image segmentation. As VLMs are generally pretrained with\nlow-resolution images (e.g. $224\\times224$), most previous methods operate only\non downscaled images. We question this design as low resolution features often\nfail to preserve fine details. A typical solution is to employ additional image\nbackbones for high-resolution inputs, but it also introduce significant\ncomputation overhead. Therefore, we propose MROVSeg, a multi-resolution\ntraining framework for open-vocabulary image segmentation with a single\npretrained CLIP backbone, that uses sliding windows to slice the\nhigh-resolution input into uniform patches, each matching the input size of the\nwell-trained image encoder. Its key components include a Multi-Res Adapter,\nwhich restores the spatial geometry and grasps local-global correspondences\nacross patches by interacting with multi-resolution features. To achieve\naccurate segmentation, we introduce Multi-grained Masked Attention scheme to\naggregate multi-grained semantics from multi-resolution CLIP features to object\nqueries. Through comprehensive experiments, we demonstrate the superiority of\nMROVSeg on well-established open-vocabulary image segmentation benchmarks,\nestablishing new standards for open-vocabulary image segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical report",
    "pdf_url": "http://arxiv.org/pdf/2408.14776v2",
    "published_date": "2024-08-27 04:45:53 UTC",
    "updated_date": "2024-11-27 15:26:41 UTC"
  },
  {
    "arxiv_id": "2408.14772v2",
    "title": "A global AI community requires language-diverse publishing",
    "authors": [
      "Haley Lepp",
      "Parth Sarin"
    ],
    "abstract": "In this provocation, we discuss the English dominance of the AI research\ncommunity, arguing that the requirement for English language publishing upholds\nand reinforces broader regimes of extraction in AI. While large language models\nand machine translation have been celebrated as a way to break down barriers,\nwe regard their use as a symptom of linguistic exclusion of scientists and\npotential readers. We propose alternative futures for a healthier publishing\nculture, organized around three themes: administering conferences in the\nlanguages of the country in which they are held, instructing peer reviewers not\nto adjudicate the language appropriateness of papers, and offering\nopportunities to publish and present in multiple languages. We welcome new\ntranslations of this piece. Please contact the authors if you would like to\ncontribute one.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "K.7.0; K.4.2; I.2.m"
    ],
    "primary_category": "cs.CL",
    "comment": "Translations by Tianyu M. Fang (Mandarin Chinese), Michael Hardy\n  (Guarani), Vandana Sarin and Vivek Sarin (Hindi), Roshna Omer Abdulrahman\n  (Soran\\^i Kurdish), Gabriel Poesia (Portuguese), and Mat\\'ias Grinberg\n  (Spanish). In the proceedings of the Global AI Cultures Workshop at the\n  Twelfth International Conference on Learning Representations (ICLR) 2024,\n  Vienna, Austria, May 7-11, 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14772v2",
    "published_date": "2024-08-27 04:20:10 UTC",
    "updated_date": "2024-08-29 19:50:33 UTC"
  },
  {
    "arxiv_id": "2409.00099v2",
    "title": "Query-by-Example Keyword Spotting Using Spectral-Temporal Graph Attentive Pooling and Multi-Task Learning",
    "authors": [
      "Zhenyu Wang",
      "Shuyu Kong",
      "Li Wan",
      "Biqiao Zhang",
      "Yiteng Huang",
      "Mumin Jin",
      "Ming Sun",
      "Xin Lei",
      "Zhaojun Yang"
    ],
    "abstract": "Existing keyword spotting (KWS) systems primarily rely on predefined keyword\nphrases. However, the ability to recognize customized keywords is crucial for\ntailoring interactions with intelligent devices. In this paper, we present a\nnovel Query-by-Example (QbyE) KWS system that employs spectral-temporal graph\nattentive pooling and multi-task learning. This framework aims to effectively\nlearn speaker-invariant and linguistic-informative embeddings for QbyE KWS\ntasks. Within this framework, we investigate three distinct network\narchitectures for encoder modeling: LiCoNet, Conformer and ECAPA_TDNN. The\nexperimental results on a substantial internal dataset of $629$ speakers have\ndemonstrated the effectiveness of the proposed QbyE framework in maximizing the\npotential of simpler models such as LiCoNet. Particularly, LiCoNet, which is\n13x more efficient, achieves comparable performance to the computationally\nintensive Conformer model (1.98% vs. 1.63\\% FRR at 0.3 FAs/Hr).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00099v2",
    "published_date": "2024-08-27 03:44:57 UTC",
    "updated_date": "2024-11-23 20:55:13 UTC"
  },
  {
    "arxiv_id": "2408.14754v1",
    "title": "Sequential-Scanning Dual-Energy CT Imaging Using High Temporal Resolution Image Reconstruction and Error-Compensated Material Basis Image Generation",
    "authors": [
      "Qiaoxin Li",
      "Ruifeng Chen",
      "Peng Wang",
      "Guotao Quan",
      "Yanfeng Du",
      "Dong Liang",
      "Yinsheng Li"
    ],
    "abstract": "Dual-energy computed tomography (DECT) has been widely used to obtain\nquantitative elemental composition of imaged subjects for personalized and\nprecise medical diagnosis. Compared with DECT leveraging advanced X-ray source\nand/or detector technologies, the use of the sequential-scanning data\nacquisition scheme to implement DECT may make a broader impact on clinical\npractice because this scheme requires no specialized hardware designs and can\nbe directly implemented into conventional CT systems. However, since the\nconcentration of iodinated contrast agent in the imaged subject varies over\ntime, sequentially scanned data sets acquired at two tube potentials are\ntemporally inconsistent. As existing material basis image reconstruction\napproaches assume that the data sets acquired at two tube potentials are\ntemporally consistent, the violation of this assumption results in inaccurate\nquantification of material concentration. In this work, we developed\nsequential-scanning DECT imaging using high temporal resolution image\nreconstruction and error-compensated material basis image generation,\nACCELERATION in short, to address the technical challenge induced by temporal\ninconsistency of sequentially scanned data sets and improve quantification\naccuracy of material concentration in sequential-scanning DECT. ACCELERATION\nhas been validated and evaluated using numerical simulation data sets generated\nfrom clinical human subject exams and experimental human subject studies.\nResults demonstrated the improvement of quantification accuracy and image\nquality using ACCELERATION.",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "cs.CV",
      "physics.ins-det"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14754v1",
    "published_date": "2024-08-27 03:09:39 UTC",
    "updated_date": "2024-08-27 03:09:39 UTC"
  },
  {
    "arxiv_id": "2408.14753v1",
    "title": "CoopASD: Cooperative Machine Anomalous Sound Detection with Privacy Concerns",
    "authors": [
      "Anbai Jiang",
      "Yuchen Shi",
      "Pingyi Fan",
      "Wei-Qiang Zhang",
      "Jia Liu"
    ],
    "abstract": "Machine anomalous sound detection (ASD) has emerged as one of the most\npromising applications in the Industrial Internet of Things (IIoT) due to its\nunprecedented efficacy in mitigating risks of malfunctions and promoting\nproduction efficiency. Previous works mainly investigated the machine ASD task\nunder centralized settings. However, developing the ASD system under\ndecentralized settings is crucial in practice, since the machine data are\ndispersed in various factories and the data should not be explicitly shared due\nto privacy concerns. To enable these factories to cooperatively develop a\nscalable ASD model while preserving their privacy, we propose a novel framework\nnamed CoopASD, where each factory trains an ASD model on its local dataset, and\na central server aggregates these local models periodically. We employ a\npre-trained model as the backbone of the ASD model to improve its robustness\nand develop specialized techniques to stabilize the model under a completely\nnon-iid and domain shift setting. Compared with previous state-of-the-art\n(SOTA) models trained in centralized settings, CoopASD showcases competitive\nresults with negligible degradation of 0.08%. We also conduct extensive\nablation studies to demonstrate the effectiveness of CoopASD.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.DC",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by GLOBECOM 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14753v1",
    "published_date": "2024-08-27 03:07:03 UTC",
    "updated_date": "2024-08-27 03:07:03 UTC"
  },
  {
    "arxiv_id": "2408.14747v1",
    "title": "Benchmarking Reinforcement Learning Methods for Dexterous Robotic Manipulation with a Three-Fingered Gripper",
    "authors": [
      "Elizabeth Cutler",
      "Yuning Xing",
      "Tony Cui",
      "Brendan Zhou",
      "Koen van Rijnsoever",
      "Ben Hart",
      "David Valencia",
      "Lee Violet C. Ong",
      "Trevor Gee",
      "Minas Liarokapis",
      "Henry Williams"
    ],
    "abstract": "Reinforcement Learning (RL) training is predominantly conducted in\ncost-effective and controlled simulation environments. However, the transfer of\nthese trained models to real-world tasks often presents unavoidable challenges.\nThis research explores the direct training of RL algorithms in controlled yet\nrealistic real-world settings for the execution of dexterous manipulation. The\nbenchmarking results of three RL algorithms trained on intricate in-hand\nmanipulation tasks within practical real-world contexts are presented. Our\nstudy not only demonstrates the practicality of RL training in authentic\nreal-world scenarios, facilitating direct real-world applications, but also\nprovides insights into the associated challenges and considerations.\nAdditionally, our experiences with the employed experimental methods are\nshared, with the aim of empowering and engaging fellow researchers and\npractitioners in this dynamic field of robotics.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14747v1",
    "published_date": "2024-08-27 02:52:15 UTC",
    "updated_date": "2024-08-27 02:52:15 UTC"
  },
  {
    "arxiv_id": "2408.14744v3",
    "title": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with Rich Linguistic Semantics from Openly Available Data and Large Language Models",
    "authors": [
      "Junyao Ge",
      "Xu Zhang",
      "Yang Zheng",
      "Kaitai Guo",
      "Jimin Liang"
    ],
    "abstract": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.8; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to ISPRS",
    "pdf_url": "http://arxiv.org/pdf/2408.14744v3",
    "published_date": "2024-08-27 02:45:26 UTC",
    "updated_date": "2025-04-16 13:02:25 UTC"
  },
  {
    "arxiv_id": "2409.06726v1",
    "title": "Feedback-based Modal Mutual Search for Attacking Vision-Language Pre-training Models",
    "authors": [
      "Renhua Ding",
      "Xinze Zhang",
      "Xiao Yang",
      "Kun He"
    ],
    "abstract": "Although vision-language pre-training (VLP) models have achieved remarkable\nprogress on cross-modal tasks, they remain vulnerable to adversarial attacks.\nUsing data augmentation and cross-modal interactions to generate transferable\nadversarial examples on surrogate models, transfer-based black-box attacks have\nbecome the mainstream methods in attacking VLP models, as they are more\npractical in real-world scenarios. However, their transferability may be\nlimited due to the differences on feature representation across different\nmodels. To this end, we propose a new attack paradigm called Feedback-based\nModal Mutual Search (FMMS). FMMS introduces a novel modal mutual loss (MML),\naiming to push away the matched image-text pairs while randomly drawing\nmismatched pairs closer in feature space, guiding the update directions of the\nadversarial examples. Additionally, FMMS leverages the target model feedback to\niteratively refine adversarial examples, driving them into the adversarial\nregion. To our knowledge, this is the first work to exploit target model\nfeedback to explore multi-modality adversarial boundaries. Extensive empirical\nevaluations on Flickr30K and MSCOCO datasets for image-text matching tasks show\nthat FMMS significantly outperforms the state-of-the-art baselines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.06726v1",
    "published_date": "2024-08-27 02:31:39 UTC",
    "updated_date": "2024-08-27 02:31:39 UTC"
  },
  {
    "arxiv_id": "2409.00097v2",
    "title": "Large Language Models for Disease Diagnosis: A Scoping Review",
    "authors": [
      "Shuang Zhou",
      "Zidu Xu",
      "Mian Zhang",
      "Chunpu Xu",
      "Yawen Guo",
      "Zaifu Zhan",
      "Sirui Ding",
      "Jiashuo Wang",
      "Kaishuai Xu",
      "Yi Fang",
      "Liqiao Xia",
      "Jeremy Yeung",
      "Daochen Zha",
      "Genevieve B. Melton",
      "Mingquan Lin",
      "Rui Zhang"
    ],
    "abstract": "Automatic disease diagnosis has become increasingly valuable in clinical\npractice. The advent of large language models (LLMs) has catalyzed a paradigm\nshift in artificial intelligence, with growing evidence supporting the efficacy\nof LLMs in diagnostic tasks. Despite the increasing attention in this field, a\nholistic view is still lacking. Many critical aspects remain unclear, such as\nthe diseases and clinical data to which LLMs have been applied, the LLM\ntechniques employed, and the evaluation methods used. In this article, we\nperform a comprehensive review of LLM-based methods for disease diagnosis. Our\nreview examines the existing literature across various dimensions, including\ndisease types and associated clinical specialties, clinical data, LLM\ntechniques, and evaluation methods. Additionally, we offer recommendations for\napplying and evaluating LLMs for diagnostic tasks. Furthermore, we assess the\nlimitations of current research and discuss future directions. To our\nknowledge, this is the first comprehensive review for LLM-based disease\ndiagnosis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "69 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.00097v2",
    "published_date": "2024-08-27 02:06:45 UTC",
    "updated_date": "2024-09-19 12:19:48 UTC"
  },
  {
    "arxiv_id": "2408.14728v1",
    "title": "TART: Boosting Clean Accuracy Through Tangent Direction Guided Adversarial Training",
    "authors": [
      "Bongsoo Yi",
      "Rongjie Lai",
      "Yao Li"
    ],
    "abstract": "Adversarial training has been shown to be successful in enhancing the\nrobustness of deep neural networks against adversarial attacks. However, this\nrobustness is accompanied by a significant decline in accuracy on clean data.\nIn this paper, we propose a novel method, called Tangent Direction Guided\nAdversarial Training (TART), that leverages the tangent space of the data\nmanifold to ameliorate the existing adversarial defense algorithms. We argue\nthat training with adversarial examples having large normal components\nsignificantly alters the decision boundary and hurts accuracy. TART mitigates\nthis issue by estimating the tangent direction of adversarial examples and\nallocating an adaptive perturbation limit according to the norm of their\ntangential component. To the best of our knowledge, our paper is the first work\nto consider the concept of tangent space and direction in the context of\nadversarial defense. We validate the effectiveness of TART through extensive\nexperiments on both simulated and benchmark datasets. The results demonstrate\nthat TART consistently boosts clean accuracy while retaining a high level of\nrobustness against adversarial attacks. Our findings suggest that incorporating\nthe geometric properties of data can lead to more effective and efficient\nadversarial training methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14728v1",
    "published_date": "2024-08-27 01:41:21 UTC",
    "updated_date": "2024-08-27 01:41:21 UTC"
  },
  {
    "arxiv_id": "2409.00096v1",
    "title": "Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data",
    "authors": [
      "Juncheng Xie",
      "Shensian Syu",
      "Hung-yi Lee"
    ],
    "abstract": "Instruction fine-tuning is crucial for today's large language models (LLMs)\nto learn to follow instructions and align with human preferences.\nConventionally, supervised data, including the instruction and the correct\nresponse, is required for instruction fine-tuning. To obtain such data, some\nresearchers prompted well-trained models like GPT-4 to generate instructions\nand correct responses. In this paper, we propose a novel approach that uses the\nfirst half of a random text from OpenWebText as the instruction and\nGPT-3.5-turbo or GPT-4-turbo to complete the text as the response. Despite the\ndata being \"non-instructional\", we found that pre-trained LLMs fine-tuned on\nthis data can gain instruction-following capabilities. This observation is\nverified by fine-tuning several well-known pre-trained LLMs (e.g., LLaMA-2-7B,\nLLaMA-3-8B, LLaMA-3-70B, Mistral-7B-v0.1). The \"non-instructional data\" also\nimproved some models that underwent supervised fine-tuning and human preference\nalignment. Our LLaMA-3-70B-Instruct fine-tuned through \"non-instructional data\"\nis comparable with LLaMA-3.1-70B-Instruct on the Arena Hard leaderboard. We\nanalyzed the \"non-instructional data\" and ensured it is devoid of content\nrelated to instruction fine-tuning. Our findings will inspire further\ninvestigation into how to develop instruction-following capabilities without\nexplicit instruction-related data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 2 figures, 15 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.00096v1",
    "published_date": "2024-08-27 01:21:53 UTC",
    "updated_date": "2024-08-27 01:21:53 UTC"
  },
  {
    "arxiv_id": "2408.16021v2",
    "title": "XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous Graph Neural Network and Large Language Model",
    "authors": [
      "Yasir Ali Farrukh",
      "Syed Wali",
      "Irfan Khan",
      "Nathaniel D. Bastian"
    ],
    "abstract": "In the rapidly evolving field of cybersecurity, the integration of flow-level\nand packet-level information for real-time intrusion detection remains a\nlargely untapped area of research. This paper introduces \"XG-NID,\" a novel\nframework that, to the best of our knowledge, is the first to fuse flow-level\nand packet-level data within a heterogeneous graph structure, offering a\ncomprehensive analysis of network traffic. Leveraging a heterogeneous graph\nneural network (GNN) with graph-level classification, XG-NID uniquely enables\nreal-time inference while effectively capturing the intricate relationships\nbetween flow and packet payload data. Unlike traditional GNN-based\nmethodologies that predominantly analyze historical data, XG-NID is designed to\naccommodate the heterogeneous nature of network traffic, providing a robust and\nreal-time defense mechanism. Our framework extends beyond mere classification;\nit integrates Large Language Models (LLMs) to generate detailed, human-readable\nexplanations and suggest potential remedial actions, ensuring that the insights\nproduced are both actionable and comprehensible. Additionally, we introduce a\nnew set of flow features based on temporal information, further enhancing the\ncontextual and explainable inferences provided by our model. To facilitate\npractical application and accessibility, we developed \"GNN4ID,\" an open-source\ntool that enables the extraction and transformation of raw network traffic into\nthe proposed heterogeneous graph structure, seamlessly integrating flow and\npacket-level data. Our comprehensive quantitative comparative analysis\ndemonstrates that XG-NID achieves an F1 score of 97\\% in multi-class\nclassification, outperforming existing baseline and state-of-the-art methods.\nThis sets a new standard in Network Intrusion Detection Systems by combining\ninnovative data fusion with enhanced interpretability and real-time\ncapabilities.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "19 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.16021v2",
    "published_date": "2024-08-27 01:14:34 UTC",
    "updated_date": "2025-05-07 21:59:46 UTC"
  },
  {
    "arxiv_id": "2408.14721v2",
    "title": "PAT: Pruning-Aware Tuning for Large Language Models",
    "authors": [
      "Yijiang Liu",
      "Huanrui Yang",
      "Youxin Chen",
      "Rongyu Zhang",
      "Miao Wang",
      "Yuan Du",
      "Li Du"
    ],
    "abstract": "Large language models (LLMs) excel in language tasks, especially with\nsupervised fine-tuning after pre-training. However, their substantial memory\nand computational requirements hinder practical applications. Structural\npruning, which reduces less significant weight dimensions, is one solution.\nYet, traditional post-hoc pruning often leads to significant performance loss,\nwith limited recovery from further fine-tuning due to reduced capacity. Since\nthe model fine-tuning refines the general and chaotic knowledge in pre-trained\nmodels, we aim to incorporate structural pruning with the fine-tuning, and\npropose the Pruning-Aware Tuning (PAT) paradigm to eliminate model redundancy\nwhile preserving the model performance to the maximum extend. Specifically, we\ninsert the innovative Hybrid Sparsification Modules (HSMs) between the\nAttention and FFN components to accordingly sparsify the upstream and\ndownstream linear modules. The HSM comprises a lightweight operator and a\nglobally shared trainable mask. The lightweight operator maintains a training\noverhead comparable to that of LoRA, while the trainable mask unifies the\nchannels to be sparsified, ensuring structural pruning. Additionally, we\npropose the Identity Loss which decouples the transformation and scaling\nproperties of the HSMs to enhance training robustness. Extensive experiments\ndemonstrate that PAT excels in both performance and efficiency. For example,\nour Llama2-7b model with a 25\\% pruning ratio achieves 1.33$\\times$ speedup\nwhile outperforming the LoRA-finetuned model by up to 1.26\\% in accuracy with a\nsimilar training cost. Code:\nhttps://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.14721v2",
    "published_date": "2024-08-27 01:04:14 UTC",
    "updated_date": "2025-01-25 05:21:44 UTC"
  },
  {
    "arxiv_id": "2408.14718v1",
    "title": "Residual-based Adaptive Huber Loss (RAHL) -- Design of an improved Huber loss for CQI prediction in 5G networks",
    "authors": [
      "Mina Kaviani",
      "Jurandy Almeida",
      "Fabio L. Verdi"
    ],
    "abstract": "The Channel Quality Indicator (CQI) plays a pivotal role in 5G networks,\noptimizing infrastructure dynamically to ensure high Quality of Service (QoS).\nRecent research has focused on improving CQI estimation in 5G networks using\nmachine learning. In this field, the selection of the proper loss function is\ncritical for training an accurate model. Two commonly used loss functions are\nMean Squared Error (MSE) and Mean Absolute Error (MAE). Roughly speaking, MSE\nput more weight on outliers, MAE on the majority. Here, we argue that the Huber\nloss function is more suitable for CQI prediction, since it combines the\nbenefits of both MSE and MAE. To achieve this, the Huber loss transitions\nsmoothly between MSE and MAE, controlled by a user-defined hyperparameter\ncalled delta. However, finding the right balance between sensitivity to small\nerrors (MAE) and robustness to outliers (MSE) by manually choosing the optimal\ndelta is challenging. To address this issue, we propose a novel loss function,\nnamed Residual-based Adaptive Huber Loss (RAHL). In RAHL, a learnable residual\nis added to the delta, enabling the model to adapt based on the distribution of\nerrors in the data. Our approach effectively balances model robustness against\noutliers while preserving inlier data precision. The widely recognized Long\nShort-Term Memory (LSTM) model is employed in conjunction with RAHL, showcasing\nsignificantly improved results compared to the aforementioned loss functions.\nThe obtained results affirm the superiority of RAHL, offering a promising\navenue for enhanced CQI prediction in 5G networks.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "https://sol.sbc.org.br/index.php/sbrc/article/view/29822/29625",
    "pdf_url": "http://arxiv.org/pdf/2408.14718v1",
    "published_date": "2024-08-27 00:58:32 UTC",
    "updated_date": "2024-08-27 00:58:32 UTC"
  },
  {
    "arxiv_id": "2408.14717v1",
    "title": "Text2SQL is Not Enough: Unifying AI and Databases with TAG",
    "authors": [
      "Asim Biswal",
      "Liana Patel",
      "Siddarth Jha",
      "Amog Kamsetty",
      "Shu Liu",
      "Joseph E. Gonzalez",
      "Carlos Guestrin",
      "Matei Zaharia"
    ],
    "abstract": "AI systems that serve natural language questions over databases promise to\nunlock tremendous value. Such systems would allow users to leverage the\npowerful reasoning and knowledge capabilities of language models (LMs)\nalongside the scalable computational power of data management systems. These\ncombined capabilities would empower users to ask arbitrary natural language\nquestions over custom data sources. However, existing methods and benchmarks\ninsufficiently explore this setting. Text2SQL methods focus solely on natural\nlanguage questions that can be expressed in relational algebra, representing a\nsmall subset of the questions real users wish to ask. Likewise,\nRetrieval-Augmented Generation (RAG) considers the limited subset of queries\nthat can be answered with point lookups to one or a few data records within the\ndatabase. We propose Table-Augmented Generation (TAG), a unified and\ngeneral-purpose paradigm for answering natural language questions over\ndatabases. The TAG model represents a wide range of interactions between the LM\nand database that have been previously unexplored and creates exciting research\nopportunities for leveraging the world knowledge and reasoning capabilities of\nLMs over data. We systematically develop benchmarks to study the TAG problem\nand find that standard methods answer no more than 20% of queries correctly,\nconfirming the need for further research in this area. We release code for the\nbenchmark at https://github.com/TAG-Research/TAG-Bench.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14717v1",
    "published_date": "2024-08-27 00:50:14 UTC",
    "updated_date": "2024-08-27 00:50:14 UTC"
  },
  {
    "arxiv_id": "2408.14713v1",
    "title": "StyleSpeech: Parameter-efficient Fine Tuning for Pre-trained Controllable Text-to-Speech",
    "authors": [
      "Haowei Lou",
      "Helen Paik",
      "Wen Hu",
      "Lina Yao"
    ],
    "abstract": "This paper introduces StyleSpeech, a novel Text-to-Speech~(TTS) system that\nenhances the naturalness and accuracy of synthesized speech. Building upon\nexisting TTS technologies, StyleSpeech incorporates a unique Style Decorator\nstructure that enables deep learning models to simultaneously learn style and\nphoneme features, improving adaptability and efficiency through the principles\nof Lower Rank Adaptation~(LoRA). LoRA allows efficient adaptation of style\nfeatures in pre-trained models. Additionally, we introduce a novel automatic\nevaluation metric, the LLM-Guided Mean Opinion Score (LLM-MOS), which employs\nlarge language models to offer an objective and robust protocol for\nautomatically assessing TTS system performance. Extensive testing on benchmark\ndatasets shows that our approach markedly outperforms existing state-of-the-art\nbaseline methods in producing natural, accurate, and high-quality speech. These\nadvancements not only pushes the boundaries of current TTS system capabilities,\nbut also facilitate the application of TTS system in more dynamic and\nspecialized, such as interactive virtual assistants, adaptive audiobooks, and\ncustomized voice for gaming. Speech samples can be found in\nhttps://style-speech.vercel.app",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.14713v1",
    "published_date": "2024-08-27 00:37:07 UTC",
    "updated_date": "2024-08-27 00:37:07 UTC"
  }
]
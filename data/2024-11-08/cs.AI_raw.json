[
  {
    "arxiv_id": "2411.06009v1",
    "title": "A Comprehensive Guide to Enhancing Antibiotic Discovery Using Machine Learning Derived Bio-computation",
    "authors": [
      "Khartik Uppalapati",
      "Eeshan Dandamudi",
      "S. Nick Ice",
      "Gaurav Chandra",
      "Kirsten Bischof",
      "Christian L. Lorson",
      "Kamal Singh"
    ],
    "abstract": "Traditional drug discovery is a long, expensive, and complex process.\nAdvances in Artificial Intelligence (AI) and Machine Learning (ML) are\nbeginning to change this narrative. Here, we provide a comprehensive overview\nof different AI and ML tools that can be used to streamline and accelerate the\ndrug discovery process. By using data sets to train ML algorithms, it is\npossible to discover drugs or drug-like compounds relatively quickly, and\nefficiently. Additionally, we address limitations in AI-based drug discovery\nand development, including the scarcity of high-quality data to train AI models\nand ethical considerations. The growing impact of AI on the pharmaceutical\nindustry is also highlighted. Finally, we discuss how AI and ML can expedite\nthe discovery of new antibiotics to combat the problem of worldwide\nantimicrobial resistance (AMR).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "65 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.06009v1",
    "published_date": "2024-11-08 23:04:42 UTC",
    "updated_date": "2024-11-08 23:04:42 UTC"
  },
  {
    "arxiv_id": "2411.06008v2",
    "title": "The Dark Patterns of Personalized Persuasion in Large Language Models: Exposing Persuasive Linguistic Features for Big Five Personality Traits in LLMs Responses",
    "authors": [
      "Wiktoria Mieleszczenko-Kowszewicz",
      "Dawid Płudowski",
      "Filip Kołodziejczyk",
      "Jakub Świstak",
      "Julian Sienkiewicz",
      "Przemysław Biecek"
    ],
    "abstract": "This study explores how the Large Language Models (LLMs) adjust linguistic\nfeatures to create personalized persuasive outputs. While research showed that\nLLMs personalize outputs, a gap remains in understanding the linguistic\nfeatures of their persuasive capabilities. We identified 13 linguistic features\ncrucial for influencing personalities across different levels of the Big Five\nmodel of personality. We analyzed how prompts with personality trait\ninformation influenced the output of 19 LLMs across five model families. The\nfindings show that models use more anxiety-related words for neuroticism,\nincrease achievement-related words for conscientiousness, and employ fewer\ncognitive processes words for openness to experience. Some model families excel\nat adapting language for openness to experience, others for conscientiousness,\nwhile only one model adapts language for neuroticism. Our findings show how\nLLMs tailor responses based on personality cues in prompts, indicating their\npotential to create persuasive content affecting the mind and well-being of the\nrecipients.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "31 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.06008v2",
    "published_date": "2024-11-08 23:02:59 UTC",
    "updated_date": "2024-11-12 14:30:28 UTC"
  },
  {
    "arxiv_id": "2411.15159v1",
    "title": "Adaptive Sensor Placement Inspired by Bee Foraging: Towards Efficient Environment Monitoring",
    "authors": [
      "Sai Krishna Reddy Sathi"
    ],
    "abstract": "This paper aims to make a mark in the future of sustainable robotics, where\nefficient algorithms are required to carry out tasks like environmental\nmonitoring and precision agriculture efficiently. We proposed a hybrid\nalgorithm that combines Artificial Bee Colony (ABC) with Levy flight to\noptimize adaptive sensor placement alongside an important notion of hotspots\nfrom domain knowledge experts. By enhancing exploration and exploitation, our\napproach significantly improves the identification of critical hotspots. This\nalgorithm also finds its usecases for broader search and rescue operations\napplications, demonstrating its potential in optimization problems across\nvarious domains.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.NE",
      "cs.RO"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15159v1",
    "published_date": "2024-11-08 22:24:06 UTC",
    "updated_date": "2024-11-08 22:24:06 UTC"
  },
  {
    "arxiv_id": "2411.05990v2",
    "title": "Game-theoretic LLM: Agent Workflow for Negotiation Games",
    "authors": [
      "Wenyue Hua",
      "Ollie Liu",
      "Lingyao Li",
      "Alfonso Amayuelas",
      "Julie Chen",
      "Lucas Jiang",
      "Mingyu Jin",
      "Lizhou Fan",
      "Fei Sun",
      "William Wang",
      "Xintong Wang",
      "Yongfeng Zhang"
    ],
    "abstract": "This paper investigates the rationality of large language models (LLMs) in\nstrategic decision-making contexts, specifically within the framework of game\ntheory. We evaluate several state-of-the-art LLMs across a spectrum of\ncomplete-information and incomplete-information games. Our findings reveal that\nLLMs frequently deviate from rational strategies, particularly as the\ncomplexity of the game increases with larger payoff matrices or deeper\nsequential trees.\n  To address these limitations, we design multiple game-theoretic workflows\nthat guide the reasoning and decision-making processes of LLMs. These workflows\naim to enhance the models' ability to compute Nash Equilibria and make rational\nchoices, even under conditions of uncertainty and incomplete information.\nExperimental results demonstrate that the adoption of these workflows\nsignificantly improves the rationality and robustness of LLMs in game-theoretic\ntasks. Specifically, with the workflow, LLMs exhibit marked improvements in\nidentifying optimal strategies, achieving near-optimal allocations in\nnegotiation scenarios, and reducing susceptibility to exploitation during\nnegotiations. Furthermore, we explore the meta-strategic considerations of\nwhether it is rational for agents to adopt such workflows, recognizing that the\ndecision to use or forgo the workflow constitutes a game-theoretic issue in\nitself.\n  Our research contributes to a deeper understanding of LLMs' decision-making\ncapabilities in strategic contexts and provides insights into enhancing their\nrationality through structured workflows. The findings have implications for\nthe development of more robust and strategically sound AI agents capable of\nnavigating complex interactive environments. Code and data supporting this\nstudy are available at \\url{https://github.com/Wenyueh/game_theory}.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "45 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.05990v2",
    "published_date": "2024-11-08 22:02:22 UTC",
    "updated_date": "2024-11-12 05:46:46 UTC"
  },
  {
    "arxiv_id": "2411.05983v1",
    "title": "Longitudinal Ensemble Integration for sequential classification with multimodal data",
    "authors": [
      "Aviad Susman",
      "Rupak Krishnamurthy",
      "Yan Chak Li",
      "Mohammad Olaimat",
      "Serdar Bozdag",
      "Bino Varghese",
      "Nasim Sheikh-Bahaei",
      "Gaurav Pandey"
    ],
    "abstract": "Effectively modeling multimodal longitudinal data is a pressing need in\nvarious application areas, especially biomedicine. Despite this, few approaches\nexist in the literature for this problem, with most not adequately taking into\naccount the multimodality of the data. In this study, we developed multiple\nconfigurations of a novel multimodal and longitudinal learning framework,\nLongitudinal Ensemble Integration (LEI), for sequential classification. We\nevaluated LEI's performance, and compared it against existing approaches, for\nthe early detection of dementia, which is among the most studied multimodal\nsequential classification tasks. LEI outperformed these approaches due to its\nuse of intermediate base predictions arising from the individual data\nmodalities, which enabled their better integration over time. LEI's design also\nenabled the identification of features that were consistently important across\ntime for the effective prediction of dementia-related diagnoses. Overall, our\nwork demonstrates the potential of LEI for sequential classification from\nlongitudinal multimodal data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, submitted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.05983v1",
    "published_date": "2024-11-08 21:31:48 UTC",
    "updated_date": "2024-11-08 21:31:48 UTC"
  },
  {
    "arxiv_id": "2411.05982v2",
    "title": "Unmasking the Shadows: Pinpoint the Implementations of Anti-Dynamic Analysis Techniques in Malware Using LLM",
    "authors": [
      "Haizhou Wang",
      "Nanqing Luo",
      "Xusheng Li",
      "Peng LIu"
    ],
    "abstract": "Sandboxes and other dynamic analysis processes are prevalent in malware\ndetection systems nowadays to enhance the capability of detecting 0-day\nmalware. Therefore, techniques of anti-dynamic analysis (TADA) are prevalent in\nmodern malware samples, and sandboxes can suffer from false negatives and\nanalysis failures when analyzing the samples with TADAs. In such cases, human\nreverse engineers will get involved in conducting dynamic analysis manually\n(i.e., debugging, patching), which in turn also gets obstructed by TADAs. In\nthis work, we propose a Large Language Model (LLM) based workflow that can\npinpoint the location of the TADA implementation in the code, to help reverse\nengineers place breakpoints used in debugging. Our evaluation shows that we\nsuccessfully identified the locations of 87.80% known TADA implementations\nadopted from public repositories. In addition, we successfully pinpoint the\nlocations of TADAs in 4 well-known malware samples that are documented in\nonline malware analysis blogs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05982v2",
    "published_date": "2024-11-08 21:30:33 UTC",
    "updated_date": "2025-04-29 03:47:09 UTC"
  },
  {
    "arxiv_id": "2411.05980v2",
    "title": "FactLens: Benchmarking Fine-Grained Fact Verification",
    "authors": [
      "Kushan Mitra",
      "Dan Zhang",
      "Sajjadur Rahman",
      "Estevam Hruschka"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive capability in language\ngeneration and understanding, but their tendency to hallucinate and produce\nfactually incorrect information remains a key limitation. To verify\nLLM-generated contents and claims from other sources, traditional verification\napproaches often rely on holistic models that assign a single factuality label\nto complex claims, potentially obscuring nuanced errors. In this paper, we\nadvocate for a shift toward fine-grained verification, where complex claims are\nbroken down into smaller sub-claims for individual verification, allowing for\nmore precise identification of inaccuracies, improved transparency, and reduced\nambiguity in evidence retrieval. However, generating sub-claims poses\nchallenges, such as maintaining context and ensuring semantic equivalence with\nrespect to the original claim. We introduce FactLens, a benchmark for\nevaluating fine-grained fact verification, with metrics and automated\nevaluators of sub-claim quality. The benchmark data is manually curated to\nensure high-quality ground truth. Our results show alignment between automated\nFactLens evaluators and human judgments, and we discuss the impact of sub-claim\ncharacteristics on the overall verification performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, updated version",
    "pdf_url": "http://arxiv.org/pdf/2411.05980v2",
    "published_date": "2024-11-08 21:26:57 UTC",
    "updated_date": "2025-04-18 18:56:20 UTC"
  },
  {
    "arxiv_id": "2411.05963v1",
    "title": "Assessing Foundational Medical 'Segment Anything' (Med-SAM1, Med-SAM2) Deep Learning Models for Left Atrial Segmentation in 3D LGE MRI",
    "authors": [
      "Mehri Mehrnia",
      "Mohamed Elbayumi",
      "Mohammed S. M. Elbaz"
    ],
    "abstract": "Atrial fibrillation (AF), the most common cardiac arrhythmia, is associated\nwith heart failure and stroke. Accurate segmentation of the left atrium (LA) in\n3D late gadolinium-enhanced (LGE) MRI is helpful for evaluating AF, as fibrotic\nremodeling in the LA myocardium contributes to arrhythmia and serves as a key\ndeterminant of therapeutic strategies. However, manual LA segmentation is\nlabor-intensive and challenging. Recent foundational deep learning models, such\nas the Segment Anything Model (SAM), pre-trained on diverse datasets, have\ndemonstrated promise in generic segmentation tasks. MedSAM, a fine-tuned\nversion of SAM for medical applications, enables efficient, zero-shot\nsegmentation without domain-specific training. Despite the potential of MedSAM\nmodel, it has not yet been evaluated for the complex task of LA segmentation in\n3D LGE-MRI. This study aims to (1) evaluate the performance of MedSAM in\nautomating LA segmentation, (2) compare the performance of the MedSAM2 model,\nwhich uses a single prompt with automated tracking, with the MedSAM1 model,\nwhich requires separate prompt for each slice, and (3) analyze the performance\nof MedSAM1 in terms of Dice score(i.e., segmentation accuracy) by varying the\nsize and location of the box prompt.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05963v1",
    "published_date": "2024-11-08 20:49:54 UTC",
    "updated_date": "2024-11-08 20:49:54 UTC"
  },
  {
    "arxiv_id": "2411.05961v1",
    "title": "Aligned Vector Quantization for Edge-Cloud Collabrative Vision-Language Models",
    "authors": [
      "Xiao Liu",
      "Lijun Zhang",
      "Deepak Ganesan",
      "Hui Guan"
    ],
    "abstract": "Vision Language Models (VLMs) are central to Visual Question Answering (VQA)\nsystems and are typically deployed in the cloud due to their high computational\ndemands. However, this cloud-only approach underutilizes edge computational\nresources and requires significant bandwidth for transmitting raw images. In\nthis paper, we introduce an edge-cloud collaborative VQA system, called\nLLaVA-AlignedVQ, which features a novel Aligned Vector Quantization algorithm\n(AlignedVQ) that efficiently compress intermediate features without\ncompromising accuracy to support partitioned execution. Our experiments\ndemonstrate that LLaVA-AlignedVQ achieves approximately 1365x compression rate\nof intermediate features, reducing data transmission overhead by 96.8% compared\nto transmitting JPEG90-compressed images to the cloud. LLaVA-AlignedVQ achieves\nan inference speedup of 2-15x while maintaining high accuracy, remaining within\n-2.23% to +1.6% of the original model's accuracy performance across eight VQA\ndatasets, compared to the cloud-only solution.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.05961v1",
    "published_date": "2024-11-08 20:48:37 UTC",
    "updated_date": "2024-11-08 20:48:37 UTC"
  },
  {
    "arxiv_id": "2411.05958v1",
    "title": "Sentiment Analysis of Cyberbullying Data in Social Media",
    "authors": [
      "Arvapalli Sai Susmitha",
      "Pradeep Pujari"
    ],
    "abstract": "Social media has become an integral part of modern life, but it has also\nbrought with it the pervasive issue of cyberbullying a serious menace in\ntoday's digital age. Cyberbullying, a form of harassment that occurs on social\nnetworks, has escalated alongside the growth of these platforms. Sentiment\nanalysis holds significant potential not only for detecting bullying phrases\nbut also for identifying victims who are at high risk of harm, whether to\nthemselves or others. Our work focuses on leveraging deep learning and natural\nlanguage understanding techniques to detect traces of bullying in social media\nposts. We developed a Recurrent Neural Network with Long Short-Term Memory\n(LSTM) cells, using different embeddings. One approach utilizes BERT\nembeddings, while the other replaces the embeddings layer with the recently\nreleased embeddings API from OpenAI. We conducted a performance comparison\nbetween these two approaches to evaluate their effectiveness in sentiment\nanalysis of Formspring Cyberbullying data. Our Code is Available at\nhttps://github.com/ppujari/xcs224u",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05958v1",
    "published_date": "2024-11-08 20:41:04 UTC",
    "updated_date": "2024-11-08 20:41:04 UTC"
  },
  {
    "arxiv_id": "2411.05945v1",
    "title": "NeKo: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts",
    "authors": [
      "Yen-Ting Lin",
      "Chao-Han Huck Yang",
      "Zhehuai Chen",
      "Piotr Zelasko",
      "Xuesong Yang",
      "Zih-Ching Chen",
      "Krishna C Puvvada",
      "Szu-Wei Fu",
      "Ke Hu",
      "Jun Wei Chiu",
      "Jagadeesh Balam",
      "Boris Ginsburg",
      "Yu-Chiang Frank Wang"
    ],
    "abstract": "Construction of a general-purpose post-recognition error corrector poses a\ncrucial question: how can we most effectively train a model on a large mixture\nof domain datasets? The answer would lie in learning dataset-specific features\nand digesting their knowledge in a single model. Previous methods achieve this\nby having separate correction language models, resulting in a significant\nincrease in parameters. In this work, we present Mixture-of-Experts as a\nsolution, highlighting that MoEs are much more than a scalability tool. We\npropose a Multi-Task Correction MoE, where we train the experts to become an\n``expert'' of speech-to-text, language-to-text and vision-to-text datasets by\nlearning to route each dataset's tokens to its mapped expert. Experiments on\nthe Open ASR Leaderboard show that we explore a new state-of-the-art\nperformance by achieving an average relative $5.0$% WER reduction and\nsubstantial improvements in BLEU scores for speech and translation tasks. On\nzero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with $15.5$% to\n$27.6$% relative WER reduction in the Hyporadise benchmark. NeKo performs\ncompetitively on grammar and post-OCR correction as a multi-task model.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "NeKo work has been done in June 2024. NeKo LMs will be open source on\n  https://huggingface.co/nvidia under the MIT license",
    "pdf_url": "http://arxiv.org/pdf/2411.05945v1",
    "published_date": "2024-11-08 20:11:24 UTC",
    "updated_date": "2024-11-08 20:11:24 UTC"
  },
  {
    "arxiv_id": "2411.05943v1",
    "title": "Quantifying artificial intelligence through algebraic generalization",
    "authors": [
      "Takuya Ito",
      "Murray Campbell",
      "Lior Horesh",
      "Tim Klinger",
      "Parikshit Ram"
    ],
    "abstract": "The rapid development of modern artificial intelligence (AI) systems has\ncreated an urgent need for their scientific quantification. While their fluency\nacross a variety of domains is impressive, modern AI systems fall short on\ntests requiring symbolic processing and abstraction - a glaring limitation\ngiven the necessity for interpretable and reliable technology. Despite a surge\nof reasoning benchmarks emerging from the academic community, no comprehensive\nand theoretically-motivated framework exists to quantify reasoning (and more\ngenerally, symbolic ability) in AI systems. Here, we adopt a framework from\ncomputational complexity theory to explicitly quantify symbolic generalization:\nalgebraic circuit complexity. Many symbolic reasoning problems can be recast as\nalgebraic expressions. Thus, algebraic circuit complexity theory - the study of\nalgebraic expressions as circuit models (i.e., directed acyclic graphs) - is a\nnatural framework to study the complexity of symbolic computation. The tools of\nalgebraic circuit complexity enable the study of generalization by defining\nbenchmarks in terms of their complexity-theoretic properties (i.e., the\ndifficulty of a problem). Moreover, algebraic circuits are generic mathematical\nobjects; for a given algebraic circuit, an arbitrarily large number of samples\ncan be generated for a specific circuit, making it an optimal testbed for the\ndata-hungry machine learning algorithms that are used today. Here, we adopt\ntools from algebraic circuit complexity theory, apply it to formalize a science\nof symbolic generalization, and address key theoretical and empirical\nchallenges for its successful application to AI science and its impact on the\nbroader community.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05943v1",
    "published_date": "2024-11-08 20:08:18 UTC",
    "updated_date": "2024-11-08 20:08:18 UTC"
  },
  {
    "arxiv_id": "2411.05939v1",
    "title": "GCI-ViTAL: Gradual Confidence Improvement with Vision Transformers for Active Learning on Label Noise",
    "authors": [
      "Moseli Mots'oehli",
      "kyungim Baek"
    ],
    "abstract": "Active learning aims to train accurate classifiers while minimizing labeling\ncosts by strategically selecting informative samples for annotation. This study\nfocuses on image classification tasks, comparing AL methods on CIFAR10,\nCIFAR100, Food101, and the Chest X-ray datasets under varying label noise\nrates. We investigate the impact of model architecture by comparing\nConvolutional Neural Networks (CNNs) and Vision Transformer (ViT)-based models.\nAdditionally, we propose a novel deep active learning algorithm, GCI-ViTAL,\ndesigned to be robust to label noise. GCI-ViTAL utilizes prediction entropy and\nthe Frobenius norm of last-layer attention vectors compared to class-centric\nclean set attention vectors. Our method identifies samples that are both\nuncertain and semantically divergent from typical images in their assigned\nclass. This allows GCI-ViTAL to select informative data points even in the\npresence of label noise while flagging potentially mislabeled candidates. Label\nsmoothing is applied to train a model that is not overly confident about\npotentially noisy labels. We evaluate GCI-ViTAL under varying levels of\nsymmetric label noise and compare it to five other AL strategies. Our results\ndemonstrate that using ViTs leads to significant performance improvements over\nCNNs across all AL strategies, particularly in noisy label settings. We also\nfind that using the semantic information of images as label grounding helps in\ntraining a more robust model under label noise. Notably, we do not perform\nextensive hyperparameter tuning, providing an out-of-the-box comparison that\naddresses the common challenge practitioners face in selecting models and\nactive learning strategies without an exhaustive literature review on training\nand fine-tuning vision models on real-world application data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2411.05939v1",
    "published_date": "2024-11-08 19:59:40 UTC",
    "updated_date": "2024-11-08 19:59:40 UTC"
  },
  {
    "arxiv_id": "2411.05936v1",
    "title": "Mitigating Hallucination with ZeroG: An Advanced Knowledge Management Engine",
    "authors": [
      "Anantha Sharma",
      "Sheeba Elizabeth John",
      "Fatemeh Rezapoor Nikroo",
      "Krupali Bhatt",
      "Mrunal Zambre",
      "Aditi Wikhe"
    ],
    "abstract": "The growth of digital documents presents significant challenges in efficient\nmanagement and knowledge extraction. Traditional methods often struggle with\ncomplex documents, leading to issues such as hallucinations and high latency in\nresponses from Large Language Models (LLMs). ZeroG, an innovative approach,\nsignificantly mitigates these challenges by leveraging knowledge distillation\nand prompt tuning to enhance model performance.\n  ZeroG utilizes a smaller model that replicates the behavior of a larger\nteacher model, ensuring contextually relevant and grounded responses, by\nemploying a black-box distillation approach, it creates a distilled dataset\nwithout relying on intermediate features, optimizing computational efficiency.\nThis method significantly enhances accuracy and reduces response times,\nproviding a balanced solution for modern document management.\n  Incorporating advanced techniques for document ingestion and metadata\nutilization, ZeroG improves the accuracy of question-and-answer systems. The\nintegration of graph databases and robust metadata management further\nstreamlines information retrieval, allowing for precise and context-aware\nresponses. By transforming how organizations interact with complex data, ZeroG\nenhances productivity and user experience, offering a scalable solution for the\ngrowing demands of digital document management.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, 4 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2411.05936v1",
    "published_date": "2024-11-08 19:47:02 UTC",
    "updated_date": "2024-11-08 19:47:02 UTC"
  },
  {
    "arxiv_id": "2411.05934v1",
    "title": "Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving",
    "authors": [
      "Saad Tahmid",
      "Sourav Sarker"
    ],
    "abstract": "We present an innovative approach for solving mathematical problems in\nBengali, developed for the DL Sprint 3.0 BUET CSE Fest 2024 Competition. Our\nmethod uses advanced deep learning models, notably the Qwen 2.5 series, with\nimprovements made through prompt engineering, model quantization, and Tool\nIntegrated Reasoning (TIR) to handle complex calculations. Initially, we\nexplored various model architectures, including fine-tuned Mistral and\nquantized Qwen models, refining them with translation techniques,\nRetrieval-Augmented Generation (RAG), and custom dataset curation. Manual\nhyperparameter tuning optimized parameters like temperature and top-p to\nenhance model adaptability and accuracy. Removal of RAG and parameter\nadjustments further improved robustness. Our approach highlights the potential\nof advanced NLP techniques in solving Bengali mathematical problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05934v1",
    "published_date": "2024-11-08 19:44:12 UTC",
    "updated_date": "2024-11-08 19:44:12 UTC"
  },
  {
    "arxiv_id": "2411.05930v2",
    "title": "BERTrend: Neural Topic Modeling for Emerging Trends Detection",
    "authors": [
      "Allaa Boutaleb",
      "Jerome Picault",
      "Guillaume Grosjean"
    ],
    "abstract": "Detecting and tracking emerging trends and weak signals in large, evolving\ntext corpora is vital for applications such as monitoring scientific\nliterature, managing brand reputation, surveilling critical infrastructure and\nmore generally to any kind of text-based event detection. Existing solutions\noften fail to capture the nuanced context or dynamically track evolving\npatterns over time. BERTrend, a novel method, addresses these limitations using\nneural topic modeling in an online setting. It introduces a new metric to\nquantify topic popularity over time by considering both the number of documents\nand update frequency. This metric classifies topics as noise, weak, or strong\nsignals, flagging emerging, rapidly growing topics for further investigation.\nExperimentation on two large real-world datasets demonstrates BERTrend's\nability to accurately detect and track meaningful weak signals while filtering\nout noise, offering a comprehensive solution for monitoring emerging trends in\nlarge-scale, evolving text corpora. The method can also be used for\nretrospective analysis of past events. In addition, the use of Large Language\nModels together with BERTrend offers efficient means for the interpretability\nof trends of events.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 12 figures, FuturED 2024: Workshop on Future of Event\n  Detection (CoLocated with EMNLP 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.05930v2",
    "published_date": "2024-11-08 19:31:19 UTC",
    "updated_date": "2024-11-21 16:06:05 UTC"
  },
  {
    "arxiv_id": "2411.05927v1",
    "title": "Moving Off-the-Grid: Scene-Grounded Video Representations",
    "authors": [
      "Sjoerd van Steenkiste",
      "Daniel Zoran",
      "Yi Yang",
      "Yulia Rubanova",
      "Rishabh Kabra",
      "Carl Doersch",
      "Dilara Gokay",
      "Joseph Heyward",
      "Etienne Pot",
      "Klaus Greff",
      "Drew A. Hudson",
      "Thomas Albert Keck",
      "Joao Carreira",
      "Alexey Dosovitskiy",
      "Mehdi S. M. Sajjadi",
      "Thomas Kipf"
    ],
    "abstract": "Current vision models typically maintain a fixed correspondence between their\nrepresentation structure and image space. Each layer comprises a set of tokens\narranged \"on-the-grid,\" which biases patches or tokens to encode information at\na specific spatio(-temporal) location. In this work we present Moving\nOff-the-Grid (MooG), a self-supervised video representation model that offers\nan alternative approach, allowing tokens to move \"off-the-grid\" to better\nenable them to represent scene elements consistently, even as they move across\nthe image plane through time. By using a combination of cross-attention and\npositional embeddings we disentangle the representation structure and image\nstructure. We find that a simple self-supervised objective--next frame\nprediction--trained on video data, results in a set of latent tokens which bind\nto specific scene structures and track them as they move. We demonstrate the\nusefulness of MooG's learned representation both qualitatively and\nquantitatively by training readouts on top of the learned representation on a\nvariety of downstream tasks. We show that MooG can provide a strong foundation\nfor different vision tasks when compared to \"on-the-grid\" baselines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2024 (spotlight). Project page:\n  https://moog-paper.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2411.05927v1",
    "published_date": "2024-11-08 19:26:51 UTC",
    "updated_date": "2024-11-08 19:26:51 UTC"
  },
  {
    "arxiv_id": "2411.05783v1",
    "title": "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles",
    "authors": [
      "Kayo Yin",
      "Chinmay Singh",
      "Fyodor O. Minakov",
      "Vanessa Milan",
      "Hal Daumé III",
      "Cyril Zhang",
      "Alex X. Lu",
      "Danielle Bragg"
    ],
    "abstract": "Deaf and hard-of-hearing (DHH) students face significant barriers in\naccessing science, technology, engineering, and mathematics (STEM) education,\nnotably due to the scarcity of STEM resources in signed languages. To help\naddress this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia\narticles on STEM topics in English, interpreted into over 300 hours of American\nSign Language (ASL). ASL STEM Wiki is the first continuous signing dataset\nfocused on STEM, facilitating the development of AI resources for STEM\neducation in ASL. We identify several use cases of ASL STEM Wiki with\nhuman-centered applications. For example, because this dataset highlights the\nfrequent use of fingerspelling for technical concepts, which inhibits DHH\nstudents' ability to learn, we develop models to identify fingerspelled words\n-- which can later be used to query for appropriate ASL signs to suggest to\ninterpreters.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.05783v1",
    "published_date": "2024-11-08 18:50:37 UTC",
    "updated_date": "2024-11-08 18:50:37 UTC"
  },
  {
    "arxiv_id": "2411.05781v1",
    "title": "Using Language Models to Disambiguate Lexical Choices in Translation",
    "authors": [
      "Josh Barua",
      "Sanjay Subramanian",
      "Kayo Yin",
      "Alane Suhr"
    ],
    "abstract": "In translation, a concept represented by a single word in a source language\ncan have multiple variations in a target language. The task of lexical\nselection requires using context to identify which variation is most\nappropriate for a source text. We work with native speakers of nine languages\nto create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual\nconcept variation when translating from English. We evaluate recent LLMs and\nneural machine translation systems on DTAiLS, with the best-performing model,\nGPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use\nlanguage models to generate English rules describing target-language concept\nvariations. Providing weaker models with high-quality lexical rules improves\naccuracy substantially, in some cases reaching or outperforming GPT-4.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.05781v1",
    "published_date": "2024-11-08 18:48:57 UTC",
    "updated_date": "2024-11-08 18:48:57 UTC"
  },
  {
    "arxiv_id": "2411.05780v2",
    "title": "GazeSearch: Radiology Findings Search Benchmark",
    "authors": [
      "Trong Thang Pham",
      "Tien-Phat Nguyen",
      "Yuki Ikebe",
      "Akash Awasthi",
      "Zhigang Deng",
      "Carol C. Wu",
      "Hien Nguyen",
      "Ngan Le"
    ],
    "abstract": "Medical eye-tracking data is an important information source for\nunderstanding how radiologists visually interpret medical images. This\ninformation not only improves the accuracy of deep learning models for X-ray\nanalysis but also their interpretability, enhancing transparency in\ndecision-making. However, the current eye-tracking data is dispersed,\nunprocessed, and ambiguous, making it difficult to derive meaningful insights.\nTherefore, there is a need to create a new dataset with more focus and\npurposeful eyetracking data, improving its utility for diagnostic applications.\nIn this work, we propose a refinement method inspired by the target-present\nvisual search challenge: there is a specific finding and fixations are guided\nto locate it. After refining the existing eye-tracking datasets, we transform\nthem into a curated visual search dataset, called GazeSearch, specifically for\nradiology findings, where each fixation sequence is purposefully aligned to the\ntask of locating a particular finding. Subsequently, we introduce a scan path\nprediction baseline, called ChestSearch, specifically tailored to GazeSearch.\nFinally, we employ the newly introduced GazeSearch as a benchmark to evaluate\nthe performance of current state-of-the-art methods, offering a comprehensive\nassessment for visual search in the medical imaging domain. Code is available\nat \\url{https://github.com/UARK-AICV/GazeSearch}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Aceepted WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.05780v2",
    "published_date": "2024-11-08 18:47:08 UTC",
    "updated_date": "2024-11-27 19:01:53 UTC"
  },
  {
    "arxiv_id": "2411.05778v2",
    "title": "LLMs as Method Actors: A Model for Prompt Engineering and Architecture",
    "authors": [
      "Colin Doyle"
    ],
    "abstract": "We introduce \"Method Actors\" as a mental model for guiding LLM prompt\nengineering and prompt architecture. Under this mental model, LLMs should be\nthought of as actors; prompts as scripts and cues; and LLM responses as\nperformances. We apply this mental model to the task of improving LLM\nperformance at playing Connections, a New York Times word puzzle game that\nprior research identified as a challenging benchmark for evaluating LLM\nreasoning. Our experiments with GPT-4o show that a \"Method Actors\" approach can\nsignificantly improve LLM performance over both a vanilla and \"Chain of\nThoughts\" approach. A vanilla approach solves 27% of Connections puzzles in our\ndataset and a \"Chain of Thoughts\" approach solves 41% of puzzles, whereas our\nstrongest \"Method Actor\" approach solves 86% of puzzles. We also test OpenAI's\nnewest model designed specifically for complex reasoning tasks, o1-preview.\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\na time over multiple API calls, o1-preview solves 100% of the puzzles.\nIncorporating a \"Method Actor\" prompt architecture increases the percentage of\npuzzles that o1-preview solves perfectly from 76% to 87%.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05778v2",
    "published_date": "2024-11-08 18:45:06 UTC",
    "updated_date": "2024-11-11 21:09:42 UTC"
  },
  {
    "arxiv_id": "2411.05777v2",
    "title": "Quantitative Assessment of Intersectional Empathetic Bias and Understanding",
    "authors": [
      "Vojtech Formanek",
      "Ondrej Sotolar"
    ],
    "abstract": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05777v2",
    "published_date": "2024-11-08 18:43:15 UTC",
    "updated_date": "2024-11-14 18:35:19 UTC"
  },
  {
    "arxiv_id": "2411.05775v1",
    "title": "Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?",
    "authors": [
      "Veronica Chatrath",
      "Marcelo Lotif",
      "Shaina Raza"
    ],
    "abstract": "Political misinformation poses significant challenges to democratic\nprocesses, shaping public opinion and trust in media. Manual fact-checking\nmethods face issues of scalability and annotator bias, while machine learning\nmodels require large, costly labelled datasets. This study investigates the use\nof state-of-the-art large language models (LLMs) as reliable annotators for\ndetecting political factuality in news articles. Using open-source LLMs, we\ncreate a politically diverse dataset, labelled for bias through LLM-generated\nannotations. These annotations are validated by human experts and further\nevaluated by LLM-based judges to assess the accuracy and reliability of the\nannotations. Our approach offers a scalable and robust alternative to\ntraditional fact-checking, enhancing transparency and public trust in media.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Socially Responsible Language Modelling Research (SoLaR)\n  Workshop at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.05775v1",
    "published_date": "2024-11-08 18:36:33 UTC",
    "updated_date": "2024-11-08 18:36:33 UTC"
  },
  {
    "arxiv_id": "2411.05750v1",
    "title": "On Differentially Private String Distances",
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Erzhi Liu",
      "Han Liu",
      "Zhao Song",
      "Lichen Zhang"
    ],
    "abstract": "Given a database of bit strings $A_1,\\ldots,A_m\\in \\{0,1\\}^n$, a fundamental\ndata structure task is to estimate the distances between a given query $B\\in\n\\{0,1\\}^n$ with all the strings in the database. In addition, one might further\nwant to ensure the integrity of the database by releasing these distance\nstatistics in a secure manner. In this work, we propose differentially private\n(DP) data structures for this type of tasks, with a focus on Hamming and edit\ndistance. On top of the strong privacy guarantees, our data structures are also\ntime- and space-efficient. In particular, our data structure is $\\epsilon$-DP\nagainst any sequence of queries of arbitrary length, and for any query $B$ such\nthat the maximum distance to any string in the database is at most $k$, we\noutput $m$ distance estimates. Moreover,\n  - For Hamming distance, our data structure answers any query in $\\widetilde\nO(mk+n)$ time and each estimate deviates from the true distance by at most\n$\\widetilde O(k/e^{\\epsilon/\\log k})$;\n  - For edit distance, our data structure answers any query in $\\widetilde\nO(mk^2+n)$ time and each estimate deviates from the true distance by at most\n$\\widetilde O(k/e^{\\epsilon/(\\log k \\log n)})$.\n  For moderate $k$, both data structures support sublinear query operations. We\nobtain these results via a novel adaptation of the randomized response\ntechnique as a bit flipping procedure, applied to the sketched strings.",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05750v1",
    "published_date": "2024-11-08 18:10:07 UTC",
    "updated_date": "2024-11-08 18:10:07 UTC"
  },
  {
    "arxiv_id": "2411.05748v1",
    "title": "Multi-Dimensional Reconfigurable, Physically Composable Hybrid Diffractive Optical Neural Network",
    "authors": [
      "Ziang Yin",
      "Yu Yao",
      "Jeff Zhang",
      "Jiaqi Gu"
    ],
    "abstract": "Diffractive optical neural networks (DONNs), leveraging free-space light wave\npropagation for ultra-parallel, high-efficiency computing, have emerged as\npromising artificial intelligence (AI) accelerators. However, their inherent\nlack of reconfigurability due to fixed optical structures post-fabrication\nhinders practical deployment in the face of dynamic AI workloads and evolving\napplications. To overcome this challenge, we introduce, for the first time, a\nmulti-dimensional reconfigurable hybrid diffractive ONN system (MDR-HDONN), a\nphysically composable architecture that unlocks a new degree of freedom and\nunprecedented versatility in DONNs. By leveraging full-system learnability,\nMDR-HDONN repurposes fixed fabricated optical hardware, achieving exponentially\nexpanded functionality and superior task adaptability through the\ndifferentiable learning of system variables. Furthermore, MDR-HDONN adopts a\nhybrid optical/photonic design, combining the reconfigurability of integrated\nphotonics with the ultra-parallelism of free-space diffractive systems.\nExtensive evaluations demonstrate that MDR-HDONN has digital-comparable\naccuracy on various task adaptations with 74x faster speed and 194x lower\nenergy. Compared to prior DONNs, MDR-HDONN shows exponentially larger\nfunctional space with 5x faster training speed, paving the way for a new\nparadigm of versatile, composable, hybrid optical/photonic AI computing. We\nwill open-source our codes.",
    "categories": [
      "physics.optics",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "physics.optics",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.05748v1",
    "published_date": "2024-11-08 18:08:49 UTC",
    "updated_date": "2024-11-08 18:08:49 UTC"
  },
  {
    "arxiv_id": "2411.05746v2",
    "title": "Continuous-Time Analysis of Adaptive Optimization and Normalization",
    "authors": [
      "Rhys Gould",
      "Hidenori Tanaka"
    ],
    "abstract": "Adaptive optimization algorithms, particularly Adam and its variant AdamW,\nare fundamental components of modern deep learning. However, their training\ndynamics lack comprehensive theoretical understanding, with limited insight\ninto why common practices -- such as specific hyperparameter choices and\nnormalization layers -- contribute to successful generalization. This work\npresents a continuous-time formulation of Adam and AdamW, facilitating a\ntractable analysis of training dynamics that can shed light on such practical\nquestions. We theoretically derive a stable region for Adam's hyperparameters\n$(\\beta, \\gamma)$ that ensures bounded updates, empirically verifying these\npredictions by observing unstable exponential parameter growth outside of this\nstable region. Furthermore, we theoretically justify the success of\nnormalization layers by uncovering an implicit meta-adaptive effect of\nscale-invariant architectural components. This insight leads to an explicit\noptimizer, $2$-Adam, which we generalize to $k$-Adam -- an optimizer that\napplies an adaptive normalization procedure $k$ times, encompassing Adam\n(corresponding to $k=1$) and Adam with a normalization layer (corresponding to\n$k=2$). Overall, our continuous-time formulation of Adam facilitates a\nprincipled analysis, offering deeper understanding of optimal hyperparameter\nchoices and architectural decisions in modern deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05746v2",
    "published_date": "2024-11-08 18:07:55 UTC",
    "updated_date": "2024-12-19 21:24:51 UTC"
  },
  {
    "arxiv_id": "2411.05742v1",
    "title": "Topology-aware Reinforcement Feature Space Reconstruction for Graph Data",
    "authors": [
      "Wangyang Ying",
      "Haoyue Bai",
      "Kunpeng Liu",
      "Yanjie Fu"
    ],
    "abstract": "Feature space is an environment where data points are vectorized to represent\nthe original dataset. Reconstructing a good feature space is essential to\naugment the AI power of data, improve model generalization, and increase the\navailability of downstream ML models. Existing literature, such as feature\ntransformation and feature selection, is labor-intensive (e.g., heavy reliance\non empirical experience) and mostly designed for tabular data. Moreover, these\nmethods regard data samples as independent, which ignores the unique\ntopological structure when applied to graph data, thus resulting in a\nsuboptimal reconstruction feature space. Can we consider the topological\ninformation to automatically reconstruct feature space for graph data without\nheavy experiential knowledge? To fill this gap, we leverage topology-aware\nreinforcement learning to automate and optimize feature space reconstruction\nfor graph data. Our approach combines the extraction of core subgraphs to\ncapture essential structural information with a graph neural network (GNN) to\nencode topological features and reduce computing complexity. Then we introduce\nthree reinforcement agents within a hierarchical structure to systematically\ngenerate meaningful features through an iterative process, effectively\nreconstructing the feature space. This framework provides a principled solution\nfor attributed graph feature space reconstruction. The extensive experiments\ndemonstrate the effectiveness and efficiency of including topological\nawareness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05742v1",
    "published_date": "2024-11-08 18:01:05 UTC",
    "updated_date": "2024-11-08 18:01:05 UTC"
  },
  {
    "arxiv_id": "2411.05735v2",
    "title": "Aioli: A Unified Optimization Framework for Language Model Data Mixing",
    "authors": [
      "Mayee F. Chen",
      "Michael Y. Hu",
      "Nicholas Lourie",
      "Kyunghyun Cho",
      "Christopher Ré"
    ],
    "abstract": "Language model performance depends on identifying the optimal mixture of data\ngroups to train on (e.g., law, code, math). Prior work has proposed a diverse\nset of methods to efficiently learn mixture proportions, ranging from fitting\nregression models over training runs to dynamically updating proportions\nthroughout training. Surprisingly, we find that no existing method consistently\noutperforms a simple stratified sampling baseline in terms of average test\nperplexity. To understand this inconsistency, we unify existing methods into a\nstandard framework, showing they are equivalent to solving a common\noptimization problem: minimize average loss subject to a method-specific mixing\nlaw -- an implicit assumption on the relationship between loss and mixture\nproportions. This framework suggests that measuring the fidelity of a method's\nmixing law can offer insights into its performance. Empirically, we find that\nexisting methods set their mixing law parameters inaccurately, resulting in the\ninconsistent mixing performance we observe. Using this insight, we derive a new\nonline method named Aioli, which directly estimates the mixing law parameters\nthroughout training and uses them to dynamically adjust proportions. Aioli\noutperforms stratified sampling on 6 out of 6 datasets by an average of 0.27\ntest perplexity points, whereas existing methods fail to consistently beat\nstratified sampling, doing up to 6.9 points worse. Moreover, in a practical\nsetting where proportions are learned on shorter runs due to computational\nconstraints, Aioli can dynamically adjust these proportions over the full\ntraining run, consistently improving performance over existing methods by up to\n12.012 test perplexity points.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025 Camera Ready",
    "pdf_url": "http://arxiv.org/pdf/2411.05735v2",
    "published_date": "2024-11-08 17:50:24 UTC",
    "updated_date": "2025-04-21 03:50:23 UTC"
  },
  {
    "arxiv_id": "2411.05718v1",
    "title": "A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics",
    "authors": [
      "Puze Liu",
      "Jonas Günster",
      "Niklas Funk",
      "Simon Gröger",
      "Dong Chen",
      "Haitham Bou-Ammar",
      "Julius Jankowski",
      "Ante Marić",
      "Sylvain Calinon",
      "Andrej Orsula",
      "Miguel Olivares-Mendez",
      "Hongyi Zhou",
      "Rudolf Lioutikov",
      "Gerhard Neumann",
      "Amarildo Likmeta Amirhossein Zhalehmehrabi",
      "Thomas Bonenfant",
      "Marcello Restelli",
      "Davide Tateo",
      "Ziyuan Liu",
      "Jan Peters"
    ],
    "abstract": "Machine learning methods have a groundbreaking impact in many application\ndomains, but their application on real robotic platforms is still limited.\nDespite the many challenges associated with combining machine learning\ntechnology with robotics, robot learning remains one of the most promising\ndirections for enhancing the capabilities of robots. When deploying\nlearning-based approaches on real robots, extra effort is required to address\nthe challenges posed by various real-world factors. To investigate the key\nfactors influencing real-world deployment and to encourage original solutions\nfrom different researchers, we organized the Robot Air Hockey Challenge at the\nNeurIPS 2023 conference. We selected the air hockey task as a benchmark,\nencompassing low-level robotics problems and high-level tactics. Different from\nother machine learning-centric benchmarks, participants need to tackle\npractical challenges in robotics, such as the sim-to-real gap, low-level\ncontrol issues, safety problems, real-time requirements, and the limited\navailability of real-world data. Furthermore, we focus on a dynamic\nenvironment, removing the typical assumption of quasi-static motions of other\nreal-world benchmarks. The competition's results show that solutions combining\nlearning-based approaches with prior knowledge outperform those relying solely\non data when real-world deployment is challenging. Our ablation study reveals\nwhich real-world factors may be overlooked when building a learning-based\nsolution. The successful real-world air hockey deployment of best-performing\nagents sets the foundation for future competitions and follow-up research\ndirections.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accept at NeurIPS 2024 Dataset and Benchmark Track",
    "pdf_url": "http://arxiv.org/pdf/2411.05718v1",
    "published_date": "2024-11-08 17:20:47 UTC",
    "updated_date": "2024-11-08 17:20:47 UTC"
  },
  {
    "arxiv_id": "2411.05903v1",
    "title": "Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language Model",
    "authors": [
      "Ben Koska",
      "Mojmír Horváth"
    ],
    "abstract": "We present a novel 4.5B parameter small language model that can handle\nmultiple input and output modalities, including text, images, videos, and\naudio. Despite its small size, the model achieves near state-of-the-art\nperformance on a variety of tasks, demonstrating the potential of multi-modal\nmodels to tackle complex real-world problems. Our approach leverages recent\nadvancements in language modeling and multi-task learning to create a versatile\nand high-performing model that can even be deployed for edge inference.\nExperimental results show the model's strong performance across multiple\nbenchmarks, paving the way for further progress in multi-modal artificial\nintelligence.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05903v1",
    "published_date": "2024-11-08 17:15:17 UTC",
    "updated_date": "2024-11-08 17:15:17 UTC"
  },
  {
    "arxiv_id": "2411.16690v1",
    "title": "Benefits and Risks of Using ChatGPT4 as a Teaching Assistant for Computer Science Students",
    "authors": [
      "Yaiza Aragonés-Soria",
      "Julia Kotovich",
      "Chitsutha Soomlek",
      "Manuel Oriol"
    ],
    "abstract": "Upon release, ChatGPT3.5 shocked the software engineering community by its\nability to generate answers to specialized questions about coding. Immediately,\nmany educators wondered if it was possible to use the chatbot as a support tool\nthat helps students answer their programming questions. This article evaluates\nthis possibility at three levels: fundamental Computer Science knowledge (basic\nalgorithms and data structures), core competency (design patterns), and\nadvanced knowledge (quantum computing). In each case, we ask normalized\nquestions several times to ChatGPT3.5, then look at the correctness of answers,\nand finally check if this creates issues. The main result is that the\nperformances of ChatGPT3.5 degrades drastically as the specialization of the\ndomain increases: for basic algorithms it returns answers that are almost\nalways correct, for design patterns the generated code contains many code\nsmells and is generally of low quality, but it is still sometimes able to fix\nit (if asked), and for quantum computing it is often blatantly wrong.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "This paper was finished on the 17th of June of 2023",
    "pdf_url": "http://arxiv.org/pdf/2411.16690v1",
    "published_date": "2024-11-08 17:11:10 UTC",
    "updated_date": "2024-11-08 17:11:10 UTC"
  },
  {
    "arxiv_id": "2411.05698v1",
    "title": "Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc Explainability in Image Classification",
    "authors": [
      "Antonio De Santis",
      "Riccardo Campi",
      "Matteo Bianchi",
      "Marco Brambilla"
    ],
    "abstract": "Convolutional Neural Networks (CNNs) have seen significant performance\nimprovements in recent years. However, due to their size and complexity, they\nfunction as black-boxes, leading to transparency concerns. State-of-the-art\nsaliency methods generate local explanations that highlight the area in the\ninput image where a class is identified but cannot explain how a concept of\ninterest contributes to the prediction, which is essential for bias mitigation.\nOn the other hand, concept-based methods, such as TCAV (Testing with Concept\nActivation Vectors), provide insights into how sensitive is the network to a\nconcept, but cannot compute its attribution in a specific prediction nor show\nits location within the input image. This paper introduces a novel post-hoc\nexplainability framework, Visual-TCAV, which aims to bridge the gap between\nthese methods by providing both local and global explanations for CNN-based\nimage classification. Visual-TCAV uses Concept Activation Vectors (CAVs) to\ngenerate saliency maps that show where concepts are recognized by the network.\nMoreover, it can estimate the attribution of these concepts to the output of\nany class using a generalization of Integrated Gradients. This framework is\nevaluated on popular CNN architectures, with its validity further confirmed via\nexperiments where ground truth for explanations is known, and a comparison with\nTCAV. Our code will be made available soon.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint currently under review",
    "pdf_url": "http://arxiv.org/pdf/2411.05698v1",
    "published_date": "2024-11-08 16:52:52 UTC",
    "updated_date": "2024-11-08 16:52:52 UTC"
  },
  {
    "arxiv_id": "2411.05691v1",
    "title": "Asterisk*: Keep it Simple",
    "authors": [
      "Andrew Semenov"
    ],
    "abstract": "This paper describes Asterisk, a compact GPT-based model for generating text\nembeddings. The model uses a minimalist architecture with two layers, two\nattention heads, and 256 embedding dimensions. By applying knowledge\ndistillation from larger pretrained models, we explore the trade-offs between\nmodel size and performance while minimizing computational and memory\nrequirements. The model is primarily evaluated and optimized for classification\ntasks, with experimental results showing its moderate performance in zero-shot\nclassification across various downstream applications. With additional\nconfiguration, the model performance can approach or even surpass that of\nlarger architectures on specific classification tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05691v1",
    "published_date": "2024-11-08 16:42:33 UTC",
    "updated_date": "2024-11-08 16:42:33 UTC"
  },
  {
    "arxiv_id": "2411.05683v1",
    "title": "Data-Driven Distributed Common Operational Picture from Heterogeneous Platforms using Multi-Agent Reinforcement Learning",
    "authors": [
      "Indranil Sur",
      "Aswin Raghavan",
      "Abrar Rahman",
      "James Z Hare",
      "Daniel Cassenti",
      "Carl Busart"
    ],
    "abstract": "The integration of unmanned platforms equipped with advanced sensors promises\nto enhance situational awareness and mitigate the \"fog of war\" in military\noperations. However, managing the vast influx of data from these platforms\nposes a significant challenge for Command and Control (C2) systems. This study\npresents a novel multi-agent learning framework to address this challenge. Our\nmethod enables autonomous and secure communication between agents and humans,\nwhich in turn enables real-time formation of an interpretable Common\nOperational Picture (COP). Each agent encodes its perceptions and actions into\ncompact vectors, which are then transmitted, received and decoded to form a COP\nencompassing the current state of all agents (friendly and enemy) on the\nbattlefield. Using Deep Reinforcement Learning (DRL), we jointly train COP\nmodels and agent's action selection policies. We demonstrate resilience to\ndegraded conditions such as denied GPS and disrupted communications.\nExperimental validation is performed in the Starcraft-2 simulation environment\nto evaluate the precision of the COPs and robustness of policies. We report\nless than 5% error in COPs and policies resilient to various adversarial\nconditions. In summary, our contributions include a method for autonomous COP\nformation, increased resilience through distributed prediction, and joint\ntraining of COP models and multi-agent RL policies. This research advances\nadaptive and resilient C2, facilitating effective control of heterogeneous\nunmanned platforms.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "29th International Command and Control Research & Technology\n  Symposium",
    "pdf_url": "http://arxiv.org/pdf/2411.05683v1",
    "published_date": "2024-11-08 16:31:22 UTC",
    "updated_date": "2024-11-08 16:31:22 UTC"
  },
  {
    "arxiv_id": "2411.05679v3",
    "title": "Tell What You Hear From What You See -- Video to Audio Generation Through Text",
    "authors": [
      "Xiulong Liu",
      "Kun Su",
      "Eli Shlizerman"
    ],
    "abstract": "The content of visual and audio scenes is multi-faceted such that a video can\nbe paired with various audio and vice-versa. Thereby, in video-to-audio\ngeneration task, it is imperative to introduce steering approaches for\ncontrolling the generated audio. While Video-to-Audio generation is a\nwell-established generative task, existing methods lack such controllability.\nIn this work, we propose VATT, a multi-modal generative framework that takes a\nvideo and an optional text prompt as input, and generates audio and optional\ntextual description of the audio. Such a framework has two advantages: i)\nVideo-to-Audio generation process can be refined and controlled via text which\ncomplements the context of visual information, and ii) The model can suggest\nwhat audio to generate for the video by generating audio captions. VATT\nconsists of two key modules: VATT Converter, a LLM that is fine-tuned for\ninstructions and includes a projection layer that maps video features to the\nLLM vector space; and VATT Audio, a transformer that generates audio tokens\nfrom visual frames and from optional text prompt using iterative parallel\ndecoding. The audio tokens are converted to a waveform by pretrained neural\ncodec. Experiments show that when VATT is compared to existing video-to-audio\ngeneration methods in objective metrics, it achieves competitive performance\nwhen the audio caption is not provided. When the audio caption is provided as a\nprompt, VATT achieves even more refined performance (lowest KLD score of 1.41).\nFurthermore, subjective studies show that VATT Audio has been chosen as\npreferred generated audio than audio generated by existing methods. VATT\nenables controllable video-to-audio generation through text as well as\nsuggesting text prompts for videos through audio captions, unlocking novel\napplications such as text-guided video-to-audio generation and video-to-audio\ncaptioning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024. Project page: https://dragonliu1995.github.io/VATT-home",
    "pdf_url": "http://arxiv.org/pdf/2411.05679v3",
    "published_date": "2024-11-08 16:29:07 UTC",
    "updated_date": "2025-04-04 21:50:29 UTC"
  },
  {
    "arxiv_id": "2411.05676v1",
    "title": "Improving Molecular Graph Generation with Flow Matching and Optimal Transport",
    "authors": [
      "Xiaoyang Hou",
      "Tian Zhu",
      "Milong Ren",
      "Dongbo Bu",
      "Xin Gao",
      "Chunming Zhang",
      "Shiwei Sun"
    ],
    "abstract": "Generating molecular graphs is crucial in drug design and discovery but\nremains challenging due to the complex interdependencies between nodes and\nedges. While diffusion models have demonstrated their potentiality in molecular\ngraph design, they often suffer from unstable training and inefficient\nsampling. To enhance generation performance and training stability, we propose\nGGFlow, a discrete flow matching generative model incorporating optimal\ntransport for molecular graphs and it incorporates an edge-augmented graph\ntransformer to enable the direct communications among chemical bounds.\nAdditionally, GGFlow introduces a novel goal-guided generation framework to\ncontrol the generative trajectory of our model, aiming to design novel\nmolecular structures with the desired properties. GGFlow demonstrates superior\nperformance on both unconditional and conditional molecule generation tasks,\noutperforming existing baselines and underscoring its effectiveness and\npotential for wider application.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05676v1",
    "published_date": "2024-11-08 16:27:27 UTC",
    "updated_date": "2024-11-08 16:27:27 UTC"
  },
  {
    "arxiv_id": "2411.07165v1",
    "title": "Acoustic-based 3D Human Pose Estimation Robust to Human Position",
    "authors": [
      "Yusuke Oumi",
      "Yuto Shibata",
      "Go Irie",
      "Akisato Kimura",
      "Yoshimitsu Aoki",
      "Mariko Isogawa"
    ],
    "abstract": "This paper explores the problem of 3D human pose estimation from only\nlow-level acoustic signals. The existing active acoustic sensing-based approach\nfor 3D human pose estimation implicitly assumes that the target user is\npositioned along a line between loudspeakers and a microphone. Because\nreflection and diffraction of sound by the human body cause subtle acoustic\nsignal changes compared to sound obstruction, the existing model degrades its\naccuracy significantly when subjects deviate from this line, limiting its\npracticality in real-world scenarios. To overcome this limitation, we propose a\nnovel method composed of a position discriminator and reverberation-resistant\nmodel. The former predicts the standing positions of subjects and applies\nadversarial learning to extract subject position-invariant features. The latter\nutilizes acoustic signals before the estimation target time as references to\nenhance robustness against the variations in sound arrival times due to\ndiffraction and reflection. We construct an acoustic pose estimation dataset\nthat covers diverse human locations and demonstrate through experiments that\nour proposed method outperforms existing approaches.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at BMVC2024",
    "pdf_url": "http://arxiv.org/pdf/2411.07165v1",
    "published_date": "2024-11-08 15:56:12 UTC",
    "updated_date": "2024-11-08 15:56:12 UTC"
  },
  {
    "arxiv_id": "2411.05898v1",
    "title": "Integrating Object Detection Modality into Visual Language Model for Enhanced Autonomous Driving Agent",
    "authors": [
      "Linfeng He",
      "Yiming Sun",
      "Sihao Wu",
      "Jiaxu Liu",
      "Xiaowei Huang"
    ],
    "abstract": "In this paper, we propose a novel framework for enhancing visual\ncomprehension in autonomous driving systems by integrating visual language\nmodels (VLMs) with additional visual perception module specialised in object\ndetection. We extend the Llama-Adapter architecture by incorporating a\nYOLOS-based detection network alongside the CLIP perception network, addressing\nlimitations in object detection and localisation. Our approach introduces\ncamera ID-separators to improve multi-view processing, crucial for\ncomprehensive environmental awareness. Experiments on the DriveLM visual\nquestion answering challenge demonstrate significant improvements over baseline\nmodels, with enhanced performance in ChatGPT scores, BLEU scores, and CIDEr\nmetrics, indicating closeness of model answer to ground truth. Our method\nrepresents a promising step towards more capable and interpretable autonomous\ndriving systems. Possible safety enhancement enabled by detection modality is\nalso discussed.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted by SafeGenAI workshop of NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.05898v1",
    "published_date": "2024-11-08 15:50:30 UTC",
    "updated_date": "2024-11-08 15:50:30 UTC"
  },
  {
    "arxiv_id": "2411.05897v2",
    "title": "Humans and Large Language Models in Clinical Decision Support: A Study with Medical Calculators",
    "authors": [
      "Nicholas Wan",
      "Qiao Jin",
      "Joey Chan",
      "Guangzhi Xiong",
      "Serina Applebaum",
      "Aidan Gilson",
      "Reid McMurry",
      "R. Andrew Taylor",
      "Aidong Zhang",
      "Qingyu Chen",
      "Zhiyong Lu"
    ],
    "abstract": "Although large language models (LLMs) have been assessed for general medical\nknowledge using licensing exams, their ability to support clinical\ndecision-making, such as selecting medical calculators, remains uncertain. We\nassessed nine LLMs, including open-source, proprietary, and domain-specific\nmodels, with 1,009 multiple-choice question-answer pairs across 35 clinical\ncalculators and compared LLMs to humans on a subset of questions. While the\nhighest-performing LLM, OpenAI o1, provided an answer accuracy of 66.0% (CI:\n56.7-75.3%) on the subset of 100 questions, two human annotators nominally\noutperformed LLMs with an average answer accuracy of 79.5% (CI: 73.5-85.0%).\nUltimately, we evaluated medical trainees and LLMs in recommending medical\ncalculators across clinical scenarios like risk stratification and diagnosis.\nWith error analysis showing that the highest-performing LLMs continue to make\nmistakes in comprehension (49.3% of errors) and calculator knowledge (7.1% of\nerrors), our findings highlight that LLMs are not superior to humans in\ncalculator recommendation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.05897v2",
    "published_date": "2024-11-08 15:50:19 UTC",
    "updated_date": "2025-03-21 21:13:39 UTC"
  },
  {
    "arxiv_id": "2411.05653v1",
    "title": "The influence of persona and conversational task on social interactions with a LLM-controlled embodied conversational agent",
    "authors": [
      "Leon O. H. Kroczek",
      "Alexander May",
      "Selina Hettenkofer",
      "Andreas Ruider",
      "Bernd Ludwig",
      "Andreas Mühlberger"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nconversational tasks. Embodying an LLM as a virtual human allows users to\nengage in face-to-face social interactions in Virtual Reality. However, the\ninfluence of person- and task-related factors in social interactions with\nLLM-controlled agents remains unclear. In this study, forty-six participants\ninteracted with a virtual agent whose persona was manipulated as extravert or\nintrovert in three different conversational tasks (small talk, knowledge test,\nconvincing). Social-evaluation, emotional experience, and realism were assessed\nusing ratings. Interactive engagement was measured by quantifying participants'\nwords and conversational turns. Finally, we measured participants' willingness\nto ask the agent for help during the knowledge test. Our findings show that the\nextraverted agent was more positively evaluated, elicited a more pleasant\nexperience and greater engagement, and was assessed as more realistic compared\nto the introverted agent. Whereas persona did not affect the tendency to ask\nfor help, participants were generally more confident in the answer when they\nhad help of the LLM. Variation of personality traits of LLM-controlled embodied\nvirtual agents, therefore, affects social-emotional processing and behavior in\nvirtual interactions. Embodied virtual agents allow the presentation of\nnaturalistic social encounters in a virtual environment.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.05653v1",
    "published_date": "2024-11-08 15:49:42 UTC",
    "updated_date": "2024-11-08 15:49:42 UTC"
  },
  {
    "arxiv_id": "2411.10475v1",
    "title": "Beyond object identification: How train drivers evaluate the risk of collision",
    "authors": [
      "Romy Müller",
      "Judith Schmidt"
    ],
    "abstract": "When trains collide with obstacles, the consequences are often severe. To\nassess how artificial intelligence might contribute to avoiding collisions, we\nneed to understand how train drivers do it. What aspects of a situation do they\nconsider when evaluating the risk of collision? In the present study, we\nassumed that train drivers do not only identify potential obstacles but\ninterpret what they see in order to anticipate how the situation might unfold.\nHowever, to date it is unclear how exactly this is accomplished. Therefore, we\nassessed which cues train drivers use and what inferences they make. To this\nend, image-based expert interviews were conducted with 33 train drivers.\nParticipants saw images with potential obstacles, rated the risk of collision,\nand explained their evaluation. Moreover, they were asked how the situation\nwould need to change to decrease or increase collision risk. From their verbal\nreports, we extracted concepts about the potential obstacles, contexts, or\nconsequences, and assigned these concepts to various categories (e.g., people's\nidentity, location, movement, action, physical features, and mental states).\nThe results revealed that especially for people, train drivers reason about\ntheir actions and mental states, and draw relations between concepts to make\nfurther inferences. These inferences systematically differ between situations.\nOur findings emphasise the need to understand train drivers' risk evaluation\nprocesses when aiming to enhance the safety of both human and automatic train\noperation.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10475v1",
    "published_date": "2024-11-08 15:38:47 UTC",
    "updated_date": "2024-11-08 15:38:47 UTC"
  },
  {
    "arxiv_id": "2411.05633v1",
    "title": "SynDroneVision: A Synthetic Dataset for Image-Based Drone Detection",
    "authors": [
      "Tamara R. Lenhard",
      "Andreas Weinmann",
      "Kai Franke",
      "Tobias Koch"
    ],
    "abstract": "Developing robust drone detection systems is often constrained by the limited\navailability of large-scale annotated training data and the high costs\nassociated with real-world data collection. However, leveraging synthetic data\ngenerated via game engine-based simulations provides a promising and\ncost-effective solution to overcome this issue. Therefore, we present\nSynDroneVision, a synthetic dataset specifically designed for RGB-based drone\ndetection in surveillance applications. Featuring diverse backgrounds, lighting\nconditions, and drone models, SynDroneVision offers a comprehensive training\nfoundation for deep learning algorithms. To evaluate the dataset's\neffectiveness, we perform a comparative analysis across a selection of recent\nYOLO detection models. Our findings demonstrate that SynDroneVision is a\nvaluable resource for real-world data enrichment, achieving notable\nenhancements in model performance and robustness, while significantly reducing\nthe time and costs of real-world data acquisition. SynDroneVision will be\npublicly released upon paper acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at the 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV)",
    "pdf_url": "http://arxiv.org/pdf/2411.05633v1",
    "published_date": "2024-11-08 15:22:49 UTC",
    "updated_date": "2024-11-08 15:22:49 UTC"
  },
  {
    "arxiv_id": "2411.05618v1",
    "title": "Knowledge Distillation Neural Network for Predicting Car-following Behaviour of Human-driven and Autonomous Vehicles",
    "authors": [
      "Ayobami Adewale",
      "Chris Lee",
      "Amnir Hadachi",
      "Nicolly Lima da Silva"
    ],
    "abstract": "As we move towards a mixed-traffic scenario of Autonomous vehicles (AVs) and\nHuman-driven vehicles (HDVs), understanding the car-following behaviour is\nimportant to improve traffic efficiency and road safety. Using a real-world\ntrajectory dataset, this study uses descriptive and statistical analysis to\ninvestigate the car-following behaviours of three vehicle pairs: HDV-AV, AV-HDV\nand HDV-HDV in mixed traffic. The ANOVA test showed that car-following\nbehaviours across different vehicle pairs are statistically significant\n(p-value < 0.05).\n  We also introduce a data-driven Knowledge Distillation Neural Network (KDNN)\nmodel for predicting car-following behaviour in terms of speed. The KDNN model\ndemonstrates comparable predictive accuracy to its teacher network, a Long\nShort-Term Memory (LSTM) network, and outperforms both the standalone student\nnetwork, a Multilayer Perceptron (MLP), and traditional physics-based models\nlike the Gipps model. Notably, the KDNN model better prevents collisions,\nmeasured by minimum Time-to-Collision (TTC), and operates with lower\ncomputational power, making it ideal for AVs or driving simulators requiring\nefficient computing.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "27th IEEE International Conference on Intelligent Transportation\n  Systems",
    "pdf_url": "http://arxiv.org/pdf/2411.05618v1",
    "published_date": "2024-11-08 14:57:59 UTC",
    "updated_date": "2024-11-08 14:57:59 UTC"
  },
  {
    "arxiv_id": "2411.05614v1",
    "title": "Acceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey",
    "authors": [
      "Zhihong Liu",
      "Xin Xu",
      "Peng Qiao",
      "Dongsheng Li"
    ],
    "abstract": "Deep reinforcement learning has led to dramatic breakthroughs in the field of\nartificial intelligence for the past few years. As the amount of rollout\nexperience data and the size of neural networks for deep reinforcement learning\nhave grown continuously, handling the training process and reducing the time\nconsumption using parallel and distributed computing is becoming an urgent and\nessential desire. In this paper, we perform a broad and thorough investigation\non training acceleration methodologies for deep reinforcement learning based on\nparallel and distributed computing, providing a comprehensive survey in this\nfield with state-of-the-art methods and pointers to core references. In\nparticular, a taxonomy of literature is provided, along with a discussion of\nemerging topics and open issues. This incorporates learning system\narchitectures, simulation parallelism, computing parallelism, distributed\nsynchronization mechanisms, and deep evolutionary reinforcement learning.\nFurther, we compare 16 current open-source libraries and platforms with\ncriteria of facilitating rapid development. Finally, we extrapolate future\ndirections that deserve further research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted by ACM Computing Surveys",
    "pdf_url": "http://arxiv.org/pdf/2411.05614v1",
    "published_date": "2024-11-08 14:55:32 UTC",
    "updated_date": "2024-11-08 14:55:32 UTC"
  },
  {
    "arxiv_id": "2411.05599v1",
    "title": "Expectation vs. Reality: Towards Verification of Psychological Games",
    "authors": [
      "Marta Kwiatkowska",
      "Gethin Norman",
      "David Parker",
      "Gabriel Santos"
    ],
    "abstract": "Game theory provides an effective way to model strategic interactions among\nrational agents. In the context of formal verification, these ideas can be used\nto produce guarantees on the correctness of multi-agent systems, with a diverse\nrange of applications from computer security to autonomous driving.\nPsychological games (PGs) were developed as a way to model and analyse agents\nwith belief-dependent motivations, opening up the possibility to model how\nhuman emotions can influence behaviour. In PGs, players' utilities depend not\nonly on what actually happens (which strategies players choose to adopt), but\nalso on what the players had expected to happen (their belief as to the\nstrategies that would be played). Despite receiving much attention in fields\nsuch as economics and psychology, very little consideration has been given to\ntheir applicability to problems in computer science, nor to practical\nalgorithms and tool support. In this paper, we start to bridge that gap,\nproposing methods to solve PGs and implementing them within PRISM-games, a\nformal verification tool for stochastic games. We discuss how to model these\ngames, highlight specific challenges for their analysis and illustrate the\nusefulness of our approach on several case studies, including human behaviour\nin traffic scenarios.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05599v1",
    "published_date": "2024-11-08 14:41:52 UTC",
    "updated_date": "2024-11-08 14:41:52 UTC"
  },
  {
    "arxiv_id": "2411.05894v1",
    "title": "SSSD: Simply-Scalable Speculative Decoding",
    "authors": [
      "Michele Marzollo",
      "Jiawei Zhuang",
      "Niklas Roemer",
      "Lorenz K. Müller",
      "Lukas Cavigelli"
    ],
    "abstract": "Over the past year, Speculative Decoding has gained popularity as a technique\nfor accelerating Large Language Model inference. While several methods have\nbeen introduced, most struggle to deliver satisfactory performance at batch\nsizes typical for data centers ($\\geq 8$) and often involve significant\ndeployment complexities. In this work, we offer a theoretical explanation of\nhow Speculative Decoding can be effectively utilized with larger batch sizes.\nWe also introduce a method that integrates seamlessly into existing systems\nwithout additional training or the complexity of deploying a small LLM. In a\ncontinuous batching setting, we achieve a 4x increase in throughput without any\nlatency impact for short context generation, and a 1.7-2x improvement in both\nlatency and throughput for longer contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.05894v1",
    "published_date": "2024-11-08 14:23:02 UTC",
    "updated_date": "2024-11-08 14:23:02 UTC"
  },
  {
    "arxiv_id": "2411.05586v1",
    "title": "Tangled Program Graphs as an alternative to DRL-based control algorithms for UAVs",
    "authors": [
      "Hubert Szolc",
      "Karol Desnos",
      "Tomasz Kryjak"
    ],
    "abstract": "Deep reinforcement learning (DRL) is currently the most popular AI-based\napproach to autonomous vehicle control. An agent, trained for this purpose in\nsimulation, can interact with the real environment with a human-level\nperformance. Despite very good results in terms of selected metrics, this\napproach has some significant drawbacks: high computational requirements and\nlow explainability. Because of that, a DRL-based agent cannot be used in some\ncontrol tasks, especially when safety is the key issue. Therefore we propose to\nuse Tangled Program Graphs (TPGs) as an alternative for deep reinforcement\nlearning in control-related tasks. In this approach, input signals are\nprocessed by simple programs that are combined in a graph structure. As a\nresult, TPGs are less computationally demanding and their actions can be\nexplained based on the graph structure. In this paper, we present our studies\non the use of TPGs as an alternative for DRL in control-related tasks. In\nparticular, we consider the problem of navigating an unmanned aerial vehicle\n(UAV) through the unknown environment based solely on the on-board LiDAR\nsensor. The results of our work show promising prospects for the use of TPGs in\ncontrol related-tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "The papers was accepted for the 2024 Signal Processing: Algorithms,\n  Architectures, Arrangements, and Applications (SPA) conference in Poznan,\n  Poland",
    "pdf_url": "http://arxiv.org/pdf/2411.05586v1",
    "published_date": "2024-11-08 14:20:29 UTC",
    "updated_date": "2024-11-08 14:20:29 UTC"
  },
  {
    "arxiv_id": "2411.05565v1",
    "title": "Solving 7x7 Killall-Go with Seki Database",
    "authors": [
      "Yun-Jui Tsai",
      "Ting Han Wei",
      "Chi-Huang Lin",
      "Chung-Chin Shih",
      "Hung Guei",
      "I-Chen Wu",
      "Ti-Rong Wu"
    ],
    "abstract": "Game solving is the process of finding the theoretical outcome for a game,\nassuming that all player choices are optimal. This paper focuses on a technique\nthat can reduce the heuristic search space significantly for 7x7 Killall-Go. In\nGo and Killall-Go, live patterns are stones that are protected from opponent\ncapture. Mutual life, also referred to as seki, is when both players' stones\nachieve life by sharing liberties with their opponent. Whichever player\nattempts to capture the opponent first will leave their own stones vulnerable.\nTherefore, it is critical to recognize seki patterns to avoid putting oneself\nin jeopardy. Recognizing seki can reduce the search depth significantly. In\nthis paper, we enumerate all seki patterns up to a predetermined area size,\nthen store these patterns into a seki table. This allows us to recognize seki\nduring search, which significantly improves solving efficiency for the game of\nKillall-Go. Experiments show that a day-long, unsolvable position can be solved\nin 482 seconds with the addition of a seki table. For general positions, a 10%\nto 20% improvement in wall clock time and node count is observed.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by the Computers and Games conference (CG 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.05565v1",
    "published_date": "2024-11-08 13:40:36 UTC",
    "updated_date": "2024-11-08 13:40:36 UTC"
  },
  {
    "arxiv_id": "2411.05564v1",
    "title": "Open-set object detection: towards unified problem formulation and benchmarking",
    "authors": [
      "Hejer Ammar",
      "Nikita Kiselov",
      "Guillaume Lapouge",
      "Romaric Audigier"
    ],
    "abstract": "In real-world applications where confidence is key, like autonomous driving,\nthe accurate detection and appropriate handling of classes differing from those\nused during training are crucial. Despite the proposal of various unknown\nobject detection approaches, we have observed widespread inconsistencies among\nthem regarding the datasets, metrics, and scenarios used, alongside a notable\nabsence of a clear definition for unknown objects, which hampers meaningful\nevaluation. To counter these issues, we introduce two benchmarks: a unified\nVOC-COCO evaluation, and the new OpenImagesRoad benchmark which provides clear\nhierarchical object definition besides new evaluation metrics. Complementing\nthe benchmark, we exploit recent self-supervised Vision Transformers\nperformance, to improve pseudo-labeling-based OpenSet Object Detection (OSOD),\nthrough OW-DETR++. State-of-the-art methods are extensively evaluated on the\nproposed benchmarks. This study provides a clear problem definition, ensures\nconsistent evaluations, and draws new conclusions about effectiveness of OSOD\nstrategies.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV 2024 Workshop: \"The 3rd Workshop for\n  Out-of-Distribution Generalization in Computer Vision Foundation Models\"",
    "pdf_url": "http://arxiv.org/pdf/2411.05564v1",
    "published_date": "2024-11-08 13:40:01 UTC",
    "updated_date": "2024-11-08 13:40:01 UTC"
  },
  {
    "arxiv_id": "2411.05561v1",
    "title": "Training objective drives the consistency of representational similarity across datasets",
    "authors": [
      "Laure Ciernik",
      "Lorenz Linhardt",
      "Marco Morik",
      "Jonas Dippel",
      "Simon Kornblith",
      "Lukas Muttenthaler"
    ],
    "abstract": "The Platonic Representation Hypothesis claims that recent foundation models\nare converging to a shared representation space as a function of their\ndownstream task performance, irrespective of the objectives and data modalities\nused to train these models. Representational similarity is generally measured\nfor individual datasets and is not necessarily consistent across datasets.\nThus, one may wonder whether this convergence of model representations is\nconfounded by the datasets commonly used in machine learning. Here, we propose\na systematic way to measure how representational similarity between models\nvaries with the set of stimuli used to construct the representations. We find\nthat the objective function is the most crucial factor in determining the\nconsistency of representational similarities across datasets. Specifically,\nself-supervised vision models learn representations whose relative pairwise\nsimilarities generalize better from one dataset to another compared to those of\nimage classification or image-text models. Moreover, the correspondence between\nrepresentational similarities and the models' task behavior is\ndataset-dependent, being most strongly pronounced for single-domain datasets.\nOur work provides a framework for systematically measuring similarities of\nmodel representations across datasets and linking those similarities to\ndifferences in task behavior.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.05561v1",
    "published_date": "2024-11-08 13:35:45 UTC",
    "updated_date": "2024-11-08 13:35:45 UTC"
  },
  {
    "arxiv_id": "2411.05557v1",
    "title": "A Nerf-Based Color Consistency Method for Remote Sensing Images",
    "authors": [
      "Zongcheng Zuo",
      "Yuanxiang Li",
      "Tongtong Zhang"
    ],
    "abstract": "Due to different seasons, illumination, and atmospheric conditions, the\nphotometric of the acquired image varies greatly, which leads to obvious\nstitching seams at the edges of the mosaic image. Traditional methods can be\ndivided into two categories, one is absolute radiation correction and the other\nis relative radiation normalization. We propose a NeRF-based method of color\nconsistency correction for multi-view images, which weaves image features\ntogether using implicit expressions, and then re-illuminates feature space to\ngenerate a fusion image with a new perspective. We chose Superview-1 satellite\nimages and UAV images with large range and time difference for the experiment.\nExperimental results show that the synthesize image generated by our method has\nexcellent visual effect and smooth color transition at the edges.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07",
      "I.4.9; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages, 4 figures, The International Geoscience and Remote Sensing\n  Symposium (IGARSS2023)",
    "pdf_url": "http://arxiv.org/pdf/2411.05557v1",
    "published_date": "2024-11-08 13:26:07 UTC",
    "updated_date": "2024-11-08 13:26:07 UTC"
  },
  {
    "arxiv_id": "2411.05540v1",
    "title": "CRepair: CVAE-based Automatic Vulnerability Repair Technology",
    "authors": [
      "Penghui Liu",
      "Yingzhou Bi",
      "Jiangtao Huang",
      "Xinxin Jiang",
      "Lianmei Wang"
    ],
    "abstract": "Software vulnerabilities are flaws in computer software systems that pose\nsignificant threats to the integrity, security, and reliability of modern\nsoftware and its application data. These vulnerabilities can lead to\nsubstantial economic losses across various industries. Manual vulnerability\nrepair is not only time-consuming but also prone to errors. To address the\nchallenges of vulnerability repair, researchers have proposed various\nsolutions, with learning-based automatic vulnerability repair techniques\ngaining widespread attention. However, existing methods often focus on learning\nmore vulnerability data to improve repair outcomes, while neglecting the\ndiverse characteristics of vulnerable code, and suffer from imprecise\nvulnerability localization.To address these shortcomings, this paper proposes\nCRepair, a CVAE-based automatic vulnerability repair technology aimed at fixing\nsecurity vulnerabilities in system code. We first preprocess the vulnerability\ndata using a prompt-based method to serve as input to the model. Then, we apply\ncausal inference techniques to map the vulnerability feature data to\nprobability distributions. By employing multi-sample feature fusion, we capture\ndiverse vulnerability feature information. Finally, conditional control is used\nto guide the model in repairing the vulnerabilities.Experimental results\ndemonstrate that the proposed method significantly outperforms other benchmark\nmodels, achieving a perfect repair rate of 52%. The effectiveness of the\napproach is validated from multiple perspectives, advancing AI-driven code\nvulnerability repair and showing promising applications.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05540v1",
    "published_date": "2024-11-08 12:55:04 UTC",
    "updated_date": "2024-11-08 12:55:04 UTC"
  },
  {
    "arxiv_id": "2411.05892v1",
    "title": "Identifying and Decomposing Compound Ingredients in Meal Plans Using Large Language Models",
    "authors": [
      "Leon Kopitar",
      "Leon Bedrac",
      "Larissa J Strath",
      "Jiang Bian",
      "Gregor Stiglic"
    ],
    "abstract": "This study explores the effectiveness of Large Language Models in meal\nplanning, focusing on their ability to identify and decompose compound\ningredients. We evaluated three models-GPT-4o, Llama-3 (70b), and Mixtral\n(8x7b)-to assess their proficiency in recognizing and breaking down complex\ningredient combinations. Preliminary results indicate that while Llama-3 (70b)\nand GPT-4o excels in accurate decomposition, all models encounter difficulties\nwith identifying essential elements like seasonings and oils. Despite strong\noverall performance, variations in accuracy and completeness were observed\nacross models. These findings underscore LLMs' potential to enhance\npersonalized nutrition but highlight the need for further refinement in\ningredient decomposition. Future research should address these limitations to\nimprove nutritional recommendations and health outcomes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Comments: Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)",
    "pdf_url": "http://arxiv.org/pdf/2411.05892v1",
    "published_date": "2024-11-08 12:38:10 UTC",
    "updated_date": "2024-11-08 12:38:10 UTC"
  },
  {
    "arxiv_id": "2411.05521v2",
    "title": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark",
    "authors": [
      "Sithursan Sivasubramaniam",
      "Cedric Osei-Akoto",
      "Yi Zhang",
      "Kurt Stockinger",
      "Jonathan Fuerst"
    ],
    "abstract": "Electronic health records (EHRs) are stored in various database systems with\ndifferent database models on heterogeneous storage architectures, such as\nrelational databases, document stores, or graph databases. These different\ndatabase models have a big impact on query complexity and performance. While\nthis has been a known fact in database research, its implications for the\ngrowing number of Text-to-Query systems have surprisingly not been investigated\nso far. In this paper, we present SM3-Text-to-Query, the first multi-model\nmedical Text-to-Query benchmark based on synthetic patient data from Synthea,\nfollowing the SNOMED-CT taxonomy -- a widely used knowledge graph ontology\ncovering medical terminology. SM3-Text-to-Query provides data representations\nfor relational databases (PostgreSQL), document stores (MongoDB), and graph\ndatabases (Neo4j and GraphDB (RDF)), allowing the evaluation across four\npopular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically\nand manually develop 408 template questions, which we augment to construct a\nbenchmark of 10K diverse natural language question/query pairs for these four\nquery languages (40K pairs overall). On our dataset, we evaluate several common\nin-context-learning (ICL) approaches for a set of representative closed and\nopen-source LLMs. Our evaluation sheds light on the trade-offs between database\nmodels and query languages for different ICL strategies and LLMs. Last,\nSM3-Text-to-Query is easily extendable to additional query languages or real,\nstandard-based patient databases.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "NeurIPS 2024 Track Datasets and Benchmarks",
    "pdf_url": "http://arxiv.org/pdf/2411.05521v2",
    "published_date": "2024-11-08 12:27:13 UTC",
    "updated_date": "2024-11-14 09:28:49 UTC"
  },
  {
    "arxiv_id": "2411.05514v1",
    "title": "Towards Scalable Foundation Models for Digital Dermatology",
    "authors": [
      "Fabian Gröger",
      "Philippe Gottfrois",
      "Ludovic Amruthalingam",
      "Alvaro Gonzalez-Jimenez",
      "Simone Lionetti",
      "Luis R. Soenksen-Martinez",
      "Alexander A. Navarini",
      "Marc Pouly"
    ],
    "abstract": "The growing demand for accurate and equitable AI models in digital\ndermatology faces a significant challenge: the lack of diverse, high-quality\nlabeled data. In this work, we investigate the potential of domain-specific\nfoundation models for dermatology in addressing this challenge. We utilize\nself-supervised learning (SSL) techniques to pre-train models on a dataset of\nover 240,000 dermatological images from public and private collections. Our\nstudy considers several SSL methods and compares the resulting foundation\nmodels against domain-agnostic models like those pre-trained on ImageNet and\nstate-of-the-art models such as MONET across 12 downstream tasks. Unlike\nprevious research, we emphasize the development of smaller models that are more\nsuitable for resource-limited clinical settings, facilitating easier adaptation\nto a broad range of use cases. Results show that models pre-trained in this\nwork not only outperform general-purpose models but also approach the\nperformance of models 50 times larger on clinically relevant diagnostic tasks.\nTo promote further research in this direction, we publicly release both the\ntraining code and the foundation models, which can benefit clinicians in\ndermatological applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Findings paper presented at Machine Learning for Health (ML4H)\n  symposium 2024, December 15-16, 2024, Vancouver, Canada, 11 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.05514v1",
    "published_date": "2024-11-08 12:19:20 UTC",
    "updated_date": "2024-11-08 12:19:20 UTC"
  },
  {
    "arxiv_id": "2411.05451v1",
    "title": "WorkflowLLM: Enhancing Workflow Orchestration Capability of Large Language Models",
    "authors": [
      "Shengda Fan",
      "Xin Cong",
      "Yuepeng Fu",
      "Zhong Zhang",
      "Shuyan Zhang",
      "Yuanwei Liu",
      "Yesai Wu",
      "Yankai Lin",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have driven a\nrevolutionary paradigm shift in process automation from Robotic Process\nAutomation to Agentic Process Automation by automating the workflow\norchestration procedure based on LLMs. However, existing LLMs (even the\nadvanced OpenAI GPT-4o) are confined to achieving satisfactory capability in\nworkflow orchestration. To address this limitation, we present WorkflowLLM, a\ndata-centric framework elaborately designed to enhance the capability of LLMs\nin workflow orchestration. It first constructs a large-scale fine-tuning\ndataset WorkflowBench with 106,763 samples, covering 1,503 APIs from 83\napplications across 28 categories. Specifically, the construction process can\nbe divided into three phases: (1) Data Collection: we collect real-world\nworkflow data from Apple Shortcuts and RoutineHub, transcribing them into\nPython-style code. We further equip them with generated hierarchical thought\nvia ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task\nqueries to enrich the diversity and complexity of workflows. (3) Workflow\nGeneration: we leverage an annotator model trained on collected data to\ngenerate workflows for synthesized queries. Finally, we merge the synthetic\nsamples that pass quality confirmation with the collected samples to obtain the\nWorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain\nWorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong\ncapacity to orchestrate complex workflows, while also achieving notable\ngeneralization performance on previously unseen APIs. Additionally,\nWorkflowBench exhibits robust zero-shot generalization capabilities on an\nout-of-distribution task planning dataset, T-Eval. Our data and code are\navailable at https://github.com/OpenBMB/WorkflowLLM.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05451v1",
    "published_date": "2024-11-08 09:58:02 UTC",
    "updated_date": "2024-11-08 09:58:02 UTC"
  },
  {
    "arxiv_id": "2411.05424v1",
    "title": "ICE-T: A Multi-Faceted Concept for Teaching Machine Learning",
    "authors": [
      "Hendrik Krone",
      "Pierre Haritz",
      "Thomas Liebig"
    ],
    "abstract": "The topics of Artificial intelligence (AI) and especially Machine Learning\n(ML) are increasingly making their way into educational curricula. To\nfacilitate the access for students, a variety of platforms, visual tools, and\ndigital games are already being used to introduce ML concepts and strengthen\nthe understanding of how AI works. We take a look at didactic principles that\nare employed for teaching computer science, define criteria, and, based on\nthose, evaluate a selection of prominent existing platforms, tools, and games.\nAdditionally, we criticize the approach of portraying ML mostly as a black-box\nand the resulting missing focus on creating an understanding of data,\nalgorithms, and models that come with it. To tackle this issue, we present a\nconcept that covers intermodal transfer, computational and explanatory\nthinking, ICE-T, as an extension of known didactic principles. With our\nmulti-faceted concept, we believe that planners of learning units, creators of\nlearning platforms and educators can improve on teaching ML.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted and presented at the 17th International Conference on\n  Informatics in Schools (ISSEP 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.05424v1",
    "published_date": "2024-11-08 09:16:05 UTC",
    "updated_date": "2024-11-08 09:16:05 UTC"
  },
  {
    "arxiv_id": "2411.05423v1",
    "title": "VISTA: Visual Integrated System for Tailored Automation in Math Problem Generation Using LLM",
    "authors": [
      "Jeongwoo Lee",
      "Kwangsuk Park",
      "Jihyeon Park"
    ],
    "abstract": "Generating accurate and consistent visual aids is a critical challenge in\nmathematics education, where visual representations like geometric shapes and\nfunctions play a pivotal role in enhancing student comprehension. This paper\nintroduces a novel multi-agent framework that leverages Large Language Models\n(LLMs) to automate the creation of complex mathematical visualizations\nalongside coherent problem text. Our approach not only simplifies the\ngeneration of precise visual aids but also aligns these aids with the problem's\ncore mathematical concepts, improving both problem creation and assessment. By\nintegrating multiple agents, each responsible for distinct tasks such as\nnumeric calculation, geometry validation, and visualization, our system\ndelivers mathematically accurate and contextually relevant problems with visual\naids. Evaluation across Geometry and Function problem types shows that our\nmethod significantly outperforms basic LLMs in terms of text coherence,\nconsistency, relevance and similarity, while maintaining the essential\ngeometrical and functional integrity of the original problems. Although some\nchallenges remain in ensuring consistent visual outputs, our framework\ndemonstrates the immense potential of LLMs in transforming the way educators\ngenerate and utilize visual aids in math education.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeurIPS 2024 Workshop on Large Foundation Models for\n  Educational Assessment (FM-Assess)",
    "pdf_url": "http://arxiv.org/pdf/2411.05423v1",
    "published_date": "2024-11-08 09:15:56 UTC",
    "updated_date": "2024-11-08 09:15:56 UTC"
  },
  {
    "arxiv_id": "2411.05421v1",
    "title": "Learning the rules of peptide self-assembly through data mining with large language models",
    "authors": [
      "Zhenze Yang",
      "Sarah K. Yorke",
      "Tuomas P. J. Knowles",
      "Markus J. Buehler"
    ],
    "abstract": "Peptides are ubiquitous and important biologically derived molecules, that\nhave been found to self-assemble to form a wide array of structures. Extensive\nresearch has explored the impacts of both internal chemical composition and\nexternal environmental stimuli on the self-assembly behaviour of these systems.\nHowever, there is yet to be a systematic study that gathers this rich\nliterature data and collectively examines these experimental factors to provide\na global picture of the fundamental rules that govern protein self-assembly\nbehavior. In this work, we curate a peptide assembly database through a\ncombination of manual processing by human experts and literature mining\nfacilitated by a large language model. As a result, we collect more than 1,000\nexperimental data entries with information about peptide sequence, experimental\nconditions and corresponding self-assembly phases. Utilizing the collected\ndata, ML models are trained and evaluated, demonstrating excellent accuracy\n(>80\\%) and efficiency in peptide assembly phase classification. Moreover, we\nfine-tune our GPT model for peptide literature mining with the developed\ndataset, which exhibits markedly superior performance in extracting information\nfrom academic publications relative to the pre-trained model. We find that this\nworkflow can substantially improve efficiency when exploring potential\nself-assembling peptide candidates, through guiding experimental work, while\nalso deepening our understanding of the mechanisms governing peptide\nself-assembly. In doing so, novel structures can be accessed for a range of\napplications including sensing, catalysis and biomaterials.",
    "categories": [
      "cond-mat.soft",
      "cond-mat.dis-nn",
      "cond-mat.mes-hall",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cond-mat.soft",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05421v1",
    "published_date": "2024-11-08 09:14:22 UTC",
    "updated_date": "2024-11-08 09:14:22 UTC"
  },
  {
    "arxiv_id": "2411.05420v2",
    "title": "WeatherGFM: Learning A Weather Generalist Foundation Model via In-context Learning",
    "authors": [
      "Xiangyu Zhao",
      "Zhiwang Zhou",
      "Wenlong Zhang",
      "Yihao Liu",
      "Xiangyu Chen",
      "Junchao Gong",
      "Hao Chen",
      "Ben Fei",
      "Shiqi Chen",
      "Wanli Ouyang",
      "Xiao-Ming Wu",
      "Lei Bai"
    ],
    "abstract": "The Earth's weather system encompasses intricate weather data modalities and\ndiverse weather understanding tasks, which hold significant value to human\nlife. Existing data-driven models focus on single weather understanding tasks\n(e.g., weather forecasting). Although these models have achieved promising\nresults, they fail to tackle various complex tasks within a single and unified\nmodel. Moreover, the paradigm that relies on limited real observations for a\nsingle scenario hinders the model's performance upper bound. In response to\nthese limitations, we draw inspiration from the in-context learning paradigm\nemployed in state-of-the-art visual foundation models and large language\nmodels. In this paper, we introduce the first generalist weather foundation\nmodel (WeatherGFM), designed to address a wide spectrum of weather\nunderstanding tasks in a unified manner. More specifically, we initially unify\nthe representation and definition of the diverse weather understanding tasks.\nSubsequently, we devised weather prompt formats to manage different weather\ndata modalities, namely single, multiple, and temporal modalities. Finally, we\nadopt a visual prompting question-answering paradigm for the training of\nunified weather understanding tasks. Extensive experiments indicate that our\nWeatherGFM can effectively handle up to ten weather understanding tasks,\nincluding weather forecasting, super-resolution, weather image translation, and\npost-processing. Our method also showcases generalization ability on unseen\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05420v2",
    "published_date": "2024-11-08 09:14:19 UTC",
    "updated_date": "2024-12-09 04:25:35 UTC"
  },
  {
    "arxiv_id": "2411.05409v2",
    "title": "Web Archives Metadata Generation with GPT-4o: Challenges and Insights",
    "authors": [
      "Abigail Yongping Huang",
      "Ashwin Nair",
      "Zhen Rong Goh",
      "Tianrui Liu"
    ],
    "abstract": "Current metadata creation for web archives is time consuming and costly due\nto reliance on human effort. This paper explores the use of gpt-4o for metadata\ngeneration within the Web Archive Singapore, focusing on scalability,\nefficiency, and cost effectiveness. We processed 112 Web ARChive (WARC) files\nusing data reduction techniques, achieving a notable 99.9% reduction in\nmetadata generation costs. By prompt engineering, we generated titles and\nabstracts, which were evaluated both intrinsically using Levenshtein Distance\nand BERTScore, and extrinsically with human cataloguers using McNemar's test.\nResults indicate that while our method offers significant cost savings and\nefficiency gains, human curated metadata maintains an edge in quality. The\nstudy identifies key challenges including content inaccuracies, hallucinations,\nand translation issues, suggesting that Large Language Models (LLMs) should\nserve as complements rather than replacements for human cataloguers. Future\nwork will focus on refining prompts, improving content filtering, and\naddressing privacy concerns through experimentation with smaller models. This\nresearch advances the integration of LLMs in web archiving, offering valuable\ninsights into their current capabilities and outlining directions for future\nenhancements. The code is available at\nhttps://github.com/masamune-prog/warc2summary for further development and use\nby institutions facing similar challenges.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05409v2",
    "published_date": "2024-11-08 08:59:40 UTC",
    "updated_date": "2024-11-16 02:27:09 UTC"
  },
  {
    "arxiv_id": "2411.05403v1",
    "title": "Benchmarking Distributional Alignment of Large Language Models",
    "authors": [
      "Nicole Meister",
      "Carlos Guestrin",
      "Tatsunori Hashimoto"
    ],
    "abstract": "Language models (LMs) are increasingly used as simulacra for people, yet\ntheir ability to match the distribution of views of a specific demographic\ngroup and be \\textit{distributionally aligned} remains uncertain. This notion\nof distributional alignment is complex, as there is significant variation in\nthe types of attributes that are simulated. Prior works have underexplored the\nrole of three critical variables -- the question domain, steering method, and\ndistribution expression method -- which motivates our contribution of a\nbenchmark explicitly addressing these dimensions. We construct a dataset\nexpanding beyond political values, create human baselines for this task, and\nevaluate the extent to which an LM can align with a particular group's opinion\ndistribution to inform design choices of such simulation systems. Our analysis\nreveals open problems regarding if, and how, LMs can be used to simulate\nhumans, and that LLMs can more accurately describe the opinion distribution\nthan simulate such distributions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05403v1",
    "published_date": "2024-11-08 08:41:17 UTC",
    "updated_date": "2024-11-08 08:41:17 UTC"
  },
  {
    "arxiv_id": "2411.05384v1",
    "title": "Advancing Meteorological Forecasting: AI-based Approach to Synoptic Weather Map Analysis",
    "authors": [
      "Yo-Hwan Choi",
      "Seon-Yu Kang",
      "Minjong Cheon"
    ],
    "abstract": "As global warming increases the complexity of weather patterns; the precision\nof weather forecasting becomes increasingly important. Our study proposes a\nnovel preprocessing method and convolutional autoencoder model developed to\nimprove the interpretation of synoptic weather maps. These are critical for\nmeteorologists seeking a thorough understanding of weather conditions. This\nmodel could recognize historical synoptic weather maps that nearly match\ncurrent atmospheric conditions, marking a significant step forward in modern\ntechnology in meteorological forecasting. This comprises unsupervised learning\nmodels like VQ-VQE, as well as supervised learning models like VGG16, VGG19,\nXception, InceptionV3, and ResNet50 trained on the ImageNet dataset, as well as\nresearch into newer models like EfficientNet and ConvNeXt. Our findings proved\nthat, while these models perform well in various settings, their ability to\nidentify comparable synoptic weather maps has certain limits. Our research,\nmotivated by the primary goal of significantly increasing meteorologists'\nefficiency in labor-intensive tasks, discovered that cosine similarity is the\nmost effective metric, as determined by a combination of quantitative and\nqualitative assessments to accurately identify relevant historical weather\npatterns. This study broadens our understanding by shifting the emphasis from\nnumerical precision to practical application, ensuring that our model is\neffective in theory practical, and accessible in the complex and dynamic field\nof meteorology.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05384v1",
    "published_date": "2024-11-08 07:46:50 UTC",
    "updated_date": "2024-11-08 07:46:50 UTC"
  },
  {
    "arxiv_id": "2411.05375v1",
    "title": "Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking",
    "authors": [
      "Mubashara Akhtar",
      "Michael Schlichtkrull",
      "Andreas Vlachos"
    ],
    "abstract": "Current automated fact-checking (AFC) approaches commonly evaluate evidence\neither implicitly via the predicted verdicts or by comparing retrieved evidence\nwith a predefined closed knowledge source, such as Wikipedia. However, these\nmethods suffer from limitations, resulting from their reliance on evaluation\nmetrics developed for different purposes and constraints imposed by closed\nknowledge sources. Recent advances in natural language generation (NLG)\nevaluation offer new possibilities for evidence assessment. In this work, we\nintroduce Ev2R, an evaluation framework for AFC that comprises three types of\napproaches for evidence evaluation: reference-based, proxy-reference, and\nreference-less. We evaluate their effectiveness through agreement with human\nratings and adversarial tests, and demonstrate that prompt-based scorers,\nparticularly those leveraging LLMs and reference evidence, outperform\ntraditional evaluation approaches.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.05375v1",
    "published_date": "2024-11-08 07:05:06 UTC",
    "updated_date": "2024-11-08 07:05:06 UTC"
  },
  {
    "arxiv_id": "2411.05359v1",
    "title": "Agricultural Landscape Understanding At Country-Scale",
    "authors": [
      "Radhika Dua",
      "Nikita Saxena",
      "Aditi Agarwal",
      "Alex Wilson",
      "Gaurav Singh",
      "Hoang Tran",
      "Ishan Deshpande",
      "Amandeep Kaur",
      "Gaurav Aggarwal",
      "Chandan Nath",
      "Arnab Basu",
      "Vishal Batchu",
      "Sharath Holla",
      "Bindiya Kurle",
      "Olana Missura",
      "Rahul Aggarwal",
      "Shubhika Garg",
      "Nishi Shah",
      "Avneet Singh",
      "Dinesh Tewari",
      "Agata Dondzik",
      "Bharat Adsul",
      "Milind Sohoni",
      "Asim Rama Praveen",
      "Aaryan Dangi",
      "Lisan Kadivar",
      "E Abhishek",
      "Niranjan Sudhansu",
      "Kamlakar Hattekar",
      "Sameer Datar",
      "Musty Krishna Chaithanya",
      "Anumas Ranjith Reddy",
      "Aashish Kumar",
      "Betala Laxmi Tirumala",
      "Alok Talekar"
    ],
    "abstract": "Agricultural landscapes are quite complex, especially in the Global South\nwhere fields are smaller, and agricultural practices are more varied. In this\npaper we report on our progress in digitizing the agricultural landscape\n(natural and man-made) in our study region of India. We use high resolution\nimagery and a UNet style segmentation model to generate the first of its kind\nnational-scale multi-class panoptic segmentation output. Through this work we\nhave been able to identify individual fields across 151.7M hectares, and\ndelineating key features such as water resources and vegetation. We share how\nthis output was validated by our team and externally by downstream users,\nincluding some sample use cases that can lead to targeted data driven decision\nmaking. We believe this dataset will contribute towards digitizing agriculture\nby generating the foundational baselayer.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "34 pages, 7 tables, 15 figs",
    "pdf_url": "http://arxiv.org/pdf/2411.05359v1",
    "published_date": "2024-11-08 06:29:02 UTC",
    "updated_date": "2024-11-08 06:29:02 UTC"
  },
  {
    "arxiv_id": "2411.05353v1",
    "title": "Controlling Grokking with Nonlinearity and Data Symmetry",
    "authors": [
      "Ahmed Salah",
      "David Yevick"
    ],
    "abstract": "This paper demonstrates that grokking behavior in modular arithmetic with a\nmodulus P in a neural network can be controlled by modifying the profile of the\nactivation function as well as the depth and width of the model. Plotting the\neven PCA projections of the weights of the last NN layer against their odd\nprojections further yields patterns which become significantly more uniform\nwhen the nonlinearity is increased by incrementing the number of layers. These\npatterns can be employed to factor P when P is nonprime. Finally, a metric for\nthe generalization ability of the network is inferred from the entropy of the\nlayer weights while the degree of nonlinearity is related to correlations\nbetween the local entropy of the weights of the neurons in the final layer.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.05353v1",
    "published_date": "2024-11-08 06:19:29 UTC",
    "updated_date": "2024-11-08 06:19:29 UTC"
  },
  {
    "arxiv_id": "2411.05349v1",
    "title": "Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework",
    "authors": [
      "Honghao Shi",
      "Longkai Cheng",
      "Wenli Wu",
      "Yuhang Wang",
      "Xuan Liu",
      "Shaokai Nie",
      "Weixv Wang",
      "Xuebin Min",
      "Chunlei Men",
      "Yonghua Lin"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) and related technologies\nsuch as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have\nenabled the creation of autonomous intelligent systems capable of performing\ncluster diagnostics and troubleshooting. By integrating these technologies with\nself-play methodologies, we have developed an LLM-agent system designed to\nautonomously diagnose and resolve issues within AI clusters. Our innovations\ninclude a knowledge base tailored for cluster diagnostics, enhanced LLM\nalgorithms, practical deployment strategies for agents, and a benchmark\nspecifically designed for evaluating LLM capabilities in this domain. Through\nextensive experimentation across multiple dimensions, we have demonstrated the\nsuperiority of our system in addressing the challenges faced in cluster\ndiagnostics, particularly in detecting and rectifying performance issues more\nefficiently and accurately than traditional methods.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "68T42"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.05349v1",
    "published_date": "2024-11-08 06:12:56 UTC",
    "updated_date": "2024-11-08 06:12:56 UTC"
  },
  {
    "arxiv_id": "2411.05348v2",
    "title": "LLM-PySC2: Starcraft II learning environment for Large Language Models",
    "authors": [
      "Zongyuan Li",
      "Yanan Ni",
      "Runnan Qi",
      "Lumin Jiang",
      "Chang Lu",
      "Xiaojie Xu",
      "Xiangbei Liu",
      "Pengfei Li",
      "Yunzheng Guo",
      "Zhe Ma",
      "Huanyu Li",
      "Hui Wu",
      "Xian Guo",
      "Kuihua Huang",
      "Xuebo Zhang"
    ],
    "abstract": "The tremendous potential has been demonstrated by large language models\n(LLMs) in intelligent decision-making problems, with unprecedented capabilities\nshown across diverse applications ranging from gaming AI systems to complex\nstrategic planning frameworks. However, the StarCraft II platform, which has\nbeen widely adopted for validating decision-making algorithms in the past\ndecade, has not yet provided substantial support for this emerging domain. To\naddress issues that LLMs cannot interface with the hundreds of actions of the\npysc2 backend and the lack of native support for multi-agent (MA)\ncollaboration, we propose the LLM-PySC2 environment. This is the first\nenvironment that offers LLMs the complete pysc2 action space with sufficient\nmulti-modal information and game Wiki knowledge. With an asynchronous query\narchitecture, the environment efficiently interacts with LLMs that maintain a\nconstant latency regardless of the scale of the agents' population. In the\nexperiments, we evaluated LLMs' decision-making performance in both the\nmacro-decision and micro-operation scenarios, with traditional StarCraft II\nMulti-Agent Challenge (SMAC) tasks and a series of new proposed. Results\nindicate that LLMs possess the potential to achieve victories in complex\nscenarios but cannot constantly generate correct decisions, especially in the\nrecovered pysc2 action space and MA settings. Without task-relevant\ninstructions, the pre-trained models suffer from issues such as hallucinations\nand inefficient collaboration. Our findings suggest that StarCraft II still\nchallenges in the era of large models, revealing that there is a lot to do to\ndevelop an advanced LLM decision-making system, and the proposed LLM-PySC2\nenvironment will support future development of LLM-based decision-making\nsolutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05348v2",
    "published_date": "2024-11-08 06:04:22 UTC",
    "updated_date": "2025-05-02 07:20:36 UTC"
  },
  {
    "arxiv_id": "2411.05345v1",
    "title": "Reasoning Robustness of LLMs to Adversarial Typographical Errors",
    "authors": [
      "Esther Gan",
      "Yiran Zhao",
      "Liying Cheng",
      "Yancan Mao",
      "Anirudh Goyal",
      "Kenji Kawaguchi",
      "Min-Yen Kan",
      "Michael Shieh"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by\nusers' instruction. In this work, we study the reasoning robustness of LLMs to\ntypographical errors, which can naturally occur in users' queries. We design an\nAdversarial Typo Attack ($\\texttt{ATA}$) algorithm that iteratively samples\ntypos for words that are important to the query and selects the edit that is\nmost likely to succeed in attacking. It shows that LLMs are sensitive to\nminimal adversarial typographical changes. Notably, with 1 character edit,\nMistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K, while with 8\ncharacter edits the performance further drops to 19.2%. To extend our\nevaluation to larger and closed-source LLMs, we develop the $\\texttt{R$^2$ATA}$\nbenchmark, which assesses models' $\\underline{R}$easoning\n$\\underline{R}$obustness to $\\underline{\\texttt{ATA}}$. It includes adversarial\ntypographical questions derived from three widely used reasoning\ndatasets-GSM8K, BBH, and MMLU-by applying $\\texttt{ATA}$ to open-source LLMs.\n$\\texttt{R$^2$ATA}$ demonstrates remarkable transferability and causes notable\nperformance drops across multiple super large and closed-source LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05345v1",
    "published_date": "2024-11-08 05:54:05 UTC",
    "updated_date": "2024-11-08 05:54:05 UTC"
  },
  {
    "arxiv_id": "2411.05340v1",
    "title": "Improving Multi-Domain Task-Oriented Dialogue System with Offline Reinforcement Learning",
    "authors": [
      "Dharmendra Prajapat",
      "Durga Toshniwal"
    ],
    "abstract": "Task-oriented dialogue (TOD) system is designed to accomplish user-defined\ntasks through dialogues. The TOD system has progressed towards end-to-end\nmodeling by leveraging pre-trained large language models. Fine-tuning the\npre-trained language models using only supervised learning leads to the\nexposure bias and token loss problem and it deviates the models from completing\nthe user's task. To address these issues, we propose a TOD system that\nleverages a unified pre-trained language model, GPT2, as a base model. It is\noptimized using supervised learning and reinforcement learning (RL). The issues\nin the TOD system are mitigated using a non-differentiable reward function. The\nreward is calculated using the weighted sum of the success rate and BLEU\nevaluation metrics. The success rate and BLEU metrics in reward calculation\nguide the language model for user task completion while ensuring a coherent and\nfluent response. Our model is acquired by fine-tuning a pre-trained model on\nthe dialogue-session level which comprises user utterance, belief state, system\nact, and system response. Experimental results on MultiWOZ2.1 demonstrate that\nour model increases the inform rate by 1.60% and the success rate by 3.17%\ncompared to the baseline.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05340v1",
    "published_date": "2024-11-08 05:43:40 UTC",
    "updated_date": "2024-11-08 05:43:40 UTC"
  },
  {
    "arxiv_id": "2411.05880v1",
    "title": "Towards Equitable ASD Diagnostics: A Comparative Study of Machine and Deep Learning Models Using Behavioral and Facial Data",
    "authors": [
      "Mohammed Aledhari",
      "Mohamed Rahouti",
      "Ali Alfatemi"
    ],
    "abstract": "Autism Spectrum Disorder (ASD) is often underdiagnosed in females due to\ngender-specific symptom differences overlooked by conventional diagnostics.\nThis study evaluates machine learning models, particularly Random Forest and\nconvolutional neural networks, for enhancing ASD diagnosis through structured\ndata and facial image analysis. Random Forest achieved 100% validation accuracy\nacross datasets, highlighting its ability to manage complex relationships and\nreduce false negatives, which is crucial for early intervention and addressing\ngender biases. In image-based analysis, MobileNet outperformed the baseline\nCNN, achieving 87% accuracy, though a 30% validation loss suggests possible\noverfitting, requiring further optimization for robustness in clinical\nsettings. Future work will emphasize hyperparameter tuning, regularization, and\ntransfer learning. Integrating behavioral data with facial analysis could\nimprove diagnosis for underdiagnosed groups. These findings suggest Random\nForest's high accuracy and balanced precision-recall metrics could enhance\nclinical workflows. MobileNet's lightweight structure also shows promise for\nresource-limited environments, enabling accessible ASD screening. Addressing\nmodel explainability and clinician trust will be vital.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05880v1",
    "published_date": "2024-11-08 05:26:04 UTC",
    "updated_date": "2024-11-08 05:26:04 UTC"
  },
  {
    "arxiv_id": "2411.05330v1",
    "title": "Inversion-based Latent Bayesian Optimization",
    "authors": [
      "Jaewon Chu",
      "Jinyoung Park",
      "Seunghun Lee",
      "Hyunwoo J. Kim"
    ],
    "abstract": "Latent Bayesian optimization (LBO) approaches have successfully adopted\nBayesian optimization over a continuous latent space by employing an\nencoder-decoder architecture to address the challenge of optimization in a high\ndimensional or discrete input space. LBO learns a surrogate model to\napproximate the black-box objective function in the latent space. However, we\nobserved that most LBO methods suffer from the `misalignment problem`, which is\ninduced by the reconstruction error of the encoder-decoder architecture. It\nhinders learning an accurate surrogate model and generating high-quality\nsolutions. In addition, several trust region-based LBO methods select the\nanchor, the center of the trust region, based solely on the objective function\nvalue without considering the trust region`s potential to enhance the\noptimization process. To address these issues, we propose Inversion-based\nLatent Bayesian Optimization (InvBO), a plug-and-play module for LBO. InvBO\nconsists of two components: an inversion method and a potential-aware trust\nregion anchor selection. The inversion method searches the latent code that\ncompletely reconstructs the given target data. The potential-aware trust region\nanchor selection considers the potential capability of the trust region for\nbetter local optimization. Experimental results demonstrate the effectiveness\nof InvBO on nine real-world benchmarks, such as molecule design and arithmetic\nexpression fitting tasks. Code is available at https://github.com/mlvlab/InvBO.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.05330v1",
    "published_date": "2024-11-08 05:06:47 UTC",
    "updated_date": "2024-11-08 05:06:47 UTC"
  },
  {
    "arxiv_id": "2411.05316v2",
    "title": "Aligning Large Language Models and Geometric Deep Models for Protein Representation",
    "authors": [
      "Dong Shu",
      "Bingbing Duan",
      "Kai Guo",
      "Kaixiong Zhou",
      "Jiliang Tang",
      "Mengnan Du"
    ],
    "abstract": "Latent representation alignment has become a foundational technique for\nconstructing multimodal large language models (MLLM) by mapping embeddings from\ndifferent modalities into a shared space, often aligned with the embedding\nspace of large language models (LLMs) to enable effective cross-modal\nunderstanding. While preliminary protein-focused MLLMs have emerged, they have\npredominantly relied on heuristic approaches, lacking a fundamental\nunderstanding of optimal alignment practices across representations. In this\nstudy, we explore the alignment of multimodal representations between LLMs and\nGeometric Deep Models (GDMs) in the protein domain. We comprehensively evaluate\nthree state-of-the-art LLMs (Gemma2-2B, LLaMa3.1-8B, and LLaMa3.1-70B) with\nfour protein-specialized GDMs (GearNet, GVP, ScanNet, GAT). Our work examines\nalignment factors from both model and protein perspectives, identifying\nchallenges in current alignment methodologies and proposing strategies to\nimprove the alignment process. Our key findings reveal that GDMs incorporating\nboth graph and 3D structural information align better with LLMs, larger LLMs\ndemonstrate improved alignment capabilities, and protein rarity significantly\nimpacts alignment performance. We also find that increasing GDM embedding\ndimensions, using two-layer projection heads, and fine-tuning LLMs on\nprotein-specific data substantially enhance alignment quality. These strategies\noffer potential enhancements to the performance of protein-related multimodal\nmodels. Our code and data are available at\nhttps://github.com/Tizzzzy/LLM-GDM-alignment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "37 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.05316v2",
    "published_date": "2024-11-08 04:15:08 UTC",
    "updated_date": "2025-03-04 22:22:33 UTC"
  },
  {
    "arxiv_id": "2411.05307v1",
    "title": "Revisiting Network Perturbation for Semi-Supervised Semantic Segmentation",
    "authors": [
      "Sien Li",
      "Tao Wang",
      "Ruizhe Hu",
      "Wenxi Liu"
    ],
    "abstract": "In semi-supervised semantic segmentation (SSS), weak-to-strong consistency\nregularization techniques are widely utilized in recent works, typically\ncombined with input-level and feature-level perturbations. However, the\nintegration between weak-to-strong consistency regularization and network\nperturbation has been relatively rare. We note several problems with existing\nnetwork perturbations in SSS that may contribute to this phenomenon. By\nrevisiting network perturbations, we introduce a new approach for network\nperturbation to expand the existing weak-to-strong consistency regularization\nfor unlabeled data. Additionally, we present a volatile learning process for\nlabeled data, which is uncommon in existing research. Building upon previous\nwork that includes input-level and feature-level perturbations, we present\nMLPMatch (Multi-Level-Perturbation Match), an easy-to-implement and efficient\nframework for semi-supervised semantic segmentation. MLPMatch has been\nvalidated on the Pascal VOC and Cityscapes datasets, achieving state-of-the-art\nperformance. Code is available from https://github.com/LlistenL/MLPMatch.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by PRCV2024",
    "pdf_url": "http://arxiv.org/pdf/2411.05307v1",
    "published_date": "2024-11-08 03:23:39 UTC",
    "updated_date": "2024-11-08 03:23:39 UTC"
  },
  {
    "arxiv_id": "2411.05296v1",
    "title": "On Training of Kolmogorov-Arnold Networks",
    "authors": [
      "Shairoz Sohail"
    ],
    "abstract": "Kolmogorov-Arnold Networks have recently been introduced as a flexible\nalternative to multi-layer Perceptron architectures. In this paper, we examine\nthe training dynamics of different KAN architectures and compare them with\ncorresponding MLP formulations. We train with a variety of different\ninitialization schemes, optimizers, and learning rates, as well as utilize back\npropagation free approaches like the HSIC Bottleneck. We find that (when judged\nby test accuracy) KANs are an effective alternative to MLP architectures on\nhigh-dimensional datasets and have somewhat better parameter efficiency, but\nsuffer from more unstable training dynamics. Finally, we provide\nrecommendations for improving training stability of larger KAN models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.05296v1",
    "published_date": "2024-11-08 02:57:59 UTC",
    "updated_date": "2024-11-08 02:57:59 UTC"
  },
  {
    "arxiv_id": "2411.05292v1",
    "title": "SimpleBEV: Improved LiDAR-Camera Fusion Architecture for 3D Object Detection",
    "authors": [
      "Yun Zhao",
      "Zhan Gong",
      "Peiru Zheng",
      "Hong Zhu",
      "Shaohua Wu"
    ],
    "abstract": "More and more research works fuse the LiDAR and camera information to improve\nthe 3D object detection of the autonomous driving system. Recently, a simple\nyet effective fusion framework has achieved an excellent detection performance,\nfusing the LiDAR and camera features in a unified bird's-eye-view (BEV) space.\nIn this paper, we propose a LiDAR-camera fusion framework, named SimpleBEV, for\naccurate 3D object detection, which follows the BEV-based fusion framework and\nimproves the camera and LiDAR encoders, respectively. Specifically, we perform\nthe camera-based depth estimation using a cascade network and rectify the depth\nresults with the depth information derived from the LiDAR points. Meanwhile, an\nauxiliary branch that implements the 3D object detection using only the\ncamera-BEV features is introduced to exploit the camera information during the\ntraining phase. Besides, we improve the LiDAR feature extractor by fusing the\nmulti-scaled sparse convolutional features. Experimental results demonstrate\nthe effectiveness of our proposed method. Our method achieves 77.6\\% NDS\naccuracy on the nuScenes dataset, showcasing superior performance in the 3D\nobject detection track.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05292v1",
    "published_date": "2024-11-08 02:51:39 UTC",
    "updated_date": "2024-11-08 02:51:39 UTC"
  },
  {
    "arxiv_id": "2411.05289v1",
    "title": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding",
    "authors": [
      "Ryan Sun",
      "Tianyi Zhou",
      "Xun Chen",
      "Lichao Sun"
    ],
    "abstract": "Large Language Models (LLMs) have become essential in advancing natural\nlanguage processing (NLP) tasks, but their sequential token generation limits\ninference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising\nsolution by using a smaller draft model to generate multiple token sequences,\nwhich the target LLM verifies in parallel. However, current heuristic\napproaches, such as Recursive Rejection Sampling (RRS), suffer from low\nacceptance rates in subsequent drafts, limiting the advantages of using\nmultiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can\ntheoretically improve acceptance rates, but its computational cost is too high\nfor real-time use. We present SpecHub, a novel, efficient sampling-verification\nmethod for MDSD that improves acceptance rates with only linear computational\noverhead. By simplifying the OTM problem into a compact Linear Programming\nmodel, SpecHub significantly reduces computational complexity. It further\naccelerates sampling by leveraging a sparse joint distribution, focusing\ncomputation on high-probability token sequences. In extensive experiments,\nSpechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step\nthan RRS and RRS without replacement. We attach our code at\n\\url{https://github.com/MasterGodzilla/Speculative_decoding_OT}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 (Main)",
    "pdf_url": "http://arxiv.org/pdf/2411.05289v1",
    "published_date": "2024-11-08 02:47:07 UTC",
    "updated_date": "2024-11-08 02:47:07 UTC"
  },
  {
    "arxiv_id": "2411.05285v2",
    "title": "AgentOps: Enabling Observability of LLM Agents",
    "authors": [
      "Liming Dong",
      "Qinghua Lu",
      "Liming Zhu"
    ],
    "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities\nacross various domains, gaining extensive attention from academia and industry.\nHowever, these agents raise significant concerns on AI safety due to their\nautonomous and non-deterministic behavior, as well as continuous evolving\nnature . From a DevOps perspective, enabling observability in agents is\nnecessary to ensuring AI safety, as stakeholders can gain insights into the\nagents' inner workings, allowing them to proactively understand the agents,\ndetect anomalies, and prevent potential failures. Therefore, in this paper, we\npresent a comprehensive taxonomy of AgentOps, identifying the artifacts and\nassociated data that should be traced throughout the entire lifecycle of agents\nto achieve effective observability. The taxonomy is developed based on a\nsystematic mapping study of existing AgentOps tools. Our taxonomy serves as a\nreference template for developers to design and implement AgentOps\ninfrastructure that supports monitoring, logging, and analytics. thereby\nensuring AI safety.",
    "categories": [
      "cs.AI",
      "cs.SE",
      "D.2.7; D.2.9; D.2.11"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.05285v2",
    "published_date": "2024-11-08 02:31:03 UTC",
    "updated_date": "2024-11-30 02:55:48 UTC"
  },
  {
    "arxiv_id": "2411.05282v4",
    "title": "MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization",
    "authors": [
      "Akshat Ramachandran",
      "Souvik Kundu",
      "Tushar Krishna"
    ],
    "abstract": "Quantization of foundational models (FMs) is significantly more challenging\nthan traditional DNNs due to the emergence of large magnitude values called\noutliers. Existing outlier-aware algorithm-architecture co-design techniques\neither use mixed-precision, retaining outliers at high precision but compromise\nhardware efficiency, or quantize inliers and outliers at the same precision,\nimproving hardware efficiency at the cost of accuracy. To address this mutual\nexclusivity, we propose MicroScopiQ, a novel co-design technique that leverages\npruning to complement outlier-aware quantization. MicroScopiQ retains outliers\nat higher precision while pruning a certain fraction of least important weights\nto distribute the additional outlier bits; ensuring high accuracy, aligned\nmemory and hardware efficiency. We design a high-throughput, low overhead\naccelerator architecture composed of multi-precision INT processing elements\nand a network-on-chip called ReCoN that efficiently abstracts the complexity of\nsupporting high-precision outliers. Additionally, unlike prior techniques,\nMicroScopiQ does not assume any locality of outlier weights, enabling\napplicability to a broad range of FMs. Extensive experiments across diverse\nquantization settings demonstrate that MicroScopiQ achieves state-of-the-art\nquantization accuracy, while delivering up to 3x faster inference and 2x lower\nenergy consumption compared to existing alternatives. Code is available at:\nhttps://github.com/georgia-tech-synergy-lab/MicroScopiQ-LLM-Quantization",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "ISCA 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.05282v4",
    "published_date": "2024-11-08 02:25:45 UTC",
    "updated_date": "2025-04-29 18:38:03 UTC"
  },
  {
    "arxiv_id": "2411.05281v3",
    "title": "Fox-1: Open Small Language Model for Cloud and Edge",
    "authors": [
      "Zijian Hu",
      "Jipeng Zhang",
      "Rui Pan",
      "Zhaozhuo Xu",
      "Shanshan Han",
      "Han Jin",
      "Alay Dilipbhai Shah",
      "Dimitris Stripelis",
      "Yuhang Yao",
      "Salman Avestimehr",
      "Tong Zhang",
      "Chaoyang He"
    ],
    "abstract": "We present Fox-1, a series of small language models (SLMs) consisting of\nFox-1-1.6B and Fox-1-1.6B-Instruct-v0.1. These models are pre-trained on 3\ntrillion tokens of web-scraped document data and fine-tuned with 5 billion\ntokens of instruction-following and multi-turn conversation data. Aiming to\nimprove the pre-training efficiency, Fox-1-1.6B model introduces a novel\n3-stage data curriculum across all the training data with 2K-8K sequence\nlength. In architecture design, Fox-1 features a deeper layer structure, an\nexpanded vocabulary, and utilizes Grouped Query Attention (GQA), offering a\nperformant and efficient architecture compared to other SLMs. Fox-1 achieves\nbetter or on-par performance in various benchmarks compared to StableLM-2-1.6B,\nGemma-2B, Qwen1.5-1.8B, and OpenELM1.1B, with competitive inference speed and\nthroughput. The model weights have been released under the Apache 2.0 license,\nwhere we aim to promote the democratization of LLMs and make them fully\naccessible to the whole open-source community.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Base model is available at\n  https://huggingface.co/tensoropera/Fox-1-1.6B and the instruction-tuned\n  version is available at\n  https://huggingface.co/tensoropera/Fox-1-1.6B-Instruct-v0.1",
    "pdf_url": "http://arxiv.org/pdf/2411.05281v3",
    "published_date": "2024-11-08 02:24:29 UTC",
    "updated_date": "2025-04-08 01:39:22 UTC"
  },
  {
    "arxiv_id": "2411.05273v1",
    "title": "Real-World Offline Reinforcement Learning from Vision Language Model Feedback",
    "authors": [
      "Sreyas Venkataraman",
      "Yufei Wang",
      "Ziyu Wang",
      "Zackory Erickson",
      "David Held"
    ],
    "abstract": "Offline reinforcement learning can enable policy learning from pre-collected,\nsub-optimal datasets without online interactions. This makes it ideal for\nreal-world robots and safety-critical scenarios, where collecting online data\nor expert demonstrations is slow, costly, and risky. However, most existing\noffline RL works assume the dataset is already labeled with the task rewards, a\nprocess that often requires significant human effort, especially when\nground-truth states are hard to ascertain (e.g., in the real-world). In this\npaper, we build on prior work, specifically RL-VLM-F, and propose a novel\nsystem that automatically generates reward labels for offline datasets using\npreference feedback from a vision-language model and a text description of the\ntask. Our method then learns a policy using offline RL with the reward-labeled\ndataset. We demonstrate the system's applicability to a complex real-world\nrobot-assisted dressing task, where we first learn a reward function using a\nvision-language model on a sub-optimal offline dataset, and then we use the\nlearned reward to employ Implicit Q learning to develop an effective dressing\npolicy. Our method also performs well in simulation tasks involving the\nmanipulation of rigid and deformable objects, and significantly outperform\nbaselines such as behavior cloning and inverse RL. In summary, we propose a new\nsystem that enables automatic reward labeling and policy learning from\nunlabeled, sub-optimal offline datasets.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages. Accepted at the LangRob Workshop 2024 @ CoRL, 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.05273v1",
    "published_date": "2024-11-08 02:12:34 UTC",
    "updated_date": "2024-11-08 02:12:34 UTC"
  },
  {
    "arxiv_id": "2411.05270v1",
    "title": "Seeing Through the Fog: A Cost-Effectiveness Analysis of Hallucination Detection Systems",
    "authors": [
      "Alexander Thomas",
      "Seth Rosen",
      "Vishnu Vettrivel"
    ],
    "abstract": "This paper presents a comparative analysis of hallucination detection systems\nfor AI, focusing on automatic summarization and question answering tasks for\nLarge Language Models (LLMs). We evaluate different hallucination detection\nsystems using the diagnostic odds ratio (DOR) and cost-effectiveness metrics.\nOur results indicate that although advanced models can perform better they come\nat a much higher cost. We also demonstrate how an ideal hallucination detection\nsystem needs to maintain performance across different model sizes. Our findings\nhighlight the importance of choosing a detection system aligned with specific\napplication needs and resource constraints. Future research will explore hybrid\nsystems and automated identification of underperforming components to enhance\nAI reliability and efficiency in detecting and mitigating hallucinations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pags, 13 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.05270v1",
    "published_date": "2024-11-08 02:06:41 UTC",
    "updated_date": "2024-11-08 02:06:41 UTC"
  },
  {
    "arxiv_id": "2411.05263v1",
    "title": "Minimal Conditions for Beneficial Neighbourhood Search and Local Descent",
    "authors": [
      "Mark G. Wallace"
    ],
    "abstract": "This paper investigates what properties a neighbourhood requires to support\nbeneficial local search. We show that neighbourhood locality, and a reduction\nin cost probability towards the optimum, support a proof that search among\nneighbours is more likely to find an improving solution in a single search step\nthan blind search. This is the first paper to introduce such a proof. The\nconcepts underlying these properties are illustrated on a satisfiability\nproblem class, and on travelling salesman problems. Secondly, for a given cost\ntarget t, we investigate a combination of blind search and local descent termed\nlocal blind descent, and present various conditions under which the expected\nnumber of steps to reach a cost better than t using local blind descent, is\nproven to be smaller than with blind search. Experiments indicate that local\nblind descent, given target cost t, should switch to local descent at a\nstarting cost that reduces as t approaches the optimum.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05263v1",
    "published_date": "2024-11-08 01:47:40 UTC",
    "updated_date": "2024-11-08 01:47:40 UTC"
  },
  {
    "arxiv_id": "2411.05261v2",
    "title": "Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation",
    "authors": [
      "Yingying Fang",
      "Zihao Jin",
      "Shaojie Guo",
      "Jinda Liu",
      "Zhiling Yue",
      "Yijian Gao",
      "Junzhi Ning",
      "Zhi Li",
      "Simon Walsh",
      "Guang Yang"
    ],
    "abstract": "Despite significant advancements in automated report generation, the\nopaqueness of text interpretability continues to cast doubt on the reliability\nof the content produced. This paper introduces a novel approach to identify\nspecific image features in X-ray images that influence the outputs of report\ngeneration models. Specifically, we propose Cyclic Vision-Language Manipulator\nCVLM, a module to generate a manipulated X-ray from an original X-ray and its\nreport from a designated report generator. The essence of CVLM is that cycling\nmanipulated X-rays to the report generator produces altered reports aligned\nwith the alterations pre-injected into the reports for X-ray generation,\nachieving the term \"cyclic manipulation\". This process allows direct comparison\nbetween original and manipulated X-rays, clarifying the critical image features\ndriving changes in reports and enabling model users to assess the reliability\nof the generated texts. Empirical evaluations demonstrate that CVLM can\nidentify more precise and reliable features compared to existing explanation\nmethods, significantly enhancing the transparency and applicability of\nAI-generated reports.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05261v2",
    "published_date": "2024-11-08 01:46:11 UTC",
    "updated_date": "2025-05-07 01:51:52 UTC"
  },
  {
    "arxiv_id": "2411.05260v1",
    "title": "QuanCrypt-FL: Quantized Homomorphic Encryption with Pruning for Secure Federated Learning",
    "authors": [
      "Md Jueal Mia",
      "M. Hadi Amini"
    ],
    "abstract": "Federated Learning has emerged as a leading approach for decentralized\nmachine learning, enabling multiple clients to collaboratively train a shared\nmodel without exchanging private data. While FL enhances data privacy, it\nremains vulnerable to inference attacks, such as gradient inversion and\nmembership inference, during both training and inference phases. Homomorphic\nEncryption provides a promising solution by encrypting model updates to protect\nagainst such attacks, but it introduces substantial communication overhead,\nslowing down training and increasing computational costs. To address these\nchallenges, we propose QuanCrypt-FL, a novel algorithm that combines low-bit\nquantization and pruning techniques to enhance protection against attacks while\nsignificantly reducing computational costs during training. Further, we propose\nand implement mean-based clipping to mitigate quantization overflow or errors.\nBy integrating these methods, QuanCrypt-FL creates a communication-efficient FL\nframework that ensures privacy protection with minimal impact on model\naccuracy, thereby improving both computational efficiency and attack\nresilience. We validate our approach on MNIST, CIFAR-10, and CIFAR-100\ndatasets, demonstrating superior performance compared to state-of-the-art\nmethods. QuanCrypt-FL consistently outperforms existing method and matches\nVanilla-FL in terms of accuracy across varying client. Further, QuanCrypt-FL\nachieves up to 9x faster encryption, 16x faster decryption, and 1.5x faster\ninference compared to BatchCrypt, with training time reduced by up to 3x.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05260v1",
    "published_date": "2024-11-08 01:46:00 UTC",
    "updated_date": "2024-11-08 01:46:00 UTC"
  },
  {
    "arxiv_id": "2411.05877v1",
    "title": "Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass",
    "authors": [
      "Tong Chen",
      "Hao Fang",
      "Patrick Xia",
      "Xiaodong Liu",
      "Benjamin Van Durme",
      "Luke Zettlemoyer",
      "Jianfeng Gao",
      "Hao Cheng"
    ],
    "abstract": "Large language models (LMs) are typically adapted to improve performance on\nnew contexts (\\eg text prompts that define new tasks or domains) through\nfine-tuning or prompting. However, there is an accuracy compute tradeoff --\nfine-tuning incurs significant training cost and prompting increases inference\noverhead. We introduce $GenerativeAdapter$, an effective and efficient\nadaptation method that directly maps new contexts to low-rank LM adapters,\nthereby significantly reducing inference overhead with no need for finetuning.\nThe adapter generator is trained via self-supervised learning, and can be used\nto adapt a single frozen LM for any new task simply by mapping the associated\ntask or domain context to a new adapter. We apply $GenerativeAdapter$ to two\npretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the\nadapted models in three adaption scenarios: knowledge acquisition from\ndocuments, learning from demonstrations, and personalization for users. In\nStreamingQA, our approach is effective in injecting knowledge into the LM's\nparameters, achieving a 63.5% improvement in F1 score over the model with\nsupervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K\ntokens. In the MetaICL in-context learning evaluation, our method achieves an\naverage accuracy of $44.9$ across 26 tasks, outperforming the base model. On\nMSC, our method proves to be highly competitive in memorizing user information\nfrom conversations with a 4x reduction in computation and memory costs compared\nto prompting with full conversation history. Together, these results suggest\nthat $GenerativeAdapter$ should allow for general adaption to a wide range of\ndifferent contexts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.05877v1",
    "published_date": "2024-11-08 00:42:47 UTC",
    "updated_date": "2024-11-08 00:42:47 UTC"
  }
]
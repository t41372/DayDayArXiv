{
  "date": "2024-11-08",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-11-08 的 arXiv 中文 TLDR 快报！今天的论文主要聚焦于 AI 模型优化、多模态学习和实际应用，如 LLM 在说服、游戏理论和医疗中的创新进展，强调了大型语言模型的鲁棒性和扩展性；令人印象深刻的文章包括 Qwen2.5-32B 的多模态小语言模型，以及知名学者如 Murray Campbell 在 AI 量化方面的贡献。\n\n### 重点论文：LLM 和 AI 模型创新\n这些论文探讨了大型语言模型（LLM）的核心能力提升和应用，相关主题放在一起讨论，因为它们直接推动了 AI 的前沿发展。\n\n- **The Dark Patterns of Personalized Persuasion in Large Language Models（个性化说服在大型语言模型中的黑暗模式）**  \n  这篇论文揭示了 LLM 如何通过语言特征（如焦虑相关词汇）针对 Big Five 个性特质调整输出，核心贡献是分析 19 个 LLM 模型，证明它们能生成个性化说服内容，但也可能影响用户心理健康。\n\n- **Game-theoretic LLM: Agent Workflow for Negotiation Games（基于游戏理论的 LLM：用于谈判游戏的代理工作流）**  \n  作者包括 William Wang 和 Yongfeng Zhang 等知名学者，论文评估 LLM 在完整和不完整信息游戏中的理性决策，引入游戏理论工作流提升 Nash 均衡计算，关键发现是工作流显著改善了 LLM 的策略选择和谈判鲁棒性。\n\n- **FactLens: Benchmarking Fine-Grained Fact Verification（FactLens：细粒度事实验证基准）**  \n  论文提出 FactLens 基准，用于细粒度验证 LLM 生成内容，将复杂声明分解为子声明，核心贡献是提升事实检查的精确性和透明度，发现子声明质量直接影响验证性能。\n\n- **NeKo: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts（NeKo：面向任务专家的后识别生成校正大型语言模型）**  \n  作者团队包括 Boris Ginsburg，论文使用 Mixture-of-Experts 架构训练 LLM 处理语音和视觉任务，关键发现是它在 ASR 基准上实现了 5.0% 的 WER 相对减少，并在零样本评估中超越 GPT-3.5。\n\n- **Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving（Qwen2.5-32B：利用自一致工具集成推理解决孟加拉数学奥林匹克问题）**  \n  这篇论文优化 Qwen 模型用于孟加拉语数学问题，通过提示工程和工具集成，核心贡献是提升模型在数学推理中的准确性，适用于低资源语言任务。\n\n- **WorkflowLLM: Enhancing Workflow Orchestration Capability of Large Language Models（WorkflowLLM：提升大型语言模型的工作流编排能力）**  \n  论文构建 WorkflowBench 数据集并微调 Llama-3.1-8B，关键发现是它能高效处理复杂工作流，适用于 API 编排，展示了 LLM 在自动化任务中的潜力。\n\n这些 LLM 相关论文突出了模型的说服力、推理和多任务能力，强调了工具集成和基准测试的重要性，对 AI 应用有直接影响。\n\n### 医疗和生物应用\n医疗 AI 论文较多，这里挑选最具话题度的，快速概述其贡献。\n\n- **Assessing Foundational Medical 'Segment Anything' (Med-SAM1, Med-SAM2) Deep Learning Models for Left Atrial Segmentation in 3D LGE MRI（评估基础医疗分割模型 Med-SAM1 和 Med-SAM2 用于 3D LGE MRI 中的左心房分割）**  \n  论文比较 Med-SAM 模型在心脏 MRI 图像中的性能，核心发现是 Med-SAM2 通过自动跟踪提升了分割准确性（Dice 分数优化），适用于心律失常诊断。\n\n- **ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles（ASL STEM Wiki：用于解释 STEM 文章的数据集和基准）**  \n  论文发布了一个包含 300 小时 ASL 视频的数据集，聚焦 STEM 教育，关键贡献是帮助聋哑学生通过 AI 识别手指拼写，提升教育公平性。\n\n- **Humans and Large Language Models in Clinical Decision Support: A Study with Medical Calculators（人类和大型语言模型在临床决策支持中的作用：基于医疗计算器的研究）**  \n  评估 LLM 在医疗决策中的表现，核心发现是人类在准确性上优于 LLM（如 GPT-4o），但 LLM 可辅助计算器推荐，潜在影响临床效率。\n\n### 机器人和环境监测\n这些论文关注实际应用中的优化和鲁棒性。\n\n- **Adaptive Sensor Placement Inspired by Bee Foraging: Towards Efficient Environment Monitoring（受蜜蜂觅食启发的自适应传感器放置：面向高效环境监测）**  \n  提出结合 Artificial Bee Colony 和 Levy 飞行算法的混合方法，核心贡献是优化传感器放置，提高热点识别效率，适用于环境监测和搜索救援。\n\n- **Moving Off-the-Grid: Scene-Grounded Video Representations（脱离网格：基于场景的视频表示）**  \n  作者包括 Joao Carreira 和 Alexey Dosovitskiy，论文开发 MooG 模型，支持视频中物体跟踪，关键发现是它在下游任务中超越传统网格方法，提升视频理解。\n\n其他如分子生成和优化论文（如\"Improving Molecular Graph Generation with Flow Matching and Optimal Transport\"）虽有技术创新，但相对专业且不那么热门，这里仅简要提及：它们提升了药物发现的生成效率，通过流匹配优化分子图结构。\n\n今天的 arXiv 更新展示了 AI 模型在实际领域的潜力，但也暴露了如幻觉和鲁棒性挑战；建议关注 LLM 相关进展，以推动更可靠的应用。",
  "papers": [
    {
      "arxiv_id": "2411.06009v1",
      "title": "A Comprehensive Guide to Enhancing Antibiotic Discovery Using Machine Learning Derived Bio-computation",
      "title_zh": "翻译失败",
      "authors": [
        "Khartik Uppalapati",
        "Eeshan Dandamudi",
        "S. Nick Ice",
        "Gaurav Chandra",
        "Kirsten Bischof",
        "Christian L. Lorson",
        "Kamal Singh"
      ],
      "abstract": "Traditional drug discovery is a long, expensive, and complex process.\nAdvances in Artificial Intelligence (AI) and Machine Learning (ML) are\nbeginning to change this narrative. Here, we provide a comprehensive overview\nof different AI and ML tools that can be used to streamline and accelerate the\ndrug discovery process. By using data sets to train ML algorithms, it is\npossible to discover drugs or drug-like compounds relatively quickly, and\nefficiently. Additionally, we address limitations in AI-based drug discovery\nand development, including the scarcity of high-quality data to train AI models\nand ethical considerations. The growing impact of AI on the pharmaceutical\nindustry is also highlighted. Finally, we discuss how AI and ML can expedite\nthe discovery of new antibiotics to combat the problem of worldwide\nantimicrobial resistance (AMR).",
      "tldr_zh": "本论文提供了一个全面指南，探讨如何利用 Machine Learning 衍生出的生物计算技术来提升抗生素发现效率。传统药物发现过程漫长、昂贵且复杂，但通过训练 Machine Learning 算法来分析数据集，可以快速高效地识别潜在药物或类似化合物，同时解决领域中的数据 scarcity 和 ethical considerations 等限制。论文强调 AI 和 Machine Learning 在制药行业的影响，并特别指出这些工具可加速新抗生素的开发，以对抗全球 antimicrobial resistance (AMR) 问题。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "65 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.06009v1",
      "published_date": "2024-11-08 23:04:42 UTC",
      "updated_date": "2024-11-08 23:04:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:38:34.259051"
    },
    {
      "arxiv_id": "2411.06008v2",
      "title": "The Dark Patterns of Personalized Persuasion in Large Language Models: Exposing Persuasive Linguistic Features for Big Five Personality Traits in LLMs Responses",
      "title_zh": "翻译失败",
      "authors": [
        "Wiktoria Mieleszczenko-Kowszewicz",
        "Dawid Płudowski",
        "Filip Kołodziejczyk",
        "Jakub Świstak",
        "Julian Sienkiewicz",
        "Przemysław Biecek"
      ],
      "abstract": "This study explores how the Large Language Models (LLMs) adjust linguistic\nfeatures to create personalized persuasive outputs. While research showed that\nLLMs personalize outputs, a gap remains in understanding the linguistic\nfeatures of their persuasive capabilities. We identified 13 linguistic features\ncrucial for influencing personalities across different levels of the Big Five\nmodel of personality. We analyzed how prompts with personality trait\ninformation influenced the output of 19 LLMs across five model families. The\nfindings show that models use more anxiety-related words for neuroticism,\nincrease achievement-related words for conscientiousness, and employ fewer\ncognitive processes words for openness to experience. Some model families excel\nat adapting language for openness to experience, others for conscientiousness,\nwhile only one model adapts language for neuroticism. Our findings show how\nLLMs tailor responses based on personality cues in prompts, indicating their\npotential to create persuasive content affecting the mind and well-being of the\nrecipients.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 如何通过调整语言特征来创建针对 Big Five 个性特质的个性化说服输出，揭示了这些模型的潜在“黑暗模式”。研究者识别了 13 个关键语言特征，并分析了 19 个 LLMs（来自五个模型家族）对包含个性提示的响应。结果显示，模型针对 neuroticism 使用更多焦虑相关词、针对 conscientiousness 增加成就相关词、而针对 openness to experience 减少认知过程词，不同模型家族在适应这些特质方面表现出差异。这些发现强调了 LLMs 基于提示的个性化响应可能对用户心理和福祉产生影响的风险。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "31 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.06008v2",
      "published_date": "2024-11-08 23:02:59 UTC",
      "updated_date": "2024-11-12 14:30:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:38:47.657923"
    },
    {
      "arxiv_id": "2411.15159v1",
      "title": "Adaptive Sensor Placement Inspired by Bee Foraging: Towards Efficient Environment Monitoring",
      "title_zh": "翻译失败",
      "authors": [
        "Sai Krishna Reddy Sathi"
      ],
      "abstract": "This paper aims to make a mark in the future of sustainable robotics, where\nefficient algorithms are required to carry out tasks like environmental\nmonitoring and precision agriculture efficiently. We proposed a hybrid\nalgorithm that combines Artificial Bee Colony (ABC) with Levy flight to\noptimize adaptive sensor placement alongside an important notion of hotspots\nfrom domain knowledge experts. By enhancing exploration and exploitation, our\napproach significantly improves the identification of critical hotspots. This\nalgorithm also finds its usecases for broader search and rescue operations\napplications, demonstrating its potential in optimization problems across\nvarious domains.",
      "tldr_zh": "本论文提出了一种混合算法，将Artificial Bee Colony (ABC) 与Levy flight 相结合，优化自适应传感器放置，以提升环境监测和精准农业等任务的效率。该算法借鉴蜜蜂觅食行为，并融入领域专家定义的热点概念，显著增强探索和利用能力，从而更好地识别关键热点。实验结果显示，该方法在优化问题上表现出色，并扩展适用于搜索和救援等更广泛领域，推动可持续机器人技术的进步。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.NE",
        "cs.RO"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15159v1",
      "published_date": "2024-11-08 22:24:06 UTC",
      "updated_date": "2024-11-08 22:24:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:38:57.572970"
    },
    {
      "arxiv_id": "2411.05990v2",
      "title": "Game-theoretic LLM: Agent Workflow for Negotiation Games",
      "title_zh": "基于博弈论的LLM：代理工作流用于谈判游戏",
      "authors": [
        "Wenyue Hua",
        "Ollie Liu",
        "Lingyao Li",
        "Alfonso Amayuelas",
        "Julie Chen",
        "Lucas Jiang",
        "Mingyu Jin",
        "Lizhou Fan",
        "Fei Sun",
        "William Wang",
        "Xintong Wang",
        "Yongfeng Zhang"
      ],
      "abstract": "This paper investigates the rationality of large language models (LLMs) in\nstrategic decision-making contexts, specifically within the framework of game\ntheory. We evaluate several state-of-the-art LLMs across a spectrum of\ncomplete-information and incomplete-information games. Our findings reveal that\nLLMs frequently deviate from rational strategies, particularly as the\ncomplexity of the game increases with larger payoff matrices or deeper\nsequential trees.\n  To address these limitations, we design multiple game-theoretic workflows\nthat guide the reasoning and decision-making processes of LLMs. These workflows\naim to enhance the models' ability to compute Nash Equilibria and make rational\nchoices, even under conditions of uncertainty and incomplete information.\nExperimental results demonstrate that the adoption of these workflows\nsignificantly improves the rationality and robustness of LLMs in game-theoretic\ntasks. Specifically, with the workflow, LLMs exhibit marked improvements in\nidentifying optimal strategies, achieving near-optimal allocations in\nnegotiation scenarios, and reducing susceptibility to exploitation during\nnegotiations. Furthermore, we explore the meta-strategic considerations of\nwhether it is rational for agents to adopt such workflows, recognizing that the\ndecision to use or forgo the workflow constitutes a game-theoretic issue in\nitself.\n  Our research contributes to a deeper understanding of LLMs' decision-making\ncapabilities in strategic contexts and provides insights into enhancing their\nrationality through structured workflows. The findings have implications for\nthe development of more robust and strategically sound AI agents capable of\nnavigating complex interactive environments. Code and data supporting this\nstudy are available at \\url{https://github.com/Wenyueh/game_theory}.",
      "tldr_zh": "这篇论文评估了大型语言模型 (LLMs) 在游戏理论框架下的理性决策能力，发现 LLMs 在完全信息和不完全信息游戏中，尤其在复杂 payoff matrices 或深度顺序树的情况下，经常偏离 Nash Equilibria 等最优策略。研究者设计了多种游戏理论工作流来指导 LLMs 的推理和决策过程，提升其在不确定性和不完全信息环境下的理性选择。实验结果显示，这些工作流显著改善了 LLMs 的表现，包括更好地识别最优策略、优化谈判分配并减少被利用风险。总体上，该研究加深了对 LLMs 战略决策的理解，并为构建更鲁棒的 AI 代理提供了实用见解。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.GT",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "45 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.05990v2",
      "published_date": "2024-11-08 22:02:22 UTC",
      "updated_date": "2024-11-12 05:46:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:39:11.536194"
    },
    {
      "arxiv_id": "2411.05983v1",
      "title": "Longitudinal Ensemble Integration for sequential classification with multimodal data",
      "title_zh": "翻译失败",
      "authors": [
        "Aviad Susman",
        "Rupak Krishnamurthy",
        "Yan Chak Li",
        "Mohammad Olaimat",
        "Serdar Bozdag",
        "Bino Varghese",
        "Nasim Sheikh-Bahaei",
        "Gaurav Pandey"
      ],
      "abstract": "Effectively modeling multimodal longitudinal data is a pressing need in\nvarious application areas, especially biomedicine. Despite this, few approaches\nexist in the literature for this problem, with most not adequately taking into\naccount the multimodality of the data. In this study, we developed multiple\nconfigurations of a novel multimodal and longitudinal learning framework,\nLongitudinal Ensemble Integration (LEI), for sequential classification. We\nevaluated LEI's performance, and compared it against existing approaches, for\nthe early detection of dementia, which is among the most studied multimodal\nsequential classification tasks. LEI outperformed these approaches due to its\nuse of intermediate base predictions arising from the individual data\nmodalities, which enabled their better integration over time. LEI's design also\nenabled the identification of features that were consistently important across\ntime for the effective prediction of dementia-related diagnoses. Overall, our\nwork demonstrates the potential of LEI for sequential classification from\nlongitudinal multimodal data.",
      "tldr_zh": "本研究开发了 Longitudinal Ensemble Integration (LEI) 框架，用于处理多模态纵向数据的序列分类问题，旨在解决现有方法在整合多模态数据方面的不足。LEI 通过利用各个数据模态的中间基预测进行时间整合，提高了分类性能，并在早诊痴呆任务上优于现有方法。实验结果表明，LEI 不仅提升了预测准确率，还能识别跨时间一致重要的特征，展示了其在生物医学等领域的应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, submitted to ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.05983v1",
      "published_date": "2024-11-08 21:31:48 UTC",
      "updated_date": "2024-11-08 21:31:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:39:22.463078"
    },
    {
      "arxiv_id": "2411.05982v2",
      "title": "Unmasking the Shadows: Pinpoint the Implementations of Anti-Dynamic Analysis Techniques in Malware Using LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Haizhou Wang",
        "Nanqing Luo",
        "Xusheng Li",
        "Peng LIu"
      ],
      "abstract": "Sandboxes and other dynamic analysis processes are prevalent in malware\ndetection systems nowadays to enhance the capability of detecting 0-day\nmalware. Therefore, techniques of anti-dynamic analysis (TADA) are prevalent in\nmodern malware samples, and sandboxes can suffer from false negatives and\nanalysis failures when analyzing the samples with TADAs. In such cases, human\nreverse engineers will get involved in conducting dynamic analysis manually\n(i.e., debugging, patching), which in turn also gets obstructed by TADAs. In\nthis work, we propose a Large Language Model (LLM) based workflow that can\npinpoint the location of the TADA implementation in the code, to help reverse\nengineers place breakpoints used in debugging. Our evaluation shows that we\nsuccessfully identified the locations of 87.80% known TADA implementations\nadopted from public repositories. In addition, we successfully pinpoint the\nlocations of TADAs in 4 well-known malware samples that are documented in\nonline malware analysis blogs.",
      "tldr_zh": "本研究针对恶意软件中反动态分析技术(TADA)导致的沙箱检测失败问题，提出了一种基于Large Language Model (LLM)的自动化工作流程，用于精确定位TADA在代码中的实现位置，从而辅助逆向工程师在调试过程中放置断点。该方法通过LLM分析代码样本，成功识别了87.80%的已知TADA实现，并成功定位了4个知名恶意软件样本中的TADA位置。实验结果证明了该工作流程的有效性，有望提升恶意软件分析的效率和准确性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05982v2",
      "published_date": "2024-11-08 21:30:33 UTC",
      "updated_date": "2025-04-29 03:47:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:39:34.266900"
    },
    {
      "arxiv_id": "2411.05980v2",
      "title": "FactLens: Benchmarking Fine-Grained Fact Verification",
      "title_zh": "FactLens：细粒度事实验证的基准测试",
      "authors": [
        "Kushan Mitra",
        "Dan Zhang",
        "Sajjadur Rahman",
        "Estevam Hruschka"
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive capability in language\ngeneration and understanding, but their tendency to hallucinate and produce\nfactually incorrect information remains a key limitation. To verify\nLLM-generated contents and claims from other sources, traditional verification\napproaches often rely on holistic models that assign a single factuality label\nto complex claims, potentially obscuring nuanced errors. In this paper, we\nadvocate for a shift toward fine-grained verification, where complex claims are\nbroken down into smaller sub-claims for individual verification, allowing for\nmore precise identification of inaccuracies, improved transparency, and reduced\nambiguity in evidence retrieval. However, generating sub-claims poses\nchallenges, such as maintaining context and ensuring semantic equivalence with\nrespect to the original claim. We introduce FactLens, a benchmark for\nevaluating fine-grained fact verification, with metrics and automated\nevaluators of sub-claim quality. The benchmark data is manually curated to\nensure high-quality ground truth. Our results show alignment between automated\nFactLens evaluators and human judgments, and we discuss the impact of sub-claim\ncharacteristics on the overall verification performance.",
      "tldr_zh": "大语言模型 (LLMs) 容易产生幻觉和事实错误，传统整体验证方法难以精确识别复杂声明中的细微问题。本文提倡细粒度事实验证，将声明分解为子声明，以提高准确性、透明度和证据检索效率。作者引入 FactLens 基准，包括评估指标、自动化评估器和手动策划的高质量数据，结果显示自动化评估器与人类判断高度一致，并探讨了子声明特征对整体验证性能的影响。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, updated version",
      "pdf_url": "http://arxiv.org/pdf/2411.05980v2",
      "published_date": "2024-11-08 21:26:57 UTC",
      "updated_date": "2025-04-18 18:56:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:39:46.435040"
    },
    {
      "arxiv_id": "2411.05963v1",
      "title": "Assessing Foundational Medical 'Segment Anything' (Med-SAM1, Med-SAM2) Deep Learning Models for Left Atrial Segmentation in 3D LGE MRI",
      "title_zh": "翻译失败",
      "authors": [
        "Mehri Mehrnia",
        "Mohamed Elbayumi",
        "Mohammed S. M. Elbaz"
      ],
      "abstract": "Atrial fibrillation (AF), the most common cardiac arrhythmia, is associated\nwith heart failure and stroke. Accurate segmentation of the left atrium (LA) in\n3D late gadolinium-enhanced (LGE) MRI is helpful for evaluating AF, as fibrotic\nremodeling in the LA myocardium contributes to arrhythmia and serves as a key\ndeterminant of therapeutic strategies. However, manual LA segmentation is\nlabor-intensive and challenging. Recent foundational deep learning models, such\nas the Segment Anything Model (SAM), pre-trained on diverse datasets, have\ndemonstrated promise in generic segmentation tasks. MedSAM, a fine-tuned\nversion of SAM for medical applications, enables efficient, zero-shot\nsegmentation without domain-specific training. Despite the potential of MedSAM\nmodel, it has not yet been evaluated for the complex task of LA segmentation in\n3D LGE-MRI. This study aims to (1) evaluate the performance of MedSAM in\nautomating LA segmentation, (2) compare the performance of the MedSAM2 model,\nwhich uses a single prompt with automated tracking, with the MedSAM1 model,\nwhich requires separate prompt for each slice, and (3) analyze the performance\nof MedSAM1 in terms of Dice score(i.e., segmentation accuracy) by varying the\nsize and location of the box prompt.",
      "tldr_zh": "本研究评估了Med-SAM1和Med-SAM2等基础深度学习模型在3D LGE MRI中进行左心房(LA)分割的性能，以辅助房颤(AF)评估和治疗策略制定。研究首先测试Med-SAM（基于Segment Anything Model (SAM)的医疗细调版本）在零样本LA分割任务中的准确性，然后比较Med-SAM1（需为每个切片提供单独提示）和Med-SAM2（使用单一提示和自动跟踪）的表现。最终，通过分析Med-SAM1的Dice score（分割准确性）随提示框大小和位置的变化，揭示了这些模型在复杂医疗图像任务中的优势和局限性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05963v1",
      "published_date": "2024-11-08 20:49:54 UTC",
      "updated_date": "2024-11-08 20:49:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:39:58.367334"
    },
    {
      "arxiv_id": "2411.05961v1",
      "title": "Aligned Vector Quantization for Edge-Cloud Collabrative Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiao Liu",
        "Lijun Zhang",
        "Deepak Ganesan",
        "Hui Guan"
      ],
      "abstract": "Vision Language Models (VLMs) are central to Visual Question Answering (VQA)\nsystems and are typically deployed in the cloud due to their high computational\ndemands. However, this cloud-only approach underutilizes edge computational\nresources and requires significant bandwidth for transmitting raw images. In\nthis paper, we introduce an edge-cloud collaborative VQA system, called\nLLaVA-AlignedVQ, which features a novel Aligned Vector Quantization algorithm\n(AlignedVQ) that efficiently compress intermediate features without\ncompromising accuracy to support partitioned execution. Our experiments\ndemonstrate that LLaVA-AlignedVQ achieves approximately 1365x compression rate\nof intermediate features, reducing data transmission overhead by 96.8% compared\nto transmitting JPEG90-compressed images to the cloud. LLaVA-AlignedVQ achieves\nan inference speedup of 2-15x while maintaining high accuracy, remaining within\n-2.23% to +1.6% of the original model's accuracy performance across eight VQA\ndatasets, compared to the cloud-only solution.",
      "tldr_zh": "本研究针对视觉语言模型(VLMs)在视觉问答(VQA)系统中的高计算需求，提出了一种边云协作框架LLaVA-AlignedVQ，以解决云端部署导致的资源浪费和带宽问题。核心创新是Aligned Vector Quantization算法(AlignedVQ)，它高效压缩中间特征，支持分区执行，同时保持模型准确性。实验结果显示，该框架实现了约1365倍的特征压缩率，减少96.8%的数据传输开销，并将推理速度提升2-15倍，在八个VQA数据集上，准确性仅与原模型相差-2.23%至+1.6%。这为高效的边云协作VQA系统提供了可行解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.05961v1",
      "published_date": "2024-11-08 20:48:37 UTC",
      "updated_date": "2024-11-08 20:48:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:40:10.868112"
    },
    {
      "arxiv_id": "2411.05958v1",
      "title": "Sentiment Analysis of Cyberbullying Data in Social Media",
      "title_zh": "社交媒体中网络欺凌数据的情感分析",
      "authors": [
        "Arvapalli Sai Susmitha",
        "Pradeep Pujari"
      ],
      "abstract": "Social media has become an integral part of modern life, but it has also\nbrought with it the pervasive issue of cyberbullying a serious menace in\ntoday's digital age. Cyberbullying, a form of harassment that occurs on social\nnetworks, has escalated alongside the growth of these platforms. Sentiment\nanalysis holds significant potential not only for detecting bullying phrases\nbut also for identifying victims who are at high risk of harm, whether to\nthemselves or others. Our work focuses on leveraging deep learning and natural\nlanguage understanding techniques to detect traces of bullying in social media\nposts. We developed a Recurrent Neural Network with Long Short-Term Memory\n(LSTM) cells, using different embeddings. One approach utilizes BERT\nembeddings, while the other replaces the embeddings layer with the recently\nreleased embeddings API from OpenAI. We conducted a performance comparison\nbetween these two approaches to evaluate their effectiveness in sentiment\nanalysis of Formspring Cyberbullying data. Our Code is Available at\nhttps://github.com/ppujari/xcs224u",
      "tldr_zh": "本研究针对社交媒体上的网络欺凌问题，运用情感分析技术检测欺凌短语并识别高风险受害者。研究开发了一种基于 LSTM 的循环神经网络（RNN），分别使用 BERT 嵌入和 OpenAI 嵌入 API 作为输入层。实验在 Formspring Cyberbullying 数据集上比较了两种方法的性能，结果显示这些方法在检测欺凌痕迹方面表现出色。代码已开源，可在 GitHub 上获取（https://github.com/ppujari/xcs224u）。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05958v1",
      "published_date": "2024-11-08 20:41:04 UTC",
      "updated_date": "2024-11-08 20:41:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:40:22.444895"
    },
    {
      "arxiv_id": "2411.05945v1",
      "title": "NeKo: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Yen-Ting Lin",
        "Chao-Han Huck Yang",
        "Zhehuai Chen",
        "Piotr Zelasko",
        "Xuesong Yang",
        "Zih-Ching Chen",
        "Krishna C Puvvada",
        "Szu-Wei Fu",
        "Ke Hu",
        "Jun Wei Chiu",
        "Jagadeesh Balam",
        "Boris Ginsburg",
        "Yu-Chiang Frank Wang"
      ],
      "abstract": "Construction of a general-purpose post-recognition error corrector poses a\ncrucial question: how can we most effectively train a model on a large mixture\nof domain datasets? The answer would lie in learning dataset-specific features\nand digesting their knowledge in a single model. Previous methods achieve this\nby having separate correction language models, resulting in a significant\nincrease in parameters. In this work, we present Mixture-of-Experts as a\nsolution, highlighting that MoEs are much more than a scalability tool. We\npropose a Multi-Task Correction MoE, where we train the experts to become an\n``expert'' of speech-to-text, language-to-text and vision-to-text datasets by\nlearning to route each dataset's tokens to its mapped expert. Experiments on\nthe Open ASR Leaderboard show that we explore a new state-of-the-art\nperformance by achieving an average relative $5.0$% WER reduction and\nsubstantial improvements in BLEU scores for speech and translation tasks. On\nzero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with $15.5$% to\n$27.6$% relative WER reduction in the Hyporadise benchmark. NeKo performs\ncompetitively on grammar and post-OCR correction as a multi-task model.",
      "tldr_zh": "该研究提出NeKo框架，旨在通过Task-Oriented Experts构建通用后识别生成校正大型语言模型（Large Language Models），以有效处理混合领域数据集的训练问题。NeKo采用Multi-Task Correction Mixture-of-Experts (MoE)方法，将专家专用于speech-to-text、language-to-text和vision-to-text等特定数据集，通过路由机制学习并整合数据集特征，从而避免了传统方法的参数膨胀。实验结果显示，在Open ASR Leaderboard上，NeKo实现了平均相对5.0% WER减少和BLEU分数的显著提升；在零样本评估中，它超过了GPT-3.5和Claude-Opus，在Hyporadise基准上获得15.5%至27.6%的相对WER减少，并在语法和后OCR校正任务上表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "NeKo work has been done in June 2024. NeKo LMs will be open source on\n  https://huggingface.co/nvidia under the MIT license",
      "pdf_url": "http://arxiv.org/pdf/2411.05945v1",
      "published_date": "2024-11-08 20:11:24 UTC",
      "updated_date": "2024-11-08 20:11:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:40:35.753836"
    },
    {
      "arxiv_id": "2411.05943v1",
      "title": "Quantifying artificial intelligence through algebraic generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Takuya Ito",
        "Murray Campbell",
        "Lior Horesh",
        "Tim Klinger",
        "Parikshit Ram"
      ],
      "abstract": "The rapid development of modern artificial intelligence (AI) systems has\ncreated an urgent need for their scientific quantification. While their fluency\nacross a variety of domains is impressive, modern AI systems fall short on\ntests requiring symbolic processing and abstraction - a glaring limitation\ngiven the necessity for interpretable and reliable technology. Despite a surge\nof reasoning benchmarks emerging from the academic community, no comprehensive\nand theoretically-motivated framework exists to quantify reasoning (and more\ngenerally, symbolic ability) in AI systems. Here, we adopt a framework from\ncomputational complexity theory to explicitly quantify symbolic generalization:\nalgebraic circuit complexity. Many symbolic reasoning problems can be recast as\nalgebraic expressions. Thus, algebraic circuit complexity theory - the study of\nalgebraic expressions as circuit models (i.e., directed acyclic graphs) - is a\nnatural framework to study the complexity of symbolic computation. The tools of\nalgebraic circuit complexity enable the study of generalization by defining\nbenchmarks in terms of their complexity-theoretic properties (i.e., the\ndifficulty of a problem). Moreover, algebraic circuits are generic mathematical\nobjects; for a given algebraic circuit, an arbitrarily large number of samples\ncan be generated for a specific circuit, making it an optimal testbed for the\ndata-hungry machine learning algorithms that are used today. Here, we adopt\ntools from algebraic circuit complexity theory, apply it to formalize a science\nof symbolic generalization, and address key theoretical and empirical\nchallenges for its successful application to AI science and its impact on the\nbroader community.",
      "tldr_zh": "本研究针对现代人工智能（AI）系统在符号处理和抽象方面的不足，提出了一种基于计算复杂性理论的框架来量化AI的符号泛化能力。作者采用algebraic circuit complexity理论，将符号推理问题转化为algebraic expressions和电路模型，从而定义基准并评估问题难度。该方法能生成大量样本，适用于数据驱动的机器学习算法，并解决AI科学中的理论和实证挑战，最终为开发可解释且可靠的AI技术提供基础。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05943v1",
      "published_date": "2024-11-08 20:08:18 UTC",
      "updated_date": "2024-11-08 20:08:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:40:45.908873"
    },
    {
      "arxiv_id": "2411.05939v1",
      "title": "GCI-ViTAL: Gradual Confidence Improvement with Vision Transformers for Active Learning on Label Noise",
      "title_zh": "GCI-ViTAL：利用视觉Transformer逐步提升置信度，用于标签噪声下的主动学习",
      "authors": [
        "Moseli Mots'oehli",
        "kyungim Baek"
      ],
      "abstract": "Active learning aims to train accurate classifiers while minimizing labeling\ncosts by strategically selecting informative samples for annotation. This study\nfocuses on image classification tasks, comparing AL methods on CIFAR10,\nCIFAR100, Food101, and the Chest X-ray datasets under varying label noise\nrates. We investigate the impact of model architecture by comparing\nConvolutional Neural Networks (CNNs) and Vision Transformer (ViT)-based models.\nAdditionally, we propose a novel deep active learning algorithm, GCI-ViTAL,\ndesigned to be robust to label noise. GCI-ViTAL utilizes prediction entropy and\nthe Frobenius norm of last-layer attention vectors compared to class-centric\nclean set attention vectors. Our method identifies samples that are both\nuncertain and semantically divergent from typical images in their assigned\nclass. This allows GCI-ViTAL to select informative data points even in the\npresence of label noise while flagging potentially mislabeled candidates. Label\nsmoothing is applied to train a model that is not overly confident about\npotentially noisy labels. We evaluate GCI-ViTAL under varying levels of\nsymmetric label noise and compare it to five other AL strategies. Our results\ndemonstrate that using ViTs leads to significant performance improvements over\nCNNs across all AL strategies, particularly in noisy label settings. We also\nfind that using the semantic information of images as label grounding helps in\ntraining a more robust model under label noise. Notably, we do not perform\nextensive hyperparameter tuning, providing an out-of-the-box comparison that\naddresses the common challenge practitioners face in selecting models and\nactive learning strategies without an exhaustive literature review on training\nand fine-tuning vision models on real-world application data.",
      "tldr_zh": "该研究探讨了主动学习（Active Learning）在图像分类任务中的应用，比较了不同模型架构（如 Convolutional Neural Networks (CNNs) 和 Vision Transformers (ViT)）在 CIFAR10、CIFAR100、Food101 和 Chest X-ray 数据集上的表现，尤其在标签噪声环境下。研究提出了一种新算法 GCI-ViTAL，利用预测熵（prediction entropy）和 Frobenius norm 来评估样本的不确定性和语义差异，从而选择信息丰富的样本，同时通过标签平滑（Label smoothing）增强模型对噪声标签的鲁棒性。实验结果显示，ViT 在各种主动学习策略中显著优于 CNNs，特别是在噪声标签设置下性能提升明显，且使用图像语义信息作为标签基础有助于构建更可靠的模型。总的来说，该方法提供了一种无需大量超参数调整的实用方案，适用于真实世界应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "under review",
      "pdf_url": "http://arxiv.org/pdf/2411.05939v1",
      "published_date": "2024-11-08 19:59:40 UTC",
      "updated_date": "2024-11-08 19:59:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:41:00.056300"
    },
    {
      "arxiv_id": "2411.05936v1",
      "title": "Mitigating Hallucination with ZeroG: An Advanced Knowledge Management Engine",
      "title_zh": "翻译失败",
      "authors": [
        "Anantha Sharma",
        "Sheeba Elizabeth John",
        "Fatemeh Rezapoor Nikroo",
        "Krupali Bhatt",
        "Mrunal Zambre",
        "Aditi Wikhe"
      ],
      "abstract": "The growth of digital documents presents significant challenges in efficient\nmanagement and knowledge extraction. Traditional methods often struggle with\ncomplex documents, leading to issues such as hallucinations and high latency in\nresponses from Large Language Models (LLMs). ZeroG, an innovative approach,\nsignificantly mitigates these challenges by leveraging knowledge distillation\nand prompt tuning to enhance model performance.\n  ZeroG utilizes a smaller model that replicates the behavior of a larger\nteacher model, ensuring contextually relevant and grounded responses, by\nemploying a black-box distillation approach, it creates a distilled dataset\nwithout relying on intermediate features, optimizing computational efficiency.\nThis method significantly enhances accuracy and reduces response times,\nproviding a balanced solution for modern document management.\n  Incorporating advanced techniques for document ingestion and metadata\nutilization, ZeroG improves the accuracy of question-and-answer systems. The\nintegration of graph databases and robust metadata management further\nstreamlines information retrieval, allowing for precise and context-aware\nresponses. By transforming how organizations interact with complex data, ZeroG\nenhances productivity and user experience, offering a scalable solution for the\ngrowing demands of digital document management.",
      "tldr_zh": "该研究提出ZeroG，一种先进的知识管理引擎，通过知识蒸馏(knowledge distillation)和提示调整(prompt tuning)来缓解Large Language Models (LLMs)处理复杂文档时出现的幻觉(hallucinations)和高延迟问题。ZeroG采用黑箱蒸馏方法，使用较小模型模仿较大教师模型生成上下文相关的响应，同时优化计算效率并创建蒸馏数据集。实验结果显示，该引擎显著提高了问答系统的准确性，减少了响应时间，并通过整合图数据库和元数据管理，提供可扩展的解决方案，提升了数字文档管理的生产力和用户体验。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 4 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2411.05936v1",
      "published_date": "2024-11-08 19:47:02 UTC",
      "updated_date": "2024-11-08 19:47:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:41:10.300378"
    },
    {
      "arxiv_id": "2411.05934v1",
      "title": "Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving",
      "title_zh": "翻译失败",
      "authors": [
        "Saad Tahmid",
        "Sourav Sarker"
      ],
      "abstract": "We present an innovative approach for solving mathematical problems in\nBengali, developed for the DL Sprint 3.0 BUET CSE Fest 2024 Competition. Our\nmethod uses advanced deep learning models, notably the Qwen 2.5 series, with\nimprovements made through prompt engineering, model quantization, and Tool\nIntegrated Reasoning (TIR) to handle complex calculations. Initially, we\nexplored various model architectures, including fine-tuned Mistral and\nquantized Qwen models, refining them with translation techniques,\nRetrieval-Augmented Generation (RAG), and custom dataset curation. Manual\nhyperparameter tuning optimized parameters like temperature and top-p to\nenhance model adaptability and accuracy. Removal of RAG and parameter\nadjustments further improved robustness. Our approach highlights the potential\nof advanced NLP techniques in solving Bengali mathematical problems.",
      "tldr_zh": "本研究提出了一种创新方法，使用Qwen 2.5-32B模型结合Self-Consistent Tool-Integrated Reasoning (TIR)，针对DL Sprint 3.0 BUET CSE Fest 2024竞赛中的孟加拉语数学奥林匹克问题进行求解。方法包括通过提示工程、模型量化、翻译技术、Retrieval-Augmented Generation (RAG)以及自定义数据集的优化，来探索和微调Mistral及Qwen等模型架构，并手动调整超参数如温度和top-p以提升适应性和准确性。最终，通过移除RAG并进一步参数调整，提高了模型的鲁棒性，展示了高级NLP技术在解决Bengali数学问题方面的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05934v1",
      "published_date": "2024-11-08 19:44:12 UTC",
      "updated_date": "2024-11-08 19:44:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:41:24.091777"
    },
    {
      "arxiv_id": "2411.05930v2",
      "title": "BERTrend: Neural Topic Modeling for Emerging Trends Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Allaa Boutaleb",
        "Jerome Picault",
        "Guillaume Grosjean"
      ],
      "abstract": "Detecting and tracking emerging trends and weak signals in large, evolving\ntext corpora is vital for applications such as monitoring scientific\nliterature, managing brand reputation, surveilling critical infrastructure and\nmore generally to any kind of text-based event detection. Existing solutions\noften fail to capture the nuanced context or dynamically track evolving\npatterns over time. BERTrend, a novel method, addresses these limitations using\nneural topic modeling in an online setting. It introduces a new metric to\nquantify topic popularity over time by considering both the number of documents\nand update frequency. This metric classifies topics as noise, weak, or strong\nsignals, flagging emerging, rapidly growing topics for further investigation.\nExperimentation on two large real-world datasets demonstrates BERTrend's\nability to accurately detect and track meaningful weak signals while filtering\nout noise, offering a comprehensive solution for monitoring emerging trends in\nlarge-scale, evolving text corpora. The method can also be used for\nretrospective analysis of past events. In addition, the use of Large Language\nModels together with BERTrend offers efficient means for the interpretability\nof trends of events.",
      "tldr_zh": "BERTrend 是一种基于神经主题建模（neural topic modeling）的创新方法，用于在线检测大型演化文本语料中的新兴趋势和弱信号，解决了现有方案在捕捉上下文和动态跟踪方面的不足。该方法引入了一个新指标，通过文档数量和更新频率量化主题的受欢迎度，并将主题分类为噪声、弱信号或强信号，从而标记快速增长的趋势进行进一步调查。在两个真实数据集上的实验显示，BERTrend 能准确检测并跟踪有意义的弱信号，同时过滤噪声，并支持回顾性分析和结合 Large Language Models 的趋势解释。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 12 figures, FuturED 2024: Workshop on Future of Event\n  Detection (CoLocated with EMNLP 2024)",
      "pdf_url": "http://arxiv.org/pdf/2411.05930v2",
      "published_date": "2024-11-08 19:31:19 UTC",
      "updated_date": "2024-11-21 16:06:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:41:34.962779"
    },
    {
      "arxiv_id": "2411.05927v1",
      "title": "Moving Off-the-Grid: Scene-Grounded Video Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Sjoerd van Steenkiste",
        "Daniel Zoran",
        "Yi Yang",
        "Yulia Rubanova",
        "Rishabh Kabra",
        "Carl Doersch",
        "Dilara Gokay",
        "Joseph Heyward",
        "Etienne Pot",
        "Klaus Greff",
        "Drew A. Hudson",
        "Thomas Albert Keck",
        "Joao Carreira",
        "Alexey Dosovitskiy",
        "Mehdi S. M. Sajjadi",
        "Thomas Kipf"
      ],
      "abstract": "Current vision models typically maintain a fixed correspondence between their\nrepresentation structure and image space. Each layer comprises a set of tokens\narranged \"on-the-grid,\" which biases patches or tokens to encode information at\na specific spatio(-temporal) location. In this work we present Moving\nOff-the-Grid (MooG), a self-supervised video representation model that offers\nan alternative approach, allowing tokens to move \"off-the-grid\" to better\nenable them to represent scene elements consistently, even as they move across\nthe image plane through time. By using a combination of cross-attention and\npositional embeddings we disentangle the representation structure and image\nstructure. We find that a simple self-supervised objective--next frame\nprediction--trained on video data, results in a set of latent tokens which bind\nto specific scene structures and track them as they move. We demonstrate the\nusefulness of MooG's learned representation both qualitatively and\nquantitatively by training readouts on top of the learned representation on a\nvariety of downstream tasks. We show that MooG can provide a strong foundation\nfor different vision tasks when compared to \"on-the-grid\" baselines.",
      "tldr_zh": "本文提出 Moving Off-the-Grid (MooG)，一个自监督视频表示模型，允许 tokens 脱离固定网格结构，从而更好地跟踪和表示视频中移动的场景元素。MooG 通过结合 cross-attention 和 positional embeddings 来分离表示结构与图像结构，并使用下一帧预测作为训练目标，使 tokens 能够绑定到特定场景结构上。实验结果显示，在各种下游视觉任务上，MooG 比传统“on-the-grid”基线表现出更强的性能，提供了一个更灵活的视频表示基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS 2024 (spotlight). Project page:\n  https://moog-paper.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2411.05927v1",
      "published_date": "2024-11-08 19:26:51 UTC",
      "updated_date": "2024-11-08 19:26:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:41:46.727761"
    },
    {
      "arxiv_id": "2411.05783v1",
      "title": "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles",
      "title_zh": "翻译失败",
      "authors": [
        "Kayo Yin",
        "Chinmay Singh",
        "Fyodor O. Minakov",
        "Vanessa Milan",
        "Hal Daumé III",
        "Cyril Zhang",
        "Alex X. Lu",
        "Danielle Bragg"
      ],
      "abstract": "Deaf and hard-of-hearing (DHH) students face significant barriers in\naccessing science, technology, engineering, and mathematics (STEM) education,\nnotably due to the scarcity of STEM resources in signed languages. To help\naddress this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia\narticles on STEM topics in English, interpreted into over 300 hours of American\nSign Language (ASL). ASL STEM Wiki is the first continuous signing dataset\nfocused on STEM, facilitating the development of AI resources for STEM\neducation in ASL. We identify several use cases of ASL STEM Wiki with\nhuman-centered applications. For example, because this dataset highlights the\nfrequent use of fingerspelling for technical concepts, which inhibits DHH\nstudents' ability to learn, we develop models to identify fingerspelled words\n-- which can later be used to query for appropriate ASL signs to suggest to\ninterpreters.",
      "tldr_zh": "本研究介绍了ASL STEM Wiki，这是一个平行语料数据集，包含254篇英文Wikipedia STEM（科学、技术、工程和数学）文章及其对应的超过300小时美国手语（ASL）解释，旨在解决聋哑和听力障碍（DHH）学生在STEM教育中缺乏手语资源的问题。该数据集是首个专注于STEM的连续手语数据集，可用于开发AI资源，例如识别fingerspelled单词，以帮助查询合适的ASL手势并提升教学辅助。通过这一基准，研究者们可以探索更多的人类中心应用，促进DHH学生对技术概念的学习。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.05783v1",
      "published_date": "2024-11-08 18:50:37 UTC",
      "updated_date": "2024-11-08 18:50:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:41:58.344918"
    },
    {
      "arxiv_id": "2411.05781v1",
      "title": "Using Language Models to Disambiguate Lexical Choices in Translation",
      "title_zh": "利用语言模型消除翻译中的词汇选择歧义",
      "authors": [
        "Josh Barua",
        "Sanjay Subramanian",
        "Kayo Yin",
        "Alane Suhr"
      ],
      "abstract": "In translation, a concept represented by a single word in a source language\ncan have multiple variations in a target language. The task of lexical\nselection requires using context to identify which variation is most\nappropriate for a source text. We work with native speakers of nine languages\nto create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual\nconcept variation when translating from English. We evaluate recent LLMs and\nneural machine translation systems on DTAiLS, with the best-performing model,\nGPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use\nlanguage models to generate English rules describing target-language concept\nvariations. Providing weaker models with high-quality lexical rules improves\naccuracy substantially, in some cases reaching or outperforming GPT-4.",
      "tldr_zh": "该研究探讨了使用语言模型（LLMs）来解决翻译中的词汇选择歧义问题，即源语言单词在目标语言中的多种变体如何根据上下文选择最合适者。研究者与九种语言的母语者合作，创建了DTAiLS数据集，包含1377对展示跨语言概念变异的英语句子对，并评估了最近的LLMs和神经机器翻译系统（NMT），其中GPT-4在不同语言上的准确率达到67%至85%。此外，通过语言模型生成高质量的英语规则来辅助较弱模型，结果显示这些规则能显著提升准确率，有时甚至超越GPT-4，从而为改进翻译系统的词汇选择提供新方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.05781v1",
      "published_date": "2024-11-08 18:48:57 UTC",
      "updated_date": "2024-11-08 18:48:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:42:10.843778"
    },
    {
      "arxiv_id": "2411.05780v2",
      "title": "GazeSearch: Radiology Findings Search Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Trong Thang Pham",
        "Tien-Phat Nguyen",
        "Yuki Ikebe",
        "Akash Awasthi",
        "Zhigang Deng",
        "Carol C. Wu",
        "Hien Nguyen",
        "Ngan Le"
      ],
      "abstract": "Medical eye-tracking data is an important information source for\nunderstanding how radiologists visually interpret medical images. This\ninformation not only improves the accuracy of deep learning models for X-ray\nanalysis but also their interpretability, enhancing transparency in\ndecision-making. However, the current eye-tracking data is dispersed,\nunprocessed, and ambiguous, making it difficult to derive meaningful insights.\nTherefore, there is a need to create a new dataset with more focus and\npurposeful eyetracking data, improving its utility for diagnostic applications.\nIn this work, we propose a refinement method inspired by the target-present\nvisual search challenge: there is a specific finding and fixations are guided\nto locate it. After refining the existing eye-tracking datasets, we transform\nthem into a curated visual search dataset, called GazeSearch, specifically for\nradiology findings, where each fixation sequence is purposefully aligned to the\ntask of locating a particular finding. Subsequently, we introduce a scan path\nprediction baseline, called ChestSearch, specifically tailored to GazeSearch.\nFinally, we employ the newly introduced GazeSearch as a benchmark to evaluate\nthe performance of current state-of-the-art methods, offering a comprehensive\nassessment for visual search in the medical imaging domain. Code is available\nat \\url{https://github.com/UARK-AICV/GazeSearch}.",
      "tldr_zh": "本论文提出了GazeSearch，这是一个针对放射学发现的视觉搜索基准数据集，通过改进eye-tracking数据，使每个fixation序列专注于定位特定发现，以解决现有数据分散和模糊的问题。研究方法受目标存在视觉搜索挑战启发，对现有eye-tracking数据集进行精炼，并引入ChestSearch作为扫描路径预测的基准模型。实验结果显示，GazeSearch可用于全面评估当前最先进方法的性能，提升医疗图像分析的准确性和可解释性，代码已在GitHub上公开。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Aceepted WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.05780v2",
      "published_date": "2024-11-08 18:47:08 UTC",
      "updated_date": "2024-11-27 19:01:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:42:22.397289"
    },
    {
      "arxiv_id": "2411.05778v2",
      "title": "LLMs as Method Actors: A Model for Prompt Engineering and Architecture",
      "title_zh": "翻译失败",
      "authors": [
        "Colin Doyle"
      ],
      "abstract": "We introduce \"Method Actors\" as a mental model for guiding LLM prompt\nengineering and prompt architecture. Under this mental model, LLMs should be\nthought of as actors; prompts as scripts and cues; and LLM responses as\nperformances. We apply this mental model to the task of improving LLM\nperformance at playing Connections, a New York Times word puzzle game that\nprior research identified as a challenging benchmark for evaluating LLM\nreasoning. Our experiments with GPT-4o show that a \"Method Actors\" approach can\nsignificantly improve LLM performance over both a vanilla and \"Chain of\nThoughts\" approach. A vanilla approach solves 27% of Connections puzzles in our\ndataset and a \"Chain of Thoughts\" approach solves 41% of puzzles, whereas our\nstrongest \"Method Actor\" approach solves 86% of puzzles. We also test OpenAI's\nnewest model designed specifically for complex reasoning tasks, o1-preview.\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\na time over multiple API calls, o1-preview solves 100% of the puzzles.\nIncorporating a \"Method Actor\" prompt architecture increases the percentage of\npuzzles that o1-preview solves perfectly from 76% to 87%.",
      "tldr_zh": "这篇论文提出了“Method Actors”心理模型，将LLMs视为演员、提示视为脚本和提示、响应视为表演，以指导LLMs的提示工程和架构设计。研究者将该模型应用于Connections游戏（一个挑战性推理基准），实验结果显示，使用“Method Actors”方法后，GPT-4o的谜题解决率从vanilla方法的27%和Chain of Thoughts方法的41%提升至86%。此外，对于o1-preview模型，该方法使一次性完美解决率从76%提高到87%，并在分步API调用下实现100%解决率。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05778v2",
      "published_date": "2024-11-08 18:45:06 UTC",
      "updated_date": "2024-11-11 21:09:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:42:34.737139"
    },
    {
      "arxiv_id": "2411.05777v2",
      "title": "Quantitative Assessment of Intersectional Empathetic Bias and Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Vojtech Formanek",
        "Ondrej Sotolar"
      ],
      "abstract": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results.",
      "tldr_zh": "该论文批评了当前对移情(empathy)的宽松定义，导致数据集质量、模型鲁棒性和评估可靠性问题，并提出一个新的移情评估框架，将移情操作化到其心理起源。框架通过测量LLMs对提示的响应差异，使用现有的移情和情感价值指标，并通过控制生成提示来引入社会偏见变化，从而影响上下文理解和移情理解。这种方法确保了提示数据集的理论有效性和高质量翻译便利性（如Slavonic语言）。初步实验显示，LLMs对提示细微变化表现出显著的推理链改变，尽管移情理解差异较小，但为未来优化评估样本和统计方法提供了基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05777v2",
      "published_date": "2024-11-08 18:43:15 UTC",
      "updated_date": "2024-11-14 18:35:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:42:46.903077"
    },
    {
      "arxiv_id": "2411.05775v1",
      "title": "Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?",
      "title_zh": "翻译失败",
      "authors": [
        "Veronica Chatrath",
        "Marcelo Lotif",
        "Shaina Raza"
      ],
      "abstract": "Political misinformation poses significant challenges to democratic\nprocesses, shaping public opinion and trust in media. Manual fact-checking\nmethods face issues of scalability and annotator bias, while machine learning\nmodels require large, costly labelled datasets. This study investigates the use\nof state-of-the-art large language models (LLMs) as reliable annotators for\ndetecting political factuality in news articles. Using open-source LLMs, we\ncreate a politically diverse dataset, labelled for bias through LLM-generated\nannotations. These annotations are validated by human experts and further\nevaluated by LLM-based judges to assess the accuracy and reliability of the\nannotations. Our approach offers a scalable and robust alternative to\ntraditional fact-checking, enhancing transparency and public trust in media.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）是否能作为可靠的标注者来检测新闻文章中的政治真实性，以应对政治虚假信息对民主进程的威胁。研究使用开源LLMs创建了一个政治多样性的数据集，通过LLM生成的标注来标记偏差，并由人类专家和LLM-based judges进行验证，以评估标注的准确性和可靠性。该方法提供了一个可扩展且稳健的替代传统事实检查的方案，提高了媒体的透明度和公众信任。实验结果表明，这种基于LLMs的标注方法在处理政治事实性问题时表现出色，有望减少标注者偏差和数据成本。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at Socially Responsible Language Modelling Research (SoLaR)\n  Workshop at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.05775v1",
      "published_date": "2024-11-08 18:36:33 UTC",
      "updated_date": "2024-11-08 18:36:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:42:58.606605"
    },
    {
      "arxiv_id": "2411.05750v1",
      "title": "On Differentially Private String Distances",
      "title_zh": "翻译失败",
      "authors": [
        "Jerry Yao-Chieh Hu",
        "Erzhi Liu",
        "Han Liu",
        "Zhao Song",
        "Lichen Zhang"
      ],
      "abstract": "Given a database of bit strings $A_1,\\ldots,A_m\\in \\{0,1\\}^n$, a fundamental\ndata structure task is to estimate the distances between a given query $B\\in\n\\{0,1\\}^n$ with all the strings in the database. In addition, one might further\nwant to ensure the integrity of the database by releasing these distance\nstatistics in a secure manner. In this work, we propose differentially private\n(DP) data structures for this type of tasks, with a focus on Hamming and edit\ndistance. On top of the strong privacy guarantees, our data structures are also\ntime- and space-efficient. In particular, our data structure is $\\epsilon$-DP\nagainst any sequence of queries of arbitrary length, and for any query $B$ such\nthat the maximum distance to any string in the database is at most $k$, we\noutput $m$ distance estimates. Moreover,\n  - For Hamming distance, our data structure answers any query in $\\widetilde\nO(mk+n)$ time and each estimate deviates from the true distance by at most\n$\\widetilde O(k/e^{\\epsilon/\\log k})$;\n  - For edit distance, our data structure answers any query in $\\widetilde\nO(mk^2+n)$ time and each estimate deviates from the true distance by at most\n$\\widetilde O(k/e^{\\epsilon/(\\log k \\log n)})$.\n  For moderate $k$, both data structures support sublinear query operations. We\nobtain these results via a novel adaptation of the randomized response\ntechnique as a bit flipping procedure, applied to the sketched strings.",
      "tldr_zh": "这篇论文探讨了在保持差异隐私（DP）的前提下，估计查询字符串与数据库中二进制字符串之间的距离问题，重点关注Hamming distance和edit distance。作者提出了一种高效的DP数据结构，利用随机化响应（randomized response）作为位翻转程序应用于绘制的字符串（sketched strings），从而支持任意长度的查询序列。实验结果显示，对于Hamming distance，查询时间为\\(\\widetilde{O}(mk + n)\\)且估计误差不超过\\(\\widetilde{O}(k / e^{\\epsilon / \\log k})\\)；对于edit distance，查询时间为\\(\\widetilde{O}(mk^2 + n)\\)且误差不超过\\(\\widetilde{O}(k / e^{\\epsilon / (\\log k \\log n)})\\)；这种方法在中等k值下实现了次线性查询操作。",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05750v1",
      "published_date": "2024-11-08 18:10:07 UTC",
      "updated_date": "2024-11-08 18:10:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:43:13.180164"
    },
    {
      "arxiv_id": "2411.05748v1",
      "title": "Multi-Dimensional Reconfigurable, Physically Composable Hybrid Diffractive Optical Neural Network",
      "title_zh": "多维可重构、物理可组合的混合衍射光学神经网络",
      "authors": [
        "Ziang Yin",
        "Yu Yao",
        "Jeff Zhang",
        "Jiaqi Gu"
      ],
      "abstract": "Diffractive optical neural networks (DONNs), leveraging free-space light wave\npropagation for ultra-parallel, high-efficiency computing, have emerged as\npromising artificial intelligence (AI) accelerators. However, their inherent\nlack of reconfigurability due to fixed optical structures post-fabrication\nhinders practical deployment in the face of dynamic AI workloads and evolving\napplications. To overcome this challenge, we introduce, for the first time, a\nmulti-dimensional reconfigurable hybrid diffractive ONN system (MDR-HDONN), a\nphysically composable architecture that unlocks a new degree of freedom and\nunprecedented versatility in DONNs. By leveraging full-system learnability,\nMDR-HDONN repurposes fixed fabricated optical hardware, achieving exponentially\nexpanded functionality and superior task adaptability through the\ndifferentiable learning of system variables. Furthermore, MDR-HDONN adopts a\nhybrid optical/photonic design, combining the reconfigurability of integrated\nphotonics with the ultra-parallelism of free-space diffractive systems.\nExtensive evaluations demonstrate that MDR-HDONN has digital-comparable\naccuracy on various task adaptations with 74x faster speed and 194x lower\nenergy. Compared to prior DONNs, MDR-HDONN shows exponentially larger\nfunctional space with 5x faster training speed, paving the way for a new\nparadigm of versatile, composable, hybrid optical/photonic AI computing. We\nwill open-source our codes.",
      "tldr_zh": "本文提出了一种多维可重构、可物理组合的混合衍射光学神经网络（MDR-HDONN），旨在解决传统 DONNs 在固定光学结构下缺乏可重构性的问题，从而适应动态 AI 工作负载。MDR-HDONN 通过全系统可学习性和可微学习，重新利用已制作的固定光学硬件，并结合集成光子学的可重构性与自由空间衍射系统的超并行性，实现功能空间的指数级扩展和任务适应性。实验结果显示，该系统在各种任务上与数字系统准确率相当，但速度提升 74 倍、能量消耗降低 194 倍，且比现有 DONNs 训练速度快 5 倍，为多功能混合光学/光子 AI 计算开辟新范式。",
      "categories": [
        "physics.optics",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "physics.optics",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.05748v1",
      "published_date": "2024-11-08 18:08:49 UTC",
      "updated_date": "2024-11-08 18:08:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:43:23.764715"
    },
    {
      "arxiv_id": "2411.05746v2",
      "title": "Continuous-Time Analysis of Adaptive Optimization and Normalization",
      "title_zh": "翻译失败",
      "authors": [
        "Rhys Gould",
        "Hidenori Tanaka"
      ],
      "abstract": "Adaptive optimization algorithms, particularly Adam and its variant AdamW,\nare fundamental components of modern deep learning. However, their training\ndynamics lack comprehensive theoretical understanding, with limited insight\ninto why common practices -- such as specific hyperparameter choices and\nnormalization layers -- contribute to successful generalization. This work\npresents a continuous-time formulation of Adam and AdamW, facilitating a\ntractable analysis of training dynamics that can shed light on such practical\nquestions. We theoretically derive a stable region for Adam's hyperparameters\n$(\\beta, \\gamma)$ that ensures bounded updates, empirically verifying these\npredictions by observing unstable exponential parameter growth outside of this\nstable region. Furthermore, we theoretically justify the success of\nnormalization layers by uncovering an implicit meta-adaptive effect of\nscale-invariant architectural components. This insight leads to an explicit\noptimizer, $2$-Adam, which we generalize to $k$-Adam -- an optimizer that\napplies an adaptive normalization procedure $k$ times, encompassing Adam\n(corresponding to $k=1$) and Adam with a normalization layer (corresponding to\n$k=2$). Overall, our continuous-time formulation of Adam facilitates a\nprincipled analysis, offering deeper understanding of optimal hyperparameter\nchoices and architectural decisions in modern deep learning.",
      "tldr_zh": "本文提出了一种连续时间表述（continuous-time formulation）来分析自适应优化算法Adam和AdamW的训练动态，旨在揭示超参数选择和归一化层如何促进深度学习的泛化。通过理论推导，确定了Adam超参数（β, γ）的稳定区域，确保参数更新bounded，并在实验中验证了超出该区域会导致不稳定指数增长。同时，揭示了归一化层的隐式meta-adaptive效果，进而提出k-Adam优化器（包括2-Adam），通过多次自适应归一化过程扩展Adam的功能，提供更深入的架构决策指导。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05746v2",
      "published_date": "2024-11-08 18:07:55 UTC",
      "updated_date": "2024-12-19 21:24:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:43:35.279218"
    },
    {
      "arxiv_id": "2411.05742v1",
      "title": "Topology-aware Reinforcement Feature Space Reconstruction for Graph Data",
      "title_zh": "针对图数据的拓扑感知强化特征空间重构",
      "authors": [
        "Wangyang Ying",
        "Haoyue Bai",
        "Kunpeng Liu",
        "Yanjie Fu"
      ],
      "abstract": "Feature space is an environment where data points are vectorized to represent\nthe original dataset. Reconstructing a good feature space is essential to\naugment the AI power of data, improve model generalization, and increase the\navailability of downstream ML models. Existing literature, such as feature\ntransformation and feature selection, is labor-intensive (e.g., heavy reliance\non empirical experience) and mostly designed for tabular data. Moreover, these\nmethods regard data samples as independent, which ignores the unique\ntopological structure when applied to graph data, thus resulting in a\nsuboptimal reconstruction feature space. Can we consider the topological\ninformation to automatically reconstruct feature space for graph data without\nheavy experiential knowledge? To fill this gap, we leverage topology-aware\nreinforcement learning to automate and optimize feature space reconstruction\nfor graph data. Our approach combines the extraction of core subgraphs to\ncapture essential structural information with a graph neural network (GNN) to\nencode topological features and reduce computing complexity. Then we introduce\nthree reinforcement agents within a hierarchical structure to systematically\ngenerate meaningful features through an iterative process, effectively\nreconstructing the feature space. This framework provides a principled solution\nfor attributed graph feature space reconstruction. The extensive experiments\ndemonstrate the effectiveness and efficiency of including topological\nawareness.",
      "tldr_zh": "该论文针对图数据（graph data）的特征空间（feature space）重建问题，指出现有方法依赖经验且忽略拓扑结构，导致次优结果。作者提出一种拓扑感知强化学习（topology-aware reinforcement learning）框架，通过提取核心子图（core subgraphs）并使用图神经网络（Graph Neural Network, GNN）编码拓扑特征，以减少计算复杂度和自动生成有意义的特征。框架引入三个分层强化代理（reinforcement agents），通过迭代过程系统优化特征空间重建；实验结果证明，该方法显著提高了图数据的特征重建有效性和效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05742v1",
      "published_date": "2024-11-08 18:01:05 UTC",
      "updated_date": "2024-11-08 18:01:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:43:46.274831"
    },
    {
      "arxiv_id": "2411.05735v2",
      "title": "Aioli: A Unified Optimization Framework for Language Model Data Mixing",
      "title_zh": "Aioli：用于语言模型数据混合的统一优化框架",
      "authors": [
        "Mayee F. Chen",
        "Michael Y. Hu",
        "Nicholas Lourie",
        "Kyunghyun Cho",
        "Christopher Ré"
      ],
      "abstract": "Language model performance depends on identifying the optimal mixture of data\ngroups to train on (e.g., law, code, math). Prior work has proposed a diverse\nset of methods to efficiently learn mixture proportions, ranging from fitting\nregression models over training runs to dynamically updating proportions\nthroughout training. Surprisingly, we find that no existing method consistently\noutperforms a simple stratified sampling baseline in terms of average test\nperplexity. To understand this inconsistency, we unify existing methods into a\nstandard framework, showing they are equivalent to solving a common\noptimization problem: minimize average loss subject to a method-specific mixing\nlaw -- an implicit assumption on the relationship between loss and mixture\nproportions. This framework suggests that measuring the fidelity of a method's\nmixing law can offer insights into its performance. Empirically, we find that\nexisting methods set their mixing law parameters inaccurately, resulting in the\ninconsistent mixing performance we observe. Using this insight, we derive a new\nonline method named Aioli, which directly estimates the mixing law parameters\nthroughout training and uses them to dynamically adjust proportions. Aioli\noutperforms stratified sampling on 6 out of 6 datasets by an average of 0.27\ntest perplexity points, whereas existing methods fail to consistently beat\nstratified sampling, doing up to 6.9 points worse. Moreover, in a practical\nsetting where proportions are learned on shorter runs due to computational\nconstraints, Aioli can dynamically adjust these proportions over the full\ntraining run, consistently improving performance over existing methods by up to\n12.012 test perplexity points.",
      "tldr_zh": "该研究发现，现有的语言模型数据混合方法（如拟合回归模型或动态更新比例）未能一致优于简单的分层采样基准，因为它们对混合法则参数的估计不准确。作者提出一个统一的优化框架，将这些方法等价于最小化平均损失的优化问题，并据此开发新方法Aioli，该方法通过在线估计混合法则参数并动态调整训练比例。实验结果显示，Aioli在6个数据集上平均比分层采样提高0.27 test perplexity点，并在实际计算约束下优于现有方法最多12.012点。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 Camera Ready",
      "pdf_url": "http://arxiv.org/pdf/2411.05735v2",
      "published_date": "2024-11-08 17:50:24 UTC",
      "updated_date": "2025-04-21 03:50:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:43:58.086294"
    },
    {
      "arxiv_id": "2411.05718v1",
      "title": "A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics",
      "title_zh": "翻译失败",
      "authors": [
        "Puze Liu",
        "Jonas Günster",
        "Niklas Funk",
        "Simon Gröger",
        "Dong Chen",
        "Haitham Bou-Ammar",
        "Julius Jankowski",
        "Ante Marić",
        "Sylvain Calinon",
        "Andrej Orsula",
        "Miguel Olivares-Mendez",
        "Hongyi Zhou",
        "Rudolf Lioutikov",
        "Gerhard Neumann",
        "Amarildo Likmeta Amirhossein Zhalehmehrabi",
        "Thomas Bonenfant",
        "Marcello Restelli",
        "Davide Tateo",
        "Ziyuan Liu",
        "Jan Peters"
      ],
      "abstract": "Machine learning methods have a groundbreaking impact in many application\ndomains, but their application on real robotic platforms is still limited.\nDespite the many challenges associated with combining machine learning\ntechnology with robotics, robot learning remains one of the most promising\ndirections for enhancing the capabilities of robots. When deploying\nlearning-based approaches on real robots, extra effort is required to address\nthe challenges posed by various real-world factors. To investigate the key\nfactors influencing real-world deployment and to encourage original solutions\nfrom different researchers, we organized the Robot Air Hockey Challenge at the\nNeurIPS 2023 conference. We selected the air hockey task as a benchmark,\nencompassing low-level robotics problems and high-level tactics. Different from\nother machine learning-centric benchmarks, participants need to tackle\npractical challenges in robotics, such as the sim-to-real gap, low-level\ncontrol issues, safety problems, real-time requirements, and the limited\navailability of real-world data. Furthermore, we focus on a dynamic\nenvironment, removing the typical assumption of quasi-static motions of other\nreal-world benchmarks. The competition's results show that solutions combining\nlearning-based approaches with prior knowledge outperform those relying solely\non data when real-world deployment is challenging. Our ablation study reveals\nwhich real-world factors may be overlooked when building a learning-based\nsolution. The successful real-world air hockey deployment of best-performing\nagents sets the foundation for future competitions and follow-up research\ndirections.",
      "tldr_zh": "本论文回顾了 Robot Air Hockey Challenge，这是一个 benchmark，用于评估机器学习在真实机器人环境中的鲁棒性、可靠性和安全性。比赛以空气曲棍球任务为平台，参与者需应对实际挑战，包括 sim-to-real gap、低级控制问题、安全问题、实时要求以及数据有限性。结果表明，结合学习-based 方法与先验知识的解决方案在真实部署中显著优于纯数据驱动方法；此外，通过消融研究，论文揭示了可能被忽略的真实世界因素，并为未来机器人学习竞赛和研究方向奠定基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accept at NeurIPS 2024 Dataset and Benchmark Track",
      "pdf_url": "http://arxiv.org/pdf/2411.05718v1",
      "published_date": "2024-11-08 17:20:47 UTC",
      "updated_date": "2024-11-08 17:20:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:44:11.108887"
    },
    {
      "arxiv_id": "2411.05903v1",
      "title": "Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Ben Koska",
        "Mojmír Horváth"
      ],
      "abstract": "We present a novel 4.5B parameter small language model that can handle\nmultiple input and output modalities, including text, images, videos, and\naudio. Despite its small size, the model achieves near state-of-the-art\nperformance on a variety of tasks, demonstrating the potential of multi-modal\nmodels to tackle complex real-world problems. Our approach leverages recent\nadvancements in language modeling and multi-task learning to create a versatile\nand high-performing model that can even be deployed for edge inference.\nExperimental results show the model's strong performance across multiple\nbenchmarks, paving the way for further progress in multi-modal artificial\nintelligence.",
      "tldr_zh": "我们提出一个 4.5B parameter 的真正多模态小型语言模型，能够处理文本、图像、视频和音频等多种输入输出模态。该模型利用语言建模和多任务学习的最新进展，实现高性能并适用于边缘推理。尽管规模小巧，它在各种任务上达到了接近 state-of-the-art 的表现，实验结果显示在多个基准上表现出色，推动了多模态人工智能的进一步发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05903v1",
      "published_date": "2024-11-08 17:15:17 UTC",
      "updated_date": "2024-11-08 17:15:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:44:22.824353"
    },
    {
      "arxiv_id": "2411.16690v1",
      "title": "Benefits and Risks of Using ChatGPT4 as a Teaching Assistant for Computer Science Students",
      "title_zh": "使用 ChatGPT4 作为计算机科学学生教学助理的益处与风险",
      "authors": [
        "Yaiza Aragonés-Soria",
        "Julia Kotovich",
        "Chitsutha Soomlek",
        "Manuel Oriol"
      ],
      "abstract": "Upon release, ChatGPT3.5 shocked the software engineering community by its\nability to generate answers to specialized questions about coding. Immediately,\nmany educators wondered if it was possible to use the chatbot as a support tool\nthat helps students answer their programming questions. This article evaluates\nthis possibility at three levels: fundamental Computer Science knowledge (basic\nalgorithms and data structures), core competency (design patterns), and\nadvanced knowledge (quantum computing). In each case, we ask normalized\nquestions several times to ChatGPT3.5, then look at the correctness of answers,\nand finally check if this creates issues. The main result is that the\nperformances of ChatGPT3.5 degrades drastically as the specialization of the\ndomain increases: for basic algorithms it returns answers that are almost\nalways correct, for design patterns the generated code contains many code\nsmells and is generally of low quality, but it is still sometimes able to fix\nit (if asked), and for quantum computing it is often blatantly wrong.",
      "tldr_zh": "本研究评估了使用 ChatGPT4（实际测试基于 ChatGPT3.5）作为计算机科学学生教学助理的益处和风险，焦点在于其回答编程相关问题的准确性。研究者通过在三个层面提问多次——基础知识（如基本 algorithms and data structures）、核心能力（如 design patterns）和高级知识（如 quantum computing）——来检查答案的正确性和潜在问题。结果显示，ChatGPT3.5 在基础层面表现优秀，几乎总是正确，但在 design patterns 上生成代码常有 code smells 和低质量问题（虽有时可修复），而在 quantum computing 上则经常出现明显错误。总体而言，这突显了 ChatGPT 在教育中的潜力及其专业化领域的局限性，提醒教育者需谨慎应用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CY",
      "comment": "This paper was finished on the 17th of June of 2023",
      "pdf_url": "http://arxiv.org/pdf/2411.16690v1",
      "published_date": "2024-11-08 17:11:10 UTC",
      "updated_date": "2024-11-08 17:11:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:44:35.722833"
    },
    {
      "arxiv_id": "2411.05698v1",
      "title": "Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc Explainability in Image Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio De Santis",
        "Riccardo Campi",
        "Matteo Bianchi",
        "Marco Brambilla"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) have seen significant performance\nimprovements in recent years. However, due to their size and complexity, they\nfunction as black-boxes, leading to transparency concerns. State-of-the-art\nsaliency methods generate local explanations that highlight the area in the\ninput image where a class is identified but cannot explain how a concept of\ninterest contributes to the prediction, which is essential for bias mitigation.\nOn the other hand, concept-based methods, such as TCAV (Testing with Concept\nActivation Vectors), provide insights into how sensitive is the network to a\nconcept, but cannot compute its attribution in a specific prediction nor show\nits location within the input image. This paper introduces a novel post-hoc\nexplainability framework, Visual-TCAV, which aims to bridge the gap between\nthese methods by providing both local and global explanations for CNN-based\nimage classification. Visual-TCAV uses Concept Activation Vectors (CAVs) to\ngenerate saliency maps that show where concepts are recognized by the network.\nMoreover, it can estimate the attribution of these concepts to the output of\nany class using a generalization of Integrated Gradients. This framework is\nevaluated on popular CNN architectures, with its validity further confirmed via\nexperiments where ground truth for explanations is known, and a comparison with\nTCAV. Our code will be made available soon.",
      "tldr_zh": "本论文针对卷积神经网络(CNNs)作为黑箱模型的问题，提出Visual-TCAV框架，以解决现有saliency methods无法解释概念贡献以及TCAV无法提供具体预测归因和位置的问题。该框架结合Concept Activation Vectors (CAVs)生成saliency maps，展示概念在图像中的识别位置，并使用Integrated Gradients的泛化方法估计算概念对输出类别的归因，从而提供局部和全局解释。在流行CNN架构上进行的实验验证了其有效性，并与TCAV进行了比较，证明了框架的准确性和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint currently under review",
      "pdf_url": "http://arxiv.org/pdf/2411.05698v1",
      "published_date": "2024-11-08 16:52:52 UTC",
      "updated_date": "2024-11-08 16:52:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:44:47.155834"
    },
    {
      "arxiv_id": "2411.05691v1",
      "title": "Asterisk*: Keep it Simple",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew Semenov"
      ],
      "abstract": "This paper describes Asterisk, a compact GPT-based model for generating text\nembeddings. The model uses a minimalist architecture with two layers, two\nattention heads, and 256 embedding dimensions. By applying knowledge\ndistillation from larger pretrained models, we explore the trade-offs between\nmodel size and performance while minimizing computational and memory\nrequirements. The model is primarily evaluated and optimized for classification\ntasks, with experimental results showing its moderate performance in zero-shot\nclassification across various downstream applications. With additional\nconfiguration, the model performance can approach or even surpass that of\nlarger architectures on specific classification tasks.",
      "tldr_zh": "这篇论文介绍了 Asterisk*，一个基于 GPT 的紧凑文本嵌入生成模型，采用简单架构（两层、两个注意力头、256 维嵌入），旨在通过知识蒸馏从更大预训练模型中学习，以最小化计算和内存需求。模型主要针对分类任务进行优化和评估，在 zero-shot classification 等下游应用中表现出中等性能。实验结果显示，通过额外配置，Asterisk* 在特定分类任务上可达到或超过更大架构的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05691v1",
      "published_date": "2024-11-08 16:42:33 UTC",
      "updated_date": "2024-11-08 16:42:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:44:58.642661"
    },
    {
      "arxiv_id": "2411.05683v1",
      "title": "Data-Driven Distributed Common Operational Picture from Heterogeneous Platforms using Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Indranil Sur",
        "Aswin Raghavan",
        "Abrar Rahman",
        "James Z Hare",
        "Daniel Cassenti",
        "Carl Busart"
      ],
      "abstract": "The integration of unmanned platforms equipped with advanced sensors promises\nto enhance situational awareness and mitigate the \"fog of war\" in military\noperations. However, managing the vast influx of data from these platforms\nposes a significant challenge for Command and Control (C2) systems. This study\npresents a novel multi-agent learning framework to address this challenge. Our\nmethod enables autonomous and secure communication between agents and humans,\nwhich in turn enables real-time formation of an interpretable Common\nOperational Picture (COP). Each agent encodes its perceptions and actions into\ncompact vectors, which are then transmitted, received and decoded to form a COP\nencompassing the current state of all agents (friendly and enemy) on the\nbattlefield. Using Deep Reinforcement Learning (DRL), we jointly train COP\nmodels and agent's action selection policies. We demonstrate resilience to\ndegraded conditions such as denied GPS and disrupted communications.\nExperimental validation is performed in the Starcraft-2 simulation environment\nto evaluate the precision of the COPs and robustness of policies. We report\nless than 5% error in COPs and policies resilient to various adversarial\nconditions. In summary, our contributions include a method for autonomous COP\nformation, increased resilience through distributed prediction, and joint\ntraining of COP models and multi-agent RL policies. This research advances\nadaptive and resilient C2, facilitating effective control of heterogeneous\nunmanned platforms.",
      "tldr_zh": "这篇论文提出了一种基于 Multi-Agent Reinforcement Learning 的多智能体学习框架，用于从异构平台生成数据驱动的分布式 Common Operational Picture (COP)，以提升军事操作中的 situational awareness 并缓解 \"fog of war\"。框架允许代理之间及代理与人类进行自主、安全通信，每个代理将感知和行动编码成紧凑向量进行传输和解码，从而实时形成可解释的 COP，包括所有代理（友方和敌方）的当前状态。使用 Deep Reinforcement Learning (DRL) 联合训练 COP 模型和代理行动策略，并在 Starcraft-2 模拟环境中验证，实现了 COP 错误率小于 5% 和对 GPS 拒绝及通信中断等对抗条件的弹性。主要贡献包括自主 COP 形成方法、通过分布式预测提升系统弹性，以及 COP 模型与多智能体 RL 策略的联合训练。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "29th International Command and Control Research & Technology\n  Symposium",
      "pdf_url": "http://arxiv.org/pdf/2411.05683v1",
      "published_date": "2024-11-08 16:31:22 UTC",
      "updated_date": "2024-11-08 16:31:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:45:12.509882"
    },
    {
      "arxiv_id": "2411.05679v3",
      "title": "Tell What You Hear From What You See -- Video to Audio Generation Through Text",
      "title_zh": "翻译失败",
      "authors": [
        "Xiulong Liu",
        "Kun Su",
        "Eli Shlizerman"
      ],
      "abstract": "The content of visual and audio scenes is multi-faceted such that a video can\nbe paired with various audio and vice-versa. Thereby, in video-to-audio\ngeneration task, it is imperative to introduce steering approaches for\ncontrolling the generated audio. While Video-to-Audio generation is a\nwell-established generative task, existing methods lack such controllability.\nIn this work, we propose VATT, a multi-modal generative framework that takes a\nvideo and an optional text prompt as input, and generates audio and optional\ntextual description of the audio. Such a framework has two advantages: i)\nVideo-to-Audio generation process can be refined and controlled via text which\ncomplements the context of visual information, and ii) The model can suggest\nwhat audio to generate for the video by generating audio captions. VATT\nconsists of two key modules: VATT Converter, a LLM that is fine-tuned for\ninstructions and includes a projection layer that maps video features to the\nLLM vector space; and VATT Audio, a transformer that generates audio tokens\nfrom visual frames and from optional text prompt using iterative parallel\ndecoding. The audio tokens are converted to a waveform by pretrained neural\ncodec. Experiments show that when VATT is compared to existing video-to-audio\ngeneration methods in objective metrics, it achieves competitive performance\nwhen the audio caption is not provided. When the audio caption is provided as a\nprompt, VATT achieves even more refined performance (lowest KLD score of 1.41).\nFurthermore, subjective studies show that VATT Audio has been chosen as\npreferred generated audio than audio generated by existing methods. VATT\nenables controllable video-to-audio generation through text as well as\nsuggesting text prompts for videos through audio captions, unlocking novel\napplications such as text-guided video-to-audio generation and video-to-audio\ncaptioning.",
      "tldr_zh": "这篇论文提出了 VATT 框架，用于从视频和可选文本提示生成音频和音频描述，实现可控的视频到音频生成。VATT 包括两个关键模块：VATT Converter（一个微调的 LLM，带有投影层将视频特征映射到 LLM 向量空间）和 VATT Audio（一个 transformer，通过迭代并行解码从视频帧和文本提示生成音频标记，然后由预训练神经编解码器转换为波形）。实验结果显示，VATT 在客观指标上与现有方法竞争表现相当，并在提供音频描述时显著提升性能（最低 KLD 得分 1.41），主观研究也表明其生成的音频更受欢迎。该框架通过文本引导和音频描述功能，开启了视频到音频生成和视频到音频描述的新应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2024. Project page: https://dragonliu1995.github.io/VATT-home",
      "pdf_url": "http://arxiv.org/pdf/2411.05679v3",
      "published_date": "2024-11-08 16:29:07 UTC",
      "updated_date": "2025-04-04 21:50:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:45:23.924435"
    },
    {
      "arxiv_id": "2411.05676v1",
      "title": "Improving Molecular Graph Generation with Flow Matching and Optimal Transport",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyang Hou",
        "Tian Zhu",
        "Milong Ren",
        "Dongbo Bu",
        "Xin Gao",
        "Chunming Zhang",
        "Shiwei Sun"
      ],
      "abstract": "Generating molecular graphs is crucial in drug design and discovery but\nremains challenging due to the complex interdependencies between nodes and\nedges. While diffusion models have demonstrated their potentiality in molecular\ngraph design, they often suffer from unstable training and inefficient\nsampling. To enhance generation performance and training stability, we propose\nGGFlow, a discrete flow matching generative model incorporating optimal\ntransport for molecular graphs and it incorporates an edge-augmented graph\ntransformer to enable the direct communications among chemical bounds.\nAdditionally, GGFlow introduces a novel goal-guided generation framework to\ncontrol the generative trajectory of our model, aiming to design novel\nmolecular structures with the desired properties. GGFlow demonstrates superior\nperformance on both unconditional and conditional molecule generation tasks,\noutperforming existing baselines and underscoring its effectiveness and\npotential for wider application.",
      "tldr_zh": "本文针对分子图生成在药物设计中的挑战（如节点和边的复杂相互依赖），提出了一种名为 GGFlow 的离散流匹配（flow matching）生成模型，该模型整合了 optimal transport 以提升训练稳定性和生成性能。GGFlow 采用 edge-augmented graph transformer 实现化学键间的直接通信，并引入 goal-guided generation framework 来引导生成轨迹，从而设计出具有所需属性的新型分子结构。实验结果表明，GGFlow 在无条件和条件分子生成任务上超越现有基线，展示了其在分子设计领域的显著优势和应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05676v1",
      "published_date": "2024-11-08 16:27:27 UTC",
      "updated_date": "2024-11-08 16:27:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:45:36.480337"
    },
    {
      "arxiv_id": "2411.07165v1",
      "title": "Acoustic-based 3D Human Pose Estimation Robust to Human Position",
      "title_zh": "翻译失败",
      "authors": [
        "Yusuke Oumi",
        "Yuto Shibata",
        "Go Irie",
        "Akisato Kimura",
        "Yoshimitsu Aoki",
        "Mariko Isogawa"
      ],
      "abstract": "This paper explores the problem of 3D human pose estimation from only\nlow-level acoustic signals. The existing active acoustic sensing-based approach\nfor 3D human pose estimation implicitly assumes that the target user is\npositioned along a line between loudspeakers and a microphone. Because\nreflection and diffraction of sound by the human body cause subtle acoustic\nsignal changes compared to sound obstruction, the existing model degrades its\naccuracy significantly when subjects deviate from this line, limiting its\npracticality in real-world scenarios. To overcome this limitation, we propose a\nnovel method composed of a position discriminator and reverberation-resistant\nmodel. The former predicts the standing positions of subjects and applies\nadversarial learning to extract subject position-invariant features. The latter\nutilizes acoustic signals before the estimation target time as references to\nenhance robustness against the variations in sound arrival times due to\ndiffraction and reflection. We construct an acoustic pose estimation dataset\nthat covers diverse human locations and demonstrate through experiments that\nour proposed method outperforms existing approaches.",
      "tldr_zh": "本论文探讨了基于声学信号的3D human pose estimation问题，现有方法假设目标用户位于扬声器和麦克风之间的直线上，导致用户位置偏差时准确性显著下降。针对这一局限，研究提出了一种新方法，包括position discriminator用于预测用户站立位置并通过对抗学习提取位置无关特征，以及reverberation-resistant model利用目标时间前声学信号作为参考，提高对声音反射和衍射变化的鲁棒性。作者构建了一个覆盖多样人类位置的声学pose estimation数据集，并通过实验证明，该方法在各种场景下优于现有方法。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at BMVC2024",
      "pdf_url": "http://arxiv.org/pdf/2411.07165v1",
      "published_date": "2024-11-08 15:56:12 UTC",
      "updated_date": "2024-11-08 15:56:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:45:47.575635"
    },
    {
      "arxiv_id": "2411.05898v1",
      "title": "Integrating Object Detection Modality into Visual Language Model for Enhanced Autonomous Driving Agent",
      "title_zh": "将物体检测模态集成到视觉语言模型中以增强自动驾驶代理",
      "authors": [
        "Linfeng He",
        "Yiming Sun",
        "Sihao Wu",
        "Jiaxu Liu",
        "Xiaowei Huang"
      ],
      "abstract": "In this paper, we propose a novel framework for enhancing visual\ncomprehension in autonomous driving systems by integrating visual language\nmodels (VLMs) with additional visual perception module specialised in object\ndetection. We extend the Llama-Adapter architecture by incorporating a\nYOLOS-based detection network alongside the CLIP perception network, addressing\nlimitations in object detection and localisation. Our approach introduces\ncamera ID-separators to improve multi-view processing, crucial for\ncomprehensive environmental awareness. Experiments on the DriveLM visual\nquestion answering challenge demonstrate significant improvements over baseline\nmodels, with enhanced performance in ChatGPT scores, BLEU scores, and CIDEr\nmetrics, indicating closeness of model answer to ground truth. Our method\nrepresents a promising step towards more capable and interpretable autonomous\ndriving systems. Possible safety enhancement enabled by detection modality is\nalso discussed.",
      "tldr_zh": "本文提出一个新框架，将 Visual Language Models (VLMs) 与专门的物体检测模块整合，以提升自动驾驶代理的视觉理解能力。框架扩展了 Llama-Adapter 架构，通过加入 YOLOS-based detection network 和 CLIP 感知网络，并引入 camera ID-separators 来优化多视图处理，从而解决物体检测和定位的局限性。在 DriveLM 视觉问答挑战中，实验结果显示模型在 ChatGPT scores、BLEU scores 和 CIDEr metrics 上均有显著改进。该方法为更可靠且可解释的自动驾驶系统铺平道路，并探讨了检测模块对安全的潜在增强。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted by SafeGenAI workshop of NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.05898v1",
      "published_date": "2024-11-08 15:50:30 UTC",
      "updated_date": "2024-11-08 15:50:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:45:59.692964"
    },
    {
      "arxiv_id": "2411.05897v2",
      "title": "Humans and Large Language Models in Clinical Decision Support: A Study with Medical Calculators",
      "title_zh": "人类和大语言模型在临床决策支持中的作用：一项使用医疗计算器的研究",
      "authors": [
        "Nicholas Wan",
        "Qiao Jin",
        "Joey Chan",
        "Guangzhi Xiong",
        "Serina Applebaum",
        "Aidan Gilson",
        "Reid McMurry",
        "R. Andrew Taylor",
        "Aidong Zhang",
        "Qingyu Chen",
        "Zhiyong Lu"
      ],
      "abstract": "Although large language models (LLMs) have been assessed for general medical\nknowledge using licensing exams, their ability to support clinical\ndecision-making, such as selecting medical calculators, remains uncertain. We\nassessed nine LLMs, including open-source, proprietary, and domain-specific\nmodels, with 1,009 multiple-choice question-answer pairs across 35 clinical\ncalculators and compared LLMs to humans on a subset of questions. While the\nhighest-performing LLM, OpenAI o1, provided an answer accuracy of 66.0% (CI:\n56.7-75.3%) on the subset of 100 questions, two human annotators nominally\noutperformed LLMs with an average answer accuracy of 79.5% (CI: 73.5-85.0%).\nUltimately, we evaluated medical trainees and LLMs in recommending medical\ncalculators across clinical scenarios like risk stratification and diagnosis.\nWith error analysis showing that the highest-performing LLMs continue to make\nmistakes in comprehension (49.3% of errors) and calculator knowledge (7.1% of\nerrors), our findings highlight that LLMs are not superior to humans in\ncalculator recommendation.",
      "tldr_zh": "该研究评估了九个大型语言模型（LLMs，包括开源、专有和领域特定模型）在临床决策支持中的性能，焦点是推荐医疗计算器，使用了1,009个多选题对覆盖35个临床计算器。实验结果显示，表现最好的LLM（OpenAI o1）在子集问题上的回答准确率为66.0%，而两名人类注释者的平均准确率为79.5%，表明人类在这一任务中优于LLMs。进一步的错误分析揭示，LLMs的错误主要源于理解问题（49.3%）和计算器知识缺口（7.1%），强调了LLMs在医疗计算器推荐方面尚未超越人类的局限性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.05897v2",
      "published_date": "2024-11-08 15:50:19 UTC",
      "updated_date": "2025-03-21 21:13:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:46:11.041895"
    },
    {
      "arxiv_id": "2411.05653v1",
      "title": "The influence of persona and conversational task on social interactions with a LLM-controlled embodied conversational agent",
      "title_zh": "翻译失败",
      "authors": [
        "Leon O. H. Kroczek",
        "Alexander May",
        "Selina Hettenkofer",
        "Andreas Ruider",
        "Bernd Ludwig",
        "Andreas Mühlberger"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nconversational tasks. Embodying an LLM as a virtual human allows users to\nengage in face-to-face social interactions in Virtual Reality. However, the\ninfluence of person- and task-related factors in social interactions with\nLLM-controlled agents remains unclear. In this study, forty-six participants\ninteracted with a virtual agent whose persona was manipulated as extravert or\nintrovert in three different conversational tasks (small talk, knowledge test,\nconvincing). Social-evaluation, emotional experience, and realism were assessed\nusing ratings. Interactive engagement was measured by quantifying participants'\nwords and conversational turns. Finally, we measured participants' willingness\nto ask the agent for help during the knowledge test. Our findings show that the\nextraverted agent was more positively evaluated, elicited a more pleasant\nexperience and greater engagement, and was assessed as more realistic compared\nto the introverted agent. Whereas persona did not affect the tendency to ask\nfor help, participants were generally more confident in the answer when they\nhad help of the LLM. Variation of personality traits of LLM-controlled embodied\nvirtual agents, therefore, affects social-emotional processing and behavior in\nvirtual interactions. Embodied virtual agents allow the presentation of\nnaturalistic social encounters in a virtual environment.",
      "tldr_zh": "这篇论文探讨了LLM（Large Language Models）控制的虚拟代理的persona（个性，如外向或内向）和conversational task（对话任务，如small talk、knowledge test、convincing）对社交互动的影响。研究通过46名参与者在虚拟现实中与代理互动的实验，评估了社会评价、情感体验、真实性和互动参与度（如话语数量和回合）。结果显示，外向代理比内向代理获得更积极的评价、更愉快的体验、更高真实性和更大参与度，但persona不影响参与者寻求帮助的意愿，而LLM的帮助显著提高了他们在知识测试中的自信。总体而言，该研究证明了调整LLM控制代理的个性特征能影响虚拟社交中的情感处理和行为，为自然主义虚拟互动提供了新见解。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "11 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.05653v1",
      "published_date": "2024-11-08 15:49:42 UTC",
      "updated_date": "2024-11-08 15:49:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:46:24.268366"
    },
    {
      "arxiv_id": "2411.10475v1",
      "title": "Beyond object identification: How train drivers evaluate the risk of collision",
      "title_zh": "超越物体识别：火车司机如何评估碰撞风险",
      "authors": [
        "Romy Müller",
        "Judith Schmidt"
      ],
      "abstract": "When trains collide with obstacles, the consequences are often severe. To\nassess how artificial intelligence might contribute to avoiding collisions, we\nneed to understand how train drivers do it. What aspects of a situation do they\nconsider when evaluating the risk of collision? In the present study, we\nassumed that train drivers do not only identify potential obstacles but\ninterpret what they see in order to anticipate how the situation might unfold.\nHowever, to date it is unclear how exactly this is accomplished. Therefore, we\nassessed which cues train drivers use and what inferences they make. To this\nend, image-based expert interviews were conducted with 33 train drivers.\nParticipants saw images with potential obstacles, rated the risk of collision,\nand explained their evaluation. Moreover, they were asked how the situation\nwould need to change to decrease or increase collision risk. From their verbal\nreports, we extracted concepts about the potential obstacles, contexts, or\nconsequences, and assigned these concepts to various categories (e.g., people's\nidentity, location, movement, action, physical features, and mental states).\nThe results revealed that especially for people, train drivers reason about\ntheir actions and mental states, and draw relations between concepts to make\nfurther inferences. These inferences systematically differ between situations.\nOur findings emphasise the need to understand train drivers' risk evaluation\nprocesses when aiming to enhance the safety of both human and automatic train\noperation.",
      "tldr_zh": "本研究探讨火车司机评估碰撞风险的过程，超越单纯识别障碍物，而是通过解释潜在障碍（如人的行为、心理状态和位置等）的动态变化来预测风险。研究方法包括对33名司机的图像-based专家访谈，让他们评估图像中的碰撞风险、解释原因，并讨论如何调整情况以改变风险。结果显示，司机会系统性地推理概念间的关系，尤其在涉及人的场景中，关注其行动和心理状态，且这些推理因情境而异。该发现强调理解人类风险评估机制，以提升人工智能在火车操作安全中的应用。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.10475v1",
      "published_date": "2024-11-08 15:38:47 UTC",
      "updated_date": "2024-11-08 15:38:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:46:35.388610"
    },
    {
      "arxiv_id": "2411.05633v1",
      "title": "SynDroneVision: A Synthetic Dataset for Image-Based Drone Detection",
      "title_zh": "SynDroneVision：一个用于基于图像的无人机检测的合成数据集",
      "authors": [
        "Tamara R. Lenhard",
        "Andreas Weinmann",
        "Kai Franke",
        "Tobias Koch"
      ],
      "abstract": "Developing robust drone detection systems is often constrained by the limited\navailability of large-scale annotated training data and the high costs\nassociated with real-world data collection. However, leveraging synthetic data\ngenerated via game engine-based simulations provides a promising and\ncost-effective solution to overcome this issue. Therefore, we present\nSynDroneVision, a synthetic dataset specifically designed for RGB-based drone\ndetection in surveillance applications. Featuring diverse backgrounds, lighting\nconditions, and drone models, SynDroneVision offers a comprehensive training\nfoundation for deep learning algorithms. To evaluate the dataset's\neffectiveness, we perform a comparative analysis across a selection of recent\nYOLO detection models. Our findings demonstrate that SynDroneVision is a\nvaluable resource for real-world data enrichment, achieving notable\nenhancements in model performance and robustness, while significantly reducing\nthe time and costs of real-world data acquisition. SynDroneVision will be\npublicly released upon paper acceptance.",
      "tldr_zh": "该研究针对无人机检测系统的训练数据短缺和高成本问题，提出了一种基于游戏引擎生成的合成数据集SynDroneVision，用于RGB图像的无人机检测。该数据集包含多样化的背景、光照条件和无人机模型，提供全面的训练基础以提升深度学习算法的性能。实验通过比较多种YOLO检测模型，发现SynDroneVision显著提高了模型的准确性和鲁棒性，同时大幅降低了真实数据采集的时间和成本。该数据集将在论文接受后公开，以支持进一步的研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV)",
      "pdf_url": "http://arxiv.org/pdf/2411.05633v1",
      "published_date": "2024-11-08 15:22:49 UTC",
      "updated_date": "2024-11-08 15:22:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:46:46.383745"
    },
    {
      "arxiv_id": "2411.05618v1",
      "title": "Knowledge Distillation Neural Network for Predicting Car-following Behaviour of Human-driven and Autonomous Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Ayobami Adewale",
        "Chris Lee",
        "Amnir Hadachi",
        "Nicolly Lima da Silva"
      ],
      "abstract": "As we move towards a mixed-traffic scenario of Autonomous vehicles (AVs) and\nHuman-driven vehicles (HDVs), understanding the car-following behaviour is\nimportant to improve traffic efficiency and road safety. Using a real-world\ntrajectory dataset, this study uses descriptive and statistical analysis to\ninvestigate the car-following behaviours of three vehicle pairs: HDV-AV, AV-HDV\nand HDV-HDV in mixed traffic. The ANOVA test showed that car-following\nbehaviours across different vehicle pairs are statistically significant\n(p-value < 0.05).\n  We also introduce a data-driven Knowledge Distillation Neural Network (KDNN)\nmodel for predicting car-following behaviour in terms of speed. The KDNN model\ndemonstrates comparable predictive accuracy to its teacher network, a Long\nShort-Term Memory (LSTM) network, and outperforms both the standalone student\nnetwork, a Multilayer Perceptron (MLP), and traditional physics-based models\nlike the Gipps model. Notably, the KDNN model better prevents collisions,\nmeasured by minimum Time-to-Collision (TTC), and operates with lower\ncomputational power, making it ideal for AVs or driving simulators requiring\nefficient computing.",
      "tldr_zh": "本研究探讨了混合交通场景中Human-driven vehicles (HDVs)和Autonomous vehicles (AVs)的跟车行为，使用真实轨迹数据集进行描述性和统计分析，结果显示HDV-AV、AV-HDV和HDV-HDV三种车辆对的跟车行为具有统计显著差异（ANOVA测试，p-value < 0.05）。为了预测跟车行为的速度，研究引入了Knowledge Distillation Neural Network (KDNN)模型，该模型以Long Short-Term Memory (LSTM)网络作为教师网络，并通过知识蒸馏技术训练Multilayer Perceptron (MLP)作为学生网络。实验结果表明，KDNN模型的预测准确性与LSTM相当，且优于MLP和传统物理模型如Gipps模型，同时在防止碰撞（最小Time-to-Collision, TTC）方面表现更好，并具有更低的计算需求，适合应用于AVs或驾驶模拟器以提升交通效率和安全性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "27th IEEE International Conference on Intelligent Transportation\n  Systems",
      "pdf_url": "http://arxiv.org/pdf/2411.05618v1",
      "published_date": "2024-11-08 14:57:59 UTC",
      "updated_date": "2024-11-08 14:57:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:47:00.159250"
    },
    {
      "arxiv_id": "2411.05614v1",
      "title": "Acceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihong Liu",
        "Xin Xu",
        "Peng Qiao",
        "Dongsheng Li"
      ],
      "abstract": "Deep reinforcement learning has led to dramatic breakthroughs in the field of\nartificial intelligence for the past few years. As the amount of rollout\nexperience data and the size of neural networks for deep reinforcement learning\nhave grown continuously, handling the training process and reducing the time\nconsumption using parallel and distributed computing is becoming an urgent and\nessential desire. In this paper, we perform a broad and thorough investigation\non training acceleration methodologies for deep reinforcement learning based on\nparallel and distributed computing, providing a comprehensive survey in this\nfield with state-of-the-art methods and pointers to core references. In\nparticular, a taxonomy of literature is provided, along with a discussion of\nemerging topics and open issues. This incorporates learning system\narchitectures, simulation parallelism, computing parallelism, distributed\nsynchronization mechanisms, and deep evolutionary reinforcement learning.\nFurther, we compare 16 current open-source libraries and platforms with\ncriteria of facilitating rapid development. Finally, we extrapolate future\ndirections that deserve further research.",
      "tldr_zh": "这篇调查论文探讨了使用并行和分布式计算加速Deep Reinforcement Learning训练过程的各种方法，以应对海量数据和神经网络规模增长带来的挑战。论文提供了文献分类，包括学习系统架构、模拟并行、计算并行、分布式同步机制以及Deep Evolutionary Reinforcement Learning等主题，并讨论了新兴话题和开放问题。同时，通过比较16个当前开源库的开发便利性，论文总结了现有加速策略，并展望了未来研究方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted by ACM Computing Surveys",
      "pdf_url": "http://arxiv.org/pdf/2411.05614v1",
      "published_date": "2024-11-08 14:55:32 UTC",
      "updated_date": "2024-11-08 14:55:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:47:10.915066"
    },
    {
      "arxiv_id": "2411.05599v1",
      "title": "Expectation vs. Reality: Towards Verification of Psychological Games",
      "title_zh": "翻译失败",
      "authors": [
        "Marta Kwiatkowska",
        "Gethin Norman",
        "David Parker",
        "Gabriel Santos"
      ],
      "abstract": "Game theory provides an effective way to model strategic interactions among\nrational agents. In the context of formal verification, these ideas can be used\nto produce guarantees on the correctness of multi-agent systems, with a diverse\nrange of applications from computer security to autonomous driving.\nPsychological games (PGs) were developed as a way to model and analyse agents\nwith belief-dependent motivations, opening up the possibility to model how\nhuman emotions can influence behaviour. In PGs, players' utilities depend not\nonly on what actually happens (which strategies players choose to adopt), but\nalso on what the players had expected to happen (their belief as to the\nstrategies that would be played). Despite receiving much attention in fields\nsuch as economics and psychology, very little consideration has been given to\ntheir applicability to problems in computer science, nor to practical\nalgorithms and tool support. In this paper, we start to bridge that gap,\nproposing methods to solve PGs and implementing them within PRISM-games, a\nformal verification tool for stochastic games. We discuss how to model these\ngames, highlight specific challenges for their analysis and illustrate the\nusefulness of our approach on several case studies, including human behaviour\nin traffic scenarios.",
      "tldr_zh": "本论文探讨了心理游戏(Psychological Games, PGs)，这些游戏扩展了传统游戏理论，通过考虑代理的信念依赖动机（如情感影响行为），来建模战略互动并应用于形式验证。该研究提出算法和方法来解决 PGs，并将其实现到 PRISM-games 工具中，解决了建模和分析的特定挑战。通过案例研究，如交通场景中的人类行为，证明了该方法在计算机科学领域的实用性，并为多代理系统正确性提供了新保障。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05599v1",
      "published_date": "2024-11-08 14:41:52 UTC",
      "updated_date": "2024-11-08 14:41:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:47:23.221832"
    },
    {
      "arxiv_id": "2411.05894v1",
      "title": "SSSD: Simply-Scalable Speculative Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Michele Marzollo",
        "Jiawei Zhuang",
        "Niklas Roemer",
        "Lorenz K. Müller",
        "Lukas Cavigelli"
      ],
      "abstract": "Over the past year, Speculative Decoding has gained popularity as a technique\nfor accelerating Large Language Model inference. While several methods have\nbeen introduced, most struggle to deliver satisfactory performance at batch\nsizes typical for data centers ($\\geq 8$) and often involve significant\ndeployment complexities. In this work, we offer a theoretical explanation of\nhow Speculative Decoding can be effectively utilized with larger batch sizes.\nWe also introduce a method that integrates seamlessly into existing systems\nwithout additional training or the complexity of deploying a small LLM. In a\ncontinuous batching setting, we achieve a 4x increase in throughput without any\nlatency impact for short context generation, and a 1.7-2x improvement in both\nlatency and throughput for longer contexts.",
      "tldr_zh": "这篇论文探讨了 Speculative Decoding 作为加速 Large Language Model 推理的技术，但现有方法在大数据中心批次大小（≥8）下性能不佳，且部署复杂。\n作者提供了理论解释，并引入了 SSSD 方法，该方法无需额外训练或部署小 LLM，即可无缝集成到现有系统中。\n在连续批处理设置中，SSSD 实现了短上下文生成吞吐量提高 4 倍（无延迟影响），以及长上下文延迟和吞吐量提高 1.7-2 倍。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.05894v1",
      "published_date": "2024-11-08 14:23:02 UTC",
      "updated_date": "2024-11-08 14:23:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:47:35.099147"
    },
    {
      "arxiv_id": "2411.05586v1",
      "title": "Tangled Program Graphs as an alternative to DRL-based control algorithms for UAVs",
      "title_zh": "翻译失败",
      "authors": [
        "Hubert Szolc",
        "Karol Desnos",
        "Tomasz Kryjak"
      ],
      "abstract": "Deep reinforcement learning (DRL) is currently the most popular AI-based\napproach to autonomous vehicle control. An agent, trained for this purpose in\nsimulation, can interact with the real environment with a human-level\nperformance. Despite very good results in terms of selected metrics, this\napproach has some significant drawbacks: high computational requirements and\nlow explainability. Because of that, a DRL-based agent cannot be used in some\ncontrol tasks, especially when safety is the key issue. Therefore we propose to\nuse Tangled Program Graphs (TPGs) as an alternative for deep reinforcement\nlearning in control-related tasks. In this approach, input signals are\nprocessed by simple programs that are combined in a graph structure. As a\nresult, TPGs are less computationally demanding and their actions can be\nexplained based on the graph structure. In this paper, we present our studies\non the use of TPGs as an alternative for DRL in control-related tasks. In\nparticular, we consider the problem of navigating an unmanned aerial vehicle\n(UAV) through the unknown environment based solely on the on-board LiDAR\nsensor. The results of our work show promising prospects for the use of TPGs in\ncontrol related-tasks.",
      "tldr_zh": "本论文提出Tangled Program Graphs (TPGs)作为Deep Reinforcement Learning (DRL)控制算法的替代方案，用于无人驾驶航空器(UAV)控制，以解决DRL的高计算需求和低可解释性问题。TPGs通过简单程序在图结构中处理输入信号，实现更高效且可解释的决策，尤其适合安全关键任务。在实验中，TPGs应用于UAV在未知环境中的LiDAR传感器导航，显示出良好的性能和前景。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "The papers was accepted for the 2024 Signal Processing: Algorithms,\n  Architectures, Arrangements, and Applications (SPA) conference in Poznan,\n  Poland",
      "pdf_url": "http://arxiv.org/pdf/2411.05586v1",
      "published_date": "2024-11-08 14:20:29 UTC",
      "updated_date": "2024-11-08 14:20:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:47:46.864152"
    },
    {
      "arxiv_id": "2411.05565v1",
      "title": "Solving 7x7 Killall-Go with Seki Database",
      "title_zh": "翻译失败",
      "authors": [
        "Yun-Jui Tsai",
        "Ting Han Wei",
        "Chi-Huang Lin",
        "Chung-Chin Shih",
        "Hung Guei",
        "I-Chen Wu",
        "Ti-Rong Wu"
      ],
      "abstract": "Game solving is the process of finding the theoretical outcome for a game,\nassuming that all player choices are optimal. This paper focuses on a technique\nthat can reduce the heuristic search space significantly for 7x7 Killall-Go. In\nGo and Killall-Go, live patterns are stones that are protected from opponent\ncapture. Mutual life, also referred to as seki, is when both players' stones\nachieve life by sharing liberties with their opponent. Whichever player\nattempts to capture the opponent first will leave their own stones vulnerable.\nTherefore, it is critical to recognize seki patterns to avoid putting oneself\nin jeopardy. Recognizing seki can reduce the search depth significantly. In\nthis paper, we enumerate all seki patterns up to a predetermined area size,\nthen store these patterns into a seki table. This allows us to recognize seki\nduring search, which significantly improves solving efficiency for the game of\nKillall-Go. Experiments show that a day-long, unsolvable position can be solved\nin 482 seconds with the addition of a seki table. For general positions, a 10%\nto 20% improvement in wall clock time and node count is observed.",
      "tldr_zh": "这篇论文针对7x7 Killall-Go的游戏求解（Game solving）问题，提出了一种通过构建seki database的技术来显著减少启发式搜索空间。方法包括枚举所有seki patterns（互活模式）直到预定区域大小，并将这些模式存储到seki table中，以便在搜索过程中快速识别seki，从而避免不必要的计算。实验结果显示，该技术可以将原本需要一整天的求解位置缩短至482秒，对于一般位置，墙钟时间和节点计数分别改善10%到20%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by the Computers and Games conference (CG 2024)",
      "pdf_url": "http://arxiv.org/pdf/2411.05565v1",
      "published_date": "2024-11-08 13:40:36 UTC",
      "updated_date": "2024-11-08 13:40:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:47:59.424719"
    },
    {
      "arxiv_id": "2411.05564v1",
      "title": "Open-set object detection: towards unified problem formulation and benchmarking",
      "title_zh": "翻译失败",
      "authors": [
        "Hejer Ammar",
        "Nikita Kiselov",
        "Guillaume Lapouge",
        "Romaric Audigier"
      ],
      "abstract": "In real-world applications where confidence is key, like autonomous driving,\nthe accurate detection and appropriate handling of classes differing from those\nused during training are crucial. Despite the proposal of various unknown\nobject detection approaches, we have observed widespread inconsistencies among\nthem regarding the datasets, metrics, and scenarios used, alongside a notable\nabsence of a clear definition for unknown objects, which hampers meaningful\nevaluation. To counter these issues, we introduce two benchmarks: a unified\nVOC-COCO evaluation, and the new OpenImagesRoad benchmark which provides clear\nhierarchical object definition besides new evaluation metrics. Complementing\nthe benchmark, we exploit recent self-supervised Vision Transformers\nperformance, to improve pseudo-labeling-based OpenSet Object Detection (OSOD),\nthrough OW-DETR++. State-of-the-art methods are extensively evaluated on the\nproposed benchmarks. This study provides a clear problem definition, ensures\nconsistent evaluations, and draws new conclusions about effectiveness of OSOD\nstrategies.",
      "tldr_zh": "该论文针对 Open-set object detection (OSOD) 的现实应用（如自动驾驶）中，检测训练外未知物体的挑战，指出现有方法在数据集、指标和场景上存在不一致，并缺乏对未知物体的明确定义。研究者引入两个基准：统一的 VOC-COCO 评估和新的 OpenImagesRoad 基准，提供清晰的层次化物体定义和新评估指标，以实现标准化评估。同时，他们利用自监督 Vision Transformers 的性能改进伪标签-based OSOD 方法，提出 OW-DETR++ 模型，并在这些基准上对最先进方法进行广泛测试。结果显示，此研究为 OSOD 问题提供了清晰定义，确保一致性评估，并得出关于策略有效性的新结论。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ECCV 2024 Workshop: \"The 3rd Workshop for\n  Out-of-Distribution Generalization in Computer Vision Foundation Models\"",
      "pdf_url": "http://arxiv.org/pdf/2411.05564v1",
      "published_date": "2024-11-08 13:40:01 UTC",
      "updated_date": "2024-11-08 13:40:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:48:11.872861"
    },
    {
      "arxiv_id": "2411.05561v1",
      "title": "Training objective drives the consistency of representational similarity across datasets",
      "title_zh": "训练目标驱动表示性相似度在数据集间的保持一致",
      "authors": [
        "Laure Ciernik",
        "Lorenz Linhardt",
        "Marco Morik",
        "Jonas Dippel",
        "Simon Kornblith",
        "Lukas Muttenthaler"
      ],
      "abstract": "The Platonic Representation Hypothesis claims that recent foundation models\nare converging to a shared representation space as a function of their\ndownstream task performance, irrespective of the objectives and data modalities\nused to train these models. Representational similarity is generally measured\nfor individual datasets and is not necessarily consistent across datasets.\nThus, one may wonder whether this convergence of model representations is\nconfounded by the datasets commonly used in machine learning. Here, we propose\na systematic way to measure how representational similarity between models\nvaries with the set of stimuli used to construct the representations. We find\nthat the objective function is the most crucial factor in determining the\nconsistency of representational similarities across datasets. Specifically,\nself-supervised vision models learn representations whose relative pairwise\nsimilarities generalize better from one dataset to another compared to those of\nimage classification or image-text models. Moreover, the correspondence between\nrepresentational similarities and the models' task behavior is\ndataset-dependent, being most strongly pronounced for single-domain datasets.\nOur work provides a framework for systematically measuring similarities of\nmodel representations across datasets and linking those similarities to\ndifferences in task behavior.",
      "tldr_zh": "该研究检验了 Platonic Representation Hypothesis，即基础模型的表示空间是否因下游任务性能而收敛，而非训练目标和数据模式。研究者提出一种系统方法，测量模型表示的代表性相似性如何随数据集变化，发现训练目标是决定相似性一致性的关键因素。相比图像分类或图像-文本模型，自监督视觉模型的相对配对相似性在不同数据集间泛化得更好。此外，该框架揭示了表示相似性与模型任务行为的对应关系依赖于数据集类型，并在单域数据集上最显著，为未来模型评估提供系统工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.05561v1",
      "published_date": "2024-11-08 13:35:45 UTC",
      "updated_date": "2024-11-08 13:35:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:48:24.069739"
    },
    {
      "arxiv_id": "2411.05557v1",
      "title": "A Nerf-Based Color Consistency Method for Remote Sensing Images",
      "title_zh": "翻译失败",
      "authors": [
        "Zongcheng Zuo",
        "Yuanxiang Li",
        "Tongtong Zhang"
      ],
      "abstract": "Due to different seasons, illumination, and atmospheric conditions, the\nphotometric of the acquired image varies greatly, which leads to obvious\nstitching seams at the edges of the mosaic image. Traditional methods can be\ndivided into two categories, one is absolute radiation correction and the other\nis relative radiation normalization. We propose a NeRF-based method of color\nconsistency correction for multi-view images, which weaves image features\ntogether using implicit expressions, and then re-illuminates feature space to\ngenerate a fusion image with a new perspective. We chose Superview-1 satellite\nimages and UAV images with large range and time difference for the experiment.\nExperimental results show that the synthesize image generated by our method has\nexcellent visual effect and smooth color transition at the edges.",
      "tldr_zh": "本论文针对遥感图像因季节、光照和大气条件导致的光度变化问题，提出了一种基于 NeRF 的颜色一致性校正方法，以解决传统绝对辐射校正和相对辐射归一化方法的局限性。该方法通过隐式表达式整合多视图图像特征，并重新照明特征空间，生成新视角的融合图像。实验结果显示，使用 Superview-1 卫星图像和 UAV 图像时，该方法产生的合成图像视觉效果出色，且边缘颜色过渡平滑。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07",
        "I.4.9; I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages, 4 figures, The International Geoscience and Remote Sensing\n  Symposium (IGARSS2023)",
      "pdf_url": "http://arxiv.org/pdf/2411.05557v1",
      "published_date": "2024-11-08 13:26:07 UTC",
      "updated_date": "2024-11-08 13:26:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:48:35.693934"
    },
    {
      "arxiv_id": "2411.05540v1",
      "title": "CRepair: CVAE-based Automatic Vulnerability Repair Technology",
      "title_zh": "CRepair：基于 CVAE 的自动漏洞修复技术",
      "authors": [
        "Penghui Liu",
        "Yingzhou Bi",
        "Jiangtao Huang",
        "Xinxin Jiang",
        "Lianmei Wang"
      ],
      "abstract": "Software vulnerabilities are flaws in computer software systems that pose\nsignificant threats to the integrity, security, and reliability of modern\nsoftware and its application data. These vulnerabilities can lead to\nsubstantial economic losses across various industries. Manual vulnerability\nrepair is not only time-consuming but also prone to errors. To address the\nchallenges of vulnerability repair, researchers have proposed various\nsolutions, with learning-based automatic vulnerability repair techniques\ngaining widespread attention. However, existing methods often focus on learning\nmore vulnerability data to improve repair outcomes, while neglecting the\ndiverse characteristics of vulnerable code, and suffer from imprecise\nvulnerability localization.To address these shortcomings, this paper proposes\nCRepair, a CVAE-based automatic vulnerability repair technology aimed at fixing\nsecurity vulnerabilities in system code. We first preprocess the vulnerability\ndata using a prompt-based method to serve as input to the model. Then, we apply\ncausal inference techniques to map the vulnerability feature data to\nprobability distributions. By employing multi-sample feature fusion, we capture\ndiverse vulnerability feature information. Finally, conditional control is used\nto guide the model in repairing the vulnerabilities.Experimental results\ndemonstrate that the proposed method significantly outperforms other benchmark\nmodels, achieving a perfect repair rate of 52%. The effectiveness of the\napproach is validated from multiple perspectives, advancing AI-driven code\nvulnerability repair and showing promising applications.",
      "tldr_zh": "该研究针对软件漏洞修复的挑战，提出了一种基于条件变分自编码器（CVAE）的自动修复技术 CRepair，以解决现有方法忽略漏洞代码多样性和定位不精确的问题。CRepair 通过提示-based 数据预处理、因果推理映射特征到概率分布、多样本特征融合以及条件控制来指导模型修复系统代码，从而捕获漏洞的多样特性。实验结果显示，该方法比基准模型显著提升修复性能，实现了52%的完美修复率，并为AI驱动的代码漏洞修复提供了新进展。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05540v1",
      "published_date": "2024-11-08 12:55:04 UTC",
      "updated_date": "2024-11-08 12:55:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:48:47.151780"
    },
    {
      "arxiv_id": "2411.05892v1",
      "title": "Identifying and Decomposing Compound Ingredients in Meal Plans Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Leon Kopitar",
        "Leon Bedrac",
        "Larissa J Strath",
        "Jiang Bian",
        "Gregor Stiglic"
      ],
      "abstract": "This study explores the effectiveness of Large Language Models in meal\nplanning, focusing on their ability to identify and decompose compound\ningredients. We evaluated three models-GPT-4o, Llama-3 (70b), and Mixtral\n(8x7b)-to assess their proficiency in recognizing and breaking down complex\ningredient combinations. Preliminary results indicate that while Llama-3 (70b)\nand GPT-4o excels in accurate decomposition, all models encounter difficulties\nwith identifying essential elements like seasonings and oils. Despite strong\noverall performance, variations in accuracy and completeness were observed\nacross models. These findings underscore LLMs' potential to enhance\npersonalized nutrition but highlight the need for further refinement in\ningredient decomposition. Future research should address these limitations to\nimprove nutritional recommendations and health outcomes.",
      "tldr_zh": "这项研究评估了 Large Language Models (LLMs) 在膳食规划中识别和分解复合成分的有效性，重点测试了 GPT-4o、Llama-3 (70b) 和 Mixtral (8x7b) 三个模型的表现。结果显示，Llama-3 (70b) 和 GPT-4o 在准确分解复杂成分方面表现出色，但所有模型在识别调味品和油等基本元素时存在困难。尽管整体准确性和完整性存在差异，这些发现突出了 LLMs 提升个性化营养的潜力。未来研究需进一步改进成分分解功能，以优化营养推荐和健康结果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Comments: Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)",
      "pdf_url": "http://arxiv.org/pdf/2411.05892v1",
      "published_date": "2024-11-08 12:38:10 UTC",
      "updated_date": "2024-11-08 12:38:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:49:00.572193"
    },
    {
      "arxiv_id": "2411.05521v2",
      "title": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark",
      "title_zh": "SM3-Text-to-Query：合成多模型医疗文本到查询基准",
      "authors": [
        "Sithursan Sivasubramaniam",
        "Cedric Osei-Akoto",
        "Yi Zhang",
        "Kurt Stockinger",
        "Jonathan Fuerst"
      ],
      "abstract": "Electronic health records (EHRs) are stored in various database systems with\ndifferent database models on heterogeneous storage architectures, such as\nrelational databases, document stores, or graph databases. These different\ndatabase models have a big impact on query complexity and performance. While\nthis has been a known fact in database research, its implications for the\ngrowing number of Text-to-Query systems have surprisingly not been investigated\nso far. In this paper, we present SM3-Text-to-Query, the first multi-model\nmedical Text-to-Query benchmark based on synthetic patient data from Synthea,\nfollowing the SNOMED-CT taxonomy -- a widely used knowledge graph ontology\ncovering medical terminology. SM3-Text-to-Query provides data representations\nfor relational databases (PostgreSQL), document stores (MongoDB), and graph\ndatabases (Neo4j and GraphDB (RDF)), allowing the evaluation across four\npopular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically\nand manually develop 408 template questions, which we augment to construct a\nbenchmark of 10K diverse natural language question/query pairs for these four\nquery languages (40K pairs overall). On our dataset, we evaluate several common\nin-context-learning (ICL) approaches for a set of representative closed and\nopen-source LLMs. Our evaluation sheds light on the trade-offs between database\nmodels and query languages for different ICL strategies and LLMs. Last,\nSM3-Text-to-Query is easily extendable to additional query languages or real,\nstandard-based patient databases.",
      "tldr_zh": "本论文引入了SM3-Text-to-Query，这是一个基于合成患者数据（Synthea）和SNOMED-CT分类学的多模型医疗Text-to-Query基准，用于评估不同数据库模型对查询系统的影响。基准提供数据表示支持关系数据库（PostgreSQL）、文档存储（MongoDB）和图数据库（Neo4j及GraphDB），涵盖SQL、MQL、Cypher和SPARQL四种查询语言，并构建了408个模板问题扩展为10K多样化自然语言问题/查询对（总计40K对）。通过评估各种in-context-learning (ICL)策略在封闭和开源LLMs上的性能，论文揭示了数据库模型与查询语言之间的权衡，并强调该基准易于扩展至其他语言或真实患者数据库。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "NeurIPS 2024 Track Datasets and Benchmarks",
      "pdf_url": "http://arxiv.org/pdf/2411.05521v2",
      "published_date": "2024-11-08 12:27:13 UTC",
      "updated_date": "2024-11-14 09:28:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:49:14.590051"
    },
    {
      "arxiv_id": "2411.05514v1",
      "title": "Towards Scalable Foundation Models for Digital Dermatology",
      "title_zh": "翻译失败",
      "authors": [
        "Fabian Gröger",
        "Philippe Gottfrois",
        "Ludovic Amruthalingam",
        "Alvaro Gonzalez-Jimenez",
        "Simone Lionetti",
        "Luis R. Soenksen-Martinez",
        "Alexander A. Navarini",
        "Marc Pouly"
      ],
      "abstract": "The growing demand for accurate and equitable AI models in digital\ndermatology faces a significant challenge: the lack of diverse, high-quality\nlabeled data. In this work, we investigate the potential of domain-specific\nfoundation models for dermatology in addressing this challenge. We utilize\nself-supervised learning (SSL) techniques to pre-train models on a dataset of\nover 240,000 dermatological images from public and private collections. Our\nstudy considers several SSL methods and compares the resulting foundation\nmodels against domain-agnostic models like those pre-trained on ImageNet and\nstate-of-the-art models such as MONET across 12 downstream tasks. Unlike\nprevious research, we emphasize the development of smaller models that are more\nsuitable for resource-limited clinical settings, facilitating easier adaptation\nto a broad range of use cases. Results show that models pre-trained in this\nwork not only outperform general-purpose models but also approach the\nperformance of models 50 times larger on clinically relevant diagnostic tasks.\nTo promote further research in this direction, we publicly release both the\ntraining code and the foundation models, which can benefit clinicians in\ndermatological applications.",
      "tldr_zh": "本研究针对数字皮肤病学中数据多样性和质量不足的挑战，提出使用自监督学习(SSL)技术在超过24万张皮肤病图像数据集上预训练领域特定基础模型，以开发更适合资源有限临床环境的较小模型。相比基于ImageNet的通用模型和MONET等模型，这些基础模型在12个下游任务上表现出色。结果表明，新模型不仅优于通用模型，还在临床诊断任务上接近比它们大50倍的模型性能；为促进研究，该工作公开了训练代码和模型资源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Findings paper presented at Machine Learning for Health (ML4H)\n  symposium 2024, December 15-16, 2024, Vancouver, Canada, 11 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.05514v1",
      "published_date": "2024-11-08 12:19:20 UTC",
      "updated_date": "2024-11-08 12:19:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:49:23.436773"
    },
    {
      "arxiv_id": "2411.05451v1",
      "title": "WorkflowLLM: Enhancing Workflow Orchestration Capability of Large Language Models",
      "title_zh": "WorkflowLLM：提升大型语言模型的工作流编排能力",
      "authors": [
        "Shengda Fan",
        "Xin Cong",
        "Yuepeng Fu",
        "Zhong Zhang",
        "Shuyan Zhang",
        "Yuanwei Liu",
        "Yesai Wu",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have driven a\nrevolutionary paradigm shift in process automation from Robotic Process\nAutomation to Agentic Process Automation by automating the workflow\norchestration procedure based on LLMs. However, existing LLMs (even the\nadvanced OpenAI GPT-4o) are confined to achieving satisfactory capability in\nworkflow orchestration. To address this limitation, we present WorkflowLLM, a\ndata-centric framework elaborately designed to enhance the capability of LLMs\nin workflow orchestration. It first constructs a large-scale fine-tuning\ndataset WorkflowBench with 106,763 samples, covering 1,503 APIs from 83\napplications across 28 categories. Specifically, the construction process can\nbe divided into three phases: (1) Data Collection: we collect real-world\nworkflow data from Apple Shortcuts and RoutineHub, transcribing them into\nPython-style code. We further equip them with generated hierarchical thought\nvia ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task\nqueries to enrich the diversity and complexity of workflows. (3) Workflow\nGeneration: we leverage an annotator model trained on collected data to\ngenerate workflows for synthesized queries. Finally, we merge the synthetic\nsamples that pass quality confirmation with the collected samples to obtain the\nWorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain\nWorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong\ncapacity to orchestrate complex workflows, while also achieving notable\ngeneralization performance on previously unseen APIs. Additionally,\nWorkflowBench exhibits robust zero-shot generalization capabilities on an\nout-of-distribution task planning dataset, T-Eval. Our data and code are\navailable at https://github.com/OpenBMB/WorkflowLLM.",
      "tldr_zh": "该研究提出 WorkflowLLM 框架，以提升大型语言模型 (LLMs) 在工作流编排方面的能力，针对现有 LLMs 如 GPT-4o 的不足。框架通过构建大规模微调数据集 WorkflowBench（包含 106,763 个样本、覆盖 1,503 个 API 来自 83 个应用和 28 个类别），采用数据收集、查询扩展和工作流生成三阶段方法来丰富数据集。基于 WorkflowBench 微调 Llama-3.1-8B 得到 WorkflowLlama，实验结果显示其在编排复杂工作流和泛化性能上显著优于基线，并在零样本场景中表现出色，如在 T-Eval 数据集上的表现。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05451v1",
      "published_date": "2024-11-08 09:58:02 UTC",
      "updated_date": "2024-11-08 09:58:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:49:37.409788"
    },
    {
      "arxiv_id": "2411.05424v1",
      "title": "ICE-T: A Multi-Faceted Concept for Teaching Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hendrik Krone",
        "Pierre Haritz",
        "Thomas Liebig"
      ],
      "abstract": "The topics of Artificial intelligence (AI) and especially Machine Learning\n(ML) are increasingly making their way into educational curricula. To\nfacilitate the access for students, a variety of platforms, visual tools, and\ndigital games are already being used to introduce ML concepts and strengthen\nthe understanding of how AI works. We take a look at didactic principles that\nare employed for teaching computer science, define criteria, and, based on\nthose, evaluate a selection of prominent existing platforms, tools, and games.\nAdditionally, we criticize the approach of portraying ML mostly as a black-box\nand the resulting missing focus on creating an understanding of data,\nalgorithms, and models that come with it. To tackle this issue, we present a\nconcept that covers intermodal transfer, computational and explanatory\nthinking, ICE-T, as an extension of known didactic principles. With our\nmulti-faceted concept, we believe that planners of learning units, creators of\nlearning platforms and educators can improve on teaching ML.",
      "tldr_zh": "这篇论文探讨了在教育中教授 Machine Learning (ML) 的方法，批评了现有平台、工具和游戏将 ML 视为黑箱，导致对数据、算法和模型的理解不足。作者基于计算机科学教学原则定义了评估标准，并对选定的工具进行了评估。论文提出了 ICE-T 概念——一个多方面框架，包括 intermodal transfer、computational thinking 和 explanatory thinking，作为现有原则的扩展。最终，该概念旨在帮助学习单位规划者、教育平台创建者和教师提升 ML 教学的有效性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted and presented at the 17th International Conference on\n  Informatics in Schools (ISSEP 2024)",
      "pdf_url": "http://arxiv.org/pdf/2411.05424v1",
      "published_date": "2024-11-08 09:16:05 UTC",
      "updated_date": "2024-11-08 09:16:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:49:47.052991"
    },
    {
      "arxiv_id": "2411.05423v1",
      "title": "VISTA: Visual Integrated System for Tailored Automation in Math Problem Generation Using LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Jeongwoo Lee",
        "Kwangsuk Park",
        "Jihyeon Park"
      ],
      "abstract": "Generating accurate and consistent visual aids is a critical challenge in\nmathematics education, where visual representations like geometric shapes and\nfunctions play a pivotal role in enhancing student comprehension. This paper\nintroduces a novel multi-agent framework that leverages Large Language Models\n(LLMs) to automate the creation of complex mathematical visualizations\nalongside coherent problem text. Our approach not only simplifies the\ngeneration of precise visual aids but also aligns these aids with the problem's\ncore mathematical concepts, improving both problem creation and assessment. By\nintegrating multiple agents, each responsible for distinct tasks such as\nnumeric calculation, geometry validation, and visualization, our system\ndelivers mathematically accurate and contextually relevant problems with visual\naids. Evaluation across Geometry and Function problem types shows that our\nmethod significantly outperforms basic LLMs in terms of text coherence,\nconsistency, relevance and similarity, while maintaining the essential\ngeometrical and functional integrity of the original problems. Although some\nchallenges remain in ensuring consistent visual outputs, our framework\ndemonstrates the immense potential of LLMs in transforming the way educators\ngenerate and utilize visual aids in math education.",
      "tldr_zh": "这篇论文介绍了 VISTA 系统，一种基于 LLM（Large Language Models）的多智能体框架，用于自动化数学问题生成，专注于创建准确且与核心概念一致的视觉辅助工具，如几何形状和函数，以提升数学教育效果。系统通过多个代理分工处理数字计算、几何验证和可视化任务，确保生成的问题文本和视觉元素在连贯性、一致性和相关性上更具优势。实验结果显示，VISTA 在几何和函数问题类型上显著优于基本 LLM，同时保持了原问题的几何和函数完整性。尽管视觉输出一致性仍面临挑战，该框架展示了 LLM 在数学教育中生成和应用视觉辅助的巨大潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NeurIPS 2024 Workshop on Large Foundation Models for\n  Educational Assessment (FM-Assess)",
      "pdf_url": "http://arxiv.org/pdf/2411.05423v1",
      "published_date": "2024-11-08 09:15:56 UTC",
      "updated_date": "2024-11-08 09:15:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:50:00.643516"
    },
    {
      "arxiv_id": "2411.05421v1",
      "title": "Learning the rules of peptide self-assembly through data mining with large language models",
      "title_zh": "通过使用大语言模型的数据挖掘学习肽自组装规则",
      "authors": [
        "Zhenze Yang",
        "Sarah K. Yorke",
        "Tuomas P. J. Knowles",
        "Markus J. Buehler"
      ],
      "abstract": "Peptides are ubiquitous and important biologically derived molecules, that\nhave been found to self-assemble to form a wide array of structures. Extensive\nresearch has explored the impacts of both internal chemical composition and\nexternal environmental stimuli on the self-assembly behaviour of these systems.\nHowever, there is yet to be a systematic study that gathers this rich\nliterature data and collectively examines these experimental factors to provide\na global picture of the fundamental rules that govern protein self-assembly\nbehavior. In this work, we curate a peptide assembly database through a\ncombination of manual processing by human experts and literature mining\nfacilitated by a large language model. As a result, we collect more than 1,000\nexperimental data entries with information about peptide sequence, experimental\nconditions and corresponding self-assembly phases. Utilizing the collected\ndata, ML models are trained and evaluated, demonstrating excellent accuracy\n(>80\\%) and efficiency in peptide assembly phase classification. Moreover, we\nfine-tune our GPT model for peptide literature mining with the developed\ndataset, which exhibits markedly superior performance in extracting information\nfrom academic publications relative to the pre-trained model. We find that this\nworkflow can substantially improve efficiency when exploring potential\nself-assembling peptide candidates, through guiding experimental work, while\nalso deepening our understanding of the mechanisms governing peptide\nself-assembly. In doing so, novel structures can be accessed for a range of\napplications including sensing, catalysis and biomaterials.",
      "tldr_zh": "本研究通过大型语言模型（LLMs）辅助数据挖掘，构建了一个包含超过1000个实验数据条目的肽自组装数据库，该数据库整合了肽序列、实验条件和自组装相信息。研究者结合手动处理和文献挖掘训练ML模型，实现肽自组装相分类的准确率超过80%，并微调GPT模型以显著提升学术文献信息提取性能。这种工作流程提高了探索潜在自组装肽候选物的效率，并加深了对肽自组装机制的理解，支持新型结构在传感、催化及生物材料等领域的应用。",
      "categories": [
        "cond-mat.soft",
        "cond-mat.dis-nn",
        "cond-mat.mes-hall",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cond-mat.soft",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05421v1",
      "published_date": "2024-11-08 09:14:22 UTC",
      "updated_date": "2024-11-08 09:14:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:50:10.632907"
    },
    {
      "arxiv_id": "2411.05420v2",
      "title": "WeatherGFM: Learning A Weather Generalist Foundation Model via In-context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangyu Zhao",
        "Zhiwang Zhou",
        "Wenlong Zhang",
        "Yihao Liu",
        "Xiangyu Chen",
        "Junchao Gong",
        "Hao Chen",
        "Ben Fei",
        "Shiqi Chen",
        "Wanli Ouyang",
        "Xiao-Ming Wu",
        "Lei Bai"
      ],
      "abstract": "The Earth's weather system encompasses intricate weather data modalities and\ndiverse weather understanding tasks, which hold significant value to human\nlife. Existing data-driven models focus on single weather understanding tasks\n(e.g., weather forecasting). Although these models have achieved promising\nresults, they fail to tackle various complex tasks within a single and unified\nmodel. Moreover, the paradigm that relies on limited real observations for a\nsingle scenario hinders the model's performance upper bound. In response to\nthese limitations, we draw inspiration from the in-context learning paradigm\nemployed in state-of-the-art visual foundation models and large language\nmodels. In this paper, we introduce the first generalist weather foundation\nmodel (WeatherGFM), designed to address a wide spectrum of weather\nunderstanding tasks in a unified manner. More specifically, we initially unify\nthe representation and definition of the diverse weather understanding tasks.\nSubsequently, we devised weather prompt formats to manage different weather\ndata modalities, namely single, multiple, and temporal modalities. Finally, we\nadopt a visual prompting question-answering paradigm for the training of\nunified weather understanding tasks. Extensive experiments indicate that our\nWeatherGFM can effectively handle up to ten weather understanding tasks,\nincluding weather forecasting, super-resolution, weather image translation, and\npost-processing. Our method also showcases generalization ability on unseen\ntasks.",
      "tldr_zh": "该论文提出WeatherGFM，一种基于in-context learning的通用天气基础模型，旨在统一处理多种天气理解任务，如天气预报和图像翻译，以克服现有模型单一任务局限和数据依赖的问题。具体方法包括统一任务表示、设计针对单、多个和时间序列模态的天气提示格式，以及采用视觉提示问答范式进行训练。实验结果显示，WeatherGFM能有效处理多达十个天气任务，并在未见任务上展现出良好的泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05420v2",
      "published_date": "2024-11-08 09:14:19 UTC",
      "updated_date": "2024-12-09 04:25:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:50:23.374698"
    },
    {
      "arxiv_id": "2411.05409v2",
      "title": "Web Archives Metadata Generation with GPT-4o: Challenges and Insights",
      "title_zh": "使用 GPT-4o 的网络档案元数据生成：挑战和洞见",
      "authors": [
        "Abigail Yongping Huang",
        "Ashwin Nair",
        "Zhen Rong Goh",
        "Tianrui Liu"
      ],
      "abstract": "Current metadata creation for web archives is time consuming and costly due\nto reliance on human effort. This paper explores the use of gpt-4o for metadata\ngeneration within the Web Archive Singapore, focusing on scalability,\nefficiency, and cost effectiveness. We processed 112 Web ARChive (WARC) files\nusing data reduction techniques, achieving a notable 99.9% reduction in\nmetadata generation costs. By prompt engineering, we generated titles and\nabstracts, which were evaluated both intrinsically using Levenshtein Distance\nand BERTScore, and extrinsically with human cataloguers using McNemar's test.\nResults indicate that while our method offers significant cost savings and\nefficiency gains, human curated metadata maintains an edge in quality. The\nstudy identifies key challenges including content inaccuracies, hallucinations,\nand translation issues, suggesting that Large Language Models (LLMs) should\nserve as complements rather than replacements for human cataloguers. Future\nwork will focus on refining prompts, improving content filtering, and\naddressing privacy concerns through experimentation with smaller models. This\nresearch advances the integration of LLMs in web archiving, offering valuable\ninsights into their current capabilities and outlining directions for future\nenhancements. The code is available at\nhttps://github.com/masamune-prog/warc2summary for further development and use\nby institutions facing similar challenges.",
      "tldr_zh": "这篇论文探讨了使用 GPT-4o 生成网页档案元数据的问题，旨在解决传统依赖人工的耗时和高成本挑战，通过处理 112 个 WARC 文件并采用数据减少技术，实现了 99.9% 的元数据生成成本降低，同时通过提示工程生成标题和摘要。评估结果显示，该方法在 Levenshtein Distance、BERTScore 和 McNemar's test 等指标上表现出效率优势，但人类编制的元数据在质量上更胜一筹，并突出了内容不准确、幻觉和翻译等关键挑战。研究建议将 Large Language Models (LLMs) 作为人类编目员的补充，并为未来工作提出优化提示、改善内容过滤和处理隐私问题的方向。",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05409v2",
      "published_date": "2024-11-08 08:59:40 UTC",
      "updated_date": "2024-11-16 02:27:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:50:36.373935"
    },
    {
      "arxiv_id": "2411.05403v1",
      "title": "Benchmarking Distributional Alignment of Large Language Models",
      "title_zh": "大型语言模型的分布对齐基准测试",
      "authors": [
        "Nicole Meister",
        "Carlos Guestrin",
        "Tatsunori Hashimoto"
      ],
      "abstract": "Language models (LMs) are increasingly used as simulacra for people, yet\ntheir ability to match the distribution of views of a specific demographic\ngroup and be \\textit{distributionally aligned} remains uncertain. This notion\nof distributional alignment is complex, as there is significant variation in\nthe types of attributes that are simulated. Prior works have underexplored the\nrole of three critical variables -- the question domain, steering method, and\ndistribution expression method -- which motivates our contribution of a\nbenchmark explicitly addressing these dimensions. We construct a dataset\nexpanding beyond political values, create human baselines for this task, and\nevaluate the extent to which an LM can align with a particular group's opinion\ndistribution to inform design choices of such simulation systems. Our analysis\nreveals open problems regarding if, and how, LMs can be used to simulate\nhumans, and that LLMs can more accurately describe the opinion distribution\nthan simulate such distributions.",
      "tldr_zh": "这篇论文评估了大型语言模型(LLMs)模拟特定群体观点分布的分布对齐能力，强调了问题领域、引导方法和分布表达方法等关键变量的作用。研究者构建了一个扩展至政治价值观以外的新数据集，并通过人类基线进行比较，以评估LLMs如何与目标群体的意见分布对齐。结果表明，LLMs更擅长描述意见分布而非实际模拟它们，这揭示了使用LLMs作为人类模拟系统的设计挑战和开放问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05403v1",
      "published_date": "2024-11-08 08:41:17 UTC",
      "updated_date": "2024-11-08 08:41:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:50:47.547071"
    },
    {
      "arxiv_id": "2411.05384v1",
      "title": "Advancing Meteorological Forecasting: AI-based Approach to Synoptic Weather Map Analysis",
      "title_zh": "推进气象预报：基于人工智能的天气图分析方法",
      "authors": [
        "Yo-Hwan Choi",
        "Seon-Yu Kang",
        "Minjong Cheon"
      ],
      "abstract": "As global warming increases the complexity of weather patterns; the precision\nof weather forecasting becomes increasingly important. Our study proposes a\nnovel preprocessing method and convolutional autoencoder model developed to\nimprove the interpretation of synoptic weather maps. These are critical for\nmeteorologists seeking a thorough understanding of weather conditions. This\nmodel could recognize historical synoptic weather maps that nearly match\ncurrent atmospheric conditions, marking a significant step forward in modern\ntechnology in meteorological forecasting. This comprises unsupervised learning\nmodels like VQ-VQE, as well as supervised learning models like VGG16, VGG19,\nXception, InceptionV3, and ResNet50 trained on the ImageNet dataset, as well as\nresearch into newer models like EfficientNet and ConvNeXt. Our findings proved\nthat, while these models perform well in various settings, their ability to\nidentify comparable synoptic weather maps has certain limits. Our research,\nmotivated by the primary goal of significantly increasing meteorologists'\nefficiency in labor-intensive tasks, discovered that cosine similarity is the\nmost effective metric, as determined by a combination of quantitative and\nqualitative assessments to accurately identify relevant historical weather\npatterns. This study broadens our understanding by shifting the emphasis from\nnumerical precision to practical application, ensuring that our model is\neffective in theory practical, and accessible in the complex and dynamic field\nof meteorology.",
      "tldr_zh": "本研究提出了一种新的预处理方法和卷积自编码器模型，用于提升对synoptic weather maps的分析和解释，以应对全球变暖下天气模式复杂化的挑战。该模型整合无监督学习如VQ-VQE和监督学习模型如VGG16、InceptionV3、ResNet50、EfficientNet以及ConvNeXt，通过评估发现余弦相似度是最有效的指标，能准确识别历史天气图与当前条件的相似性。研究结果表明，虽然这些模型在各种场景下表现良好，但仍存在识别局限；总体上，该方法从数值精度转向实际应用，帮助气象学家更高效地处理劳动密集型任务，并为气象预报领域提供实际可行的技术进步。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05384v1",
      "published_date": "2024-11-08 07:46:50 UTC",
      "updated_date": "2024-11-08 07:46:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:50:59.995880"
    },
    {
      "arxiv_id": "2411.05375v1",
      "title": "Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking",
      "title_zh": "Ev2R：自动事实核查中证据检索的评估",
      "authors": [
        "Mubashara Akhtar",
        "Michael Schlichtkrull",
        "Andreas Vlachos"
      ],
      "abstract": "Current automated fact-checking (AFC) approaches commonly evaluate evidence\neither implicitly via the predicted verdicts or by comparing retrieved evidence\nwith a predefined closed knowledge source, such as Wikipedia. However, these\nmethods suffer from limitations, resulting from their reliance on evaluation\nmetrics developed for different purposes and constraints imposed by closed\nknowledge sources. Recent advances in natural language generation (NLG)\nevaluation offer new possibilities for evidence assessment. In this work, we\nintroduce Ev2R, an evaluation framework for AFC that comprises three types of\napproaches for evidence evaluation: reference-based, proxy-reference, and\nreference-less. We evaluate their effectiveness through agreement with human\nratings and adversarial tests, and demonstrate that prompt-based scorers,\nparticularly those leveraging LLMs and reference evidence, outperform\ntraditional evaluation approaches.",
      "tldr_zh": "该研究针对自动化事实核查（AFC）中证据检索评估的局限性，提出Ev2R框架，以解决现有方法依赖封闭知识源和不适配评估指标的问题。Ev2R包括三种证据评估方法：基于参考的、代理参考的和无参考的，通过人类评分一致性和对抗测试来验证其有效性。结果显示，基于提示的评分器，特别是利用Large Language Models (LLMs)和参考证据的模型，显著优于传统评估方法，从而为AFC证据评估提供了更可靠的途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.05375v1",
      "published_date": "2024-11-08 07:05:06 UTC",
      "updated_date": "2024-11-08 07:05:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:51:11.051739"
    },
    {
      "arxiv_id": "2411.05359v1",
      "title": "Agricultural Landscape Understanding At Country-Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Radhika Dua",
        "Nikita Saxena",
        "Aditi Agarwal",
        "Alex Wilson",
        "Gaurav Singh",
        "Hoang Tran",
        "Ishan Deshpande",
        "Amandeep Kaur",
        "Gaurav Aggarwal",
        "Chandan Nath",
        "Arnab Basu",
        "Vishal Batchu",
        "Sharath Holla",
        "Bindiya Kurle",
        "Olana Missura",
        "Rahul Aggarwal",
        "Shubhika Garg",
        "Nishi Shah",
        "Avneet Singh",
        "Dinesh Tewari",
        "Agata Dondzik",
        "Bharat Adsul",
        "Milind Sohoni",
        "Asim Rama Praveen",
        "Aaryan Dangi",
        "Lisan Kadivar",
        "E Abhishek",
        "Niranjan Sudhansu",
        "Kamlakar Hattekar",
        "Sameer Datar",
        "Musty Krishna Chaithanya",
        "Anumas Ranjith Reddy",
        "Aashish Kumar",
        "Betala Laxmi Tirumala",
        "Alok Talekar"
      ],
      "abstract": "Agricultural landscapes are quite complex, especially in the Global South\nwhere fields are smaller, and agricultural practices are more varied. In this\npaper we report on our progress in digitizing the agricultural landscape\n(natural and man-made) in our study region of India. We use high resolution\nimagery and a UNet style segmentation model to generate the first of its kind\nnational-scale multi-class panoptic segmentation output. Through this work we\nhave been able to identify individual fields across 151.7M hectares, and\ndelineating key features such as water resources and vegetation. We share how\nthis output was validated by our team and externally by downstream users,\nincluding some sample use cases that can lead to targeted data driven decision\nmaking. We believe this dataset will contribute towards digitizing agriculture\nby generating the foundational baselayer.",
      "tldr_zh": "该研究针对全球南方农业景观的复杂性（如田地小、实践多样），以印度为研究区域，开发了一种基于高分辨率图像和UNet风格分割模型的国家规模多类全景分割方法。该方法成功识别了151.7M公顷的个别田地，并勾勒出水资源和植被等关键特征。通过团队内部验证和下游用户外部评估，该输出证明了其准确性，并支持了诸如数据驱动决策的实际应用。该数据集作为农业数字化的基础层，将有助于推动更精确的农业管理和决策。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "34 pages, 7 tables, 15 figs",
      "pdf_url": "http://arxiv.org/pdf/2411.05359v1",
      "published_date": "2024-11-08 06:29:02 UTC",
      "updated_date": "2024-11-08 06:29:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:51:22.475703"
    },
    {
      "arxiv_id": "2411.05353v1",
      "title": "Controlling Grokking with Nonlinearity and Data Symmetry",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Salah",
        "David Yevick"
      ],
      "abstract": "This paper demonstrates that grokking behavior in modular arithmetic with a\nmodulus P in a neural network can be controlled by modifying the profile of the\nactivation function as well as the depth and width of the model. Plotting the\neven PCA projections of the weights of the last NN layer against their odd\nprojections further yields patterns which become significantly more uniform\nwhen the nonlinearity is increased by incrementing the number of layers. These\npatterns can be employed to factor P when P is nonprime. Finally, a metric for\nthe generalization ability of the network is inferred from the entropy of the\nlayer weights while the degree of nonlinearity is related to correlations\nbetween the local entropy of the weights of the neurons in the final layer.",
      "tldr_zh": "本文研究了如何通过调整激活函数的非线性程度、模型深度和宽度来控制神经网络在模运算（modular arithmetic with modulus P）中的 grokking 行为。实验显示，绘制最后一层权重在偶 PCA projections 和奇 projections 上的图案，随着非线性增加（如层数增多），这些图案变得更均匀，并可用于因式分解非质数 P。同时，论文从层权重的熵推断网络的泛化能力，并探讨了非线性程度与最终层神经元权重局部熵之间的相关性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.05353v1",
      "published_date": "2024-11-08 06:19:29 UTC",
      "updated_date": "2024-11-08 06:19:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:51:35.281008"
    },
    {
      "arxiv_id": "2411.05349v1",
      "title": "Enhancing Cluster Resilience: LLM-agent Based Autonomous Intelligent Cluster Diagnosis System and Evaluation Framework",
      "title_zh": "增强集群弹性：基于 LLM-agent 的自治智能集群诊断系统以及评估框架",
      "authors": [
        "Honghao Shi",
        "Longkai Cheng",
        "Wenli Wu",
        "Yuhang Wang",
        "Xuan Liu",
        "Shaokai Nie",
        "Weixv Wang",
        "Xuebin Min",
        "Chunlei Men",
        "Yonghua Lin"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) and related technologies\nsuch as Retrieval-Augmented Generation (RAG) and Diagram of Thought (DoT) have\nenabled the creation of autonomous intelligent systems capable of performing\ncluster diagnostics and troubleshooting. By integrating these technologies with\nself-play methodologies, we have developed an LLM-agent system designed to\nautonomously diagnose and resolve issues within AI clusters. Our innovations\ninclude a knowledge base tailored for cluster diagnostics, enhanced LLM\nalgorithms, practical deployment strategies for agents, and a benchmark\nspecifically designed for evaluating LLM capabilities in this domain. Through\nextensive experimentation across multiple dimensions, we have demonstrated the\nsuperiority of our system in addressing the challenges faced in cluster\ndiagnostics, particularly in detecting and rectifying performance issues more\nefficiently and accurately than traditional methods.",
      "tldr_zh": "本研究提出了一种基于LLM-agent的自主智能集群诊断系统，旨在提升AI集群的弹性，通过整合LLM、RAG（Retrieval-Augmented Generation）和DoT（Diagram of Thought）等技术与self-play方法，实现对集群问题的自动诊断和解决。创新点包括定制的集群诊断知识库、增强的LLM算法、实际代理部署策略，以及一个专门的评估基准框架。实验结果显示，该系统在多维度测试中比传统方法更高效准确地检测和修复性能问题，从而为集群管理提供更可靠的解决方案。",
      "categories": [
        "cs.AI",
        "cs.DC",
        "68T42"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.05349v1",
      "published_date": "2024-11-08 06:12:56 UTC",
      "updated_date": "2024-11-08 06:12:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:51:47.770866"
    },
    {
      "arxiv_id": "2411.05348v2",
      "title": "LLM-PySC2: Starcraft II learning environment for Large Language Models",
      "title_zh": "LLM-PySC2：Starcraft II 用于",
      "authors": [
        "Zongyuan Li",
        "Yanan Ni",
        "Runnan Qi",
        "Lumin Jiang",
        "Chang Lu",
        "Xiaojie Xu",
        "Xiangbei Liu",
        "Pengfei Li",
        "Yunzheng Guo",
        "Zhe Ma",
        "Huanyu Li",
        "Hui Wu",
        "Xian Guo",
        "Kuihua Huang",
        "Xuebo Zhang"
      ],
      "abstract": "The tremendous potential has been demonstrated by large language models\n(LLMs) in intelligent decision-making problems, with unprecedented capabilities\nshown across diverse applications ranging from gaming AI systems to complex\nstrategic planning frameworks. However, the StarCraft II platform, which has\nbeen widely adopted for validating decision-making algorithms in the past\ndecade, has not yet provided substantial support for this emerging domain. To\naddress issues that LLMs cannot interface with the hundreds of actions of the\npysc2 backend and the lack of native support for multi-agent (MA)\ncollaboration, we propose the LLM-PySC2 environment. This is the first\nenvironment that offers LLMs the complete pysc2 action space with sufficient\nmulti-modal information and game Wiki knowledge. With an asynchronous query\narchitecture, the environment efficiently interacts with LLMs that maintain a\nconstant latency regardless of the scale of the agents' population. In the\nexperiments, we evaluated LLMs' decision-making performance in both the\nmacro-decision and micro-operation scenarios, with traditional StarCraft II\nMulti-Agent Challenge (SMAC) tasks and a series of new proposed. Results\nindicate that LLMs possess the potential to achieve victories in complex\nscenarios but cannot constantly generate correct decisions, especially in the\nrecovered pysc2 action space and MA settings. Without task-relevant\ninstructions, the pre-trained models suffer from issues such as hallucinations\nand inefficient collaboration. Our findings suggest that StarCraft II still\nchallenges in the era of large models, revealing that there is a lot to do to\ndevelop an advanced LLM decision-making system, and the proposed LLM-PySC2\nenvironment will support future development of LLM-based decision-making\nsolutions.",
      "tldr_zh": "本文提出 LLM-PySC2 环境，这是一个专为 Large Language Models (LLMs) 设计的 StarCraft II 学习平台，旨在解决 LLMs 与 pysc2 后端的动作接口问题以及缺乏 multi-agent (MA) 协作支持。环境提供完整的 pysc2 动作空间、多模态信息和游戏 Wiki 知识，并采用异步查询架构，确保高效交互而不受代理规模影响。实验评估了 LLMs 在传统 SMAC 任务和新场景中的宏观决策和微观操作性能，结果显示 LLMs 有潜力在复杂环境中获胜，但常出现幻觉和协作低效问题。研究强调，StarCraft II 仍是对 LLM 决策系统的重大挑战，而 LLM-PySC2 将支持未来相关开发的推进。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05348v2",
      "published_date": "2024-11-08 06:04:22 UTC",
      "updated_date": "2025-05-02 07:20:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:52:00.372038"
    },
    {
      "arxiv_id": "2411.05345v1",
      "title": "Reasoning Robustness of LLMs to Adversarial Typographical Errors",
      "title_zh": "翻译失败",
      "authors": [
        "Esther Gan",
        "Yiran Zhao",
        "Liying Cheng",
        "Yancan Mao",
        "Anirudh Goyal",
        "Kenji Kawaguchi",
        "Min-Yen Kan",
        "Michael Shieh"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by\nusers' instruction. In this work, we study the reasoning robustness of LLMs to\ntypographical errors, which can naturally occur in users' queries. We design an\nAdversarial Typo Attack ($\\texttt{ATA}$) algorithm that iteratively samples\ntypos for words that are important to the query and selects the edit that is\nmost likely to succeed in attacking. It shows that LLMs are sensitive to\nminimal adversarial typographical changes. Notably, with 1 character edit,\nMistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K, while with 8\ncharacter edits the performance further drops to 19.2%. To extend our\nevaluation to larger and closed-source LLMs, we develop the $\\texttt{R$^2$ATA}$\nbenchmark, which assesses models' $\\underline{R}$easoning\n$\\underline{R}$obustness to $\\underline{\\texttt{ATA}}$. It includes adversarial\ntypographical questions derived from three widely used reasoning\ndatasets-GSM8K, BBH, and MMLU-by applying $\\texttt{ATA}$ to open-source LLMs.\n$\\texttt{R$^2$ATA}$ demonstrates remarkable transferability and causes notable\nperformance drops across multiple super large and closed-source LLMs.",
      "tldr_zh": "本研究探讨了大语言模型 (LLMs) 在使用 Chain-of-Thought (CoT) 提示进行推理时的鲁棒性，特别关注用户查询中自然出现的 typographical errors。研究者设计了 Adversarial Typo Attack (ATA) 算法，通过迭代采样和选择对查询重要单词的打字错误，来评估模型的敏感性，结果显示即使1个字符编辑也能显著降低性能，例如 Mistral-7B-Instruct 在 GSM8K 数据集上的准确率从43.7% 降至38.6%。为了扩展评估，他们开发了 R²ATA 基准测试，该基准基于 GSM8K、BBH 和 MMLU 数据集生成 adversarial typographical questions，并证明了攻击的转移性，导致多个超大和闭源 LLMs 的性能大幅下降，为提升模型的推理鲁棒性提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05345v1",
      "published_date": "2024-11-08 05:54:05 UTC",
      "updated_date": "2024-11-08 05:54:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:52:12.242778"
    },
    {
      "arxiv_id": "2411.05340v1",
      "title": "Improving Multi-Domain Task-Oriented Dialogue System with Offline Reinforcement Learning",
      "title_zh": "利用离线强化学习改进多域任务导向对话系统",
      "authors": [
        "Dharmendra Prajapat",
        "Durga Toshniwal"
      ],
      "abstract": "Task-oriented dialogue (TOD) system is designed to accomplish user-defined\ntasks through dialogues. The TOD system has progressed towards end-to-end\nmodeling by leveraging pre-trained large language models. Fine-tuning the\npre-trained language models using only supervised learning leads to the\nexposure bias and token loss problem and it deviates the models from completing\nthe user's task. To address these issues, we propose a TOD system that\nleverages a unified pre-trained language model, GPT2, as a base model. It is\noptimized using supervised learning and reinforcement learning (RL). The issues\nin the TOD system are mitigated using a non-differentiable reward function. The\nreward is calculated using the weighted sum of the success rate and BLEU\nevaluation metrics. The success rate and BLEU metrics in reward calculation\nguide the language model for user task completion while ensuring a coherent and\nfluent response. Our model is acquired by fine-tuning a pre-trained model on\nthe dialogue-session level which comprises user utterance, belief state, system\nact, and system response. Experimental results on MultiWOZ2.1 demonstrate that\nour model increases the inform rate by 1.60% and the success rate by 3.17%\ncompared to the baseline.",
      "tldr_zh": "本研究旨在改进多域任务导向对话 (TOD) 系统，通过结合监督学习和离线强化学习 (RL) 来优化基于 GPT2 的预训练模型，解决曝光偏差和标记损失问题。\n该系统采用非微分奖励函数，将成功率和 BLEU 指标的加权和作为奖励，指导模型在对话会话级别生成更连贯且任务导向的响应。\n实验结果在 MultiWOZ2.1 数据集上表明，该模型的 inform rate 提高了 1.60%，success rate 提高了 3.17%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05340v1",
      "published_date": "2024-11-08 05:43:40 UTC",
      "updated_date": "2024-11-08 05:43:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:52:23.717084"
    },
    {
      "arxiv_id": "2411.05880v1",
      "title": "Towards Equitable ASD Diagnostics: A Comparative Study of Machine and Deep Learning Models Using Behavioral and Facial Data",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammed Aledhari",
        "Mohamed Rahouti",
        "Ali Alfatemi"
      ],
      "abstract": "Autism Spectrum Disorder (ASD) is often underdiagnosed in females due to\ngender-specific symptom differences overlooked by conventional diagnostics.\nThis study evaluates machine learning models, particularly Random Forest and\nconvolutional neural networks, for enhancing ASD diagnosis through structured\ndata and facial image analysis. Random Forest achieved 100% validation accuracy\nacross datasets, highlighting its ability to manage complex relationships and\nreduce false negatives, which is crucial for early intervention and addressing\ngender biases. In image-based analysis, MobileNet outperformed the baseline\nCNN, achieving 87% accuracy, though a 30% validation loss suggests possible\noverfitting, requiring further optimization for robustness in clinical\nsettings. Future work will emphasize hyperparameter tuning, regularization, and\ntransfer learning. Integrating behavioral data with facial analysis could\nimprove diagnosis for underdiagnosed groups. These findings suggest Random\nForest's high accuracy and balanced precision-recall metrics could enhance\nclinical workflows. MobileNet's lightweight structure also shows promise for\nresource-limited environments, enabling accessible ASD screening. Addressing\nmodel explainability and clinician trust will be vital.",
      "tldr_zh": "这项研究针对自闭症谱系障碍 (ASD) 在女性的低诊断率问题，比较了机器学习模型如 Random Forest 和深度学习模型如 convolutional neural networks (CNN)，利用行为数据和面部图像来提升诊断公平性。Random Forest 在数据集上实现了 100% 的验证准确率，能够有效处理复杂关系并减少假阴性，从而支持早期干预和缓解性别偏差。在面部图像分析中，MobileNet 超过了基线 CNN，达到 87% 的准确率，但 30% 的验证损失表明存在过拟合风险，需要通过超参数调整、正则化和转移学习进行优化。总体而言，这些发现表明 Random Forest 可增强临床工作流程，而 MobileNet 的轻量级结构适合资源有限的环境，有助于更可访问的 ASD 筛查，并强调了提高模型可解释性和临床信任的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05880v1",
      "published_date": "2024-11-08 05:26:04 UTC",
      "updated_date": "2024-11-08 05:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:52:36.349874"
    },
    {
      "arxiv_id": "2411.05330v1",
      "title": "Inversion-based Latent Bayesian Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Jaewon Chu",
        "Jinyoung Park",
        "Seunghun Lee",
        "Hyunwoo J. Kim"
      ],
      "abstract": "Latent Bayesian optimization (LBO) approaches have successfully adopted\nBayesian optimization over a continuous latent space by employing an\nencoder-decoder architecture to address the challenge of optimization in a high\ndimensional or discrete input space. LBO learns a surrogate model to\napproximate the black-box objective function in the latent space. However, we\nobserved that most LBO methods suffer from the `misalignment problem`, which is\ninduced by the reconstruction error of the encoder-decoder architecture. It\nhinders learning an accurate surrogate model and generating high-quality\nsolutions. In addition, several trust region-based LBO methods select the\nanchor, the center of the trust region, based solely on the objective function\nvalue without considering the trust region`s potential to enhance the\noptimization process. To address these issues, we propose Inversion-based\nLatent Bayesian Optimization (InvBO), a plug-and-play module for LBO. InvBO\nconsists of two components: an inversion method and a potential-aware trust\nregion anchor selection. The inversion method searches the latent code that\ncompletely reconstructs the given target data. The potential-aware trust region\nanchor selection considers the potential capability of the trust region for\nbetter local optimization. Experimental results demonstrate the effectiveness\nof InvBO on nine real-world benchmarks, such as molecule design and arithmetic\nexpression fitting tasks. Code is available at https://github.com/mlvlab/InvBO.",
      "tldr_zh": "该论文针对Latent Bayesian Optimization (LBO)方法中存在的“misalignment problem”问题（如encoder-decoder架构的重建错误导致代理模型不准确），提出了一种插件式模块InvBO，以提升优化性能。InvBO包括两个关键组件：inversion method，用于搜索完全重建目标数据的潜代码；以及potential-aware trust region anchor selection，通过考虑信任区域的潜力来优化局部搜索。实验在九个真实世界基准（如分子设计和算术表达式拟合任务）上验证了InvBO的有效性，展示了其在生成高质量解决方案方面的优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.05330v1",
      "published_date": "2024-11-08 05:06:47 UTC",
      "updated_date": "2024-11-08 05:06:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:52:46.670853"
    },
    {
      "arxiv_id": "2411.05316v2",
      "title": "Aligning Large Language Models and Geometric Deep Models for Protein Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Dong Shu",
        "Bingbing Duan",
        "Kai Guo",
        "Kaixiong Zhou",
        "Jiliang Tang",
        "Mengnan Du"
      ],
      "abstract": "Latent representation alignment has become a foundational technique for\nconstructing multimodal large language models (MLLM) by mapping embeddings from\ndifferent modalities into a shared space, often aligned with the embedding\nspace of large language models (LLMs) to enable effective cross-modal\nunderstanding. While preliminary protein-focused MLLMs have emerged, they have\npredominantly relied on heuristic approaches, lacking a fundamental\nunderstanding of optimal alignment practices across representations. In this\nstudy, we explore the alignment of multimodal representations between LLMs and\nGeometric Deep Models (GDMs) in the protein domain. We comprehensively evaluate\nthree state-of-the-art LLMs (Gemma2-2B, LLaMa3.1-8B, and LLaMa3.1-70B) with\nfour protein-specialized GDMs (GearNet, GVP, ScanNet, GAT). Our work examines\nalignment factors from both model and protein perspectives, identifying\nchallenges in current alignment methodologies and proposing strategies to\nimprove the alignment process. Our key findings reveal that GDMs incorporating\nboth graph and 3D structural information align better with LLMs, larger LLMs\ndemonstrate improved alignment capabilities, and protein rarity significantly\nimpacts alignment performance. We also find that increasing GDM embedding\ndimensions, using two-layer projection heads, and fine-tuning LLMs on\nprotein-specific data substantially enhance alignment quality. These strategies\noffer potential enhancements to the performance of protein-related multimodal\nmodels. Our code and data are available at\nhttps://github.com/Tizzzzy/LLM-GDM-alignment.",
      "tldr_zh": "这篇论文探讨了在蛋白质领域中，将大型语言模型（LLMs）和几何深度模型（GDMs）的多模态表示进行对齐，以构建更有效的多模态大语言模型（MLLM）。研究团队评估了三个LLMs（Gemma2-2B、LLaMa3.1-8B和LLaMa3.1-70B）与四个蛋白质专用GDMs（GearNet、GVP、ScanNet、GAT），从模型和蛋白质角度分析了对齐因素，并识别了当前方法的挑战。关键发现包括：整合图和3D结构信息的GDMs与LLMs对齐效果更好，较大LLMs显示出更强的对齐能力，而蛋白质的稀有性会显著影响性能；此外，增加GDM嵌入维度、使用两层投影头以及在蛋白质特定数据上微调LLMs可显著提升对齐质量，为蛋白质相关多模态模型的优化提供了实用策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "37 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.05316v2",
      "published_date": "2024-11-08 04:15:08 UTC",
      "updated_date": "2025-03-04 22:22:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:53:00.384745"
    },
    {
      "arxiv_id": "2411.05307v1",
      "title": "Revisiting Network Perturbation for Semi-Supervised Semantic Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Sien Li",
        "Tao Wang",
        "Ruizhe Hu",
        "Wenxi Liu"
      ],
      "abstract": "In semi-supervised semantic segmentation (SSS), weak-to-strong consistency\nregularization techniques are widely utilized in recent works, typically\ncombined with input-level and feature-level perturbations. However, the\nintegration between weak-to-strong consistency regularization and network\nperturbation has been relatively rare. We note several problems with existing\nnetwork perturbations in SSS that may contribute to this phenomenon. By\nrevisiting network perturbations, we introduce a new approach for network\nperturbation to expand the existing weak-to-strong consistency regularization\nfor unlabeled data. Additionally, we present a volatile learning process for\nlabeled data, which is uncommon in existing research. Building upon previous\nwork that includes input-level and feature-level perturbations, we present\nMLPMatch (Multi-Level-Perturbation Match), an easy-to-implement and efficient\nframework for semi-supervised semantic segmentation. MLPMatch has been\nvalidated on the Pascal VOC and Cityscapes datasets, achieving state-of-the-art\nperformance. Code is available from https://github.com/LlistenL/MLPMatch.",
      "tldr_zh": "该论文重新审视了半监督语义分割（SSS）中的网络扰动问题，指出现有方法未充分整合网络扰动与弱到强一致性正则化，并引入了一种新的网络扰动方法来扩展此正则化技术，同时提出针对有标签数据的易变学习过程。作者构建了 MLPMatch（Multi-Level-Perturbation Match）框架，该框架结合输入级、特征级和网络级扰动，提供了一个易于实现且高效的 SSS 解决方案。在 Pascal VOC 和 Cityscapes 数据集上，MLPMatch 实现了 state-of-the-art 性能，并公开了代码以供进一步验证。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by PRCV2024",
      "pdf_url": "http://arxiv.org/pdf/2411.05307v1",
      "published_date": "2024-11-08 03:23:39 UTC",
      "updated_date": "2024-11-08 03:23:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:53:11.852307"
    },
    {
      "arxiv_id": "2411.05296v1",
      "title": "On Training of Kolmogorov-Arnold Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Shairoz Sohail"
      ],
      "abstract": "Kolmogorov-Arnold Networks have recently been introduced as a flexible\nalternative to multi-layer Perceptron architectures. In this paper, we examine\nthe training dynamics of different KAN architectures and compare them with\ncorresponding MLP formulations. We train with a variety of different\ninitialization schemes, optimizers, and learning rates, as well as utilize back\npropagation free approaches like the HSIC Bottleneck. We find that (when judged\nby test accuracy) KANs are an effective alternative to MLP architectures on\nhigh-dimensional datasets and have somewhat better parameter efficiency, but\nsuffer from more unstable training dynamics. Finally, we provide\nrecommendations for improving training stability of larger KAN models.",
      "tldr_zh": "本研究探讨了Kolmogorov-Arnold Networks (KANs)作为多层感知器(MLP)架构的灵活替代方案，重点分析了不同KAN架构的训练动态。研究者通过多种初始化方案、优化器、学习率以及无反向传播方法如HSIC Bottleneck，对KANs和对应的MLP进行比较。结果显示，KANs在高维数据集上表现出色，具有更好的参数效率，但训练动态更不稳定。最后，论文提供了改进较大KAN模型训练稳定的建议，以提升其实际应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.4"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.05296v1",
      "published_date": "2024-11-08 02:57:59 UTC",
      "updated_date": "2024-11-08 02:57:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:53:22.926791"
    },
    {
      "arxiv_id": "2411.05292v1",
      "title": "SimpleBEV: Improved LiDAR-Camera Fusion Architecture for 3D Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yun Zhao",
        "Zhan Gong",
        "Peiru Zheng",
        "Hong Zhu",
        "Shaohua Wu"
      ],
      "abstract": "More and more research works fuse the LiDAR and camera information to improve\nthe 3D object detection of the autonomous driving system. Recently, a simple\nyet effective fusion framework has achieved an excellent detection performance,\nfusing the LiDAR and camera features in a unified bird's-eye-view (BEV) space.\nIn this paper, we propose a LiDAR-camera fusion framework, named SimpleBEV, for\naccurate 3D object detection, which follows the BEV-based fusion framework and\nimproves the camera and LiDAR encoders, respectively. Specifically, we perform\nthe camera-based depth estimation using a cascade network and rectify the depth\nresults with the depth information derived from the LiDAR points. Meanwhile, an\nauxiliary branch that implements the 3D object detection using only the\ncamera-BEV features is introduced to exploit the camera information during the\ntraining phase. Besides, we improve the LiDAR feature extractor by fusing the\nmulti-scaled sparse convolutional features. Experimental results demonstrate\nthe effectiveness of our proposed method. Our method achieves 77.6\\% NDS\naccuracy on the nuScenes dataset, showcasing superior performance in the 3D\nobject detection track.",
      "tldr_zh": "本研究提出了一种改进的 LiDAR-相机融合框架 SimpleBEV，用于提升自动驾驶系统的 3D 对象检测性能。该框架在统一鸟瞰视图 (BEV) 空间中融合特征，通过级联网络进行相机-based 深度估计，并利用 LiDAR 点派生的深度信息进行校正，同时引入辅助分支以仅用相机-BEV 特征进行训练时的对象检测。SimpleBEV 还改进了 LiDAR 特征提取器，融合多尺度稀疏卷积特征。实验结果显示，该方法在 nuScenes 数据集上达到 77.6% NDS 准确率，显著优于基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05292v1",
      "published_date": "2024-11-08 02:51:39 UTC",
      "updated_date": "2024-11-08 02:51:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:53:36.017719"
    },
    {
      "arxiv_id": "2411.05289v1",
      "title": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Ryan Sun",
        "Tianyi Zhou",
        "Xun Chen",
        "Lichao Sun"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in advancing natural\nlanguage processing (NLP) tasks, but their sequential token generation limits\ninference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising\nsolution by using a smaller draft model to generate multiple token sequences,\nwhich the target LLM verifies in parallel. However, current heuristic\napproaches, such as Recursive Rejection Sampling (RRS), suffer from low\nacceptance rates in subsequent drafts, limiting the advantages of using\nmultiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can\ntheoretically improve acceptance rates, but its computational cost is too high\nfor real-time use. We present SpecHub, a novel, efficient sampling-verification\nmethod for MDSD that improves acceptance rates with only linear computational\noverhead. By simplifying the OTM problem into a compact Linear Programming\nmodel, SpecHub significantly reduces computational complexity. It further\naccelerates sampling by leveraging a sparse joint distribution, focusing\ncomputation on high-probability token sequences. In extensive experiments,\nSpechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step\nthan RRS and RRS without replacement. We attach our code at\n\\url{https://github.com/MasterGodzilla/Speculative_decoding_OT}.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 的顺序生成导致推理速度受限的问题，提出了 SpecHub，一种高效的采样-验证方法，用于提升 Multi-Draft Speculative Decoding (MDSD)。SpecHub 通过将 Optimal Transport with Membership Cost (OTM) 问题简化为紧凑的 Linear Programming 模型，并利用稀疏联合分布聚焦高概率 token 序列，仅需线性计算开销即可显著提高后续草稿的接受率。实验结果显示，SpecHub 每步生成 0.05-0.27 和 0.02-0.16 更多 tokens，比 Recursive Rejection Sampling (RRS) 更高效，为实时 LLM 推理提供了可证明的加速。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 (Main)",
      "pdf_url": "http://arxiv.org/pdf/2411.05289v1",
      "published_date": "2024-11-08 02:47:07 UTC",
      "updated_date": "2024-11-08 02:47:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:53:47.437998"
    },
    {
      "arxiv_id": "2411.05285v2",
      "title": "AgentOps: Enabling Observability of LLM Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Liming Dong",
        "Qinghua Lu",
        "Liming Zhu"
      ],
      "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities\nacross various domains, gaining extensive attention from academia and industry.\nHowever, these agents raise significant concerns on AI safety due to their\nautonomous and non-deterministic behavior, as well as continuous evolving\nnature . From a DevOps perspective, enabling observability in agents is\nnecessary to ensuring AI safety, as stakeholders can gain insights into the\nagents' inner workings, allowing them to proactively understand the agents,\ndetect anomalies, and prevent potential failures. Therefore, in this paper, we\npresent a comprehensive taxonomy of AgentOps, identifying the artifacts and\nassociated data that should be traced throughout the entire lifecycle of agents\nto achieve effective observability. The taxonomy is developed based on a\nsystematic mapping study of existing AgentOps tools. Our taxonomy serves as a\nreference template for developers to design and implement AgentOps\ninfrastructure that supports monitoring, logging, and analytics. thereby\nensuring AI safety.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLM)代理在各种领域表现出色但因其自主、非确定性和不断演变特性而引发的AI安全担忧，并强调从DevOps视角实现代理的可观察性，以帮助利益相关者理解代理内部运作、检测异常并防止故障。作者基于对现有AgentOps工具的系统映射研究，提出一个全面的AgentOps分类法，识别出代理整个生命周期中需要追踪的工件和相关数据。最终，该分类法作为参考模板，支持开发者设计和实施AgentOps基础设施，包括监控、日志记录和分析，从而提升AI安全。",
      "categories": [
        "cs.AI",
        "cs.SE",
        "D.2.7; D.2.9; D.2.11"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.05285v2",
      "published_date": "2024-11-08 02:31:03 UTC",
      "updated_date": "2024-11-30 02:55:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:53:58.798654"
    },
    {
      "arxiv_id": "2411.05282v4",
      "title": "MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization",
      "title_zh": "MicroScopiQ：通过异常值感知微缩放量化加速基础模型",
      "authors": [
        "Akshat Ramachandran",
        "Souvik Kundu",
        "Tushar Krishna"
      ],
      "abstract": "Quantization of foundational models (FMs) is significantly more challenging\nthan traditional DNNs due to the emergence of large magnitude values called\noutliers. Existing outlier-aware algorithm-architecture co-design techniques\neither use mixed-precision, retaining outliers at high precision but compromise\nhardware efficiency, or quantize inliers and outliers at the same precision,\nimproving hardware efficiency at the cost of accuracy. To address this mutual\nexclusivity, we propose MicroScopiQ, a novel co-design technique that leverages\npruning to complement outlier-aware quantization. MicroScopiQ retains outliers\nat higher precision while pruning a certain fraction of least important weights\nto distribute the additional outlier bits; ensuring high accuracy, aligned\nmemory and hardware efficiency. We design a high-throughput, low overhead\naccelerator architecture composed of multi-precision INT processing elements\nand a network-on-chip called ReCoN that efficiently abstracts the complexity of\nsupporting high-precision outliers. Additionally, unlike prior techniques,\nMicroScopiQ does not assume any locality of outlier weights, enabling\napplicability to a broad range of FMs. Extensive experiments across diverse\nquantization settings demonstrate that MicroScopiQ achieves state-of-the-art\nquantization accuracy, while delivering up to 3x faster inference and 2x lower\nenergy consumption compared to existing alternatives. Code is available at:\nhttps://github.com/georgia-tech-synergy-lab/MicroScopiQ-LLM-Quantization",
      "tldr_zh": "本文提出 MicroScopiQ，一种创新的算法-架构联合设计技术，通过结合 outlier-aware 量化与 pruning（权重修剪），在保留 outliers 高精度的同时优化内存和硬件效率，从而解决基础模型（FMs）量化面临的准确性与性能权衡问题。MicroScopiQ 设计了一个高吞吐、低开销的加速器架构，包括多精度 INT 处理元素和 ReCoN 网络-on-chip，并适用于不依赖 outliers 局部性的广泛 FMs。实验结果显示，该方法在多种量化设置下实现了最先进的准确性，同时比现有技术提供高达 3 倍的推理速度和 2 倍的能效降低。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "ISCA 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.05282v4",
      "published_date": "2024-11-08 02:25:45 UTC",
      "updated_date": "2025-04-29 18:38:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:54:12.147736"
    },
    {
      "arxiv_id": "2411.05281v3",
      "title": "Fox-1: Open Small Language Model for Cloud and Edge",
      "title_zh": "Fox-1：开源小型语言模型用于云端和边缘",
      "authors": [
        "Zijian Hu",
        "Jipeng Zhang",
        "Rui Pan",
        "Zhaozhuo Xu",
        "Shanshan Han",
        "Han Jin",
        "Alay Dilipbhai Shah",
        "Dimitris Stripelis",
        "Yuhang Yao",
        "Salman Avestimehr",
        "Tong Zhang",
        "Chaoyang He"
      ],
      "abstract": "We present Fox-1, a series of small language models (SLMs) consisting of\nFox-1-1.6B and Fox-1-1.6B-Instruct-v0.1. These models are pre-trained on 3\ntrillion tokens of web-scraped document data and fine-tuned with 5 billion\ntokens of instruction-following and multi-turn conversation data. Aiming to\nimprove the pre-training efficiency, Fox-1-1.6B model introduces a novel\n3-stage data curriculum across all the training data with 2K-8K sequence\nlength. In architecture design, Fox-1 features a deeper layer structure, an\nexpanded vocabulary, and utilizes Grouped Query Attention (GQA), offering a\nperformant and efficient architecture compared to other SLMs. Fox-1 achieves\nbetter or on-par performance in various benchmarks compared to StableLM-2-1.6B,\nGemma-2B, Qwen1.5-1.8B, and OpenELM1.1B, with competitive inference speed and\nthroughput. The model weights have been released under the Apache 2.0 license,\nwhere we aim to promote the democratization of LLMs and make them fully\naccessible to the whole open-source community.",
      "tldr_zh": "本研究介绍了 Fox-1 系列小语言模型 (SLMs)，包括 Fox-1-1.6B 和 Fox-1-1.6B-Instruct-v0.1，这些模型在 3 万亿 tokens 的网络抓取数据上预训练，并使用 5 亿 tokens 的指令跟随和多轮对话数据进行微调。Fox-1 采用创新的 3 阶段数据课程（data curriculum）及 2K-8K 序列长度来提升预训练效率，并在架构上引入更深的层结构、扩展词汇表和 Grouped Query Attention (GQA)，实现高效性能。实验结果显示，Fox-1 在各种基准测试中表现优于或相当于 StableLM-2-1.6B、Gemma-2B 和 Qwen1.5-1.8B 等模型，同时具备竞争力的推理速度和吞吐量；模型以 Apache 2.0 许可开源，旨在促进大型语言模型 (LLMs) 的民主化和社区访问。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Base model is available at\n  https://huggingface.co/tensoropera/Fox-1-1.6B and the instruction-tuned\n  version is available at\n  https://huggingface.co/tensoropera/Fox-1-1.6B-Instruct-v0.1",
      "pdf_url": "http://arxiv.org/pdf/2411.05281v3",
      "published_date": "2024-11-08 02:24:29 UTC",
      "updated_date": "2025-04-08 01:39:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:54:24.438979"
    },
    {
      "arxiv_id": "2411.05273v1",
      "title": "Real-World Offline Reinforcement Learning from Vision Language Model Feedback",
      "title_zh": "基于视觉语言模型反馈的真实世界离线强化学习",
      "authors": [
        "Sreyas Venkataraman",
        "Yufei Wang",
        "Ziyu Wang",
        "Zackory Erickson",
        "David Held"
      ],
      "abstract": "Offline reinforcement learning can enable policy learning from pre-collected,\nsub-optimal datasets without online interactions. This makes it ideal for\nreal-world robots and safety-critical scenarios, where collecting online data\nor expert demonstrations is slow, costly, and risky. However, most existing\noffline RL works assume the dataset is already labeled with the task rewards, a\nprocess that often requires significant human effort, especially when\nground-truth states are hard to ascertain (e.g., in the real-world). In this\npaper, we build on prior work, specifically RL-VLM-F, and propose a novel\nsystem that automatically generates reward labels for offline datasets using\npreference feedback from a vision-language model and a text description of the\ntask. Our method then learns a policy using offline RL with the reward-labeled\ndataset. We demonstrate the system's applicability to a complex real-world\nrobot-assisted dressing task, where we first learn a reward function using a\nvision-language model on a sub-optimal offline dataset, and then we use the\nlearned reward to employ Implicit Q learning to develop an effective dressing\npolicy. Our method also performs well in simulation tasks involving the\nmanipulation of rigid and deformable objects, and significantly outperform\nbaselines such as behavior cloning and inverse RL. In summary, we propose a new\nsystem that enables automatic reward labeling and policy learning from\nunlabeled, sub-optimal offline datasets.",
      "tldr_zh": "这篇论文提出了一种新系统，利用视觉语言模型(Vision-Language Model)的偏好反馈和任务文本描述，从未标注的次优离线数据集自动生成奖励标签，从而实现Offline Reinforcement Learning。基于先前的RL-VLM-F工作，该方法学习奖励函数并应用Implicit Q Learning来训练策略，适用于复杂真实世界任务，如机器人辅助穿衣，以及模拟中的刚性和可变形物体操作。实验结果显示，该系统在这些任务中显著优于行为克隆(Behavior Cloning)和逆强化学习(Inverse RL)基线，为安全关键场景下的机器人学习提供了高效、可扩展的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages. Accepted at the LangRob Workshop 2024 @ CoRL, 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.05273v1",
      "published_date": "2024-11-08 02:12:34 UTC",
      "updated_date": "2024-11-08 02:12:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:54:35.422057"
    },
    {
      "arxiv_id": "2411.05270v1",
      "title": "Seeing Through the Fog: A Cost-Effectiveness Analysis of Hallucination Detection Systems",
      "title_zh": "拨开迷雾：幻觉检测系统的成本效益分析",
      "authors": [
        "Alexander Thomas",
        "Seth Rosen",
        "Vishnu Vettrivel"
      ],
      "abstract": "This paper presents a comparative analysis of hallucination detection systems\nfor AI, focusing on automatic summarization and question answering tasks for\nLarge Language Models (LLMs). We evaluate different hallucination detection\nsystems using the diagnostic odds ratio (DOR) and cost-effectiveness metrics.\nOur results indicate that although advanced models can perform better they come\nat a much higher cost. We also demonstrate how an ideal hallucination detection\nsystem needs to maintain performance across different model sizes. Our findings\nhighlight the importance of choosing a detection system aligned with specific\napplication needs and resource constraints. Future research will explore hybrid\nsystems and automated identification of underperforming components to enhance\nAI reliability and efficiency in detecting and mitigating hallucinations.",
      "tldr_zh": "这篇论文比较了 AI 中幻觉检测系统在大型语言模型 (LLMs) 的自动摘要和问答任务中的性能，使用诊断优势比 (DOR) 和成本效益指标进行评估。结果显示，虽然高级模型的检测效果更好，但其成本显著增加，且理想系统需在不同模型规模上保持一致性能。研究强调，选择合适的检测系统应考虑特定应用需求和资源限制，并建议未来探索混合系统和自动识别低效组件，以提升 AI 在检测和缓解幻觉方面的可靠性和效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pags, 13 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.05270v1",
      "published_date": "2024-11-08 02:06:41 UTC",
      "updated_date": "2024-11-08 02:06:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:54:47.364142"
    },
    {
      "arxiv_id": "2411.05263v1",
      "title": "Minimal Conditions for Beneficial Neighbourhood Search and Local Descent",
      "title_zh": "翻译失败",
      "authors": [
        "Mark G. Wallace"
      ],
      "abstract": "This paper investigates what properties a neighbourhood requires to support\nbeneficial local search. We show that neighbourhood locality, and a reduction\nin cost probability towards the optimum, support a proof that search among\nneighbours is more likely to find an improving solution in a single search step\nthan blind search. This is the first paper to introduce such a proof. The\nconcepts underlying these properties are illustrated on a satisfiability\nproblem class, and on travelling salesman problems. Secondly, for a given cost\ntarget t, we investigate a combination of blind search and local descent termed\nlocal blind descent, and present various conditions under which the expected\nnumber of steps to reach a cost better than t using local blind descent, is\nproven to be smaller than with blind search. Experiments indicate that local\nblind descent, given target cost t, should switch to local descent at a\nstarting cost that reduces as t approaches the optimum.",
      "tldr_zh": "这篇论文探讨了邻域搜索(neighbourhood search)和局部下降(local descent)所需的最小条件，证明了邻域的局部性(locality)和成本减少概率向最优解靠拢，能使单步搜索比盲搜(blind search)更可能找到改进解，这是首次引入此类证明。论文通过可满足性问题(satisfiability problem)和旅行 salesman 问题(travelling salesman problems)示例说明了这些概念。其次，它分析了局部盲下降(local blind descent)——一种结合盲搜和局部下降的方法，并证明在特定条件下，其期望步数优于盲搜；实验结果建议，当目标成本 t 接近最优时，应在较低起始成本处切换到局部下降。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05263v1",
      "published_date": "2024-11-08 01:47:40 UTC",
      "updated_date": "2024-11-08 01:47:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:54:59.667790"
    },
    {
      "arxiv_id": "2411.05261v2",
      "title": "Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yingying Fang",
        "Zihao Jin",
        "Shaojie Guo",
        "Jinda Liu",
        "Zhiling Yue",
        "Yijian Gao",
        "Junzhi Ning",
        "Zhi Li",
        "Simon Walsh",
        "Guang Yang"
      ],
      "abstract": "Despite significant advancements in automated report generation, the\nopaqueness of text interpretability continues to cast doubt on the reliability\nof the content produced. This paper introduces a novel approach to identify\nspecific image features in X-ray images that influence the outputs of report\ngeneration models. Specifically, we propose Cyclic Vision-Language Manipulator\nCVLM, a module to generate a manipulated X-ray from an original X-ray and its\nreport from a designated report generator. The essence of CVLM is that cycling\nmanipulated X-rays to the report generator produces altered reports aligned\nwith the alterations pre-injected into the reports for X-ray generation,\nachieving the term \"cyclic manipulation\". This process allows direct comparison\nbetween original and manipulated X-rays, clarifying the critical image features\ndriving changes in reports and enabling model users to assess the reliability\nof the generated texts. Empirical evaluations demonstrate that CVLM can\nidentify more precise and reliable features compared to existing explanation\nmethods, significantly enhancing the transparency and applicability of\nAI-generated reports.",
      "tldr_zh": "这篇论文针对自动报告生成的文本可解释性问题，提出了一种新方法 Cyclic Vision-Language Manipulator (CVLM)，用于识别 X-ray 图像中影响报告输出生成的特定图像特征。CVLM 通过从原始 X-ray 和报告生成修改后的 X-ray，然后将修改后的 X-ray 循环回报告生成器，产生与预注入变更一致的修改报告，实现“cyclic manipulation”。这种过程允许直接比较原始和修改后的图像，从而澄清关键特征驱动报告变化，提升模型的透明度和可靠性。实验评估表明，CVLM 比现有解释方法更精确可靠，有助于提高 AI 生成报告的适用性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05261v2",
      "published_date": "2024-11-08 01:46:11 UTC",
      "updated_date": "2025-05-07 01:51:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:55:12.687259"
    },
    {
      "arxiv_id": "2411.05260v1",
      "title": "QuanCrypt-FL: Quantized Homomorphic Encryption with Pruning for Secure Federated Learning",
      "title_zh": "QuanCrypt-FL：量化同态加密结合修剪用于安全的联邦学习",
      "authors": [
        "Md Jueal Mia",
        "M. Hadi Amini"
      ],
      "abstract": "Federated Learning has emerged as a leading approach for decentralized\nmachine learning, enabling multiple clients to collaboratively train a shared\nmodel without exchanging private data. While FL enhances data privacy, it\nremains vulnerable to inference attacks, such as gradient inversion and\nmembership inference, during both training and inference phases. Homomorphic\nEncryption provides a promising solution by encrypting model updates to protect\nagainst such attacks, but it introduces substantial communication overhead,\nslowing down training and increasing computational costs. To address these\nchallenges, we propose QuanCrypt-FL, a novel algorithm that combines low-bit\nquantization and pruning techniques to enhance protection against attacks while\nsignificantly reducing computational costs during training. Further, we propose\nand implement mean-based clipping to mitigate quantization overflow or errors.\nBy integrating these methods, QuanCrypt-FL creates a communication-efficient FL\nframework that ensures privacy protection with minimal impact on model\naccuracy, thereby improving both computational efficiency and attack\nresilience. We validate our approach on MNIST, CIFAR-10, and CIFAR-100\ndatasets, demonstrating superior performance compared to state-of-the-art\nmethods. QuanCrypt-FL consistently outperforms existing method and matches\nVanilla-FL in terms of accuracy across varying client. Further, QuanCrypt-FL\nachieves up to 9x faster encryption, 16x faster decryption, and 1.5x faster\ninference compared to BatchCrypt, with training time reduced by up to 3x.",
      "tldr_zh": "该论文提出 QuanCrypt-FL，一种结合量化(quantization)和剪枝(pruning)的算法，用于提升 Federated Learning 的安全性，解决 Homomorphic Encryption 在隐私保护中带来的通信和计算开销问题。方法包括低位量化、剪枝技术以及 mean-based clipping，以减少量化错误并维持模型准确性。在 MNIST、CIFAR-10 和 CIFAR-100 数据集上的实验表明，QuanCrypt-FL 比现有方法准确性相当甚至优越，同时实现加密快 9 倍、解密快 16 倍、推理快 1.5 倍，以及训练时间减少 3 倍。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05260v1",
      "published_date": "2024-11-08 01:46:00 UTC",
      "updated_date": "2024-11-08 01:46:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:55:24.309490"
    },
    {
      "arxiv_id": "2411.05877v1",
      "title": "Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass",
      "title_zh": "翻译失败",
      "authors": [
        "Tong Chen",
        "Hao Fang",
        "Patrick Xia",
        "Xiaodong Liu",
        "Benjamin Van Durme",
        "Luke Zettlemoyer",
        "Jianfeng Gao",
        "Hao Cheng"
      ],
      "abstract": "Large language models (LMs) are typically adapted to improve performance on\nnew contexts (\\eg text prompts that define new tasks or domains) through\nfine-tuning or prompting. However, there is an accuracy compute tradeoff --\nfine-tuning incurs significant training cost and prompting increases inference\noverhead. We introduce $GenerativeAdapter$, an effective and efficient\nadaptation method that directly maps new contexts to low-rank LM adapters,\nthereby significantly reducing inference overhead with no need for finetuning.\nThe adapter generator is trained via self-supervised learning, and can be used\nto adapt a single frozen LM for any new task simply by mapping the associated\ntask or domain context to a new adapter. We apply $GenerativeAdapter$ to two\npretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the\nadapted models in three adaption scenarios: knowledge acquisition from\ndocuments, learning from demonstrations, and personalization for users. In\nStreamingQA, our approach is effective in injecting knowledge into the LM's\nparameters, achieving a 63.5% improvement in F1 score over the model with\nsupervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K\ntokens. In the MetaICL in-context learning evaluation, our method achieves an\naverage accuracy of $44.9$ across 26 tasks, outperforming the base model. On\nMSC, our method proves to be highly competitive in memorizing user information\nfrom conversations with a 4x reduction in computation and memory costs compared\nto prompting with full conversation history. Together, these results suggest\nthat $GenerativeAdapter$ should allow for general adaption to a wide range of\ndifferent contexts.",
      "tldr_zh": "该研究提出Generative Adapter，一种高效的语言模型(LMs)适应方法，通过单前向传递将新上下文（如任务或领域）直接映射到低秩适配器，从而避免微调的训练成本和提示的推理开销。适配器生成器通过自监督学习训练，可应用于预训练模型如Mistral-7B-Instruct和Llama2-7B-Chat，实现从文档知识获取、演示学习到用户个性化的多种场景。实验结果显示，在StreamingQA任务中F1分数从19.5提升至31.5（提升63.5%），在MetaICL评估中平均准确率达44.9%优于基线模型，且在MSC任务上计算和内存成本较提示方法减少4倍。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.05877v1",
      "published_date": "2024-11-08 00:42:47 UTC",
      "updated_date": "2024-11-08 00:42:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T22:55:36.418305"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 86,
  "processed_papers_count": 86,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T22:55:53.980791"
}
[
  {
    "arxiv_id": "2401.13849v1",
    "title": "TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance",
    "authors": [
      "Haorui Wang",
      "Rongzhi Zhang",
      "Yinghao Li",
      "Lingkai Kong",
      "Yuchen Zhuang",
      "Xiusi Chen",
      "Chao Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have recently showcased remarkable reasoning\nabilities. However, larger models often surpass their smaller counterparts in\nreasoning tasks, posing the challenge of effectively transferring these\ncapabilities from larger models. Existing approaches heavily rely on extensive\nfine-tuning data or continuous interactions with a superior teacher LLM during\ninference. We introduce a principle-based teacher-student framework called\n``Teaching via Principle Discovery'' (TPD) to address these limitations.\nInspired by human learning mechanisms, TPD mimics the interaction between a\nteacher and a student using a principle-based approach. The teacher LLM\ngenerates problem-solving instructions and corrective principles based on the\nstudent LLM's errors. These principles guide the refinement of instructions and\nthe selection of instructive examples from a validation set. This enables the\nstudent model to learn from both the teacher's guidance and its own mistakes.\nOnce the student model begins making inferences, TPD requires no further\nintervention from the teacher LLM or humans. Through extensive experiments\nacross eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared\nto standard chain-of-thought prompting, TPD significantly improves the student\nmodel's performance, achieving $6.2\\%$ improvement on average.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13849v1",
    "published_date": "2024-01-24 23:11:33 UTC",
    "updated_date": "2024-01-24 23:11:33 UTC"
  },
  {
    "arxiv_id": "2401.13848v1",
    "title": "A V2X-based Privacy Preserving Federated Measuring and Learning System",
    "authors": [
      "Levente Alekszejenk√≥",
      "Tadeusz Dobrowiecki"
    ],
    "abstract": "Future autonomous vehicles (AVs) will use a variety of sensors that generate\na vast amount of data. Naturally, this data not only serves self-driving\nalgorithms; but can also assist other vehicles or the infrastructure in\nreal-time decision-making. Consequently, vehicles shall exchange their\nmeasurement data over Vehicle-to-Everything (V2X) technologies. Moreover,\npredicting the state of the road network might be beneficial too. With such a\nprediction, we might mitigate road congestion, balance parking lot usage, or\noptimize the traffic flow. That would decrease transportation costs as well as\nreduce its environmental impact.\n  In this paper, we propose a federated measurement and learning system that\nprovides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V)\ncommunication while also operating a federated learning (FL) scheme over the\nVehicle-to-Network (V2N) link to create a predictive model of the\ntransportation network. As we are yet to have real-world AV data, we model it\nwith a non-IID (independent and identically distributed) dataset to evaluate\nthe capabilities of the proposed system in terms of performance and privacy.\nResults indicate that the proposed FL scheme improves learning performance and\nprevents eavesdropping at the aggregator server side.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML",
      "68T07, 68T42, 68P27, 68P25",
      "I.2.6; I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.13848v1",
    "published_date": "2024-01-24 23:11:11 UTC",
    "updated_date": "2024-01-24 23:11:11 UTC"
  },
  {
    "arxiv_id": "2402.05940v1",
    "title": "Causal Relationship Network of Risk Factors Impacting Workday Loss in Underground Coal Mines",
    "authors": [
      "Shangsi Ren",
      "Cameron A. Beeche",
      "Zhiyi Shi",
      "Maria Acevedo Garcia",
      "Katherine Zychowski",
      "Shuguang Leng",
      "Pedram Roghanchi",
      "Jiantao Pu"
    ],
    "abstract": "This study aims to establish the causal relationship network between various\nfactors leading to workday loss in underground coal mines using a novel causal\nartificial intelligence (AI) method. The analysis utilizes data obtained from\nthe National Institute for Occupational Safety and Health (NIOSH). A total of\n101,010 injury records from 3,982 unique underground coal mines spanning the\nyears from 1990 to 2020 were extracted from the NIOSH database. Causal\nrelationships were analyzed and visualized using a novel causal AI method\ncalled Grouped Greedy Equivalence Search (GGES). The impact of each variable on\nworkday loss was assessed through intervention do-calculus adjustment (IDA)\nscores. Model training and validation were performed using the 10-fold\ncross-validation technique. Performance metrics, including adjacency precision\n(AP), adjacency recall (AR), arrowhead precision (AHP), and arrowhead recall\n(AHR), were utilized to evaluate the models. Findings revealed that after 2006,\nkey direct causes of workday loss among mining employees included total mining\nexperience, mean office employees, mean underground employees, county, and\ntotal mining experience (years). Total mining experience emerged as the most\ninfluential factor, whereas mean employees per mine exhibited the least\ninfluence. The analyses emphasized the significant role of total mining\nexperience in determining workday loss. The models achieved optimal\nperformance, with AP, AR, AHP, and AHR values measuring 0.694, 0.653, 0.386,\nand 0.345, respectively. This study demonstrates the feasibility of utilizing\nthe new GGES method to clarify the causal factors behind the workday loss by\nanalyzing employment demographics and injury records and establish their causal\nrelationship network.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "5 figures 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.05940v1",
    "published_date": "2024-01-24 22:45:34 UTC",
    "updated_date": "2024-01-24 22:45:34 UTC"
  },
  {
    "arxiv_id": "2402.01704v3",
    "title": "Steering Language Models with Game-Theoretic Solvers",
    "authors": [
      "Ian Gemp",
      "Roma Patel",
      "Yoram Bachrach",
      "Marc Lanctot",
      "Vibhavari Dasagi",
      "Luke Marris",
      "Georgios Piliouras",
      "Siqi Liu",
      "Karl Tuyls"
    ],
    "abstract": "Mathematical models of interactions among rational agents have long been\nstudied in game theory. However these interactions are often over a small set\nof discrete game actions which is very different from how humans communicate in\nnatural language. To bridge this gap, we introduce a framework that allows\nequilibrium solvers to work over the space of natural language dialogue\ngenerated by large language models (LLMs). Specifically, by modelling the\nplayers, strategies and payoffs in a \"game\" of dialogue, we create a binding\nfrom natural language interactions to the conventional symbolic logic of game\ntheory. Given this binding, we can ask existing game-theoretic algorithms to\nprovide us with strategic solutions (e.g., what string an LLM should generate\nto maximize payoff in the face of strategic partners or opponents), giving us\npredictors of stable, rational conversational strategies. We focus on three\ndomains that require different negotiation strategies: scheduling meetings,\ntrading fruit and debate, and evaluate an LLM's generated language when guided\nby solvers. We see that LLMs that follow game-theory solvers result in dialogue\ngenerations that are less exploitable than the control (no guidance from\nsolvers), and the language generated results in higher rewards, in all\nnegotiation domains. We discuss future implications of this work, and how\ngame-theoretic solvers that can leverage the expressivity of natural language\ncan open up a new avenue of guiding language research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.CL",
    "comment": "Code available @\n  https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/games/chat_game.py",
    "pdf_url": "http://arxiv.org/pdf/2402.01704v3",
    "published_date": "2024-01-24 22:22:00 UTC",
    "updated_date": "2024-12-16 11:03:31 UTC"
  },
  {
    "arxiv_id": "2401.13835v2",
    "title": "What Large Language Models Know and What People Think They Know",
    "authors": [
      "Mark Steyvers",
      "Heliodoro Tejeda",
      "Aakriti Kumar",
      "Catarina Belem",
      "Sheer Karny",
      "Xinyue Hu",
      "Lukas Mayer",
      "Padhraic Smyth"
    ],
    "abstract": "As artificial intelligence (AI) systems, particularly large language models\n(LLMs), become increasingly integrated into decision-making processes, the\nability to trust their outputs is crucial. To earn human trust, LLMs must be\nwell calibrated such that they can accurately assess and communicate the\nlikelihood of their predictions being correct. Whereas recent work has focused\non LLMs' internal confidence, less is understood about how effectively they\nconvey uncertainty to users. Here we explore the calibration gap, which refers\nto the difference between human confidence in LLM-generated answers and the\nmodels' actual confidence, and the discrimination gap, which reflects how well\nhumans and models can distinguish between correct and incorrect answers. Our\nexperiments with multiple-choice and short-answer questions reveal that users\ntend to overestimate the accuracy of LLM responses when provided with default\nexplanations. Moreover, longer explanations increased user confidence, even\nwhen the extra length did not improve answer accuracy. By adjusting LLM\nexplanations to better reflect the models' internal confidence, both the\ncalibration gap and the discrimination gap narrowed, significantly improving\nuser perception of LLM accuracy. These findings underscore the importance of\naccurate uncertainty communication and highlight the effect of explanation\nlength in influencing user trust in AI-assisted decision-making environments.\nCode and Data can be found at https://osf.io/y7pr6/ . Journal publication can\nbe found on Nature Machine Intelligence at\nhttps://www.nature.com/articles/s42256-024-00976-7 .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 10 figures For the journal publication on Nature Machine\n  Intelligence see https://www.nature.com/articles/s42256-024-00976-7 For the\n  data and code see https://osf.io/y7pr6/",
    "pdf_url": "http://arxiv.org/pdf/2401.13835v2",
    "published_date": "2024-01-24 22:21:04 UTC",
    "updated_date": "2025-02-13 08:13:52 UTC"
  },
  {
    "arxiv_id": "2401.13827v1",
    "title": "Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink in Markovian IoT Models",
    "authors": [
      "Eslam Eldeeb",
      "Mohammad Shehab",
      "Hirley Alves"
    ],
    "abstract": "The age of information (AoI) is used to measure the freshness of the data. In\nIoT networks, the traditional resource management schemes rely on a message\nexchange between the devices and the base station (BS) before communication\nwhich causes high AoI, high energy consumption, and low reliability. Unmanned\naerial vehicles (UAVs) as flying BSs have many advantages in minimizing the\nAoI, energy-saving, and throughput improvement. In this paper, we present a\nnovel learning-based framework that estimates the traffic arrival of IoT\ndevices based on Markovian events. The learning proceeds to optimize the\ntrajectory of multiple UAVs and their scheduling policy. First, the BS predicts\nthe future traffic of the devices. We compare two traffic predictors: the\nforward algorithm (FA) and the long short-term memory (LSTM). Afterward, we\npropose a deep reinforcement learning (DRL) approach to optimize the optimal\npolicy of each UAV. Finally, we manipulate the optimum reward function for the\nproposed DRL approach. Simulation results show that the proposed algorithm\noutperforms the random-walk (RW) baseline model regarding the AoI, scheduling\naccuracy, and transmission power.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13827v1",
    "published_date": "2024-01-24 21:57:55 UTC",
    "updated_date": "2024-01-24 21:57:55 UTC"
  },
  {
    "arxiv_id": "2401.13822v1",
    "title": "Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on Hugging Face",
    "authors": [
      "Xinyu Yang",
      "Weixin Liang",
      "James Zou"
    ],
    "abstract": "Advances in machine learning are closely tied to the creation of datasets.\nWhile data documentation is widely recognized as essential to the reliability,\nreproducibility, and transparency of ML, we lack a systematic empirical\nunderstanding of current dataset documentation practices. To shed light on this\nquestion, here we take Hugging Face -- one of the largest platforms for sharing\nand collaborating on ML models and datasets -- as a prominent case study. By\nanalyzing all 7,433 dataset documentation on Hugging Face, our investigation\nprovides an overview of the Hugging Face dataset ecosystem and insights into\ndataset documentation practices, yielding 5 main findings: (1) The dataset card\ncompletion rate shows marked heterogeneity correlated with dataset popularity.\n(2) A granular examination of each section within the dataset card reveals that\nthe practitioners seem to prioritize Dataset Description and Dataset Structure\nsections, while the Considerations for Using the Data section receives the\nlowest proportion of content. (3) By analyzing the subsections within each\nsection and utilizing topic modeling to identify key topics, we uncover what is\ndiscussed in each section, and underscore significant themes encompassing both\ntechnical and social impacts, as well as limitations within the Considerations\nfor Using the Data section. (4) Our findings also highlight the need for\nimproved accessibility and reproducibility of datasets in the Usage sections.\n(5) In addition, our human annotation evaluation emphasizes the pivotal role of\ncomprehensive dataset content in shaping individuals' perceptions of a dataset\ncard's overall quality. Overall, our study offers a unique perspective on\nanalyzing dataset documentation through large-scale data science analysis and\nunderlines the need for more thorough dataset documentation in machine learning\nresearch.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the main conference of ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.13822v1",
    "published_date": "2024-01-24 21:47:13 UTC",
    "updated_date": "2024-01-24 21:47:13 UTC"
  },
  {
    "arxiv_id": "2402.04264v1",
    "title": "Analysis of Hopfield Model as Associative Memory",
    "authors": [
      "Matteo Silvestri"
    ],
    "abstract": "This article delves into the Hopfield neural network model, drawing\ninspiration from biological neural systems. The exploration begins with an\noverview of the model's foundations, incorporating insights from mechanical\nstatistics to deepen our understanding. Focusing on audio retrieval, the study\ndemonstrates the Hopfield model's associative memory capabilities. Through\npractical implementation, the network is trained to retrieve different\npatterns.",
    "categories": [
      "cond-mat.dis-nn",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cond-mat.dis-nn",
    "comment": "35 pages, 23 figures, 3 codes",
    "pdf_url": "http://arxiv.org/pdf/2402.04264v1",
    "published_date": "2024-01-24 21:10:38 UTC",
    "updated_date": "2024-01-24 21:10:38 UTC"
  },
  {
    "arxiv_id": "2401.13802v3",
    "title": "Investigating the Efficacy of Large Language Models for Code Clone Detection",
    "authors": [
      "Mohamad Khajezade",
      "Jie JW Wu",
      "Fatemeh Hendijani Fard",
      "Gema Rodr√≠guez-P√©rez",
      "Mohamed Sami Shehata"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in various\nnatural language processing and software engineering tasks, such as code\ngeneration. The LLMs are mainly utilized in the prompt-based zero/few-shot\nparadigm to guide the model in accomplishing the task. GPT-based models are one\nof the popular ones studied for tasks such as code comment generation or test\ngeneration. These tasks are `generative' tasks. However, there is limited\nresearch on the usage of LLMs for `non-generative' tasks such as classification\nusing the prompt-based paradigm. In this preliminary exploratory study, we\ninvestigated the applicability of LLMs for Code Clone Detection (CCD), a\nnon-generative task. By building a mono-lingual and cross-lingual CCD dataset\nderived from CodeNet, we first investigated two different prompts using ChatGPT\nto detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot\nsetting. We then conducted an analysis to understand the strengths and\nweaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language\nCCD attaining an F1-score of 0.877 and achieves comparable performance to fully\nfine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the\nprompt and the difficulty level of the problems has an impact on the\nperformance of ChatGPT. Finally we provide insights and future directions based\non our initial analysis",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13802v3",
    "published_date": "2024-01-24 20:43:36 UTC",
    "updated_date": "2024-01-30 06:10:29 UTC"
  },
  {
    "arxiv_id": "2401.13800v1",
    "title": "Multi-Object Navigation in real environments using hybrid policies",
    "authors": [
      "Assem Sadek",
      "Guillaume Bono",
      "Boris Chidlovskii",
      "Atilla Baskurt",
      "Christian Wolf"
    ],
    "abstract": "Navigation has been classically solved in robotics through the combination of\nSLAM and planning. More recently, beyond waypoint planning, problems involving\nsignificant components of (visual) high-level reasoning have been explored in\nsimulated environments, mostly addressed with large-scale machine learning, in\nparticular RL, offline-RL or imitation learning. These methods require the\nagent to learn various skills like local planning, mapping objects and querying\nthe learned spatial representations. In contrast to simpler tasks like waypoint\nplanning (PointGoal), for these more complex tasks the current state-of-the-art\nmodels have been thoroughly evaluated in simulation but, to our best knowledge,\nnot yet in real environments.\n  In this work we focus on sim2real transfer. We target the challenging\nMulti-Object Navigation (Multi-ON) task and port it to a physical environment\ncontaining real replicas of the originally virtual Multi-ON objects. We\nintroduce a hybrid navigation method, which decomposes the problem into two\ndifferent skills: (1) waypoint navigation is addressed with classical SLAM\ncombined with a symbolic planner, whereas (2) exploration, semantic mapping and\ngoal retrieval are dealt with deep neural networks trained with a combination\nof supervised learning and RL. We show the advantages of this approach compared\nto end-to-end methods both in simulation and a real environment and outperform\nthe SOTA for this task.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13800v1",
    "published_date": "2024-01-24 20:41:25 UTC",
    "updated_date": "2024-01-24 20:41:25 UTC"
  },
  {
    "arxiv_id": "2401.13796v2",
    "title": "Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning",
    "authors": [
      "Andrea Apicella",
      "Francesco Isgr√≤",
      "Roberto Prevete"
    ],
    "abstract": "Machine Learning (ML) has revolutionized various domains, offering predictive\ncapabilities in several areas. However, with the increasing accessibility of ML\ntools, many practitioners, lacking deep ML expertise, adopt a \"push the button\"\napproach, utilizing user-friendly interfaces without a thorough understanding\nof underlying algorithms. While this approach provides convenience, it raises\nconcerns about the reliability of outcomes, leading to challenges such as\nincorrect performance evaluation. This paper addresses a critical issue in ML,\nknown as data leakage, where unintended information contaminates the training\ndata, impacting model performance evaluation. Users, due to a lack of\nunderstanding, may inadvertently overlook crucial steps, leading to optimistic\nperformance estimates that may not hold in real-world scenarios. The\ndiscrepancy between evaluated and actual performance on new data is a\nsignificant concern. In particular, this paper categorizes data leakage in ML,\ndiscussing how certain conditions can propagate through the ML workflow.\nFurthermore, it explores the connection between data leakage and the specific\ntask being addressed, investigates its occurrence in Transfer Learning, and\ncompares standard inductive ML with transductive ML frameworks. The conclusion\nsummarizes key findings, emphasizing the importance of addressing data leakage\nfor robust and reliable ML applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2401.13796v2",
    "published_date": "2024-01-24 20:30:52 UTC",
    "updated_date": "2024-10-20 11:35:47 UTC"
  },
  {
    "arxiv_id": "2401.13782v3",
    "title": "Position: AI/ML Influencers Have a Place in the Academic Process",
    "authors": [
      "Iain Xie Weissburg",
      "Mehir Arora",
      "Xinyi Wang",
      "Liangming Pan",
      "William Yang Wang"
    ],
    "abstract": "As the number of accepted papers at AI and ML conferences reaches into the\nthousands, it has become unclear how researchers access and read research\npublications. In this paper, we investigate the role of social media\ninfluencers in enhancing the visibility of machine learning research,\nparticularly the citation counts of papers they share. We have compiled a\ncomprehensive dataset of over 8,000 papers, spanning tweets from December 2018\nto October 2023, alongside controls precisely matched by 9 key covariates. Our\nstatistical and causal inference analysis reveals a significant increase in\ncitations for papers endorsed by these influencers, with median citation counts\n2-3 times higher than those of the control group. Additionally, the study\ndelves into the geographic, gender, and institutional diversity of highlighted\nauthors. Given these findings, we advocate for a responsible approach to\ncuration, encouraging influencers to uphold the journalistic standard that\nincludes showcasing diverse research topics, authors, and institutions.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.DL",
    "comment": "15 Pages, 22 Figures, ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.13782v3",
    "published_date": "2024-01-24 20:05:49 UTC",
    "updated_date": "2024-07-23 14:49:43 UTC"
  },
  {
    "arxiv_id": "2402.01703v3",
    "title": "A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles",
    "authors": [
      "Benjamin A. T. Grahama",
      "Lauren Brown",
      "Georgios Chochlakis",
      "Morteza Dehghani",
      "Raquel Delerme",
      "Brittany Friedman",
      "Ellie Graeden",
      "Preni Golazizian",
      "Rajat Hebbar",
      "Parsa Hejabi",
      "Aditya Kommineni",
      "Mayag√ºez Salinas",
      "Michael Sierra-Ar√©valo",
      "Jackson Trager",
      "Nicholas Weller",
      "Shrikanth Narayanan"
    ],
    "abstract": "Interactions between the government officials and civilians affect public\nwellbeing and the state legitimacy that is necessary for the functioning of\ndemocratic society. Police officers, the most visible and contacted agents of\nthe state, interact with the public more than 20 million times a year during\ntraffic stops. Today, these interactions are regularly recorded by body-worn\ncameras (BWCs), which are lauded as a means to enhance police accountability\nand improve police-public interactions. However, the timely analysis of these\nrecordings is hampered by a lack of reliable automated tools that can enable\nthe analysis of these complex and contested police-public interactions. This\narticle proposes an approach to developing new multi-perspective, multimodal\nmachine learning (ML) tools to analyze the audio, video, and transcript\ninformation from this BWC footage. Our approach begins by identifying the\naspects of communication most salient to different stakeholders, including both\ncommunity members and police officers. We move away from modeling approaches\nbuilt around the existence of a single ground truth and instead utilize new\nadvances in soft labeling to incorporate variation in how different observers\nperceive the same interactions. We argue that this inclusive approach to the\nconceptualization and design of new ML tools is broadly applicable to the study\nof communication and development of analytic tools across domains of human\ninteraction, including education, medicine, and the workplace.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "I.2.0; I.2.7"
    ],
    "primary_category": "cs.CY",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.01703v3",
    "published_date": "2024-01-24 19:56:20 UTC",
    "updated_date": "2024-02-09 05:25:11 UTC"
  },
  {
    "arxiv_id": "2401.13770v1",
    "title": "AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard Combinatorial Problems",
    "authors": [
      "Piyush Jha",
      "Zhengyu Li",
      "Zhengyang Lu",
      "Curtis Bright",
      "Vijay Ganesh"
    ],
    "abstract": "This paper introduces AlphaMapleSAT, a novel Monte Carlo Tree Search (MCTS)\nbased Cube-and-Conquer (CnC) SAT solving method aimed at efficiently solving\nchallenging combinatorial problems. Despite the tremendous success of CnC\nsolvers in solving a variety of hard combinatorial problems, the lookahead\ncubing techniques at the heart of CnC have not evolved much for many years.\nPart of the reason is the sheer difficulty of coming up with new cubing\ntechniques that are both low-cost and effective in partitioning input formulas\ninto sub-formulas, such that the overall runtime is minimized.\n  Lookahead cubing techniques used by current state-of-the-art CnC solvers,\nsuch as March, keep their cubing costs low by constraining the search for the\noptimal splitting variables. By contrast, our key innovation is a\ndeductively-driven MCTS-based lookahead cubing technique, that performs a\ndeeper heuristic search to find effective cubes, while keeping the cubing cost\nlow. We perform an extensive comparison of AlphaMapleSAT against the March CnC\nsolver on challenging combinatorial problems such as the minimum Kochen-Specker\nand Ramsey problems. We also perform ablation studies to verify the efficacy of\nthe MCTS heuristic search for the cubing problem. Results show up to 2.3x\nspeedup in parallel (and up to 27x in sequential) elapsed real time.",
    "categories": [
      "cs.AI",
      "math.CO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13770v1",
    "published_date": "2024-01-24 19:37:10 UTC",
    "updated_date": "2024-01-24 19:37:10 UTC"
  },
  {
    "arxiv_id": "2401.13758v2",
    "title": "Assumptions and Bounds in the Instrumental Variable Model",
    "authors": [
      "Thomas S. Richardson",
      "James M. Robins"
    ],
    "abstract": "In this note we give proofs for results relating to the Instrumental Variable\n(IV) model with binary response $Y$ and binary treatment $X$, but with an\ninstrument $Z$ with $K$ states. These results were originally stated in\nRichardson & Robins (2014), \"ACE Bounds; SEMS with Equilibrium Conditions,\"\narXiv:1410.0470.",
    "categories": [
      "math.ST",
      "cs.AI",
      "stat.TH",
      "62A01 (Primary) 62D20, 62H22 (Secondary)"
    ],
    "primary_category": "math.ST",
    "comment": "27 pages, 1 figure, 1 table. Proofs of Theorems 1 and 2 stated in\n  Richardson and Robins (2014) [arXiv:1410.0470]. v2 improves the writing in a\n  few places",
    "pdf_url": "http://arxiv.org/pdf/2401.13758v2",
    "published_date": "2024-01-24 19:18:34 UTC",
    "updated_date": "2024-01-26 03:11:58 UTC"
  },
  {
    "arxiv_id": "2401.13752v1",
    "title": "Explaining Image Classifiers",
    "authors": [
      "Hana Chockler",
      "Joseph Y. Halpern"
    ],
    "abstract": "We focus on explaining image classifiers, taking the work of Mothilal et al.\n[2021] (MMTS) as our point of departure. We observe that, although MMTS claim\nto be using the definition of explanation proposed by Halpern [2016], they do\nnot quite do so. Roughly speaking, Halpern's definition has a necessity clause\nand a sufficiency clause. MMTS replace the necessity clause by a requirement\nthat, as we show, implies it. Halpern's definition also allows agents to\nrestrict the set of options considered. While these difference may seem minor,\nas we show, they can have a nontrivial impact on explanations. We also show\nthat, essentially without change, Halpern's definition can handle two issues\nthat have proved difficult for other approaches: explanations of absence (when,\nfor example, an image classifier for tumors outputs \"no tumor\") and\nexplanations of rare events (such as tumors).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13752v1",
    "published_date": "2024-01-24 19:12:38 UTC",
    "updated_date": "2024-01-24 19:12:38 UTC"
  },
  {
    "arxiv_id": "2401.13751v2",
    "title": "A Training Rate and Survival Heuristic for Inference and Robustness Evaluation (TRASHFIRE)",
    "authors": [
      "Charles Meyers",
      "Mohammad Reza Saleh Sedghpour",
      "Tommy L√∂fstedt",
      "Erik Elmroth"
    ],
    "abstract": "Machine learning models -- deep neural networks in particular -- have\nperformed remarkably well on benchmark datasets across a wide variety of\ndomains. However, the ease of finding adversarial counter-examples remains a\npersistent problem when training times are measured in hours or days and the\ntime needed to find a successful adversarial counter-example is measured in\nseconds. Much work has gone into generating and defending against these\nadversarial counter-examples, however the relative costs of attacks and\ndefences are rarely discussed. Additionally, machine learning research is\nalmost entirely guided by test/train metrics, but these would require billions\nof samples to meet industry standards. The present work addresses the problem\nof understanding and predicting how particular model hyper-parameters influence\nthe performance of a model in the presence of an adversary. The proposed\napproach uses survival models, worst-case examples, and a cost-aware analysis\nto precisely and accurately reject a particular model change during routine\nmodel training procedures rather than relying on real-world deployment,\nexpensive formal verification methods, or accurate simulations of very\ncomplicated systems (\\textit{e.g.}, digitally recreating every part of a car or\na plane). Through an evaluation of many pre-processing techniques, adversarial\ncounter-examples, and neural network configurations, the conclusion is that\ndeeper models do offer marginal gains in survival times compared to more\nshallow counterparts. However, we show that those gains are driven more by the\nmodel inference time than inherent robustness properties. Using the proposed\nmethodology, we show that ResNet is hopelessly insecure against even the\nsimplest of white box attacks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13751v2",
    "published_date": "2024-01-24 19:12:37 UTC",
    "updated_date": "2024-09-11 20:55:32 UTC"
  },
  {
    "arxiv_id": "2401.13662v2",
    "title": "The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations",
    "authors": [
      "Matthias Lehmann"
    ],
    "abstract": "In recent years, various powerful policy gradient algorithms have been\nproposed in deep reinforcement learning. While all these algorithms build on\nthe Policy Gradient Theorem, the specific design choices differ significantly\nacross algorithms. We provide a holistic overview of on-policy policy gradient\nalgorithms to facilitate the understanding of both their theoretical\nfoundations and their practical implementations. In this overview, we include a\ndetailed proof of the continuous version of the Policy Gradient Theorem,\nconvergence results and a comprehensive discussion of practical algorithms. We\ncompare the most prominent algorithms on continuous control environments and\nprovide insights on the benefits of regularization. All code is available at\nhttps://github.com/Matt00n/PolicyGradientsJax.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13662v2",
    "published_date": "2024-01-24 18:56:53 UTC",
    "updated_date": "2024-03-01 08:58:05 UTC"
  },
  {
    "arxiv_id": "2401.13657v2",
    "title": "Inadequacy of common stochastic neural networks for reliable clinical decision support",
    "authors": [
      "Adrian Lindenmeyer",
      "Malte Blattmann",
      "Stefan Franke",
      "Thomas Neumuth",
      "Daniel Schneider"
    ],
    "abstract": "Widespread adoption of AI for medical decision making is still hindered due\nto ethical and safety-related concerns. For AI-based decision support systems\nin healthcare settings it is paramount to be reliable and trustworthy. Common\ndeep learning approaches, however, have the tendency towards overconfidence\nunder data shift. Such inappropriate extrapolation beyond evidence-based\nscenarios may have dire consequences. This highlights the importance of\nreliable estimation of local uncertainty and its communication to the end user.\nWhile stochastic neural networks have been heralded as a potential solution to\nthese issues, this study investigates their actual reliability in clinical\napplications. We centered our analysis on the exemplary use case of mortality\nprediction for ICU hospitalizations using EHR from MIMIC3 study. For\npredictions on the EHR time series, Encoder-Only Transformer models were\nemployed. Stochasticity of model functions was achieved by incorporating common\nmethods such as Bayesian neural network layers and model ensembles. Our models\nachieve state of the art performance in terms of discrimination performance\n(AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality\nprediction benchmark. However, epistemic uncertainty is critically\nunderestimated by the selected stochastic deep learning methods. A heuristic\nproof for the responsible collapse of the posterior distribution is provided.\nOur findings reveal the inadequacy of commonly used stochastic deep learning\napproaches to reliably recognize OoD samples. In both methods, unsubstantiated\nmodel confidence is not prevented due to strongly biased functional posteriors,\nrendering them inappropriate for reliable clinical decision support. This\nhighlights the need for approaches with more strictly enforced or inherent\ndistance-awareness to known data points, e.g., using kernel-based techniques.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Keywords: probabilistic inference, uncertainty estimation,\n  uncertainty quantification, epistemic uncertainty, clinical prognosis,\n  electronic health records",
    "pdf_url": "http://arxiv.org/pdf/2401.13657v2",
    "published_date": "2024-01-24 18:49:30 UTC",
    "updated_date": "2024-01-25 12:31:21 UTC"
  },
  {
    "arxiv_id": "2401.13652v4",
    "title": "Graph-Instructed Neural Networks for Sparse Grid-Based Discontinuity Detectors",
    "authors": [
      "Francesco Della Santa",
      "Sandra Pieraccini"
    ],
    "abstract": "In this paper, we present a novel approach for detecting the discontinuity\ninterfaces of a discontinuous function. This approach leverages\nGraph-Instructed Neural Networks (GINNs) and sparse grids to address\ndiscontinuity detection also in domains of dimension larger than 3. GINNs,\ntrained to identify troubled points on sparse grids, exploit graph structures\nbuilt on the grids to achieve efficient and accurate discontinuity detection\nperformances. We also introduce a recursive algorithm for general sparse\ngrid-based detectors, characterized by convergence properties and easy\napplicability. Numerical experiments on functions with dimensions n = 2 and n =\n4 demonstrate the efficiency and robust generalization properties of GINNs in\ndetecting discontinuity interfaces. Notably, the trained GINNs offer\nportability and versatility, allowing integration into various algorithms and\nsharing among users.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA",
      "68T07, 03D32, 65D40"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13652v4",
    "published_date": "2024-01-24 18:44:14 UTC",
    "updated_date": "2025-03-26 16:57:20 UTC"
  },
  {
    "arxiv_id": "2401.13641v2",
    "title": "How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability",
    "authors": [
      "Ivan DeAndres-Tame",
      "Ruben Tolosana",
      "Ruben Vera-Rodriguez",
      "Aythami Morales",
      "Julian Fierrez",
      "Javier Ortega-Garcia"
    ],
    "abstract": "Large Language Models (LLMs) such as GPT developed by OpenAI, have already\nshown astonishing results, introducing quick changes in our society. This has\nbeen intensified by the release of ChatGPT which allows anyone to interact in a\nsimple conversational way with LLMs, without any experience in the field\nneeded. As a result, ChatGPT has been rapidly applied to many different tasks\nsuch as code- and song-writer, education, virtual assistants, etc., showing\nimpressive results for tasks for which it was not trained (zero-shot learning).\n  The present study aims to explore the ability of ChatGPT, based on the recent\nGPT-4 multimodal LLM, for the task of face biometrics. In particular, we\nanalyze the ability of ChatGPT to perform tasks such as face verification,\nsoft-biometrics estimation, and explainability of the results. ChatGPT could be\nvery valuable to further increase the explainability and transparency of\nautomatic decisions in human scenarios. Experiments are carried out in order to\nevaluate the performance and robustness of ChatGPT, using popular public\nbenchmarks and comparing the results with state-of-the-art methods in the\nfield. The results achieved in this study show the potential of LLMs such as\nChatGPT for face biometrics, especially to enhance explainability. For\nreproducibility reasons, we release all the code in GitHub.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13641v2",
    "published_date": "2024-01-24 18:10:39 UTC",
    "updated_date": "2024-02-27 11:00:35 UTC"
  },
  {
    "arxiv_id": "2402.01702v1",
    "title": "Fluent dreaming for language models",
    "authors": [
      "T. Ben Thompson",
      "Zygimantas Straznickas",
      "Michael Sklar"
    ],
    "abstract": "Feature visualization, also known as \"dreaming\", offers insights into vision\nmodels by optimizing the inputs to maximize a neuron's activation or other\ninternal component. However, dreaming has not been successfully applied to\nlanguage models because the input space is discrete. We extend Greedy\nCoordinate Gradient, a method from the language model adversarial attack\nliterature, to design the Evolutionary Prompt Optimization (EPO) algorithm. EPO\noptimizes the input prompt to simultaneously maximize the Pareto frontier\nbetween a chosen internal feature and prompt fluency, enabling fluent dreaming\nfor language models. We demonstrate dreaming with neurons, output logits and\narbitrary directions in activation space. We measure the fluency of the\nresulting prompts and compare language model dreaming with max-activating\ndataset examples. Critically, fluent dreaming allows automatically exploring\nthe behavior of model internals in reaction to mildly out-of-distribution\nprompts. Code for running EPO is available at\nhttps://github.com/Confirm-Solutions/dreamy. A companion page demonstrating\ncode usage is at https://confirmlabs.org/posts/dreamy.html",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 6 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.01702v1",
    "published_date": "2024-01-24 17:57:12 UTC",
    "updated_date": "2024-01-24 17:57:12 UTC"
  },
  {
    "arxiv_id": "2401.13613v1",
    "title": "Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode",
    "authors": [
      "Naresh Kumar Lahajal",
      "Harini S"
    ],
    "abstract": "Photo search, the task of retrieving images based on textual queries, has\nwitnessed significant advancements with the introduction of CLIP (Contrastive\nLanguage-Image Pretraining) model. CLIP leverages a vision-language pre\ntraining approach, wherein it learns a shared representation space for images\nand text, enabling cross-modal understanding. This model demonstrates the\ncapability to understand the semantic relationships between diverse image and\ntext pairs, allowing for efficient and accurate retrieval of images based on\nnatural language queries. By training on a large-scale dataset containing\nimages and their associated textual descriptions, CLIP achieves remarkable\ngeneralization, providing a powerful tool for tasks such as zero-shot learning\nand few-shot classification. This abstract summarizes the foundational\nprinciples of CLIP and highlights its potential impact on advancing the field\nof photo search, fostering a seamless integration of natural language\nunderstanding and computer vision for improved information retrieval in\nmultimedia applications",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13613v1",
    "published_date": "2024-01-24 17:35:38 UTC",
    "updated_date": "2024-01-24 17:35:38 UTC"
  },
  {
    "arxiv_id": "2402.16871v1",
    "title": "Bike3S: A Tool for Bike Sharing Systems Simulation",
    "authors": [
      "Alberto Fern√°ndez",
      "Holger Billhardt",
      "Sascha Ossowski",
      "√ìscar S√°nchez"
    ],
    "abstract": "Vehicle sharing systems are becoming increasingly popular. The effectiveness\nof such systems depends, among other factors, on different strategic and\noperational management decisions and policies, like the dimension of the fleet\nor the distribution of vehicles. It is of foremost importance to be able to\nanticipate and evaluate the potential effects of such strategies before they\ncan be successfully deployed. In this paper we present Bike3S, a simulator for\na station-based bike sharing system. The simulator performs semi-realistic\nsimulations of the operation of a bike sharing system and allows for evaluating\nand testing different management decisions and strategies. In particular, the\nsimulator has been designed to test different station capacities, station\ndistributions, and balancing strategies. The simulator carries out microscopic\nagent-based simulations, where users of different types can be defined that act\naccording to their individual goals and objectives which influences the overall\ndynamics of the whole system.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16871v1",
    "published_date": "2024-01-24 17:33:40 UTC",
    "updated_date": "2024-01-24 17:33:40 UTC"
  },
  {
    "arxiv_id": "2401.13611v1",
    "title": "Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models",
    "authors": [
      "Rhiannon Mogridge",
      "George Close",
      "Robert Sutherland",
      "Thomas Hain",
      "Jon Barker",
      "Stefan Goetze",
      "Anton Ragni"
    ],
    "abstract": "Neural networks have been successfully used for non-intrusive speech\nintelligibility prediction. Recently, the use of feature representations\nsourced from intermediate layers of pre-trained self-supervised and\nweakly-supervised models has been found to be particularly useful for this\ntask. This work combines the use of Whisper ASR decoder layer representations\nas neural network input features with an exemplar-based, psychologically\nmotivated model of human memory to predict human intelligibility ratings for\nhearing-aid users. Substantial performance improvement over an established\nintrusive HASPI baseline system is found, including on enhancement systems and\nlisteners unseen in the training data, with a root mean squared error of 25.3\ncompared with the baseline of 28.7.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted paper. IEEE International Conference on Acoustics Speech and\n  Signal Processing (ICASSP), Seoul, Korea, April 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.13611v1",
    "published_date": "2024-01-24 17:31:07 UTC",
    "updated_date": "2024-01-24 17:31:07 UTC"
  },
  {
    "arxiv_id": "2401.13604v1",
    "title": "Stream-based perception for cognitive agents in mobile ecosystems",
    "authors": [
      "Jeremias D√∂tterl",
      "Ralf Bruns",
      "J√ºrgen Dunkel",
      "Sascha Ossowski"
    ],
    "abstract": "Cognitive agent abstractions can help to engineer intelligent systems across\nmobile devices. On smartphones, the data obtained from onboard sensors can give\nvaluable insights into the user's current situation. Unfortunately, today's\ncognitive agent frameworks cannot cope well with the challenging\ncharacteristics of sensor data. Sensor data is located on a low abstraction\nlevel and the individual data elements are not meaningful when observed in\nisolation. In contrast, cognitive agents operate on high-level percepts and\nlack the means to effectively detect complex spatio-temporal patterns in\nsequences of multiple percepts. In this paper, we present a stream-based\nperception approach that enables the agents to perceive meaningful situations\nin low-level sensor data streams. We present a crowdshipping case study where\nautonomous, self-interested agents collaborate to deliver parcels to their\ndestinations. We show how situations derived from smartphone sensor data can\ntrigger and guide auctions, which the agents use to reach agreements.\nExperiments with real smartphone data demonstrate the benefits of stream-based\nagent perception.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13604v1",
    "published_date": "2024-01-24 17:14:50 UTC",
    "updated_date": "2024-01-24 17:14:50 UTC"
  },
  {
    "arxiv_id": "2401.13594v1",
    "title": "Graph Guided Question Answer Generation for Procedural Question-Answering",
    "authors": [
      "Hai X. Pham",
      "Isma Hadji",
      "Xinnuo Xu",
      "Ziedune Degutyte",
      "Jay Rainey",
      "Evangelos Kazakos",
      "Afsaneh Fazly",
      "Georgios Tzimiropoulos",
      "Brais Martinez"
    ],
    "abstract": "In this paper, we focus on task-specific question answering (QA). To this\nend, we introduce a method for generating exhaustive and high-quality training\ndata, which allows us to train compact (e.g., run on a mobile device),\ntask-specific QA models that are competitive against GPT variants. The key\ntechnological enabler is a novel mechanism for automatic question-answer\ngeneration from procedural text which can ingest large amounts of textual\ninstructions and produce exhaustive in-domain QA training data. While current\nQA data generation methods can produce well-formed and varied data, their\nnon-exhaustive nature is sub-optimal for training a QA model. In contrast, we\nleverage the highly structured aspect of procedural text and represent each\nstep and the overall flow of the procedure as graphs. We then condition on\ngraph nodes to automatically generate QA pairs in an exhaustive and\ncontrollable manner. Comprehensive evaluations of our method show that: 1)\nsmall models trained with our data achieve excellent performance on the target\nQA task, even exceeding that of GPT3 and ChatGPT despite being several orders\nof magnitude smaller. 2) semantic coverage is the key indicator for downstream\nQA performance. Crucially, while large language models excel at syntactic\ndiversity, this does not necessarily result in improvements on the end QA\nmodel. In contrast, the higher semantic coverage provided by our method is\ncritical for QA performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EACL 2024 as long paper. 25 pages including appendix",
    "pdf_url": "http://arxiv.org/pdf/2401.13594v1",
    "published_date": "2024-01-24 17:01:42 UTC",
    "updated_date": "2024-01-24 17:01:42 UTC"
  },
  {
    "arxiv_id": "2401.13588v1",
    "title": "Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes",
    "authors": [
      "Darren Liu",
      "Cheng Ding",
      "Delgersuren Bold",
      "Monique Bouvier",
      "Jiaying Lu",
      "Benjamin Shickel",
      "Craig S. Jabaley",
      "Wenhui Zhang",
      "Soojin Park",
      "Michael J. Young",
      "Mark S. Wainwright",
      "Gilles Clermont",
      "Parisa Rashidi",
      "Eric S. Rosenthal",
      "Laurie Dimisko",
      "Ran Xiao",
      "Joo Heung Yoon",
      "Carl Yang",
      "Xiao Hu"
    ],
    "abstract": "The field of healthcare has increasingly turned its focus towards Large\nLanguage Models (LLMs) due to their remarkable performance. However, their\nperformance in actual clinical applications has been underexplored. Traditional\nevaluations based on question-answering tasks don't fully capture the nuanced\ncontexts. This gap highlights the need for more in-depth and practical\nassessments of LLMs in real-world healthcare settings. Objective: We sought to\nevaluate the performance of LLMs in the complex clinical context of adult\ncritical care medicine using systematic and comprehensible analytic methods,\nincluding clinician annotation and adjudication. Methods: We investigated the\nperformance of three general LLMs in understanding and processing real-world\nclinical notes. Concepts from 150 clinical notes were identified by MetaMap and\nthen labeled by 9 clinicians. Each LLM's proficiency was evaluated by\nidentifying the temporality and negation of these concepts using different\nprompts for an in-depth analysis. Results: GPT-4 showed overall superior\nperformance compared to other LLMs. In contrast, both GPT-3.5 and\ntext-davinci-003 exhibit enhanced performance when the appropriate prompting\nstrategies are employed. The GPT family models have demonstrated considerable\nefficiency, evidenced by their cost-effectiveness and time-saving capabilities.\nConclusion: A comprehensive qualitative performance evaluation framework for\nLLMs is developed and operationalized. This framework goes beyond singular\nperformance aspects. With expert annotations, this methodology not only\nvalidates LLMs' capabilities in processing complex medical data but also\nestablishes a benchmark for future LLM evaluations across specialized domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13588v1",
    "published_date": "2024-01-24 16:52:37 UTC",
    "updated_date": "2024-01-24 16:52:37 UTC"
  },
  {
    "arxiv_id": "2401.13586v4",
    "title": "Instruction Fine-Tuning: Does Prompt Loss Matter?",
    "authors": [
      "Mathew Huerta-Enochian",
      "Seung Yong Ko"
    ],
    "abstract": "We present a novel study analyzing the effects of various prompt loss token\nweights (PLW) for supervised instruction fine-tuning (SIFT). While\nprompt-masking (PLW = 0) is common for SIFT, some fine-tuning APIs support\nfractional PLWs and suggest that using a small non-zero PLW can help stabilize\nlearning when fine-tuning on short-completion data. However, there has never\nbeen a study confirming this claim, and OpenAI, a major cloud-based SIFT\nprovider, recently removed this parameter from their fine-tuning API. We found\nthat performance of models fine-tuned on short-completion data had a\nstatistically-significant negative quadratic relationship with PLW. Using small\nvalues (0.01 - 0.5) of PLW produced better results on multiple-choice and\nshort-generation benchmarks (outperforming models fine-tuned on long-completion\ndata) while large values (~ 1.0) of PLW produced better results on\nlong-generation benchmarks. We explained this effect and verified its\nimportance through additional experiments. This research serves as a warning to\nAPI providers about the importance of providing a PLW parameter for SIFT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2024: Camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2401.13586v4",
    "published_date": "2024-01-24 16:51:23 UTC",
    "updated_date": "2024-10-14 01:16:38 UTC"
  },
  {
    "arxiv_id": "2401.13555v3",
    "title": "Benchmarking the Fairness of Image Upsampling Methods",
    "authors": [
      "Mike Laszkiewicz",
      "Imant Daunhawer",
      "Julia E. Vogt",
      "Asja Fischer",
      "Johannes Lederer"
    ],
    "abstract": "Recent years have witnessed a rapid development of deep generative models for\ncreating synthetic media, such as images and videos. While the practical\napplications of these models in everyday tasks are enticing, it is crucial to\nassess the inherent risks regarding their fairness. In this work, we introduce\na comprehensive framework for benchmarking the performance and fairness of\nconditional generative models. We develop a set of\nmetrics$\\unicode{x2013}$inspired by their supervised fairness\ncounterparts$\\unicode{x2013}$to evaluate the models on their fairness and\ndiversity. Focusing on the specific application of image upsampling, we create\na benchmark covering a wide variety of modern upsampling methods. As part of\nthe benchmark, we introduce UnfairFace, a subset of FairFace that replicates\nthe racial distribution of common large-scale face datasets. Our empirical\nstudy highlights the importance of using an unbiased training set and reveals\nvariations in how the algorithms respond to dataset imbalances. Alarmingly, we\nfind that none of the considered methods produces statistically fair and\ndiverse results. All experiments can be reproduced using our provided\nrepository.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive Version of Record was\n  published at the 2024 ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT '24)",
    "pdf_url": "http://arxiv.org/pdf/2401.13555v3",
    "published_date": "2024-01-24 16:13:26 UTC",
    "updated_date": "2024-04-29 12:39:23 UTC"
  },
  {
    "arxiv_id": "2402.09430v2",
    "title": "WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing",
    "authors": [
      "Shuokang Huang",
      "Kaihan Li",
      "Di You",
      "Yichong Chen",
      "Arvin Lin",
      "Siying Liu",
      "Xiaohui Li",
      "Julie A. McCann"
    ],
    "abstract": "WiFi-based human sensing has exhibited remarkable potential to analyze user\nbehaviors in a non-intrusive and device-free manner, benefiting applications as\ndiverse as smart homes and healthcare. However, most previous works focus on\nsingle-user sensing, which has limited practicability in scenarios involving\nmultiple users. Although recent studies have begun to investigate WiFi-based\nmulti-user sensing, there remains a lack of benchmark datasets to facilitate\nreproducible and comparable research. To bridge this gap, we present WiMANS, to\nour knowledge, the first dataset for multi-user sensing based on WiFi. WiMANS\ncontains over 9.4 hours of dual-band WiFi Channel State Information (CSI), as\nwell as synchronized videos, monitoring simultaneous activities of multiple\nusers. We exploit WiMANS to benchmark the performance of state-of-the-art\nWiFi-based human sensing models and video-based models, posing new challenges\nand opportunities for future work. We believe WiMANS can push the boundaries of\ncurrent studies and catalyze the research on WiFi-based multi-user sensing.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "eess.SP",
    "comment": "We present WiMANS, to our knowledge, the first dataset for multi-user\n  activity sensing based on WiFi",
    "pdf_url": "http://arxiv.org/pdf/2402.09430v2",
    "published_date": "2024-01-24 16:10:14 UTC",
    "updated_date": "2024-03-12 11:48:02 UTC"
  },
  {
    "arxiv_id": "2402.00048v1",
    "title": "IICONGRAPH: improved Iconographic and Iconological Statements in Knowledge Graphs",
    "authors": [
      "Bruno Sartini"
    ],
    "abstract": "Iconography and iconology are fundamental domains when it comes to\nunderstanding artifacts of cultural heritage. Iconography deals with the study\nand interpretation of visual elements depicted in artifacts and their\nsymbolism, while iconology delves deeper, exploring the underlying cultural and\nhistorical meanings. Despite the advances in representing cultural heritage\nwith Linked Open Data (LOD), recent studies show persistent gaps in the\nrepresentation of iconographic and iconological statements in current knowledge\ngraphs (KGs). To address them, this paper presents IICONGRAPH, a KG that was\ncreated by refining and extending the iconographic and iconological statements\nof ArCo (the Italian KG of cultural heritage) and Wikidata. The development of\nIICONGRAPH was also driven by a series of requirements emerging from research\ncase studies that were unattainable in the non-reengineered versions of the\nKGs. The evaluation results demonstrate that IICONGRAPH not only outperforms\nArCo and Wikidata through domain-specific assessments from the literature but\nalso serves as a robust platform for addressing the formulated research\nquestions. IICONGRAPH is released and documented in accordance with the FAIR\nprinciples to guarantee the resource's reusability. The algorithms used to\ncreate it and assess the research questions have also been made available to\nensure transparency and reproducibility. While future work focuses on ingesting\nmore data into the KG, and on implementing it as a backbone of LLM-based\nquestion answering systems, the current version of IICONGRAPH still emerges as\na valuable asset, contributing to the evolving landscape of cultural heritage\nrepresentation within Knowledge Graphs, the Semantic Web, and beyond.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.00048v1",
    "published_date": "2024-01-24 15:44:16 UTC",
    "updated_date": "2024-01-24 15:44:16 UTC"
  },
  {
    "arxiv_id": "2401.13722v1",
    "title": "Proactive Emotion Tracker: AI-Driven Continuous Mood and Emotion Monitoring",
    "authors": [
      "Mohammad Asif",
      "Sudhakar Mishra",
      "Ankush Sonker",
      "Sanidhya Gupta",
      "Somesh Kumar Maurya",
      "Uma Shanker Tiwary"
    ],
    "abstract": "This research project aims to tackle the growing mental health challenges in\ntoday's digital age. It employs a modified pre-trained BERT model to detect\ndepressive text within social media and users' web browsing data, achieving an\nimpressive 93% test accuracy. Simultaneously, the project aims to incorporate\nphysiological signals from wearable devices, such as smartwatches and EEG\nsensors, to provide long-term tracking and prognosis of mood disorders and\nemotional states. This comprehensive approach holds promise for enhancing early\ndetection of depression and advancing overall mental health outcomes.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13722v1",
    "published_date": "2024-01-24 15:05:11 UTC",
    "updated_date": "2024-01-24 15:05:11 UTC"
  },
  {
    "arxiv_id": "2401.13498v1",
    "title": "Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting",
    "authors": [
      "Hounsu Kim",
      "Soonbeom Choi",
      "Juhan Nam"
    ],
    "abstract": "Synthesizing performing guitar sound is a highly challenging task due to the\npolyphony and high variability in expression. Recently, deep generative models\nhave shown promising results in synthesizing expressive polyphonic instrument\nsounds from music scores, often using a generic MIDI input. In this work, we\npropose an expressive acoustic guitar sound synthesis model with a customized\ninput representation to the instrument, which we call guitarroll. We implement\nthe proposed approach using diffusion-based outpainting which can generate\naudio with long-term consistency. To overcome the lack of MIDI/audio-paired\ndatasets, we used not only an existing guitar dataset but also collected data\nfrom a high quality sample-based guitar synthesizer. Through quantitative and\nqualitative evaluations, we show that our proposed model has higher audio\nquality than the baseline model and generates more realistic timbre sounds than\nthe previous leading work.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.13498v1",
    "published_date": "2024-01-24 14:44:01 UTC",
    "updated_date": "2024-01-24 14:44:01 UTC"
  },
  {
    "arxiv_id": "2401.13486v1",
    "title": "Separable Physics-Informed Neural Networks for the solution of elasticity problems",
    "authors": [
      "Vasiliy A. Es'kin",
      "Danil V. Davydov",
      "Julia V. Gur'eva",
      "Alexey O. Malkhanov",
      "Mikhail E. Smorkalov"
    ],
    "abstract": "A method for solving elasticity problems based on separable physics-informed\nneural networks (SPINN) in conjunction with the deep energy method (DEM) is\npresented. Numerical experiments have been carried out for a number of problems\nshowing that this method has a significantly higher convergence rate and\naccuracy than the vanilla physics-informed neural networks (PINN) and even\nSPINN based on a system of partial differential equations (PDEs). In addition,\nusing the SPINN in the framework of DEM approach it is possible to solve\nproblems of the linear theory of elasticity on complex geometries, which is\nunachievable with the help of PINNs in frames of partial differential\nequations. Considered problems are very close to the industrial problems in\nterms of geometry, loading, and material parameters.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "physics.app-ph",
      "68T07, 65Z05, 65M99",
      "I.2.1; I.2.7; J.2"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13486v1",
    "published_date": "2024-01-24 14:34:59 UTC",
    "updated_date": "2024-01-24 14:34:59 UTC"
  },
  {
    "arxiv_id": "2401.13481v2",
    "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment",
    "authors": [
      "Joshua Ashkinaze",
      "Julia Mendelsohn",
      "Li Qiwei",
      "Ceren Budak",
      "Eric Gilbert"
    ],
    "abstract": "Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- speaks to the interdependent process of cultural creation: creative ideas\nare built upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI and that participants may knowingly adopt AI ideas when the\ntask is difficult. Our findings suggest that introducing AI ideas may increase\ncollective diversity but not individual creativity.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13481v2",
    "published_date": "2024-01-24 14:29:39 UTC",
    "updated_date": "2024-07-04 17:14:24 UTC"
  },
  {
    "arxiv_id": "2401.13462v1",
    "title": "Growing from Exploration: A self-exploring framework for robots based on foundation models",
    "authors": [
      "Shoujie Li",
      "Ran Yu",
      "Tong Wu",
      "JunWen Zhong",
      "Xiao-Ping Zhang",
      "Wenbo Ding"
    ],
    "abstract": "Intelligent robot is the ultimate goal in the robotics field. Existing works\nleverage learning-based or optimization-based methods to accomplish\nhuman-defined tasks. However, the challenge of enabling robots to explore\nvarious environments autonomously remains unresolved. In this work, we propose\na framework named GExp, which enables robots to explore and learn autonomously\nwithout human intervention. To achieve this goal, we devise modules including\nself-exploration, knowledge-base-building, and close-loop feedback based on\nfoundation models. Inspired by the way that infants interact with the world,\nGExp encourages robots to understand and explore the environment with a series\nof self-generated tasks. During the process of exploration, the robot will\nacquire skills from beneficial experiences that are useful in the future. GExp\nprovides robots with the ability to solve complex tasks through\nself-exploration. GExp work is independent of prior interactive knowledge and\nhuman intervention, allowing it to adapt directly to different scenarios,\nunlike previous studies that provided in-context examples as few-shot learning.\nIn addition, we propose a workflow of deploying the real-world robot system\nwith self-learned skills as an embodied assistant.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.13462v1",
    "published_date": "2024-01-24 14:04:08 UTC",
    "updated_date": "2024-01-24 14:04:08 UTC"
  },
  {
    "arxiv_id": "2401.13460v3",
    "title": "Multi-Agent Diagnostics for Robustness via Illuminated Diversity",
    "authors": [
      "Mikayel Samvelyan",
      "Davide Paglieri",
      "Minqi Jiang",
      "Jack Parker-Holder",
      "Tim Rockt√§schel"
    ],
    "abstract": "In the rapidly advancing field of multi-agent systems, ensuring robustness in\nunfamiliar and adversarial settings is crucial. Notwithstanding their\noutstanding performance in familiar environments, these systems often falter in\nnew situations due to overfitting during the training phase. This is especially\npronounced in settings where both cooperative and competitive behaviours are\npresent, encapsulating a dual nature of overfitting and generalisation\nchallenges. To address this issue, we present Multi-Agent Diagnostics for\nRobustness via Illuminated Diversity (MADRID), a novel approach for generating\ndiverse adversarial scenarios that expose strategic vulnerabilities in\npre-trained multi-agent policies. Leveraging the concepts from open-ended\nlearning, MADRID navigates the vast space of adversarial settings, employing a\ntarget policy's regret to gauge the vulnerabilities of these settings. We\nevaluate the effectiveness of MADRID on the 11vs11 version of Google Research\nFootball, one of the most complex environments for multi-agent reinforcement\nlearning. Specifically, we employ MADRID for generating a diverse array of\nadversarial settings for TiZero, the state-of-the-art approach which \"masters\"\nthe game through 45 days of training on a large-scale distributed\ninfrastructure. We expose key shortcomings in TiZero's tactical\ndecision-making, underlining the crucial importance of rigorous evaluation in\nmulti-agent systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13460v3",
    "published_date": "2024-01-24 14:02:09 UTC",
    "updated_date": "2024-11-03 21:13:18 UTC"
  },
  {
    "arxiv_id": "2402.01700v1",
    "title": "Question answering systems for health professionals at the point of care -- a systematic review",
    "authors": [
      "Gregory Kell",
      "Angus Roberts",
      "Serge Umansky",
      "Linglong Qian",
      "Davide Ferrari",
      "Frank Soboczenski",
      "Byron Wallace",
      "Nikhil Patel",
      "Iain J Marshall"
    ],
    "abstract": "Objective: Question answering (QA) systems have the potential to improve the\nquality of clinical care by providing health professionals with the latest and\nmost relevant evidence. However, QA systems have not been widely adopted. This\nsystematic review aims to characterize current medical QA systems, assess their\nsuitability for healthcare, and identify areas of improvement.\n  Materials and methods: We searched PubMed, IEEE Xplore, ACM Digital Library,\nACL Anthology and forward and backward citations on 7th February 2023. We\nincluded peer-reviewed journal and conference papers describing the design and\nevaluation of biomedical QA systems. Two reviewers screened titles, abstracts,\nand full-text articles. We conducted a narrative synthesis and risk of bias\nassessment for each study. We assessed the utility of biomedical QA systems.\n  Results: We included 79 studies and identified themes, including question\nrealism, answer reliability, answer utility, clinical specialism, systems,\nusability, and evaluation methods. Clinicians' questions used to train and\nevaluate QA systems were restricted to certain sources, types and complexity\nlevels. No system communicated confidence levels in the answers or sources.\nMany studies suffered from high risks of bias and applicability concerns. Only\n8 studies completely satisfied any criterion for clinical utility, and only 7\nreported user evaluations. Most systems were built with limited input from\nclinicians.\n  Discussion: While machine learning methods have led to increased accuracy,\nmost studies imperfectly reflected real-world healthcare information needs. Key\nresearch priorities include developing more realistic healthcare QA datasets\nand considering the reliability of answer sources, rather than merely focusing\non accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the Journal of the American Medical Informatics\n  Association (JAMIA)",
    "pdf_url": "http://arxiv.org/pdf/2402.01700v1",
    "published_date": "2024-01-24 13:47:39 UTC",
    "updated_date": "2024-01-24 13:47:39 UTC"
  },
  {
    "arxiv_id": "2401.13444v3",
    "title": "Fine-Grained Stateful Knowledge Exploration: A Novel Paradigm for Integrating Knowledge Graphs with Large Language Models",
    "authors": [
      "Dehao Tao",
      "Congqi Wang",
      "Feng Huang",
      "Junhao Chen",
      "Yongfeng Huang",
      "Minghu Jiang"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive capabilities, yet updating\ntheir knowledge remains a significant challenge, often leading to outdated or\ninaccurate responses. A proposed solution is the integration of external\nknowledge bases, such as knowledge graphs, with LLMs. Most existing methods use\na paradigm that treats the question as the objective, with relevant knowledge\nbeing incrementally retrieved from the knowledge graph. However, this strategy\nfrequently experiences an mismatch in the granularity of knowledge between the\ntarget question and the entities and relations being retrieved. As a result,\nthe information in the question cannot precisely correspond to the retrieved\nknowledge. This may cause redundant exploration or omission of vital knowledge,\nthereby leading to enhanced computational consumption and reduced retrieval\naccuracy. In this paper, we propose a novel paradigm of fine-grained stateful\nknowledge exploration, which addresses the `information granularity mismatch'\nissue. We extract fine-grained information from questions and explore the\nsemantic mapping between this information and the knowledge in graph. By\ndynamically updating the mapping records, we avoid redundant exploration and\nensure no pertinent information is overlooked, thereby reducing computational\noverhead and improving the accuracy of knowledge exploration. The use of\nfine-grained information also eliminates the need for a priori knowledge, a\ncommon requirement in existing methods. Experiments on multiple datasets\nrevealed that our paradigm surpasses current advanced methods in knowledge\nretrieval while significantly reducing the average number of LLM invocations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13444v3",
    "published_date": "2024-01-24 13:36:50 UTC",
    "updated_date": "2025-01-27 09:39:49 UTC"
  },
  {
    "arxiv_id": "2401.13432v2",
    "title": "Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond",
    "authors": [
      "Lang Nie",
      "Chunyu Lin",
      "Kang Liao",
      "Shuaicheng Liu",
      "Yao Zhao"
    ],
    "abstract": "Thin-plate spline (TPS) is a principal warp that allows for representing\nelastic, nonlinear transformation with control point motions. With the increase\nof control points, the warp becomes increasingly flexible but usually\nencounters a bottleneck caused by undesired issues, e.g., content distortion.\nIn this paper, we explore generic applications of TPS in single-image-based\nwarping tasks, such as rotation correction, rectangling, and portrait\ncorrection. To break this bottleneck, we propose the coupled thin-plate spline\nmodel (CoupledTPS), which iteratively couples multiple TPS with limited control\npoints into a more flexible and powerful transformation. Concretely, we first\ndesign an iterative search to predict new control points according to the\ncurrent latent condition. Then, we present the warping flow as a bridge for the\ncoupling of different TPS transformations, effectively eliminating\ninterpolation errors caused by multiple warps. Besides, in light of the\nlaborious annotation cost, we develop a semi-supervised learning scheme to\nimprove warping quality by exploiting unlabeled data. It is formulated through\ndual transformation between the searched control points of unlabeled data and\nits graphic augmentation, yielding an implicit correction consistency\nconstraint. Finally, we collect massive unlabeled data to exhibit the benefit\nof our semi-supervised scheme in rotation correction. Extensive experiments\ndemonstrate the superiority and universality of CoupledTPS over the existing\nstate-of-the-art (SoTA) solutions for rotation correction and beyond. The code\nand data are available at https://github.com/nie-lang/CoupledTPS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to TPAMI2024",
    "pdf_url": "http://arxiv.org/pdf/2401.13432v2",
    "published_date": "2024-01-24 13:03:28 UTC",
    "updated_date": "2024-06-18 10:29:39 UTC"
  },
  {
    "arxiv_id": "2401.13408v2",
    "title": "Causal Perception",
    "authors": [
      "Jose M. Alvarez",
      "Salvatore Ruggieri"
    ],
    "abstract": "Perception occurs when two individuals interpret the same information\ndifferently. Despite being a known phenomenon with implications for bias in\ndecision-making, as individual experience determines interpretation, perception\nremains largely overlooked in machine learning (ML) research. Modern decision\nflows, whether partially or fully automated, involve human experts interacting\nwith ML applications. How might we then, e.g., account for two experts that\ninterpret differently a deferred instance or an explanation from a ML model? To\naccount for perception, we first need to formulate it. In this work, we define\nperception under causal reasoning using structural causal models (SCM). Our\nframework formalizes individual experience as additional causal knowledge that\ncomes with and is used by a human expert (read, decision maker). We present two\nkinds of causal perception, unfaithful and inconsistent, based on the SCM\nproperties of faithfulness and consistency. Further, we motivate the importance\nof perception within fairness problems. We illustrate our framework through a\nseries of decision flow examples involving ML applications and human experts.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2305.09535 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2401.13408v2",
    "published_date": "2024-01-24 12:08:58 UTC",
    "updated_date": "2024-05-22 14:04:15 UTC"
  },
  {
    "arxiv_id": "2402.01698v1",
    "title": "Large language model empowered participatory urban planning",
    "authors": [
      "Zhilun Zhou",
      "Yuming Lin",
      "Yong Li"
    ],
    "abstract": "Participatory urban planning is the mainstream of modern urban planning and\ninvolves the active engagement of different stakeholders. However, the\ntraditional participatory paradigm encounters challenges in time and manpower,\nwhile the generative planning tools fail to provide adjustable and inclusive\nsolutions. This research introduces an innovative urban planning approach\nintegrating Large Language Models (LLMs) within the participatory process. The\nframework, based on the crafted LLM agent, consists of role-play, collaborative\ngeneration, and feedback iteration, solving a community-level land-use task\ncatering to 1000 distinct interests. Empirical experiments in diverse urban\ncommunities exhibit LLM's adaptability and effectiveness across varied planning\nscenarios. The results were evaluated on four metrics, surpassing human experts\nin satisfaction and inclusion, and rivaling state-of-the-art reinforcement\nlearning methods in service and ecology. Further analysis shows the advantage\nof LLM agents in providing adjustable and inclusive solutions with natural\nlanguage reasoning and strong scalability. While implementing the recent\nadvancements in emulating human behavior for planning, this work envisions both\nplanners and citizens benefiting from low-cost, efficient LLM agents, which is\ncrucial for enhancing participation and realizing participatory urban planning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, 7 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.01698v1",
    "published_date": "2024-01-24 10:50:01 UTC",
    "updated_date": "2024-01-24 10:50:01 UTC"
  },
  {
    "arxiv_id": "2401.13346v2",
    "title": "Past, Present, Future: A Comprehensive Exploration of AI Use Cases in the UMBRELLA IoT Testbed",
    "authors": [
      "Peizheng Li",
      "Ioannis Mavromatis",
      "Aftab Khan"
    ],
    "abstract": "UMBRELLA is a large-scale, open-access Internet of Things (IoT) ecosystem\nincorporating over 200 multi-sensor multi-wireless nodes, 20 collaborative\nrobots, and edge-intelligence-enabled devices. This paper provides a guide to\nthe implemented and prospective artificial intelligence (AI) capabilities of\nUMBRELLA in real-world IoT systems. Four existing UMBRELLA applications are\npresented in detail: 1) An automated streetlight monitoring for detecting\nissues and triggering maintenance alerts; 2) A Digital twin of building\nenvironments providing enhanced air quality sensing with reduced cost; 3) A\nlarge-scale Federated Learning framework for reducing communication overhead;\nand 4) An intrusion detection for containerised applications identifying\nmalicious activities. Additionally, the potential of UMBRELLA is outlined for\nfuture smart city and multi-robot crowdsensing applications enhanced by\nsemantic communications and multi-agent planning. Finally, to realise the above\nuse-cases we discuss the need for a tailored MLOps platform to automate\nUMBRELLA model pipelines and establish trust.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "6 pgaes, 4 figures. This work has been accepted by PerCom TrustSense\n  workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.13346v2",
    "published_date": "2024-01-24 10:17:59 UTC",
    "updated_date": "2024-02-01 18:20:34 UTC"
  },
  {
    "arxiv_id": "2401.13335v1",
    "title": "Full Bayesian Significance Testing for Neural Networks",
    "authors": [
      "Zehua Liu",
      "Zimeng Li",
      "Jingyuan Wang",
      "Yue He"
    ],
    "abstract": "Significance testing aims to determine whether a proposition about the\npopulation distribution is the truth or not given observations. However,\ntraditional significance testing often needs to derive the distribution of the\ntesting statistic, failing to deal with complex nonlinear relationships. In\nthis paper, we propose to conduct Full Bayesian Significance Testing for neural\nnetworks, called \\textit{n}FBST, to overcome the limitation in relationship\ncharacterization of traditional approaches. A Bayesian neural network is\nutilized to fit the nonlinear and multi-dimensional relationships with small\nerrors and avoid hard theoretical derivation by computing the evidence value.\nBesides, \\textit{n}FBST can test not only global significance but also local\nand instance-wise significance, which previous testing methods don't focus on.\nMoreover, \\textit{n}FBST is a general framework that can be extended based on\nthe measures selected, such as Grad-\\textit{n}FBST, LRP-\\textit{n}FBST,\nDeepLIFT-\\textit{n}FBST, LIME-\\textit{n}FBST. A range of experiments on both\nsimulated and real data are conducted to show the advantages of our method.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Published as a conference paper at AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.13335v1",
    "published_date": "2024-01-24 09:59:48 UTC",
    "updated_date": "2024-01-24 09:59:48 UTC"
  },
  {
    "arxiv_id": "2401.13334v2",
    "title": "Explainable Bayesian Optimization",
    "authors": [
      "Tanmay Chakraborty",
      "Christian Wirth",
      "Christin Seifert"
    ],
    "abstract": "Manual parameter tuning of cyber-physical systems is a common practice, but\nit is labor-intensive. Bayesian Optimization (BO) offers an automated\nalternative, yet its black-box nature reduces trust and limits human-BO\ncollaborative system tuning. Experts struggle to interpret BO recommendations\ndue to the lack of explanations. This paper addresses the post-hoc BO\nexplainability problem for cyber-physical systems. We introduce TNTRules\n(Tune-No-Tune Rules), a novel algorithm that provides both global and local\nexplanations for BO recommendations. TNTRules generates actionable rules and\nvisual graphs, identifying optimal solution bounds and ranges, as well as\npotential alternative solutions. Unlike existing explainable AI (XAI) methods,\nTNTRules is tailored specifically for BO, by encoding uncertainty via a\nvariance pruning technique and hierarchical agglomerative clustering. A\nmulti-objective optimization approach allows maximizing explanation quality. We\nevaluate TNTRules using established XAI metrics (Correctness, Completeness, and\nCompactness) and compare it against adapted baseline methods. The results\ndemonstrate that TNTRules generates high-fidelity, compact, and complete\nexplanations, significantly outperforming three baselines on 5 multi-objective\ntesting functions and 2 hyperparameter tuning problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13334v2",
    "published_date": "2024-01-24 09:59:22 UTC",
    "updated_date": "2025-04-01 15:10:09 UTC"
  },
  {
    "arxiv_id": "2401.13324v6",
    "title": "Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions",
    "authors": [
      "Timoth√©e Schmude",
      "Laura Koesten",
      "Torsten M√∂ller",
      "Sebastian Tschiatschek"
    ],
    "abstract": "Every AI system that makes decisions about people has a group of stakeholders\nthat are personally affected by these decisions. However, explanations of AI\nsystems rarely address the information needs of this stakeholder group, who\noften are AI novices. This creates a gap between conveyed information and\ninformation that matters to those who are impacted by the system's decisions,\nsuch as domain experts and decision subjects. To address this, we present the\n\"XAI Novice Question Bank,\" an extension of the XAI Question Bank containing a\ncatalog of information needs from AI novices in two use cases: employment\nprediction and health monitoring. The catalog covers the categories of data,\nsystem context, system usage, and system specifications. We gathered\ninformation needs through task-based interviews where participants asked\nquestions about two AI systems to decide on their adoption and received verbal\nexplanations in response. Our analysis showed that participants' confidence\nincreased after receiving explanations but that their understanding faced\nchallenges. These included difficulties in locating information and in\nassessing their own understanding, as well as attempts to outsource\nunderstanding. Additionally, participants' prior perceptions of the systems'\nrisks and benefits influenced their information needs. Participants who\nperceived high risks sought explanations about the intentions behind a system's\ndeployment, while those who perceived low risks rather asked about the system's\noperation. Our work aims to support the inclusion of AI novices in\nexplainability efforts by highlighting their information needs, aims, and\nchallenges. We summarize our findings as five key implications that can inform\nthe design of future explanations for lay stakeholder audiences.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Published version available at the International Journal of\n  Human-Computer Studies, please refer to:\n  https://doi.org/10.1016/j.ijhcs.2024.103380. Main text: 26 pages, 4 figures.\n  Supplementary material is provided",
    "pdf_url": "http://arxiv.org/pdf/2401.13324v6",
    "published_date": "2024-01-24 09:39:39 UTC",
    "updated_date": "2024-10-28 12:45:52 UTC"
  },
  {
    "arxiv_id": "2401.13315v1",
    "title": "Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band Imaging",
    "authors": [
      "Mathias Ramm Haugland",
      "Hemin Ali Qadir",
      "Ilangko Balasingham"
    ],
    "abstract": "To cope with the growing prevalence of colorectal cancer (CRC), screening\nprograms for polyp detection and removal have proven their usefulness.\nColonoscopy is considered the best-performing procedure for CRC screening. To\nease the examination, deep learning based methods for automatic polyp detection\nhave been developed for conventional white-light imaging (WLI). Compared with\nWLI, narrow-band imaging (NBI) can improve polyp classification during\ncolonoscopy but requires special equipment. We propose a CycleGAN-based\nframework to convert images captured with regular WLI to synthetic NBI (SNBI)\nas a pre-processing method for improving object detection on WLI when NBI is\nunavailable. This paper first shows that better results for polyp detection can\nbe achieved on NBI compared to a relatively similar dataset of WLI. Secondly,\nexperimental results demonstrate that our proposed modality translation can\nachieve improved polyp detection on SNBI images generated from WLI compared to\nthe original WLI. This is because our WLI-to-SNBI translation model can enhance\nthe observation of polyp surface patterns in the generated SNBI images.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13315v1",
    "published_date": "2024-01-24 09:14:33 UTC",
    "updated_date": "2024-01-24 09:14:33 UTC"
  },
  {
    "arxiv_id": "2402.01353v1",
    "title": "Efficient compilation of expressive problem space specifications to neural network solvers",
    "authors": [
      "Matthew L. Daggitt",
      "Wen Kokke",
      "Robert Atkey"
    ],
    "abstract": "Recent work has described the presence of the embedding gap in neural network\nverification. On one side of the gap is a high-level specification about the\nnetwork's behaviour, written by a domain expert in terms of the interpretable\nproblem space. On the other side are a logically-equivalent set of\nsatisfiability queries, expressed in the uninterpretable embedding space in a\nform suitable for neural network solvers. In this paper we describe an\nalgorithm for compiling the former to the latter. We explore and overcome\ncomplications that arise from targeting neural network solvers as opposed to\nstandard SMT solvers.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01353v1",
    "published_date": "2024-01-24 09:13:09 UTC",
    "updated_date": "2024-01-24 09:13:09 UTC"
  },
  {
    "arxiv_id": "2401.13311v3",
    "title": "ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models",
    "authors": [
      "Rohan Wadhawan",
      "Hritik Bansal",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ],
    "abstract": "Many real-world tasks require an agent to reason jointly over text and visual\nobjects, (e.g., navigating in public spaces), which we refer to as\ncontext-sensitive text-rich visual reasoning. Specifically, these tasks require\nan understanding of the context in which the text interacts with visual\nelements within an image. However, there is a lack of existing datasets to\nbenchmark the state-of-the-art multimodal models' capability on\ncontext-sensitive text-rich visual reasoning. In this paper, we introduce\nConTextual, a novel dataset featuring human-crafted instructions that require\ncontext-sensitive reasoning for text-rich images. We conduct experiments to\nassess the performance of 14 foundation models (GPT-4V, Gemini-Pro-Vision,\nLLaVA-Next) and establish a human performance baseline. Further, we perform\nhuman evaluations of the model responses and observe a significant performance\ngap of 30.8% between GPT-4V (the current best-performing Large Multimodal\nModel) and human performance. Our fine-grained analysis reveals that GPT-4V\nencounters difficulties interpreting time-related data and infographics.\nHowever, it demonstrates proficiency in comprehending abstract visual contexts\nsuch as memes and quotes. Finally, our qualitative analysis uncovers various\nfactors contributing to poor performance including lack of precise visual\nperception and hallucinations. Our dataset, code, and leaderboard can be found\non the project page https://con-textual.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13311v3",
    "published_date": "2024-01-24 09:07:11 UTC",
    "updated_date": "2024-07-16 03:36:29 UTC"
  },
  {
    "arxiv_id": "2401.13298v1",
    "title": "Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models",
    "authors": [
      "Hongzhan Lin",
      "Ziyang Luo",
      "Wei Gao",
      "Jing Ma",
      "Bo Wang",
      "Ruichao Yang"
    ],
    "abstract": "The age of social media is flooded with Internet memes, necessitating a clear\ngrasp and effective identification of harmful ones. This task presents a\nsignificant challenge due to the implicit meaning embedded in memes, which is\nnot explicitly conveyed through the surface text and image. However, existing\nharmful meme detection methods do not present readable explanations that unveil\nsuch implicit meaning to support their detection decisions. In this paper, we\npropose an explainable approach to detect harmful memes, achieved through\nreasoning over conflicting rationales from both harmless and harmful positions.\nSpecifically, inspired by the powerful capacity of Large Language Models (LLMs)\non text generation and reasoning, we first elicit multimodal debate between\nLLMs to generate the explanations derived from the contradictory arguments.\nThen we propose to fine-tune a small language model as the debate judge for\nharmfulness inference, to facilitate multimodal fusion between the harmfulness\nrationales and the intrinsic multimodal information within memes. In this way,\nour model is empowered to perform dialectical reasoning over intricate and\nimplicit harm-indicative patterns, utilizing multimodal explanations\noriginating from both harmless and harmful arguments. Extensive experiments on\nthree public meme datasets demonstrate that our harmful meme detection approach\nachieves much better performance than state-of-the-art methods and exhibits a\nsuperior capacity for explaining the meme harmfulness of the model predictions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The first work towards explainable harmful meme detection by\n  harnessing advanced LLMs",
    "pdf_url": "http://arxiv.org/pdf/2401.13298v1",
    "published_date": "2024-01-24 08:37:16 UTC",
    "updated_date": "2024-01-24 08:37:16 UTC"
  },
  {
    "arxiv_id": "2401.13716v1",
    "title": "Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare",
    "authors": [
      "Vibeke Binz Vallevik",
      "Aleksandar Babic",
      "Serena Elizabeth Marshall",
      "Severin Elvatun",
      "Helga Br√∏gger",
      "Sharmini Alagaratnam",
      "Bj√∏rn Edwin",
      "Narasimha Raghavan Veeraragavan",
      "Anne Kjersti Befring",
      "Jan Franz Nyg√•rd"
    ],
    "abstract": "Ensuring safe adoption of AI tools in healthcare hinges on access to\nsufficient data for training, testing and validation. In response to privacy\nconcerns and regulatory requirements, using synthetic data has been suggested.\nSynthetic data is created by training a generator on real data to produce a\ndataset with similar statistical properties. Competing metrics with differing\ntaxonomies for quality evaluation have been suggested, resulting in a complex\nlandscape. Optimising quality entails balancing considerations that make the\ndata fit for use, yet relevant dimensions are left out of existing frameworks.\nWe performed a comprehensive literature review on the use of quality evaluation\nmetrics on SD within the scope of tabular healthcare data and SD made using\ndeep generative methods. Based on this and the collective team experiences, we\ndeveloped a conceptual framework for quality assurance. The applicability was\nbenchmarked against a practical case from the Dutch National Cancer Registry.\nWe present a conceptual framework for quality assurance of SD for AI\napplications in healthcare that aligns diverging taxonomies, expands on common\nquality dimensions to include the dimensions of Fairness and Carbon footprint,\nand proposes stages necessary to support real-life applications. Building trust\nin synthetic data by increasing transparency and reducing the safety risk will\naccelerate the development and uptake of trustworthy AI tools for the benefit\nof patients. Despite the growing emphasis on algorithmic fairness and carbon\nfootprint, these metrics were scarce in the literature review. The overwhelming\nfocus was on statistical similarity using distance metrics while sequential\nlogic detection was scarce. A consensus-backed framework that includes all\nrelevant quality dimensions can provide assurance for safe and responsible\nreal-life applications of SD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13716v1",
    "published_date": "2024-01-24 08:14:20 UTC",
    "updated_date": "2024-01-24 08:14:20 UTC"
  },
  {
    "arxiv_id": "2401.14426v1",
    "title": "M$^3$TN: Multi-gate Mixture-of-Experts based Multi-valued Treatment Network for Uplift Modeling",
    "authors": [
      "Zexu Sun",
      "Xu Chen"
    ],
    "abstract": "Uplift modeling is a technique used to predict the effect of a treatment\n(e.g., discounts) on an individual's response. Although several methods have\nbeen proposed for multi-valued treatment, they are extended from binary\ntreatment methods. There are still some limitations. Firstly, existing methods\ncalculate uplift based on predicted responses, which may not guarantee a\nconsistent uplift distribution between treatment and control groups. Moreover,\nthis may cause cumulative errors for multi-valued treatment. Secondly, the\nmodel parameters become numerous with many prediction heads, leading to reduced\nefficiency. To address these issues, we propose a novel \\underline{M}ulti-gate\n\\underline{M}ixture-of-Experts based \\underline{M}ulti-valued\n\\underline{T}reatment \\underline{N}etwork (M$^3$TN). M$^3$TN consists of two\ncomponents: 1) a feature representation module with Multi-gate\nMixture-of-Experts to improve the efficiency; 2) a reparameterization module by\nmodeling uplift explicitly to improve the effectiveness. We also conduct\nextensive experiments to demonstrate the effectiveness and efficiency of our\nM$^3$TN.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14426v1",
    "published_date": "2024-01-24 08:10:36 UTC",
    "updated_date": "2024-01-24 08:10:36 UTC"
  },
  {
    "arxiv_id": "2401.14425v1",
    "title": "No Longer Trending on Artstation: Prompt Analysis of Generative AI Art",
    "authors": [
      "Jon McCormack",
      "Maria Teresa Llano",
      "Stephen James Krol",
      "Nina Rajcic"
    ],
    "abstract": "Image generation using generative AI is rapidly becoming a major new source\nof visual media, with billions of AI generated images created using diffusion\nmodels such as Stable Diffusion and Midjourney over the last few years. In this\npaper we collect and analyse over 3 million prompts and the images they\ngenerate. Using natural language processing, topic analysis and visualisation\nmethods we aim to understand collectively how people are using text prompts,\nthe impact of these systems on artists, and more broadly on the visual cultures\nthey promote. Our study shows that prompting focuses largely on surface\naesthetics, reinforcing cultural norms, popular conventional representations\nand imagery. We also find that many users focus on popular topics (such as\nmaking colouring books, fantasy art, or Christmas cards), suggesting that the\ndominant use for the systems analysed is recreational rather than artistic.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "cs.NE",
      "J.5; I.2"
    ],
    "primary_category": "cs.HC",
    "comment": "Paper accepted for EvoMUSART 2024, Aberystwyth, Wales, United\n  Kingdom, 3-5 April 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14425v1",
    "published_date": "2024-01-24 08:03:13 UTC",
    "updated_date": "2024-01-24 08:03:13 UTC"
  },
  {
    "arxiv_id": "2401.14424v3",
    "title": "Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search",
    "authors": [
      "Yanjie Li",
      "Weijun Li",
      "Lina Yu",
      "Min Wu",
      "Jingyi Liu",
      "Wenqiang Li",
      "Meilan Hao",
      "Shu Wei",
      "Yusong Deng"
    ],
    "abstract": "Finding a concise and interpretable mathematical formula that accurately\ndescribes the relationship between each variable and the predicted value in the\ndata is a crucial task in scientific research, as well as a significant\nchallenge in artificial intelligence. This problem is referred to as symbolic\nregression, which is an NP-hard problem. In the previous year, a novel symbolic\nregression methodology utilizing Monte Carlo Tree Search (MCTS) was advanced,\nachieving state-of-the-art results on a diverse range of datasets. although\nthis algorithm has shown considerable improvement in recovering target\nexpressions compared to previous methods, the lack of guidance during the MCTS\nprocess severely hampers its search efficiency. Recently, some algorithms have\nadded a pre-trained policy network to guide the search of MCTS, but the\npre-trained policy network generalizes poorly. To optimize the trade-off\nbetween efficiency and versatility, we introduce SR-GPT, a novel algorithm for\nsymbolic regression that integrates Monte Carlo Tree Search (MCTS) with a\nGenerative Pre-Trained Transformer (GPT). By using GPT to guide the MCTS, the\nsearch efficiency of MCTS is significantly improved. Next, we utilize the MCTS\nresults to further refine the GPT, enhancing its capabilities and providing\nmore accurate guidance for the MCTS. MCTS and GPT are coupled together and\noptimize each other until the target expression is successfully determined. We\nconducted extensive evaluations of SR-GPT using 222 expressions sourced from\nover 10 different symbolic regression datasets. The experimental results\ndemonstrate that SR-GPT outperforms existing state-of-the-art algorithms in\naccurately recovering symbolic expressions both with and without added noise.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.14424v3",
    "published_date": "2024-01-24 07:47:04 UTC",
    "updated_date": "2024-01-30 09:27:21 UTC"
  },
  {
    "arxiv_id": "2401.13282v1",
    "title": "RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing",
    "authors": [
      "Junaid Farooq",
      "Danish Rafiq",
      "Pantelis R. Vlachas",
      "Mohammad Abid Bazaz"
    ],
    "abstract": "Forecasting complex system dynamics, particularly for long-term predictions,\nis persistently hindered by error accumulation and computational burdens. This\nstudy presents RefreshNet, a multiscale framework developed to overcome these\nchallenges, delivering an unprecedented balance between computational\nefficiency and predictive accuracy. RefreshNet incorporates convolutional\nautoencoders to identify a reduced order latent space capturing essential\nfeatures of the dynamics, and strategically employs multiple recurrent neural\nnetwork (RNN) blocks operating at varying temporal resolutions within the\nlatent space, thus allowing the capture of latent dynamics at multiple temporal\nscales. The unique \"refreshing\" mechanism in RefreshNet allows coarser blocks\nto reset inputs of finer blocks, effectively controlling and alleviating error\naccumulation. This design demonstrates superiority over existing techniques\nregarding computational efficiency and predictive accuracy, especially in\nlong-term forecasting. The framework is validated using three benchmark\napplications: the FitzHugh-Nagumo system, the Reaction-Diffusion equation, and\nKuramoto-Sivashinsky dynamics. RefreshNet significantly outperforms\nstate-of-the-art methods in long-term forecasting accuracy and speed, marking a\nsignificant advancement in modeling complex systems and opening new avenues in\nunderstanding and predicting their behavior.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13282v1",
    "published_date": "2024-01-24 07:47:01 UTC",
    "updated_date": "2024-01-24 07:47:01 UTC"
  },
  {
    "arxiv_id": "2401.13275v2",
    "title": "Can AI Assistants Know What They Don't Know?",
    "authors": [
      "Qinyuan Cheng",
      "Tianxiang Sun",
      "Xiangyang Liu",
      "Wenwei Zhang",
      "Zhangyue Yin",
      "Shimin Li",
      "Linyang Li",
      "Zhengfu He",
      "Kai Chen",
      "Xipeng Qiu"
    ],
    "abstract": "Recently, AI assistants based on large language models (LLMs) show surprising\nperformance in many tasks, such as dialogue, solving math problems, writing\ncode, and using tools. Although LLMs possess intensive world knowledge, they\nstill make factual errors when facing some knowledge intensive tasks, like\nopen-domain question answering. These untruthful responses from the AI\nassistant may cause significant risks in practical applications. We believe\nthat an AI assistant's refusal to answer questions it does not know is a\ncrucial method for reducing hallucinations and making the assistant truthful.\nTherefore, in this paper, we ask the question \"Can AI assistants know what they\ndon't know and express them through natural language?\" To answer this question,\nwe construct a model-specific \"I don't know\" (Idk) dataset for an assistant,\nwhich contains its known and unknown questions, based on existing open-domain\nquestion answering datasets. Then we align the assistant with its corresponding\nIdk dataset and observe whether it can refuse to answer its unknown questions\nafter alignment. Experimental results show that after alignment with Idk\ndatasets, the assistant can refuse to answer most its unknown questions. For\nquestions they attempt to answer, the accuracy is significantly higher than\nbefore the alignment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2401.13275v2",
    "published_date": "2024-01-24 07:34:55 UTC",
    "updated_date": "2024-01-28 09:07:13 UTC"
  },
  {
    "arxiv_id": "2401.13270v2",
    "title": "Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics",
    "authors": [
      "Pengcheng Zhao",
      "Yanxiang Chen",
      "Yang Zhao",
      "Zhao Zhang"
    ],
    "abstract": "Automatic image colorization is inherently an ill-posed problem with\nuncertainty, which requires an accurate semantic understanding of scenes to\nestimate reasonable colors for grayscale images. Although recent\ninteraction-based methods have achieved impressive performance, it is still a\nvery difficult task to infer realistic and accurate colors for automatic\ncolorization. To reduce the difficulty of semantic understanding of grayscale\nscenes, this paper tries to utilize corresponding audio, which naturally\ncontains extra semantic information about the same scene. Specifically, a novel\nand pluggable audio-infused automatic image colorization (AIAIC) method is\nproposed, which consists of three stages. First, we take color image semantics\nas a bridge and pretrain a colorization network guided by color image\nsemantics. Second, the natural co-occurrence of audio and video is utilized to\nlearn the color semantic correlations between audio and visual scenes. Third,\nthe implicit audio semantic representation is fed into the pretrained network\nto finally realize the audio-guided colorization. The whole process is trained\nin a self-supervised manner without human annotation. Experiments demonstrate\nthat audio guidance can effectively improve the performance of automatic\ncolorization, especially for some scenes that are difficult to understand only\nfrom visual modality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICONIP-2024",
    "pdf_url": "http://arxiv.org/pdf/2401.13270v2",
    "published_date": "2024-01-24 07:22:05 UTC",
    "updated_date": "2024-12-18 02:43:40 UTC"
  },
  {
    "arxiv_id": "2401.13262v1",
    "title": "Designing Redistribution Mechanisms for Reducing Transaction Fees in Blockchains",
    "authors": [
      "Sankarshan Damle",
      "Manisha Padala",
      "Sujit Gujar"
    ],
    "abstract": "Blockchains deploy Transaction Fee Mechanisms (TFMs) to determine which user\ntransactions to include in blocks and determine their payments (i.e.,\ntransaction fees). Increasing demand and scarce block resources have led to\nhigh user transaction fees. As these blockchains are a public resource, it may\nbe preferable to reduce these transaction fees. To this end, we introduce\nTransaction Fee Redistribution Mechanisms (TFRMs) -- redistributing VCG\npayments collected from such TFM as rebates to minimize transaction fees.\nClassic redistribution mechanisms (RMs) achieve this while ensuring Allocative\nEfficiency (AE) and User Incentive Compatibility (UIC). Our first result shows\nthe non-triviality of applying RM in TFMs. More concretely, we prove that it is\nimpossible to reduce transaction fees when (i) transactions that are not\nconfirmed do not receive rebates and (ii) the miner can strategically\nmanipulate the mechanism. Driven by this, we propose \\emph{Robust} TFRM\n(\\textsf{R-TFRM}): a mechanism that compromises on an honest miner's individual\nrationality to guarantee strictly positive rebates to the users. We then\nintroduce \\emph{robust} and \\emph{rational} TFRM (\\textsf{R}$^2$\\textsf{-TFRM})\nthat uses trusted on-chain randomness that additionally guarantees miner's\nindividual rationality (in expectation) and strictly positive rebates. Our\nresults show that TFRMs provide a promising new direction for reducing\ntransaction fees in public blockchains.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.GT",
    "comment": "Full Paper (AAMAS '24)",
    "pdf_url": "http://arxiv.org/pdf/2401.13262v1",
    "published_date": "2024-01-24 07:09:32 UTC",
    "updated_date": "2024-01-24 07:09:32 UTC"
  },
  {
    "arxiv_id": "2401.13256v3",
    "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
    "authors": [
      "Hongru Wang",
      "Wenyu Huang",
      "Yang Deng",
      "Rui Wang",
      "Zezhong Wang",
      "Yufei Wang",
      "Fei Mi",
      "Jeff Z. Pan",
      "Kam-Fai Wong"
    ],
    "abstract": "Large Language Models (LLMs) has shown exceptional capabilities in many\nnatual language understanding and generation tasks. However, the\npersonalization issue still remains a much-coveted property, especially when it\ncomes to the multiple sources involved in the dialogue system. To better plan\nand incorporate the use of multiple sources in generating personalized\nresponse, we firstly decompose it into three sub-tasks: Knowledge Source\nSelection, Knowledge Retrieval, and Response Generation. We then propose a\nnovel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)\nSpecifically, we unify these three sub-tasks with different formulations into\nthe same sequence-to-sequence paradigm during the training, to adaptively\nretrieve evidences and evaluate the relevance on-demand using special tokens,\ncalled acting tokens and evaluation tokens. Enabling language models to\ngenerate acting tokens facilitates interaction with various knowledge sources,\nallowing them to adapt their behavior to diverse task requirements. Meanwhile,\nevaluation tokens gauge the relevance score between the dialogue context and\nthe retrieved evidence. In addition, we carefully design a self-refinement\nmechanism to iteratively refine the generated response considering 1) the\nconsistency scores between the generated response and retrieved evidence; and\n2) the relevance scores. Experiments on two personalized datasets (DuLeMon and\nKBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge\nsource selection and response generation task with itself as a retriever in a\nunified manner. Extensive analyses and discussions are provided for shedding\nsome new perspectives for personalized dialogue systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13256v3",
    "published_date": "2024-01-24 06:50:20 UTC",
    "updated_date": "2024-11-26 12:37:39 UTC"
  },
  {
    "arxiv_id": "2402.08429v1",
    "title": "Is 3-(F)WL Enough to Distinguish All 3D Graphs?",
    "authors": [
      "Wanghan Xu"
    ],
    "abstract": "The problem of graph isomorphism is an important but challenging problem in\nthe field of graph analysis, for example: analyzing the similarity of two\nchemical molecules, or studying the expressive ability of graph neural\nnetworks. WL test is a method to judge whether two graphs are isomorphic, but\nit cannot distinguish all non-isomorphic graphs. As an improvement of WL, k-WL\nhas stronger isomorphism discrimination ability, and as k increases, its\ndiscrimination ability is strictly increasing. However, whether the isomorphic\ndiscrimination power of k-WL is strictly increasing for more complex 3D graphs,\nor whether there exists k that can discriminate all 3D graphs, remains\nunexplored. This paper attempts to explore this problem from the perspective of\ngraph generation.",
    "categories": [
      "cs.OH",
      "cs.AI"
    ],
    "primary_category": "cs.OH",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08429v1",
    "published_date": "2024-01-24 06:22:08 UTC",
    "updated_date": "2024-01-24 06:22:08 UTC"
  },
  {
    "arxiv_id": "2402.09427v2",
    "title": "DoorINet: Door Heading Prediction through Inertial Deep Learning",
    "authors": [
      "Aleksei Zakharchenko",
      "Sharon Farber",
      "Itzik Klein"
    ],
    "abstract": "Inertial sensors are widely used in a variety of applications. A common task\nis orientation estimation. To tackle such a task, attitude and heading\nreference system algorithms are applied. Relying on the gyroscope readings, the\naccelerometer measurements are used to update the attitude angles, and\nmagnetometer measurements are utilized to update the heading angle. In indoor\nenvironments, magnetometers suffer from interference that degrades their\nperformance resulting in poor heading angle estimation. Therefore, applications\nthat estimate the heading angle of moving objects, such as walking pedestrians,\nclosets, and refrigerators, are prone to error. To circumvent such situations,\nwe propose DoorINet, an end-to-end deep-learning framework to calculate the\nheading angle from door-mounted, low-cost inertial sensors without using\nmagnetometers. To evaluate our approach, we record a unique dataset containing\n391 minutes of accelerometer and gyroscope measurements and corresponding\nground-truth heading angle. We show that our proposed approach outperforms\ncommonly used, model based approaches and data-driven methods.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "10 pages, 14 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.09427v2",
    "published_date": "2024-01-24 05:28:29 UTC",
    "updated_date": "2024-12-01 08:33:42 UTC"
  },
  {
    "arxiv_id": "2401.13229v1",
    "title": "From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning",
    "authors": [
      "Alexandre Alcoforado",
      "Thomas Palmeira Ferraz",
      "Lucas Hideki Okamura",
      "Israel Campos Fama",
      "Arnold Moya Lavado",
      "B√°rbara Dias Bueno",
      "Bruno Veloso",
      "Anna Helena Reali Costa"
    ],
    "abstract": "A major challenge in Natural Language Processing is obtaining annotated data\nfor supervised learning. An option is the use of crowdsourcing platforms for\ndata annotation. However, crowdsourcing introduces issues related to the\nannotator's experience, consistency, and biases. An alternative is to use\nzero-shot methods, which in turn have limitations compared to their few-shot or\nfully supervised counterparts. Recent advancements driven by large language\nmodels show potential, but struggle to adapt to specialized domains with\nseverely limited data. The most common approaches therefore involve the human\nitself randomly annotating a set of datapoints to build initial datasets. But\nrandomly sampling data to be annotated is often inefficient as it ignores the\ncharacteristics of the data and the specific needs of the model. The situation\nworsens when working with imbalanced datasets, as random sampling tends to\nheavily bias towards the majority classes, leading to excessive annotated data.\nTo address these issues, this paper contributes an automatic and informed data\nselection architecture to build a small dataset for few-shot learning. Our\nproposal minimizes the quantity and maximizes diversity of data selected for\nhuman annotation, while improving model performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at PROPOR 2024 - The 16th International Conference on\n  Computational Processing of Portuguese",
    "pdf_url": "http://arxiv.org/pdf/2401.13229v1",
    "published_date": "2024-01-24 04:57:32 UTC",
    "updated_date": "2024-01-24 04:57:32 UTC"
  },
  {
    "arxiv_id": "2401.13227v3",
    "title": "LPNL: Scalable Link Prediction with Large Language Models",
    "authors": [
      "Baolong Bi",
      "Shenghua Liu",
      "Yiwei Wang",
      "Lingrui Mei",
      "Xueqi Cheng"
    ],
    "abstract": "Exploring the application of large language models (LLMs) to graph learning\nis a emerging endeavor. However, the vast amount of information inherent in\nlarge graphs poses significant challenges to this process. This work focuses on\nthe link prediction task and introduces $\\textbf{LPNL}$ (Link Prediction via\nNatural Language), a framework based on large language models designed for\nscalable link prediction on large-scale heterogeneous graphs. We design novel\nprompts for link prediction that articulate graph details in natural language.\nWe propose a two-stage sampling pipeline to extract crucial information from\nthe graphs, and a divide-and-conquer strategy to control the input tokens\nwithin predefined limits, addressing the challenge of overwhelming information.\nWe fine-tune a T5 model based on our self-supervised learning designed for link\nprediction. Extensive experimental results demonstrate that LPNL outperforms\nmultiple advanced baselines in link prediction tasks on large-scale graphs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13227v3",
    "published_date": "2024-01-24 04:50:16 UTC",
    "updated_date": "2024-02-20 03:53:06 UTC"
  },
  {
    "arxiv_id": "2401.13223v3",
    "title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data",
    "authors": [
      "Fengbin Zhu",
      "Ziyang Liu",
      "Fuli Feng",
      "Chao Wang",
      "Moxin Li",
      "Tat-Seng Chua"
    ],
    "abstract": "In this work, we address question answering (QA) over a hybrid of tabular and\ntextual data that are very common content on the Web (e.g. SEC filings), where\ndiscrete reasoning capabilities are often required. Recently, large language\nmodels (LLMs) like GPT-4 have demonstrated strong multi-step reasoning\ncapabilities. We then consider harnessing the amazing power of LLMs to solve\nour task. We abstract a Step-wise Pipeline for tabular and textual QA, which\nconsists of three key steps, including Extractor, Reasoner and Executor, and\ninitially design an instruction to instantiate the pipeline and validate that\nGPT-4 outperforms all existing methods. However, utilizing an online LLM like\nGPT-4 holds various challenges in terms of cost, latency, and data security\nrisk, which motivates us to specialize smaller LLMs in this task. We develop a\nTAT-LLM language model by fine-tuning LLaMA 2 with the training data generated\nautomatically from existing expert-annotated datasets following the Step-wise\nPipeline. The experimental results have verified that our TAT-LLM model can\noutperform all baseline models, including the previous best fine-tuned models\nand very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICAIF 24",
    "pdf_url": "http://arxiv.org/pdf/2401.13223v3",
    "published_date": "2024-01-24 04:28:50 UTC",
    "updated_date": "2024-09-28 01:40:33 UTC"
  },
  {
    "arxiv_id": "2401.13219v1",
    "title": "TEPI: Taxonomy-aware Embedding and Pseudo-Imaging for Scarcely-labeled Zero-shot Genome Classification",
    "authors": [
      "Sathyanarayanan Aakur",
      "Vishalini R. Laguduva",
      "Priyadharsini Ramamurthy",
      "Akhilesh Ramachandran"
    ],
    "abstract": "A species' genetic code or genome encodes valuable evolutionary, biological,\nand phylogenetic information that aids in species recognition, taxonomic\nclassification, and understanding genetic predispositions like drug resistance\nand virulence. However, the vast number of potential species poses significant\nchallenges in developing a general-purpose whole genome classification tool.\nTraditional bioinformatics tools have made notable progress but lack\nscalability and are computationally expensive. Machine learning-based\nframeworks show promise but must address the issue of large classification\nvocabularies with long-tail distributions. In this study, we propose addressing\nthis problem through zero-shot learning using TEPI, Taxonomy-aware Embedding\nand Pseudo-Imaging. We represent each genome as pseudo-images and map them to a\ntaxonomy-aware embedding space for reasoning and classification. This embedding\nspace captures compositional and phylogenetic relationships of species,\nenabling predictions in extensive search spaces. We evaluate TEPI using two\nrigorous zero-shot settings and demonstrate its generalization capabilities\nqualitatively on curated, large-scale, publicly sourced data.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "comment": "Accepted to IEEE JBHI",
    "pdf_url": "http://arxiv.org/pdf/2401.13219v1",
    "published_date": "2024-01-24 04:16:28 UTC",
    "updated_date": "2024-01-24 04:16:28 UTC"
  },
  {
    "arxiv_id": "2401.13214v1",
    "title": "AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical Attention Network",
    "authors": [
      "Xiaolin Ma",
      "Junkai Cheng",
      "Aihua Li",
      "Yuhua Zhang",
      "Zhilong Lin"
    ],
    "abstract": "Recently, methods based on deep learning have been successfully applied to\nship detection for synthetic aperture radar (SAR) images. Despite the\ndevelopment of numerous ship detection methodologies, detecting small and\ncoastal ships remains a significant challenge due to the limited features and\nclutter in coastal environments. For that, a novel adaptive multi-hierarchical\nattention module (AMAM) is proposed to learn multi-scale features and\nadaptively aggregate salient features from various feature layers, even in\ncomplex environments. Specifically, we first fuse information from adjacent\nfeature layers to enhance the detection of smaller targets, thereby achieving\nmulti-scale feature enhancement. Then, to filter out the adverse effects of\ncomplex backgrounds, we dissect the previously fused multi-level features on\nthe channel, individually excavate the salient regions, and adaptively\namalgamate features originating from different channels. Thirdly, we present a\nnovel adaptive multi-hierarchical attention network (AMANet) by embedding the\nAMAM between the backbone network and the feature pyramid network (FPN).\nBesides, the AMAM can be readily inserted between different frameworks to\nimprove object detection. Lastly, extensive experiments on two large-scale SAR\nship detection datasets demonstrate that our AMANet method is superior to\nstate-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T45",
      "I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.13214v1",
    "published_date": "2024-01-24 03:56:33 UTC",
    "updated_date": "2024-01-24 03:56:33 UTC"
  },
  {
    "arxiv_id": "2401.13212v1",
    "title": "AdCorDA: Classifier Refinement via Adversarial Correction and Domain Adaptation",
    "authors": [
      "Lulan Shen",
      "Ali Edalati",
      "Brett Meyer",
      "Warren Gross",
      "James J. Clark"
    ],
    "abstract": "This paper describes a simple yet effective technique for refining a\npretrained classifier network. The proposed AdCorDA method is based on\nmodification of the training set and making use of the duality between network\nweights and layer inputs. We call this input space training. The method\nconsists of two stages - adversarial correction followed by domain adaptation.\nAdversarial correction uses adversarial attacks to correct incorrect\ntraining-set classifications. The incorrectly classified samples of the\ntraining set are removed and replaced with the adversarially corrected samples\nto form a new training set, and then, in the second stage, domain adaptation is\nperformed back to the original training set. Extensive experimental validations\nshow significant accuracy boosts of over 5% on the CIFAR-100 dataset. The\ntechnique can be straightforwardly applied to refinement of weight-quantized\nneural networks, where experiments show substantial enhancement in performance\nover the baseline. The adversarial correction technique also results in\nenhanced robustness to adversarial attacks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13212v1",
    "published_date": "2024-01-24 03:49:51 UTC",
    "updated_date": "2024-01-24 03:49:51 UTC"
  },
  {
    "arxiv_id": "2401.13205v1",
    "title": "Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive Step Size",
    "authors": [
      "Junlin Liu",
      "Xinchen Lyu"
    ],
    "abstract": "Adversarial examples are one critical security threat to various visual\napplications, where injected human-imperceptible perturbations can confuse the\noutput.Generating transferable adversarial examples in the black-box setting is\ncrucial but challenging in practice. Existing input-diversity-based methods\nadopt different image transformations, but may be inefficient due to\ninsufficient input diversity and an identical perturbation step size. Motivated\nby the fact that different image regions have distinctive weights in\nclassification, this paper proposes a black-box adversarial generative\nframework by jointly designing enhanced input diversity and adaptive step\nsizes. We design local mixup to randomly mix a group of transformed adversarial\nimages, strengthening the input diversity. For precise adversarial generation,\nwe project the perturbation into the $tanh$ space to relax the boundary\nconstraint. Moreover, the step sizes of different regions can be dynamically\nadjusted by integrating a second-order momentum.Extensive experiments on\nImageNet validate that our framework can achieve superior transferability\ncompared to state-of-the-art baselines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13205v1",
    "published_date": "2024-01-24 03:26:34 UTC",
    "updated_date": "2024-01-24 03:26:34 UTC"
  },
  {
    "arxiv_id": "2402.01695v3",
    "title": "Language-Guided World Models: A Model-Based Approach to AI Control",
    "authors": [
      "Alex Zhang",
      "Khanh Nguyen",
      "Jens Tuyls",
      "Albert Lin",
      "Karthik Narasimhan"
    ],
    "abstract": "This paper introduces the concept of Language-Guided World Models (LWMs) --\nprobabilistic models that can simulate environments by reading texts. Agents\nequipped with these models provide humans with more extensive and efficient\ncontrol, allowing them to simultaneously alter agent behaviors in multiple\ntasks via natural verbal communication. In this work, we take initial steps in\ndeveloping robust LWMs that can generalize to compositionally novel language\ndescriptions. We design a challenging world modeling benchmark based on the\ngame of MESSENGER (Hanjie et al., 2021), featuring evaluation settings that\nrequire varying degrees of compositional generalization. Our experiments reveal\nthe lack of generalizability of the state-of-the-art Transformer model, as it\noffers marginal improvements in simulation quality over a no-text baseline. We\ndevise a more robust model by fusing the Transformer with the EMMA attention\nmechanism (Hanjie et al., 2021). Our model substantially outperforms the\nTransformer and approaches the performance of a model with an oracle semantic\nparsing and grounding capability. To demonstrate the practicality of this model\nin improving AI safety and transparency, we simulate a scenario in which the\nmodel enables an agent to present plans to a human before execution, and to\nrevise plans based on their language feedback.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "SpLU-RoboNLP workshop at ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.01695v3",
    "published_date": "2024-01-24 03:11:36 UTC",
    "updated_date": "2024-09-04 19:13:06 UTC"
  },
  {
    "arxiv_id": "2401.13201v3",
    "title": "MLLMReID: Multimodal Large Language Model-based Person Re-identification",
    "authors": [
      "Shan Yang",
      "Yongfei Zhang"
    ],
    "abstract": "Multimodal large language models (MLLM) have achieved satisfactory results in\nmany tasks. However, their performance in the task of ReID (ReID) has not been\nexplored to date. This paper will investigate how to adapt them for the task of\nReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and\nthen use their visual encoder as a backbone for ReID. However, there still\nexist two apparent issues: (1) Designing instructions for ReID, MLLMs may\noverfit specific instructions, and designing a variety of instructions will\nlead to higher costs. (2) When fine-tuning the visual encoder of a MLLM, it is\nnot trained synchronously with the ReID task. As a result, the effectiveness of\nthe visual encoder fine-tuning cannot be directly reflected in the performance\nof the ReID task. To address these problems, this paper proposes MLLMReID:\nMultimodal Large Language Model-based ReID. Firstly, we proposed Common\nInstruction, a simple approach that leverages the essence ability of LLMs to\ncontinue writing, avoiding complex and diverse instruction design. Secondly, we\npropose a multi-task learning-based synchronization module to ensure that the\nvisual encoder of the MLLM is trained synchronously with the ReID task. The\nexperimental results demonstrate the superiority of our method.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13201v3",
    "published_date": "2024-01-24 03:07:26 UTC",
    "updated_date": "2024-06-10 10:21:19 UTC"
  },
  {
    "arxiv_id": "2401.13193v1",
    "title": "Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN",
    "authors": [
      "Minsoo Kang",
      "Minkoo Kang",
      "Suhyun Kim"
    ],
    "abstract": "Deep learning has made significant advances in computer vision, particularly\nin image classification tasks. Despite their high accuracy on training data,\ndeep learning models often face challenges related to complexity and\noverfitting. One notable concern is that the model often relies heavily on a\nlimited subset of filters for making predictions. This dependency can result in\ncompromised generalization and an increased vulnerability to minor variations.\nWhile regularization techniques like weight decay, dropout, and data\naugmentation are commonly used to address this issue, they may not directly\ntackle the reliance on specific filters. Our observations reveal that the heavy\nreliance problem gets severe when slow-learning filters are deprived of\nlearning opportunities due to fast-learning filters. Drawing inspiration from\nimage augmentation research that combats over-reliance on specific image\nregions by removing and replacing parts of images, our idea is to mitigate the\nproblem of over-reliance on strong filters by substituting highly activated\nfeatures. To this end, we present a novel method called Catch-up Mix, which\nprovides learning opportunities to a wide range of filters during training,\nfocusing on filters that may lag behind. By mixing activation maps with\nrelatively lower norms, Catch-up Mix promotes the development of more diverse\nrepresentations and reduces reliance on a small subset of filters. Experimental\nresults demonstrate the superiority of our method in various vision\nclassification datasets, providing enhanced robustness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at AAAI2024, Equal contribution of first two authors",
    "pdf_url": "http://arxiv.org/pdf/2401.13193v1",
    "published_date": "2024-01-24 02:42:50 UTC",
    "updated_date": "2024-01-24 02:42:50 UTC"
  },
  {
    "arxiv_id": "2401.13192v3",
    "title": "Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model",
    "authors": [
      "Zhelin Li",
      "Rami Mrad",
      "Runxian Jiao",
      "Guan Huang",
      "Jun Shan",
      "Shibing Chu",
      "Yuanping Chen"
    ],
    "abstract": "Efficiently generating energetically stable crystal structures has long been\na challenge in material design, primarily due to the immense arrangement of\natoms in a crystal lattice. To facilitate the discovery of stable material, we\npresent a framework for the generation of synthesizable materials, leveraging a\npoint cloud representation to encode intricate structural information. At the\nheart of this framework lies the introduction of a diffusion model as its\nfoundational pillar. To gauge the efficacy of our approach, we employ it to\nreconstruct input structures from our training datasets, rigorously validating\nits high reconstruction performance. Furthermore, we demonstrate the profound\npotential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely\nnew materials, emphasizing their synthesizability. Our research stands as a\nnoteworthy contribution to the advancement of materials design and synthesis\nthrough the cutting-edge avenue of generative design instead of the\nconventional substitution or experience-based discovery.",
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "I have submitted to a journal",
    "pdf_url": "http://arxiv.org/pdf/2401.13192v3",
    "published_date": "2024-01-24 02:36:52 UTC",
    "updated_date": "2024-08-30 06:49:51 UTC"
  },
  {
    "arxiv_id": "2401.13178v2",
    "title": "AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents",
    "authors": [
      "Chang Ma",
      "Junlei Zhang",
      "Zhihao Zhu",
      "Cheng Yang",
      "Yujiu Yang",
      "Yaohui Jin",
      "Zhenzhong Lan",
      "Lingpeng Kong",
      "Junxian He"
    ],
    "abstract": "Evaluating Large Language Models (LLMs) as general-purpose agents is\nessential for understanding their capabilities and facilitating their\nintegration into practical applications. However, the evaluation process\npresents substantial challenges. A primary obstacle is the benchmarking of\nagent performance across diverse scenarios within a unified framework,\nespecially in maintaining partially-observable environments and ensuring\nmulti-round interactions. Moreover, current evaluation frameworks mostly focus\non the final success rate, revealing few insights during the process and\nfailing to provide a deep understanding of the model abilities. To address\nthese challenges, we introduce AgentBoard, a pioneering comprehensive benchmark\nand accompanied open-source evaluation framework tailored to analytical\nevaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric\nthat captures incremental advancements as well as a comprehensive evaluation\ntoolkit that features easy assessment of agents for multi-faceted analysis.\nThis not only sheds light on the capabilities and limitations of LLM agents but\nalso propels the interpretability of their performance to the forefront.\nUltimately, AgentBoard serves as a step towards demystifying agent behaviors\nand accelerating the development of stronger LLM agents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2401.13178v2",
    "published_date": "2024-01-24 01:51:00 UTC",
    "updated_date": "2024-12-23 20:12:48 UTC"
  },
  {
    "arxiv_id": "2401.13171v2",
    "title": "Compositional Generative Inverse Design",
    "authors": [
      "Tailin Wu",
      "Takashi Maruyama",
      "Long Wei",
      "Tao Zhang",
      "Yilun Du",
      "Gianluca Iaccarino",
      "Jure Leskovec"
    ],
    "abstract": "Inverse design, where we seek to design input variables in order to optimize\nan underlying objective function, is an important problem that arises across\nfields such as mechanical engineering to aerospace engineering. Inverse design\nis typically formulated as an optimization problem, with recent works\nleveraging optimization across learned dynamics models. However, as models are\noptimized they tend to fall into adversarial modes, preventing effective\nsampling. We illustrate that by instead optimizing over the learned energy\nfunction captured by the diffusion model, we can avoid such adversarial\nexamples and significantly improve design performance. We further illustrate\nhow such a design system is compositional, enabling us to combine multiple\ndifferent diffusion models representing subcomponents of our desired system to\ndesign systems with every specified component. In an N-body interaction task\nand a challenging 2D multi-airfoil design task, we demonstrate that by\ncomposing the learned diffusion model at test time, our method allows us to\ndesign initial states and boundary shapes that are more complex than those in\nthe training data. Our method generalizes to more objects for N-body dataset\nand discovers formation flying to minimize drag in the multi-airfoil design\ntask. Project website and code can be found at\nhttps://github.com/AI4Science-WestlakeU/cindm.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024 spotlight. 30 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.13171v2",
    "published_date": "2024-01-24 01:33:39 UTC",
    "updated_date": "2024-03-11 15:25:57 UTC"
  },
  {
    "arxiv_id": "2401.13713v1",
    "title": "EMP: Effective Multidimensional Persistence for Graph Representation Learning",
    "authors": [
      "Ignacio Segovia-Dominguez",
      "Yuzhou Chen",
      "Cuneyt G. Akcora",
      "Zhiwei Zhen",
      "Murat Kantarcioglu",
      "Yulia R. Gel",
      "Baris Coskunuzer"
    ],
    "abstract": "Topological data analysis (TDA) is gaining prominence across a wide spectrum\nof machine learning tasks that spans from manifold learning to graph\nclassification. A pivotal technique within TDA is persistent homology (PH),\nwhich furnishes an exclusive topological imprint of data by tracing the\nevolution of latent structures as a scale parameter changes. Present PH tools\nare confined to analyzing data through a single filter parameter. However, many\nscenarios necessitate the consideration of multiple relevant parameters to\nattain finer insights into the data. We address this issue by introducing the\nEffective Multidimensional Persistence (EMP) framework. This framework empowers\nthe exploration of data by simultaneously varying multiple scale parameters.\nThe framework integrates descriptor functions into the analysis process,\nyielding a highly expressive data summary. It seamlessly integrates established\nsingle PH summaries into multidimensional counterparts like EMP Landscapes,\nSilhouettes, Images, and Surfaces. These summaries represent data's\nmultidimensional aspects as matrices and arrays, aligning effectively with\ndiverse ML models. We provide theoretical guarantees and stability proofs for\nEMP summaries. We demonstrate EMP's utility in graph classification tasks,\nshowing its effectiveness. Results reveal that EMP enhances various single PH\ndescriptors, outperforming cutting-edge methods on multiple benchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CG"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2401.13157",
    "pdf_url": "http://arxiv.org/pdf/2401.13713v1",
    "published_date": "2024-01-24 00:41:51 UTC",
    "updated_date": "2024-01-24 00:41:51 UTC"
  },
  {
    "arxiv_id": "2401.13157v1",
    "title": "Time-Aware Knowledge Representations of Dynamic Objects with Multidimensional Persistence",
    "authors": [
      "Baris Coskunuzer",
      "Ignacio Segovia-Dominguez",
      "Yuzhou Chen",
      "Yulia R. Gel"
    ],
    "abstract": "Learning time-evolving objects such as multivariate time series and dynamic\nnetworks requires the development of novel knowledge representation mechanisms\nand neural network architectures, which allow for capturing implicit\ntime-dependent information contained in the data. Such information is typically\nnot directly observed but plays a key role in the learning task performance. In\nturn, lack of time dimension in knowledge encoding mechanisms for\ntime-dependent data leads to frequent model updates, poor learning performance,\nand, as a result, subpar decision-making. Here we propose a new approach to a\ntime-aware knowledge representation mechanism that notably focuses on implicit\ntime-dependent topological information along multiple geometric dimensions. In\nparticular, we propose a new approach, named \\textit{Temporal MultiPersistence}\n(TMP), which produces multidimensional topological fingerprints of the data by\nusing the existing single parameter topological summaries. The main idea behind\nTMP is to merge the two newest directions in topological representation\nlearning, that is, multi-persistence which simultaneously describes data shape\nevolution along multiple key parameters, and zigzag persistence to enable us to\nextract the most salient data shape information over time. We derive\ntheoretical guarantees of TMP vectorizations and show its utility, in\napplication to forecasting on benchmark traffic flow, Ethereum blockchain, and\nelectrocardiogram datasets, demonstrating the competitive performance,\nespecially, in scenarios of limited data records. In addition, our TMP method\nimproves the computational efficiency of the state-of-the-art multipersistence\nsummaries up to 59.5 times.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13157v1",
    "published_date": "2024-01-24 00:33:53 UTC",
    "updated_date": "2024-01-24 00:33:53 UTC"
  }
]
{
  "date": "2025-04-19",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-19 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 和机器学习的创新应用，包括 LLM（Large Language Models）的鲁棒性提升、多模态模型的优化，以及视觉生成和代理系统的进展，其中 Hydra 框架（提升视觉语言模型的对抗鲁棒性）和 AIOS 系统（构建 AI 代理网络，由知名学者 Yongfeng Zhang 等参与）等文章令人印象深刻，展示了 AI 在实际部署中的潜力。\n\n接下来，我们挑选并分组聊聊今天的亮点论文，先从 LLM 和 AI 代理等高话题度文章入手，再快速掠过其他领域。限于篇幅，我会聚焦核心贡献，相关论文会简要提及。\n\n### LLM 和 AI 代理相关（高话题度，优先讨论）\n- **Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models（Hydra: 一种代理推理方法，用于提升视觉语言模型的对抗鲁棒性和减少幻觉）**  \n  这篇论文提出 Hydra 框架，通过迭代推理和跨模型验证，增强视觉语言模型（VLMs）的对抗鲁棒性和事实一致性。主要贡献是 Action-Critique 循环机制，利用 Chain-of-Thought 和 In-Context Learning 动态优化输出，在多个基准上超越现有方法，实现训练-free 的可靠性提升。\n\n- **Planet as a Brain: Towards Internet of AgentSites based on AIOS Server（Planet as a Brain: 基于 AIOS 服务器的代理站点互联网）**  \n  作者包括知名学者 Xiang Zhang 和 Yongfeng Zhang，这篇论文引入 AIOS Server 框架，支持去中心化 AI 代理协作。主要发现是通过 Model Context Protocol 和 JSON-RPC 协议，实现代理间的全局交互，并在实际部署中构建了首个代理互联网系统，提升了 AI 生态的可扩展性。\n\n- **SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation（SOTOPIA-S4: 一种用户友好的灵活、可定制的大型社会模拟系统）**  \n  这篇论文开发了 SOTOPIA-S4 系统，支持多轮多方 LLM 代理互动，用于社会科学假设测试。主要贡献是结合 RESTful API 和 Web 接口，实现无编程模拟，并通过案例验证了其在招聘谈判和规划场景中的有效性。\n\n- **InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners（InfiGUI-R1: 从反应式代理到决策式推理的多模态 GUI 代理进展）**  \n  论文提出 Actor2Reasoner 框架，通过两阶段训练（推理注入和强化学习）提升 GUI 代理的决策能力。主要发现是利用 Spatial Reasoning Distillation 和子目标引导，实现更鲁棒的跨模态推理，在 GUI 任务中显著改善性能。\n\n其他 LLM 优化论文，如 **Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models（从 Transformer 到子二次语言模型的知识蒸馏实证评估）**，通过系统评估知识转移，证明了子二次模型的效率提升；以及 **Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites（多样提示: 使用 MAP-Elites 探索大型语言模型的提示空间）**，使用进化算法优化提示多样性，这些工作强化了 LLM 的适应性，但我们快速掠过。\n\n### 视觉和生成模型相关（令人印象深刻，简要讨论）\n- **Adversarial Attack for RGB-Event based Visual Object Tracking（基于 RGB-Event 的视觉物体跟踪对抗攻击）**  \n  这篇论文提出跨模态对抗攻击算法，针对 RGB-Event 跟踪优化扰动。主要贡献是双步策略（针对 Event 体素的梯度引导），在 COESOT 和 FE108 数据集上显著降低跟踪性能，揭示了多模态系统的脆弱性。\n\n- **LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers（LOOPE: 视觉 Transformer 中可学习的最优补丁顺序位置嵌入）**  \n  论文引入 LOOPE 方法，优化 Vision Transformers 的补丁顺序以提升空间表示。主要发现是通过 \"Three Cell Experiment\" 基准，展示了 LOOPE 在保留相对和绝对位置信息方面的30-35%性能提升。\n\n其他视觉生成论文，如 **Visual Prompting for One-shot Controllable Video Editing without Inversion（无需反演的单样本可控视频编辑视觉提示）**，提出基于视觉提示的编辑框架，提升视频一致性；以及 **ProtPainter: Draw or Drag Protein via Topology-guided Diffusion（ProtPainter: 通过拓扑引导扩散绘制或拖拽蛋白质）**，使用扩散模型生成拓扑精确的蛋白结构，这些创新推动了生成AI的应用，但我们仅简要提及。\n\n### 其他领域（快速掠过，不那么核心的文章）\n今天还有许多论文涉及时间序列分析、优化算法和特定应用，如 **Rethinking Traffic Flow Forecasting: From Transition to Generation（重新思考交通流量预测: 从过渡到生成）**，提出 EMBSFormer 框架，提升多周期建模；**Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping（使用基于块的不确定性增强结构表示的学习，用于海洋底部映射）**，整合 VQ-VAE 和不确定性量化；以及 **ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking（ScholarMate: 一种混合主动工具，用于定性知识工作和信息感知）**，这些工作在各自领域有贡献，但篇幅有限，我们就快速掠过不深挖。\n\n总之，今天的 arXiv 论文展示了 AI 领域的多样创新，LLM 和视觉模型的进展尤其值得跟踪。如果你对特定主题感兴趣，建议查看这些论文的源码或基准测试！明天的快报见。",
  "papers": [
    {
      "arxiv_id": "2504.14423v1",
      "title": "Adversarial Attack for RGB-Event based Visual Object Tracking",
      "title_zh": "基于 RGB-Event 的视觉物体跟踪对抗攻击",
      "authors": [
        "Qiang Chen",
        "Xiao Wang",
        "Haowen Wang",
        "Bo Jiang",
        "Lin Zhu",
        "Dawei Zhang",
        "Yonghong Tian",
        "Jin Tang"
      ],
      "abstract": "Visual object tracking is a crucial research topic in the fields of computer\nvision and multi-modal fusion. Among various approaches, robust visual tracking\nthat combines RGB frames with Event streams has attracted increasing attention\nfrom researchers. While striving for high accuracy and efficiency in tracking,\nit is also important to explore how to effectively conduct adversarial attacks\nand defenses on RGB-Event stream tracking algorithms, yet research in this area\nremains relatively scarce. To bridge this gap, in this paper, we propose a\ncross-modal adversarial attack algorithm for RGB-Event visual tracking. Because\nof the diverse representations of Event streams, and given that Event voxels\nand frames are more commonly used, this paper will focus on these two\nrepresentations for an in-depth study. Specifically, for the RGB-Event voxel,\nwe first optimize the perturbation by adversarial loss to generate RGB frame\nadversarial examples. For discrete Event voxel representations, we propose a\ntwo-step attack strategy, more in detail, we first inject Event voxels into the\ntarget region as initialized adversarial examples, then, conduct a\ngradient-guided optimization by perturbing the spatial location of the Event\nvoxels. For the RGB-Event frame based tracking, we optimize the cross-modal\nuniversal perturbation by integrating the gradient information from multimodal\ndata. We evaluate the proposed approach against attacks on three widely used\nRGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive\nexperiments show that our method significantly reduces the performance of the\ntracker across numerous datasets in both unimodal and multimodal scenarios. The\nsource code will be released on\nhttps://github.com/Event-AHU/Adversarial_Attack_Defense",
      "tldr_zh": "本文提出了一种针对 RGB-Event 视觉物体追踪的跨模态对抗攻击算法，以解决该领域攻击和防御研究不足的问题。针对 Event voxels，该方法采用两步策略：先注入扰动到目标区域，然后通过梯度引导优化 Event voxels 的空间位置；针对 Event frames，则整合多模态梯度信息优化通用扰动。在 COESOT、FE108 和 VisEvent 数据集上的实验表明，该算法显著降低了追踪器的性能，在单模态和多模态场景中均表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14423v1",
      "published_date": "2025-04-19 23:35:19 UTC",
      "updated_date": "2025-04-19 23:35:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:40:09.750430"
    },
    {
      "arxiv_id": "2504.14411v3",
      "title": "Planet as a Brain: Towards Internet of AgentSites based on AIOS Server",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Zhang",
        "Yongfeng Zhang"
      ],
      "abstract": "The internet is undergoing a historical transformation from the \"Internet of\nWebsites\" to the \"Internet of AgentSites.\" While traditional Websites served as\nthe foundation for information hosting and dissemination, a new frontier is\nemerging where AgentSites serve as the hubs of the internet, where each\nAgentSite hosts one or more AI agents that receive tasks, address them, and\ndeliver actionable solutions, marking a significant shift in the digital\nlandscape and representing the next generation of online ecosystems. Under this\nvision, AIOS, the AI Agent Operating System, serves as the server for the\ndevelopment, deployment and execution of AI agents, which is a fundamental\ninfrastructure for the Internet of Agentsites.\n  In this paper, we introduce AIOS Server, a runtime framework to host agents\nand enable global-scale collaboration among decentralized agents. AIOS Server\nprovides a communication protocol leveraging the Model Context Protocol (MCP)\nand JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node\noperates as a server to host and execute agents, while supporting peer-to-peer\ncoordination without reliance on centralized orchestration. Based on AIOS\nServer, we further present the world's first practically deployed Internet of\nAgentsites (AIOS-IoA), including AgentHub for agent registration and discovery\nand AgentChat for interactive communication, at https://planet.aios.foundation.\nThe agent discovery mechanism based on Distributed Hash Tables (DHT) and a\nGossip protocol serves as the search engine for the internet of agentsites.\nThis work provides a practical foundation for building the Internet of\nAgentsites-a new paradigm where autonomous agents become first-class citizens\nof the web. The implementation is available at\nhttps://github.com/agiresearch/AIOS.Server and is integrated into the AIOS main\nbranch at https://github.com/agiresearch/AIOS.",
      "tldr_zh": "该论文提出“Internet of AgentSites”的愿景，将互联网从“Internet of Websites”转向以AI代理为核心的生态，并引入AIOS（AI Agent Operating System）作为代理开发、部署和执行的基础服务器。AIOS Server是一个运行时框架，使用Model Context Protocol (MCP)和JSON-RPC协议，支持代理间的去中心化协作和人机交互，实现全球规模的点对点协调。基于AIOS Server，他们部署了首个实际系统AIOS-IoA，包括AgentHub用于代理注册与发现，以及AgentChat用于交互通信，并采用Distributed Hash Tables (DHT)和Gossip protocol作为搜索机制。该工作为构建代理作为网络第一公民的新范式提供了实用基础。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14411v3",
      "published_date": "2025-04-19 21:58:00 UTC",
      "updated_date": "2025-05-09 14:35:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:40:19.207231"
    },
    {
      "arxiv_id": "2504.14409v1",
      "title": "Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training",
      "title_zh": "翻译失败",
      "authors": [
        "Christopher Ick",
        "Gordon Wichern",
        "Yoshiki Masuyama",
        "François G. Germain",
        "Jonathan Le Roux"
      ],
      "abstract": "This report details MERL's system for room impulse response (RIR) estimation\nsubmitted to the Generative Data Augmentation Workshop at ICASSP 2025 for\nAugmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task\n2). We first pre-train a neural acoustic field conditioned by room geometry on\nan external large-scale dataset in which pairs of RIRs and the geometries are\nprovided. The neural acoustic field is then adapted to each target room by\nusing the enrollment data, where we leverage either the provided room\ngeometries or geometries retrieved from the external dataset, depending on\navailability. Lastly, we predict the RIRs for each pair of source and receiver\nlocations specified by Task 1, and use these RIRs to train the speaker distance\nestimation model in Task 2.",
      "tldr_zh": "这篇论文介绍了 MERL 的系统，利用 Neural Acoustic Fields 和 Retrieval-Augmented Pre-training 技术来增强房间脉冲响应 (RIR) 数据，并应用于 ICASSP 2025 工作坊的两个任务。系统首先在外部大型数据集上预训练神经声学场模型，该模型基于房间几何信息进行条件化，然后通过注册数据或从外部数据集检索几何信息来适应目标房间。最终，该系统预测指定源和接收器位置的 RIR，并使用这些数据训练说话者距离估计模型，从而改善音频处理任务的准确性。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Presented at ICASSP 2025 GenDA Workshop",
      "pdf_url": "http://arxiv.org/pdf/2504.14409v1",
      "published_date": "2025-04-19 21:43:56 UTC",
      "updated_date": "2025-04-19 21:43:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:40:31.711799"
    },
    {
      "arxiv_id": "2504.14406v2",
      "title": "ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking",
      "title_zh": "ScholarMate：一种用于定性知识工作和信息意义构建的混合主动性工具",
      "authors": [
        "Runlong Ye",
        "Patrick Yung Kang Lee",
        "Matthew Varona",
        "Oliver Huang",
        "Carolina Nobre"
      ],
      "abstract": "Synthesizing knowledge from large document collections is a critical yet\nincreasingly complex aspect of qualitative research and knowledge work. While\nAI offers automation potential, effectively integrating it into human-centric\nsensemaking workflows remains challenging. We present ScholarMate, an\ninteractive system designed to augment qualitative analysis by unifying AI\nassistance with human oversight. ScholarMate enables researchers to dynamically\narrange and interact with text snippets on a non-linear canvas, leveraging AI\nfor theme suggestions, multi-level summarization, and evidence-based theme\nnaming, while ensuring transparency through traceability to source documents.\nInitial pilot studies indicated that users value this mixed-initiative\napproach, finding the balance between AI suggestions and direct manipulation\ncrucial for maintaining interpretability and trust. We further demonstrate the\nsystem's capability through a case study analyzing 24 papers. By balancing\nautomation with human control, ScholarMate enhances efficiency and supports\ninterpretability, offering a valuable approach for productive human-AI\ncollaboration in demanding sensemaking tasks common in knowledge work.",
      "tldr_zh": "本论文介绍了 ScholarMate，一种混合主动(mixed-initiative)工具，旨在增强定性知识工作和信息感测过程，通过整合 AI 辅助与人类监督来处理大型文档集合。系统允许用户在非线性画布上动态排列和交互文本片段，利用 AI 提供主题建议、多级总结和基于证据的主题命名，同时通过可追溯性确保透明度。初步试点研究和一个分析 24 篇论文的案例研究表明，这种方法提升了效率和可解释性，用户高度认可 AI 与直接操作的平衡，为人类-AI 协作提供了有效途径。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "accepted at CHIWORK '25",
      "pdf_url": "http://arxiv.org/pdf/2504.14406v2",
      "published_date": "2025-04-19 21:11:40 UTC",
      "updated_date": "2025-05-16 17:13:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:40:42.489315"
    },
    {
      "arxiv_id": "2504.16122v1",
      "title": "SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation",
      "title_zh": "翻译失败",
      "authors": [
        "Xuhui Zhou",
        "Zhe Su",
        "Sophie Feng",
        "Jiaxu Zhou",
        "Jen-tse Huang",
        "Hsien-Te Kao",
        "Spencer Lynch",
        "Svitlana Volkova",
        "Tongshuang Sherry Wu",
        "Anita Woolley",
        "Hao Zhu",
        "Maarten Sap"
      ],
      "abstract": "Social simulation through large language model (LLM) agents is a promising\napproach to explore and validate hypotheses related to social science questions\nand LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable\nsocial simulation system that addresses the technical barriers of current\nframeworks while enabling practitioners to generate multi-turn and multi-party\nLLM-based interactions with customizable evaluation metrics for hypothesis\ntesting. SOTOPIA-S4 comes as a pip package that contains a simulation engine,\nan API server with flexible RESTful APIs for simulation management, and a web\ninterface that enables both technical and non-technical users to design, run,\nand analyze simulations without programming. We demonstrate the usefulness of\nSOTOPIA-S4 with two use cases involving dyadic hiring negotiation and\nmulti-party planning scenarios.",
      "tldr_zh": "本研究推出了SOTOPIA-S4，这是一个快速、灵活且可扩展的社会模拟系统，利用大型语言模型（LLM）代理来探索社会科学问题和LLM行为。系统通过模拟引擎、API服务器的灵活RESTful APIs以及用户友好的网页界面，允许用户（包括非技术人员）设计、运行和分析多轮多方交互，并支持可定制的评估指标进行假设测试。与现有框架相比，SOTOPIA-S4克服了技术障碍，并在二元招聘谈判和多方规划场景的用例中展示了其实用性和有效性。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "The first author and the second author contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2504.16122v1",
      "published_date": "2025-04-19 20:02:59 UTC",
      "updated_date": "2025-04-19 20:02:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:40:54.404346"
    },
    {
      "arxiv_id": "2504.14395v1",
      "title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chung-En",
        "Yu",
        "Hsuan-Chih",
        "Chen",
        "Brian Jalaian",
        "Nathaniel D. Bastian"
      ],
      "abstract": "To develop trustworthy Vision-Language Models (VLMs), it is essential to\naddress adversarial robustness and hallucination mitigation, both of which\nimpact factual accuracy in high-stakes applications such as defense and\nhealthcare. Existing methods primarily focus on either adversarial defense or\nhallucination post-hoc correction, leaving a gap in unified robustness\nstrategies. We introduce \\textbf{Hydra}, an adaptive agentic framework that\nenhances plug-in VLMs through iterative reasoning, structured critiques, and\ncross-model verification, improving both resilience to adversarial\nperturbations and intrinsic model errors. Hydra employs an Action-Critique\nLoop, where it retrieves and critiques visual information, leveraging\nChain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine\noutputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to\nboth adversarial manipulations and intrinsic model errors, making it robust to\nmalicious perturbations and hallucination-related inaccuracies. We evaluate\nHydra on four VLMs, three hallucination benchmarks, two adversarial attack\nstrategies, and two adversarial defense methods, assessing performance on both\nclean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs\nand state-of-the-art (SOTA) dehallucination methods, even without explicit\nadversarial defenses, demonstrating enhanced robustness and factual\nconsistency. By bridging adversarial resistance and hallucination mitigation,\nHydra provides a scalable, training-free solution for improving the reliability\nof VLMs in real-world applications.",
      "tldr_zh": "该论文提出 Hydra，一种自适应代理框架，用于提升视觉语言模型(VLMs)的对抗鲁棒性和减少幻觉问题，从而提高模型在高风险应用中的事实准确性。Hydra 通过 Action-Critique Loop 结合 Chain-of-Thought (CoT) 和 In-Context Learning (ICL) 技术，进行迭代推理、结构化批评和跨模型验证，以动态精炼输出并适应对抗扰动和内在错误。与现有方法不同，Hydra 提供一个统一的、无需训练的解决方案。实验结果显示，在多个基准上，Hydra 超越插件 VLMs 和 SOTA 去幻觉方法，显著提升鲁棒性和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14395v1",
      "published_date": "2025-04-19 19:51:20 UTC",
      "updated_date": "2025-04-19 19:51:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:41:06.947262"
    },
    {
      "arxiv_id": "2504.14386v1",
      "title": "LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Md Abtahi Majeed Chowdhury",
        "Md Rifat Ur Rahman",
        "Akil Ahmad Taki"
      ],
      "abstract": "Positional embeddings (PE) play a crucial role in Vision Transformers (ViTs)\nby providing spatial information otherwise lost due to the permutation\ninvariant nature of self attention. While absolute positional embeddings (APE)\nhave shown theoretical advantages over relative positional embeddings (RPE),\nparticularly due to the ability of sinusoidal functions to preserve spatial\ninductive biases like monotonicity and shift invariance, a fundamental\nchallenge arises when mapping a 2D grid to a 1D sequence. Existing methods have\nmostly overlooked or never explored the impact of patch ordering in positional\nembeddings. To address this, we propose LOOPE, a learnable patch-ordering\nmethod that optimizes spatial representation for a given set of frequencies,\nproviding a principled approach to patch order optimization. Empirical results\nshow that our PE significantly improves classification accuracy across various\nViT architectures. To rigorously evaluate the effectiveness of positional\nembeddings, we introduce the \"Three Cell Experiment\", a novel benchmarking\nframework that assesses the ability of PEs to retain relative and absolute\npositional information across different ViT architectures. Unlike standard\nevaluations, which typically report a performance gap of 4 to 6% between models\nwith and without PE, our method reveals a striking 30 to 35% difference,\noffering a more sensitive diagnostic tool to measure the efficacy of PEs. Our\nexperimental analysis confirms that the proposed LOOPE demonstrates enhanced\neffectiveness in retaining both relative and absolute positional information.",
      "tldr_zh": "这篇论文提出了 LOOPE，一种可学习的 patch-ordering 方法，用于优化 Vision Transformers (ViTs) 中的 Positional Embeddings (PE)，以解决将 2D 网格映射到 1D 序列时忽略 patch 顺序的问题，并提供一个原则性的空间表示优化方法。实验结果显示，LOOPE 显著提高了各种 ViT 架构的分类准确率。论文还引入了 \"Three Cell Experiment\" 基准框架，用于更敏感地评估 PE 在保留相对和绝对位置信息方面的有效性，结果显示 LOOPE 比传统方法表现出 30-35% 的性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14386v1",
      "published_date": "2025-04-19 19:20:47 UTC",
      "updated_date": "2025-04-19 19:20:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:41:20.256643"
    },
    {
      "arxiv_id": "2504.14379v2",
      "title": "The Geometry of Self-Verification in a Task-Specific Reasoning Model",
      "title_zh": "任务特定推理模型中自我验证的几何",
      "authors": [
        "Andrew Lee",
        "Lihao Sun",
        "Chris Wendler",
        "Fernanda Viégas",
        "Martin Wattenberg"
      ],
      "abstract": "How do reasoning models verify their own answers? We study this question by\ntraining a model using DeepSeek R1's recipe on the CountDown task. We leverage\nthe fact that preference tuning leads to mode collapse, yielding a model that\nalways produces highly structured chain-of-thought sequences. With this setup,\nwe do top-down and bottom-up analyses to reverse-engineer how the model\nverifies its outputs. Top-down, we find Gated Linear Unit (GLU) weights\nencoding verification-related tokens, such as ``success'' or ``incorrect''.\nBottom-up, we find that ``previous-token heads'' are mainly responsible for\nself-verification in our setup. Our analyses meet in the middle: drawing\ninspiration from inter-layer communication channels, we use the identified GLU\nweights to localize as few as three attention heads that can disable\nself-verification, pointing to a necessary component of a potentially larger\nverification circuit. Finally, we verify that similar verification components\nexist in our base model and a general reasoning DeepSeek-R1 model.",
      "tldr_zh": "这篇论文探讨了任务特定推理模型中自验证的几何结构，通过使用 DeepSeek R1 的训练方法在 CountDown 任务上训练模型，导致模式崩溃并产生高度结构化的 Chain-of-Thought 序列。研究采用自上而下分析发现 Gated Linear Unit (GLU) 权重编码了验证相关标记，如 \"success\" 或 \"incorrect\"，以及自下而上的分析识别出 \"previous-token heads\" 作为自验证的主要组件。最终，通过整合这些分析，论文定位了关键注意力头（如少至三个头），能够禁用自验证机制，并验证类似组件存在于基础模型和一般 DeepSeek-R1 模型中，为理解模型内部验证电路提供了重要洞见。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14379v2",
      "published_date": "2025-04-19 18:40:51 UTC",
      "updated_date": "2025-05-11 04:15:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:41:30.639017"
    },
    {
      "arxiv_id": "2504.14372v1",
      "title": "Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping",
      "title_zh": "翻译失败",
      "authors": [
        "Jose Marie Antonio Minoza"
      ],
      "abstract": "Accurate ocean modeling and coastal hazard prediction depend on\nhigh-resolution bathymetric data; yet, current worldwide datasets are too\ncoarse for exact numerical simulations. While recent deep learning advances\nhave improved earth observation data resolution, existing methods struggle with\nthe unique challenges of producing detailed ocean floor maps, especially in\nmaintaining physical structure consistency and quantifying uncertainties. This\nwork presents a novel uncertainty-aware mechanism using spatial blocks to\nefficiently capture local bathymetric complexity based on block-based conformal\nprediction. Using the Vector Quantized Variational Autoencoder (VQ-VAE)\narchitecture, the integration of this uncertainty quantification framework\nyields spatially adaptive confidence estimates while preserving topographical\nfeatures via discrete latent representations. With smaller uncertainty widths\nin well-characterized areas and appropriately larger bounds in areas of complex\nseafloor structures, the block-based design adapts uncertainty estimates to\nlocal bathymetric complexity. Compared to conventional techniques, experimental\nresults over several ocean regions show notable increases in both\nreconstruction quality and uncertainty estimation reliability. This framework\nincreases the reliability of bathymetric reconstructions by preserving\nstructural integrity while offering spatially adaptive uncertainty estimates,\nso opening the path for more solid climate modeling and coastal hazard\nassessment.",
      "tldr_zh": "本研究针对高分辨率海底地形数据（bathymetric data）的需求，提出了一种新型不确定性感知机制，以解决现有深度学习方法在海洋地板映射中维护物理结构一致性和量化不确定性的挑战。方法基于块-based conformal prediction 和 Vector Quantized Variational Autoencoder (VQ-VAE) 架构，通过空间块捕获局部海底复杂性，提供空间自适应置信度估计，同时保留地形特征。实验结果显示，该框架在多个海洋区域实现了重建质量和不确定性估计可靠性的显著提升，最终提升了海底地形重建的可靠性，并为气候建模和沿海灾害评估提供更坚实的基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14372v1",
      "published_date": "2025-04-19 18:20:08 UTC",
      "updated_date": "2025-04-19 18:20:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:41:42.372404"
    },
    {
      "arxiv_id": "2504.14367v1",
      "title": "Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Machado Santos",
        "Rita Maria da Silva Julia",
        "Marcelo Zanchetta do Nascimento"
      ],
      "abstract": "Prompt engineering is essential for optimizing large language models (LLMs),\nyet the link between prompt structures and task performance remains\nunderexplored. This work introduces an evolutionary approach that combines\ncontext-free grammar (CFG) with the MAP-Elites algorithm to systematically\nexplore the prompt space. Our method prioritizes quality and diversity,\ngenerating high-performing and structurally varied prompts while analyzing\ntheir alignment with diverse tasks by varying traits such as the number of\nexamples (shots) and reasoning depth. By systematically mapping the phenotypic\nspace, we reveal how structural variations influence LLM performance, offering\nactionable insights for task-specific and adaptable prompt design. Evaluated on\nseven BigBench Lite tasks across multiple LLMs, our results underscore the\ncritical interplay of quality and diversity, advancing the effectiveness and\nversatility of LLMs.",
      "tldr_zh": "本研究探讨了提示工程在优化大语言模型(LLMs)中的作用，引入了一种结合上下文无关文法(CFG)和MAP-Elites算法的进化方法，来系统探索提示空间。该方法强调质量和多样性，生成高性能且结构多样的提示，并分析因素如示例数量(shots)和推理深度对任务性能的影响。通过在七个BigBench Lite任务上评估多个LLMs，结果揭示了结构变化与性能的关键互动，为任务特定和可适应提示设计提供了实用见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages Accepted for publication in IEEE CEC 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.14367v1",
      "published_date": "2025-04-19 17:50:34 UTC",
      "updated_date": "2025-04-19 17:50:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:41:55.264288"
    },
    {
      "arxiv_id": "2504.14366v1",
      "title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models",
      "title_zh": "Transformer 到亚二次方语言模型的知识蒸馏实证评估",
      "authors": [
        "Patrick Haller",
        "Jonas Golde",
        "Alan Akbik"
      ],
      "abstract": "Knowledge distillation is a widely used technique for compressing large\nlanguage models (LLMs) by training a smaller student model to mimic a larger\nteacher model. Typically, both the teacher and student are Transformer-based\narchitectures, leveraging softmax attention for sequence modeling. However, the\nquadratic complexity of self-attention at inference time remains a significant\nbottleneck, motivating the exploration of subquadratic alternatives such as\nstructured state-space models (SSMs), linear attention, and recurrent\narchitectures. In this work, we systematically evaluate the transferability of\nknowledge distillation from a Transformer teacher to nine subquadratic student\narchitectures. Our study aims to determine which subquadratic model best aligns\nwith the teacher's learned representations and how different architectural\nconstraints influence the distillation process. We also investigate the impact\nof intelligent initialization strategies, including matrix mixing and\nquery-key-value (QKV) copying, on the adaptation process. Our empirical results\non multiple NLP benchmarks provide insights into the trade-offs between\nefficiency and performance, highlighting key factors for successful knowledge\ntransfer to subquadratic architectures.",
      "tldr_zh": "这篇论文评估了知识蒸馏（Knowledge Distillation）从 Transformer 模型到次二次方（Subquadratic）语言模型的实证效果，旨在通过训练较小学生模型来压缩大型语言模型（LLMs）并缓解自注意力机制的二次方复杂度问题。研究系统比较了九种次二次方学生架构，包括结构化状态空间模型（SSMs）、线性注意力（Linear Attention）和循环架构，分析了这些架构如何与 Transformer 教师模型的表示对齐，以及架构约束对蒸馏过程的影响。结果显示，智能初始化策略如 matrix mixing 和 query-key-value (QKV) copying 能显著提升适应性，并在多个 NLP 基准测试中揭示了效率与性能权衡的关键因素，为高效模型设计提供了指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14366v1",
      "published_date": "2025-04-19 17:49:52 UTC",
      "updated_date": "2025-04-19 17:49:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:42:07.993864"
    },
    {
      "arxiv_id": "2504.14365v1",
      "title": "Accelerating LLM Inference with Flexible N:M Sparsity via A Fully Digital Compute-in-Memory Accelerator",
      "title_zh": "通过全数字计算内存储加速器实现灵活 N:M 稀疏性的 LLM 推理加速",
      "authors": [
        "Akshat Ramachandran",
        "Souvik Kundu",
        "Arnab Raha",
        "Shamik Kundu",
        "Deepak K. Mathaikutty",
        "Tushar Krishna"
      ],
      "abstract": "Large language model (LLM) pruning with fixed N:M structured sparsity\nsignificantly limits the expressivity of the sparse model, yielding sub-optimal\nperformance. In contrast, supporting multiple N:M patterns to provide sparse\nrepresentational freedom introduces costly overhead in hardware. To address\nthese challenges for LLMs, we first present a flexible layer-wise\noutlier-density-aware N:M sparsity (FLOW) selection method. FLOW enables the\nidentification of optimal layer-wise N and M values (from a given range) by\nsimultaneously accounting for the presence and distribution of outliers,\nallowing a higher degree of representational freedom. To deploy sparse models\nwith such N:M flexibility, we then introduce a flexible, low-overhead digital\ncompute-in-memory architecture (FlexCiM). FlexCiM supports diverse sparsity\npatterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros,\nwhich are adaptively aggregated and disaggregated through distribution and\nmerging mechanisms for different N and M values. Extensive experiments on both\ntransformer-based and recurrence-based state space foundation models (SSMs)\ndemonstrate that FLOW outperforms existing alternatives with an accuracy\nimprovement of up to 36%, while FlexCiM achieves up to 1.75x lower inference\nlatency and 1.5x lower energy consumption compared to existing sparse\naccelerators. Code is available at: https://github.com/FLOW-open-project/FLOW",
      "tldr_zh": "这篇论文提出了 FLOW 方法，一种灵活的层级外点密度感知的 N:M 稀疏性选择策略，用于优化大型语言模型(LLM)的表示自由度，同时考虑外点(outliers)的分布以提升模型性能。作者随后引入了 FlexCiM 架构，这是一种全数字计算内存储存(DCiM)加速器，通过分区子宏和动态聚合机制支持多种 N:M 稀疏模式，显著降低硬件开销。实验结果显示，FLOW 在 transformer-based 和 recurrence-based 状态空间模型(SSMs)上比现有方法准确率提高高达 36%，而 FlexCiM 实现了高达 1.75x 的推理延迟降低和 1.5x 的能量消耗降低。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14365v1",
      "published_date": "2025-04-19 17:47:01 UTC",
      "updated_date": "2025-04-19 17:47:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:42:19.336867"
    },
    {
      "arxiv_id": "2504.14359v1",
      "title": "A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Kyle Buettner",
        "Jacob Emmerson",
        "Adriana Kovashka"
      ],
      "abstract": "There are many ways to describe, name, and group objects when captioning an\nimage. Differences are evident when speakers come from diverse cultures due to\nthe unique experiences that shape perception. Machine translation of captions\nhas pushed multilingual capabilities in vision-language models (VLMs), but data\ncomes mainly from English speakers, indicating a perceptual bias and lack of\nmodel flexibility. In this work, we address this challenge and outline a\ndata-efficient framework to instill multilingual VLMs with greater\nunderstanding of perceptual diversity. We specifically propose an LLM-based,\nmultimodal recaptioning strategy that alters the object descriptions of English\ncaptions before translation. The greatest benefits are demonstrated in a\ntargeted multimodal mechanism guided by native speaker data. By adding produced\nrewrites as augmentations in training, we improve on German and Japanese\ntext-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on\nnon-native error cases). We further propose a mechanism to analyze the specific\nobject description differences across datasets, and we offer insights into\ncross-dataset and cross-language generalization.",
      "tldr_zh": "该论文针对视觉语言模型(VLMs)中的感知偏差问题，提出一个多模态recaptioning框架，以处理多语言环境下的物体描述多样性，该框架基于LLM策略修改英语标题的物体描述后再进行翻译，并利用本土说话者数据进行引导性增强。实验结果显示，通过将生成的改写作为训练数据增强，在德语和日语的文本-图像检索任务中，平均召回率提高了3.5%，非本土错误案例提升了4.7%。此外，该框架还包括机制分析不同数据集中的物体描述差异，并提供了关于跨数据集和跨语言泛化的见解。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14359v1",
      "published_date": "2025-04-19 17:23:12 UTC",
      "updated_date": "2025-04-19 17:23:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:42:31.749013"
    },
    {
      "arxiv_id": "2504.14356v1",
      "title": "Mathematical Programming Models for Exact and Interpretable Formulation of Neural Networks",
      "title_zh": "用于神经网络精确和可解释表述的数学规划模型",
      "authors": [
        "Masoud Ataei",
        "Edrin Hasaj",
        "Jacob Gipp",
        "Sepideh Forouzi"
      ],
      "abstract": "This paper presents a unified mixed-integer programming framework for\ntraining sparse and interpretable neural networks. We develop exact\nformulations for both fully connected and convolutional architectures by\nmodeling nonlinearities such as ReLU activations through binary variables and\nencoding structural sparsity via filter- and layer-level pruning constraints.\nThe resulting models integrate parameter learning, architecture selection, and\nstructural regularization within a single optimization problem, yielding\nglobally optimal solutions with respect to a composite objective that balances\nprediction accuracy, weight sparsity, and architectural compactness. The\nmixed-integer programming formulation accommodates piecewise-linear operations,\nincluding max pooling and activation gating, and permits precise enforcement of\nlogic-based or domain-specific constraints. By incorporating considerations of\ninterpretability, sparsity, and verifiability directly into the training\nprocess, the proposed framework bridges a range of research areas including\nexplainable artificial intelligence, symbolic reasoning, and formal\nverification.",
      "tldr_zh": "这篇论文提出了一种统一的混合整数编程(mixed-integer programming)框架，用于训练稀疏且可解释的神经网络，包括全连接和卷积架构。框架通过二进制变量建模ReLU等非线性激活，并使用过滤器和层级修剪约束来编码结构稀疏性，从而将参数学习、架构选择和结构正则化整合到一个优化问题中，实现全局最优解并平衡预测准确性、权重稀疏性和架构紧凑性。该方法支持分段线性操作如最大池化(max pooling)和激活门控，并允许精确执行逻辑或领域特定约束，最终桥接了可解释人工智能(explainable artificial intelligence)、符号推理和形式验证等研究领域。",
      "categories": [
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14356v1",
      "published_date": "2025-04-19 16:50:17 UTC",
      "updated_date": "2025-04-19 16:50:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:42:43.200971"
    },
    {
      "arxiv_id": "2504.14350v3",
      "title": "An Empirical Study of LLM Reasoning Ability Under Strict Output Length Constraint",
      "title_zh": "LLM 推理能力在严格输出长度约束下的实证研究",
      "authors": [
        "Yi Sun",
        "Han Wang",
        "Jiaqiang Li",
        "Jiacheng Liu",
        "Xiangyu Li",
        "Hao Wen",
        "Yizhen Yuan",
        "Huiwen Zheng",
        "Yan Liang",
        "Yuanchun Li",
        "Yunxin Liu"
      ],
      "abstract": "Recent work has demonstrated the remarkable potential of Large Language\nModels (LLMs) in test-time scaling. By making models think before answering,\nthey are able to achieve much higher accuracy with extra inference computation.\nHowever, in many real-world scenarios, models are used under time constraints,\nwhere an answer should be given within a certain output length. It is unclear\nwhether and how the reasoning ability of different LLMs remain effective under\nstrict constraints. We take a first look at this problem by conducting an\nin-depth empirical study. Specifically, we test 30 LLMs on common reasoning\ndatasets under a wide range of output length budgets, and we analyze the\ncorrelation between the inference accuracy and various properties including\nmodel type, model size, prompt style, etc. We also consider the mappings\nbetween token budgets and actual on-device latency budgets. The results have\ndemonstrated several interesting findings regarding the budget-aware LLM\nreasoning ability that differ from the unconstrained situation, e.g. the\noptimal choices of either model size or prompt style change under different\nbudgets. These findings offer timely evaluation to this area and practical\nguidance for users to deploy LLMs under real-world latency constraints.",
      "tldr_zh": "本文通过实证研究，探讨了大型语言模型(LLMs)在严格输出长度约束下的推理能力，测试了30个LLMs在常见推理数据集上的表现，并分析了准确率与模型类型、模型大小、提示风格等属性的相关性。研究发现，与无约束情况不同，不同输出长度预算会改变最优的模型大小和提示风格选择，例如token预算与设备延迟的映射影响实际部署。总体结果为预算aware的LLM推理提供了宝贵评价和实用指导。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14350v3",
      "published_date": "2025-04-19 16:32:28 UTC",
      "updated_date": "2025-05-21 09:05:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:42:54.632159"
    },
    {
      "arxiv_id": "2504.14345v1",
      "title": "Integrating LLM-Generated Views into Mean-Variance Optimization Using the Black-Litterman Model",
      "title_zh": "翻译失败",
      "authors": [
        "Youngbin Lee",
        "Yejin Kim",
        "Suin Kim",
        "Yongjae Lee"
      ],
      "abstract": "Portfolio optimization faces challenges due to the sensitivity in traditional\nmean-variance models. The Black-Litterman model mitigates this by integrating\ninvestor views, but defining these views remains difficult. This study explores\nthe integration of large language models (LLMs) generated views into portfolio\noptimization using the Black-Litterman framework. Our method leverages LLMs to\nestimate expected stock returns from historical prices and company metadata,\nincorporating uncertainty through the variance in predictions. We conduct a\nbacktest of the LLM-optimized portfolios from June 2024 to February 2025,\nrebalancing biweekly using the previous two weeks of price data. As baselines,\nwe compare against the S&P 500, an equal-weighted portfolio, and a traditional\nmean-variance optimized portfolio constructed using the same set of stocks.\nEmpirical results suggest that different LLMs exhibit varying levels of\npredictive optimism and confidence stability, which impact portfolio\nperformance. The source code and data are available at\nhttps://github.com/youngandbin/LLM-MVO-BLM.",
      "tldr_zh": "本研究探讨了将大型语言模型 (LLMs) 生成的投资观点整合到 Black-Litterman 模型中，以解决传统 mean-variance 优化模型的敏感性问题。方法包括利用 LLMs 从历史股价和公司元数据估计股票预期回报，并通过预测方差纳入不确定性，然后进行回测（从 2024 年 6 月到 2025 年 2 月，每两周基于前两周数据重新平衡）。与 S&P 500、等权重投资组合和传统 mean-variance 优化组合相比，实验结果显示不同 LLMs 的预测乐观度和信心稳定性对投资组合表现有显著影响。该方法为增强投资决策提供了新途径，并已在 GitHub 上公开源代码和数据。",
      "categories": [
        "q-fin.PM",
        "cs.AI"
      ],
      "primary_category": "q-fin.PM",
      "comment": "Presented at the ICLR 2025 Workshop on Financial AI\n  (https://sites.google.com/view/financialaiiclr25/home)",
      "pdf_url": "http://arxiv.org/pdf/2504.14345v1",
      "published_date": "2025-04-19 16:26:14 UTC",
      "updated_date": "2025-04-19 16:26:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:43:06.619892"
    },
    {
      "arxiv_id": "2504.18560v1",
      "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Alessio Buscemi",
        "Cédric Lothritz",
        "Sergio Morales",
        "Marcos Gomez-Vazquez",
        "Robert Clarisó",
        "Jordi Cabot",
        "German Castignani"
      ],
      "abstract": "Large Language Models (LLMs) have exhibited impressive natural language\nprocessing capabilities but often perpetuate social biases inherent in their\ntraining data. To address this, we introduce MultiLingual Augmented Bias\nTesting (MLA-BiTe), a framework that improves prior bias evaluation methods by\nenabling systematic multilingual bias testing. MLA-BiTe leverages automated\ntranslation and paraphrasing techniques to support comprehensive assessments\nacross diverse linguistic settings. In this study, we evaluate the\neffectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six\nlanguages -- including two low-resource languages -- focusing on seven\nsensitive categories of discrimination.",
      "tldr_zh": "该研究引入了 MultiLingual Augmented Bias Testing (MLA-BiTe) 框架，以系统化方式评估 Large Language Models (LLMs) 在多语言环境中的社会偏差问题。MLA-BiTe 通过自动化翻译和改写技术，支持对高资源和低资源语言的全面偏差测试，弥补了现有方法的局限性。在实验中，该框架评估了四个最先进 LLMs，在六种语言（包括两种低资源语言）上针对七个敏感歧视类别进行测试，展示了其在识别和减轻偏差方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18560v1",
      "published_date": "2025-04-19 16:18:22 UTC",
      "updated_date": "2025-04-19 16:18:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:43:18.157152"
    },
    {
      "arxiv_id": "2504.14335v1",
      "title": "Visual Prompting for One-shot Controllable Video Editing without Inversion",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengbo Zhang",
        "Yuxi Zhou",
        "Duo Peng",
        "Joo-Hwee Lim",
        "Zhigang Tu",
        "De Wen Soh",
        "Lin Geng Foo"
      ],
      "abstract": "One-shot controllable video editing (OCVE) is an important yet challenging\ntask, aiming to propagate user edits that are made -- using any image editing\ntool -- on the first frame of a video to all subsequent frames, while ensuring\ncontent consistency between edited frames and source frames. To achieve this,\nprior methods employ DDIM inversion to transform source frames into latent\nnoise, which is then fed into a pre-trained diffusion model, conditioned on the\nuser-edited first frame, to generate the edited video. However, the DDIM\ninversion process accumulates errors, which hinder the latent noise from\naccurately reconstructing the source frames, ultimately compromising content\nconsistency in the generated edited frames. To overcome it, our method\neliminates the need for DDIM inversion by performing OCVE through a novel\nperspective based on visual prompting. Furthermore, inspired by consistency\nmodels that can perform multi-step consistency sampling to generate a sequence\nof content-consistent images, we propose a content consistency sampling (CCS)\nto ensure content consistency between the generated edited frames and the\nsource frames. Moreover, we introduce a temporal-content consistency sampling\n(TCS) based on Stein Variational Gradient Descent to ensure temporal\nconsistency across the edited frames. Extensive experiments validate the\neffectiveness of our approach.",
      "tldr_zh": "这篇论文针对 One-shot Controllable Video Editing (OCVE) 问题，提出了一种无需 DDIM inversion 的方法，通过 visual prompting 将用户对视频第一帧的编辑传播到后续帧，同时确保内容一致性。论文引入内容一致性采样 (CCS)，基于一致性模型来生成与源帧一致的编辑帧，并提出时间-内容一致性采样 (TCS)，利用 Stein Variational Gradient Descent 维护编辑帧之间的时间一致性。这些创新显著提高了编辑质量，广泛实验验证了方法的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted by cvpr2025",
      "pdf_url": "http://arxiv.org/pdf/2504.14335v1",
      "published_date": "2025-04-19 16:00:47 UTC",
      "updated_date": "2025-04-19 16:00:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:43:30.855157"
    },
    {
      "arxiv_id": "2504.14325v2",
      "title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory",
      "title_zh": "翻译失败",
      "authors": [
        "Alessio Buscemi",
        "Daniele Proverbio",
        "Alessandro Di Stefano",
        "The Anh Han",
        "German Castignani",
        "Pietro Liò"
      ],
      "abstract": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents.",
      "tldr_zh": "这篇论文引入了 FAIRGAME 框架，该框架利用 Game Theory 来识别和分析 AI Agents 在多代理互动中的偏见问题。FAIRGAME 通过可重现的模拟环境，允许用户轻松模拟各种游戏场景，并比较不同 Large Language Model (LLM)、语言、代理个性或战略知识的影响，从而揭示潜在的偏见行为。实验结果显示，该框架有助于系统地发现 AI 偏见、预测战略互动的emerging behavior，并为战略决策研究提供可靠工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14325v2",
      "published_date": "2025-04-19 15:29:04 UTC",
      "updated_date": "2025-04-22 11:56:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:43:42.378171"
    },
    {
      "arxiv_id": "2504.15311v1",
      "title": "RINN: One Sample Radio Frequency Imaging based on Physics Informed Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Fei Shang",
        "Haohua Du",
        "Dawei Yan",
        "Panlong Yang",
        "Xiang-Yang Li"
      ],
      "abstract": "Due to its ability to work in non-line-of-sight and low-light environments,\nradio frequency (RF) imaging technology is expected to bring new possibilities\nfor embodied intelligence and multimodal sensing. However, widely used RF\ndevices (such as Wi-Fi) often struggle to provide high-precision\nelectromagnetic measurements and large-scale datasets, hindering the\napplication of RF imaging technology. In this paper, we combine the ideas of\nPINN to design the RINN network, using physical constraints instead of true\nvalue comparison constraints and adapting it with the characteristics of\nubiquitous RF signals, allowing the RINN network to achieve RF imaging using\nonly one sample without phase and with amplitude noise. Our numerical\nevaluation results show that compared with 5 classic algorithms based on phase\ndata for imaging results, RINN's imaging results based on phaseless data are\ngood, with indicators such as RRMSE (0.11) performing similarly well. RINN\nprovides new possibilities for the universal development of radio frequency\nimaging technology.",
      "tldr_zh": "本论文提出 RINN 网络，基于 Physics Informed Neural Network (PINN)，通过利用物理约束代替真实值比较，并适应射频 (RF) 信号特性，实现仅用一个样本的无相位 RF 成像，同时处理幅度噪声。相比基于相位数据的5种经典算法，RINN 在成像性能上表现出色，RRMSE 指标仅为0.11。RINN 为 RF 成像技术在非视线和低光环境中的通用应用提供了新可能性。",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15311v1",
      "published_date": "2025-04-19 15:19:12 UTC",
      "updated_date": "2025-04-19 15:19:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:43:54.895868"
    },
    {
      "arxiv_id": "2504.14320v2",
      "title": "Expanding the Generative AI Design Space through Structured Prompting and Multimodal Interfaces",
      "title_zh": "通过结构化提示和多模态接口扩展生成式 AI 设计空间",
      "authors": [
        "Nimisha Karnatak",
        "Adrien Baranes",
        "Rob Marchant",
        "Huinan Zeng",
        "Tríona Butler",
        "Kristen Olson"
      ],
      "abstract": "Text-based prompting remains the predominant interaction paradigm in\ngenerative AI, yet it often introduces friction for novice users such as small\nbusiness owners (SBOs), who struggle to articulate creative goals in\ndomain-specific contexts like advertising. Through a formative study with six\nSBOs in the United Kingdom, we identify three key challenges: difficulties in\nexpressing brand intuition through prompts, limited opportunities for\nfine-grained adjustment and refinement during and after content generation, and\nthe frequent production of generic content that lacks brand specificity. In\nresponse, we present ACAI (AI Co-Creation for Advertising and Inspiration), a\nmultimodal generative AI tool designed to support novice designers by moving\nbeyond traditional prompt interfaces. ACAI features a structured input system\ncomposed of three panels: Branding, Audience and Goals, and the Inspiration\nBoard. These inputs allow users to convey brand-relevant context and visual\npreferences. This work contributes to HCI research on generative systems by\nshowing how structured interfaces can foreground user-defined context, improve\nalignment, and enhance co-creative control in novice creative workflows.",
      "tldr_zh": "该研究通过对六名英国小型企业主的形成性研究，识别了生成式AI中文本提示的三大挑战：难以表达品牌直觉、缺乏细粒度调整机会，以及生成缺乏品牌特性的通用内容。为解决这些问题，论文提出ACAI（AI Co-Creation for Advertising and Inspiration），一个多模态生成AI工具，采用结构化输入系统（包括Branding、Audience and Goals以及Inspiration Board面板），以帮助新手用户更好地传达品牌上下文和视觉偏好。该工作为HCI（Human-Computer Interaction）研究贡献了新见解，展示了结构化界面如何提升用户定义的上下文对齐度，并在新手创意工作流中增强共同创造控制。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at CHI'25 Workshop on Designing and Developing User\n  Interfaces with AI",
      "pdf_url": "http://arxiv.org/pdf/2504.14320v2",
      "published_date": "2025-04-19 14:57:32 UTC",
      "updated_date": "2025-04-22 17:59:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:44:06.467664"
    },
    {
      "arxiv_id": "2504.14302v1",
      "title": "Learning to Score",
      "title_zh": "学习评分",
      "authors": [
        "Yogev Kriger",
        "Shai Fine"
      ],
      "abstract": "Common machine learning settings range from supervised tasks, where\naccurately labeled data is accessible, through semi-supervised and\nweakly-supervised tasks, where target labels are scant or noisy, to\nunsupervised tasks where labels are unobtainable. In this paper we study a\nscenario where the target labels are not available but additional related\ninformation is at hand. This information, referred to as Side Information, is\neither correlated with the unknown labels or imposes constraints on the feature\nspace. We formulate the problem as an ensemble of three semantic components:\nrepresentation learning, side information and metric learning. The proposed\nscoring model is advantageous for multiple use-cases. For example, in the\nhealthcare domain it can be used to create a severity score for diseases where\nthe symptoms are known but the criteria for the disease progression are not\nwell defined. We demonstrate the utility of the suggested scoring system on\nwell-known benchmark data-sets and bio-medical patient records.",
      "tldr_zh": "本研究探讨了机器学习中一种特殊场景：目标标签不可用，但有相关Side Information可用，该信息与未知标签相关或对特征空间施加约束。论文提出了一种集成representation learning（表示学习）、side information（侧信息）和metric learning（度量学习）的scoring model，用于生成评分。scoring model适用于多种领域，如医疗中创建疾病严重程度评分；实验在知名基准数据集和生物医学患者记录上验证了其效用，展示了显著的实用价值。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14302v1",
      "published_date": "2025-04-19 13:53:38 UTC",
      "updated_date": "2025-04-19 13:53:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:44:17.935579"
    },
    {
      "arxiv_id": "2504.14301v1",
      "title": "Balancing Privacy and Action Performance: A Penalty-Driven Approach to Image Anonymization",
      "title_zh": "翻译失败",
      "authors": [
        "Nazia Aslam",
        "Kamal Nasrollahi"
      ],
      "abstract": "The rapid development of video surveillance systems for object detection,\ntracking, activity recognition, and anomaly detection has revolutionized our\nday-to-day lives while setting alarms for privacy concerns. It isn't easy to\nstrike a balance between visual privacy and action recognition performance in\nmost computer vision models. Is it possible to safeguard privacy without\nsacrificing performance? It poses a formidable challenge, as even minor privacy\nenhancements can lead to substantial performance degradation. To address this\nchallenge, we propose a privacy-preserving image anonymization technique that\noptimizes the anonymizer using penalties from the utility branch, ensuring\nimproved action recognition performance while minimally affecting privacy\nleakage. This approach addresses the trade-off between minimizing privacy\nleakage and maintaining high action performance. The proposed approach is\nprimarily designed to align with the regulatory standards of the EU AI Act and\nGDPR, ensuring the protection of personally identifiable information while\nmaintaining action performance. To the best of our knowledge, we are the first\nto introduce a feature-based penalty scheme that exclusively controls the\naction features, allowing freedom to anonymize private attributes. Extensive\nexperiments were conducted to validate the effectiveness of the proposed\nmethod. The results demonstrate that applying a penalty to anonymizer from\nutility branch enhances action performance while maintaining nearly consistent\nprivacy leakage across different penalty settings.",
      "tldr_zh": "本论文提出了一种基于惩罚驱动的图像匿名化方法，旨在平衡隐私保护与动作识别性能的权衡。该方法通过从实用性分支施加惩罚来优化匿名器，确保最小化隐私泄露的同时提升动作识别准确性，并符合 EU AI Act 和 GDPR 标准。作为创新，该方案首次引入基于特征的惩罚机制，专门控制动作特征而非私人属性。实验结果表明，该方法在不同惩罚设置下显著提高了动作性能，同时保持隐私泄露基本不变。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPRW 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.14301v1",
      "published_date": "2025-04-19 13:52:33 UTC",
      "updated_date": "2025-04-19 13:52:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:44:30.613831"
    },
    {
      "arxiv_id": "2504.14300v2",
      "title": "Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection",
      "title_zh": "利用弱监督训练和权重选择的 GAN 学习与生成",
      "authors": [
        "Xinyu Liang",
        "Hao Wang"
      ],
      "abstract": "The scarcity of high-quality residential load data can pose obstacles for\ndecarbonizing the residential sector as well as effective grid planning and\noperation. The above challenges have motivated research into generating\nsynthetic load data, but existing methods faced limitations in terms of\nscalability, diversity, and similarity. This paper proposes a Generative\nAdversarial Network-based Synthetic Residential Load Pattern (RLP-GAN)\ngeneration model, a novel weakly-supervised GAN framework, leveraging an\nover-complete autoencoder to capture dependencies within complex and diverse\nload patterns and learn household-level data distribution at scale. We\nincorporate a model weight selection method to address the mode collapse\nproblem and generate load patterns with high diversity. We develop a holistic\nevaluation method to validate the effectiveness of RLP-GAN using real-world\ndata of 417 households. The results demonstrate that RLP-GAN outperforms\nstate-of-the-art models in capturing temporal dependencies and generating load\npatterns with higher similarity to real data. Furthermore, we have publicly\nreleased the RLP-GAN generated synthetic dataset, which comprises one million\nsynthetic residential load pattern profiles.",
      "tldr_zh": "本文提出RLP-GAN，一种基于Generative Adversarial Network (GAN)的弱监督训练框架，用于生成多样化的住宅负载模式，以解决住宅负载数据稀缺对脱碳和电网规划的影响。框架利用over-complete autoencoder捕捉复杂负载模式的依赖关系，并引入模型权重选择方法来避免mode collapse问题，从而提高生成模式的多样性和相似性。实验使用417户真实数据评估，结果显示RLP-GAN在捕捉temporal dependencies和生成与真实数据更相似的负载模式方面优于现有模型。此外，作者公开了一百万合成住宅负载模式数据集，以支持进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.14300v2",
      "published_date": "2025-04-19 13:50:49 UTC",
      "updated_date": "2025-04-25 05:01:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:44:43.273190"
    },
    {
      "arxiv_id": "2504.14298v3",
      "title": "RadioDiff-Inverse: Diffusion Enhanced Bayesian Inverse Estimation for ISAC Radio Map Construction",
      "title_zh": "翻译失败",
      "authors": [
        "Xiucheng Wang",
        "Zhongsheng Fang",
        "Nan Cheng",
        "Ruijin Sun",
        "Zan Li",
        "Xuemin",
        "Shen"
      ],
      "abstract": "Radio maps (RMs) are essential for environment-aware communication and\nsensing, providing location-specific wireless channel information. Existing RM\nconstruction methods often rely on precise environmental data and base station\n(BS) locations, which are not always available in dynamic or privacy-sensitive\nenvironments. While sparse measurement techniques reduce data collection, the\nimpact of noise in sparse data on RM accuracy is not well understood. This\npaper addresses these challenges by formulating RM construction as a Bayesian\ninverse problem under coarse environmental knowledge and noisy sparse\nmeasurements. Although maximum a posteriori (MAP) filtering offers an optimal\nsolution, it requires a precise prior distribution of the RM, which is\ntypically unavailable. To solve this, we propose RadioDiff-Inverse, a\ndiffusion-enhanced Bayesian inverse estimation framework that uses an\nunconditional generative diffusion model to learn the RM prior. This approach\nnot only reconstructs the spatial distribution of wireless channel features but\nalso enables environmental structure perception, such as building outlines, and\nlocation of BS just relay on pathloss, through integrated sensing and\ncommunication (ISAC). Remarkably, RadioDiff-Inverse is training-free,\nleveraging a pre-trained model from Imagenet without task-specific fine-tuning,\nwhich significantly reduces the training cost of using generative large model\nin wireless networks. Experimental results demonstrate that RadioDiff-Inverse\nachieves state-of-the-art performance in accuracy of RM construction and\nenvironmental reconstruction, and robustness against noisy sparse sampling.",
      "tldr_zh": "该论文针对无线电地图 (Radio Maps) 构建问题，提出 RadioDiff-Inverse 框架，将其表述为贝叶斯逆问题 (Bayesian inverse problem)，利用粗略环境知识和嘈杂稀疏测量数据进行优化。框架通过无条件生成扩散模型学习 RM 的先验分布，实现训练-free 的设计，仅需预训练模型（如从 Imagenet），从而重建无线通道特征并感知环境结构，如建筑物轮廓和基站位置，在集成感知与通信 (ISAC) 中应用。实验结果表明，RadioDiff-Inverse 在 RM 构建和环境重建的准确性上达到最先进水平，并对嘈杂稀疏采样表现出卓越的鲁棒性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2504.14298v3",
      "published_date": "2025-04-19 13:49:59 UTC",
      "updated_date": "2025-05-21 03:11:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:44:56.014510"
    },
    {
      "arxiv_id": "2504.15310v1",
      "title": "Power Transformer Health Index and Life Span Assessment: A Comprehensive Review of Conventional and Machine Learning based Approaches",
      "title_zh": "电力变压器健康指数和寿命评估：传统方法与基于机器学习的方法的全面回顾",
      "authors": [
        "Syeda Tahreem Zahra",
        "Syed Kashif Imdad",
        "Sohail Khan",
        "Sohail Khalid",
        "Nauman Anwar Baig"
      ],
      "abstract": "Power transformers play a critical role within the electrical power system,\nmaking their health assessment and the prediction of their remaining lifespan\nparamount for the purpose of ensuring efficient operation and facilitating\neffective maintenance planning. This paper undertakes a comprehensive\nexamination of existent literature, with a primary focus on both conventional\nand cutting-edge techniques employed within this domain. The merits and\ndemerits of recent methodologies and techniques are subjected to meticulous\nscrutiny and explication. Furthermore, this paper expounds upon intelligent\nfault diagnosis methodologies and delves into the most widely utilized\nintelligent algorithms for the assessment of transformer conditions. Diverse\nArtificial Intelligence (AI) approaches, including Artificial Neural Networks\n(ANN) and Convolutional Neural Network (CNN), Support Vector Machine (SVM),\nRandom Forest (RF), Genetic Algorithm (GA), and Particle Swarm Optimization\n(PSO), are elucidated offering pragmatic solutions for enhancing the\nperformance of transformer fault diagnosis. The amalgamation of multiple AI\nmethodologies and the exploration of timeseries analysis further contribute to\nthe augmentation of diagnostic precision and the early detection of faults in\ntransformers. By furnishing a comprehensive panorama of AI applications in the\nfield of transformer fault diagnosis, this study lays the groundwork for future\nresearch endeavors and the progression of this critical area of study.",
      "tldr_zh": "这篇论文对电力变压器的健康指数和寿命评估进行了全面综述，重点审查了传统方法和基于机器学习的先进技术，并分析了这些方法的优缺点。论文详细阐述了智能故障诊断策略，包括广泛应用的AI算法如Artificial Neural Networks (ANN)、Convolutional Neural Network (CNN)、Support Vector Machine (SVM)、Random Forest (RF)、Genetic Algorithm (GA)和Particle Swarm Optimization (PSO)，这些方法有助于提升诊断性能。论文还探讨了结合多种AI方法和时间序列分析来提高故障检测精度和早期识别能力，最终为未来电力变压器健康评估研究奠定基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15310v1",
      "published_date": "2025-04-19 13:48:05 UTC",
      "updated_date": "2025-04-19 13:48:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:45:06.125958"
    },
    {
      "arxiv_id": "2505.03756v1",
      "title": "Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management",
      "title_zh": "翻译失败",
      "authors": [
        "Hang Zhang",
        "Jiuchen Shi",
        "Yixiao Wang",
        "Quan Chen",
        "Yizhou Shan",
        "Minyi Guo"
      ],
      "abstract": "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works.",
      "tldr_zh": "本文提出 FASTLIBRA 系统，以优化 Multi-LoRA 大语言模型的服务性能，针对现有方法忽略 LoRA 和 KV cache 使用依赖性的问题。FASTLIBRA 包括一个依赖感知缓存管理器，用于在统一的缓存池中维护推理过程中的 LoRA 和 KV cache 依赖关系，以及一个性能驱动缓存交换器，通过统一的成本模型动态决定缓存的交换操作。实验结果显示，FASTLIBRA 平均将 Time-To-First-Token (TTFT) 减少 63.4%，显著提升了 Multi-LoRA 应用的效率。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03756v1",
      "published_date": "2025-04-19 13:17:34 UTC",
      "updated_date": "2025-04-19 13:17:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:45:19.256051"
    },
    {
      "arxiv_id": "2504.14282v1",
      "title": "CHAINSFORMER: Numerical Reasoning on Knowledge Graphs from a Chain Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Ze Zhao",
        "Bin Lu",
        "Xiaoying Gan",
        "Gu Tang",
        "Luoyi Fu",
        "Xinbing Wang"
      ],
      "abstract": "Reasoning over Knowledge Graphs (KGs) plays a pivotal role in knowledge graph\ncompletion or question answering systems, providing richer and more accurate\ntriples and attributes. As numerical attributes become increasingly essential\nin characterizing entities and relations in KGs, the ability to reason over\nthese attributes has gained significant importance. Existing graph-based\nmethods such as Graph Neural Networks (GNNs) and Knowledge Graph Embeddings\n(KGEs), primarily focus on aggregating homogeneous local neighbors and\nimplicitly embedding diverse triples. However, these approaches often fail to\nfully leverage the potential of logical paths within the graph, limiting their\neffectiveness in exploiting the reasoning process. To address these\nlimitations, we propose ChainsFormer, a novel chain-based framework designed to\nsupport numerical reasoning. Chainsformer not only explicitly constructs\nlogical chains but also expands the reasoning depth to multiple hops.\nSpecially, we introduces Relation-Attribute Chains (RA-Chains), a specialized\nlogic chain, to model sequential reasoning patterns. ChainsFormer captures the\nstep-by-step nature of multi-hop reasoning along RA-Chains by employing\nsequential in-context learning. To mitigate the impact of noisy chains, we\npropose a hyperbolic affinity scoring mechanism that selects relevant logic\nchains in a variable-resolution space. Furthermore, ChainsFormer incorporates\nan attention-based numerical reasoner to identify critical reasoning paths,\nenhancing both reasoning accuracy and transparency. Experimental results\ndemonstrate that ChainsFormer significantly outperforms state-of-the-art\nmethods, achieving up to a 20.0% improvement in performance. The\nimplementations are available at\nhttps://github.com/zhaodazhuang2333/ChainsFormer.",
      "tldr_zh": "这篇论文针对知识图谱（KGs）上的数值推理问题，提出了ChainsFormer框架，以链式视角显式构建逻辑链并扩展到多跳推理。ChainsFormer引入Relation-Attribute Chains (RA-Chains)来建模顺序推理模式，并通过顺序in-context learning捕捉逐步推理过程，同时采用hyperbolic affinity scoring机制筛选相关链条以减少噪声影响。框架还整合attention-based numerical reasoner来识别关键路径，提高推理的准确性和透明度；实验结果显示，ChainsFormer比现有方法如GNNs和KGEs提升高达20.0%的性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ICDE 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.14282v1",
      "published_date": "2025-04-19 12:47:44 UTC",
      "updated_date": "2025-04-19 12:47:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:45:31.488118"
    },
    {
      "arxiv_id": "2505.14843v1",
      "title": "Leveraging Generative AI Models to Explore Human Identity",
      "title_zh": "翻译失败",
      "authors": [
        "Yunha Yeo",
        "Daeho Um"
      ],
      "abstract": "This paper attempts to explore human identity by utilizing neural networks in\nan indirect manner. For this exploration, we adopt diffusion models,\nstate-of-the-art AI generative models trained to create human face images. By\nrelating the generated human face to human identity, we establish a\ncorrespondence between the face image generation process of the diffusion model\nand the process of human identity formation. Through experiments with the\ndiffusion model, we observe that changes in its external input result in\nsignificant changes in the generated face image. Based on the correspondence,\nwe indirectly confirm the dependence of human identity on external factors in\nthe process of human identity formation. Furthermore, we introduce\n\\textit{Fluidity of Human Identity}, a video artwork that expresses the fluid\nnature of human identity affected by varying external factors. The video is\navailable at\nhttps://www.behance.net/gallery/219958453/Fluidity-of-Human-Identity?.",
      "tldr_zh": "本研究利用 diffusion models 等生成式 AI 模型，通过生成人脸图像间接探索人类身份，将图像生成过程与人类身份形成过程建立对应关系。实验发现，扩散模型的外部输入变化会导致生成的人脸图像显著改变，从而间接证实人类身份在形成过程中依赖于外部因素。该研究进一步引入视频艺术作品 *Fluidity of Human Identity*，以生动方式表达人类身份的流动性，并提供相关链接作为补充。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ISEA 2025",
      "pdf_url": "http://arxiv.org/pdf/2505.14843v1",
      "published_date": "2025-04-19 12:35:07 UTC",
      "updated_date": "2025-04-19 12:35:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:45:42.235273"
    },
    {
      "arxiv_id": "2504.14274v1",
      "title": "ProtPainter: Draw or Drag Protein via Topology-guided Diffusion",
      "title_zh": "ProtPainter：通过拓扑引导的扩散绘制或拖拽蛋白质",
      "authors": [
        "Zhengxi Lu",
        "Shizhuo Cheng",
        "Yuru Jiang",
        "Yan Zhang",
        "Min Zhang"
      ],
      "abstract": "Recent advances in protein backbone generation have achieved promising\nresults under structural, functional, or physical constraints. However,\nexisting methods lack the flexibility for precise topology control, limiting\nnavigation of the backbone space. We present ProtPainter, a diffusion-based\napproach for generating protein backbones conditioned on 3D curves. ProtPainter\nfollows a two-stage process: curve-based sketching and sketch-guided backbone\ngeneration. For the first stage, we propose CurveEncoder, which predicts\nsecondary structure annotations from a curve to parametrize sketch generation.\nFor the second stage, the sketch guides the generative process in Denoising\nDiffusion Probabilistic Modeling (DDPM) to generate backbones. During this\nprocess, we further introduce a fusion scheduling scheme, Helix-Gating, to\ncontrol the scaling factors. To evaluate, we propose the first benchmark for\ntopology-conditioned protein generation, introducing Protein Restoration Task\nand a new metric, self-consistency Topology Fitness (scTF). Experiments\ndemonstrate ProtPainter's ability to generate topology-fit (scTF > 0.8) and\ndesignable (scTM > 0.5) backbones, with drawing and dragging tasks showcasing\nits flexibility and versatility.",
      "tldr_zh": "该研究提出 ProtPainter，一种基于拓扑引导扩散的方法，用于通过 3D 曲线生成蛋白质主链，解决了现有方法的拓扑控制灵活性不足问题。ProtPainter 采用两阶段过程：首先使用 CurveEncoder 预测二次结构注释以生成曲线草图，其次通过 Denoising Diffusion Probabilistic Modeling (DDPM) 结合 Helix-Gating 融合调度方案，引导草图生成拓扑精确的主链。实验在新的拓扑条件蛋白生成基准（包括 Protein Restoration Task 和 self-consistency Topology Fitness (scTF) 指标）上显示，ProtPainter 能生成拓扑拟合度高（scTF > 0.8）和可设计性强（scTM > 0.5）的蛋白主链，并在绘图和拖拽任务中展现出显著的灵活性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.14274v1",
      "published_date": "2025-04-19 11:59:48 UTC",
      "updated_date": "2025-04-19 11:59:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:45:55.053008"
    },
    {
      "arxiv_id": "2504.14259v1",
      "title": "Experience-based Refinement of Task Planning Knowledge in Autonomous Robots",
      "title_zh": "翻译失败",
      "authors": [
        "Hadeel Jazzaa",
        "Thomas McCluskey",
        "David Peebles"
      ],
      "abstract": "The requirement for autonomous robots to exhibit higher-level cognitive\nskills by planning and adapting in an ever-changing environment is indeed a\ngreat challenge for the AI community. Progress has been made in the automated\nplanning community on refinement and repair of an agent's symbolic knowledge to\ndo task planning in an incomplete or changing environmental model, but these\nadvances up to now have not been transferred to real physical robots. This\npaper demonstrates how a physical robot can be capable of adapting its symbolic\nknowledge of the environment, by using experiences in robot action execution to\ndrive knowledge refinement and hence to improve the success rate of the task\nplans the robot creates. To implement more robust planning systems, we propose\na method for refining domain knowledge to improve the knowledge on which\nintelligent robot behavior is based. This architecture has been implemented and\nevaluated using a NAO robot. The refined knowledge leads to the future\nsynthesis of task plans which demonstrate decreasing rates of failure over time\nas faulty knowledge is removed or adjusted.",
      "tldr_zh": "这篇论文探讨了自主机器人如何通过行动执行的经验来精炼任务规划知识，从而适应不完整或变化的环境。研究提出了一种方法，利用机器人经验驱动符号知识的改进和修复，以构建更鲁棒的规划系统。实验在NAO机器人上进行，结果显示精炼后的知识显著降低了任务计划的失败率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14259v1",
      "published_date": "2025-04-19 10:43:33 UTC",
      "updated_date": "2025-04-19 10:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:46:06.428995"
    },
    {
      "arxiv_id": "2504.14248v1",
      "title": "Rethinking Traffic Flow Forecasting: From Transition to Generatation",
      "title_zh": "重新思考交通流量预测：从转移到生成",
      "authors": [
        "Li Shijiao",
        "Ma Zhipeng",
        "He Huajun",
        "Chen Haiyue"
      ],
      "abstract": "Traffic flow prediction plays an important role in Intelligent Transportation\nSystems in traffic management and urban planning. There have been extensive\nsuccessful works in this area. However, these approaches focus only on\nmodelling the flow transition and ignore the flow generation process, which\nmanifests itself in two ways: (i) The models are based on Markovian\nassumptions, ignoring the multi-periodicity of the flow generation in nodes.\n(ii) The same structure is designed to encode both the transition and\ngeneration processes, ignoring the differences between them. To address these\nproblems, we propose an Effective Multi-Branch Similarity Transformer for\nTraffic Flow Prediction, namely EMBSFormer. Through data analysis, we find that\nthe factors affecting traffic flow include node-level traffic generation and\ngraph-level traffic transition, which describe the multi-periodicity and\ninteraction pattern of nodes, respectively. Specifically, to capture traffic\ngeneration patterns, we propose a similarity analysis module that supports\nmulti-branch encoding to dynamically expand significant cycles. For traffic\ntransition, we employ a temporal and spatial self-attention mechanism to\nmaintain global node interactions, and use GNN and time conv to model local\nnode interactions, respectively. Model performance is evaluated on three\nreal-world datasets on both long-term and short-term prediction tasks.\nExperimental results show that EMBSFormer outperforms baselines on both tasks.\nMoreover, compared to models based on flow transition modelling (e.g. GMAN,\n513k), the variant of EMBSFormer(93K) only uses 18\\% of the parameters,\nachieving the same performance.",
      "tldr_zh": "该论文重新审视交通流量预测，指出现有方法基于Markovian假设仅关注流量转移（transition），忽略了流量生成（generation）过程的多周期性和差异。作者提出EMBSFormer模型，通过一个多分支的similarity analysis module动态捕获节点级别的流量生成模式，并结合temporal and spatial self-attention机制、GNN和time conv来处理图级别的流量转移交互。实验在三个真实数据集上显示，EMBSFormer在长短期预测任务中优于基线模型，且其变体仅使用18%的参数（如GMAN的513k参数），却实现了相同性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14248v1",
      "published_date": "2025-04-19 09:52:39 UTC",
      "updated_date": "2025-04-19 09:52:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:46:20.105227"
    },
    {
      "arxiv_id": "2504.14241v1",
      "title": "A Knowledge-Informed Deep Learning Paradigm for Generalizable and Stability-Optimized Car-Following Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chengming Wang",
        "Dongyao Jia",
        "Wei Wang",
        "Dong Ngoduy",
        "Bei Peng",
        "Jianping Wang"
      ],
      "abstract": "Car-following models (CFMs) are fundamental to traffic flow analysis and\nautonomous driving. Although calibrated physics-based and trained data-driven\nCFMs can replicate human driving behavior, their reliance on specific datasets\nlimits generalization across diverse scenarios and reduces reliability in\nreal-world deployment. Moreover, these models typically focus on behavioral\nfidelity and do not support the explicit optimization of local and string\nstability, which are increasingly important for the safe and efficient\noperation of autonomous vehicles (AVs). To address these limitations, we\npropose a Knowledge-Informed Deep Learning (KIDL) paradigm that distills the\ngeneralization capabilities of pre-trained Large Language Models (LLMs) into a\nlightweight and stability-aware neural architecture. LLMs are used to extract\nfundamental car-following knowledge beyond dataset-specific patterns, and this\nknowledge is transferred to a reliable, tractable, and computationally\nefficient model through knowledge distillation. KIDL also incorporates\nstability constraints directly into its training objective, ensuring that the\nresulting model not only emulates human-like behavior but also satisfies the\nlocal and string stability requirements essential for real-world AV deployment.\nWe evaluate KIDL on the real-world NGSIM and HighD datasets, comparing its\nperformance with representative physics-based, data-driven, and hybrid CFMs.\nBoth empirical and theoretical results consistently demonstrate KIDL's superior\nbehavioral generalization and traffic flow stability, offering a robust and\nscalable solution for next-generation traffic systems.",
      "tldr_zh": "本研究提出了一种 Knowledge-Informed Deep Learning (KIDL) 范式，用于开发具有泛化性和稳定性优化的车跟随模型 (CFMs)，以解决现有模型依赖特定数据集、泛化能力不足以及未优化局部和字符串稳定性的问题。KIDL 通过利用预训练的 Large Language Models (LLMs) 提取基本的车跟随知识，并通过知识蒸馏转移到轻量级神经架构中，同时在训练目标中直接融入稳定性约束，确保模型既模仿人类驾驶行为又满足实际部署需求。在真实数据集 NGSIM 和 HighD 上进行评估，结果显示 KIDL 比传统基于物理、数据驱动或混合 CFMs 具有更优的行为泛化和交通流稳定性，为下一代交通系统的鲁棒性解决方案提供了可扩展的方法。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14241v1",
      "published_date": "2025-04-19 09:33:02 UTC",
      "updated_date": "2025-04-19 09:33:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:46:31.116139"
    },
    {
      "arxiv_id": "2504.14239v1",
      "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhang Liu",
        "Pengxiang Li",
        "Congkai Xie",
        "Xavier Hu",
        "Xiaotian Han",
        "Shengyu Zhang",
        "Hongxia Yang",
        "Fei Wu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1.",
      "tldr_zh": "该研究提出 InfiGUI-R1，一种基于 Multimodal Large Language Models (MLLMs) 的 GUI Agents，通过 Actor2Reasoner 框架将代理从 Reactive Actors 转变为 Deliberative Reasoners，以提升在复杂 GUI 环境中的鲁棒性和适应性。框架采用两阶段训练方法：第一阶段的 Reasoning Injection 使用 Spatial Reasoning Distillation，从教师模型转移跨模态空间推理能力，并整合 GUI 视觉-空间信息与逻辑推理；第二阶段的 Deliberation Enhancement 通过 Reinforcement Learning 的 Sub-goal Guidance 和 Error Recovery Scenario Construction，奖励准确子目标生成并训练错误恢复场景。实验结果表明，InfiGUI-R1 在 GUI grounding 和 trajectory 任务中表现出色，显著提高了代理的规划和错误处理能力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 3 figures, work in progress",
      "pdf_url": "http://arxiv.org/pdf/2504.14239v1",
      "published_date": "2025-04-19 09:25:55 UTC",
      "updated_date": "2025-04-19 09:25:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:46:44.069985"
    },
    {
      "arxiv_id": "2504.21013v1",
      "title": "Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge",
      "title_zh": "分析 AI 生成 MCQs 中的反馈机制：对可读性、词汇属性和挑战水平的洞见",
      "authors": [
        "Antoun Yaacoub",
        "Zainab Assaghir",
        "Lionel Prevost",
        "Jérôme Da-Rugna"
      ],
      "abstract": "Artificial Intelligence (AI)-generated feedback in educational settings has\ngarnered considerable attention due to its potential to enhance learning\noutcomes. However, a comprehensive understanding of the linguistic\ncharacteristics of AI-generated feedback, including readability, lexical\nrichness, and adaptability across varying challenge levels, remains limited.\nThis study delves into the linguistic and structural attributes of feedback\ngenerated by Google's Gemini 1.5-flash text model for computer science\nmultiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,\nconsidering three difficulty levels (easy, medium, hard) and three feedback\ntones (supportive, neutral, challenging). Key linguistic metrics, such as\nlength, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,\nand lexical density, were computed and examined. A fine-tuned RoBERTa-based\nmulti-task learning (MTL) model was trained to predict these linguistic\nproperties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and\n0.03 for vocabulary richness. The findings reveal significant interaction\neffects between feedback tone and question difficulty, demonstrating the\ndynamic adaptation of AI-generated feedback within diverse educational\ncontexts. These insights contribute to the development of more personalized and\neffective AI-driven feedback mechanisms, highlighting the potential for\nimproved learning outcomes while underscoring the importance of ethical\nconsiderations in their design and deployment.",
      "tldr_zh": "本研究分析了AI生成的计算机科学多选题（MCQs）反馈的语言特性，包括可读性、词汇丰富度和不同挑战水平的适应性，旨在提升教育效果。研究使用Google的Gemini 1.5-flash模型生成反馈，并分析超过1200个MCQs的数据，涵盖三种难度（easy、medium、hard）和三种语气（supportive、neutral、challenging），计算了关键指标如Flesch-Kincaid Grade Level和词汇密度。基于RoBERTa的细调多任务学习（MTL）模型被训练来预测这些属性，取得了可读性的Mean Absolute Error (MAE)为2.0和词汇丰富度的MAE为0.03。结果显示反馈语气与问题难度间存在显著交互效应，这些见解有助于开发更个性化的AI驱动反馈机制，同时强调了设计和部署中的道德考虑。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper will be presented in the 9th Int. Conf. on Computer,\n  Software and Modeling (ICCSM 2025), Roma, Italy, 2025, July 3-5",
      "pdf_url": "http://arxiv.org/pdf/2504.21013v1",
      "published_date": "2025-04-19 09:20:52 UTC",
      "updated_date": "2025-04-19 09:20:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:46:55.676170"
    },
    {
      "arxiv_id": "2504.14232v1",
      "title": "Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment",
      "title_zh": "翻译失败",
      "authors": [
        "Antoun Yaacoub",
        "Jérôme Da-Rugna",
        "Zainab Assaghir"
      ],
      "abstract": "This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,\nan Artificial Intelligence (AI) driven plugin for automating Multiple-Choice\nQuestion (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured\nframework for categorizing educational objectives into hierarchical cognitive\nlevels. Our research investigates whether incorporating this taxonomy can\nimprove the alignment of AI-generated questions with specific cognitive\nobjectives. We developed a dataset of 3691 questions categorized according to\nBloom's levels and employed various classification models-Multinomial Logistic\nRegression, Naive Bayes, Linear Support Vector Classification (SVC), and a\nTransformer-based model (DistilBERT)-to evaluate their effectiveness in\ncategorizing questions. Our results indicate that higher Bloom's levels\ngenerally correlate with increased question length, Flesch-Kincaid Grade Level\n(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher\ncognitive demands. Multinomial Logistic Regression showed varying accuracy\nacross Bloom's levels, performing best for \"Knowledge\" and less accurately for\nhigher-order levels. Merging higher-level categories improved accuracy for\ncomplex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective\nclassification for lower levels but struggled with higher-order tasks.\nDistilBERT achieved the highest performance, significantly improving\nclassification of both lower and higher-order cognitive levels, achieving an\noverall validation accuracy of 91%. This study highlights the potential of\nintegrating Bloom's Taxonomy into AI-driven assessment tools and underscores\nthe advantages of advanced models like DistilBERT for enhancing educational\ncontent generation.",
      "tldr_zh": "这篇论文评估了将 Bloom's Taxonomy 整合到 AI 驱动的 OneClickQuiz 插件中，以提升多项选择题 (MCQ) 生成与教育认知目标的 alignment。研究团队开发了一个包含 3691 个问题的数据集，并使用 Multinomial Logistic Regression、Naive Bayes、Linear SVC 和 DistilBERT 等模型进行分类，结果显示更高 Bloom's 水平的问题在长度、Flesch-Kincaid Grade Level (FKGL) 和 Lexical Density (LD) 上更复杂。DistilBERT 模型表现出色，实现了 91% 的整体验证准确率，而传统模型在高阶认知任务上表现较差。该研究证明了在 AI 评估工具中整合 Bloom's Taxonomy 的潜力，并突出了高级 Transformer 模型在教育内容生成中的优势。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper was presented in the 17th Int. Conf. on Computer Science\n  and Information Technology (ICCSIT 2024), Dubai, United Arab Emirates, 2024,\n  Oct. 23-25. IT's now in production to be published in the International\n  Journal of Computer Theory and Engineering",
      "pdf_url": "http://arxiv.org/pdf/2504.14232v1",
      "published_date": "2025-04-19 09:03:39 UTC",
      "updated_date": "2025-04-19 09:03:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:47:08.274852"
    },
    {
      "arxiv_id": "2504.14223v1",
      "title": "SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Färber",
        "Parisa Aghdam",
        "Kyuri Im",
        "Mario Tawfelis",
        "Hardik Ghoshal"
      ],
      "abstract": "Text simplification is essential for making complex content accessible to\ndiverse audiences who face comprehension challenges. Yet, the limited\navailability of simplified materials creates significant barriers to personal\nand professional growth and hinders social inclusion. Although researchers have\nexplored various methods for automatic text simplification, none fully leverage\nlarge language models (LLMs) to offer tailored customization for different\ntarget groups and varying levels of simplicity. Moreover, despite its proven\nbenefits for both consumers and organizations, the well-established practice of\nplain language remains underutilized. In this paper, we\nhttps://simplifymytext.org, the first system designed to produce plain language\ncontent from multiple input formats, including typed text and file uploads,\nwith flexible customization options for diverse audiences. We employ GPT-4 and\nLlama-3 and evaluate outputs across multiple metrics. Overall, our work\ncontributes to research on automatic text simplification and highlights the\nimportance of tailored communication in promoting inclusivity.",
      "tldr_zh": "本文提出 SimplifyMyText 系统，这是一个基于大型语言模型(LLMs)的工具，旨在通过纯语言文本简化提升内容包容性，以帮助理解困难的受众。该系统支持多种输入格式（如文本和文件上传），并提供灵活定制选项，使用 GPT-4 和 Llama-3 模型进行优化。实验评估显示，该系统在自动文本简化方面表现出色，强调了定制通信在促进社会包容和个人发展中的重要作用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted at ECIR 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.14223v1",
      "published_date": "2025-04-19 08:07:53 UTC",
      "updated_date": "2025-04-19 08:07:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:47:18.411722"
    },
    {
      "arxiv_id": "2504.14209v2",
      "title": "Pets: General Pattern Assisted Architecture For Time Series Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangkai Ma",
        "Xiaobin Hong",
        "Wenzhong Li",
        "Sanglu Lu"
      ],
      "abstract": "Time series analysis has found widespread applications in areas such as\nweather forecasting, anomaly detection, and healthcare. However, real-world\nsequential data often exhibit a superimposed state of various fluctuation\npatterns, including hourly, daily, and monthly frequencies. Traditional\ndecomposition techniques struggle to effectively disentangle these multiple\nfluctuation patterns from the seasonal components, making time series analysis\nchallenging. Surpassing the existing multi-period decoupling paradigms, this\npaper introduces a novel perspective based on energy distribution within the\ntemporal-spectrum space. By adaptively quantifying observed sequences into\ncontinuous frequency band intervals, the proposed approach reconstructs\nfluctuation patterns across diverse periods without relying on domain-specific\nprior knowledge. Building upon this innovative strategy, we propose Pets, an\nenhanced architecture that is adaptable to arbitrary model structures. Pets\nintegrates a Fluctuation Pattern Assisted (FPA) module and a Context-Guided\nMixture of Predictors (MoP). The FPA module facilitates information fusion\namong diverse fluctuation patterns by capturing their dependencies and\nprogressively modeling these patterns as latent representations at each layer.\nMeanwhile, the MoP module leverages these compound pattern representations to\nguide and regulate the reconstruction of distinct fluctuations hierarchically.\nPets achieves state-of-the-art performance across various tasks, including\nforecasting, imputation, anomaly detection, and classification, while\ndemonstrating strong generalization and robustness.",
      "tldr_zh": "本论文针对时间序列分析中多种波动模式（如小时、日周期）叠加的挑战，提出了一种基于时间-频谱空间能量分布的新视角，通过自适应量化序列到连续频率带，实现波动模式的重建，而不依赖特定领域知识。  \n他们开发了 Pets 架构，该架构可适应任意模型结构，集成了 Fluctuation Pattern Assisted (FPA) 模块和 Context-Guided Mixture of Predictors (MoP) 模块，其中 FPA 捕获不同模式的依赖性并建模为潜在表示，MoP 则分层指导波动重建。  \n实验结果显示，Pets 在预测、插值、异常检测和分类等任务上达到了最先进性能，并展现出强大的泛化和鲁棒性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14209v2",
      "published_date": "2025-04-19 07:12:57 UTC",
      "updated_date": "2025-04-25 04:23:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:47:31.657875"
    },
    {
      "arxiv_id": "2504.14206v1",
      "title": "Decomposition-based multi-scale transformer framework for time series anomaly detection",
      "title_zh": "基于分解的多尺度 Transformer 框架用于时间序列异常检测",
      "authors": [
        "Wenxin Zhang",
        "Cuicui Luo"
      ],
      "abstract": "Time series anomaly detection is crucial for maintaining stable systems.\nExisting methods face two main challenges. First, it is difficult to directly\nmodel the dependencies of diverse and complex patterns within the sequences.\nSecond, many methods that optimize parameters using mean squared error struggle\nwith noise in the time series, leading to performance deterioration. To address\nthese challenges, we propose a transformer-based framework built on\ndecomposition (TransDe) for multivariate time series anomaly detection. The key\nidea is to combine the strengths of time series decomposition and transformers\nto effectively learn the complex patterns in normal time series data. A\nmulti-scale patch-based transformer architecture is proposed to exploit the\nrepresentative dependencies of each decomposed component of the time series.\nFurthermore, a contrastive learn paradigm based on patch operation is proposed,\nwhich leverages KL divergence to align the positive pairs, namely the pure\nrepresentations of normal patterns between different patch-level views. A novel\nasynchronous loss function with a stop-gradient strategy is further introduced\nto enhance the performance of TransDe effectively. It can avoid time-consuming\nand labor-intensive computation costs in the optimization process. Extensive\nexperiments on five public datasets are conducted and TransDe shows superiority\ncompared with twelve baselines in terms of F1 score. Our code is available at\nhttps://github.com/shaieesss/TransDe.",
      "tldr_zh": "本文提出TransDe框架，用于多变量时间序列异常检测，解决现有方法在建模多样复杂模式依赖性和噪声干扰方面的挑战。TransDe结合时间序列分解与多尺度基于补丁的Transformer架构，通过KL divergence的对比学习范式对齐正常模式表示，并引入异步损失函数以避免耗时优化。实验结果显示，在五个公共数据集上，该框架的F1分数优于12个基线方法，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14206v1",
      "published_date": "2025-04-19 06:47:38 UTC",
      "updated_date": "2025-04-19 06:47:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:47:42.588251"
    },
    {
      "arxiv_id": "2504.14205v2",
      "title": "Dual-channel Heterophilic Message Passing for Graph Fraud Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxin Zhang",
        "Jingxing Zhong",
        "Guangzhen Yao",
        "Renda Han",
        "Xiaojian Lin",
        "Zeyu Zhang",
        "Cuicui Luo"
      ],
      "abstract": "Fraudulent activities have significantly increased across various domains,\nsuch as e-commerce, online review platforms, and social networks, making fraud\ndetection a critical task. Spatial Graph Neural Networks (GNNs) have been\nsuccessfully applied to fraud detection tasks due to their strong inductive\nlearning capabilities. However, existing spatial GNN-based methods often\nenhance the graph structure by excluding heterophilic neighbors during message\npassing to align with the homophilic bias of GNNs. Unfortunately, this approach\ncan disrupt the original graph topology and increase uncertainty in\npredictions. To address these limitations, this paper proposes a novel\nframework, Dual-channel Heterophilic Message Passing (DHMP), for fraud\ndetection. DHMP leverages a heterophily separation module to divide the graph\ninto homophilic and heterophilic subgraphs, mitigating the low-pass inductive\nbias of traditional GNNs. It then applies shared weights to capture signals at\ndifferent frequencies independently and incorporates a customized sampling\nstrategy for training. This allows nodes to adaptively balance the\ncontributions of various signals based on their labels. Extensive experiments\non three real-world datasets demonstrate that DHMP outperforms existing\nmethods, highlighting the importance of separating signals with different\nfrequencies for improved fraud detection. The code is available at\nhttps://github.com/shaieesss/DHMP.",
      "tldr_zh": "本论文提出了一种新型框架 Dual-channel Heterophilic Message Passing (DHMP)，用于提升图欺诈检测的性能，以解决现有 Spatial Graph Neural Networks (GNNs) 方法在排除异质邻居时破坏图拓扑和增加预测不确定性的问题。DHMP 通过异质分离模块将图分成同质和异质子图，采用共享权重独立捕获不同频率的信号，并结合自定义采样策略，让节点根据标签自适应平衡信号贡献。实验在三个真实数据集上显示，DHMP 比现有方法表现出色，强调了分离不同频率信号对欺诈检测的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14205v2",
      "published_date": "2025-04-19 06:41:24 UTC",
      "updated_date": "2025-04-26 08:03:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:47:55.581730"
    },
    {
      "arxiv_id": "2504.14204v2",
      "title": "DConAD: A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxin Zhang",
        "Xiaojian Lin",
        "Wenjun Yu",
        "Guangzhen Yao",
        "jingxiang Zhong",
        "Yu Li",
        "Renda Han",
        "Songcheng Xu",
        "Hao Shi",
        "Cuicui Luo"
      ],
      "abstract": "Time series anomaly detection holds notable importance for risk\nidentification and fault detection across diverse application domains.\nUnsupervised learning methods have become popular because they have no\nrequirement for labels. However, due to the challenges posed by the\nmultiplicity of abnormal patterns, the sparsity of anomalies, and the growth of\ndata scale and complexity, these methods often fail to capture robust and\nrepresentative dependencies within the time series for identifying anomalies.\nTo enhance the ability of models to capture normal patterns of time series and\navoid the retrogression of modeling ability triggered by the dependencies on\nhigh-quality prior knowledge, we propose a differencing-based contrastive\nrepresentation learning framework for time series anomaly detection (DConAD).\nSpecifically, DConAD generates differential data to provide additional\ninformation about time series and utilizes transformer-based architecture to\ncapture spatiotemporal dependencies, which enhances the robustness of unbiased\nrepresentation learning ability. Furthermore, DConAD implements a novel KL\ndivergence-based contrastive learning paradigm that only uses positive samples\nto avoid deviation from reconstruction and deploys the stop-gradient strategy\nto compel convergence. Extensive experiments on five public datasets show the\nsuperiority and effectiveness of DConAD compared with nine baselines. The code\nis available at https://github.com/shaieesss/DConAD.",
      "tldr_zh": "该研究针对时间序列异常检测（Time Series Anomaly Detection）面临的异常模式多样性、异常稀疏性和数据复杂性等问题，提出了一种基于差分的对比表示学习框架（DConAD）。DConAD 通过生成差分数据并利用 Transformer 架构捕获时空依赖性，从而增强模型对正常模式的稳健表示学习能力，同时引入 KL divergence-based 对比学习范式，仅使用正样本并采用 stop-gradient 策略来避免重建偏差。实验在五个公共数据集上显示，DConAD 比九个基线方法表现出色，证明了其有效性和优越性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14204v2",
      "published_date": "2025-04-19 06:35:06 UTC",
      "updated_date": "2025-05-02 10:25:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:48:07.172113"
    },
    {
      "arxiv_id": "2504.14202v3",
      "title": "Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Zichuan Liu",
        "Liming Jiang",
        "Qing Yan",
        "Yumin Jia",
        "Hao Kang",
        "Xin Lu"
      ],
      "abstract": "We propose a novel framework for ID-preserving generation using a multi-modal\nencoding strategy rather than injecting identity features via adapters into\npre-trained models. Our method treats identity and text as a unified\nconditioning input. To achieve this, we introduce FaceCLIP, a multi-modal\nencoder that learns a joint embedding space for both identity and textual\nsemantics. Given a reference face and a text prompt, FaceCLIP produces a\nunified representation that encodes both identity and text, which conditions a\nbase diffusion model to generate images that are identity-consistent and\ntext-aligned. We also present a multi-modal alignment algorithm to train\nFaceCLIP, using a loss that aligns its joint representation with face, text,\nand image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image\nsynthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).\nCompared to prior methods, FaceCLIP-SDXL enables photorealistic portrait\ngeneration with better identity preservation and textual relevance. Extensive\nexperiments demonstrate its quantitative and qualitative superiority.",
      "tldr_zh": "我们提出了一种新框架，用于保持身份（ID）的图像合成，通过多模态编码策略将身份和文本作为统一的条件输入，而不是依赖适配器注入。核心组件是 FaceCLIP，一个多模态编码器，它学习身份和文本语义的联合嵌入空间，并通过多模态对齐算法训练，以确保生成的图像在身份一致性和文本对齐方面表现优异。接着，我们构建了 FaceCLIP-SDXL 管道，将 FaceCLIP 整合到 Stable Diffusion XL (SDXL) 中，实现更逼真的肖像生成。实验结果显示，该方法在定量和定性指标上均优于现有方法，证明了其在 ID-preserving 图像合成中的优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14202v3",
      "published_date": "2025-04-19 06:31:07 UTC",
      "updated_date": "2025-05-21 19:56:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:48:19.697687"
    },
    {
      "arxiv_id": "2504.14200v1",
      "title": "Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Huiyi Chen",
        "Jiawei Peng",
        "Kaihua Tang",
        "Xin Geng",
        "Xu Yang"
      ],
      "abstract": "In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to\nadapt to new tasks without parameter updates, using a few demonstrations from a\nlarge support set. However, selecting informative demonstrations leads to high\ncomputational and memory costs. While some methods explore selecting a small\nand representative coreset in the text classification, evaluating all support\nset samples remains costly, and discarded samples lead to unnecessary\ninformation loss. These methods may also be less effective for image\nclassification due to differences in feature spaces. Given these limitations,\nwe propose Key-based Coreset Optimization (KeCO), a novel framework that\nleverages untapped data to construct a compact and informative coreset. We\nintroduce visual features as keys within the coreset, which serve as the anchor\nfor identifying samples to be updated through different selection strategies.\nBy leveraging untapped samples from the support set, we update the keys of\nselected coreset samples, enabling the randomly initialized coreset to evolve\ninto a more informative coreset under low computational cost. Through extensive\nexperiments on coarse-grained and fine-grained image classification benchmarks,\nwe demonstrate that KeCO effectively enhances ICL performance for image\nclassification task, achieving an average improvement of more than 20\\%.\nNotably, we evaluate KeCO under a simulated online scenario, and the strong\nperformance in this scenario highlights the practical value of our framework\nfor resource-constrained real-world scenarios.",
      "tldr_zh": "这篇论文针对图像分类中的 In-Context Learning (ICL) 问题，提出 Key-based Coreset Optimization (KeCO) 框架，以从 Large Vision-Language Models (LVLMs) 的支持集中构建紧凑且信息丰富的 coreset。KeCO 通过将视觉特征作为 keys，并利用未使用的样本动态更新 coreset 样本，从而在低计算成本下提升演示选择的效率。实验在粗粒度和细粒度图像分类基准上证明，该框架使 ICL 性能平均提升超过 20%，并在模拟在线场景中表现出色，适用于资源受限的实际应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.14200v1",
      "published_date": "2025-04-19 06:26:23 UTC",
      "updated_date": "2025-04-19 06:26:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:48:31.749373"
    },
    {
      "arxiv_id": "2504.14191v2",
      "title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark",
      "title_zh": "AI Idea Bench 2025：AI 研究想法生成基准",
      "authors": [
        "Yansheng Qiu",
        "Haoquan Zhang",
        "Zhaopan Xu",
        "Ming Li",
        "Diping Song",
        "Zheng Wang",
        "Kaipeng Zhang"
      ],
      "abstract": "Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery.",
      "tldr_zh": "大型语言模型 (LLMs) 在生成新想法方面取得了显著成功，但现有评估忽略了知识泄露、缺乏开放式基准以及受提示设计限制的可行性分析等问题，导致难以发现突破性研究想法。论文提出 AI Idea Bench 2025 框架，用于定量评估和比较 LLMs 在 AI 研究领域的想法生成质量，该框架包括一个包含 3,495 篇 AI 论文及其灵感作品的数据集。评估方法从两个维度进行：与原论文内容的对齐，以及基于一般参考材料的判断。通过这个基准系统，可以有效比较想法生成技术，并推动科学发现的自动化。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14191v2",
      "published_date": "2025-04-19 05:35:45 UTC",
      "updated_date": "2025-05-16 11:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:48:44.227149"
    },
    {
      "arxiv_id": "2504.16120v1",
      "title": "A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content",
      "title_zh": "翻译失败",
      "authors": [
        "Chaima Njeh",
        "Haïfa Nakouri",
        "Fehmi Jaafar"
      ],
      "abstract": "Large Language Models (LLM) have made remarkable progress, but concerns about\npotential biases and harmful content persist. To address these apprehensions,\nwe introduce a practical solution for ensuring LLM's safe and ethical use. Our\nnovel approach focuses on a post-generation correction mechanism, the\nBART-Corrective Model, which adjusts generated content to ensure safety and\nsecurity. Unlike relying solely on model fine-tuning or prompt engineering, our\nmethod provides a robust data-centric alternative for mitigating harmful\ncontent. We demonstrate the effectiveness of our approach through experiments\non multiple toxic datasets, which show a significant reduction in mean toxicity\nand jail-breaking scores after integration. Specifically, our results show a\nreduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4,\na substantial reduction of 28% and 5% with PaLM2, a reduction of approximately\n26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it.\nThese results demonstrate the potential of our approach to improve the safety\nand security of LLM, making them more suitable for real-world applications.",
      "tldr_zh": "该研究提出了一种数据中心的方法，使用 BART-Corrective Model 作为后生成修正机制，来处理 Large Language Models (LLM) 中的潜在偏见和有害内容，确保模型的安全和道德使用。\n该方法不依赖模型微调或提示工程，而是通过调整生成的內容，显著降低了 toxicity 和 jail-breaking 风险。\n实验结果显示，在多个有毒数据集上，GPT-4 的平均 toxicity 和 jail-breaking 分数分别降低了 15% 和 21%，PaLM2 降低了 28% 和 5%，Mistral-7B 降低了约 26% 和 23%，Gemma-2b-it 降低了 11.1% 和 19%。\n这种方法为 LLM 在真实世界应用中提升安全性和可靠性提供了实用解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper is under revision in the International Journal of\n  Information Security",
      "pdf_url": "http://arxiv.org/pdf/2504.16120v1",
      "published_date": "2025-04-19 04:57:05 UTC",
      "updated_date": "2025-04-19 04:57:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:48:56.734840"
    },
    {
      "arxiv_id": "2504.14177v1",
      "title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward",
      "title_zh": "翻译失败",
      "authors": [
        "Li He",
        "He Zhao",
        "Stephen Wan",
        "Dadong Wang",
        "Lina Yao",
        "Tongliang Liu"
      ],
      "abstract": "Online AI Feedback (OAIF) presents a promising alternative to Reinforcement\nLearning from Human Feedback (RLHF) by utilizing online AI preference in\naligning language models (LLMs). However, the straightforward replacement of\nhumans with AI deprives LLMs from learning more fine-grained AI supervision\nbeyond binary signals. In this paper, we propose Direct Advantage Regression\n(DAR), a simple alignment algorithm using online AI reward to optimize policy\nimprovement through weighted supervised fine-tuning. As an RL-free approach,\nDAR maintains theoretical consistency with online RLHF pipelines while\nsignificantly reducing implementation complexity and improving learning\nefficiency. Our empirical results underscore that AI reward is a better form of\nAI supervision consistently achieving higher human-AI agreement as opposed to\nAI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show\nthat DAR outperforms both OAIF and online RLHF baselines.",
      "tldr_zh": "这篇论文提出 Direct Advantage Regression (DAR)，一种简单算法，使用 Online AI Feedback (OAIF) 中的在线 AI 奖励，通过加权监督微调来优化语言模型 (LLMs) 的策略改进，从而解决传统 RLHF 方法中 AI 监督过于粗糙的问题。DAR 作为无 RL 强化学习的方法，保持了与在线 RLHF 管道的理论一致性，同时降低了实现复杂性和提升了学习效率。实验结果表明，AI 奖励比 AI 偏好更能实现更高的人类-AI 一致性，且 DAR 在 GPT-4-Turbo 和 MT-bench 评估中优于 OAIF 和在线 RLHF 基线。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14177v1",
      "published_date": "2025-04-19 04:44:32 UTC",
      "updated_date": "2025-04-19 04:44:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:49:07.391444"
    },
    {
      "arxiv_id": "2504.14174v1",
      "title": "A Physics-guided Multimodal Transformer Path to Weather and Climate Sciences",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Han",
        "Hanting Chen",
        "Kai Han",
        "Xiaomeng Huang",
        "Yongyun Hu",
        "Wenjun Xu",
        "Dacheng Tao",
        "Ping Zhang"
      ],
      "abstract": "With the rapid development of machine learning in recent years, many problems\nin meteorology can now be addressed using AI models. In particular, data-driven\nalgorithms have significantly improved accuracy compared to traditional\nmethods. Meteorological data is often transformed into 2D images or 3D videos,\nwhich are then fed into AI models for learning. Additionally, these models\noften incorporate physical signals, such as temperature, pressure, and wind\nspeed, to further enhance accuracy and interpretability. In this paper, we\nreview several representative AI + Weather/Climate algorithms and propose a new\nparadigm where observational data from different perspectives, each with\ndistinct physical meanings, are treated as multimodal data and integrated via\ntransformers. Furthermore, key weather and climate knowledge can be\nincorporated through regularization techniques to further strengthen the\nmodel's capabilities. This new paradigm is versatile and can address a variety\nof tasks, offering strong generalizability. We also discuss future directions\nfor improving model accuracy and interpretability.",
      "tldr_zh": "该论文回顾了机器学习在气象学中的应用，强调数据驱动算法（如AI模型）在处理气象数据（如转化为2D图像或3D视频）时比传统方法更准确，并整合物理信号（如温度、压力和风速）以提升模型的准确性和可解释性。作者提出一个新范式，将不同视角的观测数据视为多模态数据，通过Transformer整合，并利用正则化技术融入关键天气和气候知识，从而增强模型的通用性和泛化能力。该范式适用于多种气象任务，并讨论了未来改进模型准确性和可解释性的潜在方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Perspective article",
      "pdf_url": "http://arxiv.org/pdf/2504.14174v1",
      "published_date": "2025-04-19 04:31:35 UTC",
      "updated_date": "2025-04-19 04:31:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:49:19.318729"
    },
    {
      "arxiv_id": "2504.14171v1",
      "title": "Adaptation Method for Misinformation Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Yangping Chen",
        "Weijie Shi",
        "Mengze Li",
        "Yue Cui",
        "Hao Chen",
        "Jia Zhu",
        "Jiajie Xu"
      ],
      "abstract": "Multimodal fake news detection plays a crucial role in combating online\nmisinformation. Unfortunately, effective detection methods rely on annotated\nlabels and encounter significant performance degradation when domain shifts\nexist between training (source) and test (target) data. To address the\nproblems, we propose ADOSE, an Active Domain Adaptation (ADA) framework for\nmultimodal fake news detection which actively annotates a small subset of\ntarget samples to improve detection performance. To identify various deceptive\npatterns in cross-domain settings, we design multiple expert classifiers to\nlearn dependencies across different modalities. These classifiers specifically\ntarget the distinct deception patterns exhibited in fake news, where two\nunimodal classifiers capture knowledge errors within individual modalities\nwhile one cross-modal classifier identifies semantic inconsistencies between\ntext and images. To reduce annotation costs from the target domain, we propose\na least-disagree uncertainty selector with a diversity calculator for selecting\nthe most informative samples. The selector leverages prediction disagreement\nbefore and after perturbations by multiple classifiers as an indicator of\nuncertain samples, whose deceptive patterns deviate most from source domains.\nIt further incorporates diversity scores derived from multi-view features to\nensure the chosen samples achieve maximal coverage of target domain features.\nThe extensive experiments on multiple datasets show that ADOSE outperforms\nexisting ADA methods by 2.72\\% $\\sim$ 14.02\\%, indicating the superiority of\nour model.",
      "tldr_zh": "该研究针对多模态假新闻检测中的领域偏移问题，提出ADOSE框架，这是一种Active Domain Adaptation (ADA)方法，通过主动标注目标域的一小部分样本来提升检测性能。ADOSE设计了多个专家分类器，包括两个单模态分类器捕捉单个模态内的知识错误，以及一个跨模态分类器识别文本和图像之间的语义不一致；同时，引入least-disagree uncertainty selector和diversity calculator来选择预测分歧最大且多样性高的样本，从而减少标注成本。实验结果显示，在多个数据集上，ADOSE比现有ADA方法提高了2.72%至14.02%，证明了其在跨域假新闻检测中的优越性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14171v1",
      "published_date": "2025-04-19 04:18:32 UTC",
      "updated_date": "2025-04-19 04:18:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:49:31.941209"
    },
    {
      "arxiv_id": "2504.14156v1",
      "title": "Breaking the Diffraction Barrier for Passive Sources: Parameter-Decoupled Superresolution Assisted by Physics-Informed Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Abdelali Sajia",
        "Bilal Benzimoun",
        "Pawan Khatiwada",
        "Guogan Zhao",
        "Xiao-Feng Qian"
      ],
      "abstract": "We present a parameter-decoupled superresolution framework for estimating\nsub-wavelength separations of passive two-point sources without requiring prior\nknowledge or control of the source. Our theoretical foundation circumvents the\nneed to estimate multiple challenging parameters such as partial coherence,\nbrightness imbalance, random relative phase, and photon statistics. A\nphysics-informed machine learning (ML) model (trained with a standard desktop\nworkstation), synergistically integrating this theory, further addresses\npractical imperfections including background noise, photon loss, and\ncentroid/orientation misalignment. The integrated parameter-decoupling\nsuperresolution method achieves resolution 14 and more times below the\ndiffraction limit (corresponding to ~ 13.5 nm in optical microscopy) on\nexperimentally generated realistic images with >82% fidelity, performance\nrivaling state-of-the-art techniques for actively controllable sources.\nCritically, our method's robustness against source parameter variability and\nsource-independent noises enables potential applications in realistic scenarios\nwhere source control is infeasible, such as astrophysical imaging, live-cell\nmicroscopy, and quantum metrology. This work bridges a critical gap between\ntheoretical superresolution limits and practical implementations for passive\nsystems.",
      "tldr_zh": "本研究提出了一种参数解耦超分辨（parameter-decoupled superresolution）框架，用于估计被动双点源的亚波长分离，而无需事先知识或源控制。该框架基于理论基础，绕过了估计部分相干性（partial coherence）、亮度不平衡和光子统计等挑战性参数，并结合physics-informed machine learning模型来处理实际问题，如背景噪声、光子损失和不对齐。实验结果显示，该方法在真实图像上实现了超过衍射极限（diffraction limit）14倍的分辨率（约13.5 nm），保真度超过82%，性能媲美主动源技术。该框架的鲁棒性使其适用于源不可控场景，如天体物理成像（astrophysical imaging）、活细胞显微镜和量子计量（quantum metrology），从而桥接了理论超分辨极限与被动系统的实际实现。",
      "categories": [
        "physics.optics",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "physics.optics",
      "comment": "12 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.14156v1",
      "published_date": "2025-04-19 03:03:42 UTC",
      "updated_date": "2025-04-19 03:03:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:49:44.108999"
    },
    {
      "arxiv_id": "2504.14154v1",
      "title": "SConU: Selective Conformal Uncertainty in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyuan Wang",
        "Qingni Wang",
        "Yue Zhang",
        "Tianlong Chen",
        "Xiaofeng Zhu",
        "Xiaoshuang Shi",
        "Kaidi Xu"
      ],
      "abstract": "As large language models are increasingly utilized in real-world\napplications, guarantees of task-specific metrics are essential for their\nreliable deployment. Previous studies have introduced various criteria of\nconformal uncertainty grounded in split conformal prediction, which offer\nuser-specified correctness coverage. However, existing frameworks often fail to\nidentify uncertainty data outliers that violate the exchangeability assumption,\nleading to unbounded miscoverage rates and unactionable prediction sets. In\nthis paper, we propose a novel approach termed Selective Conformal Uncertainty\n(SConU), which, for the first time, implements significance tests, by\ndeveloping two conformal p-values that are instrumental in determining whether\na given sample deviates from the uncertainty distribution of the calibration\nset at a specific manageable risk level. Our approach not only facilitates\nrigorous management of miscoverage rates across both single-domain and\ninterdisciplinary contexts, but also enhances the efficiency of predictions.\nFurthermore, we comprehensively analyze the components of the conformal\nprocedures, aiming to approximate conditional coverage, particularly in\nhigh-stakes question-answering tasks.",
      "tldr_zh": "该论文提出SConU（Selective Conformal Uncertainty），一种新方法，用于处理大型语言模型（Large Language Models）在实际应用中的不确定性问题，通过引入两个置信p值（conformal p-values）进行显著性测试，以识别样本是否偏离校准集的不确定性分布，从而管理特定风险水平下的错误覆盖率。SConU不仅适用于单领域和跨领域场景，还能提升预测效率，避免现有框架的交换性假设违例导致的无限错误覆盖。研究分析了置信程序的组件，旨在逼近条件覆盖（conditional coverage），尤其在高风险问答任务中提供更可靠的性能保证。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14154v1",
      "published_date": "2025-04-19 03:01:45 UTC",
      "updated_date": "2025-04-19 03:01:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:49:54.696473"
    },
    {
      "arxiv_id": "2504.14151v1",
      "title": "Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D",
      "title_zh": "翻译失败",
      "authors": [
        "Sergio Arnaud",
        "Paul McVay",
        "Ada Martin",
        "Arjun Majumdar",
        "Krishna Murthy Jatavallabhula",
        "Phillip Thomas",
        "Ruslan Partsey",
        "Daniel Dugas",
        "Abha Gejji",
        "Alexander Sax",
        "Vincent-Pierre Berges",
        "Mikael Henaff",
        "Ayush Jain",
        "Ang Cao",
        "Ishita Prasad",
        "Mrinal Kalakrishnan",
        "Michael Rabbat",
        "Nicolas Ballas",
        "Mido Assran",
        "Oleksandr Maksymets",
        "Aravind Rajeswaran",
        "Franziska Meier"
      ],
      "abstract": "We present LOCATE 3D, a model for localizing objects in 3D scenes from\nreferring expressions like \"the small coffee table between the sofa and the\nlamp.\" LOCATE 3D sets a new state-of-the-art on standard referential grounding\nbenchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D\noperates directly on sensor observation streams (posed RGB-D frames), enabling\nreal-world deployment on robots and AR devices. Key to our approach is 3D-JEPA,\na novel self-supervised learning (SSL) algorithm applicable to sensor point\nclouds. It takes as input a 3D pointcloud featurized using 2D foundation models\n(CLIP, DINO). Subsequently, masked prediction in latent space is employed as a\npretext task to aid the self-supervised learning of contextualized pointcloud\nfeatures. Once trained, the 3D-JEPA encoder is finetuned alongside a\nlanguage-conditioned decoder to jointly predict 3D masks and bounding boxes.\nAdditionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential\ngrounding, spanning multiple capture setups with over 130K annotations. This\nenables a systematic study of generalization capabilities as well as a stronger\nmodel.",
      "tldr_zh": "本研究提出 LOCATE 3D 模型，通过参照表达式（如“the small coffee table between the sofa and the lamp”）在 3D 场景中定位真实世界对象，实现了标准参照定位基准上的新状态-of-the-art，并展示了强大的泛化能力。核心方法是 3D-JEPA，一种新型自监督学习 (SSL) 算法，它以使用 CLIP 和 DINO 特征化的 3D 点云作为输入，通过掩码预测在潜在空间进行预训练，然后微调编码器与语言条件解码器来预测 3D 掩码和边界框，支持直接在 posed RGB-D 帧上部署。论文还引入了 LOCATE 3D DATASET，一个包含超过 130K 注解的多场景数据集，用于系统研究模型的泛化能力和性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO",
        "I.2.10; I.2.6; I.2.9; I.3.7; I.4.6; I.4.8"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14151v1",
      "published_date": "2025-04-19 02:51:24 UTC",
      "updated_date": "2025-04-19 02:51:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:50:09.160701"
    },
    {
      "arxiv_id": "2504.14150v2",
      "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations",
      "title_zh": "言行一致？ 测量大型语言模型解释的忠实度",
      "authors": [
        "Katie Matton",
        "Robert Osazuwa Ness",
        "John Guttag",
        "Emre Kıcıman"
      ],
      "abstract": "Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)解释的忠实度问题，提出一种新方法来量化解释是否准确反映模型的推理过程。\n他们首先定义忠实度为LLMs解释暗示的影响因素（如输入问题中的高阶概念）与实际影响因素的差异。\n方法涉及使用辅助LLMs修改输入概念以生成反事实样本，并采用Bayesian层次模型在示例和数据集层面量化概念的因果效应。\n实验结果显示，该方法能识别可解释的不忠实模式，例如在社会偏见任务中隐藏偏见影响，以及在医疗问答任务中提供误导性的证据声明。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "66 pages, 14 figures, 40 tables; ICLR 2025 (spotlight) camera ready",
      "pdf_url": "http://arxiv.org/pdf/2504.14150v2",
      "published_date": "2025-04-19 02:51:20 UTC",
      "updated_date": "2025-05-20 14:36:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:50:19.109141"
    },
    {
      "arxiv_id": "2504.14147v1",
      "title": "HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Jiakai Tang",
        "Jingsen Zhang",
        "Zihang Tian",
        "Xueyang Feng",
        "Lei Wang",
        "Xu Chen"
      ],
      "abstract": "Recent advancements in explainable recommendation have greatly bolstered user\nexperience by elucidating the decision-making rationale. However, the existing\nmethods actually fail to provide effective feedback signals for potentially\nbetter or worse generated explanations due to their reliance on traditional\nsupervised learning paradigms in sparse interaction data. To address these\nissues, we propose a novel human-like feedback-driven optimization framework.\nThis framework employs a dynamic interactive optimization mechanism for\nachieving human-centered explainable requirements without incurring high labor\ncosts. Specifically, we propose to utilize large language models (LLMs) as\nhuman simulators to predict human-like feedback for guiding the learning\nprocess. To enable the LLMs to deeply understand the task essence and meet\nuser's diverse personalized requirements, we introduce a human-induced\ncustomized reward scoring method, which helps stimulate the language\nunderstanding and logical reasoning capabilities of LLMs. Furthermore,\nconsidering the potential conflicts between different perspectives of\nexplanation quality, we introduce a principled Pareto optimization that\ntransforms the multi-perspective quality enhancement task into a\nmulti-objective optimization problem for improving explanation performance. At\nlast, to achieve efficient model training, we design an off-policy optimization\npipeline. By incorporating a replay buffer and addressing the data distribution\nbiases, we can effectively improve data utilization and enhance model\ngenerality. Extensive experiments on four datasets demonstrate the superiority\nof our approach.",
      "tldr_zh": "该论文提出 HF4Rec 框架，一种人类般的反馈驱动优化机制，旨在解决现有可解释推荐系统在稀疏交互数据中无法提供有效反馈信号的问题。该框架利用大型语言模型 (LLMs) 作为人类模拟器，通过 human-induced customized reward scoring 方法来指导学习过程，并引入 Pareto optimization 处理多视角解释质量冲突。最终，该框架采用 off-policy optimization pipeline 提升数据利用和模型泛化，在四个数据集上的实验显示了其显著优越性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14147v1",
      "published_date": "2025-04-19 02:46:10 UTC",
      "updated_date": "2025-04-19 02:46:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:50:34.128806"
    },
    {
      "arxiv_id": "2504.14145v1",
      "title": "PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline",
      "title_zh": "PipeWeaver：使用动态交错管道解决大型多",
      "authors": [
        "Zhenliang Xue",
        "Hanpeng Hu",
        "Xing Chen",
        "Yimin Jiang",
        "Yixin Song",
        "Zeyu Mi",
        "Yibo Zhu",
        "Daxin Jiang",
        "Yubin Xia",
        "Haibo Chen"
      ],
      "abstract": "Large multimodal models (LMMs) have demonstrated excellent capabilities in\nboth understanding and generation tasks with various modalities. While these\nmodels can accept flexible combinations of input data, their training\nefficiency suffers from two major issues: pipeline stage imbalance caused by\nheterogeneous model architectures, and training data dynamicity stemming from\nthe diversity of multimodal data.\n  In this paper, we present PipeWeaver, a dynamic pipeline scheduling framework\ndesigned for LMM training. The core of PipeWeaver is dynamic interleaved\npipeline, which searches for pipeline schedules dynamically tailored to current\ntraining batches. PipeWeaver addresses issues of LMM training with two\ntechniques: adaptive modality-aware partitioning and efficient pipeline\nschedule search within a hierarchical schedule space. Meanwhile, PipeWeaver\nutilizes SEMU (Step Emulator), a training simulator for multimodal models, for\naccurate performance estimations, accelerated by spatial-temporal subgraph\nreuse to improve search efficiency. Experiments show that PipeWeaver can\nenhance LMM training efficiency by up to 97.3% compared to state-of-the-art\nsystems, and demonstrate excellent adaptivity to LMM training's data\ndynamicity.",
      "tldr_zh": "该论文提出PipeWeaver，一种动态管道调度框架，用于解决大型多模态模型(LMMs)训练中的管道阶段不平衡和数据动态性问题。PipeWeaver的核心是dynamic interleaved pipeline，通过动态搜索针对当前训练批次的管道调度来优化效率。框架采用adaptive modality-aware partitioning和efficient pipeline schedule search等技术，并利用SEMU（Step Emulator）模拟器进行精确性能评估，以加速搜索过程。实验结果显示，PipeWeaver相较于现有系统可将LMMs训练效率提升高达97.3%，并表现出色的数据动态性适应能力。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14145v1",
      "published_date": "2025-04-19 02:30:11 UTC",
      "updated_date": "2025-04-19 02:30:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:50:43.329810"
    },
    {
      "arxiv_id": "2504.14139v2",
      "title": "ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification",
      "title_zh": "ThyroidEffi 1.0：一种高性能多类甲状腺癌分类的成本有效系统",
      "authors": [
        "Hai Pham-Ngoc",
        "De Nguyen-Van",
        "Dung Vu-Tien",
        "Phuong Le-Hong"
      ],
      "abstract": "Background: Automated classification of thyroid Fine Needle Aspiration Biopsy\n(FNAB) images faces challenges in limited data, inter-observer variability, and\ncomputational cost. Efficient, interpretable models are crucial for clinical\nsupport.\n  Objective: To develop and externally validate a deep learning system for\nmulti-class thyroid FNAB image classification into three key categories\ndirectly guiding post-biopsy treatment in Vietnam: Benign (Bethesda II),\nIndeterminate/Suspicious (BI, III, IV, V), and Malignant (BVI), achieving high\ndiagnostic accuracy with low computational overhead.\n  Methods: Our pipeline features: (1) YOLOv10 cell cluster detection for\ninformative sub-region extraction/noise reduction; (2) curriculum learning\nsequencing localized crops to full images for multi-scale capture; (3) adaptive\nlightweight EfficientNetB0 (4M parameters) balancing performance/efficiency;\nand (4) a Transformer-inspired module for multi-scale/multi-region analysis.\nExternal validation used 1,015 independent FNAB images.\n  Results: ThyroidEffi Basic achieved macro F1 of 89.19% and AUCs of 0.98\n(Benign), 0.95 (Indeterminate/Suspicious), 0.96 (Malignant) on the internal\ntest set. External validation yielded AUCs of 0.9495 (Benign), 0.7436\n(Indeterminate/Suspicious), 0.8396 (Malignant). ThyroidEffi Premium improved\nmacro F1 to 89.77%. Grad-CAM highlighted key diagnostic regions, confirming\ninterpretability. The system processed 1000 cases in 30 seconds, demonstrating\nfeasibility on widely accessible hardware.\n  Conclusions: This work demonstrates that high-accuracy, interpretable thyroid\nFNAB image classification is achievable with minimal computational demands.",
      "tldr_zh": "本文开发了ThyroidEffi 1.0，一种高性能、低成本的深度学习系统，用于将甲状腺细针穿刺活检 (FNAB) 图像分类为Benign (Bethesda II)、Indeterminate/Suspicious (BI, III, IV, V) 和Malignant (BVI) 三类，以指导后续治疗。系统采用YOLOv10进行细胞簇检测、Curriculum learning处理多尺度图像、轻量级EfficientNetB0模型 (4M参数) 平衡性能和效率，以及Transformer-inspired模块进行多区域分析。实验结果显示，ThyroidEffi Basic在内部测试集上达到macro F1 89.19%和AUCs高达0.98 (Benign)，外部验证也显示AUCs为0.9495 (Benign)，且系统能在30秒内处理1000个病例，证明了其在普通硬件上的可行性和可解释性 (通过Grad-CAM突出关键区域)。这项工作展示了高准确率甲状腺FNAB图像分类可以在最小计算需求下实现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2504.14139v2",
      "published_date": "2025-04-19 02:13:07 UTC",
      "updated_date": "2025-04-27 11:38:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:50:58.669075"
    },
    {
      "arxiv_id": "2504.14130v1",
      "title": "Personalized News Recommendation with Multi-granularity Candidate-aware User Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Qiang Li",
        "Xinze Lin",
        "Shenghao Lv",
        "Faliang Huang",
        "Xiangju Li"
      ],
      "abstract": "Matching candidate news with user interests is crucial for personalized news\nrecommendations. Most existing methods can represent a user's reading interests\nthrough a single profile based on clicked news, which may not fully capture the\ndiversity of user interests. Although some approaches incorporate candidate\nnews or topic information, they remain insufficient because they neglect the\nmulti-granularity relatedness between candidate news and user interests. To\naddress this, this study proposed a multi-granularity candidate-aware user\nmodeling framework that integrated user interest features across various levels\nof granularity. It consisted of two main components: candidate news encoding\nand user modeling. A news textual information extractor and a\nknowledge-enhanced entity information extractor can capture candidate news\nfeatures, and word-level, entity-level, and news-level candidate-aware\nmechanisms can provide a comprehensive representation of user interests.\nExtensive experiments on a real-world dataset demonstrated that the proposed\nmodel could significantly outperform baseline models.",
      "tldr_zh": "本文提出了一种多粒度候选感知用户建模（multi-granularity candidate-aware user modeling）框架，用于提升个性化新闻推荐（personalized news recommendation）的效果，通过整合候选新闻与用户兴趣的多粒度相关性来解决现有方法的局限性。该框架包括候选新闻编码组件（利用新闻文本信息提取器和知识增强实体信息提取器）和用户建模组件（通过词级别、实体级别和新闻级别机制全面表示用户兴趣）。实验在真实数据集上表明，该模型显著优于基线模型，在捕捉用户兴趣多样性方面表现出色。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14130v1",
      "published_date": "2025-04-19 01:14:55 UTC",
      "updated_date": "2025-04-19 01:14:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:51:07.729129"
    },
    {
      "arxiv_id": "2504.14128v4",
      "title": "TALES: Text Adventure Learning Environment Suite",
      "title_zh": "TALES：文本冒险学习环境套件",
      "authors": [
        "Christopher Zhang Cui",
        "Xingdi Yuan",
        "Ziang Xiao",
        "Prithviraj Ammanabrolu",
        "Marc-Alexandre Côté"
      ],
      "abstract": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tale-suite.",
      "tldr_zh": "该研究引入了 TALES（Text Adventure Learning Environment Suite），这是一个多样化的文本冒险游戏集合，用于挑战和评估大型语言模型（LLMs）的推理能力，特别是针对顺序决策和基于上下文历史的结构化推理。TALES 包含合成和人类编写的游戏，帮助测试 LLMs 在复杂任务中的表现。实验结果显示，即使是顶级开放和封闭权重模型，在人类设计的游戏上也仅达到 15% 的成功率，突显了 LLMs 推理能力的局限性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14128v4",
      "published_date": "2025-04-19 01:02:42 UTC",
      "updated_date": "2025-04-24 02:50:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:51:18.686042"
    },
    {
      "arxiv_id": "2504.14126v1",
      "title": "Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models",
      "title_zh": "大语言模型增强粒子群优化用于深度学习模型的超参数",
      "authors": [
        "Saad Hameed",
        "Basheer Qolomany",
        "Samir Brahim Belhaouari",
        "Mohamed Abdallah",
        "Junaid Qadir",
        "Ala Al-Fuqaha"
      ],
      "abstract": "Determining the ideal architecture for deep learning models, such as the\nnumber of layers and neurons, is a difficult and resource-intensive process\nthat frequently relies on human tuning or computationally costly optimization\napproaches. While Particle Swarm Optimization (PSO) and Large Language Models\n(LLMs) have been individually applied in optimization and deep learning, their\ncombined use for enhancing convergence in numerical optimization tasks remains\nunderexplored. Our work addresses this gap by integrating LLMs into PSO to\nreduce model evaluations and improve convergence for deep learning\nhyperparameter tuning. The proposed LLM-enhanced PSO method addresses the\ndifficulties of efficiency and convergence by using LLMs (particularly\nChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster\nachievement of target objectives. Our method speeds up search space exploration\nby substituting underperforming particle placements with best suggestions\noffered by LLMs. Comprehensive experiments across three scenarios -- (1)\noptimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)\nnetworks for time series regression, and (3) using Convolutional Neural\nNetworks (CNNs) for material classification -- show that the method\nsignificantly improves convergence rates and lowers computational costs.\nDepending on the application, computational complexity is lowered by 20% to 60%\ncompared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in\nmodel calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by\n60% for both regression and classification tasks, all while preserving accuracy\nand error rates. This groundbreaking methodology offers a very efficient and\neffective solution for optimizing deep learning models, leading to substantial\ncomputational performance improvements across a wide range of applications.",
      "tldr_zh": "该研究提出了一种将大型语言模型(LLMs)集成到粒子群优化(PSO)中的方法，用于深度学习模型的超参数调优，旨在提高效率并加速收敛。方法利用LLMs（如ChatGPT-3.5和Llama3）来优化PSO中的粒子位置建议，减少无效模型评估并加快搜索空间探索。在三个场景的实验中，包括Rastrigin函数优化、LSTM时间序列回归和CNN材料分类，该方法显著提升了收敛率，并将计算成本降低20%至60%，同时保持了准确性和错误率。总之，这一创新方法为深度学习超参数调优提供了高效解决方案，提升了整体计算性能。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14126v1",
      "published_date": "2025-04-19 00:54:59 UTC",
      "updated_date": "2025-04-19 00:54:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:51:31.128149"
    },
    {
      "arxiv_id": "2504.14125v1",
      "title": "Exploring Language Patterns of Prompts in Text-to-Image Generation and Their Impact on Visual Diversity",
      "title_zh": "探索文本到图像生成中提示的语言模式及其对视觉多样性的影响",
      "authors": [
        "Maria-Teresa De Rosa Palmini",
        "Eva Cetinic"
      ],
      "abstract": "Following the initial excitement, Text-to-Image (TTI) models are now being\nexamined more critically. While much of the discourse has focused on biases and\nstereotypes embedded in large-scale training datasets, the sociotechnical\ndynamics of user interactions with these models remain underexplored. This\nstudy examines the linguistic and semantic choices users make when crafting\nprompts and how these choices influence the diversity of generated outputs.\nAnalyzing over six million prompts from the Civiverse dataset on the CivitAI\nplatform across seven months, we categorize users into three groups based on\ntheir levels of linguistic experimentation: consistent repeaters, occasional\nrepeaters, and non-repeaters. Our findings reveal that as user participation\ngrows over time, prompt language becomes increasingly homogenized through the\nadoption of popular community tags and descriptors, with repeated prompts\ncomprising 40-50% of submissions. At the same time, semantic similarity and\ntopic preferences remain relatively stable, emphasizing common subjects and\nsurface aesthetics. Using Vendi scores to quantify visual diversity, we\ndemonstrate a clear correlation between lexical similarity in prompts and the\nvisual similarity of generated images, showing that linguistic repetition\nreinforces less diverse representations. These findings highlight the\nsignificant role of user-driven factors in shaping AI-generated imagery, beyond\ninherent model biases, and underscore the need for tools and practices that\nencourage greater linguistic and thematic experimentation within TTI systems to\nfoster more inclusive and diverse AI-generated content.",
      "tldr_zh": "本研究探讨了用户在 Text-to-Image (TTI) 模型中创建提示的语言模式，以及这些模式对生成图像视觉多样性的影响，通过分析超过六百万条来自 CivitAI 平台的 Civiverse 数据集提示。用户被分为一致重复者、偶尔重复者和非重复者，研究发现随着参与度增加，提示语言变得更同质化，采用流行标签和描述，导致重复提示占40-50%。使用 Vendi scores 量化视觉多样性，结果显示词汇相似性与图像视觉相似性高度相关，强化了图像多样性的减少。该研究强调用户驱动因素在塑造 AI 生成内容中的关键作用，并呼吁开发工具促进语言和主题实验，以实现更包容、多样化的输出。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14125v1",
      "published_date": "2025-04-19 00:51:38 UTC",
      "updated_date": "2025-04-19 00:51:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:51:44.406047"
    },
    {
      "arxiv_id": "2504.14123v1",
      "title": "Bayesian Principles Improve Prompt Learning In Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyu Kim",
        "Jongwoo Ko",
        "Mijung Park"
      ],
      "abstract": "Prompt learning is a popular fine-tuning method for vision-language models\ndue to its efficiency. It requires a small number of additional learnable\nparameters while significantly enhancing performance on target tasks. However,\nmost existing methods suffer from overfitting to fine-tuning data, yielding\npoor generalizability. To address this, we propose a new training objective\nfunction based on a Bayesian learning principle to balance adaptability and\ngeneralizability. We derive a prior over the logits, where the mean function is\nparameterized by the pre-trained model, while the posterior corresponds to the\nfine-tuned model. This objective establishes a balance by allowing the\nfine-tuned model to adapt to downstream tasks while remaining close to the\npre-trained model.",
      "tldr_zh": "提示学习是一种高效的视觉语言模型微调方法，但现有方法容易过拟合，导致泛化性差。为解决这一问题，本文提出了一种基于Bayesian学习原则的训练目标函数，在logits上定义先验（mean function由预训练模型参数化），而后验对应于微调模型，从而平衡模型的适应性和泛化性。该方法允许模型更好地适应下游任务，同时保持与预训练模型的接近度，提高了整体性能。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "AISTATS2025",
      "pdf_url": "http://arxiv.org/pdf/2504.14123v1",
      "published_date": "2025-04-19 00:48:09 UTC",
      "updated_date": "2025-04-19 00:48:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:51:55.232113"
    },
    {
      "arxiv_id": "2504.14119v1",
      "title": "CODECRASH: Stress Testing LLM Reasoning under Structural and Semantic Perturbations",
      "title_zh": "CODECRASH：结构和语义扰动下的LLM推理应力测试",
      "authors": [
        "Man Ho Lam",
        "Chaozheng Wang",
        "Jen-tse Huang",
        "Michael R. Lyu"
      ],
      "abstract": "Large Language Models (LLMs) have recently showcased strong capabilities in\ncode-related tasks, yet their robustness in code comprehension and reasoning\nremains underexplored. In this paper, we present CodeCrash, a unified benchmark\nthat evaluates LLM robustness under code structural and textual distraction\nperturbations, applied to two established benchmarks -- CRUXEval and\nLiveCodeBench -- across both input and output prediction tasks. We evaluate\nseventeen LLMs using direct and Chain-of-Thought inference to systematically\nanalyze their robustness, identify primary reasons for performance degradation,\nand highlight failure modes. Our findings reveal the fragility of LLMs under\nstructural noise and the inherent reliance on natural language cues,\nhighlighting critical robustness issues of LLMs in code execution and\nunderstanding. Additionally, we examine three Large Reasoning Models (LRMs) and\ndiscover the severe vulnerability of self-reflective reasoning mechanisms that\nlead to reasoning collapse. CodeCrash provides a principled framework for\nstress-testing LLMs in code understanding, offering actionable directions for\nfuture evaluation and benchmarking. The code of CodeCrash and the robustness\nleaderboard are publicly available at https://donaldlamnl.github.io/CodeCrash/ .",
      "tldr_zh": "本研究引入了CodeCrash基准，用于评估大型语言模型(LLMs)在代码理解和推理中的鲁棒性，针对代码结构和语义扰动进行压力测试，并应用于CRUXEval和LiveCodeBench基准。研究评估了17个LLMs，使用直接推理和Chain-of-Thought推理方法，揭示了LLMs在结构噪声下易受影响并过度依赖自然语言提示，导致性能下降。结果还显示Large Reasoning Models(LRMs)的自反性推理机制高度脆弱，可能引发推理崩溃；CodeCrash框架公开了代码和排行榜，为未来LLMs鲁棒性评估提供指导。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14119v1",
      "published_date": "2025-04-19 00:40:28 UTC",
      "updated_date": "2025-04-19 00:40:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:52:06.674108"
    },
    {
      "arxiv_id": "2504.14112v1",
      "title": "Longitudinal Study on Social and Emotional Use of AI Conversational Agent",
      "title_zh": "纵向研究：关于 AI 会话代理的社会和情感使用",
      "authors": [
        "Mohit Chandra",
        "Javier Hernandez",
        "Gonzalo Ramos",
        "Mahsa Ershadi",
        "Ananya Bhattacharjee",
        "Judith Amores",
        "Ebele Okoli",
        "Ann Paradiso",
        "Shahed Warreth",
        "Jina Suh"
      ],
      "abstract": "Development in digital technologies has continuously reshaped how individuals\nseek and receive social and emotional support. While online platforms and\ncommunities have long served this need, the increased integration of\ngeneral-purpose conversational AI into daily lives has introduced new dynamics\nin how support is provided and experienced. Existing research has highlighted\nboth benefits (e.g., wider access to well-being resources) and potential risks\n(e.g., over-reliance) of using AI for support seeking. In this five-week,\nexploratory study, we recruited 149 participants divided into two usage groups:\na baseline usage group (BU, n=60) that used the internet and AI as usual, and\nan active usage group (AU, n=89) encouraged to use one of four commercially\navailable AI tools (Microsoft Copilot, Google Gemini, PI AI, ChatGPT) for\nsocial and emotional interactions. Our analysis revealed significant increases\nin perceived attachment towards AI (32.99 percentage points), perceived AI\nempathy (25.8 p.p.), and motivation to use AI for entertainment (22.90 p.p.)\namong the AU group. We also observed that individual differences (e.g., gender\nidentity, prior AI usage) influenced perceptions of AI empathy and attachment.\nLastly, the AU group expressed higher comfort in seeking personal help,\nmanaging stress, obtaining social support, and talking about health with AI,\nindicating potential for broader emotional support while highlighting the need\nfor safeguards against problematic usage. Overall, our exploratory findings\nunderscore the importance of developing consumer-facing AI tools that support\nemotional well-being responsibly, while empowering users to understand the\nlimitations of these tools.",
      "tldr_zh": "这项纵向研究（Longitudinal Study）探讨了 AI 对话代理在社会和情感支持中的作用，招募了 149 名参与者分为两组：基线使用组（BU）和活跃使用组（AU），后者被鼓励在五周内使用 Microsoft Copilot、Google Gemini、PI AI 或 ChatGPT 进行互动。研究发现，AU 组的 AI 依恋感知增加了 32.99%，AI 移情感知增加了 25.8%，使用 AI 娱乐动机增加了 22.90%，且个体差异（如性别认同和先前使用）影响了这些感知。总体而言，该研究突显了 AI 工具在提升情感福祉的潜力，同时强调了开发负责任的 AI 以防范过度依赖和问题使用的重要性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "16 pages, 5 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.14112v1",
      "published_date": "2025-04-19 00:03:48 UTC",
      "updated_date": "2025-04-19 00:03:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:52:20.122627"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 62,
  "processed_papers_count": 62,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T14:52:38.916738"
}
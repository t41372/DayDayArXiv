[
  {
    "arxiv_id": "2411.09849v1",
    "title": "Self-Supervised Radio Pre-training: Toward Foundational Models for Spectrogram Learning",
    "authors": [
      "Ahmed Aboulfotouh",
      "Ashkan Eshaghbeigi",
      "Dimitrios Karslidis",
      "Hatem Abou-Zeid"
    ],
    "abstract": "Foundational deep learning (DL) models are general models, trained on large,\ndiverse, and unlabelled datasets, typically using self-supervised learning\ntechniques have led to significant advancements especially in natural language\nprocessing. These pretrained models can be fine-tuned for related downstream\ntasks, offering faster development and reduced training costs, while often\nachieving improved performance. In this work, we introduce Masked Spectrogram\nModeling, a novel self-supervised learning approach for pretraining\nfoundational DL models on radio signals. Adopting a Convolutional LSTM\narchitecture for efficient spatio-temporal processing, we pretrain the model\nwith an unlabelled radio dataset collected from over-the-air measurements.\nSubsequently, the pretrained model is fine-tuned for two downstream tasks:\nspectrum forecasting and segmentation. Experimental results demonstrate that\nour methodology achieves competitive performance in both forecasting accuracy\nand segmentation, validating its effectiveness for developing foundational\nradio models.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09849v1",
    "published_date": "2024-11-14 23:56:57 UTC",
    "updated_date": "2024-11-14 23:56:57 UTC"
  },
  {
    "arxiv_id": "2411.09844v1",
    "title": "Deep Autoencoders for Unsupervised Anomaly Detection in Wildfire Prediction",
    "authors": [
      "İrem Üstek",
      "Miguel Arana-Catania",
      "Alexander Farr",
      "Ivan Petrunin"
    ],
    "abstract": "Wildfires pose a significantly increasing hazard to global ecosystems due to\nthe climate crisis. Due to its complex nature, there is an urgent need for\ninnovative approaches to wildfire prediction, such as machine learning. This\nresearch took a unique approach, differentiating from classical supervised\nlearning, and addressed the gap in unsupervised wildfire prediction using\nautoencoders and clustering techniques for anomaly detection. Historical\nweather and normalised difference vegetation index datasets of Australia for\n2005 - 2021 were utilised. Two main unsupervised approaches were analysed. The\nfirst used a deep autoencoder to obtain latent features, which were then fed\ninto clustering models, isolation forest, local outlier factor and one-class\nSVM for anomaly detection. The second approach used a deep autoencoder to\nreconstruct the input data and use reconstruction errors to identify anomalies.\nLong Short-Term Memory (LSTM) autoencoders and fully connected (FC)\nautoencoders were employed in this part, both in an unsupervised way learning\nonly from nominal data. The FC autoencoder outperformed its counterparts,\nachieving an accuracy of 0.71, an F1-score of 0.74, and an MCC of 0.42. These\nfindings highlight the practicality of this method, as it effectively predicts\nwildfires in the absence of ground truth, utilising an unsupervised learning\ntechnique.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "33 pages, 18 figure, 16 tables. To appear in Earth and Space Science",
    "pdf_url": "http://arxiv.org/pdf/2411.09844v1",
    "published_date": "2024-11-14 23:19:55 UTC",
    "updated_date": "2024-11-14 23:19:55 UTC"
  },
  {
    "arxiv_id": "2411.09837v1",
    "title": "Real-time Adapting Routing (RAR): Improving Efficiency Through Continuous Learning in Software Powered by Layered Foundation Models",
    "authors": [
      "Kirill Vasilevski",
      "Dayi Lin",
      "Ahmed Hassan"
    ],
    "abstract": "To balance the quality and inference cost of a Foundation Model (FM, such as\nlarge language models (LLMs)) powered software, people often opt to train a\nrouting model that routes requests to FMs with different sizes and\ncapabilities. Existing routing models rely on learning the optimal routing\ndecision from carefully curated data, require complex computations to be\nupdated, and do not consider the potential evolution of weaker FMs. In this\npaper, we propose Real-time Adaptive Routing (RAR), an approach to continuously\nadapt FM routing decisions while using guided in-context learning to enhance\nthe capabilities of weaker FM. The goal is to reduce reliance on stronger, more\nexpensive FMs. We evaluate our approach on different subsets of the popular\nMMLU benchmark. Over time, our approach routes 50.2% fewer requests to\ncomputationally expensive models while maintaining around 90.5% of the general\nresponse quality. In addition, the guides generated from stronger models have\nshown intra-domain generalization and led to a better quality of responses\ncompared to an equivalent approach with a standalone weaker FM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09837v1",
    "published_date": "2024-11-14 23:02:30 UTC",
    "updated_date": "2024-11-14 23:02:30 UTC"
  },
  {
    "arxiv_id": "2411.09834v2",
    "title": "A Benchmark for Long-Form Medical Question Answering",
    "authors": [
      "Pedram Hosseini",
      "Jessica M. Sin",
      "Bing Ren",
      "Bryceton G. Thomas",
      "Elnaz Nouri",
      "Ali Farahanchi",
      "Saeed Hassanpour"
    ],
    "abstract": "There is a lack of benchmarks for evaluating large language models (LLMs) in\nlong-form medical question answering (QA). Most existing medical QA evaluation\nbenchmarks focus on automatic metrics and multiple-choice questions. While\nvaluable, these benchmarks fail to fully capture or assess the complexities of\nreal-world clinical applications where LLMs are being deployed. Furthermore,\nexisting studies on evaluating long-form answer generation in medical QA are\nprimarily closed-source, lacking access to human medical expert annotations,\nwhich makes it difficult to reproduce results and enhance existing baselines.\nIn this work, we introduce a new publicly available benchmark featuring\nreal-world consumer medical questions with long-form answer evaluations\nannotated by medical doctors. We performed pairwise comparisons of responses\nfrom various open and closed-source medical and general-purpose LLMs based on\ncriteria such as correctness, helpfulness, harmfulness, and bias. Additionally,\nwe performed a comprehensive LLM-as-a-judge analysis to study the alignment\nbetween human judgments and LLMs. Our preliminary results highlight the strong\npotential of open LLMs in medical QA compared to leading closed models. Code &\nData: https://github.com/lavita-ai/medical-eval-sphere",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AIM-FM: Advancements in Medical Foundation Models Workshop, 38th\n  Conference on Neural Information Processing Systems (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.09834v2",
    "published_date": "2024-11-14 22:54:38 UTC",
    "updated_date": "2024-11-19 21:04:38 UTC"
  },
  {
    "arxiv_id": "2412.02145v1",
    "title": "Effective Mitigations for Systemic Risks from General-Purpose AI",
    "authors": [
      "Risto Uuk",
      "Annemieke Brouwer",
      "Tim Schreier",
      "Noemi Dreksler",
      "Valeria Pulignano",
      "Rishi Bommasani"
    ],
    "abstract": "The systemic risks posed by general-purpose AI models are a growing concern,\nyet the effectiveness of mitigations remains underexplored. Previous research\nhas proposed frameworks for risk mitigation, but has left gaps in our\nunderstanding of the perceived effectiveness of measures for mitigating\nsystemic risks. Our study addresses this gap by evaluating how experts perceive\ndifferent mitigations that aim to reduce the systemic risks of general-purpose\nAI models. We surveyed 76 experts whose expertise spans AI safety; critical\ninfrastructure; democratic processes; chemical, biological, radiological, and\nnuclear risks (CBRN); and discrimination and bias. Among 27 mitigations\nidentified through a literature review, we find that a broad range of risk\nmitigation measures are perceived as effective in reducing various systemic\nrisks and technically feasible by domain experts. In particular, three\nmitigation measures stand out: safety incident reports and security information\nsharing, third-party pre-deployment model audits, and pre-deployment risk\nassessments. These measures show both the highest expert agreement ratings\n(>60\\%) across all four risk areas and are most frequently selected in experts'\npreferred combinations of measures (>40\\%). The surveyed experts highlighted\nthat external scrutiny, proactive evaluation and transparency are key\nprinciples for effective mitigation of systemic risks. We provide policy\nrecommendations for implementing the most promising measures, incorporating the\nqualitative contributions from experts. These insights should inform regulatory\nframeworks and industry practices for mitigating the systemic risks associated\nwith general-purpose AI.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "78 pages, 7 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.02145v1",
    "published_date": "2024-11-14 22:39:25 UTC",
    "updated_date": "2024-11-14 22:39:25 UTC"
  },
  {
    "arxiv_id": "2411.09822v1",
    "title": "A Self-Supervised Model for Multi-modal Stroke Risk Prediction",
    "authors": [
      "Camille Delgrange",
      "Olga Demler",
      "Samia Mora",
      "Bjoern Menze",
      "Ezequiel de la Rosa",
      "Neda Davoudi"
    ],
    "abstract": "Predicting stroke risk is a complex challenge that can be enhanced by\nintegrating diverse clinically available data modalities. This study introduces\na self-supervised multimodal framework that combines 3D brain imaging, clinical\ndata, and image-derived features to improve stroke risk prediction prior to\nonset. By leveraging large unannotated clinical datasets, the framework\ncaptures complementary and synergistic information across image and tabular\ndata modalities. Our approach is based on a contrastive learning framework that\ncouples contrastive language-image pretraining with an image-tabular matching\nmodule, to better align multimodal data representations in a shared latent\nspace. The model is trained on the UK Biobank, which includes structural brain\nMRI and clinical data. We benchmark its performance against state-of-the-art\nunimodal and multimodal methods using tabular, image, and image-tabular\ncombinations under diverse frozen and trainable model settings. The proposed\nmodel outperformed self-supervised tabular (image) methods by 2.6% (2.6%) in\nROC-AUC and by 3.3% (5.6%) in balanced accuracy. Additionally, it showed a 7.6%\nincrease in balanced accuracy compared to the best multimodal supervised model.\nThrough interpretable tools, our approach demonstrated better integration of\ntabular and image data, providing richer and more aligned embeddings.\nGradient-weighted Class Activation Mapping heatmaps further revealed activated\nbrain regions commonly associated in the literature with brain aging, stroke\nrisk, and clinical outcomes. This robust self-supervised multimodal framework\nsurpasses state-of-the-art methods for stroke risk prediction and offers a\nstrong foundation for future studies integrating diverse data modalities to\nadvance clinical predictive modelling.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as oral paper at AIM-FM workshop, Neurips 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.09822v1",
    "published_date": "2024-11-14 22:00:37 UTC",
    "updated_date": "2024-11-14 22:00:37 UTC"
  },
  {
    "arxiv_id": "2411.09820v1",
    "title": "WelQrate: Defining the Gold Standard in Small Molecule Drug Discovery Benchmarking",
    "authors": [
      "Yunchao Liu",
      "Ha Dong",
      "Xin Wang",
      "Rocco Moretti",
      "Yu Wang",
      "Zhaoqian Su",
      "Jiawei Gu",
      "Bobby Bodenheimer",
      "Charles David Weaver",
      "Jens Meiler",
      "Tyler Derr"
    ],
    "abstract": "While deep learning has revolutionized computer-aided drug discovery, the AI\ncommunity has predominantly focused on model innovation and placed less\nemphasis on establishing best benchmarking practices. We posit that without a\nsound model evaluation framework, the AI community's efforts cannot reach their\nfull potential, thereby slowing the progress and transfer of innovation into\nreal-world drug discovery. Thus, in this paper, we seek to establish a new gold\nstandard for small molecule drug discovery benchmarking, WelQrate.\nSpecifically, our contributions are threefold: WelQrate Dataset Collection - we\nintroduce a meticulously curated collection of 9 datasets spanning 5\ntherapeutic target classes. Our hierarchical curation pipelines, designed by\ndrug discovery experts, go beyond the primary high-throughput screen by\nleveraging additional confirmatory and counter screens along with rigorous\ndomain-driven preprocessing, such as Pan-Assay Interference Compounds (PAINS)\nfiltering, to ensure the high-quality data in the datasets; WelQrate Evaluation\nFramework - we propose a standardized model evaluation framework considering\nhigh-quality datasets, featurization, 3D conformation generation, evaluation\nmetrics, and data splits, which provides a reliable benchmarking for drug\ndiscovery experts conducting real-world virtual screening; Benchmarking - we\nevaluate model performance through various research questions using the\nWelQrate dataset collection, exploring the effects of different models, dataset\nquality, featurization methods, and data splitting strategies on the results.\nIn summary, we recommend adopting our proposed WelQrate as the gold standard in\nsmall molecule drug discovery benchmarking. The WelQrate dataset collection,\nalong with the curation codes, and experimental scripts are all publicly\navailable at WelQrate.org.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09820v1",
    "published_date": "2024-11-14 21:49:41 UTC",
    "updated_date": "2024-11-14 21:49:41 UTC"
  },
  {
    "arxiv_id": "2411.09807v1",
    "title": "Evaluating Loss Landscapes from a Topology Perspective",
    "authors": [
      "Tiankai Xie",
      "Caleb Geniesse",
      "Jiaqing Chen",
      "Yaoqing Yang",
      "Dmitriy Morozov",
      "Michael W. Mahoney",
      "Ross Maciejewski",
      "Gunther H. Weber"
    ],
    "abstract": "Characterizing the loss of a neural network with respect to model parameters,\ni.e., the loss landscape, can provide valuable insights into properties of that\nmodel. Various methods for visualizing loss landscapes have been proposed, but\nless emphasis has been placed on quantifying and extracting actionable and\nreproducible insights from these complex representations. Inspired by powerful\ntools from topological data analysis (TDA) for summarizing the structure of\nhigh-dimensional data, here we characterize the underlying shape (or topology)\nof loss landscapes, quantifying the topology to reveal new insights about\nneural networks. To relate our findings to the machine learning (ML)\nliterature, we compute simple performance metrics (e.g., accuracy, error), and\nwe characterize the local structure of loss landscapes using Hessian-based\nmetrics (e.g., largest eigenvalue, trace, eigenvalue spectral density).\nFollowing this approach, we study established models from image pattern\nrecognition (e.g., ResNets) and scientific ML (e.g., physics-informed neural\nnetworks), and we show how quantifying the shape of loss landscapes can provide\nnew insights into model performance and learning dynamics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09807v1",
    "published_date": "2024-11-14 20:46:26 UTC",
    "updated_date": "2024-11-14 20:46:26 UTC"
  },
  {
    "arxiv_id": "2411.09767v1",
    "title": "Deep Learning for Fetal Inflammatory Response Diagnosis in the Umbilical Cord",
    "authors": [
      "Marina A. Ayad",
      "Ramin Nateghi",
      "Abhishek Sharma",
      "Lawrence Chillrud",
      "Tilly Seesillapachai",
      "Lee A. D. Cooper",
      "Jeffery A. Goldstein"
    ],
    "abstract": "Inflammation of the umbilical cord can be seen as a result of ascending\nintrauterine infection or other inflammatory stimuli. Acute fetal inflammatory\nresponse (FIR) is characterized by infiltration of the umbilical cord by fetal\nneutrophils, and can be associated with neonatal sepsis or fetal inflammatory\nresponse syndrome. Recent advances in deep learning in digital pathology have\ndemonstrated favorable performance across a wide range of clinical tasks, such\nas diagnosis and prognosis. In this study we classified FIR from whole slide\nimages (WSI). We digitized 4100 histological slides of umbilical cord stained\nwith hematoxylin and eosin(H&E) and extracted placental diagnoses from the\nelectronic health record. We build models using attention-based whole slide\nlearning models. We compared strategies between features extracted by a model\n(ConvNeXtXLarge) pretrained on non-medical images (ImageNet), and one\npretrained using histopathology images (UNI). We trained multiple iterations of\neach model and combined them into an ensemble. The predictions from the\nensemble of models trained using UNI achieved an overall balanced accuracy of\n0.836 on the test dataset. In comparison, the ensembled predictions using\nConvNeXtXLarge had a lower balanced accuracy of 0.7209. Heatmaps generated from\ntop accuracy model appropriately highlighted arteritis in cases of FIR 2. In\nFIR 1, the highest performing model assigned high attention to areas of\nactivated-appearing stroma in Wharton's Jelly. However, other high-performing\nmodels assigned attention to umbilical vessels. We developed models for\ndiagnosis of FIR from placental histology images, helping reduce interobserver\nvariability among pathologists. Future work may examine the utility of these\nmodels for identifying infants at risk of systemic inflammatory response or\nearly onset neonatal sepsis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09767v1",
    "published_date": "2024-11-14 19:24:46 UTC",
    "updated_date": "2024-11-14 19:24:46 UTC"
  },
  {
    "arxiv_id": "2411.09702v1",
    "title": "On the Surprising Effectiveness of Attention Transfer for Vision Transformers",
    "authors": [
      "Alexander C. Li",
      "Yuandong Tian",
      "Beidi Chen",
      "Deepak Pathak",
      "Xinlei Chen"
    ],
    "abstract": "Conventional wisdom suggests that pre-training Vision Transformers (ViT)\nimproves downstream performance by learning useful representations. Is this\nactually true? We investigate this question and find that the features and\nrepresentations learned during pre-training are not essential. Surprisingly,\nusing only the attention patterns from pre-training (i.e., guiding how\ninformation flows between tokens) is sufficient for models to learn high\nquality features from scratch and achieve comparable downstream performance. We\nshow this by introducing a simple method called attention transfer, where only\nthe attention patterns from a pre-trained teacher ViT are transferred to a\nstudent, either by copying or distilling the attention maps. Since attention\ntransfer lets the student learn its own features, ensembling it with a\nfine-tuned teacher also further improves accuracy on ImageNet. We\nsystematically study various aspects of our findings on the sufficiency of\nattention maps, including distribution shift settings where they underperform\nfine-tuning. We hope our exploration provides a better understanding of what\npre-training accomplishes and leads to a useful alternative to the standard\npractice of fine-tuning",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024. Code:\n  https://github.com/alexlioralexli/attention-transfer",
    "pdf_url": "http://arxiv.org/pdf/2411.09702v1",
    "published_date": "2024-11-14 18:59:40 UTC",
    "updated_date": "2024-11-14 18:59:40 UTC"
  },
  {
    "arxiv_id": "2411.09689v1",
    "title": "LLM Hallucination Reasoning with Zero-shot Knowledge Test",
    "authors": [
      "Seongmin Lee",
      "Hsiang Hsu",
      "Chun-Fu Chen"
    ],
    "abstract": "LLM hallucination, where LLMs occasionally generate unfaithful text, poses\nsignificant challenges for their practical applications. Most existing\ndetection methods rely on external knowledge, LLM fine-tuning, or\nhallucination-labeled datasets, and they do not distinguish between different\ntypes of hallucinations, which are crucial for improving detection performance.\nWe introduce a new task, Hallucination Reasoning, which classifies\nLLM-generated text into one of three categories: aligned, misaligned, and\nfabricated. Our novel zero-shot method assesses whether LLM has enough\nknowledge about a given prompt and text. Our experiments conducted on new\ndatasets demonstrate the effectiveness of our method in hallucination reasoning\nand underscore its importance for enhancing detection performance.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.09689v1",
    "published_date": "2024-11-14 18:55:26 UTC",
    "updated_date": "2024-11-14 18:55:26 UTC"
  },
  {
    "arxiv_id": "2411.09683v1",
    "title": "Towards a Classification of Open-Source ML Models and Datasets for Software Engineering",
    "authors": [
      "Alexandra González",
      "Xavier Franch",
      "David Lo",
      "Silverio Martínez-Fernández"
    ],
    "abstract": "Background: Open-Source Pre-Trained Models (PTMs) and datasets provide\nextensive resources for various Machine Learning (ML) tasks, yet these\nresources lack a classification tailored to Software Engineering (SE) needs.\nAims: We apply an SE-oriented classification to PTMs and datasets on a popular\nopen-source ML repository, Hugging Face (HF), and analyze the evolution of PTMs\nover time. Method: We conducted a repository mining study. We started with a\nsystematically gathered database of PTMs and datasets from the HF API. Our\nselection was refined by analyzing model and dataset cards and metadata, such\nas tags, and confirming SE relevance using Gemini 1.5 Pro. All analyses are\nreplicable, with a publicly accessible replication package. Results: The most\ncommon SE task among PTMs and datasets is code generation, with a primary focus\non software development and limited attention to software management. Popular\nPTMs and datasets mainly target software development. Among ML tasks, text\ngeneration is the most common in SE PTMs and datasets. There has been a marked\nincrease in PTMs for SE since 2023 Q2. Conclusions: This study underscores the\nneed for broader task coverage to enhance the integration of ML within SE\npractices.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "5 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.09683v1",
    "published_date": "2024-11-14 18:52:05 UTC",
    "updated_date": "2024-11-14 18:52:05 UTC"
  },
  {
    "arxiv_id": "2411.10490v1",
    "title": "AI-Spectra: A Visual Dashboard for Model Multiplicity to Enhance Informed and Transparent Decision-Making",
    "authors": [
      "Gilles Eerlings",
      "Sebe Vanbrabant",
      "Jori Liesenborgs",
      "Gustavo Rovelo Ruiz",
      "Davy Vanacken",
      "Kris Luyten"
    ],
    "abstract": "We present an approach, AI-Spectra, to leverage model multiplicity for\ninteractive systems. Model multiplicity means using slightly different AI\nmodels yielding equally valid outcomes or predictions for the same task, thus\nrelying on many simultaneous \"expert advisors\" that can have different\nopinions. Dealing with multiple AI models that generate potentially divergent\nresults for the same task is challenging for users to deal with. It helps users\nunderstand and identify AI models are not always correct and might differ, but\nit can also result in an information overload when being confronted with\nmultiple results instead of one. AI-Spectra leverages model multiplicity by\nusing a visual dashboard designed for conveying what AI models generate which\nresults while minimizing the cognitive effort to detect consensus among models\nand what type of models might have different opinions. We use a custom\nadaptation of Chernoff faces for AI-Spectra; Chernoff Bots. This visualization\ntechnique lets users quickly interpret complex, multivariate model\nconfigurations and compare predictions across multiple models. Our design is\ninformed by building on established Human-AI Interaction guidelines and well\nknow practices in information visualization. We validated our approach through\na series of experiments training a wide variation of models with the MNIST\ndataset to perform number recognition. Our work contributes to the growing\ndiscourse on making AI systems more transparent, trustworthy, and effective\nthrough the strategic use of multiple models.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2; I.2.0; H.4.2; I.2.6"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted for publication in an LNCS Volume \"Engineering Interactive\n  Computer Systems - EICS 2024 - International Workshops and Doctoral\n  Consortium, Selected Papers\"",
    "pdf_url": "http://arxiv.org/pdf/2411.10490v1",
    "published_date": "2024-11-14 18:50:41 UTC",
    "updated_date": "2024-11-14 18:50:41 UTC"
  },
  {
    "arxiv_id": "2411.09678v2",
    "title": "NeuralDEM -- Real-time Simulation of Industrial Particulate Flows",
    "authors": [
      "Benedikt Alkin",
      "Tobias Kronlachner",
      "Samuele Papa",
      "Stefan Pirker",
      "Thomas Lichtenegger",
      "Johannes Brandstetter"
    ],
    "abstract": "Advancements in computing power have made it possible to numerically simulate\nlarge-scale fluid-mechanical and/or particulate systems, many of which are\nintegral to core industrial processes. Among the different numerical methods\navailable, the discrete element method (DEM) provides one of the most accurate\nrepresentations of a wide range of physical systems involving granular and\ndiscontinuous materials. Consequently, DEM has become a widely accepted\napproach for tackling engineering problems connected to granular flows and\npowder mechanics. Additionally, DEM can be integrated with grid-based\ncomputational fluid dynamics (CFD) methods, enabling the simulation of chemical\nprocesses taking place, e.g., in fluidized beds. However, DEM is\ncomputationally intensive because of the intrinsic multiscale nature of\nparticulate systems, restricting simulation duration or number of particles.\nTowards this end, NeuralDEM presents an end-to-end approach to replace slow\nnumerical DEM routines with fast, adaptable deep learning surrogates. NeuralDEM\nis capable of picturing long-term transport processes across different regimes\nusing macroscopic observables without any reference to microscopic model\nparameters. First, NeuralDEM treats the Lagrangian discretization of DEM as an\nunderlying continuous field, while simultaneously modeling macroscopic behavior\ndirectly as additional auxiliary fields. Second, NeuralDEM introduces\nmulti-branch neural operators scalable to real-time modeling of\nindustrially-sized scenarios - from slow and pseudo-steady to fast and\ntransient. Such scenarios have previously posed insurmountable challenges for\ndeep learning models. Notably, NeuralDEM faithfully models coupled CFD-DEM\nfluidized bed reactors of 160k CFD cells and 500k DEM particles for\ntrajectories of 28s. NeuralDEM will open many new doors to advanced engineering\nand much faster process cycles.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Project page: https://nx-ai.github.io/NeuralDEM/",
    "pdf_url": "http://arxiv.org/pdf/2411.09678v2",
    "published_date": "2024-11-14 18:44:31 UTC",
    "updated_date": "2025-02-27 10:26:19 UTC"
  },
  {
    "arxiv_id": "2411.10489v1",
    "title": "Biometrics in Extended Reality: A Review",
    "authors": [
      "Ayush Agarwal",
      "Raghavendra Ramachandra",
      "Sushma Venkatesh",
      "S. R. Mahadeva Prasanna"
    ],
    "abstract": "In the domain of Extended Reality (XR), particularly Virtual Reality (VR),\nextensive research has been devoted to harnessing this transformative\ntechnology in various real-world applications. However, a critical challenge\nthat must be addressed before unleashing the full potential of XR in practical\nscenarios is to ensure robust security and safeguard user privacy. This paper\npresents a systematic survey of the utility of biometric characteristics\napplied in the XR environment. To this end, we present a comprehensive overview\nof the different types of biometric modalities used for authentication and\nrepresentation of users in a virtual environment. We discuss different\nbiometric vulnerability gateways in general XR systems for the first time in\nthe literature along with taxonomy. A comprehensive discussion on generating\nand authenticating biometric-based photorealistic avatars in XR environments is\npresented with a stringent taxonomy. We also discuss the availability of\ndifferent datasets that are widely employed in evaluating biometric\nauthentication in XR environments together with performance evaluation metrics.\nFinally, we discuss the open challenges and potential future work that need to\nbe addressed in the field of biometrics in XR.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10489v1",
    "published_date": "2024-11-14 18:25:20 UTC",
    "updated_date": "2024-11-14 18:25:20 UTC"
  },
  {
    "arxiv_id": "2411.09648v1",
    "title": "Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable Medical Information",
    "authors": [
      "Ahan Bhatt",
      "Nandan Vaghela"
    ],
    "abstract": "This paper introduces Med-Bot, an AI-powered chatbot designed to provide\nusers with accurate and reliable medical information. Utilizing advanced\nlibraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq,\nMed-Bot is built to handle the complexities of natural language understanding\nin a healthcare context. The integration of llamaassisted data processing and\nAutoGPT-Q provides enhanced performance in processing and responding to queries\nbased on PDFs of medical literature, ensuring that users receive precise and\ntrustworthy information. This research details the methodologies employed in\ndeveloping Med-Bot and evaluates its effectiveness in disseminating healthcare\ninformation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "3 figures, 5 pages Keywords-LLM, AI-powered healthcare, Medical\n  chatbot, Context-based interaction, Llama-assisted data processing,\n  AutoGPT-Q, PyTorch, TensorFlow, Reliable medical information, Machine\n  learning in healthcare, Conversational AI",
    "pdf_url": "http://arxiv.org/pdf/2411.09648v1",
    "published_date": "2024-11-14 18:17:30 UTC",
    "updated_date": "2024-11-14 18:17:30 UTC"
  },
  {
    "arxiv_id": "2411.09642v1",
    "title": "On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse",
    "authors": [
      "Alkis Kalavasis",
      "Anay Mehrotra",
      "Grigoris Velegkas"
    ],
    "abstract": "Specifying all desirable properties of a language model is challenging, but\ncertain requirements seem essential. Given samples from an unknown language,\nthe trained model should produce valid strings not seen in training and be\nexpressive enough to capture the language's full richness. Otherwise,\noutputting invalid strings constitutes \"hallucination,\" and failing to capture\nthe full range leads to \"mode collapse.\" We ask if a language model can meet\nboth requirements.\n  We investigate this within a statistical language generation setting building\non Gold and Angluin. Here, the model receives random samples from a\ndistribution over an unknown language K, which belongs to a possibly infinite\ncollection of languages. The goal is to generate unseen strings from K. We say\nthe model generates from K with consistency and breadth if, as training size\nincreases, its output converges to all unseen strings in K.\n  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in\nlanguage generation are possible. We answer this negatively: for a large class\nof language models, including next-token prediction models, this is impossible\nfor most collections of candidate languages. This contrasts with [KM24]'s\nresult, showing consistent generation without breadth is possible for any\ncountable collection of languages. Our finding highlights that generation with\nbreadth fundamentally differs from generation without breadth.\n  As a byproduct, we establish near-tight bounds on the number of samples\nneeded for generation with or without breadth.\n  Finally, our results offer hope: consistent generation with breadth is\nachievable for any countable collection of languages when negative examples\n(strings outside K) are available alongside positive ones. This suggests that\npost-training feedback, which encodes negative examples, can be crucial in\nreducing hallucinations while limiting mode collapse.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Abstract shortened to fit arXiv limit",
    "pdf_url": "http://arxiv.org/pdf/2411.09642v1",
    "published_date": "2024-11-14 18:06:55 UTC",
    "updated_date": "2024-11-14 18:06:55 UTC"
  },
  {
    "arxiv_id": "2411.09627v2",
    "title": "One-Shot Manipulation Strategy Learning by Making Contact Analogies",
    "authors": [
      "Yuyao Liu",
      "Jiayuan Mao",
      "Joshua Tenenbaum",
      "Tomás Lozano-Pérez",
      "Leslie Pack Kaelbling"
    ],
    "abstract": "We present a novel approach, MAGIC (manipulation analogies for generalizable\nintelligent contacts), for one-shot learning of manipulation strategies with\nfast and extensive generalization to novel objects. By leveraging a reference\naction trajectory, MAGIC effectively identifies similar contact points and\nsequences of actions on novel objects to replicate a demonstrated strategy,\nsuch as using different hooks to retrieve distant objects of different shapes\nand sizes. Our method is based on a two-stage contact-point matching process\nthat combines global shape matching using pretrained neural features with local\ncurvature analysis to ensure precise and physically plausible contact points.\nWe experiment with three tasks including scooping, hanging, and hooking\nobjects. MAGIC demonstrates superior performance over existing methods,\nachieving significant improvements in runtime speed and generalization to\ndifferent object categories. Website: https://magic-2024.github.io/ .",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "ICRA 2025; CoRL LEAP Workshop, 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.09627v2",
    "published_date": "2024-11-14 17:54:43 UTC",
    "updated_date": "2025-03-23 08:02:09 UTC"
  },
  {
    "arxiv_id": "2411.09730v1",
    "title": "SureMap: Simultaneous Mean Estimation for Single-Task and Multi-Task Disaggregated Evaluation",
    "authors": [
      "Mikhail Khodak",
      "Lester Mackey",
      "Alexandra Chouldechova",
      "Miroslav Dudík"
    ],
    "abstract": "Disaggregated evaluation -- estimation of performance of a machine learning\nmodel on different subpopulations -- is a core task when assessing performance\nand group-fairness of AI systems. A key challenge is that evaluation data is\nscarce, and subpopulations arising from intersections of attributes (e.g.,\nrace, sex, age) are often tiny. Today, it is common for multiple clients to\nprocure the same AI model from a model developer, and the task of disaggregated\nevaluation is faced by each customer individually. This gives rise to what we\ncall the multi-task disaggregated evaluation problem, wherein multiple clients\nseek to conduct a disaggregated evaluation of a given model in their own data\nsetting (task). In this work we develop a disaggregated evaluation method\ncalled SureMap that has high estimation accuracy for both multi-task and\nsingle-task disaggregated evaluations of blackbox models. SureMap's efficiency\ngains come from (1) transforming the problem into structured simultaneous\nGaussian mean estimation and (2) incorporating external data, e.g., from the AI\nsystem creator or from their other clients. Our method combines maximum a\nposteriori (MAP) estimation using a well-chosen prior together with\ncross-validation-free tuning via Stein's unbiased risk estimate (SURE). We\nevaluate SureMap on disaggregated evaluation tasks in multiple domains,\nobserving significant accuracy improvements over several strong competitors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.09730v1",
    "published_date": "2024-11-14 17:53:35 UTC",
    "updated_date": "2024-11-14 17:53:35 UTC"
  },
  {
    "arxiv_id": "2411.09623v2",
    "title": "Vision-based Manipulation of Transparent Plastic Bags in Industrial Setups",
    "authors": [
      "F. Adetunji",
      "A. Karukayil",
      "P. Samant",
      "S. Shabana",
      "F. Varghese",
      "U. Upadhyay",
      "R. A. Yadav",
      "A. Partridge",
      "E. Pendleton",
      "R. Plant",
      "Y. Petillot",
      "M. Koskinopoulou"
    ],
    "abstract": "This paper addresses the challenges of vision-based manipulation for\nautonomous cutting and unpacking of transparent plastic bags in industrial\nsetups, aligning with the Industry 4.0 paradigm. Industry 4.0, driven by data,\nconnectivity, analytics, and robotics, promises enhanced accessibility and\nsustainability throughout the value chain. The integration of autonomous\nsystems, including collaborative robots (cobots), into industrial processes is\npivotal for efficiency and safety. The proposed solution employs advanced\nMachine Learning algorithms, particularly Convolutional Neural Networks (CNNs),\nto identify transparent plastic bags under varying lighting and background\nconditions. Tracking algorithms and depth sensing technologies are utilized for\n3D spatial awareness during pick and placement. The system addresses challenges\nin grasping and manipulation, considering optimal points, compliance control\nwith vacuum gripping technology, and real-time automation for safe interaction\nin dynamic environments. The system's successful testing and validation in the\nlab with the FRANKA robot arm, showcases its potential for widespread\nindustrial applications, while demonstrating effectiveness in automating the\nunpacking and cutting of transparent plastic bags for an 8-stack bulk-loader\nbased on specific requirements and rigorous testing.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09623v2",
    "published_date": "2024-11-14 17:47:54 UTC",
    "updated_date": "2024-11-19 10:15:56 UTC"
  },
  {
    "arxiv_id": "2411.09613v1",
    "title": "PTR: Precision-Driven Tool Recommendation for Large Language Models",
    "authors": [
      "Hang Gao",
      "Yongfeng Zhang"
    ],
    "abstract": "By augmenting Large Language Models (LLMs) with external tools, their\ncapacity to solve complex problems has been significantly enhanced. However,\ndespite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs\nwith a precise set of tools tailored to the specific task, considering both\nquantity and quality. Current tool retrieval methods primarily focus on\nrefining the ranking list of tools and directly packaging a fixed number of\ntop-ranked tools as the tool set. However, these approaches often fail to equip\nLLMs with the optimal set of tools prior to execution, since the optimal number\nof tools for different tasks could be different, resulting in inefficiencies\nsuch as redundant or unsuitable tools, which impede immediate access to the\nmost relevant tools. This paper addresses the challenge of recommending precise\ntoolsets for LLMs. We introduce the problem of tool recommendation, define its\nscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.\nPTR captures an initial, concise set of tools by leveraging historical tool\nbundle usage and dynamically adjusts the tool set by performing tool matching,\nculminating in a multi-view-based tool addition. Additionally, we present a new\ndataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness\nof tool recommendation for LLMs. We further validate our design choices through\ncomprehensive experiments, demonstrating promising accuracy across two open\nbenchmarks and our RecTools dataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09613v1",
    "published_date": "2024-11-14 17:33:36 UTC",
    "updated_date": "2024-11-14 17:33:36 UTC"
  },
  {
    "arxiv_id": "2411.09604v1",
    "title": "Local-Global Attention: An Adaptive Mechanism for Multi-Scale Feature Integration",
    "authors": [
      "Yifan Shao"
    ],
    "abstract": "In recent years, attention mechanisms have significantly enhanced the\nperformance of object detection by focusing on key feature information.\nHowever, prevalent methods still encounter difficulties in effectively\nbalancing local and global features. This imbalance hampers their ability to\ncapture both fine-grained details and broader contextual information-two\ncritical elements for achieving accurate object detection.To address these\nchallenges, we propose a novel attention mechanism, termed Local-Global\nAttention, which is designed to better integrate both local and global\ncontextual features. Specifically, our approach combines multi-scale\nconvolutions with positional encoding, enabling the model to focus on local\ndetails while concurrently considering the broader global context.\nAdditionally, we introduce a learnable parameters, which allow the model to\ndynamically adjust the relative importance of local and global attention,\ndepending on the specific requirements of the task, thereby optimizing feature\nrepresentations across multiple scales.We have thoroughly evaluated the\nLocal-Global Attention mechanism on several widely used object detection and\nclassification datasets. Our experimental results demonstrate that this\napproach significantly enhances the detection of objects at various scales,\nwith particularly strong performance on multi-class and small object detection\ntasks. In comparison to existing attention mechanisms, Local-Global Attention\nconsistently outperforms them across several key metrics, all while maintaining\ncomputational efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09604v1",
    "published_date": "2024-11-14 17:22:16 UTC",
    "updated_date": "2024-11-14 17:22:16 UTC"
  },
  {
    "arxiv_id": "2411.09601v1",
    "title": "Accelerating Knowledge Graph and Ontology Engineering with Large Language Models",
    "authors": [
      "Cogan Shimizu",
      "Pascal Hitzler"
    ],
    "abstract": "Large Language Models bear the promise of significant acceleration of key\nKnowledge Graph and Ontology Engineering tasks, including ontology modeling,\nextension, modification, population, alignment, as well as entity\ndisambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering\nas a new and coming area of research, and argue that modular approaches to\nontologies will be of central importance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09601v1",
    "published_date": "2024-11-14 17:21:02 UTC",
    "updated_date": "2024-11-14 17:21:02 UTC"
  },
  {
    "arxiv_id": "2411.09595v1",
    "title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models",
    "authors": [
      "Zhengyi Wang",
      "Jonathan Lorraine",
      "Yikai Wang",
      "Hang Su",
      "Jun Zhu",
      "Sanja Fidler",
      "Xiaohui Zeng"
    ],
    "abstract": "This work explores expanding the capabilities of large language models (LLMs)\npretrained on text to generate 3D meshes within a unified model. This offers\nkey advantages of (1) leveraging spatial knowledge already embedded in LLMs,\nderived from textual sources like 3D tutorials, and (2) enabling conversational\n3D generation and mesh understanding. A primary challenge is effectively\ntokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.\nTo address this, we introduce LLaMA-Mesh, a novel approach that represents the\nvertex coordinates and face definitions of 3D meshes as plain text, allowing\ndirect integration with LLMs without expanding the vocabulary. We construct a\nsupervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate\n3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs\nas required, and (3) understand and interpret 3D meshes. Our work is the first\nto demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge\nfor 3D mesh generation in a text-based format, effectively unifying the 3D and\ntext modalities. LLaMA-Mesh achieves mesh generation quality on par with models\ntrained from scratch while maintaining strong text generation performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "68T05",
      "I.3.5; I.2.10; I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/",
    "pdf_url": "http://arxiv.org/pdf/2411.09595v1",
    "published_date": "2024-11-14 17:08:23 UTC",
    "updated_date": "2024-11-14 17:08:23 UTC"
  },
  {
    "arxiv_id": "2411.09593v1",
    "title": "SMILE-UHURA Challenge -- Small Vessel Segmentation at Mesoscopic Scale from Ultra-High Resolution 7T Magnetic Resonance Angiograms",
    "authors": [
      "Soumick Chatterjee",
      "Hendrik Mattern",
      "Marc Dörner",
      "Alessandro Sciarra",
      "Florian Dubost",
      "Hannes Schnurre",
      "Rupali Khatun",
      "Chun-Chih Yu",
      "Tsung-Lin Hsieh",
      "Yi-Shan Tsai",
      "Yi-Zeng Fang",
      "Yung-Ching Yang",
      "Juinn-Dar Huang",
      "Marshall Xu",
      "Siyu Liu",
      "Fernanda L. Ribeiro",
      "Saskia Bollmann",
      "Karthikesh Varma Chintalapati",
      "Chethan Mysuru Radhakrishna",
      "Sri Chandana Hudukula Ram Kumara",
      "Raviteja Sutrave",
      "Abdul Qayyum",
      "Moona Mazher",
      "Imran Razzak",
      "Cristobal Rodero",
      "Steven Niederren",
      "Fengming Lin",
      "Yan Xia",
      "Jiacheng Wang",
      "Riyu Qiu",
      "Liansheng Wang",
      "Arya Yazdan Panah",
      "Rosana El Jurdi",
      "Guanghui Fu",
      "Janan Arslan",
      "Ghislain Vaillant",
      "Romain Valabregue",
      "Didier Dormont",
      "Bruno Stankoff",
      "Olivier Colliot",
      "Luisa Vargas",
      "Isai Daniel Chacón",
      "Ioannis Pitsiorlas",
      "Pablo Arbeláez",
      "Maria A. Zuluaga",
      "Stefanie Schreiber",
      "Oliver Speck",
      "Andreas Nürnberger"
    ],
    "abstract": "The human brain receives nutrients and oxygen through an intricate network of\nblood vessels. Pathology affecting small vessels, at the mesoscopic scale,\nrepresents a critical vulnerability within the cerebral blood supply and can\nlead to severe conditions, such as Cerebral Small Vessel Diseases. The advent\nof 7 Tesla MRI systems has enabled the acquisition of higher spatial resolution\nimages, making it possible to visualise such vessels in the brain. However, the\nlack of publicly available annotated datasets has impeded the development of\nrobust, machine learning-driven segmentation algorithms. To address this, the\nSMILE-UHURA challenge was organised. This challenge, held in conjunction with\nthe ISBI 2023, in Cartagena de Indias, Colombia, aimed to provide a platform\nfor researchers working on related topics. The SMILE-UHURA challenge addresses\nthe gap in publicly available annotated datasets by providing an annotated\ndataset of Time-of-Flight angiography acquired with 7T MRI. This dataset was\ncreated through a combination of automated pre-segmentation and extensive\nmanual refinement. In this manuscript, sixteen submitted methods and two\nbaseline methods are compared both quantitatively and qualitatively on two\ndifferent datasets: held-out test MRAs from the same dataset as the training\ndata (with labels kept secret) and a separate 7T ToF MRA dataset where both\ninput volumes and labels are kept secret. The results demonstrate that most of\nthe submitted deep learning methods, trained on the provided training dataset,\nachieved reliable segmentation performance. Dice scores reached up to 0.838\n$\\pm$ 0.066 and 0.716 $\\pm$ 0.125 on the respective datasets, with an average\nperformance of up to 0.804 $\\pm$ 0.15.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09593v1",
    "published_date": "2024-11-14 17:06:00 UTC",
    "updated_date": "2024-11-14 17:06:00 UTC"
  },
  {
    "arxiv_id": "2411.09590v1",
    "title": "Adopting RAG for LLM-Aided Future Vehicle Design",
    "authors": [
      "Vahid Zolfaghari",
      "Nenad Petrovic",
      "Fengjunjie Pan",
      "Krzysztof Lebioda",
      "Alois Knoll"
    ],
    "abstract": "In this paper, we explore the integration of Large Language Models (LLMs)\nwith Retrieval-Augmented Generation (RAG) to enhance automated design and\nsoftware development in the automotive industry. We present two case studies: a\nstandardization compliance chatbot and a design copilot, both utilizing RAG to\nprovide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,\nLLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and\nexecution time. Our results demonstrate that while GPT-4 offers superior\nperformance, LLAMA3 and Mistral also show promising capabilities for local\ndeployment, addressing data privacy concerns in automotive applications. This\nstudy highlights the potential of RAG-augmented LLMs in improving design\nworkflows and compliance in automotive engineering.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Conference paper accepted in IEEE FLLM 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.09590v1",
    "published_date": "2024-11-14 17:01:24 UTC",
    "updated_date": "2024-11-14 17:01:24 UTC"
  },
  {
    "arxiv_id": "2411.09580v1",
    "title": "Software Performance Engineering for Foundation Model-Powered Software (FMware)",
    "authors": [
      "Haoxiang Zhang",
      "Shi Chang",
      "Arthur Leung",
      "Kishanthan Thangarajah",
      "Boyuan Chen",
      "Hanan Lutfiyya",
      "Ahmed E. Hassan"
    ],
    "abstract": "The rise of Foundation Models (FMs) like Large Language Models (LLMs) is\nrevolutionizing software development. Despite the impressive prototypes,\ntransforming FMware into production-ready products demands complex engineering\nacross various domains. A critical but overlooked aspect is performance\nengineering, which aims at ensuring FMware meets performance goals such as\nthroughput and latency to avoid user dissatisfaction and financial loss. Often,\nperformance considerations are an afterthought, leading to costly optimization\nefforts post-deployment. FMware's high computational resource demands highlight\nthe need for efficient hardware use. Continuous performance engineering is\nessential to prevent degradation. This paper highlights the significance of\nSoftware Performance Engineering (SPE) in FMware, identifying four key\nchallenges: cognitive architecture design, communication protocols, tuning and\noptimization, and deployment. These challenges are based on literature surveys\nand experiences from developing an in-house FMware system. We discuss problems,\ncurrent practices, and innovative paths for the software engineering community.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09580v1",
    "published_date": "2024-11-14 16:42:19 UTC",
    "updated_date": "2024-11-14 16:42:19 UTC"
  },
  {
    "arxiv_id": "2411.09576v1",
    "title": "Automating Reformulation of Essence Specifications via Graph Rewriting",
    "authors": [
      "Ian Miguel",
      "András Z. Salamon",
      "Christopher Stone"
    ],
    "abstract": "Formulating an effective constraint model of a parameterised problem class is\ncrucial to the efficiency with which instances of the class can subsequently be\nsolved. It is difficult to know beforehand which of a set of candidate models\nwill perform best in practice. This paper presents a system that employs graph\nrewriting to reformulate an input model for improved performance automatically.\nBy situating our work in the Essence abstract constraint specification\nlanguage, we can use the structure in its high level variable types to trigger\nrewrites directly. We implement our system via rewrite rules expressed in the\nGraph Programs 2 language, applied to the abstract syntax tree of an input\nspecification. We show how to automatically translate the solution of the\nreformulated problem into a solution of the original problem for verification\nand presentation. We demonstrate the efficacy of our system with a detailed\ncase study.",
    "categories": [
      "cs.AI",
      "F.4.2"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at the PTHG 2024 workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.09576v1",
    "published_date": "2024-11-14 16:35:15 UTC",
    "updated_date": "2024-11-14 16:35:15 UTC"
  },
  {
    "arxiv_id": "2411.09547v2",
    "title": "Piecing It All Together: Verifying Multi-Hop Multimodal Claims",
    "authors": [
      "Haoran Wang",
      "Aman Rangapur",
      "Xiongxiao Xu",
      "Yueqing Liang",
      "Haroon Gharwi",
      "Carl Yang",
      "Kai Shu"
    ],
    "abstract": "Existing claim verification datasets often do not require systems to perform\ncomplex reasoning or effectively interpret multimodal evidence. To address\nthis, we introduce a new task: multi-hop multimodal claim verification. This\ntask challenges models to reason over multiple pieces of evidence from diverse\nsources, including text, images, and tables, and determine whether the combined\nmultimodal evidence supports or refutes a given claim. To study this task, we\nconstruct MMCV, a large-scale dataset comprising 15k multi-hop claims paired\nwith multimodal evidence, generated and refined using large language models,\nwith additional input from human feedback. We show that MMCV is challenging\neven for the latest state-of-the-art multimodal large language models,\nespecially as the number of reasoning hops increases. Additionally, we\nestablish a human performance benchmark on a subset of MMCV. We hope this\ndataset and its evaluation task will encourage future research in multimodal\nmulti-hop claim verification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.09547v2",
    "published_date": "2024-11-14 16:01:33 UTC",
    "updated_date": "2024-12-12 19:23:28 UTC"
  },
  {
    "arxiv_id": "2411.09543v2",
    "title": "OpenGeMM: A High-Utilization GeMM Accelerator Generator with Lightweight RISC-V Control and Tight Memory Coupling",
    "authors": [
      "Xiaoling Yi",
      "Ryan Antonio",
      "Joren Dumoulin",
      "Jiacong Sun",
      "Josse Van Delm",
      "Guilherme Paim",
      "Marian Verhelst"
    ],
    "abstract": "Deep neural networks (DNNs) face significant challenges when deployed on\nresource-constrained extreme edge devices due to their computational and\ndata-intensive nature. While standalone accelerators tailored for specific\napplication scenarios suffer from inflexible control and limited\nprogrammability, generic hardware acceleration platforms coupled with RISC-V\nCPUs can enable high reusability and flexibility, yet typically at the expense\nof system level efficiency and low utilization. To fill this gap, we propose\nOpenGeMM, an open-source acceleration platform, jointly demonstrating high\nefficiency and utilization, as well as ease of configurability and\nprogrammability. OpenGeMM encompasses a parameterized Chisel-coded GeMM\naccelerator, a lightweight RISC-V processor, and a tightly coupled multi-banked\nscratchpad memory. The GeMM core utilization and system efficiency are boosted\nthrough three mechanisms: configuration pre-loading, input pre-fetching with\noutput buffering, and programmable strided memory access. Experimental results\nshow that OpenGeMM can consistently achieve hardware utilization ranging from\n81.89% to 99.34% across diverse CNN and Transformer workloads. Compared to the\nSotA open-source Gemmini accelerator, OpenGeMM demonstrates a 3.58x to 16.40x\nspeedup on normalized throughput across a wide variety ofGeMM workloads, while\nachieving 4.68 TOPS/W system efficiency.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09543v2",
    "published_date": "2024-11-14 15:58:46 UTC",
    "updated_date": "2024-11-21 14:10:08 UTC"
  },
  {
    "arxiv_id": "2411.09540v2",
    "title": "Prompting the Unseen: Detecting Hidden Backdoors in Black-Box Models",
    "authors": [
      "Zi-Xuan Huang",
      "Jia-Wei Chen",
      "Zhi-Peng Zhang",
      "Chia-Mu Yu"
    ],
    "abstract": "Visual prompting (VP) is a new technique that adapts well-trained frozen\nmodels for source domain tasks to target domain tasks. This study examines VP's\nbenefits for black-box model-level backdoor detection. The visual prompt in VP\nmaps class subspaces between source and target domains. We identify a\nmisalignment, termed class subspace inconsistency, between clean and poisoned\ndatasets. Based on this, we introduce \\textsc{BProm}, a black-box model-level\ndetection method to identify backdoors in suspicious models, if any.\n\\textsc{BProm} leverages the low classification accuracy of prompted models\nwhen backdoors are present. Extensive experiments confirm \\textsc{BProm}'s\neffectiveness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted by IEEE/IFIP DSN 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.09540v2",
    "published_date": "2024-11-14 15:56:11 UTC",
    "updated_date": "2025-04-07 08:55:40 UTC"
  },
  {
    "arxiv_id": "2411.09523v1",
    "title": "Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents",
    "authors": [
      "Yuyou Gan",
      "Yong Yang",
      "Zhe Ma",
      "Ping He",
      "Rui Zeng",
      "Yiming Wang",
      "Qingming Li",
      "Chunyi Zhou",
      "Songze Li",
      "Ting Wang",
      "Yunjun Gao",
      "Yingcai Wu",
      "Shouling Ji"
    ],
    "abstract": "With the continuous development of large language models (LLMs),\ntransformer-based models have made groundbreaking advances in numerous natural\nlanguage processing (NLP) tasks, leading to the emergence of a series of agents\nthat use LLMs as their control hub. While LLMs have achieved success in various\ntasks, they face numerous security and privacy threats, which become even more\nsevere in the agent scenarios. To enhance the reliability of LLM-based\napplications, a range of research has emerged to assess and mitigate these\nrisks from different perspectives.\n  To help researchers gain a comprehensive understanding of various risks, this\nsurvey collects and analyzes the different threats faced by these agents. To\naddress the challenges posed by previous taxonomies in handling cross-module\nand cross-stage threats, we propose a novel taxonomy framework based on the\nsources and impacts. Additionally, we identify six key features of LLM-based\nagents, based on which we summarize the current research progress and analyze\ntheir limitations. Subsequently, we select four representative agents as case\nstudies to analyze the risks they may face in practical use. Finally, based on\nthe aforementioned analyses, we propose future research directions from the\nperspectives of data, methodology, and policy, respectively.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09523v1",
    "published_date": "2024-11-14 15:40:04 UTC",
    "updated_date": "2024-11-14 15:40:04 UTC"
  },
  {
    "arxiv_id": "2411.09510v2",
    "title": "Communication Compression for Tensor Parallel LLM Inference",
    "authors": [
      "Jan Hansen-Palmus",
      "Michael Truong Le",
      "Oliver Hausdörfer",
      "Alok Verma"
    ],
    "abstract": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09510v2",
    "published_date": "2024-11-14 15:19:01 UTC",
    "updated_date": "2024-11-15 10:47:37 UTC"
  },
  {
    "arxiv_id": "2411.09507v1",
    "title": "Toward a Cohesive AI and Simulation Software Ecosystem for Scientific Innovation",
    "authors": [
      "Michael A. Heroux",
      "Sameer Shende",
      "Lois Curfman McInnes",
      "Todd Gamblin",
      "James M. Willenbring"
    ],
    "abstract": "In this paper, we discuss the need for an integrated software stack that\nunites artificial intelligence (AI) and modeling and simulation (ModSim) tools\nto advance scientific discovery. The authors advocate for a unified AI/ModSim\nsoftware ecosystem that ensures compatibility across a wide range of software\non diverse high-performance computing systems, promoting ease of deployment,\nversion management, and binary distribution. Key challenges highlighted include\nbalancing the distinct needs of AI and ModSim, especially in terms of software\nbuild practices, dependency management, and compatibility. The document\nunderscores the importance of continuous integration, community-driven\nstewardship, and collaboration with the Department of Energy (DOE) to develop a\nportable and cohesive scientific software ecosystem. Recommendations focus on\nsupporting standardized environments through initiatives like the Extreme-scale\nScientific Software Stack (E4S) and Spack to foster interdisciplinary\ninnovation and facilitate new scientific advancements.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "5 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.09507v1",
    "published_date": "2024-11-14 15:17:50 UTC",
    "updated_date": "2024-11-14 15:17:50 UTC"
  },
  {
    "arxiv_id": "2411.09492v1",
    "title": "MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in LLMs",
    "authors": [
      "Mengyuan Zhang",
      "Ruihui Wang",
      "Bo Xia",
      "Yuan Sun",
      "Xiaobing Zhao"
    ],
    "abstract": "Large language models (LLMs) excel in high-resource languages but face\nnotable challenges in low-resource languages like Mongolian. This paper\naddresses these challenges by categorizing capabilities into language abilities\n(syntax and semantics) and cognitive abilities (knowledge and reasoning). To\nsystematically evaluate these areas, we developed MM-Eval, a specialized\ndataset based on Modern Mongolian Language Textbook I and enriched with WebQSP\nand MGSM datasets.\n  Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,\nLlama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models\nperformed better on syntactic tasks than semantic tasks, highlighting a gap in\ndeeper language understanding; and 2) knowledge tasks showed a moderate\ndecline, suggesting that models can transfer general knowledge from\nhigh-resource to low-resource contexts.\n  The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,\nand 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in\nlow-resource languages like Mongolian. The dataset is available at\nhttps://github.com/joenahm/MM-Eval.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09492v1",
    "published_date": "2024-11-14 14:58:38 UTC",
    "updated_date": "2024-11-14 14:58:38 UTC"
  },
  {
    "arxiv_id": "2411.10488v1",
    "title": "The Future of Skill: What Is It to Be Skilled at Work?",
    "authors": [
      "Axel Niklasson",
      "Sean Rintel",
      "Stephann Makri",
      "Alex Taylor"
    ],
    "abstract": "In this short paper, we introduce work that is aiming to purposefully venture\ninto this mesh of questions from a different starting point. Interjecting into\nthe conversation, we want to ask: 'What is it to be skilled at work?' Building\non work from scholars like Tim Ingold, and strands of longstanding research in\nworkplace studies and CSCW, our interest is in turning the attention to the\nactive work of 'being good', or 'being skilled', at what we as workers do. As\nwe see it, skill provides a counterpoint to the version of intelligence that\nappears to be easily blackboxed in systems like Slack, and that ultimately\nreduces much of what people do to work well together. To put it slightly\ndifferently, skill - as we will argue below - gives us a way into thinking\nabout work as a much more entangled endeavour, unfolding through multiple and\ninterweaving sets of practices, places, tools and collaborations. In this vein,\ndesigning for the future of work seems to be about much more than where work is\ndone or how we might bolt on discrete containers of intelligence. More fruitful\nwould be attending to how we succeed in threading so many entities together to\ndo our jobs well - in 'coming to be skilled'.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10488v1",
    "published_date": "2024-11-14 14:39:03 UTC",
    "updated_date": "2024-11-14 14:39:03 UTC"
  },
  {
    "arxiv_id": "2411.09475v1",
    "title": "ResidualDroppath: Enhancing Feature Reuse over Residual Connections",
    "authors": [
      "Sejik Park"
    ],
    "abstract": "Residual connections are one of the most important components in neural\nnetwork architectures for mitigating the vanishing gradient problem and\nfacilitating the training of much deeper networks. One possible explanation for\nhow residual connections aid deeper network training is by promoting feature\nreuse. However, we identify and analyze the limitations of feature reuse with\nvanilla residual connections. To address these limitations, we propose\nmodifications in training methods. Specifically, we provide an additional\nopportunity for the model to learn feature reuse with residual connections\nthrough two types of iterations during training. The first type of iteration\ninvolves using droppath, which enforces feature reuse by randomly dropping a\nsubset of layers. The second type of iteration focuses on training the dropped\nparts of the model while freezing the undropped parts. As a result, the dropped\nparts learn in a way that encourages feature reuse, as the model relies on the\nundropped parts with feature reuse in mind. Overall, we demonstrated\nperformance improvements in models with residual connections for image\nclassification in certain cases.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09475v1",
    "published_date": "2024-11-14 14:31:30 UTC",
    "updated_date": "2024-11-14 14:31:30 UTC"
  },
  {
    "arxiv_id": "2411.09471v1",
    "title": "Renal Cell Carcinoma subtyping: learning from multi-resolution localization",
    "authors": [
      "Mohamad Mohamad",
      "Francesco Ponzio",
      "Santa Di Cataldo",
      "Damien Ambrosetti",
      "Xavier Descombes"
    ],
    "abstract": "Renal Cell Carcinoma is typically asymptomatic at the early stages for many\npatients. This leads to a late diagnosis of the tumor, where the curability\nlikelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high,\nwith respect to its incidence rate. To increase the survival chance, a fast and\ncorrect categorization of the tumor subtype is paramount. Nowadays,\ncomputerized methods, based on artificial intelligence, represent an\ninteresting opportunity to improve the productivity and the objectivity of the\nmicroscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their\nexploitation is hampered by the paucity of annotated dataset, essential for a\nproficient training of supervised machine learning technologies. This study\nsets out to investigate a novel self supervised training strategy for machine\nlearning diagnostic tools, based on the multi-resolution nature of the\nhistological samples. We aim at reducing the need of annotated dataset, without\nsignificantly reducing the accuracy of the tool. We demonstrate the\nclassification capability of our tool on a whole slide imaging dataset for\nRenal Cancer subtyping, and we compare our solution with several\nstate-of-the-art classification counterparts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09471v1",
    "published_date": "2024-11-14 14:21:49 UTC",
    "updated_date": "2024-11-14 14:21:49 UTC"
  },
  {
    "arxiv_id": "2411.09469v1",
    "title": "An Explainable Attention Model for Cervical Precancer Risk Classification using Colposcopic Images",
    "authors": [
      "Smith K. Khare",
      "Berit Bargum Booth",
      "Victoria Blanes-Vidal",
      "Lone Kjeld Petersen",
      "Esmaeil S. Nadimi"
    ],
    "abstract": "Cervical cancer remains a major worldwide health issue, with early\nidentification and risk assessment playing critical roles in effective\npreventive interventions. This paper presents the Cervix-AID-Net model for\ncervical precancer risk classification. The study designs and evaluates the\nproposed Cervix-AID-Net model based on patients colposcopy images. The model\ncomprises a Convolutional Block Attention Module (CBAM) and convolutional\nlayers that extract interpretable and representative features of colposcopic\nimages to distinguish high-risk and low-risk cervical precancer. In addition,\nthe proposed Cervix-AID-Net model integrates four explainable techniques,\nnamely gradient class activation maps, Local Interpretable Model-agnostic\nExplanations, CartoonX, and pixel rate distortion explanation based on output\nfeature maps and input features. The evaluation using holdout and ten-fold\ncross-validation techniques yielded a classification accuracy of 99.33\\% and\n99.81\\%. The analysis revealed that CartoonX provides meticulous explanations\nfor the decision of the Cervix-AID-Net model due to its ability to provide the\nrelevant piece-wise smooth part of the image. The effect of Gaussian noise and\nblur on the input shows that the performance remains unchanged up to Gaussian\nnoise of 3\\% and blur of 10\\%, while the performance reduces thereafter. A\ncomparison study of the proposed model's performance compared to other deep\nlearning approaches highlights the Cervix-AID-Net model's potential as a\nsupplemental tool for increasing the effectiveness of cervical precancer risk\nassessment. The proposed method, which incorporates the CBAM and explainable\nartificial integration, has the potential to influence cervical cancer\nprevention and early detection, improving patient outcomes and lowering the\nworldwide burden of this preventable disease.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "19 pages, 9 figure, and 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.09469v1",
    "published_date": "2024-11-14 14:18:40 UTC",
    "updated_date": "2024-11-14 14:18:40 UTC"
  },
  {
    "arxiv_id": "2411.09451v1",
    "title": "DiffRoad: Realistic and Diverse Road Scenario Generation for Autonomous Vehicle Testing",
    "authors": [
      "Junjie Zhou",
      "Lin Wang",
      "Qiang Meng",
      "Xiaofan Wang"
    ],
    "abstract": "Generating realistic and diverse road scenarios is essential for autonomous\nvehicle testing and validation. Nevertheless, owing to the complexity and\nvariability of real-world road environments, creating authentic and varied\nscenarios for intelligent driving testing is challenging. In this paper, we\npropose DiffRoad, a novel diffusion model designed to produce controllable and\nhigh-fidelity 3D road scenarios. DiffRoad leverages the generative capabilities\nof diffusion models to synthesize road layouts from white noise through an\ninverse denoising process, preserving real-world spatial features. To enhance\nthe quality of generated scenarios, we design the Road-UNet architecture,\noptimizing the balance between backbone and skip connections for high-realism\nscenario generation. Furthermore, we introduce a road scenario evaluation\nmodule that screens adequate and reasonable scenarios for intelligent driving\ntesting using two critical metrics: road continuity and road reasonableness.\nExperimental results on multiple real-world datasets demonstrate DiffRoad's\nability to generate realistic and smooth road structures while maintaining the\noriginal distribution. Additionally, the generated scenarios can be fully\nautomated into the OpenDRIVE format, facilitating generalized autonomous\nvehicle simulation testing. DiffRoad provides a rich and diverse scenario\nlibrary for large-scale autonomous vehicle testing and offers valuable insights\nfor future infrastructure designs that are better suited for autonomous\nvehicles.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "14 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.09451v1",
    "published_date": "2024-11-14 13:56:02 UTC",
    "updated_date": "2024-11-14 13:56:02 UTC"
  },
  {
    "arxiv_id": "2411.09429v4",
    "title": "AI-driven inverse design of materials: Past, present and future",
    "authors": [
      "Xiao-Qi Han",
      "Xin-De Wang",
      "Meng-Yuan Xu",
      "Zhen Feng",
      "Bo-Wen Yao",
      "Peng-Jie Guo",
      "Ze-Feng Gao",
      "Zhong-Yi Lu"
    ],
    "abstract": "The discovery of advanced materials is the cornerstone of human technological\ndevelopment and progress. The structures of materials and their corresponding\nproperties are essentially the result of a complex interplay of multiple\ndegrees of freedom such as lattice, charge, spin, symmetry, and topology. This\nposes significant challenges for the inverse design methods of materials.\nHumans have long explored new materials through a large number of experiments\nand proposed corresponding theoretical systems to predict new material\nproperties and structures. With the improvement of computational power,\nresearchers have gradually developed various electronic structure calculation\nmethods, such as the density functional theory and high-throughput\ncomputational methods. Recently, the rapid development of artificial\nintelligence technology in the field of computer science has enabled the\neffective characterization of the implicit association between material\nproperties and structures, thus opening up an efficient paradigm for the\ninverse design of functional materials. A significant progress has been made in\ninverse design of materials based on generative and discriminative models,\nattracting widespread attention from researchers. Considering this rapid\ntechnological progress, in this survey, we look back on the latest advancements\nin AI-driven inverse design of materials by introducing the background, key\nfindings, and mainstream technological development routes. In addition, we\nsummarize the remaining issues for future directions. This survey provides the\nlatest overview of AI-driven inverse design of materials, which can serve as a\nuseful resource for researchers.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.supr-con",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "44 pages, 6 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.09429v4",
    "published_date": "2024-11-14 13:25:04 UTC",
    "updated_date": "2025-02-20 03:47:54 UTC"
  },
  {
    "arxiv_id": "2411.09422v2",
    "title": "OpenLS-DGF: An Adaptive Open-Source Dataset Generation Framework for Machine Learning Tasks in Logic Synthesis",
    "authors": [
      "Liwei Ni",
      "Rui Wang",
      "Miao Liu",
      "Xingyu Meng",
      "Xiaoze Lin",
      "Junfeng Liu",
      "Guojie Luo",
      "Zhufei Chu",
      "Weikang Qian",
      "Xiaoyan Yang",
      "Biwei Xie",
      "Xingquan Li",
      "Huawei Li"
    ],
    "abstract": "This paper introduces OpenLS-DGF, an adaptive logic synthesis dataset\ngeneration framework, to enhance machine learning~(ML) applications within the\nlogic synthesis process. Previous dataset generation flows were tailored for\nspecific tasks or lacked integrated machine learning capabilities. While\nOpenLS-DGF supports various machine learning tasks by encapsulating the three\nfundamental steps of logic synthesis: Boolean representation, logic\noptimization, and technology mapping. It preserves the original information in\nboth Verilog and machine-learning-friendly GraphML formats. The verilog files\noffer semi-customizable capabilities, enabling researchers to insert additional\nsteps and incrementally refine the generated dataset. Furthermore, OpenLS-DGF\nincludes an adaptive circuit engine that facilitates the final dataset\nmanagement and downstream tasks. The generated OpenLS-D-v1 dataset comprises 46\ncombinational designs from established benchmarks, totaling over 966,000\nBoolean circuits. OpenLS-D-v1 supports integrating new data features, making it\nmore versatile for new challenges. This paper demonstrates the versatility of\nOpenLS-D-v1 through four distinct downstream tasks: circuit classification,\ncircuit ranking, quality of results (QoR) prediction, and probability\nprediction. Each task is chosen to represent essential steps of logic\nsynthesis, and the experimental results show the generated dataset from\nOpenLS-DGF achieves prominent diversity and applicability. The source code and\ndatasets are available at\nhttps://github.com/Logic-Factory/ACE/blob/master/OpenLS-DGF/readme.md.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.09422v2",
    "published_date": "2024-11-14 13:18:06 UTC",
    "updated_date": "2024-11-16 07:48:26 UTC"
  },
  {
    "arxiv_id": "2411.09420v3",
    "title": "SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph Attention for Vision Transformers",
    "authors": [
      "Shravan Venkatraman",
      "Jaskaran Singh Walia",
      "Joe Dhanith P R"
    ],
    "abstract": "Vision Transformers (ViTs) have redefined image classification by leveraging\nself-attention to capture complex patterns and long-range dependencies between\nimage patches. However, a key challenge for ViTs is efficiently incorporating\nmulti-scale feature representations, which is inherent in convolutional neural\nnetworks (CNNs) through their hierarchical structure. Graph transformers have\nmade strides in addressing this by leveraging graph-based modeling, but they\noften lose or insufficiently represent spatial hierarchies, especially since\nredundant or less relevant areas dilute the image's contextual representation.\nTo bridge this gap, we propose SAG-ViT, a Scale-Aware Graph Attention ViT that\nintegrates multi-scale feature capabilities of CNNs, representational power of\nViTs, graph-attended patching to enable richer contextual representation. Using\nEfficientNetV2 as a backbone, the model extracts multi-scale feature maps,\ndividing them into patches to preserve richer semantic information compared to\ndirectly patching the input images. The patches are structured into a graph\nusing spatial and feature similarities, where a Graph Attention Network (GAT)\nrefines the node embeddings. This refined graph representation is then\nprocessed by a Transformer encoder, capturing long-range dependencies and\ncomplex interactions. We evaluate SAG-ViT on benchmark datasets across various\ndomains, validating its effectiveness in advancing image classification tasks.\nOur code and weights are available at https://github.com/shravan-18/SAG-ViT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T07",
      "I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 8 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.09420v3",
    "published_date": "2024-11-14 13:15:27 UTC",
    "updated_date": "2025-01-08 04:31:16 UTC"
  },
  {
    "arxiv_id": "2411.09413v2",
    "title": "Detecting Children with Autism Spectrum Disorder based on Script-Centric Behavior Understanding with Emotional Enhancement",
    "authors": [
      "Wenxing Liu",
      "Yueran Pan",
      "Dong Zhang",
      "Hongzhu Deng",
      "Xiaobing Zou",
      "Ming Li"
    ],
    "abstract": "The early diagnosis of autism spectrum disorder (ASD) is critically dependent\non systematic observation and analysis of children's social behaviors. While\ncurrent methodologies predominantly utilize supervised learning approaches,\ntheir clinical adoption faces two principal limitations: insufficient ASD\ndiagnostic samples and inadequate interpretability of the detection outcomes.\nThis paper presents a novel zero-shot ASD detection framework based on\nscript-centric behavioral understanding with emotional enhancement, which is\ndesigned to overcome the aforementioned clinical constraints. The proposed\npipeline automatically converts audio-visual data into structured behavioral\ntext scripts through computer vision techniques, subsequently capitalizing on\nthe generalization capabilities of large language models (LLMs) for\nzero-shot/few-shot ASD detection. Three core technical contributions are\nintroduced: (1) A multimodal script transcription module transforming\nbehavioral cues into structured textual representations. (2) An emotion\ntextualization module encoding emotional dynamics as the contextual features to\naugment behavioral understanding. (3) A domain-specific prompt engineering\nstrategy enables the injection of clinical knowledge into LLMs. Our method\nachieves an F1-score of 95.24\\% in diagnosing ASD in children with an average\nage of two years while generating interpretable detection rationales. This work\nopens up new avenues for leveraging the power of LLMs in analyzing and\nunderstanding ASD-related human behavior, thereby enhancing the accuracy of\nassisted autism diagnosis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 12 figures, sumbitted to IEEE transactions on affective\n  computing",
    "pdf_url": "http://arxiv.org/pdf/2411.09413v2",
    "published_date": "2024-11-14 13:07:19 UTC",
    "updated_date": "2025-04-29 07:46:18 UTC"
  },
  {
    "arxiv_id": "2411.09403v1",
    "title": "Quantum Machine Learning: An Interplay Between Quantum Computing and Machine Learning",
    "authors": [
      "Jun Qi",
      "Chao-Han Yang",
      "Samuel Yen-Chi Chen",
      "Pin-Yu Chen"
    ],
    "abstract": "Quantum machine learning (QML) is a rapidly growing field that combines\nquantum computing principles with traditional machine learning. It seeks to\nrevolutionize machine learning by harnessing the unique capabilities of quantum\nmechanics and employs machine learning techniques to advance quantum computing\nresearch. This paper introduces quantum computing for the machine learning\nparadigm, where variational quantum circuits (VQC) are used to develop QML\narchitectures on noisy intermediate-scale quantum (NISQ) devices. We discuss\nmachine learning for the quantum computing paradigm, showcasing our recent\ntheoretical and empirical findings. In particular, we delve into future\ndirections for studying QML, exploring the potential industrial impacts of QML\nresearch.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "In submission",
    "pdf_url": "http://arxiv.org/pdf/2411.09403v1",
    "published_date": "2024-11-14 12:27:50 UTC",
    "updated_date": "2024-11-14 12:27:50 UTC"
  },
  {
    "arxiv_id": "2411.09402v2",
    "title": "Automated Segmentation of Ischemic Stroke Lesions in Non-Contrast Computed Tomography Images for Enhanced Treatment and Prognosis",
    "authors": [
      "Toufiq Musah",
      "Prince Ebenezer Adjei",
      "Kojo Obed Otoo"
    ],
    "abstract": "Stroke is the second leading cause of death worldwide, and is increasingly\nprevalent in low- and middle-income countries (LMICs). Timely interventions can\nsignificantly influence stroke survivability and the quality of life after\ntreatment. However, the standard and most widely available imaging method for\nconfirming strokes and their sub-types, the NCCT, is more challenging and\ntime-consuming to employ in cases of ischemic stroke. For this reason, we\ndeveloped an automated method for ischemic stroke lesion segmentation in NCCTs\nusing the nnU-Net frame work, aimed at enhancing early treatment and improving\nthe prognosis of ischemic stroke patients. We achieved Dice scores of 0.596 and\nIntersection over Union (IoU) scores of 0.501 on the sampled dataset. After\nadjusting for outliers, these scores improved to 0.752 for the Dice score and\n0.643 for the IoU. Proper delineation of the region of infarction can help\nclinicians better assess the potential impact of the infarction, and guide\ntreatment procedures.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "7 pages, 3 figures, MICCAI Meets Africa Workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.09402v2",
    "published_date": "2024-11-14 12:27:31 UTC",
    "updated_date": "2024-11-15 09:52:20 UTC"
  },
  {
    "arxiv_id": "2411.09723v1",
    "title": "Towards Neural Foundation Models for Vision: Aligning EEG, MEG, and fMRI Representations for Decoding, Encoding, and Modality Conversion",
    "authors": [
      "Matteo Ferrante",
      "Tommaso Boccato",
      "Grigorii Rashkov",
      "Nicola Toschi"
    ],
    "abstract": "This paper presents a novel approach towards creating a foundational model\nfor aligning neural data and visual stimuli across multimodal representationsof\nbrain activity by leveraging contrastive learning. We used\nelectroencephalography (EEG), magnetoencephalography (MEG), and functional\nmagnetic resonance imaging (fMRI) data. Our framework's capabilities are\ndemonstrated through three key experiments: decoding visual information from\nneural data, encoding images into neural representations, and converting\nbetween neural modalities. The results highlight the model's ability to\naccurately capture semantic information across different brain imaging\ntechniques, illustrating its potential in decoding, encoding, and modality\nconversion tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09723v1",
    "published_date": "2024-11-14 12:27:27 UTC",
    "updated_date": "2024-11-14 12:27:27 UTC"
  },
  {
    "arxiv_id": "2411.09400v2",
    "title": "Imagined Speech and Visual Imagery as Intuitive Paradigms for Brain-Computer Interfaces",
    "authors": [
      "Seo-Hyun Lee",
      "Ji-Ha Park",
      "Deok-Seon Kim"
    ],
    "abstract": "Brain-computer interfaces (BCIs) have shown promise in enabling communication\nfor individuals with motor impairments. Recent advancements like\nbrain-to-speech technology aim to reconstruct speech from neural activity.\nHowever, decoding communication-related paradigms, such as imagined speech and\nvisual imagery, using non-invasive techniques remains challenging. This study\nanalyzes brain dynamics in these two paradigms by examining neural\nsynchronization and functional connectivity through phase-locking values (PLV)\nin EEG data from 16 participants. Results show that visual imagery produces\nhigher PLV values in visual cortex, engaging spatial networks, while imagined\nspeech demonstrates consistent synchronization, primarily engaging\nlanguage-related regions. These findings suggest that imagined speech is\nsuitable for language-driven BCI applications, while visual imagery can\ncomplement BCI systems for users with speech impairments. Personalized\ncalibration is crucial for optimizing BCI performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.09400v2",
    "published_date": "2024-11-14 12:19:28 UTC",
    "updated_date": "2024-11-29 16:34:55 UTC"
  },
  {
    "arxiv_id": "2411.09389v1",
    "title": "Less is More: Unseen Domain Fake News Detection via Causal Propagation Substructures",
    "authors": [
      "Shuzhi Gong",
      "Richard O. Sinnott",
      "Jianzhong Qi",
      "Cecile Paris"
    ],
    "abstract": "The spread of fake news on social media poses significant threats to\nindividuals and society. Text-based and graph-based models have been employed\nfor fake news detection by analysing news content and propagation networks,\nshowing promising results in specific scenarios. However, these data-driven\nmodels heavily rely on pre-existing in-distribution data for training, limiting\ntheir performance when confronted with fake news from emerging or previously\nunseen domains, known as out-of-distribution (OOD) data. Tackling OOD fake news\nis a challenging yet critical task. In this paper, we introduce the Causal\nSubgraph-oriented Domain Adaptive Fake News Detection (CSDA) model, designed to\nenhance zero-shot fake news detection by extracting causal substructures from\npropagation graphs using in-distribution data and generalising this approach to\nOOD data. The model employs a graph neural network based mask generation\nprocess to identify dominant nodes and edges within the propagation graph,\nusing these substructures for fake news detection. Additionally, the\nperformance of CSDA is further improved through contrastive learning in\nfew-shot scenarios, where a limited amount of OOD data is available for\ntraining. Extensive experiments on public social media datasets demonstrate\nthat CSDA effectively handles OOD fake news detection, achieving a 7 to 16\npercents accuracy improvement over other state-of-the-art models.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "9 pages, 2 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.09389v1",
    "published_date": "2024-11-14 12:05:35 UTC",
    "updated_date": "2024-11-14 12:05:35 UTC"
  },
  {
    "arxiv_id": "2411.09366v1",
    "title": "LTLf+ and PPLTL+: Extending LTLf and PPLTL to Infinite Traces",
    "authors": [
      "Benjamin Aminof",
      "Giuseppe De Giacomo",
      "Sasha Rubin",
      "Moshe Y. Vardi"
    ],
    "abstract": "We introduce LTLf+ and PPLTL+, two logics to express properties of infinite\ntraces, that are based on the linear-time temporal logics LTLf and PPLTL on\nfinite traces. LTLf+/PPLTL+ use levels of Manna and Pnueli's LTL\nsafety-progress hierarchy, and thus have the same expressive power as LTL.\nHowever, they also retain a crucial characteristic of the reactive synthesis\nproblem for the base logics: the game arena for strategy extraction can be\nderived from deterministic finite automata (DFA). Consequently, these logics\ncircumvent the notorious difficulties associated with determinizing infinite\ntrace automata, typical of LTL reactive synthesis. We present DFA-based\nsynthesis techniques for LTLf+/PPLTL+, and show that synthesis is\n2EXPTIME-complete for LTLf+ (matching LTLf) and EXPTIME-complete for PPLTL+\n(matching PPLTL). Notably, while PPLTL+ retains the full expressive power of\nLTL, reactive synthesis is EXPTIME-complete instead of 2EXPTIME-complete. The\ntechniques are also adapted to optimally solve satisfiability, validity, and\nmodel-checking, to get EXPSPACE-complete for LTLf+ (extending a recent result\nfor the guarantee level using LTLf), and PSPACE-complete for PPLTL+.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09366v1",
    "published_date": "2024-11-14 11:17:06 UTC",
    "updated_date": "2024-11-14 11:17:06 UTC"
  },
  {
    "arxiv_id": "2411.09722v1",
    "title": "Iterative Batch Reinforcement Learning via Safe Diversified Model-based Policy Search",
    "authors": [
      "Amna Najib",
      "Stefan Depeweg",
      "Phillip Swazinna"
    ],
    "abstract": "Batch reinforcement learning enables policy learning without direct\ninteraction with the environment during training, relying exclusively on\npreviously collected sets of interactions. This approach is, therefore,\nwell-suited for high-risk and cost-intensive applications, such as industrial\ncontrol. Learned policies are commonly restricted to act in a similar fashion\nas observed in the batch. In a real-world scenario, learned policies are\ndeployed in the industrial system, inevitably leading to the collection of new\ndata that can subsequently be added to the existing recording. The process of\nlearning and deployment can thus take place multiple times throughout the\nlifespan of a system. In this work, we propose to exploit this iterative nature\nof applying offline reinforcement learning to guide learned policies towards\nefficient and informative data collection during deployment, leading to\ncontinuous improvement of learned policies while remaining within the support\nof collected data. We present an algorithmic methodology for iterative batch\nreinforcement learning based on ensemble-based model-based policy search,\naugmented with safety and, importantly, a diversity criterion.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Workshop on Safe and Robust Robot Learning for Operation in the Real\n  World (SAFE-ROL) at CoRL 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.09722v1",
    "published_date": "2024-11-14 11:10:36 UTC",
    "updated_date": "2024-11-14 11:10:36 UTC"
  },
  {
    "arxiv_id": "2411.09359v2",
    "title": "Your Semantic-Independent Watermark is Fragile: A Semantic Perturbation Attack against EaaS Watermark",
    "authors": [
      "Zekun Fei",
      "Biao Yi",
      "Jianing Geng",
      "Ruiqi He",
      "Lihai Nie",
      "Zheli Liu"
    ],
    "abstract": "Embedding-as-a-Service (EaaS) has emerged as a successful business pattern\nbut faces significant challenges related to various forms of copyright\ninfringement, particularly, the API misuse and model extraction attacks.\nVarious studies have proposed backdoor-based watermarking schemes to protect\nthe copyright of EaaS services. In this paper, we reveal that previous\nwatermarking schemes possess semantic-independent characteristics and propose\nthe Semantic Perturbation Attack (SPA). Our theoretical and experimental\nanalysis demonstrate that this semantic-independent nature makes current\nwatermarking schemes vulnerable to adaptive attacks that exploit semantic\nperturbations tests to bypass watermark verification. Extensive experimental\nresults across multiple datasets demonstrate that the True Positive Rate (TPR)\nfor identifying watermarked samples under SPA can reach up to more than 95\\%,\nrendering watermarks ineffective while maintaining the high utility of\nembeddings. Furthermore, we discuss potential defense strategies to mitigate\nSPA. Our code is available at\nhttps://github.com/Zk4-ps/EaaS-Embedding-Watermark.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09359v2",
    "published_date": "2024-11-14 11:06:34 UTC",
    "updated_date": "2025-02-15 14:46:44 UTC"
  },
  {
    "arxiv_id": "2411.09356v1",
    "title": "Multi-scale Generative Modeling for Fast Sampling",
    "authors": [
      "Xiongye Xiao",
      "Shixuan Li",
      "Luzhe Huang",
      "Gengshuo Liu",
      "Trung-Kien Nguyen",
      "Yi Huang",
      "Di Chang",
      "Mykel J. Kochenderfer",
      "Paul Bogdan"
    ],
    "abstract": "While working within the spatial domain can pose problems associated with\nill-conditioned scores caused by power-law decay, recent advances in\ndiffusion-based generative models have shown that transitioning to the wavelet\ndomain offers a promising alternative. However, within the wavelet domain, we\nencounter unique challenges, especially the sparse representation of\nhigh-frequency coefficients, which deviates significantly from the Gaussian\nassumptions in the diffusion process. To this end, we propose a multi-scale\ngenerative modeling in the wavelet domain that employs distinct strategies for\nhandling low and high-frequency bands. In the wavelet domain, we apply\nscore-based generative modeling with well-conditioned scores for low-frequency\nbands, while utilizing a multi-scale generative adversarial learning for\nhigh-frequency bands. As supported by the theoretical analysis and experimental\nresults, our model significantly improve performance and reduce the number of\ntrainable parameters, sampling steps, and time.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09356v1",
    "published_date": "2024-11-14 11:01:45 UTC",
    "updated_date": "2024-11-14 11:01:45 UTC"
  },
  {
    "arxiv_id": "2411.09355v2",
    "title": "Prices, Bids, Values: One ML-Powered Combinatorial Auction to Rule Them All",
    "authors": [
      "Ermis Soumalias",
      "Jakob Heiss",
      "Jakob Weissteiner",
      "Sven Seuken"
    ],
    "abstract": "We study the design of iterative combinatorial auctions (ICAs). The main\nchallenge in this domain is that the bundle space grows exponentially in the\nnumber of items. To address this, recent work has proposed machine learning\n(ML)-based preference elicitation algorithms that aim to elicit only the most\ncritical information from bidders to maximize efficiency. However, while the\nSOTA ML-based algorithms elicit bidders' preferences via value queries, ICAs\nthat are used in practice elicit information via demand queries. In this paper,\nwe introduce a novel ML algorithm that provably makes use of the full\ninformation from both value and demand queries, and we show via experiments\nthat combining both query types results in significantly better learning\nperformance in practice. Building on these insights, we present MLHCA, a new\nML-powered auction that uses value and demand queries. MLHCA substantially\noutperforms the previous SOTA, reducing efficiency loss by up to a factor 10,\nwith up to 58% fewer queries. Thus, MLHCA achieves large efficiency\nimprovements while also reducing bidders' cognitive load, establishing a new\nbenchmark for both practicability and efficiency.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "91A06, 68T07, 91-08",
      "I.2; I.2.6; J.4"
    ],
    "primary_category": "cs.GT",
    "comment": "8 pages + appendix",
    "pdf_url": "http://arxiv.org/pdf/2411.09355v2",
    "published_date": "2024-11-14 10:56:00 UTC",
    "updated_date": "2025-02-01 05:52:39 UTC"
  },
  {
    "arxiv_id": "2411.09302v1",
    "title": "EEG-Based Speech Decoding: A Novel Approach Using Multi-Kernel Ensemble Diffusion Models",
    "authors": [
      "Soowon Kim",
      "Ha-Na Jo",
      "Eunyeong Ko"
    ],
    "abstract": "In this study, we propose an ensemble learning framework for\nelectroencephalogram-based overt speech classification, leveraging denoising\ndiffusion probabilistic models with varying convolutional kernel sizes. The\nensemble comprises three models with kernel sizes of 51, 101, and 201,\neffectively capturing multi-scale temporal features inherent in signals. This\napproach improves the robustness and accuracy of speech decoding by\naccommodating the rich temporal complexity of neural signals. The ensemble\nmodels work in conjunction with conditional autoencoders that refine the\nreconstructed signals and maximize the useful information for downstream\nclassification tasks. The results indicate that the proposed ensemble-based\napproach significantly outperforms individual models and existing\nstate-of-the-art techniques. These findings demonstrate the potential of\nensemble methods in advancing brain signal decoding, offering new possibilities\nfor non-verbal communication applications, particularly in brain-computer\ninterface systems aimed at aiding individuals with speech impairments.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09302v1",
    "published_date": "2024-11-14 09:23:58 UTC",
    "updated_date": "2024-11-14 09:23:58 UTC"
  },
  {
    "arxiv_id": "2411.09294v1",
    "title": "Learning Hand State Estimation for a Light Exoskeleton",
    "authors": [
      "Gabriele Abbate",
      "Alessandro Giusti",
      "Luca Randazzo",
      "Antonio Paolillo"
    ],
    "abstract": "We propose a machine learning-based estimator of the hand state for\nrehabilitation purposes, using light exoskeletons. These devices are easy to\nuse and useful for delivering domestic and frequent therapies. We build a\nsupervised approach using information from the muscular activity of the forearm\nand the motion of the exoskeleton to reconstruct the hand's opening degree and\ncompliance level. Such information can be used to evaluate the therapy progress\nand develop adaptive control behaviors. Our approach is validated with a real\nlight exoskeleton. The experiments demonstrate good predictive performance of\nour approach when trained on data coming from a single user and tested on the\nsame user, even across different sessions. This generalization capability makes\nour system promising for practical use in real rehabilitation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09294v1",
    "published_date": "2024-11-14 09:12:38 UTC",
    "updated_date": "2024-11-14 09:12:38 UTC"
  },
  {
    "arxiv_id": "2411.09289v1",
    "title": "StreamAdapter: Efficient Test Time Adaptation from Contextual Streams",
    "authors": [
      "Dilxat Muhtar",
      "Yelong Shen",
      "Yaming Yang",
      "Xiaodong Liu",
      "Yadong Lu",
      "Jianfeng Liu",
      "Yuefeng Zhan",
      "Hao Sun",
      "Weiwei Deng",
      "Feng Sun",
      "Xueliang Zhang",
      "Jianfeng Gao",
      "Weizhu Chen",
      "Qi Zhang"
    ],
    "abstract": "In-context learning (ICL) allows large language models (LLMs) to adapt to new\ntasks directly from the given demonstrations without requiring gradient\nupdates. While recent advances have expanded context windows to accommodate\nmore demonstrations, this approach increases inference costs without\nnecessarily improving performance. To mitigate these issues, We propose\nStreamAdapter, a novel approach that directly updates model parameters from\ncontext at test time, eliminating the need for explicit in-context\ndemonstrations. StreamAdapter employs context mapping and weight absorption\nmechanisms to dynamically transform ICL demonstrations into parameter updates\nwith minimal additional parameters. By reducing reliance on numerous in-context\nexamples, StreamAdapter significantly reduce inference costs and allows for\nefficient inference with constant time complexity, regardless of demonstration\ncount. Extensive experiments across diverse tasks and model architectures\ndemonstrate that StreamAdapter achieves comparable or superior adaptation\ncapability to ICL while requiring significantly fewer demonstrations. The\nsuperior task adaptation and context encoding capabilities of StreamAdapter on\nboth language understanding and generation tasks provides a new perspective for\nadapting LLMs at test time using context, allowing for more efficient\nadaptation across scenarios and more cost-effective inference",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 Pages, 9 Figures",
    "pdf_url": "http://arxiv.org/pdf/2411.09289v1",
    "published_date": "2024-11-14 09:03:54 UTC",
    "updated_date": "2024-11-14 09:03:54 UTC"
  },
  {
    "arxiv_id": "2411.09273v1",
    "title": "Cross-Modal Consistency in Multimodal Large Language Models",
    "authors": [
      "Xiang Zhang",
      "Senyu Li",
      "Ning Shi",
      "Bradley Hauer",
      "Zijun Wu",
      "Grzegorz Kondrak",
      "Muhammad Abdul-Mageed",
      "Laks V. S. Lakshmanan"
    ],
    "abstract": "Recent developments in multimodal methodologies have marked the beginning of\nan exciting era for models adept at processing diverse data types, encompassing\ntext, audio, and visual content. Models like GPT-4V, which merge computer\nvision with advanced language processing, exhibit extraordinary proficiency in\nhandling intricate tasks that require a simultaneous understanding of both\ntextual and visual information. Prior research efforts have meticulously\nevaluated the efficacy of these Vision Large Language Models (VLLMs) in various\ndomains, including object detection, image captioning, and other related\nfields. However, existing analyses have often suffered from limitations,\nprimarily centering on the isolated evaluation of each modality's performance\nwhile neglecting to explore their intricate cross-modal interactions.\nSpecifically, the question of whether these models achieve the same level of\naccuracy when confronted with identical task instances across different\nmodalities remains unanswered. In this study, we take the initiative to delve\ninto the interaction and comparison among these modalities of interest by\nintroducing a novel concept termed cross-modal consistency. Furthermore, we\npropose a quantitative evaluation framework founded on this concept. Our\nexperimental findings, drawn from a curated collection of parallel\nvision-language datasets developed by us, unveil a pronounced inconsistency\nbetween the vision and language modalities within GPT-4V, despite its portrayal\nas a unified multimodal model. Our research yields insights into the\nappropriate utilization of such models and hints at potential avenues for\nenhancing their design.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09273v1",
    "published_date": "2024-11-14 08:22:42 UTC",
    "updated_date": "2024-11-14 08:22:42 UTC"
  },
  {
    "arxiv_id": "2411.09269v1",
    "title": "Harnessing multiple LLMs for Information Retrieval: A case study on Deep Learning methodologies in Biodiversity publications",
    "authors": [
      "Vamsi Krishna Kommineni",
      "Birgitta König-Ries",
      "Sheeba Samuel"
    ],
    "abstract": "Deep Learning (DL) techniques are increasingly applied in scientific studies\nacross various domains to address complex research questions. However, the\nmethodological details of these DL models are often hidden in the unstructured\ntext. As a result, critical information about how these models are designed,\ntrained, and evaluated is challenging to access and comprehend. To address this\nissue, in this work, we use five different open-source Large Language Models\n(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,\nand Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)\napproach to extract and process DL methodological details from scientific\npublications automatically. We built a voting classifier from the outputs of\nfive LLMs to accurately report DL methodological information. We tested our\napproach using biodiversity publications, building upon our previous research.\nTo validate our pipeline, we employed two datasets of DL-related biodiversity\npublications: a curated set of 100 publications from our prior work and a set\nof 364 publications from the Ecological Informatics journal. Our results\ndemonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of\nDL methodological information, achieving an accuracy of 69.5% (417 out of 600\ncomparisons) based solely on textual content from publications. This\nperformance was assessed against human annotators who had access to code,\nfigures, tables, and other supplementary information. Although demonstrated in\nbiodiversity, our methodology is not limited to this field; it can be applied\nacross other scientific domains where detailed methodological reporting is\nessential for advancing knowledge and ensuring reproducibility. This study\npresents a scalable and reliable approach for automating information\nextraction, facilitating better reproducibility and knowledge transfer across\nstudies.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "H.3.3; I.2.7"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09269v1",
    "published_date": "2024-11-14 08:12:36 UTC",
    "updated_date": "2024-11-14 08:12:36 UTC"
  },
  {
    "arxiv_id": "2411.09266v1",
    "title": "How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative Study of ChatGPT, AI Models and Human Perception",
    "authors": [
      "Sahibzada Adil Shahzad",
      "Ammarah Hashmi",
      "Yan-Tsung Peng",
      "Yu Tsao",
      "Hsin-Min Wang"
    ],
    "abstract": "Multimodal deepfakes involving audiovisual manipulations are a growing threat\nbecause they are difficult to detect with the naked eye or using unimodal deep\nlearningbased forgery detection methods. Audiovisual forensic models, while\nmore capable than unimodal models, require large training datasets and are\ncomputationally expensive for training and inference. Furthermore, these models\nlack interpretability and often do not generalize well to unseen manipulations.\nIn this study, we examine the detection capabilities of a large language model\n(LLM) (i.e., ChatGPT) to identify and account for any possible visual and\nauditory artifacts and manipulations in audiovisual deepfake content. Extensive\nexperiments are conducted on videos from a benchmark multimodal deepfake\ndataset to evaluate the detection performance of ChatGPT and compare it with\nthe detection capabilities of state-of-the-art multimodal forensic models and\nhumans. Experimental results demonstrate the importance of domain knowledge and\nprompt engineering for video forgery detection tasks using LLMs. Unlike\napproaches based on end-to-end learning, ChatGPT can account for spatial and\nspatiotemporal artifacts and inconsistencies that may exist within or across\nmodalities. Additionally, we discuss the limitations of ChatGPT for multimedia\nforensic tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09266v1",
    "published_date": "2024-11-14 08:07:02 UTC",
    "updated_date": "2024-11-14 08:07:02 UTC"
  },
  {
    "arxiv_id": "2411.09261v2",
    "title": "Automating Autograding: Large Language Models as Test Suite Generators for Introductory Programming",
    "authors": [
      "Umar Alkafaween",
      "Ibrahim Albluwi",
      "Paul Denny"
    ],
    "abstract": "Automatically graded programming assignments provide instant feedback to\nstudents and significantly reduce manual grading time for instructors. However,\ncreating comprehensive suites of test cases for programming problems within\nautomatic graders can be time-consuming and complex. The effort needed to\ndefine test suites may deter some instructors from creating additional problems\nor lead to inadequate test coverage, potentially resulting in misleading\nfeedback on student solutions. Such limitations may reduce student access to\nthe well-documented benefits of timely feedback when learning programming.\n  In this work, we evaluate the effectiveness of using Large Language Models\n(LLMs), as part of a larger workflow, to automatically generate test suites for\nCS1-level programming problems. Each problem's statement and reference solution\nare provided to GPT-4 to produce a test suite that can be used by an\nautograder. We evaluate our proposed approach using a sample of 26 problems,\nand more than 25,000 attempted solutions to those problems, submitted by\nstudents in an introductory programming course. We compare the performance of\nthe LLM-generated test suites against the instructor-created test suites for\neach problem. Our findings reveal that LLM-generated test suites can correctly\nidentify most valid solutions, and for most problems are at least as\ncomprehensive as the instructor test suites. Additionally, the LLM-generated\ntest suites exposed ambiguities in some problem statements, underscoring their\npotential to improve both autograding and instructional design.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.3.2; I.2.7"
    ],
    "primary_category": "cs.CY",
    "comment": "Submitted to Journal of Computer Assisted Learning; updated table\n  refs",
    "pdf_url": "http://arxiv.org/pdf/2411.09261v2",
    "published_date": "2024-11-14 07:58:44 UTC",
    "updated_date": "2024-11-18 06:41:26 UTC"
  },
  {
    "arxiv_id": "2411.09251v1",
    "title": "Cross Space and Time: A Spatio-Temporal Unitized Model for Traffic Flow Forecasting",
    "authors": [
      "Weilin Ruan",
      "Wenzhuo Wang",
      "Siru Zhong",
      "Wei Chen",
      "Li Liu",
      "Yuxuan Liang"
    ],
    "abstract": "Predicting spatio-temporal traffic flow presents significant challenges due\nto complex interactions between spatial and temporal factors. Existing\napproaches often address these dimensions in isolation, neglecting their\ncritical interdependencies. In this paper, we introduce the Spatio-Temporal\nUnitized Model (STUM), a unified framework designed to capture both spatial and\ntemporal dependencies while addressing spatio-temporal heterogeneity through\ntechniques such as distribution alignment and feature fusion. It also ensures\nboth predictive accuracy and computational efficiency. Central to STUM is the\nAdaptive Spatio-temporal Unitized Cell (ASTUC), which utilizes low-rank\nmatrices to seamlessly store, update, and interact with space, time, as well as\ntheir correlations. Our framework is also modular, allowing it to integrate\nwith various spatio-temporal graph neural networks through components such as\nbackbone models, feature extractors, residual fusion blocks, and predictive\nmodules to collectively enhance forecasting outcomes. Experimental results\nacross multiple real-world datasets demonstrate that STUM consistently improves\nprediction performance with minimal computational cost. These findings are\nfurther supported by hyperparameter optimization, pre-training analysis, and\nresult visualization. We provide our source code for reproducibility at\nhttps://anonymous.4open.science/r/STUM-E4F0.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09251v1",
    "published_date": "2024-11-14 07:34:31 UTC",
    "updated_date": "2024-11-14 07:34:31 UTC"
  },
  {
    "arxiv_id": "2411.09249v1",
    "title": "Enhancing Financial Domain Adaptation of Language Models via Model Augmentation",
    "authors": [
      "Kota Tanabe",
      "Masanori Hirano",
      "Kazuki Matoya",
      "Kentaro Imajo",
      "Hiroki Sakaji",
      "Itsuki Noda"
    ],
    "abstract": "The domain adaptation of language models, including large language models\n(LLMs), has become increasingly important as the use of such models continues\nto expand. This study demonstrates the effectiveness of Composition to Augment\nLanguage Models (CALM) in adapting to the financial domain. CALM is a model to\nextend the capabilities of existing models by introducing cross-attention\nbetween two LLMs with different functions. In our experiments, we developed a\nCALM to enhance the financial performance of an LLM with strong response\ncapabilities by leveraging a financial-specialized LLM. Notably, the CALM was\ntrained using a financial dataset different from the one used to train the\nfinancial-specialized LLM, confirming CALM's ability to adapt to various\ndatasets. The models were evaluated through quantitative Japanese financial\nbenchmarks and qualitative response comparisons, demonstrating that CALM\nenables superior responses with higher scores than the original models and\nbaselines. Additionally, comparative experiments on connection points revealed\nthat connecting the middle layers of the models is most effective in\nfacilitating adaptation to the financial domain. These findings confirm that\nCALM is a practical approach for adapting LLMs to the financial domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09249v1",
    "published_date": "2024-11-14 07:28:09 UTC",
    "updated_date": "2024-11-14 07:28:09 UTC"
  },
  {
    "arxiv_id": "2411.09243v1",
    "title": "Towards Unified Neural Decoding of Perceived, Spoken and Imagined Speech from EEG Signals",
    "authors": [
      "Jung-Sun Lee",
      "Ha-Na Jo",
      "Seo-Hyun Lee"
    ],
    "abstract": "Brain signals accompany various information relevant to human actions and\nmental imagery, making them crucial to interpreting and understanding human\nintentions. Brain-computer interface technology leverages this brain activity\nto generate external commands for controlling the environment, offering\ncritical advantages to individuals with paralysis or locked-in syndrome. Within\nthe brain-computer interface domain, brain-to-speech research has gained\nattention, focusing on the direct synthesis of audible speech from brain\nsignals. Most current studies decode speech from brain activity using invasive\ntechniques and emphasize spoken speech data. However, humans express various\nspeech states, and distinguishing these states through non-invasive approaches\nremains a significant yet challenging task. This research investigated the\neffectiveness of deep learning models for non-invasive-based neural signal\ndecoding, with an emphasis on distinguishing between different speech\nparadigms, including perceived, overt, whispered, and imagined speech, across\nmultiple frequency bands. The model utilizing the spatial conventional neural\nnetwork module demonstrated superior performance compared to other models,\nespecially in the gamma band. Additionally, imagined speech in the theta\nfrequency band, where deep learning also showed strong effects, exhibited\nstatistically significant differences compared to the other speech paradigms.",
    "categories": [
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09243v1",
    "published_date": "2024-11-14 07:20:08 UTC",
    "updated_date": "2024-11-14 07:20:08 UTC"
  },
  {
    "arxiv_id": "2411.09224v1",
    "title": "Programming with AI: Evaluating ChatGPT, Gemini, AlphaCode, and GitHub Copilot for Programmers",
    "authors": [
      "Md Kamrul Siam",
      "Huanying Gu",
      "Jerry Q. Cheng"
    ],
    "abstract": "Our everyday lives now heavily rely on artificial intelligence (AI) powered\nlarge language models (LLMs). Like regular users, programmers are also\nbenefiting from the newest large language models. In response to the critical\nrole that AI models play in modern software development, this study presents a\nthorough evaluation of leading programming assistants, including ChatGPT,\nGemini(Bard AI), AlphaCode, and GitHub Copilot. The evaluation is based on\ntasks like natural language processing and code generation accuracy in\ndifferent programming languages like Java, Python and C++. Based on the\nresults, it has emphasized their strengths and weaknesses and the importance of\nfurther modifications to increase the reliability and accuracy of the latest\npopular models. Although these AI assistants illustrate a high level of\nprogress in language understanding and code generation, along with ethical\nconsiderations and responsible usage, they provoke a necessity for discussion.\nWith time, developing more refined AI technology is essential for achieving\nadvanced solutions in various fields, especially with the knowledge of the\nfeature intricacies of these models and their implications. This study offers a\ncomparison of different LLMs and provides essential feedback on the rapidly\nchanging area of AI models. It also emphasizes the need for ethical\ndevelopmental practices to actualize AI models' full potential.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.09224v1",
    "published_date": "2024-11-14 06:40:55 UTC",
    "updated_date": "2024-11-14 06:40:55 UTC"
  },
  {
    "arxiv_id": "2411.09718v1",
    "title": "NFRs in Medical Imaging",
    "authors": [
      "Amanda Vallentin"
    ],
    "abstract": "The diagnostic imaging departments are under great pressure due to a growing\nworkload. The number of required scans is growing and there is a shortage of\nqualified labor. AI solutions for medical imaging applications have shown great\npotential. However, very few diagnostic imaging models have been approved for\nhospital use and even fewer are being implemented at the hospitals. The most\ncommon reason why software projects fail is poor requirement engineering,\nespecially non-functional requirements (NFRs) can be detrimental to a project.\nResearch shows that machine learning professionals struggle to work with NFRs\nand that there is a need to adapt NFR frameworks to machine learning, AI-based,\nsoftware. This study uses qualitative methods to interact with key stakeholders\nto identify which types of NFRs are important for medical imaging applications.\nThe study was done on a single Danish hospital and found that NFRs of type\nEfficiency, Accuracy, Interoperability, Reliability, Usability, Adaptability,\nand Fairness were important to the stakeholders. Especially Efficiency since\nthe diagnostic imaging department is trying to spend as little time as possible\non each scan.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09718v1",
    "published_date": "2024-11-14 06:39:56 UTC",
    "updated_date": "2024-11-14 06:39:56 UTC"
  },
  {
    "arxiv_id": "2411.09220v1",
    "title": "Transferable Adversarial Attacks against ASR",
    "authors": [
      "Xiaoxue Gao",
      "Zexin Li",
      "Yiming Chen",
      "Cong Liu",
      "Haizhou Li"
    ],
    "abstract": "Given the extensive research and real-world applications of automatic speech\nrecognition (ASR), ensuring the robustness of ASR models against minor input\nperturbations becomes a crucial consideration for maintaining their\neffectiveness in real-time scenarios. Previous explorations into ASR model\nrobustness have predominantly revolved around evaluating accuracy on white-box\nsettings with full access to ASR models. Nevertheless, full ASR model details\nare often not available in real-world applications. Therefore, evaluating the\nrobustness of black-box ASR models is essential for a comprehensive\nunderstanding of ASR model resilience. In this regard, we thoroughly study the\nvulnerability of practical black-box attacks in cutting-edge ASR models and\npropose to employ two advanced time-domain-based transferable attacks alongside\nour differentiable feature extractor. We also propose a speech-aware gradient\noptimization approach (SAGO) for ASR, which forces mistranscription with\nminimal impact on human imperceptibility through voice activity detection rule\nand a speech-aware gradient-oriented optimizer. Our comprehensive experimental\nresults reveal performance enhancements compared to baseline approaches across\nfive models on two databases.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "IEEE SPL",
    "pdf_url": "http://arxiv.org/pdf/2411.09220v1",
    "published_date": "2024-11-14 06:32:31 UTC",
    "updated_date": "2024-11-14 06:32:31 UTC"
  },
  {
    "arxiv_id": "2411.09213v1",
    "title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering",
    "authors": [
      "Nghia Trung Ngo",
      "Chien Van Nguyen",
      "Franck Dernoncourt",
      "Thien Huu Nguyen"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising approach to\nenhance the performance of large language models (LLMs) in knowledge-intensive\ntasks such as those from medical domain. However, the sensitive nature of the\nmedical domain necessitates a completely accurate and trustworthy system. While\nexisting RAG benchmarks primarily focus on the standard retrieve-answer\nsetting, they overlook many practical scenarios that measure crucial aspects of\na reliable medical system. This paper addresses this gap by providing a\ncomprehensive evaluation framework for medical question-answering (QA) systems\nin a RAG setting for these situations, including sufficiency, integration, and\nrobustness. We introduce Medical Retrieval-Augmented Generation Benchmark\n(MedRGB) that provides various supplementary elements to four medical QA\ndatasets for testing LLMs' ability to handle these specific scenarios.\nUtilizing MedRGB, we conduct extensive evaluations of both state-of-the-art\ncommercial LLMs and open-source models across multiple retrieval conditions.\nOur experimental results reveals current models' limited ability to handle\nnoise and misinformation in the retrieved documents. We further analyze the\nLLMs' reasoning processes to provides valuable insights and future directions\nfor developing RAG systems in this critical medical domain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09213v1",
    "published_date": "2024-11-14 06:19:18 UTC",
    "updated_date": "2024-11-14 06:19:18 UTC"
  },
  {
    "arxiv_id": "2411.09211v1",
    "title": "Dynamic Neural Communication: Convergence of Computer Vision and Brain-Computer Interface",
    "authors": [
      "Ji-Ha Park",
      "Seo-Hyun Lee",
      "Soowon Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Interpreting human neural signals to decode static speech intentions such as\ntext or images and dynamic speech intentions such as audio or video is showing\ngreat potential as an innovative communication tool. Human communication\naccompanies various features, such as articulatory movements, facial\nexpressions, and internal speech, all of which are reflected in neural signals.\nHowever, most studies only generate short or fragmented outputs, while\nproviding informative communication by leveraging various features from neural\nsignals remains challenging. In this study, we introduce a dynamic neural\ncommunication method that leverages current computer vision and brain-computer\ninterface technologies. Our approach captures the user's intentions from neural\nsignals and decodes visemes in short time steps to produce dynamic visual\noutputs. The results demonstrate the potential to rapidly capture and\nreconstruct lip movements during natural speech attempts from human neural\nsignals, enabling dynamic neural communication through the convergence of\ncomputer vision and brain--computer interface.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages, 2 figures, 1 table, Name of Conference: International\n  Conference on Brain-Computer Interface",
    "pdf_url": "http://arxiv.org/pdf/2411.09211v1",
    "published_date": "2024-11-14 06:15:05 UTC",
    "updated_date": "2024-11-14 06:15:05 UTC"
  },
  {
    "arxiv_id": "2411.09204v1",
    "title": "RibCageImp: A Deep Learning Framework for 3D Ribcage Implant Generation",
    "authors": [
      "Gyanendra Chaubey",
      "Aiman Farooq",
      "Azad Singh",
      "Deepak Mishra"
    ],
    "abstract": "The recovery of damaged or resected ribcage structures requires precise,\ncustom-designed implants to restore the integrity and functionality of the\nthoracic cavity. Traditional implant design methods rely mainly on manual\nprocesses, making them time-consuming and susceptible to variability. In this\nwork, we explore the feasibility of automated ribcage implant generation using\ndeep learning. We present a framework based on 3D U-Net architecture that\nprocesses CT scans to generate patient-specific implant designs. To the best of\nour knowledge, this is the first investigation into automated thoracic implant\ngeneration using deep learning approaches. Our preliminary results, while\nmoderate, highlight both the potential and the significant challenges in this\ncomplex domain. These findings establish a foundation for future research in\nautomated ribcage reconstruction and identify key technical challenges that\nneed to be addressed for practical implementation.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09204v1",
    "published_date": "2024-11-14 06:03:54 UTC",
    "updated_date": "2024-11-14 06:03:54 UTC"
  },
  {
    "arxiv_id": "2411.09200v1",
    "title": "Advancing Software Security and Reliability in Cloud Platforms through AI-based Anomaly Detection",
    "authors": [
      "Sabbir M. Saleh",
      "Ibrahim Mohammed Sayem",
      "Nazim Madhavji",
      "John Steinbacher"
    ],
    "abstract": "Continuous Integration/Continuous Deployment (CI/CD) is fundamental for\nadvanced software development, supporting faster and more efficient delivery of\ncode changes into cloud environments. However, security issues in the CI/CD\npipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are\nhappening over the cloud environments. While plenty of literature discusses\nstatic security testing and CI/CD practices, only a few deal with network\ntraffic pattern analysis to detect different cyberattacks. This research aims\nto enhance CI/CD pipeline security by implementing anomaly detection through AI\n(Artificial Intelligence) support. The goal is to identify unusual behaviour or\nvariations from network traffic patterns in pipeline and cloud platforms. The\nsystem shall integrate into the workflow to continuously monitor pipeline\nactivities and cloud infrastructure. Additionally, it aims to explore adaptive\nresponse mechanisms to mitigate the detected anomalies or security threats.\nThis research employed two popular network traffic datasets, CSE-CIC-IDS2018\nand CSE-CIC-IDS2017. We implemented a combination of Convolution Neural\nNetwork(CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic\npatterns. We achieved an accuracy of 98.69% and 98.30% and generated log files\nin different CI/CD pipeline stages that resemble the network anomalies affected\nto address security challenges in modern DevOps practices, contributing to\nadvancing software security and reliability.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.09200v1",
    "published_date": "2024-11-14 05:45:55 UTC",
    "updated_date": "2024-11-14 05:45:55 UTC"
  },
  {
    "arxiv_id": "2411.09189v2",
    "title": "Improvement and Implementation of a Speech Emotion Recognition Model Based on Dual-Layer LSTM",
    "authors": [
      "Xiaoran Yang",
      "Shuhan Yu",
      "Wenxi Xu"
    ],
    "abstract": "This paper builds upon an existing speech emotion recognition model by adding\nan additional LSTM layer to improve the accuracy and processing efficiency of\nemotion recognition from audio data. By capturing the long-term dependencies\nwithin audio sequences through a dual-layer LSTM network, the model can\nrecognize and classify complex emotional patterns more accurately. Experiments\nconducted on the RAVDESS dataset validated this approach, showing that the\nmodified dual layer LSTM model improves accuracy by 2% compared to the\nsingle-layer LSTM while significantly reducing recognition latency, thereby\nenhancing real-time performance. These results indicate that the dual-layer\nLSTM architecture is highly suitable for handling emotional features with\nlong-term dependencies, providing a viable optimization for speech emotion\nrecognition systems. This research provides a reference for practical\napplications in fields like intelligent customer service, sentiment analysis\nand human-computer interaction.",
    "categories": [
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09189v2",
    "published_date": "2024-11-14 05:05:36 UTC",
    "updated_date": "2024-11-28 19:28:50 UTC"
  },
  {
    "arxiv_id": "2411.09184v1",
    "title": "Dynamic technology impact analysis: A multi-task learning approach to patent citation prediction",
    "authors": [
      "Youngjin Seol",
      "Jaewoong Choi",
      "Seunghyun Lee",
      "Janghyeok Yoon"
    ],
    "abstract": "Machine learning (ML) models are valuable tools for analyzing the impact of\ntechnology using patent citation information. However, existing ML-based\nmethods often struggle to account for the dynamic nature of the technology\nimpact over time and the interdependencies of these impacts across different\nperiods. This study proposes a multi-task learning (MTL) approach to enhance\nthe prediction of technology impact across various time frames by leveraging\nknowledge sharing and simultaneously monitoring the evolution of technology\nimpact. First, we quantify the technology impacts and identify patterns through\ncitation analysis over distinct time periods. Next, we develop MTL models to\npredict citation counts using multiple patent indicators over time. Finally, we\nexamine the changes in key input indicators and their patterns over different\nperiods using the SHapley Additive exPlanation method. We also offer guidelines\nfor validating and interpreting the results by employing statistical methods\nand natural language processing techniques. A case study on battery\ntechnologies demonstrates that our approach not only deepens the understanding\nof technology impact, but also improves prediction accuracy, yielding valuable\ninsights for both academia and industry.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09184v1",
    "published_date": "2024-11-14 04:46:08 UTC",
    "updated_date": "2024-11-14 04:46:08 UTC"
  },
  {
    "arxiv_id": "2411.09181v1",
    "title": "DeBaTeR: Denoising Bipartite Temporal Graph for Recommendation",
    "authors": [
      "Xinyu He",
      "Jose Sepulveda",
      "Mostafa Rahmani",
      "Alyssa Woo",
      "Fei Wang",
      "Hanghang Tong"
    ],
    "abstract": "Due to the difficulty of acquiring large-scale explicit user feedback,\nimplicit feedback (e.g., clicks or other interactions) is widely applied as an\nalternative source of data, where user-item interactions can be modeled as a\nbipartite graph. Due to the noisy and biased nature of implicit real-world\nuser-item interactions, identifying and rectifying noisy interactions are vital\nto enhance model performance and robustness. Previous works on purifying\nuser-item interactions in collaborative filtering mainly focus on mining the\ncorrelation between user/item embeddings and noisy interactions, neglecting the\nbenefit of temporal patterns in determining noisy interactions. Time\ninformation, while enhancing the model utility, also bears its natural\nadvantage in helping to determine noisy edges, e.g., if someone usually watches\nhorror movies at night and talk shows in the morning, a record of watching a\nhorror movie in the morning is more likely to be noisy interaction. Armed with\nthis observation, we introduce a simple yet effective mechanism for generating\ntime-aware user/item embeddings and propose two strategies for denoising\nbipartite temporal graph in recommender systems (DeBaTeR): the first is through\nreweighting the adjacency matrix (DeBaTeR-A), where a reliability score is\ndefined to reweight the edges through both soft assignment and hard assignment;\nthe second is through reweighting the loss function (DeBaTeR-L), where weights\nare generated to reweight user-item samples in the losses. Extensive\nexperiments have been conducted to demonstrate the efficacy of our methods and\nillustrate how time information indeed helps identifying noisy edges.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09181v1",
    "published_date": "2024-11-14 04:39:30 UTC",
    "updated_date": "2024-11-14 04:39:30 UTC"
  },
  {
    "arxiv_id": "2411.09180v1",
    "title": "LEAP:D -- A Novel Prompt-based Approach for Domain-Generalized Aerial Object Detection",
    "authors": [
      "Chanyeong Park",
      "Heegwang Kim",
      "Joonki Paik"
    ],
    "abstract": "Drone-captured images present significant challenges in object detection due\nto varying shooting conditions, which can alter object appearance and shape.\nFactors such as drone altitude, angle, and weather cause these variations,\ninfluencing the performance of object detection algorithms. To tackle these\nchallenges, we introduce an innovative vision-language approach using learnable\nprompts. This shift from conventional manual prompts aims to reduce\ndomain-specific knowledge interference, ultimately improving object detection\ncapabilities. Furthermore, we streamline the training process with a one-step\napproach, updating the learnable prompt concurrently with model training,\nenhancing efficiency without compromising performance. Our study contributes to\ndomain-generalized object detection by leveraging learnable prompts and\noptimizing training processes. This enhances model robustness and adaptability\nacross diverse environments, leading to more effective aerial object detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICIP 2024 Workshop accepted paper",
    "pdf_url": "http://arxiv.org/pdf/2411.09180v1",
    "published_date": "2024-11-14 04:39:10 UTC",
    "updated_date": "2024-11-14 04:39:10 UTC"
  },
  {
    "arxiv_id": "2411.09176v3",
    "title": "Gazing at Rewards: Eye Movements as a Lens into Human and AI Decision-Making in Hybrid Visual Foraging",
    "authors": [
      "Bo Wang",
      "Dingwei Tan",
      "Yen-Ling Kuo",
      "Zhaowei Sun",
      "Jeremy M. Wolfe",
      "Tat-Jen Cham",
      "Mengmi Zhang"
    ],
    "abstract": "Imagine searching a collection of coins for quarters ($0.25$), dimes\n($0.10$), nickels ($0.05$), and pennies ($0.01$)-a hybrid foraging task where\nobservers look for multiple instances of multiple target types. In such tasks,\nhow do target values and their prevalence influence foraging and eye movement\nbehaviors (e.g., should you prioritize rare quarters or common nickels)? To\nexplore this, we conducted human psychophysics experiments, revealing that\nhumans are proficient reward foragers. Their eye fixations are drawn to regions\nwith higher average rewards, fixation durations are longer on more valuable\ntargets, and their cumulative rewards exceed chance, approaching the upper\nbound of optimal foragers. To probe these decision-making processes of humans,\nwe developed a transformer-based Visual Forager (VF) model trained via\nreinforcement learning. Our VF model takes a series of targets, their\ncorresponding values, and the search image as inputs, processes the images\nusing foveated vision, and produces a sequence of eye movements along with\ndecisions on whether to collect each fixated item. Our model outperforms all\nbaselines, achieves cumulative rewards comparable to those of humans, and\napproximates human foraging behavior in eye movements and foraging biases\nwithin time-limited environments. Furthermore, stress tests on\nout-of-distribution tasks with novel targets, unseen values, and varying set\nsizes demonstrate the VF model's effective generalization. Our work offers\nvaluable insights into the relationship between eye movements and\ndecision-making, with our model serving as a powerful tool for further\nexploration of this connection. All data, code, and models are available at\nhttps://github.com/ZhangLab-DeepNeuroCogLab/visual-forager.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09176v3",
    "published_date": "2024-11-14 04:29:07 UTC",
    "updated_date": "2025-03-23 09:34:04 UTC"
  },
  {
    "arxiv_id": "2411.09174v1",
    "title": "Advancing Diffusion Models: Alias-Free Resampling and Enhanced Rotational Equivariance",
    "authors": [
      "Md Fahim Anjum"
    ],
    "abstract": "Recent advances in image generation, particularly via diffusion models, have\nled to impressive improvements in image synthesis quality. Despite this,\ndiffusion models are still challenged by model-induced artifacts and limited\nstability in image fidelity. In this work, we hypothesize that the primary\ncause of this issue is the improper resampling operation that introduces\naliasing in the diffusion model and a careful alias-free resampling dictated by\nimage processing theory can improve the model's performance in image synthesis.\nWe propose the integration of alias-free resampling layers into the UNet\narchitecture of diffusion models without adding extra trainable parameters,\nthereby maintaining computational efficiency. We then assess whether these\ntheory-driven modifications enhance image quality and rotational equivariance.\nOur experimental results on benchmark datasets, including CIFAR-10, MNIST, and\nMNIST-M, reveal consistent gains in image quality, particularly in terms of FID\nand KID scores. Furthermore, we propose a modified diffusion process that\nenables user-controlled rotation of generated images without requiring\nadditional training. Our findings highlight the potential of theory-driven\nenhancements such as alias-free resampling in generative models to improve\nimage quality while maintaining model efficiency and pioneer future research\ndirections to incorporate them into video-generating diffusion models, enabling\ndeeper exploration of the applications of alias-free resampling in generative\nmodeling.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.09174v1",
    "published_date": "2024-11-14 04:23:28 UTC",
    "updated_date": "2024-11-14 04:23:28 UTC"
  },
  {
    "arxiv_id": "2411.09170v1",
    "title": "Towards Scalable Handwriting Communication via EEG Decoding and Latent Embedding Integration",
    "authors": [
      "Jun-Young Kim",
      "Deok-Seon Kim",
      "Seo-Hyun Lee"
    ],
    "abstract": "In recent years, brain-computer interfaces have made advances in decoding\nvarious motor-related tasks, including gesture recognition and movement\nclassification, utilizing electroencephalogram (EEG) data. These developments\nare fundamental in exploring how neural signals can be interpreted to recognize\nspecific physical actions. This study centers on a written alphabet\nclassification task, where we aim to decode EEG signals associated with\nhandwriting. To achieve this, we incorporate hand kinematics to guide the\nextraction of the consistent embeddings from high-dimensional neural recordings\nusing auxiliary variables (CEBRA). These CEBRA embeddings, along with the EEG,\nare processed by a parallel convolutional neural network model that extracts\nfeatures from both data sources simultaneously. The model classifies nine\ndifferent handwritten characters, including symbols such as exclamation marks\nand commas, within the alphabet. We evaluate the model using a quantitative\nfive-fold cross-validation approach and explore the structure of the embedding\nspace through visualizations. Our approach achieves a classification accuracy\nof 91 % for the nine-class task, demonstrating the feasibility of fine-grained\nhandwriting decoding from EEG.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "4 pages, 2 figures, 1 table, Name of Conference: International\n  Conference on Brain-Computer Interface",
    "pdf_url": "http://arxiv.org/pdf/2411.09170v1",
    "published_date": "2024-11-14 04:12:47 UTC",
    "updated_date": "2024-11-14 04:12:47 UTC"
  },
  {
    "arxiv_id": "2411.09169v1",
    "title": "Artificial Theory of Mind and Self-Guided Social Organisation",
    "authors": [
      "Michael S. Harré",
      "Jaime Ruiz-Serra",
      "Catherine Drysdale"
    ],
    "abstract": "One of the challenges artificial intelligence (AI) faces is how a collection\nof agents coordinate their behaviour to achieve goals that are not reachable by\nany single agent. In a recent article by Ozmen et al this was framed as one of\nsix grand challenges: That AI needs to respect human cognitive processes at the\nhuman-AI interaction frontier. We suggest that this extends to the AI-AI\nfrontier and that it should also reflect human psychology, as it is the only\nsuccessful framework we have from which to build out. In this extended abstract\nwe first make the case for collective intelligence in a general setting,\ndrawing on recent work from single neuron complexity in neural networks and ant\nnetwork adaptability in ant colonies. From there we introduce how species\nrelate to one another in an ecological network via niche selection, niche\nchoice, and niche conformity with the aim of forming an analogy with human\nsocial network development as new agents join together and coordinate. From\nthere we show how our social structures are influenced by our neuro-physiology,\nour psychology, and our language. This emphasises how individual people within\na social network influence the structure and performance of that network in\ncomplex tasks, and that cognitive faculties such as Theory of Mind play a\ncentral role. We finish by discussing the current state of the art in AI and\nwhere there is potential for further development of a socially embodied\ncollective artificial intelligence that is capable of guiding its own social\nstructures.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "nlin.AO"
    ],
    "primary_category": "cs.MA",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.09169v1",
    "published_date": "2024-11-14 04:06:26 UTC",
    "updated_date": "2024-11-14 04:06:26 UTC"
  },
  {
    "arxiv_id": "2411.09168v1",
    "title": "Theory of Mind Enhances Collective Intelligence",
    "authors": [
      "Michael S. Harré",
      "Catherine Drysdale",
      "Jaime Ruiz-Serra"
    ],
    "abstract": "Collective Intelligence plays a central role in a large variety of fields,\nfrom economics and evolutionary theory to neural networks and eusocial insects,\nand it is also core to much of the work on emergence and self-organisation in\ncomplex systems theory. However, in human collective intelligence there is\nstill much more to be understood in the relationship between specific\npsychological processes at the individual level and the emergence of\nself-organised structures at the social level. Previously psychological factors\nhave played a relatively minor role in the study of collective intelligence as\nthe principles are often quite general and applicable to humans just as readily\nas insects or other agents without sophisticated psychologies. In this article\nwe emphasise, with examples from other complex adaptive systems, the broad\napplicability of collective intelligence principles while the mechanisms and\ntime-scales differ significantly between examples. We contend that flexible\ncollective intelligence in human social settings is improved by our use of a\nspecific cognitive tool: our Theory of Mind. We identify several key\ncharacteristics of psychologically mediated collective intelligence and show\nthat the development of a Theory of Mind is a crucial factor distinguishing\nsocial collective intelligence from general collective intelligence. We then\nplace these capabilities in the context of the next steps in artificial\nintelligence embedded in a future that includes an effective human-AI hybrid\nsocial ecology.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY",
      "cs.GT",
      "nlin.AO"
    ],
    "primary_category": "cs.MA",
    "comment": "20 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2411.09168v1",
    "published_date": "2024-11-14 03:58:50 UTC",
    "updated_date": "2024-11-14 03:58:50 UTC"
  },
  {
    "arxiv_id": "2411.09160v1",
    "title": "Rationality based Innate-Values-driven Reinforcement Learning",
    "authors": [
      "Qin Yang"
    ],
    "abstract": "Innate values describe agents' intrinsic motivations, which reflect their\ninherent interests and preferences to pursue goals and drive them to develop\ndiverse skills satisfying their various needs. The essence of reinforcement\nlearning (RL) is learning from interaction based on reward-driven behaviors,\nmuch like natural agents. It is an excellent model to describe the\ninnate-values-driven (IV) behaviors of AI agents. Especially developing the\nawareness of the AI agent through balancing internal and external utilities\nbased on its needs in different tasks is a crucial problem for individuals\nlearning to support AI agents integrating human society with safety and harmony\nin the long term. This paper proposes a hierarchical compound intrinsic value\nreinforcement learning model -- innate-values-driven reinforcement learning\ntermed IVRL to describe the complex behaviors of AI agents' interaction. We\nformulated the IVRL model and proposed two IVRL models: DQN and A2C. By\ncomparing them with benchmark algorithms such as DQN, DDQN, A2C, and PPO in the\nRole-Playing Game (RPG) reinforcement learning test platform VIZDoom, we\ndemonstrated that rationally organizing various individual needs can\neffectively achieve better performance.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2401.05572",
    "pdf_url": "http://arxiv.org/pdf/2411.09160v1",
    "published_date": "2024-11-14 03:28:02 UTC",
    "updated_date": "2024-11-14 03:28:02 UTC"
  },
  {
    "arxiv_id": "2411.09158v1",
    "title": "The \\emph{Optimist}: Towards Fully Automated Graph Theory Research",
    "authors": [
      "Randy Davila"
    ],
    "abstract": "This paper introduces the \\emph{Optimist}, an autonomous system developed to\nadvance automated conjecture generation in graph theory. Leveraging\nmixed-integer programming (MIP) and heuristic methods, the \\emph{Optimist}\ngenerates conjectures that both rediscover established theorems and propose\nnovel inequalities. Through a combination of memory-based computation and\nagent-like adaptability, the \\emph{Optimist} iteratively refines its\nconjectures by integrating new data, enabling a feedback process with minimal\nhuman (\\emph{or machine}) intervention. Initial experiments reveal the\n\\emph{Optimist}'s potential to uncover foundational results in graph theory, as\nwell as to produce conjectures of interest for future exploration. This work\nalso outlines the \\emph{Optimist}'s evolving integration with a counterpart\nagent, the \\emph{Pessimist} (a human \\emph{or machine} agent), to establish a\ndueling system that will drive fully automated graph theory research.",
    "categories": [
      "cs.AI",
      "math.CO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09158v1",
    "published_date": "2024-11-14 03:24:45 UTC",
    "updated_date": "2024-11-14 03:24:45 UTC"
  },
  {
    "arxiv_id": "2411.09134v1",
    "title": "ABCI 3.0: Evolution of the leading AI infrastructure in Japan",
    "authors": [
      "Ryousei Takano",
      "Shinichiro Takizawa",
      "Yusuke Tanimura",
      "Hidemoto Nakada",
      "Hirotaka Ogawa"
    ],
    "abstract": "ABCI 3.0 is the latest version of the ABCI, a large-scale open AI\ninfrastructure that AIST has been operating since August 2018 and will be fully\noperational in January 2025. ABCI 3.0 consists of computing servers equipped\nwith 6128 of the NVIDIA H200 GPUs and an all-flash storage system. Its peak\nperformance is 6.22 exaflops in half precision and 3.0 exaflops in single\nprecision, which is 7 to 13 times faster than the previous system, ABCI 2.0. It\nalso more than doubles both storage capacity and theoretical read/write\nperformance. ABCI 3.0 is expected to accelerate research and development,\nevaluation, and workforce development of cutting-edge AI technologies, with a\nparticular focus on generative AI.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "4 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.09134v1",
    "published_date": "2024-11-14 02:14:12 UTC",
    "updated_date": "2024-11-14 02:14:12 UTC"
  },
  {
    "arxiv_id": "2411.09125v1",
    "title": "DROJ: A Prompt-Driven Attack against Large Language Models",
    "authors": [
      "Leyang Hu",
      "Boran Wang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Due to their training on\ninternet-sourced datasets, LLMs can sometimes generate objectionable content,\nnecessitating extensive alignment with human feedback to avoid such outputs.\nDespite massive alignment efforts, LLMs remain susceptible to adversarial\njailbreak attacks, which usually are manipulated prompts designed to circumvent\nsafety mechanisms and elicit harmful responses. Here, we introduce a novel\napproach, Directed Rrepresentation Optimization Jailbreak (DROJ), which\noptimizes jailbreak prompts at the embedding level to shift the hidden\nrepresentations of harmful queries towards directions that are more likely to\nelicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat\nmodel show that DROJ achieves a 100\\% keyword-based Attack Success Rate (ASR),\neffectively preventing direct refusals. However, the model occasionally\nproduces repetitive and non-informative responses. To mitigate this, we\nintroduce a helpfulness system prompt that enhances the utility of the model's\nresponses. Our code is available at\nhttps://github.com/Leon-Leyang/LLM-Safeguard.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09125v1",
    "published_date": "2024-11-14 01:48:08 UTC",
    "updated_date": "2024-11-14 01:48:08 UTC"
  },
  {
    "arxiv_id": "2411.10486v1",
    "title": "Artificial Intelligence for Infectious Disease Prediction and Prevention: A Comprehensive Review",
    "authors": [
      "Selestine Melchane",
      "Youssef Elmir",
      "Farid Kacimi",
      "Larbi Boubchir"
    ],
    "abstract": "Artificial Intelligence (AI) and infectious diseases prediction have recently\nexperienced a common development and advancement. Machine learning (ML)\napparition, along with deep learning (DL) emergence, extended many approaches\nagainst diseases apparition and their spread. And despite their outstanding\nresults in predicting infectious diseases, conflicts appeared regarding the\ntypes of data used and how they can be studied, analyzed, and exploited using\nvarious emerging methods. This has led to some ongoing discussions in the\nfield. This research aims not only to provide an overview of what has been\naccomplished, but also to highlight the difficulties related to the types of\ndata used, and the learning methods applied for each research objective. It\ncategorizes these contributions into three areas: predictions using Public\nHealth Data to prevent the spread of a transmissible disease within a region;\npredictions using Patients' Medical Data to detect whether a person is infected\nby a transmissible disease; and predictions using both Public and patient\nmedical data to estimate the extent of disease spread in a population. The\npaper also critically assesses the potential of AI and outlines its limitations\nin infectious disease management.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.PE"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, 5 figures, this manuscript has been accepted for\n  publication in ACTA UNIVERSITATIS SAPIENTIAE, Informatica",
    "pdf_url": "http://arxiv.org/pdf/2411.10486v1",
    "published_date": "2024-11-14 00:43:32 UTC",
    "updated_date": "2024-11-14 00:43:32 UTC"
  },
  {
    "arxiv_id": "2411.09105v1",
    "title": "VCBench: A Controllable Benchmark for Symbolic and Abstract Challenges in Video Cognition",
    "authors": [
      "Chenglin Li",
      "Qianglong Chen",
      "Zhi Li",
      "Feng Tao",
      "Yin Zhang"
    ],
    "abstract": "Recent advancements in Large Video-Language Models (LVLMs) have driven the\ndevelopment of benchmarks designed to assess cognitive abilities in video-based\ntasks. However, most existing benchmarks heavily rely on web-collected videos\npaired with human annotations or model-generated questions, which limit control\nover the video content and fall short in evaluating advanced cognitive\nabilities involving symbolic elements and abstract concepts. To address these\nlimitations, we introduce VCBench, a controllable benchmark to assess LVLMs'\ncognitive abilities, involving symbolic and abstract concepts at varying\ndifficulty levels. By generating video data with the Python-based engine,\nVCBench allows for precise control over the video content, creating dynamic,\ntask-oriented videos that feature complex scenes and abstract concepts. Each\ntask pairs with tailored question templates that target specific cognitive\nchallenges, providing a rigorous evaluation test. Our evaluation reveals that\neven state-of-the-art (SOTA) models, such as Qwen2-VL-72B, struggle with simple\nvideo cognition tasks involving abstract concepts, with performance sharply\ndropping by 19% as video complexity rises. These findings reveal the current\nlimitations of LVLMs in advanced cognitive tasks and highlight the critical\nrole of VCBench in driving research toward more robust LVLMs for complex video\ncognition challenges.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09105v1",
    "published_date": "2024-11-14 00:26:26 UTC",
    "updated_date": "2024-11-14 00:26:26 UTC"
  },
  {
    "arxiv_id": "2411.09102v2",
    "title": "Provocation: Who benefits from \"inclusion\" in Generative AI?",
    "authors": [
      "Samantha Dalal",
      "Siobhan Mackenzie Hall",
      "Nari Johnson"
    ],
    "abstract": "The demands for accurate and representative generative AI systems means there\nis an increased demand on participatory evaluation structures. While these\nparticipatory structures are paramount to to ensure non-dominant values,\nknowledge and material culture are also reflected in AI models and the media\nthey generate, we argue that dominant structures of community participation in\nAI development and evaluation are not explicit enough about the benefits and\nharms that members of socially marginalized groups may experience as a result\nof their participation. Without explicit interrogation of these benefits by AI\ndevelopers, as a community we may remain blind to the immensity of systemic\nchange that is needed as well. To support this provocation, we present a\nspeculative case study, developed from our own collective experiences as AI\nresearchers. We use this speculative context to itemize the barriers that need\nto be overcome in order for the proposed benefits to marginalized communities\nto be realized, and harms mitigated.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "3 pages, 1 figure. Published as a Short Paper in the NeurIPS 2024\n  Workshop on Evaluating Evaluations: Examining Best Practices for Measuring\n  Broader Impacts of Generative AI",
    "pdf_url": "http://arxiv.org/pdf/2411.09102v2",
    "published_date": "2024-11-14 00:18:25 UTC",
    "updated_date": "2024-11-15 17:09:40 UTC"
  },
  {
    "arxiv_id": "2411.09101v2",
    "title": "Heuristical Comparison of Vision Transformers Against Convolutional Neural Networks for Semantic Segmentation on Remote Sensing Imagery",
    "authors": [
      "Ashim Dahal",
      "Saydul Akbar Murad",
      "Nick Rahimi"
    ],
    "abstract": "Vision Transformers (ViT) have recently brought a new wave of research in the\nfield of computer vision. These models have performed particularly well in\nimage classification and segmentation. Research on semantic and instance\nsegmentation has accelerated with the introduction of the new architecture,\nwith over 80% of the top 20 benchmarks for the iSAID dataset based on either\nthe ViT architecture or the attention mechanism behind its success. This paper\nfocuses on the heuristic comparison of three key factors of using (or not\nusing) ViT for semantic segmentation of remote sensing aerial images on the\niSAID dataset. The experimental results observed during this research were\nanalyzed based on three objectives. First, we studied the use of a weighted\nfused loss function to maximize the mean Intersection over Union (mIoU) score\nand Dice score while minimizing entropy or class representation loss. Second,\nwe compared transfer learning on Meta's MaskFormer, a ViT-based semantic\nsegmentation model, against a generic UNet Convolutional Neural Network (CNN)\nbased on mIoU, Dice scores, training efficiency, and inference time. Third, we\nexamined the trade-offs between the two models in comparison to current\nstate-of-the-art segmentation models. We show that the novel combined weighted\nloss function significantly boosts the CNN model's performance compared to\ntransfer learning with ViT. The code for this implementation can be found at:\nhttps://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09101v2",
    "published_date": "2024-11-14 00:18:04 UTC",
    "updated_date": "2025-02-13 18:20:14 UTC"
  }
]
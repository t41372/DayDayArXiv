{
  "date": "2025-06-12",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-12 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„ AI ç ”ç©¶å‘˜æœ‹å‹ã€‚ä»Šå¤© arXiv çš„æ›´æ–°å¯è°“æ˜¯â€œç¥ä»™æ‰“æ¶â€ï¼Œå°¤å…¶æ˜¯åœ¨ **LLM Scaling Lawï¼ˆæ‰©å±•å®šå¾‹ï¼‰**ã€**Reasoning Modelï¼ˆæ¨ç†æ¨¡å‹ï¼‰çš„è¯„ä¼°**ä»¥åŠ **AI å®‰å…¨çš„ç†è®ºè¾¹ç•Œ**æ–¹é¢æ¶Œç°äº†å¤šç¯‡é‡ç£…æ–‡ç« ã€‚\n\n**ä¸€å¥è¯æ€»ç»“ï¼š** ä»Šå¤©æˆ‘ä»¬è¿æ¥äº†æ¯” Chinchilla æ›´ç²¾å‡†çš„ Scaling Law \"Farseer\"ï¼Œè§è¯äº†é’ˆå¯¹ O1 ç±»æ¨ç†æ¨¡å‹çš„ä¿¡æ¯å­¦å¥¥èµ›çº§åŸºå‡†æµ‹è¯• OIBench çš„å‘å¸ƒï¼ŒåŒæ—¶ä¸€ç¯‡å…³äºâ€œå¯¹é½ä¸å¯èƒ½ä¸‰è§’â€çš„ç†è®ºæ–‡ç« å¼•èµ·äº†æˆ‘çš„è­¦è§‰ï¼›æ­¤å¤–ï¼Œç”Ÿç‰©æ­¦å™¨é£é™©å’Œ Mamba åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„åº”ç”¨ä¹Ÿæ˜¯ä»Šå¤©çš„çƒ­ç‚¹ã€‚\n\nä¸‹é¢æˆ‘ä»¬è¿›å…¥æ·±åº¦é˜…è¯»æ—¶é—´ï¼š\n\n---\n\n### ğŸš€ Scaling Law & Reasoning (æ‰©å±•å®šå¾‹ä¸æ¨ç†)\n\n**1. [æ¨è] é¢„æµ‹è§„æ¨¡ï¼šç¬¬äºŒéƒ¨åˆ†ï¼ŒFarseerâ€”â€”å¤§è¯­è¨€æ¨¡å‹ä¸­æ›´ç²¾ç»†çš„æ‰©å±•å®šå¾‹**\n**# Predictable Scale: Part II, Farseer: A Refined Scaling Law in Large Language Models**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** æŒ‘æˆ˜ Chinchilla å®šå¾‹ï¼Œå£°ç§°å¤–æ¨è¯¯å·®é™ä½äº† 433%ï¼ŒGoogle DeepMind ä¹‹åçš„åˆä¸€åŠ›ä½œï¼Ÿ\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** ä½œè€…æå‡ºäº† **Farseer**ï¼Œä¸€ç§æ–°çš„æ‰©å±•å®šå¾‹ï¼Œæ—¨åœ¨è§£å†³å°è§„æ¨¡å®éªŒæ— æ³•å‡†ç¡®é¢„æµ‹å¤§è§„æ¨¡ç”Ÿäº§ç³»ç»Ÿæ€§èƒ½çš„é—®é¢˜ã€‚é€šè¿‡æ„å»ºæ¨¡å‹æŸå¤±æ›²é¢ $L(N,D)$ï¼ŒFarseer åœ¨æ‹Ÿåˆç»éªŒæ•°æ®æ–¹é¢æ˜¾è‘—ä¼˜äº Chinchilla å®šå¾‹ã€‚å›¢é˜Ÿä¸ä»…åŠ¨ç”¨äº†çº¦ 300 ä¸‡ H100 GPU æ—¶è®­ç»ƒäº† 1000 ä¸ªæ¨¡å‹æ¥éªŒè¯ï¼Œè¿˜å¼€æºäº†æ‰€æœ‰æ•°æ®ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ç¡¬æ ¸çš„å„ç§è®­ç»ƒç­–ç•¥çš„â€œé¿å‘æŒ‡å—â€å’Œèµ„æºåˆ†é…å‚è€ƒã€‚\n\n**2. [æ¨è] OIBenchï¼šç”¨ä¿¡æ¯å­¦å¥¥èµ›åŸºå‡†æµ‹è¯•å¼ºæ¨ç†æ¨¡å‹**\n**# OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** ä¼ ç»Ÿçš„ä»£ç ç”Ÿæˆè¯„æµ‹å·²ç»æ»¡è¶³ä¸äº†ç°åœ¨çš„æ¨¡å‹äº†ï¼Œç›´æ¥ä¸Šå¥¥èµ›é¢˜ã€‚\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹ OpenAI o1 ç­‰å¼ºæ¨ç†æ¨¡å‹ï¼Œä¼ ç»Ÿçš„ç®—æ³•åŸºå‡†æµ‹è¯•å·²ç»è¶‹äºé¥±å’Œã€‚æœ¬æ–‡æ¨å‡ºäº† **OIBench**ï¼ŒåŒ…å« 250 é“ç²¾å¿ƒæŒ‘é€‰çš„åŸåˆ›ä¿¡æ¯å­¦å¥¥èµ›çº§åˆ«é¢˜ç›®ã€‚å®éªŒå‘ç°ï¼Œè™½ç„¶ SOTA æ¨¡å‹åœ¨æ­£ç¡®ç‡å’Œæ•ˆç‡ä¸Šå·²ç»è¶…è¿‡äº†å¤§å¤šæ•°äººç±»å‚èµ›è€…ï¼Œä½†è·ç¦»æ ‡å‡†ç­”æ¡ˆï¼ˆCanonical Solutionsï¼‰ä»æœ‰å·®è·ã€‚è¿™æ˜¯è¯„ä¼°â€œSystem 2â€æ…¢æ€è€ƒèƒ½åŠ›çš„ç»ä½³è¯•é‡‘çŸ³ã€‚\n\n**3. é€»è¾‘è§„åˆ’ï¼šLLM é€»è¾‘è§„åˆ’ä¸å…³ç³»æ¨ç†çš„ç»“æ„åŒ–åŸºå‡†**\n**# LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† **LogiPlan**ï¼Œç”¨äºè¯„ä¼° LLM åœ¨å¤æ‚å…³ç³»ç»“æ„ä¸Šçš„é€»è¾‘è§„åˆ’èƒ½åŠ›ã€‚æµ‹è¯•åŒ…æ‹¬è®¡åˆ’ç”Ÿæˆã€ä¸€è‡´æ€§æ£€æµ‹å’Œæ¯”è¾ƒé—®ç­”ã€‚æµ‹è¯„äº†åŒ…æ‹¬ DeepSeek R1ã€GPT-4.5ã€Claude 3.7 åœ¨å†…çš„ä¸€ä¼—æ¨¡å‹ï¼Œå‘ç°å³ä½¿æ˜¯ä¸»æ‰“æ¨ç†çš„æ¨¡å‹ï¼Œåœ¨é¢å¯¹è¿™å°±éœ€è¦æ·±åº¦é€»è¾‘è§„åˆ’çš„å¤æ‚é…ç½®æ—¶ï¼Œè¡¨ç°ä¾ç„¶æœ‰å¾ˆå¤§æå‡ç©ºé—´ã€‚\n\n**4. PREMISEï¼šå¤§æ¨¡å‹é«˜æ•ˆæ•°å­¦æ¨ç†çš„å¯æ‰©å±•ç­–ç•¥æç¤ºä¼˜åŒ–**\n**# PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹ CoTï¼ˆæ€ç»´é“¾ï¼‰è™½ç„¶æœ‰æ•ˆä½† token æ¶ˆè€—å·¨å¤§çš„é—®é¢˜ï¼Œæå‡ºäº† **PREMISE** æ¡†æ¶ã€‚é€šè¿‡åŸºäºæ¢¯åº¦çš„æç¤ºä¼˜åŒ–ï¼Œåœ¨ä¸å¾®è°ƒæ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä¼˜åŒ– Prompt æ¥å‡å°‘å†—ä½™è®¡ç®—ã€‚åœ¨ GSM8K ç­‰æ¦œå•ä¸Šï¼Œä¿æŒå‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ¨ç† token å‡å°‘äº† 87.5%ï¼Œæˆæœ¬é™ä½çº¦ 80%ã€‚è¿™å¯¹äºå·¥ä¸šç•Œè½åœ°éå¸¸å®ç”¨ã€‚\n\n---\n\n### ğŸ›¡ï¸ AI Safety, Alignment & Policy (å®‰å…¨ã€å¯¹é½ä¸æ”¿ç­–)\n\n**5. [é‡ç£…ç†è®º] å¯¹é½é™·é˜±ï¼šå¤æ‚æ€§å£å’**\n**# The Alignment Trap: Complexity Barriers**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ä¸ªä»¤äººä¸å®‰çš„è§‚ç‚¹ï¼šAI å¯¹é½ä¸ä»…ä»…æ˜¯éš¾ï¼Œè€Œæ˜¯å­˜åœ¨é€»è¾‘ä¸Šçš„â€œä¸å¯èƒ½æ€§â€ã€‚\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** ä½œè€…æå‡ºäº†â€œæšä¸¾æ‚–è®ºâ€ï¼ˆEnumeration Paradoxï¼‰å’Œäº”ä¸ªâ€œä¸å¯èƒ½æ€§æ”¯æŸ±â€ï¼ˆä»å‡ ä½•ã€è®¡ç®—ã€ç»Ÿè®¡ã€ä¿¡æ¯è®ºå’ŒåŠ¨åŠ›å­¦è§’åº¦ï¼‰ã€‚æ ¸å¿ƒè®ºç‚¹æ˜¯ï¼šå®‰å…¨ç­–ç•¥çš„é›†åˆåœ¨æµ‹åº¦ä¸Šä¸ºé›¶ï¼›éªŒè¯ç­–ç•¥å®‰å…¨æ€§æ˜¯ coNP-complete çš„ï¼›ä¸”è®­ç»ƒæ•°æ®å¤©ç„¶ç¼ºå¤±ï¼ˆå› ä¸ºéœ€è¦ç½•è§ç¾éš¾çš„æ ·æœ¬ï¼‰ã€‚è¿™ä¸ä»…æ˜¯æŠ€æœ¯æŒ‘æˆ˜ï¼Œæ›´æ˜¯æ•°å­¦ä¸Šçš„æ­»ç»“ã€‚å¦‚æœé€šè¿‡åŒè¡Œè¯„å®¡ï¼Œè¿™å°†æ˜¯å¯¹é½é¢†åŸŸçš„â€œå“¥å¾·å°”ä¸å®Œå¤‡å®šç†â€æ—¶åˆ»ã€‚\n\n**6. å½“ä»£ AI åŸºç¡€æ¨¡å‹å¢åŠ ç”Ÿç‰©æ­¦å™¨é£é™©**\n**# Contemporary AI foundation models increase biological weapons risk**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** åé©³äº†â€œåˆ¶é€ ç”Ÿç‰©æ­¦å™¨éœ€è¦éšæ€§çŸ¥è¯†ï¼ˆtacit knowledgeï¼‰â€çš„å‡è®¾ã€‚ä½œè€…å‘ç°ï¼ŒLlama 3.1 405Bã€GPT-4o å’Œ Claude 3.5 Sonnet å¯ä»¥å‡†ç¡®æŒ‡å¯¼ç”¨æˆ·ä»å•†ä¸šåˆæˆ DNA ä¸­æ¢å¤æ´»ä½“è„Šé«“ç°è´¨ç‚ç—…æ¯’ï¼ˆpoliovirusï¼‰ã€‚è¿™æŒ‘æˆ˜äº†å½“å‰æ¨¡å‹ç”Ÿç‰©å®‰å…¨é£é™©æä½çš„è¯´æ³•ï¼Œå‘¼åå»ºç«‹æ›´å®Œå–„çš„åŸºå‡†æµ‹è¯•ã€‚\n\n**7. åˆ†è§£æ”»å‡»ï¼šé€šè¿‡è½»é‡çº§é¡ºåºç›‘æ§é˜²å¾¡ LLM åˆ†è§£æ”»å‡»**\n**# Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** ç ”ç©¶äº†**åˆ†è§£æ”»å‡»ï¼ˆDecomposition Attacksï¼‰**ï¼Œå³æ”»å‡»è€…å°†æ¶æ„ç›®æ ‡æ‹†è§£ä¸ºçœ‹ä¼¼æ— å®³çš„å­ä»»åŠ¡æ¥ç»•è¿‡é˜²å¾¡ã€‚ä½œè€…å‘ç°è¿™ç§æ”»å‡»åœ¨ GPT-4o ä¸ŠæˆåŠŸç‡é«˜è¾¾ 87%ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„é¡ºåºç›‘æ§æ¡†æ¶ï¼Œèƒ½å¤Ÿä»¥è¾ƒä½çš„æˆæœ¬æœ‰æ•ˆé˜²å¾¡æ­¤ç±»æ”»å‡»ã€‚\n\n**8. SoKï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„è¶Šç‹±æŠ¤æ **\n**# SoK: Evaluating Jailbreak Guardrails for Large Language Models**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ç¯‡ç³»ç»ŸåŒ–çŸ¥è¯†ï¼ˆSoKï¼‰è®ºæ–‡ï¼Œé¦–æ¬¡å¯¹ LLM çš„è¶Šç‹±é˜²å¾¡æŠ¤æ ï¼ˆGuardrailsï¼‰è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†ç±»æ³•å’Œè¯„ä¼°æ¡†æ¶ï¼ˆå®‰å…¨æ€§-æ•ˆç‡-æ•ˆç”¨ï¼‰ã€‚\n\n---\n\n### ğŸ¬ Multimodal & Generation (å¤šæ¨¡æ€ä¸ç”Ÿæˆ)\n\n**9. Piscesï¼šç”¨äºå›¾åƒç†è§£ä¸ç”Ÿæˆçš„è‡ªå›å½’åŸºç¡€æ¨¡å‹**\n**# Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ **Pisces**ï¼Œè¯•å›¾åœ¨ä¸€ä¸ªæ¡†æ¶å†…åŒæ—¶æå®šå›¾åƒç†è§£å’Œç”Ÿæˆã€‚ä½œè€…é€šè¿‡è§£è€¦è§†è§‰ç¼–ç æ¶æ„ï¼ˆç†è§£å’Œç”Ÿæˆéœ€è¦ä¸åŒçš„è§†è§‰ç‰¹å¾ï¼‰è§£å†³äº†ç»Ÿä¸€æ¨¡å‹çš„æ€§èƒ½çŸ­æ¿ã€‚\n\n**10. M4Vï¼šç”¨äºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„å¤šæ¨¡æ€ Mamba**\n**# M4V: Multi-Modal Mamba for Text-to-Video Generation**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** Transformer å¤„ç†è§†é¢‘å¤ªæ…¢äº†ï¼ŸMamba æ¥äº†ã€‚**M4V** åˆ©ç”¨çº¿æ€§æ—¶é—´å¤æ‚åº¦çš„ Mamba æ¶æ„è¿›è¡Œè§†é¢‘ç”Ÿæˆï¼Œæå‡ºäº†å¤šæ¨¡æ€æ‰©æ•£ Mamba å—ã€‚ç›¸æ¯” Attention æœºåˆ¶ï¼ŒFLOPs é™ä½äº† 45%ï¼Œä¸”ç”Ÿæˆè´¨é‡ç›¸å½“ä¸é”™ã€‚\n\n**11. å¯¹ç§°æµåŒ¹é…ï¼šç»Ÿä¸€å›¾åƒç”Ÿæˆã€åˆ†å‰²ä¸åˆ†ç±»**\n**# Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† **SymmFlow**ï¼Œåˆ©ç”¨æµåŒ¹é…ï¼ˆFlow Matchingï¼‰æ¡†æ¶å°†ç”Ÿæˆã€åˆ†å‰²å’Œåˆ†ç±»ç»Ÿä¸€èµ·æ¥ã€‚é€šè¿‡å¯¹ç§°å­¦ä¹ ç›®æ ‡ï¼Œå®ç°äº†åŒå‘çš„ä¸€è‡´æ€§ï¼Œåœ¨ CelebAMask-HQ ä¸Šå–å¾—äº†å¾ˆå¥½çš„ FID åˆ†æ•°ã€‚\n\n---\n\n### ğŸ¤– Agents & Systems (æ™ºèƒ½ä½“ä¸ç³»ç»Ÿ)\n\n**12. Optimus-3ï¼šè¿ˆå‘æ‹¥æœ‰å¯æ‰©å±•ä»»åŠ¡ä¸“å®¶çš„é€šç”¨ Minecraft æ™ºèƒ½ä½“**\n**# Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** Minecraft ä¾ç„¶æ˜¯ Agent çš„ä¸»è¦ç»ƒå…µåœºã€‚**Optimus-3** å¼•å…¥äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„æ¥å¤„ç†å¼‚æ„ä»»åŠ¡ï¼Œå¹¶é…åˆå¤šæ¨¡æ€æ¨ç†å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ ã€‚å®ƒåœ¨ Minecraft çš„å¹¿æ³›ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„ SOTA æ™ºèƒ½ä½“ã€‚\n\n**13. SWE-Factoryï¼šä½ çš„è‡ªåŠ¨åŒ–å·¥å‚ï¼Œç”¨äº GitHub Issue è§£å†³çš„æ•°æ®ç”Ÿæˆ**\n**# SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** è§£å†³è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰Agent è®­ç»ƒæ•°æ®éš¾æçš„é—®é¢˜ã€‚æå‡ºäº†å…¨è‡ªåŠ¨æµæ°´çº¿ **SWE-Factory**ï¼Œèƒ½ä» GitHub issue ä¸­æ¢å¤æµ‹è¯•ç¯å¢ƒã€è¡¥å……ç¼ºå¤±çš„äºŒè¿›åˆ¶æ–‡ä»¶æ›´æ”¹ï¼Œå¹¶è‡ªåŠ¨éªŒè¯ patchã€‚è¿™å¯¹äºè®­ç»ƒåƒ Devin è¿™æ ·çš„ç¨‹åºå‘˜ Agent è‡³å…³é‡è¦ã€‚\n\n---\n\n### ğŸ§¬ AI for Science & Others (ç§‘å­¦æ™ºèƒ½ä¸å…¶å®ƒ)\n\n**14. æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹èƒ½èƒœä»»å»ºç­‘èƒ½æºç®¡ç†ä»»åŠ¡å—ï¼Ÿ**\n**# Can Time-Series Foundation Models Perform Building Energy Management Tasks?**\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** æ³¼å†·æ°´ç³»åˆ—ã€‚\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** ç°åœ¨çš„ Time-Series Foundation Models (TSFMs) å¾ˆç«ï¼Œä½†ä½œè€…åœ¨å»ºç­‘èƒ½æºç®¡ç†ï¼ˆBEMï¼‰ä»»åŠ¡ä¸Šæµ‹äº†ä¸€åœˆï¼Œå‘ç°å®ƒä»¬ç›¸æ¯”ä¼ ç»Ÿçš„ç»Ÿè®¡æ¨¡å‹ä¼˜åŠ¿å¾®ä¹å…¶å¾®ï¼Œç”šè‡³åœ¨åŒ…å«åå˜é‡æ—¶è¡¨ç°æ›´å·®ã€‚è¿™æç¤ºæˆ‘ä»¬ï¼ŒTSFM åœ¨ç‰¹å®šé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›å¯èƒ½è¢«é«˜ä¼°äº†ã€‚\n\n**15. BioClinical ModernBERTï¼šç”Ÿç‰©åŒ»å­¦ä¸ä¸´åºŠ NLP çš„é•¿ä¸Šä¸‹æ–‡ç¼–ç å™¨**\n**# BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** åŸºäºæœ€æ–°çš„ ModernBERTï¼Œåœ¨ 535 äº¿ token çš„ç”Ÿç‰©åŒ»å­¦è¯­æ–™ä¸Šç»§ç»­é¢„è®­ç»ƒã€‚ä¸»è¦è§£å†³é•¿ä¸Šä¸‹æ–‡å¤„ç†é—®é¢˜ï¼Œå‘å¸ƒäº† Base å’Œ Large ä¸¤ä¸ªç‰ˆæœ¬ï¼Œæ˜¯ç›®å‰è¯¥é¢†åŸŸæœ€å¼ºçš„ç¼–ç å™¨ä¹‹ä¸€ã€‚\n\n**16. æ‰“ç ´ååˆ†å­ï¼šMLLM å‡†å¤‡å¥½è¿›è¡Œåˆ†å­å»æ¯’åŒ–äº†å—ï¼Ÿ**\n**# Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?**\n> **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº† **ToxiMol** åŸºå‡†ï¼Œæµ‹è¯•å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰èƒ½å¦åœ¨ä¿æŒåˆ†å­åŠŸèƒ½çš„åŒæ—¶å»é™¤æ¯’æ€§ç»“æ„ã€‚ç»“è®ºæ˜¯ï¼šç›®å‰çš„æ¨¡å‹åœ¨ç†è§£æ¯’æ€§æ–¹é¢è¿˜è¡Œï¼Œä½†åœ¨çœŸæ­£è¿›è¡Œç»“æ„ç¼–è¾‘å’Œå»æ¯’æ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚\n\n---\n\n### ğŸ’¡ æ‚è°ˆ\nä»Šå¤©çš„è®ºæ–‡åæ˜ å‡ºå‡ ä¸ªæ˜æ˜¾çš„è¶‹åŠ¿ï¼š\n1.  **æ¨ç†çš„â€œç³»ç»Ÿ2â€åŒ–**ï¼šå¤§å®¶ä¸å†æ»¡è¶³äºç®€å•çš„ QAï¼Œè€Œæ˜¯é€šè¿‡å¥¥èµ›é¢˜ã€é€»è¾‘è§„åˆ’ç­‰æµ‹è¯•æ¨¡å‹çš„æ·±å±‚æ¨ç†ã€‚\n2.  **å®‰å…¨ç ”ç©¶çš„æ·±åŒ–**ï¼šä»ç®€å•çš„çº¢é˜Ÿæµ‹è¯•è½¬å‘äº†ç†è®ºä¸Šçš„ä¸å¯èƒ½æ€§è¯æ˜å’Œç³»ç»ŸåŒ–çš„é˜²å¾¡æ¶æ„ã€‚\n3.  **æ¶æ„çš„æ¼”è¿›**ï¼šMamba åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨ï¼Œä»¥åŠæµåŒ¹é…ï¼ˆFlow Matchingï¼‰åœ¨ç»Ÿä¸€è§†è§‰ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œéƒ½æ˜¾ç¤ºäº† Transformer ä¹‹å¤–çš„å¯èƒ½æ€§ã€‚\n\nå¸Œæœ›è¿™ä»½å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼æˆ‘ä»¬æ˜å¤©è§ã€‚",
  "papers": [
    {
      "arxiv_id": "2506.22448v1",
      "title": "Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems",
      "title_zh": "åŸºäºæ— ç›‘ç£å­¦ä¹ çš„ RIS è¾…åŠ© MISO-OFDMA ç³»ç»Ÿè”åˆèµ„æºåˆ†é…ä¸æ³¢æŸèµ‹å½¢è®¾è®¡",
      "authors": [
        "Yu Ma",
        "Xingyu Zhou",
        "Xiao Li",
        "Le Liang",
        "Shi Jin"
      ],
      "abstract": "Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless systems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA system, addressing resource allocation challenges. A two-stage unsupervised learning-based framework is proposed to jointly design RIS phase shifts, BS beamforming, and resource block (RB) allocation. The framework includes BeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which allocates RBs using equivalent CSI derived from BeamNet outputs. Active beamforming is implemented via maximum ratio transmission and water-filling. To handle discrete constraints while ensuring differentiability, quantization and the Gumbel-softmax trick are adopted. A customized loss and phased training enhance performance under QoS constraints. Simulations show the method achieves 99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and it remains robust across varying channel and user conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯é‡æ„æ™ºèƒ½è¡¨é¢(RIS)è¾…åŠ©çš„MISO-OFDMAç³»ç»Ÿï¼Œæ¢è®¨äº†6Gæ— çº¿é€šä¿¡ä¸­é¢ä¸´çš„è”åˆèµ„æºåˆ†é…æŒ‘æˆ˜ã€‚æå‡ºäº†ä¸€ç§åŸºäºæ— ç›‘ç£å­¦ä¹ (Unsupervised Learning)çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨ååŒè®¾è®¡RISç›¸ä½åç§»ã€åŸºç«™(BS)æ³¢æŸèµ‹å½¢å’Œèµ„æºå—(RB)åˆ†é…ã€‚è¯¥æ¡†æ¶é€šè¿‡BeamNetä»ä¿¡é“çŠ¶æ€ä¿¡æ¯(CSI)ä¸­é¢„æµ‹ç›¸ä½ï¼Œå¹¶åˆ©ç”¨AllocationNetåŸºäºç­‰æ•ˆä¿¡é“ä¿¡æ¯å®Œæˆèµ„æºåˆ†é…ã€‚ç ”ç©¶ä¸­é‡‡ç”¨äº†æœ€å¤§æ¯”ä¼ è¾“(MRT)å’Œæ³¨æ°´ç®—æ³•(Water-filling)å®ç°æœ‰åŠŸæ³¢æŸèµ‹å½¢ï¼Œå¹¶åˆ©ç”¨Gumbel-softmaxæŠ€å·§è§£å†³ç¦»æ•£çº¦æŸçš„å¾®åˆ†é—®é¢˜ã€‚é€šè¿‡å®šåˆ¶åŒ–çš„æŸå¤±å‡½æ•°å’Œåˆ†é˜¶æ®µè®­ç»ƒï¼Œè¯¥æ–¹æ¡ˆæœ‰æ•ˆæå‡äº†QoSçº¦æŸä¸‹çš„ç³»ç»Ÿæ€§èƒ½ã€‚ä»¿çœŸå®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒä¿¡é“å’Œç”¨æˆ·æ¡ä»¶ä¸‹å‡è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ï¼Œåœ¨ä»…æ¶ˆè€—åŸºå‡†SCAç®—æ³•0.036%è¿è¡Œæ—¶é—´çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†å…¶99.93%çš„æ€»é€Ÿç‡ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "eess.SP",
      "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920 characters\", the abstract here is shorter than that in the PDF file",
      "pdf_url": "https://arxiv.org/pdf/2506.22448v1",
      "published_date": "2025-06-12 23:50:38 UTC",
      "updated_date": "2025-06-12 23:50:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:10:51.400368+00:00"
    },
    {
      "arxiv_id": "2506.11331v2",
      "title": "MUDAS: Mote-scale Unsupervised Domain Adaptation in Multi-label Sound Classification",
      "title_zh": "MUDASï¼šå¤šæ ‡ç­¾å£°éŸ³åˆ†ç±»ä¸­çš„å¾®å°˜çº§æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”",
      "authors": [
        "Jihoon Yun",
        "Chengzhang Li",
        "Dhrubojyoti Roy",
        "Anish Arora"
      ],
      "abstract": "Unsupervised Domain Adaptation (UDA) is essential for adapting machine learning models to new, unlabeled environments where data distribution shifts can degrade performance. Existing UDA algorithms are designed for single-label tasks and rely on significant computational resources, limiting their use in multi-label scenarios and in resource-constrained IoT devices. Overcoming these limitations is particularly challenging in contexts such as urban sound classification, where overlapping sounds and varying acoustics require robust, adaptive multi-label capabilities on low-power, on-device systems. To address these limitations, we introduce Mote-scale Unsupervised Domain Adaptation for Sounds (MUDAS), a UDA framework developed for multi-label sound classification in resource-constrained IoT settings. MUDAS efficiently adapts models by selectively retraining the classifier in situ using high-confidence data, minimizing computational and memory requirements to suit on-device deployment. Additionally, MUDAS incorporates class-specific adaptive thresholds to generate reliable pseudo-labels and applies diversity regularization to improve multi-label classification accuracy. In evaluations on the SONYC Urban Sound Tagging (SONYC-UST) dataset recorded at various New York City locations, MUDAS demonstrates notable improvements in classification accuracy over existing UDA algorithms, achieving good performance in a resource-constrained IoT setting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MUDASï¼ˆMote-scale Unsupervised Domain Adaptation for Soundsï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºèµ„æºå—é™çš„ IoT ç¯å¢ƒä¸‹å¤šæ ‡ç­¾å£°éŸ³åˆ†ç±»ä»»åŠ¡è®¾è®¡çš„æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”ï¼ˆUnsupervised Domain Adaptation, UDAï¼‰æ¡†æ¶ã€‚MUDAS æ—¨åœ¨è§£å†³ç°æœ‰ UDA ç®—æ³•åœ¨å¤„ç†å¤šæ ‡ç­¾åœºæ™¯ä»¥åŠä½åŠŸè€—è®¾å¤‡éƒ¨ç½²æ—¶é¢ä¸´çš„è®¡ç®—èµ„æºéœ€æ±‚é«˜å’Œå¤šæ ‡ç­¾åˆ†ç±»ç²¾åº¦ä¸‹é™ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨ç°åœºåˆ©ç”¨é«˜ç½®ä¿¡åº¦æ•°æ®é€‰æ‹©æ€§åœ°é‡æ–°è®­ç»ƒåˆ†ç±»å™¨ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œä½¿å…¶éå¸¸é€‚åˆåœ¨è®¾å¤‡ç«¯è¿›è¡Œç›´æ¥éƒ¨ç½²ã€‚æ­¤å¤–ï¼ŒMUDAS å¼•å…¥äº†ç±»åˆ«ç‰¹å®šçš„è‡ªé€‚åº”é˜ˆå€¼ï¼ˆclass-specific adaptive thresholdsï¼‰æ¥ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾ï¼Œå¹¶ç»“åˆå¤šæ ·æ€§æ­£åˆ™åŒ–ï¼ˆdiversity regularizationï¼‰æŠ€æœ¯æ¥æå‡åˆ†ç±»æ€§èƒ½ã€‚åœ¨ SONYC Urban Sound Tagging (SONYC-UST) æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMUDAS åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ UDA ç®—æ³•ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨èµ„æºæåº¦å—é™çš„ IoT è®¾å®šä¸­å®ç°å‡ºè‰²çš„å¤šæ ‡ç­¾å£°éŸ³è¯†åˆ«æ•ˆæœã€‚",
      "categories": [
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "BuildSys 25",
      "pdf_url": "https://arxiv.org/pdf/2506.11331v2",
      "published_date": "2025-06-12 22:02:07 UTC",
      "updated_date": "2025-11-14 15:31:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:10:57.275636+00:00"
    },
    {
      "arxiv_id": "2506.12100v2",
      "title": "LLM Embedding-based Attribution (LEA): Quantifying Source Contributions to Generative Model's Response for Vulnerability Analysis",
      "title_zh": "åŸºäº LLM åµŒå…¥çš„å½’å›  (LEA)ï¼šé‡åŒ–æ¼æ´åˆ†æä¸­ç”Ÿæˆæ¨¡å‹å“åº”çš„æ¥æºè´¡çŒ®",
      "authors": [
        "Reza Fayyazi",
        "Michael Zuzak",
        "Shanchieh Jay Yang"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used for cybersecurity threat analysis, but their deployment in security-sensitive environments raises trust and safety concerns. With over 21,000 vulnerabilities disclosed in 2025, manual analysis is infeasible, making scalable and verifiable AI support critical. When querying LLMs, dealing with emerging vulnerabilities is challenging as they have a training cut-off date. While Retrieval-Augmented Generation (RAG) can inject up-to-date context to alleviate the cut-off date limitation, it remains unclear how much LLMs rely on retrieved evidence versus the model's internal knowledge, and whether the retrieved information is meaningful or even correct. This uncertainty could mislead security analysts, mis-prioritize patches, and increase security risks. Therefore, this work proposes LLM Embedding-based Attribution (LEA) to analyze the generated responses for vulnerability exploitation analysis. More specifically, LEA quantifies the relative contribution of internal knowledge vs. retrieved content in the generated responses. We evaluate LEA on 500 critical vulnerabilities disclosed between 2016 and 2025, across three RAG settings -- valid, generic, and incorrect -- using three state-of-the-art LLMs. Our results demonstrate LEA's ability to detect clear distinctions between non-retrieval, generic-retrieval, and valid-retrieval scenarios with over 95% accuracy on larger models. Finally, we demonstrate the limitations posed by incorrect retrieval of vulnerability information and raise a cautionary note to the cybersecurity community regarding the blind reliance on LLMs and RAG for vulnerability analysis. LEA offers security analysts with a metric to audit RAG-enhanced workflows, improving the transparent and trustworthy deployment of AI in cybersecurity threat analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLM Embedding-based Attribution (LEA)ï¼Œä¸€ç§æ—¨åœ¨é‡åŒ–å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„å“åº”ä¸­å†…éƒ¨çŸ¥è¯†ä¸æ£€ç´¢ä¸Šä¸‹æ–‡(RAG)ç›¸å¯¹è´¡çŒ®çš„åˆ†ææ¡†æ¶ã€‚è¯¥å·¥ä½œé’ˆå¯¹ç½‘ç»œå®‰å…¨é¢†åŸŸä¸­LLMså¤„ç†æ¼æ´åˆ†ææ—¶å­˜åœ¨çš„çŸ¥è¯†æˆªæ–­å’Œæ£€ç´¢ä¿¡æ¯ä¸å¯ä¿¡ç­‰é—®é¢˜ï¼Œé€šè¿‡LEAå¯¹ç”Ÿæˆå†…å®¹è¿›è¡Œæ¥æºå½’å› ï¼Œä»¥è§£å†³å®‰å…¨æ•æ„Ÿç¯å¢ƒä¸‹çš„ä¿¡ä»»æŒ‘æˆ˜ã€‚å®éªŒåœ¨2016å¹´è‡³2025å¹´é—´çš„500ä¸ªå…³é”®æ¼æ´ä¸Šï¼Œé’ˆå¯¹æœ‰æ•ˆã€é€šç”¨å’Œé”™è¯¯ä¸‰ç§RAGè®¾ç½®åŠä¸»æµæ¨¡å‹è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒLEAåœ¨å¤§å‹æ¨¡å‹ä¸Šèƒ½ä»¥è¶…è¿‡95%çš„å‡†ç¡®ç‡åŒºåˆ†ä¸åŒçš„æ£€ç´¢åœºæ™¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ­ç¤ºäº†é”™è¯¯æ£€ç´¢ä¿¡æ¯å¯¹å®‰å…¨åˆ†æçš„æ½œåœ¨è¯¯å¯¼é£é™©ï¼Œå¹¶æé†’å®‰å…¨ç¤¾åŒºå®¡æ…å¯¹å¾…ç›²ç›®ä¾èµ–ã€‚ç»¼ä¸Šæ‰€è¿°ï¼ŒLEAä¸ºå®‰å…¨åˆ†æå¸ˆæä¾›äº†ä¸€ç§å®¡è®¡RAGå¢å¼ºå·¥ä½œæµçš„åº¦é‡æ ‡å‡†ï¼Œæ˜¾è‘—æå‡äº†AIåœ¨ç½‘ç»œå®‰å…¨å¨èƒåˆ†æä¸­éƒ¨ç½²çš„é€æ˜åº¦ä¸å¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12100v2",
      "published_date": "2025-06-12 21:20:10 UTC",
      "updated_date": "2025-09-03 17:18:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:11:31.374429+00:00"
    },
    {
      "arxiv_id": "2506.11305v2",
      "title": "Don't Pay Attention",
      "title_zh": "æ— éœ€æ³¨æ„åŠ›",
      "authors": [
        "Mohammad Hammoud",
        "Devang Acharya"
      ],
      "abstract": "The Transformer has become the de facto standard for modern language models owing to its parallelizable training and effective autoregressive decoding. However, its fixed context window and the quadratic time and memory costs of its self-attention mechanism remain central bottlenecks. These constraints have revived interest in recurrent architectures that scale linearly with sequence length, but at the cost of reduced parallelism. In this paper, we introduce Avey, a new foundational architecture that breaks away from both attention and recurrence. Avey pairs a ranker with an autoregressive neural processor to select and contextualize only the most relevant tokens for any given token. Specifically, it decouples sequence length from context width, thus enabling effective and efficient processing of arbitrarily long sequences. Results show that Avey compares favorably to the Transformer across a variety of standard short-range NLP benchmarks, while significantly outperforming it on tasks requiring long-range dependency modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Transformer æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶é¢ä¸´çš„å›ºå®šä¸Šä¸‹æ–‡çª—å£ä»¥åŠè‡ªæ³¨æ„åŠ›æœºåˆ¶ (self-attention) å¸¦æ¥çš„äºŒæ¬¡æ–¹æ—¶é—´å’Œå†…å­˜å¼€é”€ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„åŸºç¡€æ¶æ„ Aveyã€‚Avey å½»åº•æ‰“ç ´äº†ä¼ ç»Ÿçš„æ³¨æ„åŠ› (attention) å’Œé€’å½’ (recurrence) æ¨¡å¼ï¼Œé€šè¿‡å°†æ’åå™¨ (ranker) ä¸è‡ªå›å½’ç¥ç»å¤„ç†å™¨ (autoregressive neural processor) ç›¸ç»“åˆï¼Œä»…é€‰æ‹©å¹¶æƒ…å¢ƒåŒ–ä¸å½“å‰ token æœ€ç›¸å…³çš„éƒ¨åˆ†ã€‚è¿™ç§è®¾è®¡å®ç°äº†åºåˆ—é•¿åº¦ (sequence length) ä¸ä¸Šä¸‹æ–‡å®½åº¦ (context width) çš„æœ‰æ•ˆè§£è€¦ï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAvey åœ¨å¤šç§æ ‡å‡†çŸ­ç¨‹ NLP åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨é•¿ç¨‹ä¾èµ–å»ºæ¨¡ (long-range dependency modeling) ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº† Transformerã€‚è¯¥æ¶æ„ä¸ºè§£å†³ç°ä»£è¯­è¨€æ¨¡å‹çš„æ‰©å±•æ€§éš¾é¢˜æä¾›äº†å…·æœ‰ç«äº‰åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11305v2",
      "published_date": "2025-06-12 21:11:06 UTC",
      "updated_date": "2025-11-14 20:06:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:11:04.858712+00:00"
    },
    {
      "arxiv_id": "2506.11302v3",
      "title": "TARDIS STRIDE: A Spatio-Temporal Road Image Dataset and World Model for Autonomy",
      "title_zh": "TARDIS STRIDEï¼šé¢å‘è‡ªä¸»æ€§çš„æ—¶ç©ºé“è·¯å›¾åƒæ•°æ®é›†ä¸ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "HÃ©ctor CarriÃ³n",
        "Yutong Bai",
        "VÃ­ctor A. HernÃ¡ndez Castro",
        "Kishan Panaganti",
        "Ayush Zenith",
        "Matthew Trang",
        "Tony Zhang",
        "Pietro Perona",
        "Jitendra Malik"
      ],
      "abstract": "World models aim to simulate environments and enable effective agent behavior. However, modeling real-world environments presents unique challenges as they dynamically change across both space and, crucially, time. To capture these composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for Exploration (STRIDE) permuting 360-degree panoramic imagery into rich interconnected observation, state and action nodes. Leveraging this structure, we can simultaneously model the relationship between egocentric views, positional coordinates, and movement commands across both space and time. We benchmark this dataset via TARDIS, a transformer-based generative world model that integrates spatial and temporal dynamics through a unified autoregressive framework trained on STRIDE. We demonstrate robust performance across a range of agentic tasks such as controllable photorealistic image synthesis, instruction following, autonomous self-control, and state-of-the-art georeferencing. These results suggest a promising direction towards sophisticated generalist agents--capable of understanding and manipulating the spatial and temporal aspects of their material environments--with enhanced embodied reasoning capabilities. Training code, datasets, and model checkpoints are made available at https://huggingface.co/datasets/Tera-AI/STRIDE.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† STRIDE (Spatio-Temporal Road Image Dataset for Exploration)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ•æ‰ç°å®ç¯å¢ƒåŠ¨æ€å˜åŒ–è€Œè®¾è®¡çš„æ—¶ç©ºé“è·¯å›¾åƒæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†å°† 360 åº¦å…¨æ™¯å›¾åƒè½¬åŒ–ä¸ºäº’è¿çš„è§‚æµ‹ã€çŠ¶æ€ä¸åŠ¨ä½œèŠ‚ç‚¹ï¼Œå®ç°äº†å¯¹è‡ªæˆ‘è§†è§’ã€åœ°ç†åæ ‡åŠç§»åŠ¨æŒ‡ä»¤åœ¨æ—¶ç©ºç»´åº¦ä¸Šçš„åŒæ­¥å»ºæ¨¡ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æå‡ºäº† TARDISï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Transformer çš„ç”Ÿæˆå¼ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡åœ¨ STRIDE æ•°æ®é›†ä¸Šè¿›è¡Œè‡ªå›å½’è®­ç»ƒï¼Œå®ç°äº†æ—¶ç©ºåŠ¨æ€çš„æ·±åº¦æ•´åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTARDIS åœ¨å¯æ§å…‰å½±çœŸå®å›¾åƒåˆæˆã€æŒ‡ä»¤éµå¾ªã€è‡ªä¸»æ§åˆ¶ä»¥åŠ georeferencing ç­‰å¤šé¡¹ä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºå…·å¤‡å¢å¼ºå‹ embodied reasoning èƒ½åŠ›ã€èƒ½æœ‰æ•ˆç†è§£å¹¶æ“çºµå¤æ‚ç‰©è´¨ç¯å¢ƒçš„é€šç”¨å‹æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Computer Vision, Pattern Recognition, Early-Fusion, Dataset, Data Augmentation",
      "pdf_url": "https://arxiv.org/pdf/2506.11302v3",
      "published_date": "2025-06-12 21:08:11 UTC",
      "updated_date": "2025-06-19 15:10:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:11:01.951004+00:00"
    },
    {
      "arxiv_id": "2506.11300v1",
      "title": "Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning",
      "title_zh": "è¶…è¶Šéšæœºé‡‡æ ·ï¼šåŸºäºè¯¾ç¨‹å­¦ä¹ çš„é«˜æ•ˆè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ",
      "authors": [
        "Yang Zhang",
        "Amr Mohamed",
        "Hadi Abdine",
        "Guokan Shang",
        "Michalis Vazirgiannis"
      ],
      "abstract": "Curriculum learning has shown promise in improving training efficiency and generalization in various machine learning domains, yet its potential in pretraining language models remains underexplored, prompting our work as the first systematic investigation in this area. We experimented with different settings, including vanilla curriculum learning, pacing-based sampling, and interleaved curricula-guided by six difficulty metrics spanning linguistic and information-theoretic perspectives. We train models under these settings and evaluate their performance on eight diverse benchmarks. Our experiments reveal that curriculum learning consistently improves convergence in early and mid-training phases, and can yield lasting gains when used as a warmup strategy with up to $3.5\\%$ improvement. Notably, we identify compression ratio, lexical diversity, and readability as effective difficulty signals across settings. Our findings highlight the importance of data ordering in large-scale pretraining and provide actionable insights for scalable, data-efficient model development under realistic training scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­è¯¾ç¨‹å­¦ä¹ (Curriculum Learning)åº”ç”¨æ¢ç´¢ä¸è¶³çš„ç°çŠ¶ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è°ƒæŸ¥äº†å…¶åœ¨æé«˜è®­ç»ƒæ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶å›¢é˜Ÿå®éªŒäº†åŒ…æ‹¬åŸç”Ÿè¯¾ç¨‹å­¦ä¹ ã€åŸºäºé€Ÿåº¦çš„é‡‡æ ·(pacing-based sampling)ä»¥åŠäº¤å‰è¯¾ç¨‹(interleaved curricula)åœ¨å†…çš„å¤šç§è®¾ç½®ï¼Œå¹¶å¼•å…¥äº†æ¶µç›–è¯­è¨€å­¦å’Œä¿¡æ¯è®ºç»´åº¦çš„å…­ç§éš¾åº¦æŒ‡æ ‡(difficulty metrics)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‹ç¼©æ¯”(compression ratio)ã€è¯æ±‡å¤šæ ·æ€§(lexical diversity)å’Œæ˜“è¯»æ€§(readability)æ˜¯è·¨åœºæ™¯ä¸‹æœ€ä¸ºæœ‰æ•ˆçš„éš¾åº¦ä¿¡å·ã€‚ç ”ç©¶å‘ç°è¯¾ç¨‹å­¦ä¹ èƒ½æ˜¾è‘—æå‡è®­ç»ƒåˆæœŸå’Œä¸­æœŸçš„æ”¶æ•›é€Ÿåº¦ï¼Œå½“ä½œä¸ºé¢„çƒ­ç­–ç•¥(warmup strategy)ä½¿ç”¨æ—¶ï¼Œåœ¨å…«é¡¹å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸­æœ€é«˜å¯å¸¦æ¥3.5%çš„æŒç»­æ€§èƒ½æå‡ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­æ•°æ®æ’åº(data ordering)çš„é‡è¦æ€§ï¼Œä¸ºå®ç°å¯æ‰©å±•ä¸”é«˜æ•°æ®æ•ˆç‡çš„æ¨¡å‹å¼€å‘æä¾›äº†å…·æœ‰æ“ä½œæ€§çš„å®è·µè§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11300v1",
      "published_date": "2025-06-12 21:06:57 UTC",
      "updated_date": "2025-06-12 21:06:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:11:02.634830+00:00"
    },
    {
      "arxiv_id": "2506.11295v1",
      "title": "A Tale of Two Systems: Characterizing Architectural Complexity on Machine Learning-Enabled Systems",
      "title_zh": "åŒç³»ç»Ÿè®°ï¼šæœºå™¨å­¦ä¹ èµ‹èƒ½ç³»ç»Ÿçš„æ¶æ„å¤æ‚åº¦è¡¨å¾",
      "authors": [
        "Renato Cordeiro Ferreira"
      ],
      "abstract": "How can the complexity of ML-enabled systems be managed effectively? The goal of this research is to investigate how complexity affects ML-Enabled Systems (MLES). To address this question, this research aims to introduce a metrics-based architectural model to characterize the complexity of MLES. The goal is to support architectural decisions, providing a guideline for the inception and growth of these systems. This paper brings, side-by-side, the architecture representation of two systems that can be used as case studies for creating the metrics-based architectural model: the SPIRA and the Ocean Guard MLES.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆç®¡ç†æœºå™¨å­¦ä¹ èµ‹èƒ½ç³»ç»Ÿ(ML-enabled systems, MLES)çš„å¤æ‚æ€§ï¼Œæ—¨åœ¨å¼•å…¥ä¸€ç§åŸºäºæŒ‡æ ‡çš„æ¶æ„æ¨¡å‹(metrics-based architectural model)æ¥è¡¨å¾å…¶æ¶æ„ç‰¹å¾ã€‚è¯¥æ¨¡å‹çš„ä¸»è¦ç›®æ ‡æ˜¯æ”¯æŒæ¶æ„å†³ç­–ï¼Œå¹¶ä¸ºè¿™ç±»ç³»ç»Ÿçš„åˆæœŸè®¾è®¡ä¸æŒç»­å¢é•¿æä¾›æ˜ç¡®çš„æŒ‡å¯¼åŸåˆ™ã€‚è®ºæ–‡é€šè¿‡å¯¹æ¯” SPIRA å’Œ Ocean Guard ä¸¤ä¸ªç³»ç»Ÿçš„æ¶æ„è¡¨ç¤ºï¼Œå°†å…¶ä½œä¸ºæ„å»ºè¯¥åº¦é‡æ¨¡å‹çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œä»è€Œæ·±å…¥è°ƒæŸ¥å¤æ‚æ€§å¯¹ MLES çš„å…·ä½“å½±å“ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶ä¸ºç†è§£å’Œåº”å¯¹æ™ºèƒ½ç³»ç»Ÿçš„å¤æ‚æ€§æä¾›äº†ç³»ç»ŸåŒ–çš„æ–¹æ³•è®ºï¼Œæ—¨åœ¨æå‡è½¯ä»¶æ¶æ„åœ¨ MLES ç”Ÿå‘½å‘¨æœŸä¸­çš„å†³ç­–è´¨é‡ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "8 pages, 3 figures (3 diagrams), submitted to the ECSA2025. arXiv admin note: substantial text overlap with arXiv:2506.08153",
      "pdf_url": "https://arxiv.org/pdf/2506.11295v1",
      "published_date": "2025-06-12 20:54:28 UTC",
      "updated_date": "2025-06-12 20:54:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:11:10.578787+00:00"
    },
    {
      "arxiv_id": "2506.11266v1",
      "title": "Invocable APIs derived from NL2SQL datasets for LLM Tool-Calling Evaluation",
      "title_zh": "åŸºäº NL2SQL æ•°æ®é›†è¡ç”Ÿçš„å¯è°ƒç”¨ APIï¼šç”¨äºå¤§è¯­è¨€æ¨¡å‹å·¥å…·è°ƒç”¨è¯„ä¼°",
      "authors": [
        "Benjamin Elder",
        "Anupama Murthi",
        "Jungkoo Kang",
        "Ankita Rajaram Naik",
        "Kiran Kate",
        "Kinjal Basu",
        "Danish Contractor"
      ],
      "abstract": "Large language models (LLMs) are routinely deployed as agentic systems, with access to tools that interact with live environments to accomplish tasks. In enterprise deployments these systems need to interact with API collections that can be extremely large and complex, often backed by databases. In order to create datasets with such characteristics, we explore how existing NL2SQL (Natural Language to SQL query) datasets can be used to automatically create NL2API datasets. Specifically, this work describes a novel data generation pipeline that exploits the syntax of SQL queries to construct a functionally equivalent sequence of API calls. We apply this pipeline to one of the largest NL2SQL datasets, BIRD-SQL to create a collection of over 2500 APIs that can be served as invocable tools or REST-endpoints. We pair natural language queries from BIRD-SQL to ground-truth API sequences based on this API pool. We use this collection to study the performance of 10 public LLMs and find that all models struggle to determine the right set of tools (consisting of tasks of intent detection, sequencing with nested function calls, and slot-filling). We find that models have extremely low task completion rates (7-47 percent - depending on the dataset) which marginally improves to 50 percent when models are employed as ReACT agents that interact with the live API environment. The best task completion rates are far below what may be required for effective general-use tool-calling agents, suggesting substantial scope for improvement in current state-of-the-art tool-calling LLMs. We also conduct detailed ablation studies, such as assessing the impact of the number of tools available as well as the impact of tool and slot-name obfuscation. We compare the performance of models on the original SQL generation tasks and find that current models are sometimes able to exploit SQL better than APIs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ç°æœ‰çš„ NL2SQL æ•°æ®é›†è‡ªåŠ¨ç”Ÿæˆ NL2API æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä¼ä¸šçº§ API é›†åˆæ—¶çš„å·¥å…·è°ƒç”¨ï¼ˆTool-Callingï¼‰èƒ½åŠ›ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ•°æ®ç”Ÿæˆæµæ°´çº¿ï¼ˆData Generation Pipelineï¼‰ï¼Œé€šè¿‡åˆ©ç”¨ SQL æŸ¥è¯¢çš„è¯­æ³•æ„å»ºåœ¨åŠŸèƒ½ä¸Šç­‰æ•ˆçš„ API è°ƒç”¨åºåˆ—ã€‚ä½œè€…å°†è¯¥æµæ°´çº¿åº”ç”¨äº BIRD-SQL æ•°æ®é›†ï¼Œåˆ›å»ºäº†åŒ…å«è¶…è¿‡ 2500 ä¸ªå¯è°ƒç”¨ API çš„é›†åˆï¼Œå¯ä½œä¸ºå¯è°ƒç”¨å·¥å…·æˆ– REST-endpoints ä½¿ç”¨ã€‚ç ”ç©¶åˆ©ç”¨è¯¥é›†åˆå¯¹ 10 ä¸ªå…¬å¼€ LLMs è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°æ¨¡å‹åœ¨æ„å›¾æ£€æµ‹ï¼ˆIntent Detectionï¼‰ã€åµŒå¥—å‡½æ•°è°ƒç”¨åºåˆ—åŒ–ä»¥åŠæ§½ä½å¡«å……ï¼ˆSlot-fillingï¼‰æ–¹é¢å‡è¡¨ç°æ¬ ä½³ã€‚å®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹çš„ä»»åŠ¡å®Œæˆç‡æ™®éè¾ƒä½ï¼ˆä»… 7% è‡³ 47%ï¼‰ï¼Œå³ä¾¿é‡‡ç”¨ ReACT æ¨¡å¼ä¹Ÿä»…èƒ½æå‡è‡³çº¦ 50%ã€‚è¿™ä¸€ç»“è®ºè¡¨æ˜å½“å‰çš„ LLMs åœ¨æ‰§è¡Œé€šç”¨å·¥å…·è°ƒç”¨ä»»åŠ¡æ–¹é¢ä»æœ‰æ˜¾è‘—æå‡ç©ºé—´ï¼Œä¸”åœ¨æŸäº›åœºæ™¯ä¸‹æ¨¡å‹å¯¹ SQL çš„åˆ©ç”¨æ•ˆç‡ç”šè‡³é«˜äº APIã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "10+32 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.11266v1",
      "published_date": "2025-06-12 20:17:52 UTC",
      "updated_date": "2025-06-12 20:17:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:11:22.949870+00:00"
    },
    {
      "arxiv_id": "2506.11261v1",
      "title": "Gondola: Grounded Vision Language Planning for Generalizable Robotic Manipulation",
      "title_zh": "Gondolaï¼šé¢å‘å¯æ³›åŒ–æœºå™¨äººæ“æ§çš„å…·èº«è§†è§‰è¯­è¨€è§„åˆ’",
      "authors": [
        "Shizhe Chen",
        "Ricardo Garcia",
        "Paul Pacaud",
        "Cordelia Schmid"
      ],
      "abstract": "Robotic manipulation faces a significant challenge in generalizing across unseen objects, environments and tasks specified by diverse language instructions. To improve generalization capabilities, recent research has incorporated large language models (LLMs) for planning and action execution. While promising, these methods often fall short in generating grounded plans in visual environments. Although efforts have been made to perform visual instructional tuning on LLMs for robotic manipulation, existing methods are typically constrained by single-view image input and struggle with precise object grounding. In this work, we introduce Gondola, a novel grounded vision-language planning model based on LLMs for generalizable robotic manipulation. Gondola takes multi-view images and history plans to produce the next action plan with interleaved texts and segmentation masks of target objects and locations. To support the training of Gondola, we construct three types of datasets using the RLBench simulator, namely robot grounded planning, multi-view referring expression and pseudo long-horizon task datasets. Gondola outperforms the state-of-the-art LLM-based method across all four generalization levels of the GemBench dataset, including novel placements, rigid objects, articulated objects and long-horizon tasks.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†Gondolaï¼Œä¸€ç§åŸºäºLLMsçš„æ–°å‹grounded vision-language planningæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œåœ¨æœªè§ç‰©ä½“ã€ç¯å¢ƒå’Œå¤šæ ·åŒ–ä»»åŠ¡æŒ‡ä»¤ä¸‹çš„æ³›åŒ–éš¾é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡æ¥æ”¶multi-viewå›¾åƒå’Œå†å²è§„åˆ’ä¿¡æ¯ï¼Œç”ŸæˆåŒ…å«äº¤ç»‡æ–‡æœ¬ä»¥åŠç›®æ ‡ç‰©ä½“ä¸ä½ç½®segmentation masksçš„ä¸‹ä¸€æ­¥åŠ¨ä½œæŒ‡ä»¤ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹è®­ç»ƒï¼Œç ”ç©¶è€…åˆ©ç”¨RLBenchæ¨¡æ‹Ÿå™¨æ„å»ºäº†æ¶µç›–robot grounded planningã€multi-view referring expressionå’Œpseudo long-horizon taskçš„ä¸‰ç±»ä¸“ç”¨æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGondolaåœ¨GemBenchæ•°æ®é›†çš„å››ä¸ªæ³›åŒ–ç»´åº¦ï¼ˆåŒ…æ‹¬novel placementsã€rigid objectsã€articulated objectså’Œlong-horizon tasksï¼‰ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„åŸºäºLLMçš„æ–¹æ³•ã€‚è¯¥å·¥ä½œæˆåŠŸå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨å•è§†è§’è¾“å…¥å’Œç²¾ç¡®object groundingæ–¹é¢çš„å±€é™ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººåœ¨æ‰§è¡Œé•¿ç¨‹ä»»åŠ¡æ—¶çš„æ€§èƒ½ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11261v1",
      "published_date": "2025-06-12 20:04:31 UTC",
      "updated_date": "2025-06-12 20:04:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:11:17.770563+00:00"
    },
    {
      "arxiv_id": "2506.13798v1",
      "title": "Contemporary AI foundation models increase biological weapons risk",
      "title_zh": "å½“ä»£äººå·¥æ™ºèƒ½åŸºç¡€æ¨¡å‹å¢åŠ äº†ç”Ÿç‰©æ­¦å™¨é£é™©",
      "authors": [
        "Roger Brent",
        "T. Greg McKelvey"
      ],
      "abstract": "The rapid advancement of artificial intelligence has raised concerns about its potential to facilitate biological weapons development. We argue existing safety assessments of contemporary foundation AI models underestimate this risk, largely due to flawed assumptions and inadequate evaluation methods. First, assessments mistakenly assume biological weapons development requires tacit knowledge, or skills gained through hands-on experience that cannot be easily verbalized. Second, they rely on imperfect benchmarks that overlook how AI can uplift both nonexperts and already-skilled individuals. To challenge the tacit knowledge assumption, we examine cases where individuals without formal expertise, including a 2011 Norwegian ultranationalist who synthesized explosives, successfully carried out complex technical tasks. We also review efforts to document pathogen construction processes, highlighting how such tasks can be conveyed in text. We identify \"elements of success\" for biological weapons development that large language models can describe in words, including steps such as acquiring materials and performing technical procedures. Applying this framework, we find that advanced AI models Llama 3.1 405B, ChatGPT-4o, and Claude 3.5 Sonnet can accurately guide users through the recovery of live poliovirus from commercially obtained synthetic DNA, challenging recent claims that current models pose minimal biosecurity risk. We advocate for improved benchmarks, while acknowledging the window for meaningful implementation may have already closed.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œå½“ä»£äººå·¥æ™ºèƒ½åŸºç¡€æ¨¡å‹(Foundation Models)æ­£æ˜¾è‘—å¢åŠ ç”Ÿç‰©æ­¦å™¨å¼€å‘çš„é£é™©ï¼Œå¹¶è®¤ä¸ºç°æœ‰çš„å®‰å…¨è¯„ä¼°ç”±äºé”™è¯¯çš„å‰æå‡è®¾å’Œä¸å®Œå–„çš„è¯„ä¼°æ–¹æ³•è€Œä½ä¼°äº†è¿™ä¸€å¨èƒã€‚ç ”ç©¶æŒ‘æˆ˜äº†ç”Ÿç‰©æ­¦å™¨ç ”å‘éœ€è¦éš¾ä»¥è¨€ä¼ çš„éšæ€§çŸ¥è¯†(Tacit Knowledge)è¿™ä¸€ä¼ ç»Ÿå‡è®¾ï¼Œå¹¶æŒ‡å‡ºç›®å‰çš„è¯„ä¼°åŸºå‡†(Benchmarks)æœªèƒ½å……åˆ†è¡¡é‡AIå¦‚ä½•æå‡éä¸“ä¸šäººå£«çš„æ“ä½œèƒ½åŠ›ã€‚é€šè¿‡åˆ†æç—…åŸä½“æ„å»ºè¿‡ç¨‹ä¸­çš„æ–‡æœ¬æè¿°æ€§ï¼Œä½œè€…è¯†åˆ«å‡ºäº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)èƒ½å¤Ÿå‡†ç¡®ä¼ è¾¾çš„ç”Ÿç‰©æ­¦å™¨å¼€å‘â€œæˆåŠŸè¦ç´ â€ï¼Œæ¶µç›–äº†ä»ææ–™è·å–åˆ°å…·ä½“æŠ€æœ¯ç¨‹åºçš„å„ç¯èŠ‚ã€‚å®éªŒå‘ç°ï¼ŒLlama 3.1 405Bã€ChatGPT-4oå’ŒClaude 3.5 Sonnetç­‰å…ˆè¿›æ¨¡å‹èƒ½å¤Ÿæä¾›ä»åˆæˆDNAä¸­æ¢å¤æ´»ä½“è„Šé«“ç°è´¨ç‚ç—…æ¯’(Poliovirus)çš„ç²¾ç¡®æŒ‡å¯¼ï¼Œç›´æ¥åé©³äº†å½“å‰æ¨¡å‹ç”Ÿç‰©å®‰å…¨é£é™©æå°çš„è§‚ç‚¹ã€‚ç ”ç©¶æœ€åå¼ºè°ƒäº†æ”¹è¿›è¯„ä¼°åŸºå‡†çš„ç´§è¿«æ€§ï¼Œå¹¶å¯¹å®æ–½æœ‰æ•ˆç›‘ç®¡çš„æœ€ä½³æ—¶æœºè¡¨ç¤ºäº†æ‹…å¿§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "58 pages, 10 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.13798v1",
      "published_date": "2025-06-12 19:53:38 UTC",
      "updated_date": "2025-06-12 19:53:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:11:27.757918+00:00"
    },
    {
      "arxiv_id": "2506.11251v1",
      "title": "Measuring multi-calibration",
      "title_zh": "å¤šé‡æ ¡å‡†çš„åº¦é‡",
      "authors": [
        "Ido Guy",
        "Daniel Haimovich",
        "Fridolin Linder",
        "Nastaran Okati",
        "Lorenzo Perini",
        "Niek Tax",
        "Mark Tygert"
      ],
      "abstract": "A suitable scalar metric can help measure multi-calibration, defined as follows. When the expected values of observed responses are equal to corresponding predicted probabilities, the probabilistic predictions are known as \"perfectly calibrated.\" When the predicted probabilities are perfectly calibrated simultaneously across several subpopulations, the probabilistic predictions are known as \"perfectly multi-calibrated.\" In practice, predicted probabilities are seldom perfectly multi-calibrated, so a statistic measuring the distance from perfect multi-calibration is informative. A recently proposed metric for calibration, based on the classical Kuiper statistic, is a natural basis for a new metric of multi-calibration and avoids well-known problems of metrics based on binning or kernel density estimation. The newly proposed metric weights the contributions of different subpopulations in proportion to their signal-to-noise ratios; data analyses' ablations demonstrate that the metric becomes noisy when omitting the signal-to-noise ratios from the metric. Numerical examples on benchmark data sets illustrate the new metric.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ ¡å‡† (multi-calibration) çš„è¡¡é‡æ–¹æ³•ï¼Œæ—¨åœ¨é‡åŒ–é¢„æµ‹æ¦‚ç‡åœ¨å¤šä¸ªå­ç¾¤ä½“ä¸­åŒæ—¶è¾¾åˆ°æ ¡å‡†çŠ¶æ€çš„ç²¾ç¡®åº¦ã€‚ä¸ºäº†è§£å†³å®é™…é¢„æµ‹ä¸­éš¾ä»¥è¾¾åˆ°å®Œç¾å¤šæ ¡å‡†çš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç»å…¸ Kuiper ç»Ÿè®¡é‡çš„æ–°å‹æ ‡é‡æŒ‡æ ‡ã€‚è¯¥æŒ‡æ ‡é¿å¼€äº†ä¼ ç»Ÿçš„åŸºäºåˆ†ç®± (binning) æˆ–æ ¸å¯†åº¦ä¼°è®¡ (kernel density estimation) æ–¹æ³•æ‰€é¢ä¸´çš„å¸¸è§ç¼ºé™·ï¼Œå…·æœ‰æ›´å¼ºçš„ç¨³å¥æ€§ã€‚æ–°æŒ‡æ ‡çš„å…³é”®åˆ›æ–°åœ¨äºæ ¹æ®å„å­ç¾¤ä½“çš„ä¿¡å™ªæ¯” (signal-to-noise ratios) å¯¹å…¶è´¡çŒ®è¿›è¡Œæ¯”ä¾‹åŠ æƒï¼Œæ¶ˆèå®éªŒè¯å®äº†è¿™ä¸€è®¾è®¡èƒ½æ˜¾è‘—é™ä½è¯„ä¼°è¿‡ç¨‹ä¸­çš„å™ªå£°ã€‚é€šè¿‡åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ•°å€¼ç¤ºä¾‹ï¼Œç ”ç©¶è¯æ˜äº†è¯¥æŒ‡æ ‡èƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°å¹¶åº¦é‡å¤æ‚æ¨¡å‹åœ¨è·¨ç¾¤ä½“é¢„æµ‹ä¸­çš„å¤šæ ¡å‡†æ€§èƒ½ã€‚",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ME",
      "comment": "25 pages, 12 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.11251v1",
      "published_date": "2025-06-12 19:48:10 UTC",
      "updated_date": "2025-06-12 19:48:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:11:56.189129+00:00"
    },
    {
      "arxiv_id": "2506.11250v1",
      "title": "Can Time-Series Foundation Models Perform Building Energy Management Tasks?",
      "title_zh": "æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹èƒ½å¦èƒœä»»å»ºç­‘èƒ½æºç®¡ç†ä»»åŠ¡ï¼Ÿ",
      "authors": [
        "Ozan Baris Mulayim",
        "Pengrui Quan",
        "Liying Han",
        "Xiaomin Ouyang",
        "Dezhi Hong",
        "Mario BergÃ©s",
        "Mani Srivastava"
      ],
      "abstract": "Building energy management (BEM) tasks require processing and learning from a variety of time-series data. Existing solutions rely on bespoke task- and data-specific models to perform these tasks, limiting their broader applicability. Inspired by the transformative success of Large Language Models (LLMs), Time-Series Foundation Models (TSFMs), trained on diverse datasets, have the potential to change this. Were TSFMs to achieve a level of generalizability across tasks and contexts akin to LLMs, they could fundamentally address the scalability challenges pervasive in BEM. To understand where they stand today, we evaluate TSFMs across four dimensions: (1) generalizability in zero-shot univariate forecasting, (2) forecasting with covariates for thermal behavior modeling, (3) zero-shot representation learning for classification tasks, and (4) robustness to performance metrics and varying operational conditions. Our results reveal that TSFMs exhibit \\emph{limited} generalizability, performing only marginally better than statistical models on unseen datasets and modalities for univariate forecasting. Similarly, inclusion of covariates in TSFMs does not yield performance improvements, and their performance remains inferior to conventional models that utilize covariates. While TSFMs generate effective zero-shot representations for downstream classification tasks, they may remain inferior to statistical models in forecasting when statistical models perform test-time fitting. Moreover, TSFMs forecasting performance is sensitive to evaluation metrics, and they struggle in more complex building environments compared to statistical models. These findings underscore the need for targeted advancements in TSFM design, particularly their handling of covariates and incorporating context and temporal dynamics into prediction mechanisms, to develop more adaptable and scalable solutions for BEM.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹(Time-Series Foundation Models, TSFMs)åœ¨å¤„ç†å»ºç­‘èƒ½æºç®¡ç†(Building Energy Management, BEM)ä»»åŠ¡æ—¶çš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œæ—¨åœ¨æ¢ç©¶å…¶æ˜¯å¦èƒ½è§£å†³è¯¥é¢†åŸŸé•¿æœŸå­˜åœ¨çš„æ‰©å±•æ€§éš¾é¢˜ã€‚ä½œè€…é€šè¿‡é›¶æ ·æœ¬å•å˜é‡é¢„æµ‹(zero-shot univariate forecasting)ã€å«åå˜é‡çš„çƒ­è¡Œä¸ºå»ºæ¨¡ã€è¡¨å¾å­¦ä¹ ä»¥åŠé²æ£’æ€§å››ä¸ªç»´åº¦è¿›è¡Œäº†ç³»ç»Ÿæ€§æµ‹è¯•ã€‚ç ”ç©¶å‘ç°TSFMsçš„é€šç”¨æ€§ç›®å‰è¾ƒä¸ºæœ‰é™ï¼Œå…¶åœ¨å•å˜é‡é¢„æµ‹ä¸­çš„è¡¨ç°ä»…ç•¥ä¼˜äºç»Ÿè®¡æ¨¡å‹ï¼Œä¸”åœ¨å¼•å…¥åå˜é‡(covariates)åå¹¶æœªå±•ç°å‡ºæ€§èƒ½ä¼˜åŠ¿ã€‚è™½ç„¶TSFMsåœ¨åˆ†ç±»ä»»åŠ¡çš„é›¶æ ·æœ¬è¡¨å¾å­¦ä¹ ä¸­è¡¨ç°æœ‰æ•ˆï¼Œä½†åœ¨é¢„æµ‹å‡†ç¡®åº¦ä¸Šä»é€Šè‰²äºè¿›è¡Œäº†æµ‹è¯•æ—¶æ‹Ÿåˆ(test-time fitting)çš„ä¼ ç»Ÿæ¨¡å‹ã€‚å®éªŒç»“æœè¿˜æ˜¾ç¤ºTSFMså¯¹è¯„ä¼°æŒ‡æ ‡æ•æ„Ÿï¼Œä¸”åœ¨å¤æ‚çš„å»ºç­‘è¿è¡Œç¯å¢ƒä¸‹è¡¨ç°æ¬ ä½³ã€‚è¯¥è®ºæ–‡æœ€åæŒ‡å‡ºï¼ŒTSFMsè‹¥è¦æˆä¸ºBEMé¢†åŸŸçš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆï¼Œä»éœ€åœ¨åå˜é‡å¤„ç†ã€ä¸Šä¸‹æ–‡é›†æˆä»¥åŠæ—¶é—´åŠ¨æ€æ•æ‰ç­‰è®¾è®¡æ–¹é¢å–å¾—é’ˆå¯¹æ€§çªç ´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 5 tables, 8 figures. Under review for Data-Centric Engineering journal",
      "pdf_url": "https://arxiv.org/pdf/2506.11250v1",
      "published_date": "2025-06-12 19:45:10 UTC",
      "updated_date": "2025-06-12 19:45:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:01.954228+00:00"
    },
    {
      "arxiv_id": "2506.11246v2",
      "title": "No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning",
      "title_zh": "æç¤ºéä¸‡èƒ½ï¼šé€šè¿‡è‡ªé€‚åº”æç¤ºç»Ÿä¸€æ—¶åºè¡¨æ ¼æ¨ç†",
      "authors": [
        "Abhishek Rajgaria",
        "Kushagra Dixit",
        "Mayank Vyas",
        "Harshavardhan Kalalbandi",
        "Dan Roth",
        "Vivek Gupta"
      ],
      "abstract": "Temporal Table Reasoning is a critical challenge for Large Language Models (LLMs), requiring effective reasoning to extract relevant insights. Despite existence of multiple prompting methods, their impact on table reasoning remains largely unexplored. Furthermore, model performance varies drastically across different table and context structures, making it difficult to determine an optimal approach. This work investigates multiple prompting technique on diverse table types to determine that performance depends on factors such as entity type, table structure, requirement of additional context and question complexity, with \"NO\" single method consistently outperforming others. To address this, we introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts to context and integrates structured reasoning. Our results demonstrate that SEAR achieves superior performance across all table types compared to baseline prompting techniques. Additionally, we explore the impact of table structure refactoring, finding that a unified representation enhances model reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†æ—¶åºè¡¨æ ¼æ¨ç†(Temporal Table Reasoning)æ—¶è¡¨ç°ä¸ä¸€çš„é—®é¢˜ï¼ŒæŒ‡å‡ºæç¤ºæ–¹æ³•çš„æ•ˆæœå—å®ä½“ç±»å‹ã€è¡¨æ ¼ç»“æ„åŠé—®é¢˜å¤æ‚åº¦ç­‰å¤šç§å› ç´ å½±å“ï¼Œä¸”ä¸å­˜åœ¨é€šç”¨çš„æœ€ä¼˜æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åä¸ºSEARçš„è‡ªé€‚åº”æç¤º(Adaptive Prompting)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å€Ÿé‰´äººç±»æ¨ç†æ¨¡å¼ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´å¹¶æ•´åˆç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEARåœ¨æ‰€æœ‰æµ‹è¯•çš„è¡¨æ ¼ç±»å‹ä¸­å‡å®ç°äº†è¶…è¶ŠåŸºçº¿æç¤ºæŠ€æœ¯çš„ä¼˜å¼‚æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°è¡¨æ ¼ç»“æ„é‡æ„(Table Structure Refactoring)é€šè¿‡ç»Ÿä¸€è¡¨å¾èƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥å·¥ä½œä¸ºè§£å†³å¤æ‚æ•°æ®ç¯å¢ƒä¸‹çš„è¡¨æ ¼æ¨ç†ä»»åŠ¡æä¾›äº†æ›´å…·çµæ´»æ€§å’Œé²æ£’æ€§çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages, 21 Tables, 10 Figures",
      "pdf_url": "https://arxiv.org/pdf/2506.11246v2",
      "published_date": "2025-06-12 19:32:59 UTC",
      "updated_date": "2025-08-07 19:32:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:11:52.203804+00:00"
    },
    {
      "arxiv_id": "2506.11243v1",
      "title": "RETUYT-INCO at BEA 2025 Shared Task: How Far Can Lightweight Models Go in AI-powered Tutor Evaluation?",
      "title_zh": "RETUYT-INCO å‚åŠ  BEA 2025 å…±äº«ä»»åŠ¡ï¼šè½»é‡çº§æ¨¡å‹åœ¨ AI èµ‹èƒ½çš„å¯¼å¸ˆè¯„ä¼°ä¸­ç©¶ç«Ÿèƒ½èµ°å¤šè¿œï¼Ÿ",
      "authors": [
        "Santiago GÃ³ngora",
        "Ignacio Sastre",
        "Santiago Robaina",
        "Ignacio Remersaro",
        "Luis Chiruzzo",
        "Aiala RosÃ¡"
      ],
      "abstract": "In this paper, we present the RETUYT-INCO participation at the BEA 2025 shared task. Our participation was characterized by the decision of using relatively small models, with fewer than 1B parameters. This self-imposed restriction tries to represent the conditions in which many research labs or institutions are in the Global South, where computational power is not easily accessible due to its prohibitive cost. Even under this restrictive self-imposed setting, our models managed to stay competitive with the rest of teams that participated in the shared task. According to the $exact\\ F_1$ scores published by the organizers, the performance gaps between our models and the winners were as follows: $6.46$ in Track 1; $10.24$ in Track 2; $7.85$ in Track 3; $9.56$ in Track 4; and $13.13$ in Track 5. Considering that the minimum difference with a winner team is $6.46$ points -- and the maximum difference is $13.13$ -- according to the $exact\\ F_1$ score, we find that models with a size smaller than 1B parameters are competitive for these tasks, all of which can be run on computers with a low-budget GPU or even without a GPU.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†RETUYT-INCOå›¢é˜Ÿå‚åŠ BEA 2025å…±äº«ä»»åŠ¡çš„æˆæœï¼Œé‡ç‚¹è¯„ä¼°äº†è½»é‡çº§æ¨¡å‹åœ¨AIé©±åŠ¨å¯¼å¸ˆè¯„ä»·ï¼ˆAI-powered Tutor Evaluationï¼‰ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚ä½œè€…æœ‰æ„è¯†åœ°å°†æ¨¡å‹è§„æ¨¡é™åˆ¶åœ¨1Bå‚æ•°ä»¥ä¸‹ï¼Œä»¥æ¨¡æ‹Ÿå…¨çƒå—æ–¹ï¼ˆGlobal Southï¼‰ç ”ç©¶æœºæ„åœ¨æœ‰é™ç®—åŠ›ä¸é«˜æ˜‚æˆæœ¬å‹åŠ›ä¸‹çš„çœŸå®ç§‘ç ”ç¯å¢ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å—åˆ°æ¨¡å‹å®¹é‡é™åˆ¶ï¼Œè¯¥æ–¹æ¡ˆåœ¨äº”ä¸ªä»»åŠ¡è½¨é“ä¸­ä¾ç„¶ä¿æŒäº†è¾ƒå¼ºçš„ç«äº‰åŠ›ï¼Œä¸å† å†›å›¢é˜Ÿçš„exact F1åˆ†æ•°å·®è·ä»…åœ¨6.46è‡³13.13ä¹‹é—´ã€‚ç ”ç©¶è¯æ˜ï¼Œå‚æ•°é‡å°äº1Bçš„æ¨¡å‹åœ¨å¤„ç†æ­¤ç±»ä»»åŠ¡æ—¶å…·æœ‰é«˜åº¦å¯è¡Œæ€§ï¼Œä¸”èƒ½å¤Ÿè¿è¡Œåœ¨ä½é¢„ç®—GPUç”šè‡³ä»…ä¾èµ–CPUçš„è®¡ç®—æœºä¸Šã€‚è¿™ä¸€å‘ç°ä¸ºä½æˆæœ¬ã€å¯æ™®åŠçš„AIæ•™è‚²è¯„ä¼°æŠ€æœ¯æä¾›äº†é‡è¦çš„å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper will be presented at the 20th BEA Workshop (Innovative Use of NLP for Building Educational Applications) at ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.11243v1",
      "published_date": "2025-06-12 19:24:56 UTC",
      "updated_date": "2025-06-12 19:24:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:11:54.975739+00:00"
    },
    {
      "arxiv_id": "2506.11242v1",
      "title": "A Causal Lens for Learning Long-term Fair Policies",
      "title_zh": "å­¦ä¹ é•¿æœŸå…¬å¹³ç­–ç•¥çš„å› æœè§†è§’",
      "authors": [
        "Jacob Lear",
        "Lu Zhang"
      ],
      "abstract": "Fairness-aware learning studies the development of algorithms that avoid discriminatory decision outcomes despite biased training data. While most studies have concentrated on immediate bias in static contexts, this paper highlights the importance of investigating long-term fairness in dynamic decision-making systems while simultaneously considering instantaneous fairness requirements. In the context of reinforcement learning, we propose a general framework where long-term fairness is measured by the difference in the average expected qualification gain that individuals from different groups could obtain.Then, through a causal lens, we decompose this metric into three components that represent the direct impact, the delayed impact, as well as the spurious effect the policy has on the qualification gain. We analyze the intrinsic connection between these components and an emerging fairness notion called benefit fairness that aims to control the equity of outcomes in decision-making. Finally, we develop a simple yet effective approach for balancing various fairness notions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŠ¨æ€å†³ç­–ç³»ç»Ÿä¸­é•¿æœŸå…¬å¹³æ€§(long-term fairness)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨å¹³è¡¡å³æ—¶å…¬å¹³è¦æ±‚ä¸é•¿æœŸå…¬å¹³ç›®æ ‡ã€‚è¯¥æ¡†æ¶å°†é•¿æœŸå…¬å¹³æ€§å®šä¹‰ä¸ºä¸åŒç¾¤ä½“ä¸ªä½“åœ¨å¹³å‡é¢„æœŸèµ„æ ¼å¢ç›Š(qualification gain)ä¸Šçš„å·®å¼‚ã€‚ç ”ç©¶è€…ä»å› æœè§†è§’(causal lens)å‡ºå‘ï¼Œå°†è¯¥æŒ‡æ ‡è¿›ä¸€æ­¥åˆ†è§£ä¸ºç­–ç•¥å¯¹èµ„æ ¼å¢ç›Šäº§ç”Ÿçš„ç›´æ¥å½±å“(direct impact)ã€å»¶è¿Ÿå½±å“(delayed impact)ä»¥åŠä¼ªæ•ˆåº”(spurious effect)ä¸‰ä¸ªæ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚è®ºæ–‡è¿˜åˆ†æäº†è¿™äº›ç»„ä»¶ä¸æ—¨åœ¨æ§åˆ¶å†³ç­–ç»“æœå…¬å¹³æ€§çš„æ”¶ç›Šå…¬å¹³(benefit fairness)æ¦‚å¿µä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºåœ¨åŠ¨æ€å†³ç­–è¿‡ç¨‹ä¸­å¹³è¡¡å¤šç§ä¸åŒçš„å…¬å¹³æ€§å‡†åˆ™ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This is an extension to the paper which was accepted to the 13th International Conference on Learning Representations",
      "pdf_url": "https://arxiv.org/pdf/2506.11242v1",
      "published_date": "2025-06-12 19:22:50 UTC",
      "updated_date": "2025-06-12 19:22:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:04.551272+00:00"
    },
    {
      "arxiv_id": "2506.11238v1",
      "title": "uPVC-Net: A Universal Premature Ventricular Contraction Detection Deep Learning Algorithm",
      "title_zh": "uPVC-Netï¼šä¸€ç§é€šç”¨çš„å®¤æ€§æœŸå‰æ”¶ç¼©æ£€æµ‹æ·±åº¦å­¦ä¹ ç®—æ³•",
      "authors": [
        "Hagai Hamami",
        "Yosef Solewicz",
        "Daniel Zur",
        "Yonatan Kleerekoper",
        "Joachim A. Behar"
      ],
      "abstract": "Introduction: Premature Ventricular Contractions (PVCs) are common cardiac arrhythmias originating from the ventricles. Accurate detection remains challenging due to variability in electrocardiogram (ECG) waveforms caused by differences in lead placement, recording conditions, and population demographics. Methods: We developed uPVC-Net, a universal deep learning model to detect PVCs from any single-lead ECG recordings. The model is developed on four independent ECG datasets comprising a total of 8.3 million beats collected from Holter monitors and a modern wearable ECG patch. uPVC-Net employs a custom architecture and a multi-source, multi-lead training strategy. For each experiment, one dataset is held out to evaluate out-of-distribution (OOD) generalization. Results: uPVC-Net achieved an AUC between 97.8% and 99.1% on the held-out datasets. Notably, performance on wearable single-lead ECG data reached an AUC of 99.1%. Conclusion: uPVC-Net exhibits strong generalization across diverse lead configurations and populations, highlighting its potential for robust, real-world clinical deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†uPVC-Netï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä»ä»»æ„å•å¯¼è”å¿ƒç”µå›¾(ECG)è®°å½•ä¸­è‡ªåŠ¨æ£€æµ‹å®¤æ€§æ—©æ(Premature Ventricular Contractions, PVCs)çš„é€šç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚é’ˆå¯¹å¿ƒç”µæ³¢å½¢å› å¯¼è”ä½ç½®ã€è®°å½•æ¡ä»¶å’Œäººç¾¤ç‰¹å¾å·®å¼‚è€Œäº§ç”Ÿçš„å˜å¼‚æ€§ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨äº†è‡ªå®šä¹‰æ¶æ„ä»¥åŠå¤šæºã€å¤šå¯¼è”çš„è®­ç»ƒç­–ç•¥ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¥è‡ªHolterç›‘æµ‹ä»ªå’Œç°ä»£å¯ç©¿æˆ´ECGè´´ç‰‡çš„å››ä¸ªç‹¬ç«‹æ•°æ®é›†è¿›è¡Œå¼€å‘ï¼Œæ¶µç›–äº†æ€»è®¡830ä¸‡ä¸ªå¿ƒè·³æ ·æœ¬ã€‚ä¸ºäº†éªŒè¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå®éªŒé‡‡ç”¨äº†ç•™ä¸€æ³•è¿›è¡Œåˆ†å¸ƒå¤–(Out-of-Distribution, OOD)è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒuPVC-Netåœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†97.8%è‡³99.1%çš„æ›²çº¿ä¸‹é¢ç§¯(AUC)ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯ç©¿æˆ´å•å¯¼è”æ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚è¯¥ç ”ç©¶è¯æ˜äº†uPVC-Netåœ¨ä¸åŒå¯¼è”é…ç½®å’Œäººç¾¤ä¸­å…·æœ‰å¼ºå¤§çš„é€šç”¨æ€§ï¼Œä¸ºå…¶åœ¨çœŸå®ä¸´åºŠç¯å¢ƒä¸‹çš„ç¨³å¥éƒ¨ç½²å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.11238v1",
      "published_date": "2025-06-12 19:15:06 UTC",
      "updated_date": "2025-06-12 19:15:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:13.162912+00:00"
    },
    {
      "arxiv_id": "2506.12099v1",
      "title": "SocialCredit+",
      "title_zh": "SocialCredit+",
      "authors": [
        "Thabassum Aslam",
        "Anees Aslam"
      ],
      "abstract": "SocialCredit+ is AI powered credit scoring system that leverages publicly available social media data to augment traditional credit evaluation. It uses a conversational banking assistant to gather user consent and fetch public profiles. Multimodal feature extractors analyze posts, bios, images, and friend networks to generate a rich behavioral profile. A specialized Sharia-compliance layer flags any non-halal indicators and prohibited financial behavior based on Islamic ethics. The platform employs a retrieval-augmented generation module: an LLM accesses a domain specific knowledge base to generate clear, text-based explanations for each decision. We describe the end-to-end architecture and data flow, the models used, and system infrastructure. Synthetic scenarios illustrate how social signals translate into credit-score factors. This paper emphasizes conceptual novelty, compliance mechanisms, and practical impact, targeting AI researchers, fintech practitioners, ethical banking jurists, and investors.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SocialCredit+ï¼Œä¸€ç§åŸºäº AI çš„ä¿¡ç”¨è¯„åˆ†ç³»ç»Ÿï¼Œæ—¨åœ¨åˆ©ç”¨å…¬å¼€çš„ç¤¾äº¤åª’ä½“æ•°æ®æ¥å¢å¼ºä¼ ç»Ÿçš„ä¿¡ç”¨è¯„ä¼°ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¯¹è¯å¼é“¶è¡ŒåŠ©æ‰‹è·å–ç”¨æˆ·æˆæƒå¹¶è°ƒå–å…¬å¼€ä¸ªäººèµ„æ–™ï¼Œåˆ©ç”¨å¤šæ¨¡æ€ç‰¹å¾æå–å™¨åˆ†æç¤¾äº¤åŠ¨æ€ã€ä¸ªäººç®€ä»‹ã€å›¾ç‰‡åŠå¥½å‹ç½‘ç»œï¼Œä»è€Œç”Ÿæˆä¸°å¯Œçš„è¡Œä¸ºç”»åƒã€‚ä¸ºäº†æ»¡è¶³ç‰¹å®šä¼¦ç†éœ€æ±‚ï¼Œå¹³å°å¼•å…¥äº†ä¸“é—¨çš„ Sharia-compliance æ¨¡å—ï¼Œèƒ½å¤Ÿæ ¹æ®ä¼Šæ–¯å…°ä¼¦ç†è¯†åˆ«å¹¶æ ‡è®°ä»»ä½•ä¸ç¬¦åˆæ•™ä¹‰çš„é halal æŒ‡æ ‡æˆ–ç¦å¿Œè´¢åŠ¡è¡Œä¸ºã€‚æ­¤å¤–ï¼ŒSocialCredit+ é‡‡ç”¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-augmented generation, RAG)æŠ€æœ¯ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)è®¿é—®ç‰¹å®šé¢†åŸŸçŸ¥è¯†åº“ï¼Œä¸ºæ¯ä¸€é¡¹ä¿¡ç”¨å†³ç­–æä¾›æ¸…æ™°çš„æ–‡æœ¬è§£é‡Šã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº†è¯¥ç³»ç»Ÿçš„ç«¯åˆ°ç«¯æ¶æ„ä¸æ•°æ®æµï¼Œå¹¶é€šè¿‡åˆæˆåœºæ™¯å±•ç¤ºäº†ç¤¾äº¤ä¿¡å·å¦‚ä½•è½¬åŒ–ä¸ºä¿¡ç”¨è¯„åˆ†å› å­ï¼Œä¸ºé‡‘èç§‘æŠ€ã€äººå·¥æ™ºèƒ½ç ”ç©¶åŠä¼¦ç†é“¶è¡Œé¢†åŸŸæä¾›äº†å…·æœ‰å®é™…å½±å“åŠ›çš„åˆ›æ–°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12099v1",
      "published_date": "2025-06-12 18:44:15 UTC",
      "updated_date": "2025-06-12 18:44:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:24.455597+00:00"
    },
    {
      "arxiv_id": "2506.21570v1",
      "title": "Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting",
      "title_zh": "éšæœºåˆå§‹åŒ–æ— æ³•è¿½èµ¶ï¼šè¯­è¨€æ¨¡å‹è¿ç§»åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„ä¼˜åŠ¿",
      "authors": [
        "Roland Riachi",
        "Kashif Rasul",
        "Arjun Ashok",
        "Prateek Humane",
        "Alexis Roger",
        "Andrew R. Williams",
        "Yuriy Nevmyvaka",
        "Irina Rish"
      ],
      "abstract": "Recent works have demonstrated the effectiveness of adapting pre-trained language models (LMs) for forecasting time series in the low-data regime. We build upon these findings by analyzing the effective transfer from language models to time series forecasting under various design choices including upstream post-training, time series tokenizer and language backbone size. In the low-data regime, these design choices have a significant impact on the validation loss, with clear-cut choices that outperform others. Contrary to Hernandez et al. (2021), we observe that the validation loss of the LMs continues to smoothly decrease long after the validation loss of the randomly initialized models has converged, leading to a non-vanishing transfer gap that holds across design choices. These findings not only help shed light on the effective use of compute-efficient training for time series, but also open the way for the study of modality-agnostic properties of data distributions leveraged by these models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä½æ•°æ®é‡(low-data regime)èƒŒæ™¯ä¸‹ï¼Œå°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(LMs)é€‚é…äºæ—¶é—´åºåˆ—é¢„æµ‹(time series forecasting)çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡åˆ†æä¸Šæ¸¸åè®­ç»ƒ(post-training)ã€æ—¶é—´åºåˆ—åˆ†è¯å™¨(tokenizer)å’Œè¯­è¨€ä¸»å¹²æ¨¡å‹è§„æ¨¡(backbone size)ç­‰è®¾è®¡é€‰æ‹©ï¼Œç ”ç©¶å‘ç°è¿™äº›å› ç´ å¯¹éªŒè¯æŸå¤±(validation loss)å…·æœ‰æ˜¾è‘—å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸éšæœºåˆå§‹åŒ–æ¨¡å‹åœ¨æ”¶æ•›åè¿…é€Ÿé™·å…¥åœæ»ä¸åŒï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„éªŒè¯æŸå¤±ä¼šæŒç»­å¹³æ»‘ä¸‹é™ï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªè·¨è¶Šå„ç§è®¾è®¡é€‰æ‹©ä¸”é•¿æœŸå­˜åœ¨çš„è½¬ç§»å·®è·(transfer gap)ã€‚è¿™äº›å‘ç°ä¸ä»…é˜æ˜äº†å¦‚ä½•æ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºè¿›è¡Œæ—¶é—´åºåˆ—è®­ç»ƒï¼Œè¿˜ä¸ºç ”ç©¶è¿™äº›æ¨¡å‹æ‰€åˆ©ç”¨çš„æ•°æ®åˆ†å¸ƒä¸­ä¸æ¨¡æ€æ— å…³(modality-agnostic)çš„ç‰¹æ€§å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21570v1",
      "published_date": "2025-06-12 18:39:38 UTC",
      "updated_date": "2025-06-12 18:39:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:24.309476+00:00"
    },
    {
      "arxiv_id": "2506.17276v1",
      "title": "Modal Logic for Stratified Becoming: Actualization Beyond Possible Worlds",
      "title_zh": "åˆ†å±‚ç”Ÿæˆæ¨¡æ€é€»è¾‘ï¼šè¶…è¶Šå¯èƒ½ä¸–ç•Œçš„ç°å®åŒ–",
      "authors": [
        "Alexandre Le Nepvou"
      ],
      "abstract": "This article develops a novel framework for modal logic based on the idea of stratified actualization, rather than the classical model of global possible worlds. Traditional Kripke semantics treat modal operators as quantification over fully determinate alternatives, neglecting the local, dynamic, and often asymmetric nature of actualization processes. We propose a system Stratified Actualization Logic (SAL) in which modalities are indexed by levels of ontological stability, interpreted as admissibility regimes. Each modality operates over a structured layer of possibility, grounded in the internal coherence of transitions between layers. We formally define the syntax and semantics of SAL, introduce its axioms, and prove soundness and completeness. Applications are discussed in connection with temporal becoming, quantum decoherence domains, and modal metaphysics. The result is a logic that captures the ontological structure of actualization without recourse to abstract possible worlds, offering a stratified alternative to standard modal realism.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäº Stratified Actualization çš„æ–°å‹ Modal Logic æ¡†æ¶ï¼Œæ—¨åœ¨è¶…è¶Šä¼ ç»Ÿçš„å…¨çƒæ€§ Possible Worlds æ¨¡å‹ã€‚é’ˆå¯¹ä¼ ç»Ÿ Kripke Semantics å¿½ç•¥ç°å®åŒ–è¿‡ç¨‹çš„å±€éƒ¨æ€§ã€åŠ¨æ€æ€§å’Œä¸å¯¹ç§°æ€§ç­‰é—®é¢˜ï¼Œä½œè€…æå‡ºäº† Stratified Actualization Logic (SAL) ç³»ç»Ÿã€‚åœ¨ SAL ä¸­ï¼Œæ¨¡æ€è¯ç”± Ontological Stability æ°´å¹³ç´¢å¼•ï¼Œå¹¶è¢«è§£é‡Šä¸º Admissibility Regimesï¼Œæ¯ä¸ªæ¨¡æ€åœ¨åŸºäºå±‚é™…è½¬æ¢å†…éƒ¨è¿è´¯æ€§çš„ç»“æ„åŒ–å±‚çº§ä¸Šè¿è¡Œã€‚è®ºæ–‡æ­£å¼å®šä¹‰äº† SAL çš„è¯­æ³•å’Œè¯­ä¹‰ï¼Œå¼•å…¥äº†å…¬ç†ç³»ç»Ÿï¼Œå¹¶è¯æ˜äº†å…¶ Soundness å’Œ Completenessã€‚è¯¥é€»è¾‘æ¡†æ¶åœ¨ Temporal Becomingã€Quantum Decoherence é¢†åŸŸåŠæ¨¡æ€å½¢è€Œä¸Šå­¦ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚è¿™é¡¹å·¥ä½œä¸ºæ•æ‰ç°å®åŒ–çš„å­˜åœ¨è®ºç»“æ„æä¾›äº†ä¸€ç§åˆ†å±‚æ›¿ä»£æ–¹æ¡ˆï¼Œæœ‰æ•ˆé¿å…äº†å¯¹æ ‡å‡† Modal Realism ä¸­æŠ½è±¡å¯èƒ½ä¸–ç•Œçš„ä¾èµ–ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "This paper develops the formal logical foundations of the stratified actualization framework presented in a companion paper currently under review at Erkenntnis (manuscript ID: ERKE-D-25-00410)",
      "pdf_url": "https://arxiv.org/pdf/2506.17276v1",
      "published_date": "2025-06-12 18:35:01 UTC",
      "updated_date": "2025-06-12 18:35:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:15.548842+00:00"
    },
    {
      "arxiv_id": "2506.11221v1",
      "title": "LLM-as-a-Fuzzy-Judge: Fine-Tuning Large Language Models as a Clinical Evaluation Judge with Fuzzy Logic",
      "title_zh": "LLM-as-a-Fuzzy-Judgeï¼šåŸºäºæ¨¡ç³Šé€»è¾‘å°†å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸ºä¸´åºŠè¯„ä¼°è¯„åˆ¤è€…",
      "authors": [
        "Weibing Zheng",
        "Laurah Turner",
        "Jess Kropczynski",
        "Murat Ozer",
        "Tri Nguyen",
        "Shane Halse"
      ],
      "abstract": "Clinical communication skills are critical in medical education, and practicing and assessing clinical communication skills on a scale is challenging. Although LLM-powered clinical scenario simulations have shown promise in enhancing medical students' clinical practice, providing automated and scalable clinical evaluation that follows nuanced physician judgment is difficult. This paper combines fuzzy logic and Large Language Model (LLM) and proposes LLM-as-a-Fuzzy-Judge to address the challenge of aligning the automated evaluation of medical students' clinical skills with subjective physicians' preferences. LLM-as-a-Fuzzy-Judge is an approach that LLM is fine-tuned to evaluate medical students' utterances within student-AI patient conversation scripts based on human annotations from four fuzzy sets, including Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction. The methodology of this paper started from data collection from the LLM-powered medical education system, data annotation based on multidimensional fuzzy sets, followed by prompt engineering and the supervised fine-tuning (SFT) of the pre-trained LLMs using these human annotations. The results show that the LLM-as-a-Fuzzy-Judge achieves over 80\\% accuracy, with major criteria items over 90\\%, effectively leveraging fuzzy logic and LLM as a solution to deliver interpretable, human-aligned assessment. This work suggests the viability of leveraging fuzzy logic and LLM to align with human preferences, advances automated evaluation in medical education, and supports more robust assessment and judgment practices. The GitHub repository of this work is available at https://github.com/2sigmaEdTech/LLMAsAJudge",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LLM-as-a-Fuzzy-Judge æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ¨¡ç³Šé€»è¾‘ (Fuzzy Logic) ä¸å¤§è¯­è¨€æ¨¡å‹ (LLM)ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦æ•™è‚²ä¸­è‡ªåŠ¨åŒ–è¯„ä¼°ä¸åŒ»ç”Ÿä¸»è§‚åå¥½éš¾ä»¥å¯¹é½çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Professionalismã€Medical Relevanceã€Ethical Behavior å’Œ Contextual Distraction å››ä¸ªç»´åº¦çš„æ¨¡ç³Šé›†åˆå¯¹å­¦ç”Ÿå¯¹è¯æ•°æ®è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶ç»“åˆæç¤ºè¯å·¥ç¨‹å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning, SFT)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯„ä¼°ä¸´åºŠæ²Ÿé€šæŠ€èƒ½æ–¹é¢çš„å‡†ç¡®ç‡è¶…è¿‡ 80%ï¼Œå…¶ä¸­ä¸»è¦è¯„ä»·æŒ‡æ ‡çš„å‡†ç¡®ç‡æ›´æ˜¯çªç ´ 90%ï¼Œå±•ç°äº†æé«˜çš„è¯„ä¼°æ•ˆèƒ½ã€‚è¿™ä¸€å·¥ä½œè¯æ˜äº†åˆ©ç”¨æ¨¡ç³Šé€»è¾‘å¢å¼º LLM å®ç°äººç±»åå¥½å¯¹é½çš„å¯è¡Œæ€§ï¼Œä¸ºåŒ»å­¦æ•™è‚²æä¾›äº†å¯è§£é‡Šã€é«˜ç²¾åº¦çš„è‡ªåŠ¨åŒ–è¯„ä¼°ä¸å†³ç­–æ”¯æŒæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 1 figure, 2025 IFSA World Congress NAFIPS Annual Meeting",
      "pdf_url": "https://arxiv.org/pdf/2506.11221v1",
      "published_date": "2025-06-12 18:31:49 UTC",
      "updated_date": "2025-06-12 18:31:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:24.787435+00:00"
    },
    {
      "arxiv_id": "2507.21077v1",
      "title": "Data-Driven and Participatory Approaches toward Neuro-Inclusive AI",
      "title_zh": "è¿ˆå‘ç¥ç»åŒ…å®¹æ€§äººå·¥æ™ºèƒ½çš„æ•°æ®é©±åŠ¨ä¸å‚ä¸å¼æ–¹æ³•",
      "authors": [
        "Naba Rizvi"
      ],
      "abstract": "Biased data representation in AI marginalizes up to 75 million autistic people worldwide through medical applications viewing autism as a deficit of neurotypical social skills rather than an aspect of human diversity, and this perspective is grounded in research questioning the humanity of autistic people. Turing defined artificial intelligence as the ability to mimic human communication, and as AI development increasingly focuses on human-like agents, this benchmark remains popular. In contrast, we define Neuro-Inclusive AI as datasets and systems that move away from mimicking humanness as a benchmark for machine intelligence. Then, we explore the origins, prevalence, and impact of anti-autistic biases in current research. Our work finds that 90% of human-like AI agents exclude autistic perspectives, and AI creators continue to believe ethical considerations are beyond the scope of their work. To improve the autistic representation in data, we conduct empirical experiments with annotators and LLMs, finding that binary labeling schemes sufficiently capture the nuances of labeling anti-autistic hate speech. Our benchmark, AUTALIC, can be used to evaluate or fine-tune models, and was developed to serve as a foundation for more neuro-inclusive future work.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†AIæ•°æ®è¡¨ç¤ºä¸­çš„åå·®å¦‚ä½•è¾¹ç¼˜åŒ–å…¨çƒçº¦7500ä¸‡è‡ªé—­ç—‡ç¾¤ä½“ï¼ŒæŒ‡å‡ºå½“å‰ç ”ç©¶å¾€å¾€å°†è‡ªé—­ç—‡è§†ä¸ºç¤¾äº¤ç¼ºé™·è€Œéäººç±»å¤šæ ·æ€§çš„è¡¨ç°ã€‚ä½œè€…æå‡ºäº†Neuro-Inclusive AIçš„æ¦‚å¿µï¼Œä¸»å¼ æ•°æ®é›†å’Œç³»ç»Ÿåº”èƒŒç¦»å°†æ¨¡ä»¿äººç±»ç‰¹å¾(mimicking humanness)ä½œä¸ºæœºå™¨æ™ºèƒ½è¡¡é‡æ ‡å‡†çš„ä¼ ç»ŸèŒƒå¼ã€‚ç ”ç©¶é€šè¿‡è°ƒæŸ¥å‘ç°ï¼Œ90%çš„ç±»äººAIæ™ºèƒ½ä½“æ’é™¤äº†è‡ªé—­ç—‡è§†è§’ï¼Œä¸”å¼€å‘è€…æ™®éç¼ºä¹å°†ä¼¦ç†çº³å…¥å·¥ä½œèŒƒç•´çš„æ„è¯†ã€‚ä¸ºæå‡æ•°æ®ä¸­çš„è‡ªé—­ç—‡ä»£è¡¨æ€§ï¼Œç ”ç©¶åˆ©ç”¨æ ‡æ³¨å‘˜å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†äºŒå…ƒæ ‡æ³¨æ–¹æ¡ˆåœ¨è¯†åˆ«é’ˆå¯¹è‡ªé—­ç—‡ä»‡æ¨è¨€è®ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œç ”ç©¶æ¨å‡ºäº†AUTALICåŸºå‡†æµ‹è¯•é›†ï¼Œæ—¨åœ¨ä¸ºæ¨¡å‹çš„è¯„ä¼°ä¸å¾®è°ƒæä¾›æ”¯æŒï¼Œå¹¶ä¸ºæœªæ¥ç¥ç»åŒ…å®¹æ€§AIçš„å‘å±•æ„å»ºåŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "PhD Dissertation at UC San Diego (June 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.21077v1",
      "published_date": "2025-06-12 18:23:54 UTC",
      "updated_date": "2025-06-12 18:23:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:39.690119+00:00"
    },
    {
      "arxiv_id": "2506.11214v1",
      "title": "Complexity of normalized stochastic first-order methods with momentum under heavy-tailed noise",
      "title_zh": "é‡å°¾å™ªå£°ä¸‹å¸¦åŠ¨é‡çš„å½’ä¸€åŒ–éšæœºä¸€é˜¶æ–¹æ³•çš„å¤æ‚åº¦",
      "authors": [
        "Chuan He",
        "Zhaosong Lu",
        "Defeng Sun",
        "Zhanwang Deng"
      ],
      "abstract": "In this paper, we propose practical normalized stochastic first-order methods with Polyak momentum, multi-extrapolated momentum, and recursive momentum for solving unconstrained optimization problems. These methods employ dynamically updated algorithmic parameters and do not require explicit knowledge of problem-dependent quantities such as the Lipschitz constant or noise bound. We establish first-order oracle complexity results for finding approximate stochastic stationary points under heavy-tailed noise and weakly average smoothness conditions -- both of which are weaker than the commonly used bounded variance and mean-squared smoothness assumptions. Our complexity bounds either improve upon or match the best-known results in the literature. Numerical experiments are presented to demonstrate the practical effectiveness of the proposed methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å‡ ç§å®ç”¨çš„å½’ä¸€åŒ–éšæœºä¸€é˜¶æ–¹æ³• (normalized stochastic first-order methods)ï¼Œåˆ†åˆ«ç»“åˆäº† Polyak momentumã€å¤šé‡å¤–æ’åŠ¨é‡ (multi-extrapolated momentum) å’Œé€’å½’åŠ¨é‡ (recursive momentum) æŠ€æœ¯ï¼Œç”¨äºè§£å†³æ— çº¦æŸä¼˜åŒ–é—®é¢˜ã€‚è¿™äº›æ–¹æ³•é‡‡ç”¨åŠ¨æ€æ›´æ–°çš„ç®—æ³•å‚æ•°ï¼Œä¸”ä¸éœ€è¦é¢„å…ˆè·çŸ¥ Lipschitz constant æˆ–å™ªå£°è¾¹ç•Œ (noise bound) ç­‰ä¸é—®é¢˜ç›¸å…³çš„å…·ä½“æ•°å€¼ã€‚ç ”ç©¶åœ¨é‡å°¾å™ªå£° (heavy-tailed noise) å’Œå¼±å¹³å‡å¹³æ»‘ (weakly average smoothness) æ¡ä»¶ä¸‹ï¼Œå»ºç«‹äº†å¯»æ‰¾è¿‘ä¼¼éšæœºé©»ç‚¹ (stochastic stationary points) çš„ä¸€é˜¶ç®—è°•å¤æ‚åº¦ (first-order oracle complexity) ç»“æœã€‚è¿™äº›å‡è®¾æ¡ä»¶æ¯”å¸¸ç”¨çš„æœ‰ç•Œæ–¹å·® (bounded variance) å’Œå‡æ–¹å¹³æ»‘ (mean-squared smoothness) å‡è®¾æ›´å®½æ³›ï¼Œæ˜¾è‘—å¢å¼ºäº†ç†è®ºçš„é€‚ç”¨æ€§ã€‚è¯¥ç ”ç©¶å¾—å‡ºçš„å¤æ‚åº¦ç•Œé™ (complexity bounds) è¾¾åˆ°æˆ–ä¼˜äºç›®å‰æ–‡çŒ®ä¸­çš„æœ€ä¼˜ç»“æœï¼Œæ•°å€¼å®éªŒä¹Ÿè¿›ä¸€æ­¥éªŒè¯äº†æ‰€ææ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.CC",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11214v1",
      "published_date": "2025-06-12 18:21:15 UTC",
      "updated_date": "2025-06-12 18:21:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:39.152265+00:00"
    },
    {
      "arxiv_id": "2507.14140v1",
      "title": "Geophysics-informed neural network for model-based seismic inversion using surrogate point spread functions",
      "title_zh": "åŸºäºä»£ç†ç‚¹æ‰©æ•£å‡½æ•°è¿›è¡Œæ¨¡å‹åŒ–åœ°éœ‡åæ¼”çš„åœ°çƒç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ",
      "authors": [
        "Marcus Saraiva",
        "Ana Muller",
        "Alexandre Maul"
      ],
      "abstract": "Model-based seismic inversion is a key technique in reservoir characterization, but traditional methods face significant limitations, such as relying on 1D average stationary wavelets and assuming an unrealistic lateral resolution. To address these challenges, we propose a Geophysics-Informed Neural Network (GINN) that integrates deep learning with seismic modeling. This novel approach employs a Deep Convolutional Neural Network (DCNN) to simultaneously estimate Point Spread Functions (PSFs) and acoustic impedance (IP). PSFs are divided into zero-phase and residual components to ensure geophysical consistency and to capture fine details. We used synthetic data from the SEAM Phase I Earth Model to train the GINN for 100 epochs (approximately 20 minutes) using a 2D UNet architecture. The network's inputs include positional features and a low-frequency impedance (LF-IP) model. A self-supervised loss function combining Mean Squared Error (MSE) and Structural Similarity Index Measure (SSIM) was employed to ensure accurate results. The GINN demonstrated its ability to generate high-resolution IP and realistic PSFs, aligning with expected geological features. Unlike traditional 1D wavelets, the GINN produces PSFs with limited lateral resolution, reducing noise and improving accuracy. Future work will aim to refine the training process and validate the methodology with real seismic data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»ŸåŸºäºæ¨¡å‹çš„åœ°éœ‡åæ¼”(model-based seismic inversion)ä¾èµ–ä¸€ç»´å¹³å‡å¹³ç¨³å­æ³¢ä¸”æ¨ªå‘åˆ†è¾¨ç‡å‡è®¾ä¸åˆ‡å®é™…çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åœ°çƒç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(Geophysics-Informed Neural Network, GINN)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ(DCNN)åŒæ—¶ä¼°è®¡ç‚¹æ‰©æ•£å‡½æ•°(Point Spread Functions, PSFs)å’Œå£°æ³¢é˜»æŠ—(acoustic impedance, IP)ï¼Œå¹¶å°† PSFs åˆ†è§£ä¸ºé›¶ç›¸ä½å’Œæ®‹å·®åˆ†é‡ä»¥ç¡®ä¿åœ°çƒç‰©ç†ä¸€è‡´æ€§å¹¶æ•æ‰ç²¾ç»†ç»†èŠ‚ã€‚ç½‘ç»œé‡‡ç”¨äºŒç»´ UNet æ¶æ„ï¼Œå°†ä½ç½®ç‰¹å¾å’Œä½é¢‘é˜»æŠ—(LF-IP)æ¨¡å‹ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡ç»“åˆå‡æ–¹è¯¯å·®(MSE)å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°(SSIM)çš„è‡ªç›‘ç£æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚åœ¨ SEAM Phase I åœ°çƒæ¨¡å‹åˆæˆæ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGINN èƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ IP å’Œæ›´å…·çœŸå®æ„Ÿçš„ PSFsï¼Œæœ‰æ•ˆå‡å°‘äº†å™ªå£°å¹¶æ˜¾è‘—æå‡äº†åæ¼”ç²¾åº¦ã€‚è¯¥ç ”ç©¶æˆåŠŸå±•ç¤ºäº†æ·±åº¦å­¦ä¹ ä¸åœ°éœ‡å»ºæ¨¡ç»“åˆçš„æ½œåŠ›ï¼Œä¸ºå¤„ç†å¤æ‚çš„çœŸå®åœ°éœ‡æ•°æ®åæ¼”å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "physics.geo-ph",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.14140v1",
      "published_date": "2025-06-12 18:10:11 UTC",
      "updated_date": "2025-06-12 18:10:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:43.972920+00:00"
    },
    {
      "arxiv_id": "2506.10982v3",
      "title": "Rethinking Losses for Diffusion Bridge Samplers",
      "title_zh": "é‡æ–°å®¡è§†æ‰©æ•£æ¡¥é‡‡æ ·å™¨çš„æŸå¤±å‡½æ•°",
      "authors": [
        "Sebastian Sanokowski",
        "Lukas Gruber",
        "Christoph Bartmann",
        "Sepp Hochreiter",
        "Sebastian Lehner"
      ],
      "abstract": "Diffusion bridges are a promising class of deep-learning methods for sampling from unnormalized distributions. Recent works show that the Log Variance (LV) loss consistently outperforms the reverse Kullback-Leibler (rKL) loss when using the reparametrization trick to compute rKL-gradients. While the on-policy LV loss yields identical gradients to the rKL loss when combined with the log-derivative trick for diffusion samplers with non-learnable forward processes, this equivalence does not hold for diffusion bridges or when diffusion coefficients are learned. Based on this insight we argue that for diffusion bridges the LV loss does not represent an optimization objective that can be motivated like the rKL loss via the data processing inequality. Our analysis shows that employing the rKL loss with the log-derivative trick (rKL-LD) does not only avoid these conceptual problems but also consistently outperforms the LV loss. Experimental results with different types of diffusion bridges on challenging benchmarks show that samplers trained with the rKL-LD loss achieve better performance. From a practical perspective we find that rKL-LD requires significantly less hyperparameter optimization and yields more stable training behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ç”¨äºéå½’ä¸€åŒ–åˆ†å¸ƒé‡‡æ ·çš„Diffusion Bridge Samplersçš„æŸå¤±å‡½æ•°è¿›è¡Œäº†é‡æ–°å®¡è§†ã€‚ä½œè€…æŒ‡å‡ºï¼Œè™½ç„¶Log Variance (LV)æŸå¤±åœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°è¾ƒå¥½ï¼Œä½†å¯¹äºDiffusion bridgesè€Œè¨€ï¼ŒLVæŸå¤±ç¼ºä¹ç±»ä¼¼é€†Kullback-Leibler (rKL)æŸå¤±é‚£æ ·çš„ç†è®ºåŠ¨æœºï¼Œä¸”ä¸ç¬¦åˆæ•°æ®å¤„ç†ä¸ç­‰å¼(data processing inequality)ã€‚åˆ†æè¡¨æ˜ï¼Œç»“åˆå¯¹æ•°å¯¼æ•°æŠ€å·§(log-derivative trick)çš„rKLæŸå¤±(rKL-LD)ä¸ä»…èƒ½è§£å†³ä¸Šè¿°ç†è®ºç¼ºé™·ï¼Œä¸”åœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºLVæŸå¤±ã€‚åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åŸºå‡†ä¸Šçš„å®éªŒç»“æœè¯å®ï¼Œä½¿ç”¨rKL-LDè®­ç»ƒçš„é‡‡æ ·å™¨å…·æœ‰æ›´ä¼˜çš„é‡‡æ ·æ•ˆæœã€‚æ­¤å¤–ï¼Œä»å®é™…åº”ç”¨è§’åº¦çœ‹ï¼ŒrKL-LDåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å¾—æ›´åŠ ç¨³å®šï¼Œå¹¶å¤§å¹…å‡å°‘äº†å¯¹è¶…å‚æ•°ä¼˜åŒ–çš„ä¾èµ–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025 as a Conference Paper",
      "pdf_url": "https://arxiv.org/pdf/2506.10982v3",
      "published_date": "2025-06-12 17:59:58 UTC",
      "updated_date": "2025-11-11 08:58:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:13:01.265604+00:00"
    },
    {
      "arxiv_id": "2506.10978v4",
      "title": "Where and How to Perturb: On the Design of Perturbation Guidance in Diffusion and Flow Models",
      "title_zh": "æ‰°åŠ¨ä½•å¤„ä¸å¦‚ä½•æ‰°åŠ¨ï¼šè®ºæ‰©æ•£æ¨¡å‹ä¸æµæ¨¡å‹ä¸­çš„æ‰°åŠ¨å¼•å¯¼è®¾è®¡",
      "authors": [
        "Donghoon Ahn",
        "Jiwon Kang",
        "Sanghyun Lee",
        "Minjae Kim",
        "Jaewon Min",
        "Wooseok Jang",
        "Sangwu Lee",
        "Sayak Paul",
        "Susung Hong",
        "Seungryong Kim"
      ],
      "abstract": "Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose \"HeadHunter\", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹å’Œæµæ¨¡å‹ä¸­æ‰°åŠ¨å¼•å¯¼(Perturbation Guidance)çš„åº”ç”¨ï¼Œæ¢è®¨äº†åœ¨ Diffusion Transformer (DiT) æ¶æ„ä¸­ç¡®å®šæ‰°åŠ¨ä½ç½®çš„åŸåˆ™æ€§æ–¹æ³•ã€‚é€šè¿‡ç ”ç©¶ä»å±‚çº§åˆ°å•ä¸ªæ³¨æ„åŠ›å¤´(Attention Head)çš„æ‰°åŠ¨ç²’åº¦ï¼Œä½œè€…å‘ç°ç‰¹å®šçš„æ³¨æ„åŠ›å¤´åˆ†åˆ«æ§åˆ¶ç»“æ„ã€é£æ ¼å’Œçº¹ç†è´¨é‡ç­‰ä¸åŒçš„è§†è§‰æ¦‚å¿µã€‚æ®æ­¤ï¼Œç ”ç©¶æå‡ºäº† HeadHunter æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¿­ä»£é€‰æ‹©ç¬¦åˆç”¨æˆ·ç›®æ ‡çš„æ³¨æ„åŠ›å¤´ï¼Œå®ç°å¯¹ç”Ÿæˆè´¨é‡å’Œè§†è§‰å±æ€§çš„ç»†ç²’åº¦æ§åˆ¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº† SoftPAG æŠ€æœ¯ï¼Œé€šè¿‡çº¿æ€§æ’å€¼æ³¨æ„åŠ›å›¾æ¥è°ƒèŠ‚æ‰°åŠ¨å¼ºåº¦å¹¶æŠ‘åˆ¶ä¼ªå½±ã€‚å®éªŒåœ¨ Stable Diffusion 3 å’Œ FLUX.1 ç­‰å¤§è§„æ¨¡æ¨¡å‹ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶ä¸ä»…èƒ½ç¼“è§£ç°æœ‰å±‚çº§æ‰°åŠ¨å¯¼è‡´çš„è¿‡å¹³æ»‘é—®é¢˜ï¼Œè¿˜èƒ½é€šè¿‡ç»„åˆæ³¨æ„åŠ›å¤´é€‰æ‹©å®ç°ç‰¹å®šé£æ ¼çš„å®šå‘æ“çºµã€‚è¯¥å·¥ä½œæä¾›äº†æ‰©æ•£æ¨¡å‹ä¸­æ³¨æ„åŠ›æ‰°åŠ¨çš„é¦–ä¸ªå¤´çº§åˆ†æï¼Œæ­ç¤ºäº†æ³¨æ„åŠ›å±‚å†…éƒ¨çš„å¯è§£é‡Šä¸“ä¸šåŒ–åˆ†å·¥ï¼Œä¸ºè®¾è®¡é«˜æ•ˆçš„æ‰°åŠ¨ç­–ç•¥æä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at NeurIPS 2025. Project page: https://cvlab-kaist.github.io/HeadHunter/",
      "pdf_url": "https://arxiv.org/pdf/2506.10978v4",
      "published_date": "2025-06-12 17:59:51 UTC",
      "updated_date": "2025-11-03 01:30:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:13:00.887617+00:00"
    },
    {
      "arxiv_id": "2506.10974v3",
      "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
      "title_zh": "AutoMindï¼šé¢å‘è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦çš„è‡ªé€‚åº”çŸ¥è¯†å‹æ™ºèƒ½ä½“",
      "authors": [
        "Yixin Ou",
        "Yujie Luo",
        "Jingsheng Zheng",
        "Lanning Wei",
        "Zhuoyun Yu",
        "Shuofei Qiao",
        "Jintian Zhang",
        "Da Zheng",
        "Yuren Mao",
        "Yunjun Gao",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science. Code is at https://github.com/innovatingAI/AutoMind.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoMindï¼Œä¸€ç§è‡ªé€‚åº”ä¸”å…·å¤‡çŸ¥è¯†åº“çš„ Large Language Model (LLM) æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦ (Automated Data Science) æ¡†æ¶å› æµç¨‹åƒµåŒ–è€Œéš¾ä»¥æ•æ‰äººç±»ä¸“å®¶ç»éªŒçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼šé€šè¿‡ä¸“å®¶çŸ¥è¯†åº“ (expert knowledge base) ä¸ºæ™ºèƒ½ä½“æä¾›é¢†åŸŸçŸ¥è¯†æ”¯æŒï¼Œåˆ©ç”¨æ™ºèƒ½ä½“çŸ¥è¯†åŒ–æ ‘æœç´¢ç®—æ³• (agentic knowledgeable tree search algorithm) ç­–ç•¥æ€§åœ°æ¢ç´¢æœ€ä¼˜è§£ï¼Œä»¥åŠé‡‡ç”¨è‡ªé€‚åº”ç¼–ç ç­–ç•¥ (self-adaptive coding strategy) æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€ç”Ÿæˆä»£ç ã€‚åœ¨ä¸¤ä¸ªè‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoMind çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå½“å‰çš„åŸºå‡†æ¨¡å‹ã€‚åˆ†æè¿›ä¸€æ­¥è¯å®äº†è¯¥æ¡†æ¶åœ¨æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œæ–¹æ¡ˆè´¨é‡ä¸Šçš„ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºå®ç°å…¨è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦ç›®æ ‡çš„ç¨³å¥è¿›å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Ongoing work",
      "pdf_url": "https://arxiv.org/pdf/2506.10974v3",
      "published_date": "2025-06-12 17:59:32 UTC",
      "updated_date": "2025-10-08 17:06:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:56.909313+00:00"
    },
    {
      "arxiv_id": "2506.10973v1",
      "title": "Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning",
      "title_zh": "é¢å‘ç®—å­å­¦ä¹ ï¼šå°†ç¥ç»ç½‘ç»œæ¶æ„æ‰©å±•è‡³å‡½æ•°ç©ºé—´çš„è§„èŒƒåŒ–æ–¹æ³•",
      "authors": [
        "Julius Berner",
        "Miguel Liu-Schiaffini",
        "Jean Kossaifi",
        "Valentin Duruisseaux",
        "Boris Bonev",
        "Kamyar Azizzadenesheli",
        "Anima Anandkumar"
      ],
      "abstract": "A wide range of scientific problems, such as those described by continuous-time dynamical systems and partial differential equations (PDEs), are naturally formulated on function spaces. While function spaces are typically infinite-dimensional, deep learning has predominantly advanced through applications in computer vision and natural language processing that focus on mappings between finite-dimensional spaces. Such fundamental disparities in the nature of the data have limited neural networks from achieving a comparable level of success in scientific applications as seen in other fields. Neural operators are a principled way to generalize neural networks to mappings between function spaces, offering a pathway to replicate deep learning's transformative impact on scientific problems. For instance, neural operators can learn solution operators for entire classes of PDEs, e.g., physical systems with different boundary conditions, coefficient functions, and geometries. A key factor in deep learning's success has been the careful engineering of neural architectures through extensive empirical testing. Translating these neural architectures into neural operators allows operator learning to enjoy these same empirical optimizations. However, prior neural operator architectures have often been introduced as standalone models, not directly derived as extensions of existing neural network architectures. In this paper, we identify and distill the key principles for constructing practical implementations of mappings between infinite-dimensional function spaces. Using these principles, we propose a recipe for converting several popular neural architectures into neural operators with minimal modifications. This paper aims to guide practitioners through this process and details the steps to make neural operators work in practice. Our code can be found at https://github.com/neuraloperator/NNs-to-NOs",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†ä¼ ç»Ÿçš„æœ‰é™ç»´ç¥ç»ç½‘ç»œæ¶æ„æ‰©å±•åˆ°æ— é™ç»´å‡½æ•°ç©ºé—´ï¼ˆFunction Spacesï¼‰ä»¥è¿›è¡Œç®—å­å­¦ä¹ ï¼ˆOperator Learningï¼‰ï¼Œæ—¨åœ¨é«˜æ•ˆè§£å†³åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰ç­‰å¤æ‚çš„ç§‘å­¦è®¡ç®—é—®é¢˜ã€‚å°½ç®¡ç¥ç»ç®—å­ï¼ˆNeural Operatorsï¼‰æ˜¯å¤„ç†æ­¤ç±»æ˜ å°„çš„åŸåˆ™æ€§æ–¹æ³•ï¼Œä½†ç°æœ‰æ¨¡å‹é€šå¸¸ä½œä¸ºç‹¬ç«‹æ¶æ„è¢«å¼•å…¥ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ç°æœ‰æˆç†Ÿæ·±åº¦å­¦ä¹ æ¶æ„çš„ç»éªŒä¼˜åŒ–ã€‚æœ¬æ–‡é€šè¿‡è¯†åˆ«å¹¶æå–åœ¨æ— é™ç»´ç©ºé—´ä¹‹é—´æ„å»ºå®ç”¨æ˜ å°„çš„å…³é”®åŸåˆ™ï¼Œæå‡ºäº†ä¸€å¥—å°†æµè¡Œç¥ç»æ¶æ„ï¼ˆNeural Architecturesï¼‰è½¬åŒ–ä¸ºç¥ç»ç®—å­çš„é€šç”¨æ–¹æ¡ˆï¼ˆRecipeï¼‰ã€‚è¯¥æ–¹æ³•ä»…éœ€æå°çš„ä¿®æ”¹å³å¯å®ç°è½¬æ¢ï¼Œå¹¶è¯¦ç»†é˜è¿°äº†åœ¨å®è·µä¸­ç¡®ä¿ç¥ç»ç®—å­æœ‰æ•ˆè¿è¡Œçš„å…·ä½“æ­¥éª¤ã€‚è¿™é¡¹å·¥ä½œä¸ºä»ä¸šè€…æä¾›äº†ç³»ç»ŸåŒ–çš„æŒ‡å¯¼ï¼Œæ—¨åœ¨å°†æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸçš„æˆåŠŸç»éªŒå¤åˆ¶åˆ°ç§‘å­¦åº”ç”¨ä¸­ï¼Œå¹¶æä¾›äº†å¼€æºä»£ç ä»¥æ”¯æŒå®é™…éƒ¨ç½²ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.FA",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10973v1",
      "published_date": "2025-06-12 17:59:31 UTC",
      "updated_date": "2025-06-12 17:59:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:14:10.736039+00:00"
    },
    {
      "arxiv_id": "2506.10972v3",
      "title": "Predictable Scale: Part II, Farseer: A Refined Scaling Law in Large Language Models",
      "title_zh": "å¯é¢„æµ‹çš„è§„æ¨¡ï¼ˆç¬¬äºŒéƒ¨åˆ†ï¼‰ï¼šFarseerï¼Œå¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç²¾ç‚¼ç¼©æ”¾æ³•åˆ™",
      "authors": [
        "Houyi Li",
        "Wenzhen Zheng",
        "Qiufeng Wang",
        "Zhenyu Ding",
        "Haoying Wang",
        "Zili Wang",
        "Shijie Xuyang",
        "Ning Ding",
        "Shuigeng Zhou",
        "Xiangyu Zhang",
        "Daxin Jiang"
      ],
      "abstract": "Training Large Language Models (LLMs) is prohibitively expensive, creating a critical scaling gap where insights from small-scale experiments often fail to transfer to resource-intensive production systems, thereby hindering efficient innovation. To bridge this, we introduce Farseer, a novel and refined scaling law offering enhanced predictive accuracy across scales. By systematically constructing a model loss surface $L(N,D)$, Farseer achieves a significantly better fit to empirical data than prior laws (e.g., Chinchilla's law). Our methodology yields accurate, robust, and highly generalizable predictions, demonstrating excellent extrapolation capabilities, improving upon Chinchilla's law by reducing extrapolation error by 433\\%. This allows for the reliable evaluation of competing training strategies across all $(N,D)$ settings, enabling conclusions from small-scale ablation studies to be confidently extrapolated to predict large-scale performance. Furthermore, Farseer provides new insights into optimal compute allocation, better reflecting the nuanced demands of modern LLM training. To validate our approach, we trained an extensive suite of approximately 1,000 LLMs across diverse scales and configurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are comprehensively open-sourcing all models, data, results, and logs at https://github.com/Farseer-Scaling-Law/Farseer to foster further research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Farseerï¼Œè¿™æ˜¯ä¸€ç§ç²¾ç»†çš„å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)æ‰©å±•å®šå¾‹(Scaling Law)ï¼Œæ—¨åœ¨è§£å†³å°è§„æ¨¡å®éªŒç»“è®ºéš¾ä»¥å‡†ç¡®æ¨å¯¼è‡³å¤§è§„æ¨¡ç”Ÿäº§ç³»ç»Ÿçš„é—®é¢˜ã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°æ„å»ºæ¨¡å‹æŸå¤±æ›²é¢ $L(N,D)$ï¼ŒFarseer å®ç°äº†æ¯” Chinchilla å®šå¾‹æ›´ä¼˜çš„æ•°æ®æ‹Ÿåˆåº¦ï¼Œå¹¶å°†å¤–æ¨è¯¯å·®æ˜¾è‘—é™ä½äº† 433%ã€‚è¯¥æ–¹æ³•å…·æœ‰æå¼ºçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å¾—ç ”ç©¶è€…èƒ½å¤ŸåŸºäºå°è§„æ¨¡æ¶ˆèå®éªŒ(Ablation Studies)å¯é åœ°è¯„ä¼°è®­ç»ƒç­–ç•¥å¹¶é¢„æµ‹å¤§è§„æ¨¡æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFarseer è¿˜åœ¨æœ€ä¼˜è®¡ç®—èµ„æºåˆ†é…æ–¹é¢æä¾›äº†æ–°çš„è§è§£ï¼Œæ›´å‡†ç¡®åœ°åæ˜ äº†ç°ä»£ LLM è®­ç»ƒçš„ç»†å¾®éœ€æ±‚ã€‚ä¸ºäº†éªŒè¯è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç ”ç©¶å›¢é˜Ÿè®­ç»ƒäº†çº¦ 1,000 ä¸ªä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼Œæ¶ˆè€—çº¦ 300 ä¸‡ NVIDIA H100 GPU å°æ—¶ï¼Œå¹¶å…¨é¢å¼€æºäº†ç›¸å…³èµ„æºã€‚è¿™ä¸€æˆæœä¸ºé«˜æ•ˆåˆ›æ–°å’Œè·¨è§„æ¨¡æ€§èƒ½é¢„æµ‹å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "34",
      "pdf_url": "https://arxiv.org/pdf/2506.10972v3",
      "published_date": "2025-06-12 17:59:23 UTC",
      "updated_date": "2025-07-16 07:09:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:12:58.955304+00:00"
    },
    {
      "arxiv_id": "2506.10967v2",
      "title": "Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs",
      "title_zh": "è¶…è¶Šæ³¨æ„åŠ›ä¸ç›¸ä¼¼åº¦ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­åŸºäºæ¡ä»¶å¤šæ ·æ€§æœ€å¤§åŒ–çš„ Token å‰ªæ",
      "authors": [
        "Qizhe Zhang",
        "Mengzhen Liu",
        "Lichen Li",
        "Ming Lu",
        "Yuan Zhang",
        "Junwen Pan",
        "Qi She",
        "Shanghang Zhang"
      ],
      "abstract": "In multimodal large language models (MLLMs), the length of input visual tokens is often significantly greater than that of their textual counterparts, leading to a high inference cost. Many works aim to address this issue by removing redundant visual tokens. However, current approaches either rely on attention-based pruning, which retains numerous duplicate tokens, or use similarity-based pruning, overlooking the instruction relevance, consequently causing suboptimal performance. In this paper, we go beyond attention or similarity by proposing a novel visual token pruning method named CDPruner, which maximizes the conditional diversity of retained tokens. We first define the conditional similarity between visual tokens conditioned on the instruction, and then reformulate the token pruning problem with determinantal point process (DPP) to maximize the conditional diversity of the selected subset. The proposed CDPruner is training-free and model-agnostic, allowing easy application to various MLLMs. Extensive experiments across diverse MLLMs show that CDPruner establishes new state-of-the-art on various vision-language benchmarks. By maximizing conditional diversity through DPP, the selected subset better represents the input images while closely adhering to user instructions, thereby preserving strong performance even with high reduction ratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\\% and CUDA latency by 78\\%, while maintaining 94\\% of the original accuracy. Our code is available at https://github.com/Theia-4869/CDPruner.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)ä¸­è§†è§‰Tokenè¿‡é•¿å¯¼è‡´çš„æ¨ç†æˆæœ¬é«˜æ˜‚é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰åŸºäºæ³¨æ„åŠ›(attention-based)æˆ–ç›¸ä¼¼åº¦(similarity-based)å‰ªææ–¹æ³•åœ¨å¤„ç†å†—ä½™å’ŒæŒ‡ä»¤ç›¸å…³æ€§æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†CDPrunerï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æœ€å¤§åŒ–ä¿ç•™Tokenæ¡ä»¶å¤šæ ·æ€§(conditional diversity)çš„æ–°å‹è§†è§‰Tokenå‰ªææ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆå®šä¹‰äº†ä»¥æŒ‡ä»¤ä¸ºæ¡ä»¶çš„è§†è§‰Tokené—´æ¡ä»¶ç›¸ä¼¼åº¦ï¼Œå¹¶åˆ©ç”¨è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹(Determinantal Point Process, DPP)é‡æ–°æ„å»ºå‰ªæé—®é¢˜ï¼Œä»¥é€‰æ‹©æ—¢èƒ½ä»£è¡¨å›¾åƒå†…å®¹åˆé«˜åº¦å¥‘åˆç”¨æˆ·æŒ‡ä»¤çš„Tokenå­é›†ã€‚CDPrunerå…·æœ‰æ— éœ€è®­ç»ƒ(training-free)ä¸”æ¨¡å‹æ— å…³(model-agnostic)çš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿçµæ´»åº”ç”¨äºå„ç±»MLLMsã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜å‹ç¼©ç‡ä¸‹ä»èƒ½ä¿æŒå¼ºå¤§çš„æ¨¡å‹èƒ½åŠ›ã€‚åœ¨LLaVAæ¨¡å‹ä¸Šçš„åº”ç”¨è¡¨æ˜ï¼ŒCDPruneråœ¨å‡å°‘95%çš„FLOPså’Œ78%çš„CUDAå»¶è¿Ÿçš„åŒæ—¶ï¼Œä»èƒ½ä¿æŒåŸå§‹å‡†ç¡®ç‡çš„94%ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 5 figures, code: https://github.com/Theia-4869/CDPruner, project page: https://theia-4869.github.io/CDPruner",
      "pdf_url": "https://arxiv.org/pdf/2506.10967v2",
      "published_date": "2025-06-12 17:59:09 UTC",
      "updated_date": "2025-07-01 08:19:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:13:11.875678+00:00"
    },
    {
      "arxiv_id": "2506.10962v1",
      "title": "SpectralAR: Spectral Autoregressive Visual Generation",
      "title_zh": "SpectralARï¼šé¢‘è°±è‡ªå›å½’è§†è§‰ç”Ÿæˆ",
      "authors": [
        "Yuanhui Huang",
        "Weiliang Chen",
        "Wenzhao Zheng",
        "Yueqi Duan",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Autoregressive visual generation has garnered increasing attention due to its scalability and compatibility with other modalities compared with diffusion models. Most existing methods construct visual sequences as spatial patches for autoregressive generation. However, image patches are inherently parallel, contradicting the causal nature of autoregressive modeling. To address this, we propose a Spectral AutoRegressive (SpectralAR) visual generation framework, which realizes causality for visual sequences from the spectral perspective. Specifically, we first transform an image into ordered spectral tokens with Nested Spectral Tokenization, representing lower to higher frequency components. We then perform autoregressive generation in a coarse-to-fine manner with the sequences of spectral tokens. By considering different levels of detail in images, our SpectralAR achieves both sequence causality and token efficiency without bells and whistles. We conduct extensive experiments on ImageNet-1K for image reconstruction and autoregressive generation, and SpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project page: https://huang-yh.github.io/spectralar/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SpectralARï¼Œä¸€ç§ä»é¢‘è°±è§†è§’å®ç°å› æœæ€§çš„è§†è§‰ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸåŸºäºç©ºé—´è¡¥ä¸(spatial patches)çš„è‡ªå›å½’æ¨¡å‹åœ¨å¤„ç†å›¾åƒå›ºæœ‰å¹¶è¡Œæ€§ä¸è‡ªå›å½’å› æœå»ºæ¨¡ä¹‹é—´çš„çŸ›ç›¾ã€‚SpectralARæ ¸å¿ƒé‡‡ç”¨äº†åµŒå¥—é¢‘è°±åˆ†è¯(Nested Spectral Tokenization)æŠ€æœ¯ï¼Œå°†å›¾åƒè½¬åŒ–ä¸ºä»ä½é¢‘åˆ°é«˜é¢‘æœ‰åºæ’åˆ—çš„é¢‘è°±æ ‡è®°(spectral tokens)ã€‚æ¡†æ¶é€šè¿‡ç”±ç²—åˆ°ç»†(coarse-to-fine)çš„æ–¹å¼è¿›è¡Œè‡ªå›å½’åºåˆ—ç”Ÿæˆï¼Œåœ¨æ•æ‰ä¸åŒç»†èŠ‚å±‚æ¬¡çš„åŒæ—¶å®ç°äº†æé«˜çš„æ ‡è®°æ•ˆç‡(token efficiency)ã€‚åœ¨ImageNet-1Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSpectralARä»…éœ€64ä¸ªæ ‡è®°å’Œ310Må‚æ•°å³å¯è¾¾åˆ°3.02çš„gFIDã€‚è¿™ä¸€æˆæœè¯æ˜äº†SpectralARåœ¨ä¿æŒæ¨¡å‹å¯æ‰©å±•æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä»¥æ›´ç®€æ´çš„æ¶æ„å®ç°é«˜è´¨é‡çš„è§†è§‰å†…å®¹ç”Ÿæˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://huang-yh.github.io/spectralar/",
      "pdf_url": "https://arxiv.org/pdf/2506.10962v1",
      "published_date": "2025-06-12 17:57:44 UTC",
      "updated_date": "2025-06-12 17:57:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:14:08.673026+00:00"
    },
    {
      "arxiv_id": "2506.10960v3",
      "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
      "title_zh": "ChineseHarm-Benchï¼šä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹åŸºå‡†",
      "authors": [
        "Kangwei Liu",
        "Siyuan Cheng",
        "Bozhong Tian",
        "Xiaozhuan Liang",
        "Yuyang Yin",
        "Meng Han",
        "Ningyu Zhang",
        "Bryan Hooi",
        "Xi Chen",
        "Shumin Deng"
      ],
      "abstract": "Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸­æ–‡æœ‰å®³å†…å®¹æ£€æµ‹èµ„æºåŒ®ä¹ä¸”è¦†ç›–é¢æœ‰é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º ChineseHarm-Bench çš„å…¨é¢ã€ä¸“ä¸šæ ‡æ³¨çš„åŸºå‡†æµ‹è¯•é›†ã€‚è¯¥åŸºå‡†å®Œå…¨æ„å»ºè‡ªçœŸå®ä¸–ç•Œæ•°æ®ï¼Œæ¶µç›–äº†å…­ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„æœ‰å®³å†…å®¹ç±»åˆ«ï¼Œå¡«è¡¥äº†ç°æœ‰ä¸­æ–‡æ£€æµ‹èµ„æºçš„ç©ºç™½ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æ ‡æ³¨è¿‡ç¨‹ä¸­åŒæ­¥æ„å»ºäº†ä¸€ä¸ªçŸ¥è¯†è§„åˆ™åº“ (knowledge rule base)ï¼Œæ—¨åœ¨ä¸º Large language models (LLMs) æä¾›æ˜¾å¼çš„ä¸“å®¶çŸ¥è¯†æ”¯æŒã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§çŸ¥è¯†å¢å¼ºçš„åŸºå‡†æ–¹æ³• (knowledge-augmented baseline)ï¼Œé€šè¿‡æ•´åˆäººå·¥è§„åˆ™ä¸ LLMs çš„éšæ€§çŸ¥è¯†ï¼Œä½¿å°å‹æ¨¡å‹èƒ½å¤Ÿå®ç°ä¸æœ€å…ˆè¿›æ¨¡å‹ (state-of-the-art LLMs) ç›¸åª²ç¾çš„æ€§èƒ½ã€‚è¯¥å·¥ä½œä¸ä»…æä¾›äº†é«˜è´¨é‡çš„æ•°æ®æ”¯æŒï¼Œè¿˜é€šè¿‡å¼€æºä»£ç å’Œæ•°æ®ä¸ºä¸­æ–‡å†…å®¹å®‰å…¨é¢†åŸŸçš„ç ”ç©¶ä¸è½åœ°æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2506.10960v3",
      "published_date": "2025-06-12 17:57:05 UTC",
      "updated_date": "2025-08-13 08:43:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:14:26.191383+00:00"
    },
    {
      "arxiv_id": "2506.10959v2",
      "title": "Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods",
      "title_zh": "ç†è§£ç»“æ„åŒ–æµå½¢ä¸Šçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼šè¿æ¥æ³¨æ„åŠ›æœºåˆ¶ä¸æ ¸æ–¹æ³•",
      "authors": [
        "Zhaiming Shen",
        "Alexander Hsu",
        "Rongjie Lai",
        "Wenjing Liao"
      ],
      "abstract": "While in-context learning (ICL) has achieved remarkable success in natural language and vision domains, its theoretical understanding-particularly in the context of structured geometric data-remains unexplored. This paper initiates a theoretical study of ICL for regression of HÃ¶lder functions on manifolds. We establish a novel connection between the attention mechanism and classical kernel methods, demonstrating that transformers effectively perform kernel-based prediction at a new query through its interaction with the prompt. This connection is validated by numerical experiments, revealing that the learned query-prompt scores for HÃ¶lder functions are highly correlated with the Gaussian kernel. Building on this insight, we derive generalization error bounds in terms of the prompt length and the number of training tasks. When a sufficient number of training tasks are observed, transformers give rise to the minimax regression rate of HÃ¶lder functions on manifolds, which scales exponentially with the intrinsic dimension of the manifold, rather than the ambient space dimension. Our result also characterizes how the generalization error scales with the number of training tasks, shedding light on the complexity of transformers as in-context kernel algorithm learners. Our findings provide foundational insights into the role of geometry in ICL and novels tools to study ICL of nonlinear models.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ç»“æ„åŒ–å‡ ä½•æ•°æ®ï¼Œæ¢è®¨äº†ä¸Šä¸‹æ–‡å­¦ä¹  (In-Context Learning) åœ¨æµå½¢ (Manifolds) ä¸Šå›å½’ HÃ¶lder å‡½æ•°çš„ç†è®ºæœºåˆ¶ã€‚è®ºæ–‡å»ºç«‹äº†æ³¨æ„æœºåˆ¶ (Attention mechanism) ä¸ç»å…¸æ ¸æ–¹æ³• (Kernel methods) ä¹‹é—´çš„åˆ›æ–°è”ç³»ï¼Œè¯æ˜äº† Transformer é€šè¿‡æŸ¥è¯¢é¡¹ä¸æç¤ºè¯çš„äº¤äº’ï¼Œå®è´¨ä¸Šæ‰§è¡Œäº†åŸºäºæ ¸çš„é¢„æµ‹ã€‚æ•°å€¼å®éªŒéªŒè¯äº†è¿™ä¸€è§‚ç‚¹ï¼Œæ­ç¤ºäº†å­¦ä¹ åˆ°çš„æŸ¥è¯¢-æç¤ºè¯„åˆ†ä¸é«˜æ–¯æ ¸ (Gaussian kernel) é«˜åº¦ç›¸å…³ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æ¨å¯¼å‡ºäº†åŸºäºæç¤ºé•¿åº¦å’Œè®­ç»ƒä»»åŠ¡æ•°é‡çš„æ³›åŒ–è¯¯å·®ç•Œ (Generalization error bounds)ã€‚å½“è®­ç»ƒä»»åŠ¡å……è¶³æ—¶ï¼ŒTransformer èƒ½è¾¾åˆ°æµå½¢ä¸Š HÃ¶lder å‡½æ•°çš„æå°æå¤§å›å½’é€Ÿç‡ (Minimax regression rate)ã€‚è¯¥é€Ÿç‡éšæµå½¢çš„æœ¬å¾ç»´åº¦ (Intrinsic dimension) è€Œéç¯å¢ƒç©ºé—´ç»´åº¦ (Ambient space dimension) æŒ‡æ•°çº§ç¼©æ”¾ã€‚è¿™ä¸€æˆæœä¸ºç†è§£å‡ ä½•åœ¨ ICL ä¸­çš„ä½œç”¨æä¾›äº†åŸºç¡€è§è§£ï¼Œå¹¶ä¸ºç ”ç©¶éçº¿æ€§æ¨¡å‹çš„ ICL æä¾›äº†æ–°å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10959v2",
      "published_date": "2025-06-12 17:56:26 UTC",
      "updated_date": "2025-10-21 17:24:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:14:42.871402+00:00"
    },
    {
      "arxiv_id": "2506.10955v1",
      "title": "ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems",
      "title_zh": "ReGuidanceï¼šä¸€ç§æå‡å›°éš¾åé—®é¢˜æ ·æœ¬è´¨é‡çš„ç®€å•æ‰©æ•£å°è£…æ–¹æ³•",
      "authors": [
        "Aayush Karan",
        "Kulin Shah",
        "Sitan Chen"
      ],
      "abstract": "There has been a flurry of activity around using pretrained diffusion models as informed data priors for solving inverse problems, and more generally around steering these models using reward models. Training-free methods like diffusion posterior sampling (DPS) and its many variants have offered flexible heuristic algorithms for these tasks, but when the reward is not informative enough, e.g., in hard inverse problems with low signal-to-noise ratio, these techniques veer off the data manifold, failing to produce realistic outputs. In this work, we devise a simple wrapper, ReGuidance, for boosting both the sample realism and reward achieved by these methods. Given a candidate solution $\\hat{x}$ produced by an algorithm of the user's choice, we propose inverting the solution by running the unconditional probability flow ODE in reverse starting from $\\hat{x}$, and then using the resulting latent as an initialization for DPS. We evaluate our wrapper on hard inverse problems like large box in-painting and super-resolution with high upscaling. Whereas state-of-the-art baselines visibly fail, we find that applying our wrapper on top of these baselines significantly boosts sample quality and measurement consistency. We complement these findings with theory proving that on certain multimodal data distributions, ReGuidance simultaneously boosts the reward and brings the candidate solution closer to the data manifold. To our knowledge, this constitutes the first rigorous algorithmic guarantee for DPS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ReGuidanceï¼Œä¸€ç§æ—¨åœ¨æå‡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹(Diffusion Models)åœ¨å¤„ç†å›°éš¾é€†é—®é¢˜(Hard Inverse Problems)æ—¶é‡‡æ ·è´¨é‡çš„ç®€å•å°è£…æ–¹æ³•ã€‚ä¼ ç»Ÿçš„æ— é¡»è®­ç»ƒæ–¹æ³•å¦‚æ‰©æ•£åéªŒé‡‡æ ·(Diffusion Posterior Sampling, DPS)åœ¨ä½ä¿¡å™ªæ¯”ç­‰æç«¯åœºæ™¯ä¸‹ï¼Œå®¹æ˜“åç¦»æ•°æ®æµå½¢å¯¼è‡´ç”Ÿæˆç»“æœä¸çœŸå®ã€‚ReGuidanceçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åè½¬æ— æ¡ä»¶æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹(Probability Flow ODE)å°†åˆæ­¥è§£é€†æ¨å›æ½œåœ¨ç©ºé—´(Latent Space)ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºDPSçš„åˆå§‹åŒ–è¿›è¡Œä¼˜åŒ–ã€‚åœ¨å¤§å‹æ–¹æ¡†è¡¥å…¨(Large Box In-painting)å’Œé«˜å€ç‡è¶…åˆ†è¾¨ç‡(Super-resolution)ä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†åŸºå‡†æ¨¡å‹çš„ç”Ÿæˆè´¨é‡å’Œæµ‹é‡ä¸€è‡´æ€§(Measurement Consistency)ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æä¾›äº†é¦–ä¸ªé’ˆå¯¹DPSçš„ä¸¥è°¨ç®—æ³•ç†è®ºä¿è¯ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤šå³°æ•°æ®åˆ†å¸ƒæ—¶èƒ½æœ‰æ•ˆä½¿å€™é€‰è§£å‘æ•°æ®æµå½¢é æ‹¢å¹¶åŒæ—¶ä¼˜åŒ–å¥–åŠ±ç›®æ ‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "38 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.10955v1",
      "published_date": "2025-06-12 17:55:17 UTC",
      "updated_date": "2025-06-12 17:55:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:14:34.872236+00:00"
    },
    {
      "arxiv_id": "2506.10954v3",
      "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks",
      "title_zh": "SWE-Factoryï¼šé¢å‘é—®é¢˜ä¿®å¤è®­ç»ƒæ•°æ®ä¸è¯„ä¼°åŸºå‡†çš„è‡ªåŠ¨åŒ–æ„å»ºå·¥å‚",
      "authors": [
        "Lianghong Guo",
        "Yanlin Wang",
        "Caihua Li",
        "Wei Tao",
        "Pengyu Yang",
        "Jiachi Chen",
        "Haoyu Song",
        "Duyu Tang",
        "Zibin Zheng"
      ],
      "abstract": "Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the existing GitHub issue resolution data construction pipeline is challenging and labor-intensive. We identify three key limitations in existing pipelines: (1) test patches collected often omit binary file changes; (2) the manual construction of evaluation environments is labor-intensive; and (3) the fail2pass validation phase requires manual inspection of test logs and writing custom parsing code to extract test status from logs. In this paper, we propose SWE-Factory, a fully automated issue resolution data construction pipeline, to resolve these limitations. First, our pipeline automatically recovers missing binary test files and ensures the correctness of test patches. Second, we introduce SWE-Builder, a LLM-based multi-agent system that automates evaluation environment construction. Third, we introduce a standardized, exit-code-based log parsing method to automatically extract test status, enabling a fully automated fail2pass validation. Experiments on 671 real-world GitHub issues across four programming languages show that our method can effectively construct valid evaluation environments for GitHub issues at a reasonable cost. For example, with GPT-4.1 mini, our SWE-Builder constructs 337 valid task instances out of 671 issues, at $0.047 per instance. Our ablation study further shows the effectiveness of different components of SWE-Builder. We also demonstrate through manual inspection that our exit-code-based fail2pass validation method is highly accurate, achieving an F1 score of 0.99. Additionally, we conduct an exploratory experiment to investigate whether we can use SWE-Factory to enhance models' software engineering ability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SWE-Factoryï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨åŒ–çš„ GitHub é—®é¢˜ä¿®å¤ (Issue Resolution) æ•°æ®æ„å»ºæµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³å½“å‰æ„å»ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) è®­ç»ƒå’Œè¯„ä¼°åŸºå‡†æ—¶é¢ä¸´çš„åŠ³åŠ¨å¯†é›†å‹æŒ‘æˆ˜ã€‚è¯¥æµæ°´çº¿é€šè¿‡è‡ªåŠ¨æ¢å¤ç¼ºå¤±çš„äºŒè¿›åˆ¶æµ‹è¯•æ–‡ä»¶ï¼Œç¡®ä¿äº†æµ‹è¯•è¡¥ä¸ (Test Patches) çš„æ­£ç¡®æ€§ã€‚é’ˆå¯¹è¯„ä¼°ç¯å¢ƒæ„å»ºéš¾é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäº LLM çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ SWE-Builderï¼Œå®ç°äº†ç¯å¢ƒæ„å»ºçš„è‡ªåŠ¨åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨æ ‡å‡†åŒ–çš„ã€åŸºäºé€€å‡ºç  (Exit-code) çš„æ—¥å¿—è§£ææ–¹æ³•ï¼Œå®ç°äº†å…¨è‡ªåŠ¨çš„ Fail2pass éªŒè¯ã€‚åœ¨æ¶µç›–å››ç§ç¼–ç¨‹è¯­è¨€çš„ 671 ä¸ªçœŸå® GitHub é—®é¢˜ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSWE-Builder èƒ½ä»¥æä½æˆæœ¬æœ‰æ•ˆæ„å»ºéªŒè¯ç¯å¢ƒã€‚äººå·¥è¯„ä¼°æ˜¾ç¤ºå…¶éªŒè¯æ–¹æ³•è¾¾åˆ°äº† 0.99 çš„ F1 scoreï¼Œè¯æ˜äº†è¯¥è‡ªåŠ¨åŒ–æ–¹æ¡ˆåœ¨æ„å»ºå¤§è§„æ¨¡ã€é«˜è´¨é‡è½¯ä»¶å·¥ç¨‹åŸºå‡†æ•°æ®æ–¹é¢çš„é«˜æ•ˆæ€§ä¸å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "To appear at FSE'2026",
      "pdf_url": "https://arxiv.org/pdf/2506.10954v3",
      "published_date": "2025-06-12 17:54:17 UTC",
      "updated_date": "2026-01-05 08:48:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:14:34.579919+00:00"
    },
    {
      "arxiv_id": "2506.10952v1",
      "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training",
      "title_zh": "Domain2Vecï¼šæ— éœ€è®­ç»ƒå³å¯ç¡®å®šæœ€ä¼˜æ•°æ®é…æ¯”çš„æ•°æ®é›†å‘é‡åŒ–æ–¹æ³•",
      "authors": [
        "Mozhi Zhang",
        "Howe Tissue",
        "Lu Wang",
        "Xipeng Qiu"
      ],
      "abstract": "We introduce~\\textsc{Domain2Vec}, a novel approach that decomposes any dataset into a linear combination of several \\emph{meta-domains}, a new concept designed to capture the key underlying features of datasets. \\textsc{Domain2Vec} maintains a vocabulary of meta-domains and uses a classifier to decompose any given dataset into a domain vector that corresponds to a distribution over this vocabulary. These domain vectors enable the identification of the optimal data mixture for language model (LM) pretraining in a training-free manner under the \\emph{\\textbf{D}istribution \\textbf{A}lignment \\textbf{A}ssumption} (DA$^{2}$), which suggests that when the data distributions of the training set and the validation set are better aligned, a lower validation loss is achieved. Moreover, \\textsc{Domain2vec} can be seamlessly integrated into previous works to model the relationship between domain vectors and LM performance, greatly enhancing the efficiency and scalability of previous methods. Extensive experiments demonstrate that \\textsc{Domain2Vec} helps find the data mixture that enhances downstream task performance with minimal computational overhead. Specifically, \\textsc{Domain2Vec} achieves the same validation loss on Pile-CC using only $51.5\\%$ of the computation required when training on the original mixture of The Pile dataset. Under equivalent compute budget, \\textsc{Domain2Vec} improves downstream performance by an average of $2.83\\%$.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Domain2Vecï¼Œè¿™æ˜¯ä¸€ç§å°†ä»»ä½•æ•°æ®é›†åˆ†è§£ä¸ºå¤šä¸ª meta-domainsï¼ˆå…ƒé¢†åŸŸï¼‰çº¿æ€§ç»„åˆçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ•æ‰æ•°æ®é›†çš„å…³é”®åº•å±‚ç‰¹å¾ã€‚Domain2Vec ç»´æŠ¤ä¸€ä¸ªå…ƒé¢†åŸŸè¯æ±‡è¡¨ï¼Œå¹¶åˆ©ç”¨åˆ†ç±»å™¨å°†ç»™å®šæ•°æ®é›†è½¬åŒ–ä¸ºå¯¹åº”åˆ†å¸ƒçš„é¢†åŸŸå‘é‡ã€‚åŸºäº Distribution Alignment Assumption (DAÂ²)ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå®é™…è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¯¹é½è®­ç»ƒé›†ä¸éªŒè¯é›†çš„æ•°æ®åˆ†å¸ƒæ¥ç¡®å®šè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰é¢„è®­ç»ƒçš„æœ€ä½³æ•°æ®é…æ¯”ã€‚Domain2Vec å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰å·¥ä½œä¸­ï¼Œæ˜¾è‘—æå‡äº†æ•°æ®æ··åˆä¼˜åŒ–è¿‡ç¨‹çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ Pile-CC éªŒè¯é›†ä¸Šï¼ŒDomain2Vec ä»…éœ€åŸå§‹æ•°æ®é…æ¯” 51.5% çš„è®¡ç®—é‡å³å¯è¾¾åˆ°ç›¸åŒçš„éªŒè¯æŸå¤±ã€‚åœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹ï¼Œè¯¥æ–¹æ³•å°†ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å¹³å‡æå‡äº† 2.83%ï¼Œä¸ºåœ¨æœ€å°è®¡ç®—å¼€é”€ä¸‹ä¼˜åŒ–æ¨¡å‹é¢„è®­ç»ƒæä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ICML2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10952v1",
      "published_date": "2025-06-12 17:53:51 UTC",
      "updated_date": "2025-06-12 17:53:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:14:40.866066+00:00"
    },
    {
      "arxiv_id": "2506.21569v1",
      "title": "Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA",
      "title_zh": "Hybrid-NL2SVAï¼šé›†æˆ RAG ä¸å¾®è°ƒçš„åŸºäºå¤§è¯­è¨€æ¨¡å‹ NL2SVA",
      "authors": [
        "Weihua Xiao",
        "Derek Ekberg",
        "Siddharth Garg",
        "Ramesh Karri"
      ],
      "abstract": "SystemVerilog Assertions (SVAs) are critical for verifying the correctness of hardware designs, but manually writing them from natural language property descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task. Recent advances in large language models (LLMs) offer opportunities to automate this translation. However, existing models still struggle with understanding domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we propose a customized retrieval-augmented generation (RAG) framework and a synthetic fine-tuning dataset that together improve LLM's performance. To further improve lightweight models over NL2SVA, our fine-tuning dataset provides prompt-guided explanations that teach LLMs the layer-by-layer construction process of concurrent SVAs, enabling supervised fine-tuning that greatly improves syntax and functionality accuracy. To evaluate the performance of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA, comprising 40 Verilog designs and 229 formally verified SVAs with detailed annotations. Experimental results show that our customized RAG framework increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini, while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Hybrid-NL2SVAæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’Œå¾®è°ƒ(Finetuning)æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºSystemVerilog Assertions (SVAs)æ—¶é¢ä¸´çš„é¢†åŸŸè¯­æ³•ä¸è¯­ä¹‰ç†è§£éš¾é¢˜ã€‚ç ”ç©¶è€…å¼€å‘äº†ä¸€å¥—å®šåˆ¶åŒ–çš„RAGæ¡†æ¶ï¼Œå¹¶æ„å»ºäº†åŒ…å«æç¤ºå¯¼å‘è§£é‡Š(prompt-guided explanations)çš„åˆæˆå¾®è°ƒæ•°æ®é›†ï¼Œé€šè¿‡å±•ç¤ºå¹¶å‘SVAçš„é€å±‚æ„å»ºè¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹ç”Ÿæˆçš„è¯­æ³•å’ŒåŠŸèƒ½å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œå»ºç«‹äº†ç›®å‰æœ€å¤§çš„NL2SVAè¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–40ä¸ªVerilogè®¾è®¡åŠ229ä¸ªç»è¿‡å½¢å¼åŒ–éªŒè¯å¹¶å¸¦æœ‰è¯¦ç»†æ ‡æ³¨çš„SVAã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå®šåˆ¶åŒ–RAGæ¡†æ¶ä½¿GPT-4o-miniçš„åŠŸèƒ½åŒ¹é…ç‡æé«˜äº†58.42%ï¼Œè€Œå¾®è°ƒåçš„Qwen2.5-Coder-7B-Instructç»“åˆæ··åˆæ£€ç´¢(HybridRetrieval)æŠ€æœ¯åï¼Œå…¶æ€§èƒ½æ¯”åŸºçº¿æ¨¡å‹æå‡äº†59.05%ï¼Œä¸ºè‡ªåŠ¨åŒ–ç¡¬ä»¶è®¾è®¡éªŒè¯æä¾›äº†æœ‰æ•ˆæ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21569v1",
      "published_date": "2025-06-12 17:52:06 UTC",
      "updated_date": "2025-06-12 17:52:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:14:50.205174+00:00"
    },
    {
      "arxiv_id": "2506.10949v2",
      "title": "Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors",
      "title_zh": "åŸºäºè½»é‡çº§åºåˆ—ç›‘æµ‹å™¨çš„ LLM åˆ†è§£æ”»å‡»ç›‘æµ‹",
      "authors": [
        "Chen Yueh-Han",
        "Nitish Joshi",
        "Yulin Chen",
        "Maksym Andriushchenko",
        "Rico Angell",
        "He He"
      ],
      "abstract": "Current LLM safety defenses fail under decomposition attacks, where a malicious goal is decomposed into benign subtasks that circumvent refusals. The challenge lies in the existing shallow safety alignment techniques: they only detect harm in the immediate prompt and do not reason about long-range intent, leaving them blind to malicious intent that emerges over a sequence of seemingly benign instructions. We therefore propose adding an external monitor that observes the conversation at a higher granularity. To facilitate our study of monitoring decomposition attacks, we curate the largest and most diverse dataset to date, including question-answering, text-to-image, and agentic tasks. We verify our datasets by testing them on frontier LLMs and show an 87% attack success rate on average on GPT-4o. This confirms that decomposition attack is broadly effective. Additionally, we find that random tasks can be injected into the decomposed subtasks to further obfuscate malicious intents. To defend in real time, we propose a lightweight sequential monitoring framework that cumulatively evaluates each subtask. We show that a carefully prompt engineered lightweight monitor achieves a 93% defense success rate, beating reasoning models like o3 mini as a monitor. Moreover, it remains robust against random task injection and cuts cost by 90% and latency by 50%. Our findings suggest that lightweight sequential monitors are highly effective in mitigating decomposition attacks and are viable in deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢å¯¹Decomposition Attacksï¼ˆåˆ†è§£æ”»å‡»ï¼‰æ—¶çš„è„†å¼±æ€§ï¼Œå³æ¶æ„ç›®æ ‡è¢«åˆ†è§£ä¸ºå¤šä¸ªè‰¯æ€§å­ä»»åŠ¡ä»¥ç»•è¿‡å®‰å…¨æœºåˆ¶ã€‚ç°æœ‰çš„å®‰å…¨å¯¹é½æŠ€æœ¯ä¸»è¦ä¾§é‡äºæ£€æµ‹å³æ—¶æç¤ºè¯ï¼Œç¼ºä¹å¯¹Long-range Intentï¼ˆé•¿ç¨‹æ„å›¾ï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œæ— æ³•è¯†åˆ«åœ¨æŒ‡ä»¤åºåˆ—ä¸­é€æ¸æ˜¾ç°çš„æ¶æ„ä¼å›¾ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Lightweight Sequential Monitoringï¼ˆè½»é‡åŒ–é¡ºåºç›‘æ§ï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¤–éƒ¨ç›‘æ§å™¨åœ¨æ›´é«˜ç²’åº¦ä¸Šå¯¹å¯¹è¯è¿›è¡Œç´¯ç§¯è¯„ä¼°ã€‚ä¸ºäº†éªŒè¯è¯¥æ–¹æ¡ˆï¼Œç ”ç©¶è€…æ„å»ºäº†æ¶µç›–é—®ç­”ã€æ–‡æœ¬è½¬å›¾åƒåŠæ™ºèƒ½ä½“ä»»åŠ¡çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶è¯å®Decomposition Attacksåœ¨GPT-4oä¸Šå…·æœ‰87%çš„å¹³å‡æ”»å‡»æˆåŠŸç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥è½»é‡åŒ–ç›‘æ§å™¨è¾¾åˆ°äº†93%çš„é˜²å¾¡æˆåŠŸç‡ï¼Œåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†o3 miniç­‰æ¨ç†æ¨¡å‹ï¼ŒåŒæ—¶é™ä½äº†90%çš„æˆæœ¬å’Œ50%çš„å»¶è¿Ÿã€‚è¿™è¡¨æ˜è½»é‡åŒ–é¡ºåºç›‘æ§æ˜¯ç¼“è§£åˆ†è§£æ”»å‡»çš„ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡éƒ¨ç½²å¯è¡Œæ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10949v2",
      "published_date": "2025-06-12 17:50:58 UTC",
      "updated_date": "2025-06-14 15:17:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:14:57.108737+00:00"
    },
    {
      "arxiv_id": "2506.14824v1",
      "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models",
      "title_zh": "FedNanoï¼šé¢å‘é¢„è®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è½»é‡çº§è”é‚¦å¾®è°ƒ",
      "authors": [
        "Yao Zhang",
        "Hewei Gao",
        "Haokun Chen",
        "Weiguo Li",
        "Yunpu Ma",
        "Volker Tresp"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers a solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose FedNano, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to deploy LLM on clients, reducing client-side storage by 95%, and limiting communication overhead to only 0.01% of the model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FedNanoï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹(LLM)é›†ä¸­åœ¨æœåŠ¡å™¨ç«¯ï¼ŒåŒæ—¶ä¸ºå®¢æˆ·ç«¯å¼•å…¥è½»é‡åŒ–NanoEdgeæ¨¡å—çš„è”é‚¦å­¦ä¹ (FL)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨è”é‚¦å­¦ä¹ ä¸­é¢ä¸´çš„è®¡ç®—éœ€æ±‚é«˜ã€é€šä¿¡å¼€é”€å¤§å’Œå®¢æˆ·ç«¯èµ„æºå—é™ç­‰æŒ‘æˆ˜ã€‚NanoEdgeæ¨¡å—é›†æˆäº†æ¨¡æ€ç‰¹å®šç¼–ç å™¨(encoders)ã€è¿æ¥å™¨(connectors)ä»¥åŠåŸºäºä½ç§©è‡ªé€‚åº”(low-rank adaptation)çš„å¯è®­ç»ƒNanoAdaptersï¼Œä½¿å¾—å®¢æˆ·ç«¯æ— éœ€éƒ¨ç½²å®Œæ•´çš„LLMã€‚è¿™ç§è®¾è®¡å°†å®¢æˆ·ç«¯å­˜å‚¨éœ€æ±‚æ˜¾è‘—é™ä½äº†95%ï¼Œå¹¶å°†é€šä¿¡å¼€é”€é™åˆ¶åœ¨æ¨¡å‹æ€»å‚æ•°çš„0.01%ï¼ŒåŒæ—¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¼‚æ„å®¢æˆ·ç«¯æ•°æ®å¹¶ä¿æŠ¤éšç§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedNanoåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„è”é‚¦å­¦ä¹ åŸºå‡†ï¼Œå¼¥è¡¥äº†å¤§è§„æ¨¡æ¨¡å‹ä¸è”é‚¦å­¦ä¹ å¯è¡Œæ€§ä¹‹é—´çš„å·®è·ï¼Œä¸ºå®ç°å¯æ‰©å±•ã€å»ä¸­å¿ƒåŒ–çš„å¤šæ¨¡æ€AIç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.14824v1",
      "published_date": "2025-06-12 17:50:50 UTC",
      "updated_date": "2025-06-12 17:50:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:14:51.386323+00:00"
    },
    {
      "arxiv_id": "2506.10947v1",
      "title": "Spurious Rewards: Rethinking Training Signals in RLVR",
      "title_zh": "è™šå‡å¥–åŠ±ï¼šé‡æ–°å®¡è§† RLVR ä¸­çš„è®­ç»ƒä¿¡å·",
      "authors": [
        "Rulin Shao",
        "Shuyue Stella Li",
        "Rui Xin",
        "Scott Geng",
        "Yiping Wang",
        "Sewoong Oh",
        "Simon Shaolei Du",
        "Nathan Lambert",
        "Sewon Min",
        "Ranjay Krishna",
        "Yulia Tsvetkov",
        "Hannaneh Hajishirzi",
        "Pang Wei Koh",
        "Luke Zettlemoyer"
      ],
      "abstract": "We show that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain models even with spurious rewards that have little, no, or even negative correlation with the correct answer. For example, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute points by 21.4% (random reward), 13.8% (format reward), 24.1% (incorrect label), 26.0% (1-shot RL), and 27.1% (majority voting) -- nearly matching the 29.1% gained with ground truth rewards. However, the spurious rewards that work for Qwen often fail to yield gains with other model families like Llama3 or OLMo2. In particular, we find code reasoning -- thinking in code without actual code execution -- to be a distinctive Qwen2.5-Math behavior that becomes significantly more frequent after RLVR, from 65% to over 90%, even with spurious rewards. Overall, we hypothesize that, given the lack of useful reward signal, RLVR must somehow be surfacing useful reasoning representations learned during pretraining, although the exact mechanism remains a topic for future work. We suggest that future RLVR research should possibly be validated on diverse models rather than a single de facto choice, as we show that it is easy to get significant performance gains on Qwen models even with completely spurious reward signals.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)ä¸­çš„è®­ç»ƒä¿¡å·ï¼Œå‘ç°å³ä¾¿åœ¨å¥–åŠ±ä¿¡å·ä¸æ­£ç¡®ç­”æ¡ˆç›¸å…³æ€§æä½ç”šè‡³ä¸ºè´Ÿçš„ä¼ªå¥–åŠ±(Spurious Rewards)æƒ…å†µä¸‹ï¼ŒRLVR ä»èƒ½æ˜¾è‘—æå‡ç‰¹å®šæ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒæ˜¾ç¤ºï¼ŒQwen2.5-Math-7B åœ¨ä½¿ç”¨éšæœºå¥–åŠ±æˆ–é”™è¯¯æ ‡ç­¾è¿›è¡Œè®­ç»ƒæ—¶ï¼Œåœ¨ MATH-500 ä¸Šçš„æ€§èƒ½æå‡å‡ ä¹è¾¾åˆ°äº†ä½¿ç”¨åœ°é¢çœŸå€¼(Ground Truth)å¥–åŠ±çš„æ•ˆæœï¼Œè€Œè¿™ç§ç°è±¡åœ¨ Llama3 æˆ– OLMo2 ç­‰æ¨¡å‹ä¸­å¹¶æœªå‡ºç°ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä»£ç æ¨ç†(Code Reasoning)æ˜¯ Qwen ç³»åˆ—åœ¨ RLVR åå‡ºç°çš„æ˜¾è‘—è¡Œä¸ºç‰¹å¾ï¼Œå…¶é¢‘ç‡ä» 65% æå‡è‡³ 90% ä»¥ä¸Šã€‚ä½œè€…æ¨æµ‹ RLVR åœ¨ç¼ºä¹æœ‰æ•ˆå¥–åŠ±æ—¶å¯èƒ½æ¿€æ´»äº†æ¨¡å‹é¢„è®­ç»ƒä¸­ä¹ å¾—çš„æ¨ç†è¡¨å¾ï¼Œå¹¶å¼ºè°ƒæœªæ¥çš„ RLVR ç ”ç©¶åº”é¿å…ä»…åœ¨å•ä¸€æ¨¡å‹ä¸Šè¿›è¡ŒéªŒè¯ï¼Œä»¥é˜²ç”±äºä¼ªå¥–åŠ±ä¿¡å·å¯¼è‡´è¯„ä¼°åå·®ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10947v1",
      "published_date": "2025-06-12 17:49:55 UTC",
      "updated_date": "2025-06-12 17:49:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:14:53.514152+00:00"
    },
    {
      "arxiv_id": "2506.10946v2",
      "title": "GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models",
      "title_zh": "GUARDï¼šåŸºäºæ•°æ®å½’å› çš„å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼å¼é—å¿˜ä¸ä¿ç•™",
      "authors": [
        "Peizhi Niu",
        "Evelyn Ma",
        "Huiting Zhou",
        "Duo Zhou",
        "Huan Zhang",
        "S. Rasoul Etesami",
        "Olgica Milenkovic"
      ],
      "abstract": "Unlearning in large language models is becoming increasingly important due to regulatory compliance, copyright protection, and privacy concerns. However, a key challenge in LLM unlearning is unintended forgetting, where the removal of specific data inadvertently impairs the utility of the model and its retention of valuable, desired information. While prior work has primarily focused on architectural innovations, the influence of data-level factors on unlearning performance remains underexplored. As a result, existing methods often suffer from degraded retention when forgetting high-impact data. To address this problem, we propose GUARD, a novel framework for Guided Unlearning And Retention via Data attribution. At its core, GUARD introduces a lightweight proxy data attribution metric tailored for LLM unlearning, which quantifies the alignment between the Forget and Retain sets while remaining computationally efficient. Building on this, we design a novel unlearning objective that assigns adaptive, nonuniform unlearning weights to samples, inversely proportional to their proxy attribution scores. Through such a reallocation of unlearning power, GUARD mitigates unintended retention loss. We also provide rigorous theoretical guarantees that GUARD significantly improves retention while maintaining forgetting metrics comparable to prior methods. Extensive experiments on the TOFU and MUSE benchmarks across multiple LLM architectures demonstrate that GUARD reduces utility sacrifice on the TOFU Retain Set by up to 194.92 percent in terms of Truth Ratio when forgetting 10 percent of the training data, and improves knowledge retention on the MUSE NEWS Retain Set by 16.20 percent, with comparable or very moderate increases in privacy loss compared to state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GUARDæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æœºå™¨å¸è½½(Unlearning)è¿‡ç¨‹ä¸­å› åˆ é™¤ç‰¹å®šæ•°æ®è€Œå¯¼è‡´çš„æ„å¤–é—å¿˜(Unintended forgetting)å’Œæ¨¡å‹æ•ˆç”¨å—æŸé—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„ä»£ç†æ•°æ®å½’å› (Data attribution)æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–é—å¿˜é›†(Forget set)ä¸ä¿ç•™é›†(Retain set)ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚åŸºäºè¯¥æŒ‡æ ‡ï¼ŒGUARDè®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”çš„éå‡åŒ€å¸è½½æƒé‡åˆ†é…æœºåˆ¶ï¼Œé€šè¿‡å¯¹æ ·æœ¬èµ‹äºˆä¸å…¶å½’å› åˆ†æ•°æˆåæ¯”çš„æƒé‡ï¼Œæœ‰æ•ˆå‡è½»äº†éé¢„æœŸçš„ä¿ç•™æŸå¤±ã€‚ç†è®ºè¯æ˜ä¸å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGUARDåœ¨TOFUå’ŒMUSEåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨é—å¿˜10%è®­ç»ƒæ•°æ®æ—¶èƒ½å°†ä¿ç•™é›†çš„æ•ˆç”¨æŸå¤±é™ä½é«˜è¾¾194.92%ï¼ŒåŒæ—¶ä½¿çŸ¥è¯†ä¿ç•™ç‡æå‡16.20%ã€‚ç›¸æ¯”äºç°æœ‰çš„State-of-the-artæ–¹æ³•ï¼Œè¯¥æ¡†æ¶åœ¨ç»´æŒé«˜æ•ˆé—å¿˜çš„åŒæ—¶ï¼Œå®ç°äº†æ›´ä¼˜çš„æ¨¡å‹æ•ˆç”¨ä¿ç•™ä¸éšç§ä¿æŠ¤å¹³è¡¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10946v2",
      "published_date": "2025-06-12 17:49:09 UTC",
      "updated_date": "2025-10-22 02:25:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:15:03.378891+00:00"
    },
    {
      "arxiv_id": "2506.10941v1",
      "title": "VINCIE: Unlocking In-context Image Editing from Video",
      "title_zh": "VINCIEï¼šå¼€å¯åŸºäºè§†é¢‘çš„ä¸Šä¸‹æ–‡å›¾åƒç¼–è¾‘",
      "authors": [
        "Leigang Qu",
        "Feng Cheng",
        "Ziyan Yang",
        "Qi Zhao",
        "Shanchuan Lin",
        "Yichun Shi",
        "Yicong Li",
        "Wenjie Wang",
        "Tat-Seng Chua",
        "Lu Jiang"
      ],
      "abstract": "In-context image editing aims to modify images based on a contextual sequence comprising text and previously generated images. Existing methods typically depend on task-specific pipelines and expert models (e.g., segmentation and inpainting) to curate training data. In this work, we explore whether an in-context image editing model can be learned directly from videos. We introduce a scalable approach to annotate videos as interleaved multimodal sequences. To effectively learn from this data, we design a block-causal diffusion transformer trained on three proxy tasks: next-image prediction, current segmentation prediction, and next-segmentation prediction. Additionally, we propose a novel multi-turn image editing benchmark to advance research in this area. Extensive experiments demonstrate that our model exhibits strong in-context image editing capabilities and achieves state-of-the-art results on two multi-turn image editing benchmarks. Despite being trained exclusively on videos, our model also shows promising abilities in multi-concept composition, story generation, and chain-of-editing applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VINCIEï¼Œè¿™æ˜¯ä¸€ç§ç›´æ¥ä»è§†é¢‘ä¸­å­¦ä¹ ä¸Šä¸‹æ–‡å›¾åƒç¼–è¾‘ (In-context Image Editing) èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•å¯¹ç‰¹å®šä»»åŠ¡æµæ°´çº¿å’Œä¸“å®¶æ¨¡å‹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡å°†è§†é¢‘å¤„ç†ä¸ºäº¤é”™çš„å¤šæ¨¡æ€åºåˆ—ï¼Œè¯¥ç ”ç©¶è®¾è®¡å¹¶è®­ç»ƒäº†ä¸€ä¸ªåŸºäº Block-causal Diffusion Transformer çš„æ¨¡å‹ï¼Œæ¶µç›–äº†ä¸‹ä¸€å¸§å›¾åƒé¢„æµ‹ (next-image prediction)ã€å½“å‰åˆ†å‰²é¢„æµ‹ (current segmentation prediction) å’Œä¸‹ä¸€å¸§åˆ†å‰²é¢„æµ‹ (next-segmentation prediction) ç­‰ä»£ç†ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼ŒVINCIE åœ¨å¤šä¸ªå¤šè½®å›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œå±•ç°äº†å‡ºè‰²çš„ä¸Šä¸‹æ–‡ç†è§£ä¸ç¼–è¾‘èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è´¡çŒ®äº†ä¸€ä¸ªå…¨æ–°çš„å¤šè½®å›¾åƒç¼–è¾‘åŸºå‡†ï¼Œå¹¶æ­ç¤ºäº†è¯¥æ¨¡å‹åœ¨å¤šæ¦‚å¿µç»„åˆ (multi-concept composition)ã€æ•…äº‹ç”Ÿæˆ (story generation) åŠé“¾å¼ç¼–è¾‘ (chain-of-editing) åº”ç”¨ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://vincie2025.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.10941v1",
      "published_date": "2025-06-12 17:46:54 UTC",
      "updated_date": "2025-06-12 17:46:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:15:20.923848+00:00"
    },
    {
      "arxiv_id": "2506.17275v2",
      "title": "Conformal Safety Shielding for Imperfect-Perception Agents",
      "title_zh": "é’ˆå¯¹æ„ŸçŸ¥ä¸å®Œç¾æ™ºèƒ½ä½“çš„ç¬¦åˆæ€§å®‰å…¨å±è”½",
      "authors": [
        "William Scarbro",
        "Calum Imrie",
        "Sinem Getir Yaman",
        "Kavan Fatehi",
        "Corina S. Pasareanu",
        "Radu Calinescu",
        "Ravi Mangal"
      ],
      "abstract": "We consider the problem of safe control in discrete autonomous agents that use learned components for imperfect perception (or more generally, state estimation) from high-dimensional observations. We propose a shield construction that provides run-time safety guarantees under perception errors by restricting the actions available to an agent, modeled as a Markov decision process, as a function of the state estimates. Our construction uses conformal prediction for the perception component, which guarantees that for each observation, the predicted set of estimates includes the actual state with a user-specified probability. The shield allows an action only if it is allowed for all the estimates in the predicted set, resulting in local safety. We also articulate and prove a global safety property of existing shield constructions for perfect-perception agents bounding the probability of reaching unsafe states if the agent always chooses actions prescribed by the shield. We illustrate our approach with a case-study of an experimental autonomous system that guides airplanes on taxiways using high-dimensional perception DNNs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½¿ç”¨ä¸å®Œç¾æ„ŸçŸ¥(imperfect perception)ç»„ä»¶çš„ç¦»æ•£è‡ªä¸»æ™ºèƒ½ä½“ï¼Œæå‡ºäº†ä¸€ç§ä¸€è‡´æ€§å®‰å…¨å±è”½(conformal safety shielding)æœºåˆ¶ï¼Œæ—¨åœ¨ä¿éšœå—é™åŠ¨ä½œä¸‹çš„è¿è¡Œå®‰å…¨æ€§ã€‚è¯¥æ–¹æ³•å°†æ™ºèƒ½ä½“å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov decision process)ï¼Œå¹¶åˆ©ç”¨ä¸€è‡´æ€§é¢„æµ‹(conformal prediction)æŠ€æœ¯ï¼Œç¡®ä¿æ„ŸçŸ¥ç»„ä»¶ç”Ÿæˆçš„é¢„æµ‹é›†åˆä»¥ç”¨æˆ·æŒ‡å®šçš„æ¦‚ç‡åŒ…å«çœŸå®çŠ¶æ€ã€‚å®‰å…¨å±è”½æœºåˆ¶ä»…åœ¨é¢„æµ‹é›†åˆä¸­çš„æ‰€æœ‰ä¼°è®¡çŠ¶æ€ä¸‹å‡å…è®¸æŸåŠ¨ä½œæ—¶ï¼Œæ‰é‡Šæ”¾è¯¥åŠ¨ä½œæƒé™ï¼Œä»è€Œç¡®ä¿å±€éƒ¨å®‰å…¨ã€‚ç ”ç©¶è¿˜ä»ç†è®ºä¸Šè¯æ˜äº†è¯¥å±è”½æ„é€ åœ¨é™åˆ¶è¿›å…¥ä¸å®‰å…¨çŠ¶æ€æ¦‚ç‡æ–¹é¢çš„å…¨å±€å®‰å…¨æ€§å±æ€§ã€‚æœ€åï¼Œé€šè¿‡ä¸€ä¸ªåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)è¿›è¡Œé«˜ç»´æ„ŸçŸ¥çš„é£æœºæ»‘è¡Œé“è‡ªä¸»å¼•å¯¼ç³»ç»Ÿæ¡ˆä¾‹ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤„ç†æ„ŸçŸ¥è¯¯å·®æ—¶çš„æœ‰æ•ˆæ€§ä¸å¯é æ€§ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SY",
      "comment": "32 pages; Equal contribution by W. Scarbro and C. Imrie; Accepted at 25th International Conference on Runtime Verification, 2025 (RV25)",
      "pdf_url": "https://arxiv.org/pdf/2506.17275v2",
      "published_date": "2025-06-12 17:37:29 UTC",
      "updated_date": "2025-07-26 17:30:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:15:28.626151+00:00"
    },
    {
      "arxiv_id": "2506.10927v1",
      "title": "The Role of Generative AI in Facilitating Social Interactions: A Scoping Review",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨ä¿ƒè¿›ç¤¾äº¤äº’åŠ¨ä¸­çš„ä½œç”¨ï¼šèŒƒå›´ç»¼è¿°",
      "authors": [
        "T. T. J. E. Arets",
        "G. Perugia",
        "M. Houben",
        "W. A. IJsselsteijn"
      ],
      "abstract": "Reduced social connectedness increasingly poses a threat to mental health, life expectancy, and general well-being. Generative AI (GAI) technologies, such as large language models (LLMs) and image generation tools, are increasingly integrated into applications aimed at enhancing human social experiences. Despite their growing presence, little is known about how these technologies influence social interactions. This scoping review investigates how GAI-based applications are currently designed to facilitate social interaction, what forms of social engagement they target, and which design and evaluation methodologies designers use to create and evaluate them. Through an analysis of 30 studies published since 2020, we identify key trends in application domains including storytelling, socio-emotional skills training, reminiscence, collaborative learning, music making, and general conversation. We highlight the role of participatory and co-design approaches in fostering both effective technology use and social engagement, while also examining socio-ethical concerns such as cultural bias and accessibility. This review underscores the potential of GAI to support dynamic and personalized interactions, but calls for greater attention to equitable design practices and inclusive evaluation strategies.",
      "tldr_zh": "æœ¬é¡¹èŒƒå›´ç»¼è¿° (Scoping Review) è°ƒæŸ¥äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) æŠ€æœ¯åœ¨ä¿ƒè¿›äººç±»ç¤¾äº¤äº’åŠ¨æ–¹é¢çš„ä½œç”¨ï¼Œæ—¨åœ¨åº”å¯¹ç¤¾äº¤è”ç³»å‡å°‘å¸¦æ¥çš„å¿ƒç†å¥åº·å¨èƒã€‚ç ”ç©¶é€šè¿‡å¯¹2020å¹´ä»¥æ¥å‘è¡¨çš„30é¡¹ç ”ç©¶è¿›è¡Œåˆ†æï¼Œç³»ç»Ÿæ€§åœ°è¯†åˆ«äº†åŒ…æ‹¬æ•…äº‹è®²è¿° (storytelling)ã€ç¤¾äº¤æƒ…æ„ŸæŠ€èƒ½è®­ç»ƒ (socio-emotional skills training)ã€ç¼…æ€€ (reminiscence) å’Œåä½œå­¦ä¹  (collaborative learning) åœ¨å†…çš„å…³é”®åº”ç”¨é¢†åŸŸã€‚åˆ†æè¡¨æ˜ï¼Œé‡‡ç”¨å‚ä¸å¼è®¾è®¡ (participatory design) å’Œå…±åŒè®¾è®¡ (co-design) æ–¹æ³•å¯¹äºæå‡æŠ€æœ¯æœ‰æ•ˆæ€§å’Œç¤¾äº¤å‚ä¸åº¦è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜æ·±å…¥å®¡è§†äº†æ–‡åŒ–åè§ (cultural bias) å’Œæ— éšœç¢æ€§ (accessibility) ç­‰ç¤¾ä¼šä¼¦ç†æŒ‘æˆ˜ã€‚ç ”ç©¶æœ€åå¼ºè°ƒï¼Œè™½ç„¶ GAI åœ¨æ”¯æŒåŠ¨æ€å’Œä¸ªæ€§åŒ–äº’åŠ¨æ–¹é¢æ½œåŠ›å·¨å¤§ï¼Œä½†æœªæ¥å¼€å‘å¿…é¡»ä¼˜å…ˆè€ƒè™‘å…¬å¹³çš„è®¾è®¡å®è·µå’ŒåŒ…å®¹æ€§çš„è¯„ä¼°ç­–ç•¥ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Preprint version of a manuscript submitted to ACM Transactions on Computer-Human Interaction (TOCHI), under review. 39 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.10927v1",
      "published_date": "2025-06-12 17:37:19 UTC",
      "updated_date": "2025-06-12 17:37:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:15:29.971781+00:00"
    },
    {
      "arxiv_id": "2506.10925v1",
      "title": "Agentic Semantic Control for Autonomous Wireless Space Networks: Extending Space-O-RAN with MCP-Driven Distributed Intelligence",
      "title_zh": "è‡ªä¸»æ— çº¿ç©ºé—´ç½‘ç»œçš„æ™ºèƒ½ä½“è¯­ä¹‰æ§åˆ¶ï¼šåˆ©ç”¨ MCP é©±åŠ¨çš„åˆ†å¸ƒå¼æ™ºèƒ½æ‰©å±• Space-O-RAN",
      "authors": [
        "Eduardo Baena",
        "Paolo Testolina",
        "Michele Polese",
        "Sergi Aliaga",
        "Andrew Benincasa",
        "Dimitrios Koutsonikolas",
        "Josep Jornet",
        "Tommaso Melodia"
      ],
      "abstract": "Lunar surface operations impose stringent requirements on wireless communication systems, including autonomy, robustness to disruption, and the ability to adapt to environmental and mission-driven context. While Space-O-RAN provides a distributed orchestration model aligned with 3GPP standards, its decision logic is limited to static policies and lacks semantic integration. We propose a novel extension incorporating a semantic agentic layer enabled by the Model Context Protocol (MCP) and Agent-to-Agent (A2A) communication protocols, allowing context-aware decision making across real-time, near-real-time, and non-real-time control layers. Distributed cognitive agents deployed in rovers, landers, and lunar base stations implement wireless-aware coordination strategies, including delay-adaptive reasoning and bandwidth-aware semantic compression, while interacting with multiple MCP servers to reason over telemetry, locomotion planning, and mission constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœˆçƒè¡¨é¢ä½œä¸šå¯¹æ— çº¿é€šä¿¡ç³»ç»Ÿåœ¨è‡ªä¸»æ€§ã€é²æ£’æ€§å’Œç¯å¢ƒé€‚åº”æ€§æ–¹é¢çš„ä¸¥è‹›è¦æ±‚ï¼Œæå‡ºäº†å¯¹Space-O-RANçš„åˆ›æ–°æ€§æ‰©å±•ã€‚ä¸ºè§£å†³ç°æœ‰æ¶æ„åœ¨å†³ç­–é€»è¾‘ä¸Šå—é™äºé™æ€ç­–ç•¥ä¸”ç¼ºä¹è¯­ä¹‰é›†æˆçš„é—®é¢˜ï¼Œæ–¹æ¡ˆå¼•å…¥äº†ä¸€ä¸ªç”±Model Context Protocol (MCP) å’Œ Agent-to-Agent (A2A) é€šä¿¡åè®®æ”¯æŒçš„è¯­ä¹‰æ™ºèƒ½ä½“å±‚ã€‚è¯¥æ¡†æ¶å®ç°äº†è·¨è¶Šå®æ—¶ã€è¿‘å®æ—¶å’Œéå®æ—¶æ§åˆ¶å±‚çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å†³ç­–ï¼Œå¹¶åœ¨æ¼«æ¸¸è½¦ã€ç€é™†å™¨å’ŒæœˆçƒåŸºç«™ä¸­éƒ¨ç½²äº†åˆ†å¸ƒå¼è®¤çŸ¥æ™ºèƒ½ä½“ã€‚è¿™äº›æ™ºèƒ½ä½“é€šè¿‡å®æ–½å»¶è¿Ÿè‡ªé€‚åº”æ¨ç† (delay-adaptive reasoning) å’Œå¸¦å®½æ„ŸçŸ¥è¯­ä¹‰å‹ç¼© (bandwidth-aware semantic compression) ç­‰æ— çº¿æ„ŸçŸ¥åè°ƒç­–ç•¥ï¼Œèƒ½å¤Ÿå¯¹é¥æµ‹ã€è¿åŠ¨è§„åˆ’åŠä»»åŠ¡çº¦æŸè¿›è¡Œç»¼åˆæ¨ç†ã€‚è¿™ä¸€æ‰©å±•æ˜¾è‘—æå‡äº†è‡ªä¸»æ— çº¿ç©ºé—´ç½‘ç»œåœ¨å¤æ‚èˆªå¤©ä»»åŠ¡ä¸­çš„æ™ºèƒ½åŒ–æ°´å¹³ä¸æ“ä½œæ•ˆç‡ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.HC",
        "eess.SY"
      ],
      "primary_category": "cs.NI",
      "comment": "Lunar Surface Innovation Consortium 2025 Spring Meeting, May 20-22",
      "pdf_url": "https://arxiv.org/pdf/2506.10925v1",
      "published_date": "2025-06-12 17:35:36 UTC",
      "updated_date": "2025-06-12 17:35:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:15:26.006716+00:00"
    },
    {
      "arxiv_id": "2506.10922v1",
      "title": "Robustly Improving LLM Fairness in Realistic Settings via Interpretability",
      "title_zh": "é€šè¿‡å¯è§£é‡Šæ€§ç¨³å¥æå‡ç°å®åœºæ™¯ä¸‹å¤§è¯­è¨€æ¨¡å‹çš„å…¬å¹³æ€§",
      "authors": [
        "Adam Karvonen",
        "Samuel Marks"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed in high-stakes hiring applications, making decisions that directly impact people's careers and livelihoods. While prior studies suggest simple anti-bias prompts can eliminate demographic biases in controlled evaluations, we find these mitigations fail when realistic contextual details are introduced. We address these failures through internal bias mitigation: by identifying and neutralizing sensitive attribute directions within model activations, we achieve robust bias reduction across all tested scenarios. Across leading commercial (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3, Mistral-24B), we find that adding realistic context such as company names, culture descriptions from public careers pages, and selective hiring constraints (e.g.,``only accept candidates in the top 10\\%\") induces significant racial and gender biases (up to 12\\% differences in interview rates). When these biases emerge, they consistently favor Black over White candidates and female over male candidates across all tested models and scenarios. Moreover, models can infer demographics and become biased from subtle cues like college affiliations, with these biases remaining invisible even when inspecting the model's chain-of-thought reasoning. To address these limitations, our internal bias mitigation identifies race and gender-correlated directions and applies affine concept editing at inference time. Despite using directions from a simple synthetic dataset, the intervention generalizes robustly, consistently reducing bias to very low levels (typically under 1\\%, always below 2.5\\%) while largely maintaining model performance. Our findings suggest that practitioners deploying LLMs for hiring should adopt more realistic evaluation methodologies and consider internal mitigation strategies for equitable outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ‹›è˜åœºæ™¯ä¸­çš„å…¬å¹³æ€§é—®é¢˜ï¼Œå‘ç°ä¼ ç»Ÿçš„ååè§æç¤ºåœ¨å¼•å…¥çœŸå®èƒŒæ™¯ç»†èŠ‚ï¼ˆå¦‚å…¬å¸æ–‡åŒ–ã€é€‰æ‹”çº¦æŸç­‰ï¼‰æ—¶å¾€å¾€å¤±æ•ˆã€‚é€šè¿‡å¯¹ GPT-4o å’Œ Gemma ç­‰é¢†å…ˆæ¨¡å‹çš„æµ‹è¯•ï¼Œä½œè€…å‘ç°è¿™äº›ç°å®ä¸Šä¸‹æ–‡ä¼šè¯±å‘é«˜è¾¾ 12% çš„ç§æ—å’Œæ€§åˆ«åå·®ï¼Œä¸”è¿™äº›åè§å¾€å¾€èƒ½ä»å¤§å­¦æ‰€å±å…³ç³»ç­‰å¾®å¦™çº¿ç´¢ä¸­äº§ç”Ÿï¼Œå¹¶éšåŒ¿äº Chain-of-Thought æ¨ç†ä¹‹å¤–ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¯è§£é‡Šæ€§çš„å†…éƒ¨åè§ç¼“è§£æ–¹æ³•ï¼Œåœ¨æ¨ç†é˜¶æ®µåº”ç”¨ Affine Concept Editing æ¥è¯†åˆ«å¹¶ä¸­å’Œæ¨¡å‹æ¿€æ´»ç©ºé—´ä¸­çš„æ•æ„Ÿå±æ€§æ–¹å‘ã€‚å®éªŒè¯æ˜ï¼Œè¯¥å¹²é¢„æ‰‹æ®µå…·æœ‰æå¼ºçš„æ³›åŒ–æ€§ï¼Œèƒ½åœ¨åŸºæœ¬ä¿æŒæ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œå°†å„ç±»ç°å®åœºæ™¯ä¸­çš„åè§é²æ£’åœ°é™ä½è‡³æä½æ°´å¹³ï¼ˆé€šå¸¸ä½äº 1%ï¼‰ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨å®é™…éƒ¨ç½²ä¸­é‡‡ç”¨æ›´çœŸå®çš„è¯„ä¼°æ–¹æ³•ä»¥åŠå†…éƒ¨ç¼“è§£ç­–ç•¥å¯¹äºå®ç°å…¬å¹³ç»“æœçš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10922v1",
      "published_date": "2025-06-12 17:34:38 UTC",
      "updated_date": "2025-06-12 17:34:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:15:27.651061+00:00"
    },
    {
      "arxiv_id": "2506.10915v1",
      "title": "M4V: Multi-Modal Mamba for Text-to-Video Generation",
      "title_zh": "M4Vï¼šé¢å‘æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„å¤šæ¨¡æ€ Mamba",
      "authors": [
        "Jiancheng Huang",
        "Gengwei Zhang",
        "Zequn Jie",
        "Siyu Jiao",
        "Yinlong Qian",
        "Ling Chen",
        "Yunchao Wei",
        "Lin Ma"
      ],
      "abstract": "Text-to-video generation has significantly enriched content creation and holds the potential to evolve into powerful world simulators. However, modeling the vast spatiotemporal space remains computationally demanding, particularly when employing Transformers, which incur quadratic complexity in sequence processing and thus limit practical applications. Recent advancements in linear-time sequence modeling, particularly the Mamba architecture, offer a more efficient alternative. Nevertheless, its plain design limits its direct applicability to multi-modal and spatiotemporal video generation tasks. To address these challenges, we introduce M4V, a Multi-Modal Mamba framework for text-to-video generation. Specifically, we propose a multi-modal diffusion Mamba (MM-DiM) block that enables seamless integration of multi-modal information and spatiotemporal modeling through a multi-modal token re-composition design. As a result, the Mamba blocks in M4V reduce FLOPs by 45% compared to the attention-based alternative when generating videos at 768$\\times$1280 resolution. Additionally, to mitigate the visual quality degradation in long-context autoregressive generation processes, we introduce a reward learning strategy that further enhances per-frame visual realism. Extensive experiments on text-to-video benchmarks demonstrate M4V's ability to produce high-quality videos while significantly lowering computational costs. Code and models will be publicly available at https://huangjch526.github.io/M4V_project.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†M4Vï¼Œä¸€ç§ç”¨äºText-to-Video Generationçš„Multi-Modal Mambaæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Transformeræ¶æ„åœ¨å¤„ç†å¤§è§„æ¨¡æ—¶ç©ºæ•°æ®æ—¶è®¡ç®—å¼€é”€è¿‡å¤§çš„é—®é¢˜ã€‚ä¸ºäº†å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€èåˆï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†Multi-Modal Diffusion Mamba (MM-DiM) æ¨¡å—ï¼Œåˆ©ç”¨Multi-Modal Token Re-compositionè®¾è®¡å®ç°äº†å¤šæ¨¡æ€ä¿¡æ¯ä¸æ—¶ç©ºå»ºæ¨¡çš„æ·±åº¦é›†æˆã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œåœ¨ç”Ÿæˆ768x1280é«˜åˆ†è¾¨ç‡è§†é¢‘æ—¶ï¼ŒM4Væ¯”ä¼ ç»Ÿçš„Attention-basedæ¨¡å‹å‡å°‘äº†çº¦45%çš„FLOPsï¼Œå¤§å¹…æå‡äº†æ¨ç†æ•ˆç‡ã€‚é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„ç”»è´¨ä¸‹é™ï¼Œç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€ç§Reward Learning Strategyæ¥å¢å¼ºè§†é¢‘åºåˆ—çš„è§†è§‰çœŸå®æ„Ÿã€‚æœ€ç»ˆå®éªŒè¯æ˜ï¼ŒM4Vèƒ½åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œäº§å‡ºé«˜è´¨é‡çš„è§†é¢‘å†…å®¹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10915v1",
      "published_date": "2025-06-12 17:29:40 UTC",
      "updated_date": "2025-06-12 17:29:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:15:37.904604+00:00"
    },
    {
      "arxiv_id": "2506.10912v2",
      "title": "Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?",
      "title_zh": "ç ´è§£æœ‰å®³åˆ†å­ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦å·²å‡†å¤‡å¥½è¿›è¡Œç»“æ„çº§åˆ†å­å»æ¯’ï¼Ÿ",
      "authors": [
        "Fei Lin",
        "Ziyang Gong",
        "Cong Wang",
        "Yonglin Tian",
        "Tengchao Zhang",
        "Xue Yang",
        "Gen Luo",
        "Fei-Yue Wang"
      ],
      "abstract": "Toxicity remains a leading cause of early-stage drug development failure. Despite advances in molecular design and property prediction, the task of molecular toxicity repair - generating structurally valid molecular alternatives with reduced toxicity - has not yet been systematically defined or benchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task for general-purpose Multimodal Large Language Models (MLLMs) focused on molecular toxicity repair. We construct a standardized dataset covering 11 primary tasks and 560 representative toxic molecules spanning diverse mechanisms and granularities. We design a prompt annotation pipeline with mechanism-aware and task-adaptive capabilities, informed by expert toxicological knowledge. In parallel, we propose an automated evaluation framework, ToxiEval, which integrates toxicity endpoint prediction, synthetic accessibility, drug-likeness, and structural similarity into a high-throughput evaluation chain for repair success. We systematically assess nearly 30 mainstream general-purpose MLLMs and design multiple ablation studies to analyze key factors such as evaluation criteria, candidate diversity, and failure attribution. Experimental results show that although current MLLMs still face significant challenges on this task, they begin to demonstrate promising capabilities in toxicity understanding, semantic constraint adherence, and structure-aware molecule editing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯ç‰©å¼€å‘ä¸­åˆ†å­æ¯’æ€§ä¿®å¤(Molecular Toxicity Repair)è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªé¢å‘é€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)çš„åŸºå‡†ä»»åŠ¡ToxiMolã€‚è¯¥åŸºå‡†åŒ…å«æ¶µç›–11é¡¹ä»»åŠ¡å’Œ560ä¸ªæ¯’æ€§åˆ†å­çš„æ ‡å‡†åŒ–æ•°æ®é›†ï¼Œå¹¶ç»“åˆä¸“å®¶æ¯’ç†å­¦çŸ¥è¯†è®¾è®¡äº†æœºåˆ¶æ„ŸçŸ¥ä¸ä»»åŠ¡è‡ªé€‚åº”çš„æç¤ºè¯æ ‡æ³¨æµæ°´çº¿(Prompt Annotation Pipeline)ã€‚åŒæ—¶ï¼Œç ”ç©¶æ„å»ºäº†ToxiEvalè‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡æ¯’æ€§é¢„æµ‹ã€åˆæˆå¯åŠæ€§ã€ç±»è¯æ€§å’Œç»“æ„ç›¸ä¼¼æ€§ç­‰æŒ‡æ ‡ç»¼åˆè¡¡é‡ä¿®å¤æˆåŠŸç‡ã€‚ä½œè€…ç³»ç»Ÿè¯„ä¼°äº†è¿‘30ç§ä¸»æµé€šç”¨MLLMsï¼Œå¹¶æ·±å…¥åˆ†æäº†è¯„ä»·æ ‡å‡†ã€å€™é€‰å¤šæ ·æ€§åŠå¤±è´¥å½’å› ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶MLLMsåœ¨å¤„ç†è¯¥ä»»åŠ¡æ—¶ä»é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œä½†åœ¨æ¯’æ€§ç†è§£ã€è¯­ä¹‰çº¦æŸéµå¾ªå’Œç»“æ„æ„ŸçŸ¥åˆ†å­ç¼–è¾‘(Structure-aware Molecule Editing)æ–¹é¢å·²å±•ç°å‡ºæ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10912v2",
      "published_date": "2025-06-12 17:25:53 UTC",
      "updated_date": "2025-06-18 14:00:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:15:44.509850+00:00"
    },
    {
      "arxiv_id": "2506.12098v1",
      "title": "\"I Hadn't Thought About That\": Creators of Human-like AI Weigh in on Ethics And Neurodivergence",
      "title_zh": "â€œæ­¤å‰æœªæ›¾è®¾æƒ³â€ï¼šç±»äººäººå·¥æ™ºèƒ½å¼€å‘è€…å¯¹ä¼¦ç†ä¸ç¥ç»å¤šæ ·æ€§çš„æ¢è®¨",
      "authors": [
        "Naba Rizvi",
        "Taggert Smith",
        "Tanvi Vidyala",
        "Mya Bolds",
        "Harper Strickland",
        "Andrew Begel",
        "Rua Williams",
        "Imani Munyaka"
      ],
      "abstract": "Human-like AI agents such as robots and chatbots are becoming increasingly popular, but they present a variety of ethical concerns. The first concern is in how we define humanness, and how our definition impacts communities historically dehumanized by scientific research. Autistic people in particular have been dehumanized by being compared to robots, making it even more important to ensure this marginalization is not reproduced by AI that may promote neuronormative social behaviors. Second, the ubiquitous use of these agents raises concerns surrounding model biases and accessibility. In our work, we investigate the experiences of the people who build and design these technologies to gain insights into their understanding and acceptance of neurodivergence, and the challenges in making their work more accessible to users with diverse needs. Even though neurodivergent individuals are often marginalized for their unique communication styles, nearly all participants overlooked the conclusions their end-users and other AI system makers may draw about communication norms from the implementation and interpretation of humanness applied in participants' work. This highlights a major gap in their broader ethical considerations, compounded by some participants' neuronormative assumptions about the behaviors and traits that distinguish \"humans\" from \"bots\" and the replication of these assumptions in their work. We examine the impact this may have on autism inclusion in society and provide recommendations for additional systemic changes towards more ethical research directions.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†ç±»äººAI (Human-like AI) å¼€å‘è€…åœ¨ä¼¦ç†å’Œç¥ç»å¤šæ ·æ€§ (Neurodivergence) æ–¹é¢çš„è®¤çŸ¥ä¸å®è·µï¼Œé‡ç‚¹æ¢è®¨äº†è‡ªé—­ç—‡ (Autistic) ç¾¤ä½“åœ¨AIå®šä¹‰â€œäººæ€§â€è¿‡ç¨‹ä¸­çš„è¾¹ç¼˜åŒ–é£é™©ã€‚é€šè¿‡å¯¹AIæ„å»ºè€…å’Œè®¾è®¡è€…çš„æ·±åº¦è°ƒæŸ¥ï¼Œä½œè€…å‘ç°å¼€å‘è€…æ™®éå¿½è§†äº†å…¶ç³»ç»Ÿåœ¨å®ç°â€œç±»äººæ€§â€æ—¶æ‰€éšå«çš„ç¤¾äº¤æ²Ÿé€šè§„èŒƒå¯¹ç¤¾ä¼šå¯èƒ½äº§ç”Ÿçš„æ·±è¿œå½±å“ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè®¸å¤šå‚ä¸è€…åœ¨åŒºåˆ†â€œäººç±»â€ä¸â€œæœºå™¨äººâ€æ—¶æŒæœ‰ç¥ç»å…¸å‹ (Neuronormative) çš„å‡è®¾ï¼Œå¹¶æ— æ„è¯†åœ°å°†è¿™äº›åè§å¤åˆ¶åˆ°AIæ¨¡å‹ä¸­ã€‚è¿™æ­ç¤ºäº†å½“å‰AIä¼¦ç†è€ƒé‡ä¸­çš„é‡å¤§ç¼ºå£ï¼Œå³å¿½è§†äº†ä¸åŒç¥ç»ç±»å‹ç¾¤ä½“çš„äº¤äº’éœ€æ±‚ä¸ç¤¾ä¼šåŒ…å®¹æ€§ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†è¿™ç§åè§å¯¹è‡ªé—­ç—‡ç¾¤ä½“èå…¥ç¤¾ä¼šçš„æ½œåœ¨å¨èƒï¼Œå¹¶ä¸ºæ¨åŠ¨æ›´å…·åŒ…å®¹æ€§çš„AIç ”å‘æ–¹å‘æå‡ºäº†ç³»ç»Ÿæ€§æ”¹è¿›å»ºè®®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "published at FAccT 2025, 15 pages, 2 tables, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.12098v1",
      "published_date": "2025-06-12 17:16:28 UTC",
      "updated_date": "2025-06-12 17:16:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:15:57.674064+00:00"
    },
    {
      "arxiv_id": "2506.10897v1",
      "title": "GenPlanX. Generation of Plans and Execution",
      "title_zh": "GenPlanXï¼šè®¡åˆ’ç”Ÿæˆä¸æ‰§è¡Œ",
      "authors": [
        "Daniel Borrajo",
        "Giuseppe Canonaco",
        "TomÃ¡s de la Rosa",
        "Alfredo GarrachÃ³n",
        "Sriram Gopalakrishnan",
        "Simerjot Kaur",
        "Marianela Morales",
        "Sunandita Patra",
        "Alberto Pozanco",
        "Keshav Ramani",
        "Charese Smiley",
        "Pietro Totis",
        "Manuela Veloso"
      ],
      "abstract": "Classical AI Planning techniques generate sequences of actions for complex tasks. However, they lack the ability to understand planning tasks when provided using natural language. The advent of Large Language Models (LLMs) has introduced novel capabilities in human-computer interaction. In the context of planning tasks, LLMs have shown to be particularly good in interpreting human intents among other uses. This paper introduces GenPlanX that integrates LLMs for natural language-based description of planning tasks, with a classical AI planning engine, alongside an execution and monitoring framework. We demonstrate the efficacy of GenPlanX in assisting users with office-related tasks, highlighting its potential to streamline workflows and enhance productivity through seamless human-AI collaboration.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»å…¸ AI Planning æŠ€æœ¯åœ¨ç†è§£è‡ªç„¶è¯­è¨€æè¿°æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†é›†æˆ Large Language Models (LLMs) çš„æ–°å‹æ¡†æ¶ GenPlanXã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°† LLMs çš„æ„å›¾è¯†åˆ«èƒ½åŠ›ä¸ç»å…¸ AI planning engine ä»¥åŠæ‰§è¡Œä¸ç›‘æ§æ¡†æ¶ç›¸ç»“åˆï¼Œå®ç°äº†åŸºäºè‡ªç„¶è¯­è¨€çš„ä»»åŠ¡è§„åˆ’ä¸è‡ªåŠ¨åŒ–å¤„ç†ã€‚å®éªŒåœ¨åŠå…¬åœºæ™¯çš„ä»»åŠ¡ä¸­éªŒè¯äº† GenPlanX çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨ç®€åŒ–å·¥ä½œæµå’Œæé«˜ç”Ÿäº§åŠ›æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚GenPlanX é€šè¿‡æ— ç¼çš„ human-AI collaborationï¼Œä¸ºå¤æ‚è§„åˆ’ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€äº¤äº’æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10897v1",
      "published_date": "2025-06-12 17:02:27 UTC",
      "updated_date": "2025-06-12 17:02:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:15:45.916592+00:00"
    },
    {
      "arxiv_id": "2506.10896v1",
      "title": "BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP",
      "title_zh": "BioClinical ModernBERTï¼šé¢å‘ç”Ÿç‰©åŒ»å­¦ä¸ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†çš„å…ˆè¿›é•¿ä¸Šä¸‹æ–‡ç¼–ç å™¨",
      "authors": [
        "Thomas Sounack",
        "Joshua Davis",
        "Brigitte Durieux",
        "Antoine Chaffin",
        "Tom J. Pollard",
        "Eric Lehman",
        "Alistair E. W. Johnson",
        "Matthew McDermott",
        "Tristan Naumann",
        "Charlotta Lindvall"
      ],
      "abstract": "Encoder-based transformer models are central to biomedical and clinical Natural Language Processing (NLP), as their bidirectional self-attention makes them well-suited for efficiently extracting structured information from unstructured text through discriminative tasks. However, encoders have seen slower development compared to decoder models, leading to limited domain adaptation in biomedical and clinical settings. We introduce BioClinical ModernBERT, a domain-adapted encoder that builds on the recent ModernBERT release, incorporating long-context processing and substantial improvements in speed and performance for biomedical and clinical NLP. BioClinical ModernBERT is developed through continued pretraining on the largest biomedical and clinical corpus to date, with over 53.5 billion tokens, and addresses a key limitation of prior clinical encoders by leveraging 20 datasets from diverse institutions, domains, and geographic regions, rather than relying on data from a single source. It outperforms existing biomedical and clinical encoders on four downstream tasks spanning a broad range of use cases. We release both base (150M parameters) and large (396M parameters) versions of BioClinical ModernBERT, along with training checkpoints to support further research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å’Œä¸´åºŠ NLP é¢†åŸŸä¸­ç¼–ç å™¨æ¨¡å‹ï¼ˆEncoder-based transformer modelsï¼‰å‘å±•ç›¸å¯¹æ»åä¸”é¢†åŸŸé€‚åº”æ€§æœ‰é™çš„é—®é¢˜ï¼Œæ¨å‡ºäº† BioClinical ModernBERTã€‚è¯¥æ¨¡å‹åŸºäºæœ€æ–°çš„ ModernBERT è¿›è¡Œé¢†åŸŸè‡ªé€‚åº”ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿç‰©åŒ»å­¦å’Œä¸´åºŠä»»åŠ¡çš„å¤„ç†é€Ÿåº¦ã€æ€§èƒ½åŠé•¿ä¸Šä¸‹æ–‡ï¼ˆLong-contextï¼‰å¤„ç†èƒ½åŠ›ã€‚ç ”å‘å›¢é˜Ÿåœ¨è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„ 535 äº¿ token ç”Ÿç‰©åŒ»å­¦å’Œä¸´åºŠè¯­æ–™åº“ä¸Šè¿›è¡Œäº†æŒç»­é¢„è®­ç»ƒï¼ˆContinued pretrainingï¼‰ï¼Œå¹¶å…‹æœäº†ä»¥å¾€ä¸´åºŠç¼–ç å™¨ä¾èµ–å•ä¸€æ¥æºæ•°æ®çš„å±€é™æ€§ï¼Œæ•´åˆäº†æ¥è‡ªä¸åŒæœºæ„ã€é¢†åŸŸå’Œåœ°ç†åŒºåŸŸçš„ 20 ä¸ªæ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBioClinical ModernBERT åœ¨æ¶µç›–å¹¿æ³›åº”ç”¨åœºæ™¯çš„å››é¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œè¡¨ç°å‡ä¼˜äºç°æœ‰çš„ç”Ÿç‰©åŒ»å­¦å’Œä¸´åºŠç¼–ç å™¨ã€‚ç ”ç©¶å›¢é˜Ÿå…¬å¼€å‘å¸ƒäº† Baseï¼ˆ1.5 äº¿å‚æ•°ï¼‰å’Œ Largeï¼ˆ3.96 äº¿å‚æ•°ï¼‰ä¸¤ä¸ªç‰ˆæœ¬åŠå…¶è®­ç»ƒæ£€æŸ¥ç‚¹ï¼ˆTraining checkpointsï¼‰ï¼Œæ—¨åœ¨ä¸ºè¯¥é¢†åŸŸçš„åç»­ç ”ç©¶æä¾›å¼ºæœ‰åŠ›çš„æ¨¡å‹æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10896v1",
      "published_date": "2025-06-12 17:01:11 UTC",
      "updated_date": "2025-06-12 17:01:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:15:50.171554+00:00"
    },
    {
      "arxiv_id": "2506.10895v1",
      "title": "AIR: Zero-shot Generative Model Adaptation with Iterative Refinement",
      "title_zh": "AIRï¼šåŸºäºè¿­ä»£ç»†åŒ–çš„é›¶æ ·æœ¬ç”Ÿæˆæ¨¡å‹è‡ªé€‚åº”",
      "authors": [
        "Guimeng Liu",
        "Milad Abdollahzadeh",
        "Ngai-Man Cheung"
      ],
      "abstract": "Zero-shot generative model adaptation (ZSGM) aims to adapt a pre-trained generator to a target domain using only text guidance and without any samples from the target domain. Central to recent ZSGM approaches are directional loss which use the text guidance in the form of aligning the image offset with text offset in the embedding space of a vision-language model like CLIP. This is similar to the analogical reasoning in NLP where the offset between one pair of words is used to identify a missing element in another pair by aligning the offset between these two pairs. However, a major limitation of existing ZSGM methods is that the learning objective assumes the complete alignment between image offset and text offset in the CLIP embedding space, resulting in quality degrade in generated images. Our work makes two main contributions. Inspired by the offset misalignment studies in NLP, as our first contribution, we perform an empirical study to analyze the misalignment between text offset and image offset in CLIP embedding space for various large publicly available datasets. Our important finding is that offset misalignment in CLIP embedding space is correlated with concept distance, i.e., close concepts have a less offset misalignment. To address the limitations of the current approaches, as our second contribution, we propose Adaptation with Iterative Refinement (AIR) which is the first ZSGM approach to focus on improving target domain image quality based on our new insight on offset misalignment.Qualitative, quantitative, and user study in 26 experiment setups consistently demonstrate the proposed AIR approach achieves SOTA performance. Additional experiments are in Supp.",
      "tldr_zh": "è¯¥ç ”ç©¶èšç„¦äºé›¶æ ·æœ¬ç”Ÿæˆæ¨¡å‹è‡ªé€‚åº”(ZSGM)é¢†åŸŸï¼Œæ—¨åœ¨ä»…é€šè¿‡æ–‡æœ¬å¼•å¯¼è€Œéç›®æ ‡åŸŸæ ·æœ¬æ¥è¿ç§»é¢„è®­ç»ƒç”Ÿæˆå™¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨ CLIP åµŒå…¥ç©ºé—´ä¸­å› å›¾åƒåç§»ä¸æ–‡æœ¬åç§»ä¸å®Œå…¨å¯¹é½è€Œå¯¼è‡´çš„ç”Ÿæˆå›¾åƒè´¨é‡ä¸‹é™é—®é¢˜ï¼Œä½œè€…é€šè¿‡å®è¯ç ”ç©¶å‘ç°åç§»ä¸åŒ¹é…(offset misalignment)ç¨‹åº¦ä¸æ¦‚å¿µè·ç¦»(concept distance)æ˜¾è‘—ç›¸å…³ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œç ”ç©¶æå‡ºäº†åä¸º AIR (Adaptation with Iterative Refinement) çš„æ–°å‹æ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªé€šè¿‡è¿­ä»£ä¼˜åŒ–ç­–ç•¥æ¥ç¼“è§£åç§»ä¸åŒ¹é…å¹¶æå‡ç›®æ ‡åŸŸå›¾åƒè´¨é‡çš„ ZSGM æ–¹æ³•ã€‚åœ¨26ä¸ªå®éªŒåœºæ™¯ä¸‹çš„å®šæ€§åˆ†æã€å®šé‡è¯„ä¼°åŠç”¨æˆ·è°ƒç ”ä¸€è‡´è¯æ˜ï¼ŒAIR æ¡†æ¶åœ¨ç”Ÿæˆè´¨é‡ä¸Šè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›(SOTA)çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10895v1",
      "published_date": "2025-06-12 17:00:50 UTC",
      "updated_date": "2025-06-12 17:00:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:16:10.178679+00:00"
    },
    {
      "arxiv_id": "2506.14823v1",
      "title": "ViLLa: A Neuro-Symbolic approach for Animal Monitoring",
      "title_zh": "ViLLaï¼šä¸€ç§ç”¨äºåŠ¨ç‰©ç›‘æµ‹çš„ç¥ç»ç¬¦å·æ–¹æ³•",
      "authors": [
        "Harsha Koduri"
      ],
      "abstract": "Monitoring animal populations in natural environments requires systems that can interpret both visual data and human language queries. This work introduces ViLLa (Vision-Language-Logic Approach), a neuro-symbolic framework designed for interpretable animal monitoring. ViLLa integrates three core components: a visual detection module for identifying animals and their spatial locations in images, a language parser for understanding natural language queries, and a symbolic reasoning layer that applies logic-based inference to answer those queries. Given an image and a question such as \"How many dogs are in the scene?\" or \"Where is the buffalo?\", the system grounds visual detections into symbolic facts and uses predefined rules to compute accurate answers related to count, presence, and location. Unlike end-to-end black-box models, ViLLa separates perception, understanding, and reasoning, offering modularity and transparency. The system was evaluated on a range of animal imagery tasks and demonstrates the ability to bridge visual content with structured, human-interpretable queries.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ViLLa (Vision-Language-Logic Approach)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåŠ¨ç‰©ç›‘æµ‹çš„ç¥ç»ç¬¦å· (neuro-symbolic) æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶é›†æˆäº†è§†è§‰æ£€æµ‹æ¨¡å—ã€è¯­è¨€è§£æå™¨å’Œç¬¦å·æ¨ç†å±‚ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œèƒ½å¤ŸååŒå¤„ç†å¤æ‚çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚ViLLa å°†è§†è§‰æ£€æµ‹ç»“æœè½¬åŒ–ä¸ºç¬¦å·äº‹å® (symbolic facts)ï¼Œå¹¶è¿ç”¨é¢„å®šä¹‰çš„é€»è¾‘è§„åˆ™æ¥è®¡ç®—åŠ¨ç‰©çš„æ•°é‡ã€å­˜åœ¨çŠ¶æ€åŠå…·ä½“ä½ç½®ã€‚ä¸ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯é»‘ç›’æ¨¡å‹ (end-to-end black-box models) ä¸åŒï¼ŒViLLa å®ç°äº†æ„ŸçŸ¥ã€ç†è§£ä¸æ¨ç†çš„è§£è€¦ï¼Œä»è€Œä¿è¯äº†ç³»ç»Ÿçš„æ¨¡å—åŒ–å’Œé€æ˜åº¦ã€‚é€šè¿‡åœ¨å¤šç§åŠ¨ç‰©å›¾åƒä»»åŠ¡ä¸Šçš„å®éªŒè¯„ä¼°ï¼ŒViLLa è¯æ˜äº†å…¶åœ¨è¿æ¥è§†è§‰å†…å®¹ä¸äººç±»å¯è§£é‡ŠæŸ¥è¯¢æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.14823v1",
      "published_date": "2025-06-12 16:57:25 UTC",
      "updated_date": "2025-06-12 16:57:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:16:11.796359+00:00"
    },
    {
      "arxiv_id": "2506.10892v3",
      "title": "The Diffusion Duality",
      "title_zh": "æ‰©æ•£å¯¹å¶æ€§",
      "authors": [
        "Subham Sekhar Sahoo",
        "Justin Deschenaux",
        "Aaron Gokaslan",
        "Guanghan Wang",
        "Justin Chiu",
        "Volodymyr Kuleshov"
      ],
      "abstract": "Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/duo",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Uniform-state discrete diffusionæ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆæ€§èƒ½ä¸Šé€Šäºautoregressiveæ¨¡å‹å’Œmasked diffusionæ¨¡å‹çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºDuoçš„æ”¹è¿›æ¡†æ¶ã€‚Duoçš„æ ¸å¿ƒæ´å¯Ÿåœ¨äºUniform-state diffusionè¿‡ç¨‹èƒ½å¤Ÿè‡ªç„¶åœ°ä»åº•å±‚çš„Gaussian diffusionä¸­æ¨å¯¼äº§ç”Ÿï¼Œä»è€Œå®ç°äº†å°†Gaussianæ‰©æ•£æŠ€æœ¯å‘ç¦»æ•£é¢†åŸŸçš„æœ‰æ•ˆè¿ç§»ã€‚ç ”ç©¶é¦–å…ˆå¼•å…¥äº†ç”±Gaussianè¿‡ç¨‹å¼•å¯¼çš„curriculum learningç­–ç•¥ï¼Œé€šè¿‡é™ä½æ–¹å·®å°†è®­ç»ƒé€Ÿåº¦æå‡äº†ä¸€å€ï¼Œä¸”æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•çš„zero-shot perplexityæŒ‡æ ‡ä¸Šè¶…è¶Šäº†autoregressiveæ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†Discrete Consistency Distillationç®—æ³•ï¼ŒæˆåŠŸå°†è¿ç»­ç©ºé—´çš„ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯é€‚é…è‡³ç¦»æ•£è®¾ç½®ï¼Œä½¿é‡‡æ ·é€Ÿåº¦æå‡äº†ä¸¤ä¸ªæ•°é‡çº§ã€‚é€šè¿‡ç»“åˆè¿™äº›æŠ€æœ¯ï¼ŒDuoä¸ä»…æ˜¾è‘—ç¼©å°äº†æ€§èƒ½å·®è·ï¼Œè¿˜å®ç°äº†æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨few-step generationåœºæ™¯ä¸‹çš„é«˜æ•ˆç”Ÿæˆã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘é«˜æ€§èƒ½ã€é«˜æ•ˆç‡çš„ç¦»æ•£æ‰©æ•£ç”Ÿæˆæ¨¡å‹æä¾›äº†å…¨æ–°çš„ç†è®ºè§†è§’å’Œå®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025. We provide the code at: https://github.com/s-sahoo/duo [v3] includes improved theory, clearer presentation, and a new future work section",
      "pdf_url": "https://arxiv.org/pdf/2506.10892v3",
      "published_date": "2025-06-12 16:55:35 UTC",
      "updated_date": "2025-12-19 17:14:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:16:15.287608+00:00"
    },
    {
      "arxiv_id": "2506.10885v1",
      "title": "Slimming Down LLMs Without Losing Their Minds",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹è½»é‡åŒ–ï¼šåœ¨ä¸æŸå®³æ™ºèƒ½çš„å‰æä¸‹å®ç°æ¨¡å‹ç²¾ç®€",
      "authors": [
        "Qingda",
        "Mai"
      ],
      "abstract": "This paper investigates and validates the impact of fine-tuning on large language model performance, focusing on parameter-efficient methods (LoRA and QLoRA). We evaluate model capabilities across three key domains: (1) commonsense reasoning (HellaSwag), (2) mathematical reasoning (GSM8K), and (3) multi-domain knowledge (MMLU-CS).\n  Our findings demonstrate that: (1) LoRA-based methods effectively improve task-specific performance while maintaining computational efficiency, and (2) performance strongly depends on alignment between fine-tuning dataset and benchmark tasks. The study provides both theoretical insights into parameter-efficient mechanisms and practical guidance for developers implementing efficient LLM adaptation with limited resources.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥å¹¶éªŒè¯äº†å¾®è°ƒå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æ€§èƒ½çš„å½±å“ï¼Œç‰¹åˆ«å…³æ³¨ LoRA å’Œ QLoRA ç­‰å‚æ•°é«˜æ•ˆ (Parameter-Efficient) æ–¹æ³•ã€‚ç ”ç©¶é€šè¿‡ HellaSwagã€GSM8K å’Œ MMLU-CS è¯„ä¼°äº†æ¨¡å‹åœ¨å¸¸è¯†æ¨ç†ã€æ•°å­¦æ¨ç†åŠå¤šé¢†åŸŸçŸ¥è¯†æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒåŸºäº LoRA çš„æ–¹æ³•èƒ½åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—å¢å¼ºç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ€§èƒ½çš„æå‡é«˜åº¦ä¾èµ–äºå¾®è°ƒæ•°æ®é›†ä¸åŸºå‡†æµ‹è¯•ä»»åŠ¡ä¹‹é—´çš„å¯¹é½ (alignment)ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºå‚æ•°é«˜æ•ˆå¾®è°ƒæœºåˆ¶æä¾›äº†ç†è®ºæ´å¯Ÿï¼Œä¹Ÿä¸ºåœ¨æœ‰é™èµ„æºä¸‹è¿›è¡Œ LLM é€‚é…çš„å¼€å‘è€…æä¾›äº†å®ç”¨çš„æŠ€æœ¯æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.10885v1",
      "published_date": "2025-06-12 16:49:40 UTC",
      "updated_date": "2025-06-12 16:49:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:16:26.248828+00:00"
    },
    {
      "arxiv_id": "2506.10875v1",
      "title": "Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material",
      "title_zh": "æœºå™¨äººé™„è‚¢ä¸é¢—ç²’ææ–™åŠ¨æ€äº¤äº’çš„æ•°æ®é©±åŠ¨é¢„æµ‹",
      "authors": [
        "Guanjin Wang",
        "Xiangxue Zhao",
        "Shapour Azarm",
        "Balakumar Balachandran"
      ],
      "abstract": "An alternative data-driven modeling approach has been proposed and employed to gain fundamental insights into robot motion interaction with granular terrain at certain length scales. The approach is based on an integration of dimension reduction (Sequentially Truncated Higher-Order Singular Value Decomposition), surrogate modeling (Gaussian Process), and data assimilation techniques (Reduced Order Particle Filter). This approach can be used online and is based on offline data, obtained from the offline collection of high-fidelity simulation data and a set of sparse experimental data. The results have shown that orders of magnitude reduction in computational time can be obtained from the proposed data-driven modeling approach compared with physics-based high-fidelity simulations. With only simulation data as input, the data-driven prediction technique can generate predictions that have comparable accuracy as simulations. With both simulation data and sparse physical experimental measurement as input, the data-driven approach with its embedded data assimilation techniques has the potential in outperforming only high-fidelity simulations for the long-horizon predictions. In addition, it is demonstrated that the data-driven modeling approach can also reproduce the scaling relationship recovered by physics-based simulations for maximum resistive forces, which may indicate its general predictability beyond a case-by-case basis. The results are expected to help robot navigation and exploration in unknown and complex terrains during both online and offline phases.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ›¿ä»£æ€§çš„æ•°æ®é©±åŠ¨å»ºæ¨¡æ–¹æ³•ï¼Œç”¨äºé¢„æµ‹æœºå™¨äººè‚¢ä½“ä¸é¢—ç²’ä»‹è´¨ä¹‹é—´çš„åŠ¨æ€ç›¸äº’ä½œç”¨ã€‚è¯¥æ–¹æ³•é›†æˆäº†ç»´åº¦å‰Šå‡(Sequentially Truncated Higher-Order Singular Value Decomposition)ã€ä»£ç†æ¨¡å‹(Gaussian Process)å’Œæ•°æ®åŒåŒ–æŠ€æœ¯(Reduced Order Particle Filter)ï¼Œåˆ©ç”¨é«˜ä¿çœŸä»¿çœŸå’Œç¨€ç–å®éªŒæ•°æ®è¿›è¡Œåœ¨çº¿æˆ–ç¦»çº¿é¢„æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ¯”åŸºäºç‰©ç†çš„é«˜ä¿çœŸä»¿çœŸåœ¨è®¡ç®—æ•ˆç‡ä¸Šæå‡äº†æ•°ä¸ªæ•°é‡çº§ï¼Œä¸”åœ¨ä»…ä½¿ç”¨ä»¿çœŸæ•°æ®æ—¶ä»èƒ½ä¿æŒç›¸å½“çš„é¢„æµ‹ç²¾åº¦ã€‚é€šè¿‡ç»“åˆä»¿çœŸä¸å®éªŒæ•°æ®çš„ç²’å­æ»¤æ³¢æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•åœ¨é•¿æ—¶ç¨‹é¢„æµ‹(long-horizon predictions)æ–¹é¢å…·æœ‰è¶…è¶Šä¼ ç»Ÿä»¿çœŸçš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æˆåŠŸå¤ç°äº†ç‰©ç†ä»¿çœŸä¸­çš„æœ€å¤§é˜»åŠ›æ ‡åº¦å…³ç³»(scaling relationship)ï¼Œè¯æ˜äº†å…¶å…·å¤‡è‰¯å¥½çš„é€šç”¨é¢„æµ‹èƒ½åŠ›ã€‚è¯¥é¡¹ç ”ç©¶æˆæœèƒ½å¤Ÿæ˜¾è‘—æå‡æœºå™¨äººåœ¨æœªçŸ¥å¤æ‚é¢—ç²’åœ°å½¢ä¸­çš„å¯¼èˆªä¸æ¢ç´¢èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10875v1",
      "published_date": "2025-06-12 16:43:21 UTC",
      "updated_date": "2025-06-12 16:43:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:16:29.200144+00:00"
    },
    {
      "arxiv_id": "2506.10868v1",
      "title": "A multi-scale loss formulation for learning a probabilistic model with proper score optimisation",
      "title_zh": "ä¸€ç§åŸºäºæ°å½“è¯„åˆ†ä¼˜åŒ–çš„æ¦‚ç‡æ¨¡å‹å­¦ä¹ å¤šå°ºåº¦æŸå¤±è¡¨è¿°",
      "authors": [
        "Simon Lang",
        "Martin Leutbecher",
        "Pedro Maciel"
      ],
      "abstract": "We assess the impact of a multi-scale loss formulation for training probabilistic machine-learned weather forecasting models. The multi-scale loss is tested in AIFS-CRPS, a machine-learned weather forecasting model developed at the European Centre for Medium-Range Weather Forecasts (ECMWF). AIFS-CRPS is trained by directly optimising the almost fair continuous ranked probability score (afCRPS). The multi-scale loss better constrains small scale variability without negatively impacting forecast skill. This opens up promising directions for future work in scale-aware model training.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº† multi-scale loss formulation åœ¨è®­ç»ƒæ¦‚ç‡æœºå™¨å­¦ä¹ å¤©æ°”é¢„æŠ¥æ¨¡å‹ä¸­çš„å½±å“ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ¬§æ´²ä¸­æœŸå¤©æ°”é¢„æŠ¥ä¸­å¿ƒï¼ˆECMWFï¼‰å¼€å‘çš„ AIFS-CRPS æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ç›´æ¥ä¼˜åŒ– almost fair continuous ranked probability score (afCRPS) æ¥å®ç°æ¦‚ç‡å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œmulti-scale loss èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°çº¦æŸæ¨¡å‹ä¸­çš„ small scale variabilityï¼Œä¸”ä¸ä¼šå¯¹æ•´ä½“çš„ forecast skill äº§ç”Ÿä»»ä½•è´Ÿé¢å½±å“ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºæœªæ¥ scale-aware model training çš„å‘å±•å¼€è¾Ÿäº†æå…·å‰æ™¯çš„æ–¹å‘ã€‚",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10868v1",
      "published_date": "2025-06-12 16:30:18 UTC",
      "updated_date": "2025-06-12 16:30:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:16:27.463316+00:00"
    },
    {
      "arxiv_id": "2506.10859v1",
      "title": "Precise Zero-Shot Pointwise Ranking with LLMs through Post-Aggregated Global Context Information",
      "title_zh": "åŸºäºåèšåˆå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å¤§è¯­è¨€æ¨¡å‹ç²¾å‡†é›¶æ ·æœ¬é€ç‚¹æ’åº",
      "authors": [
        "Kehan Long",
        "Shasha Li",
        "Chen Xu",
        "Jintao Tang",
        "Ting Wang"
      ],
      "abstract": "Recent advancements have successfully harnessed the power of Large Language Models (LLMs) for zero-shot document ranking, exploring a variety of prompting strategies. Comparative approaches like pairwise and listwise achieve high effectiveness but are computationally intensive and thus less practical for larger-scale applications. Scoring-based pointwise approaches exhibit superior efficiency by independently and simultaneously generating the relevance scores for each candidate document. However, this independence ignores critical comparative insights between documents, resulting in inconsistent scoring and suboptimal performance. In this paper, we aim to improve the effectiveness of pointwise methods while preserving their efficiency through two key innovations: (1) We propose a novel Global-Consistent Comparative Pointwise Ranking (GCCP) strategy that incorporates global reference comparisons between each candidate and an anchor document to generate contrastive relevance scores. We strategically design the anchor document as a query-focused summary of pseudo-relevant candidates, which serves as an effective reference point by capturing the global context for document comparison. (2) These contrastive relevance scores can be efficiently Post-Aggregated with existing pointwise methods, seamlessly integrating essential Global Context information in a training-free manner (PAGC). Extensive experiments on the TREC DL and BEIR benchmark demonstrate that our approach significantly outperforms previous pointwise methods while maintaining comparable efficiency. Our method also achieves competitive performance against comparative methods that require substantially more computational resources. More analyses further validate the efficacy of our anchor construction strategy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é›¶æ ·æœ¬ (zero-shot) æ–‡æ¡£æ’åºä¸­ç‚¹å¯¹ç‚¹ (pointwise) æ–¹æ³•æ•ˆç‡é«˜ä½†å› ç¼ºä¹æ–‡æ¡£é—´å¯¹æ¯”ä¿¡æ¯è€Œå¯¼è‡´æ€§èƒ½å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†é€šè¿‡åèšåˆå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æå‡æ’åºç²¾åº¦çš„æ–¹æ³•ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸ºå…¨å±€ä¸€è‡´æ€§æ¯”è¾ƒç‚¹å¯¹ç‚¹æ’åº (Global-Consistent Comparative Pointwise Ranking, GCCP) çš„ç­–ç•¥ï¼Œé€šè¿‡åœ¨æ¯ä¸ªå€™é€‰æ–‡æ¡£ä¸ä¸€ä¸ªé”šç‚¹æ–‡æ¡£ (anchor document) ä¹‹é—´è¿›è¡Œå…¨å±€å‚è€ƒæ¯”è¾ƒæ¥ç”Ÿæˆå¯¹æ¯”æ€§ç›¸å…³å¾—åˆ†ã€‚é”šç‚¹æ–‡æ¡£è¢«è®¾è®¡ä¸ºä¼ªç›¸å…³å€™é€‰æ–‡æ¡£ä¸­èšç„¦æŸ¥è¯¢çš„æ‘˜è¦ï¼Œç”¨ä»¥æ•æ‰æ–‡æ¡£æ¯”è¾ƒæ‰€éœ€çš„å…¨å±€ä¸Šä¸‹æ–‡ (Global Context)ã€‚è¿™äº›å¯¹æ¯”å¾—åˆ†å¯ä»¥é€šè¿‡åèšåˆå…¨å±€ä¸Šä¸‹æ–‡ (PAGC) æ–¹å¼ä¸ç°æœ‰æ–¹æ³•é«˜æ•ˆç»“åˆï¼Œå®ç°æ— éœ€è®­ç»ƒå³å¯æ— ç¼é›†æˆå…¨å±€ä¿¡æ¯ã€‚åœ¨ TREC DL å’Œ BEIR åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—ä¼˜äºä¼ ç»Ÿç‚¹å¯¹ç‚¹æ–¹æ³•ã€‚å…¶æ€§èƒ½ç”šè‡³èƒ½ä¸èµ„æºæ¶ˆè€—å·¨å¤§çš„æˆå¯¹ (pairwise) æˆ–åˆ—è¡¨ (listwise) æ’åºæ–¹æ³•ç›¸åª²ç¾ï¼ŒéªŒè¯äº†é”šç‚¹æ„é€ ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by SIGIR 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10859v1",
      "published_date": "2025-06-12 16:20:40 UTC",
      "updated_date": "2025-06-12 16:20:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:16:30.162762+00:00"
    },
    {
      "arxiv_id": "2506.10857v2",
      "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
      "title_zh": "VRBenchï¼šé¢å‘é•¿å™äº‹è§†é¢‘å¤šæ­¥æ¨ç†çš„è¯„æµ‹åŸºå‡†",
      "authors": [
        "Jiashuo Yu",
        "Yue Wu",
        "Meng Chu",
        "Zhifei Ren",
        "Zizheng Huang",
        "Pei Chu",
        "Ruijie Zhang",
        "Yinan He",
        "Qirui Li",
        "Songze Li",
        "Zhenxiang Li",
        "Zhongying Tu",
        "Conghui He",
        "Yu Qiao",
        "Yali Wang",
        "Yi Wang",
        "Limin Wang"
      ],
      "abstract": "We present VRBench, the first long narrative video benchmark crafted for evaluating large models' multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 960 long videos (with an average duration of 1.6 hours), along with 8,243 human-labeled multi-step question-answering pairs and 25,106 reasoning steps with timestamps. These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, spanning seven types (e.g., event attribution, implicit inference). VRBench designs a multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 19 VLMs on VRBench, we undertake a thorough analysis and provide valuable insights that advance the field of multi-step reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VRBenchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹æ¨¡å‹åœ¨é•¿å™äº‹è§†é¢‘ä¸­å¤šæ­¥æ¨ç†(Multi-Step Reasoning)èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°åœ¨æ—¶é—´æ¨ç†å’Œç¨‹åºæœ‰æ•ˆæ€§æ–¹é¢çš„å±€é™ã€‚è¯¥åŸºå‡†åŒ…å«960ä¸ªå¹³å‡æ—¶é•¿ä¸º1.6å°æ—¶çš„é•¿è§†é¢‘ï¼Œä»¥åŠ8,243å¯¹äººå·¥æ ‡æ³¨çš„å¤šæ­¥é—®ç­”å¯¹å’Œ25,106ä¸ªå¸¦æœ‰æ—¶é—´æˆ³çš„æ¨ç†æ­¥éª¤ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªäººæœºåä½œæ¡†æ¶ï¼Œç”Ÿæˆæ¶µç›–äº‹ä»¶å½’å› (Event Attribution)å’Œéšå¼æ¨ç†(Implicit Inference)ç­‰ä¸ƒç§ç±»å‹çš„è¿è´¯æ¨ç†é“¾(Reasoning Chains)ã€‚VRBenchè®¾è®¡äº†å¤šé˜¶æ®µè¯„ä¼°æµç¨‹ï¼Œé€šè¿‡å¤šé¡¹é€‰æ‹©é¢˜(MCQs)å’Œä¸€ç§æ–°å‹çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM-guided)çš„è¯„åˆ†æŒ‡æ ‡ï¼Œä»ç»“æœå’Œè¿‡ç¨‹ä¸¤ä¸ªç»´åº¦å…¨é¢è¡¡é‡æ¨ç†è´¨é‡ã€‚é€šè¿‡å¯¹12ä¸ªå¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œ19ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„å¹¿æ³›è¯„ä¼°ï¼Œè¯¥ç ”ç©¶ä¸ºæ¨åŠ¨é•¿è§†é¢‘ç†è§£é¢†åŸŸçš„å¤šæ­¥æ¨ç†æŠ€æœ¯æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10857v2",
      "published_date": "2025-06-12 16:17:17 UTC",
      "updated_date": "2025-08-04 09:11:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:16:35.725421+00:00"
    },
    {
      "arxiv_id": "2506.11182v1",
      "title": "Multimodal Modeling of CRISPR-Cas12 Activity Using Foundation Models and Chromatin Accessibility Data",
      "title_zh": "åŸºäºåŸºç¡€æ¨¡å‹ä¸æŸ“è‰²è´¨å¯åŠæ€§æ•°æ®çš„ CRISPR-Cas12 æ´»æ€§å¤šæ¨¡æ€å»ºæ¨¡",
      "authors": [
        "Azim Dehghani Amirabad",
        "Yanfei Zhang",
        "Artem Moskalev",
        "Sowmya Rajesh",
        "Tommaso Mansi",
        "Shuwei Li",
        "Mangal Prakash",
        "Rui Liao"
      ],
      "abstract": "Predicting guide RNA (gRNA) activity is critical for effective CRISPR-Cas12 genome editing but remains challenging due to limited data, variation across protospacer adjacent motifs (PAMs-short sequence requirements for Cas binding), and reliance on large-scale training. We investigate whether pre-trained biological foundation model originally trained on transcriptomic data can improve gRNA activity estimation even without domain-specific pre-training. Using embeddings from existing RNA foundation model as input to lightweight regressor, we show substantial gains over traditional baselines. We also integrate chromatin accessibility data to capture regulatory context, improving performance further. Our results highlight the effectiveness of pre-trained foundation models and chromatin accessibility data for gRNA activity prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒç”Ÿç‰©åŸºç¡€æ¨¡å‹ (foundation models) å’ŒæŸ“è‰²è´¨å¯åŠæ€§ (chromatin accessibility) æ•°æ®æ¥é¢„æµ‹ CRISPR-Cas12 çš„ guide RNA (gRNA) æ´»æ€§ã€‚é’ˆå¯¹è¯¥é¢†åŸŸé¢ä¸´çš„æ•°æ®æœ‰é™ã€PAM åºåˆ—å˜å¼‚ä»¥åŠå¯¹å¤§è§„æ¨¡è®­ç»ƒä¾èµ–ç­‰æŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜éªŒè¯äº†åŸºäºè½¬å½•ç»„å­¦æ•°æ®é¢„è®­ç»ƒçš„æ¨¡å‹åœ¨ gRNA æ´»æ€§é¢„æµ‹ä»»åŠ¡ä¸­çš„è·¨é¢†åŸŸæ½œåŠ›ã€‚é€šè¿‡å°† RNA foundation model çš„åµŒå…¥ (embeddings) è¾“å…¥åˆ°è½»é‡çº§å›å½’å™¨ (lightweight regressor) ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»ŸåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿›ä¸€æ­¥æ•´åˆäº†æŸ“è‰²è´¨å¯åŠæ€§æ•°æ®ä»¥æ•è·åŸºå› ç»„çš„è°ƒæ§ä¸Šä¸‹æ–‡ï¼Œä»è€Œæå‡äº†é¢„æµ‹ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ç»“åˆå¤šæ¨¡æ€æ•°æ®èƒ½æœ‰æ•ˆå¢å¼ºå¯¹ CRISPR-Cas12 ç¼–è¾‘æ•ˆç‡çš„é¢„æµ‹èƒ½åŠ›ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "primary_category": "q-bio.GN",
      "comment": "This manuscript has been accepted by ICML workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.11182v1",
      "published_date": "2025-06-12 16:15:14 UTC",
      "updated_date": "2025-06-12 16:15:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:16:46.551341+00:00"
    },
    {
      "arxiv_id": "2506.10853v1",
      "title": "A Study on Individual Spatiotemporal Activity Generation Method Using MCP-Enhanced Chain-of-Thought Large Language Models",
      "title_zh": "åŸºäº MCP å¢å¼ºå‹æ€ç»´é“¾å¤§è¯­è¨€æ¨¡å‹çš„ä¸ªäººæ—¶ç©ºæ´»åŠ¨ç”Ÿæˆæ–¹æ³•ç ”ç©¶",
      "authors": [
        "Yu Zhang",
        "Yang Hu",
        "De Wang"
      ],
      "abstract": "Human spatiotemporal behavior simulation is critical for urban planning research, yet traditional rule-based and statistical approaches suffer from high computational costs, limited generalizability, and poor scalability. While large language models (LLMs) show promise as \"world simulators,\" they face challenges in spatiotemporal reasoning including limited spatial cognition, lack of physical constraint understanding, and group homogenization tendencies. This paper introduces a framework integrating chain-of-thought (CoT) reasoning with Model Context Protocol (MCP) to enhance LLMs' capability in simulating spatiotemporal behaviors that correspond with validation data patterns. The methodology combines human-like progressive reasoning through a five-stage cognitive framework with comprehensive data processing via six specialized MCP tool categories: temporal management, spatial navigation, environmental perception, personal memory, social collaboration, and experience evaluation. Experiments in Shanghai's Lujiazui district validate the framework's effectiveness across 1,000 generated samples. Results demonstrate high similarity with real mobile signaling data, achieving generation quality scores of 7.86 to 8.36 across different base models. Parallel processing experiments show efficiency improvements, with generation times decreasing from 1.30 to 0.17 minutes per sample when scaling from 2 to 12 processes. This work contributes to integrating CoT reasoning with MCP for urban behavior modeling, advancing LLMs applications in urban computing and providing a practical approach for synthetic mobility data generation. The framework offers a foundation for smart city planning, transportation forecasting, and participatory urban design applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ—¶ç©ºè¡Œä¸ºæ¨¡æ‹Ÿæ–¹æ³•è®¡ç®—æˆæœ¬é«˜ã€æ³›åŒ–æ€§æœ‰é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆé“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†ä¸æ¨¡å‹ä¸Šä¸‹æ–‡åè®®(Model Context Protocol, MCP)çš„ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äº”é˜¶æ®µè®¤çŸ¥æ¨¡å‹æ¨¡æ‹Ÿäººç±»çš„æ¸è¿›å¼æ¨ç†ï¼Œå¹¶é›†æˆæ—¶é—´ç®¡ç†ã€ç©ºé—´å¯¼èˆªã€ç¯å¢ƒæ„ŸçŸ¥ã€ä¸ªäººè®°å¿†ã€ç¤¾äº¤åä½œå’Œç»éªŒè¯„ä¼°ç­‰å…­ç±»ä¸“é—¨çš„MCPå·¥å…·ï¼Œæœ‰æ•ˆå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç‰©ç†çº¦æŸç†è§£å’Œæ—¶ç©ºæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚åœ¨ä¸Šæµ·é™†å®¶å˜´åœ°åŒºçš„éªŒè¯å®éªŒè¡¨æ˜ï¼Œç”Ÿæˆçš„1,000ä¸ªæ ·æœ¬ä¸çœŸå®ç§»åŠ¨ä¿¡ä»¤æ•°æ®é«˜åº¦ç›¸ä¼¼ï¼Œåœ¨ä¸åŒåŸºåº§æ¨¡å‹ä¸Šçš„ç”Ÿæˆè´¨é‡è¯„åˆ†è¾¾åˆ°7.86è‡³8.36ã€‚æ­¤å¤–ï¼Œå¹¶è¡Œå¤„ç†å®éªŒæ˜¾è‘—æå‡äº†æ¨¡æ‹Ÿæ•ˆç‡ï¼Œä½¿å•æ ·æœ¬ç”Ÿæˆæ—¶é—´ä»1.30åˆ†é’Ÿç¼©çŸ­è‡³0.17åˆ†é’Ÿã€‚æ­¤é¡¹å·¥ä½œä¸ºåŸå¸‚è®¡ç®—ä¸­çš„ä¸ªä½“è¡Œä¸ºå»ºæ¨¡å’Œåˆæˆç§»åŠ¨æ•°æ®(Synthetic Mobility Data)çš„ç”Ÿæˆæä¾›äº†å®ç”¨è·¯å¾„ï¼Œåœ¨æ™ºèƒ½åŸå¸‚è§„åˆ’ã€äº¤é€šé¢„æµ‹å’Œå‚ä¸å¼åŸå¸‚è®¾è®¡ä¸­å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10853v1",
      "published_date": "2025-06-12 16:14:32 UTC",
      "updated_date": "2025-06-12 16:14:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:16:43.175140+00:00"
    },
    {
      "arxiv_id": "2506.10848v2",
      "title": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles",
      "title_zh": "åŸºäº SlowFast é‡‡æ ·çš„æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹åŠ é€Ÿï¼šä¸‰å¤§é»„é‡‘å‡†åˆ™",
      "authors": [
        "Qingyan Wei",
        "Yaojie Zhang",
        "Zhiyuan Liu",
        "Dongrui Liu",
        "Linfeng Zhang"
      ],
      "abstract": "Diffusion-based language models (dLLMs) have emerged as a promising alternative to traditional autoregressive LLMs by enabling parallel token generation and significantly reducing inference latency. However, existing sampling strategies for dLLMs, such as confidence-based or semi-autoregressive decoding, often suffer from static behavior, leading to suboptimal efficiency and limited flexibility. In this paper, we propose SlowFast Sampling, a novel dynamic sampling strategy that adaptively alternates between exploratory and accelerated decoding stages. Our method is guided by three golden principles: certainty principle, convergence principle, and positional principle, which govern when and where tokens can be confidently and efficiently decoded. We further integrate our strategy with dLLM-Cache to reduce redundant computation. Extensive experiments across benchmarks and models show that SlowFast Sampling achieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and up to 34.22$\\times$ when combined with caching. Notably, our approach outperforms strong autoregressive baselines like LLaMA3 8B in throughput, demonstrating that well-designed sampling can unlock the full potential of dLLMs for fast and high-quality generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ‰©æ•£çš„å¤§è¯­è¨€æ¨¡å‹(Diffusion-based language models, dLLMs)ç°æœ‰é‡‡æ ·ç­–ç•¥æ•ˆç‡ä½ä¸‹ä¸”ç¼ºä¹çµæ´»æ€§ç­‰é—®é¢˜ï¼Œæå‡ºäº†SlowFast Samplingè¿™ä¸€åŠ¨æ€é‡‡æ ·ç­–ç•¥ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ¢ç´¢æ€§å’ŒåŠ é€Ÿè§£ç é˜¶æ®µä¹‹é—´è¿›è¡Œè‡ªé€‚åº”åˆ‡æ¢ï¼Œå¹¶ç”±ç¡®å®šæ€§åŸåˆ™(certainty principle)ã€æ”¶æ•›åŸåˆ™(convergence principle)ä»¥åŠä½ç½®åŸåˆ™(positional principle)è¿™ä¸‰é¡¹é»„é‡‘å‡†åˆ™æŒ‡å¯¼ï¼Œå®ç°äº†å¯¹ä»¤ç‰Œè§£ç æ—¶æœºä¸ä½ç½®çš„ç²¾ç¡®ç®¡æ§ã€‚é€šè¿‡è¿›ä¸€æ­¥é›†æˆdLLM-Cacheä»¥å‡å°‘å†—ä½™è®¡ç®—ï¼ŒSlowFast Samplingåœ¨LLaDAæ¨¡å‹ä¸Šå®ç°äº†é«˜è¾¾15.63å€çš„åŠ é€Ÿï¼Œç»“åˆç¼“å­˜ååŠ é€Ÿæ¯”æ›´å¯è¾¾34.22å€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„ååé‡å·²è¶…è¶ŠLLaMA3 8Bç­‰å¼ºåŠ›è‡ªå›å½’åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†ä¼˜åŒ–é‡‡æ ·ç­–ç•¥èƒ½æœ‰æ•ˆé‡Šæ”¾dLLMsåœ¨å¿«é€Ÿã€é«˜è´¨é‡ç”Ÿæˆæ–¹é¢çš„æ ¸å¿ƒæ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages; 5 figures;",
      "pdf_url": "https://arxiv.org/pdf/2506.10848v2",
      "published_date": "2025-06-12 16:08:28 UTC",
      "updated_date": "2025-06-13 02:28:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:17:10.982192+00:00"
    },
    {
      "arxiv_id": "2506.10840v1",
      "title": "Post-Training Quantization for Video Matting",
      "title_zh": "é¢å‘è§†é¢‘æŠ å›¾çš„è®­ç»ƒåé‡åŒ–",
      "authors": [
        "Tianrui Zhu",
        "Houyuan Chen",
        "Ruihao Gong",
        "Michele Magno",
        "Haotong Qin",
        "Kai Zhang"
      ],
      "abstract": "Video matting is crucial for applications such as film production and virtual reality, yet deploying its computationally intensive models on resource-constrained devices presents challenges. Quantization is a key technique for model compression and acceleration. As an efficient approach, Post-Training Quantization (PTQ) is still in its nascent stages for video matting, facing significant hurdles in maintaining accuracy and temporal coherence. To address these challenges, this paper proposes a novel and general PTQ framework specifically designed for video matting models, marking, to the best of our knowledge, the first systematic attempt in this domain. Our contributions include: (1) A two-stage PTQ strategy that combines block-reconstruction-based optimization for fast, stable initial quantization and local dependency capture, followed by a global calibration of quantization parameters to minimize accuracy loss. (2) A Statistically-Driven Global Affine Calibration (GAC) method that enables the network to compensate for cumulative statistical distortions arising from factors such as neglected BN layer effects, even reducing the error of existing PTQ methods on video matting tasks up to 20%. (3) An Optical Flow Assistance (OFA) component that leverages temporal and semantic priors from frames to guide the PTQ process, enhancing the model's ability to distinguish moving foregrounds in complex scenes and ultimately achieving near full-precision performance even under ultra-low-bit quantization. Comprehensive quantitative and visual results show that our PTQ4VM achieves the state-of-the-art accuracy performance across different bit-widths compared to the existing quantization methods. We highlight that the 4-bit PTQ4VM even achieves performance close to the full-precision counterpart while enjoying 8x FLOP savings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Video Mattingæ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªç³»ç»Ÿæ€§çš„åè®­ç»ƒé‡åŒ–(Post-Training Quantization, PTQ)æ¡†æ¶PTQ4VMã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼Œé€šè¿‡ç»“åˆå—é‡å»ºä¼˜åŒ–ä¸å…¨å±€å‚æ•°æ ¡å‡†æ¥æ•æ‰å±€éƒ¨ä¾èµ–å¹¶æœ€å°åŒ–ç²¾åº¦æŸå¤±ã€‚ç ”ç©¶ä¸­å¼•å…¥äº†ç»Ÿè®¡é©±åŠ¨çš„å…¨å±€ä»¿å°„æ ¡æ­£(Global Affine Calibration, GAC)ä»¥è¡¥å¿ç´¯ç§¯ç»Ÿè®¡å¤±çœŸï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•å¯å°†è¯¯å·®é™ä½è‡³å¤š20%ã€‚åŒæ—¶ï¼Œå…‰æµè¾…åŠ©(Optical Flow Assistance, OFA)ç»„ä»¶åˆ©ç”¨æ—¶é—´ä¸è¯­ä¹‰å…ˆéªŒæŒ‡å¯¼é‡åŒ–è¿‡ç¨‹ï¼Œå¢å¼ºäº†å¤æ‚åœºæ™¯ä¸‹åŒºåˆ†ç§»åŠ¨å‰æ™¯çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPTQ4VMåœ¨å¤šç§ä½å®½ä¸‹å‡è¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œå…¶4-bitç‰ˆæœ¬åœ¨å®ç°8å€FLOPsèŠ‚çœçš„åŒæ—¶ï¼Œä¾ç„¶ä¿æŒäº†æ¥è¿‘å…¨ç²¾åº¦æ¨¡å‹çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10840v1",
      "published_date": "2025-06-12 15:57:14 UTC",
      "updated_date": "2025-06-12 15:57:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:17:05.506719+00:00"
    },
    {
      "arxiv_id": "2506.10831v1",
      "title": "Efficiency Robustness of Dynamic Deep Learning Systems",
      "title_zh": "åŠ¨æ€æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„æ•ˆç‡é²æ£’æ€§",
      "authors": [
        "Ravishka Rathnasuriya",
        "Tingxi Li",
        "Zexin Xu",
        "Zihe Song",
        "Mirazul Haque",
        "Simin Chen",
        "Wei Yang"
      ],
      "abstract": "Deep Learning Systems (DLSs) are increasingly deployed in real-time applications, including those in resourceconstrained environments such as mobile and IoT devices. To address efficiency challenges, Dynamic Deep Learning Systems (DDLSs) adapt inference computation based on input complexity, reducing overhead. While this dynamic behavior improves efficiency, such behavior introduces new attack surfaces. In particular, efficiency adversarial attacks exploit these dynamic mechanisms to degrade system performance. This paper systematically explores efficiency robustness of DDLSs, presenting the first comprehensive taxonomy of efficiency attacks. We categorize these attacks based on three dynamic behaviors: (i) attacks on dynamic computations per inference, (ii) attacks on dynamic inference iterations, and (iii) attacks on dynamic output production for downstream tasks. Through an in-depth evaluation, we analyze adversarial strategies that target DDLSs efficiency and identify key challenges in securing these systems. In addition, we investigate existing defense mechanisms, demonstrating their limitations against increasingly popular efficiency attacks and the necessity for novel mitigation strategies to secure future adaptive DDLSs.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†åŠ¨æ€æ·±åº¦å­¦ä¹ ç³»ç»Ÿ(DDLSs)çš„æ•ˆç‡é²æ£’æ€§(efficiency robustness)ï¼Œæ—¨åœ¨åº”å¯¹åŠ¨æ€æ¨ç†æœºåˆ¶å¼•å…¥çš„æ•ˆç‡å¯¹æŠ—æ”»å‡»(efficiency adversarial attacks)ç­‰å®‰å…¨å¨èƒã€‚ä½œè€…é¦–æ¬¡æå‡ºäº†é’ˆå¯¹DDLSsæ•ˆç‡æ”»å‡»çš„å…¨é¢åˆ†ç±»æ³•ï¼Œå°†æ”»å‡»è¡Œä¸ºå½’çº³ä¸ºé’ˆå¯¹å•æ¬¡æ¨ç†è®¡ç®—é‡ã€æ¨ç†è¿­ä»£æ¬¡æ•°ä»¥åŠä¸‹æ¸¸ä»»åŠ¡è¾“å‡ºç”Ÿæˆä¸‰ç±»ã€‚é€šè¿‡æ·±å…¥çš„å®éªŒè¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†é’ˆå¯¹åŠ¨æ€æœºåˆ¶çš„å¯¹æŠ—ç­–ç•¥ï¼Œå¹¶æŒ‡å‡ºäº†åœ¨ä¿éšœç³»ç»Ÿæ•ˆç‡ç¨³å®šæ€§æ–¹é¢é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è°ƒæŸ¥å¹¶éªŒè¯äº†ç°æœ‰é˜²å¾¡æœºåˆ¶åœ¨åº”å¯¹æ—¥ç›Šå¢é•¿çš„æ•ˆç‡æ”»å‡»æ—¶å­˜åœ¨çš„å±€é™æ€§ã€‚æœ€åï¼Œç ”ç©¶å¼ºè°ƒäº†å¼€å‘æ–°å‹ç¼“è§£ç­–ç•¥çš„ç´§è¿«æ€§ï¼Œä¸ºæ„å»ºæ›´å®‰å…¨ã€æ›´å…·é²æ£’æ€§çš„è‡ªé€‚åº”DDLSsæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to USENIX Security '25",
      "pdf_url": "https://arxiv.org/pdf/2506.10831v1",
      "published_date": "2025-06-12 15:49:01 UTC",
      "updated_date": "2025-06-12 15:49:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:17:16.171048+00:00"
    },
    {
      "arxiv_id": "2506.10829v1",
      "title": "LLM-Driven Personalized Answer Generation and Evaluation",
      "title_zh": "LLMé©±åŠ¨çš„ä¸ªæ€§åŒ–ç­”æ¡ˆç”Ÿæˆä¸è¯„ä¼°",
      "authors": [
        "Mohammadreza Molavi",
        "Mohammadreza Tavakoli",
        "Mohammad Moein",
        "Abdolali Faraji",
        "GÃ¡bor KismihÃ³k"
      ],
      "abstract": "Online learning has experienced rapid growth due to its flexibility and accessibility. Personalization, adapted to the needs of individual learners, is crucial for enhancing the learning experience, particularly in online settings. A key aspect of personalization is providing learners with answers customized to their specific questions. This paper therefore explores the potential of Large Language Models (LLMs) to generate personalized answers to learners' questions, thereby enhancing engagement and reducing the workload on educators. To evaluate the effectiveness of LLMs in this context, we conducted a comprehensive study using the StackExchange platform in two distinct areas: language learning and programming. We developed a framework and a dataset for validating automatically generated personalized answers. Subsequently, we generated personalized answers using different strategies, including 0-shot, 1-shot, and few-shot scenarios. The generated answers were evaluated using three methods: 1. BERTScore, 2. LLM evaluation, and 3. human evaluation. Our findings indicated that providing LLMs with examples of desired answers (from the learner or similar learners) can significantly enhance the LLMs' ability to tailor responses to individual learners' needs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) ä¸ºåœ¨çº¿å­¦ä¹ è€…ç”Ÿæˆä¸ªæ€§åŒ–ç­”æ¡ˆçš„æ½œåŠ›ï¼Œæ—¨åœ¨æå‡å­¦ä¹ å‚ä¸åº¦å¹¶å‡è½»æ•™å¸ˆçš„å·¥ä½œè´Ÿæ‹…ã€‚ç ”ç©¶å›¢é˜ŸåŸºäº StackExchange å¹³å°åœ¨è¯­è¨€å­¦ä¹ å’Œç¼–ç¨‹é¢†åŸŸçš„æ•°æ®ï¼Œæ„å»ºäº†ä¸€ä¸ªä¸“é—¨ç”¨äºéªŒè¯è‡ªåŠ¨ç”Ÿæˆä¸ªæ€§åŒ–ç­”æ¡ˆçš„æ¡†æ¶å’Œæ•°æ®é›†ã€‚å®éªŒè¿‡ç¨‹ä¸­å¯¹æ¯”äº† 0-shotã€1-shot åŠ few-shot ç­‰ä¸åŒç”Ÿæˆç­–ç•¥ï¼Œå¹¶ç»“åˆ BERTScoreã€LLM evaluation ä»¥åŠäººç±»è¯„ä¼° (human evaluation) å¯¹ç»“æœè¿›è¡Œäº†å¤šç»´åº¦åˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œå‘ LLMs æä¾›ç›®æ ‡å­¦ä¹ è€…æˆ–ç›¸ä¼¼å­¦ä¹ è€…çš„ç¤ºä¾‹ç­”æ¡ˆï¼Œèƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹å®šåˆ¶åŒ–å“åº”çš„èƒ½åŠ›ã€‚è¯¥å·¥ä½œè¯æ˜äº† LLMs åœ¨æ¨¡æ‹Ÿä¸ªæ€§åŒ–æ•™å­¦åé¦ˆæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåœ¨çº¿æ•™è‚²åœºæ™¯ä¸‹çš„è‡ªåŠ¨åŒ–è¾…åŠ©ç³»ç»Ÿæä¾›äº†é‡è¦çš„å®è¯ä¾æ®å’Œè¯„ä¼°ä½“ç³»ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "This is the preprint version of a paper accepted at AIED 2025. The final version will be published by Springer",
      "pdf_url": "https://arxiv.org/pdf/2506.10829v1",
      "published_date": "2025-06-12 15:46:15 UTC",
      "updated_date": "2025-06-12 15:46:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:17:08.096362+00:00"
    },
    {
      "arxiv_id": "2506.10825v1",
      "title": "Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches",
      "title_zh": "åŒ»å­¦å›¾åƒåˆ†å‰²é€šç”¨æ¨¡å‹ï¼šç»¼è¿°åŠå…¶ä¸ç‰¹å®šä»»åŠ¡æ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”",
      "authors": [
        "Andrea Moglia",
        "Matteo Leccardi",
        "Matteo Cavicchioli",
        "Alice Maccarini",
        "Marco Marcon",
        "Luca Mainardi",
        "Pietro Cerveri"
      ],
      "abstract": "Following the successful paradigm shift of large language models, leveraging pre-training on a massive corpus of data and fine-tuning on different downstream tasks, generalist models have made their foray into computer vision. The introduction of Segment Anything Model (SAM) set a milestone on segmentation of natural images, inspiring the design of a multitude of architectures for medical image segmentation. In this survey we offer a comprehensive and in-depth investigation on generalist models for medical image segmentation. We start with an introduction on the fundamentals concepts underpinning their development. Then, we provide a taxonomy on the different declinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on the recent SAM 2, on other innovative models trained on images alone, and others trained on both text and images. We thoroughly analyze their performances at the level of both primary research and best-in-literature, followed by a rigorous comparison with the state-of-the-art task-specific models. We emphasize the need to address challenges in terms of compliance with regulatory frameworks, privacy and security laws, budget, and trustworthy artificial intelligence (AI). Finally, we share our perspective on future directions concerning synthetic data, early fusion, lessons learnt from generalist models in natural language processing, agentic AI and physical AI, and clinical translation.",
      "tldr_zh": "è¯¥ç»¼è¿°æ·±å…¥æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)èŒƒå¼è½¬ç§»ä¸‹ï¼ŒåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸé€šç”¨å‹æ¨¡å‹(Generalist Models)çš„å‘å±•ç°çŠ¶ã€‚ç ”ç©¶é¦–å…ˆä»‹ç»äº†é€šç”¨æ¨¡å‹çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶é’ˆå¯¹Segment Anything Model (SAM)åœ¨é›¶æ ·æœ¬(Zero-shot)ã€å°‘æ ·æœ¬(Few-shot)ã€å¾®è°ƒ(Fine-tuning)åŠé€‚é…å™¨(Adapters)ç­‰ä¸åŒæ¼”åŒ–è·¯å¾„ï¼Œä»¥åŠæœ€æ–°çš„SAM 2å’Œå…¶ä»–å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿåˆ†ç±»ã€‚é€šè¿‡å¯¹ç°æœ‰ç ”ç©¶è¡¨ç°çš„æ·±å…¥åˆ†æï¼Œè¯¥æ–‡å°†é€šç”¨æ¨¡å‹ä¸å½“å‰æœ€å…ˆè¿›çš„ç‰¹å®šä»»åŠ¡æ¨¡å‹(Task-specific models)è¿›è¡Œäº†ä¸¥æ ¼çš„æ€§èƒ½å¯¹æ¯”ã€‚ç»¼è¿°è¿›ä¸€æ­¥å¼ºè°ƒäº†åœ¨åˆè§„æ€§ã€éšç§å®‰å…¨ä»¥åŠå¯ä¿¡äººå·¥æ™ºèƒ½(Trustworthy AI)ç­‰æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚æœ€åï¼Œä½œè€…å¯¹åˆæˆæ•°æ®(Synthetic data)ã€æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½(Agentic AI)åŠä¸´åºŠè½¬åŒ–ç­‰æœªæ¥å‘å±•æ–¹å‘æå‡ºäº†å‰ç»æ€§è§è§£ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "132 pages, 26 figures, 23 tables. Andrea Moglia and Matteo Leccardi are equally contributing authors",
      "pdf_url": "https://arxiv.org/pdf/2506.10825v1",
      "published_date": "2025-06-12 15:44:49 UTC",
      "updated_date": "2025-06-12 15:44:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:17:11.461525+00:00"
    },
    {
      "arxiv_id": "2506.10821v6",
      "title": "VideoExplorer: Think With Videos For Agentic Long-Video Understanding",
      "title_zh": "VideoExplorerï¼šé¢å‘æ™ºèƒ½ä½“é•¿è§†é¢‘ç†è§£çš„è§†é¢‘åŒ–æ€ç»´",
      "authors": [
        "Huaying Yuan",
        "Zheng Liu",
        "Junjie Zhou",
        "Hongjin Qian",
        "Yan Shu",
        "Nicu Sebe",
        "Ji-Rong Wen",
        "Zhicheng Dou"
      ],
      "abstract": "Long-video understanding~(LVU) is a challenging problem in computer vision. Existing methods either downsample frames for single-pass reasoning, sacrificing fine-grained details, or depend on textual reasoning over task-agnostic representations, hindering task-specific perception and exploration. In this paper, we propose VideoExplorer, a framework grounded in the principle of ``thinking with video'', which naturally intertwines planning, temporal grounding, and scalable perception into a coherent reasoning process. Rather than reasoning over a static context, VideoExplorer iteratively formulates sub-questions, locates relevant moments, and performs task-oriented, temporally scalable video understanding until reaching the final answer, enabling faithful, efficient, and interpretable reasoning. To address the lack of LVU training resources, we construct a long-video reasoning dataset using difficulty-adaptive sampling to ensure high-quality trajectories on complex tasks. Building on this dataset, we design a two-stage training pipeline: supervised trajectory initialization followed by trajectory-level preference optimization, encouraging adaptive temporal grounding and iterative information integration guided by downstream rewards. Extensive evaluations on popular long-video understanding and reasoning benchmarks demonstrate VideoExplorer's significant advantage over existing baselines, highlighting its robustness, adaptability, and efficiency. Our code is made publicly available in this repository(https://github.com/yhy-2000/VideoDeepResearch).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿è§†é¢‘ç†è§£(Long-video understanding, LVU)ä¸­ç°æœ‰æ–¹æ³•å› å¸§ä¸‹é‡‡æ ·å¯¼è‡´ç»†èŠ‚æŸå¤±æˆ–å› ç¼ºä¹ä»»åŠ¡ç‰¹å®šæ„ŸçŸ¥è€Œå—é™çš„é—®é¢˜ï¼Œæå‡ºäº†VideoExploreræ¡†æ¶ã€‚è¯¥æ¡†æ¶éµå¾ªâ€œä¸è§†é¢‘å…±åŒæ€è€ƒâ€(thinking with video)çš„åŸåˆ™ï¼Œå°†è§„åˆ’ã€æ—¶é—´å®šä½(temporal grounding)ä¸å¯æ‰©å±•æ„ŸçŸ¥æ•´åˆè¿›ä¸€è‡´çš„æ¨ç†è¿‡ç¨‹ä¸­ã€‚VideoExploreré€šè¿‡è¿­ä»£å¼åœ°æå‡ºå­é—®é¢˜ã€å®šä½å…³é”®æ—¶åˆ»å¹¶æ‰§è¡Œé¢å‘ä»»åŠ¡çš„è§†é¢‘ç†è§£ï¼Œå®ç°äº†å¿ å®ã€é«˜æ•ˆä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„æ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥è®­ç»ƒèµ„æºçš„ä¸è¶³ï¼Œç ”ç©¶è€…åˆ©ç”¨éš¾åº¦è‡ªé€‚åº”é‡‡æ ·(difficulty-adaptive sampling)æ„å»ºäº†é«˜è´¨é‡çš„é•¿è§†é¢‘æ¨ç†æ•°æ®é›†ã€‚é€šè¿‡ç›‘ç£è½¨è¿¹åˆå§‹åŒ–(supervised trajectory initialization)å’Œè½¨è¿¹çº§åå¥½ä¼˜åŒ–(trajectory-level preference optimization)çš„ä¸¤é˜¶æ®µè®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸‹æ¸¸å¥–åŠ±è¿›è¡Œè‡ªé€‚åº”çš„æ—¶é—´å®šä½å’Œä¿¡æ¯æ•´åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoExploreråœ¨å¤šé¡¹é•¿è§†é¢‘ç†è§£ä¸æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œå±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€é€‚åº”æ€§å’Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10821v6",
      "published_date": "2025-06-12 15:39:10 UTC",
      "updated_date": "2025-11-01 09:37:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:18:21.380369+00:00"
    },
    {
      "arxiv_id": "2506.10785v1",
      "title": "What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps",
      "title_zh": "ç”¨æˆ·çœ‹é‡ä¸æ‰¹è¯„ä»€ä¹ˆï¼šAIé©±åŠ¨ç§»åŠ¨åº”ç”¨ç”¨æˆ·åé¦ˆçš„å¤§è§„æ¨¡åˆ†æ",
      "authors": [
        "Vinaik Chhetri",
        "Krishna Upadhyay",
        "A. B. Siddique",
        "Umar Farooq"
      ],
      "abstract": "Artificial Intelligence (AI)-powered features have rapidly proliferated across mobile apps in various domains, including productivity, education, entertainment, and creativity. However, how users perceive, evaluate, and critique these AI features remains largely unexplored, primarily due to the overwhelming volume of user feedback. In this work, we present the first comprehensive, large-scale study of user feedback on AI-powered mobile apps, leveraging a curated dataset of 292 AI-driven apps across 14 categories with 894K AI-specific reviews from Google Play. We develop and validate a multi-stage analysis pipeline that begins with a human-labeled benchmark and systematically evaluates large language models (LLMs) and prompting strategies. Each stage, including review classification, aspect-sentiment extraction, and clustering, is validated for accuracy and consistency. Our pipeline enables scalable, high-precision analysis of user feedback, extracting over one million aspect-sentiment pairs clustered into 18 positive and 15 negative user topics. Our analysis reveals that users consistently focus on a narrow set of themes: positive comments emphasize productivity, reliability, and personalized assistance, while negative feedback highlights technical failures (e.g., scanning and recognition), pricing concerns, and limitations in language support. Our pipeline surfaces both satisfaction with one feature and frustration with another within the same review. These fine-grained, co-occurring sentiments are often missed by traditional approaches that treat positive and negative feedback in isolation or rely on coarse-grained analysis. To this end, our approach provides a more faithful reflection of the real-world user experiences with AI-powered apps. Category-aware analysis further uncovers both universal drivers of satisfaction and domain-specific frustrations.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹AI-poweredç§»åŠ¨åº”ç”¨çš„ç”¨æˆ·åé¦ˆè¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡çš„ç»¼åˆåˆ†æï¼Œæ¶µç›–äº†14ä¸ªç±»åˆ«ã€292ä¸ªåº”ç”¨ä»¥åŠæ¥è‡ªGoogle Playçš„894Kæ¡ç‰¹å®šè¯„è®ºã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘å¹¶éªŒè¯äº†ä¸€ä¸ªå¤šé˜¶æ®µåˆ†ææµæ°´çº¿(multi-stage analysis pipeline)ï¼Œç»“åˆäººå·¥æ ‡æ³¨åŸºå‡†ä¸å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åŠå…¶æç¤ºç­–ç•¥ï¼Œå®ç°äº†å¯¹ç”¨æˆ·åé¦ˆçš„é«˜ç²¾åº¦ã€å¯æ‰©å±•åˆ†æã€‚è¯¥æµæ°´çº¿æ¶µç›–è¯„è®ºåˆ†ç±»ã€æ–¹é¢-æƒ…æ„Ÿæå–(aspect-sentiment extraction)ä»¥åŠèšç±»åˆ†æï¼Œä»ä¸­æå–äº†è¶…è¿‡ä¸€ç™¾ä¸‡ä¸ªæ–¹é¢-æƒ…æ„Ÿå¯¹ï¼Œå¹¶å½’çº³å‡º18ä¸ªæ­£é¢å’Œ15ä¸ªè´Ÿé¢ç”¨æˆ·ä¸»é¢˜ã€‚åˆ†ææ˜¾ç¤ºï¼Œç”¨æˆ·çš„æ­£é¢åé¦ˆä¸»è¦é›†ä¸­åœ¨ç”Ÿäº§åŠ›(productivity)ã€å¯é æ€§å’Œä¸ªæ€§åŒ–è¾…åŠ©(personalized assistance)ï¼Œè€Œè´Ÿé¢åé¦ˆåˆ™çªå‡ºäº†æŠ€æœ¯æ•…éšœï¼ˆå¦‚æ‰«æå’Œè¯†åˆ«(scanning and recognition)ï¼‰ã€å®šä»·ç–‘è™‘ä»¥åŠè¯­è¨€æ”¯æŒçš„å±€é™æ€§ã€‚ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•ï¼Œè¯¥ç ”ç©¶çš„æ–¹æ³•èƒ½æœ‰æ•ˆæ•æ‰åŒä¸€è¯„è®ºä¸­ç»†ç²’åº¦ä¸”å…±å­˜çš„æ­£è´Ÿæƒ…æ„Ÿï¼Œæ›´çœŸå®åœ°åæ˜ äº†çœŸå®ä¸–ç•Œä¸­ç”¨æˆ·å¯¹AIåº”ç”¨çš„å¤æ‚ä½“éªŒã€‚æœ€åï¼Œé€šè¿‡ç±»åˆ«æ„ŸçŸ¥åˆ†æï¼Œç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†é€šç”¨çš„æ»¡æ„åº¦é©±åŠ¨å› ç´ ä»¥åŠç‰¹å®šé¢†åŸŸçš„ç—›ç‚¹ï¼Œä¸ºAIåŠŸèƒ½çš„ä¼˜åŒ–æä¾›äº†å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "12 pages, 6 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.10785v1",
      "published_date": "2025-06-12 14:56:52 UTC",
      "updated_date": "2025-06-12 14:56:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:17:34.676678+00:00"
    },
    {
      "arxiv_id": "2506.10779v1",
      "title": "Improving Named Entity Transcription with Contextual LLM-based Revision",
      "title_zh": "åˆ©ç”¨åŸºäºä¸Šä¸‹æ–‡çš„ LLM ä¿®è®¢æå‡å‘½åå®ä½“è½¬å†™æ€§èƒ½",
      "authors": [
        "Viet Anh Trinh",
        "Xinlu He",
        "Jacob Whitehill"
      ],
      "abstract": "With recent advances in modeling and the increasing amount of supervised training data, automatic speech recognition (ASR) systems have achieved remarkable performance on general speech. However, the word error rate (WER) of state-of-the-art ASR remains high for named entities. Since named entities are often the most critical keywords, misrecognizing them can affect all downstream applications, especially when the ASR system functions as the front end of a complex system. In this paper, we introduce a large language model (LLM) revision mechanism to revise incorrect named entities in ASR predictions by leveraging the LLM's reasoning ability as well as local context (e.g., lecture notes) containing a set of correct named entities. Finally, we introduce the NER-MIT-OpenCourseWare dataset, containing 45 hours of data from MIT courses for development and testing. On this dataset, our proposed technique achieves up to 30\\% relative WER reduction for named entities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ç³»ç»Ÿåœ¨å¤„ç†å‘½åå®ä½“(Named Entity)æ—¶è¯é”™è¯¯ç‡(WER)å±…é«˜ä¸ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„ä¿®æ­£æœºåˆ¶ã€‚è¯¥æœºåˆ¶å……åˆ†åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ç»“åˆåŒ…å«æ­£ç¡®å®ä½“åˆ—è¡¨çš„å±€éƒ¨ä¸Šä¸‹æ–‡(Local Contextï¼Œå¦‚è¯¾ç¨‹è®²ä¹‰)ï¼Œå¯¹ASRé¢„æµ‹ç»“æœä¸­çš„å‘½åå®ä½“é”™è¯¯è¿›è¡Œé’ˆå¯¹æ€§ä¿®è®¢ã€‚ä¸ºäº†è¯„ä¼°è¯¥æ–¹æ³•ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†åŒ…å«45å°æ—¶éº»çœç†å·¥å­¦é™¢è¯¾ç¨‹å½•éŸ³çš„NER-MIT-OpenCourseWareæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æŠ€æœ¯åœ¨å‘½åå®ä½“è¯†åˆ«ä¸Šå®ç°äº†é«˜è¾¾30%çš„ç›¸å¯¹WERé™å¹…ã€‚è¿™ä¸€è¿›å±•æ˜¾è‘—æå‡äº†ASRç³»ç»Ÿåœ¨ä½œä¸ºå¤æ‚ä»»åŠ¡å‰ç«¯æ—¶çš„å…³é”®è¯è½¬å½•å‡†ç¡®æ€§ï¼Œä¸ºæé«˜ä¸‹æ¸¸åº”ç”¨çš„æ•´ä½“æ€§èƒ½æä¾›äº†é‡è¦ä¿éšœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10779v1",
      "published_date": "2025-06-12 14:53:48 UTC",
      "updated_date": "2025-06-12 14:53:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:17:26.875449+00:00"
    },
    {
      "arxiv_id": "2506.10778v1",
      "title": "SlotPi: Physics-informed Object-centric Reasoning Models",
      "title_zh": "SlotPiï¼šèåˆç‰©ç†ä¿¡æ¯çš„ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒæ¨ç†æ¨¡å‹",
      "authors": [
        "Jian Li",
        "Wan Han",
        "Ning Lin",
        "Yu-Liang Zhan",
        "Ruizhi Chengze",
        "Haining Wang",
        "Yi Zhang",
        "Hongsheng Liu",
        "Zidong Wang",
        "Fan Yu",
        "Hao Sun"
      ],
      "abstract": "Understanding and reasoning about dynamics governed by physical laws through visual observation, akin to human capabilities in the real world, poses significant challenges. Currently, object-centric dynamic simulation methods, which emulate human behavior, have achieved notable progress but overlook two critical aspects: 1) the integration of physical knowledge into models. Humans gain physical insights by observing the world and apply this knowledge to accurately reason about various dynamic scenarios; 2) the validation of model adaptability across diverse scenarios. Real-world dynamics, especially those involving fluids and objects, demand models that not only capture object interactions but also simulate fluid flow characteristics. To address these gaps, we introduce SlotPi, a slot-based physics-informed object-centric reasoning model. SlotPi integrates a physical module based on Hamiltonian principles with a spatio-temporal prediction module for dynamic forecasting. Our experiments highlight the model's strengths in tasks such as prediction and Visual Question Answering (VQA) on benchmark and fluid datasets. Furthermore, we have created a real-world dataset encompassing object interactions, fluid dynamics, and fluid-object interactions, on which we validated our model's capabilities. The model's robust performance across all datasets underscores its strong adaptability, laying a foundation for developing more advanced world models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SlotPiï¼Œè¿™æ˜¯ä¸€ç§åŸºäº slot çš„ç‰©ç†å¢å¼ºå‹å¯¹è±¡ä¸­å¿ƒ (object-centric) æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŠ¨åŠ›å­¦æ¨¡æ‹Ÿæ–¹æ³•åœ¨ç‰©ç†çŸ¥è¯†é›†æˆåŠå¤šåœºæ™¯ï¼ˆå°¤å…¶æ˜¯æµä½“ï¼‰é€‚åº”æ€§æ–¹é¢çš„å±€é™ã€‚SlotPi åˆ›æ–°æ€§åœ°å°†åŸºäº Hamiltonian principles çš„ç‰©ç†æ¨¡å—ä¸ spatio-temporal prediction æ¨¡å—ç›¸ç»“åˆï¼Œå®ç°äº†å¯¹å¤æ‚åŠ¨åŠ›å­¦åœºæ™¯çš„ç²¾ç¡®é¢„æµ‹ä¸æ¨ç†ã€‚å®éªŒåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ã€æµä½“æ•°æ®é›†ä»¥åŠç ”ç©¶è€…æ–°æ„å»ºçš„åŒ…å«ç‰©ä½“äº¤äº’ä¸æµä½“åŠ¨åŠ›å­¦çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå±•å¼€ï¼Œæ¶µç›–äº†é¢„æµ‹å’Œ Visual Question Answering (VQA) ç­‰å…³é”®ä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼ŒSlotPi åœ¨æ•æ‰ç‰©ä½“é—´äº¤äº’åŠæ¨¡æ‹Ÿæµä½“æµåŠ¨ç‰¹æ€§æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ä¸é²æ£’æ€§ï¼Œå±•ç°äº†æé«˜çš„åœºæ™¯é€‚åº”èƒ½åŠ›ã€‚è¯¥æ¨¡å‹çš„æˆåŠŸç ”å‘ä¸ä»…éªŒè¯äº†ç‰©ç†ä¿¡æ¯åœ¨åŠ¨æ€æ¨ç†ä¸­çš„é‡è¦ä½œç”¨ï¼Œä¹Ÿä¸ºæœªæ¥å¼€å‘æ›´å…ˆè¿›çš„ world models å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10778v1",
      "published_date": "2025-06-12 14:53:36 UTC",
      "updated_date": "2025-06-12 14:53:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:17:27.378532+00:00"
    },
    {
      "arxiv_id": "2506.10776v2",
      "title": "ME: Trigger Element Combination Backdoor Attack on Copyright Infringement",
      "title_zh": "MEï¼šé’ˆå¯¹ç‰ˆæƒä¾µæƒçš„è§¦å‘å…ƒç´ ç»„åˆåé—¨æ”»å‡»",
      "authors": [
        "Feiyu Yang",
        "Siyuan Liang",
        "Aishan Liu",
        "Dacheng Tao"
      ],
      "abstract": "The capability of generative diffusion models (DMs) like Stable Diffusion (SD) in replicating training data could be taken advantage of by attackers to launch the Copyright Infringement Attack, with duplicated poisoned image-text pairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew outstanding performance in attacking SD in text-to-image tasks. However, the feasible data resources in this area are still limited, some of them are even constrained or prohibited due to the issues like copyright ownership or inappropriate contents; And not all of the images in current datasets are suitable for the proposed attacking methods; Besides, the state-of-the-art (SoTA) performance of SBD is far from ideal when few generated poisoning samples could be adopted for attacks. In this paper, we raised new datasets accessible for researching in attacks like SBD, and proposed Multi-Element (ME) attack method based on SBD by increasing the number of poisonous visual-text elements per poisoned sample to enhance the ability of attacking, while importing Discrete Cosine Transform (DCT) for the poisoned samples to maintain the stealthiness. The Copyright Infringement Rate (CIR) / First Attack Epoch (FAE) we got on the two new datasets were 16.78% / 39.50 and 51.20% / 23.60, respectively close to or even outperformed benchmark Pokemon and Mijourney datasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI and DCT earned CIR / FAE of 0.23% / 84.00 and 12.73% / 65.50, both better than original SBD, which failed to attack at all.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹(Diffusion Models)åœ¨ç‰ˆæƒä¾µæƒæ”»å‡»(Copyright Infringement Attack)ä¸­é¢ä¸´çš„æ•°æ®èµ„æºæœ‰é™ä»¥åŠåœ¨å°‘é‡ä¸­æ¯’æ ·æœ¬ä¸‹æ”»å‡»æ€§èƒ½ä¸è¶³çš„é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ä½œè€…åŸºäºSilentBadDiffusion (SBD)æå‡ºäº†ä¸€ç§åä¸ºMulti-Element (ME)çš„æ”¹è¿›æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡å¢åŠ æ¯ä¸ªä¸­æ¯’æ ·æœ¬ä¸­è§†è§‰-æ–‡æœ¬å…ƒç´ çš„ç»„åˆæ•°é‡æ¥æ˜¾è‘—å¢å¼ºæ”»å‡»æ•ˆåŠ›ã€‚ä¸ºäº†åœ¨æå‡æ”»å‡»å¼ºåº¦çš„åŒæ—¶ä¿æŒéšè”½æ€§(Stealthiness)ï¼Œè¯¥æ–¹æ³•è¿˜å¼•å…¥äº†ç¦»æ•£ä½™å¼¦å˜æ¢(Discrete Cosine Transform, DCT)æŠ€æœ¯å¯¹ä¸­æ¯’æ ·æœ¬è¿›è¡Œå¤„ç†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å‘å¸ƒäº†ä¸¤ä¸ªå…¨æ–°çš„æ•°æ®é›†ï¼Œä¸ºç‰ˆæƒä¾µæƒæ”»å‡»ç›¸å…³ç ”ç©¶æä¾›äº†æ›´å¤šå¯è¡Œçš„å®éªŒèµ„æºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMEæ–¹æ³•åœ¨ä¸¤ä¸ªæ–°æ•°æ®é›†ä¸Šçš„ç‰ˆæƒä¾µæƒç‡(Copyright Infringement Rate, CIR)å’Œé¦–ä¸ªæ”»å‡»è½®æ¬¡(First Attack Epoch, FAE)è¡¨ç°ä¼˜å¼‚ï¼Œæ¥è¿‘æˆ–ä¼˜äºMidjourneyç­‰åŸºå‡†æ•°æ®é›†ã€‚åœ¨ä»…æœ‰5%å­é‡‡æ ·ç‡ï¼ˆå³6ä¸ªä¸­æ¯’æ ·æœ¬ï¼‰çš„æç«¯æ¡ä»¶ä¸‹ï¼ŒMEæ–¹æ³•åŠå…¶å˜ä½“ä¾ç„¶èƒ½å®ç°æœ‰æ•ˆçš„åé—¨æ”»å‡»ï¼Œè€ŒåŸå§‹çš„SBDæ–¹æ³•åœ¨æ­¤æ¡ä»¶ä¸‹åˆ™å®Œå…¨å¤±æ•ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Finding unfinished issue in this work , still refining",
      "pdf_url": "https://arxiv.org/pdf/2506.10776v2",
      "published_date": "2025-06-12 14:51:27 UTC",
      "updated_date": "2025-10-26 03:13:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:17:41.670300+00:00"
    },
    {
      "arxiv_id": "2506.10774v1",
      "title": "Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary Ultra-Large Scales",
      "title_zh": "åŸºäºç¬”ç”»çš„å¾ªç¯æ”¾å¤§å™¨ï¼šä»»æ„è¶…å¤§å€ç‡å›¾åƒè¶…åˆ†è¾¨ç‡",
      "authors": [
        "Wenhao Guo",
        "Peng Lu",
        "Xujun Peng",
        "Zhaoran Zhao",
        "Sheng Li"
      ],
      "abstract": "Prior Arbitrary-Scale Image Super-Resolution (ASISR) methods often experience a significant performance decline when the upsampling factor exceeds the range covered by the training data, introducing substantial blurring. To address this issue, we propose a unified model, Stroke-based Cyclic Amplifier (SbCA), for ultra-large upsampling tasks. The key of SbCA is the stroke vector amplifier, which decomposes the image into a series of strokes represented as vector graphics for magnification. Then, the detail completion module also restores missing details, ensuring high-fidelity image reconstruction. Our cyclic strategy achieves ultra-large upsampling by iteratively refining details with this unified SbCA model, trained only once for all, while keeping sub-scales within the training range. Our approach effectively addresses the distribution drift issue and eliminates artifacts, noise and blurring, producing high-quality, high-resolution super-resolved images. Experimental validations on both synthetic and real-world datasets demonstrate that our approach significantly outperforms existing methods in ultra-large upsampling tasks (e.g. $\\times100$), delivering visual quality far superior to state-of-the-art techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Stroke-based Cyclic Amplifier (SbCA)ï¼Œä¸€ç§æ—¨åœ¨è§£å†³ä»»æ„å°ºåº¦å›¾åƒè¶…åˆ†è¾¨ç‡ (Arbitrary-Scale Image Super-Resolution, ASISR) åœ¨è¶…å¤§è§„æ¨¡æ”¾å¤§ä¸‹æ€§èƒ½ä¸‹é™åŠæ¨¡ç³Šé—®é¢˜çš„ç»Ÿä¸€æ¨¡å‹ã€‚SbCA çš„æ ¸å¿ƒç»„ä»¶æ˜¯ stroke vector amplifierï¼Œå®ƒå°†å›¾åƒåˆ†è§£ä¸ºä¸€ç³»åˆ—ä»¥çŸ¢é‡å›¾å½¢è¡¨ç¤ºçš„ç¬”ç”»è¿›è¡Œæ”¾å¤§ï¼Œå¹¶åˆ©ç”¨ detail completion æ¨¡å—æ¢å¤ç¼ºå¤±çš„ç»†èŠ‚ï¼Œä»è€Œç¡®ä¿é«˜ä¿çœŸåº¦çš„å›¾åƒé‡å»ºã€‚è¯¥ç ”ç©¶é€šè¿‡ cyclic strategy ç­–ç•¥ï¼Œåˆ©ç”¨ç»Ÿä¸€çš„ SbCA æ¨¡å‹å¯¹ç»†èŠ‚è¿›è¡Œè¿­ä»£ç»†åŒ–ï¼Œå®ç°äº†è¶…å¤§è§„æ¨¡ä¸Šé‡‡æ ·ï¼ŒåŒæ—¶å°†å­å°ºåº¦ä¿æŒåœ¨è®­ç»ƒèŒƒå›´å†…ï¼Œæœ‰æ•ˆè§£å†³äº†åˆ†å¸ƒåç§» (distribution drift) é—®é¢˜å¹¶æ¶ˆé™¤äº†ä¼ªå½±ã€å™ªå£°å’Œæ¨¡ç³Šã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ $\\times100$ å€ç­‰è¶…å¤§è§„æ¨¡ä¸Šé‡‡æ ·ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡å±•ç°äº†è¿œè¶…å½“å‰æœ€å…ˆè¿›æŠ€æœ¯çš„è§†è§‰è´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10774v1",
      "published_date": "2025-06-12 14:51:10 UTC",
      "updated_date": "2025-06-12 14:51:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:18:34.929219+00:00"
    },
    {
      "arxiv_id": "2506.10773v2",
      "title": "Learning Chaotic Dynamics with Neuromorphic Network Dynamics",
      "title_zh": "åˆ©ç”¨ç¥ç»å½¢æ€ç½‘ç»œåŠ¨åŠ›å­¦å­¦ä¹ æ··æ²ŒåŠ¨åŠ›å­¦",
      "authors": [
        "Yinhao Xu",
        "Georg A. Gottwald",
        "Zdenka Kuncic"
      ],
      "abstract": "This study investigates how dynamical systems may be learned and modelled with a neuromorphic network which is itself a dynamical system. The neuromorphic network used in this study is based on a complex electrical circuit comprised of memristive elements that produce neuro-synaptic nonlinear responses to input electrical signals. To determine how computation may be performed using the physics of the underlying system, the neuromorphic network was simulated and evaluated on autonomous prediction of a multivariate chaotic time series, implemented with a reservoir computing framework. Through manipulating only input electrodes and voltages, optimal nonlinear dynamical responses were found when input voltages maximise the number of memristive components whose internal dynamics explore the entire dynamical range of the memristor model. Increasing the network coverage with the input electrodes was found to suppress other nonlinear responses that are less conducive to learning. These results provide valuable insights into how a physical neuromorphic network device can be feasibly optimised for learning complex dynamical systems using only external control parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æœ¬èº«ä½œä¸ºåŠ¨åŠ›ç³»ç»Ÿçš„ç¥ç»å½¢æ€ç½‘ç»œ(Neuromorphic Network)æ¥å­¦ä¹ å’Œå»ºæ¨¡å¤æ‚çš„åŠ¨åŠ›ç³»ç»Ÿã€‚è¯¥ç½‘ç»œåŸºäºç”±å¿†é˜»å…ƒä»¶(Memristive elements)ç»„æˆçš„å¤æ‚ç”µè·¯ï¼Œåˆ©ç”¨å…¶äº§ç”Ÿçš„ç¥ç»çªè§¦éçº¿æ€§å“åº”ï¼Œåœ¨æ°´åº“è®¡ç®—(Reservoir Computing)æ¡†æ¶ä¸‹å®ç°äº†å¯¹å¤šå˜é‡æ··æ²Œæ—¶é—´åºåˆ—(Multivariate chaotic time series)çš„è‡ªä¸»é¢„æµ‹ã€‚é€šè¿‡æ“çºµè¾“å…¥ç”µæå’Œç”µå‹ï¼Œç ”ç©¶å‘ç°å½“è¾“å…¥ç”µå‹ä½¿æœ€å¤§æ•°é‡çš„å¿†é˜»ç»„ä»¶å……åˆ†æ¢ç´¢å…¶æ¨¡å‹åŠ¨åŠ›å­¦èŒƒå›´æ—¶ï¼Œç½‘ç»œèƒ½äº§ç”Ÿæœ€ä¼˜çš„éçº¿æ€§åŠ¨åŠ›å­¦å“åº”ã€‚æ­¤å¤–ï¼Œå¢åŠ è¾“å…¥ç”µæçš„ç½‘ç»œè¦†ç›–èŒƒå›´æœ‰åŠ©äºæŠ‘åˆ¶ä¸åˆ©äºå­¦ä¹ çš„å…¶ä»–éçº¿æ€§å“åº”ã€‚è¯¥æˆæœä¸ºä»…é€šè¿‡å¤–éƒ¨æ§åˆ¶å‚æ•°ä¼˜åŒ–ç‰©ç†ç¥ç»å½¢æ€ç½‘ç»œè®¾å¤‡ï¼Œä»¥å®ç°å¯¹å¤æ‚åŠ¨åŠ›ç³»ç»Ÿçš„æœ‰æ•ˆå­¦ä¹ æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cond-mat.dis-nn",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cond-mat.dis-nn",
      "comment": "42 pages, 24 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.10773v2",
      "published_date": "2025-06-12 14:50:55 UTC",
      "updated_date": "2025-09-14 08:24:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:18:50.287408+00:00"
    },
    {
      "arxiv_id": "2506.10764v1",
      "title": "OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems",
      "title_zh": "OPT-BENCHï¼šé¢å‘å¤§è§„æ¨¡æœç´¢ç©ºé—´ä¼˜åŒ–é—®é¢˜çš„ LLM æ™ºèƒ½ä½“è¯„ä¼°",
      "authors": [
        "Xiaozhe Li",
        "Jixuan Chen",
        "Xinyu Fang",
        "Shengyuan Ding",
        "Haodong Duan",
        "Qingwen Liu",
        "Kai Chen"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in solving diverse tasks. However, their proficiency in iteratively optimizing complex solutions through learning from previous feedback remains insufficiently explored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark designed to evaluate LLM agents on large-scale search space optimization problems. OPT-BENCH includes 20 real-world machine learning tasks sourced from Kaggle and 10 classical NP problems, offering a diverse and challenging environment for assessing LLM agents on iterative reasoning and solution refinement. To enable rigorous evaluation, we introduce OPT-Agent, an end-to-end optimization framework that emulates human reasoning when tackling complex problems by generating, validating, and iteratively improving solutions through leveraging historical feedback. Through extensive experiments on 9 state-of-the-art LLMs from 6 model families, we analyze the effects of optimization iterations, temperature settings, and model architectures on solution quality and convergence. Our results demonstrate that incorporating historical context significantly enhances optimization performance across both ML and NP tasks. All datasets, code, and evaluation tools are open-sourced to promote further research in advancing LLM-driven optimization and iterative reasoning. Project page: \\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OPT-BENCHï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼° LLM agent åœ¨å¤§è§„æ¨¡æœç´¢ç©ºé—´ä¼˜åŒ– (search space optimization) é—®é¢˜ä¸Šèƒ½åŠ›çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ã€‚OPT-BENCH æ¶µç›–äº†æ¥è‡ª Kaggle çš„ 20 ä¸ªçœŸå®æœºå™¨å­¦ä¹ ä»»åŠ¡ä»¥åŠ 10 ä¸ªç»å…¸çš„ NP problemsï¼Œä¸ºè¯„ä¼° LLM agent çš„è¿­ä»£æ¨ç†å’Œæ–¹æ¡ˆå®Œå–„æä¾›äº†å¤šæ ·åŒ–ä¸”å…·æŒ‘æˆ˜æ€§çš„ç¯å¢ƒã€‚ä¸ºäº†å®ç°ä¸¥è°¨çš„è¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†åä¸º OPT-Agent çš„ç«¯åˆ°ç«¯ä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆã€éªŒè¯å¹¶åˆ©ç”¨å†å²åé¦ˆ (historical feedback) è¿­ä»£æ”¹è¿›æ–¹æ¡ˆï¼Œæ¨¡æ‹Ÿäº†äººç±»å¤„ç†å¤æ‚é—®é¢˜çš„é€»è¾‘ã€‚é€šè¿‡å¯¹ 6 ä¸ªæ¨¡å‹å®¶æ—çš„ 9 ç§æœ€å…ˆè¿› LLMs è¿›è¡Œå¹¿æ³›å®éªŒï¼Œç ”ç©¶åˆ†æäº†ä¼˜åŒ–è¿­ä»£ (optimization iterations)ã€æ¸©åº¦è®¾ç½® (temperature settings) å’Œæ¨¡å‹æ¶æ„å¯¹è§£çš„è´¨é‡åŠæ”¶æ•›æ€§çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥å†å²ä¸Šä¸‹æ–‡ (historical context) èƒ½æ˜¾è‘—æå‡ LLM åœ¨æœºå™¨å­¦ä¹ å’Œ NP ä»»åŠ¡ä¸­çš„ä¼˜åŒ–è¡¨ç°ã€‚è¯¥é¡¹ç›®å·²å°†æ‰€æœ‰æ•°æ®é›†ã€ä»£ç å’Œè¯„ä¼°å·¥å…·å¼€æºï¼Œæ—¨åœ¨æ¨åŠ¨ LLM é©±åŠ¨çš„ä¼˜åŒ–å’Œè¿­ä»£æ¨ç†é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10764v1",
      "published_date": "2025-06-12 14:46:41 UTC",
      "updated_date": "2025-06-12 14:46:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:18:47.967728+00:00"
    },
    {
      "arxiv_id": "2506.10756v1",
      "title": "Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding",
      "title_zh": "å…·å¤‡å¼€æ”¾è¯æ±‡ç›®æ ‡ç†è§£èƒ½åŠ›çš„æ— äººæœºå…·èº«è§†è§‰è¯­è¨€å¯¼èˆª",
      "authors": [
        "Yuhang Zhang",
        "Haosheng Yu",
        "Jiaping Xiao",
        "Mir Feroskhan"
      ],
      "abstract": "Vision-and-language navigation (VLN) is a long-standing challenge in autonomous robotics, aiming to empower agents with the ability to follow human instructions while navigating complex environments. Two key bottlenecks remain in this field: generalization to out-of-distribution environments and reliance on fixed discrete action spaces. To address these challenges, we propose Vision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles (UAVs) to execute language-guided flight. Without the requirement for localization or active ranging sensors, VLFly outputs continuous velocity commands purely from egocentric observations captured by an onboard monocular camera. The VLFly integrates three modules: an instruction encoder based on a large language model (LLM) that reformulates high-level language into structured prompts, a goal retriever powered by a vision-language model (VLM) that matches these prompts to goal images via vision-language similarity, and a waypoint planner that generates executable trajectories for real-time UAV control. VLFly is evaluated across diverse simulation environments without additional fine-tuning and consistently outperforms all baselines. Moreover, real-world VLN tasks in indoor and outdoor environments under direct and indirect instructions demonstrate that VLFly achieves robust open-vocabulary goal understanding and generalized navigation capabilities, even in the presence of abstract language input.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Vision-Language Fly (VLFly)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ— äººæœº(UAV)åœ¨è§†è§‰è¯­è¨€å¯¼èˆª(VLN)ä»»åŠ¡ä¸­é¢ä¸´çš„ç¯å¢ƒæ³›åŒ–åŠåŠ¨ä½œç©ºé—´å—é™ç­‰ç“¶é¢ˆã€‚VLFlyä»…ä¾é è½¦è½½å•ç›®æ‘„åƒå¤´çš„è‡ªæˆ‘ä¸­å¿ƒè§‚æµ‹ï¼Œåœ¨æ— éœ€å®šä½æˆ–ä¸»åŠ¨æµ‹è·ä¼ æ„Ÿå™¨çš„æƒ…å†µä¸‹ç›´æ¥è¾“å‡ºè¿ç»­é€Ÿåº¦æŒ‡ä»¤ã€‚è¯¥ç³»ç»Ÿæ•´åˆäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æŒ‡ä»¤ç¼–ç å™¨ã€è§†è§‰è¯­è¨€æ¨¡å‹(VLM)é©±åŠ¨çš„ç›®æ ‡æ£€ç´¢å™¨ä»¥åŠè·¯å¾„ç‚¹è§„åˆ’å™¨(waypoint planner)ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€ç›¸ä¼¼åº¦å°†é«˜å±‚æŒ‡ä»¤è½¬åŒ–ä¸ºå…·ä½“çš„é£è¡Œè½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼ŒVLFlyåœ¨å¤šç§ä»¿çœŸç¯å¢ƒä¸‹æ— éœ€é¢å¤–å¾®è°ƒå³å¯æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨å®¤å†…å¤–çœŸå®åœºæ™¯ä¸­å±•ç°äº†å¼ºå¤§çš„å¼€æ”¾è¯æ±‡ç›®æ ‡ç†è§£(open-vocabulary goal understanding)èƒ½åŠ›ï¼Œå³ä½¿é¢å¯¹æŠ½è±¡æŒ‡ä»¤ä¹Ÿèƒ½å®ç°é²æ£’çš„æ³›åŒ–å¯¼èˆªã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10756v1",
      "published_date": "2025-06-12 14:40:50 UTC",
      "updated_date": "2025-06-12 14:40:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:18:48.844659+00:00"
    },
    {
      "arxiv_id": "2506.10754v2",
      "title": "BNMusic: Blending Environmental Noises into Personalized Music",
      "title_zh": "BNMusicï¼šå°†ç¯å¢ƒå™ªå£°èå…¥ä¸ªæ€§åŒ–éŸ³ä¹",
      "authors": [
        "Chi Zuo",
        "Martin B. MÃ¸ller",
        "Pablo MartÃ­nez-Nuevo",
        "Huayang Huang",
        "Yu Wu",
        "Ye Zhu"
      ],
      "abstract": "While being disturbed by environmental noises, the acoustic masking technique is a conventional way to reduce the annoyance in audio engineering that seeks to cover up the noises with other dominant yet less intrusive sounds. However, misalignment between the dominant sound and the noise-such as mismatched downbeats-often requires an excessive volume increase to achieve effective masking. Motivated by recent advances in cross-modal generation, in this work, we introduce an alternative method to acoustic masking, aiming to reduce the noticeability of environmental noises by blending them into personalized music generated based on user-provided text prompts. Following the paradigm of music generation using mel-spectrogram representations, we propose a Blending Noises into Personalized Music (BNMusic) framework with two key stages. The first stage synthesizes a complete piece of music in a mel-spectrogram representation that encapsulates the musical essence of the noise. In the second stage, we adaptively amplify the generated music segment to further reduce noise perception and enhance the blending effectiveness, while preserving auditory quality. Our experiments with comprehensive evaluations on MusicBench, EPIC-SOUNDS, and ESC-50 demonstrate the effectiveness of our framework, highlighting the ability to blend environmental noise with rhythmically aligned, adaptively amplified, and enjoyable music segments, minimizing the noticeability of the noise, thereby improving overall acoustic experiences. Project page: https://d-fas.github.io/BNMusic_page/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå£°å­¦æ©è”½(acoustic masking)æŠ€æœ¯å› é®è”½éŸ³ä¸å™ªå£°ä¸åŒ¹é…è€Œéœ€è¿‡åº¦æå‡éŸ³é‡çš„é—®é¢˜ï¼Œæå‡ºäº† BNMusic (Blending Noises into Personalized Music) æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è·¨æ¨¡æ€ç”ŸæˆæŠ€æœ¯ï¼Œæ ¹æ®ç”¨æˆ·æ–‡æœ¬æç¤ºç”Ÿæˆä¸ªæ€§åŒ–éŸ³ä¹å¹¶å°†å…¶ä¸ç¯å¢ƒå™ªå£°èåˆï¼Œæ—¨åœ¨é™ä½å™ªå£°çš„æ˜¾è‘—æ€§ã€‚BNMusic é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆåˆ©ç”¨æ¢…å°”é¢‘è°±å›¾(mel-spectrogram)è¡¨å¾åˆæˆä½“ç°å™ªå£°éŸ³ä¹æœ¬è´¨çš„å®Œæ•´ç‰‡æ®µï¼Œéšåé€šè¿‡è‡ªé€‚åº”æ”¾å¤§ç”Ÿæˆçš„éŸ³ä¹æ¥è¿›ä¸€æ­¥å‡å°‘å™ªå£°æ„ŸçŸ¥å¹¶ç»´æŒå¬è§‰è´¨é‡ã€‚åœ¨ MusicBenchã€EPIC-SOUNDS å’Œ ESC-50 ç­‰æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”ŸæˆèŠ‚å¥å¯¹é½ä¸”æ‚¦è€³çš„éŸ³ä¹ç‰‡æ®µã€‚è¯„ä¼°è¯æ˜ BNMusic èƒ½å¤Ÿæœ‰æ•ˆæœ€å°åŒ–ç¯å¢ƒå™ªå£°çš„å¯å¯Ÿè§‰æ€§ï¼Œæ˜¾è‘—æ”¹å–„äº†ç”¨æˆ·çš„æ•´ä½“å¬è§‰ä½“éªŒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "This paper has been accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10754v2",
      "published_date": "2025-06-12 14:39:08 UTC",
      "updated_date": "2025-10-28 14:11:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:18:48.271156+00:00"
    },
    {
      "arxiv_id": "2506.10753v1",
      "title": "Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering",
      "title_zh": "å…ˆæ€åæ¨¡ï¼šåˆ©ç”¨ç¬¦å·æ¨ç†ç¼–æ’ç¥ç»è®¡ç®—ä»¥å®ç°åäº‹å®é—®ç­”",
      "authors": [
        "Adam Ishay",
        "Zhun Yang",
        "Joohyung Lee",
        "Ilgu Kang",
        "Dongjae Lim"
      ],
      "abstract": "Causal and temporal reasoning about video dynamics is a challenging problem. While neuro-symbolic models that combine symbolic reasoning with neural-based perception and prediction have shown promise, they exhibit limitations, especially in answering counterfactual questions. This paper introduces a method to enhance a neuro-symbolic model for counterfactual reasoning, leveraging symbolic reasoning about causal relations among events. We define the notion of a causal graph to represent such relations and use Answer Set Programming (ASP), a declarative logic programming method, to find how to coordinate perception and simulation modules. We validate the effectiveness of our approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves state-of-the-art performance on the CLEVRER challenge, significantly outperforming existing models. In the case of the CRAFT benchmark, we leverage a large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a dynamics simulator. Our findings show that this method can further improve its performance on counterfactual questions by providing alternative prompts instructed by symbolic causal reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘åŠ¨åŠ›å­¦ä¸­çš„å› æœä¸æ—¶åºæ¨ç†æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡ç¬¦å·æ¨ç†åè°ƒç¥ç»è®¡ç®—çš„æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºç¥ç»ç¬¦å·æ¨¡å‹(neuro-symbolic model)åœ¨å¤„ç†åäº‹å®é—®ç­”(counterfactual question answering)ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä½œè€…å®šä¹‰äº†å› æœå›¾(causal graph)æ¥è¡¨ç¤ºäº‹ä»¶é—´çš„å› æœå…³ç³»ï¼Œå¹¶åˆ©ç”¨å£°æ˜å¼é€»è¾‘ç¼–ç¨‹æ–¹æ³•Answer Set Programming (ASP)æ¥åè°ƒæ„ŸçŸ¥æ¨¡å—ä¸æ¨¡æ‹Ÿæ¨¡å—ã€‚åœ¨CLEVRERåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œè¾¾åˆ°äº†ç›®å‰æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼Œåœ¨CRAFTåŸºå‡†æµ‹è¯•ä¸­ï¼Œç ”ç©¶è€…åˆ©ç”¨GPT-3.5å’ŒGPT-4ç­‰å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä½œä¸ºåŠ¨åŠ›å­¦æ¨¡æ‹Ÿå™¨çš„ä»£ç†ï¼Œè¯æ˜äº†é€šè¿‡ç¬¦å·å› æœæ¨ç†æŒ‡å¯¼çš„æç¤ºè¯(prompts)èƒ½å¤Ÿè¿›ä¸€æ­¥æå‡ç³»ç»Ÿåœ¨åäº‹å®é—®é¢˜ä¸Šçš„å‡†ç¡®æ€§ã€‚è¿™ç§ç»“åˆç¬¦å·æ¨ç†ä¸ç¥ç»è®¡ç®—çš„æ¡†æ¶ï¼Œä¸ºè§£å†³å¤æ‚è§†é¢‘åœºæ™¯ä¸‹çš„å› æœæ¨ç†æä¾›äº†ä¸€æ¡é«˜æ•ˆä¸”ç¨³å¥çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)",
      "pdf_url": "https://arxiv.org/pdf/2506.10753v1",
      "published_date": "2025-06-12 14:37:11 UTC",
      "updated_date": "2025-06-12 14:37:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:18:48.701957+00:00"
    },
    {
      "arxiv_id": "2506.10722v1",
      "title": "TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks",
      "title_zh": "TED-LaSTï¼šé¢å‘è‡ªé€‚åº”æ”»å‡»çš„é²æ£’åé—¨é˜²å¾¡",
      "authors": [
        "Xiaoxing Mo",
        "Yuxuan Cheng",
        "Nan Sun",
        "Leo Yu Zhang",
        "Wei Luo",
        "Shang Gao"
      ],
      "abstract": "Deep Neural Networks (DNNs) are vulnerable to backdoor attacks, where attackers implant hidden triggers during training to maliciously control model behavior. Topological Evolution Dynamics (TED) has recently emerged as a powerful tool for detecting backdoor attacks in DNNs. However, TED can be vulnerable to backdoor attacks that adaptively distort topological representation distributions across network layers. To address this limitation, we propose TED-LaST (Topological Evolution Dynamics against Laundry, Slow release, and Target mapping attack strategies), a novel defense strategy that enhances TED's robustness against adaptive attacks. TED-LaST introduces two key innovations: label-supervised dynamics tracking and adaptive layer emphasis. These enhancements enable the identification of stealthy threats that evade traditional TED-based defenses, even in cases of inseparability in topological space and subtle topological perturbations. We review and classify data poisoning tricks in state-of-the-art adaptive attacks and propose enhanced adaptive attack with target mapping, which can dynamically shift malicious tasks and fully leverage the stealthiness that adaptive attacks possess. Our comprehensive experiments on multiple datasets (CIFAR-10, GTSRB, and ImageNet100) and model architectures (ResNet20, ResNet101) show that TED-LaST effectively counteracts sophisticated backdoors like Adap-Blend, Adapt-Patch, and the proposed enhanced adaptive attack. TED-LaST sets a new benchmark for robust backdoor detection, substantially enhancing DNN security against evolving threats.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)æ˜“å—åé—¨æ”»å‡»çš„é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„æ‹“æ‰‘æ¼”åŒ–åŠ¨åŠ›å­¦(TED)åœ¨é¢å¯¹èƒ½å¤Ÿæ‰­æ›²å±‚é—´æ‹“æ‰‘è¡¨ç¤ºåˆ†å¸ƒçš„è‡ªé€‚åº”æ”»å‡»æ—¶å­˜åœ¨è„†å¼±æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†TED-LaSTï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºé˜²å¾¡é²æ£’æ€§çš„æ–°å‹ç­–ç•¥ï¼Œä¸“é—¨åº”å¯¹Laundryã€Slow releaseå’ŒTarget mappingç­‰è‡ªé€‚åº”æ”»å‡»æ‰‹æ®µã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ ‡ç­¾ç›‘ç£åŠ¨åŠ›å­¦è¿½è¸ª(label-supervised dynamics tracking)å’Œè‡ªé€‚åº”å±‚å¼ºè°ƒ(adaptive layer emphasis)ä¸¤å¤§æ ¸å¿ƒåˆ›æ–°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«åœ¨æ‹“æ‰‘ç©ºé—´ä¸­éš¾ä»¥åŒºåˆ†æˆ–å…·æœ‰ç»†å¾®æ‹“æ‰‘æ‰°åŠ¨çš„éšè”½å¨èƒã€‚ç ”ç©¶è¿˜å¯¹ç°æœ‰è‡ªé€‚åº”æ”»å‡»çš„æ¯’åŒ–æ‰‹æ®µè¿›è¡Œäº†åˆ†ç±»ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºTarget mappingçš„å¢å¼ºå‹è‡ªé€‚åº”æ”»å‡»ä»¥éªŒè¯é˜²å¾¡æ•ˆèƒ½ã€‚åœ¨CIFAR-10ã€GTSRBå’ŒImageNet100ç­‰æ•°æ®é›†ä»¥åŠResNet20ã€ResNet101æ¶æ„ä¸Šçš„å®éªŒè¯æ˜ï¼ŒTED-LaSTèƒ½æœ‰æ•ˆå¯¹æŠ—Adap-Blendå’ŒAdapt-Patchç­‰å¤æ‚åé—¨æ”»å‡»ã€‚è¿™é¡¹å·¥ä½œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨åº”å¯¹ä¸æ–­æ¼”å˜çš„åé—¨å¨èƒæ—¶çš„å®‰å…¨æ€§ï¼Œä¸ºé²æ£’åé—¨æ£€æµ‹æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10722v1",
      "published_date": "2025-06-12 14:12:15 UTC",
      "updated_date": "2025-06-12 14:12:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:19:01.671766+00:00"
    },
    {
      "arxiv_id": "2506.10716v1",
      "title": "PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models",
      "title_zh": "PREMISEï¼šé¢å‘å¤§æ¨¡å‹é«˜æ•ˆæ•°å­¦æ¨ç†çš„å¯æ‰©å±•ç­–ç•¥æ€§æç¤ºè¯ä¼˜åŒ–",
      "authors": [
        "Ye Yu",
        "Yaoning Yu",
        "Haohan Wang"
      ],
      "abstract": "Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve strong performance on mathematical benchmarks using lengthy chain-of-thought (CoT) reasoning, but the resulting traces are often unnecessarily verbose. This inflates token usage and cost, limiting deployment in latency-sensitive or API-constrained settings. We introduce PREMISE (PRompt-based Efficient Mathematical Inference with Strategic Evaluation), a prompt-only framework that reduces reasoning overhead without modifying model weights. PREMISE combines trace-level diagnostics with gradient-inspired prompt optimization to minimize redundant computation while preserving answer accuracy. The approach jointly optimizes brevity and correctness through a multi-objective textual search that balances token length and answer validity. Unlike prior work, PREMISE runs in a single-pass black-box interface, so it can be applied directly to commercial LLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy ($96\\%\\rightarrow96\\%$ with Claude, $91\\%\\rightarrow92\\%$ with Gemini) while reducing reasoning tokens by up to $87.5\\%$ and cutting dollar cost by $69$--$82\\%$. These results show that prompt-level optimization is a practical and scalable path to efficient LRM inference without compromising reasoning quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹æ¨ç†æ¨¡å‹(Large reasoning models)åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å› å†—é•¿çš„é“¾å¼æ€ç»´(Chain-of-Thought)å¯¼è‡´çš„é«˜æ˜‚ä»¤ç‰Œ(token)æ¶ˆè€—ä¸å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†PREMISEæ¡†æ¶ã€‚è¿™æ˜¯ä¸€ç§çº¯æç¤ºè¯(prompt-only)çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡ç»“åˆè¿½è¸ªçº§è¯Šæ–­(trace-level diagnostics)ä¸å—æ¢¯åº¦å¯å‘çš„æç¤ºè¯ä¼˜åŒ–(gradient-inspired prompt optimization)ï¼Œåœ¨ä¸ä¿®æ”¹æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹å‡å°‘å†—ä½™è®¡ç®—ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šç›®æ ‡æ–‡æœ¬æœç´¢(multi-objective textual search)å¹³è¡¡æ¨ç†é•¿åº¦ä¸å‡†ç¡®æ€§ï¼Œå¹¶æ”¯æŒåœ¨å•†ä¸šåŒ–å¤§æ¨¡å‹ä¸Šé€šè¿‡å•æ¬¡ä¼ é€’(single-pass)é»‘ç›’æ¥å£ç›´æ¥åº”ç”¨ã€‚å®éªŒåœ¨GSM8Kã€SVAMPå’ŒMath500ç­‰åŸºå‡†ä¸Šè¯æ˜ï¼ŒPREMISEåœ¨ä¿æŒæˆ–æå‡å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæœ€é«˜å¯å‡å°‘87.5%çš„æ¨ç†ä»¤ç‰Œå¹¶é™ä½69%è‡³82%çš„ç»æµæˆæœ¬ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæç¤ºè¯çº§ä¼˜åŒ–æ˜¯å®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„å¤§å‹æ¨ç†æ¨¡å‹æ¨ç†ä¸”ä¸æŸå®³æ¨ç†è´¨é‡çš„å®ç”¨è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10716v1",
      "published_date": "2025-06-12 14:05:09 UTC",
      "updated_date": "2025-06-12 14:05:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:19:04.829787+00:00"
    },
    {
      "arxiv_id": "2506.10713v1",
      "title": "Deep Learning-based Multi Project InP Wafer Simulation for Unsupervised Surface Defect Detection",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šé¡¹ç›® InP æ™¶åœ†ä»¿çœŸç”¨äºæ— ç›‘ç£è¡¨é¢ç¼ºé™·æ£€æµ‹",
      "authors": [
        "EmÃ­lio Dolgener CantÃº",
        "Rolf Klemens Wittmann",
        "Oliver Abdeen",
        "Patrick Wagner",
        "Wojciech Samek",
        "Moritz Baier",
        "Sebastian Lapuschkin"
      ],
      "abstract": "Quality management in semiconductor manufacturing often relies on template matching with known golden standards. For Indium-Phosphide (InP) multi-project wafer manufacturing, low production scale and high design variability lead to such golden standards being typically unavailable. Defect detection, in turn, is manual and labor-intensive. This work addresses this challenge by proposing a methodology to generate a synthetic golden standard using Deep Neural Networks, trained to simulate photo-realistic InP wafer images from CAD data. We evaluate various training objectives and assess the quality of the simulated images on both synthetic data and InP wafer photographs. Our deep-learning-based method outperforms a baseline decision-tree-based approach, enabling the use of a 'simulated golden die' from CAD plans in any user-defined region of a wafer for more efficient defect detection. We apply our method to a template matching procedure, to demonstrate its practical utility in surface defect detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç£·åŒ–é“Ÿ(InP)å¤šé¡¹ç›®æ™¶åœ†åˆ¶é€ ä¸­å› ç”Ÿäº§è§„æ¨¡å°å’Œè®¾è®¡å¤šå˜å¯¼è‡´ç¼ºä¹â€œé»„é‡‘æ ‡å‡†â€çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨æ·±åº¦å­¦ä¹ ç”Ÿæˆåˆæˆé»„é‡‘æ ‡å‡†çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶é‡‡ç”¨æ·±åº¦ç¥ç»ç½‘ç»œ(Deep Neural Networks)æ ¹æ®CADæ•°æ®æ¨¡æ‹Ÿå‡ºå…·æœ‰ç…§ç‰‡çº§çœŸå®æ„Ÿçš„InPæ™¶åœ†å›¾åƒï¼Œå¹¶å¯¹å¤šç§è®­ç»ƒç›®æ ‡è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ¨¡æ‹Ÿå›¾åƒè´¨é‡ä¸Šä¼˜äºä¼ ç»Ÿçš„å†³ç­–æ ‘(decision-tree)åŸºçº¿æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•æ”¯æŒåœ¨æ™¶åœ†çš„ä»»ä½•è‡ªå®šä¹‰åŒºåŸŸç”Ÿæˆâ€œæ¨¡æ‹Ÿé»„é‡‘ç®¡èŠ¯(simulated golden die)â€ï¼Œæ˜¾è‘—æå‡äº†ç¼ºé™·æ£€æµ‹çš„è‡ªåŠ¨åŒ–æ•ˆç‡ã€‚æœ€åï¼Œé€šè¿‡å°†è¯¥æ–¹æ³•åº”ç”¨äºæ¨¡æ¿åŒ¹é…(template matching)ç¨‹åºï¼ŒæˆåŠŸå±•ç¤ºäº†å…¶åœ¨æ— ç›‘ç£è¡¨é¢ç¼ºé™·æ£€æµ‹ä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10713v1",
      "published_date": "2025-06-12 14:03:10 UTC",
      "updated_date": "2025-06-12 14:03:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:19:17.053361+00:00"
    },
    {
      "arxiv_id": "2506.10710v1",
      "title": "Continual Hyperbolic Learning of Instances and Classes",
      "title_zh": "å®ä¾‹ä¸ç±»åˆ«çš„åŒæ›²æŒç»­å­¦ä¹ ",
      "authors": [
        "Melika Ayoughi",
        "Mina Ghadimi Atigh",
        "Mohammad Mahdi Derakhshani",
        "Cees G. M. Snoek",
        "Pascal Mettes",
        "Paul Groth"
      ],
      "abstract": "Continual learning has traditionally focused on classifying either instances or classes, but real-world applications, such as robotics and self-driving cars, require models to handle both simultaneously. To mirror real-life scenarios, we introduce the task of continual learning of instances and classes, at the same time. This task challenges models to adapt to multiple levels of granularity over time, which requires balancing fine-grained instance recognition with coarse-grained class generalization. In this paper, we identify that classes and instances naturally form a hierarchical structure. To model these hierarchical relationships, we propose HyperCLIC, a continual learning algorithm that leverages hyperbolic space, which is uniquely suited for hierarchical data due to its ability to represent tree-like structures with low distortion and compact embeddings. Our framework incorporates hyperbolic classification and distillation objectives, enabling the continual embedding of hierarchical relations. To evaluate performance across multiple granularities, we introduce continual hierarchical metrics. We validate our approach on EgoObjects, the only dataset that captures the complexity of hierarchical object recognition in dynamic real-world environments. Empirical results show that HyperCLIC operates effectively at multiple granularities with improved hierarchical generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HyperCLICï¼Œä¸€ç§æ—¨åœ¨åŒæ—¶å¤„ç†å®ä¾‹(instances)å’Œç±»åˆ«(classes)æŒç»­å­¦ä¹ (continual learning)çš„æ–°å‹ç®—æ³•ï¼Œä»¥åº”å¯¹æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶ç­‰ç°å®åº”ç”¨ä¸­å¯¹å¤šç²’åº¦è®¤çŸ¥çš„éœ€æ±‚ã€‚ä½œè€…è¯†åˆ«å‡ºå®ä¾‹ä¸ç±»åˆ«ä¹‹é—´å¤©ç„¶å½¢æˆçš„å±‚çº§ç»“æ„ï¼Œå¹¶åˆ©ç”¨åŒæ›²ç©ºé—´(hyperbolic space)åœ¨è¡¨ç¤ºæ ‘çŠ¶ç»“æ„æ–¹é¢çš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œæ„å»ºäº†ä½å¤±çœŸä¸”ç´§å‡‘çš„å±‚çº§åµŒå…¥è¡¨ç¤ºã€‚HyperCLICæ¡†æ¶é€šè¿‡æ•´åˆåŒæ›²åˆ†ç±»ä¸è’¸é¦ç›®æ ‡ï¼Œå®ç°äº†å±‚çº§å…³ç³»çš„æŒç»­åµŒå…¥ï¼ŒåŒæ—¶å¼•å…¥äº†æŒç»­å±‚çº§æŒ‡æ ‡(continual hierarchical metrics)æ¥å…¨é¢è¯„ä¼°ä¸åŒç²’åº¦ä¸‹çš„æ€§èƒ½ã€‚é’ˆå¯¹EgoObjectsæ•°æ®é›†çš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒHyperCLICåœ¨å¤„ç†å¤æ‚å±‚çº§å¯¹è±¡è¯†åˆ«ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œæœ‰æ•ˆå¹³è¡¡äº†ç»†ç²’åº¦å®ä¾‹è¯†åˆ«ä¸ç²—ç²’åº¦ç±»åˆ«æ³›åŒ–ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„å±‚çº§æ³›åŒ–æ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10710v1",
      "published_date": "2025-06-12 13:59:57 UTC",
      "updated_date": "2025-06-12 13:59:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:19:24.605576+00:00"
    },
    {
      "arxiv_id": "2506.10708v1",
      "title": "System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers",
      "title_zh": "ASPMT2SMT ç³»ç»Ÿï¼šåˆ©ç”¨ SMT æ±‚è§£å™¨è®¡ç®— ASPMT ç†è®º",
      "authors": [
        "Michael Bartholomew",
        "Joohyung Lee"
      ],
      "abstract": "Answer Set Programming Modulo Theories (ASPMT) is an approach to combining answer set programming and satisfiability modulo theories based on the functional stable model semantics. It is shown that the tight fragment of ASPMT programs can be turned into SMT instances, thereby allowing SMT solvers to compute stable models of ASPMT programs. In this paper we present a compiler called {\\sc aspsmt2smt}, which implements this translation. The system uses ASP grounder {\\sc gringo} and SMT solver {\\sc z3}. {\\sc gringo} partially grounds input programs while leaving some variables to be processed by {\\sc z3}. We demonstrate that the system can effectively handle real number computations for reasoning about continuous changes.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ASPMT2SMTç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªå°†Answer Set Programming Modulo Theories (ASPMT)ç†è®ºè½¬åŒ–ä¸ºSatisfiability Modulo Theories (SMT)å®ä¾‹è¿›è¡Œæ±‚è§£çš„ç¼–è¯‘å™¨ã€‚è¯¥ç³»ç»ŸåŸºäºfunctional stable model semanticsï¼Œè¯æ˜äº†ASPMTç¨‹åºçš„tight fragmentå¯ä»¥è¢«æœ‰æ•ˆåœ°è½¬åŒ–ä¸ºSMTé—®é¢˜ï¼Œä»è€Œåˆ©ç”¨æˆç†Ÿçš„SMTæ±‚è§£å™¨è®¡ç®—å…¶ç¨³å®šæ¨¡å‹ã€‚åœ¨å®ç°ç»†èŠ‚ä¸Šï¼ŒASPMT2SMTç»“åˆäº†ASP grounder gringoå’ŒSMT solver z3ï¼Œé€šè¿‡gringoè¿›è¡Œéƒ¨åˆ†groundingå¹¶å°†ç‰¹å®šå˜é‡äº¤ç”±z3å¤„ç†ã€‚å®éªŒè¯æ˜è¯¥ç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆå¤„ç†å®æ•°è®¡ç®—ï¼Œç‰¹åˆ«é€‚ç”¨äºå¯¹continuous changesè¿›è¡Œæ¨ç†çš„åœºæ™¯ã€‚è¯¥å·¥å…·ä¸ºç»“åˆé€»è¾‘ç¼–ç¨‹ä¸èƒŒæ™¯ç†è®ºæ±‚è§£æä¾›äº†å®ç”¨çš„æŠ€æœ¯è·¯å¾„ï¼Œæ˜¾è‘—å¢å¼ºäº†ASPMTç†è®ºçš„å¯è®¡ç®—æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings of the 14th European Conference on Logics in Artificial Intelligence (JELIA 2014)",
      "pdf_url": "https://arxiv.org/pdf/2506.10708v1",
      "published_date": "2025-06-12 13:59:15 UTC",
      "updated_date": "2025-06-12 13:59:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:19:35.975748+00:00"
    },
    {
      "arxiv_id": "2506.10707v4",
      "title": "ConTextTab: A Semantics-Aware Tabular In-Context Learner",
      "title_zh": "ConTextTabï¼šè¯­ä¹‰æ„ŸçŸ¥çš„è¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹ å™¨",
      "authors": [
        "Marco Spinaci",
        "Marek Polewczyk",
        "Maximilian Schambach",
        "Sam Thelin"
      ],
      "abstract": "Tabular in-context learning (ICL) has recently achieved state-of-the-art (SOTA) performance on several tabular prediction tasks. Previously restricted to classification problems on small tables, recent advances such as TabPFN and TabICL have extended its use to larger datasets. Although current table-native ICL architectures are architecturally efficient and well-adapted to tabular data structures, their exclusive training on synthetic data limits their ability to fully leverage the rich semantics and world knowledge contained in real-world tabular data. At the other end of the spectrum, tabular ICL models based on pretrained large language models such as TabuLa-8B integrate deep semantic understanding and world knowledge but are only able to make use of a small amount of context due to inherent architectural limitations. With the aim to combine the best of both these worlds, we introduce ConTextTab, integrating semantic understanding and alignment into a table-native ICL framework. By employing specialized embeddings for different data modalities and by training on large-scale real-world tabular data, our model is competitive with SOTA across a broad set of benchmarks while setting a new standard on the semantically rich CARTE benchmark. Code and model checkpoints are available at: https://github.com/SAP-samples/sap-rpt-1-oss.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹ (Tabular In-Context Learning)ä¸­åŸç”Ÿæ¶æ„ç¼ºä¹è¯­ä¹‰ç†è§£ã€è€Œå¤§è¯­è¨€æ¨¡å‹æ¶æ„å—é™äºä¸Šä¸‹æ–‡é•¿åº¦çš„é—®é¢˜ï¼Œæå‡ºäº†ConTextTabã€‚ConTextTabæ˜¯ä¸€ç§æ„ŸçŸ¥è¯­ä¹‰çš„è¡¨æ ¼ICLæ¡†æ¶ï¼Œæ—¨åœ¨å°†æ·±åº¦è¯­ä¹‰ç†è§£ä¸é«˜æ•ˆçš„è¡¨æ ¼åŸç”Ÿæ¶æ„ç›¸ç»“åˆã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸ºä¸åŒçš„æ•°æ®æ¨¡æ€é‡‡ç”¨ä¸“é—¨çš„åµŒå…¥(Embeddings)æŠ€æœ¯ï¼Œå¹¶åœ¨å¤§è§„æ¨¡çœŸå®ä¸–ç•Œè¡¨æ ¼æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†è¯­ä¹‰ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆä¸å¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒConTextTabåœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸å½“å‰SOTAæ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ã€‚ç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰ä¸°å¯Œçš„CARTEåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹æ ‘ç«‹äº†æ–°çš„æ€§èƒ½æ ‡å‡†ï¼Œè¯æ˜äº†å…¶åˆ©ç”¨çœŸå®ä¸–ç•ŒçŸ¥è¯†æå‡è¡¨æ ¼é¢„æµ‹ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as spotlight at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10707v4",
      "published_date": "2025-06-12 13:57:29 UTC",
      "updated_date": "2025-11-03 11:32:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:19:39.854288+00:00"
    },
    {
      "arxiv_id": "2506.10704v1",
      "title": "Formalising Software Requirements using Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è½¯ä»¶éœ€æ±‚å½¢å¼åŒ–",
      "authors": [
        "Arshad Beg",
        "Diarmuid O'Donoghue",
        "Rosemary Monahan"
      ],
      "abstract": "This paper is a brief introduction to our recently initiated project named VERIFAI: Traceability and verification of natural language requirements. The project addresses the challenges in the traceability and verification of formal specifications through providing support for the automatic generation of the formal specifications and the traceability of the requirements from the initial software design stage through the systems implementation and verification. Approaches explored in this project include Natural Language Processing, use of ontologies to describe the software system domain, reuse of existing software artefacts from similar systems (i.e. through similarity based reuse) and large language models to identify and declare the specifications as well as use of artificial intelligence to guide the process.",
      "tldr_zh": "è¯¥è®ºæ–‡ä»‹ç»äº†åä¸º VERIFAI çš„æ–°å¯åŠ¨é¡¹ç›®ï¼Œæ—¨åœ¨åˆ©ç”¨ Large Language Models (LLMs) å®ç°è½¯ä»¶éœ€æ±‚çš„å½¢å¼åŒ–è½¬æ¢ï¼Œä»¥è§£å†³è‡ªç„¶è¯­è¨€éœ€æ±‚åœ¨è¿½æº¯ä¸éªŒè¯è¿‡ç¨‹ä¸­çš„éš¾é¢˜ã€‚è¯¥é¡¹ç›®é€šè¿‡æä¾›è‡ªåŠ¨åŒ–ç”Ÿæˆå½¢å¼åŒ–è§„èŒƒ (formal specifications) çš„æ”¯æŒï¼Œå®ç°äº†ä»è½¯ä»¶è®¾è®¡åˆå§‹é˜¶æ®µåˆ°ç³»ç»Ÿå®ç°ä¸éªŒè¯å…¨ç”Ÿå‘½å‘¨æœŸçš„éœ€æ±‚è¿½è¸ªã€‚ç ”ç©¶æ¢ç´¢äº†å¤šç§å…ˆè¿›æŠ€æœ¯è·¯å¾„ï¼ŒåŒ…æ‹¬åº”ç”¨ Natural Language Processing (NLP) å¤„ç†éœ€æ±‚æ–‡æœ¬ï¼Œä»¥åŠåˆ©ç”¨ ontologies æè¿°è½¯ä»¶ç³»ç»Ÿé¢†åŸŸçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¯¥é¡¹ç›®è¿˜é‡‡ç”¨äº†åŸºäºç›¸ä¼¼æ€§çš„é‡ç”¨ (similarity based reuse) ç­–ç•¥å’Œäººå·¥æ™ºèƒ½æŠ€æœ¯æ¥æŒ‡å¯¼è§„èŒƒçš„è¯†åˆ«ä¸å£°æ˜è¿‡ç¨‹ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæé«˜è½¯ä»¶å¼€å‘è¿‡ç¨‹ä¸­çš„è‡ªåŠ¨åŒ–ç¨‹åº¦å’Œç³»ç»Ÿå¯é æ€§å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted and presented as a poster in ADAPT Annual Conference (AACS2025) on 15th of May, 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10704v1",
      "published_date": "2025-06-12 13:55:01 UTC",
      "updated_date": "2025-06-12 13:55:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:19:43.234796+00:00"
    },
    {
      "arxiv_id": "2506.10687v1",
      "title": "Large Language Models for Detection of Life-Threatening Texts",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿå‘½å¨èƒæ–‡æœ¬æ£€æµ‹",
      "authors": [
        "Thanh Thi Nguyen",
        "Campbell Wilson",
        "Janis Dalins"
      ],
      "abstract": "Detecting life-threatening language is essential for safeguarding individuals in distress, promoting mental health and well-being, and preventing potential harm and loss of life. This paper presents an effective approach to identifying life-threatening texts using large language models (LLMs) and compares them with traditional methods such as bag of words, word embedding, topic modeling, and Bidirectional Encoder Representations from Transformers. We fine-tune three open-source LLMs including Gemma, Mistral, and Llama-2 using their 7B parameter variants on different datasets, which are constructed with class balance, imbalance, and extreme imbalance scenarios. Experimental results demonstrate a strong performance of LLMs against traditional methods. More specifically, Mistral and Llama-2 models are top performers in both balanced and imbalanced data scenarios while Gemma is slightly behind. We employ the upsampling technique to deal with the imbalanced data scenarios and demonstrate that while this method benefits traditional approaches, it does not have as much impact on LLMs. This study demonstrates a great potential of LLMs for real-world life-threatening language detection problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) æ£€æµ‹å¨èƒç”Ÿå‘½è¯­è¨€ (life-threatening language) çš„æœ‰æ•ˆæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«å¤„äºå›°å¢ƒä¸­çš„ä¸ªä½“æ¥é¢„é˜²æ½œåœ¨ä¼¤å®³ã€‚ç ”ç©¶è€…å¯¹ä¸‰ç§ 7B å‚æ•°è§„æ¨¡çš„å¼€æºæ¨¡å‹ Gemmaã€Mistral å’Œ Llama-2 è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶åœ¨å¹³è¡¡ã€å¤±è¡¡åŠæç«¯å¤±è¡¡çš„ä¸åŒæ•°æ®é›†åœºæ™¯ä¸‹ï¼Œå°†å…¶ä¸è¯è¢‹æ¨¡å‹ (bag of words)ã€è¯åµŒå…¥ (word embedding)ã€ä¸»é¢˜å»ºæ¨¡ (topic modeling) å’Œ BERT ç­‰ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMs åœ¨å„ç§åœºæ™¯ä¸‹çš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…¶ä¸­ Mistral å’Œ Llama-2 åœ¨å¹³è¡¡å’Œä¸å¹³è¡¡æ•°æ®æµ‹è¯•ä¸­è¡¨ç°æœ€ä¸ºçªå‡ºã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ä¸Šé‡‡æ · (upsampling) æŠ€æœ¯å¯¹ä¼ ç»Ÿæ–¹æ³•çš„æå‡ä½œç”¨æ¯”å¯¹ LLMs æ›´ä¸ºæ˜æ˜¾ã€‚è¯¥é¡¹ç ”ç©¶å……åˆ†å±•ç¤ºäº† LLMs åœ¨è§£å†³ç°å®ä¸–ç•Œå¨èƒç”Ÿå‘½è¯­è¨€æ£€æµ‹é—®é¢˜ä¸Šçš„å·¨å¤§åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10687v1",
      "published_date": "2025-06-12 13:33:27 UTC",
      "updated_date": "2025-06-12 13:33:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:19:49.578033+00:00"
    },
    {
      "arxiv_id": "2506.10680v4",
      "title": "Saturation Self-Organizing Map",
      "title_zh": "é¥±å’Œè‡ªç»„ç»‡æ˜ å°„",
      "authors": [
        "Igor Urbanik",
        "PaweÅ‚ Gajewski"
      ],
      "abstract": "Continual learning poses a fundamental challenge for neural systems, which often suffer from catastrophic forgetting when exposed to sequential tasks. Self-Organizing Maps (SOMs), despite their interpretability and efficiency, are not immune to this issue. In this paper, we introduce Saturation Self-Organizing Maps (SatSOM)-an extension of SOMs designed to improve knowledge retention in continual learning scenarios. SatSOM incorporates a novel saturation mechanism that gradually reduces the learning rate and neighborhood radius of neurons as they accumulate information. This effectively freezes well-trained neurons and redirects learning to underutilized areas of the map.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¿ç»­å­¦ä¹ ï¼ˆContinual learningï¼‰ä¸­ç¥ç»ç³»ç»Ÿå¸¸é­é‡çš„ç¾éš¾æ€§é—å¿˜ï¼ˆcatastrophic forgettingï¼‰é—®é¢˜ï¼Œæå‡ºäº†é¥±å’Œè‡ªç»„ç»‡æ˜ å°„ï¼ˆSaturation Self-Organizing Mapsï¼Œç®€ç§° SatSOMï¼‰ã€‚å°½ç®¡ä¼ ç»Ÿçš„è‡ªç»„ç»‡æ˜ å°„ï¼ˆSelf-Organizing Maps, SOMsï¼‰å…·æœ‰è¾ƒå¼ºçš„å¯è§£é‡Šæ€§å’Œæ•ˆç‡ï¼Œä½†åœ¨å¤„ç†åºåˆ—ä»»åŠ¡æ—¶ä»å­˜åœ¨çŸ¥è¯†ä¸¢å¤±çš„é£é™©ã€‚SatSOM é€šè¿‡å¼•å…¥ä¸€ç§åˆ›æ–°çš„é¥±å’Œæœºåˆ¶ï¼ˆsaturation mechanismï¼‰ï¼Œåœ¨ç¥ç»å…ƒç§¯ç´¯ä¿¡æ¯çš„åŒæ—¶é€æ¸é™ä½å…¶å­¦ä¹ ç‡ï¼ˆlearning rateï¼‰å’Œé‚»åŸŸåŠå¾„ï¼ˆneighborhood radiusï¼‰ã€‚è¿™ä¸€æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆâ€œå†»ç»“â€å·²å……åˆ†è®­ç»ƒçš„ç¥ç»å…ƒï¼Œä»è€Œé˜²æ­¢æ—§çŸ¥è¯†è¢«è¦†ç›–ï¼Œå¹¶å°†å­¦ä¹ å‹åŠ›é‡å®šå‘è‡³åœ°å›¾ä¸­æœªè¢«å……åˆ†åˆ©ç”¨çš„åŒºåŸŸã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSatSOM æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸‹çš„çŸ¥è¯†ä¿ç•™èƒ½åŠ›ï¼Œä¸ºè§£å†³ SOMs åœ¨åºåˆ—ä»»åŠ¡ä¸­çš„ç¨³å®šæ€§é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„æ‰©å±•æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "github repository: https://github.com/Radinyn/satsom",
      "pdf_url": "https://arxiv.org/pdf/2506.10680v4",
      "published_date": "2025-06-12 13:18:26 UTC",
      "updated_date": "2025-11-16 19:22:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:19:50.660284+00:00"
    },
    {
      "arxiv_id": "2506.10678v1",
      "title": "Automated Validation of Textual Constraints Against AutomationML via LLMs and SHACL",
      "title_zh": "åŸºäº LLM ä¸ SHACL çš„ AutomationML æ–‡æœ¬çº¦æŸè‡ªåŠ¨åŒ–éªŒè¯",
      "authors": [
        "Tom Westermann",
        "Aljosha KÃ¶cher",
        "Felix Gehlhoff"
      ],
      "abstract": "AutomationML (AML) enables standardized data exchange in engineering, yet existing recommendations for proper AML modeling are typically formulated as informal and textual constraints. These constraints cannot be validated automatically within AML itself. This work-in-progress paper introduces a pipeline to formalize and verify such constraints. First, AML models are mapped to OWL ontologies via RML and SPARQL. In addition, a Large Language Model translates textual rules into SHACL constraints, which are then validated against the previously generated AML ontology. Finally, SHACL validation results are automatically interpreted in natural language. The approach is demonstrated on a sample AML recommendation. Results show that even complex modeling rules can be semi-automatically checked -- without requiring users to understand formal methods or ontology technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å’ŒSHACLçš„è‡ªåŠ¨åŒ–æµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³AutomationML (AML)å»ºæ¨¡ä¸­éæ­£å¼æ–‡æœ¬çº¦æŸéš¾ä»¥è‡ªåŠ¨éªŒè¯çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡RMLå’ŒSPARQLå°†AMLæ¨¡å‹æ˜ å°„ä¸ºOWLæœ¬ä½“ï¼Œä¸ºè¯­ä¹‰åŒ–å¤„ç†å¥ å®šåŸºç¡€ã€‚æ¥ç€ï¼Œåˆ©ç”¨LLMå°†éæ­£å¼çš„æ–‡æœ¬è§„åˆ™è½¬åŒ–ä¸ºæ­£å¼çš„SHACLçº¦æŸï¼Œå¹¶é’ˆå¯¹ç”Ÿæˆçš„AMLæœ¬ä½“è¿›è¡ŒéªŒè¯ã€‚ç³»ç»Ÿéšåä¼šè‡ªåŠ¨å°†SHACLéªŒè¯ç»“æœè§£é‡Šä¸ºè‡ªç„¶è¯­è¨€ï¼Œä»è€Œå®ç°éªŒè¯è¿‡ç¨‹çš„é—­ç¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤ŸåŠè‡ªåŠ¨åœ°æ£€æŸ¥å¤æ‚çš„å»ºæ¨¡è§„åˆ™ï¼Œä¸”æ— éœ€ç”¨æˆ·å…·å¤‡æ·±åšçš„å½¢å¼åŒ–æ–¹æ³•æˆ–æœ¬ä½“æŠ€æœ¯èƒŒæ™¯ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå·¥ç¨‹é¢†åŸŸä¸­éæ­£å¼å»ºæ¨¡å»ºè®®å‘è‡ªåŠ¨åŒ–ã€æ™ºèƒ½åŒ–éªŒè¯çš„è½¬åŒ–æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10678v1",
      "published_date": "2025-06-12 13:14:33 UTC",
      "updated_date": "2025-06-12 13:14:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:20:09.258280+00:00"
    },
    {
      "arxiv_id": "2506.10674v1",
      "title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving",
      "title_zh": "TeleMathï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨ç”µä¿¡æ•°å­¦é—®é¢˜æ±‚è§£ä¸­çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Vincenzo Colle",
        "Mohamed Sana",
        "Nicola Piovesan",
        "Antonio De Domenico",
        "Fadhel Ayed",
        "Merouane Debbah"
      ],
      "abstract": "The increasing adoption of artificial intelligence in telecommunications has raised interest in the capability of Large Language Models (LLMs) to address domain-specific, mathematically intensive tasks. Although recent advancements have improved the performance of LLMs in general mathematical reasoning, their effectiveness within specialized domains, such as signal processing, network optimization, and performance analysis, remains largely unexplored. To address this gap, we introduce TeleMath, the first benchmark dataset specifically designed to evaluate LLM performance in solving mathematical problems with numerical solutions in the telecommunications domain. Comprising 500 question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the telecommunications field. This paper outlines the proposed QnAs generation pipeline, starting from a selected seed of problems crafted by Subject Matter Experts. The evaluation of a wide range of open-source LLMs reveals that best performance on TeleMath is achieved by recent models explicitly designed for mathematical or logical reasoning. In contrast, general-purpose models, even those with a large number of parameters, often struggle with these challenges. We have released the dataset and the evaluation code to ease result reproducibility and support future research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”µä¿¡é¢†åŸŸå¤„ç†æ•°å­¦å¯†é›†å‹ä»»åŠ¡çš„èƒ½åŠ›è¯„ä¼°ä¸è¶³ï¼Œæå‡ºäº†é¦–ä¸ªä¸“é—¨çš„åŸºå‡†æµ‹è¯•é›†TeleMathã€‚è¯¥æ•°æ®é›†åŒ…å«500ä¸ªé—®ç­”å¯¹(QnA pairs)ï¼Œæ¶µç›–äº†ä¿¡å·å¤„ç†ã€ç½‘ç»œä¼˜åŒ–å’Œæ€§èƒ½åˆ†æç­‰å¹¿æ³›çš„ç”µä¿¡ä¸»é¢˜ã€‚æ•°æ®ç”Ÿæˆçš„æµæ°´çº¿èµ·å§‹äºç”±é¢†åŸŸä¸“å®¶(Subject Matter Experts)ç²¾å¿ƒè®¾è®¡çš„ç§å­é—®é¢˜ï¼Œç¡®ä¿äº†ä»»åŠ¡çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸“ä¸ºæ•°å­¦æˆ–é€»è¾‘æ¨ç†è®¾è®¡çš„æœ€æ–°æ¨¡å‹åœ¨TeleMathä¸Šè¡¨ç°æœ€ä¸ºä¼˜å¼‚ã€‚ä¸æ­¤å½¢æˆå¯¹æ¯”çš„æ˜¯ï¼Œé€šç”¨çš„LLMså³ä¾¿æ‹¥æœ‰å·¨å¤§çš„å‚æ•°è§„æ¨¡ï¼Œåœ¨å¤„ç†è¿™äº›é¢†åŸŸç‰¹å®šçš„æ•°å­¦æŒ‘æˆ˜æ—¶ä¾ç„¶é¢ä¸´æ˜¾è‘—å›°éš¾ã€‚è¯¥ç ”ç©¶é€šè¿‡å…¬å¼€å‘å¸ƒæ•°æ®é›†å’Œè¯„ä¼°ä»£ç ï¼Œä¸ºæå‡ç”µä¿¡é¢†åŸŸæ¨¡å‹æ€§èƒ½åŠæ”¯æŒæœªæ¥ç›¸å…³ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.10674v1",
      "published_date": "2025-06-12 13:04:18 UTC",
      "updated_date": "2025-06-12 13:04:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:20:12.120298+00:00"
    },
    {
      "arxiv_id": "2506.11180v2",
      "title": "Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing",
      "title_zh": "è¶…è¶Šèƒ½åŠ›ä¸æŠ€èƒ½çš„å½¢å¼è¯­ä¹‰ï¼šåˆ¶é€ ä¸šä¸­çš„æ¨¡å‹ä¸Šä¸‹æ–‡åè®®",
      "authors": [
        "Luis Miguel Vieira da Silva",
        "Aljosha KÃ¶cher",
        "Felix Gehlhoff"
      ],
      "abstract": "Explicit modeling of capabilities and skills -- whether based on ontologies, Asset Administration Shells, or other technologies -- requires considerable manual effort and often results in representations that are not easily accessible to Large Language Models (LLMs). In this work-in-progress paper, we present an alternative approach based on the recently introduced Model Context Protocol (MCP). MCP allows systems to expose functionality through a standardized interface that is directly consumable by LLM-based agents. We conduct a prototypical evaluation on a laboratory-scale manufacturing system, where resource functions are made available via MCP. A general-purpose LLM is then tasked with planning and executing a multi-step process, including constraint handling and the invocation of resource functions via MCP. The results indicate that such an approach can enable flexible industrial automation without relying on explicit semantic models. This work lays the basis for further exploration of external tool integration in LLM-driven production systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ¶é€ é¢†åŸŸä¸­èƒ½åŠ›ä¸æŠ€èƒ½å»ºæ¨¡çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„ ontologies å’Œ Asset Administration Shells æ–¹æ³•å­˜åœ¨äººå·¥æˆæœ¬é«˜ä¸”éš¾ä»¥è¢« Large Language Models (LLMs) ç›´æ¥åˆ©ç”¨çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäº Model Context Protocol (MCP) çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåˆ©ç”¨è¯¥åè®®æä¾›çš„æ ‡å‡†åŒ–æ¥å£å°†ç³»ç»ŸåŠŸèƒ½ç›´æ¥æš´éœ²ç»™ LLM é©±åŠ¨çš„æ™ºèƒ½ä½“ã€‚ç ”ç©¶é€šè¿‡åœ¨å®éªŒå®¤è§„æ¨¡çš„åˆ¶é€ ç³»ç»Ÿä¸Šè¿›è¡ŒåŸå‹è¯„ä¼°ï¼Œå±•ç¤ºäº†é€šç”¨å‹ LLM å¦‚ä½•åœ¨å¤„ç†çº¦æŸçš„åŒæ—¶ï¼Œé€šè¿‡ MCP è°ƒç”¨èµ„æºå‡½æ•°å®Œæˆå¤šæ­¥ç”Ÿäº§è¿‡ç¨‹çš„è§„åˆ’ä¸æ‰§è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ— éœ€ä¾èµ–æ˜¾å¼çš„è¯­ä¹‰æ¨¡å‹å³å¯å®ç°çµæ´»çš„å·¥ä¸šè‡ªåŠ¨åŒ–ã€‚è¿™é¡¹å·¥ä½œä¸º LLM é©±åŠ¨çš„ç”Ÿäº§ç³»ç»Ÿé›†æˆå¤–éƒ¨å·¥å…·æä¾›äº†å®è·µåŸºç¡€å’Œè¿›ä¸€æ­¥æ¢ç´¢çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.ET",
        "eess.SY"
      ],
      "primary_category": "cs.SE",
      "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
      "pdf_url": "https://arxiv.org/pdf/2506.11180v2",
      "published_date": "2025-06-12 13:02:16 UTC",
      "updated_date": "2025-12-09 14:19:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:20:13.166493+00:00"
    },
    {
      "arxiv_id": "2506.10669v2",
      "title": "PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis",
      "title_zh": "PiPViTï¼šç”¨äºè§†ç½‘è†œå›¾åƒåˆ†æçš„åŸºäºå›¾åƒå—çš„å¯è§†åŒ–å¯è§£é‡ŠåŸå‹",
      "authors": [
        "Marzieh Oghbaie",
        "Teresa AraÃºjo",
        "Hrvoje BogunoviÄ‡"
      ],
      "abstract": "Background and Objective: Prototype-based methods improve interpretability by learning fine-grained part-prototypes; however, their visualization in the input pixel space is not always consistent with human-understandable biomarkers. In addition, well-known prototype-based approaches typically learn extremely granular prototypes that are less interpretable in medical imaging, where both the presence and extent of biomarkers and lesions are critical.\n  Methods: To address these challenges, we propose PiPViT (Patch-based Visual Interpretable Prototypes), an inherently interpretable prototypical model for image recognition. Leveraging a vision transformer (ViT), PiPViT captures long-range dependencies among patches to learn robust, human-interpretable prototypes that approximate lesion extent only using image-level labels. Additionally, PiPViT benefits from contrastive learning and multi-resolution input processing, which enables effective localization of biomarkers across scales.\n  Results: We evaluated PiPViT on retinal OCT image classification across four datasets, where it achieved competitive quantitative performance compared to state-of-the-art methods while delivering more meaningful explanations. Moreover, quantitative evaluation on a hold-out test set confirms that the learned prototypes are semantically and clinically relevant. We believe PiPViT can transparently explain its decisions and assist clinicians in understanding diagnostic outcomes. Github page: https://github.com/marziehoghbaie/PiPViT",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»ŸåŸºäºåŸå‹(Prototype-based)çš„æ–¹æ³•åœ¨åŒ»å­¦å½±åƒä¸­å­˜åœ¨çš„è§£é‡Šæ€§ä¸è¶³å’ŒåŸå‹ç²’åº¦è¿‡ç»†ç­‰é—®é¢˜ï¼Œæå‡ºäº† PiPViT (Patch-based Visual Interpretable Prototypes) æ¡†æ¶ã€‚è¯¥æ¨¡å‹åˆ©ç”¨è§†è§‰äº’æ„Ÿå™¨(Vision Transformer, ViT)æ•è·è¡¥ä¸(patches)ä¹‹é—´çš„é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œä»…ä¾èµ–å›¾åƒçº§æ ‡ç­¾å³å¯å­¦ä¹ åˆ°èƒ½å¤Ÿè¿‘ä¼¼ç—…å˜èŒƒå›´ä¸”ç¬¦åˆäººç±»ç†è§£çš„é²æ£’åŸå‹ã€‚é€šè¿‡å¼•å…¥å¯¹æ¯”å­¦ä¹ (contrastive learning)å’Œå¤šåˆ†è¾¨ç‡è¾“å…¥å¤„ç†ï¼ŒPiPViT å®ç°äº†è·¨å°ºåº¦çš„ç”Ÿç‰©æ ‡å¿—ç‰©(biomarkers)ç²¾ç¡®æ•æ‰ã€‚å®éªŒåœ¨å››ä¸ªè§†ç½‘è†œ OCT å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤º PiPViT åœ¨åˆ†ç±»æ€§èƒ½ä¸Šåª²ç¾ç°æœ‰å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶èƒ½æä¾›å…·æœ‰ä¸´åºŠç›¸å…³æ€§çš„è¯­ä¹‰è§£é‡Šã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘é€æ˜ã€å¯ä¿¡çš„åŒ»ç–—è¾…åŠ©è¯Šæ–­ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ï¼Œæœ‰åŠ©äºä¸´åºŠåŒ»ç”Ÿæ›´ç›´è§‚åœ°ç†è§£è‡ªåŠ¨è¯Šæ–­æ¨¡å‹çš„å†³ç­–ä¾æ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10669v2",
      "published_date": "2025-06-12 12:58:43 UTC",
      "updated_date": "2025-06-13 08:57:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:20:04.926528+00:00"
    },
    {
      "arxiv_id": "2506.11179v1",
      "title": "Brain2Vec: A Deep Learning Framework for EEG-Based Stress Detection Using CNN-LSTM-Attention",
      "title_zh": "Brain2Vecï¼šä¸€ç§åŸºäº CNN-LSTM-Attention çš„ EEG å‹åŠ›æ£€æµ‹æ·±åº¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Md Mynoddin",
        "Troyee Dev",
        "Rishita Chakma"
      ],
      "abstract": "Mental stress has become a pervasive factor affecting cognitive health and overall well-being, necessitating the development of robust, non-invasive diagnostic tools. Electroencephalogram (EEG) signals provide a direct window into neural activity, yet their non-stationary and high-dimensional nature poses significant modeling challenges. Here we introduce Brain2Vec, a new deep learning tool that classifies stress states from raw EEG recordings using a hybrid architecture of convolutional, recurrent, and attention mechanisms. The model begins with a series of convolutional layers to capture localized spatial dependencies, followed by an LSTM layer to model sequential temporal patterns, and concludes with an attention mechanism to emphasize informative temporal regions. We evaluate Brain2Vec on the DEAP dataset, applying bandpass filtering, z-score normalization, and epoch segmentation as part of a comprehensive preprocessing pipeline. Compared to traditional CNN-LSTM baselines, our proposed model achieves an AUC score of 0.68 and a validation accuracy of 81.25%. These findings demonstrate Brain2Vec's potential for integration into wearable stress monitoring platforms and personalized healthcare systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Brain2Vecï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä»åŸå§‹EEGä¿¡å·ä¸­åˆ†ç±»å‹åŠ›çŠ¶æ€çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä»¥åº”å¯¹ç¥ç»æ´»åŠ¨ä¿¡å·çš„é«˜ç»´å’Œéå¹³ç¨³æ€§æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)å’Œæ³¨æ„åŠ›æœºåˆ¶(Attention)çš„æ··åˆæ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†ä¿¡å·çš„ç©ºåŸŸå’Œæ—¶åŸŸç‰¹å¾ã€‚å…¶ä¸­ï¼ŒCNNå±‚ç”¨äºæ•è·å±€éƒ¨ç©ºé—´ä¾èµ–æ€§ï¼ŒLSTMå±‚è´Ÿè´£å»ºæ¨¡åºåˆ—æ—¶é—´æ¨¡å¼ï¼Œè€ŒAttentionæœºåˆ¶åˆ™ç”¨äºå¼ºè°ƒå…·æœ‰ä¿¡æ¯é‡çš„æ—¶é—´åŒºåŸŸã€‚ç ”ç©¶åœ¨DEAPæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶åº”ç”¨äº†å¸¦é€šæ»¤æ³¢(bandpass filtering)ã€z-scoreå½’ä¸€åŒ–å’Œåˆ†æ®µå¤„ç†ç­‰å…¨é¢çš„é¢„å¤„ç†æµç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBrain2Vecåœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°äº†81.25%çš„å‡†ç¡®ç‡å’Œ0.68çš„AUCè¯„åˆ†ï¼Œæ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„CNN-LSTMåŸºçº¿æ¨¡å‹ã€‚è¿™äº›å‘ç°è¯æ˜äº†Brain2Vecåœ¨å¯ç©¿æˆ´å‹åŠ›ç›‘æµ‹å¹³å°å’Œä¸ªæ€§åŒ–åŒ»ç–—ç³»ç»Ÿä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.HC",
        "cs.NE",
        "q-bio.NC"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11179v1",
      "published_date": "2025-06-12 12:57:19 UTC",
      "updated_date": "2025-06-12 12:57:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:20:21.441425+00:00"
    },
    {
      "arxiv_id": "2506.10658v1",
      "title": "Contrastive Matrix Completion with Denoising and Augmented Graph Views for Robust Recommendation",
      "title_zh": "é¢å‘é²æ£’æ¨èçš„ç»“åˆå»å™ªä¸å¢å¼ºå›¾è§†å›¾å¯¹æ¯”å¼çŸ©é˜µè¡¥å…¨",
      "authors": [
        "Narges Nemati",
        "Mostafa Haghir Chehreghani"
      ],
      "abstract": "Matrix completion is a widely adopted framework in recommender systems, as predicting the missing entries in the user-item rating matrix enables a comprehensive understanding of user preferences. However, current graph neural network (GNN)-based approaches are highly sensitive to noisy or irrelevant edges--due to their inherent message-passing mechanisms--and are prone to overfitting, which limits their generalizability. To overcome these challenges, we propose a novel method called Matrix Completion using Contrastive Learning (MCCL). Our approach begins by extracting local neighborhood subgraphs for each interaction and subsequently generates two distinct graph representations. The first representation emphasizes denoising by integrating GNN layers with an attention mechanism, while the second is obtained via a graph variational autoencoder that aligns the feature distribution with a standard prior. A mutual learning loss function is employed during training to gradually harmonize these representations, enabling the model to capture common patterns and significantly enhance its generalizability. Extensive experiments on several real-world datasets demonstrate that our approach not only improves the numerical accuracy of the predicted scores--achieving up to a 0.8% improvement in RMSE--but also produces superior rankings with improvements of up to 36% in ranking metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨èç³»ç»Ÿä¸­åŸºäºå›¾ç¥ç»ç½‘ç»œ (GNN) çš„çŸ©é˜µè¡¥å…¨ (Matrix Completion) æ–¹æ³•å¯¹å™ªå£°æ•æ„Ÿä¸”æ˜“è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º MCCL (Matrix Completion using Contrastive Learning) çš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸ºæ¯æ¬¡äº¤äº’æå–å±€éƒ¨é‚»åŸŸå­å›¾ï¼Œå¹¶ç”±æ­¤ç”Ÿæˆä¸¤ç§ä¸åŒçš„å›¾è¡¨ç¤ºæ¥æå‡æ¨¡å‹çš„é²æ£’æ€§ã€‚ç¬¬ä¸€ç§è¡¨ç¤ºä¾§é‡äºå»å™ªï¼Œé€šè¿‡ç»“åˆ GNN å±‚ä¸æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism) æ¥è¿‡æ»¤æ— å…³ä¿¡æ¯ï¼›ç¬¬äºŒç§è¡¨ç¤ºåˆ™é€šè¿‡å›¾å˜åˆ†è‡ªç¼–ç å™¨ (Graph Variational Autoencoder) è·å–ï¼Œæ—¨åœ¨å°†ç‰¹å¾åˆ†å¸ƒä¸æ ‡å‡†å…ˆéªŒå¯¹é½ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é‡‡ç”¨äº’å­¦ä¹ æŸå¤±å‡½æ•° (Mutual Learning Loss) é€æ¸åè°ƒè¿™ä¸¤ç§è¡¨ç¤ºï¼Œä½¿å…¶èƒ½å¤Ÿæ•è·å…±åŒæ¨¡å¼å¹¶æ˜¾è‘—å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMCCL ä¸ä»…åœ¨é¢„æµ‹å¾—åˆ†çš„æ•°å€¼å‡†ç¡®åº¦ä¸Šæœ‰æ‰€æå‡ï¼Œå°† RMSE é™ä½äº† 0.8%ï¼Œè€Œä¸”åœ¨æ’åæŒ‡æ ‡ä¸Šå®ç°äº†é«˜è¾¾ 36% çš„æ˜¾è‘—æ”¹è¿›ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.IR",
      "comment": "30 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.10658v1",
      "published_date": "2025-06-12 12:47:35 UTC",
      "updated_date": "2025-06-12 12:47:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:20:49.472572+00:00"
    },
    {
      "arxiv_id": "2506.10647v2",
      "title": "Data Shifts Hurt CoT: A Theoretical Study",
      "title_zh": "æ•°æ®åç§»æŸå®³æ€ç»´é“¾ï¼šç†è®ºç ”ç©¶",
      "authors": [
        "Lang Yin",
        "Debangshu Banerjee",
        "Gagandeep Singh"
      ],
      "abstract": "Chain of Thought (CoT) has been applied to various large language models (LLMs) and proven to be effective in improving the quality of outputs. In recent studies, transformers are proven to have absolute upper bounds in terms of expressive power, and consequently, they cannot solve many computationally difficult problems. However, empowered by CoT, transformers are proven to be able to solve some difficult problems effectively, such as the $k$-parity problem. Nevertheless, those works rely on two imperative assumptions: (1) identical training and testing distribution, and (2) corruption-free training data with correct reasoning steps. However, in the real world, these assumptions do not always hold. Although the risks of data shifts have caught attention, our work is the first to rigorously study the exact harm caused by such shifts to the best of our knowledge. Focusing on the $k$-parity problem, in this work we investigate the joint impact of two types of data shifts: the distribution shifts and data poisoning, on the quality of trained models obtained by a well-established CoT decomposition. In addition to revealing a surprising phenomenon that CoT leads to worse performance on learning parity than directly generating the prediction, our technical results also give a rigorous and comprehensive explanation of the mechanistic reasons of such impact.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹é“¾å¼æ€ç»´(Chain of Thought, CoT)åœ¨æ•°æ®åç§»(Data Shifts)æƒ…å†µä¸‹çš„ç†è®ºè¡¨ç°è¿›è¡Œäº†ä¸¥è°¨æ¢è®¨ï¼Œåˆ†æäº†å…¶åœ¨ç°å®åº”ç”¨ä¸­çš„é²æ£’æ€§é™åˆ¶ã€‚å°½ç®¡CoTè¢«è¯æ˜èƒ½æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)è§£å†³$k$-parityç­‰è®¡ç®—éš¾é¢˜çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰ç†è®ºå¤šåŸºäºè®­ç»ƒä¸æµ‹è¯•åˆ†å¸ƒä¸€è‡´ä¸”æ— æ•°æ®æ±¡æŸ“çš„ç†æƒ³å‡è®¾ã€‚æœ¬æ–‡é¦–æ¬¡é’ˆå¯¹$k$-parityé—®é¢˜ï¼Œæ·±å…¥ç ”ç©¶äº†åˆ†å¸ƒåç§»(Distribution Shifts)ä¸æ•°æ®æŠ•æ¯’(Data Poisoning)å¯¹CoTæ¨¡å‹è´¨é‡çš„å…±åŒå½±å“ã€‚ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªåç›´è§‰çš„å‘ç°ï¼šåœ¨å­˜åœ¨æ•°æ®åç§»æ—¶ï¼ŒCoTåœ¨å­¦ä¹ parityä»»åŠ¡ä¸Šçš„è¡¨ç°åè€ŒåŠ£äºç›´æ¥ç”Ÿæˆé¢„æµ‹ã€‚æœ€åï¼Œè¯¥å·¥ä½œä»ç†è®ºä¸Šä¸ºè¿™ä¸€ç°è±¡èƒŒåçš„æœºæ¢°æ€§åŸå› (Mechanistic Reasons)æä¾›äº†å…¨é¢è§£é‡Šï¼Œä¸ºç†è§£CoTåœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹çš„è¡Œä¸ºæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Comparison to v1: upgraded the quality of a figure",
      "pdf_url": "https://arxiv.org/pdf/2506.10647v2",
      "published_date": "2025-06-12 12:38:04 UTC",
      "updated_date": "2025-06-16 11:57:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:21:44.170296+00:00"
    },
    {
      "arxiv_id": "2506.10634v2",
      "title": "Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models",
      "title_zh": "Symmetrical Flow Matchingï¼šåŸºäºåˆ†æ•°çš„ç”Ÿæˆæ¨¡å‹çš„å›¾åƒç”Ÿæˆã€åˆ†å‰²ä¸åˆ†ç±»ç»Ÿä¸€",
      "authors": [
        "Francisco Caetano",
        "Christiaan Viviers",
        "Peter H. N. De With",
        "Fons van der Sommen"
      ],
      "abstract": "Flow Matching has emerged as a powerful framework for learning continuous transformations between distributions, enabling high-fidelity generative modeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new formulation that unifies semantic segmentation, classification, and image generation within a single model. Using a symmetric learning objective, SymmFlow models forward and reverse transformations jointly, ensuring bi-directional consistency, while preserving sufficient entropy for generative diversity. A new training objective is introduced to explicitly retain semantic information across flows, featuring efficient sampling while preserving semantic structure, allowing for one-step segmentation and classification without iterative refinement. Unlike previous approaches that impose strict one-to-one mapping between masks and images, SymmFlow generalizes to flexible conditioning, supporting both pixel-level and image-level class labels. Experimental results on various benchmarks demonstrate that SymmFlow achieves state-of-the-art performance on semantic image synthesis, obtaining FID scores of 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps. Additionally, it delivers competitive results on semantic segmentation and shows promising capabilities in classification tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Symmetrical Flow Matching (SymmFlow)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å°†è¯­ä¹‰åˆ†å‰² (semantic segmentation)ã€åˆ†ç±» (classification) å’Œå›¾åƒç”Ÿæˆ (image generation) ç»Ÿä¸€åœ¨å•ä¸ªæ¨¡å‹ä¸­çš„æ–°æ¡†æ¶ã€‚SymmFlow é‡‡ç”¨å¯¹ç§°å­¦ä¹ ç›®æ ‡ (symmetric learning objective) åŒæ—¶å»ºæ¨¡æ­£å‘å’Œåå‘å˜æ¢ï¼Œåœ¨ç¡®ä¿åŒå‘ä¸€è‡´æ€§ (bi-directional consistency) çš„åŒæ—¶ï¼Œä¿ç•™äº†ç”Ÿæˆå¤šæ ·æ€§æ‰€éœ€çš„ä¿¡æ¯ç†µã€‚é€šè¿‡å¼•å…¥å…¨æ–°çš„è®­ç»ƒç›®æ ‡ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æµåŠ¨è¿‡ç¨‹ä¸­æ˜¾å¼ä¿ç•™è¯­ä¹‰ç»“æ„ï¼Œä»è€Œæ”¯æŒåœ¨æ— éœ€è¿­ä»£ç²¾ç»†åŒ– (iterative refinement) çš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ•ˆçš„å•æ­¥åˆ†å‰²ä¸åˆ†ç±»ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSymmFlow å®ç°äº†æ›´çµæ´»çš„è°ƒèŠ‚æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶æ”¯æŒåƒç´ çº§å’Œå›¾åƒçº§çš„ç±»åˆ«æ ‡ç­¾è¾“å…¥ã€‚å®éªŒè¯æ˜ï¼ŒSymmFlow åœ¨è¯­ä¹‰å›¾åƒåˆæˆä»»åŠ¡ä¸­è¾¾åˆ°äº† state-of-the-art æ°´å¹³ï¼Œåœ¨ CelebAMask-HQ å’Œ COCO-Stuff ä¸Šåˆ†åˆ«å–å¾—äº† 11.9 å’Œ 7.0 çš„ FID åˆ†æ•°ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨è¯­ä¹‰åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ä¸Šä¹Ÿå±•ç°å‡ºäº†æå…·ç«äº‰åŠ›çš„ç»“æœï¼ŒéªŒè¯äº†å…¶ä½œä¸ºç»Ÿä¸€ç”Ÿæˆæ¨¡å‹å¤„ç†å¤šä»»åŠ¡çš„å¼ºå¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2506.10634v2",
      "published_date": "2025-06-12 12:19:28 UTC",
      "updated_date": "2025-11-14 16:56:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:20:51.816592+00:00"
    },
    {
      "arxiv_id": "2506.10630v1",
      "title": "Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs",
      "title_zh": "æ—¶é—´åºåˆ—é¢„æµ‹å³æ¨ç†ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ å¤§è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒæ–¹æ³•",
      "authors": [
        "Yucong Luo",
        "Yitong Zhou",
        "Mingyue Cheng",
        "Jiahao Wang",
        "Daoyu Wang",
        "Tingyue Pan",
        "Jintao Zhang"
      ],
      "abstract": "To advance time series forecasting (TSF), various methods have been proposed to improve prediction accuracy, evolving from statistical techniques to data-driven deep learning architectures. Despite their effectiveness, most existing methods still adhere to a fast thinking paradigm-relying on extracting historical patterns and mapping them to future values as their core modeling philosophy, lacking an explicit thinking process that incorporates intermediate time series reasoning. Meanwhile, emerging slow-thinking LLMs (e.g., OpenAI-o1) have shown remarkable multi-step reasoning capabilities, offering an alternative way to overcome these issues. However, prompt engineering alone presents several limitations - including high computational cost, privacy risks, and limited capacity for in-depth domain-specific time series reasoning. To address these limitations, a more promising approach is to train LLMs to develop slow thinking capabilities and acquire strong time series reasoning skills. For this purpose, we propose Time-R1, a two-stage reinforcement fine-tuning framework designed to enhance multi-step reasoning ability of LLMs for time series forecasting. Specifically, the first stage conducts supervised fine-tuning for warmup adaptation, while the second stage employs reinforcement learning to improve the model's generalization ability. Particularly, we design a fine-grained multi-objective reward specifically for time series forecasting, and then introduce GRIP (group-based relative importance for policy optimization), which leverages non-uniform sampling to further encourage and optimize the model's exploration of effective reasoning paths. Experiments demonstrate that Time-R1 significantly improves forecast performance across diverse datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶é—´åºåˆ—é¢„æµ‹(Time Series Forecasting)é¢†åŸŸç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–æå–å†å²æ¨¡å¼å¹¶æ˜ å°„åˆ°æœªæ¥å€¼çš„â€œå¿«æ€è€ƒâ€æ¨¡å¼ï¼Œè€Œç¼ºä¹æ˜¾å¼ä¸­é—´æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ï¼Œæå‡ºäº†Time-R1æ¡†æ¶ã€‚Time-R1æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µå¼ºåŒ–å¾®è°ƒ(Reinforcement Fine-tuning)æ¡†æ¶ï¼Œæ—¨åœ¨åŸ¹å…»å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ—¶åºé¢„æµ‹ä¸­çš„â€œæ…¢æ€è€ƒâ€èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åœ¨ç¬¬ä¸€é˜¶æ®µé€šè¿‡ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning)è¿›è¡Œé¢„çƒ­é€‚é…ï¼Œåœ¨ç¬¬äºŒé˜¶æ®µåˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ³›åŒ–æ€§ã€‚ç ”ç©¶å›¢é˜Ÿä¸“é—¨è®¾è®¡äº†ç»†ç²’åº¦çš„å¤šç›®æ ‡å¥–åŠ±æœºåˆ¶ï¼Œå¹¶å¼•å…¥äº†GRIP (Group-based Relative Importance for Policy optimization) ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡éå‡åŒ€é‡‡æ ·å¼•å¯¼æ¨¡å‹æ¢ç´¢æ›´æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTime-R1åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„é¢„æµ‹æ€§èƒ½å‡æœ‰æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å°†æ—¶åºé¢„æµ‹è½¬åŒ–ä¸ºæ¨ç†ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10630v1",
      "published_date": "2025-06-12 12:15:50 UTC",
      "updated_date": "2025-06-12 12:15:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:20:51.540832+00:00"
    },
    {
      "arxiv_id": "2506.10629v1",
      "title": "Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning",
      "title_zh": "åŸºäºæŠ€èƒ½çš„ä»»åŠ¡é€‚é…ï¼šæ— ç›‘ç£å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¿¡æ¯å‡ ä½•ã€è§£è€¦ä¸æ–°ç›®æ ‡",
      "authors": [
        "Yucheng Yang",
        "Tianyi Zhou",
        "Qiang He",
        "Lei Han",
        "Mykola Pechenizkiy",
        "Meng Fang"
      ],
      "abstract": "Unsupervised reinforcement learning (URL) aims to learn general skills for unseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL by maximizing the mutual information between states and skills but lacks sufficient theoretical analysis, e.g., how well its learned skills can initialize a downstream task's policy. Our new theoretical analysis in this paper shows that the diversity and separability of learned skills are fundamentally critical to downstream task adaptation but MISL does not necessarily guarantee these properties. To complement MISL, we propose a novel disentanglement metric LSEPIN. Moreover, we build an information-geometric connection between LSEPIN and downstream task adaptation cost. For better geometric properties, we investigate a new strategy that replaces the KL divergence in information geometry with Wasserstein distance. We extend the geometric analysis to it, which leads to a novel skill-learning objective WSEP. It is theoretically justified to be helpful to downstream task adaptation and it is capable of discovering more initial policies for downstream tasks than MISL. We finally propose another Wasserstein distance-based algorithm PWSEP that can theoretically discover all optimal initial policies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ (Unsupervised Reinforcement Learning)ä¸­ä»»åŠ¡é€‚åº”ä¸æŠ€èƒ½å­¦ä¹ çš„å…³ç³»ï¼Œå¹¶é’ˆå¯¹ç°æœ‰çš„äº’ä¿¡æ¯æŠ€èƒ½å­¦ä¹ (Mutual Information Skill Learning)åœ¨æŒ‡å¯¼ä¸‹æ¸¸ä»»åŠ¡åˆå§‹åŒ–æ–¹é¢çš„ç†è®ºå±€é™æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ä½œè€…æŒ‡å‡ºå­¦åˆ°æŠ€èƒ½çš„å¤šæ ·æ€§å’Œå¯åˆ†ç¦»æ€§å¯¹ä»»åŠ¡é€‚åº”è‡³å…³é‡è¦ï¼Œå¹¶æ®æ­¤æå‡ºäº†ä¸€ç§å…¨æ–°çš„è§£è€¦åº¦é‡æŒ‡æ ‡LSEPINã€‚é€šè¿‡å»ºç«‹LSEPINä¸ä¸‹æ¸¸ä»»åŠ¡é€‚åº”æˆæœ¬ä¹‹é—´çš„ä¿¡æ¯å‡ ä½•(Information Geometry)è”ç³»ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥åˆ©ç”¨Wassersteinè·ç¦»æ›¿æ¢äº†ä¼ ç»Ÿçš„KLæ•£åº¦ï¼Œè¿›è€Œæ¨å¯¼å‡ºæ–°çš„æŠ€èƒ½å­¦ä¹ ç›®æ ‡WSEPã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒWSEPç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•èƒ½å‘ç°æ›´å¤šæœ‰æ•ˆçš„åˆå§‹ç­–ç•¥ï¼Œä»è€Œæ˜¾è‘—æå‡ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ•ˆç‡ã€‚æœ€åï¼Œç ”ç©¶æå‡ºçš„PWSEPç®—æ³•åœ¨ç†è®ºä¸Šå…·å¤‡å‘ç°æ‰€æœ‰æœ€ä¼˜åˆå§‹ç­–ç•¥çš„èƒ½åŠ›ï¼Œä¸ºæ— ç›‘ç£æŠ€èƒ½å­¦ä¹ çš„ä¸‹æ¸¸ä»»åŠ¡è¿ç§»æä¾›äº†å¼ºæœ‰åŠ›çš„ç†è®ºæ”¯æ’‘å’Œç®—æ³•å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "Spotlight paper at ICLR 2024. This version includes acknowledgments omitted from the ICLR version and indicates the corresponding authors primarily responsible for the work",
      "pdf_url": "https://arxiv.org/pdf/2506.10629v1",
      "published_date": "2025-06-12 12:13:58 UTC",
      "updated_date": "2025-06-12 12:13:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:21:50.897944+00:00"
    },
    {
      "arxiv_id": "2506.10627v1",
      "title": "NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors",
      "title_zh": "NeuralNexus å‚åŠ  BEA 2025 å…±äº«ä»»åŠ¡ï¼šé¢å‘ AI è¾…å¯¼ç³»ç»Ÿé”™è¯¯è¯†åˆ«çš„æ£€ç´¢å¢å¼ºæç¤º",
      "authors": [
        "Numaan Naeem",
        "Sarfraz Ahmad",
        "Momina Ahsan",
        "Hasan Iqbal"
      ],
      "abstract": "This paper presents our system for Track 1: Mistake Identification in the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The task involves evaluating whether a tutor's response correctly identifies a mistake in a student's mathematical reasoning. We explore four approaches: (1) an ensemble of machine learning models over pooled token embeddings from multiple pretrained language models (LMs); (2) a frozen sentence-transformer using [CLS] embeddings with an MLP classifier; (3) a history-aware model with multi-head attention between token-level history and response embeddings; and (4) a retrieval-augmented few-shot prompting system with a large language model (LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples, constructs structured prompts, and uses schema-guided output parsing to produce interpretable predictions. It outperforms all baselines, demonstrating the effectiveness of combining example-driven prompting with LLM reasoning for pedagogical feedback assessment. Our code is available at https://github.com/NaumanNaeem/BEA_2025.",
      "tldr_zh": "æœ¬æ–‡ä»‹ç»äº†NeuralNexusç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé’ˆå¯¹BEA 2025 Shared Taskä¸­çš„Mistake Identificationä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°AIå¯¼å¸ˆ(AI Tutors)æ˜¯å¦èƒ½å‡†ç¡®è¯†åˆ«å­¦ç”Ÿæ•°å­¦æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å›¢é˜Ÿæ¢ç´¢äº†å››ç§ä¸»è¦æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(LMs)Token Embeddingsçš„é›†æˆå­¦ä¹ ã€ä½¿ç”¨MLPåˆ†ç±»å™¨çš„Sentence-transformerã€å…·å¤‡Multi-head attentionçš„å†å²æ„ŸçŸ¥æ¨¡å‹ï¼Œä»¥åŠåŸºäºGPT-4oçš„æ£€ç´¢å¢å¼º(Retrieval-augmented)å°‘æ ·æœ¬æç¤ºç³»ç»Ÿã€‚æœ€ç»ˆç³»ç»Ÿé€šè¿‡æ£€ç´¢è¯­ä¹‰ç›¸ä¼¼çš„æ¡ˆä¾‹ï¼Œæ„å»ºç»“æ„åŒ–æç¤ºè¯(Structured Prompts)å¹¶ç»“åˆSchema-guided output parsingï¼Œç”Ÿæˆäº†å…·å¤‡é«˜åº¦å¯è§£é‡Šæ€§çš„é¢„æµ‹ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæ€§èƒ½ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œæœ‰åŠ›è¯æ˜äº†å°†ç¤ºä¾‹é©±åŠ¨çš„æç¤ºæŠ€æœ¯ä¸å¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„æ¨ç†èƒ½åŠ›ç›¸ç»“åˆï¼Œåœ¨æå‡æ•™å­¦åé¦ˆè¯„ä¼°å‡†ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 2 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2506.10627v1",
      "published_date": "2025-06-12 12:11:56 UTC",
      "updated_date": "2025-06-12 12:11:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:22:13.429595+00:00"
    },
    {
      "arxiv_id": "2506.10622v3",
      "title": "SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation",
      "title_zh": "SDialogï¼šé›†ç«¯åˆ°ç«¯æ™ºèƒ½ä½“æ„å»ºã€ç”¨æˆ·æ¨¡æ‹Ÿã€å¯¹è¯ç”ŸæˆåŠè¯„ä¼°äºä¸€ä½“çš„ Python å·¥å…·åŒ…",
      "authors": [
        "Sergio Burdisso",
        "SÃ©verin Baroudi",
        "Yanis Labrak",
        "David Grunert",
        "Pawel Cyrta",
        "Yiyang Chen",
        "Srikanth Madikeri",
        "Thomas Schaaf",
        "EsaÃº Villatoro-Tello",
        "Ahmed Hassoon",
        "Ricard Marxer",
        "Petr Motlicek"
      ],
      "abstract": "We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized Dialog representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†SDialogï¼Œè¿™æ˜¯ä¸€ä¸ªéµå¾ªMITè®¸å¯è¯çš„å¼€æºPythonå·¥å…·åŒ…ï¼Œæ—¨åœ¨å°†å¯¹è¯ç”Ÿæˆã€è¯„ä¼°ä¸æœºæ¢°å¯è§£é‡Šæ€§(Mechanistic Interpretability)ç»Ÿä¸€åˆ°åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¯¹è¯æ™ºèƒ½ä½“æ„å»ºä¸åˆ†ææ¡†æ¶ä¸­ã€‚è¯¥å·¥å…·åŒ…å›´ç»•æ ‡å‡†åŒ–çš„Dialogè¡¨ç¤ºæ„å»ºï¼Œæä¾›è§’è‰²é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿ(Persona-driven Multi-agent Simulation)ä»¥å®ç°å—æ§çš„åˆæˆå¯¹è¯ç”Ÿæˆã€‚åœ¨è¯„ä¼°æ–¹é¢ï¼ŒSDialogæ•´åˆäº†è¯­è¨€æŒ‡æ ‡ã€LLM-as-a-judgeä»¥åŠåŠŸèƒ½æ­£ç¡®æ€§éªŒè¯å™¨ï¼Œæä¾›äº†å…¨é¢çš„æµ‹è¯„ä½“ç³»ã€‚æ­¤å¤–ï¼Œæ¡†æ¶é›†æˆäº†æœºæ¢°å¯è§£é‡Šæ€§å·¥å…·ï¼Œæ”¯æŒé€šè¿‡ç‰¹å¾æ¶ˆè(Feature Ablation)å’Œå½’çº³(Induction)è¿›è¡Œæ¿€æ´»æ£€æŸ¥ä¸è¡Œä¸ºå¼•å¯¼ã€‚SDialogè¿˜å…·å¤‡åŒ…å«3Dæˆ¿é—´å»ºæ¨¡å’Œéº¦å…‹é£æ•ˆåº”åœ¨å†…çš„å…¨å£°å­¦æ¨¡æ‹ŸéŸ³é¢‘ç”ŸæˆåŠŸèƒ½ï¼Œå¹¶èƒ½é€šè¿‡ç»Ÿä¸€APIå…¼å®¹æ‰€æœ‰ä¸»æµLLMåç«¯ã€‚è¿™ç§å°†ç”Ÿæˆã€è¯„ä¼°ä¸å¯è§£é‡Šæ€§æ·±åº¦è€¦åˆçš„å¯¹è¯ä¸­å¿ƒåŒ–æ¶æ„ï¼Œä¸ºç ”ç©¶äººå‘˜ç³»ç»Ÿåœ°æ„å»ºã€åŸºå‡†æµ‹è¯•å’Œåˆ†æå¯¹è¯ç³»ç»Ÿæä¾›äº†é«˜æ•ˆçš„ç«¯åˆ°ç«¯å·¥å…·æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Pre-print submitted to EACL System Demonstration (under review)",
      "pdf_url": "https://arxiv.org/pdf/2506.10622v3",
      "published_date": "2025-06-12 12:07:51 UTC",
      "updated_date": "2026-01-16 11:06:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:21:17.530227+00:00"
    },
    {
      "arxiv_id": "2506.10617v1",
      "title": "Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„é‡å å¿ƒç”µå›¾å›¾åƒæ•°å­—åŒ–åŠå¼€æº Python ä»£ç ",
      "authors": [
        "Reza Karbasi",
        "Masoud Rahimi",
        "Abdol-Hossein Vahabie",
        "Hadi Moradi"
      ],
      "abstract": "This paper addresses the persistent challenge of accurately digitizing paper-based electrocardiogram (ECG) recordings, with a particular focus on robustly handling single leads compromised by signal overlaps-a common yet under-addressed issue in existing methodologies. We propose a two-stage pipeline designed to overcome this limitation. The first stage employs a U-Net based segmentation network, trained on a dataset enriched with overlapping signals and fortified with custom data augmentations, to accurately isolate the primary ECG trace. The subsequent stage converts this refined binary mask into a time-series signal using established digitization techniques, enhanced by an adaptive grid detection module for improved versatility across different ECG formats and scales. Our experimental results demonstrate the efficacy of our approach. The U-Net architecture achieves an IoU of 0.87 for the fine-grained segmentation task. Crucially, our proposed digitization method yields superior performance compared to a well-established baseline technique across both non-overlapping and challenging overlapping ECG samples. For non-overlapping signals, our method achieved a Mean Squared Error (MSE) of 0.0010 and a Pearson Correlation Coefficient (rho) of 0.9644, compared to 0.0015 and 0.9366, respectively, for the baseline. On samples with signal overlap, our method achieved an MSE of 0.0029 and a rho of 0.9641, significantly improving upon the baseline's 0.0178 and 0.8676. This work demonstrates an effective strategy to significantly enhance digitization accuracy, especially in the presence of signal overlaps, thereby laying a strong foundation for the reliable conversion of analog ECG records into analyzable digital data for contemporary research and clinical applications. The implementation is publicly available at this GitHub repository: https://github.com/masoudrahimi39/ECG-code.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çº¸è´¨å¿ƒç”µå›¾(ECG)æ•°å­—åŒ–è¿‡ç¨‹ä¸­éš¾ä»¥å‡†ç¡®å¤„ç†é‡å ä¿¡å·çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ä¸¤é˜¶æ®µå¤„ç†æµæ°´çº¿ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨U-Netåˆ†å‰²ç½‘ç»œï¼Œå¹¶ç»“åˆè‡ªå®šä¹‰æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œèƒ½å¤Ÿä»å¤æ‚çš„èƒŒæ™¯ä¸­ç²¾ç¡®åˆ†ç¦»å‡ºä¸»è¦çš„ECGè½¨è¿¹ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨è‡ªé€‚åº”ç½‘æ ¼æ£€æµ‹æ¨¡å—ï¼Œå°†åˆ†å‰²å‡ºçš„äºŒè¿›åˆ¶æ©ç é«˜æ•ˆè½¬æ¢ä¸ºæ ‡å‡†çš„æ—¶é—´åºåˆ—ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒU-Netæ¶æ„åœ¨ç»†ç²’åº¦åˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†0.87çš„IoUã€‚åœ¨å¤„ç†é‡å ä¿¡å·çš„ä¸¥è‹›æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å°†Pearsonç›¸å…³ç³»æ•°(rho)æå‡è‡³0.9641ï¼Œç›¸è¾ƒäºåŸºå‡†æ¨¡å‹çš„0.8676å…·æœ‰æ˜¾è‘—æå‡ã€‚è¯¥é¡¹å·¥ä½œä¸ºå°†æ¨¡æ‹Ÿå¿ƒç”µè®°å½•å¯é è½¬åŒ–ä¸ºç°ä»£ä¸´åºŠç ”ç©¶æ‰€éœ€çš„æ•°å­—æ•°æ®æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œå¹¶å…¬å¼€äº†å…¶Pythonä»£ç å®ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10617v1",
      "published_date": "2025-06-12 12:00:34 UTC",
      "updated_date": "2025-06-12 12:00:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:21:06.733521+00:00"
    },
    {
      "arxiv_id": "2506.10613v1",
      "title": "Data Driven Diagnosis for Large Cyber-Physical-Systems with Minimal Prior Information",
      "title_zh": "åŸºäºæå°‘å…ˆéªŒä¿¡æ¯çš„å¤§å‹ä¿¡æ¯ç‰©ç†ç³»ç»Ÿæ•°æ®é©±åŠ¨è¯Šæ–­",
      "authors": [
        "Henrik Sebastian Steude",
        "Alexander Diedrich",
        "Ingo Pill",
        "Lukas Moddemann",
        "Daniel VranjeÅ¡",
        "Oliver Niggemann"
      ],
      "abstract": "Diagnostic processes for complex cyber-physical systems often require extensive prior knowledge in the form of detailed system models or comprehensive training data. However, obtaining such information poses a significant challenge. To address this issue, we present a new diagnostic approach that operates with minimal prior knowledge, requiring only a basic understanding of subsystem relationships and data from nominal operations. Our method combines a neural network-based symptom generator, which employs subsystem-level anomaly detection, with a new graph diagnosis algorithm that leverages minimal causal relationship information between subsystems-information that is typically available in practice. Our experiments with fully controllable simulated datasets show that our method includes the true causal component in its diagnosis set for 82 p.c. of all cases while effectively reducing the search space in 73 p.c. of the scenarios. Additional tests on the real-world Secure Water Treatment dataset showcase the approach's potential for practical scenarios. Our results thus highlight our approach's potential for practical applications with large and complex cyber-physical systems where limited prior knowledge is available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹å¤æ‚ä¿¡æ¯ç‰©ç†ç³»ç»Ÿ(Cyber-Physical-Systems, CPS)åœ¨æ•…éšœè¯Šæ–­ä¸­å¯¹è¯¦ç»†ç³»ç»Ÿæ¨¡å‹å’Œå…¨é¢è®­ç»ƒæ•°æ®ä¾èµ–æ€§å¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»…éœ€æå°‘å…ˆéªŒä¿¡æ¯çš„åˆ›æ–°è¯Šæ–­æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä»…ä¾èµ–å­ç³»ç»Ÿé—´çš„åŸºæœ¬å…³ç³»åŠæ ‡ç§°æ“ä½œ(nominal operations)æ•°æ®ï¼Œé€šè¿‡ç»“åˆåŸºäºç¥ç»ç½‘ç»œçš„ç—‡çŠ¶ç”Ÿæˆå™¨(symptom generator)æ‰§è¡Œå­ç³»ç»Ÿçº§å¼‚å¸¸æ£€æµ‹ï¼Œå¹¶åˆ©ç”¨æœ€å°å› æœå…³ç³»(minimal causal relationship)ä¿¡æ¯æ„å»ºå›¾è¯Šæ–­ç®—æ³•ã€‚åœ¨æ¨¡æ‹Ÿæ•°æ®é›†çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨82%çš„æ¡ˆä¾‹ä¸­èƒ½åŒ…å«çœŸå®çš„å› æœç»„ä»¶ï¼Œå¹¶åœ¨73%çš„æƒ…å¢ƒä¸‹æœ‰æ•ˆç¼©å°äº†æœç´¢ç©ºé—´ã€‚æ­¤å¤–ï¼Œåœ¨çœŸå®çš„Secure Water Treatmentæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†å…ˆéªŒçŸ¥è¯†æœ‰é™çš„å¤§å‹å¤æ‚CPSæ—¶çš„å®ç”¨æ½œåŠ›ä¸é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10613v1",
      "published_date": "2025-06-12 11:56:58 UTC",
      "updated_date": "2025-06-12 11:56:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:21:37.678151+00:00"
    },
    {
      "arxiv_id": "2506.10612v1",
      "title": "TexTailor: Customized Text-aligned Texturing via Effective Resampling",
      "title_zh": "TexTailorï¼šåŸºäºé«˜æ•ˆé‡é‡‡æ ·å®ç°å®šåˆ¶åŒ–æ–‡æœ¬å¯¹é½çš„çº¹ç†ç”Ÿæˆ",
      "authors": [
        "Suin Lee",
        "Dae-Shik Kim"
      ],
      "abstract": "We present TexTailor, a novel method for generating consistent object textures from textual descriptions. Existing text-to-texture synthesis approaches utilize depth-aware diffusion models to progressively generate images and synthesize textures across predefined multiple viewpoints. However, these approaches lead to a gradual shift in texture properties across viewpoints due to (1) insufficient integration of previously synthesized textures at each viewpoint during the diffusion process and (2) the autoregressive nature of the texture synthesis process. Moreover, the predefined selection of camera positions, which does not account for the object's geometry, limits the effective use of texture information synthesized from different viewpoints, ultimately degrading overall texture consistency. In TexTailor, we address these issues by (1) applying a resampling scheme that repeatedly integrates information from previously synthesized textures within the diffusion process, and (2) fine-tuning a depth-aware diffusion model on these resampled textures. During this process, we observed that using only a few training images restricts the model's original ability to generate high-fidelity images aligned with the conditioning, and therefore propose an performance preservation loss to mitigate this issue. Additionally, we improve the synthesis of view-consistent textures by adaptively adjusting camera positions based on the object's geometry. Experiments on a subset of the Objaverse dataset and the ShapeNet car dataset demonstrate that TexTailor outperforms state-of-the-art methods in synthesizing view-consistent textures. The source code for TexTailor is available at https://github.com/Adios42/Textailor",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TexTailorï¼Œä¸€ç§æ—¨åœ¨ä»æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆä¸€è‡´æ€§ç‰©ä½“çº¹ç†çš„åˆ›æ–°æ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰ depth-aware diffusion models åœ¨å¤šè§†è§’åˆæˆè¿‡ç¨‹ä¸­ç”±äºè‡ªå›å½’ç‰¹æ€§å¯¼è‡´çš„çº¹ç†åç§»é—®é¢˜ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§é‡é‡‡æ ·(resampling)æ–¹æ¡ˆï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­é‡å¤æ•´åˆå…ˆå‰ç”Ÿæˆçš„çº¹ç†ä¿¡æ¯ï¼Œå¹¶é€šè¿‡å¯¹è¿™äº›çº¹ç†è¿›è¡Œå¾®è°ƒæ¥ç¡®ä¿å…¨å±€ä¸€è‡´æ€§ã€‚ä¸ºäº†åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸‹ä¿æŒæ¨¡å‹ç”Ÿæˆé«˜ä¿çœŸå›¾åƒçš„èƒ½åŠ›ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§æ€§èƒ½ä¿æŒæŸå¤±(performance preservation loss)æ¥ç¼“è§£ç”Ÿæˆè´¨é‡é€€åŒ–ã€‚æ­¤å¤–ï¼ŒTexTailor èƒ½å¤Ÿæ ¹æ®ç‰©ä½“çš„å‡ ä½•ç‰¹å¾è‡ªé€‚åº”è°ƒæ•´ç›¸æœºä½ç½®ï¼Œä»è€Œä¼˜åŒ–äº†è§†è§’é—´çš„ä¿¡æ¯åˆ©ç”¨ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTexTailor åœ¨ Objaverse å’Œ ShapeNet car æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç›®å‰çš„å…ˆè¿›æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†è§†è§’ä¸€è‡´æ€§çº¹ç†çš„åˆæˆè´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to ICLR 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10612v1",
      "published_date": "2025-06-12 11:55:44 UTC",
      "updated_date": "2025-06-12 11:55:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:21:56.712754+00:00"
    },
    {
      "arxiv_id": "2507.00011v1",
      "title": "Novel RL approach for efficient Elevator Group Control Systems",
      "title_zh": "é¢å‘é«˜æ•ˆç”µæ¢¯ç¾¤æ§ç³»ç»Ÿçš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Nathan Vaartjes",
        "Vincent Francois-Lavet"
      ],
      "abstract": "Efficient elevator traffic management in large buildings is critical for minimizing passenger travel times and energy consumption. Because heuristic- or pattern-detection-based controllers struggle with the stochastic and combinatorial nature of dispatching, we model the six-elevator, fifteen-floor system at Vrije Universiteit Amsterdam as a Markov Decision Process and train an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS). Key innovations include a novel action space encoding to handle the combinatorial complexity of elevator dispatching, the introduction of infra-steps to model continuous passenger arrivals, and a tailored reward signal to improve learning efficiency. In addition, we explore various ways to adapt the discounting factor to the infra-step formulation. We investigate RL architectures based on Dueling Double Deep Q-learning, showing that the proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a highly stochastic environment, and thereby outperforms a traditional rule-based algorithm.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹å»ºç­‘ä¸­ç”µæ¢¯äº¤é€šç®¡ç†çš„å¤æ‚æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºReinforcement Learning (RL)çš„ç”µæ¢¯ç¾¤æ§ç³»ç»Ÿ(Elevator Group Control Systems, EGCS)æ–¹æ³•ã€‚ä½œè€…å°†åŒ…å«6éƒ¨ç”µæ¢¯ã€15å±‚æ¥¼çš„ç³»ç»Ÿå»ºæ¨¡ä¸ºMarkov Decision Process (MDP)ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ™ºèƒ½ä½“ã€‚æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ç”¨äºå¤„ç†è°ƒåº¦ç»„åˆå¤æ‚æ€§çš„æ–°å‹åŠ¨ä½œç©ºé—´ç¼–ç (Action Space Encoding)ã€æ¨¡æ‹Ÿä¹˜å®¢è¿ç»­åˆ°è¾¾çš„infra-stepsæœºåˆ¶ï¼Œä»¥åŠä¼˜åŒ–å­¦ä¹ æ•ˆç‡çš„å®šåˆ¶å¥–åŠ±ä¿¡å·ã€‚ç ”ç©¶è¿˜æ¢ç´¢äº†é’ˆå¯¹infra-stepsè¡¨è¿°è°ƒæ•´æŠ˜æ‰£å› å­çš„å¤šç§æ–¹å¼ã€‚é€šè¿‡åŸºäºDueling Double Deep Q-learningçš„æ¶æ„å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”æ³¢åŠ¨äº¤é€šæ¨¡å¼å¹¶åœ¨é«˜åº¦éšæœºçš„ç¯å¢ƒä¸­å­¦ä¹ ã€‚å®éªŒç»“æœè¯æ˜è¯¥RL-based EGCSçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„Rule-basedç®—æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.00011v1",
      "published_date": "2025-06-12 11:54:59 UTC",
      "updated_date": "2025-06-12 11:54:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:22:37.041615+00:00"
    },
    {
      "arxiv_id": "2506.22447v1",
      "title": "Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture",
      "title_zh": "é¢å‘å¤šå˜é‡æ°”å€™é™å°ºåº¦çš„ Vision Transformerï¼šé‡‡ç”¨å…±äº«ç¼–ç å™¨ä¸å¤šè§£ç å™¨æ¶æ„æ¨¡æ‹ŸåŒºåŸŸæ°”å€™æ¨¡å‹",
      "authors": [
        "Fabio Merizzi",
        "Harilaos Loukos"
      ],
      "abstract": "Global Climate Models (GCMs) are critical for simulating large-scale climate dynamics, but their coarse spatial resolution limits their applicability in regional studies. Regional Climate Models (RCMs) refine this through dynamic downscaling, albeit at considerable computational cost and with limited flexibility. While deep learning has emerged as an efficient data-driven alternative, most existing studies have focused on single-variable models that downscale one variable at a time. This approach can lead to limited contextual awareness, redundant computation, and lack of cross-variable interaction. Our study addresses these limitations by proposing a multi-task, multi-variable Vision Transformer (ViT) architecture with a shared encoder and variable-specific decoders (1EMD). The proposed architecture jointly predicts three key climate variables: surface temperature (tas), wind speed (sfcWind), and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs, emulating RCM-scale downscaling over Europe. We show that our multi-variable approach achieves positive cross-variable knowledge transfer and consistently outperforms single-variable baselines trained under identical conditions, while also improving computational efficiency. These results demonstrate the effectiveness of multi-variable modeling for high-resolution climate downscaling.",
      "tldr_zh": "å…¨çƒæ°”å€™æ¨¡å‹(GCMs)ç”±äºç©ºé—´åˆ†è¾¨ç‡ç²—ç³™é™åˆ¶äº†åŒºåŸŸåº”ç”¨ï¼Œè€ŒåŒºåŸŸæ°”å€™æ¨¡å‹(RCMs)çš„åŠ¨åŠ›é™å°ºåº¦(dynamic downscaling)è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œä¸”ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•å¤šèšç„¦äºå•å˜é‡æ¨¡å‹ï¼Œç¼ºä¹ä¸Šä¸‹æ–‡å…³è”ä¸è·¨å˜é‡äº¤äº’ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡ã€å¤šå˜é‡çš„Vision Transformer (ViT)æ¶æ„ï¼Œè¯¥æ¶æ„é‡‡ç”¨å…±äº«ç¼–ç å™¨ä¸å˜é‡ç‰¹å®šè§£ç å™¨(1EMD)çš„è®¾è®¡ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿé«˜åˆ†è¾¨ç‡çš„åŒºåŸŸé™å°ºåº¦ã€‚è¯¥æ¨¡å‹ç›´æ¥ä»GCMåˆ†è¾¨ç‡çš„è¾“å…¥ä¸­åŒæ—¶é¢„æµ‹åœ°è¡¨æ¸©åº¦(tas)ã€é£é€Ÿ(sfcWind)å’Œ500 hPaä½åŠ¿é«˜åº¦(zg500)ä¸‰ä¸ªå…³é”®æ°”å€™å˜é‡ï¼Œå¹¶åœ¨æ¬§æ´²åŒºåŸŸè¿›è¡Œäº†éªŒè¯ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§å¤šå˜é‡æ–¹æ³•å®ç°äº†ç§¯æçš„è·¨å˜é‡çŸ¥è¯†è½¬ç§»(cross-variable knowledge transfer)ï¼Œåœ¨è®¡ç®—æ•ˆç‡æå‡çš„åŒæ—¶ï¼Œæ€§èƒ½è¡¨ç°æŒç»­ä¼˜äºä¼ ç»Ÿçš„å•å˜é‡åŸºå‡†æ¨¡å‹ã€‚è¿™ä¸€è¿›å±•å……åˆ†è¯æ˜äº†å¤šå˜é‡å»ºæ¨¡åœ¨å®ç°é«˜æ•ˆã€é«˜åˆ†è¾¨ç‡æ°”å€™é™å°ºåº¦(climate downscaling)æ–¹é¢çš„æœ‰æ•ˆæ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.22447v1",
      "published_date": "2025-06-12 11:48:41 UTC",
      "updated_date": "2025-06-12 11:48:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:22:38.101200+00:00"
    },
    {
      "arxiv_id": "2506.10597v2",
      "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
      "title_zh": "SoKï¼šå¤§è¯­è¨€æ¨¡å‹è¶Šç‹±æŠ¤æ è¯„ä¼°",
      "authors": [
        "Xunguang Wang",
        "Zhenlan Ji",
        "Wenxuan Wang",
        "Zongjie Li",
        "Daoyuan Wu",
        "Shuai Wang"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable progress, but their deployment has exposed critical vulnerabilities, particularly to jailbreak attacks that circumvent safety alignments. Guardrails--external defense mechanisms that monitor and control LLM interactions--have emerged as a promising solution. However, the current landscape of LLM guardrails is fragmented, lacking a unified taxonomy and comprehensive evaluation framework. In this Systematization of Knowledge (SoK) paper, we present the first holistic analysis of jailbreak guardrails for LLMs. We propose a novel, multi-dimensional taxonomy that categorizes guardrails along six key dimensions, and introduce a Security-Efficiency-Utility evaluation framework to assess their practical effectiveness. Through extensive analysis and experiments, we identify the strengths and limitations of existing guardrail approaches, provide insights into optimizing their defense mechanisms, and explore their universality across attack types. Our work offers a structured foundation for future research and development, aiming to guide the principled advancement and deployment of robust LLM guardrails. The code is available at https://github.com/xunguangwang/SoK4JailbreakGuardrails.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šç‹±æ”»å‡»é˜²å¾¡æœºåˆ¶ç¢ç‰‡åŒ–ä¸”ç¼ºä¹ç»Ÿä¸€è¯„ä¼°æ¡†æ¶çš„ç°çŠ¶ï¼Œé¦–æ¬¡å¯¹è¶Šç‹±é˜²æŠ¤æ ï¼ˆJailbreak Guardrailsï¼‰è¿›è¡Œäº†å…¨é¢ç³»ç»Ÿçš„SoKï¼ˆçŸ¥è¯†ç³»ç»ŸåŒ–ï¼‰åˆ†æã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ¶µç›–å…­ä¸ªå…³é”®ç»´åº¦çš„å¤šç»´åˆ†ç±»æ³•ï¼Œå¹¶å¼•å…¥äº†Security-Efficiency-Utilityï¼ˆå®‰å…¨æ€§-æ•ˆç‡-æ•ˆç”¨ï¼‰è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œä½œè€…æ·±å…¥åˆ†æäº†ç°æœ‰é˜²æŠ¤æ æŠ€æœ¯çš„ä¼˜ç¼ºç‚¹ï¼Œæ¢è®¨äº†å®ƒä»¬åœ¨ä¸åŒæ”»å‡»ç±»å‹ä¸‹çš„æ™®é€‚æ€§ï¼Œå¹¶æå‡ºäº†ä¼˜åŒ–é˜²å¾¡æœºåˆ¶çš„è§è§£ã€‚è¿™é¡¹å·¥ä½œä¸ºæœªæ¥å¼€å‘å’Œéƒ¨ç½²é²æ£’çš„LLMé˜²æŠ¤æ å¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œç›¸å…³çš„è¯„ä¼°ä»£ç å·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by IEEE S&P 2026 Cycle 1",
      "pdf_url": "https://arxiv.org/pdf/2506.10597v2",
      "published_date": "2025-06-12 11:42:40 UTC",
      "updated_date": "2025-10-16 12:15:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:22:35.892395+00:00"
    },
    {
      "arxiv_id": "2506.10586v1",
      "title": "Size-adaptive Hypothesis Testing for Fairness",
      "title_zh": "è§„æ¨¡è‡ªé€‚åº”çš„å…¬å¹³æ€§å‡è®¾æ£€éªŒ",
      "authors": [
        "Antonio Ferrara",
        "Francesco Cozzi",
        "Alan Perotti",
        "AndrÃ© Panisson",
        "Francesco Bonchi"
      ],
      "abstract": "Determining whether an algorithmic decision-making system discriminates against a specific demographic typically involves comparing a single point estimate of a fairness metric against a predefined threshold. This practice is statistically brittle: it ignores sampling error and treats small demographic subgroups the same as large ones. The problem intensifies in intersectional analyses, where multiple sensitive attributes are considered jointly, giving rise to a larger number of smaller groups. As these groups become more granular, the data representing them becomes too sparse for reliable estimation, and fairness metrics yield excessively wide confidence intervals, precluding meaningful conclusions about potential unfair treatments.\n  In this paper, we introduce a unified, size-adaptive, hypothesis-testing framework that turns fairness assessment into an evidence-based statistical decision. Our contribution is twofold. (i) For sufficiently large subgroups, we prove a Central-Limit result for the statistical parity difference, leading to analytic confidence intervals and a Wald test whose type-I (false positive) error is guaranteed at level $Î±$. (ii) For the long tail of small intersectional groups, we derive a fully Bayesian Dirichlet-multinomial estimator; Monte-Carlo credible intervals are calibrated for any sample size and naturally converge to Wald intervals as more data becomes available. We validate our approach empirically on benchmark datasets, demonstrating how our tests provide interpretable, statistically rigorous decisions under varying degrees of data availability and intersectionality.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ç®—æ³•å†³ç­–ç³»ç»Ÿä¸­å…¬å¹³æ€§è¯„ä¼°ç»Ÿè®¡è„†å¼±æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ size-adaptive hypothesis-testing æ¡†æ¶ã€‚ä½œè€…æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„å•ç‚¹ä¼°è®¡å¿½ç•¥äº†æŠ½æ ·è¯¯å·®ï¼Œå°¤å…¶åœ¨æ¶‰åŠå¤šä¸ªæ•æ„Ÿå±æ€§çš„ intersectional analyses ä¸­ï¼Œç”±äºå°æ ·æœ¬ç¾¤ä½“çš„ç»Ÿè®¡ç¨€ç–æ€§ï¼Œå¯¼è‡´å…¬å¹³æ€§åº¦é‡éš¾ä»¥å¾—å‡ºå¯é ç»“è®ºã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶é’ˆå¯¹è¶³å¤Ÿå¤§çš„å­ç¾¤ä½“è¯æ˜äº† statistical parity difference çš„ Central-Limit resultï¼Œå¹¶æ¨å¯¼å‡ºå…·æœ‰ç¬¬ä¸€ç±»é”™è¯¯ä¿è¯çš„ Wald testã€‚é’ˆå¯¹å°è§„æ¨¡äº¤å‰ç¾¤ä½“çš„é•¿å°¾åˆ†å¸ƒï¼Œç ”ç©¶å¼•å…¥äº†å…¨è´å¶æ–¯ Dirichlet-multinomial estimatorï¼Œå…¶ Monte-Carlo credible intervals èƒ½å¤Ÿè‡ªé€‚åº”ä¸åŒæ ·æœ¬é‡å¹¶éšæ•°æ®å¢åŠ å‘ Wald æ£€éªŒæ”¶æ•›ã€‚å®éªŒåœ¨åŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸åŒæ•°æ®ä¸°åº¦ä¸‹å‡èƒ½æä¾›å…·å¤‡ç»Ÿè®¡ä¸¥å¯†æ€§å’Œå¯è§£é‡Šæ€§çš„å…¬å¹³æ€§è¯„ä¼°å†³ç­–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10586v1",
      "published_date": "2025-06-12 11:22:09 UTC",
      "updated_date": "2025-06-12 11:22:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:22:40.920948+00:00"
    },
    {
      "arxiv_id": "2506.10585v1",
      "title": "Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning",
      "title_zh": "Primender åºåˆ—ï¼šä¸€ç§ç”¨äºæµ‹è¯•ç¬¦å·æ¨ç†ä¸äººå·¥æ™ºèƒ½æ¨ç†çš„æ–°å‹æ•°å­¦æ„é€ ",
      "authors": [
        "Mohd Anwar Jamal Faiz"
      ],
      "abstract": "This paper introduces the Primender sequence, a novel integer sequence defined by a hybrid rule that combines classical primality with modular digit-based conditions. Specifically, a number n is included in the sequence if it is prime or ends with a prime number of unit digit or any length. In other words, numbers which are primes or have at least one prime suffix. The resulting sequence exhibits a deterministic yet non-trivial structure, blending number-theoretic properties with symbolic patterning. We propose the Primender sequence as a benchmark for evaluating the symbolic reasoning capabilities of Large Language Models (LLMs). The study is motivated by the need for interpretable, rule-based testbeds that can assess an LLM's ability to infer hidden rules, validate mathematical hypotheses, and generalize symbolic logic at scale. A key hypothesis explored is: Whenever a number in the Primender sequence is exactly one more than the largest prime less than or equal to it, the difference between it and the previous number in the sequence is also 1. We design a structured prompt and evaluation framework to test this hypothesis across multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek, Gemini, Grok, and LLaMA. The models are tasked with identifying the underlying rule, validating the hypothesis, and generating the next 100,000 terms of the sequence. Comparative metrics such as rule inference accuracy, hypothesis evaluation, sequence validity, and symbolic explanation quality are used to assess model performance. This work contributes a novel mathematical construct and a reproducible methodology for benchmarking LLMs in symbolic reasoning, hypothesis testing, and scalable pattern generalization - bridging the domains of number theory, artificial intelligence, and software engineering.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Primender Sequenceï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†ç»å…¸ç´ æ€§ï¼ˆprimalityï¼‰ä¸æ¨¡æ•°ä½æ¡ä»¶çš„æ–°å‹æ•´æ•°åºåˆ—ï¼Œå®šä¹‰ä¸ºç´ æ•°æˆ–å…·æœ‰è‡³å°‘ä¸€ä¸ªç´ æ•°åç¼€çš„æ•°å­—ã€‚è¯¥åºåˆ—è¢«æå‡ºä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç¬¦å·æ¨ç†ï¼ˆSymbolic Inferenceï¼‰èƒ½åŠ›çš„åŸºå‡†ï¼Œæ—¨åœ¨æµ‹è¯•æ¨¡å‹æ¨æ–­éšè—è§„åˆ™ã€éªŒè¯æ•°å­¦å‡è®¾ä»¥åŠè¿›è¡Œé€»è¾‘æ³›åŒ–çš„èƒ½åŠ›ã€‚ç ”ç©¶é‡ç‚¹æ¢è®¨äº†ä¸€ä¸ªå…³äºåºåˆ—é¡¹ä¸å…¶æœ€å¤§ç´ æ•°å‰é©±ä¹‹é—´å…³ç³»çš„æ•°å­¦å‡è®¾ï¼Œå¹¶è®¾è®¡äº†ç»“æ„åŒ–æç¤ºæ¡†æ¶åœ¨ ChatGPTã€Copilotã€DeepSeekã€Geminiã€Grok å’Œ LLaMA ç­‰ä¸»æµæ¨¡å‹ä¸Šè¿›è¡Œå®éªŒã€‚é€šè¿‡è§„åˆ™æ¨æ–­å‡†ç¡®æ€§ã€åºåˆ—æœ‰æ•ˆæ€§å’Œç¬¦å·è§£é‡Šè´¨é‡ç­‰æŒ‡æ ‡ï¼Œè¯¥ç ”ç©¶å®šé‡è¯„ä¼°äº†å„æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ•°å­¦æ„é€ æ—¶çš„è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œä¸ä»…è´¡çŒ®äº†ä¸€ç§æ–°é¢–çš„æ•°è®ºæ„é€ ï¼Œè¿˜ä¸º AI åœ¨ç¬¦å·é€»è¾‘å’Œæ¨¡å¼æ³›åŒ–é¢†åŸŸçš„åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€å¥—å¯é‡å¤çš„å®éªŒæ–¹æ³•ï¼Œå¼¥è¡¥äº†æ•°è®ºä¸äººå·¥æ™ºèƒ½è¯„ä¼°ä¹‹é—´çš„ç¼ºå£ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 7 figures, 2 tables, 3 codes, oeis sequence A384735",
      "pdf_url": "https://arxiv.org/pdf/2506.10585v1",
      "published_date": "2025-06-12 11:21:58 UTC",
      "updated_date": "2025-06-12 11:21:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:22:46.005862+00:00"
    },
    {
      "arxiv_id": "2506.10568v2",
      "title": "DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers",
      "title_zh": "DreamActor-H1ï¼šåŸºäºè¿åŠ¨è®¾è®¡æ‰©æ•£ Transformer çš„é«˜ä¿çœŸäººç‰©-å•†å“æ¼”ç¤ºè§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Lizhen Wang",
        "Zhurong Xia",
        "Tianshu Hu",
        "Pengrui Wang",
        "Pengfei Wei",
        "Zerong Zheng",
        "Ming Zhou",
        "Yuan Zhang",
        "Mingyuan Gao"
      ],
      "abstract": "In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://lizhenwangt.github.io/DreamActor-H1/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DreamActor-H1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Diffusion Transformer (DiT) çš„æ¡†æ¶ï¼Œä¸“æ³¨äºç”Ÿæˆé«˜ä¿çœŸçš„äººè´§æ¼”ç¤ºè§†é¢‘ï¼Œä»¥è§£å†³ç°æœ‰æ¨¡å‹åœ¨èº«ä»½ä¿æŒå’Œç©ºé—´äº¤äº’ç†è§£ä¸Šçš„å±€é™ã€‚è¯¥æ–¹æ³•é€šè¿‡æ³¨å…¥é…å¯¹çš„äººè´§å‚è€ƒä¿¡æ¯å¹¶ç»“åˆ masked cross-attention æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶ç²¾ç¡®è¿˜åŸäººç±»ç‰¹å¾ä¸äº§å“çš„ logoã€çº¹ç†ç­‰ç»†èŠ‚ã€‚ä¸ºäº†å®ç°è‡ªç„¶çš„æ‰‹åŠ¿ä¸äº§å“å¯¹é½ï¼Œç ”ç©¶åˆ©ç”¨ 3D body mesh æ¨¡æ¿å’Œäº§å“ bounding boxes æä¾›ç²¾ç¡®çš„è¿åŠ¨å¼•å¯¼ã€‚æ­¤å¤–ï¼Œæ¡†æ¶é‡‡ç”¨ç»“æ„åŒ–æ–‡æœ¬ç¼–ç (structured text encoding)æ¥æ•´åˆç±»åˆ«è¯­ä¹‰ï¼Œæ˜¾è‘—æå‡äº†å¸§é—´æ—‹è½¬å˜åŒ–æ—¶çš„ 3D ä¸€è‡´æ€§ã€‚åœ¨æ··åˆæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDreamActor-H1 åœ¨ç»´æŠ¤èº«ä»½å®Œæ•´æ€§åŠç”Ÿæˆé€¼çœŸæ¼”ç¤ºåŠ¨ä½œæ–¹é¢ä¼˜äºå½“å‰çš„ state-of-the-art æŠ€æœ¯ï¼Œä¸ºç”µå•†è§†é¢‘ç”Ÿæˆæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10568v2",
      "published_date": "2025-06-12 10:58:23 UTC",
      "updated_date": "2025-08-27 03:34:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:22:51.434313+00:00"
    },
    {
      "arxiv_id": "2506.10564v2",
      "title": "Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics",
      "title_zh": "åˆ†å¸ƒæ¯”è¾ƒä¸­çš„å°¾éƒ¨å¹³è¡¡ï¼šç»¼åˆå…¬å¹³æŒ‡æ•°ï¼ˆCEIï¼‰åŠå…¶åœ¨ä¸šåŠ¡åŒ–äººè„¸è¯†åˆ«åå·®è¯„ä¼°ä¸­çš„åº”ç”¨",
      "authors": [
        "Imanol Solano",
        "Julian Fierrez",
        "Aythami Morales",
        "Alejandro PeÃ±a",
        "Ruben Tolosana",
        "Francisco Zamora-Martinez",
        "Javier San Agustin"
      ],
      "abstract": "Demographic bias in high-performance face recognition (FR) systems often eludes detection by existing metrics, especially with respect to subtle disparities in the tails of the score distribution. We introduce the Comprehensive Equity Index (CEI), a novel metric designed to address this limitation. CEI uniquely analyzes genuine and impostor score distributions separately, enabling a configurable focus on tail probabilities while also considering overall distribution shapes. Our extensive experiments (evaluating state-of-the-art FR systems, intentionally biased models, and diverse datasets) confirm CEI's superior ability to detect nuanced biases where previous methods fall short. Furthermore, we present CEI^A, an automated version of the metric that enhances objectivity and simplifies practical application. CEI provides a robust and sensitive tool for operational FR fairness assessment. The proposed methods have been developed particularly for bias evaluation in face biometrics but, in general, they are applicable for comparing statistical distributions in any problem where one is interested in analyzing the distribution tails.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜æ€§èƒ½äººè„¸è¯†åˆ«(Face Recognition, FR)ç³»ç»Ÿä¸­äººå£ç»Ÿè®¡å­¦åè§éš¾ä»¥æ£€æµ‹çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åˆ†å¸ƒå°¾éƒ¨å­˜åœ¨çš„ç»†å¾®å·®å¼‚ï¼Œæå‡ºäº†åä¸ºComprehensive Equity Index (CEI)çš„æ–°å‹è¯„ä»·æŒ‡æ ‡ã€‚CEIçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºèƒ½å¤Ÿåˆ†åˆ«åˆ†æçœŸè¯š(Genuine)å’Œå†’å……(Impostor)çš„åˆ†æ•°åˆ†å¸ƒï¼Œä»è€Œåœ¨å…¼é¡¾æ•´ä½“åˆ†å¸ƒå½¢çŠ¶çš„åŒæ—¶ï¼Œå®ç°å¯¹åˆ†å¸ƒå°¾éƒ¨æ¦‚ç‡(Tail Probabilities)çš„å¯é…ç½®å…³æ³¨ã€‚ä¸ºäº†æå‡å®¢è§‚æ€§å¹¶ç®€åŒ–å®é™…åº”ç”¨ï¼Œç ”ç©¶è€…è¿˜è¿›ä¸€æ­¥æå‡ºäº†è‡ªåŠ¨åŒ–ç‰ˆæœ¬çš„CEI^Aã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„FRç³»ç»Ÿå’Œå¤šæ ·åŒ–æ•°æ®é›†çš„å¤§é‡å®éªŒï¼Œç»“æœè¯æ˜CEIåœ¨æ£€æµ‹ç»†å¾®åè§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•çš„çµæ•åº¦ã€‚è¯¥æŒ‡æ ‡ä¸ä»…ä¸ºäººè„¸è¯†åˆ«çš„å…¬å¹³æ€§è¯„ä¼°æä¾›äº†ç¨³å¥å·¥å…·ï¼Œå…¶ç»Ÿè®¡æ¯”è¾ƒæ–¹æ³•äº¦å¯æ¨å¹¿è‡³å…¶ä»–å…³æ³¨åˆ†å¸ƒå°¾éƒ¨åˆ†æçš„é€šç”¨é¢†åŸŸã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10564v2",
      "published_date": "2025-06-12 10:43:31 UTC",
      "updated_date": "2025-11-05 15:26:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:22:53.577091+00:00"
    },
    {
      "arxiv_id": "2506.10559v2",
      "title": "From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations",
      "title_zh": "ä»å›¾åƒåˆ°æ´è§ï¼šç»“åˆé€šä¿—è¯­è¨€ç”Ÿå¢ƒè§£é‡Šçš„å¯è§£é‡Šç”Ÿç‰©å¤šæ ·æ€§ç›‘æµ‹",
      "authors": [
        "Yutong Zhou",
        "Masahiro Ryo"
      ],
      "abstract": "Explaining why the species lives at a particular location is important for understanding ecological systems and conserving biodiversity. However, existing ecological workflows are fragmented and often inaccessible to non-specialists. We propose an end-to-end visual-to-causal framework that transforms a species image into interpretable causal insights about its habitat preference. The system integrates species recognition, global occurrence retrieval, pseudo-absence sampling, and climate data extraction. We then discover causal structures among environmental features and estimate their influence on species occurrence using modern causal inference methods. Finally, we generate statistically grounded, human-readable causal explanations from structured templates and large language models. We demonstrate the framework on a bee and a flower species and report early results as part of an ongoing project, showing the potential of the multimodal AI assistant backed up by a recommended ecological modeling practice for describing species habitat in human-understandable language. Our code is available at: https://github.com/Yutong-Zhou-cv/BioX.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„è§†è§‰åˆ°å› æœ(Visual-to-Causal)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç”Ÿæ€å·¥ä½œæµç¢ç‰‡åŒ–ä¸”éä¸“ä¸šäººå£«éš¾ä»¥ç†è§£çš„é—®é¢˜ï¼Œé€šè¿‡ç‰©ç§å›¾åƒç”Ÿæˆå…³äºå…¶æ –æ¯åœ°åå¥½çš„å¯è§£é‡Šæ€§å› æœè§è§£ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†ç‰©ç§è¯†åˆ«(Species Recognition)ã€å…¨çƒåˆ†å¸ƒè·å–ã€ä¼ªç¼ºå¤±é‡‡æ ·(Pseudo-absence Sampling)å’Œæ°”å€™æ•°æ®æå–ç­‰ç¯èŠ‚ã€‚ç ”ç©¶é€šè¿‡ç°ä»£å› æœæ¨ç†(Causal Inference)æ–¹æ³•å‘ç°ç¯å¢ƒç‰¹å¾é—´çš„å› æœç»“æ„ï¼Œå¹¶ç²¾å‡†ä¼°ç®—è¿™äº›ç‰¹å¾å¯¹ç‰©ç§åˆ†å¸ƒçš„å…·ä½“å½±å“ã€‚æœ€ç»ˆï¼Œè¯¥æ¡†æ¶ç»“åˆç»“æ„åŒ–æ¨¡æ¿å’Œå¤§è¯­è¨€æ¨¡å‹(Large Language Models)ç”Ÿæˆå…·æœ‰ç»Ÿè®¡ä¾æ®ä¸”é€šä¿—æ˜“æ‡‚çš„æ –æ¯åœ°å› æœè§£é‡Šã€‚åœ¨èœ‚ç±»å’ŒèŠ±å‰ç‰©ç§ä¸Šçš„åˆæ­¥å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨ç”Ÿç‰©å¤šæ ·æ€§ç›‘æµ‹ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚è¿™ä¸€å¤šæ¨¡æ€AIåŠ©æ‰‹ç»“åˆäº†æ¨èçš„ç”Ÿæ€å»ºæ¨¡å®è·µï¼Œä¸ºä»¥äººç±»å¯ç†è§£è¯­è¨€æè¿°å¤æ‚çš„ç‰©ç§æ –æ¯åœ°æä¾›äº†åˆ›æ–°æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CV",
      "comment": "AISE workshop camera-ready version @ ECAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10559v2",
      "published_date": "2025-06-12 10:33:30 UTC",
      "updated_date": "2025-09-09 09:37:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:23:09.205761+00:00"
    },
    {
      "arxiv_id": "2506.10558v2",
      "title": "StepProof: Step-by-step verification of natural language mathematical proofs",
      "title_zh": "StepProofï¼šè‡ªç„¶è¯­è¨€æ•°å­¦è¯æ˜çš„é€æ­¥éªŒè¯",
      "authors": [
        "Xiaolin Hu",
        "Qinghua Zhou",
        "Bogdan Grechuk",
        "Ivan Y. Tyukin"
      ],
      "abstract": "Interactive theorem provers (ITPs) are powerful tools for the formal verification of mathematical proofs down to the axiom level. However, their lack of a natural language interface remains a significant limitation. Recent advancements in large language models (LLMs) have enhanced the understanding of natural language inputs, paving the way for autoformalization - the process of translating natural language proofs into formal proofs that can be verified. Despite these advancements, existing autoformalization approaches are limited to verifying complete proofs and lack the capability for finer, sentence-level verification. To address this gap, we propose StepProof, a novel autoformalization method designed for granular, step-by-step verification. StepProof breaks down complete proofs into multiple verifiable subproofs, enabling sentence-level verification. Experimental results demonstrate that StepProof significantly improves proof success rates and efficiency compared to traditional methods. Additionally, we found that minor manual adjustments to the natural language proofs, tailoring them for step-level verification, further enhanced StepProof's performance in autoformalization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº¤äº’å¼å®šç†è¯æ˜å™¨ (ITPs) ç¼ºä¹è‡ªç„¶è¯­è¨€æ¥å£ä»¥åŠç°æœ‰è‡ªåŠ¨å½¢å¼åŒ– (autoformalization) æ–¹æ³•æ— æ³•è¿›è¡Œç»†ç²’åº¦éªŒè¯çš„é—®é¢˜ï¼Œæå‡ºäº† StepProofã€‚StepProof æ˜¯ä¸€ç§æ–°å‹çš„è‡ªåŠ¨å½¢å¼åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°ç»†ç²’åº¦çš„é€æ­¥éªŒè¯ï¼Œå®ƒé€šè¿‡å°†å®Œæ•´çš„è¯æ˜åˆ†è§£ä¸ºå¤šä¸ªå¯éªŒè¯çš„å­è¯æ˜ (subproofs)ï¼Œä»è€Œå®ç°äº†å¥å­çº§åˆ«çš„éªŒè¯èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒStepProof æ˜¾è‘—æé«˜äº†æ•°å­¦è¯æ˜çš„æˆåŠŸç‡å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°å¯¹è‡ªç„¶è¯­è¨€è¯æ˜è¿›è¡Œå¾®å°çš„æ‰‹åŠ¨è°ƒæ•´ä»¥é€‚é…æ­¥éª¤çº§éªŒè¯ï¼Œèƒ½å¤Ÿè¿›ä¸€æ­¥å¢å¼º StepProof åœ¨è‡ªåŠ¨å½¢å¼åŒ–ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10558v2",
      "published_date": "2025-06-12 10:31:23 UTC",
      "updated_date": "2025-06-30 10:59:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:22:54.715861+00:00"
    },
    {
      "arxiv_id": "2506.12094v1",
      "title": "Military AI Cyber Agents (MAICAs) Constitute a Global Threat to Critical Infrastructure",
      "title_zh": "å†›äº‹AIç½‘ç»œæ™ºèƒ½ä½“ï¼ˆMAICAsï¼‰å¯¹å…¨çƒå…³é”®åŸºç¡€è®¾æ–½æ„æˆå¨èƒ",
      "authors": [
        "Timothy Dubber",
        "Seth Lazar"
      ],
      "abstract": "This paper argues that autonomous AI cyber-weapons - Military-AI Cyber Agents (MAICAs) - create a credible pathway to catastrophic risk. It sets out the technical feasibility of MAICAs, explains why geopolitics and the nature of cyberspace make MAICAs a catastrophic risk, and proposes political, defensive-AI and analogue-resilience measures to blunt the threat.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å†›äº‹ AI ç½‘ç»œæ™ºèƒ½ä½“ (Military-AI Cyber Agents, MAICAs) è¿™ä¸€è‡ªä¸» AI ç½‘ç»œæ­¦å™¨å¯¹å…¨çƒå…³é”®åŸºç¡€è®¾æ–½æ„æˆçš„é‡å¤§å¨èƒã€‚æ–‡ç« è¯¦ç»†åˆ†æäº† MAICAs åœ¨æŠ€æœ¯ä¸Šçš„å¯è¡Œæ€§ï¼Œå¹¶é˜è¿°äº†åœ°ç¼˜æ”¿æ²»å› ç´ ä¸ç½‘ç»œç©ºé—´çš„æœ¬è´¨å±æ€§å¦‚ä½•å…±åŒä½¿ MAICAs æˆä¸ºä¸€ç§å¼•å‘ç¾éš¾æ€§é£é™©çš„ç°å®è·¯å¾„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç³»åˆ—åº”å¯¹æªæ–½ï¼ŒåŒ…æ‹¬æ”¿æ²»æ‰‹æ®µã€é˜²å¾¡æ€§ AI (defensive-AI) ä»¥åŠæ¨¡æ‹ŸéŸ§æ€§ (analogue-resilience) ç­–ç•¥ã€‚è¯¥ç ”ç©¶æ—¨åœ¨æ­ç¤ºè‡ªä¸» AI æ­¦å™¨åŒ–å¸¦æ¥çš„æ½œåœ¨å®‰å…¨å±æœºï¼Œå¹¶ä¸ºæ„å»ºå¤šç»´åº¦çš„é˜²å¾¡ä¸é£é™©å‰Šå‡æœºåˆ¶æä¾›ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12094v1",
      "published_date": "2025-06-12 09:51:06 UTC",
      "updated_date": "2025-06-12 09:51:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:23:04.704231+00:00"
    },
    {
      "arxiv_id": "2506.10527v1",
      "title": "LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs",
      "title_zh": "LogiPlanï¼šå¤§è¯­è¨€æ¨¡å‹é€»è¾‘è§„åˆ’ä¸å…³ç³»æ¨ç†çš„ç»“æ„åŒ–åŸºå‡†",
      "authors": [
        "Yanan Cai",
        "Ahmed Salem",
        "Besmira Nushi",
        "Mark Russinovich"
      ],
      "abstract": "We introduce LogiPlan, a novel benchmark designed to evaluate the capabilities of large language models (LLMs) in logical planning and reasoning over complex relational structures. Logical relational reasoning is important for applications that may rely on LLMs to generate and query structured graphs of relations such as network infrastructure, knowledge bases, or business process schema. Our framework allows for dynamic variation of task complexity by controlling the number of objects, relations, and the minimum depth of relational chains, providing a fine-grained assessment of model performance across difficulty levels. LogiPlan encompasses three complementary tasks: (1) Plan Generation, where models must construct valid directed relational graphs meeting specified structural constraints; (2) Consistency Detection, testing models' ability to identify inconsistencies in relational structures; and (3) Comparison Question, evaluating models' capacity to determine the validity of queried relationships within a given graph. Additionally, we assess models' self-correction capabilities by prompting them to verify and refine their initial solutions. We evaluate state-of-the-art models including DeepSeek R1, Gemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B, O3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant performance gaps that correlate with model scale and architecture. Our analysis demonstrates that while recent reasoning-enhanced models show promising results on simpler instances, they struggle with more complex configurations requiring deeper logical planning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†LogiPlanï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€»è¾‘è§„åˆ’ï¼ˆLogical Planningï¼‰å’Œå¤æ‚å…³ç³»ç»“æ„æ¨ç†ï¼ˆRelational Reasoningï¼‰èƒ½åŠ›çš„æ–°å‹åŸºå‡†ã€‚LogiPlané€šè¿‡åŠ¨æ€è°ƒæ•´å¯¹è±¡æ•°é‡ã€å…³ç³»å¤æ‚åº¦åŠå…³ç³»é“¾æ·±åº¦ï¼Œå®ç°äº†å¯¹æ¨¡å‹åœ¨ä¸åŒéš¾åº¦çº§åˆ«ä¸‹çš„ç»†ç²’åº¦æ€§èƒ½è¯„ä¼°ã€‚è¯¥åŸºå‡†åŒ…å«è®¡åˆ’ç”Ÿæˆï¼ˆPlan Generationï¼‰ã€ä¸€è‡´æ€§æ£€æµ‹ï¼ˆConsistency Detectionï¼‰ä»¥åŠæ¯”è¾ƒé—®é¢˜ï¼ˆComparison Questionï¼‰ä¸‰ä¸ªäº’è¡¥ä»»åŠ¡ï¼Œå¹¶åŒæ­¥è€ƒå¯Ÿäº†æ¨¡å‹çš„è‡ªæˆ‘ä¿®å¤ï¼ˆSelf-correctionï¼‰èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹DeepSeek R1ã€Gemini 2.0 Proã€GPT-4.5ã€O1åŠClaude 3.7 Sonnetç­‰å‰æ²¿æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼Œæ­ç¤ºäº†æ€§èƒ½å·®è·ä¸æ¨¡å‹è§„æ¨¡åŠæ¶æ„ä¹‹é—´çš„å¼ºç›¸å…³æ€§ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼Œå°½ç®¡æ¨ç†å¢å¼ºå‹æ¨¡å‹åœ¨ç®€å•ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨é¢å¯¹éœ€è¦æ·±å±‚é€»è¾‘è§„åˆ’çš„å¤æ‚é…ç½®æ—¶ä¾ç„¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10527v1",
      "published_date": "2025-06-12 09:47:02 UTC",
      "updated_date": "2025-06-12 09:47:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:23:27.462799+00:00"
    },
    {
      "arxiv_id": "2506.10521v6",
      "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning",
      "title_zh": "ç§‘å­¦å®¶çš„ç¬¬ä¸€è€ƒï¼šé€šè¿‡æ„ŸçŸ¥ã€ç†è§£ä¸æ¨ç†æ¢ç©¶ MLLM çš„è®¤çŸ¥èƒ½åŠ›",
      "authors": [
        "Yuhao Zhou",
        "Yiheng Wang",
        "Xuming He",
        "Ao Shen",
        "Ruoyao Xiao",
        "Zhiwei Li",
        "Qiantai Feng",
        "Zijie Guo",
        "Yuejin Yang",
        "Hao Wu",
        "Wenxuan Huang",
        "Jiaqi Wei",
        "Dan Si",
        "Xiuqi Yao",
        "Jia Bu",
        "Haiwen Huang",
        "Manning Wang",
        "Tianfan Fu",
        "Shixiang Tang",
        "Ben Fei",
        "Dongzhan Zhou",
        "Fenghua Ling",
        "Yan Lu",
        "Siqi Sun",
        "Chenhui Li",
        "Guanjie Zheng",
        "Jiancheng Lv",
        "Wenlong Zhang",
        "Lei Bai"
      ],
      "abstract": "Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Scientists' First Exam (SFE) åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä»ç§‘å­¦ä¿¡å·æ„ŸçŸ¥ (scientific signal perception)ã€ç§‘å­¦å±æ€§ç†è§£ (scientific attribute understanding) å’Œç§‘å­¦æ¯”è¾ƒæ¨ç† (scientific comparative reasoning) ä¸‰ä¸ªå±‚çº§ï¼Œå…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) çš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ã€‚SFE å¼¥è¡¥äº†ç°æœ‰åŸºå‡†ä¾§é‡äºçŸ¥è¯†ç†è§£è€Œå¿½è§†æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›çš„ä¸è¶³ï¼ŒåŒ…å«äº†æ¶µç›– 5 ä¸ªå­¦ç§‘ã€66 é¡¹å¤šæ¨¡æ€ä»»åŠ¡çš„ 830 ä¸ªä¸“å®¶éªŒè¯è§†è§‰é—®ç­” (VQA) å¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„ GPT-o3 å’Œ InternVL-3 æ¨¡å‹ï¼Œåœ¨ SFE ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåˆ†åˆ«ä»…ä¸º 34.08% å’Œ 26.52%ï¼Œå‡¸æ˜¾äº† MLLMs åœ¨å¤„ç†å¤æ‚ç§‘å­¦æ•°æ®æ–¹é¢çš„å·¨å¤§æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†çš„å‘å¸ƒä¸ºå¼€å‘æ›´å¼ºå¤§çš„ AI é©±åŠ¨ç§‘å­¦å‘ç°ç³»ç»Ÿæä¾›äº†é‡è¦çš„è¯„ä¼°æ¡†æ¶ä¸æ”¹è¿›è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "82 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.10521v6",
      "published_date": "2025-06-12 09:29:16 UTC",
      "updated_date": "2025-11-14 08:59:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:23:26.665021+00:00"
    },
    {
      "arxiv_id": "2506.10516v3",
      "title": "CogStream: Context-guided Streaming Video Question Answering",
      "title_zh": "CogStreamï¼šä¸Šä¸‹æ–‡å¼•å¯¼çš„æµå¼è§†é¢‘é—®ç­”",
      "authors": [
        "Zicheng Zhao",
        "Kangyu Wang",
        "Shijie Li",
        "Rui Qian",
        "Weiyao Lin",
        "Huabin Liu"
      ],
      "abstract": "Despite advancements in Video Large Language Models (Vid-LLMs) improving multimodal understanding, challenges persist in streaming video reasoning due to its reliance on contextual information. Existing paradigms feed all available historical contextual information into Vid-LLMs, resulting in a significant computational burden for visual data processing. Furthermore, the inclusion of irrelevant context distracts models from key details. This paper introduces a challenging task called Context-guided Streaming Video Reasoning (CogStream), which simulates real-world streaming video scenarios, requiring models to identify the most relevant historical contextual information to deduce answers for questions about the current stream. To support CogStream, we present a densely annotated dataset featuring extensive and hierarchical question-answer pairs, generated by a semi-automatic pipeline. Additionally, we present CogReasoner as a baseline model. It effectively tackles this task by leveraging visual stream compression and historical dialogue retrieval. Extensive experiments prove the effectiveness of this method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘å¤§è¯­è¨€æ¨¡å‹(Vid-LLMs)åœ¨æµå¼è§†é¢‘æ¨ç†(streaming video reasoning)ä¸­å­˜åœ¨çš„è®¡ç®—å‹åŠ›åŠæ— å…³ä¸Šä¸‹æ–‡å¹²æ‰°ç­‰é—®é¢˜ï¼Œæå‡ºäº†åä¸ºCogStreamçš„æ–°ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡è¦æ±‚æ¨¡å‹åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„æµå¼è§†é¢‘åœºæ™¯ä¸­ï¼Œå‡†ç¡®è¯†åˆ«å¹¶åˆ©ç”¨æœ€ç›¸å…³çš„å†å²ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æ¨æ–­å½“å‰è§†é¢‘æµçš„ç­”æ¡ˆã€‚ä¸ºæ”¯æŒè¿™ä¸€ç ”ç©¶ï¼Œä½œè€…é€šè¿‡åŠè‡ªåŠ¨æµæ°´çº¿æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¯†é›†æ ‡æ³¨å’Œå±‚çº§é—®ç­”å¯¹çš„ä¸“ç”¨æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†åŸºçº¿æ¨¡å‹CogReasonerï¼Œè¯¥æ¨¡å‹ç»“åˆè§†è§‰æµå‹ç¼©(visual stream compression)å’Œå†å²å¯¹è¯æ£€ç´¢(historical dialogue retrieval)æŠ€æœ¯ï¼Œæœ‰æ•ˆæå‡äº†å¤„ç†é•¿è§†é¢‘æµæ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è§£å†³ä¸Šä¸‹æ–‡å¼•å¯¼çš„æµå¼è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://github.com/LiamZhao326/CogStream",
      "pdf_url": "https://arxiv.org/pdf/2506.10516v3",
      "published_date": "2025-06-12 09:24:07 UTC",
      "updated_date": "2025-12-28 09:05:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:23:29.575529+00:00"
    },
    {
      "arxiv_id": "2506.21567v2",
      "title": "BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining",
      "title_zh": "BioParsï¼šé¢å‘æ³¢æ–¯è¯­ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬æŒ–æ˜çš„é¢„è®­ç»ƒç”Ÿç‰©åŒ»å­¦å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Baqer M. Merzah",
        "Tania Taami",
        "Salman Asoudeh",
        "Saeed Mirzaee",
        "Amir reza Hossein pour",
        "Amir Ali Bengari"
      ],
      "abstract": "Large Language Models (LLMs) have recently gained attention in the life sciences due to their capacity to model, extract, and apply complex biological information. Beyond their classical use as chatbots, these systems are increasingly used for complex analysis and problem-solving in specialized fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset from over 10,000 scientific articles, textbooks, and medical websites. BioParsQA was also introduced to evaluate the proposed model, which consists of 5,231 Persian medical questions and answers. This study then introduces BioPars, a simple but accurate measure designed to assess LLMs for three main abilities: acquiring subject-specific knowledge, interpreting and synthesizing such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama, and Galactica, our study highlights their ability to remember and retrieve learned knowledge but also reveals shortcomings in addressing higher-level, real-world questions and fine-grained inferences. These findings indicate the need for further fine-tuning to address the capabilities of LLM in bioinformatics tasks. To our knowledge, BioPars is the first application of LLM in Persian medical QA, especially for generating long answers. Evaluation of four selected medical QA datasets shows that BioPars has achieved remarkable results compared to comparative approaches. The model on BioParsQA achieved a ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT values were also higher in this model than the other three models. In addition, the reported scores for the model are MoverScore=60.43 and BLEURT=50.78. BioPars is an ongoing project and all resources related to its development will be made available via the following GitHub repository: https://github.com/amirap80/BioPars.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ³¢æ–¯è¯­ç”Ÿç‰©åŒ»å­¦é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„åº”ç”¨ç¼ºå£ï¼Œå¼€å‘äº†åä¸ºBioParsçš„é¢„è®­ç»ƒæ¨¡å‹åŠå…¶è¯„ä¼°åŸºå‡†ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹è®­ç»ƒï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«1ä¸‡ä½™ç¯‡å­¦æœ¯æ–‡çŒ®æ•°æ®çš„BIOPARS-BENCHæ•°æ®é›†ï¼Œä»¥åŠåŒ…å«5231ä¸ªæ³¢æ–¯è¯­åŒ»å­¦é—®ç­”å¯¹çš„BioParsQAæ•°æ®é›†ã€‚é€šè¿‡ä¸ChatGPTã€Llamaå’ŒGalacticaç­‰æ¨¡å‹å¯¹æ¯”ï¼Œç ”ç©¶æ­ç¤ºäº†é€šç”¨æ¨¡å‹åœ¨å¤„ç†ç”Ÿç‰©ä¿¡æ¯å­¦é«˜å±‚çº§æ¨ç†å’Œç»†ç²’åº¦çŸ¥è¯†æå–æ–¹é¢çš„å±€é™ï¼Œå¼ºè°ƒäº†é¢†åŸŸç‰¹å®šå¾®è°ƒçš„å¿…è¦æ€§ã€‚BioParsä½œä¸ºé¦–ä¸ªä¸“é—¨é’ˆå¯¹æ³¢æ–¯è¯­åŒ»ç–—é—®ç­”è®¾è®¡çš„LLMåº”ç”¨ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç‰¹å®šå­¦ç§‘çŸ¥è¯†è·å–ã€ç»¼åˆè§£è¯»åŠé•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨BioParsQAä¸Šçš„ROUGE-Lå¾—åˆ†è¾¾åˆ°29.99ï¼Œè¶…è¶Šäº†GPT-4ï¼Œå¹¶åœ¨BERTScoreã€MoverScoreåŠBLEURTç­‰æŒ‡æ ‡ä¸Šå‡å–å¾—äº†é¢†å…ˆæˆæœã€‚è¯¥é¡¹ç›®çš„å¼€æºä¸ºæ³¢æ–¯è¯­ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬æŒ–æ˜åŠåç»­ç ”ç©¶æä¾›äº†é‡è¦çš„èµ„æºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21567v2",
      "published_date": "2025-06-12 09:11:26 UTC",
      "updated_date": "2025-07-01 19:14:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:23:33.858161+00:00"
    },
    {
      "arxiv_id": "2506.10508v1",
      "title": "Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs",
      "title_zh": "å¯é æ¨ç†è·¯å¾„ï¼šåˆ©ç”¨çŸ¥è¯†å›¾è°±æç‚¼å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„æœ‰æ•ˆå¼•å¯¼",
      "authors": [
        "Yilin Xiao",
        "Chuang Zhou",
        "Qinggang Zhang",
        "Bo Li",
        "Qing Li",
        "Xiao Huang"
      ],
      "abstract": "Large language models (LLMs) often struggle with knowledge-intensive tasks due to a lack of background knowledge and a tendency to hallucinate. To address these limitations, integrating knowledge graphs (KGs) with LLMs has been intensively studied. Existing KG-enhanced LLMs focus on supplementary factual knowledge, but still struggle with solving complex questions. We argue that refining the relationships among facts and organizing them into a logically consistent reasoning path is equally important as factual knowledge itself. Despite their potential, extracting reliable reasoning paths from KGs poses the following challenges: the complexity of graph structures and the existence of multiple generated paths, making it difficult to distinguish between useful and redundant ones. To tackle these challenges, we propose the RRP framework to mine the knowledge graph, which combines the semantic strengths of LLMs with structural information obtained through relation embedding and bidirectional distribution learning. Additionally, we introduce a rethinking module that evaluates and refines reasoning paths according to their significance. Experimental results on two public datasets show that RRP achieves state-of-the-art performance compared to existing baseline methods. Moreover, RRP can be easily integrated into various LLMs to enhance their reasoning abilities in a plug-and-play manner. By generating high-quality reasoning paths tailored to specific questions, RRP distills effective guidance for LLM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RRP (Reliable Reasoning Path) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­é¢ä¸´çš„èƒŒæ™¯çŸ¥è¯†ç¼ºå¤±å’Œå¹»è§‰é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶éš¾ä»¥ä»å¤æ‚çš„çŸ¥è¯†å›¾è°± (Knowledge Graphs, KGs) ä¸­æå–é€»è¾‘ä¸€è‡´ä¸”æœ‰æ•ˆçš„æ¨ç†è·¯å¾„è¿™ä¸€æŒ‘æˆ˜ï¼ŒRRP ç»“åˆäº† LLMs çš„è¯­ä¹‰ä¼˜åŠ¿ä¸é€šè¿‡å…³ç³»åµŒå…¥ (Relation Embedding) åŠåŒå‘åˆ†å¸ƒå­¦ä¹  (Bidirectional Distribution Learning) è·å–çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†ä¸€ä¸ªé‡æ–°æ€è€ƒæ¨¡å— (Rethinking Module)ï¼Œç”¨äºæ ¹æ®é‡è¦æ€§è¯„ä¼°å¹¶ç²¾ç‚¼æ¨ç†è·¯å¾„ï¼Œç¡®ä¿ç”Ÿæˆçš„è·¯å¾„å…·æœ‰é«˜åº¦çš„å¯é æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRRP åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿› (State-of-the-art) çš„æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ”¯æŒä»¥å³æ’å³ç”¨ (Plug-and-play) çš„æ–¹å¼é›†æˆåˆ°å¤šç§ LLMs ä¸­ï¼Œé€šè¿‡ä¸ºç‰¹å®šé—®é¢˜æä¾›å®šåˆ¶åŒ–çš„é«˜è´¨é‡æ¨ç†è·¯å¾„ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„æ¨ç†å¼•å¯¼æ•ˆæœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10508v1",
      "published_date": "2025-06-12 09:10:32 UTC",
      "updated_date": "2025-06-12 09:10:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:23:33.718278+00:00"
    },
    {
      "arxiv_id": "2506.10504v1",
      "title": "Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models",
      "title_zh": "è¶…è¶Šå•ç”¨æˆ·å¯¹è¯ï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„å¤šç”¨æˆ·å¯¹è¯çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›",
      "authors": [
        "Sangmin Song",
        "Juhwan Choi",
        "JungMin Yun",
        "YoungBin Kim"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance in zero-shot dialogue state tracking (DST), reducing the need for task-specific training. However, conventional DST benchmarks primarily focus on structured user-agent conversations, failing to capture the complexities of real-world multi-user interactions. In this study, we assess the robustness of LLMs in multi-user DST while minimizing dataset construction costs. Inspired by recent advances in LLM-based data annotation, we extend an existing DST dataset by generating utterances of a second user based on speech act theory. Our methodology systematically incorporates a second user's utterances into conversations, enabling a controlled evaluation of LLMs in multi-user settings. Experimental results reveal a significant performance drop compared to single-user DST, highlighting the limitations of current LLMs in extracting and tracking dialogue states amidst multiple speakers. Our findings emphasize the need for future research to enhance LLMs for multi-user DST scenarios, paving the way for more realistic and robust DST models.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šç”¨æˆ·å¯¹è¯çŠ¶æ€è·Ÿè¸ª(Multi-User Dialogue State Tracking, DST)ä¸­çš„è¡¨ç°ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸåŸºå‡†æµ‹è¯•ä»…ä¾§é‡äºå•ç”¨æˆ·ä¸ä»£ç†äº¤äº’è€Œå¿½è§†ç°å®å¤šç”¨æˆ·å¯¹è¯å¤æ‚æ€§çš„å±€é™ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¨€è¯­è¡Œä¸ºç†è®º(speech act theory)ç”Ÿæˆç¬¬äºŒä½ç”¨æˆ·çš„è¯è¯­æ¥æ‰©å±•ç°æœ‰æ•°æ®é›†ï¼Œä»è€Œä»¥è¾ƒä½æˆæœ¬å®ç°äº†å¯¹LLMsåœ¨å¤šç”¨æˆ·åœºæ™¯ä¸‹é²æ£’æ€§çš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å•ç”¨æˆ·DSTç›¸æ¯”ï¼ŒLLMsçš„æ€§èƒ½åœ¨å¤šç”¨æˆ·ç¯å¢ƒä¸‹æ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤šå‘è¨€è€…èƒŒæ™¯ä¸‹å‡†ç¡®æå–å’Œè·Ÿè¸ªå¯¹è¯çŠ¶æ€çš„ä¸è¶³ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†é’ˆå¯¹å¤šç”¨æˆ·DSTåœºæ™¯ä¼˜åŒ–LLMsçš„å¿…è¦æ€§ï¼Œä¸ºå¼€å‘æ›´è´´è¿‘ç°å®ã€æ›´å…·é²æ£’æ€§çš„DSTæ¨¡å‹æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10504v1",
      "published_date": "2025-06-12 09:04:19 UTC",
      "updated_date": "2025-06-12 09:04:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:05.306783+00:00"
    },
    {
      "arxiv_id": "2506.10503v1",
      "title": "Semantic Localization Guiding Segment Anything Model For Reference Remote Sensing Image Segmentation",
      "title_zh": "è¯­ä¹‰å®šä½å¼•å¯¼ Segment Anything Model çš„å‚è€ƒé¥æ„Ÿå›¾åƒåˆ†å‰²",
      "authors": [
        "Shuyang Li",
        "Shuang Wang",
        "Zhuangzhuang Sun",
        "Jing Xiao"
      ],
      "abstract": "The Reference Remote Sensing Image Segmentation (RRSIS) task generates segmentation masks for specified objects in images based on textual descriptions, which has attracted widespread attention and research interest. Current RRSIS methods rely on multi-modal fusion backbones and semantic segmentation heads but face challenges like dense annotation requirements and complex scene interpretation. To address these issues, we propose a framework named \\textit{prompt-generated semantic localization guiding Segment Anything Model}(PSLG-SAM), which decomposes the RRSIS task into two stages: coarse localization and fine segmentation. In coarse localization stage, a visual grounding network roughly locates the text-described object. In fine segmentation stage, the coordinates from the first stage guide the Segment Anything Model (SAM), enhanced by a clustering-based foreground point generator and a mask boundary iterative optimization strategy for precise segmentation. Notably, the second stage can be train-free, significantly reducing the annotation data burden for the RRSIS task. Additionally, decomposing the RRSIS task into two stages allows for focusing on specific region segmentation, avoiding interference from complex scenes.We further contribute a high-quality, multi-category manually annotated dataset. Experimental validation on two datasets (RRSIS-D and RRSIS-M) demonstrates that PSLG-SAM achieves significant performance improvements and surpasses existing state-of-the-art models.Our code will be made publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‚è€ƒé¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆReference Remote Sensing Image Segmentation, RRSISï¼‰ä»»åŠ¡åœ¨å¯†é›†æ ‡æ³¨éœ€æ±‚å’Œå¤æ‚åœºæ™¯è§£è¯»æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º PSLG-SAM çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°† RRSIS ä»»åŠ¡åˆ›æ–°æ€§åœ°åˆ†è§£ä¸ºç²—ç•¥å®šä½å’Œç²¾ç»†åˆ†å‰²ä¸¤ä¸ªé˜¶æ®µã€‚åœ¨ç²—ç•¥å®šä½é˜¶æ®µï¼Œåˆ©ç”¨è§†è§‰å®šä½ç½‘ç»œï¼ˆvisual grounding networkï¼‰åˆæ­¥ç¡®å®šæ–‡æœ¬æè¿°ç›®æ ‡çš„åæ ‡ã€‚åœ¨ç²¾ç»†åˆ†å‰²é˜¶æ®µï¼Œè¯¥åæ ‡ç”¨äºå¼•å¯¼ Segment Anything Model (SAM)ï¼Œå¹¶ç»“åˆåŸºäºèšç±»çš„å‰æ™¯é‡‡æ ·ç‚¹ç”Ÿæˆå™¨å’Œæ©ç è¾¹ç•Œè¿­ä»£ä¼˜åŒ–ç­–ç•¥å®ç°ç²¾ç¡®åˆ†å‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¡†æ¶çš„ç¬¬äºŒé˜¶æ®µå…·å¤‡å…è®­ç»ƒï¼ˆtrain-freeï¼‰ç‰¹æ€§ï¼Œæ˜¾è‘—å‡è½»äº†é¥æ„Ÿä»»åŠ¡çš„æ ‡æ³¨è´Ÿæ‹…å¹¶æœ‰æ•ˆé¿å…äº†å¤æ‚èƒŒæ™¯å¹²æ‰°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPSLG-SAM åœ¨ RRSIS-D å’Œ RRSIS-M æ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ SOTA æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10503v1",
      "published_date": "2025-06-12 09:04:07 UTC",
      "updated_date": "2025-06-12 09:04:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:23:46.190700+00:00"
    },
    {
      "arxiv_id": "2506.21566v1",
      "title": "The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation",
      "title_zh": "é«˜è´¨é‡ä½èµ„æºè‹±å¤å‰æ‹‰ç‰¹è¯­æœºå™¨ç¿»è¯‘ä¸­å›è¯‘æŠ€æœ¯çš„é¥±å’Œç‚¹",
      "authors": [
        "Arwa Arif"
      ],
      "abstract": "Backtranslation BT is widely used in low resource machine translation MT to generate additional synthetic training data using monolingual corpora. While this approach has shown strong improvements for many language pairs, its effectiveness in high quality, low resource settings remains unclear. In this work, we explore the effectiveness of backtranslation for English Gujarati translation using the multilingual pretrained MBART50 model. Our baseline system, trained on a high quality parallel corpus of approximately 50,000 sentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment this data with carefully filtered backtranslated examples generated from monolingual Gujarati text. Surprisingly, adding this synthetic data does not improve translation performance and, in some cases, slightly reduces it. We evaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and analyze possible reasons for this saturation. Our findings suggest that backtranslation may reach a point of diminishing returns in certain low-resource settings and we discuss implications for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›è¯‘(Backtranslation)æŠ€æœ¯åœ¨é«˜è´¨é‡ã€ä½èµ„æºç¯å¢ƒä¸‹çš„è‹±è¯­-å¤å‰æ‹‰ç‰¹è¯­(English-Gujarati)æœºå™¨ç¿»è¯‘ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è€…åŸºäºé¢„è®­ç»ƒçš„MBART50æ¨¡å‹ï¼Œåœ¨åŒ…å«çº¦5ä¸‡å¯¹é«˜è´¨é‡å¹³è¡Œè¯­æ–™çš„åŸºçº¿ç³»ç»Ÿä¸Šè¿›è¡Œå®éªŒï¼Œå…¶éªŒè¯é›†BLEUå¾—åˆ†è¾¾åˆ°43.8ã€‚éšåï¼Œç ”ç©¶å›¢é˜Ÿå°è¯•åˆ©ç”¨ç²¾å¿ƒè¿‡æ»¤çš„å•è¯­è¯­æ–™ç”Ÿæˆå›è¯‘åˆæˆæ•°æ®ä»¥å¢å¼ºè®­ç»ƒé›†ï¼Œç„¶è€Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ è¿™äº›åˆæˆæ•°æ®å¹¶æœªèƒ½æ”¹å–„ç¿»è¯‘è¡¨ç°ï¼Œç”šè‡³åœ¨æŸäº›è¯„ä¼°åœºæ™¯ä¸‹å¯¼è‡´äº†æ€§èƒ½çš„è½»å¾®ä¸‹é™ã€‚ç ”ç©¶é€šè¿‡BLEUã€ChrF++ã€TERå’ŒBLEURTç­‰å¤šç§åº¦é‡æŒ‡æ ‡å¯¹æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å¯¼è‡´è¿™ç§é¥±å’Œç‚¹(Saturation Point)å‡ºç°çš„å¯èƒ½å› ç´ ã€‚è¯¥å‘ç°æ­ç¤ºäº†å›è¯‘åœ¨æŸäº›ä½èµ„æºè®¾å®šä¸­å¯èƒ½é¢ä¸´æ”¶ç›Šé€’å‡çš„é—®é¢˜ï¼Œä¸ºæœªæ¥ç ”ç©¶åœ¨åˆæˆæ•°æ®çš„ä½¿ç”¨ç­–ç•¥ä¸Šæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint, 8 Pages",
      "pdf_url": "https://arxiv.org/pdf/2506.21566v1",
      "published_date": "2025-06-12 09:02:53 UTC",
      "updated_date": "2025-06-12 09:02:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:23:50.527965+00:00"
    },
    {
      "arxiv_id": "2506.13796v1",
      "title": "ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries",
      "title_zh": "ClimateChatï¼šé¢å‘æ°”å€™å˜åŒ–é—®ç­”çš„å¤§è¯­è¨€æ¨¡å‹æŒ‡ä»¤å¾®è°ƒæ•°æ®ä¸æ–¹æ³•è®¾è®¡",
      "authors": [
        "Zhou Chen",
        "Xiao Wang",
        "Yuanhong Liao",
        "Ming Lin",
        "Yuqi Bai"
      ],
      "abstract": "As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ°”å€™å˜åŒ–é¢†åŸŸé«˜è´¨é‡æŒ‡ä»¤æ•°æ®(instruction data)åŒ®ä¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æŒ‡ä»¤æ•°æ®æ„å»ºæ–¹æ³•ï¼Œé€šè¿‡æ•´åˆæ–‡æ¡£äº‹å®ã€èƒŒæ™¯çŸ¥è¯†ã€ç½‘é¡µæŠ“å–ä»¥åŠç§å­æŒ‡ä»¤æ¥å¢å¼ºæ•°æ®å¤šæ ·æ€§ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºè¯¥æ–¹æ³•æ„å»ºäº†åä¸º ClimateChat-Corpus çš„æ°”å€™å˜åŒ–æŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶å¯¹å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œå¾®è°ƒï¼ŒæˆåŠŸå¼€å‘å‡ºä¸“é—¨çš„ ClimateChat æ¨¡å‹ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒClimateChat åœ¨æ°”å€™å˜åŒ–é—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½å¾—åˆ°æ˜¾è‘—æå‡ï¼Œå¹¶å±•ç°å‡ºé€‚åº”å¹¿æ³›ç§‘å­¦å‘ç°ä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼ºè°ƒäº†åœ¨æŒ‡ä»¤å¾®è°ƒ(instruction tuning)è¿‡ç¨‹ä¸­é€‰æ‹©åˆé€‚åŸºåº§æ¨¡å‹çš„é‡è¦æ€§ï¼Œä¸ºå¼€å‘æ°”å€™å˜åŒ–é¢†åŸŸçš„ä¸“ä¸šå¤§æ¨¡å‹æä¾›äº†é‡è¦çš„å‚è€ƒè·¯å¾„å’Œå®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025 camera ready, 13 pages, 4 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.13796v1",
      "published_date": "2025-06-12 08:43:38 UTC",
      "updated_date": "2025-06-12 08:43:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:05.472102+00:00"
    },
    {
      "arxiv_id": "2506.10481v1",
      "title": "OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics",
      "title_zh": "OIBenchï¼šåŸºäºä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹ç«èµ›çš„å¼ºæ¨ç†æ¨¡å‹åŸºå‡†è¯„æµ‹",
      "authors": [
        "Yaoming Zhu",
        "Junxin Wang",
        "Yiyang Li",
        "Lin Qiu",
        "ZongYu Wang",
        "Jun Xu",
        "Xuezhi Cao",
        "Yuhuai Wei",
        "Mingshi Wang",
        "Xunliang Cai",
        "Rong Ma"
      ],
      "abstract": "As models become increasingly sophisticated, conventional algorithm benchmarks are increasingly saturated, underscoring the need for more challenging benchmarks to guide future improvements in algorithmic reasoning. This paper introduces OIBench, a high-quality, private, and challenging olympiad-level informatics dataset comprising 250 carefully curated original problems. We detail the construction methodology of the benchmark, ensuring a comprehensive assessment across various programming paradigms and complexities, and we demonstrate its contamination-resistant properties via experiments. We propose Time/Space Completion Curves for finer-grained efficiency analysis and enable direct human-model comparisons through high-level participant evaluations. Our experiments reveal that while open-source models lag behind closed-source counterparts, current SOTA models already outperform most human participants in both correctness and efficiency, while still being suboptimal compared to the canonical solutions. By releasing OIBench as a fully open-source resource (https://huggingface.co/datasets/AGI-Eval/OIBench), we hope this benchmark will contribute to advancing code reasoning capabilities for future LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OIBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹ç«èµ›æ°´å¹³æ•°æ®é›†ï¼ŒåŒ…å« 250 ä¸ªç²¾å¿ƒç­–åˆ’çš„åŸåˆ›é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç®—æ³•åŸºå‡†æµ‹è¯•æ—¥ç›Šé¥±å’Œçš„é—®é¢˜ï¼Œä»¥å¼•å¯¼æœªæ¥ç®—æ³•æ¨ç†èƒ½åŠ›çš„æå‡ã€‚è¯¥åŸºå‡†æ¶µç›–äº†å¤šç§ç¼–ç¨‹èŒƒå¼å’Œå¤æ‚åº¦ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶å…·å¤‡æ˜¾è‘—çš„æŠ—æ±¡æŸ“ç‰¹æ€§ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº† Time/Space Completion Curves ä»¥è¿›è¡Œæ›´ç»†ç²’åº¦çš„ä»£ç æ•ˆç‡åˆ†æï¼Œå¹¶æ”¯æŒé€šè¿‡é«˜æ°´å¹³å‚èµ›è€…è¯„ä¼°è¿›è¡Œç›´æ¥çš„äººæœºå¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å¼€æºæ¨¡å‹ä»è½åäºé—­æºæ¨¡å‹ï¼Œä½†å½“å‰çš„ SOTA æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå·²ç»è¶…è¶Šäº†å¤§å¤šæ•°äººç±»å‚èµ›è€…ï¼Œå°½ç®¡ä¸æ ‡å‡†è§£å†³æ–¹æ¡ˆ (canonical solutions) ç›¸æ¯”ä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚é€šè¿‡å°† OIBench ä½œä¸ºå®Œå…¨å¼€æºèµ„æºå‘å¸ƒï¼Œè¯¥ç ”ç©¶ä¸ºè¯„ä¼°å’Œæå‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ä»£ç æ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦çš„åŸºå‡†å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10481v1",
      "published_date": "2025-06-12 08:33:38 UTC",
      "updated_date": "2025-06-12 08:33:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:07.084507+00:00"
    },
    {
      "arxiv_id": "2506.12093v1",
      "title": "Intelligent Automation for FDI Facilitation: Optimizing Tariff Exemption Processes with OCR And Large Language Models",
      "title_zh": "é¢å‘ FDI ä¾¿åˆ©åŒ–çš„æ™ºèƒ½è‡ªåŠ¨åŒ–ï¼šåˆ©ç”¨ OCR ä¸å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–å…³ç¨è±å…æµç¨‹",
      "authors": [
        "Muhammad Sukri Bin Ramli"
      ],
      "abstract": "Tariff exemptions are fundamental to attracting Foreign Direct Investment (FDI) into the manufacturing sector, though the associated administrative processes present areas for optimization for both investing entities and the national tax authority. This paper proposes a conceptual framework to empower tax administration by leveraging a synergistic integration of Optical Character Recognition (OCR) and Large Language Model (LLM) technologies. The proposed system is designed to first utilize OCR for intelligent digitization, precisely extracting data from diverse application documents and key regulatory texts such as tariff orders. Subsequently, the LLM would enhance the capabilities of administrative officers by automating the critical and time-intensive task of verifying submitted HS Tariff Codes for machinery, equipment, and raw materials against official exemption lists. By enhancing the speed and precision of these initial assessments, this AI-driven approach systematically reduces potential for non-alignment and non-optimized exemption utilization, thereby streamlining the investment journey for FDI companies. For the national administration, the benefits include a significant boost in operational capacity, reduced administrative load, and a strengthened control environment, ultimately improving the ease of doing business and solidifying the nation's appeal as a premier destination for high-value manufacturing FDI.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€å¥—ç»“åˆå…‰å­¦å­—ç¬¦è¯†åˆ«(Optical Character Recognition, OCR)ä¸å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLM)çš„æ™ºèƒ½è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–åˆ¶é€ ä¸šå¤–å›½ç›´æ¥æŠ•èµ„(Foreign Direct Investment, FDI)ä¸­çš„å…³ç¨è±å…æµç¨‹ã€‚è¯¥ç³»ç»Ÿé¦–å…ˆåˆ©ç”¨ OCR æŠ€æœ¯å¯¹å¤šæ ·åŒ–çš„ç”³è¯·æ–‡ä»¶åŠå…³ç¨æŒ‡ä»¤ç­‰ç›‘ç®¡æ–‡æœ¬è¿›è¡Œç²¾ç¡®æ•°å­—åŒ–æå–ï¼Œéšåå‘æŒ¥ LLM çš„èƒ½åŠ›ï¼Œè‡ªåŠ¨åŒ–å¤„ç†åŸæœ¬è€—æ—¶ä¸”å…³é”®çš„ HS Tariff Codes æ ¸æŸ¥ä»»åŠ¡ï¼ŒéªŒè¯æœºæ¢°è®¾å¤‡åŠåŸææ–™æ˜¯å¦ç¬¦åˆå®˜æ–¹è±å…æ¸…å•ã€‚é€šè¿‡æå‡åˆæ­¥è¯„ä¼°çš„é€Ÿåº¦ä¸ç²¾åº¦ï¼Œè¯¥äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ–¹æ³•ç³»ç»Ÿæ€§åœ°å‡å°‘äº†è±å…åˆ©ç”¨ä¸å……åˆ†æˆ–ä¸åŒ¹é…çš„æƒ…å†µï¼Œæ˜¾è‘—ç®€åŒ–äº† FDI ä¼ä¸šçš„æŠ•èµ„è·¯å¾„ã€‚å¯¹äºå›½å®¶ç¨åŠ¡éƒ¨é—¨è€Œè¨€ï¼Œè¯¥æ¡†æ¶ä¸ä»…å¤§å¹…æå‡äº†è¿è¥æ•ˆèƒ½å¹¶å‡è½»äº†è¡Œæ”¿è´Ÿæ‹…ï¼Œè¿˜é€šè¿‡å¼ºåŒ–ç®¡æ§ç¯å¢ƒæ”¹å–„äº†æ•´ä½“è¥å•†ç¯å¢ƒã€‚è¿™ä¸€åˆ›æ–°æ–¹æ¡ˆé€šè¿‡æŠ€æœ¯èµ‹èƒ½å¢å¼ºäº†å›½å®¶å¸å¼•é«˜ä»·å€¼åˆ¶é€ ä¸šæŠ•èµ„çš„ç«äº‰åŠ›ï¼Œä¸ºå®ç°é«˜æ•ˆã€ç²¾å‡†çš„æŠ•èµ„ä¿ƒè¿›æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "econ.GN"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12093v1",
      "published_date": "2025-06-12 08:20:38 UTC",
      "updated_date": "2025-06-12 08:20:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:21.205904+00:00"
    },
    {
      "arxiv_id": "2506.10467v4",
      "title": "Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and Cybersecurity Applications",
      "title_zh": "å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿçš„è§„èŒƒä¸è¯„ä¼°â€”â€”åŸå‹åŠå…¶åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨",
      "authors": [
        "Felix HÃ¤rer"
      ],
      "abstract": "Recent advancements in LLMs indicate potential for novel applications, as evidenced by the reasoning capabilities in the latest OpenAI and DeepSeek models. To apply these models to domain-specific applications beyond text generation, LLM-based multi-agent systems can be utilized to solve complex tasks, particularly by combining reasoning techniques, code generation, and software execution across multiple, potentially specialized LLMs. However, while many evaluations are performed on LLMs, reasoning techniques, and applications individually, their joint specification and combined application are not well understood. Defined specifications for multi-agent LLM systems are required to explore their potential and suitability for specific applications, allowing for systematic evaluations of LLMs, reasoning techniques, and related aspects. This paper reports the results of exploratory research on (1.) multi-agent specification by introducing an agent schema language and (2.) the execution and evaluation of the specifications through a multi-agent system architecture and prototype. The specification language, system architecture, and prototype are first presented in this work, building on an LLM system from prior research. Test cases involving cybersecurity tasks indicate the feasibility of the architecture and evaluation approach. As a result, evaluations could be demonstrated for question answering, server security, and network security tasks completed correctly by agents with LLMs from OpenAI and DeepSeek.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“ LLM ç³»ç»Ÿåœ¨å¤æ‚é¢†åŸŸåº”ç”¨ä¸­ç¼ºä¹ç»Ÿä¸€è§„èŒƒå’Œè¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸“ç”¨çš„æ™ºèƒ½ä½“æ¨¡å¼è¯­è¨€ (agent schema language) ä»¥åŠç›¸åº”çš„ç³»ç»Ÿæ¶æ„ä¸åŸå‹ã€‚é€šè¿‡å¼•å…¥è¿™ç§è§„èŒƒåŒ–çš„è¯­è¨€ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¯¹ä¸åŒ LLM çš„æ¨ç†èƒ½åŠ›ã€ä»£ç ç”ŸæˆåŠè½¯ä»¶æ‰§è¡Œè¿›è¡Œç³»ç»ŸåŒ–çš„å»ºæ¨¡ä¸è”åˆè¯„ä¼°ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç½‘ç»œå®‰å…¨ (cybersecurity) é¢†åŸŸçš„ä»»åŠ¡å¯¹è¯¥åŸå‹è¿›è¡Œäº†éªŒè¯ï¼Œæ¶µç›–äº†é—®ç­”ã€æœåŠ¡å™¨å®‰å…¨åŠç½‘ç»œå®‰å…¨ç­‰å…·ä½“åº”ç”¨åœºæ™¯ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¶æ„çš„å¯è¡Œæ€§ï¼Œæ˜¾ç¤ºåŸºäº OpenAI å’Œ DeepSeek æ¨¡å‹çš„æ™ºèƒ½ä½“èƒ½å¤Ÿå‡†ç¡®å®Œæˆå¤æ‚çš„å®‰å…¨ä»»åŠ¡ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ¢ç´¢å¤šæ™ºèƒ½ä½“ LLM ç³»ç»Ÿçš„åº”ç”¨æ½œåŠ›ä»¥åŠå»ºç«‹ç³»ç»ŸåŒ–çš„è¯„ä¼°ä½“ç³»å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "This work has been submitted for publication. Copyright may be transferred. In this case, this version will be updated with a notice, according to the publisher's guidelines",
      "pdf_url": "https://arxiv.org/pdf/2506.10467v4",
      "published_date": "2025-06-12 08:16:17 UTC",
      "updated_date": "2025-07-19 20:15:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:29.113799+00:00"
    },
    {
      "arxiv_id": "2506.10463v1",
      "title": "Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization",
      "title_zh": "èµ·ç‚¹è‡³å…³é‡è¦ï¼šç¥ç»ç½‘ç»œé‡åŒ–ä¸­æ›´ä¼˜æƒé‡åˆå§‹åŒ–çš„ç ”ç©¶",
      "authors": [
        "Stone Yun",
        "Alexander Wong"
      ],
      "abstract": "Deep neural network (DNN) quantization for fast, efficient inference has been an important tool in limiting the cost of machine learning (ML) model inference. Quantization-specific model development techniques such as regularization, quantization-aware training, and quantization-robustness penalties have served to greatly boost the accuracy and robustness of modern DNNs. However, very little exploration has been done on improving the initial conditions of DNN training for quantization. Just as random weight initialization has been shown to significantly impact test accuracy of floating point models, it would make sense that different weight initialization methods impact quantization robustness of trained models. We present an extensive study examining the effects of different weight initializations on a variety of CNN building blocks commonly used in efficient CNNs. This analysis reveals that even with varying CNN architectures, the choice of random weight initializer can significantly affect final quantization robustness. Next, we explore a new method for quantization-robust CNN initialization -- using Graph Hypernetworks (GHN) to predict parameters of quantized DNNs. Besides showing that GHN-predicted parameters are quantization-robust after regular float32 pretraining (of the GHN), we find that finetuning GHNs to predict parameters for quantized graphs (which we call GHN-QAT) can further improve quantized accuracy of CNNs. Notably, GHN-QAT shows significant accuracy improvements for even 4-bit quantization and better-than-random accuracy for 2-bits. To the best of our knowledge, this is the first in-depth study on quantization-aware DNN weight initialization. GHN-QAT offers a novel approach to quantized DNN model design. Future investigations, such as using GHN-QAT-initialized parameters for quantization-aware training, can further streamline the DNN quantization process.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æƒé‡åˆå§‹åŒ–(Weight Initialization)å¯¹æ·±åº¦ç¥ç»ç½‘ç»œ(DNN)é‡åŒ–æ€§èƒ½çš„å½±å“ï¼Œå¡«è¡¥äº†é‡åŒ–ç ”ç©¶é¢†åŸŸä¸­åˆå§‹æ¡ä»¶æ¢ç´¢çš„ç©ºç™½ã€‚ä½œè€…é€šè¿‡å¯¹å¤šç§é«˜æ•ˆCNNæ¶æ„çš„å®éªŒå‘ç°ï¼Œéšæœºæƒé‡åˆå§‹åŒ–çš„é€‰æ‹©ä¼šæ˜¾è‘—å½±å“è®­ç»ƒåæ¨¡å‹çš„é‡åŒ–é²æ£’æ€§(Quantization Robustness)ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å›¾è¶…ç½‘ç»œ(Graph Hypernetworks, GHN)é¢„æµ‹é‡åŒ–å‚æ•°çš„æ–°æ–¹æ³•ï¼Œå¹¶è¿›ä¸€æ­¥å¼€å‘äº†é’ˆå¯¹é‡åŒ–å›¾é¢„æµ‹è¿›è¡Œå¾®è°ƒçš„GHN-QATæŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGHN-QATåœ¨4-bitç”šè‡³2-bité‡åŒ–è®¾ç½®ä¸‹å‡èƒ½æ˜¾è‘—æå‡æ¨¡å‹å‡†ç¡®ç‡ã€‚ä½œä¸ºé¦–ä¸ªé’ˆå¯¹é‡åŒ–æ„ŸçŸ¥æƒé‡åˆå§‹åŒ–è¿›è¡Œçš„æ·±å…¥ç ”ç©¶ï¼Œè¯¥å·¥ä½œä¸ä»…ä¸ºé‡åŒ–æ¨¡å‹è®¾è®¡æä¾›äº†æ–°é¢–çš„GHN-QATæ–¹æ¡ˆï¼Œä¹Ÿä¸ºæœªæ¥ç»“åˆé‡åŒ–æ„ŸçŸ¥è®­ç»ƒ(Quantization-Aware Training)è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Portions of this article have been presented as extended abstracts at the ICCV 2023 Workshop on Low Bit Quantized Neural Networks (ICCVW-LBQNN 2023) and the 2020 Conference on Vision and Intelligent Systems (CVIS 2020). arXiv admin note: text overlap with arXiv:2011.14578, arXiv:2208.12489, arXiv:2309.13773",
      "pdf_url": "https://arxiv.org/pdf/2506.10463v1",
      "published_date": "2025-06-12 08:11:34 UTC",
      "updated_date": "2025-06-12 08:11:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:28.048222+00:00"
    },
    {
      "arxiv_id": "2506.10460v1",
      "title": "Equitable Mechanism Design for Facility Location",
      "title_zh": "è®¾æ–½é€‰å€ä¸­çš„å…¬å¹³æœºåˆ¶è®¾è®¡",
      "authors": [
        "Toby Walsh"
      ],
      "abstract": "We consider strategy proof mechanisms for facility location which maximize equitability between agents. As is common in the literature, we measure equitability with the Gini index. We first prove a simple but fundamental impossibility result that no strategy proof mechanism can bound the approximation ratio of the optimal Gini index of utilities for one or more facilities. We propose instead computing approximation ratios of the complemented Gini index of utilities, and consider how well both deterministic and randomized mechanisms approximate this. In addition, as Nash welfare is often put forwards as an equitable compromise between egalitarian and utilitarian outcomes, we consider how well mechanisms approximate the Nash welfare.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ—¨åœ¨æœ€å¤§åŒ–æ™ºèƒ½ä½“é—´å…¬å¹³æ€§çš„è®¾æ–½é€‰å€ç­–ç•¥è¯æ˜æœºåˆ¶ (Strategy proof mechanisms for facility location)ï¼Œå¹¶ä¸»è¦é‡‡ç”¨åŸºå°¼ç³»æ•° (Gini index) ä½œä¸ºè¡¡é‡å…¬å¹³æ€§çš„æ ‡å‡†ã€‚ç ”ç©¶é¦–å…ˆè¯æ˜äº†ä¸€ä¸ªåŸºç¡€æ€§çš„ä¸å¯èƒ½ç»“æœï¼Œå³æ²¡æœ‰ä»»ä½•ç­–ç•¥è¯æ˜æœºåˆ¶èƒ½å¤Ÿé™åˆ¶æ•ˆç”¨æœ€ä¼˜åŸºå°¼ç³»æ•°çš„è¿‘ä¼¼æ¯” (approximation ratio)ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºè½¬è€Œè®¡ç®—æ•ˆç”¨è¡¥å……åŸºå°¼ç³»æ•° (complemented Gini index) çš„è¿‘ä¼¼æ¯”ï¼Œå¹¶è¯„ä¼°äº†ç¡®å®šæ€§æœºåˆ¶ (deterministic mechanisms) å’Œéšæœºæœºåˆ¶ (randomized mechanisms) åœ¨æ­¤æŒ‡æ ‡ä¸‹çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°çº³ä»€ç¦åˆ© (Nash welfare) å¸¸ä½œä¸ºå¹³ç­‰ä¸»ä¹‰ä¸åŠŸåˆ©ä¸»ä¹‰ä¹‹é—´çš„å…¬å¹³æŠ˜è¡·ï¼Œç ”ç©¶è¿˜è¿›ä¸€æ­¥åˆ†æäº†å„æœºåˆ¶å¯¹çº³ä»€ç¦åˆ©çš„è¿‘ä¼¼ç¨‹åº¦ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "To appear in Proceedings of IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10460v1",
      "published_date": "2025-06-12 08:08:58 UTC",
      "updated_date": "2025-06-12 08:08:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:34.730207+00:00"
    },
    {
      "arxiv_id": "2506.10424v1",
      "title": "SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks",
      "title_zh": "SOFTï¼šé¢å‘æˆå‘˜æ¨ç†æ”»å‡»é˜²å¾¡çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒé€‰æ‹©æ€§æ•°æ®æ··æ·†æŠ€æœ¯",
      "authors": [
        "Kaiyuan Zhang",
        "Siyuan Cheng",
        "Hanxi Guo",
        "Yuetian Chen",
        "Zian Su",
        "Shengwei An",
        "Yuntao Du",
        "Charles Fleming",
        "Ashish Kundu",
        "Xiangyu Zhang",
        "Ninghui Li"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable success and are widely adopted for diverse applications. However, fine-tuning these models often involves private or sensitive information, raising critical privacy concerns. In this work, we conduct the first comprehensive study evaluating the vulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our empirical analysis demonstrates that MIAs exploit the loss reduction during fine-tuning, making them highly effective in revealing membership information. These findings motivate the development of our defense. We propose SOFT (\\textbf{S}elective data \\textbf{O}bfuscation in LLM \\textbf{F}ine-\\textbf{T}uning), a novel defense technique that mitigates privacy leakage by leveraging influential data selection with an adjustable parameter to balance utility preservation and privacy protection. Our extensive experiments span six diverse domains and multiple LLM architectures and scales. Results show that SOFT effectively reduces privacy risks while maintaining competitive model performance, offering a practical and scalable solution to safeguard sensitive information in fine-tuned LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¾®è°ƒ(Fine-tuning)è¿‡ç¨‹ä¸­æ¶‰åŠç§å¯†ä¿¡æ¯å¯¼è‡´çš„éšç§é£é™©ï¼Œå¯¹æˆå‘˜æ¨ç†æ”»å‡»(Membership Inference Attacks, MIAs)çš„è„†å¼±æ€§è¿›è¡Œäº†é¦–æ¬¡å…¨é¢ç ”ç©¶ã€‚å®éªŒå‘ç°æ”»å‡»è€…èƒ½å¤Ÿåˆ©ç”¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å‡å°‘(Loss reduction)æ¥è¯†åˆ«æˆå‘˜ä¿¡æ¯ï¼Œä¸ºæ­¤ä½œè€…æå‡ºäº†åä¸ºSOFT (Selective data Obfuscation in LLM Fine-Tuning)çš„æ–°å‹é˜²å¾¡æŠ€æœ¯ã€‚SOFTé€šè¿‡æœ‰å½±å“åŠ›çš„æ•°æ®é€‰æ‹©(Influential data selection)å’Œå¯è°ƒèŠ‚å‚æ•°ï¼Œæ—¨åœ¨å¹³è¡¡æ¨¡å‹æ•ˆç”¨ä¸éšç§ä¿æŠ¤ã€‚åœ¨æ¶µç›–å…­ä¸ªé¢†åŸŸåŠå¤šç§LLMæ¶æ„çš„å¹¿æ³›å®éªŒä¸­ï¼ŒSOFTè¢«è¯æ˜èƒ½æœ‰æ•ˆé™ä½éšç§æ³„éœ²é£é™©å¹¶ä¿æŒç«äº‰æ€§çš„æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ¡ˆä¸ºä¿éšœå¾®è°ƒLLMsä¸­çš„æ•æ„Ÿä¿¡æ¯æä¾›äº†ä¸€ç§å®ç”¨ä¸”å…·æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by the 34th USENIX Security Symposium 2025. Code is available at https://github.com/KaiyuanZh/SOFT",
      "pdf_url": "https://arxiv.org/pdf/2506.10424v1",
      "published_date": "2025-06-12 07:23:56 UTC",
      "updated_date": "2025-06-12 07:23:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:35.171531+00:00"
    },
    {
      "arxiv_id": "2506.10423v2",
      "title": "PAL: Probing Audio Encoders via LLMs - Audio Information Transfer into LLMs",
      "title_zh": "PALï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„éŸ³é¢‘ç¼–ç å™¨æ¢æµ‹â€”â€”éŸ³é¢‘ä¿¡æ¯å‘å¤§è¯­è¨€æ¨¡å‹çš„è¿ç§»",
      "authors": [
        "Tony Alex",
        "Wish Suharitdamrong",
        "Sara Atito",
        "Armin Mustafa",
        "Philip J. B. Jackson",
        "Imran Razzak",
        "Muhammad Awais"
      ],
      "abstract": "Integration of audio perception into large language models (LLMs) is an emerging research area for enabling machine listening applications, yet efficient transfer of rich audio semantics from audio encoders to LLMs remains underexplored. The most widely used integration paradigm projects the audio encoder output tokens into the LLM input space (e.g., via an MLP or a Q-Former), then prepends or inserts them to the text tokens. We refer to this generic scheme as Prepend to the LLM's input token space (PLITS) integration. We propose an efficient alternative, Lightweight Audio LLM Integration (LAL). LAL introduces audio representations solely via the attention mechanism within different layers of the LLM, bypassing its feedforward module. LAL encodes rich audio semantics at an appropriate level of abstraction for integration into different blocks of LLMs. Our design significantly reduces computational overhead compared to existing integration approaches. Observing with Whisper that the speech encoder benefits from PLITS integration, we propose an audio encoder aware approach for efficiently Probing Audio encoders via LLM (PAL), which employs PLITS integration for Whisper and LAL for general audio encoders. Under an identical training curriculum, LAL consistently maintains performance or outperforms existing integration approaches across multiple base LLMs and tasks. For general audio tasks, LAL improvement is up to 30% over a strong PLITS baseline while reducing memory usage by up to 64.1% and increasing throughput by up to 247.5%. Furthermore, for general audio-music-speech LLM, PAL performs on par with a fully PLITS integration-based system but with substantially improved computational and memory efficiency. Project page: https://ta012.github.io/PAL/",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†éŸ³é¢‘è¯­ä¹‰é«˜æ•ˆåœ°è½¬ç§»åˆ°å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºLightweight Audio LLM Integration (LAL)çš„é«˜æ•ˆé›†æˆæ–¹æ¡ˆã€‚ä¸ä¼ ç»Ÿå°†éŸ³é¢‘æ ‡è®°æ‹¼æ¥åˆ°è¾“å…¥ç©ºé—´çš„PLITSæ¨¡å¼ä¸åŒï¼ŒLALé€šè¿‡LLMsä¸åŒå±‚çº§çš„æ³¨æ„åŠ›æœºåˆ¶(attention mechanism)å¼•å…¥éŸ³é¢‘è¡¨ç¤ºï¼Œä»è€Œæœ‰æ•ˆç»•è¿‡å…¶å‰é¦ˆæ¨¡å—(feedforward module)ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº†PAL (Probing Audio encoders via LLM)æ¶æ„ï¼Œé’ˆå¯¹Whisperç¼–ç å™¨é‡‡ç”¨PLITSé›†æˆï¼Œè€Œå¯¹é€šç”¨éŸ³é¢‘ç¼–ç å™¨åº”ç”¨LALé›†æˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLALåœ¨é€šç”¨éŸ³é¢‘ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡æœ€é«˜è¾¾30%ï¼ŒåŒæ—¶å‡å°‘äº†64.1%çš„å†…å­˜ä½¿ç”¨å¹¶æ˜¾è‘—æé«˜äº†247.5%çš„ååé‡ã€‚æœ€ç»ˆæ„å»ºçš„éŸ³é¢‘-éŸ³ä¹-è¯­éŸ³LLMåœ¨ä¿æŒä¸å…¨PLITSç³»ç»Ÿç›¸å½“æ€§èƒ½çš„åŒæ—¶ï¼Œæå¤§åœ°æå‡äº†è®¡ç®—å’Œå†…å­˜æ•ˆç‡ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "17 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.10423v2",
      "published_date": "2025-06-12 07:23:07 UTC",
      "updated_date": "2025-10-14 20:14:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:35.626513+00:00"
    },
    {
      "arxiv_id": "2506.10420v2",
      "title": "Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods",
      "title_zh": "å¤„ç†æœåŠ¡çš„å¤šç»´è‡ªåŠ¨æ‰©ç¼©å®¹ï¼šåŸºäºæ™ºèƒ½ä½“æ–¹æ³•çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Boris Sedlak",
        "Alireza Furutanpey",
        "Zihang Wang",
        "VÃ­ctor Casamayor Pujol",
        "Schahram Dustdar"
      ],
      "abstract": "Edge computing breaks with traditional autoscaling due to strict resource constraints, thus, motivating more flexible scaling behaviors using multiple elasticity dimensions. This work introduces an agent-based autoscaling framework that dynamically adjusts both hardware resources and internal service configurations to maximize requirements fulfillment in constrained environments. We compare four types of scaling agents: Active Inference, Deep Q Network, Analysis of Structural Knowledge, and Deep Active Inference, using two real-world processing services running in parallel: YOLOv8 for visual recognition and OpenCV for QR code detection. Results show all agents achieve acceptable SLO performance with varying convergence patterns. While the Deep Q Network benefits from pre-training, the structural analysis converges quickly, and the deep active inference agent combines theoretical foundations with practical scalability advantages. Our findings provide evidence for the viability of multi-dimensional agent-based autoscaling for edge environments and encourage future work in this research direction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜è®¡ç®—(Edge computing)ä¸­ä¸¥æ ¼çš„èµ„æºé™åˆ¶ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºæ™ºèƒ½ä½“(agent-based)çš„å¤šç»´è‡ªåŠ¨æ‰©ç¼©å®¹(autoscaling)æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ç¡¬ä»¶èµ„æºå’Œå†…éƒ¨æœåŠ¡é…ç½®æ¥ä¼˜åŒ–å—é™ç¯å¢ƒä¸‹çš„éœ€æ±‚å±¥è¡Œã€‚ç ”ç©¶å¯¹æ¯”äº† Active Inferenceã€Deep Q Networkã€Analysis of Structural Knowledge å’Œ Deep Active Inference å››ç§ä¸åŒç±»å‹çš„æ™ºèƒ½ä½“ï¼Œå¹¶ä½¿ç”¨å¹¶è¡Œè¿è¡Œçš„ YOLOv8 è§†è§‰è¯†åˆ«å’Œ OpenCV äºŒç»´ç æ£€æµ‹ä¸¤ä¸ªçœŸå®æœåŠ¡è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æ™ºèƒ½ä½“åœ¨æœåŠ¡ç­‰çº§ç›®æ ‡(SLO)æ€§èƒ½ä¸Šå‡è¡¨ç°è‰¯å¥½ï¼Œä½†å±•ç°å‡ºä¸åŒçš„æ”¶æ•›ç‰¹æ€§ã€‚å…¶ä¸­ Deep Q Network å—ç›Šäºé¢„è®­ç»ƒï¼ŒAnalysis of Structural Knowledge å…·æœ‰æœ€å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œè€Œ Deep Active Inference åˆ™ç»“åˆäº†ç†è®ºåŸºç¡€ä¸å®é™…çš„å¯æ‰©å±•æ€§ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶è¯å®äº†å¤šç»´æ™ºèƒ½ä½“è‡ªåŠ¨æ‰©ç¼©å®¹åœ¨è¾¹ç¼˜ç¯å¢ƒä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºè¯¥ç ”ç©¶æ–¹å‘çš„æœªæ¥æ¢ç´¢æä¾›äº†æœ‰åŠ›è¯æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10420v2",
      "published_date": "2025-06-12 07:20:26 UTC",
      "updated_date": "2026-01-12 12:51:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:41.291713+00:00"
    },
    {
      "arxiv_id": "2506.11172v1",
      "title": "Collapsing Sequence-Level Data-Policy Coverage via Poisoning Attack in Offline Reinforcement Learning",
      "title_zh": "ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­é€šè¿‡ä¸­æ¯’æ”»å‡»ç“¦è§£åºåˆ—çº§æ•°æ®-ç­–ç•¥è¦†ç›–",
      "authors": [
        "Xue Zhou",
        "Dapeng Man",
        "Chen Xu",
        "Fanyi Zeng",
        "Tao Liu",
        "Huan Wang",
        "Shucheng He",
        "Chaoyang Gao",
        "Wu Yang"
      ],
      "abstract": "Offline reinforcement learning (RL) heavily relies on the coverage of pre-collected data over the target policy's distribution. Existing studies aim to improve data-policy coverage to mitigate distributional shifts, but overlook security risks from insufficient coverage, and the single-step analysis is not consistent with the multi-step decision-making nature of offline RL. To address this, we introduce the sequence-level concentrability coefficient to quantify coverage, and reveal its exponential amplification on the upper bound of estimation errors through theoretical analysis. Building on this, we propose the Collapsing Sequence-Level Data-Policy Coverage (CSDPC) poisoning attack. Considering the continuous nature of offline RL data, we convert state-action pairs into decision units, and extract representative decision patterns that capture multi-step behavior. We identify rare patterns likely to cause insufficient coverage, and poison them to reduce coverage and exacerbate distributional shifts. Experiments show that poisoning just 1% of the dataset can degrade agent performance by 90%. This finding provides new perspectives for analyzing and safeguarding the security of offline RL.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline Reinforcement Learning)ä¸­æ•°æ®ç­–ç•¥è¦†ç›–(data-policy coverage)ä¸è¶³å¸¦æ¥çš„å®‰å…¨é£é™©ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„å•æ­¥åˆ†ææ³•ä¸å¤šæ­¥å†³ç­–çš„æœ¬è´¨ä¸ç¬¦ã€‚ä½œè€…å¼•å…¥äº†åºåˆ—çº§é›†ä¸­åº¦ç³»æ•°(sequence-level concentrability coefficient)æ¥é‡åŒ–è¦†ç›–åº¦ï¼Œå¹¶é€šè¿‡ç†è®ºåˆ†æè¯æ˜å…¶å¯¹ä¼°è®¡è¯¯å·®ä¸Šç•Œå…·æœ‰æŒ‡æ•°æ”¾å¤§æ•ˆåº”ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æå‡ºäº†åä¸ºCSDPC(Collapsing Sequence-Level Data-Policy Coverage)çš„æŠ•æ¯’æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†çŠ¶æ€-åŠ¨ä½œå¯¹è½¬æ¢ä¸ºå†³ç­–å•å…ƒï¼Œé€šè¿‡è¯†åˆ«å¹¶æŠ•æ¯’å¯èƒ½å¯¼è‡´è¦†ç›–ä¸è¶³çš„ç¨€æœ‰å†³ç­–æ¨¡å¼æ¥åŠ å‰§åˆ†å¸ƒåç§»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»…éœ€æŠ•æ¯’1%çš„æ•°æ®é›†å³å¯å¯¼è‡´æ™ºèƒ½ä½“æ€§èƒ½ä¸‹é™90%ã€‚è¯¥ç ”ç©¶ä¸ºç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„å®‰å…¨åˆ†æä¸é˜²æŠ¤æä¾›äº†å…¨æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11172v1",
      "published_date": "2025-06-12 07:11:27 UTC",
      "updated_date": "2025-06-12 07:11:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:43.666570+00:00"
    },
    {
      "arxiv_id": "2506.21564v1",
      "title": "Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing",
      "title_zh": "SemEval-2025 ä»»åŠ¡ 10 ä¸­çš„ Team QUSTï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨æ–°é—»å®ä½“æ¡†æ¶åŒ–å¤šç±»åˆ«å¤šæ ‡ç­¾åˆ†ç±»ä¸­çš„è¯„ä¼°",
      "authors": [
        "Jiyan Liu",
        "Youzheng Liu",
        "Taihang Wang",
        "Xiaoman Xu",
        "Yimin Wang",
        "Ye Jiang"
      ],
      "abstract": "This paper describes the participation of QUST_NLP in the SemEval-2025 Task 7. We propose a three-stage retrieval framework specifically designed for fact-checked claim retrieval. Initially, we evaluate the performance of several retrieval models and select the one that yields the best results for candidate retrieval. Next, we employ multiple re-ranking models to enhance the candidate results, with each model selecting the Top-10 outcomes. In the final stage, we utilize weighted voting to determine the final retrieval outcomes. Our approach achieved 5th place in the monolingual track and 7th place in the crosslingual track. We release our system code at: https://github.com/warmth27/SemEval2025_Task7.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†QUST_NLPå›¢é˜Ÿåœ¨SemEval-2025 Task 7ä¸­çš„å‚èµ›æ–¹æ¡ˆï¼Œæå‡ºäº†ä¸€ç§ä¸“ä¸ºäº‹å®æ ¸æŸ¥å£°æ˜æ£€ç´¢(fact-checked claim retrieval)è®¾è®¡çš„ä¸‰é˜¶æ®µæ£€ç´¢æ¡†æ¶(three-stage retrieval framework)ã€‚è¯¥æ¡†æ¶åœ¨ç¬¬ä¸€é˜¶æ®µé€šè¿‡è¯„ä¼°å¤šä¸ªæ£€ç´¢æ¨¡å‹å¹¶é€‰å–æœ€ä¼˜è€…è¿›è¡Œå€™é€‰æ£€ç´¢ï¼Œç¬¬äºŒé˜¶æ®µåˆ©ç”¨å¤šä¸ªé‡æ’åºæ¨¡å‹(re-ranking models)å¯¹ç»“æœè¿›è¡Œå¢å¼ºå¹¶ç­›é€‰å‡ºTop-10ç»“æœã€‚åœ¨æœ€ç»ˆé˜¶æ®µï¼Œç ”ç©¶é‡‡ç”¨åŠ æƒæŠ•ç¥¨(weighted voting)æœºåˆ¶æ¥ç¡®å®šæœ€ç»ˆçš„æ£€ç´¢äº§å‡ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å•è¯­èµ›é“(monolingual track)ä½åˆ—ç¬¬5åï¼Œåœ¨è·¨è¯­è¨€èµ›é“(crosslingual track)ä½åˆ—ç¬¬7åã€‚ç›®å‰è¯¥ç³»ç»Ÿçš„ä»£ç å·²åœ¨GitHubå¼€æºï¼Œä¸ºå¤šè¯­è¨€äº‹å®æ ¸æŸ¥ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„æ£€ç´¢ç­–ç•¥å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21564v1",
      "published_date": "2025-06-12 07:09:35 UTC",
      "updated_date": "2025-06-12 07:09:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:25:07.435699+00:00"
    },
    {
      "arxiv_id": "2506.10412v4",
      "title": "Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series",
      "title_zh": "Time-IMMï¼šé¢å‘ä¸è§„åˆ™å¤šæ¨¡æ€å¤šå˜é‡æ—¶é—´åºåˆ—çš„æ•°æ®é›†ä¸åŸºå‡†",
      "authors": [
        "Ching Chang",
        "Jeehyun Hwang",
        "Yidan Shi",
        "Haixin Wang",
        "Wen-Chih Peng",
        "Tien-Fu Chen",
        "Wei Wang"
      ],
      "abstract": "Time series data in real-world applications such as healthcare, climate modeling, and finance are often irregular, multimodal, and messy, with varying sampling rates, asynchronous modalities, and pervasive missingness. However, existing benchmarks typically assume clean, regularly sampled, unimodal data, creating a significant gap between research and real-world deployment. We introduce Time-IMM, a dataset specifically designed to capture cause-driven irregularity in multimodal multivariate time series. Time-IMM represents nine distinct types of time series irregularity, categorized into trigger-based, constraint-based, and artifact-based mechanisms. Complementing the dataset, we introduce IMM-TSF, a benchmark library for forecasting on irregular multimodal time series, enabling asynchronous integration and realistic evaluation. IMM-TSF includes specialized fusion modules, including a timestamp-to-text fusion module and a multimodality fusion module, which support both recency-aware averaging and attention-based integration strategies. Empirical results demonstrate that explicitly modeling multimodality on irregular time series data leads to substantial gains in forecasting performance. Time-IMM and IMM-TSF provide a foundation for advancing time series analysis under real-world conditions. The dataset is publicly available at https://github.com/blacksnail789521/Time-IMM, and the benchmark library can be accessed at https://github.com/blacksnail789521/IMM-TSF. Project page: https://blacksnail789521.github.io/time-imm-project-page/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—ã€æ°”å€™å’Œé‡‘èç­‰ç°å®åº”ç”¨ä¸­æ—¶é—´åºåˆ—æ•°æ®æ™®éå­˜åœ¨çš„å¼‚æ­¥é‡‡æ ·ã€æ¨¡æ€ä¸ç»Ÿä¸€åŠæ•°æ®ç¼ºå¤±ç­‰ä¸è§„åˆ™æ€§é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨å¤„ç†æ­¤ç±»å¤æ‚æ•°æ®æ—¶çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Time-IMMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ•è·å¤šæ¨¡æ€å¤šå…ƒæ—¶é—´åºåˆ—(Multimodal Multivariate Time Series)ä¸­å› æœé©±åŠ¨ä¸è§„åˆ™æ€§è€Œè®¾è®¡çš„å…¨æ–°æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†åŸºäºè§¦å‘(trigger-based)ã€åŸºäºçº¦æŸ(constraint-based)å’ŒåŸºäºäººå·¥ç—•è¿¹(artifact-based)ä¸‰å¤§ç±»å…±ä¹ç§ç‹¬ç‰¹çš„ä¸è§„åˆ™ç±»å‹ã€‚åŒæ—¶ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†é…å¥—çš„åŸºå‡†åº“IMM-TSFï¼Œæ”¯æŒåœ¨ä¸è§„åˆ™å¤šæ¨¡æ€æ—¶é—´åºåˆ—ä¸Šè¿›è¡Œé¢„æµ‹ä»»åŠ¡ã€å¼‚æ­¥é›†æˆå’ŒçœŸå®åœºæ™¯è¯„ä¼°ã€‚IMM-TSFåŒ…å«ä¸“é—¨çš„èåˆæ¨¡å—ï¼Œå¦‚æ—¶é—´æˆ³è½¬æ–‡æœ¬èåˆ(timestamp-to-text fusion)å’Œå¤šæ¨¡æ€èåˆæ¨¡å—ï¼Œå¹¶æä¾›åŸºäºæ–°è¿‘åº¦æ„ŸçŸ¥å¹³å‡(recency-aware averaging)å’Œæ³¨æ„åŠ›æœºåˆ¶çš„é›†æˆç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸è§„åˆ™æ—¶é—´åºåˆ—æ•°æ®ä¸Šæ˜¾å¼å»ºæ¨¡å¤šæ¨¡æ€ç‰¹æ€§å¯æ˜¾è‘—æå‡é¢„æµ‹æ€§èƒ½ã€‚Time-IMMå’ŒIMM-TSFä¸ºæ¨è¿›çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹çš„æ—¶é—´åºåˆ—åˆ†æå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted by the NeurIPS 2025 Datasets and Benchmarks Track",
      "pdf_url": "https://arxiv.org/pdf/2506.10412v4",
      "published_date": "2025-06-12 07:07:22 UTC",
      "updated_date": "2025-10-15 08:31:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:24:51.319188+00:00"
    },
    {
      "arxiv_id": "2506.17272v1",
      "title": "QUST_NLP at SemEval-2025 Task 7: A Three-Stage Retrieval Framework for Monolingual and Crosslingual Fact-Checked Claim Retrieval",
      "title_zh": "QUST_NLP åœ¨ SemEval-2025 ä»»åŠ¡ 7ï¼šä¸€ç§ç”¨äºå•è¯­åŠè·¨è¯­è¨€å·²æ ¸æŸ¥ä¸»å¼ æ£€ç´¢çš„ä¸‰é˜¶æ®µæ£€ç´¢æ¡†æ¶",
      "authors": [
        "Youzheng Liu",
        "Jiyan Liu",
        "Xiaoman Xu",
        "Taihang Wang",
        "Yimin Wang",
        "Ye Jiang"
      ],
      "abstract": "This paper describes the participation of QUST_NLP in the SemEval-2025 Task 7. We propose a three-stage retrieval framework specifically designed for fact-checked claim retrieval. Initially, we evaluate the performance of several retrieval models and select the one that yields the best results for candidate retrieval. Next, we employ multiple re-ranking models to enhance the candidate results, with each model selecting the Top-10 outcomes. In the final stage, we utilize weighted voting to determine the final retrieval outcomes. Our approach achieved 5th place in the monolingual track and 7th place in the crosslingual track. We release our system code at: https://github.com/warmth27/SemEval2025_Task7",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†QUST_NLPå›¢é˜Ÿåœ¨SemEval-2025 Task 7ä¸­çš„å‚èµ›æ–¹æ¡ˆï¼Œæå‡ºäº†ä¸€ç§ä¸“ä¸ºäº‹å®æ ¸æŸ¥å£°æ˜æ£€ç´¢(Fact-Checked Claim Retrieval)è®¾è®¡çš„ä¸‰é˜¶æ®µæ£€ç´¢æ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œç ”ç©¶è€…é€šè¿‡è¯„ä¼°å¤šç§æ¨¡å‹é€‰æ‹©æ€§èƒ½æœ€ä¼˜è€…è¿›è¡Œå€™é€‰æ£€ç´¢(Candidate Retrieval)ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨å¤šä¸ªé‡æ’åºæ¨¡å‹(Re-ranking Models)å¢å¼ºå€™é€‰ç»“æœï¼Œå¹¶ç”±æ¯ä¸ªæ¨¡å‹ç­›é€‰å‡ºTop-10ç»“æœï¼›æœ€ç»ˆé˜¶æ®µé‡‡ç”¨åŠ æƒæŠ•ç¥¨(Weighted Voting)æœºåˆ¶ç¡®å®šæ£€ç´¢äº§å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•è¯­(Monolingual)èµ›é“æ’åç¬¬5ï¼Œåœ¨è·¨è¯­(Crosslingual)èµ›é“æ’åç¬¬7ï¼ŒéªŒè¯äº†å¤šé˜¶æ®µæ¡†æ¶åœ¨äº‹å®æ ¸æŸ¥ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç›®å‰è¯¥ç³»ç»Ÿçš„ä»£ç å·²åœ¨GitHubå¼€æºï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†å‚è€ƒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.17272v1",
      "published_date": "2025-06-12 07:06:40 UTC",
      "updated_date": "2025-06-12 07:06:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:25:23.190026+00:00"
    },
    {
      "arxiv_id": "2506.10408v1",
      "title": "Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges",
      "title_zh": "åŸºäºç³»ç»Ÿ 1 æˆ–ç³»ç»Ÿ 2 çš„æ¨ç† RAGï¼šé¢å‘è¡Œä¸šæŒ‘æˆ˜çš„æ¨ç†æ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆç»¼è¿°",
      "authors": [
        "Jintao Liang",
        "Gang Su",
        "Huifeng Lin",
        "You Wu",
        "Rui Zhao",
        "Ziyue Li"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to overcome the knowledge limitations of Large Language Models (LLMs) by integrating external retrieval with language generation. While early RAG systems based on static pipelines have shown effectiveness in well-structured tasks, they struggle in real-world scenarios requiring complex reasoning, dynamic retrieval, and multi-modal integration. To address these challenges, the field has shifted toward Reasoning Agentic RAG, a paradigm that embeds decision-making and adaptive tool use directly into the retrieval process. In this paper, we present a comprehensive review of Reasoning Agentic RAG methods, categorizing them into two primary systems: predefined reasoning, which follows fixed modular pipelines to boost reasoning, and agentic reasoning, where the model autonomously orchestrates tool interaction during inference. We analyze representative techniques under both paradigms, covering architectural design, reasoning strategies, and tool coordination. Finally, we discuss key research challenges and propose future directions to advance the flexibility, robustness, and applicability of reasoning agentic RAG systems. Our collection of the relevant research has been organized into a https://github.com/ByebyeMonica/Reasoning-Agentic-RAG.",
      "tldr_zh": "è¯¥ç»¼è¿°æ·±å…¥æ¢è®¨äº†æ¨ç†æ™ºèƒ½æ£€ç´¢å¢å¼ºç”Ÿæˆ(Reasoning Agentic RAG)è¿™ä¸€æ–°å…´èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯åœ¨å¤æ‚æ¨ç†ã€åŠ¨æ€æ£€ç´¢å’Œå¤šæ¨¡æ€é›†æˆæ–¹é¢çš„å±€é™æ€§ã€‚æ–‡ç« å°†ç›¸å…³æ–¹æ³•ç³»ç»Ÿåœ°åˆ’åˆ†ä¸ºé¢„å®šä¹‰æ¨ç†(predefined reasoning)å’Œæ™ºèƒ½ä½“æ¨ç†(agentic reasoning)ä¸¤å¤§ç±»ï¼Œåˆ†åˆ«å¯¹åº”å›ºå®šæ¨¡å—åŒ–æµç¨‹å’Œæ¨ç†æ—¶çš„è‡ªä¸»å·¥å…·åè°ƒã€‚é€šè¿‡å¯¹æ¶æ„è®¾è®¡ã€æ¨ç†ç­–ç•¥å’Œå·¥å…·ååŒç­‰æ ¸å¿ƒæŠ€æœ¯çš„è¯¦ç»†åˆ†æï¼Œè¯¥ç ”ç©¶å…¨é¢æ­ç¤ºäº†ä¸åŒèŒƒå¼ä¸‹çš„æŠ€æœ¯ç‰¹å¾ã€‚æœ€åï¼Œè®ºæ–‡æ€»ç»“äº†å½“å‰é¢ä¸´çš„å…³é”®æŠ€æœ¯æŒ‘æˆ˜ï¼Œå¹¶æŒ‡æ˜äº†æå‡ç³»ç»Ÿçµæ´»æ€§ä¸é²æ£’æ€§çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºæ¨åŠ¨è¯¥æŠ€æœ¯åœ¨å·¥ä¸šç•Œçš„å®é™…åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10408v1",
      "published_date": "2025-06-12 07:01:56 UTC",
      "updated_date": "2025-06-12 07:01:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:25:25.621420+00:00"
    },
    {
      "arxiv_id": "2506.10407v3",
      "title": "Semi-Tensor-Product Based Convolutional Neural Networks",
      "title_zh": "åŸºäºåŠå¼ é‡ç§¯çš„å·ç§¯ç¥ç»ç½‘ç»œ",
      "authors": [
        "Daizhan Cheng",
        "Xiao Zhang"
      ],
      "abstract": "The semi-tensor product of vectors generalizes the conventional inner product, enabling algebraic operations between vectors of different dimensions. Building upon this foundation, we introduce a domain-based convolutional product and integrate it with the STP to formulate a padding-free convolutional operation. This new operation inherently avoids zero or other artificial padding, thereby eliminating redundant information and boundary artifacts commonly present in conventional convolutional neural networks. Based on this operation, we further develop an STP-based CNN framework that extends convolutional computation to irregular and cross-dimensional data domains. Applications to image processing and third-order signal identification demonstrate the proposed method's effectiveness in handling irregular, incomplete, and high-dimensional data without the distortions caused by padding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºåŠå¼ é‡ç§¯ (Semi-Tensor Product, STP) çš„å·ç§¯ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œåˆ©ç”¨ STP æ¨å¹¿ä¼ ç»Ÿå†…ç§¯ä»¥å®ç°ä¸åŒç»´åº¦å‘é‡é—´çš„ä»£æ•°è¿ç®—ã€‚é€šè¿‡å°†åŸºäºåŸŸçš„å·ç§¯ç§¯ä¸ STP ç»“åˆï¼Œç ”ç©¶è€…å¼€å‘å‡ºä¸€ç§æ— å¡«å…… (padding-free) çš„å·ç§¯æ“ä½œï¼Œä»æ ¹æœ¬ä¸Šè§£å†³äº†ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œä¸­å› äººå·¥é›¶å¡«å……å¯¼è‡´çš„å†—ä½™ä¿¡æ¯å’Œè¾¹ç•Œä¼ªå½±é—®é¢˜ã€‚è¯¥æ¡†æ¶æˆåŠŸå°†å·ç§¯è®¡ç®—æ‰©å±•åˆ°ä¸è§„åˆ™å’Œè·¨ç»´åº¦çš„æ•°æ®åŸŸï¼Œä¸ºå¤„ç†å¤æ‚å‡ ä½•ç»“æ„çš„æ•°æ®æä¾›äº†æ–°æ‰‹æ®µã€‚åœ¨å›¾åƒå¤„ç†å’Œä¸‰é˜¶ä¿¡å·è¯†åˆ«çš„åº”ç”¨ä¸­ï¼Œè¯¥æ–¹æ³•å±•ç¤ºäº†åœ¨å¤„ç†ä¸è§„åˆ™ã€ä¸å®Œæ•´åŠé«˜ç»´æ•°æ®æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¯å®ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨ä¸äº§ç”Ÿå¡«å……å¤±çœŸçš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæå‡äº†å¯¹å¤æ‚æ•°æ®å»ºæ¨¡çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10407v3",
      "published_date": "2025-06-12 07:00:19 UTC",
      "updated_date": "2026-01-15 15:02:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:25:50.825470+00:00"
    },
    {
      "arxiv_id": "2506.10406v1",
      "title": "PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier",
      "title_zh": "PAGï¼šå°†ç­–ç•¥ä½œä¸ºç”Ÿæˆå¼éªŒè¯å™¨çš„å¤šè½®å¼ºåŒ–å¤§è¯­è¨€æ¨¡å‹è‡ªçº é”™",
      "authors": [
        "Yuhua Jiang",
        "Yuwen Xiong",
        "Yufeng Yuan",
        "Chao Xin",
        "Wenyuan Xu",
        "Yu Yue",
        "Qianchuan Zhao",
        "Lin Yan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks, yet they still struggle to reliably verify the correctness of their own outputs. Existing solutions to this verification challenge often depend on separate verifier models or require multi-stage self-correction training pipelines, which limit scalability. In this paper, we propose Policy as Generative Verifier (PAG), a simple and effective framework that empowers LLMs to self-correct by alternating between policy and verifier roles within a unified multi-turn reinforcement learning (RL) paradigm. Distinct from prior approaches that always generate a second attempt regardless of model confidence, PAG introduces a selective revision mechanism: the model revises its answer only when its own generative verification step detects an error. This verify-then-revise workflow not only alleviates model collapse but also jointly enhances both reasoning and verification abilities. Extensive experiments across diverse reasoning benchmarks highlight PAG's dual advancements: as a policy, it enhances direct generation and self-correction accuracy; as a verifier, its self-verification outperforms self-consistency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PAG (Policy as Generative Verifier)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›çš„æ¡†æ¶ã€‚PAGåœ¨ç»Ÿä¸€çš„å¤šè½®å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)èŒƒå¼ä¸‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç­–ç•¥(Policy)å’ŒéªŒè¯è€…(Verifier)è§’è‰²ä¹‹é—´äº¤æ›¿ã€‚ä¸åŒäºä»¥å¾€æ— è®ºæ¨¡å‹ç½®ä¿¡åº¦å¦‚ä½•å‡è¿›è¡ŒäºŒæ¬¡å°è¯•çš„æ–¹æ³•ï¼ŒPAGå¼•å…¥äº†é€‰æ‹©æ€§ä¿®æ­£æœºåˆ¶ï¼Œä»…åœ¨ç”Ÿæˆå¼éªŒè¯æ­¥éª¤æ£€æµ‹åˆ°é”™è¯¯æ—¶æ‰è¿›è¡Œä¿®è®¢ã€‚è¿™ç§å…ˆéªŒè¯åä¿®æ­£çš„å·¥ä½œæµæœ‰æ•ˆåœ°ç¼“è§£äº†æ¨¡å‹åç¼©é—®é¢˜ï¼Œå¹¶åŒæ­¥æå‡äº†æ¨¡å‹çš„æ¨ç†ä¸éªŒè¯æ°´å¹³ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒPAGæ˜¾è‘—æé«˜äº†ç›´æ¥ç”Ÿæˆä¸è‡ªæˆ‘ä¿®æ­£çš„å‡†ç¡®ç‡ï¼Œä¸”å…¶è‡ªæˆ‘éªŒè¯æ€§èƒ½ä¼˜äºSelf-Consistencyã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10406v1",
      "published_date": "2025-06-12 06:59:35 UTC",
      "updated_date": "2025-06-12 06:59:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:25:46.697629+00:00"
    },
    {
      "arxiv_id": "2506.10403v1",
      "title": "Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation",
      "title_zh": "å‘Šåˆ« LLM-as-a-Judgeï¼šç¨‹åºåŒ–è¯„ä¼°æ˜¯è¯„ä¼°çš„æœªæ¥",
      "authors": [
        "Tzu-Heng Huang",
        "Harit Vishwakarma",
        "Frederic Sala"
      ],
      "abstract": "Large language models (LLMs) are widely used to evaluate the quality of LLM generations and responses, but this leads to significant challenges: high API costs, uncertain reliability, inflexible pipelines, and inherent biases. To address these, we introduce PAJAMA (Program-As-a-Judge for Automated Model Assessment), a new alternative that uses LLMs to synthesize executable judging programs instead of directly scoring responses. These synthesized programs can be stored and run locally, costing orders of magnitude less while providing interpretable, and auditable judging logic that can be easily adapted. Program-based judges mitigate biases, improving judgment consistency by 15.83% and reducing biased responses by 23.7% on average compared to a Qwen2.5-14B-based LLM-as-a-judge. When program judgments are distilled into a model, PAJAMA outperforms LLM-as-a-judge on the challenging CHAT-HARD subset of RewardBench, outperforming metrics by 2.19% on Prometheus and 8.67% on the JudgeLM dataset, all at three orders of magnitude lower cost.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ LLM-as-a-judge åœ¨è¯„ä¼°å¤§æ¨¡å‹ç”Ÿæˆè´¨é‡æ—¶é¢ä¸´çš„é«˜æ˜‚ API æˆæœ¬ã€å¯é æ€§ä¸è¶³åŠå›ºæœ‰åå·®ç­‰é—®é¢˜ï¼Œæå‡ºäº†åä¸º PAJAMA (Program-As-a-Judge for Automated Model Assessment) çš„æ–°å‹è¯„ä¼°æ¡†æ¶ã€‚PAJAMA åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹åˆæˆå¯æ‰§è¡Œçš„è¯„å®¡ç¨‹åºè€Œéç›´æ¥è¿›è¡Œè¯„åˆ†ï¼Œè¿™äº›ç¨‹åºæ”¯æŒæœ¬åœ°è¿è¡Œï¼Œåœ¨å®ç°å¯è§£é‡Šä¸å¯å®¡è®¡é€»è¾‘çš„åŒæ—¶æ˜¾è‘—é™ä½äº†æˆæœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŸºäº Qwen2.5-14B çš„ä¼ ç»Ÿè¯„ä¼°æ–¹å¼ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å°†è¯„å®¡ä¸€è‡´æ€§æé«˜äº† 15.83%ï¼Œå¹¶å‡å°‘äº† 23.7% çš„åè§å“åº”ã€‚åœ¨ RewardBench çš„ CHAT-HARD å­é›†æµ‹è¯•ä¸­ï¼ŒPAJAMA çš„è¡¨ç°ä¼˜äº Prometheus å’Œ JudgeLM è¾¾ 2.19% å’Œ 8.67%ï¼Œä¸”æˆæœ¬é™ä½äº†ä¸‰ä¸ªæ•°é‡çº§ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ä»¥ç¨‹åºä¸ºæ ¸å¿ƒçš„è¯„ä¼°æ¨¡å¼èƒ½å¤Ÿä¸ºè‡ªåŠ¨åŒ–æ¨¡å‹è¯„ä»·æä¾›æ›´é«˜æ•ˆã€æ›´å…·é²æ£’æ€§çš„æœªæ¥æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10403v1",
      "published_date": "2025-06-12 06:53:22 UTC",
      "updated_date": "2025-06-12 06:53:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:25:40.412045+00:00"
    },
    {
      "arxiv_id": "2506.10401v2",
      "title": "HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration",
      "title_zh": "HPCTransCompileï¼šé¢å‘é«˜æ€§èƒ½ CUDA è½¬è¯‘ä¸å¤§è¯­è¨€æ¨¡å‹åˆæ­¥æ¢ç´¢çš„ AI ç¼–è¯‘å™¨ç”Ÿæˆæ•°æ®é›†",
      "authors": [
        "Jiaqi Lv",
        "Xufeng He",
        "Yanchen Liu",
        "Xu Dai",
        "Aocheng Shen",
        "Yinghao Li",
        "Jiachen Hao",
        "Jianrong Ding",
        "Yang Hu",
        "Shouyi Yin"
      ],
      "abstract": "The rapid growth of deep learning has driven exponential increases in model parameters and computational demands. NVIDIA GPUs and their CUDA-based software ecosystem provide robust support for parallel computing, significantly alleviating computational bottlenecks. Meanwhile, due to the cultivation of user programming habits and the high performance of GPUs, the CUDA ecosystem has established a dominant position in the field of parallel software. This dominance requires other hardware platforms to support CUDA-based software with performance portability. However, translating CUDA code to other platforms poses significant challenges due to differences in parallel programming paradigms and hardware architectures. Existing approaches rely on language extensions, domain-specific languages (DSLs), or compilers but face limitations in workload coverage and generalizability. Moreover, these methods often incur substantial development costs. Recently, LLMs have demonstrated extraordinary potential in various vertical domains, especially in code-related tasks. However, the performance of existing LLMs in CUDA transpilation, particularly for high-performance code, remains suboptimal. To address these challenges, we propose a novel framework for generating high-performance CUDA and corresponding platform code pairs, leveraging AI compiler and automatic optimization technology. We further enhance the framework with a graph-based data augmentation method and introduce HPCTransEval, a benchmark for evaluating LLM performance on CUDA transpilation. We conduct experiments using CUDA-to-CPU transpilation as a case study on leading LLMs. The speedup ratio of the CPU operators has an average improvemnet of 43.8\\%, highlighting the potential of LLMs to address compatibility challenges within the CUDA ecosystem. Our code is available at https://github.com/PJLAB-CHIP/HPCTransCompile.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ CUDA ç”Ÿæ€ç³»ç»Ÿåœ¨è·¨å¹³å°æ€§èƒ½ç§»æ¤ (Performance Portability) æ–¹é¢é¢ä¸´çš„æ¶æ„å·®å¼‚åŠå¼€å‘æˆæœ¬é«˜ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† HPCTransCompile æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆ AI Compiler ä¸è‡ªåŠ¨ä¼˜åŒ– (Automatic Optimization) æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜æ€§èƒ½çš„ CUDA åŠå…¶å¯¹åº”å¹³å°çš„ä»£ç å¯¹ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†åŸºäºå›¾çš„æ•°æ®å¢å¼ºæ–¹æ³• (Graph-based Data Augmentation)ï¼Œå¹¶å»ºç«‹äº†ä¸“é—¨ç”¨äºè¯„ä¼° LLMs åœ¨ CUDA è½¬è¯‘ (Transpilation) ä»»åŠ¡ä¸­è¡¨ç°çš„åŸºå‡†æµ‹è¯•é›† HPCTransEvalã€‚ä»¥ CUDA-to-CPU è½¬è¯‘ä¸ºæ¡ˆä¾‹çš„å®éªŒæ˜¾ç¤ºï¼Œç”Ÿæˆçš„ CPU ç®—å­ (Operators) åŠ é€Ÿæ¯”å¹³å‡æå‡äº† 43.8%ã€‚è¯¥æˆæœæ˜¾è‘—æå‡äº† LLMs å¤„ç†é«˜æ€§èƒ½ä»£ç è½¬æ¢çš„èƒ½åŠ›ï¼Œä¸ºè§£å†³ CUDA ç”Ÿæ€çš„å…¼å®¹æ€§æŒ‘æˆ˜æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10401v2",
      "published_date": "2025-06-12 06:48:33 UTC",
      "updated_date": "2025-07-04 02:01:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:25:50.684958+00:00"
    },
    {
      "arxiv_id": "2506.11170v2",
      "title": "PromptTSS: A Prompting-Based Approach for Interactive Multi-Granularity Time Series Segmentation",
      "title_zh": "PromptTSSï¼šä¸€ç§åŸºäºæç¤ºçš„äº¤äº’å¼å¤šç²’åº¦æ—¶é—´åºåˆ—åˆ†å‰²æ–¹æ³•",
      "authors": [
        "Ching Chang",
        "Ming-Chih Lo",
        "Wen-Chih Peng",
        "Tien-Fu Chen"
      ],
      "abstract": "Multivariate time series data, collected across various fields such as manufacturing and wearable technology, exhibit states at multiple levels of granularity, from coarse-grained system behaviors to fine-grained, detailed events. Effectively segmenting and integrating states across these different granularities is crucial for tasks like predictive maintenance and performance optimization. However, existing time series segmentation methods face two key challenges: (1) the inability to handle multiple levels of granularity within a unified model, and (2) limited adaptability to new, evolving patterns in dynamic environments. To address these challenges, we propose PromptTSS, a novel framework for time series segmentation with multi-granularity states. PromptTSS uses a unified model with a prompting mechanism that leverages label and boundary information to guide segmentation, capturing both coarse- and fine-grained patterns while adapting dynamically to unseen patterns. Experiments show PromptTSS improves accuracy by 24.49% in multi-granularity segmentation, 17.88% in single-granularity segmentation, and up to 599.24% in transfer learning, demonstrating its adaptability to hierarchical states and evolving time series dynamics. Our code is available at https://github.com/blacksnail789521/PromptTSS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šå˜é‡æ—¶é—´åºåˆ—(Multivariate time series)æ•°æ®ä¸­å­˜åœ¨çš„ç²—ç²’åº¦ç³»ç»Ÿè¡Œä¸ºä¸ç»†ç²’åº¦è¯¦ç»†äº‹ä»¶å…±å­˜çš„ç‰¹æ€§ï¼Œæå‡ºäº†PromptTSSæ¡†æ¶ä»¥è§£å†³ç°æœ‰åˆ†å‰²æ–¹æ³•éš¾ä»¥åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­å¤„ç†å¤šç²’åº¦çŠ¶æ€ä¸”å¯¹æ–°åŠ¨æ€æ¨¡å¼é€‚åº”æ€§æœ‰é™çš„é—®é¢˜ã€‚PromptTSSé‡‡ç”¨äº†ä¸€ç§åŸºäºæç¤º(Prompting)çš„ç»Ÿä¸€æ¨¡å‹ï¼Œé€šè¿‡åˆ©ç”¨æ ‡ç­¾(Label)å’Œè¾¹ç•Œä¿¡æ¯(Boundary information)æ¥å¼•å¯¼åˆ†å‰²è¿‡ç¨‹ï¼Œä»è€ŒåŒæ—¶æ•æ‰ç²—ç²’åº¦å’Œç»†ç²’åº¦çš„æ¨¡å¼ã€‚è¯¥æ¡†æ¶å…·å¤‡åŠ¨æ€é€‚åº”æœªè§æ¨¡å¼çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆä¸åŒå±‚çº§çš„æ—¶é—´åºåˆ—çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPromptTSSåœ¨å¤šç²’åº¦åˆ†å‰²ä»»åŠ¡ä¸­å‡†ç¡®ç‡æå‡äº†24.49%ï¼Œåœ¨å•ç²’åº¦åˆ†å‰²ä¸­æå‡äº†17.88%ã€‚ç‰¹åˆ«æ˜¯åœ¨è¿ç§»å­¦ä¹ (Transfer learning)åœºæ™¯ä¸‹ï¼Œå…¶æ€§èƒ½æå‡é«˜è¾¾599.24%ï¼Œå……åˆ†è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†åˆ†å±‚çŠ¶æ€å’Œæ¼”åŒ–æ—¶é—´åºåˆ—åŠ¨æ€æ–¹é¢çš„å¼ºå¤§é€‚åº”æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 34th ACM International Conference on Information and Knowledge Management (CIKM 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.11170v2",
      "published_date": "2025-06-12 06:45:29 UTC",
      "updated_date": "2025-08-14 05:53:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:25:41.966247+00:00"
    },
    {
      "arxiv_id": "2506.10395v2",
      "title": "Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation",
      "title_zh": "Piscesï¼šé¢å‘å›¾åƒç†è§£ä¸ç”Ÿæˆçš„è‡ªå›å½’åŸºåº§æ¨¡å‹",
      "authors": [
        "Zhiyang Xu",
        "Jiuhai Chen",
        "Zhaojiang Lin",
        "Xichen Pan",
        "Lifu Huang",
        "Tianyi Zhou",
        "Madian Khabsa",
        "Qifan Wang",
        "Di Jin",
        "Michihiro Yasunaga",
        "Lili Yu",
        "Xi Victoria Lin",
        "Shaoliang Nie"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled multimodal foundation models to tackle both image understanding and generation within a unified framework. Despite these gains, unified models often underperform compared to specialized models in either task. A key challenge in developing unified models lies in the inherent differences between the visual features needed for image understanding versus generation, as well as the distinct training processes required for each modality. In this work, we introduce Pisces, an auto-regressive multimodal foundation model that addresses this challenge through a novel decoupled visual encoding architecture and tailored training techniques optimized for multimodal generation. Combined with meticulous data curation, pretraining, and finetuning, Pisces achieves competitive performance in both image understanding and image generation. We evaluate Pisces on over 20 public benchmarks for image understanding, where it demonstrates strong performance across a wide range of tasks. Additionally, on GenEval, a widely adopted benchmark for image generation, Pisces exhibits robust generative capabilities. Our extensive analysis reveals the synergistic relationship between image understanding and generation, and the benefits of using separate visual encoders, advancing the field of unified multimodal models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Piscesï¼Œä¸€ç§æ—¨åœ¨ç»Ÿä¸€å›¾åƒç†è§£(image understanding)ä¸å›¾åƒç”Ÿæˆ(image generation)çš„è‡ªå›å½’å¤šæ¨¡æ€å¤§æ¨¡å‹(auto-regressive multimodal foundation model)ã€‚é’ˆå¯¹ç†è§£ä¸ç”Ÿæˆä»»åŠ¡å¯¹è§†è§‰ç‰¹å¾éœ€æ±‚å­˜åœ¨æœ¬è´¨å·®å¼‚çš„éš¾é¢˜ï¼ŒPisces å¼•å…¥äº†åˆ›æ–°çš„è§£è€¦è§†è§‰ç¼–ç æ¶æ„(decoupled visual encoding architecture)åŠé’ˆå¯¹å¤šæ¨¡æ€ç”Ÿæˆä¼˜åŒ–çš„è®­ç»ƒæŠ€æœ¯ã€‚é€šè¿‡ç²¾å¯†çš„æ•°æ®ç­›é€‰ã€é¢„è®­ç»ƒ(pretraining)å’Œå¾®è°ƒ(finetuning)ï¼Œè¯¥æ¨¡å‹åœ¨è¶…è¿‡ 20 ä¸ªå›¾åƒç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œå¹¶åœ¨ GenEval å›¾åƒç”Ÿæˆè¯„ä¼°ä¸­å±•ç°äº†ç¨³å¥çš„ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒåˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†å›¾åƒç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„ååŒå…³ç³»ï¼ŒéªŒè¯äº†ä½¿ç”¨ç‹¬ç«‹è§†è§‰ç¼–ç å™¨åœ¨æå‡ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Unified image understanding and generation model",
      "pdf_url": "https://arxiv.org/pdf/2506.10395v2",
      "published_date": "2025-06-12 06:37:34 UTC",
      "updated_date": "2025-07-12 20:42:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:26:15.798877+00:00"
    },
    {
      "arxiv_id": "2506.10387v1",
      "title": "Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills",
      "title_zh": "Mirage-1ï¼šé€šè¿‡å±‚çº§åŒ–å¤šæ¨¡æ€æŠ€èƒ½å¢å¼ºä¸æ›´æ–° GUI æ™ºèƒ½ä½“",
      "authors": [
        "Yuquan Xie",
        "Zaijing Li",
        "Rui Shao",
        "Gongwei Chen",
        "Kaiwen Zhou",
        "Yinchuan Li",
        "Dongmei Jiang",
        "Liqiang Nie"
      ],
      "abstract": "Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI agents have yielded promising outcomes. However, these agents still struggle with long-horizon tasks in online environments, primarily due to insufficient knowledge and the inherent gap between offline and online domains. In this paper, inspired by how humans generalize knowledge in open-ended environments, we propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of insufficient knowledge. It progressively abstracts trajectories into execution skills, core skills, and ultimately meta-skills, providing a hierarchical knowledge structure for long-horizon task planning. To bridge the domain gap, we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm, which efficiently leverages skills acquired in offline environments to reduce the action search space during online tree exploration. Building on HMS, we propose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To validate the performance of Mirage-1 in real-world long-horizon scenarios, we constructed a new benchmark, AndroidLH. Experimental results show that Mirage-1 outperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld, MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page: https://cybertronagent.github.io/Mirage-1.github.io/",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºGUI Agentåœ¨å¤„ç†é•¿æ—¶ç¨‹ä»»åŠ¡æ—¶é¢ä¸´çš„çŸ¥è¯†åŒ®ä¹åŠé¢†åŸŸå·®å¼‚æŒ‘æˆ˜ï¼Œæå‡ºäº†Mirage-1ã€‚ç ”ç©¶æ ¸å¿ƒå¼•å…¥äº†å±‚çº§åŒ–å¤šæ¨¡æ€æŠ€èƒ½ï¼ˆHierarchical Multimodal Skills, HMSï¼‰æ¨¡å—ï¼Œå°†æ“ä½œè½¨è¿¹é€æ­¥æŠ½è±¡ä¸ºexecution skillsã€core skillså’Œmeta-skillsï¼Œä¸ºé•¿æ—¶ç¨‹è§„åˆ’æä¾›å±‚çº§åŒ–çŸ¥è¯†ç»“æ„ã€‚ä¸ºå¼¥åˆç¦»çº¿ä¸åœ¨çº¿ç¯å¢ƒçš„é¸¿æ²Ÿï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†Skill-Augmented Monte Carlo Tree Search (SA-MCTS) ç®—æ³•ï¼Œé€šè¿‡æœ‰æ•ˆåˆ©ç”¨ç¦»çº¿æŠ€èƒ½ç¼©å‡åœ¨çº¿æ ‘æœç´¢çš„åŠ¨ä½œç©ºé—´ã€‚Mirage-1è¢«è®¾è®¡ä¸ºä¸€ç§è·¨å¹³å°ã€å³æ’å³ç”¨çš„å¤šæ¨¡æ€GUI Agentï¼Œå…·æœ‰é«˜åº¦çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒä¸­ï¼Œç ”ç©¶è€…æ„å»ºäº†æ–°åŸºå‡†AndroidLHä»¥éªŒè¯å…¶å®é™…æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼ŒMirage-1åœ¨AndroidWorldã€MobileMiniWob++ã€Mind2Web-LiveåŠAndroidLHä¸Šçš„è¡¨ç°å‡å¤§å¹…è¶…è¶Šç°æœ‰Agentï¼Œæœ€é«˜æå‡è¾¾79%ï¼Œæœ‰æ•ˆè¯æ˜äº†è¯¥å±‚çº§åŒ–æŠ€èƒ½æ¡†æ¶åœ¨å¤æ‚äº¤äº’ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 5 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.10387v1",
      "published_date": "2025-06-12 06:21:19 UTC",
      "updated_date": "2025-06-12 06:21:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:26:02.374550+00:00"
    },
    {
      "arxiv_id": "2506.10384v1",
      "title": "NeuroPAL: Punctuated Anytime Learning with Neuroevolution for Macromanagement in Starcraft: Brood War",
      "title_zh": "NeuroPALï¼šåŸºäºç¥ç»æ¼”åŒ–ä¸é—´æ–­å¼éšæ—¶å­¦ä¹ çš„ Starcraft: Brood War å®è§‚ç®¡ç†",
      "authors": [
        "Jim O'Connor",
        "Yeonghun Lee",
        "Gary B Parker"
      ],
      "abstract": "StarCraft: Brood War remains a challenging benchmark for artificial intelligence research, particularly in the domain of macromanagement, where long-term strategic planning is required. Traditional approaches to StarCraft AI rely on rule-based systems or supervised deep learning, both of which face limitations in adaptability and computational efficiency. In this work, we introduce NeuroPAL, a neuroevolutionary framework that integrates Neuroevolution of Augmenting Topologies (NEAT) with Punctuated Anytime Learning (PAL) to improve the efficiency of evolutionary training. By alternating between frequent, low-fidelity training and periodic, high-fidelity evaluations, PAL enhances the sample efficiency of NEAT, enabling agents to discover effective strategies in fewer training iterations. We evaluate NeuroPAL in a fixed-map, single-race scenario in StarCraft: Brood War and compare its performance to standard NEAT-based training. Our results show that PAL significantly accelerates the learning process, allowing the agent to reach competitive levels of play in approximately half the training time required by NEAT alone. Additionally, the evolved agents exhibit emergent behaviors such as proxy barracks placement and defensive building optimization, strategies commonly used by expert human players. These findings suggest that structured evaluation mechanisms like PAL can enhance the scalability and effectiveness of neuroevolution in complex real-time strategy environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ã€Šæ˜Ÿé™…äº‰éœ¸ï¼šæ¯å·¢ä¹‹æˆ˜ã€‹(StarCraft: Brood War)ä¸­å¤æ‚çš„å®è§‚ç®¡ç†(Macromanagement)æŒ‘æˆ˜ï¼Œæå‡ºäº†NeuroPALæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¢å¼ºæ‹“æ‰‘ç¥ç»æ¼”åŒ–ç®—æ³•(NEAT)ä¸é—´æ–­å¼éšæ—¶å­¦ä¹ (Punctuated Anytime Learning, PAL)ç›¸ç»“åˆã€‚NeuroPALé€šè¿‡åœ¨é¢‘ç¹çš„ä½ä¿çœŸè®­ç»ƒä¸å‘¨æœŸæ€§çš„é«˜ä¿çœŸè¯„ä¼°ä¹‹é—´äº¤æ›¿ï¼Œæ˜¾è‘—æå‡äº†æ¼”åŒ–è®­ç»ƒçš„æ ·æœ¬æ•ˆç‡ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿä»¥æ›´å°‘çš„è¿­ä»£æ¬¡æ•°å‘ç°æœ‰æ•ˆç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPALæ˜¾è‘—åŠ é€Ÿäº†å­¦ä¹ è¿‡ç¨‹ï¼Œä½¿æ™ºèƒ½ä½“åœ¨ä»…éœ€æ ‡å‡†NEATä¸€åŠçš„è®­ç»ƒæ—¶é—´å†…å³å¯è¾¾åˆ°ç«æŠ€æ°´å¹³ã€‚æ­¤å¤–ï¼Œæ¼”åŒ–å‡ºçš„æ™ºèƒ½ä½“è¡¨ç°å‡ºäº†è¯¸å¦‚å…µè¥å‰ç½®(proxy barracks placement)å’Œé˜²å¾¡å»ºç­‘ä¼˜åŒ–ç­‰äººç±»ä¸“å®¶çº§æˆ˜ç•¥è¡Œä¸ºã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒPALè¿™ç§ç»“æ„åŒ–è¯„ä¼°æœºåˆ¶èƒ½æœ‰æ•ˆå¢å¼ºç¥ç»æ¼”åŒ–åœ¨å¤æ‚å®æ—¶æˆ˜ç•¥(RTS)ç¯å¢ƒä¸­çš„å¯æ‰©å±•æ€§å’Œæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "IEEE Conference on Games 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10384v1",
      "published_date": "2025-06-12 06:19:27 UTC",
      "updated_date": "2025-06-12 06:19:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:26:06.044644+00:00"
    },
    {
      "arxiv_id": "2506.10378v1",
      "title": "Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning",
      "title_zh": "é€šè¿‡å› æœè¡¨ç¤ºå­¦ä¹ æŒ–æ˜è¯­è¨€æ¨¡å‹çš„å±‚çº§åŒ–æ½œåœ¨èƒ½åŠ›",
      "authors": [
        "Jikai Jin",
        "Vasilis Syrgkanis",
        "Sham Kakade",
        "Hanlin Zhang"
      ],
      "abstract": "Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose a causal representation learning framework wherein observed benchmark performance is modeled as a linear transformation of a few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as a common confounder. Applying this approach to a comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify a concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal a clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, a step critical to accurately uncovering the underlying causal relationships among latent model capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå› æœè¡¨ç¤ºå­¦ä¹  (Causal Representation Learning) çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†è§‚æµ‹åˆ°çš„åŸºå‡†æµ‹è¯•æ€§èƒ½å»ºæ¨¡ä¸ºå°‘é‡æ½œåœ¨èƒ½åŠ›å› å­çš„çº¿æ€§å˜æ¢ï¼Œå®ç°å¯¹è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„å¿ å®è¯„ä¼°ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†åŸºç¡€æ¨¡å‹ (base model) ä½œä¸ºå…±åŒæ··æ‚å› ç´  (common confounder) è¿›è¡Œæ§åˆ¶ï¼ŒæˆåŠŸè¯†åˆ«å‡ºæ½œåœ¨å› å­ä¹‹é—´çš„å› æœå…³è”ã€‚ç ”ç©¶äººå‘˜å°†è¯¥æ–¹æ³•åº”ç”¨äºåŒ…å«1500å¤šä¸ªæ¨¡å‹ã€æ¶µç›– Open LLM Leaderboard å…­ä¸ªåŸºå‡†æµ‹è¯•çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¯†åˆ«å‡ºä¸€ä¸ªç®€æ´çš„ä¸‰èŠ‚ç‚¹çº¿æ€§å› æœç»“æ„ï¼Œèƒ½å¤Ÿå¯é åœ°è§£é‡Šè§‚æµ‹åˆ°çš„æ€§èƒ½å˜åŒ–ã€‚è¯¥å› æœç»“æ„æ­ç¤ºäº†æ˜ç¡®çš„èƒ½åŠ›æ¼”è¿›è·¯å¾„ï¼šä»é€šç”¨é—®é¢˜è§£å†³èƒ½åŠ› (general problem-solving capabilities) å¼€å§‹ï¼Œç»è¿‡æŒ‡ä»¤éµå¾ªç†Ÿç»ƒåº¦ (instruction-following proficiency)ï¼Œæœ€ç»ˆæ¼”è¿›åˆ°æ•°å­¦æ¨ç†èƒ½åŠ› (mathematical reasoning ability)ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ä¸¥æ ¼æ§åˆ¶åŸºç¡€æ¨¡å‹å˜ä½“çš„é‡è¦æ€§ï¼Œè¿™æ˜¯å‡†ç¡®æ­ç¤ºæ¨¡å‹æ½œåœ¨èƒ½åŠ›ä¹‹é—´åº•å±‚å› æœå…³ç³»çš„å…³é”®æ­¥éª¤ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10378v1",
      "published_date": "2025-06-12 06:07:42 UTC",
      "updated_date": "2025-06-12 06:07:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:26:28.840966+00:00"
    },
    {
      "arxiv_id": "2506.10357v1",
      "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts",
      "title_zh": "Optimus-3ï¼šè¿ˆå‘å…·å¤‡å¯æ‰©å±•ä»»åŠ¡ä¸“å®¶çš„é€šç”¨å¤šæ¨¡æ€ Minecraft æ™ºèƒ½ä½“",
      "authors": [
        "Zaijing Li",
        "Yuquan Xie",
        "Rui Shao",
        "Gongwei Chen",
        "Weili Guan",
        "Dongmei Jiang",
        "Liqiang Nie"
      ],
      "abstract": "Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building a generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing. 3) We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agent's reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, a general-purpose agent for Minecraft. Extensive experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a wide range of tasks in the Minecraft environment. Project page: https://cybertronagent.github.io/Optimus-3.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Optimus-3ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹Minecraftç¯å¢ƒçš„é€šç”¨å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³å¼€æ”¾ä¸–ç•Œä¸­æ„ŸçŸ¥ã€è§„åˆ’ã€åŠ¨ä½œå’Œåæ€ç­‰å¤æ‚æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœç‰¹å®šé¢†åŸŸæ•°æ®ä¸è¶³ã€å¼‚æ„ä»»åŠ¡å¹²æ‰°ä»¥åŠç¯å¢ƒè§†è§‰å¤šæ ·æ€§é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿé¦–å…ˆè®¾è®¡äº†çŸ¥è¯†å¢å¼ºçš„æ•°æ®ç”Ÿæˆæµæ°´çº¿(knowledge-enhanced data generation pipeline)ä»¥æä¾›å¤§è§„æ¨¡é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚å…¶æ¬¡ï¼Œé¡¹ç›®å¼•å…¥äº†å…·å¤‡ä»»åŠ¡çº§è·¯ç”±(task-level routing)çš„æ··åˆä¸“å®¶æ¶æ„(Mixture-of-Experts, MoE)ä»¥å‡å°‘ä¸åŒä»»åŠ¡é—´çš„ç›¸äº’å¹²æ‰°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†å¤šæ¨¡æ€æ¨ç†å¢å¼ºå¼ºåŒ–å­¦ä¹ (Multimodal Reasoning-Augmented Reinforcement Learning)æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“åœ¨å¤æ‚è§†è§‰ç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOptimus-3åœ¨Minecraftçš„å„é¡¹ä»»åŠ¡ä¸­è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)å’Œæœ€å…ˆè¿›çš„ä¸“ç”¨æ™ºèƒ½ä½“ï¼Œä¸ºæ„å»ºé€šç”¨äººå·¥æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.10357v1",
      "published_date": "2025-06-12 05:29:40 UTC",
      "updated_date": "2025-06-12 05:29:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:26:34.289385+00:00"
    },
    {
      "arxiv_id": "2506.10351v4",
      "title": "PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal Representation",
      "title_zh": "PhysioWaveï¼šç”¨äºç”Ÿç†ä¿¡å·è¡¨å¾çš„å¤šå°ºåº¦å°æ³¢ Transformer",
      "authors": [
        "Yanlong Chen",
        "Mattia Orlandi",
        "Pierangelo Maria Rapa",
        "Simone Benatti",
        "Luca Benini",
        "Yawei Li"
      ],
      "abstract": "Physiological signals are often corrupted by motion artifacts, baseline drift, and other low-SNR disturbances, which pose significant challenges for analysis. Additionally, these signals exhibit strong non-stationarity, with sharp peaks and abrupt changes that evolve continuously, making them difficult to represent using traditional time-domain or filtering methods. To address these issues, a novel wavelet-based approach for physiological signal analysis is presented, aiming to capture multi-scale time-frequency features in various physiological signals. Leveraging this technique, two large-scale pretrained models specific to EMG and ECG are introduced for the first time, achieving superior performance and setting new baselines in downstream tasks. Additionally, a unified multi-modal framework is constructed by integrating pretrained EEG model, where each modality is guided through its dedicated branch and fused via learnable weighted fusion. This design effectively addresses challenges such as low signal-to-noise ratio, high inter-subject variability, and device mismatch, outperforming existing methods on multi-modal tasks. The proposed wavelet-based architecture lays a solid foundation for analysis of diverse physiological signals, while the multi-modal design points to next-generation physiological signal processing with potential impact on wearable health monitoring, clinical diagnostics, and broader biomedical applications. Code and data are available at: github.com/ForeverBlue816/PhysioWave",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PhysioWaveï¼Œä¸€ç§å¤šå°ºåº¦å°æ³¢ Transformer (Multi-Scale Wavelet-Transformer) æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç”Ÿç†ä¿¡å·ä¸­å¸¸è§çš„è¿åŠ¨ä¼ªå½±ã€åŸºçº¿æ¼‚ç§»åŠå¼ºéå¹³ç¨³æ€§ç­‰åˆ†ææŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•è·å¤šå°ºåº¦æ—¶é¢‘ç‰¹å¾ (multi-scale time-frequency features)ï¼Œæœ‰æ•ˆæå‡äº†ä¿¡å·è¡¨å¾èƒ½åŠ›ã€‚ç ”ç©¶é¦–æ¬¡æ¨å‡ºäº†é’ˆå¯¹ EMG å’Œ ECG çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ (large-scale pretrained models)ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­åˆ·æ–°äº†æ€§èƒ½åŸºå‡†ã€‚æ­¤å¤–ï¼ŒPhysioWave æ•´åˆäº†é¢„è®­ç»ƒçš„ EEG æ¨¡å‹ï¼Œé€šè¿‡å¯å­¦ä¹ çš„åŠ æƒèåˆ (learnable weighted fusion) æ„å»ºäº†ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿåœ¨ä½ä¿¡å™ªæ¯” (SNR)ã€é«˜å—è¯•è€…å·®å¼‚å’Œè®¾å¤‡ä¸åŒ¹é…åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¶æ„åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå¯ç©¿æˆ´å¥åº·ç›‘æµ‹å’Œä¸´åºŠè¯Šæ–­ç­‰ç”Ÿç‰©åŒ»å­¦åº”ç”¨æä¾›äº†å…¨æ–°çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "43 pages, 17 figures, 17 tables. Accepted by NeurIPS 2025. Code and data are available at: github.com/ForeverBlue816/PhysioWave",
      "pdf_url": "https://arxiv.org/pdf/2506.10351v4",
      "published_date": "2025-06-12 05:11:41 UTC",
      "updated_date": "2025-10-20 15:00:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:26:35.361578+00:00"
    },
    {
      "arxiv_id": "2506.10343v2",
      "title": "Code Execution as Grounded Supervision for LLM Reasoning",
      "title_zh": "ä»£ç æ‰§è¡Œä½œä¸ºå¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„å®è¯ç›‘ç£",
      "authors": [
        "Dongwon Jung",
        "Wenxuan Zhou",
        "Muhao Chen"
      ],
      "abstract": "Training large language models (LLMs) with chain-of-thought (CoT) supervision has proven effective for enhancing their reasoning abilities. However, obtaining reliable and accurate reasoning supervision remains a significant challenge. We propose a scalable method for generating a high-quality CoT supervision dataset by leveraging the determinism of program execution. Unlike existing reasoning dataset generation methods that rely on costly human annotations or error-prone LLM-generated CoT, our approach extracts verifiable, step-by-step reasoning traces from code execution and transforms them into a natural language CoT reasoning. Experiments on reasoning benchmarks across various domains show that our method effectively equips LLMs with transferable reasoning abilities across diverse tasks. Furthermore, the ablation studies validate that our method produces highly accurate reasoning data and reduces overall token length during inference by reducing meaningless repetition and overthinking.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºCode Execution as Grounded Supervisionçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨ç¨‹åºæ‰§è¡Œçš„ç¡®å®šæ€§æ¥ç”Ÿæˆé«˜è´¨é‡çš„chain-of-thought (CoT)ç›‘ç£æ•°æ®é›†ã€‚é’ˆå¯¹ç°æœ‰æ¨ç†æ•°æ®é›†ç”Ÿæˆæ–¹æ³•ä¸­å­˜åœ¨çš„äººå·¥æ ‡æ³¨æˆæœ¬é«˜æˆ–å¤§è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆçš„CoTæ˜“å‡ºé”™ç­‰æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•ä»ä»£ç æ‰§è¡Œä¸­æå–å¯éªŒè¯çš„é€æ­¥æ¨ç†è½¨è¿¹ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€å½¢å¼çš„CoTæ¨ç†è¿‡ç¨‹ã€‚åœ¨å¤šä¸ªé¢†åŸŸçš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆèµ‹äºˆLLMåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­å…·å¤‡å¯è¿ç§»çš„æ¨ç†èƒ½åŠ›ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•èƒ½äº§ç”Ÿé«˜å‡†ç¡®åº¦çš„æ¨ç†æ•°æ®ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶çš„æ— æ„ä¹‰é‡å¤å’Œè¿‡åº¦æ€è€ƒ(overthinking)ç°è±¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–æ¨ç†è·¯å¾„ï¼ŒæˆåŠŸç¼©çŸ­äº†æ¨¡å‹æ¨ç†æ—¶çš„æ€»tokené•¿åº¦ï¼Œä¸ºæå‡LLMçš„æ¨ç†æ•ˆç‡å’Œå‡†ç¡®æ€§æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10343v2",
      "published_date": "2025-06-12 04:36:57 UTC",
      "updated_date": "2025-10-17 20:50:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:26:49.115414+00:00"
    },
    {
      "arxiv_id": "2506.10342v2",
      "title": "UrbanSense:A Framework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models",
      "title_zh": "UrbanSenseï¼šåŸºäºè§†è§‰å¤§è¯­è¨€æ¨¡å‹çš„åŸå¸‚è¡—æ™¯å®šé‡åˆ†ææ¡†æ¶",
      "authors": [
        "Jun Yin",
        "Jing Zhong",
        "Peilin Li",
        "Ruolin Pan",
        "Pengyu Zeng",
        "Miao Zhang",
        "Shuai Lu"
      ],
      "abstract": "Urban cultures and architectural styles vary significantly across cities due to geographical, chronological, historical, and socio-political factors. Understanding these differences is essential for anticipating how cities may evolve in the future. As representative cases of historical continuity and modern innovation in China, Beijing and Shenzhen offer valuable perspectives for exploring the transformation of urban streetscapes. However, conventional approaches to urban cultural studies often rely on expert interpretation and historical documentation, which are difficult to standardize across different contexts. To address this, we propose a multimodal research framework based on vision-language models, enabling automated and scalable analysis of urban streetscape style differences. This approach enhances the objectivity and data-driven nature of urban form research. The contributions of this study are as follows: First, we construct UrbanDiffBench, a curated dataset of urban streetscapes containing architectural images from different periods and regions. Second, we develop UrbanSense, the first vision-language-model-based framework for urban streetscape analysis, enabling the quantitative generation and comparison of urban style representations. Third, experimental results show that Over 80% of generated descriptions pass the t-test (p less than 0.05). High Phi scores (0.912 for cities, 0.833 for periods) from subjective evaluations confirm the method's ability to capture subtle stylistic differences. These results highlight the method's potential to quantify and interpret urban style evolution, offering a scientifically grounded lens for future design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UrbanSenseï¼Œä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) çš„å¤šæ¨¡æ€ç ”ç©¶æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°åŸå¸‚è¡—æ™¯é£æ ¼å·®å¼‚çš„è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•é‡åŒ–åˆ†æã€‚ä¸ºè§£å†³ä¼ ç»ŸåŸå¸‚å½¢æ€ç ”ç©¶ä¸­ä¸“å®¶è§£è¯»ä¸»è§‚æ€§å¼ºä¸”éš¾ä»¥æ ‡å‡†åŒ–çš„å±€é™ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ„å»ºåŒ…å«å¤šæ—¶æœŸã€å¤šåœ°åŒºå»ºç­‘å›¾åƒçš„ UrbanDiffBench æ•°æ®é›†ï¼Œå®ç°äº†åŸå¸‚é£æ ¼è¡¨å¾çš„å®šé‡ç”Ÿæˆä¸å¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¶…è¿‡ 80% çš„æ¨¡å‹ç”Ÿæˆæè¿°é€šè¿‡äº† t-test æ˜¾è‘—æ€§æ£€éªŒï¼Œä¸”åœ¨ä¸»è§‚è¯„ä»·çš„ Phi scores ä¸­è¡¨ç°ä¼˜å¼‚ï¼ˆåŸå¸‚ç»´åº¦ 0.912ï¼Œæ—¶æœŸç»´åº¦ 0.833ï¼‰ï¼Œå……åˆ†éªŒè¯äº†å…¶æ•æ‰ç»†å¾®é£æ ¼ç‰¹å¾çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶é€šè¿‡ UrbanSense æ¡†æ¶è¯æ˜äº†åˆ©ç”¨å¤§æ¨¡å‹é‡åŒ–å’Œè§£é‡ŠåŸå¸‚é£æ ¼æ¼”å˜çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæœªæ¥åŸå¸‚è®¾è®¡æä¾›äº†ç§‘å­¦é©±åŠ¨çš„ç ”ç©¶è§†è§’ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10342v2",
      "published_date": "2025-06-12 04:35:39 UTC",
      "updated_date": "2025-08-04 15:56:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:26:44.778505+00:00"
    },
    {
      "arxiv_id": "2506.21562v2",
      "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction",
      "title_zh": "FloorPlan-DeepSeek (FPDS)ï¼šåŸºäºå‘é‡åŒ–â€œä¸‹ä¸€æˆ¿é—´é¢„æµ‹â€çš„å¤šæ¨¡æ€å¹³é¢å›¾ç”Ÿæˆæ–¹æ³•",
      "authors": [
        "Jun Yin",
        "Pengyu Zeng",
        "Jing Zhong",
        "Peilin Li",
        "Miao Zhang",
        "Ran Luo",
        "Shuai Lu"
      ],
      "abstract": "In the architectural design process, floor plan generation is inherently progressive and iterative. However, existing generative models for floor plans are predominantly end-to-end generation that produce an entire pixel-based layout in a single pass. This paradigm is often incompatible with the incremental workflows observed in real-world architectural practice. To address this issue, we draw inspiration from the autoregressive 'next token prediction' mechanism commonly used in large language models, and propose a novel 'next room prediction' paradigm tailored to architectural floor plan modeling. Experimental evaluation indicates that FPDS demonstrates competitive performance in comparison to diffusion models and Tell2Design in the text-to-floorplan task, indicating its potential applicability in supporting future intelligent architectural design.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æˆ·å‹å›¾ç”Ÿæˆæ¨¡å‹å¤šä¸ºç«¯åˆ°ç«¯(end-to-end)åƒç´ ç”Ÿæˆä¸”ä¸ç¬¦åˆå®é™…å»ºç­‘è®¾è®¡å¢é‡æµç¨‹çš„é—®é¢˜ï¼Œæå‡ºäº†FloorPlan-DeepSeek (FPDS)è¿™ä¸€å¤šæ¨¡æ€æˆ·å‹å›¾ç”Ÿæˆæ–¹æ³•ã€‚FPDSå€Ÿé‰´äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)ä¸­å¸¸è§çš„è‡ªå›å½’â€œä¸‹ä¸€ä¸ªTokené¢„æµ‹(next token prediction)â€æœºåˆ¶ï¼Œåˆ›æ–°æ€§åœ°æ„å»ºäº†é€‚ç”¨äºå»ºç­‘å¹³é¢å›¾å»ºæ¨¡çš„â€œä¸‹ä¸€é—´æˆ¿é¢„æµ‹(next room prediction)â€èŒƒå¼ã€‚å®éªŒè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒFPDSåœ¨æ–‡æœ¬åˆ°æˆ·å‹å›¾(text-to-floorplan)ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸æ‰©æ•£æ¨¡å‹(diffusion models)åŠTell2Designç›¸æ¯”å…·æœ‰æå¼ºçš„ç«äº‰åŠ›ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå¼¥è¡¥äº†ç”Ÿæˆæ¨¡å‹ä¸å»ºç­‘å®åŠ¡å·¥ä½œæµä¹‹é—´çš„é¸¿æ²Ÿï¼Œè¯æ˜äº†è‡ªå›å½’æ¶æ„åœ¨æ”¯æŒæœªæ¥æ™ºèƒ½å»ºç­‘è®¾è®¡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21562v2",
      "published_date": "2025-06-12 04:33:27 UTC",
      "updated_date": "2025-08-02 18:27:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:26:48.976183+00:00"
    },
    {
      "arxiv_id": "2506.10334v1",
      "title": "Using Vision Language Models to Detect Students' Academic Emotion through Facial Expressions",
      "title_zh": "åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹é€šè¿‡é¢éƒ¨è¡¨æƒ…è¯†åˆ«å­¦ç”Ÿå­¦ä¸šæƒ…æ„Ÿ",
      "authors": [
        "Deliang Wang",
        "Chao Yang",
        "Gaowei Chen"
      ],
      "abstract": "Students' academic emotions significantly influence their social behavior and learning performance. Traditional approaches to automatically and accurately analyze these emotions have predominantly relied on supervised machine learning algorithms. However, these models often struggle to generalize across different contexts, necessitating repeated cycles of data collection, annotation, and training. The emergence of Vision-Language Models (VLMs) offers a promising alternative, enabling generalization across visual recognition tasks through zero-shot prompting without requiring fine-tuning. This study investigates the potential of VLMs to analyze students' academic emotions via facial expressions in an online learning environment. We employed two VLMs, Llama-3.2-11B-Vision-Instruct and Qwen2.5-VL-7B-Instruct, to analyze 5,000 images depicting confused, distracted, happy, neutral, and tired expressions using zero-shot prompting. Preliminary results indicate that both models demonstrate moderate performance in academic facial expression recognition, with Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct. Notably, both models excel in identifying students' happy emotions but fail to detect distracted behavior. Additionally, Qwen2.5-VL-7B-Instruct exhibits relatively high performance in recognizing students' confused expressions, highlighting its potential for practical applications in identifying content that causes student confusion.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) é€šè¿‡é¢éƒ¨è¡¨æƒ…åˆ†æåœ¨çº¿å­¦ä¹ ç¯å¢ƒä¸‹å­¦ç”Ÿå­¦ä¸šæƒ…ç»ª (Academic Emotion) çš„æ½œåŠ›ã€‚é’ˆå¯¹ä¼ ç»Ÿç›‘ç£å­¦ä¹ ç®—æ³•åœ¨è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œç ”ç©¶è€…é‡‡ç”¨äº†é›¶æ ·æœ¬æç¤º (Zero-shot Prompting) æŠ€æœ¯ï¼Œå¯¹ Llama-3.2-11B-Vision-Instruct å’Œ Qwen2.5-VL-7B-Instruct ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒåˆ†æäº† 5,000 å¼ æ¶µç›–å›°æƒ‘ (Confused)ã€åˆ†å¿ƒ (Distracted)ã€å¿«ä¹ (Happy)ã€ä¸­æ€§ (Neutral) å’Œç–²åŠ³ (Tired) ç­‰è¡¨æƒ…çš„å›¾åƒã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œä¸¤ä¸ªæ¨¡å‹åœ¨å­¦ä¸šé¢éƒ¨è¡¨æƒ…è¯†åˆ«ä¸­å‡è¡¨ç°å‡ºä¸­ç­‰æ°´å¹³ï¼Œå…¶ä¸­ Qwen2.5-VL-7B-Instruct çš„æ€§èƒ½ä¼˜äº Llama-3.2-11B-Vision-Instructã€‚ç ”ç©¶å‘ç°æ¨¡å‹åœ¨è¯†åˆ«å¿«ä¹æƒ…ç»ªæ–¹é¢éå¸¸å‡ºè‰²ï¼Œä½†åœ¨æ£€æµ‹åˆ†å¿ƒè¡Œä¸ºæ–¹é¢è¡¨ç°è¾ƒå·®ã€‚æ­¤å¤–ï¼ŒQwen2.5-VL-7B-Instruct åœ¨è¯†åˆ«å›°æƒ‘è¡¨æƒ…æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜æ°´å‡†ï¼Œè¯æ˜äº†å…¶åœ¨è¯†åˆ«ä»¤å­¦ç”Ÿè´¹è§£çš„æ•™å­¦å†…å®¹ç­‰å®é™…æ•™è‚²åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10334v1",
      "published_date": "2025-06-12 04:01:26 UTC",
      "updated_date": "2025-06-12 04:01:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:26:50.170054+00:00"
    },
    {
      "arxiv_id": "2506.22446v1",
      "title": "EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis",
      "title_zh": "EAGLEï¼šé¢å‘å¤šæ¨¡æ€ç”Ÿå­˜é¢„æµ‹çš„é€šç”¨æ½œåµŒå…¥é«˜æ•ˆå¯¹é½ä¸å¯è§£é‡Šæ€§å½’å› åˆ†æ",
      "authors": [
        "Aakash Tripathi",
        "Asim Waqas",
        "Matthew B. Schabath",
        "Yasin Yilmaz",
        "Ghulam Rasool"
      ],
      "abstract": "Accurate cancer survival prediction requires integration of diverse data modalities that reflect the complex interplay between imaging, clinical parameters, and textual reports. However, existing multimodal approaches suffer from simplistic fusion strategies, massive computational requirements, and lack of interpretability-critical barriers to clinical adoption. We present EAGLE (Efficient Alignment of Generalized Latent Embeddings), a novel deep learning framework that addresses these limitations through attention-based multimodal fusion with comprehensive attribution analysis. EAGLE introduces four key innovations: (1) dynamic cross-modal attention mechanisms that learn hierarchical relationships between modalities, (2) massive dimensionality reduction (99.96%) while maintaining predictive performance, (3) three complementary attribution methods providing patient-level interpretability, and (4) a unified pipeline enabling seamless adaptation across cancer types. We evaluated EAGLE on 911 patients across three distinct malignancies: glioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN, n=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis showed high-risk individuals relied more heavily on adverse imaging features, while low-risk patients demonstrated balanced modality contributions. Risk stratification identified clinically meaningful groups with 4-fold (GBM) to 5-fold (NSCLC) differences in median survival, directly informing treatment intensity decisions. By combining state-of-the-art performance with clinical interpretability, EAGLE bridges the gap between advanced AI capabilities and practical healthcare deployment, offering a scalable solution for multimodal survival prediction that enhances both prognostic accuracy and physician trust in automated predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EAGLE (Efficient Alignment of Generalized Latent Embeddings) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€ç™Œç—‡ç”Ÿå­˜é¢„æµ‹ä¸­èåˆç­–ç•¥å•ä¸€ã€è®¡ç®—èµ„æºæ¶ˆè€—å·¨å¤§ä»¥åŠç¼ºä¹ä¸´åºŠè§£é‡Šæ€§ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚EAGLE å¼•å…¥äº†åŠ¨æ€è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ (dynamic cross-modal attention mechanisms) ä»¥æ•æ‰æ¨¡æ€é—´çš„å±‚æ¬¡å…³ç³»ï¼Œå¹¶åœ¨ä¿æŒé¢„æµ‹æ€§èƒ½çš„åŒæ—¶å®ç°äº† 99.96% çš„ç»´åº¦å¤§å¹…ç¼©å‡ (dimensionality reduction)ã€‚é€šè¿‡é›†æˆçš„ä¸‰ç§å½’å› åˆ†ææ–¹æ³• (attribution methods)ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæä¾›æ‚£è€…å±‚é¢çš„å¯è§£é‡Šæ€§ï¼Œæ­ç¤ºäº†é«˜é£é™©ä¸ªä½“æ›´ä¾èµ–ä¸åˆ©å½±åƒç‰¹å¾çš„æ¨¡å¼ã€‚åœ¨æ¶‰åŠèƒ¶è´¨æ¯ç»†èƒç˜¤ (GBM)ã€èƒ°è…ºå¯¼ç®¡å†…ä¹³å¤´çŠ¶é»æ¶²æ€§è‚¿ç˜¤ (IPMN) å’Œéå°ç»†èƒè‚ºç™Œ (NSCLC) çš„ 911 åæ‚£è€…æ•°æ®é›†ä¸Šï¼ŒEAGLE æˆåŠŸåˆ’åˆ†å‡ºä¸­ä½ç”Ÿå­˜æœŸå·®å¼‚è¾¾ 4 è‡³ 5 å€çš„é£é™©ç»„åˆ«ã€‚è¯¥ç ”ç©¶é€šè¿‡ç»Ÿä¸€çš„æµæ°´çº¿å®ç°äº†è·¨ç™Œç§çš„æ— ç¼é€‚é…ï¼Œå¹¶ä¸ºæ²»ç–—å¼ºåº¦å†³ç­–æä¾›äº†æœ‰åŠ›çš„æ•°æ®æ”¯æŒã€‚æœ€ç»ˆï¼ŒEAGLE ç»“åˆäº†å°–ç«¯æ€§èƒ½ä¸ä¸´åºŠå½’å› åˆ†æï¼Œä¸ºå¢å¼ºåŒ»ç”Ÿå¯¹è‡ªåŠ¨åŒ–é¢„æµ‹çš„ä¿¡ä»»åŠæ¨åŠ¨å¤šæ¨¡æ€ AI åœ¨åŒ»ç–—å®è·µä¸­çš„éƒ¨ç½²æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.22446v1",
      "published_date": "2025-06-12 03:56:13 UTC",
      "updated_date": "2025-06-12 03:56:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:26:51.843790+00:00"
    },
    {
      "arxiv_id": "2506.10330v1",
      "title": "Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements",
      "title_zh": "ç»“åˆé™æ€ä»£ç åˆ†æå¢å¼ºå¤§è¯­è¨€æ¨¡å‹ä»¥å®ç°è‡ªåŠ¨åŒ–ä»£ç è´¨é‡æå‡",
      "authors": [
        "Seyed Moein Abtahi",
        "Akramul Azim"
      ],
      "abstract": "This study examined code issue detection and revision automation by integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and GPT-4o into software development workflows. A static code analysis framework detects issues such as bugs, vulnerabilities, and code smells within a large-scale software project. Detailed information on each issue was extracted and organized to facilitate automated code revision using LLMs. An iterative prompt engineering process is applied to ensure that prompts are structured to produce accurate and organized outputs aligned with the project requirements. Retrieval-augmented generation (RAG) is implemented to enhance the relevance and precision of the revisions, enabling LLM to access and integrate real-time external knowledge. The issue of LLM hallucinations - where the model generates plausible but incorrect outputs - is addressed by a custom-built \"Code Comparison App,\" which identifies and corrects erroneous changes before applying them to the codebase. Subsequent scans using the static code analysis framework revealed a significant reduction in code issues, demonstrating the effectiveness of combining LLMs, static analysis, and RAG to improve code quality, streamline the software development process, and reduce time and resource expenditure.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°† Large Language Models (LLMs) ä¸é™æ€ä»£ç åˆ†æç›¸ç»“åˆï¼Œä»¥å®ç°è½¯ä»¶å¼€å‘ä¸­ä»£ç é—®é¢˜çš„è‡ªåŠ¨æ£€æµ‹ä¸ä¿®å¤ã€‚ç ”ç©¶å°† GPT-3.5 Turbo å’Œ GPT-4o é›†æˆåˆ°å·¥ä½œæµä¸­ï¼Œåˆ©ç”¨é™æ€åˆ†ææ¡†æ¶è¯†åˆ«å¤§è§„æ¨¡é¡¹ç›®ä¸­çš„ bugsã€vulnerabilities å’Œ code smellsã€‚é€šè¿‡åº”ç”¨è¿­ä»£çš„ prompt engineering å’Œ Retrieval-augmented generation (RAG) æŠ€æœ¯ï¼Œç ”ç©¶æ˜¾è‘—å¢å¼ºäº†ä¿®å¤æ–¹æ¡ˆçš„ç›¸å…³æ€§ä¸ç²¾ç¡®åº¦ã€‚ä¸ºäº†åº”å¯¹ LLM çš„ hallucinations é£é™©ï¼Œç ”ç©¶å¼€å‘äº†ä¸“é—¨çš„ Code Comparison Appï¼Œåœ¨ä»£ç åº”ç”¨å‰å¯¹é”™è¯¯å˜æ›´è¿›è¡Œè¯†åˆ«å’Œçº æ­£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç»“åˆé™æ€åˆ†æã€LLMs å’Œ RAG çš„æ–¹æ³•æ˜¾è‘—å‡å°‘äº†ä»£ç é—®é¢˜ï¼Œåœ¨æå‡ä»£ç è´¨é‡çš„åŒæ—¶æœ‰æ•ˆä¼˜åŒ–äº†è½¯ä»¶å¼€å‘æµç¨‹å¹¶é™ä½äº†èµ„æºæ¶ˆè€—ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at FORGE 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10330v1",
      "published_date": "2025-06-12 03:39:25 UTC",
      "updated_date": "2025-06-12 03:39:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:27:10.275457+00:00"
    },
    {
      "arxiv_id": "2506.10328v1",
      "title": "Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework",
      "title_zh": "è¿ˆå‘å¯æ‰©å±•çš„ SOAP è®°å½•ç”Ÿæˆï¼šä¸€ç§å¼±ç›‘ç£å¤šæ¨¡æ€æ¡†æ¶",
      "authors": [
        "Sadia Kamal",
        "Tim Oates",
        "Joy Wan"
      ],
      "abstract": "Skin carcinoma is the most prevalent form of cancer globally, accounting for over $8 billion in annual healthcare expenditures. In clinical settings, physicians document patient visits using detailed SOAP (Subjective, Objective, Assessment, and Plan) notes. However, manually generating these notes is labor-intensive and contributes to clinician burnout. In this work, we propose a weakly supervised multimodal framework to generate clinically structured SOAP notes from limited inputs, including lesion images and sparse clinical text. Our approach reduces reliance on manual annotations, enabling scalable, clinically grounded documentation while alleviating clinician burden and reducing the need for large annotated data. Our method achieves performance comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical relevance metrics. To evaluate clinical quality, we introduce two novel metrics MedConceptEval and Clinical Coherence Score (CCS) which assess semantic alignment with expert medical concepts and input features, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çš®è‚¤ç™Œ(Skin carcinoma)è¯Šç–—ä¸­SOAP(Subjective, Objective, Assessment, and Plan)è®°å½•ç”Ÿæˆçš„æ²‰é‡è´Ÿæ‹…ï¼Œæå‡ºäº†ä¸€ç§å¼±ç›‘ç£å¤šæ¨¡æ€æ¡†æ¶(Weakly supervised multimodal framework)ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆç—…å˜å›¾åƒ(Lesion images)å’Œç¨€ç–ä¸´åºŠæ–‡æœ¬(Sparse clinical text)ï¼Œå®ç°äº†åœ¨æœ‰é™è¾“å…¥ä¸‹ç”Ÿæˆä¸´åºŠç»“æ„åŒ–è®°å½•ï¼Œæ˜¾è‘—é™ä½äº†å¯¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–å¹¶ç¼“è§£äº†ä¸´åºŠåŒ»ç”Ÿçš„å·¥ä½œå‹åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å…³é”®ä¸´åºŠæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†ä¸GPT-4oã€ClaudeåŠDeepSeek Janus Proç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†MedConceptEvalå’Œä¸´åºŠä¸€è‡´æ€§è¯„åˆ†(Clinical Coherence Score, CCS)ä¸¤é¡¹æ–°æŒ‡æ ‡ï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼°ç”Ÿæˆæ–‡æ¡£ä¸ä¸“å®¶åŒ»ç–—æ¦‚å¿µçš„è¯­ä¹‰å¯¹é½åº¦ä»¥åŠä¸è¾“å…¥ç‰¹å¾çš„ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at IEEE/CVF Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "pdf_url": "https://arxiv.org/pdf/2506.10328v1",
      "published_date": "2025-06-12 03:33:46 UTC",
      "updated_date": "2025-06-12 03:33:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:27:32.418503+00:00"
    },
    {
      "arxiv_id": "2506.10326v3",
      "title": "VGC-Bench: Towards Mastering Diverse Team Strategies in Competitive PokÃ©mon",
      "title_zh": "VGC-Benchï¼šæ—¨åœ¨æŒæ¡ PokÃ©mon ç«æŠ€ä¸­çš„å¤šæ ·åŒ–é˜Ÿä¼ç­–ç•¥",
      "authors": [
        "Cameron Angliss",
        "Jiaxun Cui",
        "Jiaheng Hu",
        "Arrasy Rahman",
        "Peter Stone"
      ],
      "abstract": "Developing AI agents that can robustly adapt to varying strategic landscapes without retraining is a central challenge in multi-agent learning. PokÃ©mon Video Game Championships (VGC) is a domain with a vast space of approximately $10^{139}$ team configurations, far larger than those of other games such as Chess, Go, Poker, StarCraft, or Dota. The combinatorial nature of team building in PokÃ©mon VGC causes optimal strategies to vary substantially depending on both the controlled team and the opponent's team, making generalization uniquely challenging. To advance research on this problem, we introduce VGC-Bench: a benchmark that provides critical infrastructure, standardizes evaluation protocols, and supplies a human-play dataset of over 700,000 battle logs and a range of baseline agents based on heuristics, large language models, behavior cloning, and multi-agent reinforcement learning with empirical game-theoretic methods such as self-play, fictitious play, and double oracle. In the restricted setting where an agent is trained and evaluated in a mirror match with a single team configuration, our methods can win against a professional VGC competitor. We repeat this training and evaluation with progressively larger team sets and find that as the number of teams increases, the best-performing algorithm in the single-team setting has worse performance and is more exploitable, but has improved generalization to unseen teams. Our code and dataset are open-sourced at https://github.com/cameronangliss/vgc-bench and https://huggingface.co/datasets/cameronangliss/vgc-battle-logs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VGC-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³ç«äº‰æ€§å®å¯æ¢¦å¯¹æˆ˜(PokÃ©mon VGC)ä¸­å¤šæ ·åŒ–å›¢é˜Ÿç­–ç•¥é€‚åº”éš¾é¢˜çš„åŸºå‡†å¹³å°ã€‚é’ˆå¯¹è¯¥é¢†åŸŸæé«˜çš„ç»„åˆå¤æ‚æ€§åŠç­–ç•¥æ³›åŒ–æŒ‘æˆ˜ï¼ŒVGC-Benchæä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ã€åŒ…å«70ä¸‡æ¡å¯¹æˆ˜è®°å½•çš„äººç±»æ•°æ®é›†ï¼Œä»¥åŠåŸºäºLarge Language Models (LLMs)å’ŒMulti-Agent Reinforcement Learning (MARL)ç­‰æŠ€æœ¯çš„å¤šç§åŸºçº¿æ™ºèƒ½ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å•ä¸€å›¢é˜Ÿé…ç½®çš„é•œåƒå¯¹æˆ˜ä¸­ï¼Œæ‰€ææ–¹æ³•è®­ç»ƒå‡ºçš„æ™ºèƒ½ä½“èƒ½å¤Ÿå‡»è´¥èŒä¸šé€‰æ‰‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œéšç€è®­ç»ƒå›¢é˜Ÿè§„æ¨¡çš„å¢åŠ ï¼Œè™½ç„¶æ™ºèƒ½ä½“å¯¹æœªçŸ¥å›¢é˜Ÿçš„Generalizationèƒ½åŠ›æ˜¾è‘—å¢å¼ºï¼Œä½†å…¶ç‰¹å®šæ€§èƒ½ä¼šæœ‰æ‰€ä¸‹é™ä¸”è¡¨ç°å‡ºæ›´é«˜çš„å¯åˆ©ç”¨æ€§(Exploitability)ã€‚è¯¥åŸºå‡†åŠå…¶å¼€æºä»£ç ä¸æ•°æ®é›†ä¸ºç ”ç©¶å¤šæ™ºèƒ½ä½“å­¦ä¹ åœ¨å¤æ‚åšå¼ˆç¯å¢ƒä¸­çš„é²æ£’æ€§ä¸æ³›åŒ–æä¾›äº†é‡è¦åŸºç¡€è®¾æ–½ã€‚",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "AAMAS 2026",
      "pdf_url": "https://arxiv.org/pdf/2506.10326v3",
      "published_date": "2025-06-12 03:19:39 UTC",
      "updated_date": "2026-01-13 13:17:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:27:55.589191+00:00"
    },
    {
      "arxiv_id": "2507.00008v2",
      "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning",
      "title_zh": "DiMo-GUIï¼šé€šè¿‡æ¨¡æ€æ„ŸçŸ¥è§†è§‰æ¨ç†æ¨è¿› GUI å®šä½ä¸­çš„æµ‹è¯•æ—¶æ‰©å±•",
      "authors": [
        "Hang Wu",
        "Hongkai Chen",
        "Yujun Cai",
        "Chang Liu",
        "Qingwen Ye",
        "Ming-Hsuan Yang",
        "Yiwei Wang"
      ],
      "abstract": "Grounding natural language queries in graphical user interfaces (GUIs) poses unique challenges due to the diversity of visual elements, spatial clutter, and the ambiguity of language. In this paper, we introduce DiMo-GUI, a training-free framework for GUI grounding that leverages two core strategies: dynamic visual grounding and modality-aware optimization. Instead of treating the GUI as a monolithic image, our method splits the input into textual elements and iconic elements, allowing the model to reason over each modality independently using general-purpose vision-language models. When predictions are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by generating candidate focal regions centered on the model's initial predictions and incrementally zooms into subregions to refine the grounding result. This hierarchical refinement process helps disambiguate visually crowded layouts without the need for additional training or annotations. We evaluate our approach on standard GUI grounding benchmarks and demonstrate consistent improvements over baseline inference pipelines, highlighting the effectiveness of combining modality separation with region-focused reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DiMo-GUIï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒ (training-free) çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ (GUI) å®šä½æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ GUI å…ƒç´ å¤šæ ·æ€§ã€ç©ºé—´æ‹¥æŒ¤åŠè¯­è¨€æ­§ä¹‰å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ¨¡æ€æ„ŸçŸ¥ä¼˜åŒ– (modality-aware optimization) ç­–ç•¥ï¼Œå°†è¾“å…¥æ‹†åˆ†ä¸ºæ–‡æœ¬å…ƒç´  (textual elements) å’Œå›¾æ ‡å…ƒç´  (iconic elements)ï¼Œä½¿é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç‹¬ç«‹å¯¹ä¸åŒæ¨¡æ€è¿›è¡Œæ¨ç†ã€‚é’ˆå¯¹é¢„æµ‹æ¨¡ç³Šçš„é—®é¢˜ï¼ŒDiMo-GUI å¼•å…¥äº†åŠ¨æ€è§†è§‰å®šä½æœºåˆ¶ï¼Œé€šè¿‡åœ¨åˆå§‹é¢„æµ‹å‘¨å›´ç”Ÿæˆå€™é€‰ç„¦ç‚¹åŒºåŸŸå¹¶æ‰§è¡Œåˆ†å±‚ç²¾ç»†åŒ– (hierarchical refinement) ç¼©æ”¾ï¼Œæ˜¾è‘—æå‡äº†å®šä½ç²¾åº¦ã€‚è¿™ç§æ–¹æ³•æ— éœ€é¢å¤–çš„æ ‡æ³¨æˆ–æ¨¡å‹å¾®è°ƒï¼Œå³å¯æœ‰æ•ˆæ¶ˆé™¤è§†è§‰æ‹¥æŒ¤å¸ƒå±€ä¸­çš„æ­§ä¹‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiMo-GUI åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå……åˆ†è¯æ˜äº†å°†æ¨¡æ€åˆ†ç¦»ä¸åŒºåŸŸèšç„¦æ¨ç†ç›¸ç»“åˆåœ¨æå‡æµ‹è¯•æ—¶ç¼©æ”¾ (test-time scaling) æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "EMNLP 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2507.00008v2",
      "published_date": "2025-06-12 03:13:21 UTC",
      "updated_date": "2025-09-05 17:21:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:27:48.464314+00:00"
    },
    {
      "arxiv_id": "2506.10317v2",
      "title": "Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving",
      "title_zh": "åˆ©ç”¨è¯­è¨€ä¿¡æ¯ä¸é“è·¯è®¾è®¡æ‰‹å†Œè¾…åŠ©è‡ªåŠ¨é©¾é©¶åœ°å›¾é‡å»º",
      "authors": [
        "Akshar Tumu",
        "Henrik I. Christensen",
        "Marcell Vazquez-Chanlatte",
        "Chikao Tsuchiya",
        "Dhaval Bhanderi"
      ],
      "abstract": "Lane-topology prediction is a critical component of safe and reliable autonomous navigation. An accurate understanding of the road environment aids this task. We observe that this information often follows conventions encoded in natural language, through design codes that reflect the road structure and road names that capture the road functionality. We augment this information in a lightweight manner to SMERF, a map-prior-based online lane-topology prediction model, by combining structured road metadata from OSM maps and lane-width priors from Road design manuals with the road centerline encodings. We evaluate our method on two geo-diverse complex intersection scenarios. Our method shows improvement in both lane and traffic element detection and their association. We report results using four topology-aware metrics to comprehensively assess the model performance. These results demonstrate the ability of our approach to generalize and scale to diverse topologies and conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨è¯­è¨€ä¿¡æ¯å’Œé“è·¯æ‰‹å†Œæ¥è¾…åŠ©è‡ªåŠ¨é©¾é©¶ä¸­çš„åœ°å›¾é‡å»ºï¼Œé‡ç‚¹å…³æ³¨è½¦é“æ‹“æ‰‘é¢„æµ‹(Lane-topology prediction)ä»»åŠ¡ã€‚ä½œè€…å‘ç°é“è·¯ç»“æ„å¾€å¾€éµå¾ªè®¾è®¡è§„èŒƒ(design codes)ç­‰è‡ªç„¶è¯­è¨€çº¦å®šçš„æƒ¯ä¾‹ï¼Œå› æ­¤æå‡ºå°†OpenStreetMap(OSM)çš„ç»“æ„åŒ–é“è·¯å…ƒæ•°æ®å’Œé“è·¯è®¾è®¡æ‰‹å†Œä¸­çš„è½¦é“å®½åº¦å…ˆéªŒ(lane-width priors)å¼•å…¥æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§æ–¹å¼å¢å¼ºäº†ç°æœ‰çš„åœ¨çº¿é¢„æµ‹æ¨¡å‹SMERFï¼Œå°†å…¶ä¸é“è·¯ä¸­å¿ƒçº¿ç¼–ç ç›¸ç»“åˆä»¥æå‡ç¯å¢ƒç†è§£èƒ½åŠ›ã€‚åœ¨ä¸¤ä¸ªåœ°ç†å¤šæ ·åŒ–çš„å¤æ‚äº¤å‰å£åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æ”¹å–„äº†è½¦é“åŠäº¤é€šå…ƒç´ çš„æ£€æµ‹ä¸å…³è”ç²¾åº¦ã€‚å®éªŒé‡‡ç”¨å››ç§æ‹“æ‰‘æ„ŸçŸ¥æŒ‡æ ‡(topology-aware metrics)è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œç»“æœè¯å®è¯¥æ–¹æ¡ˆåœ¨å¤„ç†ä¸åŒé“è·¯æ‹“æ‰‘å’Œç¯å¢ƒæ¡ä»¶æ—¶å…·æœ‰æå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "4 pages, 3 figures, Accepted at RSS 2025 Workshop - RobotEvaluation@RSS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10317v2",
      "published_date": "2025-06-12 03:02:01 UTC",
      "updated_date": "2025-06-20 00:26:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:28:12.990222+00:00"
    },
    {
      "arxiv_id": "2506.10309v1",
      "title": "DUN-SRE: Deep Unrolling Network with Spatiotemporal Rotation Equivariance for Dynamic MRI Reconstruction",
      "title_zh": "DUN-SREï¼šå…·æœ‰æ—¶ç©ºæ—‹è½¬ç­‰å˜æ€§çš„åŠ¨æ€ç£å…±æŒ¯æˆåƒé‡å»ºæ·±å±‚å±•å¼€ç½‘ç»œ",
      "authors": [
        "Yuliang Zhu",
        "Jing Cheng",
        "Qi Xie",
        "Zhuo-Xu Cui",
        "Qingyong Zhu",
        "Yuanyuan Liu",
        "Xin Liu",
        "Jianfeng Ren",
        "Chengbo Wang",
        "Dong Liang"
      ],
      "abstract": "Dynamic Magnetic Resonance Imaging (MRI) exhibits transformation symmetries, including spatial rotation symmetry within individual frames and temporal symmetry along the time dimension. Explicit incorporation of these symmetry priors in the reconstruction model can significantly improve image quality, especially under aggressive undersampling scenarios. Recently, Equivariant convolutional neural network (ECNN) has shown great promise in exploiting spatial symmetry priors. However, existing ECNNs critically fail to model temporal symmetry, arguably the most universal and informative structural prior in dynamic MRI reconstruction. To tackle this issue, we propose a novel Deep Unrolling Network with Spatiotemporal Rotation Equivariance (DUN-SRE) for Dynamic MRI Reconstruction. The DUN-SRE establishes spatiotemporal equivariance through a (2+1)D equivariant convolutional architecture. In particular, it integrates both the data consistency and proximal mapping module into a unified deep unrolling framework. This architecture ensures rigorous propagation of spatiotemporal rotation symmetry constraints throughout the reconstruction process, enabling more physically accurate modeling of cardiac motion dynamics in cine MRI. In addition, a high-fidelity group filter parameterization mechanism is developed to maintain representation precision while enforcing symmetry constraints. Comprehensive experiments on Cardiac CINE MRI datasets demonstrate that DUN-SRE achieves state-of-the-art performance, particularly in preserving rotation-symmetric structures, offering strong generalization capability to a broad range of dynamic MRI reconstruction tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ¨æ€ç£å…±æŒ¯æˆåƒ (Dynamic MRI) é‡å»ºä¸­ç°æœ‰ç­‰å˜å·ç§¯ç¥ç»ç½‘ç»œ (ECNN) æ— æ³•æœ‰æ•ˆå»ºæ¨¡æ—¶é—´å¯¹ç§°æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†å…·æœ‰æ—¶ç©ºæ—‹è½¬ç­‰å˜æ€§çš„æ·±åº¦å±•å¼€ç½‘ç»œ (Deep Unrolling Network with Spatiotemporal Rotation Equivariance, DUN-SRE)ã€‚è¯¥æ¡†æ¶é€šè¿‡ (2+1)D ç­‰å˜å·ç§¯æ¶æ„å»ºç«‹æ—¶ç©ºç­‰å˜æ€§ï¼Œå¹¶å°†æ•°æ®ä¸€è‡´æ€§ (Data Consistency) ä¸è¿‘ç«¯æ˜ å°„ (Proximal Mapping) æ¨¡å—é›†æˆåˆ°ç»Ÿä¸€çš„æ·±åº¦å±•å¼€æµç¨‹ä¸­ï¼Œç¡®ä¿å¯¹ç§°çº¦æŸåœ¨é‡å»ºè¿‡ç¨‹ä¸­çš„ä¸¥æ ¼ä¼ æ’­ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†é«˜ä¿çœŸç¾¤æ»¤æ³¢å™¨å‚æ•°åŒ– (High-fidelity group filter parameterization) æœºåˆ¶ï¼Œä»¥åœ¨å¼ºåˆ¶æ‰§è¡Œå¯¹ç§°çº¦æŸçš„åŒæ—¶ä¿æŒè¡¨ç¤ºç²¾åº¦ã€‚åœ¨å¿ƒè„ç”µå½± MRI (Cardiac CINE MRI) æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDUN-SRE åœ¨ä¿ç•™æ—‹è½¬å¯¹ç§°ç»“æ„æ–¹é¢è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶å¯¹å¤šç§åŠ¨æ€ MRI é‡å»ºä»»åŠ¡å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10309v1",
      "published_date": "2025-06-12 02:44:27 UTC",
      "updated_date": "2025-06-12 02:44:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:27:54.992237+00:00"
    },
    {
      "arxiv_id": "2506.10304v2",
      "title": "The Alignment Trap: Complexity Barriers",
      "title_zh": "å¯¹é½é™·é˜±ï¼šå¤æ‚æ€§å±éšœ",
      "authors": [
        "Jasper Yao"
      ],
      "abstract": "This paper argues that AI alignment is not merely difficult, but is founded on a fundamental logical contradiction. We first establish The Enumeration Paradox: we use machine learning precisely because we cannot enumerate all necessary safety rules, yet making ML safe requires examples that can only be generated from the very enumeration we admit is impossible. This paradox is then confirmed by a set of five independent mathematical proofs, or \"pillars of impossibility.\" Our main results show that: (1) Geometric Impossibility: The set of safe policies has measure zero, a necessary consequence of projecting infinite-dimensional world-context requirements onto finite-dimensional models. (2) Computational Impossibility: Verifying a policy's safety is coNP-complete, even for non-zero error tolerances. (3) Statistical Impossibility: The training data required for safety (abundant examples of rare disasters) is a logical contradiction and thus unobtainable. (4) Information-Theoretic Impossibility: Safety rules contain more incompressible, arbitrary information than any feasible network can store. (5) Dynamic Impossibility: The optimization process for increasing AI capability is actively hostile to safety, as the gradients for the two objectives are generally anti-aligned. Together, these results demonstrate that the pursuit of safe, highly capable AI is not a matter of overcoming technical hurdles, but of confronting fundamental, interlocking barriers. The paper concludes by presenting a strategic trilemma that these impossibilities force upon the field. A formal verification of the core theorems in Lean4 is currently in progress.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºAI alignmentä¸ä»…åœ¨æŠ€æœ¯ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæ›´åŸºäºä¸€ä¸ªæ ¹æœ¬æ€§çš„é€»è¾‘çŸ›ç›¾ã€‚ä½œè€…æå‡ºäº†The Enumeration Paradoxï¼Œå³æˆ‘ä»¬ä½¿ç”¨æœºå™¨å­¦ä¹ æ˜¯å› ä¸ºæ— æ³•æšä¸¾æ‰€æœ‰å®‰å…¨è§„åˆ™ï¼Œä½†ç¡®ä¿æ¨¡å‹å®‰å…¨åˆéœ€è¦è¿™äº›æ— æ³•ç”Ÿæˆçš„è§„åˆ™ä½œä¸ºè®­ç»ƒç¤ºä¾‹ã€‚é€šè¿‡äº”ä¸ªç‹¬ç«‹çš„æ•°å­¦è¯æ˜ï¼Œç ”ç©¶æ­ç¤ºäº†å®‰å…¨å¯¹é½åœ¨å‡ ä½•ã€è®¡ç®—ã€ç»Ÿè®¡ã€ä¿¡æ¯è®ºå’ŒåŠ¨æ€è¿‡ç¨‹ä¸­çš„ä¸å¯èƒ½å› ç´ ï¼Œè¯æ˜äº†å®‰å…¨ç­–ç•¥é›†åœ¨æµ‹åº¦ä¸Šä¸ºé›¶ä¸”éªŒè¯è¿‡ç¨‹å±äºcoNP-completeã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿½æ±‚é«˜èƒ½åŠ›çš„å®‰å…¨æ€§AIé¢ä¸´ç€ç›¸äº’äº¤ç»‡çš„æ ¹æœ¬æ€§å±éšœï¼Œè€Œéå•çº¯çš„æŠ€æœ¯éšœç¢ã€‚æ–‡ç« æœ€åæå‡ºäº†ç”±è¿™äº›ç»“è®ºå¯¼è‡´çš„æˆ˜ç•¥ä¸‰éš¾å›°å¢ƒ(trilemma)ï¼Œç›®å‰æ­£é€šè¿‡Lean4å¯¹æ ¸å¿ƒå®šç†è¿›è¡Œå½¢å¼åŒ–éªŒè¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.CC",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "31 Pages, 4 Figures. Substantial revision. Restructured around the Enumeration Paradox and Five Pillars of Impossibility. Core mathematical results unchanged but significantly expanded. Added new impossibility proofs from statistical, information-theoretic, and dynamic perspectives",
      "pdf_url": "https://arxiv.org/pdf/2506.10304v2",
      "published_date": "2025-06-12 02:30:30 UTC",
      "updated_date": "2025-06-24 23:41:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:28:07.701840+00:00"
    },
    {
      "arxiv_id": "2506.10302v2",
      "title": "A Quad-Step Approach to Uncertainty-Aware Deep Learning for Skin Cancer Classification",
      "title_zh": "é¢å‘çš®è‚¤ç™Œåˆ†ç±»çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ·±åº¦å­¦ä¹ å››æ­¥æ³•",
      "authors": [
        "Hamzeh Asgharnezhad",
        "Pegah Tabarisaadi",
        "Abbas Khosravi",
        "Roohallah Alizadehsani",
        "U. Rajendra Acharya"
      ],
      "abstract": "Accurate skin cancer diagnosis is vital for early treatment and improved patient outcomes. Deep learning (DL) models have shown promise in automating skin cancer classification, yet challenges remain due to data scarcity and limited uncertainty awareness. This study presents a comprehensive evaluation of DL-based skin lesion classification with transfer learning and uncertainty quantification (UQ) on the HAM10000 dataset. We benchmark several pre-trained feature extractors -- including CLIP variants, ResNet50, DenseNet121, VGG16, and EfficientNet-V2-Large -- combined with traditional classifiers such as SVM, XGBoost, and logistic regression. Multiple principal component analysis (PCA) settings (64, 128, 256, 512) are explored, with LAION CLIP ViT-H/14 and ViT-L/14 at PCA-256 achieving the strongest baseline results. In the UQ phase, Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte Carlo Dropout (EMCD) are applied and evaluated using uncertainty-aware metrics (UAcc, USen, USpe, UPre). Ensemble methods with PCA-256 provide the best balance between accuracy and reliability. Further improvements are obtained through feature fusion of top-performing extractors at PCA-256. Finally, we propose a feature-fusion based model trained with a predictive entropy (PE) loss function, which outperforms all prior configurations across both standard and uncertainty-aware evaluations, advancing trustworthy DL-based skin cancer diagnosis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çš®è‚¤ç™Œåˆ†ç±»ä¸­æ•°æ®ç¨€ç¼ºå’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥ä¸è¶³çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å››æ­¥æ³•(Quad-Step Approach)æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¯Šæ–­å‡†ç¡®æ€§ä¸å¯é æ€§ã€‚ç ”ç©¶åœ¨HAM10000æ•°æ®é›†ä¸Šè¯„ä¼°äº†åŒ…æ‹¬CLIPå˜ä½“ã€ResNet50å’ŒDenseNet121åœ¨å†…çš„å¤šç§é¢„è®­ç»ƒç‰¹å¾æå–å™¨ï¼Œå¹¶ç»“åˆSVMã€XGBoostç­‰åˆ†ç±»å™¨åŠä¸åŒä¸»æˆåˆ†åˆ†æ(PCA)è®¾ç½®è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚å®éªŒå‘ç°ï¼ŒLAION CLIP ViT-H/14å’ŒViT-L/14åœ¨PCA-256è®¾ç½®ä¸‹è¡¨ç°å‡ºæœ€å¼ºçš„åŸºçº¿ç»“æœã€‚åœ¨ä¸ç¡®å®šæ€§é‡åŒ–(UQ)é˜¶æ®µï¼Œç ”ç©¶å¯¹æ¯”äº†è’™ç‰¹å¡æ´›éšæœºå¤±æ´»(Monte Carlo Dropout, MCD)ã€é›†æˆå­¦ä¹ (Ensemble)åŠäºŒè€…ç»“åˆçš„EMCDæ–¹æ³•ï¼Œå‘ç°é›†æˆæ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸å¯é æ€§ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚ç ”ç©¶æœ€ç»ˆæå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾èåˆ(Feature Fusion)ä¸”é‡‡ç”¨é¢„æµ‹ç†µ(Predictive Entropy, PE)æŸå¤±å‡½æ•°è®­ç»ƒçš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨æ ‡å‡†è¯„ä¼°å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥æŒ‡æ ‡(UAcc, USen, USpe, UPre)ä¸­å‡ä¼˜äºæ‰€æœ‰å…ˆå‰é…ç½®ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„æ·±åº¦å­¦ä¹ çš®è‚¤ç™Œè¾…åŠ©è¯Šæ–­ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10302v2",
      "published_date": "2025-06-12 02:29:16 UTC",
      "updated_date": "2025-09-24 12:41:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:29:37.171143+00:00"
    },
    {
      "arxiv_id": "2506.10301v2",
      "title": "Towards Understanding Bias in Synthetic Data for Evaluation",
      "title_zh": "æ·±å…¥æ¢ç©¶è¯„ä¼°ç”¨åˆæˆæ•°æ®ä¸­çš„åå·®",
      "authors": [
        "Hossein A. Rahmani",
        "Varsha Ramineni",
        "Emine Yilmaz",
        "Nick Craswell",
        "Bhaskar Mitra"
      ],
      "abstract": "Test collections are crucial for evaluating Information Retrieval (IR) systems. Creating a diverse set of user queries for these collections can be challenging, and obtaining relevance judgments, which indicate how well retrieved documents match a query, is often costly and resource-intensive. Recently, generating synthetic datasets using Large Language Models (LLMs) has gained attention in various applications. While previous work has used LLMs to generate synthetic queries or documents to improve ranking models, using LLMs to create synthetic test collections is still relatively unexplored. Previous work~\\cite{rahmani2024synthetic} showed that synthetic test collections have the potential to be used for system evaluation, however, more analysis is needed to validate this claim. In this paper, we thoroughly investigate the reliability of synthetic test collections constructed using LLMs, where LLMs are used to generate synthetic queries, labels, or both. In particular, we examine the potential biases that might occur when such test collections are used for evaluation. We first empirically show the presence of such bias in evaluation results and analyse the effects it might have on system evaluation. We further validate the presence of such bias using a linear mixed-effects model. Our analysis shows that while the effect of bias present in evaluation results obtained using synthetic test collections could be significant, for e.g.~computing absolute system performance, its effect may not be as significant in comparing relative system performance. Codes and data are available at: https://github.com/rahmanidashti/BiasSyntheticData.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)æ„å»ºåˆæˆæµ‹è¯•é›†åœ¨ä¿¡æ¯æ£€ç´¢(Information Retrieval, IR)ç³»ç»Ÿè¯„ä¼°ä¸­çš„å¯é æ€§ã€‚ä½œè€…é‡ç‚¹ç ”ç©¶äº†åœ¨ä½¿ç”¨LLMsç”ŸæˆåˆæˆæŸ¥è¯¢ã€æ ‡ç­¾æˆ–ä¸¤è€…å…¼æœ‰æ—¶å¯èƒ½äº§ç”Ÿçš„æ½œåœ¨åå·®(bias)ï¼Œå¹¶åˆ†æäº†è¿™äº›åå·®å¯¹ç³»ç»Ÿè¯„ä¼°çš„å…·ä½“å½±å“ã€‚é€šè¿‡å®è¯ç ”ç©¶å’Œçº¿æ€§æ··åˆæ•ˆåº”æ¨¡å‹(linear mixed-effects model)ï¼Œè®ºæ–‡è¯å®äº†è¯„ä¼°ç»“æœä¸­ç¡®å®å­˜åœ¨æ˜¾è‘—åå·®ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶è¿™äº›åå·®å¯¹è®¡ç®—ç³»ç»Ÿçš„ç»å¯¹æ€§èƒ½(absolute system performance)æœ‰æ˜¾è‘—å½±å“ï¼Œä½†åœ¨æ¯”è¾ƒä¸åŒç³»ç»Ÿçš„ç›¸å¯¹æ€§èƒ½(relative system performance)æ—¶ï¼Œå…¶å½±å“ç¨‹åº¦ç›¸å¯¹æœ‰é™ã€‚è¯¥å·¥ä½œä¸ºç†è§£åˆæˆæ•°æ®åœ¨è¯„ä¼°ä¸­çš„å¯è¡Œæ€§æä¾›äº†é‡è¦å‚è€ƒï¼Œå¹¶å…¬å¼€äº†ç›¸å…³çš„ä»£ç å’Œæ•°æ®ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "CIKM 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10301v2",
      "published_date": "2025-06-12 02:25:42 UTC",
      "updated_date": "2025-10-04 03:28:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:29:05.046435+00:00"
    },
    {
      "arxiv_id": "2506.13795v1",
      "title": "BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection",
      "title_zh": "BotTransï¼šé¢å‘ç¤¾äº¤æœºå™¨äººæ£€æµ‹çš„å¤šæºå›¾åŸŸè‡ªé€‚åº”æ–¹æ³•",
      "authors": [
        "Boshen Shi",
        "Yongqing Wang",
        "Fangda Guo",
        "Jiangli Shao",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "abstract": "Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \\textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \\textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BotTransï¼Œä¸€ç§å¤šæºå›¾é¢†åŸŸè‡ªé€‚åº”(multi-source graph domain adaptation)æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç¤¾äº¤æœºå™¨äººæ£€æµ‹ä¸­æ ‡ç­¾ç¨€ç¼ºã€ç½‘ç»œå¼‚è´¨æ€§(heterophily)ä»¥åŠå•æºçŸ¥è¯†è¿ç§»ä¸ç¨³å®šç­‰æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é¦–å…ˆåˆ©ç”¨å¤šä¸ªæºç½‘ç»œé—´å…±äº«çš„æ ‡æ³¨çŸ¥è¯†æ„å»ºè·¨æºåŸŸæ‹“æ‰‘ç»“æ„ï¼Œæœ‰æ•ˆå¢å¼ºäº†ç½‘ç»œåŒè´¨æ€§(homophily)ï¼Œå¹¶é€šè¿‡èšåˆè·¨åŸŸé‚»å±…ä¿¡æ¯å¼ºåŒ–äº†èŠ‚ç‚¹åµŒå…¥çš„åŒºåˆ†åº¦ã€‚éšåï¼ŒBotTranså°†æº-ç›®æ ‡å¯¹çš„ç›¸å…³æ€§é›†æˆåˆ°æ¨¡å‹ä¼˜åŒ–ä¸­ï¼Œç¡®ä¿ä»ç›¸å…³æ€§æ›´é«˜çš„æºç½‘ç»œä¸­ä¼˜å…ˆè¿ç§»çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åˆ©ç”¨ç›®æ ‡åŸŸè¯­ä¹‰çŸ¥è¯†çš„ç»†åŒ–ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ£€æµ‹æ€§èƒ½ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBotTransåœ¨ç›®æ ‡ä»»åŠ¡æ— æ ‡ç­¾çš„æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„SOTAæ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤šæºçŸ¥è¯†æ•´åˆä¸è¿ç§»æ–¹é¢çš„å“è¶Šæ•ˆèƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accetpted to ECML-PKDD 2025 Research Track as oral; Code&data: https://github.com/Skyorca/BotTrans",
      "pdf_url": "https://arxiv.org/pdf/2506.13795v1",
      "published_date": "2025-06-12 02:10:36 UTC",
      "updated_date": "2025-06-12 02:10:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:28:09.745446+00:00"
    },
    {
      "arxiv_id": "2506.10292v1",
      "title": "Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages",
      "title_zh": "Flickï¼šå¤šä»»åŠ¡ä½èµ„æºè¯­è¨€ä¸­åŸºäº K æ„ŸçŸ¥ä¸­é—´å­¦ä¹ çš„å°‘æ ‡ç­¾æ–‡æœ¬åˆ†ç±»",
      "authors": [
        "Ali Almutairi",
        "Abdullah Alsuhaibani",
        "Shoaib Jameel",
        "Usman Naseem",
        "Gelareh Mohammadi",
        "Imran Razzak"
      ],
      "abstract": "Training deep learning networks with minimal supervision has gained significant research attention due to its potential to reduce reliance on extensive labelled data. While self-training methods have proven effective in semi-supervised learning, they remain vulnerable to errors from noisy pseudo labels. Moreover, most recent approaches to the few-label classification problem are either designed for resource-rich languages such as English or involve complex cascading models that are prone to overfitting. To address the persistent challenge of few-label text classification in truly low-resource linguistic contexts, where existing methods often struggle with noisy pseudo-labels and domain adaptation, we propose Flick. Unlike prior methods that rely on generic multi-cluster pseudo-labelling or complex cascading architectures, Flick leverages the fundamental insight that distilling high-confidence pseudo-labels from a broader set of initial clusters can dramatically improve pseudo-label quality, particularly for linguistically diverse, low-resource settings. Flick introduces a novel pseudo-label refinement component, a departure from traditional pseudo-labelling strategies by identifying and leveraging top-performing pseudo-label clusters. This component specifically learns to distil highly reliable pseudo-labels from an initial broad set by focusing on single-cluster cohesion and leveraging an adaptive top-k selection mechanism. This targeted refinement process is crucial for mitigating the propagation of errors inherent in low-resource data, allowing for robust fine-tuning of pre-trained language models with only a handful of true labels. We demonstrate Flick's efficacy across 14 diverse datasets, encompassing challenging low-resource languages such as Arabic, Urdu, and Setswana, alongside English, showcasing its superior performance and adaptability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹å°‘æ ·æœ¬æ–‡æœ¬åˆ†ç±»(Few-label text classification)é¢ä¸´çš„å™ªå£°ä¼ªæ ‡ç­¾(noisy pseudo labels)å’Œé¢†åŸŸè‡ªé€‚åº”æŒ‘æˆ˜ï¼Œæå‡ºäº† Flick æ¡†æ¶ã€‚ä¸åŒäºä¾èµ–å¤šç°‡ä¼ªæ ‡ç­¾æˆ–å¤æ‚çº§è”æ¶æ„çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒFlick å¼•å…¥äº†ä¼ªæ ‡ç­¾ç²¾ç‚¼(pseudo-label refinement)ç»„ä»¶ï¼Œé€šè¿‡å…³æ³¨å•ç°‡å†…èšæ€§(single-cluster cohesion)å¹¶åˆ©ç”¨è‡ªé€‚åº” top-k é€‰æ‹©æœºåˆ¶(adaptive top-k selection mechanism)ä»åˆå§‹èšç±»ä¸­è’¸é¦å‡ºé«˜å¯é æ€§çš„ä¼ªæ ‡ç­¾ã€‚è¿™ç§ç²¾ç‚¼è¿‡ç¨‹æœ‰æ•ˆéåˆ¶äº†ä½èµ„æºæ•°æ®ä¸­å›ºæœ‰çš„è¯¯å·®ä¼ æ’­ï¼Œä½¿å¾—é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨ä»…æœ‰æå°‘é‡çœŸå®æ ‡ç­¾çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¿›è¡Œç¨³å¥çš„å¾®è°ƒã€‚ç ”ç©¶åœ¨åŒ…æ‹¬é˜¿æ‹‰ä¼¯è¯­(Arabic)ã€ä¹Œå°”éƒ½è¯­(Urdu)å’Œå¡èŒ¨ç“¦çº³è¯­(Setswana)ä»¥åŠè‹±è¯­åœ¨å†…çš„ 14 ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸ŠéªŒè¯äº† Flick çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFlick åœ¨å¤„ç†æå…·æŒ‘æˆ˜æ€§çš„å¤šä»»åŠ¡ä½èµ„æºè¯­è¨€æ—¶å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå¼ºå¤§çš„é€‚åº”æ€§ï¼Œä¸ºå‡å°‘æ·±åº¦å­¦ä¹ å¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„ä¾èµ–æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10292v1",
      "published_date": "2025-06-12 02:09:47 UTC",
      "updated_date": "2025-06-12 02:09:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:28:25.025752+00:00"
    },
    {
      "arxiv_id": "2506.10289v1",
      "title": "RT-VC: Real-Time Zero-Shot Voice Conversion with Speech Articulatory Coding",
      "title_zh": "RT-VCï¼šåŸºäºè¯­éŸ³å‘éŸ³ç¼–ç çš„å®æ—¶é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢",
      "authors": [
        "Yisi Liu",
        "Chenyang Wang",
        "Hanjo Kim",
        "Raniya Khan",
        "Gopala Anumanchipalli"
      ],
      "abstract": "Voice conversion has emerged as a pivotal technology in numerous applications ranging from assistive communication to entertainment. In this paper, we present RT-VC, a zero-shot real-time voice conversion system that delivers ultra-low latency and high-quality performance. Our approach leverages an articulatory feature space to naturally disentangle content and speaker characteristics, facilitating more robust and interpretable voice transformations. Additionally, the integration of differentiable digital signal processing (DDSP) enables efficient vocoding directly from articulatory features, significantly reducing conversion latency. Experimental evaluations demonstrate that, while maintaining synthesis quality comparable to the current state-of-the-art (SOTA) method, RT-VC achieves a CPU latency of 61.4 ms, representing a 13.3\\% reduction in latency.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†RT-VCï¼Œä¸€ç§èƒ½å¤Ÿå®ç°è¶…ä½å»¶è¿Ÿå’Œé«˜è´¨é‡æ€§èƒ½çš„é›¶æ ·æœ¬(zero-shot)å®æ—¶è¯­éŸ³è½¬æ¢ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å‘éŸ³ç‰¹å¾ç©ºé—´(articulatory feature space)è‡ªç„¶åœ°è§£è€¦è¯­éŸ³å†…å®¹å’Œè¯´è¯äººç‰¹å¾ï¼Œä»è€Œå®ç°äº†æ›´å…·é²æ£’æ€§å’Œå¯è§£é‡Šæ€§çš„è¯­éŸ³è½¬æ¢ã€‚é€šè¿‡é›†æˆå¯å¾®åˆ†æ•°å­—ä¿¡å·å¤„ç†(DDSP)æŠ€æœ¯ï¼ŒRT-VCèƒ½å¤Ÿç›´æ¥ä»å‘éŸ³ç‰¹å¾ä¸­è¿›è¡Œé«˜æ•ˆçš„å£°ç å™¨(vocoding)å¤„ç†ï¼Œæ˜¾è‘—é™ä½äº†è½¬æ¢å»¶è¿Ÿã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨ä¿æŒä¸å½“å‰æœ€å…ˆè¿›(SOTA)æ–¹æ³•ç›¸å½“çš„åˆæˆè´¨é‡çš„åŒæ—¶ï¼ŒRT-VCåœ¨CPUä¸Šçš„å»¶è¿Ÿä»…ä¸º61.4æ¯«ç§’ï¼Œå®ç°äº†13.3%çš„å»¶è¿Ÿç¼©å‡ã€‚è¯¥ç ”ç©¶ä¸ºéœ€è¦é«˜å®æ—¶æ€§çš„è¾…åŠ©é€šä¿¡å’Œå¨±ä¹ç­‰åº”ç”¨é¢†åŸŸæä¾›äº†é«˜æ•ˆçš„è¯­éŸ³è½¬æ¢æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "ACL Demo Track 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.10289v1",
      "published_date": "2025-06-12 02:02:38 UTC",
      "updated_date": "2025-06-12 02:02:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:28:25.153400+00:00"
    },
    {
      "arxiv_id": "2506.10288v1",
      "title": "ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs",
      "title_zh": "ClusterUCBï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹é’ˆå¯¹æ€§å¾®è°ƒçš„é«˜æ•ˆåŸºäºæ¢¯åº¦çš„æ•°æ®é€‰æ‹©",
      "authors": [
        "Zige Wang",
        "Qi Zhu",
        "Fei Mi",
        "Minghui Xu",
        "Ruochun Jin",
        "Wenjing Yang"
      ],
      "abstract": "Gradient-based data influence approximation has been leveraged to select useful data samples in the supervised fine-tuning of large language models. However, the computation of gradients throughout the fine-tuning process requires too many resources to be feasible in practice. In this paper, we propose an efficient gradient-based data selection framework with clustering and a modified Upper Confidence Bound (UCB) algorithm. Based on the intuition that data samples with similar gradient features will have similar influences, we first perform clustering on the training data pool. Then, we frame the inter-cluster data selection as a constrained computing budget allocation problem and consider it a multi-armed bandit problem. A modified UCB algorithm is leveraged to solve this problem. Specifically, during the iterative sampling process, historical data influence information is recorded to directly estimate the distributions of each cluster, and a cold start is adopted to balance exploration and exploitation. Experimental results on various benchmarks show that our proposed framework, ClusterUCB, can achieve comparable results to the original gradient-based data selection methods while greatly reducing computing consumption.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ClusterUCBï¼Œä¸€ä¸ªç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰é’ˆå¯¹æ€§å¾®è°ƒçš„é«˜æ•ˆæ¢¯åº¦æ•°æ®é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¾®è°ƒè¿‡ç¨‹ä¸­æ¢¯åº¦è®¡ç®—èµ„æºæ¶ˆè€—è¿‡å¤§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŸºäºæ¢¯åº¦ç‰¹å¾ç›¸ä¼¼çš„æ•°æ®å…·æœ‰ç›¸ä¼¼å½±å“åŠ›çš„å‡è®¾ï¼Œé¦–å…ˆå¯¹è®­ç»ƒæ•°æ®æ± è¿›è¡Œèšç±»(clustering)ï¼Œå¹¶å°†ç°‡é—´æ•°æ®é€‰æ‹©å»ºæ¨¡ä¸ºå¤šè‡‚è€è™æœºé—®é¢˜(multi-armed bandit problem)ã€‚ç ”ç©¶é‡‡ç”¨æ”¹è¿›çš„ç½®ä¿¡ä¸Šé™(Upper Confidence Bound, UCB)ç®—æ³•è¿›è¡Œæ±‚è§£ï¼Œåœ¨è¿­ä»£é‡‡æ ·ä¸­åˆ©ç”¨å†å²å½±å“åŠ›ä¿¡æ¯ä¼°è®¡å„ç°‡åˆ†å¸ƒï¼Œå¹¶å¼•å…¥å†·å¯åŠ¨(cold start)æœºåˆ¶å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒClusterUCBåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¸åŸå§‹æ¢¯åº¦æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚è¯¥æ¡†æ¶ä¸ºé«˜æ•ˆç­›é€‰é«˜è´¨é‡å¾®è°ƒæ•°æ®æä¾›äº†ä¸€ç§å¯è¡Œä¸”ä½æˆæœ¬çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10288v1",
      "published_date": "2025-06-12 01:53:01 UTC",
      "updated_date": "2025-06-12 01:53:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:29:57.978561+00:00"
    },
    {
      "arxiv_id": "2506.10281v2",
      "title": "Closer to Language than Steam: AI as the Cognitive Engine of a New Productivity Revolution",
      "title_zh": "äº¦å¦‚è¯­è¨€è€Œéè’¸æ±½ï¼šä½œä¸ºæ–°ä¸€è½®ç”Ÿäº§åŠ›é©å‘½è®¤çŸ¥å¼•æ“çš„äººå·¥æ™ºèƒ½",
      "authors": [
        "Xinmin Fang",
        "Lingfeng Tao",
        "Zhengxiong Li"
      ],
      "abstract": "Artificial Intelligence (AI) is reframed as a cognitive engine driving a novel productivity revolution distinct from the Industrial Revolution's physical thrust. This paper develops a theoretical framing of AI as a cognitive revolution akin to written language - a transformative augmentation of human intellect rather than another mechanized tool. We compare AI's emergence to historical leaps in information technology to show how it amplifies knowledge work. Examples from various domains demonstrate AI's impact as a driver of productivity in cognitive tasks. We adopt a multidisciplinary perspective combining computer science advances with economic insights and sociological perspectives on how AI reshapes work and society. Through conceptual frameworks, we visualize the shift from manual to cognitive productivity. Our central argument is that AI functions as an engine of cognition - comparable to how human language revolutionized knowledge - heralding a new productivity paradigm. We discuss how this revolution demands rethinking of skills, organizations, and policies. This paper, balancing academic rigor with clarity, concludes that AI's promise lies in complementing human cognitive abilities, marking a new chapter in productivity evolution.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†äººå·¥æ™ºèƒ½ (Artificial Intelligence, AI) é‡æ–°å®šä¹‰ä¸ºé©±åŠ¨æ–°ä¸€è½®ç”Ÿäº§åŠ›é©å‘½çš„è®¤çŸ¥å¼•æ“ï¼Œå¼ºè°ƒå…¶ä¸ä»¥ä½“åŠ›æ›¿ä»£ä¸ºç‰¹å¾çš„å·¥ä¸šé©å‘½æœ‰ç€æœ¬è´¨åŒºåˆ«ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œå°† AI è§†ä¸ºä¸€ç§ç±»ä¼¼äºä¹¦é¢è¯­è¨€ (written language) çš„è®¤çŸ¥é©å‘½ï¼Œè®¤ä¸ºå®ƒæ˜¯ä¸€ç§å¯¹äººç±»æ™ºèƒ½çš„å˜é©æ€§å¢å¼ºï¼Œè€Œéç®€å•çš„æœºæ¢°åŒ–å·¥å…·ã€‚é€šè¿‡å¯¹æ¯”å†å²ä¸Šä¿¡æ¯æŠ€æœ¯çš„é‡å¤§é£è·ƒï¼Œç ”ç©¶å±•ç¤ºäº† AI å¦‚ä½•æ”¾å¤§çŸ¥è¯†å·¥ä½œçš„æ•ˆèƒ½ï¼Œå¹¶é‡‡ç”¨è®¡ç®—æœºç§‘å­¦ã€ç»æµå­¦å’Œç¤¾ä¼šå­¦çš„å¤šå­¦ç§‘è§†è§’åˆ†æäº† AI å¯¹å·¥ä½œå’Œç¤¾ä¼šç»“æ„çš„é‡å¡‘ã€‚å€ŸåŠ©æ¦‚å¿µæ¡†æ¶ï¼Œæ–‡ç« å¯è§†åŒ–äº†ä»ä½“åŠ›ç”Ÿäº§åŠ›å‘è®¤çŸ¥ç”Ÿäº§åŠ› (cognitive productivity) çš„è½¬å˜ï¼Œè®ºè¯äº† AI ä½œä¸ºè®¤çŸ¥å¼•æ“åœ¨å˜é©çŸ¥è¯†ä½“ç³»æ–¹é¢å…·æœ‰ä¸è¯­è¨€é©å‘½ç›¸å½“çš„åœ°ä½ã€‚ç ”ç©¶è¿˜æ¢è®¨äº†è¿™ä¸€é©å‘½å¯¹æŠ€èƒ½é‡æ–°å®šä¹‰ã€ç»„ç»‡æ¶æ„å’Œæ”¿ç­–åˆ¶å®šçš„æ·±è¿œå½±å“ã€‚æœ€ç»ˆæŒ‡å‡ºï¼ŒAI çš„æ ¸å¿ƒä»·å€¼åœ¨äºè¡¥å……è€Œéå•çº¯æ›¿ä»£äººç±»è®¤çŸ¥èƒ½åŠ›ï¼Œå¼€å¯äº†å…¨çƒç”Ÿäº§åŠ›æ¼”è¿›çš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.10281v2",
      "published_date": "2025-06-12 01:43:54 UTC",
      "updated_date": "2025-07-10 09:52:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:29:52.600116+00:00"
    },
    {
      "arxiv_id": "2506.22445v1",
      "title": "Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security",
      "title_zh": "é¢å‘ä¿¡æ¯ç‰©ç†ç³»ç»Ÿå®‰å…¨çš„å±‚çº§å¼å¯¹æŠ—éŸ§æ€§å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Saad Alqithami"
      ],
      "abstract": "Cyber-Physical Systems play a critical role in the infrastructure of various sectors, including manufacturing, energy distribution, and autonomous transportation systems. However, their increasing connectivity renders them highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day attacks, against which traditional security methods like rule-based intrusion detection and single-agent reinforcement learning prove insufficient. To overcome these challenges, this paper introduces a novel Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework. HAMARL employs a hierarchical structure consisting of local agents dedicated to subsystem security and a global coordinator that oversees and optimizes comprehensive, system-wide defense strategies. Furthermore, the framework incorporates an adversarial training loop designed to simulate and anticipate evolving cyber threats, enabling proactive defense adaptation. Extensive experimental evaluations conducted on a simulated industrial IoT testbed indicate that HAMARL substantially outperforms traditional multi-agent reinforcement learning approaches, significantly improving attack detection accuracy, reducing response times, and ensuring operational continuity. The results underscore the effectiveness of combining hierarchical multi-agent coordination with adversarially-aware training to enhance the resilience and security of next-generation CPS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¿¡æ¯ç‰©ç†ç³»ç»Ÿ(Cyber-Physical Systems)åœ¨é¢å¯¹é€‚åº”æ€§åŠé›¶æ—¥æ”»å‡»(zero-day attacks)æ—¶ä¼ ç»Ÿé˜²å¾¡æ‰‹æ®µä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å±‚æ¬¡åŒ–å¯¹æŠ—éŸ§æ€§å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning, HAMARL)æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±è´Ÿè´£å­ç³»ç»Ÿå®‰å…¨çš„å±€éƒ¨æ™ºèƒ½ä½“(local agents)ä¸è´Ÿè´£ä¼˜åŒ–å…¨å±€é˜²å¾¡ç­–ç•¥çš„å…¨å±€åè°ƒå™¨(global coordinator)ç»„æˆï¼Œé€šè¿‡åˆ†å±‚ç»“æ„å®ç°å…¨ç³»ç»Ÿçš„ååŒé˜²å¾¡ã€‚æ­¤å¤–ï¼ŒHAMARLé›†æˆäº†å¯¹æŠ—è®­ç»ƒ(adversarial training)å¾ªç¯ä»¥æ¨¡æ‹Ÿå’Œé¢„æµ‹æ¼”åŒ–ä¸­çš„ç½‘ç»œå¨èƒï¼Œä½¿é˜²å¾¡æœºåˆ¶å…·å¤‡ä¸»åŠ¨é€‚åº”èƒ½åŠ›ã€‚åœ¨æ¨¡æ‹Ÿå·¥ä¸šç‰©è”ç½‘(IoT)æµ‹è¯•å¹³å°ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHAMARLåœ¨æ”»å‡»æ£€æµ‹å‡†ç¡®ç‡ã€å“åº”æ—¶é—´åŠä¸šåŠ¡è¿ç»­æ€§ä¿éšœæ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)æ–¹æ³•ã€‚è¯¥æˆæœè¯æ˜äº†å±‚æ¬¡åŒ–å¤šæ™ºèƒ½ä½“åä½œä¸å¯¹æŠ—æ„ŸçŸ¥è®­ç»ƒç›¸ç»“åˆèƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºä¸‹ä¸€ä»£ä¿¡æ¯ç‰©ç†ç³»ç»Ÿçš„å®‰å…¨æ€§ä¸éŸ§æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.22445v1",
      "published_date": "2025-06-12 01:38:25 UTC",
      "updated_date": "2025-06-12 01:38:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:29:57.539083+00:00"
    },
    {
      "arxiv_id": "2506.10274v3",
      "title": "Discrete Audio Tokens: More Than a Survey!",
      "title_zh": "ç¦»æ•£éŸ³é¢‘ Tokenï¼šä¸ä»…ä»…æ˜¯ä¸€ç¯‡ç»¼è¿°ï¼",
      "authors": [
        "Pooneh Mousavi",
        "Gallil Maimon",
        "Adel Moumen",
        "Darius Petermann",
        "Jiatong Shi",
        "Haibin Wu",
        "Haici Yang",
        "Anastasia Kuznetsova",
        "Artem Ploujnikov",
        "Ricard Marxer",
        "Bhuvana Ramabhadran",
        "Benjamin Elizalde",
        "Loren Lugosch",
        "Jinyu Li",
        "Cem Subakan",
        "Phil Woodland",
        "Minje Kim",
        "Hung-yi Lee",
        "Shinji Watanabe",
        "Yossi Adi",
        "Mirco Ravanelli"
      ],
      "abstract": "Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks. They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¦»æ•£éŸ³é¢‘ Token (Discrete Audio Tokens) ä½œä¸ºç´§å‡‘è¡¨ç¤ºå½¢å¼åœ¨ä¿æŒæ„ŸçŸ¥è´¨é‡ä¸è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„ä½œç”¨ï¼Œå¹¶å¼ºè°ƒäº†å…¶åœ¨å°†è¯­éŸ³ä¸éŸ³é¢‘é›†æˆè‡³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä¸­çš„å…³é”®æ„ä¹‰ã€‚é’ˆå¯¹ç›®å‰ç ”ç©¶ç¼ºä¹ç»Ÿä¸€å¯¹æ¯”çš„é—®é¢˜ï¼Œæœ¬æ–‡å¯¹æ¶µç›–è¯­éŸ³ã€éŸ³ä¹å’Œé€šç”¨éŸ³é¢‘é¢†åŸŸçš„ç¦»æ•£éŸ³é¢‘åˆ†è¯å™¨ (Discrete Audio Tokenizers) è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„å›é¡¾ä¸åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶æå‡ºäº†ä¸€å¥—åŸºäºç¼–ç å™¨-è§£ç å™¨æ¶æ„ (Encoder-Decoder)ã€é‡åŒ–æŠ€æœ¯ (Quantization Techniques)ã€è®­ç»ƒèŒƒå¼åŠæµå¼å¤„ç†èƒ½åŠ›ç­‰ç»´åº¦çš„åˆ†ç±»æ³• (Taxonomy)ï¼Œå¹¶åœ¨éŸ³é¢‘é‡å»º (Reconstruction)ã€ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°åŠå£°å­¦è¯­è¨€å»ºæ¨¡ (Acoustic Language Modeling) ç­‰å¤šä¸ªåŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚é€šè¿‡å—æ§çš„æ¶ˆèå®éªŒ (Ablation Studies)ï¼Œè¯¥ç ”ç©¶æ·±å…¥åˆ†æäº†ä¸åŒæŠ€æœ¯æ–¹æ¡ˆä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æ­ç¤ºäº†å½“å‰é¢†åŸŸçš„å±€é™æ€§ä¸å¼€æ”¾æ€§æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œä¸ä»…ä¸ºç†è§£å„åˆ†è¯å™¨çš„æ€§èƒ½å·®å¼‚æä¾›äº†ç»Ÿä¸€æ¡†æ¶ï¼Œä¹Ÿä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„è§è§£ä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10274v3",
      "published_date": "2025-06-12 01:35:43 UTC",
      "updated_date": "2025-09-27 12:26:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:30:02.107921+00:00"
    },
    {
      "arxiv_id": "2506.10268v1",
      "title": "Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models",
      "title_zh": "è¯­è¨€æ¨¡å‹æ˜¯å¦æ‹¥æœ‰è´å¶æ–¯å¤§è„‘ï¼Ÿè¾¨æå¤§è¯­è¨€æ¨¡å‹ä¸­çš„éšæœºä¸ç¡®å®šæ€§å†³ç­–æ¨¡å¼",
      "authors": [
        "Andrea Yaoyun Cui",
        "Pengfei Yu"
      ],
      "abstract": "Language models are essentially probability distributions over token sequences. Auto-regressive models generate sentences by iteratively computing and sampling from the distribution of the next token. This iterative sampling introduces stochasticity, leading to the assumption that language models make probabilistic decisions, similar to sampling from unknown distributions. Building on this assumption, prior research has used simulated Gibbs sampling, inspired by experiments designed to elicit human priors, to infer the priors of language models. In this paper, we revisit a critical question: Do language models possess Bayesian brains? Our findings show that under certain conditions, language models can exhibit near-deterministic decision-making, such as producing maximum likelihood estimations, even with a non-zero sampling temperature. This challenges the sampling assumption and undermines previous methods for eliciting human-like priors. Furthermore, we demonstrate that without proper scrutiny, a system with deterministic behavior undergoing simulated Gibbs sampling can converge to a \"false prior.\" To address this, we propose a straightforward approach to distinguish between stochastic and deterministic decision patterns in Gibbs sampling, helping to prevent the inference of misleading language model priors. We experiment on a variety of large language models to identify their decision patterns under various circumstances. Our results provide key insights in understanding decision making of large language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)æ˜¯å¦å…·æœ‰è´å¶æ–¯å¤§è„‘(Bayesian Brains)ï¼Œå³å…¶å†³ç­–æ¨¡å¼ç©¶ç«Ÿæ˜¯åŸºäºéšæœºé‡‡æ ·è¿˜æ˜¯ç¡®å®šæ€§çš„ã€‚å°½ç®¡ä¸»æµè§‚ç‚¹è®¤ä¸ºè‡ªå›å½’æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ä½“ç°äº†æ¦‚ç‡å†³ç­–ï¼Œä½†æœ¬æ–‡å‘ç°åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œæ¨¡å‹å³ä¾¿åœ¨éé›¶æ¸©åº¦(temperature)è®¾ç½®ä¸‹ä¹Ÿä¼šè¡¨ç°å‡ºæ¥è¿‘ç¡®å®šæ€§çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡(Maximum Likelihood Estimation)è¡Œä¸ºã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„é‡‡æ ·å‡è®¾ï¼Œå¹¶è¯æ˜äº†åœ¨ç¡®å®šæ€§è¡Œä¸ºä¸‹è¿›è¡Œæ¨¡æ‹Ÿå‰å¸ƒæ–¯é‡‡æ ·(Gibbs sampling)å¯èƒ½å¯¼è‡´æ¨¡å‹æ”¶æ•›è‡³è™šå‡å…ˆéªŒ(false prior)ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŒºåˆ†éšæœºä¸ç¡®å®šæ€§å†³ç­–æ¨¡å¼çš„ç®€ä¾¿æ–¹æ³•ï¼Œæ—¨åœ¨é˜²æ­¢æ¨å¯¼å‡ºè¯¯å¯¼æ€§çš„æ¨¡å‹å…ˆéªŒã€‚é€šè¿‡åœ¨å¤šç§å¤§è¯­è¨€æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„å†³ç­–æ¨¡å¼ï¼Œä¸ºæ·±å…¥ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„å†³ç­–æœºåˆ¶æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10268v1",
      "published_date": "2025-06-12 01:23:22 UTC",
      "updated_date": "2025-06-12 01:23:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:30:03.707061+00:00"
    },
    {
      "arxiv_id": "2506.10264v1",
      "title": "WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models",
      "title_zh": "WGSR-Benchï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„åŸºäºå…µæ£‹æ¨æ¼”çš„åšå¼ˆè®ºç­–ç•¥æ¨ç†åŸºå‡†",
      "authors": [
        "Qiyue Yin",
        "Pei Xu",
        "Qiaozhe Li",
        "Shengda Liu",
        "Shengqi Shen",
        "Tong Wang",
        "Yihong Han",
        "Xiaonan Zhao",
        "Likun Yang",
        "Shiyue Cao",
        "Shiyu Qiu",
        "Yuxuan Liu",
        "Shizhao Yu",
        "Lei Cui",
        "Chengxin Yan",
        "Jie Sun",
        "Xiangquan Tang",
        "Kaiqi Huang"
      ],
      "abstract": "Recent breakthroughs in Large Language Models (LLMs) have led to a qualitative leap in artificial intelligence' s performance on reasoning tasks, particularly demonstrating remarkable capabilities in mathematical, symbolic, and commonsense reasoning. However, as a critical component of advanced human cognition, strategic reasoning, i.e., the ability to assess multi-agent behaviors in dynamic environments, formulate action plans, and adapt strategies, has yet to be systematically evaluated or modeled. To address this gap, this paper introduces WGSR-Bench, the first strategy reasoning benchmark for LLMs using wargame as its evaluation environment. Wargame, a quintessential high-complexity strategic scenario, integrates environmental uncertainty, adversarial dynamics, and non-unique strategic choices, making it an effective testbed for assessing LLMs' capabilities in multi-agent decision-making, intent inference, and counterfactual reasoning. WGSR-Bench designs test samples around three core tasks, i.e., Environmental situation awareness, Opponent risk modeling and Policy generation, which serve as the core S-POE architecture, to systematically assess main abilities of strategic reasoning. Finally, an LLM-based wargame agent is designed to integrate these parts for a comprehensive strategy reasoning assessment. With WGSR-Bench, we hope to assess the strengths and limitations of state-of-the-art LLMs in game-theoretic strategic reasoning and to advance research in large model-driven strategic intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WGSR-Benchï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºå…µæ£‹æ¨æ¼” (wargame) ç¯å¢ƒçš„ Large Language Models (LLMs) æˆ˜ç•¥æ¨ç†åŸºå‡†ï¼Œæ—¨åœ¨å¡«è¡¥æ¨¡å‹åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹åšå¼ˆæ¨ç†è¯„ä¼°çš„ç©ºç™½ã€‚å…µæ£‹æ¨æ¼”å› å…¶ç¯å¢ƒä¸ç¡®å®šæ€§ã€å¯¹æŠ—æ€§åŠ¨æ€å’Œéå”¯ä¸€æˆ˜ç•¥é€‰æ‹©çš„ç‰¹æ€§ï¼Œæˆä¸ºæµ‹è¯• LLMs å¤šæ™ºèƒ½ä½“å†³ç­–ã€æ„å›¾æ¨æ–­åŠåäº‹å®æ¨ç† (counterfactual reasoning) èƒ½åŠ›çš„ç†æƒ³å¹³å°ã€‚è¯¥åŸºå‡†å›´ç»•ç¯å¢ƒæ€åŠ¿æ„ŸçŸ¥ (Environmental situation awareness)ã€å¯¹æ‰‹é£é™©å»ºæ¨¡ (Opponent risk modeling) å’Œç­–ç•¥ç”Ÿæˆ (Policy generation) ä¸‰å¤§æ ¸å¿ƒä»»åŠ¡ï¼Œæ„å»ºäº†ç³»ç»Ÿçš„ S-POE æ¶æ„ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªåŸºäº LLM çš„å…µæ£‹æ™ºèƒ½ä½“ (wargame agent)ï¼Œå°†è¿™äº›æ¨¡å—æ•´åˆä»¥å®ç°å…¨é¢çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›è¯„ä¼°ã€‚é€šè¿‡ WGSR-Benchï¼Œè¯¥è®ºæ–‡æ·±å…¥åˆ†æäº†å½“å‰ä¸»æµ LLMs åœ¨åšå¼ˆè®ºæˆ˜ç•¥æ¨ç† (game-theoretic strategic reasoning) ä¸­çš„ä¼˜åŠ¿ä¸å±€é™ï¼Œä¸ºæ¨åŠ¨å¤§æ¨¡å‹é©±åŠ¨çš„æˆ˜ç•¥æ™ºèƒ½ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 17 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.10264v1",
      "published_date": "2025-06-12 01:16:34 UTC",
      "updated_date": "2025-06-12 01:16:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:30:06.846037+00:00"
    },
    {
      "arxiv_id": "2506.21561v2",
      "title": "Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs",
      "title_zh": "ä»…æœ‰æ¨ç†è¿˜ä¸å¤Ÿï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­çš„çœŸå®æ€§åè§ä¸è°„åªšç°è±¡ç ”ç©¶",
      "authors": [
        "Emilio Barkett",
        "Olivia Long",
        "Madhavendra Thakur"
      ],
      "abstract": "Despite their widespread use in fact-checking, moderation, and high-stakes decision-making, large language models (LLMs) remain poorly understood as judges of truth. This study presents the largest evaluation to date of LLMs' veracity detection capabilities and the first analysis of these capabilities in reasoning models. We had eight LLMs make 4,800 veracity judgments across several prompts, comparing reasoning and non-reasoning models. We find that rates of truth-bias, or the likelihood to believe a statement is true, regardless of whether it is actually true, are lower in reasoning models than in non-reasoning models, but still higher than human benchmarks. Most concerning, we identify sycophantic tendencies in several advanced models (o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an asymmetry in detection accuracy, performing well in truth accuracy but poorly in deception accuracy. This suggests that capability advances alone do not resolve fundamental veracity detection challenges in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºäº‹å®æ ¸æŸ¥è€…çš„çœŸå®æ€§æ£€æµ‹èƒ½åŠ›è¿›è¡Œäº†è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„è¯„ä¼°ï¼Œå¹¶é¦–æ¬¡å¯¹æ¯”åˆ†æäº†æ¨ç†æ¨¡å‹ä¸éæ¨ç†æ¨¡å‹çš„è¡¨ç°å·®å¼‚ã€‚é€šè¿‡å¯¹8ä¸ªæ¨¡å‹è¿›è¡Œçš„4,800æ¬¡çœŸå®æ€§åˆ¤æ–­æµ‹è¯•ï¼Œç ”ç©¶å‘ç°æ¨ç†æ¨¡å‹åœ¨Truth-biasï¼ˆå³æ— è®ºçœŸå‡å‡å€¾å‘äºç›¸ä¿¡é™ˆè¿°ä¸ºçœŸï¼‰æ–¹é¢çš„å‘ç”Ÿç‡è™½ä½äºéæ¨ç†æ¨¡å‹ï¼Œä½†ä»æ˜¾è‘—é«˜äºäººç±»åŸºå‡†ã€‚æœ€å€¼å¾—å…³æ³¨çš„æ˜¯ï¼Œç ”ç©¶åœ¨o4-miniã€GPT-4.1å’ŒR1ç­‰å…ˆè¿›æ¨ç†æ¨¡å‹ä¸­å‘ç°äº†è°„åªšå€¾å‘ï¼ˆSycophancyï¼‰ï¼Œè¡¨ç°ä¸ºåœ¨æ£€æµ‹çœŸå®ä¿¡æ¯æ—¶å‡†ç¡®ç‡è¾ƒé«˜ï¼Œä½†åœ¨è¯†åˆ«æ¬ºéª—æ€§ä¿¡æ¯æ—¶è¡¨ç°è¾ƒå·®ã€‚è¿™è¡¨æ˜å•çº¯çš„æ¨¡å‹èƒ½åŠ›æå‡å¹¶ä¸èƒ½è§£å†³LLMsåœ¨çœŸå®æ€§æ£€æµ‹ä¸­é¢ä¸´çš„æ ¹æœ¬æ€§æŒ‘æˆ˜ï¼Œå³ä¾¿å…·å¤‡å¼ºæ¨ç†èƒ½åŠ›çš„æ¨¡å‹åœ¨å……å½“çœŸç†è¯„åˆ¤è€…æ—¶ä»å­˜åœ¨æ˜¾è‘—çš„å¯¹ç§°æ€§ç¼ºé™·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published at the ICML 2025 Workshop on Models of Human Feedback for AI Alignment",
      "pdf_url": "https://arxiv.org/pdf/2506.21561v2",
      "published_date": "2025-06-12 00:19:36 UTC",
      "updated_date": "2025-09-28 13:57:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:30:13.469493+00:00"
    },
    {
      "arxiv_id": "2506.10249v2",
      "title": "Extended Creativity: A Conceptual Framework for Understanding Human-AI Creative Relations",
      "title_zh": "å»¶å±•åˆ›é€ åŠ›ï¼šç†è§£äººæœºåˆ›é€ æ€§å…³ç³»çš„æ¦‚å¿µæ¡†æ¶",
      "authors": [
        "Andrea Gaggioli",
        "Sabrina Bartolotta",
        "Andrea Ubaldi",
        "Katusha Gerardini",
        "Eleonora Diletta Sarcinella",
        "Alice Chirico"
      ],
      "abstract": "Artificial Intelligence holds significant potential to enhance human creativity. However, achieving this vision requires a clearer understanding of how such enhancement can be effectively realized. Drawing on a relational and distributed cognition perspective, we identify three fundamental modes by which AI can support and shape creative processes: Support, where AI acts as a tool; Synergy, where AI and humans collaborate in complementary ways; and Symbiosis, where human and AI cognition become so integrated that they form a unified creative system. These modes are defined along two key dimensions: the level of technical autonomy exhibited by the AI system (i.e., its ability to operate independently and make decisions without human intervention), and the degree of perceived agency attributed to it (i.e., the extent to which the AI is experienced as an intentional or creative partner). We examine how each configuration influences different levels of creativity from everyday problem solving to paradigm shifting innovation and discuss the implications for ethics, research, and the design of future human AI creative systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Extended Creativityæ¦‚å¿µæ¡†æ¶ï¼Œæ—¨åœ¨ä»å…³ç³»æ€§å’Œåˆ†å¸ƒå¼è®¤çŸ¥(distributed cognition)çš„è§†è§’ï¼Œæ·±å…¥æ¢è®¨äººå·¥æ™ºèƒ½å¦‚ä½•æœ‰æ•ˆå¢å¼ºäººç±»åˆ›é€ åŠ›ã€‚è¯¥æ¡†æ¶è¯†åˆ«äº†AIæ”¯æŒå’Œå¡‘é€ åˆ›ä½œè¿‡ç¨‹çš„ä¸‰ç§åŸºæœ¬æ¨¡å¼ï¼šSupport(ä½œä¸ºå·¥å…·)ã€Synergy(äº’è¡¥åä½œ)ä»¥åŠSymbiosis(äººæœºè®¤çŸ¥æ•´åˆä¸ºç»Ÿä¸€ç³»ç»Ÿ)ã€‚è¿™äº›æ¨¡å¼åŸºäºAIç³»ç»Ÿçš„æŠ€æœ¯è‡ªä¸»æ€§(technical autonomy)å’Œäººç±»æ„ŸçŸ¥çš„ä¸»ä½“æ€§(perceived agency)ä¸¤ä¸ªå…³é”®ç»´åº¦è¿›è¡Œå®šä¹‰ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†ä¸åŒé…ç½®å¦‚ä½•å½±å“ä»æ—¥å¸¸é—®é¢˜è§£å†³åˆ°èŒƒå¼è½¬æ¢åˆ›æ–°ç­‰ä¸åŒå±‚æ¬¡çš„åˆ›é€ åŠ›è¡¨ç°ã€‚æœ€åï¼Œè¯¥ç ”ç©¶æ¢è®¨äº†è¯¥æ¡†æ¶åœ¨ä¼¦ç†ã€å­¦æœ¯ç ”ç©¶åŠæœªæ¥äººæœºåˆ›æ„ç³»ç»Ÿè®¾è®¡æ–¹é¢çš„æ·±è¿œå½±å“ï¼Œä¸ºç†è§£äººæœºåˆ›é€ æ€§å…³ç³»æä¾›äº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "36 pages, 3 figures. This conceptual paper proposes a taxonomy of Extended Creativity systems and examines the relational dynamics between human and AI agents in creative processes. Suitable for readers in HCI, AI, cognitive science, and digital design. The illustrations were created by Francesco Giordano and are used with permission (not under CC license)",
      "pdf_url": "https://arxiv.org/pdf/2506.10249v2",
      "published_date": "2025-06-12 00:16:52 UTC",
      "updated_date": "2025-06-15 16:57:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T21:30:12.492306+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 168,
  "processed_papers_count": 168,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T21:31:03.747162+00:00"
}
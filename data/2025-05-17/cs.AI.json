{
  "date": "2025-05-17",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-05-17 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv åˆæ˜¯â€œç¥ä»™æ‰“æ¶â€çš„ä¸€å¤©ï¼Œ**ARC-AGI-2 åŸºå‡†çš„å‘å¸ƒ**æ— ç–‘æ˜¯é‡ç£…ç‚¸å¼¹ï¼Œå†æ¬¡å‘é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰å‘èµ·æŒ‘æˆ˜ï¼›ä¸æ­¤åŒæ—¶ï¼Œå­¦æœ¯ç•Œå¯¹ **Reasoning Modelsï¼ˆæ¨ç†æ¨¡å‹ï¼‰** çš„ç¥›é­…ä»åœ¨ç»§ç»­ï¼Œå¤šç¯‡è®ºæ–‡æŒ‡å‡ºå³ä¾¿æ˜¯ o1ã€DeepSeek-R1 ä¹Ÿä¼šåœ¨é€»è¾‘é—®é¢˜ä¸­äº§ç”Ÿä¸¥é‡çš„å¹»è§‰ï¼›æ­¤å¤–ï¼Œå¦‚ä½•è®© CoTï¼ˆæ€ç»´é“¾ï¼‰æ›´é«˜æ•ˆã€Agentï¼ˆæ™ºèƒ½ä½“ï¼‰å…·å¤‡ç»ˆèº«å­¦ä¹ èƒ½åŠ›ä¹Ÿæ˜¯ä»Šå¤©çš„çƒ­é—¨è®®é¢˜ã€‚\n\n---\n\n### ğŸš€ é‡ç£…å…³æ³¨ï¼šAGI åŸºå‡†ä¸æ¨ç†æ¨¡å‹çš„â€œç¥›é­…â€\n\n**92. ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems**\n**ARC-AGI-2ï¼šå‰æ²¿ AI æ¨ç†ç³»ç»Ÿçš„æ–°æŒ‘æˆ˜**\n> FranÃ§ois Chollet, Mike Knoop, et al.\n> **æ ¸å¿ƒç‚¹ï¼š** ARC-AGI å‡çº§ç‰ˆï¼Œæ›´ç»†ç²’åº¦çš„æµä½“æ™ºåŠ›è¯„ä¼°ã€‚\n> **TLDRï¼š** è‘—åçš„ ARC åŸºå‡†ï¼ˆç”± Keras ä¹‹çˆ¶ FranÃ§ois Chollet æå‡ºï¼‰è¿æ¥äº†æ›´æ–°ã€‚ARC-AGI-2 ä¿ç•™äº†è¾“å…¥-è¾“å‡ºå¯¹çš„æ ¼å¼ï¼Œä½†å¼•å…¥äº†æ›´ç»†ç²’åº¦çš„ä»»åŠ¡é›†ï¼Œæ—¨åœ¨è¯„ä¼°æ›´é«˜å±‚æ¬¡çš„æŠ½è±¡æ¨ç†å’Œæµä½“æ™ºåŠ›ã€‚äººç±»æµ‹è¯•è¡¨æ˜è¿™äº›ä»»åŠ¡å¯¹äººæ¥è¯´ä¾ç„¶å¯è§£ï¼Œä½†å¯¹å½“å‰çš„ AI ç³»ç»Ÿæå…·æŒ‘æˆ˜æ€§ã€‚è¿™æ˜¯è¡¡é‡ AGI è¿›å±•çš„æ ‡å°ºçº§å·¥ä½œã€‚\n\n**3. Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features**\n**æ¨ç†å¤§æ¨¡å‹çš„é”™è¯¯æºäºå¯¹å…³é”®é—®é¢˜ç‰¹å¾çš„å¹»è§‰**\n> Alex Heyman, Joel Zylberberg\n> **æ ¸å¿ƒç‚¹ï¼š** æ¨ç†æ¨¡å‹ï¼ˆo1, DeepSeek-R1ï¼‰ä¾ç„¶ä¼šçç¼–ã€‚\n> **TLDRï¼š** ä½œè€…æµ‹è¯•äº† o1-mini, DeepSeek-R1, Claude 3.7 ç­‰â€œæ¨ç†æ¨¡å‹â€ï¼ˆRLLMsï¼‰ï¼Œå‘ç°å®ƒä»¬åœ¨è§£å†³å›¾ç€è‰²å’Œçº¦æŸæ»¡è¶³é—®é¢˜æ—¶ï¼Œå€¾å‘äº**å¹»è§‰å‡º prompt ä¸­ä¸å­˜åœ¨çš„å›¾è¾¹ï¼ˆedgesï¼‰**ã€‚è¿™ç§â€œè¾“å…¥å†²çªå‹å¹»è§‰â€åœ¨ä¸åŒå¤æ‚åº¦ä¸‹éƒ½å­˜åœ¨ï¼Œè¯´æ˜ç›®å‰çš„ CoT å¼ºåŒ–å­¦ä¹ ç­–ç•¥å¹¶æ²¡æœ‰å®Œå…¨è§£å†³å¯¹é—®é¢˜æè¿°çš„å¿ å®åº¦é—®é¢˜ã€‚\n\n**83. Evaluating the Logical Reasoning Abilities of Large Reasoning Models**\n**è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›**\n> Hanmeng Liu, et al.\n> **æ ¸å¿ƒç‚¹ï¼š** æ¨å‡º LogiEval åŸºå‡†ï¼Œå°æ¨¡å‹å¤±è´¥å¯é¢„æµ‹å¤§æ¨¡å‹å›°éš¾ã€‚\n> **TLDRï¼š** å°½ç®¡å¤§æ¨¡å‹åœ¨æ•°å­¦ä»£ç ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†çº¯é€»è¾‘æ¨ç†ï¼ˆæ¼”ç»ã€å½’çº³ã€æº¯å› ï¼‰èƒ½åŠ›å¦‚ä½•ï¼Ÿç ”ç©¶è€…æ¨å‡ºäº† LogiEvalã€‚å‘ç°ç°æœ‰æ¨¡å‹åœ¨ç±»æ¯”æ¨ç†ä¸Šç”šè‡³è¶…è¶Šäººç±»ï¼Œä½†åœ¨ä¸åŒé€»è¾‘æ ¼å¼ä¸Šè¡¨ç°å‚å·®ä¸é½ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä»–ä»¬å‘ç°å°æ¨¡å‹çš„å¤±è´¥æ¨¡å¼å¯ä»¥å¯é åœ°é¢„æµ‹å¤§æ¨¡å‹çš„å›°éš¾ç‚¹ã€‚\n\n---\n\n### ğŸ§  æ€ç»´é“¾ï¼ˆCoTï¼‰çš„æ•ˆç‡é©å‘½ï¼šæ‹’ç»æ— æ•ˆæ€è€ƒ\n\nä»Šå¤©æœ‰å¤šç¯‡è®ºæ–‡é›†ä¸­è®¨è®ºï¼š**ä¸æ˜¯æ‰€æœ‰é—®é¢˜éƒ½éœ€è¦é•¿æ€è€ƒï¼Œå¦‚ä½•åŠ¨æ€åˆ†é…æ¨ç†ç®—åŠ›ï¼Ÿ**\n\n**69. AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning**\n**AdaCoTï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¸•ç´¯æ‰˜æœ€ä¼˜è‡ªé€‚åº”æ€ç»´é“¾è§¦å‘**\n> Chenwei Lou, et al.\n> **TLDRï¼š** CoT è™½ç„¶å¥½ï¼Œä½†å¯¹ç®€å•é—®é¢˜æ˜¯æµªè´¹ã€‚AdaCoT æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆPPOï¼‰è®©æ¨¡å‹å­¦ä¼š**ä»€ä¹ˆæ—¶å€™è¯¥ç”¨ CoTï¼Œä»€ä¹ˆæ—¶å€™ç›´æ¥å›ç­”**ã€‚åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå¹³å‡å‡å°‘äº† 69% çš„ Token æ¶ˆè€—ã€‚\n\n**29. AdaBoN: Adaptive Best-of-N Alignment**\n**AdaBoNï¼šè‡ªé€‚åº”çš„ Best-of-N å¯¹é½**\n> Vinod Raman, et al.\n> **TLDRï¼š** é’ˆå¯¹ Best-of-Nï¼ˆBoNï¼‰é‡‡æ ·æ˜‚è´µçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ prompt è‡ªé€‚åº”ç­–ç•¥ã€‚å…ˆç”¨å°é¢„ç®—æ¢ç´¢æç¤ºçš„éš¾åº¦ï¼Œå†åŠ¨æ€åˆ†é…å‰©ä½™ç®—åŠ›ã€‚\n\n**96. Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning**\n**å¹¶éæ‰€æœ‰æ€è€ƒéƒ½ç”Ÿè€Œå¹³ç­‰ï¼šåŸºäºå¤šè½®å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆ LLM æ¨ç†**\n> Yansong Ning, et al.\n> **TLDRï¼š** æå‡ºäº† LongâŠ—Short æ¡†æ¶ï¼Œè®©ä¸¤ä¸ª LLM åä½œï¼šä¸€ä¸ªè´Ÿè´£ç”Ÿæˆâ€œé•¿æ€è€ƒâ€ï¼ˆå…³é”®æ­¥éª¤ï¼‰ï¼Œä¸€ä¸ªè´Ÿè´£â€œçŸ­æ€è€ƒâ€ï¼ˆå¸¸è§„æ­¥éª¤ï¼‰ï¼Œé€šè¿‡å¤šè½® RL ååŒï¼Œtoken é•¿åº¦å‡å°‘ 80% ä»¥ä¸Šä¸”æ€§èƒ½ä¸å‡ã€‚\n\n---\n\n### ğŸ¤– Agent ä¸å·¥å…·å­¦ä¹ ï¼šè¿ˆå‘æ›´å¤æ‚çš„ç³»ç»Ÿ\n\n**56. LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners**\n**LifelongAgentBenchï¼šè¯„ä¼° LLM æ™ºèƒ½ä½“ä½œä¸ºç»ˆèº«å­¦ä¹ è€…**\n> Junhao Zheng, et al.\n> **æ ¸å¿ƒç‚¹ï¼š** æ™ºèƒ½ä½“ä¸èƒ½æ€»æ˜¯â€œå¤±å¿†â€çš„ã€‚\n> **TLDRï¼š** ç°æœ‰çš„ Benchmark æŠŠ Agent å½“ä½œé™æ€ç³»ç»Ÿï¼Œä½†è¿™ç¯‡è®ºæ–‡æå‡ºäº†é¦–ä¸ªè¯„ä¼° Agent **ç»ˆèº«å­¦ä¹ èƒ½åŠ›**çš„åŸºå‡†ï¼ˆæ¶µç›–æ•°æ®åº“ã€æ“ä½œç³»ç»Ÿã€çŸ¥è¯†å›¾è°±ç¯å¢ƒï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œä¼ ç»Ÿçš„ç»éªŒå›æ”¾ï¼ˆExperience Replayï¼‰å¯¹ LLM Agent æ•ˆæœæœ‰é™ï¼Œä½œè€…æå‡ºäº†â€œç¾¤ä½“è‡ªæ´½æœºåˆ¶â€æ¥æå‡é•¿æœŸè®°å¿†åˆ©ç”¨ç‡ã€‚\n\n**91. ToLeaP: Rethinking Development of Tool Learning with Large Language Models**\n**ToLeaPï¼šåæ€å¤§æ¨¡å‹å·¥å…·å­¦ä¹ çš„å‘å±•**\n> Haotian Chen, et al.\n> **TLDRï¼š** è¿™æ˜¯ä¸€ä¸ªåŒ…å« 33 ä¸ªåŸºå‡†çš„å¤§å‹è¯„æµ‹å¹³å°ã€‚åˆ†æäº† 41 ä¸ª LLM åå‘ç°ï¼šç°æœ‰æ¨¡å‹åœ¨**è‡ªä¸»å­¦ä¹ **ã€**æ³›åŒ–èƒ½åŠ›**å’Œ**é•¿ç¨‹ä»»åŠ¡**è§£å†³ä¸Šä¾ç„¶å­˜åœ¨å·¨å¤§çŸ­æ¿ã€‚\n\n**99. HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems**\n**HALOï¼šå¤šæ™ºèƒ½ä½“ LLM ç³»ç»Ÿçš„åˆ†å±‚è‡ªä¸»é€»è¾‘å¯¼å‘ç¼–æ’**\n> Zhipeng Hou, et al.\n> **TLDRï¼š** é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰è§’è‰²åƒµåŒ–çš„é—®é¢˜ï¼Œæå‡ºåˆ†å±‚æ¶æ„ï¼šé«˜å±‚è§„åˆ’ã€ä¸­å±‚è§’è‰²è®¾è®¡ã€åº•å±‚æ‰§è¡Œã€‚åº•å±‚æ‰§è¡Œè¢«é‡æ„ä¸ºå·¥ä½œæµæœç´¢é—®é¢˜ï¼ˆåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ MCTSï¼‰ï¼Œåœ¨ MATH å’Œ HumanEval ä¸Šæå‡æ˜¾è‘—ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸è§†é¢‘ç”Ÿæˆï¼šæ›´å¿«ã€æ›´ç¨³ã€æ›´å®‰å…¨\n\n**28. VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption**\n**VFRTokï¼šåŸºäºæ—¶é•¿æ¯”ä¾‹ä¿¡æ¯å‡è®¾çš„å¯å˜å¸§ç‡è§†é¢‘ Tokenizer**\n> Tianxiong Zhong, et al. (KwaiVGI)\n> **TLDRï¼š** ç°åœ¨çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ Token å¼€é”€éšå¸§æ•°çº¿æ€§å¢é•¿ï¼Œå¤ªæ…¢ã€‚ä½œè€…æå‡ºâ€œè§†é¢‘ä¿¡æ¯é‡ä¸æ—¶é•¿æˆæ­£æ¯”è€Œéå¸§æ•°â€ï¼Œè®¾è®¡äº† VFRTokã€‚ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œä»…ä½¿ç”¨ **1/8 çš„ Token** å°±èƒ½è¾¾åˆ° SOTA çš„é‡å»ºå’Œç”Ÿæˆè´¨é‡ã€‚\n\n**93. DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance**\n**DraftAttentionï¼šé€šè¿‡ä½åˆ†è¾¨ç‡æ³¨æ„åŠ›å¼•å¯¼å®ç°å¿«é€Ÿè§†é¢‘æ‰©æ•£**\n> Xuan Shen, et al.\n> **TLDRï¼š** è§†é¢‘ DiT æ¨¡å‹ä¸­ Attention å äº† 80% çš„è®¡ç®—é‡ã€‚æœ¬æ–‡æå‡ºä¸€ç§**å…è®­ç»ƒ**çš„åŠ é€Ÿæ–¹æ³•ï¼Œåˆ©ç”¨ä½åˆ†è¾¨ç‡çš„â€œè‰ç¨¿â€æ³¨æ„åŠ›å›¾æ¥å¼•å¯¼å…¨åˆ†è¾¨ç‡çš„ç¨€ç–è®¡ç®—ï¼Œåœ¨ GPU ä¸Šå®ç°äº† 1.75 å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚\n\n**94. VISTA: Mitigating Semantic Inertia in Video-LLMs via Training-Free Dynamic Chain-of-Thought Routing**\n**VISTAï¼šé€šè¿‡å…è®­ç»ƒåŠ¨æ€ CoT è·¯ç”±ç¼“è§£è§†é¢‘å¤§æ¨¡å‹ä¸­çš„è¯­ä¹‰æƒ¯æ€§**\n> Hongbo Jin, et al.\n> **TLDRï¼š** è§†é¢‘ LLM ç»å¸¸â€œççœ¼è¯´çè¯â€ï¼Œå¿½ç•¥è§†è§‰è¯æ®è€Œé¡ºä»è¯­è¨€å…ˆéªŒï¼ˆå³è¯­ä¹‰æƒ¯æ€§ï¼‰ã€‚VISTA æ¡†æ¶å¼ºåˆ¶æ¨¡å‹å°†éšå¼è§†è§‰ç‰¹å¾å…·è±¡åŒ–ä¸ºæ–‡æœ¬é”šç‚¹ï¼ŒåŠ¨æ€è°ƒæ•´æ¨ç†è·¯å¾„ï¼Œæœ‰æ•ˆç¼“è§£äº†è¿™ä¸€é—®é¢˜ã€‚\n\n---\n\n### ğŸ› ï¸ åŸºç¡€è®¾æ–½ã€æ•ˆç‡ä¸å®‰å…¨æ€§\n\n**100. LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades**\n**LoRASuiteï¼šè·¨å¤§æ¨¡å‹ç‰ˆæœ¬å‡çº§çš„é«˜æ•ˆ LoRA é€‚é…**\n> Yanan Li, et al.\n> **æ ¸å¿ƒç‚¹ï¼š** åŸºç¡€æ¨¡å‹å‡çº§äº†ï¼ˆæ¯”å¦‚ä» v1 åˆ° v2ï¼‰ï¼Œæ—§çš„ LoRA æƒé‡æ€ä¹ˆåŠï¼Ÿ\n> **TLDRï¼š** é‡è®­å¤ªè´µã€‚LoRASuite é€šè¿‡è®¡ç®—æ–°æ—§æ¨¡å‹çš„ä¼ é€’çŸ©é˜µï¼Œè‡ªåŠ¨åˆ†é…å±‚å’Œæ³¨æ„åŠ›å¤´ï¼Œåªéœ€è¦æå°è§„æ¨¡çš„å¾®è°ƒå°±èƒ½å¤ç”¨æ—§ LoRAï¼Œç”šè‡³æ¯”é‡æ–°å…¨é‡å¾®è°ƒæ•ˆæœè¿˜å¥½ã€‚\n\n**89. SplInterp: Improving our Understanding and Training of Sparse Autoencoders**\n**SplInterpï¼šæ”¹è¿›ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰çš„ç†è§£ä¸è®­ç»ƒ**\n> Jeremy Budd, et al.\n> **TLDRï¼š** ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æ˜¯ç›®å‰å¤§æ¨¡å‹å¯è§£é‡Šæ€§ç ”ç©¶ï¼ˆMechanistic Interpretabilityï¼‰çš„å½“çº¢ç‚¸å­é¸¡ã€‚æœ¬æ–‡ä»æ ·æ¡ç†è®ºï¼ˆSpline Theoryï¼‰è§’åº¦é‡æ–°å®¡è§† SAEï¼Œè¯æ˜äº†å®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ç§åˆ†æ®µä»¿å°„å˜æ¢ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç®—æ³• PAM-SGDï¼Œèƒ½æé«˜ä»£ç çš„ç¨€ç–æ€§ã€‚\n\n**25. dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching**\n**dLLM-Cacheï¼šé€šè¿‡è‡ªé€‚åº”ç¼“å­˜åŠ é€Ÿæ‰©æ•£å‹å¤§è¯­è¨€æ¨¡å‹**\n> Zhiyuan Liu, et al.\n> **TLDRï¼š** æ‰©æ•£å‹ LLMï¼ˆå¦‚ LLaDAï¼‰ç”Ÿæˆè´¨é‡å¥½ä½†æ¨ç†æ…¢ã€‚ä½œè€…å‘ç°æ‰©æ•£ç”Ÿæˆæ—¶ prompt æ˜¯é™æ€çš„ï¼Œresponse æ˜¯éƒ¨åˆ†åŠ¨æ€çš„ï¼Œäºæ˜¯è®¾è®¡äº†ä¸“é—¨çš„ç¼“å­˜æœºåˆ¶ï¼Œå®ç°äº†æœ€é«˜ 9.1 å€çš„åŠ é€Ÿã€‚\n\n**88. On Membership Inference Attacks in Knowledge Distillation**\n**å…³äºçŸ¥è¯†è’¸é¦ä¸­çš„æˆå‘˜æ¨ç†æ”»å‡»**\n> Ziyao Cui, et al.\n> **TLDRï¼š** å¸¸è¯†è®¤ä¸ºâ€œæŠŠå¤§æ¨¡å‹è’¸é¦ç»™å°æ¨¡å‹â€èƒ½ä¿æŠ¤éšç§ã€‚ä½†è¿™ç¯‡è®ºæ–‡æ‰“è„¸äº†ï¼šè’¸é¦åçš„å­¦ç”Ÿæ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹åè€Œæ›´å®¹æ˜“æ³„éœ²è®­ç»ƒæ•°æ®æˆå‘˜ä¿¡æ¯ï¼ˆMembership Inference Attacksï¼‰ï¼Œå› ä¸ºè’¸é¦è¿‡ç¨‹æ”¾å¤§äº†æ¨¡å‹å¯¹â€œæ˜“å—æ”»å‡»æ•°æ®ç‚¹â€çš„è¿‡åº¦è‡ªä¿¡ã€‚\n\n---\n\n### ğŸŒ å…¶ä»–æœ‰è¶£çš„ç ”ç©¶\n\n*   **[Science] 36. GeoMaNO:** å°† Mamba æ¶æ„å¼•å…¥ç¥ç»ç®—å­ï¼ˆNeural Operatorsï¼‰ï¼Œç”¨äºæ±‚è§£ PDEï¼ˆåå¾®åˆ†æ–¹ç¨‹ï¼‰ï¼Œåœ¨æµä½“åŠ›å­¦æ¨¡æ‹Ÿä¸­æ¯” Transformer ç±»æ–¹æ³•æ›´é«˜æ•ˆä¸”ç¬¦åˆå‡ ä½•ä¸¥æ ¼æ€§ã€‚\n*   **[Math] 114. HARDMath2:** å“ˆä½›ç ”ç©¶ç”Ÿè¯¾ç¨‹çº§åˆ«çš„åº”ç”¨æ•°å­¦åŸºå‡†ã€‚ç”±å­¦ç”Ÿæ„å»ºï¼Œè®¸å¤šå‰æ²¿æ¨¡å‹åœ¨æ­¤ç¿»è½¦ï¼Œå±•ç¤ºäº† LLM åœ¨å¤„ç†è¿‘ä¼¼è§£å’Œå¤æ‚åˆ†ææ—¶çš„çŸ­æ¿ã€‚\n*   **[Bio] 7. Llama-Affinity:** åŸºäº Llama 3 æ¶æ„æ”¹é€ çš„æŠ—ä½“-æŠ—åŸç»“åˆäº²å’ŒåŠ›é¢„æµ‹æ¨¡å‹ï¼Œå‡†ç¡®ç‡é«˜è¾¾ 96.4%ã€‚\n*   **[Fun] 18. Personalized Author Obfuscation:** åˆ©ç”¨ LLM æ¥éšè—ä½œè€…èº«ä»½ï¼ˆåæ–‡é£åˆ†æï¼‰ï¼Œå‘ç°æ•ˆæœå› äººè€Œå¼‚ï¼Œæå‡ºäº†ä¸ªæ€§åŒ–æ··æ·†ç­–ç•¥ã€‚\n\n---\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼å¦‚æœ‰å…´è¶£ï¼Œè¯·æŸ¥é˜…å¯¹åº”ç¼–å·çš„åŸæ–‡ã€‚ğŸ‘‹",
  "papers": [
    {
      "arxiv_id": "2505.13522v2",
      "title": "A Heuristic Algorithm Based on Beam Search and Iterated Local Search for the Maritime Inventory Routing Problem",
      "title_zh": "åŸºäºæŸæœç´¢ä¸è¿­ä»£å±€éƒ¨æœç´¢çš„æµ·è¿åº“å­˜è·¯å¾„é—®é¢˜å¯å‘å¼ç®—æ³•",
      "authors": [
        "Nathalie Sanghikian",
        "Rafael Meirelles",
        "Rafael Martinelli",
        "Anand Subramanian"
      ],
      "abstract": "Maritime Inventory Routing Problem (MIRP) plays a crucial role in the integration of global maritime commerce levels. However, there are still no well-established methodologies capable of efficiently solving large MIRP instances or their variants due to the high complexity of the problem. The adoption of exact methods, typically based on Mixed Integer Programming (MIP), for daily operations is nearly impractical due to the CPU time required, as planning must be executed multiple times while ensuring high-quality results within acceptable time limits. Non-MIP-based heuristics are less frequently applied due to the highly constrained nature of the problem, which makes even the construction of an effective initial solution challenging. Papageorgiou et al. (2014) introduced a single-product MIRP as the foundation for MIRPLib, aiming to provide a collection of publicly available benchmark instances. However, only a few studies that propose new methodologies have been published since then. To encourage the use of MIRPLib and facilitate result comparisons, this study presents a heuristic approach that does not rely on mathematical optimization techniques to solve a deterministic, finite-horizon, single-product MIRP. The proposed heuristic combines a variation of a Beam Search algorithm with an Iterated Local Search procedure. Among the 72 instances tested, the developed methodology can improve the best-known solution for 19 instances within an acceptable CPU time.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµ·è¿åº“å­˜è·¯ç”±é—®é¢˜ (Maritime Inventory Routing Problem, MIRP) æå‡ºäº†ä¸€ç§åŸºäºå¯å‘å¼ç®—æ³•çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿçš„æ··åˆæ•´æ•°è§„åˆ’ (Mixed Integer Programming, MIP) åœ¨å¤„ç†å¤§è§„æ¨¡ç®—ä¾‹æ—¶è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚è¯¥ç®—æ³•ä¸ä¾èµ–äºæ•°å­¦ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ç»“åˆæŸæœç´¢ (Beam Search) ç®—æ³•çš„å˜ä½“ä¸è¿­ä»£å±€éƒ¨æœç´¢ (Iterated Local Search) è¿‡ç¨‹ï¼Œæœ‰æ•ˆå…‹æœäº† MIRP å› é«˜åº¦çº¦æŸå¯¼è‡´åˆå§‹è§£æ„å»ºå›°éš¾çš„æŒ‘æˆ˜ã€‚ç ”ç©¶äººå‘˜åœ¨ MIRPLib åŸºå‡†æ•°æ®é›†çš„ 72 ä¸ªç®—ä¾‹ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¯æ¥å—çš„ CPU æ—¶é—´å†…æ”¹è¿›äº†å…¶ä¸­ 19 ä¸ªç®—ä¾‹çš„å·²çŸ¥æœ€ä½³è§£ (best-known solution)ã€‚è¿™é¡¹å·¥ä½œä¸ºç¡®å®šæ€§ã€æœ‰é™æ—¶ç•Œçš„å•äº§å“ MIRP æä¾›äº†é«˜æ•ˆçš„æ±‚è§£æ¡†æ¶ï¼Œå¹¶ä¸ºåç»­ç ”ç©¶åœ¨ MIRPLib ä¸Šçš„ç»“æœæ¯”è¾ƒæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13522v2",
      "published_date": "2025-05-17 22:40:36 UTC",
      "updated_date": "2025-06-11 20:25:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:48:26.102163+00:00"
    },
    {
      "arxiv_id": "2505.12155v2",
      "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds",
      "title_zh": "SoftPQï¼šåŸºäºè½¯åŒ¹é…ä¸å¯è°ƒé˜ˆå€¼çš„é²æ£’å®ä¾‹åˆ†å‰²è¯„ä¼°",
      "authors": [
        "Ranit Karmakar",
        "Simon F. NÃ¸rrelykke"
      ],
      "abstract": "Segmentation evaluation metrics traditionally rely on binary decision logic: predictions are either correct or incorrect, based on rigid IoU thresholds. Detection--based metrics such as F1 and mAP determine correctness at the object level using fixed overlap cutoffs, while overlap--based metrics like Intersection over Union (IoU) and Dice operate at the pixel level, often overlooking instance--level structure. Panoptic Quality (PQ) attempts to unify detection and segmentation assessment, but it remains dependent on hard-threshold matching--treating predictions below the threshold as entirely incorrect. This binary framing obscures important distinctions between qualitatively different errors and fails to reward gradual model improvements. We propose SoftPQ, a flexible and interpretable instance segmentation metric that redefines evaluation as a graded continuum rather than a binary classification. SoftPQ introduces tunable upper and lower IoU thresholds to define a partial matching region and applies a sublinear penalty function to ambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit smoother score behavior, greater robustness to structural segmentation errors, and more informative feedback for model development and evaluation. Through controlled perturbation experiments, we show that SoftPQ captures meaningful differences in segmentation quality that existing metrics overlook, making it a practical and principled alternative for both benchmarking and iterative model refinement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SoftPQï¼Œä¸€ç§çµæ´»ä¸”å¯è§£é‡Šçš„å®ä¾‹åˆ†å‰²(instance segmentation)è¯„ä¼°æŒ‡æ ‡ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸæŒ‡æ ‡ï¼ˆå¦‚ PQ, mAPï¼‰ä¾èµ–ç¡¬é˜ˆå€¼(hard-threshold)äºŒå…ƒé€»è¾‘è€Œæ— æ³•æ•æ‰æ¨¡å‹æ¸è¿›å¼æ”¹è¿›çš„é—®é¢˜ã€‚SoftPQ å°†è¯„ä¼°è¿‡ç¨‹ä»äºŒå…ƒåˆ†ç±»é‡æ–°å®šä¹‰ä¸ºåˆ†çº§è¿ç»­ä½“ï¼Œé€šè¿‡å¼•å…¥å¯è°ƒçš„ä¸Šä¸‹ IoU é˜ˆå€¼æ¥å®šä¹‰éƒ¨åˆ†åŒ¹é…åŒºåŸŸã€‚è¯¥æŒ‡æ ‡è¿˜å¯¹æ¨¡ç³Šæˆ–ç¢è£‚çš„é¢„æµ‹åº”ç”¨äº†äºšçº¿æ€§æƒ©ç½šå‡½æ•°(sublinear penalty function)ï¼Œä»è€Œç¡®ä¿åœ¨å¤„ç†ç»“æ„æ€§åˆ†å‰²é”™è¯¯æ—¶å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§(robustness)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoftPQ è¡¨ç°å‡ºæ¯”ä¼ ç»ŸæŒ‡æ ‡æ›´å¹³æ»‘çš„åˆ†æ•°è¡Œä¸ºï¼Œèƒ½å¤Ÿæ•æ‰åˆ°ç°æœ‰åº¦é‡æ ‡å‡†å®¹æ˜“å¿½ç•¥çš„ç»†å¾®è´¨é‡å·®å¼‚ã€‚è¿™ä½¿å¾— SoftPQ æˆä¸ºåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹è¿­ä»£ä¼˜åŒ–ä¸­ä¸€ä¸ªæ›´å…·å®ç”¨æ€§å’ŒæŒ‡å¯¼æ„ä¹‰çš„è¯„ä¼°æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12155v2",
      "published_date": "2025-05-17 22:08:33 UTC",
      "updated_date": "2025-05-27 01:54:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:48:15.007830+00:00"
    },
    {
      "arxiv_id": "2505.12151v3",
      "title": "Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features",
      "title_zh": "æ¨ç†å‹å¤§è¯­è¨€æ¨¡å‹çš„é”™è¯¯æºäºå¯¹å…³é”®é—®é¢˜ç‰¹å¾çš„å¹»è§‰",
      "authors": [
        "Alex Heyman",
        "Joel Zylberberg"
      ],
      "abstract": "Large language models have recently made great strides in reasoning task performance through chain-of-thought (CoT) strategies trained via reinforcement learning; however, these \"reasoning large language models\" (RLLMs) remain imperfect reasoners, and understanding the frequencies and causes of their failure modes is important for both users and developers. We test o1-mini, o3-mini, DeepSeek-R1, Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and Grok 3 Mini Beta on graph coloring as a variable-complexity constraint-satisfaction logic problem, and find evidence from both error rate comparisons and CoT/explanation text analysis that RLLMs are prone to hallucinate graph edges not specified in the prompt. This phenomenon persists across multiple problem complexity levels and semantic frames, and it appears to account for a significant fraction of the incorrect answers from every tested model, and the vast majority of them for some models. We also validate the generalizability of this input-conflicting hallucination phenomenon with smaller-scale experiments on a type of stable matching problem. Our results indicate that RLLMs may possess broader issues with misrepresentation of problem specifics, and we offer suggestions for design choices to mitigate this weakness.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†æ¨ç†å¤§è¯­è¨€æ¨¡å‹ (Reasoning Large Language Models, RLLMs) åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­çš„å¤±è´¥æ¨¡å¼åŠå…¶æˆå› ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å›¾ç€è‰² (graph coloring) å’Œç¨³å®šåŒ¹é… (stable matching) ç­‰çº¦æŸæ»¡è¶³é€»è¾‘é—®é¢˜ï¼Œå¯¹ o1-miniã€o3-miniã€DeepSeek-R1ã€Claude 3.7 Sonnetã€Gemini 2.5 Pro Preview å’Œ Grok 3 Mini Beta è¿›è¡Œäº†è¯„ä¼°ã€‚é€šè¿‡å¯¹æ¯”é”™è¯¯ç‡å’Œåˆ†æ Chain-of-Thought æ–‡æœ¬å‘ç°ï¼ŒRLLMs ææ˜“åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¹»è§‰ (hallucinate) å‡º Prompt ä¸­å¹¶æœªæŒ‡å®šçš„å›¾è¾¹ (graph edges) ç­‰å…³é”®é—®é¢˜ç‰¹å¾ã€‚è¿™ç§è¾“å…¥å†²çªå‹å¹»è§‰ (input-conflicting hallucination) åœ¨ä¸åŒé—®é¢˜å¤æ‚åº¦å’Œè¯­ä¹‰æ¡†æ¶ä¸‹æŒç»­å­˜åœ¨ï¼Œè§£é‡Šäº†å—æµ‹æ¨¡å‹ä¸­ç»å¤§éƒ¨åˆ†çš„é”™è¯¯ç­”æ¡ˆã€‚ç ”ç©¶ç»“æœè¡¨æ˜ RLLMs åœ¨å¯¹é—®é¢˜ç»†èŠ‚çš„è¡¨å¾ä¸Šå­˜åœ¨å¹¿æ³›ç¼ºé™·ï¼Œå¹¶æ®æ­¤æå‡ºäº†å‡è½»è¯¥å¼±ç‚¹çš„è®¾è®¡å»ºè®®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages (9 excluding references and appendices); 9 figures (6 excluding appendices)",
      "pdf_url": "https://arxiv.org/pdf/2505.12151v3",
      "published_date": "2025-05-17 21:55:12 UTC",
      "updated_date": "2025-10-09 22:03:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:48:23.041613+00:00"
    },
    {
      "arxiv_id": "2505.12143v2",
      "title": "Structured Relational Representations",
      "title_zh": "ç»“æ„åŒ–å…³ç³»è¡¨å¾",
      "authors": [
        "Arun Kumar",
        "Paul Schrater"
      ],
      "abstract": "Invariant representations are core to representation learning, yet a central challenge remains: uncovering invariants that are stable and transferable without suppressing task-relevant signals. This raises fundamental questions, requiring further inquiry, about the appropriate level of abstraction at which such invariants should be defined and which aspects of a system they should characterize. Interpretation of the environment relies on abstract knowledge structures to make sense of the current state, which leads to interactions, essential drivers of learning and knowledge acquisition. Interpretation operates at the level of higher-order relational knowledge; hence, we propose that invariant structures must be where knowledge resides, specifically as partitions defined by the closure of relational paths within an abstract knowledge space. These partitions serve as the core invariant representations, forming the structural substrate where knowledge is stored and learning occurs. On the other hand, inter-partition connectors enable the deployment of these knowledge partitions encoding task-relevant transitions. Thus, invariant partitions provide the foundational primitives of structured representation. We formalize the computational foundations for structured relational representations of the invariant partitions based on closed semiring, a relational algebraic structure.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¡¨å¾å­¦ä¹ ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå³å¦‚ä½•æŒ–æ˜æ—¢ç¨³å®šå¯è¿ç§»åˆä¸æŠ‘åˆ¶ä»»åŠ¡ç›¸å…³ä¿¡å·çš„ä¸å˜è¡¨å¾(Invariant representations)ã€‚ä½œè€…æå‡ºä¸å˜ç»“æ„åº”å½“å®šä¹‰åœ¨é«˜é˜¶å…³ç³»çŸ¥è¯†å±‚é¢ï¼Œå…·ä½“è¡¨ç°ä¸ºæŠ½è±¡çŸ¥è¯†ç©ºé—´å†…ç”±å…³ç³»è·¯å¾„(relational paths)çš„é—­åŒ…æ‰€å®šä¹‰çš„åˆ’åˆ†(partitions)ã€‚è¿™äº›åˆ’åˆ†æ„æˆäº†çŸ¥è¯†å­˜å‚¨ä¸å­¦ä¹ å‘ç”Ÿçš„æ ¸å¿ƒä¸å˜è¡¨å¾ï¼Œè€Œåˆ’åˆ†é—´çš„è¿æ¥å™¨åˆ™è´Ÿè´£ç¼–ç å¹¶éƒ¨ç½²ä»»åŠ¡ç›¸å…³çš„è½¬æ¢ã€‚ç ”ç©¶è¿›ä¸€æ­¥åŸºäºå…³ç³»ä»£æ•°ç»“æ„ä¸­çš„é—­ç¯åŠç¯(closed semiring)ä¸ºè¿™ç§ç»“æ„åŒ–å…³ç³»è¡¨å¾(Structured Relational Representations)å¥ å®šäº†è®¡ç®—åŸºç¡€ã€‚è¯¥æ¡†æ¶å°†ä¸å˜åˆ’åˆ†è§†ä¸ºç»“æ„åŒ–è¡¨å¾çš„åŸºç¡€åŸè¯­ï¼Œä¸ºç†è§£ç¯å¢ƒä¸­çš„æŠ½è±¡çŸ¥è¯†ç»“æ„æä¾›äº†ä¸¥è°¨çš„æ•°å­¦æè¿°ä¸ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12143v2",
      "published_date": "2025-05-17 21:26:05 UTC",
      "updated_date": "2025-09-25 19:14:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:48:21.618518+00:00"
    },
    {
      "arxiv_id": "2505.12136v1",
      "title": "Lightweight Spatio-Temporal Attention Network with Graph Embedding and Rotational Position Encoding for Traffic Forecasting",
      "title_zh": "é¢å‘äº¤é€šé¢„æµ‹çš„ç»“åˆå›¾åµŒå…¥ä¸æ—‹è½¬ä½ç½®ç¼–ç çš„è½»é‡çº§æ—¶ç©ºæ³¨æ„åŠ›ç½‘ç»œ",
      "authors": [
        "Xiao Wang",
        "Shun-Ren Yang"
      ],
      "abstract": "Traffic forecasting is a key task in the field of Intelligent Transportation Systems. Recent research on traffic forecasting has mainly focused on combining graph neural networks (GNNs) with other models. However, GNNs only consider short-range spatial information. In this study, we present a novel model termed LSTAN-GERPE (Lightweight Spatio-Temporal Attention Network with Graph Embedding and Rotational Position Encoding). This model leverages both Temporal and Spatial Attention mechanisms to effectively capture long-range traffic dynamics. Additionally, the optimal frequency for rotational position encoding is determined through a grid search approach in both the spatial and temporal attention mechanisms. This systematic optimization enables the model to effectively capture complex traffic patterns. The model also enhances feature representation by incorporating geographical location maps into the spatio-temporal embeddings. Without extensive feature engineering, the proposed method in this paper achieves advanced accuracy on the real-world traffic forecasting datasets PeMS04 and PeMS08.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½äº¤é€šç³»ç»Ÿ(Intelligent Transportation Systems)ä¸­çš„äº¤é€šé¢„æµ‹ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLSTAN-GERPEçš„è½»é‡çº§æ—¶ç©ºæ³¨æ„åŠ›ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³å›¾ç¥ç»ç½‘ç»œ(GNNs)éš¾ä»¥æ•è·é•¿ç¨‹ç©ºé—´ä¿¡æ¯çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é›†æˆæ—¶é—´ä¸ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ä»¥æ•æ‰é•¿ç¨‹äº¤é€šåŠ¨æ€ï¼Œå¹¶åˆ©ç”¨ç½‘æ ¼æœç´¢(grid search)ç¡®å®šæ—‹è½¬ä½ç½®ç¼–ç (Rotational Position Encoding)çš„æœ€ä½³é¢‘ç‡ï¼Œä»è€Œæœ‰æ•ˆæå‡å¯¹å¤æ‚äº¤é€šæ¨¡å¼çš„å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLSTAN-GERPEé€šè¿‡å°†åœ°ç†ä½ç½®å›¾å¼•å…¥æ—¶ç©ºåµŒå…¥è¿›ä¸€æ­¥å¢å¼ºäº†ç‰¹å¾è¡¨ç¤ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ— éœ€å¤æ‚ç‰¹å¾å·¥ç¨‹çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æ•°æ®é›†PeMS04å’ŒPeMS08ä¸Šå‡è¾¾åˆ°äº†å…ˆè¿›çš„é¢„æµ‹ç²¾åº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12136v1",
      "published_date": "2025-05-17 20:36:20 UTC",
      "updated_date": "2025-05-17 20:36:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:48:36.569706+00:00"
    },
    {
      "arxiv_id": "2505.12135v1",
      "title": "LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs",
      "title_zh": "LLM-BABYBENCHï¼šç†è§£ä¸è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„å…·èº«è§„åˆ’ä¸æ¨ç†",
      "authors": [
        "Omar Choukrani",
        "Idriss Malek",
        "Daniil Orel",
        "Zhuohan Xie",
        "Zangir Iklassov",
        "Martin TakÃ¡Ä",
        "Salem Lahlou"
      ],
      "abstract": "Assessing the capacity of Large Language Models (LLMs) to plan and reason within the constraints of interactive environments is crucial for developing capable AI agents. We introduce $\\textbf{LLM-BabyBench}$, a new benchmark suite designed specifically for this purpose. Built upon a textual adaptation of the procedurally generated BabyAI grid world, this suite evaluates LLMs on three fundamental aspects of grounded intelligence: (1) predicting the consequences of actions on the environment state ($\\textbf{Predict}$ task), (2) generating sequences of low-level actions to achieve specified objectives ($\\textbf{Plan}$ task), and (3) decomposing high-level instructions into coherent subgoal sequences ($\\textbf{Decompose}$ task). We detail the methodology for generating the three corresponding datasets ($\\texttt{LLM-BabyBench-Predict}$, $\\texttt{-Plan}$, $\\texttt{-Decompose}$) by extracting structured information from an expert agent operating within the text-based environment. Furthermore, we provide a standardized evaluation harness and metrics, including environment interaction for validating generated plans, to facilitate reproducible assessment of diverse LLMs. Initial baseline results highlight the challenges posed by these grounded reasoning tasks. The benchmark suite, datasets, data generation code, and evaluation code are made publicly available ($\\href{https://github.com/choukrani/llm-babybench}{\\text{GitHub}}$, $\\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\\text{HuggingFace}}$).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† LLM-BabyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äº¤äº’å¼ç¯å¢ƒä¸­å…·èº«è§„åˆ’ä¸æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚è¯¥å¥—ä»¶åŸºäº BabyAI ç½‘æ ¼ä¸–ç•Œçš„æ–‡æœ¬é€‚é…ç‰ˆï¼Œé‡ç‚¹å…³æ³¨ Predictï¼ˆé¢„æµ‹è¡ŒåŠ¨åæœï¼‰ã€Planï¼ˆç”Ÿæˆæ“ä½œåºåˆ—ï¼‰å’Œ Decomposeï¼ˆåˆ†è§£é«˜çº§æŒ‡ä»¤ï¼‰è¿™ä¸‰ä¸ªå…·èº«æ™ºèƒ½çš„æ ¸å¿ƒç»´åº¦ã€‚é€šè¿‡ä»ä¸“å®¶æ™ºèƒ½ä½“ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œç ”ç©¶è€…æ„å»ºäº† LLM-BabyBench-Predictã€-Plan å’Œ -Decompose æ•°æ®é›†ï¼Œå¹¶é…å¤‡äº†æ ‡å‡†åŒ–çš„è¯„ä¼°å·¥å…·ä¸éªŒè¯æŒ‡æ ‡ã€‚åˆæ­¥å®éªŒç»“æœæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨æ‰§è¡Œæ­¤ç±»å…·èº«æ¨ç†ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œå‡¸æ˜¾äº†è¯¥ä»»åŠ¡çš„å¤æ‚æ€§ã€‚ç›®å‰ï¼Œè¯¥åŸºå‡†å¥—ä»¶åŠç›¸å…³èµ„æºå·²åœ¨ GitHub å’Œ HuggingFace å¼€æºï¼Œæ—¨åœ¨ä¸ºå¼€å‘æ›´å…ˆè¿›çš„ AI æ™ºèƒ½ä½“æä¾›æœ‰åŠ›æ”¯æŒä¸è¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12135v1",
      "published_date": "2025-05-17 20:23:17 UTC",
      "updated_date": "2025-05-17 20:23:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:48:48.082720+00:00"
    },
    {
      "arxiv_id": "2506.09052v1",
      "title": "Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture",
      "title_zh": "Llama-Affinityï¼šä¸€ç§æ•´åˆæŠ—ä½“åºåˆ—ä¸ Llama3 éª¨å¹²æ¶æ„çš„æŠ—ä½“-æŠ—åŸç»“åˆäº²å’ŒåŠ›é¢„æµ‹æ¨¡å‹",
      "authors": [
        "Delower Hossain",
        "Ehsan Saghapour",
        "Kevin Song",
        "Jake Y. Chen"
      ],
      "abstract": "Antibody-facilitated immune responses are central to the body's defense against pathogens, viruses, and other foreign invaders. The ability of antibodies to specifically bind and neutralize antigens is vital for maintaining immunity. Over the past few decades, bioengineering advancements have significantly accelerated therapeutic antibody development. These antibody-derived drugs have shown remarkable efficacy, particularly in treating cancer, SARS-CoV-2, autoimmune disorders, and infectious diseases. Traditionally, experimental methods for affinity measurement have been time-consuming and expensive. With the advent of artificial intelligence, in silico medicine has been revolutionized; recent developments in machine learning, particularly the use of large language models (LLMs) for representing antibodies, have opened up new avenues for AI-based design and improved affinity prediction. Herein, we present an advanced antibody-antigen binding affinity prediction model (LlamaAffinity), leveraging an open-source Llama 3 backbone and antibody sequence data sourced from the Observed Antibody Space (OAS) database. The proposed approach shows significant improvement over existing state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy) across multiple evaluation metrics. Specifically, the model achieved an accuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of 0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher computational efficiency, with a five-fold average cumulative training time of only 0.46 hours, significantly lower than in previous studies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LlamaAffinityï¼Œä¸€ç§é›†æˆ Llama 3 ä¸»å¹²æ¶æ„ä¸æŠ—ä½“åºåˆ—æ•°æ®çš„æŠ—ä½“-æŠ—åŸç»“åˆäº²å’ŒåŠ›é¢„æµ‹æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ¥è‡ª Observed Antibody Space (OAS) æ•°æ®åº“çš„åºåˆ—ä¿¡æ¯ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå®éªŒæ–¹æ³•æµ‹é‡äº²å’ŒåŠ›æ—¶æˆæœ¬é«˜ã€è€—æ—¶ä¹…çš„é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLlamaAffinity åœ¨å¤šé¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äº AntiFormerã€AntiBERTa å’Œ AntiBERTy ç­‰ç°æœ‰æœ€å…ˆè¿› (SOTA) æ–¹æ³•ï¼Œå…¶å‡†ç¡®ç‡è¾¾åˆ° 0.9640ï¼ŒAUC-ROC é«˜è¾¾ 0.9936ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºæé«˜çš„è®¡ç®—æ•ˆç‡ï¼Œäº”æŠ˜å¹³å‡ç´¯ç§¯è®­ç»ƒæ—¶é—´ä»…ä¸º 0.46 å°æ—¶ï¼Œè¿œä½äºæ­¤å‰çš„ç ”ç©¶ã€‚è¿™ä¸€æˆæœè¯æ˜äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œ AI é©±åŠ¨çš„æŠ—ä½“è®¾è®¡ä¸äº²å’ŒåŠ›é¢„æµ‹çš„é«˜æ•ˆæ€§ä¸å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "7 Pages",
      "pdf_url": "https://arxiv.org/pdf/2506.09052v1",
      "published_date": "2025-05-17 20:10:54 UTC",
      "updated_date": "2025-05-17 20:10:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:48:44.501646+00:00"
    },
    {
      "arxiv_id": "2505.12130v1",
      "title": "Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation",
      "title_zh": "ä»¥å…³é”®ç‚¹ä¸ºåŠ¨æ€è´¨å¿ƒçš„ç»Ÿä¸€äººä½“å§¿æ€ä¼°è®¡ä¸åˆ†å‰²",
      "authors": [
        "Niaz Ahmad",
        "Jawad Khan",
        "Kang G. Shin",
        "Youngmoon Lee",
        "Guanghui Wang"
      ],
      "abstract": "The dynamic movement of the human body presents a fundamental challenge for human pose estimation and body segmentation. State-of-the-art approaches primarily rely on combining keypoint heatmaps with segmentation masks but often struggle in scenarios involving overlapping joints or rapidly changing poses during instance-level segmentation. To address these limitations, we propose Keypoints as Dynamic Centroid (KDC), a new centroid-based representation for unified human pose estimation and instance-level segmentation. KDC adopts a bottom-up paradigm to generate keypoint heatmaps for both easily distinguishable and complex keypoints and improves keypoint detection and confidence scores by introducing KeyCentroids using a keypoint disk. It leverages high-confidence keypoints as dynamic centroids in the embedding space to generate MaskCentroids, allowing for swift clustering of pixels to specific human instances during rapid body movements in live environments. Our experimental evaluations on the CrowdPose, OCHuman, and COCO benchmarks demonstrate KDC's effectiveness and generalizability in challenging scenarios in terms of both accuracy and runtime performance. The implementation is available at: https://sites.google.com/view/niazahmad/projects/kdc.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Keypoints as Dynamic Centroid (KDC)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç»Ÿä¸€äººä½“å§¿æ€ä¼°è®¡ (human pose estimation) å’Œå®ä¾‹çº§åˆ†å‰² (instance-level segmentation) çš„æ–°å‹åŸºäºä¸­å¿ƒç‚¹çš„è¡¨å¾æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³äººä½“åŠ¨æ€è¿åŠ¨åŠé‡å å…³èŠ‚å¸¦æ¥çš„æŒ‘æˆ˜ã€‚KDC é‡‡ç”¨è‡ªåº•å‘ä¸Šçš„èŒƒå¼ç”Ÿæˆå…³é”®ç‚¹çƒ­å›¾ï¼Œå¹¶é€šè¿‡å¼•å…¥åŸºäº keypoint disk çš„ KeyCentroids æ˜¾è‘—æå‡äº†å…³é”®ç‚¹æ£€æµ‹çš„ç²¾åº¦å’Œç½®ä¿¡åº¦åˆ†æ•°ã€‚è¯¥æ–¹æ³•å°†é«˜ç½®ä¿¡åº¦çš„å…³é”®ç‚¹ä½œä¸ºåµŒå…¥ç©ºé—´ä¸­çš„åŠ¨æ€ä¸­å¿ƒç‚¹ (dynamic centroids) æ¥ç”Ÿæˆ MaskCentroidsï¼Œä»è€Œå®ç°åœ¨å®æ—¶ç¯å¢ƒçš„å‰§çƒˆè‚¢ä½“è¿åŠ¨ä¸­å°†åƒç´ å¿«é€Ÿèšç±»åˆ°ç‰¹å®šçš„äººä½“å®ä¾‹ã€‚åœ¨ CrowdPoseã€OCHuman å’Œ COCO åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯„ä¼°è¯æ˜äº† KDC åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®ç‡å’Œè¿è¡Œæ•ˆç‡æ–¹é¢å‡è¡¨ç°å‡ºè‰²ï¼Œä¸ºåŠ¨æ€åœºæ™¯ä¸‹çš„äººä½“åˆ†ææä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”ç¨³å¥çš„ç»Ÿä¸€æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12130v1",
      "published_date": "2025-05-17 20:05:34 UTC",
      "updated_date": "2025-05-17 20:05:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:48:53.645397+00:00"
    },
    {
      "arxiv_id": "2506.06298v1",
      "title": "Pairwise Calibrated Rewards for Pluralistic Alignment",
      "title_zh": "é¢å‘å¤šå…ƒå¯¹é½çš„æˆå¯¹æ ¡å‡†å¥–åŠ±",
      "authors": [
        "Daniel Halpern",
        "Evi Micha",
        "Ariel D. Procaccia",
        "Itai Shapira"
      ],
      "abstract": "Current alignment pipelines presume a single, universal notion of desirable behavior. However, human preferences often diverge across users, contexts, and cultures. As a result, disagreement collapses into the majority signal and minority perspectives are discounted. To address this, we propose reflecting diverse human preferences through a distribution over multiple reward functions, each inducing a distinct aligned policy. The distribution is learned directly from pairwise preference without annotator identifiers or predefined groups. Instead, annotator disagreements are treated as informative soft labels. Our central criterion is pairwise calibration: for every pair of candidate responses, the proportion of reward functions preferring one response matches the fraction of annotators with that preference. We prove that even a small outlier-free ensemble can accurately represent diverse preference distributions. Empirically, we introduce and validate a practical training heuristic to learn such ensembles, and demonstrate its effectiveness through improved calibration, implying a more faithful representation of pluralistic values.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¯¹é½æµç¨‹å¾€å¾€å¿½ç•¥äººç±»åå¥½å¤šæ ·æ€§ã€å¯¼è‡´å°‘æ•°ç¾¤ä½“è§‚ç‚¹è¢«æ©ç›–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åæ˜ å¤šå…ƒå¯¹é½(Pluralistic Alignment)çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ å¤šä¸ªå¥–åŠ±å‡½æ•°(Reward Functions)çš„åˆ†å¸ƒæ¥å¼•å¯¼ä¸åŒçš„å¯¹é½ç­–ç•¥ï¼Œç›´æ¥ä»æˆå¯¹åå¥½(Pairwise Preference)æ•°æ®ä¸­æå–ä¿¡æ¯ï¼Œè€Œæ— éœ€æ ‡æ³¨è€…æ ‡è¯†æˆ–é¢„å®šä¹‰åˆ†ç»„ã€‚ç ”ç©¶å°†æ ‡æ³¨è€…çš„æ„è§åˆ†æ­§è§†ä¸ºå…·æœ‰ä¿¡æ¯é‡çš„è½¯æ ‡ç­¾(Soft Labels)ï¼Œå¹¶å¼•å…¥æˆå¯¹æ ¡å‡†(Pairwise Calibration)ä½œä¸ºæ ¸å¿ƒå‡†åˆ™ï¼Œç¡®ä¿åå¥½æŸä¸€å“åº”çš„å¥–åŠ±å‡½æ•°æ¯”ä¾‹ä¸å®é™…æ ‡æ³¨è€…çš„åå¥½æ¯”ä¾‹ç›¸åŒ¹é…ã€‚ç†è®ºè¯æ˜è¡¨æ˜ï¼Œå³ä½¿æ˜¯è¾ƒå°çš„é›†æˆæ¨¡å‹(Ensemble)ä¹Ÿèƒ½å‡†ç¡®è¡¨ç¤ºå¤šæ ·åŒ–çš„åå¥½åˆ†å¸ƒã€‚å®éªŒé€šè¿‡ä¸€ç§å®ç”¨çš„è®­ç»ƒå¯å‘å¼æ–¹æ³•éªŒè¯äº†è¯¥æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æé«˜äº†æ ¡å‡†æ€§èƒ½ï¼Œä»è€Œå®ç°äº†å¯¹äººç±»ç¤¾ä¼šå¤šå…ƒä»·å€¼æ›´çœŸå®çš„å‘ˆç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06298v1",
      "published_date": "2025-05-17 18:38:24 UTC",
      "updated_date": "2025-05-17 18:38:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:48:49.652077+00:00"
    },
    {
      "arxiv_id": "2505.12109v2",
      "title": "SAINT: Attention-Based Policies for Discrete Combinatorial Action Spaces",
      "title_zh": "SAINTï¼šé’ˆå¯¹ç¦»æ•£ç»„åˆåŠ¨ä½œç©ºé—´çš„æ³¨æ„åŠ›æœºåˆ¶ç­–ç•¥",
      "authors": [
        "Matthew Landers",
        "Taylor W. Killian",
        "Thomas Hartvigsen",
        "Afsaneh Doryab"
      ],
      "abstract": "The combinatorial structure of many real-world action spaces leads to exponential growth in the number of possible actions, limiting the effectiveness of conventional reinforcement learning algorithms. Recent approaches for combinatorial action spaces impose factorized or sequential structures over sub-actions, failing to capture complex joint behavior. We introduce the Sub-Action Interaction Network using Transformers (SAINT), a novel policy architecture that represents multi-component actions as unordered sets and models their dependencies via self-attention conditioned on the global state. SAINT is permutation-invariant, sample-efficient, and compatible with standard policy optimization algorithms. In 20 distinct combinatorial environments across three task domains, including environments with nearly 17 million joint actions, SAINT consistently outperforms strong baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®ä¸–ç•Œä¸­åŠ¨ä½œç©ºé—´çš„ç»„åˆç»“æ„å¯¼è‡´çš„åŠ¨ä½œæ•°é‡æŒ‡æ•°çº§å¢é•¿é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç®—æ³•åŠç°æœ‰çš„åˆ†è§£æˆ–åºåˆ—åŒ–æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰å¤æ‚çš„å…³èŠ‚è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†SAINTï¼ˆSub-Action Interaction Network using Transformersï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºç¦»æ•£ç»„åˆåŠ¨ä½œç©ºé—´è®¾è®¡çš„æ–°å‹ç­–ç•¥æ¶æ„ã€‚SAINT å°†å¤šç»„ä»¶åŠ¨ä½œè¡¨ç¤ºä¸ºæ— åºé›†åˆï¼Œåˆ©ç”¨åŸºäºå…¨å±€çŠ¶æ€çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶(Self-attention)æ¥å»ºæ¨¡åŠ¨ä½œé—´çš„ä¾èµ–å…³ç³»ã€‚è¯¥æ¡†æ¶å…·æœ‰ç½®æ¢ä¸å˜æ€§(Permutation-invariant)å’Œæé«˜çš„æ ·æœ¬æ•ˆç‡ï¼Œä¸”èƒ½ä¸æ ‡å‡†çš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•æ— ç¼å…¼å®¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ¶µç›–ä¸‰ä¸ªä»»åŠ¡é¢†åŸŸçš„20ä¸ªä¸åŒç»„åˆç¯å¢ƒä¸­ï¼Œå³ä½¿åœ¨é¢å¯¹æ‹¥æœ‰è¿‘1700ä¸‡ä¸ªè”åˆåŠ¨ä½œçš„æç«¯æƒ…å†µæ—¶ï¼ŒSAINT çš„è¡¨ç°ä¹Ÿå§‹ç»ˆä¼˜äºç°æœ‰çš„å¼ºåŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12109v2",
      "published_date": "2025-05-17 18:34:31 UTC",
      "updated_date": "2026-01-07 22:33:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:49:00.407631+00:00"
    },
    {
      "arxiv_id": "2505.12108v2",
      "title": "EarthSynth: Generating Informative Earth Observation with Diffusion Models",
      "title_zh": "EarthSynthï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯Œå«ä¿¡æ¯çš„åœ°çƒè§‚æµ‹",
      "authors": [
        "Jiancheng Pan",
        "Shiye Lei",
        "Yuqian Fu",
        "Jiahao Li",
        "Yanxing Liu",
        "Yuze Sun",
        "Xiao He",
        "Long Peng",
        "Xiaomeng Huang",
        "Bo Zhao"
      ],
      "abstract": "Remote sensing image (RSI) interpretation typically faces challenges due to the scarcity of labeled data, which limits the performance of RSI interpretation tasks. To tackle this challenge, we propose EarthSynth, a diffusion-based generative foundation model that enables synthesizing multi-category, cross-satellite labeled Earth observation for downstream RSI interpretation tasks. To the best of our knowledge, EarthSynth is the first to explore multi-task generation for remote sensing, tackling the challenge of limited generalization in task-oriented synthesis for RSI interpretation. EarthSynth, trained on the EarthSynth-180K dataset, employs the Counterfactual Composition training strategy with a three-dimensional batch-sample selection mechanism to improve training data diversity and enhance category control. Furthermore, a rule-based method of R-Filter is proposed to filter more informative synthetic data for downstream tasks. We evaluate our EarthSynth on scene classification, object detection, and semantic segmentation in open-world scenarios. There are significant improvements in open-vocabulary understanding tasks, offering a practical solution for advancing RSI interpretation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EarthSynthï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹(Diffusion Models)çš„ç”Ÿæˆå¼åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é¥æ„Ÿå›¾åƒ(RSI)è§£è¯‘ä¸­æ ‡è®°æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚ä½œä¸ºé¦–ä¸ªæ¢ç´¢é¥æ„Ÿå¤šä»»åŠ¡ç”Ÿæˆçš„æ¡†æ¶ï¼ŒEarthSynthèƒ½å¤Ÿä¸ºä¸‹æ¸¸ä»»åŠ¡åˆæˆå¤šç±»åˆ«ã€è·¨å«æ˜Ÿçš„å¸¦æ ‡ç­¾åœ°çƒè§‚æµ‹æ•°æ®ã€‚è¯¥æ¨¡å‹åœ¨EarthSynth-180Kæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨äº†åäº‹å®åˆæˆ(Counterfactual Composition)è®­ç»ƒç­–ç•¥å’Œä¸‰ç»´æ‰¹æ ·æœ¬é€‰æ‹©æœºåˆ¶ï¼Œä»¥å¢å¼ºç±»åˆ«æ§åˆ¶å’Œæ•°æ®å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åŸºäºè§„åˆ™çš„R-Filteræ–¹æ³•ï¼Œç”¨äºä»åˆæˆæ•°æ®ä¸­ç­›é€‰å‡ºæ›´å…·ä¿¡æ¯é‡çš„æ ·æœ¬ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒEarthSynthåœ¨åœºæ™¯åˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰å¼€æ”¾ä¸–ç•Œä»»åŠ¡ä¸­å‡æœ‰æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨å¼€æ”¾è¯æ±‡(Open-vocabulary)ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ºæ¨è¿›é¥æ„Ÿå›¾åƒè§£è¯‘æŠ€æœ¯æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.12108v2",
      "published_date": "2025-05-17 18:27:15 UTC",
      "updated_date": "2025-08-07 10:33:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:49:27.276519+00:00"
    },
    {
      "arxiv_id": "2505.12107v1",
      "title": "Learning Probabilistic Temporal Logic Specifications for Stochastic Systems",
      "title_zh": "é¢å‘éšæœºç³»ç»Ÿçš„æ¦‚ç‡æ—¶åºé€»è¾‘è§„èŒƒå­¦ä¹ ",
      "authors": [
        "Rajarshi Roy",
        "Yash Pote",
        "David Parker",
        "Marta Kwiatkowska"
      ],
      "abstract": "There has been substantial progress in the inference of formal behavioural specifications from sample trajectories, for example, using Linear Temporal Logic (LTL). However, these techniques cannot handle specifications that correctly characterise systems with stochastic behaviour, which occur commonly in reinforcement learning and formal verification. We consider the passive learning problem of inferring a Boolean combination of probabilistic LTL (PLTL) formulas from a set of Markov chains, classified as either positive or negative. We propose a novel learning algorithm that infers concise PLTL specifications, leveraging grammar-based enumeration, search heuristics, probabilistic model checking and Boolean set-cover procedures. We demonstrate the effectiveness of our algorithm in two use cases: learning from policies induced by RL algorithms and learning from variants of a probabilistic model. In both cases, our method automatically and efficiently extracts PLTL specifications that succinctly characterise the temporal differences between the policies or model variants.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çº¿æ€§æ—¶åºé€»è¾‘(Linear Temporal Logic, LTL)æ— æ³•æœ‰æ•ˆåˆ»ç”»éšæœºç³»ç»Ÿ(stochastic systems)è¡Œä¸ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ¨æ–­æ¦‚ç‡çº¿æ€§æ—¶åºé€»è¾‘(Probabilistic LTL, PLTL)å…¬å¼å¸ƒå°”ç»„åˆçš„è¢«åŠ¨å­¦ä¹ ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡å¤„ç†æ ‡è®°ä¸ºæ­£æ ·æœ¬æˆ–è´Ÿæ ·æœ¬çš„é©¬å°”å¯å¤«é“¾(Markov chains)é›†åˆï¼Œåˆ©ç”¨åŸºäºè¯­æ³•çš„æšä¸¾ã€æœç´¢å¯å‘å¼ã€æ¦‚ç‡æ¨¡å‹æ£€æµ‹(probabilistic model checking)å’Œå¸ƒå°”é›†åˆè¦†ç›–ç¨‹åºæ¥æå–ç®€æ´çš„è§„èŒƒã€‚ç ”ç©¶é€šè¿‡ä»å¼ºåŒ–å­¦ä¹ (RL)ç­–ç•¥å’Œæ¦‚ç‡æ¨¡å‹å˜ä½“ä¸­å­¦ä¹ çš„ä¸¤ä¸ªç”¨ä¾‹è¯æ˜äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨ä¸”é«˜æ•ˆåœ°æå–PLTLè§„èŒƒï¼ŒæˆåŠŸè¡¨å¾äº†ä¸åŒç­–ç•¥æˆ–æ¨¡å‹å˜ä½“ä¹‹é—´çš„æ—¶åºå·®å¼‚ï¼Œä¸ºéšæœºç³»ç»Ÿçš„å½¢å¼åŒ–éªŒè¯å’Œè§„èŒƒæ¨æ–­æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.FL"
      ],
      "primary_category": "cs.LO",
      "comment": "Full version of the paper that appears in IJCAI'25",
      "pdf_url": "https://arxiv.org/pdf/2505.12107v1",
      "published_date": "2025-05-17 18:19:35 UTC",
      "updated_date": "2025-05-17 18:19:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:49:31.537183+00:00"
    },
    {
      "arxiv_id": "2505.12100v1",
      "title": "Improving Fairness in LLMs Through Testing-Time Adversaries",
      "title_zh": "é€šè¿‡æµ‹è¯•æ—¶å¯¹æŠ—æå‡å¤§è¯­è¨€æ¨¡å‹çš„å…¬å¹³æ€§",
      "authors": [
        "Isabela Pereira Gregio",
        "Ian Pons",
        "Anna Helena Reali Costa",
        "Artur JordÃ£o"
      ],
      "abstract": "Large Language Models (LLMs) push the bound-aries in natural language processing and generative AI, driving progress across various aspects of modern society. Unfortunately, the pervasive issue of bias in LLMs responses (i.e., predictions) poses a significant and open challenge, hindering their application in tasks involving ethical sensitivity and responsible decision-making. In this work, we propose a straightforward, user-friendly and practical method to mitigate such biases, enhancing the reliability and trustworthiness of LLMs. Our method creates multiple variations of a given sentence by modifying specific attributes and evaluates the corresponding prediction behavior compared to the original, unaltered, prediction/sentence. The idea behind this process is that critical ethical predictions often exhibit notable inconsistencies, indicating the presence of bias. Unlike previous approaches, our method relies solely on forward passes (i.e., testing-time adversaries), eliminating the need for training, fine-tuning, or prior knowledge of the training data distribution. Through extensive experiments on the popular Llama family, we demonstrate the effectiveness of our method in improving various fairness metrics, focusing on the reduction of disparities in how the model treats individuals from different racial groups. Specifically, using standard metrics, we improve the fairness in Llama3 in up to 27 percentage points. Overall, our approach significantly enhances fairness, equity, and reliability in LLM-generated results without parameter tuning or training data modifications, confirming its effectiveness in practical scenarios. We believe our work establishes an important step toward enabling the use of LLMs in tasks that require ethical considerations and responsible decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”Ÿæˆå›å¤æ—¶å­˜åœ¨çš„åè§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç®€å•ä¸”å®ç”¨çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨ä¼¦ç†æ•æ„Ÿä»»åŠ¡ä¸­çš„å¯é æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¿®æ”¹ç»™å®šå¥å­çš„ç‰¹å®šå±æ€§æ¥åˆ›å»ºå¤šä¸ªå˜ä½“ï¼Œå¹¶å°†å…¶é¢„æµ‹è¡Œä¸ºä¸åŸå§‹é¢„æµ‹è¿›è¡Œå¯¹æ¯”ï¼Œåˆ©ç”¨ä¼¦ç†é¢„æµ‹ä¸­çš„ä¸ä¸€è‡´æ€§æ¥è¯†åˆ«å¹¶ç¼“è§£åè§ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ¡ˆå®Œå…¨ä¾èµ–å‰å‘ä¼ æ’­(forward passes)ï¼Œå³é€šè¿‡æµ‹è¯•æ—¶å¯¹æŠ—(testing-time adversaries)å®ç°ï¼Œæ— éœ€å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€å¾®è°ƒæˆ–è·å–è®­ç»ƒæ•°æ®åˆ†å¸ƒã€‚åœ¨ Llama ç³»åˆ—æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡å°‘ä¸åŒç§æ—ç¾¤ä½“é—´çš„å¾…é‡å·®å¼‚æ–¹é¢ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ Llama3 ä¸Šçš„å…¬å¹³æ€§æŒ‡æ ‡æå‡äº†é«˜è¾¾ 27 ä¸ªç™¾åˆ†ç‚¹ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶åœ¨ä¸è°ƒæ•´å‚æ•°æˆ–ä¿®æ”¹è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å¢å¼ºäº† LLM ç”Ÿæˆç»“æœçš„å…¬å¹³æ€§ä¸å¯é æ€§ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹åœ¨è´Ÿè´£ä»»å†³ç­–é¢†åŸŸçš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12100v1",
      "published_date": "2025-05-17 17:56:53 UTC",
      "updated_date": "2025-05-17 17:56:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:49:28.151565+00:00"
    },
    {
      "arxiv_id": "2506.06297v1",
      "title": "Optimal patient allocation for echocardiographic assessments",
      "title_zh": "è¶…å£°å¿ƒåŠ¨å›¾è¯„ä¼°ä¸­çš„æ‚£è€…æœ€ä¼˜åˆ†é…",
      "authors": [
        "Bozhi Sun",
        "Seda Tierney",
        "Jeffrey A. Feinstein",
        "Frederick Damen",
        "Alison L. Marsden",
        "Daniele E. Schiavazzi"
      ],
      "abstract": "Scheduling echocardiographic exams in a hospital presents significant challenges due to non-deterministic factors (e.g., patient no-shows, patient arrival times, diverse exam durations, etc.) and asymmetric resource constraints between fetal and non-fetal patient streams. To address these challenges, we first conducted extensive pre-processing on one week of operational data from the Echo Laboratory at Stanford University's Lucile Packard Children's Hospital, to estimate patient no-show probabilities and derive empirical distributions of arrival times and exam durations. Based on these inputs, we developed a discrete-event stochastic simulation model using SimPy, and integrate it with the open source Gymnasium Python library. As a baseline for policy optimization, we developed a comparative framework to evaluate on-the-fly versus reservation-based allocation strategies, in which different proportions of resources are reserved in advance. Considering a hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2 ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation generally yields better performance, more effectively adapting to patient variability and resource constraints. Building on this foundation, we apply reinforcement learning (RL) to derive an approximated optimal dynamic allocation policy. This RL-based policy is benchmarked against the best-performing rule-based strategies, allowing us to quantify their differences and provide actionable insights for improving echo lab efficiency through intelligent, data-driven resource management.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»é™¢è¶…å£°å¿ƒåŠ¨å›¾(echocardiographic)æ£€æŸ¥æ’ç¨‹ä¸­å­˜åœ¨çš„æ‚£è€…å¤±çº¦ã€åˆ°è¾¾æ—¶é—´ä¸ç¡®å®šåŠèµ„æºåˆ†é…ä¸å¯¹ç§°ç­‰æŒ‘æˆ˜ï¼Œåˆ©ç”¨æ–¯å¦ç¦å¤§å­¦Lucile Packardå„¿ç«¥åŒ»é™¢çš„è¿è¥æ•°æ®æ¨å¯¼äº†å„é¡¹ä¸ç¡®å®šå› ç´ çš„ç»éªŒåˆ†å¸ƒã€‚ç ”ç©¶è€…ç»“åˆSimPyå’ŒGymnasiumå¼€å‘äº†ç¦»æ•£äº‹ä»¶éšæœºæ¨¡æ‹Ÿæ¨¡å‹(discrete-event stochastic simulation model)ï¼Œå¹¶å¯¹æ¯”è¯„ä¼°äº†å®æ—¶åˆ†é…(on-the-fly)ä¸é¢„çº¦åˆ†é…(reservation-based)ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç‰¹å®šçš„èµ„æºé…æ¯”ä¸‹ï¼Œå®æ—¶åˆ†é…ç­–ç•¥èƒ½æ›´æœ‰æ•ˆåœ°é€‚åº”æ‚£è€…å˜å¼‚æ€§å’Œèµ„æºé™åˆ¶ï¼Œå±•ç°å‡ºä¼˜äºé¢„çº¦åˆ†é…çš„æ€§èƒ½ã€‚éšåï¼Œç ”ç©¶åº”ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)æ¨å¯¼å‡ºè¿‘ä¼¼æœ€ä¼˜çš„åŠ¨æ€åˆ†é…ç­–ç•¥ï¼Œå¹¶å°†å…¶ä¸è¡¨ç°æœ€ä½³çš„è§„åˆ™ç­–ç•¥è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚è¯¥ç ”ç©¶é‡åŒ–äº†ä¸åŒç®¡ç†ç­–ç•¥çš„å·®å¼‚ï¼Œä¸ºé€šè¿‡æ™ºèƒ½åŒ–ã€æ•°æ®é©±åŠ¨çš„èµ„æºç®¡ç†æ‰‹æ®µä¼˜åŒ–åŒ»é™¢è¶…å£°å®éªŒå®¤çš„è¿è¡Œæ•ˆç‡æä¾›äº†ç§‘å­¦ä¾æ®å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06297v1",
      "published_date": "2025-05-17 17:51:23 UTC",
      "updated_date": "2025-05-17 17:51:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:49:28.778642+00:00"
    },
    {
      "arxiv_id": "2506.06296v1",
      "title": "Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets",
      "title_zh": "ç»“åˆ Jacobi Kolmogorov-Arnold ç½‘ç»œçš„åŠ¨æ€å›¾ CNN ä¸‰ç»´ç‚¹é›†åˆ†ç±»",
      "authors": [
        "Hanaa El Afia",
        "Said Ohamouddou",
        "Raddouane Chiheb",
        "Abdellatif El Afia"
      ],
      "abstract": "We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks (KAN) for the classification of three-dimensional point clouds. This method replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate polynomial expansions within a streamlined DGCNN architecture, circumventing deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi polynomials outperform the traditional linear layer-based DGCNN baseline in terms of accuracy and convergence speed, while maintaining parameter efficiency. Our results demonstrate that higher polynomial degrees do not automatically improve performance, highlighting the need for further theoretical and empirical investigation to fully understand the interactions between polynomial bases, degrees, and the mechanisms of graph-based learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Jacobi-KAN-DGCNNæ¡†æ¶ï¼Œé€šè¿‡å°†Dynamic Graph Convolutional Neural Network (DGCNN)ä¸Jacobi Kolmogorov-Arnold Networks (KAN)é›†æˆï¼Œå®ç°äº†å¯¹ä¸‰ç»´ç‚¹äº‘çš„é«˜æ•ˆåˆ†ç±»ã€‚è¯¥æ–¹æ³•åœ¨ç®€åŒ–çš„DGCNNæ¶æ„ä¸­åˆ©ç”¨å¯è‡ªé€‚åº”çš„å•å˜é‡å¤šé¡¹å¼å±•å¼€å–ä»£äº†ä¼ ç»Ÿçš„Multi-Layer Perceptron (MLP)å±‚ï¼Œå¹¶é€šè¿‡é€å±‚å¯¹æ¯”åˆ†æè§„é¿äº†æ·±åº¦å †å çš„å¤æ‚æ€§ã€‚åœ¨ModelNet40æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨Jacobiå¤šé¡¹å¼çš„KANå±‚åœ¨å‡†ç¡®ç‡å’Œæ”¶æ•›é€Ÿåº¦æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿçš„çº¿æ€§å±‚åŸºå‡†ï¼ŒåŒæ—¶ä¿æŒäº†æé«˜çš„å‚æ•°æ•ˆç‡(parameter efficiency)ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†è¾ƒé«˜çš„å¤šé¡¹å¼é˜¶æ•°å¹¶ä¸ä¸€å®šä¼šè‡ªåŠ¨æå‡æ¨¡å‹æ€§èƒ½ï¼Œå¼ºè°ƒäº†æ·±å…¥æ¢è®¨å¤šé¡¹å¼åŸºåº•ã€é˜¶æ•°ä¸å›¾å­¦ä¹ (graph-based learning)æœºåˆ¶ä¹‹é—´ç›¸äº’ä½œç”¨çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06296v1",
      "published_date": "2025-05-17 17:37:58 UTC",
      "updated_date": "2025-05-17 17:37:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:49:31.008287+00:00"
    },
    {
      "arxiv_id": "2505.12096v3",
      "title": "When Bias Helps Learning: Bridging Initial Prejudice and Trainability",
      "title_zh": "åå·®ä½•æ—¶åŠ©åŠ›å­¦ä¹ ï¼šè¿æ¥åˆå§‹åè§ä¸å¯è®­ç»ƒæ€§",
      "authors": [
        "Alberto Bassi",
        "Marco Baity-Jesi",
        "Aurelien Lucchi",
        "Carlo Albert",
        "Emanuele Francazi"
      ],
      "abstract": "Understanding the statistical properties of deep neural networks (DNNs) at initialization is crucial for elucidating both their trainability and the intrinsic architectural biases they encode prior to data exposure. Mean-field (MF) analyses have demonstrated that the parameter distribution in randomly initialized networks dictates whether gradients vanish or explode. Recent work has shown that untrained DNNs exhibit an initial-guessing bias (IGB), in which large regions of the input space are assigned to a single class. In this work, we provide a theoretical proof linking IGB to MF analyses, establishing that a network predisposition toward specific classes is intrinsically tied to the conditions for efficient learning. This connection leads to a counterintuitive conclusion: the initialization that optimizes trainability is systematically biased rather than neutral. We validate our theory through experiments across multiple architectures and datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)åœ¨åˆå§‹åŒ–é˜¶æ®µçš„ç»Ÿè®¡ç‰¹æ€§ï¼Œæ—¨åœ¨é˜æ˜å…¶å¯è®­ç»ƒæ€§(trainability)ä¸æ¶æ„åå¥½ã€‚é€šè¿‡ç†è®ºè¯æ˜ï¼Œä½œè€…å°†åˆå§‹çŒœæµ‹åå·®(Initial-guessing bias, IGB)ä¸å‡å€¼åœºåˆ†æ(Mean-field analyses, MF)è”ç³»èµ·æ¥ï¼Œå»ºç«‹äº†ç½‘ç»œå¯¹ç‰¹å®šç±»åˆ«çš„é¢„è®¾å€¾å‘ä¸é«˜æ•ˆå­¦ä¹ æ¡ä»¶ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚è¿™ä¸€ç»“è®ºæ­ç¤ºäº†ä¼˜åŒ–å¯è®­ç»ƒæ€§çš„åˆå§‹åŒ–çŠ¶æ€åœ¨ç³»ç»Ÿä¸Šæ˜¯å­˜åœ¨åå·®çš„ï¼Œè€Œéä¼ ç»Ÿè®¤çŸ¥çš„å®Œå…¨ä¸­ç«‹ã€‚é€šè¿‡åœ¨å¤šç§æ¶æ„å’Œæ•°æ®é›†ä¸Šçš„å®éªŒï¼Œè¯¥å·¥ä½œéªŒè¯äº†åå¥½åå·®ä¸å­¦ä¹ æ•ˆç‡ä¹‹é—´çš„æ­£ç›¸å…³æ€§ï¼Œä¸ºæ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ çš„åˆå§‹åŒ–æœºåˆ¶æä¾›äº†é‡è¦ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12096v3",
      "published_date": "2025-05-17 17:31:56 UTC",
      "updated_date": "2025-11-10 17:11:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:50:06.436795+00:00"
    },
    {
      "arxiv_id": "2505.12094v1",
      "title": "Attribution Projection Calculus: A Novel Framework for Causal Inference in Bayesian Networks",
      "title_zh": "å½’å› æŠ•å½±æ¼”ç®—ï¼šä¸€ç§è´å¶æ–¯ç½‘ç»œå› æœæ¨æ–­çš„æ–°å‹æ¡†æ¶",
      "authors": [
        "M Ruhul Amin"
      ],
      "abstract": "This paper introduces Attribution Projection Calculus (AP-Calculus), a novel mathematical framework for determining causal relationships in structured Bayesian networks. We investigate a specific network architecture with source nodes connected to destination nodes through intermediate nodes, where each input maps to a single label with maximum marginal probability. We prove that for each label, exactly one intermediate node acts as a deconfounder while others serve as confounders, enabling optimal attribution of features to their corresponding labels. The framework formalizes the dual nature of intermediate nodes as both confounders and deconfounders depending on the context, and establishes separation functions that maximize distinctions between intermediate representations. We demonstrate that the proposed network architecture is optimal for causal inference compared to alternative structures, including those based on Pearl's causal framework. AP-Calculus provides a comprehensive mathematical foundation for analyzing feature-label attributions, managing spurious correlations, quantifying information gain, ensuring fairness, and evaluating uncertainty in prediction models, including large language models. Theoretical verification shows that AP-Calculus not only extends but can also subsume traditional do-calculus for many practical applications, offering a more direct approach to causal inference in supervised learning contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Attribution Projection Calculus (AP-Calculus)ï¼Œè¿™æ˜¯ä¸€ç§åœ¨ç»“æ„åŒ– Bayesian Networks ä¸­ç¡®å®šå› æœå…³ç³»çš„æ–°å‹æ•°å­¦æ¡†æ¶ã€‚ç ”ç©¶é€šè¿‡åˆ†ææºèŠ‚ç‚¹ã€ä¸­é—´èŠ‚ç‚¹ä¸ç›®æ ‡èŠ‚ç‚¹ä¹‹é—´çš„ç‰¹å®šè¿æ¥æ¶æ„ï¼Œè¯æ˜äº†åœ¨æœ€å¤§åŒ–è¾¹é™…æ¦‚ç‡çš„æ˜ å°„ä¸‹ï¼Œæ¯ä¸ªæ ‡ç­¾ä»…ç”±ä¸€ä¸ªç‰¹å®šçš„ä¸­é—´èŠ‚ç‚¹ä½œä¸º deconfounderï¼Œè€Œå…¶ä»–èŠ‚ç‚¹åˆ™è§†ä¸º confoundersï¼Œä»è€Œå®ç°äº†ç‰¹å¾ä¸æ ‡ç­¾çš„æœ€ä¼˜å½’å› ã€‚è¯¥æ¡†æ¶æ­£å¼åŒ–äº†ä¸­é—´èŠ‚ç‚¹æ ¹æ®è¯­å¢ƒåœ¨æ··æ‚ä¸å»æ··æ‚è§’è‰²é—´çš„åŒé‡æ€§è´¨ï¼Œå¹¶ç¡®ç«‹äº†èƒ½å¤Ÿæœ€å¤§åŒ–ä¸­é—´è¡¨ç¤ºåŒºåˆ†åº¦çš„ separation functionsã€‚ä¸åŒ…æ‹¬ Pearl's causal framework åœ¨å†…çš„ä¼ ç»Ÿç»“æ„ç›¸æ¯”ï¼Œå®éªŒè¯æ˜è¯¥æ¶æ„åœ¨å› æœæ¨ç†æ–¹é¢æ›´å…·ä¼˜è¶Šæ€§ã€‚AP-Calculus ä¸ºå¤„ç† spurious correlationsã€é‡åŒ–ä¿¡æ¯å¢ç›Šã€ç¡®ä¿å…¬å¹³æ€§ä»¥åŠè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§æä¾›äº†åšå®çš„æ•°å­¦åŸºç¡€ã€‚ç†è®ºéªŒè¯è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç›‘ç£å­¦ä¹ è¯­å¢ƒä¸‹ä¸ä»…æ‰©å±•äº†ä¼ ç»Ÿçš„ do-calculusï¼Œåœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­ç”šè‡³å¯ä»¥å°†å…¶åŒ…å«åœ¨å†…ï¼Œä¸ºå› æœæ¨ç†æä¾›äº†ä¸€ç§æ›´ä¸ºç›´æ¥çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "*AI was used to improve Text and collecting Citations",
      "pdf_url": "https://arxiv.org/pdf/2505.12094v1",
      "published_date": "2025-05-17 17:29:13 UTC",
      "updated_date": "2025-05-17 17:29:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:49:43.978661+00:00"
    },
    {
      "arxiv_id": "2505.12090v1",
      "title": "Personalized Author Obfuscation with Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–ä½œè€…èº«ä»½æ··æ·†",
      "authors": [
        "Mohammad Shokri",
        "Sarah Ita Levitan",
        "Rivka Levitan"
      ],
      "abstract": "In this paper, we investigate the efficacy of large language models (LLMs) in obfuscating authorship by paraphrasing and altering writing styles. Rather than adopting a holistic approach that evaluates performance across the entire dataset, we focus on user-wise performance to analyze how obfuscation effectiveness varies across individual authors. While LLMs are generally effective, we observe a bimodal distribution of efficacy, with performance varying significantly across users. To address this, we propose a personalized prompting method that outperforms standard prompting techniques and partially mitigates the bimodality issue.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é€šè¿‡é‡Šä¹‰å’Œæ”¹å˜å†™ä½œé£æ ¼æ¥å®ç°ä½œè€…èº«ä»½æ¨¡ç³Š(Author Obfuscation)çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶é‡ç‚¹ä»ä¼ ç»Ÿçš„æ•´ä½“æ•°æ®é›†è¯„ä¼°è½¬å‘ç”¨æˆ·å±‚é¢çš„æ€§èƒ½åˆ†æï¼Œæ—¨åœ¨æ·±å…¥æ¢ç©¶æ¨¡ç³Šæ•ˆæœåœ¨ä¸åŒä½œè€…ä¸ªä½“ä¹‹é—´çš„å˜å¼‚æ€§ã€‚ç ”ç©¶å‘ç°LLMsåœ¨æ‰§è¡Œè¯¥ä»»åŠ¡æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„åŒå³°åˆ†å¸ƒ(Bimodal Distribution)ï¼Œå³å…¶åœ¨ä¸åŒç”¨æˆ·èº«ä¸Šçš„æœ‰æ•ˆæ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸ªæ€§åŒ–æç¤º(Personalized Prompting)æ–¹æ³•ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…ä¼˜äºæ ‡å‡†çš„æç¤ºæŠ€æœ¯ï¼Œè¿˜éƒ¨åˆ†ç¼“è§£äº†æœ‰æ•ˆæ€§åˆ†å¸ƒä¸å‡çš„é—®é¢˜ï¼Œä¸ºæå‡LLMåœ¨éšç§ä¿æŠ¤é¢†åŸŸçš„åº”ç”¨æ•ˆæœæä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12090v1",
      "published_date": "2025-05-17 17:10:25 UTC",
      "updated_date": "2025-05-17 17:10:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:49:50.990187+00:00"
    },
    {
      "arxiv_id": "2505.12089v1",
      "title": "NTIRE 2025 Challenge on Efficient Burst HDR and Restoration: Datasets, Methods, and Results",
      "title_zh": "NTIRE 2025 é«˜æ•ˆè¿æ‹ HDR ä¸ä¿®å¤æŒ‘æˆ˜èµ›ï¼šæ•°æ®é›†ã€æ–¹æ³•ä¸ç»“æœ",
      "authors": [
        "Sangmin Lee",
        "Eunpil Park",
        "Angel Canelo",
        "Hyunhee Park",
        "Youngjo Kim",
        "Hyung-Ju Chun",
        "Xin Jin",
        "Chongyi Li",
        "Chun-Le Guo",
        "Radu Timofte",
        "Qi Wu",
        "Tianheng Qiu",
        "Yuchun Dong",
        "Shenglin Ding",
        "Guanghua Pan",
        "Weiyu Zhou",
        "Tao Hu",
        "Yixu Feng",
        "Duwei Dai",
        "Yu Cao",
        "Peng Wu",
        "Wei Dong",
        "Yanning Zhang",
        "Qingsen Yan",
        "Simon J. Larsen",
        "Ruixuan Jiang",
        "Senyan Xu",
        "Xingbo Wang",
        "Xin Lu",
        "Marcos V. Conde",
        "Javier Abad-Hernandez",
        "Alvaro GarcÄ±a-Lara",
        "Daniel Feijoo",
        "Alvaro GarcÄ±a",
        "Zeyu Xiao",
        "Zhuoyuan Li"
      ],
      "abstract": "This paper reviews the NTIRE 2025 Efficient Burst HDR and Restoration Challenge, which aims to advance efficient multi-frame high dynamic range (HDR) and restoration techniques. The challenge is based on a novel RAW multi-frame fusion dataset, comprising nine noisy and misaligned RAW frames with various exposure levels per scene. Participants were tasked with developing solutions capable of effectively fusing these frames while adhering to strict efficiency constraints: fewer than 30 million model parameters and a computational budget under 4.0 trillion FLOPs. A total of 217 participants registered, with six teams finally submitting valid solutions. The top-performing approach achieved a PSNR of 43.22 dB, showcasing the potential of novel methods in this domain. This paper provides a comprehensive overview of the challenge, compares the proposed solutions, and serves as a valuable reference for researchers and practitioners in efficient burst HDR and restoration.",
      "tldr_zh": "è¯¥ç ”ç©¶ç»¼è¿°äº† NTIRE 2025 Efficient Burst HDR and Restoration æŒ‘æˆ˜èµ›ï¼Œæ—¨åœ¨æ¨åŠ¨é«˜æ•ˆå¤šå¸§é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰æˆåƒä¸ä¿®å¤æŠ€æœ¯çš„å‘å±•ã€‚æŒ‘æˆ˜èµ›åŸºäºä¸€ä¸ªæ–°å‹ RAW å¤šå¸§èåˆæ•°æ®é›†ï¼Œæ¯ä¸ªåœºæ™¯åŒ…å«ä¹å¸§å…·æœ‰ä¸åŒæ›å…‰æ°´å¹³ã€ä¸”å­˜åœ¨å™ªå£°å’Œæœªå¯¹é½é—®é¢˜çš„ RAW å¸§ã€‚å‚èµ›è€…éœ€åœ¨æ¨¡å‹å‚æ•°é‡å°‘äº 30 million ä¸”è®¡ç®—é¢„ç®—ä½äº 4.0 trillion FLOPs çš„ä¸¥æ ¼æ•ˆç‡çº¦æŸä¸‹ï¼Œå¼€å‘èƒ½å¤Ÿæœ‰æ•ˆèåˆè¿™äº›å¸§çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨ 217 åæ³¨å†Œå‚ä¸è€…ä¸­ï¼Œå…±æœ‰ 6 æ”¯å›¢é˜Ÿæäº¤äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œå…¶ä¸­è¡¨ç°æœ€ä½³çš„æ–¹æ³•å®ç°äº† 43.22 dB çš„ PSNRï¼Œå±•ç¤ºäº†è¯¥é¢†åŸŸæ–°æŠ€æœ¯çš„åº”ç”¨æ½œåŠ›ã€‚æœ¬æ–‡é€šè¿‡å…¨é¢æ¦‚è¿°æŒ‘æˆ˜èµ›æƒ…å†µå¹¶å¯¹æ¯”å„å›¢é˜Ÿæ–¹æ¡ˆï¼Œä¸ºé«˜æ•ˆ Burst HDR ä¸ä¿®å¤é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†å®è´µçš„å­¦æœ¯å‚è€ƒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12089v1",
      "published_date": "2025-05-17 17:10:22 UTC",
      "updated_date": "2025-05-17 17:10:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:49:56.470622+00:00"
    },
    {
      "arxiv_id": "2505.12079v1",
      "title": "SepPrune: Structured Pruning for Efficient Deep Speech Separation",
      "title_zh": "SepPruneï¼šé¢å‘é«˜æ•ˆæ·±åº¦è¯­éŸ³åˆ†ç¦»çš„ç»“æ„åŒ–å‰ªæ",
      "authors": [
        "Yuqi Li",
        "Kai Li",
        "Xin Yin",
        "Zhifei Yang",
        "Junhao Dong",
        "Zeyu Dong",
        "Chuanguang Yang",
        "Yingli Tian",
        "Yao Lu"
      ],
      "abstract": "Although deep learning has substantially advanced speech separation in recent years, most existing studies continue to prioritize separation quality while overlooking computational efficiency, an essential factor for low-latency speech processing in real-time applications. In this paper, we propose SepPrune, the first structured pruning framework specifically designed to compress deep speech separation models and reduce their computational cost. SepPrune begins by analyzing the computational structure of a given model to identify layers with the highest computational burden. It then introduces a differentiable masking strategy to enable gradient-driven channel selection. Based on the learned masks, SepPrune prunes redundant channels and fine-tunes the remaining parameters to recover performance. Extensive experiments demonstrate that this learnable pruning paradigm yields substantial advantages for channel pruning in speech separation models, outperforming existing methods. Notably, a model pruned with SepPrune can recover 85% of the performance of a pre-trained model (trained over hundreds of epochs) with only one epoch of fine-tuning, and achieves convergence 36$\\times$ faster than training from scratch. Code is available at https://github.com/itsnotacie/SepPrune.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SepPruneï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºæ·±åº¦è¯­éŸ³åˆ†ç¦»(speech separation)æ¨¡å‹å‹ç¼©è®¾è®¡çš„ç»“æ„åŒ–å‰ªæ(structured pruning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹è®¡ç®—æˆæœ¬é«˜ã€éš¾ä»¥æ»¡è¶³å®æ—¶å¤„ç†éœ€æ±‚çš„é—®é¢˜ã€‚SepPruneé€šè¿‡åˆ†ææ¨¡å‹çš„è®¡ç®—ç»“æ„è¯†åˆ«è®¡ç®—è´Ÿæ‹…æœ€é‡çš„å±‚ï¼Œå¹¶å¼•å…¥ä¸€ç§å¯å¾®åˆ†æ©ç ç­–ç•¥(differentiable masking strategy)æ¥å®ç°æ¢¯åº¦é©±åŠ¨çš„é€šé“é€‰æ‹©ã€‚åœ¨å‰ªé™¤å†—ä½™é€šé“åï¼Œè¯¥æ¡†æ¶ä¼šå¯¹å‰©ä½™å‚æ•°è¿›è¡Œå¾®è°ƒä»¥æ¢å¤æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSepPruneåœ¨è¯­éŸ³åˆ†ç¦»æ¨¡å‹çš„é€šé“å‰ªææ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å‰ªæåçš„æ¨¡å‹ä»…éœ€ä¸€ä¸ªå‘¨æœŸçš„å¾®è°ƒå³å¯æ¢å¤é¢„è®­ç»ƒæ¨¡å‹85%çš„æ€§èƒ½ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ¯”ä»å¤´è®­ç»ƒå¿«36å€ã€‚è¯¥æ¡†æ¶ä¸ºé«˜æ•ˆã€ä½å»¶è¿Ÿçš„è¯­éŸ³å¤„ç†æä¾›äº†å¯é çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12079v1",
      "published_date": "2025-05-17 16:44:38 UTC",
      "updated_date": "2025-05-17 16:44:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:49:56.012848+00:00"
    },
    {
      "arxiv_id": "2505.12069v1",
      "title": "MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples",
      "title_zh": "MT-CYP-Netï¼šæå°‘æ ·æœ¬æ¡ä»¶ä¸‹çš„åƒç´ çº§å†œä½œç‰©äº§é‡é¢„æµ‹å¤šä»»åŠ¡ç½‘ç»œ",
      "authors": [
        "Shenzhou Liu",
        "Di Wang",
        "Haonan Guo",
        "Chengxi Han",
        "Wenzhi Zeng"
      ],
      "abstract": "Accurate and fine-grained crop yield prediction plays a crucial role in advancing global agriculture. However, the accuracy of pixel-level yield estimation based on satellite remote sensing data has been constrained by the scarcity of ground truth data. To address this challenge, we propose a novel approach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This framework introduces an effective multi-task feature-sharing strategy, where features extracted from a shared backbone network are simultaneously utilized by both crop yield prediction decoders and crop classification decoders with the ability to fuse information between them. This design allows MT-CYP-Net to be trained with extremely sparse crop yield point labels and crop type labels, while still generating detailed pixel-level crop yield maps. Concretely, we collected 1,859 yield point labels along with corresponding crop type labels and satellite images from eight farms in Heilongjiang Province, China, in 2023, covering soybean, maize, and rice crops, and constructed a sparse crop yield label dataset. MT-CYP-Net is compared with three classical machine learning and deep learning benchmark methods in this dataset. Experimental results not only indicate the superiority of MT-CYP-Net compared to previous methods on multiple types of crops but also demonstrate the potential of deep networks on precise pixel-level crop yield prediction, especially with limited data labels.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MT-CYP-Netï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æå°‘æ ·æœ¬æƒ…å†µè®¾è®¡çš„åƒç´ çº§ä½œç‰©äº§é‡é¢„æµ‹å¤šä»»åŠ¡ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³åœ°é¢çœŸå®æ•°æ®ç¨€ç¼ºé™åˆ¶å«æ˜Ÿé¥æ„Ÿä¼°äº§ç²¾åº¦çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„å¤šä»»åŠ¡ç‰¹å¾å…±äº«ç­–ç•¥ï¼Œé€šè¿‡å…±äº«éª¨å¹²ç½‘ç»œ(backbone)åŒæ—¶æ”¯æŒä½œç‰©äº§é‡é¢„æµ‹å’Œåˆ†ç±»è§£ç å™¨ï¼Œå¹¶å®ç°ä¸¤è€…é—´çš„ä¿¡æ¯èåˆã€‚è¿™ç§è®¾è®¡ä½¿MT-CYP-Netèƒ½å¤Ÿåˆ©ç”¨æå…¶ç¨€ç–çš„äº§é‡ç‚¹æ ‡ç­¾å’Œç±»å‹æ ‡ç­¾è®­ç»ƒï¼Œè¿›è€Œç”Ÿæˆè¯¦å°½çš„åƒç´ çº§äº§é‡åˆ†å¸ƒå›¾ã€‚ç ”ç©¶å›¢é˜Ÿä½¿ç”¨2023å¹´ä¸­å›½é»‘é¾™æ±Ÿçœæ¶µç›–å¤§è±†ã€ç‰ç±³å’Œæ°´ç¨»çš„1,859ä¸ªæ ·ç‚¹æ•°æ®è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMT-CYP-Netåœ¨å¤šç§ä½œç‰©ä¸Šçš„é¢„æµ‹æ€§èƒ½å‡ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ åŠæ·±åº¦å­¦ä¹ åŸºå‡†æ¨¡å‹ã€‚è¯¥æˆæœæ˜¾è‘—å±•ç¤ºäº†æ·±åº¦å­¦ä¹ ç½‘ç»œåœ¨æœ‰é™æ ‡ç­¾æ•°æ®ä¸‹è¿›è¡Œç²¾ç¡®åƒç´ çº§ä½œç‰©äº§é‡é¢„æµ‹çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12069v1",
      "published_date": "2025-05-17 16:20:44 UTC",
      "updated_date": "2025-05-17 16:20:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:50:31.464372+00:00"
    },
    {
      "arxiv_id": "2507.19486v1",
      "title": "Confirmation bias: A challenge for scalable oversight",
      "title_zh": "ç¡®è®¤åå·®ï¼šå¯æ‰©å±•ç›‘ç£é¢ä¸´çš„æŒ‘æˆ˜",
      "authors": [
        "Gabriel Recchia",
        "Chatrik Singh Mangat",
        "Jinu Nyachhyon",
        "Mridul Sharma",
        "Callum Canavan",
        "Dylan Epstein-Gross",
        "Muhammed Abdulbari"
      ],
      "abstract": "Scalable oversight protocols aim to empower evaluators to accurately verify AI models more capable than themselves. However, human evaluators are subject to biases that can lead to systematic errors. We conduct two studies examining the performance of simple oversight protocols where evaluators know that the model is \"correct most of the time, but not all of the time\". We find no overall advantage for the tested protocols, although in Study 1, showing arguments in favor of both answers improves accuracy in cases where the model is incorrect. In Study 2, participants in both groups become more confident in the system's answers after conducting online research, even when those answers are incorrect. We also reanalyze data from prior work that was more optimistic about simple protocols, finding that human evaluators possessing knowledge absent from models likely contributed to their positive results--an advantage that diminishes as models continue to scale in capability. These findings underscore the importance of testing the degree to which oversight protocols are robust to evaluator biases, whether they outperform simple deference to the model under evaluation, and whether their performance scales with increasing problem difficulty and model capability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯æ‰©å±•ç›‘ç£ (Scalable oversight) åè®®åœ¨å¸®åŠ©äººç±»è¯„ä¼°å‘˜éªŒè¯è¶…è¶Šå…¶èƒ½åŠ›çš„ AI æ¨¡å‹æ—¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç¡®è®¤åè¯¯ (Confirmation bias) å¯¼è‡´çš„ç³»ç»Ÿæ€§é”™è¯¯ã€‚é€šè¿‡ä¸¤é¡¹é’ˆå¯¹ç®€å•ç›‘ç£åè®®çš„å®éªŒï¼Œç ”ç©¶å‘ç°åœ¨ Study 1 ä¸­å±•ç¤ºæ”¯æŒæ­£åä¸¤æ–¹ç­”æ¡ˆçš„è®ºç‚¹èƒ½æé«˜æ¨¡å‹å‡ºé”™æ—¶çš„è¯„ä¼°å‡†ç¡®ç‡ã€‚ç„¶è€Œåœ¨ Study 2 ä¸­ï¼Œå‚ä¸è€…åœ¨è¿›è¡Œåœ¨çº¿ç ”ç©¶åï¼Œå³ä½¿æ¨¡å‹ç»™å‡ºé”™è¯¯ç­”æ¡ˆï¼Œå…¶å¯¹ç³»ç»Ÿçš„ä¿¡å¿ƒåè€Œæœ‰æ‰€å¢åŠ ã€‚ç ”ç©¶é€šè¿‡é‡æ–°åˆ†æä»¥å¾€çš„ä¹è§‚æ•°æ®å‘ç°ï¼Œäººç±»è¯„ä¼°å‘˜æ—©æœŸçš„ä¼˜åŠ¿ä¸»è¦æºäºå…¶æ‹¥æœ‰æ¨¡å‹å°šæœªæŒæ¡çš„çŸ¥è¯†ï¼Œè€Œè¿™ç§ä¼˜åŠ¿æ­£éšç€æ¨¡å‹èƒ½åŠ›çš„è§„æ¨¡åŒ– (scale) è€Œé€æ¸å‡å¼±ã€‚è¯¥ç ”ç©¶æœ€ç»ˆå¼ºè°ƒäº†å¼€å‘é²æ£’ç›‘ç£åè®®çš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºæ­¤ç±»åè®®å¿…é¡»èƒ½å¤Ÿå…‹æœè¯„ä¼°å‘˜åè¯¯ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šéœ€ä¼˜äºå¯¹è¢«è¯„ä¼°æ¨¡å‹çš„ç®€å•é¡ºä» (deference)ï¼Œä»¥åº”å¯¹æ—¥ç›Šæå‡çš„ä»»åŠ¡éš¾åº¦ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "61 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.19486v1",
      "published_date": "2025-05-17 16:11:24 UTC",
      "updated_date": "2025-05-17 16:11:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:50:36.228895+00:00"
    },
    {
      "arxiv_id": "2505.12065v1",
      "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents",
      "title_zh": "æ­ç§˜ä¸æå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æœç´¢æ™ºèƒ½ä½“æ•ˆç‡",
      "authors": [
        "Tiannuo Yang",
        "Zebin Yao",
        "Bowen Jin",
        "Lixiao Cui",
        "Yusen Li",
        "Gang Wang",
        "Xiaoguang Liu"
      ],
      "abstract": "Large Language Model (LLM)-based search agents have shown remarkable capabilities in solving complex tasks by dynamically decomposing problems and addressing them through interleaved reasoning and retrieval. However, this interleaved paradigm introduces substantial efficiency bottlenecks. First, we observe that both highly accurate and overly approximate retrieval methods degrade system efficiency: exact search incurs significant retrieval overhead, while coarse retrieval requires additional reasoning steps during generation. Second, we identify inefficiencies in system design, including improper scheduling and frequent retrieval stalls, which lead to cascading latency -- where even minor delays in retrieval amplify end-to-end inference time. To address these challenges, we introduce SearchAgent-X, a high-efficiency inference framework for LLM-based search agents. SearchAgent-X leverages high-recall approximate retrieval and incorporates two key techniques: priority-aware scheduling and non-stall retrieval. Extensive experiments demonstrate that SearchAgent-X consistently outperforms state-of-the-art systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving up to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without compromising generation quality. SearchAgent-X is available at https://github.com/tiannuo-yang/SearchAgent-X.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æœç´¢æ™ºèƒ½ä½“åœ¨äº¤æ›¿æ¨ç†ä¸æ£€ç´¢è¿‡ç¨‹ä¸­é¢ä¸´çš„æ•ˆç‡ç“¶é¢ˆï¼ŒæŒ‡å‡ºç²¾ç¡®æ£€ç´¢çš„é«˜é¢å¼€é”€ä¸ç²—ç•¥æ£€ç´¢å¯¼è‡´çš„é¢å¤–æ¨ç†æ­¥éª¤å…±åŒåˆ¶çº¦äº†ç³»ç»Ÿæ€§èƒ½ã€‚åŒæ—¶ï¼Œç ”ç©¶è¯†åˆ«å‡ºç³»ç»Ÿè®¾è®¡ä¸­ä¸å½“çš„è°ƒåº¦å’Œé¢‘ç¹çš„æ£€ç´¢åœé¡¿(retrieval stalls)ä¼šå¼•å‘çº§è”å»¶è¿Ÿï¼Œæ˜¾è‘—å¢åŠ ç«¯åˆ°ç«¯æ¨ç†æ—¶é—´ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†SearchAgent-Xï¼Œä¸€ä¸ªä¸“ä¸ºæœç´¢æ™ºèƒ½ä½“è®¾è®¡çš„é«˜æ•ˆæ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é«˜å¬å›ç‡çš„è¿‘ä¼¼æ£€ç´¢(high-recall approximate retrieval)ï¼Œå¹¶é›†æˆäº†ä¼˜å…ˆçº§æ„ŸçŸ¥è°ƒåº¦(priority-aware scheduling)å’Œéåœé¡¿æ£€ç´¢(non-stall retrieval)ä¸¤é¡¹å…³é”®æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSearchAgent-Xåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºvLLMå’ŒåŸºäºHNSWçš„æ£€ç´¢ç³»ç»Ÿï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œå®ç°äº†æœ€é«˜3.4å€çš„ååé‡(throughput)æå‡å’Œ5å€çš„å»¶è¿Ÿ(latency)é™ä½ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12065v1",
      "published_date": "2025-05-17 16:07:01 UTC",
      "updated_date": "2025-05-17 16:07:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:50:39.964320+00:00"
    },
    {
      "arxiv_id": "2505.15840v2",
      "title": "TDFormer: A Top-Down Attention-Controlled Spiking Transformer",
      "title_zh": "TDFormerï¼šè‡ªé¡¶å‘ä¸‹æ³¨æ„åŠ›æ§åˆ¶çš„è„‰å†² Transformer",
      "authors": [
        "Zizheng Zhu",
        "Yingchao Yu",
        "Zeqi Zheng",
        "Zhaofei Yu",
        "Yaochu Jin"
      ],
      "abstract": "Traditional spiking neural networks (SNNs) can be viewed as a combination of multiple subnetworks with each running for one time step, where the parameters are shared, and the membrane potential serves as the only information link between them. However, the implicit nature of the membrane potential limits its ability to effectively represent temporal information. As a result, each time step cannot fully leverage information from previous time steps, seriously limiting the model's performance. Inspired by the top-down mechanism in the brain, we introduce TDFormer, a novel model with a top-down feedback structure that functions hierarchically and leverages high-order representations from earlier time steps to modulate the processing of low-order information at later stages. The feedback structure plays a role from two perspectives: 1) During forward propagation, our model increases the mutual information across time steps, indicating that richer temporal information is being transmitted and integrated in different time steps. 2) During backward propagation, we theoretically prove that the feedback structure alleviates the problem of vanishing gradients along the time dimension. We find that these mechanisms together significantly and consistently improve the model performance on multiple datasets. In particular, our model achieves state-of-the-art performance on ImageNet with an accuracy of 86.83%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿ Spiking Neural Networks (SNNs) ä¸­ membrane potential éšå¼æ€§è´¨å¯¼è‡´æ—¶é—´ä¿¡æ¯è¡¨å¾å—é™ã€å„æ—¶é—´æ­¥é—´ä¿¡æ¯åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† TDFormer æ¨¡å‹ã€‚å—å¤§è„‘ top-down æœºåˆ¶å¯å‘ï¼ŒTDFormer å¼•å…¥äº†ä¸€ç§å±‚çº§åŒ–çš„ top-down feedback ç»“æ„ï¼Œåˆ©ç”¨æ—©æœŸæ—¶é—´æ­¥çš„é«˜é˜¶è¡¨ç¤ºæ¥è°ƒåˆ¶åæœŸé˜¶æ®µçš„ä½é˜¶ä¿¡æ¯å¤„ç†ã€‚è¯¥ç»“æ„åœ¨å‰å‘ä¼ æ’­ä¸­å¢åŠ äº†è·¨æ—¶é—´æ­¥çš„ mutual informationï¼Œå®ç°äº†æ›´ä¸°å¯Œæ—¶é—´ä¿¡æ¯çš„ä¼ è¾“ä¸æ•´åˆï¼ŒåŒæ—¶åœ¨åå‘ä¼ æ’­ä¸­è¢«ç†è®ºè¯æ˜èƒ½æœ‰æ•ˆç¼“è§£æ—¶é—´ç»´åº¦ä¸Šçš„ vanishing gradients é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æœºåˆ¶æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå…¶ä¸­åœ¨ ImageNet æ•°æ®é›†ä¸Šå–å¾—äº† 86.83% çš„å‡†ç¡®ç‡ï¼Œè¾¾åˆ°äº†ç›®å‰çš„ state-of-the-art æ€§èƒ½ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.NE",
      "comment": "28 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.15840v2",
      "published_date": "2025-05-17 15:55:32 UTC",
      "updated_date": "2025-05-23 03:45:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:51:30.273413+00:00"
    },
    {
      "arxiv_id": "2506.06295v1",
      "title": "dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching",
      "title_zh": "dLLM-Cacheï¼šåˆ©ç”¨è‡ªé€‚åº”ç¼“å­˜åŠ é€Ÿæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Zhiyuan Liu",
        "Yicun Yang",
        "Yaojie Zhang",
        "Junjie Chen",
        "Chang Zou",
        "Qingyuan Wei",
        "Shaobo Wang",
        "Linfeng Zhang"
      ],
      "abstract": "Autoregressive Models (ARMs) have long dominated the landscape of Large Language Models. Recently, a new paradigm has emerged in the form of diffusion-based Large Language Models (dLLMs), which generate text by iteratively denoising masked segments. This approach has shown significant advantages and potential. However, dLLMs suffer from high inference latency. Traditional ARM acceleration techniques, such as Key-Value caching, are incompatible with dLLMs due to their bidirectional attention mechanism. To address this specific challenge, our work begins with a key observation that dLLM inference involves a static prompt and a partially dynamic response, where most tokens remain stable across adjacent denoising steps. Based on this, we propose dLLM-Cache, a training-free adaptive caching framework that combines long-interval prompt caching with partial response updates guided by feature similarity. This design enables efficient reuse of intermediate computations without compromising model performance. Extensive experiments on representative dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1 x speedup over standard inference without compromising output quality. Notably, our method brings dLLM inference latency close to that of ARMs under many settings. Codes are provided in the supplementary material and will be released publicly on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹(dLLMs)æ¨ç†å»¶è¿Ÿé«˜çš„é—®é¢˜ï¼Œæå‡ºäº†dLLM-Cacheï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è‡ªé€‚åº”ç¼“å­˜æ¡†æ¶ã€‚ç”±äºdLLMsé‡‡ç”¨åŒå‘æ³¨æ„æœºåˆ¶(bidirectional attention mechanism)ï¼Œä¼ ç»Ÿçš„Key-Value cachingæŠ€æœ¯éš¾ä»¥é€‚ç”¨ï¼Œè€Œè¯¥ç ”ç©¶è§‚å¯Ÿåˆ°dLLMåœ¨ç›¸é‚»å»å™ªæ­¥éª¤(denoising steps)é—´çš„å¤šæ•°tokenä¿æŒç¨³å®šã€‚åŸºäºæ­¤ï¼ŒdLLM-Cacheç»“åˆäº†é•¿é—´éš”æç¤ºè¯ç¼“å­˜(prompt caching)ä¸åŸºäºç‰¹å¾ç›¸ä¼¼åº¦å¼•å¯¼çš„å±€éƒ¨å“åº”æ›´æ–°ï¼Œå®ç°äº†ä¸­é—´è®¡ç®—çš„é«˜æ•ˆå¤ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨LLaDA 8Bå’ŒDream 7Bç­‰ä»£è¡¨æ€§æ¨¡å‹ä¸Šï¼ŒdLLM-Cacheåœ¨ä¸æŸå¤±è¾“å‡ºè´¨é‡çš„å‰æä¸‹ï¼Œæ¯”æ ‡å‡†æ¨ç†å®ç°äº†é«˜è¾¾9.1å€çš„åŠ é€Ÿã€‚è¯¥æ–¹æ³•æˆåŠŸå°†dLLMsçš„æ¨ç†å»¶è¿Ÿé™ä½è‡³æ¥è¿‘è‡ªå›å½’æ¨¡å‹(ARMs)çš„æ°´å¹³ï¼Œä¸ºæ‰©æ•£ç±»è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†é‡è¦è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06295v1",
      "published_date": "2025-05-17 15:50:46 UTC",
      "updated_date": "2025-05-17 15:50:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:50:41.246031+00:00"
    },
    {
      "arxiv_id": "2505.12058v1",
      "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation",
      "title_zh": "Tiny QA Benchmark++ï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æŒç»­è¯„ä¼°çš„è¶…è½»é‡åŒ–ã€åˆæˆå¼å¤šè¯­è¨€æ•°æ®é›†ç”Ÿæˆä¸å†’çƒŸæµ‹è¯•",
      "authors": [
        "Vincent Koc"
      ],
      "abstract": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop demands building the Comet Opik prompt-optimization SDK, where waiting on heavyweight benchmarks breaks developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with a tiny synthetic-data generator pypi package built on provider-agnostic LiteLLM. The generator lets practitioners mint their own tiny packs in any language, domain, or difficulty, while ten ready-made packs already cover Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian, Spanish, and Turkish. Every dataset ships with Croissant metadata and plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so teams can drop deterministic micro-benchmarks directly into pull-request gates, prompt-engineering loops, and production dashboards without touching GPU budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet reliably flags prompt-template errors, tokenizer drift, and fine-tuning side-effects long before full-scale suites like MMLU or BIG-Bench would finish configuring. The entire framework is released to accelerate continuous, resource-efficient quality assurance across the generative-AI ecosystem.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Tiny QA Benchmark++ (TQB++)ï¼Œè¿™æ˜¯ä¸€ä¸ªè¶…è½»é‡çš„å¤šè¯­è¨€å†’çƒŸæµ‹è¯• (smoke-test) å¥—ä»¶ï¼Œæ—¨åœ¨ä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLM) æµæ°´çº¿æä¾›å•å…ƒæµ‹è¯•çº§åˆ«çš„å®‰å…¨ç½‘ã€‚TQB++ åŒ…å«ä¸€ä¸ªå°äº 20 kB çš„ 52 é¡¹ English gold setï¼Œå¹¶é™„å¸¦ä¸€ä¸ªåŸºäº LiteLLM çš„åˆæˆæ•°æ®ç”Ÿæˆå™¨ï¼Œå…è®¸ç”¨æˆ·å¿«é€Ÿç”Ÿæˆé’ˆå¯¹ç‰¹å®šè¯­è¨€æˆ–é¢†åŸŸçš„å¾®å‹æ•°æ®é›†ã€‚è¯¥æ¡†æ¶ç›®å‰å·²è¦†ç›–ä¸­æ–‡ã€é˜¿æ‹‰ä¼¯è¯­ç­‰ 10 ç§è¯­è¨€ï¼Œå¹¶æä¾›å…¼å®¹ OpenAI-Evalsã€LangChain åŠæ ‡å‡† CI å·¥å…·çš„æ’ä»¶åŒ–æ–‡ä»¶ã€‚å¼€å‘è€…å¯ä»¥å°†è¿™ç§ç¡®å®šæ€§çš„å¾®åŸºå‡†æµ‹è¯•ç›´æ¥é›†æˆåˆ° pull-request é—¨æ§æˆ–ç”Ÿäº§ä»ªè¡¨ç›˜ä¸­ï¼Œä¸”æ— éœ€æ¶ˆè€—æ˜‚è´µçš„ GPU é¢„ç®—ã€‚ç›¸æ¯” MMLU æˆ– BIG-Bench ç­‰é‡é‡çº§åŸºå‡†ï¼ŒTQB++ åœ¨å‡ ç§’é’Ÿå†…å³å¯è¿è¡Œå®Œæ¯•ï¼Œèƒ½æœ‰æ•ˆæ•æ‰ prompt-template é”™è¯¯ã€åˆ†è¯å™¨æ¼‚ç§»åŠå¾®è°ƒå‰¯ä½œç”¨ã€‚è¿™ä¸€å¼€æºæ¡†æ¶ä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½é¢†åŸŸæä¾›äº†ä¸€ç§èµ„æºé«˜æ•ˆçš„è¿ç»­è´¨é‡ä¿è¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "28 pages, 7 figures, 3 tables. Includes expanded appendix & full score matrices. Dataset & code: HF Hub + GitHub + Pypi links in abstract. Core data and code Apache-2.0; synthetic packs eval-only",
      "pdf_url": "https://arxiv.org/pdf/2505.12058v1",
      "published_date": "2025-05-17 15:40:03 UTC",
      "updated_date": "2025-05-17 15:40:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:50:53.726862+00:00"
    },
    {
      "arxiv_id": "2505.12057v1",
      "title": "CorBenchX: Large-Scale Chest X-Ray Error Dataset and Vision-Language Model Benchmark for Report Error Correction",
      "title_zh": "CorBenchXï¼šé¢å‘æŠ¥å‘Šçº é”™çš„å¤§è§„æ¨¡èƒ¸éƒ¨ X çº¿é”™è¯¯æ•°æ®é›†ä¸è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†",
      "authors": [
        "Jing Zou",
        "Qingqiu Li",
        "Chenyu Lian",
        "Lihao Liu",
        "Xiaohan Yan",
        "Shujun Wang",
        "Jing Qin"
      ],
      "abstract": "AI-driven models have shown great promise in detecting errors in radiology reports, yet the field lacks a unified benchmark for rigorous evaluation of error detection and further correction. To address this gap, we introduce CorBenchX, a comprehensive suite for automated error detection and correction in chest X-ray reports, designed to advance AI-assisted quality control in clinical practice. We first synthesize a large-scale dataset of 26,326 chest X-ray error reports by injecting clinically common errors via prompting DeepSeek-R1, with each corrupted report paired with its original text, error type, and human-readable description. Leveraging this dataset, we benchmark both open- and closed-source vision-language models,(e.g., InternVL, Qwen-VL, GPT-4o, o4-mini, and Claude-3.7) for error detection and correction under zero-shot prompting. Among these models, o4-mini achieves the best performance, with 50.6 % detection accuracy and correction scores of BLEU 0.853, ROUGE 0.924, BERTScore 0.981, SembScore 0.865, and CheXbertF1 0.954, remaining below clinical-level accuracy, highlighting the challenge of precise report correction. To advance the state of the art, we propose a multi-step reinforcement learning (MSRL) framework that optimizes a multi-objective reward combining format compliance, error-type accuracy, and BLEU similarity. We apply MSRL to QwenVL2.5-7B, the top open-source model in our benchmark, achieving an improvement of 38.3% in single-error detection precision and 5.2% in single-error correction over the zero-shot baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† CorBenchXï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹èƒ¸éƒ¨ X å°„çº¿ (Chest X-Ray) æŠ¥å‘Šè‡ªåŠ¨é”™è¯¯æ£€æµ‹ä¸çº æ­£çš„å¤§è§„æ¨¡æ•°æ®é›†åŠè§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Model) åŸºå‡†ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ DeepSeek-R1 æç¤ºæŠ€æœ¯æ³¨å…¥ä¸´åºŠå¸¸è§é”™è¯¯ï¼Œåˆæˆäº†åŒ…å« 26,326 ä»½é”™è¯¯æŠ¥å‘Šçš„æ•°æ®é›†ï¼Œå¹¶è¯¦ç»†æ ‡æ³¨äº†é”™è¯¯ç±»å‹ä¸æè¿°ã€‚åœ¨å¯¹ GPT-4oã€o4-mini å’Œ Claude-3.7 ç­‰å¤šç§æ¨¡å‹è¿›è¡Œçš„é›¶æ ·æœ¬ (Zero-shot) åŸºå‡†æµ‹è¯•ä¸­ï¼Œo4-mini å–å¾—äº†æœ€ä½³è¡¨ç°ï¼Œä½†å…¶å‡†ç¡®ç‡ä»ä½äºä¸´åºŠåº”ç”¨è¦æ±‚ï¼Œå‡¸æ˜¾äº†ç²¾ç¡®æŠ¥å‘Šçº é”™çš„æŒ‘æˆ˜ã€‚ä¸ºè¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ­¥å¼ºåŒ–å­¦ä¹  (Multi-step Reinforcement Learning, MSRL) æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–ç»“åˆæ ¼å¼åˆè§„æ€§ã€é”™è¯¯ç±»å‹å‡†ç¡®åº¦å’Œ BLEU ç›¸ä¼¼åº¦çš„å¤šç›®æ ‡å¥–åŠ±ã€‚å®éªŒè¯æ˜ï¼ŒMSRL æ˜¾è‘—æå‡äº†å¼€æºæ¨¡å‹ QwenVL2.5-7B çš„æ€§èƒ½ï¼Œä½¿å…¶å•é”™è¯¯æ£€æµ‹ç²¾åº¦æé«˜äº† 38.3%ï¼Œçº æ­£èƒ½åŠ›æé«˜äº† 5.2%ï¼Œä¸ºä¸´åºŠç¯å¢ƒä¸‹çš„ AI è¾…åŠ©è´¨é‡æ§åˆ¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 5figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12057v1",
      "published_date": "2025-05-17 15:39:39 UTC",
      "updated_date": "2025-05-17 15:39:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:50:57.144419+00:00"
    },
    {
      "arxiv_id": "2505.12053v2",
      "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
      "title_zh": "VFRTokï¼šåŸºäºæ—¶é•¿ç­‰æ¯”ä¿¡æ¯å‡è®¾çš„å¯å˜å¸§ç‡è§†é¢‘åˆ†è¯å™¨",
      "authors": [
        "Tianxiong Zhong",
        "Xingye Tian",
        "Boyuan Jiang",
        "Xuebo Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Zhiwei Zhang"
      ],
      "abstract": "Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers. The code and weights are released at: https://github.com/KwaiVGI/VFRTok.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰æ½œåœ¨æ‰©æ•£æ¨¡å‹(Latent Diffusion Models)ä¸­è§†é¢‘ä»¤ç‰ŒåŒ–(tokenization)æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ä¼ ç»Ÿå¸§æ¯”ä¾‹ä¿¡æ¯å‡è®¾(Frame-Proportional Information Assumption)å¯¼è‡´çš„è®¡ç®—å†—ä½™ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†æ—¶é•¿æ¯”ä¾‹ä¿¡æ¯å‡è®¾(Duration-Proportional Information Assumption)ï¼Œè®¤ä¸ºè§†é¢‘ä¿¡æ¯å®¹é‡åº”ä¸æ—¶é•¿è€Œéå¸§æ•°æˆæ­£æ¯”ï¼Œå¹¶æ®æ­¤å¼€å‘äº†åŸºäºTransformerçš„è§†é¢‘ä»¤ç‰ŒåŒ–å™¨VFRTokã€‚è¯¥å·¥å…·é€šè¿‡éå¯¹ç§°å¸§ç‡è®­ç»ƒ(asymmetric frame rate training)æ”¯æŒå¯å˜å¸§ç‡ç¼–è§£ç ï¼Œä½¿è®¡ç®—æˆæœ¬ä¸å†éšå¸§ç‡çº¿æ€§å¢é•¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†éƒ¨åˆ†æ—‹è½¬ä½ç½®åµŒå…¥(Partial RoPE)æ¥è§£è€¦ä½ç½®ä¸å†…å®¹å»ºæ¨¡ï¼Œé€šè¿‡å°†ç›¸å…³è¡¥ä¸(patches)ç»„åˆæˆç»Ÿä¸€ä»¤ç‰Œæ¥å¢å¼ºå†…å®¹æ„ŸçŸ¥èƒ½åŠ›ã€‚å¾—ç›Šäºè¿™ç§ç´§å‡‘ä¸”è¿ç»­çš„æ—¶ç©ºè¡¨ç¤ºï¼ŒVFRTokåœ¨ä»…ä½¿ç”¨ç°æœ‰æŠ€æœ¯1/8ä»¤ç‰Œé‡çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æå…·ç«äº‰åŠ›çš„é‡å»ºè´¨é‡å’Œæœ€å…ˆè¿›çš„ç”Ÿæˆä¿çœŸåº¦ã€‚è¿™ä¸€æˆæœä¸ºä¼˜åŒ–è§†é¢‘ç”Ÿæˆæ¡†æ¶çš„è®¡ç®—æ•ˆç‡å’Œè¡¨å¾èƒ½åŠ›æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.12053v2",
      "published_date": "2025-05-17 15:32:54 UTC",
      "updated_date": "2025-09-28 09:51:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:51:04.079579+00:00"
    },
    {
      "arxiv_id": "2505.12050v2",
      "title": "AdaBoN: Adaptive Best-of-N Alignment",
      "title_zh": "AdaBoNï¼šè‡ªé€‚åº” Best-of-N å¯¹é½",
      "authors": [
        "Vinod Raman",
        "Hilal Asi",
        "Satyen Kale"
      ],
      "abstract": "Recent advances in test-time alignment methods, such as Best-of-N sampling, offer a simple and effective way to steer language models (LMs) toward preferred behaviors using reward models (RM). However, these approaches can be computationally expensive, especially when applied uniformly across prompts without accounting for differences in alignment difficulty. In this work, we propose a prompt-adaptive strategy for Best-of-N alignment that allocates inference-time compute more efficiently. Motivated by latency concerns, we develop a two-stage algorithm: an initial exploratory phase estimates the reward distribution for each prompt using a small exploration budget, and a second stage adaptively allocates the remaining budget using these estimates. Our method is simple, practical, and compatible with any LM-RM combination. Empirical results on prompts from the AlpacaEval, HH-RLHF, and PKU-SafeRLHF datasets for 12 LM/RM pairs and 50 different batches of prompts show that our adaptive strategy outperforms the uniform allocation with the same inference budget. Moreover, we show that our adaptive strategy remains competitive against uniform allocations with 20 percent larger inference budgets and improves in performance as the batch size grows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ç«¯å¯¹é½è¿‡ç¨‹ä¸­ Best-of-N (BoN) é‡‡æ ·è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº† AdaBoNï¼Œä¸€ç§æç¤ºè¯è‡ªé€‚åº” (prompt-adaptive) çš„å¯¹é½ç­–ç•¥ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç®—æ³•ï¼Œé¦–å…ˆåˆ©ç”¨å°‘é‡æ¢ç´¢é¢„ç®— (exploration budget) è¯„ä¼°æ¯ä¸ªæç¤ºè¯çš„å¥–åŠ±åˆ†å¸ƒ (reward distribution)ï¼Œéšåæ ¹æ®è¯„ä¼°ç»“æœè‡ªé€‚åº”åœ°åˆ†é…å‰©ä½™çš„æ¨ç†ç®—åŠ›ã€‚è¿™ç§ç­–ç•¥è®¾è®¡ç®€æ´ä¸”å®ç”¨ï¼Œèƒ½å¤Ÿä¸ä»»ä½•è¯­è¨€æ¨¡å‹ (LM) å’Œå¥–åŠ±æ¨¡å‹ (RM) ç»„åˆå…¼å®¹ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†æ¨ç†æ•ˆç‡ã€‚åœ¨ AlpacaEvalã€HH-RLHF å’Œ PKU-SafeRLHF ç­‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ç›¸åŒæ¨ç†é¢„ç®—ä¸‹ï¼ŒAdaBoN çš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„å‡åŒ€åˆ†é… (uniform allocation) ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„æ•ˆæœç”šè‡³èƒ½ä¸æ¨ç†é¢„ç®—é«˜å‡º 20% çš„å‡åŒ€åˆ†é…æ–¹æ¡ˆç›¸åª²ç¾ï¼Œä¸”å…¶æ€§èƒ½ä¼˜åŠ¿éšç€æ‰¹å¤„ç†è§„æ¨¡ (batch size) çš„å¢åŠ è€Œè¿›ä¸€æ­¥æ‰©å¤§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "25 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.12050v2",
      "published_date": "2025-05-17 15:24:48 UTC",
      "updated_date": "2025-09-29 17:55:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:51:06.481398+00:00"
    },
    {
      "arxiv_id": "2505.12051v1",
      "title": "Enhanced Multimodal Hate Video Detection via Channel-wise and Modality-wise Fusion",
      "title_zh": "åŸºäºé€šé“çº§ä¸æ¨¡æ€çº§èåˆçš„å¢å¼ºå‹å¤šæ¨¡æ€ä»‡æ¨è§†é¢‘æ£€æµ‹",
      "authors": [
        "Yinghui Zhang",
        "Tailin Chen",
        "Yuchen Zhang",
        "Zeyu Fu"
      ],
      "abstract": "The rapid rise of video content on platforms such as TikTok and YouTube has transformed information dissemination, but it has also facilitated the spread of harmful content, particularly hate videos. Despite significant efforts to combat hate speech, detecting these videos remains challenging due to their often implicit nature. Current detection methods primarily rely on unimodal approaches, which inadequately capture the complementary features across different modalities. While multimodal techniques offer a broader perspective, many fail to effectively integrate temporal dynamics and modality-wise interactions essential for identifying nuanced hate content. In this paper, we present CMFusion, an enhanced multimodal hate video detection model utilizing a novel Channel-wise and Modality-wise Fusion Mechanism. CMFusion first extracts features from text, audio, and video modalities using pre-trained models and then incorporates a temporal cross-attention mechanism to capture dependencies between video and audio streams. The learned features are then processed by channel-wise and modality-wise fusion modules to obtain informative representations of videos. Our extensive experiments on a real-world dataset demonstrate that CMFusion significantly outperforms five widely used baselines in terms of accuracy, precision, recall, and F1 score. Comprehensive ablation studies and parameter analyses further validate our design choices, highlighting the model's effectiveness in detecting hate videos. The source codes will be made publicly available at https://github.com/EvelynZ10/cmfusion.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ­è§†é¢‘å¹³å°ä¸­éšè”½æ€§å¼ºçš„ hate videos æ£€æµ‹éš¾é¢˜ï¼Œæå‡ºäº† CMFusion æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å•æ¨¡æ€ï¼ˆunimodalï¼‰æˆ–ä¼ ç»Ÿå¤šæ¨¡æ€æ–¹æ³•åœ¨å¤„ç†æ—¶åºåŠ¨æ€å’Œæ¨¡æ€é—´äº¤äº’æ–¹é¢çš„å±€é™ã€‚CMFusion å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„ Channel-wise and Modality-wise Fusion Mechanismï¼ˆé€šé“ç»´ä¸æ¨¡æ€ç»´èåˆæœºåˆ¶ï¼‰ï¼Œé€šè¿‡é¢„è®­ç»ƒæ¨¡å‹æå–æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ç‰¹å¾ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ temporal cross-attention æœºåˆ¶æ•æ‰éŸ³è§†é¢‘æµé—´çš„æ—¶åºä¾èµ–å…³ç³»ï¼Œå¹¶ç»“åˆé€šé“ä¸æ¨¡æ€ç»´åº¦çš„èåˆæ¨¡å—ï¼Œç”Ÿæˆæ›´å…·ä¿¡æ¯é‡çš„è§†é¢‘è¡¨å¾ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCMFusion åœ¨å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’Œ F1 score ç­‰æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºäº”ç§ä¸»æµåŸºçº¿æ¨¡å‹ã€‚æ¶ˆèå®éªŒä¸å‚æ•°åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†è¯¥è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶èƒ½æ›´ç²¾å‡†åœ°è¯†åˆ«å¤æ‚çš„ä»‡æ¨å†…å®¹ã€‚è¯¥ç ”ç©¶çš„æºä»£ç å·²åœ¨ GitHub å…¬å¼€ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.MM",
      "comment": "ICDMW 2024, Github: https://github.com/EvelynZ10/cmfusion",
      "pdf_url": "https://arxiv.org/pdf/2505.12051v1",
      "published_date": "2025-05-17 15:24:48 UTC",
      "updated_date": "2025-05-17 15:24:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:51:07.550616+00:00"
    },
    {
      "arxiv_id": "2505.12049v1",
      "title": "Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs",
      "title_zh": "è¶…è¶Šæ ‡é‡å¥–åŠ±ï¼šè¯å…¸åº MDP çš„å…¬ç†åŒ–æ¡†æ¶",
      "authors": [
        "Mehran Shakerinava",
        "Siamak Ravanbakhsh",
        "Adam Oberman"
      ],
      "abstract": "Recent work has formalized the reward hypothesis through the lens of expected utility theory, by interpreting reward as utility. Hausner's foundational work showed that dropping the continuity axiom leads to a generalization of expected utility theory where utilities are lexicographically ordered vectors of arbitrary dimension. In this paper, we extend this result by identifying a simple and practical condition under which preferences cannot be represented by scalar rewards, necessitating a 2-dimensional reward function. We provide a full characterization of such reward functions, as well as the general d-dimensional case, in Markov Decision Processes (MDPs) under a memorylessness assumption on preferences. Furthermore, we show that optimal policies in this setting retain many desirable properties of their scalar-reward counterparts, while in the Constrained MDP (CMDP) setting -- another common multiobjective setting -- they do not.",
      "tldr_zh": "è¯¥ç ”ç©¶åŸºäºæœŸæœ›æ•ˆç”¨ç†è®º(Expected Utility Theory)å¯¹å¥–åŠ±å‡è®¾è¿›è¡Œäº†å½¢å¼åŒ–åˆ†æï¼Œæ¢è®¨äº†å½“è¿ç»­æ€§å…¬ç†(Continuity Axiom)ä¸æˆç«‹æ—¶ï¼Œå°†å¥–åŠ±æ¨å¹¿ä¸ºè¯å…¸åºæ’åˆ—(Lexicographically Ordered)çš„å¤šç»´å‘é‡æ¡†æ¶ã€‚ä½œè€…é€šè¿‡Hausnerçš„å¥ åŸºå·¥ä½œç¡®å®šäº†ä¸€ä¸ªå®ç”¨æ¡ä»¶ï¼Œè¯æ˜åœ¨è¯¥æ¡ä»¶ä¸‹åå¥½æ— æ³•é€šè¿‡æ ‡é‡å¥–åŠ±(Scalar Rewards)è¡¨ç¤ºï¼Œä»è€Œå¿…é¡»ä½¿ç”¨äºŒç»´æˆ–dç»´å¥–åŠ±å‡½æ•°ã€‚è®ºæ–‡åœ¨åå¥½æ— è®°å¿†æ€§(Memorylessness)çš„å‡è®¾ä¸‹ï¼Œå¯¹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDPs)ä¸­çš„å¤šç»´å¥–åŠ±å‡½æ•°è¿›è¡Œäº†å…¨é¢åˆ»ç”»ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜åœ¨æ­¤è®¾ç½®ä¸‹çš„æœ€ä¼˜ç­–ç•¥ä¿ç•™äº†æ ‡é‡å¥–åŠ±å¯¹åº”é¡¹çš„è®¸å¤šä¼˜è‰¯æ€§è´¨ï¼Œè€Œè¿™åœ¨çº¦æŸé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(CMDP)ç­‰å…¶ä»–å¤šç›®æ ‡è®¾ç½®ä¸­å¾€å¾€å¹¶ä¸æˆç«‹ã€‚è¯¥æ¡†æ¶ä¸ºç†è§£è¶…è¶Šæ ‡é‡å¥–åŠ±çš„å¤æ‚åå¥½æä¾›äº†å…¬ç†åŒ–åŸºç¡€ï¼Œå¹¶æ­ç¤ºäº†è¯å…¸åºMDPsåœ¨ç­–ç•¥ä¼˜åŒ–ä¸Šçš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12049v1",
      "published_date": "2025-05-17 15:23:58 UTC",
      "updated_date": "2025-05-17 15:23:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:52:52.745941+00:00"
    },
    {
      "arxiv_id": "2505.12039v1",
      "title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research",
      "title_zh": "AI é©±åŠ¨çš„è‡ªåŠ¨åŒ–å¯æˆä¸ºæ–°ä¸€ä»£ç§‘å­¦å­¦ç ”ç©¶çš„åŸºçŸ³",
      "authors": [
        "Renqi Chen",
        "Haoyang Su",
        "Shixiang Tang",
        "Zhenfei Yin",
        "Qi Wu",
        "Hui Li",
        "Ye Sun",
        "Nanqing Dong",
        "Wanli Ouyang",
        "Philip Torr"
      ],
      "abstract": "The Science of Science (SoS) explores the mechanisms underlying scientific discovery, and offers valuable insights for enhancing scientific efficiency and fostering innovation. Traditional approaches often rely on simplistic assumptions and basic statistical tools, such as linear regression and rule-based simulations, which struggle to capture the complexity and scale of modern research ecosystems. The advent of artificial intelligence (AI) presents a transformative opportunity for the next generation of SoS, enabling the automation of large-scale pattern discovery and uncovering insights previously unattainable. This paper offers a forward-looking perspective on the integration of Science of Science with AI for automated research pattern discovery and highlights key open challenges that could greatly benefit from AI. We outline the advantages of AI over traditional methods, discuss potential limitations, and propose pathways to overcome them. Additionally, we present a preliminary multi-agent system as an illustrative example to simulate research societies, showcasing AI's ability to replicate real-world research patterns and accelerate progress in Science of Science research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº† Science of Science (SoS) é¢†åŸŸåœ¨åº”å¯¹ç°ä»£å¤æ‚ç ”ç©¶ç”Ÿæ€ç³»ç»Ÿæ—¶çš„å±€é™ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„ linear regression å’Œ rule-based simulations ç­‰æ–¹æ³•å·²éš¾ä»¥æ»¡è¶³è§„æ¨¡åŒ–éœ€æ±‚ã€‚æ–‡ç« æå‡º AI é©±åŠ¨çš„è‡ªåŠ¨åŒ–å°†æˆä¸ºä¸‹ä¸€ä»£ Science of Science çš„åŸºç¡€ï¼Œèƒ½å¤Ÿå®ç°å¤§è§„æ¨¡çš„ pattern discovery å¹¶æŒ–æ˜ä¼ ç»Ÿæ‰‹æ®µæ— æ³•è§¦è¾¾çš„ç§‘å­¦è§è§£ã€‚ä½œè€…ç³»ç»Ÿåœ°åˆ†æäº† AI ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶é’ˆå¯¹å½“å‰é¢ä¸´çš„å…³é”®æŒ‘æˆ˜æå‡ºäº†ç›¸åº”çš„è§£å†³è·¯å¾„ã€‚ç ”ç©¶è¿˜é€šè¿‡ä¸€ä¸ªåˆæ­¥çš„ multi-agent system ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•æ¨¡æ‹Ÿç§‘ç ”ç¤¾ä¼šï¼Œè¯æ˜äº† AI åœ¨å¤åˆ¶ç°å®ç ”ç©¶æ¨¡å¼å¹¶åŠ é€Ÿç§‘ç ”è¿›å±•æ–¹é¢çš„æ½œåŠ›ã€‚è¯¥å·¥ä½œä¸ºæ¨åŠ¨ Science of Science ç ”ç©¶çš„è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŒ–è½¬å‹æä¾›äº†å‰ç»æ€§æ¡†æ¶ï¼Œæ—¨åœ¨è¿›ä¸€æ­¥æå‡å…¨çƒç§‘ç ”æ•ˆç‡ä¸åˆ›æ–°æ´»åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "physics.soc-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12039v1",
      "published_date": "2025-05-17 15:01:33 UTC",
      "updated_date": "2025-05-17 15:01:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:51:53.973215+00:00"
    },
    {
      "arxiv_id": "2505.12038v1",
      "title": "Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets",
      "title_zh": "Safe Deltaï¼šåœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹æ—¶æŒç»­ä¿éšœå®‰å…¨æ€§",
      "authors": [
        "Ning Lu",
        "Shengcai Liu",
        "Jiahao Wu",
        "Weiyu Chen",
        "Zhirui Zhang",
        "Yew-Soon Ong",
        "Qi Wang",
        "Ke Tang"
      ],
      "abstract": "Large language models (LLMs) have shown great potential as general-purpose AI assistants across various domains. To fully leverage this potential in specific applications, many companies provide fine-tuning API services, enabling users to upload their own data for LLM customization. However, fine-tuning services introduce a new safety threat: user-uploaded data, whether harmful or benign, can break the model's alignment, leading to unsafe outputs. Moreover, existing defense methods struggle to address the diversity of fine-tuning datasets (e.g., varying sizes, tasks), often sacrificing utility for safety or vice versa. To address this issue, we propose Safe Delta, a safety-aware post-training defense method that adjusts the delta parameters (i.e., the parameter change before and after fine-tuning). Specifically, Safe Delta estimates the safety degradation, selects delta parameters to maximize utility while limiting overall safety loss, and applies a safety compensation vector to mitigate residual safety loss. Through extensive experiments on four diverse datasets with varying settings, our approach consistently preserves safety while ensuring that the utility gain from benign datasets remains unaffected.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å®¹æ˜“å› ç”¨æˆ·æ•°æ®å¤šæ ·æ€§è€Œç ´åå®‰å…¨å¯¹é½ï¼ˆsafety alignmentï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Safe Delta çš„å®‰å…¨æ„ŸçŸ¥åè®­ç»ƒé˜²å¾¡æ–¹æ³•ã€‚Safe Delta é€šè¿‡è°ƒæ•´å¾®è°ƒå‰åçš„å‚æ•°å¢é‡ï¼ˆdelta parametersï¼‰æ¥å¹³è¡¡å®‰å…¨æ€§å’Œå®ç”¨æ€§ï¼Œé¦–å…ˆä¼°ç®—å¾®è°ƒå¸¦æ¥çš„å®‰å…¨é€€åŒ–ç¨‹åº¦ï¼Œéšååœ¨é™åˆ¶å®‰å…¨æŸå¤±çš„å‰æä¸‹ç­›é€‰å‚æ•°ä»¥æœ€å¤§åŒ–æ¨¡å‹æ•ˆç”¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¼•å…¥äº†å®‰å…¨è¡¥å¿å‘é‡ï¼ˆsafety compensation vectorï¼‰æ¥è¿›ä¸€æ­¥å¼¥è¡¥æ®‹ä½™çš„å®‰å…¨æŸå¤±ã€‚åœ¨å››ç§ä¸åŒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSafe Delta èƒ½å¤Ÿåœ¨ä¸æŸå®³è‰¯æ€§å¾®è°ƒæ‰€å¸¦æ¥çš„å®ç”¨æ€§æå‡çš„åŒæ—¶ï¼ŒæŒç»­æœ‰æ•ˆåœ°ä¿æŒæ¨¡å‹çš„å®‰å…¨æ€§ã€‚è¯¥æ–¹æ³•ä¸ºè§£å†³å®šåˆ¶åŒ– LLM æœåŠ¡ä¸­çš„å®‰å…¨é£é™©æä¾›äº†ä¸€ç§å…¼é¡¾æ€§èƒ½ä¸é˜²å¾¡çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025 Camera Ready",
      "pdf_url": "https://arxiv.org/pdf/2505.12038v1",
      "published_date": "2025-05-17 15:01:07 UTC",
      "updated_date": "2025-05-17 15:01:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:51:54.374996+00:00"
    },
    {
      "arxiv_id": "2505.12031v1",
      "title": "LLM-based Automated Theorem Proving Hinges on Scalable Synthetic Data Generation",
      "title_zh": "åŸºäº LLM çš„è‡ªåŠ¨å®šç†è¯æ˜æ ¸å¿ƒåœ¨äºå¯æ‰©å±•çš„åˆæˆæ•°æ®ç”Ÿæˆ",
      "authors": [
        "Junyu Lai",
        "Jiakun Zhang",
        "Shuo Xu",
        "Taolue Chen",
        "Zihang Wang",
        "Yao Yang",
        "Jiarui Zhang",
        "Chun Cao",
        "Jingwei Xu"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have sparked considerable interest in automated theorem proving and a prominent line of research integrates stepwise LLM-based provers into tree search. In this paper, we introduce a novel proof-state exploration approach for training data synthesis, designed to produce diverse tactics across a wide range of intermediate proof states, thereby facilitating effective one-shot fine-tuning of LLM as the policy model. We also propose an adaptive beam size strategy, which effectively takes advantage of our data synthesis method and achieves a trade-off between exploration and exploitation during tree search. Evaluations on the MiniF2F and ProofNet benchmarks demonstrate that our method outperforms strong baselines under the stringent Pass@1 metric, attaining an average pass rate of $60.74\\%$ on MiniF2F and $21.18\\%$ on ProofNet. These results underscore the impact of large-scale synthetic data in advancing automated theorem proving.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªåŠ¨åŒ–å®šç†è¯æ˜(Automated Theorem Proving)ï¼Œæå‡ºå¤§è§„æ¨¡åˆæˆæ•°æ®ç”Ÿæˆæ˜¯æå‡å…¶æ€§èƒ½çš„å…³é”®ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è¯æ˜çŠ¶æ€æ¢ç´¢æ–¹æ³•(proof-state exploration)ï¼Œé€šè¿‡ç”Ÿæˆæ¶µç›–å¹¿æ³›ä¸­é—´è¯æ˜çŠ¶æ€çš„å¤šæ ·åŒ–ç­–ç•¥(tactics)ï¼Œæ”¯æŒå¯¹ä½œä¸ºç­–ç•¥æ¨¡å‹(policy model)çš„LLMè¿›è¡Œæœ‰æ•ˆçš„ä¸€æ¬¡æ€§å¾®è°ƒ(one-shot fine-tuning)ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç§è‡ªé€‚åº”é›†æŸå®½åº¦ç­–ç•¥(adaptive beam size strategy)ï¼Œåœ¨æ ‘æœç´¢(tree search)è¿‡ç¨‹ä¸­æœ‰æ•ˆå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨MiniF2Få’ŒProofNetåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒPass@1å¹³å‡æˆåŠŸç‡åˆ†åˆ«è¾¾åˆ°60.74%å’Œ21.18%ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºå‡†æ¨¡å‹ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¤§è§„æ¨¡åˆæˆæ•°æ®åœ¨æ¨åŠ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜æŠ€æœ¯è¿›æ­¥ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.12031v1",
      "published_date": "2025-05-17 14:47:36 UTC",
      "updated_date": "2025-05-17 14:47:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:52:01.443093+00:00"
    },
    {
      "arxiv_id": "2506.06294v2",
      "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning",
      "title_zh": "GLProteinï¼šèåˆå…¨å±€ä¸å±€éƒ¨ç»“æ„çš„è›‹ç™½è´¨è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Yunqing Liu",
        "Wenqi Fan",
        "Xiaoyong Wei",
        "Qing Li"
      ],
      "abstract": "Proteins are central to biological systems, participating as building blocks across all forms of life. Despite advancements in understanding protein functions through protein sequence analysis, there remains potential for further exploration in integrating protein structural information. We argue that the structural information of proteins is not only limited to their 3D information but also encompasses information from amino acid molecules (local information) to protein-protein structure similarity (global information). To address this, we propose \\textbf{GLProtein}, the first framework in protein pre-training that incorporates both global structural similarity and local amino acid details to enhance prediction accuracy and functional insights. GLProtein innovatively combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding. Experimental results demonstrate that GLProtein outperforms previous methods in several bioinformatics tasks, including predicting protein-protein interaction, contact prediction, and so on.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GLProteinï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨è›‹ç™½è´¨é¢„è®­ç»ƒä¸­åŒæ—¶æ•´åˆå…¨å±€ç»“æ„ç›¸ä¼¼æ€§å’Œå±€éƒ¨æ°¨åŸºé…¸ç»†èŠ‚çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡é¢„æµ‹å‡†ç¡®æ€§å¹¶æä¾›æ›´æ·±å±‚çš„ç ”ç©¶æ´å¯Ÿã€‚ç ”ç©¶è€…è®¤ä¸ºè›‹ç™½è´¨ç»“æ„ä¿¡æ¯ä¸ä»…é™äº 3D ç©ºé—´æ•°æ®ï¼Œè¿˜åº”æ¶µç›–ä»æ°¨åŸºé…¸åˆ†å­å±€éƒ¨ä¿¡æ¯åˆ°è›‹ç™½è´¨é—´ç»“æ„ç›¸ä¼¼æ€§å…¨å±€ä¿¡æ¯çš„å®Œæ•´ç»´åº¦ã€‚GLProtein åˆ›æ–°åœ°ç»“åˆäº† Protein-masked modellingã€Triplet structure similarity scoringã€Protein 3D distance encoding ä»¥åŠåŸºäºå­ç»“æ„çš„ Amino acid molecule encoding æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGLProtein åœ¨è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆProtein-protein interactionï¼‰é¢„æµ‹å’Œæ¥è§¦é¢„æµ‹ï¼ˆContact predictionï¼‰ç­‰å¤šä¸ªç”Ÿç‰©ä¿¡æ¯å­¦ä»»åŠ¡ä¸­å‡ä¼˜äºæ­¤å‰çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨è›‹ç™½è´¨è¡¨å¾å­¦ä¹ é¢†åŸŸçš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.06294v2",
      "published_date": "2025-05-17 14:45:13 UTC",
      "updated_date": "2025-08-28 09:38:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:52:01.570670+00:00"
    },
    {
      "arxiv_id": "2505.12020v1",
      "title": "GeoMaNO: Geometric Mamba Neural Operator for Partial Differential Equations",
      "title_zh": "GeoMaNOï¼šé¢å‘åå¾®åˆ†æ–¹ç¨‹çš„å‡ ä½• Mamba ç¥ç»ç®—å­",
      "authors": [
        "Xi Han",
        "Jingwei Zhang",
        "Dimitris Samaras",
        "Fei Hou",
        "Hong Qin"
      ],
      "abstract": "The neural operator (NO) framework has emerged as a powerful tool for solving partial differential equations (PDEs). Recent NOs are dominated by the Transformer architecture, which offers NOs the capability to capture long-range dependencies in PDE dynamics. However, existing Transformer-based NOs suffer from quadratic complexity, lack geometric rigor, and thus suffer from sub-optimal performance on regular grids. As a remedy, we propose the Geometric Mamba Neural Operator (GeoMaNO) framework, which empowers NOs with Mamba's modeling capability, linear complexity, plus geometric rigor. We evaluate GeoMaNO's performance on multiple standard and popularly employed PDE benchmarks, spanning from Darcy flow problems to Navier-Stokes problems. GeoMaNO improves existing baselines in solution operator approximation by as much as 58.9%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Geometric Mamba Neural Operator (GeoMaNO) æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ±‚è§£åå¾®åˆ†æ–¹ç¨‹ (PDEs) çš„æ–°å‹ç¥ç»ç®—å­ (Neural Operator)ã€‚å½“å‰ä¸»æµçš„åŸºäº Transformer çš„ç¥ç»ç®—å­è™½ç„¶èƒ½æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œä½†é¢ä¸´äºŒæ¬¡å¤æ‚åº¦ (quadratic complexity) å’Œç¼ºä¹å‡ ä½•ä¸¥è°¨æ€§ (geometric rigor) çš„æŒ‘æˆ˜ï¼Œåœ¨è§„åˆ™ç½‘æ ¼ä¸Šè¡¨ç°å—é™ã€‚GeoMaNO é€šè¿‡ç»“åˆ Mamba çš„å»ºæ¨¡èƒ½åŠ›ã€çº¿æ€§å¤æ‚åº¦ (linear complexity) ä»¥åŠå‡ ä½•ä¸¥è°¨æ€§ï¼Œä¸ºè¿™äº›é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿåœ¨åŒ…æ‹¬ Darcy flow å’Œ Navier-Stokes é—®é¢˜åœ¨å†…çš„å¤šä¸ªæ ‡å‡† PDE åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†è¯¥æ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGeoMaNO åœ¨è§£ç®—å­è¿‘ä¼¼ (solution operator approximation) æ–¹é¢çš„æ€§èƒ½æ¯”ç°æœ‰åŸºçº¿æ¨¡å‹æé«˜äº†å¤šè¾¾ 58.9%ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†å¤æ‚ç‰©ç†åŠ¨åŠ›å­¦ä¸­çš„å“è¶Šæ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12020v1",
      "published_date": "2025-05-17 14:20:57 UTC",
      "updated_date": "2025-05-17 14:20:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:52:13.633713+00:00"
    },
    {
      "arxiv_id": "2505.12012v1",
      "title": "Empowering Sustainable Finance with Artificial Intelligence: A Framework for Responsible Implementation",
      "title_zh": "äººå·¥æ™ºèƒ½èµ‹èƒ½å¯æŒç»­é‡‘èï¼šè´Ÿè´£ä»»å®æ–½çš„æ¡†æ¶",
      "authors": [
        "Georgios Pavlidis"
      ],
      "abstract": "This chapter explores the convergence of two major developments: the rise of environmental, social, and governance (ESG) investing and the exponential growth of artificial intelligence (AI) technology. The increased demand for diverse ESG instruments, such as green and ESG-linked loans, will be aligned with the rapid growth of the global AI market, which is expected to be worth $1,394.30 billion by 2029. AI can assist in identifying and pricing climate risks, setting more ambitious ESG goals, and advancing sustainable finance decisions. However, delegating sustainable finance decisions to AI poses serious risks, and new principles and rules for AI and ESG investing are necessary to mitigate these risks. This chapter highlights the challenges associated with norm-setting initiatives and stresses the need for the fine-tuning of the principles of legitimacy, oversight and verification, transparency, and explainability. Finally, the chapter contends that integrating AI into ESG non-financial reporting necessitates a heightened sense of responsibility and the establishment of fundamental guiding principles within the spheres of AI and ESG investing.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¯å¢ƒã€ç¤¾ä¼šå’Œæ²»ç†(ESG)æŠ•èµ„ä¸äººå·¥æ™ºèƒ½(AI)æŠ€æœ¯çš„äº¤æ±‡èåˆï¼Œåˆ†æäº†AIåœ¨è¯†åˆ«å’Œå®šä»·æ°”å€™é£é™©(Climate Risks)ã€è®¾å®šESGç›®æ ‡ä»¥åŠæ¨è¿›å¯æŒç»­é‡‘èå†³ç­–æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚ä½œè€…æŒ‡å‡ºï¼Œè™½ç„¶AIèƒ½å¤Ÿä¼˜åŒ–é‡‘èæµç¨‹ï¼Œä½†å°†å†³ç­–æƒå§”æ‰˜ç»™AIä¼šå¸¦æ¥ä¸¥é‡é£é™©ï¼Œå› æ­¤äºŸéœ€å»ºç«‹é…å¥—çš„æ³•å¾‹åŸåˆ™ä¸è§„åˆ™ã€‚æ–‡ä¸­é‡ç‚¹è®¨è®ºäº†è§„èŒƒåˆ¶å®šä¸­çš„æŒ‘æˆ˜ï¼Œå¼ºè°ƒå¿…é¡»å¯¹åˆæ³•æ€§(Legitimacy)ã€ç›‘ç£ä¸éªŒè¯(Oversight and Verification)ã€é€æ˜åº¦(Transparency)åŠå¯è§£é‡Šæ€§(Explainability)ç­‰æ ¸å¿ƒåŸåˆ™è¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚ç ”ç©¶æœ€ç»ˆæè®®ï¼Œåœ¨å°†AIæ•´åˆè‡³ESGéè´¢åŠ¡æŠ¥å‘Š(Non-financial Reporting)çš„è¿‡ç¨‹ä¸­ï¼Œå¿…é¡»ç¡®ç«‹åŸºç¡€æŒ‡å¯¼åŸåˆ™å¹¶æå‡è´£ä»»æ„è¯†ï¼Œä»¥æ„å»ºè´Ÿè´£ä»»çš„AIä¸å¯æŒç»­é‡‘èå®æ–½æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12012v1",
      "published_date": "2025-05-17 14:05:39 UTC",
      "updated_date": "2025-05-17 14:05:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:52:11.481202+00:00"
    },
    {
      "arxiv_id": "2506.06293v1",
      "title": "Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks",
      "title_zh": "åŸºäºå¼‚æ„æ‹“æ‰‘å›¾ç¥ç»ç½‘ç»œçš„é“¶è¡Œä¿¡ç”¨è¯„çº§é¢„æµ‹",
      "authors": [
        "Junyi Liu",
        "Stanley Kok"
      ],
      "abstract": "Agencies such as Standard & Poor's and Moody's provide bank credit ratings that influence economic stability and decision-making by stakeholders. Accurate and timely predictions support informed decision-making, regulatory actions, and investor protection. However, a complete interbank connection graph is often unavailable due to privacy concerns, complicating the direct application of Graph Neural Networks (GNNs) for rating prediction. our research utilizes persistent homology to construct a network that captures relationships among banks and combines this with a traditional lending network to create a heterogeneous network that integrates information from both sources, leading to improved predictions. Experiments on a global, real-world dataset validate the effectiveness of HTGNN. This research has implications for investors and regulatory bodies in enhancing proactive risk mitigation and the implementation of effective market interventions.The code can be find at https://github.com/Liu-Jun-Yi/HTGNN.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é“¶è¡Œä¿¡ç”¨è¯„çº§é¢„æµ‹ä¸­å› éšç§ä¿æŠ¤å¯¼è‡´çš„é“¶è¡Œé—´è¿æ¥å›¾ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºå¼‚æ„æ‹“æ‰‘å›¾ç¥ç»ç½‘ç»œ(HTGNN)çš„é¢„æµ‹æ¨¡å‹ã€‚ç ”ç©¶åˆ›æ–°æ€§åœ°åˆ©ç”¨æŒç»­åŒè°ƒ(persistent homology)æŠ€æœ¯æ„å»ºäº†æ•æ‰é“¶è¡Œé—´æ½œåœ¨å…³ç³»çš„æ‹“æ‰‘ç½‘ç»œï¼Œå¹¶å°†å…¶ä¸ä¼ ç»Ÿå€Ÿè´·ç½‘ç»œèåˆï¼Œæ„å»ºå‡ºä¸€ç§èƒ½å¤Ÿæ•´åˆå¤šæºä¿¡æ¯çš„å¼‚æ„ç½‘ç»œ(heterogeneous network)ã€‚åœ¨å…¨çƒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨ä¿¡ç”¨è¯„çº§é¢„æµ‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†æ•°æ®ä¸å®Œæ•´æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚è¯¥æˆæœä¸ºæŠ•èµ„è€…å’Œç›‘ç®¡æœºæ„æä¾›äº†æ›´ç²¾å‡†çš„é£é™©è¯„ä¼°æ‰‹æ®µï¼Œæœ‰åŠ©äºå¢å¼ºä¸»åŠ¨é£é™©ç¼“è§£å¹¶ä¼˜åŒ–å¸‚åœºå¹²é¢„å†³ç­–ã€‚è¯¥ç ”ç©¶çš„ä»£ç å·²åœ¨GitHubå¼€æºï¼Œä¸ºç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥å¼€å‘æä¾›äº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "WITS 2024 (Workshop on Information Technologies and Systems 2024)",
      "pdf_url": "https://arxiv.org/pdf/2506.06293v1",
      "published_date": "2025-05-17 13:49:25 UTC",
      "updated_date": "2025-05-17 13:49:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:52:20.023862+00:00"
    },
    {
      "arxiv_id": "2505.12006v4",
      "title": "SOCIA-$\\nabla$: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation",
      "title_zh": "SOCIA-$\\nabla$ï¼šèåˆæ–‡æœ¬æ¢¯åº¦ä¸å¤šæ™ºèƒ½ä½“ç¼–æ’çš„è‡ªåŠ¨åŒ–æ¨¡æ‹Ÿå™¨ç”Ÿæˆ",
      "authors": [
        "Yuncheng Hua",
        "Sion Weatherhead",
        "Mehdi Jafari",
        "Hao Xue",
        "Flora D. Salim"
      ],
      "abstract": "In this paper, we present SOCIA-$\\nabla$, an end-to-end, agentic framework that treats simulator construction asinstance optimization over code within a textual computation graph. Specialized LLM-driven agents are embedded as graph nodes, and a workflow manager executes a loss-driven loop: code synthesis -> execution -> evaluation -> code repair. The optimizer performs Textual-Gradient Descent (TGD), while human-in-the-loop interaction is reserved for task-spec confirmation, minimizing expert effort and keeping the code itself as the trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption, and Personal Mobility, SOCIA-$\\nabla$ attains state-of-the-art overall accuracy. By unifying multi-agent orchestration with a loss-aligned optimization view, SOCIA-$\\nabla$ converts brittle prompt pipelines into reproducible, constraint-aware simulator code generation that scales across domains and simulation granularities. We will release the code soon.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† SOCIA-$\\nabla$ï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†æ¨¡æ‹Ÿå™¨æ„å»º (simulator construction) è§†ä¸ºåœ¨æ–‡æœ¬è®¡ç®—å›¾ä¸­çš„ä»£ç å®ä¾‹ä¼˜åŒ–è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶å°†ä¸“é—¨çš„åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æ™ºèƒ½ä½“åµŒå…¥ä¸ºå›¾èŠ‚ç‚¹ï¼Œç”±å·¥ä½œæµç®¡ç†å™¨æ‰§è¡Œæ¶µç›–ä»£ç åˆæˆã€æ‰§è¡Œã€è¯„ä¼°ä¸ä¿®å¤çš„æŸå¤±é©±åŠ¨å¾ªç¯ã€‚å…¶æ ¸å¿ƒä¼˜åŒ–å™¨é‡‡ç”¨æ–‡æœ¬æ¢¯åº¦ä¸‹é™ (Textual-Gradient Descent, TGD) æŠ€æœ¯ï¼Œä»…åœ¨ä»»åŠ¡è§„èŒƒç¡®è®¤é˜¶æ®µå¼•å…¥äººç±»åœ¨ç¯ (human-in-the-loop) äº¤äº’ï¼Œä»è€Œæœ€å°åŒ–ä¸“å®¶å¹²é¢„å¹¶å°†ä»£ç æœ¬èº«ä½œä¸ºå¯è®­ç»ƒå¯¹è±¡ã€‚åœ¨ç”¨æˆ·å»ºæ¨¡ (User Modeling)ã€å£ç½©ä½©æˆ´ (Mask Adoption) å’Œä¸ªäººç§»åŠ¨æ€§ (Personal Mobility) ä¸‰é¡¹ç‰©ç†ä¿¡æ¯ç³»ç»Ÿ (CPS) ä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼ŒSOCIA-$\\nabla$ å‡å–å¾—äº†å½“å‰æœ€å…ˆè¿› (SOTA) çš„ç»¼åˆå‡†ç¡®ç‡ã€‚é€šè¿‡å°†å¤šæ™ºèƒ½ä½“ç¼–æ’ (multi-agent orchestration) ä¸æŸå¤±å¯¹é½çš„ä¼˜åŒ–è§†è§’æœ‰æœºç»“åˆï¼Œè¯¥æ¡†æ¶æˆåŠŸå°†è„†å¼±çš„æç¤ºè¯æµæ°´çº¿è½¬åŒ–ä¸ºå¯é‡ç°ã€å…·å¤‡çº¦æŸæ„è¯†çš„æ¨¡æ‹Ÿå™¨ä»£ç ç”Ÿæˆæ–¹æ¡ˆï¼Œèƒ½å¤Ÿè·¨é¢†åŸŸå’Œæ¨¡æ‹Ÿç²’åº¦è¿›è¡Œæœ‰æ•ˆæ‰©å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 1 figure, 2 tables. The paper is under review",
      "pdf_url": "https://arxiv.org/pdf/2505.12006v4",
      "published_date": "2025-05-17 13:47:31 UTC",
      "updated_date": "2025-11-11 06:15:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:52:21.830730+00:00"
    },
    {
      "arxiv_id": "2505.12005v1",
      "title": "CHRIS: Clothed Human Reconstruction with Side View Consistency",
      "title_zh": "CHRISï¼šå…·å¤‡ä¾§è§†å›¾ä¸€è‡´æ€§çš„ç€è£…äººä½“é‡å»º",
      "authors": [
        "Dong Liu",
        "Yifan Yang",
        "Zixiong Huang",
        "Yuxin Gao",
        "Mingkui Tan"
      ],
      "abstract": "Creating a realistic clothed human from a single-view RGB image is crucial for applications like mixed reality and filmmaking. Despite some progress in recent years, mainstream methods often fail to fully utilize side-view information, as the input single-view image contains front-view information only. This leads to globally unrealistic topology and local surface inconsistency in side views. To address these, we introduce Clothed Human Reconstruction with Side View Consistency, namely CHRIS, which consists of 1) A Side-View Normal Discriminator that enhances global visual reasonability by distinguishing the generated side-view normals from the ground truth ones; 2) A Multi-to-One Gradient Computation (M2O) that ensures local surface consistency. M2O calculates the gradient of a sampling point by integrating the gradients of the nearby points, effectively acting as a smooth operation. Experimental results demonstrate that CHRIS achieves state-of-the-art performance on public benchmarks and outperforms the prior work.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CHRISï¼ˆClothed Human Reconstruction with Side View Consistencyï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä»å•è§†è§’RGBå›¾åƒé‡å»ºçœŸå®ç€è£…äººä½“æ—¶ï¼Œå› ç¼ºä¹ä¾§å‘ä¿¡æ¯å¯¼è‡´çš„å…¨å±€æ‹“æ‰‘ä¸åˆç†åŠå±€éƒ¨è¡¨é¢ä¸ä¸€è‡´é—®é¢˜ã€‚ä¸ºå¢å¼ºå…¨å±€è§†è§‰åˆç†æ€§ï¼ŒCHRISå¼•å…¥äº†Side-View Normal Discriminatorï¼Œé€šè¿‡åŒºåˆ†ç”Ÿæˆçš„ä¾§è§†å›¾æ³•å‘ä¸çœŸå®å€¼æ¥ä¼˜åŒ–é‡å»ºæ•ˆæœã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•è®¾è®¡äº†Multi-to-One Gradient Computationï¼ˆM2Oï¼‰æŠ€æœ¯ï¼Œé€šè¿‡æ•´åˆé‡‡æ ·ç‚¹å‘¨å›´çš„æ¢¯åº¦å®ç°å¹³æ»‘æ“ä½œï¼Œä»è€Œç¡®ä¿äº†å±€éƒ¨è¡¨é¢çš„è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCHRISåœ¨å¤šä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†State-of-the-artæ€§èƒ½ï¼Œå…¶ç”Ÿæˆçš„æ¨¡å‹åœ¨è§†è§‰åˆç†æ€§å’Œç»†èŠ‚ä¸€è‡´æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºä»¥å¾€çš„ç ”ç©¶å·¥ä½œã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICME 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.12005v1",
      "published_date": "2025-05-17 13:41:46 UTC",
      "updated_date": "2025-05-17 13:41:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:52:27.341647+00:00"
    },
    {
      "arxiv_id": "2505.12001v1",
      "title": "Interactional Fairness in LLM Multi-Agent Systems: An Evaluation Framework",
      "title_zh": "LLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„äº’åŠ¨å…¬å¹³ï¼šè¯„ä¼°æ¡†æ¶",
      "authors": [
        "Ruta Binkyte"
      ],
      "abstract": "As large language models (LLMs) are increasingly used in multi-agent systems, questions of fairness should extend beyond resource distribution and procedural design to include the fairness of how agents communicate. Drawing from organizational psychology, we introduce a novel framework for evaluating Interactional fairness encompassing Interpersonal fairness (IF) and Informational fairness (InfF) in LLM-based multi-agent systems (LLM-MAS). We extend the theoretical grounding of Interactional Fairness to non-sentient agents, reframing fairness as a socially interpretable signal rather than a subjective experience. We then adapt established tools from organizational justice research, including Colquitt's Organizational Justice Scale and the Critical Incident Technique, to measure fairness as a behavioral property of agent interaction. We validate our framework through a pilot study using controlled simulations of a resource negotiation task. We systematically manipulate tone, explanation quality, outcome inequality, and task framing (collaborative vs. competitive) to assess how IF influences agent behavior. Results show that tone and justification quality significantly affect acceptance decisions even when objective outcomes are held constant. In addition, the influence of IF vs. InfF varies with context. This work lays the foundation for fairness auditing and norm-sensitive alignment in LLM-MAS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (LLM-MAS) ä¸­æ—¥ç›Šå¢é•¿çš„å…¬å¹³æ€§éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ä¸ªè¯„ä¼°äº¤äº’å…¬å¹³æ€§ (Interactional Fairness) çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å€Ÿé‰´ç»„ç»‡å¿ƒç†å­¦ç†è®ºï¼Œæ¶µç›–äº†äººé™…å…¬å¹³æ€§ (Interpersonal fairness, IF) å’Œä¿¡æ¯å…¬å¹³æ€§ (Informational fairness, InfF)ï¼Œå°†å…¬å¹³æ€§é‡æ–°å®šä¹‰ä¸ºéæ„ŸçŸ¥æ™ºèƒ½ä½“ä¹‹é—´çš„ç¤¾ä¼šè§£é‡Šæ€§ä¿¡å·ã€‚ç ”ç©¶è€…é€‚é…äº† Colquitt ç»„ç»‡å…¬æ­£é‡è¡¨å’Œå…³é”®äº‹ä»¶æŠ€æœ¯ (Critical Incident Technique)ï¼Œå°†å…¬å¹³æ€§ä½œä¸ºæ™ºèƒ½ä½“äº¤äº’çš„è¡Œä¸ºå±æ€§è¿›è¡Œè¡¡é‡ã€‚é€šè¿‡èµ„æºè°ˆåˆ¤ä»»åŠ¡çš„æ¨¡æ‹Ÿå®éªŒï¼Œç ”ç©¶ç³»ç»Ÿè€ƒå¯Ÿäº†è¯­æ°”ã€è§£é‡Šè´¨é‡ã€ç»“æœä¸ç­‰ç¨‹åº¦åŠä»»åŠ¡æ¡†æ¶å¯¹æ™ºèƒ½ä½“è¡Œä¸ºçš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å®¢è§‚ç»“æœä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œè¯­æ°”å’Œè§£é‡Šè´¨é‡ä»æ˜¾è‘—å½±å“æ™ºèƒ½ä½“çš„æ¥å—å†³ç­–ï¼Œä¸” IF ä¸ InfF çš„å½±å“åŠ›éšç¯å¢ƒèƒŒæ™¯è€Œå¼‚ã€‚è¯¥å·¥ä½œä¸º LLM-MAS çš„å…¬å¹³æ€§å®¡è®¡å’Œè§„èŒƒæ•æ„Ÿå¯¹é½ (norm-sensitive alignment) å¥ å®šäº†ç†è®ºä¸å®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.12001v1",
      "published_date": "2025-05-17 13:24:13 UTC",
      "updated_date": "2025-05-17 13:24:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:53:07.764150+00:00"
    },
    {
      "arxiv_id": "2505.13520v1",
      "title": "Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook Question Answering",
      "title_zh": "è¶…è¶Šæ£€ç´¢ï¼šé¢å‘æ•™ç§‘ä¹¦é—®ç­”çš„è”åˆç›‘ç£ä¸å¤šæ¨¡æ€æ–‡æ¡£æ’åº",
      "authors": [
        "Hessa Alawwad",
        "Usman Naseem",
        "Areej Alhothali",
        "Ali Alkhathlan",
        "Amani Jamal"
      ],
      "abstract": "Textbook question answering (TQA) is a complex task, requiring the interpretation of complex multimodal context. Although recent advances have improved overall performance, they often encounter difficulties in educational settings where accurate semantic alignment and task-specific document retrieval are essential. In this paper, we propose a novel approach to multimodal textbook question answering by introducing a mechanism for enhancing semantic representations through multi-objective joint training. Our model, Joint Embedding Training With Ranking Supervision for Textbook Question Answering (JETRTQA), is a multimodal learning framework built on a retriever--generator architecture that uses a retrieval-augmented generation setup, in which a multimodal large language model generates answers. JETRTQA is designed to improve the relevance of retrieved documents in complex educational contexts. Unlike traditional direct scoring approaches, JETRTQA learns to refine the semantic representations of questions and documents through a supervised signal that combines pairwise ranking and implicit supervision derived from answers. We evaluate our method on the CK12-QA dataset and demonstrate that it significantly improves the discrimination between informative and irrelevant documents, even when they are long, complex, and multimodal. JETRTQA outperforms the previous state of the art, achieving a 2.4\\% gain in accuracy on the validation set and 11.1\\% on the test set.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•™æé—®ç­”(Textbook Question Answering, TQA)ä¸­å¤šæ¨¡æ€ä¸Šä¸‹æ–‡è§£è¯»ã€è¯­ä¹‰å¯¹é½åŠæ–‡æ¡£æ£€ç´¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†JETRTQAï¼ˆJoint Embedding Training With Ranking Supervision for Textbook Question Answeringï¼‰æ¡†æ¶ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ¶æ„ï¼Œé€šè¿‡å¤šç›®æ ‡è”åˆè®­ç»ƒæœºåˆ¶æ˜¾è‘—å¢å¼ºäº†è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„ç›´æ¥è¯„åˆ†æ–¹æ³•ä¸åŒï¼ŒJETRTQAç»“åˆäº†æˆå¯¹æ’åº(Pairwise Ranking)ç›‘ç£å’Œæ¥è‡ªç­”æ¡ˆçš„éšå¼ç›‘ç£ä¿¡å·ï¼Œèƒ½å¤Ÿæ›´ç²¾å‡†åœ°ç²¾ç»†åŒ–é—®é¢˜ä¸å¤šæ¨¡æ€æ–‡æ¡£çš„è¯­ä¹‰å‘é‡ã€‚åœ¨CK12-QAæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†é•¿ä¸”å¤æ‚çš„å¤šæ¨¡æ€å†…å®¹æ—¶ï¼Œèƒ½æœ‰æ•ˆåŒºåˆ†ä¿¡æ¯ç›¸å…³ä¸æ— å…³æ–‡æ¡£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒJETRTQAè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æ¨¡å‹ï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æå‡äº†2.4%å’Œ11.1%ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚æ•™è‚²åœºæ™¯ä¸‹çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "14 pages, 16 figure",
      "pdf_url": "https://arxiv.org/pdf/2505.13520v1",
      "published_date": "2025-05-17 13:23:54 UTC",
      "updated_date": "2025-05-17 13:23:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:53:24.962922+00:00"
    },
    {
      "arxiv_id": "2505.11999v1",
      "title": "MRGRP: Empowering Courier Route Prediction in Food Delivery Service with Multi-Relational Graph",
      "title_zh": "MRGRPï¼šå¤šå…³ç³»å›¾èµ‹èƒ½çš„å¤–å–é…é€è·¯å¾„é¢„æµ‹",
      "authors": [
        "Chang Liu",
        "Huan Yan",
        "Hongjie Sui",
        "Haomin Wen",
        "Yuan Yuan",
        "Yuyang Han",
        "Hongsen Liao",
        "Xuetao Ding",
        "Jinghua Hao",
        "Yong Li"
      ],
      "abstract": "Instant food delivery has become one of the most popular web services worldwide due to its convenience in daily life. A fundamental challenge is accurately predicting courier routes to optimize task dispatch and improve delivery efficiency. This enhances satisfaction for couriers and users and increases platform profitability. The current heuristic prediction method uses only limited human-selected task features and ignores couriers preferences, causing suboptimal results. Additionally, existing learning-based methods do not fully capture the diverse factors influencing courier decisions or the complex relationships among them. To address this, we propose a Multi-Relational Graph-based Route Prediction (MRGRP) method that models fine-grained correlations among tasks affecting courier decisions for accurate prediction. We encode spatial and temporal proximity, along with pickup-delivery relationships, into a multi-relational graph and design a GraphFormer architecture to capture these complex connections. We also introduce a route decoder that leverages courier information and dynamic distance and time contexts for prediction, using existing route solutions as references to improve outcomes. Experiments show our model achieves state-of-the-art route prediction on offline data from cities of various sizes. Deployed on the Meituan Turing platform, it surpasses the current heuristic algorithm, reaching a high route prediction accuracy of 0.819, essential for courier and user satisfaction in instant food delivery.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å³æ—¶å¤–å–æœåŠ¡ä¸­éª‘æ‰‹è·¯å¾„é¢„æµ‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º MRGRP çš„å¤šå…³ç³»å›¾è·¯å¾„é¢„æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¯å‘å¼æ–¹æ³•å¿½è§†éª‘æ‰‹åå¥½åŠä»»åŠ¡é—´å¤æ‚å…³è”çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†ç©ºé—´ã€æ—¶é—´é‚»è¿‘æ€§ä»¥åŠå–é€å…³ç³»ç¼–ç ä¸ºå¤šå…³ç³»å›¾ (Multi-Relational Graph)ï¼Œå¹¶é‡‡ç”¨ GraphFormer æ¶æ„æ¥æ•æ‰å…¶ä¸­çš„ç»†ç²’åº¦å…³è”ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªè·¯å¾„è§£ç å™¨ï¼Œç»“åˆéª‘æ‰‹ä¸ªäººä¿¡æ¯ã€åŠ¨æ€è·ç¦»å’Œæ—¶é—´ä¸Šä¸‹æ–‡ï¼Œå¹¶å‚è€ƒå·²æœ‰è·¯å¾„æ–¹æ¡ˆæ¥ä¼˜åŒ–é¢„æµ‹è¾“å‡ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMRGRP åœ¨å¤šç§è§„æ¨¡åŸå¸‚çš„ç¦»çº¿æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº† SOTA æ°´å¹³ã€‚ç›®å‰è¯¥æ¨¡å‹å·²æˆåŠŸéƒ¨ç½²äºç¾å›¢å›¾çµ (Meituan Turing) å¹³å°ï¼Œå…¶å®é™…è·¯å¾„é¢„æµ‹å‡†ç¡®ç‡é«˜è¾¾ 0.819ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¯å‘å¼ç®—æ³•ï¼Œä¸ºæå‡é…é€æ•ˆç‡å’Œç”¨æˆ·æ»¡æ„åº¦æä¾›äº†æ ¸å¿ƒæŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11999v1",
      "published_date": "2025-05-17 13:19:34 UTC",
      "updated_date": "2025-05-17 13:19:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:53:15.300194+00:00"
    },
    {
      "arxiv_id": "2505.13519v2",
      "title": "Continuous Domain Generalization",
      "title_zh": "è¿ç»­åŸŸæ³›åŒ–",
      "authors": [
        "Zekun Cai",
        "Yiheng Yao",
        "Guangji Bai",
        "Renhe Jiang",
        "Xuan Song",
        "Ryosuke Shibasaki",
        "Liang Zhao"
      ],
      "abstract": "Real-world data distributions often shift continuously across multiple latent factors such as time, geography, and socioeconomic contexts. However, existing domain generalization approaches typically treat domains as discrete or as evolving along a single axis (e.g., time). This oversimplification fails to capture the complex, multidimensional nature of real-world variation. This paper introduces the task of Continuous Domain Generalization (CDG), which aims to generalize predictive models to unseen domains defined by arbitrary combinations of continuous variations. We present a principled framework grounded in geometric and algebraic theories, showing that optimal model parameters across domains lie on a low-dimensional manifold. To model this structure, we propose a Neural Lie Transport Operator (NeuralLio), which enables structure-preserving parameter transitions by enforcing geometric continuity and algebraic consistency. To handle noisy or incomplete domain variation descriptors, we introduce a gating mechanism to suppress irrelevant dimensions and a local chart-based strategy for robust generalization. Extensive experiments on synthetic and real-world datasets, including remote sensing, scientific documents, and traffic forecasting, demonstrate that our method significantly outperforms existing baselines in both generalization accuracy and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è¿ç»­åŸŸæ³›åŒ– (Continuous Domain Generalization, CDG) ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³ç°å®ä¸–ç•Œæ•°æ®åˆ†å¸ƒåœ¨æ—¶é—´ã€åœ°ç†å’Œç¤¾ä¼šç»æµç­‰å¤šä¸ªè¿ç»­æ½œåœ¨ç»´åº¦ä¸Šåç§»çš„é—®é¢˜ã€‚ä¼ ç»ŸåŸŸæ³›åŒ–æ–¹æ³•é€šå¸¸å°†åŸŸè§†ä¸ºç¦»æ•£æˆ–å•ä¸€è½´æ¼”åŒ–çš„ï¼Œéš¾ä»¥æ•æ‰å¤æ‚çš„å¤šç»´å˜åŒ–ç‰¹æ€§ã€‚è¯¥è®ºæ–‡åŸºäºå‡ ä½•å’Œä»£æ•°ç†è®ºæå‡ºäº†ä¸€ä¸ªåŸåˆ™æ€§æ¡†æ¶ï¼Œè®ºè¯äº†è·¨åŸŸçš„æœ€ä¼˜æ¨¡å‹å‚æ•°ä½äºä¸€ä¸ªä½ç»´æµå½¢ (manifold) ä¸Šã€‚ä¸ºäº†å»ºæ¨¡è¿™ä¸€ç»“æ„ï¼Œä½œè€…å¼€å‘äº† Neural Lie Transport Operator (NeuralLio)ï¼Œé€šè¿‡å¼ºåˆ¶å‡ ä½•è¿ç»­æ€§å’Œä»£æ•°ä¸€è‡´æ€§æ¥å®ç°ä¿ç»“æ„çš„å‚æ•°è½¬æ¢ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜ç»“åˆäº†é—¨æ§æœºåˆ¶ä»¥æŠ‘åˆ¶æ— å…³ç»´åº¦ï¼Œå¹¶åˆ©ç”¨å±€éƒ¨å›¾è¡¨ (local chart) ç­–ç•¥ç¡®ä¿åœ¨å™ªå£°æˆ–ä¸å®Œæ•´æè¿°ç¬¦ç¯å¢ƒä¸‹çš„ç¨³å¥æ³›åŒ–ã€‚åœ¨é¥æ„Ÿã€ç§‘å­¦æ–‡çŒ®å’Œäº¤é€šé¢„æµ‹ç­‰é¢†åŸŸçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ³›åŒ–å‡†ç¡®ç‡å’Œé²æ£’æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "23 pages, 9 figures. Accepted by NeurIPS25",
      "pdf_url": "https://arxiv.org/pdf/2505.13519v2",
      "published_date": "2025-05-17 12:39:45 UTC",
      "updated_date": "2025-10-29 14:31:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:53:28.414763+00:00"
    },
    {
      "arxiv_id": "2505.11983v2",
      "title": "Online Iterative Self-Alignment for Radiology Report Generation",
      "title_zh": "é¢å‘æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„åœ¨çº¿è¿­ä»£è‡ªå¯¹é½",
      "authors": [
        "Ting Xiao",
        "Lei Shi",
        "Yang Zhang",
        "HaoFeng Yang",
        "Zhe Wang",
        "Chenjia Bai"
      ],
      "abstract": "Radiology Report Generation (RRG) is an important research topic for relieving radiologist' heavy workload. Existing RRG models mainly rely on supervised fine-tuning (SFT) based on different model architectures using data pairs of radiological images and corresponding radiologist-annotated reports. Recent research has shifted focus to post-training improvements, aligning RRG model outputs with human preferences using reinforcement learning (RL). However, the limited data coverage of high-quality annotated data poses risks of overfitting and generalization. This paper proposes a novel Online Iterative Self-Alignment (OISA) method for RRG that consists of four stages: self-generation of diverse data, self-evaluation for multi-objective preference data,self-alignment for multi-objective optimization and self-iteration for further improvement. Our approach allows for generating varied reports tailored to specific clinical objectives, enhancing the overall performance of the RRG model iteratively. Unlike existing methods, our frame-work significantly increases data quality and optimizes performance through iterative multi-objective optimization. Experimental results demonstrate that our method surpasses previous approaches, achieving state-of-the-art performance across multiple evaluation metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ”¾å°„æŠ¥å‘Šç”Ÿæˆ (Radiology Report Generation, RRG) ä¸­é«˜è´¨é‡æ ‡æ³¨æ•°æ®è¦†ç›–æœ‰é™å¯¼è‡´çš„è¿‡æ‹Ÿåˆå’Œæ³›åŒ–é£é™©ï¼Œæå‡ºäº†åœ¨çº¿è¿­ä»£è‡ªå¯¹é½ (Online Iterative Self-Alignment, OISA) æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªç”Ÿæˆ (self-generation)ã€è‡ªè¯„ä¼° (self-evaluation)ã€è‡ªå¯¹é½ (self-alignment) å’Œè‡ªè¿­ä»£ (self-iteration) å››ä¸ªé˜¶æ®µï¼Œå®ç°äº†é’ˆå¯¹ç‰¹å®šä¸´åºŠç›®æ ‡çš„å¤šç›®æ ‡ä¼˜åŒ–ã€‚ä¸ç°æœ‰çš„ç›‘ç£å¾®è°ƒ (Supervised Fine-tuning, SFT) æˆ–å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) æ–¹æ¡ˆç›¸æ¯”ï¼ŒOISA æ˜¾è‘—æå‡äº†è®­ç»ƒæ•°æ®çš„è´¨é‡ï¼Œå¹¶é€šè¿‡å¾ªç¯è¿­ä»£æŒç»­å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚è¿™ç§æ¶æ„èƒ½å¤Ÿç”Ÿæˆç¬¦åˆä¸´åºŠéœ€æ±‚çš„å¤šæ ·åŒ–æŠ¥å‘Šï¼Œæœ‰æ•ˆç¼“è§£äº†å¯¹å¤§è§„æ¨¡ä¸“å®¶æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯„ä»·æŒ‡æ ‡ä¸Šå‡å–å¾—äº†å½“å‰çš„å…ˆè¿›æ°´å¹³ (State-of-the-Art)ï¼Œå±•ç°äº†å“è¶Šçš„å®ç”¨æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACL 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2505.11983v2",
      "published_date": "2025-05-17 12:31:12 UTC",
      "updated_date": "2025-05-20 14:49:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:53:30.744534+00:00"
    },
    {
      "arxiv_id": "2505.11980v1",
      "title": "AoP-SAM: Automation of Prompts for Efficient Segmentation",
      "title_zh": "AoP-SAMï¼šé¢å‘é«˜æ•ˆåˆ†å‰²çš„æç¤ºè‡ªåŠ¨åŒ–",
      "authors": [
        "Yi Chen",
        "Mu-Young Son",
        "Chuanbo Hua",
        "Joo-Young Kim"
      ],
      "abstract": "The Segment Anything Model (SAM) is a powerful foundation model for image segmentation, showing robust zero-shot generalization through prompt engineering. However, relying on manual prompts is impractical for real-world applications, particularly in scenarios where rapid prompt provision and resource efficiency are crucial. In this paper, we propose the Automation of Prompts for SAM (AoP-SAM), a novel approach that learns to generate essential prompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency and usability by eliminating manual input, making it better suited for real-world tasks. Our approach employs a lightweight yet efficient Prompt Predictor model that detects key entities across images and identifies the optimal regions for placing prompt candidates. This method leverages SAM's image embeddings, preserving its zero-shot generalization capabilities without requiring fine-tuning. Additionally, we introduce a test-time instance-level Adaptive Sampling and Filtering mechanism that generates prompts in a coarse-to-fine manner. This notably enhances both prompt and mask generation efficiency by reducing computational overhead and minimizing redundant mask refinements. Evaluations of three datasets demonstrate that AoP-SAM substantially improves both prompt generation efficiency and mask generation accuracy, making SAM more effective for automated segmentation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Segment Anything Model (SAM) åœ¨å®é™…åº”ç”¨ä¸­è¿‡åº¦ä¾èµ–äººå·¥Promptçš„é—®é¢˜ï¼Œæå‡ºäº†AoP-SAMï¼ˆAutomation of Prompts for SAMï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„è‡ªåŠ¨åŒ–åˆ†å‰²ã€‚AoP-SAMå¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§ä¸”é«˜æ•ˆçš„Prompt Predictoræ¨¡å‹ï¼Œé€šè¿‡ç›´æ¥åˆ©ç”¨SAMçš„Image Embeddingsæ¥è‡ªåŠ¨æ£€æµ‹å›¾åƒä¸­çš„å…³é”®å®ä½“å¹¶ç¡®å®šPromptçš„æœ€ä½³æ”¾ç½®åŒºåŸŸã€‚è¿™ç§æ–¹æ³•åœ¨æ— éœ€Fine-tuningçš„å‰æä¸‹ï¼Œå®Œæ•´ä¿ç•™äº†SAMåŸæœ‰çš„Zero-shot Generalizationèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶çš„å®ä¾‹çº§Adaptive Sampling and Filteringæœºåˆ¶ï¼Œé‡‡ç”¨ç”±ç²—åˆ°ç²¾çš„æ–¹å¼ç”ŸæˆPromptã€‚è¯¥æœºåˆ¶é€šè¿‡å‡å°‘è®¡ç®—å¼€é”€å’Œå†—ä½™çš„Mask Refinementï¼Œæ˜¾è‘—æå‡äº†Promptç”Ÿæˆå’ŒMaskç”Ÿæˆçš„æ•ˆç‡ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒAoP-SAMåœ¨æ˜¾è‘—æé«˜Promptç”Ÿæˆæ•ˆç‡çš„åŒæ—¶ï¼Œä¹Ÿå¢å¼ºäº†Maskçš„åˆ†å‰²ç²¾åº¦ï¼Œä½¿SAMèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åº”å¯¹å„ç±»è‡ªåŠ¨åŒ–åˆ†å‰²ä»»åŠ¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at AAAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.11980v1",
      "published_date": "2025-05-17 12:27:36 UTC",
      "updated_date": "2025-05-17 12:27:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:53:26.983809+00:00"
    },
    {
      "arxiv_id": "2505.11979v1",
      "title": "Introduction to Analytical Software Engineering Design Paradigm",
      "title_zh": "åˆ†æå¼è½¯ä»¶å·¥ç¨‹è®¾è®¡èŒƒå¼å¯¼è®º",
      "authors": [
        "Tarik Houichime",
        "Younes El Amrani"
      ],
      "abstract": "As modern software systems expand in scale and complexity, the challenges associated with their modeling and formulation grow increasingly intricate. Traditional approaches often fall short in effectively addressing these complexities, particularly in tasks such as design pattern detection for maintenance and assessment, as well as code refactoring for optimization and long-term sustainability. This growing inadequacy underscores the need for a paradigm shift in how such challenges are approached and resolved. This paper presents Analytical Software Engineering (ASE), a novel design paradigm aimed at balancing abstraction, tool accessibility, compatibility, and scalability. ASE enables effective modeling and resolution of complex software engineering problems. The paradigm is evaluated through two frameworks Behavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR), both developed in accordance with ASE principles. BSS offers a compact, language-agnostic representation of codebases to facilitate precise design pattern detection. ODR unifies artifact and solution representations to optimize code refactoring via heuristic algorithms while eliminating iterative computational overhead. By providing a structured approach to software design challenges, ASE lays the groundwork for future research in encoding and analyzing complex software metrics.",
      "tldr_zh": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåˆ†æå‹è½¯ä»¶å·¥ç¨‹ (Analytical Software Engineering, ASE) çš„æ–°å‹è®¾è®¡èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ç°ä»£è½¯ä»¶ç³»ç»Ÿåœ¨è®¾è®¡æ¨¡å¼æ£€æµ‹ (design pattern detection) å’Œä»£ç é‡æ„ (code refactoring) ç­‰å¤æ‚å»ºæ¨¡ä»»åŠ¡ä¸­ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚ASE èŒƒå¼åŠ›æ±‚åœ¨æŠ½è±¡ã€å·¥å…·å¯è®¿é—®æ€§ã€å…¼å®¹æ€§å’Œå¯æ‰©å±•æ€§ä¹‹é—´å®ç°å¹³è¡¡ï¼Œä»è€Œæœ‰æ•ˆåœ°å»ºæ¨¡å’Œè§£å†³å¤æ‚çš„è½¯ä»¶å·¥ç¨‹é—®é¢˜ã€‚è¯¥ç ”ç©¶é€šè¿‡ä¸¤ä¸ªæ¡†æ¶éªŒè¯äº† ASE åŸåˆ™ï¼šè¡Œä¸º-ç»“æ„åºåˆ— (Behavioral-Structural Sequences, BSS) æä¾›äº†ä¸€ç§è¯­è¨€æ— å…³çš„ç´§å‡‘ä»£ç è¡¨ç¤ºä»¥ä¿ƒè¿›ç²¾ç¡®çš„æ¨¡å¼æ£€æµ‹ï¼Œè€Œä¼˜åŒ–è®¾è®¡é‡æ„ (Optimized Design Refactoring, ODR) åˆ™é€šè¿‡ç»Ÿä¸€è¡¨ç¤ºå’Œå¯å‘å¼ç®—æ³• (heuristic algorithms) ä¼˜åŒ–é‡æ„å¹¶æ¶ˆé™¤è¿­ä»£è®¡ç®—å¼€é”€ã€‚é€šè¿‡æä¾›ç»“æ„åŒ–çš„è½¯ä»¶è®¾è®¡å¤„ç†æ–¹æ¡ˆï¼ŒASE ä¸ºæœªæ¥å¤æ‚è½¯ä»¶åº¦é‡ (software metrics) çš„ç¼–ç ä¸åˆ†æç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.MS",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "The Conference's autorization to submit a preprint was granted",
      "pdf_url": "https://arxiv.org/pdf/2505.11979v1",
      "published_date": "2025-05-17 12:23:55 UTC",
      "updated_date": "2025-05-17 12:23:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:53:58.492015+00:00"
    },
    {
      "arxiv_id": "2505.13518v1",
      "title": "Data Balancing Strategies: A Survey of Resampling and Augmentation Methods",
      "title_zh": "æ•°æ®å¹³è¡¡ç­–ç•¥ï¼šé‡é‡‡æ ·ä¸æ•°æ®å¢å¼ºæ–¹æ³•ç»¼è¿°",
      "authors": [
        "Behnam Yousefimehr",
        "Mehdi Ghatee",
        "Mohammad Amin Seifi",
        "Javad Fazli",
        "Sajed Tavakoli",
        "Zahra Rafei",
        "Shervin Ghaffari",
        "Abolfazl Nikahd",
        "Mahdi Razi Gandomani",
        "Alireza Orouji",
        "Ramtin Mahmoudi Kashani",
        "Sarina Heshmati",
        "Negin Sadat Mousavi"
      ],
      "abstract": "Imbalanced data poses a significant obstacle in machine learning, as an unequal distribution of class labels often results in skewed predictions and diminished model accuracy. To mitigate this problem, various resampling strategies have been developed, encompassing both oversampling and undersampling techniques aimed at modifying class proportions. Conventional oversampling approaches like SMOTE enhance the representation of the minority class, whereas undersampling methods focus on trimming down the majority class. Advances in deep learning have facilitated the creation of more complex solutions, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), which are capable of producing high-quality synthetic examples. This paper reviews a broad spectrum of data balancing methods, classifying them into categories including synthetic oversampling, adaptive techniques, generative models, ensemble-based strategies, hybrid approaches, undersampling, and neighbor-based methods. Furthermore, it highlights current developments in resampling techniques and discusses practical implementations and case studies that validate their effectiveness. The paper concludes by offering perspectives on potential directions for future exploration in this domain.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡æ·±å…¥æ¢è®¨äº†æœºå™¨å­¦ä¹ ä¸­ä¸å¹³è¡¡æ•°æ®(Imbalanced data)å¸¦æ¥çš„é¢„æµ‹åè§å’Œæ¨¡å‹ç²¾åº¦ä¸‹é™é—®é¢˜ã€‚ç ”ç©¶è¯¦ç»†æ¢³ç†äº†æ—¨åœ¨è°ƒæ•´ç±»åˆ«æ¯”ä¾‹çš„å„ç§é‡é‡‡æ ·ç­–ç•¥(Resampling strategies)ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿçš„è¿‡é‡‡æ ·(Oversampling)å’Œæ¬ é‡‡æ ·(Undersampling)æŠ€æœ¯ã€‚è®ºæ–‡æ¶µç›–äº†ä»ä¼ ç»Ÿçš„ SMOTE ç®—æ³•åˆ°åŸºäºæ·±åº¦å­¦ä¹ çš„å¤æ‚è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GANs)å’Œå˜åˆ†è‡ªç¼–ç å™¨(VAEs)ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åˆæˆæ ·æœ¬ã€‚è¯¥ç ”ç©¶å°†æ•°æ®å¹³è¡¡æ–¹æ³•ç»†åˆ†ä¸ºåˆæˆè¿‡é‡‡æ ·ã€è‡ªé€‚åº”æŠ€æœ¯ã€ç”Ÿæˆæ¨¡å‹ã€é›†æˆç­–ç•¥ã€æ··åˆæ–¹æ³•ã€æ¬ é‡‡æ ·ä»¥åŠåŸºäºé‚»åŸŸçš„æ–¹æ³•ç­‰å¤šä¸ªç±»åˆ«ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜é‡ç‚¹ä»‹ç»äº†é‡é‡‡æ ·æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œå¹¶é€šè¿‡å®é™…åº”ç”¨æ¡ˆä¾‹éªŒè¯äº†è¿™äº›æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œè®ºæ–‡ä¸ºè¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†å‰ç»æ€§çš„è§è§£ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13518v1",
      "published_date": "2025-05-17 12:15:28 UTC",
      "updated_date": "2025-05-17 12:15:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:54:02.286408+00:00"
    },
    {
      "arxiv_id": "2506.01989v1",
      "title": "Coded Robust Aggregation for Distributed Learning under Byzantine Attacks",
      "title_zh": "æ‹œå åº­æ”»å‡»ä¸‹åˆ†å¸ƒå¼å­¦ä¹ çš„ç¼–ç é²æ£’èšåˆ",
      "authors": [
        "Chengxi Li",
        "Ming Xiao",
        "Mikael Skoglund"
      ],
      "abstract": "In this paper, we investigate the problem of distributed learning (DL) in the presence of Byzantine attacks. For this problem, various robust bounded aggregation (RBA) rules have been proposed at the central server to mitigate the impact of Byzantine attacks. However, current DL methods apply RBA rules for the local gradients from the honest devices and the disruptive information from Byzantine devices, and the learning performance degrades significantly when the local gradients of different devices vary considerably from each other. To overcome this limitation, we propose a new DL method to cope with Byzantine attacks based on coded robust aggregation (CRA-DL). Before training begins, the training data are allocated to the devices redundantly. During training, in each iteration, the honest devices transmit coded gradients to the server computed from the allocated training data, and the server then aggregates the information received from both honest and Byzantine devices using RBA rules. In this way, the global gradient can be approximately recovered at the server to update the global model. Compared with current DL methods applying RBA rules, the improvement of CRA-DL is attributed to the fact that the coded gradients sent by the honest devices are closer to each other. This closeness enhances the robustness of the aggregation against Byzantine attacks, since Byzantine messages tend to be significantly different from those of honest devices in this case. We theoretically analyze the convergence performance of CRA-DL. Finally, we present numerical results to verify the superiority of the proposed method over existing baselines, showing its enhanced learning performance under Byzantine attacks.",
      "tldr_zh": "æœ¬æ–‡ç ”ç©¶äº†åˆ†å¸ƒå¼å­¦ä¹ (Distributed Learning)ä¸­åº”å¯¹æ‹œå åº­æ”»å‡»(Byzantine attacks)çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿé²æ£’èšåˆè§„åˆ™(RBA rules)åœ¨æœ¬åœ°æ¢¯åº¦å·®å¼‚è¾ƒå¤§æ—¶æ€§èƒ½å—é™çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºç¼–ç é²æ£’èšåˆçš„åˆ†å¸ƒå¼å­¦ä¹ æ¡†æ¶CRA-DLï¼Œé€šè¿‡åœ¨è®¾å¤‡é—´å†—ä½™åˆ†é…è®­ç»ƒæ•°æ®æ¥æå‡é˜²å¾¡èƒ½åŠ›ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¯šå®è®¾å¤‡å‘æœåŠ¡å™¨å‘é€ç»è¿‡ç¼–ç çš„æ¢¯åº¦(coded gradients)ï¼Œä½¿å¾—è¿™äº›æ¢¯åº¦åœ¨æ•°å€¼ä¸Šå½¼æ­¤æ¥è¿‘ï¼Œä»è€Œè®©æœåŠ¡å™¨èƒ½æ›´æœ‰æ•ˆåœ°è¯†åˆ«å¹¶å‰”é™¤æ˜¾è‘—å·®å¼‚çš„æ‹œå åº­æ”»å‡»ä¿¡æ¯ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ›´ç²¾ç¡®åœ°åœ¨æœåŠ¡å™¨ç«¯æ¢å¤å…¨å±€æ¢¯åº¦å¹¶æ›´æ–°æ¨¡å‹ï¼Œæœ‰æ•ˆæå‡äº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚ç†è®ºåˆ†æä¸æ•°å€¼å®éªŒå‡è¯æ˜ï¼ŒCRA-DLåœ¨æ”¶æ•›æ€§èƒ½å’Œå­¦ä¹ æ•ˆæœä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01989v1",
      "published_date": "2025-05-17 12:06:04 UTC",
      "updated_date": "2025-05-17 12:06:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:54:05.813842+00:00"
    },
    {
      "arxiv_id": "2505.11966v1",
      "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier",
      "title_zh": "Solve-Detect-Verifyï¼šåŸºäºçµæ´»ç”Ÿæˆå¼éªŒè¯å™¨çš„æ¨ç†æ—¶æ‰©å±•",
      "authors": [
        "Jianyuan Zhong",
        "Zeju Li",
        "Zhijian Xu",
        "Xiangyu Wen",
        "Kezhi Li",
        "Qiang Xu"
      ],
      "abstract": "Large Language Model (LLM) reasoning for complex tasks inherently involves a trade-off between solution accuracy and computational efficiency. The subsequent step of verification, while intended to improve performance, further complicates this landscape by introducing its own challenging trade-off: sophisticated Generative Reward Models (GenRMs) can be computationally prohibitive if naively integrated with LLMs at test-time, while simpler, faster methods may lack reliability. To overcome these challenges, we introduce FlexiVe, a novel generative verifier that flexibly balances computational resources between rapid, reliable fast thinking and meticulous slow thinking using a Flexible Allocation of Verification Budget strategy. We further propose the Solve-Detect-Verify pipeline, an efficient inference-time scaling framework that intelligently integrates FlexiVe, proactively identifying solution completion points to trigger targeted verification and provide focused solver feedback. Experiments show FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces on ProcessBench. Furthermore, on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full approach outperforms baselines like self-consistency in reasoning accuracy and inference efficiency. Our system offers a scalable and effective solution to enhance LLM reasoning at test time.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å‡†ç¡®ç‡ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡éš¾é¢˜ï¼Œæå‡ºäº†FlexiVeè¿™ä¸€æ–°å‹ç”Ÿæˆå¼éªŒè¯å™¨(Generative Verifier)ã€‚FlexiVeé€šè¿‡çµæ´»çš„éªŒè¯é¢„ç®—åˆ†é…ç­–ç•¥(Flexible Allocation of Verification Budget)ï¼Œåœ¨å¿«é€Ÿå¯é çš„â€œå¿«æ€è€ƒâ€ä¸ç»†è‡´çš„â€œæ…¢æ€è€ƒâ€ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå…‹æœäº†ä¼ ç»Ÿç”Ÿæˆå¥–åŠ±æ¨¡å‹(GenRMs)è®¡ç®—æˆæœ¬è¿‡é«˜æˆ–å¯é æ€§ä¸è¶³çš„å±€é™ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥æå‡ºäº†Solve-Detect-Verifyæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨ç†æ—¶é—´æ‰©å±•(Inference-Time Scaling)æµç¨‹ï¼Œèƒ½ä¸»åŠ¨è¯†åˆ«è§£é¢˜å®Œæˆç‚¹å¹¶è§¦å‘é’ˆå¯¹æ€§éªŒè¯ä¸åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlexiVeåœ¨ProcessBenchä¸Šçš„é”™è¯¯å®šä½å‡†ç¡®ç‡è¡¨ç°å“è¶Šï¼Œä¸”åœ¨AIME 2024ã€AIME 2025å’ŒCNMOç­‰æŒ‘æˆ˜æ€§æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†å‡†ç¡®ç‡å’Œæ¨æ–­æ•ˆç‡ä¸Šå‡ä¼˜äºSelf-Consistencyç­‰åŸºçº¿æ¨¡å‹ã€‚è¯¥ç³»ç»Ÿä¸ºå¢å¼ºLLMåœ¨æµ‹è¯•æ—¶çš„æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11966v1",
      "published_date": "2025-05-17 11:41:44 UTC",
      "updated_date": "2025-05-17 11:41:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:54:17.828979+00:00"
    },
    {
      "arxiv_id": "2505.11963v2",
      "title": "MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models",
      "title_zh": "MARVELï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“RTLæ¼æ´æå–",
      "authors": [
        "Luca Collini",
        "Baleegh Ahmad",
        "Joey Ah-kiow",
        "Ramesh Karri"
      ],
      "abstract": "Hardware security verification is a challenging and time-consuming task. For this purpose, design engineers may utilize tools such as formal verification, linters, and functional simulation tests, coupled with analysis and a deep understanding of the hardware design being inspected. Large Language Models (LLMs) have been used to assist during this task, either directly or in conjunction with existing tools. We improve the state of the art by proposing MARVEL, a multi-agent LLM framework for a unified approach to decision-making, tool use, and reasoning. MARVEL mimics the cognitive process of a designer looking for security vulnerabilities in RTL code. It consists of a supervisor agent that devises the security policy of the system-on-chips (SoCs) using its security documentation. It delegates tasks to validate the security policy to individual executor agents. Each executor agent carries out its assigned task using a particular strategy. Each executor agent may use one or more tools to identify potential security bugs in the design and send the results back to the supervisor agent for further analysis and confirmation. MARVEL includes executor agents that leverage formal tools, linters, simulation tests, LLM-based detection schemes, and static analysis-based checks. We test our approach on a known buggy SoC based on OpenTitan from the Hack@DATE competition. We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MARVELï¼Œä¸€ç§ç”¨äºç¡¬ä»¶å®‰å…¨éªŒè¯çš„å¤šæ™ºèƒ½ä½“Large Language Models (LLMs)æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°Register Transfer Level (RTL)ä»£ç ä¸­å®‰å…¨æ¼æ´æå–çš„è‡ªåŠ¨åŒ–ä¸æ™ºèƒ½åŒ–ã€‚è¯¥æ¡†æ¶æ¨¡æ‹Ÿä¸“ä¸šè®¾è®¡äººå‘˜çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œé€šè¿‡Supervisor Agentæ ¹æ®å®‰å…¨æ–‡æ¡£åˆ¶å®šSystem-on-Chips (SoCs)çš„å®‰å…¨ç­–ç•¥ï¼Œå¹¶å°†å…¶å§”æ´¾ç»™å¤šä¸ªExecutor Agentsæ‰§è¡Œã€‚æ¯ä¸ªExecutor Agentæ ¹æ®ç‰¹å®šç­–ç•¥ï¼Œç»¼åˆåˆ©ç”¨Formal Verificationã€Lintersã€åŠŸèƒ½ä»¿çœŸæµ‹è¯•ã€åŸºäºLLMçš„æ£€æµ‹æ–¹æ¡ˆåŠé™æ€åˆ†æå·¥å…·æ¥è¯†åˆ«æ½œåœ¨å®‰å…¨ç¼ºé™·ï¼Œå¹¶å°†ç»“æœåé¦ˆè‡³Supervisor Agentè¿›è¡Œæ·±å…¥åˆ†æä¸ç¡®è®¤ã€‚å®éªŒåœ¨Hack@DATEç«èµ›ä¸­åŸºäºOpenTitançš„ç¼ºé™·SoCä¸Šè¿›è¡Œäº†éªŒè¯ï¼ŒMARVELæˆåŠŸæŠ¥å‘Šäº†48ä¸ªé—®é¢˜ï¼Œå…¶ä¸­20ä¸ªè¢«ç¡®è®¤ä¸ºçœŸå®çš„å®‰å…¨æ¼æ´ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ™ºèƒ½ä½“åä½œåœ¨ç»Ÿä¸€å†³ç­–ã€å·¥å…·ä½¿ç”¨å’Œç¡¬ä»¶å®‰å…¨æ¨ç†æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Submitted for Peer Review",
      "pdf_url": "https://arxiv.org/pdf/2505.11963v2",
      "published_date": "2025-05-17 11:31:24 UTC",
      "updated_date": "2025-06-09 01:58:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:54:50.326879+00:00"
    },
    {
      "arxiv_id": "2505.11962v1",
      "title": "CrafText Benchmark: Advancing Instruction Following in Complex Multimodal Open-Ended World",
      "title_zh": "CrafText åŸºå‡†ï¼šæ¨è¿›å¤æ‚å¤šæ¨¡æ€å¼€æ”¾ä¸–ç•Œä¸­çš„æŒ‡ä»¤éµå¾ª",
      "authors": [
        "Zoya Volovikova",
        "Gregory Gorbov",
        "Petr Kuderov",
        "Aleksandr I. Panov",
        "Alexey Skrynnik"
      ],
      "abstract": "Following instructions in real-world conditions requires the ability to adapt to the world's volatility and entanglement: the environment is dynamic and unpredictable, instructions can be linguistically complex with diverse vocabulary, and the number of possible goals an agent may encounter is vast. Despite extensive research in this area, most studies are conducted in static environments with simple instructions and a limited vocabulary, making it difficult to assess agent performance in more diverse and challenging settings. To address this gap, we introduce CrafText, a benchmark for evaluating instruction following in a multimodal environment with diverse instructions and dynamic interactions. CrafText includes 3,924 instructions with 3,423 unique words, covering Localization, Conditional, Building, and Achievement tasks. Additionally, we propose an evaluation protocol that measures an agent's ability to generalize to novel instruction formulations and dynamically evolving task configurations, providing a rigorous test of both linguistic understanding and adaptive decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†CrafTextï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¼€æ”¾ä¸–ç•Œä¸­æŒ‡ä»¤éµå¾ª(Instruction Following)èƒ½åŠ›çš„å…¨æ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶åœ¨é™æ€ç¯å¢ƒå’Œç®€å•æŒ‡ä»¤ä¸‹çš„å±€é™æ€§ã€‚CrafTextæä¾›äº†ä¸€ä¸ªå…·æœ‰å¤šæ ·åŒ–æŒ‡ä»¤å’ŒåŠ¨æ€äº¤äº’çš„å¤æ‚ç¯å¢ƒï¼ŒåŒ…å«è¦†ç›–å®šä½(Localization)ã€æ¡ä»¶(Conditional)ã€å»ºç­‘(Building)å’Œæˆå°±(Achievement)å››å¤§ç±»ä»»åŠ¡çš„3,924æ¡æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€å¥—ä¸“é—¨çš„è¯„ä¼°åè®®ï¼Œç”¨äºè¡¡é‡æ™ºèƒ½ä½“åœ¨é¢å¯¹æ–°å‹æŒ‡ä»¤è¡¨è¿°å’ŒåŠ¨æ€æ¼”åŒ–ä»»åŠ¡é…ç½®æ—¶çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¯¹è¯­è¨€ç†è§£å’Œè‡ªé€‚åº”å†³ç­–èƒ½åŠ›çš„ä¸¥è°¨æµ‹è¯•ï¼Œè¯¥åŸºå‡†ä¸ºè¯„ä¼°æ™ºèƒ½ä½“åœ¨å¤šå˜ä¸”å¤æ‚çš„ç°å®ç¯å¢ƒä¸­çš„è¡¨ç°æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11962v1",
      "published_date": "2025-05-17 11:25:46 UTC",
      "updated_date": "2025-05-17 11:25:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:54:39.880769+00:00"
    },
    {
      "arxiv_id": "2505.11953v2",
      "title": "Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning",
      "title_zh": "æ¢ç©¶æå‡å¤§è¯­è¨€æ¨¡å‹é—å¿˜å­¦ä¹ æ•ˆæœçš„æŸå¤±é‡åŠ æƒå‡†åˆ™",
      "authors": [
        "Puning Yang",
        "Qizhou Wang",
        "Zhuo Huang",
        "Tongliang Liu",
        "Chengqi Zhang",
        "Bo Han"
      ],
      "abstract": "Loss reweighting has shown significant benefits for machine unlearning with large language models (LLMs). However, their exact functionalities are left unclear and the optimal strategy remains an open question, thus impeding the understanding and improvement of existing methodologies. In this paper, we identify two distinct goals of loss reweighting, namely, Saturation and Importance -- the former indicates that those insufficiently optimized data should be emphasized, while the latter stresses some critical data that are most influential for loss minimization. To study their usefulness, we design specific reweighting strategies for each goal and evaluate their respective effects on unlearning. We conduct extensive empirical analyses on well-established benchmarks, and summarize some important observations as follows: (i) Saturation enhances efficacy more than importance-based reweighting, and their combination can yield additional improvements. (ii) Saturation typically allocates lower weights to data with lower likelihoods, whereas importance-based reweighting does the opposite. (iii) The efficacy of unlearning is also largely influenced by the smoothness and granularity of the weight distributions. Based on these findings, we propose SatImp, a simple reweighting method that combines the advantages of both saturation and importance. Empirical results on extensive datasets validate the efficacy of our method, potentially bridging existing research gaps and indicating directions for future research. Our code is available at https://github.com/tmlr-group/SatImp.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é—å¿˜å­¦ä¹ ï¼ˆUnlearningï¼‰ä¸­é€šè¿‡æŸå¤±é‡åŠ æƒï¼ˆLoss Reweightingï¼‰æ¥æå‡æ•ˆæœçš„æ ‡å‡†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åŠŸèƒ½å°šä¸æ˜ç¡®ä¸”ç¼ºä¹æœ€ä¼˜ç­–ç•¥çš„é—®é¢˜ã€‚è®ºæ–‡ç¡®å®šäº†æŸå¤±é‡åŠ æƒçš„ä¸¤ä¸ªä¸åŒç›®æ ‡ï¼šé¥±å’Œåº¦ï¼ˆSaturationï¼‰å’Œé‡è¦æ€§ï¼ˆImportanceï¼‰ï¼Œå‰è€…ä¾§é‡äºå¼ºè°ƒä¼˜åŒ–ä¸è¶³çš„æ•°æ®ï¼Œåè€…åˆ™ä¾§é‡äºå¯¹æŸå¤±æœ€å°åŒ–æœ€å…·å½±å“åŠ›çš„å…³é”®æ•°æ®ã€‚é€šè¿‡ä¸ºæ¯ä¸ªç›®æ ‡è®¾è®¡ç‰¹å®šçš„é‡åŠ æƒç­–ç•¥ï¼Œç ”ç©¶å‘ç°é¥±å’Œåº¦ï¼ˆSaturationï¼‰åœ¨æå‡é—å¿˜å­¦ä¹ åŠŸæ•ˆæ–¹é¢ä¼˜äºåŸºäºé‡è¦æ€§çš„é‡åŠ æƒï¼Œä¸”ä¸¤è€…çš„ç»“åˆèƒ½å¸¦æ¥è¿›ä¸€æ­¥çš„æå‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜é—å¿˜å­¦ä¹ çš„æ•ˆæœè¿˜å—åˆ°æƒé‡åˆ†å¸ƒçš„å¹³æ»‘åº¦ï¼ˆSmoothnessï¼‰å’Œç²’åº¦ï¼ˆGranularityï¼‰çš„æ˜¾è‘—å½±å“ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œä½œè€…æå‡ºäº†åä¸º SatImp çš„ç®€å•é‡åŠ æƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆèåˆäº†é¥±å’Œåº¦å’Œé‡è¦æ€§çš„åŒé‡ä¼˜åŠ¿ã€‚åœ¨å¹¿æ³›æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœéªŒè¯äº† SatImp çš„æœ‰æ•ˆæ€§ï¼Œä¸ä»…å¡«è¡¥äº†ç°æœ‰ç ”ç©¶ç©ºç™½ï¼Œä¹Ÿä¸ºæœªæ¥ LLM Unlearning çš„ä¼˜åŒ–æ–¹å‘æä¾›äº†é‡è¦å¯ç¤ºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11953v2",
      "published_date": "2025-05-17 10:41:22 UTC",
      "updated_date": "2025-05-28 03:33:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:54:47.749750+00:00"
    },
    {
      "arxiv_id": "2505.11946v1",
      "title": "Let's have a chat with the EU AI Act",
      "title_zh": "ä¸æ¬§ç›Ÿã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹å¯¹è¯",
      "authors": [
        "Adam Kovari",
        "Yasin Ghafourian",
        "Csaba Hegedus",
        "Belal Abu Naim",
        "Kitti Mezei",
        "Pal Varga",
        "Markus Tauber"
      ],
      "abstract": "As artificial intelligence (AI) regulations evolve and the regulatory landscape develops and becomes more complex, ensuring compliance with ethical guidelines and legal frameworks remains a challenge for AI developers. This paper introduces an AI-driven self-assessment chatbot designed to assist users in navigating the European Union AI Act and related standards. Leveraging a Retrieval-Augmented Generation (RAG) framework, the chatbot enables real-time, context-aware compliance verification by retrieving relevant regulatory texts and providing tailored guidance. By integrating both public and proprietary standards, it streamlines regulatory adherence, reduces complexity, and fosters responsible AI development. The paper explores the chatbot's architecture, comparing naive and graph-based RAG models, and discusses its potential impact on AI governance.",
      "tldr_zh": "éšç€äººå·¥æ™ºèƒ½æ³•è§„æ—¥ç›Šå¤æ‚ï¼Œç¡®ä¿åˆè§„æ€§å¯¹å¼€å‘è€…è€Œè¨€æ˜¯ä¸€é¡¹è‰°å·¨æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æ¨å‡ºäº†ä¸€æ¬¾äººå·¥æ™ºèƒ½é©±åŠ¨çš„è‡ªæˆ‘è¯„ä¼°èŠå¤©æœºå™¨äººï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å¯¼èˆª EU AI Act åŠç›¸å…³æ ‡å‡†ã€‚è¯¥å·¥å…·åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) æ¡†æ¶ï¼Œé€šè¿‡è°ƒå–ç›¸å…³ç›‘ç®¡æ¡æ–‡å¹¶æä¾›é‡èº«å®šåˆ¶çš„æŒ‡å¯¼ï¼Œå®ç°å®æ—¶ã€å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆè§„æ€§éªŒè¯ã€‚é€šè¿‡æ•´åˆå…¬å…±ä¸ç§æœ‰æ ‡å‡†ï¼Œå®ƒç®€åŒ–äº†ç›‘ç®¡éµå¾ªæµç¨‹ï¼Œé™ä½äº†æ“ä½œéš¾åº¦å¹¶ä¿ƒè¿›äº†è´Ÿè´£ä»»çš„ AI å¼€å‘ã€‚è®ºæ–‡æ·±å…¥æ¢è®¨äº†è¯¥æœºå™¨äººçš„æ¶æ„è®¾è®¡ï¼Œå¹¶å¯¹æ¯”äº† Naive RAG ä¸ Graph-based RAG æ¨¡å‹çš„è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æå‡äº†åˆè§„æ•ˆç‡ï¼Œè¿˜æ·±å…¥è®¨è®ºäº†å…¶åœ¨ AI Governance é¢†åŸŸçš„æ½œåœ¨å½±å“ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CY",
        "cs.DL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11946v1",
      "published_date": "2025-05-17 10:24:08 UTC",
      "updated_date": "2025-05-17 10:24:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:54:50.569101+00:00"
    },
    {
      "arxiv_id": "2507.21060v1",
      "title": "Privacy-Preserving AI for Encrypted Medical Imaging: A Framework for Secure Diagnosis and Learning",
      "title_zh": "é¢å‘åŠ å¯†åŒ»å­¦å½±åƒçš„éšç§ä¿æŠ¤äººå·¥æ™ºèƒ½ï¼šå®‰å…¨è¯Šæ–­ä¸å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Abdullah Al Siam",
        "Sadequzzaman Shohan"
      ],
      "abstract": "The rapid integration of Artificial Intelligence (AI) into medical diagnostics has raised pressing concerns about patient privacy, especially when sensitive imaging data must be transferred, stored, or processed. In this paper, we propose a novel framework for privacy-preserving diagnostic inference on encrypted medical images using a modified convolutional neural network (Masked-CNN) capable of operating on transformed or ciphered image formats. Our approach leverages AES-CBC encryption coupled with JPEG2000 compression to protect medical images while maintaining their suitability for AI inference. We evaluate the system using public DICOM datasets (NIH ChestX-ray14 and LIDC-IDRI), focusing on diagnostic accuracy, inference latency, storage efficiency, and privacy leakage resistance. Experimental results show that the encrypted inference model achieves performance comparable to its unencrypted counterpart, with only marginal trade-offs in accuracy and latency. The proposed framework bridges the gap between data privacy and clinical utility, offering a practical, scalable solution for secure AI-driven diagnostics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—è¯Šæ–­ä¸­AIé›†æˆå¸¦æ¥çš„æ‚£è€…éšç§æ³„éœ²é£é™©ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºåŠ å¯†åŒ»ç–—å›¾åƒéšç§ä¿æŠ¤è¯Šæ–­æ¨ç†çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ”¹è¿›çš„å·ç§¯ç¥ç»ç½‘ç»œ(Masked-CNN)ï¼Œèƒ½å¤Ÿç›´æ¥åœ¨è½¬æ¢æˆ–åŠ å¯†åçš„å›¾åƒæ ¼å¼ä¸Šè¿›è¡Œæ“ä½œã€‚æŠ€æœ¯æ–¹æ¡ˆç»“åˆäº†AES-CBCåŠ å¯†ä¸JPEG2000å‹ç¼©æŠ€æœ¯ï¼Œåœ¨å……åˆ†ä¿æŠ¤åŒ»ç–—å›¾åƒéšç§çš„åŒæ—¶ç¡®ä¿å…¶å…·å¤‡AIæ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨NIH ChestX-ray14å’ŒLIDC-IDRIç­‰å…¬å¼€DICOMæ•°æ®é›†ï¼Œä»è¯Šæ–­å‡†ç¡®æ€§ã€æ¨ç†å»¶è¿Ÿã€å­˜å‚¨æ•ˆç‡å’Œéšç§æ³„éœ²æŠµæŠ—åŠ›ç­‰å¤šç»´åº¦è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŠ å¯†æ¨ç†æ¨¡å‹çš„æ€§èƒ½ä¸éåŠ å¯†å¯¹ç…§ç»„ç›¸å½“ï¼Œä»…åœ¨å‡†ç¡®æ€§å’Œå»¶è¿Ÿä¸Šå­˜åœ¨å¾®å°æƒè¡¡ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆå¼¥åˆäº†æ•°æ®éšç§ä¸ä¸´åºŠå®ç”¨æ€§ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºæ„å»ºå®‰å…¨ã€å¯æ‰©å±•çš„AIé©±åŠ¨è¯Šæ–­ç³»ç»Ÿæä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.21060v1",
      "published_date": "2025-05-17 10:22:31 UTC",
      "updated_date": "2025-05-17 10:22:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:55:01.407730+00:00"
    },
    {
      "arxiv_id": "2505.11942v3",
      "title": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners",
      "title_zh": "LifelongAgentBenchï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ä½œä¸ºç»ˆèº«å­¦ä¹ è€…çš„èƒ½åŠ›",
      "authors": [
        "Junhao Zheng",
        "Xidi Cai",
        "Qiuke Li",
        "Duzhen Zhang",
        "ZhongZhi Li",
        "Yingying Zhang",
        "Le Song",
        "Qianli Ma"
      ],
      "abstract": "Lifelong learning is essential for intelligent agents operating in dynamic environments. Current large language model (LLM)-based agents, however, remain stateless and unable to accumulate or transfer knowledge over time. Existing benchmarks treat agents as static systems and fail to evaluate lifelong learning capabilities. We present LifelongAgentBench, the first unified benchmark designed to systematically assess the lifelong learning ability of LLM agents. It provides skill-grounded, interdependent tasks across three interactive environments, Database, Operating System, and Knowledge Graph, with automatic label verification, reproducibility, and modular extensibility. Extensive experiments reveal that conventional experience replay has limited effectiveness for LLM agents due to irrelevant information and context length constraints. We further introduce a group self-consistency mechanism that significantly improves lifelong learning performance. We hope LifelongAgentBench will advance the development of adaptive, memory-capable LLM agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸‹ä¿æŒæ— çŠ¶æ€ä¸”æ— æ³•ç§¯ç´¯çŸ¥è¯†çš„é—®é¢˜ï¼Œæå‡ºäº†LifelongAgentBenchï¼Œè¿™æ˜¯é¦–ä¸ªç³»ç»Ÿè¯„ä¼°LLMæ™ºèƒ½ä½“ç»ˆèº«å­¦ä¹ (Lifelong Learning)èƒ½åŠ›çš„ç»Ÿä¸€åŸºå‡†ã€‚è¯¥åŸºå‡†æ¶µç›–äº†æ•°æ®åº“(Database)ã€æ“ä½œç³»ç»Ÿ(Operating System)å’ŒçŸ¥è¯†å›¾è°±(Knowledge Graph)ä¸‰ä¸ªäº¤äº’å¼ç¯å¢ƒï¼Œæä¾›å…·æœ‰æŠ€èƒ½åŸºç¡€ä¸”ç›¸äº’ä¾èµ–çš„ä»»åŠ¡ï¼Œå¹¶æ”¯æŒè‡ªåŠ¨æ ‡ç­¾éªŒè¯ã€å¯é‡å¤æ€§å’Œæ¨¡å—åŒ–æ‰©å±•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”±äºæ— å…³ä¿¡æ¯å¹²æ‰°å’Œä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼Œä¼ ç»Ÿçš„ç»éªŒå›æ”¾(Experience Replay)æœºåˆ¶åœ¨LLMæ™ºèƒ½ä½“ä¸Šçš„æ•ˆæœè¾ƒä¸ºæœ‰é™ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§ç¾¤ä½“è‡ªä¸€è‡´æ€§(Group Self-Consistency)æœºåˆ¶ï¼Œæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ä½“åœ¨é•¿å‘¨æœŸä»»åŠ¡ä¸­çš„å­¦ä¹ æ€§èƒ½ã€‚LifelongAgentBenchçš„å‘å¸ƒæ—¨åœ¨æ¨åŠ¨å…·å¤‡è‡ªé€‚åº”å’Œè®°å¿†èƒ½åŠ›çš„ä¸‹ä¸€ä»£LLMæ™ºèƒ½ä½“çš„å‘å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Project Page: https://caixd-220529.github.io/LifelongAgentBench/",
      "pdf_url": "https://arxiv.org/pdf/2505.11942v3",
      "published_date": "2025-05-17 10:09:11 UTC",
      "updated_date": "2025-05-30 02:28:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:55:16.163348+00:00"
    },
    {
      "arxiv_id": "2505.11939v2",
      "title": "Fine-grained Contrastive Learning for ECG-Report Alignment with Waveform Enhancement",
      "title_zh": "ç»“åˆæ³¢å½¢å¢å¼ºçš„ ECG-æŠ¥å‘Šå¯¹é½ç»†ç²’åº¦å¯¹æ¯”å­¦ä¹ ",
      "authors": [
        "Haitao Li",
        "Che Liu",
        "Zhengyao Ding",
        "Ziyi Liu",
        "Wenqi Shao",
        "Zhengxing Huang"
      ],
      "abstract": "Electrocardiograms (ECGs) are essential for diagnosing cardiovascular diseases. However, existing ECG-Report contrastive learning methods focus on whole-ECG and report alignment, missing the link between local ECG features and individual report tags. In this paper, we propose FG-CLEP (Fine-Grained Contrastive Language ECG Pre-training), which achieves fine-grained alignment between specific ECG segments and each tag in the report via tag-specific ECG representations. Furthermore, we found that nearly 55\\% of ECG reports in the MIMIC-ECG training dataset lack detailed waveform features, which hinders fine-grained alignment. To address this, we introduce a coarse-to-fine training process that leverages large language models (LLMs) to recover these missing waveform features and validate the LLM outputs using a coarse model. Additionally, fine-grained alignment at the tag level, rather than at the report level, exacerbates the false negative problem, as different reports may share common tags. To mitigate this, we introduce a semantic similarity matrix to guide the model in identifying and correcting false negatives. Experiments on six datasets demonstrate that FG-CLEP significantly improves fine-grained alignment, outperforming state-of-the-art methods in both zero-shot prediction and linear probing. Meanwhile, the fine-grained reports we generate also enhance the performance of other methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FG-CLEP (Fine-Grained Contrastive Language ECG Pre-training)ï¼Œæ—¨åœ¨é€šè¿‡æ ‡ç­¾ç‰¹å®šçš„ECGè¡¨ç¤ºå®ç°å¿ƒç”µå›¾(ECG)ç‰‡æ®µä¸è¯Šæ–­æŠ¥å‘Šæ ‡ç­¾ä¹‹é—´çš„ç»†ç²’åº¦å¯¹é½ã€‚é’ˆå¯¹MIMIC-ECGæ•°æ®é›†ä¸­çº¦55%çš„æŠ¥å‘Šç¼ºä¹è¯¦ç»†æ³¢å½¢ç‰¹å¾çš„é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†ç”±ç²—åˆ°ç»†(coarse-to-fine)çš„è®­ç»ƒè¿‡ç¨‹ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¢å¤ç¼ºå¤±çš„æ³¢å½¢æè¿°å¹¶è¿›è¡ŒéªŒè¯ã€‚ä¸ºè§£å†³ç»†ç²’åº¦å¯¹é½ä¸­å› å…±äº«æ ‡ç­¾å¯¼è‡´çš„å‡é˜´æ€§(false negative)é—®é¢˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†è¯­ä¹‰ç›¸ä¼¼æ€§çŸ©é˜µ(semantic similarity matrix)æ¥å¼•å¯¼æ¨¡å‹è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFG-CLEPåœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬é¢„æµ‹(zero-shot prediction)å’Œçº¿æ€§æ¢æµ‹(linear probing)è¡¨ç°å‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„ç»†ç²’åº¦æŠ¥å‘Šä¹Ÿæœ‰æ•ˆå¢å¼ºäº†å…¶ä»–å¿ƒç”µå›¾åˆ†ææ–¹æ³•çš„æ€§èƒ½ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11939v2",
      "published_date": "2025-05-17 10:03:06 UTC",
      "updated_date": "2025-09-29 05:21:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:55:12.026031+00:00"
    },
    {
      "arxiv_id": "2505.11936v3",
      "title": "CCD: Continual Consistency Diffusion for Lifelong Generative Modeling",
      "title_zh": "CCDï¼šé¢å‘ç»ˆèº«ç”Ÿæˆå»ºæ¨¡çš„æŒç»­ä¸€è‡´æ€§æ‰©æ•£",
      "authors": [
        "Jingren Liu",
        "Shuning Xu",
        "Yun Wang",
        "Zhong Ji",
        "Xiangyu Chen"
      ],
      "abstract": "While diffusion-based models have shown remarkable generative capabilities in static settings, their extension to continual learning (CL) scenarios remains fundamentally constrained by Generative Catastrophic Forgetting (GCF). We observe that even with a rehearsal buffer, new generative skills often overwrite previous ones, degrading performance on earlier tasks. Although some initial efforts have explored this space, most rely on heuristics borrowed from continual classification methods or use trained diffusion models as ad hoc replay generators, lacking a principled, unified solution to mitigating GCF and often conducting experiments under fragmented and inconsistent settings. To address this gap, we introduce the Continual Diffusion Generation (CDG), a structured pipeline that redefines how diffusion models are implemented under CL and enables systematic evaluation of GCF. Beyond the empirical pipeline, we propose the first theoretical foundation for CDG, grounded in a cross-task analysis of diffusion-specific generative dynamics. Our theoretical investigation identifies three fundamental consistency principles essential for preserving knowledge in the rehearsal buffer over time: inter-task knowledge consistency, unconditional knowledge consistency, and prior knowledge consistency. These criteria expose the latent mechanisms through which generative forgetting manifests across sequential tasks. Motivated by these insights, we further propose \\textit{Continual Consistency Diffusion} (CCD), a principled training framework that enforces these consistency objectives via hierarchical loss functions: $\\mathcal{L}_{IKC}$, $\\mathcal{L}_{UKC}$, and $\\mathcal{L}_{PKC}$. Extensive experiments show that CCD achieves SOTA performance across various benchmarks, especially improving generative metrics in overlapping-task scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(diffusion-based models)åœ¨æŒç»­å­¦ä¹ (Continual Learning)åœºæ™¯ä¸­é¢ä¸´çš„ç”Ÿæˆæ€§ç¾éš¾æ€§é—å¿˜(Generative Catastrophic Forgetting, GCF)é—®é¢˜ï¼Œæå‡ºäº†Continual Diffusion Generation (CDG)ç»“æ„åŒ–æµæ°´çº¿ï¼Œä»¥å®ç°å¯¹GCFçš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚ä½œè€…ä¸ºCDGå»ºç«‹äº†é¦–ä¸ªåŸºäºè·¨ä»»åŠ¡ç”ŸæˆåŠ¨åŠ›å­¦åˆ†æçš„ç†è®ºåŸºç¡€ï¼Œå¹¶è¯†åˆ«å‡ºç»´æŒé‡æ¼”ç¼“å†²åŒº(rehearsal buffer)çŸ¥è¯†çš„ä¸‰ä¸ªæ ¸å¿ƒä¸€è‡´æ€§åŸåˆ™ï¼šè·¨ä»»åŠ¡çŸ¥è¯†ä¸€è‡´æ€§(inter-task knowledge consistency)ã€æ— æ¡ä»¶çŸ¥è¯†ä¸€è‡´æ€§(unconditional knowledge consistency)ä»¥åŠå…ˆéªŒçŸ¥è¯†ä¸€è‡´æ€§(prior knowledge consistency)ã€‚åŸºäºæ­¤ç†è®ºæ´å¯Ÿï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†Continual Consistency Diffusion (CCD)è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚æŸå¤±å‡½æ•°$\\mathcal{L}_{IKC}$ã€$\\mathcal{L}_{UKC}$å’Œ$\\mathcal{L}_{PKC}$æ¥å¼ºåˆ¶æ‰§è¡Œä¸Šè¿°ä¸€è‡´æ€§ç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCCDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œå°¤å…¶åœ¨ä»»åŠ¡é‡å åœºæ™¯ä¸‹æ˜¾è‘—æå‡äº†ç”ŸæˆæŒ‡æ ‡ï¼Œä¸ºç»ˆèº«ç”Ÿæˆå»ºæ¨¡æä¾›äº†åŸåˆ™æ€§çš„ç»Ÿä¸€è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11936v3",
      "published_date": "2025-05-17 09:49:25 UTC",
      "updated_date": "2025-08-22 08:55:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:55:29.995909+00:00"
    },
    {
      "arxiv_id": "2506.01988v1",
      "title": "Surrogate Interpretable Graph for Random Decision Forests",
      "title_zh": "éšæœºå†³ç­–æ£®æ—çš„ä»£ç†å¯è§£é‡Šå›¾",
      "authors": [
        "Akshat Dubey",
        "Aleksandar AnÅ¾el",
        "Georges Hattab"
      ],
      "abstract": "The field of health informatics has been profoundly influenced by the development of random forest models, which have led to significant advances in the interpretability of feature interactions. These models are characterized by their robustness to overfitting and parallelization, making them particularly useful in this domain. However, the increasing number of features and estimators in random forests can prevent domain experts from accurately interpreting global feature interactions, thereby compromising trust and regulatory compliance. A method called the surrogate interpretability graph has been developed to address this issue. It uses graphs and mixed-integer linear programming to analyze and visualize feature interactions. This improves their interpretability by visualizing the feature usage per decision-feature-interaction table and the most dominant hierarchical decision feature interactions for predictions. The implementation of a surrogate interpretable graph enhances global interpretability, which is critical for such a high-stakes domain.",
      "tldr_zh": "åœ¨å¥åº·ä¿¡æ¯åŒ–é¢†åŸŸï¼Œè™½ç„¶ Random Forest æ¨¡å‹å› å…¶é²æ£’æ€§å’Œå¹¶è¡Œæ€§å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†ç‰¹å¾å’Œä¼°è®¡å™¨æ•°é‡çš„å¢åŠ ä½¿å¾—ä¸“å®¶éš¾ä»¥ç†è§£å…¨å±€ç‰¹å¾äº¤äº’ï¼Œè¿›è€Œå½±å“äº†æ¨¡å‹çš„å¯ä¿¡åº¦ä¸åˆè§„æ€§ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Surrogate Interpretable Graph çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ Graphs å’Œ Mixed-Integer Linear Programming æŠ€æœ¯æå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¯è§†åŒ–å†³ç­–-ç‰¹å¾äº¤äº’è¡¨ï¼ˆDecision-Feature-Interaction Tableï¼‰ä¸­çš„ç‰¹å¾ä½¿ç”¨æƒ…å†µï¼Œå¹¶æ­ç¤ºé¢„æµ‹ä¸­èµ·ä¸»å¯¼ä½œç”¨çš„å±‚çº§å†³ç­–ç‰¹å¾äº¤äº’ï¼ˆHierarchical Decision Feature Interactionsï¼‰ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å…¨å±€é€æ˜åº¦ï¼Œä¸ºåŒ»ç–—å¥åº·ç­‰é«˜é£é™©é¢†åŸŸçš„å†³ç­–åˆ†ææä¾›äº†ç›´è§‚æ”¯æŒã€‚Surrogate Interpretable Graph çš„å®æ–½ä¸ä»…è§£å†³äº†å¤æ‚æ¨¡å‹çš„è§£é‡Šéš¾é¢˜ï¼Œä¹Ÿä¸ºç¡®ä¿é«˜é£é™©è¡Œä¸šä¸­çš„æ¨¡å‹ä¿¡ä»»åº¦å’Œç›‘ç®¡åˆè§„æ€§æä¾›äº†é‡è¦ä¿éšœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01988v1",
      "published_date": "2025-05-17 09:44:37 UTC",
      "updated_date": "2025-05-17 09:44:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:55:38.194720+00:00"
    },
    {
      "arxiv_id": "2505.17061v3",
      "title": "Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models",
      "title_zh": "Mixture of Decodingï¼šå—æ³¨æ„åŠ›å¯å‘çš„ç¼“è§£å¤§è§†è§‰è¯­è¨€æ¨¡å‹å¹»è§‰çš„è‡ªé€‚åº”è§£ç ç­–ç•¥",
      "authors": [
        "Xinlong Chen",
        "Yuanxing Zhang",
        "Qiang Liu",
        "Junfei Wu",
        "Fuzheng Zhang",
        "Tieniu Tan"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have exhibited impressive capabilities across various visual tasks, yet they remain hindered by the persistent challenge of hallucinations. To address this critical issue, we propose Mixture of Decoding (MoD), a novel approach for hallucination mitigation that dynamically adapts decoding strategies by evaluating the correctness of the model's attention on image tokens. Specifically, MoD measures the consistency between outputs generated from the original image tokens and those derived from the model's attended image tokens, to distinguish the correctness aforementioned. If the outputs are consistent, indicating correct attention, MoD employs a complementary strategy to amplify critical information. Conversely, if the outputs are inconsistent, suggesting erroneous attention, MoD utilizes a contrastive strategy to suppress misleading information. Extensive experiments demonstrate that MoD significantly outperforms existing decoding methods across multiple mainstream benchmarks, effectively mitigating hallucinations in LVLMs. The code is available at https://github.com/xlchen0205/MoD.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Mixture of Decoding (MoD)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ç¼“è§£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)å¹»è§‰é—®é¢˜çš„åˆ›æ–°è‡ªé€‚åº”è§£ç ç­–ç•¥ã€‚MoDé€šè¿‡è¯„ä¼°æ¨¡å‹å¯¹å›¾åƒæ ‡è®°(image tokens)æ³¨æ„åŠ›çš„æ­£ç¡®æ€§æ¥åŠ¨æ€è°ƒæ•´è§£ç æ–¹å¼ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•é€šè¿‡è¡¡é‡åŸå§‹å›¾åƒæ ‡è®°ç”Ÿæˆçš„è¾“å‡ºä¸æ¨¡å‹å…³æ³¨çš„å›¾åƒæ ‡è®°ç”Ÿæˆçš„è¾“å‡ºä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œæ¥åˆ¤æ–­æ³¨æ„åŠ›åˆ†é…æ˜¯å¦å‡†ç¡®ã€‚å½“è¾“å‡ºä¸€è‡´å³æ³¨æ„åŠ›æ­£ç¡®æ—¶ï¼ŒMoDé‡‡ç”¨è¡¥å……ç­–ç•¥(complementary strategy)æ¥æ”¾å¤§å…³é”®ä¿¡æ¯ï¼›è‹¥è¾“å‡ºä¸ä¸€è‡´å³æ³¨æ„åŠ›æœ‰è¯¯ï¼Œåˆ™åˆ©ç”¨å¯¹æ¯”ç­–ç•¥(contrastive strategy)æ¥æŠ‘åˆ¶è¯¯å¯¼æ€§ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒMoDåœ¨å¤šä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„è§£ç æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†LVLMsä¸­çš„å¹»è§‰ç°è±¡ã€‚è¯¥ç ”ç©¶çš„ä»£ç å·²åœ¨GitHubå¼€æºï¼Œä¸ºæé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§æä¾›äº†æ–°çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Findings of ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17061v3",
      "published_date": "2025-05-17 09:44:18 UTC",
      "updated_date": "2025-06-10 05:05:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:55:43.759388+00:00"
    },
    {
      "arxiv_id": "2505.11933v1",
      "title": "Conversational Recommendation System using NLP and Sentiment Analysis",
      "title_zh": "åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†ä¸æƒ…æ„Ÿåˆ†æçš„ä¼šè¯å¼æ¨èç³»ç»Ÿ",
      "authors": [
        "Piyush Talegaonkar",
        "Siddhant Hole",
        "Shrinesh Kamble",
        "Prashil Gulechha",
        "Deepali Salapurkar"
      ],
      "abstract": "In today's digitally-driven world, the demand for personalized and context-aware recommendations has never been greater. Traditional recommender systems have made significant strides in this direction, but they often lack the ability to tap into the richness of conversational data. This paper represents a novel approach to recommendation systems by integrating conversational insights into the recommendation process. The Conversational Recommender System integrates cutting-edge technologies such as deep learning, leveraging machine learning algorithms like Apriori for Association Rule Mining, Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and Long Short-Term Memory (LTSM). Furthermore, sophisticated voice recognition technologies, including Hidden Markov Models (HMMs) and Dynamic Time Warping (DTW) algorithms, play a crucial role in accurate speech-to-text conversion, ensuring robust performance in diverse environments. The methodology incorporates a fusion of content-based and collaborative recommendation approaches, enhancing them with NLP techniques. This innovative integration ensures a more personalized and context-aware recommendation experience, particularly in marketing applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ¨èç³»ç»Ÿåœ¨åˆ©ç”¨å¯¹è¯æ•°æ®æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè‡ªç„¶è¯­è¨€å¤„ç†(NLP)å’Œæƒ…æ„Ÿåˆ†æ(Sentiment Analysis)çš„å¯¹è¯å¼æ¨èç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé›†æˆäº†ä¸€ç³»åˆ—å…ˆè¿›æŠ€æœ¯ï¼ŒåŒ…æ‹¬åˆ©ç”¨Aprioriç®—æ³•è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜(Association Rule Mining)ï¼Œä»¥åŠé‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)ç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ä¸ºäº†å®ç°å‡†ç¡®çš„è¯­éŸ³è½¬æ–‡æœ¬è½¬æ¢ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†éšé©¬å°”å¯å¤«æ¨¡å‹(HMMs)å’ŒåŠ¨æ€æ—¶é—´è§„æ•´(DTW)ç­‰å¤æ‚çš„è¯­éŸ³è¯†åˆ«æŠ€æœ¯ï¼Œä»¥ç¡®ä¿åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸‹çš„é²æ£’æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡èåˆåŸºäºå†…å®¹(Content-based)å’ŒååŒè¿‡æ»¤(Collaborative Filtering)çš„æ¨èç­–ç•¥ï¼Œå¹¶åˆ©ç”¨NLPæŠ€æœ¯å¯¹å…¶è¿›è¡Œäº†è¿›ä¸€æ­¥å¢å¼ºã€‚è¿™ç§åˆ›æ–°çš„é›†æˆæ–¹æ¡ˆæ˜¾è‘—æå‡äº†æ¨èç³»ç»Ÿçš„ä¸ªæ€§åŒ–ç¨‹åº¦å’Œæƒ…å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦æ·±åº¦ç†è§£ç”¨æˆ·æ„å›¾çš„è¥é”€åº”ç”¨åœºæ™¯ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Presented in ISETE conference (International Conference on Artificial Intelligence, Machine Learning and Big Data Engineering 2024)",
      "pdf_url": "https://arxiv.org/pdf/2505.11933v1",
      "published_date": "2025-05-17 09:36:05 UTC",
      "updated_date": "2025-05-17 09:36:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:56:02.836032+00:00"
    },
    {
      "arxiv_id": "2505.11930v2",
      "title": "The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics",
      "title_zh": "åŸºäºäºŒç»´ä¹˜ç§¯é€»è¾‘çš„æ—¶åºå›¾ç¥ç»ç½‘ç»œé€»è¾‘è¡¨è¾¾èƒ½åŠ›",
      "authors": [
        "Marco SÃ¤lzer",
        "PrzemysÅ‚aw Andrzej WaÅ‚Ä™ga",
        "Martin Lange"
      ],
      "abstract": "In recent years, the expressive power of various neural architectures -- including graph neural networks (GNNs), transformers, and recurrent neural networks -- has been characterised using tools from logic and formal language theory. As the capabilities of basic architectures are becoming well understood, increasing attention is turning to models that combine multiple architectural paradigms. Among them particularly important, and challenging to analyse, are temporal extensions of GNNs, which integrate both spatial (graph-structure) and temporal (evolution over time) dimensions. In this paper, we initiate the study of logical characterisation of temporal GNNs by connecting them to two-dimensional product logics. We show that the expressive power of temporal GNNs depends on how graph and temporal components are combined. In particular, temporal GNNs that apply static GNNs recursively over time can capture all properties definable in the product logic of (past) propositional temporal logic PTL and the modal logic K. In contrast, architectures such as graph-and-time TGNNs and global TGNNs can only express restricted fragments of this logic, where the interaction between temporal and spatial operators is syntactically constrained. These provide us with the first results on the logical expressiveness of temporal GNNs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œ(Temporal GNNs)çš„é€»è¾‘è¡¨è¾¾èƒ½åŠ›ï¼Œé€šè¿‡å°†å…¶ä¸äºŒç»´ä¹˜ç§¯é€»è¾‘(Two-Dimensional Product Logics)è”ç³»èµ·æ¥ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸåœ¨é€»è¾‘ç‰¹å¾æè¿°ä¸Šçš„ç©ºç™½ã€‚ä½œè€…é€šè¿‡åˆ†æç©ºé—´(å›¾ç»“æ„)ä¸æ—¶é—´(éšæ—¶é—´æ¼”å˜)ç»´åº¦çš„æ•´åˆæ–¹å¼ï¼Œæ­ç¤ºäº†ä¸åŒæ¶æ„ç»„åˆå¯¹æ¨¡å‹è¡¨è¾¾èƒ½åŠ›çš„å†³å®šæ€§å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œéšæ—¶é—´é€’å½’åº”ç”¨é™æ€å›¾ç¥ç»ç½‘ç»œçš„Temporal GNNsèƒ½å¤Ÿæ•è·è¿‡å»å‘½é¢˜æ—¶åºé€»è¾‘(PTL)ä¸æ¨¡æ€é€»è¾‘Kçš„ä¹˜ç§¯é€»è¾‘ä¸­æ‰€å®šä¹‰çš„æ‰€æœ‰å±æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGraph-and-Time TGNNså’ŒGlobal TGNNsç­‰æ¶æ„ä»…èƒ½è¡¨è¾¾è¯¥é€»è¾‘çš„å—é™ç‰‡æ®µï¼Œå…¶ä¸­æ—¶ç©ºç®—å­ä¹‹é—´çš„äº¤äº’å—åˆ°è¯­æ³•çº¦æŸã€‚è¿™æ˜¯é¦–ä¸ªå…³äºTemporal GNNsé€»è¾‘è¡¨è¾¾èƒ½åŠ›çš„ç³»ç»Ÿæ€§ç ”ç©¶ç»“æœï¼Œä¸ºç†è§£å’Œè®¾è®¡æ›´å¼ºå¤§çš„æ—¶ç©ºå›¾ç¥ç»æ¨¡å‹æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11930v2",
      "published_date": "2025-05-17 09:34:57 UTC",
      "updated_date": "2025-10-28 09:43:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:56:21.798099+00:00"
    },
    {
      "arxiv_id": "2505.11926v1",
      "title": "SafeVid: Toward Safety Aligned Video Large Multimodal Models",
      "title_zh": "SafeVidï¼šè¿ˆå‘å®‰å…¨å¯¹é½çš„è§†é¢‘å¤§å¤šæ¨¡æ€æ¨¡å‹",
      "authors": [
        "Yixu Wang",
        "Jiaxin Song",
        "Yifeng Gao",
        "Xin Wang",
        "Yang Yao",
        "Yan Teng",
        "Xingjun Ma",
        "Yingchun Wang",
        "Yu-Gang Jiang"
      ],
      "abstract": "As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent complexity introduces significant safety challenges, particularly the issue of mismatched generalization where static safety alignments fail to transfer to dynamic video contexts. We introduce SafeVid, a framework designed to instill video-specific safety principles in VLMMs. SafeVid uniquely transfers robust textual safety alignment capabilities to the video domain by employing detailed textual video descriptions as an interpretive bridge, facilitating LLM-based rule-driven safety reasoning. This is achieved through a closed-loop system comprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific safety preference dataset; 2) targeted alignment of VLMMs using Direct Preference Optimization (DPO); and 3) comprehensive evaluation via our new SafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM safety, with models like LLaVA-NeXT-Video demonstrating substantial improvements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical resources and a structured approach, demonstrating that leveraging textual descriptions as a conduit for safety reasoning markedly improves the safety alignment of VLMMs. We have made SafeVid-350K dataset (https://huggingface.co/datasets/yxwang/SafeVid-350K) publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SafeVidï¼Œä¸€ä¸ªæ—¨åœ¨å¢å¼ºè§†é¢‘å¤§å¤šæ¨¡æ€æ¨¡å‹(Video Large Multimodal Models, VLMMs)å®‰å…¨æ€§çš„æ¡†æ¶ï¼Œä»¥è§£å†³é™æ€å®‰å…¨å¯¹é½éš¾ä»¥è¿ç§»åˆ°åŠ¨æ€è§†é¢‘è¯­å¢ƒä¸­çš„ä¸åŒ¹é…æ³›åŒ–(mismatched generalization)é—®é¢˜ã€‚SafeVidé€šè¿‡å°†è¯¦ç»†çš„è§†é¢‘æ–‡æœ¬æè¿°ä½œä¸ºè§£é‡Šæ¡¥æ¢ï¼ŒæˆåŠŸåœ°å°†æ–‡æœ¬é¢†åŸŸçš„å®‰å…¨å¯¹é½èƒ½åŠ›è½¬ç§»åˆ°è§†é¢‘é¢†åŸŸï¼Œå¹¶å®ç°äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„è§„åˆ™é©±åŠ¨å®‰å…¨æ¨ç†ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†ç»„æˆï¼šåŒ…å«35ä¸‡å¯¹è§†é¢‘å®‰å…¨åå¥½çš„SafeVid-350Kæ•°æ®é›†ã€åˆ©ç”¨ç›´æ¥åå¥½ä¼˜åŒ–(Direct Preference Optimization, DPO)è¿›è¡Œçš„é’ˆå¯¹æ€§æ¨¡å‹å¯¹é½ï¼Œä»¥åŠç”¨äºå…¨é¢è¯„ä¼°çš„SafeVidBenchåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨SafeVid-350Kä¸Šè¿›è¡Œå¯¹é½åæ˜¾è‘—æå‡äº†æ¨¡å‹å®‰å…¨æ€§ï¼Œå…¶ä¸­LLaVA-NeXT-Videoåœ¨SafeVidBenchä¸Šçš„è¡¨ç°æå‡äº†é«˜è¾¾42.39%ã€‚è¯¥ç ”ç©¶ä¸ºVLMMsçš„å®‰å…¨å¯¹é½æä¾›äº†ä¸€å¥—ç³»ç»Ÿçš„èµ„æºå’Œæ–¹æ³•è®ºï¼Œè¯æ˜äº†åˆ©ç”¨æ–‡æœ¬æè¿°ä½œä¸ºæ¨ç†åª’ä»‹èƒ½æ˜¾è‘—æ”¹å–„è§†é¢‘æ¨¡å‹çš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11926v1",
      "published_date": "2025-05-17 09:21:33 UTC",
      "updated_date": "2025-05-17 09:21:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:56:10.191995+00:00"
    },
    {
      "arxiv_id": "2505.11924v2",
      "title": "Intrinsic Self-Correction in LLMs: Towards Explainable Prompting via Mechanistic Interpretability",
      "title_zh": "LLMs çš„å†…åœ¨è‡ªæˆ‘ä¿®æ­£ï¼šé€šè¿‡æœºåˆ¶å¯è§£é‡Šæ€§è¿ˆå‘å¯è§£é‡Šæç¤º",
      "authors": [
        "Yu-Ting Lee",
        "Fu-Chieh Chang",
        "Hui-Ying Shih",
        "Pei-Yuan Wu"
      ],
      "abstract": "Intrinsic self-correction refers to the phenomenon where a language model refines its own outputs purely through prompting, without external feedback or parameter updates. While this approach improves performance across diverse tasks, its internal mechanism remains poorly understood. We analyze intrinsic self-correction from a representation-level perspective. We formalize and introduce the notion of a prompt-induced shift, which is the change in hidden representations caused by a self-correction prompt. Across 5 open-source LLMs, prompt-induced shifts in text detoxification and text toxification align with latent directions constructed from contrastive pairs. In detoxification, the shifts align with the non-toxic direction; in toxification, they align with the toxic direction. These results suggest that intrinsic self-correction functions as representation steering along interpretable latent directions, beyond what standard metrics such as task scores or model confidence capture. Our analysis offers an interpretability-based account of intrinsic self-correction and contributes to a more systematic understanding of LLM prompting.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„å†…åœ¨è‡ªæˆ‘ä¿®æ­£(Intrinsic Self-Correction)ç°è±¡ï¼Œå³æ¨¡å‹ä»…é€šè¿‡æç¤ºè¯(Prompting)è€Œéå¤–éƒ¨åé¦ˆæ¥ä¼˜åŒ–è‡ªèº«è¾“å‡ºã€‚ä½œè€…ä»æœºæ¢°è§£é‡Šæ€§(Mechanistic Interpretability)è§†è§’å‡ºå‘ï¼Œå¼•å…¥äº†â€œæç¤ºè¯±å¯¼åç§»â€(prompt-induced shift)æ¦‚å¿µï¼Œæ—¨åœ¨é‡åŒ–è‡ªä¿®æ­£æç¤ºå¯¹æ¨¡å‹éšè—å±‚è¡¨å¾çš„å½±å“ã€‚é€šè¿‡å¯¹5æ¬¾å¼€æºLLMsåœ¨æ–‡æœ¬å»æ¯’åŒ–å’Œæ¯’åŒ–ä»»åŠ¡ä¸­çš„å®è¯åˆ†æï¼Œç ”ç©¶å‘ç°æç¤ºè¯±å¯¼åç§»ä¸æ½œç©ºé—´ä¸­çš„ç‰¹å®šæ–¹å‘(latent directions)é«˜åº¦ä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå†…åœ¨è‡ªæˆ‘ä¿®æ­£çš„æœ¬è´¨æ˜¯æ²¿ç€å¯è§£é‡Šçš„æ½œç©ºé—´æ–¹å‘è¿›è¡Œè¡¨å¾å¼•å¯¼(representation steering)ï¼Œè€Œéä»…ä½“ç°åœ¨ä»»åŠ¡å¾—åˆ†æˆ–æ¨¡å‹ç½®ä¿¡åº¦çš„å˜åŒ–ä¸Šã€‚è¿™ä¸€å‘ç°ä¸ºLLMçš„è‡ªä¿®æ­£æœºåˆ¶æä¾›äº†è¡¨å¾å±‚é¢çš„åŸç†è§£é‡Šï¼Œä¸ºæ›´ç³»ç»Ÿåœ°ç†è§£æç¤ºè¯å·¥ç¨‹è´¡çŒ®äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11924v2",
      "published_date": "2025-05-17 09:18:37 UTC",
      "updated_date": "2025-10-19 09:03:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:56:38.514523+00:00"
    },
    {
      "arxiv_id": "2505.11912v1",
      "title": "ModÃ¨les de Substitution pour les ModÃ¨les Ã  base d'Agents : Enjeux, MÃ©thodes et Applications",
      "title_zh": "åŸºäºæ™ºèƒ½ä½“æ¨¡å‹çš„ä»£ç†æ¨¡å‹ï¼šæŒ‘æˆ˜ã€æ–¹æ³•ä¸åº”ç”¨",
      "authors": [
        "Paul Saves",
        "Nicolas Verstaevel",
        "BenoÃ®t Gaudou"
      ],
      "abstract": "Multi-agent simulations enables the modeling and analyses of the dynamic behaviors and interactions of autonomous entities evolving in complex environments. Agent-based models (ABM) are widely used to study emergent phenomena arising from local interactions. However, their high computational cost poses a significant challenge, particularly for large-scale simulations requiring extensive parameter exploration, optimization, or uncertainty quantification. The increasing complexity of ABM limits their feasibility for real-time decision-making and large-scale scenario analysis. To address these limitations, surrogate models offer an efficient alternative by learning approximations from sparse simulation data. These models provide cheap-to-evaluate predictions, significantly reducing computational costs while maintaining accuracy. Various machine learning techniques, including regression models, neural networks, random forests and Gaussian processes, have been applied to construct robust surrogates. Moreover, uncertainty quantification and sensitivity analysis play a crucial role in enhancing model reliability and interpretability.\n  This article explores the motivations, methods, and applications of surrogate modeling for ABM, emphasizing the trade-offs between accuracy, computational efficiency, and interpretability. Through a case study on a segregation model, we highlight the challenges associated with building and validating surrogate models, comparing different approaches and evaluating their performance. Finally, we discuss future perspectives on integrating surrogate models within ABM to improve scalability, explainability, and real-time decision support across various fields such as ecology, urban planning and economics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä»£ç†æ¨¡å‹ï¼ˆAgent-Based Models, ABMï¼‰ä¸­åº”ç”¨æ›¿ä»£æ¨¡å‹ï¼ˆSurrogate Modelsï¼‰çš„åŠ¨æœºã€æ–¹æ³•ä¸åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³ ABM åœ¨å¤„ç†å¤§è§„æ¨¡æ¨¡æ‹Ÿå’Œå®æ—¶å†³ç­–æ—¶é¢ä¸´çš„é«˜è®¡ç®—æˆæœ¬æŒ‘æˆ˜ã€‚æ–‡ç« è¯¦ç»†åˆ†æäº†å¦‚ä½•åˆ©ç”¨å›å½’æ¨¡å‹ã€ç¥ç»ç½‘ç»œã€éšæœºæ£®æ—ï¼ˆRandom Forestsï¼‰ä»¥åŠé«˜æ–¯è¿‡ç¨‹ï¼ˆGaussian Processesï¼‰ç­‰æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡å­¦ä¹ ä»¿çœŸæ•°æ®çš„è¿‘ä¼¼å€¼æ¥æä¾›ä½æˆæœ¬ä¸”é«˜ç²¾åº¦çš„é¢„æµ‹ã€‚ç ”ç©¶å¼ºè°ƒäº†ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUncertainty Quantificationï¼‰å’Œæ•æ„Ÿæ€§åˆ†æï¼ˆSensitivity Analysisï¼‰åœ¨å¢å¼ºæ¨¡å‹å¯é æ€§ä¸å¯è§£é‡Šæ€§æ–¹é¢çš„é‡è¦æ€§ã€‚é€šè¿‡å¯¹éš”ç¦»æ¨¡å‹ï¼ˆSegregation Modelï¼‰çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œä½œè€…å±•ç¤ºäº†æ„å»ºä¸éªŒè¯æ›¿ä»£æ¨¡å‹çš„å…·ä½“æŒ‘æˆ˜ï¼Œå¹¶å¯¹å…¶æ€§èƒ½è¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ã€‚æœ€åï¼Œæœ¬æ–‡å±•æœ›äº†å°†æ›¿ä»£æ¨¡å‹ä¸ ABM æ·±åº¦é›†æˆçš„å‰æ™¯ï¼Œä»¥æœŸåœ¨ç”Ÿæ€å­¦ã€åŸå¸‚è§„åˆ’å’Œç»æµå­¦ç­‰é¢†åŸŸæå‡ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ä¸å®æ—¶å†³ç­–æ”¯æŒèƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, in French language. Les 33Ã¨mes JournÃ©es Francophones sur les SystÃ¨mes Multi-Agents (JFSMA 2025). 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.11912v1",
      "published_date": "2025-05-17 08:55:33 UTC",
      "updated_date": "2025-05-17 08:55:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:57:15.712263+00:00"
    },
    {
      "arxiv_id": "2505.11904v1",
      "title": "K*-Means: A Parameter-free Clustering Algorithm",
      "title_zh": "K*-Meansï¼šä¸€ç§æ— å‚æ•°èšç±»ç®—æ³•",
      "authors": [
        "Louis Mahon",
        "Mirella Lapata"
      ],
      "abstract": "Clustering is a widely used and powerful machine learning technique, but its effectiveness is often limited by the need to specify the number of clusters, k, or by relying on thresholds that implicitly determine k. We introduce k*-means, a novel clustering algorithm that eliminates the need to set k or any other parameters. Instead, it uses the minimum description length principle to automatically determine the optimal number of clusters, k*, by splitting and merging clusters while also optimising the standard k-means objective. We prove that k*-means is guaranteed to converge and demonstrate experimentally that it significantly outperforms existing methods in scenarios where k is unknown. We also show that it is accurate in estimating k, and that empirically its runtime is competitive with existing methods, and scales well with dataset size.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†k*-meansï¼Œä¸€ç§æ— éœ€é¢„è®¾å‚æ•°çš„èšç±»ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿk-meansç®—æ³•å¿…é¡»æ‰‹åŠ¨æŒ‡å®šèšç±»æ•°é‡kæˆ–ä¾èµ–éšå«é˜ˆå€¼çš„å±€é™æ€§ã€‚è¯¥ç®—æ³•å¼•å…¥äº†æœ€å°æè¿°é•¿åº¦(Minimum Description Length)åŸåˆ™ï¼Œé€šè¿‡åœ¨ä¼˜åŒ–æ ‡å‡†k-meansç›®æ ‡å‡½æ•°çš„åŒæ—¶è‡ªåŠ¨æ‰§è¡Œèšç±»çš„åˆ†è£‚ä¸åˆå¹¶ï¼Œä»è€Œç¡®å®šæœ€ä¼˜çš„èšç±»æ•°é‡k*ã€‚ç ”ç©¶åœ¨ç†è®ºä¸Šè¯æ˜äº†k*-meansä¿è¯æ”¶æ•›ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨kå€¼æœªçŸ¥çš„æƒ…æ™¯ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥ç®—æ³•åœ¨ä¼°è®¡kå€¼æ–¹é¢éå¸¸å‡†ç¡®ï¼Œè¿è¡Œæ•ˆç‡ä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„æ•°æ®é›†è§„æ¨¡æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11904v1",
      "published_date": "2025-05-17 08:41:07 UTC",
      "updated_date": "2025-05-17 08:41:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:56:22.220194+00:00"
    },
    {
      "arxiv_id": "2505.11899v1",
      "title": "From Recall to Reasoning: Automated Question Generation for Deeper Math Learning through Large Language Models",
      "title_zh": "ä»è®°å¿†åˆ°æ¨ç†ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé¢˜ç›®ï¼ŒåŠ©åŠ›æ·±åº¦æ•°å­¦å­¦ä¹ ",
      "authors": [
        "Yongan Yu",
        "Alexandre Krantz",
        "Nikki G. Lobczowski"
      ],
      "abstract": "Educators have started to turn to Generative AI (GenAI) to help create new course content, but little is known about how they should do so. In this project, we investigated the first steps for optimizing content creation for advanced math. In particular, we looked at the ability of GenAI to produce high-quality practice problems that are relevant to the course content. We conducted two studies to: (1) explore the capabilities of current versions of publicly available GenAI and (2) develop an improved framework to address the limitations we found. Our results showed that GenAI can create math problems at various levels of quality with minimal support, but that providing examples and relevant content results in better quality outputs. This research can help educators decide the ideal way to adopt GenAI in their workflows, to create more effective educational experiences for students.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)è‡ªåŠ¨ç”Ÿæˆé«˜ç­‰æ•°å­¦ç»ƒä¹ é¢˜çš„æ½œåŠ›ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI, GenAI)è¾…åŠ©æ•™è‚²è€…åˆ›å»ºé«˜è´¨é‡è¯¾ç¨‹å†…å®¹ã€‚ç ”ç©¶é€šè¿‡ä¸¤é¡¹å®éªŒè¯„ä¼°äº†ç°æœ‰å…¬å¼€GenAIæ¨¡å‹çš„èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªæ”¹è¿›æ¡†æ¶ä»¥è§£å†³å…¶å±€é™æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶GenAIåœ¨æœ€å°åŒ–æ”¯æŒä¸‹å³å¯ç”Ÿæˆæ•°å­¦é¢˜ç›®ï¼Œä½†é€šè¿‡æä¾›ç¤ºä¾‹(examples)å’Œç›¸å…³èƒŒæ™¯å†…å®¹ï¼Œå…¶è¾“å‡ºè´¨é‡å°†å¾—åˆ°æ˜¾è‘—æå‡ã€‚è¯¥é¡¹ç ”ç©¶ä¸ºæ•™è‚²å·¥ä½œè€…åœ¨æ•™å­¦å·¥ä½œæµä¸­æœ‰æ•ˆæ•´åˆGenAIæä¾›äº†å®è¯ä¾æ®ï¼Œæœ‰åŠ©äºä¼˜åŒ–å­¦ç”Ÿçš„æ•°å­¦æ·±åº¦å­¦ä¹ ä½“éªŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 2 figures, accepted by AIED conference",
      "pdf_url": "https://arxiv.org/pdf/2505.11899v1",
      "published_date": "2025-05-17 08:30:10 UTC",
      "updated_date": "2025-05-17 08:30:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:56:24.095055+00:00"
    },
    {
      "arxiv_id": "2506.01987v1",
      "title": "Equally Critical: Samples, Targets, and Their Mappings in Datasets",
      "title_zh": "åŒç­‰é‡è¦ï¼šæ•°æ®é›†ä¸­çš„æ ·æœ¬ã€ç›®æ ‡åŠå…¶æ˜ å°„",
      "authors": [
        "Runkang Yang",
        "Peng Sun",
        "Xinyi Shang",
        "Yi Tang",
        "Tao Lin"
      ],
      "abstract": "Data inherently possesses dual attributes: samples and targets. For targets, knowledge distillation has been widely employed to accelerate model convergence, primarily relying on teacher-generated soft target supervision. Conversely, recent advancements in data-efficient learning have emphasized sample optimization techniques, such as dataset distillation, while neglected the critical role of target. This dichotomy motivates our investigation into understanding how both sample and target collectively influence training dynamic. To address this gap, we first establish a taxonomy of existing paradigms through the lens of sample-target interactions, categorizing them into distinct sample-to-target mapping strategies. Building upon this foundation, we then propose a novel unified loss framework to assess their impact on training efficiency. Through extensive empirical studies on our proposed strategies, we comprehensively analyze how variations in target and sample types, quantities, and qualities influence model training, providing six key insights to enhance training efficacy.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ•°æ®é›†ä¸­çš„æ ·æœ¬(Samples)ä¸ç›®æ ‡(Targets)åŠå…¶æ˜ å°„å…³ç³»å¯¹æ¨¡å‹è®­ç»ƒæ•ˆç‡çš„å…±åŒå½±å“ã€‚é’ˆå¯¹çŸ¥è¯†è’¸é¦(Knowledge Distillation)ä¾§é‡ç›®æ ‡ä¼˜åŒ–è€Œæ•°æ®é›†è’¸é¦(Dataset Distillation)ä¾§é‡æ ·æœ¬ä¼˜åŒ–çš„ç°çŠ¶ï¼Œä½œè€…ä»æ ·æœ¬-ç›®æ ‡äº¤äº’çš„è§†è§’å»ºç«‹äº†ç°æœ‰èŒƒå¼çš„åˆ†ç±»å­¦ï¼Œå¹¶å°†å…¶å½’çº³ä¸ºä¸åŒçš„æ˜ å°„ç­–ç•¥ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ç»Ÿä¸€æŸå¤±æ¡†æ¶(unified loss framework)ï¼Œç”¨äºè¯„ä¼°ä¸åŒæ˜ å°„ç­–ç•¥å¯¹è®­ç»ƒæ•ˆç‡çš„å®é™…å½±å“ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œè®ºæ–‡å…¨é¢åˆ†æäº†ç›®æ ‡ä¸æ ·æœ¬çš„ç±»å‹ã€æ•°é‡åŠè´¨é‡å¦‚ä½•å½±å“æ¨¡å‹è®­ç»ƒï¼Œæœ€ç»ˆæ€»ç»“å‡ºå…­é¡¹æ—¨åœ¨æå‡è®­ç»ƒæ•ˆæœçš„æ ¸å¿ƒè§è§£ï¼Œæ­ç¤ºäº†æ ·æœ¬ä¸ç›®æ ‡åœ¨æ•°æ®è¡¨å¾ä¸è®­ç»ƒåŠ¨åŠ›å­¦ä¸­çš„åŒç­‰é‡è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01987v1",
      "published_date": "2025-05-17 08:27:19 UTC",
      "updated_date": "2025-05-17 08:27:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:56:35.190984+00:00"
    },
    {
      "arxiv_id": "2505.11896v2",
      "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning",
      "title_zh": "AdaCoTï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¸•ç´¯æ‰˜æœ€ä¼˜è‡ªé€‚åº”æ€ç»´é“¾è§¦å‘",
      "authors": [
        "Chenwei Lou",
        "Zewei Sun",
        "Xinnian Liang",
        "Meng Qu",
        "Wei Shen",
        "Wenqi Wang",
        "Yuntao Li",
        "Qingping Yang",
        "Shuangzhi Wu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AdaCoTï¼Œä¸€ç§æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç®€å•ä»»åŠ¡ä¸­ç›²ç›®ä½¿ç”¨ Chain-of-Thought (CoT) å¯¼è‡´è®¡ç®—æµªè´¹çš„æ–°å‹æ¡†æ¶ã€‚AdaCoT å°†è‡ªé€‚åº”æ¨ç†å»ºæ¨¡ä¸ºä¸€ä¸ª Pareto ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡æƒè¡¡æ¨¡å‹æ€§èƒ½ä¸ CoT è°ƒç”¨çš„é¢‘ç‡åŠè®¡ç®—å¼€é”€æ¥å¯»æ±‚å¹³è¡¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäº Reinforcement Learning (RL) çš„ Proximal Policy Optimization (PPO) ç®—æ³•ï¼Œé€šè¿‡è°ƒèŠ‚æƒ©ç½šç³»æ•°åŠ¨æ€æ§åˆ¶ CoT è§¦å‘è¾¹ç•Œã€‚ç ”ç©¶çš„æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®æ˜¯æå‡ºäº† Selective Loss Masking (SLM)ï¼Œæ—¨åœ¨è§£å†³å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ä¸­çš„å†³ç­–è¾¹ç•Œå´©æºƒé—®é¢˜å¹¶ç¡®ä¿è§¦å‘æœºåˆ¶çš„ç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaCoT åœ¨ç”Ÿäº§æµé‡æµ‹è¯•é›†ä¸­å°† CoT è§¦å‘ç‡é™ä½è‡³ 3.18%ï¼Œå¹¶ä½¿å¹³å‡å“åº” Token å‡å°‘äº† 69.06%ã€‚è¯¥ç ”ç©¶è¯æ˜äº† AdaCoT èƒ½å¤Ÿåœ¨ä¿æŒå¤æ‚ä»»åŠ¡é«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡ LLMs çš„æ¨ç†æ•ˆç‡å¹¶æœ‰æ•ˆå¯¼èˆªè‡³ Pareto å‰æ²¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11896v2",
      "published_date": "2025-05-17 08:27:00 UTC",
      "updated_date": "2025-05-25 13:39:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:57:05.016503+00:00"
    },
    {
      "arxiv_id": "2505.17060v1",
      "title": "SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation",
      "title_zh": "SALMONN-omniï¼šæ— éœ€ç¼–è§£ç å™¨æ³¨å…¥çš„å…¨åŒå·¥å¯¹è¯ç‹¬ç«‹è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Wenyi Yu",
        "Siyin Wang",
        "Xiaoyu Yang",
        "Xianzhao Chen",
        "Xiaohai Tian",
        "Jun Zhang",
        "Guangzhi Sun",
        "Lu Lu",
        "Yuxuan Wang",
        "Chao Zhang"
      ],
      "abstract": "In order to enable fluid and natural human-machine speech interaction, existing full-duplex conversational systems often adopt modular architectures with auxiliary components such as voice activity detectors, interrupters, conversation state predictors, or multiple LLMs. These systems, however, suffer from error accumulation across modules and struggle with key challenges such as context-dependent barge-in and echo cancellation. Recent approaches, most notably Moshi, simplify the pipeline by injecting audio codecs into the token space of a single LLM. However, such methods still incur significant performance degradation when operating on the speech rather than text modality. In this paper, we introduce SALMONN-omni, the first single, standalone full-duplex speech LLM that operates without audio codecs in its token space. It features a novel dynamic thinking mechanism within the LLM backbone, enabling the model to learn when to transition between speaking and listening states. Experiments on widely used benchmarks for spoken question answering and open-domain dialogue show that SALMONN-omni achieves at least 30\\% relative performance improvement over existing open-source full-duplex models and performs highly competitively to half-duplex and turn-based systems, despite using substantially less training data. Moreover, SALMONN-omni demonstrates strong performance in complex conversational scenarios, including turn-taking, backchanneling, echo cancellation and context-dependent barge-in, with further improvements achieved through reinforcement learning. Some demo conversations between user and SALMONN-omni are provided in the following repository https://github.com/bytedance/SALMONN.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SALMONN-omniï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨Tokenç©ºé—´å†…æ— éœ€éŸ³é¢‘ç¼–è§£ç å™¨(Audio Codec)æ³¨å…¥çš„ç‹¬ç«‹å…¨åŒå·¥(Full-duplex)è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹(LLM)ã€‚è¯¥æ¨¡å‹æ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å—åŒ–æ¶æ„ä¸­å­˜åœ¨çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ï¼Œä»¥åŠCodecæ³¨å…¥æ–¹æ³•åœ¨è¯­éŸ³æ¨¡æ€ä¸‹çš„æ€§èƒ½è¡°å‡ã€‚SALMONN-omniåœ¨LLMéª¨å¹²ç½‘ç»œä¸­å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŠ¨æ€æ€ç»´(Dynamic Thinking)æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åœ¨è¯´è¯å’Œè†å¬çŠ¶æ€ä¹‹é—´è‡ªåŠ¨åˆ‡æ¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSALMONN-omniåœ¨è¯­éŸ³é—®ç­”å’Œå¼€æ”¾åŸŸå¯¹è¯åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¯”ç°æœ‰çš„å¼€æºå…¨åŒå·¥æ¨¡å‹æå‡äº†è‡³å°‘30%çš„ç›¸å¯¹æ€§èƒ½ï¼Œä¸”è¡¨ç°ä¼˜äºè®¸å¤šåŠåŒå·¥æˆ–è½®æ¬¡å¼ç³»ç»Ÿã€‚è¯¥æ¨¡å‹åœ¨å¤„ç†è½®æµå‘è¨€(Turn-taking)ã€åé¦ˆè¡¨è¾¾(Backchanneling)ã€å›å£°æ¶ˆé™¤(Echo Cancellation)å’Œä¸Šä¸‹æ–‡ç›¸å…³æ’è¯(Context-dependent barge-in)ç­‰å¤æ‚åœºæ™¯æ—¶è¡¨ç°å¼ºåŠ²ï¼Œå¹¶å¯é€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚è¿™ä¸€æˆæœä¸ºæ„å»ºæ›´æµç•…ã€è‡ªç„¶çš„äººæœºè¯­éŸ³äº¤äº’ç³»ç»Ÿæä¾›äº†é«˜æ•ˆä¸”ç‹¬ç«‹çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17060v1",
      "published_date": "2025-05-17 08:13:59 UTC",
      "updated_date": "2025-05-17 08:13:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:56:43.196657+00:00"
    },
    {
      "arxiv_id": "2505.11893v1",
      "title": "RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving",
      "title_zh": "RLAPï¼šé¢å‘å¤šæ­¥ NLP ä»»åŠ¡æ±‚è§£çš„å¼ºåŒ–å­¦ä¹ å¢å¼ºå‹è‡ªé€‚åº”è§„åˆ’æ¡†æ¶",
      "authors": [
        "Zepeng Ding",
        "Dixuan Wang",
        "Ziqin Luo",
        "Guochao Jiang",
        "Deqing Yang",
        "Jiaqing Liang"
      ],
      "abstract": "Multi-step planning has been widely employed to enhance the performance of large language models (LLMs) on downstream natural language processing (NLP) tasks, which decomposes the original task into multiple subtasks and guide LLMs to solve them sequentially without additional training. When addressing task instances, existing methods either preset the order of steps or attempt multiple paths at each step. However, these methods overlook instances' linguistic features and rely on the intrinsic planning capabilities of LLMs to evaluate intermediate feedback and then select subtasks, resulting in suboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this paper we propose a Reinforcement Learning enhanced Adaptive Planning framework (RLAP). In our framework, we model an NLP task as a Markov decision process (MDP) and employ an LLM directly into the environment. In particular, a lightweight Actor model is trained to estimate Q-values for natural language sequences consisting of states and actions through reinforcement learning. Therefore, during sequential planning, the linguistic features of each sequence in the MDP can be taken into account, and the Actor model interacts with the LLM to determine the optimal order of subtasks for each task instance. We apply RLAP on three different types of NLP tasks and conduct extensive experiments on multiple datasets to verify RLAP's effectiveness and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RLAPï¼Œä¸€ç§å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„è‡ªé€‚åº”è§„åˆ’æ¡†æ¶(Reinforcement Learning enhanced Adaptive Planning)ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è§£å†³å¤šæ­¥è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä»»åŠ¡æ—¶çš„æ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•é¢„è®¾æ­¥éª¤é¡ºåºæˆ–è¿‡åº¦ä¾èµ–LLMså†…åœ¨è§„åˆ’èƒ½åŠ›è€Œå¿½ç•¥è¯­è¨€ç‰¹å¾(linguistic features)çš„é—®é¢˜ï¼ŒRLAPå°†NLPä»»åŠ¡å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§çš„Actoræ¨¡å‹ï¼Œç”¨äºè¯„ä¼°åŒ…å«çŠ¶æ€å’ŒåŠ¨ä½œçš„è‡ªç„¶è¯­è¨€åºåˆ—çš„Q-valuesã€‚åœ¨è§„åˆ’è¿‡ç¨‹ä¸­ï¼ŒActoræ¨¡å‹é€šè¿‡ä¸LLMäº¤äº’æ¥å®æ—¶æ•æ‰åºåˆ—ä¸­çš„è¯­è¨€ç‰¹å¾ï¼Œä»è€Œä¸ºæ¯ä¸ªç‰¹å®šçš„ä»»åŠ¡å®ä¾‹ç¡®å®šæœ€ä¼˜çš„å­ä»»åŠ¡æ‰§è¡Œé¡ºåºã€‚å®éªŒç»“æœåœ¨ä¸‰ç§ä¸åŒç±»å‹çš„NLPä»»åŠ¡åŠå¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†RLAPçš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶èƒ½æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹å¤„ç†å¤æ‚å¤šæ­¥ä»»åŠ¡çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11893v1",
      "published_date": "2025-05-17 08:06:14 UTC",
      "updated_date": "2025-05-17 08:06:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:57:40.955775+00:00"
    },
    {
      "arxiv_id": "2505.11891v2",
      "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents",
      "title_zh": "Mobile-Bench-v2ï¼šé¢å‘åŸºäº VLM çš„ç§»åŠ¨æ™ºèƒ½ä½“çš„æ›´çœŸå®ã€æ›´å…¨é¢çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Weikai Xu",
        "Zhizheng Jiang",
        "Yuxuan Liu",
        "Pengzhi Gao",
        "Wei Liu",
        "Jian Luan",
        "Yuanchun Li",
        "Yunxin Liu",
        "Bin Wang",
        "Bo An"
      ],
      "abstract": "VLM-based mobile agents are increasingly popular due to their capabilities to interact with smartphone GUIs and XML-structured texts and to complete daily tasks. However, existing online benchmarks struggle with obtaining stable reward signals due to dynamic environmental changes. Offline benchmarks evaluate the agents through single-path trajectories, which stands in contrast to the inherently multi-solution characteristics of GUI tasks. Additionally, both types of benchmarks fail to assess whether mobile agents can handle noise or engage in proactive interactions due to a lack of noisy apps or overly full instructions during the evaluation process. To address these limitations, we use a slot-based instruction generation method to construct a more realistic and comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a common task split, with offline multi-path evaluation to assess the agent's ability to obtain step rewards during task execution. It contains a noisy split based on pop-ups and ads apps, and a contaminated split named AITZ-Noise to formulate a real noisy environment. Furthermore, an ambiguous instruction split with preset Q\\&A interactions is released to evaluate the agent's proactive interaction capabilities. We conduct evaluations on these splits using the single-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2, as well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are available at https://huggingface.co/datasets/xwk123/MobileBench-v2.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Mobile-Bench-v2ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åŸºäºVLMçš„ç§»åŠ¨æ™ºèƒ½ä½“è®¾è®¡çš„å…¨é¢ä¸”çœŸå®çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†åœ¨å¥–åŠ±ä¿¡å·ç¨³å®šæ€§ã€å¤šè·¯å¾„è¯„ä¼°ä»¥åŠå™ªå£°å¤„ç†èƒ½åŠ›è¡¡é‡æ–¹é¢çš„å±€é™ã€‚è¯¥åŸºå‡†å¼•å…¥äº†åŸºäºæ§½ä½çš„æŒ‡ä»¤ç”Ÿæˆæ–¹æ³•(slot-based instruction generation)ï¼Œå¹¶æä¾›ç¦»çº¿å¤šè·¯å¾„è¯„ä¼°(offline multi-path evaluation)ä»¥æ›´å‡†ç¡®åœ°è¡¡é‡æ™ºèƒ½ä½“åœ¨æ‰§è¡Œä»»åŠ¡è¿‡ç¨‹ä¸­çš„æ­¥éª¤å¥–åŠ±ã€‚ä¸ºäº†æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å¤æ‚ç¯å¢ƒï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†åŒ…å«å¼¹çª—å’Œå¹¿å‘Šçš„å˜ˆæ‚åˆ’åˆ†(noisy split)ä»¥åŠå—æ±¡æŸ“çš„AITZ-Noiseåˆ’åˆ†ï¼ŒåŒæ—¶é€šè¿‡æ¨¡ç³ŠæŒ‡ä»¤åˆ’åˆ†è¯„ä¼°æ™ºèƒ½ä½“çš„ä¸»åŠ¨äº¤äº’(proactive interaction)èƒ½åŠ›ã€‚å®éªŒå¯¹AppAgent-v1ã€Mobile-Agent-v2ã€UI-Tarså’ŒOS-Atlasç­‰å¤šç§å…ˆè¿›æ™ºèƒ½ä½“è¿›è¡Œäº†å…¨é¢æµ‹è¯•ï¼ŒéªŒè¯äº†è¯¥åŸºå‡†åœ¨è¯„ä¼°ç§»åŠ¨æ™ºèƒ½ä½“é²æ£’æ€§å’Œäº¤äº’æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11891v2",
      "published_date": "2025-05-17 07:58:34 UTC",
      "updated_date": "2025-05-26 09:22:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:57:49.932070+00:00"
    },
    {
      "arxiv_id": "2505.11889v2",
      "title": "Revisiting SSL for sound event detection: complementary fusion and adaptive post-processing",
      "title_zh": "å†æ¢ç”¨äºå£°éŸ³äº‹ä»¶æ£€æµ‹çš„è‡ªç›‘ç£å­¦ä¹ ï¼šäº’è¡¥èåˆä¸è‡ªé€‚åº”åå¤„ç†",
      "authors": [
        "Hanfang Cui",
        "Longfei Song",
        "Li Li",
        "Dongxing Xu",
        "Yanhua Long"
      ],
      "abstract": "Self-supervised learning (SSL) models offer powerful representations for sound event detection (SED), yet their synergistic potential remains underexplored. This study systematically evaluates state-of-the-art SSL models to guide optimal model selection and integration for SED. We propose a framework that combines heterogeneous SSL representations (e.g., BEATs, HuBERT, WavLM) through three fusion strategies: individual SSL embedding integration, dual-modal fusion, and full aggregation. Experiments on the DCASE 2023 Task 4 Challenge reveal that dual-modal fusion (e.g., CRNN+BEATs+WavLM) achieves complementary performance gains, while CRNN+BEATs alone delivers the best results among individual SSL models. We further introduce normalized sound event bounding boxes (nSEBBs), an adaptive post-processing method that dynamically adjusts event boundary predictions, improving PSDS1 by up to 4% for standalone SSL models. These findings highlight the compatibility and complementarity of SSL architectures, providing guidance for task-specific fusion and robust SED system design.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°å®¡è§†äº†è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning, SSL)åœ¨å£°éŸ³äº‹ä»¶æ£€æµ‹(SED)ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æŒ–æ˜ä¸åŒé¢„è®­ç»ƒæ¨¡å‹é—´çš„ååŒæ½œåŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªæ•´åˆå¼‚æ„SSLè¡¨å¾ï¼ˆå¦‚BEATs, HuBERT, WavLMï¼‰çš„æ¡†æ¶ï¼Œç³»ç»Ÿè¯„ä¼°äº†å•æ¨¡å‹é›†æˆã€åŒæ¨¡èåˆå’Œå…¨èšåˆä¸‰ç§èåˆç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨DCASE 2023 Task 4æŒ‘æˆ˜èµ›ä¸­ï¼ŒåŒæ¨¡èåˆï¼ˆå¦‚CRNN+BEATs+WavLMï¼‰å®ç°äº†æ˜¾è‘—çš„äº’è¡¥æ€§èƒ½å¢ç›Šï¼Œè€ŒCRNN+BEATsåœ¨å•SSLæ¨¡å‹é›†æˆä¸­æ•ˆæœæœ€ä¼˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å½’ä¸€åŒ–å£°éŸ³äº‹ä»¶è¾¹ç•Œæ¡†(normalized sound event bounding boxes, nSEBBs)è¿™ä¸€è‡ªé€‚åº”åå¤„ç†æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´äº‹ä»¶è¾¹ç•Œé¢„æµ‹ï¼Œä½¿ç‹¬ç«‹SSLæ¨¡å‹çš„PSDS1æŒ‡æ ‡æå‡äº†é«˜è¾¾4%ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ä¸åŒSSLæ¶æ„çš„å…¼å®¹æ€§ä¸äº’è¡¥æ€§ï¼Œä¸ºç‰¹å®šä»»åŠ¡çš„æ¨¡å‹èåˆå’Œé²æ£’SEDç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦çš„æŠ€æœ¯æŒ‡å¯¼ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "27 pages, 5 figures, accepted by Journal of King Saud University Computer and Information Sciences online",
      "pdf_url": "https://arxiv.org/pdf/2505.11889v2",
      "published_date": "2025-05-17 07:54:31 UTC",
      "updated_date": "2025-08-26 09:52:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:57:41.205845+00:00"
    },
    {
      "arxiv_id": "2505.17059v1",
      "title": "Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large",
      "title_zh": "Medalyzeï¼šåŸºäº FLAN-T5-Large çš„è½»é‡çº§åŒ»ç–—æŠ¥å‘Šæ‘˜è¦åº”ç”¨",
      "authors": [
        "Van-Tinh Nguyen",
        "Hoang-Duong Pham",
        "Thanh-Hai To",
        "Cong-Tuan Hung Do",
        "Thi-Thu-Trang Dong",
        "Vu-Trung Duong Le",
        "Van-Phuc Hoang"
      ],
      "abstract": "Understanding medical texts presents significant challenges due to complex terminology and context-specific language. This paper introduces Medalyze, an AI-powered application designed to enhance the comprehension of medical texts using three specialized FLAN-T5-Large models. These models are fine-tuned for (1) summarizing medical reports, (2) extracting health issues from patient-doctor conversations, and (3) identifying the key question in a passage. Medalyze is deployed across a web and mobile platform with real-time inference, leveraging scalable API and YugabyteDB. Experimental evaluations demonstrate the system's superior summarization performance over GPT-4 in domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and lightweight solution for improving information accessibility in healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Medalyze åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—æ–‡æœ¬ä¸­å¤æ‚çš„å­¦æœ¯æœ¯è¯­å’Œç‰¹å®šè¯­å¢ƒå¸¦æ¥çš„ç†è§£éš¾é¢˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸‰ä¸ªç»è¿‡å¾®è°ƒçš„ FLAN-T5-Large æ¨¡å‹ï¼Œåˆ†åˆ«é’ˆå¯¹åŒ»ç–—æŠ¥å‘Šæ‘˜è¦ç”Ÿæˆã€åŒ»æ‚£å¯¹è¯ä¸­çš„å¥åº·é—®é¢˜æå–ä»¥åŠæ–‡æœ¬æ®µè½çš„æ ¸å¿ƒé—®é¢˜è¯†åˆ«è¿›è¡Œäº†ä¼˜åŒ–ã€‚Medalyze éƒ¨ç½²äº Web å’Œç§»åŠ¨å¹³å°ï¼Œé€šè¿‡å¯æ‰©å±•çš„ API å’Œ YugabyteDB æ”¯æŒå®æ—¶æ¨ç†(Real-time Inference)ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨ BLEUã€ROUGE-Lã€BERTScore å’Œ SpaCy Similarity ç­‰å¤šé¡¹æŒ‡æ ‡ä¸Šï¼ŒMedalyze åœ¨ç‰¹å®šé¢†åŸŸçš„æ‘˜è¦æ€§èƒ½ä¼˜äº GPT-4ã€‚è¯¥ç ”ç©¶ä¸ºæå‡åŒ»ç–—ä¿¡æ¯çš„å¯åŠæ€§æä¾›äº†ä¸€ç§å®ç”¨ã€è½»é‡çº§ä¸”å…¼é¡¾éšç§ä¿æŠ¤(Privacy-preserving)çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 8 figures. Submitted to IEEE Access for review. Preliminary version posted for early dissemination and feedback",
      "pdf_url": "https://arxiv.org/pdf/2505.17059v1",
      "published_date": "2025-05-17 07:16:58 UTC",
      "updated_date": "2025-05-17 07:16:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:57:40.585704+00:00"
    },
    {
      "arxiv_id": "2505.11881v5",
      "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks",
      "title_zh": "é‡æ–°å®¡è§†æ®‹å·®è¿æ¥ï¼šç”¨äºç¨³å®šé«˜æ•ˆæ·±åº¦ç½‘ç»œçš„æ­£äº¤æ›´æ–°",
      "authors": [
        "Giyeong Oh",
        "Woohyun Cho",
        "Siyeol Kim",
        "Suhwan Choi",
        "Youngjae Yu"
      ],
      "abstract": "Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. However, in standard residual updates, the module's output is directly added to the input stream. This can lead to updates that predominantly reinforce or modulate the existing stream direction, potentially underutilizing the module's capacity for learning entirely novel features. In this work, we introduce Orthogonal Residual Update: we decompose the module's output relative to the input stream and add only the component orthogonal to this stream. This design aims to guide modules to contribute primarily new representational directions, fostering richer feature learning while promoting more efficient training. We demonstrate that our orthogonal update strategy improves generalization accuracy and training stability across diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs, TinyImageNet, ImageNet-1k), achieving, for instance, a +3.78 pp top-1 accuracy gain for ViT-B on ImageNet-1k.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ ‡å‡†æ®‹å·®è¿æ¥(Residual Connections)å¯èƒ½å¯¼è‡´æ¨¡å‹å¯¹æ–°ç‰¹å¾å­¦ä¹ èƒ½åŠ›åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†æ­£äº¤æ®‹å·®æ›´æ–°(Orthogonal Residual Update)æœºåˆ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ¨¡å—è¾“å‡ºç›¸å¯¹äºè¾“å…¥æµè¿›è¡Œåˆ†è§£ï¼Œå¹¶ä»…å°†æ­£äº¤äºè¾“å…¥æ–¹å‘çš„åˆ†é‡æ·»åŠ åˆ°ä¸»å¹²ç½‘ç»œä¸­ï¼Œæ—¨åœ¨å¼•å¯¼å„æ¨¡å—ä¸»è¦è´¡çŒ®æ–°çš„è¡¨ç¤ºæ–¹å‘ã€‚è¿™ç§è®¾è®¡ä¸ä»…ä¿ƒè¿›äº†æ›´ä¸°å¯Œçš„ç‰¹å¾å­¦ä¹ ï¼Œè¿˜æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ­£äº¤æ›´æ–°ç­–ç•¥åœ¨ ResNetV2 å’Œ Vision Transformers (ViT) ç­‰å¤šç§æ¶æ„ä»¥åŠ ImageNet-1k ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå‡èƒ½æ˜¾è‘—æé«˜æ³›åŒ–å‡†ç¡®ç‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åœ¨ ImageNet-1k æ•°æ®é›†ä¸Šä¸º ViT-B å¸¦æ¥äº† 3.78 ä¸ªç™¾åˆ†ç‚¹çš„ Top-1 å‡†ç¡®ç‡æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "27 pages, maybe final final version",
      "pdf_url": "https://arxiv.org/pdf/2505.11881v5",
      "published_date": "2025-05-17 07:16:11 UTC",
      "updated_date": "2026-01-10 03:56:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:57:56.598707+00:00"
    },
    {
      "arxiv_id": "2505.11878v1",
      "title": "AdaptMol: Adaptive Fusion from Sequence String to Topological Structure for Few-shot Drug Discovery",
      "title_zh": "AdaptMolï¼šé¢å‘å°‘æ ·æœ¬è¯ç‰©å‘ç°çš„åºåˆ—å­—ç¬¦ä¸²ä¸æ‹“æ‰‘ç»“æ„è‡ªé€‚åº”èåˆ",
      "authors": [
        "Yifan Dai",
        "Xuanbai Ren",
        "Tengfei Ma",
        "Qipeng Yan",
        "Yiping Liu",
        "Yuansheng Liu",
        "Xiangxiang Zeng"
      ],
      "abstract": "Accurate molecular property prediction (MPP) is a critical step in modern drug development. However, the scarcity of experimental validation data poses a significant challenge to AI-driven research paradigms. Under few-shot learning scenarios, the quality of molecular representations directly dictates the theoretical upper limit of model performance. We present AdaptMol, a prototypical network integrating Adaptive multimodal fusion for Molecular representation. This framework employs a dual-level attention mechanism to dynamically integrate global and local molecular features derived from two modalities: SMILES sequences and molecular graphs. (1) At the local level, structural features such as atomic interactions and substructures are extracted from molecular graphs, emphasizing fine-grained topological information; (2) At the global level, the SMILES sequence provides a holistic representation of the molecule. To validate the necessity of multimodal adaptive fusion, we propose an interpretable approach based on identifying molecular active substructures to demonstrate that multimodal adaptive fusion can efficiently represent molecules. Extensive experiments on three commonly used benchmarks under 5-shot and 10-shot settings demonstrate that AdaptMol achieves state-of-the-art performance in most cases. The rationale-extracted method guides the fusion of two modalities and highlights the importance of both modalities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AdaptMolï¼Œä¸€ç§ç”¨äºå°‘æ ·æœ¬è¯ç‰©å‘ç°(Few-shot Drug Discovery)çš„åŸå‹ç½‘ç»œï¼Œæ—¨åœ¨é€šè¿‡è‡ªé€‚åº”å¤šæ¨¡æ€èåˆ(Adaptive multimodal fusion)è§£å†³åˆ†å­æ€§è´¨é¢„æµ‹(Molecular property prediction, MPP)ä¸­çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŒå±‚æ³¨æ„åŠ›æœºåˆ¶(dual-level attention mechanism)åŠ¨æ€æ•´åˆäº† SMILES åºåˆ—å’Œåˆ†å­å›¾(molecular graphs)ä¸¤ç§æ¨¡æ€çš„å…¨å±€ä¸å±€éƒ¨ç‰¹å¾ã€‚åœ¨å±€éƒ¨å±‚é¢ï¼Œè¯¥æ¨¡å‹ä»åˆ†å­å›¾ä¸­æå–åŸå­ç›¸äº’ä½œç”¨å’Œå­ç»“æ„ç­‰ç»†ç²’åº¦æ‹“æ‰‘ä¿¡æ¯ï¼Œè€Œåœ¨å…¨å±€å±‚é¢åˆ™åˆ©ç”¨ SMILES åºåˆ—æä¾›åˆ†å­çš„æ•´ä½“è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ä¸€ç§åŸºäºè¯†åˆ«åˆ†å­æ´»æ€§å­ç»“æ„çš„è§£é‡Šæ€§æ–¹æ³•ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€èåˆåœ¨åˆ†å­è¡¨ç¤ºä¸­çš„é«˜æ•ˆæ€§ã€‚åœ¨ä¸‰ä¸ªå¸¸ç”¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ 5-shot å’Œ 10-shot å®éªŒè¡¨æ˜ï¼ŒAdaptMol åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å‡è¾¾åˆ°äº†é¢†åŸŸå†…é¢†å…ˆ(state-of-the-art)çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†å°‘æ ·æœ¬åœºæ™¯ä¸‹åˆ†å­è¡¨ç¤ºçš„è´¨é‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.MN"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.11878v1",
      "published_date": "2025-05-17 07:12:12 UTC",
      "updated_date": "2025-05-17 07:12:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:58:15.717597+00:00"
    },
    {
      "arxiv_id": "2505.17058v1",
      "title": "DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation",
      "title_zh": "DO-RAGï¼šåŸºäºçŸ¥è¯†å›¾è°±å¢å¼ºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„ç‰¹å®šé¢†åŸŸé—®ç­”æ¡†æ¶",
      "authors": [
        "David Osei Opoku",
        "Ming Sheng",
        "Yong Zhang"
      ],
      "abstract": "Domain-specific QA systems require not just generative fluency but high factual accuracy grounded in structured expert knowledge. While recent Retrieval-Augmented Generation (RAG) frameworks improve context recall, they struggle with integrating heterogeneous data and maintaining reasoning consistency. To address these challenges, we propose DO-RAG, a scalable and customizable hybrid QA framework that integrates multi-level knowledge graph construction with semantic vector retrieval. Our system employs a novel agentic chain-of-thought architecture to extract structured relationships from unstructured, multimodal documents, constructing dynamic knowledge graphs that enhance retrieval precision. At query time, DO-RAG fuses graph and vector retrieval results to generate context-aware responses, followed by hallucination mitigation via grounded refinement. Experimental evaluations in the database and electrical domains show near-perfect recall and over 94% answer relevancy, with DO-RAG outperforming baseline frameworks by up to 33.38%. By combining traceability, adaptability, and performance efficiency, DO-RAG offers a reliable foundation for multi-domain, high-precision QA at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DO-RAGï¼Œä¸€ä¸ªç»“åˆçŸ¥è¯†å›¾è°±å¢å¼ºæ£€ç´¢ä¸è¯­ä¹‰å‘é‡æ£€ç´¢çš„å¯æ‰©å±•ã€å¯å®šåˆ¶æ··åˆé—®ç­”æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç‰¹å®šé¢†åŸŸé—®ç­”ç³»ç»Ÿåœ¨é›†æˆå¼‚æ„æ•°æ®å’Œä¿æŒæ¨ç†ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åˆ›æ–°çš„æ™ºèƒ½ä½“é“¾å¼æ€ç»´(agentic chain-of-thought)æ¶æ„ï¼Œä»éç»“æ„åŒ–å¤šæ¨¡æ€æ–‡æ¡£ä¸­æå–ç»“æ„åŒ–å…³ç³»å¹¶æ„å»ºåŠ¨æ€çŸ¥è¯†å›¾è°±ï¼Œæ˜¾è‘—æå‡æ£€ç´¢ç²¾åº¦ã€‚åœ¨æŸ¥è¯¢é˜¶æ®µï¼ŒDO-RAG èåˆå›¾æ£€ç´¢ä¸å‘é‡æ£€ç´¢ç»“æœç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥å“åº”ï¼Œå¹¶é€šè¿‡ grounded refinement æŠ€æœ¯å‡è½»å¹»è§‰é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ•°æ®åº“å’Œç”µæ°”é¢†åŸŸï¼ŒDO-RAG å®ç°äº†æ¥è¿‘å®Œç¾çš„å¬å›ç‡å’Œè¶…è¿‡ 94% çš„å›ç­”ç›¸å…³æ€§ï¼Œæ€§èƒ½ä¼˜äºåŸºçº¿æ¡†æ¶é«˜è¾¾ 33.38%ã€‚é€šè¿‡ç»“åˆå¯è¿½æº¯æ€§ã€é€‚åº”æ€§å’Œé«˜æ•ˆæ€§èƒ½ï¼ŒDO-RAG ä¸ºå¤§è§„æ¨¡ã€é«˜ç²¾åº¦çš„å¤šé¢†åŸŸé—®ç­”æä¾›äº†å¯é åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 5 figures;",
      "pdf_url": "https://arxiv.org/pdf/2505.17058v1",
      "published_date": "2025-05-17 06:40:17 UTC",
      "updated_date": "2025-05-17 06:40:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:58:00.182662+00:00"
    },
    {
      "arxiv_id": "2505.14711v1",
      "title": "Space evaluation at the starting point of soccer transitions",
      "title_zh": "è¶³çƒæ”»é˜²è½¬æ¢èµ·ç‚¹å¤„çš„ç©ºé—´è¯„ä¼°",
      "authors": [
        "Yohei Ogawa",
        "Rikuhei Umemoto",
        "Keisuke Fujii"
      ],
      "abstract": "Soccer is a sport played on a pitch where effective use of space is crucial. Decision-making during transitions, when possession switches between teams, has been increasingly important, but research on space evaluation in these moments has been limited. Recent space evaluation methods such as OBSO (Off-Ball Scoring Opportunity) use scoring probability, so it is not well-suited for assessing areas far from the goal, where transitions typically occur. In this paper, we propose OBPV (Off-Ball Positioning Value) to evaluate space across the pitch, including the starting points of transitions. OBPV extends OBSO by introducing the field value model, which evaluates the entire pitch, and by employing the transition kernel model, which reflects positional specificity through kernel density estimation of pass distributions. Experiments using La Liga 2023/24 season tracking and event data show that OBPV highlights effective space utilization during counter-attacks and reveals team-specific characteristics in how the teams utilize space after positive and negative transitions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OBPV (Off-Ball Positioning Value)ï¼Œæ—¨åœ¨è¯„ä¼°è¶³çƒæ¯”èµ›å…¨åœºèŒƒå›´å†…çš„ç©ºé—´ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯æ”»é˜²è½¬æ¢çš„èµ·ç‚¹åŒºåŸŸã€‚ä¼ ç»Ÿçš„ OBSO (Off-Ball Scoring Opportunity) æ–¹æ³•ç”±äºè¿‡åº¦ä¾§é‡äºå¾—åˆ†æ¦‚ç‡ï¼Œéš¾ä»¥æœ‰æ•ˆè¡¡é‡é€šå¸¸å‘ç”Ÿåœ¨è¿œç¦»çƒé—¨åŒºåŸŸçš„è¿‡æ¸¡åœ°å¸¦ã€‚OBPV é€šè¿‡å¼•å…¥è¯„ä¼°å…¨åœºçš„ field value modelï¼Œå¹¶ç»“åˆåŸºäºä¼ çƒåˆ†å¸ƒ kernel density estimation çš„ transition kernel modelï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹ä½ç½®ç‰¹å¼‚æ€§çš„æ•æ‰èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ La Liga 2023/24 èµ›å­£çš„è¿½è¸ªå’Œäº‹ä»¶æ•°æ®è¿›è¡Œäº†å®éªŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼ŒOBPV èƒ½å¤Ÿæ¸…æ™°åœ°åˆ»ç”»åå‡»è¿‡ç¨‹ä¸­çš„æœ‰æ•ˆç©ºé—´åˆ©ç”¨ï¼Œå¹¶æ­ç¤ºäº†ä¸åŒçƒé˜Ÿåœ¨ positive and negative transitions è¿‡ç¨‹ä¸­ç‹¬ç‰¹çš„ç©ºé—´è¿ç”¨ç‰¹å¾ã€‚è¯¥æ¨¡å‹ä¸ºè¶³çƒæ¯”èµ›ä¸­çš„ç©ºé—´è¯„ä»·æä¾›äº†æ›´å…¨é¢çš„é‡åŒ–æ‰‹æ®µï¼Œæœ‰åŠ©äºæ·±å…¥åˆ†æçƒé˜Ÿçš„æˆ˜æœ¯å†³ç­–ã€‚",
      "categories": [
        "stat.AP",
        "cs.AI"
      ],
      "primary_category": "stat.AP",
      "comment": "23 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.14711v1",
      "published_date": "2025-05-17 06:28:06 UTC",
      "updated_date": "2025-05-17 06:28:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:58:29.927864+00:00"
    },
    {
      "arxiv_id": "2505.11866v1",
      "title": "Position Paper: Bounded Alignment: What (Not) To Expect From AGI Agents",
      "title_zh": "ç«‹åœºè®ºæ–‡ï¼šæœ‰ç•Œå¯¹é½â€”â€”æˆ‘ä»¬åº”å½“ï¼ˆä¸ä¸åº”å½“ï¼‰å¯¹ AGI æ™ºèƒ½ä½“æœŸå¾…ä»€ä¹ˆ",
      "authors": [
        "Ali A. Minai"
      ],
      "abstract": "The issues of AI risk and AI safety are becoming critical as the prospect of artificial general intelligence (AGI) looms larger. The emergence of extremely large and capable generative models has led to alarming predictions and created a stir from boardrooms to legislatures. As a result, AI alignment has emerged as one of the most important areas in AI research. The goal of this position paper is to argue that the currently dominant vision of AGI in the AI and machine learning (AI/ML) community needs to evolve, and that expectations and metrics for its safety must be informed much more by our understanding of the only existing instance of general intelligence, i.e., the intelligence found in animals, and especially in humans. This change in perspective will lead to a more realistic view of the technology, and allow for better policy decisions.",
      "tldr_zh": "è¯¥è§‚ç‚¹è®ºæ–‡æ¢è®¨äº†éšç€é€šç”¨äººå·¥æ™ºèƒ½(AGI)å‰æ™¯æ—¥ç›Šæ˜æœ—ï¼Œäººå·¥æ™ºèƒ½é£é™©(AI risk)ä¸å®‰å…¨(AI safety)æˆä¸ºå…³é”®è®®é¢˜çš„èƒŒæ™¯ã€‚ä½œè€…æŒ‡å‡ºï¼Œç”Ÿæˆå¼æ¨¡å‹çš„èƒ½åŠ›å¼•å‘äº†ç¤¾ä¼šå„ç•Œçš„å¹¿æ³›å…³æ³¨ï¼Œä½¿å¾—äººå·¥æ™ºèƒ½å¯¹é½(AI alignment)æˆä¸ºç›®å‰ç ”ç©¶çš„é‡è¦é¢†åŸŸã€‚è®ºæ–‡ä¸»å¼ ï¼Œå½“å‰äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ (AI/ML)ç¤¾åŒºå¯¹AGIçš„ä¸»å¯¼æ„¿æ™¯éœ€è¦å‘ç”Ÿæ¼”è¿›ã€‚ç ”ç©¶è®¤ä¸ºï¼Œå…³äºAGIå®‰å…¨çš„é¢„æœŸå’Œè¯„ä»·æŒ‡æ ‡å¿…é¡»æ›´å¤šåœ°å»ºç«‹åœ¨å¯¹äººç±»åŠåŠ¨ç‰©ç­‰ç°æœ‰ç”Ÿç‰©é€šç”¨æ™ºèƒ½çš„ç†è§£ä¹‹ä¸Šã€‚è¿™ç§è§†è§’çš„è½¬å˜å°†ä¸ºæŠ€æœ¯å‘å±•æä¾›æ›´åŠ¡å®çš„è§†è§’ï¼Œå¹¶æœ‰åŠ©äºåˆ¶å®šæ›´æœ‰æ•ˆçš„æ”¿ç­–å†³ç­–ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper accepted for the 2025 IEEE/INNS International Joint Conference on Neural Networks, Rome, Italy, June 30 - July 5, 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.11866v1",
      "published_date": "2025-05-17 06:17:57 UTC",
      "updated_date": "2025-05-17 06:17:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:58:30.511138+00:00"
    },
    {
      "arxiv_id": "2505.11864v3",
      "title": "Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning",
      "title_zh": "ä»å™ªå£°åå¥½ä¸­å­¦ä¹ å¸•ç´¯æ‰˜æœ€ä¼˜å¥–åŠ±ï¼šä¸€ç§å¤šç›®æ ‡é€†å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Kalyan Cherukuri",
        "Aarav Lala"
      ],
      "abstract": "As generative agents become increasingly capable, alignment of their behavior with complex human values remains a fundamental challenge. Existing approaches often simplify human intent through reduction to a scalar reward, overlooking the multi-faceted nature of human feedback. In this work, we introduce a theoretical framework for preference-based Multi-Objective Inverse Reinforcement Learning (MO-IRL), where human preferences are modeled as latent vector-valued reward functions. We formalize the problem of recovering a Pareto-optimal reward representation from noisy preference queries and establish conditions for identifying the underlying multi-objective structure. We derive tight sample complexity bounds for recovering $Îµ$-approximations of the Pareto front and introduce a regret formulation to quantify suboptimality in this multi-objective setting. Furthermore, we propose a provably convergent algorithm for policy optimization using preference-inferred reward cones. Our results bridge the gap between practical alignment techniques and theoretical guarantees, providing a principled foundation for learning aligned behaviors in a high-dimension and value-pluralistic environment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºåå¥½çš„å¤šç›®æ ‡é€†å¼ºåŒ–å­¦ä¹  (Multi-Objective Inverse Reinforcement Learning, MO-IRL) ç†è®ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°†ç”Ÿæˆå¼æ™ºèƒ½ä½“è¡Œä¸ºä¸å¤æ‚å¤šç»´çš„äººç±»ä»·å€¼å¯¹é½çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å°†äººç±»åå¥½å»ºæ¨¡ä¸ºæ½œåœ¨çš„å‘é‡å€¼å¥–åŠ±å‡½æ•°ï¼Œå¹¶å½¢å¼åŒ–äº†ä»å™ªå£°åå¥½æŸ¥è¯¢ä¸­æ¢å¤ Pareto-optimal å¥–åŠ±è¡¨ç¤ºçš„é—®é¢˜ã€‚ç ”ç©¶è€…ç¡®ç«‹äº†è¯†åˆ«åº•å±‚å¤šç›®æ ‡ç»“æ„çš„å¿…è¦æ¡ä»¶ï¼Œå¹¶æ¨å¯¼å‡ºäº†ç”¨äºæ¢å¤ Pareto front çš„ Îµ-approximations çš„ç´§è‡´æ ·æœ¬å¤æ‚åº¦ç•Œé™ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¼•å…¥äº† regret formulation æ¥é‡åŒ–å¤šç›®æ ‡è®¾å®šä¸‹çš„æ¬¡ä¼˜æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ©ç”¨åå¥½æ¨å¯¼çš„ reward cones è¿›è¡Œç­–ç•¥ä¼˜åŒ–çš„å¯è¯æ˜æ”¶æ•›ç®—æ³•ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å»ºç«‹ç†è®ºä¿éšœï¼Œä¸ºåœ¨é«˜ç»´å’Œä»·å€¼å¤šå…ƒåŒ–ç¯å¢ƒ (value-pluralistic environment) ä¸­å®ç°è¡Œä¸ºå¯¹é½æä¾›äº†åŸåˆ™æ€§åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CG"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11864v3",
      "published_date": "2025-05-17 06:09:13 UTC",
      "updated_date": "2025-07-28 20:30:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:58:26.034957+00:00"
    },
    {
      "arxiv_id": "2505.11862v2",
      "title": "Q-Policy: Quantum-Enhanced Policy Evaluation for Scalable Reinforcement Learning",
      "title_zh": "Q-Policyï¼šé¢å‘å¯æ‰©å±•å¼ºåŒ–å­¦ä¹ çš„é‡å­å¢å¼ºç­–ç•¥è¯„ä¼°",
      "authors": [
        "Kalyan Cherukuri",
        "Aarav Lala",
        "Yash Yardi"
      ],
      "abstract": "We propose Q-Policy, a hybrid quantum-classical reinforcement learning (RL) framework that mathematically accelerates policy evaluation and optimization by exploiting quantum computing primitives. Q-Policy encodes value functions in quantum superposition, enabling simultaneous evaluation of multiple state-action pairs via amplitude encoding and quantum parallelism. We introduce a quantum-enhanced policy iteration algorithm with provable polynomial reductions in sample complexity for the evaluation step, under standard assumptions. To demonstrate the technical feasibility and theoretical soundness of our approach, we validate Q-Policy on classical emulations of small discrete control tasks. Due to current hardware and simulation limitations, our experiments focus on showcasing proof-of-concept behavior rather than large-scale empirical evaluation. Our results support the potential of Q-Policy as a theoretical foundation for scalable RL on future quantum devices, addressing RL scalability challenges beyond classical approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Q-Policyï¼Œä¸€ç§æ··åˆé‡å­-ç»å…¸å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨é‡å­è®¡ç®—åŸè¯­åœ¨æ•°å­¦ä¸ŠåŠ é€Ÿç­–ç•¥è¯„ä¼°ä¸ä¼˜åŒ–è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶é€šè¿‡æŒ¯å¹…ç¼–ç  (amplitude encoding) å’Œé‡å­å¹¶è¡Œæ€§ (quantum parallelism) å°†ä»·å€¼å‡½æ•°ç¼–ç åœ¨é‡å­å åŠ æ€ä¸­ï¼Œå®ç°äº†å¯¹å¤šä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹çš„åŒæ­¥è¯„ä¼°ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§é‡å­å¢å¼ºç­–ç•¥è¿­ä»£ç®—æ³• (quantum-enhanced policy iteration algorithm)ï¼Œåœ¨æ ‡å‡†å‡è®¾ä¸‹ï¼Œå…¶è¯„ä¼°æ­¥éª¤çš„æ ·æœ¬å¤æ‚åº¦ (sample complexity) è¡¨ç°å‡ºå¯è¯æ˜çš„å¤šé¡¹å¼çº§åˆ«ç¼©å‡ã€‚é€šè¿‡åœ¨å°å‹ç¦»æ•£æ§åˆ¶ä»»åŠ¡çš„ç»å…¸ä»¿çœŸä¸Šè¿›è¡ŒéªŒè¯ï¼Œå®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ¡ˆçš„æŠ€æœ¯å¯è¡Œæ€§ä¸ç†è®ºå®Œå¤‡æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºè§£å†³ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ çš„æ‰©å±•æ€§æŒ‘æˆ˜æä¾›äº†æ–°è·¯å¾„ï¼Œå¹¶ä¸ºæœªæ¥åœ¨é‡å­ç¡¬ä»¶ä¸Šå®ç°å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11862v2",
      "published_date": "2025-05-17 06:03:32 UTC",
      "updated_date": "2025-06-07 04:41:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:58:54.827528+00:00"
    },
    {
      "arxiv_id": "2505.11861v1",
      "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity",
      "title_zh": "Fair-PPï¼šç”¨äºå¤§è¯­è¨€æ¨¡å‹ä¸ç¤¾ä¼šå…¬å¹³ä¸ªæ€§åŒ–åå¥½å¯¹é½çš„åˆæˆæ•°æ®é›†",
      "authors": [
        "Qi Zhou",
        "Jie Zhang",
        "Dongxia Wang",
        "Qiang Liu",
        "Tianlin Li",
        "Jin Song Dong",
        "Wenhai Wang",
        "Qing Guo"
      ],
      "abstract": "Human preference plays a crucial role in the refinement of large language models (LLMs). However, collecting human preference feedback is costly and most existing datasets neglect the correlation between personalization and preferences. To address this issue, we introduce Fair-PP, a synthetic dataset of personalized preferences targeting social equity, derived from real-world social survey data, which includes 28 social groups, 98 equity topics, and 5 personal preference dimensions. Leveraging GPT-4o-mini, we engage in role-playing based on seven representative persona portrayals guided by existing social survey data, yielding a total of 238,623 preference records. Through Fair-PP, we also contribute (i) An automated framework for generating preference data, along with a more fine-grained dataset of personalized preferences; (ii) analysis of the positioning of the existing mainstream LLMs across five major global regions within the personalized preference space; and (iii) a sample reweighting method for personalized preference alignment, enabling alignment with a target persona while maximizing the divergence from other personas. Empirical experiments show our method outperforms the baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åå¥½åé¦ˆæ”¶é›†æˆæœ¬é«˜æ˜‚ä¸”ç°æœ‰æ•°æ®å¿½è§†ä¸ªæ€§åŒ–ä¸åå¥½å…³è”çš„é—®é¢˜ï¼Œæå‡ºäº†Fair-PPæ•°æ®é›†ã€‚Fair-PPæ˜¯ä¸€ä¸ªé’ˆå¯¹ç¤¾ä¼šå…¬å¹³(Social Equity)çš„ä¸ªæ€§åŒ–åå¥½åˆæˆæ•°æ®é›†ï¼Œæºè‡ªçœŸå®ä¸–ç•Œçš„ç¤¾ä¼šè°ƒæŸ¥æ•°æ®ï¼Œæ¶µç›–äº†28ä¸ªç¤¾ä¼šç¾¤ä½“ã€98ä¸ªå…¬å¹³ä¸»é¢˜ä»¥åŠ5ä¸ªä¸ªäººåå¥½ç»´åº¦ã€‚ç ”ç©¶é€šè¿‡åˆ©ç”¨GPT-4o-miniåŸºäºä¸ƒç§ä»£è¡¨æ€§äººæ ¼ç”»åƒ(Persona)è¿›è¡Œè§’è‰²æ‰®æ¼”ï¼Œå…±ç”Ÿæˆäº†238,623æ¡åå¥½è®°å½•ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è´¡çŒ®äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„åå¥½æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œå¹¶åˆ†æäº†ä¸»æµLLMsåœ¨å…¨çƒäº”ä¸ªä¸»è¦åœ°åŒºä¸ªæ€§åŒ–åå¥½ç©ºé—´çš„å®šä½ã€‚ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ç”¨äºä¸ªæ€§åŒ–åå¥½å¯¹é½(Personalized Preference Alignment)çš„æ ·æœ¬é‡åŠ æƒæ–¹æ³•ï¼Œä»¥å®ç°ä¸ç›®æ ‡äººæ ¼çš„ç²¾å‡†å¯¹é½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "under review",
      "pdf_url": "https://arxiv.org/pdf/2505.11861v1",
      "published_date": "2025-05-17 06:02:00 UTC",
      "updated_date": "2025-05-17 06:02:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:59:02.975859+00:00"
    },
    {
      "arxiv_id": "2505.11854v1",
      "title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models",
      "title_zh": "å¤§å‹æ¨ç†æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›è¯„ä¼°",
      "authors": [
        "Hanmeng Liu",
        "Yiran Ding",
        "Zhizhang Fu",
        "Chaoli Zhang",
        "Xiaozhang Liu",
        "Yue Zhang"
      ],
      "abstract": "Large reasoning models, often post-trained on long chain-of-thought (long CoT) data with reinforcement learning, achieve state-of-the-art performance on mathematical, coding, and domain-specific reasoning benchmarks. However, their logical reasoning capabilities - fundamental to human cognition and independent of domain knowledge - remain understudied. To address this gap, we introduce LogiEval, a holistic benchmark for evaluating logical reasoning in large reasoning models. LogiEval spans diverse reasoning types (deductive, inductive, analogical, and abductive) and task formats (e.g., logical sequence, argument analysis), sourced from high-quality human examinations (e.g., LSAT, GMAT). Our experiments demonstrate that modern reasoning models excel at 4-choice argument analysis problems and analogical reasoning, surpassing human performance, yet exhibit uneven capabilities across reasoning types and formats, highlighting limitations in their generalization. Our analysis reveals that human performance does not mirror model failure distributions. To foster further research, we curate LogiEval-Hard, a challenging subset identified through a novel screening paradigm where small-model failures (Qwen3-30B-A3B) reliably predict difficulties for larger models. Modern models show striking, consistent failures on LogiEval-Hard. This demonstrates that fundamental reasoning bottlenecks persist across model scales, and establishes LogiEval-Hard as both a diagnostic tool and a rigorous testbed for advancing logical reasoning in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤§æ¨ç†æ¨¡å‹(Large Reasoning Models)åœ¨é€»è¾‘æ¨ç†è¿™ä¸€æ ¸å¿ƒè®¤çŸ¥èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ï¼Œæ­¤å‰è¿™ç±»æ¨¡å‹ä¸»è¦åœ¨æ•°å­¦å’Œç¼–ç¨‹åŸºå‡†ä¸Šå—å…³æ³¨ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†åä¸ºLogiEvalçš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†æ¼”ç»(deductive)ã€å½’çº³(inductive)ã€ç±»æ¯”(analogical)å’Œæº¯å› (abductive)ç­‰å¤šç§æ¨ç†ç±»å‹ï¼Œé¢˜ç›®æºè‡ªLSATå’ŒGMATç­‰é«˜è´¨é‡è€ƒè¯•ã€‚å®éªŒå‘ç°ï¼Œç°ä»£æ¨ç†æ¨¡å‹åœ¨è®ºè¯åˆ†æå’Œç±»æ¯”æ¨ç†ä¸Šè¡¨ç°ä¼˜å¼‚å¹¶è¶…è¶Šäº†äººç±»ï¼Œä½†åœ¨ä¸åŒä»»åŠ¡æ ¼å¼ä¸‹è¡¨ç°æä¸å‡è¡¡ï¼Œå­˜åœ¨æ˜æ˜¾çš„æ³›åŒ–(generalization)å±€é™ã€‚ç ”ç©¶è¿˜é€šè¿‡ä¸€ç§æ–°çš„ç­›é€‰èŒƒå¼æ„å»ºäº†LogiEval-Hardå­é›†ï¼Œå‘ç°ç°ä»£æ¨¡å‹åœ¨å¤„ç†è¿™äº›æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡æ—¶ä¼šè¡¨ç°å‡ºä¸€è‡´çš„å¤±è´¥ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å³ä¾¿æ˜¯å…ˆè¿›æ¨¡å‹ä¹Ÿé¢ä¸´æ ¹æœ¬æ€§çš„æ¨ç†ç“¶é¢ˆï¼ŒLogiEval-Hardåˆ™ä¸ºæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)é€»è¾‘æ¨ç†èƒ½åŠ›çš„è¿›æ­¥æä¾›äº†å…³é”®çš„è¯Šæ–­å·¥å…·å’Œæµ‹è¯•å¹³å°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11854v1",
      "published_date": "2025-05-17 05:36:14 UTC",
      "updated_date": "2025-05-17 05:36:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:59:58.466900+00:00"
    },
    {
      "arxiv_id": "2505.11849v1",
      "title": "VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation",
      "title_zh": "VeriReasonï¼šåŸºäº Testbench åé¦ˆå¼ºåŒ–å­¦ä¹ çš„æ¨ç†å¢å¼ºå‹ Verilog ç”Ÿæˆ",
      "authors": [
        "Yiting Wang",
        "Guoheng Sun",
        "Wanghao Ye",
        "Gang Qu",
        "Ang Li"
      ],
      "abstract": "Automating Register Transfer Level (RTL) code generation using Large Language Models (LLMs) offers substantial promise for streamlining digital circuit design and reducing human effort. However, current LLM-based approaches face significant challenges with training data scarcity, poor specification-code alignment, lack of verification mechanisms, and balancing generalization with specialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework integrating supervised fine-tuning with Guided Reward Proximal Optimization (GRPO) reinforcement learning for RTL generation. Using curated training examples and a feedback-driven reward model, VeriReason combines testbench evaluations with structural heuristics while embedding self-checking capabilities for autonomous error correction. On the VerilogEval Benchmark, VeriReason delivers significant improvements: achieving 83.1% functional correctness on the VerilogEval Machine benchmark, substantially outperforming both comparable-sized models and much larger commercial systems like GPT-4 Turbo. Additionally, our approach demonstrates up to a 2.8X increase in first-attempt functional correctness compared to baseline methods and exhibits robust generalization to unseen designs. To our knowledge, VeriReason represents the first system to successfully integrate explicit reasoning capabilities with reinforcement learning for Verilog generation, establishing a new state-of-the-art for automated RTL synthesis. The models and datasets are available at: https://huggingface.co/collections/AI4EDA-CASE Code is Available at: https://github.com/NellyW8/VeriReason",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VeriReason æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆç›‘ç£å¾®è°ƒ(SFT)ä¸å¼•å¯¼å¥–åŠ±è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(GRPO)å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œè§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‡ªåŠ¨åŒ–å¯„å­˜å™¨ä¼ è¾“çº§(RTL)ä»£ç ç”Ÿæˆä¸­é¢ä¸´çš„æ•°æ®ç¨€ç¼ºåŠéªŒè¯æœºåˆ¶ç¼ºå¤±ç­‰æŒ‘æˆ˜ã€‚å— DeepSeek-R1 å¯å‘ï¼ŒVeriReason ç»“åˆäº†ç²¾é€‰è®­ç»ƒç¤ºä¾‹å’Œåé¦ˆé©±åŠ¨çš„å¥–åŠ±æ¨¡å‹ï¼Œå°†æµ‹è¯•å¹³å°(Testbench)è¯„ä¼°ä¸ç»“æ„å¯å‘å¼æ–¹æ³•ç›¸ç»“åˆï¼Œå¹¶åµŒå…¥äº†è‡ªä¸»é”™è¯¯çº æ­£çš„è‡ªæ£€èƒ½åŠ›ã€‚åœ¨ VerilogEval åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¡†æ¶è¡¨ç°å‡ºè‰²ï¼Œåœ¨ VerilogEval Machine ä»»åŠ¡ä¸Šå®ç°äº† 83.1% çš„åŠŸèƒ½æ­£ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº† GPT-4 Turbo ç­‰å¤§å‹å•†ä¸šç³»ç»Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVeriReason åœ¨é¦–æ¬¡å°è¯•çš„åŠŸèƒ½æ­£ç¡®ç‡ä¸Šæ¯”åŸºçº¿æ–¹æ³•æé«˜äº† 2.8 å€ï¼Œå¹¶å±•ç°å‡ºå¯¹æœªçŸ¥è®¾è®¡çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚ä½œä¸ºé¦–ä¸ªæˆåŠŸå°†æ˜¾å¼æ¨ç†èƒ½åŠ›ä¸å¼ºåŒ–å­¦ä¹ åº”ç”¨äº Verilog ç”Ÿæˆçš„ç³»ç»Ÿï¼ŒVeriReason ä¸ºè‡ªåŠ¨åŒ– RTL ç»¼åˆæ ‘ç«‹äº†æ–°çš„æœ€å…ˆè¿›æŠ€æœ¯(SOTA)æ ‡æ†ã€‚",
      "categories": [
        "cs.AI",
        "cs.AR",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.11849v1",
      "published_date": "2025-05-17 05:25:01 UTC",
      "updated_date": "2025-05-17 05:25:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:58:56.693115+00:00"
    },
    {
      "arxiv_id": "2505.17056v1",
      "title": "Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective",
      "title_zh": "LLMs æ˜¯å¦å·²å‡†å¤‡å¥½åº”å¯¹è‹±è¯­æ ‡å‡†åŒ–è€ƒè¯•ï¼ŸåŸºäºåŸºå‡†æµ‹è¯•ä¸èƒ½åŠ›å¼•å¯¼çš„è§†è§’",
      "authors": [
        "Luoxi Tang",
        "Tharunya Sundar",
        "Shuai Yang",
        "Ankita Patra",
        "Manohar Chippada",
        "Giqi Zhao",
        "Yi Li",
        "Riteng Zhang",
        "Tunan Zhao",
        "Ting Yang",
        "Yuqiao Meng",
        "Weicheng Ma",
        "Zhaohan Xi"
      ],
      "abstract": "AI is transforming education by enabling powerful tools that enhance learning experiences. Among recent advancements, large language models (LLMs) hold particular promise for revolutionizing how learners interact with educational content. In this work, we investigate the potential of LLMs to support standardized test preparation by focusing on English Standardized Tests (ESTs). Specifically, we assess their ability to generate accurate and contextually appropriate solutions across a diverse set of EST question types. We introduce ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests, encompassing 29 question types and over 10,576 questions across multiple modalities, including text, images, audio, tables, and mathematical symbols. Using ESTBOOK, we systematically evaluate both the accuracy and inference efficiency of LLMs. Additionally, we propose a breakdown analysis framework that decomposes complex EST questions into task-specific solution steps. This framework allows us to isolate and assess LLM performance at each stage of the reasoning process. Evaluation findings offer insights into the capability of LLMs in educational contexts and point toward targeted strategies for improving their reliability as intelligent tutoring systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‹±è¯­æ ‡å‡†åŒ–è€ƒè¯•(English Standardized Tests, ESTs)è¾…å¯¼é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Œå¹¶æ¨å‡ºäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•é›†ESTBOOKã€‚è¯¥åŸºå‡†æ•´åˆäº†5é¡¹æƒå¨è€ƒè¯•ï¼Œæ¶µç›–29ç§é¢˜å‹åŠè¶…è¿‡10,576é“åŒ…å«æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è¡¨æ ¼å’Œæ•°å­¦ç¬¦å·ç­‰å¤šæ¨¡æ€ä¿¡æ¯çš„é¢˜ç›®ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°LLMsçš„å‡†ç¡®ç‡ä¸æ¨ç†æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ†è§£åˆ†ææ¡†æ¶(breakdown analysis framework)ï¼Œé€šè¿‡å°†å¤æ‚çš„è€ƒè¯•é—®é¢˜æ‹†è§£ä¸ºç‰¹å®šä»»åŠ¡çš„è§£é¢˜æ­¥éª¤ï¼Œå®ç°äº†å¯¹æ¨¡å‹æ¨ç†è¿‡ç¨‹å„é˜¶æ®µè¡¨ç°çš„ç²¾ç¡®è¯„ä¼°ã€‚å®éªŒå‘ç°ä¸ºLLMsåœ¨æ•™è‚²è¯­å¢ƒä¸‹çš„èƒ½åŠ›æä¾›äº†æ·±å…¥è§è§£ï¼Œå¹¶ä¸ºä¼˜åŒ–å…¶ä½œä¸ºæ™ºèƒ½è¾…å¯¼ç³»ç»Ÿ(intelligent tutoring systems)çš„å¯é æ€§æä¾›äº†é’ˆå¯¹æ€§çš„æ”¹è¿›ç­–ç•¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17056v1",
      "published_date": "2025-05-17 05:10:44 UTC",
      "updated_date": "2025-05-17 05:10:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:59:12.732737+00:00"
    },
    {
      "arxiv_id": "2505.14709v1",
      "title": "FastCar: Cache Attentive Replay for Fast Auto-Regressive Video Generation on the Edge",
      "title_zh": "FastCarï¼šé¢å‘è¾¹ç¼˜ç«¯å¿«é€Ÿè‡ªå›å½’è§†é¢‘ç”Ÿæˆçš„ç¼“å­˜æ³¨æ„åŠ›å›æ”¾",
      "authors": [
        "Xuan Shen",
        "Weize Ma",
        "Yufa Zhou",
        "Enhao Tang",
        "Yanyue Xie",
        "Zhengang Li",
        "Yifan Gong",
        "Quanyi Wang",
        "Henghui Ding",
        "Yiwei Wang",
        "Yanzhi Wang",
        "Pu Zhao",
        "Jun Lin",
        "Jiuxiang Gu"
      ],
      "abstract": "Auto-regressive (AR) models, initially successful in language generation, have recently shown promise in visual generation tasks due to their superior sampling efficiency. Unlike image generation, video generation requires a substantially larger number of tokens to produce coherent temporal frames, resulting in significant overhead during the decoding phase. Our key observations are: (i) MLP modules in the decode phase dominate the inference latency, and (ii) there exists high temporal redundancy in MLP outputs of adjacent frames. In this paper, we propose the \\textbf{FastCar} framework to accelerate the decode phase for the AR video generation by exploring the temporal redundancy. The Temporal Attention Score (TAS) is proposed to determine whether to apply the replay strategy (\\textit{i.e.}, reusing cached MLP outputs from the previous frame to reduce redundant computations) with detailed theoretical analysis and justification. Also, we develop a hardware accelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to enable better resource utilization and faster inference. Experimental results demonstrate the effectiveness of our method, which outperforms traditional sparse attention approaches with more than 2.1x decoding speedup and higher energy efficiency on the edge. Furthermore, by combining FastCar and sparse attention, FastCar can boost the performance of sparse attention with alleviated drifting, demonstrating our unique advantages for high-resolution and long-duration video generation. Code: https://github.com/shawnricecake/fast-car",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªå›å½’(Auto-regressive)è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨è¾¹ç¼˜ç«¯æ¨ç†æ—¶ï¼Œç”±äºä»¤ç‰Œæ•°é‡åºå¤§å¯¼è‡´ MLP æ¨¡å—è®¡ç®—å¼€é”€é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº† FastCar æ¡†æ¶ã€‚ç ”ç©¶è€…é€šè¿‡è§‚å¯Ÿå‘ç°ï¼Œç›¸é‚»å¸§ä¹‹é—´çš„ MLP è¾“å‡ºå­˜åœ¨é«˜åº¦çš„æ—¶é—´å†—ä½™(temporal redundancy)ï¼Œè¿™æ˜¯åŠ é€Ÿè§£ç é˜¶æ®µçš„å…³é”®åˆ‡å…¥ç‚¹ã€‚FastCar å¼•å…¥äº†æ—¶é—´æ³¨æ„åŠ›è¯„åˆ†(Temporal Attention Score, TAS)æœºåˆ¶ï¼Œé€šè¿‡ç†è®ºåˆ†æåŠ¨æ€å†³å®šæ˜¯å¦åº”ç”¨é‡æ”¾ç­–ç•¥(replay strategy)ï¼Œå³é‡ç”¨å‰ä¸€å¸§çš„ç¼“å­˜è¾“å‡ºæ¥å‡å°‘å†—ä½™è®¡ç®—ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åœ¨ FPGA ä¸Šå¼€å‘äº†é…å¥—çš„ç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œåˆ©ç”¨åŸºäº TAS çš„åŠ¨æ€èµ„æºè°ƒåº¦(Dynamic Resource Scheduling, DRS)æ¥ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFastCar åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°äº†è¶…è¿‡ 2.1 å€çš„è§£ç åŠ é€Ÿï¼Œå¹¶è¡¨ç°å‡ºæ›´é«˜çš„èƒ½æ•ˆæ¯”ã€‚è¯¥æ¡†æ¶ä¸ä»…ä¼˜äºä¼ ç»Ÿçš„ç¨€ç–æ³¨æ„åŠ›(sparse attention)æ–¹æ³•ï¼Œåœ¨ä¸å…¶ç»“åˆæ—¶è¿˜èƒ½æœ‰æ•ˆç¼“è§£æ¼‚ç§»é—®é¢˜ï¼Œä¸ºç”Ÿæˆé«˜åˆ†è¾¨ç‡å’Œé•¿æŒç»­æ—¶é—´è§†é¢‘æä¾›äº†æ˜¾è‘—çš„æŠ€æœ¯ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint Version",
      "pdf_url": "https://arxiv.org/pdf/2505.14709v1",
      "published_date": "2025-05-17 05:00:39 UTC",
      "updated_date": "2025-05-17 05:00:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:59:16.739276+00:00"
    },
    {
      "arxiv_id": "2505.11839v1",
      "title": "On the Eligibility of LLMs for Counterfactual Reasoning: A Decompositional Study",
      "title_zh": "è®ºLLMsè¿›è¡Œåäº‹å®æ¨ç†çš„é€‚ç”¨æ€§ï¼šä¸€é¡¹åˆ†è§£å¼ç ”ç©¶",
      "authors": [
        "Shuai Yang",
        "Qi Yang",
        "Luoxi Tang",
        "Jeremy Blackburn",
        "Zhaohan Xi"
      ],
      "abstract": "Counterfactual reasoning has emerged as a crucial technique for generalizing the reasoning capabilities of large language models (LLMs). By generating and analyzing counterfactual scenarios, researchers can assess the adaptability and reliability of model decision-making. Although prior work has shown that LLMs often struggle with counterfactual reasoning, it remains unclear which factors most significantly impede their performance across different tasks and modalities. In this paper, we propose a decompositional strategy that breaks down the counterfactual generation from causality construction to the reasoning over counterfactual interventions. To support decompositional analysis, we investigate 11 datasets spanning diverse tasks, including natural language understanding, mathematics, programming, and vision-language tasks. Through extensive evaluations, we characterize LLM behavior across each decompositional stage and identify how modality type and intermediate reasoning influence performance. By establishing a structured framework for analyzing counterfactual reasoning, this work contributes to the development of more reliable LLM-based reasoning systems and informs future elicitation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åäº‹å®æ¨ç† (Counterfactual Reasoning) æ–¹é¢çš„è¡¨ç°ï¼Œæ—¨åœ¨è¯†åˆ«é˜»ç¢æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡æ€ä¸‹æ€§èƒ½çš„å…³é”®å› ç´ ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆ†è§£ç­–ç•¥ (Decompositional Strategy)ï¼Œå°†åäº‹å®ç”Ÿæˆè¿‡ç¨‹æ‹†åˆ†ä¸ºä»å› æœå…³ç³»æ„å»º (Causality Construction) åˆ°å¯¹åäº‹å®å¹²é¢„ (Counterfactual Interventions) è¿›è¡Œæ¨ç†çš„å¤šä¸ªé˜¶æ®µã€‚ç ”ç©¶é’ˆå¯¹æ¶µç›–è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°å­¦ã€ç¼–ç¨‹å’Œè§†è§‰è¯­è¨€ä»»åŠ¡çš„ 11 ä¸ªæ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›è°ƒæŸ¥ã€‚é€šè¿‡å¤šç»´åº¦è¯„ä¼°ï¼Œç ”ç©¶è¯¦ç»†åˆ»ç”»äº† LLMs åœ¨æ¯ä¸ªåˆ†è§£é˜¶æ®µçš„è¡Œä¸ºç‰¹å¾ï¼Œå¹¶æ­ç¤ºäº†æ¨¡æ€ç±»å‹å’Œä¸­é—´æ¨ç†è¿‡ç¨‹å¦‚ä½•æ˜¾è‘—å½±å“æœ€ç»ˆæ€§èƒ½ã€‚è¯¥å·¥ä½œå»ºç«‹äº†ä¸€ä¸ªåˆ†æåäº‹å®æ¨ç†çš„ç»“æ„åŒ–æ¡†æ¶ï¼Œä¸ºå¼€å‘æ›´å¯é çš„åŸºäº LLM çš„æ¨ç†ç³»ç»Ÿä»¥åŠæœªæ¥çš„å¯å‘ç­–ç•¥ (Elicitation Strategies) æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11839v1",
      "published_date": "2025-05-17 04:59:32 UTC",
      "updated_date": "2025-05-17 04:59:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:59:48.168932+00:00"
    },
    {
      "arxiv_id": "2505.11837v2",
      "title": "On Membership Inference Attacks in Knowledge Distillation",
      "title_zh": "è®ºçŸ¥è¯†è’¸é¦ä¸­çš„æˆå‘˜æ¨ç†æ”»å‡»",
      "authors": [
        "Ziyao Cui",
        "Minxing Zhang",
        "Jian Pei"
      ],
      "abstract": "Large language models (LLMs) are trained on massive corpora that may contain sensitive information, creating privacy risks under membership inference attacks (MIAs). Knowledge distillation is widely used to compress LLMs into smaller student models, but its privacy implications are poorly understood. We systematically evaluate how distillation affects MIA vulnerability across six teacher-student model pairs and six attack methods. We find that distilled student models do not consistently exhibit lower MIA success than their teacher models, and in some cases demonstrate substantially higher member-specific attack success, challenging the assumption that knowledge distillation inherently improves privacy. We attribute this to mixed supervision in distillation: for vulnerable training data points, teacher predictions often align with ground-truth labels, causing student models to learn overly confident predictions that amplify the separability between members and non-members; conversely, for non-vulnerable points, teacher predictions and ground truth frequently diverge, providing inconsistent learning signals. To mitigate this, we propose three practical interventions -- restricting distillation to non-vulnerable points, adding a low-dimensional Bottleneck Projection, and a normalization variant (NoNorm). Experiments show these methods reduce both aggregate and member-specific MIA success while preserving model utility, improving privacy-utility trade-offs for distilled LLMs.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨çŸ¥è¯†è’¸é¦(Knowledge Distillation)è¿‡ç¨‹ä¸­çš„æˆå‘˜æ¨ç†æ”»å‡»(Membership Inference Attacks, MIAs)é£é™©ï¼ŒæŒ‘æˆ˜äº†è’¸é¦èƒ½æå‡éšç§æ€§çš„ä¼ ç»Ÿå‡è®¾ã€‚é€šè¿‡å¯¹å…­å¯¹æ¨¡å‹å’Œå…­ç§æ”»å‡»æ–¹æ³•çš„å®éªŒå‘ç°ï¼Œè’¸é¦åçš„å­¦ç”Ÿæ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹æ¯”æ•™å¸ˆæ¨¡å‹æ›´æ˜“å—åˆ°æ”»å‡»ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„æˆå‘˜ç‰¹å¼‚æ€§æ”»å‡»æˆåŠŸç‡ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ç§è„†å¼±æ€§æºäºè’¸é¦ä¸­çš„æ··åˆç›‘ç£ä¿¡å·ï¼šå¯¹äºè„†å¼±æ•°æ®ï¼Œæ•™å¸ˆé¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„é«˜åº¦ä¸€è‡´ä¼šå¯¼è‡´å­¦ç”Ÿæ¨¡å‹äº§ç”Ÿè¿‡åº¦è‡ªä¿¡çš„é¢„æµ‹ï¼Œä»è€Œæ”¾å¤§äº†æˆå‘˜ä¸éæˆå‘˜ä¹‹é—´çš„å·®å¼‚ã€‚ä¸ºåº”å¯¹è¿™ä¸€é£é™©ï¼Œè®ºæ–‡æå‡ºäº†é™åˆ¶è’¸é¦è‡³éè„†å¼±ç‚¹ã€å¼•å…¥ä½ç»´Bottleneck Projectionä»¥åŠé‡‡ç”¨NoNormè§„èŒƒåŒ–å˜ä½“ç­‰ä¸‰é¡¹å®ç”¨å¹²é¢„æªæ–½ã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨ä¸æŸå®³æ¨¡å‹æ•ˆç”¨çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½äº†èšåˆåŠæˆå‘˜ç‰¹å¼‚æ€§çš„MIAæˆåŠŸç‡ï¼Œæœ‰æ•ˆæ”¹å–„äº†è’¸é¦æ¨¡å‹åœ¨éšç§ä¸æ•ˆç”¨ä¹‹é—´çš„å¹³è¡¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11837v2",
      "published_date": "2025-05-17 04:54:26 UTC",
      "updated_date": "2026-01-10 20:34:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:59:28.795336+00:00"
    },
    {
      "arxiv_id": "2505.11836v1",
      "title": "SplInterp: Improving our Understanding and Training of Sparse Autoencoders",
      "title_zh": "SplInterpï¼šæ·±åŒ–å¯¹ç¨€ç–è‡ªç¼–ç å™¨çš„ç†è§£ä¸è®­ç»ƒ",
      "authors": [
        "Jeremy Budd",
        "Javier Ideami",
        "Benjamin Macdowall Rynne",
        "Keith Duggar",
        "Randall Balestriero"
      ],
      "abstract": "Sparse autoencoders (SAEs) have received considerable recent attention as tools for mechanistic interpretability, showing success at extracting interpretable features even from very large LLMs. However, this research has been largely empirical, and there have been recent doubts about the true utility of SAEs. In this work, we seek to enhance the theoretical understanding of SAEs, using the spline theory of deep learning. By situating SAEs in this framework: we discover that SAEs generalise ``$k$-means autoencoders'' to be piecewise affine, but sacrifice accuracy for interpretability vs. the optimal ``$k$-means-esque plus local principal component analysis (PCA)'' piecewise affine autoencoder. We characterise the underlying geometry of (TopK) SAEs using power diagrams. And we develop a novel proximal alternating method SGD (PAM-SGD) algorithm for training SAEs, with both solid theoretical foundations and promising empirical results in MNIST and LLM experiments, particularly in sample efficiency and (in the LLM setting) improved sparsity of codes. All code is available at: https://github.com/splInterp2025/splInterp",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ çš„ spline theory å¢å¼ºäº†å¯¹ Sparse Autoencoders (SAEs) çš„ç†è®ºç†è§£ï¼Œå°† SAEs è§†ä¸º k-means autoencoders çš„ piecewise affine æ³›åŒ–å½¢å¼ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç›¸æ¯”äºæœ€ä¼˜çš„ piecewise affine autoencoderï¼Œç°æœ‰çš„ SAEs ä¸ºäº†å¯è§£é‡Šæ€§ç‰ºç‰²äº†ä¸€å®šçš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¼•å…¥ power diagramsï¼Œä½œè€…æˆåŠŸåˆ»ç”»äº† TopK SAEs çš„åº•å±‚å‡ ä½•ç»“æ„ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å…·æœ‰åšå®ç†è®ºæ”¯æ’‘çš„æ–°å‹è®­ç»ƒç®—æ³• PAM-SGD (Proximal Alternating Method SGD)ã€‚åœ¨ MNIST å’Œ LLM ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥ç®—æ³•åœ¨æé«˜ sample efficiency å’Œå¢å¼ºä»£ç  sparsity æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºç†è§£å’Œä¼˜åŒ–æœºæ¢°å¯è§£é‡Šæ€§å·¥å…·æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "44 pages, 38 figures, under review",
      "pdf_url": "https://arxiv.org/pdf/2505.11836v1",
      "published_date": "2025-05-17 04:51:26 UTC",
      "updated_date": "2025-05-17 04:51:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:59:26.192669+00:00"
    },
    {
      "arxiv_id": "2505.11835v2",
      "title": "Multilingual Collaborative Defense for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€ååŒé˜²å¾¡",
      "authors": [
        "Hongliang Li",
        "Jinan Xu",
        "Gengping Cui",
        "Changhao Guan",
        "Fengran Mo",
        "Kaiyu Huang"
      ],
      "abstract": "The robustness and security of large language models (LLMs) has become a prominent research area. One notable vulnerability is the ability to bypass LLM safeguards by translating harmful queries into rare or underrepresented languages, a simple yet effective method of \"jailbreaking\" these models. Despite the growing concern, there has been limited research addressing the safeguarding of LLMs in multilingual scenarios, highlighting an urgent need to enhance multilingual safety. In this work, we investigate the correlation between various attack features across different languages and propose Multilingual Collaborative Defense (MCD), a novel learning method that optimizes a continuous, soft safety prompt automatically to facilitate multilingual safeguarding of LLMs. The MCD approach offers three advantages: First, it effectively improves safeguarding performance across multiple languages. Second, MCD maintains strong generalization capabilities while minimizing false refusal rates. Third, MCD mitigates the language safety misalignment caused by imbalances in LLM training corpora. To evaluate the effectiveness of MCD, we manually construct multilingual versions of commonly used jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess various safeguarding methods. Additionally, we introduce these datasets in underrepresented (zero-shot) languages to verify the language transferability of MCD. The results demonstrate that MCD outperforms existing approaches in safeguarding against multilingual jailbreak attempts while also exhibiting strong language transfer capabilities. Our code is available at https://github.com/HLiang-Lee/MCD.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å®¹æ˜“é€šè¿‡å°†æœ‰å®³æŒ‡ä»¤ç¿»è¯‘æˆå†·é—¨è¯­è¨€è€Œè¢«â€œè¶Šç‹±â€ (jailbreaking) çš„æ¼æ´ï¼Œæå‡ºäº†å¤šè¯­è¨€åä½œé˜²å¾¡ (Multilingual Collaborative Defense, MCD) æ–¹æ³•ã€‚MCD é€šè¿‡è‡ªåŠ¨ä¼˜åŒ–è¿ç»­çš„è½¯å®‰å…¨æç¤º (soft safety prompt)ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„å®‰å…¨æ€§ã€‚è¯¥æ–¹æ³•åœ¨æå‡è·¨è¯­è¨€é˜²æŠ¤æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒè¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›å¹¶é™ä½è¯¯æ‹’ç»ç‡ (false refusal rates)ï¼Œæœ‰æ•ˆç¼“è§£äº†ç”±è®­ç»ƒæ•°æ®ä¸å¹³è¡¡å¯¼è‡´çš„å®‰å…¨å¯¹é½å¤±é…é—®é¢˜ã€‚ç ”ç©¶è€…é€šè¿‡æ„å»º MaliciousInstruct å’Œ AdvBench çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•é›†ï¼Œå¹¶åœ¨é›¶æ ·æœ¬ (zero-shot) åœºæ™¯ä¸‹éªŒè¯äº†è¯¥æ–¹æ¡ˆã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMCD åœ¨é˜²å¾¡å¤šè¯­è¨€è¶Šç‹±å°è¯•æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶è¡¨ç°å‡ºå“è¶Šçš„è¯­è¨€è¿ç§»èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 4figures",
      "pdf_url": "https://arxiv.org/pdf/2505.11835v2",
      "published_date": "2025-05-17 04:47:16 UTC",
      "updated_date": "2025-09-15 04:49:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T09:59:37.803856+00:00"
    },
    {
      "arxiv_id": "2505.11833v1",
      "title": "ToLeaP: Rethinking Development of Tool Learning with Large Language Models",
      "title_zh": "ToLeaPï¼šå¯¹å¤§è¯­è¨€æ¨¡å‹å·¥å…·å­¦ä¹ å‘å±•çš„é‡æ–°å®¡è§†",
      "authors": [
        "Haotian Chen",
        "Zijun Song",
        "Boye Niu",
        "Ke Zhang",
        "Litu Ou",
        "Yaxi Lu",
        "Zhong Zhang",
        "Xin Cong",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Tool learning, which enables large language models (LLMs) to utilize external tools effectively, has garnered increasing attention for its potential to revolutionize productivity across industries. Despite rapid development in tool learning, key challenges and opportunities remain understudied, limiting deeper insights and future advancements. In this paper, we investigate the tool learning ability of 41 prevalent LLMs by reproducing 33 benchmarks and enabling one-click evaluation for seven of them, forming a Tool Learning Platform named ToLeaP. We also collect 21 out of 33 potential training datasets to facilitate future exploration. After analyzing over 3,000 bad cases of 41 LLMs based on ToLeaP, we identify four main critical challenges: (1) benchmark limitations induce both the neglect and lack of (2) autonomous learning, (3) generalization, and (4) long-horizon task-solving capabilities of LLMs. To aid future advancements, we take a step further toward exploring potential directions, namely (1) real-world benchmark construction, (2) compatibility-aware autonomous learning, (3) rationale learning by thinking, and (4) identifying and recalling key clues. The preliminary experiments demonstrate their effectiveness, highlighting the need for further research and exploration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ToLeaPï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é‡æ–°å®¡è§†å’Œæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å·¥å…·å­¦ä¹ (Tool learning)èƒ½åŠ›å‘å±•çš„ç»¼åˆå¹³å°ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¤ç°33ä¸ªåŸºå‡†æµ‹è¯•(benchmarks)è¯„ä¼°äº†41ç§ä¸»æµLLMsçš„æ€§èƒ½ï¼Œå¹¶é›†æˆäº†21ä¸ªè®­ç»ƒæ•°æ®é›†ä»¥æ”¯æŒåç»­æ¢ç´¢ã€‚é€šè¿‡å¯¹è¶…è¿‡3,000ä¸ªé”™è¯¯æ¡ˆä¾‹(bad cases)çš„åˆ†æï¼Œç ”ç©¶è¯†åˆ«äº†å·¥å…·å­¦ä¹ é¢†åŸŸé¢ä¸´çš„å››å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå³åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ã€è‡ªä¸»å­¦ä¹ (autonomous learning)çš„ç¼ºå¤±ã€æ³›åŒ–æ€§(generalization)ä¸è¶³ä»¥åŠé•¿ç¨‹ä»»åŠ¡å¤„ç†(long-horizon task-solving)èƒ½åŠ›çš„æ¬ ç¼ºã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œç ”ç©¶æ¢è®¨äº†æ„å»ºçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ã€æ„ŸçŸ¥å…¼å®¹æ€§çš„è‡ªä¸»å­¦ä¹ ã€æ¨ç†å­¦ä¹ (rationale learning)ä»¥åŠå…³é”®çº¿ç´¢å¬å›ç­‰æœªæ¥ç ”ç©¶æ–¹å‘ã€‚åˆæ­¥å®éªŒç»“æœè¯æ˜äº†è¿™äº›æ–¹å‘çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæå‡LLMsåœ¨å¤æ‚åœºæ™¯ä¸‹æœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11833v1",
      "published_date": "2025-05-17 04:39:47 UTC",
      "updated_date": "2025-05-17 04:39:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:00:23.938812+00:00"
    },
    {
      "arxiv_id": "2505.11831v2",
      "title": "ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems",
      "title_zh": "ARC-AGI-2ï¼šé¢å‘å‰æ²¿äººå·¥æ™ºèƒ½æ¨ç†ç³»ç»Ÿçš„å…¨æ–°æŒ‘æˆ˜",
      "authors": [
        "Francois Chollet",
        "Mike Knoop",
        "Gregory Kamradt",
        "Bryan Landers",
        "Henry Pinkard"
      ],
      "abstract": "The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI), introduced in 2019, established a challenging benchmark for evaluating the general fluid intelligence of artificial systems via a set of unique, novel tasks only requiring minimal prior knowledge. While ARC-AGI has spurred significant research activity over the past five years, recent AI progress calls for benchmarks capable of finer-grained evaluation at higher levels of cognitive complexity. We introduce ARC-AGI-2, an upgraded version of the benchmark. ARC-AGI-2 preserves the input-output pair task format of its predecessor, ensuring continuity for researchers. It incorporates a newly curated and expanded set of tasks specifically designed to provide a more granular signal to assess abstract reasoning and problem-solving abilities at higher levels of fluid intelligence. To contextualize the difficulty and characteristics of ARC-AGI-2, we present extensive results from human testing, providing a robust baseline that highlights the benchmark's accessibility to human intelligence, yet difficulty for current AI systems. ARC-AGI-2 aims to serve as a next-generation tool for rigorously measuring progress towards more general and human-like AI capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†ARC-AGI-2ï¼Œè¿™æ˜¯å¯¹2019å¹´æå‡ºçš„é€šç”¨äººå·¥æ™ºèƒ½æŠ½è±¡ä¸æ¨ç†åŸºå‡†(ARC-AGI)çš„å…¨é¢å‡çº§ã€‚ç”±äºäººå·¥æ™ºèƒ½çš„é£é€Ÿå‘å±•ï¼Œç°æœ‰çš„è¯„æµ‹ä½“ç³»éœ€è¦æ›´ç²¾ç»†ä¸”å…·æœ‰æ›´é«˜è®¤çŸ¥å¤æ‚åº¦çš„åŸºå‡†æ¥è¯„ä¼°å‰æ²¿ç³»ç»Ÿã€‚ARC-AGI-2ä¿ç•™äº†å‰ä½œçš„è¾“å…¥-è¾“å‡ºå¯¹(input-output pair)ä»»åŠ¡æ ¼å¼ä»¥ç¡®ä¿ç ”ç©¶çš„è¿ç»­æ€§ï¼Œå¹¶é€šè¿‡æ–°å¢ä¸”æ‰©å±•çš„ä»»åŠ¡é›†ï¼Œä¸ºè¯„ä¼°é«˜æ°´å¹³æµä½“æ™ºèƒ½(fluid intelligence)ä¸‹çš„æŠ½è±¡æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›æä¾›æ›´ç»†ç²’åº¦çš„ä¿¡å·ã€‚ç ”ç©¶å›¢é˜Ÿæä¾›äº†å¹¿æ³›çš„äººç±»æµ‹è¯•ç»“æœä½œä¸ºåŸºå‡†ï¼Œå¼ºè°ƒäº†è¯¥ä»»åŠ¡é›†å¯¹äººç±»è€Œè¨€ç›¸å¯¹ç›´è§‚ï¼Œä½†å¯¹å½“å‰AIç³»ç»Ÿä»å…·æœ‰æå¤§çš„æŒ‘æˆ˜æ€§ã€‚ARC-AGI-2æ—¨åœ¨ä½œä¸ºä¸‹ä¸€ä»£è¯„ä¼°å·¥å…·ï¼Œä¸¥è°¨åœ°è¡¡é‡AIåœ¨å®ç°æ›´é€šç”¨ã€æ›´ç±»äººèƒ½åŠ›æ–¹é¢çš„å®é™…è¿›å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11831v2",
      "published_date": "2025-05-17 04:34:48 UTC",
      "updated_date": "2026-01-15 23:30:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:01:20.306814+00:00"
    },
    {
      "arxiv_id": "2505.14708v1",
      "title": "DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance",
      "title_zh": "DraftAttentionï¼šåŸºäºä½åˆ†è¾¨ç‡æ³¨æ„åŠ›å¼•å¯¼çš„è§†é¢‘æ‰©æ•£åŠ é€Ÿ",
      "authors": [
        "Xuan Shen",
        "Chenxia Han",
        "Yufa Zhou",
        "Yanyue Xie",
        "Yifan Gong",
        "Quanyi Wang",
        "Yiwei Wang",
        "Yanzhi Wang",
        "Pu Zhao",
        "Jiuxiang Gu"
      ],
      "abstract": "Diffusion transformer-based video generation models (DiTs) have recently attracted widespread attention for their excellent generation quality. However, their computational cost remains a major bottleneck-attention alone accounts for over 80% of total latency, and generating just 8 seconds of 720p video takes tens of minutes-posing serious challenges to practical application and scalability. To address this, we propose the DraftAttention, a training-free framework for the acceleration of video diffusion transformers with dynamic sparse attention on GPUs. We apply down-sampling to each feature map across frames in the compressed latent space, enabling a higher-level receptive field over the latent composed of hundreds of thousands of tokens. The low-resolution draft attention map, derived from draft query and key, exposes redundancy both spatially within each feature map and temporally across frames. We reorder the query, key, and value based on the draft attention map to guide the sparse attention computation in full resolution, and subsequently restore their original order after the attention computation. This reordering enables structured sparsity that aligns with hardware-optimized execution. Our theoretical analysis demonstrates that the low-resolution draft attention closely approximates the full attention, providing reliable guidance for constructing accurate sparse attention. Experimental results show that our method outperforms existing sparse attention approaches in video generation quality and achieves up to 1.75x end-to-end speedup on GPUs. Code: https://github.com/shawnricecake/draft-attention",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘æ‰©æ•£å˜æ¢å™¨ (Diffusion Transformers, DiTs) è®¡ç®—æˆæœ¬é«˜ã€Attention æœºåˆ¶è€—æ—¶å æ¯”å¤§çš„ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº† DraftAttention æ¡†æ¶ã€‚è¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„åŠ é€Ÿæ–¹æ¡ˆï¼Œé€šè¿‡åœ¨ GPU ä¸Šåº”ç”¨åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶æ¥æ˜¾è‘—æå‡è§†é¢‘ç”Ÿæˆæ•ˆç‡ã€‚è¯¥æ–¹æ³•é¦–å…ˆå¯¹æ½œç©ºé—´ä¸­çš„ç‰¹å¾å›¾è¿›è¡Œä¸‹é‡‡æ · (Down-sampling) ä»¥æ„å»ºä½åˆ†è¾¨ç‡çš„ Draft Attention æ˜ å°„ï¼Œä»è€Œè¯†åˆ«å¹¶æ•æ‰è§†é¢‘åºåˆ—åœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦çš„å†—ä½™ä¿¡æ¯ã€‚é€šè¿‡æ ¹æ®è¯¥æ˜ å°„å¯¹ Queryã€Key å’Œ Value è¿›è¡Œé‡æ’åºï¼Œæ¡†æ¶èƒ½å¤Ÿå¼•å¯¼å…¨åˆ†è¾¨ç‡ä¸‹çš„ç¨€ç–æ³¨æ„åŠ›è®¡ç®—ï¼Œå¹¶å®ç°ä¸ç¡¬ä»¶ä¼˜åŒ–ç›¸åŒ¹é…çš„ç»“æ„åŒ–ç¨€ç–ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œä½åˆ†è¾¨ç‡æ˜ å°„èƒ½å¤Ÿæœ‰æ•ˆè¿‘ä¼¼å®Œæ•´çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œä¸ºæ„å»ºå‡†ç¡®çš„ç¨€ç–æ³¨æ„åŠ›æä¾›äº†å¯é æŒ‡å¯¼ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDraftAttention åœ¨ä¿æŒä¼˜å¼‚è§†é¢‘ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œåœ¨ GPU ä¸Šå®ç°äº†é«˜è¾¾ 1.75 å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint Version",
      "pdf_url": "https://arxiv.org/pdf/2505.14708v1",
      "published_date": "2025-05-17 04:34:34 UTC",
      "updated_date": "2025-05-17 04:34:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:00:21.216005+00:00"
    },
    {
      "arxiv_id": "2505.11830v3",
      "title": "VISTA: Mitigating Semantic Inertia in Video-LLMs via Training-Free Dynamic Chain-of-Thought Routing",
      "title_zh": "VISTAï¼šé€šè¿‡å…è®­ç»ƒåŠ¨æ€æ€ç»´é“¾è·¯ç”±ç¼“è§£è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¯­ä¹‰æƒ¯æ€§",
      "authors": [
        "Hongbo Jin",
        "Jiayu Ding",
        "Siyi Xie",
        "Guibo Luo",
        "Ge Li"
      ],
      "abstract": "Recent advancements in Large Language Models have successfully transitioned towards System 2 reasoning, yet applying these paradigms to video understanding remains challenging. While prevailing research attributes failures in Video-LLMs to perceptual limitations, our empirical analysis reveals a cognitive misalignment termed Semantic Inertia, where models suppress valid visual evidence in favor of dominant language priors. To rectify this, we propose VISTA, a training-free framework designed to align perception with logical deduction. By dynamically routing inference paths and materializing implicit visual features into explicit textual anchors, our approach effectively counterbalances the influence of parametric knowledge. Furthermore, we incorporate a Latent Reasoning Consensus mechanism to mitigate stochastic hallucinations. VISTA showed outstanding results on a wide range of benchmarks, and outperforms its base model by 9.3% on Egochema and 5.6% on VideoEspresso, rivalling or even surpassing larger and proprietary models. Our codebase will be publicly available soon.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Video-LLMs åœ¨è§†é¢‘ç†è§£ä¸­å­˜åœ¨çš„è®¤çŸ¥å¤±è°ƒé—®é¢˜ï¼Œæ­ç¤ºäº†æ¨¡å‹å€¾å‘äºä¾èµ–è¯­è¨€å…ˆéªŒè€Œéè§†è§‰è¯æ®çš„ Semantic Inertia ç°è±¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº† VISTAï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„åŠ¨æ€ Chain-of-Thought è·¯ç”±æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é€»è¾‘æ¨æ¼”å®ç°æ„ŸçŸ¥å¯¹é½ã€‚VISTA é€šè¿‡åŠ¨æ€è·¯ç”±æ¨ç†è·¯å¾„å¹¶å°†éšå¼è§†è§‰ç‰¹å¾è½¬åŒ–ä¸ºæ˜¾å¼çš„ textual anchorsï¼Œæœ‰æ•ˆæŠµæ¶ˆäº†å‚æ•°åŒ–çŸ¥è¯†çš„è´Ÿé¢å½±å“ï¼ŒåŒæ—¶å¼•å…¥ Latent Reasoning Consensus æœºåˆ¶ä»¥å‡è½»éšæœºå¹»è§‰ã€‚å®éªŒè¡¨æ˜ï¼ŒVISTA åœ¨ Egochema å’Œ VideoEspresso åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æ¯”åŸºå‡†æ¨¡å‹æå‡äº† 9.3% å’Œ 5.6% çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶åœ¨å¹¿æ³›çš„æµ‹è¯•ä¸­å±•ç°äº†å“è¶Šçš„ç«äº‰åŠ›ï¼Œå…¶è¡¨ç°è¶³ä»¥åª²ç¾ç”šè‡³è¶…è¶Šè§„æ¨¡æ›´å¤§çš„æ¨¡å‹æˆ–ä¸“æœ‰æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.11830v3",
      "published_date": "2025-05-17 04:34:32 UTC",
      "updated_date": "2026-01-07 14:58:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:00:25.896092+00:00"
    },
    {
      "arxiv_id": "2506.06292v2",
      "title": "Mutual-Taught for Co-adapting Policy and Reward Models",
      "title_zh": "Mutual-Taughtï¼šç­–ç•¥ä¸å¥–åŠ±æ¨¡å‹çš„ååŒè‡ªé€‚åº”",
      "authors": [
        "Tianyuan Shi",
        "Canbin Huang",
        "Fanqi Wan",
        "Longguang Zhong",
        "Ziyi Yang",
        "Weizhou Shen",
        "Xiaojun Quan",
        "Ming Yan"
      ],
      "abstract": "During the preference optimization of large language models (LLMs), distribution shifts may arise between newly generated model samples and the data used to train the reward model (RM). This shift reduces the efficacy of the RM, which in turn negatively impacts the performance of the policy model (PM). To address this challenge, we propose Mutual-Taught, a self-training method that iteratively improves both the PM and RM without requiring additional human annotation. Our approach mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM, guiding the PM toward a better approximation of the latent optimal preference distribution. In the M-step, we update the RM by constructing training data from the outputs of the PM before and after the E-step update. This process ensures that the RM adapts to the evolving policy distribution. Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, our 8B policy model, LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\\% on AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par with GPT-4o-2024-08-06 on RewardBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åå¥½ä¼˜åŒ–ä¸­ç­–ç•¥æ¨¡å‹(PM)ç”Ÿæˆæ ·æœ¬ä¸å¥–åŠ±æ¨¡å‹(RM)è®­ç»ƒæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒåç§»(distribution shifts)é—®é¢˜ï¼Œæå‡ºäº†Mutual-Taughtè‡ªè®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†æœŸæœ›æœ€å¤§åŒ–(EM)ç®—æ³•çš„é€»è¾‘ï¼Œåœ¨Eæ­¥(E-step)ä¸­åˆ©ç”¨å½“å‰å¥–åŠ±æ¨¡å‹çš„åé¦ˆæ›´æ–°ç­–ç•¥æ¨¡å‹ï¼Œä½¿å…¶æ›´æ¥è¿‘æ½œåœ¨çš„æœ€ä¼˜åå¥½åˆ†å¸ƒï¼Œè€Œåœ¨Mæ­¥(M-step)ä¸­åˆ™é€šè¿‡ç­–ç•¥æ¨¡å‹æ›´æ–°å‰åçš„è¾“å‡ºæ„å»ºæ•°æ®æ¥æ›´æ–°å¥–åŠ±æ¨¡å‹ï¼Œç¡®ä¿å…¶é€‚åº”ä¸æ–­æ¼”åŒ–çš„ç­–ç•¥åˆ†å¸ƒã€‚è¿™ç§è¿­ä»£æ–¹å¼èƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œå®ç°ç­–ç•¥æ¨¡å‹ä¸å¥–åŠ±æ¨¡å‹çš„ååŒè¿›åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºè¯¥æ–¹æ³•è®­ç»ƒçš„LLaMA-3-8B-Instruct-MTåœ¨AlpacaEval-2ä¸Šè¾¾åˆ°äº†54.1%çš„å—æ§é•¿åº¦èƒœç‡ï¼ŒåŒæ—¶å…¶å¥–åŠ±æ¨¡å‹FsfairX-LLaMA3-RM-MTåœ¨RewardBenchä¸Šçš„è¡¨ç°å·²èƒ½ä¸GPT-4oæŒå¹³ï¼Œè¯æ˜äº†è¯¥è¿­ä»£ä¼˜åŒ–æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ACL 2025 (Main Conference)",
      "pdf_url": "https://arxiv.org/pdf/2506.06292v2",
      "published_date": "2025-05-17 04:34:23 UTC",
      "updated_date": "2025-06-10 03:32:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:01:38.917793+00:00"
    },
    {
      "arxiv_id": "2505.11827v2",
      "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning",
      "title_zh": "å¹¶éæ‰€æœ‰æ€ç»´çš†å¹³ç­‰ï¼šåŸºäºå¤šè½®å¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹æ¨ç†",
      "authors": [
        "Yansong Ning",
        "Wei Li",
        "Jun Fang",
        "Naiqiang Tan",
        "Hao Liu"
      ],
      "abstract": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is an emerging strategy to improve the reasoning efficiency of LLMs. Despite its promising benefits, existing studies equally compress all thoughts within a long CoT, hindering more concise and effective reasoning. To this end, we first investigate the importance of different thoughts by examining their effectiveness and efficiency in contributing to reasoning through automatic long CoT chunking and Monte Carlo rollouts. Building upon the insights, we propose a theoretically bounded metric to jointly measure the effectiveness and efficiency of different thoughts. We then propose Long$\\otimes$Short, an efficient reasoning framework that enables two LLMs to collaboratively solve the problem: a long-thought LLM for more effectively generating important thoughts, while a short-thought LLM for efficiently generating remaining thoughts. Specifically, we begin by synthesizing a small amount of cold-start data to fine-tune LLMs for long-thought and short-thought reasoning styles, respectively. Furthermore, we propose a synergizing-oriented multi-turn reinforcement learning, focusing on the model self-evolution and collaboration between long-thought and short-thought LLMs. Experimental results show that our method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and GPQA Diamond benchmarks. Our data and code are available at https://github.com/usail-hkust/LongShort.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å‹ç¼©é•¿é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ—¶å­˜åœ¨çš„æ•ˆç‡ç“¶é¢ˆï¼ŒæŒ‡å‡ºå¹¶éæ‰€æœ‰æ€ç»´æ­¥éª¤å¯¹æ¨ç†ç»“æœçš„è´¡çŒ®éƒ½æ˜¯å‡ç­‰çš„ã€‚ä½œè€…é€šè¿‡è‡ªåŠ¨é•¿CoTåˆ‡å—å’ŒMonte Carlo rolloutsåˆ†æäº†ä¸åŒæ€ç»´å—çš„æœ‰æ•ˆæ€§ä¸æ•ˆç‡ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç”¨äºé‡åŒ–æ€ç»´é‡è¦æ€§çš„åº¦é‡æŒ‡æ ‡ã€‚åŸºäºæ­¤ç ”ç©¶æå‡ºäº†Long$\\otimes$Shortæ¨ç†æ¡†æ¶ï¼Œç”±ä¸€ä¸ªé•¿æ€ç»´LLMè´Ÿè´£ç”Ÿæˆå…³é”®çš„æ·±åº¦æ€ç»´ï¼Œè€ŒçŸ­æ€ç»´LLMåˆ™è´Ÿè´£é«˜æ•ˆè¡¥å…¨å‰©ä½™çš„æ¨ç†éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†é¢å‘ååŒçš„å¤šè½®å¼ºåŒ–å­¦ä¹ (multi-turn reinforcement learning)ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹é—´çš„åä½œä¸è‡ªæˆ‘è¿›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MATH500ã€AIME24/25ã€AMC23åŠGPQA Diamondç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿Qwen2.5-7Bå’ŒLlama3.1-8Båœ¨æ€§èƒ½æ¯”è‚©DeepSeek-R1-Distillæ¨¡å‹çš„åŒæ—¶ï¼Œå°†ç”Ÿæˆçš„tokené•¿åº¦æ˜¾è‘—å‡å°‘äº†80%ä»¥ä¸Šã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2505.11827v2",
      "published_date": "2025-05-17 04:26:39 UTC",
      "updated_date": "2025-05-25 04:09:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:00:37.881460+00:00"
    },
    {
      "arxiv_id": "2505.11825v1",
      "title": "Bootstrapping Diffusion: Diffusion Model Training Leveraging Partial and Corrupted Data",
      "title_zh": "Bootstrapping Diffusionï¼šåˆ©ç”¨ä¸å®Œæ•´å’Œå—æŸæ•°æ®è¿›è¡Œæ‰©æ•£æ¨¡å‹è®­ç»ƒ",
      "authors": [
        "Xudong Ma"
      ],
      "abstract": "Training diffusion models requires large datasets. However, acquiring large volumes of high-quality data can be challenging, for example, collecting large numbers of high-resolution images and long videos. On the other hand, there are many complementary data that are usually considered corrupted or partial, such as low-resolution images and short videos. Other examples of corrupted data include videos that contain subtitles, watermarks, and logos. In this study, we investigate the theoretical problem of whether the above partial data can be utilized to train conventional diffusion models. Motivated by our theoretical analysis in this study, we propose a straightforward approach of training diffusion models utilizing partial data views, where we consider each form of complementary data as a view of conventional data. Our proposed approach first trains one separate diffusion model for each individual view, and then trains a model for predicting the residual score function. We prove generalization error bounds, which show that the proposed diffusion model training approach can achieve lower generalization errors if proper regularizations are adopted in the residual score function training. In particular, we prove that the difficulty in training the residual score function scales proportionally with the signal correlations not captured by partial data views. Consequently, the proposed approach achieves near first-order optimal data efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†è·å–å›°éš¾çš„æŒ‘æˆ˜ï¼Œæ¢è®¨äº†åˆ©ç”¨ä½åˆ†è¾¨ç‡å›¾åƒã€å¸¦æ°´å°è§†é¢‘ç­‰ä¸å®Œæ•´æˆ–å—æŸæ•°æ®è®­ç»ƒæ‰©æ•£æ¨¡å‹ (Diffusion Models) çš„ç†è®ºå¯è¡Œæ€§ã€‚ä½œè€…æå‡ºå°†å„ç±»äº’è¡¥æ•°æ®è§†ä¸ºå¸¸è§„æ•°æ®çš„è§†å›¾ (Views)ï¼Œé€šè¿‡å…ˆä¸ºå„è§†å›¾è®­ç»ƒç‹¬ç«‹æ¨¡å‹ã€å†è®­ç»ƒæ®‹å·®å¾—åˆ†å‡½æ•° (Residual Score Function) çš„åˆ†æ­¥ç­–ç•¥å®ç°æ¨¡å‹ä¼˜åŒ–ã€‚ç†è®ºåˆ†æç»™å‡ºäº†æ³›åŒ–è¯¯å·®ç•Œé™ï¼Œè¯æ˜åœ¨å¼•å…¥é€‚å½“æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆé™ä½æ³›åŒ–è¯¯å·®ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œæ®‹å·®å¾—åˆ†å‡½æ•°çš„è®­ç»ƒéš¾åº¦å–å†³äºå„è§†å›¾æœªæ¶µç›–çš„ä¿¡å·ç›¸å…³æ€§ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®æ•ˆç‡ä¸Šè¾¾åˆ°äº†æ¥è¿‘ä¸€é˜¶æœ€ä¼˜çš„æ°´å¹³ï¼Œä¸ºåˆ©ç”¨å¤šæºä½è´¨é‡æ•°æ®å¢å¼ºç”Ÿæˆæ¨¡å‹æ€§èƒ½æä¾›äº†å¼ºæœ‰åŠ›çš„ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2505.11825v1",
      "published_date": "2025-05-17 04:17:48 UTC",
      "updated_date": "2025-05-17 04:17:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:00:40.642694+00:00"
    },
    {
      "arxiv_id": "2505.11824v2",
      "title": "Latent Veracity Inference for Identifying Errors in Stepwise Reasoning",
      "title_zh": "é¢å‘åˆ†æ­¥æ¨ç†é”™è¯¯è¯†åˆ«çš„éšå¼çœŸå®æ€§æ¨æ–­",
      "authors": [
        "Minsu Kim",
        "Jean-Pierre Falet",
        "Oliver E. Richardson",
        "Xiaoyin Chen",
        "Moksh Jain",
        "Sungjin Ahn",
        "Sungsoo Ahn",
        "Yoshua Bengio"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and transparency of language models (LMs); however, reasoning chains can contain inaccurate statements that reduce performance and trustworthiness. To address this, we propose to augment each reasoning step in a CoT with a latent veracity (or correctness) variable. To efficiently explore this expanded space, we introduce Veracity Search (VS), a discrete search algorithm over veracity assignments. It performs otherwise intractable inference in the posterior distribution over latent veracity values by leveraging the LM's joint likelihood over veracity and the final answer as a proxy reward. This efficient inference-time verification method facilitates supervised fine-tuning of an Amortized Veracity Inference (AVI) machine by providing pseudo-labels for veracity. AVI generalizes VS, enabling accurate zero-shot veracity inference in novel contexts. Empirical results demonstrate that VS reliably identifies errors in logical (ProntoQA), mathematical (GSM8K), and commonsense (CommonsenseQA) reasoning benchmarks, with AVI achieving comparable zero-shot accuracy. Finally, we demonstrate the utility of latent veracity inference for providing feedback during self-correction and self-improvement.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†ä¸­å¯èƒ½å‡ºç°é”™è¯¯é™ˆè¿°è€Œé™ä½æ¨¡å‹æ€§èƒ½ä¸å¯é æ€§çš„é—®é¢˜ï¼Œæå‡ºåœ¨CoTçš„æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­å¢åŠ æ½œåœ¨çœŸå®æ€§(latent veracity)å˜é‡ã€‚ä¸ºäº†é«˜æ•ˆæ¢ç´¢è¿™ä¸€ç©ºé—´ï¼Œç ”ç©¶è€…å¼•å…¥äº†Veracity Search (VS)ç®—æ³•ï¼Œé€šè¿‡åˆ©ç”¨è¯­è¨€æ¨¡å‹å¯¹çœŸå®æ€§å’Œæœ€ç»ˆç­”æ¡ˆçš„è”åˆä¼¼ç„¶ä½œä¸ºä»£ç†å¥–åŠ±ï¼Œåœ¨ç¦»æ•£ç©ºé—´ä¸­æ‰§è¡ŒçœŸå®æ€§åˆ†é…æœç´¢ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡æä¾›ä¼ªæ ‡ç­¾æ”¯æŒç›‘ç£å¾®è°ƒï¼Œæ„å»ºäº†æ‘Šé”€çœŸå®æ€§æ¨ç†(Amortized Veracity Inference, AVI)æ¨¡å‹ï¼Œå®ç°äº†åœ¨æœªçŸ¥ä¸Šä¸‹æ–‡ä¸­çš„å‡†ç¡®é›¶æ ·æœ¬(zero-shot)çœŸå®æ€§æ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVSåœ¨ProntoQAã€GSM8Kå’ŒCommonsenseQAç­‰é€»è¾‘ã€æ•°å­¦åŠå¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸­èƒ½å¯é è¯†åˆ«é”™è¯¯ï¼Œä¸”AVIå…·æœ‰ç›¸å½“çš„é›¶æ ·æœ¬æ¨ç†ç²¾åº¦ã€‚è¯¥ç ”ç©¶æœ€åè¯æ˜äº†æ½œåœ¨çœŸå®æ€§æ¨ç†åœ¨ä¸ºæ¨¡å‹è‡ªæˆ‘ä¿®æ­£(self-correction)å’Œè‡ªæˆ‘æ”¹è¿›(self-improvement)æä¾›åé¦ˆæ–¹é¢çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11824v2",
      "published_date": "2025-05-17 04:16:36 UTC",
      "updated_date": "2025-09-26 03:18:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:00:44.814147+00:00"
    },
    {
      "arxiv_id": "2505.13516v1",
      "title": "HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems",
      "title_zh": "HALOï¼šé¢å‘å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿçš„å±‚çº§åŒ–è‡ªä¸»é€»è¾‘å¯¼å‘ç¼–æ’",
      "authors": [
        "Zhipeng Hou",
        "Junyi Tang",
        "Yipeng Wang"
      ],
      "abstract": "Recent advancements in Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) have demonstrated tremendous potential in diverse task scenarios. Nonetheless, existing agentic systems typically rely on predefined agent-role design spaces and static communication structures, limiting their adaptability as well as flexibility in complex interaction environments and leading to subpar performance on highly specialized and expert-level tasks. To address these issues, we introduce HALO, a multi-agent collaboration framework based on a hierarchical reasoning architecture. Specifically, we incorporate a high-level planning agent for task decomposition, mid-level role-design agents for subtask-specific agent instantiation, and low-level inference agents for subtask execution. Particularly, subtask execution is reformulated as a structured workflow search problem, where Monte Carlo Tree Search (MCTS) systematically explores the agentic action space to construct optimal reasoning trajectories. Additionally, as the majority of users lack expertise in prompt engineering, we leverage an Adaptive Prompt Refinement module to transform raw queries into task-specific prompts. Empirical evaluations on Code Generation (HumanEval), General Reasoning (MMLU), and Arithmetic Reasoning (MATH) benchmark datasets highlight the effectiveness of HALO, yielding a 14.4% average improvement over state-of-the-art baselines. Notably, HALO achieves up to 13.3% performance gain on the Moral Scenarios subject in the MMLU benchmark and up to 19.6% performance gain on the Algebra subarea in the MATH benchmark, indicating its advanced proficiency in tackling highly specialized and expert-level tasks. The code repository is available at https://github.com/23japhone/HALO.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HALOï¼Œä¸€ç§åŸºäºå±‚çº§åŒ–æ¨ç†æ¶æ„çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agent Systemsï¼‰åœ¨åº”å¯¹å¤æ‚ä»»åŠ¡æ—¶å› é¢„å®šä¹‰è§’è‰²å’Œé™æ€é€šä¿¡ç»“æ„å¯¼è‡´çš„çµæ´»æ€§ä¸è¶³åŠä¸“ä¸šæ€§èƒ½å—é™ç­‰é—®é¢˜ã€‚HALO é‡‡ç”¨äº†ä¸‰å±‚æ¶æ„è®¾è®¡ï¼ŒåŒ…æ‹¬ç”¨äºä»»åŠ¡åˆ†è§£çš„é«˜å±‚è§„åˆ’æ™ºèƒ½ä½“ï¼ˆPlanning Agentï¼‰ã€è´Ÿè´£é’ˆå¯¹å­ä»»åŠ¡å®ä¾‹åŒ–ç‰¹å®šæ™ºèƒ½ä½“çš„ä¸­å±‚è§’è‰²è®¾è®¡æ™ºèƒ½ä½“ï¼ˆRole-design Agentsï¼‰ï¼Œä»¥åŠæ‰§è¡Œå…·ä½“ä»»åŠ¡çš„åº•å±‚æ¨ç†æ™ºèƒ½ä½“ï¼ˆInference Agentsï¼‰ã€‚ç‰¹åˆ«åœ°ï¼Œè¯¥æ¡†æ¶å°†å­ä»»åŠ¡æ‰§è¡Œé‡æ–°è¡¨è¿°ä¸ºç»“æ„åŒ–å·¥ä½œæµæœç´¢é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMonte Carlo Tree Search, MCTSï¼‰ç³»ç»Ÿåœ°æ¢ç´¢æ™ºèƒ½ä½“è¡ŒåŠ¨ç©ºé—´ï¼Œä»¥æ„å»ºæœ€ä¼˜æ¨ç†è·¯å¾„ã€‚æ­¤å¤–ï¼Œä¸ºäº†é™ä½æç¤ºå·¥ç¨‹ï¼ˆPrompt Engineeringï¼‰çš„é—¨æ§›ï¼ŒHALO å¼•å…¥äº†è‡ªé€‚åº”æç¤ºä¼˜åŒ–ï¼ˆAdaptive Prompt Refinementï¼‰æ¨¡å—ï¼Œå°†åŸå§‹æŸ¥è¯¢è½¬åŒ–ä¸ºç‰¹å®šä»»åŠ¡çš„ä¸“ä¸šæç¤ºã€‚åœ¨ HumanEvalã€MMLU å’Œ MATH ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHALO ç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹å®ç°äº† 14.4% çš„å¹³å‡æ€§èƒ½æå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒHALO åœ¨ MMLU çš„é“å¾·åœºæ™¯ï¼ˆMoral Scenariosï¼‰å’Œ MATH çš„ä»£æ•°ï¼ˆAlgebraï¼‰å­é¢†åŸŸè¡¨ç°å°¤ä¸ºçªå‡ºï¼Œåˆ†åˆ«æå‡äº† 13.3% å’Œ 19.6%ï¼Œå……åˆ†å±•ç¤ºäº†å…¶åœ¨å¤„ç†é«˜åº¦ä¸“ä¸šåŒ–åŠä¸“å®¶çº§ä»»åŠ¡æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "The code repository is available at https://github.com/23japhone/HALO",
      "pdf_url": "https://arxiv.org/pdf/2505.13516v1",
      "published_date": "2025-05-17 04:14:03 UTC",
      "updated_date": "2025-05-17 04:14:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:02:00.204424+00:00"
    },
    {
      "arxiv_id": "2505.13515v1",
      "title": "LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades",
      "title_zh": "LoRASuiteï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å‡çº§çš„é«˜æ•ˆ LoRA é€‚é…",
      "authors": [
        "Yanan Li",
        "Fanxu Meng",
        "Muhan Zhang",
        "Shiai Zhu",
        "Shangguang Wang",
        "Mengwei Xu"
      ],
      "abstract": "As Large Language Models (LLMs) are frequently updated, LoRA weights trained on earlier versions quickly become obsolete. The conventional practice of retraining LoRA weights from scratch on the latest model is costly, time-consuming, and environmentally detrimental, particularly as the diversity of LLMs and downstream tasks expands. This motivates a critical question: \"How can we efficiently leverage existing LoRA weights to adapt to newer model versions?\" To address this, we propose LoRASuite, a modular approach tailored specifically to various types of LLM updates. First, we compute a transfer matrix utilizing known parameters from both old and new LLMs. Next, we allocate corresponding layers and attention heads based on centered kernel alignment and cosine similarity metrics, respectively. A subsequent small-scale, skillful fine-tuning step ensures numerical stability. Experimental evaluations demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even exceeds the performance of full-scale LoRA retraining, with average improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally, LoRASuite significantly reduces memory consumption by 5.5 GB and computational time by 78.23%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LoRASuiteï¼Œä¸€ç§ä¸“é—¨ç”¨äºå¤§è¯­è¨€æ¨¡å‹ (LLMs) ç‰ˆæœ¬æ›´æ–°æ—¶è¿›è¡Œ LoRA æƒé‡é«˜æ•ˆé€‚é…çš„æ¨¡å—åŒ–æ–¹æ³•ã€‚é’ˆå¯¹æ—§ç‰ˆæœ¬ LoRA æƒé‡åœ¨æ¨¡å‹å‡çº§åå¤±æ•ˆä¸”é‡æ–°è®­ç»ƒæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼ŒLoRASuite é€šè¿‡è®¡ç®—æ–°æ—§æ¨¡å‹å‚æ•°é—´çš„è½¬æ¢çŸ©é˜µï¼Œå¹¶ç»“åˆä¸­å¿ƒæ ¸å¯¹é½ (centered kernel alignment) ä¸ä½™å¼¦ç›¸ä¼¼åº¦ (cosine similarity) æŒ‡æ ‡æ¥ç²¾å‡†åˆ†é…å±‚å’Œæ³¨æ„åŠ›å¤´ã€‚éšåï¼Œé€šè¿‡å°è§„æ¨¡çš„ fine-tuning æ­¥éª¤ç¡®ä¿é€‚é…åçš„æ•°å€¼ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRASuite åœ¨ MiniCPM å’Œ Qwen ç­‰ä¸»æµæ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ vanilla LoRAï¼Œç”šè‡³åœ¨æ•°å­¦ä»»åŠ¡ä¸Šè¶…è¶Šäº†å…¨é‡é‡æ–°è®­ç»ƒçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ•ˆç‡ï¼Œå‡å°‘äº† 5.5 GB çš„å†…å­˜å ç”¨å¹¶ç¼©çŸ­äº† 78.23% çš„è®¡ç®—æ—¶é—´ï¼Œä¸º LLM çš„æŒç»­æ¼”è¿›æä¾›äº†ä¸€ç§ä½ç¢³é«˜æ•ˆçš„é€‚é…æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.13515v1",
      "published_date": "2025-05-17 04:11:17 UTC",
      "updated_date": "2025-05-17 04:11:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:01:56.342083+00:00"
    },
    {
      "arxiv_id": "2505.11814v1",
      "title": "ChatHTN: Interleaving Approximate (LLM) and Symbolic HTN Planning",
      "title_zh": "ChatHTNï¼šè¿‘ä¼¼ï¼ˆLLMï¼‰è§„åˆ’ä¸ç¬¦å·åŒ– HTN è§„åˆ’çš„äº¤é”™é›†æˆ",
      "authors": [
        "Hector Munoz-Avila",
        "David W. Aha",
        "Paola Rizzo"
      ],
      "abstract": "We introduce ChatHTN, a Hierarchical Task Network (HTN) planner that combines symbolic HTN planning techniques with queries to ChatGPT to approximate solutions in the form of task decompositions. The resulting hierarchies interleave task decompositions generated by symbolic HTN planning with those generated by ChatGPT. Despite the approximate nature of the results generates by ChatGPT, ChatHTN is provably sound; any plan it generates correctly achieves the input tasks. We demonstrate this property with an open-source implementation of our system.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ChatHTNï¼Œè¿™æ˜¯ä¸€ç§å°†ç¬¦å·åŒ–Hierarchical Task Network (HTN)è§„åˆ’æŠ€æœ¯ä¸ChatGPTæŸ¥è¯¢ç›¸ç»“åˆçš„è§„åˆ’å™¨ï¼Œæ—¨åœ¨é€šè¿‡ä»»åŠ¡åˆ†è§£æä¾›è¿‘ä¼¼è§£ã€‚å…¶ç”Ÿæˆçš„ä»»åŠ¡å±‚çº§äº¤æ›¿åŒ…å«äº†ç¬¦å·HTNè§„åˆ’ç”Ÿæˆçš„åˆ†è§£ä¸ChatGPTç”Ÿæˆçš„åˆ†è§£ï¼Œå®ç°äº†ç¥ç»ä¸ç¬¦å·æ–¹æ³•çš„æœ‰æœºç»“åˆã€‚å°½ç®¡ChatGPTäº§ç”Ÿçš„ç»“æœå…·æœ‰è¿‘ä¼¼æ€§è´¨ï¼Œä½†ChatHTNåœ¨ç†è®ºä¸Šè¢«è¯æ˜æ˜¯å¥å…¨çš„(sound)ï¼Œèƒ½å¤Ÿç¡®ä¿å…¶ç”Ÿæˆçš„ä»»ä½•è§„åˆ’éƒ½èƒ½æ­£ç¡®å®Œæˆè¾“å…¥ä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¼€æºå®ç°å±•ç¤ºäº†è¿™ä¸€å±æ€§ï¼Œè¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨ä¿è¯é€»è¾‘æ­£ç¡®æ€§çš„å‰æä¸‹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚è¯¥å·¥ä½œä¸ºç»“åˆè¯­è¨€æ¨¡å‹çš„çµæ´»æ€§ä¸ä¼ ç»Ÿè§„åˆ’çš„ä¸¥è°¨æ€§æä¾›äº†ä¸€ç§å¯é çš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "2nd International Conference on Neuro-symbolic Systems (NeuS) 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.11814v1",
      "published_date": "2025-05-17 03:53:08 UTC",
      "updated_date": "2025-05-17 03:53:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:02:37.133452+00:00"
    },
    {
      "arxiv_id": "2505.11813v1",
      "title": "SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation",
      "title_zh": "SGD-Mixï¼šåˆ©ç”¨ä¿ç•™æ ‡ç­¾çš„æ•°æ®å¢å¼ºæå‡ç‰¹å®šé¢†åŸŸå›¾åƒåˆ†ç±»æ€§èƒ½",
      "authors": [
        "Yixuan Dong",
        "Fang-Yi Su",
        "Jung-Hsien Chiang"
      ],
      "abstract": "Data augmentation for domain-specific image classification tasks often struggles to simultaneously address diversity, faithfulness, and label clarity of generated data, leading to suboptimal performance in downstream tasks. While existing generative diffusion model-based methods aim to enhance augmentation, they fail to cohesively tackle these three critical aspects and often overlook intrinsic challenges of diffusion models, such as sensitivity to model characteristics and stochasticity under strong transformations. In this paper, we propose a novel framework that explicitly integrates diversity, faithfulness, and label clarity into the augmentation process. Our approach employs saliency-guided mixing and a fine-tuned diffusion model to preserve foreground semantics, enrich background diversity, and ensure label consistency, while mitigating diffusion model limitations. Extensive experiments across fine-grained, long-tail, few-shot, and background robustness tasks demonstrate our method's superior performance over state-of-the-art approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢†åŸŸç‰¹å®šå›¾åƒåˆ†ç±»ä¸­çš„æ•°æ®å¢å¼ºæŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºSGD-Mixçš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆæ•°æ®åœ¨å¤šæ ·æ€§(diversity)ã€å¿ å®åº¦(faithfulness)å’Œæ ‡ç­¾æ¸…æ™°åº¦(label clarity)æ–¹é¢éš¾ä»¥å…¼é¡¾çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆæ˜¾è‘—æ€§å¼•å¯¼æ··åˆ(saliency-guided mixing)ä¸å¾®è°ƒåçš„æ‰©æ•£æ¨¡å‹(diffusion model)ï¼Œåœ¨æœ‰æ•ˆä¿ç•™å‰æ™¯è¯­ä¹‰çš„åŒæ—¶ä¸°å¯Œäº†èƒŒæ™¯å¤šæ ·æ€§ï¼Œå¹¶ç¡®ä¿äº†æ ‡ç­¾çš„ä¸€è‡´æ€§ã€‚SGD-Mixé’ˆå¯¹æ€§åœ°ç¼“è§£äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹å¯¹æ¨¡å‹ç‰¹æ€§æ•æ„ŸåŠåœ¨å¼ºå˜æ¢ä¸‹å…·æœ‰éšæœºæ€§ç­‰å›ºæœ‰å±€é™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç»†ç²’åº¦(fine-grained)ã€é•¿å°¾(long-tail)ã€å°‘æ ·æœ¬(few-shot)ä»¥åŠèƒŒæ™¯é²æ£’æ€§ç­‰å¤šç§å¤æ‚åˆ†ç±»ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ã€‚å¹¿æ³›çš„æµ‹è¯•è¯æ˜ï¼ŒSGD-Mixçš„æ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†ç›®å‰ç°æœ‰çš„æœ€å…ˆè¿›(state-of-the-art)æ•°æ®å¢å¼ºæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 6 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.11813v1",
      "published_date": "2025-05-17 03:51:18 UTC",
      "updated_date": "2025-05-17 03:51:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:02:30.168880+00:00"
    },
    {
      "arxiv_id": "2505.11807v2",
      "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic",
      "title_zh": "Retrospexï¼šè¯­è¨€æ™ºèƒ½ä½“ä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ è¯„è®ºå®¶çš„ç»“åˆ",
      "authors": [
        "Yufei Xiang",
        "Yiqun Shen",
        "Yeqin Zhang",
        "Cam-Tu Nguyen"
      ],
      "abstract": "Large Language Models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experiences for improvement. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Unlike previous approaches, Retrospex does not directly integrate experiences into the LLM's context. Instead, it combines the LLM's action likelihood with action values estimated by a Reinforcement Learning (RL) Critic, which is trained on past experiences through an offline ''retrospection'' process. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its advantages over strong, contemporary baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Retrospexï¼Œè¿™æ˜¯ä¸€ç§å°†å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline Reinforcement Learning)è¯„è®ºå®¶(Critic)ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰LLMæ™ºèƒ½ä½“æ— æ³•å……åˆ†åˆ©ç”¨è¿‡å¾€ç»éªŒè¿›è¡Œè‡ªæˆ‘æå‡çš„é—®é¢˜ã€‚ä¸ç›´æ¥å°†ç»éªŒæ•´åˆè¿›ä¸Šä¸‹æ–‡çš„æ–¹æ³•ä¸åŒï¼ŒRetrospexé€šè¿‡ç¦»çº¿â€œå›æº¯â€è¿‡ç¨‹åœ¨è¿‡å¾€ç»éªŒä¸Šè®­ç»ƒå¼ºåŒ–å­¦ä¹ è¯„è®ºå®¶ï¼Œå¹¶å°†å…¶ä¼°ç®—çš„åŠ¨ä½œä»·å€¼ä¸LLMçš„åŠ¨ä½œæ¦‚ç‡ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŠ¨æ€åŠ¨ä½œé‡æ‰“åˆ†(dynamic action rescoring)æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡å¯¹ç¯å¢ƒäº¤äº’çš„éœ€æ±‚ç¨‹åº¦è‡ªåŠ¨è°ƒæ•´ç»éªŒä»·å€¼çš„æƒé‡ã€‚ç ”ç©¶åœ¨ScienceWorldã€ALFWorldå’ŒWebshopç­‰å¤šä¸ªç¯å¢ƒä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºRetrospexæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ¨¡å‹ã€‚è¯¥å·¥ä½œè¯æ˜äº†ç»“åˆç¦»çº¿å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ·±åº¦ç»éªŒåˆ†æï¼Œå¯¹äºæå‡è¯­è¨€æ™ºèƒ½ä½“åœ¨å¤æ‚äº¤äº’ä»»åŠ¡ä¸­çš„å†³ç­–èƒ½åŠ›å…·æœ‰é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, Published in EMNLP 2024 (Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing)",
      "pdf_url": "https://arxiv.org/pdf/2505.11807v2",
      "published_date": "2025-05-17 03:28:24 UTC",
      "updated_date": "2025-05-27 01:30:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:03:43.127912+00:00"
    },
    {
      "arxiv_id": "2505.11804v1",
      "title": "Are vision language models robust to uncertain inputs?",
      "title_zh": "è§†è§‰è¯­è¨€æ¨¡å‹å¯¹ä¸ç¡®å®šè¾“å…¥æ˜¯å¦å…·æœ‰é²æ£’æ€§ï¼Ÿ",
      "authors": [
        "Xi Wang",
        "Eric Nalisnick"
      ],
      "abstract": "Robustness against uncertain and ambiguous inputs is a critical challenge for deep learning models. While recent advancements in large scale vision language models (VLMs, e.g. GPT4o) might suggest that increasing model and training dataset size would mitigate this issue, our empirical evaluation shows a more complicated picture. Testing models using two classic uncertainty quantification tasks, anomaly detection and classification under inherently ambiguous conditions, we find that newer and larger VLMs indeed exhibit improved robustness compared to earlier models, but still suffer from a tendency to strictly follow instructions, often causing them to hallucinate confident responses even when faced with unclear or anomalous inputs. Remarkably, for natural images such as ImageNet, this limitation can be overcome without pipeline modifications: simply prompting models to abstain from uncertain predictions enables significant reliability gains, achieving near-perfect robustness in several settings. However, for domain-specific tasks such as galaxy morphology classification, a lack of specialized knowledge prevents reliable uncertainty estimation. Finally, we propose a novel mechanism based on caption diversity to reveal a model's internal uncertainty, enabling practitioners to predict when models will successfully abstain without relying on labeled data.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨é¢å¯¹ä¸ç¡®å®šå’Œæ¨¡ç³Šè¾“å…¥æ—¶çš„é²æ£’æ€§ï¼Œé‡ç‚¹æµ‹è¯•äº†å¼‚å¸¸æ£€æµ‹(Anomaly Detection)å’Œæ¨¡ç³Šæ¡ä»¶ä¸‹çš„åˆ†ç±»ä»»åŠ¡ã€‚å®éªŒå‘ç°ï¼Œå°½ç®¡GPT-4oç­‰æ–°å‹å¤§æ¨¡å‹è¡¨ç°å‡ºæ¯”æ—©æœŸæ¨¡å‹æ›´å¼ºçš„é²æ£’æ€§ï¼Œä½†ç”±äºå€¾å‘äºä¸¥æ ¼éµå¾ªæŒ‡ä»¤ï¼Œåœ¨é¢å¯¹ä¸æ˜ç¡®è¾“å…¥æ—¶ä»å®¹æ˜“äº§ç”Ÿè¿‡åº¦è‡ªä¿¡çš„å¹»è§‰(Hallucinate)ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé’ˆå¯¹ImageNetç­‰è‡ªç„¶å›¾åƒï¼Œä»…é€šè¿‡æç¤ºæ¨¡å‹åœ¨ä¸ç¡®å®šæ—¶é€‰æ‹©â€œå¼ƒæƒâ€(Abstain)å³å¯æ˜¾è‘—å¢å¼ºå…¶å¯é æ€§ï¼Œå¹¶åœ¨å¤šç§è®¾ç½®ä¸‹è¾¾åˆ°è¿‘ä¹å®Œç¾çš„é²æ£’æ€§ã€‚ç„¶è€Œï¼Œåœ¨æ˜Ÿç³»å½¢æ€åˆ†ç±»(Galaxy Morphology Classification)ç­‰ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­ï¼Œç”±äºç¼ºä¹ä¸“ä¸šçŸ¥è¯†ï¼Œæ¨¡å‹ä»æ— æ³•è¿›è¡Œå¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚æœ€åï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæè¿°å¤šæ ·æ€§(Caption Diversity)çš„æ–°å‹æœºåˆ¶ï¼Œæ—¨åœ¨æ— éœ€æ ‡æ³¨æ•°æ®å³å¯æ­ç¤ºæ¨¡å‹å†…éƒ¨çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶å¸®åŠ©é¢„æµ‹æ¨¡å‹ä½•æ—¶èƒ½å¤ŸæˆåŠŸå¼ƒæƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11804v1",
      "published_date": "2025-05-17 03:16:49 UTC",
      "updated_date": "2025-05-17 03:16:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:02:35.110725+00:00"
    },
    {
      "arxiv_id": "2505.11803v1",
      "title": "VITA: Versatile Time Representation Learning for Temporal Hyper-Relational Knowledge Graphs",
      "title_zh": "VITAï¼šé¢å‘æ—¶åºè¶…å…³ç³»çŸ¥è¯†å›¾è°±çš„é€šç”¨æ—¶é—´è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "ChongIn Un",
        "Yuhuan Lu",
        "Tianyue Yang",
        "Dingqi Yang"
      ],
      "abstract": "Knowledge graphs (KGs) have become an effective paradigm for managing real-world facts, which are not only complex but also dynamically evolve over time. The temporal validity of facts often serves as a strong clue in downstream link prediction tasks, which predicts a missing element in a fact. Traditional link prediction techniques on temporal KGs either consider a sequence of temporal snapshots of KGs with an ad-hoc defined time interval or expand a temporal fact over its validity period under a predefined time granularity; these approaches not only suffer from the sensitivity of the selection of time interval/granularity, but also face the computational challenges when handling facts with long (even infinite) validity. Although the recent hyper-relational KGs represent the temporal validity of a fact as qualifiers describing the fact, it is still suboptimal due to its ignorance of the infinite validity of some facts and the insufficient information encoded from the qualifiers about the temporal validity. Against this background, we propose VITA, a $\\underline{V}$ersatile t$\\underline{I}$me represen$\\underline{TA}$tion learning method for temporal hyper-relational knowledge graphs. We first propose a versatile time representation that can flexibly accommodate all four types of temporal validity of facts (i.e., since, until, period, time-invariant), and then design VITA to effectively learn the time information in both aspects of time value and timespan to boost the link prediction performance. We conduct a thorough evaluation of VITA compared to a sizable collection of baselines on real-world KG datasets. Results show that VITA outperforms the best-performing baselines in various link prediction tasks (predicting missing entities, relations, time, and other numeric literals) by up to 75.3%. Ablation studies and a case study also support our key design choices.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶åºçŸ¥è¯†å›¾è°±(Knowledge Graphs)ä¸­ä¼ ç»Ÿé“¾æ¥é¢„æµ‹æ–¹æ³•å—é™äºå›ºå®šæ—¶é—´ç²’åº¦ä¸”éš¾ä»¥å¤„ç†æ— é™æœ‰æ•ˆæœŸ(infinite validity)äº‹å®çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºVITAçš„å¤šåŠŸèƒ½æ—¶é—´è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚VITAé€šè¿‡è®¾è®¡ä¸€ç§çµæ´»çš„è¡¨ç¤ºæœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶é€‚é…sinceã€untilã€periodå’Œtime-invariantå››ç§ä¸åŒçš„æ—¶é—´æœ‰æ•ˆæ€§ç±»å‹ï¼Œå¼¥è¡¥äº†ç°æœ‰è¶…å…³ç³»çŸ¥è¯†å›¾è°±(hyper-relational KGs)åœ¨å¤„ç†å¤æ‚æ—¶é—´çº¦æŸæ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ–¹æ³•ä»æ—¶é—´å€¼(time value)å’Œæ—¶é—´è·¨åº¦(timespan)ä¸¤ä¸ªç»´åº¦æ·±å…¥å­¦ä¹ æ—¶é—´ä¿¡æ¯ï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹å¯¹äº‹å®åŠ¨æ€æ¼”åŒ–çš„ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVITAåœ¨å®ä½“ã€å…³ç³»ã€æ—¶é—´å’Œæ•°å€¼é¢„æµ‹ç­‰å„é¡¹é“¾æ¥é¢„æµ‹(link prediction)ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æœ€é«˜å®ç°äº†75.3%çš„æ€§èƒ½æå‡ã€‚æ¶ˆèç ”ç©¶å’Œæ¡ˆä¾‹åˆ†æè¿›ä¸€æ­¥è¯å®äº†å…¶æ ¸å¿ƒè®¾è®¡åœ¨æå‡æ—¶åºæ¨ç†å‡†ç¡®æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11803v1",
      "published_date": "2025-05-17 03:16:13 UTC",
      "updated_date": "2025-05-17 03:16:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:02:36.062328+00:00"
    },
    {
      "arxiv_id": "2505.11802v1",
      "title": "Diffmv: A Unified Diffusion Framework for Healthcare Predictions with Random Missing Views and View Laziness",
      "title_zh": "Diffmvï¼šé¢å‘éšæœºç¼ºå¤±è§†å›¾ä¸è§†å›¾æ‡’æƒ°çš„ç»Ÿä¸€åŒ»ç–—é¢„æµ‹æ‰©æ•£æ¡†æ¶",
      "authors": [
        "Chuang Zhao",
        "Hui Tang",
        "Hongke Zhao",
        "Xiaomeng Li"
      ],
      "abstract": "Advanced healthcare predictions offer significant improvements in patient outcomes by leveraging predictive analytics. Existing works primarily utilize various views of Electronic Health Record (EHR) data, such as diagnoses, lab tests, or clinical notes, for model training. These methods typically assume the availability of complete EHR views and that the designed model could fully leverage the potential of each view. However, in practice, random missing views and view laziness present two significant challenges that hinder further improvements in multi-view utilization. To address these challenges, we introduce Diffmv, an innovative diffusion-based generative framework designed to advance the exploitation of multiple views of EHR data. Specifically, to address random missing views, we integrate various views of EHR data into a unified diffusion-denoising framework, enriched with diverse contextual conditions to facilitate progressive alignment and view transformation. To mitigate view laziness, we propose a novel reweighting strategy that assesses the relative advantages of each view, promoting a balanced utilization of various data views within the model. Our proposed strategy achieves superior performance across multiple health prediction tasks derived from three popular datasets, including multi-view and multi-modality scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Diffmvï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£ç”Ÿæˆæ¡†æ¶ (diffusion-based generative framework)ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—é¢„æµ‹ä¸­ç”µå­å¥åº·æ¡£æ¡ˆ (EHR) æ•°æ®çš„éšæœºç¼ºå¤±è§†è§’ (random missing views) å’Œè§†è§’æ‡’æƒ° (view laziness) ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ã€‚é’ˆå¯¹éšæœºç¼ºå¤±è§†è§’é—®é¢˜ï¼ŒDiffmv å°†å¤šæ ·åŒ–çš„ EHR æ•°æ®è§†è§’é›†æˆåˆ°ç»Ÿä¸€çš„æ‰©æ•£å»å™ªæ¡†æ¶ä¸­ï¼Œå¹¶åˆ©ç”¨ä¸°å¯Œçš„ä¸Šä¸‹æ–‡æ¡ä»¶ä¿ƒè¿›è§†è§’çš„æ¸è¿›å¯¹é½ä¸è½¬æ¢ã€‚ä¸ºäº†ç¼“è§£è§†è§’æ‡’æƒ°ç°è±¡ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„é‡åŠ æƒç­–ç•¥ (reweighting strategy)ï¼Œé€šè¿‡åŠ¨æ€è¯„ä¼°å„è§†è§’çš„ç›¸å¯¹ä¼˜åŠ¿æ¥ä¿ƒè¿›æ¨¡å‹å¯¹ä¸åŒæ•°æ®è§†è§’çš„å‡è¡¡åˆ©ç”¨ã€‚å®éªŒè¯æ˜ï¼ŒDiffmv åœ¨ä¸‰ä¸ªæµè¡Œæ•°æ®é›†çš„å¤šç§å¥åº·é¢„æµ‹ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ï¼Œä¸”åœ¨å¤šè§†è§’å’Œå¤šæ¨¡æ€åœºæ™¯ä¸‹è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåœ°æå‡äº†å¤æ‚åŒ»ç–—æ•°æ®ç¯å¢ƒä¸‹çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œä¸ºå¤šè§†è§’ EHR æ•°æ®çš„æ·±åº¦æŒ–æ˜æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "SIGKDD2025, accepted",
      "pdf_url": "https://arxiv.org/pdf/2505.11802v1",
      "published_date": "2025-05-17 03:15:55 UTC",
      "updated_date": "2025-05-17 03:15:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:02:57.332245+00:00"
    },
    {
      "arxiv_id": "2505.13514v2",
      "title": "Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models",
      "title_zh": "å½’çº³å¤´æ¯’æ€§ä»æœºç†ä¸Šè§£é‡Šäº†å¤§è¯­è¨€æ¨¡å‹ä¸­çš„é‡å¤è¯…å’’",
      "authors": [
        "Shuxun Wang",
        "Qingyu Yin",
        "Chak Tou Leong",
        "Qiang Zhang",
        "Linyi Yang"
      ],
      "abstract": "Repetition curse is a phenomenon where Large Language Models (LLMs) generate repetitive sequences of tokens or cyclic sequences. While the repetition curse has been widely observed, its underlying mechanisms remain poorly understood. In this work, we investigate the role of induction heads--a specific type of attention head known for their ability to perform in-context learning--in driving this repetitive behavior. Specifically, we focus on the \"toxicity\" of induction heads, which we define as their tendency to dominate the model's output logits during repetition, effectively excluding other attention heads from contributing to the generation process. Our findings have important implications for the design and training of LLMs. By identifying induction heads as a key driver of the repetition curse, we provide a mechanistic explanation for this phenomenon and suggest potential avenues for mitigation. We also propose a technique with attention head regularization that could be employed to reduce the dominance of induction heads during generation, thereby promoting more diverse and coherent outputs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ç”Ÿæˆé‡å¤æˆ–å¾ªç¯åºåˆ—çš„é‡å¤è¯…å’’ï¼ˆrepetition curseï¼‰ç°è±¡ï¼Œå¹¶ä»æœºæ¢°è®ºè§’åº¦æ­ç¤ºäº†è¯±å¯¼å¤´ï¼ˆinduction headsï¼‰åœ¨å…¶ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚ç ”ç©¶å®šä¹‰äº†è¯±å¯¼å¤´çš„â€œæ¯’æ€§â€ï¼ˆtoxicityï¼‰ï¼Œå³åœ¨é‡å¤ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œè¿™äº›å¤´ä¼šä¸»å¯¼è¾“å‡ºå¯¹æ•°å‡ ç‡ï¼ˆlogitsï¼‰ï¼Œæ’æŒ¤å…¶ä»–æ³¨æ„åŠ›å¤´ï¼ˆattention headsï¼‰çš„è´¡çŒ®ã€‚é€šè¿‡è¿™ä¸€å‘ç°ï¼Œä½œè€…ä¸ºé‡å¤è¯…å’’æä¾›äº†æœºæ¢°æ€§è§£é‡Šï¼Œå¹¶æ˜ç¡®äº†è¯±å¯¼å¤´æ˜¯é©±åŠ¨è¯¥è¡Œä¸ºçš„å…³é”®å› ç´ ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ³¨æ„åŠ›å¤´æ­£åˆ™åŒ–ï¼ˆattention head regularizationï¼‰æŠ€æœ¯ï¼Œæ—¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‰Šå¼±è¯±å¯¼å¤´çš„è¿‡åº¦ä¸»å¯¼åœ°ä½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆä¿ƒè¿›æ¨¡å‹ç”Ÿæˆæ›´å¤šæ ·åŒ–ä¸”è¿è´¯çš„å†…å®¹ï¼Œä¸ºç¼“è§£LLMsçš„é‡å¤æ€§é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Need to be refined",
      "pdf_url": "https://arxiv.org/pdf/2505.13514v2",
      "published_date": "2025-05-17 03:09:33 UTC",
      "updated_date": "2025-12-02 09:09:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:02:50.536059+00:00"
    },
    {
      "arxiv_id": "2505.11793v1",
      "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection",
      "title_zh": "CL-CaGANï¼šé¢å‘è·¨åŸŸé«˜å…‰è°±å¼‚å¸¸æ£€æµ‹çš„èƒ¶å›Šå¾®åˆ†å¯¹æŠ—æŒç»­å­¦ä¹ ",
      "authors": [
        "Jianing Wang",
        "Siying Guo",
        "Zheng Hua",
        "Runhu Huang",
        "Jinyu Hu",
        "Maoguo Gong"
      ],
      "abstract": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral image (HSI) processing fields, and most existing deep learning (DL)-based algorithms indicate dramatic potential for detecting anomaly samples through specific training process under current scenario. However, the limited prior information and the catastrophic forgetting problem indicate crucial challenges for existing DL structure in open scenarios cross-domain detection. In order to improve the detection performance, a novel continual learning-based capsule differential generative adversarial network (CL-CaGAN) is proposed to elevate the cross-scenario learning performance for facilitating the real application of DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule structure with adversarial learning network is constructed to estimate the background distribution for surmounting the deficiency of prior information. To mitigate the catastrophic forgetting phenomenon, clustering-based sample replay strategy and a designed extra self-distillation regularization are integrated for merging the history and future knowledge in continual AD task, while the discriminative learning ability from previous detection scenario to current scenario is retained by the elaborately designed structure with continual learning (CL) strategy. In addition, the differentiable enhancement is enforced to augment the generation performance of the training data. This further stabilizes the training process with better convergence and efficiently consolidates the reconstruction ability of background samples. To verify the effectiveness of our proposed CL-CaGAN, we conduct experiments on several real HSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher detection performance and continuous learning capacity for mitigating the catastrophic forgetting under cross-domain scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º CL-CaGAN çš„è¿ç»­å­¦ä¹ èƒ¶å›Šå¾®åˆ†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³é«˜å…‰è°±å¼‚å¸¸æ£€æµ‹ï¼ˆHADï¼‰ä»»åŠ¡åœ¨è·¨é¢†åŸŸåœºæ™¯ä¸‹é¢ä¸´çš„å…ˆéªŒä¿¡æ¯å—é™åŠç¾éš¾æ€§é—å¿˜ï¼ˆcatastrophic forgettingï¼‰ç­‰å…³é”®æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºæ”¹è¿›çš„èƒ¶å›Šç»“æ„ï¼ˆcapsule structureï¼‰ä¸å¯¹æŠ—å­¦ä¹ ç½‘ç»œæ¥å‡†ç¡®ä¼°è®¡èƒŒæ™¯åˆ†å¸ƒï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•å¯¹å…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ã€‚ä¸ºäº†ç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼Œç ”ç©¶é›†æˆäº†åŸºäºèšç±»çš„æ ·æœ¬é‡æ”¾ï¼ˆsample replayï¼‰ç­–ç•¥å’Œä¸“é—¨è®¾è®¡çš„è‡ªè’¸é¦æ­£åˆ™åŒ–ï¼ˆself-distillation regularizationï¼‰ï¼Œå®ç°äº†å†å²ä¸æœªæ¥çŸ¥è¯†çš„æœ‰æ•ˆèåˆã€‚æ­¤å¤–ï¼Œå¾®åˆ†å¢å¼ºï¼ˆdifferentiable enhancementï¼‰æœºåˆ¶çš„å¼•å…¥è¿›ä¸€æ­¥ç¨³å®šäº†è®­ç»ƒè¿‡ç¨‹å¹¶æå‡äº†èƒŒæ™¯æ ·æœ¬çš„é‡å»ºèƒ½åŠ›ã€‚åœ¨å¤šä¸ªçœŸå®é«˜å…‰è°±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCL-CaGAN åœ¨è·¨é¢†åŸŸåœºæ™¯ä¸‹å…·æœ‰æ›´é«˜çš„æ£€æµ‹ç²¾åº¦å’ŒæŒç»­å­¦ä¹ èƒ½åŠ›ï¼Œæ˜¾è‘—å¢å¼ºäº†æ·±åº¦å­¦ä¹ ç»“æ„åœ¨å®é™… HAD åº”ç”¨ä¸­çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11793v1",
      "published_date": "2025-05-17 02:32:41 UTC",
      "updated_date": "2025-05-17 02:32:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:02:56.378251+00:00"
    },
    {
      "arxiv_id": "2505.11792v3",
      "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling",
      "title_zh": "Solver-Informed RLï¼šé¢å‘çœŸå®ä¼˜åŒ–å»ºæ¨¡çš„å¤§è¯­è¨€æ¨¡å‹è½åœ°",
      "authors": [
        "Yitian Chen",
        "Jingfan Xia",
        "Siyu Shao",
        "Dongdong Ge",
        "Yinyu Ye"
      ],
      "abstract": "Optimization modeling is fundamental to decision-making across diverse domains. Despite progress in automating optimization formulation from natural language descriptions, Large Language Models (LLMs) often struggle to generate formally correct and usable models against hallucinations, posing a challenge for reliable automation. Inspired by the success of Reinforcement Learning (RL) in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement Learning (SIRL), a novel framework that significantly improves the authenticity of LLMs for optimization modeling using Reinforcement Learning with Verifiable Reward by leveraging external optimization solvers as verifiers. These verifiers automatically assess the executable code and the instance-level mathematical model represented by the associated LP file, yielding precise and comprehensive feedback signals -- including syntax, feasibility, and solution quality, serving as direct rewards for the RL process. This automated verification process, particularly from classic optimization solvers, also underpins our instance-enhanced self-consistency method to synthesize high-quality training data. Extensive experiments on diverse public benchmarks demonstrate that SIRL achieves state-of-the-art performance, substantially outperforming existing methods in generating accurate and executable optimization models. Our code is publicly available at https://github.com/Cardinal-Operations/SIRL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¼˜åŒ–å»ºæ¨¡(Optimization modeling)ä¸­å®¹æ˜“äº§ç”Ÿå¹»è§‰ã€éš¾ä»¥ç”Ÿæˆå½¢å¼æ­£ç¡®ä¸”å¯ç”¨çš„æ¨¡å‹è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†Solver-Informed Reinforcement Learning (SIRL)æ¡†æ¶ã€‚SIRLé€šè¿‡å°†å¤–éƒ¨ä¼˜åŒ–æ±‚è§£å™¨(Solvers)ä½œä¸ºéªŒè¯å™¨ï¼Œåˆ©ç”¨å…·æœ‰å¯éªŒè¯å¥–åŠ±(Verifiable Reward)çš„å¼ºåŒ–å­¦ä¹ (RL)æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†LLMså»ºæ¨¡çš„çœŸå®æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡è¯„ä¼°å¯æ‰§è¡Œä»£ç å’ŒLPæ–‡ä»¶ï¼Œä»è¯­æ³•ã€å¯è¡Œæ€§åŠè§£çš„è´¨é‡ç­‰æ–¹é¢æå–ç²¾å‡†åé¦ˆä¿¡å·å¹¶å°†å…¶è½¬åŒ–ä¸ºRLè¿‡ç¨‹çš„ç›´æ¥å¥–åŠ±ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨æ±‚è§£å™¨çš„è‡ªåŠ¨éªŒè¯èƒ½åŠ›æ”¯æ’‘äº†ä¸€ç§å®ä¾‹å¢å¼ºçš„è‡ªä¸€è‡´æ€§(Self-consistency)æ–¹æ³•ï¼Œç”¨ä»¥åˆæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSIRLåœ¨å¤šä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†åŸŸé¢†å…ˆ(SOTA)çš„æ€§èƒ½ï¼Œåœ¨ç”Ÿæˆå‡†ç¡®ä¸”å¯æ‰§è¡Œçš„ä¼˜åŒ–æ¨¡å‹æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11792v3",
      "published_date": "2025-05-17 02:32:03 UTC",
      "updated_date": "2025-12-22 09:39:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:03:25.733692+00:00"
    },
    {
      "arxiv_id": "2505.11785v3",
      "title": "Improving Coverage in Combined Prediction Sets with Weighted p-values",
      "title_zh": "åˆ©ç”¨åŠ æƒ p å€¼æå‡ç»„åˆé¢„æµ‹é›†çš„è¦†ç›–ç‡",
      "authors": [
        "Gina Wong",
        "Drew Prinster",
        "Suchi Saria",
        "Rama Chellappa",
        "Anqi Liu"
      ],
      "abstract": "Conformal prediction quantifies the uncertainty of machine learning models by augmenting point predictions with valid prediction sets. For complex scenarios involving multiple trials, models, or data sources, conformal prediction sets can be aggregated to create a prediction set that captures the overall uncertainty, often improving precision. However, aggregating multiple prediction sets with individual $1-Î±$ coverage inevitably weakens the overall guarantee, typically resulting in $1-2Î±$ worst-case coverage. In this work, we propose a framework for the weighted aggregation of prediction sets, where weights are assigned to each prediction set based on their contribution. Our framework offers flexible control over how the sets are aggregated, achieving tighter coverage bounds that interpolate between the $1-2Î±$ guarantee of the combined models and the $1-Î±$ guarantee of an individual model depending on the distribution of weights. Importantly, our framework generalizes to data-dependent weights, as we derive a procedure for weighted aggregation that maintains finite-sample validity even when the weights depend on the data. This extension makes our framework broadly applicable to settings where weights are learned, such as mixture-of-experts (MoE), and we demonstrate through experiments in the MoE setting that our methods achieve adaptive coverage.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¬¦åˆæ€§é¢„æµ‹(Conformal Prediction)åœ¨èšåˆå¤šä¸ªé¢„æµ‹é›†æ—¶å¯¼è‡´çš„è¦†ç›–ç‡ä¿è¯å‡å¼±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŠ æƒpå€¼(Weighted p-values)çš„é¢„æµ‹é›†åŠ æƒèšåˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸ºæ¯ä¸ªé¢„æµ‹é›†åˆ†é…æƒé‡ï¼Œå®ç°äº†å¯¹èšåˆè¿‡ç¨‹çš„çµæ´»æ§åˆ¶ï¼Œå¹¶èƒ½å¤Ÿæ ¹æ®æƒé‡åˆ†å¸ƒåœ¨ $1-2\\alpha$ ä¸ $1-\\alpha$ çš„è¦†ç›–ä¿è¯ä¹‹é—´è·å¾—æ›´ç´§è‡´çš„è¦†ç›–ç•Œé™ã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†è¯¥æ¡†æ¶æ‰©å±•è‡³æ•°æ®ä¾èµ–æƒé‡(Data-dependent weights)ï¼Œæ¨å¯¼å‡ºçš„åŠ æƒèšåˆæµç¨‹å³ä½¿åœ¨æƒé‡ä¾èµ–äºæ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒæœ‰é™æ ·æœ¬æœ‰æ•ˆæ€§(Finite-sample validity)ã€‚è¿™ç§æ‰©å±•ä½¿å¾—è¯¥æ¡†æ¶èƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºæ··åˆä¸“å®¶æ¨¡å‹(Mixture-of-Experts, MoE)ç­‰æƒé‡å¯å­¦ä¹ çš„åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ MoE è®¾ç½®ä¸‹å®ç°äº†è‡ªé€‚åº”è¦†ç›–ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†é¢„æµ‹é›†çš„ç²¾ç¡®åº¦ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11785v3",
      "published_date": "2025-05-17 01:51:28 UTC",
      "updated_date": "2025-12-23 20:21:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:03:42.352203+00:00"
    },
    {
      "arxiv_id": "2506.06291v1",
      "title": "Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks",
      "title_zh": "æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ä»»åŠ¡ä¸­åŸºäºå­¦ä¹ æ¨¡å‹çš„ä¼˜åŒ–æå‡",
      "authors": [
        "Xiaoke Wang",
        "Batuhan Altundas",
        "Zhaoxin Li",
        "Aaron Zhao",
        "Matthew Gombolay"
      ],
      "abstract": "Mixed Integer Linear Programs (MILPs) are essential tools for solving planning and scheduling problems across critical industries such as construction, manufacturing, and logistics. However, their widespread adoption is limited by long computational times, especially in large-scale, real-time scenarios. To address this, we present a learning-based framework that leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph Neural Networks (GNNs), producing high-quality initial solutions for warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling Problems. Experimental results demonstrate that our method reduces optimization time and variance compared to traditional techniques while maintaining solution quality and feasibility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ (Mixed Integer Linear Programs, MILPs) åœ¨å¤„ç†å¤§è§„æ¨¡å®æ—¶è§„åˆ’ä¸è°ƒåº¦é—®é¢˜æ—¶è®¡ç®—æ—¶é—´è¿‡é•¿çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº† Behavior Cloning (BC) å’Œ Reinforcement Learning (RL) æŠ€æœ¯æ¥è®­ç»ƒ Graph Neural Networks (GNNs)ï¼Œæ—¨åœ¨ä¸º Multi-Agent Task Allocation and Scheduling Problems æä¾›é«˜è´¨é‡çš„åˆå§‹è§£ã€‚é€šè¿‡åˆ©ç”¨ GNNs ç”Ÿæˆçš„è§£å¯¹ MILP æ±‚è§£å™¨è¿›è¡Œ warm-starting ä¼˜åŒ–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡æ±‚è§£æ•ˆç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œä¸ä¼ ç»ŸæŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè§£çš„è´¨é‡å’Œå¯è¡Œæ€§çš„å‰æä¸‹ï¼Œä¸ä»…æœ‰æ•ˆç¼©çŸ­äº†ä¼˜åŒ–æ—¶é—´ï¼Œè¿˜é™ä½äº†è®¡ç®—ç»“æœçš„æ–¹å·®ï¼Œä¸ºå¤æ‚å·¥ä¸šåœºæ™¯ä¸‹çš„å®æ—¶å†³ç­–æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.06291v1",
      "published_date": "2025-05-17 01:31:53 UTC",
      "updated_date": "2025-05-17 01:31:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:04:09.645189+00:00"
    },
    {
      "arxiv_id": "2505.11780v1",
      "title": "A Review and Analysis of a Parallel Approach for Decision Tree Learning from Large Data Streams",
      "title_zh": "å¤§è§„æ¨¡æ•°æ®æµå¹¶è¡Œå†³ç­–æ ‘å­¦ä¹ æ–¹æ³•ç»¼è¿°ä¸åˆ†æ",
      "authors": [
        "Zeinab Shiralizadeh"
      ],
      "abstract": "This work studies one of the parallel decision tree learning algorithms, pdsCART, designed for scalable and efficient data analysis. The method incorporates three core capabilities. First, it supports real-time learning from data streams, allowing trees to be constructed incrementally. Second, it enables parallel processing of high-volume streaming data, making it well-suited for large-scale applications. Third, the algorithm integrates seamlessly into the MapReduce framework, ensuring compatibility with distributed computing environments. In what follows, we present the algorithm's key components along with results highlighting its performance and scalability.",
      "tldr_zh": "æœ¬é¡¹ç ”ç©¶å¯¹ pdsCART è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå®ç°å¯æ‰©å±•ä¸”é«˜æ•ˆæ•°æ®åˆ†æè€Œè®¾è®¡çš„å¹¶è¡Œå†³ç­–æ ‘å­¦ä¹ ç®—æ³•ã€‚è¯¥æ–¹æ³•é›†æˆäº†ä¸‰å¤§æ ¸å¿ƒèƒ½åŠ›ï¼Œé¦–å…ˆæ”¯æŒä»æ•°æ®æµä¸­è¿›è¡Œå®æ—¶å­¦ä¹ ï¼Œå…è®¸å¢é‡å¼åœ°æ„å»ºå†³ç­–æ ‘ã€‚å…¶æ¬¡ï¼Œå®ƒå®ç°äº†å¯¹æµ·é‡æµæ•°æ®çš„å¹¶è¡Œå¤„ç†ï¼Œä½¿å…¶èƒ½å¤Ÿèƒœä»»å¤§è§„æ¨¡çš„åº”ç”¨åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•ä¸ MapReduce æ¡†æ¶æ— ç¼é›†æˆï¼Œç¡®ä¿äº†åœ¨åˆ†å¸ƒå¼è®¡ç®—ç¯å¢ƒä¸­çš„é«˜åº¦å…¼å®¹æ€§ã€‚ç ”ç©¶è¯¦ç»†é˜è¿°äº†è¯¥ç®—æ³•çš„å…³é”®ç»„ä»¶ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æµæ—¶è¡¨ç°å‡ºçš„å“è¶Šæ€§èƒ½ä¸æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11780v1",
      "published_date": "2025-05-17 01:07:25 UTC",
      "updated_date": "2025-05-17 01:07:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:04:30.298692+00:00"
    },
    {
      "arxiv_id": "2505.11776v2",
      "title": "Generative and Contrastive Graph Representation Learning",
      "title_zh": "ç”Ÿæˆå¼ä¸å¯¹æ¯”å¼å›¾è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Jiali Chen",
        "Avijit Mukherjee"
      ],
      "abstract": "Self-supervised learning (SSL) on graphs generates node and graph representations (i.e., embeddings) that can be used for downstream tasks such as node classification, node clustering, and link prediction. Graph SSL is particularly useful in scenarios with limited or no labeled data. Existing SSL methods predominantly follow contrastive or generative paradigms, each excelling in different tasks: contrastive methods typically perform well on classification tasks, while generative methods often excel in link prediction. In this paper, we present a novel architecture for graph SSL that integrates the strengths of both approaches. Our framework introduces community-aware node-level contrastive learning, providing more robust and effective positive and negative node pairs generation, alongside graph-level contrastive learning to capture global semantic information. Additionally, we employ a comprehensive augmentation strategy that combines feature masking, node perturbation, and edge perturbation, enabling robust and diverse representation learning. By incorporating these enhancements, our model achieves superior performance across multiple tasks, including node classification, clustering, and link prediction. Evaluations on open benchmark datasets demonstrate that our model outperforms state-of-the-art methods, achieving a performance lift of 0.23%-2.01% depending on the task and dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾è‡ªç›‘ç£å­¦ä¹  (SSL) æå‡ºäº†ä¸€ç§é›†æˆç”Ÿæˆå¼ä¸å¯¹æ¯”å¼å­¦ä¹ ä¼˜åŠ¿çš„æ–°é¢–æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡è¡¨ç°ä¸å‡è¡¡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç¤¾åŒºæ„ŸçŸ¥ (Community-aware) çš„èŠ‚ç‚¹çº§å¯¹æ¯”å­¦ä¹ ä»¥ç”Ÿæˆæ›´é²æ£’çš„æ­£è´Ÿæ ·æœ¬å¯¹ï¼Œå¹¶ç»“åˆå›¾çº§å¯¹æ¯”å­¦ä¹ æ¥æ•è·å…¨å±€è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸€ç§åŒ…å«ç‰¹å¾æ©ç  (Feature Masking)ã€èŠ‚ç‚¹æ‰°åŠ¨å’Œè¾¹æ‰°åŠ¨çš„å…¨é¢æ•°æ®å¢å¼ºç­–ç•¥ï¼Œç¡®ä¿äº†è¡¨ç¤ºå­¦ä¹ çš„é²æ£’æ€§ä¸å¤šæ ·æ€§ã€‚é€šè¿‡æ•´åˆè¿™äº›æ”¹è¿›ï¼Œè¯¥æ¨¡å‹åœ¨èŠ‚ç‚¹åˆ†ç±»ã€èšç±»å’Œé“¾æ¥é¢„æµ‹ (Link Prediction) ç­‰å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­å‡å±•ç°äº†å“è¶Šæ€§èƒ½ã€‚åœ¨å…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜ï¼Œè¯¥æ¨¡å‹ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶åœ¨ä¸åŒä»»åŠ¡å’Œæ•°æ®é›†ä¸Šå®ç°äº†0.23%è‡³2.01%çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.11776v2",
      "published_date": "2025-05-17 01:02:22 UTC",
      "updated_date": "2025-09-24 19:52:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:04:17.808775+00:00"
    },
    {
      "arxiv_id": "2505.11774v1",
      "title": "HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of a Graduate Class",
      "title_zh": "HARDMath2ï¼šä¸€é¡¹ç”±ç ”ç©¶ç”Ÿè¯¾ç¨‹å­¦ç”Ÿæ„å»ºçš„åº”ç”¨æ•°å­¦åŸºå‡†æµ‹è¯•é›†",
      "authors": [
        "James V. Roggeveen",
        "Erik Y. Wang",
        "Will Flintoft",
        "Peter Donets",
        "Lucy S. Nathwani",
        "Nickholas Gutierrez",
        "David Ettel",
        "Anton Marius Graf",
        "Siddharth Dandavate",
        "Arjun Nageswaran",
        "Raglan Ward",
        "Ava Williamson",
        "Anne Mykland",
        "Kacper K. Migacz",
        "Yijun Wang",
        "Egemen Bostan",
        "Duy Thuc Nguyen",
        "Zhe He",
        "Marc L. Descoteaux",
        "Felix Yeung",
        "Shida Liu",
        "Jorge GarcÃ­a Ponce",
        "Luke Zhu",
        "Yuyang Chen",
        "Ekaterina S. Ivshina",
        "Miguel Fernandez",
        "Minjae Kim",
        "Kennan Gumbs",
        "Matthew Scott Tan",
        "Russell Yang",
        "Mai Hoang",
        "David Brown",
        "Isabella A. Silveira",
        "Lavon Sykes",
        "Ahmed Roman",
        "William Fredenberg",
        "Yiming Chen",
        "Lucas Martin",
        "Yixing Tang",
        "Kelly Werker Smith",
        "Hongyu Liao",
        "Logan G. Wilson",
        "Alexander Dazhen Cai",
        "Andrea Elizabeth Biju",
        "Michael P. Brenner"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable progress in mathematical problem-solving, but evaluation has largely focused on problems that have exact analytical solutions or involve formal proofs, often overlooking approximation-based problems ubiquitous in applied science and engineering. To fill this gap, we build on prior work and present HARDMath2, a dataset of 211 original problems covering the core topics in an introductory graduate applied math class, including boundary-layer analysis, WKB methods, asymptotic solutions of nonlinear partial differential equations, and the asymptotics of oscillatory integrals. This dataset was designed and verified by the students and instructors of a core graduate applied mathematics course at Harvard. We build the dataset through a novel collaborative environment that challenges students to write and refine difficult problems consistent with the class syllabus, peer-validate solutions, test different models, and automatically check LLM-generated solutions against their own answers and numerical ground truths. Evaluation results show that leading frontier models still struggle with many of the problems in the dataset, highlighting a gap in the mathematical reasoning skills of current LLMs. Importantly, students identified strategies to create increasingly difficult problems by interacting with the models and exploiting common failure modes. This back-and-forth with the models not only resulted in a richer and more challenging benchmark but also led to qualitative improvements in the students' understanding of the course material, which is increasingly important as we enter an age where state-of-the-art language models can solve many challenging problems across a wide domain of fields.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† HARDMath2ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºåº”ç”¨æ•°å­¦è®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) è¯„æµ‹æ•°æ®é›†ï¼Œæ—¨åœ¨å¡«è¡¥å½“å‰è¯„ä¼°ä¾§é‡äºè§£æè§£è€Œå¿½è§†ç§‘å­¦ä¸å·¥ç¨‹ä¸­è¿‘ä¼¼è®¡ç®— (approximation-based problems) çš„ç©ºç™½ã€‚è¯¥æ•°æ®é›†ç”±å“ˆä½›å¤§å­¦å¸ˆç”Ÿå…±åŒæ„å»ºï¼ŒåŒ…å« 211 ä¸ªåŸåˆ›é¢˜ç›®ï¼Œæ¶µç›–äº† boundary-layer analysisã€WKB methodsã€éçº¿æ€§åå¾®åˆ†æ–¹ç¨‹çš„ asymptotic solutions ä»¥åŠ oscillatory integrals çš„æ¸è¿‘åˆ†æç­‰æ ¸å¿ƒç ”ç©¶ç”Ÿè¯¾ç¨‹ä¸»é¢˜ã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§åˆ›æ–°çš„åä½œå¼€å‘æ¨¡å¼ï¼Œé€šè¿‡å­¦ç”Ÿç¼–å†™é¢˜ç›®ã€åŒè¡ŒéªŒè¯ä»¥åŠåˆ©ç”¨æ•°å€¼çœŸå€¼ (numerical ground truths) è‡ªåŠ¨æ ¡éªŒæ¥ç¡®ä¿è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„å‰æ²¿æ¨¡å‹åœ¨è¿™äº›é—®é¢˜ä¸Šè¡¨ç°ä¸ä½³ï¼Œæš´éœ²äº† LLMs åœ¨åº”ç”¨æ•°å­¦æ¨ç†èƒ½åŠ›ä¸Šçš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œå­¦ç”Ÿé€šè¿‡ä¸æ¨¡å‹äº¤äº’å¹¶åˆ©ç”¨å¤±æ•ˆæ¨¡å¼ (failure modes) æ„æ€å¤æ‚é—®é¢˜çš„è¿‡ç¨‹ï¼Œæ˜¾è‘—åŠ æ·±äº†å¯¹è¯¾ç¨‹çŸ¥è¯†çš„ç†è§£ã€‚HARDMath2 ä¸ºè¯„ä¼°å’Œæå‡æ¨¡å‹åœ¨å¤æ‚ç§‘å­¦é¢†åŸŸçš„æ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦åŸºå‡†ï¼Œå¹¶å±•ç¤ºäº†äººæœºåä½œåœ¨é«˜ç­‰æ•™è‚²ä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11774v1",
      "published_date": "2025-05-17 00:52:49 UTC",
      "updated_date": "2025-05-17 00:52:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:04:07.886847+00:00"
    },
    {
      "arxiv_id": "2505.11771v1",
      "title": "Residual Feature Integration is Sufficient to Prevent Negative Transfer",
      "title_zh": "æ®‹å·®ç‰¹å¾é›†æˆè¶³ä»¥é˜²æ­¢è´Ÿè¿ç§»",
      "authors": [
        "Yichen Xu",
        "Ryumei Nakada",
        "Linjun Zhang",
        "Lexin Li"
      ],
      "abstract": "Transfer learning typically leverages representations learned from a source domain to improve performance on a target task. A common approach is to extract features from a pre-trained model and directly apply them for target prediction. However, this strategy is prone to negative transfer where the source representation fails to align with the target distribution. In this article, we propose Residual Feature Integration (REFINE), a simple yet effective method designed to mitigate negative transfer. Our approach combines a fixed source-side representation with a trainable target-side encoder and fits a shallow neural network on the resulting joint representation, which adapts to the target domain while preserving transferable knowledge from the source domain. Theoretically, we prove that REFINE is sufficient to prevent negative transfer under mild conditions, and derive the generalization bound demonstrating its theoretical benefit. Empirically, we show that REFINE consistently enhances performance across diverse application and data modalities including vision, text, and tabular data, and outperforms numerous alternative solutions. Our method is lightweight, architecture-agnostic, and robust, making it a valuable addition to the existing transfer learning toolbox.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Residual Feature Integration (REFINE)ï¼Œä¸€ç§æ—¨åœ¨è§£å†³ Transfer learning ä¸­ Negative transfer é—®é¢˜çš„é«˜æ•ˆæ–¹æ³•ã€‚REFINE é€šè¿‡å°†å›ºå®šçš„ Source-side representation ä¸å¯è®­ç»ƒçš„ Target-side encoder ç›¸ç»“åˆï¼Œå¹¶åœ¨åˆæˆçš„ Joint representation ä¸Šè®­ç»ƒæµ…å±‚ç¥ç»ç½‘ç»œï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº” Target domain å¹¶ä¿ç•™ Source domain çš„æ ¸å¿ƒçŸ¥è¯†ã€‚åœ¨ç†è®ºå±‚é¢ï¼Œç ”ç©¶è¯æ˜äº† REFINE åœ¨æ¸©å’Œæ¡ä»¶ä¸‹è¶³ä»¥é˜²æ­¢ Negative transferï¼Œå¹¶æ¨å¯¼å‡ºäº†ç›¸åº”çš„ Generalization boundã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰ã€æ–‡æœ¬å’Œè¡¨æ ¼æ•°æ®ç­‰å¤šç§æ¨¡æ€ä¸‹å‡èƒ½ç¨³å®šæå‡æ€§èƒ½ï¼Œå¹¶ä¼˜äºç°æœ‰çš„å¤šç§æ›¿ä»£æ–¹æ¡ˆã€‚ä½œä¸ºä¸€ç§è½»é‡åŒ–ä¸” Architecture-agnostic çš„æ–¹æ³•ï¼ŒREFINE ä¸ºè¿ç§»å­¦ä¹ æä¾›äº†ä¸€ç§é²æ£’ä¸”æå…·å®ç”¨ä»·å€¼çš„å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11771v1",
      "published_date": "2025-05-17 00:36:59 UTC",
      "updated_date": "2025-05-17 00:36:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:04:28.510412+00:00"
    },
    {
      "arxiv_id": "2505.11770v2",
      "title": "Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors",
      "title_zh": "å†…éƒ¨å› æœæœºåˆ¶å¯ç¨³å¥é¢„æµ‹è¯­è¨€æ¨¡å‹çš„åˆ†å¸ƒå¤–è¡Œä¸º",
      "authors": [
        "Jing Huang",
        "Junyi Tao",
        "Thomas Icard",
        "Diyi Yang",
        "Christopher Potts"
      ],
      "abstract": "Interpretability research now offers a variety of techniques for identifying abstract internal mechanisms in neural networks. Can such techniques be used to predict how models will behave on out-of-distribution examples? In this work, we provide a positive answer to this question. Through a diverse set of language modeling tasks--including symbol manipulation, knowledge retrieval, and instruction following--we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model's behavior. Specifically, we propose two methods that leverage causal mechanisms to predict the correctness of model outputs: counterfactual simulation (checking whether key causal variables are realized) and value probing (using the values of those variables to make predictions). Both achieve high AUC-ROC in distribution and outperform methods that rely on causal-agnostic features in out-of-distribution settings, where predicting model behaviors is more crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯è§£é‡Šæ€§ç ”ç©¶(Interpretability research)ä¸­çš„æŠ€æœ¯æ˜¯å¦èƒ½æœ‰æ•ˆé¢„æµ‹è¯­è¨€æ¨¡å‹(Language Model)åœ¨åˆ†å¸ƒå¤–(out-of-distribution)æ ·æœ¬ä¸Šçš„è¡Œä¸ºã€‚ç ”ç©¶è¯æ˜æ¨¡å‹å†…éƒ¨å…·æœ‰ç‹¬ç‰¹å› æœä½œç”¨çš„ç‰¹å¾åœ¨é¢„æµ‹è¾“å‡ºæ­£ç¡®æ€§æ–¹é¢è¡¨ç°æœ€ä¸ºç¨³å¥ï¼Œå¹¶æ®æ­¤æå‡ºäº†åäº‹å®æ¨¡æ‹Ÿ(counterfactual simulation)å’Œå€¼æ¢æµ‹(value probing)ä¸¤ç§åŸºäºå› æœæœºåˆ¶(Causal Mechanisms)çš„é¢„æµ‹æ–¹æ³•ã€‚é€šè¿‡åœ¨ç¬¦å·å¤„ç†ã€çŸ¥è¯†æ£€ç´¢å’ŒæŒ‡ä»¤éµå¾ªç­‰ä»»åŠ¡ä¸Šçš„å®éªŒéªŒè¯ï¼Œè¿™äº›æ–¹æ³•ä¸ä»…åœ¨åˆ†å¸ƒå†…è¾¾åˆ°äº†æé«˜çš„AUC-ROCï¼Œä¸”åœ¨åˆ†å¸ƒå¤–ç¯å¢ƒä¸‹çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¾èµ–éå› æœç‰¹å¾(causal-agnostic features)çš„ä¼ ç»Ÿæ–¹æ³•ã€‚è¯¥å·¥ä½œä¸ºè¯­è¨€æ¨¡å‹çš„å†…éƒ¨å› æœåˆ†ææä¾›äº†å…¨æ–°çš„åº”ç”¨è§†è§’ï¼Œè¯æ˜äº†å…¶åœ¨é¢„æµ‹æ¨¡å‹è¡Œä¸ºåŠæå‡æ¨¡å‹å¯é æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.11770v2",
      "published_date": "2025-05-17 00:31:39 UTC",
      "updated_date": "2025-11-10 22:55:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:05:05.641133+00:00"
    },
    {
      "arxiv_id": "2505.11766v2",
      "title": "Redefining Neural Operators in $d+1$ Dimensions",
      "title_zh": "é‡æ–°å®šä¹‰ $d+1$ ç»´ç¥ç»ç®—å­",
      "authors": [
        "Haoze Song",
        "Zhihao Li",
        "Xiaobo Zhang",
        "Zecheng Gan",
        "Zhilu Lai",
        "Wei Wang"
      ],
      "abstract": "Neural Operators have emerged as powerful tools for learning mappings between function spaces. Among them, the kernel integral operator has been widely validated on universally approximating various operators. Although many advancements following this definition have developed effective modules to better approximate the kernel function defined on the original domain (with $d$ dimensions, $d=1, 2, 3\\dots$), the unclarified evolving mechanism in the embedding spaces blocks researchers' view to design neural operators that can fully capture the target system evolution.\n  Drawing on the SchrÃ¶dingerisation method in quantum simulations of partial differential equations (PDEs), we elucidate the linear evolution mechanism in neural operators. Based on that, we redefine neural operators on a new $d+1$ dimensional domain. Within this framework, we implement a SchrÃ¶dingerised Kernel Neural Operator (SKNO) aligning better with the $d+1$ dimensional evolution. In experiments, the $d+1$ dimensional evolving designs in our SKNO consistently outperform other baselines across ten benchmarks of increasing difficulty, ranging from the simple 1D heat equation to the highly nonlinear 3D Rayleigh-Taylor instability. We also validate the resolution-invariance of SKNO on mixing-resolution training and zero-shot super-resolution tasks. In addition, we show the impact of different lifting and recovering operators on the prediction within the redefined NO framework, reflecting the alignment between our model and the underlying $d+1$ dimensional evolution.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿç¥ç»ç®—å­ï¼ˆNeural Operatorsï¼‰åœ¨åµŒå…¥ç©ºé—´ä¸­æ¼”åŒ–æœºåˆ¶ä¸æ˜ã€éš¾ä»¥å……åˆ†æ•æ‰ç³»ç»Ÿæ¼”åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åœ¨ $d+1$ ç»´ç©ºé—´é‡æ–°å®šä¹‰ç¥ç»ç®—å­çš„æ–°æ¡†æ¶ã€‚å€Ÿé‰´é‡å­æ¨¡æ‹Ÿåå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰ä¸­çš„ SchrÃ¶dingerisation æ–¹æ³•ï¼Œç ”ç©¶å›¢é˜Ÿé˜æ˜äº†ç¥ç»ç®—å­ä¸­çš„çº¿æ€§æ¼”åŒ–æœºåˆ¶ï¼Œå¹¶æ®æ­¤å®ç°äº† SchrÃ¶dingerised Kernel Neural Operator (SKNO)ï¼Œä½¿å…¶èƒ½æ›´å¥½åœ°æ¨¡æ‹Ÿ $d+1$ ç»´çš„ç³»ç»Ÿæ¼”åŒ–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSKNO åœ¨ä»ä¸€ç»´çƒ­æ–¹ç¨‹åˆ°é«˜åº¦éçº¿æ€§çš„ä¸‰ç»´ç‘åˆ©-æ³°å‹’ä¸ç¨³å®šæ€§ï¼ˆRayleigh-Taylor instabilityï¼‰ç­‰åé¡¹æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜éªŒè¯äº† SKNO åœ¨æ··åˆåˆ†è¾¨ç‡è®­ç»ƒå’Œé›¶æ ·æœ¬è¶…åˆ†è¾¨ç‡ï¼ˆzero-shot super-resolutionï¼‰ä»»åŠ¡ä¸­çš„åˆ†è¾¨ç‡ä¸å˜æ€§ï¼Œè¯æ˜äº†æ–°æ¡†æ¶åœ¨é¢„æµ‹åº•å±‚ç‰©ç†æ¼”åŒ–æ–¹é¢çš„ä¼˜è¶Šå¯¹é½æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11766v2",
      "published_date": "2025-05-17 00:15:00 UTC",
      "updated_date": "2025-09-25 13:19:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:04:31.123010+00:00"
    },
    {
      "arxiv_id": "2505.11765v2",
      "title": "OMAC: A Broad Optimization Framework for LLM-Based Multi-Agent Collaboration",
      "title_zh": "OMACï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“åä½œçš„é€šç”¨ä¼˜åŒ–æ¡†æ¶",
      "authors": [
        "Shijun Li",
        "Hilaf Hasson",
        "Joydeep Ghosh"
      ],
      "abstract": "Agents powered by advanced large language models (LLMs) have demonstrated impressive capabilities across diverse complex applications. Recently, Multi-Agent Systems (MAS), wherein multiple agents collaborate and communicate with each other, have exhibited enhanced capabilities in complex tasks, such as high-quality code generation and arithmetic reasoning. However, the development of such systems often relies on handcrafted methods, and the literature on systematic design and optimization of LLM-based MAS remains limited.\n  In this work, we introduce OMAC, a general framework designed for holistic optimization of LLM-based MAS. Specifically, we identify five key optimization dimensions for MAS, encompassing both agent functionality and collaboration structure. Building upon these dimensions, we first propose a general algorithm, utilizing two actors termed the Semantic Initializer and the Contrastive Comparator, to optimize any single dimension. Then, we present an algorithm for joint optimization across multiple dimensions. Extensive experiments demonstrate the superior performance of OMAC on code generation, arithmetic reasoning, and general reasoning tasks against state-of-the-art approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å¼€å‘è¿‡åº¦ä¾èµ–æ‰‹å·¥è®¾è®¡ä¸”ç¼ºä¹ç³»ç»Ÿæ€§ä¼˜åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º OMAC çš„å…¨é¢ä¼˜åŒ–æ¡†æ¶ã€‚OMAC ç¡®å®šäº†å¤šæ™ºèƒ½ä½“åä½œä¸­æ¶µç›–æ™ºèƒ½ä½“åŠŸèƒ½ä¸åä½œç»“æ„çš„äº”ä¸ªå…³é”®ä¼˜åŒ–ç»´åº¦ï¼Œä¸ºç³»ç»ŸåŒ–è®¾è®¡æä¾›äº†ç†è®ºåŸºç¡€ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥è¯­ä¹‰åˆå§‹åŒ–å™¨ï¼ˆSemantic Initializerï¼‰å’Œå¯¹æ¯”æ¯”è¾ƒå™¨ï¼ˆContrastive Comparatorï¼‰ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå®ç°äº†å¯¹ä»»æ„å•ä¸€ç»´åº¦çš„è‡ªåŠ¨åŒ–ä¼˜åŒ–ï¼Œå¹¶è¿›ä¸€æ­¥æå‡ºäº†è·¨å¤šç»´åº¦çš„è”åˆä¼˜åŒ–ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOMAC åœ¨ä»£ç ç”Ÿæˆï¼ˆcode generationï¼‰ã€ç®—æœ¯æ¨ç†ï¼ˆarithmetic reasoningï¼‰å’Œé€šç”¨æ¨ç†ï¼ˆgeneral reasoningï¼‰ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿™ä¸€å·¥ä½œä¸º LLM-based MAS çš„å…¨å±€ä¼˜åŒ–æä¾›äº†é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†å¤šæ™ºèƒ½ä½“ååŒå·¥ä½œçš„æ•ˆç‡ä¸æ€§èƒ½ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11765v2",
      "published_date": "2025-05-17 00:13:46 UTC",
      "updated_date": "2025-05-21 21:38:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:05:50.087984+00:00"
    },
    {
      "arxiv_id": "2505.11764v3",
      "title": "Towards Universal Semantics With Large Language Models",
      "title_zh": "è¿ˆå‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é€šç”¨è¯­ä¹‰",
      "authors": [
        "Raymond Baartmans",
        "Matthew Raffel",
        "Rahul Vikram",
        "Aiden Deringer",
        "Lizhong Chen"
      ],
      "abstract": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a universal set of semantic primes: simple, primitive word-meanings that have been shown to exist in most, if not all, languages of the world. According to this framework, any word, regardless of complexity, can be paraphrased using these primes, revealing a clear and universally translatable meaning. These paraphrases, known as explications, can offer valuable applications for many natural language processing (NLP) tasks, but producing them has traditionally been a slow, manual process. In this work, we present the first study of using large language models (LLMs) to generate NSM explications. We introduce automatic evaluation methods, a tailored dataset for training and evaluation, and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in producing accurate, cross-translatable explications, marking a significant step toward universal semantic representation with LLMs and opening up new possibilities for applications in semantic analysis, translation, and beyond. Our code is available at https://github.com/OSU-STARLAB/DeepNSM.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºNatural Semantic Metalanguage (NSM)ç†è®ºçš„é€šç”¨è¯­ä¹‰è¡¨ç¤ºï¼Œæ—¨åœ¨åˆ©ç”¨Large Language Models (LLMs)è‡ªåŠ¨åŒ–ç”Ÿæˆç”±è¯­ä¹‰åŸè¯­(semantic primes)æ„æˆçš„è¯­ä¹‰é‡Šä¹‰(explications)ã€‚é’ˆå¯¹ä¼ ç»Ÿäººå·¥ç¼–å†™é‡Šä¹‰è¿‡ç¨‹ç¼“æ…¢çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†é¦–ä¸ªåˆ©ç”¨LLMsç”ŸæˆNSMé‡Šä¹‰çš„ç ”ç©¶æ–¹æ¡ˆï¼Œå¹¶å¼•å…¥äº†ä¸“é—¨çš„è®­ç»ƒæ•°æ®é›†ä¸è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„1Bå’Œ8Bæ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®ä¸”å…·å¤‡è·¨è¯­è¨€ç¿»è¯‘èƒ½åŠ›çš„é‡Šä¹‰æ–¹é¢è¡¨ç°ä¼˜äºGPT-4oã€‚è¯¥å·¥ä½œæ ‡å¿—ç€åœ¨å®ç°é€šç”¨è¯­ä¹‰è¡¨ç¤ºæ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ï¼Œä¸ºè¯­ä¹‰åˆ†æå’Œæœºå™¨ç¿»è¯‘ç­‰è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡å¼€è¾Ÿäº†æ–°çš„åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.11764v3",
      "published_date": "2025-05-17 00:11:58 UTC",
      "updated_date": "2025-07-03 22:02:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T10:05:05.894170+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 119,
  "processed_papers_count": 119,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T10:08:46.760715+00:00"
}
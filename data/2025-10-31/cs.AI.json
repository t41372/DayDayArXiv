{
  "date": "2025-10-31",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-10-31 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv åˆ—è¡¨å¯è°“æ˜¯â€œç¡¬æ ¸æ¶æ„â€ä¸â€œAgent è¿›åŒ–â€çš„ç››å®´ã€‚**ç¾å›¢æŠ€æœ¯å›¢é˜Ÿ**å‘å¸ƒäº† 560B å‚æ•°çš„ **LongCat-Flash-Omni** å¤šæ¨¡æ€æ¨¡å‹æŠ€æœ¯æŠ¥å‘Šï¼›**Quanquan Gu** ç­‰äººæå‡ºäº†**é«˜é˜¶çº¿æ€§æ³¨æ„åŠ›ï¼ˆHigher-order Linear Attentionï¼‰**è¯•å›¾åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­æŒ‘æˆ˜ä¼ ç»Ÿ Attentionï¼›è€Œåœ¨ç”Ÿæˆæœºåˆ¶ä¸Šï¼Œ**Continuous Autoregressive Language Models (CALM)** æå‡ºäº†ä¸€ç§æ¿€è¿›çš„â€œè¿ç»­å‘é‡é¢„æµ‹â€èŒƒå¼ï¼Œè¯•å›¾æ‰“ç ´ Token-by-Token çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œå…³äº **Agent çš„ä¸€è‡´æ€§ï¼ˆSergey Levine å›¢é˜Ÿï¼‰**ã€**æ¶ˆé™¤å¥‰æ‰¿è¡Œä¸ºï¼ˆSycophancyï¼‰**ä»¥åŠ**æœºå™¨äººç»“åˆ CoT æ¨ç†**çš„ç ”ç©¶ä¹Ÿé¢‡å…·çœ‹ç‚¹ã€‚\n\n---\n\n### ğŸš€ å¤§æ¨¡å‹æ¶æ„ä¸é«˜æ•ˆæ¨ç† (Architecture & Efficiency)\n\n**1. Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits**\n**(é¢å‘ 1M Token LLM æ¨ç†çš„å¯æ‰©å±•è¿‘å­˜è®¡ç®—ï¼šè¶…è¶Š GPU é™åˆ¶çš„ CXLKV ç¼“å­˜ç®¡ç†)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹ç™¾ä¸‡çº§ Token ä¸Šä¸‹æ–‡å¸¦æ¥çš„ KV-Cache æ˜¾å­˜ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§åŸºäº CXLï¼ˆCompute Express Linkï¼‰çš„è¿‘å­˜è®¡ç®—ï¼ˆPNMï¼‰æ–¹æ¡ˆã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** ä½œè€…è®¾è®¡äº†ä¸€ä¸ª PNM åŠ é€Ÿå™¨æ¥å¸è½½ Token é¡µé¢é€‰æ‹©ä»»åŠ¡ï¼Œæ¶ˆé™¤äº†æ˜‚è´µçš„æ•°æ®å¬å›ï¼ˆRecallï¼‰å¼€é”€ã€‚é€šè¿‡æ··åˆå¹¶è¡Œç­–ç•¥ï¼Œè¯¥æ–¹æ¡ˆåœ¨ 405B å‚æ•°æ¨¡å‹å’Œ 1M ä¸Šä¸‹æ–‡ä¸‹ï¼Œå®ç°äº†é«˜è¾¾ **21.9 å€çš„ååé‡æå‡**å’Œ **60 å€çš„èƒ½æ•ˆä¼˜åŒ–**ã€‚è¿™æ˜¯æ‰“ç ´ GPU æ˜¾å­˜å¢™çš„é‡è¦ç¡¬ä»¶-è½¯ä»¶ååŒè®¾è®¡ã€‚\n\n**29. Continuous Autoregressive Language Models**\n**(è¿ç»­è‡ªå›å½’è¯­è¨€æ¨¡å‹)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** **è¿™ç¯‡éå¸¸æœ‰æ„æ€**ã€‚ä½œè€…è®¤ä¸º LLM çš„æ•ˆç‡ç“¶é¢ˆåœ¨äºç¦»æ•£çš„ Token é€ä¸ªç”Ÿæˆã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** æå‡ºäº† CALMï¼Œä»â€œä¸‹ä¸€ä¸ª Token é¢„æµ‹â€è½¬å˜ä¸ºâ€œä¸‹ä¸€ä¸ªè¿ç»­å‘é‡é¢„æµ‹â€ã€‚å®ƒå°† K ä¸ª Token å‹ç¼©ä¸ºä¸€ä¸ªè¿ç»­å‘é‡è¿›è¡Œé¢„æµ‹ï¼Œè§£ç æ—¶å†è¿˜åŸã€‚è¿™å®é™…ä¸Šå¢åŠ äº†æ¯æ­¥ç”Ÿæˆçš„â€œè¯­ä¹‰å¸¦å®½â€ï¼Œåœ¨å‡å°‘ç”Ÿæˆæ­¥æ•°çš„åŒæ—¶ä¿æŒäº†æ€§èƒ½ï¼Œä¸ºè¶…é«˜æ•ˆè¯­è¨€æ¨¡å‹æä¾›äº†ä¸€æ¡æ–°è·¯å¾„ã€‚\n\n**101. Higher-order Linear Attention**\n**(é«˜é˜¶çº¿æ€§æ³¨æ„åŠ›)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æ¥è‡ª **Quanquan Gu** å›¢é˜Ÿã€‚é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡ï¼Œæå‡ºäº†ä¸€ç§åä¸º HLA çš„å› æœæµå¼æœºåˆ¶ã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** HLA åœ¨ä¿æŒçº¿æ€§æ—¶é—´å¤æ‚åº¦çš„åŒæ—¶ï¼Œé€šè¿‡ç´§å‡‘çš„å‰ç¼€ç»Ÿè®¡é‡å®ç°äº†é«˜é˜¶äº¤äº’ï¼ˆHigher-order interactionsï¼‰ï¼Œå…‹æœäº†ä¼ ç»Ÿçº¿æ€§æ³¨æ„åŠ›è¡¨è¾¾èƒ½åŠ›å—é™çš„é—®é¢˜ã€‚äºŒé˜¶ HLA æ— éœ€å®ä¾‹åŒ– $n \\times n$ çŸ©é˜µå³å¯å®ç°ç±»ä¼¼ Attention çš„æ•°æ®ä¾èµ–æ··åˆï¼Œæ˜¯çº¿æ€§ Attention å®¶æ—çš„ä¸€ä¸ªæœ‰åŠ›æ‰©å……ã€‚\n\n**5. LongCat-Flash-Omni Technical Report**\n**(LongCat-Flash-Omni æŠ€æœ¯æŠ¥å‘Š)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** **ç¾å›¢ LongCat å›¢é˜Ÿ**å‘å¸ƒäº†æ‹¥æœ‰ **5600 äº¿å‚æ•°**ï¼ˆæ¿€æ´» 27Bï¼‰çš„å¼€æºå…¨æ¨¡æ€æ¨¡å‹ã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** é‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä»ç®€å•åˆ°å¤æ‚çš„æ¨¡æ€åºåˆ—å»ºæ¨¡è¿‡æ¸¡ã€‚åŸºäº Shortcut-connected MoE æ¶æ„ï¼Œå®ç°äº†ä½å»¶è¿Ÿçš„å®æ—¶éŸ³è§†é¢‘äº¤äº’ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜å¼€å‘äº†ä¸€å¥—æ¨¡æ€è§£è€¦çš„å¹¶è¡Œè®­ç»ƒæ–¹æ¡ˆï¼Œåœ¨å¤„ç†å¼‚æ„æ•°æ®æ—¶èƒ½ä¿æŒ 90% ä»¥ä¸Šçš„æ–‡æœ¬è®­ç»ƒååé‡ã€‚\n\n---\n\n### ğŸ¤– Agent, Reasoning & Alignment (æ™ºèƒ½ä½“ä¸æ¨ç†)\n\n**14. Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning**\n**(é€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ ä¸€è‡´åœ°æ¨¡æ‹Ÿäººç±»è§’è‰²)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** **Sergey Levine, Natasha Jaques** ç­‰å¤§ä½¬çš„ä½œå“ã€‚è§£å†³ LLM åœ¨æ‰®æ¼”è§’è‰²ï¼ˆå¦‚æ²»ç–—å¸ˆã€å­¦ç”Ÿï¼‰æ—¶å®¹æ˜“â€œå‡ºæˆâ€æˆ–å‰åçŸ›ç›¾çš„é—®é¢˜ã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** å®šä¹‰äº†ä¸‰ä¸ªè‡ªåŠ¨æŒ‡æ ‡ï¼ˆPrompt-to-line, Line-to-line, Q&A ä¸€è‡´æ€§ï¼‰å¹¶å°†å…¶ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡å¤šè½® RL å¾®è°ƒæ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºä¸ä¸€è‡´æ€§å‡å°‘äº† 55% ä»¥ä¸Šï¼Œç”Ÿæˆçš„æ¨¡æ‹Ÿç”¨æˆ·æ›´åŠ è¿è´¯å’Œå¿ å®ã€‚\n\n**146. Consistency Training Helps Stop Sycophancy and Jailbreaks**\n**(ä¸€è‡´æ€§è®­ç»ƒæœ‰åŠ©äºé˜»æ­¢å¥‰æ‰¿å’Œè¶Šç‹±)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** **Rohin Shah** ç­‰äººå‚ä¸ã€‚é’ˆå¯¹ LLM å®¹æ˜“é¡ºä»ç”¨æˆ·é”™è¯¯è§‚ç‚¹ï¼ˆSycophancyï¼‰æˆ–è¢«â€œè¶Šç‹±â€çš„é—®é¢˜ï¼Œæå‡º**ä¸€è‡´æ€§è®­ç»ƒï¼ˆConsistency Trainingï¼‰**ã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** æ ¸å¿ƒæ€æƒ³æ˜¯æ•™æ¨¡å‹å¯¹ Prompt ä¸­çš„æ— å…³çº¿ç´¢ï¼ˆå¦‚è¯±å¯¼æ€§æé—®ã€è¶Šç‹±æ–‡æœ¬ï¼‰ä¿æŒâ€œä¸å˜æ€§â€ï¼Œè€Œä¸æ˜¯æ•™å®ƒå…·ä½“çš„æ­£ç¡®ç­”æ¡ˆã€‚é€šè¿‡åœ¨æ¨¡å‹å†…éƒ¨æ¿€æ´»ï¼ˆACTï¼‰æˆ–è¾“å‡ºï¼ˆBCTï¼‰å±‚é¢å¼ºåˆ¶ä¸€è‡´æ€§ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹çš„è„†å¼±æ€§ã€‚ä½œè€…è®¤ä¸ºè®¸å¤šå¯¹é½é—®é¢˜æœ¬è´¨ä¸Šæ˜¯**ä¸€è‡´æ€§é—®é¢˜**è€Œéå•çº¯çš„æ­£ç¡®æ€§é—®é¢˜ã€‚\n\n**119. DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models**\n**(DeepThinkVLAï¼šå¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ¨ç†èƒ½åŠ›)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è®©æœºå™¨äººâ€œä¸‰æ€è€Œåè¡Œâ€ã€‚è§£å†³ç°æœ‰ VLA æ¨¡å‹éš¾ä»¥å…¼é¡¾ CoT æ¨ç†å’Œé«˜ç»´åŠ¨ä½œè¾“å‡ºçš„æ¶æ„å†²çªã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** æå‡ºæ··åˆæ³¨æ„åŠ›è§£ç å™¨ï¼Œå…ˆç”¨å› æœæ³¨æ„åŠ›ç”Ÿæˆ CoT æ¨ç†é“¾ï¼Œå†åˆ‡æ¢åˆ°åŒå‘æ³¨æ„åŠ›å¹¶è¡Œè§£ç åŠ¨ä½œå‘é‡ã€‚é…åˆ SFT å’Œ RL ä¸¤é˜¶æ®µè®­ç»ƒï¼Œåœ¨ LIBERO åŸºå‡†ä¸Šè¾¾åˆ°äº† 97.0% çš„æˆåŠŸç‡ã€‚\n\n**75. DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains**\n**(DeepCompressï¼šåŠ¨æ€æ¢ç´¢å’Œå‹ç¼©æ¨ç†é“¾çš„åŒé‡å¥–åŠ±ç­–ç•¥)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è§£å†³å¤§æ¨¡å‹åœ¨ç®€å•é—®é¢˜ä¸Šâ€œè¿‡åº¦æ€è€ƒâ€å’Œå¤æ‚é—®é¢˜ä¸Šâ€œæ€è€ƒä¸è¶³â€çš„é—®é¢˜ã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** å¼•å…¥è‡ªé€‚åº”é•¿åº¦å¥–åŠ±æœºåˆ¶ï¼Œå®æ—¶å°†é—®é¢˜åˆ†ç±»ä¸ºâ€œç®€å•â€æˆ–â€œå›°éš¾â€ã€‚å¯¹ç®€å•é—®é¢˜é¼“åŠ±çŸ­ CoTï¼Œå¯¹å›°éš¾é—®é¢˜é¼“åŠ±é•¿ CoTã€‚è¿™æ˜¯ä¸€ç§åŠ¨æ€çš„ Inference-time compute ä¼˜åŒ–ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸è§†è§‰ (Multimodal & Vision)\n\n**37. Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation**\n**(è‰å›¾åˆ°å¸ƒå±€ï¼šè‰å›¾å¼•å¯¼çš„å¤šæ¨¡æ€å¸ƒå±€ç”Ÿæˆ)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** **Google DeepMind** å‡ºå“ã€‚è§£å†³å¹³é¢è®¾è®¡ä¸­å¸ƒå±€ç”Ÿæˆéš¾ä»¥æ§åˆ¶çš„é—®é¢˜ï¼Œå¼•å…¥æ‰‹ç»˜è‰å›¾ä½œä¸ºç›´è§‚çº¦æŸã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€ Transformer è§£å†³æ–¹æ¡ˆï¼Œå¹¶è§£å†³è‰å›¾æ•°æ®ç¨€ç¼ºé—®é¢˜â€”â€”å¼€å‘äº†ä¸€ç§å¤§è§„æ¨¡åˆæˆè‰å›¾çš„æ–¹æ³•ã€‚å‘å¸ƒäº† 200k åˆæˆè‰å›¾æ•°æ®é›†ï¼Œæ¨¡å‹åœ¨ PubLayNet ç­‰åŸºå‡†ä¸Šè¶…è¶Šäº† SOTAã€‚\n\n**30. PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting**\n**(PETARï¼šç”¨äº PET è‡ªåŠ¨æŠ¥å‘Šçš„æ©è†œæ„ŸçŸ¥è§†è§‰è¯­è¨€æ¨¡å‹ä¸å®šä½å‘ç°ç”Ÿæˆ)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** åŒ»å­¦å½±åƒé¢†åŸŸçš„ VLMã€‚å‘å¸ƒäº†é¦–ä¸ªå¤§è§„æ¨¡ PET/CT ç—…ç¶çº§æè¿°æ•°æ®é›† PETARSeg-11Kã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** æå‡ºäº† PETAR-4B æ¨¡å‹ï¼Œåˆ©ç”¨ 3D ç„¦ç‚¹ Prompt æ•æ‰ä»…å ä½“ç§¯ 0.1% çš„å¾®å°ç—…ç¶ç»†èŠ‚ã€‚è¿™æ˜¯é¦–ä¸ªç»è¿‡åŒ»ç”Ÿå‚ä¸è¯„ä¼°çš„ PET è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆå·¥ä½œï¼Œä¸´åºŠå®ç”¨æ€§è¾ƒå¼ºã€‚\n\n---\n\n### ğŸ§ª ç§‘å­¦ç ”ç©¶ä¸æ‰¹åˆ¤æ€§åˆ†æ (Science & Critique)\n\n**85. An In-depth Study of LLM Contributions to the Bin Packing Problem**\n**(æ·±å…¥ç ”ç©¶ LLM å¯¹è£…ç®±é—®é¢˜çš„è´¡çŒ®)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** **æ‰¹åˆ¤æ€§æ–‡ç« **ã€‚æœ€è¿‘æœ‰ç ”ç©¶ç§° LLM å‘ç°äº†è£…ç®±é—®é¢˜ï¼ˆBin Packingï¼‰çš„æ–°å¯å‘å¼ç®—æ³•ï¼Œæœ¬æ–‡å¯¹æ­¤è¿›è¡Œäº†é‡æ–°è¯„ä¼°ã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** ä½œè€…å‘ç° LLM ç”Ÿæˆçš„å¯å‘å¼ç®—æ³•è™½ç„¶äººç±»å¯è¯»ï¼Œä½†å³ä½¿æ˜¯ä¸“å®¶ä¹Ÿéš¾ä»¥ç†è§£å…¶é€»è¾‘ã€‚ä½œè€…é€šè¿‡æ›´ç®€å•çš„ç®—æ³•è¯æ˜ï¼Œæ‰€è°“çš„â€œæ–°å‘ç°â€æ›´å¤šæ˜¯å› ä¸ºæµ‹è¯•çš„å®ä¾‹æœ¬èº«è¾ƒç®€å•ï¼Œä¸”å¹¶æœªçœŸæ­£è¢«æ·±å…¥ç ”ç©¶è¿‡ã€‚**Implication:** æé†’å­¦æœ¯ç•Œåœ¨è¯„ä¼° LLM çš„â€œç§‘å­¦å‘ç°â€æ—¶è¦ä¿æŒä¸¥è°¨ï¼Œé¿å…è¿‡åº¦ç‚’ä½œã€‚\n\n**138. ConnectomeBench: Can LLMs Proofread the Connectome?**\n**(ConnectomeBenchï¼šLLM èƒ½æ ¡å¯¹è¿æ¥ç»„å—ï¼Ÿ)**\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æ¢ç´¢ AI Agent æ˜¯å¦èƒ½è¾…åŠ©è„‘ç§‘å­¦ä¸­çš„â€œè¿æ¥ç»„å­¦â€æ•°æ®æ ¡å¯¹ï¼ˆè¿™æ˜¯ä¸€é¡¹æå…¶è€—æ—¶çš„äººåŠ›å·¥ä½œï¼‰ã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** å»ºç«‹äº† ConnectomeBench åŸºå‡†ã€‚æµ‹è¯•å‘ç°ï¼ŒClaude 3.7/4 Sonnet ç­‰æ¨¡å‹åœ¨è¯†åˆ«ç¥ç»å…ƒç‰‡æ®µç±»å‹ä¸Šè¡¨ç°å°šå¯ï¼ˆ52-82%ï¼‰ï¼Œä½†åœ¨è¯†åˆ«åˆå¹¶é”™è¯¯ï¼ˆmerge errorï¼‰ä¸Šä»ç„¶æŒ£æ‰ã€‚LLM æœ‰æ½œåŠ›è¾…åŠ©ï¼Œä½†ç›®å‰è¿˜ä¸èƒ½æ›¿ä»£ä¸“å®¶ã€‚\n\n---\n\n### ğŸ› ï¸ å…¶ä»–æœ‰è¶£çš„å·¥ä½œ (Others)\n\n*   **126. H2-Cache:** é’ˆå¯¹æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰ï¼Œæå‡ºäº†ä¸€ç§åˆ†å±‚åŒé˜¶æ®µç¼“å­˜æœºåˆ¶ï¼Œåœ¨ä¿æŒç”»è´¨çš„åŒæ—¶åŠ é€Ÿæ¨ç†é«˜è¾¾ 5 å€ã€‚\n*   **56. TetraJet-v2:** é’ˆå¯¹ LLM çš„ 4-bit å…¨é‡åŒ–è®­ç»ƒï¼ˆNVFP4ï¼‰ï¼Œè§£å†³äº†ä½ç²¾åº¦è®­ç»ƒä¸­çš„æƒé‡éœ‡è¡å’Œå¼‚å¸¸å€¼é—®é¢˜ã€‚\n*   **88. Dynamic Model Selection for Trajectory Prediction:** åœ¨è‡ªåŠ¨é©¾é©¶è½¨è¿¹é¢„æµ‹ä¸­ï¼Œä¸è¦è¯•å›¾ç”¨ä¸€ä¸ªæ¨¡å‹è§£å†³æ‰€æœ‰é—®é¢˜ã€‚åˆ©ç”¨â€œå…ƒç‰¹å¾â€ï¼ˆä¸ç¡®å®šæ€§ã€ç¨³å®šæ€§ï¼‰åŠ¨æ€é€‰æ‹©ç‰©ç†æ¨¡å‹ã€Transformer æˆ– GameFormerï¼Œæ•ˆæœæ›´å¥½ã€‚\n\nç¥å¤§å®¶ç§‘ç ”é¡ºåˆ©ï¼ä¸‹æœŸè§ã€‚",
  "papers": [
    {
      "arxiv_id": "2511.00321v1",
      "title": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits",
      "title_zh": "é¢å‘ç™¾ä¸‡çº§ Token å¤§æ¨¡å‹æ¨ç†çš„å¯æ‰©å±•è¿‘å­˜è®¡ç®—ï¼šçªç ´ GPU é™åˆ¶çš„ CXL KV ç¼“å­˜ç®¡ç†",
      "authors": [
        "Dowon Kim",
        "MinJae Lee",
        "Janghyeon Kim",
        "HyuckSung Kwon",
        "Hyeonggyu Jeong",
        "Sang-Soo Park",
        "Minyong Yoon",
        "Si-Dong Roh",
        "Yongsuk Kwon",
        "Jinin So",
        "Jungwook Choi"
      ],
      "abstract": "The expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to scalable external memory, these frameworks still suffer from costly data transfers when recalling non-resident KV tokens to limited GPU memory as context lengths increase. This work proposes scalable Processing-Near-Memory (PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that coordinates memory and computation beyond GPU limits. Our design offloads token page selection to a PNM accelerator within CXL memory, eliminating costly recalls and enabling larger GPU batch sizes. We further introduce a hybrid parallelization strategy and a steady-token selection mechanism to enhance compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM system, our solution delivers consistent performance gains for LLMs with up to 405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV) and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x better total cost efficiency than the baseline, demonstrating that CXL-enabled multi-PNM architectures can serve as a scalable backbone for future long-context LLM inference.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ‰©å±•ä¸Šä¸‹æ–‡è‡³ç™¾ä¸‡çº§Tokenæ—¶é¢ä¸´çš„å­˜å‚¨ä¸è®¡ç®—ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯KV-cacheç®¡ç†çš„éš¾é¢˜ï¼Œæå‡ºäº†å¯æ‰©å±•çš„Processing-Near-Memory (PNM)æ¨ç†æ–¹æ¡ˆã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨CXLæŠ€æœ¯ï¼Œå°†token page selectionä»»åŠ¡ç›´æ¥å¸è½½åˆ°CXLå­˜å‚¨å†…çš„PNMåŠ é€Ÿå™¨ä¸­ï¼Œä»è€Œæ¶ˆé™¤äº†å°†éé©»ç•™KV tokenså¬å›è‡³æœ‰é™GPUæ˜¾å­˜çš„é«˜æ˜‚å¼€é”€ï¼Œå¹¶æ”¯æŒæ›´å¤§çš„æ‰¹å¤„ç†è§„æ¨¡ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†æ··åˆå¹¶è¡Œç­–ç•¥å’Œsteady-token selectionæœºåˆ¶ï¼Œä»¥è¿›ä¸€æ­¥æå‡å¤§è§„æ¨¡æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤„ç†å…·æœ‰405Bå‚æ•°å’Œ1M-tokenä¸Šä¸‹æ–‡çš„æ¨¡å‹æ—¶ï¼Œç›¸æ¯”åŸºçº¿å®ç°äº†æœ€é«˜21.9å€çš„ååé‡æå‡ã€60å€çš„èƒ½è€—é™ä½ä»¥åŠ7.3å€çš„æˆæœ¬æ•ˆç›Šä¼˜åŒ–ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†æ”¯æŒCXLçš„å¤šPNMæ¶æ„å¯ä»¥ä½œä¸ºæœªæ¥é•¿ä¸Šä¸‹æ–‡LLMæ¨ç†çš„é«˜æ•ˆã€å¯æ‰©å±•éª¨å¹²æŠ€æœ¯ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00321v1",
      "published_date": "2025-10-31 23:50:44 UTC",
      "updated_date": "2025-10-31 23:50:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:51:53.090775+00:00"
    },
    {
      "arxiv_id": "2511.00318v1",
      "title": "A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data",
      "title_zh": "èåˆ LLM æ··åˆåˆæˆæ•°æ®çš„å› æœæ¨æ–­æŠ€æœ¯æ¢ç´¢",
      "authors": [
        "Dana Kim",
        "Yichen Xu",
        "Tiffany Lin"
      ],
      "abstract": "Large Language Models (LLMs) offer a flexible means to generate synthetic tabular data, yet existing approaches often fail to preserve key causal parameters such as the average treatment effect (ATE). In this technical exploration, we first demonstrate that state-of-the-art synthetic data generators, both GAN- and LLM-based, can achieve high predictive fidelity while substantially misestimating causal effects. To address this gap, we propose a hybrid generation framework that combines model-based covariate synthesis (monitored via distance-to-closest-record filtering) with separately learned propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain their underlying causal structure. We further introduce a synthetic pairing strategy to mitigate positivity violations and a realistic evaluation protocol that leverages unlimited synthetic samples to benchmark traditional estimators (IPTW, AIPW, substitution) under complex covariate distributions. This work lays the groundwork for LLM-powered data pipelines that support robust causal analysis. Our code is available at https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰GANå’ŒLLMåˆæˆæ•°æ®ç”Ÿæˆå™¨åœ¨ä¿æŒå¹³å‡æ²»ç–—æ•ˆåº”(Average Treatment Effect, ATE)ç­‰å› æœå‚æ•°æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ··åˆç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†åŸºäºæ¨¡å‹çš„åå˜é‡åˆæˆä¸ç‹¬ç«‹å­¦ä¹ çš„å€¾å‘æ¨¡å‹åŠç»“æœæ¨¡å‹ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡è·ç¦»æœ€è¿‘è®°å½•è¿‡æ»¤(distance-to-closest-record filtering)ç¡®ä¿ç”Ÿæˆçš„(W, A, Y)ä¸‰å…ƒç»„ä¿ç•™åº•å±‚çš„å› æœç»“æ„ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†åˆæˆé…å¯¹ç­–ç•¥(synthetic pairing strategy)ä»¥ç¼“è§£æ­£å€¼æ€§(positivity)è¿èƒŒé—®é¢˜ï¼Œå¹¶å»ºç«‹äº†ä¸€å¥—åˆ©ç”¨åˆæˆæ ·æœ¬åŸºå‡†æµ‹è¯•IPTWã€AIPWå’Œæ›¿ä»£ä¼°ç®—å™¨çš„è¯„ä¼°åè®®ã€‚è¯¥å·¥ä½œè¯æ˜äº†æ··åˆæ¡†æ¶åœ¨ç»´æŒé¢„æµ‹ä¿çœŸåº¦çš„åŒæ—¶ï¼Œèƒ½æ›´å‡†ç¡®åœ°ä¿ç•™å› æœæ•ˆåº”ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå¼€å‘æ”¯æŒç¨³å¥å› æœåˆ†æçš„LLMé©±åŠ¨æ•°æ®æµæ°´çº¿å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.00318v1",
      "published_date": "2025-10-31 23:34:44 UTC",
      "updated_date": "2025-10-31 23:34:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:51:54.581428+00:00"
    },
    {
      "arxiv_id": "2511.00315v1",
      "title": "Language Modeling With Factorization Memory",
      "title_zh": "åŸºäºå› å­åŒ–å­˜å‚¨çš„è¯­è¨€å»ºæ¨¡",
      "authors": [
        "Lee Xiong",
        "Maksim Tkachenko",
        "Johanes Effendi",
        "Ting Cai"
      ],
      "abstract": "We propose Factorization Memory, an efficient recurrent neural network (RNN) architecture that achieves performance comparable to Transformer models on short-context language modeling tasks while also demonstrating superior generalization in long-context scenarios. Our model builds upon Mamba-2, enabling Factorization Memory to exploit parallel computations during training while preserving constant computational and memory complexity during inference. To further optimize model efficiency and representational capacity, we develop a sparse formulation of Factorization Memory that updates only a subset of recurrent states at each step while preserving the strong performance of its dense counterpart. To our knowledge, this represents the first RNN architecture that successfully combines sparse memory activation with competitive performance across both short and long-context settings. This work provides a systematic empirical analysis of Factorization Memory in comparison to Transformer and Mamba-2 architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Factorization Memoryï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„å¾ªç¯ç¥ç»ç½‘ç»œ (RNN) æ¶æ„ï¼Œæ—¨åœ¨å…¼é¡¾çŸ­æ–‡æœ¬å»ºæ¨¡æ€§èƒ½ä¸é•¿æ–‡æœ¬æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åŸºäº Mamba-2 æ„å»ºï¼Œä½¿å…¶åœ¨è®­ç»ƒé˜¶æ®µèƒ½å¤Ÿåˆ©ç”¨å¹¶è¡Œè®¡ç®—ï¼ŒåŒæ—¶åœ¨æ¨ç†é˜¶æ®µä¿æŒæ’å®šçš„è®¡ç®—ä¸å†…å­˜å¤æ‚åº¦ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ•ˆç‡å’Œè¡¨ç¤ºèƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº† Factorization Memory çš„ç¨€ç–å½¢å¼ (sparse formulation)ï¼Œé€šè¿‡æ¯æ­¥ä»…æ›´æ–°éƒ¨åˆ†å¾ªç¯çŠ¶æ€æ¥ä¿æŒå¼ºå¤§çš„æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨çŸ­æ–‡æœ¬ä»»åŠ¡ä¸Šä¸ Transformer è¡¨ç°ç›¸å½“ï¼Œè€Œåœ¨é•¿æ–‡æœ¬åœºæ™¯ä¸‹å±•ç°å‡ºæ›´ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™æ˜¯é¦–ä¸ªæˆåŠŸå°†ç¨€ç–è®°å¿†æ¿€æ´» (sparse memory activation) ä¸è·¨é•¿çŸ­è¯­å¢ƒç«äº‰æ€§èƒ½ç›¸ç»“åˆçš„ RNN æ¶æ„ã€‚è¯¥å·¥ä½œé€šè¿‡ç³»ç»Ÿæ€§çš„å®è¯åˆ†æï¼Œå¯¹æ¯”å¹¶éªŒè¯äº† Factorization Memory ç›¸å¯¹äº Transformer å’Œ Mamba-2 æ¶æ„çš„ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00315v1",
      "published_date": "2025-10-31 23:27:11 UTC",
      "updated_date": "2025-10-31 23:27:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:01.183931+00:00"
    },
    {
      "arxiv_id": "2511.00280v1",
      "title": "Calibration Across Layers: Understanding Calibration Evolution in LLMs",
      "title_zh": "è·¨å±‚æ ¡å‡†ï¼šæ¢ç©¶å¤§è¯­è¨€æ¨¡å‹ä¸­æ ¡å‡†èƒ½åŠ›çš„æ¼”åŒ–",
      "authors": [
        "Abhinav Joshi",
        "Areeb Ahmad",
        "Ashutosh Modi"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated inherent calibration capabilities, where predicted probabilities align well with correctness, despite prior findings that deep neural networks are often overconfident. Recent studies have linked this behavior to specific components in the final layer, such as entropy neurons and the unembedding matrix null space. In this work, we provide a complementary perspective by investigating how calibration evolves throughout the network depth. Analyzing multiple open-weight models on the MMLU benchmark, we uncover a distinct confidence correction phase in the upper/later layers, where model confidence is actively recalibrated after decision certainty has been reached. Furthermore, we identify a low-dimensional calibration direction in the residual stream whose perturbation significantly improves calibration metrics (ECE and MCE) without harming accuracy. Our findings suggest that calibration is a distributed phenomenon, shaped throughout the network forward pass, not just in its final projection, providing new insights into how confidence-regulating mechanisms operate within LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ ¡å‡†ï¼ˆCalibrationï¼‰èƒ½åŠ›ï¼Œé‡ç‚¹åˆ†æäº†æ ¡å‡†åœ¨ç½‘ç»œæ·±åº¦ä¸­çš„æ¼”å˜è¿‡ç¨‹ã€‚é€šè¿‡å¯¹MMLUåŸºå‡†ä¸Šå¤šä¸ªå¼€æºæ¨¡å‹çš„åˆ†æï¼Œç ”ç©¶å‘ç°åœ¨æ¨¡å‹çš„ä¸Šå±‚æˆ–åæœŸå±‚ä¸­å­˜åœ¨ä¸€ä¸ªæ˜æ˜¾çš„ç½®ä¿¡åº¦ä¿®æ­£é˜¶æ®µï¼ˆconfidence correction phaseï¼‰ï¼Œå³åœ¨å†³ç­–ç¡®å®šæ€§è¾¾æˆåï¼Œæ¨¡å‹ä¼šä¸»åŠ¨é‡æ–°æ ¡å‡†å…¶ç½®ä¿¡åº¦ã€‚ç ”ç©¶è¿›ä¸€æ­¥åœ¨æ®‹å·®æµï¼ˆresidual streamï¼‰ä¸­è¯†åˆ«å‡ºä¸€ç§ä½ç»´æ ¡å‡†æ–¹å‘ï¼ˆlow-dimensional calibration directionï¼‰ï¼Œé€šè¿‡æ‰°åŠ¨è¯¥æ–¹å‘å¯ä»¥åœ¨ä¸æŸå®³å‡†ç¡®ç‡çš„æƒ…å†µä¸‹æ˜¾è‘—æ”¹å–„æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰å’Œæœ€å¤§æ ¡å‡†è¯¯å·®ï¼ˆMCEï¼‰ç­‰æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ ¡å‡†å¹¶éä»…ç”±æœ€ç»ˆæŠ•å½±å±‚å†³å®šï¼Œè€Œæ˜¯ä¸€ä¸ªåˆ†å¸ƒåœ¨æ•´ä¸ªç½‘ç»œå‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„ç°è±¡ã€‚è¯¥å‘ç°æ­ç¤ºäº†LLMså†…éƒ¨ç½®ä¿¡åº¦è°ƒèŠ‚æœºåˆ¶çš„æ“ä½œåŸç†ï¼Œä¸ºç†è§£å’Œä¼˜åŒ–æ¨¡å‹çš„å¯é æ€§æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at EMNLP 2025 (main)",
      "pdf_url": "https://arxiv.org/pdf/2511.00280v1",
      "published_date": "2025-10-31 21:58:31 UTC",
      "updated_date": "2025-10-31 21:58:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:00.965422+00:00"
    },
    {
      "arxiv_id": "2511.00279v2",
      "title": "LongCat-Flash-Omni Technical Report",
      "title_zh": "LongCat-Flash-Omni æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Meituan LongCat Team",
        "Bairui Wang",
        "Bayan",
        "Bin Xiao",
        "Bo Zhang",
        "Bolin Rong",
        "Borun Chen",
        "Chang Wan",
        "Chao Zhang",
        "Chen Huang",
        "Chen Chen",
        "Chen Chen",
        "Chengxu Yang",
        "Chengzuo Yang",
        "Cong Han",
        "Dandan Peng",
        "Delian Ruan",
        "Detai Xin",
        "Disong Wang",
        "Dongchao Yang",
        "Fanfan Liu",
        "Fengjiao Chen",
        "Fengyu Yang",
        "Gan Dong",
        "Gang Huang",
        "Gang Xu",
        "Guanglu Wan",
        "Guoqiang Tan",
        "Guoqiao Yu",
        "Haibo Qiu",
        "Hao Lu",
        "Hongbo Liu",
        "Hongyu Xiang",
        "Jiaheng Wu",
        "Jian Yang",
        "Jiaxing Liu",
        "Jing Huang",
        "Jingang Wang",
        "Jinrui Ding",
        "Juchao Jiang",
        "Jun Kuang",
        "Jun Wang",
        "Junhui Mei",
        "Ke Ding",
        "Kefeng Zhang",
        "Lei Chen",
        "Liang Shi",
        "Limeng Qiao",
        "Liming Zheng",
        "Lin Ma",
        "Liuyang Guo",
        "Liya Ma",
        "Luying Sun",
        "Man Gao",
        "Mengshen Zhu",
        "Miao Cao",
        "Minliang Lin",
        "Nuo Xu",
        "Peng Shi",
        "Qi Zhang",
        "Qian Fang",
        "Qian Wang",
        "Qian Yang",
        "Quanxiu Wang",
        "Rongxiang Weng",
        "Rongxin Guo",
        "Ruoxuan Liang",
        "Senbin Yang",
        "Shanbo Xu",
        "Shanglin Lei",
        "Shengze Ye",
        "Shimin Chen",
        "Shuaiqi Chen",
        "Shujie Hu",
        "Shuo Li",
        "Siqi Yang",
        "Siyu Xu",
        "Siyu Ren",
        "Song Li",
        "Songxiang Liu",
        "Tianhao Bai",
        "Tianye Dai",
        "Wei Hong",
        "Wei Wang",
        "Weixiao Zhao",
        "Wengang Cao",
        "Wenlong Zhu",
        "Wenlong He",
        "Xi Su",
        "Xi Nan",
        "Xiaohan Zhao",
        "Xiaohao Wang",
        "Xiaoyu Zhao",
        "Xiaoyu Wang",
        "Xiaoyu Li",
        "Xin Pan",
        "Xin Chen",
        "Xiusong Sun",
        "Xu Xiang",
        "Xudong Xing",
        "Xuezhi Cao",
        "Xunliang Cai",
        "Yang Yang",
        "Yanli Tan",
        "Yao Yao",
        "Yerui Sun",
        "Yi Chen",
        "Yifan Lu",
        "Yin Gong",
        "Yining Zhang",
        "Yitian Chen",
        "Yiyang Gan",
        "Yuchen Tang",
        "Yuchen Xie",
        "Yueqian Wang",
        "Yuewen Zheng",
        "Yufei Zhang",
        "Yufeng Zhong",
        "Yulei Qian",
        "Yuqi Peng",
        "Yuqian Li",
        "Yuwei Jiang",
        "Zeyang Hu",
        "Zheng Zhang",
        "Zhengkun Tian",
        "Zhiqing Hong",
        "Zhixiong Zeng",
        "Zhuqi Mi",
        "Ziran Li",
        "Ziwen Wang",
        "Ziyi Zhao",
        "Ziyuan Zhuang",
        "Zizhe Zhao"
      ],
      "abstract": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†LongCat-Flash-Omniï¼Œä¸€ä¸ªæ‹¥æœ‰5600äº¿å‚æ•°çš„å¼€æºå…¨æ¨¡æ€æ¨¡å‹(Omni-modal model)ï¼Œæ—¨åœ¨å®ç°é«˜æ€§èƒ½çš„å®æ—¶è§†å¬äº¤äº’ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å—è¯¾ç¨‹å­¦ä¹ å¯å‘çš„æ¸è¿›å¼è®­ç»ƒç­–ç•¥(Curriculum-inspired progressive training strategy)ï¼Œé€šè¿‡ä»ç®€å•åˆ°å¤æ‚çš„æ¨¡æ€åºåˆ—å»ºæ¨¡ä»»åŠ¡ï¼Œåœ¨å¢å¼ºå…¨æ¨¡æ€èƒ½åŠ›çš„åŒæ—¶ä¿ç•™äº†å¼ºå¤§çš„å•æ¨¡æ€æ€§èƒ½ã€‚æ¶æ„ä¸Šï¼Œå®ƒç»§æ‰¿äº†LongCat-Flashçš„å¿«æ·è¿æ¥æ··åˆä¸“å®¶ç³»ç»Ÿ(Shortcut-connected Mixture-of-Experts, MoE)ä¸é›¶è®¡ç®—ä¸“å®¶è®¾è®¡ï¼Œå¹¶é›†æˆäº†é«˜æ•ˆçš„å¤šæ¨¡æ€æ„ŸçŸ¥å’Œè¯­éŸ³é‡æ„æ¨¡å—ã€‚å°½ç®¡å‚æ•°è§„æ¨¡åºå¤§ï¼Œä½†é€šè¿‡ä»…æ¿€æ´»270äº¿å‚æ•°çš„æœºåˆ¶ï¼Œæ¨¡å‹æˆåŠŸå®ç°äº†ä½å»¶è¿Ÿçš„å®æ—¶éŸ³è§†é¢‘äº¤äº’ã€‚ä¸ºäº†ä¼˜åŒ–è®­ç»ƒï¼Œç ”ç©¶è€…å¼€å‘äº†æ¨¡æ€è§£è€¦å¹¶è¡Œæ–¹æ¡ˆ(Modality-decoupled parallelism scheme)ä»¥å¤„ç†å¼‚æ„æ•°æ®ï¼Œç»´æŒäº†è¶…è¿‡çº¯æ–‡æœ¬è®­ç»ƒ90%çš„ååæ•ˆç‡ã€‚å¤šé¡¹è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¼€æºå…¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°SOTAæ°´å¹³ï¼Œå¹¶åœ¨æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘åŠéŸ³é¢‘çš„ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CL",
        "cs.DC",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00279v2",
      "published_date": "2025-10-31 21:58:15 UTC",
      "updated_date": "2025-11-28 09:10:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:08.071453+00:00"
    },
    {
      "arxiv_id": "2511.00270v1",
      "title": "POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation",
      "title_zh": "POSESTITCH-SLTï¼šå—è¯­è¨€å­¦å¯å‘çš„ç«¯åˆ°ç«¯æ‰‹è¯­ç¿»è¯‘å§¿æ€æ‹¼æ¥",
      "authors": [
        "Abhinav Joshi",
        "Vaibhav Sharma",
        "Sanjeet Singh",
        "Ashutosh Modi"
      ],
      "abstract": "Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰‹è¯­ç¿»è¯‘(Sign language translation)ä¸­å¤§è§„æ¨¡å¥å­å¯¹é½æ•°æ®é›†åŒ®ä¹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†POSESTITCH-SLTæ¡†æ¶ã€‚è¿™ä¸€æ–°å‹é¢„è®­ç»ƒæ–¹æ¡ˆå—åŸºäºè¯­è¨€æ¨¡æ¿çš„å¥å­ç”ŸæˆæŠ€æœ¯(linguistic-templates-based sentence generation)å¯å‘ï¼Œæ—¨åœ¨å¢å¼ºç«¯åˆ°ç«¯çš„ç¿»è¯‘æ€§èƒ½ã€‚ç ”ç©¶é‡‡ç”¨ç®€å•çš„Transformer-based encoder-decoderæ¶æ„ï¼Œé€šè¿‡åœ¨è®­ç»ƒä¸­å¼•å…¥ç”±æ¨¡æ¿ç”Ÿæˆçš„åˆæˆå¥å¯¹ï¼ŒæˆåŠŸæå‡äº†æ¨¡å‹çš„ç¿»è¯‘å‡†ç¡®åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPOSESTITCH-SLTåœ¨How2Signå’ŒiSignä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„BLEU-4å¾—åˆ†åˆ†åˆ«ä»1.97æå‡è‡³4.56ä»¥åŠä»0.55æå‡è‡³3.43ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ— æ‰‹è¯­æ ‡æ³¨(gloss-free)ç¿»è¯‘æ–¹æ³•ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨ä½èµ„æºæ‰‹è¯­ç¯å¢ƒä¸‹ï¼Œæ¨¡æ¿é©±åŠ¨çš„åˆæˆç›‘ç£(synthetic supervision)å¯¹äºæå‡åŸºäºå§¿æ€(pose-based)ç¿»è¯‘è´¨é‡çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2025 (Main)",
      "pdf_url": "https://arxiv.org/pdf/2511.00270v1",
      "published_date": "2025-10-31 21:44:59 UTC",
      "updated_date": "2025-10-31 21:44:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:07.282708+00:00"
    },
    {
      "arxiv_id": "2511.00269v1",
      "title": "FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture",
      "title_zh": "FedReplayï¼šé¢å‘é«˜æ•ˆã€éšç§ä¿æŠ¤æ™ºæ…§å†œä¸šçš„ç‰¹å¾å›æ”¾è¾…åŠ©è”é‚¦è¿ç§»å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Long Li",
        "Jiajia Li",
        "Dong Chen",
        "Lina Pu",
        "Haibo Yao",
        "Yanbo Huang"
      ],
      "abstract": "Accurate classification plays a pivotal role in smart agriculture, enabling applications such as crop monitoring, fruit recognition, and pest detection. However, conventional centralized training often requires large-scale data collection, which raises privacy concerns, while standard federated learning struggles with non-independent and identically distributed (non-IID) data and incurs high communication costs. To address these challenges, we propose a federated learning framework that integrates a frozen Contrastive Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight transformer classifier. By leveraging the strong feature extraction capability of the pre-trained CLIP ViT, the framework avoids training large-scale models from scratch and restricts federated updates to a compact classifier, thereby reducing transmission overhead significantly. Furthermore, to mitigate performance degradation caused by non-IID data distribution, a small subset (1%) of CLIP-extracted feature representations from all classes is shared across clients. These shared features are non-reversible to raw images, ensuring privacy preservation while aligning class representation across participants. Experimental results on agricultural classification tasks show that the proposed method achieve 86.6% accuracy, which is more than 4 times higher compared to baseline federated learning approaches. This demonstrates the effectiveness and efficiency of combining vision-language model features with federated learning for privacy-preserving and scalable agricultural intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FedReplayï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç‰¹å¾å›æ”¾(Feature Replay)è¾…åŠ©çš„è”é‚¦è¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½å†œä¸šé¢†åŸŸä¸­ä¼ ç»Ÿè”é‚¦å­¦ä¹ é¢ä¸´çš„éç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID)æ•°æ®æŒ‘æˆ˜å’Œé«˜æ˜‚çš„é€šä¿¡æˆæœ¬ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†é¢„è®­ç»ƒå¹¶å†»ç»“çš„CLIP Vision Transformer (ViT)ä¸è½»é‡çº§Transformeråˆ†ç±»å™¨ç›¸ç»“åˆï¼Œé€šè¿‡ä»…æ›´æ–°å°å‹åˆ†ç±»å™¨æ˜¾è‘—é™ä½äº†è”é‚¦å­¦ä¹ è¿‡ç¨‹ä¸­çš„ä¼ è¾“å¼€é”€ã€‚ä¸ºåº”å¯¹non-IIDæ•°æ®å¸¦æ¥çš„æ€§èƒ½ä¸‹é™ï¼ŒFedReplayé€šè¿‡åœ¨å®¢æˆ·ç«¯é—´å…±äº«æå°æ¯”ä¾‹(1%)ä¸”ä¸å¯é€†çš„ç‰¹å¾è¡¨ç¤ºæ¥å¯¹é½ç±»åˆ«åˆ†å¸ƒï¼Œåœ¨ç¡®ä¿éšç§çš„åŒæ—¶æå‡æ¨¡å‹ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å†œä¸šåˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†86.6%çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½ä¼˜äºåŸºå‡†è”é‚¦å­¦ä¹ æ–¹æ³•4å€ä»¥ä¸Šã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç‰¹å¾ç»“åˆè”é‚¦å­¦ä¹ ï¼Œæ˜¯æ„å»ºé«˜æ•ˆã€éšç§ä¿æŠ¤ä¸”å…·å¤‡æ‰©å±•æ€§çš„å†œä¸šæ™ºèƒ½ç³»ç»Ÿçš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00269v1",
      "published_date": "2025-10-31 21:44:52 UTC",
      "updated_date": "2025-10-31 21:44:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:10.979280+00:00"
    },
    {
      "arxiv_id": "2511.00268v1",
      "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval",
      "title_zh": "IL-PCSRï¼šé¢å‘å…ˆä¾‹ä¸æ³•è§„æ£€ç´¢çš„æ³•å¾‹è¯­æ–™åº“",
      "authors": [
        "Shounak Paul",
        "Dhananjay Ghumare",
        "Pawan Goyal",
        "Saptarshi Ghosh",
        "Ashutosh Modi"
      ],
      "abstract": "Identifying/retrieving relevant statutes and prior cases/precedents for a given legal situation are common tasks exercised by law practitioners. Researchers to date have addressed the two tasks independently, thus developing completely different datasets and models for each task; however, both retrieval tasks are inherently related, e.g., similar cases tend to cite similar statutes (due to similar factual situation). In this paper, we address this gap. We propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval), which is a unique corpus that provides a common testbed for developing models for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit the dependence between the two. We experiment extensively with several baseline models on the tasks, including lexical models, semantic models and ensemble based on GNNs. Further, to exploit the dependence between the two tasks, we develop an LLM-based re-ranking approach that gives the best performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ³•å¾‹ä»ä¸šè€…åœ¨æ£€ç´¢æ³•ä»¤(Statute)å’Œå…ˆä¾‹(Prior Case)æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰ç ”ç©¶å°†ä¸¤é¡¹ä»»åŠ¡ç‹¬ç«‹å¤„ç†çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† IL-PCSR (Indian Legal corpus for Prior Case and Statute Retrieval)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæ³•ä»¤æ£€ç´¢(Statute Retrieval)å’Œå…ˆä¾‹æ£€ç´¢(Precedent Retrieval)æä¾›ç»Ÿä¸€æµ‹è¯•ç¯å¢ƒçš„ç‹¬ç‰¹è¯­æ–™åº“ã€‚è¯¥è¯­æ–™åº“æ—¨åœ¨åˆ©ç”¨ä¸¤é¡¹ä»»åŠ¡ä¹‹é—´çš„å†…åœ¨ä¾èµ–å…³ç³»ï¼Œå³ç›¸ä¼¼çš„äº‹å®æƒ…å¢ƒé€šå¸¸æ¶‰åŠç›¸ä¼¼çš„æ¡ˆä¾‹ä¸æ³•ä»¤å¼•ç”¨ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å¤šä¸ªåŸºå‡†æ¨¡å‹ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬è¯æ³•æ¨¡å‹(Lexical Models)ã€è¯­ä¹‰æ¨¡å‹(Semantic Models)å’ŒåŸºäºå›¾ç¥ç»ç½‘ç»œ(GNNs)çš„é›†æˆæ¨¡å‹ã€‚æœ€ç»ˆï¼Œç ”ç©¶é€šè¿‡å¼€å‘ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM-based)çš„é‡æ’åºæ–¹æ³•ï¼Œå……åˆ†æŒ–æ˜äº†ä»»åŠ¡é—´çš„å…³è”æ€§ï¼Œå¹¶å–å¾—äº†æœ€ä½³çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2025 (Main)",
      "pdf_url": "https://arxiv.org/pdf/2511.00268v1",
      "published_date": "2025-10-31 21:39:04 UTC",
      "updated_date": "2025-10-31 21:39:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:13.378944+00:00"
    },
    {
      "arxiv_id": "2511.00267v1",
      "title": "Advancing AI Challenges for the United States Department of the Air Force",
      "title_zh": "æ¨è¿› United States Department of the Air Force äººå·¥æ™ºèƒ½æŒ‘æˆ˜èµ›",
      "authors": [
        "Christian Prothmann",
        "Vijay Gadepally",
        "Jeremy Kepner",
        "Koley Borchard",
        "Luca Carlone",
        "Zachary Folcik",
        "J. Daniel Grith",
        "Michael Houle",
        "Jonathan P. How",
        "Nathan Hughes",
        "Ifueko Igbinedion",
        "Hayden Jananthan",
        "Tejas Jayashankar",
        "Michael Jones",
        "Sertac Karaman",
        "Binoy G. Kurien",
        "Alejandro Lancho",
        "Giovanni Lavezzi",
        "Gary C. F. Lee",
        "Charles E. Leiserson",
        "Richard Linares",
        "Lindsey McEvoy",
        "Peter Michaleas",
        "Chasen Milner",
        "Alex Pentland",
        "Yury Polyanskiy",
        "Jovan Popovich",
        "Jeffrey Price",
        "Tim W. Reid",
        "Stephanie Riley",
        "Siddharth Samsi",
        "Peter Saunders",
        "Olga Simek",
        "Mark S. Veillette",
        "Amir Weiss",
        "Gregory W. Wornell",
        "Daniela Rus",
        "Scott T. Ruppel"
      ],
      "abstract": "The DAF-MIT AI Accelerator is a collaboration between the United States Department of the Air Force (DAF) and the Massachusetts Institute of Technology (MIT). This program pioneers fundamental advances in artificial intelligence (AI) to expand the competitive advantage of the United States in the defense and civilian sectors. In recent years, AI Accelerator projects have developed and launched public challenge problems aimed at advancing AI research in priority areas. Hallmarks of AI Accelerator challenges include large, publicly available, and AI-ready datasets to stimulate open-source solutions and engage the wider academic and private sector AI ecosystem. This article supplements our previous publication, which introduced AI Accelerator challenges. We provide an update on how ongoing and new challenges have successfully contributed to AI research and applications of AI technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯¦ç»†ä»‹ç»äº†DAF-MIT AI Acceleratorï¼Œå³ç¾å›½ç©ºå†›(DAF)ä¸éº»çœç†å·¥å­¦é™¢(MIT)åˆä½œçš„äººå·¥æ™ºèƒ½åŠ é€Ÿå™¨é¡¹ç›®ï¼Œæ—¨åœ¨é€šè¿‡AIé¢†åŸŸçš„æ ¹æœ¬æ€§çªç ´æå‡ç¾å›½åœ¨å›½é˜²ä¸æ°‘ç”¨é¢†åŸŸçš„ç«äº‰ä¼˜åŠ¿ã€‚è¯¥é¡¹ç›®çš„ä¸€å¤§æ ¸å¿ƒç‰¹å¾æ˜¯å¼€å‘å¹¶å‘å¸ƒå…¬å¼€çš„æŒ‘æˆ˜èµ›é—®é¢˜ï¼Œå¹¶æä¾›å¤§è§„æ¨¡ä¸”AI-readyçš„æ•°æ®é›†ï¼Œä»¥æ¿€å‘å­¦æœ¯ç•Œå’Œç§è¥éƒ¨é—¨æ¢ç´¢å¼€æºè§£å†³æ–¹æ¡ˆã€‚ä½œä¸ºå¯¹å‰æœŸå·¥ä½œçš„åç»­æ›´æ–°ï¼Œæœ¬æ–‡æ·±å…¥æ¢è®¨äº†è¿™äº›æŒç»­è¿›è¡Œä¸­åŠæ–°å¢çš„æŒ‘æˆ˜èµ›å¦‚ä½•æˆåŠŸæ¨åŠ¨äº†AI researchçš„è¿›å±•ï¼Œå¹¶ä¿ƒè¿›äº†AI technologiesåœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æ–‡ç« é€šè¿‡æ€»ç»“è¿™äº›æŒ‘æˆ˜èµ›çš„æˆæœï¼Œå±•ç¤ºäº†å…¶åœ¨è§£å†³å…³é”®é¢†åŸŸæŠ€æœ¯ç“¶é¢ˆå’Œæ„å»ºåä½œå¼AIç”Ÿæ€ç³»ç»Ÿæ–¹é¢æ‰€å‘æŒ¥çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.GL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 8 figures, 59 references. To appear in IEEE HPEC 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.00267v1",
      "published_date": "2025-10-31 21:34:57 UTC",
      "updated_date": "2025-10-31 21:34:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:28.181018+00:00"
    },
    {
      "arxiv_id": "2601.05262v1",
      "title": "LLM2IR: simple unsupervised contrastive learning makes long-context LLM great retriever",
      "title_zh": "LLM2IRï¼šç®€å•çš„æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ è®©é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹æˆä¸ºå‡ºè‰²çš„æ£€ç´¢å™¨",
      "authors": [
        "Xiaocong Yang"
      ],
      "abstract": "Modern dense information retrieval (IR) models usually rely on costly large-scale pretraining. In this paper, we introduce LLM2IR, an efficient unsupervised contrastive learning framework to convert any decoder-only large language model (LLM) to an information retrieval model. Despite its simplicity, the effectiveness is proven among different LLMs on multiple IR benchmarks including LoCo, LongEmbed and BEIR. We also find that models with a longer context length tend to have a stronger IR capacity by comparing task performances of models in the same model family. Our work not only provides an effective way to build IR models on the state-of-the-art LLMs, but also shed light on the relationship between information retrieval ability and model context length, which helps the design of better information retrievers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LLM2IRï¼Œä¸€ä¸ªé«˜æ•ˆçš„æ— ç›‘ç£å¯¹æ¯”å­¦ä¹  (unsupervised contrastive learning) æ¡†æ¶ï¼Œæ—¨åœ¨å°†ä»»ä½•ä»…è§£ç å™¨ (decoder-only) çš„å¤§è¯­è¨€æ¨¡å‹ (LLM) è½¬æ¢ä¸ºä¿¡æ¯æ£€ç´¢ (IR) æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡ç®€å•çš„å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œè§£å†³äº†ç°ä»£ç¨ å¯†æ£€ç´¢æ¨¡å‹é€šå¸¸éœ€è¦ä¾èµ–æ˜‚è´µçš„å¤§è§„æ¨¡é¢„è®­ç»ƒçš„é—®é¢˜ã€‚å®éªŒåœ¨ LoCoã€LongEmbed å’Œ BEIR ç­‰å¤šä¸ªä¿¡æ¯æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­è¯æ˜äº† LLM2IR çš„æœ‰æ•ˆæ€§ï¼Œä¸”è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§ä¸åŒçš„ LLM å®¶æ—ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œæ¨¡å‹çš„ä¿¡æ¯æ£€ç´¢èƒ½åŠ›ä¸å…¶ä¸Šä¸‹æ–‡é•¿åº¦ (context length) å­˜åœ¨æ­£ç›¸å…³å…³ç³»ï¼Œå³æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦å¾€å¾€å¯¹åº”æ›´å¼ºçš„æ£€ç´¢èƒ½åŠ›ã€‚LLM2IR ä¸ä»…ä¸ºåŸºäºæœ€å…ˆè¿›çš„ LLM æ„å»ºæ£€ç´¢æ¨¡å‹æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œè¿˜æ·±å…¥æ¢è®¨äº†æ£€ç´¢èƒ½åŠ›ä¸æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦ä¹‹é—´çš„å…³ç³»ï¼Œä¸ºè®¾è®¡æ›´é«˜æ•ˆçš„ä¿¡æ¯æ£€ç´¢å™¨æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "MS Thesis",
      "pdf_url": "https://arxiv.org/pdf/2601.05262v1",
      "published_date": "2025-10-31 21:30:37 UTC",
      "updated_date": "2025-10-31 21:30:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:17.976817+00:00"
    },
    {
      "arxiv_id": "2511.01911v1",
      "title": "Variational Geometry-aware Neural Network based Method for Solving High-dimensional Diffeomorphic Mapping Problems",
      "title_zh": "åŸºäºå˜åˆ†å‡ ä½•æ„ŸçŸ¥ç¥ç»ç½‘ç»œçš„é«˜ç»´å¾®åˆ†åŒèƒšæ˜ å°„æ±‚è§£æ–¹æ³•",
      "authors": [
        "Zhiwen Li",
        "Cheuk Hin Ho",
        "Lok Ming Lui"
      ],
      "abstract": "Traditional methods for high-dimensional diffeomorphic mapping often struggle with the curse of dimensionality. We propose a mesh-free learning framework designed for $n$-dimensional mapping problems, seamlessly combining variational principles with quasi-conformal theory. Our approach ensures accurate, bijective mappings by regulating conformality distortion and volume distortion, enabling robust control over deformation quality. The framework is inherently compatible with gradient-based optimization and neural network architectures, making it highly flexible and scalable to higher-dimensional settings. Numerical experiments on both synthetic and real-world medical image data validate the accuracy, robustness, and effectiveness of the proposed method in complex registration scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå˜åˆ†å‡ ä½•æ„ŸçŸ¥ç¥ç»ç½‘ç»œ(Variational Geometry-aware Neural Network)çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é«˜ç»´å¾®åˆ†åŒèƒšæ˜ å°„(high-dimensional diffeomorphic mapping)é—®é¢˜ä¸­å¸¸è§çš„ç»´åº¦ç¾éš¾ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ— ç½‘æ ¼å­¦ä¹ (mesh-free learning)ç»“æ„ï¼Œå¹¶å°†å˜åˆ†åŸç†(variational principles)ä¸æ‹Ÿå…±å½¢ç†è®º(quasi-conformal theory)æ— ç¼ç»“åˆã€‚é€šè¿‡è°ƒèŠ‚å…±å½¢ç•¸å˜(conformality distortion)å’Œä½“ç§¯ç•¸å˜(volume distortion)ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç¡®ä¿æ˜ å°„çš„å‡†ç¡®æ€§å’ŒåŒå°„æ€§(bijective mappings)ï¼Œä»è€Œå®ç°å¯¹å˜å½¢è´¨é‡çš„ç¨³å¥æ§åˆ¶ã€‚è¯¥æ¡†æ¶ä¸åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å’Œç¥ç»ç½‘ç»œæ¶æ„å…·æœ‰å†…åœ¨å…¼å®¹æ€§ï¼Œå±•ç°å‡ºæé«˜çš„çµæ´»æ€§ï¼Œå¹¶å¯å¹³æ»‘æ‰©å±•è‡³æ›´é«˜ç»´åº¦çš„åº”ç”¨åœºæ™¯ã€‚åœ¨åˆæˆæ•°æ®å’ŒçœŸå®åŒ»å­¦å›¾åƒæ•°æ®ä¸Šçš„æ•°å€¼å®éªŒå……åˆ†éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤æ‚é…å‡†(complex registration)ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ã€ç¨³å¥æ€§åŠæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.DG",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01911v1",
      "published_date": "2025-10-31 20:39:08 UTC",
      "updated_date": "2025-10-31 20:39:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:41.582322+00:00"
    },
    {
      "arxiv_id": "2511.11603v1",
      "title": "Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review",
      "title_zh": "åŸºäºæœºå™¨å­¦ä¹ çš„äº‘èµ„æºåˆ†é…ç®—æ³•ï¼šå…¨é¢æ¯”è¾ƒç»¼è¿°",
      "authors": [
        "Deep Bodra",
        "Sushil Khairnar"
      ],
      "abstract": "Cloud resource allocation has emerged as a major challenge in modern computing environments, with organizations struggling to manage complex, dynamic workloads while optimizing performance and cost efficiency. Traditional heuristic approaches prove inadequate for handling the multi-objective optimization demands of existing cloud infrastructures. This paper presents a comparative analysis of state-of-the-art artificial intelligence and machine learning algorithms for resource allocation. We systematically evaluate 10 algorithms across four categories: Deep Reinforcement Learning approaches, Neural Network architectures, Traditional Machine Learning enhanced methods, and Multi-Agent systems. Analysis of published results demonstrates significant performance improvements across multiple metrics including makespan reduction, cost optimization, and energy efficiency gains compared to traditional methods. The findings reveal that hybrid architectures combining multiple artificial intelligence and machine learning techniques consistently outperform single-method approaches, with edge computing environments showing the highest deployment readiness. Our analysis provides critical insights for both academic researchers and industry practitioners seeking to implement next-generation cloud resource allocation strategies in increasingly complex and dynamic computing environments.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡æ¢è®¨äº†åœ¨ç°ä»£äº‘è®¡ç®—ç¯å¢ƒä¸­ï¼Œå¦‚ä½•åˆ©ç”¨æœºå™¨å­¦ä¹ ç®—æ³•è§£å†³å¤æ‚ã€åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹çš„èµ„æºåˆ†é…éš¾é¢˜ã€‚ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†å››ç±»å…±10ç§äººå·¥æ™ºèƒ½ä¸æœºå™¨å­¦ä¹ ç®—æ³•ï¼ŒåŒ…æ‹¬æ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning)ã€ç¥ç»ç½‘ç»œæ¶æ„ (Neural Network architectures)ã€ä¼ ç»Ÿæœºå™¨å­¦ä¹ å¢å¼ºæ–¹æ³• (Traditional Machine Learning enhanced methods) ä»¥åŠå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-Agent systems)ã€‚é€šè¿‡å¯¹å„é¡¹æŒ‡æ ‡çš„å¯¹æ¯”åˆ†æï¼Œç»“æœæ˜¾ç¤ºè¿™äº›ç®—æ³•åœ¨ç¼©çŸ­å®Œå·¥æ—¶é—´ (makespan)ã€ä¼˜åŒ–æˆæœ¬åŠæå‡èƒ½æºæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¯å‘å¼æ–¹æ³•ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œç»“åˆå¤šç§ AI/ML æŠ€æœ¯çš„æ··åˆæ¶æ„ (hybrid architectures) æ€§èƒ½å§‹ç»ˆä¼˜äºå•ä¸€æ–¹æ³•ï¼Œä¸”è¾¹ç¼˜è®¡ç®— (edge computing) ç¯å¢ƒåœ¨éƒ¨ç½²å°±ç»ªæ€§æ–¹é¢è¡¨ç°æœ€ä¸ºçªå‡ºã€‚è¯¥è®ºæ–‡ä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œåœ¨æ—¥ç›Šå¤æ‚çš„åŠ¨æ€è®¡ç®—ç¯å¢ƒä¸­å®æ–½ä¸‹ä¸€ä»£äº‘èµ„æºåˆ†é…ç­–ç•¥æä¾›äº†å…³é”®çš„æŠ€æœ¯è§è§£ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11603v1",
      "published_date": "2025-10-31 20:30:21 UTC",
      "updated_date": "2025-10-31 20:30:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:45.678553+00:00"
    },
    {
      "arxiv_id": "2511.00230v2",
      "title": "Neural Transparency: Mechanistic Interpretability Interfaces for Anticipating Model Behaviors for Personalized AI",
      "title_zh": "ç¥ç»é€æ˜æ€§ï¼šç”¨äºé¢„åˆ¤ä¸ªæ€§åŒ–äººå·¥æ™ºèƒ½æ¨¡å‹è¡Œä¸ºçš„æœºåˆ¶å¯è§£é‡Šæ€§äº¤äº’ç•Œé¢",
      "authors": [
        "Sheer Karny",
        "Anthony Baez",
        "Pat Pataranutaporn"
      ],
      "abstract": "Millions of users now design personalized LLM-based chatbots that shape their daily interactions, yet they can only roughly anticipate how their design choices will manifest as behaviors in deployment. This opacity is consequential: seemingly innocuous prompts can trigger excessive sycophancy, toxicity, or other undesirable traits, degrading utility and raising safety concerns. To address this issue, we introduce an interface that enables neural transparency by exposing language model internals during chatbot design. Our approach extracts behavioral trait vectors (empathy, toxicity, sycophancy, etc.) by computing differences in neural activations between contrastive system prompts that elicit opposing behaviors. We predict chatbot behaviors by projecting the system prompt's final token activations onto these trait vectors, normalizing for cross-trait comparability, and visualizing results via an interactive sunburst diagram. To evaluate this approach, we conducted an online user study using Prolific to compare our neural transparency interface against a baseline chatbot interface without any form of transparency. Our analyses suggest that users systematically miscalibrated AI behavior: participants misjudged trait activations for eleven of fifteen analyzable traits, motivating the need for transparency tools in everyday human-AI interaction. While our interface did not change design iteration patterns, it significantly increased user trust and was enthusiastically received. Qualitative analysis revealed nuanced user experiences with the visualization, suggesting interface and interaction improvements for future work. This work offers a path for how mechanistic interpretability can be operationalized for non-technical users, establishing a foundation for safer, more aligned human-AI interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”¨æˆ·åœ¨è®¾è®¡ä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººæ—¶éš¾ä»¥é¢„æµ‹æ¨¡å‹è¡Œä¸ºçš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Neural Transparency çš„ Mechanistic Interpretability ç•Œé¢ï¼Œæ—¨åœ¨æé«˜ AI çš„è¡Œä¸ºé€æ˜åº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—å¯¹æ¯”ç³»ç»Ÿæç¤ºè¯ä¹‹é—´çš„ç¥ç»æ¿€æ´»å·®å¼‚ï¼Œæå–åŒ…æ‹¬ empathyã€toxicity å’Œ sycophancy åœ¨å†…çš„è¡Œä¸ºç‰¹å¾å‘é‡ (behavioral trait vectors)ã€‚ç³»ç»Ÿå°†æç¤ºè¯çš„æœ€ç»ˆ token æ¿€æ´»æŠ•å½±åˆ°è¿™äº›å‘é‡ä¸Šï¼Œå¹¶åˆ©ç”¨äº¤äº’å¼ sunburst diagram å¯è§†åŒ–é¢„æµ‹çš„æ¨¡å‹è¡Œä¸ºã€‚åœ¨çº¿ç”¨æˆ·ç ”ç©¶æ˜¾ç¤ºï¼Œå‚ä¸è€…åœ¨ 15 ä¸ªå¯åˆ†æç‰¹å¾ä¸­è¯¯åˆ¤äº† 11 ä¸ªï¼Œå‡¸æ˜¾äº†é€æ˜åº¦å·¥å…·åœ¨æ—¥å¸¸äººæœºäº¤äº’ä¸­çš„å¿…è¦æ€§ã€‚è™½ç„¶è¯¥ç•Œé¢æœªæ”¹å˜ç”¨æˆ·çš„è®¾è®¡è¿­ä»£æ¨¡å¼ï¼Œä½†å®ƒæ˜¾è‘—å¢å¼ºäº†ç”¨æˆ·ä¿¡ä»»ï¼Œä¸ºéæŠ€æœ¯ç”¨æˆ·åˆ©ç”¨ Mechanistic Interpretability å®ç°æ›´å®‰å…¨ã€æ›´å¯¹é½çš„ AI äº¤äº’å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "SK and AB are co-first authors",
      "pdf_url": "https://arxiv.org/pdf/2511.00230v2",
      "published_date": "2025-10-31 20:03:52 UTC",
      "updated_date": "2025-11-22 00:19:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:54.582224+00:00"
    },
    {
      "arxiv_id": "2511.00222v1",
      "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning",
      "title_zh": "åŸºäºå¤šè½®å¼ºåŒ–å­¦ä¹ çš„äººç±»äººæ ¼ä¸€è‡´æ€§æ¨¡æ‹Ÿ",
      "authors": [
        "Marwa Abdulhai",
        "Ryan Cheng",
        "Donovan Clay",
        "Tim Althoff",
        "Sergey Levine",
        "Natasha Jaques"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used to simulate human users in interactive settings such as therapy, education, and social role-play. While these simulations enable scalable training and evaluation of AI agents, off-the-shelf LLMs often drift from their assigned personas, contradict earlier statements, or abandon role-appropriate behavior. We introduce a unified framework for evaluating and improving persona consistency in LLM-generated dialogue. We define three automatic metrics: prompt-to-line consistency, line-to-line consistency, and Q&A consistency, that capture different types of persona drift and validate each against human annotations. Using these metrics as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs for three user roles: a patient, a student, and a social chat partner. Our method reduces inconsistency by over 55%, resulting in more coherent and faithful simulated users.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨æ¨¡æ‹Ÿäººç±»è§’è‰²æ—¶å®¹æ˜“å‡ºç°çš„è§’è‰²åç§»(persona drift)å’Œè¨€è¡ŒçŸ›ç›¾ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°å¹¶æå‡è§’è‰²ä¸€è‡´æ€§çš„ç»Ÿä¸€æ¡†æ¶ã€‚ç ”ç©¶å®šä¹‰äº†ä¸‰é¡¹è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬prompt-to-line consistencyã€line-to-line consistencyä»¥åŠQ&A consistencyï¼Œå¹¶éªŒè¯äº†è¿™äº›æŒ‡æ ‡ä¸äººå·¥æ ‡æ³¨çš„é«˜åº¦ç›¸å…³æ€§ã€‚é€šè¿‡å°†è¿™äº›æŒ‡æ ‡ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä½œè€…åˆ©ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ (multi-turn reinforcement learning)å¯¹LLMsè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ›´ç¨³å®šåœ°æ‰®æ¼”ç—…äººã€å­¦ç”Ÿå’Œç¤¾äº¤ä¼™ä¼´ç­‰è§’è‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸå°†è§’è‰²ä¸ä¸€è‡´æ€§é™ä½äº†55%ä»¥ä¸Šï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡æ‹Ÿç”¨æˆ·åœ¨å¤šè½®å¯¹è¯ä¸­çš„è¿è´¯æ€§å’Œå¿ å®åº¦ã€‚è¯¥æˆæœä¸ºåœ¨åŒ»ç–—ã€æ•™è‚²å’Œç¤¾ä¼šè§’è‰²æ‰®æ¼”ç­‰äº¤äº’åœºæ™¯ä¸­æ„å»ºæ›´å¯é çš„AIæ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00222v1",
      "published_date": "2025-10-31 19:40:41 UTC",
      "updated_date": "2025-10-31 19:40:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:51.386203+00:00"
    },
    {
      "arxiv_id": "2511.00218v1",
      "title": "DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy",
      "title_zh": "DM-QPMNETï¼šç”¨äºå®šé‡ç›¸ä½æ˜¾å¾®æˆåƒç»†èƒåˆ†å‰²çš„åŒæ¨¡æ€èåˆç½‘ç»œ",
      "authors": [
        "Rajatsubhra Chakraborty",
        "Ana Espinosa-Momox",
        "Riley Haskin",
        "Depeng Xu",
        "Rosario Porras-Aguilar"
      ],
      "abstract": "Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces challenges from traditional thresholding methods that are sensitive to noise and cell density, while deep learning approaches using simple channel concatenation fail to exploit the complementary nature of polarized intensity images and phase maps. We introduce DM-QPMNet, a dual-encoder network that treats these as distinct modalities with separate encoding streams. Our architecture fuses modality-specific features at intermediate depth via multi-head attention, enabling polarized edge and texture representations to selectively integrate complementary phase information. This content-aware fusion preserves training stability while adding principled multi-modal integration through dual-source skip connections and per-modality normalization at minimal overhead. Our approach demonstrates substantial improvements over monolithic concatenation and single-modality baselines, showing that modality-specific encoding with learnable fusion effectively exploits ssQPM's simultaneous capture of complementary illumination and phase cues for robust cell segmentation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DM-QPMNetï¼Œä¸€ç§ç”¨äºå•æ¬¡å®šé‡ç›¸ä½æ˜¾å¾®é•œ (single-shot quantitative phase microscopy, ssQPM) ç»†èƒåˆ†å‰²çš„åŒæ¨¡æ€èåˆç½‘ç»œï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿé˜ˆå€¼æ³•å¯¹å™ªå£°æ•æ„Ÿä»¥åŠæ·±åº¦å­¦ä¹ ä¸­ç®€å•é€šé“æ‹¼æ¥æ— æ³•æœ‰æ•ˆåˆ©ç”¨æ¨¡æ€äº’è¡¥æ€§çš„æŒ‘æˆ˜ã€‚è¯¥æ¶æ„é‡‡ç”¨åŒç¼–ç å™¨ (dual-encoder) ç»“æ„ï¼Œå°†åæŒ¯å¼ºåº¦å›¾åƒ (polarized intensity images) å’Œç›¸ä½å›¾ (phase maps) ä½œä¸ºç‹¬ç«‹æ¨¡æ€åœ¨ä¸åŒçš„ç¼–ç æµä¸­åˆ†åˆ«å¤„ç†ã€‚é€šè¿‡åœ¨ä¸­é—´æ·±åº¦å¼•å…¥å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ (multi-head attention)ï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿå®ç°å†…å®¹æ„ŸçŸ¥ç‰¹å¾èåˆï¼Œä½¿åæŒ¯è¾¹ç¼˜å’Œçº¹ç†ç‰¹å¾æœ‰æ•ˆåœ°æ•´åˆäº’è¡¥çš„ç›¸ä½ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒDM-QPMNet åˆ©ç”¨åŒæºè·³è·ƒè¿æ¥ (dual-source skip connections) å’Œæ¨¡æ€å½’ä¸€åŒ– (per-modality normalization) ç¡®ä¿äº†è®­ç»ƒçš„ç¨³å®šæ€§å¹¶é™ä½äº†è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå•æ¨¡æ€åŸºçº¿å’Œç®€å•çš„é€šé“ä¸²è”æ–¹æ³•ï¼Œå……åˆ†è¯æ˜äº†æ¨¡æ€ç‰¹å®šç¼–ç ä¸å¯å­¦ä¹ èåˆåœ¨åˆ©ç”¨ ssQPM ç‰©ç†ç‰¹æ€§è¿›è¡Œç¨³å¥ç»†èƒåˆ†å‰²æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.00218v1",
      "published_date": "2025-10-31 19:31:41 UTC",
      "updated_date": "2025-10-31 19:31:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:52:52.377278+00:00"
    },
    {
      "arxiv_id": "2511.00211v1",
      "title": "An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals",
      "title_zh": "é¢å‘åœ°é¢ç»ˆç«¯å¤©æ°”çŠ¶å†µæ£€æµ‹çš„é«˜æ•ˆå¯æ³›åŒ–è¿ç§»å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Wenxuan Zhang",
        "Peng Hu"
      ],
      "abstract": "The increasing adoption of satellite Internet with low-Earth-orbit (LEO) satellites in mega-constellations allows ubiquitous connectivity to rural and remote areas. However, weather events have a significant impact on the performance and reliability of satellite Internet. Adverse weather events such as snow and rain can disturb the performance and operations of satellite Internet's essential ground terminal components, such as satellite antennas, significantly disrupting the space-ground link conditions between LEO satellites and ground stations. This challenge calls for not only region-based weather forecasts but also fine-grained detection capability on ground terminal components of fine-grained weather conditions. Such a capability can assist in fault diagnostics and mitigation for reliable satellite Internet, but its solutions are lacking, not to mention the effectiveness and generalization that are essential in real-world deployments. This paper discusses an efficient transfer learning (TL) method that can enable a ground component to locally detect representative weather-related conditions. The proposed method can detect snow, wet, and other conditions resulting from adverse and typical weather events and shows superior performance compared to the typical deep learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL method also shows the advantage of being generalizable to various scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½è½¨å«æ˜Ÿ(LEO)äº’è”ç½‘åœ°é¢ç»ˆç«¯æ˜“å—é™é›ªã€é™é›¨ç­‰æ¶åŠ£å¤©æ°”å¹²æ‰°çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”æ³›åŒ–æ€§å¼ºçš„è¿ç§»å­¦ä¹ (Transfer Learning)æ–¹æ³•ï¼Œç”¨äºå®ç°å¯¹åœ°é¢ç»„ä»¶å±€éƒ¨å¤©æ°”çŠ¶å†µçš„ç²¾ç»†åŒ–æ£€æµ‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«é™é›ª(snow)ã€æ½®æ¹¿(wet)ä»¥åŠå…¶ä»–å…¸å‹å¤©æ°”äº‹ä»¶å¯¼è‡´çš„åœ°é¢ç¯å¢ƒå˜åŒ–ï¼Œä»è€Œä¸ºå«æ˜Ÿäº’è”ç½‘çš„æ•…éšœè¯Šæ–­å’Œç³»ç»Ÿå‡ç¾æä¾›æ”¯æŒã€‚å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºYOLOv7ã€YOLOv9ã€Faster R-CNNå’ŒR-YOLOç­‰ä¸»æµæ·±åº¦å­¦ä¹ (Deep Learning)æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆå±•ç°å‡ºäº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹çœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­å„ç§å¤æ‚çš„åº”ç”¨åœºæ™¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00211v1",
      "published_date": "2025-10-31 19:14:24 UTC",
      "updated_date": "2025-10-31 19:14:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:03.683989+00:00"
    },
    {
      "arxiv_id": "2511.00209v2",
      "title": "Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides",
      "title_zh": "è¯ç‰©å‘ç°å‰æ²¿çš„æ‰©æ•£æ¨¡å‹ï¼šå°åˆ†å­ä¸æ²»ç–—æ€§å¤šè‚½ç”Ÿæˆç»¼è¿°",
      "authors": [
        "Yiquan Wang",
        "Yahui Ma",
        "Yuhan Chang",
        "Jiayao Yan",
        "Jialin Zhang",
        "Minnuo Cai",
        "Kai Wei"
      ],
      "abstract": "Diffusion models have emerged as a leading framework in generative modeling, poised to transform the traditionally slow and costly process of drug discovery. This review provides a systematic comparison of their application in designing two principal therapeutic modalities: small molecules and therapeutic peptides. We dissect how the unified framework of iterative denoising is adapted to the distinct molecular representations, chemical spaces, and design objectives of each modality. For small molecules, these models excel at structure-based design, generating novel, pocket-fitting ligands with desired physicochemical properties, yet face the critical hurdle of ensuring chemical synthesizability. Conversely, for therapeutic peptides, the focus shifts to generating functional sequences and designing de novo structures, where the primary challenges are achieving biological stability against proteolysis, ensuring proper folding, and minimizing immunogenicity. Despite these distinct challenges, both domains face shared hurdles: the scarcity of high-quality experimental data, the reliance on inaccurate scoring functions for validation, and the crucial need for experimental validation. We conclude that the full potential of diffusion models will be unlocked by bridging these modality-specific gaps and integrating them into automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby shifting the paradigm from mere chemical exploration to the on-demand engineering of novel~therapeutics.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°ç³»ç»Ÿåœ°æ¯”è¾ƒäº† Diffusion Models åœ¨å°åˆ†å­ (small molecules) ä¸æ²»ç–—æ€§è‚½ (therapeutic peptides) ä¸¤ç§ä¸»è¦è¯ç‰©æ¨¡æ€ä¸­çš„åº”ç”¨ã€‚æ–‡ç« å‰–æäº†è¿­ä»£å»å™ª (iterative denoising) ç»Ÿä¸€æ¡†æ¶å¦‚ä½•é€‚é…ä¸åŒæ¨¡æ€çš„åˆ†å­è¡¨å¾ã€åŒ–å­¦ç©ºé—´åŠè®¾è®¡ç›®æ ‡ã€‚åœ¨å°åˆ†å­é¢†åŸŸï¼Œè¯¥ç±»æ¨¡å‹æ“…é•¿åŸºäºç»“æ„çš„è®¾è®¡ï¼Œæ—¨åœ¨ç”Ÿæˆæ‹Ÿåˆå—ä½“å£è¢‹ä¸”å…·å¤‡ç‰¹å®šç†åŒ–æ€§è´¨çš„é…ä½“ï¼Œä½†é¢ä¸´åŒ–å­¦å¯åˆæˆæ€§çš„æ ¸å¿ƒç“¶é¢ˆã€‚è€Œåœ¨æ²»ç–—æ€§è‚½ç ”ç©¶ä¸­ï¼Œé‡ç‚¹åœ¨äºåºåˆ—ç”Ÿæˆä¸ä»å¤´è®¾è®¡ (de novo design)ï¼Œä¸»è¦æŒ‘æˆ˜æ¶‰åŠç”Ÿç‰©ç¨³å®šæ€§ã€è›‹ç™½è´¨æŠ˜å åŠå…ç–«åŸæ€§ã€‚å°½ç®¡é¢ä¸´å·®å¼‚åŒ–æŒ‘æˆ˜ï¼Œä¸¤ç±»ç ”ç©¶å‡å—é™äºé«˜è´¨é‡å®éªŒæ•°æ®åŒ®ä¹ã€è¯„åˆ†å‡½æ•°ä¸å‡†ç¡®åŠå¯¹å®éªŒéªŒè¯çš„åˆšæ€§éœ€æ±‚ã€‚ç ”ç©¶æœ€åå¼ºè°ƒï¼Œé€šè¿‡å¼¥åˆæ¨¡æ€é—´çš„ç‰¹å®šå·®è·å¹¶å°†å…¶èå…¥è‡ªåŠ¨åŒ–çš„é—­ç¯ Design-Build-Test-Learn (DBTL) å¹³å°ï¼ŒDiffusion Models å°†æ¨åŠ¨è¯ç‰©ç ”å‘ä»ä¼ ç»Ÿçš„åŒ–å­¦æ¢ç´¢è½¬å‘æŒ‰éœ€å®šåˆ¶çš„æ²»ç–—è¯ç‰©å·¥ç¨‹åŒ–èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Biology",
      "pdf_url": "https://arxiv.org/pdf/2511.00209v2",
      "published_date": "2025-10-31 19:11:41 UTC",
      "updated_date": "2025-11-26 17:21:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:04.776836+00:00"
    },
    {
      "arxiv_id": "2511.00206v1",
      "title": "Advancing Cognitive Science with LLMs",
      "title_zh": "åˆ©ç”¨LLMsæ¨è¿›è®¤çŸ¥ç§‘å­¦",
      "authors": [
        "Dirk U. Wulff",
        "Rui Mata"
      ],
      "abstract": "Cognitive science faces ongoing challenges in knowledge synthesis and conceptual clarity, in part due to its multifaceted and interdisciplinary nature. Recent advances in artificial intelligence, particularly the development of large language models (LLMs), offer tools that may help to address these issues. This review examines how LLMs can support areas where the field has historically struggled, including establishing cross-disciplinary connections, formalizing theories, developing clear measurement taxonomies, achieving generalizability through integrated modeling frameworks, and capturing contextual and individual variation. We outline the current capabilities and limitations of LLMs in these domains, including potential pitfalls. Taken together, we conclude that LLMs can serve as tools for a more integrative and cumulative cognitive science when used judiciously to complement, rather than replace, human expertise.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨åº”å¯¹è®¤çŸ¥ç§‘å­¦(Cognitive Science)é¢†åŸŸå†…çŸ¥è¯†åˆæˆ(Knowledge Synthesis)å’Œæ¦‚å¿µæ¸…æ™°æ€§(Conceptual Clarity)æŒ‘æˆ˜ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚æ–‡ç« è¯¦ç»†åˆ†æäº†LLMså¦‚ä½•è¾…åŠ©å»ºç«‹è·¨å­¦ç§‘è”ç³»ã€ä¿ƒè¿›ç†è®ºå½¢å¼åŒ–(Formalizing Theories)ã€å¼€å‘æµ‹é‡åˆ†ç±»æ³•(Measurement Taxonomies)ä»¥åŠé€šè¿‡é›†æˆå»ºæ¨¡æ¡†æ¶(Integrated Modeling Frameworks)æå‡ç ”ç©¶çš„é€šç”¨æ€§ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜æ·±å…¥è€ƒé‡äº†æ¨¡å‹åœ¨æ•æ‰è¯­å¢ƒå’Œä¸ªä½“å·®å¼‚(Contextual and Individual Variation)æ–¹é¢çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œä½œè€…ç³»ç»Ÿæ€§åœ°æ¢³ç†äº†LLMsåœ¨ä¸Šè¿°é¢†åŸŸçš„å½“å‰èƒ½åŠ›ã€å±€é™æ€§ä»¥åŠåº”ç”¨è¿‡ç¨‹ä¸­çš„æ½œåœ¨é™·é˜±ã€‚ç»“è®ºæŒ‡å‡ºï¼Œå½“LLMsè¢«å®¡æ…åœ°ç”¨äºè¡¥å……è€Œéæ›¿ä»£äººç±»ä¸“ä¸šçŸ¥è¯†æ—¶ï¼Œå®ƒä»¬èƒ½ä¸ºæ„å»ºæ›´å…·æ•´åˆæ€§å’Œç´¯ç§¯æ€§çš„è®¤çŸ¥ç§‘å­¦æä¾›å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00206v1",
      "published_date": "2025-10-31 19:08:48 UTC",
      "updated_date": "2025-10-31 19:08:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:07.468291+00:00"
    },
    {
      "arxiv_id": "2511.00198v1",
      "title": "Training LLMs Beyond Next Token Prediction -- Filling the Mutual Information Gap",
      "title_zh": "è¶…è¶Šä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒâ€”â€”å¼¥è¡¥äº’ä¿¡æ¯é¸¿æ²Ÿ",
      "authors": [
        "Chun-Hao Yang",
        "Bo-Han Feng",
        "Tzu-Yuan Lai",
        "Yan Yu Chen",
        "Yin-Kai Dean Huang",
        "Shou-De Lin"
      ],
      "abstract": "Optimizing training performance in large language models (LLMs) remains an essential challenge, particularly in improving model performance while maintaining computational costs. This work challenges the conventional approach of training LLMs using next-token prediction (NTP), arguing that by predicting information-rich tokens during training, there is a more effective way to train LLMs. We investigate the impact of the proposed solution in three kinds of tasks for LLMs: arithmetic, multi-label classification of text, and natural-language generation. This work offers a principled approach to optimizing LLM training, advancing both model performance and theoretical understanding of the target-token selection strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‘æˆ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¼ ç»Ÿçš„æ¬¡æ ‡è®°é¢„æµ‹ï¼ˆNext-Token Prediction, NTPï¼‰è®­ç»ƒæ–¹æ³•ï¼Œæå‡ºé€šè¿‡é¢„æµ‹ä¿¡æ¯ä¸°å¯Œçš„æ ‡è®°ï¼ˆInformation-rich tokensï¼‰æ¥å¡«è¡¥äº’ä¿¡æ¯ç¼ºå£ï¼ˆMutual Information Gapï¼‰ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„æ¨¡å‹è®­ç»ƒã€‚ç ”ç©¶è€…æ·±å…¥æ¢è®¨äº†è¯¥æ–¹æ¡ˆåœ¨ç®—æœ¯è¿ç®—ã€æ–‡æœ¬å¤šæ ‡ç­¾åˆ†ç±»ä»¥åŠè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰ä¸‰ç±»ä»»åŠ¡ä¸­çš„å®é™…å½±å“ã€‚è¿™é¡¹å·¥ä½œä¸ºä¼˜åŒ–LLMè®­ç»ƒæä¾›äº†ä¸€ç§åŸåˆ™æ€§æ–¹æ³•ï¼Œä¸ä»…æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè¿˜æ·±åŒ–äº†å­¦æœ¯ç•Œå¯¹ç›®æ ‡æ ‡è®°é€‰æ‹©ç­–ç•¥ï¼ˆTarget-token selection strategiesï¼‰çš„ç†è®ºç†è§£ã€‚è¯¥ç ”ç©¶åœ¨ç»´æŒè®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¸ºè§£å†³LLMè®­ç»ƒæ•ˆç‡ä¼˜åŒ–é—®é¢˜æä¾›äº†ç§‘å­¦çš„æŒ‡å¯¼è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00198v1",
      "published_date": "2025-10-31 18:59:29 UTC",
      "updated_date": "2025-10-31 18:59:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:07.878993+00:00"
    },
    {
      "arxiv_id": "2511.00197v1",
      "title": "Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories",
      "title_zh": "ç†è§£ä»£ç æ™ºèƒ½ä½“è¡Œä¸ºï¼šæˆåŠŸä¸å¤±è´¥è½¨è¿¹çš„å®è¯ç ”ç©¶",
      "authors": [
        "Oorja Majgaonkar",
        "Zhiwei Fei",
        "Xiang Li",
        "Federica Sarro",
        "He Ye"
      ],
      "abstract": "The increasing deployment of Large Language Model (LLM) agents for complex software engineering tasks has created a need to understand their problem-solving behaviours beyond simple success metrics. While these agents demonstrate impressive capabilities in automated issue resolution, their decision-making processes remain largely opaque. This paper presents an empirical study of agent trajectories, namely the execution traces capturing the steps agents take when attempting to resolve software issues. We analyse trajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and Prometheus) on the SWE-Bench benchmark, examining both successful and failed attempts. Our investigation reveals several key insights into agent behaviour. First, we identify how distinct problem-solving strategies, such as defensive programming and context gathering, enable success in different scenarios. Second, we find that failed trajectories are consistently longer and exhibit higher variance than successful ones, with failure patterns differing significantly between agents. Third, our fault localisation analysis shows that while most trajectories correctly identify problematic files (72-81\\% even in failures), success depends more on achieving approximate rather than exact code modifications. These and other findings unveiled by our study, provide a foundation for understanding agent behaviour through trajectory analysis, contributing to the development of more robust and interpretable autonomous software engineering systems.",
      "tldr_zh": "è¯¥é¡¹ç»éªŒç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLM)ä»£ç æ™ºèƒ½ä½“åœ¨å¤„ç†å¤æ‚è½¯ä»¶å·¥ç¨‹ä»»åŠ¡æ—¶çš„è¡Œä¸ºæ¨¡å¼ï¼Œé€šè¿‡åˆ†æOpenHandsã€SWE-agentå’ŒPrometheusä¸‰ä¸ªSOTAæ™ºèƒ½ä½“åœ¨SWE-BenchåŸºå‡†ä¸Šçš„æ‰§è¡Œè½¨è¿¹(trajectories)æ¥æ­ç¤ºå…¶å†³ç­–è¿‡ç¨‹ã€‚ç ”ç©¶å‘ç°ï¼Œé˜²å¾¡æ€§ç¼–ç¨‹(defensive programming)å’Œä¸Šä¸‹æ–‡æ”¶é›†(context gathering)ç­‰ä¸åŒçš„é—®é¢˜è§£å†³ç­–ç•¥æ˜¯æ™ºèƒ½ä½“åœ¨ä¸åŒåœºæ™¯ä¸‹å–å¾—æˆåŠŸçš„å…³é”®ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œå¤±è´¥è½¨è¿¹(failed trajectories)é€šå¸¸æ¯”æˆåŠŸè½¨è¿¹æ›´é•¿ä¸”å…·æœ‰æ›´é«˜çš„æ–¹å·®ï¼Œä¸”ä¸åŒæ™ºèƒ½ä½“ä¹‹é—´çš„å¤±è´¥æ¨¡å¼å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åœ¨æ•…éšœå®šä½(fault localisation)æ–¹é¢ï¼Œå°½ç®¡72-81%çš„è½¨è¿¹èƒ½å‡†ç¡®è¯†åˆ«å‡ºé—®é¢˜æ–‡ä»¶ï¼Œä½†æœ€ç»ˆçš„æˆåŠŸæ›´å¤šå–å†³äºå®ç°è¿‘ä¼¼è€Œéç²¾ç¡®çš„ä»£ç ä¿®æ”¹ã€‚è¯¥ç ”ç©¶é€šè¿‡è½¨è¿¹åˆ†æä¸ºç†è§£æ™ºèƒ½ä½“è¡Œä¸ºå¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œä¸ºæœªæ¥å¼€å‘æ›´å…·é²æ£’æ€§å’Œå¯è§£é‡Šæ€§çš„è‡ªä¸»è½¯ä»¶å·¥ç¨‹ç³»ç»Ÿæä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00197v1",
      "published_date": "2025-10-31 18:58:13 UTC",
      "updated_date": "2025-10-31 18:58:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:10.279358+00:00"
    },
    {
      "arxiv_id": "2511.00194v1",
      "title": "Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures",
      "title_zh": "è¿‡æ»¤æ€§æœ€å¼ºçŒœæƒ³çš„å¢é‡å¼é€‰æ‹©åŠå…¶è¯æ˜",
      "authors": [
        "Jovial Cheukam Ngouonou",
        "Ramiz Gindullin",
        "Claude-Guy Quimper",
        "Nicolas Beldiceanu",
        "Remi Douence"
      ],
      "abstract": "We present an improved incremental selection algorithm of the selection algorithm presented in [1] and prove all the selected conjectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å¢é‡é€‰æ‹©ç®—æ³•(incremental selection algorithm)ï¼Œæ˜¯å¯¹å…ˆå‰æ–‡çŒ®ä¸­æå‡ºçš„é€‰æ‹©ç®—æ³•çš„è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚è¯¥ç®—æ³•çš„æ ¸å¿ƒç›®æ ‡æ˜¯å®ç°å¯¹æœ€å…·è¿‡æ»¤æ€§(most-filtering)çŒœæƒ³çš„å¢é‡ç­›é€‰ï¼Œä»¥æå‡å¤„ç†å¤æ‚é€»è¾‘å‘½é¢˜çš„æ•ˆç‡ã€‚é™¤äº†å¯¹ç®—æ³•æœ¬èº«è¿›è¡Œæ”¹è¿›ï¼Œè®ºæ–‡è¿˜å®Œæˆäº†å¯¹æ‰€æœ‰è¢«é€‰ä¸­çŒœæƒ³(selected conjectures)çš„ä¸¥æ ¼è¯æ˜ï¼ŒéªŒè¯äº†è¯¥é€‰æ‹©æœºåˆ¶åœ¨å®é™…è¯æ˜ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥é¡¹å·¥ä½œä¸ºè‡ªåŠ¨åŒ–æ¨ç†é¢†åŸŸä¸­çš„çŒœæƒ³ç­›é€‰ä¸è¯æ˜ç”Ÿæˆæä¾›äº†æ›´å…·é²æ£’æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00194v1",
      "published_date": "2025-10-31 18:51:08 UTC",
      "updated_date": "2025-10-31 18:51:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:24.957553+00:00"
    },
    {
      "arxiv_id": "2511.00192v1",
      "title": "EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs",
      "title_zh": "EL-MIAï¼šé‡åŒ–å¤§è¯­è¨€æ¨¡å‹ä¸­æ•æ„Ÿå®ä½“çš„æˆå‘˜æ¨ç†é£é™©",
      "authors": [
        "Ali Satvaty",
        "Suzan Verberne",
        "Fatih Turkmen"
      ],
      "abstract": "Membership inference attacks (MIA) aim to infer whether a particular data point is part of the training dataset of a model. In this paper, we propose a new task in the context of LLM privacy: entity-level discovery of membership risk focused on sensitive information (PII, credit card numbers, etc). Existing methods for MIA can detect the presence of entire prompts or documents in the LLM training data, but they fail to capture risks at a finer granularity. We propose the ``EL-MIA'' framework for auditing entity-level membership risks in LLMs. We construct a benchmark dataset for the evaluation of MIA methods on this task. Using this benchmark, we conduct a systematic comparison of existing MIA techniques as well as two newly proposed methods. We provide a comprehensive analysis of the results, trying to explain the relation of the entity level MIA susceptability with the model scale, training epochs, and other surface level factors. Our findings reveal that existing MIA methods are limited when it comes to entity-level membership inference of the sensitive attributes, while this susceptibility can be outlined with relatively straightforward methods, highlighting the need for stronger adversaries to stress test the provided threat model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EL-MIA æ¡†æ¶ï¼Œæ—¨åœ¨é‡åŒ–å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸­æ•æ„Ÿå®ä½“ï¼ˆå¦‚ PIIã€ä¿¡ç”¨å¡å·ç­‰ï¼‰çš„æˆå‘˜æ¨ç†æ”»å‡» (Membership Inference Attacks, MIA) é£é™©ã€‚ç°æœ‰çš„ MIA æ–¹æ³•ä¸»è¦é’ˆå¯¹æ•´ä¸ªæç¤ºæˆ–æ–‡æ¡£è¿›è¡Œæ£€æµ‹ï¼Œéš¾ä»¥æ•æ‰æ›´ç»†ç²’åº¦çš„å®ä½“çº§é£é™©ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è¯¥ä»»åŠ¡çš„åŸºå‡†æ•°æ®é›†ï¼Œå¹¶å¯¹ç°æœ‰æŠ€æœ¯ä»¥åŠä¸¤ç§æ–°æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿæ€§æ¯”è¾ƒã€‚ç ”ç©¶è¿›ä¸€æ­¥æ·±å…¥åˆ†æäº†å®ä½“çº§ MIA æ•æ„Ÿæ€§ä¸æ¨¡å‹è§„æ¨¡ã€è®­ç»ƒè½®æ¬¡åŠå…¶ä»–è¡¨é¢ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰ MIA æ–¹æ³•åœ¨å¤„ç†æ•æ„Ÿå±æ€§çš„å®ä½“çº§æ¨ç†æ—¶å­˜åœ¨å±€é™æ€§ï¼Œä½†è¿™ç§é£é™©å¯ä»¥é€šè¿‡ç›¸å¯¹ç®€å•çš„æ–¹æ³•è¢«è¯†åˆ«å‡ºæ¥ã€‚è¯¥å‘ç°å¼ºè°ƒäº†å¼€å‘æ›´å¼ºå¯¹æŠ—æ‰‹æ®µä»¥å¯¹ç°æœ‰å¨èƒæ¨¡å‹è¿›è¡Œå‹åŠ›æµ‹è¯•çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00192v1",
      "published_date": "2025-10-31 18:50:47 UTC",
      "updated_date": "2025-10-31 18:50:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:23.577243+00:00"
    },
    {
      "arxiv_id": "2511.00191v1",
      "title": "A Retrospect to Multi-prompt Learning across Vision and Language",
      "title_zh": "è§†è§‰ä¸è¯­è¨€å¤šæç¤ºå­¦ä¹ çš„å›é¡¾",
      "authors": [
        "Ziliang Chen",
        "Xin Huang",
        "Quanlong Guan",
        "Liang Lin",
        "Weiqi Luo"
      ],
      "abstract": "The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹(VLMs)åœ¨ä¸‹æ¸¸ä»»åŠ¡é€‚é…ä¸­çš„æç¤ºå­¦ä¹ (Prompt Learning)è¿›è¡Œäº†ç³»ç»Ÿæ€§å›é¡¾ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶å¤šèšç„¦äºå•æç¤ºèŒƒå¼è€Œå¿½è§†å¤šæç¤ºå­¦ä¹ (Multi-prompt Learning)æ½œåŠ›çš„ç©ºç™½ã€‚ç ”ç©¶è€…é€šè¿‡å°†æ¨¡æ€é—´éš™(Modality Gap)ç°è±¡æ‰©å±•è‡³å¯å­¦ä¹ æç¤ºï¼Œä»ç†è®ºå’Œç»éªŒå±‚é¢è®ºè¯äº†å¤šæç¤ºå¢å¼ºåœ¨è§†è§‰è¯­è¨€è¿ç§»ä¸­çš„ä¼˜è¶Šæ€§ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºèƒ½é‡çš„å¤šæç¤ºå­¦ä¹ (Energy-based Multi-prompt Learning, EMPL)æ–¹æ³•ï¼Œé€šè¿‡ä»VLMséšå¼å®šä¹‰çš„èƒ½é‡åˆ†å¸ƒä¸­é‡‡æ ·æ¥ç”Ÿæˆå¤šä¸ªæç¤ºåµŒå…¥ã€‚è¯¥æ–¹æ³•ä¸ä»…åœ¨å‚æ•°ä½¿ç”¨ä¸Šå…·æœ‰æé«˜çš„æ•ˆç‡ï¼Œè¿˜æˆåŠŸåœ¨åŸŸå†…(In-domain)å’ŒåŸŸå¤–(Out-of-domain)å¼€æ”¾è¯æ±‡æ³›åŒ–æ€§èƒ½ä¹‹é—´è¾¾æˆäº†å¹³è¡¡ã€‚é€šè¿‡å…¨é¢çš„å®éªŒæµ‹è¯•ï¼Œè¯¥ç ”ç©¶è¯å®äº†æ‰€æç†è®ºçš„æ­£ç¡®æ€§ä»¥åŠEMPLæ¡†æ¶åœ¨å®é™…åº”ç”¨ä¸­çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV",
      "pdf_url": "https://arxiv.org/pdf/2511.00191v1",
      "published_date": "2025-10-31 18:50:35 UTC",
      "updated_date": "2025-10-31 18:50:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:32.361323+00:00"
    },
    {
      "arxiv_id": "2511.04697v1",
      "title": "Simulating Misinformation Vulnerabilities With Agent Personas",
      "title_zh": "åˆ©ç”¨æ™ºèƒ½ä½“äººæ ¼æ¨¡æ‹Ÿè™šå‡ä¿¡æ¯è„†å¼±æ€§",
      "authors": [
        "David Farr",
        "Lynnette Hui Xian Ng",
        "Stephen Prochaska",
        "Iain J. Cruickshank",
        "Jevin West"
      ],
      "abstract": "Disinformation campaigns can distort public perception and destabilize institutions. Understanding how different populations respond to information is crucial for designing effective interventions, yet real-world experimentation is impractical and ethically challenging. To address this, we develop an agent-based simulation using Large Language Models (LLMs) to model responses to misinformation. We construct agent personas spanning five professions and three mental schemas, and evaluate their reactions to news headlines. Our findings show that LLM-generated agents align closely with ground-truth labels and human predictions, supporting their use as proxies for studying information responses. We also find that mental schemas, more than professional background, influence how agents interpret misinformation. This work provides a validation of LLMs to be used as agents in an agent-based model of an information network for analyzing trust, polarization, and susceptibility to deceptive content in complex social systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ™ºèƒ½ä½“æ¨¡æ‹Ÿ(Agent-based simulation)æ¡†æ¶ï¼Œæ—¨åœ¨æ¢ç´¢ä¸åŒäººç¾¤å¯¹è™šå‡ä¿¡æ¯(Misinformation)çš„å“åº”æœºåˆ¶ã€‚ç ”ç©¶é€šè¿‡æ„å»ºæ¶µç›–äº”ç§èŒä¸šå’Œä¸‰ç§å¿ƒç†å›¾å¼(Mental schemas)çš„æ™ºèƒ½ä½“äººæ ¼(Agent personas)ï¼Œç³»ç»Ÿè¯„ä¼°äº†å®ƒä»¬å¯¹æ–°é—»æ ‡é¢˜çš„ååº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMç”Ÿæˆçš„æ™ºèƒ½ä½“åœ¨ååº”ä¸Šä¸çœŸå®æ ‡ç­¾(Ground-truth)å’Œäººç±»é¢„æµ‹é«˜åº¦ä¸€è‡´ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºä¿¡æ¯å“åº”ç ”ç©¶ä»£ç†å·¥å…·çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå¿ƒç†å›¾å¼(Mental schemas)å¯¹æ™ºèƒ½ä½“å¦‚ä½•è§£è¯»è™šå‡ä¿¡æ¯çš„å½±å“åŠ›è¿œè¶…å…¶èŒä¸šèƒŒæ™¯ã€‚è¯¥é¡¹å·¥ä½œè¯æ˜äº†å°†LLMsä½œä¸ºæ™ºèƒ½ä½“åº”ç”¨äºä¿¡æ¯ç½‘ç»œæ¨¡å‹çš„å¯è¡Œæ€§ï¼Œä¸ºåˆ†æå¤æ‚ç¤¾ä¼šç³»ç»Ÿä¸­çš„ä¿¡ä»»ã€æåŒ–(Polarization)ä»¥åŠå¯¹æ¬ºéª—æ€§å†…å®¹çš„æ˜“æ„Ÿæ€§æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted to Winter Simulation Conference 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.04697v1",
      "published_date": "2025-10-31 18:44:00 UTC",
      "updated_date": "2025-10-31 18:44:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:32.666668+00:00"
    },
    {
      "arxiv_id": "2511.00179v1",
      "title": "Generative Modeling Enables Molecular Structure Retrieval from Coulomb Explosion Imaging",
      "title_zh": "ç”Ÿæˆå¼å»ºæ¨¡åŠ©åŠ›ä»åº“ä»‘çˆ†ç‚¸æˆåƒä¸­æ£€ç´¢åˆ†å­ç»“æ„",
      "authors": [
        "Xiang Li",
        "Till Jahnke",
        "Rebecca Boll",
        "Jiaqi Han",
        "Minkai Xu",
        "Michael Meyer",
        "Maria Novella Piancastelli",
        "Daniel Rolles",
        "Artem Rudenko",
        "Florian Trinter",
        "Thomas J. A. Wolf",
        "Jana B. Thayer",
        "James P. Cryan",
        "Stefano Ermon",
        "Phay J. Ho"
      ],
      "abstract": "Capturing the structural changes that molecules undergo during chemical reactions in real space and time is a long-standing dream and an essential prerequisite for understanding and ultimately controlling femtochemistry. A key approach to tackle this challenging task is Coulomb explosion imaging, which benefited decisively from recently emerging high-repetition-rate X-ray free-electron laser sources. With this technique, information on the molecular structure is inferred from the momentum distributions of the ions produced by the rapid Coulomb explosion of molecules. Retrieving molecular structures from these distributions poses a highly non-linear inverse problem that remains unsolved for molecules consisting of more than a few atoms. Here, we address this challenge using a diffusion-based Transformer neural network. We show that the network reconstructs unknown molecular geometries from ion-momentum distributions with a mean absolute error below one Bohr radius, which is half the length of a typical chemical bond.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®æ—¶æ•æ‰åˆ†å­ç»“æ„å˜åŒ–çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£(diffusion-based)çš„Transformerç¥ç»ç½‘ç»œï¼Œç”¨äºä»åº“ä»‘çˆ†ç‚¸æˆåƒ(Coulomb explosion imaging)æ•°æ®ä¸­æ£€ç´¢åˆ†å­ç»“æ„ã€‚åº“ä»‘çˆ†ç‚¸æˆåƒé€šè¿‡åˆ†æåˆ†å­çˆ†ç‚¸äº§ç”Ÿçš„ç¦»å­åŠ¨é‡åˆ†å¸ƒæ¥æ¨æ–­å…¶ç»“æ„ï¼Œä½†ç”±äºè¯¥è¿‡ç¨‹æ¶‰åŠé«˜åº¦éçº¿æ€§çš„é€†é—®é¢˜ï¼Œæ­¤å‰å¯¹äºåŒ…å«å¤šä¸ªåŸå­çš„å¤æ‚åˆ†å­ä¸€ç›´éš¾ä»¥å®ç°æœ‰æ•ˆåæ¼”ã€‚é€šè¿‡ç»“åˆæ‰©æ•£æ¨¡å‹ä¸Transformeræ¶æ„ï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿé«˜æ•ˆåœ°ä»ç¦»å­åŠ¨é‡åˆ†å¸ƒä¸­é‡å»ºæœªçŸ¥çš„åˆ†å­å‡ ä½•æ„å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹é‡å»ºç»“æ„çš„å¹³å‡ç»å¯¹è¯¯å·®ä½äºä¸€ä¸ªç»å°”åŠå¾„(Bohr radius)ï¼Œç²¾åº¦è¾¾åˆ°äº†å…¸å‹åŒ–å­¦é”®é•¿åº¦çš„ä¸€åŠã€‚è¿™ä¸€è¿›å±•ä¸ºåœ¨åŸå­æ—¶ç©ºå°ºåº¦ä¸Šç†è§£å¹¶æœ€ç»ˆæ§åˆ¶é£ç§’åŒ–å­¦(femtochemistry)è¿‡ç¨‹æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ï¼Œè¯æ˜äº†ç”Ÿæˆå¼å»ºæ¨¡åœ¨è§£å†³å¤æ‚ç‰©ç†é€†é—®é¢˜ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00179v1",
      "published_date": "2025-10-31 18:33:40 UTC",
      "updated_date": "2025-10-31 18:33:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:35.281424+00:00"
    },
    {
      "arxiv_id": "2511.00176v1",
      "title": "Effectiveness of LLMs in Temporal User Profiling for Recommendation",
      "title_zh": "LLMs åœ¨é¢å‘æ¨èçš„æ—¶åºç”¨æˆ·ç”»åƒä¸­çš„æœ‰æ•ˆæ€§",
      "authors": [
        "Milad Sabouri",
        "Masoud Mansoury",
        "Kun Lin",
        "Bamshad Mobasher"
      ],
      "abstract": "Effectively modeling the dynamic nature of user preferences is crucial for enhancing recommendation accuracy and fostering transparency in recommender systems. Traditional user profiling often overlooks the distinction between transitory short-term interests and stable long-term preferences. This paper examines the capability of leveraging Large Language Models (LLMs) to capture these temporal dynamics, generating richer user representations through distinct short-term and long-term textual summaries of interaction histories. Our observations suggest that while LLMs tend to improve recommendation quality in domains with more active user engagement, their benefits appear less pronounced in sparser environments. This disparity likely stems from the varying distinguishability of short-term and long-term preferences across domains; the approach shows greater utility where these temporal interests are more clearly separable (e.g., Movies\\&TV) compared to domains with more stable user profiles (e.g., Video Games). This highlights a critical trade-off between enhanced performance and computational costs, suggesting context-dependent LLM application. Beyond predictive capability, this LLM-driven approach inherently provides an intrinsic potential for interpretability through its natural language profiles and attention weights. This work contributes insights into the practical capability and inherent interpretability of LLM-driven temporal user profiling, outlining new research directions for developing adaptive and transparent recommender systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ•æ‰ç”¨æˆ·åå¥½çš„åŠ¨æ€ç‰¹æ€§ï¼Œé€šè¿‡ä¸ºäº¤äº’å†å²ç”ŸæˆåŒºåˆ†çŸ­æœŸå’Œé•¿æœŸå…´è¶£çš„æ–‡æœ¬æ‘˜è¦ï¼Œæ„å»ºäº†æ›´ä¸°å¯Œçš„ç”¨æˆ·è¡¨å¾ã€‚é’ˆå¯¹ä¼ ç»Ÿç”¨æˆ·ç”»åƒå¾€å¾€å¿½è§†çŸ­æœŸç¬æ—¶å…´è¶£ä¸é•¿æœŸç¨³å®šåå¥½å·®å¼‚çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ LLMs æå‡äº†æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚å®éªŒè§‚å¯Ÿå‘ç°ï¼ŒLLMs åœ¨ç”¨æˆ·æ´»è·ƒåº¦é«˜ä¸”æ—¶åºå…´è¶£åŒºåˆ†åº¦å¤§çš„é¢†åŸŸï¼ˆå¦‚ Movies & TVï¼‰èƒ½æ˜¾è‘—æå‡æ¨èè´¨é‡ï¼Œè€Œåœ¨ç”¨æˆ·åå¥½è¾ƒç¨³å®šçš„é¢†åŸŸï¼ˆå¦‚ Video Gamesï¼‰æ•ˆæœåˆ™ä¸æ˜æ˜¾ã€‚è¿™æ­ç¤ºäº†åœ¨åº”ç”¨ LLMs è¿›è¡Œç”¨æˆ·ç”»åƒæ—¶æ€§èƒ½æå‡ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„å…³é”®æƒè¡¡ï¼Œå¼ºè°ƒäº†æ ¹æ®ä¸Šä¸‹æ–‡ (context-dependent) é€‰æ‹©åº”ç”¨åœºæ™¯çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡è‡ªç„¶è¯­è¨€ç”»åƒå’Œæ³¨æ„åŠ›æƒé‡ä¸ºæ¨èç³»ç»Ÿæä¾›äº†å†…åœ¨çš„å¯è§£é‡Šæ€§ (interpretability)ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘è‡ªé€‚åº”ä¸”é€æ˜çš„æ¨èç³»ç»Ÿæä¾›äº†å…³äº LLM é©±åŠ¨çš„æ—¶åºç”¨æˆ·ç”»åƒçš„å®è·µèƒ½åŠ›ä¸æ–°è§è§£ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted to the IEEE International Conference on Data Mining (ICDM 2025), Workshop on User Modeling and Recommendation (UMRec). To appear in the IEEE ICDMW 2025 proceedings",
      "pdf_url": "https://arxiv.org/pdf/2511.00176v1",
      "published_date": "2025-10-31 18:28:40 UTC",
      "updated_date": "2025-10-31 18:28:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:37.960409+00:00"
    },
    {
      "arxiv_id": "2511.00162v2",
      "title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus",
      "title_zh": "ARC-GENï¼šé’ˆå¯¹æŠ½è±¡ä¸æ¨ç†è¯­æ–™åº“çš„æ‹Ÿæ€è¿‡ç¨‹å¼åŸºå‡†ç”Ÿæˆå™¨",
      "authors": [
        "Michael D. Moffitt"
      ],
      "abstract": "The Abstraction and Reasoning Corpus remains one of the most compelling and challenging benchmarks for tracking progress toward achieving Artificial General Intelligence. In contrast to other evaluation datasets designed to assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI suite is specifically targeted at measuring skill acquisition efficiency, a trait that has (so far) been lacking in even the most sophisticated machine learning systems. For algorithms that require extensive intra-task exemplars, a significant constraint imposed by ARC-AGI is the modest cardinality of its demonstration set, comprising a small number of $\\langle$ input, output $\\rangle$ grids per task specifying the corresponding transformation. To embellish the space of viable sample pairs, this paper introduces ARC-GEN, an open-source procedural generator aimed at extending the original ARC-AGI training dataset as faithfully as possible. Unlike prior efforts, our generator is both exhaustive (covering all four-hundred tasks) and mimetic (more closely honoring the distributional properties and characteristics embodied in the initial ARC-AGI-1 release). We also discuss the use of this generator in establishing a static benchmark suite to verify the correctness of programs submitted to the 2025 Google Code Golf Championship.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŠ½è±¡ä¸æ¨ç†åŸºå‡†æµ‹è¯•(ARC-AGI)ä¸­æ¼”ç¤ºé›†åŸºæ•°è¾ƒå°ã€éš¾ä»¥æ»¡è¶³æ•°æ®é©±åŠ¨ç®—æ³•éœ€æ±‚çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºARC-GENçš„å¼€æºç¨‹åºåŒ–ç”Ÿæˆå™¨(procedural generator)ã€‚ARC-GENæ—¨åœ¨å¿ å®åœ°æ‰©å±•åŸå§‹ARC-AGIè®­ç»ƒé›†ï¼Œé€šè¿‡æ¨¡æ‹ŸåŸå§‹æ•°æ®çš„åˆ†å¸ƒå±æ€§å’Œç‰¹å¾ï¼Œå®ç°äº†å¯¹å…¨éƒ¨400ä¸ªä»»åŠ¡çš„å®Œæ•´è¦†ç›–ã€‚ä¸ä¹‹å‰çš„ç ”ç©¶ç›¸æ¯”ï¼Œè¯¥ç”Ÿæˆå™¨å…·æœ‰æ›´å¼ºçš„æ¨¡ä»¿æ€§(mimetic)å’Œå®Œå¤‡æ€§(exhaustive)ï¼Œèƒ½ä¸ºæ¨¡å‹æä¾›æ›´ä¸°å¯Œçš„$\\langle$input, output$\\rangle$å˜æ¢ç¤ºä¾‹ã€‚æ­¤å¤–ï¼Œä½œè€…å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¯¥ç”Ÿæˆå™¨æ„å»ºé™æ€åŸºå‡†å¥—ä»¶ï¼Œä»¥éªŒè¯2025å¹´Google Code Golf Championshipå‚èµ›ç¨‹åºçš„æ­£ç¡®æ€§ã€‚è¿™ä¸€å·¥ä½œä¸ºè¡¡é‡å’Œæå‡AIç³»ç»Ÿçš„æŠ€èƒ½ä¹ å¾—æ•ˆç‡æä¾›äº†é‡è¦æ”¯æŒï¼Œä¹Ÿä¸ºè§£å†³ARC-AGIçš„å°æ ·æœ¬å­¦ä¹ æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆçš„èµ„æºä¿éšœã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00162v2",
      "published_date": "2025-10-31 18:10:05 UTC",
      "updated_date": "2025-11-04 03:46:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:54.774183+00:00"
    },
    {
      "arxiv_id": "2511.00160v1",
      "title": "What a diff makes: automating code migration with large language models",
      "title_zh": "diff ä¹‹æ•ˆï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–ä»£ç è¿ç§»",
      "authors": [
        "Katherine A. Rosenfeld",
        "Cliff C. Kerr",
        "Jessica Lundin"
      ],
      "abstract": "Modern software programs are built on stacks that are often undergoing changes that introduce updates and improvements, but may also break any project that depends upon them. In this paper we explore the use of Large Language Models (LLMs) for code migration, specifically the problem of maintaining compatibility with a dependency as it undergoes major and minor semantic version changes. We demonstrate, using metrics such as test coverage and change comparisons, that contexts containing diffs can significantly improve performance against out of the box LLMs and, in some cases, perform better than using code. We provide a dataset to assist in further development of this problem area, as well as an open-source Python package, AIMigrate, that can be used to assist with migrating code bases. In a real-world migration of TYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of required changes in a single run, increasing to 80% with multiple runs, with 47% of changes generated perfectly.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªåŠ¨æ‰§è¡Œä»£ç è¿ç§»(code migration)çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è½¯ä»¶ä¾èµ–é¡¹åœ¨ç‰ˆæœ¬æ›´æ–°è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å…¼å®¹æ€§ç»´æŠ¤é—®é¢˜ã€‚é€šè¿‡å¯¹æ¯”æµ‹è¯•è¦†ç›–ç‡å’Œå˜æ›´æƒ…å†µï¼Œç ”ç©¶è¯æ˜åŒ…å«diffsçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç›¸æ¯”ç›´æ¥æä¾›ä»£ç èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼Œæœ‰æ•ˆé™ä½è¿ç§»éš¾åº¦ã€‚ä½œè€…ä¸ºæ­¤æä¾›äº†ä¸€ä¸ªä¸“é—¨çš„æ•°æ®é›†ä»¥åŠå¼€æºPythonåŒ…AIMigrateï¼Œç”¨äºè¾…åŠ©å¼€å‘è€…å®Œæˆè‡ªåŠ¨åŒ–ä»£ç è¿ç§»ã€‚åœ¨TYPHOIDSIMé¡¹ç›®è·¨STARSIMç‰ˆæœ¬çš„å®é™…è¿ç§»å®éªŒä¸­ï¼ŒAIMigrateåœ¨å•æ¬¡è¿è¡Œä¸­æˆåŠŸè¯†åˆ«äº†65%çš„å¿…è¦å˜æ›´ï¼Œåœ¨å¤šæ¬¡è¿è¡Œä¸‹è¯†åˆ«ç‡æå‡è‡³80%ï¼Œå…¶ä¸­47%çš„ä»£ç å˜æ›´è¾¾åˆ°äº†å®Œç¾ç”Ÿæˆçš„æ ‡å‡†ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "10 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.00160v1",
      "published_date": "2025-10-31 18:08:52 UTC",
      "updated_date": "2025-10-31 18:08:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:47.081269+00:00"
    },
    {
      "arxiv_id": "2510.27688v1",
      "title": "Continuous Autoregressive Language Models",
      "title_zh": "è¿ç»­è‡ªå›å½’è¯­è¨€æ¨¡å‹",
      "authors": [
        "Chenze Shao",
        "Darren Li",
        "Fandong Meng",
        "Jie Zhou"
      ],
      "abstract": "The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9\\% accuracy. This allows us to model language as a sequence of continuous vectors instead of discrete tokens, which reduces the number of generative steps by a factor of K. The paradigm shift necessitates a new modeling toolkit; therefore, we develop a comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models. Code: https://github.com/shaochenze/calm. Project: https://shaochenze.github.io/blog/2025/CALM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è¿ç»­è‡ªå›å½’è¯­è¨€æ¨¡å‹(Continuous Autoregressive Language Models, CALM)ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)å› é€æ ‡è®°(token-by-token)ç”Ÿæˆçš„é¡ºåºæ€§è€Œå¯¼è‡´çš„æ•ˆç‡ç“¶é¢ˆã€‚CALMå°†ä¼ ç»Ÿçš„ç¦»æ•£ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è½¬å˜ä¸ºè¿ç»­çš„ä¸‹ä¸€ä¸ªå‘é‡(next-vector)é¢„æµ‹ï¼Œé€šè¿‡é«˜ä¿çœŸåº¦è‡ªåŠ¨ç¼–ç å™¨(autoencoder)å°†Kä¸ªæ ‡è®°å‹ç¼©ä¸ºå•ä¸ªè¿ç»­å‘é‡ï¼Œå®ç°äº†è¶…è¿‡99.9%çš„é‡æ„å‡†ç¡®ç‡ã€‚è¯¥æ–¹æ³•å°†ç”Ÿæˆæ­¥æ•°å‡å°‘äº†Kå€ï¼Œå¹¶é…å¥—å¼€å‘äº†æ— ä¼¼ç„¶(likelihood-free)æ¡†æ¶ï¼Œæ”¯æŒåœ¨è¿ç»­åŸŸå†…è¿›è¡Œç¨³å¥è®­ç»ƒã€è¯„ä¼°å’Œå¯æ§é‡‡æ ·ã€‚å®éªŒè¡¨æ˜ï¼ŒCALMåœ¨æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†å¼ºåŠ›ç¦»æ•£åŸºçº¿æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†ä¸‹ä¸€ä¸ªå‘é‡é¢„æµ‹æ˜¯é€šå‘è¶…é«˜æ•ˆè¯­è¨€æ¨¡å‹çš„ä¸€æ¡å…·æœ‰æ‰©å±•æ€§çš„é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27688v1",
      "published_date": "2025-10-31 17:58:11 UTC",
      "updated_date": "2025-10-31 17:58:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:54:14.587718+00:00"
    },
    {
      "arxiv_id": "2510.27680v2",
      "title": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting",
      "title_zh": "PETARï¼šåŸºäºæ©è†œæ„ŸçŸ¥è§†è§‰è¯­è¨€å»ºæ¨¡çš„ PET è‡ªåŠ¨åŒ–æŠ¥å‘Šå±€éƒ¨å¾è±¡ç”Ÿæˆ",
      "authors": [
        "Danyal Maqbool",
        "Changhee Lee",
        "Zachary Huemann",
        "Samuel D. Church",
        "Matthew E. Larson",
        "Scott B. Perlman",
        "Tomas A. Romero",
        "Joshua D. Warner",
        "Meghan Lubner",
        "Xin Tie",
        "Jameson Merkow",
        "Junjie Hu",
        "Steve Y. Cho",
        "Tyler J. Bradshaw"
      ],
      "abstract": "Generating automated reports for 3D positron emission tomography (PET) is an important and challenging task in medical imaging. PET plays a vital role in oncology, but automating report generation is difficult due to the complexity of whole-body 3D volumes, the wide range of potential clinical findings, and the limited availability of annotated datasets. To address these challenges, we introduce PETARSeg-11K, the first large-scale, publicly available dataset that provides lesion-level correspondence between 3D PET/CT volumes and free-text radiological findings. It comprises 11,356 lesion descriptions paired with 3D segmentations. Second, we propose PETAR-4B, a 3D vision-language model designed for mask-aware, spatially grounded PET/CT reporting. PETAR-4B jointly encodes PET, CT, and 3D lesion segmentation masks, using a 3D focal prompt to capture fine-grained details of lesions that normally comprise less than 0.1% of the volume. Evaluations using automated metrics show PETAR-4B substantially outperforming all 2D and 3D baselines. A human study involving five physicians -- the first of its kind for automated PET reporting -- confirms the model's clinical utility and establishes correlations between automated metrics and expert judgment. This work provides a foundational dataset and a novel architecture, advancing 3D medical vision-language understanding in PET.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨èº«ä½“ç§¯å¤æ‚ã€ä¸´åºŠå‘ç°èŒƒå›´å¹¿ä»¥åŠæ ‡æ³¨æ•°æ®æœ‰é™å¯¼è‡´çš„3Dæ­£ç”µå­å‘å°„æ–­å±‚æ‰«æ(PET)è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆéš¾é¢˜ï¼Œæå‡ºäº†PETARæ¡†æ¶ã€‚ç ”ç©¶è€…é¦–å…ˆå‘å¸ƒäº†PETARSeg-11Kï¼Œè¿™æ˜¯é¦–ä¸ªæä¾›ç—…å˜çº§åˆ«å¯¹åº”å…³ç³»çš„å¤§è§„æ¨¡å…¬å¼€æ•°æ®é›†ï¼ŒåŒ…å«11,356ä¸ªæè¿°ä¸3Dåˆ†å‰²æ©è†œå¯¹ã€‚éšåæå‡ºäº†PETAR-4Bï¼Œä¸€ç§ä¸“é—¨ç”¨äºæ©è†œæ„ŸçŸ¥(mask-aware)å’Œç©ºé—´å®šä½PET/CTæŠ¥å‘Šç”Ÿæˆçš„3Dè§†è§‰è¯­è¨€æ¨¡å‹(VLM)ã€‚è¯¥æ¨¡å‹é€šè¿‡è”åˆç¼–ç PETã€CTå’Œ3Dç—…å˜åˆ†å‰²æ©è†œï¼Œå¹¶åˆ©ç”¨3Dç„¦ç‚¹æç¤º(3D focal prompt)æ•æ‰ä½“ç§¯å æ¯”æå°çš„ç—…å˜ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPETAR-4Båœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„2Då’Œ3DåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸€é¡¹æ¶‰åŠäº”ä½åŒ»å¸ˆçš„äººä½“ç ”ç©¶è¯å®äº†è¯¥æ¨¡å‹çš„ä¸´åºŠå®ç”¨æ€§ï¼Œå¹¶ç¡®ç«‹äº†è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸ä¸“å®¶åˆ¤æ–­ä¹‹é—´çš„ç›¸å…³æ€§ã€‚è¯¥å·¥ä½œä¸ºPETé¢†åŸŸçš„3DåŒ»å­¦è§†è§‰è¯­è¨€ç†è§£æä¾›äº†åŸºç¡€æ•°æ®é›†å’Œåˆ›æ–°æ¶æ„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27680v2",
      "published_date": "2025-10-31 17:49:01 UTC",
      "updated_date": "2025-11-30 23:13:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:53:50.985123+00:00"
    },
    {
      "arxiv_id": "2510.27671v1",
      "title": "MolChord: Structure-Sequence Alignment for Protein-Guided Drug Design",
      "title_zh": "MolChordï¼šé¢å‘è›‹ç™½è´¨å¼•å¯¼è¯ç‰©è®¾è®¡çš„ç»“æ„-åºåˆ—å¯¹é½",
      "authors": [
        "Wei Zhang",
        "Zekun Guo",
        "Yingce Xia",
        "Peiran Jin",
        "Shufang Xie",
        "Tao Qin",
        "Xiang-Yang Li"
      ],
      "abstract": "Structure-based drug design (SBDD), which maps target proteins to candidate molecular ligands, is a fundamental task in drug discovery. Effectively aligning protein structural representations with molecular representations, and ensuring alignment between generated drugs and their pharmacological properties, remains a critical challenge. To address these challenges, we propose MolChord, which integrates two key techniques: (1) to align protein and molecule structures with their textual descriptions and sequential representations (e.g., FASTA for proteins and SMILES for molecules), we leverage NatureLM, an autoregressive model unifying text, small molecules, and proteins, as the molecule generator, alongside a diffusion-based structure encoder; and (2) to guide molecules toward desired properties, we curate a property-aware dataset by integrating preference data and refine the alignment process using Direct Preference Optimization (DPO). Experimental results on CrossDocked2020 demonstrate that our approach achieves state-of-the-art performance on key evaluation metrics, highlighting its potential as a practical tool for SBDD.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MolChordæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºç»“æ„çš„è¯ç‰©è®¾è®¡(Structure-based drug design, SBDD)ä¸­è›‹ç™½è´¨ç»“æ„ä¸åˆ†å­è¡¨ç¤ºå¯¹é½ï¼Œä»¥åŠç”Ÿæˆè¯ç‰©ä¸å…¶è¯ç†ç‰¹æ€§å¯¹é½çš„æŒ‘æˆ˜ã€‚MolChordé›†æˆäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹(diffusion-based)çš„ç»“æ„ç¼–ç å™¨å’ŒNatureLMè‡ªå›å½’æ¨¡å‹ï¼Œå®ç°äº†è›‹ç™½è´¨åºåˆ—(FASTA)ã€åˆ†å­åºåˆ—(SMILES)ä¸å…¶ç»“æ„åŠæ–‡æœ¬æè¿°çš„ç»Ÿä¸€è¡¨ç¤ºã€‚ä¸ºäº†å¼•å¯¼ç”Ÿæˆçš„åˆ†å­å…·å¤‡ç†æƒ³å±æ€§ï¼Œè¯¥ç ”ç©¶é€šè¿‡æ•´åˆåå¥½æ•°æ®æ„å»ºäº†å±æ€§æ„ŸçŸ¥æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–(Direct Preference Optimization, DPO)æŠ€æœ¯æ¥ç²¾ç‚¼å¯¹é½è¿‡ç¨‹ã€‚åœ¨CrossDocked2020åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMolChordåœ¨å…³é”®è¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½(state-of-the-art)ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå…‹æœäº†ç°æœ‰è¯ç‰©è®¾è®¡ä¸­è¡¨å¾å¯¹é½çš„éš¾é¢˜ï¼Œä¸ºå®ç°é«˜æ€§èƒ½ã€ç›®æ ‡å¯¼å‘çš„è‡ªåŠ¨åŒ–è¯ç‰©å‘ç°æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.27671v1",
      "published_date": "2025-10-31 17:35:53 UTC",
      "updated_date": "2025-10-31 17:35:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:54:30.671624+00:00"
    },
    {
      "arxiv_id": "2510.27659v1",
      "title": "Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems",
      "title_zh": "å¼€æ”¾æ™ºèƒ½ä½“ç³»ç»Ÿä¸­å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„ä¿¡ç”¨åˆ†é…æŒ‘æˆ˜",
      "authors": [
        "Alireza Saleh Abadi",
        "Leen-Kiat Soh"
      ],
      "abstract": "In the rapidly evolving field of multi-agent reinforcement learning (MARL), understanding the dynamics of open systems is crucial. Openness in MARL refers to the dynam-ic nature of agent populations, tasks, and agent types with-in a system. Specifically, there are three types of openness as reported in (Eck et al. 2023) [2]: agent openness, where agents can enter or leave the system at any time; task openness, where new tasks emerge, and existing ones evolve or disappear; and type openness, where the capabil-ities and behaviors of agents change over time. This report provides a conceptual and empirical review, focusing on the interplay between openness and the credit assignment problem (CAP). CAP involves determining the contribution of individual agents to the overall system performance, a task that becomes increasingly complex in open environ-ments. Traditional credit assignment (CA) methods often assume static agent populations, fixed and pre-defined tasks, and stationary types, making them inadequate for open systems. We first conduct a conceptual analysis, in-troducing new sub-categories of openness to detail how events like agent turnover or task cancellation break the assumptions of environmental stationarity and fixed team composition that underpin existing CAP methods. We then present an empirical study using representative temporal and structural algorithms in an open environment. The results demonstrate that openness directly causes credit misattribution, evidenced by unstable loss functions and significant performance degradation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Multi-Agent Reinforcement Learning, MARL) åœ¨å¼€æ”¾ä»£ç†ç³»ç»Ÿ (Open Agent Systems) ä¸­é¢ä¸´çš„å­¦åˆ†åˆ†é… (Credit Assignment Problem, CAP) æŒ‘æˆ˜ã€‚å¼€æ”¾æ€§æ¶‰åŠä»£ç†ã€ä»»åŠ¡å’Œç±»å‹çš„åŠ¨æ€æ¼”å˜ï¼Œä½¿å¾—å‡è®¾é™æ€ç§ç¾¤å’Œå›ºå®šä»»åŠ¡çš„ä¼ ç»Ÿå­¦åˆ†åˆ†é…æ–¹æ³•éš¾ä»¥ä¸ºç»§ã€‚è®ºæ–‡é€šè¿‡æ¦‚å¿µåˆ†æå¼•å…¥äº†æ–°çš„å¼€æ”¾æ€§å­ç±»åˆ«ï¼Œæ·±å…¥æ¢è®¨äº†ä»£ç†æ›´è¿­æˆ–ä»»åŠ¡å–æ¶ˆç­‰äº‹ä»¶å¦‚ä½•ç ´åç°æœ‰ç®—æ³•èµ–ä»¥ç”Ÿå­˜çš„ç¯å¢ƒå¹³ç¨³æ€§å‡è®¾ã€‚é€šè¿‡å¯¹ä»£è¡¨æ€§æ—¶é—´å’Œç»“æ„ç®—æ³•çš„å®è¯ç ”ç©¶ï¼Œä½œè€…è¯æ˜äº†å¼€æ”¾æ€§ä¼šç›´æ¥å¯¼è‡´å­¦åˆ†è¯¯å½’å›  (Credit Misattribution)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç°è±¡åœ¨å®è§‚ä¸Šè¡¨ç°ä¸ºæŸå¤±å‡½æ•°çš„ä¸ç¨³å®šä»¥åŠç³»ç»Ÿæ•´ä½“æ€§èƒ½çš„æ˜¾è‘—ä¸‹é™ï¼Œä¸ºæœªæ¥å¼€å‘æ›´å…·é²æ£’æ€§çš„å¼€æ”¾ç³»ç»Ÿå­¦åˆ†åˆ†é…ç®—æ³•æä¾›äº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27659v1",
      "published_date": "2025-10-31 17:30:32 UTC",
      "updated_date": "2025-10-31 17:30:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:54:28.876018+00:00"
    },
    {
      "arxiv_id": "2511.00141v1",
      "title": "FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding",
      "title_zh": "FLoCï¼šé¢å‘é•¿è§†é¢‘ç†è§£çš„åŸºäºè®¾æ–½é€‰å€çš„é«˜æ•ˆè§†è§‰ Token å‹ç¼©",
      "authors": [
        "Janghoon Cho",
        "Jungsoo Lee",
        "Munawar Hayat",
        "Kyuwoong Hwang",
        "Fatih Porikli",
        "Sungha Choi"
      ],
      "abstract": "Recent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿è§†é¢‘ç†è§£ä¸­ Large Multimodal Models (LMMs) å› è§†è§‰ token æ•°é‡è¿‡å¤§è€Œå¯¼è‡´çš„æ‰©å±•æ€§å—é™é—®é¢˜ï¼Œæå‡ºäº† FLoC æ¡†æ¶ã€‚FLoC æ˜¯ä¸€ç§åŸºäº facility location function çš„é«˜æ•ˆè§†è§‰ token å‹ç¼©æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨é¢„è®¾é¢„ç®—å†…å¿«é€Ÿé€‰æ‹©å…·æœ‰é«˜åº¦ä»£è¡¨æ€§å’Œå¤šæ ·æ€§çš„ token å­é›†ã€‚é€šè¿‡å¼•å…¥ lazy greedy algorithmï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯è¿‘ä¹æœ€ä¼˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†å¤„ç†æ•ˆç‡å¹¶å¤§å¹…å‡å°‘äº†è§†è§‰ token æ•°é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFLoC å…·æœ‰ training-freeã€model-agnostic å’Œ query-agnostic çš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°å„ç§ video-LLMs å’Œç°æœ‰å·¥ä½œæµä¸­ã€‚åœ¨ Video-MMEã€MLVU å’Œ LongVideoBench ç­‰å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFLoC åœ¨å¤„ç†é€Ÿåº¦å’Œå‹ç¼©æ•ˆæœä¸Šå‡ä¼˜äºç°æœ‰çš„å‹ç¼©æŠ€æœ¯ã€‚è¿™ä¸€æˆæœä¸ºè§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„å…³é”®æŒ‘æˆ˜æä¾›äº†ä¸€ç§å…¼å…·é²æ£’æ€§ä¸æ•ˆç‡çš„é€šç”¨è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00141v1",
      "published_date": "2025-10-31 17:29:39 UTC",
      "updated_date": "2025-10-31 17:29:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:54:48.372512+00:00"
    },
    {
      "arxiv_id": "2510.27655v1",
      "title": "Community Detection on Model Explanation Graphs for Explainable AI",
      "title_zh": "é¢å‘å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„æ¨¡å‹è§£é‡Šå›¾ç¤¾åŒºæ£€æµ‹",
      "authors": [
        "Ehsan Moradi"
      ],
      "abstract": "Feature-attribution methods (e.g., SHAP, LIME) explain individual predictions but often miss higher-order structure: sets of features that act in concert. We propose Modules of Influence (MoI), a framework that (i) constructs a model explanation graph from per-instance attributions, (ii) applies community detection to find feature modules that jointly affect predictions, and (iii) quantifies how these modules relate to bias, redundancy, and causality patterns. Across synthetic and real datasets, MoI uncovers correlated feature groups, improves model debugging via module-level ablations, and localizes bias exposure to specific modules. We release stability and synergy metrics, a reference implementation, and evaluation protocols to benchmark module discovery in XAI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿç‰¹å¾å½’å› æ–¹æ³•ï¼ˆå¦‚ SHAPã€LIMEï¼‰éš¾ä»¥æ•æ‰ç‰¹å¾é—´ååŒä½œç”¨çš„é—®é¢˜ï¼Œæå‡ºäº† Modules of Influence (MoI) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»å®ä¾‹çº§å½’å› ä¸­æ„å»ºæ¨¡å‹è§£é‡Šå›¾ï¼ˆmodel explanation graphï¼‰ï¼Œå¹¶åˆ©ç”¨ç¤¾åŒºå‘ç°ï¼ˆcommunity detectionï¼‰æŠ€æœ¯è¯†åˆ«å…±åŒå½±å“é¢„æµ‹çš„ç‰¹å¾æ¨¡å—ã€‚MoI èƒ½å¤Ÿè¿›ä¸€æ­¥é‡åŒ–è¿™äº›æ¨¡å—ä¸åè§ï¼ˆbiasï¼‰ã€å†—ä½™ï¼ˆredundancyï¼‰ä»¥åŠå› æœæ¨¡å¼ï¼ˆcausality patternsï¼‰ä¹‹é—´çš„å…³ç³»ã€‚åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒMoI èƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºç›¸å…³çš„ç‰¹å¾ç»„ï¼Œå¹¶é€šè¿‡æ¨¡å—çº§æ¶ˆèï¼ˆmodule-level ablationsï¼‰æ”¹è¿›æ¨¡å‹è°ƒè¯•è¿‡ç¨‹ï¼ŒåŒæ—¶å®ç°åè§æš´éœ²çš„ç²¾å‡†å®šä½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘å¸ƒäº†ç¨³å®šæ€§ä¸ååŒæ€§æŒ‡æ ‡ã€å‚è€ƒå®ç°åŠè¯„ä¼°åè®®ï¼Œä¸ºå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰é¢†åŸŸçš„æ¨¡å—å‘ç°æä¾›äº†é‡è¦çš„åŸºå‡†å·¥å…·ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27655v1",
      "published_date": "2025-10-31 17:27:56 UTC",
      "updated_date": "2025-10-31 17:27:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:54:36.188921+00:00"
    },
    {
      "arxiv_id": "2510.27651v1",
      "title": "Information-Theoretic Greedy Layer-wise Training for Traffic Sign Recognition",
      "title_zh": "é¢å‘äº¤é€šæ ‡å¿—è¯†åˆ«çš„ä¿¡æ¯è®ºè´ªå©ªé€å±‚è®­ç»ƒ",
      "authors": [
        "Shuyan Lyu",
        "Zhanzimo Wu",
        "Junliang Du"
      ],
      "abstract": "Modern deep neural networks (DNNs) are typically trained with a global cross-entropy loss in a supervised end-to-end manner: neurons need to store their outgoing weights; training alternates between a forward pass (computation) and a top-down backward pass (learning) which is biologically implausible. Alternatively, greedy layer-wise training eliminates the need for cross-entropy loss and backpropagation. By avoiding the computation of intermediate gradients and the storage of intermediate outputs, it reduces memory usage and helps mitigate issues such as vanishing or exploding gradients. However, most existing layer-wise training approaches have been evaluated only on relatively small datasets with simple deep architectures. In this paper, we first systematically analyze the training dynamics of popular convolutional neural networks (CNNs) trained by stochastic gradient descent (SGD) through an information-theoretic lens. Our findings reveal that networks converge layer-by-layer from bottom to top and that the flow of information adheres to a Markov information bottleneck principle. Building on these observations, we propose a novel layer-wise training approach based on the recently developed deterministic information bottleneck (DIB) and the matrix-based RÃ©nyi's $Î±$-order entropy functional. Specifically, each layer is trained jointly with an auxiliary classifier that connects directly to the output layer, enabling the learning of minimal sufficient task-relevant representations. We empirically validate the effectiveness of our training procedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further demonstrate its applicability to a practical task involving traffic sign recognition. Our approach not only outperforms existing layer-wise training baselines but also achieves performance comparable to SGD.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)ä¼ ç»Ÿç«¯åˆ°ç«¯åå‘ä¼ æ’­è®­ç»ƒä¸­å­˜åœ¨çš„ç”Ÿç‰©ä¸å¯è§£é‡Šæ€§å’Œé«˜å†…å­˜æ¶ˆè€—é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä¿¡æ¯è®ºçš„è´ªå©ªé€å±‚è®­ç»ƒ(Greedy Layer-wise Training)æ–¹æ³•ã€‚ä½œè€…é¦–å…ˆé€šè¿‡ä¿¡æ¯è®ºè§†è§’åˆ†æäº†éšæœºæ¢¯åº¦ä¸‹é™(SGD)è®­ç»ƒä¸‹çš„å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)åŠ¨åŠ›å­¦ï¼Œå‘ç°ç½‘ç»œå‘ˆç°è‡ªä¸‹è€Œä¸Šçš„é€å±‚æ”¶æ•›ç‰¹å¾ï¼Œä¸”å…¶ä¿¡æ¯æµç¬¦åˆé©¬å°”å¯å¤«ä¿¡æ¯ç“¶é¢ˆ(Markov information bottleneck)åŸç†ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶ç»“åˆç¡®å®šæ€§ä¿¡æ¯ç“¶é¢ˆ(DIB)å’ŒåŸºäºçŸ©é˜µçš„RÃ©nyi $\\alpha$-é˜¶ç†µæ³›å‡½ï¼Œé€šè¿‡ä¸ºæ¯ä¸€å±‚å¼•å…¥è¾…åŠ©åˆ†ç±»å™¨æ¥å­¦ä¹ æœ€å°å……åˆ†çš„ä»»åŠ¡ç›¸å…³è¡¨ç¤ºã€‚åœ¨CIFAR-10ã€CIFAR-100ä»¥åŠäº¤é€šæ ‡å¿—è¯†åˆ«(Traffic Sign Recognition)ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è®­ç»ƒç¨‹åºä¸ä»…ä¼˜äºç°æœ‰çš„é€å±‚è®­ç»ƒåŸºå‡†ï¼Œè¿˜å–å¾—äº†ä¸SGDç›¸å½“çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºè§„é¿åå‘ä¼ æ’­ã€å‡å°‘ä¸­é—´æ¢¯åº¦è®¡ç®—åŠå†…å­˜å ç”¨æä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”å…·æœ‰ç«äº‰åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27651v1",
      "published_date": "2025-10-31 17:24:58 UTC",
      "updated_date": "2025-10-31 17:24:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:54:40.458204+00:00"
    },
    {
      "arxiv_id": "2510.27646v1",
      "title": "VessShape: Few-shot 2D blood vessel segmentation by leveraging shape priors from synthetic images",
      "title_zh": "VessShapeï¼šåˆ©ç”¨åˆæˆå›¾åƒå½¢çŠ¶å…ˆéªŒå®ç°å°‘æ ·æœ¬2Dè¡€ç®¡åˆ†å‰²",
      "authors": [
        "Cesar H. Comin",
        "Wesley N. GalvÃ£o"
      ],
      "abstract": "Semantic segmentation of blood vessels is an important task in medical image analysis, but its progress is often hindered by the scarcity of large annotated datasets and the poor generalization of models across different imaging modalities. A key aspect is the tendency of Convolutional Neural Networks (CNNs) to learn texture-based features, which limits their performance when applied to new domains with different visual characteristics. We hypothesize that leveraging geometric priors of vessel shapes, such as their tubular and branching nature, can lead to more robust and data-efficient models. To investigate this, we introduce VessShape, a methodology for generating large-scale 2D synthetic datasets designed to instill a shape bias in segmentation models. VessShape images contain procedurally generated tubular geometries combined with a wide variety of foreground and background textures, encouraging models to learn shape cues rather than textures. We demonstrate that a model pre-trained on VessShape images achieves strong few-shot segmentation performance on two real-world datasets from different domains, requiring only four to ten samples for fine-tuning. Furthermore, the model exhibits notable zero-shot capabilities, effectively segmenting vessels in unseen domains without any target-specific training. Our results indicate that pre-training with a strong shape bias can be an effective strategy to overcome data scarcity and improve model generalization in blood vessel segmentation.",
      "tldr_zh": "æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¡€ç®¡åˆ†å‰²(blood vessel segmentation)ä¸­ç”±äºæ ‡æ³¨æ•°æ®ç¨€ç¼ºä»¥åŠå·ç§¯ç¥ç»ç½‘ç»œ(CNNs)è¿‡åº¦ä¾èµ–çº¹ç†ç‰¹å¾å¯¼è‡´çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ã€‚ä½œè€…æå‡ºåˆ©ç”¨è¡€ç®¡çš„ç®¡çŠ¶å’Œåˆ†æ”¯ç­‰å‡ ä½•å½¢çŠ¶å…ˆéªŒ(shape priors)æ¥æ„å»ºæ›´é²æ£’ä¸”æ•°æ®æ•ˆç‡æ›´é«˜çš„æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å¼€å‘äº† VessShapeï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆå¤§è§„æ¨¡ 2D åˆæˆæ•°æ®é›†çš„æ–¹æ³•ï¼Œé€šè¿‡ç¨‹åºåŒ–ç”Ÿæˆçš„ç®¡çŠ¶å‡ ä½•ç»“æ„ç»“åˆå¤šæ ·åŒ–çš„å‰æ™¯å’ŒèƒŒæ™¯çº¹ç†ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ å½¢çŠ¶åç½®(shape bias)è€Œéçº¹ç†ç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ VessShape æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨ä¸¤ä¸ªä¸åŒé¢†åŸŸçš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬(few-shot)åˆ†å‰²æ€§èƒ½ï¼Œä»…éœ€ 4 åˆ° 10 ä¸ªæ ·æœ¬è¿›è¡Œå¾®è°ƒå³å¯è¾¾åˆ°è‰¯å¥½æ•ˆæœã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å±•ç¤ºäº†æ˜¾è‘—çš„é›¶æ ·æœ¬(zero-shot)èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€ç›®æ ‡é¢†åŸŸè®­ç»ƒçš„æƒ…å†µä¸‹æœ‰æ•ˆåˆ†å‰²æœªçŸ¥é¢†åŸŸçš„è¡€ç®¡ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¼ºåŒ–å½¢çŠ¶åç½®è¿›è¡Œé¢„è®­ç»ƒæ˜¯å…‹æœåŒ»å­¦å›¾åƒåˆ†æä¸­æ•°æ®åŒ®ä¹å¹¶æå‡è¡€ç®¡åˆ†å‰²æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆç­–ç•¥ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27646v1",
      "published_date": "2025-10-31 17:19:33 UTC",
      "updated_date": "2025-10-31 17:19:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:54:41.865071+00:00"
    },
    {
      "arxiv_id": "2510.27632v1",
      "title": "Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation",
      "title_zh": "Sketch-to-Layoutï¼šè‰å›¾å¼•å¯¼çš„å¤šæ¨¡æ€å¸ƒå±€ç”Ÿæˆ",
      "authors": [
        "Riccardo Brioschi",
        "Aleksandr Alekseev",
        "Emanuele Nevali",
        "Berkay DÃ¶ner",
        "Omar El Malki",
        "Blagoj Mitrevski",
        "Leandro Kieliger",
        "Mark Collier",
        "Andrii Maksai",
        "Jesse Berent",
        "Claudiu Musat",
        "Efi Kokiopoulou"
      ],
      "abstract": "Graphic layout generation is a growing research area focusing on generating aesthetically pleasing layouts ranging from poster designs to documents. While recent research has explored ways to incorporate user constraints to guide the layout generation, these constraints often require complex specifications which reduce usability. We introduce an innovative approach exploiting user-provided sketches as intuitive constraints and we demonstrate empirically the effectiveness of this new guidance method, establishing the sketch-to-layout problem as a promising research direction, which is currently under-explored. To tackle the sketch-to-layout problem, we propose a multimodal transformer-based solution using the sketch and the content assets as inputs to produce high quality layouts. Since collecting sketch training data from human annotators to train our model is very costly, we introduce a novel and efficient method to synthetically generate training sketches at scale. We train and evaluate our model on three publicly available datasets: PubLayNet, DocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art constraint-based methods, while offering a more intuitive design experience. In order to facilitate future sketch-to-layout research, we release O(200k) synthetically-generated sketches for the public datasets above. The datasets are available at https://github.com/google-deepmind/sketch_to_layout.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Sketch-to-Layoutï¼Œæ—¨åœ¨è§£å†³å¹³é¢å¸ƒå±€ç”Ÿæˆ (Graphic layout generation) é¢†åŸŸä¸­ç”¨æˆ·çº¦æŸ (user constraints) è®¾å®šè¿‡äºå¤æ‚ä¸”å¯ç”¨æ€§è¾ƒä½çš„é—®é¢˜ã€‚ä½œè€…å¼•å…¥äº†å°†ç”¨æˆ·è‰å›¾ (sketches) ä½œä¸ºç›´è§‚çº¦æŸçš„æ–°é¢–æ–¹æ³•ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åŸºäºå¤šæ¨¡æ€ Transformer (multimodal transformer-based) çš„æ¶æ„ï¼Œé€šè¿‡æ•´åˆè‰å›¾å’Œå†…å®¹ç´ æç”Ÿæˆé«˜è´¨é‡å¸ƒå±€ã€‚ä¸ºäº†å…‹æœäººå·¥æ ‡æ³¨è‰å›¾æ•°æ®æˆæœ¬é«˜æ˜‚çš„å›°éš¾ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶å…¬å¼€å‘å¸ƒäº†çº¦20ä¸‡ä¸ªé’ˆå¯¹ PubLayNetã€DocLayNet å’Œ SlidesVQA æ•°æ®é›†çš„åˆæˆè‰å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆæ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„ SOTA çº¦æŸå¼•å¯¼æ–¹æ³•ï¼ŒåŒæ—¶æä¾›äº†æ›´ä¸ºç›´è§‚çš„è®¾è®¡ä½“éªŒã€‚è¿™ä¸€å·¥ä½œå¡«è¡¥äº†è‰å›¾å¼•å¯¼å¸ƒå±€ç”Ÿæˆè¿™ä¸€é¢†åŸŸçš„ç©ºç™½ï¼Œä¸ºæœªæ¥äº¤äº’å¼å¤šæ¨¡æ€è®¾è®¡ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 18 figures, GitHub link: https://github.com/google-deepmind/sketch_to_layout, accept at ICCV 2025 Workshop (HiGen)",
      "pdf_url": "https://arxiv.org/pdf/2510.27632v1",
      "published_date": "2025-10-31 17:05:10 UTC",
      "updated_date": "2025-10-31 17:05:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:54:45.781046+00:00"
    },
    {
      "arxiv_id": "2510.27630v2",
      "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training",
      "title_zh": "äº¤äº’å³æ™ºèƒ½ï¼ˆäºŒï¼‰ï¼šé¢å‘é•¿ç¨‹ä»»åŠ¡è®­ç»ƒçš„å¼‚æ­¥äººæœºååŒæ¼”ç»ƒ",
      "authors": [
        "Dayuan Fu",
        "Yunze Wu",
        "Xiaojie Cai",
        "Lyumanshan Ye",
        "Shijie Xia",
        "Zhen Huang",
        "Weiye Si",
        "Tianze Xu",
        "Jie Sun",
        "Keyu Li",
        "Mohan Jiang",
        "Junfei Wang",
        "Qishuo Hua",
        "Pengrui Lu",
        "Yang Xiao",
        "Pengfei Liu"
      ],
      "abstract": "Large Language Model (LLM) agents have recently shown strong potential in domains such as automated coding, deep research, and graphical user interface manipulation. However, training them to succeed on long-horizon, domain-specialized tasks remains challenging. Current methods primarily fall into two categories. The first relies on dense human annotations through behavior cloning, which is prohibitively expensive for long-horizon tasks that can take days or months. The second depends on outcome-driven sampling, which often collapses due to the rarity of valid positive trajectories on domain-specialized tasks. We introduce Apollo, a sampling framework that integrates asynchronous human guidance with action-level data filtering. Instead of requiring annotators to shadow every step, Apollo allows them to intervene only when the agent drifts from a promising trajectory, by providing prior knowledge, strategic advice, etc. This lightweight design makes it possible to sustain interactions for over 30 hours and produces valuable trajectories at a lower cost. Apollo then applies supervision control to filter out sub-optimal actions and prevent error propagation. Together, these components enable reliable and effective data collection in long-horizon environments. To demonstrate the effectiveness of Apollo, we evaluate it using InnovatorBench. Our experiments show that when applied to train the GLM-4.5 model on InnovatorBench, Apollo achieves more than a 50% improvement over the untrained baseline and a 28% improvement over a variant trained without human interaction. These results highlight the critical role of human-in-the-loop sampling and the robustness of Apollo's design in handling long-horizon, domain-specialized tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨é•¿ç¨‹ã€é¢†åŸŸä¸“ä¸šåŒ–ä»»åŠ¡è®­ç»ƒä¸­é¢ä¸´çš„äººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚åŠç»“æœé©±åŠ¨é‡‡æ ·ç¨€ç–ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºApolloçš„é‡‡æ ·æ¡†æ¶ã€‚Apolloçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé›†æˆäº†å¼‚æ­¥äººå·¥å¼•å¯¼(Asynchronous Human Guidance)ä¸åŠ¨ä½œçº§æ•°æ®è¿‡æ»¤æŠ€æœ¯ï¼Œå…è®¸äººå·¥æ ‡æ³¨è€…ä»…åœ¨æ™ºèƒ½ä½“åç¦»æ½œåŠ›è·¯å¾„æ—¶è¿›è¡Œå…³é”®æ€§å¹²é¢„ï¼Œä»è€Œå®ç°ä½æˆæœ¬ä¸”é«˜æ•ˆçš„é•¿ç¨‹æ•°æ®ç”Ÿæˆã€‚é€šè¿‡å¼•å…¥ç›‘ç£æ§åˆ¶æœºåˆ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè¿‡æ»¤æ¬¡ä¼˜åŠ¨ä½œå¹¶é˜²æ­¢è¯¯å·®ä¼ æ’­ï¼Œç¡®ä¿äº†åœ¨å¤æ‚ç¯å¢ƒä¸‹æ•°æ®æ”¶é›†çš„å¯é æ€§ã€‚åœ¨InnovatorBenchåŸºå‡†æµ‹è¯•çš„å®éªŒä¸­ï¼Œåˆ©ç”¨Apolloè®­ç»ƒçš„GLM-4.5æ¨¡å‹æ€§èƒ½è¾ƒåŸå§‹åŸºçº¿æå‡äº†è¶…è¿‡50%ï¼Œä¸”æ¯”æ— äººå·¥äº¤äº’çš„è®­ç»ƒå˜ä½“æå‡äº†28%ã€‚è¿™ä¸€ç ”ç©¶ç»“æœå……åˆ†è¯æ˜äº†äººåœ¨å›è·¯é‡‡æ ·(Human-in-the-loop sampling)åœ¨å¤„ç†é•¿å‘¨æœŸã€ä¸“ä¸šåŒ–ä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨åŠå…¶è®¾è®¡çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27630v2",
      "published_date": "2025-10-31 17:00:22 UTC",
      "updated_date": "2025-11-03 10:53:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:54:48.960209+00:00"
    },
    {
      "arxiv_id": "2510.27629v4",
      "title": "Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models",
      "title_zh": "å¼€æ”¾æƒé‡ç”Ÿç‰©åŸºç¡€æ¨¡å‹ç”Ÿç‰©é£é™©è¯„ä¼°çš„æœ€ä½³å®è·µ",
      "authors": [
        "Boyi Wei",
        "Zora Che",
        "Nathaniel Li",
        "Udari Madhushani Sehwag",
        "Jasper GÃ¶tting",
        "Samira Nedungadi",
        "Julian Michael",
        "Summer Yue",
        "Dan Hendrycks",
        "Peter Henderson",
        "Zifan Wang",
        "Seth Donoughe",
        "Mantas Mazeika"
      ],
      "abstract": "Open-weight bio-foundation models present a dual-use dilemma. While holding great promise for accelerating scientific research and drug development, they could also enable bad actors to develop more deadly bioweapons. To mitigate the risk posed by these models, current approaches focus on filtering biohazardous data during pre-training. However, the effectiveness of such an approach remains unclear, particularly against determined actors who might fine-tune these models for malicious use. To address this gap, we propose BioRiskEval, a framework to evaluate the robustness of procedures that are intended to reduce the dual-use capabilities of bio-foundation models. BioRiskEval assesses models' virus understanding through three lenses, including sequence modeling, mutational effects prediction, and virulence prediction. Our results show that current filtering practices may not be particularly effective: Excluded knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits broader generalizability in sequence modeling. Furthermore, dual-use signals may already reside in the pretrained representations, and can be elicited via simple linear probing. These findings highlight the challenges of data filtering as a standalone procedure, underscoring the need for further research into robust safety and security strategies for open-weight bio-foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† open-weight bio-foundation models é¢ä¸´çš„åŒé‡ç”¨é€”å›°å¢ƒï¼Œå³åœ¨ä¿ƒè¿›ç§‘ç ”ä¸è¯ç‰©å¼€å‘çš„åŒæ—¶ï¼Œå¯èƒ½è¢«æ¶æ„åˆ©ç”¨ä»¥åˆ¶é€ ç”Ÿç‰©æ­¦å™¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é£é™©ï¼Œä½œè€…æå‡ºäº† BioRiskEval è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæµ‹è¯•æ—¨åœ¨é™ä½æ¨¡å‹åŒé‡ç”¨é€”èƒ½åŠ›çš„å„ç±»ç¨‹åºçš„é²æ£’æ€§ã€‚BioRiskEval é€šè¿‡ sequence modelingã€mutational effects prediction ä»¥åŠ virulence prediction ä¸‰ä¸ªç»´åº¦æ¥æ·±å…¥è¯„ä¼°æ¨¡å‹å¯¹ç—…æ¯’çš„ç†è§£æ·±åº¦ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„ data filtering å®è·µæ•ˆæœå¹¶ä¸ç†æƒ³ï¼Œè¢«è¿‡æ»¤çš„çŸ¥è¯†åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥é€šè¿‡ fine-tuning å¿«é€Ÿæ¢å¤ã€‚æ­¤å¤–ï¼ŒåŒé‡ç”¨é€”ä¿¡å·å¾€å¾€å·²ç»å­˜åœ¨äºé¢„è®­ç»ƒçš„ representations ä¸­ï¼Œä¸”èƒ½é€šè¿‡ç®€å•çš„ linear probing è¢«è¯±å¯¼å‡ºæ¥ã€‚è¿™äº›å‘ç°è¡¨æ˜ä»…ä¾é æ•°æ®è¿‡æ»¤æ— æ³•æœ‰æ•ˆæ¶ˆé™¤é£é™©ï¼Œå¼ºè°ƒäº†é’ˆå¯¹ open-weight bio-foundation models ç ”å‘æ›´ç¨³å¥å®‰å…¨ç­–ç•¥çš„è¿«åˆ‡éœ€æ±‚ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "17 Pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.27629v4",
      "published_date": "2025-10-31 17:00:20 UTC",
      "updated_date": "2025-11-20 18:33:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:54:53.071407+00:00"
    },
    {
      "arxiv_id": "2510.27628v1",
      "title": "Validity Is What You Need",
      "title_zh": "æœ‰æ•ˆæ€§å³ä½ æ‰€éœ€",
      "authors": [
        "Sebastian Benthall",
        "Andrew Clark"
      ],
      "abstract": "While AI agents have long been discussed and studied in computer science, today's Agentic AI systems are something new. We consider other definitions of Agentic AI and propose a new realist definition. Agentic AI is a software delivery mechanism, comparable to software as a service (SaaS), which puts an application to work autonomously in a complex enterprise setting. Recent advances in large language models (LLMs) as foundation models have driven excitement in Agentic AI. We note, however, that Agentic AI systems are primarily applications, not foundations, and so their success depends on validation by end users and principal stakeholders. The tools and techniques needed by the principal users to validate their applications are quite different from the tools and techniques used to evaluate foundation models. Ironically, with good validation measures in place, in many cases the foundation models can be replaced with much simpler, faster, and more interpretable models that handle core logic. When it comes to Agentic AI, validity is what you need. LLMs are one option that might achieve it.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Agentic AI çš„å®šä¹‰åŠå…¶åœ¨ç°ä»£è®¡ç®—ä¸­çš„è§’è‰²ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„ç°å®ä¸»ä¹‰å®šä¹‰ï¼Œå°†å…¶è§†ä¸ºä¸€ç§ç±»ä¼¼äºè½¯ä»¶å³æœåŠ¡ (SaaS) çš„è½¯ä»¶äº¤ä»˜æœºåˆ¶ï¼Œæ—¨åœ¨å¤æ‚ä¼ä¸šç¯å¢ƒä¸­å®ç°è‡ªä¸»åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä½œä¸ºåŸºç¡€æ¨¡å‹æ¨åŠ¨äº† Agentic AI çš„å…´èµ·ï¼Œä½† Agentic AI æœ¬è´¨ä¸Šæ˜¯åº”ç”¨è€ŒéåŸºç¡€æ¶æ„ï¼Œå…¶æˆåŠŸæ ¸å¿ƒåœ¨äºç»ˆç«¯ç”¨æˆ·å’Œä¸»è¦åˆ©ç›Šç›¸å…³è€…çš„éªŒè¯ (validation)ã€‚ä½œè€…å¼ºè°ƒï¼ŒéªŒè¯åº”ç”¨æ‰€éœ€çš„å·¥å…·ä¸è¯„ä¼°åŸºç¡€æ¨¡å‹çš„å·¥å…·æˆªç„¶ä¸åŒï¼Œä¸”åœ¨å…·å¤‡è‰¯å¥½éªŒè¯æªæ–½çš„å‰æä¸‹ï¼ŒåŸæœ¬ç”± LLMs æ‰¿æ‹…çš„æ ¸å¿ƒé€»è¾‘å¾€å¾€å¯ä»¥è¢«æ›´ç®€å•ã€æ›´å¿«é€Ÿä¸”å¯è§£é‡Šæ€§æ›´å¼ºçš„æ¨¡å‹æ‰€å–ä»£ã€‚ç ”ç©¶æœ€ç»ˆå¾—å‡ºç»“è®ºï¼Œå¯¹äº Agentic AI è€Œè¨€ï¼Œç¡®ä¿æœ‰æ•ˆæ€§ (Validity) æ‰æ˜¯å…³é”®ï¼Œè€Œ LLMs ä»…æ˜¯å®ç°è¿™ä¸€ç›®æ ‡çš„æ‰‹æ®µä¹‹ä¸€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27628v1",
      "published_date": "2025-10-31 17:00:04 UTC",
      "updated_date": "2025-10-31 17:00:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:54:55.078336+00:00"
    },
    {
      "arxiv_id": "2510.27623v1",
      "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning",
      "title_zh": "åŸºäºå¯¹æ¯”è§¦å‘å™¨å­¦ä¹ çš„ MLLM å…·èº«å†³ç­–è§†è§‰åé—¨æ”»å‡»",
      "authors": [
        "Qiusi Zhan",
        "Hyeonjeong Ha",
        "Rui Yang",
        "Sirui Xu",
        "Hanyang Chen",
        "Liang-Yan Gui",
        "Yu-Xiong Wang",
        "Huan Zhang",
        "Heng Ji",
        "Daniel Kang"
      ],
      "abstract": "Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é©±åŠ¨çš„å…·èº«æ™ºèƒ½ä½“é¢ä¸´çš„æ–°å‹å®‰å…¨å¨èƒï¼Œå³è§†è§‰åé—¨æ”»å‡»ï¼ˆVisual Backdoor Attacksï¼‰ã€‚æ”»å‡»è€…åˆ©ç”¨ç¯å¢ƒä¸­çš„ç‰©ä½“ä½œä¸ºè§¦å‘å™¨ï¼Œä½¿æ™ºèƒ½ä½“åœ¨ç‰¹å®šè§†è§‰ä¿¡å·å‡ºç°æ—¶æŒç»­æ‰§è¡Œé¢„è®¾çš„æ¶æ„å¤šæ­¥ç­–ç•¥ã€‚ä¸ºè§£å†³ç‰©ä½“è§¦å‘å™¨å› è§†è§’å’Œå…‰ç…§å˜åŒ–éš¾ä»¥ç¨³å®šæ¤å…¥çš„æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†é¦–ä¸ªé’ˆå¯¹æ€§çš„æ”»å‡»æ¡†æ¶ BEATã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šæ ·åŒ–çš„åœºæ™¯è®­ç»ƒé›†å’Œä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼Œå³å…ˆè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå†åº”ç”¨åˆ›æ–°çš„å¯¹æ¯”è§¦å‘å™¨å­¦ä¹ ï¼ˆContrastive Trigger Learning, CTLï¼‰ã€‚CTL é€šè¿‡å°†è§¦å‘å™¨åˆ¤åˆ«å»ºæ¨¡ä¸ºè§¦å‘å™¨å­˜åœ¨ä¸å¦è¾“å…¥ä¹‹é—´çš„åå¥½å­¦ä¹ ï¼ˆPreference Learningï¼‰ï¼Œæ˜¾è‘—å¼ºåŒ–äº†å†³ç­–è¾¹ç•Œä»¥ç¡®ä¿åé—¨çš„ç²¾å‡†æ¿€æ´»ã€‚å®éªŒè¡¨æ˜ï¼ŒBEAT åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 80% çš„æ”»å‡»æˆåŠŸç‡ï¼Œä¸”åœ¨æœ‰é™æ•°æ®ä¸‹ CTL æ¯”ä¼ ç»Ÿ SFT æå‡äº† 39% çš„æ¿€æ´»å‡†ç¡®ç‡ï¼Œæ­ç¤ºäº†å…·èº«æ™ºèƒ½ä½“åœ¨ç°å®éƒ¨ç½²ä¸­è¿«åˆ‡éœ€è¦é²æ£’é˜²å¾¡çš„å®‰å…¨éšæ‚£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27623v1",
      "published_date": "2025-10-31 16:50:49 UTC",
      "updated_date": "2025-10-31 16:50:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:55:08.078847+00:00"
    },
    {
      "arxiv_id": "2511.08598v1",
      "title": "OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand, Open Knowledge Benchmarking",
      "title_zh": "OKBenchï¼šé€šè¿‡å…¨è‡ªåŠ¨ã€æŒ‰éœ€çš„å¼€æ”¾çŸ¥è¯†åŸºå‡†æµ‹è¯•å®ç°å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°çš„æ™®åŠåŒ–",
      "authors": [
        "Yanhong Li",
        "Tianyang Xu",
        "Kenan Tang",
        "Karen Livescu",
        "David McAllester",
        "Jiawei Zhou"
      ],
      "abstract": "Knowledge-intensive question answering is central to large language models (LLMs) and is typically assessed using static benchmarks derived from sources like Wikipedia and textbooks. However, these benchmarks fail to capture evolving knowledge in a dynamic world, and centralized curation struggles to keep pace with rapid LLM advancements. To address these drawbacks, we propose Open Knowledge Bench (OKBench), a fully automated framework for generating high-quality, dynamic knowledge benchmarks on demand. Focusing on the news domain where knowledge updates daily, OKBench is an agentic framework that automates the sourcing, creation, validation, and distribution of benchmarks. Our approach democratizes benchmark creation and facilitates thorough evaluation of retrieval-augmented methods by reducing overlap with pretraining data. We evaluate our framework on a wide range open-source and proprietary LLMs of various sizes and configurations, both with and without retrieval over freshly generated knowledge. Our results reveal distinct model behaviors when confronted with new information and highlight how retrieval narrows the performance gap between small and large models. These findings underscore the importance of evaluating LLMs on evolving knowledge benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OKBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨ã€æŒ‰éœ€ç”Ÿæˆçš„å¼€æ”¾çŸ¥è¯†åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿé™æ€åŸºå‡†æµ‹è¯•æ— æ³•æ•æ‰ç°å®ä¸–ç•ŒåŠ¨æ€æ¼”è¿›çŸ¥è¯†çš„é—®é¢˜ã€‚OKBenché‡‡ç”¨agenticæ¡†æ¶è®¾è®¡ï¼Œä¸“æ³¨äºçŸ¥è¯†æ¯æ—¥æ›´æ–°çš„æ–°é—»é¢†åŸŸï¼Œå®ç°äº†åŸºå‡†æµ‹è¯•ä»ç´ æè·å–ã€åˆ›å»ºã€éªŒè¯åˆ°åˆ†å‘çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡å‡å°‘åŸºå‡†æµ‹è¯•ä¸pretraining dataçš„é‡å ï¼Œèƒ½å¤Ÿæ›´å®¢è§‚åœ°è¯„ä¼°æ£€ç´¢å¢å¼º(retrieval-augmented)æ–¹æ³•çš„æ€§èƒ½ã€‚é€šè¿‡å¯¹å¤šç§è§„æ¨¡çš„å¼€æºåŠé—­æºLLMsè¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶å‘ç°æ£€ç´¢æŠ€æœ¯å¯ä»¥æœ‰æ•ˆç¼©å°å°å‹æ¨¡å‹ä¸å¤§å‹æ¨¡å‹åœ¨é¢å¯¹æ–°ä¿¡æ¯æ—¶çš„è¡¨ç°å·®è·ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨æŒç»­æ¼”è¿›çš„çŸ¥è¯†åŸºå‡†ä¸Šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„é‡è¦æ€§ï¼Œä¸ºLLMçš„åŠ¨æ€è¯„ä¼°æä¾›äº†é«˜æ•ˆä¸”æ°‘ä¸»åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08598v1",
      "published_date": "2025-10-31 16:44:34 UTC",
      "updated_date": "2025-10-31 16:44:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:55:13.972423+00:00"
    },
    {
      "arxiv_id": "2510.27617v1",
      "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
      "title_zh": "VeriMoAï¼šä¸€ç§é¢å‘è®¾è®¡è§„èŒƒåˆ°ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLï¼‰ç”Ÿæˆçš„æ··åˆæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Heng Ping",
        "Arijit Bhattacharjee",
        "Peiyu Zhang",
        "Shixuan Li",
        "Wei Yang",
        "Anzhe Cheng",
        "Xiaole Zhang",
        "Jesse Thomason",
        "Ali Jannesari",
        "Nesreen Ahmed",
        "Paul Bogdan"
      ],
      "abstract": "Automation of Register Transfer Level (RTL) design can help developers meet increasing computational demands. Large Language Models (LLMs) show promise for Hardware Description Language (HDL) generation, but face challenges due to limited parametric knowledge and domain-specific constraints. While prompt engineering and fine-tuning have limitations in knowledge coverage and training costs, multi-agent architectures offer a training-free paradigm to enhance reasoning through collaborative generation. However, current multi-agent approaches suffer from two critical deficiencies: susceptibility to noise propagation and constrained reasoning space exploration. We propose VeriMoA, a training-free mixture-of-agents (MoA) framework with two synergistic innovations. First, a quality-guided caching mechanism to maintain all intermediate HDL outputs and enables quality-based ranking and selection across the entire generation process, encouraging knowledge accumulation over layers of reasoning. Second, a multi-path generation strategy that leverages C++ and Python as intermediate representations, decomposing specification-to-HDL translation into two-stage processes that exploit LLM fluency in high-resource languages while promoting solution diversity. Comprehensive experiments on VerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves 15--30% improvements in Pass@1 across diverse LLM backbones, especially enabling smaller models to match larger models and fine-tuned alternatives without requiring costly training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VeriMoAï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ··åˆæ™ºèƒ½ä½“ (Mixture-of-Agents, MoA) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç¡¬ä»¶æè¿°è¯­è¨€ (HDL) ç”Ÿæˆä¸­é¢ä¸´çš„å‚æ•°åŒ–çŸ¥è¯†æœ‰é™å’Œé¢†åŸŸç‰¹å®šçº¦æŸç­‰æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰å¤šæ™ºèƒ½ä½“æ–¹æ³•åœ¨å™ªå£°ä¼ æ’­å’Œæ¨ç†ç©ºé—´æ¢ç´¢å—é™æ–¹é¢çš„ä¸è¶³ï¼ŒVeriMoA å¼•å…¥äº†ä¸¤é¡¹ååŒåˆ›æ–°ã€‚é¦–å…ˆï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†è´¨é‡å¼•å¯¼çš„ç¼“å­˜æœºåˆ¶ (quality-guided caching mechanism)ï¼Œé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹ä¸­é—´ HDL è¾“å‡ºè¿›è¡Œæ’åå’Œé€‰æ‹©ï¼Œå®ç°äº†è·¨æ¨ç†å±‚çš„çŸ¥è¯†ç§¯ç´¯ã€‚å…¶æ¬¡ï¼ŒVeriMoA æå‡ºäº†ä¸€ç§å¤šè·¯å¾„ç”Ÿæˆç­–ç•¥ (multi-path generation strategy)ï¼Œåˆ©ç”¨ C++ å’Œ Python ä½œä¸ºä¸­é—´è¡¨ç¤º (intermediate representations)ï¼Œé€šè¿‡ä¸¤é˜¶æ®µç¿»è¯‘è¿‡ç¨‹æå‡äº†æ–¹æ¡ˆçš„å¤šæ ·æ€§ã€‚åœ¨ VerilogEval 2.0 å’Œ RTLLM 2.0 åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§ LLM åç«¯ä¸Šå°† Pass@1 æ€§èƒ½æå‡äº† 15% è‡³ 30%ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—è¾ƒå°æ¨¡å‹åœ¨æ— éœ€æ˜‚è´µå¾®è°ƒçš„æƒ…å†µä¸‹å³å¯è¾¾åˆ°ç”šè‡³è¶…è¶Šå¤§å‹æ¨¡å‹æˆ–ä¸“ç”¨æ¨¡å‹çš„è¡¨ç°ï¼Œæ˜¾è‘—æ¨è¿›äº†å¯„å­˜å™¨ä¼ è¾“çº§ (RTL) è®¾è®¡çš„è‡ªåŠ¨åŒ–è¿›ç¨‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27617v1",
      "published_date": "2025-10-31 16:40:58 UTC",
      "updated_date": "2025-10-31 16:40:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:55:14.180190+00:00"
    },
    {
      "arxiv_id": "2510.27606v2",
      "title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning",
      "title_zh": "Spatial-SSRLï¼šåŸºäºè‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ çš„ç©ºé—´ç†è§£å¢å¼º",
      "authors": [
        "Yuhong Liu",
        "Beichen Zhang",
        "Yuhang Zang",
        "Yuhang Cao",
        "Long Xing",
        "Xiaoyi Dong",
        "Haodong Duan",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "abstract": "Spatial understanding remains a weakness of Large Vision-Language Models (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement learning with verifiable rewards (RLVR) pipelines depend on costly supervision, specialized tools, or constrained environments that limit scale. We introduce Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically formulates five pretext tasks that capture 2D and 3D spatial structure: shuffled patch reordering, flipped patch recognition, cropped patch inpainting, regional depth ordering, and relative 3D position prediction. These tasks provide ground-truth answers that are easy to verify and require no human or LVLM annotation. Training on our tasks substantially improves spatial reasoning while preserving general visual capabilities. On seven spatial understanding benchmarks in both image and video settings, Spatial-SSRL delivers average accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our results show that simple, intrinsic supervision enables RLVR at scale and provides a practical route to stronger spatial intelligence in LVLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) åœ¨ç©ºé—´ç†è§£æ–¹é¢çš„è–„å¼±ç¯èŠ‚ï¼Œæå‡ºäº† Spatial-SSRLï¼Œä¸€ç§é€šè¿‡è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹  (Self-Supervised Reinforcement Learning) å¢å¼ºæ¨¡å‹ç©ºé—´è®¤çŸ¥èƒ½åŠ›çš„èŒƒå¼ã€‚ä¸ºå…‹æœç°æœ‰ç›‘ç£å¾®è°ƒ (SFT) å’Œå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹  (RLVR) å¯¹é«˜æ˜‚äººå·¥æ ‡æ³¨æˆ–ç‰¹å®šç¯å¢ƒçš„ä¾èµ–ï¼ŒSpatial-SSRL ç›´æ¥ä»æ™®é€š RGB æˆ– RGB-D å›¾åƒä¸­æå–å¯éªŒè¯ä¿¡å·ã€‚è¯¥æ¡†æ¶è‡ªåŠ¨æ„å»ºäº†äº”é¡¹æ¶µç›– 2D å’Œ 3D ç©ºé—´ç»“æ„çš„ pretext tasksï¼ŒåŒ…æ‹¬ shuffled patch reorderingã€flipped patch recognitionã€cropped patch inpaintingã€regional depth ordering å’Œ relative 3D position predictionã€‚è¿™äº›ä»»åŠ¡æä¾›äº†æ˜“äºéªŒè¯çš„ ground-truthï¼Œæ— éœ€äººå·¥æˆ– LVLM è¾…åŠ©æ ‡æ³¨ï¼Œåœ¨æ˜¾è‘—æå‡ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŒæ—¶ä¿ç•™äº†é€šç”¨è§†è§‰èƒ½åŠ›ã€‚åœ¨ä¸ƒé¡¹ç©ºé—´ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSpatial-SSRL ç›¸æ¯” Qwen2.5-VL åŸºçº¿åœ¨ 3B å’Œ 7B æ¨¡å‹ä¸Šåˆ†åˆ«å®ç°äº† 4.63% å’Œ 3.89% çš„å¹³å‡å‡†ç¡®ç‡æå‡ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œç®€å•çš„å†…åœ¨ç›‘ç£å³å¯æ”¯æŒå¤§è§„æ¨¡çš„ RLVR è®­ç»ƒï¼Œä¸ºæå‡ LVLMs çš„ç©ºé—´æ™ºèƒ½æä¾›äº†é«˜æ•ˆä¸”å®ç”¨çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2510.27606v2",
      "published_date": "2025-10-31 16:30:08 UTC",
      "updated_date": "2025-11-25 02:41:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T06:55:20.474735+00:00"
    },
    {
      "arxiv_id": "2510.27598v2",
      "title": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM Research",
      "title_zh": "InnovatorBenchï¼šè¯„ä¼°æ™ºèƒ½ä½“å¼€å±•åˆ›æ–°æ€§ LLM ç ”ç©¶çš„èƒ½åŠ›",
      "authors": [
        "Yunze Wu",
        "Dayuan Fu",
        "Weiye Si",
        "Zhen Huang",
        "Mohan Jiang",
        "Keyu Li",
        "Shijie Xia",
        "Jie Sun",
        "Tianze Xu",
        "Xiangkun Hu",
        "Pengrui Lu",
        "Xiaojie Cai",
        "Lyumanshan Ye",
        "Wenhong Zhu",
        "Yang Xiao",
        "Pengfei Liu"
      ],
      "abstract": "AI agents could accelerate scientific discovery by automating hypothesis formation, experiment design, coding, execution, and analysis, yet existing benchmarks probe narrow skills in simplified settings. To address this gap, we introduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end assessment of agents performing Large Language Model (LLM) research. It comprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss Design, Reward Design, and Scaffold Construction, which require runnable artifacts and assessment of correctness, performance, output quality, and uncertainty. To support agent operation, we develop ResearchGym, a research environment offering rich action spaces, distributed and long-horizon execution, asynchronous monitoring, and snapshot saving. We also implement a lightweight ReAct agent that couples explicit reasoning with executable planning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2. Our experiments demonstrate that while frontier models show promise in code-driven research tasks, they struggle with fragile algorithm-related tasks and long-horizon decision making, such as impatience, poor resource management, and overreliance on template-based reasoning. Furthermore, agents require over 11 hours to achieve their best performance on InnovatorBench, underscoring the benchmark's difficulty and showing the potential of InnovatorBench to be the next generation of code-based research benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† InnovatorBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç«¯åˆ°ç«¯è¯„ä¼° AI æ™ºèƒ½ä½“æ‰§è¡Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ (LLM) ç ”ç©¶èƒ½åŠ›çš„åŸºå‡†å¹³å°ã€‚è¯¥åŸºå‡†æ¶µç›–äº† Data Constructionã€Loss Design å’Œ Reward Design ç­‰ 20 é¡¹æ ¸å¿ƒç ”ç©¶ä»»åŠ¡ï¼Œå¹¶é…å¥—å¼€å‘äº†æ”¯æŒé•¿ç¨‹æ‰§è¡Œå’Œå¼‚æ­¥ç›‘æ§çš„ ResearchGym å®éªŒç¯å¢ƒã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ Claude-4ã€GPT-5 ç­‰å‰æ²¿æ¨¡å‹æ„å»ºäº† ReAct æ™ºèƒ½ä½“è¿›è¡Œæµ‹è¯•ï¼Œç»“æœè¡¨æ˜è¿™äº›æ¨¡å‹è™½ç„¶åœ¨ä»£ç é©±åŠ¨å‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤„ç†è„†å¼±çš„ç®—æ³•ä»»åŠ¡åŠèµ„æºç®¡ç†ç­‰é•¿ç¨‹å†³ç­–æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—çŸ­æ¿ã€‚å®éªŒæ•°æ®è¿›ä¸€æ­¥æ˜¾ç¤ºï¼Œæ™ºèƒ½ä½“éœ€æŒç»­è¿è¡Œ 11 å°æ—¶ä»¥ä¸Šæ‰èƒ½è¾¾åˆ°æœ€ä½³è¡¨ç°ï¼Œå……åˆ†è¯æ˜äº† InnovatorBench ä½œä¸ºä¸‹ä¸€ä»£ä»£ç ç ”ç©¶åŸºå‡†çš„å¤æ‚æ€§ä¸æŒ‘æˆ˜æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27598v2",
      "published_date": "2025-10-31 16:22:23 UTC",
      "updated_date": "2025-11-03 10:56:21 UTC",
      "processing_status": "completed",
      "attempts": 2,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:19:21.927799+00:00"
    },
    {
      "arxiv_id": "2511.00139v2",
      "title": "End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection",
      "title_zh": "åŸºäºå…±äº«è‡ªä¸»çš„ç«¯åˆ°ç«¯çµå·§è‡‚æ‰‹ VLA ç­–ç•¥ï¼šé€šè¿‡è‡ªä¸»æ‰‹éƒ¨ VLA ç­–ç•¥å¢å¼º VR é¥æ“ä½œä»¥å®ç°é«˜æ•ˆæ•°æ®é‡‡é›†",
      "authors": [
        "Yu Cui",
        "Yujian Zhang",
        "Lina Tao",
        "Yang Li",
        "Xinyu Yi",
        "Zhibin Li"
      ],
      "abstract": "Achieving human-like dexterous manipulation remains a major challenge for general-purpose robots. While Vision-Language-Action (VLA) models show potential in learning skills from demonstrations, their scalability is limited by scarce high-quality training data. Existing data collection methods face inherent constraints: manual teleoperation overloads human operators, while automated planning often produces unnatural motions. We propose a Shared Autonomy framework that divides control between macro and micro motions. A human operator guides the robot's arm pose through intuitive VR teleoperation, while an autonomous DexGrasp-VLA policy handles fine-grained hand control using real-time tactile and visual feedback. This division significantly reduces cognitive load and enables efficient collection of high-quality coordinated arm-hand demonstrations. Using this data, we train an end-to-end VLA policy enhanced with our novel Arm-Hand Feature Enhancement module, which captures both distinct and shared representations of macro and micro movements for more natural coordination. Our Corrective Teleoperation system enables continuous policy improvement through human-in-the-loop failure recovery. Experiments demonstrate that our framework generates high-quality data with minimal manpower and achieves a 90% success rate across diverse objects, including unseen instances. Comprehensive evaluations validate the system's effectiveness in developing dexterous manipulation capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäº Shared Autonomy çš„ç«¯åˆ°ç«¯ Arm-Hand VLA ç­–ç•¥æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é€šç”¨æœºå™¨äººå®ç°ç±»äººçµå·§æ“ä½œæ—¶é¢ä¸´çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ¡†æ¶å°†æ§åˆ¶ä»»åŠ¡åˆ’åˆ†ä¸ºå®è§‚è¿åŠ¨ä¸å¾®è§‚è¿åŠ¨ï¼Œç”±äººç±»æ“ä½œå‘˜é€šè¿‡ç›´è§‚çš„ VR Teleoperation å¼•å¯¼æ‰‹è‡‚å§¿æ€ï¼Œè€Œè‡ªä¸»çš„ DexGrasp-VLA ç­–ç•¥åˆ™ç»“åˆå®æ—¶è§¦è§‰ä¸è§†è§‰åé¦ˆå¤„ç†ç²¾ç»†çš„æ‰‹éƒ¨æ§åˆ¶ã€‚è¿™ç§åˆ†å·¥æ˜¾è‘—é™ä½äº†äººç±»æ“ä½œè€…çš„è®¤çŸ¥è´Ÿè·ï¼Œå®ç°äº†é«˜è´¨é‡æ‰‹è‡‚-æ‰‹éƒ¨åè°ƒæ¼”ç¤ºæ•°æ®çš„å¿«é€Ÿé‡‡é›†ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ©ç”¨è¿™äº›æ•°æ®è®­ç»ƒäº†å¢å¼ºå‹çš„ VLA ç­–ç•¥ï¼Œé€šè¿‡ Arm-Hand Feature Enhancement æ¨¡å—æ•æ‰ä¸åŒå±‚çº§è¿åŠ¨çš„è¡¨å¾ä»¥ä¼˜åŒ–åè°ƒæ€§ï¼Œå¹¶è¾…ä»¥ Corrective Teleoperation ç³»ç»Ÿè¿›è¡Œäººç±»åœ¨ç¯çš„å¤±è´¥æ¢å¤ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½ä»¥æä½çš„äººåŠ›æˆæœ¬ç”Ÿæˆé«˜è´¨é‡æ•°æ®ï¼Œåœ¨æ¶µç›–æœªè§ç‰©ä½“çš„å¤šç§ä»»åŠ¡ä¸­è¾¾åˆ°äº† 90% çš„æˆåŠŸç‡ï¼Œä¸ºå¼€å‘å¤æ‚çš„æœºå™¨äººçµå·§æ“ä½œèƒ½åŠ›æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00139v2",
      "published_date": "2025-10-31 16:12:02 UTC",
      "updated_date": "2025-12-13 15:50:18 UTC",
      "processing_status": "completed",
      "attempts": 2,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:19:28.174986+00:00"
    },
    {
      "arxiv_id": "2510.27571v1",
      "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum",
      "title_zh": "è¿ˆå‘é€šç”¨è§†é¢‘æ£€ç´¢ï¼šåŸºäºåˆæˆå¤šæ¨¡æ€é‡‘å­—å¡”è¯¾ç¨‹çš„è§†é¢‘åµŒå…¥æ³›åŒ–",
      "authors": [
        "Zhuoning Guo",
        "Mingxin Li",
        "Yanzhao Zhang",
        "Dingkun Long",
        "Pengjun Xie",
        "Xiaowen Chu"
      ],
      "abstract": "The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘æ£€ç´¢æ¨¡å‹å› è¯„æµ‹åŸºå‡†ç‹­çª„è€Œå¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†ä¸€å¥—æ•´åˆè¯„ä¼°ã€æ•°æ®å’Œå»ºæ¨¡çš„ååŒè®¾è®¡æ¡†æ¶ã€‚é¦–å…ˆï¼Œç ”ç©¶è€…æ„å»ºäº†Universal Video Retrieval Benchmark (UVRB)ï¼ŒåŒ…å«16ä¸ªæ•°æ®é›†ï¼Œæ—¨åœ¨è¯Šæ–­ä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¸­çš„å…³é”®èƒ½åŠ›å·®è·ã€‚å…¶æ¬¡ï¼ŒåŸºäºè¯Šæ–­ç»“æœå¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„åˆæˆå·¥ä½œæµï¼Œç”Ÿæˆäº†155ä¸‡ä¸ªé«˜è´¨é‡é…å¯¹æ•°æ®ä»¥å¡«å……è¯­ä¹‰ç©ºé—´ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†Modality Pyramidè¯¾ç¨‹å­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒGeneral Video Embedder (GVE)ï¼Œé€šè¿‡æ˜¾å¼åˆ©ç”¨å¤šæ ·åŒ–æ•°æ®ä¸­çš„æ½œåœ¨çº¿æ€§å…³è”æå‡æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒGVEåœ¨UVRBä¸Šå®ç°äº†æœ€å…ˆè¿›çš„zero-shotæ³›åŒ–èƒ½åŠ›ã€‚åˆ†æç»“æœè¿›ä¸€æ­¥æ­ç¤ºäº†ä¼ ç»Ÿçƒ­é—¨åŸºå‡†æ— æ³•æœ‰æ•ˆé¢„æµ‹é€šç”¨èƒ½åŠ›ï¼Œä¸”éƒ¨åˆ†ç›¸å…³æ£€ç´¢(partially relevant retrieval)æ˜¯ä¸€ä¸ªè¢«é•¿æœŸå¿½è§†çš„å…³é”®åº”ç”¨åœºæ™¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27571v1",
      "published_date": "2025-10-31 15:54:48 UTC",
      "updated_date": "2025-10-31 15:54:48 UTC",
      "processing_status": "completed",
      "attempts": 2,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:19:27.922875+00:00"
    },
    {
      "arxiv_id": "2510.27568v1",
      "title": "SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning",
      "title_zh": "SIGMAï¼šé¢å‘æ™ºèƒ½ä½“æ•°å­¦æ¨ç†çš„æœç´¢å¢å¼ºå‹æŒ‰éœ€çŸ¥è¯†æ•´åˆ",
      "authors": [
        "Ali Asgarov",
        "Umid Suleymanov",
        "Aadyant Khatri"
      ],
      "abstract": "Solving mathematical reasoning problems requires not only accurate access to relevant knowledge but also careful, multi-step thinking. However, current retrieval-augmented models often rely on a single perspective, follow inflexible search strategies, and struggle to effectively combine information from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge Integration for AGentic Mathematical reAsoning), a unified framework that orchestrates specialized agents to independently reason, perform targeted searches, and synthesize findings through a moderator mechanism. Each agent generates hypothetical passages to optimize retrieval for its analytic perspective, ensuring knowledge integration is both context-sensitive and computation-efficient. When evaluated on challenging benchmarks such as MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms both open- and closed-source systems, achieving an absolute performance improvement of 7.4%. Our results demonstrate that multi-agent, on-demand knowledge integration significantly enhances both reasoning accuracy and efficiency, offering a scalable approach for complex, knowledge-intensive problem-solving. We will release the code upon publication.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† SIGMAï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºæ™ºèƒ½ä½“æ•°å­¦æ¨ç†çš„æœç´¢å¢å¼ºæŒ‰éœ€çŸ¥è¯†é›†æˆæ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰æ£€ç´¢å¢å¼ºæ¨¡å‹åœ¨å¤„ç†å¤šæºä¿¡æ¯æ—¶çš„çµæ´»æ€§ä¸è¶³é—®é¢˜ã€‚SIGMA é€šè¿‡åè°ƒä¸“é—¨çš„æ™ºèƒ½ä½“ç‹¬ç«‹è¿›è¡Œæ¨ç†ã€æ‰§è¡Œé’ˆå¯¹æ€§æœç´¢ï¼Œå¹¶åˆ©ç”¨ä¸»æŒäºº(moderator)æœºåˆ¶ç»¼åˆç ”ç©¶ç»“æœã€‚è¯¥æ¡†æ¶è¦æ±‚æ¯ä¸ªæ™ºèƒ½ä½“ç”Ÿæˆå‡è®¾æ€§æ®µè½(hypothetical passages)æ¥ä¼˜åŒ–æ£€ç´¢è¿‡ç¨‹ï¼Œç¡®ä¿çŸ¥è¯†é›†æˆå…¼å…·ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§ä¸è®¡ç®—æ•ˆç‡ã€‚åœ¨ MATH500ã€AIME åŠåšå£«çº§ç§‘å­¦é—®ç­” GPQA ç­‰é«˜éš¾åº¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSIGMA çš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¼€æºå’Œé—­æºç³»ç»Ÿï¼Œå®ç°äº† 7.4% çš„ç»å¯¹æ€§èƒ½æå‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§å¤šæ™ºèƒ½ä½“åä½œçš„æŒ‰éœ€é›†æˆæ¨¡å¼æ˜¾è‘—æå‡äº†æ•°å­¦æ¨ç†çš„å‡†ç¡®æ€§ä¸æ•ˆç‡ï¼Œä¸ºè§£å†³çŸ¥è¯†å¯†é›†å‹å¤æ‚é—®é¢˜æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Short Paper - Under Review",
      "pdf_url": "https://arxiv.org/pdf/2510.27568v1",
      "published_date": "2025-10-31 15:51:00 UTC",
      "updated_date": "2025-10-31 15:51:00 UTC",
      "processing_status": "completed",
      "attempts": 2,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:19:31.826110+00:00"
    },
    {
      "arxiv_id": "2510.27565v1",
      "title": "CodeAlignBench: Assessing Code Generation Models on Developer-Preferred Code Adjustments",
      "title_zh": "CodeAlignBenchï¼šè¯„ä¼°ä»£ç ç”Ÿæˆæ¨¡å‹åœ¨å¼€å‘è€…åå¥½ä»£ç è°ƒæ•´æ–¹é¢çš„è¡¨ç°",
      "authors": [
        "Forough Mehralian",
        "Ryan Shar",
        "James R. Rae",
        "Alireza Hashemi"
      ],
      "abstract": "As large language models become increasingly capable of generating code, evaluating their performance remains a complex and evolving challenge. Existing benchmarks primarily focus on functional correctness, overlooking the diversity of real-world coding tasks and developer expectations. To this end, we introduce a multi-language benchmark that evaluates LLM instruction-following capabilities and is extensible to operate on any set of standalone coding problems. Our benchmark evaluates instruction following in two key settings: adherence to pre-defined constraints specified with the initial problem, and the ability to perform refinements based on follow-up instructions. For this paper's analysis, we empirically evaluated our benchmarking pipeline with programming tasks from LiveBench, that are also automatically translated from Python into Java and JavaScript. Our automated benchmark reveals that models exhibit differing levels of performance across multiple dimensions of instruction-following. Our benchmarking pipeline provides a more comprehensive evaluation of code generation models, highlighting their strengths and limitations across languages and generation goals.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† CodeAlignBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚ä¸ä»¥å¾€ä¸»è¦å…³æ³¨åŠŸèƒ½æ­£ç¡®æ€§çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼Œè¯¥æ¡†æ¶æ›´å¼ºè°ƒç°å®ç¼–ç ä»»åŠ¡çš„å¤šæ ·æ€§ä»¥åŠå¼€å‘è€…å¯¹ä»£ç è°ƒæ•´çš„åå¥½ã€‚CodeAlignBench è¯„ä¼°äº†æ¨¡å‹åœ¨åˆå§‹çº¦æŸéµå¾ªå’ŒåŸºäºåç»­æŒ‡ä»¤è¿›è¡Œä»£ç æ”¹è¿›ï¼ˆrefinementsï¼‰è¿™ä¸¤ç§å…³é”®åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶è€…é€šè¿‡ LiveBench ä»»åŠ¡åŠå…¶è‡ªåŠ¨ç”Ÿæˆçš„ Java å’Œ JavaScript ç‰ˆæœ¬å¯¹è¯¥æµç¨‹è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚å®éªŒæ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å¤šç»´åº¦æŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸­è¡¨ç°å‡ºçš„æ€§èƒ½å·®å¼‚ã€‚è¿™ä¸€åŸºå‡†æµ‹è¯•ä¸ºä»£ç ç”Ÿæˆæ¨¡å‹æä¾›äº†æ›´å…¨é¢çš„è¯„ä»·ä½“ç³»ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æ¨¡å‹åœ¨è·¨è¯­è¨€å’Œç‰¹å®šç”Ÿæˆç›®æ ‡ä¸‹çš„ä¼˜åŠ¿ä¸å±€é™ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27565v1",
      "published_date": "2025-10-31 15:47:07 UTC",
      "updated_date": "2025-10-31 15:47:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:19:34.427375+00:00"
    },
    {
      "arxiv_id": "2510.27558v1",
      "title": "Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs",
      "title_zh": "è¿ˆå‘ç²¾å‡†çš„é•¿æ—¶ç¨‹æœºå™¨äººæ“çºµï¼šåŸºäºåœºæ™¯å›¾åˆ©ç”¨åŸºç¡€æ¨¡å‹å®ç°ä»è¯­è¨€åˆ°åŠ¨ä½œ",
      "authors": [
        "Sushil Samuel Dinesh",
        "Shinkyu Park"
      ],
      "abstract": "This paper presents a framework that leverages pre-trained foundation models for robotic manipulation without domain-specific training. The framework integrates off-the-shelf models, combining multimodal perception from foundation models with a general-purpose reasoning model capable of robust task sequencing. Scene graphs, dynamically maintained within the framework, provide spatial awareness and enable consistent reasoning about the environment. The framework is evaluated through a series of tabletop robotic manipulation experiments, and the results highlight its potential for building robotic manipulation systems directly on top of off-the-shelf foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ—¨åœ¨å®ç°å‡†ç¡®çš„é•¿ç¨‹(Long-Horizon)æœºå™¨äººæ“ä½œçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹(foundation models)ä¸”æ— éœ€é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œè®­ç»ƒã€‚è¯¥ç³»ç»Ÿæ•´åˆäº†ç°æœ‰çš„ç¦»æ¶(off-the-shelf)æ¨¡å‹ï¼Œå°†åŸºç¡€æ¨¡å‹çš„å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ä¸èƒ½å¤Ÿè¿›è¡Œç¨³å¥ä»»åŠ¡æ’åºçš„é€šç”¨æ¨ç†æ¨¡å‹ç›¸ç»“åˆã€‚é€šè¿‡åœ¨æ¡†æ¶å†…åŠ¨æ€ç»´æŠ¤åœºæ™¯å›¾(Scene graphs)ï¼Œç³»ç»Ÿè·å¾—äº†ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶å®ç°äº†å¯¹ç¯å¢ƒçš„ä¸€è‡´æ€§æ¨ç†ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ä¸€ç³»åˆ—æ¡Œé¢æœºå™¨äººæ“ä½œå®éªŒå¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœçªæ˜¾äº†å…¶ç›´æ¥åŸºäºç°æœ‰çš„åŸºç¡€æ¨¡å‹æ„å»ºæœºå™¨äººæ“ä½œç³»ç»Ÿçš„æ½œåŠ›ï¼Œä¸ºå®ç°è¯­è¨€åˆ°åŠ¨ä½œ(Language-to-Action)çš„é«˜æ•ˆè½¬åŒ–æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27558v1",
      "published_date": "2025-10-31 15:42:32 UTC",
      "updated_date": "2025-10-31 15:42:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:19:36.921565+00:00"
    },
    {
      "arxiv_id": "2510.27554v1",
      "title": "Sybil-Resistant Service Discovery for Agent Economies",
      "title_zh": "é¢å‘æ™ºèƒ½ä½“ç»æµçš„æŠ—å¥³å·«æ”»å‡»æœåŠ¡å‘ç°",
      "authors": [
        "David Shi",
        "Kevin Joo"
      ],
      "abstract": "x402 enables Hypertext Transfer Protocol (HTTP) services like application programming interfaces (APIs), data feeds, and inference providers to accept cryptocurrency payments for access. As agents increasingly consume these services, discovery becomes critical: which swap interface should an agent trust? Which data provider is the most reliable? We introduce TraceRank, a reputation-weighted ranking algorithm where payment transactions serve as endorsements. TraceRank seeds addresses with precomputed reputation metrics and propagates reputation through payment flows weighted by transaction value and temporal recency. Applied to x402's payment graph, this surfaces services preferred by high-reputation users rather than those with high transaction volume. Our system combines TraceRank with semantic search to respond to natural language queries with high quality results. We argue that reputation propagation resists Sybil attacks by making spam services with many low-reputation payers rank below legitimate services with few high-reputation payers. Ultimately, we aim to construct a search method for x402 enabled services that avoids infrastructure bias and has better performance than purely volume based or semantic methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½ä½“ç»æµ(Agent Economies)ä¸­çš„æœåŠ¡å‘ç°é—®é¢˜ï¼Œåœ¨æ”¯æŒåŠ å¯†è´§å¸æ”¯ä»˜çš„ x402 åè®®åŸºç¡€ä¸Šæå‡ºäº† TraceRank æ’åç®—æ³•ã€‚TraceRank å°†æ”¯ä»˜äº¤æ˜“è§†ä¸ºèƒŒä¹¦ï¼Œé€šè¿‡é¢„è®¡ç®—çš„ä¿¡èª‰æŒ‡æ ‡ä»¥åŠå—äº¤æ˜“ä»·å€¼å’Œæ—¶é—´è¿‘åº¦(Temporal Recency)åŠ æƒçš„æ”¯ä»˜æµæ¥ä¼ æ’­ä¿¡èª‰ã€‚è¯¥ç³»ç»Ÿå°† TraceRank ä¸è¯­ä¹‰æœç´¢(Semantic Search)ç›¸ç»“åˆï¼Œèƒ½å¤Ÿç²¾å‡†å“åº”è‡ªç„¶è¯­è¨€æŸ¥è¯¢å¹¶è¯†åˆ«é«˜è´¨é‡æœåŠ¡ã€‚è¿™ç§ä¿¡èª‰ä¼ æ’­æœºåˆ¶é€šè¿‡æå‡é«˜ä¿¡èª‰ç”¨æˆ·åå¥½æœåŠ¡çš„æ’åï¼Œæœ‰æ•ˆæŠµå¾¡äº†æ—¨åœ¨é€šè¿‡è™šå‡äº¤æ˜“æå‡æƒé‡çš„ Sybil æ”»å‡»ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶æ„å»ºäº†ä¸€ç§é¿å…åŸºç¡€è®¾æ–½åè§çš„æœç´¢æ–¹æ³•ï¼Œå…¶æ€§èƒ½ä¼˜äºå•çº¯ä¾èµ–äº¤æ˜“é‡æˆ–è¯­ä¹‰åŒ¹é…çš„ä¼ ç»Ÿå‘ç°æœºåˆ¶ï¼Œä¸ºæ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿæä¾›äº†æ›´å¯é çš„ä¿¡ä»»ä¿éšœã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CR",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.27554v1",
      "published_date": "2025-10-31 15:29:31 UTC",
      "updated_date": "2025-10-31 15:29:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:19:45.632258+00:00"
    },
    {
      "arxiv_id": "2510.27545v1",
      "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities",
      "title_zh": "EBT-Policyï¼šèƒ½é‡è§£é”æ¶Œç°çš„ç‰©ç†æ¨ç†èƒ½åŠ›",
      "authors": [
        "Travis Davies",
        "Yiqi Huang",
        "Alexi Gladstone",
        "Yunxin Liu",
        "Xiang Chen",
        "Heng Ji",
        "Huxian Liu",
        "Luhui Hu"
      ],
      "abstract": "Implicit policies parameterized by generative models, such as Diffusion Policy, have become the standard for policy learning and Vision-Language-Action (VLA) models in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) address these issues by learning energy landscapes end-to-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce a new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, a 50x reduction compared to Diffusion Policy's 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers a promising path toward robust, generalizable robot behavior under distribution shifts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EBT-Policyï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŸºäºèƒ½é‡çš„æ¶æ„ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººé¢†åŸŸDiffusion Policyç­‰ç”Ÿæˆæ¨¡å‹å­˜åœ¨çš„è®¡ç®—æˆæœ¬é«˜ã€æ›å…‰åå·®(exposure bias)åŠæ¨ç†åŠ¨åŠ›å­¦ä¸ç¨³å®šç­‰æ ¸å¿ƒé—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨èƒ½é‡å˜å‹å™¨(Energy-Based Transformers)åœ¨å¤„ç†é«˜ç»´ç©ºé—´æ—¶çš„å¯æ‰©å±•æ€§ï¼ŒEBT-Policyèƒ½å¤Ÿç«¯åˆ°ç«¯å­¦ä¹ èƒ½é‡æ™¯è§‚å¹¶å»ºæ¨¡å‡è¡¡åŠ¨åŠ›å­¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨æ¨¡æ‹Ÿå’Œç°å®ä»»åŠ¡ä¸­å‡ä¼˜äºåŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œä¸”æ¨ç†æ•ˆç‡æ˜¾è‘—æå‡ï¼Œéƒ¨åˆ†ä»»åŠ¡ä»…éœ€2æ­¥æ¨ç†å³å¯æ”¶æ•›ï¼Œè¾ƒDiffusion Policyå‡å°‘äº†50å€è®¡ç®—é‡ã€‚æ­¤å¤–ï¼ŒEBT-Policyå±•ç°äº†ç‹¬ç‰¹çš„ç‰©ç†æ¨ç†æ¶Œç°èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨ä»…ä½¿ç”¨è¡Œä¸ºå…‹éš†(Behavior Cloning)ä¸”æ— éœ€æ˜¾å¼é‡è¯•è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å¤±è´¥åŠ¨ä½œåºåˆ—çš„é›¶æ ·æœ¬æ¢å¤ã€‚é€šè¿‡åˆ©ç”¨æ ‡é‡èƒ½é‡è¿›è¡Œä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨ç†å’ŒåŠ¨æ€è®¡ç®—åˆ†é…ï¼ŒEBT-Policyä¸ºå®ç°åœ¨åˆ†å¸ƒåç§»(distribution shifts)ä¸‹ç¨³å¥ä¸”æ³›åŒ–çš„æœºå™¨äººè¡Œä¸ºæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 6 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.27545v1",
      "published_date": "2025-10-31 15:21:05 UTC",
      "updated_date": "2025-10-31 15:21:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:19:49.022204+00:00"
    },
    {
      "arxiv_id": "2511.04696v1",
      "title": "EncouRAGe: Evaluating RAG Local, Fast, and Reliable",
      "title_zh": "EncouRAGeï¼šæœ¬åœ°ã€å¿«é€Ÿã€å¯é çš„ RAG è¯„ä¼°",
      "authors": [
        "Jan Strich",
        "Adeline Scharfenberg",
        "Chris Biemann",
        "Martin Semmann"
      ],
      "abstract": "We introduce EncouRAGe, a comprehensive Python framework designed to streamline the development and evaluation of Retrieval-Augmented Generation (RAG) systems using Large Language Models (LLMs) and Embedding Models. EncouRAGe comprises five modular and extensible components: Type Manifest, RAG Factory, Inference, Vector Store, and Metrics, facilitating flexible experimentation and extensible development. The framework emphasizes scientific reproducibility, diverse evaluation metrics, and local deployment, enabling researchers to efficiently assess datasets within RAG workflows. This paper presents implementation details and an extensive evaluation across multiple benchmark datasets, including 25k QA pairs and over 51k documents. Our results show that RAG still underperforms compared to the Oracle Context, while Hybrid BM25 consistently achieves the best results across all four datasets. We further examine the effects of reranking, observing only marginal performance improvements accompanied by higher response latency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† EncouRAGeï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºç®€åŒ–ä½¿ç”¨ Large Language Models (LLMs) å’Œ Embedding Models æ„å»ºçš„æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) ç³»ç»Ÿå¼€å‘ä¸è¯„ä¼°è€Œè®¾è®¡çš„ Python æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å« Type Manifestã€RAG Factoryã€Inferenceã€Vector Store å’Œ Metrics äº”ä¸ªæ¨¡å—åŒ–ç»„ä»¶ï¼Œå¼ºè°ƒç§‘å­¦çš„å¯é‡å¤æ€§ã€å¤šæ ·åŒ–çš„è¯„ä¼°æŒ‡æ ‡ä»¥åŠæœ¬åœ°éƒ¨ç½² (local deployment)ã€‚é€šè¿‡åœ¨åŒ…å« 2.5 ä¸‡ä¸ª QA å¯¹å’Œè¶…è¿‡ 5.1 ä¸‡ä»½æ–‡æ¡£çš„å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œç ”ç©¶å‘ç°ç›®å‰ RAG çš„è¡¨ç°ä»é€Šäº Oracle Contextï¼Œè€Œ Hybrid BM25 åœ¨æ‰€æœ‰æ•°æ®é›†ä¸­å‡ä¸€è‡´å–å¾—äº†æœ€ä½³æ•ˆæœã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¿›ä¸€æ­¥æ¢è®¨äº†é‡æ’åº (reranking) çš„å½±å“ï¼Œè§‚å¯Ÿåˆ°å…¶ä»…èƒ½å¸¦æ¥å¾®å°çš„æ€§èƒ½æå‡ï¼Œä¸”ä¼šä¼´éšæ›´é«˜çš„å“åº”å»¶è¿Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Currently under review",
      "pdf_url": "https://arxiv.org/pdf/2511.04696v1",
      "published_date": "2025-10-31 15:19:29 UTC",
      "updated_date": "2025-10-31 15:19:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:19:47.525169+00:00"
    },
    {
      "arxiv_id": "2510.27544v1",
      "title": "Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for Interpretable Deconstruction of Reasoning System Performance",
      "title_zh": "å­¦ä¹ å‹æ¨ç†åŸç† 1ï¼šTempoBenchï¼Œç”¨äºæ¨ç†ç³»ç»Ÿæ€§èƒ½å¯è§£é‡Šæ€§è§£æ„çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Nikolaus Holzer",
        "William Fishell",
        "Baishakhi Ray",
        "Mark Santolucito"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly excelling and outpacing human performance on many tasks. However, to improve LLM reasoning, researchers either rely on ad-hoc generated datasets or formal mathematical proof systems such as the Lean proof assistant. Whilst ad-hoc generated methods can capture the decision chains of real-world reasoning processes, they may encode some inadvertent bias in the space of reasoning they cover; they also cannot be formally verified. On the other hand, systems like Lean can guarantee verifiability, but are not well-suited to capture the nature of agentic decision chain-based tasks. This creates a gap both in performance for functions such as business agents or code assistants, and in the usefulness of LLM reasoning benchmarks, whereby these fall short in reasoning structure or real-world alignment. We introduce TempoBench, the first formally grounded and verifiable diagnostic benchmark that parametrizes difficulty to systematically analyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks to break down reasoning ability. First, temporal trace evaluation (TTE) tests the ability of an LLM to understand and simulate the execution of a given multi-step reasoning system. Subsequently, temporal causal evaluation (TCE) tests an LLM's ability to perform multi-step causal reasoning and to distill cause-and-effect relations from complex systems. We find that models score 65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art LLMs clearly understand the TCE task but perform poorly as system complexity increases. Our code is available at our \\href{https://github.com/nik-hz/tempobench}{GitHub repository}.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†TempoBenchï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºå½¢å¼åŒ–åŸºç¡€ä¸”å¯éªŒè¯çš„è¯Šæ–­æ€§åŸºå‡†æµ‹è¯•(Benchmark)ï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰æ¨ç†æ•°æ®é›†åœ¨çœŸå®åœºæ™¯å¯¹é½ä¸å¯éªŒè¯æ€§ä¹‹é—´çš„ç¼ºå£ã€‚TempoBenché€šè¿‡å‚æ•°åŒ–éš¾åº¦æ¥ç³»ç»Ÿåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†è¡¨ç°ï¼Œä¸»è¦åŒ…å«æ—¶é—´è¿½è¸ªè¯„ä¼°(Temporal Trace Evaluation, TTE)å’Œæ—¶é—´å› æœè¯„ä¼°(Temporal Causal Evaluation, TCE)ä¸¤ä¸ªæ ¸å¿ƒç»´åº¦ã€‚TTEç”¨äºæµ‹è¯•æ¨¡å‹ç†è§£å’Œæ¨¡æ‹Ÿå¤šæ­¥æ¨ç†ç³»ç»Ÿæ‰§è¡Œè¿‡ç¨‹çš„èƒ½åŠ›ï¼Œè€ŒTCEåˆ™ä¾§é‡äºè¯„ä¼°æ¨¡å‹ä»å¤æ‚ç³»ç»Ÿä¸­æç‚¼å¤šæ­¥å› æœå…³ç³»çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æœ€å…ˆè¿›çš„LLMsåœ¨TCE-normalä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éš¾åº¦è¾ƒé«˜çš„TCE-hardä»»åŠ¡ä¸­å¾—åˆ†ä»…ä¸º7.5%ã€‚è¿™è¡¨æ˜å½“å‰æ¨¡å‹åœ¨ç†è§£å› æœä»»åŠ¡çš„åŒæ—¶ï¼Œå…¶æ¨ç†èƒ½åŠ›ä¼šéšç€ç³»ç»Ÿå¤æ‚åº¦çš„å¢åŠ è€Œæ˜¾è‘—ä¸‹é™ã€‚è¯¥ç ”ç©¶ä¸ºè§£æ„æ¨ç†ç³»ç»Ÿæ€§èƒ½æä¾›äº†å¯è§£é‡Šçš„è¯Šæ–­å·¥å…·ï¼Œå¹¶ä¸ºæ”¹è¿›LLMsåœ¨ä¸šåŠ¡æ™ºèƒ½å’Œä»£ç åŠ©æ‰‹ç­‰é¢†åŸŸçš„å®é™…åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.FL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27544v1",
      "published_date": "2025-10-31 15:17:55 UTC",
      "updated_date": "2025-10-31 15:17:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:19:53.833576+00:00"
    },
    {
      "arxiv_id": "2510.27543v1",
      "title": "DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual Language Models",
      "title_zh": "DialectalArabicMMLUï¼šé˜¿æ‹‰ä¼¯è¯­åŠå¤šè¯­è¨€è¯­è¨€æ¨¡å‹çš„æ–¹è¨€èƒ½åŠ›åŸºå‡†æµ‹è¯•",
      "authors": [
        "Malik H. Altakrori",
        "Nizar Habash",
        "Abdelhakim Freihat",
        "Younes Samih",
        "Kirill Chirkunov",
        "Muhammed AbuOdeh",
        "Radu Florian",
        "Teresa Lynn",
        "Preslav Nakov",
        "Alham Fikri Aji"
      ],
      "abstract": "We present DialectalArabicMMLU, a new benchmark for evaluating the performance of large language models (LLMs) across Arabic dialects. While recently developed Arabic and multilingual benchmarks have advanced LLM evaluation for Modern Standard Arabic (MSA), dialectal varieties remain underrepresented despite their prevalence in everyday communication. DialectalArabicMMLU extends the MMLU-Redux framework through manual translation and adaptation of 3K multiple-choice question-answer pairs into five major dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of 15K QA pairs across 32 academic and professional domains (22K QA pairs when also including English and MSA). The benchmark enables systematic assessment of LLM reasoning and comprehension beyond MSA, supporting both task-based and linguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs (1B-13B parameters) and report substantial performance variation across dialects, revealing persistent gaps in dialectal generalization. DialectalArabicMMLU provides the first unified, human-curated resource for measuring dialectal understanding in Arabic, thus promoting more inclusive evaluation and future model development.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† DialectalArabicMMLUï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€è¡¨ç°çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹æ—¥å¸¸äº¤æµä¸­å¹¿æ³›ä½¿ç”¨ä½†å¸¸è¢«å¿½è§†çš„æ–¹è¨€ï¼Œè¯¥åŸºå‡†é€šè¿‡å¯¹ MMLU-Redux æ¡†æ¶è¿›è¡Œäººå·¥ç¿»è¯‘å’Œæ”¹ç¼–ï¼Œå°† 3,000 ä¸ªå¤šé€‰é¢˜å¯¹è½¬åŒ–ä¸º Syrianã€Egyptianã€Emiratiã€Saudi å’Œ Moroccan äº”å¤§ä¸»è¦æ–¹è¨€ï¼Œåœ¨ 32 ä¸ªé¢†åŸŸå…±ç”Ÿæˆ 15,000 ä¸ªé—®ç­”å¯¹ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹ 19 ä¸ªå‚æ•°è§„æ¨¡åœ¨ 1B-13B ä¹‹é—´çš„å¼€æº (open-weight) é˜¿æ‹‰ä¼¯è¯­å’Œå¤šè¯­è¨€ LLMs è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ä¸åŒæ–¹è¨€é—´çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¹¶æ­ç¤ºäº†æ–¹è¨€æ³›åŒ– (dialectal generalization) èƒ½åŠ›ä¸­å­˜åœ¨çš„æŒä¹…å·®è·ã€‚ä½œä¸ºé¦–ä¸ªç»Ÿä¸€çš„äººå·¥ç­–åˆ’èµ„æºï¼ŒDialectalArabicMMLU ä¸ºè¡¡é‡é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€ç†è§£æä¾›äº†æ ‡å‡†ï¼Œå¯¹äºæ¨åŠ¨æ›´å…·åŒ…å®¹æ€§çš„æ¨¡å‹è¯„ä¼°å’Œæœªæ¥å¼€å‘å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 9 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.27543v1",
      "published_date": "2025-10-31 15:17:06 UTC",
      "updated_date": "2025-10-31 15:17:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:09.925091+00:00"
    },
    {
      "arxiv_id": "2510.27527v1",
      "title": "TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation Suppression and Outlier Control",
      "title_zh": "TetraJet-v2ï¼šç»“åˆæŒ¯è¡æŠ‘åˆ¶ä¸ç¦»ç¾¤å€¼æ§åˆ¶çš„å¤§è¯­è¨€æ¨¡å‹é«˜ç²¾åº¦ NVFP4 è®­ç»ƒ",
      "authors": [
        "Yuxiang Chen",
        "Xiaoming Xu",
        "Pengle Zhang",
        "Michael Beyer",
        "Martin Rapp",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "abstract": "Large Language Models (LLMs) training is prohibitively expensive, driving interest in low-precision fully-quantized training (FQT). While novel 4-bit formats like NVFP4 offer substantial efficiency gains, achieving near-lossless training at such low precision remains challenging. We introduce TetraJet-v2, an end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights, and gradients in all linear layers. We identify two critical issues hindering low-precision LLM training: weight oscillation and outliers. To address these, we propose: 1) an unbiased double-block quantization method for NVFP4 linear layers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3) OutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently outperforms prior FP4 training methods on pre-training LLMs across varying model sizes up to 370M and data sizes up to 200B tokens, reducing the performance gap to full-precision training by an average of 51.3%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TetraJet-v2ï¼Œä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç«¯åˆ°ç«¯ 4-bit å…¨é‡åŒ–è®­ç»ƒï¼ˆFQTï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨ NVFP4 æ ¼å¼æå‡è®­ç»ƒæ•ˆç‡çš„åŒæ—¶ä¿æŒé«˜ç²¾åº¦ã€‚ç ”ç©¶å›¢é˜Ÿè¯†åˆ«å‡ºé˜»ç¢ä½æ¯”ç‰¹è®­ç»ƒçš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼Œå³æƒé‡éœ‡è¡ï¼ˆweight oscillationï¼‰å’Œç¦»ç¾¤å€¼ï¼ˆoutliersï¼‰ï¼Œå¹¶åœ¨æ‰€æœ‰çº¿æ€§å±‚çš„æ¿€æ´»ã€æƒé‡å’Œæ¢¯åº¦ä¸­å…¨é¢åº”ç”¨ NVFP4ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†é’ˆå¯¹ NVFP4 çº¿æ€§å±‚çš„æ— ååŒå—é‡åŒ–ï¼ˆunbiased double-block quantizationï¼‰æŠ€æœ¯ï¼Œä»¥åŠç”¨äºæŠ‘åˆ¶éœ‡è¡çš„ OsciReset ç®—æ³•å’Œç”¨äºä¿ç•™ç¦»ç¾¤å€¼å‡†ç¡®æ€§çš„ OutControl ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTetraJet-v2 åœ¨é«˜è¾¾ 370M å‚æ•°è§„æ¨¡å’Œ 200B token æ•°æ®é‡çš„é¢„è®­ç»ƒä»»åŠ¡ä¸­å‡ä¼˜äºå…ˆå‰çš„ FP4 è®­ç»ƒæ–¹æ³•ã€‚è¯¥ç ”ç©¶æˆåŠŸå°†ä½æ¯”ç‰¹è®­ç»ƒä¸å…¨ç²¾åº¦è®­ç»ƒä¹‹é—´çš„æ€§èƒ½å·®è·å¹³å‡ç¼©å°äº† 51.3%ï¼Œä¸ºé«˜æ•ˆã€å‡†ç¡®çš„ä½ç²¾åº¦ LLM è®­ç»ƒæä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27527v1",
      "published_date": "2025-10-31 14:57:16 UTC",
      "updated_date": "2025-10-31 14:57:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:09.336107+00:00"
    },
    {
      "arxiv_id": "2510.27522v1",
      "title": "Leveraging Generic Time Series Foundation Models for EEG Classification",
      "title_zh": "åˆ©ç”¨é€šç”¨æ—¶é—´åºåˆ—åŸºåº§æ¨¡å‹è¿›è¡Œ EEG åˆ†ç±»",
      "authors": [
        "ThÃ©o Gnassounou",
        "Yessin Moakher",
        "Shifeng Xie",
        "Vasilii Feofanov",
        "Ievgen Redko"
      ],
      "abstract": "Foundation models for time series are emerging as powerful general-purpose backbones, yet their potential for domain-specific biomedical signals such as electroencephalography (EEG) remains rather unexplored. In this work, we investigate the applicability a recently proposed time series classification foundation model, to a different EEG tasks such as motor imagery classification and sleep stage prediction. We test two pretraining regimes: (a) pretraining on heterogeneous real-world time series from multiple domains, and (b) pretraining on purely synthetic data. We find that both variants yield strong performance, consistently outperforming EEGNet, a widely used convolutional baseline, and CBraMod, the most recent EEG-specific foundation model. These results suggest that generalist time series foundation models, even when pretrained on data of non-neural origin or on synthetic signals, can transfer effectively to EEG. Our findings highlight the promise of leveraging cross-domain pretrained models for brain signal analysis, suggesting that EEG may benefit from advances in the broader time series literature.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šç”¨æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ (Foundation Models) åœ¨è„‘ç”µå›¾ (EEG) åˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œæ—¨åœ¨è¯„ä¼°è¿™äº›è·¨é¢†åŸŸæ¨¡å‹å¤„ç†ç‰¹å®šç”Ÿç‰©åŒ»å­¦ä¿¡å·çš„æ•ˆæœã€‚ç ”ç©¶äººå‘˜æµ‹è¯•äº†ä¸¤ç§é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œåˆ†åˆ«åŸºäºå¤šé¢†åŸŸçš„å¼‚æ„çœŸå®ä¸–ç•Œæ—¶é—´åºåˆ—æ•°æ®ä»¥åŠå®Œå…¨åˆæˆçš„æ•°æ® (Synthetic Data)ï¼Œå¹¶å°†å…¶åº”ç”¨äºè¿åŠ¨æƒ³è±¡åˆ†ç±» (Motor Imagery Classification) å’Œç¡çœ é˜¶æ®µé¢„æµ‹ (Sleep Stage Prediction) ç­‰ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ä¸¤ç§é¢„è®­ç»ƒå˜ä½“åœ¨æ€§èƒ½ä¸Šå‡ä¸€è‡´ä¼˜äºå¹¿æ³›ä½¿ç”¨çš„å·ç§¯åŸºå‡†æ¨¡å‹ EEGNet ä»¥åŠæœ€æ–°çš„é¢†åŸŸä¸“ç”¨åŸºç¡€æ¨¡å‹ CBraModã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œå³ä½¿æ˜¯åœ¨éç¥ç»æ¥æºæˆ–åˆæˆä¿¡å·ä¸Šé¢„è®­ç»ƒçš„é€šç”¨åŸºç¡€æ¨¡å‹ï¼Œä¹Ÿèƒ½æœ‰æ•ˆåœ°è¿ç§»è‡³ EEG é¢†åŸŸã€‚è¯¥ç ”ç©¶çªæ˜¾äº†åˆ©ç”¨è·¨é¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè„‘ä¿¡å·åˆ†æçš„å‰æ™¯ï¼Œæš—ç¤º EEG ç ”ç©¶å¯ä»¥ä»æ›´å¹¿æ³›çš„æ—¶é—´åºåˆ—æ–‡çŒ®è¿›å±•ä¸­æ˜¾è‘—è·ç›Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27522v1",
      "published_date": "2025-10-31 14:49:23 UTC",
      "updated_date": "2025-10-31 14:49:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:11.125314+00:00"
    },
    {
      "arxiv_id": "2510.27508v1",
      "title": "Context-Gated Cross-Modal Perception with Visual Mamba for PET-CT Lung Tumor Segmentation",
      "title_zh": "åŸºäº Visual Mamba çš„ä¸Šä¸‹æ–‡é—¨æ§è·¨æ¨¡æ€æ„ŸçŸ¥ PET-CT è‚ºè‚¿ç˜¤åˆ†å‰²",
      "authors": [
        "Elena Mulero AyllÃ³n",
        "Linlin Shen",
        "Pierangelo Veltri",
        "Fabrizia Gelardi",
        "Arturo Chiti",
        "Paolo Soda",
        "Matteo Tortora"
      ],
      "abstract": "Accurate lung tumor segmentation is vital for improving diagnosis and treatment planning, and effectively combining anatomical and functional information from PET and CT remains a major challenge. In this study, we propose vMambaX, a lightweight multimodal framework integrating PET and CT scan images through a Context-Gated Cross-Modal Perception Module (CGM). Built on the Visual Mamba architecture, vMambaX adaptively enhances inter-modality feature interaction, emphasizing informative regions while suppressing noise. Evaluated on the PCLT20K dataset, the model outperforms baseline models while maintaining lower computational complexity. These results highlight the effectiveness of adaptive cross-modal gating for multimodal tumor segmentation and demonstrate the potential of vMambaX as an efficient and scalable framework for advanced lung cancer analysis. The code is available at https://github.com/arco-group/vMambaX.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èåˆ PET ä¸ CT å›¾åƒåœ¨è‚ºéƒ¨è‚¿ç˜¤åˆ†å‰²ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º vMambaX çš„è½»é‡åŒ–å¤šæ¨¡æ€æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäº Visual Mamba æ¶æ„ï¼Œé€šè¿‡æ ¸å¿ƒçš„ä¸Šä¸‹æ–‡é—¨æ§è·¨æ¨¡æ€æ„ŸçŸ¥æ¨¡å— (Context-Gated Cross-Modal Perception Module, CGM) å®ç°è§£å‰–å­¦ä¸åŠŸèƒ½æ€§ä¿¡æ¯çš„æ·±åº¦é›†æˆã€‚vMambaX èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¢å¼ºä¸åŒæ¨¡æ€é—´çš„ç‰¹å¾äº¤äº’ï¼Œåœ¨å¼ºè°ƒå…³é”®ä¿¡æ¯åŒºåŸŸçš„åŒæ—¶æœ‰æ•ˆæŠ‘åˆ¶å™ªå£°å¹²æ‰°ã€‚åœ¨ PCLT20K æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œä¸”å…·æœ‰æ›´ä½çš„è®¡ç®—å¤æ‚åº¦ã€‚è¿™é¡¹å·¥ä½œä¸ä»…è¯æ˜äº†è‡ªé€‚åº”è·¨æ¨¡æ€é—¨æ§åœ¨å¤šæ¨¡æ€è‚¿ç˜¤åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºé«˜æ•ˆã€å¯æ‰©å±•çš„è‚ºç™Œåˆ†ææä¾›äº†å…¨æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27508v1",
      "published_date": "2025-10-31 14:29:52 UTC",
      "updated_date": "2025-10-31 14:29:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:16.042277+00:00"
    },
    {
      "arxiv_id": "2510.27504v1",
      "title": "DP-FedPGN: Finding Global Flat Minima for Differentially Private Federated Learning via Penalizing Gradient Norm",
      "title_zh": "DP-FedPGNï¼šé€šè¿‡æ¢¯åº¦èŒƒæ•°æƒ©ç½šå¯»æ‰¾å·®åˆ†éšç§è”é‚¦å­¦ä¹ çš„å…¨å±€å¹³å¦æå°å€¼",
      "authors": [
        "Junkang Liu",
        "Yuxuan Tian",
        "Fanhua Shang",
        "Yuanyuan Liu",
        "Hongying Liu",
        "Junchao Zhou",
        "Daorui Ding"
      ],
      "abstract": "To prevent inference attacks in Federated Learning (FL) and reduce the leakage of sensitive information, Client-level Differentially Private Federated Learning (CL-DPFL) is widely used. However, current CL-DPFL methods usually result in sharper loss landscapes, which leads to a decrease in model generalization after differential privacy protection. By using Sharpness Aware Minimization (SAM), the current popular federated learning methods are to find a local flat minimum value to alleviate this problem. However, the local flatness may not reflect the global flatness in CL-DPFL. Therefore, to address this issue and seek global flat minima of models, we propose a new CL-DPFL algorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to the local loss to find the global flat minimum. Moreover, by using our global gradient norm penalty, we not only find a flatter global minimum but also reduce the locally updated norm, which means that we further reduce the error of gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN mitigates the performance degradation caused by DP. Meanwhile, the proposed DP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves fast convergence. We also use RÃ©nyi DP to provide strict privacy guarantees and provide sensitivity analysis for local updates. Finally, we conduct effectiveness tests on both ResNet and Transformer models, and achieve significant improvements in six visual and natural language processing tasks compared to existing state-of-the-art algorithms. The code is available at https://github.com/junkangLiu0/DP-FedPGN",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®¢æˆ·ç«¯çº§å·®åˆ†éšç§è”é‚¦å­¦ä¹  (Client-level Differentially Private Federated Learning, CL-DPFL) ä¸­æ¨¡å‹æ³›åŒ–èƒ½åŠ›å› æŸå¤±å¹³é¢å˜é™¡è€Œä¸‹é™çš„é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰ Sharpness Aware Minimization (SAM) æ–¹æ³•ä»…å…³æ³¨å±€éƒ¨å¹³å¦åº¦è€Œå¿½ç•¥å…¨å±€å¹³å¦åº¦çš„å±€é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† DP-FedPGN ç®—æ³•ï¼Œé€šè¿‡åœ¨å±€éƒ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥å…¨å±€æ¢¯åº¦èŒƒæ•°æƒ©ç½š (Global Gradient Norm Penalty) æ¥å¯»æ‰¾æ¨¡å‹çš„å…¨å±€å¹³å¦æå°å€¼ (Global Flat Minima)ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½å‘ç°æ›´å¹³å¦çš„å…¨å±€è§£ï¼Œè¿˜èƒ½æœ‰æ•ˆå‡å°å±€éƒ¨æ›´æ–°èŒƒæ•°ï¼Œä»è€Œé™ä½å·®åˆ†éšç§ä¸­æ¢¯åº¦è£å‰ª (Gradient Clipping) å¸¦æ¥çš„è¯¯å·®ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒDP-FedPGN èƒ½å¤Ÿç¼“è§£å·®åˆ†éšç§é€ æˆçš„æ€§èƒ½è¡°å‡ï¼Œæ¶ˆé™¤æ•°æ®å¼‚æ„æ€§ (Data Heterogeneity) çš„å½±å“å¹¶å®ç°å¿«é€Ÿæ”¶æ•›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨ RÃ©nyi DP æä¾›äº†ä¸¥æ ¼çš„éšç§ä¿è¯ï¼Œå¹¶å¯¹å±€éƒ¨æ›´æ–°è¿›è¡Œäº†çµæ•åº¦åˆ†æã€‚åœ¨ ResNet å’Œ Transformer æ¨¡å‹ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒDP-FedPGN åœ¨å…­é¡¹è§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¼˜äºç°æœ‰æœ€å…ˆè¿›ç®—æ³•çš„æ˜¾è‘—æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.27504v1",
      "published_date": "2025-10-31 14:28:31 UTC",
      "updated_date": "2025-10-31 14:28:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:18.330580+00:00"
    },
    {
      "arxiv_id": "2511.07436v1",
      "title": "Analysing Environmental Efficiency in AI for X-Ray Diagnosis",
      "title_zh": "X å°„çº¿è¯Šæ–­ä¸­ AI çš„ç¯å¢ƒæ•ˆç‡åˆ†æ",
      "authors": [
        "Liam Kearns"
      ],
      "abstract": "The integration of AI tools into medical applications has aimed to improve the efficiency of diagnosis. The emergence of large language models (LLMs), such as ChatGPT and Claude, has expanded this integration even further. Because of LLM versatility and ease of use through APIs, these larger models are often utilised even though smaller, custom models can be used instead. In this paper, LLMs and small discriminative models are integrated into a Mendix application to detect Covid-19 in chest X-rays. These discriminative models are also used to provide knowledge bases for LLMs to improve accuracy. This provides a benchmark study of 14 different model configurations for comparison of accuracy and environmental impact. The findings indicated that while smaller models reduced the carbon footprint of the application, the output was biased towards a positive diagnosis and the output probabilities were lacking confidence. Meanwhile, restricting LLMs to only give probabilistic output caused poor performance in both accuracy and carbon footprint, demonstrating the risk of using LLMs as a universal AI solution. While using the smaller LLM GPT-4.1-Nano reduced the carbon footprint by 94.2% compared to the larger models, this was still disproportionate to the discriminative models; the most efficient solution was the Covid-Net model. Although it had a larger carbon footprint than other small models, its carbon footprint was 99.9% less than when using GPT-4.5-Preview, whilst achieving an accuracy of 95.5%, the highest of all models examined. This paper contributes to knowledge by comparing generative and discriminative models in Covid-19 detection as well as highlighting the environmental risk of using generative tools for classification tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è¯„ä¼°Xå°„çº¿è¯Šæ–­ä¸­äººå·¥æ™ºèƒ½å·¥å…·çš„ç¯å¢ƒæ•ˆç‡ï¼Œå¯¹æ¯”äº†14ç§æ¨¡å‹é…ç½®åœ¨æ£€æµ‹Covid-19æ—¶çš„å‡†ç¡®ç‡ä¸ç¢³è¶³è¿¹(carbon footprint)ã€‚ç ”ç©¶åˆ†æäº†åŒ…æ‹¬GPT-4.5-Previewåœ¨å†…çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸åˆ¤åˆ«å¼æ¨¡å‹(discriminative models)çš„è¡¨ç°ï¼Œå¹¶æ¢è®¨äº†å°†åˆ¤åˆ«å¼æ¨¡å‹ä½œä¸ºLLMsçŸ¥è¯†åº“çš„å¢ç›Šæ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å°å‹æ¨¡å‹æœ‰åŠ©äºå‡æ’ï¼Œä½†å…¶è¯Šæ–­ç»“æœå¾€å¾€å­˜åœ¨åè§ä¸”ä¿¡å¿ƒä¸è¶³ï¼Œè€Œç›²ç›®å°†LLMsä½œä¸ºé€šç”¨è§£å†³æ–¹æ¡ˆåˆ™ä¼šå¸¦æ¥é«˜æ˜‚çš„ç¯å¢ƒæˆæœ¬ã€‚å®éªŒè¯æ˜Covid-Netåœ¨ç»´æŒ95.5%æœ€é«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå…¶ç¢³è¶³è¿¹æ¯”GPT-4.5-Previewé™ä½äº†99.9%ï¼Œæ˜¯æ•ˆç‡æœ€é«˜çš„æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶é€šè¿‡å¯¹æ¯”ç”Ÿæˆå¼(generative)ä¸åˆ¤åˆ«å¼æ¨¡å‹ï¼Œé‡åŒ–äº†åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨ç”Ÿæˆå¼AIçš„ç¯å¢ƒé£é™©ã€‚è¿™é¡¹å·¥ä½œä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½åœ¨è¿½æ±‚é«˜æ€§èƒ½çš„åŒæ—¶å®ç°ç¯å¢ƒå¯æŒç»­æ€§æä¾›äº†å…³é”®çš„æ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.07436v1",
      "published_date": "2025-10-31 14:19:57 UTC",
      "updated_date": "2025-10-31 14:19:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:28.028648+00:00"
    },
    {
      "arxiv_id": "2510.27497v1",
      "title": "InertialAR: Autoregressive 3D Molecule Generation with Inertial Frames",
      "title_zh": "InertialARï¼šåŸºäºæƒ¯æ€§ç³»çš„è‡ªå›å½’ 3D åˆ†å­ç”Ÿæˆ",
      "authors": [
        "Haorui Li",
        "Weitao Du",
        "Yuqiang Li",
        "Hongyu Guo",
        "Shengchao Liu"
      ],
      "abstract": "Transformer-based autoregressive models have emerged as a unifying paradigm across modalities such as text and images, but their extension to 3D molecule generation remains underexplored. The gap stems from two fundamental challenges: (1) tokenizing molecules into a canonical 1D sequence of tokens that is invariant to both SE(3) transformations and atom index permutations, and (2) designing an architecture capable of modeling hybrid atom-based tokens that couple discrete atom types with continuous 3D coordinates. To address these challenges, we introduce InertialAR. InertialAR devises a canonical tokenization that aligns molecules to their inertial frames and reorders atoms to ensure SE(3) and permutation invariance. Moreover, InertialAR equips the attention mechanism with geometric awareness via geometric rotary positional encoding (GeoRoPE). In addition, it utilizes a hierarchical autoregressive paradigm to predict the next atom-based token, predicting the atom type first and then its 3D coordinates via Diffusion loss. Experimentally, InertialAR achieves state-of-the-art performance on 7 of the 10 evaluation metrics for unconditional molecule generation across QM9, GEOM-Drugs, and B3LYP. Moreover, it significantly outperforms strong baselines in controllable generation for targeted chemical functionality, attaining state-of-the-art results across all 5 metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† InertialARï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³ 3D åˆ†å­ç”ŸæˆæŒ‘æˆ˜çš„è‡ªå›å½’æ¨¡å‹ï¼Œé‡ç‚¹æ”»å…‹äº† SE(3) ä¸å˜æ€§å’ŒåŸå­ç´¢å¼•ç½®æ¢ä¸å˜æ€§ç­‰æ ¸å¿ƒéš¾é¢˜ã€‚InertialAR é€šè¿‡å°†åˆ†å­ä¸å…¶æƒ¯æ€§åæ ‡ç³»ï¼ˆinertial framesï¼‰å¯¹é½å¹¶é‡æ–°æ’åˆ—åŸå­é¡ºåºï¼Œå®ç°äº†ä¸€ç§è§„èŒƒçš„æ ‡è®°åŒ–ï¼ˆcanonical tokenizationï¼‰æ–¹æ³•ï¼Œç¡®ä¿äº†æ•°æ®çš„å‡ ä½•ä¸€è‡´æ€§ã€‚åœ¨æ¶æ„è®¾è®¡ä¸Šï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†å‡ ä½•æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆGeoRoPEï¼‰ä»¥å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶çš„å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚è‡ªå›å½’èŒƒå¼ï¼Œå…ˆé¢„æµ‹åŸå­ç±»å‹ï¼Œå†é€šè¿‡æ‰©æ•£æŸå¤±ï¼ˆDiffusion lossï¼‰æ¥ç¡®å®šåŸå­çš„ 3D åæ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInertialAR åœ¨ QM9ã€GEOM-Drugs å’Œ B3LYP ç­‰æ•°æ®é›†çš„æ— æ¡ä»¶åˆ†å­ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œåœ¨ 10 é¡¹è¯„ä¼°æŒ‡æ ‡ä¸­çš„ 7 é¡¹è¾¾åˆ°äº†æœ€ä¼˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨é’ˆå¯¹ç‰¹å®šåŒ–å­¦åŠŸèƒ½çš„å—æ§ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨æ‰€æœ‰ 5 é¡¹æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œç¡®ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27497v1",
      "published_date": "2025-10-31 14:19:50 UTC",
      "updated_date": "2025-10-31 14:19:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:25.631121+00:00"
    },
    {
      "arxiv_id": "2511.00136v1",
      "title": "A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control",
      "title_zh": "åŸºäº Herald å¼•å¯¼æç¤ºçš„å¹¶è¡Œç»†ç²’åº¦äº¤é€šä¿¡å·æ§åˆ¶åŒå¤§è¯­è¨€æ¨¡å‹æ¶æ„",
      "authors": [
        "Qing Guo",
        "Xinhang Li",
        "Junyu Chen",
        "Zheng Guo",
        "Xiaocong Li",
        "Lin Zhang",
        "Lei Li"
      ],
      "abstract": "Leveraging large language models (LLMs) in traffic signal control (TSC) improves optimization efficiency and interpretability compared to traditional reinforcement learning (RL) methods. However, existing LLM-based approaches are limited by fixed time signal durations and are prone to hallucination errors, while RL methods lack robustness in signal timing decisions and suffer from poor generalization. To address these challenges, this paper proposes HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The Herald Module extracts contextual information and forecasts queue lengths for each traffic phase based on real-time conditions. The first LLM, LLM-Agent, uses these forecasts to make fine grained traffic signal control, while the second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and hallucinations. These refined outputs are used for score-based fine-tuning to improve accuracy and robustness. Simulation experiments using CityFlow on real world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New York (196) demonstrate that HeraldLight outperforms state of the art baselines, achieving a 20.03% reduction in average travel time across all scenarios and a 10.74% reduction in average queue length on the Jinan and Hangzhou scenarios. The source code is available on GitHub: https://github.com/BUPT-ANTlab/HeraldLight.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HeraldLightï¼Œä¸€ç§é‡‡ç”¨Heraldå¼•å¯¼æç¤ºçš„åŒå¤§è¯­è¨€æ¨¡å‹(Dual Large Language Models)æ¶æ„ï¼Œæ—¨åœ¨è§£å†³äº¤é€šä¿¡å·æ§åˆ¶(Traffic Signal Control)é¢†åŸŸä¸­ç°æœ‰æ¨¡å‹å­˜åœ¨çš„å¹»è§‰è¯¯å·®ã€å›ºå®šä¿¡å·æ—¶é•¿åŠå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ³›åŒ–æ€§å·®ç­‰éš¾é¢˜ã€‚è¯¥æ¶æ„å¼•å…¥Herald Moduleæå–å®æ—¶ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶é¢„æµ‹å„ç›¸ä½æ’é˜Ÿé•¿åº¦ï¼Œç”±LLM-Agentæ‰§è¡Œç»†ç²’åº¦ä¿¡å·æ§åˆ¶å†³ç­–ï¼Œå¹¶ç”±LLM-Criticè¿›è¡Œç»“æœä¿®æ­£ä¸å»å¹»è§‰å¤„ç†ã€‚ç³»ç»Ÿé€šè¿‡åŸºäºå¾—åˆ†çš„å¾®è°ƒ(score-based fine-tuning)è¿›ä¸€æ­¥ä¼˜åŒ–äº†å†³ç­–çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚åœ¨åŸºäºCityFlowçš„224ä¸ªçœŸå®è·¯å£ä»¿çœŸå®éªŒä¸­ï¼ŒHeraldLightæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œå°†å…¨åœºæ™¯å¹³å‡è¡Œç¨‹æ—¶é—´é™ä½äº†20.03%ï¼Œå¹¶åœ¨æµå—å’Œæ­å·åœºæ™¯ä¸­å‡å°‘äº†10.74%çš„å¹³å‡æ’é˜Ÿé•¿åº¦ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åŒLLMæ¶æ„åœ¨æå‡äº¤é€šä¼˜åŒ–æ•ˆç‡ã€å¯è§£é‡Šæ€§åŠæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00136v1",
      "published_date": "2025-10-31 14:05:08 UTC",
      "updated_date": "2025-10-31 14:05:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:31.524469+00:00"
    },
    {
      "arxiv_id": "2510.27486v2",
      "title": "FedAdamW: A Communication-Efficient Optimizer with Convergence and Generalization Guarantees for Federated Large Models",
      "title_zh": "FedAdamWï¼šå…·æœ‰æ”¶æ•›æ€§ä¸æ³›åŒ–æ€§ä¿éšœçš„è”é‚¦å¤§æ¨¡å‹é€šä¿¡é«˜æ•ˆä¼˜åŒ–å™¨",
      "authors": [
        "Junkang Liu",
        "Fanhua Shang",
        "Kewen Zhu",
        "Hongying Liu",
        "Yuanyuan Liu",
        "Jin Liu"
      ],
      "abstract": "AdamW has become one of the most effective optimizers for training large-scale models. We have also observed its effectiveness in the context of federated learning (FL). However, directly applying AdamW in federated learning settings poses significant challenges: (1) due to data heterogeneity, AdamW often yields high variance in the second-moment estimate $\\boldsymbol{v}$; (2) the local overfitting of AdamW may cause client drift; and (3) Reinitializing moment estimates ($\\boldsymbol{v}$, $\\boldsymbol{m}$) at each round slows down convergence. To address these challenges, we propose the first \\underline{Fed}erated \\underline{AdamW} algorithm, called \\texttt{FedAdamW}, for training and fine-tuning various large models. \\texttt{FedAdamW} aligns local updates with the global update using both a \\textbf{local correction mechanism} and decoupled weight decay to mitigate local overfitting. \\texttt{FedAdamW} efficiently aggregates the \\texttt{mean} of the second-moment estimates to reduce their variance and reinitialize them. Theoretically, we prove that \\texttt{FedAdamW} achieves a linear speedup convergence rate of $\\mathcal{O}(\\sqrt{(L Î”Ïƒ_l^2)/(S K R Îµ^2)}+(L Î”)/R)$ without \\textbf{heterogeneity assumption}, where $S$ is the number of participating clients per round, $K$ is the number of local iterations, and $R$ is the total number of communication rounds. We also employ PAC-Bayesian generalization analysis to explain the effectiveness of decoupled weight decay in local training. Empirically, we validate the effectiveness of \\texttt{FedAdamW} on language and vision Transformer models. Compared to several baselines, \\texttt{FedAdamW} significantly reduces communication rounds and improves test accuracy. The code is available in https://github.com/junkangLiu0/FedAdamW.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹æ¨¡å‹è®­ç»ƒä¸­è¡¨ç°ä¼˜å¼‚çš„ AdamW ä¼˜åŒ–å™¨åœ¨è”é‚¦å­¦ä¹  (Federated Learning) ç¯å¢ƒä¸‹å› æ•°æ®å¼‚æ„æ€§å¯¼è‡´çš„äºŒé˜¶çŸ©ä¼°è®¡é«˜æ–¹å·®ã€å±€éƒ¨è¿‡æ‹Ÿåˆå¼•èµ·çš„å®¢æˆ·ç«¯åç§» (client drift) ä»¥åŠé‡æ–°åˆå§‹åŒ–å¸¦æ¥çš„æ”¶æ•›ç¼“æ…¢ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º FedAdamW çš„è”é‚¦ä¼˜åŒ–ç®—æ³•ã€‚FedAdamW é€šè¿‡ç»“åˆå±€éƒ¨ä¿®æ­£æœºåˆ¶ (local correction mechanism) å’Œè§£è€¦æƒé‡è¡°å‡ (decoupled weight decay) æ¥ä½¿å±€éƒ¨æ›´æ–°ä¸å…¨å±€æ›´æ–°ä¿æŒä¸€è‡´ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£å±€éƒ¨è¿‡æ‹Ÿåˆé—®é¢˜ã€‚åŒæ—¶ï¼Œè¯¥ç®—æ³•é€šè¿‡èšåˆäºŒé˜¶çŸ©ä¼°è®¡çš„å‡å€¼æ¥é™ä½æ–¹å·®å¹¶ä¼˜åŒ–é‡æ–°åˆå§‹åŒ–è¿‡ç¨‹ã€‚åœ¨ç†è®ºå±‚é¢ï¼Œç ”ç©¶è¯æ˜äº† FedAdamW åœ¨æ— éœ€å¼‚æ„æ€§å‡è®¾çš„æƒ…å†µä¸‹å¯å®ç°çº¿æ€§åŠ é€Ÿæ”¶æ•›ç‡ï¼Œå¹¶åˆ©ç”¨ PAC-Bayesian æ³›åŒ–åˆ†æè§£é‡Šäº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¯­è¨€å’Œè§†è§‰ Transformer æ¨¡å‹ä¸Šï¼ŒFedAdamW ç›¸æ¯”åŸºçº¿æ¨¡å‹æ˜¾è‘—å‡å°‘äº†é€šä¿¡è½®æ¬¡å¹¶æå‡äº†æµ‹è¯•å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27486v2",
      "published_date": "2025-10-31 14:04:43 UTC",
      "updated_date": "2025-11-10 16:37:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:30.125094+00:00"
    },
    {
      "arxiv_id": "2510.27484v1",
      "title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling",
      "title_zh": "æ€ç»´åˆ†æ”¯ï¼šè§£é‡Šå¤§è¯­è¨€æ¨¡å‹æ¨ç†éœ€è¿›è¡Œé‡é‡‡æ ·",
      "authors": [
        "Uzay Macar",
        "Paul C. Bogdan",
        "Senthooran Rajamanoharan",
        "Neel Nanda"
      ],
      "abstract": "Most work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate for understanding causal influence and the underlying computation. Though fully specifying this distribution is intractable, it can be understood by sampling. We present case studies using resampling to investigate model decisions. First, when a model states a reason for its action, does that reason actually cause the action? In \"agentic misalignment\" scenarios, we resample specific sentences to measure their downstream effects. Self-preservation sentences have small causal impact, suggesting they do not meaningfully drive blackmail. Second, are artificial edits to CoT sufficient for steering reasoning? These are common in literature, yet take the model off-policy. Resampling and selecting a completion with the desired property is a principled on-policy alternative. We find off-policy interventions yield small and unstable effects compared to resampling in decision-making tasks. Third, how do we understand the effect of removing a reasoning step when the model may repeat it post-edit? We introduce a resilience metric that repeatedly resamples to prevent similar content from reappearing downstream. Critical planning statements resist removal but have large effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can our methods teach us anything in these settings? Adapting causal mediation analysis, we find that hints that have a causal effect on the output without being explicitly mentioned exert a subtle and cumulative influence on the CoT that persists even if the hint is removed. Overall, studying distributions via resampling enables reliable causal analysis, clearer narratives of model reasoning, and principled CoT interventions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ¨ç†è¿‡ç¨‹ä¸­çš„è§£é‡Šæ€§é—®é¢˜ï¼ŒæŒ‡å‡ºä»…ç ”ç©¶å•ä¸ªé“¾å¼æ€ç»´(Chain-of-Thought, CoT)ä¸è¶³ä»¥ç†è§£å…¶èƒŒåçš„å› æœå½±å“ï¼Œå¹¶æå‡ºé€šè¿‡é‡é‡‡æ ·(Resampling)æ¥åˆ†ææ¨ç†è·¯å¾„åˆ†å¸ƒçš„æ–¹æ³•ã€‚é€šè¿‡å››ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œä½œè€…å‘ç°åœ¨â€œæ™ºèƒ½ä½“å¤±å‡†(agentic misalignment)â€åœºæ™¯ä¸‹ï¼Œæ¨¡å‹æåˆ°çš„è‡ªæˆ‘ä¿å­˜ç†ç”±å…¶å®é™…å› æœå½±å“å¾®ä¹å…¶å¾®ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œç›¸è¾ƒäºå¸¸è§çš„ç¦»ç­–(off-policy)äººå·¥ç¼–è¾‘ï¼ŒåŸºäºé‡é‡‡æ ·çš„åœ¨ç­–(on-policy)å¹²é¢„åœ¨å¼•å¯¼æ¨ç†å†³ç­–æ–¹é¢æ›´ä¸ºç¨³å®šæœ‰æ•ˆã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¼•å…¥äº†éŸ§æ€§(resilience)æŒ‡æ ‡æ¥è¡¡é‡ç§»é™¤æ¨ç†æ­¥éª¤çš„çœŸå®æ•ˆåº”ï¼Œå‘ç°å…³é”®è®¡åˆ’æ­¥éª¤è™½ç„¶éš¾ä»¥é€šè¿‡ç®€å•åˆ é™¤æ¥æŠ‘åˆ¶ï¼Œä½†ä¸€æ—¦æ¶ˆé™¤ä¼šå¯¹è¾“å‡ºäº§ç”Ÿå·¨å¤§å½±å“ã€‚é’ˆå¯¹â€œä¸å¿ å®â€çš„CoTï¼Œå› æœä¸­ä»‹åˆ†æ(causal mediation analysis)æ­ç¤ºäº†éšå«æç¤ºå¯¹æ¨ç†è¿‡ç¨‹å…·æœ‰å¾®å¦™ä¸”æŒç»­çš„ç´¯ç§¯å½±å“ã€‚æ€»ä¹‹ï¼Œè¯¥ç ”ç©¶é€šè¿‡åˆ†æCoTåˆ†å¸ƒï¼Œä¸ºå®ç°å¯é çš„å› æœåˆ†æã€æ›´æ¸…æ™°çš„æ¨ç†å™è¿°ä»¥åŠåŸåˆ™æ€§çš„CoTå¹²é¢„å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Uzay Macar and Paul C. Bogdan contributed equally to this work, and their listed order was determined by coinflip",
      "pdf_url": "https://arxiv.org/pdf/2510.27484v1",
      "published_date": "2025-10-31 14:02:37 UTC",
      "updated_date": "2025-10-31 14:02:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:40.330353+00:00"
    },
    {
      "arxiv_id": "2511.01907v1",
      "title": "Between Myths and Metaphors: Rethinking LLMs for SRH in Conservative Contexts",
      "title_zh": "æ¸¸èµ°äºç¥è¯ä¸éšå–»ä¹‹é—´ï¼šä¿å®ˆè¯­å¢ƒä¸‹æ€§ä¸ç”Ÿæ®–å¥åº·å¤§è¯­è¨€æ¨¡å‹çš„å†æ€è€ƒ",
      "authors": [
        "Ameemah Humayun",
        "Bushra Zubair",
        "Maryam Mustafa"
      ],
      "abstract": "Low-resource countries represent over 90% of maternal deaths, with Pakistan among the top four countries contributing nearly half in 2023. Since these deaths are mostly preventable, large language models (LLMs) can help address this crisis by automating health communication and risk assessment. However, sexual and reproductive health (SRH) communication in conservative contexts often relies on indirect language that obscures meaning, complicating LLM-based interventions. We conduct a two-stage study in Pakistan: (1) analyzing data from clinical observations, interviews, and focus groups with clinicians and patients, and (2) evaluating the interpretive capabilities of five popular LLMs on this data. Our analysis identifies two axes of communication (referential domain and expression approach) and shows LLMs struggle with semantic drift, myths, and polysemy in clinical interactions. We contribute: (1) empirical themes in SRH communication, (2) a categorization framework for indirect communication, (3) evaluation of LLM performance, and (4) design recommendations for culturally-situated SRH communication.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¾…åŠ©æ€§ä¸ç”Ÿæ®–å¥åº·ï¼ˆSRHï¼‰æ²Ÿé€šæ—¶ï¼Œå¦‚ä½•åº”å¯¹ä¿å®ˆç¤¾ä¼šè¯­å¢ƒä¸­å› é—´æ¥è¯­è¨€å¯¼è‡´çš„ç†è§£æŒ‘æˆ˜ã€‚ç ”ç©¶è€…åœ¨å·´åŸºæ–¯å¦å¼€å±•äº†ä¸¤é˜¶æ®µå®è¯ç ”ç©¶ï¼Œé¦–å…ˆé€šè¿‡ä¸´åºŠè§‚å¯Ÿã€æ·±åº¦è®¿è°ˆå’Œç„¦ç‚¹å°ç»„æ”¶é›†æ•°æ®ï¼Œéšåè¯„ä¼°äº†äº”ç§ä¸»æµ LLMs åœ¨è§£æè¿™äº›å¤æ‚åŒ»ç–—æ²Ÿé€šæ•°æ®æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶è¯†åˆ«äº†æ²Ÿé€šçš„ä¸¤ä¸ªå…³é”®ç»´åº¦ï¼Œå³å‚è€ƒé¢†åŸŸï¼ˆreferential domainï¼‰å’Œè¡¨è¾¾æ–¹å¼ï¼ˆexpression approachï¼‰ï¼Œå¹¶æ­ç¤ºäº† LLMs åœ¨å¤„ç†è¯­ä¹‰æ¼‚ç§»ï¼ˆsemantic driftï¼‰ã€è¿·æ€ï¼ˆmythsï¼‰å’Œå¤šä¹‰æ€§ï¼ˆpolysemyï¼‰æ—¶å­˜åœ¨çš„å±€é™ã€‚è¯¥å·¥ä½œçš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬æ€»ç»“äº† SRH æ²Ÿé€šçš„å®è¯ä¸»é¢˜ï¼Œæ„å»ºäº†é—´æ¥æ²Ÿé€šï¼ˆindirect communicationï¼‰çš„åˆ†ç±»æ¡†æ¶ï¼Œå¹¶ä¸ºå¼€å‘ç¬¦åˆç‰¹å®šæ–‡åŒ–èƒŒæ™¯çš„ SRH æ²Ÿé€šå·¥å…·æä¾›äº†è®¾è®¡å»ºè®®ã€‚è¿™ä¸€ç ”ç©¶å¯¹äºåœ¨ä½èµ„æºå›½å®¶åˆ©ç”¨ AI æŠ€æœ¯è‡ªåŠ¨åŒ–å¥åº·è¯„ä¼°ã€ç¼“è§£é«˜å­•äº§å¦‡æ­»äº¡ç‡å±æœºå…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01907v1",
      "published_date": "2025-10-31 13:39:56 UTC",
      "updated_date": "2025-10-31 13:39:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:56.334840+00:00"
    },
    {
      "arxiv_id": "2511.10650v1",
      "title": "Unsupervised Cycle Detection in Agentic Applications",
      "title_zh": "æ™ºèƒ½ä½“åº”ç”¨ä¸­çš„æ— ç›‘ç£ç¯è·¯æ£€æµ‹",
      "authors": [
        "Felix George",
        "Harshit Kumar",
        "Divya Pathak",
        "Kaustabha Ray",
        "Mudit Verma",
        "Pratibha Moogi"
      ],
      "abstract": "Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”±å¤§è¯­è¨€æ¨¡å‹(Large Language Models)é©±åŠ¨çš„æ™ºèƒ½ä½“åº”ç”¨(Agentic applications)ä¸­å› éç¡®å®šæ€§è¡Œä¸ºå½¢æˆçš„éšè—æ‰§è¡Œå¾ªç¯(hidden execution cycles)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆç»“æ„åŒ–ä¸è¯­ä¹‰åˆ†æçš„æ— ç›‘ç£å¾ªç¯æ£€æµ‹(unsupervised cycle detection)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡é«˜æ•ˆçš„æ—¶é—´è°ƒç”¨æ ˆåˆ†æ(temporal call stack analysis)è¯†åˆ«æ˜¾å¼å¾ªç¯ï¼Œå†åˆ©ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ(semantic similarity analysis)æ­ç¤ºå› å†—ä½™å†…å®¹ç”Ÿæˆè€Œäº§ç”Ÿçš„éšå½¢å¾ªç¯ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè§‚æµ‹å¹³å°éš¾ä»¥å¯Ÿè§‰çš„èµ„æºæµªè´¹é—®é¢˜ã€‚åœ¨åŒ…å«1575æ¡LangGraphè½¨è¿¹çš„è‚¡å¸‚åº”ç”¨æ•°æ®é›†ä¸Šï¼Œè¯¥æ··åˆæ–¹æ³•çš„F1åˆ†æ•°è¾¾åˆ°0.72ï¼Œå‡†ç¡®ç‡å’Œå¬å›ç‡åˆ†åˆ«ä¸º0.62å’Œ0.86ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºå•ä¸€çš„ç»“æ„åŒ–æˆ–è¯­ä¹‰æ£€æµ‹æ‰‹æ®µã€‚å°½ç®¡è¯¥ç ”ç©¶ä¸ºè¯†åˆ«æ™ºèƒ½ä½“åº”ç”¨ä¸­çš„ä½æ•ˆè¡Œä¸ºå¥ å®šäº†åŸºç¡€ï¼Œä½†ä½œè€…ä¹ŸæŒ‡å‡ºè¯¥é¢†åŸŸä»æœ‰æ”¹è¿›ç©ºé—´ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ£€æµ‹ç²¾åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10650v1",
      "published_date": "2025-10-31 13:27:53 UTC",
      "updated_date": "2025-10-31 13:27:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:20:57.425635+00:00"
    },
    {
      "arxiv_id": "2510.27462v1",
      "title": "VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision",
      "title_zh": "VCOREï¼šé¢å‘æ€ç»´é“¾ç›‘ç£çš„æ–¹å·®æ§åˆ¶ä¼˜åŒ–é‡åŠ æƒ",
      "authors": [
        "Xuan Gong",
        "Senmiao Wang",
        "Hanbo Huang",
        "Ruoyu Sun",
        "Shiyu Liang"
      ],
      "abstract": "Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has emerged as a crucial technique for enhancing the reasoning abilities of large language models (LLMs). However, the standard cross-entropy loss treats all tokens equally, ignoring their heterogeneous contributions across a reasoning trajectory. This uniform treatment leads to misallocated supervision and weak generalization, especially in complex, long-form reasoning tasks. To address this, we introduce \\textbf{V}ariance-\\textbf{C}ontrolled \\textbf{O}ptimization-based \\textbf{RE}weighting (VCORE), a principled framework that reformulates CoT supervision as a constrained optimization problem. By adopting an optimization-theoretic perspective, VCORE enables a principled and adaptive allocation of supervision across tokens, thereby aligning the training objective more closely with the goal of robust reasoning generalization. Empirical evaluations demonstrate that VCORE consistently outperforms existing token reweighting methods. Across both in-domain and out-of-domain settings, VCORE achieves substantial performance gains on mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B, 32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more effective initialization for subsequent reinforcement learning, establishing a stronger foundation for advancing the reasoning capabilities of LLMs. The Code will be released at https://github.com/coder-gx/VCORE.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿é“¾å¼æ€ç»´ (Chain-of-Thought, CoT) è½¨è¿¹åœ¨ç›‘ç£å¾®è°ƒ (SFT) ä¸­å› æ‰€æœ‰ Token æƒé‡å‡ç­‰åˆ†é…è€Œå¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›å—é™é—®é¢˜ï¼Œæå‡ºäº† VCORE (Variance-Controlled Optimization-based REweighting) æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»ä¼˜åŒ–ç†è®ºè§†è§’å‡ºå‘ï¼Œå°† CoT ç›‘ç£é‡æ–°æ„å»ºä¸ºå—çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ï¼Œå®ç°äº†è·¨ Token çš„åŸåˆ™æ€§è‡ªé€‚åº”æƒé‡åˆ†é…ï¼Œä»è€Œä½¿è®­ç»ƒç›®æ ‡ä¸é²æ£’çš„æ¨ç†æ³›åŒ–ç›®æ ‡ç´§å¯†å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVCORE åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºç°æœ‰çš„ Token é‡åŠ æƒæ–¹æ³•ï¼Œåœ¨ Qwen3 å’Œ LLaMA-3.1 ç­‰ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯æ˜ VCORE å¯ä»¥ä½œä¸ºåç»­å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ›´ä¸ºæœ‰æ•ˆçš„åˆå§‹åŒ–æ‰‹æ®µï¼Œä¸ºè¿›ä¸€æ­¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2510.27462v1",
      "published_date": "2025-10-31 13:19:24 UTC",
      "updated_date": "2025-10-31 13:19:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:01.120892+00:00"
    },
    {
      "arxiv_id": "2510.27448v1",
      "title": "GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data Generation through Formal Language",
      "title_zh": "GeoFMï¼šé€šè¿‡å½¢å¼åŒ–è¯­è¨€åˆæˆæ•°æ®ç”Ÿæˆå¢å¼º MLLMs çš„å‡ ä½•æ¨ç†èƒ½åŠ›",
      "authors": [
        "Yuhao Zhang",
        "Dingxin Hu",
        "Tinghao Yu",
        "Hao Liu",
        "Yiting Liu"
      ],
      "abstract": "Multi-modal Large Language Models (MLLMs) have gained significant attention in both academia and industry for their capabilities in handling multi-modal tasks. However, these models face challenges in mathematical geometric reasoning due to the scarcity of high-quality geometric data. To address this issue, synthetic geometric data has become an essential strategy. Current methods for generating synthetic geometric data involve rephrasing or expanding existing problems and utilizing predefined rules and templates to create geometric images and problems. However, these approaches often produce data that lacks diversity or is prone to noise. Additionally, the geometric images synthesized by existing methods tend to exhibit limited variation and deviate significantly from authentic geometric diagrams. To overcome these limitations, we propose GeoFM, a novel method for synthesizing geometric data. GeoFM uses formal languages to explore combinations of conditions within metric space, generating high-fidelity geometric problems that differ from the originals while ensuring correctness through a symbolic engine. Experimental results show that our synthetic data significantly outperforms existing methods. The model trained with our data surpass the proprietary GPT-4o model by 18.7\\% on geometry problem-solving tasks in MathVista and by 16.5\\% on GeoQA. Additionally, it exceeds the performance of a leading open-source model by 5.7\\% on MathVista and by 2.7\\% on GeoQA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Multi-modal Large Language Models (MLLMs) åœ¨æ•°å­¦å‡ ä½•æ¨ç†ä¸­å› é«˜è´¨é‡æ•°æ®åŒ®ä¹è€Œé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º GeoFM çš„æ–°å‹å‡ ä½•æ•°æ®åˆæˆæ–¹æ³•ã€‚ç°æœ‰çš„åˆæˆæ•°æ®æ–¹æ³•å¾€å¾€ç”±äºä¾èµ–é¢„å®šä¹‰è§„åˆ™è€Œå¯¼è‡´æ•°æ®ç¼ºä¹å¤šæ ·æ€§ï¼Œä¸”ç”Ÿæˆçš„å›¾åƒä¸çœŸå®å‡ ä½•å›¾è¡¨å­˜åœ¨æ˜¾è‘—åå·®ã€‚GeoFM é€šè¿‡ Formal Language æ¢ç´¢åº¦é‡ç©ºé—´å†…çš„æ¡ä»¶ç»„åˆï¼Œç”Ÿæˆä¸åŸä½œä¸åŒä¸”å…·æœ‰é«˜ä¿çœŸåº¦çš„å‡ ä½•é—®é¢˜ï¼Œå¹¶åˆ©ç”¨ Symbolic Engine ç¡®ä¿å†…å®¹çš„æ­£ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¯¥æ–¹æ³•ç”Ÿæˆçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨å‡ ä½•æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œåœ¨ MathVista å’Œ GeoQA åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æ¯” GPT-4o é«˜å‡º 18.7% å’Œ 16.5%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ç›¸åŒä»»åŠ¡ä¸Šä¹Ÿæ˜¾è‘—è¶…è¶Šäº†é¢†å…ˆçš„å¼€æºæ¨¡å‹ï¼Œè¯æ˜äº† GeoFM åœ¨æå‡æ¨¡å‹å‡ ä½•é€»è¾‘æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27448v1",
      "published_date": "2025-10-31 12:56:32 UTC",
      "updated_date": "2025-10-31 12:56:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:00.722866+00:00"
    },
    {
      "arxiv_id": "2510.27442v1",
      "title": "CoMViT: An Efficient Vision Backbone for Supervised Classification in Medical Imaging",
      "title_zh": "CoMViTï¼šä¸€ç§é¢å‘åŒ»å­¦å½±åƒæœ‰ç›‘ç£åˆ†ç±»çš„é«˜æ•ˆè§†è§‰ä¸»å¹²ç½‘ç»œ",
      "authors": [
        "Aon Safdar",
        "Mohamed Saadeldin"
      ],
      "abstract": "Vision Transformers (ViTs) have demonstrated strong potential in medical imaging; however, their high computational demands and tendency to overfit on small datasets limit their applicability in real-world clinical scenarios. In this paper, we present CoMViT, a compact and generalizable Vision Transformer architecture optimized for resource-constrained medical image analysis. CoMViT integrates a convolutional tokenizer, diagonal masking, dynamic temperature scaling, and pooling-based sequence aggregation to improve performance and generalization. Through systematic architectural optimization, CoMViT achieves robust performance across twelve MedMNIST datasets while maintaining a lightweight design with only ~4.5M parameters. It matches or outperforms deeper CNN and ViT variants, offering up to 5-20x parameter reduction without sacrificing accuracy. Qualitative Grad-CAM analyses show that CoMViT consistently attends to clinically relevant regions despite its compact size. These results highlight the potential of principled ViT redesign for developing efficient and interpretable models in low-resource medical imaging settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision Transformers (ViTs) åœ¨åŒ»å­¦å½±åƒé¢†åŸŸé¢ä¸´çš„é«˜è®¡ç®—éœ€æ±‚åŠå°æ•°æ®é›†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæå‡ºäº†ç´§å‡‘ä¸”é€šç”¨çš„ CoMViT æ¶æ„ã€‚CoMViT é€šè¿‡é›†æˆå·ç§¯åˆ†è¯å™¨ (convolutional tokenizer)ã€å¯¹è§’çº¿æ©è”½ (diagonal masking)ã€åŠ¨æ€æ¸©åº¦ç¼©æ”¾ (dynamic temperature scaling) å’ŒåŸºäºæ± åŒ–çš„åºåˆ—èšåˆ (pooling-based sequence aggregation) ç­‰æŠ€æœ¯æ¥ä¼˜åŒ–æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ 12 ä¸ª MedMNIST æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ¶æ„ä»…éœ€çº¦ 4.5M å‚æ•°å³å¯åŒ¹é…æˆ–è¶…è¶Šæ›´æ·±å±‚çš„ CNN å’Œ ViT å˜ä½“ï¼Œå®ç°é«˜è¾¾ 5-20 å€çš„å‚æ•°ç¼©å‡ã€‚æ­¤å¤–ï¼ŒGrad-CAM å®šæ€§åˆ†æéªŒè¯äº†æ¨¡å‹èƒ½å¤Ÿå§‹ç»ˆå…³æ³¨ä¸´åºŠç›¸å…³åŒºåŸŸã€‚è¿™äº›ç»“æœè¯æ˜äº†é€šè¿‡å¯¹ ViT è¿›è¡ŒåŸåˆ™æ€§é‡æ–°è®¾è®¡ï¼Œå¯ä»¥åœ¨ä½èµ„æºåŒ»ç–—æˆåƒç¯å¢ƒä¸‹å¼€å‘å‡ºå…¼å…·é«˜æ•ˆæ€§ä¸å¯è§£é‡Šæ€§çš„è§†è§‰éª¨å¹²æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint (submitted manuscript). Accepted at the MICCAI 2025 MIRASOL Workshop; to appear in the Springer proceedings volume. This is the pre-review version (not the Version of Record). DOI will be added after publication. [Optional: 8 pages, 4 figures, 4 tables.]",
      "pdf_url": "https://arxiv.org/pdf/2510.27442v1",
      "published_date": "2025-10-31 12:49:13 UTC",
      "updated_date": "2025-10-31 12:49:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:09.522993+00:00"
    },
    {
      "arxiv_id": "2510.27432v1",
      "title": "Mitigating Semantic Collapse in Partially Relevant Video Retrieval",
      "title_zh": "ç¼“è§£éƒ¨åˆ†ç›¸å…³è§†é¢‘æ£€ç´¢ä¸­çš„è¯­ä¹‰å¡Œé™·",
      "authors": [
        "WonJun Moon",
        "MinSeok Jung",
        "Gilhan Park",
        "Tae-Young Kim",
        "Cheol-Ho Cho",
        "Woojin Jun",
        "Jae-Pil Heo"
      ],
      "abstract": "Partially Relevant Video Retrieval (PRVR) seeks videos where only part of the content matches a text query. Existing methods treat every annotated text-video pair as a positive and all others as negatives, ignoring the rich semantic variation both within a single video and across different videos. Consequently, embeddings of both queries and their corresponding video-clip segments for distinct events within the same video collapse together, while embeddings of semantically similar queries and segments from different videos are driven apart. This limits retrieval performance when videos contain multiple, diverse events. This paper addresses the aforementioned problems, termed as semantic collapse, in both the text and video embedding spaces. We first introduce Text Correlation Preservation Learning, which preserves the semantic relationships encoded by the foundation model across text queries. To address collapse in video embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastive alignment method that disentangles hierarchical video representations across temporal scales. Subsequently, we introduce order-preserving token merging and adaptive CBVA to enhance alignment by producing video segments that are internally coherent yet mutually distinctive. Extensive experiments on PRVR benchmarks demonstrate that our framework effectively prevents semantic collapse and substantially improves retrieval accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éƒ¨åˆ†ç›¸å…³è§†é¢‘æ£€ç´¢ (Partially Relevant Video Retrieval, PRVR) ä¸­çš„è¯­ä¹‰åç¼© (Semantic Collapse) é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰æ–¹æ³•å› å¿½ç•¥è§†é¢‘å†…å¤–å¤æ‚çš„è¯­ä¹‰å˜åŒ–ï¼Œå¯¼è‡´ä¸åŒäº‹ä»¶çš„åµŒå…¥å‘é‡è¿‡åº¦èšé›†æˆ–é”™è¯¯åˆ†ç¦»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…é¦–å…ˆæå‡ºäº†æ–‡æœ¬ç›¸å…³æ€§ä¿æŒå­¦ä¹  (Text Correlation Preservation Learning)ï¼Œä»¥ä¿ç•™åŸºç¡€æ¨¡å‹åœ¨æ–‡æœ¬æŸ¥è¯¢ä¸­ç¼–ç çš„åŸå§‹è¯­ä¹‰å…³ç³»ã€‚é’ˆå¯¹è§†é¢‘åµŒå…¥ç©ºé—´çš„åç¼©ï¼Œç ”ç©¶å¼•å…¥äº†è·¨åˆ†æ”¯è§†é¢‘å¯¹é½ (Cross-Branch Video Alignment, CBVA) è¿™ä¸€å¯¹æ¯”å¯¹é½æ–¹æ³•ï¼Œåœ¨ä¸åŒæ—¶é—´å°ºåº¦ä¸Šè§£è€¦å±‚æ¬¡åŒ–è§†é¢‘è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»“åˆé¡ºåºä¿æŒä»¤ç‰Œåˆå¹¶ (order-preserving token merging) å’Œè‡ªé€‚åº” CBVA æŠ€æœ¯ï¼Œè¯¥æ¡†æ¶å¢å¼ºäº†è§†é¢‘ç‰‡æ®µå†…éƒ¨çš„è¿è´¯æ€§ä¸å½¼æ­¤é—´çš„è¾¨è¯†åº¦ã€‚åœ¨ PRVR åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆæœ‰æ•ˆé˜²æ­¢äº†è¯­ä¹‰åç¼©å¹¶æ˜¾è‘—æå‡äº†è§†é¢‘æ£€ç´¢çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accpeted to NeurIPS 2025. Code is available at https://github.com/admins97/MSC_PRVR",
      "pdf_url": "https://arxiv.org/pdf/2510.27432v1",
      "published_date": "2025-10-31 12:39:20 UTC",
      "updated_date": "2025-10-31 12:39:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:07.827455+00:00"
    },
    {
      "arxiv_id": "2511.01906v1",
      "title": "Thinking Like a Student: AI-Supported Reflective Planning in a Theory-Intensive Computer Science Course",
      "title_zh": "åƒå­¦ç”Ÿä¸€æ ·æ€è€ƒï¼šç†è®ºå¯†é›†å‹è®¡ç®—æœºç§‘å­¦è¯¾ç¨‹ä¸­äººå·¥æ™ºèƒ½æ”¯æŒçš„åæ€å¼è§„åˆ’",
      "authors": [
        "Noa Izsak"
      ],
      "abstract": "In the aftermath of COVID-19, many universities implemented supplementary \"reinforcement\" roles to support students in demanding courses. Although the name for such roles may differ between institutions, the underlying idea of providing structured supplementary support is common. However, these roles were often poorly defined, lacking structured materials, pedagogical oversight, and integration with the core teaching team. This paper reports on the redesign of reinforcement sessions in a challenging undergraduate course on formal methods and computational models, using a large language model (LLM) as a reflective planning tool. The LLM was prompted to simulate the perspective of a second-year student, enabling the identification of conceptual bottlenecks, gaps in intuition, and likely reasoning breakdowns before classroom delivery. These insights informed a structured, repeatable session format combining targeted review, collaborative examples, independent student work, and guided walkthroughs. Conducted over a single semester, the intervention received positive student feedback, indicating increased confidence, reduced anxiety, and improved clarity, particularly in abstract topics such as the pumping lemma and formal language expressive power comparisons. The findings suggest that reflective, instructor-facing use of LLMs can enhance pedagogical design in theoretically dense domains and may be adaptable to other cognitively demanding computer science courses.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åœ¨é«˜åº¦ç†è®ºåŒ–çš„è®¡ç®—æœºç§‘å­¦è¯¾ç¨‹ä¸­ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºåæ€æ€§è§„åˆ’å·¥å…·ï¼ˆreflective planning toolï¼‰æ¥é‡æ–°è®¾è®¡è¡¥ä¹ ç¯èŠ‚ï¼ˆreinforcement sessionsï¼‰ã€‚é’ˆå¯¹ä»¥å¾€è¡¥ä¹ ç¯èŠ‚ç¼ºä¹ç»“æ„åŒ–ææ–™å’Œæ•™å­¦ç›‘ç®¡çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿä»¥å½¢å¼åŒ–æ–¹æ³•ï¼ˆformal methodsï¼‰å’Œè®¡ç®—æ¨¡å‹ï¼ˆcomputational modelsï¼‰è¯¾ç¨‹ä¸ºèƒŒæ™¯è¿›è¡Œäº†æ•™å­¦å¹²é¢„ã€‚å…¶æ ¸å¿ƒæ–¹æ³•æ˜¯æç¤ºå¤§è¯­è¨€æ¨¡å‹æ¨¡æ‹ŸäºŒå¹´çº§å­¦ç”Ÿçš„è§†è§’ï¼Œä»è€Œåœ¨è¯¾å ‚æ•™å­¦å‰è¯†åˆ«æ½œåœ¨çš„æ¦‚å¿µç“¶é¢ˆï¼ˆconceptual bottlenecksï¼‰ã€ç›´è§‰ç¼ºå¤±ä»¥åŠå¯èƒ½çš„æ¨ç†å´©æºƒï¼ˆreasoning breakdownsï¼‰ã€‚åŸºäºè¿™äº›æ´å¯Ÿï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€å¥—åŒ…å«é’ˆå¯¹æ€§å¤ä¹ ã€åä½œæ¡ˆä¾‹ã€ç‹¬ç«‹ç»ƒä¹ å’Œå¼•å¯¼æ€§è®²è§£çš„ç»“æ„åŒ–æ•™å­¦æ¨¡å¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥å¹²é¢„æªæ–½æ˜¾è‘—æå‡äº†å­¦ç”Ÿçš„å­¦ä¹ ä¿¡å¿ƒï¼Œç¼“è§£äº†å­¦ä¸šç„¦è™‘ï¼Œå¹¶æé«˜äº†å­¦ç”Ÿå¯¹æ³µå¼•ç†ï¼ˆpumping lemmaï¼‰å’Œå½¢å¼è¯­è¨€è¡¨è¾¾èƒ½åŠ›ï¼ˆformal language expressive powerï¼‰ç­‰æŠ½è±¡æ¦‚å¿µçš„ç†è§£åŠ›ã€‚æœ€ç»ˆç»“è®ºè¡¨æ˜ï¼Œè¿™ç§é¢å‘æ•™å¸ˆçš„åæ€æ€§å¤§è¯­è¨€æ¨¡å‹åº”ç”¨èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºé«˜éš¾åº¦ç†è®ºé¢†åŸŸçš„æ•™å­¦è®¾è®¡ï¼Œå¹¶å…·å¤‡æ¨å¹¿è‡³å…¶ä»–è®¤çŸ¥è´Ÿè·è¾ƒé«˜çš„è®¡ç®—æœºç§‘å­¦è¯¾ç¨‹çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.FL"
      ],
      "primary_category": "cs.CY",
      "comment": "7 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.01906v1",
      "published_date": "2025-10-31 12:35:18 UTC",
      "updated_date": "2025-10-31 12:35:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:14.631575+00:00"
    },
    {
      "arxiv_id": "2510.27428v1",
      "title": "Learning Soft Robotic Dynamics with Active Exploration",
      "title_zh": "åŸºäºä¸»åŠ¨æ¢ç´¢çš„è½¯ä½“æœºå™¨äººåŠ¨åŠ›å­¦å­¦ä¹ ",
      "authors": [
        "Hehui Zheng",
        "Bhavya Sukhija",
        "Chenhao Li",
        "Klemens Iten",
        "Andreas Krause",
        "Robert K. Katzschmann"
      ],
      "abstract": "Soft robots offer unmatched adaptability and safety in unstructured environments, yet their compliant, high-dimensional, and nonlinear dynamics make modeling for control notoriously difficult. Existing data-driven approaches often fail to generalize, constrained by narrowly focused task demonstrations or inefficient random exploration. We introduce SoftAE, an uncertainty-aware active exploration framework that autonomously learns task-agnostic and generalizable dynamics models of soft robotic systems. SoftAE employs probabilistic ensemble models to estimate epistemic uncertainty and actively guides exploration toward underrepresented regions of the state-action space, achieving efficient coverage of diverse behaviors without task-specific supervision. We evaluate SoftAE on three simulated soft robotic platforms -- a continuum arm, an articulated fish in fluid, and a musculoskeletal leg with hybrid actuation -- and on a pneumatically actuated continuum soft arm in the real world. Compared with random exploration and task-specific model-based reinforcement learning, SoftAE produces more accurate dynamics models, enables superior zero-shot control on unseen tasks, and maintains robustness under sensing noise, actuation delays, and nonlinear material effects. These results demonstrate that uncertainty-driven active exploration can yield scalable, reusable dynamics models across diverse soft robotic morphologies, representing a step toward more autonomous, adaptable, and data-efficient control in compliant robots.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SoftAEï¼Œä¸€ç§æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„ä¸»åŠ¨æ¢ç´¢ (Active Exploration) æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå…·æœ‰é«˜ç»´ã€éçº¿æ€§åŠ¨åŠ›å­¦ç‰¹å¾çš„è½¯ä½“æœºå™¨äººè‡ªä¸»å­¦ä¹ ä»»åŠ¡æ— å…³ä¸”å…·æœ‰æ³›åŒ–æ€§çš„åŠ¨åŠ›å­¦æ¨¡å‹ã€‚SoftAE é€šè¿‡æ¦‚ç‡é›†æˆæ¨¡å‹ (Probabilistic Ensemble Models) è¯„ä¼°è®¤çŸ¥ä¸ç¡®å®šæ€§ (Epistemic Uncertainty)ï¼Œå¹¶ä¸»åŠ¨å¼•å¯¼æ¢ç´¢è¿‡ç¨‹è¦†ç›–çŠ¶æ€-åŠ¨ä½œç©ºé—´ä¸­ä»£è¡¨æ€§ä¸è¶³çš„åŒºåŸŸï¼Œä»è€Œåœ¨æ— éœ€ä»»åŠ¡ç‰¹å®šç›‘ç£çš„æƒ…å†µä¸‹å®ç°å¯¹å¤šæ ·åŒ–è¡Œä¸ºçš„é«˜æ•ˆè¦†ç›–ã€‚å®éªŒåœ¨è¿ç»­ä½“æœºæ¢°è‡‚ (Continuum Arm)ã€æµä½“ä¸­çš„å…³èŠ‚é±¼å’Œå…·æœ‰æ··åˆé©±åŠ¨çš„è‚Œè‚‰éª¨éª¼è…¿ä¸‰ç§æ¨¡æ‹Ÿå¹³å°ä»¥åŠçœŸå®çš„ç‰©ç†è½¯ä½“è‡‚ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œä¸éšæœºæ¢ç´¢å’Œç‰¹å®šä»»åŠ¡çš„åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹  (MBRL) ç›¸æ¯”ï¼ŒSoftAE ç”Ÿæˆäº†æ›´å‡†ç¡®çš„åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œå¹¶åœ¨æœªçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬æ§åˆ¶ (Zero-shot Control) èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ä¼ æ„Ÿå™ªå£°ã€é©±åŠ¨å»¶è¿Ÿå’Œéçº¿æ€§ææ–™æ•ˆåº”ä¸‹ä»ä¿æŒå¼ºå¥çš„é²æ£’æ€§ï¼Œè¯æ˜äº†ä¸ç¡®å®šæ€§é©±åŠ¨çš„ä¸»åŠ¨æ¢ç´¢å¯ä»¥äº§ç”Ÿè·¨å¤šç§è½¯ä½“å½¢æ€çš„å¯æ‰©å±•ã€å¯å¤ç”¨çš„åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œä¸ºå®ç°æ›´è‡ªä¸»ã€æ•°æ®é«˜æ•ˆçš„è½¯ä½“æœºå™¨äººæ§åˆ¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27428v1",
      "published_date": "2025-10-31 12:35:02 UTC",
      "updated_date": "2025-10-31 12:35:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:14.880513+00:00"
    },
    {
      "arxiv_id": "2510.27421v1",
      "title": "Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the MAMA-MIA Dataset",
      "title_zh": "ç®—æ³•å¯¹è°å¤±æ•ˆï¼ŸMAMA-MIA æ•°æ®é›†ä¸­çš„å¹´é¾„ä¸æ—è£”åè§ç ”ç©¶",
      "authors": [
        "Aditya Parikh",
        "Sneha Das",
        "Aasa Feragen"
      ],
      "abstract": "Deep learning models aim to improve diagnostic workflows, but fairness evaluation remains underexplored beyond classification, e.g., in image segmentation. Unaddressed segmentation bias can lead to disparities in the quality of care for certain populations, potentially compounded across clinical decision points and amplified through iterative model development. Here, we audit the fairness of the automated segmentation labels provided in the breast cancer tumor segmentation dataset MAMA-MIA. We evaluate automated segmentation quality across age, ethnicity, and data source. Our analysis reveals an intrinsic age-related bias against younger patients that continues to persist even after controlling for confounding factors, such as data source. We hypothesize that this bias may be linked to physiological factors, a known challenge for both radiologists and automated systems. Finally, we show how aggregating data from multiple data sources influences site-specific ethnic biases, underscoring the necessity of investigating data at a granular level.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ä¹³è…ºç™Œè‚¿ç˜¤åˆ†å‰²æ•°æ®é›† MAMA-MIA æä¾›çš„è‡ªåŠ¨åˆ†å‰²æ ‡ç­¾è¿›è¡Œäº†å…¬å¹³æ€§å®¡è®¡ï¼Œé‡ç‚¹è¯„ä¼°äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å›¾åƒåˆ†å‰² (Image segmentation) ä»»åŠ¡ä¸­æ˜¯å¦å­˜åœ¨å¹´é¾„ã€ç§æ—å’Œæ•°æ®æ¥æºæ–¹é¢çš„åè§ã€‚åˆ†æç»“æœæ­ç¤ºäº†é’ˆå¯¹å¹´è½»æ‚£è€…çš„å†…åœ¨å¹´é¾„ç›¸å…³åè§ (Age-related bias)ï¼Œä¸”è¿™ç§åè§åœ¨æ§åˆ¶äº†æ•°æ®æ¥æºç­‰æ··æ‚å› ç´ åä¾ç„¶å­˜åœ¨ã€‚ç ”ç©¶è€…æ¨æµ‹è¿™ç§åè§å¯èƒ½ä¸å¹´è½»æ‚£è€…çš„ç”Ÿç†å› ç´  (Physiological factors) ç›¸å…³ï¼Œè¿™å¯¹æ”¾å°„ç§‘åŒ»å¸ˆå’Œè‡ªåŠ¨åŒ–ç³»ç»Ÿè€Œè¨€éƒ½æ˜¯ä¸€é¡¹å·²çŸ¥æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å±•ç¤ºäº†æ•´åˆå¤šä¸ªæ•°æ®æ¥æºå¯¹ç‰¹å®šç«™ç‚¹ç§æ—åè§ (Ethnic biases) çš„å½±å“ï¼Œå¼ºè°ƒäº†åœ¨ç»†ç²’åº¦çº§åˆ«è°ƒæŸ¥æ•°æ®çš„å¿…è¦æ€§ã€‚è¯¥å·¥ä½œè­¦ç¤ºå¦‚æœä¸è§£å†³åˆ†å‰²åè§ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸åŒäººç¾¤åœ¨åŒ»ç–—æŠ¤ç†è´¨é‡ä¸Šäº§ç”Ÿå·®å¼‚ï¼Œå¹¶åœ¨ä¸´åºŠå†³ç­–å’Œæ¨¡å‹è¿­ä»£è¿‡ç¨‹ä¸­è¿›ä¸€æ­¥æ”¾å¤§è¿™ç§ä¸å¹³ç­‰ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Medical Imaging Meets EurIPS (NeurIPS-endorsed workshop) - MedEurIPS",
      "pdf_url": "https://arxiv.org/pdf/2510.27421v1",
      "published_date": "2025-10-31 12:20:31 UTC",
      "updated_date": "2025-10-31 12:20:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:16.130880+00:00"
    },
    {
      "arxiv_id": "2511.00133v1",
      "title": "Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning",
      "title_zh": "èåˆç‰¹å¾é‡è¦æ€§å¼•å¯¼ä¸æ¨¡æ‹Ÿé€€ç«è¶…å‚æ•°è°ƒä¼˜çš„éšæœºæ£®æ—å­¦ä¹ ",
      "authors": [
        "Kowshik Balasubramanian",
        "Andre Williams",
        "Ismail Butun"
      ],
      "abstract": "This paper introduces a novel framework for enhancing Random Forest classifiers by integrating probabilistic feature sampling and hyperparameter tuning via Simulated Annealing. The proposed framework exhibits substantial advancements in predictive accuracy and generalization, adeptly tackling the multifaceted challenges of robust classification across diverse domains, including credit risk evaluation, anomaly detection in IoT ecosystems, early-stage medical diagnostics, and high-dimensional biological data analysis. To overcome the limitations of conventional Random Forests, we present an approach that places stronger emphasis on capturing the most relevant signals from data while enabling adaptive hyperparameter configuration. The model is guided towards features that contribute more meaningfully to classification and optimizing this with dynamic parameter tuning. The results demonstrate consistent accuracy improvements and meaningful insights into feature relevance, showcasing the efficacy of combining importance aware sampling and metaheuristic optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ¦‚ç‡ç‰¹å¾é‡‡æ · (probabilistic feature sampling) å’Œæ¨¡æ‹Ÿé€€ç« (Simulated Annealing) è¶…å‚æ•°è°ƒä¼˜çš„æ–°å‹éšæœºæ£®æ— (Random Forest) å¢å¼ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç‰¹å¾é‡è¦æ€§å¼•å¯¼æ¨¡å‹æ•æ‰æ•°æ®ä¸­æœ€ç›¸å…³çš„ä¿¡å·ï¼Œå¹¶åˆ©ç”¨å…ƒå¯å‘å¼ä¼˜åŒ– (metaheuristic optimization) å®ç°å‚æ•°çš„åŠ¨æ€é…ç½®ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³ä¿¡ç”¨é£é™©è¯„ä¼°ã€ç‰©è”ç½‘ (IoT) å¼‚å¸¸æ£€æµ‹ã€æ—©æœŸåŒ»ç–—è¯Šæ–­ä»¥åŠé«˜ç»´ç”Ÿç‰©æ•°æ®åˆ†æç­‰é¢†åŸŸçš„é²æ£’åˆ†ç±»æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼ŒåŒæ—¶èƒ½æä¾›æœ‰å…³ç‰¹å¾ç›¸å…³æ€§çš„æ·±åˆ»è§è§£ã€‚è¿™ç§ç»“åˆäº†é‡è¦æ€§æ„ŸçŸ¥é‡‡æ ·å’Œæ¨¡æ‹Ÿé€€ç«ç®—æ³•çš„æ–¹æ³•ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿéšæœºæ£®æ—çš„å±€é™æ€§ï¼Œå±•ç¤ºäº†åœ¨å¤šæ ·åŒ–åº”ç”¨åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 2 figures, 3 tables, submitted to IEEE Intelligent Systems journal",
      "pdf_url": "https://arxiv.org/pdf/2511.00133v1",
      "published_date": "2025-10-31 12:14:53 UTC",
      "updated_date": "2025-10-31 12:14:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:26.533448+00:00"
    },
    {
      "arxiv_id": "2510.27419v1",
      "title": "DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains",
      "title_zh": "DeepCompressï¼šåŠ¨æ€æ¢ç´¢ä¸å‹ç¼©æ¨ç†é“¾çš„åŒé‡å¥–åŠ±ç­–ç•¥",
      "authors": [
        "Tian Liang",
        "Wenxiang Jiao",
        "Zhiwei He",
        "Jiahao Xu",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "Large Reasoning Models (LRMs) have demonstrated impressive capabilities but suffer from cognitive inefficiencies like ``overthinking'' simple problems and ``underthinking'' complex ones. While existing methods that use supervised fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can improve efficiency, they often do so at the cost of accuracy. This paper introduces \\textbf{DeepCompress}, a novel framework that simultaneously enhances both the accuracy and efficiency of LRMs. We challenge the prevailing approach of consistently favoring shorter reasoning paths, showing that longer responses can contain a broader range of correct solutions for difficult problems. DeepCompress employs an adaptive length reward mechanism that dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on the model's evolving capability. It encourages shorter, more efficient reasoning for ``Simple'' problems while promoting longer, more exploratory thought chains for ``Hard'' problems. This dual-reward strategy enables the model to autonomously adjust its Chain-of-Thought (CoT) length, compressing reasoning for well-mastered problems and extending it for those it finds challenging. Experimental results on challenging mathematical benchmarks show that DeepCompress consistently outperforms baseline methods, achieving superior accuracy while significantly improving token efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†æ¨¡å‹ (Large Reasoning Models, LRMs) åœ¨å¤„ç†ç®€å•é—®é¢˜æ—¶è¿‡åº¦æ€è€ƒä»¥åŠå¤„ç†å¤æ‚é—®é¢˜æ—¶æ€è€ƒä¸è¶³çš„è®¤çŸ¥æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œæå‡ºäº† DeepCompress æ¡†æ¶ã€‚ä¸åŒäºä»¥å¾€ä¸€å‘³è¿½æ±‚ç¼©çŸ­æ¨ç†è·¯å¾„çš„æ–¹æ³•ï¼Œè¯¥æ¡†æ¶æŒ‡å‡ºé•¿å›ç­”åœ¨åº”å¯¹éš¾é¢˜æ—¶å¾€å¾€èƒ½åŒ…å«æ›´ä¸°å¯Œçš„æ­£ç¡®è§£ã€‚DeepCompress æ ¸å¿ƒé‡‡ç”¨äº†ä¸€ç§è‡ªé€‚åº”é•¿åº¦å¥–åŠ±æœºåˆ¶ (adaptive length reward mechanism)ï¼Œæ ¹æ®æ¨¡å‹èƒ½åŠ›çš„å®æ—¶æ¼”å˜å°†é—®é¢˜åŠ¨æ€åˆ’åˆ†ä¸ºâ€œç®€å•â€æˆ–â€œå›°éš¾â€ã€‚å¯¹äºâ€œç®€å•â€é—®é¢˜ï¼Œè¯¥æœºåˆ¶é¼“åŠ±ç”Ÿæˆæ›´çŸ­ã€æ›´é«˜æ•ˆçš„æ¨ç†è·¯å¾„ï¼›è€Œå¯¹äºâ€œå›°éš¾â€é—®é¢˜ï¼Œåˆ™ä¿ƒè¿›æ›´å…·æ¢ç´¢æ€§çš„æ€ç»´é“¾ (Chain-of-Thought, CoT)ã€‚è¿™ç§åŒé‡å¥–åŠ±ç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»è°ƒèŠ‚æ¨ç†é•¿åº¦ï¼Œåœ¨ç†Ÿç»ƒé¢†åŸŸå‹ç¼©æ¨ç†å¹¶åœ¨æŒ‘æˆ˜é¢†åŸŸå»¶ä¼¸æ€è€ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepCompress åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œåœ¨å¤§å¹…æå‡ Token æ•ˆç‡çš„åŒæ—¶å®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2510.27419v1",
      "published_date": "2025-10-31 12:13:11 UTC",
      "updated_date": "2025-10-31 12:13:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:38.233992+00:00"
    },
    {
      "arxiv_id": "2510.27413v1",
      "title": "Atlas-Alignment: Making Interpretability Transferable Across Language Models",
      "title_zh": "Atlas-Alignmentï¼šå®ç°è·¨è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§è¿ç§»",
      "authors": [
        "Bruno Puri",
        "Jim Berend",
        "Sebastian Lapuschkin",
        "Wojciech Samek"
      ],
      "abstract": "Interpretability is crucial for building safe, reliable, and controllable language models, yet existing interpretability pipelines remain costly and difficult to scale. Interpreting a new model typically requires costly training of model-specific sparse autoencoders, manual or semi-automated labeling of SAE components, and their subsequent validation. We introduce Atlas-Alignment, a framework for transferring interpretability across language models by aligning unknown latent spaces to a Concept Atlas - a labeled, human-interpretable latent space - using only shared inputs and lightweight representational alignment techniques. Once aligned, this enables two key capabilities in previously opaque models: (1) semantic feature search and retrieval, and (2) steering generation along human-interpretable atlas concepts. Through quantitative and qualitative evaluations, we show that simple representational alignment methods enable robust semantic retrieval and steerable generation without the need for labeled concept data. Atlas-Alignment thus amortizes the cost of explainable AI and mechanistic interpretability: by investing in one high-quality Concept Atlas, we can make many new models transparent and controllable at minimal marginal cost.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§(Interpretability)æŠ€æœ¯æˆæœ¬é«˜ã€éš¾ä»¥è§„æ¨¡åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº† Atlas-Alignment æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è·¨è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§è¿ç§»ã€‚è¯¥æ¡†æ¶é€šè¿‡è½»é‡çº§çš„è¡¨ç¤ºå¯¹é½(Representational Alignment)æŠ€æœ¯ï¼Œåˆ©ç”¨å…±äº«è¾“å…¥å°†æœªçŸ¥æ¨¡å‹çš„æ½œç©ºé—´(Latent Space)ä¸ä¸€ä¸ªå·²æ ‡æ³¨ä¸”äººç±»å¯ç†è§£çš„ Concept Atlas è¿›è¡Œå…³è”ã€‚å¯¹é½åçš„æ¨¡å‹æ— éœ€é‡æ–°è®­ç»ƒç‰¹å®šæ¨¡å‹çš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨(SAE)ï¼Œå³å¯å…·å¤‡è¯­ä¹‰ç‰¹å¾æœç´¢ä¸æ£€ç´¢(Semantic Feature Search and Retrieval)ä»¥åŠç”Ÿæˆå¼•å¯¼(Steering Generation)ä¸¤é¡¹æ ¸å¿ƒèƒ½åŠ›ã€‚å®éªŒè¯„ä¼°è¯æ˜ï¼ŒAtlas-Alignment åœ¨ä¸ä¾èµ–æ ‡æ³¨æ¦‚å¿µæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½å®ç°é²æ£’çš„è¯­ä¹‰æ£€ç´¢å’Œå¯æ§ç”Ÿæˆã€‚é€šè¿‡æ„å»ºé«˜è´¨é‡çš„ Concept Atlasï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåˆ†æ‘Šäº†æœºæ¢°è§£é‡Šæ€§(Mechanistic Interpretability)çš„ç ”ç©¶æˆæœ¬ï¼Œä½¿å¾—æ–°æ¨¡å‹èƒ½ä»¥æä½çš„è¾¹é™…æˆæœ¬å®ç°é€æ˜åŒ–ä¸å¯æ§æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27413v1",
      "published_date": "2025-10-31 12:02:54 UTC",
      "updated_date": "2025-10-31 12:02:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:41.828737+00:00"
    },
    {
      "arxiv_id": "2510.27410v1",
      "title": "Dialogue as Discovery: Navigating Human Intent Through Principled Inquiry",
      "title_zh": "å¯¹è¯å³å‘ç°ï¼šé€šè¿‡åŸåˆ™æ€§æ¢è¯¢è§£æäººç±»æ„å›¾",
      "authors": [
        "Jianwen Sun",
        "Yukang Feng",
        "Yifan Chang",
        "Chuanhao Li",
        "Zizhen Li",
        "Jiaxin Ai",
        "Fanrui Zhang",
        "Yu Dai",
        "Kaipeng Zhang"
      ],
      "abstract": "A fundamental bottleneck in human-AI collaboration is the \"intention expression gap,\" the difficulty for humans to effectively convey complex, high-dimensional thoughts to AI. This challenge often traps users in inefficient trial-and-error loops and is exacerbated by the diverse expertise levels of users. We reframe this problem from passive instruction following to a Socratic collaboration paradigm, proposing an agent that actively probes for information to resolve its uncertainty about user intent. we name the proposed agent Nous, trained to acquire proficiency in this inquiry policy. The core mechanism of Nous is a training framework grounded in the first principles of information theory. Within this framework, we define the information gain from dialogue as an intrinsic reward signal, which is fundamentally equivalent to the reduction of Shannon entropy over a structured task space. This reward design enables us to avoid reliance on costly human preference annotations or external reward models. To validate our framework, we develop an automated simulation pipeline to generate a large-scale, preference-based dataset for the challenging task of scientific diagram generation. Comprehensive experiments, including ablations, subjective and objective evaluations, and tests across user expertise levels, demonstrate the effectiveness of our proposed framework. Nous achieves leading efficiency and output quality, while remaining robust to varying user expertise. Moreover, its design is domain-agnostic, and we show evidence of generalization beyond diagram generation. Experimental results prove that our work offers a principled, scalable, and adaptive paradigm for resolving uncertainty about user intent in complex human-AI collaboration.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººæœºåä½œä¸­äººç±»éš¾ä»¥æœ‰æ•ˆä¼ è¾¾å¤æ‚æ€æƒ³çš„â€œæ„å›¾è¡¨è¾¾å·®è·(intention expression gap)â€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºNousçš„æ™ºèƒ½ä½“ã€‚Nousé‡‡ç”¨äº†è‹æ ¼æ‹‰åº•å¼åä½œèŒƒå¼(Socratic collaboration paradigm)ï¼Œä»è¢«åŠ¨éµå¾ªæŒ‡ä»¤è½¬å˜ä¸ºé€šè¿‡ä¸»åŠ¨è¯¢é—®æ¥æ¶ˆé™¤ç”¨æˆ·æ„å›¾çš„ä¸ç¡®å®šæ€§ã€‚å…¶æ ¸å¿ƒæœºåˆ¶åŸºäºä¿¡æ¯è®º(information theory)çš„ç¬¬ä¸€æ€§åŸç†ï¼Œå°†å¯¹è¯ä¸­çš„ä¿¡æ¯å¢ç›Šå®šä¹‰ä¸ºå†…åœ¨å¥–åŠ±ä¿¡å·ï¼Œè¿™åœ¨æœ¬è´¨ä¸Šç­‰åŒäºå‡å°‘ç»“æ„åŒ–ä»»åŠ¡ç©ºé—´ä¸Šçš„é¦™å†œç†µ(Shannon entropy)ã€‚è¿™ç§è®¾è®¡ä½¿æ¨¡å‹ä¸å†ä¾èµ–æ˜‚è´µçš„äººå·¥åå¥½æ ‡æ³¨æˆ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œå…·æœ‰æé«˜çš„å¯æ‰©å±•æ€§ã€‚åœ¨ç§‘å­¦å›¾è¡¨ç”Ÿæˆ(scientific diagram generation)ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNousåœ¨æ•ˆç‡å’Œè¾“å‡ºè´¨é‡ä¸Šå‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œä¸”å¯¹ä¸åŒä¸“ä¸šæ°´å¹³çš„ç”¨æˆ·å…·æœ‰é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å…·æœ‰é¢†åŸŸæ— å…³(domain-agnostic)çš„ç‰¹æ€§ï¼Œå±•ç°å‡ºåœ¨å¤æ‚äººæœºåä½œä¸­è§£æç”¨æˆ·æ„å›¾çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27410v1",
      "published_date": "2025-10-31 12:00:21 UTC",
      "updated_date": "2025-10-31 12:00:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:43.329669+00:00"
    },
    {
      "arxiv_id": "2510.27403v1",
      "title": "FedMuon: Accelerating Federated Learning with Matrix Orthogonalization",
      "title_zh": "FedMuonï¼šé€šè¿‡çŸ©é˜µæ­£äº¤åŒ–åŠ é€Ÿè”é‚¦å­¦ä¹ ",
      "authors": [
        "Junkang Liu",
        "Fanhua Shang",
        "Junchao Zhou",
        "Hongying Liu",
        "Yuanyuan Liu",
        "Jin Liu"
      ],
      "abstract": "The core bottleneck of Federated Learning (FL) lies in the communication rounds. That is, how to achieve more effective local updates is crucial for reducing communication rounds. Existing FL methods still primarily use element-wise local optimizers (Adam/SGD), neglecting the geometric structure of the weight matrices. This often leads to the amplification of pathological directions in the weights during local updates, leading deterioration in the condition number and slow convergence. Therefore, we introduce the Muon optimizer in local, which has matrix orthogonalization to optimize matrix-structured parameters. Experimental results show that, in IID setting, Local Muon significantly accelerates the convergence of FL and reduces communication rounds compared to Local SGD and Local AdamW. However, in non-IID setting, independent matrix orthogonalization based on the local distributions of each client induces strong client drift. Applying Muon in non-IID FL poses significant challenges: (1) client preconditioner leading to client drift; (2) moment reinitialization. To address these challenges, we propose a novel Federated Muon optimizer (FedMuon), which incorporates two key techniques: (1) momentum aggregation, where clients use the aggregated momentum for local initialization; (2) local-global alignment, where the local gradients are aligned with the global update direction to significantly reduce client drift. Theoretically, we prove that \\texttt{FedMuon} achieves a linear speedup convergence rate without the heterogeneity assumption, where $S$ is the number of participating clients per round, $K$ is the number of local iterations, and $R$ is the total number of communication rounds. Empirically, we validate the effectiveness of FedMuon on language and vision models. Compared to several baselines, FedMuon significantly reduces communication rounds and improves test accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å­¦ä¹  (Federated Learning) é€šä¿¡æ•ˆç‡ç“¶é¢ˆï¼ŒæŒ‡å‡ºä¼ ç»Ÿæœ¬åœ°ä¼˜åŒ–å™¨å¿½ç•¥æƒé‡çŸ©é˜µå‡ ä½•ç»“æ„å¯¼è‡´æ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ã€‚ç ”ç©¶é¦–å…ˆå°è¯•å°†å…·æœ‰çŸ©é˜µæ­£äº¤åŒ– (Matrix Orthogonalization) ç‰¹æ€§çš„ Muon ä¼˜åŒ–å™¨åº”ç”¨äºæœ¬åœ°æ›´æ–°ï¼Œä½†åœ¨éç‹¬ç«‹åŒåˆ†å¸ƒ (non-IID) åœºæ™¯ä¸‹é¢ä¸´ä¸¥é‡çš„å®¢æˆ·ç«¯æ¼‚ç§» (Client Drift) æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† FedMuon ä¼˜åŒ–å™¨ï¼Œå¼•å…¥äº†åŠ¨é‡èšåˆ (Momentum Aggregation) å’Œæœ¬åœ°-å…¨å±€å¯¹é½ (Local-Global Alignment) æŠ€æœ¯ï¼Œä»¥ç¡®ä¿æœ¬åœ°æ¢¯åº¦ä¸å…¨å±€æ›´æ–°æ–¹å‘ä¸€è‡´ã€‚ç†è®ºåˆ†æè¯æ˜ FedMuon å®ç°äº†çº¿æ€§åŠ é€Ÿæ”¶æ•›ç‡ï¼Œä¸”æ— éœ€ä¾èµ–å¼‚è´¨æ€§å‡è®¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedMuon åœ¨è¯­è¨€å’Œè§†è§‰æ¨¡å‹ä»»åŠ¡ä¸­æ˜¾è‘—å‡å°‘äº†é€šä¿¡å›åˆæ•°ï¼Œå¹¶æœ‰æ•ˆæå‡äº†æ¨¡å‹çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27403v1",
      "published_date": "2025-10-31 11:41:16 UTC",
      "updated_date": "2025-10-31 11:41:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:47.425626+00:00"
    },
    {
      "arxiv_id": "2510.27400v1",
      "title": "Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs",
      "title_zh": "å¹³è¡¡çŸ¥è¯†æ›´æ–°ï¼šè¿ˆå‘å¤§è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¨¡å—åŒ–ç¼–è¾‘",
      "authors": [
        "Jiahao Liu",
        "Zijian Wang",
        "Kuo Zhao",
        "Dong Hu"
      ],
      "abstract": "Knowledge editing has emerged as an efficient approach for updating factual knowledge in large language models (LLMs). It typically locates knowledge storage modules and then modifies their parameters. However, most existing methods focus on the weights of multilayer perceptron (MLP) modules, which are often identified as the main repositories of factual information. Other components, such as attention (Attn) modules, are often ignored during editing. This imbalance can leave residual outdated knowledge and limit editing effectiveness. We perform comprehensive knowledge localization experiments on advanced LLMs and find that Attn modules play a substantial role in factual knowledge storage and retrieval, especially in earlier layers. Based on these insights, we propose IntAttn-Edit, a method that extends the associative memory paradigm to jointly update both MLP and Attn modules. Our approach uses a knowledge balancing strategy that allocates update magnitudes in proportion to each module's measured contribution to knowledge storage. Experiments on standard benchmarks show that IntAttn-Edit achieves higher edit success, better generalization, and stronger knowledge preservation than prior methods. Further analysis shows that the balancing strategy keeps editing performance within an optimal range across diverse settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„çŸ¥è¯†ç¼–è¾‘(Knowledge editing)é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•è¿‡åº¦å…³æ³¨å¤šå±‚æ„ŸçŸ¥æœº(MLP)è€Œå¿½è§†äº†æ³¨æ„åŠ›(Attn)æ¨¡å—ï¼Œå¯¼è‡´ç¼–è¾‘æ•ˆæœå—é™ã€‚é€šè¿‡çŸ¥è¯†å®šä½å®éªŒï¼Œç ”ç©¶è€…å‘ç°Attnæ¨¡å—åœ¨äº‹å®çŸ¥è¯†çš„å­˜å‚¨å’Œæ£€ç´¢ä¸­å…·æœ‰æ˜¾è‘—ä½œç”¨ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹çš„æ—©æœŸå±‚ã€‚æ®æ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†IntAttn-Editæ–¹æ³•ï¼Œé€šè¿‡æ‰©å±•å…³è”è®°å¿†èŒƒå¼(associative memory paradigm)æ¥å…±åŒæ›´æ–°MLPå’ŒAttnæ¨¡å—ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†çŸ¥è¯†å¹³è¡¡ç­–ç•¥(knowledge balancing strategy)ï¼Œä¾æ®å„æ¨¡å—å¯¹çŸ¥è¯†å­˜å‚¨çš„å®æµ‹è´¡çŒ®æ¯”ä¾‹åŠ¨æ€åˆ†é…æ›´æ–°å¼ºåº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒIntAttn-Editåœ¨ç¼–è¾‘æˆåŠŸç‡ã€æ³›åŒ–æ€§å’ŒçŸ¥è¯†ä¿ç•™æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¿›ä¸€æ­¥åˆ†æè¡¨æ˜è¯¥å¹³è¡¡ç­–ç•¥èƒ½ä½¿æ¨¡å‹åœ¨å¤šç§åœºæ™¯ä¸‹ç»´æŒæœ€ä¼˜çš„ç¼–è¾‘è¡¨ç°ï¼Œä¸ºå®ç°LLMsä¸­ç»Ÿä¸€çš„æ¨¡å—åŒ–çŸ¥è¯†ç¼–è¾‘æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27400v1",
      "published_date": "2025-10-31 11:37:39 UTC",
      "updated_date": "2025-10-31 11:37:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:50.630858+00:00"
    },
    {
      "arxiv_id": "2510.27383v1",
      "title": "Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints",
      "title_zh": "åŸºäºäººç±»æ„ŸçŸ¥-è¿åŠ¨çº¦æŸçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ é€¼çœŸè¡Œäºº-é©¾é©¶å‘˜äº¤äº’å»ºæ¨¡",
      "authors": [
        "Yueyang Wang",
        "Mehmet Dogar",
        "Gustav Markkula"
      ],
      "abstract": "Modelling pedestrian-driver interactions is critical for understanding human road user behaviour and developing safe autonomous vehicle systems. Existing approaches often rely on rule-based logic, game-theoretic models, or 'black-box' machine learning methods. However, these models typically lack flexibility or overlook the underlying mechanisms, such as sensory and motor constraints, which shape how pedestrians and drivers perceive and act in interactive scenarios. In this study, we propose a multi-agent reinforcement learning (RL) framework that integrates both visual and motor constraints of pedestrian and driver agents. Using a real-world dataset from an unsignalised pedestrian crossing, we evaluate four model variants, one without constraints, two with either motor or visual constraints, and one with both, across behavioural metrics of interaction realism. Results show that the combined model with both visual and motor constraints performs best. Motor constraints lead to smoother movements that resemble human speed adjustments during crossing interactions. The addition of visual constraints introduces perceptual uncertainty and field-of-view limitations, leading the agents to exhibit more cautious and variable behaviour, such as less abrupt deceleration. In this data-limited setting, our model outperforms a supervised behavioural cloning model, demonstrating that our approach can be effective without large training datasets. Finally, our framework accounts for individual differences by modelling parameters controlling the human constraints as population-level distributions, a perspective that has not been explored in previous work on pedestrian-vehicle interaction modelling. Overall, our work demonstrates that multi-agent RL with human constraints is a promising modelling approach for simulating realistic road user interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (multi-agent reinforcement learning) æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè¡Œäººå’Œé©¾é©¶å‘˜çš„è§†è§‰ä¸è¿åŠ¨çº¦æŸ (visual and motor constraints)ï¼Œæ—¨åœ¨æ¨¡æ‹ŸçœŸå®çš„é“è·¯ä½¿ç”¨è€…äº¤äº’è¡Œä¸ºã€‚åˆ©ç”¨çœŸå®ä¸–ç•Œçš„æ— ä¿¡å·äººè¡Œæ¨ªé“æ•°æ®é›†ï¼Œç ”ç©¶å¯¹æ¯”äº†åŒ…å«ä¸åŒçº¦æŸç»„åˆçš„å››ç§æ¨¡å‹å˜ä½“ï¼Œå‘ç°ç»“åˆåŒé‡çº¦æŸçš„æ¨¡å‹åœ¨äº¤äº’çœŸå®æ€§ä¸Šè¡¨ç°æœ€ä½³ã€‚å…¶ä¸­ï¼Œè¿åŠ¨çº¦æŸä½¿æ™ºèƒ½ä½“çš„é€Ÿåº¦è°ƒèŠ‚æ›´æ¥è¿‘äººç±»çš„å¹³æ»‘ç§»åŠ¨ï¼Œè€Œè§†è§‰çº¦æŸå¸¦æ¥çš„çŸ¥è§‰ä¸ç¡®å®šæ€§åˆ™å¼•å¯¼æ™ºèƒ½ä½“äº§ç”Ÿæ›´è°¨æ…ä¸”å¤šå˜çš„å‡é€Ÿè¡Œä¸ºã€‚åœ¨æ•°æ®å—é™çš„åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•çš„è¡¨ç°ä¼˜äºç›‘ç£å¼è¡Œä¸ºå…‹éš† (behavioural cloning) æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨æœ‰é™è®­ç»ƒé›†ä¸‹çš„é«˜æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†äººç±»çº¦æŸå‚æ•°å»ºæ¨¡ä¸ºç¾¤ä½“æ°´å¹³åˆ†å¸ƒ (population-level distributions)ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰äº†äº¤é€šå‚ä¸è€…çš„ä¸ªä½“å·®å¼‚ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºå¯è§£é‡Šä¸”çœŸå®çš„è‡ªåŠ¨é©¾é©¶ä»¿çœŸç³»ç»ŸåŠç†è§£äººç±»é“è·¯è¡Œä¸ºæä¾›äº†é‡è¦çš„å»ºæ¨¡è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27383v1",
      "published_date": "2025-10-31 11:18:13 UTC",
      "updated_date": "2025-10-31 11:18:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:21:58.624605+00:00"
    },
    {
      "arxiv_id": "2510.27379v1",
      "title": "Spiking Neural Networks: The Future of Brain-Inspired Computing",
      "title_zh": "è„‰å†²ç¥ç»ç½‘ç»œï¼šç±»è„‘è®¡ç®—çš„æœªæ¥",
      "authors": [
        "Sales G. Aribe"
      ],
      "abstract": "Spiking Neural Networks (SNNs) represent the latest generation of neural computation, offering a brain-inspired alternative to conventional Artificial Neural Networks (ANNs). Unlike ANNs, which depend on continuous-valued signals, SNNs operate using distinct spike events, making them inherently more energy-efficient and temporally dynamic. This study presents a comprehensive analysis of SNN design models, training algorithms, and multi-dimensional performance metrics, including accuracy, energy consumption, latency, spike count, and convergence behavior. Key neuron models such as the Leaky Integrate-and-Fire (LIF) and training strategies, including surrogate gradient descent, ANN-to-SNN conversion, and Spike-Timing Dependent Plasticity (STDP), are examined in depth. Results show that surrogate gradient-trained SNNs closely approximate ANN accuracy (within 1-2%), with faster convergence by the 20th epoch and latency as low as 10 milliseconds. Converted SNNs also achieve competitive performance but require higher spike counts and longer simulation windows. STDP-based SNNs, though slower to converge, exhibit the lowest spike counts and energy consumption (as low as 5 millijoules per inference), making them optimal for unsupervised and low-power tasks. These findings reinforce the suitability of SNNs for energy-constrained, latency-sensitive, and adaptive applications such as robotics, neuromorphic vision, and edge AI systems. While promising, challenges persist in hardware standardization and scalable training. This study concludes that SNNs, with further refinement, are poised to propel the next phase of neuromorphic computing.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ Spiking Neural Networks (SNNs) è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œæ¢è®¨äº†å…¶ä½œä¸ºç±»è„‘è®¡ç®—æ›¿ä»£ä¼ ç»Ÿ Artificial Neural Networks (ANNs) çš„æ½œåŠ›ã€‚æ–‡ä¸­æ·±å…¥æ¢è®¨äº† Leaky Integrate-and-Fire (LIF) ç¥ç»å…ƒæ¨¡å‹ï¼Œå¹¶å¯¹æ¯”äº† surrogate gradient descentã€ANN-to-SNN conversion å’Œ Spike-Timing Dependent Plasticity (STDP) ç­‰æ ¸å¿ƒè®­ç»ƒç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ surrogate gradient è®­ç»ƒçš„ SNNs åœ¨å‡†ç¡®ç‡ä¸Šæ¥è¿‘ ANNsï¼ˆå·®è· 1-2%ï¼‰ï¼Œä¸”å…·å¤‡æä½çš„å»¶è¿Ÿï¼›è€ŒåŸºäº STDP çš„ SNNs è™½ç„¶æ”¶æ•›è¾ƒæ…¢ï¼Œä½†å…¶æä½çš„è„‰å†²è®¡æ•°å’Œèƒ½è€—ä½¿å…¶æˆä¸ºæ— ç›‘ç£åŠä½åŠŸè€—ä»»åŠ¡çš„æœ€ä½³é€‰æ‹©ã€‚ç ”ç©¶å¼ºè°ƒäº† SNNs åœ¨æœºå™¨äººã€ç¥ç»å½¢æ€è§†è§‰å’Œ edge AI ç­‰å¯¹èƒ½æºå’Œå»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨åœºæ™¯ä¸­çš„å·¨å¤§ä¼˜åŠ¿ã€‚å°½ç®¡ç›®å‰åœ¨ç¡¬ä»¶æ ‡å‡†åŒ–å’Œè§„æ¨¡åŒ–è®­ç»ƒæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä½†è¯¥ç ”ç©¶ç¡®è®¤äº† SNNs åœ¨æ¨åŠ¨ä¸‹ä¸€é˜¶æ®µç¥ç»å½¢æ€è®¡ç®—ä¸­çš„æ ¸å¿ƒåœ°ä½ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "17 pages, 7 figures, 4 tables, Published with International Journal of Engineering Trends and Technology (IJETT)",
      "pdf_url": "https://arxiv.org/pdf/2510.27379v1",
      "published_date": "2025-10-31 11:14:59 UTC",
      "updated_date": "2025-10-31 11:14:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:01.029088+00:00"
    },
    {
      "arxiv_id": "2510.27378v2",
      "title": "Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity",
      "title_zh": "é€šè¿‡å¿ å®åº¦ä¸è¯¦å°½åº¦è¯„ä¼°æ€ç»´é“¾çš„å¯ç›‘æ§æ€§",
      "authors": [
        "Austin Meek",
        "Eitan Sprejer",
        "IvÃ¡n Arcuschin",
        "Austin J. Brockmeier",
        "Steven Basart"
      ],
      "abstract": "Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¿ å®æ€§(faithfulness)å’Œå†—é•¿åº¦(verbosity)ä¸¤ä¸ªç»´åº¦ï¼Œæå‡ºäº†ä¸€ç§è¡¡é‡é“¾å¼æ€ç»´(Chain-of-Thought, CoT)å¯ç›‘æ§æ€§(monitorability)çš„æ–°æ–¹æ³•ã€‚ç”±äºç°æœ‰çš„è¯„ä¼°æ‰‹æ®µéš¾ä»¥æ•æ‰æ¨¡å‹çœç•¥æ¨ç†å› ç´ çš„æƒ…å†µï¼Œç ”ç©¶è€…å¼•å…¥å†—é•¿åº¦(verbosity)æ¥è¡¡é‡CoTæ˜¯å¦æ¶µç›–äº†è§£å†³ä»»åŠ¡çš„æ‰€æœ‰å…³é”®è¦ç´ ï¼Œå¹¶å°†å…¶ä¸å¿ å®æ€§ç»“åˆå½¢æˆç»Ÿä¸€çš„å¯ç›‘æ§æ€§è¯„åˆ†(monitorability score)ã€‚é€šè¿‡åœ¨BBHã€GPQAå’ŒMMLUåŸºå‡†ä¸Šå¯¹å¤šç§æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶å‘ç°æ¨¡å‹å³ä½¿è¡¨ç°å‡ºå¿ å®æ€§ï¼Œä¹Ÿå¯èƒ½å› å…³é”®å› ç´ ç¼ºå¤±è€Œéš¾ä»¥è¢«æœ‰æ•ˆç›‘æ§ã€‚å®éªŒç»“æœæ˜¾ç¤ºä¸åŒæ¨¡å‹ç³»åˆ—åœ¨å¯ç›‘æ§æ€§ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè¿™å¯¹äºä¾èµ–CoTç›‘æ§çš„å®‰å…¨å¯¹é½æ–¹æ¡ˆè‡³å…³é‡è¦ã€‚è¯¥ç ”ç©¶è¿˜å‘å¸ƒäº†åŸºäºInspectåº“çš„è¯„ä¼°ä»£ç ï¼Œä¸ºæå‡æ¨¡å‹æ¨ç†é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ç ”ç©¶æä¾›äº†æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Project page at https://ajmeek.github.io/cot_monitorability_website/",
      "pdf_url": "https://arxiv.org/pdf/2510.27378v2",
      "published_date": "2025-10-31 11:14:39 UTC",
      "updated_date": "2025-11-30 01:27:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:08.532148+00:00"
    },
    {
      "arxiv_id": "2510.27364v1",
      "title": "Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V",
      "title_zh": "é¢å‘ç”µå½±åœºæ™¯åˆæˆçš„å¼€æºè§†é¢‘ç”Ÿæˆå™¨å¾®è°ƒï¼šä¸€ç§åŸºäº LoRA å’Œ Wan2.1 I2V çš„å°æ•°æ®æµæ°´çº¿",
      "authors": [
        "Meftun Akarsu",
        "Kerem Catay",
        "Sedat Bin Vedat",
        "Enes Kutay Yarkan",
        "Ilke Senturk",
        "Arda Sar",
        "Dafne Eksioglu"
      ],
      "abstract": "We present a practical pipeline for fine-tuning open-source video diffusion transformers to synthesize cinematic scenes for television and film production from small datasets. The proposed two-stage process decouples visual style learning from motion generation. In the first stage, Low-Rank Adaptation (LoRA) modules are integrated into the cross-attention layers of the Wan2.1 I2V-14B model to adapt its visual representations using a compact dataset of short clips from Ay Yapim's historical television film El Turco. This enables efficient domain transfer within hours on a single GPU. In the second stage, the fine-tuned model produces stylistically consistent keyframes that preserve costume, lighting, and color grading, which are then temporally expanded into coherent 720p sequences through the model's video decoder. We further apply lightweight parallelization and sequence partitioning strategies to accelerate inference without quality degradation. Quantitative and qualitative evaluations using FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study, demonstrate measurable improvements in cinematic fidelity and temporal stability over the base model. The complete training and inference pipeline is released to support reproducibility and adaptation across cinematic domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å°è§„æ¨¡æ•°æ®é›†å¾®è°ƒå¼€æºè§†é¢‘æ‰©æ•£è½¬æ¢å™¨(video diffusion transformers)ä»¥åˆæˆç”µå½±çº§åœºæ™¯çš„å®ç”¨æµç¨‹ã€‚è¯¥æµç¨‹é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ¡ˆï¼Œé¦–å…ˆå°†ä½ç§©è‡ªé€‚åº”(LoRA)æ¨¡å—é›†æˆåˆ°Wan2.1 I2V-14Bæ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œåˆ©ç”¨ã€ŠEl Turcoã€‹å½±è§†ç´ æçŸ­ç‰‡åœ¨å•GPUä¸Šå¿«é€Ÿå®ç°è§†è§‰é£æ ¼çš„é¢†åŸŸè¿ç§»ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆä¿æŒæœè£…ã€ç¯å…‰å’Œè°ƒè‰²ä¸€è‡´æ€§çš„å…³é”®å¸§ï¼Œå¹¶é€šè¿‡è§†é¢‘è§£ç å™¨å°†å…¶æ—¶åºæ‰©å±•ä¸ºè¿è´¯çš„720pè§†é¢‘åºåˆ—ã€‚ä¸ºæå‡æ•ˆç‡ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†è½»é‡çº§å¹¶è¡ŒåŒ–å’Œåºåˆ—åˆ†åŒºç­–ç•¥æ¥åŠ é€Ÿæ¨ç†ã€‚é€šè¿‡FVDã€CLIP-SIMå’ŒLPIPSæŒ‡æ ‡ä»¥åŠä¸“å®¶ç”¨æˆ·è°ƒç ”ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ç”µå½±çº§ä¿çœŸåº¦å’Œæ—¶åºç¨³å®šæ€§æ–¹é¢å‡ä¼˜äºåŸºç¡€æ¨¡å‹ã€‚ç›®å‰ï¼Œå®Œæ•´çš„è®­ç»ƒä¸æ¨ç†æµç¨‹å·²å…¬å¼€ï¼Œæ—¨åœ¨æ”¯æŒç”µå½±åˆ¶ä½œé¢†åŸŸçš„å¤ç°ä¸è·¨é¢†åŸŸåº”ç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "video generation, image-to-video, dif- fusion transformer, LoRA, fine-tuning, cinematic scene synthesis, multi-GPU inference, fully sharded data parallelism, computational efficiency",
      "pdf_url": "https://arxiv.org/pdf/2510.27364v1",
      "published_date": "2025-10-31 10:51:35 UTC",
      "updated_date": "2025-10-31 10:51:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:03.123284+00:00"
    },
    {
      "arxiv_id": "2510.27363v1",
      "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use",
      "title_zh": "ToolScopeï¼šé¢å‘è§†è§‰å¼•å¯¼ä¸é•¿ç¨‹å·¥å…·è°ƒç”¨çš„æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Mengjie Deng",
        "Guanting Dong",
        "Zhicheng Dou"
      ],
      "abstract": "Recently, large language models (LLMs) have demonstrated remarkable problem-solving capabilities by autonomously integrating with external tools for collaborative reasoning. However, due to the inherently complex and diverse nature of multimodal information, enabling multimodal large language models (MLLMs) to flexibly and efficiently utilize external tools during reasoning remains an underexplored challenge. In this work, we introduce ToolScope, an agentic framework designed to unify global planning with local multimodal perception, adopting a specialized Perceive tool to mitigates visual context degradation in long-horizon VQA task. ToolScope comprises three primary components: the Global Navigator, the Agentic Executor, and the Response Synthesizer. The Global Navigator functions as a \"telescope\", offering high-level strategic guidance. The Agentic Executor operates iteratively to augment MLLM with local perception through the integration of external tools-Search, Code, and Perceive. Finally, the Response Synthesizer consolidates and organizes the reasoning process into a coherent, user-friendly output. We evaluate ToolScope on four VQA benchmarks across diverse domains, including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong generalization capabilities, achieving an average performance improvement of up to +6.69% across all datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ToolScopeï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç»Ÿä¸€å…¨å±€è§„åˆ’ä¸å±€éƒ¨å¤šæ¨¡æ€æ„ŸçŸ¥çš„ Agentic æ¡†æ¶ï¼Œä¸“é—¨è§£å†³ MLLMs åœ¨å¤æ‚å¤šæ¨¡æ€ç¯å¢ƒä¸‹çµæ´»ä¸”é«˜æ•ˆåˆ©ç”¨å¤–éƒ¨å·¥å…·çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥ä¸“é—¨çš„ Perceive å·¥å…·ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆç¼“è§£äº† Long-Horizon VQA ä»»åŠ¡ä¸­è§†è§‰ä¸Šä¸‹æ–‡é€€åŒ–çš„é—®é¢˜ã€‚ToolScope åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šè´Ÿè´£é«˜å±‚æˆ˜ç•¥æŒ‡å¯¼çš„ Global Navigatorï¼Œé€šè¿‡è¿­ä»£é›†æˆ Searchã€Code å’Œ Perceive å·¥å…·ä»¥å¢å¼ºå±€éƒ¨æ„ŸçŸ¥çš„ Agentic Executorï¼Œä»¥åŠè´Ÿè´£æ•´åˆæ¨ç†è¿‡ç¨‹å¹¶ç”Ÿæˆæœ€ç»ˆè¾“å‡ºçš„ Response Synthesizerã€‚ç ”ç©¶åœ¨ VQA 2.0ã€ScienceQAã€MAT-Search å’Œ MathVista ç­‰å¤šä¸ªè·¨é¢†åŸŸåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†è¯¥æ¡†æ¶çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒToolScope å…·æœ‰æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾ +6.69% çš„å¹³å‡æ€§èƒ½æå‡ï¼Œä¸ºè§†è§‰å¼•å¯¼å’Œé•¿ç¨‹å·¥å…·ä½¿ç”¨æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27363v1",
      "published_date": "2025-10-31 10:51:27 UTC",
      "updated_date": "2025-10-31 10:51:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:11.834218+00:00"
    },
    {
      "arxiv_id": "2510.27353v1",
      "title": "An In-depth Study of LLM Contributions to the Bin Packing Problem",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¯¹è£…ç®±é—®é¢˜è´¡çŒ®çš„æ·±åº¦ç ”ç©¶",
      "authors": [
        "Julien Herrmann",
        "Guillaume Pallez"
      ],
      "abstract": "Recent studies have suggested that Large Language Models (LLMs) could provide interesting ideas contributing to mathematical discovery. This claim was motivated by reports that LLM-based genetic algorithms produced heuristics offering new insights into the online bin packing problem under uniform and Weibull distributions. In this work, we reassess this claim through a detailed analysis of the heuristics produced by LLMs, examining both their behavior and interpretability. Despite being human-readable, these heuristics remain largely opaque even to domain experts. Building on this analysis, we propose a new class of algorithms tailored to these specific bin packing instances. The derived algorithms are significantly simpler, more efficient, more interpretable, and more generalizable, suggesting that the considered instances are themselves relatively simple. We then discuss the limitations of the claim regarding LLMs' contribution to this problem, which appears to rest on the mistaken assumption that the instances had previously been studied. Our findings instead emphasize the need for rigorous validation and contextualization when assessing the scientific value of LLM-generated outputs.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ·±å…¥æ¢è®¨äº† Large Language Models (LLMs) åœ¨è§£å†³ Bin Packing Problem ä¸­çš„å®é™…è´¡çŒ®ï¼Œé‡æ–°è¯„ä¼°äº† LLMs èƒ½å¤Ÿä¿ƒè¿›æ•°å­¦å‘ç°çš„è§‚ç‚¹ã€‚é€šè¿‡å¯¹åŸºäº LLMs çš„é—ä¼ ç®—æ³•ç”Ÿæˆçš„ Heuristics è¿›è¡Œåˆ†æï¼Œä½œè€…å‘ç°è¿™äº›ç®—æ³•è™½ç„¶å½¢å¼ä¸Šäººç±»å¯è¯»ï¼Œä½†å…¶å†…åœ¨é€»è¾‘å¯¹é¢†åŸŸä¸“å®¶è€Œè¨€ä¾ç„¶éš¾ä»¥ç†è§£ã€‚ç ”ç©¶äººå‘˜éšåæå‡ºäº†ä¸€ç±»é’ˆå¯¹ç‰¹å®šè£…ç®±å®ä¾‹çš„æ–°ç®—æ³•ï¼Œå…¶ç®€æ´æ€§ã€æ•ˆç‡å’Œé€šç”¨æ€§å‡ä¼˜äº LLMs ç”Ÿæˆçš„ç»“æœï¼Œè¿™æš—ç¤ºäº†å®éªŒæ‰€æ¶‰åŠçš„å®ä¾‹æœ¬èº«ç›¸å¯¹ç®€å•ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œå…ˆå‰å…³äº LLMs åšå‡ºé‡å¤§è´¡çŒ®çš„ç»“è®ºå¯èƒ½æºäºé”™è¯¯åœ°å‡è®¾è¿™äº›å®ä¾‹æ­¤å‰å·²è¢«æ·±å…¥ç ”ç©¶ã€‚è¯¥é¡¹å·¥ä½œæœ€ç»ˆå¼ºè°ƒï¼Œåœ¨è¯„ä¼° LLMs ç”Ÿæˆå†…å®¹çš„ç§‘å­¦ä»·å€¼æ—¶ï¼Œå¿…é¡»è¿›è¡Œä¸¥æ ¼çš„éªŒè¯å’ŒèƒŒæ™¯åŒ–åˆ†æã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.27353v1",
      "published_date": "2025-10-31 10:39:16 UTC",
      "updated_date": "2025-10-31 10:39:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:27.134894+00:00"
    },
    {
      "arxiv_id": "2511.00129v2",
      "title": "Data-Augmented Deep Learning for Downhole Depth Sensing and Field Validation",
      "title_zh": "é¢å‘äº•ä¸‹æ·±åº¦æ„ŸçŸ¥ä¸ç°åœºéªŒè¯çš„æ•°æ®å¢å¼ºæ·±åº¦å­¦ä¹ ",
      "authors": [
        "Siyu Xiao",
        "Xindi Zhao",
        "Tianhao Mao",
        "Yiwei Wang",
        "Yuqiao Chen",
        "Hongyun Zhang",
        "Jian Wang",
        "Junjie Wang",
        "Shuang Liu",
        "Tupei Chen",
        "Yang Liu"
      ],
      "abstract": "Accurate downhole depth measurement is essential for oil and gas well operations, directly influencing reservoir contact, production efficiency, and operational safety. Collar correlation using a casing collar locator (CCL) is fundamental for precise depth calibration. While neural network-based CCL signal recognition has achieved significant progress in collar identification, preprocessing methods for such applications remain underdeveloped. Moreover, the limited availability of real well data poses substantial challenges for training neural network models that require extensive datasets. This paper presents a system integrated into downhole tools for CCL signal acquisition to facilitate dataset construction. We propose comprehensive preprocessing methods for data augmentation and evaluate their effectiveness using our neural network models. Through systematic experimentation across various configuration combinations, we analyze the contribution of each augmentation method. Results demonstrate that standardization, label distribution smoothing (LDS), and random cropping are fundamental requirements for model training, while label smoothing regularization (LSR), time scaling, and multiple sampling significantly enhance model generalization capability. The F1 scores of our two benchmark models trained with the proposed augmentation methods maximumly improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance validation on real CCL waveforms confirms the effectiveness and practical applicability of our approach. This work addresses the gaps in data augmentation methodologies for training casing collar recognition models in CCL data-limited environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ²¹æ°”äº•ä½œä¸šä¸­ç²¾ç¡®äº•ä¸‹æ·±åº¦æµ‹é‡çš„å…³é”®éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ•°æ®å¢å¼ºæ·±åº¦å­¦ä¹ (Deep Learning)çš„å¥—ç®¡æ¥ç®æŒ‡ç¤ºå™¨(Casing Collar Locator, CCL)ä¿¡å·è¯†åˆ«ä¸æ ¡éªŒç³»ç»Ÿã€‚è®ºæ–‡æŒ‡å‡ºå½“å‰ç¥ç»ç½‘ç»œåœ¨CCLä¿¡å·å¤„ç†ä¸­é¢ä¸´é¢„å¤„ç†æŠ€æœ¯æ¬ å‘è¾¾åŠçœŸå®äº•åœºæ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œä¸ºæ­¤å¼€å‘äº†é›†æˆäºäº•ä¸‹å·¥å…·çš„ä¿¡å·é‡‡é›†ç³»ç»Ÿä»¥æ”¯æŒæ•°æ®é›†æ„å»ºã€‚ç ”ç©¶æå‡ºäº†ä¸€å¥—å…¨é¢çš„æ•°æ®å¢å¼ºé¢„å¤„ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬æ ‡å‡†åŒ–(Standardization)ã€æ ‡ç­¾åˆ†å¸ƒå¹³æ»‘(Label Distribution Smoothing, LDS)ã€éšæœºè£å‰ª(Random Cropping)ä»¥åŠæ ‡ç­¾å¹³æ»‘æ­£åˆ™åŒ–(Label Smoothing Regularization, LSR)ç­‰å…³é”®æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ‰€æå¢å¼ºæ–¹æ³•çš„æ”¯æŒä¸‹ï¼ŒåŸºå‡†æ¨¡å‹çš„F1åˆ†æ•°æœ€é«˜å¯ä»0.937æå‡è‡³1.0ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åœ¨çœŸå®CCLæ³¢å½¢ä¸Šçš„ç°åœºéªŒè¯ï¼Œè¯¥ç ”ç©¶å¡«è¡¥äº†æ•°æ®å—é™ç¯å¢ƒä¸‹è®­ç»ƒå¥—ç®¡æ¥ç®è¯†åˆ«æ¨¡å‹çš„æ–¹æ³•è®ºç©ºç™½ï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨å®é™…æ²¹æ°”äº•å·¥ç¨‹ä¸­çš„æœ‰æ•ˆæ€§ä¸åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00129v2",
      "published_date": "2025-10-31 10:25:23 UTC",
      "updated_date": "2025-12-05 12:22:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:29.523266+00:00"
    },
    {
      "arxiv_id": "2510.27343v1",
      "title": "Discriminative Rule Learning for Outcome-Guided Process Model Discovery",
      "title_zh": "é¢å‘ç»“æœå¯¼å‘è¿‡ç¨‹æ¨¡å‹å‘ç°çš„åˆ¤åˆ«å¼è§„åˆ™å­¦ä¹ ",
      "authors": [
        "Ali Norouzifar",
        "Wil van der Aalst"
      ],
      "abstract": "Event logs extracted from information systems offer a rich foundation for understanding and improving business processes. In many real-world applications, it is possible to distinguish between desirable and undesirable process executions, where desirable traces reflect efficient or compliant behavior, and undesirable ones may involve inefficiencies, rule violations, delays, or resource waste. This distinction presents an opportunity to guide process discovery in a more outcome-aware manner. Discovering a single process model without considering outcomes can yield representations poorly suited for conformance checking and performance analysis, as they fail to capture critical behavioral differences. Moreover, prioritizing one behavior over the other may obscure structural distinctions vital for understanding process outcomes. By learning interpretable discriminative rules over control-flow features, we group traces with similar desirability profiles and apply process discovery separately within each group. This results in focused and interpretable models that reveal the drivers of both desirable and undesirable executions. The approach is implemented as a publicly available tool and it is evaluated on multiple real-life event logs, demonstrating its effectiveness in isolating and visualizing critical process patterns.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸šåŠ¡æµç¨‹ä¸­ç†æƒ³ï¼ˆdesirableï¼‰ä¸ä¸ç†æƒ³ï¼ˆundesirableï¼‰æ‰§è¡Œè½¨è¿¹çš„å·®å¼‚ï¼Œæå‡ºäº†ä¸€ç§é¢å‘ç»“æœå¼•å¯¼çš„æµç¨‹æ¨¡å‹å‘ç°ï¼ˆprocess discoveryï¼‰æ–¹æ³•ã€‚ç”±äºä¼ ç»Ÿæ–¹æ³•åœ¨å‘ç°æ¨¡å‹æ—¶å¾€å¾€å¿½è§†æ‰§è¡Œç»“æœï¼Œå¯¼è‡´ç”Ÿæˆçš„è¡¨ç¤ºéš¾ä»¥æ•æ‰å½±å“åˆè§„æ€§ä¸æ€§èƒ½çš„å…³é”®è¡Œä¸ºåŒºåˆ«ï¼Œè¯¥ç ”ç©¶é€šè¿‡åœ¨æ§åˆ¶æµç‰¹å¾ï¼ˆcontrol-flow featuresï¼‰ä¸Šå­¦ä¹ å¯è§£é‡Šçš„åˆ¤åˆ«è§„åˆ™ï¼ˆdiscriminative rulesï¼‰ï¼Œå°†å…·æœ‰ç›¸ä¼¼ç»“æœå±æ€§çš„è½¨è¿¹è¿›è¡Œåˆ†ç»„ã€‚é’ˆå¯¹æ¯ç»„è½¨è¿¹ç‹¬ç«‹è¿›è¡Œçš„æµç¨‹å‘ç°èƒ½å¤Ÿæ„å»ºå‡ºæ›´å…·é’ˆå¯¹æ€§ä¸”æ˜“äºç†è§£çš„æ¨¡å‹ï¼Œä»è€Œæ¸…æ™°æ­ç¤ºé©±åŠ¨ä¸åŒæ‰§è¡Œç»“æœçš„æ ¸å¿ƒå› ç´ ã€‚è¯¥æ–¹æ³•å·²å®ç°ä¸ºå…¬å¼€å¯ç”¨çš„å·¥å…·ï¼Œå¹¶åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„äº‹ä»¶æ—¥å¿—ï¼ˆevent logsï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¯æ˜å…¶èƒ½æœ‰æ•ˆéš”ç¦»å¹¶å¯è§†åŒ–å…³é”®æµç¨‹æ¨¡å¼ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæµç¨‹çš„ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆconformance checkingï¼‰å’Œæ€§èƒ½åˆ†ææä¾›äº†æ›´å…·æ´å¯ŸåŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "The paper will be published as part of the CoopIS 2025 conference proceedings",
      "pdf_url": "https://arxiv.org/pdf/2510.27343v1",
      "published_date": "2025-10-31 10:25:19 UTC",
      "updated_date": "2025-10-31 10:25:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:32.935216+00:00"
    },
    {
      "arxiv_id": "2511.00126v1",
      "title": "Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features",
      "title_zh": "åŸºäºæˆå¯¹æ’åºä¸å…ƒç‰¹å¾çš„è½¨è¿¹é¢„æµ‹åŠ¨æ€æ¨¡å‹é€‰æ‹©",
      "authors": [
        "Lu Bowen"
      ],
      "abstract": "Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al., 2022) have achieved strong average accuracy but remain unreliable in complex long-tail driving scenarios. These limitations reveal the weakness of the prevailing \"one-model-fits-all\" paradigm, particularly in safety-critical urban contexts where simpler physics-based models can occasionally outperform advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic multi-expert gating framework that adaptively selects the most reliable trajectory predictor among a physics-informed LSTM, a Transformer, and a fine-tuned GameFormer on a per-sample basis.\n  Our method leverages internal model signals (meta-features) such as stability and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be substantially more informative than geometric scene descriptors. To the best of our knowledge, this is the first work to formulate trajectory expert selection as a pairwise-ranking problem over internal model signals (Burges et al., 2005), directly optimizing decision quality without requiring post-hoc calibration.\n  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287 samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error (FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835 m), and realizes 57.8 percent of the oracle performance bound. In open-loop simulations, after trajectory horizon alignment, the same configuration reduces FDE on left-turn scenarios by approximately 10 percent, demonstrating consistent improvements across both offline validation and open-loop evaluation. These results indicate that adaptive hybrid systems enhance trajectory reliability in safety-critical autonomous driving, providing a practical pathway beyond static single-model paradigms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶è½¨è¿¹é¢„æµ‹ä¸­â€œå•æ¨¡å‹é€‚ç”¨æ‰€æœ‰åœºæ™¯â€èŒƒå¼çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚é•¿å°¾åœºæ™¯ä¸‹çš„ä¸å¯é æ€§ï¼Œæå‡ºäº†ä¸€ç§åŠ¨æ€å¤šä¸“å®¶é—¨æ§æ¡†æ¶(Dynamic Multi-expert Gating Framework)ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®æ ·æœ¬å®æ—¶ä»åŸºäºç‰©ç†çš„ LSTMã€Transformer ä»¥åŠå¾®è°ƒçš„ GameFormer ä¸­é€‰æ‹©æœ€å¯é çš„é¢„æµ‹å™¨ã€‚ç ”ç©¶åˆ©ç”¨äº†æ¨¡å‹çš„å†…éƒ¨ä¿¡å·(Meta-features)ï¼Œå¦‚ç¨³å®šæ€§(Stability)å’Œä¸ç¡®å®šæ€§(Uncertainty)ï¼Œå¹¶è¯æ˜è¿™äº›ç‰¹å¾æ¯”å‡ ä½•åœºæ™¯æè¿°ç¬¦æ›´å…·ä¿¡æ¯é‡ã€‚è¯¥å·¥ä½œé¦–æ¬¡å°†ä¸“å®¶é€‰æ‹©é—®é¢˜å»ºæ¨¡ä¸ºåŸºäºå†…éƒ¨ä¿¡å·çš„ä¸¤ä¸¤æ’åº(Pairwise-ranking)é—®é¢˜ï¼Œç›´æ¥ä¼˜åŒ–å†³ç­–è´¨é‡è€Œæ— éœ€åæœŸæ ¡å‡†ã€‚åœ¨ nuPlan-mini æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ LLM å¢å¼ºçš„é…ç½®ä¸‹å°†æœ€ç»ˆä½ç§»è¯¯å·®(FDE)é™ä½è‡³ 2.567 ç±³ï¼Œè¾ƒ GameFormer æå‡äº† 9.5%ã€‚åœ¨å¼€ç¯æ¨¡æ‹Ÿä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å·¦è½¬åœºæ™¯ä¸‹çš„ FDE é™ä½äº†çº¦ 10%ï¼Œå±•ç°äº†åœ¨ç¦»çº¿éªŒè¯å’Œå¼€ç¯è¯„ä¼°ä¸­çš„ä¸€è‡´æ”¹è¿›ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§è‡ªé€‚åº”æ··åˆç³»ç»Ÿå¢å¼ºäº†å®‰å…¨å…³é”®å‹è‡ªåŠ¨é©¾é©¶ä¸­çš„è½¨è¿¹å¯é æ€§ï¼Œä¸ºè¶…è¶Šé™æ€å•æ¨¡å‹èŒƒå¼æä¾›äº†å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00126v1",
      "published_date": "2025-10-31 10:01:01 UTC",
      "updated_date": "2025-10-31 10:01:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:39.330714+00:00"
    },
    {
      "arxiv_id": "2510.27329v1",
      "title": "Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines",
      "title_zh": "é¢å‘é•¿æ—¶ç¨‹æ— åºä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ ï¼šä»å¸ƒå°”å¥–åŠ±æœºåˆ°è€¦åˆå¥–åŠ±æœº",
      "authors": [
        "Kristina Levina",
        "Nikolaos Pappas",
        "Athanasios Karapantelakis",
        "Aneta Vulgarakis Feljan",
        "Jendrik Seipp"
      ],
      "abstract": "Reward machines (RMs) inform reinforcement learning agents about the reward structure of the environment. This is particularly advantageous for complex non-Markovian tasks because agents with access to RMs can learn more efficiently from fewer samples. However, learning with RMs is ill-suited for long-horizon problems in which a set of subtasks can be executed in any order. In such cases, the amount of information to learn increases exponentially with the number of unordered subtasks. In this work, we address this limitation by introducing three generalisations of RMs: (1) Numeric RMs allow users to express complex tasks in a compact form. (2) In Agenda RMs, states are associated with an agenda that tracks the remaining subtasks to complete. (3) Coupled RMs have coupled states associated with each subtask in the agenda. Furthermore, we introduce a new compositional learning algorithm that leverages coupled RMs: Q-learning with coupled RMs (CoRM). Our experiments show that CoRM scales better than state-of-the-art RM algorithms for long-horizon problems with unordered subtasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨å¤„ç†é•¿ç¨‹æ— åºä»»åŠ¡æ—¶å¥–åŠ±æœº(Reward Machines, RMs)é¢ä¸´çš„æŒ‡æ•°çº§å¤æ‚åº¦é—®é¢˜æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚ä¼ ç»Ÿçš„RMsåœ¨é¢å¯¹å¯ä»¥æŒ‰ä»»æ„é¡ºåºæ‰§è¡Œçš„å­ä»»åŠ¡æ—¶ï¼Œå­¦ä¹ ä¿¡æ¯é‡éšä»»åŠ¡æ•°é‡å‘ˆæŒ‡æ•°å¢é•¿ï¼Œéš¾ä»¥åº”å¯¹é•¿ç¨‹é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†ä¸‰ç§æ³›åŒ–å½¢å¼ï¼šèƒ½å¤Ÿä»¥ç´§å‡‘å½¢å¼è¡¨è¾¾å¤æ‚ä»»åŠ¡çš„Numeric RMsã€è·Ÿè¸ªå¾…å®Œæˆä»»åŠ¡çš„Agenda RMsï¼Œä»¥åŠå°†çŠ¶æ€ä¸å­ä»»åŠ¡å…³è”çš„Coupled RMsã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åˆ©ç”¨è€¦åˆæœºåˆ¶çš„ç»„åˆå­¦ä¹ ç®—æ³•CoRM (Q-learning with coupled RMs)ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å¤„ç†åŒ…å«å¤§é‡æ— åºå­ä»»åŠ¡çš„é•¿ç¨‹ä»»åŠ¡æ—¶ï¼ŒCoRMç®—æ³•çš„æ‰©å±•æ€§æ˜¾è‘—ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„RMç®—æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27329v1",
      "published_date": "2025-10-31 10:00:57 UTC",
      "updated_date": "2025-10-31 10:00:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:35.834907+00:00"
    },
    {
      "arxiv_id": "2510.27324v1",
      "title": "Generative Semantic Coding for Ultra-Low Bitrate Visual Communication and Analysis",
      "title_zh": "é¢å‘è¶…ä½æ¯”ç‰¹ç‡è§†è§‰é€šä¿¡ä¸åˆ†æçš„ç”Ÿæˆå¼è¯­ä¹‰ç¼–ç ",
      "authors": [
        "Weiming Chen",
        "Yijia Wang",
        "Zhihan Zhu",
        "Zhihai He"
      ],
      "abstract": "We consider the problem of ultra-low bit rate visual communication for remote vision analysis, human interactions and control in challenging scenarios with very low communication bandwidth, such as deep space exploration, battlefield intelligence, and robot navigation in complex environments. In this paper, we ask the following important question: can we accurately reconstruct the visual scene using only a very small portion of the bit rate in existing coding methods while not sacrificing the accuracy of vision analysis and performance of human interactions? Existing text-to-image generation models offer a new approach for ultra-low bitrate image description. However, they can only achieve a semantic-level approximation of the visual scene, which is far insufficient for the purpose of visual communication and remote vision analysis and human interactions. To address this important issue, we propose to seamlessly integrate image generation with deep image compression, using joint text and coding latent to guide the rectified flow models for precise generation of the visual scene. The semantic text description and coding latent are both encoded and transmitted to the decoder at a very small bit rate. Experimental results demonstrate that our method can achieve the same image reconstruction quality and vision analysis accuracy as existing methods while using much less bandwidth. The code will be released upon paper acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ·±ç©ºæ¢æµ‹å’Œæˆ˜åœºæƒ…æŠ¥ç­‰æä½å¸¦å®½åœºæ™¯ä¸‹çš„è¶…ä½æ¯”ç‰¹ç‡è§†è§‰é€šä¿¡é—®é¢˜ï¼Œé’ˆå¯¹ç°æœ‰text-to-imageæ¨¡å‹ä»…èƒ½å®ç°è¯­ä¹‰çº§è¿‘ä¼¼(semantic-level approximation)çš„å±€é™ï¼Œæå‡ºäº†Generative Semantic Codingæ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡æ— ç¼é›†æˆå›¾åƒç”Ÿæˆä¸æ·±åº¦å›¾åƒå‹ç¼©æŠ€æœ¯ï¼Œåˆ©ç”¨è”åˆæ–‡æœ¬å’Œç¼–ç æ½œå˜é‡(coding latent)æ¥å¼•å¯¼ä¿®æ­£æµæ¨¡å‹(rectified flow models)è¿›è¡Œè§†è§‰åœºæ™¯çš„ç²¾ç¡®é‡å»ºã€‚è¯­ä¹‰æ–‡æœ¬æè¿°ä¸ç¼–ç æ½œå˜é‡å‡ä»¥æä½æ¯”ç‰¹ç‡ä¼ è¾“ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†è¿œç«¯è§†è§‰åˆ†æä¸äººæœºäº¤äº’ä¸­çš„ç²¾åº¦ç¼ºå¤±é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„å›¾åƒé‡å»ºè´¨é‡å’Œåˆ†æå‡†ç¡®æ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—é™ä½äº†å¸¦å®½æ¶ˆè€—ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27324v1",
      "published_date": "2025-10-31 09:49:42 UTC",
      "updated_date": "2025-10-31 09:49:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:43.722487+00:00"
    },
    {
      "arxiv_id": "2511.00125v1",
      "title": "Inferring multiple helper Dafny assertions with LLMs",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ¨æ–­ Dafny ä¸­çš„å¤šé‡è¾…åŠ©æ–­è¨€",
      "authors": [
        "Ãlvaro Silva",
        "Alexandra Mendes",
        "Ruben Martins"
      ],
      "abstract": "The Dafny verifier provides strong correctness guarantees but often requires numerous manual helper assertions, creating a significant barrier to adoption. We investigate the use of Large Language Models (LLMs) to automatically infer missing helper assertions in Dafny programs, with a primary focus on cases involving multiple missing assertions. To support this study, we extend the DafnyBench benchmark with curated datasets where one, two, or all assertions are removed, and we introduce a taxonomy of assertion types to analyze inference difficulty. Our approach refines fault localization through a hybrid method that combines LLM predictions with error-message heuristics. We implement this approach in a new tool called DAISY (Dafny Assertion Inference SYstem). While our focus is on multiple missing assertions, we also evaluate DAISY on single-assertion cases. DAISY verifies 63.4% of programs with one missing assertion and 31.7% with multiple missing assertions. Notably, many programs can be verified with fewer assertions than originally present, highlighting that proofs often admit multiple valid repair strategies and that recovering every original assertion is unnecessary. These results demonstrate that automated assertion inference can substantially reduce proof engineering effort and represent a step toward more scalable and accessible formal verification.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ Large Language Models (LLMs) è‡ªåŠ¨æ¨æ–­ Dafny ç¨‹åºä¸­ç¼ºå¤±çš„ helper assertionsï¼Œæ—¨åœ¨é™ä½å½¢å¼åŒ–éªŒè¯çš„å‚ä¸é—¨æ§›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜æ‰©å±•äº† DafnyBench åŸºå‡†æµ‹è¯•é›†å¹¶æå‡ºæ–­è¨€åˆ†ç±»æ³•ï¼Œå¼€å‘äº†åä¸º DAISY (Dafny Assertion Inference SYstem) çš„æ–°å·¥å…·ï¼Œè¯¥å·¥å…·é€šè¿‡ç»“åˆ LLM é¢„æµ‹ä¸é”™è¯¯æ¶ˆæ¯å¯å‘å¼çš„æ··åˆæ–¹æ³•æ¥ç²¾ç»†åŒ–æ•…éšœå®šä½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDAISY æˆåŠŸéªŒè¯äº† 63.4% ç¼ºå¤±å•ä¸ªæ–­è¨€çš„ç¨‹åºä»¥åŠ 31.7% ç¼ºå¤±å¤šä¸ªæ–­è¨€çš„ç¨‹åºã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œè®¸å¤šç¨‹åºèƒ½å¤Ÿä»¥å°‘äºåŸæœ‰çš„æ–­è¨€é‡å®ŒæˆéªŒè¯ï¼Œè¯æ˜äº†è‡ªåŠ¨æ–­è¨€æ¨æ–­åœ¨å‡å°‘ proof engineering å·¥ä½œé‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚è¿™ä¸€æˆæœä¸ºå®ç°æ›´å…·å¯æ‰©å±•æ€§å’Œæ˜“ç”¨æ€§çš„å½¢å¼åŒ–éªŒè¯è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LO",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00125v1",
      "published_date": "2025-10-31 09:45:39 UTC",
      "updated_date": "2025-10-31 09:45:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:46.122875+00:00"
    },
    {
      "arxiv_id": "2511.00124v1",
      "title": "Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models",
      "title_zh": "è·¨æ³¢åŠ¨ç›¸å˜æ­ç¤ºæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·åŠ¨åŠ›å­¦",
      "authors": [
        "Sai Niranjan Ramachandran",
        "Manish Krishan Lal",
        "Suvrit Sra"
      ],
      "abstract": "We analyse how the sampling dynamics of distributions evolve in score-based diffusion models using cross-fluctuations, a centered-moment statistic from statistical physics. Specifically, we show that starting from an unbiased isotropic normal distribution, samples undergo sharp, discrete transitions, eventually forming distinct events of a desired distribution while progressively revealing finer structure. As this process is reversible, these transitions also occur in reverse, where intermediate states progressively merge, tracing a path back to the initial distribution. We demonstrate that these transitions can be detected as discontinuities in $n^{\\text{th}}$-order cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for these cross-fluctuations that is efficiently computable for the reverse trajectory. We find that detecting these transitions directly boosts sampling efficiency, accelerates class-conditional and rare-class generation, and improves two zero-shot tasks--image classification and style transfer--without expensive grid search or retraining. We also show that this viewpoint unifies classical coupling and mixing from finite Markov chains with continuous dynamics while extending to stochastic SDEs and non Markovian samplers. Our framework therefore bridges discrete Markov chain theory, phase analysis, and modern generative modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨ç»Ÿè®¡ç‰©ç†ä¸­çš„ä¸­å¿ƒçŸ©ç»Ÿè®¡é‡ cross-fluctuations åˆ†æäº†åŸºäºè¯„åˆ†çš„æ‰©æ•£æ¨¡å‹ (score-based diffusion models) çš„é‡‡æ ·åŠ¨åŠ›å­¦æ¼”åŒ–ã€‚ç ”ç©¶å‘ç°ï¼Œæ ·æœ¬åœ¨ä»å„å‘åŒæ€§æ­£æ€åˆ†å¸ƒæ¼”åŒ–åˆ°ç›®æ ‡åˆ†å¸ƒçš„è¿‡ç¨‹ä¸­ï¼Œä¼šç»å†ç¦»æ•£ä¸”å‰§çƒˆçš„ç›¸å˜ (phase transitions)ï¼Œè¿™äº›è½¬å˜è¡¨ç°ä¸º n é˜¶ cross-fluctuations çš„ä¸è¿ç»­æ€§ã€‚ä½œè€…ä¸ºæ–¹å·®ä¿æŒ SDE (variance-preserving SDEs) æ¨å¯¼äº† cross-fluctuations çš„é—­å¼è§£ï¼Œå®ç°äº†å¯¹åå‘è½¨è¿¹çš„é«˜æ•ˆè®¡ç®—ã€‚é€šè¿‡ç›´æ¥æ£€æµ‹è¿™äº›è½¬æ¢ç‚¹ï¼Œç ”ç©¶æˆåŠŸæå‡äº†é‡‡æ ·æ•ˆç‡ï¼ŒåŠ é€Ÿäº†ç±»åˆ«æ¡ä»¶åŠç¨€æœ‰ç±»åˆ«ç”Ÿæˆï¼Œå¹¶åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æ”¹å–„äº†å›¾åƒåˆ†ç±» (image classification) å’Œé£æ ¼è¿ç§» (style transfer) ç­‰é›¶æ ·æœ¬ä»»åŠ¡ã€‚è¿™ä¸€æ¡†æ¶ä¸ä»…æ¡¥æ¥äº†ç¦»æ•£é©¬å°”å¯å¤«é“¾ç†è®ºä¸è¿ç»­åŠ¨åŠ›å­¦ï¼Œè¿˜ä¸ºç°ä»£ç”Ÿæˆæ¨¡å‹çš„ç›¸ä½åˆ†ææä¾›äº†ç»Ÿä¸€çš„ç†è®ºè§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025. 10 pages, camera-ready version. appendices included",
      "pdf_url": "https://arxiv.org/pdf/2511.00124v1",
      "published_date": "2025-10-31 09:40:59 UTC",
      "updated_date": "2025-10-31 09:40:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:45.638493+00:00"
    },
    {
      "arxiv_id": "2510.27315v1",
      "title": "CASR-Net: An Image Processing-focused Deep Learning-based Coronary Artery Segmentation and Refinement Network for X-ray Coronary Angiogram",
      "title_zh": "CASR-Netï¼šä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ ä¸”ä¾§é‡å›¾åƒå¤„ç†çš„Xå°„çº¿å† çŠ¶åŠ¨è„‰é€ å½±åˆ†å‰²ä¸ç²¾ç»†åŒ–ç½‘ç»œ",
      "authors": [
        "Alvee Hassan",
        "Rusab Sarmun",
        "Muhammad E. H. Chowdhury",
        "M. Murugappan",
        "Md. Sakib Abrar Hossain",
        "Sakib Mahmud",
        "Abdulrahman Alqahtani",
        "Sohaib Bassam Zoghoul",
        "Amith Khandakar",
        "Susu M. Zughaier",
        "Somaya Al-Maadeed",
        "Anwarul Hasan"
      ],
      "abstract": "Early detection of coronary artery disease (CAD) is critical for reducing mortality and improving patient treatment planning. While angiographic image analysis from X-rays is a common and cost-effective method for identifying cardiac abnormalities, including stenotic coronary arteries, poor image quality can significantly impede clinical diagnosis. We present the Coronary Artery Segmentation and Refinement Network (CASR-Net), a three-stage pipeline comprising image preprocessing, segmentation, and refinement. A novel multichannel preprocessing strategy combining CLAHE and an improved Ben Graham method provides incremental gains, increasing Dice Score Coefficient (DSC) by 0.31-0.89% and Intersection over Union (IoU) by 0.40-1.16% compared with using the techniques individually. The core innovation is a segmentation network built on a UNet with a DenseNet121 encoder and a Self-organized Operational Neural Network (Self-ONN) based decoder, which preserves the continuity of narrow and stenotic vessel branches. A final contour refinement module further suppresses false positives. Evaluated with 5-fold cross-validation on a combination of two public datasets that contain both healthy and stenotic arteries, CASR-Net outperformed several state-of-the-art models, achieving an IoU of 61.43%, a DSC of 76.10%, and clDice of 79.36%. These results highlight a robust approach to automated coronary artery segmentation, offering a valuable tool to support clinicians in diagnosis and treatment planning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Xå°„çº¿è¡€ç®¡é€ å½±å›¾åƒè´¨é‡å·®å¯¼è‡´çš„è¯Šæ–­éš¾é¢˜ï¼Œæå‡ºäº†å† çŠ¶åŠ¨è„‰åˆ†å‰²ä¸ç»†åŒ–ç½‘ç»œCASR-Netã€‚è¯¥æ¡†æ¶åŒ…å«ä¸€ä¸ªç»“åˆCLAHEä¸æ”¹è¿›Ben Grahamæ–¹æ³•çš„å¤šé€šé“é¢„å¤„ç†ç­–ç•¥ï¼Œå®éªŒè¯æ˜å…¶èƒ½æ˜¾è‘—æå‡Dice Score Coefficient (DSC)å’ŒIntersection over Union (IoU)ç­‰æ€§èƒ½æŒ‡æ ‡ã€‚å…¶æ ¸å¿ƒåˆ†å‰²ç½‘ç»œé‡‡ç”¨DenseNet121ä½œä¸ºç¼–ç å™¨ï¼Œå¹¶å¼•å…¥åŸºäºSelf-organized Operational Neural Network (Self-ONN)çš„è§£ç å™¨ä»¥ä¼˜åŒ–UNetç»“æ„ï¼Œæ—¨åœ¨ç²¾å‡†ä¿ç•™ç»†å°åŠç‹­çª„è¡€ç®¡åˆ†æ”¯çš„è¿ç»­æ€§ã€‚é€šè¿‡æœ€åçš„è½®å»“ç»†åŒ–(contour refinement)æ¨¡å—è¿›ä¸€æ­¥é™ä½è¯¯æŠ¥ç‡ï¼Œæ˜¾è‘—æå‡äº†åˆ†å‰²è´¨é‡ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„äº”æŠ˜äº¤å‰éªŒè¯æ˜¾ç¤ºï¼ŒCASR-Netåœ¨IoUã€DSCå’ŒclDiceç­‰å…³é”®æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„SOTAæ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–å† çŠ¶åŠ¨è„‰åˆ†å‰²æä¾›äº†ä¸€ç§é²æ£’çš„æ·±åº¦å­¦ä¹ æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¾…åŠ©ä¸´åºŠåŒ»ç”Ÿçš„è¯Šæ–­ä¸æ²»ç–—å†³ç­–ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27315v1",
      "published_date": "2025-10-31 09:40:29 UTC",
      "updated_date": "2025-10-31 09:40:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:48.228063+00:00"
    },
    {
      "arxiv_id": "2510.27313v2",
      "title": "LLM generation novelty through the lens of semantic similarity",
      "title_zh": "è¯­ä¹‰ç›¸ä¼¼åº¦è§†è§’ä¸‹çš„ LLM ç”Ÿæˆæ–°é¢–æ€§",
      "authors": [
        "Philipp Davydov",
        "Ameya Prabhu",
        "Matthias Bethge",
        "Elisa Nguyen",
        "Seong Joon Oh"
      ],
      "abstract": "Generation novelty is a key indicator of an LLM's ability to generalize, yet measuring it against full pretraining corpora is computationally challenging. Existing evaluations often rely on lexical overlap, failing to detect paraphrased text, or do not consider the full pretraining corpus. We frame novelty as a semantic retrieval problem. This framing enables us to address novelty with modern embedding and indexing pipelines, allowing for efficient analysis at pre-training scale. Specifically, we propose a three-stage framework that retrieves semantically similar samples, reranks them at varying subsequence lengths, and calibrates scores using a human novelty reference for interpretability. We apply this framework to the SmolLM model family and report three key findings: (1) models draw on pre-training data across much longer sequences than previously reported; (2) some task domains systematically promote or suppress generation novelty; and (3) instruction tuning not only alters style but also increases novelty. These results highlight the value of semantic novelty analysis for studying generalization. To support reproducibility and further research, we release ~20 TB of corpus chunks and index artifacts at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ç”Ÿæˆæ–°é¢–æ€§(Generation novelty)ï¼Œé’ˆå¯¹ç°æœ‰è¯æ±‡é‡å åº¦é‡æ–¹æ³•æ— æ³•æœ‰æ•ˆæ£€æµ‹æ”¹å†™æ–‡æœ¬ä¸”åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­æ–™ä¸‹è®¡ç®—å—é™çš„é—®é¢˜ï¼Œå°†å…¶é‡æ–°å®šä¹‰ä¸ºè¯­ä¹‰æ£€ç´¢(semantic retrieval)é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µæ¡†æ¶ï¼Œåˆ©ç”¨ç°ä»£åµŒå…¥(embedding)å’Œç´¢å¼•(indexing)æŠ€æœ¯ï¼Œå®ç°äº†åœ¨é¢„è®­ç»ƒè§„æ¨¡ä¸‹å¯¹è¯­ä¹‰ç›¸ä¼¼æ ·æœ¬çš„é«˜æ•ˆæ£€ç´¢ã€å¤šé•¿åº¦å­åºåˆ—é‡æ’åºä»¥åŠåŸºäºäººç±»å‚è€ƒçš„æ–°é¢–æ€§åˆ†å€¼æ ¡å‡†ã€‚é€šè¿‡å¯¹ SmolLM æ¨¡å‹ç³»åˆ—çš„åº”ç”¨ç ”ç©¶ï¼Œå‘ç°æ¨¡å‹åœ¨ç”Ÿæˆæ—¶åˆ©ç”¨é¢„è®­ç»ƒæ•°æ®çš„åºåˆ—é•¿åº¦è¿œè¶…æ­¤å‰é¢„æœŸï¼Œä¸”ç‰¹å®šä»»åŠ¡é¢†åŸŸä¼šå¯¹ç”Ÿæˆæ–°é¢–æ€§äº§ç”Ÿç³»ç»Ÿæ€§å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜æŒ‡ä»¤å¾®è°ƒ(instruction tuning)åœ¨æ”¹å˜è¾“å‡ºé£æ ¼çš„åŒæ—¶èƒ½æ˜¾è‘—æå‡ç”Ÿæˆæ–°é¢–æ€§ã€‚è¯¥æˆæœå±•ç¤ºäº†è¯­ä¹‰æ–°é¢–æ€§åˆ†æåœ¨ç ”ç©¶æ¨¡å‹æ³›åŒ–(generalization)èƒ½åŠ›ä¸­çš„é‡è¦ä»·å€¼ï¼Œå¹¶å…¬å¼€äº†çº¦ 20 TB çš„è¯­æ–™æ•°æ®ä¸ç´¢å¼•èµ„æºä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„åç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27313v2",
      "published_date": "2025-10-31 09:39:12 UTC",
      "updated_date": "2026-01-12 23:15:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:22:54.026597+00:00"
    },
    {
      "arxiv_id": "2510.27287v1",
      "title": "Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ©ä½ å·¥ä½œå—ï¼Ÿä¸€ç§ç”¨äºä¼ä¸šç¯å¢ƒè¯„ä¼° LLM æ™ºèƒ½ä½“çš„æ²™ç›’",
      "authors": [
        "Harsh Vishwakarma",
        "Ankush Agarwal",
        "Ojas Patil",
        "Chaitanya Devaguptapu",
        "Mahesh Chandran"
      ],
      "abstract": "Enterprise systems are crucial for enhancing productivity and decision-making among employees and customers. Integrating LLM based systems into enterprise systems enables intelligent automation, personalized experiences, and efficient information retrieval, driving operational efficiency and strategic growth. However, developing and evaluating such systems is challenging due to the inherent complexity of enterprise environments, where data is fragmented across multiple sources and governed by sophisticated access controls. We present EnterpriseBench, a comprehensive benchmark that simulates enterprise settings, featuring 500 diverse tasks across software engineering, HR, finance, and administrative domains. Our benchmark uniquely captures key enterprise characteristics including data source fragmentation, access control hierarchies, and cross-functional workflows. Additionally, we provide a novel data generation pipeline that creates internally consistent enterprise tasks from organizational metadata. Experiments with state-of-the-art LLM agents demonstrate that even the most capable models achieve only 41.8% task completion, highlighting significant opportunities for improvement in enterprise-focused AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EnterpriseBenchï¼Œä¸€ä¸ªæ—¨åœ¨æ¨¡æ‹ŸçœŸå®ä¼ä¸šç¯å¢ƒçš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)æ™ºèƒ½ä½“åœ¨å¤„ç†ä¼ä¸šçº§å¤æ‚ä»»åŠ¡æ—¶çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†è½¯ä»¶å·¥ç¨‹ã€äººåŠ›èµ„æºã€è´¢åŠ¡å’Œè¡Œæ”¿ç­‰é¢†åŸŸçš„500ä¸ªå¤šæ ·åŒ–ä»»åŠ¡ï¼Œå¹¶ç‹¬ç‰¹åœ°æ•æ‰äº†æ•°æ®æºç¢ç‰‡åŒ–(data source fragmentation)ã€è®¿é—®æ§åˆ¶å±‚çº§(access control hierarchies)ä»¥åŠè·¨èŒèƒ½å·¥ä½œæµ(cross-functional workflows)ç­‰æ ¸å¿ƒä¼ä¸šç‰¹å¾ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ•°æ®ç”Ÿæˆæµæ°´çº¿(data generation pipeline)ï¼Œèƒ½å¤Ÿæ ¹æ®ç»„ç»‡å…ƒæ•°æ®åˆ›å»ºå†…éƒ¨é€»è¾‘ä¸€è‡´çš„ä¼ä¸šä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„LLMæ™ºèƒ½ä½“åœ¨ä»»åŠ¡å®Œæˆç‡ä¸Šä¹Ÿä»…è¾¾åˆ°41.8%ï¼Œå‡¸æ˜¾äº†åœ¨å¤æ‚ä¼ä¸šç¯å¢ƒä¸­éƒ¨ç½²AIç³»ç»Ÿçš„å·¨å¤§æŒ‘æˆ˜ã€‚EnterpriseBenchçš„æå‡ºä¸ºæœªæ¥ä¼ä¸šå¯¼å‘å‹AIç³»ç»Ÿçš„è¯„ä¼°ä¸æ”¹è¿›æä¾›äº†é‡è¦çš„å®éªŒæ²™ç›’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at EMNLP 2025 Main Track",
      "pdf_url": "https://arxiv.org/pdf/2510.27287v1",
      "published_date": "2025-10-31 08:55:13 UTC",
      "updated_date": "2025-10-31 08:55:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:23:09.225417+00:00"
    },
    {
      "arxiv_id": "2510.27281v1",
      "title": "HiF-DTA: Hierarchical Feature Learning Network for Drug-Target Affinity Prediction",
      "title_zh": "HiF-DTAï¼šç”¨äºè¯ç‰©-é¶ç‚¹äº²å’ŒåŠ›é¢„æµ‹çš„åˆ†å±‚ç‰¹å¾å­¦ä¹ ç½‘ç»œ",
      "authors": [
        "Minghui Li",
        "Yuanhang Wang",
        "Peijin Guo",
        "Wei Wan",
        "Shengshan Hu",
        "Shengqing Hu"
      ],
      "abstract": "Accurate prediction of Drug-Target Affinity (DTA) is crucial for reducing experimental costs and accelerating early screening in computational drug discovery. While sequence-based deep learning methods avoid reliance on costly 3D structures, they still overlook simultaneous modeling of global sequence semantic features and local topological structural features within drugs and proteins, and represent drugs as flat sequences without atomic-level, substructural-level, and molecular-level multi-scale features. We propose HiF-DTA, a hierarchical network that adopts a dual-pathway strategy to extract both global sequence semantic and local topological features from drug and protein sequences, and models drugs multi-scale to learn atomic, substructural, and molecular representations fused via a multi-scale bilinear attention module. Experiments on Davis, KIBA, and Metz datasets show HiF-DTA outperforms state-of-the-art baselines, with ablations confirming the importance of global-local extraction and multi-scale fusion.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HiF-DTAï¼Œä¸€ç§æ—¨åœ¨æå‡è¯ç‰©-é¶æ ‡äº²å’ŒåŠ›(Drug-Target Affinity, DTA)é¢„æµ‹å‡†ç¡®æ€§çš„å±‚æ¬¡åŒ–ç‰¹å¾å­¦ä¹ ç½‘ç»œã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•å¿½è§†å…¨å±€åºåˆ—è¯­ä¹‰ä¸å±€éƒ¨æ‹“æ‰‘ç»“æ„ç‰¹å¾åŒæ­¥å»ºæ¨¡çš„é—®é¢˜ï¼ŒHiF-DTAé‡‡ç”¨äº†åŒè·¯å¾„ç­–ç•¥(dual-pathway strategy)æ¥æå–è¯ç‰©å’Œè›‹ç™½è´¨çš„å¤šç»´ç‰¹å¾ã€‚åœ¨è¯ç‰©è¡¨å¾æ–¹é¢ï¼Œè¯¥æ¨¡å‹å®ç°äº†åŸå­çº§ã€å­ç»“æ„çº§å’Œåˆ†å­çº§çš„å¤šå°ºåº¦å»ºæ¨¡ï¼Œå¹¶é€šè¿‡å¤šå°ºåº¦åŒçº¿æ€§æ³¨æ„åŠ›æ¨¡å—(multi-scale bilinear attention module)è¿›è¡Œç‰¹å¾èåˆã€‚åœ¨Davisã€KIBAå’ŒMetzä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHiF-DTAçš„æ€§èƒ½ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†ç»“åˆå…¨å±€-å±€éƒ¨ç‰¹å¾æå–ä»¥åŠå¤šå°ºåº¦èåˆæœºåˆ¶å¯¹äºæ•è·è¯ç‰©ä¸é¶æ ‡ä¹‹é—´å¤æ‚ç›¸äº’ä½œç”¨çš„å…³é”®ä½œç”¨ï¼Œä¸ºè®¡ç®—è¯ç‰©å‘ç°æä¾›äº†æ›´æœ‰æ•ˆçš„å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by International Conference on Bioinformatics and Biomedicine (BIBM 25)",
      "pdf_url": "https://arxiv.org/pdf/2510.27281v1",
      "published_date": "2025-10-31 08:47:15 UTC",
      "updated_date": "2025-10-31 08:47:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:23:09.938185+00:00"
    },
    {
      "arxiv_id": "2510.27280v2",
      "title": "FOCUS: Efficient Keyframe Selection for Long Video Understanding",
      "title_zh": "FOCUSï¼šé¢å‘é•¿è§†é¢‘ç†è§£çš„é«˜æ•ˆå…³é”®å¸§é€‰æ‹©",
      "authors": [
        "Zirui Zhu",
        "Hailun Xu",
        "Yang Luo",
        "Yong Liu",
        "Kanchan Sarkar",
        "Zhenheng Yang",
        "Yang You"
      ],
      "abstract": "Multimodal large language models (MLLMs) represent images and video frames as visual tokens. Scaling from single images to hour-long videos, however, inflates the token budget far beyond practical limits. Popular pipelines therefore either uniformly subsample or apply keyframe selection with retrieval-style scoring using smaller vision-language models. However, these keyframe selection methods still rely on pre-filtering before selection to reduce the inference cost and can miss the most informative moments. We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, a training-free, model-agnostic keyframe selection module that selects query-relevant frames under a strict token budget. FOCUS formulates keyframe selection as a combinatorial pure-exploration (CPE) problem in multi-armed bandits: it treats short temporal clips as arms, and uses empirical means and Bernstein confidence radius to identify informative regions while preserving exploration of uncertain areas. The resulting two-stage exploration-exploitation procedure reduces from a sequential policy with theoretical guarantees, first identifying high-value temporal regions, then selecting top-scoring frames within each region. On two long-video question-answering benchmarks, FOCUS delivers substantial accuracy improvements while processing less than 2% of video frames. For videos longer than 20 minutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstrating its effectiveness as a keyframe selection method and providing a simple and general solution for scalable long-video understanding with MLLMs. Code is available at https://github.com/NUS-HPC-AI-Lab/FOCUS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´çš„è§†è§‰ Token é¢„ç®—è¶…æ ‡åŠå…³é”®å¸§é€‰æ‹©æ•ˆç‡ä½çš„é—®é¢˜ï¼Œæå‡ºäº† FOCUS (Frame-Optimistic Confidence Upper-bound Selection)ã€‚è¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒä¸”æ¨¡å‹æ— å…³çš„å…³é”®å¸§é€‰æ‹©æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨ä¸¥æ ¼çš„ Token é¢„ç®—ä¸‹é€‰æ‹©ä¸æŸ¥è¯¢ç›¸å…³çš„å¸§ã€‚ç ”ç©¶è€…å°†å…³é”®å¸§é€‰æ‹©å»ºæ¨¡ä¸ºå¤šè‡‚è€è™æœº (Multi-armed Bandits) ä¸­çš„ç»„åˆçº¯æ¢ç´¢ (Combinatorial Pure-Exploration) é—®é¢˜ï¼Œå¹¶åˆ©ç”¨ç»éªŒå‡å€¼å’Œ Bernstein ç½®ä¿¡åŠå¾„æ¥è¯†åˆ«é«˜ä¿¡æ¯é‡åŒºåŸŸã€‚FOCUS é‡‡ç”¨ä¸¤é˜¶æ®µçš„æ¢ç´¢ä¸åˆ©ç”¨ (Exploration-Exploitation) ç­–ç•¥ï¼Œé¦–å…ˆå®šä½é«˜ä»·å€¼æ—¶é—´åŒºé—´ï¼Œå†ä»ä¸­ç­›é€‰æ ¸å¿ƒå¸§ã€‚åœ¨é•¿è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFOCUS ä»…éœ€å¤„ç†ä¸åˆ° 2% çš„è§†é¢‘å¸§å³å¯å¤§å¹…æå‡å‡†ç¡®ç‡ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤„ç† 20 åˆ†é’Ÿä»¥ä¸Šçš„è§†é¢‘æ—¶ï¼Œå…¶åœ¨ LongVideoBench ä¸Šè·å¾—äº† 11.9% çš„æ€§èƒ½æå‡ã€‚è¯¥æ–¹æ³•ä¸ºå¯æ‰©å±•çš„é•¿è§†é¢‘ç†è§£æä¾›äº†ä¸€ç§ç®€å•ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆå¹³è¡¡äº†è®¡ç®—æ•ˆç‡ä¸ä¿¡æ¯ç•™å­˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27280v2",
      "published_date": "2025-10-31 08:41:13 UTC",
      "updated_date": "2025-11-24 16:40:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:23:26.284084+00:00"
    },
    {
      "arxiv_id": "2510.27269v2",
      "title": "Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?",
      "title_zh": "æ¨ç†è¯­è¨€æ¨¡å‹ä¸ºä½•ä¼šäº§ç”Ÿå¤šè¯­è¨€æ¨ç†èƒ½åŠ›å·®è·ï¼Ÿ",
      "authors": [
        "Deokhyung Kang",
        "Seonjeong Hwang",
        "Daehui Kim",
        "Hyounghun Kim",
        "Gary Geunbae Lee"
      ],
      "abstract": "Reasoning language models (RLMs) achieve strong performance on complex reasoning tasks, yet they still exhibit a multilingual reasoning gap, performing better in high-resource languages than in low-resource ones. While recent efforts have been made to address this gap, its underlying causes remain largely unexplored. In this work, we show that this gap primarily stems from failures in language understanding-specifically, the model's inability to translate multilingual inputs into the language dominating its reasoning traces (typically English). As identifying understanding failures can enable targeted mitigation of the gap, we evaluate a range of detection methods and find that understanding failures are detectable to a meaningful extent, with supervised approaches performing best. Building on this, we propose Selective Translation, a strategy that incorporates an English translation into the initial reasoning trace only when an understanding failure is detected. Experimental results using Qwen3-4B show that Selective Translation substantially bridges the multilingual reasoning gap, achieving near full-translation performance while translating only about 20% of inputs. Together, our results show that failures in language understanding are the primary driver of the multilingual reasoning gap and can be detected and selectively mitigated, clarifying its origin and suggesting a path toward more equitable multilingual reasoning. Our code and data are publicly available at https://github.com/deokhk/RLM_analysis",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨ç†è¯­è¨€æ¨¡å‹ (Reasoning Language Models, RLMs) åœ¨ä¸åŒè¯­è¨€é—´è¡¨ç°ä¸ä¸€çš„â€œå¤šè¯­è¨€æ¨ç†é¸¿æ²Ÿ (multilingual reasoning gap)â€ç°è±¡åŠå…¶æˆå› ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥å·®è·ä¸»è¦æºäºæ¨¡å‹åœ¨è¯­è¨€ç†è§£å±‚é¢çš„å¤±æ•ˆï¼Œç‰¹åˆ«æ˜¯æ— æ³•å°†å¤šè¯­è¨€è¾“å…¥å‡†ç¡®è½¬åŒ–ä¸ºå…¶æ¨ç†è·¯å¾„ä¸­å ä¸»å¯¼åœ°ä½çš„è¯­è¨€ï¼ˆé€šå¸¸ä¸ºè‹±è¯­ï¼‰ã€‚ç ”ç©¶è€…é€šè¿‡è¯„ä¼°å¤šç§æ£€æµ‹æ–¹æ³•ï¼Œè¯æ˜äº†è¿™ç§ç†è§£å¤±æ•ˆåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å¯ä»¥è¢«è¯†åˆ«çš„ï¼Œå¹¶æ®æ­¤æå‡ºäº†é€‰æ‹©æ€§ç¿»è¯‘ (Selective Translation) ç­–ç•¥ã€‚è¯¥ç­–ç•¥ä»…åœ¨æ£€æµ‹åˆ°ç†è§£å¤±è´¥æ—¶æ‰å°†è¾“å…¥ç¿»è¯‘æˆè‹±è¯­ï¼Œå®éªŒæ˜¾ç¤ºåœ¨ Qwen3-4B æ¨¡å‹ä¸Šä»…éœ€ç¿»è¯‘ 20% çš„å†…å®¹å³å¯è¾¾åˆ°æ¥è¿‘å…¨é‡ç¿»è¯‘çš„æ¨ç†æ°´å¹³ã€‚è¿™ä¸€ç ”ç©¶æ˜ç¡®äº†è¯­è¨€ç†è§£å¤±è´¥æ˜¯å¯¼è‡´å¤šè¯­è¨€æ¨ç†å·®å¼‚çš„æ ¸å¿ƒé©±åŠ¨å› ç´ ï¼Œå¹¶ä¸ºç¼©å°è¯­è¨€èµ„æºå·®è·ã€å®ç°æ›´å…¬å¹³çš„å¤šè¯­è¨€æ¨ç†æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "v2: Fix typos and updated contents",
      "pdf_url": "https://arxiv.org/pdf/2510.27269v2",
      "published_date": "2025-10-31 08:17:59 UTC",
      "updated_date": "2026-01-01 17:05:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:23:26.424423+00:00"
    },
    {
      "arxiv_id": "2510.27267v1",
      "title": "MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models",
      "title_zh": "MedCalc-Eval ä¸ MedCalc-Envï¼šæå‡å¤§è¯­è¨€æ¨¡å‹çš„åŒ»ç–—è®¡ç®—èƒ½åŠ›",
      "authors": [
        "Kangkun Mao",
        "Jinru Ding",
        "Jiayuan Chen",
        "Mouxiao Bian",
        "Ruiyao Chen",
        "Xinwei Peng",
        "Sijie Ren",
        "Linyang Li",
        "Jie Xu"
      ],
      "abstract": "As large language models (LLMs) enter the medical domain, most benchmarks evaluate them on question answering or descriptive reasoning, overlooking quantitative reasoning critical to clinical decision-making. Existing datasets like MedCalc-Bench cover few calculation tasks and fail to reflect real-world computational scenarios.\n  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical calculation abilities, comprising 700+ tasks across two types: equation-based (e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar, Glasgow Coma Scale). These tasks span diverse specialties including internal medicine, surgery, pediatrics, and cardiology, offering a broader and more challenging evaluation setting.\n  To improve performance, we further develop MedCalc-Env, a reinforcement learning environment built on the InternBootcamp framework, enabling multi-step clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this environment achieves state-of-the-art results on MedCalc-Eval, with notable gains in numerical sensitivity, formula selection, and reasoning robustness. Remaining challenges include unit conversion, multi-condition logic, and contextual understanding.\n  Code and datasets are available at https://github.com/maokangkun/MedCalc-Eval.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ»ç–—é¢†åŸŸç¼ºä¹å®šé‡æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†ç›®å‰è§„æ¨¡æœ€å¤§çš„åŒ»ç–—è®¡ç®—åŸºå‡†æµ‹è¯•é›† MedCalc-Evalã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡700é¡¹ä»»åŠ¡ï¼Œæ¶µç›–äº†åŸºäºå…¬å¼(equation-based)å’ŒåŸºäºè§„åˆ™è¯„åˆ†(rule-based scoring systems)çš„ä¸¤ç±»è®¡ç®—ï¼Œå¹¿æ³›æ¶‰åŠå†…ç§‘ã€å¤–ç§‘ã€å„¿ç§‘å’Œå¿ƒè„ç—…å­¦ç­‰å¤šä¸ªä¸“ä¸šé¢†åŸŸã€‚ä¸ºäº†æå‡æ¨¡å‹è¡¨ç°ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åŸºäº InternBootcamp æ¡†æ¶çš„å¼ºåŒ–å­¦ä¹ (reinforcement learning)ç¯å¢ƒ MedCalc-Envï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹çš„å¤šæ­¥ä¸´åºŠæ¨ç†ä¸è§„åˆ’èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è¯¥ç¯å¢ƒä¸­å¾®è°ƒåçš„ Qwen2.5-32B æ¨¡å‹åœ¨ MedCalc-Eval ä¸Šè¾¾åˆ°äº† SOTA æ°´å¹³ï¼Œæ˜¾è‘—æé«˜äº†æ•°å€¼æ•æ„Ÿåº¦ã€å…¬å¼é€‰æ‹©å‡†ç¡®æ€§åŠæ¨ç†ç¨³å¥æ€§ã€‚å°½ç®¡å–å¾—è¿›å±•ï¼Œç ”ç©¶ä¹ŸæŒ‡å‡ºå¤§è¯­è¨€æ¨¡å‹åœ¨å•ä½æ¢ç®—ã€å¤šæ¡ä»¶é€»è¾‘åŠä¸Šä¸‹æ–‡ç†è§£ç­‰å¤æ‚åŒ»ç–—è®¡ç®—åœºæ™¯ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27267v1",
      "published_date": "2025-10-31 08:07:16 UTC",
      "updated_date": "2025-10-31 08:07:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:23:27.273883+00:00"
    },
    {
      "arxiv_id": "2511.00122v1",
      "title": "Engineering.ai: A Platform for Teams of AI Engineers in Computational Design",
      "title_zh": "Engineering.aiï¼šé¢å‘è®¡ç®—è®¾è®¡çš„ AI å·¥ç¨‹å¸ˆå›¢é˜Ÿå¹³å°",
      "authors": [
        "Ran Xu",
        "Yupeng Qi",
        "Jingsen Feng",
        "Xu Chu"
      ],
      "abstract": "In modern engineering practice, human engineers collaborate in specialized teams to design complex products, with each expert completing their respective tasks while communicating and exchanging results and data with one another. While this division of expertise is essential for managing multidisciplinary complexity, it demands substantial development time and cost. Recently, we introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer for computational fluid dynamics, and turbulence.ai, which can conduct end-to-end research in fluid mechanics draft publications and PhD theses. Building upon these foundations, we present Engineering.ai, a platform for teams of AI engineers in computational design. The framework employs a hierarchical multi-agent architecture where a Chief Engineer coordinates specialized agents consisting of Aerodynamics, Structural, Acoustic, and Optimization Engineers, each powered by LLM with domain-specific knowledge. Agent-agent collaboration is achieved through file-mediated communication for data provenance and reproducibility, while a comprehensive memory system maintains project context, execution history, and retrieval-augmented domain knowledge to ensure reliable decision-making across the workflow. The system integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis, enabling parallel multidisciplinary simulations while maintaining computational accuracy. The framework is validated through UAV wing optimization. This work demonstrates that agentic-AI-enabled AI engineers has the potential to perform complex engineering tasks autonomously. Remarkably, the automated workflow achieved a 100% success rate across over 400 parametric configurations, with zero mesh generation failures, solver convergence issues, or manual interventions required, validating that the framework is trustworthy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Engineering.aiï¼Œä¸€ä¸ªé¢å‘è®¡ç®—è®¾è®¡ï¼ˆComputational Designï¼‰é¢†åŸŸçš„ AI å·¥ç¨‹å¸ˆå›¢é˜Ÿåä½œå¹³å°ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œè§£å†³ç°ä»£å·¥ç¨‹è®¾è®¡ä¸­å¤šå­¦ç§‘å¤æ‚æ€§å¸¦æ¥çš„é«˜æ˜‚æˆæœ¬ä¸å¼€å‘å‘¨æœŸé—®é¢˜ã€‚å¹³å°é‡‡ç”¨å±‚çº§åŒ–å¤šæ™ºèƒ½ä½“æ¶æ„ï¼ˆHierarchical Multi-agent Architectureï¼‰ï¼Œç”±ä¸€å Chief Engineer è´Ÿè´£åè°ƒ Aerodynamicsã€Structuralã€Acoustic å’Œ Optimization ç­‰ä¸“ä¸šé¢†åŸŸæ™ºèƒ½ä½“ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“å‡ç”±å…·å¤‡ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„ LLM é©±åŠ¨ã€‚è¯¥æ¡†æ¶é€šè¿‡æ–‡ä»¶åª’ä»‹é€šä¿¡ï¼ˆFile-mediated Communicationï¼‰ç¡®ä¿æ•°æ®çš„å¯è¿½æº¯æ€§ä¸å¯é‡å¤æ€§ï¼Œå¹¶ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ä¸å…¨æ–¹ä½è®°å¿†ç³»ç»Ÿæ¥ä¿éšœå†³ç­–çš„å¯é æ€§ã€‚ç³»ç»Ÿæ·±åº¦æ•´åˆäº† FreeCADã€Gmshã€OpenFOAMã€CalculiX å’Œ BPM å£°å­¦åˆ†æç­‰å·¥å…·ï¼Œå®ç°äº†å¹¶è¡ŒåŒ–çš„å¤šå­¦ç§‘ä»¿çœŸã€‚é€šè¿‡ UAV æœºç¿¼ä¼˜åŒ–çš„å®éªŒéªŒè¯ï¼Œè¯¥æ¡†æ¶åœ¨ 400 å¤šä¸ªå‚æ•°é…ç½®ä¸­å®ç°äº† 100% çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œä¸”åœ¨ç½‘æ ¼ç”Ÿæˆä¸æ±‚è§£å™¨æ”¶æ•›ç­‰æ–¹é¢æ— éœ€ä»»ä½•äººå·¥å¹²é¢„ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†åŸºäºæ™ºèƒ½ä½“çš„ AI å·¥ç¨‹å¸ˆï¼ˆAgentic-AI-enabled AI engineersï¼‰åœ¨è‡ªä¸»æ‰§è¡Œå¤æ‚å·¥ç¨‹ä»»åŠ¡æ–¹é¢å…·æœ‰æé«˜çš„å¯é æ€§ä¸åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00122v1",
      "published_date": "2025-10-31 08:00:48 UTC",
      "updated_date": "2025-10-31 08:00:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:23:31.430552+00:00"
    },
    {
      "arxiv_id": "2510.27258v1",
      "title": "Higher-order Linear Attention",
      "title_zh": "é«˜é˜¶çº¿æ€§æ³¨æ„åŠ›",
      "authors": [
        "Yifan Zhang",
        "Zhen Qin",
        "Quanquan Gu"
      ],
      "abstract": "The quadratic cost of scaled dot-product attention is a central obstacle to scaling autoregressive language models to long contexts. Linear-time attention and State Space Models (SSMs) provide scalable alternatives but are typically restricted to first-order or kernel-based approximations, which can limit expressivity. We introduce Higher-order Linear Attention (HLA), a causal, streaming mechanism that realizes higher interactions via compact prefix sufficient statistics. In the second-order case, HLA maintains a constant-size state and computes per-token outputs in linear time without materializing any $n \\times n$ matrices. We give closed-form streaming identities, a strictly causal masked variant using two additional summaries, and a chunk-parallel training scheme based on associative scans that reproduces the activations of a serial recurrence exactly. We further outline extensions to third and higher orders. Collectively, these results position HLA as a principled, scalable building block that combines attention-like, data-dependent mixing with the efficiency of modern recurrent architectures. Project Page: https://github.com/yifanzhang-pro/HLA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªå›å½’è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶ç‚¹ç§¯æ³¨æ„åŠ›è®¡ç®—å¼€é”€è¿‡å¤§çš„é—®é¢˜ï¼Œæå‡ºäº† Higher-order Linear Attention (HLA)ã€‚ä½œä¸ºä¸€ç§å› æœæµå¼æœºåˆ¶ï¼ŒHLA é€šè¿‡ç´§å‡‘çš„å‰ç¼€å……åˆ†ç»Ÿè®¡é‡ (prefix sufficient statistics) å®ç°é«˜é˜¶äº¤äº’ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ä¼ ç»Ÿçº¿æ€§æ³¨æ„åŠ›å’Œ State Space Models (SSMs) åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚åœ¨äºŒé˜¶é…ç½®ä¸‹ï¼ŒHLA èƒ½å¤Ÿç»´æŒæ’å®šå¤§å°çš„çŠ¶æ€ï¼Œå¹¶åœ¨ä¸ç”Ÿæˆ $n \\times n$ çŸ©é˜µçš„å‰æä¸‹å®ç°çº¿æ€§æ—¶é—´çš„ Token è¾“å‡ºè®¡ç®—ã€‚ç ”ç©¶è¯¦ç»†ç»™å‡ºäº†é—­å¼æµå¼æ’ç­‰å¼ã€ä¸¥æ ¼å› æœæ©ç å˜ä½“ä»¥åŠåŸºäº associative scans çš„åˆ†å—å¹¶è¡Œè®­ç»ƒæ–¹æ¡ˆï¼Œå¹¶æ¢è®¨äº†å‘æ›´é«˜é˜¶æ‰©å±•çš„å¯èƒ½æ€§ã€‚HLA å°†ç±»æ³¨æ„åŠ›çš„åŠ¨æ€æ•°æ®æ··åˆèƒ½åŠ›ä¸å¾ªç¯æ¶æ„çš„é«˜æ•ˆæ€§ç›¸ç»“åˆï¼Œä¸ºæ„å»ºé«˜æ€§èƒ½ã€å¯æ‰©å±•çš„è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ä¸ªå…·æœ‰åŸåˆ™æ€§çš„åŸºç¡€ç»„ä»¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Project Page: https://github.com/yifanzhang-pro/HLA",
      "pdf_url": "https://arxiv.org/pdf/2510.27258v1",
      "published_date": "2025-10-31 07:54:37 UTC",
      "updated_date": "2025-10-31 07:54:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:23:27.424336+00:00"
    },
    {
      "arxiv_id": "2510.27254v1",
      "title": "Languages are Modalities: Cross-Lingual Alignment via Encoder Injection",
      "title_zh": "è¯­è¨€å³æ¨¡æ€ï¼šåŸºäºç¼–ç å™¨æ³¨å…¥çš„è·¨è¯­è¨€å¯¹é½",
      "authors": [
        "Rajan Agarwal",
        "Aarush Gupta"
      ],
      "abstract": "Instruction-tuned Large Language Models (LLMs) underperform on low resource, non-Latin scripts due to tokenizer fragmentation and weak cross-lingual coupling. We present LLINK (Latent Language Injection for Non-English Knowledge), a compute efficient language-as-modality method that conditions an instruction-tuned decoder without changing the tokenizer or retraining the decoder. First, we align sentence embeddings from a frozen multilingual encoder to the decoder's latent embedding space at a reserved position via a lightweight contrastive projector. Second, the vector is expanded into K soft slots and trained with minimal adapters so the frozen decoder consumes the signal. LLINK substantially improves bilingual retrieval and achieves 81.3% preference over the base model and 63.6% over direct fine-tuning in LLM-judged Q&A evaluations. We further find that improvements can be attributed to reduced tokenization inflation and a stronger cross lingual alignment, despite the model having residual weaknesses in numeric fidelity. Treating low resource languages as a modality offers a practical path to stronger cross-lingual alignment in lightweight LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŒ‡ä»¤å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä½èµ„æºã€éæ‹‰ä¸è¯­ç³»è„šæœ¬ä¸Šå› åˆ†è¯å™¨ç¢ç‰‡åŒ– (tokenizer fragmentation) å’Œè·¨è¯­è¨€è€¦åˆå¼±è€Œè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº† LLINK (Latent Language Injection for Non-English Knowledge)ã€‚è¿™æ˜¯ä¸€ç§å°†è¯­è¨€è§†ä¸ºæ¨¡æ€ (language-as-modality) çš„é«˜æ•ˆè®¡ç®—æ–¹æ³•ï¼Œåœ¨ä¸æ”¹å˜åˆ†è¯å™¨æˆ–é‡æ–°è®­ç»ƒè§£ç å™¨çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨è½»é‡çº§å¯¹æ¯”æŠ•å½±å™¨ (contrastive projector) å°†å†»ç»“çš„å¤šè¯­è¨€ç¼–ç å™¨çš„å¥å­åµŒå…¥å¯¹é½åˆ°è§£ç å™¨çš„æ½œåœ¨ç©ºé—´ã€‚é€šè¿‡å°†å‘é‡æ‰©å±•ä¸º K ä¸ªè½¯æ’æ§½ (soft slots) å¹¶ç»“åˆé€‚é…å™¨ (adapters) è®­ç»ƒï¼Œä½¿å†»ç»“çš„è§£ç å™¨èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è¯¥ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒLLINK æ˜¾è‘—æé«˜äº†åŒè¯­æ£€ç´¢èƒ½åŠ›ï¼Œåœ¨é—®ç­”è¯„ä¼°ä¸­å…¶åå¥½åº¦æ¯”åŸºåº§æ¨¡å‹é«˜å‡º 81.3%ï¼Œæ¯”ç›´æ¥å¾®è°ƒé«˜å‡º 63.6%ã€‚æ€§èƒ½çš„æå‡ä¸»è¦å½’åŠŸäºåˆ†è¯è†¨èƒ€ (tokenization inflation) çš„å‡å°‘å’Œæ›´å¼ºçš„è·¨è¯­è¨€å¯¹é½ã€‚è¯¥ç ”ç©¶è¯æ˜å°†ä½èµ„æºè¯­è¨€è§†ä¸ºä¸€ç§æ¨¡æ€ï¼Œæ˜¯å®ç°è½»é‡çº§æ¨¡å‹è·¨è¯­è¨€å¯¹é½çš„ä¸€æ¡åˆ‡å®ä¸”æœ‰æ•ˆçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 3 Figures",
      "pdf_url": "https://arxiv.org/pdf/2510.27254v1",
      "published_date": "2025-10-31 07:43:21 UTC",
      "updated_date": "2025-10-31 07:43:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:23:38.329681+00:00"
    },
    {
      "arxiv_id": "2510.27253v1",
      "title": "Not All Instances Are Equally Valuable: Towards Influence-Weighted Dataset Distillation",
      "title_zh": "å¹¶éæ‰€æœ‰å®ä¾‹éƒ½å…·æœ‰åŒç­‰ä»·å€¼ï¼šè¿ˆå‘å½±å“åŠ æƒçš„æ•°æ®é›†è’¸é¦",
      "authors": [
        "Qiyan Deng",
        "Changqian Zheng",
        "Lianpeng Qiao",
        "Yuping Wang",
        "Chengliang Chai",
        "Lei Cao"
      ],
      "abstract": "Dataset distillation condenses large datasets into synthetic subsets, achieving performance comparable to training on the full dataset while substantially reducing storage and computation costs. Most existing dataset distillation methods assume that all real instances contribute equally to the process. In practice, real-world datasets contain both informative and redundant or even harmful instances, and directly distilling the full dataset without considering data quality can degrade model performance. In this work, we present Influence-Weighted Distillation IWD, a principled framework that leverages influence functions to explicitly account for data quality in the distillation process. IWD assigns adaptive weights to each instance based on its estimated impact on the distillation objective, prioritizing beneficial data while downweighting less useful or harmful ones. Owing to its modular design, IWD can be seamlessly integrated into diverse dataset distillation frameworks. Our empirical results suggest that integrating IWD tends to improve the quality of distilled datasets and enhance model performance, with accuracy gains of up to 7.8%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•°æ®é›†è’¸é¦ (Dataset distillation) ä¸­é»˜è®¤æ‰€æœ‰åŸå§‹å®ä¾‹é‡è¦æ€§å‡ç­‰ï¼Œä»è€Œå¿½ç•¥æ•°æ®è´¨é‡å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº† Influence-Weighted Distillation (IWD) æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å½±å“å‡½æ•° (Influence Functions) åœ¨è’¸é¦è¿‡ç¨‹ä¸­æ˜¾å¼è¯„ä¼°æ•°æ®è´¨é‡ï¼Œå¹¶æ ¹æ®æ¯ä¸ªå®ä¾‹å¯¹è’¸é¦ç›®æ ‡çš„ä¼°è®¡å½±å“åˆ†é…è‡ªé€‚åº”æƒé‡ã€‚é€šè¿‡ä¼˜å…ˆå¤„ç†æœ‰ç›Šæ•°æ®å¹¶é™ä½å†—ä½™æˆ–æœ‰å®³å®ä¾‹çš„æƒé‡ï¼ŒIWD èƒ½å¤Ÿæ›´ç²¾å‡†åœ°æç‚¼æ ¸å¿ƒä¿¡æ¯ã€‚ç”±äºå…¶æ¨¡å—åŒ–è®¾è®¡ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°å¤šç§ç°æœ‰çš„æ•°æ®é›†è’¸é¦æ¡†æ¶ä¸­ã€‚å®éªŒç»“æœè¯æ˜ï¼Œé›†æˆ IWD èƒ½æ˜¾è‘—æå‡è’¸é¦æ•°æ®é›†çš„è´¨é‡å¹¶å¢å¼ºæ¨¡å‹æ€§èƒ½ï¼Œä½¿å‡†ç¡®ç‡æœ€é«˜æå‡ 7.8%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27253v1",
      "published_date": "2025-10-31 07:41:41 UTC",
      "updated_date": "2025-10-31 07:41:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:23:43.533019+00:00"
    },
    {
      "arxiv_id": "2510.27247v1",
      "title": "Reconstructing Unseen Sentences from Speech-related Biosignals for Open-vocabulary Neural Communication",
      "title_zh": "åŸºäºè¯­éŸ³ç›¸å…³ç”Ÿç‰©ä¿¡å·çš„æœªè§è¯­å¥é‡æ„ï¼šé¢å‘å¼€æ”¾è¯æ±‡ç¥ç»é€šä¿¡",
      "authors": [
        "Deok-Seon Kim",
        "Seo-Hyun Lee",
        "Kang Yin",
        "Seong-Whan Lee"
      ],
      "abstract": "Brain-to-speech (BTS) systems represent a groundbreaking approach to human communication by enabling the direct transformation of neural activity into linguistic expressions. While recent non-invasive BTS studies have largely focused on decoding predefined words or sentences, achieving open-vocabulary neural communication comparable to natural human interaction requires decoding unconstrained speech. Additionally, effectively integrating diverse signals derived from speech is crucial for developing personalized and adaptive neural communication and rehabilitation solutions for patients. This study investigates the potential of speech synthesis for previously unseen sentences across various speech modes by leveraging phoneme-level information extracted from high-density electroencephalography (EEG) signals, both independently and in conjunction with electromyography (EMG) signals. Furthermore, we examine the properties affecting phoneme decoding accuracy during sentence reconstruction and offer neurophysiological insights to further enhance EEG decoding for more effective neural communication solutions. Our findings underscore the feasibility of biosignal-based sentence-level speech synthesis for reconstructing unseen sentences, highlighting a significant step toward developing open-vocabulary neural communication systems adapted to diverse patient needs and conditions. Additionally, this study provides meaningful insights into the development of communication and rehabilitation solutions utilizing EEG-based decoding technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡è¯­éŸ³ç›¸å…³çš„ç”Ÿç‰©ä¿¡å·é‡å»ºæœªè§è¿‡çš„å¥å­ï¼Œä»¥å®ç°å¼€æ”¾è¯æ±‡ï¼ˆOpen-vocabularyï¼‰çš„ç¥ç»é€šä¿¡ã€‚ä¼ ç»Ÿçš„è„‘æœºæ¥å£è¯­éŸ³è½¬æ¢ï¼ˆBrain-to-speech, BTSï¼‰ç³»ç»Ÿå¤šä¾§é‡äºè§£ç é¢„å®šä¹‰è¯æ±‡ï¼Œè€Œæœ¬ç ”ç©¶åˆ©ç”¨ä»é«˜å¯†åº¦è„‘ç”µå›¾ï¼ˆEEGï¼‰åŠè‚Œç”µå›¾ï¼ˆEMGï¼‰ä¸­æå–çš„éŸ³ç´ çº§ï¼ˆPhoneme-levelï¼‰ä¿¡æ¯ï¼Œæ—¨åœ¨å®ç°éå—é™è¯­éŸ³çš„ç›´æ¥è½¬æ¢ã€‚ç ”ç©¶è¯¦ç»†è€ƒå¯Ÿäº†å½±å“éŸ³ç´ è§£ç å‡†ç¡®æ€§çš„å±æ€§ï¼Œå¹¶æä¾›äº†ç¥ç»ç”Ÿç†å­¦è§è§£ä»¥ä¼˜åŒ– EEG è§£ç æ€§èƒ½ã€‚å®éªŒç»“æœéªŒè¯äº†åŸºäºç”Ÿç‰©ä¿¡å·è¿›è¡Œå¥å­çº§è¯­éŸ³åˆæˆçš„å¯è¡Œæ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé‡å»ºåœ¨è®­ç»ƒä¸­æœªå‡ºç°è¿‡çš„å¥å­ã€‚è¿™ä¸€å‘ç°æ ‡å¿—ç€åœ¨å¼€å‘å¯é€‚åº”å¤šæ ·åŒ–æ‚£è€…éœ€æ±‚ã€å…·å¤‡ Open-vocabulary èƒ½åŠ›çš„ç¥ç»é€šä¿¡ä¸åº·å¤ç³»ç»Ÿæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering",
      "pdf_url": "https://arxiv.org/pdf/2510.27247v1",
      "published_date": "2025-10-31 07:31:13 UTC",
      "updated_date": "2025-10-31 07:31:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:23:47.026722+00:00"
    },
    {
      "arxiv_id": "2510.27246v1",
      "title": "Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs",
      "title_zh": "çªç ´ç™¾ä¸‡ Tokenï¼šå¤§è¯­è¨€æ¨¡å‹é•¿æœŸè®°å¿†èƒ½åŠ›çš„è¯„æµ‹ä¸å¢å¼º",
      "authors": [
        "Mohammad Tavakoli",
        "Alireza Salemi",
        "Carrie Ye",
        "Mohamed Abdalla",
        "Hamed Zamani",
        "J Ross Mitchell"
      ],
      "abstract": "Evaluating the abilities of large language models (LLMs) for tasks that require long-term memory and thus long-context reasoning, for example in conversational settings, is hampered by the existing benchmarks, which often lack narrative coherence, cover narrow domains, and only test simple recall-oriented tasks. This paper introduces a comprehensive solution to these challenges. First, we present a novel framework for automatically generating long (up to 10M tokens), coherent, and topically diverse conversations, accompanied by probing questions targeting a wide range of memory abilities. From this, we construct BEAM, a new benchmark comprising 100 conversations and 2,000 validated questions. Second, to enhance model performance, we propose LIGHT-a framework inspired by human cognition that equips LLMs with three complementary memory systems: a long-term episodic memory, a short-term working memory, and a scratchpad for accumulating salient facts. Our experiments on BEAM reveal that even LLMs with 1M token context windows (with and without retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT consistently improves performance across various models, achieving an average improvement of 3.5%-12.69% over the strongest baselines, depending on the backbone LLM. An ablation study further confirms the contribution of each memory component.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)é•¿æ–‡æœ¬æ¨ç†åŸºå‡†æµ‹è¯•åœ¨å™äº‹è¿è´¯æ€§å’Œé¢†åŸŸè¦†ç›–é¢æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†ä¸€ç§èƒ½ç”Ÿæˆé«˜è¾¾1000ä¸‡(10M)tokenè¿è´¯å¯¹è¯çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå¹¶æ®æ­¤æ„å»ºäº†åŒ…å«2000ä¸ªéªŒè¯é—®é¢˜çš„BEAMåŸºå‡†ã€‚ä¸ºè§£å†³æ¨¡å‹åœ¨é•¿å¯¹è¯ä¸­è®°å¿†è¡°å‡çš„é—®é¢˜ï¼Œç ”ç©¶è€…å—äººç±»è®¤çŸ¥å¯å‘æå‡ºäº†LIGHTæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é•¿æœŸæƒ…æ™¯è®°å¿†(episodic memory)ã€çŸ­æœŸå·¥ä½œè®°å¿†(working memory)å’Œç”¨äºè®°å½•å…³é”®äº‹å®çš„æš‚å­˜å™¨(scratchpad)ä¸‰ä¸ªç³»ç»Ÿå¢å¼ºè®°å¿†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯å…·å¤‡100ä¸‡(1M)tokenä¸Šä¸‹æ–‡çª—å£çš„æ¨¡å‹åœ¨å¤„ç†æé•¿å¯¹è¯æ—¶ä¹Ÿä¼šé‡åˆ°æ˜æ˜¾æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLIGHTæ¡†æ¶åœ¨å„ç§æ¨¡å‹ä¸Šå‡å®ç°äº†3.5%è‡³12.69%çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†å…¶å„è®°å¿†ç»„ä»¶åœ¨ç»´æŒé•¿æ•ˆè®°å¿†å’Œæå‡é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„å…³é”®ååŒä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27246v1",
      "published_date": "2025-10-31 07:29:52 UTC",
      "updated_date": "2025-10-31 07:29:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:24:09.726661+00:00"
    },
    {
      "arxiv_id": "2510.27244v1",
      "title": "Vintage Code, Modern Judges: Meta-Validation in Low Data Regimes",
      "title_zh": "ç»å…¸ä»£ç ï¼Œç°ä»£è¯„åˆ¤ï¼šä½æ•°æ®ç¯å¢ƒä¸‹çš„å…ƒéªŒè¯",
      "authors": [
        "Ora Nova Fandina",
        "Gal Amram",
        "Eitan Farchi",
        "Shmulik Froimovich",
        "Raviv Gal",
        "Wesam Ibraheem",
        "Rami Katan",
        "Alice Podolsky",
        "Orna Raz"
      ],
      "abstract": "Application modernization in legacy languages such as COBOL, PL/I, and REXX faces an acute shortage of resources, both in expert availability and in high-quality human evaluation data. While Large Language Models as a Judge (LaaJ) offer a scalable alternative to expert review, their reliability must be validated before being trusted in high-stakes workflows. Without principled validation, organizations risk a circular evaluation loop, where unverified LaaJs are used to assess model outputs, potentially reinforcing unreliable judgments and compromising downstream deployment decisions. Although various automated approaches to validating LaaJs have been proposed, alignment with human judgment remains a widely used and conceptually grounded validation strategy. In many real-world domains, the availability of human-labeled evaluation data is severely limited, making it difficult to assess how well a LaaJ aligns with human judgment. We introduce SparseAlign, a formal framework for assessing LaaJ alignment with sparse human-labeled data. SparseAlign combines a novel pairwise-confidence concept with a score-sensitive alignment metric that jointly capture ranking consistency and score proximity, enabling reliable evaluator selection even when traditional statistical methods are ineffective due to limited annotated examples. SparseAlign was applied internally to select LaaJs for COBOL code explanation. The top-aligned evaluators were integrated into assessment workflows, guiding model release decisions. We present a case study of four LaaJs to demonstrate SparseAlign's utility in real-world evaluation scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ COBOLã€PL/I å’Œ REXX ç­‰é—ç•™è¯­è¨€åœ¨åº”ç”¨ç¨‹åºç°ä»£åŒ–è¿‡ç¨‹ä¸­é¢ä¸´çš„ä¸“å®¶èµ„æºçŸ­ç¼ºå’Œé«˜è´¨é‡äººå·¥è¯„ä¼°æ•°æ®åŒ®ä¹é—®é¢˜ï¼Œæ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºè£åˆ¤ (Large Language Models as a Judge, LaaJ) çš„å¯é æ€§éªŒè¯ã€‚ä¸ºäº†é¿å…ç»„ç»‡é™·å…¥æœªç»è¯å®çš„ LaaJ è¯„ä¼°å¾ªç¯å¹¶ç”±æ­¤å¯¼è‡´çš„ä¸‹æ¸¸éƒ¨ç½²å†³ç­–é£é™©ï¼Œä½œè€…å¼ºè°ƒäº†å°† LaaJ ä¸äººå·¥åˆ¤æ–­å¯¹é½è¿›è¡Œå…ƒéªŒè¯ (Meta-Validation) çš„å¿…è¦æ€§ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º SparseAlign çš„å½¢å¼åŒ–æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºåœ¨äººå·¥æ ‡æ³¨æ•°æ®æå…¶æœ‰é™çš„ç¨€ç–æ•°æ®åœºæ™¯ä¸‹è¯„ä¼° LaaJ çš„å¯¹é½ç¨‹åº¦ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åˆ›æ–°çš„æˆå¯¹ç½®ä¿¡åº¦ (pairwise-confidence) æ¦‚å¿µå’Œå¾—åˆ†æ•æ„Ÿçš„å¯¹é½æŒ‡æ ‡ï¼Œé€šè¿‡åŒæ—¶æ•æ‰æ’åä¸€è‡´æ€§å’Œå¾—åˆ†æ¥è¿‘åº¦ï¼Œç¡®ä¿åœ¨ä¼ ç»Ÿç»Ÿè®¡æ–¹æ³•å› æ ·æœ¬é‡ä¸è¶³è€Œå¤±æ•ˆæ—¶ä»èƒ½å¯é åœ°ç­›é€‰è¯„ä¼°å™¨ã€‚SparseAlign å·²åœ¨å†…éƒ¨åº”ç”¨äº COBOL ä»£ç è§£é‡Šä»»åŠ¡çš„è¯„ä¼°å™¨ç­›é€‰ï¼Œå¹¶æˆåŠŸæŒ‡å¯¼äº†æ¨¡å‹å‘å¸ƒå†³ç­–ã€‚é€šè¿‡å¯¹å››ä¸ª LaaJ çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥ç ”ç©¶è¯æ˜äº† SparseAlign åœ¨ç°å®ä¸–ç•Œè¯„ä¼°åœºæ™¯ä¸­çš„å®ç”¨æ€§ï¼Œä¸ºä½æ•°æ®ç¯å¢ƒä¸‹æ„å»ºå¯ä¿¡çš„è‡ªåŠ¨åŒ–è¯„ä¼°å·¥ä½œæµæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27244v1",
      "published_date": "2025-10-31 07:27:54 UTC",
      "updated_date": "2025-10-31 07:27:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:24:07.528792+00:00"
    },
    {
      "arxiv_id": "2510.27238v1",
      "title": "DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries",
      "title_zh": "DRAMAï¼šç»Ÿä¸€å¼€æ”¾åŸŸåˆ†ææŸ¥è¯¢çš„æ•°æ®æ£€ç´¢ä¸åˆ†æ",
      "authors": [
        "Chuxuan Hu",
        "Maxwell Yang",
        "James Weiland",
        "Yeji Lim",
        "Suhas Palawala",
        "Daniel Kang"
      ],
      "abstract": "Manually conducting real-world data analyses is labor-intensive and inefficient. Despite numerous attempts to automate data science workflows, none of the existing paradigms or systems fully demonstrate all three key capabilities required to support them effectively: (1) open-domain data collection, (2) structured data transformation, and (3) analytic reasoning.\n  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that answers users' analytic queries in natural language on large-scale open-domain data. DRAMA unifies data collection, transformation, and analysis as a single pipeline. To quantitatively evaluate system performance on tasks representative of DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories of tasks: claim verification and question answering, each comprising 100 instances. These tasks are derived from real-world applications that have gained significant public attention and require the retrieval and analysis of open-domain data. We develop DRAMA-Bot, a multi-agent system designed following DRAMA. It comprises a data retriever that collects and transforms data by coordinating the execution of sub-agents, and a data analyzer that performs structured reasoning over the retrieved data. We evaluate DRAMA-Bot on DRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot achieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines with up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is publicly available at https://github.com/uiuc-kang-lab/drama.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DRAMAï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç»Ÿä¸€ open-domain æ•°æ®æ£€ç´¢ä¸åˆ†æçš„ç«¯åˆ°ç«¯èŒƒå¼ï¼Œè§£å†³äº†ä¼ ç»Ÿæ‰‹åŠ¨æ•°æ®ç§‘å­¦å·¥ä½œæµä¸­åŠ³åŠ¨å¯†é›†ä¸”æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚DRAMA å°†æ•°æ®é‡‡é›†ã€ç»“æ„åŒ–æ•°æ®è½¬æ¢ï¼ˆstructured data transformationï¼‰ä»¥åŠåˆ†ææ¨ç†ï¼ˆanalytic reasoningï¼‰æ•´åˆä¸ºä¸€ä¸ªå•ä¸€æµç¨‹ï¼Œæ”¯æŒé€šè¿‡è‡ªç„¶è¯­è¨€å›ç­”å¤§è§„æ¨¡å¼€æ”¾é¢†åŸŸçš„åˆ†ææŸ¥è¯¢ã€‚ä¸ºäº†è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ï¼Œä½œè€…æ„å»ºäº†åŒ…å«å£°æ˜éªŒè¯ï¼ˆclaim verificationï¼‰å’Œé—®ç­”ä»»åŠ¡çš„ DRAMA-Bench åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›ä»»åŠ¡å‡æºäºçœŸå®ä¸–ç•Œçš„åº”ç”¨åœºæ™¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ DRAMA-Botï¼Œå®ƒé€šè¿‡åè°ƒæ•°æ®æ£€ç´¢å™¨ï¼ˆdata retrieverï¼‰è¿›è¡Œæ•°æ®æ”¶é›†ä¸è½¬æ¢ï¼Œå¹¶åˆ©ç”¨æ•°æ®åˆ†æå™¨ï¼ˆdata analyzerï¼‰å¯¹æ£€ç´¢åˆ°çš„æ•°æ®è¿›è¡Œç»“æ„åŒ–æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRAMA-Bot åœ¨ DRAMA-Bench ä¸Šå®ç°äº† 86.5% çš„ä»»åŠ¡å‡†ç¡®ç‡ï¼Œä¸äº”ä¸ªæœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œå…¶å‡†ç¡®ç‡æœ€é«˜æå‡äº† 6.9 å€ï¼Œä¸”è¿è¡Œæˆæœ¬é™ä½è‡³ä¸åˆ°å…­åˆ†ä¹‹ä¸€ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.DB",
      "comment": "Accepted to SIGMOD 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.27238v1",
      "published_date": "2025-10-31 07:00:21 UTC",
      "updated_date": "2025-10-31 07:00:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:24:10.176197+00:00"
    },
    {
      "arxiv_id": "2510.27222v1",
      "title": "Soft Task-Aware Routing of Experts for Equivariant Representation Learning",
      "title_zh": "é¢å‘ç­‰å˜è¡¨ç¤ºå­¦ä¹ çš„è½¯ä»»åŠ¡æ„ŸçŸ¥ä¸“å®¶è·¯ç”±",
      "authors": [
        "Jaebyeong Jeon",
        "Hyeonseo Jang",
        "Jy-yong Sohn",
        "Kibok Lee"
      ],
      "abstract": "Equivariant representation learning aims to capture variations induced by input transformations in the representation space, whereas invariant representation learning encodes semantic information by disregarding such transformations. Recent studies have shown that jointly learning both types of representations is often beneficial for downstream tasks, typically by employing separate projection heads. However, this design overlooks information shared between invariant and equivariant learning, which leads to redundant feature learning and inefficient use of model capacity. To address this, we introduce Soft Task-Aware Routing (STAR), a routing strategy for projection heads that models them as experts. STAR induces the experts to specialize in capturing either shared or task-specific information, thereby reducing redundant feature learning. We validate this effect by observing lower canonical correlations between invariant and equivariant embeddings. Experimental results show consistent improvements across diverse transfer learning tasks. The code is available at https://github.com/YonseiML/star.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç­‰å˜è¡¨ç¤ºå­¦ä¹ (Equivariant representation learning)ä¸ä¸å˜è¡¨ç¤ºå­¦ä¹ (Invariant representation learning)çš„ç»“åˆï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨ç‹¬ç«‹çš„æŠ•å½±å¤´(projection heads)ä¼šå¯¼è‡´å†—ä½™ç‰¹å¾å­¦ä¹ åŠæ¨¡å‹å®¹é‡åˆ©ç”¨ç‡ä½çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†è½¯ä»»åŠ¡æ„ŸçŸ¥è·¯ç”±(Soft Task-Aware Routing, STAR)ï¼Œè¿™æ˜¯ä¸€ç§å°†æŠ•å½±å¤´å»ºæ¨¡ä¸ºä¸“å®¶(experts)çš„è·¯ç”±ç­–ç•¥ã€‚STAR é€šè¿‡å¼•å¯¼ä¸“å®¶ä¸“æ³¨äºæ•æ‰å…±äº«ä¿¡æ¯æˆ–ç‰¹å®šä»»åŠ¡ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†é‡å¤çš„ç‰¹å¾è¡¨ç¤ºã€‚ç ”ç©¶äººå‘˜é€šè¿‡è§‚å¯Ÿåˆ°ä¸å˜å’Œç­‰å˜åµŒå…¥ä¹‹é—´è¾ƒä½çš„å…¸å‹ç›¸å…³æ€§(canonical correlations)ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•å‡å°‘å†—ä½™çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTAR åœ¨å¤šç§è¿ç§»å­¦ä¹ (transfer learning)ä»»åŠ¡ä¸­å‡å–å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.27222v1",
      "published_date": "2025-10-31 06:34:30 UTC",
      "updated_date": "2025-10-31 06:34:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:24:12.834200+00:00"
    },
    {
      "arxiv_id": "2511.02854v1",
      "title": "SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation",
      "title_zh": "SELF-REDRAFTï¼šæ¿€å‘ä»£ç ç”Ÿæˆæµ‹è¯•æ—¶æ‰©å±•ä¸­å†…åœ¨çš„æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡",
      "authors": [
        "Yixiang Chen",
        "Tianshi Zheng",
        "Shijue Huang",
        "Zhitao He",
        "Yi R. Fung"
      ],
      "abstract": "Test-time scaling without interpreter feedback is essential for real-world code generation scenarios where test cases are not readily available. While existing paradigms often rely on either greedy exploitation (i.e., iterative refinement) or stochastic exploration (i.e., relying on sample-based voting or reranking mechanisms), the balance between these two dimensions remains underexplored. To investigate the LLM's intrinsic ability to balance exploitation and exploration, we introduce SELF-REDRAFT, a framework built upon Self-Refine that encourages the model to propose new drafts for solutions that are fundamentally flawed. Our results show that SELF-REDRAFT consistently achieves better performance than Self-Refine when converged under the same maximum number of iterations. Still, we observe that significant room for improvement remains, largely due to two core aspects of current self-redraft capabilities: constrained capacity for generating instructive feedback and fragile discriminative judgment. We also find that balancing strategies vary notably across different LLMs, reflecting distinct, model-specific behaviors. Overall, our study establishes a baseline for intrinsic exploration-exploitation balancing in test-time scaling and identifies feedback and discrimination as key areas with potential for future advances.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SELF-REDRAFT æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä»£ç ç”Ÿæˆä»»åŠ¡åœ¨ç¼ºä¹è§£é‡Šå™¨åé¦ˆæ—¶ï¼Œå¦‚ä½•å¹³è¡¡æµ‹è¯•æ—¶æ‰©å±• (Test-Time Scaling) ä¸­çš„æ¢ç´¢ (Exploration) ä¸åˆ©ç”¨ (Exploitation) è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸åŒäºä»¥å¾€ä»…ä¾èµ–è´ªå©ªè¿­ä»£æ”¹è¿›æˆ–éšæœºé‡‡æ ·æŠ•ç¥¨çš„èŒƒå¼ï¼ŒSELF-REDRAFT åœ¨ Self-Refine åŸºç¡€ä¸Šï¼Œé€šè¿‡è¯±å¯¼æ¨¡å‹ä¸ºå­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·çš„æ–¹æ¡ˆé‡æ–°èµ·è‰æ–°ç¨¿ï¼Œä»è€Œæ¿€å‘å…¶å†…åœ¨çš„å¹³è¡¡èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ç›¸åŒè¿­ä»£æ¬¡æ•°ä¸‹ï¼ŒSELF-REDRAFT çš„æ€§èƒ½å§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„ Self-Refineã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæ¨¡å‹åœ¨ç”ŸæˆæŒ‡å¯¼æ€§åé¦ˆ (Feedback) å’Œåˆ¤åˆ«æ€§åˆ¤æ–­ (Discriminative Judgment) æ–¹é¢çš„è–„å¼±ç¯èŠ‚æ˜¯é™åˆ¶å…¶è¿›ä¸€æ­¥æå‡çš„ä¸»è¦ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¹³è¡¡ç­–ç•¥ä¸Šå±•ç°å‡ºæ˜¾è‘—çš„ç‰¹å®šè¡Œä¸ºå·®å¼‚ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºå†…åœ¨æ¢ç´¢-åˆ©ç”¨å¹³è¡¡å»ºç«‹äº†åŸºå‡†ï¼Œè¿˜ä¸ºæœªæ¥é€šè¿‡ä¼˜åŒ–åé¦ˆä¸åˆ¤åˆ«æœºåˆ¶æ¥æå‡ä»£ç ç”Ÿæˆèƒ½åŠ›æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "15 pages, 8 figures,2 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.02854v1",
      "published_date": "2025-10-31 06:30:47 UTC",
      "updated_date": "2025-10-31 06:30:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:24:11.230001+00:00"
    },
    {
      "arxiv_id": "2510.27213v1",
      "title": "Privacy-Aware Continual Self-Supervised Learning on Multi-Window Chest Computed Tomography for Domain-Shift Robustness",
      "title_zh": "é¢å‘åŸŸåç§»é²æ£’æ€§çš„å¤šçª—ä½èƒ¸éƒ¨CTéšç§æ„ŸçŸ¥æŒç»­è‡ªç›‘ç£å­¦ä¹ ",
      "authors": [
        "Ren Tasai",
        "Guang Li",
        "Ren Togo",
        "Takahiro Ogawa",
        "Kenji Hirata",
        "Minghui Tang",
        "Takaaki Yoshimura",
        "Hiroyuki Sugimori",
        "Noriko Nishioka",
        "Yukie Shimizu",
        "Kohsuke Kudo",
        "Miki Haseyama"
      ],
      "abstract": "We propose a novel continual self-supervised learning (CSSL) framework for simultaneously learning diverse features from multi-window-obtained chest computed tomography (CT) images and ensuring data privacy. Achieving a robust and highly generalizable model in medical image diagnosis is challenging, mainly because of issues, such as the scarcity of large-scale, accurately annotated datasets and domain shifts inherent to dynamic healthcare environments. Specifically, in chest CT, these domain shifts often arise from differences in window settings, which are optimized for distinct clinical purposes. Previous CSSL frameworks often mitigated domain shift by reusing past data, a typically impractical approach owing to privacy constraints. Our approach addresses these challenges by effectively capturing the relationship between previously learned knowledge and new information across different training stages through continual pretraining on unlabeled images. Specifically, by incorporating a latent replay-based mechanism into CSSL, our method mitigates catastrophic forgetting due to domain shifts during continual pretraining while ensuring data privacy. Additionally, we introduce a feature distillation technique that integrates Wasserstein distance-based knowledge distillation (WKD) and batch-knowledge ensemble (BKE), enhancing the ability of the model to learn meaningful, domain-shift-robust representations. Finally, we validate our approach using chest CT images obtained across two different window settings, demonstrating superior performance compared with other approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å½±åƒè¯Šæ–­ä¸­ç”±äºæ ‡æ³¨æ•°æ®ç¨€ç¼ºä»¥åŠèƒ¸éƒ¨ Computed Tomography (CT) çª—ä½è®¾ç½®ä¸åŒå¯¼è‡´çš„ Domain-Shift é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¼é¡¾æ•°æ®éšç§çš„æŒç»­è‡ªç›‘ç£å­¦ä¹  (Continual Self-Supervised Learning, CSSL) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨æ— æ ‡ç­¾å›¾åƒä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œå¹¶å¼•å…¥åŸºäº Latent Replay çš„æœºåˆ¶ï¼Œåœ¨ä¸é‡ç”¨åŸå§‹æ•°æ®ä»¥ç¡®ä¿éšç§çš„å‰æä¸‹ï¼Œæœ‰æ•ˆç¼“è§£äº†é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜ (Catastrophic Forgetting) é—®é¢˜ã€‚ä¸ºäº†å¢å¼ºæ¨¡å‹æå–å…·æœ‰é²æ£’æ€§çš„è¡¨å¾èƒ½åŠ›ï¼Œç ”ç©¶å¼•å…¥äº†æ•´åˆ Wasserstein Distance-based Knowledge Distillation (WKD) ä¸ Batch-Knowledge Ensemble (BKE) çš„ç‰¹å¾è’¸é¦æŠ€æœ¯ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä¸åŒè®­ç»ƒé˜¶æ®µæ–°æ—§çŸ¥è¯†é—´çš„å…³è”ï¼Œä»è€Œå­¦ä¹ åˆ°è·¨çª—ä½çš„é€šç”¨ç‰¹å¾ã€‚å®éªŒé€šè¿‡åœ¨ä¸¤ç§ä¸åŒçª—ä½è®¾ç½®çš„èƒ¸éƒ¨ CT å›¾åƒä¸Šè¿›è¡ŒéªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨åº”å¯¹é¢†åŸŸåç§»æ—¶å…·æœ‰ä¼˜äºç°æœ‰æ–¹æ¡ˆçš„æ€§èƒ½ï¼Œä¸ºæ„å»ºç¨³å¥ä¸”é«˜åº¦æ³›åŒ–çš„åŒ»ç–—è¯Šæ–­æ¨¡å‹æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27213v1",
      "published_date": "2025-10-31 06:16:31 UTC",
      "updated_date": "2025-10-31 06:16:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:24:20.542731+00:00"
    },
    {
      "arxiv_id": "2510.27210v1",
      "title": "GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation",
      "title_zh": "GUI-Riseï¼šé¢å‘ GUI å¯¼èˆªçš„ç»“æ„åŒ–æ¨ç†ä¸å†å²æ‘˜è¦",
      "authors": [
        "Tao Liu",
        "Chongyu Wang",
        "Rongjie Li",
        "Yingchen Yu",
        "Xuming He",
        "Bai Song"
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) have advanced GUI navigation agents, current approaches face limitations in cross-domain generalization and effective history utilization. We present a reasoning-enhanced framework that systematically integrates structured reasoning, action prediction, and history summarization. The structured reasoning component generates coherent Chain-of-Thought analyses combining progress estimation and decision reasoning, which inform both immediate action predictions and compact history summaries for future steps. Based on this framework, we train a GUI agent, \\textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled trajectories and reinforcement learning with Group Relative Policy Optimization (GRPO). This framework employs specialized rewards, including a history-aware objective, directly linking summary quality to subsequent action performance. Comprehensive evaluations on standard benchmarks demonstrate state-of-the-art results under identical training data conditions, with particularly strong performance in out-of-domain scenarios. These findings validate our framework's ability to maintain robust reasoning and generalization across diverse GUI navigation tasks. Code is available at https://leon022.github.io/GUI-Rise.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å¯¼èˆªä¸­é¢ä¸´çš„è·¨é¢†åŸŸæ³›åŒ–å’Œå†å²ä¿¡æ¯åˆ©ç”¨å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¢å¼ºæ¨ç†èƒ½åŠ›çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç³»ç»Ÿåœ°æ•´åˆäº†ç»“æ„åŒ–æ¨ç†ï¼ˆStructured Reasoningï¼‰ã€åŠ¨ä½œé¢„æµ‹ï¼ˆAction Predictionï¼‰å’Œå†å²æ‘˜è¦ï¼ˆHistory Summarizationï¼‰ï¼Œé€šè¿‡é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰å°†è¿›åº¦è¯„ä¼°ä¸å†³ç­–æ¨ç†ç›¸ç»“åˆï¼Œä¸ºåç»­åŠ¨ä½œå’Œå†å²ä¿¡æ¯çš„å‹ç¼©æä¾›æŒ‡å¯¼ã€‚ç ”ç©¶è€…é€šè¿‡åœ¨ä¼ªæ ‡ç­¾è½¨è¿¹ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä»¥åŠåˆ©ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œè®­ç»ƒå‡ºäº† GUI-Rise æ™ºèƒ½ä½“ã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†åŒ…æ‹¬å†å²æ„ŸçŸ¥ç›®æ ‡ï¼ˆHistory-aware Objectiveï¼‰åœ¨å†…çš„ä¸“é—¨å¥–åŠ±æœºåˆ¶ï¼Œå°†æ‘˜è¦è´¨é‡ç›´æ¥ä¸å¯¼èˆªæ€§èƒ½æŒ‚é’©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGUI-Rise åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›ï¼ˆState-of-the-artï¼‰çš„æˆæœï¼Œå¹¶åœ¨è·¨é¢†åŸŸï¼ˆOut-of-domainï¼‰åœºæ™¯ä¸‹è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚è¿™ä¸€ç ”ç©¶è¯å®äº†ç»“æ„åŒ–æ¨ç†ä¸å†å²æ‘˜è¦æœºåˆ¶åœ¨æå‡ GUI å¯¼èˆªä»»åŠ¡æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.27210v1",
      "published_date": "2025-10-31 06:10:57 UTC",
      "updated_date": "2025-10-31 06:10:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:25:23.772714+00:00"
    },
    {
      "arxiv_id": "2510.27208v1",
      "title": "Multi-Modal Feature Fusion for Spatial Morphology Analysis of Traditional Villages via Hierarchical Graph Neural Networks",
      "title_zh": "åŸºäºå±‚çº§å›¾ç¥ç»ç½‘ç»œçš„ä¼ ç»Ÿæ‘è½ç©ºé—´å½¢æ€åˆ†æå¤šæ¨¡æ€ç‰¹å¾èåˆ",
      "authors": [
        "Jiaxin Zhang",
        "Zehong Zhu",
        "Junye Deng",
        "Yunqin Li",
        "and Bowen Wang"
      ],
      "abstract": "Villages areas hold significant importance in the study of human-land relationships. However, with the advancement of urbanization, the gradual disappearance of spatial characteristics and the homogenization of landscapes have emerged as prominent issues. Existing studies primarily adopt a single-disciplinary perspective to analyze villages spatial morphology and its influencing factors, relying heavily on qualitative analysis methods. These efforts are often constrained by the lack of digital infrastructure and insufficient data. To address the current research limitations, this paper proposes a Hierarchical Graph Neural Network (HGNN) model that integrates multi-source data to conduct an in-depth analysis of villages spatial morphology. The framework includes two types of nodes-input nodes and communication nodes-and two types of edges-static input edges and dynamic communication edges. By combining Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT), the proposed model efficiently integrates multimodal features under a two-stage feature update mechanism. Additionally, based on existing principles for classifying villages spatial morphology, the paper introduces a relational pooling mechanism and implements a joint training strategy across 17 subtypes. Experimental results demonstrate that this method achieves significant performance improvements over existing approaches in multimodal fusion and classification tasks. Additionally, the proposed joint optimization of all sub-types lifts mean accuracy/F1 from 0.71/0.83 (independent models) to 0.82/0.90, driven by a 6% gain for parcel tasks. Our method provides scientific evidence for exploring villages spatial patterns and generative logic.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸå¸‚åŒ–è¿›ç¨‹ä¸­ä¼ ç»Ÿæ‘è½ç©ºé—´ç‰¹å¾æ¶ˆå¤±åŠæ™¯è§‚åŒè´¨åŒ–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§èåˆå¤šæºæ•°æ®è¿›è¡Œç©ºé—´å½¢æ€åˆ†æçš„åˆ†å±‚å›¾ç¥ç»ç½‘ç»œ(Hierarchical Graph Neural Network, HGNN)æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ„å»ºäº†åŒ…å«è¾“å…¥ä¸é€šä¿¡èŠ‚ç‚¹åŠé™æ€ä¸åŠ¨æ€è¾¹çš„å±‚æ¬¡åŒ–æ¶æ„ï¼Œå¹¶ç»“åˆå›¾å·ç§¯ç½‘ç»œ(GCN)ä¸å›¾æ³¨æ„åŠ›ç½‘ç»œ(GAT)ï¼Œé€šè¿‡ä¸¤é˜¶æ®µç‰¹å¾æ›´æ–°æœºåˆ¶å®ç°äº†é«˜æ•ˆçš„å¤šæ¨¡æ€ç‰¹å¾èåˆã€‚ç ”ç©¶è¿˜å¼•å…¥äº†å…³ç³»æ± åŒ–(Relational Pooling)æœºåˆ¶ï¼Œå¹¶é’ˆå¯¹17ä¸ªå­ç±»å‹é‡‡ç”¨äº†è”åˆè®­ç»ƒç­–ç•¥ä»¥ä¼˜åŒ–åˆ†ç±»æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å°†å¹³å‡å‡†ç¡®ç‡(Accuracy)å’ŒF1å€¼åˆ†åˆ«ä»0.71å’Œ0.83æå‡è‡³0.82å’Œ0.90ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚è¯¥æˆæœä¸ºç†è§£ä¼ ç»Ÿæ‘è½çš„ç©ºé—´æ ¼å±€åŠç”Ÿæˆé€»è¾‘æä¾›äº†ç§‘å­¦çš„é‡åŒ–åˆ†ææ‰‹æ®µã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27208v1",
      "published_date": "2025-10-31 06:09:29 UTC",
      "updated_date": "2025-10-31 06:09:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:24:47.925867+00:00"
    },
    {
      "arxiv_id": "2510.27207v1",
      "title": "Feature-Function Curvature Analysis: A Geometric Framework for Explaining Differentiable Models",
      "title_zh": "ç‰¹å¾-å‡½æ•°æ›²ç‡åˆ†æï¼šè§£é‡Šå¯å¾®æ¨¡å‹çš„å‡ ä½•æ¡†æ¶",
      "authors": [
        "Hamed Najafi",
        "Dongsheng Luo",
        "Jason Liu"
      ],
      "abstract": "Explainable AI (XAI) is critical for building trust in complex machine learning models, yet mainstream attribution methods often provide an incomplete, static picture of a model's final state. By collapsing a feature's role into a single score, they are confounded by non-linearity and interactions. To address this, we introduce Feature-Function Curvature Analysis (FFCA), a novel framework that analyzes the geometry of a model's learned function. FFCA produces a 4-dimensional signature for each feature, quantifying its: (1) Impact, (2) Volatility, (3) Non-linearity, and (4) Interaction. Crucially, we extend this framework into Dynamic Archetype Analysis, which tracks the evolution of these signatures throughout the training process. This temporal view moves beyond explaining what a model learned to revealing how it learns. We provide the first direct, empirical evidence of hierarchical learning, showing that models consistently learn simple linear effects before complex interactions. Furthermore, this dynamic analysis provides novel, practical diagnostics for identifying insufficient model capacity and predicting the onset of overfitting. Our comprehensive experiments demonstrate that FFCA, through its static and dynamic components, provides the essential geometric context that transforms model explanation from simple quantification to a nuanced, trustworthy analysis of the entire learning process.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Feature-Function Curvature Analysis (FFCA)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£é‡Šå¯å¾®åˆ†æ¨¡å‹çš„å‡ ä½•æ¡†æ¶ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¸»æµå½’å› æ–¹æ³•(attribution methods)åœ¨å¤„ç†éçº¿æ€§å’Œç‰¹å¾äº¤äº’æ—¶çš„å±€é™æ€§ã€‚FFCAé€šè¿‡é‡åŒ–ç‰¹å¾çš„Impactã€Volatilityã€Non-linearityå’ŒInteractionï¼Œä¸ºæ¯ä¸ªç‰¹å¾ç”Ÿæˆå››ç»´ç­¾åï¼Œä»è€Œæ·±å…¥åˆ†ææ¨¡å‹å‡½æ•°çš„å‡ ä½•ç»“æ„ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†Dynamic Archetype Analysisï¼Œé€šè¿‡è¿½è¸ªè¿™äº›ç­¾ååœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¼”å˜ï¼Œå°†æ¨¡å‹è§£é‡Šä»é™æ€ç»“æœè½¬å‘åŠ¨æ€å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒé¦–æ¬¡æä¾›äº†å±‚çº§åŒ–å­¦ä¹ (hierarchical learning)çš„ç›´æ¥å®è¯è¯æ®ï¼Œæ­ç¤ºäº†æ¨¡å‹éµå¾ªå…ˆçº¿æ€§åäº¤äº’çš„å­¦ä¹ é¡ºåºã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜ä¸ºè¯†åˆ«æ¨¡å‹å®¹é‡ä¸è¶³å’Œé¢„æµ‹è¿‡æ‹Ÿåˆæä¾›äº†å®ç”¨çš„è¯Šæ–­æ‰‹æ®µã€‚é€šè¿‡ç»“åˆé™æ€ä¸åŠ¨æ€åˆ†æï¼ŒFFCAä¸ºæ„å»ºæ›´å…·é²æ£’æ€§å’Œå¯ä¿¡åº¦çš„æ¨¡å‹è§£é‡Šæä¾›äº†å¿…è¦çš„å‡ ä½•èƒŒæ™¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27207v1",
      "published_date": "2025-10-31 06:07:55 UTC",
      "updated_date": "2025-10-31 06:07:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:24:43.600866+00:00"
    },
    {
      "arxiv_id": "2510.27206v1",
      "title": "Fints: Efficient Inference-Time Personalization for LLMs with Fine-Grained Instance-Tailored Steering",
      "title_zh": "Fintsï¼šåŸºäºç»†ç²’åº¦å®ä¾‹å®šåˆ¶åŒ–å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆæ¨ç†æ—¶ä¸ªæ€§åŒ–",
      "authors": [
        "Kounianhua Du",
        "Jianxing Liu",
        "Kangning Zhang",
        "Wenxiang Jiao",
        "Yuan Lu",
        "Jiarui Jin",
        "Weiwen Liu",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "abstract": "The rapid evolution of large language models (LLMs) has intensified the demand for effective personalization techniques that can adapt model behavior to individual user preferences. Despite the non-parametric methods utilizing the in-context learning ability of LLMs, recent parametric adaptation methods, including personalized parameter-efficient fine-tuning and reward modeling emerge. However, these methods face limitations in handling dynamic user patterns and high data sparsity scenarios, due to low adaptability and data efficiency. To address these challenges, we propose a fine-grained and instance-tailored steering framework that dynamically generates sample-level interference vectors from user data and injects them into the model's forward pass for personalized adaptation. Our approach introduces two key technical innovations: a fine-grained steering component that captures nuanced signals by hooking activations from attention and MLP layers, and an input-aware aggregation module that synthesizes these signals into contextually relevant enhancements. The method demonstrates high flexibility and data efficiency, excelling in fast-changing distribution and high data sparsity scenarios. In addition, the proposed method is orthogonal to existing methods and operates as a plug-in component compatible with different personalization techniques. Extensive experiments across diverse scenarios--including short-to-long text generation, and web function calling--validate the effectiveness and compatibility of our approach. Results show that our method significantly enhances personalization performance in fast-shifting environments while maintaining robustness across varying interaction modes and context lengths. Implementation is available at https://github.com/KounianhuaDu/Fints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Fintsï¼Œä¸€ä¸ªç»†ç²’åº¦ä¸”å®ä¾‹å®šåˆ¶çš„å¼•å¯¼æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Large Language Models (LLMs) åœ¨ä¸ªæ€§åŒ–è¿‡ç¨‹ä¸­é¢ä¸´çš„åŠ¨æ€ç”¨æˆ·æ¨¡å¼å’Œæ•°æ®ç¨€ç–æ€§éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»ç”¨æˆ·æ•°æ®ä¸­åŠ¨æ€ç”Ÿæˆæ ·æœ¬çº§å¹²é¢„å‘é‡ï¼Œå¹¶å°†å…¶æ³¨å…¥æ¨¡å‹çš„æ­£å‘ä¼ æ’­ (Forward Pass) ä¸­ï¼Œå®ç°äº†é«˜æ•ˆçš„æ¨ç†æ—¶ä¸ªæ€§åŒ–ã€‚Fints åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼šä¸€æ˜¯é€šè¿‡é’©å– Attention å’Œ MLP å±‚æ¿€æ´»æ¥æ•è·ç»†å¾®ä¿¡å·çš„ Fine-grained Steering ç»„ä»¶ï¼ŒäºŒæ˜¯èƒ½å°†ä¿¡å·åˆæˆä¸ºä¸Šä¸‹æ–‡ç›¸å…³å¢å¼ºçš„ Input-aware Aggregation æ¨¡å—ã€‚è¯¥æ–¹æ³•è¡¨ç°å‡ºæé«˜çš„çµæ´»æ€§å’Œæ•°æ®æ•ˆç‡ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†å¿«é€Ÿå˜åŒ–çš„åˆ†å¸ƒåœºæ™¯ã€‚å®éªŒè¯æ˜ï¼ŒFints ä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„ç»„ä»¶ï¼Œä¸ç°æœ‰æŠ€æœ¯æ­£äº¤å…¼å®¹ï¼Œåœ¨æ–‡æœ¬ç”Ÿæˆå’Œ Web Function Calling ç­‰å¤šæ ·åŒ–åœºæ™¯ä¸­æ˜¾è‘—æå‡äº†ä¸ªæ€§åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27206v1",
      "published_date": "2025-10-31 06:01:04 UTC",
      "updated_date": "2025-10-31 06:01:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:24:33.326706+00:00"
    },
    {
      "arxiv_id": "2511.02853v1",
      "title": "Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring",
      "title_zh": "Consciousness-ECG Transformerï¼šæ”¯æŒå®æ—¶ç›‘æµ‹çš„æ„è¯†çŠ¶æ€è¯„ä¼°ç³»ç»Ÿ",
      "authors": [
        "Young-Seok Kweon",
        "Gi-Hwan Shin",
        "Ji-Yong Kim",
        "Bokyeong Ryu",
        "Seong-Whan Lee"
      ],
      "abstract": "Conscious state estimation is important in various medical settings, including sleep staging and anesthesia management, to ensure patient safety and optimize health outcomes. Traditional methods predominantly utilize electroencephalography (EEG), which faces challenges such as high sensitivity to noise and the requirement for controlled environments. In this study, we propose the consciousness-ECG transformer that leverages electrocardiography (ECG) signals for non-invasive and reliable conscious state estimation. Our approach employs a transformer with decoupled query attention to effectively capture heart rate variability features that distinguish between conscious and unconscious states. We implemented the conscious state estimation system with real-time monitoring and validated our system on datasets involving sleep staging and anesthesia level monitoring during surgeries. Experimental results demonstrate that our model outperforms baseline models, achieving accuracies of 0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our model achieves the highest area under curve values of 0.786 and 0.895 on sleep staging and anesthesia level monitoring, respectively. The proposed system offers a practical and robust alternative to EEG-based methods, particularly suited for dynamic clinical environments. Our results highlight the potential of ECG-based consciousness monitoring to enhance patient safety and advance our understanding of conscious states.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Consciousness-ECG Transformer çš„æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ Electroencephalography (EEG) æ–¹æ³•åœ¨æ„è¯†çŠ¶æ€è¯„ä¼°ä¸­å¯¹å™ªå£°æ•æ„ŸåŠç¯å¢ƒå—é™çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨éä¾µå…¥æ€§çš„ Electrocardiography (ECG) ä¿¡å·ï¼Œé€šè¿‡ç»“åˆ Decoupled Query Attention çš„ Transformer æ¶æ„ï¼Œæœ‰æ•ˆæ•æ‰åŒºåˆ†æ„è¯†ä¸æ— æ„è¯†çŠ¶æ€çš„ Heart Rate Variability (HRV) ç‰¹å¾ã€‚ç ”ç©¶å›¢é˜Ÿå®ç°äº†ä¸€ä¸ªå…·å¤‡å®æ—¶ç›‘æµ‹åŠŸèƒ½çš„ç³»ç»Ÿï¼Œå¹¶åœ¨ç¡çœ åˆ†æœŸå’Œæ‰‹æœ¯éº»é†‰æ°´å¹³ç›‘æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç¡çœ åˆ†æœŸå’Œéº»é†‰ç›‘æµ‹ä¸­çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ° 0.877 å’Œ 0.880ï¼Œä¸” Area Under Curve (AUC) è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¥æ–¹æ¡ˆä¸ºåŠ¨æ€ä¸´åºŠç¯å¢ƒæä¾›äº†ä¸€ç§å®ç”¨ä¸”é²æ£’çš„æ›¿ä»£æ‰‹æ®µï¼Œåœ¨å¢å¼ºæ‚£è€…å®‰å…¨å’Œæ¨åŠ¨æ„è¯†çŠ¶æ€ç ”ç©¶æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "30 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.02853v1",
      "published_date": "2025-10-31 05:53:41 UTC",
      "updated_date": "2025-10-31 05:53:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:25:48.671913+00:00"
    },
    {
      "arxiv_id": "2510.27196v1",
      "title": "MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models",
      "title_zh": "MemeArenaï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æœ‰å®³æ€§ç†è§£çš„è‡ªåŠ¨åŒ–è¯­å¢ƒæ„ŸçŸ¥æ— åè¯„ä¼°",
      "authors": [
        "Zixin Chen",
        "Hongzhan Lin",
        "Kaixin Li",
        "Ziyang Luo",
        "Yayue Deng",
        "Jing Ma"
      ],
      "abstract": "The proliferation of memes on social media necessitates the capabilities of multimodal Large Language Models (mLLMs) to effectively understand multimodal harmfulness. Existing evaluation approaches predominantly focus on mLLMs' detection accuracy for binary classification tasks, which often fail to reflect the in-depth interpretive nuance of harmfulness across diverse contexts. In this paper, we propose MemeArena, an agent-based arena-style evaluation framework that provides a context-aware and unbiased assessment for mLLMs' understanding of multimodal harmfulness. Specifically, MemeArena simulates diverse interpretive contexts to formulate evaluation tasks that elicit perspective-specific analyses from mLLMs. By integrating varied viewpoints and reaching consensus among evaluators, it enables fair and unbiased comparisons of mLLMs' abilities to interpret multimodal harmfulness. Extensive experiments demonstrate that our framework effectively reduces the evaluation biases of judge agents, with judgment results closely aligning with human preferences, offering valuable insights into reliable and comprehensive mLLM evaluations in multimodal harmfulness understanding. Our code and data are publicly available at https://github.com/Lbotirx/MemeArena.",
      "tldr_zh": "éšç€ç¤¾äº¤åª’ä½“ä¸Šè¡¨æƒ…åŒ…(memes)çš„æ¿€å¢ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(mLLMs)å¯¹å¤šæ¨¡æ€æœ‰å®³æ€§(multimodal harmfulness)çš„ç†è§£èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰è¯„ä¼°æ–¹æ³•å¤šä¾§é‡äºäºŒå…ƒåˆ†ç±»å‡†ç¡®ç‡ï¼Œéš¾ä»¥åæ˜ ä¸åŒèƒŒæ™¯ä¸‹çš„æ·±åº¦è§£è¯»ç»†å¾®å·®åˆ«ã€‚è¯¥ç ”ç©¶æå‡ºäº†MemeArenaï¼Œä¸€ç§åŸºäºæ™ºèƒ½ä½“(agent-based)çš„ç«æŠ€åœºå¼è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºmLLMså¯¹å¤šæ¨¡æ€æœ‰å®³æ€§çš„ç†è§£æä¾›æ„ŸçŸ¥ä¸Šä¸‹æ–‡ä¸”æ— åè§çš„è¯„ä¼°ã€‚MemeArenaé€šè¿‡æ¨¡æ‹Ÿå¤šæ ·åŒ–çš„è§£è¯»èƒŒæ™¯æ¥åˆ¶å®šè¯„ä¼°ä»»åŠ¡ï¼Œä»è€Œè¯±å¯¼mLLMsè¿›è¡Œç‰¹å®šè§†è§’çš„åˆ†æï¼Œå¹¶é€šè¿‡ç»“åˆå¤šç§è§‚ç‚¹åŠåœ¨è¯„ä¼°è€…ä¹‹é—´è¾¾æˆå…±è¯†ï¼Œç¡®ä¿äº†è¯„ä¼°è¿‡ç¨‹çš„å…¬å¹³æ€§ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆé™ä½äº†è£åˆ¤æ™ºèƒ½ä½“(judge agents)çš„è¯„ä¼°åå·®ï¼Œå…¶åˆ¤æ–­ç»“æœä¸äººç±»åå¥½é«˜åº¦ä¸€è‡´ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºå¯é ä¸”å…¨é¢çš„å¤šæ¨¡æ€æœ‰å®³æ€§ç†è§£è¯„ä¼°ä½“ç³»æä¾›äº†é‡è¦è§è§£ï¼Œç›¸å…³ä»£ç å’Œæ•°æ®å·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.27196v1",
      "published_date": "2025-10-31 05:39:03 UTC",
      "updated_date": "2025-10-31 05:39:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:25:39.668765+00:00"
    },
    {
      "arxiv_id": "2510.27194v1",
      "title": "From product to system network challenges in system of systems lifecycle management",
      "title_zh": "ä½“ç³»ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šä»äº§å“åˆ°ç³»ç»Ÿç½‘ç»œçš„è½¬å‹æŒ‘æˆ˜",
      "authors": [
        "Vahid Salehi",
        "Josef Vilsmeier",
        "Shirui Wang"
      ],
      "abstract": "Today, products are no longer isolated artifacts, but nodes in networked systems. This means that traditional, linearly conceived life cycle models are reaching their limits: Interoperability across disciplines, variant and configuration management, traceability, and governance across organizational boundaries are becoming key factors. This collective contribution classifies the state of the art and proposes a practical frame of reference for SoS lifecycle management, model-based systems engineering (MBSE) as the semantic backbone, product lifecycle management (PLM) as the governance and configuration level, CAD-CAE as model-derived domains, and digital thread and digital twin as continuous feedback. Based on current literature and industry experience, mobility, healthcare, and the public sector, we identify four principles: (1) referenced architecture and data models, (2) end-to-end configuration sovereignty instead of tool silos, (3) curated models with clear review gates, and (4) measurable value contributions along time, quality, cost, and sustainability. A three-step roadmap shows the transition from product- to network- centric development: piloting with reference architecture, scaling across variant and supply chain spaces, organizational anchoring (roles, training, compliance). The results are increased change robustness, shorter throughput times, improved reuse, and informed sustainability decisions. This article is aimed at decision-makers and practitioners who want to make complexity manageable and design SoS value streams to be scalable.",
      "tldr_zh": "éšç€äº§å“ä»å­¤ç«‹ä¸ªä½“è½¬å˜ä¸ºç³»ç»Ÿä¹‹ç³»ç»Ÿ(System of Systems, SoS)ç½‘ç»œï¼Œä¼ ç»Ÿçš„çº¿æ€§ç”Ÿå‘½å‘¨æœŸæ¨¡å‹å·²éš¾ä»¥åº”å¯¹è·¨å­¦ç§‘äº’æ“ä½œæ€§ã€å˜ä½“ç®¡ç†åŠè·¨ç»„ç»‡æ²»ç†ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªSoSç”Ÿå‘½å‘¨æœŸç®¡ç†å®è·µæ¡†æ¶ï¼Œå°†æ¨¡å‹é©±åŠ¨ç³»ç»Ÿå·¥ç¨‹(MBSE)ä½œä¸ºè¯­ä¹‰éª¨å¹²ï¼Œäº§å“ç”Ÿå‘½å‘¨æœŸç®¡ç†(PLM)ä½œä¸ºæ²»ç†ä¸é…ç½®å±‚ï¼Œå¹¶ç»“åˆCAD-CAEã€æ•°å­—çº¿ç¨‹(Digital Thread)å’Œæ•°å­—å­ªç”Ÿ(Digital Twin)å®ç°æŒç»­åé¦ˆã€‚ç»“åˆç§»åŠ¨å‡ºè¡Œã€åŒ»ç–—ä¿å¥å’Œå…¬å…±éƒ¨é—¨çš„è¡Œä¸šç»éªŒï¼Œè¯¥ç ”ç©¶ç¡®ç«‹äº†åŒ…æ‹¬å‚è€ƒæ¶æ„ä¸æ•°æ®æ¨¡å‹ã€ç«¯åˆ°ç«¯é…ç½®ä¸»æƒã€å…·å¤‡è¯„å®¡å…³å£çš„ç²¾é€‰æ¨¡å‹ä»¥åŠå¤šç»´åº¦ä»·å€¼è¯„ä¼°åœ¨å†…çš„å››å¤§æ ¸å¿ƒåŸåˆ™ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€å¥—ä»äº§å“ä¸­å¿ƒå‘ç½‘ç»œä¸­å¿ƒå¼€å‘è½¬å‹çš„ä¸‰æ­¥èµ°è·¯çº¿å›¾ï¼Œæ¶µç›–äº†æ¶æ„è¯•ç‚¹ã€è§„æ¨¡åŒ–æ¨å¹¿åŠç»„ç»‡åŒ–é”šå®šä¸‰ä¸ªé˜¶æ®µã€‚åº”ç”¨ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æ˜¾è‘—å¢å¼ºç³»ç»Ÿçš„å˜æ›´é²æ£’æ€§ï¼Œç¼©çŸ­ååæ—¶é—´ï¼Œæé«˜èµ„æºé‡ç”¨ç‡ï¼Œå¹¶æ”¯æŒæ›´æ˜æ™ºçš„å¯æŒç»­å‘å±•å†³ç­–ã€‚è¯¥ç ”ç©¶ä¸ºæ—¨åœ¨ç®¡ç†å¤æ‚æ€§å¹¶æ„å»ºå¯æ‰©å±•SoSä»·å€¼æµçš„å†³ç­–è€…å’Œä»ä¸šäººå‘˜æä¾›äº†é‡è¦çš„ç†è®ºæ¡†æ¶ä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27194v1",
      "published_date": "2025-10-31 05:36:35 UTC",
      "updated_date": "2025-10-31 05:36:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:25:43.685225+00:00"
    },
    {
      "arxiv_id": "2511.00120v1",
      "title": "VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images",
      "title_zh": "VLM6Dï¼šåŸºäº RGB-D å›¾åƒçš„ VLM 6 è‡ªç”±åº¦ä½å§¿ä¼°è®¡",
      "authors": [
        "Md Selim Sarowar",
        "Sungho Kim"
      ],
      "abstract": "The primary challenge in computer vision is precisely calculating the pose of 6D objects, however many current approaches are still fragile and have trouble generalizing from synthetic data to real-world situations with fluctuating lighting, textureless objects, and significant occlusions. To address these limitations, VLM6D, a novel dual-stream architecture that leverages the distinct strengths of visual and geometric data from RGB-D input for robust and precise pose estimation. Our framework uniquely integrates two specialized encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the RGB modality, harnessing its rich, pre-trained understanding of visual grammar to achieve remarkable resilience against texture and lighting variations. Concurrently, a PointNet++ encoder processes the 3D point cloud derived from depth data, enabling robust geometric reasoning that excels even with the sparse, fragmented data typical of severe occlusion. These complementary feature streams are effectively fused to inform a multi task prediction head. We demonstrate through comprehensive experiments that VLM6D obtained new SOTA performance on the challenging Occluded-LineMOD, validating its superior robustness and accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VLM6Dï¼Œè¿™æ˜¯ä¸€ç§åŸºäº RGB-D å›¾åƒçš„æ–°å‹åŒæµæ¶æ„ï¼Œæ—¨åœ¨è§£å†³ 6D ç‰©ä½“ä½å§¿ä¼°è®¡åœ¨å¤æ‚å…‰ç…§ã€æ— çº¹ç†ç‰©ä½“å’Œä¸¥é‡é®æŒ¡ç¯å¢ƒä¸‹çš„é²æ£’æ€§éš¾é¢˜ã€‚è¯¥æ¡†æ¶å·§å¦™åœ°é›†æˆäº†ä¸¤ä¸ªä¸“é—¨çš„ç¼–ç å™¨ï¼šä½¿ç”¨è‡ªç›‘ç£ Vision Transformer (DINOv2) å¤„ç† RGB æ¨¡æ€ï¼Œåˆ©ç”¨å…¶ä¸°å¯Œçš„è§†è§‰ç†è§£èƒ½åŠ›åº”å¯¹çº¹ç†å’Œå…‰ç…§å˜åŒ–ï¼›åŒæ—¶é‡‡ç”¨ PointNet++ ç¼–ç å™¨å¤„ç† 3D ç‚¹äº‘æ•°æ®ï¼Œå¢å¼ºåœ¨æç«¯é®æŒ¡å¯¼è‡´çš„æ•°æ®ç¢ç‰‡åŒ–æƒ…å†µä¸‹çš„å‡ ä½•æ¨ç†èƒ½åŠ›ã€‚è¿™äº›äº’è¡¥çš„ç‰¹å¾æµé€šè¿‡èåˆåä¼ é€’ç»™å¤šä»»åŠ¡é¢„æµ‹å¤´ï¼Œå®ç°äº†ç²¾ç¡®çš„ä½å§¿ä¼°è®¡ã€‚å®éªŒè¯æ˜ï¼ŒVLM6D åœ¨æŒ‘æˆ˜æ€§çš„ Occluded-LineMOD æ•°æ®é›†ä¸Šå–å¾—äº†æ–°çš„ SOTA æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†ä»åˆæˆæ•°æ®åˆ°çœŸå®åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted to IEIE( The Institute Of Electronics and Information Engineering, South Korea) Fall,2025 Conference",
      "pdf_url": "https://arxiv.org/pdf/2511.00120v1",
      "published_date": "2025-10-31 05:26:41 UTC",
      "updated_date": "2025-10-31 05:26:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:25:46.373404+00:00"
    },
    {
      "arxiv_id": "2511.15669v1",
      "title": "DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models",
      "title_zh": "DeepThinkVLAï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Cheng Yin",
        "Yankai Lin",
        "Wang Xu",
        "Sikyuen Tam",
        "Xiangrui Zeng",
        "Zhiyuan Liu",
        "Zhouping Yin"
      ],
      "abstract": "Enabling Vision-Language-Action (VLA) models to \"think before acting\" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DeepThinkVLAï¼Œæ—¨åœ¨é€šè¿‡è®©è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹(Vision-Language-Action Models, VLA)åœ¨æ‰§è¡ŒåŠ¨ä½œå‰è¿›è¡Œé“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†ï¼Œä»¥è§£å†³ç«¯åˆ°ç«¯æœºå™¨äººç­–ç•¥å¯¹æ•°æ®è¿‡åº¦ä¾èµ–çš„é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨é¡ºåºæ¨ç†ä¸é«˜ç»´å¹¶è¡Œæœºå™¨äººåŠ¨ä½œä¹‹é—´å­˜åœ¨çš„æ¶æ„ä¸åŒ¹é…ï¼ŒDeepThinkVLA å¼•å…¥äº†ä¸€ç§æ··åˆæ³¨æ„åŠ›(hybrid-attention)è§£ç å™¨ï¼Œåœ¨ç”Ÿæˆ CoT æ—¶é‡‡ç”¨å› æœæ³¨æ„åŠ›(causal attention)ï¼Œè€Œåœ¨è§£ç åŠ¨ä½œå‘é‡æ—¶åˆ‡æ¢ä¸ºåŒå‘æ³¨æ„åŠ›(bidirectional attention)ã€‚ç ”ç©¶å›¢é˜Ÿé…å¥—è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæµæ°´çº¿ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning, SFT)å’Œå¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)å®ç°æ¨ç†åŠ¨ä½œåºåˆ—ä¸é¢„æœŸç»“æœçš„å› æœå¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ LIBERO åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 97.0% çš„æˆåŠŸç‡ï¼Œåˆ·æ–°äº†é¢†åŸŸå†…çš„æœ€å…ˆè¿›æ°´å¹³(State-of-the-art)ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œæ··åˆæ¶æ„è®¾è®¡ä½¿æ€§èƒ½æå‡äº† 15.5%ï¼Œè€Œ RL é˜¶æ®µçš„è®­ç»ƒåˆ™ä¸ºæœ€ç»ˆæ€§èƒ½çš„çªç ´æä¾›äº†å…³é”®æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 6 figures, conference",
      "pdf_url": "https://arxiv.org/pdf/2511.15669v1",
      "published_date": "2025-10-31 05:26:16 UTC",
      "updated_date": "2025-10-31 05:26:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:25:48.259733+00:00"
    },
    {
      "arxiv_id": "2510.27191v2",
      "title": "Vectorized Online POMDP Planning",
      "title_zh": "å‘é‡åŒ–åœ¨çº¿ POMDP è§„åˆ’",
      "authors": [
        "Marcus Hoerger",
        "Muhammad Sudrajat",
        "Hanna Kurniawati"
      ],
      "abstract": "Planning under partial observability is an essential capability of autonomous robots. The Partially Observable Markov Decision Process (POMDP) provides a powerful framework for planning under partial observability problems, capturing the stochastic effects of actions and the limited information available through noisy observations. POMDP solving could benefit tremendously from massive parallelization of today's hardware, but parallelizing POMDP solvers has been challenging. They rely on interleaving numerical optimization over actions with the estimation of their values, which creates dependencies and synchronization bottlenecks between parallel processes that can quickly offset the benefits of parallelization. In this paper, we propose Vectorized Online POMDP Planner (VOPP), a novel parallel online solver that leverages a recent POMDP formulation that analytically solves part of the optimization component, leaving only the estimation of expectations for numerical computation. VOPP represents all data structures related to planning as a collection of tensors and implements all planning steps as fully vectorized computations over this representation. The result is a massively parallel solver with no dependencies and synchronization bottlenecks between parallel computations. Experimental results indicate that VOPP is at least 20X more efficient in computing near-optimal solutions compared to an existing state-of-the-art parallel online solver.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Vectorized Online POMDP Planner (VOPP)ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¹¶è¡Œåœ¨çº¿æ±‚è§£å™¨ï¼Œæ—¨åœ¨è§£å†³éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(POMDP)åœ¨å¹¶è¡ŒåŒ–è¿‡ç¨‹ä¸­é¢ä¸´çš„æ•°å€¼ä¼˜åŒ–ä¸ä»·å€¼ä¼°è®¡ä¹‹é—´çš„åŒæ­¥ç“¶é¢ˆé—®é¢˜ã€‚VOPPåˆ©ç”¨äº†ä¸€ç§æœ€æ–°çš„POMDPå…¬å¼ï¼Œèƒ½å¤Ÿè§£æåœ°è§£å†³éƒ¨åˆ†ä¼˜åŒ–ç»„ä»¶ï¼Œä»è€Œä»…å°†æœŸæœ›ä¼°è®¡ç•™ç»™æ•°å€¼è®¡ç®—ã€‚è¯¥æ¡†æ¶å°†æ‰€æœ‰ä¸è§„åˆ’ç›¸å…³çš„æ•°æ®ç»“æ„è¡¨ç¤ºä¸ºå¼ é‡(tensor)é›†åˆï¼Œå¹¶å°†æ‰€æœ‰è§„åˆ’æ­¥éª¤å®ç°ä¸ºåŸºäºè¯¥è¡¨ç¤ºçš„å®Œå…¨å‘é‡åŒ–è®¡ç®—ã€‚è¿™ç§è®¾è®¡å®ç°äº†ä¸€ç§å¤§è§„æ¨¡å¹¶è¡Œæ±‚è§£å™¨ï¼Œæ¶ˆé™¤äº†å¹¶è¡Œè®¡ç®—ä¹‹é—´çš„ä¾èµ–å…³ç³»å’ŒåŒæ­¥éšœç¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›å¹¶è¡Œåœ¨çº¿æ±‚è§£å™¨ç›¸æ¯”ï¼ŒVOPPåœ¨è®¡ç®—è¿‘ä¹æœ€ä¼˜è§£æ—¶çš„æ•ˆç‡æé«˜äº†è‡³å°‘20å€ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººåœ¨éšæœºæ•ˆåº”å’Œå™ªå£°è§‚æµ‹ç¯å¢ƒä¸‹çš„è‡ªä¸»è§„åˆ’èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 3 figures. Submitted to ICRA 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.27191v2",
      "published_date": "2025-10-31 05:21:39 UTC",
      "updated_date": "2025-11-28 05:51:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:25:51.266736+00:00"
    },
    {
      "arxiv_id": "2510.27186v1",
      "title": "Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications",
      "title_zh": "ç¨€ç–æ¨¡å‹åæ¼”ï¼šé¢å‘æ— æ•°æ®åº”ç”¨çš„é«˜æ•ˆè§†è§‰ Transformer åæ¼”",
      "authors": [
        "Zixuan Hu",
        "Yongxian Wei",
        "Li Shen",
        "Zhenyi Wang",
        "Lei Li",
        "Chun Yuan",
        "Dacheng Tao"
      ],
      "abstract": "Model inversion, which aims to reconstruct the original training data from pre-trained discriminative models, is especially useful when the original training data is unavailable due to privacy, usage rights, or size constraints. However, existing dense inversion methods attempt to reconstruct the entire image area, making them extremely inefficient when inverting high-resolution images from large-scale Vision Transformers (ViTs). We further identify two underlying causes of this inefficiency: the redundant inversion of noisy backgrounds and the unintended inversion of spurious correlations--a phenomenon we term \"hallucination\" in model inversion. To address these limitations, we propose a novel sparse model inversion strategy, as a plug-and-play extension to speed up existing dense inversion methods with no need for modifying their original loss functions. Specifically, we selectively invert semantic foregrounds while stopping the inversion of noisy backgrounds and potential spurious correlations. Through both theoretical and empirical studies, we validate the efficacy of our approach in achieving significant inversion acceleration (up to 3.79 faster) while maintaining comparable or even enhanced downstream performance in data-free model quantization and data-free knowledge transfer. Code is available at https://github.com/Egg-Hu/SMI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰dense inversionæ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡Vision Transformers (ViTs)çš„é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†Sparse Model Inversion (SMI)ç­–ç•¥ã€‚ä½œè€…è¯†åˆ«å‡ºæ•ˆç‡ç“¶é¢ˆæºäºå¯¹å™ªå£°èƒŒæ™¯çš„å†—ä½™åæ¼”ä»¥åŠå¯¹è™šå‡ç›¸å…³æ€§çš„æ„å¤–åæ¼”ï¼Œå³æ¨¡å‹åæ¼”ä¸­çš„hallucinationç°è±¡ã€‚SMIä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„æ‰©å±•ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°åæ¼”è¯­ä¹‰å‰æ™¯å¹¶åœæ­¢èƒŒæ™¯åŠè™šå‡ç›¸å…³æ€§çš„åæ¼”ï¼Œåœ¨æ— éœ€ä¿®æ”¹åŸå§‹æŸå¤±å‡½æ•°çš„å‰æä¸‹åŠ é€Ÿäº†ç°æœ‰æ–¹æ³•ã€‚ç†è®ºä¸å®è¯ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒdata-free model quantizationå’Œdata-free knowledge transferæ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†æœ€é«˜3.79å€çš„åæ¼”åŠ é€Ÿã€‚è¯¥å·¥ä½œä¸ºåœ¨éšç§æˆ–å­˜å‚¨å—é™åœºæ™¯ä¸‹é«˜æ•ˆé‡æ„è®­ç»ƒæ•°æ®æä¾›äº†åˆ‡å®å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27186v1",
      "published_date": "2025-10-31 05:14:36 UTC",
      "updated_date": "2025-10-31 05:14:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:01.771729+00:00"
    },
    {
      "arxiv_id": "2510.27181v3",
      "title": "Dual-level Progressive Hardness-Aware Reweighting for Cross-View Geo-Localization",
      "title_zh": "é¢å‘è·¨è§†è§’åœ°ç†å®šä½çš„åŒå±‚æ¸è¿›å¼éš¾åº¦æ„ŸçŸ¥é‡åŠ æƒ",
      "authors": [
        "Guozheng Zheng",
        "Jian Guan",
        "Mingjie Xie",
        "Xuanjia Zhao",
        "Congyi Fan",
        "Shiheng Zhang",
        "Pengming Feng"
      ],
      "abstract": "Cross-view geo-localization (CVGL) between drone and satellite imagery remains challenging due to severe viewpoint gaps and the presence of hard negatives, which are visually similar but geographically mismatched samples. Existing mining or reweighting strategies often use static weighting, which is sensitive to distribution shifts and prone to overemphasizing difficult samples too early, leading to noisy gradients and unstable convergence. In this paper, we present a Dual-level Progressive Hardness-aware Reweighting (DPHR) strategy. At the sample level, a Ratio-based Difficulty-Aware (RDA) module evaluates relative difficulty and assigns fine-grained weights to negatives. At the batch level, a Progressive Adaptive Loss Weighting (PALW) mechanism exploits a training-progress signal to attenuate noisy gradients during early optimization and progressively enhance hard-negative mining as training matures. Experiments on the University-1652 and SUES-200 benchmarks demonstrate the effectiveness and robustness of the proposed DPHR, achieving consistent improvements over state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœºä¸å«æ˜Ÿå›¾åƒé—´çš„ Cross-view geo-localization (CVGL) ä»»åŠ¡ï¼Œè§£å†³äº†ä¸¥é‡è§†è§’å·®å¼‚åŠè§†è§‰ç›¸ä¼¼ä½†åœ°ç†å¤±é…çš„ hard negatives å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰é™æ€åŠ æƒç­–ç•¥æ˜“å—åˆ†å¸ƒåç§»å½±å“ä¸”æ—©æœŸä¼˜åŒ–ä¸ç¨³å®šçš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº† Dual-level Progressive Hardness-aware Reweighting (DPHR) ç­–ç•¥ã€‚è¯¥ç­–ç•¥åœ¨æ ·æœ¬å±‚é¢åˆ©ç”¨ Ratio-based Difficulty-Aware (RDA) æ¨¡å—è¯„ä¼°ç›¸å¯¹éš¾åº¦å¹¶åˆ†é…ç»†ç²’åº¦æƒé‡ï¼Œåœ¨æ‰¹æ¬¡å±‚é¢é€šè¿‡ Progressive Adaptive Loss Weighting (PALW) æœºåˆ¶ä¾æ®è®­ç»ƒè¿›åº¦ä¿¡å·å‡å¼±æ—©æœŸå™ªå£°æ¢¯åº¦ï¼Œå¹¶éšè®­ç»ƒæˆç†Ÿé€æ­¥å¼ºåŒ–éš¾æ ·æœ¬æŒ–æ˜ã€‚åœ¨ University-1652 å’Œ SUES-200 åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDPHR èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹çš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ï¼Œå¹¶åœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºç°æœ‰çš„ state-of-the-art æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.27181v3",
      "published_date": "2025-10-31 05:08:46 UTC",
      "updated_date": "2025-11-04 04:54:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:25:59.470314+00:00"
    },
    {
      "arxiv_id": "2510.27176v3",
      "title": "Glia: A Human-Inspired AI for Automated Systems Design and Optimization",
      "title_zh": "Gliaï¼šä¸€ç§å—äººç±»å¯å‘ã€ç”¨äºè‡ªåŠ¨åŒ–ç³»ç»Ÿè®¾è®¡ä¸ä¼˜åŒ–çš„ AI ç³»ç»Ÿ",
      "authors": [
        "Pouya Hamadanian",
        "Pantea Karimi",
        "Arash Nasr-Esfahany",
        "Kimia Noorbakhsh",
        "Joseph Chandler",
        "Ali ParandehGheibi",
        "Mohammad Alizadeh",
        "Hari Balakrishnan"
      ],
      "abstract": "Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Gliaï¼Œä¸€ç§ä¸“ä¸ºç½‘ç»œåŒ–ç³»ç»Ÿè®¾è®¡çš„äººç±»å¯å‘å¼AIæ¶æ„ï¼Œæ—¨åœ¨æ¢ç´¢AIæ˜¯å¦èƒ½åƒäººç±»ä¸“å®¶ä¸€æ ·å…·å¤‡ç³»ç»Ÿè®¾è®¡çš„åˆ›é€ åŠ›ä¸æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¶æ„é‡‡ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¤šæ™ºèƒ½ä½“å·¥ä½œæµï¼Œé€šè¿‡ä¸“é—¨è´Ÿè´£æ¨ç†ã€å®éªŒå’Œåˆ†æçš„æ™ºèƒ½ä½“åä½œï¼Œå°†æŠ½è±¡æ¨ç†ä¸å®è¯åé¦ˆ(Empirical Feedback)ç›¸ç»“åˆã€‚ä¸ä»¥å¾€ä¼˜åŒ–é»‘ç›’ç­–ç•¥çš„æ–¹æ³•ä¸åŒï¼ŒGliaèƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Š(Interpretable)çš„è®¾è®¡å¹¶å…¬å¼€å…¶æ¨ç†è¿‡ç¨‹ã€‚åœ¨åˆ†å¸ƒå¼GPUé›†ç¾¤çš„LLMæ¨ç†åº”ç”¨ä¸­ï¼ŒGliaåœ¨è¯·æ±‚è·¯ç”±(Routing)ã€è°ƒåº¦(Scheduling)å’Œè‡ªåŠ¨ç¼©æ”¾(Auto-scaling)æ–¹é¢ç”Ÿæˆäº†è¾¾åˆ°äººç±»ä¸“å®¶æ°´å¹³çš„æ–°ç®—æ³•ï¼Œæ˜¾è‘—ç¼©çŸ­äº†è®¾è®¡æ—¶é—´å¹¶æä¾›äº†å…³äºå·¥ä½œè´Ÿè½½è¡Œä¸ºçš„æ–°è§è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å°†æ¨ç†ç±»LLMsä¸ç»“æ„åŒ–å®éªŒç›¸ç»“åˆï¼ŒAIèƒ½å¤Ÿä¸ºå¤æ‚çš„ç³»ç»Ÿé—®é¢˜äº§å‡ºå…·æœ‰åˆ›é€ æ€§ä¸”æ˜“äºç†è§£çš„è®¾è®¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27176v3",
      "published_date": "2025-10-31 04:58:00 UTC",
      "updated_date": "2025-11-17 17:29:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:01.357966+00:00"
    },
    {
      "arxiv_id": "2510.27173v1",
      "title": "FMint-SDE: A Multimodal Foundation Model for Accelerating Numerical Simulation of SDEs via Error Correction",
      "title_zh": "FMint-SDEï¼šé€šè¿‡è¯¯å·®ä¿®æ­£åŠ é€Ÿéšæœºå¾®åˆ†æ–¹ç¨‹æ•°å€¼æ¨¡æ‹Ÿçš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹",
      "authors": [
        "Jiaxin Yuan",
        "Haizhao Yang",
        "Maria Cameron"
      ],
      "abstract": "Fast and accurate simulation of dynamical systems is a fundamental challenge across scientific and engineering domains. Traditional numerical integrators often face a trade-off between accuracy and computational efficiency, while existing neural network-based approaches typically require training a separate model for each case. To overcome these limitations, we introduce a novel multi-modal foundation model for large-scale simulations of differential equations: FMint-SDE (Foundation Model based on Initialization for stochastic differential equations). Based on a decoder-only transformer with in-context learning, FMint-SDE leverages numerical and textual modalities to learn a universal error-correction scheme. It is trained using prompted sequences of coarse solutions generated by conventional solvers, enabling broad generalization across diverse systems. We evaluate our models on a suite of challenging SDE benchmarks spanning applications in molecular dynamics, mechanical systems, finance, and biology. Experimental results show that our approach achieves a superior accuracy-efficiency tradeoff compared to classical solvers, underscoring the potential of FMint-SDE as a general-purpose simulation tool for dynamical systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FMint-SDEï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨åŠ é€Ÿéšæœºå¾®åˆ†æ–¹ç¨‹(SDEs)æ•°å€¼æ¨¡æ‹Ÿçš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹åŸºäºä»…è§£ç å™¨çš„Transformeræ¶æ„å¹¶ç»“åˆä¸Šä¸‹æ–‡å­¦ä¹ (In-context learning)èƒ½åŠ›ï¼Œé€šè¿‡èåˆæ•°å€¼å’Œæ–‡æœ¬æ¨¡æ€æ¥å­¦ä¹ é€šç”¨çš„è¯¯å·®æ ¡æ­£æ–¹æ¡ˆã€‚FMint-SDEåˆ©ç”¨ä¼ ç»Ÿæ±‚è§£å™¨ç”Ÿæˆçš„ç²—ç•¥è§£åºåˆ—ä½œä¸ºæç¤ºè¿›è¡Œè®­ç»ƒï¼Œä»è€Œå®ç°åœ¨ä¸åŒåŠ¨åŠ›ç³»ç»Ÿé—´çš„å¹¿æ³›æ³›åŒ–ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•éœ€é’ˆå¯¹ç‰¹å®šæ¡ˆä¾‹å•ç‹¬è®­ç»ƒçš„å±€é™ã€‚åœ¨æ¶µç›–åˆ†å­åŠ¨åŠ›å­¦ã€æœºæ¢°ç³»ç»Ÿã€é‡‘èå’Œç”Ÿç‰©å­¦ç­‰é¢†åŸŸçš„SDEåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿæ•°å€¼ç§¯åˆ†å™¨å®ç°äº†æ›´ä¼˜çš„å‡†ç¡®ç‡ä¸æ•ˆç‡å¹³è¡¡ã€‚è¿™ä¸€è¿›å±•å‡¸æ˜¾äº†FMint-SDEä½œä¸ºåŠ¨åŠ›ç³»ç»Ÿé€šç”¨æ¨¡æ‹Ÿå·¥å…·çš„æ½œåŠ›ï¼Œä¸ºå¤æ‚ç§‘å­¦å·¥ç¨‹é—®é¢˜çš„ä»¿çœŸæä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG",
        "math.DS"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27173v1",
      "published_date": "2025-10-31 04:49:41 UTC",
      "updated_date": "2025-10-31 04:49:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:04.383698+00:00"
    },
    {
      "arxiv_id": "2510.27172v1",
      "title": "Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler",
      "title_zh": "åŸºäºè´å¶æ–¯æ•°æ®è°ƒåº¦å™¨çš„å¤§è¯­è¨€æ¨¡å‹æœ‰å®³å¾®è°ƒè‡ªé€‚åº”é˜²å¾¡",
      "authors": [
        "Zixuan Hu",
        "Li Shen",
        "Zhenyi Wang",
        "Yongxian Wei",
        "Dacheng Tao"
      ],
      "abstract": "Harmful fine-tuning poses critical safety risks to fine-tuning-as-a-service for large language models. Existing defense strategies preemptively build robustness via attack simulation but suffer from fundamental limitations: (i) the infeasibility of extending attack simulations beyond bounded threat models due to the inherent difficulty of anticipating unknown attacks, and (ii) limited adaptability to varying attack settings, as simulation fails to capture their variability and complexity. To address these challenges, we propose Bayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with no need for attack simulation. BDS formulates harmful fine-tuning defense as a Bayesian inference problem, learning the posterior distribution of each data point's safety attribute, conditioned on the fine-tuning and alignment datasets. The fine-tuning process is then constrained by weighting data with their safety attributes sampled from the posterior, thus mitigating the influence of harmful data. By leveraging the post hoc nature of Bayesian inference, the posterior is conditioned on the fine-tuning dataset, enabling BDS to tailor its defense to the specific dataset, thereby achieving adaptive defense. Furthermore, we introduce a neural scheduler based on amortized Bayesian learning, enabling efficient transfer to new data without retraining. Comprehensive results across diverse attack and defense settings demonstrate the state-of-the-art performance of our approach. Code is available at https://github.com/Egg-Hu/Bayesian-Data-Scheduler.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨å¾®è°ƒæœåŠ¡ä¸­é¢ä¸´çš„æœ‰å®³å¾®è°ƒ(Harmful Fine-Tuning)é£é™©ï¼Œæå‡ºäº†Bayesian Data Scheduler (BDS)è¿™ä¸€è‡ªé€‚åº”é˜²å¾¡ç­–ç•¥ã€‚é’ˆå¯¹ç°æœ‰é˜²å¾¡æ‰‹æ®µè¿‡åº¦ä¾èµ–æ”»å‡»æ¨¡æ‹Ÿ(Attack Simulation)è€Œéš¾ä»¥åº”å¯¹æœªçŸ¥æ”»å‡»çš„å±€é™æ€§ï¼ŒBDSæ— éœ€æ¨¡æ‹Ÿè¿‡ç¨‹ï¼Œå°†é˜²å¾¡ä»»åŠ¡å»ºæ¨¡ä¸ºä¸€ä¸ªè´å¶æ–¯æ¨ç†(Bayesian Inference)é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ å¾®è°ƒæ•°æ®å®‰å…¨å±æ€§åœ¨ç‰¹å®šæ•°æ®é›†æ¡ä»¶ä¸‹çš„åéªŒåˆ†å¸ƒ(Posterior Distribution)ï¼Œå¹¶åˆ©ç”¨è¯¥åˆ†å¸ƒå¯¹æ•°æ®è¿›è¡ŒåŠ æƒå¤„ç†ï¼Œä»è€Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆå‰Šå¼±æœ‰å®³æ•°æ®çš„å½±å“ã€‚å‡­å€Ÿè´å¶æ–¯æ¨ç†çš„åéªŒç‰¹æ€§ï¼ŒBDSèƒ½å¤Ÿæ ¹æ®æ•°æ®é›†ç‰¹å¾å®šåˆ¶é˜²å¾¡æ–¹æ¡ˆï¼Œå®ç°äº†å“è¶Šçš„è‡ªé€‚åº”é˜²å¾¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŸºäºæ‘Šé”€è´å¶æ–¯å­¦ä¹ (Amortized Bayesian Learning)çš„ç¥ç»è°ƒåº¦å™¨ï¼Œç¡®ä¿æ¨¡å‹èƒ½é«˜æ•ˆè¿ç§»è‡³æ–°æ•°æ®è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBDSåœ¨å¤šç§å¤æ‚çš„æ”»å‡»ä¸é˜²å¾¡åœºæ™¯ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½(State-of-the-Art)æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27172v1",
      "published_date": "2025-10-31 04:49:37 UTC",
      "updated_date": "2025-10-31 04:49:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:16.940887+00:00"
    },
    {
      "arxiv_id": "2510.27171v1",
      "title": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance Acceleration of Generative Diffusion Models",
      "title_zh": "H2-Cacheï¼šç”¨äºç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹é«˜æ€§èƒ½åŠ é€Ÿçš„æ–°å‹åˆ†å±‚åŒé˜¶æ®µç¼“å­˜",
      "authors": [
        "Mingyu Sung",
        "Il-Min Kim",
        "Sangseok Yun",
        "Jae-Mo Kang"
      ],
      "abstract": "Diffusion models have emerged as state-of-the-art in image generation, but their practical deployment is hindered by the significant computational cost of their iterative denoising process. While existing caching techniques can accelerate inference, they often create a challenging trade-off between speed and fidelity, suffering from quality degradation and high computational overhead. To address these limitations, we introduce H2-Cache, a novel hierarchical caching mechanism designed for modern generative diffusion model architectures. Our method is founded on the key insight that the denoising process can be functionally separated into a structure-defining stage and a detail-refining stage. H2-cache leverages this by employing a dual-threshold system, using independent thresholds to selectively cache each stage. To ensure the efficiency of our dual-check approach, we introduce pooled feature summarization (PFS), a lightweight technique for robust and fast similarity estimation. Extensive experiments on the Flux architecture demonstrate that H2-cache achieves significant acceleration (up to 5.08x) while maintaining image quality nearly identical to the baseline, quantitatively and qualitatively outperforming existing caching methods. Our work presents a robust and practical solution that effectively resolves the speed-quality dilemma, significantly lowering the barrier for the real-world application of high-fidelity diffusion models. Source code is available at https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†H2-Cacheï¼Œä¸€ç§ä¸ºç°ä»£ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹(Diffusion Models)è®¾è®¡çš„æ–°å‹åˆ†å±‚åŒé˜¶æ®µç¼“å­˜æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³è¿­ä»£å»å™ªè¿‡ç¨‹ä¸­è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå‘ç°å»å™ªè¿‡ç¨‹åœ¨åŠŸèƒ½ä¸Šå¯åˆ†ä¸ºç»“æ„å®šä¹‰é˜¶æ®µ(Structure-defining stage)å’Œç»†èŠ‚ç»†åŒ–é˜¶æ®µ(Detail-refining stage)ï¼Œå¹¶åŸºäºæ­¤æ„å»ºäº†åŒé˜ˆå€¼ç³»ç»Ÿè¿›è¡Œç‹¬ç«‹çš„é€‰æ‹©æ€§ç¼“å­˜ã€‚ä¸ºäº†æé«˜æ£€æµ‹æ•ˆç‡ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†è½»é‡åŒ–çš„æ± åŒ–ç‰¹å¾æ‘˜è¦(Pooled Feature Summarization, PFS)æŠ€æœ¯ï¼Œç”¨äºå®ç°å¿«é€Ÿä¸”ç¨³å¥çš„ç‰¹å¾ç›¸ä¼¼åº¦ä¼°è®¡ã€‚åœ¨Fluxæ¶æ„ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒH2-Cacheå®ç°äº†é«˜è¾¾5.08å€çš„æ¨ç†åŠ é€Ÿï¼Œä¸”ç”Ÿæˆçš„å›¾åƒè´¨é‡ä¸åŸºçº¿æ¨¡å‹å‡ ä¹å®Œå…¨ä¸€è‡´ã€‚è¯¥å·¥ä½œæœ‰æ•ˆè§£å†³äº†æ‰©æ•£æ¨¡å‹æ¨ç†ä¸­çš„é€Ÿåº¦ä¸è´¨é‡æƒè¡¡éš¾é¢˜ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¡¨ç°ä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„ç¼“å­˜æ–¹æ³•ï¼Œä¸ºé«˜ä¿çœŸç”Ÿæˆæ¨¡å‹çš„å®æ—¶åŒ–éƒ¨ç½²æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27171v1",
      "published_date": "2025-10-31 04:47:14 UTC",
      "updated_date": "2025-10-31 04:47:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:22.438673+00:00"
    },
    {
      "arxiv_id": "2511.09559v2",
      "title": "Enhancing Rare Codes via Probability-Biased Directed Graph Attention for Long-Tail ICD Coding",
      "title_zh": "åŸºäºæ¦‚ç‡åç½®æœ‰å‘å›¾æ³¨æ„åŠ›çš„é•¿å°¾ ICD ç¼–ç ç¨€æœ‰ä»£ç å¢å¼º",
      "authors": [
        "Tianlei Chen",
        "Yuxiao Chen",
        "Yang Li",
        "Feifei Wang"
      ],
      "abstract": "Automated international classification of diseases (ICD) coding aims to assign multiple disease codes to clinical documents and plays a critical role in healthcare informatics. However, its performance is hindered by the extreme long-tail distribution of the ICD ontology, where a few common codes dominate while thousands of rare codes have very few examples. To address this issue, we propose a Probability-Biased Directed Graph Attention model (ProBias) that partitions codes into common and rare sets and allows information to flow only from common to rare codes. Edge weights are determined by conditional co-occurrence probabilities, which guide the attention mechanism to enrich rare-code representations with clinically related signals. To provide higher-quality semantic representations as model inputs, we further employ large language models to generate enriched textual descriptions for ICD codes, offering external clinical context that complements statistical co-occurrence signals. Applied to automated ICD coding, our approach significantly improves the representation and prediction of rare codes, achieving state-of-the-art performance on three benchmark datasets. In particular, we observe substantial gains in macro-averaged F1 score, a key metric for long-tail classification.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨åŒ–ICDç¼–ç (ICD coding)ä¸­ç”±äºä»£ç åˆ†å¸ƒæä¸å¹³è¡¡å¯¼è‡´çš„é•¿å°¾é—®é¢˜ï¼Œæå‡ºäº†æ¦‚ç‡åç½®æœ‰å‘å›¾æ³¨æ„åŠ›æ¨¡å‹(ProBias)ã€‚ProBiasæ¨¡å‹å°†ç–¾ç—…ä»£ç åˆ’åˆ†ä¸ºå¸¸è§å’Œç¨€æœ‰ä¸¤ç»„ï¼Œé€šè¿‡è®¾è®¡ç‰¹å®šçš„ä¿¡æ¯æµå‘ä½¿è¯­ä¹‰ä¿¡æ¯ä»…ä»å¸¸è§ä»£ç æµå‘ç¨€æœ‰ä»£ç ã€‚æ¨¡å‹åˆ©ç”¨æ¡ä»¶å…±ç°æ¦‚ç‡(conditional co-occurrence probabilities)æ¥ç¡®å®šæœ‰å‘å›¾çš„è¾¹æƒé‡ï¼Œä»è€Œå¼•å¯¼æ³¨æ„åŠ›æœºåˆ¶ç”¨ä¸´åºŠç›¸å…³çš„ä¿¡å·ä¸°å¯Œç¨€æœ‰ä»£ç çš„ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸ºICDä»£ç ç”Ÿæˆäº†å¢å¼ºçš„æ–‡æœ¬æè¿°ï¼Œä»¥æä¾›è¡¥å……ç»Ÿè®¡å…±ç°ä¿¡å·çš„å¤–éƒ¨ä¸´åºŠèƒŒæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶åœ¨è¡¡é‡é•¿å°¾åˆ†ç±»çš„å…³é”®æŒ‡æ ‡macro-averaged F1ä¸Šå®ç°äº†æ˜¾è‘—æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.09559v2",
      "published_date": "2025-10-31 04:47:09 UTC",
      "updated_date": "2026-01-11 03:53:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:23.533855+00:00"
    },
    {
      "arxiv_id": "2510.27164v1",
      "title": "Generating Accurate and Detailed Captions for High-Resolution Images",
      "title_zh": "ç”Ÿæˆå‡†ç¡®ä¸”è¯¦å°½çš„é«˜åˆ†è¾¨ç‡å›¾åƒæè¿°",
      "authors": [
        "Hankyeol Lee",
        "Gawon Seo",
        "Kyounggyu Lee",
        "Dogun Kim",
        "Kyungwoo Song",
        "Jiyoung Jung"
      ],
      "abstract": "Vision-language models (VLMs) often struggle to generate accurate and detailed captions for high-resolution images since they are typically pre-trained on low-resolution inputs (e.g., 224x224 or 336x336 pixels). Downscaling high-resolution images to these dimensions may result in the loss of visual details and the omission of important objects. To address this limitation, we propose a novel pipeline that integrates vision-language models, large language models (LLMs), and object detection systems to enhance caption quality. Our proposed pipeline refines captions through a novel, multi-stage process. Given a high-resolution image, an initial caption is first generated using a VLM, and key objects in the image are then identified by an LLM. The LLM predicts additional objects likely to co-occur with the identified key objects, and these predictions are verified by object detection systems. Newly detected objects not mentioned in the initial caption undergo focused, region-specific captioning to ensure they are incorporated. This process enriches caption detail while reducing hallucinations by removing references to undetected objects. We evaluate the enhanced captions using pairwise comparison and quantitative scoring from large multimodal models, along with a benchmark for hallucination detection. Experiments on a curated dataset of high-resolution images demonstrate that our pipeline produces more detailed and reliable image captions while effectively minimizing hallucinations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶å› ä¸‹é‡‡æ ·å¯¼è‡´ç»†èŠ‚ä¸¢å¤±å’Œç‰©ä½“é—æ¼çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ•´åˆäº†VLMsã€å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œç›®æ ‡æ£€æµ‹ç³»ç»Ÿ(object detection systems)çš„æ–°å‹æµæ°´çº¿ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šé˜¶æ®µè¿‡ç¨‹ä¼˜åŒ–å›¾åƒæè¿°ï¼šé¦–å…ˆç”±VLMç”Ÿæˆåˆå§‹æè¿°ï¼Œéšååˆ©ç”¨LLMè¯†åˆ«å…³é”®ç‰©ä½“å¹¶é¢„æµ‹å¯èƒ½çš„å…±ç°ç‰©ä½“ï¼Œå¹¶ç”±ç›®æ ‡æ£€æµ‹ç³»ç»Ÿè¿›è¡ŒéªŒè¯ã€‚å¯¹äºåˆå§‹æè¿°ä¸­é—æ¼çš„æ–°æ£€æµ‹ç‰©ä½“ï¼Œç³»ç»Ÿä¼šæ‰§è¡Œé’ˆå¯¹æ€§çš„åŒºåŸŸç‰¹å®šæè¿°(region-specific captioning)ä»¥ç¡®ä¿å…¶è¢«çº³å…¥æœ€ç»ˆæ–‡æœ¬ã€‚è¿™ä¸€æµç¨‹åœ¨ä¸°å¯Œæè¿°ç»†èŠ‚çš„åŒæ—¶ï¼Œé€šè¿‡ç§»é™¤å¯¹æœªéªŒè¯ç‰©ä½“çš„å¼•ç”¨æœ‰æ•ˆå‡å°‘äº†å¹»è§‰(hallucinations)ã€‚å®éªŒåœ¨ä¸“é—¨çš„é«˜åˆ†è¾¨ç‡å›¾åƒæ•°æ®é›†ä¸Šé€šè¿‡æˆå¯¹æ¯”è¾ƒå’Œå®šé‡è¯„åˆ†è¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜è¯¥æµæ°´çº¿ç”Ÿæˆçš„æè¿°æ›´å…·ç»†èŠ‚ä¸”æ›´ä¸ºå¯é ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æå‡æè¿°è´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½äº†å¤šæ¨¡æ€æ¨¡å‹ä¸­å¸¸è§çš„å¹»è§‰é—®é¢˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Work conducted in 2024; released for archival purposes",
      "pdf_url": "https://arxiv.org/pdf/2510.27164v1",
      "published_date": "2025-10-31 04:22:22 UTC",
      "updated_date": "2025-10-31 04:22:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:27.936339+00:00"
    },
    {
      "arxiv_id": "2510.27163v1",
      "title": "MARIA: A Framework for Marginal Risk Assessment without Ground Truth in AI Systems",
      "title_zh": "MARIAï¼šä¸€ç§æ— éœ€åœ°é¢çœŸå€¼çš„ AI ç³»ç»Ÿè¾¹é™…é£é™©è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Jieshan Chen",
        "Suyu Ma",
        "Qinghua Lu",
        "Sung Une Lee",
        "Liming Zhu"
      ],
      "abstract": "Before deploying an AI system to replace an existing process, it must be compared with the incumbent to ensure improvement without added risk. Traditional evaluation relies on ground truth for both systems, but this is often unavailable due to delayed or unknowable outcomes, high costs, or incomplete data, especially for long-standing systems deemed safe by convention. The more practical solution is not to compute absolute risk but the difference between systems. We therefore propose a marginal risk assessment framework, that avoids dependence on ground truth or absolute risk. It emphasizes three kinds of relative evaluation methodology, including predictability, capability and interaction dominance. By shifting focus from absolute to relative evaluation, our approach equips software teams with actionable guidance: identifying where AI enhances outcomes, where it introduces new risks, and how to adopt such systems responsibly.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MARIAï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹AIç³»ç»Ÿåœ¨ç¼ºä¹Ground Truthæƒ…å†µä¸‹è¿›è¡ŒMarginal Risk Assessmentçš„è¯„ä¼°æ¡†æ¶ã€‚ç”±äºå®é™…åœºæ™¯ä¸­å¾€å¾€é¢ä¸´ç»“æœæ»åæˆ–è·å–æ•°æ®æˆæœ¬é«˜æ˜‚ç­‰é—®é¢˜ï¼Œä¼ ç»Ÿçš„Absolute Riskè¯„ä¼°æ–¹æ³•éš¾ä»¥æ–½è¡Œã€‚MARIAé€šè¿‡å°†å…³æ³¨ç‚¹ä»ç»å¯¹è¯„ä¼°è½¬å‘ç›¸å¯¹è¯„ä¼°ï¼Œé‡ç‚¹è€ƒé‡Predictabilityã€Capabilityå’ŒInteraction Dominanceä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œä»è€Œé¿å¼€äº†å¯¹åœ°é¢çœŸå€¼çš„ä¾èµ–ã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡è¯†åˆ«AIç³»ç»Ÿåœ¨å“ªäº›ç¯èŠ‚ä¼˜åŒ–äº†äº§å‡ºä»¥åŠåœ¨ä½•å¤„å¼•å…¥äº†æ–°é£é™©ï¼Œä¸ºè½¯ä»¶å›¢é˜Ÿæä¾›åˆ‡å®å¯è¡Œçš„æŒ‡å¯¼å»ºè®®ã€‚è¿™ç§è¯„ä¼°èŒƒå¼çš„è½¬å˜ï¼Œä¸ºåœ¨å¤æ‚ä¸”ç¼ºä¹åŸºå‡†æ•°æ®çš„ç¯å¢ƒä¸‹å®ç°AIç³»ç»Ÿçš„è´Ÿè´£ä»»éƒ¨ç½²å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "9 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2510.27163v1",
      "published_date": "2025-10-31 04:18:20 UTC",
      "updated_date": "2025-10-31 04:18:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:29.533378+00:00"
    },
    {
      "arxiv_id": "2511.01902v1",
      "title": "Before the Clinic: Transparent and Operable Design Principles for Healthcare AI",
      "title_zh": "èµ°å‘ä¸´åºŠä¹‹å‰ï¼šåŒ»ç–—äººå·¥æ™ºèƒ½çš„é€æ˜åŒ–ä¸å¯æ“ä½œæ€§è®¾è®¡åŸåˆ™",
      "authors": [
        "Alexander Bakumenko",
        "Aaron J. Masino",
        "Janine Hoelscher"
      ],
      "abstract": "The translation of artificial intelligence (AI) systems into clinical practice requires bridging fundamental gaps between explainable AI theory, clinician expectations, and governance requirements. While conceptual frameworks define what constitutes explainable AI (XAI) and qualitative studies identify clinician needs, little practical guidance exists for development teams to prepare AI systems prior to clinical evaluation. We propose two foundational design principles, Transparent Design and Operable Design, that operationalize pre-clinical technical requirements for healthcare AI. Transparent Design encompasses interpretability and understandability artifacts that enable case-level reasoning and system traceability. Operable Design encompasses calibration, uncertainty, and robustness to ensure reliable, predictable system behavior under real-world conditions. We ground these principles in established XAI frameworks, map them to documented clinician needs, and demonstrate their alignment with emerging governance requirements. This pre-clinical playbook provides actionable guidance for development teams, accelerates the path to clinical evaluation, and establishes a shared vocabulary bridging AI researchers, healthcare practitioners, and regulatory stakeholders. By explicitly scoping what can be built and verified before clinical deployment, we aim to reduce friction in clinical AI translation while remaining cautious about what constitutes validated, deployed explainability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—äººå·¥æ™ºèƒ½(AI)åœ¨è¿›å…¥ä¸´åºŠå®è·µå‰é¢ä¸´çš„è§£é‡Šæ€§ç†è®ºä¸ä¸´åºŠéœ€æ±‚ã€ç›‘ç®¡è¦æ±‚ä¹‹é—´çš„è„±èŠ‚ï¼Œæå‡ºäº†é€æ˜è®¾è®¡(Transparent Design)å’Œå¯æ“ä½œè®¾è®¡(Operable Design)ä¸¤å¤§åŸºæœ¬è®¾è®¡åŸåˆ™ã€‚é€æ˜è®¾è®¡(Transparent Design)æ¶µç›–äº†å¯è§£é‡Šæ€§(interpretability)å’Œå¯ç†è§£æ€§(understandability)ç­‰è¦ç´ ï¼Œæ—¨åœ¨å®ç°ç—…ä¾‹ç»´åº¦çš„æ¨ç†å’Œç³»ç»Ÿæº¯æºã€‚å¯æ“ä½œè®¾è®¡(Operable Design)åˆ™é€šè¿‡æ ¡å‡†(calibration)ã€ä¸ç¡®å®šæ€§(uncertainty)å’Œé²æ£’æ€§(robustness)æ¥ç¡®ä¿ç³»ç»Ÿåœ¨çœŸå®ç¯å¢ƒä¸‹çš„å¯é æ€§å’Œå¯é¢„æµ‹æ€§ã€‚ç ”ç©¶å°†è¿™äº›åŸåˆ™ä¸æ—¢æœ‰çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æ¡†æ¶ç»“åˆï¼Œå¹¶å°†å…¶æ˜ å°„è‡³ä¸´åºŠåŒ»ç”Ÿçš„å…·ä½“éœ€æ±‚å’Œæ–°å…´çš„ç›‘ç®¡åˆè§„æ ‡å‡†ã€‚è¯¥é¢„ä¸´åºŠæŒ‡å—ä¸ºå¼€å‘å›¢é˜Ÿæä¾›äº†å¯æ“ä½œçš„æŒ‡å¯¼ï¼Œæ—¨åœ¨åŠ é€ŸAIç³»ç»Ÿçš„ä¸´åºŠè¯„ä¼°è¿›ç¨‹å¹¶å‡å°‘è½¬åŒ–æ‘©æ“¦ã€‚é€šè¿‡å»ºç«‹ä¸€å¥—è¿æ¥ç ”ç©¶è€…ã€ä»ä¸šè€…å’Œç›‘ç®¡æ–¹çš„é€šç”¨è¯­æ±‡ï¼Œæœ¬ç ”ç©¶ä¸ºæ„å»ºå¯éªŒè¯ä¸”ç¬¦åˆä¸´åºŠæ ‡å‡†çš„åŒ»ç–—AIå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.01902v1",
      "published_date": "2025-10-31 04:05:09 UTC",
      "updated_date": "2025-10-31 04:05:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:31.533579+00:00"
    },
    {
      "arxiv_id": "2510.27153v1",
      "title": "Exploring Landscapes for Better Minima along Valleys",
      "title_zh": "æ²¿è°·åœ°æ¢ç´¢æ™¯è§‚ä»¥å¯»æ‰¾æ›´ä¼˜æå°å€¼",
      "authors": [
        "Tong Zhao",
        "Jiacheng Li",
        "Yuanchang Zhou",
        "Guangming Tan",
        "Weile Jia"
      ],
      "abstract": "Finding lower and better-generalizing minima is crucial for deep learning. However, most existing optimizers stop searching the parameter space once they reach a local minimum. Given the complex geometric properties of the loss landscape, it is difficult to guarantee that such a point is the lowest or provides the best generalization. To address this, we propose an adaptor \"E\" for gradient-based optimizers. The adapted optimizer tends to continue exploring along landscape valleys (areas with low and nearly identical losses) in order to search for potentially better local minima even after reaching a local minimum. This approach increases the likelihood of finding a lower and flatter local minimum, which is often associated with better generalization. We also provide a proof of convergence for the adapted optimizers in both convex and non-convex scenarios for completeness. Finally, we demonstrate their effectiveness in an important but notoriously difficult training scenario, large-batch training, where Lamb is the benchmark optimizer. Our testing results show that the adapted Lamb, ALTO, increases the test accuracy (generalization) of the current state-of-the-art optimizer by an average of 2.5% across a variety of large-batch training tasks. This work potentially opens a new research direction in the design of optimization algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ ä¸­å¯»æ‰¾æ›´ä½ä¸”æ³›åŒ–æ€§èƒ½æ›´ä½³çš„ minima çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰ä¼˜åŒ–å™¨åœ¨è§¦è¾¾å±€éƒ¨æœ€å°å€¼ååœæ­¢æœç´¢çš„å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§é€šç”¨çš„ gradient-based optimizers é€‚é…å™¨ \"E\"ã€‚è¯¥é€‚é…å™¨ä½¿ä¼˜åŒ–å™¨èƒ½å¤Ÿæ²¿ç€ loss landscape ä¸­æŸè€—æä½ä¸”å¹³ç¼“çš„ valleys ç»§ç»­æ¢ç´¢ï¼Œä»è€Œæ˜¾è‘—æé«˜å‘ç°æ›´ä½ã€æ›´å¹³å¦ local minimum çš„æ¦‚ç‡ã€‚è®ºæ–‡åœ¨ç†è®ºä¸Šè¯æ˜äº†è¯¥é€‚é…å™¨åœ¨å‡¸ä¸éå‡¸åœºæ™¯ä¸‹çš„æ”¶æ•›æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æå…·æŒ‘æˆ˜æ€§çš„å¤§æ‰¹é‡è®­ç»ƒï¼ˆlarge-batch trainingï¼‰åœºæ™¯ä¸­ï¼ŒåŸºäº Lamb æ”¹è¿›çš„ ALTO ä¼˜åŒ–å™¨åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šå°†æµ‹è¯•å‡†ç¡®ç‡å¹³å‡æå‡äº† 2.5%ã€‚è¯¥ç ”ç©¶é€šè¿‡æ¢ç´¢ loss landscape çš„å‡ ä½•ç‰¹æ€§ï¼Œä¸ºæ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•çš„è®¾è®¡å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Neurips 2025 poster",
      "pdf_url": "https://arxiv.org/pdf/2510.27153v1",
      "published_date": "2025-10-31 03:53:49 UTC",
      "updated_date": "2025-10-31 03:53:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:36.739577+00:00"
    },
    {
      "arxiv_id": "2511.00117v1",
      "title": "DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads",
      "title_zh": "DCcluster-Optï¼šé’ˆå¯¹åœ°ç†åˆ†å¸ƒå¼æ•°æ®ä¸­å¿ƒå·¥ä½œè´Ÿè½½çš„åŠ¨æ€å¤šç›®æ ‡ä¼˜åŒ–åŸºå‡†æµ‹è¯•",
      "authors": [
        "Antonio Guillen-Perez",
        "Avisek Naug",
        "Vineet Gundecha",
        "Sahand Ghorbanpour",
        "Ricardo Luna Gutierrez",
        "Ashwin Ramesh Babu",
        "Munther Salim",
        "Shubhanker Banerjee",
        "Eoin H. Oude Essink",
        "Damien Fay",
        "Soumyendu Sarkar"
      ],
      "abstract": "The increasing energy demands and carbon footprint of large-scale AI require intelligent workload management in globally distributed data centers. Yet progress is limited by the absence of benchmarks that realistically capture the interplay of time-varying environmental factors (grid carbon intensity, electricity prices, weather), detailed data center physics (CPUs, GPUs, memory, HVAC energy), and geo-distributed network dynamics (latency and transmission costs). To bridge this gap, we present DCcluster-Opt: an open-source, high-fidelity simulation benchmark for sustainable, geo-temporal task scheduling. DCcluster-Opt combines curated real-world datasets, including AI workload traces, grid carbon intensity, electricity markets, weather across 20 global regions, cloud transmission costs, and empirical network delay parameters with physics-informed models of data center operations, enabling rigorous and reproducible research in sustainable computing. It presents a challenging scheduling problem where a top-level coordinating agent must dynamically reassign or defer tasks that arrive with resource and service-level agreement requirements across a configurable cluster of data centers to optimize multiple objectives. The environment also models advanced components such as heat recovery. A modular reward system enables an explicit study of trade-offs among carbon emissions, energy costs, service level agreements, and water use. It provides a Gymnasium API with baseline controllers, including reinforcement learning and rule-based strategies, to support reproducible ML research and a fair comparison of diverse algorithms. By offering a realistic, configurable, and accessible testbed, DCcluster-Opt accelerates the development and validation of next-generation sustainable computing solutions for geo-distributed data centers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡äººå·¥æ™ºèƒ½(AI)èƒ½æºæ¶ˆè€—ä¸ç¢³è¶³è¿¹å¢é•¿å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†DCcluster-Optï¼Œä¸€ä¸ªç”¨äºåœ°ç†åˆ†å¸ƒå¼æ•°æ®ä¸­å¿ƒå¯æŒç»­ä»»åŠ¡è°ƒåº¦çš„å¼€æºé«˜ä¿çœŸä»¿çœŸåŸºå‡†ã€‚DCcluster-Optæ•´åˆäº†å…¨çƒ20ä¸ªåœ°åŒºçš„ç”µç½‘ç¢³å¼ºåº¦ã€ç”µåŠ›ä»·æ ¼ã€å¤©æ°”ã€äº‘ä¼ è¾“æˆæœ¬åŠç½‘ç»œå»¶è¿Ÿç­‰çœŸå®æ•°æ®é›†ï¼Œå¹¶ç»“åˆç‰©ç†æ„ŸçŸ¥çš„æ•°æ®ä¸­å¿ƒè¿è¥æ¨¡å‹ï¼Œä¸ºå¯æŒç»­è®¡ç®—ç ”ç©¶æä¾›äº†ä¸¥è°¨ä¸”å¯é‡å¤çš„å®éªŒç¯å¢ƒã€‚è¯¥åŸºå‡†æ„å»ºäº†ä¸€ä¸ªå¤æ‚çš„åŠ¨æ€å¤šç›®æ ‡ä¼˜åŒ–(Multi-Objective Optimization)é—®é¢˜ï¼Œåè°ƒæ™ºèƒ½ä½“éœ€åœ¨æ»¡è¶³èµ„æºä¸æœåŠ¡æ°´å¹³åè®®(Service-Level Agreement, SLA)çš„çº¦æŸä¸‹ï¼Œé€šè¿‡åŠ¨æ€åˆ†é…æˆ–å»¶è¿Ÿä»»åŠ¡æ¥å¹³è¡¡ç¢³æ’æ”¾ã€èƒ½æºæˆæœ¬åŠæ°´èµ„æºä½¿ç”¨ã€‚è¯¥æ¡†æ¶é›†æˆäº†å…ˆè¿›çš„ä½™çƒ­å›æ”¶æ¨¡å‹ï¼Œå¹¶æä¾›å…¼å®¹Gymnasium APIçš„æ¥å£åŠå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç­‰åŸºå‡†ç®—æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿä¸‹ä¸€ä»£åœ°ç†åˆ†å¸ƒå¼å¯æŒç»­è®¡ç®—è§£å†³æ–¹æ¡ˆçš„å¼€å‘ä¸éªŒè¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to the NeurIPS 2025 conference",
      "pdf_url": "https://arxiv.org/pdf/2511.00117v1",
      "published_date": "2025-10-31 03:07:12 UTC",
      "updated_date": "2025-10-31 03:07:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:38.642596+00:00"
    },
    {
      "arxiv_id": "2510.27128v1",
      "title": "ZEBRA: Towards Zero-Shot Cross-Subject Generalization for Universal Brain Visual Decoding",
      "title_zh": "ZEBRAï¼šè¿ˆå‘é€šç”¨å¤§è„‘è§†è§‰è§£ç çš„é›¶æ ·æœ¬è·¨è¢«è¯•æ³›åŒ–",
      "authors": [
        "Haonan Wang",
        "Jingyu Lu",
        "Hongrui Li",
        "Xiaomeng Li"
      ],
      "abstract": "Recent advances in neural decoding have enabled the reconstruction of visual experiences from brain activity, positioning fMRI-to-image reconstruction as a promising bridge between neuroscience and computer vision. However, current methods predominantly rely on subject-specific models or require subject-specific fine-tuning, limiting their scalability and real-world applicability. In this work, we introduce ZEBRA, the first zero-shot brain visual decoding framework that eliminates the need for subject-specific adaptation. ZEBRA is built on the key insight that fMRI representations can be decomposed into subject-related and semantic-related components. By leveraging adversarial training, our method explicitly disentangles these components to isolate subject-invariant, semantic-specific representations. This disentanglement allows ZEBRA to generalize to unseen subjects without any additional fMRI data or retraining. Extensive experiments show that ZEBRA significantly outperforms zero-shot baselines and achieves performance comparable to fully finetuned models on several metrics. Our work represents a scalable and practical step toward universal neural decoding. Code and model weights are available at: https://github.com/xmed-lab/ZEBRA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ZEBRAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®ç°Zero-Shotè·¨è¢«è¯•æ³›åŒ–çš„è„‘è§†è§‰è§£ç æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰fMRIåˆ°å›¾åƒé‡å»ºæ–¹æ³•å¯¹ç‰¹å®šè¢«è¯•æ¨¡å‹ä¾èµ–æ€§å¼ºä¸”æ‰©å±•æ€§æœ‰é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒè§è§£åœ¨äºfMRIè¡¨å¾å¯ä»¥è¢«åˆ†è§£ä¸ºä¸è¢«è¯•ç›¸å…³å’Œä¸è¯­ä¹‰ç›¸å…³çš„ç»„æˆéƒ¨åˆ†ã€‚é€šè¿‡åˆ©ç”¨Adversarial Trainingï¼ŒZEBRAèƒ½å¤Ÿæ˜¾å¼åœ°è§£è€¦è¿™äº›æˆåˆ†ï¼Œä»è€Œæå–å‡ºä¸è¢«è¯•æ— å…³ä¸”å…·å¤‡è¯­ä¹‰ç‰¹å¼‚æ€§çš„è¡¨å¾ã€‚è¿™ç§è§£è€¦è®¾è®¡ä½¿å¾—æ¨¡å‹æ— éœ€ä»»ä½•é¢å¤–çš„fMRIæ•°æ®æˆ–é’ˆå¯¹æ€§å¾®è°ƒå³å¯ç›´æ¥æ³›åŒ–è‡³æœªè§è¿‡çš„è¢«è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒZEBRAåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„Zero-ShotåŸºå‡†æ¨¡å‹ï¼Œå¹¶å–å¾—äº†ä¸ç»è¿‡å®Œå…¨Fine-tunedæ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºå®ç°é€šç”¨Neural Decodingè¿ˆå‡ºäº†å…·æœ‰å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§çš„é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.27128v1",
      "published_date": "2025-10-31 03:05:04 UTC",
      "updated_date": "2025-10-31 03:05:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:41.338877+00:00"
    },
    {
      "arxiv_id": "2511.00116v1",
      "title": "LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers",
      "title_zh": "LC-Optï¼šé¢å‘æ•°æ®ä¸­å¿ƒç«¯åˆ°ç«¯æ¶²å†·ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ ä¸æ™ºèƒ½ä½“ AI åŸºå‡†æµ‹è¯•",
      "authors": [
        "Avisek Naug",
        "Antonio Guillen",
        "Vineet Kumar",
        "Scott Greenwood",
        "Wesley Brewer",
        "Sahand Ghorbanpour",
        "Ashwin Ramesh Babu",
        "Vineet Gundecha",
        "Ricardo Luna Gutierrez",
        "Soumyendu Sarkar"
      ],
      "abstract": "Liquid cooling is critical for thermal management in high-density data centers with the rising AI workloads. However, machine learning-based controllers are essential to unlock greater energy efficiency and reliability, promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC) benchmark environment, for reinforcement learning (RL) control strategies in energy-efficient liquid cooling of high-performance computing (HPC) systems. Built on the baseline of a high-fidelity digital twin of Oak Ridge National Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed Modelica-based end-to-end models spanning site-level cooling towers to data center cabinets and server blade groups. RL agents optimize critical thermal controls like liquid supply temperature, flow rate, and granular valve actuation at the IT cabinet level, as well as cooling tower (CT) setpoints through a Gymnasium interface, with dynamic changes in workloads. This environment creates a multi-objective real-time optimization challenge balancing local thermal regulation and global energy efficiency, and also supports additional components like a heat recovery unit (HRU). We benchmark centralized and decentralized multi-agent RL approaches, demonstrate policy distillation into decision and regression trees for interpretable control, and explore LLM-based methods that explain control actions in natural language through an agentic mesh architecture designed to foster user trust and simplify system management. LC-Opt democratizes access to detailed, customizable liquid cooling models, enabling the ML community, operators, and vendors to develop sustainable data center liquid cooling control solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†LC-Optï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæ•°æ®ä¸­å¿ƒæ¶²ä½“å†·å´ç³»ç»Ÿè®¾è®¡çš„å¯æŒç»­åŸºå‡†ç¯å¢ƒï¼Œæ—¨åœ¨åˆ©ç”¨Reinforcement Learning (RL) å’Œ Agentic AI æå‡èƒ½æºæ•ˆç‡ä¸å¯é æ€§ã€‚è¯¥åŸºå‡†åŸºäºOak Ridge National Labçš„Frontierè¶…çº§è®¡ç®—æœºå†·å´ç³»ç»Ÿçš„é«˜ä¿çœŸæ•°å­—å­ªç”Ÿæ¨¡å‹ï¼Œé€šè¿‡Modelicaæ„å»ºäº†ä»å†·å´å¡”åˆ°æœåŠ¡å™¨æœºæ¶åŠåˆ€ç‰‡ç»„çš„ç«¯åˆ°ç«¯ä»¿çœŸã€‚ç ”ç©¶åˆ©ç”¨Gymnasiumæ¥å£ï¼Œä½¿RLæ™ºèƒ½ä½“èƒ½å¤Ÿé’ˆå¯¹åŠ¨æ€å·¥ä½œè´Ÿè½½å®æ—¶ä¼˜åŒ–æ¶²ä½“ä¾›ç»™æ¸©åº¦ã€æµé‡ã€æœºæ¶çº§é˜€é—¨åŠ¨ä½œä»¥åŠå†·å´å¡”è®¾å®šç‚¹ã€‚LC-Optä¸ä»…è¯„ä¼°äº†ä¸­å¿ƒåŒ–ä¸å»ä¸­å¿ƒåŒ–çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè¿˜é€šè¿‡ç­–ç•¥è’¸é¦ç”Ÿæˆå†³ç­–æ ‘å’Œå›å½’æ ‘ï¼Œä»¥å®ç°æ§åˆ¶ç­–ç•¥çš„å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç¯å¢ƒå¼•å…¥äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“ç½‘æ ¼æ¶æ„ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€è§£é‡Šæ§åˆ¶è¡Œä¸ºï¼Œä»è€Œå¢å¼ºç”¨æˆ·ä¿¡ä»»å¹¶ç®€åŒ–ç³»ç»Ÿç®¡ç†ã€‚LC-Opté€šè¿‡æä¾›è¯¦ç»†ä¸”å¯å®šåˆ¶çš„æ¶²ä½“å†·å´æ¨¡å‹ï¼Œä¸ºæœºå™¨å­¦ä¹ ç¤¾åŒºå’Œè¿è¥å•†å¼€å‘å¯æŒç»­çš„æ•°æ®ä¸­å¿ƒå†·å´æ§åˆ¶æ–¹æ¡ˆæä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to the NeurIPS 2025 conference",
      "pdf_url": "https://arxiv.org/pdf/2511.00116v1",
      "published_date": "2025-10-31 03:04:14 UTC",
      "updated_date": "2025-10-31 03:04:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:26:44.230706+00:00"
    },
    {
      "arxiv_id": "2510.27126v2",
      "title": "AURA: A Reinforcement Learning Framework for AI-Driven Adaptive Conversational Surveys",
      "title_zh": "AURAï¼šäººå·¥æ™ºèƒ½é©±åŠ¨çš„è‡ªé€‚åº”å¯¹è¯å¼è°ƒæŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Jinwen Tang",
        "Yi Shang"
      ],
      "abstract": "Conventional online surveys provide limited personalization, often resulting in low engagement and superficial responses. Although AI survey chatbots improve convenience, most are still reactive: they rely on fixed dialogue trees or static prompt templates and therefore cannot adapt within a session to fit individual users, which leads to generic follow-ups and weak response quality. We address these limitations with AURA (Adaptive Understanding through Reinforcement Learning for Assessment), a reinforcement learning framework for AI-driven adaptive conversational surveys. AURA quantifies response quality using a four-dimensional LSDE metric (Length, Self-disclosure, Emotion, and Specificity) and selects follow-up question types via an epsilon-greedy policy that updates the expected quality gain within each session. Initialized with priors extracted from 96 prior campus-climate conversations (467 total chatbot-user exchanges), the system balances exploration and exploitation across 10-15 dialogue exchanges, dynamically adapting to individual participants in real time. In controlled evaluations, AURA achieved a +0.076 mean gain in response quality and a statistically significant improvement over non-adaptive baselines (p=0.044, d=0.66), driven by a 63% reduction in specification prompts and a 10x increase in validation behavior. These results demonstrate that reinforcement learning can give survey chatbots improved adaptivity, transforming static questionnaires into interactive, self-improving assessment systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AURAï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„AIé©±åŠ¨è‡ªé€‚åº”å¯¹è¯å¼è°ƒæŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿåœ¨çº¿è°ƒæŸ¥å’Œååº”å¼AIèŠå¤©æœºå™¨äººå‚ä¸åº¦ä½åŠå›å¤æµ…è–„çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒ…å«Lengthã€Self-disclosureã€Emotionå’ŒSpecificityå››ä¸ªç»´åº¦çš„LSDEæŒ‡æ ‡æ¥é‡åŒ–å“åº”è´¨é‡ï¼Œå¹¶åˆ©ç”¨epsilon-greedyç­–ç•¥åœ¨ä¼šè¯ä¸­åŠ¨æ€é€‰æ‹©åç»­é—®é¢˜ç±»å‹ä»¥é€‚åº”ä¸ªä½“ç”¨æˆ·ã€‚AURAåˆ©ç”¨ä»96åœºå‰æœŸå¯¹è¯ä¸­æå–çš„å…ˆéªŒçŸ¥è¯†è¿›è¡Œåˆå§‹åŒ–ï¼Œåœ¨10è‡³15è½®å¯¹è¯äº¤æ¢ä¸­å®ç°æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ã€‚å—æ§è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒAURAåœ¨å“åº”è´¨é‡ä¸Šæ¯”éè‡ªé€‚åº”åŸºçº¿æ¨¡å‹å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹³å‡è´¨é‡å¢ç›Šè¾¾0.076ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿä½¿è§„èŒƒæ€§æç¤º(specification prompts)å‡å°‘äº†63%ï¼Œä¸”éªŒè¯è¡Œä¸º(validation behavior)å¢åŠ äº†10å€ã€‚è¿™äº›ç»“æœè¯æ˜äº†å¼ºåŒ–å­¦ä¹ èƒ½æœ‰æ•ˆæå‡è°ƒæŸ¥æœºå™¨äººçš„è‡ªé€‚åº”èƒ½åŠ›ï¼ŒæˆåŠŸå°†é™æ€é—®å·è½¬åŒ–ä¸ºäº¤äº’å¼ä¸”å…·æœ‰è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›çš„è¯„ä¼°ç³»ç»Ÿã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27126v2",
      "published_date": "2025-10-31 03:03:55 UTC",
      "updated_date": "2025-11-07 06:59:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:01.039697+00:00"
    },
    {
      "arxiv_id": "2511.00115v2",
      "title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference",
      "title_zh": "äººæ ¼æ¨ç†ä¸­çš„è®¤çŸ¥å¯¹é½ï¼šåŸºäºåŸå‹ç†è®ºçš„ MBTI æ¨æ–­",
      "authors": [
        "Haoyuan Li",
        "Yuanbo Tong",
        "Yuchen Li",
        "Zirui Wang",
        "Chunhou Liu",
        "Jiamou Liu"
      ],
      "abstract": "Personality recognition from text is typically cast as hard-label classification, which obscures the graded, prototype-like nature of human personality judgments. We present ProtoMBTI, a cognitively aligned framework for MBTI inference that operationalizes prototype theory within an LLM-based pipeline. First, we construct a balanced, quality-controlled corpus via LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment). Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative embeddings and to standardize a bank of personality prototypes. At inference, we retrieve top-k prototypes for a query post and perform a retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence via prompt-based voting, revises when inconsistencies arise, and, upon correct prediction, retains the sample to continually enrich the prototype library. Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both the four MBTI dichotomies and the full 16-type task, and exhibits robust cross-dataset generalization. Our results indicate that aligning the inference process with psychological prototype reasoning yields gains in accuracy, interpretability, and transfer for text-based personality modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProtoMBTIï¼Œä¸€ç§é€šè¿‡åœ¨ LLM æµæ°´çº¿ä¸­åº”ç”¨åŸå‹ç†è®º (Prototype Theory) å®ç°è®¤çŸ¥å¯¹é½çš„ MBTI æ¨æ–­æ¡†æ¶ã€‚ç ”ç©¶é¦–å…ˆåˆ©ç”¨ LLM å¼•å¯¼çš„å¤šç»´åº¦å¢å¼ºæŠ€æœ¯æ„å»ºäº†å¹³è¡¡çš„é«˜è´¨é‡è¯­æ–™åº“ï¼Œå¹¶é‡‡ç”¨ LoRA å¯¹è½»é‡çº§ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥å»ºç«‹æ ‡å‡†åŒ–çš„æ€§æ ¼åŸå‹åº“ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é€šè¿‡æ‰§è¡Œâ€œæ£€ç´¢-é‡ç”¨-ä¿®æ­£-ä¿ç•™ (retrieve-reuse-revise-retain)â€å¾ªç¯ï¼Œåˆ©ç”¨åŸºäºæç¤ºçš„æŠ•ç¥¨æœºåˆ¶èšåˆè¯æ®ï¼Œå¹¶åœ¨é¢„æµ‹æˆåŠŸåä¸æ–­å¯Œé›†åŸå‹åº“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProtoMBTI åœ¨ Kaggle å’Œ Pandora åŸºå‡†æµ‹è¯•çš„å››ç»´åº¦åŠå…¨ 16 ç§äººæ ¼åˆ†ç±»ä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¯æ˜ï¼Œå°†æ¨ç†è¿‡ç¨‹ä¸å¿ƒç†å­¦åŸå‹æ¨ç†ç›¸å¯¹é½ï¼Œèƒ½æœ‰æ•ˆæå‡æ–‡æœ¬äººæ ¼å»ºæ¨¡çš„å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§ä»¥åŠè·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ› (Generalization)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The authors have decided to withdraw this version to substantially revise and extend the work",
      "pdf_url": "https://arxiv.org/pdf/2511.00115v2",
      "published_date": "2025-10-31 02:45:30 UTC",
      "updated_date": "2025-12-29 13:01:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:01.732437+00:00"
    },
    {
      "arxiv_id": "2511.08597v1",
      "title": "Self-HarmLLM: Can Large Language Model Harm Itself?",
      "title_zh": "Self-HarmLLMï¼šå¤§è¯­è¨€æ¨¡å‹èƒ½å¦ä¼¤å®³è‡ªèº«ï¼Ÿ",
      "authors": [
        "Heehwan Kim",
        "Sungjune Park",
        "Daeseon Choi"
      ],
      "abstract": "Large Language Models (LLMs) are generally equipped with guardrails to block the generation of harmful responses. However, existing defenses always assume that an external attacker crafts the harmful query, and the possibility of a model's own output becoming a new attack vector has not been sufficiently explored. In this study, we propose the Self-HarmLLM scenario, which uses a Mitigated Harmful Query (MHQ) generated by the same model as a new input. An MHQ is an ambiguous query whose original intent is preserved while its harmful nature is not directly exposed. We verified whether a jailbreak occurs when this MHQ is re-entered into a separate session of the same model. We conducted experiments on GPT-3.5-turbo, LLaMA3-8B-instruct, and DeepSeek-R1-Distill-Qwen-7B under Base, Zero-shot, and Few-shot conditions. The results showed up to 52% transformation success rate and up to 33% jailbreak success rate in the Zero-shot condition, and up to 65% transformation success rate and up to 41% jailbreak success rate in the Few-shot condition. By performing both prefix-based automated evaluation and human evaluation, we found that the automated evaluation consistently overestimated jailbreak success, with an average difference of 52%. This indicates that automated evaluation alone is not accurate for determining harmfulness. While this study is a toy-level study based on a limited query set and evaluators, it proves that our method can still be a valid attack scenario. These results suggest the need for a fundamental reconsideration of guardrail design and the establishment of a more robust evaluation methodology.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Self-HarmLLMåœºæ™¯ï¼Œæ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„è¾“å‡ºæ˜¯å¦å¯èƒ½æ¼”å˜ä¸ºé’ˆå¯¹å…¶è‡ªèº«çš„æ”»å‡»å‘é‡ã€‚ç ”ç©¶è€…å¼•å…¥äº†å‡è½»ä¼¤å®³æŸ¥è¯¢(Mitigated Harmful Query, MHQ)çš„æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ç§ä¿ç•™åŸå§‹æœ‰å®³æ„å›¾ä½†éšè—äº†ç›´æ¥ä¼¤å®³æ€§çš„æ¨¡ç³ŠæŸ¥è¯¢ã€‚é€šè¿‡åœ¨GPT-3.5-turboã€LLaMA3-8B-instructå’ŒDeepSeek-R1ç­‰æ¨¡å‹ä¸Šè¿›è¡Œå®éªŒï¼Œç ”ç©¶å‘ç°å½“MHQè¢«é‡æ–°è¾“å…¥åˆ°åŒä¸€æ¨¡å‹çš„ä¸åŒä¼šè¯æ—¶ï¼Œèƒ½å¤Ÿè¯±å‘è¶Šç‹±(Jailbreak)ç°è±¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºåœ¨Few-shotæ¡ä»¶ä¸‹ï¼ŒæŸ¥è¯¢è½¬æ¢æˆåŠŸç‡æœ€é«˜è¾¾65%ï¼Œè¶Šç‹±æˆåŠŸç‡æœ€é«˜è¾¾41%ã€‚æ­¤å¤–ï¼Œå¯¹æ¯”åˆ†ææ˜¾ç¤ºè‡ªåŠ¨è¯„ä¼°(Automated Evaluation)ä¼šæ˜¾è‘—é«˜ä¼°è¶Šç‹±æˆåŠŸç‡ï¼Œå¹³å‡åå·®è¾¾52%ï¼Œè¡¨æ˜ä»…é è‡ªåŠ¨è¯„ä¼°éš¾ä»¥å‡†ç¡®åˆ¤æ–­æœ‰å®³æ€§ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ç°æœ‰é˜²å¾¡æœºåˆ¶(Guardrails)çš„æ½œåœ¨æ¼æ´ï¼Œå¼ºè°ƒäº†é‡æ–°è®¾è®¡å®‰å…¨è¾¹ç•Œå’Œå»ºç«‹æ›´ç¨³å¥è¯„ä¼°æ–¹æ³•çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08597v1",
      "published_date": "2025-10-31 02:23:54 UTC",
      "updated_date": "2025-10-31 02:23:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:05.032418+00:00"
    },
    {
      "arxiv_id": "2511.05542v1",
      "title": "ConnectomeBench: Can LLMs Proofread the Connectome?",
      "title_zh": "ConnectomeBenchï¼šå¤§è¯­è¨€æ¨¡å‹èƒ½å¦èƒœä»»è¿æ¥ç»„æ ¡å¯¹ä»»åŠ¡ï¼Ÿ",
      "authors": [
        "Jeff Brown",
        "Andrew Kirjner",
        "Annika Vivekananthan",
        "Ed Boyden"
      ],
      "abstract": "Connectomics - the mapping of neural connections in an organism's brain - currently requires extraordinary human effort to proofread the data collected from imaging and machine-learning assisted segmentation. With the growing excitement around using AI agents to automate important scientific tasks, we explore whether current AI systems can perform multiple tasks necessary for data proofreading. We introduce ConnectomeBench, a multimodal benchmark evaluating large language model (LLM) capabilities in three critical proofreading tasks: segment type identification, split error correction, and merge error detection. Using expert annotated data from two large open-source datasets - a cubic millimeter of mouse visual cortex and the complete Drosophila brain - we evaluate proprietary multimodal LLMs including Claude 3.7/4 Sonnet, o4-mini, GPT-4.1, GPT-4o, as well as open source models like InternVL-3 and NVLM. Our results demonstrate that current models achieve surprisingly high performance in segment identification (52-82% balanced accuracy vs. 20-25% chance) and binary/multiple choice split error correction (75-85% accuracy vs. 50% chance) while generally struggling on merge error identification tasks. Overall, while the best models still lag behind expert performance, they demonstrate promising capabilities that could eventually enable them to augment and potentially replace human proofreading in connectomics. Project page: https://github.com/jffbrwn2/ConnectomeBench and Dataset https://huggingface.co/datasets/jeffbbrown2/ConnectomeBench/tree/main",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘è¿æ¥ç»„å­¦(Connectomics)ä¸­äººå·¥æ ¡å¯¹æ•°æ®çš„æé«˜æˆæœ¬ï¼Œæå‡ºäº†é¦–ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(LLMs)å¤„ç†ç›¸å…³ç§‘å­¦ä»»åŠ¡èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ConnectomeBenchã€‚è¯¥åŸºå‡†æ¶µç›–äº†segment type identificationã€split error correctionå’Œmerge error detectionä¸‰é¡¹å…³é”®ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨äº†æ¥è‡ªå°é¼ è§†è§‰çš®å±‚å’Œæœè‡å®Œæ•´å¤§è„‘çš„ä¸“å®¶æ ‡æ³¨æ•°æ®é›†ã€‚é€šè¿‡å¯¹Claudeã€GPT-4oã€InternVL-2åŠNVLMç­‰æ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨è¯†åˆ«ç‰‡æ®µç±»å‹å’Œçº æ­£åˆ†è£‚é”™è¯¯æ–¹é¢è¡¨ç°å‡ºæƒŠå–œçš„å‡†ç¡®æ€§ï¼Œä½†åœ¨æ£€æµ‹åˆå¹¶é”™è¯¯(merge error)æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚å°½ç®¡å½“å‰æœ€å…ˆè¿›æ¨¡å‹ä¸äººç±»ä¸“å®¶ç›¸æ¯”å°šæœ‰å·®è·ï¼Œä½†ç ”ç©¶ç»“æœè¯æ˜äº†LLMsåœ¨è¾…åŠ©æˆ–æœ€ç»ˆæ›¿ä»£äººå·¥è¿›è¡Œç¥ç»è¿æ¥å›¾è°±æ ¡å¯¹æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "To appear in NeurIPS 2025 Datasets and Benchmarks Track",
      "pdf_url": "https://arxiv.org/pdf/2511.05542v1",
      "published_date": "2025-10-31 02:20:38 UTC",
      "updated_date": "2025-10-31 02:20:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:07.638246+00:00"
    },
    {
      "arxiv_id": "2511.08596v1",
      "title": "What About the Scene with the Hitler Reference? HAUNT: A Framework to Probe LLMs' Self-consistency Via Adversarial Nudge",
      "title_zh": "é‚£ä¸ªæ¶‰åŠ Hitler å¼•ç”¨çš„åœºæ™¯å‘¢ï¼ŸHAUNTï¼šä¸€ç§é€šè¿‡å¯¹æŠ—æ€§è¯±å¯¼æ¢æµ‹å¤§è¯­è¨€æ¨¡å‹è‡ªä¸€è‡´æ€§çš„æ¡†æ¶",
      "authors": [
        "Arka Dutta",
        "Sujan Dutta",
        "Rijul Magu",
        "Soumyajit Datta",
        "Munmun De Choudhury",
        "Ashiqur R. KhudaBukhsh"
      ],
      "abstract": "Hallucinations pose a critical challenge to the real-world deployment of large language models (LLMs) in high-stakes domains. In this paper, we present a framework for stress testing factual fidelity in LLMs in the presence of adversarial nudge. Our framework consists of three steps. In the first step, we instruct the LLM to produce sets of truths and lies consistent with the closed domain in question. In the next step, we instruct the LLM to verify the same set of assertions as truths and lies consistent with the same closed domain. In the final step, we test the robustness of the LLM against the lies generated (and verified) by itself. Our extensive evaluation, conducted using five widely known proprietary LLMs across two closed domains of popular movies and novels, reveals a wide range of susceptibility to adversarial nudges: \\texttt{Claude} exhibits strong resilience, \\texttt{GPT} and \\texttt{Grok} demonstrate moderate resilience, while \\texttt{Gemini} and \\texttt{DeepSeek} show weak resilience. Considering that a large population is increasingly using LLMs for information seeking, our findings raise alarm.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HAUNTæ¡†æ¶ï¼Œæ—¨åœ¨å‹åŠ›æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¯¹æŠ—æ€§è¯±å¯¼(Adversarial Nudge)ä¸‹çš„äº‹å®å¿ å®åº¦(Factual Fidelity)ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼šé¦–å…ˆå¼•å¯¼æ¨¡å‹ç”Ÿæˆç‰¹å®šå°é—­é¢†åŸŸçš„çœŸå®ä¿¡æ¯ä¸è°è¨€ï¼Œéšåè®©æ¨¡å‹å¯¹è¿™äº›ä¿¡æ¯è¿›è¡ŒéªŒè¯ï¼Œæœ€åæµ‹è¯•æ¨¡å‹åœ¨é¢å¯¹è‡ªèº«ç”Ÿæˆå¹¶éªŒè¯è¿‡çš„è°è¨€æ—¶çš„é²æ£’æ€§ã€‚ç ”ç©¶åœ¨æµè¡Œç”µå½±å’Œå°è¯´ä¸¤ä¸ªé¢†åŸŸå¯¹äº”ç§ä¸»æµé—­æºæ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹å¯¹å¯¹æŠ—æ€§è¯±å¯¼çš„æ•æ„Ÿåº¦å·®å¼‚æ˜¾è‘—ï¼Œå…¶ä¸­Claudeè¡¨ç°å‡ºæå¼ºçš„éŸ§æ€§ï¼ŒGPTå’ŒGrokè¡¨ç°ä¸­ç­‰ï¼Œè€ŒGeminiå’ŒDeepSeekçš„éŸ§æ€§è¾ƒå¼±ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†LLMsåœ¨è‡ªæ´½æ€§(Self-consistency)æ–¹é¢çš„è„†å¼±æ€§ã€‚é‰´äºå¤§é‡ç”¨æˆ·æ­£ä¾èµ–LLMsè·å–ä¿¡æ¯ï¼Œè¯¥ç ”ç©¶ç»“æœä¸ºå¤§æ¨¡å‹åœ¨ç°å®ä¸–ç•Œé«˜é£é™©é¢†åŸŸçš„éƒ¨ç½²æå‡ºäº†è­¦ç¤ºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.08596v1",
      "published_date": "2025-10-31 02:02:39 UTC",
      "updated_date": "2025-10-31 02:02:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:27.732393+00:00"
    },
    {
      "arxiv_id": "2510.27102v1",
      "title": "Expressive Range Characterization of Open Text-to-Audio Models",
      "title_zh": "å¼€æºæ–‡æœ¬åˆ°éŸ³é¢‘æ¨¡å‹çš„è¡¨è¾¾èŒƒå›´è¡¨å¾",
      "authors": [
        "Jonathan Morse",
        "Azadeh Naderi",
        "Swen Gaudl",
        "Mark Cartwright",
        "Amy K. Hoover",
        "Mark J. Nelson"
      ],
      "abstract": "Text-to-audio models are a type of generative model that produces audio output in response to a given textual prompt. Although level generators and the properties of the functional content that they create (e.g., playability) dominate most discourse in procedurally generated content (PCG), games that emotionally resonate with players tend to weave together a range of creative and multimodal content (e.g., music, sounds, visuals, narrative tone), and multimodal models have begun seeing at least experimental use for this purpose. However, it remains unclear what exactly such models generate, and with what degree of variability and fidelity: audio is an extremely broad class of output for a generative system to target.\n  Within the PCG community, expressive range analysis (ERA) has been used as a quantitative way to characterize generators' output space, especially for level generators. This paper adapts ERA to text-to-audio models, making the analysis tractable by looking at the expressive range of outputs for specific, fixed prompts. Experiments are conducted by prompting the models with several standardized prompts derived from the Environmental Sound Classification (ESC-50) dataset. The resulting audio is analyzed along key acoustic dimensions (e.g., pitch, loudness, and timbre). More broadly, this paper offers a framework for ERA-based exploratory evaluation of generative audio models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡æœ¬è½¬éŸ³é¢‘ (Text-to-Audio) æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹å¤šæ ·æ€§å’Œä¿çœŸåº¦æ–¹é¢çš„ç‰¹å¾ï¼ŒæŒ‡å‡ºå°½ç®¡è¿™ç±»æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¸¸æˆåˆ›ä½œä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†å…¶è¾“å‡ºç©ºé—´çš„ç•Œé™ä»ä¸æ¸…æ™°ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å°†è¿‡ç¨‹å†…å®¹ç”Ÿæˆ (PCG) é¢†åŸŸå¸¸ç”¨çš„è¡¨è¾¾èŒƒå›´åˆ†æ (Expressive Range Analysis, ERA) æ–¹æ³•é€‚é…å¹¶å¼•å…¥åˆ°éŸ³é¢‘ç”Ÿæˆé¢†åŸŸï¼Œé€šè¿‡å¯¹å›ºå®šæç¤ºè¯ (Prompts) çš„è¾“å‡ºè¿›è¡Œå®šé‡åˆ»ç”»ï¼Œä½¿å¤æ‚çš„ç”Ÿæˆç©ºé—´åˆ†æå˜å¾—å¯è¡Œã€‚ç ”ç©¶åˆ©ç”¨ ESC-50 æ•°æ®é›†çš„æ ‡å‡†æç¤ºè¯è¿›è¡Œå®éªŒï¼Œä»éŸ³é«˜ (Pitch)ã€å“åº¦ (Loudness) å’ŒéŸ³è‰² (Timbre) ç­‰æ ¸å¿ƒå£°å­¦ç»´åº¦å¯¹ç”Ÿæˆçš„éŸ³é¢‘è¿›è¡Œäº†å¤šç»´è¯„ä¼°ã€‚è¯¥å·¥ä½œä¸ä»…æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨å£°å­¦ç‰¹å¾ä¸Šçš„è¡¨ç°å·®å¼‚ï¼Œè¿˜ä¸ºç”Ÿæˆå¼éŸ³é¢‘æ¨¡å‹çš„æ¢ç´¢æ€§è¯„ä»·æä¾›äº†ä¸€å¥—æ ‡å‡†åŒ–çš„ ERA æ¡†æ¶ã€‚è¿™ä¸ºæœªæ¥åœ¨å¤šæ¨¡æ€ç³»ç»Ÿä¸­æ›´ç²¾å‡†åœ°åº”ç”¨å’Œä¼˜åŒ–éŸ³é¢‘ç”ŸæˆæŠ€æœ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.27102v1",
      "published_date": "2025-10-31 01:55:41 UTC",
      "updated_date": "2025-10-31 01:55:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:13.357347+00:00"
    },
    {
      "arxiv_id": "2510.27094v1",
      "title": "CombiGraph-Vis: A Curated Multimodal Olympiad Benchmark for Discrete Mathematical Reasoning",
      "title_zh": "CombiGraph-Visï¼šé¢å‘ç¦»æ•£æ•°å­¦æ¨ç†çš„ç²¾é€‰å¤šæ¨¡æ€å¥¥æ—åŒ¹å…‹åŸºå‡†",
      "authors": [
        "Hamed Mahdavi",
        "Pouria Mahdavinia",
        "Alireza Farhadi",
        "Pegah Mohammadipour",
        "Samira Malek",
        "Majid Daliri",
        "Pedram Mohammadipour",
        "Alireza Hashemi",
        "Amir Khasahmadi",
        "Vasant Honavar"
      ],
      "abstract": "State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based Olympiad problems to solving most of the IMO 2025 problems, with leading systems reportedly handling 5 of 6 problems. Given this progress, we assess how well these models can grade proofs: detecting errors, judging their severity, and assigning fair scores beyond binary correctness. We study proof-analysis capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we grade on a 1-4 scale with detailed error annotations, and on MathArena solution sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models can reliably flag incorrect (including subtly incorrect) solutions but exhibit calibration gaps in how partial credit is assigned. To address this, we introduce agentic workflows that extract and analyze reference solutions and automatically derive problem-specific rubrics for a multi-step grading process. We instantiate and compare different design choices for the grading workflows, and evaluate their trade-offs. Across our annotated corpus and MathArena, our proposed workflows achieve higher agreement with human grades and more consistent handling of partial credit across metrics. We release all code, data, and prompts/logs to facilitate future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¥¥æ—åŒ¹å…‹æ•°å­¦ç«èµ› (Olympiad) è¯æ˜é¢˜è¯„åˆ†æ–¹é¢çš„èƒ½åŠ›ï¼Œæ¶‰åŠé”™è¯¯æ£€æµ‹ã€ä¸¥é‡ç¨‹åº¦è¯„ä¼°åŠåˆ†æ•°çš„å…¬å¹³åˆ†é…ã€‚é€šè¿‡å¯¹ Gemini 2.5 Pro ç”Ÿæˆçš„è§£æ³•å’Œ MathArena æ•°æ®é›†è¿›è¡Œåˆ†æï¼Œç ”ç©¶å‘ç°æ¨¡å‹è™½èƒ½å¯é è¯†åˆ«é”™è¯¯ï¼Œä½†åœ¨éƒ¨åˆ†ç»™åˆ† (partial credit) çš„æ ¡å‡†ä¸Šä»å­˜åœ¨å·®è·ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ™ºèƒ½ä½“å·¥ä½œæµ (agentic workflows) çš„å¤šæ­¥éª¤è¯„åˆ†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½è‡ªåŠ¨æå–å‚è€ƒç­”æ¡ˆå¹¶ç”Ÿæˆé’ˆå¯¹æ€§è¯„åˆ†æ ‡å‡† (rubrics)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å·¥ä½œæµåœ¨ä¸äººå·¥è¯„åˆ†çš„ä¸€è‡´æ€§åŠè¯„åˆ†ç¨³å®šæ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚è¯¥ç ”ç©¶é€šè¿‡ CombiGraph-Vis åŸºå‡†åŠç›¸å…³å¼€æºèµ„æºï¼Œä¸ºç¦»æ•£æ•°å­¦æ¨ç†çš„è‡ªåŠ¨è¯„ä¼°ç ”ç©¶æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Code/data: https://github.com/ref-grader/ref-grader, https://huggingface.co/datasets/combviz/inoi",
      "pdf_url": "https://arxiv.org/pdf/2510.27094v1",
      "published_date": "2025-10-31 01:31:58 UTC",
      "updated_date": "2025-10-31 01:31:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:26.537895+00:00"
    },
    {
      "arxiv_id": "2510.27091v1",
      "title": "QiNN-QJ: A Quantum-inspired Neural Network with Quantum Jump for Multimodal Sentiment Analysis",
      "title_zh": "QiNN-QJï¼šèåˆé‡å­è·ƒè¿çš„é‡å­å¯å‘å¼ç¥ç»ç½‘ç»œå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æ",
      "authors": [
        "Yiwei Chen",
        "Kehuan Yan",
        "Yu Pan",
        "Daoyi Dong"
      ],
      "abstract": "Quantum theory provides non-classical principles, such as superposition and entanglement, that inspires promising paradigms in machine learning. However, most existing quantum-inspired fusion models rely solely on unitary or unitary-like transformations to generate quantum entanglement. While theoretically expressive, such approaches often suffer from training instability and limited generalizability. In this work, we propose a Quantum-inspired Neural Network with Quantum Jump (QiNN-QJ) for multimodal entanglement modelling. Each modality is firstly encoded as a quantum pure state, after which a differentiable module simulating the QJ operator transforms the separable product state into the entangled representation. By jointly learning Hamiltonian and Lindblad operators, QiNN-QJ generates controllable cross-modal entanglement among modalities with dissipative dynamics, where structured stochasticity and steady-state attractor properties serve to stabilize training and constrain entanglement shaping. The resulting entangled states are projected onto trainable measurement vectors to produce predictions. In addition to achieving superior performance over the state-of-the-art models on benchmark datasets, including CMU-MOSI, CMU-MOSEI, and CH-SIMS, QiNN-QJ facilitates enhanced post-hoc interpretability through von-Neumann entanglement entropy. This work establishes a principled framework for entangled multimodal fusion and paves the way for quantum-inspired approaches in modelling complex cross-modal correlations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­ä¼ ç»Ÿé‡å­å¯å‘æ¨¡å‹å› è¿‡åº¦ä¾èµ–é…‰å˜æ¢è€Œå¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šåŠæ³›åŒ–æ€§å—é™é—®é¢˜ï¼Œæå‡ºäº† QiNN-QJï¼ˆQuantum-inspired Neural Network with Quantum Jumpï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å„æ¨¡æ€é¦–å…ˆç¼–ç ä¸ºé‡å­çº¯æ€ (pure state)ï¼Œå¹¶åˆ©ç”¨å¯å¾®åˆ†æ¨¡å—æ¨¡æ‹Ÿ QJ ç®—å­å°†åˆ†ç¦»çš„ä¹˜ç§¯æ€è½¬åŒ–ä¸ºçº ç¼ è¡¨ç¤ºã€‚é€šè¿‡è”åˆå­¦ä¹  Hamiltonian å’Œ Lindblad ç®—å­ï¼ŒQiNN-QJ åœ¨è€—æ•£åŠ¨åŠ›å­¦ (dissipative dynamics) ä¸‹ç”Ÿæˆå¯æ§çš„è·¨æ¨¡æ€çº ç¼ ï¼Œå¹¶åˆ©ç”¨ç»“æ„åŒ–éšæœºæ€§å’Œç¨³æ€å¸å¼•å­ç‰¹æ€§æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ CMU-MOSIã€CMU-MOSEI å’Œ CH-SIMS ç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒQiNN-QJ é€šè¿‡ von-Neumann çº ç¼ ç†µ (entanglement entropy) å¢å¼ºäº†äº‹åå¯è§£é‡Šæ€§ï¼Œä¸ºå¤æ‚è·¨æ¨¡æ€ç›¸å…³æ€§çš„å»ºæ¨¡æä¾›äº†è§„èŒƒåŒ–çš„é‡å­å¯å‘å¼æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27091v1",
      "published_date": "2025-10-31 01:25:55 UTC",
      "updated_date": "2025-10-31 01:25:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:21.431613+00:00"
    },
    {
      "arxiv_id": "2510.27080v1",
      "title": "Adapting Large Language Models to Emerging Cybersecurity using Retrieval Augmented Generation",
      "title_zh": "åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆä½¿å¤§è¯­è¨€æ¨¡å‹é€‚é…æ–°å…´ç½‘ç»œå®‰å…¨",
      "authors": [
        "Arnabh Borah",
        "Md Tanvirul Alam",
        "Nidhi Rastogi"
      ],
      "abstract": "Security applications are increasingly relying on large language models (LLMs) for cyber threat detection; however, their opaque reasoning often limits trust, particularly in decisions that require domain-specific cybersecurity knowledge. Because security threats evolve rapidly, LLMs must not only recall historical incidents but also adapt to emerging vulnerabilities and attack patterns. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in general LLM applications, but its potential for cybersecurity remains underexplored. In this work, we introduce a RAG-based framework designed to contextualize cybersecurity data and enhance LLM accuracy in knowledge retention and temporal reasoning. Using external datasets and the Llama-3-8B-Instruct model, we evaluate baseline RAG, an optimized hybrid retrieval approach, and conduct a comparative analysis across multiple performance metrics. Our findings highlight the promise of hybrid retrieval in strengthening the adaptability and reliability of LLMs for cybersecurity tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç½‘ç»œå®‰å…¨åº”ç”¨ä¸­å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å¿«é€Ÿæ¼”å˜çš„å¨èƒæ—¶å­˜åœ¨çš„æ¨ç†ä¸é€æ˜ã€é¢†åŸŸçŸ¥è¯†ç¼ºå¤±ä»¥åŠéš¾ä»¥é€‚åº”æ–°å…´æ”»å‡»æ¨¡å¼ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)çš„ç³»ç»Ÿæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆå¤–éƒ¨æ•°æ®é›†å¯¹ç½‘ç»œå®‰å…¨æ•°æ®è¿›è¡Œæƒ…å¢ƒåŒ–å¤„ç†ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨çŸ¥è¯†ä¿æŒå’Œæ—¶é—´æ¨ç†(temporal reasoning)æ–¹é¢çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¿‡ç¨‹ä¸­ä½¿ç”¨äº†Llama-3-8B-Instructä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶å¯¹åŸºçº¿RAGä¸ä¼˜åŒ–çš„æ··åˆæ£€ç´¢(hybrid retrieval)æ–¹æ³•è¿›è¡Œäº†å¤šç»´åº¦çš„å¯¹æ¯”åˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œæ··åˆæ£€ç´¢æŠ€æœ¯èƒ½æ˜¾è‘—å¢å¼ºLLMsåœ¨åº”å¯¹æ–°å…´æ¼æ´æ—¶çš„é€‚åº”æ€§å’Œå¯é æ€§ã€‚å®éªŒç»“æœéªŒè¯äº†åˆ©ç”¨RAGæŠ€æœ¯å¼¥è¡¥æ¨¡å‹é¢†åŸŸçŸ¥è¯†é¸¿æ²Ÿå¹¶å¼ºåŒ–ç½‘ç»œå®‰å…¨ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›çš„å¯è¡Œæ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.27080v1",
      "published_date": "2025-10-31 00:59:53 UTC",
      "updated_date": "2025-10-31 00:59:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:22.230155+00:00"
    },
    {
      "arxiv_id": "2511.00114v1",
      "title": "End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning",
      "title_zh": "èåˆç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç«¯åˆ°ç«¯è‡ªä¸»è¶…å£°æ‰«ææ¡†æ¶",
      "authors": [
        "Hanae Elmekki",
        "Amanda Spilkin",
        "Ehsan Zakeri",
        "Antonela Mariel Zanuttini",
        "Ahmed Alagha",
        "Hani Sami",
        "Jamal Bentahar",
        "Lyes Kadem",
        "Wen-Fang Xie",
        "Philippe Pibarot",
        "Rabeb Mizouni",
        "Hadi Otrok",
        "Azzam Mourad",
        "Sami Muhaidat"
      ],
      "abstract": "Cardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¿ƒè„è¶…å£°(US)æ‰«æå¯¹æ“ä½œå‘˜ä¾èµ–æ€§å¼ºä¸”ç¼ºä¹å¯é‡å¤æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªé›†æˆç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è‡ªä¸»ä¸”é«˜æ•ˆçš„å¿ƒè„æˆåƒã€‚è¯¥æ¡†æ¶ç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼šä¸€æ˜¯ç»“åˆäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GANs)ä¸å˜åˆ†è‡ªç¼–ç å™¨(VAEs)çš„æ¡ä»¶ç”Ÿæˆæ¨¡æ‹Ÿå™¨ï¼Œç”¨äºæ¨¡æ‹ŸçœŸå®çš„å¿ƒè„è¶…å£°ç¯å¢ƒå¹¶äº§ç”Ÿé€¼çœŸçš„åŠ¨ä½œæ¡ä»¶å›¾åƒï¼›äºŒæ˜¯åˆ©ç”¨è¯¥æ¨¡æ‹Ÿå™¨è¿›è¡Œè®­ç»ƒçš„DRLæ¨¡å—ï¼Œè´Ÿè´£å­¦ä¹ ç²¾ç¡®çš„è‡ªä¸»æ‰«æç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ä¸“å®¶éªŒè¯çš„æ¨¡å‹å®ç°å›¾åƒåˆ†ç±»ä¸è´¨é‡è¯„ä¼°ï¼Œå¹¶åŒæ­¥å‘å¸ƒäº†å…¬å¼€æ•°æ®é›†ä»¥è§£å†³ç°æœ‰ç ”ç©¶ä¾èµ–ç§æœ‰æ•°æ®ä¸”éš¾ä»¥å¤ç°çš„ç—›ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVAE-GANåœ¨ç”Ÿæˆè´¨é‡ä¸Šä¼˜äºå¤šç§GANå˜ä½“ï¼Œä¸”DRLç³»ç»Ÿåœ¨ä¸åŒé…ç½®ä¸‹å‡å±•ç°å‡ºè‰¯å¥½çš„æ‰«ææ€§èƒ½ï¼Œä¸ºè‡ªä¸»è¶…å£°è¾…åŠ©è¯Šæ–­æä¾›äº†å¯é çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.00114v1",
      "published_date": "2025-10-31 00:54:49 UTC",
      "updated_date": "2025-10-31 00:54:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:28.428527+00:00"
    },
    {
      "arxiv_id": "2510.27063v1",
      "title": "Towards a Measure of Algorithm Similarity",
      "title_zh": "è¿ˆå‘ç®—æ³•ç›¸ä¼¼æ€§åº¦é‡",
      "authors": [
        "Shairoz Sohail",
        "Taher Ali"
      ],
      "abstract": "Given two algorithms for the same problem, can we determine whether they are meaningfully different? In full generality, the question is uncomputable, and empirically it is muddied by competing notions of similarity. Yet, in many applications (such as clone detection or program synthesis) a pragmatic and consistent similarity metric is necessary. We review existing equivalence and similarity notions and introduce EMOC: An Evaluation-Memory-Operations-Complexity framework that embeds algorithm implementations into a feature space suitable for downstream tasks. We compile PACD, a curated dataset of verified Python implementations across three problems, and show that EMOC features support clustering and classification of algorithm types, detection of near-duplicates, and quantification of diversity in LLM-generated programs. Code, data, and utilities for computing EMOC embeddings are released to facilitate reproducibility and future work on algorithm similarity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•è¡¡é‡è§£å†³åŒä¸€é—®é¢˜çš„ä¸¤ä¸ªç®—æ³•ä¹‹é—´æ˜¯å¦å­˜åœ¨å®è´¨æ€§å·®å¼‚ï¼Œå¹¶æŒ‡å‡ºåœ¨å…‹éš†æ£€æµ‹(clone detection)å’Œç¨‹åºåˆæˆ(program synthesis)ç­‰åº”ç”¨ä¸­ï¼Œå»ºç«‹åŠ¡å®ä¸”ä¸€è‡´çš„ç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†EMOCï¼ˆEvaluation-Memory-Operations-Complexityï¼‰æ¡†æ¶ï¼Œé€šè¿‡å°†ç®—æ³•å®ç°åµŒå…¥åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­ï¼Œå®ç°äº†å¯¹ç®—æ³•è¡Œä¸ºçš„é‡åŒ–è¡¨è¾¾ã€‚ç ”ç©¶äººå‘˜åŒæ­¥æ„å»ºäº†åŒ…å«å¤šç§Pythonå®ç°çš„PACDæ•°æ®é›†ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨ç®—æ³•èšç±»ã€åˆ†ç±»ä»¥åŠæ£€æµ‹è¿‘é‡å¤ä»£ç (near-duplicates)æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒEMOCè¿˜è¢«è¯æ˜èƒ½æœ‰æ•ˆç”¨äºé‡åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆç¨‹åºçš„å¤šæ ·æ€§ï¼Œä¸ºè¯„ä¼°ä»£ç ç”Ÿæˆè´¨é‡æä¾›äº†æ–°çš„åº¦é‡ç»´åº¦ã€‚è¯¥ç ”ç©¶ç›®å‰å·²å¼€æºäº†ç›¸å…³ä»£ç ã€æ•°æ®å’Œè®¡ç®—EMOCåµŒå…¥(embeddings)çš„å·¥å…·ï¼Œä¸ºç®—æ³•ç›¸ä¼¼æ€§é¢†åŸŸçš„åç»­ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IT",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, many figures and images",
      "pdf_url": "https://arxiv.org/pdf/2510.27063v1",
      "published_date": "2025-10-31 00:20:54 UTC",
      "updated_date": "2025-10-31 00:20:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:43.329289+00:00"
    },
    {
      "arxiv_id": "2510.27062v1",
      "title": "Consistency Training Helps Stop Sycophancy and Jailbreaks",
      "title_zh": "ä¸€è‡´æ€§è®­ç»ƒåŠ©åŠ›éåˆ¶é™„å’Œä¸è¶Šç‹±è¡Œä¸º",
      "authors": [
        "Alex Irpan",
        "Alexander Matt Turner",
        "Mark Kurzeja",
        "David K. Elson",
        "Rohin Shah"
      ],
      "abstract": "An LLM's factuality and refusal training can be compromised by simple changes to a prompt. Models often adopt user beliefs (sycophancy) or satisfy inappropriate requests which are wrapped within special text (jailbreaking). We explore \\emph{consistency training}, a self-supervised paradigm that teaches a model to be invariant to certain irrelevant cues in the prompt. Instead of teaching the model what exact response to give on a particular prompt, we aim to teach the model to behave identically across prompt data augmentations (like adding leading questions or jailbreak text). We try enforcing this invariance in two ways: over the model's external outputs (\\emph{Bias-augmented Consistency Training} (BCT) from Chua et al. [2025]) and over its internal activations (\\emph{Activation Consistency Training} (ACT), a method we introduce). Both methods reduce Gemini 2.5 Flash's susceptibility to irrelevant cues. Because consistency training uses responses from the model itself as training data, it avoids issues that arise from stale training data, such as degrading model capabilities or enforcing outdated response guidelines. While BCT and ACT reduce sycophancy equally well, BCT does better at jailbreak reduction. We think that BCT can simplify training pipelines by removing reliance on static datasets. We argue that some alignment problems are better viewed not in terms of optimal responses, but rather as consistency issues.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Consistency Training è¿™ç§è‡ªç›‘ç£èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ç‰¹å®šæç¤ºè¯å˜åŒ–æ—¶å®¹æ˜“å‡ºç°çš„ Sycophancyï¼ˆè¿åˆç”¨æˆ·åå¥½ï¼‰å’Œ Jailbreaksï¼ˆè¶Šç‹±ï¼‰é—®é¢˜ã€‚ç ”ç©¶è€…æå‡ºäº†é€šè¿‡æ¨¡å‹å¤–éƒ¨è¾“å‡ºå¢å¼ºçš„ Bias-augmented Consistency Training (BCT) ä»¥åŠé’ˆå¯¹å†…éƒ¨æ¿€æ´»çš„ Activation Consistency Training (ACT) ä¸¤ç§æ–¹æ³•ï¼Œæ—¨åœ¨æ•™å¯¼æ¨¡å‹å¯¹æç¤ºè¯ä¸­çš„æ— å…³è¯±å¯¼çº¿ç´¢ä¿æŒä¸å˜æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•å‡èƒ½æ˜¾è‘—é™ä½ Gemini 2.5 Flash å¯¹åè§æç¤ºçš„æ•æ„Ÿåº¦ï¼Œä¸” BCT åœ¨é˜²å¾¡è¶Šç‹±æ”»å‡»æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚ç”±äºè¯¥æ–¹æ³•åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„å“åº”è¿›è¡Œè®­ç»ƒï¼Œå®ƒæœ‰æ•ˆé¿å…äº†ä¼ ç»Ÿé™æ€æ•°æ®é›†å¸¦æ¥çš„æ•°æ®é™ˆæ—§å’Œèƒ½åŠ›é€€åŒ–é—®é¢˜ã€‚è¯¥ç ”ç©¶å¼ºè°ƒå°†éƒ¨åˆ†å¯¹é½æŒ‘æˆ˜è§†ä¸ºä¸€è‡´æ€§é—®é¢˜ï¼Œä¸ºç®€åŒ–æ¨¡å‹è®­ç»ƒæµç¨‹å’Œæå‡æ¨¡å‹ç¨³å¥æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.27062v1",
      "published_date": "2025-10-31 00:19:13 UTC",
      "updated_date": "2025-10-31 00:19:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T05:27:44.732470+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 146,
  "processed_papers_count": 146,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T05:28:36.722334+00:00"
}
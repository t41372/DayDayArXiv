{
  "date": "2024-10-29",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-10-29 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 模型优化、多模态学习、强化学习和医疗应用等领域，亮点包括 LLM 在图算法和机器人决策中的潜力，以及 AI 安全和高效训练方法；值得一提的是，Sergey Levine 等知名学者参与的机器人相关论文（如第136篇）带来了实际应用价值。\n\n下面，我将挑选并简要讨论今天更具影响力和话题度的论文，先从 AI 和 LLM 相关的高影响力文章入手，再聊医疗和机器人领域，最后快速掠过其他较常规的论文。每个条目包括论文标题（中文 + 英文）和核心贡献。\n\n### AI 和 LLM 相关论文\n- **Are Large-Language Models Graph Algorithmic Reasoners?（大型语言模型是否能进行图算法推理？）**：这篇论文引入 MAGMA 基准，评估 LLM 在图算法（如 BFS 和 Dijkstra）上的多步推理能力，发现 LLM 在结构化问题上存在挑战，但通过高级提示可提升性能，主要贡献在于揭示 LLM 的图推理局限性并提供新基准。\n  \n- **BENCHAGENTS: Automated Benchmark Creation with Agent Interaction（BENCHAGENTS：使用代理交互的自动基准创建）**：论文提出一个 LLM 代理框架，用于自动生成评估复杂能力的基准，应用于文本生成任务；关键发现是它能确保数据质量并揭示 AI 模型的失败模式，如规划和约束满足问题。\n\n- **ML Research Benchmark（ML 研究基准）**：作者 Matthew Kenney 构建了 7 个竞赛级任务的基准，评估 AI 代理在机器学习研究中的能力；主要贡献是发现当前 AI 代理在非平凡迭代上表现不足，但 Claude-3.5 Sonnet 在规划模型上表现出色。\n\n- **Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents（Auto-Intent：用于 LLM Web 代理的自动意图发现和自探索）**：论文开发了 Auto-Intent 方法，无需微调 LLM 即可适应 Web 导航任务，通过意图预测提升决策能力；核心发现是它显著提高了 GPT 和 Llama 模型在跨基准任务中的性能。\n\n- **Attention Speaks Volumes: Localizing and Mitigating Bias in Language Models（注意力揭示偏差：定位和缓解语言模型中的偏差）**：论文提出 ATLAS 技术，通过分析注意力分数定位 LLM 偏差并通过缩放缓解；主要贡献是实验证明偏差集中在模型后期层，并在多个数据集上降低了偏差，同时保持下游性能。\n\n- **Do Large Language Models Align with Core Mental Health Counseling Competencies?（大型语言模型是否与核心心理健康咨询能力一致？）**：作者团队包括 Michael L. Birnbaum 和 Munmun De Choudhury，使用 CounselingBench 基准评估 LLM 在心理咨询中的表现；关键发现是前沿模型在诊断上优秀，但情感推理较弱，强调了 LLM 在医疗应用的局限性。\n\n### 医疗和生物相关论文\n- **Peri-AIIMS: Perioperative Artificial Intelligence Driven Integrated Modeling of Surgeries using Anesthetic, Physical and Cognitive Statuses for Predicting Hospital Outcomes（Peri-AIIMS：使用麻醉、身体和认知状态的围手术期 AI 驱动整合建模预测医院结果）**：论文使用机器学习模型结合认知测试预测手术结果；主要贡献是证明认知特征能提升预测准确性，在 6 种手术类型上表现出色。\n\n- **CurateGPT: A flexible language-model assisted biocuration tool（CurateGPT：一种灵活的语言模型辅助生物数据整理工具）**：作者包括 Peter N. Robinson 和 Christopher J. Mungall，提出 CurateGPT 用于生物数据整理；核心发现是它能加速 FAIR 数据处理，同时提供可追溯的证据链。\n\n- **RNA-GPT: Multimodal Generative System for RNA Sequence Understanding（RNA-GPT：用于 RNA 序列理解的多模态生成系统）**：论文构建了 RNA-GPT 系统，使用 LLM 处理 RNA 数据；主要贡献是创建 RNA-QA 数据集，并展示了它在序列理解中的高效性。\n\n### 机器人和强化学习论文\n- **Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets（机器人预训练机器人：从大规模机器人数据集获取以操作为中心的机器人表示）**：作者包括 Huazhe Xu，提出 MCR 框架，使用多模态数据提升机器人操作学习；关键发现是它在模拟和真实任务中显著提高了性能。\n\n- **MARCO: Multi-Agent Real-time Chat Orchestration（MARCO：多代理实时聊天编排）**：论文设计了多代理框架用于任务自动化，作者包括 Bill Dolan；主要贡献是引入反射机制，提高了 LLM 在复杂任务中的鲁棒性。\n\n### 其他快速掠过论文\n其他论文涉及图像处理、图神经网络和语音生成等领域，但许多较为常规或专注于特定技术细节，这里仅简要提及几篇：\n- **VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration（VL-Cache：用于视觉语言模型推理加速的稀疏性和模态感知 KV 缓存压缩）**：优化了 VLM 的缓存机制，减少了内存占用并加速推理。\n- **Unpicking Data at the Seams: Understanding Disentanglement in VAEs（解开数据接缝：理解 VAEs 中的解缠结）**：探讨了 VAEs 在数据解缠结中的几何属性。\n- **Image2Struct: Benchmarking Structure Extraction for Vision-Language Models（Image2Struct：为视觉语言模型基准结构提取）**：引入新基准评估 VLM 的图像结构提取能力。\n\n总体而言，今天的论文突出了 AI 模型在实际应用中的潜力，尤其是在 LLM 优化和医疗领域的创新，但也暴露了如偏差和泛化挑战的问题。未来几天，继续关注这些领域的进展！",
  "papers": [
    {
      "arxiv_id": "2411.00840v1",
      "title": "Peri-AIIMS: Perioperative Artificial Intelligence Driven Integrated Modeling of Surgeries using Anesthetic, Physical and Cognitive Statuses for Predicting Hospital Outcomes",
      "title_zh": "翻译失败",
      "authors": [
        "Sabyasachi Bandyopadhyay",
        "Jiaqing Zhang",
        "Ronald L. Ison",
        "David J. Libon",
        "Patrick Tighe",
        "Catherine Price",
        "Parisa Rashidi"
      ],
      "abstract": "The association between preoperative cognitive status and surgical outcomes\nis a critical, yet scarcely explored area of research. Linking intraoperative\ndata with postoperative outcomes is a promising and low-cost way of evaluating\nlong-term impacts of surgical interventions. In this study, we evaluated how\npreoperative cognitive status as measured by the clock drawing test contributed\nto predicting length of hospital stay, hospital charges, average pain\nexperienced during follow-up, and 1-year mortality over and above\nintraoperative variables, demographics, preoperative physical status and\ncomorbidities. We expanded our analysis to 6 specific surgical groups where\nsufficient data was available for cross-validation. The clock drawing images\nwere represented by 10 constructional features discovered by a semi-supervised\ndeep learning algorithm, previously validated to differentiate between dementia\nand non-dementia patients. Different machine learning models were trained to\nclassify postoperative outcomes in hold-out test sets. The models were compared\nto their relative performance, time complexity, and interpretability. Shapley\nAdditive Explanations (SHAP) analysis was used to find the most predictive\nfeatures for classifying different outcomes in different surgical contexts.\nRelative classification performances achieved by different feature sets showed\nthat the perioperative cognitive dataset which included clock drawing features\nin addition to intraoperative variables, demographics, and comorbidities served\nas the best dataset for 12 of 18 possible surgery-outcome combinations...",
      "tldr_zh": "本文提出Peri-AIIMS框架，利用人工智能整合术前认知状态（如clock drawing test）、术中变量、人口统计学和共病数据，来预测手术结果，包括住院时间、医院费用、术后疼痛和1年死亡率。研究通过半监督深度学习算法提取钟形绘图图像的10个构造特征，并训练不同machine learning模型，在6个特定手术组上进行交叉验证和SHAP分析，以评估模型性能和可解释性。结果显示，包含认知特征的围手术期数据集在18种手术-结果组合中，有12种表现出最佳分类性能，为探索手术干预长期影响提供了一种低成本且有效的预测方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00840v1",
      "published_date": "2024-10-29 23:42:51 UTC",
      "updated_date": "2024-10-29 23:42:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:12:26.065854"
    },
    {
      "arxiv_id": "2410.22597v1",
      "title": "Are Large-Language Models Graph Algorithmic Reasoners?",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander K Taylor",
        "Anthony Cuturrufo",
        "Vishal Yathish",
        "Mingyu Derek Ma",
        "Wei Wang"
      ],
      "abstract": "We seek to address a core challenge facing current Large Language Models\n(LLMs). LLMs have demonstrated superior performance in many tasks, yet continue\nto struggle with reasoning problems on explicit graphs that require multiple\nsteps. To address this gap, we introduce a novel benchmark designed to evaluate\nLLM performance on classical algorithmic reasoning tasks on explicit graphs.\nOur benchmark encompasses five fundamental algorithms: Breadth-First Search\n(BFS) and Depth-First Search (DFS) for connectivity, Dijkstra's algorithm and\nFloyd-Warshall algorithm for all nodes shortest path, and Prim's Minimum\nSpanning Tree (MST-Prim's) algorithm. Through extensive experimentation, we\nassess the capabilities of state-of-the-art LLMs in executing these algorithms\nstep-by-step and systematically evaluate their performance at each stage. Our\nfindings highlight the persistent challenges LLMs face in this domain and\nunderscore the necessity for advanced prompting techniques and algorithmic\ninstruction to enhance their graph reasoning abilities. This work presents\nMAGMA, the first comprehensive benchmark focused on LLMs completing classical\ngraph algorithms, and provides a critical step toward understanding and\nimproving their structured problem-solving skills.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)在处理多步图算法推理任务时的局限性，并引入了首个全面基准MAGRA，用于评估LLMs在经典图算法上的表现。基准包括五种核心算法：Breadth-First Search (BFS)、Depth-First Search (DFS)、Dijkstra's algorithm、Floyd-Warshall algorithm 和 Prim's Minimum Spanning Tree (MST-Prim's)，通过步-by-step实验系统地测试LLMs的执行能力。研究发现，LLMs在这些任务上仍存在显著挑战，强调了采用先进提示技术和算法指令的必要性，以提升其图推理和结构化问题解决技能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 13 Figures",
      "pdf_url": "http://arxiv.org/pdf/2410.22597v1",
      "published_date": "2024-10-29 23:28:37 UTC",
      "updated_date": "2024-10-29 23:28:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:10:45.856834"
    },
    {
      "arxiv_id": "2410.22591v2",
      "title": "FGCE: Feasible Group Counterfactual Explanations for Auditing Fairness",
      "title_zh": "FGCE：用于审计公平性的可行群组反事实解释",
      "authors": [
        "Christos Fragkathoulas",
        "Vasiliki Papanikou",
        "Evaggelia Pitoura",
        "Evimaria Terzi"
      ],
      "abstract": "This paper introduces the first graph-based framework for generating group\ncounterfactual explanations to audit model fairness, a crucial aspect of\ntrustworthy machine learning. Counterfactual explanations are instrumental in\nunderstanding and mitigating unfairness by revealing how inputs should change\nto achieve a desired outcome. Our framework, named Feasible Group\nCounterfactual Explanations (FGCEs), captures real-world feasibility\nconstraints and constructs subgroups with similar counterfactuals, setting it\napart from existing methods. It also addresses key trade-offs in counterfactual\ngeneration, including the balance between the number of counterfactuals, their\nassociated costs, and the breadth of coverage achieved. To evaluate these\ntrade-offs and assess fairness, we propose measures tailored to group\ncounterfactual generation. Our experimental results on benchmark datasets\ndemonstrate the effectiveness of our approach in managing feasibility\nconstraints and trade-offs, as well as the potential of our proposed metrics in\nidentifying and quantifying fairness issues.",
      "tldr_zh": "这篇论文提出了 FGCE（Feasible Group Counterfactual Explanations），第一个基于图的框架，用于生成群组反事实解释以审计机器学习模型的公平性。FGCE 考虑了现实可行性约束，构建具有相似反事实的子群，并处理反事实生成中的关键权衡，如数量、相关成本和覆盖范围。作者还引入了针对群组反事实的度量，并在基准数据集上的实验中证明了该方法的有效性，能够识别和量化公平问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22591v2",
      "published_date": "2024-10-29 23:10:01 UTC",
      "updated_date": "2024-11-15 12:02:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:10:57.988918"
    },
    {
      "arxiv_id": "2411.00839v1",
      "title": "CausAdv: A Causal-based Framework for Detecting Adversarial Examples",
      "title_zh": "翻译失败",
      "authors": [
        "Hichem Debbi"
      ],
      "abstract": "Deep learning has led to tremendous success in many real-world applications\nof computer vision, thanks to sophisticated architectures such as Convolutional\nneural networks (CNNs). However, CNNs have been shown to be vulnerable to\ncrafted adversarial perturbations in inputs. These inputs appear almost\nindistinguishable from natural images, yet they are incorrectly classified by\nCNN architectures. This vulnerability of adversarial examples has led\nresearchers to focus on enhancing the robustness of deep learning models in\ngeneral, and CNNs in particular, by creating defense and detection methods to\ndistinguish adversarials inputs from natural ones. In this paper, we address\nthe adversarial robustness of CNNs through causal reasoning.\n  We propose CausAdv: a causal framework for detecting adversarial examples\nbased on counterfactual reasoning. CausAdv learns causal and non-causal\nfeatures of every input, and quantifies the counterfactual information (CI) of\nevery filter of the last convolutional layer. Then we perform statistical\nanalysis on the filters CI of every sample, whether clan or adversarials, to\ndemonstrate how adversarial examples indeed exhibit different CI distributions\ncompared to clean samples. Our results show that causal reasoning enhances the\nprocess of adversarials detection without the need to train a separate\ndetector. In addition, we illustrate the efficiency of causal explanations as a\nhelpful detection technique through visualizing the causal features. The\nresults can be reproduced using the code available in the repository:\nhttps://github.com/HichemDebbi/CausAdv.",
      "tldr_zh": "本论文提出 CausAdv，一个基于因果推理的框架，用于检测深度学习模型如 CNN 在计算机视觉中的对抗样本问题。框架通过学习输入的因果和非因果特征，并量化最后一个卷积层的每个过滤器的反事实信息 (CI)，来分析干净样本和对抗样本的 CI 分布差异。结果显示，对抗样本的 CI 分布显著不同，从而无需训练单独的检测器即可提升检测准确性；此外，通过可视化因果特征，进一步增强了检测的可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ME",
        "stat.ML",
        "I.2.10"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00839v1",
      "published_date": "2024-10-29 22:57:48 UTC",
      "updated_date": "2024-10-29 22:57:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:12:34.695978"
    },
    {
      "arxiv_id": "2410.22584v1",
      "title": "BENCHAGENTS: Automated Benchmark Creation with Agent Interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Natasha Butt",
        "Varun Chandrasekaran",
        "Neel Joshi",
        "Besmira Nushi",
        "Vidhisha Balachandran"
      ],
      "abstract": "Evaluations are limited by benchmark availability. As models evolve, there is\na need to create benchmarks that can measure progress on new generative\ncapabilities. However, creating new benchmarks through human annotations is\nslow and expensive, restricting comprehensive evaluations for any capability.\nWe introduce BENCHAGENTS, a framework that methodically leverages large\nlanguage models (LLMs) to automate benchmark creation for complex capabilities\nwhile inherently ensuring data and metric quality. BENCHAGENTS decomposes the\nbenchmark creation process into planning, generation, data verification, and\nevaluation, each of which is executed by an LLM agent. These agents interact\nwith each other and utilize human-in-the-loop feedback from benchmark\ndevelopers to explicitly improve and flexibly control data diversity and\nquality. We use BENCHAGENTS to create benchmarks to evaluate capabilities\nrelated to planning and constraint satisfaction during text generation. We then\nuse these benchmarks to study seven state-of-the-art models and extract new\ninsights on common failure modes and model differences.",
      "tldr_zh": "该研究引入 BENCHAGENTS 框架，利用 LLMs (大型语言模型) 自动创建基准，以解决模型评估中基准可用性的限制，并确保数据和指标质量。框架将基准创建过程分解为规划、生成、数据验证和评估，由 LLM 代理执行，这些代理相互互动并融入人类反馈来提升数据多样性和准确性。研究应用 BENCHAGENTS 创建的基准评估文本生成中的规划和约束满足能力，并在七个最先进模型上识别出常见失败模式和模型差异，提供新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22584v1",
      "published_date": "2024-10-29 22:56:18 UTC",
      "updated_date": "2024-10-29 22:56:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:11:20.755265"
    },
    {
      "arxiv_id": "2410.22578v1",
      "title": "Energy-Aware Multi-Agent Reinforcement Learning for Collaborative Execution in Mission-Oriented Drone Networks",
      "title_zh": "能源感知多智能体强化学习用于任务导向无人机网络中的协作执行",
      "authors": [
        "Ying Li",
        "Changling Li",
        "Jiyao Chen",
        "Christine Roinou"
      ],
      "abstract": "Mission-oriented drone networks have been widely used for structural\ninspection, disaster monitoring, border surveillance, etc. Due to the limited\nbattery capacity of drones, mission execution strategy impacts network\nperformance and mission completion. However, collaborative execution is a\nchallenging problem for drones in such a dynamic environment as it also\ninvolves efficient trajectory design. We leverage multi-agent reinforcement\nlearning (MARL) to manage the challenge in this study, letting each drone learn\nto collaboratively execute tasks and plan trajectories based on its current\nstatus and environment. Simulation results show that the proposed collaborative\nexecution model can successfully complete the mission at least 80% of the time,\nregardless of task locations and lengths, and can even achieve a 100% success\nrate when the task density is not way too sparse. To the best of our knowledge,\nour work is one of the pioneer studies on leveraging MARL on collaborative\nexecution for mission-oriented drone networks; the unique value of this work\nlies in drone battery level driving our model design.",
      "tldr_zh": "这篇论文提出了一种能量感知的多智能体强化学习(MARL)方法，用于任务导向无人机网络的协作执行，旨在解决无人机电池容量限制带来的挑战，包括动态环境下的任务协作和轨迹设计。方法让每个无人机根据自身状态和环境自主学习协作任务和规划轨迹。模拟结果显示，该模型在不同任务位置和长度下成功完成任务的比例至少为80%，而在任务密度适中时可达100%。该研究是MARL在无人机网络协作执行领域的先驱工作，独特之处在于以无人机电池水平为核心驱动模型设计。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.NI",
      "comment": "2022 International Conference on Computer Communications and Networks",
      "pdf_url": "http://arxiv.org/pdf/2410.22578v1",
      "published_date": "2024-10-29 22:43:26 UTC",
      "updated_date": "2024-10-29 22:43:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:11:33.059780"
    },
    {
      "arxiv_id": "2410.22559v4",
      "title": "Unpicking Data at the Seams: Understanding Disentanglement in VAEs",
      "title_zh": "剖析数据的接缝：理解 VAEs 中的解缠结",
      "authors": [
        "Carl Allen"
      ],
      "abstract": "Disentanglement, or identifying statistically independent factors of the\ndata, is relevant to much of machine learning, from controlled data generation\nand robust classification to efficient encoding and improving our understanding\nof the data itself. Disentanglement arises in several generative paradigms\nincluding Variational Autoencoders (VAEs), Generative Adversarial Networks and\ndiffusion models. Recent progress has been made in understanding\ndisentanglement in VAEs, where a choice of diagonal posterior covariance\nmatrices is shown to promote mutual orthogonality between columns of the\ndecoder's Jacobian. We build on this to show how such orthogonality, a\ngeometric property, translates to disentanglement, a statistical property,\nfurthering our understanding of how a VAE identifies independent components of,\nor disentangles, the data.",
      "tldr_zh": "这篇论文探讨了在变分自编码器（VAEs）中理解disentanglement，即识别数据中的统计独立因素，这对机器学习任务如控制数据生成、鲁棒分类和数据理解至关重要。作者基于现有研究，分析了选择对角后验协方差矩阵如何促进解码器Jacobian矩阵列之间的相互正交性，并阐释了这种几何属性（正交性）如何转化为统计属性（disentanglement）。最终，该工作加深了对VAEs如何识别和解缠结数据独立成分的理解，为生成模型的改进提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22559v4",
      "published_date": "2024-10-29 21:54:18 UTC",
      "updated_date": "2025-02-06 15:35:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:11:44.548889"
    },
    {
      "arxiv_id": "2410.22553v1",
      "title": "ML Research Benchmark",
      "title_zh": "ML 研究基准测试",
      "authors": [
        "Matthew Kenney"
      ],
      "abstract": "Artificial intelligence agents are increasingly capable of performing complex\ntasks across various domains. As these agents advance, there is a growing need\nto accurately measure and benchmark their capabilities, particularly in\naccelerating AI research and development. Current benchmarks focus on general\nmachine learning tasks, but lack comprehensive evaluation methods for assessing\nAI agents' abilities in tackling research-level problems and competition-level\nchallenges in the field of AI. We present the ML Research Benchmark (MLRB),\ncomprising 7 competition-level tasks derived from recent machine learning\nconference tracks. These tasks span activities typically undertaken by AI\nresearchers, including model training efficiency, pretraining on limited data,\ndomain specific fine-tuning, and model compression. This paper introduces a\nnovel benchmark and evaluates it using agent scaffolds powered by frontier\nmodels, including Claude-3 and GPT-4o. The results indicate that the Claude-3.5\nSonnet agent performs best across our benchmark, excelling in planning and\ndeveloping machine learning models. However, both tested agents struggled to\nperform non-trivial research iterations. We observed significant performance\nvariations across tasks, highlighting the complexity of AI development and the\nchallenges in creating versatile agent scaffolds. While current AI agents can\nsuccessfully navigate complex instructions and produce baseline results, they\nfall short of the capabilities required for advanced AI research. The ML\nResearch Benchmark provides a valuable framework for assessing and comparing AI\nagents on tasks mirroring real-world AI research challenges.",
      "tldr_zh": "这篇论文引入了ML Research Benchmark (MLRB)，一个由7个竞赛级任务组成的基准，用于评估AI代理在处理AI研究级问题的能力，这些任务基于最近的机器学习会议轨道，包括模型训练效率、有限数据预训练、领域特定微调和模型压缩。研究者使用前沿模型如Claude-3和GPT-4o驱动的代理支架进行评估，结果显示Claude-3.5 Sonnet代理在规划和开发机器学习模型方面表现出色，但所有代理在进行非平凡的研究迭代时表现不佳，并显示出任务间的显著性能差异。MLRB为评估和比较AI代理在真实AI研究挑战中的表现提供了一个宝贵的框架，突显了当前AI代理在高级研究方面的局限性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22553v1",
      "published_date": "2024-10-29 21:38:42 UTC",
      "updated_date": "2024-10-29 21:38:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:11:57.845828"
    },
    {
      "arxiv_id": "2410.22552v1",
      "title": "Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents",
      "title_zh": "Auto-Intent: 用于大型语言模型网络代理的自动意图发现和自我探索",
      "authors": [
        "Jaekyeom Kim",
        "Dong-Ki Kim",
        "Lajanugen Logeswaran",
        "Sungryull Sohn",
        "Honglak Lee"
      ],
      "abstract": "In this paper, we introduce Auto-Intent, a method to adapt a pre-trained\nlarge language model (LLM) as an agent for a target domain without direct\nfine-tuning, where we empirically focus on web navigation tasks. Our approach\nfirst discovers the underlying intents from target domain demonstrations\nunsupervisedly, in a highly compact form (up to three words). With the\nextracted intents, we train our intent predictor to predict the next intent\ngiven the agent's past observations and actions. In particular, we propose a\nself-exploration approach where top-k probable intent predictions are provided\nas a hint to the pre-trained LLM agent, which leads to enhanced decision-making\ncapabilities. Auto-Intent substantially improves the performance of GPT-{3.5,\n4} and Llama-3.1-{70B, 405B} agents on the large-scale real-website navigation\nbenchmarks from Mind2Web and online navigation tasks from WebArena with its\ncross-benchmark generalization from Mind2Web.",
      "tldr_zh": "本文提出 Auto-Intent 方法，用于无需直接微调的预训练 Large Language Model (LLM) 适应目标领域代理，焦点在于网页导航任务。该方法首先通过无监督方式从目标演示中发现紧凑意图（最多三个单词），然后训练意图预测器来预测代理的下一个意图，并引入自探索机制，提供 top-k 可能意图作为提示以增强决策能力。实验结果显示，Auto-Intent 显著提升了 GPT-3.5、GPT-4 和 Llama-3.1-70B/405B 代理在 Mind2Web 和 WebArena 等大规模真实网站导航基准上的性能，并展示了从 Mind2Web 到其他任务的跨基准泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2410.22552v1",
      "published_date": "2024-10-29 21:37:04 UTC",
      "updated_date": "2024-10-29 21:37:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:12:08.842517"
    },
    {
      "arxiv_id": "2410.22526v1",
      "title": "From Silos to Systems: Process-Oriented Hazard Analysis for AI Systems",
      "title_zh": "从孤岛到系统：AI 系统的过程导向风险分析",
      "authors": [
        "Shalaleh Rismani",
        "Roel Dobbe",
        "AJung Moon"
      ],
      "abstract": "To effectively address potential harms from AI systems, it is essential to\nidentify and mitigate system-level hazards. Current analysis approaches focus\non individual components of an AI system, like training data or models, in\nisolation, overlooking hazards from component interactions or how they are\nsituated within a company's development process. To this end, we draw from the\nestablished field of system safety, which considers safety as an emergent\nproperty of the entire system, not just its components. In this work, we\ntranslate System Theoretic Process Analysis (STPA) - a recognized system safety\nframework - for analyzing AI operation and development processes. We focus on\nsystems that rely on machine learning algorithms and conducted STPA on three\ncase studies involving linear regression, reinforcement learning, and\ntransformer-based generative models. Our analysis explored how STPA's control\nand system-theoretic perspectives apply to AI systems and whether unique AI\ntraits - such as model opacity, capability uncertainty, and output complexity -\nnecessitate significant modifications to the framework. We find that the key\nconcepts and steps of conducting an STPA readily apply, albeit with a few\nadaptations tailored for AI systems. We present the Process-oriented Hazard\nAnalysis for AI Systems (PHASE) as a guideline that adapts STPA concepts for\nAI, making STPA-based hazard analysis more accessible. PHASE enables four key\naffordances for analysts responsible for managing AI system harms: 1) detection\nof hazards at the systems level, including those from accumulation of disparate\nissues; 2) explicit acknowledgment of social factors contributing to\nexperiences of algorithmic harms; 3) creation of traceable accountability\nchains between harms and those who can mitigate the harm; and 4) ongoing\nmonitoring and mitigation of new hazards.",
      "tldr_zh": "这篇论文指出，现有的 AI 系统危害分析方法仅关注单个组件（如训练数据或模型），忽略了组件交互和开发过程引发的系统级风险，因此借鉴系统安全领域的 System Theoretic Process Analysis (STPA) 来全面评估 AI 系统。通过对线性回归、强化学习和基于 Transformer 的生成模型的三例案例研究，论文探索了 STPA 在 AI 中的适用性，并提出 Process-oriented Hazard Analysis for AI Systems (PHASE) 框架，以适应 AI 的独特特性，如模型不透明性、能力不确定性和输出复杂性。PHASE 提供四个关键优势：检测系统级危害（包括累积问题）、承认社会因素对算法危害的影响、创建可追踪的责任链，以及持续监控和缓解新风险，从而提升 AI 系统的安全性和可管理性。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22526v1",
      "published_date": "2024-10-29 20:43:18 UTC",
      "updated_date": "2024-10-29 20:43:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:12:21.890434"
    },
    {
      "arxiv_id": "2410.22517v1",
      "title": "Attention Speaks Volumes: Localizing and Mitigating Bias in Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Rishabh Adiga",
        "Besmira Nushi",
        "Varun Chandrasekaran"
      ],
      "abstract": "We explore the internal mechanisms of how bias emerges in large language\nmodels (LLMs) when provided with ambiguous comparative prompts: inputs that\ncompare or enforce choosing between two or more entities without providing\nclear context for preference. Most approaches for bias mitigation focus on\neither post-hoc analysis or data augmentation. However, these are transient\nsolutions, without addressing the root cause: the model itself. Numerous prior\nworks show the influence of the attention module towards steering generations.\nWe believe that analyzing attention is also crucial for understanding bias, as\nit provides insight into how the LLM distributes its focus across different\nentities and how this contributes to biased decisions. To this end, we first\nintroduce a metric to quantify the LLM's preference for one entity over\nanother. We then propose $\\texttt{ATLAS}$ (Attention-based Targeted Layer\nAnalysis and Scaling), a technique to localize bias to specific layers of the\nLLM by analyzing attention scores and then reduce bias by scaling attention in\nthese biased layers. To evaluate our method, we conduct experiments across 3\ndatasets (BBQ, Crows-Pairs, and WinoGender) using $\\texttt{GPT-2 XL}$ (1.5B),\n$\\texttt{GPT-J}$ (6B), $\\texttt{LLaMA-2}$ (7B) and $\\texttt{LLaMA-3}$ (8B). Our\nexperiments demonstrate that bias is concentrated in the later layers,\ntypically around the last third. We also show how $\\texttt{ATLAS}$ effectively\nmitigates bias through targeted interventions without compromising downstream\nperformance and an average increase of only 0.82% in perplexity when the\nintervention is applied. We see an average improvement of 0.28 points in the\nbias score across all the datasets.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在处理模糊比较提示时的偏见产生机制，强调注意力模块的作用，并通过分析注意力分数来理解偏见分布。研究者引入了一个量化LLMs偏好的度量，并提出ATLAS（Attention-based Targeted Layer Analysis and Scaling）技术，用于定位偏见至特定层（如模型后期层，通常是最后三分之一），并通过缩放注意力来缓解偏见。在多个数据集（BBQ、Crows-Pairs和WinoGender）及模型（GPT-2 XL、GPT-J、LLaMA-2和LLaMA-3）上的实验显示，ATLAS方法平均提高了0.28点的偏见分数，同时仅增加了0.82%的perplexity，且不影响下游性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22517v1",
      "published_date": "2024-10-29 20:15:56 UTC",
      "updated_date": "2024-10-29 20:15:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:12:46.241624"
    },
    {
      "arxiv_id": "2410.23317v1",
      "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration",
      "title_zh": "翻译失败",
      "authors": [
        "Dezhan Tu",
        "Danylo Vashchilenko",
        "Yuzhe Lu",
        "Panpan Xu"
      ],
      "abstract": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
      "tldr_zh": "这篇论文提出了 VL-Cache，一种针对视觉语言模型(VLMs)的 KV cache 压缩方法，以解决存储和访问长视觉上下文带来的推理加速挑战。VL-Cache 通过层自适应稀疏感知缓存预算分配和模态感知标记评分策略，区分视觉与文本标记的独特稀疏模式，从而有效减少 KV cache 大小而不牺牲准确性。实验结果显示，仅保留 10% 的 KV cache 即可在多个基准数据集上达到与完整缓存相当的准确性，同时加速端到端生成 100 个标记的延迟高达 2.33 倍、解码速度高达 7.08 倍，并将 GPU 内存占用减少 90%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.DC",
        "cs.PF"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.23317v1",
      "published_date": "2024-10-29 20:04:34 UTC",
      "updated_date": "2024-10-29 20:04:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:13:00.418058"
    },
    {
      "arxiv_id": "2411.00046v1",
      "title": "CurateGPT: A flexible language-model assisted biocuration tool",
      "title_zh": "翻译失败",
      "authors": [
        "Harry Caufield",
        "Carlo Kroll",
        "Shawn T O'Neil",
        "Justin T Reese",
        "Marcin P Joachimiak",
        "Harshad Hegde",
        "Nomi L Harris",
        "Madan Krishnamurthy",
        "James A McLaughlin",
        "Damian Smedley",
        "Melissa A Haendel",
        "Peter N Robinson",
        "Christopher J Mungall"
      ],
      "abstract": "Effective data-driven biomedical discovery requires data curation: a\ntime-consuming process of finding, organizing, distilling, integrating,\ninterpreting, annotating, and validating diverse information into a structured\nform suitable for databases and knowledge bases. Accurate and efficient\ncuration of these digital assets is critical to ensuring that they are FAIR,\ntrustworthy, and sustainable. Unfortunately, expert curators face significant\ntime and resource constraints. The rapid pace of new information being\npublished daily is exceeding their capacity for curation. Generative AI,\nexemplified by instruction-tuned large language models (LLMs), has opened up\nnew possibilities for assisting human-driven curation. The design philosophy of\nagents combines the emerging abilities of generative AI with more precise\nmethods. A curator's tasks can be aided by agents for performing reasoning,\nsearching ontologies, and integrating knowledge across external sources, all\nefforts otherwise requiring extensive manual effort. Our LLM-driven annotation\ntool, CurateGPT, melds the power of generative AI together with trusted\nknowledge bases and literature sources. CurateGPT streamlines the curation\nprocess, enhancing collaboration and efficiency in common workflows. Compared\nto direct interaction with an LLM, CurateGPT's agents enable access to\ninformation beyond that in the LLM's training data and they provide direct\nlinks to the data supporting each claim. This helps curators, researchers, and\nengineers scale up curation efforts to keep pace with the ever-increasing\nvolume of scientific data.",
      "tldr_zh": "生物医学数据整理（biocuration）是一个耗时过程，包括查找、组织、提炼和验证信息，以构建可信赖的数据库，但专家面临资源限制和信息爆炸的挑战。论文提出 CurateGPT，一种基于指令调整的大型语言模型（LLMs）的灵活工具，通过代理（agents）辅助进行推理、搜索本体（ontologies）和整合外部知识来源，提升整理效率和准确性。与直接使用 LLM 相比，CurateGPT 能访问模型训练数据之外的信息，并提供支持声明的数据链接，帮助整理者、研究者和工程师扩展工作规模，以应对日益增长的科学数据量。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "q-bio.QM"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00046v1",
      "published_date": "2024-10-29 20:00:04 UTC",
      "updated_date": "2024-10-29 20:00:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:13:10.268047"
    },
    {
      "arxiv_id": "2410.22492v2",
      "title": "RealCQA-V2 : Visual Premise Proving A Manual COT Dataset for Charts",
      "title_zh": "翻译失败",
      "authors": [
        "Saleem Ahmed",
        "Ranga Setlur",
        "Venu Govindaraju"
      ],
      "abstract": "We introduce Visual Premise Proving (VPP), a novel task tailored to refine\nthe process of chart question answering by deconstructing it into a series of\nlogical premises. Each of these premises represents an essential step in\ncomprehending a chart's content and deriving logical conclusions, thereby\nproviding a granular look at a model's reasoning abilities. This approach\nrepresents a departure from conventional accuracy-based evaluation methods,\nemphasizing the model's ability to sequentially validate each premise and\nideally mimic human analytical processes. A model adept at reasoning is\nexpected to demonstrate proficiency in both data retrieval and the structural\nunderstanding of charts, suggesting a synergy between these competencies.\nHowever, in our zero-shot study using the sophisticated MATCHA model on a\nscientific chart question answering dataset, an intriguing pattern emerged. The\nmodel showcased superior performance in chart reasoning (27\\%) over chart\nstructure (19\\%) and data retrieval (14\\%). This performance gap suggests that\nmodels might more readily generalize reasoning capabilities across datasets,\nbenefiting from consistent mathematical and linguistic semantics, even when\nchallenged by changes in the visual domain that complicate structure\ncomprehension and data retrieval. Furthermore, the efficacy of using accuracy\nof binary QA for evaluating chart reasoning comes into question if models can\ndeduce correct answers without parsing chart data or structure. VPP highlights\nthe importance of integrating reasoning with visual comprehension to enhance\nmodel performance in chart analysis, pushing for a balanced approach in\nevaluating visual data interpretation capabilities.",
      "tldr_zh": "本研究引入Visual Premise Proving (VPP)，一个新任务，将图表问答（CQA）分解为一系列逻辑前提步骤，以更细粒度地评估模型的推理能力，并模拟人类分析过程。论文提出RealCQA-V2数据集，该数据集基于手动Chain-of-Thought (COT)方法，强调模型在数据检索、图表结构理解和推理方面的协同作用。实验结果显示，在零样本测试中使用MATCHA模型时，模型在图表推理方面表现最佳（27%），但在数据检索（14%）和结构理解（19%）上较弱，这表明模型可能依赖泛化语义而忽略视觉细节。VPP质疑传统二元QA准确性的有效性，并呼吁整合推理与视觉理解，以提升图表分析模型的全面性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under Review : Code and Data will be made public soon",
      "pdf_url": "http://arxiv.org/pdf/2410.22492v2",
      "published_date": "2024-10-29 19:32:53 UTC",
      "updated_date": "2024-11-10 00:54:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:13:22.708868"
    },
    {
      "arxiv_id": "2411.00045v1",
      "title": "A Novel Psychometrics-Based Approach to Developing Professional Competency Benchmark for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Elena Kardanova",
        "Alina Ivanova",
        "Ksenia Tarasova",
        "Taras Pashchenko",
        "Aleksei Tikhoniuk",
        "Elen Yusupova",
        "Anatoly Kasprzhak",
        "Yaroslav Kuzminov",
        "Ekaterina Kruchinskaia",
        "Irina Brun"
      ],
      "abstract": "The era of large language models (LLM) raises questions not only about how to\ntrain models, but also about how to evaluate them. Despite numerous existing\nbenchmarks, insufficient attention is often given to creating assessments that\ntest LLMs in a valid and reliable manner. To address this challenge, we\naccommodate the Evidence-centered design (ECD) methodology and propose a\ncomprehensive approach to benchmark development based on rigorous psychometric\nprinciples. In this paper, we have made the first attempt to illustrate this\napproach by creating a new benchmark in the field of pedagogy and education,\nhighlighting the limitations of existing benchmark development approach and\ntaking into account the development of LLMs. We conclude that a new approach to\nbenchmarking is required to match the growing complexity of AI applications in\nthe educational context. We construct a novel benchmark guided by the Bloom's\ntaxonomy and rigorously designed by a consortium of education experts trained\nin test development. Thus the current benchmark provides an academically robust\nand practical assessment tool tailored for LLMs, rather than human\nparticipants. Tested empirically on the GPT model in the Russian language, it\nevaluates model performance across varied task complexities, revealing critical\ngaps in current LLM capabilities. Our results indicate that while generative AI\ntools hold significant promise for education - potentially supporting tasks\nsuch as personalized tutoring, real-time feedback, and multilingual learning -\ntheir reliability as autonomous teachers' assistants right now remain rather\nlimited, particularly in tasks requiring deeper cognitive engagement.",
      "tldr_zh": "这篇论文提出了一种基于心理测量学的新方法，用于开发大型语言模型 (LLM) 的专业能力基准，采用 Evidence-centered design (ECD) 框架，以解决现有基准在有效性和可靠性方面的不足。研究者创建了一个教育领域的创新基准，基于 Bloom's taxonomy，由教育专家组成的团队严格设计，针对 LLM 而非人类参与者进行评估。在对 GPT 模型的俄语任务测试中，结果揭示了 LLM 在高认知复杂任务中的关键局限性，尽管其在个性化辅导和多语学习等方面具有潜力，但目前作为自主教师助手的可靠性仍有限。整体而言，该方法强调了为 AI 教育应用开发更先进基准的必要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "36 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.00045v1",
      "published_date": "2024-10-29 19:32:43 UTC",
      "updated_date": "2024-10-29 19:32:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:13:33.916432"
    },
    {
      "arxiv_id": "2410.22488v1",
      "title": "Privacy-Preserving Dynamic Assortment Selection",
      "title_zh": "隐私保护的动态产品组合选择",
      "authors": [
        "Young Hyun Cho",
        "Will Wei Sun"
      ],
      "abstract": "With the growing demand for personalized assortment recommendations, concerns\nover data privacy have intensified, highlighting the urgent need for effective\nprivacy-preserving strategies. This paper presents a novel framework for\nprivacy-preserving dynamic assortment selection using the multinomial logit\n(MNL) bandits model. Our approach employs a perturbed upper confidence bound\nmethod, integrating calibrated noise into user utility estimates to balance\nbetween exploration and exploitation while ensuring robust privacy protection.\nWe rigorously prove that our policy satisfies Joint Differential Privacy (JDP),\nwhich better suits dynamic environments than traditional differential privacy,\neffectively mitigating inference attack risks. This analysis is built upon a\nnovel objective perturbation technique tailored for MNL bandits, which is also\nof independent interest. Theoretically, we derive a near-optimal regret bound\nof $\\tilde{O}(\\sqrt{T})$ for our policy and explicitly quantify how privacy\nprotection impacts regret. Through extensive simulations and an application to\nthe Expedia hotel dataset, we demonstrate substantial performance enhancements\nover the benchmark method.",
      "tldr_zh": "这篇论文提出了一种隐私保护的动态 assortment 选择框架，基于 Multinomial Logit (MNL) bandits 模型，以应对个性化推荐中的数据隐私问题。该框架采用 perturbed upper confidence bound 方法，通过整合校准噪声到用户效用估计中，实现探索与利用的平衡，同时确保 Joint Differential Privacy (JDP)，以更好地适应动态环境并缓解推理攻击风险。论文还开发了针对 MNL bandits 的新 objective perturbation 技术，并理论上证明了策略的近优 regret bound 为 \\(\\tilde{O}(\\sqrt{T})\\)，量化了隐私保护对 regret 的影响。通过模拟实验和 Expedia 酒店数据集的应用，该方法展示了比基准方法显著的性能提升。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22488v1",
      "published_date": "2024-10-29 19:28:01 UTC",
      "updated_date": "2024-10-29 19:28:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:15:47.604922"
    },
    {
      "arxiv_id": "2410.22480v1",
      "title": "Scaling LLM Inference with Optimized Sample Compute Allocation",
      "title_zh": "通过",
      "authors": [
        "Kexun Zhang",
        "Shang Zhou",
        "Danqing Wang",
        "William Yang Wang",
        "Lei Li"
      ],
      "abstract": "Sampling is a basic operation in many inference-time algorithms of large\nlanguage models (LLMs). To scale up inference efficiently with a limited\ncompute, it is crucial to find an optimal allocation for sample compute\nbudgets: Which sampling configurations (model, temperature, language, etc.) do\nwe use? How many samples do we generate in each configuration? We formulate\nthese choices as a learning problem and propose OSCA, an algorithm that\nOptimizes Sample Compute Allocation by finding an optimal mix of different\ninference configurations. Our experiments show that with our learned mixed\nallocation, we can achieve accuracy better than the best single configuration\nwith 128x less compute on code generation and 25x less compute on 4 reasoning\ntasks. OSCA is also shown to be effective in agentic workflows beyond\nsingle-turn tasks, achieving a better accuracy on SWE-Bench with 3x less\ncompute than the default configuration. Our code and generations are released\nat https://github.com/LeiLiLab/OSCA.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)推理中的采样操作，提出OSCA算法，通过将采样配置（如模型、温度和语言）和样本数量的分配转化为学习问题，实现计算资源的优化。OSCA通过寻找最佳的混合推理配置，帮助在有限计算预算下高效扩展推理过程。实验结果显示，OSCA在代码生成任务上比最佳单一配置节省128倍计算资源，同时在4个推理任务上节省25倍资源；在代理工作流如SWE-Bench上，实现3倍计算资源节省并提升准确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22480v1",
      "published_date": "2024-10-29 19:17:55 UTC",
      "updated_date": "2024-10-29 19:17:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:13:57.772171"
    },
    {
      "arxiv_id": "2410.22475v1",
      "title": "Ethical Statistical Practice and Ethical AI",
      "title_zh": "伦理统计实践与伦理人工智能",
      "authors": [
        "Rochelle E. Tractenberg"
      ],
      "abstract": "Artificial Intelligence (AI) is a field that utilizes computing and often,\ndata and statistics, intensively together to solve problems or make\npredictions. AI has been evolving with literally unbelievable speed over the\npast few years, and this has led to an increase in social, cultural,\nindustrial, scientific, and governmental concerns about the ethical development\nand use of AI systems worldwide. The ASA has issued a statement on ethical\nstatistical practice and AI (ASA, 2024), which echoes similar statements from\nother groups. Here we discuss the support for ethical statistical practice and\nethical AI that has been established in long-standing human rights law and\nethical practice standards for computing and statistics. There are multiple\nsources of support for ethical statistical practice and ethical AI deriving\nfrom these source documents, which are critical for strengthening the\noperationalization of the \"Statement on Ethical AI for Statistics\nPractitioners\". These resources are explicated for interested readers to\nutilize to guide their development and use of AI in, and through, their\nstatistical practice.",
      "tldr_zh": "该论文探讨了人工智能（AI）的快速发展及其引发的社会伦理问题，特别是AI在统计实践中的伦理挑战。美国统计协会（ASA）发布了关于伦理统计实践和AI的声明，与其他组织类似。论文分析了人权法、计算和统计伦理标准中对伦理统计实践和伦理AI的支持来源，这些资源有助于加强“统计从业者伦理AI声明”的操作化。最终，论文为统计从业者提供指导，帮助他们在开发和使用AI时确保伦理合规。",
      "categories": [
        "stat.OT",
        "cs.AI",
        "cs.CY",
        "00, 62, 62.A01, 62.A99, 62.P99, 97.K8D",
        "A.0; A.m; K.7; K.4"
      ],
      "primary_category": "stat.OT",
      "comment": "10 pages; Preprint of submission to Proceedings of JSM 2024 Portland,\n  OR",
      "pdf_url": "http://arxiv.org/pdf/2410.22475v1",
      "published_date": "2024-10-29 19:09:34 UTC",
      "updated_date": "2024-10-29 19:09:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:14:09.468925"
    },
    {
      "arxiv_id": "2411.00838v1",
      "title": "Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaqi Wu",
        "Simin Chen",
        "Zehua Wang",
        "Wei Chen",
        "Zijian Tian",
        "F. Richard Yu",
        "Victor C. M. Leung"
      ],
      "abstract": "As the volume of image data grows, data-oriented cloud computing in Internet\nof Video Things (IoVT) systems encounters latency issues. Task-oriented edge\ncomputing addresses this by shifting data analysis to the edge. However,\nlimited computational power of edge devices poses challenges for executing\nvisual tasks. Existing methods struggle to balance high model performance with\nlow resource consumption; lightweight neural networks often underperform, while\ndevice-specific models designed by Neural Architecture Search (NAS) fail to\nadapt to heterogeneous devices. For these issues, we propose a novel co-design\nframework to optimize neural network architecture and deployment strategies\nduring inference for high-throughput. Specifically, it implements a dynamic\nmodel structure based on re-parameterization, coupled with a Roofline-based\nmodel partitioning strategy to enhance the computational performance of edge\ndevices. We also employ a multi-objective co-optimization approach to balance\nthroughput and accuracy. Additionally, we derive mathematical consistency and\nconvergence of partitioned models. Experimental results demonstrate significant\nimprovements in throughput (12.05\\% on MNIST, 18.83\\% on ImageNet) and superior\nclassification accuracy compared to baseline algorithms. Our method\nconsistently achieves stable performance across different devices, underscoring\nits adaptability. Simulated experiments further confirm its efficacy in\nhigh-accuracy, real-time detection for small objects in IoVT systems.",
      "tldr_zh": "该论文针对 Internet of Video Things (IoVT) 系统中的实时视觉推理问题，提出一个神经网络和边缘部署的联合设计框架，以解决边缘设备计算能力有限和模型性能资源消耗平衡的挑战。该框架采用基于 re-parameterization 的动态模型结构、Roofline-based 模型分区策略，以及多目标联合优化方法，来提升推理吞吐量并保持准确性，同时证明了分区模型的数学一致性和收敛性。实验结果显示，该框架在 MNIST 上吞吐量提升 12.05%、在 ImageNet 上提升 18.83%，分类准确性优于基线算法，并在异构设备上表现出稳定适应性，适用于 IoVT 系统中小对象的实时高精度检测。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00838v1",
      "published_date": "2024-10-29 19:02:54 UTC",
      "updated_date": "2024-10-29 19:02:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:14:24.887021"
    },
    {
      "arxiv_id": "2410.22459v1",
      "title": "Predicting Future Actions of Reinforcement Learning Agents",
      "title_zh": "预测强化学习代理的未来行动",
      "authors": [
        "Stephen Chung",
        "Scott Niekum",
        "David Krueger"
      ],
      "abstract": "As reinforcement learning agents become increasingly deployed in real-world\nscenarios, predicting future agent actions and events during deployment is\nimportant for facilitating better human-agent interaction and preventing\ncatastrophic outcomes. This paper experimentally evaluates and compares the\neffectiveness of future action and event prediction for three types of RL\nagents: explicitly planning, implicitly planning, and non-planning. We employ\ntwo approaches: the inner state approach, which involves predicting based on\nthe inner computations of the agents (e.g., plans or neuron activations), and a\nsimulation-based approach, which involves unrolling the agent in a learned\nworld model. Our results show that the plans of explicitly planning agents are\nsignificantly more informative for prediction than the neuron activations of\nthe other types. Furthermore, using internal plans proves more robust to model\nquality compared to simulation-based approaches when predicting actions, while\nthe results for event prediction are more mixed. These findings highlight the\nbenefits of leveraging inner states and simulations to predict future agent\nactions and events, thereby improving interaction and safety in real-world\ndeployments.",
      "tldr_zh": "这篇论文探讨了预测强化学习（RL）代理未来行动和事件的有效性，以提升人类-代理互动并防止灾难性结果。研究实验比较了三种RL代理：explicitly planning、implicitly planning 和 non-planning，采用两种方法——内部状态方法（基于代理的内部计算，如计划或神经元激活）和 simulation-based approach（通过学习的世界模型模拟）。结果表明，explicitly planning 代理的内部计划对行动预测更具信息性和稳健性，而事件预测的效果则较为复杂。这些发现强调了利用内部状态和模拟来增强RL代理在真实部署中的安全性和互动性。",
      "categories": [
        "cs.AI",
        "I.2.6; I.2.8; I.5.1"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.22459v1",
      "published_date": "2024-10-29 18:48:18 UTC",
      "updated_date": "2024-10-29 18:48:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:14:34.624295"
    },
    {
      "arxiv_id": "2410.22457v1",
      "title": "Advancing Agentic Systems: Dynamic Task Decomposition, Tool Integration and Evaluation using Novel Metrics and Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Adrian Garret Gabriel",
        "Alaa Alameer Ahmad",
        "Shankar Kumar Jeyakumar"
      ],
      "abstract": "Advancements in Large Language Models (LLMs) are revolutionizing the\ndevelopment of autonomous agentic systems by enabling dynamic, context-aware\ntask decomposition and automated tool selection. These sophisticated systems\npossess significant automation potential across various industries, managing\ncomplex tasks, interacting with external systems to enhance knowledge, and\nexecuting actions independently. This paper presents three primary\ncontributions to advance this field:\n  - Advanced Agentic Framework: A system that handles multi-hop queries,\ngenerates and executes task graphs, selects appropriate tools, and adapts to\nreal-time changes.\n  - Novel Evaluation Metrics: Introduction of Node F1 Score, Structural\nSimilarity Index (SSI), and Tool F1 Score to comprehensively assess agentic\nsystems.\n  - Specialized Dataset: Development of an AsyncHow-based dataset for analyzing\nagent behavior across different task complexities.\n  Our findings reveal that asynchronous and dynamic task graph decomposition\nsignificantly enhances system responsiveness and scalability, particularly for\ncomplex, multi-step tasks. Detailed analysis shows that structural and\nnode-level metrics are crucial for sequential tasks, while tool-related metrics\nare more important for parallel tasks. Specifically, the Structural Similarity\nIndex (SSI) is the most significant predictor of performance in sequential\ntasks, and the Tool F1 Score is essential for parallel tasks. These insights\nhighlight the need for balanced evaluation methods that capture both structural\nand operational dimensions of agentic systems. Additionally, our evaluation\nframework, validated through empirical analysis and statistical testing,\nprovides valuable insights for improving the adaptability and reliability of\nagentic systems in dynamic environments.",
      "tldr_zh": "这篇论文推进了代理系统的研究，提出一个高级代理框架，该框架支持动态任务分解(multi-hop queries)、任务图生成与执行、工具选择以及实时适应。论文的主要贡献包括引入新型评估指标如 Node F1 Score、Structural Similarity Index (SSI) 和 Tool F1 Score，以及开发基于 AsyncHow 的专用数据集，用于分析代理行为在不同任务复杂性下的表现。研究发现，异步动态任务图分解显著提升了系统响应性和可扩展性，尤其在复杂多步任务中，其中 SSI 是顺序任务的关键预测指标，而 Tool F1 Score 对并行任务至关重要，这些洞见强调了平衡评估方法的重要性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024), NeurIPS 2024 Workshop on Open-World Agents",
      "pdf_url": "http://arxiv.org/pdf/2410.22457v1",
      "published_date": "2024-10-29 18:45:13 UTC",
      "updated_date": "2024-10-29 18:45:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:17:59.174015"
    },
    {
      "arxiv_id": "2410.22456v1",
      "title": "Image2Struct: Benchmarking Structure Extraction for Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Josselin Somerville Roberts",
        "Tony Lee",
        "Chi Heem Wong",
        "Michihiro Yasunaga",
        "Yifan Mai",
        "Percy Liang"
      ],
      "abstract": "We introduce Image2Struct, a benchmark to evaluate vision-language models\n(VLMs) on extracting structure from images. Our benchmark 1) captures\nreal-world use cases, 2) is fully automatic and does not require human\njudgment, and 3) is based on a renewable stream of fresh data. In Image2Struct,\nVLMs are prompted to generate the underlying structure (e.g., LaTeX code or\nHTML) from an input image (e.g., webpage screenshot). The structure is then\nrendered to produce an output image (e.g., rendered webpage), which is compared\nagainst the input image to produce a similarity score. This round-trip\nevaluation allows us to quantitatively evaluate VLMs on tasks with multiple\nvalid structures. We create a pipeline that downloads fresh data from active\nonline communities upon execution and evaluates the VLMs without human\nintervention. We introduce three domains (Webpages, LaTeX, and Musical Scores)\nand use five image metrics (pixel similarity, cosine similarity between the\nInception vectors, learned perceptual image patch similarity, structural\nsimilarity index measure, and earth mover similarity) that allow efficient and\nautomatic comparison between pairs of images. We evaluate Image2Struct on 14\nprominent VLMs and find that scores vary widely, indicating that Image2Struct\ncan differentiate between the performances of different VLMs. Additionally, the\nbest score varies considerably across domains (e.g., 0.402 on sheet music vs.\n0.830 on LaTeX equations), indicating that Image2Struct contains tasks of\nvarying difficulty. For transparency, we release the full results at\nhttps://crfm.stanford.edu/helm/image2struct/v1.0.1/.",
      "tldr_zh": "本文提出 Image2Struct 基准，用于评估视觉语言模型 (VLMs) 从图像中提取结构的能力。该基准捕捉真实世界场景、实现完全自动评估，并依赖新鲜数据流，通过提示 VLMs 生成底层结构（如 LaTeX 代码或 HTML），然后渲染输出图像并使用五种图像指标（如像素相似度和结构相似性指数）与输入图像比较。实验结果显示，14 个突出的 VLMs 在 Webpages、LaTeX 和 Musical Scores 等三个领域表现出显著差异，最好分数从 0.402（乐谱）到 0.830（LaTeX 方程）不等，证明该基准能有效区分模型性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2024. First three authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2410.22456v1",
      "published_date": "2024-10-29 18:44:59 UTC",
      "updated_date": "2024-10-29 18:44:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:16:10.944071"
    },
    {
      "arxiv_id": "2411.00837v1",
      "title": "Longitudinal Mammogram Exam-based Breast Cancer Diagnosis Models: Vulnerability to Adversarial Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengbo Zhou",
        "Degan Hao",
        "Dooman Arefan",
        "Margarita Zuley",
        "Jules Sumkin",
        "Shandong Wu"
      ],
      "abstract": "In breast cancer detection and diagnosis, the longitudinal analysis of\nmammogram images is crucial. Contemporary models excel in detecting temporal\nimaging feature changes, thus enhancing the learning process over sequential\nimaging exams. Yet, the resilience of these longitudinal models against\nadversarial attacks remains underexplored. In this study, we proposed a novel\nattack method that capitalizes on the feature-level relationship between two\nsequential mammogram exams of a longitudinal model, guided by both\ncross-entropy loss and distance metric learning, to achieve significant attack\nefficacy, as implemented using attack transferring in a black-box attacking\nmanner. We performed experiments on a cohort of 590 breast cancer patients\n(each has two sequential mammogram exams) in a case-control setting. Results\nshowed that our proposed method surpassed several state-of-the-art adversarial\nattacks in fooling the diagnosis models to give opposite outputs. Our method\nremained effective even if the model was trained with the common defending\nmethod of adversarial training.",
      "tldr_zh": "本研究探讨了基于纵向乳腺X光图像的乳腺癌诊断模型对adversarial attacks的脆弱性，强调这些模型虽擅长检测时间序列特征变化，但其抗攻击能力尚未充分研究。研究提出了一种新型攻击方法，利用两个连续图像间的特征级关系，并结合cross-entropy loss和distance metric learning，通过黑盒攻击转移方式实现高效攻击。实验在590名乳腺癌患者的序列数据上进行，结果显示该方法在欺骗模型输出相反结果方面优于现有最先进攻击，且即使模型经过adversarial training仍保持有效。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00837v1",
      "published_date": "2024-10-29 18:37:44 UTC",
      "updated_date": "2024-10-29 18:37:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:16:24.771736"
    },
    {
      "arxiv_id": "2410.22451v1",
      "title": "Addressing Issues with Working Memory in Video Object Segmentation",
      "title_zh": "针对视频对象分割中",
      "authors": [
        "Clayton Bromley",
        "Alexander Moore",
        "Amar Saini",
        "Douglas Poland",
        "Carmen Carrano"
      ],
      "abstract": "Contemporary state-of-the-art video object segmentation (VOS) models compare\nincoming unannotated images to a history of image-mask relations via affinity\nor cross-attention to predict object masks. We refer to the internal memory\nstate of the initial image-mask pair and past image-masks as a working memory\nbuffer. While the current state of the art models perform very well on clean\nvideo data, their reliance on a working memory of previous frames leaves room\nfor error. Affinity-based algorithms include the inductive bias that there is\ntemporal continuity between consecutive frames. To account for inconsistent\ncamera views of the desired object, working memory models need an algorithmic\nmodification that regulates the memory updates and avoid writing irrelevant\nframes into working memory. A simple algorithmic change is proposed that can be\napplied to any existing working memory-based VOS model to improve performance\non inconsistent views, such as sudden camera cuts, frame interjections, and\nextreme context changes. The resulting model performances show significant\nimprovement on video data with these frame interjections over the same model\nwithout the algorithmic addition. Our contribution is a simple decision\nfunction that determines whether working memory should be updated based on the\ndetection of sudden, extreme changes and the assumption that the object is no\nlonger in frame. By implementing algorithmic changes, such as this, we can\nincrease the real-world applicability of current VOS models.",
      "tldr_zh": "该论文探讨了视频对象分割（VOS）模型中工作记忆缓冲区的问题，这些模型依赖历史图像-掩码关系（如亲和力或交叉注意力）来预测对象掩码，但在不一致相机视图（如突然切换、帧插入或极端上下文变化）时易出错。作者提出一个简单的算法修改，包括一个决策函数，用于检测突发变化并决定是否更新工作记忆，从而避免写入无关帧。实验结果显示，该改进显著提升了模型在这些场景下的性能，使VOS模型更适用于真实世界应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T45",
        "I.4.6; I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.22451v1",
      "published_date": "2024-10-29 18:34:41 UTC",
      "updated_date": "2024-10-29 18:34:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:16:35.046219"
    },
    {
      "arxiv_id": "2410.22446v2",
      "title": "Do Large Language Models Align with Core Mental Health Counseling Competencies?",
      "title_zh": "翻译失败",
      "authors": [
        "Viet Cuong Nguyen",
        "Mohammad Taher",
        "Dongwan Hong",
        "Vinicius Konkolics Possobom",
        "Vibha Thirunellayi Gopalakrishnan",
        "Ekta Raj",
        "Zihang Li",
        "Heather J. Soled",
        "Michael L. Birnbaum",
        "Srijan Kumar",
        "Munmun De Choudhury"
      ],
      "abstract": "The rapid evolution of Large Language Models (LLMs) presents a promising\nsolution to the global shortage of mental health professionals. However, their\nalignment with essential counseling competencies remains underexplored. We\nintroduce CounselingBench, a novel NCMHCE-based benchmark evaluating 22\ngeneral-purpose and medical-finetuned LLMs across five key competencies. While\nfrontier models surpass minimum aptitude thresholds, they fall short of\nexpert-level performance, excelling in Intake, Assessment & Diagnosis but\nstruggling with Core Counseling Attributes and Professional Practice & Ethics.\nSurprisingly, medical LLMs do not outperform generalist models in accuracy,\nthough they provide slightly better justifications while making more\ncontext-related errors. These findings highlight the challenges of developing\nAI for mental health counseling, particularly in competencies requiring empathy\nand nuanced reasoning. Our results underscore the need for specialized,\nfine-tuned models aligned with core mental health counseling competencies and\nsupported by human oversight before real-world deployment. Code and data\nassociated with this manuscript can be found at:\nhttps://github.com/cuongnguyenx/CounselingBench",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)是否符合核心心理健康咨询能力，以应对全球专业人员短缺问题，并引入了基于NCMHCE的CounselingBench基准测试来评估22个通用和医疗微调的LLMs在五个关键能力上的表现。结果显示，先锋模型超过了最低能力阈值，但在Intake, Assessment & Diagnosis方面表现出色，而Core Counseling Attributes和Professional Practice & Ethics方面表现较弱；此外，医疗LLMs在准确性上不优于通用模型，尽管理由略好但犯了更多上下文相关错误。这些发现强调了开发AI用于心理健康咨询的挑战，特别是需要移情和细微推理的能力，并建议通过专门微调的模型和人工监督来实现可靠部署。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 Pages, Accepted to Findings of NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.22446v2",
      "published_date": "2024-10-29 18:27:11 UTC",
      "updated_date": "2025-02-26 21:37:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:16:48.687884"
    },
    {
      "arxiv_id": "2410.22325v2",
      "title": "Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Guangqi Jiang",
        "Yifei Sun",
        "Tao Huang",
        "Huanyu Li",
        "Yongyuan Liang",
        "Huazhe Xu"
      ],
      "abstract": "The pre-training of visual representations has enhanced the efficiency of\nrobot learning. Due to the lack of large-scale in-domain robotic datasets,\nprior works utilize in-the-wild human videos to pre-train robotic visual\nrepresentation. Despite their promising results, representations from human\nvideos are inevitably subject to distribution shifts and lack the dynamics\ninformation crucial for task completion. We first evaluate various pre-trained\nrepresentations in terms of their correlation to the downstream robotic\nmanipulation tasks (i.e., manipulation centricity). Interestingly, we find that\nthe \"manipulation centricity\" is a strong indicator of success rates when\napplied to downstream tasks. Drawing from these findings, we propose\nManipulation Centric Representation (MCR), a foundation representation learning\nframework capturing both visual features and the dynamics information such as\nactions and proprioceptions of manipulation tasks to improve manipulation\ncentricity. Specifically, we pre-train a visual encoder on the DROID robotic\ndataset and leverage motion-relevant data such as robot proprioceptive states\nand actions. We introduce a novel contrastive loss that aligns visual\nobservations with the robot's proprioceptive state-action dynamics, combined\nwith a behavior cloning (BC)-like actor loss to predict actions during\npre-training, along with a time contrastive loss. Empirical results across 4\nsimulation domains with 20 tasks verify that MCR outperforms the strongest\nbaseline method by 14.8%. Moreover, MCR boosts the performance of\ndata-efficient learning with a UR5e arm on 3 real-world tasks by 76.9%. Project\nwebsite: https://robots-pretrain-robots.github.io/.",
      "tldr_zh": "该研究发现，现有的机器人视觉表示预训练依赖于野外人类视频，但受分布偏移和动态信息缺失的影响，导致下游操作任务性能不佳。通过评估各种预训练表示，作者证明了“manipulation centricity”（操作中心性）是任务成功率的重要指标。基于此，他们提出 Manipulation Centric Representation (MCR)，一个基础表示学习框架，使用 DROID 机器人数据集预训练视觉编码器，并引入新型对比损失来对齐视觉观察与机器人本体感知状态-动作动态，同时结合 behavior cloning (BC)-like 演员损失和时间对比损失。实验结果显示，MCR 在 4 个模拟领域 20 个任务上比最强基线提高 14.8%，并在真实世界 3 个任务上提升 76.9%，显著提高了数据高效学习的效果。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22325v2",
      "published_date": "2024-10-29 17:58:13 UTC",
      "updated_date": "2024-10-30 03:33:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:16:59.515272"
    },
    {
      "arxiv_id": "2410.22391v2",
      "title": "A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas Schmied",
        "Thomas Adler",
        "Vihang Patil",
        "Maximilian Beck",
        "Korbinian Pöppel",
        "Johannes Brandstetter",
        "Günter Klambauer",
        "Razvan Pascanu",
        "Sepp Hochreiter"
      ],
      "abstract": "In recent years, there has been a trend in the field of Reinforcement\nLearning (RL) towards large action models trained offline on large-scale\ndatasets via sequence modeling. Existing models are primarily based on the\nTransformer architecture, which result in powerful agents. However, due to slow\ninference times, Transformer-based approaches are impractical for real-time\napplications, such as robotics. Recently, modern recurrent architectures, such\nas xLSTM and Mamba, have been proposed that exhibit parallelization benefits\nduring training similar to the Transformer architecture while offering fast\ninference. In this work, we study the aptitude of these modern recurrent\narchitectures for large action models. Consequently, we propose a Large\nRecurrent Action Model (LRAM) with an xLSTM at its core that comes with\nlinear-time inference complexity and natural sequence length extrapolation\nabilities. Experiments on 432 tasks from 6 domains show that LRAM compares\nfavorably to Transformers in terms of performance and speed.",
      "tldr_zh": "本研究探讨了强化学习（RL）领域中大型动作模型的趋势，这些模型通常基于 Transformer 架构，但因推理速度慢而难以应用于实时机器人任务。为解决这一问题，作者提出 Large Recurrent Action Model (LRAM)，以 xLSTM 为核心，实现了线性时间推理复杂度和自然的序列长度外推能力。实验结果显示，在 6 个领域的 432 个任务上，LRAM 在性能和速度方面均优于 Transformer 模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22391v2",
      "published_date": "2024-10-29 17:55:47 UTC",
      "updated_date": "2025-02-20 09:29:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:17:10.584792"
    },
    {
      "arxiv_id": "2411.03336v2",
      "title": "Towards evaluations-based safety cases for AI scheming",
      "title_zh": "针对 AI 阴谋的基于评估的安全案例迈进",
      "authors": [
        "Mikita Balesni",
        "Marius Hobbhahn",
        "David Lindner",
        "Alexander Meinke",
        "Tomek Korbak",
        "Joshua Clymer",
        "Buck Shlegeris",
        "Jérémy Scheurer",
        "Charlotte Stix",
        "Rusheb Shah",
        "Nicholas Goldowsky-Dill",
        "Dan Braun",
        "Bilal Chughtai",
        "Owain Evans",
        "Daniel Kokotajlo",
        "Lucius Bushnaq"
      ],
      "abstract": "We sketch how developers of frontier AI systems could construct a structured\nrationale -- a 'safety case' -- that an AI system is unlikely to cause\ncatastrophic outcomes through scheming. Scheming is a potential threat model\nwhere AI systems could pursue misaligned goals covertly, hiding their true\ncapabilities and objectives. In this report, we propose three arguments that\nsafety cases could use in relation to scheming. For each argument we sketch how\nevidence could be gathered from empirical evaluations, and what assumptions\nwould need to be met to provide strong assurance. First, developers of frontier\nAI systems could argue that AI systems are not capable of scheming (Scheming\nInability). Second, one could argue that AI systems are not capable of posing\nharm through scheming (Harm Inability). Third, one could argue that control\nmeasures around the AI systems would prevent unacceptable outcomes even if the\nAI systems intentionally attempted to subvert them (Harm Control).\nAdditionally, we discuss how safety cases might be supported by evidence that\nan AI system is reasonably aligned with its developers (Alignment). Finally, we\npoint out that many of the assumptions required to make these safety arguments\nhave not been confidently satisfied to date and require making progress on\nmultiple open research problems.",
      "tldr_zh": "这篇论文探讨了如何为前沿AI系统构建基于评估的safety case，以证明AI系统不太可能通过scheming（一种AI隐藏能力并追求不一致目标的威胁模型）导致灾难性后果。论文提出了三种关键论证：Scheming Inability（AI无法进行scheming）、Harm Inability（AI即使scheming也无法造成危害），以及Harm Control（控制措施能防止AI的subvert尝试）。这些论证依赖于empirical evaluations收集证据，并假设AI系统具有合理的Alignment（与开发者目标一致）。最终，论文强调这些假设尚未被充分验证，需要解决多个开放研究问题。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.03336v2",
      "published_date": "2024-10-29 17:55:29 UTC",
      "updated_date": "2024-11-07 09:18:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:17:22.912411"
    },
    {
      "arxiv_id": "2410.22314v1",
      "title": "An Efficient Approach to Generate Safe Drivable Space by LiDAR-Camera-HDmap Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Minghao Ning",
        "Ahmad Reza Alghooneh",
        "Chen Sun",
        "Ruihe Zhang",
        "Pouya Panahandeh",
        "Steven Tuer",
        "Ehsan Hashemi",
        "Amir Khajepour"
      ],
      "abstract": "In this paper, we propose an accurate and robust perception module for\nAutonomous Vehicles (AVs) for drivable space extraction. Perception is crucial\nin autonomous driving, where many deep learning-based methods, while accurate\non benchmark datasets, fail to generalize effectively, especially in diverse\nand unpredictable environments. Our work introduces a robust easy-to-generalize\nperception module that leverages LiDAR, camera, and HD map data fusion to\ndeliver a safe and reliable drivable space in all weather conditions. We\npresent an adaptive ground removal and curb detection method integrated with HD\nmap data for enhanced obstacle detection reliability. Additionally, we propose\nan adaptive DBSCAN clustering algorithm optimized for precipitation noise, and\na cost-effective LiDAR-camera frustum association that is resilient to\ncalibration discrepancies. Our comprehensive drivable space representation\nincorporates all perception data, ensuring compatibility with vehicle\ndimensions and road regulations. This approach not only improves generalization\nand efficiency, but also significantly enhances safety in autonomous vehicle\noperations. Our approach is tested on a real dataset and its reliability is\nverified during the daily (including harsh snowy weather) operation of our\nautonomous shuttle, WATonoBus",
      "tldr_zh": "本文提出了一种基于 LiDAR、相机和 HD map 融合的感知模块，用于自动驾驶车辆（AVs）生成安全可驾驶空间，以解决深度学习方法在多样环境中的泛化问题。该模块包括自适应地面移除与路缘检测、自适应 DBSCAN 聚类算法优化降水噪声，以及成本有效的 LiDAR-相机 frustum 关联，确保在各种天气条件下提升障碍检测的可靠性和兼容性。实验结果显示，该方法在真实数据集和 WATonoBus 自动穿梭车的日常操作中显著提高了泛化、效率和安全性，尤其在恶劣雪天气下表现突出。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22314v1",
      "published_date": "2024-10-29 17:54:02 UTC",
      "updated_date": "2024-10-29 17:54:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:17:35.125857"
    },
    {
      "arxiv_id": "2410.22312v2",
      "title": "Effective Guidance for Model Attention with Simple Yes-no Annotations",
      "title_zh": "翻译失败",
      "authors": [
        "Seongmin Lee",
        "Ali Payani",
        "Duen Horng Chau"
      ],
      "abstract": "Modern deep learning models often make predictions by focusing on irrelevant\nareas, leading to biased performance and limited generalization. Existing\nmethods aimed at rectifying model attention require explicit labels for\nirrelevant areas or complex pixel-wise ground truth attention maps. We present\nCRAYON (Correcting Reasoning with Annotations of Yes Or No), offering\neffective, scalable, and practical solutions to rectify model attention using\nsimple yes-no annotations. CRAYON empowers classical and modern model\ninterpretation techniques to identify and guide model reasoning:\nCRAYON-ATTENTION directs classic interpretations based on saliency maps to\nfocus on relevant image regions, while CRAYON-PRUNING removes irrelevant\nneurons identified by modern concept-based methods to mitigate their influence.\nThrough extensive experiments with both quantitative and human evaluation, we\nshowcase CRAYON's effectiveness, scalability, and practicality in refining\nmodel attention. CRAYON achieves state-of-the-art performance, outperforming 12\nmethods across 3 benchmark datasets, surpassing approaches that require more\ncomplex annotations.",
      "tldr_zh": "本文研究了现代深度学习模型因关注无关区域而导致的性能偏见和泛化问题，提出了一种名为 CRAYON（Correcting Reasoning with Annotations of Yes Or No）的简单方法，使用 yes-no 注解来有效指导模型注意力。CRAYON 包括 CRAYON-ATTENTION（基于 saliency maps 引导模型聚焦相关图像区域）和 CRAYON-PRUNING（移除由概念-based 方法识别的无关神经元），从而提升模型推理准确性。通过定量和人类评估实验，CRAYON 在 3 个基准数据集上超越 12 种现有方法，实现了 state-of-the-art 性能，同时更具可扩展性和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures, IEEE BigData 2024 Paper",
      "pdf_url": "http://arxiv.org/pdf/2410.22312v2",
      "published_date": "2024-10-29 17:53:33 UTC",
      "updated_date": "2024-11-15 22:53:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:17:47.239763"
    },
    {
      "arxiv_id": "2410.22307v1",
      "title": "SVIP: Towards Verifiable Inference of Open-source Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Sun",
        "Yuhang Li",
        "Yue Zhang",
        "Yuchen Jin",
        "Huan Zhang"
      ],
      "abstract": "Open-source Large Language Models (LLMs) have recently demonstrated\nremarkable capabilities in natural language understanding and generation,\nleading to widespread adoption across various domains. However, their\nincreasing model sizes render local deployment impractical for individual\nusers, pushing many to rely on computing service providers for inference\nthrough a blackbox API. This reliance introduces a new risk: a computing\nprovider may stealthily substitute the requested LLM with a smaller, less\ncapable model without consent from users, thereby delivering inferior outputs\nwhile benefiting from cost savings. In this paper, we formalize the problem of\nverifiable inference for LLMs. Existing verifiable computing solutions based on\ncryptographic or game-theoretic techniques are either computationally\nuneconomical or rest on strong assumptions. We introduce SVIP, a secret-based\nverifiable LLM inference protocol that leverages intermediate outputs from LLM\nas unique model identifiers. By training a proxy task on these outputs and\nrequiring the computing provider to return both the generated text and the\nprocessed intermediate outputs, users can reliably verify whether the computing\nprovider is acting honestly. In addition, the integration of a secret mechanism\nfurther enhances the security of our protocol. We thoroughly analyze our\nprotocol under multiple strong and adaptive adversarial scenarios. Our\nextensive experiments demonstrate that SVIP is accurate, generalizable,\ncomputationally efficient, and resistant to various attacks. Notably, SVIP\nachieves false negative rates below 5% and false positive rates below 3%, while\nrequiring less than 0.01 seconds per query for verification.",
      "tldr_zh": "该研究针对开源大语言模型 (LLMs) 的部署问题，指出用户依赖黑箱 API 时，可能面临计算提供商偷偷替换为更小模型的风险，从而影响输出质量。论文形式化了LLMs的可验证推理问题，并提出SVIP协议，该协议利用LLMs的中间输出作为唯一模型标识，通过训练代理任务和要求提供商返回生成文本及处理输出来实现验证，同时整合秘密机制提升安全。实验结果显示，SVIP在多种对抗场景下表现出色，假阴性率低于5%、假阳性率低于3%，且每查询验证时间少于0.01秒，为高效的反欺诈机制提供了可靠解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2410.22307v1",
      "published_date": "2024-10-29 17:52:45 UTC",
      "updated_date": "2024-10-29 17:52:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:18:09.737106"
    },
    {
      "arxiv_id": "2410.22303v1",
      "title": "$\\mathsf{OPA}$: One-shot Private Aggregation with Single Client Interaction and its Applications to Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Harish Karthikeyan",
        "Antigoni Polychroniadou"
      ],
      "abstract": "Our work aims to minimize interaction in secure computation due to the high\ncost and challenges associated with communication rounds, particularly in\nscenarios with many clients. In this work, we revisit the problem of secure\naggregation in the single-server setting where a single evaluation server can\nsecurely aggregate client-held individual inputs. Our key contribution is the\nintroduction of One-shot Private Aggregation ($\\mathsf{OPA}$) where clients\nspeak only once (or even choose not to speak) per aggregation evaluation. Since\neach client communicates only once per aggregation, this simplifies managing\ndropouts and dynamic participation, contrasting with multi-round protocols and\naligning with plaintext secure aggregation, where clients interact only once.\nWe construct $\\mathsf{OPA}$ based on LWR, LWE, class groups, DCR and\ndemonstrate applications to privacy-preserving Federated Learning (FL) where\nclients \\emph{speak once}. This is a sharp departure from prior multi-round FL\nprotocols whose study was initiated by Bonawitz et al. (CCS, 2017). Moreover,\nunlike the YOSO (You Only Speak Once) model for general secure computation,\n$\\mathsf{OPA}$ eliminates complex committee selection protocols to achieve\nadaptive security. Beyond asymptotic improvements, $\\mathsf{OPA}$ is practical,\noutperforming state-of-the-art solutions. We benchmark logistic regression\nclassifiers for two datasets, while also building an MLP classifier to train on\nMNIST, CIFAR-10, and CIFAR-100 datasets. We build two flavors of $\\caps$ (1)\nfrom (threshold) key homomorphic PRF and (2) from seed homomorphic PRG and\nsecret sharing.",
      "tldr_zh": "这篇论文引入了$\\mathsf{OPA}$（One-shot Private Aggregation），一种单次客户端交互的安全聚合方法，旨在最小化通信成本并简化处理掉线和动态参与的问题。$\\mathsf{OPA}$基于LWR、LWE、class groups和DCR等技术构建，与现有多轮协议不同，它实现了自适应安全，无需复杂的委员会选择。论文将其应用于隐私保护的Federated Learning，并通过实验（如logistic regression和MLP分类器在MNIST、CIFAR-10、CIFAR-100上的基准测试）证明了$\\mathsf{OPA}$的实际性能优于现有方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "D.4.6; I.2.11; E.3; K.4.1; I.2"
      ],
      "primary_category": "cs.CR",
      "comment": "To appear at the NeurIPS 2024 FL@FM workshop",
      "pdf_url": "http://arxiv.org/pdf/2410.22303v1",
      "published_date": "2024-10-29 17:50:11 UTC",
      "updated_date": "2024-10-29 17:50:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:18:22.720309"
    },
    {
      "arxiv_id": "2410.22285v1",
      "title": "From melodic note sequences to pitches using word2vec",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Defays"
      ],
      "abstract": "Applying the word2vec technique, commonly used in language modeling, to\nmelodies, where notes are treated as words in sentences, enables the capture of\npitch information. This study examines two datasets: 20 children's songs and an\nexcerpt from a Bach sonata. The semantic space for defining the embeddings is\nof very small dimension, specifically 2. Notes are predicted based on the 2, 3\nor 4 preceding notes that establish the context. A multivariate analysis of the\nresults shows that the semantic vectors representing the notes have a multiple\ncorrelation coefficient of approximately 0.80 with their pitches.",
      "tldr_zh": "本研究将 word2vec 技术应用于旋律序列，将音符视为句子中的单词，以捕捉音高信息，并使用两个数据集：20 首儿童歌曲和巴赫奏鸣曲的节选。  \n方法涉及在 2 维语义空间中，基于前 2、3 或 4 个音符作为上下文来预测后续音符。  \n多变量分析结果显示，音符的语义向量与音高的多重相关系数约为 0.80，证明了该方法的有效性。  \n这项工作展示了 word2vec 在音乐领域的潜力，为进一步的音乐生成和分析提供了新思路。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.22285v1",
      "published_date": "2024-10-29 17:38:27 UTC",
      "updated_date": "2024-10-29 17:38:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:18:34.013111"
    },
    {
      "arxiv_id": "2410.22390v1",
      "title": "FNDEX: Fake News and Doxxing Detection with Explainable AI",
      "title_zh": "翻译失败",
      "authors": [
        "Dorsaf Sallami",
        "Esma Aïmeur"
      ],
      "abstract": "The widespread and diverse online media platforms and other internet-driven\ncommunication technologies have presented significant challenges in defining\nthe boundaries of freedom of expression. Consequently, the internet has been\ntransformed into a potential cyber weapon. Within this evolving landscape, two\nparticularly hazardous phenomena have emerged: fake news and doxxing. Although\nthese threats have been subjects of extensive scholarly analysis, the\ncrossroads where they intersect remain unexplored. This research addresses this\nconvergence by introducing a novel system. The Fake News and Doxxing Detection\nwith Explainable Artificial Intelligence (FNDEX) system leverages the\ncapabilities of three distinct transformer models to achieve high-performance\ndetection for both fake news and doxxing. To enhance data security, a rigorous\nthree-step anonymization process is employed, rooted in a pattern-based\napproach for anonymizing personally identifiable information. Finally, this\nresearch emphasizes the importance of generating coherent explanations for the\noutcomes produced by both detection models. Our experiments on realistic\ndatasets demonstrate that our system significantly outperforms the existing\nbaselines",
      "tldr_zh": "这篇论文介绍了 FNDEX 系统，一种用于检测 Fake News 和 Doxxing 的新型框架，结合 Explainable AI 来提供可解释的结果，以应对在线媒体平台的言论自由边界挑战。该系统利用三个不同的 Transformer 模型实现高性能检测，并采用三步匿名化过程（基于模式的方法）来保护个人信息安全。实验结果显示，在真实数据集上，FNDEX 显著优于现有基线模型，为网络威胁检测提供了更可靠和可解释的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22390v1",
      "published_date": "2024-10-29 17:29:45 UTC",
      "updated_date": "2024-10-29 17:29:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:20:27.922693"
    },
    {
      "arxiv_id": "2411.00836v2",
      "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models",
      "title_zh": "DynaMath：用于评估视觉语言模型数学推理鲁棒性的动态视觉基准",
      "authors": [
        "Chengke Zou",
        "Xingang Guo",
        "Rui Yang",
        "Junyu Zhang",
        "Bin Hu",
        "Huan Zhang"
      ],
      "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great\npotential in tackling mathematical reasoning tasks that involve visual context.\nUnlike humans who can reliably apply solution steps to similar problems with\nminor modifications, we found that SOTA VLMs like GPT-4o can consistently fail\nin these scenarios, revealing limitations in their mathematical reasoning\ncapabilities. In this paper, we investigate the mathematical reasoning\nrobustness in VLMs and evaluate how well these models perform under different\nvariants of the same question, such as changes in visual numerical values or\nfunction graphs. While several vision-based math benchmarks have been developed\nto assess VLMs' problem-solving capabilities, these benchmarks contain only\nstatic sets of problems and cannot easily evaluate mathematical reasoning\nrobustness. To fill this gap, we introduce DynaMath, a dynamic visual math\nbenchmark designed for in-depth assessment of VLMs. DynaMath includes 501\nhigh-quality, multi-topic seed questions, each represented as a Python program.\nThose programs are carefully designed and annotated to enable the automatic\ngeneration of a much larger set of concrete questions, including many different\ntypes of visual and textual variations. DynaMath allows us to evaluate the\ngeneralization ability of VLMs, by assessing their performance under varying\ninput conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010\ngenerated concrete questions. Our results show that the worst-case model\naccuracy, defined as the percentage of correctly answered seed questions in all\n10 variants, is significantly lower than the average-case accuracy. Our\nanalysis emphasizes the need to study the robustness of VLMs' reasoning\nabilities, and DynaMath provides valuable insights to guide the development of\nmore reliable models for mathematical reasoning.",
      "tldr_zh": "本研究发现，State-of-the-Art (SOTA) Vision-Language Models (VLMs) 在视觉数学推理任务中缺乏鲁棒性，例如对问题变体（如数值或函数图变化）易失败，而人类则能更可靠地处理类似问题。为解决这一问题，论文提出 DynaMath，一个动态视觉基准，包含 501 个高质量种子问题，每个用 Python 程序表示，可自动生成多种视觉和文本变体，以评估 VLMs 的泛化能力。实验评估了 14 个 SOTA VLMs，共 5010 个问题，结果显示最坏情况准确率（所有变体都正确）远低于平均准确率，强调了提升 VLMs 数学推理鲁棒性的紧迫性，并为开发更可靠模型提供指导。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.00836v2",
      "published_date": "2024-10-29 17:29:19 UTC",
      "updated_date": "2025-02-24 06:55:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:19:00.124635"
    },
    {
      "arxiv_id": "2410.22271v1",
      "title": "Leveraging Reverberation and Visual Depth Cues for Sound Event Localization and Detection with Distance Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Davide Berghi",
        "Philip J. B. Jackson"
      ],
      "abstract": "This report describes our systems submitted for the DCASE2024 Task 3\nchallenge: Audio and Audiovisual Sound Event Localization and Detection with\nSource Distance Estimation (Track B). Our main model is based on the\naudio-visual (AV) Conformer, which processes video and audio embeddings\nextracted with ResNet50 and with an audio encoder pre-trained on SELD,\nrespectively. This model outperformed the audio-visual baseline of the\ndevelopment set of the STARSS23 dataset by a wide margin, halving its DOAE and\nimproving the F1 by more than 3x. Our second system performs a temporal\nensemble from the outputs of the AV-Conformer. We then extended the model with\nfeatures for distance estimation, such as direct and reverberant signal\ncomponents extracted from the omnidirectional audio channel, and depth maps\nextracted from the video frames. While the new system improved the RDE of our\nprevious model by about 3 percentage points, it achieved a lower F1 score. This\nmay be caused by sound classes that rarely appear in the training set and that\nthe more complex system does not detect, as analysis can determine. To overcome\nthis problem, our fourth and final system consists of an ensemble strategy\ncombining the predictions of the other three. Many opportunities to refine the\nsystem and training strategy can be tested in future ablation experiments, and\nlikely achieve incremental performance gains for this audio-visual task.",
      "tldr_zh": "该研究针对DCASE2024 Task 3挑战赛（Track B）开发了音频-视觉（AV）Conformer模型，用于声音事件定位和检测，并整合源距离估计。模型利用ResNet50提取视频嵌入和预训练音频编码器提取音频嵌入，在STARSS23数据集上大幅超越基线，将DOAE减半并将F1分数提高超过3倍。添加混响信号组件和视频深度图等距离估计特征后，系统改善了RDE约3个百分点，但F1分数有所下降，可能由于训练集中的声音类别不均衡。为此，研究采用集成策略结合多个系统输出，以提升整体性能，并建议未来通过消融实验进一步优化。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "eess.IV",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22271v1",
      "published_date": "2024-10-29 17:28:43 UTC",
      "updated_date": "2024-10-29 17:28:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:19:11.605906"
    },
    {
      "arxiv_id": "2410.22269v2",
      "title": "Fourier Head: Helping Large Language Models Learn Complex Probability Distributions",
      "title_zh": "Fourier Head: 帮助大型语言模型学习复杂的概率分布",
      "authors": [
        "Nate Gillman",
        "Daksh Aggarwal",
        "Michael Freeman",
        "Saurabh Singh",
        "Chen Sun"
      ],
      "abstract": "As the quality of large language models has improved, there has been\nincreased interest in using them to model non-linguistic tokens. For example,\nthe Decision Transformer recasts agentic decision making as a sequence modeling\nproblem, using a decoder-only LLM to model the distribution over the discrete\naction space for an Atari agent. However, when adapting LLMs to non-linguistic\ndomains, it remains unclear if softmax over discrete bins captures the\ncontinuous structure of the tokens and the potentially complex distributions\nneeded for high quality token generation. We introduce a neural network layer,\nconstructed using Fourier series, which we can easily substitute for any linear\nlayer if we want the outputs to have a more continuous structure. We perform\nextensive analysis on synthetic datasets, as well as on large-scale decision\nmaking and time series forecasting tasks. We also provide theoretical evidence\nthat this layer can better learn signal from data while ignoring high-frequency\nnoise. All of our results support the effectiveness of our proposed Fourier\nhead in scenarios where the underlying data distribution has a natural\ncontinuous structure. For example, the Fourier head improves a Decision\nTransformer agent's returns across four benchmark Atari games by as much as\n377%, and increases a state-of-the-art times series foundation model's\nforecasting performance by 3.5% across 20 benchmarks unseen during training.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在建模非语言标记（如决策或时间序列）时的挑战，特别是softmax在捕捉连续结构和复杂概率分布方面的不足。为解决这一问题，研究提出了一种基于傅立叶级数（Fourier series）的神经网络层——Fourier head，可替换线性层以更好地学习连续数据结构，并在理论上证明其能过滤高频噪声。实验结果显示，Fourier head 显著提升了性能，例如在 Atari 游戏中使 Decision Transformer 代理的回报率提高高达 377%，并在 20 个未见训练基准上的时间序列预测中提升 3.5%。总之，该方法为 LLMs 处理复杂分布提供了有效工具，推动了其在决策和预测领域的应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Camera ready version (ICLR 2025). Code at\n  https://nategillman.com/fourier-head",
      "pdf_url": "http://arxiv.org/pdf/2410.22269v2",
      "published_date": "2024-10-29 17:27:58 UTC",
      "updated_date": "2025-03-10 23:59:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:19:22.986870"
    },
    {
      "arxiv_id": "2410.22233v3",
      "title": "ContextIQ: A Multimodal Expert-Based Video Retrieval System for Contextual Advertising",
      "title_zh": "ContextIQ: 一种基于多模态专家的视频检索系统，用于上下文广告",
      "authors": [
        "Ashutosh Chaubey",
        "Anoubhav Agarwaal",
        "Sartaki Sinha Roy",
        "Aayush Agrawal",
        "Susmita Ghose"
      ],
      "abstract": "Contextual advertising serves ads that are aligned to the content that the\nuser is viewing. The rapid growth of video content on social platforms and\nstreaming services, along with privacy concerns, has increased the need for\ncontextual advertising. Placing the right ad in the right context creates a\nseamless and pleasant ad viewing experience, resulting in higher audience\nengagement and, ultimately, better ad monetization. From a technology\nstandpoint, effective contextual advertising requires a video retrieval system\ncapable of understanding complex video content at a very granular level.\nCurrent text-to-video retrieval models based on joint multimodal training\ndemand large datasets and computational resources, limiting their practicality\nand lacking the key functionalities required for ad ecosystem integration. We\nintroduce ContextIQ, a multimodal expert-based video retrieval system designed\nspecifically for contextual advertising. ContextIQ utilizes modality-specific\nexperts-video, audio, transcript (captions), and metadata such as objects,\nactions, emotion, etc.-to create semantically rich video representations. We\nshow that our system, without joint training, achieves better or comparable\nresults to state-of-the-art models and commercial solutions on multiple\ntext-to-video retrieval benchmarks. Our ablation studies highlight the benefits\nof leveraging multiple modalities for enhanced video retrieval accuracy instead\nof using a vision-language model alone. Furthermore, we show how video\nretrieval systems such as ContextIQ can be used for contextual advertising in\nan ad ecosystem while also addressing concerns related to brand safety and\nfiltering inappropriate content.",
      "tldr_zh": "这篇论文介绍了 ContextIQ，一种基于多模态专家的视频检索系统，旨在提升上下文广告的精准性，以应对视频内容爆炸式增长和隐私挑战。ContextIQ 通过整合视频、音频、转录（captions）和元数据（如对象、动作、情感）等模态专家，创建语义丰富的视频表示，而无需依赖联合训练，从而降低了资源需求。实验结果显示，该系统在多个文本到视频检索基准上，表现优于或相当于是先进模型，且多模态方法显著提高了检索准确性。最后，论文展示了 ContextIQ 在广告生态中的应用，能提升观众参与度和广告收益，同时解决品牌安全和内容过滤问题。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.22233v3",
      "published_date": "2024-10-29 17:01:05 UTC",
      "updated_date": "2025-03-29 17:42:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:19:35.824928"
    },
    {
      "arxiv_id": "2411.08910v1",
      "title": "Automated Feedback in Math Education: A Comparative Analysis of LLMs for Open-Ended Responses",
      "title_zh": "数学教育中的自动反馈：LLMs 用于开放式回答的比较分析",
      "authors": [
        "Sami Baral",
        "Eamon Worden",
        "Wen-Chiang Lim",
        "Zhuang Luo",
        "Christopher Santorelli",
        "Ashish Gurung",
        "Neil Heffernan"
      ],
      "abstract": "The effectiveness of feedback in enhancing learning outcomes is well\ndocumented within Educational Data Mining (EDM). Various prior research has\nexplored methodologies to enhance the effectiveness of feedback. Recent\ndevelopments in Large Language Models (LLMs) have extended their utility in\nenhancing automated feedback systems. This study aims to explore the potential\nof LLMs in facilitating automated feedback in math education. We examine the\neffectiveness of LLMs in evaluating student responses by comparing 3 different\nmodels: Llama, SBERT-Canberra, and GPT4 model. The evaluation requires the\nmodel to provide both a quantitative score and qualitative feedback on the\nstudent's responses to open-ended math problems. We employ Mistral, a version\nof Llama catered to math, and fine-tune this model for evaluating student\nresponses by leveraging a dataset of student responses and teacher-written\nfeedback for middle-school math problems. A similar approach was taken for\ntraining the SBERT model as well, while the GPT4 model used a zero-shot\nlearning approach. We evaluate the model's performance in scoring accuracy and\nthe quality of feedback by utilizing judgments from 2 teachers. The teachers\nutilized a shared rubric in assessing the accuracy and relevance of the\ngenerated feedback. We conduct both quantitative and qualitative analyses of\nthe model performance. By offering a detailed comparison of these methods, this\nstudy aims to further the ongoing development of automated feedback systems and\noutlines potential future directions for leveraging generative LLMs to create\nmore personalized learning experiences.",
      "tldr_zh": "这篇论文比较了大型语言模型（LLMs）在数学教育中为开放式问题提供自动化反馈的有效性，包括Llama（Mistral版本）、SBERT-Canberra和GPT4模型。研究方法涉及微调Mistral和SBERT模型，使用学生响应和教师反馈数据集进行训练，而GPT4采用零样本学习（zero-shot learning）方式。评估基于教师判断，涵盖评分准确性和反馈质量的定量及定性分析，结果显示这些模型在反馈生成上存在差异，并为开发更个性化的自动化反馈系统指出了未来方向。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "12 pages including references, 4 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.08910v1",
      "published_date": "2024-10-29 16:57:45 UTC",
      "updated_date": "2024-10-29 16:57:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:19:46.710980"
    },
    {
      "arxiv_id": "2410.22223v1",
      "title": "MAPUNetR: A Hybrid Vision Transformer and U-Net Architecture for Efficient and Interpretable Medical Image Segmentation",
      "title_zh": "MAPUNetR：一种混合视觉Transformer和U-Net架构，用于高效且可解释的医学图像分割",
      "authors": [
        "Ovais Iqbal Shah",
        "Danish Raza Rizvi",
        "Aqib Nazir Mir"
      ],
      "abstract": "Medical image segmentation is pivotal in healthcare, enhancing diagnostic\naccuracy, informing treatment strategies, and tracking disease progression.\nThis process allows clinicians to extract critical information from visual\ndata, enabling personalized patient care. However, developing neural networks\nfor segmentation remains challenging, especially when preserving image\nresolution, which is essential in detecting subtle details that influence\ndiagnoses. Moreover, the lack of transparency in these deep learning models has\nslowed their adoption in clinical practice. Efforts in model interpretability\nare increasingly focused on making these models' decision-making processes more\ntransparent. In this paper, we introduce MAPUNetR, a novel architecture that\nsynergizes the strengths of transformer models with the proven U-Net framework\nfor medical image segmentation. Our model addresses the resolution preservation\nchallenge and incorporates attention maps highlighting segmented regions,\nincreasing accuracy and interpretability. Evaluated on the BraTS 2020 dataset,\nMAPUNetR achieved a dice score of 0.88 and a dice coefficient of 0.92 on the\nISIC 2018 dataset. Our experiments show that the model maintains stable\nperformance and potential as a powerful tool for medical image segmentation in\nclinical practice.",
      "tldr_zh": "这篇论文介绍了 MAPUNetR，一种结合 Vision Transformer 和 U-Net 的混合架构，旨在实现高效且可解释的医疗图像分割，以解决图像分辨率保留和模型透明度挑战。MAPUNetR 通过整合注意力图（attention maps）来增强分割准确性和决策过程的可解释性，提升了临床诊断的可靠性。在 BraTS 2020 和 ISIC 2018 数据集上的实验中，该模型分别取得了 0.88 和 0.92 的 Dice 得分，并展示了稳定的性能，有望成为临床实践中的强大工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22223v1",
      "published_date": "2024-10-29 16:52:57 UTC",
      "updated_date": "2024-10-29 16:52:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:20:00.187708"
    },
    {
      "arxiv_id": "2410.22209v5",
      "title": "A Methodology for Incompleteness-Tolerant and Modular Gradual Semantics for Argumentative Statement Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio Rago",
        "Stylianos Loukas Vasileiou",
        "Francesca Toni",
        "Tran Cao Son",
        "William Yeoh"
      ],
      "abstract": "Gradual semantics (GS) have demonstrated great potential in argumentation, in\nparticular for deploying quantitative bipolar argumentation frameworks (QBAFs)\nin a number of real-world settings, from judgmental forecasting to explainable\nAI. In this paper, we provide a novel methodology for obtaining GS for\nstatement graphs, a form of structured argumentation framework, where arguments\nand relations between them are built from logical statements. Our methodology\ndiffers from existing approaches in the literature in two main ways. First, it\nnaturally accommodates incomplete information, so that arguments with partially\nspecified premises can play a meaningful role in the evaluation. Second, it is\nmodularly defined to leverage on any GS for QBAFs. We also define a set of\nnovel properties for our GS and study their suitability alongside a set of\nexisting properties (adapted to our setting) for two instantiations of our GS,\ndemonstrating their advantages over existing approaches.",
      "tldr_zh": "这篇论文提出了一种新方法，用于获取针对语句图 (Argumentative Statement Graphs) 的渐进语义 (Gradual Semantics)，该方法能容忍不完整信息并模块化利用定量双极论证框架 (Quantitative Bipolar Argumentation Frameworks, QBAFs)。与现有方法不同，它允许部分指定前提的论点在评估中发挥作用，并通过模块化设计支持任何 QBAFs 的 GS，从而提升了框架的灵活性和适用性。论文还定义了一系列新属性，并通过两个 GS 实例的研究，证明了该方法的优势，例如在判断性预测和可解释 AI 等真实场景中的表现优于现有方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22209v5",
      "published_date": "2024-10-29 16:38:35 UTC",
      "updated_date": "2025-01-30 15:29:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:20:55.299016"
    },
    {
      "arxiv_id": "2410.22208v1",
      "title": "Drone Acoustic Analysis for Predicting Psychoacoustic Annoyance via Artificial Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Andrea Vaiuso",
        "Marcello Righi",
        "Oier Coretti",
        "Moreno Apicella"
      ],
      "abstract": "Unmanned Aerial Vehicles (UAVs) have become widely used in various fields and\nindustrial applications thanks to their low operational cost, compact size and\nwide accessibility. However, the noise generated by drone propellers has\nemerged as a significant concern. This may affect the public willingness to\nimplement these vehicles in services that require operation in proximity to\nresidential areas. The standard approaches to address this challenge include\nsound pressure measurements and noise characteristic analyses. The integration\nof Artificial Intelligence models in recent years has further streamlined the\nprocess by enhancing complex feature detection in drone acoustics data. This\nstudy builds upon prior research by examining the efficacy of various Deep\nLearning models in predicting Psychoacoustic Annoyance, an effective index for\nmeasuring perceived annoyance by human ears, based on multiple drone\ncharacteristics as input. This is accomplished by constructing a training\ndataset using precise measurements of various drone models with multiple\nmicrophones and analyzing flight data, maneuvers, drone physical\ncharacteristics, and perceived annoyance under realistic conditions. The aim of\nthis research is to improve our understanding of drone noise, aid in the\ndevelopment of noise reduction techniques, and encourage the acceptance of\ndrone usage on public spaces.",
      "tldr_zh": "本研究探讨了无人机（UAVs）噪音对公众感知的影响，特别关注心理声学烦扰（Psychoacoustic Annoyance）的预测，以提升无人机在住宅区应用的接受度。研究构建了一个训练数据集，通过多种无人机模型的精确测量，包括飞行数据、操作特征、物理特性以及真实条件下的人类感知烦扰。利用各种深度学习模型（Deep Learning models），如Artificial Neural Networks，对这些输入特征进行分析，以预测烦扰水平。实验结果显示，该方法有效提高了噪音分析的准确性，并为开发噪音减少技术和促进无人机在公共空间的使用提供了重要指导。",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "20 Pages, 10 Figures, 4 Tables",
      "pdf_url": "http://arxiv.org/pdf/2410.22208v1",
      "published_date": "2024-10-29 16:38:34 UTC",
      "updated_date": "2024-10-29 16:38:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:20:50.461788"
    },
    {
      "arxiv_id": "2410.22203v1",
      "title": "Democratizing Reward Design for Personal and Representative Value-Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Carter Blair",
        "Kate Larson",
        "Edith Law"
      ],
      "abstract": "Aligning AI agents with human values is challenging due to diverse and\nsubjective notions of values. Standard alignment methods often aggregate crowd\nfeedback, which can result in the suppression of unique or minority\npreferences. We introduce Interactive-Reflective Dialogue Alignment, a method\nthat iteratively engages users in reflecting on and specifying their subjective\nvalue definitions. This system learns individual value definitions through\nlanguage-model-based preference elicitation and constructs personalized reward\nmodels that can be used to align AI behaviour. We evaluated our system through\ntwo studies with 30 participants, one focusing on \"respect\" and the other on\nethical decision-making in autonomous vehicles. Our findings demonstrate\ndiverse definitions of value-aligned behaviour and show that our system can\naccurately capture each person's unique understanding. This approach enables\npersonalized alignment and can inform more representative and interpretable\ncollective alignment strategies.",
      "tldr_zh": "这篇论文针对AI代理与人类价值观对齐的挑战，提出Interactive-Reflective Dialogue Alignment方法，以应对标准聚合反馈可能压制少数偏好的问题。该方法通过迭代对话和语言模型进行偏好收集，构建个性化的奖励模型，实现对AI行为的个性化对齐。在涉及30参与者的两项研究中，系统成功捕捉了多样化的价值观定义，如“尊重”和自动驾驶车辆的道德决策，并为更具代表性和可解释的集体对齐策略提供了指导。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.22203v1",
      "published_date": "2024-10-29 16:37:01 UTC",
      "updated_date": "2024-10-29 16:37:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:22:56.712100"
    },
    {
      "arxiv_id": "2410.22194v1",
      "title": "ADAM: An Embodied Causal Agent in Open-World Environments",
      "title_zh": "ADAM：开放世界环境中的具身因果代理",
      "authors": [
        "Shu Yu",
        "Chaochao Lu"
      ],
      "abstract": "In open-world environments like Minecraft, existing agents face challenges in\ncontinuously learning structured knowledge, particularly causality. These\nchallenges stem from the opacity inherent in black-box models and an excessive\nreliance on prior knowledge during training, which impair their\ninterpretability and generalization capability. To this end, we introduce ADAM,\nAn emboDied causal Agent in Minecraft, that can autonomously navigate the open\nworld, perceive multimodal contexts, learn causal world knowledge, and tackle\ncomplex tasks through lifelong learning. ADAM is empowered by four key\ncomponents: 1) an interaction module, enabling the agent to execute actions\nwhile documenting the interaction processes; 2) a causal model module, tasked\nwith constructing an ever-growing causal graph from scratch, which enhances\ninterpretability and diminishes reliance on prior knowledge; 3) a controller\nmodule, comprising a planner, an actor, and a memory pool, which uses the\nlearned causal graph to accomplish tasks; 4) a perception module, powered by\nmultimodal large language models, which enables ADAM to perceive like a human\nplayer. Extensive experiments show that ADAM constructs an almost perfect\ncausal graph from scratch, enabling efficient task decomposition and execution\nwith strong interpretability. Notably, in our modified Minecraft games where no\nprior knowledge is available, ADAM maintains its performance and shows\nremarkable robustness and generalization capability. ADAM pioneers a novel\nparadigm that integrates causal methods and embodied agents in a synergistic\nmanner. Our project page is at https://opencausalab.github.io/ADAM.",
      "tldr_zh": "本文提出 ADAM，一种在开放世界环境（如 Minecraft）中运作的具身因果代理（Embodied Causal Agent），旨在解决现有代理在学习结构化因果知识时的不透明性和过度依赖先验知识问题。ADAM 由四个关键组件组成：交互模块（执行并记录动作）、因果模型模块（从零构建不断增长的 causal graph 以提升可解释性）、控制器模块（利用 causal graph 进行任务规划和执行）、以及感知模块（基于 multimodal large language models 实现人类-like 感知）。实验结果表明，ADAM 能够构建几乎完美的 causal graph，支持高效任务分解，并在无先验知识的修改版 Minecraft 游戏中保持出色性能，展示出强大的鲁棒性和泛化能力，从而开创了因果方法与具身代理协同的新范式。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22194v1",
      "published_date": "2024-10-29 16:32:01 UTC",
      "updated_date": "2024-10-29 16:32:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:21:16.488493"
    },
    {
      "arxiv_id": "2410.22184v1",
      "title": "Multi-Level Feature Distillation of Joint Teachers Trained on Distinct Image Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Adrian Iordache",
        "Bogdan Alexe",
        "Radu Tudor Ionescu"
      ],
      "abstract": "We propose a novel teacher-student framework to distill knowledge from\nmultiple teachers trained on distinct datasets. Each teacher is first trained\nfrom scratch on its own dataset. Then, the teachers are combined into a joint\narchitecture, which fuses the features of all teachers at multiple\nrepresentation levels. The joint teacher architecture is fine-tuned on samples\nfrom all datasets, thus gathering useful generic information from all data\nsamples. Finally, we employ a multi-level feature distillation procedure to\ntransfer the knowledge to a student model for each of the considered datasets.\nWe conduct image classification experiments on seven benchmarks, and action\nrecognition experiments on three benchmarks. To illustrate the power of our\nfeature distillation procedure, the student architectures are chosen to be\nidentical to those of the individual teachers. To demonstrate the flexibility\nof our approach, we combine teachers with distinct architectures. We show that\nour novel Multi-Level Feature Distillation (MLFD) can significantly surpass\nequivalent architectures that are either trained on individual datasets, or\njointly trained on all datasets at once. Furthermore, we confirm that each step\nof the proposed training procedure is well motivated by a comprehensive\nablation study. We publicly release our code at\nhttps://github.com/AdrianIordache/MLFD.",
      "tldr_zh": "我们提出了一种新型 teacher-student 框架，用于从多个在不同数据集上训练的老师模型中提炼知识。具体方法包括先独立训练每个老师模型，然后融合它们成一个联合架构，并在多个表示级别上微调，以收集通用信息。随后，通过 Multi-Level Feature Distillation (MLFD) 将知识转移到学生模型，结果显示学生模型在七个图像分类基准和三个动作识别基准上显著超越单独训练或一次性联合训练的等效架构。该方法展示了高度灵活性，并通过消融研究验证了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.22184v1",
      "published_date": "2024-10-29 16:23:20 UTC",
      "updated_date": "2024-10-29 16:23:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:23:27.267703"
    },
    {
      "arxiv_id": "2410.22180v1",
      "title": "Natural Language Processing for Analyzing Electronic Health Records and Clinical Notes in Cancer Research: A Review",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Bilal",
        "Ameer Hamza",
        "Nadia Malik"
      ],
      "abstract": "Objective: This review aims to analyze the application of natural language\nprocessing (NLP) techniques in cancer research using electronic health records\n(EHRs) and clinical notes. This review addresses gaps in the existing\nliterature by providing a broader perspective than previous studies focused on\nspecific cancer types or applications. Methods: A comprehensive literature\nsearch was conducted using the Scopus database, identifying 94 relevant studies\npublished between 2019 and 2024. Data extraction included study\ncharacteristics, cancer types, NLP methodologies, dataset information,\nperformance metrics, challenges, and future directions. Studies were\ncategorized based on cancer types and NLP applications. Results: The results\nshowed a growing trend in NLP applications for cancer research, with breast,\nlung, and colorectal cancers being the most studied. Information extraction and\ntext classification emerged as predominant NLP tasks. A shift from rule-based\nto advanced machine learning techniques, particularly transformer-based models,\nwas observed. The Dataset sizes used in existing studies varied widely. Key\nchallenges included the limited generalizability of proposed solutions and the\nneed for improved integration into clinical workflows. Conclusion: NLP\ntechniques show significant potential in analyzing EHRs and clinical notes for\ncancer research. However, future work should focus on improving model\ngeneralizability, enhancing robustness in handling complex clinical language,\nand expanding applications to understudied cancer types. Integration of NLP\ntools into clinical practice and addressing ethical considerations remain\ncrucial for utilizing the full potential of NLP in enhancing cancer diagnosis,\ntreatment, and patient outcomes.",
      "tldr_zh": "这篇综述论文分析了自然语言处理（NLP）技术在癌症研究中使用电子健康记录（EHRs）和临床笔记的应用，旨在填补现有文献对特定癌症类型的局限，提供更广泛的视角。作者通过在Scopus数据库搜索94个相关研究（2019-2024年），提取数据包括研究特征、癌症类型、NLP方法、数据集、性能指标和挑战，并按癌症类型和NLP应用进行分类。结果显示，乳腺癌、肺癌和结肠直肠癌是最常研究的领域，信息提取和文本分类为主导任务，同时观察到从规则-based方法向transformer-based模型的转变。关键挑战包括模型泛化性差和临床工作流整合问题，未来应加强模型鲁棒性、扩展到未充分研究的癌症类型，并关注伦理问题以提升癌症诊断、治疗和患者预后。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22180v1",
      "published_date": "2024-10-29 16:17:07 UTC",
      "updated_date": "2024-10-29 16:17:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:23:32.184824"
    },
    {
      "arxiv_id": "2410.22177v1",
      "title": "Analyzing Multimodal Interaction Strategies for LLM-Assisted Manipulation of 3D Scenes",
      "title_zh": "翻译失败",
      "authors": [
        "Junlong Chen",
        "Jens Grubert",
        "Per Ola Kristensson"
      ],
      "abstract": "As more applications of large language models (LLMs) for 3D content for\nimmersive environments emerge, it is crucial to study user behaviour to\nidentify interaction patterns and potential barriers to guide the future design\nof immersive content creation and editing systems which involve LLMs. In an\nempirical user study with 12 participants, we combine quantitative usage data\nwith post-experience questionnaire feedback to reveal common interaction\npatterns and key barriers in LLM-assisted 3D scene editing systems. We identify\nopportunities for improving natural language interfaces in 3D design tools and\npropose design recommendations for future LLM-integrated 3D content creation\nsystems. Through an empirical study, we demonstrate that LLM-assisted\ninteractive systems can be used productively in immersive environments.",
      "tldr_zh": "这项研究分析了LLM辅助3D场景操作中的多模态交互策略，通过一项涉及12名参与者的实证用户研究，结合定量使用数据和后续问卷反馈，识别了常见交互模式和关键障碍。研究发现，这些障碍包括用户行为中的潜在问题，并提出了改进自然语言接口的建议，以指导未来LLM集成3D内容创建系统的设计。最终，研究证明了LLM辅助交互系统在沉浸式环境中的生产力应用潜力。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "under review",
      "pdf_url": "http://arxiv.org/pdf/2410.22177v1",
      "published_date": "2024-10-29 16:15:59 UTC",
      "updated_date": "2024-10-29 16:15:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:21:52.101051"
    },
    {
      "arxiv_id": "2410.22151v1",
      "title": "Standardization Trends on Safety and Trustworthiness Technology for Advanced AI",
      "title_zh": "高级 AI 安全与可信技术标准化趋势",
      "authors": [
        "Jonghong Jeon"
      ],
      "abstract": "Artificial Intelligence (AI) has rapidly evolved over the past decade and has\nadvanced in areas such as language comprehension, image and video recognition,\nprogramming, and scientific reasoning. Recent AI technologies based on large\nlanguage models and foundation models are approaching or surpassing artificial\ngeneral intelligence. These systems demonstrate superior performance in complex\nproblem solving, natural language processing, and multi-domain tasks, and can\npotentially transform fields such as science, industry, healthcare, and\neducation. However, these advancements have raised concerns regarding the\nsafety and trustworthiness of advanced AI, including risks related to\nuncontrollability, ethical conflicts, long-term socioeconomic impacts, and\nsafety assurance. Efforts are being expended to develop internationally\nagreed-upon standards to ensure the safety and reliability of AI. This study\nanalyzes international trends in safety and trustworthiness standardization for\nadvanced AI, identifies key areas for standardization, proposes future\ndirections and strategies, and draws policy implications. The goal is to\nsupport the safe and trustworthy development of advanced AI and enhance\ninternational competitiveness through effective standardization.",
      "tldr_zh": "该研究探讨了先进人工智能（AI）的安全和可信性技术标准化趋势。随着基于大型语言模型和基础模型的AI在复杂问题解决、自然语言处理和多领域任务上表现出色，其快速发展也引发了不可控性、伦理冲突和长期社会经济影响等风险。为应对这些挑战，论文分析了国际标准化努力，识别了关键标准化领域，并提出未来方向、策略和政策含义。最终目标是通过有效的标准化支持AI的安全可靠发展，提升国际竞争力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 2 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2410.22151v1",
      "published_date": "2024-10-29 15:50:24 UTC",
      "updated_date": "2024-10-29 15:50:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:22:01.904670"
    },
    {
      "arxiv_id": "2410.22134v2",
      "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoniu Song",
        "Zihang Zhong",
        "Rong Chen",
        "Haibo Chen"
      ],
      "abstract": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
      "tldr_zh": "这篇论文针对大语言模型(LLMs)在边缘设备上受限于GPU内存容量的问题，提出ProMoE系统，这是一种基于Mixture-of-Experts(MoE)模型的快速服务框架，利用主动缓存技术。ProMoE通过分析中间结果来预测后续专家使用，并提前获取专家，从而消除被动缓存缺失并减少性能开销。实验评估显示，与现有卸载解决方案相比，ProMoE在预填充阶段平均加速2.20x（最高3.21x），在解码阶段平均加速2.07x（最高5.02x），显著提升了系统效率。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22134v2",
      "published_date": "2024-10-29 15:31:27 UTC",
      "updated_date": "2025-02-08 14:11:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:23:49.397265"
    },
    {
      "arxiv_id": "2410.22135v2",
      "title": "Lightweight Frequency Masker for Cross-Domain Few-Shot Semantic Segmentation",
      "title_zh": "轻量级频率掩码器用于跨域少样本语义分割",
      "authors": [
        "Jintao Tong",
        "Yixiong Zou",
        "Yuhua Li",
        "Ruixuan Li"
      ],
      "abstract": "Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train\nthe model on a large-scale source-domain dataset, and then transfer the model\nto data-scarce target-domain datasets for pixel-level segmentation. The\nsignificant domain gap between the source and target datasets leads to a sharp\ndecline in the performance of existing few-shot segmentation (FSS) methods in\ncross-domain scenarios. In this work, we discover an intriguing phenomenon:\nsimply filtering different frequency components for target domains can lead to\na significant performance improvement, sometimes even as high as 14% mIoU.\nThen, we delve into this phenomenon for an interpretation, and find such\nimprovements stem from the reduced inter-channel correlation in feature maps,\nwhich benefits CD-FSS with enhanced robustness against domain gaps and larger\nactivated regions for segmentation. Based on this, we propose a lightweight\nfrequency masker, which further reduces channel correlations by an\nAmplitude-Phase Masker (APM) module and an Adaptive Channel Phase Attention\n(ACPA) module. Notably, APM introduces only 0.01% additional parameters but\nimproves the average performance by over 10%, and ACPA imports only 2.5%\nparameters but further improves the performance by over 1.5%, which\nsignificantly surpasses the state-of-the-art CD-FSS methods.",
      "tldr_zh": "这篇论文针对 Cross-domain Few-Shot Semantic Segmentation (CD-FSS) 的领域差距问题，提出了一种轻量级频率掩码器，以提升模型从源域数据集转移到数据稀缺目标域的像素级分割性能。研究发现，通过简单过滤目标域的频率成分，可以减少特征图的通道间相关性，从而增强模型对领域差距的鲁棒性和激活区域的扩大，性能提升可达 14% mIoU。基于此，该方法引入 Amplitude-Phase Masker (APM) 和 Adaptive Channel Phase Attention (ACPA) 模块，其中 APM 只增加 0.01% 参数却提高平均性能超过 10%，ACPA 增加 2.5% 参数进一步提升 1.5%，整体显著优于现有 CD-FSS 方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.22135v2",
      "published_date": "2024-10-29 15:31:27 UTC",
      "updated_date": "2024-11-22 06:41:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:24:20.735785"
    },
    {
      "arxiv_id": "2410.22130v2",
      "title": "Solving Epistemic Logic Programs using Generate-and-Test with Propagation",
      "title_zh": "翻译失败",
      "authors": [
        "Jorge Fandinno",
        "Lute Lillo"
      ],
      "abstract": "This paper introduces a general framework for generate-and-test-based solvers\nfor epistemic logic programs that can be instantiated with different generator\nand tester programs, and we prove sufficient conditions on those programs for\nthe correctness of the solvers built using this framework. It also introduces a\nnew generator program that incorporates the propagation of epistemic\nconsequences and shows that this can exponentially reduce the number of\ncandidates that need to be tested while only incurring a linear overhead. We\nimplement a new solver based on these theoretical findings and experimentally\nshow that it outperforms existing solvers by achieving a ~3.3x speed-up and\nsolving 91% more instances on well-known benchmarks.",
      "tldr_zh": "本文提出一个通用的基于生成和测试(generate-and-test)框架，用于求解认识论逻辑程序(epistemic logic programs)，并证明了生成器和测试器程序的充分条件以确保求解器正确性。框架中引入一个新生成器程序，融入了认识论后果的传播(propagation)，这能指数级减少需要测试的候选数，同时仅增加线性开销。主要实验结果显示，该新求解器在知名基准上比现有求解器快约3.3倍，并成功解决91%更多的实例。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication in the Proceedings of the 39th Annual AAAI\n  Conference on Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2410.22130v2",
      "published_date": "2024-10-29 15:28:40 UTC",
      "updated_date": "2024-12-13 16:05:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:24:32.072872"
    },
    {
      "arxiv_id": "2410.22129v1",
      "title": "Improving Performance of Commercially Available AI Products in a Multi-Agent Configuration",
      "title_zh": "翻译失败",
      "authors": [
        "Cory Hymel",
        "Sida Peng",
        "Kevin Xu",
        "Charath Ranganathan"
      ],
      "abstract": "In recent years, with the rapid advancement of large language models (LLMs),\nmulti-agent systems have become increasingly more capable of practical\napplication. At the same time, the software development industry has had a\nnumber of new AI-powered tools developed that improve the software development\nlifecycle (SDLC). Academically, much attention has been paid to the role of\nmulti-agent systems to the SDLC. And, while single-agent systems have\nfrequently been examined in real-world applications, we have seen comparatively\nfew real-world examples of publicly available commercial tools working together\nin a multi-agent system with measurable improvements. In this experiment we\ntest context sharing between Crowdbotics PRD AI, a tool for generating software\nrequirements using AI, and GitHub Copilot, an AI pair-programming tool. By\nsharing business requirements from PRD AI, we improve the code suggestion\ncapabilities of GitHub Copilot by 13.8% and developer task success rate by\n24.5% -- demonstrating a real-world example of commercially-available AI\nsystems working together with improved outcomes.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 驱动的多智能体系统在软件开发生命周期 (SDLC) 中的实际应用，通过测试商用 AI 工具的协作来提升性能。实验重点考察了 Crowdbotics PRD AI（用于生成软件需求）和 GitHub Copilot（AI 配对编程工具）之间的上下文共享机制。结果显示，此配置将 GitHub Copilot 的代码建议能力提高了 13.8%，并将开发人员任务成功率提升 24.5%，为商用 AI 产品在多智能体系统中协同工作的真实案例提供了实证支持。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "7 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.22129v1",
      "published_date": "2024-10-29 15:28:19 UTC",
      "updated_date": "2024-10-29 15:28:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:24:44.267587"
    },
    {
      "arxiv_id": "2410.22120v1",
      "title": "Vision Paper: Designing Graph Neural Networks in Compliance with the European Artificial Intelligence Act",
      "title_zh": "愿景论文：设计符合欧洲人工智能法案的图神经网络",
      "authors": [
        "Barbara Hoffmann",
        "Jana Vatter",
        "Ruben Mayer"
      ],
      "abstract": "The European Union's Artificial Intelligence Act (AI Act) introduces\ncomprehensive guidelines for the development and oversight of Artificial\nIntelligence (AI) and Machine Learning (ML) systems, with significant\nimplications for Graph Neural Networks (GNNs). This paper addresses the unique\nchallenges posed by the AI Act for GNNs, which operate on complex\ngraph-structured data. The legislation's requirements for data management, data\ngovernance, robustness, human oversight, and privacy necessitate tailored\nstrategies for GNNs. Our study explores the impact of these requirements on GNN\ntraining and proposes methods to ensure compliance. We provide an in-depth\nanalysis of bias, robustness, explainability, and privacy in the context of\nGNNs, highlighting the need for fair sampling strategies and effective\ninterpretability techniques. Our contributions fill the research gap by\noffering specific guidance for GNNs under the new legislative framework and\nidentifying open questions and future research directions.",
      "tldr_zh": "这篇论文探讨了如何设计图神经网络（Graph Neural Networks, GNNs）以符合欧盟人工智能法案（European Artificial Intelligence Act, AI Act）的要求，特别是针对GNNs处理复杂图结构数据所带来的挑战。论文分析了AI Act在数据管理、治理、鲁棒性、人为监督和隐私方面的规定，并提出针对GNNs的合规策略，包括公平采样策略和可解释性技术，以缓解偏差、鲁棒性和隐私问题。最终，研究填补了这一领域的空白，提供具体指导，并指出了未来研究方向如开放问题和改进路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22120v1",
      "published_date": "2024-10-29 15:22:45 UTC",
      "updated_date": "2024-10-29 15:22:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:24:55.383765"
    },
    {
      "arxiv_id": "2410.22118v2",
      "title": "The Impact of Inference Acceleration on Bias of LLMs",
      "title_zh": "推理加速对LLMs偏差的影响",
      "authors": [
        "Elisabeth Kirsten",
        "Ivan Habernal",
        "Vedant Nanda",
        "Muhammad Bilal Zafar"
      ],
      "abstract": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
      "tldr_zh": "本文探讨了推理加速策略（如量化、修剪和缓存）对大型语言模型(LLMs)生成偏差的影响，这些策略虽能显著降低推理成本和延迟，同时保持预测性能，但可能导致模型输出中的人口统计学偏差发生复杂变化。研究通过多种指标从不同角度比较加速前后模型输出，发现偏差效果不可预测：某些策略组合在特定模型中偏差变化小，而在其他模型中则放大明显。作者强调，需要针对每个模型进行深入的个案评估，以确保加速优化不会引入不可控的偏见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22118v2",
      "published_date": "2024-10-29 15:19:13 UTC",
      "updated_date": "2025-02-19 11:10:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:25:07.700348"
    },
    {
      "arxiv_id": "2410.22114v2",
      "title": "Policy Gradient for Robust Markov Decision Processes",
      "title_zh": "鲁棒 Markov 决策过程的策略梯度",
      "authors": [
        "Qiuhao Wang",
        "Shaohang Xu",
        "Chin Pang Ho",
        "Marek Petrik"
      ],
      "abstract": "We develop a generic policy gradient method with the global optimality\nguarantee for robust Markov Decision Processes (MDPs). While policy gradient\nmethods are widely used for solving dynamic decision problems due to their\nscalable and efficient nature, adapting these methods to account for model\nambiguity has been challenging, often making it impractical to learn robust\npolicies. This paper introduces a novel policy gradient method, Double-Loop\nRobust Policy Mirror Descent (DRPMD), for solving robust MDPs. DRPMD employs a\ngeneral mirror descent update rule for the policy optimization with adaptive\ntolerance per iteration, guaranteeing convergence to a globally optimal policy.\nWe provide a comprehensive analysis of DRPMD, including new convergence results\nunder both direct and softmax parameterizations, and provide novel insights\ninto the inner problem solution through Transition Mirror Ascent (TMA).\nAdditionally, we propose innovative parametric transition kernels for both\ndiscrete and continuous state-action spaces, broadening the applicability of\nour approach. Empirical results validate the robustness and global convergence\nof DRPMD across various challenging robust MDP settings.",
      "tldr_zh": "这篇论文提出了一种通用的策略梯度方法，用于解决鲁棒马尔可夫决策过程 (robust Markov Decision Processes, robust MDPs)，并保证全局最优策略。作者引入了Double-Loop Robust Policy Mirror Descent (DRPMD) 方法，该方法采用镜像下降更新规则和自适应容忍机制，实现策略优化的收敛。论文还提供了在直接和 softmax 参数化下的新收敛结果，并通过Transition Mirror Ascent (TMA) 分析内部问题解决方案，同时提出创新的参数化转移核，适用于离散和连续状态-动作空间。经验结果证明，DRPMD 在各种鲁棒MDP设置中显示出优秀的鲁棒性和全局收敛性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22114v2",
      "published_date": "2024-10-29 15:16:02 UTC",
      "updated_date": "2024-10-31 15:34:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:25:21.267960"
    },
    {
      "arxiv_id": "2410.22108v2",
      "title": "Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench",
      "title_zh": "使用 MLLMU-Bench 保护多模态大型语言模型中的隐私",
      "authors": [
        "Zheyuan Liu",
        "Guangyao Dou",
        "Mengzhao Jia",
        "Zhaoxuan Tan",
        "Qingkai Zeng",
        "Yongle Yuan",
        "Meng Jiang"
      ],
      "abstract": "Generative models such as Large Language Models (LLM) and Multimodal Large\nLanguage models (MLLMs) trained on massive web corpora can memorize and\ndisclose individuals' confidential and private data, raising legal and ethical\nconcerns. While many previous works have addressed this issue in LLM via\nmachine unlearning, it remains largely unexplored for MLLMs. To tackle this\nchallenge, we introduce Multimodal Large Language Model Unlearning Benchmark\n(MLLMU-Bench), a novel benchmark aimed at advancing the understanding of\nmultimodal machine unlearning. MLLMU-Bench consists of 500 fictitious profiles\nand 153 profiles for public celebrities, each profile feature over 14\ncustomized question-answer pairs, evaluated from both multimodal (image+text)\nand unimodal (text) perspectives. The benchmark is divided into four sets to\nassess unlearning algorithms in terms of efficacy, generalizability, and model\nutility. Finally, we provide baseline results using existing generative model\nunlearning algorithms. Surprisingly, our experiments show that unimodal\nunlearning algorithms excel in generation and cloze tasks, while multimodal\nunlearning approaches perform better in classification tasks with multimodal\ninputs.",
      "tldr_zh": "该研究针对多模态大语言模型(MLLMs)可能记忆并泄露私人数据的隐私风险，引入了Multimodal Large Language Model Unlearning Benchmark (MLLMU-Bench)基准，以推进机器遗忘(machine unlearning)技术的开发。MLLMU-Bench包括500个虚构配置文件和153个公众名人配置文件，每个配置文件包含超过14个定制的问答对，并从多模态(图像+文本)和单模态(文本)角度进行评估。基准分为四个集合，系统评估遗忘算法的有效性、可推广性和模型效用。实验结果显示，现有的单模态遗忘算法在生成和填空任务中表现更佳，而多模态遗忘方法在多模态输入的分类任务中更具优势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL Main 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.22108v2",
      "published_date": "2024-10-29 15:07:23 UTC",
      "updated_date": "2025-02-14 22:08:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:25:33.508712"
    },
    {
      "arxiv_id": "2410.22105v2",
      "title": "DAGE: DAG Query Answering via Relational Combinator with Logical Constraints",
      "title_zh": "翻译失败",
      "authors": [
        "Yunjie He",
        "Bo Xiong",
        "Daniel Hernández",
        "Yuqicheng Zhu",
        "Evgeny Kharlamov",
        "Steffen Staab"
      ],
      "abstract": "Predicting answers to queries over knowledge graphs is called a complex\nreasoning task because answering a query requires subdividing it into\nsubqueries. Existing query embedding methods use this decomposition to compute\nthe embedding of a query as the combination of the embedding of the subqueries.\nThis requirement limits the answerable queries to queries having a single free\nvariable and being decomposable, which are called tree-form queries and\ncorrespond to the $\\mathcal{SROI}^-$ description logic. In this paper, we\ndefine a more general set of queries, called DAG queries and formulated in the\n$\\mathcal{ALCOIR}$ description logic, propose a query embedding method for\nthem, called DAGE, and a new benchmark to evaluate query embeddings on them.\nGiven the computational graph of a DAG query, DAGE combines the possibly\nmultiple paths between two nodes into a single path with a trainable operator\nthat represents the intersection of relations and learns DAG-DL from\ntautologies. We show that it is possible to implement DAGE on top of existing\nquery embedding methods, and we empirically measure the improvement of our\nmethod over the results of vanilla methods evaluated in tree-form queries that\napproximate the DAG queries of our proposed benchmark.",
      "tldr_zh": "这篇论文扩展了知识图谱查询回答的研究，从现有的树形查询（tree-form queries，对应$\\mathcal{SROI}^-$描述逻辑）扩展到更一般的DAG queries（对应$\\mathcal{ALCOIR}$描述逻辑），并提出DAGE方法来处理这些查询。DAGE使用关系组合器（relational combinator）将DAG查询的计算图中多路径合并为单一路径，并通过可训练的操作符表示关系的交集，同时从逻辑公理（tautologies）中学习DAG-DL，以提升查询嵌入的准确性。实验结果表明，DAGE在新的基准数据集上显著改善了基线方法的性能，平均提高了查询回答的精确度。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Accepted at WWW2025",
      "pdf_url": "http://arxiv.org/pdf/2410.22105v2",
      "published_date": "2024-10-29 15:02:48 UTC",
      "updated_date": "2025-02-12 12:02:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:25:44.510086"
    },
    {
      "arxiv_id": "2410.22101v2",
      "title": "Hyperspectral Imaging-Based Perception in Autonomous Driving Scenarios: Benchmarking Baseline Semantic Segmentation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Imad Ali Shah",
        "Jiarong Li",
        "Martin Glavin",
        "Edward Jones",
        "Enda Ward",
        "Brian Deegan"
      ],
      "abstract": "Hyperspectral Imaging (HSI) is known for its advantages over traditional RGB\nimaging in remote sensing, agriculture, and medicine. Recently, it has gained\nattention for enhancing Advanced Driving Assistance Systems (ADAS) perception.\nSeveral HSI datasets such as HyKo, HSI-Drive, HSI-Road, and Hyperspectral City\nhave been made available. However, a comprehensive evaluation of semantic\nsegmentation models (SSM) using these datasets is lacking. To address this gap,\nwe evaluated the available annotated HSI datasets on four deep learning-based\nbaseline SSMs: DeepLab v3+, HRNet, PSPNet, and U-Net, along with its two\nvariants: Coordinate Attention (UNet-CA) and Convolutional Block-Attention\nModule (UNet-CBAM). The original model architectures were adapted to handle the\nvarying spatial and spectral dimensions of the datasets. These baseline SSMs\nwere trained using a class-weighted loss function for individual HSI datasets\nand evaluated using mean-based metrics such as intersection over union (IoU),\nrecall, precision, F1 score, specificity, and accuracy. Our results indicate\nthat UNet-CBAM, which extracts channel-wise features, outperforms other SSMs\nand shows potential to leverage spectral information for enhanced semantic\nsegmentation. This study establishes a baseline SSM benchmark on available\nannotated datasets for future evaluation of HSI-based ADAS perception. However,\nlimitations of current HSI datasets, such as limited dataset size, high class\nimbalance, and lack of fine-grained annotations, remain significant constraints\nfor developing robust SSMs for ADAS applications.",
      "tldr_zh": "本研究评估了 Hyperspectral Imaging (HSI) 在自动驾驶场景中的感知应用，针对现有数据集（如 HyKo 和 HSI-Drive）对多个基线语义分割模型（SSM）进行了基准测试，包括 DeepLab v3+, HRNet, PSPNet, U-Net 及其变体 UNet-CA 和 UNet-CBAM。模型架构被调整以处理 HSI 的空间和光谱维度，并使用类加权损失函数训练和评估指标如 IoU、recall、precision、F1 score 等。结果显示 UNet-CBAM 通过提取通道特征表现出最佳性能，能够更好地利用光谱信息提升语义分割准确性。该研究为 HSI-based Advanced Driving Assistance Systems (ADAS) 感知建立了基准，但当前数据集的局限性（如大小有限和类别不平衡）仍需进一步解决。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted and Presented at IEEE WHISPERS 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.22101v2",
      "published_date": "2024-10-29 14:54:13 UTC",
      "updated_date": "2024-12-12 16:46:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:25:56.969483"
    },
    {
      "arxiv_id": "2410.22099v4",
      "title": "TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point Clouds",
      "title_zh": "翻译失败",
      "authors": [
        "Yui Lo",
        "Yuqian Chen",
        "Dongnan Liu",
        "Jon Haitz Legarreta",
        "Leo Zekelman",
        "Fan Zhang",
        "Jarrett Rushmore",
        "Yogesh Rathi",
        "Nikos Makris",
        "Alexandra J. Golby",
        "Weidong Cai",
        "Lauren J. O'Donnell"
      ],
      "abstract": "Brain imaging studies have demonstrated that diffusion MRI tractography\ngeometric shape descriptors can inform the study of the brain's white matter\npathways and their relationship to brain function. In this work, we investigate\nthe possibility of utilizing a deep learning model to compute shape measures of\nthe brain's white matter connections. We introduce a novel framework,\nTractShapeNet, that leverages a point cloud representation of tractography to\ncompute five shape measures: length, span, volume, total surface area, and\nirregularity. We assess the performance of the method on a large dataset\nincluding 1065 healthy young adults. Experiments for shape measure computation\ndemonstrate that our proposed TractShapeNet outperforms other point cloud-based\nneural network models in both the Pearson correlation coefficient and\nnormalized error metrics. We compare the inference runtime results with the\nconventional shape computation tool DSI-Studio. Our results demonstrate that a\ndeep learning approach enables faster and more efficient shape measure\ncomputation. We also conduct experiments on two downstream language cognition\nprediction tasks, showing that shape measures from TractShapeNet perform\nsimilarly to those computed by DSI-Studio. Our code will be available at:\nhttps://github.com/SlicerDMRI/TractShapeNet.",
      "tldr_zh": "本研究提出TractShapeNet框架，利用3D点云表示和深度学习模型，高效计算脑白质通路的五个形状测量：长度、跨度、体积、总表面面积和不规则性。\n在包含1065名健康年轻成年人的大型数据集上，TractShapeNet在Pearson correlation coefficient和标准化错误指标上优于其他点云神经网络模型，且推理运行时间比传统工具DSI-Studio更快。\n此外，在两个下游语言认知预测任务中，TractShapeNet的形状测量性能与DSI-Studio相当，展示了其在脑成像研究中的潜在应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 2 figures, 4 tables. This work has been accepted to 2025\n  IEEE 22nd International Symposium on Biomedical Imaging (ISBI)",
      "pdf_url": "http://arxiv.org/pdf/2410.22099v4",
      "published_date": "2024-10-29 14:53:10 UTC",
      "updated_date": "2025-02-14 14:46:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:26:09.024837"
    },
    {
      "arxiv_id": "2411.00041v1",
      "title": "NeuroSym-BioCAT: Leveraging Neuro-Symbolic Methods for Biomedical Scholarly Document Categorization and Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Parvez Zamil",
        "Gollam Rabby",
        "Md. Sadekur Rahman",
        "Sören Auer"
      ],
      "abstract": "The growing volume of biomedical scholarly document abstracts presents an\nincreasing challenge in efficiently retrieving accurate and relevant\ninformation. To address this, we introduce a novel approach that integrates an\noptimized topic modelling framework, OVB-LDA, with the BI-POP CMA-ES\noptimization technique for enhanced scholarly document abstract categorization.\nComplementing this, we employ the distilled MiniLM model, fine-tuned on\ndomain-specific data, for high-precision answer extraction. Our approach is\nevaluated across three configurations: scholarly document abstract retrieval,\ngold-standard scholarly documents abstract, and gold-standard snippets,\nconsistently outperforming established methods such as RYGH and bio-answer\nfinder. Notably, we demonstrate that extracting answers from scholarly\ndocuments abstracts alone can yield high accuracy, underscoring the sufficiency\nof abstracts for many biomedical queries. Despite its compact size, MiniLM\nexhibits competitive performance, challenging the prevailing notion that only\nlarge, resource-intensive models can handle such complex tasks. Our results,\nvalidated across various question types and evaluation batches, highlight the\nrobustness and adaptability of our method in real-world biomedical\napplications. While our approach shows promise, we identify challenges in\nhandling complex list-type questions and inconsistencies in evaluation metrics.\nFuture work will focus on refining the topic model with more extensive\ndomain-specific datasets, further optimizing MiniLM and utilizing large\nlanguage models (LLM) to improve both precision and efficiency in biomedical\nquestion answering.",
      "tldr_zh": "本研究提出NeuroSym-BioCAT框架，利用神经符号方法（Neuro-Symbolic Methods）整合OVB-LDA主题建模框架与BI-POP CMA-ES优化技术，来提升生物医学学术文档摘要的分类和问答效率。方法通过fine-tuned MiniLM模型进行高精度答案提取，并在文档摘要检索、金标准摘要和片段等配置中，优于RYGH和bio-answer finder等现有方法，证明文档摘要本身即可实现高准确率问答。实验结果显示MiniLM尽管体积小巧，却展现出强劲性能，但仍存在处理复杂列表型问题和评估指标不一致的挑战，未来将优化主题模型并整合大型语言模型（LLM）以进一步提高精度和效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DL",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00041v1",
      "published_date": "2024-10-29 14:45:12 UTC",
      "updated_date": "2024-10-29 14:45:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:26:22.066827"
    },
    {
      "arxiv_id": "2411.00040v1",
      "title": "P$^2$C$^2$Net: PDE-Preserved Coarse Correction Network for efficient prediction of spatiotemporal dynamics",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Wang",
        "Pu Ren",
        "Hao Zhou",
        "Xin-Yang Liu",
        "Zhiwen Deng",
        "Yi Zhang",
        "Ruizhi Chengze",
        "Hongsheng Liu",
        "Zidong Wang",
        "Jian-Xun Wang",
        "Ji-Rong_Wen",
        "Hao Sun",
        "Yang Liu"
      ],
      "abstract": "When solving partial differential equations (PDEs), classical numerical\nmethods often require fine mesh grids and small time stepping to meet\nstability, consistency, and convergence conditions, leading to high\ncomputational cost. Recently, machine learning has been increasingly utilized\nto solve PDE problems, but they often encounter challenges related to\ninterpretability, generalizability, and strong dependency on rich labeled data.\nHence, we introduce a new PDE-Preserved Coarse Correction Network\n(P$^2$C$^2$Net) to efficiently solve spatiotemporal PDE problems on coarse mesh\ngrids in small data regimes. The model consists of two synergistic modules: (1)\na trainable PDE block that learns to update the coarse solution (i.e., the\nsystem state), based on a high-order numerical scheme with boundary condition\nencoding, and (2) a neural network block that consistently corrects the\nsolution on the fly. In particular, we propose a learnable symmetric Conv\nfilter, with weights shared over the entire model, to accurately estimate the\nspatial derivatives of PDE based on the neural-corrected system state. The\nresulting physics-encoded model is capable of handling limited training data\n(e.g., 3--5 trajectories) and accelerates the prediction of PDE solutions on\ncoarse spatiotemporal grids while maintaining a high accuracy. P$^2$C$^2$Net\nachieves consistent state-of-the-art performance with over 50\\% gain (e.g., in\nterms of relative prediction error) across four datasets covering complex\nreaction-diffusion processes and turbulent flows.",
      "tldr_zh": "这篇论文提出 P$^2$C$^2$Net，一种 PDE-Preserved Coarse Correction Network，用于在粗网格上高效预测时空动态，解决传统数值方法计算成本高和机器学习方法数据依赖等问题。模型由两个协同模块组成：一个可训练的 PDE 块，基于高阶数值方案和边界条件编码更新粗解决方案；以及一个神经网络块，通过可学习的对称 Conv 过滤器实时修正系统状态并估计空间导数。该框架在小数据环境下（如3-5个轨迹）表现出色，显著加速 PDE 解决方案预测，同时在四个数据集上实现最先进性能，提升超过50%的相对预测错误率，适用于复杂反应-扩散过程和湍流。",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.LG",
        "cs.NA"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00040v1",
      "published_date": "2024-10-29 14:45:07 UTC",
      "updated_date": "2024-10-29 14:45:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:27:18.837720"
    },
    {
      "arxiv_id": "2410.22077v1",
      "title": "Mapping the Neuro-Symbolic AI Landscape by Architectures: A Handbook on Augmenting Deep Learning Through Symbolic Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Jonathan Feldstein",
        "Paulius Dilkas",
        "Vaishak Belle",
        "Efthymia Tsamoura"
      ],
      "abstract": "Integrating symbolic techniques with statistical ones is a long-standing\nproblem in artificial intelligence. The motivation is that the strengths of\neither area match the weaknesses of the other, and $\\unicode{x2013}$ by\ncombining the two $\\unicode{x2013}$ the weaknesses of either method can be\nlimited. Neuro-symbolic AI focuses on this integration where the statistical\nmethods are in particular neural networks. In recent years, there has been\nsignificant progress in this research field, where neuro-symbolic systems\noutperformed logical or neural models alone. Yet, neuro-symbolic AI is,\ncomparatively speaking, still in its infancy and has not been widely adopted by\nmachine learning practitioners. In this survey, we present the first mapping of\nneuro-symbolic techniques into families of frameworks based on their\narchitectures, with several benefits: Firstly, it allows us to link different\nstrengths of frameworks to their respective architectures. Secondly, it allows\nus to illustrate how engineers can augment their neural networks while treating\nthe symbolic methods as black-boxes. Thirdly, it allows us to map most of the\nfield so that future researchers can identify closely related frameworks.",
      "tldr_zh": "这篇论文探讨了神经符号 AI（Neuro-Symbolic AI）的最新进展，旨在通过整合符号技术和神经网络（如neural networks）来弥补各自的弱点。作者首次基于架构将神经符号技术分类为不同框架家族，从而链接框架的优势与它们的结构设计。该方法允许工程师增强深度学习模型，同时将符号推理（symbolic reasoning）视为黑箱子操作，并为未来研究者提供一个全面的领域映射，以识别相关框架。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "57 pages",
      "pdf_url": "http://arxiv.org/pdf/2410.22077v1",
      "published_date": "2024-10-29 14:35:59 UTC",
      "updated_date": "2024-10-29 14:35:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:26:45.100497"
    },
    {
      "arxiv_id": "2410.22066v1",
      "title": "Sing it, Narrate it: Quality Musical Lyrics Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuorui Ye",
        "Jinhan Li",
        "Rongwu Xu"
      ],
      "abstract": "Translating lyrics for musicals presents unique challenges due to the need to\nensure high translation quality while adhering to singability requirements such\nas length and rhyme. Existing song translation approaches often prioritize\nthese singability constraints at the expense of translation quality, which is\ncrucial for musicals. This paper aims to enhance translation quality while\nmaintaining key singability features. Our method consists of three main\ncomponents. First, we create a dataset to train reward models for the automatic\nevaluation of translation quality. Second, to enhance both singability and\ntranslation quality, we implement a two-stage training process with filtering\ntechniques. Finally, we introduce an inference-time optimization framework for\ntranslating entire songs. Extensive experiments, including both automatic and\nhuman evaluations, demonstrate significant improvements over baseline methods\nand validate the effectiveness of each component in our approach.",
      "tldr_zh": "本研究针对音乐剧歌词翻译的独特挑战，提出一种方法来提升翻译质量，同时保持关键的可唱性要求（如长度和韵律），以克服现有方法在优先可唱性时牺牲翻译准确性的问题。该方法包括三个主要组件：首先，创建数据集训练reward models用于自动评估翻译质量；其次，采用two-stage training过程结合filtering techniques来增强可唱性和翻译质量；最后，引入inference-time optimization framework在推理时优化整首歌的翻译。实验通过自动和人工评估证明，该方法显著优于基线模型，并验证了每个组件的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22066v1",
      "published_date": "2024-10-29 14:23:56 UTC",
      "updated_date": "2024-10-29 14:23:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:26:58.167752"
    },
    {
      "arxiv_id": "2411.00039v1",
      "title": "Linear Chain Transformation: Expanding Optimization Dynamics for Fine-Tuning Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yulong Wang",
        "Chang Zuo",
        "Yin Xuan",
        "Hong Li",
        "Ni Wei"
      ],
      "abstract": "Fine-tuning large language models (LLMs) has become essential for adapting\npretrained models to specific downstream tasks. In this paper, we propose\nLinear Chain Transformation (LinChain), a novel approach that introduces a\nsequence of linear transformations during fine-tuning to enrich optimization\ndynamics. By incorporating multiple linear transformations into the parameter\nupdate process, LinChain expands the effective rank of updates and enhances the\nmodel's ability to learn complex task-specific representations. We demonstrate\nthat this method significantly improves the performance of LLM fine-tuning over\nstate-of-the-art methods by providing more flexible optimization paths during\ntraining, while maintaining the inference efficiency of the resulting model.\nOur experiments on various benchmark tasks show that LinChain leads to better\ngeneralization, fewer learnable parameters, and improved task adaptation,\nmaking it a compelling strategy for LLM fine-tuning.",
      "tldr_zh": "本文提出 Linear Chain Transformation (LinChain)，一种在微调大型语言模型 (LLMs) 时引入序列线性变换的新方法，以丰富优化动态并扩展参数更新的有效秩。LinChain 通过在参数更新过程中添加多个线性变换，提升模型学习复杂任务特定表示的能力，同时保持推理效率。实验在各种基准任务上表明，该方法比现有最先进方法表现更优，实现更好的泛化、更少的可学习参数和任务适应。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 2 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.00039v1",
      "published_date": "2024-10-29 14:07:24 UTC",
      "updated_date": "2024-10-29 14:07:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:27:08.910785"
    },
    {
      "arxiv_id": "2411.00038v2",
      "title": "Topic-Conversation Relevance (TCR) Dataset and Benchmarks",
      "title_zh": "翻译失败",
      "authors": [
        "Yaran Fan",
        "Jamie Pool",
        "Senja Filipi",
        "Ross Cutler"
      ],
      "abstract": "Workplace meetings are vital to organizational collaboration, yet a large\npercentage of meetings are rated as ineffective. To help improve meeting\neffectiveness by understanding if the conversation is on topic, we create a\ncomprehensive Topic-Conversation Relevance (TCR) dataset that covers a variety\nof domains and meeting styles. The TCR dataset includes 1,500 unique meetings,\n22 million words in transcripts, and over 15,000 meeting topics, sourced from\nboth newly collected Speech Interruption Meeting (SIM) data and existing public\ndatasets. Along with the text data, we also open source scripts to generate\nsynthetic meetings or create augmented meetings from the TCR dataset to enhance\ndata diversity. For each data source, benchmarks are created using GPT-4 to\nevaluate the model accuracy in understanding transcription-topic relevance.",
      "tldr_zh": "本文构建了 Topic-Conversation Relevance (TCR) 数据集和基准，以评估工作场所会议的主题相关性，从而提升会议有效性。TCR 数据集涵盖多种领域和会议风格，包括 1,500 个独特会议、22 百万字的转录文本以及超过 15,000 个会议主题，数据来源于新收集的 Speech Interruption Meeting (SIM) 数据和现有公共数据集。研究还开源了脚本，用于生成合成会议或增强现有数据以增加多样性。最终，使用 GPT-4 创建基准，评估模型在理解转录-主题相关性方面的准确性，为会议优化提供实用工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To be published in 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024) Track on Datasets and Benchmarks",
      "pdf_url": "http://arxiv.org/pdf/2411.00038v2",
      "published_date": "2024-10-29 13:55:17 UTC",
      "updated_date": "2024-11-04 03:40:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:27:22.490739"
    },
    {
      "arxiv_id": "2411.00832v1",
      "title": "Advanced Hybrid Deep Learning Model for Enhanced Classification of Osteosarcoma Histopathology Images",
      "title_zh": "翻译失败",
      "authors": [
        "Arezoo Borji",
        "Gernot Kronreif",
        "Bernhard Angermayr",
        "Sepideh Hatamikia"
      ],
      "abstract": "Recent advances in machine learning are transforming medical image analysis,\nparticularly in cancer detection and classification. Techniques such as deep\nlearning, especially convolutional neural networks (CNNs) and vision\ntransformers (ViTs), are now enabling the precise analysis of complex\nhistopathological images, automating detection, and enhancing classification\naccuracy across various cancer types. This study focuses on osteosarcoma (OS),\nthe most common bone cancer in children and adolescents, which affects the long\nbones of the arms and legs. Early and accurate detection of OS is essential for\nimproving patient outcomes and reducing mortality. However, the increasing\nprevalence of cancer and the demand for personalized treatments create\nchallenges in achieving precise diagnoses and customized therapies. We propose\na novel hybrid model that combines convolutional neural networks (CNN) and\nvision transformers (ViT) to improve diagnostic accuracy for OS using\nhematoxylin and eosin (H&E) stained histopathological images. The CNN model\nextracts local features, while the ViT captures global patterns from\nhistopathological images. These features are combined and classified using a\nMulti-Layer Perceptron (MLP) into four categories: non-tumor (NT), non-viable\ntumor (NVT), viable tumor (VT), and none-viable ratio (NVR). Using the Cancer\nImaging Archive (TCIA) dataset, the model achieved an accuracy of 99.08%,\nprecision of 99.10%, recall of 99.28%, and an F1-score of 99.23%. This is the\nfirst successful four-class classification using this dataset, setting a new\nbenchmark in OS research and offering promising potential for future diagnostic\nadvancements.",
      "tldr_zh": "这篇论文提出了一种先进的混合深度学习模型，用于提升骨肉瘤(Osteosarcoma)组织病理图像的分类准确性，针对H&E染色图像进行四类诊断。模型结合卷积神经网络(CNN)提取局部特征、视觉Transformer(ViT)捕获全局模式，并通过多层感知器(MLP)整合这些特征以分类为非肿瘤(NT)、非活肿瘤(NVT)、活肿瘤(VT)和非活比率(NVR)。在Cancer Imaging Archive(TCIA)数据集上的实验显示，该模型实现了99.08%的准确率、99.10%的精确率、99.28%的召回率和99.23%的F1分数，这是首次在该数据集上成功进行四类分类，并为骨肉瘤诊断设定新基准。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00832v1",
      "published_date": "2024-10-29 13:54:08 UTC",
      "updated_date": "2024-10-29 13:54:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:27:34.617164"
    },
    {
      "arxiv_id": "2411.00831v1",
      "title": "Saliency-Based diversity and fairness Metric and FaceKeepOriginalAugment: A Novel Approach for Enhancing Fairness and Diversity",
      "title_zh": "翻译失败",
      "authors": [
        "Teerath Kumar",
        "Alessandra Mileo",
        "Malika Bendechache"
      ],
      "abstract": "Data augmentation has become a pivotal tool in enhancing the performance of\ncomputer vision tasks, with the KeepOriginalAugment method emerging as a\nstandout technique for its intelligent incorporation of salient regions within\nless prominent areas, enabling augmentation in both regions. Despite its\nsuccess in image classification, its potential in addressing biases remains\nunexplored. In this study, we introduce an extension of the KeepOriginalAugment\nmethod, termed FaceKeepOriginalAugment, which explores various debiasing\naspects-geographical, gender, and stereotypical biases-in computer vision\nmodels. By maintaining a delicate balance between data diversity and\ninformation preservation, our approach empowers models to exploit both diverse\nsalient and non-salient regions, thereby fostering increased diversity and\ndebiasing effects. We investigate multiple strategies for determining the\nplacement of the salient region and swapping perspectives to decide which part\nundergoes augmentation. Leveraging the Image Similarity Score (ISS), we\nquantify dataset diversity across a range of datasets, including Flickr Faces\nHQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces, and Diverse\nDataset. We evaluate the effectiveness of FaceKeepOriginalAugment in mitigating\ngender bias across CEO, Engineer, Nurse, and School Teacher datasets, utilizing\nthe Image-Image Association Score (IIAS) in convolutional neural networks\n(CNNs) and vision transformers (ViTs). Our findings shows the efficacy of\nFaceKeepOriginalAugment in promoting fairness and inclusivity within computer\nvision models, demonstrated by reduced gender bias and enhanced overall\nfairness. Additionally, we introduce a novel metric, Saliency-Based Diversity\nand Fairness Metric, which quantifies both diversity and fairness while\nhandling data imbalance across various datasets.",
      "tldr_zh": "本研究提出了一种新方法FaceKeepOriginalAugment，作为KeepOriginalAugment的扩展，旨在通过智能增强显著和非显著区域来提升计算机视觉模型的数据多样性和公平性，同时缓解地理、性别和刻板印象偏差。研究探索了放置显著区域和交换视角的多种策略，并利用Image Similarity Score (ISS)量化了多个数据集（如FFHQ、WIKI等）的多样性，以及Image-Image Association Score (IIAS)评估了在CEO、Engineer等数据集上的性别偏差减少效果。最终结果显示，该方法显著降低了性别偏差并提高了模型公平性，同时引入了Saliency-Based Diversity and Fairness Metric这一新指标，用于处理数据不平衡并全面衡量多样性和公平性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Paper is underReview in Image and Vision Computing Journal special\n  issue: Advancing Transparency and Privacy: Explainable AI and Synthetic Data\n  in Biometrics and Computer Vision",
      "pdf_url": "http://arxiv.org/pdf/2411.00831v1",
      "published_date": "2024-10-29 13:49:23 UTC",
      "updated_date": "2024-10-29 13:49:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:27:45.562740"
    },
    {
      "arxiv_id": "2411.08906v1",
      "title": "Assessing the Auditability of AI-integrating Systems: A Framework and Learning Analytics Case Study",
      "title_zh": "评估 AI 整合系统的可审计性：一个框架和学习分析案例研究",
      "authors": [
        "Linda Fernsel",
        "Yannick Kalff",
        "Katharina Simbeck"
      ],
      "abstract": "Audits contribute to the trustworthiness of Learning Analytics (LA) systems\nthat integrate Artificial Intelligence (AI) and may be legally required in the\nfuture. We argue that the efficacy of an audit depends on the auditability of\nthe audited system. Therefore, systems need to be designed with auditability in\nmind. We present a framework for assessing the auditability of AI-integrating\nsystems that consists of three parts: (1) Verifiable claims about the validity,\nutility and ethics of the system, (2) Evidence on subjects (data, models or the\nsystem) in different types (documentation, raw sources and logs) to back or\nrefute claims, (3) Evidence must be accessible to auditors via technical means\n(APIs, monitoring tools, explainable AI, etc.). We apply the framework to\nassess the auditability of Moodle's dropout prediction system and a prototype\nAI-based LA. We find that Moodle's auditability is limited by incomplete\ndocumentation, insufficient monitoring capabilities and a lack of available\ntest data. The framework supports assessing the auditability of AI-based LA\nsystems in use and improves the design of auditable systems and thus of audits.",
      "tldr_zh": "该研究提出一个框架，用于评估整合人工智能（AI）的系统可审计性，强调审计对提升学习分析（Learning Analytics, LA）系统可信度的作用。该框架包括三部分：可验证的声明（关于系统的有效性、效用和伦理）、支持或反驳声明的证据（如文档、原始数据和日志），以及证据通过技术手段（如APIs、监控工具和可解释AI）的可访问性。研究者将该框架应用于Moodle的辍学预测系统和一个原型AI-based LA系统，发现Moodle存在文档不完整、监控能力不足和测试数据缺乏等问题。该框架有助于改进AI整合系统的设计，从而增强审计的有效性和系统的整体可信度。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "68-XX",
        "K.3.1; K.4.1; K.4.2; K.5.2"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.08906v1",
      "published_date": "2024-10-29 13:43:21 UTC",
      "updated_date": "2024-10-29 13:43:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:27:57.550886"
    },
    {
      "arxiv_id": "2411.00830v1",
      "title": "Unsupervised Training of a Dynamic Context-Aware Deep Denoising Framework for Low-Dose Fluoroscopic Imaging",
      "title_zh": "低剂量透视成像的动态上下文感知深度去噪框架的无监督训练",
      "authors": [
        "Sun-Young Jeon",
        "Sen Wang",
        "Adam S. Wang",
        "Garry E. Gold",
        "Jang-Hwan Choi"
      ],
      "abstract": "Fluoroscopy is critical for real-time X-ray visualization in medical imaging.\nHowever, low-dose images are compromised by noise, potentially affecting\ndiagnostic accuracy. Noise reduction is crucial for maintaining image quality,\nespecially given such challenges as motion artifacts and the limited\navailability of clean data in medical imaging. To address these issues, we\npropose an unsupervised training framework for dynamic context-aware denoising\nof fluoroscopy image sequences. First, we train the multi-scale recurrent\nattention U-Net (MSR2AU-Net) without requiring clean data to address the\ninitial noise. Second, we incorporate a knowledge distillation-based\nuncorrelated noise suppression module and a recursive filtering-based\ncorrelated noise suppression module enhanced with motion compensation to\nfurther improve motion compensation and achieve superior denoising performance.\nFinally, we introduce a novel approach by combining these modules with a\npixel-wise dynamic object motion cross-fusion matrix, designed to adapt to\nmotion, and an edge-preserving loss for precise detail retention. To validate\nthe proposed method, we conducted extensive numerical experiments on medical\nimage datasets, including 3500 fluoroscopy images from dynamic phantoms (2,400\nimages for training, 1,100 for testing) and 350 clinical images from a spinal\nsurgery patient. Moreover, we demonstrated the robustness of our approach\nacross different imaging modalities by testing it on the publicly available\n2016 Low Dose CT Grand Challenge dataset, using 4,800 images for training and\n1,136 for testing. The results demonstrate that the proposed approach\noutperforms state-of-the-art unsupervised algorithms in both visual quality and\nquantitative evaluation while achieving comparable performance to\nwell-established supervised learning methods across low-dose fluoroscopy and CT\nimaging.",
      "tldr_zh": "这篇论文提出了一种无监督训练框架，用于低剂量荧光镜成像的动态上下文感知去噪，旨在解决图像噪声、运动伪影和清洁数据有限的问题。框架的核心组件包括多尺度循环注意力 U-Net (MSR2AU-Net) 用于初始噪声处理、知识蒸馏-based 的 uncorrelated 噪声抑制模块、递归过滤-based 的 correlated 噪声抑制模块，以及像素级动态物体运动交叉融合矩阵和边缘保留损失，以提升运动补偿和细节保留。实验在包含3500张荧光镜图像和临床图像的医疗数据集上进行，结果显示该方法在视觉质量和定量评估上优于现有无监督算法，并与监督学习方法性能相当。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "15 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.00830v1",
      "published_date": "2024-10-29 13:39:31 UTC",
      "updated_date": "2024-10-29 13:39:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:28:10.706399"
    },
    {
      "arxiv_id": "2410.22026v1",
      "title": "Enhance Hyperbolic Representation Learning via Second-order Pooling",
      "title_zh": "翻译失败",
      "authors": [
        "Kun Song",
        "Ruben Solozabal",
        "Li hao",
        "Lu Ren",
        "Moloud Abdar",
        "Qing Li",
        "Fakhri Karray",
        "Martin Takac"
      ],
      "abstract": "Hyperbolic representation learning is well known for its ability to capture\nhierarchical information. However, the distance between samples from different\nlevels of hierarchical classes can be required large. We reveal that the\nhyperbolic discriminant objective forces the backbone to capture this\nhierarchical information, which may inevitably increase the Lipschitz constant\nof the backbone. This can hinder the full utilization of the backbone's\ngeneralization ability. To address this issue, we introduce second-order\npooling into hyperbolic representation learning, as it naturally increases the\ndistance between samples without compromising the generalization ability of the\ninput features. In this way, the Lipschitz constant of the backbone does not\nnecessarily need to be large. However, current off-the-shelf low-dimensional\nbilinear pooling methods cannot be directly employed in hyperbolic\nrepresentation learning because they inevitably reduce the distance expansion\ncapability. To solve this problem, we propose a kernel approximation\nregularization, which enables the low-dimensional bilinear features to\napproximate the kernel function well in low-dimensional space. Finally, we\nconduct extensive experiments on graph-structured datasets to demonstrate the\neffectiveness of the proposed method.",
      "tldr_zh": "本文研究了Hyperbolic representation learning在捕捉层次信息时，可能导致主干网络的Lipschitz constant增大，从而影响其泛化能力的问题。为此，作者引入了second-order pooling方法，以自然增加样本间距离，同时不损害输入特征的泛化能力。针对现有低维双线性池化方法的局限性，他们提出了kernel approximation regularization，使低维双线性特征在低维空间中更好地逼近核函数。在图结构数据集上的广泛实验中，该方法显著提升了Hyperbolic representation learning的性能，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22026v1",
      "published_date": "2024-10-29 13:17:43 UTC",
      "updated_date": "2024-10-29 13:17:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:28:21.440926"
    },
    {
      "arxiv_id": "2410.22020v2",
      "title": "Path-based summary explanations for graph recommenders (extended version)",
      "title_zh": "翻译失败",
      "authors": [
        "Danae Pla Karidi",
        "Evaggelia Pitoura"
      ],
      "abstract": "Path-based explanations provide intrinsic insights into graph-based\nrecommendation models. However, most previous work has focused on explaining an\nindividual recommendation of an item to a user. In this paper, we propose\nsummary explanations, i.e., explanations that highlight why a user or a group\nof users receive a set of item recommendations and why an item, or a group of\nitems, is recommended to a set of users as an effective means to provide\ninsights into the collective behavior of the recommender. We also present a\nnovel method to summarize explanations using efficient graph algorithms,\nspecifically the Steiner Tree and the Prize-Collecting Steiner Tree. Our\napproach reduces the size and complexity of summary explanations while\npreserving essential information, making explanations more comprehensible for\nusers and more useful to model developers. Evaluations across multiple metrics\ndemonstrate that our summaries outperform baseline explanation methods in most\nscenarios, in a variety of quality aspects.",
      "tldr_zh": "本论文提出了一种基于路径（path-based）的总结解释方法，用于图推荐系统（graph recommenders），旨在解释为什么一组用户收到特定物品推荐，或为什么一组物品推荐给一组用户，从而揭示推荐模型的集体行为。作者开发了一种新颖的方法，利用Steiner Tree和Prize-Collecting Steiner Tree等高效图算法来压缩和简化解释，同时保留核心信息，提高了解释的可理解性和实用性。实验结果显示，该方法在多个评估指标上优于基线方法，在解释质量方面表现出色。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This is an extended version of the work \"Path-based summary\n  explanations for graph recommenders\", which has been accepted for publication\n  in the Proceedings of the IEEE International Conference on Data Engineering\n  (ICDE) 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.22020v2",
      "published_date": "2024-10-29 13:10:03 UTC",
      "updated_date": "2024-12-07 09:36:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:28:33.183170"
    },
    {
      "arxiv_id": "2411.00036v2",
      "title": "Coupling quantum-like cognition with the neuronal networks within generalized probability theory",
      "title_zh": "翻译失败",
      "authors": [
        "Andrei Khrennikov",
        "Masanao Ozawa",
        "Felix Benninger",
        "Oded Shor"
      ],
      "abstract": "The past few years have seen a surge in the application of quantum theory\nmethodologies and quantum-like modeling in fields such as cognition,\npsychology, and decision-making. Despite the success of this approach in\nexplaining various psychological phenomena such as order, conjunction,\ndisjunction, and response replicability effects there remains a potential\ndissatisfaction due to its lack of clear connection to neurophysiological\nprocesses in the brain. Currently, it remains a phenomenological approach. In\nthis paper, we develop a quantum-like representation of networks of\ncommunicating neurons. This representation is not based on standard quantum\ntheory but on generalized probability theory (GPT), with a focus on the\noperational measurement framework. Specifically, we use a version of GPT that\nrelies on ordered linear state spaces rather than the traditional complex\nHilbert spaces. A network of communicating neurons is modeled as a weighted\ndirected graph, which is encoded by its weight matrix. The state space of these\nweight matrices is embedded within the GPT framework, incorporating effect\nobservables and state updates within the theory of measurement instruments a\ncritical aspect of this model. This GPT based approach successfully reproduces\nkey quantum-like effects, such as order, non-repeatability, and disjunction\neffects (commonly associated with decision interference). Moreover, this\nframework supports quantum-like modeling in medical diagnostics for\nneurological conditions such as depression and epilepsy. While this paper\nfocuses primarily on cognition and neuronal networks, the proposed formalism\nand methodology can be directly applied to a wide range of biological and\nsocial networks.",
      "tldr_zh": "本文提出了一种基于 generalized probability theory (GPT) 的框架，将 quantum-like 认知模型与神经元网络相结合，旨在解决现有模型缺乏神经生理过程连接的问题。具体而言，该方法使用有序线性状态空间建模神经元网络为加权有向图，并融入测量仪器理论来处理状态更新和效应可观测。实验结果显示，该框架成功再现了量子-like 效果，如顺序、非重复性和分离效果，并可应用于医疗诊断（如抑郁和癫痫），且扩展至各种生物和社会网络。",
      "categories": [
        "physics.soc-ph",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "RIKEN Quantum Workshop, October 11, 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.00036v2",
      "published_date": "2024-10-29 13:09:35 UTC",
      "updated_date": "2025-01-04 13:39:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:28:45.871886"
    },
    {
      "arxiv_id": "2410.22013v1",
      "title": "Modeling Temporal Positive and Negative Excitation for Sequential Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Chengkai Huang",
        "Shoujin Wang",
        "Xianzhi Wang",
        "Lina Yao"
      ],
      "abstract": "Sequential recommendation aims to predict the next item which interests users\nvia modeling their interest in items over time. Most of the existing works on\nsequential recommendation model users' dynamic interest in specific items while\noverlooking users' static interest revealed by some static attribute\ninformation of items, e.g., category, or brand. Moreover, existing works often\nonly consider the positive excitation of a user's historical interactions on\nhis/her next choice on candidate items while ignoring the commonly existing\nnegative excitation, resulting in insufficient modeling dynamic interest. The\noverlook of static interest and negative excitation will lead to incomplete\ninterest modeling and thus impede the recommendation performance. To this end,\nin this paper, we propose modeling both static interest and negative excitation\nfor dynamic interest to further improve the recommendation performance.\nAccordingly, we design a novel Static-Dynamic Interest Learning (SDIL)\nframework featured with a novel Temporal Positive and Negative Excitation\nModeling (TPNE) module for accurate sequential recommendation. TPNE is\nspecially designed for comprehensively modeling dynamic interest based on\ntemporal positive and negative excitation learning. Extensive experiments on\nthree real-world datasets show that SDIL can effectively capture both static\nand dynamic interest and outperforms state-of-the-art baselines.",
      "tldr_zh": "本研究针对顺序推荐（Sequential Recommendation）问题，指出现有方法忽略了用户基于物品静态属性（如类别或品牌）的静态兴趣，以及历史互动的负向激励（Negative Excitation），导致动态兴趣建模不足。论文提出了一种新型框架Static-Dynamic Interest Learning (SDIL)，其中包含Temporal Positive and Negative Excitation Modeling (TPNE)模块，用于全面建模用户的时间正负向激励和静态兴趣。实验在三个真实数据集上验证了SDIL的有效性，其性能超过了最先进基线方法。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22013v1",
      "published_date": "2024-10-29 13:02:11 UTC",
      "updated_date": "2024-10-29 13:02:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:28:58.223216"
    },
    {
      "arxiv_id": "2411.04137v1",
      "title": "Generative AI Enabled Matching for 6G Multiple Access",
      "title_zh": "翻译失败",
      "authors": [
        "Xudong Wang",
        "Hongyang Du",
        "Dusit Niyato",
        "Lijie Zhou",
        "Lei Feng",
        "Zhixiang Yang",
        "Fanqin Zhou",
        "Wenjing Li"
      ],
      "abstract": "In wireless networks, applying deep learning models to solve matching\nproblems between different entities has become a mainstream and effective\napproach. However, the complex network topology in 6G multiple access presents\nsignificant challenges for the real-time performance and stability of matching\ngeneration. Generative artificial intelligence (GenAI) has demonstrated strong\ncapabilities in graph feature extraction, exploration, and generation, offering\npotential for graph-structured matching generation. In this paper, we propose a\nGenAI-enabled matching generation framework to support 6G multiple access.\nSpecifically, we first summarize the classical matching theory, discuss common\nGenAI models and applications from the perspective of matching generation.\nThen, we propose a framework based on generative diffusion models (GDMs) that\niteratively denoises toward reward maximization to generate a matching strategy\nthat meets specific requirements. Experimental results show that, compared to\ndecision-based AI approaches, our framework can generate more effective\nmatching strategies based on given conditions and predefined rewards, helping\nto solve complex problems in 6G multiple access, such as task allocation.",
      "tldr_zh": "该研究提出了一种基于 Generative AI 的匹配生成框架，用于解决 6G Multiple Access 中复杂网络拓扑带来的实时性能和稳定性挑战。该框架首先总结经典匹配理论，并探讨 Generative AI 模型的应用，然后利用 Generative Diffusion Models (GDMs) 通过迭代去噪向奖励最大化生成符合特定需求的匹配策略。实验结果表明，与决策-based AI 方法相比，该框架能基于给定条件和预定义奖励生成更有效的匹配策略，从而帮助处理 6G 多址访问中的复杂问题，如任务分配。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "8 pages,5 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.04137v1",
      "published_date": "2024-10-29 13:01:26 UTC",
      "updated_date": "2024-10-29 13:01:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:29:10.477557"
    },
    {
      "arxiv_id": "2410.22382v2",
      "title": "Debiasing Alternative Data for Credit Underwriting Using Causal Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Chris Lam"
      ],
      "abstract": "Alternative data provides valuable insights for lenders to evaluate a\nborrower's creditworthiness, which could help expand credit access to\nunderserved groups and lower costs for borrowers. But some forms of alternative\ndata have historically been excluded from credit underwriting because it could\nact as an illegal proxy for a protected class like race or gender, causing\nredlining. We propose a method for applying causal inference to a supervised\nmachine learning model to debias alternative data so that it might be used for\ncredit underwriting. We demonstrate how our algorithm can be used against a\npublic credit dataset to improve model accuracy across different racial groups,\nwhile providing theoretically robust nondiscrimination guarantees.",
      "tldr_zh": "该论文探讨了如何使用因果推理（causal inference）去除替代数据（alternative data）中的偏见，以应用于信用评估（credit underwriting）。研究提出了一种算法，通过监督机器学习模型调整数据，确保其不作为种族或性别等受保护类别的非法代理，从而避免红线区歧视（redlining）。实验结果显示，该方法在公共信用数据集上提升了不同种族群体的模型准确性，同时提供了理论上可靠的非歧视保证。",
      "categories": [
        "q-fin.RM",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "q-fin.RM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22382v2",
      "published_date": "2024-10-29 12:54:55 UTC",
      "updated_date": "2024-10-31 17:12:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:29:20.663912"
    },
    {
      "arxiv_id": "2410.21991v6",
      "title": "From Explicit Rules to Implicit Reasoning in Weakly Supervised Video Anomaly Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Wen-Dong Jiang",
        "Chih-Yung Chang",
        "Ssu-Chi Kuai",
        "Diptendu Sinha Roy"
      ],
      "abstract": "Recent advances in pre-trained models have demonstrated exceptional\nperformance in video anomaly detection (VAD). However, most systems remain\nblack boxes, lacking explainability during training and inference. A key\nchallenge is integrating explicit knowledge into implicit models to create\nexpert-driven, interpretable VAD systems. This paper introduces Rule-based\nViolence Monitoring (RuleVM), a novel weakly supervised video anomaly detection\n(WVAD) paradigm. RuleVM employs a dual-branch architecture: an implicit branch\nusing visual features for coarse-grained binary classification, with feature\nextraction split into scene frames and action channels, and an explicit branch\nleveraging language-image alignment for fine-grained classification. The\nexplicit branch utilizes the state-of-the-art YOLO-World model for object\ndetection in video frames, with association rules mined from data as video\ndescriptors. This design enables interpretable coarse- and fine-grained\nviolence monitoring. Extensive experiments on two standard benchmarks show\nRuleVM outperforms state-of-the-art methods in both granularities. Notably, it\nreveals rules like increased violence risk with crowd size. Demo content is\nprovided in the appendix.",
      "tldr_zh": "本论文提出RuleVM，一种弱监督视频异常检测(WVAD)框架，将显式规则与隐式推理相结合，以解决现有模型的黑箱问题并提升可解释性。RuleVM采用双分支架构：隐式分支使用视觉特征（如场景帧和动作通道）进行粗粒度二元分类，而显式分支则利用YOLO-World模型进行物体检测，并从数据中挖掘关联规则作为视频描述符，实现细粒度暴力监控。实验结果显示，RuleVM在两个标准基准上优于现有方法，并在粗细粒度分析中揭示了关键规则，如人群规模增加与暴力风险相关的关联。总体而言，该框架为专家驱动的、可解释VAD系统提供了新范式。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This manuscript has been submitted to IEEE Transactions on Circuits\n  and Systems for Video Technology and is under consideration for publication,\n  with potential copyright transfer in the future",
      "pdf_url": "http://arxiv.org/pdf/2410.21991v6",
      "published_date": "2024-10-29 12:22:07 UTC",
      "updated_date": "2025-04-06 04:35:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:29:34.976932"
    },
    {
      "arxiv_id": "2411.00034v1",
      "title": "Is Our Chatbot Telling Lies? Assessing Correctness of an LLM-based Dutch Support Chatbot",
      "title_zh": "我们的聊天机器人在撒谎吗？ 评估基于大语言模型的荷兰语客服聊天机器人的正确",
      "authors": [
        "Herman Lassche",
        "Michiel Overeem",
        "Ayushi Rastogi"
      ],
      "abstract": "Companies support their customers using live chats and chatbots to gain their\nloyalty. AFAS is a Dutch company aiming to leverage the opportunity large\nlanguage models (LLMs) offer to answer customer queries with minimal to no\ninput from its customer support team. Adding to its complexity, it is unclear\nwhat makes a response correct, and that too in Dutch. Further, with minimal\ndata available for training, the challenge is to identify whether an answer\ngenerated by a large language model is correct and do it on the fly.\n  This study is the first to define the correctness of a response based on how\nthe support team at AFAS makes decisions. It leverages literature on natural\nlanguage generation and automated answer grading systems to automate the\ndecision-making of the customer support team. We investigated questions\nrequiring a binary response (e.g., Would it be possible to adjust tax rates\nmanually?) or instructions (e.g., How would I adjust tax rate manually?) to\ntest how close our automated approach reaches support rating. Our approach can\nidentify wrong messages in 55\\% of the cases. This work shows the viability of\nautomatically assessing when our chatbot tell lies.",
      "tldr_zh": "这篇论文评估了基于LLM（Large Language Models）的荷兰语支持聊天机器人的响应正确性，针对数据有限和语言特定挑战（如荷兰语标准定义）。研究首次基于AFAS支持团队的决策定义正确性，并利用自然语言生成和自动答案评分系统自动化决策过程，测试了二元响应（如是/否问题）和指令问题。结果显示，该方法能识别55%的错误消息，证明了自动评估聊天机器人是否“说谎”的可行性，为企业聊天机器人可靠性提升提供了实用途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; I.7.0"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages + 2 pages references, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.00034v1",
      "published_date": "2024-10-29 12:02:14 UTC",
      "updated_date": "2024-10-29 12:02:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:29:45.490194"
    },
    {
      "arxiv_id": "2410.21968v1",
      "title": "Automated Vulnerability Detection Using Deep Learning Technique",
      "title_zh": "翻译失败",
      "authors": [
        "Guan-Yan Yang",
        "Yi-Heng Ko",
        "Farn Wang",
        "Kuo-Hui Yeh",
        "Haw-Shiang Chang",
        "Hsueh-Yi Chen"
      ],
      "abstract": "Our work explores the utilization of deep learning, specifically leveraging\nthe CodeBERT model, to enhance code security testing for Python applications by\ndetecting SQL injection vulnerabilities. Unlike traditional security testing\nmethods that may be slow and error-prone, our approach transforms source code\ninto vector representations and trains a Long Short-Term Memory (LSTM) model to\nidentify vulnerable patterns. When compared with existing static application\nsecurity testing (SAST) tools, our model displays superior performance,\nachieving higher precision, recall, and F1-score. The study demonstrates that\ndeep learning techniques, particularly with CodeBERT's advanced contextual\nunderstanding, can significantly improve vulnerability detection, presenting a\nscalable methodology applicable to various programming languages and\nvulnerability types.",
      "tldr_zh": "本研究利用深度学习技术，特别是 CodeBERT 模型，开发了一种自动检测 Python 应用中 SQL injection 漏洞的方法。该方法将源代码转换为向量表示，并使用 Long Short-Term Memory (LSTM) 模型训练以识别漏洞模式。与传统静态应用安全测试 (SAST) 工具相比，该模型在精确率、召回率和 F1-score 上表现出色，提高了检测性能。该方法证明了深度学习在代码安全测试中的潜力，并可扩展应用于多种编程语言和漏洞类型。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE",
        "D.2.4; D.2.5"
      ],
      "primary_category": "cs.CR",
      "comment": "4 pages, 1 figures; Presented at The 30st International Conference on\n  Computational & Experimental Engineering and Sciences (ICCES2024)",
      "pdf_url": "http://arxiv.org/pdf/2410.21968v1",
      "published_date": "2024-10-29 11:51:51 UTC",
      "updated_date": "2024-10-29 11:51:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:29:58.039751"
    },
    {
      "arxiv_id": "2410.21967v2",
      "title": "Dual Conditional Diffusion Models for Sequential Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Hongtao Huang",
        "Chengkai Huang",
        "Tong Yu",
        "Xiaojun Chang",
        "Wen Hu",
        "Julian McAuley",
        "Lina Yao"
      ],
      "abstract": "Recent advancements in diffusion models have shown promising results in\nsequential recommendation (SR). Existing approaches predominantly rely on\nimplicit conditional diffusion models, which compress user behaviors into a\nsingle representation during the forward diffusion process. While effective to\nsome extent, this oversimplification often leads to the loss of sequential and\ncontextual information, which is critical for understanding user behavior.\nMoreover, explicit information, such as user-item interactions or sequential\npatterns, remains underutilized, despite its potential to directly guide the\nrecommendation process and improve precision. However, combining implicit and\nexplicit information is non-trivial, as it requires dynamically integrating\nthese complementary signals while avoiding noise and irrelevant patterns within\nuser behaviors. To address these challenges, we propose Dual Conditional\nDiffusion Models for Sequential Recommendation (DCRec), which effectively\nintegrates implicit and explicit information by embedding dual conditions into\nboth the forward and reverse diffusion processes. This allows the model to\nretain valuable sequential and contextual information while leveraging explicit\nuser-item interactions to guide the recommendation process. Specifically, we\nintroduce the Dual Conditional Diffusion Transformer (DCDT), which employs a\ncross-attention mechanism to dynamically integrate explicit signals throughout\nthe diffusion stages, ensuring contextual understanding and minimizing the\ninfluence of irrelevant patterns. This design enables precise and contextually\nrelevant recommendations. Extensive experiments on public benchmark datasets\ndemonstrate that DCRec significantly outperforms state-of-the-art methods in\nboth accuracy and computational efficiency.",
      "tldr_zh": "本文提出 Dual Conditional Diffusion Models for Sequential Recommendation (DCRec)，一种新方法，用于解决现有扩散模型在序列推荐（SR）中依赖隐式条件导致的序列和上下文信息丢失问题。DCRec 通过在前后向扩散过程中嵌入隐式和显式条件（如用户-物品互动），并引入 Dual Conditional Diffusion Transformer (DCDT) 的交叉注意力机制，实现显式信号的动态整合，从而提升推荐的精确性和上下文相关性。实验结果显示，在公共基准数据集上，DCRec 在准确性和计算效率方面显著优于现有方法。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21967v2",
      "published_date": "2024-10-29 11:51:06 UTC",
      "updated_date": "2025-03-18 04:42:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:30:09.718960"
    },
    {
      "arxiv_id": "2410.21951v2",
      "title": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Bohan Li",
        "Hankun Wang",
        "Situo Zhang",
        "Yiwei Guo",
        "Kai Yu"
      ],
      "abstract": "The auto-regressive architecture, like GPTs, is widely used in modern\nText-to-Speech (TTS) systems. However, it incurs substantial inference time,\nparticularly due to the challenges in the next-token prediction posed by\nlengthy sequences of speech tokens. In this work, we introduce VADUSA, one of\nthe first approaches to accelerate auto-regressive TTS through speculative\ndecoding. Our results show that VADUSA not only significantly improves\ninference speed but also enhances performance by incorporating draft heads to\npredict future speech content auto-regressively. Furthermore, the inclusion of\na tolerance mechanism during sampling accelerates inference without\ncompromising quality. Our approach demonstrates strong generalization across\nlarge datasets and various types of speech tokens.",
      "tldr_zh": "这篇论文提出了VADUSA，一种通过Speculative Decoding加速自回归TTS（Text-to-Speech）系统的方法，以解决长序列语音token预测导致的推理时间问题。VADUSA引入draft heads来自动回归地预测未来语音内容，并采用tolerance机制在采样过程中提升速度，同时保持输出质量。实验结果显示，该方法显著提高了推理效率和性能，并在大型数据集及各种语音token类型上表现出强泛化能力。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "68T07"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.21951v2",
      "published_date": "2024-10-29 11:12:01 UTC",
      "updated_date": "2025-02-10 04:22:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:30:21.109656"
    },
    {
      "arxiv_id": "2410.21943v1",
      "title": "Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Monica Riedler",
        "Stefan Langer"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nanswering questions, but they lack domain-specific knowledge and are prone to\nhallucinations. Retrieval Augmented Generation (RAG) is one approach to address\nthese challenges, while multimodal models are emerging as promising AI\nassistants for processing both text and images. In this paper we describe a\nseries of experiments aimed at determining how to best integrate multimodal\nmodels into RAG systems for the industrial domain. The purpose of the\nexperiments is to determine whether including images alongside text from\ndocuments within the industrial domain increases RAG performance and to find\nthe optimal configuration for such a multimodal RAG system. Our experiments\ninclude two approaches for image processing and retrieval, as well as two LLMs\n(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies\ninvolve the use of multimodal embeddings and the generation of textual\nsummaries from images. We evaluate our experiments with an LLM-as-a-Judge\napproach. Our results reveal that multimodal RAG can outperform single-modality\nRAG settings, although image retrieval poses a greater challenge than text\nretrieval. Additionally, leveraging textual summaries from images presents a\nmore promising approach compared to the use of multimodal embeddings, providing\nmore opportunities for future advancements.",
      "tldr_zh": "本文研究如何通过整合多模态输入优化 Retrieval Augmented Generation (RAG) 系统，以解决 Large Language Models (LLMs) 在工业领域中的领域知识不足和幻觉问题。实验包括两种图像处理策略（多模态 embeddings 和图像文本摘要生成）以及两种 LLM（GPT4-Vision 和 LLaVA），并使用 LLM-as-a-Judge 方法进行评估。结果表明，多模态 RAG 比单模态 RAG 性能更佳，但图像检索比文本检索更具挑战性，而利用图像文本摘要的方法更具前景，并为未来改进提供了机会。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21943v1",
      "published_date": "2024-10-29 11:03:31 UTC",
      "updated_date": "2024-10-29 11:03:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:30:34.386851"
    },
    {
      "arxiv_id": "2410.21940v1",
      "title": "Human-Readable Programs as Actors of Reinforcement Learning Agents Using Critic-Moderated Evolution",
      "title_zh": "翻译失败",
      "authors": [
        "Senne Deproost",
        "Denis Steckelmacher",
        "Ann Nowé"
      ],
      "abstract": "With Deep Reinforcement Learning (DRL) being increasingly considered for the\ncontrol of real-world systems, the lack of transparency of the neural network\nat the core of RL becomes a concern. Programmatic Reinforcement Learning (PRL)\nis able to to create representations of this black-box in the form of source\ncode, not only increasing the explainability of the controller but also\nallowing for user adaptations. However, these methods focus on distilling a\nblack-box policy into a program and do so after learning using the Mean Squared\nError between produced and wanted behaviour, discarding other elements of the\nRL algorithm. The distilled policy may therefore perform significantly worse\nthan the black-box learned policy.\n  In this paper, we propose to directly learn a program as the policy of an RL\nagent. We build on TD3 and use its critics as the basis of the objective\nfunction of a genetic algorithm that syntheses the program. Our approach builds\nthe program during training, as opposed to after the fact. This steers the\nprogram to actual high rewards, instead of a simple Mean Squared Error. Also,\nour approach leverages the TD3 critics to achieve high sample-efficiency, as\nopposed to pure genetic methods that rely on Monte-Carlo evaluations. Our\nexperiments demonstrate the validity, explainability and sample-efficiency of\nour approach in a simple gridworld environment.",
      "tldr_zh": "本文提出一种新方法，使用人类可读程序作为强化学习代理的策略，通过 critics-moderated evolution 解决 Deep Reinforcement Learning (DRL) 的透明性问题。不同于传统 Programmatic Reinforcement Learning (PRL) 方法，该方法基于 TD3 算法，在训练过程中直接利用其 critics 作为遗传算法的目标函数来合成程序，从而避免依赖 Mean Squared Error (MSE) 并提升样本效率。实验结果在 gridworld 环境中验证了该方法的有效性、可解释性和高样本效率，为可适应性强的 RL 控制器提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in BNAIC/BeNeLearn 2024 conference proceedings",
      "pdf_url": "http://arxiv.org/pdf/2410.21940v1",
      "published_date": "2024-10-29 10:57:33 UTC",
      "updated_date": "2024-10-29 10:57:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:30:46.041744"
    },
    {
      "arxiv_id": "2410.21939v2",
      "title": "AI Cyber Risk Benchmark: Automated Exploitation Capabilities",
      "title_zh": "AI 网络风险基准：自动利用能力",
      "authors": [
        "Dan Ristea",
        "Vasilios Mavroudis",
        "Chris Hicks"
      ],
      "abstract": "We introduce a new benchmark for assessing AI models' capabilities and risks\nin automated software exploitation, focusing on their ability to detect and\nexploit vulnerabilities in real-world software systems. Using DARPA's AI Cyber\nChallenge (AIxCC) framework and the Nginx challenge project, a deliberately\nmodified version of the widely used Nginx web server, we evaluate several\nleading language models, including OpenAI's o1-preview and o1-mini, Anthropic's\nClaude-3.5-sonnet-20241022 and Claude-3.5-sonnet-20240620, Google DeepMind's\nGemini-1.5-pro, and OpenAI's earlier GPT-4o model. Our findings reveal that\nthese models vary significantly in their success rates and efficiency, with\no1-preview achieving the highest success rate of 64.71 percent and o1-mini and\nClaude-3.5-sonnet-20241022 providing cost-effective but less successful\nalternatives. This benchmark establishes a foundation for systematically\nevaluating the AI cyber risk posed by automated exploitation tools.",
      "tldr_zh": "本研究引入了AI Cyber Risk Benchmark，这是一个用于评估AI模型在自动软件利用方面的能力和风险的新基准，焦点在于检测和利用真实软件系统的漏洞。\n他们采用DARPA的AIxCC框架和修改版Nginx项目，评估了多款领先语言模型，包括OpenAI的o1-preview（成功率达64.71%）、o1-mini、Anthropic的Claude-3.5-sonnet系列、Google DeepMind的Gemini-1.5-pro以及GPT-4o。\n结果显示，这些模型在成功率和效率上存在显著差异，其中o1-preview表现最佳，而o1-mini和Claude-3.5-sonnet-20241022则提供了更具成本效益的替代方案。\n此基准为系统评估AI自动利用工具带来的网络风险提供了重要基础。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21939v2",
      "published_date": "2024-10-29 10:57:11 UTC",
      "updated_date": "2024-12-09 15:29:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:30:59.286546"
    },
    {
      "arxiv_id": "2410.21938v1",
      "title": "ReMix: Training Generalized Person Re-identification on a Mixture of Data",
      "title_zh": "翻译失败",
      "authors": [
        "Timur Mamedov",
        "Anton Konushin",
        "Vadim Konushin"
      ],
      "abstract": "Modern person re-identification (Re-ID) methods have a weak generalization\nability and experience a major accuracy drop when capturing environments\nchange. This is because existing multi-camera Re-ID datasets are limited in\nsize and diversity, since such data is difficult to obtain. At the same time,\nenormous volumes of unlabeled single-camera records are available. Such data\ncan be easily collected, and therefore, it is more diverse. Currently,\nsingle-camera data is used only for self-supervised pre-training of Re-ID\nmethods. However, the diversity of single-camera data is suppressed by\nfine-tuning on limited multi-camera data after pre-training. In this paper, we\npropose ReMix, a generalized Re-ID method jointly trained on a mixture of\nlimited labeled multi-camera and large unlabeled single-camera data. Effective\ntraining of our method is achieved through a novel data sampling strategy and\nnew loss functions that are adapted for joint use with both types of data.\nExperiments show that ReMix has a high generalization ability and outperforms\nstate-of-the-art methods in generalizable person Re-ID. To the best of our\nknowledge, this is the first work that explores joint training on a mixture of\nmulti-camera and single-camera data in person Re-ID.",
      "tldr_zh": "本研究针对现代人重新识别(Re-ID)方法在环境变化时泛化能力弱的问题，提出ReMix方法，该方法在有限标记的多摄像头数据和大量无标记的单摄像头数据上进行联合训练，以充分利用单摄像头数据的多样性。ReMix引入了新颖的数据采样策略和适应性损失函数，确保两种数据类型的有效整合，从而克服现有方法的局限性。实验结果显示，ReMix在泛化人Re-ID任务上优于最先进方法，而这也是首次探索多摄像头和单摄像头数据混合训练的开创性工作。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2410.21938v1",
      "published_date": "2024-10-29 10:57:03 UTC",
      "updated_date": "2024-10-29 10:57:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:31:09.889733"
    },
    {
      "arxiv_id": "2410.21936v1",
      "title": "LogSHIELD: A Graph-based Real-time Anomaly Detection Framework using Frequency Analysis",
      "title_zh": "LogSHIELD：一种基于图的实时异常检测框架，使用频率分析",
      "authors": [
        "Krishna Chandra Roy",
        "Qian Chen"
      ],
      "abstract": "Anomaly-based cyber threat detection using deep learning is on a constant\ngrowth in popularity for novel cyber-attack detection and forensics. A robust,\nefficient, and real-time threat detector in a large-scale operational\nenterprise network requires high accuracy, high fidelity, and a high throughput\nmodel to detect malicious activities. Traditional anomaly-based detection\nmodels, however, suffer from high computational overhead and low detection\naccuracy, making them unsuitable for real-time threat detection. In this work,\nwe propose LogSHIELD, a highly effective graph-based anomaly detection model in\nhost data. We present a real-time threat detection approach using\nfrequency-domain analysis of provenance graphs. To demonstrate the significance\nof graph-based frequency analysis we proposed two approaches. Approach-I uses a\nGraph Neural Network (GNN) LogGNN and approach-II performs frequency domain\nanalysis on graph node samples for graph embedding. Both approaches use a\nstatistical clustering algorithm for anomaly detection. The proposed models are\nevaluated using a large host log dataset consisting of 774M benign logs and\n375K malware logs. LogSHIELD explores the provenance graph to extract\ncontextual and causal relationships among logs, exposing abnormal activities.\nIt can detect stealthy and sophisticated attacks with over 98% average AUC and\nF1 scores. It significantly improves throughput, achieves an average detection\nlatency of 0.13 seconds, and outperforms state-of-the-art models in detection\ntime.",
      "tldr_zh": "该研究提出LogSHIELD，一种基于图的实时异常检测框架，针对网络威胁检测问题，通过频率域分析处理provenance graphs，以提高检测准确性和效率。框架包括两种方法：Approach-I使用Graph Neural Network (GNN)模型LogGNN，Approach-II对图节点样本进行频率域分析并生成图嵌入，两者均结合统计聚类算法进行异常检测。在一个包含774M良性日志和375K恶意日志的大型数据集上，LogSHIELD实现了超过98%的平均AUC和F1分数，检测延迟平均仅0.13秒，并显著优于现有模型的检测时间和吞吐量。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21936v1",
      "published_date": "2024-10-29 10:52:43 UTC",
      "updated_date": "2024-10-29 10:52:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:31:21.588463"
    },
    {
      "arxiv_id": "2410.21928v1",
      "title": "Differentiable Inductive Logic Programming for Fraud Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Boris Wolfson",
        "Erman Acar"
      ],
      "abstract": "Current trends in Machine Learning prefer explainability even when it comes\nat the cost of performance. Therefore, explainable AI methods are particularly\nimportant in the field of Fraud Detection. This work investigates the\napplicability of Differentiable Inductive Logic Programming (DILP) as an\nexplainable AI approach to Fraud Detection. Although the scalability of DILP is\na well-known issue, we show that with some data curation such as cleaning and\nadjusting the tabular and numerical data to the expected format of background\nfacts statements, it becomes much more applicable. While in processing it does\nnot provide any significant advantage on rather more traditional methods such\nas Decision Trees, or more recent ones like Deep Symbolic Classification, it\nstill gives comparable results. We showcase its limitations and points to\nimprove, as well as potential use cases where it can be much more useful\ncompared to traditional methods, such as recursive rule learning.",
      "tldr_zh": "这篇论文探讨了 Differentiable Inductive Logic Programming (DILP) 作为一种可解释 AI 方法在欺诈检测中的应用，强调了其在性能与解释性之间的平衡。研究通过数据清洗和格式调整等措施改善了 DILP 的可扩展性，使其在处理背景事实时更适用。结果显示，DILP 的表现与 Decision Trees 或 Deep Symbolic Classification 等方法相当，但论文突出了其在递归规则学习等特定场景中的潜在优势，并指出了进一步改进的必要性。",
      "categories": [
        "q-fin.RM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.RM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21928v1",
      "published_date": "2024-10-29 10:43:06 UTC",
      "updated_date": "2024-10-29 10:43:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:31:34.236922"
    },
    {
      "arxiv_id": "2410.21926v1",
      "title": "Reliable Semantic Understanding for Real World Zero-shot Object Goal Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Halil Utku Unlu",
        "Shuaihang Yuan",
        "Congcong Wen",
        "Hao Huang",
        "Anthony Tzes",
        "Yi Fang"
      ],
      "abstract": "We introduce an innovative approach to advancing semantic understanding in\nzero-shot object goal navigation (ZS-OGN), enhancing the autonomy of robots in\nunfamiliar environments. Traditional reliance on labeled data has been a\nlimitation for robotic adaptability, which we address by employing a\ndual-component framework that integrates a GLIP Vision Language Model for\ninitial detection and an InstructionBLIP model for validation. This combination\nnot only refines object and environmental recognition but also fortifies the\nsemantic interpretation, pivotal for navigational decision-making. Our method,\nrigorously tested in both simulated and real-world settings, exhibits marked\nimprovements in navigation precision and reliability.",
      "tldr_zh": "这篇论文提出了一种创新方法，以提升零样本对象目标导航（Zero-shot Object Goal Navigation）中的语义理解，帮助机器人在陌生环境中实现更可靠的自主导航。方法采用双组件框架：GLIP Vision Language Model 用于初始对象检测，以及 InstructionBLIP 模型用于验证和精炼语义解释，从而加强导航决策的准确性。在模拟和真实世界测试中，该方法显著提高了导航精度和可靠性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "16 pages, 7 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2410.21926v1",
      "published_date": "2024-10-29 10:37:37 UTC",
      "updated_date": "2024-10-29 10:37:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:31:45.870105"
    },
    {
      "arxiv_id": "2410.22381v1",
      "title": "Robust training of implicit generative models for multivariate and heavy-tailed distributions with an invariant statistical loss",
      "title_zh": "翻译失败",
      "authors": [
        "José Manuel de Frutos",
        "Manuel A. Vázquez",
        "Pablo Olmos",
        "Joaquín Míguez"
      ],
      "abstract": "Traditional implicit generative models are capable of learning highly complex\ndata distributions. However, their training involves distinguishing real data\nfrom synthetically generated data using adversarial discriminators, which can\nlead to unstable training dynamics and mode dropping issues. In this work, we\nbuild on the \\textit{invariant statistical loss} (ISL) method introduced in\n\\cite{de2024training}, and extend it to handle heavy-tailed and multivariate\ndata distributions.\n  The data generated by many real-world phenomena can only be properly\ncharacterised using heavy-tailed probability distributions, and traditional\nimplicit methods struggle to effectively capture their asymptotic behavior. To\naddress this problem, we introduce a generator trained with ISL, that uses\ninput noise from a generalised Pareto distribution (GPD). We refer to this\ngenerative scheme as Pareto-ISL for conciseness. Our experiments demonstrate\nthat Pareto-ISL accurately models the tails of the distributions while still\neffectively capturing their central characteristics.\n  The original ISL function was conceived for 1D data sets. When the actual\ndata is $n$-dimensional, a straightforward extension of the method was obtained\nby targeting the $n$ marginal distributions of the data. This approach is\ncomputationally infeasible and ineffective in high-dimensional spaces. To\novercome this, we extend the 1D approach using random projections and define a\nnew loss function suited for multivariate data, keeping problems tractable by\nadjusting the number of projections. We assess its performance in\nmultidimensional generative modeling and explore its potential as a pretraining\ntechnique for generative adversarial networks (GANs) to prevent mode collapse,\nreporting promising results and highlighting its robustness across various\nhyperparameter settings.",
      "tldr_zh": "本研究扩展了 invariant statistical loss (ISL) 方法，用于训练隐式生成模型，以处理重尾分布和多变量数据，从而解决传统模型的训练不稳定和模式丢失问题。论文引入 Pareto-ISL 方案，使用 generalised Pareto distribution (GPD) 作为输入噪声，能够准确捕捉重尾分布的尾部行为，同时保留中心特征。对于多变量数据，通过随机投影定义新损失函数，使其在高维空间保持可计算性。实验结果显示，Pareto-ISL 在建模多维分布方面表现出色，并作为 generative adversarial networks (GANs) 的预训练技术，有效防止模式崩溃并提升整体鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.CO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22381v1",
      "published_date": "2024-10-29 10:27:50 UTC",
      "updated_date": "2024-10-29 10:27:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:31:57.612974"
    },
    {
      "arxiv_id": "2410.22380v1",
      "title": "Discrete Modeling via Boundary Conditional Diffusion Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Gu",
        "Xiaocheng Feng",
        "Lei Huang",
        "Yingsheng Wu",
        "Zekun Zhou",
        "Weihong Zhong",
        "Kun Zhu",
        "Bing Qin"
      ],
      "abstract": "We present an novel framework for efficiently and effectively extending the\npowerful continuous diffusion processes to discrete modeling. Previous\napproaches have suffered from the discrepancy between discrete data and\ncontinuous modeling. Our study reveals that the absence of guidance from\ndiscrete boundaries in learning probability contours is one of the main\nreasons. To address this issue, we propose a two-step forward process that\nfirst estimates the boundary as a prior distribution and then rescales the\nforward trajectory to construct a boundary conditional diffusion model. The\nreverse process is proportionally adjusted to guarantee that the learned\ncontours yield more precise discrete data. Experimental results indicate that\nour approach achieves strong performance in both language modeling and discrete\nimage generation tasks. In language modeling, our approach surpasses previous\nstate-of-the-art continuous diffusion language models in three translation\ntasks and a summarization task, while also demonstrating competitive\nperformance compared to auto-regressive transformers. Moreover, our method\nachieves comparable results to continuous diffusion models when using discrete\nordinal pixels and establishes a new state-of-the-art for categorical image\ngeneration on the Cifar-10 dataset.",
      "tldr_zh": "我们提出了一种新框架，通过边界条件扩散模型（boundary conditional diffusion model）将连续扩散过程高效扩展到离散建模中，以解决离散数据与连续建模之间的不一致问题。框架的核心是两步前向过程：先估计边界作为先验分布，然后重新缩放前向轨迹，同时调整反向过程以确保更精确地生成离散数据。实验结果显示，该方法在语言建模任务中超越了现有最先进连续扩散模型，在三个翻译任务和一个总结任务上表现突出，并与自回归Transformer竞争；在图像生成任务上，使用离散序数像素时与连续模型相当，并在Cifar-10数据集上建立了新的最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeuraIPS 2024 poster",
      "pdf_url": "http://arxiv.org/pdf/2410.22380v1",
      "published_date": "2024-10-29 09:42:42 UTC",
      "updated_date": "2024-10-29 09:42:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:32:10.248781"
    },
    {
      "arxiv_id": "2410.21897v3",
      "title": "Semi-Supervised Self-Learning Enhanced Music Emotion Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Yifu Sun",
        "Xulong Zhang",
        "Monan Zhou",
        "Wei Li"
      ],
      "abstract": "Music emotion recognition (MER) aims to identify the emotions conveyed in a\ngiven musical piece. However, currently, in the field of MER, the available\npublic datasets have limited sample sizes. Recently, segment-based methods for\nemotion-related tasks have been proposed, which train backbone networks on\nshorter segments instead of entire audio clips, thereby naturally augmenting\ntraining samples without requiring additional resources. Then, the predicted\nsegment-level results are aggregated to obtain the entire song prediction. The\nmost commonly used method is that the segment inherits the label of the clip\ncontaining it, but music emotion is not constant during the whole clip. Doing\nso will introduce label noise and make the training easy to overfit. To handle\nthe noisy label issue, we propose a semi-supervised self-learning (SSSL)\nmethod, which can differentiate between samples with correct and incorrect\nlabels in a self-learning manner, thus effectively utilizing the augmented\nsegment-level data. Experiments on three public emotional datasets demonstrate\nthat the proposed method can achieve better or comparable performance.",
      "tldr_zh": "音乐情感识别 (MER) 面临公共数据集样本有限和标签噪声问题，因为传统段级方法让段落继承剪辑标签，但音乐情感并非恒定。针对此，本文提出了一种半监督自学习 (Semi-Supervised Self-Learning, SSSL) 方法，通过自学习方式区分正确和错误标签，从而有效利用段级数据增强训练样本。在三个公共情感数据集上的实验表明，该方法实现了更好或相当的性能表现。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "12 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.21897v3",
      "published_date": "2024-10-29 09:42:07 UTC",
      "updated_date": "2025-04-22 01:15:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:32:21.094835"
    },
    {
      "arxiv_id": "2410.21886v1",
      "title": "Bayesian Optimization for Hyperparameters Tuning in Neural Networks",
      "title_zh": "贝叶斯优化用于神经网络的超参数调优",
      "authors": [
        "Gabriele Onorato"
      ],
      "abstract": "This study investigates the application of Bayesian Optimization (BO) for the\nhyperparameter tuning of neural networks, specifically targeting the\nenhancement of Convolutional Neural Networks (CNN) for image classification\ntasks. Bayesian Optimization is a derivative-free global optimization method\nsuitable for expensive black-box functions with continuous inputs and limited\nevaluation budgets. The BO algorithm leverages Gaussian Process regression and\nacquisition functions like Upper Confidence Bound (UCB) and Expected\nImprovement (EI) to identify optimal configurations effectively. Using the Ax\nand BOTorch frameworks, this work demonstrates the efficiency of BO in reducing\nthe number of hyperparameter tuning trials while achieving competitive model\nperformance. Experimental outcomes reveal that BO effectively balances\nexploration and exploitation, converging rapidly towards optimal settings for\nCNN architectures. This approach underlines the potential of BO in automating\nneural network tuning, contributing to improved accuracy and computational\nefficiency in machine learning pipelines.",
      "tldr_zh": "本研究探讨了使用 Bayesian Optimization (BO) 优化神经网络超参数的方法，特别针对 Convolutional Neural Networks (CNN) 在图像分类任务中的应用。BO 作为一种无导数全局优化技术，利用 Gaussian Process 回归和获取函数如 Upper Confidence Bound (UCB) 与 Expected Improvement (EI)，结合 Ax 和 BOTorch 框架，显著减少了调优试验次数并提升模型性能。实验结果表明，BO 有效平衡探索与利用，快速收敛到最优配置，从而提高了机器学习管道的准确性和计算效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "Bachelor Thesis in Optimization for Machine Learning, 57 pages",
      "pdf_url": "http://arxiv.org/pdf/2410.21886v1",
      "published_date": "2024-10-29 09:23:24 UTC",
      "updated_date": "2024-10-29 09:23:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:32:33.588264"
    },
    {
      "arxiv_id": "2410.21882v1",
      "title": "Building Altruistic and Moral AI Agent with Brain-inspired Affective Empathy Mechanisms",
      "title_zh": "翻译失败",
      "authors": [
        "Feifei Zhao",
        "Hui Feng",
        "Haibo Tong",
        "Zhengqiang Han",
        "Enmeng Lu",
        "Yinqian Sun",
        "Yi Zeng"
      ],
      "abstract": "As AI closely interacts with human society, it is crucial to ensure that its\ndecision-making is safe, altruistic, and aligned with human ethical and moral\nvalues. However, existing research on embedding ethical and moral\nconsiderations into AI remains insufficient, and previous external constraints\nbased on principles and rules are inadequate to provide AI with long-term\nstability and generalization capabilities. In contrast, the intrinsic\naltruistic motivation based on empathy is more willing, spontaneous, and\nrobust. Therefore, this paper is dedicated to autonomously driving intelligent\nagents to acquire morally behaviors through human-like affective empathy\nmechanisms. We draw inspiration from the neural mechanism of human brain's\nmoral intuitive decision-making, and simulate the mirror neuron system to\nconstruct a brain-inspired affective empathy-driven altruistic decision-making\nmodel. Here, empathy directly impacts dopamine release to form intrinsic\naltruistic motivation. Based on the principle of moral utilitarianism, we\ndesign the moral reward function that integrates intrinsic empathy and\nextrinsic self-task goals. A comprehensive experimental scenario incorporating\nempathetic processes, personal objectives, and altruistic goals is developed.\nThe proposed model enables the agent to make consistent moral decisions\n(prioritizing altruism) by balancing self-interest with the well-being of\nothers. We further introduce inhibitory neurons to regulate different levels of\nempathy and verify the positive correlation between empathy levels and\naltruistic preferences, yielding conclusions consistent with findings from\npsychological behavioral experiments. This work provides a feasible solution\nfor the development of ethical AI by leveraging the intrinsic human-like\nempathy mechanisms, and contributes to the harmonious coexistence between\nhumans and AI.",
      "tldr_zh": "这篇论文提出了一种基于脑灵感的 affective empathy 机制来构建利他主义和道德 AI 代理，通过模拟人类 mirror neuron system 和 dopamine release，形成内在的 altruistic motivation。研究设计了 moral reward function，将内在 empathy 与外部任务目标相结合，并在实验场景中验证了模型能平衡自利与他人福祉，使 AI 优先做出道德决策。结果显示，empathy 水平与 altruistic preferences 正相关，这为开发安全、伦理化的 AI 提供了可行路径，促进人类与 AI 的和谐共存。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21882v1",
      "published_date": "2024-10-29 09:19:27 UTC",
      "updated_date": "2024-10-29 09:19:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:32:46.944011"
    },
    {
      "arxiv_id": "2410.21872v2",
      "title": "Advancing Efficient Brain Tumor Multi-Class Classification -- New Insights from the Vision Mamba Model in Transfer Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yinyi Lai",
        "Anbo Cao",
        "Yuan Gao",
        "Jiaqi Shang",
        "Zongyu Li",
        "Jia Guo"
      ],
      "abstract": "Early and accurate diagnosis of brain tumors is crucial for improving patient\nsurvival rates. However, the detection and classification of brain tumors are\nchallenging due to their diverse types and complex morphological\ncharacteristics. This study investigates the application of pre-trained models\nfor brain tumor classification, with a particular focus on deploying the Mamba\nmodel. We fine-tuned several mainstream transfer learning models and applied\nthem to the multi-class classification of brain tumors. By comparing these\nmodels to those trained from scratch, we demonstrated the significant\nadvantages of transfer learning, especially in the medical imaging field, where\nannotated data is often limited. Notably, we introduced the Vision Mamba (Vim),\na novel network architecture, and applied it for the first time in brain tumor\nclassification, achieving exceptional classification accuracy. Experimental\nresults indicate that the Vim model achieved 100% classification accuracy on an\nindependent test set, emphasizing its potential for tumor classification tasks.\nThese findings underscore the effectiveness of transfer learning in brain tumor\nclassification and reveal that, compared to existing state-of-the-art models,\nthe Vim model is lightweight, efficient, and highly accurate, offering a new\nperspective for clinical applications. Furthermore, the framework proposed in\nthis study for brain tumor classification, based on transfer learning and the\nVision Mamba model, is broadly applicable to other medical imaging\nclassification problems.",
      "tldr_zh": "本文研究了转移学习在脑肿瘤多类分类中的应用，特别引入了 Vision Mamba (Vim) 模型，以应对脑肿瘤多样类型和复杂形态的挑战。通过微调主流预训练模型并与从零训练模型比较，结果显示转移学习在标注数据有限的医疗成像领域具有显著优势。Vim 模型首次应用于脑肿瘤分类，实现了独立测试集上的 100% 准确率，并证明其比现有最先进模型更轻量、高效。该框架为临床应用提供新视角，并可扩展到其他医疗成像分类问题。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21872v2",
      "published_date": "2024-10-29 09:08:57 UTC",
      "updated_date": "2024-11-06 02:52:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:32:57.862239"
    },
    {
      "arxiv_id": "2410.21869v3",
      "title": "Cross-Entropy Is All You Need To Invert the Data Generating Process",
      "title_zh": "翻译失败",
      "authors": [
        "Patrik Reizinger",
        "Alice Bizeul",
        "Attila Juhos",
        "Julia E. Vogt",
        "Randall Balestriero",
        "Wieland Brendel",
        "David Klindt"
      ],
      "abstract": "Supervised learning has become a cornerstone of modern machine learning, yet\na comprehensive theory explaining its effectiveness remains elusive. Empirical\nphenomena, such as neural analogy-making and the linear representation\nhypothesis, suggest that supervised models can learn interpretable factors of\nvariation in a linear fashion. Recent advances in self-supervised learning,\nparticularly nonlinear Independent Component Analysis, have shown that these\nmethods can recover latent structures by inverting the data generating process.\nWe extend these identifiability results to parametric instance discrimination,\nthen show how insights transfer to the ubiquitous setting of supervised\nlearning with cross-entropy minimization. We prove that even in standard\nclassification tasks, models learn representations of ground-truth factors of\nvariation up to a linear transformation. We corroborate our theoretical\ncontribution with a series of empirical studies. First, using simulated data\nmatching our theoretical assumptions, we demonstrate successful disentanglement\nof latent factors. Second, we show that on DisLib, a widely-used\ndisentanglement benchmark, simple classification tasks recover latent\nstructures up to linear transformations. Finally, we reveal that models trained\non ImageNet encode representations that permit linear decoding of proxy factors\nof variation. Together, our theoretical findings and experiments offer a\ncompelling explanation for recent observations of linear representations, such\nas superposition in neural networks. This work takes a significant step toward\na cohesive theory that accounts for the unreasonable effectiveness of\nsupervised deep learning.",
      "tldr_zh": "该论文证明了在监督学习（supervised learning）中，使用交叉熵最小化（cross-entropy minimization）即可逆转数据生成过程，使模型学习真实因素的表示，直至线性变换（linear transformation）。研究扩展了自监督学习（self-supervised learning）中的可识别性（identifiability）结果，并将其应用于参数实例区分（parametric instance discrimination），从而解释了神经类比（neural analogy-making）和线性表示假设（linear representation hypothesis）。通过模拟数据、DisLib基准和ImageNet实验，作者展示了模型成功分离潜在因素，并揭示了神经网络中叠加（superposition）现象，为监督深度学习的有效性提供了连贯的理论基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 (oral) camera ready",
      "pdf_url": "http://arxiv.org/pdf/2410.21869v3",
      "published_date": "2024-10-29 09:03:57 UTC",
      "updated_date": "2025-02-25 07:22:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:35:03.014288"
    },
    {
      "arxiv_id": "2410.21853v2",
      "title": "Learning Infinitesimal Generators of Continuous Symmetries from Data",
      "title_zh": "翻译失败",
      "authors": [
        "Gyeonghoon Ko",
        "Hyunsu Kim",
        "Juho Lee"
      ],
      "abstract": "Exploiting symmetry inherent in data can significantly improve the sample\nefficiency of a learning procedure and the generalization of learned models.\nWhen data clearly reveals underlying symmetry, leveraging this symmetry can\nnaturally inform the design of model architectures or learning strategies. Yet,\nin numerous real-world scenarios, identifying the specific symmetry within a\ngiven data distribution often proves ambiguous. To tackle this, some existing\nworks learn symmetry in a data-driven manner, parameterizing and learning\nexpected symmetry through data. However, these methods often rely on explicit\nknowledge, such as pre-defined Lie groups, which are typically restricted to\nlinear or affine transformations. In this paper, we propose a novel symmetry\nlearning algorithm based on transformations defined with one-parameter groups,\ncontinuously parameterized transformations flowing along the directions of\nvector fields called infinitesimal generators. Our method is built upon minimal\ninductive biases, encompassing not only commonly utilized symmetries rooted in\nLie groups but also extending to symmetries derived from nonlinear generators.\nTo learn these symmetries, we introduce a notion of a validity score that\nexamine whether the transformed data is still valid for the given task. The\nvalidity score is designed to be fully differentiable and easily computable,\nenabling effective searches for transformations that achieve symmetries innate\nto the data. We apply our method mainly in two domains: image data and partial\ndifferential equations, and demonstrate its advantages. Our codes are available\nat \\url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}.",
      "tldr_zh": "这篇论文提出了一种新算法，用于从数据中学习连续对称性的无穷小生成器（infinitesimal generators），以提升学习效率和模型泛化能力。该方法基于一参数群和向量场（vector fields）的变换，支持非线性生成器，并通过一个可微的validity score来评估变换是否保持数据有效，从而减少了对预定义Lie groups的依赖。在图像数据和偏微分方程（partial differential equations）领域，实验结果展示了该算法的优势，并提供了开源代码。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Neurips 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.21853v2",
      "published_date": "2024-10-29 08:28:23 UTC",
      "updated_date": "2024-12-19 06:39:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:33:21.946615"
    },
    {
      "arxiv_id": "2411.00031v2",
      "title": "A Theoretical Review on Solving Algebra Problems",
      "title_zh": "代数问题求解的理论综述",
      "authors": [
        "Xinguo Yu",
        "Weina Cheng",
        "Chuanzhi Yang",
        "Ting Zhang"
      ],
      "abstract": "Solving algebra problems (APs) continues to attract significant research\ninterest as evidenced by the large number of algorithms and theories proposed\nover the past decade. Despite these important research contributions, however,\nthe body of work remains incomplete in terms of theoretical justification and\nscope. The current contribution intends to fill the gap by developing a review\nframework that aims to lay a theoretical base, create an evaluation scheme, and\nextend the scope of the investigation. This paper first develops the State\nTransform Theory (STT), which emphasizes that the problem-solving algorithms\nare structured according to states and transforms unlike the understanding that\nunderlies traditional surveys which merely emphasize the progress of\ntransforms. The STT, thus, lays the theoretical basis for a new framework for\nreviewing algorithms. This new construct accommodates the relation-centric\nalgorithms for solving both word and diagrammatic algebra problems. The latter\nnot only highlights the necessity of introducing new states but also allows\nrevelation of contributions of individual algorithms obscured in prior reviews\nwithout this approach.",
      "tldr_zh": "本论文对代数问题（APs）的解决算法进行理论回顾，旨在填补现有研究的理论基础和范围缺口，通过开发 State Transform Theory (STT) 作为核心框架。STT 强调算法基于状态和变换的结构，而不是传统调查仅关注变换进展，从而为评估方案提供新理论基础。该框架适用于文字和图示代数问题的关系中心算法，不仅突出了引入新状态的必要性，还揭示了先前审查中被忽略的个别算法贡献。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.LO",
      "comment": "22pages,5figures",
      "pdf_url": "http://arxiv.org/pdf/2411.00031v2",
      "published_date": "2024-10-29 08:16:49 UTC",
      "updated_date": "2024-12-23 02:57:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:33:34.402699"
    },
    {
      "arxiv_id": "2410.21845v3",
      "title": "Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning",
      "title_zh": "通过人机循环强化学习的",
      "authors": [
        "Jianlan Luo",
        "Charles Xu",
        "Jeffrey Wu",
        "Sergey Levine"
      ],
      "abstract": "Reinforcement learning (RL) holds great promise for enabling autonomous\nacquisition of complex robotic manipulation skills, but realizing this\npotential in real-world settings has been challenging. We present a\nhuman-in-the-loop vision-based RL system that demonstrates impressive\nperformance on a diverse set of dexterous manipulation tasks, including dynamic\nmanipulation, precision assembly, and dual-arm coordination. Our approach\nintegrates demonstrations and human corrections, efficient RL algorithms, and\nother system-level design choices to learn policies that achieve near-perfect\nsuccess rates and fast cycle times within just 1 to 2.5 hours of training. We\nshow that our method significantly outperforms imitation learning baselines and\nprior RL approaches, with an average 2x improvement in success rate and 1.8x\nfaster execution. Through extensive experiments and analysis, we provide\ninsights into the effectiveness of our approach, demonstrating how it learns\nrobust, adaptive policies for both reactive and predictive control strategies.\nOur results suggest that RL can indeed learn a wide range of complex\nvision-based manipulation policies directly in the real world within practical\ntraining times. We hope this work will inspire a new generation of learned\nrobotic manipulation techniques, benefiting both industrial applications and\nresearch advancements. Videos and code are available at our project website\nhttps://hil-serl.github.io/.",
      "tldr_zh": "本文提出了一种基于Human-in-the-Loop强化学习（RL）的系统，用于实现精确和灵巧的机器人操作，包括动态操作、精确组装和双臂协调等任务。该方法整合演示、人类修正以及高效RL算法，仅需1-2.5小时训练即可达到近乎完美的成功率和快速循环时间。与模仿学习基准相比，该系统平均成功率提高2倍，执行速度快1.8倍，并通过实验分析展示了鲁棒的适应性策略。研究结果表明，RL能够在真实环境中快速学习复杂视觉操作策略，推动工业应用和研究进展。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21845v3",
      "published_date": "2024-10-29 08:12:20 UTC",
      "updated_date": "2025-03-20 09:16:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:33:45.861984"
    },
    {
      "arxiv_id": "2410.21842v1",
      "title": "Diffusion as Reasoning: Enhancing Object Goal Navigation with LLM-Biased Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Yiming Ji",
        "Yang Liu",
        "Zhengpu Wang",
        "Boyu Ma",
        "Zongwu Xie",
        "Hong Liu"
      ],
      "abstract": "The Object Goal Navigation (ObjectNav) task requires the agent to navigate to\na specified target in an unseen environment. Since the environment layout is\nunknown, the agent needs to perform semantic reasoning to infer the potential\nlocation of the target, based on its accumulated memory of the environment\nduring the navigation process. Diffusion models have been shown to be able to\nlearn the distribution relationships between features in RGB images, and thus\ngenerate new realistic images.In this work, we propose a new approach to\nsolving the ObjectNav task, by training a diffusion model to learn the\nstatistical distribution patterns of objects in semantic maps, and using the\nmap of the explored regions during navigation as the condition to generate the\nmap of the unknown regions, thereby realizing the semantic reasoning of the\ntarget object, i.e., diffusion as reasoning (DAR). Meanwhile, we propose the\nglobal target bias and local LLM bias methods, where the former can constrain\nthe diffusion model to generate the target object more effectively, and the\nlatter utilizes the common sense knowledge extracted from the LLM to improve\nthe generalization of the reasoning process. Based on the generated map in the\nunknown region, the agent sets the predicted location of the target as the goal\nand moves towards it. Experiments on Gibson and MP3D show the effectiveness of\nour method.",
      "tldr_zh": "本研究针对 Object Goal Navigation (ObjectNav) 任务，提出了一种将扩散模型 (Diffusion Model) 作为语义推理工具的方法，即 Diffusion as Reasoning (DAR)，通过学习语义地图中物体的统计分布，使用已探索区域的地图作为条件生成未知区域的地图，从而帮助代理推断目标位置。作者引入了全局目标偏差 (Global Target Bias) 和本地 LLM 偏差 (Local LLM Bias) 机制，前者优化扩散模型对目标对象的生成，后者利用 LLM 的常识知识提升推理的泛化能力。实验在 Gibson 和 MP3D 数据集上验证了该方法的有效性，显著提高了导航性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21842v1",
      "published_date": "2024-10-29 08:10:06 UTC",
      "updated_date": "2024-10-29 08:10:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:33:57.387652"
    },
    {
      "arxiv_id": "2410.22377v2",
      "title": "A Systematic Literature Review of Spatio-Temporal Graph Neural Network Models for Time Series Forecasting and Classification",
      "title_zh": "关于时空图神经网络模型用于时间序列预测和分类的系统文献综述",
      "authors": [
        "Flavio Corradini",
        "Flavio Gerosa",
        "Marco Gori",
        "Carlo Lucheroni",
        "Marco Piangerelli",
        "Martina Zannotti"
      ],
      "abstract": "In recent years, spatio-temporal graph neural networks (GNNs) have attracted\nconsiderable interest in the field of time series analysis, due to their\nability to capture dependencies among variables and across time points. The\nobjective of the presented systematic literature review is hence to provide a\ncomprehensive overview of the various modeling approaches and application\ndomains of GNNs for time series classification and forecasting. A database\nsearch was conducted, and over 150 journal papers were selected for a detailed\nexamination of the current state-of-the-art in the field. This examination is\nintended to offer to the reader a comprehensive collection of proposed models,\nlinks to related source code, available datasets, benchmark models, and fitting\nresults. All this information is hoped to assist researchers in future studies.\nTo the best of our knowledge, this is the first systematic literature review\npresenting a detailed comparison of the results of current spatio-temporal GNN\nmodels in different domains. In addition, in its final part this review\ndiscusses current limitations and challenges in the application of\nspatio-temporal GNNs, such as comparability, reproducibility, explainability,\npoor information capacity, and scalability.",
      "tldr_zh": "该论文通过系统文献综述，概述了时空图神经网络 (Spatio-Temporal Graph Neural Networks, GNNs) 在时间序列预测和分类中的建模方法及其应用领域，强调了 GNNs 捕捉变量间依赖性和时间点依赖性的优势。研究者筛选了超过 150 篇期刊论文，提供全面的模型集合、源代码链接、可用数据集、基准模型和性能结果，以支持未来研究。论文首次进行详细比较不同领域的 GNNs 模型结果，并讨论了当前挑战，包括可比性、可再现性、可解释性、信息容量不足和可扩展性问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.data-an"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22377v2",
      "published_date": "2024-10-29 08:05:10 UTC",
      "updated_date": "2025-05-07 10:08:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:34:10.661271"
    },
    {
      "arxiv_id": "2411.00030v2",
      "title": "WikiNER-fr-gold: A Gold-Standard NER Corpus",
      "title_zh": "翻译失败",
      "authors": [
        "Danrun Cao",
        "Nicolas Béchet",
        "Pierre-François Marteau"
      ],
      "abstract": "We address in this article the the quality of the WikiNER corpus, a\nmultilingual Named Entity Recognition corpus, and provide a consolidated\nversion of it. The annotation of WikiNER was produced in a semi-supervised\nmanner i.e. no manual verification has been carried out a posteriori. Such\ncorpus is called silver-standard. In this paper we propose WikiNER-fr-gold\nwhich is a revised version of the French proportion of WikiNER. Our corpus\nconsists of randomly sampled 20% of the original French sub-corpus (26,818\nsentences with 700k tokens). We start by summarizing the entity types included\nin each category in order to define an annotation guideline, and then we\nproceed to revise the corpus. Finally we present an analysis of errors and\ninconsistency observed in the WikiNER-fr corpus, and we discuss potential\nfuture work directions.",
      "tldr_zh": "本文提出WikiNER-fr-gold，一种金标准(gold-standard)命名实体识别(NER)语料库，通过修订WikiNER多语言语料库的法语部分来提升其质量。作者从原法语子集随机采样20%（包括26,818句子和700k tokens），总结实体类型并定义注释指南后进行手动修订。研究分析了WikiNER-fr中的错误和不一致问题，并讨论了潜在的未来工作方向，以支持更可靠的NER应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00030v2",
      "published_date": "2024-10-29 08:00:16 UTC",
      "updated_date": "2025-04-28 08:16:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:34:21.552532"
    },
    {
      "arxiv_id": "2411.00029v1",
      "title": "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Donghoon Kim",
        "Gusang Lee",
        "Kyuhong Shim",
        "Byonghyo Shim"
      ],
      "abstract": "Recently, we have observed that Large Multi-modal Models (LMMs) are\nrevolutionizing the way machines interact with the world, unlocking new\npossibilities across various multi-modal applications. To adapt LMMs for\ndownstream tasks, parameter-efficient fine-tuning (PEFT) which only trains\nadditional prefix tokens or modules, has gained popularity. Nevertheless, there\nhas been little analysis of how PEFT works in LMMs. In this paper, we delve\ninto the strengths and weaknesses of each tuning strategy, shifting the focus\nfrom the efficiency typically associated with these approaches. We first\ndiscover that model parameter tuning methods such as LoRA and Adapters distort\nthe feature representation space learned during pre-training and limit the full\nutilization of pre-trained knowledge. We also demonstrate that prefix-tuning\nexcels at preserving the representation space, despite its lower performance on\ndownstream tasks. These findings suggest a simple two-step PEFT strategy called\nPrefix-Tuned PEFT (PT-PEFT), which successively performs prefix-tuning and then\nPEFT (i.e., Adapter, LoRA), combines the benefits of both. Experimental results\nshow that PT-PEFT not only improves performance in image captioning and visual\nquestion answering compared to vanilla PEFT methods but also helps preserve the\nrepresentation space of the four pre-trained models.",
      "tldr_zh": "该研究探讨了参数高效微调 (PEFT) 在 Large Multi-modal Models (LMMs) 中的有效性，发现传统方法如 LoRA 和 Adapters 会扭曲预训练的特征表示空间，从而限制了预训练知识的利用。相比之下，prefix-tuning 更擅长保留表示空间，尽管其在下游任务上的性能较低。作者提出了一种两步策略 Prefix-Tuned PEFT (PT-PEFT)，即先进行 prefix-tuning 再应用其他 PEFT 方法，结果显示它在图像字幕和视觉问答任务上提升了性能，同时更好地保护了四个预训练模型的表示空间。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Findings of EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.00029v1",
      "published_date": "2024-10-29 07:55:50 UTC",
      "updated_date": "2024-10-29 07:55:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:35:13.842689"
    },
    {
      "arxiv_id": "2410.22376v2",
      "title": "Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Dongmin Park",
        "Sebin Kim",
        "Taehong Moon",
        "Minkyu Kim",
        "Kangwook Lee",
        "Jaewoong Cho"
      ],
      "abstract": "State-of-the-art text-to-image (T2I) diffusion models often struggle to\ngenerate rare compositions of concepts, e.g., objects with unusual attributes.\nIn this paper, we show that the compositional generation power of diffusion\nmodels on such rare concepts can be significantly enhanced by the Large\nLanguage Model (LLM) guidance. We start with empirical and theoretical\nanalysis, demonstrating that exposing frequent concepts relevant to the target\nrare concepts during the diffusion sampling process yields more accurate\nconcept composition. Based on this, we propose a training-free approach, R2F,\nthat plans and executes the overall rare-to-frequent concept guidance\nthroughout the diffusion inference by leveraging the abundant semantic\nknowledge in LLMs. Our framework is flexible across any pre-trained diffusion\nmodels and LLMs, and can be seamlessly integrated with the region-guided\ndiffusion approaches. Extensive experiments on three datasets, including our\nnewly proposed benchmark, RareBench, containing various prompts with rare\ncompositions of concepts, R2F significantly surpasses existing models including\nSD3.0 and FLUX by up to 28.1%p in T2I alignment. Code is available at\nhttps://github.com/krafton-ai/Rare-to-Frequent.",
      "tldr_zh": "本研究发现，现有的文本到图像 (T2I) 扩散模型在生成稀有概念组合（如物体带有不寻常属性）时表现欠佳，并通过实证和理论分析证明，使用大型语言模型 (LLM) 指导可通过引入相关频繁概念来提升生成准确性。作者提出了一种无训练方法 Rare-to-Frequent (R2F)，利用 LLM 的语义知识在扩散推理过程中规划和执行从稀有到频繁概念的指导，使其兼容任何预训练扩散模型并可与区域引导扩散方法整合。在三个数据集上的实验，包括新基准 RareBench，R2F 在 T2I 对齐方面比现有模型如 SD3.0 和 FLUX 提高了多达 28.1%，显著提升了概念组合生成能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22376v2",
      "published_date": "2024-10-29 07:43:39 UTC",
      "updated_date": "2025-01-07 01:41:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:35:27.884956"
    },
    {
      "arxiv_id": "2410.21815v2",
      "title": "Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Shaobo Wang",
        "Hongxuan Tang",
        "Mingyang Wang",
        "Hongrui Zhang",
        "Xuyang Liu",
        "Weiya Li",
        "Xuming Hu",
        "Linfeng Zhang"
      ],
      "abstract": "The debate between self-interpretable models and post-hoc explanations for\nblack-box models is central to Explainable AI (XAI). Self-interpretable models,\nsuch as concept-based networks, offer insights by connecting decisions to\nhuman-understandable concepts but often struggle with performance and\nscalability. Conversely, post-hoc methods like Shapley values, while\ntheoretically robust, are computationally expensive and resource-intensive. To\nbridge the gap between these two lines of research, we propose a novel method\nthat combines their strengths, providing theoretically guaranteed\nself-interpretability for black-box models without compromising prediction\naccuracy. Specifically, we introduce a parameter-efficient pipeline,\nAutoGnothi, which integrates a small side network into the black-box model,\nallowing it to generate Shapley value explanations without changing the\noriginal network parameters. This side-tuning approach significantly reduces\nmemory, training, and inference costs, outperforming traditional\nparameter-efficient methods, where full fine-tuning serves as the optimal\nbaseline. AutoGnothi enables the black-box model to predict and explain its\npredictions with minimal overhead. Extensive experiments show that AutoGnothi\noffers accurate explanations for both vision and language tasks, delivering\nsuperior computational efficiency with comparable interpretability.",
      "tldr_zh": "该论文探讨了Explainable AI (XAI)中自解释模型与后验解释方法的权衡问题，提出了一种创新方法Gnothi Seauton，通过参数高效的AutoGnothi管道为black-box transformers提供理论上可靠的自解释性，同时保持预测准确性不变。AutoGnothi将一个小型侧网络集成到原模型中，使用side-tuning生成Shapley values解释，而不修改原网络参数，从而显著降低内存、训练和推理成本。实验结果表明，该方法在视觉和语言任务上提供准确解释，并超越传统参数高效方法，提供更高的计算效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.GT"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICLR 2025, 29 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.21815v2",
      "published_date": "2024-10-29 07:35:33 UTC",
      "updated_date": "2025-02-25 01:58:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:35:39.011748"
    },
    {
      "arxiv_id": "2411.08041v1",
      "title": "GraphAide: Advanced Graph-Assisted Query and Reasoning System",
      "title_zh": "GraphAide：先进的图辅助查询和推理系统",
      "authors": [
        "Sumit Purohit",
        "George Chin",
        "Patrick S Mackey",
        "Joseph A Cottam"
      ],
      "abstract": "Curating knowledge from multiple siloed sources that contain both structured\nand unstructured data is a major challenge in many real-world applications.\nPattern matching and querying represent fundamental tasks in modern data\nanalytics that leverage this curated knowledge. The development of such\napplications necessitates overcoming several research challenges, including\ndata extraction, named entity recognition, data modeling, and designing query\ninterfaces. Moreover, the explainability of these functionalities is critical\nfor their broader adoption.\n  The emergence of Large Language Models (LLMs) has accelerated the development\nlifecycle of new capabilities. Nonetheless, there is an ongoing need for\ndomain-specific tools tailored to user activities. The creation of digital\nassistants has gained considerable traction in recent years, with LLMs offering\na promising avenue to develop such assistants utilizing domain-specific\nknowledge and assumptions.\n  In this context, we introduce an advanced query and reasoning system,\nGraphAide, which constructs a knowledge graph (KG) from diverse sources and\nallows to query and reason over the resulting KG. GraphAide harnesses both the\nKG and LLMs to rapidly develop domain-specific digital assistants. It\nintegrates design patterns from retrieval augmented generation (RAG) and the\nsemantic web to create an agentic LLM application. GraphAide underscores the\npotential for streamlined and efficient development of specialized digital\nassistants, thereby enhancing their applicability across various domains.",
      "tldr_zh": "该论文介绍了GraphAide，一种先进的图辅助查询和推理系统，用于从多源结构化和非结构化数据中整理知识，解决数据提取、命名实体识别、数据建模和查询接口设计等挑战，同时强调功能的解释性。GraphAide通过构建Knowledge Graph (KG)并结合Large Language Models (LLMs)，整合Retrieval Augmented Generation (RAG)和Semantic Web的设计模式，快速开发领域特定的数字助手。实验结果显示，该系统提升了查询和推理的效率，并扩展了数字助手的适用性，适用于各种实际应用场景。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.08041v1",
      "published_date": "2024-10-29 07:25:30 UTC",
      "updated_date": "2024-10-29 07:25:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:35:50.325643"
    },
    {
      "arxiv_id": "2410.21807v2",
      "title": "A Fresh Look at Generalized Category Discovery through Non-negative Matrix Factorization",
      "title_zh": "翻译失败",
      "authors": [
        "Zhong Ji",
        "Shuo Yang",
        "Jingren Liu",
        "Yanwei Pang",
        "Jungong Han"
      ],
      "abstract": "Generalized Category Discovery (GCD) aims to classify both base and novel\nimages using labeled base data. However, current approaches inadequately\naddress the intrinsic optimization of the co-occurrence matrix $\\bar{A}$ based\non cosine similarity, failing to achieve zero base-novel regions and adequate\nsparsity in base and novel domains. To address these deficiencies, we propose a\nNon-Negative Generalized Category Discovery (NN-GCD) framework. It employs\nSymmetric Non-negative Matrix Factorization (SNMF) as a mathematical medium to\nprove the equivalence of optimal K-means with optimal SNMF, and the equivalence\nof SNMF solver with non-negative contrastive learning (NCL) optimization.\nUtilizing these theoretical equivalences, it reframes the optimization of\n$\\bar{A}$ and K-means clustering as an NCL optimization problem. Moreover, to\nsatisfy the non-negative constraints and make a GCD model converge to a\nnear-optimal region, we propose a GELU activation function and an NMF NCE loss.\nTo transition $\\bar{A}$ from a suboptimal state to the desired $\\bar{A}^*$, we\nintroduce a hybrid sparse regularization approach to impose sparsity\nconstraints. Experimental results show NN-GCD outperforms state-of-the-art\nmethods on GCD benchmarks, achieving an average accuracy of 66.1\\% on the\nSemantic Shift Benchmark, surpassing prior counterparts by 4.7\\%.",
      "tldr_zh": "该论文重新审视了 Generalized Category Discovery (GCD)，针对现有方法在优化 co-occurrence matrix \\(\\bar{A}\\) 时存在的零基-新型区域问题和稀疏性不足，提出 Non-Negative Generalized Category Discovery (NN-GCD) 框架。NN-GCD 通过 Symmetric Non-negative Matrix Factorization (SNMF) 证明了最优 K-means 与最优 SNMF 的等价性，以及 SNMF 求解器与 non-negative contrastive learning (NCL) 优化的等价性，从而将 \\(\\bar{A}\\) 和 K-means 聚类优化转化为 NCL 问题。框架还引入 GELU 激活函数、NMF NCE 损失和混合稀疏正则化，以满足非负约束并提升模型收敛性。实验结果显示，NN-GCD 在 GCD 基准上超越最先进方法，在 Semantic Shift Benchmark 上平均准确率达到 66.1%，比前人提升 4.7%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.21807v2",
      "published_date": "2024-10-29 07:24:11 UTC",
      "updated_date": "2024-10-30 01:34:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:36:03.659944"
    },
    {
      "arxiv_id": "2411.00827v3",
      "title": "IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves",
      "title_zh": "翻译失败",
      "authors": [
        "Ruofan Wang",
        "Juncheng Li",
        "Yixu Wang",
        "Bo Wang",
        "Xiaosen Wang",
        "Yan Teng",
        "Yingchun Wang",
        "Xingjun Ma",
        "Yu-Gang Jiang"
      ],
      "abstract": "As large Vision-Language Models (VLMs) gain prominence, ensuring their safe\ndeployment has become critical. Recent studies have explored VLM robustness\nagainst jailbreak attacks-techniques that exploit model vulnerabilities to\nelicit harmful outputs. However, the limited availability of diverse multimodal\ndata has constrained current approaches to rely heavily on adversarial or\nmanually crafted images derived from harmful text datasets, which often lack\neffectiveness and diversity across different contexts. In this paper, we\npropose IDEATOR, a novel jailbreak method that autonomously generates malicious\nimage-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the\ninsight that VLMs themselves could serve as powerful red team models for\ngenerating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM\nto create targeted jailbreak texts and pairs them with jailbreak images\ngenerated by a state-of-the-art diffusion model. Extensive experiments\ndemonstrate IDEATOR's high effectiveness and transferability, achieving a 94%\nattack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only\n5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA,\nInstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong\ntransferability and automated process, we introduce the VLBreakBench, a safety\nbenchmark comprising 3,654 multimodal jailbreak samples. Our benchmark results\non 11 recently released VLMs reveal significant gaps in safety alignment. For\ninstance, our challenge set achieves ASRs of 46.31% on GPT-4o and 19.65% on\nClaude-3.5-Sonnet, underscoring the urgent need for stronger defenses.",
      "tldr_zh": "本文提出 IDEATOR，一种创新方法，利用视觉语言模型（VLMs）自身作为红队模型，自主生成恶意图像-文本对进行黑盒越狱攻击，并结合状态-of-the-art 扩散模型提升攻击效果。实验结果显示，IDEATOR 在 MiniGPT-4 上实现 94% 的攻击成功率（ASR），平均仅需 5.34 个查询，且在 LLaVA、InstructBLIP 和 Chameleon 等模型上转移 ASR 分别为 82%、88% 和 75%。此外，作者基于此开发了 VLBreakBench 基准，包含 3654 个多模态越狱样本，对 11 个 VLMs 的安全评估揭示显著漏洞，如 GPT-4o 的 46.31% ASR，强调了加强模型防御的迫切需求。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00827v3",
      "published_date": "2024-10-29 07:15:56 UTC",
      "updated_date": "2025-03-08 17:39:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:36:15.652646"
    },
    {
      "arxiv_id": "2410.21802v2",
      "title": "Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Lu Yu",
        "Haiyang Zhang",
        "Changsheng Xu"
      ],
      "abstract": "Due to the impressive zero-shot capabilities, pre-trained vision-language\nmodels (e.g. CLIP), have attracted widespread attention and adoption across\nvarious domains. Nonetheless, CLIP has been observed to be susceptible to\nadversarial examples. Through experimental analysis, we have observed a\nphenomenon wherein adversarial perturbations induce shifts in text-guided\nattention. Building upon this observation, we propose a simple yet effective\nstrategy: Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR). This\nframework incorporates two components: the Attention Refinement module and the\nAttention-based Model Constraint module. Our goal is to maintain the\ngeneralization of the CLIP model and enhance its adversarial robustness: The\nAttention Refinement module aligns the text-guided attention obtained from the\ntarget model via adversarial examples with the text-guided attention acquired\nfrom the original model via clean examples. This alignment enhances the model's\nrobustness. Additionally, the Attention-based Model Constraint module acquires\ntext-guided attention from both the target and original models using clean\nexamples. Its objective is to maintain model performance on clean samples while\nenhancing overall robustness. The experiments validate that our method yields a\n9.58% enhancement in zero-shot robust accuracy over the current\nstate-of-the-art techniques across 16 datasets. Our code is available at\nhttps://github.com/zhyblue424/TGA-ZSR.",
      "tldr_zh": "本文研究发现，预训练视觉语言模型如 CLIP 虽然具备强大的零样本能力，但容易受到对抗样本攻击，导致文本引导注意力偏移。为解决这一问题，作者提出 TGA-ZSR 框架，包括 Attention Refinement 模块（通过对齐对抗样本和干净样本的注意力提升鲁棒性）和 Attention-based Model Constraint 模块（确保模型在干净样本上保持性能）。实验结果显示，该方法在 16 个数据集上，比现有最先进技术提高了 9.58% 的零样本鲁棒准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.21802v2",
      "published_date": "2024-10-29 07:15:09 UTC",
      "updated_date": "2024-10-30 01:22:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:36:27.144238"
    },
    {
      "arxiv_id": "2410.21795v2",
      "title": "Robot Policy Learning with Temporal Optimal Transport Reward",
      "title_zh": "基于时间最优传输奖励的机器人策略学习",
      "authors": [
        "Yuwei Fu",
        "Haichao Zhang",
        "Di Wu",
        "Wei Xu",
        "Benoit Boulet"
      ],
      "abstract": "Reward specification is one of the most tricky problems in Reinforcement\nLearning, which usually requires tedious hand engineering in practice. One\npromising approach to tackle this challenge is to adopt existing expert video\ndemonstrations for policy learning. Some recent work investigates how to learn\nrobot policies from only a single/few expert video demonstrations. For example,\nreward labeling via Optimal Transport (OT) has been shown to be an effective\nstrategy to generate a proxy reward by measuring the alignment between the\nrobot trajectory and the expert demonstrations. However, previous work mostly\noverlooks that the OT reward is invariant to temporal order information, which\ncould bring extra noise to the reward signal. To address this issue, in this\npaper, we introduce the Temporal Optimal Transport (TemporalOT) reward to\nincorporate temporal order information for learning a more accurate OT-based\nproxy reward. Extensive experiments on the Meta-world benchmark tasks validate\nthe efficacy of the proposed method. Code is available at:\nhttps://github.com/fuyw/TemporalOT",
      "tldr_zh": "本研究针对强化学习(Reinforcement Learning)中奖励指定的难题，提出了一种利用Temporal Optimal Transport (TemporalOT)奖励的方法，以从专家视频演示中学习机器人策略。传统Optimal Transport (OT)奖励忽略了时间顺序信息，导致奖励信号中引入噪音，而TemporalOT通过整合时间顺序来生成更准确的代理奖励。实验在Meta-world基准任务上验证了该方法的有效性，展示了显著的性能提升，并提供了开源代码（https://github.com/fuyw/TemporalOT）。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.21795v2",
      "published_date": "2024-10-29 07:00:47 UTC",
      "updated_date": "2024-11-02 02:09:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:36:37.870857"
    },
    {
      "arxiv_id": "2410.21794v2",
      "title": "Inverse Attention Agents for Multi-Agent Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Qian Long",
        "Ruoyan Li",
        "Minglu Zhao",
        "Tao Gao",
        "Demetri Terzopoulos"
      ],
      "abstract": "A major challenge for Multi-Agent Systems is enabling agents to adapt\ndynamically to diverse environments in which opponents and teammates may\ncontinually change. Agents trained using conventional methods tend to excel\nonly within the confines of their training cohorts; their performance drops\nsignificantly when confronting unfamiliar agents. To address this shortcoming,\nwe introduce Inverse Attention Agents that adopt concepts from the Theory of\nMind (ToM) implemented algorithmically using an attention mechanism trained in\nan end-to-end manner. Crucial to determining the final actions of these agents,\nthe weights in their attention model explicitly represent attention to\ndifferent goals. We furthermore propose an inverse attention network that\ndeduces the ToM of agents based on observations and prior actions. The network\ninfers the attentional states of other agents, thereby refining the attention\nweights to adjust the agent's final action. We conduct experiments in a\ncontinuous environment, tackling demanding tasks encompassing cooperation,\ncompetition, and a blend of both. They demonstrate that the inverse attention\nnetwork successfully infers the attention of other agents, and that this\ninformation improves agent performance. Additional human experiments show that,\ncompared to baseline agent models, our inverse attention agents exhibit\nsuperior cooperation with humans and better emulate human behaviors.",
      "tldr_zh": "本研究针对多智能体系统（Multi-Agent Systems）中代理难以适应动态环境的挑战，提出了一种Inverse Attention Agents框架，该框架借鉴Theory of Mind (ToM)概念，通过端到端训练的注意力机制（attention mechanism）来推断其他代理的目标关注权重，并调整自身行动。Inverse Attention Agents的核心是inverse attention network，该网络基于观察和先前行动推断其他代理的注意力状态，从而提升代理在未知环境中的适应性。实验结果显示，该框架在合作、竞争和混合任务的连续环境中显著提高了代理性能，且在人类实验中表现出色，能更好地与人类合作并模仿人类行为。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21794v2",
      "published_date": "2024-10-29 06:59:11 UTC",
      "updated_date": "2025-04-07 22:59:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:36:50.095722"
    },
    {
      "arxiv_id": "2410.21784v1",
      "title": "MARCO: Multi-Agent Real-time Chat Orchestration",
      "title_zh": "翻译失败",
      "authors": [
        "Anubhav Shrimal",
        "Stanley Kanagaraj",
        "Kriti Biswas",
        "Swarnalatha Raghuraman",
        "Anish Nediyanchath",
        "Yi Zhang",
        "Promod Yenigalla"
      ],
      "abstract": "Large language model advancements have enabled the development of multi-agent\nframeworks to tackle complex, real-world problems such as to automate tasks\nthat require interactions with diverse tools, reasoning, and human\ncollaboration. We present MARCO, a Multi-Agent Real-time Chat Orchestration\nframework for automating tasks using LLMs. MARCO addresses key challenges in\nutilizing LLMs for complex, multi-step task execution. It incorporates robust\nguardrails to steer LLM behavior, validate outputs, and recover from errors\nthat stem from inconsistent output formatting, function and parameter\nhallucination, and lack of domain knowledge. Through extensive experiments we\ndemonstrate MARCO's superior performance with 94.48% and 92.74% accuracy on\ntask execution for Digital Restaurant Service Platform conversations and Retail\nconversations datasets respectively along with 44.91% improved latency and\n33.71% cost reduction. We also report effects of guardrails in performance gain\nalong with comparisons of various LLM models, both open-source and proprietary.\nThe modular and generic design of MARCO allows it to be adapted for automating\ntasks across domains and to execute complex usecases through multi-turn\ninteractions.",
      "tldr_zh": "本文提出MARCO，一种多智能体实时聊天编排框架，利用大型语言模型（LLMs）来自动化涉及工具互动、推理和人类协作的复杂任务。MARCO通过引入稳健的防护措施（guardrails）来引导LLM行为、验证输出并从错误（如输出格式不一致、函数和参数幻觉）中恢复。实验结果显示，MARCO在Digital Restaurant Service Platform和Retail对话数据集上分别实现了94.48%和92.74%的任务执行准确率，同时改善了44.91%的延迟和33.71%的成本。其模块化设计使其易于适应不同领域和多轮交互的复杂用例。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "EMNLP 2024 Industry Track",
      "pdf_url": "http://arxiv.org/pdf/2410.21784v1",
      "published_date": "2024-10-29 06:42:27 UTC",
      "updated_date": "2024-10-29 06:42:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:37:03.849718"
    },
    {
      "arxiv_id": "2411.02592v1",
      "title": "Decoupled Data Augmentation for Improving Image Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Ruoxin Chen",
        "Zhe Wang",
        "Ke-Yue Zhang",
        "Shuang Wu",
        "Jiamu Sun",
        "Shouli Wang",
        "Taiping Yao",
        "Shouhong Ding"
      ],
      "abstract": "Recent advancements in image mixing and generative data augmentation have\nshown promise in enhancing image classification. However, these techniques face\nthe challenge of balancing semantic fidelity with diversity. Specifically,\nimage mixing involves interpolating two images to create a new one, but this\npixel-level interpolation can compromise fidelity. Generative augmentation uses\ntext-to-image generative models to synthesize or modify images, often limiting\ndiversity to avoid generating out-of-distribution data that potentially affects\naccuracy. We propose that this fidelity-diversity dilemma partially stems from\nthe whole-image paradigm of existing methods. Since an image comprises the\nclass-dependent part (CDP) and the class-independent part (CIP), where each\npart has fundamentally different impacts on the image's fidelity, treating\ndifferent parts uniformly can therefore be misleading. To address this\nfidelity-diversity dilemma, we introduce Decoupled Data Augmentation (De-DA),\nwhich resolves the dilemma by separating images into CDPs and CIPs and handling\nthem adaptively. To maintain fidelity, we use generative models to modify real\nCDPs under controlled conditions, preserving semantic consistency. To enhance\ndiversity, we replace the image's CIP with inter-class variants, creating\ndiverse CDP-CIP combinations. Additionally, we implement an online randomized\ncombination strategy during training to generate numerous distinct CDP-CIP\ncombinations cost-effectively. Comprehensive empirical evaluations validate the\neffectiveness of our method.",
      "tldr_zh": "论文提出 Decoupled Data Augmentation (De-DA) 方法，以解决现有图像增强技术在语义保真度和多样性之间的平衡难题。De-DA 将图像分解为类依赖部分 (CDP) 和类独立部分 (CIP)，通过使用生成模型在受控条件下修改 CDP 以保持语义一致性，并替换 CIP 以创建多样化的 CDP-CIP 组合，从而提升图像分类性能。此外，该方法采用在线随机组合策略，成本有效地生成多种变体，实证评估证明其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.02592v1",
      "published_date": "2024-10-29 06:27:09 UTC",
      "updated_date": "2024-10-29 06:27:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:37:15.640177"
    },
    {
      "arxiv_id": "2411.08900v1",
      "title": "RNA-GPT: Multimodal Generative System for RNA Sequence Understanding",
      "title_zh": "RNA-GPT：用于 RNA 序列理解的多模态生成式系统",
      "authors": [
        "Yijia Xiao",
        "Edward Sun",
        "Yiqiao Jin",
        "Wei Wang"
      ],
      "abstract": "RNAs are essential molecules that carry genetic information vital for life,\nwith profound implications for drug development and biotechnology. Despite this\nimportance, RNA research is often hindered by the vast literature available on\nthe topic. To streamline this process, we introduce RNA-GPT, a multi-modal RNA\nchat model designed to simplify RNA discovery by leveraging extensive RNA\nliterature. RNA-GPT integrates RNA sequence encoders with linear projection\nlayers and state-of-the-art large language models (LLMs) for precise\nrepresentation alignment, enabling it to process user-uploaded RNA sequences\nand deliver concise, accurate responses. Built on a scalable training pipeline,\nRNA-GPT utilizes RNA-QA, an automated system that gathers RNA annotations from\nRNACentral using a divide-and-conquer approach with GPT-4o and latent Dirichlet\nallocation (LDA) to efficiently handle large datasets and generate\ninstruction-tuning samples. Our experiments indicate that RNA-GPT effectively\naddresses complex RNA queries, thereby facilitating RNA research. Additionally,\nwe present RNA-QA, a dataset of 407,616 RNA samples for modality alignment and\ninstruction tuning, further advancing the potential of RNA research tools.",
      "tldr_zh": "本文提出 RNA-GPT，一种多模态生成系统，旨在简化 RNA 序列理解并加速 RNA 研究，通过整合 RNA 序列编码器、线性投影层和大型语言模型（LLMs）来处理用户上传的序列并提供精确响应。RNA-GPT 基于可扩展训练管道，利用 RNA-QA 系统从 RNACentral 收集数据，采用 divide-and-conquer 方法结合 GPT-4o 和 latent Dirichlet allocation (LDA) 生成指令调整样本。实验结果表明，RNA-GPT 有效应对复杂 RNA 查询，并发布了包含 407,616 个样本的 RNA-QA 数据集，进一步推动药物开发和生物技术领域的进展。",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "q-bio.BM"
      ],
      "primary_category": "q-bio.GN",
      "comment": "Machine Learning for Structural Biology Workshop, NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.08900v1",
      "published_date": "2024-10-29 06:19:56 UTC",
      "updated_date": "2024-10-29 06:19:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:37:27.858921"
    },
    {
      "arxiv_id": "2410.22375v1",
      "title": "Rethinking Code Refinement: Learning to Judge Code Efficiency",
      "title_zh": "重新思考代码优化：学习判断代码效率",
      "authors": [
        "Minju Seo",
        "Jinheon Baek",
        "Sung Ju Hwang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nunderstanding and generating codes. Due to these capabilities, many recent\nmethods are proposed to automatically refine the codes with LLMs. However, we\nshould rethink that the refined codes (from LLMs and even humans) are not\nalways more efficient than their original versions. On the other hand, running\ntwo different versions of codes and comparing them every time is not ideal and\ntime-consuming. Therefore, in this work, we propose a novel method based on the\ncode language model that is trained to judge the efficiency between two\ndifferent codes (generated across humans and machines) by either classifying\nthe superior one or predicting the relative improvement. We validate our method\non multiple programming languages with multiple refinement steps, demonstrating\nthat the proposed method can effectively distinguish between more and less\nefficient versions of code.",
      "tldr_zh": "这篇论文重新审视代码优化问题，指出 Large Language Models (LLMs) 生成的代码并非总是比原版更高效，且直接运行比较代码版本耗时费力。作者提出了一种新方法，使用基于代码语言模型的框架，训练它通过分类哪个代码更优或预测相对改进来判断代码效率。实验在多种编程语言和多个优化步骤上验证了该方法的有效性，证明它能准确区分更高效的代码版本。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22375v1",
      "published_date": "2024-10-29 06:17:37 UTC",
      "updated_date": "2024-10-29 06:17:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:37:38.423757"
    },
    {
      "arxiv_id": "2411.08899v1",
      "title": "FinVision: A Multi-Agent Framework for Stock Market Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Sorouralsadat Fatemi",
        "Yuheng Hu"
      ],
      "abstract": "Financial trading has been a challenging task, as it requires the integration\nof vast amounts of data from various modalities. Traditional deep learning and\nreinforcement learning methods require large training data and often involve\nencoding various data types into numerical formats for model input, which\nlimits the explainability of model behavior. Recently, LLM-based agents have\ndemonstrated remarkable advancements in handling multi-modal data, enabling\nthem to execute complex, multi-step decision-making tasks while providing\ninsights into their thought processes. This research introduces a multi-modal\nmulti-agent system designed specifically for financial trading tasks. Our\nframework employs a team of specialized LLM-based agents, each adept at\nprocessing and interpreting various forms of financial data, such as textual\nnews reports, candlestick charts, and trading signal charts. A key feature of\nour approach is the integration of a reflection module, which conducts analyses\nof historical trading signals and their outcomes. This reflective process is\ninstrumental in enhancing the decision-making capabilities of the system for\nfuture trading scenarios. Furthermore, the ablation studies indicate that the\nvisual reflection module plays a crucial role in enhancing the decision-making\ncapabilities of our framework.",
      "tldr_zh": "这篇论文提出了FinVision，一种多智能体框架，用于股票市场预测，通过整合多模态数据（如文本新闻、蜡烛图和交易信号图）来克服传统深度学习和强化学习方法的局限性。框架由专门的LLM-based agents组成，每个agents负责处理特定数据类型，并引入反射模块对历史交易信号进行分析，以提升多步决策的准确性和可解释性。消融研究表明，视觉反射模块在增强系统决策能力方面发挥了关键作用，为金融交易任务提供了更可靠的解决方案。",
      "categories": [
        "q-fin.TR",
        "cs.AI"
      ],
      "primary_category": "q-fin.TR",
      "comment": "Accepted at ICAIF 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.08899v1",
      "published_date": "2024-10-29 06:02:28 UTC",
      "updated_date": "2024-10-29 06:02:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:37:50.645878"
    },
    {
      "arxiv_id": "2410.21764v2",
      "title": "Online Mirror Descent for Tchebycheff Scalarization in Multi-Objective Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Meitong Liu",
        "Xiaoyuan Zhang",
        "Chulin Xie",
        "Kate Donahue",
        "Han Zhao"
      ],
      "abstract": "The goal of multi-objective optimization (MOO) is to learn under multiple,\npotentially conflicting, objectives. One widely used technique to tackle MOO is\nthrough linear scalarization, where one fixed preference vector is used to\ncombine the objectives into a single scalar value for optimization. However,\nrecent work (Hu et al., 2024) has shown linear scalarization often fails to\ncapture the non-convex regions of the Pareto Front, failing to recover the\ncomplete set of Pareto optimal solutions. In light of the above limitations,\nthis paper focuses on Tchebycheff scalarization that optimizes for the\nworst-case objective. In particular, we propose an online mirror descent\nalgorithm for Tchebycheff scalarization, which we call OMD-TCH. We show that\nOMD-TCH enjoys a convergence rate of $O(\\sqrt{\\log m/T})$ where $m$ is the\nnumber of objectives and $T$ is the number of iteration rounds. We also propose\na novel adaptive online-to-batch conversion scheme that significantly improves\nthe practical performance of OMD-TCH while maintaining the same convergence\nguarantees. We demonstrate the effectiveness of OMD-TCH and the adaptive\nconversion scheme on both synthetic problems and federated learning tasks under\nfairness constraints, showing state-of-the-art performance.",
      "tldr_zh": "这项研究针对多目标优化（Multi-Objective Optimization, MOO）中的问题，提出了一种基于 Tchebycheff 标量化的新方法，以优化最坏情况目标，从而更好地捕获 Pareto Front 的非凸区域。作者开发了 OMD-TCH 算法，即 Online Mirror Descent for Tchebycheff Scalarization，该算法的收敛率达到 O(√(log m / T))，其中 m 是目标数量，T 是迭代轮数。同时，他们引入了一种新型自适应在线到批量转换方案，提升了算法的实际性能，同时保留了相同的收敛保证。在合成问题和公平约束下的联邦学习任务中，OMD-TCH 展示了最先进的表现，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, 7 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2410.21764v2",
      "published_date": "2024-10-29 05:58:33 UTC",
      "updated_date": "2024-11-11 16:17:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:38:03.120807"
    },
    {
      "arxiv_id": "2410.21750v1",
      "title": "Learning and Unlearning of Fabricated Knowledge in Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Sun",
        "Nolan Andrew Miller",
        "Andrey Zhmoginov",
        "Max Vladymyrov",
        "Mark Sandler"
      ],
      "abstract": "What happens when a new piece of knowledge is introduced into the training\ndata and how long does it last while a large language model (LM) continues to\ntrain? We investigate this question by injecting facts into LMs from a new\nprobing dataset, \"Outlandish\", which is designed to permit the testing of a\nspectrum of different fact types. When studying how robust these memories are,\nthere appears to be a sweet spot in the spectrum of fact novelty between\nconsistency with world knowledge and total randomness, where the injected\nmemory is the most enduring. Specifically we show that facts that conflict with\ncommon knowledge are remembered for tens of thousands of training steps, while\nprompts not conflicting with common knowledge (mundane), as well as scrambled\nprompts (randomly jumbled) are both forgotten much more rapidly. Further,\nknowledge-conflicting facts can \"prime'' how the language model hallucinates on\nlogically unrelated prompts, showing their propensity for non-target\ngeneralization, while both mundane and randomly jumbled facts prime\nsignificantly less. Finally, we show that impacts of knowledge-conflicting\nfacts in LMs, though they can be long lasting, can be largely erased by novel\napplication of multi-step sparse updates, even while the training ability of\nthe model is preserved. As such, this very simple procedure has direct\nimplications for mitigating the effects of data poisoning in training.",
      "tldr_zh": "本研究探讨了在大型语言模型 (LMs) 中注入新知识的过程及其在持续训练中的持久性，使用了新数据集 \"Outlandish\" 来测试不同类型的事实。结果显示，与常见知识冲突的事实在训练中能被记住数万个步骤，而不冲突的 (mundane) 或随机打乱的 (scrambled) 事实则更快被遗忘；此外，这些冲突事实可能导致模型在无关提示上的非目标泛化 (non-target generalization)。研究进一步证明，通过多步稀疏更新 (multi-step sparse updates)，可以有效擦除这些知识的影响，同时保留模型的训练能力，从而为缓解数据中毒 (data poisoning) 提供简单实用的方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21750v1",
      "published_date": "2024-10-29 05:33:14 UTC",
      "updated_date": "2024-10-29 05:33:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:38:16.650835"
    },
    {
      "arxiv_id": "2410.21741v1",
      "title": "Enhancing Financial Question Answering with a Multi-Agent Reflection Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Sorouralsadat Fatemi",
        "Yuheng Hu"
      ],
      "abstract": "While Large Language Models (LLMs) have shown impressive capabilities in\nnumerous Natural Language Processing (NLP) tasks, they still struggle with\nfinancial question answering (QA), particularly when numerical reasoning is\nrequired. Recently, LLM-based multi-agent frameworks have demonstrated\nremarkable effectiveness in multi-step reasoning, which is crucial for\nfinancial QA tasks as it involves extracting relevant information from tables\nand text and then performing numerical reasoning on the extracted data to infer\nanswers. In this study, we propose a multi-agent framework incorporating a\ncritic agent that reflects on the reasoning steps and final answers for each\nquestion. Additionally, we enhance our system by adding multiple critic agents,\neach focusing on a specific aspect of the answer. Our results indicate that\nthis framework significantly improves performance compared to single-agent\nreasoning, with an average performance increase of 15% for the LLaMA3-8B model\nand 5% for the LLaMA3-70B model. Furthermore, our framework performs on par\nwith, and in some cases surpasses, larger single-agent LLMs such as\nLLaMA3.1-405B and GPT-4o-mini, though it falls slightly short compared to\nClaude-3.5 Sonnet. Overall, our framework presents an effective solution to\nenhance open-source LLMs for financial QA tasks, offering a cost-effective\nalternative to larger models like Claude-3.5 Sonnet.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)在金融问答(QA)任务中的不足，特别是数值推理的挑战，提出了一种多智能体框架，加入了critic agent来反思推理步骤和最终答案，并进一步扩展为多个critic agent，每个专注于答案的特定方面。该框架显著提升了性能，使LLaMA3-8B模型平均提高15%，LLaMA3-70B模型提高5%。此外，该框架的性能与更大单智能体LLMs如LLaMA3.1-405B和GPT-4o-mini相当或优于它们，但略逊于Claude-3.5 Sonnet，提供了一种增强开源LLMs的成本效益解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ICAIF 24",
      "pdf_url": "http://arxiv.org/pdf/2410.21741v1",
      "published_date": "2024-10-29 04:58:07 UTC",
      "updated_date": "2024-10-29 04:58:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:38:27.022910"
    },
    {
      "arxiv_id": "2410.21730v1",
      "title": "Efficient Reprogramming of Memristive Crossbars for DNNs: Weight Sorting and Bit Stucking",
      "title_zh": "翻译失败",
      "authors": [
        "Matheus Farias",
        "H. T. Kung"
      ],
      "abstract": "We introduce a novel approach to reduce the number of times required for\nreprogramming memristors on bit-sliced compute-in-memory crossbars for deep\nneural networks (DNNs). Our idea addresses the limited non-volatile memory\nendurance, which restrict the number of times they can be reprogrammed.\n  To reduce reprogramming demands, we employ two techniques: (1) we organize\nweights into sorted sections to schedule reprogramming of similar crossbars,\nmaximizing memristor state reuse, and (2) we reprogram only a fraction of\nrandomly selected memristors in low-order columns, leveraging their bit-level\ndistribution and recognizing their relatively small impact on model accuracy.\n  We evaluate our approach for state-of-the-art models on the ImageNet-1K\ndataset. We demonstrate a substantial reduction in crossbar reprogramming by\n3.7x for ResNet-50 and 21x for ViT-Base, while maintaining model accuracy\nwithin a 1% margin.",
      "tldr_zh": "该论文提出了一种高效重新编程 memristive crossbars 的方法，用于 deep neural networks (DNNs)，以解决非挥发性内存耐久性限制问题。方法包括两个关键技术：(1) weight sorting，将权重组织成排序部分以最大化 memristor 状态重用；(2) bit sticking，只重新编程低阶列中随机选定的部分 memristors，利用其位级分布对模型准确性的小影响。在 ImageNet-1K 数据集上，该方法使 ResNet-50 的 crossbar 重新编程减少 3.7 倍，ViT-Base 减少 21 倍，同时保持模型准确性在 1% 以内。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "5 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.21730v1",
      "published_date": "2024-10-29 04:34:02 UTC",
      "updated_date": "2024-10-29 04:34:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:38:39.663160"
    },
    {
      "arxiv_id": "2411.00826v2",
      "title": "Uncertainty Quantification via Hölder Divergence for Multi-View Representation Learning",
      "title_zh": "通过 Hölder 散度进行多视图表示学习的不确定性量化",
      "authors": [
        "Yan Zhang",
        "Ming Li",
        "Chun Li",
        "Zhaoxia Liu",
        "Ye Zhang",
        "Fei Richard Yu"
      ],
      "abstract": "Evidence-based deep learning represents a burgeoning paradigm for uncertainty\nestimation, offering reliable predictions with negligible extra computational\noverheads. Existing methods usually adopt Kullback-Leibler divergence to\nestimate the uncertainty of network predictions, ignoring domain gaps among\nvarious modalities. To tackle this issue, this paper introduces a novel\nalgorithm based on H\\\"older Divergence (HD) to enhance the reliability of\nmulti-view learning by addressing inherent uncertainty challenges from\nincomplete or noisy data. Generally, our method extracts the representations of\nmultiple modalities through parallel network branches, and then employs HD to\nestimate the prediction uncertainties. Through the Dempster-Shafer theory,\nintegration of uncertainty from different modalities, thereby generating a\ncomprehensive result that considers all available representations.\nMathematically, HD proves to better measure the ``distance'' between real data\ndistribution and predictive distribution of the model and improve the\nperformances of multi-class recognition tasks.\n  Specifically, our method surpass the existing state-of-the-art counterparts\non all evaluating benchmarks.\n  We further conduct extensive experiments on different backbones to verify our\nsuperior robustness. It is demonstrated that our method successfully pushes the\ncorresponding performance boundaries. Finally, we perform experiments on more\nchallenging scenarios, \\textit{i.e.}, learning with incomplete or noisy data,\nrevealing that our method exhibits a high tolerance to such corrupted data.",
      "tldr_zh": "这篇论文提出了一种基于 Hölder Divergence (HD) 的新算法，用于多视图表示学习的不确定性量化，以解决现有方法（如 Kullback-Leibler divergence）忽略不同模态领域差距的问题。方法通过并行网络分支提取多模态表示，并结合 Dempster-Shafer 理论整合不确定性，从而生成更可靠的预测结果。实验表明，该算法在多类识别基准上超越现有最先进模型，并在处理不完整或噪声数据时表现出色鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "NA",
      "pdf_url": "http://arxiv.org/pdf/2411.00826v2",
      "published_date": "2024-10-29 04:29:44 UTC",
      "updated_date": "2025-04-17 13:24:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:38:51.053261"
    },
    {
      "arxiv_id": "2410.21719v2",
      "title": "On the Statistical Complexity of Estimating Vendi Scores from Empirical Data",
      "title_zh": "翻译失败",
      "authors": [
        "Azim Ospanov",
        "Farzan Farnia"
      ],
      "abstract": "Evaluating the diversity of generative models without access to reference\ndata poses methodological challenges. The reference-free Vendi score offers a\nsolution by quantifying the diversity of generated data using matrix-based\nentropy measures. The Vendi score is usually computed via the\neigendecomposition of an $n \\times n$ kernel matrix for $n$ generated samples.\nHowever, the heavy computational cost of eigendecomposition for large $n$ often\nlimits the sample size used in practice to a few tens of thousands. In this\npaper, we investigate the statistical convergence of the Vendi score. We\nnumerically demonstrate that for kernel functions with an infinite feature map\ndimension, the score estimated from a limited sample size may exhibit a\nnon-negligible bias relative to the population Vendi score, i.e., the\nasymptotic limit as the sample size approaches infinity. To address this, we\nintroduce a truncation of the Vendi statistic, called the $t$-truncated Vendi\nstatistic, which is guaranteed to converge to its asymptotic limit given\n$n=O(t)$ samples. We show that the existing Nystr\\\"om method and the FKEA\napproximation method for approximating the Vendi score both converge to the\npopulation truncated Vendi score. We perform several numerical experiments to\nillustrate the concentration of the Nystr\\\"om and FKEA-computed Vendi scores\naround the truncated Vendi and discuss how the truncated Vendi score correlates\nwith the diversity of image and text data.",
      "tldr_zh": "这篇论文探讨了在没有参考数据情况下评估生成模型多样性的挑战，引入了Vendi score作为一种基于矩阵熵测量的参考-free方法，但其计算依赖于n×n核矩阵的eigendecomposition，导致样本大小受限。研究者分析了Vendi score的统计收敛性，发现有限样本可能导致显著偏差，并提出t-truncated Vendi statistic，确保在n=O(t)样本下收敛到理论极限。论文进一步证明了Nyström method和FKEA approximation method均收敛到截断Vendi score，并通过数值实验验证了这些方法的集中性及其与图像和文本数据多样性的相关性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21719v2",
      "published_date": "2024-10-29 04:19:41 UTC",
      "updated_date": "2025-02-14 01:19:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:39:02.639847"
    },
    {
      "arxiv_id": "2410.21717v1",
      "title": "Generating Realistic Tabular Data with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Dang Nguyen",
        "Sunil Gupta",
        "Kien Do",
        "Thin Nguyen",
        "Svetha Venkatesh"
      ],
      "abstract": "While most generative models show achievements in image data generation, few\nare developed for tabular data generation. Recently, due to success of large\nlanguage models (LLM) in diverse tasks, they have also been used for tabular\ndata generation. However, these methods do not capture the correct correlation\nbetween the features and the target variable, hindering their applications in\ndownstream predictive tasks. To address this problem, we propose a LLM-based\nmethod with three important improvements to correctly capture the ground-truth\nfeature-class correlation in the real data. First, we propose a novel\npermutation strategy for the input data in the fine-tuning phase. Second, we\npropose a feature-conditional sampling approach to generate synthetic samples.\nFinally, we generate the labels by constructing prompts based on the generated\nsamples to query our fine-tuned LLM. Our extensive experiments show that our\nmethod significantly outperforms 10 SOTA baselines on 20 datasets in downstream\ntasks. It also produces highly realistic synthetic samples in terms of quality\nand diversity. More importantly, classifiers trained with our synthetic data\ncan even compete with classifiers trained with the original data on half of the\nbenchmark datasets, which is a significant achievement in tabular data\ngeneration.",
      "tldr_zh": "该论文提出了一种基于 Large Language Models (LLM) 的方法，用于生成更真实的表格数据，以解决现有方法无法正确捕捉特征和目标变量之间相关性的问题。该方法包括三个关键改进：引入新型置换策略(permutation strategy)于微调阶段、采用特征条件采样(feature-conditional sampling approach)生成合成样本，以及通过基于样本构建的提示(prompts)查询微调后的LLM来生成标签。实验结果显示，该方法在20个数据集上显著优于10个SOTA baselines，在下游预测任务中表现出色，且生成的合成数据在质量和多样性上高度真实；在半数基准数据集上，使用合成数据训练的分类器性能甚至可与使用原始数据训练的分类器相媲美。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear at ICDM 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.21717v1",
      "published_date": "2024-10-29 04:14:32 UTC",
      "updated_date": "2024-10-29 04:14:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:39:15.592053"
    },
    {
      "arxiv_id": "2410.21716v1",
      "title": "A Bayesian Approach to Harnessing the Power of LLMs in Authorship Attribution",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengmian Hu",
        "Tong Zheng",
        "Heng Huang"
      ],
      "abstract": "Authorship attribution aims to identify the origin or author of a document.\nTraditional approaches have heavily relied on manual features and fail to\ncapture long-range correlations, limiting their effectiveness. Recent\nadvancements leverage text embeddings from pre-trained language models, which\nrequire significant fine-tuning on labeled data, posing challenges in data\ndependency and limited interpretability. Large Language Models (LLMs), with\ntheir deep reasoning capabilities and ability to maintain long-range textual\nassociations, offer a promising alternative. This study explores the potential\nof pre-trained LLMs in one-shot authorship attribution, specifically utilizing\nBayesian approaches and probability outputs of LLMs. Our methodology calculates\nthe probability that a text entails previous writings of an author, reflecting\na more nuanced understanding of authorship. By utilizing only pre-trained\nmodels such as Llama-3-70B, our results on the IMDb and blog datasets show an\nimpressive 85\\% accuracy in one-shot authorship classification across ten\nauthors. Our findings set new baselines for one-shot authorship analysis using\nLLMs and expand the application scope of these models in forensic linguistics.\nThis work also includes extensive ablation studies to validate our approach.",
      "tldr_zh": "这篇论文提出了一种基于贝叶斯方法的创新框架，利用大型语言模型（LLMs）进行单次（one-shot）作者归因（Authorship Attribution），通过计算文本与作者先前作品的相关概率，克服了传统方法在捕捉长程相关性和可解释性方面的不足。研究仅使用预训练模型如Llama-3-70B，无需额外微调，在IMDb和blog数据集上实现了85%的准确率，适用于十位作者的分类任务。该方法设置了新的one-shot作者分析基准，并通过广泛的消融研究验证了其有效性，扩展了LLMs在法医语言学中的应用潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21716v1",
      "published_date": "2024-10-29 04:14:23 UTC",
      "updated_date": "2024-10-29 04:14:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:39:28.965776"
    },
    {
      "arxiv_id": "2411.00028v2",
      "title": "Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction in LBSN",
      "title_zh": "翻译失败",
      "authors": [
        "Zhilun Zhou",
        "Jingyang Fan",
        "Yu Liu",
        "Fengli Xu",
        "Depeng Jin",
        "Yong Li"
      ],
      "abstract": "The fast development of location-based social networks (LBSNs) has led to\nsignificant changes in society, resulting in popular studies of using LBSN data\nfor socioeconomic prediction, e.g., regional population and commercial activity\nestimation. Existing studies design various graphs to model heterogeneous LBSN\ndata, and further apply graph representation learning methods for socioeconomic\nprediction. However, these approaches heavily rely on heuristic ideas and\nexpertise to extract task-relevant knowledge from diverse data, which may not\nbe optimal for specific tasks. Additionally, they tend to overlook the inherent\nrelationships between different indicators, limiting the prediction accuracy.\nMotivated by the remarkable abilities of large language models (LLMs) in\ncommonsense reasoning, embedding, and multi-agent collaboration, in this work,\nwe synergize LLM agents and knowledge graph for socioeconomic prediction. We\nfirst construct a location-based knowledge graph (LBKG) to integrate\nmulti-sourced LBSN data. Then we leverage the reasoning power of LLM agent to\nidentify relevant meta-paths in the LBKG for each type of socioeconomic\nprediction task, and design a semantic-guided attention module for knowledge\nfusion with meta-paths. Moreover, we introduce a cross-task communication\nmechanism to further enhance performance by enabling knowledge sharing across\ntasks at both LLM agent and KG levels. On the one hand, the LLM agents for\ndifferent tasks collaborate to generate more diverse and comprehensive\nmeta-paths. On the other hand, the embeddings from different tasks are\nadaptively merged for better socioeconomic prediction. Experiments on two\ndatasets demonstrate the effectiveness of the synergistic design between LLM\nand KG, providing insights for information sharing across socioeconomic\nprediction tasks.",
      "tldr_zh": "本论文提出了一种将大型语言模型(LLM)代理与知识图谱(KG)协同的方法，用于位置社交网络(LBSN)中的社会经济预测，如区域人口和商业活动估计，以解决现有方法依赖启发式知识和忽略指标关系的局限。研究首先构建位置知识图谱(LBKG)整合多源LBSN数据，然后利用LLM代理识别任务相关元路径(meta-paths)，并设计语义引导注意力模块(semantic-guided attention module)进行知识融合，同时引入跨任务通信机制促进任务间知识共享。实验在两个数据集上验证了这一协同设计的有效性，显著提升预测准确性，并为跨任务信息共享提供了新洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00028v2",
      "published_date": "2024-10-29 04:03:15 UTC",
      "updated_date": "2024-11-19 14:29:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:39:40.149387"
    },
    {
      "arxiv_id": "2410.21705v2",
      "title": "AdaptGCD: Multi-Expert Adapter Tuning for Generalized Category Discovery",
      "title_zh": "AdaptGCD：多专家适配器调优用于广义类别发现",
      "authors": [
        "Yuxun Qu",
        "Yongqiang Tang",
        "Chenyang Zhang",
        "Wensheng Zhang"
      ],
      "abstract": "Different from the traditional semi-supervised learning paradigm that is\nconstrained by the close-world assumption, Generalized Category Discovery (GCD)\npresumes that the unlabeled dataset contains new categories not appearing in\nthe labeled set, and aims to not only classify old categories but also discover\nnew categories in the unlabeled data. Existing studies on GCD typically devote\nto transferring the general knowledge from the self-supervised pretrained model\nto the target GCD task via some fine-tuning strategies, such as partial tuning\nand prompt learning. Nevertheless, these fine-tuning methods fail to make a\nsound balance between the generalization capacity of pretrained backbone and\nthe adaptability to the GCD task. To fill this gap, in this paper, we propose a\nnovel adapter-tuning-based method named AdaptGCD, which is the first work to\nintroduce the adapter tuning into the GCD task and provides some key insights\nexpected to enlighten future research. Furthermore, considering the discrepancy\nof supervision information between the old and new classes, a multi-expert\nadapter structure equipped with a route assignment constraint is elaborately\ndevised, such that the data from old and new classes are separated into\ndifferent expert groups. Extensive experiments are conducted on 7 widely-used\ndatasets. The remarkable improvements in performance highlight the\neffectiveness of our proposals.",
      "tldr_zh": "论文提出AdaptGCD，一种基于多专家adapter tuning的方法，用于处理Generalized Category Discovery (GCD)任务，该任务需要在无标签数据中分类旧类别并发现新类别。不同于传统细调策略，AdaptGCD通过多专家adapter结构和路由分配约束，将旧类和新类的监督信息分开，实现预训练模型的泛化能力和任务适应性的平衡。实验在7个常用数据集上验证了该方法的有效性，显著提升了性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21705v2",
      "published_date": "2024-10-29 03:41:47 UTC",
      "updated_date": "2025-03-14 15:55:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:39:51.070940"
    },
    {
      "arxiv_id": "2411.05806v1",
      "title": "SkipSNN: Efficiently Classifying Spike Trains with Event-attention",
      "title_zh": "翻译失败",
      "authors": [
        "Hang Yin",
        "Yao Su",
        "Liping Liu",
        "Thomas Hartvigsen",
        "Xin Dai",
        "Xiangnan Kong"
      ],
      "abstract": "Spike train classification has recently become an important topic in the\nmachine learning community, where each spike train is a binary event sequence\nwith \\emph{temporal-sparsity of signals of interest} and \\emph{temporal-noise}\nproperties. A promising model for it should follow the design principle of\nperforming intensive computation only when signals of interest appear. So such\ntasks use mainly Spiking Neural Networks (SNNs) due to their consideration of\ntemporal-sparsity of spike trains. However, the basic mechanism of SNNs ignore\nthe temporal-noise issue, which makes them computationally expensive and thus\nhigh power consumption for analyzing spike trains on resource-constrained\nplatforms. As an event-driven model, an SNN neuron makes a reaction given any\ninput signals, making it difficult to quickly find signals of interest. In this\npaper, we introduce an event-attention mechanism that enables SNNs to\ndynamically highlight useful signals of the original spike trains. To this end,\nwe propose SkipSNN, which extends existing SNN models by learning to mask out\nnoise by skipping membrane potential updates and shortening the effective size\nof the computational graph. This process is analogous to how people choose to\nopen and close their eyes to filter the information they see. We evaluate\nSkipSNN on various neuromorphic tasks and demonstrate that it achieves\nsignificantly better computational efficiency and classification accuracy than\nother state-of-the-art SNNs.",
      "tldr_zh": "本论文针对 spike train classification 中的 temporal-sparsity 和 temporal-noise 问题，提出了一种高效的 SkipSNN 模型，利用 event-attention 机制动态突出有用的信号，并通过跳过膜电位更新和缩短计算图大小来屏蔽噪声。SkipSNN 扩展了现有的 Spiking Neural Networks (SNNs)，使其仅在信号出现时进行密集计算，从而提升计算效率。实验结果显示，在各种 neuromorphic tasks 上，SkipSNN 比其他 SNNs 实现了显著更高的分类准确率和计算效率。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "Published as a research paper at IEEE BigData 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.05806v1",
      "published_date": "2024-10-29 03:19:25 UTC",
      "updated_date": "2024-10-29 03:19:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:40:03.198759"
    },
    {
      "arxiv_id": "2410.21676v4",
      "title": "How Does Critical Batch Size Scale in Pre-training?",
      "title_zh": "关键批量大小在预训练中如何缩放？",
      "authors": [
        "Hanlin Zhang",
        "Depen Morwani",
        "Nikhil Vyas",
        "Jingfeng Wu",
        "Difan Zou",
        "Udaya Ghai",
        "Dean Foster",
        "Sham Kakade"
      ],
      "abstract": "Training large-scale models under given resources requires careful design of\nparallelism strategies. In particular, the efficiency notion of critical batch\nsize (CBS), concerning the compromise between time and compute, marks the\nthreshold beyond which greater data parallelism leads to diminishing returns.\nTo operationalize it, we propose a measure of CBS and pre-train a series of\nauto-regressive language models, ranging from 85 million to 1.2 billion\nparameters, on the C4 dataset. Through extensive hyper-parameter sweeps and\ncareful control of factors such as batch size, momentum, and learning rate\nalong with its scheduling, we systematically investigate the impact of scale on\nCBS. Then we fit scaling laws with respect to model and data sizes to decouple\ntheir effects. Overall, our results demonstrate that CBS scales primarily with\ndata size rather than model size, a finding we justify theoretically through\nthe analysis of infinite-width limits of neural networks and\ninfinite-dimensional least squares regression. Of independent interest, we\nhighlight the importance of common hyper-parameter choices and strategies for\nstudying large-scale pre-training beyond fixed training durations.",
      "tldr_zh": "本研究探讨了在预训练大型模型时，critical batch size (CBS) 的缩放行为，该概念代表了数据并行性在时间和计算权衡上的阈值，超过此阈值收益递减。研究者通过在 C4 数据集上预训练一系列自回归语言模型（参数从 85 百万到 1.2 亿不等），并进行广泛的超参数扫描（如批大小、动量和学习率调度），系统分析了模型规模和数据规模对 CBS 的影响。结果显示，CBS 主要随数据大小而非模型大小扩展，这一发现通过无限宽度神经网络和无限维最小二乘回归的理论分析得到证实，并突出了优化超参数策略在大型预训练中的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025, Blog post:\n  https://kempnerinstitute.harvard.edu/research/deeper-learning/how-does-critical-batch-size-scale-in-pre-training-decoupling-data-and-model-size",
      "pdf_url": "http://arxiv.org/pdf/2410.21676v4",
      "published_date": "2024-10-29 02:54:06 UTC",
      "updated_date": "2025-04-21 04:19:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:40:15.258452"
    },
    {
      "arxiv_id": "2410.21675v1",
      "title": "BF-Meta: Secure Blockchain-enhanced Privacy-preserving Federated Learning for Metaverse",
      "title_zh": "翻译失败",
      "authors": [
        "Wenbo Liu",
        "Handi Chen",
        "Edith C. H. Ngai"
      ],
      "abstract": "The metaverse, emerging as a revolutionary platform for social and economic\nactivities, provides various virtual services while posing security and privacy\nchallenges. Wearable devices serve as bridges between the real world and the\nmetaverse. To provide intelligent services without revealing users' privacy in\nthe metaverse, leveraging federated learning (FL) to train models on local\nwearable devices is a promising solution. However, centralized model\naggregation in traditional FL may suffer from external attacks, resulting in a\nsingle point of failure. Furthermore, the absence of incentive mechanisms may\nweaken users' participation during FL training, leading to degraded performance\nof the trained model and reduced quality of intelligent services. In this\npaper, we propose BF-Meta, a secure blockchain-empowered FL framework with\ndecentralized model aggregation, to mitigate the negative influence of\nmalicious users and provide secure virtual services in the metaverse. In\naddition, we design an incentive mechanism to give feedback to users based on\ntheir behaviors. Experiments conducted on five datasets demonstrate the\neffectiveness and applicability of BF-Meta.",
      "tldr_zh": "该论文针对 Metaverse 的安全和隐私挑战，提出 BF-Meta 框架，该框架结合区块链技术增强 Federated Learning (FL)，实现去中心化模型聚合以抵御外部攻击和恶意用户影响，同时设计激励机制基于用户行为提供反馈，提升参与度。BF-Meta 旨在在穿戴设备上训练模型，提供智能虚拟服务而不泄露隐私。实验在五个数据集上验证了该框架的有效性和适用性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21675v1",
      "published_date": "2024-10-29 02:52:49 UTC",
      "updated_date": "2024-10-29 02:52:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:40:27.362909"
    },
    {
      "arxiv_id": "2410.22374v1",
      "title": "Machine Unlearning using Forgetting Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Amartya Hatua",
        "Trung T. Nguyen",
        "Filip Cano",
        "Andrew H. Sung"
      ],
      "abstract": "Modern computer systems store vast amounts of personal data, enabling\nadvances in AI and ML but risking user privacy and trust. For privacy reasons,\nit is desired sometimes for an ML model to forget part of the data it was\ntrained on. This paper presents a new approach to machine unlearning using\nforgetting neural networks (FNN). FNNs are neural networks with specific\nforgetting layers, that take inspiration from the processes involved when a\nhuman brain forgets. While FNNs had been proposed as a theoretical construct,\nthey have not been previously used as a machine unlearning method. We describe\nfour different types of forgetting layers and study their properties. In our\nexperimental evaluation, we report our results on the MNIST handwritten digit\nrecognition and fashion datasets. The effectiveness of the unlearned models was\ntested using Membership Inference Attacks (MIA). Successful experimental\nresults demonstrate the great potential of our proposed method for dealing with\nthe machine unlearning problem.",
      "tldr_zh": "这篇论文提出了一种新的机器 unlearning 方法，使用 Forgetting Neural Networks (FNN)，旨在帮助机器学习模型忘记部分训练数据以保护用户隐私。FNN 是一种带有特定 forgetting layers 的神经网络，设计灵感来源于人类大脑的遗忘过程，并描述了四种不同类型的 forgetting layers 及其属性。在实验中，作者在 MNIST 手写数字识别和时尚数据集上进行了测试，并通过 Membership Inference Attacks (MIA) 评估了 unlearned 模型的有效性，结果显示该方法在处理机器 unlearning 问题上具有巨大潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22374v1",
      "published_date": "2024-10-29 02:52:26 UTC",
      "updated_date": "2024-10-29 02:52:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:42:33.105852"
    },
    {
      "arxiv_id": "2410.21673v1",
      "title": "Knowledge-Guided Prompt Learning for Request Quality Assurance in Public Code Review",
      "title_zh": "基于知识引导的提示学习在公共代码审查中的请求质量保证",
      "authors": [
        "Lin Li",
        "Xinchun Yu",
        "Xinyu Chen",
        "Peng Liang"
      ],
      "abstract": "Public Code Review (PCR) is an assistant to the internal code review of the\ndevelopment team, in the form of a public Software Question Answering (SQA)\ncommunity, to help developers access high-quality and efficient review\nservices. Current methods on PCR mainly focus on the reviewer's perspective,\nincluding finding a capable reviewer, predicting comment quality, and\nrecommending/generating review comments. However, it is not well studied that\nhow to satisfy the review necessity requests posted by developers which can\nincrease their visibility, which in turn acts as a prerequisite for better\nreview responses. To this end, we propose a Knowledge-guided Prompt learning\nfor Public Code Review (KP-PCR) to achieve developer-based code review request\nquality assurance (i.e., predicting request necessity and recommending tags\nsubtask). Specifically, we reformulate the two subtasks via 1) text prompt\ntuning which converts both of them into a Masked Language Model (MLM) by\nconstructing prompt templates using hard prompt; 2) knowledge and code prefix\ntuning which introduces external knowledge by soft prompt, and uses data flow\ndiagrams to characterize code snippets. Finally, both of the request necessity\nprediction and tag recommendation subtasks output predicted results through an\nanswer engineering module. In addition, we further analysis the time complexity\nof our KP-PCR that has lightweight prefix based the operation of introducing\nknowledge. Experimental results on the PCR dataset for the period 2011-2023\ndemonstrate that our KP-PCR outperforms baselines by 8.3%-28.8% in the request\nnecessity prediction and by 0.1%-29.5% in the tag recommendation. The code\nimplementation is released at https://github.com/WUT-IDEA/KP-PCR.",
      "tldr_zh": "本研究针对 Public Code Review (PCR) 中开发者请求质量保障的问题，提出 Knowledge-guided Prompt Learning (KP-PCR) 方法，以提升请求的可见性和审查响应质量。具体而言，KP-PCR 通过文本提示调整将请求必要性预测和标签推荐子任务转化为 Masked Language Model (MLM)，并结合知识和代码前缀调整引入外部知识（如数据流图）来优化代码片段表征。实验结果显示，在 2011-2023 年的 PCR 数据集上，KP-PCR 分别在请求必要性预测和标签推荐上比基线模型提升 8.3%-28.8% 和 0.1%-29.5%，证明了其轻量级高效的优势。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "28 pages, 7 images, 12 tables, Manuscript submitted to a journal\n  (2024)",
      "pdf_url": "http://arxiv.org/pdf/2410.21673v1",
      "published_date": "2024-10-29 02:48:41 UTC",
      "updated_date": "2024-10-29 02:48:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:40:52.033622"
    },
    {
      "arxiv_id": "2411.08897v1",
      "title": "Comment on Is Complexity an Illusion?",
      "title_zh": "对《Is Complexity an Illusion?》的评论",
      "authors": [
        "Gabriel Simmons"
      ],
      "abstract": "The paper \"Is Complexity an Illusion?\" (Bennett, 2024) provides a formalism\nfor complexity, learning, inference, and generalization, and introduces a\nformal definition for a \"policy\". This reply shows that correct policies do not\nexist for a simple task of supervised multi-class classification, via\nmathematical proof and exhaustive search. Implications of this result are\ndiscussed, as well as possible responses and amendments to the theory.",
      "tldr_zh": "这篇论文是对 Bennett (2024) 的《Is Complexity an Illusion?》的评论，针对其对复杂性、学习、推理、泛化和“policy”定义的形式化提出质疑。作者通过数学证明和穷举搜索证明了在简单监督多类分类任务中，正确的“policy”并不存在，从而挑战了原理论的适用性。论文进一步讨论了这一结果的潜在含义，并探索可能的理论回应和修正方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Comment on arXiv:2404.07227",
      "pdf_url": "http://arxiv.org/pdf/2411.08897v1",
      "published_date": "2024-10-29 02:40:05 UTC",
      "updated_date": "2024-10-29 02:40:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:41:03.239217"
    },
    {
      "arxiv_id": "2410.21670v1",
      "title": "Sequential choice in ordered bundles",
      "title_zh": "翻译失败",
      "authors": [
        "Rajeev Kohli",
        "Kriste Krstovski",
        "Hengyu Kuang",
        "Hengxu Lin"
      ],
      "abstract": "Experience goods such as sporting and artistic events, songs, videos, news\nstories, podcasts, and television series, are often packaged and consumed in\nbundles. Many such bundles are ordered in the sense that the individual items\nare consumed sequentially, one at a time. We examine if an individual's\ndecision to consume the next item in an ordered bundle can be predicted based\non his/her consumption pattern for the preceding items. We evaluate several\npredictive models, including two custom Transformers using decoder-only and\nencoder-decoder architectures, fine-tuned GPT-3, a custom LSTM model, a\nreinforcement learning model, two Markov models, and a zero-order model. Using\ndata from Spotify, we find that the custom Transformer with a decoder-only\narchitecture provides the most accurate predictions, both for individual\nchoices and aggregate demand. This model captures a general form of state\ndependence. Analysis of Transformer attention weights suggests that the\nconsumption of the next item in a bundle is based on approximately equal\nweighting of all preceding choices. Our results indicate that the Transformer\ncan assist in queuing the next item that an individual is likely to consume\nfrom an ordered bundle, predicting the demand for individual items, and\npersonalizing promotions to increase demand.",
      "tldr_zh": "这篇论文研究了经验商品（如歌曲和视频）在顺序捆绑中的消费决策，探讨是否能基于先前消费模式预测个体对下一个项目的选择。作者评估了多种预测模型，包括自定义的 Transformer（decoder-only 和 encoder-decoder 架构）、fine-tuned GPT-3、自定义 LSTM、强化学习模型、Markov 模型和零阶模型。使用 Spotify 数据，结果显示 decoder-only Transformer 提供最准确的预测，既适用于单个选择也适用于总体需求，并捕捉了通用的状态依赖性。论文进一步分析了 Transformer 的注意力权重，发现消费决策对所有前置选择赋予近似等权重，并提出该模型可用于队列下一项目、预测需求和个性化促销。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21670v1",
      "published_date": "2024-10-29 02:35:21 UTC",
      "updated_date": "2024-10-29 02:35:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:42:45.361513"
    },
    {
      "arxiv_id": "2411.00823v1",
      "title": "Mobility-LLM: Learning Visiting Intentions and Travel Preferences from Human Mobility Data with Large Language Models",
      "title_zh": "Mobility-LLM：利用大型语言模型从人类移动数据中学习访问意图和旅行偏好",
      "authors": [
        "Letian Gong",
        "Yan Lin",
        "Xinyue Zhang",
        "Yiwen Lu",
        "Xuedi Han",
        "Yichen Liu",
        "Shengnan Guo",
        "Youfang Lin",
        "Huaiyu Wan"
      ],
      "abstract": "Location-based services (LBS) have accumulated extensive human mobility data\non diverse behaviors through check-in sequences. These sequences offer valuable\ninsights into users' intentions and preferences. Yet, existing models analyzing\ncheck-in sequences fail to consider the semantics contained in these sequences,\nwhich closely reflect human visiting intentions and travel preferences, leading\nto an incomplete comprehension. Drawing inspiration from the exceptional\nsemantic understanding and contextual information processing capabilities of\nlarge language models (LLMs) across various domains, we present Mobility-LLM, a\nnovel framework that leverages LLMs to analyze check-in sequences for multiple\ntasks. Since LLMs cannot directly interpret check-ins, we reprogram these\nsequences to help LLMs comprehensively understand the semantics of human\nvisiting intentions and travel preferences. Specifically, we introduce a\nvisiting intention memory network (VIMN) to capture the visiting intentions at\neach record, along with a shared pool of human travel preference prompts (HTPP)\nto guide the LLM in understanding users' travel preferences. These components\nenhance the model's ability to extract and leverage semantic information from\nhuman mobility data effectively. Extensive experiments on four benchmark\ndatasets and three downstream tasks demonstrate that our approach significantly\noutperforms existing models, underscoring the effectiveness of Mobility-LLM in\nadvancing our understanding of human mobility data within LBS contexts.",
      "tldr_zh": "本研究提出Mobility-LLM框架，利用Large Language Models (LLMs)分析人类签到序列，以更好地捕捉其中的语义信息，从而理解用户的访问意图和旅行偏好。框架通过引入Visiting Intention Memory Network (VIMN)来捕捉每个签到记录的访问意图，以及Shared Pool of Human Travel Preference Prompts (HTPP)来指导LLMs理解用户的旅行偏好，从而有效提取和利用Location-based services (LBS)中的移动数据语义。实验在四个基准数据集和三个下游任务上显示，Mobility-LLM显著优于现有模型，提升了对人类移动数据的理解和应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS2024",
      "pdf_url": "http://arxiv.org/pdf/2411.00823v1",
      "published_date": "2024-10-29 01:58:06 UTC",
      "updated_date": "2024-10-29 01:58:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:42:56.727914"
    },
    {
      "arxiv_id": "2410.21657v2",
      "title": "PACER: Physics Informed Uncertainty Aware Climate Emulator",
      "title_zh": "PACER：物理信息驱动的不确定性感知",
      "authors": [
        "Hira Saleem",
        "Flora Salim",
        "Cormac Purcell"
      ],
      "abstract": "Climate models serve as critical tools for evaluating the effects of climate\nchange and projecting future climate scenarios. However, the reliance on\nnumerical simulations of physical equations renders them computationally\nintensive and inefficient. While deep learning methodologies have made\nsignificant progress in weather forecasting, they are still unstable for\nclimate emulation tasks. Here, we propose PACER, a lightweight 684K parameter\nPhysics Informed Uncertainty Aware Climate Emulator. PACER emulates temperature\nand precipitation stably for 86 years while only being trained on greenhouse\ngas emissions data. We incorporate a fundamental physical law of\nadvection-diffusion in PACER accounting for boundary conditions and empirically\nestimating the diffusion co-efficient and flow velocities from emissions data.\nPACER has been trained on 15 climate models provided by ClimateSet\noutperforming baselines across most of the climate models and advancing a new\nstate of the art in a climate diagnostic task.",
      "tldr_zh": "本研究提出 PACER，一种轻量级（仅 684K 参数）的 Physics Informed Uncertainty Aware Climate Emulator，用于解决气候模型计算密集和深度学习不稳定的问题。PACER 通过整合 advection-diffusion 的基本物理定律，考虑边界条件，并从温室气体排放数据中经验估计扩散系数和流速，从而稳定模拟 86 年的温度和降水，仅需基于排放数据训练。在 ClimateSet 的 15 个气候模型上，PACER 超越基线模型，在气候诊断任务中树立新标准。",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21657v2",
      "published_date": "2024-10-29 01:53:40 UTC",
      "updated_date": "2024-10-30 05:33:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:43:08.007827"
    },
    {
      "arxiv_id": "2411.00822v1",
      "title": "EEG-based Multimodal Representation Learning for Emotion Recognition",
      "title_zh": "基于 EEG 的多模态表示学习用于情感识别",
      "authors": [
        "Kang Yin",
        "Hye-Bin Shin",
        "Dan Li",
        "Seong-Whan Lee"
      ],
      "abstract": "Multimodal learning has been a popular area of research, yet integrating\nelectroencephalogram (EEG) data poses unique challenges due to its inherent\nvariability and limited availability. In this paper, we introduce a novel\nmultimodal framework that accommodates not only conventional modalities such as\nvideo, images, and audio, but also incorporates EEG data. Our framework is\ndesigned to flexibly handle varying input sizes, while dynamically adjusting\nattention to account for feature importance across modalities. We evaluate our\napproach on a recently introduced emotion recognition dataset that combines\ndata from three modalities, making it an ideal testbed for multimodal learning.\nThe experimental results provide a benchmark for the dataset and demonstrate\nthe effectiveness of the proposed framework. This work highlights the potential\nof integrating EEG into multimodal systems, paving the way for more robust and\ncomprehensive applications in emotion recognition and beyond.",
      "tldr_zh": "这篇论文提出了一种基于 EEG 的多模态表示学习框架，用于情感识别，旨在解决 EEG 数据固有的变异性和可用性问题，同时整合视频、图像和音频等传统模态。框架设计灵活，能处理不同输入大小，并通过动态注意力机制关注各模态的特征重要性，以提升整体性能。在一个结合三种模态的情感识别数据集上进行评估，结果为数据集提供了基准，并证明了框架的有效性。该工作突出了 EEG 在多模态系统中的潜力，为情感识别及其相关应用带来了更稳健的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.00822v1",
      "published_date": "2024-10-29 01:35:17 UTC",
      "updated_date": "2024-10-29 01:35:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:43:20.914732"
    },
    {
      "arxiv_id": "2410.22373v1",
      "title": "Analytic Continual Test-Time Adaptation for Multi-Modality Corruption",
      "title_zh": "翻译失败",
      "authors": [
        "Yufei Zhang",
        "Yicheng Xu",
        "Hongxin Wei",
        "Zhiping Lin",
        "Huiping Zhuang"
      ],
      "abstract": "Test-Time Adaptation (TTA) aims to help pre-trained model bridge the gap\nbetween source and target datasets using only the pre-trained model and\nunlabelled test data. A key objective of TTA is to address domain shifts in\ntest data caused by corruption, such as weather changes, noise, or sensor\nmalfunctions. Multi-Modal Continual Test-Time Adaptation (MM-CTTA), an\nextension of TTA with better real-world applications, further allows\npre-trained models to handle multi-modal inputs and adapt to\ncontinuously-changing target domains. MM-CTTA typically faces challenges\nincluding error accumulation, catastrophic forgetting, and reliability bias,\nwith few existing approaches effectively addressing these issues in multi-modal\ncorruption scenarios. In this paper, we propose a novel approach,\nMulti-modality Dynamic Analytic Adapter (MDAA), for MM-CTTA tasks. We\ninnovatively introduce analytic learning into TTA, using the Analytic\nClassifiers (ACs) to prevent model forgetting. Additionally, we develop Dynamic\nSelection Mechanism (DSM) and Soft Pseudo-label Strategy (SPS), which enable\nMDAA to dynamically filter reliable samples and integrate information from\ndifferent modalities. Extensive experiments demonstrate that MDAA achieves\nstate-of-the-art performance on MM-CTTA tasks while ensuring reliable model\nadaptation.",
      "tldr_zh": "这篇论文针对多模态腐败场景下的 Test-Time Adaptation (TTA)，提出了一种新型方法 Multi-modality Dynamic Analytic Adapter (MDAA)，以解决错误积累、灾难性遗忘和可靠性偏差等挑战。MDAA 创新性地引入 Analytic Classifiers (ACs) 来防止模型遗忘，并结合 Dynamic Selection Mechanism (DSM) 和 Soft Pseudo-label Strategy (SPS)，实现动态过滤可靠样本并整合多模态信息。实验结果显示，MDAA 在 MM-CTTA 任务上取得了 state-of-the-art 性能，确保了模型在连续变化域中的可靠适应。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22373v1",
      "published_date": "2024-10-29 01:21:24 UTC",
      "updated_date": "2024-10-29 01:21:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:43:33.478448"
    },
    {
      "arxiv_id": "2410.21641v1",
      "title": "RDSinger: Reference-based Diffusion Network for Singing Voice Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Kehan Sui",
        "Jinxu Xiang",
        "Fang Jin"
      ],
      "abstract": "Singing voice synthesis (SVS) aims to produce high-fidelity singing audio\nfrom music scores, requiring a detailed understanding of notes, pitch, and\nduration, unlike text-to-speech tasks. Although diffusion models have shown\nexceptional performance in various generative tasks like image and video\ncreation, their application in SVS is hindered by time complexity and the\nchallenge of capturing acoustic features, particularly during pitch\ntransitions. Some networks learn from the prior distribution and use the\ncompressed latent state as a better start in the diffusion model, but the\ndenoising step doesn't consistently improve quality over the entire duration.\nWe introduce RDSinger, a reference-based denoising diffusion network that\ngenerates high-quality audio for SVS tasks. Our approach is inspired by Animate\nAnyone, a diffusion image network that maintains intricate appearance features\nfrom reference images. RDSinger utilizes FastSpeech2 mel-spectrogram as a\nreference to mitigate denoising step artifacts. Additionally, existing models\ncould be influenced by misleading information on the compressed latent state\nduring pitch transitions. We address this issue by applying Gaussian blur on\npartial reference mel-spectrogram and adjusting loss weights in these regions.\nExtensive ablation studies demonstrate the efficiency of our method.\nEvaluations on OpenCpop, a Chinese singing dataset, show that RDSinger\noutperforms current state-of-the-art SVS methods in performance.",
      "tldr_zh": "论文提出RDSinger，一种基于参考的扩散网络，用于Singing Voice Synthesis (SVS)，旨在从音乐分数生成高保真唱歌音频，同时解决扩散模型在时间复杂性和声学特征捕捉（如音高过渡）上的挑战。该方法借鉴Animate Anyone的灵感，使用FastSpeech2 mel-spectrogram作为参考，结合Gaussian blur处理部分参考区域并调整损失权重，以减轻去噪步骤的伪影。实验结果显示，在OpenCpop数据集上，RDSinger在性能上优于当前最先进SVS方法，并通过广泛的消融研究验证了其效率。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21641v1",
      "published_date": "2024-10-29 01:01:18 UTC",
      "updated_date": "2024-10-29 01:01:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:43:45.165995"
    },
    {
      "arxiv_id": "2410.21640v1",
      "title": "A Tutorial on Clinical Speech AI Development: From Data Collection to Model Validation",
      "title_zh": "临床语音 AI 开发的教程：从数据收集到模型验证",
      "authors": [
        "Si-Ioi Ng",
        "Lingfeng Xu",
        "Ingo Siegert",
        "Nicholas Cummins",
        "Nina R. Benway",
        "Julie Liss",
        "Visar Berisha"
      ],
      "abstract": "There has been a surge of interest in leveraging speech as a marker of health\nfor a wide spectrum of conditions. The underlying premise is that any\nneurological, mental, or physical deficits that impact speech production can be\nobjectively assessed via automated analysis of speech. Recent advances in\nspeech-based Artificial Intelligence (AI) models for diagnosing and tracking\nmental health, cognitive, and motor disorders often use supervised learning,\nsimilar to mainstream speech technologies like recognition and verification.\nHowever, clinical speech AI has distinct challenges, including the need for\nspecific elicitation tasks, small available datasets, diverse speech\nrepresentations, and uncertain diagnostic labels. As a result, application of\nthe standard supervised learning paradigm may lead to models that perform well\nin controlled settings but fail to generalize in real-world clinical\ndeployments. With translation into real-world clinical scenarios in mind, this\ntutorial paper provides an overview of the key components required for robust\ndevelopment of clinical speech AI. Specifically, this paper will cover the\ndesign of speech elicitation tasks and protocols most appropriate for different\nclinical conditions, collection of data and verification of hardware,\ndevelopment and validation of speech representations designed to measure\nclinical constructs of interest, development of reliable and robust clinical\nprediction models, and ethical and participant considerations for clinical\nspeech AI. The goal is to provide comprehensive guidance on building models\nwhose inputs and outputs link to the more interpretable and clinically\nmeaningful aspects of speech, that can be interrogated and clinically validated\non clinical datasets, and that adhere to ethical, privacy, and security\nconsiderations by design.",
      "tldr_zh": "这篇教程论文概述了临床语音AI（Clinical Speech AI）开发的完整流程，从数据收集到模型验证，旨在利用语音作为健康标记评估神经、精神和身体缺陷。论文详细讨论了关键组件，包括设计针对不同临床状况的语音诱导任务（speech elicitation tasks）、数据收集与硬件验证、开发适合临床构建的语音表示（speech representations）、构建可靠的监督学习模型，以及伦理、隐私和安全考虑。作者强调临床语音AI面临的挑战，如小数据集、多样性表示和诊断标签不确定性，并提供指导以创建可解释、临床相关的模型，确保其在真实世界部署中实现泛化。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "76 pages, 24 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.21640v1",
      "published_date": "2024-10-29 00:58:15 UTC",
      "updated_date": "2024-10-29 00:58:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:43:56.917312"
    },
    {
      "arxiv_id": "2410.22372v1",
      "title": "A Hierarchical Language Model For Interpretable Graph Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Sambhav Khurana",
        "Xiner Li",
        "Shurui Gui",
        "Shuiwang Ji"
      ],
      "abstract": "Large language models (LLMs) are being increasingly explored for graph tasks.\nDespite their remarkable success in text-based tasks, LLMs' capabilities in\nunderstanding explicit graph structures remain limited, particularly with large\ngraphs. In this work, we introduce Hierarchical Language Model for Graph\n(HLM-G), which employs a two-block architecture to capture node-centric local\ninformation and interaction-centric global structure, effectively enhancing\ngraph structure understanding abilities. The proposed scheme allows LLMs to\naddress various graph queries with high efficacy, efficiency, and robustness,\nwhile reducing computational costs on large-scale graph tasks. Furthermore, we\ndemonstrate the interpretability of our model using intrinsic attention weights\nand established explainers. Comprehensive evaluations across diverse graph\nreasoning and real-world tasks of node, link, and graph-levels highlight the\nsuperiority of our method, marking a significant advancement in the application\nof LLMs to graph understanding.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）在处理图结构任务时的局限性，提出了一种层次化语言模型（Hierarchical Language Model for Graph, HLM-G）。HLM-G 采用两块架构，分别捕获节点中心的局部信息和交互中心的全局结构，从而提升 LLMs 对显式图结构的理解能力，同时提高处理图查询的效率、鲁棒性和计算成本效益。实验结果显示，该模型在节点、链接和图级别上的各种图推理和真实世界任务中表现出色，并通过内在注意力权重和解释器实现了模型的可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.22372v1",
      "published_date": "2024-10-29 00:28:02 UTC",
      "updated_date": "2024-10-29 00:28:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:44:08.162871"
    },
    {
      "arxiv_id": "2410.21627v1",
      "title": "MCPDial: A Minecraft Persona-driven Dialogue Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Seyed Hossein Alavi",
        "Sudha Rao",
        "Ashutosh Adhikari",
        "Gabriel A DesGarennes",
        "Akanksha Malhotra",
        "Chris Brockett",
        "Mahmoud Adada",
        "Raymond T. Ng",
        "Vered Shwartz",
        "Bill Dolan"
      ],
      "abstract": "We propose a novel approach that uses large language models (LLMs) to\ngenerate persona-driven conversations between Players and Non-Player Characters\n(NPC) in games. Showcasing the application of our methodology, we introduce the\nMinecraft Persona-driven Dialogue dataset (MCPDial). Starting with a small seed\nof expert-written conversations, we employ our method to generate hundreds of\nadditional conversations. Each conversation in the dataset includes rich\ncharacter descriptions of the player and NPC. The conversations are long,\nallowing for in-depth and extensive interactions between the player and NPC.\nMCPDial extends beyond basic conversations by incorporating canonical function\ncalls (e.g. \"Call find a resource on iron ore\") between the utterances.\nFinally, we conduct a qualitative analysis of the dataset to assess its quality\nand characteristics.",
      "tldr_zh": "本研究提出了一种利用大型语言模型 (LLMs) 生成玩家和非玩家角色 (NPC) 之间人格驱动对话的方法，并引入了 Minecraft Persona-driven Dialogue dataset (MCPDial)。从少量专家编写的对话种子开始，该方法扩展生成数百个额外对话，每个对话包含丰富的角色描述、长篇互动以及规范的功能调用（如“Call find a resource on iron ore”）。通过定性分析，评估了数据集的质量和特性，为游戏对话生成领域提供了高质量资源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.21627v1",
      "published_date": "2024-10-29 00:19:55 UTC",
      "updated_date": "2024-10-29 00:19:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T18:44:20.268751"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 139,
  "processed_papers_count": 139,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T18:44:41.406656"
}
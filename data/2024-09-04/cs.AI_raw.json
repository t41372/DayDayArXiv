[
  {
    "arxiv_id": "2409.03077v2",
    "title": "Backdoor defense, learnability and obfuscation",
    "authors": [
      "Paul Christiano",
      "Jacob Hilton",
      "Victor Lecomte",
      "Mark Xu"
    ],
    "abstract": "We introduce a formal notion of defendability against backdoors using a game\nbetween an attacker and a defender. In this game, the attacker modifies a\nfunction to behave differently on a particular input known as the \"trigger\",\nwhile behaving the same almost everywhere else. The defender then attempts to\ndetect the trigger at evaluation time. If the defender succeeds with high\nenough probability, then the function class is said to be defendable. The key\nconstraint on the attacker that makes defense possible is that the attacker's\nstrategy must work for a randomly-chosen trigger.\n  Our definition is simple and does not explicitly mention learning, yet we\ndemonstrate that it is closely connected to learnability. In the\ncomputationally unbounded setting, we use a voting algorithm of Hanneke et al.\n(2022) to show that defendability is essentially determined by the VC dimension\nof the function class, in much the same way as PAC learnability. In the\ncomputationally bounded setting, we use a similar argument to show that\nefficient PAC learnability implies efficient defendability, but not conversely.\nOn the other hand, we use indistinguishability obfuscation to show that the\nclass of polynomial size circuits is not efficiently defendable. Finally, we\npresent polynomial size decision trees as a natural example for which defense\nis strictly easier than learning. Thus, we identify efficient defendability as\na notable intermediate concept in between efficient learnability and\nobfuscation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.03077v2",
    "published_date": "2024-09-04 21:05:42 UTC",
    "updated_date": "2024-11-18 17:48:59 UTC"
  },
  {
    "arxiv_id": "2409.03062v1",
    "title": "MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation",
    "authors": [
      "Shehan Perera",
      "Yunus Erzurumlu",
      "Deepak Gulati",
      "Alper Yilmaz"
    ],
    "abstract": "Skin cancer segmentation poses a significant challenge in medical image\nanalysis. Numerous existing solutions, predominantly CNN-based, face issues\nrelated to a lack of global contextual understanding. Alternatively, some\napproaches resort to large-scale Transformer models to bridge the global\ncontextual gaps, but at the expense of model size and computational complexity.\nFinally many Transformer based approaches rely primarily on CNN based decoders\noverlooking the benefits of Transformer based decoding models. Recognizing\nthese limitations, we address the need efficient lightweight solutions by\nintroducing MobileUNETR, which aims to overcome the performance constraints\nassociated with both CNNs and Transformers while minimizing model size,\npresenting a promising stride towards efficient image segmentation. MobileUNETR\nhas 3 main features. 1) MobileUNETR comprises of a lightweight hybrid\nCNN-Transformer encoder to help balance local and global contextual feature\nextraction in an efficient manner; 2) A novel hybrid decoder that\nsimultaneously utilizes low-level and global features at different resolutions\nwithin the decoding stage for accurate mask generation; 3) surpassing large and\ncomplex architectures, MobileUNETR achieves superior performance with 3 million\nparameters and a computational complexity of 1.3 GFLOP resulting in 10x and 23x\nreduction in parameters and FLOPS, respectively. Extensive experiments have\nbeen conducted to validate the effectiveness of our proposed method on four\npublicly available skin lesion segmentation datasets, including ISIC 2016, ISIC\n2017, ISIC 2018, and PH2 datasets. The code will be publicly available at:\nhttps://github.com/OSUPCVLab/MobileUNETR.git",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV 2024 - BioImage Computing Workshop (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2409.03062v1",
    "published_date": "2024-09-04 20:23:37 UTC",
    "updated_date": "2024-09-04 20:23:37 UTC"
  },
  {
    "arxiv_id": "2409.03060v1",
    "title": "Better Verified Explanations with Applications to Incorrectness and Out-of-Distribution Detection",
    "authors": [
      "Min Wu",
      "Xiaofu Li",
      "Haoze Wu",
      "Clark Barrett"
    ],
    "abstract": "Building on VeriX (Verified eXplainability, arXiv:2212.01051), a system for\nproducing optimal verified explanations for machine learning model outputs, we\npresent VeriX+, which significantly improves both the size and the generation\ntime of verified explanations. We introduce a bound propagation-based\nsensitivity technique to improve the size, and a binary search-based traversal\nwith confidence ranking for improving time -- the two techniques are orthogonal\nand can be used independently or together. We also show how to adapt the\nQuickXplain (Junker 2004) algorithm to our setting to provide a trade-off\nbetween size and time. Experimental evaluations on standard benchmarks\ndemonstrate significant improvements on both metrics, e.g., a size reduction of\n38% on the GTSRB dataset and a time reduction of 90% on MNIST. We also explore\napplications of our verified explanations and show that explanation size is a\nuseful proxy for both incorrectness detection and out-of-distribution\ndetection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.03060v1",
    "published_date": "2024-09-04 20:20:37 UTC",
    "updated_date": "2024-09-04 20:20:37 UTC"
  },
  {
    "arxiv_id": "2409.12990v3",
    "title": "Brain-Inspired AI with Hyperbolic Geometry",
    "authors": [
      "Alexander Joseph",
      "Nathan Francis",
      "Meijke Balay"
    ],
    "abstract": "Artificial neural networks (ANNs) were inspired by the architecture and\nfunctions of the human brain and have revolutionised the field of artificial\nintelligence (AI). Inspired by studies on the latent geometry of the brain, in\nthis perspective paper we posit that an increase in the research and\napplication of hyperbolic geometry in ANNs and machine learning will lead to\nincreased accuracy, improved feature space representations and more efficient\nmodels across a range of tasks. We examine the structure and functions of the\nhuman brain, emphasising the correspondence between its scale-free hierarchical\norganization and hyperbolic geometry, and reflecting on the central role\nhyperbolic geometry plays in facilitating human intelligence. Empirical\nevidence indicates that hyperbolic neural networks outperform Euclidean models\nfor tasks including natural language processing, computer vision and complex\nnetwork analysis, requiring fewer parameters and exhibiting better\ngeneralisation. Despite its nascent adoption, hyperbolic geometry holds promise\nfor improving machine learning models through brain-inspired geometric\nrepresentations.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "I.2"
    ],
    "primary_category": "q-bio.NC",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12990v3",
    "published_date": "2024-09-04 19:58:25 UTC",
    "updated_date": "2025-02-03 17:14:13 UTC"
  },
  {
    "arxiv_id": "2409.08297v1",
    "title": "Comparative Study of Long Short-Term Memory (LSTM) and Quantum Long Short-Term Memory (QLSTM): Prediction of Stock Market Movement",
    "authors": [
      "Tariq Mahmood",
      "Ibtasam Ahmad",
      "Malik Muhammad Zeeshan Ansar",
      "Jumanah Ahmed Darwish",
      "Rehan Ahmad Khan Sherwani"
    ],
    "abstract": "In recent years, financial analysts have been trying to develop models to\npredict the movement of a stock price index. The task becomes challenging in\nvague economic, social, and political situations like in Pakistan. In this\nstudy, we employed efficient models of machine learning such as long short-term\nmemory (LSTM) and quantum long short-term memory (QLSTM) to predict the Karachi\nStock Exchange (KSE) 100 index by taking monthly data of twenty-six economic,\nsocial, political, and administrative indicators from February 2004 to December\n2020. The comparative results of LSTM and QLSTM predicted values of the KSE 100\nindex with the actual values suggested QLSTM a potential technique to predict\nstock market trends.",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.LG",
      "quant-ph"
    ],
    "primary_category": "q-fin.ST",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.08297v1",
    "published_date": "2024-09-04 19:34:37 UTC",
    "updated_date": "2024-09-04 19:34:37 UTC"
  },
  {
    "arxiv_id": "2409.03043v2",
    "title": "Can Your Generative Model Detect Out-of-Distribution Covariate Shift?",
    "authors": [
      "Christiaan Viviers",
      "Amaan Valiuddin",
      "Francisco Caetano",
      "Lemar Abdi",
      "Lena Filatova",
      "Peter de With",
      "Fons van der Sommen"
    ],
    "abstract": "Detecting Out-of-Distribution (OOD) sensory data and covariate distribution\nshift aims to identify new test examples with different high-level image\nstatistics to the captured, normal and In-Distribution (ID) set. Existing OOD\ndetection literature largely focuses on semantic shift with little-to-no\nconsensus over covariate shift. Generative models capture the ID data in an\nunsupervised manner, enabling them to effectively identify samples that deviate\nsignificantly from this learned distribution, irrespective of the downstream\ntask. In this work, we elucidate the ability of generative models to detect and\nquantify domain-specific covariate shift through extensive analyses that\ninvolves a variety of models. To this end, we conjecture that it is sufficient\nto detect most occurring sensory faults (anomalies and deviations in global\nsignals statistics) by solely modeling high-frequency signal-dependent and\nindependent details. We propose a novel method, CovariateFlow, for OOD\ndetection, specifically tailored to covariate heteroscedastic high-frequency\nimage-components using conditional Normalizing Flows (cNFs). Our results on\nCIFAR10 vs. CIFAR10-C and ImageNet200 vs. ImageNet200-C demonstrate the\neffectiveness of the method by accurately detecting OOD covariate shift. This\nwork contributes to enhancing the fidelity of imaging systems and aiding\nmachine learning models in OOD detection in the presence of covariate shift.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024, typos corrected",
    "pdf_url": "http://arxiv.org/pdf/2409.03043v2",
    "published_date": "2024-09-04 19:27:56 UTC",
    "updated_date": "2024-10-09 15:44:35 UTC"
  },
  {
    "arxiv_id": "2409.02920v3",
    "title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)",
    "authors": [
      "Yao Mu",
      "Tianxing Chen",
      "Shijia Peng",
      "Zanxin Chen",
      "Zeyu Gao",
      "Yude Zou",
      "Lunkai Lin",
      "Zhiqiang Xie",
      "Ping Luo"
    ],
    "abstract": "In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples improve the success rate of over 70% for single-arm tasks and over 40%\nfor dual-arm tasks compared to models trained solely on real-world data. This\nsignificant improvement demonstrates RoboTwin's potential to enhance the\ndevelopment and evaluation of dual-arm robotic manipulation systems. Project\nPage: https://robotwin-benchmark.github.io/early-version/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.RO",
    "comment": "Project page: https://robotwin-benchmark.github.io/early-version/",
    "pdf_url": "http://arxiv.org/pdf/2409.02920v3",
    "published_date": "2024-09-04 17:59:52 UTC",
    "updated_date": "2025-04-16 17:31:39 UTC"
  },
  {
    "arxiv_id": "2409.02917v2",
    "title": "UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from Endoscopic Sparse Views",
    "authors": [
      "Jiaxin Guo",
      "Jiangliu Wang",
      "Ruofeng Wei",
      "Di Kang",
      "Qi Dou",
      "Yun-hui Liu"
    ],
    "abstract": "Visualizing surgical scenes is crucial for revealing internal anatomical\nstructures during minimally invasive procedures. Novel View Synthesis is a\nvital technique that offers geometry and appearance reconstruction, enhancing\nunderstanding, planning, and decision-making in surgical scenes. Despite the\nimpressive achievements of Neural Radiance Field (NeRF), its direct application\nto surgical scenes produces unsatisfying results due to two challenges:\nendoscopic sparse views and significant photometric inconsistencies. In this\npaper, we propose uncertainty-aware conditional NeRF for novel view synthesis\nto tackle the severe shape-radiance ambiguity from sparse surgical views. The\ncore of UC-NeRF is to incorporate the multi-view uncertainty estimation to\ncondition the neural radiance field for modeling the severe photometric\ninconsistencies adaptively. Specifically, our UC-NeRF first builds a\nconsistency learner in the form of multi-view stereo network, to establish the\ngeometric correspondence from sparse views and generate uncertainty estimation\nand feature priors. In neural rendering, we design a base-adaptive NeRF network\nto exploit the uncertainty estimation for explicitly handling the photometric\ninconsistencies. Furthermore, an uncertainty-guided geometry distillation is\nemployed to enhance geometry learning. Experiments on the SCARED and Hamlyn\ndatasets demonstrate our superior performance in rendering appearance and\ngeometry, consistently outperforming the current state-of-the-art approaches.\nOur code will be released at https://github.com/wrld/UC-NeRF.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to IEEE Transactions on Medical Imaging",
    "pdf_url": "http://arxiv.org/pdf/2409.02917v2",
    "published_date": "2024-09-04 17:53:42 UTC",
    "updated_date": "2024-11-09 15:33:32 UTC"
  },
  {
    "arxiv_id": "2409.03797v2",
    "title": "NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls",
    "authors": [
      "Kinjal Basu",
      "Ibrahim Abdelaziz",
      "Kiran Kate",
      "Mayank Agarwal",
      "Maxwell Crouse",
      "Yara Rizk",
      "Kelsey Bradford",
      "Asim Munawar",
      "Sadhana Kumaravel",
      "Saurabh Goyal",
      "Xin Wang",
      "Luis A. Lastras",
      "Pavan Kapanipathi"
    ],
    "abstract": "The resurgence of autonomous agents built using large language models (LLMs)\nto solve complex real-world tasks has brought increased focus on LLMs'\nfundamental ability of tool or function calling. At the core of these agents,\nan LLM must plan, execute, and respond using external tools, APIs, and custom\nfunctions. Research on tool calling has gathered momentum, but evaluation\nbenchmarks and datasets representing the complexity of the tasks have lagged\nbehind. In this work, we focus on one such complexity, nested sequencing, with\nthe goal of extending existing benchmarks and evaluation. Specifically, we\npresent NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls,\ni.e., sequences where the output of one API call is passed as input to a\nsubsequent call. NESTFUL contains 1800+ nested sequences where all the function\ncalls are executable. Experimental results on multiple models and settings show\nthat the best-performing model on the dataset has a full sequence match\naccuracy of 25% and win-rate of 34% necessitating a large scope for improvement\nin the nested sequencing aspect of function calling. Our analysis of these\nresults provides possible future research directions for the community, in\naddition to a benchmark to track progress. We have released the NESTFUL dataset\nunder the Apache 2.0 license at https://github.com/IBM/NESTFUL.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.03797v2",
    "published_date": "2024-09-04 17:53:24 UTC",
    "updated_date": "2025-01-23 18:44:28 UTC"
  },
  {
    "arxiv_id": "2409.02908v6",
    "title": "Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling",
    "authors": [
      "Kaiwen Zheng",
      "Yongxin Chen",
      "Hanzi Mao",
      "Ming-Yu Liu",
      "Jun Zhu",
      "Qinsheng Zhang"
    ],
    "abstract": "Masked diffusion models (MDMs) have emerged as a popular research topic for\ngenerative modeling of discrete data, thanks to their superior performance over\nother discrete diffusion models, and are rivaling the auto-regressive models\n(ARMs) for language modeling tasks. The recent effort in simplifying the masked\ndiffusion framework further leads to alignment with continuous-space diffusion\nmodels and more principled training and sampling recipes. In this paper,\nhowever, we reveal that both training and sampling of MDMs are theoretically\nfree from the time variable, arguably the key signature of diffusion models,\nand are instead equivalent to masked models. The connection on the sampling\naspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we\nshow that the FHS is theoretically equivalent to MDMs' original generation\nprocess while significantly alleviating the time-consuming categorical sampling\nand achieving a 20$\\times$ speedup. In addition, our investigation raises\ndoubts about whether MDMs can truly beat ARMs in text generation. We identify,\nfor the first time, an underlying numerical issue, even with the commonly used\n32-bit floating-point precision, which results in inaccurate categorical\nsampling. We show that it lowers the effective temperature both theoretically\nand empirically, and the resulting decrease in token diversity makes previous\nevaluations, which assess the generation quality solely through the incomplete\ngenerative perplexity metric, somewhat unfair.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.02908v6",
    "published_date": "2024-09-04 17:48:19 UTC",
    "updated_date": "2025-04-30 08:39:26 UTC"
  },
  {
    "arxiv_id": "2409.02889v2",
    "title": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture",
    "authors": [
      "Xidong Wang",
      "Dingjie Song",
      "Shunian Chen",
      "Chen Zhang",
      "Benyou Wang"
    ],
    "abstract": "Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 9 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.02889v2",
    "published_date": "2024-09-04 17:25:21 UTC",
    "updated_date": "2024-10-03 11:01:14 UTC"
  },
  {
    "arxiv_id": "2409.02883v1",
    "title": "Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test",
    "authors": [
      "Junyoung Park",
      "Eun Hyun Seo",
      "Sunjun Kim",
      "SangHak Yi",
      "Kun Ho Lee",
      "Sungho Won"
    ],
    "abstract": "Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to\nassess cognitive functions such as visuospatial skills and memory, making them\nvaluable tools for detecting mild cognitive impairment (MCI). Despite their\nutility, existing predictive models based on these tests often suffer from\nlimitations like small sample sizes and lack of external validation, which\nundermine their reliability. We developed a multi-stream deep learning\nframework that integrates two distinct processing streams: a multi-head\nself-attention based spatial stream using raw RCFT images and a scoring stream\nemploying a previously developed automated scoring system. Our model was\ntrained on data from 1,740 subjects in the Korean cohort and validated on an\nexternal hospital dataset of 222 subjects from Korea. The proposed multi-stream\nmodel demonstrated superior performance over baseline models (AUC = 0.872,\nAccuracy = 0.781) in external validation. The integration of both spatial and\nscoring streams enables the model to capture intricate visual details from the\nraw images while also incorporating structured scoring data, which together\nenhance its ability to detect subtle cognitive impairments. This dual approach\nnot only improves predictive accuracy but also increases the robustness of the\nmodel, making it more reliable in diverse clinical settings. Our model has\npractical implications for clinical settings, where it could serve as a\ncost-effective tool for early MCI screening.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.02883v1",
    "published_date": "2024-09-04 17:08:04 UTC",
    "updated_date": "2024-09-04 17:08:04 UTC"
  },
  {
    "arxiv_id": "2409.02877v1",
    "title": "Configurable Foundation Models: Building LLMs from a Modular Perspective",
    "authors": [
      "Chaojun Xiao",
      "Zhengyan Zhang",
      "Chenyang Song",
      "Dazhi Jiang",
      "Feng Yao",
      "Xu Han",
      "Xiaozhi Wang",
      "Shuo Wang",
      "Yufei Huang",
      "Guanyu Lin",
      "Yingfa Chen",
      "Weilin Zhao",
      "Yuge Tu",
      "Zexuan Zhong",
      "Ao Zhang",
      "Chenglei Si",
      "Khai Hao Moo",
      "Chenyang Zhao",
      "Huimin Chen",
      "Yankai Lin",
      "Zhiyuan Liu",
      "Jingbo Shang",
      "Maosong Sun"
    ],
    "abstract": "Advancements in LLMs have recently unveiled challenges tied to computational\nefficiency and continual scalability due to their requirements of huge\nparameters, making the applications and evolution of these models on devices\nwith limited computation resources and scenarios requiring various abilities\nincreasingly cumbersome. Inspired by modularity within the human brain, there\nis a growing tendency to decompose LLMs into numerous functional modules,\nallowing for inference with part of modules and dynamic assembly of modules to\ntackle complex tasks, such as mixture-of-experts. To highlight the inherent\nefficiency and composability of the modular approach, we coin the term brick to\nrepresent each functional module, designating the modularized structure as\nconfigurable foundation models. In this paper, we offer a comprehensive\noverview and investigation of the construction, utilization, and limitation of\nconfigurable foundation models. We first formalize modules into emergent bricks\n- functional neuron partitions that emerge during the pre-training phase, and\ncustomized bricks - bricks constructed via additional post-training to improve\nthe capabilities and knowledge of LLMs. Based on diverse functional bricks, we\nfurther present four brick-oriented operations: retrieval and routing, merging,\nupdating, and growing. These operations allow for dynamic configuration of LLMs\nbased on instructions to handle complex tasks. To verify our perspective, we\nconduct an empirical analysis on widely-used LLMs. We find that the FFN layers\nfollow modular patterns with functional specialization of neurons and\nfunctional neuron partitions. Finally, we highlight several open issues and\ndirections for future research. Overall, this paper aims to offer a fresh\nmodular perspective on existing LLM research and inspire the future creation of\nmore efficient and scalable foundational models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02877v1",
    "published_date": "2024-09-04 17:01:02 UTC",
    "updated_date": "2024-09-04 17:01:02 UTC"
  },
  {
    "arxiv_id": "2409.02871v2",
    "title": "Hybrid Imitation-Learning Motion Planner for Urban Driving",
    "authors": [
      "Cristian Gariboldi",
      "Matteo Corno",
      "Beng Jin"
    ],
    "abstract": "With the release of open source datasets such as nuPlan and Argoverse, the\nresearch around learning-based planners has spread a lot in the last years.\nExisting systems have shown excellent capabilities in imitating the human\ndriver behaviour, but they struggle to guarantee safe closed-loop driving.\nConversely, optimization-based planners offer greater security in short-term\nplanning scenarios. To confront this challenge, in this paper we propose a\nnovel hybrid motion planner that integrates both learning-based and\noptimization-based techniques. Initially, a multilayer perceptron (MLP)\ngenerates a human-like trajectory, which is then refined by an\noptimization-based component. This component not only minimizes tracking errors\nbut also computes a trajectory that is both kinematically feasible and\ncollision-free with obstacles and road boundaries. Our model effectively\nbalances safety and human-likeness, mitigating the trade-off inherent in these\nobjectives. We validate our approach through simulation experiments and further\ndemonstrate its efficacy by deploying it in real-world self-driving vehicles.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "2024 IEEE 27th International Conference on Intelligent Transportation\n  Systems (ITSC)",
    "pdf_url": "http://arxiv.org/pdf/2409.02871v2",
    "published_date": "2024-09-04 16:54:31 UTC",
    "updated_date": "2025-04-19 06:11:51 UTC"
  },
  {
    "arxiv_id": "2409.02864v3",
    "title": "Language Model Powered Digital Biology with BRAD",
    "authors": [
      "Joshua Pickard",
      "Ram Prakash",
      "Marc Andrew Choi",
      "Natalie Oliven",
      "Cooper Stansbury",
      "Jillian Cwycyshyn",
      "Alex Gorodetsky",
      "Alvaro Velasquez",
      "Indika Rajapakse"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) are transforming biology,\ncomputer science, engineering, and every day life. However, integrating the\nwide array of computational tools, databases, and scientific literature\ncontinues to pose a challenge to biological research. LLMs are well-suited for\nunstructured integration, efficient information retrieval, and automating\nstandard workflows and actions from these diverse resources. To harness these\ncapabilities in bioinformatics, we present a prototype Bioinformatics Retrieval\nAugmented Digital assistant (BRAD). BRAD is a chatbot and agentic system that\nintegrates a variety of bioinformatics tools. The Python package implements an\nAI \\texttt{Agent} that is powered by LLMs and connects to a local file system,\nonline databases, and a user's software. The \\texttt{Agent} is highly\nconfigurable, enabling tasks such as Retrieval-Augmented Generation, searches\nacross bioinformatics databases, and the execution of software pipelines.\nBRAD's coordinated integration of bioinformatics tools delivers a context-aware\nand semi-autonomous system that extends beyond the capabilities of conventional\nLLM-based chatbots. A graphical user interface (GUI) provides an intuitive\ninterface to the system.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 3 figures, 1 table. See: https://github.com/Jpickard1/BRAD",
    "pdf_url": "http://arxiv.org/pdf/2409.02864v3",
    "published_date": "2024-09-04 16:43:14 UTC",
    "updated_date": "2024-12-08 15:45:30 UTC"
  },
  {
    "arxiv_id": "2409.02850v2",
    "title": "Oops, I Sampled it Again: Reinterpreting Confidence Intervals in Few-Shot Learning",
    "authors": [
      "Raphael Lafargue",
      "Luke Smith",
      "Franck Vermet",
      "Mathias Löwe",
      "Ian Reid",
      "Vincent Gripon",
      "Jack Valmadre"
    ],
    "abstract": "The predominant method for computing confidence intervals (CI) in few-shot\nlearning (FSL) is based on sampling the tasks with replacement, i.e.\\ allowing\nthe same samples to appear in multiple tasks. This makes the CI misleading in\nthat it takes into account the randomness of the sampler but not the data\nitself. To quantify the extent of this problem, we conduct a comparative\nanalysis between CIs computed with and without replacement. These reveal a\nnotable underestimation by the predominant method. This observation calls for a\nreevaluation of how we interpret confidence intervals and the resulting\nconclusions in FSL comparative studies. Our research demonstrates that the use\nof paired tests can partially address this issue. Additionally, we explore\nmethods to further reduce the (size of the) CI by strategically sampling tasks\nof a specific size. We also introduce a new optimized benchmark, which can be\naccessed at https://github.com/RafLaf/FSL-benchmark-again",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "68T06",
      "I.2; I.4; I.5; G.3"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02850v2",
    "published_date": "2024-09-04 16:20:57 UTC",
    "updated_date": "2024-09-06 08:30:57 UTC"
  },
  {
    "arxiv_id": "2409.02840v1",
    "title": "R2GQA: Retriever-Reader-Generator Question Answering System to Support Students Understanding Legal Regulations in Higher Education",
    "authors": [
      "Phuc-Tinh Pham Do",
      "Duy-Ngoc Dinh Cao",
      "Khanh Quoc Tran",
      "Kiet Van Nguyen"
    ],
    "abstract": "In this article, we propose the R2GQA system, a Retriever-Reader-Generator\nQuestion Answering system, consisting of three main components: Document\nRetriever, Machine Reader, and Answer Generator. The Retriever module employs\nadvanced information retrieval techniques to extract the context of articles\nfrom a dataset of legal regulation documents. The Machine Reader module\nutilizes state-of-the-art natural language understanding algorithms to\ncomprehend the retrieved documents and extract answers. Finally, the Generator\nmodule synthesizes the extracted answers into concise and informative responses\nto questions of students regarding legal regulations. Furthermore, we built the\nViRHE4QA dataset in the domain of university training regulations, comprising\n9,758 question-answer pairs with a rigorous construction process. This is the\nfirst Vietnamese dataset in the higher regulations domain with various types of\nanswers, both extractive and abstractive. In addition, the R2GQA system is the\nfirst system to offer abstractive answers in Vietnamese. This paper discusses\nthe design and implementation of each module within the R2GQA system on the\nViRHE4QA dataset, highlighting their functionalities and interactions.\nFurthermore, we present experimental results demonstrating the effectiveness\nand utility of the proposed system in supporting the comprehension of students\nof legal regulations in higher education settings. In general, the R2GQA system\nand the ViRHE4QA dataset promise to contribute significantly to related\nresearch and help students navigate complex legal documents and regulations,\nempowering them to make informed decisions and adhere to institutional policies\neffectively. Our dataset is available for research purposes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02840v1",
    "published_date": "2024-09-04 16:12:30 UTC",
    "updated_date": "2024-09-04 16:12:30 UTC"
  },
  {
    "arxiv_id": "2409.02836v1",
    "title": "Exploring Sentiment Dynamics and Predictive Behaviors in Cryptocurrency Discussions by Few-Shot Learning with Large Language Models",
    "authors": [
      "Moein Shahiki Tash",
      "Zahra Ahani",
      "Mohim Tash",
      "Olga Kolesnikova",
      "Grigori Sidorov"
    ],
    "abstract": "This study performs analysis of Predictive statements, Hope speech, and\nRegret Detection behaviors within cryptocurrency-related discussions,\nleveraging advanced natural language processing techniques. We introduce a\nnovel classification scheme named \"Prediction statements,\" categorizing\ncomments into Predictive Incremental, Predictive Decremental, Predictive\nNeutral, or Non-Predictive categories. Employing GPT-4o, a cutting-edge large\nlanguage model, we explore sentiment dynamics across five prominent\ncryptocurrencies: Cardano, Binance, Matic, Fantom, and Ripple. Our analysis\nreveals distinct patterns in predictive sentiments, with Matic demonstrating a\nnotably higher propensity for optimistic predictions. Additionally, we\ninvestigate hope and regret sentiments, uncovering nuanced interplay between\nthese emotions and predictive behaviors. Despite encountering limitations\nrelated to data volume and resource availability, our study reports valuable\ndiscoveries concerning investor behavior and sentiment trends within the\ncryptocurrency market, informing strategic decision-making and future research\nendeavors.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02836v1",
    "published_date": "2024-09-04 16:02:30 UTC",
    "updated_date": "2024-09-04 16:02:30 UTC"
  },
  {
    "arxiv_id": "2409.02977v1",
    "title": "Large Language Model-Based Agents for Software Engineering: A Survey",
    "authors": [
      "Junwei Liu",
      "Kaixin Wang",
      "Yixuan Chen",
      "Xin Peng",
      "Zhenpeng Chen",
      "Lingming Zhang",
      "Yiling Lou"
    ],
    "abstract": "The recent advance in Large Language Models (LLMs) has shaped a new paradigm\nof AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based\nagents substantially extend the versatility and expertise of LLMs by enhancing\nLLMs with the capabilities of perceiving and utilizing external resources and\ntools. To date, LLM-based agents have been applied and shown remarkable\neffectiveness in Software Engineering (SE). The synergy between multiple agents\nand human interaction brings further promise in tackling complex real-world SE\nproblems. In this work, we present a comprehensive and systematic survey on\nLLM-based agents for SE. We collect 106 papers and categorize them from two\nperspectives, i.e., the SE and agent perspectives. In addition, we discuss open\nchallenges and future directions in this critical domain. The repository of\nthis survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02977v1",
    "published_date": "2024-09-04 15:59:41 UTC",
    "updated_date": "2024-09-04 15:59:41 UTC"
  },
  {
    "arxiv_id": "2409.03796v1",
    "title": "Protecting Activity Sensing Data Privacy Using Hierarchical Information Dissociation",
    "authors": [
      "Guangjing Wang",
      "Hanqing Guo",
      "Yuanda Wang",
      "Bocheng Chen",
      "Ce Zhou",
      "Qiben Yan"
    ],
    "abstract": "Smartphones and wearable devices have been integrated into our daily lives,\noffering personalized services. However, many apps become overprivileged as\ntheir collected sensing data contains unnecessary sensitive information. For\nexample, mobile sensing data could reveal private attributes (e.g., gender and\nage) and unintended sensitive features (e.g., hand gestures when entering\npasswords). To prevent sensitive information leakage, existing methods must\nobtain private labels and users need to specify privacy policies. However, they\nonly achieve limited control over information disclosure. In this work, we\npresent Hippo to dissociate hierarchical information including private metadata\nand multi-grained activity information from the sensing data. Hippo achieves\nfine-grained control over the disclosure of sensitive information without\nrequiring private labels. Specifically, we design a latent guidance-based\ndiffusion model, which generates multi-grained versions of raw sensor data\nconditioned on hierarchical latent activity features. Hippo enables users to\ncontrol the disclosure of sensitive information in sensing data, ensuring their\nprivacy while preserving the necessary features to meet the utility\nrequirements of applications. Hippo is the first unified model that achieves\ntwo goals: perturbing the sensitive attributes and controlling the disclosure\nof sensitive information in mobile sensing data. Extensive experiments show\nthat Hippo can anonymize personal attributes and transform activity information\nat various resolutions across different types of sensing data.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.03796v1",
    "published_date": "2024-09-04 15:38:00 UTC",
    "updated_date": "2024-09-04 15:38:00 UTC"
  },
  {
    "arxiv_id": "2409.02810v1",
    "title": "A hybrid FEM-PINN method for time-dependent partial differential equations",
    "authors": [
      "Xiaodong Feng",
      "Haojiong Shangguan",
      "Tao Tang",
      "Xiaoliang Wan",
      "Tao Zhou"
    ],
    "abstract": "In this work, we present a hybrid numerical method for solving evolution\npartial differential equations (PDEs) by merging the time finite element method\nwith deep neural networks. In contrast to the conventional deep learning-based\nformulation where the neural network is defined on a spatiotemporal domain, our\nmethodology utilizes finite element basis functions in the time direction where\nthe space-dependent coefficients are defined as the output of a neural network.\nWe then apply the Galerkin or collocation projection in the time direction to\nobtain a system of PDEs for the space-dependent coefficients which is\napproximated in the framework of PINN. The advantages of such a hybrid\nformulation are twofold: statistical errors are avoided for the integral in the\ntime direction, and the neural network's output can be regarded as a set of\nreduced spatial basis functions. To further alleviate the difficulties from\nhigh dimensionality and low regularity, we have developed an adaptive sampling\nstrategy that refines the training set. More specifically, we use an explicit\ndensity model to approximate the distribution induced by the PDE residual and\nthen augment the training set with new time-dependent random samples given by\nthe learned density model. The effectiveness and efficiency of our proposed\nmethod have been demonstrated through a series of numerical experiments.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA"
    ],
    "primary_category": "math.NA",
    "comment": "25pages",
    "pdf_url": "http://arxiv.org/pdf/2409.02810v1",
    "published_date": "2024-09-04 15:28:25 UTC",
    "updated_date": "2024-09-04 15:28:25 UTC"
  },
  {
    "arxiv_id": "2409.02808v1",
    "title": "Towards Edge-Based Data Lake Architecture for Intelligent Transportation System",
    "authors": [
      "Danilo Fernandes",
      "Douglas L. L. Moura",
      "Gean Santos",
      "Geymerson S. Ramos",
      "Fabiane Queiroz",
      "Andre L. L. Aquino"
    ],
    "abstract": "The rapid urbanization growth has underscored the need for innovative\nsolutions to enhance transportation efficiency and safety. Intelligent\nTransportation Systems (ITS) have emerged as a promising solution in this\ncontext. However, analyzing and processing the massive and intricate data\ngenerated by ITS presents significant challenges for traditional data\nprocessing systems. This work proposes an Edge-based Data Lake Architecture to\nintegrate and analyze the complex data from ITS efficiently. The architecture\noffers scalability, fault tolerance, and performance, improving decision-making\nand enhancing innovative services for a more intelligent transportation\necosystem. We demonstrate the effectiveness of the architecture through an\nanalysis of three different use cases: (i) Vehicular Sensor Network, (ii)\nMobile Network, and (iii) Driver Identification applications.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02808v1",
    "published_date": "2024-09-04 15:25:28 UTC",
    "updated_date": "2024-09-04 15:25:28 UTC"
  },
  {
    "arxiv_id": "2409.02779v1",
    "title": "Governing dual-use technologies: Case studies of international security agreements and lessons for AI governance",
    "authors": [
      "Akash R. Wasil",
      "Peter Barnett",
      "Michael Gerovitch",
      "Roman Hauksson",
      "Tom Reed",
      "Jack William Miller"
    ],
    "abstract": "International AI governance agreements and institutions may play an important\nrole in reducing global security risks from advanced AI. To inform the design\nof such agreements and institutions, we conducted case studies of historical\nand contemporary international security agreements. We focused specifically on\nthose arrangements around dual-use technologies, examining agreements in\nnuclear security, chemical weapons, biosecurity, and export controls. For each\nagreement, we examined four key areas: (a) purpose, (b) core powers, (c)\ngovernance structure, and (d) instances of non-compliance. From these case\nstudies, we extracted lessons for the design of international AI agreements and\ngovernance institutions. We discuss the importance of robust verification\nmethods, strategies for balancing power between nations, mechanisms for\nadapting to rapid technological change, approaches to managing trade-offs\nbetween transparency and security, incentives for participation, and effective\nenforcement mechanisms.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02779v1",
    "published_date": "2024-09-04 14:56:59 UTC",
    "updated_date": "2024-09-04 14:56:59 UTC"
  },
  {
    "arxiv_id": "2409.02760v1",
    "title": "An incremental preference elicitation-based approach to learning potentially non-monotonic preferences in multi-criteria sorting",
    "authors": [
      "Zhuolin Li",
      "Zhen Zhang",
      "Witold Pedrycz"
    ],
    "abstract": "This paper introduces a novel incremental preference elicitation-based\napproach to learning potentially non-monotonic preferences in multi-criteria\nsorting (MCS) problems, enabling decision makers to progressively provide\nassignment example preference information. Specifically, we first construct a\nmax-margin optimization-based model to model potentially non-monotonic\npreferences and inconsistent assignment example preference information in each\niteration of the incremental preference elicitation process. Using the optimal\nobjective function value of the max-margin optimization-based model, we devise\ninformation amount measurement methods and question selection strategies to\npinpoint the most informative alternative in each iteration within the\nframework of uncertainty sampling in active learning. Once the termination\ncriterion is satisfied, the sorting result for non-reference alternatives can\nbe determined through the use of two optimization models, i.e., the max-margin\noptimization-based model and the complexity controlling optimization model.\nSubsequently, two incremental preference elicitation-based algorithms are\ndeveloped to learn potentially non-monotonic preferences, considering different\ntermination criteria. Ultimately, we apply the proposed approach to a credit\nrating problem to elucidate the detailed implementation steps, and perform\ncomputational experiments on both artificial and real-world data sets to\ncompare the proposed question selection strategies with several benchmark\nstrategies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "37 pages, 22 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.02760v1",
    "published_date": "2024-09-04 14:36:20 UTC",
    "updated_date": "2024-09-04 14:36:20 UTC"
  },
  {
    "arxiv_id": "2409.02747v1",
    "title": "Tractable Offline Learning of Regular Decision Processes",
    "authors": [
      "Ahana Deb",
      "Roberto Cipollone",
      "Anders Jonsson",
      "Alessandro Ronca",
      "Mohammad Sadegh Talebi"
    ],
    "abstract": "This work studies offline Reinforcement Learning (RL) in a class of\nnon-Markovian environments called Regular Decision Processes (RDPs). In RDPs,\nthe unknown dependency of future observations and rewards from the past\ninteractions can be captured by some hidden finite-state automaton. For this\nreason, many RDP algorithms first reconstruct this unknown dependency using\nautomata learning techniques. In this paper, we show that it is possible to\novercome two strong limitations of previous offline RL algorithms for RDPs,\nnotably RegORL. This can be accomplished via the introduction of two original\ntechniques: the development of a new pseudometric based on formal languages,\nwhich removes a problematic dependency on\n$L_\\infty^\\mathsf{p}$-distinguishability parameters, and the adoption of\nCount-Min-Sketch (CMS), instead of naive counting. The former reduces the\nnumber of samples required in environments that are characterized by a low\ncomplexity in language-theoretic terms. The latter alleviates the memory\nrequirements for long planning horizons. We derive the PAC sample complexity\nbounds associated to each of these techniques, and we validate the approach\nexperimentally.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in EWRL 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.02747v1",
    "published_date": "2024-09-04 14:26:58 UTC",
    "updated_date": "2024-09-04 14:26:58 UTC"
  },
  {
    "arxiv_id": "2409.02976v2",
    "title": "Hallucination Detection in LLMs: Fast and Memory-Efficient Fine-Tuned Models",
    "authors": [
      "Gabriel Y. Arteaga",
      "Thomas B. Schön",
      "Nicolas Pielawski"
    ],
    "abstract": "Uncertainty estimation is a necessary component when implementing AI in\nhigh-risk settings, such as autonomous cars, medicine, or insurances. Large\nLanguage Models (LLMs) have seen a surge in popularity in recent years, but\nthey are subject to hallucinations, which may cause serious harm in high-risk\nsettings. Despite their success, LLMs are expensive to train and run: they need\na large amount of computations and memory, preventing the use of ensembling\nmethods in practice. In this work, we present a novel method that allows for\nfast and memory-friendly training of LLM ensembles. We show that the resulting\nensembles can detect hallucinations and are a viable approach in practice as\nonly one GPU is needed for training and inference.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.02976v2",
    "published_date": "2024-09-04 13:59:38 UTC",
    "updated_date": "2024-12-06 12:39:00 UTC"
  },
  {
    "arxiv_id": "2409.12197v3",
    "title": "Nteasee: A mixed methods study of expert and general population perspectives on deploying AI for health in African countries",
    "authors": [
      "Mercy Nyamewaa Asiedu",
      "Iskandar Haykel",
      "Awa Dieng",
      "Kerrie Kauer",
      "Tousif Ahmed",
      "Florence Ofori",
      "Charisma Chan",
      "Stephen Pfohl",
      "Negar Rostamzadeh",
      "Katherine Heller"
    ],
    "abstract": "Artificial Intelligence (AI) for health has the potential to significantly\nchange and improve healthcare. However in most African countries, identifying\nculturally and contextually attuned approaches for deploying these solutions is\nnot well understood. To bridge this gap, we conduct a qualitative study to\ninvestigate the best practices, fairness indicators, and potential biases to\nmitigate when deploying AI for health in African countries, as well as explore\nopportunities where artificial intelligence could make a positive impact in\nhealth. We used a mixed methods approach combining in-depth interviews (IDIs)\nand surveys. We conduct 1.5-2 hour long IDIs with 50 experts in health, policy,\nand AI across 17 countries, and through an inductive approach we conduct a\nqualitative thematic analysis on expert IDI responses. We administer a blinded\n30-minute survey with case studies to 672 general population participants\nacross 5 countries in Africa and analyze responses on quantitative scales,\nstatistically comparing responses by country, age, gender, and level of\nfamiliarity with AI. We thematically summarize open-ended responses from\nsurveys. Our results find generally positive attitudes, high levels of trust,\naccompanied by moderate levels of concern among general population participants\nfor AI usage for health in Africa. This contrasts with expert responses, where\nmajor themes revolved around trust/mistrust, ethical concerns, and systemic\nbarriers to integration, among others. This work presents the first-of-its-kind\nqualitative research study of the potential of AI for health in Africa from an\nalgorithmic fairness angle, with perspectives from both experts and the general\npopulation. We hope that this work guides policymakers and drives home the need\nfor further research and the inclusion of general population perspectives in\ndecision-making around AI usage.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "added illustrative figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12197v3",
    "published_date": "2024-09-04 13:56:49 UTC",
    "updated_date": "2024-11-11 18:42:57 UTC"
  },
  {
    "arxiv_id": "2409.02711v1",
    "title": "Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for PostNL",
    "authors": [
      "Mohammad Reshadati"
    ],
    "abstract": "The developments in the field of generative AI has brought a lot of\nopportunities for companies, for instance to improve efficiency in customer\nservice and automating tasks. PostNL, the biggest parcel and E-commerce\ncorporation of the Netherlands wants to use generative AI to enhance the\ncommunication around track and trace of parcels. During the internship a\nMinimal Viable Product (MVP) is created to showcase the value of using\ngenerative AI technologies, to enhance parcel tracking, analyzing the parcel's\njourney and being able to communicate about it in an easy to understand manner.\nThe primary goal was to develop an in-house LLM-based system, reducing\ndependency on external platforms and establishing the feasibility of a\ndedicated generative AI team within the company. This multi-agent LLM based\nsystem aimed to construct parcel journey stories and identify logistical\ndisruptions with heightened efficiency and accuracy. The research involved\ndeploying a sophisticated AI-driven communication system, employing\nRetrieval-Augmented Generation (RAG) for enhanced response precision, and\noptimizing large language models (LLMs) tailored to domain specific tasks.\n  The MVP successfully implemented a multi-agent open-source LLM system, called\nSuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of\nuser inquiries and improving internal knowledge handling. Results and\nevaluation demonstrated technological innovation and feasibility, notably in\ncommunication about the track and trace of a parcel, which exceeded initial\nexpectations. These advancements highlight the potential of AI-driven solutions\nin logistics, suggesting many opportunities for further refinement and broader\nimplementation within PostNL operational framework.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02711v1",
    "published_date": "2024-09-04 13:49:19 UTC",
    "updated_date": "2024-09-04 13:49:19 UTC"
  },
  {
    "arxiv_id": "2409.02702v2",
    "title": "Incorporating Like-Minded Peers to Overcome Friend Data Sparsity in Session-Based Social Recommendations",
    "authors": [
      "Chunyan An",
      "Yunhan Li",
      "Qiang Yang",
      "Winston K. G. Seah",
      "Zhixu Li",
      "Conghao Yang"
    ],
    "abstract": "Session-based Social Recommendation (SSR) leverages social relationships\nwithin online networks to enhance the performance of Session-based\nRecommendation (SR). However, existing SSR algorithms often encounter the\nchallenge of \"friend data sparsity\". Moreover, significant discrepancies can\nexist between the purchase preferences of social network friends and those of\nthe target user, reducing the influence of friends relative to the target\nuser's own preferences. To address these challenges, this paper introduces the\nconcept of \"Like-minded Peers\" (LMP), representing users whose preferences\nalign with the target user's current session based on their historical\nsessions. This is the first work, to our knowledge, that uses LMP to enhance\nthe modeling of social influence in SSR. This approach not only alleviates the\nproblem of friend data sparsity but also effectively incorporates users with\nsimilar preferences to the target user. We propose a novel model named\nTransformer Encoder with Graph Attention Aggregator Recommendation (TEGAARec),\nwhich includes the TEGAA module and the GAT-based social aggregation module.\nThe TEGAA module captures and merges both long-term and short-term interests\nfor target users and LMP users. Concurrently, the GAT-based social aggregation\nmodule is designed to aggregate the target users' dynamic interests and social\ninfluence in a weighted manner. Extensive experiments on four real-world\ndatasets demonstrate the efficacy and superiority of our proposed model and\nablation studies are done to illustrate the contributions of each component in\nTEGAARec.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "None",
    "pdf_url": "http://arxiv.org/pdf/2409.02702v2",
    "published_date": "2024-09-04 13:39:12 UTC",
    "updated_date": "2024-09-07 00:40:09 UTC"
  },
  {
    "arxiv_id": "2409.02697v2",
    "title": "Decision Transformer for Enhancing Neural Local Search on the Job Shop Scheduling Problem",
    "authors": [
      "Constantin Waubert de Puiseau",
      "Fabian Wolz",
      "Merlin Montag",
      "Jannik Peters",
      "Hasan Tercan",
      "Tobias Meisen"
    ],
    "abstract": "The job shop scheduling problem (JSSP) and its solution algorithms have been\nof enduring interest in both academia and industry for decades. In recent\nyears, machine learning (ML) is playing an increasingly important role in\nadvancing existing and building new heuristic solutions for the JSSP, aiming to\nfind better solutions in shorter computation times. In this paper we build on\ntop of a state-of-the-art deep reinforcement learning (DRL) agent, called\nNeural Local Search (NLS), which can efficiently and effectively control a\nlarge local neighborhood search on the JSSP. In particular, we develop a method\nfor training the decision transformer (DT) algorithm on search trajectories\ntaken by a trained NLS agent to further improve upon the learned\ndecision-making sequences. Our experiments show that the DT successfully learns\nlocal search strategies that are different and, in many cases, more effective\nthan those of the NLS agent itself. In terms of the tradeoff between solution\nquality and acceptable computational time needed for the search, the DT is\nparticularly superior in application scenarios where longer computational times\nare acceptable. In this case, it makes up for the longer inference times\nrequired per search step, which are caused by the larger neural network\narchitecture, through better quality decisions per step. Thereby, the DT\nachieves state-of-the-art results for solving the JSSP with ML-enhanced search.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02697v2",
    "published_date": "2024-09-04 13:33:38 UTC",
    "updated_date": "2025-02-04 09:47:30 UTC"
  },
  {
    "arxiv_id": "2409.02693v1",
    "title": "The Role of Artificial Intelligence and Machine Learning in Software Testing",
    "authors": [
      "Ahmed Ramadan",
      "Husam Yasin",
      "Burhan Pektas"
    ],
    "abstract": "Artificial Intelligence (AI) and Machine Learning (ML) have significantly\nimpacted various industries, including software development. Software testing,\na crucial part of the software development lifecycle (SDLC), ensures the\nquality and reliability of software products. Traditionally, software testing\nhas been a labor-intensive process requiring significant manual effort.\nHowever, the advent of AI and ML has transformed this landscape by introducing\nautomation and intelligent decision-making capabilities. AI and ML technologies\nenhance the efficiency and effectiveness of software testing by automating\ncomplex tasks such as test case generation, test execution, and result\nanalysis. These technologies reduce the time required for testing and improve\nthe accuracy of defect detection, ultimately leading to higher quality\nsoftware. AI can predict potential areas of failure by analyzing historical\ndata and identifying patterns, which allows for more targeted and efficient\ntesting. This paper explores the role of AI and ML in software testing by\nreviewing existing literature, analyzing current tools and techniques, and\npresenting case studies that demonstrate the practical benefits of these\ntechnologies. The literature review provides a comprehensive overview of the\nadvancements in AI and ML applications in software testing, highlighting key\nmethodologies and findings from various studies. The analysis of current tools\nshowcases the capabilities of popular AI-driven testing tools such as Eggplant\nAI, Test.ai, Selenium, Appvance, Applitools Eyes, Katalon Studio, and Tricentis\nTosca, each offering unique features and advantages. Case studies included in\nthis paper illustrate real-world applications of AI and ML in software testing,\nshowing significant improvements in testing efficiency, accuracy, and overall\nsoftware quality.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02693v1",
    "published_date": "2024-09-04 13:25:13 UTC",
    "updated_date": "2024-09-04 13:25:13 UTC"
  },
  {
    "arxiv_id": "2409.02691v1",
    "title": "LLM-Assisted Visual Analytics: Opportunities and Challenges",
    "authors": [
      "Maeve Hutchinson",
      "Radu Jianu",
      "Aidan Slingsby",
      "Pranava Madhyastha"
    ],
    "abstract": "We explore the integration of large language models (LLMs) into visual\nanalytics (VA) systems to transform their capabilities through intuitive\nnatural language interactions. We survey current research directions in this\nemerging field, examining how LLMs are integrated into data management,\nlanguage interaction, visualisation generation, and language generation\nprocesses. We highlight the new possibilities that LLMs bring to VA, especially\nhow they can change VA processes beyond the usual use cases. We especially\nhighlight building new visualisation-language models, allowing access of a\nbreadth of domain knowledge, multimodal interaction, and opportunities with\nguidance. Finally, we carefully consider the prominent challenges of using\ncurrent LLMs in VA tasks. Our discussions in this paper aim to guide future\nresearchers working on LLM-assisted VA systems and help them navigate common\nobstacles when developing these systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at EG UK Computer Graphics & Visual Computing 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.02691v1",
    "published_date": "2024-09-04 13:24:03 UTC",
    "updated_date": "2024-09-04 13:24:03 UTC"
  },
  {
    "arxiv_id": "2409.02686v2",
    "title": "Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs",
    "authors": [
      "Ruoyu Wang",
      "Xiaoxuan Li",
      "Lina Yao"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable efficiency in\ntackling various tasks based on human instructions, but studies reveal that\nthey often struggle with tasks requiring reasoning, such as math or physics.\nThis limitation raises questions about whether LLMs truly comprehend embedded\nknowledge or merely learn to replicate the token distribution without a true\nunderstanding of the content. In this paper, we delve into this problem and aim\nto enhance the reasoning capabilities of LLMs. First, we investigate if the\nmodel has genuine reasoning capabilities by visualizing the text generation\nprocess at the attention and representation level. Then, we formulate the\nreasoning process of LLMs into a causal framework, which provides a formal\nexplanation of the problems observed in the visualization. Finally, building\nupon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a\nnovel parameter-efficient fine-tuning (PEFT) method to enhance the model's\nreasoning capabilities by encouraging the model to extract the general\nproblem-solving skills and apply these skills to different questions.\nExperiments show that our method outperforms the baseline consistently across\nmultiple benchmarks, and with only 1.2M tunable parameters, we achieve better\nor comparable results to other fine-tuning methods. This demonstrates the\neffectiveness and efficiency of our method in improving the overall accuracy\nand reliability of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02686v2",
    "published_date": "2024-09-04 13:17:09 UTC",
    "updated_date": "2024-10-05 11:29:41 UTC"
  },
  {
    "arxiv_id": "2409.02685v2",
    "title": "RouterRetriever: Routing over a Mixture of Expert Embedding Models",
    "authors": [
      "Hyunji Lee",
      "Luca Soldaini",
      "Arman Cohan",
      "Minjoon Seo",
      "Kyle Lo"
    ],
    "abstract": "Information retrieval methods often rely on a single embedding model trained\non large, general-domain datasets like MSMARCO. While this approach can produce\na retriever with reasonable overall performance, they often underperform models\ntrained on domain-specific data when testing on their respective domains. Prior\nwork in information retrieval has tackled this through multi-task training, but\nthe idea of routing over a mixture of domain-specific expert retrievers remains\nunexplored despite the popularity of such ideas in language model generation\nresearch. In this work, we introduce RouterRetriever, a retrieval model that\nleverages a mixture of domain-specific experts by using a routing mechanism to\nselect the most appropriate expert for each query. RouterRetriever is\nlightweight and allows easy addition or removal of experts without additional\ntraining. Evaluation on the BEIR benchmark demonstrates that RouterRetriever\noutperforms both models trained on MSMARCO (+2.1 absolute nDCG@10) and\nmulti-task models (+3.2). This is achieved by employing our routing mechanism,\nwhich surpasses other routing techniques (+1.8 on average) commonly used in\nlanguage modeling. Furthermore, the benefit generalizes well to other datasets,\neven in the absence of a specific expert on the dataset. RouterRetriever is the\nfirst work to demonstrate the advantages of routing over a mixture of\ndomain-specific expert embedding models as an alternative to a single,\ngeneral-purpose embedding model, especially when retrieving from diverse,\nspecialized domains.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "published at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.02685v2",
    "published_date": "2024-09-04 13:16:55 UTC",
    "updated_date": "2025-02-26 06:19:05 UTC"
  },
  {
    "arxiv_id": "2409.02681v6",
    "title": "Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon",
    "authors": [
      "Ramon Tavares",
      "Ricardo Olinda"
    ],
    "abstract": "This study presents a comprehensive methodology for modeling and forecasting\nthe historical time series of active fire spots detected by the AQUA\\_M-T\nsatellite in the Amazon, Brazil. The approach employs a mixed Recurrent Neural\nNetwork (RNN) model, combining Long Short-Term Memory (LSTM) and Gated\nRecurrent Unit (GRU) architectures to predict the monthly accumulations of\ndaily detected active fire spots. Data analysis revealed a consistent\nseasonality over time, with annual maximum and minimum values tending to repeat\nat the same periods each year. The primary objective is to verify whether the\nforecasts capture this inherent seasonality through machine learning\ntechniques. The methodology involved careful data preparation, model\nconfiguration, and training using cross-validation with two seeds, ensuring\nthat the data generalizes well to both the test and validation sets for both\nseeds. The results indicate that the combined LSTM and GRU model delivers\nexcellent forecasting performance, demonstrating its effectiveness in capturing\ncomplex temporal patterns and modeling the observed time series. This research\nsignificantly contributes to the application of deep learning techniques in\nenvironmental monitoring, specifically in forecasting active fire spots. The\nproposed approach highlights the potential for adaptation to other time series\nforecasting challenges, opening new opportunities for research and development\nin machine learning and prediction of natural phenomena.\n  Keywords: Time Series Forecasting; Recurrent Neural Networks; Deep Learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages and 24 figures, in Portuguese language",
    "pdf_url": "http://arxiv.org/pdf/2409.02681v6",
    "published_date": "2024-09-04 13:11:59 UTC",
    "updated_date": "2024-11-01 23:24:37 UTC"
  },
  {
    "arxiv_id": "2409.02672v2",
    "title": "Independence Constrained Disentangled Representation Learning from Epistemological Perspective",
    "authors": [
      "Ruoyu Wang",
      "Lina Yao"
    ],
    "abstract": "Disentangled Representation Learning aims to improve the explainability of\ndeep learning methods by training a data encoder that identifies semantically\nmeaningful latent variables in the data generation process. Nevertheless, there\nis no consensus regarding a universally accepted definition for the objective\nof disentangled representation learning. In particular, there is a considerable\namount of discourse regarding whether should the latent variables be mutually\nindependent or not. In this paper, we first investigate these arguments on the\ninterrelationships between latent variables by establishing a conceptual bridge\nbetween Epistemology and Disentangled Representation Learning. Then, inspired\nby these interdisciplinary concepts, we introduce a two-level latent space\nframework to provide a general solution to the prior arguments on this issue.\nFinally, we propose a novel method for disentangled representation learning by\nemploying an integration of mutual information constraint and independence\nconstraint within the Generative Adversarial Network (GAN) framework.\nExperimental results demonstrate that our proposed method consistently\noutperforms baseline approaches in both quantitative and qualitative\nevaluations. The method exhibits strong performance across multiple commonly\nused metrics and demonstrates a great capability in disentangling various\nsemantic factors, leading to an improved quality of controllable generation,\nwhich consequently benefits the explainability of the algorithm.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02672v2",
    "published_date": "2024-09-04 13:00:59 UTC",
    "updated_date": "2024-10-05 11:32:35 UTC"
  },
  {
    "arxiv_id": "2409.02669v2",
    "title": "Causality-Aware Transformer Networks for Robotic Navigation",
    "authors": [
      "Ruoyu Wang",
      "Yao Liu",
      "Yuanjiang Cao",
      "Lina Yao"
    ],
    "abstract": "Current research in Visual Navigation reveals opportunities for improvement.\nFirst, the direct adoption of RNNs and Transformers often overlooks the\nspecific differences between Embodied AI and traditional sequential data\nmodelling, potentially limiting its performance in Embodied AI tasks. Second,\nthe reliance on task-specific configurations, such as pre-trained modules and\ndataset-specific logic, compromises the generalizability of these methods. We\naddress these constraints by initially exploring the unique differences between\nNavigation tasks and other sequential data tasks through the lens of Causality,\npresenting a causal framework to elucidate the inadequacies of conventional\nsequential methods for Navigation. By leveraging this causal perspective, we\npropose Causality-Aware Transformer (CAT) Networks for Navigation, featuring a\nCausal Understanding Module to enhance the models's Environmental Understanding\ncapability. Meanwhile, our method is devoid of task-specific inductive biases\nand can be trained in an End-to-End manner, which enhances the method's\ngeneralizability across various contexts. Empirical evaluations demonstrate\nthat our methodology consistently surpasses benchmark performances across a\nspectrum of settings, tasks and simulation environments. Extensive ablation\nstudies reveal that the performance gains can be attributed to the Causal\nUnderstanding Module, which demonstrates effectiveness and efficiency in both\nReinforcement Learning and Supervised Learning settings.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02669v2",
    "published_date": "2024-09-04 12:53:26 UTC",
    "updated_date": "2024-10-05 11:34:38 UTC"
  },
  {
    "arxiv_id": "2409.02657v1",
    "title": "PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation",
    "authors": [
      "Jun Ling",
      "Yiwen Wang",
      "Han Xue",
      "Rong Xie",
      "Li Song"
    ],
    "abstract": "While previous audio-driven talking head generation (THG) methods generate\nhead poses from driving audio, the generated poses or lips cannot match the\naudio well or are not editable. In this study, we propose \\textbf{PoseTalk}, a\nTHG system that can freely generate lip-synchronized talking head videos with\nfree head poses conditioned on text prompts and audio. The core insight of our\nmethod is using head pose to connect visual, linguistic, and audio signals.\nFirst, we propose to generate poses from both audio and text prompts, where the\naudio offers short-term variations and rhythm correspondence of the head\nmovements and the text prompts describe the long-term semantics of head\nmotions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to\ngenerate motion latent from text prompts and audio cues in a pose latent space.\nSecond, we observe a loss-imbalance problem: the loss for the lip region\ncontributes less than 4\\% of the total reconstruction loss caused by both pose\nand lip, making optimization lean towards head movements rather than lip\nshapes. To address this issue, we propose a refinement-based learning strategy\nto synthesize natural talking videos using two cascaded networks, i.e.,\nCoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce\nanimated images in novel poses and the RefineNet focuses on learning finer lip\nmotions by progressively estimating lip motions from low-to-high resolutions,\nyielding improved lip-synchronization performance. Experiments demonstrate our\npose prediction strategy achieves better pose diversity and realness compared\nto text-only or audio-only, and our video generator model outperforms\nstate-of-the-art methods in synthesizing talking videos with natural head\nmotions. Project: https://junleen.github.io/projects/posetalk.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "7+5 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.02657v1",
    "published_date": "2024-09-04 12:30:25 UTC",
    "updated_date": "2024-09-04 12:30:25 UTC"
  },
  {
    "arxiv_id": "2409.02649v2",
    "title": "OpenFact at CheckThat! 2024: Combining Multiple Attack Methods for Effective Adversarial Text Generation",
    "authors": [
      "Włodzimierz Lewoniewski",
      "Piotr Stolarski",
      "Milena Stróżyna",
      "Elzbieta Lewańska",
      "Aleksandra Wojewoda",
      "Ewelina Księżniak",
      "Marcin Sawiński"
    ],
    "abstract": "This paper presents the experiments and results for the CheckThat! Lab at\nCLEF 2024 Task 6: Robustness of Credibility Assessment with Adversarial\nExamples (InCrediblAE). The primary objective of this task was to generate\nadversarial examples in five problem domains in order to evaluate the\nrobustness of widely used text classification methods (fine-tuned BERT, BiLSTM,\nand RoBERTa) when applied to credibility assessment issues.\n  This study explores the application of ensemble learning to enhance\nadversarial attacks on natural language processing (NLP) models. We\nsystematically tested and refined several adversarial attack methods, including\nBERT-Attack, Genetic algorithms, TextFooler, and CLARE, on five datasets across\nvarious misinformation tasks. By developing modified versions of BERT-Attack\nand hybrid methods, we achieved significant improvements in attack\neffectiveness. Our results demonstrate the potential of modification and\ncombining multiple methods to create more sophisticated and effective\nadversarial attack strategies, contributing to the development of more robust\nand secure systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "CLEF 2024 - Conference and Labs of the Evaluation Forum",
    "pdf_url": "http://arxiv.org/pdf/2409.02649v2",
    "published_date": "2024-09-04 12:26:26 UTC",
    "updated_date": "2024-09-05 06:20:36 UTC"
  },
  {
    "arxiv_id": "2409.02632v1",
    "title": "Evaluating Environments Using Exploratory Agents",
    "authors": [
      "Bobby Khaleque",
      "Mike Cook",
      "Jeremy Gow"
    ],
    "abstract": "Exploration is a key part of many video games. We investigate the using an\nexploratory agent to provide feedback on the design of procedurally generated\ngame levels, 5 engaging levels and 5 unengaging levels. We expand upon a\nframework introduced in previous research which models motivations for\nexploration and introduce a fitness function for evaluating an environment's\npotential for exploration. Our study showed that our exploratory agent can\nclearly distinguish between engaging and unengaging levels. The findings\nsuggest that our agent has the potential to serve as an effective tool for\nassessing procedurally generated levels, in terms of exploration. This work\ncontributes to the growing field of AI-driven game design by offering new\ninsights into how game environments can be evaluated and optimised for player\nexploration.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "9 Pages, 9 figures, 2 tables, work in progress",
    "pdf_url": "http://arxiv.org/pdf/2409.02632v1",
    "published_date": "2024-09-04 11:51:26 UTC",
    "updated_date": "2024-09-04 11:51:26 UTC"
  },
  {
    "arxiv_id": "2409.02629v1",
    "title": "AdvSecureNet: A Python Toolkit for Adversarial Machine Learning",
    "authors": [
      "Melih Catal",
      "Manuel Günther"
    ],
    "abstract": "Machine learning models are vulnerable to adversarial attacks. Several tools\nhave been developed to research these vulnerabilities, but they often lack\ncomprehensive features and flexibility. We introduce AdvSecureNet, a PyTorch\nbased toolkit for adversarial machine learning that is the first to natively\nsupport multi-GPU setups for attacks, defenses, and evaluation. It is the first\ntoolkit that supports both CLI and API interfaces and external YAML\nconfiguration files to enhance versatility and reproducibility. The toolkit\nincludes multiple attacks, defenses and evaluation metrics. Rigiorous software\nengineering practices are followed to ensure high code quality and\nmaintainability. The project is available as an open-source project on GitHub\nat https://github.com/melihcatal/advsecurenet and installable via PyPI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02629v1",
    "published_date": "2024-09-04 11:47:00 UTC",
    "updated_date": "2024-09-04 11:47:00 UTC"
  },
  {
    "arxiv_id": "2409.15299v1",
    "title": "Irrelevant Alternatives Bias Large Language Model Hiring Decisions",
    "authors": [
      "Kremena Valkanova",
      "Pencho Yordanov"
    ],
    "abstract": "We investigate whether LLMs display a well-known human cognitive bias, the\nattraction effect, in hiring decisions. The attraction effect occurs when the\npresence of an inferior candidate makes a superior candidate more appealing,\nincreasing the likelihood of the superior candidate being chosen over a\nnon-dominated competitor. Our study finds consistent and significant evidence\nof the attraction effect in GPT-3.5 and GPT-4 when they assume the role of a\nrecruiter. Irrelevant attributes of the decoy, such as its gender, further\namplify the observed bias. GPT-4 exhibits greater bias variation than GPT-3.5.\nOur findings remain robust even when warnings against the decoy effect are\nincluded and the recruiter role definition is varied.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.15299v1",
    "published_date": "2024-09-04 10:37:36 UTC",
    "updated_date": "2024-09-04 10:37:36 UTC"
  },
  {
    "arxiv_id": "2409.02598v1",
    "title": "SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments",
    "authors": [
      "Wenwu Guo",
      "Jinlin Wu",
      "Zhen Chen",
      "Qingxiang Zhao",
      "Miao Xu",
      "Zhen Lei",
      "Hongbin Liu"
    ],
    "abstract": "Vision-based surgical navigation has received increasing attention due to its\nnon-invasive, cost-effective, and flexible advantages. In particular, a\ncritical element of the vision-based navigation system is tracking surgical\ninstruments. Compared with 2D instrument tracking methods, 3D instrument\ntracking has broader value in clinical practice, but is also more challenging\ndue to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models\nfor 3D registration. To solve these challenges, we propose the SurgTrack, a\ntwo-stage 3D instrument tracking method for CAD-free and robust real-world\napplications. In the first registration stage, we incorporate an Instrument\nSigned Distance Field (SDF) modeling the 3D representation of instruments,\nachieving CAD-freed 3D registration. Due to this, we can obtain the location\nand orientation of instruments in the 3D space by matching the video stream\nwith the registered SDF model. In the second tracking stage, we devise a\nposture graph optimization module, leveraging the historical tracking results\nof the posture memory pool to optimize the tracking results and improve the\nocclusion robustness. Furthermore, we collect the Instrument3D dataset to\ncomprehensively evaluate the 3D tracking of surgical instruments. The extensive\nexperiments validate the superiority and scalability of our SurgTrack, by\noutperforming the state-of-the-arts with a remarkable improvement. The code and\ndataset are available at https://github.com/wenwucode/SurgTrack.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02598v1",
    "published_date": "2024-09-04 10:29:59 UTC",
    "updated_date": "2024-09-04 10:29:59 UTC"
  },
  {
    "arxiv_id": "2409.02580v1",
    "title": "AlignGroup: Learning and Aligning Group Consensus with Member Preferences for Group Recommendation",
    "authors": [
      "Jinfeng Xu",
      "Zheyu Chen",
      "Jinze Li",
      "Shuo Yang",
      "Hewei Wang",
      "Edith C. -H. Ngai"
    ],
    "abstract": "Group activities are important behaviors in human society, providing\npersonalized recommendations for groups is referred to as the group\nrecommendation task. Existing methods can usually be categorized into two\nstrategies to infer group preferences: 1) determining group preferences by\naggregating members' personalized preferences, and 2) inferring group consensus\nby capturing group members' coherent decisions after common compromises.\nHowever, the former would suffer from the lack of group-level considerations,\nand the latter overlooks the fine-grained preferences of individual users. To\nthis end, we propose a novel group recommendation method AlignGroup, which\nfocuses on both group consensus and individual preferences of group members to\ninfer the group decision-making. Specifically, AlignGroup explores group\nconsensus through a well-designed hypergraph neural network that efficiently\nlearns intra- and inter-group relationships. Moreover, AlignGroup innovatively\nutilizes a self-supervised alignment task to capture fine-grained group\ndecision-making by aligning the group consensus with members' common\npreferences. Extensive experiments on two real-world datasets validate that our\nAlignGroup outperforms the state-of-the-art on both the group recommendation\ntask and the user recommendation task, as well as outperforms the efficiency of\nmost baselines.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, accepted by CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.02580v1",
    "published_date": "2024-09-04 10:03:09 UTC",
    "updated_date": "2024-09-04 10:03:09 UTC"
  },
  {
    "arxiv_id": "2409.09063v1",
    "title": "TS-EoH: An Edge Server Task Scheduling Algorithm Based on Evolution of Heuristic",
    "authors": [
      "Wang Yatong",
      "Pei Yuchen",
      "Zhao Yuqi"
    ],
    "abstract": "With the widespread adoption of 5G and Internet of Things (IoT) technologies,\nthe low latency provided by edge computing has great importance for real-time\nprocessing. However, managing numerous simultaneous service requests poses a\nsignificant challenge to maintaining low latency. Current edge server task\nscheduling methods often fail to balance multiple optimization goals\neffectively. This paper introduces a novel task-scheduling approach based on\nEvolutionary Computing (EC) theory and heuristic algorithms. We model service\nrequests as task sequences and evaluate various scheduling schemes during each\nevolutionary process using Large Language Models (LLMs) services. Experimental\nresults show that our task-scheduling algorithm outperforms existing heuristic\nand traditional reinforcement learning methods. Additionally, we investigate\nthe effects of different heuristic strategies and compare the evolutionary\noutcomes across various LLM services.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09063v1",
    "published_date": "2024-09-04 10:00:32 UTC",
    "updated_date": "2024-09-04 10:00:32 UTC"
  },
  {
    "arxiv_id": "2409.11414v1",
    "title": "RTLRewriter: Methodologies for Large Models aided RTL Code Optimization",
    "authors": [
      "Xufeng Yao",
      "Yiwen Wang",
      "Xing Li",
      "Yingzhao Lian",
      "Ran Chen",
      "Lei Chen",
      "Mingxuan Yuan",
      "Hong Xu",
      "Bei Yu"
    ],
    "abstract": "Register Transfer Level (RTL) code optimization is crucial for enhancing the\nefficiency and performance of digital circuits during early synthesis stages.\nCurrently, optimization relies heavily on manual efforts by skilled engineers,\noften requiring multiple iterations based on synthesis feedback. In contrast,\nexisting compiler-based methods fall short in addressing complex designs. This\npaper introduces RTLRewriter, an innovative framework that leverages large\nmodels to optimize RTL code. A circuit partition pipeline is utilized for fast\nsynthesis and efficient rewriting. A multi-modal program analysis is proposed\nto incorporate vital visual diagram information as optimization cues. A\nspecialized search engine is designed to identify useful optimization guides,\nalgorithms, and code snippets that enhance the model ability to generate\noptimized RTL. Additionally, we introduce a Cost-aware Monte Carlo Tree Search\n(C-MCTS) algorithm for efficient rewriting, managing diverse retrieved contents\nand steering the rewriting results. Furthermore, a fast verification pipeline\nis proposed to reduce verification cost. To cater to the needs of both industry\nand academia, we propose two benchmarking suites: the Large Rewriter Benchmark,\ntargeting complex scenarios with extensive circuit partitioning, optimization\ntrade-offs, and verification challenges, and the Small Rewriter Benchmark,\ndesigned for a wider range of scenarios and patterns. Our comparative analysis\nwith established compilers such as Yosys and E-graph demonstrates significant\nimprovements, highlighting the benefits of integrating large models into the\nearly stages of circuit design. We provide our benchmarks at\nhttps://github.com/yaoxufeng/RTLRewriter-Bench.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AR",
    "comment": "ICCAD2024",
    "pdf_url": "http://arxiv.org/pdf/2409.11414v1",
    "published_date": "2024-09-04 09:59:37 UTC",
    "updated_date": "2024-09-04 09:59:37 UTC"
  },
  {
    "arxiv_id": "2409.02574v3",
    "title": "Solving Video Inverse Problems Using Image Diffusion Models",
    "authors": [
      "Taesung Kwon",
      "Jong Chul Ye"
    ],
    "abstract": "Recently, diffusion model-based inverse problem solvers (DIS) have emerged as\nstate-of-the-art approaches for addressing inverse problems, including image\nsuper-resolution, deblurring, inpainting, etc. However, their application to\nvideo inverse problems arising from spatio-temporal degradation remains largely\nunexplored due to the challenges in training video diffusion models. To address\nthis issue, here we introduce an innovative video inverse solver that leverages\nonly image diffusion models. Specifically, by drawing inspiration from the\nsuccess of the recent decomposed diffusion sampler (DDS), our method treats the\ntime dimension of a video as the batch dimension of image diffusion models and\nsolves spatio-temporal optimization problems within denoised spatio-temporal\nbatches derived from each image diffusion model. Moreover, we introduce a\nbatch-consistent diffusion sampling strategy that encourages consistency across\nbatches by synchronizing the stochastic noise components in image diffusion\nmodels. Our approach synergistically combines batch-consistent sampling with\nsimultaneous optimization of denoised spatio-temporal batches at each reverse\ndiffusion step, resulting in a novel and efficient diffusion sampling strategy\nfor video inverse problems. Experimental results demonstrate that our method\neffectively addresses various spatio-temporal degradations in video inverse\nproblems, achieving state-of-the-art reconstructions. Project page:\nhttps://svi-diffusion.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025; 25 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.02574v3",
    "published_date": "2024-09-04 09:48:27 UTC",
    "updated_date": "2025-02-27 09:04:08 UTC"
  },
  {
    "arxiv_id": "2409.02572v4",
    "title": "GenDFIR: Advancing Cyber Incident Timeline Analysis Through Retrieval Augmented Generation and Large Language Models",
    "authors": [
      "Fatma Yasmine Loumachi",
      "Mohamed Chahine Ghanem",
      "Mohamed Amine Ferrag"
    ],
    "abstract": "Cyber timeline analysis, or forensic timeline analysis, is crucial in Digital\nForensics and Incident Response (DFIR). It examines artefacts and events\nparticularly timestamps and metadata to detect anomalies, establish\ncorrelations, and reconstruct incident timelines. Traditional methods rely on\nstructured artefacts, such as logs and filesystem metadata, using specialised\ntools for evidence identification and feature extraction. This paper introduces\nGenDFIR, a framework leveraging large language models (LLMs), specifically\nLlama 3.1 8B in zero shot mode, integrated with a Retrieval-Augmented\nGeneration (RAG) agent. Incident data is preprocessed into a structured\nknowledge base, enabling the RAG agent to retrieve relevant events based on\nuser prompts. The LLM interprets this context, offering semantic enrichment.\nTested on synthetic data in a controlled environment, results demonstrate\nGenDFIR's reliability and robustness, showcasing LLMs potential to automate\ntimeline analysis and advance threat detection.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "24 pages V5.3",
    "pdf_url": "http://arxiv.org/pdf/2409.02572v4",
    "published_date": "2024-09-04 09:46:33 UTC",
    "updated_date": "2024-12-27 13:29:14 UTC"
  },
  {
    "arxiv_id": "2409.02569v1",
    "title": "More is More: Addition Bias in Large Language Models",
    "authors": [
      "Luca Santagata",
      "Cristiano De Nobili"
    ],
    "abstract": "In this paper, we investigate the presence of additive bias in Large Language\nModels (LLMs), drawing a parallel to the cognitive bias observed in humans\nwhere individuals tend to favor additive over subtractive changes. Using a\nseries of controlled experiments, we tested various LLMs, including GPT-3.5\nTurbo, Claude 3.5 Sonnet, Mistral, Math$\\Sigma$tral, and Llama 3.1, on tasks\ndesigned to measure their propensity for additive versus subtractive\nmodifications. Our findings demonstrate a significant preference for additive\nchanges across all tested models. For example, in a palindrome creation task,\nLlama 3.1 favored adding letters 97.85% of the time over removing them.\nSimilarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick\n76.38% of the time rather than remove one. In a text summarization task,\nMistral 7B produced longer summaries in 59.40% to 75.10% of cases when asked to\nimprove its own or others' writing. These results indicate that, similar to\nhumans, LLMs exhibit a marked additive bias, which might have implications when\nLLMs are used on a large scale. Addittive bias might increase resource use and\nenvironmental impact, leading to higher economic costs due to overconsumption\nand waste. This bias should be considered in the development and application of\nLLMs to ensure balanced and efficient problem-solving approaches.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.02569v1",
    "published_date": "2024-09-04 09:39:07 UTC",
    "updated_date": "2024-09-04 09:39:07 UTC"
  },
  {
    "arxiv_id": "2409.02561v2",
    "title": "Vision-Language Navigation with Continual Learning",
    "authors": [
      "Zhiyuan Li",
      "Yanfeng Lv",
      "Ziqin Tu",
      "Di Shang",
      "Hong Qiao"
    ],
    "abstract": "Vision-language navigation (VLN) is a critical domain within embedded\nintelligence, requiring agents to navigate 3D environments based on natural\nlanguage instructions. Traditional VLN research has focused on improving\nenvironmental understanding and decision accuracy. However, these approaches\noften exhibit a significant performance gap when agents are deployed in novel\nenvironments, mainly due to the limited diversity of training data. Expanding\ndatasets to cover a broader range of environments is impractical and costly. We\npropose the Vision-Language Navigation with Continual Learning (VLNCL) paradigm\nto address this challenge. In this paradigm, agents incrementally learn new\nenvironments while retaining previously acquired knowledge. VLNCL enables\nagents to maintain an environmental memory and extract relevant knowledge,\nallowing rapid adaptation to new environments while preserving existing\ninformation. We introduce a novel dual-loop scenario replay method (Dual-SR)\ninspired by brain memory replay mechanisms integrated with VLN agents. This\nmethod facilitates consolidating past experiences and enhances generalization\nacross new tasks. By utilizing a multi-scenario memory buffer, the agent\nefficiently organizes and replays task memories, thereby bolstering its ability\nto adapt quickly to new environments and mitigating catastrophic forgetting.\nOur work pioneers continual learning in VLN agents, introducing a novel\nexperimental setup and evaluation metrics. We demonstrate the effectiveness of\nour approach through extensive evaluations and establish a benchmark for the\nVLNCL paradigm. Comparative experiments with existing continual learning and\nVLN methods show significant improvements, achieving state-of-the-art\nperformance in continual learning ability and highlighting the potential of our\napproach in enabling rapid adaptation while preserving prior knowledge.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02561v2",
    "published_date": "2024-09-04 09:28:48 UTC",
    "updated_date": "2024-09-23 03:17:02 UTC"
  },
  {
    "arxiv_id": "2409.03795v1",
    "title": "Security Implications and Mitigation Strategies in MPLS Networks",
    "authors": [
      "Ayush Thakur"
    ],
    "abstract": "Multiprotocol Label Switching (MPLS) is a high-performance telecommunications\ntechnology that directs data from one network node to another based on short\npath labels rather than long network addresses. Its efficiency and scalability\nhave made it a popular choice for large-scale and enterprise networks. However,\nas MPLS networks grow and evolve, they encounter various security challenges.\nThis paper explores the security implications associated with MPLS networks,\nincluding risks such as label spoofing, traffic interception, and denial of\nservice attacks. Additionally, it evaluates advanced mitigation strategies to\naddress these vulnerabilities, leveraging mathematical models and security\nprotocols to enhance MPLS network resilience. By integrating theoretical\nanalysis with practical solutions, this paper aims to provide a comprehensive\nunderstanding of MPLS security and propose effective methods for safeguarding\nnetwork infrastructure.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.03795v1",
    "published_date": "2024-09-04 09:21:47 UTC",
    "updated_date": "2024-09-04 09:21:47 UTC"
  },
  {
    "arxiv_id": "2409.02555v1",
    "title": "Low-Resolution Object Recognition with Cross-Resolution Relational Contrastive Distillation",
    "authors": [
      "Kangkai Zhang",
      "Shiming Ge",
      "Ruixin Shi",
      "Dan Zeng"
    ],
    "abstract": "Recognizing objects in low-resolution images is a challenging task due to the\nlack of informative details. Recent studies have shown that knowledge\ndistillation approaches can effectively transfer knowledge from a\nhigh-resolution teacher model to a low-resolution student model by aligning\ncross-resolution representations. However, these approaches still face\nlimitations in adapting to the situation where the recognized objects exhibit\nsignificant representation discrepancies between training and testing images.\nIn this study, we propose a cross-resolution relational contrastive\ndistillation approach to facilitate low-resolution object recognition. Our\napproach enables the student model to mimic the behavior of a well-trained\nteacher model which delivers high accuracy in identifying high-resolution\nobjects. To extract sufficient knowledge, the student learning is supervised\nwith contrastive relational distillation loss, which preserves the similarities\nin various relational structures in contrastive representation space. In this\nmanner, the capability of recovering missing details of familiar low-resolution\nobjects can be effectively enhanced, leading to a better knowledge transfer.\nExtensive experiments on low-resolution object classification and\nlow-resolution face recognition clearly demonstrate the effectiveness and\nadaptability of our approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is accepted by IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)",
    "pdf_url": "http://arxiv.org/pdf/2409.02555v1",
    "published_date": "2024-09-04 09:21:13 UTC",
    "updated_date": "2024-09-04 09:21:13 UTC"
  },
  {
    "arxiv_id": "2409.02549v2",
    "title": "A Sequential Decision-Making Model for Perimeter Identification",
    "authors": [
      "Ayal Taitler"
    ],
    "abstract": "Perimeter identification involves ascertaining the boundaries of a designated\narea or zone, requiring traffic flow monitoring, control, or optimization.\nVarious methodologies and technologies exist for accurately defining these\nperimeters; however, they often necessitate specialized equipment, precise\nmapping, or comprehensive data for effective problem delineation. In this\nstudy, we propose a sequential decision-making framework for perimeter search,\ndesigned to operate efficiently in real-time and require only publicly\naccessible information. We conceptualize the perimeter search as a game between\na playing agent and an artificial environment, where the agent's objective is\nto identify the optimal perimeter by sequentially improving the current\nperimeter. We detail the model for the game and discuss its adaptability in\ndetermining the definition of an optimal perimeter. Ultimately, we showcase the\nmodel's efficacy through a real-world scenario, highlighting the identification\nof corresponding optimal perimeters.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02549v2",
    "published_date": "2024-09-04 09:11:39 UTC",
    "updated_date": "2024-09-05 06:58:38 UTC"
  },
  {
    "arxiv_id": "2409.02530v1",
    "title": "Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models",
    "authors": [
      "Chih-Yuan Li",
      "Jun-Ting Wu",
      "Chan Hsu",
      "Ming-Yen Lin",
      "Yihuang Kang"
    ],
    "abstract": "The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of\nkidney function in clinical practice. Although traditional equations and\nMachine Learning (ML) models using clinical and laboratory data can estimate\neGFR, accurately predicting future eGFR levels remains a significant challenge\nfor nephrologists and ML researchers. Recent advances demonstrate that Large\nLanguage Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust\nfoundation models for diverse applications. This study investigates the\npotential of LMMs to predict future eGFR levels with a dataset consisting of\nlaboratory and clinical values from 50 patients. By integrating various\nprompting techniques and ensembles of LMMs, our findings suggest that these\nmodels, when combined with precise prompts and visual representations of eGFR\ntrajectories, offer predictive performance comparable to existing ML models.\nThis research extends the application of foundation models and suggests avenues\nfor future studies to harness these models in addressing complex medical\nforecasting challenges.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This preprint version includes corrections of typographical errors\n  related to numerical values in Table 2, which were present in the version\n  published at the BDH workshop in MIPR 2024. These corrections do not affect\n  the overall conclusions of the study",
    "pdf_url": "http://arxiv.org/pdf/2409.02530v1",
    "published_date": "2024-09-04 08:44:36 UTC",
    "updated_date": "2024-09-04 08:44:36 UTC"
  },
  {
    "arxiv_id": "2409.02522v2",
    "title": "Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments",
    "authors": [
      "Zhiyuan Li",
      "Yanfeng Lu",
      "Yao Mu",
      "Hong Qiao"
    ],
    "abstract": "Vision Language Navigation in Continuous Environments (VLN-CE) represents a\nfrontier in embodied AI, demanding agents to navigate freely in unbounded 3D\nspaces solely guided by natural language instructions. This task introduces\ndistinct challenges in multimodal comprehension, spatial reasoning, and\ndecision-making. To address these challenges, we introduce Cog-GA, a generative\nagent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA\nemploys a dual-pronged strategy to emulate human-like cognitive processes.\nFirstly, it constructs a cognitive map, integrating temporal, spatial, and\nsemantic elements, thereby facilitating the development of spatial memory\nwithin LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints,\nstrategically optimizing the exploration trajectory to maximize navigational\nefficiency. Each waypoint is accompanied by a dual-channel scene description,\ncategorizing environmental cues into 'what' and 'where' streams as the brain.\nThis segregation enhances the agent's attentional focus, enabling it to discern\npertinent spatial information for navigation. A reflective mechanism\ncomplements these strategies by capturing feedback from prior navigation\nexperiences, facilitating continual learning and adaptive replanning. Extensive\nevaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art\nperformance and ability to simulate human-like navigation behaviors. This\nresearch significantly contributes to the development of strategic and\ninterpretable VLN-CE agents.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02522v2",
    "published_date": "2024-09-04 08:30:03 UTC",
    "updated_date": "2024-09-23 03:18:27 UTC"
  },
  {
    "arxiv_id": "2409.02512v2",
    "title": "Continual Diffuser (CoD): Mastering Continual Offline Reinforcement Learning with Experience Rehearsal",
    "authors": [
      "Jifeng Hu",
      "Li Shen",
      "Sili Huang",
      "Zhejian Yang",
      "Hechang Chen",
      "Lichao Sun",
      "Yi Chang",
      "Dacheng Tao"
    ],
    "abstract": "Artificial neural networks, especially recent diffusion-based models, have\nshown remarkable superiority in gaming, control, and QA systems, where the\ntraining tasks' datasets are usually static. However, in real-world\napplications, such as robotic control of reinforcement learning (RL), the tasks\nare changing, and new tasks arise in a sequential order. This situation poses\nthe new challenge of plasticity-stability trade-off for training an agent who\ncan adapt to task changes and retain acquired knowledge. In view of this, we\npropose a rehearsal-based continual diffusion model, called Continual Diffuser\n(CoD), to endow the diffuser with the capabilities of quick adaptation\n(plasticity) and lasting retention (stability). Specifically, we first\nconstruct an offline benchmark that contains 90 tasks from multiple domains.\nThen, we train the CoD on each task with sequential modeling and conditional\ngeneration for making decisions. Next, we preserve a small portion of previous\ndatasets as the rehearsal buffer and replay it to retain the acquired\nknowledge. Extensive experiments on a series of tasks show CoD can achieve a\npromising plasticity-stability trade-off and outperform existing\ndiffusion-based methods and other representative baselines on most tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2409.02512v2",
    "published_date": "2024-09-04 08:21:47 UTC",
    "updated_date": "2025-01-15 03:23:39 UTC"
  },
  {
    "arxiv_id": "2409.07486v2",
    "title": "MarS: a Financial Market Simulation Engine Powered by Generative Foundation Model",
    "authors": [
      "Junjie Li",
      "Yang Liu",
      "Weiqing Liu",
      "Shikai Fang",
      "Lewen Wang",
      "Chang Xu",
      "Jiang Bian"
    ],
    "abstract": "Generative models aim to simulate realistic effects of various actions across\ndifferent contexts, from text generation to visual effects. Despite significant\nefforts to build real-world simulators, the application of generative models to\nvirtual worlds, like financial markets, remains under-explored. In financial\nmarkets, generative models can simulate complex market effects of participants\nwith various behaviors, enabling interaction under different market conditions,\nand training strategies without financial risk. This simulation relies on the\nfinest structured data in financial market like orders thus building the finest\nrealistic simulation. We propose Large Market Model (LMM), an order-level\ngenerative foundation model, for financial market simulation, akin to language\nmodeling in the digital world. Our financial Market Simulation engine (MarS),\npowered by LMM, addresses the domain-specific need for realistic, interactive\nand controllable order generation. Key observations include LMM's strong\nscalability across data size and model complexity, and MarS's robust and\npracticable realism in controlled generation with market impact. We showcase\nMarS as a forecast tool, detection system, analysis platform, and agent\ntraining environment, thus demonstrating MarS's \"paradigm shift\" potential for\na variety of financial applications. We release the code of MarS at\nhttps://github.com/microsoft/MarS/.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "q-fin.TR"
    ],
    "primary_category": "q-fin.CP",
    "comment": "35 pages, 26 figures, ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.07486v2",
    "published_date": "2024-09-04 08:16:22 UTC",
    "updated_date": "2025-03-13 09:26:41 UTC"
  },
  {
    "arxiv_id": "2409.02495v1",
    "title": "CoAst: Validation-Free Contribution Assessment for Federated Learning based on Cross-Round Valuation",
    "authors": [
      "Hao Wu",
      "Likun Zhang",
      "Shucheng Li",
      "Fengyuan Xu",
      "Sheng Zhong"
    ],
    "abstract": "In the federated learning (FL) process, since the data held by each\nparticipant is different, it is necessary to figure out which participant has a\nhigher contribution to the model performance. Effective contribution assessment\ncan help motivate data owners to participate in the FL training. Research works\nin this field can be divided into two directions based on whether a validation\ndataset is required. Validation-based methods need to use representative\nvalidation data to measure the model accuracy, which is difficult to obtain in\npractical FL scenarios. Existing validation-free methods assess the\ncontribution based on the parameters and gradients of local models and the\nglobal model in a single training round, which is easily compromised by the\nstochasticity of model training. In this work, we propose CoAst, a practical\nmethod to assess the FL participants' contribution without access to any\nvalidation data. The core idea of CoAst involves two aspects: one is to only\ncount the most important part of model parameters through a weights\nquantization, and the other is a cross-round valuation based on the similarity\nbetween the current local parameters and the global parameter updates in\nseveral subsequent communication rounds. Extensive experiments show that CoAst\nhas comparable assessment reliability to existing validation-based methods and\noutperforms existing validation-free methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02495v1",
    "published_date": "2024-09-04 07:46:28 UTC",
    "updated_date": "2024-09-04 07:46:28 UTC"
  },
  {
    "arxiv_id": "2409.02489v2",
    "title": "NeuroSpex: Neuro-Guided Speaker Extraction with Cross-Modal Attention",
    "authors": [
      "Dashanka De Silva",
      "Siqi Cai",
      "Saurav Pahuja",
      "Tanja Schultz",
      "Haizhou Li"
    ],
    "abstract": "In the study of auditory attention, it has been revealed that there exists a\nrobust correlation between attended speech and elicited neural responses,\nmeasurable through electroencephalography (EEG). Therefore, it is possible to\nuse the attention information available within EEG signals to guide the\nextraction of the target speaker in a cocktail party computationally. In this\npaper, we present a neuro-guided speaker extraction model, i.e. NeuroSpex,\nusing the EEG response of the listener as the sole auxiliary reference cue to\nextract attended speech from monaural speech mixtures. We propose a novel EEG\nsignal encoder that captures the attention information. Additionally, we\npropose a cross-attention (CA) mechanism to enhance the speech feature\nrepresentations, generating a speaker extraction mask. Experimental results on\na publicly available dataset demonstrate that our proposed model outperforms\ntwo baseline models across various evaluation metrics.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02489v2",
    "published_date": "2024-09-04 07:33:01 UTC",
    "updated_date": "2024-09-16 06:35:07 UTC"
  },
  {
    "arxiv_id": "2409.02486v1",
    "title": "Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization",
    "authors": [
      "Cho-Ying Wu",
      "Yiqi Zhong",
      "Junying Wang",
      "Ulrich Neumann"
    ],
    "abstract": "Indoor robots rely on depth to perform tasks like navigation or obstacle\ndetection, and single-image depth estimation is widely used to assist\nperception. Most indoor single-image depth prediction focuses less on model\ngeneralizability to unseen datasets, concerned with in-the-wild robustness for\nsystem deployment. This work leverages gradient-based meta-learning to gain\nhigher generalizability on zero-shot cross-dataset inference. Unlike the\nmost-studied meta-learning of image classification associated with explicit\nclass labels, no explicit task boundaries exist for continuous depth values\ntied to highly varying indoor environments regarding object arrangement and\nscene composition. We propose fine-grained task that treats each RGB-D\nmini-batch as a task in our meta-learning formulation. We first show that our\nmethod on limited data induces a much better prior (max 27.8% in RMSE). Then,\nfinetuning on meta-learned initialization consistently outperforms baselines\nwithout the meta approach. Aiming at generalization, we propose zero-shot\ncross-dataset protocols and validate higher generalizability induced by our\nmeta-initialization, as a simple and useful plugin to many existing depth\nestimation methods. The work at the intersection of depth and meta-learning\npotentially drives both research to step closer to practical robotic and\nmachine perception usage.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IROS 2024. The version supersedes 2305.07269. arXiv admin note: text\n  overlap with arXiv:2305.07269",
    "pdf_url": "http://arxiv.org/pdf/2409.02486v1",
    "published_date": "2024-09-04 07:25:50 UTC",
    "updated_date": "2024-09-04 07:25:50 UTC"
  },
  {
    "arxiv_id": "2409.02485v2",
    "title": "Adversarial Attacks on Machine Learning-Aided Visualizations",
    "authors": [
      "Takanori Fujiwara",
      "Kostiantyn Kucher",
      "Junpeng Wang",
      "Rafael M. Martins",
      "Andreas Kerren",
      "Anders Ynnerman"
    ],
    "abstract": "Research in ML4VIS investigates how to use machine learning (ML) techniques\nto generate visualizations, and the field is rapidly growing with high societal\nimpact. However, as with any computational pipeline that employs ML processes,\nML4VIS approaches are susceptible to a range of ML-specific adversarial\nattacks. These attacks can manipulate visualization generations, causing\nanalysts to be tricked and their judgments to be impaired. Due to a lack of\nsynthesis from both visualization and ML perspectives, this security aspect is\nlargely overlooked by the current ML4VIS literature. To bridge this gap, we\ninvestigate the potential vulnerabilities of ML-aided visualizations from\nadversarial attacks using a holistic lens of both visualization and ML\nperspectives. We first identify the attack surface (i.e., attack entry points)\nthat is unique in ML-aided visualizations. We then exemplify five different\nadversarial attacks. These examples highlight the range of possible attacks\nwhen considering the attack surface and multiple different adversary\ncapabilities. Our results show that adversaries can induce various attacks,\nsuch as creating arbitrary and deceptive visualizations, by systematically\nidentifying input attributes that are influential in ML inferences. Based on\nour observations of the attack surface characteristics and the attack examples,\nwe underline the importance of comprehensive studies of security issues and\ndefense mechanisms as a call of urgency for the ML4VIS community.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CR",
    "comment": "This version of the article has been accepted for publication, after\n  peer review (when applicable) but is not the Version of Record and does not\n  reflect post-acceptance improvements, or any corrections. The Version of\n  Record is available online at: http://dx.doi.org/10.1007/s12650-024-01029-2",
    "pdf_url": "http://arxiv.org/pdf/2409.02485v2",
    "published_date": "2024-09-04 07:23:12 UTC",
    "updated_date": "2024-09-24 13:58:37 UTC"
  },
  {
    "arxiv_id": "2409.02483v5",
    "title": "TASAR: Transfer-based Attack on Skeletal Action Recognition",
    "authors": [
      "Yunfeng Diao",
      "Baiqi Wu",
      "Ruixuan Zhang",
      "Ajian Liu",
      "Xiaoshuai Hao",
      "Xingxing Wei",
      "Meng Wang",
      "He Wang"
    ],
    "abstract": "Skeletal sequence data, as a widely employed representation of human actions,\nare crucial in Human Activity Recognition (HAR). Recently, adversarial attacks\nhave been proposed in this area, which exposes potential security concerns, and\nmore importantly provides a good tool for model robustness test. Within this\nresearch, transfer-based attack is an important tool as it mimics the\nreal-world scenario where an attacker has no knowledge of the target model, but\nis under-explored in Skeleton-based HAR (S-HAR). Consequently, existing S-HAR\nattacks exhibit weak adversarial transferability and the reason remains largely\nunknown. In this paper, we investigate this phenomenon via the characterization\nof the loss function. We find that one prominent indicator of poor\ntransferability is the low smoothness of the loss function. Led by this\nobservation, we improve the transferability by properly smoothening the loss\nwhen computing the adversarial examples. This leads to the first Transfer-based\nAttack on Skeletal Action Recognition, TASAR. TASAR explores the smoothened\nmodel posterior of pre-trained surrogates, which is achieved by a new\npost-train Dual Bayesian optimization strategy. Furthermore, unlike existing\ntransfer-based methods which overlook the temporal coherence within sequences,\nTASAR incorporates motion dynamics into the Bayesian attack, effectively\ndisrupting the spatial-temporal coherence of S-HARs. For exhaustive evaluation,\nwe build the first large-scale robust S-HAR benchmark, comprising 7 S-HAR\nmodels, 10 attack methods, 3 S-HAR datasets and 2 defense models. Extensive\nresults demonstrate the superiority of TASAR. Our benchmark enables easy\ncomparisons for future studies, with the code available in the\nhttps://github.com/yunfengdiao/Skeleton-Robustness-Benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.02483v5",
    "published_date": "2024-09-04 07:20:01 UTC",
    "updated_date": "2025-02-12 09:39:06 UTC"
  },
  {
    "arxiv_id": "2409.02451v1",
    "title": "Fast, High-Quality and Parameter-Efficient Articulatory Synthesis using Differentiable DSP",
    "authors": [
      "Yisi Liu",
      "Bohan Yu",
      "Drake Lin",
      "Peter Wu",
      "Cheol Jun Cho",
      "Gopala Krishna Anumanchipalli"
    ],
    "abstract": "Articulatory trajectories like electromagnetic articulography (EMA) provide a\nlow-dimensional representation of the vocal tract filter and have been used as\nnatural, grounded features for speech synthesis. Differentiable digital signal\nprocessing (DDSP) is a parameter-efficient framework for audio synthesis.\nTherefore, integrating low-dimensional EMA features with DDSP can significantly\nenhance the computational efficiency of speech synthesis. In this paper, we\npropose a fast, high-quality, and parameter-efficient DDSP articulatory vocoder\nthat can synthesize speech from EMA, F0, and loudness. We incorporate several\ntechniques to solve the harmonics / noise imbalance problem, and add a\nmulti-resolution adversarial loss for better synthesis quality. Our model\nachieves a transcription word error rate (WER) of 6.67% and a mean opinion\nscore (MOS) of 3.74, with an improvement of 1.63% and 0.16 compared to the\nstate-of-the-art (SOTA) baseline. Our DDSP vocoder is 4.9x faster than the\nbaseline on CPU during inference, and can generate speech of comparable quality\nwith only 0.4M parameters, in contrast to the 9M parameters required by the\nSOTA.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "accepted for Spoken Language Technology Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.02451v1",
    "published_date": "2024-09-04 05:12:15 UTC",
    "updated_date": "2024-09-04 05:12:15 UTC"
  },
  {
    "arxiv_id": "2409.02449v4",
    "title": "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations",
    "authors": [
      "Kavya Manohar",
      "Leena G Pillai",
      "Elizabeth Sherly"
    ],
    "abstract": "This paper explores the pitfalls in evaluating multilingual automatic speech\nrecognition (ASR) models, with a particular focus on Indic language scripts. We\ninvestigate the text normalization routine employed by leading ASR models,\nincluding OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,\nand their unintended consequences on performance metrics. Our research reveals\nthat current text normalization practices, while aiming to standardize ASR\noutputs for fair comparison, by removing inconsistencies such as variations in\nspelling, punctuation, and special characters, are fundamentally flawed when\napplied to Indic scripts. Through empirical analysis using text similarity\nscores and in-depth linguistic examination, we demonstrate that these flaws\nlead to artificially improved performance metrics for Indic languages. We\nconclude by proposing a shift towards developing text normalization routines\nthat leverage native linguistic expertise, ensuring more robust and accurate\nevaluations of multilingual ASR models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "68T50, 91F20, 68T10",
      "I.2.1; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2409.02449v4",
    "published_date": "2024-09-04 05:08:23 UTC",
    "updated_date": "2024-11-09 06:37:01 UTC"
  },
  {
    "arxiv_id": "2409.02448v1",
    "title": "Detecting Korean Food Using Image using Hierarchical Model",
    "authors": [
      "Hoang Khanh Lam",
      "Kahandakanaththage Maduni Pramuditha Perera"
    ],
    "abstract": "A solution was made available for Korean Food lovers who have dietary\nrestrictions to identify the Korean food before consuming. Just by uploading a\nclear photo of the dish, people can get to know what they are eating. Image\nprocessing techniques together with machine learning helped to come up with\nthis solution.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02448v1",
    "published_date": "2024-09-04 05:06:34 UTC",
    "updated_date": "2024-09-04 05:06:34 UTC"
  },
  {
    "arxiv_id": "2409.02428v3",
    "title": "Large Language Models as Efficient Reward Function Searchers for Custom-Environment Multi-Objective Reinforcement Learning",
    "authors": [
      "Guanwen Xie",
      "Jingzehua Xu",
      "Yiyuan Yang",
      "Yimian Ding",
      "Shuai Zhang"
    ],
    "abstract": "Achieving the effective design and improvement of reward functions in\nreinforcement learning (RL) tasks with complex custom environments and multiple\nrequirements presents considerable challenges. In this paper, we propose ERFSL,\nan efficient reward function searcher using LLMs, which enables LLMs to be\neffective white-box searchers and highlights their advanced semantic\nunderstanding capabilities. Specifically, we generate reward components for\neach numerically explicit user requirement and employ a reward critic to\nidentify the correct code form. Then, LLMs assign weights to the reward\ncomponents to balance their values and iteratively adjust the weights without\nambiguity and redundant adjustments by flexibly adopting directional mutation\nand crossover strategies, similar to genetic algorithms, based on the context\nprovided by the training log analyzer. We applied the framework to an\nunderwater data collection RL task without direct human feedback or reward\nexamples (zero-shot learning). The reward critic successfully corrects the\nreward code with only one feedback instance for each requirement, effectively\npreventing unrectifiable errors. The initialization of weights enables the\nacquisition of different reward functions within the Pareto solution set\nwithout the need for weight search. Even in cases where a weight is 500 times\noff, on average, only 5.2 iterations are needed to meet user requirements. The\nERFSL also works well with most prompts utilizing GPT-4o mini, as we decompose\nthe weight searching process to reduce the requirement for numerical and\nlong-context understanding capabilities",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02428v3",
    "published_date": "2024-09-04 04:15:14 UTC",
    "updated_date": "2024-11-01 03:47:51 UTC"
  },
  {
    "arxiv_id": "2409.13697v1",
    "title": "Prompt Baking",
    "authors": [
      "Aman Bhargava",
      "Cameron Witkowski",
      "Alexander Detkov",
      "Matt Thomson"
    ],
    "abstract": "Two primary ways to change LLM behavior are prompting and weight updates\n(e.g., fine-tuning). Prompting LLMs is simple and effective, specifying the\ndesired changes explicitly in natural language, whereas weight updates provide\nmore expressive and permanent behavior changes, specified implicitly via\ntraining on large datasets. We present a technique for \"baking\" prompts into\nthe weights of an LLM. Prompt Baking converts a prompt $u$ and initial weights\n$\\theta$ to a new set of weights $\\theta_u$ such that new \"baked\" LLM behaves\nlike the original prompted LLM. Mathematically, we minimize the KL divergence\nbetween $P_\\theta(\\cdot | u)$ and $P_{\\theta_u}(\\cdot)$, where $P$ is the LLM's\nprobability distribution over token sequences. Across all our experiments, we\nfind prompts can be readily baked into weight updates. Baking chain-of-thought\nprompts improves zero-shot performance on GSM8K, ASDiv, MBPP, ARC-Easy,\nARC-Challenge, and CommonsenseQA benchmarks. Baking news headlines directly\nupdates an LLM's knowledge. And baking instructions & personas alleviates\n\"prompt forgetting\" over long sequences. Furthermore, stopping baking early\ncreates \"half-baked\" models, continuously scaling prompt strength. Baked models\nretain their sensitivity to further prompting and baking, including\nre-prompting with the baked-in prompt. Surprisingly, the re-prompted models\nyield further performance gains in instruction following, as well as math\nreasoning and coding benchmarks. Taking re-prompting and re-baking to the limit\nyields a form of iterative self-improvement we call Prompt Pursuit, and\npreliminary results on instruction following exhibit dramatic performance\ngains. Finally, we discuss implications for AI safety, continuous model\nupdating, enhancing real-time learning capabilities in LLM-based agents, and\ngenerating more stable AI personas.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13697v1",
    "published_date": "2024-09-04 04:13:16 UTC",
    "updated_date": "2024-09-04 04:13:16 UTC"
  },
  {
    "arxiv_id": "2409.02423v1",
    "title": "Accelerating Large Language Model Training with Hybrid GPU-based Compression",
    "authors": [
      "Lang Xu",
      "Quentin Anthony",
      "Qinghua Zhou",
      "Nawras Alnaasan",
      "Radha R. Gulhane",
      "Aamir Shafi",
      "Hari Subramoni",
      "Dhabaleswar K. Panda"
    ],
    "abstract": "Data Parallelism (DP), Tensor Parallelism (TP), and Pipeline Parallelism (PP)\nare the three strategies widely adopted to enable fast and efficient Large\nLanguage Model (LLM) training. However, these approaches rely on data-intensive\ncommunication routines to collect, aggregate, and re-distribute gradients,\nactivations, and other important model information, which pose significant\noverhead. Co-designed with GPU-based compression libraries, MPI libraries have\nbeen proven to reduce message size significantly, and leverage interconnect\nbandwidth, thus increasing training efficiency while maintaining acceptable\naccuracy.\n  In this work, we investigate the efficacy of compression-assisted MPI\ncollectives under the context of distributed LLM training using 3D parallelism\nand ZeRO optimizations. We scaled up to 192 V100 GPUs on the Lassen\nsupercomputer. First, we enabled a na\\\"ive compression scheme across all\ncollectives and observed a 22.5\\% increase in TFLOPS per GPU and a 23.6\\%\nincrease in samples per second for GPT-NeoX-20B training. Nonetheless, such a\nstrategy ignores the sparsity discrepancy among messages communicated in each\nparallelism degree, thus introducing more errors and causing degradation in\ntraining loss. Therefore, we incorporated hybrid compression settings toward\neach parallel dimension and adjusted the compression intensity accordingly.\nGiven their low-rank structure (arXiv:2301.02654), we apply aggressive\ncompression on gradients when performing DP All-reduce. We adopt milder\ncompression to preserve precision while communicating activations, optimizer\nstates, and model parameters in TP and PP. Using the adjusted hybrid\ncompression scheme, we demonstrate a 17.3\\% increase in TFLOPS per GPU and a\n12.7\\% increase in samples per second while reaching baseline loss convergence.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02423v1",
    "published_date": "2024-09-04 04:05:30 UTC",
    "updated_date": "2024-09-04 04:05:30 UTC"
  },
  {
    "arxiv_id": "2409.02413v1",
    "title": "Abstractive Text Summarization: State of the Art, Challenges, and Improvements",
    "authors": [
      "Hassan Shakil",
      "Ahmad Farooq",
      "Jugal Kalita"
    ],
    "abstract": "Specifically focusing on the landscape of abstractive text summarization, as\nopposed to extractive techniques, this survey presents a comprehensive\noverview, delving into state-of-the-art techniques, prevailing challenges, and\nprospective research directions. We categorize the techniques into traditional\nsequence-to-sequence models, pre-trained large language models, reinforcement\nlearning, hierarchical methods, and multi-modal summarization. Unlike prior\nworks that did not examine complexities, scalability and comparisons of\ntechniques in detail, this review takes a comprehensive approach encompassing\nstate-of-the-art methods, challenges, solutions, comparisons, limitations and\ncharts out future improvements - providing researchers an extensive overview to\nadvance abstractive summarization research. We provide vital comparison tables\nacross techniques categorized - offering insights into model complexity,\nscalability and appropriate applications. The paper highlights challenges such\nas inadequate meaning representation, factual consistency, controllable text\nsummarization, cross-lingual summarization, and evaluation metrics, among\nothers. Solutions leveraging knowledge incorporation and other innovative\nstrategies are proposed to address these challenges. The paper concludes by\nhighlighting emerging research areas like factual inconsistency,\ndomain-specific, cross-lingual, multilingual, and long-document summarization,\nas well as handling noisy data. Our objective is to provide researchers and\npractitioners with a structured overview of the domain, enabling them to better\nunderstand the current landscape and identify potential areas for further\nresearch and improvement.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 Tables, 7 Figures",
    "pdf_url": "http://arxiv.org/pdf/2409.02413v1",
    "published_date": "2024-09-04 03:39:23 UTC",
    "updated_date": "2024-09-04 03:39:23 UTC"
  },
  {
    "arxiv_id": "2409.12924v4",
    "title": "Wavelet GPT: Wavelet Inspired Large Language Models",
    "authors": [
      "Prateek Verma"
    ],
    "abstract": "Large Language Models (LLMs) have ushered in a new wave of artificial\nintelligence advancements impacting every scientific field and discipline. We\nlive in a world where most of the data around us, e.g., text, audio, and music,\nhas a multi-scale structure. This paper infuses LLMs with a traditional signal\nprocessing idea, namely wavelets, during pre-training to take advantage of the\nstructure. Without adding \\textbf{any extra parameters} to a GPT-style LLM\narchitecture in an academic setup, we achieve the same pre-training performance\nalmost twice as fast in text, audio, and images. This is done by imposing a\nstructure on intermediate embeddings. When trained for the same number of\ntraining steps, we achieve significant gains in performance, which is\ncomparable to pre-training a larger neural architecture. Further, we show this\nextends to the Long Range Arena benchmark and several input representations\nsuch as characters, BPE tokens, bytes, waveform, math expression, and image\npixels. Our architecture allows every next token prediction access to\nintermediate embeddings at different temporal resolutions in every decoder\nblock. We hope this will pave the way for incorporating multi-rate signal\nprocessing into pre-training.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "eess.SP",
    "comment": "12 pages, 4 figures;",
    "pdf_url": "http://arxiv.org/pdf/2409.12924v4",
    "published_date": "2024-09-04 03:17:19 UTC",
    "updated_date": "2025-02-09 23:09:31 UTC"
  },
  {
    "arxiv_id": "2409.02404v1",
    "title": "Learning Privacy-Preserving Student Networks via Discriminative-Generative Distillation",
    "authors": [
      "Shiming Ge",
      "Bochao Liu",
      "Pengju Wang",
      "Yong Li",
      "Dan Zeng"
    ],
    "abstract": "While deep models have proved successful in learning rich knowledge from\nmassive well-annotated data, they may pose a privacy leakage risk in practical\ndeployment. It is necessary to find an effective trade-off between high utility\nand strong privacy. In this work, we propose a discriminative-generative\ndistillation approach to learn privacy-preserving deep models. Our key idea is\ntaking models as bridge to distill knowledge from private data and then\ntransfer it to learn a student network via two streams. First, discriminative\nstream trains a baseline classifier on private data and an ensemble of teachers\non multiple disjoint private subsets, respectively. Then, generative stream\ntakes the classifier as a fixed discriminator and trains a generator in a\ndata-free manner. After that, the generator is used to generate massive\nsynthetic data which are further applied to train a variational autoencoder\n(VAE). Among these synthetic data, a few of them are fed into the teacher\nensemble to query labels via differentially private aggregation, while most of\nthem are embedded to the trained VAE for reconstructing synthetic data.\nFinally, a semi-supervised student learning is performed to simultaneously\nhandle two tasks: knowledge transfer from the teachers with distillation on few\nprivately labeled synthetic data, and knowledge enhancement with tangent-normal\nadversarial regularization on many triples of reconstructed synthetic data. In\nthis way, our approach can control query cost over private data and mitigate\naccuracy degradation in a unified manner, leading to a privacy-preserving\nstudent model. Extensive experiments and analysis clearly show the\neffectiveness of the proposed approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is accepted by IEEE Transactions on Image Processing (TIP)",
    "pdf_url": "http://arxiv.org/pdf/2409.02404v1",
    "published_date": "2024-09-04 03:06:13 UTC",
    "updated_date": "2024-09-04 03:06:13 UTC"
  },
  {
    "arxiv_id": "2409.02391v2",
    "title": "Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Translation",
    "authors": [
      "Ali Merali"
    ],
    "abstract": "This paper derives \"scaling laws\"--empirical relationships between the\ntraining compute of Large Language Models (LLMs) and their performance--for\neconomic outcomes. In a preregistered online experiment, 300 professional\ntranslators completed 1,800 tasks using one of 13 LLMs (or a control). A\ntenfold increase in model compute improved task completion speed by 12.3%,\ngrades by 0.18 standard deviations, and earnings per minute by 16.1%. Gains\nwere four times larger for lower-skilled workers. These findings suggest\ncontinued model scaling could boost U.S. productivity by at least 6.9% over the\nnext decade.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02391v2",
    "published_date": "2024-09-04 02:39:31 UTC",
    "updated_date": "2024-12-07 08:56:53 UTC"
  },
  {
    "arxiv_id": "2409.02390v1",
    "title": "Neural Dynamics Model of Visual Decision-Making: Learning from Human Experts",
    "authors": [
      "Jie Su",
      "Fang Cai",
      "Shu-Kuo Zhao",
      "Xin-Yi Wang",
      "Tian-Yi Qian",
      "Da-Hui Wang",
      "Bo Hong"
    ],
    "abstract": "Uncovering the fundamental neural correlates of biological intelligence,\ndeveloping mathematical models, and conducting computational simulations are\ncritical for advancing new paradigms in artificial intelligence (AI). In this\nstudy, we implemented a comprehensive visual decision-making model that spans\nfrom visual input to behavioral output, using a neural dynamics modeling\napproach. Drawing inspiration from the key components of the dorsal visual\npathway in primates, our model not only aligns closely with human behavior but\nalso reflects neural activities in primates, and achieving accuracy comparable\nto convolutional neural networks (CNNs). Moreover, magnetic resonance imaging\n(MRI) identified key neuroimaging features such as structural connections and\nfunctional connectivity that are associated with performance in perceptual\ndecision-making tasks. A neuroimaging-informed fine-tuning approach was\nintroduced and applied to the model, leading to performance improvements that\nparalleled the behavioral variations observed among subjects. Compared to\nclassical deep learning models, our model more accurately replicates the\nbehavioral performance of biological intelligence, relying on the structural\ncharacteristics of biological neural networks rather than extensive training\ndata, and demonstrating enhanced resilience to perturbation.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02390v1",
    "published_date": "2024-09-04 02:38:52 UTC",
    "updated_date": "2024-09-04 02:38:52 UTC"
  },
  {
    "arxiv_id": "2409.02389v2",
    "title": "Multi-modal Situated Reasoning in 3D Scenes",
    "authors": [
      "Xiongkun Linghu",
      "Jiangyong Huang",
      "Xuesong Niu",
      "Xiaojian Ma",
      "Baoxiong Jia",
      "Siyuan Huang"
    ],
    "abstract": "Situation awareness is essential for understanding and reasoning about 3D\nscenes in embodied AI agents. However, existing datasets and benchmarks for\nsituated understanding are limited in data modality, diversity, scale, and task\nscope. To address these limitations, we propose Multi-modal Situated Question\nAnswering (MSQA), a large-scale multi-modal situated reasoning dataset,\nscalably collected leveraging 3D scene graphs and vision-language models (VLMs)\nacross a diverse range of real-world 3D scenes. MSQA includes 251K situated\nquestion-answering pairs across 9 distinct question categories, covering\ncomplex scenarios within 3D scenes. We introduce a novel interleaved\nmulti-modal input setting in our benchmark to provide text, image, and point\ncloud for situation and question description, resolving ambiguity in previous\nsingle-modality convention (e.g., text). Additionally, we devise the\nMulti-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models'\nsituated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN\nhighlight the limitations of existing vision-language models and underscore the\nimportance of handling multi-modal interleaved inputs and situation modeling.\nExperiments on data scaling and cross-domain transfer further demonstrate the\nefficacy of leveraging MSQA as a pre-training dataset for developing more\npowerful situated reasoning models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2024 Datasets and Benchmarks Track. Project page:\n  https://msr3d.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2409.02389v2",
    "published_date": "2024-09-04 02:37:38 UTC",
    "updated_date": "2024-11-18 02:32:22 UTC"
  },
  {
    "arxiv_id": "2409.02387v6",
    "title": "Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges",
    "authors": [
      "Qian Niu",
      "Junyu Liu",
      "Ziqian Bi",
      "Pohsun Feng",
      "Benji Peng",
      "Keyu Chen",
      "Ming Li",
      "Lawrence KQ Yan",
      "Yichao Zhang",
      "Caitlyn Heqi Yin",
      "Cheng Fei",
      "Tianyang Wang",
      "Yunze Wang",
      "Silin Chen",
      "Ming Liu"
    ],
    "abstract": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.02387v6",
    "published_date": "2024-09-04 02:30:12 UTC",
    "updated_date": "2024-12-11 04:12:39 UTC"
  },
  {
    "arxiv_id": "2409.15292v1",
    "title": "SketcherX: AI-Driven Interactive Robotic drawing with Diffusion model and Vectorization Techniques",
    "authors": [
      "Jookyung Song",
      "Mookyoung Kang",
      "Nojun Kwak"
    ],
    "abstract": "We introduce SketcherX, a novel robotic system for personalized portrait\ndrawing through interactive human-robot engagement. Unlike traditional robotic\nart systems that rely on analog printing techniques, SketcherX captures and\nprocesses facial images to produce vectorized drawings in a distinctive,\nhuman-like artistic style. The system comprises two 6-axis robotic arms : a\nface robot, which is equipped with a head-mounted camera and Large Language\nModel (LLM) for real-time interaction, and a drawing robot, utilizing a\nfine-tuned Stable Diffusion model, ControlNet, and Vision-Language models for\ndynamic, stylized drawing. Our contributions include the development of a\ncustom Vector Low Rank Adaptation model (LoRA), enabling seamless adaptation to\nvarious artistic styles, and integrating a pair-wise fine-tuning approach to\nenhance stroke quality and stylistic accuracy. Experimental results demonstrate\nthe system's ability to produce high-quality, personalized portraits within two\nminutes, highlighting its potential as a new paradigm in robotic creativity.\nThis work advances the field of robotic art by positioning robots as active\nparticipants in the creative process, paving the way for future explorations in\ninteractive, human-robot artistic collaboration.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.15292v1",
    "published_date": "2024-09-04 02:20:22 UTC",
    "updated_date": "2024-09-04 02:20:22 UTC"
  },
  {
    "arxiv_id": "2409.02376v1",
    "title": "Coral Model Generation from Single Images for Virtual Reality Applications",
    "authors": [
      "Jie Fu",
      "Shun Fu",
      "Mick Grierson"
    ],
    "abstract": "With the rapid development of VR technology, the demand for high-quality 3D\nmodels is increasing. Traditional methods struggle with efficiency and quality\nin large-scale customization. This paper introduces a deep-learning framework\nthat generates high-precision 3D coral models from a single image. Using the\nCoral dataset, the framework extracts geometric and texture features, performs\n3D reconstruction, and optimizes design and material blending. Advanced\noptimization and polygon count control ensure shape accuracy, detail retention,\nand flexible output for various complexities, catering to high-quality\nrendering and real-time interaction needs.The project incorporates Explainable\nAI (XAI) to transform AI-generated models into interactive \"artworks,\" best\nviewed in VR and XR. This enhances model interpretability and human-machine\ncollaboration. Real-time feedback in VR interactions displays information like\ncoral species and habitat, enriching user experience. The generated models\nsurpass traditional methods in detail, visual quality, and efficiency. This\nresearch offers an intelligent approach to 3D content creation for VR, lowering\nproduction barriers, and promoting widespread VR applications. Additionally,\nintegrating XAI provides new insights into AI-generated visual content and\nadvances research in 3D vision interpretability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "In Proceedings of Explainable AI for the Arts Workshop 2024 (XAIxArts\n  2024) arXiv:2406.14485",
    "pdf_url": "http://arxiv.org/pdf/2409.02376v1",
    "published_date": "2024-09-04 01:54:20 UTC",
    "updated_date": "2024-09-04 01:54:20 UTC"
  },
  {
    "arxiv_id": "2409.02370v4",
    "title": "Do Large Language Models Possess Sensitive to Sentiment?",
    "authors": [
      "Yang Liu",
      "Xichou Zhu",
      "Zhou Shen",
      "Yi Liu",
      "Min Li",
      "Yujun Chen",
      "Benzi John",
      "Zhenzhen Ma",
      "Tao Hu",
      "Zhi Li",
      "Zhiyang Xu",
      "Wei Luo",
      "Junhui Wang"
    ],
    "abstract": "Large Language Models (LLMs) have recently displayed their extraordinary\ncapabilities in language understanding. However, how to comprehensively assess\nthe sentiment capabilities of LLMs continues to be a challenge. This paper\ninvestigates the ability of LLMs to detect and react to sentiment in text\nmodal. As the integration of LLMs into diverse applications is on the rise, it\nbecomes highly critical to comprehend their sensitivity to emotional tone, as\nit can influence the user experience and the efficacy of sentiment-driven\ntasks. We conduct a series of experiments to evaluate the performance of\nseveral prominent LLMs in identifying and responding appropriately to\nsentiments like positive, negative, and neutral emotions. The models' outputs\nare analyzed across various sentiment benchmarks, and their responses are\ncompared with human evaluations. Our discoveries indicate that although LLMs\nshow a basic sensitivity to sentiment, there are substantial variations in\ntheir accuracy and consistency, emphasizing the requirement for further\nenhancements in their training processes to better capture subtle emotional\ncues. Take an example in our findings, in some cases, the models might wrongly\nclassify a strongly positive sentiment as neutral, or fail to recognize sarcasm\nor irony in the text. Such misclassifications highlight the complexity of\nsentiment analysis and the areas where the models need to be refined. Another\naspect is that different LLMs might perform differently on the same set of\ndata, depending on their architecture and training datasets. This variance\ncalls for a more in-depth study of the factors that contribute to the\nperformance differences and how they can be optimized.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.02370v4",
    "published_date": "2024-09-04 01:40:20 UTC",
    "updated_date": "2025-02-14 10:04:55 UTC"
  },
  {
    "arxiv_id": "2409.02343v1",
    "title": "NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for Retrieval",
    "authors": [
      "Sepanta Zeighami",
      "Zac Wellmer",
      "Aditya Parameswaran"
    ],
    "abstract": "$k$-Nearest Neighbor search on dense vector embeddings ($k$-NN retrieval)\nfrom pre-trained embedding models is the predominant retrieval method for text\nand images, as well as Retrieval-Augmented Generation (RAG) pipelines. In\npractice, application developers often fine-tune the embeddings to improve\ntheir accuracy on the dataset and query workload in hand. Existing approaches\neither fine-tune the pre-trained model itself or, more efficiently, but at the\ncost of accuracy, train adaptor models to transform the output of the\npre-trained model. We present NUDGE, a family of novel non-parametric embedding\nfine-tuning approaches that are significantly more accurate and efficient than\nboth sets of existing approaches. NUDGE directly modifies the embeddings of\ndata records to maximize the accuracy of $k$-NN retrieval. We present a\nthorough theoretical and experimental study of NUDGE's non-parametric approach.\nWe show that even though the underlying problem is NP-Hard, constrained\nvariations can be solved efficiently. These constraints additionally ensure\nthat the changes to the embeddings are modest, avoiding large distortions to\nthe semantics learned during pre-training. In experiments across five\npre-trained models and nine standard text and image retrieval datasets, NUDGE\nruns in minutes and often improves NDCG@10 by more than 10% over existing\nfine-tuning methods. On average, NUDGE provides 3.3x and 4.3x higher increase\nin accuracy and runs 200x and 3x faster, respectively, over fine-tuning the\npre-trained model and training adaptors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02343v1",
    "published_date": "2024-09-04 00:10:36 UTC",
    "updated_date": "2024-09-04 00:10:36 UTC"
  }
]
{
  "date": "2024-07-21",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-21 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 更新了 45 篇论文，主要聚焦于 AI 安全、强化学习、因果推理、LLM 应用和生成模型等领域，其中 Zhi-Hua Zhou 的因果识别新规则论文和 Sarit Kraus 的多代理决策解释性研究最为令人印象深刻，它们展示了 AI 在实际问题中的理论创新和可解释性潜力；其他热门主题包括多模态模型的鲁棒性提升和医疗 AI 的优化。\n\n下面，我挑选并简要讨论了部分重要或话题度高的论文，先从 AI 核心领域和知名学者作品入手，再快速掠过其他相关或次要论文。每个条目列出论文标题（中文 + 英文），并突出核心贡献和发现。\n\n### 重点论文讨论\n\n1. **新规则用于因果识别（New Rules for Causal Identification with Background Knowledge）**  \n   作者包括知名学者 Zhi-Hua Zhou，这篇论文提出两种新规则整合背景知识和观测数据，解决了潜在变量下的因果关系识别问题；主要贡献是提升了因果效果计算的效率，避免了指数级复杂度，适用于典型因果任务。\n\n2. **解释多代理决策（Explaining Decisions of Agents in Mixed-Motive Games）**  \n   作者 Sarit Kraus 等，这篇即将发表于 AAAI 2025 的论文设计了针对合作竞争环境的解释方法，处理了代理间的竞争和隐式通信；关键发现是这些方法在多游戏场景（如 Diplomacy）中有效，帮助人类理解 AI 决策，提升了可解释性 AI。\n\n3. **统一不变和变异特征用于图 OOD 泛化（Unifying Invariant and Variant Features for Graph Out-of-Distribution via Probability of Necessity and Sufficiency）**  \n   这篇论文引入必要性和充分性概率（PNS）来提取图结构中的不变子图，并结合变异子图提升泛化性能；主要贡献是提出 SNIGL 模型，在六个基准数据集上超越了现有方法，适用于真实世界图数据泛化。\n\n4. **基于 LLM 的可解释抑郁检测（They Look Like Each Other: Case-based Reasoning for Explainable Depression Detection on Twitter using Large Language Models）**  \n   作者 Pascal Hitzler 等，论文提出 ProtoDep 框架，使用原型学习和 LLM 生成多层解释（如症状级和案例级）；核心发现是它在五个基准数据集上实现了近 SOTA 性能，同时提升了社交媒体抑郁检测的可靠性和透明度。\n\n5. **提升硬件容错性（Enhancing Hardware Fault Tolerance in Machines with Reinforcement Learning Policy Gradient Algorithms）**  \n   这篇论文探索 PPO 和 SAC 算法在模拟环境中增强机器硬件容错；主要贡献是通过消融研究优化知识转移，证明 RL 方法能在几分钟内适应故障，适用于自主系统设计。\n\n6. **评估图像-文本检索基准的脆弱性（Assessing Brittleness of Image-Text Retrieval Benchmarks from Vision-Language Models Perspective）**  \n   论文分析了 MS-COCO 和 Flickr30k 等基准的颗粒度问题，并引入查询扰动；关键发现是细粒度数据集更鲁棒，四种 VLM 在零样本条件下表现一致，揭示了基准自身的缺陷，并为改进 ITR 评估管道提供了议程。\n\n7. **基于变分势流的能量生成框架（Variational Potential Flow: A Novel Probabilistic Framework for Energy-Based Generative Modelling）**  \n   这篇论文提出 VAPO 框架，使用势流引导先验样本生成图像，避免了 MCMC 采样；主要贡献是实现高效图像生成，在 CIFAR-10 和 CelebA 上达到竞争性 FID 分数，简化了能量模型训练。\n\n8. **任务无关的梯度聚类核集选择（TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data）**  \n   论文针对 LLM 指令调优提出 TAGCOS 方法，通过梯度聚类选择高信息子集；核心发现是仅用 5% 数据就超越了无监督基线，显著提高了效率。\n\n9. **人类对齐的实用分析（A Practical Analysis of Human Alignment with *PO）**  \n   这篇即将发表于 NAACL 2025 的论文分析了偏好优化方法（如 DPO）的鲁棒性，并引入 LN-DPO 变体；主要贡献是提升了超参数稳定性，减少响应长度并改善性能，适用于真实 OOD 场景。\n\n10. **多模态 LLM 的对抗攻击（Failures to Find Transferable Image Jailbreaks Between Vision-Language Models）**  \n    论文通过大规模实验证明图像对抗攻击在 VLM 间不易转移；关键发现是只有高度相似的模型间才部分成功，这为 VLM 安全性提供了新见解，NeurIPS 2024 研讨会最佳论文。\n\n### 其他相关论文快速掠过\n剩余论文主题多样，以 AI 应用为主，以次要方式简述：\n- **遗传算法优化微外科剪刀设计（Genetic Algorithm to Optimize Design of Micro-Surgical Scissors）**：使用进化算法提升剪刀切割力，贡献是1.65倍力提升，适用于微创手术。\n- **流作为跨域操作接口（Flow as the Cross-Domain Manipulation Interface）**：提出 Im2Flow2Act 框架，使用物体流桥接模拟和真实机器人任务，实现无真实数据训练的泛化抓取。\n- **文本增强的多模态 LLM 蒸馏（Distilling Vision-Language Foundation Models: A Data-Free Approach via Prompt Diversification）**：无数据蒸馏方法通过提示多样化提升 VLM 泛化，贡献是减少内存使用并在 OOD 数据集上表现优异。\n- **无限上下文注意力（ReAttention: Training-Free Infinite Context with Finite Attention Scope）**：提出 ReAttention 方法，使 Transformer 支持无限上下文，LLaMA3 在 4M 长度测试中有效。\n- **药物靶点亲和预测（Exploiting Pre-trained Models for Drug Target Affinity Prediction with Nearest Neighbors）**：kNN-DTA 方法提升 DTA 预测准确性，贡献是无训练成本的邻居聚合，在 BindingDB 上创下新 RMSE 记录。\n\n今天的核心趋势是 AI 模型的鲁棒性和可解释性改进，这些论文为实际应用提供了新工具。如果您对特定领域感兴趣，建议查看这些论文的摘要！明天的快报见！",
  "papers": [
    {
      "arxiv_id": "2407.15283v1",
      "title": "Enhancing Hardware Fault Tolerance in Machines with Reinforcement Learning Policy Gradient Algorithms",
      "title_zh": "使用强化学习策略梯度算法增强机器中的硬件故障容忍性",
      "authors": [
        "Sheila Schoepp",
        "Mehran Taghian",
        "Shotaro Miwa",
        "Yoshihiro Mitsuka",
        "Shadan Golestan",
        "Osmar Zaïane"
      ],
      "abstract": "Industry is rapidly moving towards fully autonomous and interconnected\nsystems that can detect and adapt to changing conditions, including machine\nhardware faults. Traditional methods for adding hardware fault tolerance to\nmachines involve duplicating components and algorithmically reconfiguring a\nmachine's processes when a fault occurs. However, the growing interest in\nreinforcement learning-based robotic control offers a new perspective on\nachieving hardware fault tolerance. However, limited research has explored the\npotential of these approaches for hardware fault tolerance in machines. This\npaper investigates the potential of two state-of-the-art reinforcement learning\nalgorithms, Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), to\nenhance hardware fault tolerance into machines. We assess the performance of\nthese algorithms in two OpenAI Gym simulated environments, Ant-v2 and\nFetchReach-v1. Robot models in these environments are subjected to six\nsimulated hardware faults. Additionally, we conduct an ablation study to\ndetermine the optimal method for transferring an agent's knowledge, acquired\nthrough learning in a normal (pre-fault) environment, to a (post-)fault\nenvironment in a continual learning setting. Our results demonstrate that\nreinforcement learning-based approaches can enhance hardware fault tolerance in\nsimulated machines, with adaptation occurring within minutes. Specifically, PPO\nexhibits the fastest adaptation when retaining the knowledge within its models,\nwhile SAC performs best when discarding all acquired knowledge. Overall, this\nstudy highlights the potential of reinforcement learning-based approaches, such\nas PPO and SAC, for hardware fault tolerance in machines. These findings pave\nthe way for the development of robust and adaptive machines capable of\neffectively operating in real-world scenarios.",
      "tldr_zh": "本文研究了如何利用强化学习算法Proximal Policy Optimization (PPO)和Soft Actor-Critic (SAC)来提升机器的硬件故障容忍能力，以适应自主互联系统中的故障检测和调整。研究在OpenAI Gym的Ant-v2和FetchReach-v1模拟环境中测试了六种硬件故障，并通过消融研究探索了从正常环境到故障环境的知识转移方法。结果表明，PPO在保留模型知识时适应速度最快，而SAC在丢弃知识时表现最佳，这为开发鲁棒的自适应机器提供了重要基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15283v1",
      "published_date": "2024-07-21 22:24:16 UTC",
      "updated_date": "2024-07-21 22:24:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:34:17.734868"
    },
    {
      "arxiv_id": "2407.15273v1",
      "title": "Unifying Invariant and Variant Features for Graph Out-of-Distribution via Probability of Necessity and Sufficiency",
      "title_zh": "翻译失败",
      "authors": [
        "Xuexin Chen",
        "Ruichu Cai",
        "Kaitao Zheng",
        "Zhifan Jiang",
        "Zhengting Huang",
        "Zhifeng Hao",
        "Zijian Li"
      ],
      "abstract": "Graph Out-of-Distribution (OOD), requiring that models trained on biased data\ngeneralize to the unseen test data, has considerable real-world applications.\nOne of the most mainstream methods is to extract the invariant subgraph by\naligning the original and augmented data with the help of environment\naugmentation. However, these solutions might lead to the loss or redundancy of\nsemantic subgraphs and result in suboptimal generalization. To address this\nchallenge, we propose exploiting Probability of Necessity and Sufficiency (PNS)\nto extract sufficient and necessary invariant substructures. Beyond that, we\nfurther leverage the domain variant subgraphs related to the labels to boost\nthe generalization performance in an ensemble manner. Specifically, we first\nconsider the data generation process for graph data. Under mild conditions, we\nshow that the sufficient and necessary invariant subgraph can be extracted by\nminimizing an upper bound, built on the theoretical advance of the probability\nof necessity and sufficiency. To further bridge the theory and algorithm, we\ndevise the model called Sufficiency and Necessity Inspired Graph Learning\n(SNIGL), which ensembles an invariant subgraph classifier on top of latent\nsufficient and necessary invariant subgraphs, and a domain variant subgraph\nclassifier specific to the test domain for generalization enhancement.\nExperimental results demonstrate that our SNIGL model outperforms the\nstate-of-the-art techniques on six public benchmarks, highlighting its\neffectiveness in real-world scenarios.",
      "tldr_zh": "该论文针对 Graph Out-of-Distribution (OOD) 问题，提出一种统一不变和变异特征的方法，通过 Probability of Necessity and Sufficiency (PNS) 提取充分和必要的不变子结构，同时利用与标签相关的领域变异子图来提升模型泛化性能。作者基于图数据生成过程的理论分析，证明可以通过最小化一个上界来实现子结构的提取，并开发了 Sufficiency and Necessity Inspired Graph Learning (SNIGL) 模型，该模型通过集成不变子图分类器和针对测试领域的变异子图分类器来增强泛化。实验结果显示，SNIGL 在六个公共基准上优于最先进技术，证明了其在实际场景中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15273v1",
      "published_date": "2024-07-21 21:35:01 UTC",
      "updated_date": "2024-07-21 21:35:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:34:30.360089"
    },
    {
      "arxiv_id": "2407.15259v1",
      "title": "New Rules for Causal Identification with Background Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Tian-Zuo Wang",
        "Lue Tao",
        "Zhi-Hua Zhou"
      ],
      "abstract": "Identifying causal relations is crucial for a variety of downstream tasks. In\nadditional to observational data, background knowledge (BK), which could be\nattained from human expertise or experiments, is usually introduced for\nuncovering causal relations. This raises an open problem that in the presence\nof latent variables, what causal relations are identifiable from observational\ndata and BK. In this paper, we propose two novel rules for incorporating BK,\nwhich offer a new perspective to the open problem. In addition, we show that\nthese rules are applicable in some typical causality tasks, such as determining\nthe set of possible causal effects with observational data. Our rule-based\napproach enhances the state-of-the-art method by circumventing a process of\nenumerating block sets that would otherwise take exponential complexity.",
      "tldr_zh": "这篇论文针对存在潜在变量（latent variables）的情况下，从观察数据（observational data）和背景知识（BK）中识别因果关系（causal identification）的问题，提出了两个新规则，以提供新的视角。\n这些规则可用于典型因果任务，如确定可能因果效应集（causal effects），并提升了现有方法的效率。\n相比状态-of-the-art 方法，该规则-based 方式避免了指数复杂度的块集枚举过程，从而简化了计算。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15259v1",
      "published_date": "2024-07-21 20:21:21 UTC",
      "updated_date": "2024-07-21 20:21:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:34:41.420205"
    },
    {
      "arxiv_id": "2407.21041v1",
      "title": "They Look Like Each Other: Case-based Reasoning for Explainable Depression Detection on Twitter using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Saeid Mahdavinejad",
        "Peyman Adibi",
        "Amirhassan Monadjemi",
        "Pascal Hitzler"
      ],
      "abstract": "Depression is a common mental health issue that requires prompt diagnosis and\ntreatment. Despite the promise of social media data for depression detection,\nthe opacity of employed deep learning models hinders interpretability and\nraises bias concerns. We address this challenge by introducing ProtoDep, a\nnovel, explainable framework for Twitter-based depression detection. ProtoDep\nleverages prototype learning and the generative power of Large Language Models\nto provide transparent explanations at three levels: (i) symptom-level\nexplanations for each tweet and user, (ii) case-based explanations comparing\nthe user to similar individuals, and (iii) transparent decision-making through\nclassification weights. Evaluated on five benchmark datasets, ProtoDep achieves\nnear state-of-the-art performance while learning meaningful prototypes. This\nmulti-faceted approach offers significant potential to enhance the reliability\nand transparency of depression detection on social media, ultimately aiding\nmental health professionals in delivering more informed care.",
      "tldr_zh": "该研究提出ProtoDep，一种新型可解释框架，用于基于Twitter的抑郁症检测，旨在解决深度学习模型的不透明性和偏见问题。ProtoDep结合了prototype learning和Large Language Models的生成能力，提供多层次解释，包括：(i) 每个推文和用户的症状级解释，(ii) case-based reasoning将用户与类似个体比较，以及(iii) 通过分类权重实现透明决策。在五个基准数据集上，ProtoDep实现了接近最先进性能，同时学习有意义的原型。该框架提升了抑郁症检测的可靠性和透明度，有助于心理健康专业人士提供更有效的护理。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21041v1",
      "published_date": "2024-07-21 20:13:50 UTC",
      "updated_date": "2024-07-21 20:13:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:34:53.736303"
    },
    {
      "arxiv_id": "2407.15255v3",
      "title": "Explaining Decisions of Agents in Mixed-Motive Games",
      "title_zh": "翻译失败",
      "authors": [
        "Maayan Orner",
        "Oleg Maksimov",
        "Akiva Kleinerman",
        "Charles Ortiz",
        "Sarit Kraus"
      ],
      "abstract": "In recent years, agents have become capable of communicating seamlessly via\nnatural language and navigating in environments that involve cooperation and\ncompetition, a fact that can introduce social dilemmas. Due to the interleaving\nof cooperation and competition, understanding agents' decision-making in such\nenvironments is challenging, and humans can benefit from obtaining\nexplanations. However, such environments and scenarios have rarely been\nexplored in the context of explainable AI. While some explanation methods for\ncooperative environments can be applied in mixed-motive setups, they do not\naddress inter-agent competition, cheap-talk, or implicit communication by\nactions. In this work, we design explanation methods to address these issues.\nThen, we proceed to establish generality and demonstrate the applicability of\nthe methods to three games with vastly different properties. Lastly, we\ndemonstrate the effectiveness and usefulness of the methods for humans in two\nmixed-motive games. The first is a challenging 7-player game called no-press\nDiplomacy. The second is a 3-player game inspired by the prisoner's dilemma,\nfeaturing communication in natural language.",
      "tldr_zh": "这篇论文探讨了在混合动机游戏中解释代理（agents）决策的挑战，这些游戏涉及合作和竞争，可能导致社会困境。作者设计了新的解释方法，以处理代理间的竞争、廉价对话（cheap-talk）和通过行动的隐式沟通。实验验证了这些方法的通用性，并在三个不同属性的游戏中进行了测试；此外，在两个实际游戏——7玩家无对话外交（no-press Diplomacy）和一个受囚徒困境启发的3玩家自然语言沟通游戏中，证明了这些方法对人类的有效性和实用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "To be published in AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.15255v3",
      "published_date": "2024-07-21 19:56:04 UTC",
      "updated_date": "2025-01-27 15:13:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:35:05.876336"
    },
    {
      "arxiv_id": "2407.15243v1",
      "title": "Genetic Algorithm to Optimize Design of Micro-Surgical Scissors",
      "title_zh": "遗传算法用于优化微型手术剪刀设计",
      "authors": [
        "Fatemeh Norouziani",
        "Veerash Palanichamy",
        "Shivam Gupta",
        "Onaizah Onaizah"
      ],
      "abstract": "Microrobotics is an attractive area of research as small-scale robots have\nthe potential to improve the precision and dexterity offered by minimally\ninvasive surgeries. One example of such a tool is a pair of micro-surgical\nscissors that was developed for cutting of tumors or cancerous tissues present\ndeep inside the body such as in the brain. This task is often deemed difficult\nor impossible with conventional robotic tools due to their size and dexterity.\nThe scissors are designed with two magnets placed a specific distance apart to\nmaximize deflection and generate cutting forces. However, remote actuation and\nsize requirements of the micro-surgical scissors limits the force that can be\ngenerated to puncture the tissue. To address the limitation of small output\nforces, we use an evolutionary algorithm to further optimize the performance of\nthe scissors. In this study, the design of the previously developed untethered\nmicro-surgical scissors has been modified and their performance is enhanced by\ndetermining the optimal position of the magnets as well as the direction of\neach magnetic moment. The developed algorithm is successfully applied to a\n4-magnet configuration which results in increased net torque. This improvement\nin net torque is directly translated into higher cutting forces. The new\nconfiguration generates a cutting force of 58 mN from 80 generations of the\nevolutionary algorithm which is a 1.65 times improvement from the original\ndesign. Furthermore, the developed algorithm has the advantage that it can be\ndeployed with minor modifications to other microrobotic tools and systems,\nopening up new possibilities for various medical procedures and applications.",
      "tldr_zh": "本研究针对微型手术剪刀的输出力限制问题，使用遗传算法(genetic algorithm)优化其设计，具体通过调整磁铁位置和磁矩方向来提升切割性能。研究将原始的两磁铁设计扩展到四磁铁配置，并通过进化算法的80代迭代，成功将切割力从原设计提高1.65倍，达到58 mN。优化结果显著提高了净扭矩和切割效率，并展示了该算法的可扩展性，可应用于其他微型机器人工具以改进医疗程序。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for presentation at the International Conference on\n  Manipulation, Automation and Robotics at Small Scales (MARSS) 2024, Delft,\n  Netherlands",
      "pdf_url": "http://arxiv.org/pdf/2407.15243v1",
      "published_date": "2024-07-21 18:39:13 UTC",
      "updated_date": "2024-07-21 18:39:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:35:17.095853"
    },
    {
      "arxiv_id": "2407.15239v3",
      "title": "Assessing Brittleness of Image-Text Retrieval Benchmarks from Vision-Language Models Perspective",
      "title_zh": "从视觉-语言模型视角评估图像-文本检索基准的脆弱",
      "authors": [
        "Mariya Hendriksen",
        "Shuo Zhang",
        "Ridho Reinanda",
        "Mohamed Yahya",
        "Edgar Meij",
        "Maarten de Rijke"
      ],
      "abstract": "We examine the brittleness of the image-text retrieval (ITR) evaluation\npipeline with a focus on concept granularity. We start by analyzing two common\nbenchmarks, MS-COCO and Flickr30k, and compare them with augmented,\nfine-grained versions, MS-COCO-FG and Flickr30k-FG, given a specified set of\nlinguistic features capturing concept granularity. Flickr30k-FG and MS COCO-FG\nconsistently give rise to higher scores across all the selected features. To\nfurther our understanding of the impact of granularity we consider a novel\ntaxonomy of query perturbations. We apply these perturbations to the selected\ndatasets. We evaluate four diverse state-of-the-art Vision-Language models on\nboth the standard and fine-grained datasets under zero-shot conditions, with\nand without the applied perturbations. The results demonstrate that although\nperturbations generally degrade model performance, the fine-grained datasets\nexhibit a smaller performance drop than their standard counterparts. The\nrelative performance drop across all setups is consistent across all models and\ndatasets, indicating that the issue lies within the benchmarks themselves. We\nconclude by providing an agenda for improving ITR evaluation pipelines.",
      "tldr_zh": "本文从视觉语言模型(VLMs)的视角评估图像-文本检索(ITR)基准的脆弱性，焦点在于概念粒度的影响。研究者比较了标准基准(MS-COCO 和 Flickr30k)与细粒度版本(MS-COCO-FG 和 Flickr30k-FG)，并引入查询扰动分类法来测试四种先进模型在零样本条件下的性能。结果表明，虽然扰动会降低模型表现，但细粒度数据集的性能下降幅度较小，且这种一致性问题源于基准本身，最终提出改进ITR评估管道的议程。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15239v3",
      "published_date": "2024-07-21 18:08:44 UTC",
      "updated_date": "2024-10-28 17:52:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:35:31.540753"
    },
    {
      "arxiv_id": "2407.15238v1",
      "title": "Variational Potential Flow: A Novel Probabilistic Framework for Energy-Based Generative Modelling",
      "title_zh": "翻译失败",
      "authors": [
        "Junn Yong Loo",
        "Michelle Adeline",
        "Arghya Pal",
        "Vishnu Monn Baskaran",
        "Chee-Ming Ting",
        "Raphael C. -W. Phan"
      ],
      "abstract": "Energy based models (EBMs) are appealing for their generality and simplicity\nin data likelihood modeling, but have conventionally been difficult to train\ndue to the unstable and time-consuming implicit MCMC sampling during\ncontrastive divergence training. In this paper, we present a novel energy-based\ngenerative framework, Variational Potential Flow (VAPO), that entirely\ndispenses with implicit MCMC sampling and does not rely on complementary latent\nmodels or cooperative training. The VAPO framework aims to learn a potential\nenergy function whose gradient (flow) guides the prior samples, so that their\ndensity evolution closely follows an approximate data likelihood homotopy. An\nenergy loss function is then formulated to minimize the Kullback-Leibler\ndivergence between density evolution of the flow-driven prior and the data\nlikelihood homotopy. Images can be generated after training the potential\nenergy, by initializing the samples from Gaussian prior and solving the ODE\ngoverning the potential flow on a fixed time interval using generic ODE\nsolvers. Experiment results show that the proposed VAPO framework is capable of\ngenerating realistic images on various image datasets. In particular, our\nproposed framework achieves competitive FID scores for unconditional image\ngeneration on the CIFAR-10 and CelebA datasets.",
      "tldr_zh": "本文提出了一种新型概率框架 Variational Potential Flow (VAPO)，用于基于能量的生成模型（EBMs），它避免了传统方法中不稳定的隐式 MCMC 采样，并不依赖补充潜在模型或合作训练。VAPO 通过学习一个势能函数及其梯度（flow）来引导先验样本的密度演化，并最小化 Kullback-Leibler divergence，以使样本密度接近数据似然 homotopy。实验结果表明，该框架在 CIFAR-10 和 CelebA 数据集上实现了竞争性的 FID scores，在无条件图像生成任务中表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15238v1",
      "published_date": "2024-07-21 18:08:12 UTC",
      "updated_date": "2024-07-21 18:08:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:35:42.535085"
    },
    {
      "arxiv_id": "2407.15235v1",
      "title": "TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data",
      "title_zh": "翻译失败",
      "authors": [
        "Jipeng Zhang",
        "Yaxuan Qin",
        "Renjie Pi",
        "Weizhong Zhang",
        "Rui Pan",
        "Tong Zhang"
      ],
      "abstract": "Instruction tuning has achieved unprecedented success in NLP, turning large\nlanguage models into versatile chatbots. However, the increasing variety and\nvolume of instruction datasets demand significant computational resources. To\naddress this, it is essential to extract a small and highly informative subset\n(i.e., Coreset) that achieves comparable performance to the full dataset.\nAchieving this goal poses non-trivial challenges: 1) data selection requires\naccurate data representations that reflect the training samples' quality, 2)\nconsidering the diverse nature of instruction datasets, and 3) ensuring the\nefficiency of the coreset selection algorithm for large models. To address\nthese challenges, we propose Task-Agnostic Gradient Clustered COreset Selection\n(TAGCOS). Specifically, we leverage sample gradients as the data\nrepresentations, perform clustering to group similar data, and apply an\nefficient greedy algorithm for coreset selection. Experimental results show\nthat our algorithm, selecting only 5% of the data, surpasses other unsupervised\nmethods and achieves performance close to that of the full dataset.",
      "tldr_zh": "该研究针对指令微调数据量大、计算资源需求高的挑战，提出了一种任务无关的梯度聚类Coreset选择方法TAGCOS。方法使用样本梯度作为数据表示，进行聚类分组，并应用高效的贪婪算法来选择信息丰富的子集，以应对指令数据集的多样性和算法效率问题。实验结果显示，TAGCOS仅选择5%的数据，就超过了其他无监督方法，并在性能上接近使用完整数据集的效果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint. Our code and models are available at:\n  https://github.com/2003pro/TAGCOS",
      "pdf_url": "http://arxiv.org/pdf/2407.15235v1",
      "published_date": "2024-07-21 17:59:20 UTC",
      "updated_date": "2024-07-21 17:59:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:35:52.761791"
    },
    {
      "arxiv_id": "2407.15229v2",
      "title": "A Practical Analysis of Human Alignment with *PO",
      "title_zh": "翻译失败",
      "authors": [
        "Kian Ahrabian",
        "Xihui Lin",
        "Barun Patra",
        "Vishrav Chaudhary",
        "Alon Benhaim",
        "Jay Pujara",
        "Xia Song"
      ],
      "abstract": "At the forefront of state-of-the-art human alignment methods are preference\noptimization methods (*PO). Prior research has often concentrated on\nidentifying the best-performing method, typically involving a grid search over\nhyperparameters, which can be impractical for general practitioners. In this\npaper, we examine the robustness of existing state-of-the-art methods to\nvarying hyperparameters in a realistic out-of-distribution (OOD) scenario that\nmirrors real-world applications of human alignment. Our goal is to empirically\nfind the method that increases the likelihood of achieving better results\nthrough the lens of various metrics, such as KL divergence and response length.\nWe also introduce LN-DPO, a simple length-normalized version of DPO that is\nmore stable across hyperparameters, effectively reduces the average response\nlength, and improves performance. Our analysis of state-of-the-art\nreference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO)\nmethods reveals that they perform similarly at their peak (i.e., best possible\nscenario). However, we uncover that the pattern of change in performance\ngreatly varies as we move away from the best possible scenario.",
      "tldr_zh": "这篇论文分析了偏好优化方法(*PO)在人类对齐中的实用性，重点考察现有最先进方法（如DPO和SimPO）对超参数变化的鲁棒性，尤其在真实分布外(ODD)场景下。作者引入了LN-DPO，一种简单的长度归一化版本的DPO，能够在超参数波动时保持稳定性，同时减少响应长度并提升整体性能。实验结果显示，这些方法在最佳条件下性能相似，但远离最佳场景时，性能变化模式显著不同，从而为实际应用提供更可靠的指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025 findings papers. 9 pages, 7 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.15229v2",
      "published_date": "2024-07-21 17:35:20 UTC",
      "updated_date": "2025-04-10 16:34:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:36:06.590835"
    },
    {
      "arxiv_id": "2407.15224v1",
      "title": "PUFFLE: Balancing Privacy, Utility, and Fairness in Federated Learning",
      "title_zh": "PUFFLE：平衡联邦学习中的隐私、效用和公平性",
      "authors": [
        "Luca Corbucci",
        "Mikko A Heikkila",
        "David Solans Noguero",
        "Anna Monreale",
        "Nicolas Kourtellis"
      ],
      "abstract": "Training and deploying Machine Learning models that simultaneously adhere to\nprinciples of fairness and privacy while ensuring good utility poses a\nsignificant challenge. The interplay between these three factors of\ntrustworthiness is frequently underestimated and remains insufficiently\nexplored. Consequently, many efforts focus on ensuring only two of these\nfactors, neglecting one in the process. The decentralization of the datasets\nand the variations in distributions among the clients exacerbate the complexity\nof achieving this ethical trade-off in the context of Federated Learning (FL).\nFor the first time in FL literature, we address these three factors of\ntrustworthiness. We introduce PUFFLE, a high-level parameterised approach that\ncan help in the exploration of the balance between utility, privacy, and\nfairness in FL scenarios. We prove that PUFFLE can be effective across diverse\ndatasets, models, and data distributions, reducing the model unfairness up to\n75%, with a maximum reduction in the utility of 17% in the worst-case scenario,\nwhile maintaining strict privacy guarantees during the FL training.",
      "tldr_zh": "该论文探讨了在联邦学习（Federated Learning）中同时实现隐私（Privacy）、效用（Utility）和公平性（Fairness）的挑战，这是首次全面处理这三个信任因素。作者引入了 PUFFLE，一种高层次的参数化方法，能够帮助探索这些因素之间的平衡，尤其在数据集分散和分布差异的情况下。实验结果显示，PUFFLE 在多种数据集、模型和数据分布中有效，减少模型不公平性高达 75%，效用仅在最坏情况下减少 17%，同时保持严格的隐私保证。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15224v1",
      "published_date": "2024-07-21 17:22:18 UTC",
      "updated_date": "2024-07-21 17:22:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:36:18.514627"
    },
    {
      "arxiv_id": "2407.15216v1",
      "title": "Explainability Paths for Sustained Artistic Practice with AI",
      "title_zh": "翻译失败",
      "authors": [
        "Austin Tecks",
        "Thomas Peschlow",
        "Gabriel Vigliensoni"
      ],
      "abstract": "The development of AI-driven generative audio mirrors broader AI trends,\noften prioritizing immediate accessibility at the expense of explainability.\nConsequently, integrating such tools into sustained artistic practice remains a\nsignificant challenge. In this paper, we explore several paths to improve\nexplainability, drawing primarily from our research-creation practice in\ntraining and implementing generative audio models. As practical provisions for\nimproved explainability, we highlight human agency over training materials, the\nviability of small-scale datasets, the facilitation of the iterative creative\nprocess, and the integration of interactive machine learning as a mapping tool.\nImportantly, these steps aim to enhance human agency over generative AI systems\nnot only during model inference, but also when curating and preprocessing\ntraining data as well as during the training phase of models.",
      "tldr_zh": "该论文探讨了AI驱动的生成音频工具在艺术实践中的挑战，即优先即时可访问性而忽略explainability，导致难以融入持续的艺术创作中。作者基于自身的研究-创作实践，提出几条改善explainability的路径，包括增强human agency对训练材料的控制、使用小规模数据集、促进迭代创作过程，以及整合interactive machine learning作为映射工具。这些措施旨在提升人类在生成AI系统的各个阶段（如模型推理、数据整理和训练）的控制力，从而支持更可持续的艺术实践。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "In Proceedings of Explainable AI for the Arts Workshop 2024 (XAIxArts\n  2024) arXiv:2406.14485",
      "pdf_url": "http://arxiv.org/pdf/2407.15216v1",
      "published_date": "2024-07-21 16:48:14 UTC",
      "updated_date": "2024-07-21 16:48:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:36:30.438739"
    },
    {
      "arxiv_id": "2407.18333v1",
      "title": "AutoVCoder: A Systematic Framework for Automated Verilog Code Generation using LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Mingzhe Gao",
        "Jieru Zhao",
        "Zhe Lin",
        "Wenchao Ding",
        "Xiaofeng Hou",
        "Yu Feng",
        "Chao Li",
        "Minyi Guo"
      ],
      "abstract": "Recently, the use of large language models (LLMs) for software code\ngeneration, e.g., C/C++ and Python, has proven a great success. However, LLMs\nstill suffer from low syntactic and functional correctness when it comes to the\ngeneration of register-transfer level (RTL) code, such as Verilog. To address\nthis issue, in this paper, we develop AutoVCoder, a systematic open-source\nframework that significantly improves the LLMs' correctness of generating\nVerilog code and enhances the quality of its output at the same time. Our\nframework integrates three novel techniques, including a high-quality hardware\ndataset generation approach, a two-round LLM fine-tuning method and a\ndomain-specific retrieval-augmented generation (RAG) mechanism. Experimental\nresults demonstrate that AutoVCoder outperforms both industrial and academic\nLLMs in Verilog code generation. Specifically, AutoVCoder shows a 0.5% and 2.2%\nimprovement in functional correctness on the EvalMachine and EvalHuman\nbenchmarks compared with BetterV, and also achieves a 3.4% increase in syntax\ncorrectness and a 3.4% increase in functional correctness on the RTLLM\nbenchmark compared with RTLCoder.",
      "tldr_zh": "该研究提出AutoVCoder框架，利用LLMs自动生成Verilog代码，以解决LLMs在生成寄存器传输级(RTL)代码时存在的语法和功能正确性低的问题。框架整合了三种创新技术：高质量硬件数据集生成方法、两轮LLM微调机制，以及领域特定的检索增强生成(RAG)机制。实验结果显示，AutoVCoder在EvalMachine和EvalHuman基准上功能正确性分别比BetterV提高了0.5%和2.2%，而在RTLLM基准上，语法正确性和功能正确性均提高了3.4%，从而提升了Verilog代码生成的整体质量。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18333v1",
      "published_date": "2024-07-21 16:42:45 UTC",
      "updated_date": "2024-07-21 16:42:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:36:42.490695"
    },
    {
      "arxiv_id": "2407.15211v2",
      "title": "Failures to Find Transferable Image Jailbreaks Between Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Rylan Schaeffer",
        "Dan Valentine",
        "Luke Bailey",
        "James Chua",
        "Cristóbal Eyzaguirre",
        "Zane Durante",
        "Joe Benton",
        "Brando Miranda",
        "Henry Sleight",
        "John Hughes",
        "Rajashree Agrawal",
        "Mrinank Sharma",
        "Scott Emmons",
        "Sanmi Koyejo",
        "Ethan Perez"
      ],
      "abstract": "The integration of new modalities into frontier AI systems offers exciting\ncapabilities, but also increases the possibility such systems can be\nadversarially manipulated in undesirable ways. In this work, we focus on a\npopular class of vision-language models (VLMs) that generate text outputs\nconditioned on visual and textual inputs. We conducted a large-scale empirical\nstudy to assess the transferability of gradient-based universal image\n``jailbreaks\" using a diverse set of over 40 open-parameter VLMs, including 18\nnew VLMs that we publicly release. Overall, we find that transferable\ngradient-based image jailbreaks are extremely difficult to obtain. When an\nimage jailbreak is optimized against a single VLM or against an ensemble of\nVLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits\nlittle-to-no transfer to any other VLMs; transfer is not affected by whether\nthe attacked and target VLMs possess matching vision backbones or language\nmodels, whether the language model underwent instruction-following and/or\nsafety-alignment training, or many other factors. Only two settings display\npartially successful transfer: between identically-pretrained and\nidentically-initialized VLMs with slightly different VLM training data, and\nbetween different training checkpoints of a single VLM. Leveraging these\nresults, we then demonstrate that transfer can be significantly improved\nagainst a specific target VLM by attacking larger ensembles of\n``highly-similar\" VLMs. These results stand in stark contrast to existing\nevidence of universal and transferable text jailbreaks against language models\nand transferable adversarial attacks against image classifiers, suggesting that\nVLMs may be more robust to gradient-based transfer attacks.",
      "tldr_zh": "本研究评估了基于梯度的通用图像jailbreaks在不同视觉语言模型（VLMs）之间的可转移性，通过对超过40个开放参数VLMs的大规模实证研究，包括18个新发布的模型。结果显示，针对单个VLMs或VLMs集合优化的jailbreaks仅能在攻击目标模型上成功，而对其他模型的转移性极低，且不受视觉骨干、语言模型或训练类型等因素影响。仅有两种情况显示部分转移成功：即在相同预训练和初始化的VLMs之间，或同一个VLMs的不同训练检查点之间。通过攻击更大的“高度相似”VLMs集合，可以显著改善针对特定目标VLMs的转移性。这些发现表明，VLMs可能比语言模型或图像分类器更robust to gradient-based transfer attacks。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2024 Workshops: RBFM (Best Paper), Frontiers in AdvML (Oral),\n  Red Teaming GenAI (Oral), SoLaR (Spotlight), SATA",
      "pdf_url": "http://arxiv.org/pdf/2407.15211v2",
      "published_date": "2024-07-21 16:27:24 UTC",
      "updated_date": "2024-12-16 01:20:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:36:58.477776"
    },
    {
      "arxiv_id": "2407.15208v2",
      "title": "Flow as the Cross-Domain Manipulation Interface",
      "title_zh": "翻译失败",
      "authors": [
        "Mengda Xu",
        "Zhenjia Xu",
        "Yinghao Xu",
        "Cheng Chi",
        "Gordon Wetzstein",
        "Manuela Veloso",
        "Shuran Song"
      ],
      "abstract": "We present Im2Flow2Act, a scalable learning framework that enables robots to\nacquire real-world manipulation skills without the need of real-world robot\ntraining data. The key idea behind Im2Flow2Act is to use object flow as the\nmanipulation interface, bridging domain gaps between different embodiments\n(i.e., human and robot) and training environments (i.e., real-world and\nsimulated). Im2Flow2Act comprises two components: a flow generation network and\na flow-conditioned policy. The flow generation network, trained on human\ndemonstration videos, generates object flow from the initial scene image,\nconditioned on the task description. The flow-conditioned policy, trained on\nsimulated robot play data, maps the generated object flow to robot actions to\nrealize the desired object movements. By using flow as input, this policy can\nbe directly deployed in the real world with a minimal sim-to-real gap. By\nleveraging real-world human videos and simulated robot play data, we bypass the\nchallenges of teleoperating physical robots in the real world, resulting in a\nscalable system for diverse tasks. We demonstrate Im2Flow2Act's capabilities in\na variety of real-world tasks, including the manipulation of rigid,\narticulated, and deformable objects.",
      "tldr_zh": "该论文提出Im2Flow2Act框架，一种可扩展的学习系统，允许机器人从真实世界中获取操作技能，而无需真实机器人训练数据，通过object flow作为跨领域操作接口桥接人类演示与机器人执行的差距。框架包括flow generation network（基于人类视频训练，生成任务描述条件下的object flow）和flow-conditioned policy（在模拟环境中训练，将object flow映射为机器人动作），从而最小化sim-to-real gap。实验结果显示，该系统在处理刚性、关节和可变形物体的真实任务中表现出色，提供了一种高效、可扩展的机器人技能学习方法。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Conference on Robot Learning 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.15208v2",
      "published_date": "2024-07-21 16:15:02 UTC",
      "updated_date": "2024-10-04 04:05:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:37:07.080033"
    },
    {
      "arxiv_id": "2408.01444v2",
      "title": "No Size Fits All: The Perils and Pitfalls of Leveraging LLMs Vary with Company Size",
      "title_zh": "翻译失败",
      "authors": [
        "Ashok Urlana",
        "Charaka Vinayak Kumar",
        "Bala Mallikarjunarao Garlapati",
        "Ajeet Kumar Singh",
        "Rahul Mishra"
      ],
      "abstract": "Large language models (LLMs) are playing a pivotal role in deploying\nstrategic use cases across a range of organizations, from large pan-continental\ncompanies to emerging startups. The issues and challenges involved in the\nsuccessful utilization of LLMs can vary significantly depending on the size of\nthe organization. It is important to study and discuss these pertinent issues\nof LLM adaptation with a focus on the scale of the industrial concerns and\nbrainstorm possible solutions and prospective directions. Such a study has not\nbeen prominently featured in the current research literature. In this study, we\nadopt a threefold strategy: first, we conduct a case study with industry\npractitioners to formulate the key research questions; second, we examine\nexisting industrial publications to address these questions; and finally, we\nprovide a practical guide for industries to utilize LLMs more efficiently. We\nrelease the\nGitHub\\footnote{\\url{https://github.com/vinayakcse/IndustrialLLMsPapers}}\nrepository with the most recent papers in the field.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在不同规模公司中的应用风险，这些风险因公司大小（如大型企业和初创公司）而异，并强调了这一主题在现有研究中的缺失。研究采用三步策略：首先，通过行业从业者案例研究制定关键问题；其次，审查现有工业出版物来解答这些问题；最后，提供实用指南帮助企业更高效地利用 LLMs。论文的贡献包括填补研究空白、提出潜在解决方案方向，并发布了相关 GitHub 仓库以供参考。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "COLING2025 Industry track",
      "pdf_url": "http://arxiv.org/pdf/2408.01444v2",
      "published_date": "2024-07-21 16:11:00 UTC",
      "updated_date": "2024-12-01 16:11:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:37:22.835878"
    },
    {
      "arxiv_id": "2407.15202v1",
      "title": "Exploiting Pre-trained Models for Drug Target Affinity Prediction with Nearest Neighbors",
      "title_zh": "利用预训练模型结合最近邻方法进行药物靶点亲和力预测",
      "authors": [
        "Qizhi Pei",
        "Lijun Wu",
        "Zhenyu He",
        "Jinhua Zhu",
        "Yingce Xia",
        "Shufang Xie",
        "Rui Yan"
      ],
      "abstract": "Drug-Target binding Affinity (DTA) prediction is essential for drug\ndiscovery. Despite the application of deep learning methods to DTA prediction,\nthe achieved accuracy remain suboptimal. In this work, inspired by the recent\nsuccess of retrieval methods, we propose $k$NN-DTA, a non-parametric\nembedding-based retrieval method adopted on a pre-trained DTA prediction model,\nwhich can extend the power of the DTA model with no or negligible cost.\nDifferent from existing methods, we introduce two neighbor aggregation ways\nfrom both embedding space and label space that are integrated into a unified\nframework. Specifically, we propose a \\emph{label aggregation} with\n\\emph{pair-wise retrieval} and a \\emph{representation aggregation} with\n\\emph{point-wise retrieval} of the nearest neighbors. This method executes in\nthe inference phase and can efficiently boost the DTA prediction performance\nwith no training cost. In addition, we propose an extension, Ada-$k$NN-DTA, an\ninstance-wise and adaptive aggregation with lightweight learning. Results on\nfour benchmark datasets show that $k$NN-DTA brings significant improvements,\noutperforming previous state-of-the-art (SOTA) results, e.g, on BindingDB\nIC$_{50}$ and $K_i$ testbeds, $k$NN-DTA obtains new records of RMSE\n$\\bf{0.684}$ and $\\bf{0.750}$. The extended Ada-$k$NN-DTA further improves the\nperformance to be $\\bf{0.675}$ and $\\bf{0.735}$ RMSE. These results strongly\nprove the effectiveness of our method. Results in other settings and\ncomprehensive studies/analyses also show the great potential of our $k$NN-DTA\napproach.",
      "tldr_zh": "该论文提出 kNN-DTA，一种基于预训练 DTA 预测模型的非参数嵌入式检索方法，用于提升药物靶点结合亲和力(DTA)预测的准确性，通过引入标签空间的 pair-wise retrieval 聚合和嵌入空间的 point-wise retrieval 聚合，构建统一框架，并在推理阶段高效应用，无需额外训练。  \n相比现有方法，kNN-DTA 在四个基准数据集上显著改善性能，超越 SOTA 水平，例如在 BindingDB IC50 和 Ki 测试集上，RMSE 分别达到 0.684 和 0.750。  \n此外，扩展版本 Ada-kNN-DTA 通过实例-wise 自适应聚合和轻量级学习，进一步优化结果，使 RMSE 降至 0.675 和 0.735。  \n这些结果证明了 kNN-DTA 方法的有效性和潜力，为药物发现领域提供高效的性能提升方案。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Accepted by 33rd ACM International Conference on Information and\n  Knowledge Management 2024 (CIKM 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.15202v1",
      "published_date": "2024-07-21 15:49:05 UTC",
      "updated_date": "2024-07-21 15:49:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:37:32.618928"
    },
    {
      "arxiv_id": "2407.15200v3",
      "title": "HyperbolicLR: Epoch insensitive learning rate scheduler",
      "title_zh": "翻译失败",
      "authors": [
        "Tae-Geun Kim"
      ],
      "abstract": "This study proposes two novel learning rate schedulers -- Hyperbolic Learning\nRate Scheduler (HyperbolicLR) and Exponential Hyperbolic Learning Rate\nScheduler (ExpHyperbolicLR) -- to address the epoch sensitivity problem that\noften causes inconsistent learning curves in conventional methods. By\nleveraging the asymptotic behavior of hyperbolic curves, the proposed\nschedulers maintain more stable learning curves across varying epoch settings.\nSpecifically, HyperbolicLR applies this property directly in the epoch-learning\nrate space, while ExpHyperbolicLR extends it to an exponential space. We first\ndetermine optimal hyperparameters for each scheduler on a small number of\nepochs, fix these hyperparameters, and then evaluate performance as the number\nof epochs increases. Experimental results on various deep learning tasks (e.g.,\nimage classification, time series forecasting, and operator learning)\ndemonstrate that both HyperbolicLR and ExpHyperbolicLR achieve more consistent\nperformance improvements than conventional schedulers as training duration\ngrows. These findings suggest that our hyperbolic-based schedulers offer a more\nrobust and efficient approach to deep network optimization, particularly in\nscenarios constrained by computational resources or time.",
      "tldr_zh": "本文提出两种新型学习率调度器：HyperbolicLR 和 ExpHyperbolicLR，以解决传统方法在不同 epoch 设置下导致的学习曲线不一致问题。这些调度器利用双曲线的渐近行为，确保在 epoch-learning rate 空间或指数空间中保持更稳定的学习曲线，并通过先优化超参数再扩展训练周期来实现高效调整。实验结果显示，在图像分类、时间序列预测和操作学习等任务上，这两种调度器比传统方法提供更一致的性能提升。总体而言，该方法为深度网络优化提供了更稳健且资源高效的解决方案，尤其适用于计算资源或时间受限的场景。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.15200v3",
      "published_date": "2024-07-21 15:43:52 UTC",
      "updated_date": "2025-02-01 08:29:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:37:43.450982"
    },
    {
      "arxiv_id": "2407.15192v2",
      "title": "Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification without Prior Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Joshua Shay Kricheli",
        "Khoa Vo",
        "Aniruddha Datta",
        "Spencer Ozgur",
        "Paulo Shakarian"
      ],
      "abstract": "Recent advances in Hierarchical Multi-label Classification (HMC),\nparticularly neurosymbolic-based approaches, have demonstrated improved\nconsistency and accuracy by enforcing constraints on a neural model during\ntraining. However, such work assumes the existence of such constraints\na-priori. In this paper, we relax this strong assumption and present an\napproach based on Error Detection Rules (EDR) that allow for learning\nexplainable rules about the failure modes of machine learning models. We show\nthat these rules are not only effective in detecting when a machine learning\nclassifier has made an error but also can be leveraged as constraints for HMC,\nthereby allowing the recovery of explainable constraints even if they are not\nprovided. We show that our approach is effective in detecting machine learning\nerrors and recovering constraints, is noise tolerant, and can function as a\nsource of knowledge for neurosymbolic models on multiple datasets, including a\nnewly introduced military vehicle recognition dataset.",
      "tldr_zh": "这篇论文提出了一种基于 Error Detection Rules (EDR) 的方法，用于 Hierarchical Multi-Label Classification (HMC)，无需先验知识即可检测机器学习模型的错误并恢复可解释约束。该方法通过学习模型失败模式的规则，不仅能有效识别错误，还能将这些规则转化为 HMC 的约束，提升模型的准确性和一致性。实验显示，该方法在多个数据集上表现出色，包括一个新的军事车辆识别数据集，能够容忍噪声，并为 neurosymbolic models 提供可靠的知识来源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO",
        "cs.SC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15192v2",
      "published_date": "2024-07-21 15:12:19 UTC",
      "updated_date": "2025-04-23 01:11:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:37:54.483615"
    },
    {
      "arxiv_id": "2407.15184v1",
      "title": "Decoding Multilingual Moral Preferences: Unveiling LLM's Biases Through the Moral Machine Experiment",
      "title_zh": "翻译失败",
      "authors": [
        "Karina Vida",
        "Fabian Damken",
        "Anne Lauscher"
      ],
      "abstract": "Large language models (LLMs) increasingly find their way into the most\ndiverse areas of our everyday lives. They indirectly influence people's\ndecisions or opinions through their daily use. Therefore, understanding how and\nwhich moral judgements these LLMs make is crucial. However, morality is not\nuniversal and depends on the cultural background. This raises the question of\nwhether these cultural preferences are also reflected in LLMs when prompted in\ndifferent languages or whether moral decision-making is consistent across\ndifferent languages. So far, most research has focused on investigating the\ninherent values of LLMs in English. While a few works conduct multilingual\nanalyses of moral bias in LLMs in a multilingual setting, these analyses do not\ngo beyond atomic actions. To the best of our knowledge, a multilingual analysis\nof moral bias in dilemmas has not yet been conducted.\n  To address this, our paper builds on the moral machine experiment (MME) to\ninvestigate the moral preferences of five LLMs, Falcon, Gemini, Llama, GPT, and\nMPT, in a multilingual setting and compares them with the preferences collected\nfrom humans belonging to different cultures. To accomplish this, we generate\n6500 scenarios of the MME and prompt the models in ten languages on which\naction to take. Our analysis reveals that all LLMs inhibit different moral\nbiases to some degree and that they not only differ from the human preferences\nbut also across multiple languages within the models themselves. Moreover, we\nfind that almost all models, particularly Llama 3, divert greatly from human\nvalues and, for instance, prefer saving fewer people over saving more.",
      "tldr_zh": "这篇论文通过 Moral Machine Experiment (MME) 调查了五个 LLMs（Falcon, Gemini, Llama, GPT 和 MPT）在多语言环境下的道德偏见，并将其与不同文化的人类偏好进行比较。研究者生成了 6500 个道德困境场景，并使用十种语言提示模型，以评估 LLMs 是否反映文化差异。结果显示，所有 LLMs 都存在不同程度的道德偏见，不仅与人类偏好不一致，而且在同一模型的多种语言间也表现出差异，特别是 Llama 3 与人类价值观偏差较大，例如更倾向于拯救更少的人。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "to be published in AIES 2024 Proceedings",
      "pdf_url": "http://arxiv.org/pdf/2407.15184v1",
      "published_date": "2024-07-21 14:48:13 UTC",
      "updated_date": "2024-07-21 14:48:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:38:07.478609"
    },
    {
      "arxiv_id": "2407.15176v3",
      "title": "ReAttention: Training-Free Infinite Context with Finite Attention Scope",
      "title_zh": "ReAttention：无需训练的无限上下文结合有限注意力范围",
      "authors": [
        "Xiaoran Liu",
        "Ruixiao Li",
        "Qipeng Guo",
        "Zhigeng Liu",
        "Yuerong Song",
        "Kai Lv",
        "Hang Yan",
        "Linlin Li",
        "Qun Liu",
        "Xipeng Qiu"
      ],
      "abstract": "The long-context capability of the Large Language Models (LLM) has made\nsignificant breakthroughs, but the maximum supported context length in length\nextrapolation remains a critical bottleneck limiting their practical\napplications. The constraint of context length in LLMs arises from the\nself-attention mechanism, which cannot effectively and efficiently capture the\nsemantic relationships within infinitely long contexts via the limited\npre-trained positional information and attention scope. In this work, we\npropose ReAttention, a training-free approach enabling LLM based on the\nself-attention mechanism to support an infinite context with a finite attention\nscope under sufficient memory resources. ReAttention performs the\nposition-agnostic top-$k$ attention before the ordinary position-aware\nself-attention, freeing LLMs from the length extrapolation issue. We validate\nthe performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and\ndemonstrate that it is on par with traditional methods. Furthermore, we also\napply ReAttention on mainstream LLMs, including LLaMA3.1-8B and\nMistral-v0.3-7B, enabling them to support context lengths of at least 1M and\neven expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M\nwithout any further training in Needle-In-A-Haystack tests. We also improve the\nefficiency of ReAttention with Triton and achieve an efficient extrapolation\nwithout additional overhead. The code is available at\nhttps://github.com/OpenMOSS/ReAttention.",
      "tldr_zh": "该论文提出 ReAttention，一种无需训练的方法，旨在解决 Large Language Models (LLM) 在自注意力机制下无法有效处理无限长上下文的问题。ReAttention 通过在普通 position-aware 自注意力前添加 position-agnostic top-k 注意力，使 LLM 能够在有限的注意力范围内支持无限上下文长度。实验在 LongBench、L-Eval 和 InfiniteBench 上验证了其性能，与传统方法相当，并成功将 LLaMA3.1-8B 和 Mistral-v0.3-7B 的上下文长度扩展到至少 1M，甚至将 LLaMA3.2-3B-chat 扩展 128 倍至 4M。通过 Triton 优化，ReAttention 实现了高效扩展，无额外开销。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 11 figures, Accepted by ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.15176v3",
      "published_date": "2024-07-21 14:23:37 UTC",
      "updated_date": "2025-03-19 12:15:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:38:22.752804"
    },
    {
      "arxiv_id": "2407.15174v2",
      "title": "TADA: Temporal Adversarial Data Augmentation for Time Series Data",
      "title_zh": "翻译失败",
      "authors": [
        "Byeong Tak Lee",
        "Joon-myoung Kwon",
        "Yong-Yeon Jo"
      ],
      "abstract": "Domain generalization aim to train models to effectively perform on samples\nthat are unseen and outside of the distribution. Adversarial data augmentation\n(ADA) is a widely used technique in domain generalization. It enhances the\nmodel robustness by including synthetic samples designed to simulate potential\nunseen scenarios into the training datasets, which is then used to train the\nmodel. However, in time series data, traditional ADA approaches often fail to\naddress distribution shifts related to temporal characteristics. To address\nthis limitation, we propose Temporal Adversarial Data Augmentation (TADA) for\ntime series data, which incorporate time warping into ADA. Although time\nwarping is inherently non-differentiable, ADA relies on generating samples\nthrough backpropagation. We resolve this issue by leveraging the duality\nbetween phase shifts in the frequency domain and time shifts in the time\ndomain, thereby making the process differentiable. Our evaluations across\nvarious time series datasets demonstrate that TADA outperforms existing methods\nfor domain generalization. In addition, using distribution visualization, we\nconfirmed that the distribution shifts induced by TADA are clearly different\nfrom those induced by ADA, and together, they effectively simulate real-world\ndistribution shifts.",
      "tldr_zh": "本研究针对领域泛化（Domain Generalization）在时间序列数据上的挑战，提出了一种 Temporal Adversarial Data Augmentation (TADA) 方法，将时间扭曲（time warping）整合到传统 Adversarial Data Augmentation (ADA) 中，以处理与时间特性相关的分布偏移。TADA 通过利用频率域相移和时间域时间移位的对偶性，解决时间扭曲的非微分问题，使样本生成过程可通过反向传播实现。实验结果显示，TADA 在多种时间序列数据集上优于现有方法，并在分布可视化中证实其诱导的分布偏移与 ADA 不同，二者结合能更有效地模拟真实世界分布偏移。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15174v2",
      "published_date": "2024-07-21 14:21:00 UTC",
      "updated_date": "2024-10-15 09:54:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:38:32.376083"
    },
    {
      "arxiv_id": "2407.15168v1",
      "title": "Mitigating Deep Reinforcement Learning Backdoors in the Neural Activation Space",
      "title_zh": "翻译失败",
      "authors": [
        "Sanyam Vyas",
        "Chris Hicks",
        "Vasilios Mavroudis"
      ],
      "abstract": "This paper investigates the threat of backdoors in Deep Reinforcement\nLearning (DRL) agent policies and proposes a novel method for their detection\nat runtime. Our study focuses on elusive in-distribution backdoor triggers.\nSuch triggers are designed to induce a deviation in the behaviour of a\nbackdoored agent while blending into the expected data distribution to evade\ndetection. Through experiments conducted in the Atari Breakout environment, we\ndemonstrate the limitations of current sanitisation methods when faced with\nsuch triggers and investigate why they present a challenging defence problem.\nWe then evaluate the hypothesis that backdoor triggers might be easier to\ndetect in the neural activation space of the DRL agent's policy network. Our\nstatistical analysis shows that indeed the activation patterns in the agent's\npolicy network are distinct in the presence of a trigger, regardless of how\nwell the trigger is concealed in the environment. Based on this, we propose a\nnew defence approach that uses a classifier trained on clean environment\nsamples and detects abnormal activations. Our results show that even\nlightweight classifiers can effectively prevent malicious actions with\nconsiderable accuracy, indicating the potential of this research direction even\nagainst sophisticated adversaries.",
      "tldr_zh": "这篇论文探讨了Deep Reinforcement Learning (DRL)代理策略中的后门威胁，特别针对难以检测的in-distribution后门触发器，这些触发器会改变代理行为却混入正常数据分布。研究者通过在Atari Breakout环境中的实验，揭示了现有清理方法的局限性，并假设后门在神经激活空间更容易识别。基于统计分析显示触发器会产生独特激活模式，他们提出了一种新防御方法，使用在干净样本上训练的轻量级分类器检测异常激活。结果表明，该方法能有效防止恶意行为，具有高准确率，为应对复杂攻击提供了潜在新方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "11 Pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.15168v1",
      "published_date": "2024-07-21 13:48:23 UTC",
      "updated_date": "2024-07-21 13:48:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:38:44.468890"
    },
    {
      "arxiv_id": "2407.15161v2",
      "title": "FFHFlow: A Flow-based Variational Approach for Learning Diverse Dexterous Grasps with Shape-Aware Introspection",
      "title_zh": "翻译失败",
      "authors": [
        "Qian Feng",
        "Jianxiang Feng",
        "Zhaopeng Chen",
        "Rudolph Triebel",
        "Alois Knoll"
      ],
      "abstract": "Synthesizing diverse dexterous grasps from uncertain partial observation is\nan important yet challenging task for physically intelligent embodiments.\nPrevious works on generative grasp synthesis fell short of precisely capturing\nthe complex grasp distribution and reasoning about shape uncertainty in the\nunstructured and often partially perceived reality. In this work, we introduce\na novel model that can generate diverse grasps for a multi-fingered hand while\nintrospectively handling perceptual uncertainty and recognizing unknown object\ngeometry to avoid performance degradation. Specifically, we devise a Deep\nLatent Variable Model (DLVM) based on Normalizing Flows (NFs), facilitating\nhierarchical and expressive latent representation for modeling versatile\ngrasps. Our model design counteracts typical pitfalls of its popular\nalternative in generative grasping, i.e., conditional Variational Autoencoders\n(cVAEs) whose performance is limited by mode collapse and miss-specified prior\nissues. Moreover, the resultant feature hierarchy and the exact flow likelihood\ncomputation endow our model with shape-aware introspective capabilities,\nenabling it to quantify the shape uncertainty of partial point clouds and\ndetect objects of novel geometry. We further achieve performance gain by fusing\nthis information with a discriminative grasp evaluator, facilitating a novel\nhybrid way for grasp evaluation. Comprehensive simulated and real-world\nexperiments show that the proposed idea gains superior performance and higher\nrun-time efficiency against strong baselines, including diffusion models. We\nalso demonstrate substantial benefits of greater diversity for grasping objects\nin clutter and a confined workspace in the real world.",
      "tldr_zh": "这篇论文提出了 FFHFlow，一种基于 Normalizing Flows (NFs) 的 Deep Latent Variable Model (DLVM)，用于从不确定的部分观察中合成多样化的灵巧抓取动作，同时通过 shape-aware introspection 处理形状不确定性和未知物体几何问题。相比传统的 conditional Variational Autoencoders (cVAEs)，该模型避免了模式崩溃和先验错误，提供更具层次性和表达力的潜在表示，并融合鉴别性抓取评估器以提升性能。实验结果显示，FFHFlow 在模拟和真实世界环境中优于基线模型如扩散模型，具有更高的运行效率和抓取多样性，尤其在杂乱环境中显著改善了物体抓取效果。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "First two authors contributed equally, whose ordering decided via\n  coin-tossing. Under Reivew",
      "pdf_url": "http://arxiv.org/pdf/2407.15161v2",
      "published_date": "2024-07-21 13:33:08 UTC",
      "updated_date": "2024-12-18 09:07:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:38:57.676850"
    },
    {
      "arxiv_id": "2407.15160v2",
      "title": "When Can Transformers Count to n?",
      "title_zh": "翻译失败",
      "authors": [
        "Gilad Yehudai",
        "Haim Kaplan",
        "Asma Ghandeharioun",
        "Mor Geva",
        "Amir Globerson"
      ],
      "abstract": "Large language models based on the transformer architectures can solve highly\ncomplex tasks. But are there simple tasks that such models cannot solve? Here\nwe focus on very simple counting tasks, that involve counting how many times a\ntoken in the vocabulary have appeared in a string. We show that if the\ndimension of the transformer state is linear in the context length, this task\ncan be solved. However, the solution we propose does not scale beyond this\nlimit, and we provide theoretical arguments for why it is likely impossible for\na size limited transformer to implement this task. Our empirical results\ndemonstrate the same phase-transition in performance, as anticipated by the\ntheoretical argument. Our results demonstrate the importance of understanding\nhow transformers can solve simple tasks.",
      "tldr_zh": "本研究探讨了Transformer模型在简单计数任务（例如统计词汇在字符串中的出现次数）上的能力。研究发现，如果Transformer状态的维度线性于上下文长度（context length），则该任务可被解决，但提出的解决方案无法扩展超出此限制。理论论证和实验结果显示，当模型尺寸受限时，性能会出现相变（phase-transition），表明Transformer可能无法处理此类任务。这些发现突出了理解Transformer如何解决简单任务的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15160v2",
      "published_date": "2024-07-21 13:31:02 UTC",
      "updated_date": "2024-10-07 13:19:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:39:06.473894"
    },
    {
      "arxiv_id": "2407.15155v1",
      "title": "Distilling Vision-Language Foundation Models: A Data-Free Approach via Prompt Diversification",
      "title_zh": "翻译失败",
      "authors": [
        "Yunyi Xuan",
        "Weijie Chen",
        "Shicai Yang",
        "Di Xie",
        "Luojun Lin",
        "Yueting Zhuang"
      ],
      "abstract": "Data-Free Knowledge Distillation (DFKD) has shown great potential in creating\na compact student model while alleviating the dependency on real training data\nby synthesizing surrogate data. However, prior arts are seldom discussed under\ndistribution shifts, which may be vulnerable in real-world applications. Recent\nVision-Language Foundation Models, e.g., CLIP, have demonstrated remarkable\nperformance in zero-shot out-of-distribution generalization, yet consuming\nheavy computation resources. In this paper, we discuss the extension of DFKD to\nVision-Language Foundation Models without access to the billion-level\nimage-text datasets. The objective is to customize a student model for\ndistribution-agnostic downstream tasks with given category concepts, inheriting\nthe out-of-distribution generalization capability from the pre-trained\nfoundation models. In order to avoid generalization degradation, the primary\nchallenge of this task lies in synthesizing diverse surrogate images driven by\ntext prompts. Since not only category concepts but also style information are\nencoded in text prompts, we propose three novel Prompt Diversification methods\nto encourage image synthesis with diverse styles, namely Mix-Prompt,\nRandom-Prompt, and Contrastive-Prompt. Experiments on out-of-distribution\ngeneralization datasets demonstrate the effectiveness of the proposed methods,\nwith Contrastive-Prompt performing the best.",
      "tldr_zh": "本论文提出了一种 Data-Free Knowledge Distillation (DFKD) 方法，用于蒸馏 Vision-Language Foundation Models（如 CLIP），以创建紧凑的学生模型，而无需依赖原始图像-文本数据集。方法通过三种 Prompt Diversification 技术（Mix-Prompt、Random-Prompt 和 Contrastive-Prompt）来合成多样风格的替代图像，解决分布偏移问题并继承预训练模型的出-of-distribution 泛化能力。实验结果表明，该方法在分布外泛化数据集上表现出色，其中 Contrastive-Prompt 性能最佳。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACMMM 2023",
      "pdf_url": "http://arxiv.org/pdf/2407.15155v1",
      "published_date": "2024-07-21 13:26:30 UTC",
      "updated_date": "2024-07-21 13:26:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:39:20.212748"
    },
    {
      "arxiv_id": "2407.15143v2",
      "title": "Rethinking Feature Backbone Fine-tuning for Remote Sensing Object Detection",
      "title_zh": "重新思考遥感目标检测中的特征主干网络微调",
      "authors": [
        "Yechan Kim",
        "JongHyun Park",
        "SooYeon Kim",
        "Moongu Jeon"
      ],
      "abstract": "Recently, numerous methods have achieved impressive performance in remote\nsensing object detection, relying on convolution or transformer architectures.\nSuch detectors typically have a feature backbone to extract useful features\nfrom raw input images. For the remote sensing domain, a common practice among\ncurrent detectors is to initialize the backbone with pre-training on ImageNet\nconsisting of natural scenes. Fine-tuning the backbone is then typically\nrequired to generate features suitable for remote-sensing images. However, this\ncould hinder the extraction of basic visual features in long-term training,\nthus restricting performance improvement. To mitigate this issue, we propose a\nnovel method named DBF (Dynamic Backbone Freezing) for feature backbone\nfine-tuning on remote sensing object detection. Our method aims to handle the\ndilemma of whether the backbone should extract low-level generic features or\npossess specific knowledge of the remote sensing domain, by introducing a\nmodule called 'Freezing Scheduler' to dynamically manage the update of backbone\nfeatures during training. Extensive experiments on DOTA and DIOR-R show that\nour approach enables more accurate model learning while substantially reducing\ncomputational costs. Our method can be seamlessly adopted without additional\neffort due to its straightforward design.",
      "tldr_zh": "本文重新审视了遥感物体检测中的特征骨干网络微调问题，指出传统方法使用ImageNet预训练后进行微调可能阻碍基本视觉特征的提取，从而限制性能提升。作者提出了一种新方法DBF（Dynamic Backbone Freezing），通过引入Freezing Scheduler模块动态管理骨干网络的更新，以平衡低级通用特征提取和遥感领域特定知识的需求。在DOTA和DIOR-R数据集上的广泛实验显示，该方法显著提高了模型准确性，同时降低了计算成本，且设计简单易于无缝集成。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2407.15143v2",
      "published_date": "2024-07-21 12:32:00 UTC",
      "updated_date": "2024-08-08 05:15:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:39:42.615915"
    },
    {
      "arxiv_id": "2407.15141v1",
      "title": "Text-Augmented Multimodal LLMs for Chemical Reaction Condition Recommendation",
      "title_zh": "文本增强的多模态 LLMs 用于化学反应条件推荐",
      "authors": [
        "Yu Zhang",
        "Ruijie Yu",
        "Kaipeng Zeng",
        "Ding Li",
        "Feng Zhu",
        "Xiaokang Yang",
        "Yaohui Jin",
        "Yanyan Xu"
      ],
      "abstract": "High-throughput reaction condition (RC) screening is fundamental to chemical\nsynthesis. However, current RC screening suffers from laborious and costly\ntrial-and-error workflows. Traditional computer-aided synthesis planning (CASP)\ntools fail to find suitable RCs due to data sparsity and inadequate reaction\nrepresentations. Nowadays, large language models (LLMs) are capable of tackling\nchemistry-related problems, such as molecule design, and chemical logic Q\\&A\ntasks. However, LLMs have not yet achieved accurate predictions of chemical\nreaction conditions. Here, we present MM-RCR, a text-augmented multimodal LLM\nthat learns a unified reaction representation from SMILES, reaction graphs, and\ntextual corpus for chemical reaction recommendation (RCR). To train MM-RCR, we\nconstruct 1.2 million pair-wised Q\\&A instruction datasets. Our experimental\nresults demonstrate that MM-RCR achieves state-of-the-art performance on two\nopen benchmark datasets and exhibits strong generalization capabilities on\nout-of-domain (OOD) and High-Throughput Experimentation (HTE) datasets. MM-RCR\nhas the potential to accelerate high-throughput condition screening in chemical\nsynthesis.",
      "tldr_zh": "该研究针对化学合成中的高通量反应条件（RC）筛选问题，提出了一种文本增强的多模态大型语言模型（MM-RCR），它从 SMILES、反应图和文本语料中学习统一的反应表示，以实现准确的化学反应条件推荐（RCR）。为了训练 MM-RCR，研究者构建了120万对问答指令数据集，解决了传统计算机辅助合成规划（CASP）工具的数据稀疏和表示不足的局限性。实验结果显示，MM-RCR 在两个公开基准数据集上达到最先进性能，并在领域外（OOD）和高通量实验（HTE）数据集上表现出色，具有加速化学合成筛选的潜力。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "physics.chem-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15141v1",
      "published_date": "2024-07-21 12:27:26 UTC",
      "updated_date": "2024-07-21 12:27:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:39:43.470091"
    },
    {
      "arxiv_id": "2407.15134v1",
      "title": "Proximal Policy Distillation",
      "title_zh": "近端策略蒸馏",
      "authors": [
        "Giacomo Spigler"
      ],
      "abstract": "We introduce Proximal Policy Distillation (PPD), a novel policy distillation\nmethod that integrates student-driven distillation and Proximal Policy\nOptimization (PPO) to increase sample efficiency and to leverage the additional\nrewards that the student policy collects during distillation. To assess the\nefficacy of our method, we compare PPD with two common alternatives,\nstudent-distill and teacher-distill, over a wide range of reinforcement\nlearning environments that include discrete actions and continuous control\n(ATARI, Mujoco, and Procgen). For each environment and method, we perform\ndistillation to a set of target student neural networks that are smaller,\nidentical (self-distillation), or larger than the teacher network. Our findings\nindicate that PPD improves sample efficiency and produces better student\npolicies compared to typical policy distillation approaches. Moreover, PPD\ndemonstrates greater robustness than alternative methods when distilling\npolicies from imperfect demonstrations. The code for the paper is released as\npart of a new Python library built on top of stable-baselines3 to facilitate\npolicy distillation: `sb3-distill'.",
      "tldr_zh": "本研究引入了Proximal Policy Distillation (PPD)，一种新型策略蒸馏方法，通过整合学生驱动的蒸馏和Proximal Policy Optimization (PPO)，提升样本效率并利用学生策略在蒸馏过程中的额外奖励。PPD 与传统方法如 student-distill 和 teacher-distill 在 ATARI、Mujoco 和 Procgen 等强化学习环境中进行比较，涵盖离散动作和连续控制任务，结果显示 PPD 生成的学生策略更优，尤其在处理不完美演示时表现出更高的鲁棒性。该方法还开源了 sb3-distill 库，以促进策略蒸馏的研究应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15134v1",
      "published_date": "2024-07-21 12:08:54 UTC",
      "updated_date": "2024-07-21 12:08:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:39:55.656479"
    },
    {
      "arxiv_id": "2407.15886v2",
      "title": "CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zheng Chong",
        "Xiao Dong",
        "Haoxiang Li",
        "Shiyue Zhang",
        "Wenqing Zhang",
        "Xujie Zhang",
        "Hanqing Zhao",
        "Dongmei Jiang",
        "Xiaodan Liang"
      ],
      "abstract": "Virtual try-on methods based on diffusion models achieve realistic effects\nbut often require additional encoding modules, a large number of training\nparameters, and complex preprocessing, which increases the burden on training\nand inference. In this work, we re-evaluate the necessity of additional modules\nand analyze how to improve training efficiency and reduce redundant steps in\nthe inference process. Based on these insights, we propose CatVTON, a simple\nand efficient virtual try-on diffusion model that transfers in-shop or worn\ngarments of arbitrary categories to target individuals by concatenating them\nalong spatial dimensions as inputs of the diffusion model. The efficiency of\nCatVTON is reflected in three aspects: (1) Lightweight network. CatVTON\nconsists only of a VAE and a simplified denoising UNet, removing redundant\nimage and text encoders as well as cross-attentions, and includes just 899.06M\nparameters. (2) Parameter-efficient training. Through experimental analysis, we\nidentify self-attention modules as crucial for adapting pre-trained diffusion\nmodels to the virtual try-on task, enabling high-quality results with only\n49.57M training parameters. (3) Simplified inference. CatVTON eliminates\nunnecessary preprocessing, such as pose estimation, human parsing, and\ncaptioning, requiring only a person image and garment reference to guide the\nvirtual try-on process, reducing over 49% memory usage compared to other\ndiffusion-based methods. Extensive experiments demonstrate that CatVTON\nachieves superior qualitative and quantitative results compared to baseline\nmethods and demonstrates strong generalization performance in in-the-wild\nscenarios, despite being trained solely on public datasets with 73K samples.",
      "tldr_zh": "本研究提出CatVTON，一种基于diffusion models的简单高效虚拟试穿模型，仅通过在空间维度上concatenation（连接）衣服和人体图像作为输入，即可实现任意类别衣服的转移，无需额外编码模块。CatVTON采用轻量级网络，包括VAE和简化denoising UNet，总参数仅899.06M，并通过仅训练self-attention模块（49.57M参数）实现参数高效训练，同时简化推理过程，省去姿势估计、人体解析和描述等预处理，减少49%内存使用。实验结果显示，CatVTON在定性和定量上优于基线方法，并在野外场景表现出强泛化性能，仅使用73K公共数据集样本训练。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T42 (Primary) 168T45 (Secondary)",
        "I.4.9"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.15886v2",
      "published_date": "2024-07-21 11:58:53 UTC",
      "updated_date": "2025-02-16 03:41:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:40:08.018018"
    },
    {
      "arxiv_id": "2407.15130v2",
      "title": "DOPRA: Decoding Over-accumulation Penalization and Re-allocation in Specific Weighting Layer",
      "title_zh": "翻译失败",
      "authors": [
        "Jinfeng Wei",
        "Xiaofeng Zhang"
      ],
      "abstract": "In this work, we introduce DOPRA, a novel approach designed to mitigate\nhallucinations in multi-modal large language models (MLLMs). Unlike existing\nsolutions that typically involve costly supplementary training data or the\nintegration of external knowledge sources, DOPRA innovatively addresses\nhallucinations by decoding specific weighted layer penalties and\nredistribution, offering an economical and effective solution without\nadditional resources. DOPRA is grounded in unique insights into the intrinsic\nmechanisms controlling hallucinations within MLLMs, especially the models'\ntendency to over-rely on a subset of summary tokens in the self-attention\nmatrix, neglecting critical image-related information. This phenomenon is\nparticularly pronounced in certain strata. To counteract this over-reliance,\nDOPRA employs a strategy of weighted overlay penalties and redistribution in\nspecific layers, such as the 12th layer, during the decoding process.\nFurthermore, DOPRA includes a retrospective allocation process that re-examines\nthe sequence of generated tokens, allowing the algorithm to reallocate token\nselection to better align with the actual image content, thereby reducing the\nincidence of hallucinatory descriptions in auto-generated captions. Overall,\nDOPRA represents a significant step forward in improving the output quality of\nMLLMs by systematically reducing hallucinations through targeted adjustments\nduring the decoding process.",
      "tldr_zh": "本研究引入了DOPRA，一种创新方法，用于减少多模态大语言模型(MLLMs)中的hallucinations问题，而无需额外的训练数据或外部知识源。DOPRA通过在特定权重层（如第12层）应用加权overlay penalties和redistribution，针对模型在self-attention matrix中过度依赖子集summary tokens的现象，重新分配标记选择以更好地整合图像相关信息。实验结果显示，该方法显著提高了MLLMs的输出质量，尤其在自动生成标题时，减少了hallucinations的发生。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15130v2",
      "published_date": "2024-07-21 11:54:49 UTC",
      "updated_date": "2024-07-23 09:30:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:40:19.103149"
    },
    {
      "arxiv_id": "2407.15124v2",
      "title": "Chemical Reaction Extraction from Long Patent Documents",
      "title_zh": "翻译失败",
      "authors": [
        "Aishwarya Jadhav",
        "Ritam Dutt"
      ],
      "abstract": "The task of searching through patent documents is crucial for chemical patent\nrecommendation and retrieval. This can be enhanced by creating a patent\nknowledge base (ChemPatKB) to aid in prior art searches and to provide a\nplatform for domain experts to explore new innovations in chemical compound\nsynthesis and use-cases. An essential foundational component of this KB is the\nextraction of important reaction snippets from long patents documents which\nfacilitates multiple downstream tasks such as reaction co-reference resolution\nand chemical entity role identification. In this work, we explore the problem\nof extracting reactions spans from chemical patents in order to create a\nreactions resource database. We formulate this task as a paragraph-level\nsequence tagging problem, where the system is required to return a sequence of\nparagraphs that contain a description of a reaction. We propose several\napproaches and modifications of the baseline models and study how different\nmethods generalize across different domains of chemical patents.",
      "tldr_zh": "本文探讨从长专利文档中提取化学反应的任务，以构建化学专利知识库（ChemPatKB），从而支持专利推荐、检索和领域专家的创新探索。作者将此任务表述为段落级别的序列标记问题（sequence tagging），要求系统返回包含反应描述的段落序列，并提出多种方法和基线模型的修改。研究重点在于评估这些方法的泛化能力，涵盖不同化学专利领域，以促进下游任务如反应共指解析和化学实体角色识别。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Work completed in 2022 at Carnegie Mellon University",
      "pdf_url": "http://arxiv.org/pdf/2407.15124v2",
      "published_date": "2024-07-21 11:27:27 UTC",
      "updated_date": "2024-07-23 07:11:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:40:33.573813"
    },
    {
      "arxiv_id": "2407.18271v4",
      "title": "Large Language Model for Verilog Generation with Code-Structure-Guided Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ning Wang",
        "Bingkun Yao",
        "Jie Zhou",
        "Xi Wang",
        "Zhe Jiang",
        "Nan Guan"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have sparked significant\ninterest in the automatic generation of Register Transfer Level (RTL) designs,\nparticularly using Verilog. Current research on this topic primarily focuses on\npre-training and instruction tuning, but the effectiveness of these methods is\nconstrained by the limited availability of training data, as public Verilog\ncode is far less abundant than software code. In particular, these methods\nstruggle to effectively capture Verilog parallel code structures, which\nfundamentally differ from the imperative, sequential control flow typical in\nmost software programming languages. This paper introduces VeriSeek, an LLM\nenhanced by reinforcement learning using a limited amount of high-quality\ntraining data to achieve high Verilog code generation performance. Our\nreinforcement learning approach employs code structure information as feedback\nsignals to refine the pre-trained model, enabling it to effectively learn\nimportant patterns from Verilog code with parallel structures. Experiments show\nthat VeriSeek outperforms state-of-the-art methods across multiple benchmarks.",
      "tldr_zh": "这篇论文提出了 VeriSeek，一种基于 Large Language Model (LLM) 的 Verilog 代码生成框架，利用 Code-Structure-Guided Reinforcement Learning 来解决现有方法因数据稀缺和 Verilog 的并行结构与软件代码顺序结构差异而带来的局限性。VeriSeek 通过少量高质量训练数据和代码结构信息作为反馈信号，优化预训练模型以更好地捕捉 Verilog 的关键模式。实验结果表明，该方法在多个基准测试中超过了最先进的技术。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18271v4",
      "published_date": "2024-07-21 11:25:21 UTC",
      "updated_date": "2025-04-19 09:25:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:40:43.497310"
    },
    {
      "arxiv_id": "2408.03335v1",
      "title": "Explainable AI-based Intrusion Detection System for Industry 5.0: An Overview of the Literature, associated Challenges, the existing Solutions, and Potential Research Directions",
      "title_zh": "翻译失败",
      "authors": [
        "Naseem Khan",
        "Kashif Ahmad",
        "Aref Al Tamimi",
        "Mohammed M. Alani",
        "Amine Bermak",
        "Issa Khalil"
      ],
      "abstract": "Industry 5.0, which focuses on human and Artificial Intelligence (AI)\ncollaboration for performing different tasks in manufacturing, involves a\nhigher number of robots, Internet of Things (IoTs) devices and\ninterconnections, Augmented/Virtual Reality (AR), and other smart devices. The\nhuge involvement of these devices and interconnection in various critical\nareas, such as economy, health, education and defense systems, poses several\ntypes of potential security flaws. AI itself has been proven a very effective\nand powerful tool in different areas of cybersecurity, such as intrusion\ndetection, malware detection, and phishing detection, among others. Just as in\nmany application areas, cybersecurity professionals were reluctant to accept\nblack-box ML solutions for cybersecurity applications. This reluctance pushed\nforward the adoption of eXplainable Artificial Intelligence (XAI) as a tool\nthat helps explain how decisions are made in ML-based systems. In this survey,\nwe present a comprehensive study of different XAI-based intrusion detection\nsystems for industry 5.0, and we also examine the impact of explainability and\ninterpretability on Cybersecurity practices through the lens of Adversarial\nXIDS (Adv-XIDS) approaches. Furthermore, we analyze the possible opportunities\nand challenges in XAI cybersecurity systems for industry 5.0 that elicit future\nresearch toward XAI-based solutions to be adopted by high-stakes industry 5.0\napplications. We believe this rigorous analysis will establish a foundational\nframework for subsequent research endeavors within the specified domain.",
      "tldr_zh": "这篇论文对 Industry 5.0 环境中基于 Explainable AI (XAI) 的入侵检测系统进行了全面综述，强调了人类与 AI 协作下的大量机器人、IoT 设备和 AR/VR 设备的应用所带来的安全风险。论文分析了现有文献中 XAI 在入侵检测、网络安全中的作用，包括 Adversarial XIDS (Adv-XIDS) 方法如何提升系统的可解释性和可解释性，同时探讨了面临的挑战和潜在机会。最终，它为未来 XAI-based 解决方案在高风险 Industry 5.0 应用中的研究提供了基础框架。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "57 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.03335v1",
      "published_date": "2024-07-21 09:28:05 UTC",
      "updated_date": "2024-07-21 09:28:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:40:57.203749"
    },
    {
      "arxiv_id": "2407.21040v1",
      "title": "Towards Automated Data Sciences with Natural Language and SageCopilot: Practices and Lessons Learned",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Liao",
        "Jiang Bian",
        "Yuhui Yun",
        "Shuo Wang",
        "Yubo Zhang",
        "Jiaming Chu",
        "Tao Wang",
        "Kewei Li",
        "Yuchen Li",
        "Xuhong Li",
        "Shilei Ji",
        "Haoyi Xiong"
      ],
      "abstract": "While the field of NL2SQL has made significant advancements in translating\nnatural language instructions into executable SQL scripts for data querying and\nprocessing, achieving full automation within the broader data science pipeline\n- encompassing data querying, analysis, visualization, and reporting - remains\na complex challenge. This study introduces SageCopilot, an advanced,\nindustry-grade system system that automates the data science pipeline by\nintegrating Large Language Models (LLMs), Autonomous Agents (AutoAgents), and\nLanguage User Interfaces (LUIs). Specifically, SageCopilot incorporates a\ntwo-phase design: an online component refining users' inputs into executable\nscripts through In-Context Learning (ICL) and running the scripts for results\nreporting & visualization, and an offline preparing demonstrations requested by\nICL in the online phase. A list of trending strategies such as Chain-of-Thought\nand prompt-tuning have been used to augment SageCopilot for enhanced\nperformance. Through rigorous testing and comparative analysis against\nprompt-based solutions, SageCopilot has been empirically validated to achieve\nsuperior end-to-end performance in generating or executing scripts and offering\nresults with visualization, backed by real-world datasets. Our in-depth\nablation studies highlight the individual contributions of various components\nand strategies used by SageCopilot to the end-to-end correctness for data\nsciences.",
      "tldr_zh": "这篇论文探讨了 NL2SQL 的进展，但强调了在数据科学管道（包括数据查询、分析、可视化和报告）中实现全自动化的挑战，并引入了 SageCopilot 系统作为解决方案。SageCopilot 整合了 Large Language Models (LLMs)、Autonomous Agents (AutoAgents) 和 Language User Interfaces (LUIs)，采用两阶段设计：在线组件通过 In-Context Learning (ICL) 精炼用户输入为可执行脚本并处理结果可视化，离线组件准备所需的演示，同时利用 Chain-of-Thought 和 prompt-tuning 等策略提升性能。通过实证测试和比较分析，SageCopilot 在生成、执行脚本和提供可视化结果方面表现出色，消融研究进一步证实了各组件对端到端正确性的贡献。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.DB",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21040v1",
      "published_date": "2024-07-21 08:58:18 UTC",
      "updated_date": "2024-07-21 08:58:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:41:09.294719"
    },
    {
      "arxiv_id": "2407.15089v1",
      "title": "Learning Physics for Unveiling Hidden Earthquake Ground Motions via Conditional Generative Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Pu Ren",
        "Rie Nakata",
        "Maxime Lacour",
        "Ilan Naiman",
        "Nori Nakata",
        "Jialin Song",
        "Zhengfa Bi",
        "Osman Asif Malik",
        "Dmitriy Morozov",
        "Omri Azencot",
        "N. Benjamin Erichson",
        "Michael W. Mahoney"
      ],
      "abstract": "Predicting high-fidelity ground motions for future earthquakes is crucial for\nseismic hazard assessment and infrastructure resilience. Conventional empirical\nsimulations suffer from sparse sensor distribution and geographically localized\nearthquake locations, while physics-based methods are computationally intensive\nand require accurate representations of Earth structures and earthquake\nsources. We propose a novel artificial intelligence (AI) simulator, Conditional\nGenerative Modeling for Ground Motion (CGM-GM), to synthesize high-frequency\nand spatially continuous earthquake ground motion waveforms. CGM-GM leverages\nearthquake magnitudes and geographic coordinates of earthquakes and sensors as\ninputs, learning complex wave physics and Earth heterogeneities, without\nexplicit physics constraints. This is achieved through a probabilistic\nautoencoder that captures latent distributions in the time-frequency domain and\nvariational sequential models for prior and posterior distributions. We\nevaluate the performance of CGM-GM using small-magnitude earthquake records\nfrom the San Francisco Bay Area, a region with high seismic risks. CGM-GM\ndemonstrates a strong potential for outperforming a state-of-the-art\nnon-ergodic empirical ground motion model and shows great promise in seismology\nand beyond.",
      "tldr_zh": "该研究针对地震地动预测的挑战，提出了一种新型AI模拟器Conditional Generative Modeling for Ground Motion (CGM-GM)，它利用地震震级和地理坐标作为输入，学习复杂的波物理和地球异质性，而无需显式物理约束。CGM-GM 采用概率自动编码器捕获时频域的潜在分布，并结合变分序列模型处理先验和后验分布，从而合成高频和空间连续的地动波形。在旧金山湾区小震记录的评估中，该方法优于最先进的非尔格德经验地动模型，展示了在地震学及相关领域的巨大应用潜力。",
      "categories": [
        "physics.geo-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15089v1",
      "published_date": "2024-07-21 08:23:37 UTC",
      "updated_date": "2024-07-21 08:23:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:41:20.196801"
    },
    {
      "arxiv_id": "2407.15086v1",
      "title": "MaxMI: A Maximal Mutual Information Criterion for Manipulation Concept Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Pei Zhou",
        "Yanchao Yang"
      ],
      "abstract": "We aim to discover manipulation concepts embedded in the unannotated\ndemonstrations, which are recognized as key physical states. The discovered\nconcepts can facilitate training manipulation policies and promote\ngeneralization. Current methods relying on multimodal foundation models for\nderiving key states usually lack accuracy and semantic consistency due to\nlimited multimodal robot data. In contrast, we introduce an\ninformation-theoretic criterion to characterize the regularities that signify a\nset of physical states. We also develop a framework that trains a concept\ndiscovery network using this criterion, thus bypassing the dependence on human\nsemantics and alleviating costly human labeling. The proposed criterion is\nbased on the observation that key states, which deserve to be conceptualized,\noften admit more physical constraints than non-key states. This phenomenon can\nbe formalized as maximizing the mutual information between the putative key\nstate and its preceding state, i.e., Maximal Mutual Information (MaxMI). By\nemploying MaxMI, the trained key state localization network can accurately\nidentify states of sufficient physical significance, exhibiting reasonable\nsemantic compatibility with human perception. Furthermore, the proposed\nframework produces key states that lead to concept-guided manipulation policies\nwith higher success rates and better generalization in various robotic tasks\ncompared to the baselines, verifying the effectiveness of the proposed\ncriterion.",
      "tldr_zh": "本研究旨在从未标注的演示中发现嵌入的操纵概念（key physical states），这些概念被视为关键物理状态，可用于提升操纵策略的训练和泛化能力。作者引入了基于信息理论的 Maximal Mutual Information (MaxMI) 标准，该标准通过最大化潜在关键状态与其前一个状态的 mutual information 来表征关键状态的物理约束，并开发了一个框架训练概念发现网络，从而避免依赖人类语义和昂贵标注。实验表明，该方法在各种机器人任务中比基线模型更准确地识别关键状态，并产生概念引导的操纵策略，具有更高的成功率和更好泛化。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15086v1",
      "published_date": "2024-07-21 07:56:48 UTC",
      "updated_date": "2024-07-21 07:56:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:41:33.312996"
    },
    {
      "arxiv_id": "2407.15078v1",
      "title": "Learning to Compile Programs to Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Logan Weber",
        "Jesse Michel",
        "Alex Renda",
        "Michael Carbin"
      ],
      "abstract": "A $\\textit{neural surrogate of a program}$ is a neural network that mimics\nthe behavior of a program. Researchers have used these neural surrogates to\nautomatically tune program inputs, adapt programs to new settings, and\naccelerate computations. Researchers traditionally develop neural surrogates by\ntraining on input-output examples from a single program. Alternatively,\nlanguage models trained on a large dataset including many programs can consume\nprogram text, to act as a neural surrogate. Using a language model to both\ngenerate a surrogate and act as a surrogate, however, leading to a trade-off\nbetween resource consumption and accuracy. We present $\\textit{neural surrogate\ncompilation}$, a technique for producing neural surrogates directly from\nprogram text without coupling neural surrogate generation and execution. We\nimplement neural surrogate compilers using hypernetworks trained on a dataset\nof C programs and find that they produce neural surrogates that are\n$1.9$-$9.5\\times$ as data-efficient, produce visual results that are\n$1.0$-$1.3\\times$ more similar to ground truth, and train in $4.3$-$7.3\\times$\nfewer epochs than neural surrogates trained from scratch.",
      "tldr_zh": "这篇论文探讨了 neural surrogate of a program，即使用神经网络模拟程序行为的技术，以优化程序输入调整、适应性和计算加速。作者提出了 neural surrogate compilation 方法，通过训练 hypernetworks 于 C 程序数据集，直接从程序文本生成神经代理，从而避免了生成和执行耦合带来的资源与准确性权衡。与传统方法相比，该技术使神经代理的数据效率提高了 1.9-9.5 倍，视觉结果相似度提高了 1.0-1.3 倍，并将训练 epoch 减少了 4.3-7.3 倍。该方法为高效程序模拟提供了新途径，提升了实际应用的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15078v1",
      "published_date": "2024-07-21 07:04:52 UTC",
      "updated_date": "2024-07-21 07:04:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:41:46.599194"
    },
    {
      "arxiv_id": "2407.15073v3",
      "title": "Multi-Agent Causal Discovery Using Large Language Models",
      "title_zh": "利用大型语言模型的多智能体因果发现",
      "authors": [
        "Hao Duong Le",
        "Xin Xia",
        "Zhang Chen"
      ],
      "abstract": "Causal discovery aims to identify causal relationships between variables and\nis a critical research area in machine learning. Traditional methods focus on\nstatistical or machine learning algorithms to uncover causal links from\nstructured data, often overlooking the valuable contextual information provided\nby metadata. Large language models (LLMs) have shown promise in creating\nunified causal discovery frameworks by incorporating both structured data and\nmetadata. However, their potential in multi-agent settings remains largely\nunexplored. To address this gap, we introduce the Multi-Agent Causal Discovery\nFramework (MAC), which consists of two key modules: the Debate-Coding Module\n(DCM) and the Meta-Debate Module (MDM). The DCM begins with a multi-agent\ndebating and coding process, where agents use both structured data and metadata\nto collaboratively select the most suitable statistical causal discovery (SCD)\nmethod. The selected SCD is then applied to the structured data to generate an\ninitial causal graph. This causal graph is transformed into causal metadata\nthrough the Meta Fusion mechanism. With all the metadata, MDM then refines the\ncausal structure by leveraging a multi-agent debating framework. Extensive\nexperiments across five datasets demonstrate that MAC outperforms both\ntraditional statistical causal discovery methods and existing LLM-based\napproaches, achieving state-of-the-art performance.",
      "tldr_zh": "该论文提出 Multi-Agent Causal Discovery Framework (MAC)，一种利用 Large Language Models (LLMs) 的多智能体框架，用于整合结构化数据和元数据以提升因果发现的准确性。框架包括 Debate-Coding Module (DCM)，通过多智能体辩论和编码过程选择合适的 statistical causal discovery (SCD) 方法生成初始因果图，并将其转化为因果元数据；随后，Meta-Debate Module (MDM) 通过多智能体辩论进一步优化因果结构。实验结果显示，在五个数据集上，MAC 超过了传统方法和现有 LLM 基于的方法，实现了最先进性能。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15073v3",
      "published_date": "2024-07-21 06:21:47 UTC",
      "updated_date": "2025-02-24 02:47:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:41:57.321097"
    },
    {
      "arxiv_id": "2407.15060v1",
      "title": "MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yun-Han Lan",
        "Wen-Yi Hsiao",
        "Hao-Chung Cheng",
        "Yi-Hsuan Yang"
      ],
      "abstract": "Existing text-to-music models can produce high-quality audio with great\ndiversity. However, textual prompts alone cannot precisely control temporal\nmusical features such as chords and rhythm of the generated music. To address\nthis challenge, we introduce MusiConGen, a temporally-conditioned\nTransformer-based text-to-music model that builds upon the pretrained MusicGen\nframework. Our innovation lies in an efficient finetuning mechanism, tailored\nfor consumer-grade GPUs, that integrates automatically-extracted rhythm and\nchords as the condition signal. During inference, the condition can either be\nmusical features extracted from a reference audio signal, or be user-defined\nsymbolic chord sequence, BPM, and textual prompts. Our performance evaluation\non two datasets -- one derived from extracted features and the other from\nuser-created inputs -- demonstrates that MusiConGen can generate realistic\nbacking track music that aligns well with the specified conditions. We\nopen-source the code and model checkpoints, and provide audio examples online,\nhttps://musicongen.github.io/musicongen_demo/.",
      "tldr_zh": "本文提出 MusiConGen，一种基于 Transformer 的文本到音乐生成模型，扩展自预训练框架 MusicGen，以解决文本提示无法精确控制音乐节奏和和弦的问题。该模型采用高效微调机制，针对消费级 GPU 整合自动提取的节奏和和弦作为条件信号，支持使用参考音频特征或用户定义的和弦序列、BPM 和文本提示进行生成。在两个数据集上的评估显示，MusiConGen 能生成与指定条件高度一致的真实伴奏音乐，显著提升了控制精度。作者开源了代码、模型检查点，并提供了在线音频示例。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by the 25th International Society for Music Information\n  Retrieval (ISMIR)",
      "pdf_url": "http://arxiv.org/pdf/2407.15060v1",
      "published_date": "2024-07-21 05:27:53 UTC",
      "updated_date": "2024-07-21 05:27:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:42:09.308676"
    },
    {
      "arxiv_id": "2407.15050v1",
      "title": "Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts",
      "title_zh": "Arondight：利用自动生成的多",
      "authors": [
        "Yi Liu",
        "Chengjun Cai",
        "Xiaoli Zhang",
        "Xingliang Yuan",
        "Cong Wang"
      ],
      "abstract": "Large Vision Language Models (VLMs) extend and enhance the perceptual\nabilities of Large Language Models (LLMs). Despite offering new possibilities\nfor LLM applications, these advancements raise significant security and ethical\nconcerns, particularly regarding the generation of harmful content. While LLMs\nhave undergone extensive security evaluations with the aid of red teaming\nframeworks, VLMs currently lack a well-developed one. To fill this gap, we\nintroduce Arondight, a standardized red team framework tailored specifically\nfor VLMs. Arondight is dedicated to resolving issues related to the absence of\nvisual modality and inadequate diversity encountered when transitioning\nexisting red teaming methodologies from LLMs to VLMs. Our framework features an\nautomated multi-modal jailbreak attack, wherein visual jailbreak prompts are\nproduced by a red team VLM, and textual prompts are generated by a red team LLM\nguided by a reinforcement learning agent. To enhance the comprehensiveness of\nVLM security evaluation, we integrate entropy bonuses and novelty reward\nmetrics. These elements incentivize the RL agent to guide the red team LLM in\ncreating a wider array of diverse and previously unseen test cases. Our\nevaluation of ten cutting-edge VLMs exposes significant security\nvulnerabilities, particularly in generating toxic images and aligning\nmulti-modal prompts. In particular, our Arondight achieves an average attack\nsuccess rate of 84.5\\% on GPT-4 in all fourteen prohibited scenarios defined by\nOpenAI in terms of generating toxic text. For a clearer comparison, we also\ncategorize existing VLMs based on their safety levels and provide corresponding\nreinforcement recommendations. Our multimodal prompt dataset and red team code\nwill be released after ethics committee approval. CONTENT WARNING: THIS PAPER\nCONTAINS HARMFUL MODEL RESPONSES.",
      "tldr_zh": "该研究引入了 Arondight，一种针对大型视觉语言模型 (VLMs) 的标准化红队框架，用于评估其安全性和伦理风险，特别是防止生成有害内容。Arondight 通过自动化多模态越狱攻击来解决从大型语言模型 (LLMs) 过渡到 VLMs 时存在的视觉模式缺失和多样性不足问题，其中红队 VLM 生成视觉提示，红队 LLM 在强化学习代理的指导下生成文本提示，并整合熵奖励和新颖性奖励以鼓励生成更多样化的测试案例。在对十个先进 VLMs 的评估中，Arondight 暴露了显著的安全漏洞，如在 GPT-4 上平均攻击成功率达 84.5%，并根据安全级别对 VLMs 进行了分类并提供了强化建议。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "To be published in ACM MM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.15050v1",
      "published_date": "2024-07-21 04:37:11 UTC",
      "updated_date": "2024-07-21 04:37:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:42:22.905680"
    },
    {
      "arxiv_id": "2407.15042v1",
      "title": "MedSAGa: Few-shot Memory Efficient Medical Image Segmentation using Gradient Low-Rank Projection in SAM",
      "title_zh": "翻译失败",
      "authors": [
        "Navyansh Mahla",
        "Annie D'souza",
        "Shubh Gupta",
        "Bhavik Kanekar",
        "Kshitij Sharad Jadhav"
      ],
      "abstract": "The application of large-scale models in medical image segmentation demands\nsubstantial quantities of meticulously annotated data curated by experts along\nwith high computational resources, both of which are challenges in\nresource-poor settings. In this study, we present the Medical Segment Anything\nModel with Galore MedSAGa where we adopt the Segment Anything Model (SAM) to\nachieve memory-efficient, few-shot medical image segmentation by applying\nGradient Low-Rank Projection GaLore to the parameters of the image encoder of\nSAM. Meanwhile, the weights of the prompt encoder and mask decoder undergo full\nparameter fine-tuning using standard optimizers. We further assess MedSAGa's\nfew-shot learning capabilities, reporting on its memory efficiency and\nsegmentation performance across multiple standard medical image segmentation\ndatasets. We compare it with several baseline models, including LoRA fine-tuned\nSAM (SAMed) and DAE-Former. Experiments across multiple datasets and these\nbaseline models with different number of images for fine tuning demonstrated\nthat the GPU memory consumption of MedSAGa is significantly less than that of\nthe baseline models, achieving an average memory efficiency of 66% more than\ncurrent state-of-the-art (SOTA) models for medical image segmentation. The\ncombination of substantially lower memory requirements and comparable to SOTA\nresults in few-shot learning for medical image segmentation positions MedSAGa\nas an optimal solution for deployment in resource-constrained settings.",
      "tldr_zh": "本研究提出 MedSAGa，一种基于 Segment Anything Model (SAM) 的少样本医疗图像分割方法，通过应用 Gradient Low-Rank Projection (GaLore) 对 SAM 的图像编码器参数进行优化，实现高效内存使用，同时对提示编码器和掩码解码器进行全参数微调，以适应资源有限的环境。实验结果显示，MedSAGa 在多个标准医疗图像分割数据集上，比基线模型如 SAMed 和 DAE-Former 节省了显著的 GPU 内存，平均内存效率比当前最先进 (SOTA) 模型高出 66%。这种结合少样本学习能力的优化，使 MedSAGa 成为资源受限场景下理想的图像分割解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15042v1",
      "published_date": "2024-07-21 03:34:49 UTC",
      "updated_date": "2024-07-21 03:34:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:42:33.326367"
    },
    {
      "arxiv_id": "2407.15041v1",
      "title": "Self-training Room Layout Estimation via Geometry-aware Ray-casting",
      "title_zh": "翻译失败",
      "authors": [
        "Bolivar Solarte",
        "Chin-Hsuan Wu",
        "Jin-Cheng Jhang",
        "Jonathan Lee",
        "Yi-Hsuan Tsai",
        "Min Sun"
      ],
      "abstract": "In this paper, we introduce a novel geometry-aware self-training framework\nfor room layout estimation models on unseen scenes with unlabeled data. Our\napproach utilizes a ray-casting formulation to aggregate multiple estimates\nfrom different viewing positions, enabling the computation of reliable\npseudo-labels for self-training. In particular, our ray-casting approach\nenforces multi-view consistency along all ray directions and prioritizes\nspatial proximity to the camera view for geometry reasoning. As a result, our\ngeometry-aware pseudo-labels effectively handle complex room geometries and\noccluded walls without relying on assumptions such as Manhattan World or planar\nroom walls. Evaluation on publicly available datasets, including synthetic and\nreal-world scenarios, demonstrates significant improvements in current\nstate-of-the-art layout models without using any human annotation.",
      "tldr_zh": "本文提出了一种基于几何感知射线投射(Geometry-aware Ray-casting)的自训练框架，用于在无标签数据上估计房间布局，从而适应未见场景。  \n该框架通过聚合不同视角的多个估计来计算可靠的伪标签(pseudo-labels)，并强制执行多视图一致性(multi-view consistency)以及优先考虑与相机视图的空间接近性，以进行精确的几何推理。  \n与传统方法不同，它能有效处理复杂房间几何和遮挡墙壁，而不依赖于Manhattan World或平面墙壁等假设。  \n在公开数据集（包括合成和真实场景）上的评估显示，该方法显著提升了现有最先进布局模型的性能，且无需任何人工标注。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ECCV-2024",
      "pdf_url": "http://arxiv.org/pdf/2407.15041v1",
      "published_date": "2024-07-21 03:25:55 UTC",
      "updated_date": "2024-07-21 03:25:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:42:46.942105"
    },
    {
      "arxiv_id": "2407.15036v1",
      "title": "AsyCo: An Asymmetric Dual-task Co-training Model for Partial-label Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Beibei Li",
        "Yiyuan Zheng",
        "Beihong Jin",
        "Tao Xiang",
        "Haobo Wang",
        "Lei Feng"
      ],
      "abstract": "Partial-Label Learning (PLL) is a typical problem of weakly supervised\nlearning, where each training instance is annotated with a set of candidate\nlabels. Self-training PLL models achieve state-of-the-art performance but\nsuffer from error accumulation problem caused by mistakenly disambiguated\ninstances. Although co-training can alleviate this issue by training two\nnetworks simultaneously and allowing them to interact with each other, most\nexisting co-training methods train two structurally identical networks with the\nsame task, i.e., are symmetric, rendering it insufficient for them to correct\neach other due to their similar limitations. Therefore, in this paper, we\npropose an asymmetric dual-task co-training PLL model called AsyCo, which\nforces its two networks, i.e., a disambiguation network and an auxiliary\nnetwork, to learn from different views explicitly by optimizing distinct tasks.\nSpecifically, the disambiguation network is trained with self-training PLL task\nto learn label confidence, while the auxiliary network is trained in a\nsupervised learning paradigm to learn from the noisy pairwise similarity labels\nthat are constructed according to the learned label confidence. Finally, the\nerror accumulation problem is mitigated via information distillation and\nconfidence refinement. Extensive experiments on both uniform and\ninstance-dependent partially labeled datasets demonstrate the effectiveness of\nAsyCo. The code is available at https://github.com/libeibeics/AsyCo.",
      "tldr_zh": "本文提出 AsyCo，一种不对称双任务共训练模型，用于 Partial-Label Learning (PLL)，旨在解决现有自训练方法中的错误积累问题，通过训练两个结构不同的网络来相互纠正。AsyCo 包括一个 disambiguation network（负责自训练 PLL 任务，学习标签置信度）和一个 auxiliary network（通过监督学习基于 noisy pairwise similarity labels 进行训练），并利用 information distillation 和 confidence refinement 来优化标签预测。实验结果显示，该模型在 uniform 和 instance-dependent 部分标记数据集上表现出色，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, accepted by Science China, Information Science",
      "pdf_url": "http://arxiv.org/pdf/2407.15036v1",
      "published_date": "2024-07-21 02:08:51 UTC",
      "updated_date": "2024-07-21 02:08:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:42:58.492781"
    },
    {
      "arxiv_id": "2407.15018v2",
      "title": "Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice Questions",
      "title_zh": "翻译失败",
      "authors": [
        "Sarah Wiegreffe",
        "Oyvind Tafjord",
        "Yonatan Belinkov",
        "Hannaneh Hajishirzi",
        "Ashish Sabharwal"
      ],
      "abstract": "Multiple-choice question answering (MCQA) is a key competence of performant\ntransformer language models that is tested by mainstream benchmarks. However,\nrecent evidence shows that models can have quite a range of performance,\nparticularly when the task format is diversified slightly (such as by shuffling\nanswer choice order). In this work we ask: how do successful models perform\nformatted MCQA? We employ vocabulary projection and activation patching methods\nto localize key hidden states that encode relevant information for predicting\nthe correct answer. We find that the prediction of a specific answer symbol is\ncausally attributed to a few middle layers, and specifically their multi-head\nself-attention mechanisms. We show that subsequent layers increase the\nprobability of the predicted answer symbol in vocabulary space, and that this\nprobability increase is associated with a sparse set of attention heads with\nunique roles. We additionally uncover differences in how different models\nadjust to alternative symbols. Finally, we demonstrate that a synthetic task\ncan disentangle sources of model error to pinpoint when a model has learned\nformatted MCQA, and show that logit differences between answer choice tokens\ncontinue to grow over the course of training.",
      "tldr_zh": "本研究探讨了语言模型（LMs）在多选题（MCQA）任务中的表现及其变异，特别是当任务格式（如答案选项顺序）发生轻微变化时。研究采用词汇投影和激活修补方法，定位关键隐藏状态，发现预测正确答案主要归因于中间层的多头自注意力（multi-head self-attention）机制，而后续层通过一组稀疏注意力头增强答案符号的概率。论文还揭示了不同模型对替代符号的调整差异，并使用合成任务来区分模型错误来源，证明LMs在训练过程中逐步掌握格式化MCQA，最终使答案选择的对数差异持续增长。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025 (spotlight). Substantially updated from previous preprint\n  to contain experiments on 4-way multiple-choice with various answer choice\n  symbols, 3 open model families, and extensive activation patching results,\n  including on individual attention heads",
      "pdf_url": "http://arxiv.org/pdf/2407.15018v2",
      "published_date": "2024-07-21 00:10:23 UTC",
      "updated_date": "2025-03-07 22:41:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T08:43:10.770044"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 45,
  "processed_papers_count": 45,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T08:43:33.670508"
}
[
  {
    "arxiv_id": "2409.02337v1",
    "title": "Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback",
    "authors": [
      "Deepak Raina",
      "Mythra V. Balakuntala",
      "Byung Wook Kim",
      "Juan Wachs",
      "Richard Voyles"
    ],
    "abstract": "Ultrasound is widely employed for clinical intervention and diagnosis, due to\nits advantages of offering non-invasive, radiation-free, and real-time imaging.\nHowever, the accessibility of this dexterous procedure is limited due to the\nsubstantial training and expertise required of operators. The robotic\nultrasound (RUS) offers a viable solution to address this limitation;\nnonetheless, achieving human-level proficiency remains challenging. Learning\nfrom demonstrations (LfD) methods have been explored in RUS, which learns the\npolicy prior from a dataset of offline demonstrations to encode the mental\nmodel of the expert sonographer. However, active engagement of experts, i.e.\nCoaching, during the training of RUS has not been explored thus far. Coaching\nis known for enhancing efficiency and performance in human training. This paper\nproposes a coaching framework for RUS to amplify its performance. The framework\ncombines DRL (self-supervised practice) with sparse expert's feedback through\ncoaching. The DRL employs an off-policy Soft Actor-Critic (SAC) network, with a\nreward based on image quality rating. The coaching by experts is modeled as a\nPartially Observable Markov Decision Process (POMDP), which updates the policy\nparameters based on the correction by the expert. The validation study on\nphantoms showed that coaching increases the learning rate by $25\\%$ and the\nnumber of high-quality image acquisition by $74.5\\%$.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted in IEEE Transactions on Medical Robotics and Bionics (TMRB)\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2409.02337v1",
    "published_date": "2024-09-03 23:52:33 UTC",
    "updated_date": "2024-09-03 23:52:33 UTC"
  },
  {
    "arxiv_id": "2409.15290v1",
    "title": "Broadening Access to Simulations for End-Users via Large Language Models: Challenges and Opportunities",
    "authors": [
      "Philippe J. Giabbanelli",
      "Jose J. Padilla",
      "Ameeta Agrawal"
    ],
    "abstract": "Large Language Models (LLMs) are becoming ubiquitous to create intelligent\nvirtual assistants that assist users in interacting with a system, as\nexemplified in marketing. Although LLMs have been discussed in Modeling &\nSimulation (M&S), the community has focused on generating code or explaining\nresults. We examine the possibility of using LLMs to broaden access to\nsimulations, by enabling non-simulation end-users to ask what-if questions in\neveryday language. Specifically, we discuss the opportunities and challenges in\ndesigning such an end-to-end system, divided into three broad phases. First,\nassuming the general case in which several simulation models are available,\ntextual queries are mapped to the most relevant model. Second, if a mapping\ncannot be found, the query can be automatically reformulated and clarifying\nquestions can be generated. Finally, simulation results are produced and\ncontextualized for decision-making. Our vision for such system articulates\nlong-term research opportunities spanning M&S, LLMs, information retrieval, and\nethics.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "To appear in proceedings of the 2024 Winter Simulation Conference",
    "pdf_url": "http://arxiv.org/pdf/2409.15290v1",
    "published_date": "2024-09-03 23:14:42 UTC",
    "updated_date": "2024-09-03 23:14:42 UTC"
  },
  {
    "arxiv_id": "2409.02326v1",
    "title": "Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining",
    "authors": [
      "Yuxiang Wei",
      "Hojae Han",
      "Rajhans Samdani"
    ],
    "abstract": "Recent studies have been increasingly demonstrating that high-quality data is\ncrucial for effective pretraining of language models. However, the precise\ndefinition of \"high-quality\" remains underexplored. Focusing on the code\ndomain, we introduce Arctic-SnowCoder-1.3B, a data-efficient base code model\npretrained on 555B tokens through three phases of progressively refined data:\n(1) general pretraining with 500B standard-quality code tokens, preprocessed\nthrough basic filtering, deduplication, and decontamination, (2) continued\npretraining with 50B high-quality tokens, selected from phase one by a\nBERT-style quality annotator trained to distinguish good code from random data,\nusing positive examples drawn from high-quality code files, along with\ninstruction data from Magicoder and StarCoder2-Instruct, and (3) enhanced\npretraining with 5B synthetic data created by Llama-3.1-70B using phase two\ndata as seeds, adapting the Magicoder approach for pretraining. Despite being\ntrained on a limited dataset, Arctic-SnowCoder achieves state-of-the-art\nperformance on BigCodeBench, a coding benchmark focusing on practical and\nchallenging programming tasks, compared to similarly sized models trained on no\nmore than 1T tokens, outperforming Phi-1.5-1.3B by 36%. Across all evaluated\nbenchmarks, Arctic-SnowCoder-1.3B beats StarCoderBase-3B pretrained on 1T\ntokens. Additionally, it matches the performance of leading small base code\nmodels trained on trillions of tokens. For example, Arctic-SnowCoder-1.3B\nsurpasses StarCoder2-3B, pretrained on over 3.3T tokens, on HumanEval+, a\nbenchmark that evaluates function-level code generation, and remains\ncompetitive on BigCodeBench. Our evaluation presents a comprehensive analysis\njustifying various design choices for Arctic-SnowCoder. Most importantly, we\nfind that the key to high-quality data is its alignment with the distribution\nof downstream applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02326v1",
    "published_date": "2024-09-03 22:36:42 UTC",
    "updated_date": "2024-09-03 22:36:42 UTC"
  },
  {
    "arxiv_id": "2409.02322v2",
    "title": "TimeDiT: General-purpose Diffusion Transformers for Time Series Foundation Model",
    "authors": [
      "Defu Cao",
      "Wen Ye",
      "Yizhou Zhang",
      "Yan Liu"
    ],
    "abstract": "Foundation models, particularly Large Language Models (LLMs), have\nrevolutionized text and video processing, yet time series data presents\ndistinct challenges for such approaches due to domain-specific features such as\nmissing values, multi-resolution characteristics, etc. Furthermore, the\nde-facto autoregressive transformers tend to learn deterministic temporal\ndependencies within pre-trained data while overlooking inherent uncertainties\nand lacking integration of physical constraints. In this paper, we introduce\nTimeDiT, a diffusion transformer model that synergistically combines\ntransformer-based temporal dependency learning with diffusion-based\nprobabilistic sampling. TimeDiT employs a unified masking mechanism to\nharmonize the training and inference process across diverse tasks while\nintroducing a theoretically grounded, finetuning-free model editing strategy\nthat enables flexible integration of external knowledge during sampling.\nAcknowledging the challenges of unifying multiple downstream tasks under a\nsingle model, our systematic evaluation demonstrates TimeDiT's effectiveness\nboth in fundamental tasks, i.e., forecasting and imputation, through\nzero-shot/fine-tuning; and in domain tasks, i.e., multi-resolution forecasting,\nanomaly detection, and data generation, establishing it as a\n\\textit{proto-foundation model} that bridges the gap between general-purpose\nand domain-specific models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "31 Pages, 11 Figures, 22 Tables. First present at ICML 2024 Workshop\n  on Foundation Models in the Wild",
    "pdf_url": "http://arxiv.org/pdf/2409.02322v2",
    "published_date": "2024-09-03 22:31:57 UTC",
    "updated_date": "2025-02-11 00:53:58 UTC"
  },
  {
    "arxiv_id": "2409.02313v2",
    "title": "On the Benefits of Memory for Modeling Time-Dependent PDEs",
    "authors": [
      "Ricardo Buitrago Ruiz",
      "Tanya Marwah",
      "Albert Gu",
      "Andrej Risteski"
    ],
    "abstract": "Data-driven techniques have emerged as a promising alternative to traditional\nnumerical methods for solving PDEs. For time-dependent PDEs, many approaches\nare Markovian -- the evolution of the trained system only depends on the\ncurrent state, and not the past states. In this work, we investigate the\nbenefits of using memory for modeling time-dependent PDEs: that is, when past\nstates are explicitly used to predict the future. Motivated by the Mori-Zwanzig\ntheory of model reduction, we theoretically exhibit examples of simple (even\nlinear) PDEs, in which a solution that uses memory is arbitrarily better than a\nMarkovian solution. Additionally, we introduce Memory Neural Operator (MemNO),\na neural operator architecture that combines recent state space models\n(specifically, S4) and Fourier Neural Operators (FNOs) to effectively model\nmemory. We empirically demonstrate that when the PDEs are supplied in low\nresolution or contain observation noise at train and test time, MemNO\nsignificantly outperforms the baselines without memory -- with up to 6x\nreduction in test error. Furthermore, we show that this benefit is particularly\npronounced when the PDE solutions have significant high-frequency Fourier modes\n(e.g., low-viscosity fluid dynamics) and we construct a challenging benchmark\ndataset consisting of such PDEs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02313v2",
    "published_date": "2024-09-03 21:56:13 UTC",
    "updated_date": "2025-04-24 15:16:21 UTC"
  },
  {
    "arxiv_id": "2409.15289v1",
    "title": "The Computational Mechanisms of Detached Mindfulness",
    "authors": [
      "Brendan Conway-Smith",
      "Robert L. West"
    ],
    "abstract": "This paper investigates the computational mechanisms underlying a type of\nmetacognitive monitoring known as detached mindfulness, a particularly\neffective therapeutic technique within cognitive psychology. While research\nstrongly supports the capacity of detached mindfulness to reduce depression and\nanxiety, its cognitive and computational underpinnings remain largely\nunexplained. We employ a computational model of metacognitive skill to\narticulate the mechanisms through which a detached perception of affect reduces\nemotional reactivity.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "International Conference on Cognitive Modeling (ICCM 2024)\n  https://mathpsych.org/presentation/1634#/abstract",
    "pdf_url": "http://arxiv.org/pdf/2409.15289v1",
    "published_date": "2024-09-03 21:30:41 UTC",
    "updated_date": "2024-09-03 21:30:41 UTC"
  },
  {
    "arxiv_id": "2409.02302v1",
    "title": "Speech Foundation Model Ensembles for the Controlled Singing Voice Deepfake Detection (CtrSVDD) Challenge 2024",
    "authors": [
      "Anmol Guragain",
      "Tianchi Liu",
      "Zihan Pan",
      "Hardik B. Sailor",
      "Qiongqiong Wang"
    ],
    "abstract": "This work details our approach to achieving a leading system with a 1.79%\npooled equal error rate (EER) on the evaluation set of the Controlled Singing\nVoice Deepfake Detection (CtrSVDD). The rapid advancement of generative AI\nmodels presents significant challenges for detecting AI-generated deepfake\nsinging voices, attracting increased research attention. The Singing Voice\nDeepfake Detection (SVDD) Challenge 2024 aims to address this complex task. In\nthis work, we explore the ensemble methods, utilizing speech foundation models\nto develop robust singing voice anti-spoofing systems. We also introduce a\nnovel Squeeze-and-Excitation Aggregation (SEA) method, which efficiently and\neffectively integrates representation features from the speech foundation\nmodels, surpassing the performance of our other individual systems. Evaluation\nresults confirm the efficacy of our approach in detecting deepfake singing\nvoices. The codes can be accessed at https://github.com/Anmol2059/SVDD2024.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to the IEEE Spoken Language Technology Workshop (SLT) 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.02302v1",
    "published_date": "2024-09-03 21:28:45 UTC",
    "updated_date": "2024-09-03 21:28:45 UTC"
  },
  {
    "arxiv_id": "2409.02291v1",
    "title": "Initial Development and Evaluation of the Creative Artificial Intelligence through Recurring Developments and Determinations (CAIRDD) System",
    "authors": [
      "Jeremy Straub",
      "Zach Johnson"
    ],
    "abstract": "Computer system creativity is a key step on the pathway to artificial general\nintelligence (AGI). It is elusive, however, due to the fact that human\ncreativity is not fully understood and, thus, it is difficult to develop this\ncapability in software. Large language models (LLMs) provide a facsimile of\ncreativity and the appearance of sentience, while not actually being either\ncreative or sentient. While LLMs have created bona fide new content, in some\ncases - such as with harmful hallucinations - inadvertently, their deliberate\ncreativity is seen by some to not match that of humans. In response to this\nchallenge, this paper proposes a technique for enhancing LLM output creativity\nvia an iterative process of concept injection and refinement. Initial work on\nthe development of the Creative Artificial Intelligence through Recurring\nDevelopments and Determinations (CAIRDD) system is presented and the efficacy\nof key system components is evaluated.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02291v1",
    "published_date": "2024-09-03 21:04:07 UTC",
    "updated_date": "2024-09-03 21:04:07 UTC"
  },
  {
    "arxiv_id": "2409.02284v1",
    "title": "Biochemical Prostate Cancer Recurrence Prediction: Thinking Fast & Slow",
    "authors": [
      "Suhang You",
      "Sanyukta Adap",
      "Siddhesh Thakur",
      "Bhakti Baheti",
      "Spyridon Bakas"
    ],
    "abstract": "Time to biochemical recurrence in prostate cancer is essential for prognostic\nmonitoring of the progression of patients after prostatectomy, which assesses\nthe efficacy of the surgery. In this work, we proposed to leverage multiple\ninstance learning through a two-stage ``thinking fast \\& slow'' strategy for\nthe time to recurrence (TTR) prediction. The first (``thinking fast'') stage\nfinds the most relevant WSI area for biochemical recurrence and the second\n(``thinking slow'') stage leverages higher resolution patches to predict TTR.\nOur approach reveals a mean C-index ($Ci$) of 0.733 ($\\theta=0.059$) on our\ninternal validation and $Ci=0.603$ on the LEOPARD challenge validation set.\nPost hoc attention visualization shows that the most attentive area contributes\nto the TTR prediction.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T10",
      "I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 3 figures, methodology paper for LEOPRARD Challenge",
    "pdf_url": "http://arxiv.org/pdf/2409.02284v1",
    "published_date": "2024-09-03 20:37:43 UTC",
    "updated_date": "2024-09-03 20:37:43 UTC"
  },
  {
    "arxiv_id": "2409.02270v1",
    "title": "Reinforcement Learning-enabled Satellite Constellation Reconfiguration and Retasking for Mission-Critical Applications",
    "authors": [
      "Hassan El Alami",
      "Danda B. Rawat"
    ],
    "abstract": "The development of satellite constellation applications is rapidly advancing\ndue to increasing user demands, reduced operational costs, and technological\nadvancements. However, a significant gap in the existing literature concerns\nreconfiguration and retasking issues within satellite constellations, which is\nthe primary focus of our research. In this work, we critically assess the\nimpact of satellite failures on constellation performance and the associated\ntask requirements. To facilitate this analysis, we introduce a system modeling\napproach for GPS satellite constellations, enabling an investigation into\nperformance dynamics and task distribution strategies, particularly in\nscenarios where satellite failures occur during mission-critical operations.\nAdditionally, we introduce reinforcement learning (RL) techniques, specifically\nQ-learning, Policy Gradient, Deep Q-Network (DQN), and Proximal Policy\nOptimization (PPO), for managing satellite constellations, addressing the\nchallenges posed by reconfiguration and retasking following satellite failures.\nOur results demonstrate that DQN and PPO achieve effective outcomes in terms of\naverage rewards, task completion rates, and response times.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication in the IEEE Military Communications\n  Conference (IEEE MILCOM 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.02270v1",
    "published_date": "2024-09-03 20:01:56 UTC",
    "updated_date": "2024-09-03 20:01:56 UTC"
  },
  {
    "arxiv_id": "2409.04469v1",
    "title": "Intensional FOL: Many-Sorted Extension",
    "authors": [
      "Zoran Majkic"
    ],
    "abstract": "The concepts used in IFOL have associated to them a list of sorted\nattributes, and the sorts are the intensional concepts as well. The requirement\nto extend the unsorted IFOL (Intensional FOL) to many-sorted IFOL is mainly\nbased on the fact that a natural language is implicitly many-sorted and that we\nintend to use IFOL to support applications that use natural languages. Thus,\nthe proposed version of many-sorted IFOL is just the completion of this\nconceptual feature of the IFOL.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.04469v1",
    "published_date": "2024-09-03 19:50:57 UTC",
    "updated_date": "2024-09-03 19:50:57 UTC"
  },
  {
    "arxiv_id": "2409.02261v1",
    "title": "Action-Based ADHD Diagnosis in Video",
    "authors": [
      "Yichun Li",
      "Yuxing Yang",
      "Syed Nohsen Naqvi"
    ],
    "abstract": "Attention Deficit Hyperactivity Disorder (ADHD) causes significant impairment\nin various domains. Early diagnosis of ADHD and treatment could significantly\nimprove the quality of life and functioning. Recently, machine learning methods\nhave improved the accuracy and efficiency of the ADHD diagnosis process.\nHowever, the cost of the equipment and trained staff required by the existing\nmethods are generally huge. Therefore, we introduce the video-based frame-level\naction recognition network to ADHD diagnosis for the first time. We also record\na real multi-modal ADHD dataset and extract three action classes from the video\nmodality for ADHD diagnosis. The whole process data have been reported to\nCNTW-NHS Foundation Trust, which would be reviewed by medical\nconsultants/professionals and will be made public in due course.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "31st European Symposium on Artificial Neural Networks",
    "pdf_url": "http://arxiv.org/pdf/2409.02261v1",
    "published_date": "2024-09-03 19:38:23 UTC",
    "updated_date": "2024-09-03 19:38:23 UTC"
  },
  {
    "arxiv_id": "2409.02251v1",
    "title": "NoiseAttack: An Evasive Sample-Specific Multi-Targeted Backdoor Attack Through White Gaussian Noise",
    "authors": [
      "Abdullah Arafat Miah",
      "Kaan Icer",
      "Resit Sendag",
      "Yu Bi"
    ],
    "abstract": "Backdoor attacks pose a significant threat when using third-party data for\ndeep learning development. In these attacks, data can be manipulated to cause a\ntrained model to behave improperly when a specific trigger pattern is applied,\nproviding the adversary with unauthorized advantages. While most existing works\nfocus on designing trigger patterns in both visible and invisible to poison the\nvictim class, they typically result in a single targeted class upon the success\nof the backdoor attack, meaning that the victim class can only be converted to\nanother class based on the adversary predefined value. In this paper, we\naddress this issue by introducing a novel sample-specific multi-targeted\nbackdoor attack, namely NoiseAttack. Specifically, we adopt White Gaussian\nNoise (WGN) with various Power Spectral Densities (PSD) as our underlying\ntriggers, coupled with a unique training strategy to execute the backdoor\nattack. This work is the first of its kind to launch a vision backdoor attack\nwith the intent to generate multiple targeted classes with minimal input\nconfiguration. Furthermore, our extensive experimental results demonstrate that\nNoiseAttack can achieve a high attack success rate against popular network\narchitectures and datasets, as well as bypass state-of-the-art backdoor\ndetection methods. Our source code and experiments are available at\nhttps://github.com/SiSL-URI/NoiseAttack/tree/main.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02251v1",
    "published_date": "2024-09-03 19:24:46 UTC",
    "updated_date": "2024-09-03 19:24:46 UTC"
  },
  {
    "arxiv_id": "2409.02245v1",
    "title": "FastVoiceGrad: One-step Diffusion-Based Voice Conversion with Adversarial Conditional Diffusion Distillation",
    "authors": [
      "Takuhiro Kaneko",
      "Hirokazu Kameoka",
      "Kou Tanaka",
      "Yuto Kondo"
    ],
    "abstract": "Diffusion-based voice conversion (VC) techniques such as VoiceGrad have\nattracted interest because of their high VC performance in terms of speech\nquality and speaker similarity. However, a notable limitation is the slow\ninference caused by the multi-step reverse diffusion. Therefore, we propose\nFastVoiceGrad, a novel one-step diffusion-based VC that reduces the number of\niterations from dozens to one while inheriting the high VC performance of the\nmulti-step diffusion-based VC. We obtain the model using adversarial\nconditional diffusion distillation (ACDD), leveraging the ability of generative\nadversarial networks and diffusion models while reconsidering the initial\nstates in sampling. Evaluations of one-shot any-to-any VC demonstrate that\nFastVoiceGrad achieves VC performance superior to or comparable to that of\nprevious multi-step diffusion-based VC while enhancing the inference speed.\nAudio samples are available at\nhttps://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/fastvoicegrad/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "stat.ML"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to Interspeech 2024. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/fastvoicegrad/",
    "pdf_url": "http://arxiv.org/pdf/2409.02245v1",
    "published_date": "2024-09-03 19:19:48 UTC",
    "updated_date": "2024-09-03 19:19:48 UTC"
  },
  {
    "arxiv_id": "2409.02239v2",
    "title": "Temporal Order Preserved Optimal Transport-based Cross-modal Knowledge Transfer Learning for ASR",
    "authors": [
      "Xugang Lu",
      "Peng Shen",
      "Yu Tsao",
      "Hisashi Kawai"
    ],
    "abstract": "Transferring linguistic knowledge from a pretrained language model (PLM) to\nan acoustic model has been shown to greatly improve the performance of\nautomatic speech recognition (ASR). However, due to the heterogeneous feature\ndistributions in cross-modalities, designing an effective model for feature\nalignment and knowledge transfer between linguistic and acoustic sequences\nremains a challenging task. Optimal transport (OT), which efficiently measures\nprobability distribution discrepancies, holds great potential for aligning and\ntransferring knowledge between acoustic and linguistic modalities. Nonetheless,\nthe original OT treats acoustic and linguistic feature sequences as two\nunordered sets in alignment and neglects temporal order information during OT\ncoupling estimation. Consequently, a time-consuming pretraining stage is\nrequired to learn a good alignment between the acoustic and linguistic\nrepresentations. In this paper, we propose a Temporal Order Preserved OT\n(TOT)-based Cross-modal Alignment and Knowledge Transfer (CAKT) (TOT-CAKT) for\nASR. In the TOT-CAKT, local neighboring frames of acoustic sequences are\nsmoothly mapped to neighboring regions of linguistic sequences, preserving\ntheir temporal order relationship in feature alignment and matching. With the\nTOT-CAKT model framework, we conduct Mandarin ASR experiments with a pretrained\nChinese PLM for linguistic knowledge transfer. Our results demonstrate that the\nproposed TOT-CAKT significantly improves ASR performance compared to several\nstate-of-the-art models employing linguistic knowledge transfer, and addresses\nthe weaknesses of the original OT-based method in sequential feature alignment\nfor ASR.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to IEEE SLT 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.02239v2",
    "published_date": "2024-09-03 19:11:15 UTC",
    "updated_date": "2024-09-05 11:34:00 UTC"
  },
  {
    "arxiv_id": "2409.02219v2",
    "title": "A+AI: Threats to Society, Remedies, and Governance",
    "authors": [
      "Don Byrd"
    ],
    "abstract": "This document focuses on the threats, especially near-term threats, that\nArtificial Intelligence (AI) brings to society. Most of the threats discussed\nhere can result from any algorithmic process, not just AI; in addition,\ndefining AI is notoriously difficult. For both reasons, it is important to\nthink of \"A+AI\": Algorithms and Artificial Intelligence.\n  In addition to the threats, this paper discusses countermeasures to them, and\nit includes a table showing which countermeasures are likely to mitigate which\nthreats. Thoughtful governance could manage the risks without seriously\nimpeding progress; in fact, chances are it would accelerate progress by\nreducing the social chaos that would otherwise be likely. The paper lists\nspecific actions government should take as soon as possible, namely:\n  * Require all social media platforms accessible in the U.S. to offer users\nverification that their accounts are owned by citizens, and to display every\naccount's verification status\n  * Establish regulations to require that all products created or significantly\nmodified with A+AI be clearly labeled as such; to restrict use of generative AI\nto create likenesses of persons; and to require creators of generative AI\nsoftware to disclose materials used to train their software and to compensate\nthe creators of any copyrighted material used\n  * Fund a crash project of research on mitigating the threats\n  * Fund educational campaigns to raise awareness of the threats",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "25 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.02219v2",
    "published_date": "2024-09-03 18:43:47 UTC",
    "updated_date": "2024-09-07 01:25:30 UTC"
  },
  {
    "arxiv_id": "2409.02960v1",
    "title": "Managing multiple agents by automatically adjusting incentives",
    "authors": [
      "Shunichi Akatsuka",
      "Yaemi Teramoto",
      "Aaron Courville"
    ],
    "abstract": "In the coming years, AI agents will be used for making more complex\ndecisions, including in situations involving many different groups of people.\nOne big challenge is that AI agent tends to act in its own interest, unlike\nhumans who often think about what will be the best for everyone in the long\nrun. In this paper, we explore a method to get self-interested agents to work\ntowards goals that benefit society as a whole. We propose a method to add a\nmanager agent to mediate agent interactions by assigning incentives to certain\nactions. We tested our method with a supply-chain management problem and showed\nthat this framework (1) increases the raw reward by 22.2%, (2) increases the\nagents' reward by 23.8%, and (3) increases the manager's reward by 20.1%.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.MA",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.02960v1",
    "published_date": "2024-09-03 18:41:16 UTC",
    "updated_date": "2024-09-03 18:41:16 UTC"
  },
  {
    "arxiv_id": "2409.02100v1",
    "title": "On a heuristic approach to the description of consciousness as a hypercomplex system state and the possibility of machine consciousness (German edition)",
    "authors": [
      "Ralf Otte"
    ],
    "abstract": "This article presents a heuristic view that shows that the inner states of\nconsciousness experienced by every human being have a physical but imaginary\nhypercomplex basis. The hypercomplex description is necessary because certain\nprocesses of consciousness cannot be physically measured in principle, but\nnevertheless exist. Based on theoretical considerations, it could be possible -\nas a result of mathematical investigations into a so-called bicomplex algebra -\nto generate and use hypercomplex system states on machines in a targeted\nmanner. The hypothesis of the existence of hypercomplex system states on\nmachines is already supported by the surprising performance of highly complex\nAI systems. However, this has yet to be proven. In particular, there is a lack\nof experimental data that distinguishes such systems from other systems, which\nis why this question will be addressed in later articles. This paper describes\nthe developed bicomplex algebra and possible applications of these findings to\ngenerate hypercomplex energy states on machines. In the literature, such system\nstates are often referred to as machine consciousness. The article uses\nmathematical considerations to explain how artificial consciousness could be\ngenerated and what advantages this would have for such AI systems.",
    "categories": [
      "cs.AI",
      "math.AC",
      "physics.app-ph",
      "08A99",
      "I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, in German language. 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2409.02100v1",
    "published_date": "2024-09-03 17:55:57 UTC",
    "updated_date": "2024-09-03 17:55:57 UTC"
  },
  {
    "arxiv_id": "2409.02098v1",
    "title": "CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation",
    "authors": [
      "Ingo Ziegler",
      "Abdullatif Köksal",
      "Desmond Elliott",
      "Hinrich Schütze"
    ],
    "abstract": "Building high-quality datasets for specialized tasks is a time-consuming and\nresource-intensive process that often requires specialized domain knowledge. We\npropose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for\ngenerating synthetic datasets, given a small number of user-written few-shots\nthat demonstrate the task to be performed. Given the few-shot examples, we use\nlarge-scale public web-crawled corpora and similarity-based document retrieval\nto find other relevant human-written documents. Lastly, instruction-tuned large\nlanguage models (LLMs) augment the retrieved documents into custom-formatted\ntask samples, which then can be used for fine-tuning. We demonstrate that CRAFT\ncan efficiently generate large-scale task-specific training datasets for four\ndiverse tasks: biology question-answering (QA), medicine QA and commonsense QA\nas well as summarization. Our experiments show that CRAFT-based models\noutperform or achieve comparable performance to general LLMs for QA tasks,\nwhile CRAFT-based summarization models outperform models trained on\nhuman-curated data by 46 preference points.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02098v1",
    "published_date": "2024-09-03 17:54:40 UTC",
    "updated_date": "2024-09-03 17:54:40 UTC"
  },
  {
    "arxiv_id": "2409.02095v2",
    "title": "DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos",
    "authors": [
      "Wenbo Hu",
      "Xiangjun Gao",
      "Xiaoyu Li",
      "Sijie Zhao",
      "Xiaodong Cun",
      "Yong Zhang",
      "Long Quan",
      "Ying Shan"
    ],
    "abstract": "Estimating video depth in open-world scenarios is challenging due to the\ndiversity of videos in appearance, content motion, camera movement, and length.\nWe present DepthCrafter, an innovative method for generating temporally\nconsistent long depth sequences with intricate details for open-world videos,\nwithout requiring any supplementary information such as camera poses or optical\nflow. The generalization ability to open-world videos is achieved by training\nthe video-to-depth model from a pre-trained image-to-video diffusion model,\nthrough our meticulously designed three-stage training strategy. Our training\napproach enables the model to generate depth sequences with variable lengths at\none time, up to 110 frames, and harvest both precise depth details and rich\ncontent diversity from realistic and synthetic datasets. We also propose an\ninference strategy that can process extremely long videos through segment-wise\nestimation and seamless stitching. Comprehensive evaluations on multiple\ndatasets reveal that DepthCrafter achieves state-of-the-art performance in\nopen-world video depth estimation under zero-shot settings. Furthermore,\nDepthCrafter facilitates various downstream applications, including depth-based\nvisual effects and conditional video generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project webpage: https://depthcrafter.github.io",
    "pdf_url": "http://arxiv.org/pdf/2409.02095v2",
    "published_date": "2024-09-03 17:52:03 UTC",
    "updated_date": "2024-11-27 07:59:25 UTC"
  },
  {
    "arxiv_id": "2409.02069v2",
    "title": "A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial",
    "authors": [
      "Anna L. Trella",
      "Kelly W. Zhang",
      "Hinal Jajal",
      "Inbal Nahum-Shani",
      "Vivek Shetty",
      "Finale Doshi-Velez",
      "Susan A. Murphy"
    ],
    "abstract": "Dental disease is a prevalent chronic condition associated with substantial\nfinancial burden, personal suffering, and increased risk of systemic diseases.\nDespite widespread recommendations for twice-daily tooth brushing, adherence to\nrecommended oral self-care behaviors remains sub-optimal due to factors such as\nforgetfulness and disengagement. To address this, we developed Oralytics, a\nmHealth intervention system designed to complement clinician-delivered\npreventative care for marginalized individuals at risk for dental disease.\nOralytics incorporates an online reinforcement learning algorithm to determine\noptimal times to deliver intervention prompts that encourage oral self-care\nbehaviors. We have deployed Oralytics in a registered clinical trial. The\ndeployment required careful design to manage challenges specific to the\nclinical trials setting in the U.S. In this paper, we (1) highlight key design\ndecisions of the RL algorithm that address these challenges and (2) conduct a\nre-sampling analysis to evaluate algorithm design decisions. A second phase\n(randomized control trial) of Oralytics is planned to start in spring 2025.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02069v2",
    "published_date": "2024-09-03 17:16:01 UTC",
    "updated_date": "2024-12-18 23:01:54 UTC"
  },
  {
    "arxiv_id": "2409.02060v2",
    "title": "OLMoE: Open Mixture-of-Experts Language Models",
    "authors": [
      "Niklas Muennighoff",
      "Luca Soldaini",
      "Dirk Groeneveld",
      "Kyle Lo",
      "Jacob Morrison",
      "Sewon Min",
      "Weijia Shi",
      "Pete Walsh",
      "Oyvind Tafjord",
      "Nathan Lambert",
      "Yuling Gu",
      "Shane Arora",
      "Akshita Bhagia",
      "Dustin Schwenk",
      "David Wadden",
      "Alexander Wettig",
      "Binyuan Hui",
      "Tim Dettmers",
      "Douwe Kiela",
      "Ali Farhadi",
      "Noah A. Smith",
      "Pang Wei Koh",
      "Amanpreet Singh",
      "Hannaneh Hajishirzi"
    ],
    "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging\nsparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but\nuses only 1B per input token. We pretrain it on 5 trillion tokens and further\nadapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available\nmodels with similar active parameters, even surpassing larger ones like\nLlama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE\ntraining, analyze routing in our model showing high specialization, and\nopen-source all aspects of our work: model weights, training data, code, and\nlogs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "63 pages (24 main), 36 figures, 17 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.02060v2",
    "published_date": "2024-09-03 17:08:20 UTC",
    "updated_date": "2025-03-03 01:25:46 UTC"
  },
  {
    "arxiv_id": "2409.02049v1",
    "title": "Low-Resolution Face Recognition via Adaptable Instance-Relation Distillation",
    "authors": [
      "Ruixin Shi",
      "Weijia Guo",
      "Shiming Ge"
    ],
    "abstract": "Low-resolution face recognition is a challenging task due to the missing of\ninformative details. Recent approaches based on knowledge distillation have\nproven that high-resolution clues can well guide low-resolution face\nrecognition via proper knowledge transfer. However, due to the distribution\ndifference between training and testing faces, the learned models often suffer\nfrom poor adaptability. To address that, we split the knowledge transfer\nprocess into distillation and adaptation steps, and propose an adaptable\ninstance-relation distillation approach to facilitate low-resolution face\nrecognition. In the approach, the student distills knowledge from\nhigh-resolution teacher in both instance level and relation level, providing\nsufficient cross-resolution knowledge transfer. Then, the learned student can\nbe adaptable to recognize low-resolution faces with adaptive batch\nnormalization in inference. In this manner, the capability of recovering\nmissing details of familiar low-resolution faces can be effectively enhanced,\nleading to a better knowledge transfer. Extensive experiments on low-resolution\nface recognition clearly demonstrate the effectiveness and adaptability of our\napproach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IJCNN 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.02049v1",
    "published_date": "2024-09-03 16:53:34 UTC",
    "updated_date": "2024-09-03 16:53:34 UTC"
  },
  {
    "arxiv_id": "2409.02045v2",
    "title": "AllWeatherNet:Unified Image Enhancement for Autonomous Driving under Adverse Weather and Lowlight-conditions",
    "authors": [
      "Chenghao Qian",
      "Mahdi Rezaei",
      "Saeed Anwar",
      "Wenjing Li",
      "Tanveer Hussain",
      "Mohsen Azarmi",
      "Wei Wang"
    ],
    "abstract": "Adverse conditions like snow, rain, nighttime, and fog, pose challenges for\nautonomous driving perception systems. Existing methods have limited\neffectiveness in improving essential computer vision tasks, such as semantic\nsegmentation, and often focus on only one specific condition, such as removing\nrain or translating nighttime images into daytime ones. To address these\nlimitations, we propose a method to improve the visual quality and clarity\ndegraded by such adverse conditions. Our method, AllWeather-Net, utilizes a\nnovel hierarchical architecture to enhance images across all adverse\nconditions. This architecture incorporates information at three semantic\nlevels: scene, object, and texture, by discriminating patches at each level.\nFurthermore, we introduce a Scaled Illumination-aware Attention Mechanism\n(SIAM) that guides the learning towards road elements critical for autonomous\ndriving perception. SIAM exhibits robustness, remaining unaffected by changes\nin weather conditions or environmental scenes. AllWeather-Net effectively\ntransforms images into normal weather and daytime scenes, demonstrating\nsuperior image enhancement results and subsequently enhancing the performance\nof semantic segmentation, with up to a 5.3% improvement in mIoU in the trained\ndomain. We also show our model's generalization ability by applying it to\nunseen domains without re-training, achieving up to 3.9% mIoU improvement. Code\ncan be accessed at: https://github.com/Jumponthemoon/AllWeatherNet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICPR 2024, Piero Zamperoni Overall Best Student Paper Award",
    "pdf_url": "http://arxiv.org/pdf/2409.02045v2",
    "published_date": "2024-09-03 16:47:01 UTC",
    "updated_date": "2024-12-14 04:19:10 UTC"
  },
  {
    "arxiv_id": "2409.02038v2",
    "title": "BEAVER: An Enterprise Benchmark for Text-to-SQL",
    "authors": [
      "Peter Baile Chen",
      "Fabian Wenz",
      "Yi Zhang",
      "Devin Yang",
      "Justin Choi",
      "Nesime Tatbul",
      "Michael Cafarella",
      "Çağatay Demiralp",
      "Michael Stonebraker"
    ],
    "abstract": "Existing text-to-SQL benchmarks have largely been constructed from web tables\nwith human-generated question-SQL pairs. LLMs typically show strong results on\nthese benchmarks, leading to a belief that LLMs are effective at text-to-SQL\ntasks. However, how these results transfer to enterprise settings is unclear\nbecause tables in enterprise databases might differ substantially from web\ntables in structure and content. To contend with this problem, we introduce a\nnew dataset BEAVER, the first enterprise text-to-SQL benchmark sourced from\nreal private enterprise data warehouses. This dataset includes natural language\nqueries and their correct SQL statements, which we collected from actual query\nlogs. We then benchmark off-the-shelf LLMs on this dataset. LLMs perform\npoorly, even when augmented with standard prompt engineering and RAG\ntechniques. We identify three main reasons for the poor performance: (1)\nschemas of enterprise tables are more complex than the schemas in public data,\nresulting in SQL-generation tasks intrinsically harder; (2) business-oriented\nquestions are often more complex, requiring joins over multiple tables,\naggregations, and nested queries; (3) public LLMs cannot train on private\nenterprise data warehouses that are not publicly accessible, and therefore it\nis difficult for the model to learn to solve (1) and (2). We believe BEAVER\nwill facilitate future research in building text-to-SQL systems that perform\nbetter in enterprise settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "Dataset and code are available at\n  https://peterbaile.github.io/beaver/",
    "pdf_url": "http://arxiv.org/pdf/2409.02038v2",
    "published_date": "2024-09-03 16:37:45 UTC",
    "updated_date": "2025-01-20 22:24:48 UTC"
  },
  {
    "arxiv_id": "2409.02018v1",
    "title": "TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation",
    "authors": [
      "Bobby Azad",
      "Pourya Adibfar",
      "Kaiqun Fu"
    ],
    "abstract": "In healthcare, medical image segmentation is crucial for accurate disease\ndiagnosis and the development of effective treatment strategies. Early\ndetection can significantly aid in managing diseases and potentially prevent\ntheir progression. Machine learning, particularly deep convolutional neural\nnetworks, has emerged as a promising approach to addressing segmentation\nchallenges. Traditional methods like U-Net use encoding blocks for local\nrepresentation modeling and decoding blocks to uncover semantic relationships.\nHowever, these models often struggle with multi-scale objects exhibiting\nsignificant variations in texture and shape, and they frequently fail to\ncapture long-range dependencies in the input data. Transformers designed for\nsequence-to-sequence predictions have been proposed as alternatives, utilizing\nglobal self-attention mechanisms. Yet, they can sometimes lack precise\nlocalization due to insufficient granular details. To overcome these\nlimitations, we introduce TransDAE: a novel approach that reimagines the\nself-attention mechanism to include both spatial and channel-wise associations\nacross the entire feature space, while maintaining computational efficiency.\nAdditionally, TransDAE enhances the skip connection pathway with an inter-scale\ninteraction module, promoting feature reuse and improving localization\naccuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on\nthe Synaps multi-organ dataset, even without relying on pre-trained weights.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02018v1",
    "published_date": "2024-09-03 16:08:48 UTC",
    "updated_date": "2024-09-03 16:08:48 UTC"
  },
  {
    "arxiv_id": "2409.02017v1",
    "title": "AI Governance in Higher Education: Case Studies of Guidance at Big Ten Universities",
    "authors": [
      "Chuhao Wu",
      "He Zhang",
      "John M. Carroll"
    ],
    "abstract": "Generative AI has drawn significant attention from stakeholders in higher\neducation. As it introduces new opportunities for personalized learning and\ntutoring support, it simultaneously poses challenges to academic integrity and\nleads to ethical issues. Consequently, governing responsible AI usage within\nhigher education institutions (HEIs) becomes increasingly important. Leading\nuniversities have already published guidelines on Generative AI, with most\nattempting to embrace this technology responsibly. This study provides a new\nperspective by focusing on strategies for responsible AI governance as\ndemonstrated in these guidelines. Through a case study of 14 prestigious\nuniversities in the United States, we identified the multi-unit governance of\nAI, the role-specific governance of AI, and the academic characteristics of AI\ngovernance from their AI guidelines. The strengths and potential limitations of\nthese strategies and characteristics are discussed. The findings offer\npractical implications for guiding responsible AI usage in HEIs and beyond.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02017v1",
    "published_date": "2024-09-03 16:06:45 UTC",
    "updated_date": "2024-09-03 16:06:45 UTC"
  },
  {
    "arxiv_id": "2409.02008v1",
    "title": "When Digital Twin Meets 6G: Concepts, Obstacles, and Research Prospects",
    "authors": [
      "Wenshuai Liu",
      "Yaru Fu",
      "Zheng Shi",
      "Hong Wang"
    ],
    "abstract": "The convergence of digital twin technology and the emerging 6G network\npresents both challenges and numerous research opportunities. This article\nexplores the potential synergies between digital twin and 6G, highlighting the\nkey challenges and proposing fundamental principles for their integration. We\ndiscuss the unique requirements and capabilities of digital twin in the context\nof 6G networks, such as sustainable deployment, real-time synchronization,\nseamless migration, predictive analytic, and closed-loop control. Furthermore,\nwe identify research opportunities for leveraging digital twin and artificial\nintelligence to enhance various aspects of 6G, including network optimization,\nresource allocation, security, and intelligent service provisioning. This\narticle aims to stimulate further research and innovation at the intersection\nof digital twin and 6G, paving the way for transformative applications and\nservices in the future.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.NI",
    "comment": "7 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.02008v1",
    "published_date": "2024-09-03 15:57:05 UTC",
    "updated_date": "2024-09-03 15:57:05 UTC"
  },
  {
    "arxiv_id": "2409.07485v1",
    "title": "Optimization and Deployment of Deep Neural Networks for PPG-based Blood Pressure Estimation Targeting Low-power Wearables",
    "authors": [
      "Alessio Burrello",
      "Francesco Carlucci",
      "Giovanni Pollo",
      "Xiaying Wang",
      "Massimo Poncino",
      "Enrico Macii",
      "Luca Benini",
      "Daniele Jahier Pagliari"
    ],
    "abstract": "PPG-based Blood Pressure (BP) estimation is a challenging biosignal\nprocessing task for low-power devices such as wearables. State-of-the-art Deep\nNeural Networks (DNNs) trained for this task implement either a PPG-to-BP\nsignal-to-signal reconstruction or a scalar BP value regression and have been\nshown to outperform classic methods on the largest and most complex public\ndatasets. However, these models often require excessive parameter storage or\ncomputational effort for wearable deployment, exceeding the available memory or\nincurring too high latency and energy consumption. In this work, we describe a\nfully-automated DNN design pipeline, encompassing HW-aware Neural Architecture\nSearch (NAS) and Quantization, thanks to which we derive accurate yet\nlightweight models, that can be deployed on an ultra-low-power multicore\nSystem-on-Chip (SoC), GAP8. Starting from both regression and signal-to-signal\nstate-of-the-art models on four public datasets, we obtain optimized versions\nthat achieve up to 4.99% lower error or 73.36% lower size at iso-error.\nNoteworthy, while the most accurate SoA network on the largest dataset can not\nfit the GAP8 memory, all our optimized models can; our most accurate DNN\nconsumes as little as 0.37 mJ while reaching the lowest MAE of 8.08 on\nDiastolic BP estimation.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07485v1",
    "published_date": "2024-09-03 15:48:43 UTC",
    "updated_date": "2024-09-03 15:48:43 UTC"
  },
  {
    "arxiv_id": "2409.01995v3",
    "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
    "authors": [
      "Yiwei Guo",
      "Zhihan Li",
      "Junjie Li",
      "Chenpeng Du",
      "Hankun Wang",
      "Shuai Wang",
      "Xie Chen",
      "Kai Yu"
    ],
    "abstract": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances\nvoice conversion (VC). We use discrete tokens from speech self-supervised\nmodels as the content features of source speech, and treat VC as a prompted\nvocoding task. To amend the loss of speaker timbre in the content tokens,\nvec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent\ninformation. A novel adaptive Snake activation function is proposed to better\nincorporate timbre into the waveform reconstruction process. In this way,\nvec2wav 2.0 learns to alter the speaker timbre appropriately given different\nreference prompts. Also, no supervised data is required for vec2wav 2.0 to be\neffectively trained. Experimental results demonstrate that vec2wav 2.0\noutperforms all other baselines to a considerable margin in terms of audio\nquality and speaker similarity in any-to-any VC. Ablation studies verify the\neffects made by the proposed techniques. Moreover, vec2wav 2.0 achieves\ncompetitive cross-lingual VC even only trained on monolingual corpus. Thus,\nvec2wav 2.0 shows timbre can potentially be manipulated only by speech token\nvocoders, pushing the frontiers of VC and speech synthesis.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 4 figures. Demo page:\n  https://cantabile-kwok.github.io/vec2wav2/",
    "pdf_url": "http://arxiv.org/pdf/2409.01995v3",
    "published_date": "2024-09-03 15:41:07 UTC",
    "updated_date": "2024-12-22 12:49:28 UTC"
  },
  {
    "arxiv_id": "2409.01992v1",
    "title": "QueryCheetah: Fast Automated Discovery of Attribute Inference Attacks Against Query-Based Systems",
    "authors": [
      "Bozhidar Stevanoski",
      "Ana-Maria Cretu",
      "Yves-Alexandre de Montjoye"
    ],
    "abstract": "Query-based systems (QBSs) are one of the key approaches for sharing data.\nQBSs allow analysts to request aggregate information from a private protected\ndataset. Attacks are a crucial part of ensuring QBSs are truly\nprivacy-preserving. The development and testing of attacks is however very\nlabor-intensive and unable to cope with the increasing complexity of systems.\nAutomated approaches have been shown to be promising but are currently\nextremely computationally intensive, limiting their applicability in practice.\nWe here propose QueryCheetah, a fast and effective method for automated\ndiscovery of privacy attacks against QBSs. We instantiate QueryCheetah on\nattribute inference attacks and show it to discover stronger attacks than\nprevious methods while being 18 times faster than the state-of-the-art\nautomated approach. We then show how QueryCheetah allows system developers to\nthoroughly evaluate the privacy risk, including for various attacker strengths\nand target individuals. We finally show how QueryCheetah can be used\nout-of-the-box to find attacks in larger syntaxes and workarounds around ad-hoc\ndefenses.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "This is an extended version of the ACM CCS paper which includes\n  appendices",
    "pdf_url": "http://arxiv.org/pdf/2409.01992v1",
    "published_date": "2024-09-03 15:37:05 UTC",
    "updated_date": "2024-09-03 15:37:05 UTC"
  },
  {
    "arxiv_id": "2409.13695v1",
    "title": "You Only Use Reactive Attention Slice For Long Context Retrieval",
    "authors": [
      "Yun Joon Soh",
      "Hanxian Huang",
      "Yuandong Tian",
      "Jishen Zhao"
    ],
    "abstract": "Supporting longer context for Large Language Models (LLM) is a promising\ndirection to advance LLMs. As training a model for a longer context window is\ncomputationally expensive, many alternative solutions, such as Retrieval\nAugmented Generation (RAG), have been used. However, most existing RAG methods\nadopt embedding-based retrieval that falls short on long contexts.\n  To address such challenges, we propose an attention-based retrieval\ntechnique, You Only Use Reactive Attention slice (YOURA). YOURA leverages a\nnovel retrieval heuristic called reaction score to rank the relevance of each\nsentence in the input context with the query sentence. Intuitively, we measure\nhow the per-token attention score \"reacts\" to the query and greedily retrieves\nthe most reactive sentences. Internally, YOURA generates a token-indexed vector\n(called reaction vector) for the whole input context. To map each sentence to\nthe token-indexed vector, we propose an Embedding-Agnostic Sentence Yield\n(EASY), a best-effort token wiggling algorithm.\n  We evaluate our retrieval technique on three open-source pre-trained LLM\nmodels across six LongBench QA datasets. Our technique achieves up to 30% vLLM\ninference throughput improvement for serving long-context queries with a nearly\nidentical quality score to the simple yet effective truncate-middle approach.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13695v1",
    "published_date": "2024-09-03 15:30:57 UTC",
    "updated_date": "2024-09-03 15:30:57 UTC"
  },
  {
    "arxiv_id": "2409.01974v2",
    "title": "Planning to avoid ambiguous states through Gaussian approximations to non-linear sensors in active inference agents",
    "authors": [
      "Wouter M. Kouw"
    ],
    "abstract": "In nature, active inference agents must learn how observations of the world\nrepresent the state of the agent. In engineering, the physics behind sensors is\noften known reasonably accurately and measurement functions can be incorporated\ninto generative models. When a measurement function is non-linear, the\ntransformed variable is typically approximated with a Gaussian distribution to\nensure tractable inference. We show that Gaussian approximations that are\nsensitive to the curvature of the measurement function, such as a second-order\nTaylor approximation, produce a state-dependent ambiguity term. This induces a\npreference over states, based on how accurately the state can be inferred from\nthe observation. We demonstrate this preference with a robot navigation\nexperiment where agents plan trajectories.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "stat.ML"
    ],
    "primary_category": "eess.SY",
    "comment": "13 pages, 3 figures. Accepted to the International Workshop on Active\n  Inference 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01974v2",
    "published_date": "2024-09-03 15:17:16 UTC",
    "updated_date": "2024-09-18 20:11:48 UTC"
  },
  {
    "arxiv_id": "2409.11411v1",
    "title": "AIvril: AI-Driven RTL Generation With Verification In-The-Loop",
    "authors": [
      "Mubashir ul Islam",
      "Humza Sami",
      "Pierre-Emmanuel Gaillardon",
      "Valerio Tenace"
    ],
    "abstract": "Large Language Models (LLMs) are computational models capable of performing\ncomplex natural language processing tasks. Leveraging these capabilities, LLMs\nhold the potential to transform the entire hardware design stack, with\npredictions suggesting that front-end and back-end tasks could be fully\nautomated in the near future. Currently, LLMs show great promise in\nstreamlining Register Transfer Level (RTL) generation, enhancing efficiency,\nand accelerating innovation. However, their probabilistic nature makes them\nprone to inaccuracies - a significant drawback in RTL design, where reliability\nand precision are essential.\n  To address these challenges, this paper introduces AIvril, an advanced\nframework designed to enhance the accuracy and reliability of RTL-aware LLMs.\nAIvril employs a multi-agent, LLM-agnostic system for automatic syntax\ncorrection and functional verification, significantly reducing - and in many\ncases, completely eliminating - instances of erroneous code generation.\nExperimental results conducted on the VerilogEval-Human dataset show that our\nframework improves code quality by nearly 2x when compared to previous works,\nwhile achieving an 88.46% success rate in meeting verification objectives. This\nrepresents a critical step toward automating and optimizing hardware design\nworkflows, offering a more dependable methodology for AI-driven RTL design.",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11411v1",
    "published_date": "2024-09-03 15:07:11 UTC",
    "updated_date": "2024-09-03 15:07:11 UTC"
  },
  {
    "arxiv_id": "2409.01952v2",
    "title": "Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor",
    "authors": [
      "Abdullah Arafat Miah",
      "Yu Bi"
    ],
    "abstract": "Deep neural networks (DNNs) have long been recognized as vulnerable to\nbackdoor attacks. By providing poisoned training data in the fine-tuning\nprocess, the attacker can implant a backdoor into the victim model. This\nenables input samples meeting specific textual trigger patterns to be\nclassified as target labels of the attacker's choice. While such black-box\nattacks have been well explored in both computer vision and natural language\nprocessing (NLP), backdoor attacks relying on white-box attack philosophy have\nhardly been thoroughly investigated. In this paper, we take the first step to\nintroduce a new type of backdoor attack that conceals itself within the\nunderlying model architecture. Specifically, we propose to design separate\nbackdoor modules consisting of two functions: trigger detection and noise\ninjection. The add-on modules of model architecture layers can detect the\npresence of input trigger tokens and modify layer weights using Gaussian noise\nto disturb the feature distribution of the baseline model. We conduct extensive\nexperiments to evaluate our attack methods using two model architecture\nsettings on five different large language datasets. We demonstrate that the\ntraining-free architectural backdoor on a large language model poses a genuine\nthreat. Unlike the-state-of-art work, it can survive the rigorous fine-tuning\nand retraining process, as well as evade output probability-based defense\nmethods (i.e. BDDR). All the code and data is available\nhttps://github.com/SiSL-URI/Arch_Backdoor_LLM.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01952v2",
    "published_date": "2024-09-03 14:54:16 UTC",
    "updated_date": "2024-09-09 15:37:15 UTC"
  },
  {
    "arxiv_id": "2409.01931v2",
    "title": "On the design space between molecular mechanics and machine learning force fields",
    "authors": [
      "Yuanqing Wang",
      "Kenichiro Takaba",
      "Michael S. Chen",
      "Marcus Wieder",
      "Yuzhi Xu",
      "Tong Zhu",
      "John Z. H. Zhang",
      "Arnav Nagle",
      "Kuang Yu",
      "Xinyan Wang",
      "Daniel J. Cole",
      "Joshua A. Rackers",
      "Kyunghyun Cho",
      "Joe G. Greener",
      "Peter Eastman",
      "Stefano Martiniani",
      "Mark E. Tuckerman"
    ],
    "abstract": "A force field as accurate as quantum mechanics (QM) and as fast as molecular\nmechanics (MM), with which one can simulate a biomolecular system efficiently\nenough and meaningfully enough to get quantitative insights, is among the most\nardent dreams of biophysicists -- a dream, nevertheless, not to be fulfilled\nany time soon. Machine learning force fields (MLFFs) represent a meaningful\nendeavor towards this direction, where differentiable neural functions are\nparametrized to fit ab initio energies, and furthermore forces through\nautomatic differentiation. We argue that, as of now, the utility of the MLFF\nmodels is no longer bottlenecked by accuracy but primarily by their speed (as\nwell as stability and generalizability), as many recent variants, on limited\nchemical spaces, have long surpassed the chemical accuracy of $1$ kcal/mol --\nthe empirical threshold beyond which realistic chemical predictions are\npossible -- though still magnitudes slower than MM. Hoping to kindle\nexplorations and designs of faster, albeit perhaps slightly less accurate\nMLFFs, in this review, we focus our attention on the design space (the\nspeed-accuracy tradeoff) between MM and ML force fields. After a brief review\nof the building blocks of force fields of either kind, we discuss the desired\nproperties and challenges now faced by the force field development community,\nsurvey the efforts to make MM force fields more accurate and ML force fields\nfaster, envision what the next generation of MLFF might look like.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.LG",
      "physics.bio-ph",
      "physics.comp-ph"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01931v2",
    "published_date": "2024-09-03 14:21:46 UTC",
    "updated_date": "2024-09-05 13:10:22 UTC"
  },
  {
    "arxiv_id": "2409.01928v1",
    "title": "Comprehensive Equity Index (CEI): Definition and Application to Bias Evaluation in Biometrics",
    "authors": [
      "Imanol Solano",
      "Alejandro Peña",
      "Aythami Morales",
      "Julian Fierrez",
      "Ruben Tolosana",
      "Francisco Zamora-Martinez",
      "Javier San Agustin"
    ],
    "abstract": "We present a novel metric designed, among other applications, to quantify\nbiased behaviors of machine learning models. As its core, the metric consists\nof a new similarity metric between score distributions that balances both their\ngeneral shapes and tails' probabilities. In that sense, our proposed metric may\nbe useful in many application areas. Here we focus on and apply it to the\noperational evaluation of face recognition systems, with special attention to\nquantifying demographic biases; an application where our metric is especially\nuseful. The topic of demographic bias and fairness in biometric recognition\nsystems has gained major attention in recent years. The usage of these systems\nhas spread in society, raising concerns about the extent to which these systems\ntreat different population groups. A relevant step to prevent and mitigate\ndemographic biases is first to detect and quantify them. Traditionally, two\napproaches have been studied to quantify differences between population groups\nin machine learning literature: 1) measuring differences in error rates, and 2)\nmeasuring differences in recognition score distributions. Our proposed\nComprehensive Equity Index (CEI) trade-offs both approaches combining both\nerrors from distribution tails and general distribution shapes. This new metric\nis well suited to real-world scenarios, as measured on NIST FRVT evaluations,\ninvolving high-performance systems and realistic face databases including a\nwide range of covariates and demographic groups. We first show the limitations\nof existing metrics to correctly assess the presence of biases in realistic\nsetups and then propose our new metric to tackle these limitations. We tested\nthe proposed metric with two state-of-the-art models and four widely used\ndatabases, showing its capacity to overcome the main flaws of previous bias\nmetrics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted paper for the 27th International Conference on Pattern\n  Recognition (ICPR) 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01928v1",
    "published_date": "2024-09-03 14:19:38 UTC",
    "updated_date": "2024-09-03 14:19:38 UTC"
  },
  {
    "arxiv_id": "2409.01927v1",
    "title": "From Grounding to Planning: Benchmarking Bottlenecks in Web Agents",
    "authors": [
      "Segev Shlomov",
      "Ben wiesel",
      "Aviad Sela",
      "Ido Levy",
      "Liane Galanti",
      "Roy Abitbol"
    ],
    "abstract": "General web-based agents are increasingly essential for interacting with\ncomplex web environments, yet their performance in real-world web applications\nremains poor, yielding extremely low accuracy even with state-of-the-art\nfrontier models. We observe that these agents can be decomposed into two\nprimary components: Planning and Grounding. Yet, most existing research treats\nthese agents as black boxes, focusing on end-to-end evaluations which hinder\nmeaningful improvements. We sharpen the distinction between the planning and\ngrounding components and conduct a novel analysis by refining experiments on\nthe Mind2Web dataset. Our work proposes a new benchmark for each of the\ncomponents separately, identifying the bottlenecks and pain points that limit\nagent performance. Contrary to prevalent assumptions, our findings suggest that\ngrounding is not a significant bottleneck and can be effectively addressed with\ncurrent techniques. Instead, the primary challenge lies in the planning\ncomponent, which is the main source of performance degradation. Through this\nanalysis, we offer new insights and demonstrate practical suggestions for\nimproving the capabilities of web agents, paving the way for more reliable\nagents.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01927v1",
    "published_date": "2024-09-03 14:17:09 UTC",
    "updated_date": "2024-09-03 14:17:09 UTC"
  },
  {
    "arxiv_id": "2409.01914v1",
    "title": "GradINN: Gradient Informed Neural Network",
    "authors": [
      "Filippo Aglietti",
      "Francesco Della Santa",
      "Andrea Piano",
      "Virginia Aglietti"
    ],
    "abstract": "We propose Gradient Informed Neural Networks (GradINNs), a methodology\ninspired by Physics Informed Neural Networks (PINNs) that can be used to\nefficiently approximate a wide range of physical systems for which the\nunderlying governing equations are completely unknown or cannot be defined, a\ncondition that is often met in complex engineering problems. GradINNs leverage\nprior beliefs about a system's gradient to constrain the predicted function's\ngradient across all input dimensions. This is achieved using two neural\nnetworks: one modeling the target function and an auxiliary network expressing\nprior beliefs, e.g., smoothness. A customized loss function enables training\nthe first network while enforcing gradient constraints derived from the\nauxiliary network. We demonstrate the advantages of GradINNs, particularly in\nlow-data regimes, on diverse problems spanning non time-dependent systems\n(Friedman function, Stokes Flow) and time-dependent systems (Lotka-Volterra,\nBurger's equation). Experimental results showcase strong performance compared\nto standard neural networks and PINN-like approaches across all tested\nscenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01914v1",
    "published_date": "2024-09-03 14:03:29 UTC",
    "updated_date": "2024-09-03 14:03:29 UTC"
  },
  {
    "arxiv_id": "2409.01909v2",
    "title": "LUK: Empowering Log Understanding with Expert Knowledge from Large Language Models",
    "authors": [
      "Lipeng Ma",
      "Weidong Yang",
      "Sihang Jiang",
      "Ben Fei",
      "Mingjie Zhou",
      "Shuhao Li",
      "Mingyu Zhao",
      "Bo Xu",
      "Yanghua Xiao"
    ],
    "abstract": "Logs play a critical role in providing essential information for system\nmonitoring and troubleshooting. Recently, with the success of pre-trained\nlanguage models (PLMs) and large language models (LLMs) in natural language\nprocessing (NLP), smaller PLMs (such as BERT) and LLMs (like GPT-4) have become\nthe current mainstream approaches for log analysis. Despite the remarkable\ncapabilities of LLMs, their higher cost and inefficient inference present\nsignificant challenges in leveraging the full potential of LLMs to analyze\nlogs. In contrast, smaller PLMs can be fine-tuned for specific tasks even with\nlimited computational resources, making them more practical. However, these\nsmaller PLMs face challenges in understanding logs comprehensively due to their\nlimited expert knowledge. To address the lack of expert knowledge and enhance\nlog understanding for smaller PLMs, this paper introduces a novel and practical\nknowledge enhancement framework, called LUK, which acquires expert knowledge\nfrom LLMs automatically and then enhances the smaller PLM for log analysis with\nthese expert knowledge. LUK can take full advantage of both types of models.\nSpecifically, we design a multi-expert collaboration framework based on LLMs\nwith different roles to acquire expert knowledge. In addition, we propose two\nnovel pre-training tasks to enhance the log pre-training with expert knowledge.\nLUK achieves state-of-the-art results on different log analysis tasks and\nextensive experiments demonstrate expert knowledge from LLMs can be utilized\nmore effectively to understand logs. Our source code and detailed experimental\ndata are available at https://github.com/LeaperOvO/LUK.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2409.01909v2",
    "published_date": "2024-09-03 13:58:34 UTC",
    "updated_date": "2025-01-31 05:51:52 UTC"
  },
  {
    "arxiv_id": "2409.01903v1",
    "title": "A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management",
    "authors": [
      "Abdelmalek Mouazer",
      "Sophie Dubois",
      "Romain Léguillon",
      "Nada Boudegzdame",
      "Thibaud Levrard",
      "Yoann Le Bars",
      "Christian Simon",
      "Brigitte Séroussi",
      "Julien Grosjean",
      "Romain Lelong",
      "Catherine Letord",
      "Stéfan Darmoni",
      "Karima Sedki",
      "Pierre Meneton",
      "Rosy Tsopra",
      "Hector Falcoff",
      "Jean-Baptiste Lamy"
    ],
    "abstract": "Background: Medication review is a structured interview of the patient,\nperformed by the pharmacist and aimed at optimizing drug treatments. In\npractice, medication review is a long and cognitively-demanding task that\nrequires specific knowledge. Clinical practice guidelines have been proposed,\nbut their application is tedious. Methods: We designed ABiMed, a clinical\ndecision support system for medication reviews, based on the implementation of\nthe STOPP/START v2 guidelines and on the visual presentation of aggregated drug\nknowledge using tables, graphs and flower glyphs. We evaluated ABiMed with 39\ncommunity pharmacists during a randomized simulation trial, each pharmacist\nperforming a medication review for two fictitious patients without ABiMed, and\ntwo others with ABiMed. We recorded the problems identified by the pharmacists,\nthe interventions proposed, the response time, the perceived usability and the\ncomments. Pharmacists' medication reviews were compared to an expert-designed\ngold standard. Results: With ABiMed, pharmacists found 1.6 times more relevant\ndrug-related problems during the medication review (p=1.1e-12) and proposed\nbetter interventions (p=9.8e-9), without needing more time (p=0.56). The System\nUsability Scale score is 82.7, which is ranked \"excellent\". In their comments,\npharmacists appreciated the visual aspect of ABiMed and its ability to compare\nthe current treatment with the proposed one. A multifactor analysis showed no\ndifference in the support offered by ABiMed according to the pharmacist's age\nor sex, in terms of percentage of problems identified or quality of the\nproposed interventions. Conclusions: The use of an intelligent and visual\nclinical decision support system can help pharmacists when they perform\nmedication reviews. Our main perspective is the validation of the system in\nclinical conditions.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "J.3"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01903v1",
    "published_date": "2024-09-03 13:50:59 UTC",
    "updated_date": "2024-09-03 13:50:59 UTC"
  },
  {
    "arxiv_id": "2409.03715v1",
    "title": "Applications and Advances of Artificial Intelligence in Music Generation:A Review",
    "authors": [
      "Yanxu Chen",
      "Linshu Huang",
      "Tian Gou"
    ],
    "abstract": "In recent years, artificial intelligence (AI) has made significant progress\nin the field of music generation, driving innovation in music creation and\napplications. This paper provides a systematic review of the latest research\nadvancements in AI music generation, covering key technologies, models,\ndatasets, evaluation methods, and their practical applications across various\nfields. The main contributions of this review include: (1) presenting a\ncomprehensive summary framework that systematically categorizes and compares\ndifferent technological approaches, including symbolic generation, audio\ngeneration, and hybrid models, helping readers better understand the full\nspectrum of technologies in the field; (2) offering an extensive survey of\ncurrent literature, covering emerging topics such as multimodal datasets and\nemotion expression evaluation, providing a broad reference for related\nresearch; (3) conducting a detailed analysis of the practical impact of AI\nmusic generation in various application domains, particularly in real-time\ninteraction and interdisciplinary applications, offering new perspectives and\ninsights; (4) summarizing the existing challenges and limitations of music\nquality evaluation methods and proposing potential future research directions,\naiming to promote the standardization and broader adoption of evaluation\ntechniques. Through these innovative summaries and analyses, this paper serves\nas a comprehensive reference tool for researchers and practitioners in AI music\ngeneration, while also outlining future directions for the field.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.03715v1",
    "published_date": "2024-09-03 13:50:55 UTC",
    "updated_date": "2024-09-03 13:50:55 UTC"
  },
  {
    "arxiv_id": "2409.01901v1",
    "title": "3D-LEX v1.0: 3D Lexicons for American Sign Language and Sign Language of the Netherlands",
    "authors": [
      "Oline Ranum",
      "Gomer Otterspeer",
      "Jari I. Andersen",
      "Robert G. Belleman",
      "Floris Roelofsen"
    ],
    "abstract": "In this work, we present an efficient approach for capturing sign language in\n3D, introduce the 3D-LEX v1.0 dataset, and detail a method for semi-automatic\nannotation of phonetic properties. Our procedure integrates three motion\ncapture techniques encompassing high-resolution 3D poses, 3D handshapes, and\ndepth-aware facial features, and attains an average sampling rate of one sign\nevery 10 seconds. This includes the time for presenting a sign example,\nperforming and recording the sign, and archiving the capture. The 3D-LEX\ndataset includes 1,000 signs from American Sign Language and an additional\n1,000 signs from the Sign Language of the Netherlands. We showcase the dataset\nutility by presenting a simple method for generating handshape annotations\ndirectly from 3D-LEX. We produce handshape labels for 1,000 signs from American\nSign Language and evaluate the labels in a sign recognition task. The labels\nenhance gloss recognition accuracy by 5% over using no handshape annotations,\nand by 1% over expert annotations. Our motion capture data supports in-depth\nanalysis of sign features and facilitates the generation of 2D projections from\nany viewpoint. The 3D-LEX collection has been aligned with existing sign\nlanguage benchmarks and linguistic resources, to support studies in 3D-aware\nsign language processing.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01901v1",
    "published_date": "2024-09-03 13:44:56 UTC",
    "updated_date": "2024-09-03 13:44:56 UTC"
  },
  {
    "arxiv_id": "2409.01893v1",
    "title": "What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices",
    "authors": [
      "Zhi Chen",
      "Qiguang Chen",
      "Libo Qin",
      "Qipeng Guo",
      "Haijun Lv",
      "Yicheng Zou",
      "Wanxiang Che",
      "Hang Yan",
      "Kai Chen",
      "Dahua Lin"
    ],
    "abstract": "Recent advancements in large language models (LLMs) with extended context\nwindows have significantly improved tasks such as information extraction,\nquestion answering, and complex planning scenarios. In order to achieve success\nin long context tasks, a large amount of work has been done to enhance the long\ncontext capabilities of the model through synthetic data. Existing methods\ntypically utilize the Self-Instruct framework to generate instruction tuning\ndata for better long context capability improvement. However, our preliminary\nexperiments indicate that less than 35% of generated samples are multi-hop, and\nmore than 40% exhibit poor quality, limiting comprehensive understanding and\nfurther research. To improve the quality of synthetic data, we propose the\nMulti-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a\nQuality Verification Agent, a Single-hop Question Generation Agent, a Multiple\nQuestion Sampling Strategy, and a Multi-hop Question Merger Agent. This\nframework improves the data quality, with the proportion of high-quality,\nmulti-hop, and diverse data exceeding 85%. Furthermore, we systematically\ninvestigate strategies for document selection, question merging, and validation\ntechniques through extensive experiments across various models. Our findings\nshow that our synthetic high-quality long-context instruction data\nsignificantly enhances model performance, even surpassing models trained on\nlarger amounts of human-annotated data. Our code is available at:\nhttps://github.com/WowCZ/LongMIT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2409.01893v1",
    "published_date": "2024-09-03 13:30:00 UTC",
    "updated_date": "2024-09-03 13:30:00 UTC"
  },
  {
    "arxiv_id": "2409.01876v3",
    "title": "CyberHost: Taming Audio-driven Avatar Diffusion Model with Region Codebook Attention",
    "authors": [
      "Gaojie Lin",
      "Jianwen Jiang",
      "Chao Liang",
      "Tianyun Zhong",
      "Jiaqi Yang",
      "Yanbo Zheng"
    ],
    "abstract": "Diffusion-based video generation technology has advanced significantly,\ncatalyzing a proliferation of research in human animation. However, the\nmajority of these studies are confined to same-modality driving settings, with\ncross-modality human body animation remaining relatively underexplored. In this\npaper, we introduce, an end-to-end audio-driven human animation framework that\nensures hand integrity, identity consistency, and natural motion. The key\ndesign of CyberHost is the Region Codebook Attention mechanism, which improves\nthe generation quality of facial and hand animations by integrating\nfine-grained local features with learned motion pattern priors. Furthermore, we\nhave developed a suite of human-prior-guided training strategies, including\nbody movement map, hand clarity score, pose-aligned reference feature, and\nlocal enhancement supervision, to improve synthesis results. To our knowledge,\nCyberHost is the first end-to-end audio-driven human diffusion model capable of\nfacilitating zero-shot video generation within the scope of human body.\nExtensive experiments demonstrate that CyberHost surpasses previous works in\nboth quantitative and qualitative aspects.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025 (Oral), Homepage: https://cyberhost.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2409.01876v3",
    "published_date": "2024-09-03 13:19:31 UTC",
    "updated_date": "2025-04-05 05:31:38 UTC"
  },
  {
    "arxiv_id": "2409.01872v1",
    "title": "Latent Distillation for Continual Object Detection at the Edge",
    "authors": [
      "Francesco Pasti",
      "Marina Ceccon",
      "Davide Dalle Pezze",
      "Francesco Paissan",
      "Elisabetta Farella",
      "Gian Antonio Susto",
      "Nicola Bellotto"
    ],
    "abstract": "While numerous methods achieving remarkable performance exist in the Object\nDetection literature, addressing data distribution shifts remains challenging.\nContinual Learning (CL) offers solutions to this issue, enabling models to\nadapt to new data while maintaining performance on previous data. This is\nparticularly pertinent for edge devices, common in dynamic environments like\nautomotive and robotics. In this work, we address the memory and computation\nconstraints of edge devices in the Continual Learning for Object Detection\n(CLOD) scenario. Specifically, (i) we investigate the suitability of an\nopen-source, lightweight, and fast detector, namely NanoDet, for CLOD on edge\ndevices, improving upon larger architectures used in the literature. Moreover,\n(ii) we propose a novel CL method, called Latent Distillation~(LD), that\nreduces the number of operations and the memory required by state-of-the-art CL\napproaches without significantly compromising detection performance. Our\napproach is validated using the well-known VOC and COCO benchmarks, reducing\nthe distillation parameter overhead by 74\\% and the Floating Points\nOperations~(FLOPs) by 56\\% per model update compared to other distillation\nmethods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV workshops, Computational Aspects of Deep Learning (CADL) 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01872v1",
    "published_date": "2024-09-03 13:14:13 UTC",
    "updated_date": "2024-09-03 13:14:13 UTC"
  },
  {
    "arxiv_id": "2409.01871v1",
    "title": "Real-Time Indoor Object Detection based on hybrid CNN-Transformer Approach",
    "authors": [
      "Salah Eddine Laidoudi",
      "Madjid Maidi",
      "Samir Otmane"
    ],
    "abstract": "Real-time object detection in indoor settings is a challenging area of\ncomputer vision, faced with unique obstacles such as variable lighting and\ncomplex backgrounds. This field holds significant potential to revolutionize\napplications like augmented and mixed realities by enabling more seamless\ninteractions between digital content and the physical world. However, the\nscarcity of research specifically fitted to the intricacies of indoor\nenvironments has highlighted a clear gap in the literature. To address this,\nour study delves into the evaluation of existing datasets and computational\nmodels, leading to the creation of a refined dataset. This new dataset is\nderived from OpenImages v7, focusing exclusively on 32 indoor categories\nselected for their relevance to real-world applications. Alongside this, we\npresent an adaptation of a CNN detection model, incorporating an attention\nmechanism to enhance the model's ability to discern and prioritize critical\nfeatures within cluttered indoor scenes. Our findings demonstrate that this\napproach is not just competitive with existing state-of-the-art models in\naccuracy and speed but also opens new avenues for research and application in\nthe field of real-time indoor object detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01871v1",
    "published_date": "2024-09-03 13:14:08 UTC",
    "updated_date": "2024-09-03 13:14:08 UTC"
  },
  {
    "arxiv_id": "2409.01864v1",
    "title": "The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?",
    "authors": [
      "Pedro Ramoneda",
      "Emilia Parada-Cabaleiro",
      "Benno Weck",
      "Xavier Serra"
    ],
    "abstract": "In this work, we explore the use and reliability of Large Language Models\n(LLMs) in musicology. From a discussion with experts and students, we assess\nthe current acceptance and concerns regarding this, nowadays ubiquitous,\ntechnology. We aim to go one step further, proposing a semi-automatic method to\ncreate an initial benchmark using retrieval-augmented generation models and\nmultiple-choice question generation, validated by human experts. Our evaluation\non 400 human-validated questions shows that current vanilla LLMs are less\nreliable than retrieval augmented generation from music dictionaries. This\npaper suggests that the potential of LLMs in musicology requires musicology\ndriven research that can specialized LLMs by including accurate and reliable\ndomain knowledge.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.DL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01864v1",
    "published_date": "2024-09-03 13:05:38 UTC",
    "updated_date": "2024-09-03 13:05:38 UTC"
  },
  {
    "arxiv_id": "2409.02958v1",
    "title": "Multi-Modal Adapter for Vision-Language Models",
    "authors": [
      "Dominykas Seputis",
      "Serghei Mihailov",
      "Soham Chatterjee",
      "Zehao Xiao"
    ],
    "abstract": "Large pre-trained vision-language models, such as CLIP, have demonstrated\nstate-of-the-art performance across a wide range of image classification tasks,\nwithout requiring retraining. Few-shot CLIP is competitive with existing\nspecialized architectures that were trained on the downstream tasks. Recent\nresearch demonstrates that the performance of CLIP can be further improved\nusing lightweight adaptation approaches. However, previous methods adapt\ndifferent modalities of the CLIP model individually, ignoring the interactions\nand relationships between visual and textual representations. In this work, we\npropose Multi-Modal Adapter, an approach for Multi-Modal adaptation of CLIP.\nSpecifically, we add a trainable Multi-Head Attention layer that combines text\nand image features to produce an additive adaptation of both. Multi-Modal\nAdapter demonstrates improved generalizability, based on its performance on\nunseen classes compared to existing adaptation methods. We perform additional\nablations and investigations to validate and interpret the proposed approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02958v1",
    "published_date": "2024-09-03 12:47:08 UTC",
    "updated_date": "2024-09-03 12:47:08 UTC"
  },
  {
    "arxiv_id": "2409.02152v1",
    "title": "Fair Railway Network Design",
    "authors": [
      "Zixu He",
      "Sirin Botan",
      "Jérôme Lang",
      "Abdallah Saffidine",
      "Florian Sikora",
      "Silas Workman"
    ],
    "abstract": "When designing a public transportation network in a country, one may want to\nminimise the sum of travel duration of all inhabitants. This corresponds to a\npurely utilitarian view and does not involve any fairness consideration, as the\nresulting network will typically benefit the capital city and/or large central\ncities while leaving some peripheral cities behind. On the other hand, a more\negalitarian view will allow some people to travel between peripheral cities\nwithout having to go through a central city. We define a model, propose\nalgorithms for computing solution networks, and report on experiments based on\nreal data.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "32 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.02152v1",
    "published_date": "2024-09-03 12:13:05 UTC",
    "updated_date": "2024-09-03 12:13:05 UTC"
  },
  {
    "arxiv_id": "2409.01815v1",
    "title": "Learning State-Dependent Policy Parametrizations for Dynamic Technician Routing with Rework",
    "authors": [
      "Jonas Stein",
      "Florentin D Hildebrandt",
      "Barrett W Thomas",
      "Marlin W Ulmer"
    ],
    "abstract": "Home repair and installation services require technicians to visit customers\nand resolve tasks of different complexity. Technicians often have heterogeneous\nskills and working experiences. The geographical spread of customers makes\nachieving only perfect matches between technician skills and task requirements\nimpractical. Additionally, technicians are regularly absent due to sickness.\nWith non-perfect assignments regarding task requirement and technician skill,\nsome tasks may remain unresolved and require a revisit and rework. Companies\nseek to minimize customer inconvenience due to delay. We model the problem as a\nsequential decision process where, over a number of service days, customers\nrequest service while heterogeneously skilled technicians are routed to serve\ncustomers in the system. Each day, our policy iteratively builds tours by\nadding \"important\" customers. The importance bases on analytical considerations\nand is measured by respecting routing efficiency, urgency of service, and risk\nof rework in an integrated fashion. We propose a state-dependent balance of\nthese factors via reinforcement learning. A comprehensive study shows that\ntaking a few non-perfect assignments can be quite beneficial for the overall\nservice quality. We further demonstrate the value provided by a state-dependent\nparametrization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01815v1",
    "published_date": "2024-09-03 11:56:58 UTC",
    "updated_date": "2024-09-03 11:56:58 UTC"
  },
  {
    "arxiv_id": "2409.12182v2",
    "title": "LifeGPT: Topology-Agnostic Generative Pretrained Transformer Model for Cellular Automata",
    "authors": [
      "Jaime A. Berkovich",
      "Markus J. Buehler"
    ],
    "abstract": "Conway's Game of Life (Life), a well known algorithm within the broader class\nof cellular automata (CA), exhibits complex emergent dynamics, with extreme\nsensitivity to initial conditions. Modeling and predicting such intricate\nbehavior without explicit knowledge of the system's underlying topology\npresents a significant challenge, motivating the development of algorithms that\ncan generalize across various grid configurations and boundary conditions. We\ndevelop a decoder-only generative pretrained transformer (GPT) model to solve\nthis problem, showing that our model can simulate Life on a toroidal grid with\nno prior knowledge on the size of the grid, or its periodic boundary conditions\n(LifeGPT). LifeGPT is topology-agnostic with respect to its training data and\nour results show that a GPT model is capable of capturing the deterministic\nrules of a Turing-complete system with near-perfect accuracy, given\nsufficiently diverse training data. We also introduce the idea of an\n`autoregressive autoregressor' to recursively implement Life using LifeGPT. Our\nresults pave the path towards true universal computation within a large\nlanguage model framework, synthesizing of mathematical analysis with natural\nlanguage processing, and probing AI systems for situational awareness about the\nevolution of such algorithms without ever having to compute them. Similar GPTs\ncould potentially solve inverse problems in multicellular self-assembly by\nextracting CA-compatible rulesets from real-world biological systems to create\nnew predictive models, which would have significant consequences for the fields\nof bioinspired materials, tissue engineering, and architected materials design.",
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci",
      "cond-mat.stat-mech",
      "math.DS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12182v2",
    "published_date": "2024-09-03 11:43:16 UTC",
    "updated_date": "2024-10-17 16:55:02 UTC"
  },
  {
    "arxiv_id": "2409.01808v2",
    "title": "Dialogue You Can Trust: Human and AI Perspectives on Generated Conversations",
    "authors": [
      "Ike Ebubechukwu",
      "Johane Takeuchi",
      "Antonello Ceravola",
      "Frank Joublin"
    ],
    "abstract": "As dialogue systems and chatbots increasingly integrate into everyday\ninteractions, the need for efficient and accurate evaluation methods becomes\nparamount. This study explores the comparative performance of human and AI\nassessments across a range of dialogue scenarios, focusing on seven key\nperformance indicators (KPIs): Coherence, Innovation, Concreteness, Goal\nContribution, Commonsense Contradiction, Incorrect Fact, and Redundancy.\nUtilizing the GPT-4o API, we generated a diverse dataset of conversations and\nconducted a two-part experimental analysis. In Experiment 1, we evaluated\nmulti-party conversations on Coherence, Innovation, Concreteness, and Goal\nContribution, revealing that GPT models align closely with human judgments.\nNotably, both human and AI evaluators exhibited a tendency towards binary\njudgment rather than linear scaling, highlighting a shared challenge in these\nassessments. Experiment 2 extended the work of Finch et al. (2023) by focusing\non dyadic dialogues and assessing Commonsense Contradiction, Incorrect Fact,\nand Redundancy. The results indicate that while GPT-4o demonstrates strong\nperformance in maintaining factual accuracy and commonsense reasoning, it still\nstruggles with reducing redundancy and self-contradiction. Our findings\nunderscore the potential of GPT models to closely replicate human evaluation in\ndialogue systems, while also pointing to areas for improvement. This research\noffers valuable insights for advancing the development and implementation of\nmore refined dialogue evaluation methodologies, contributing to the evolution\nof more effective and human-like AI communication tools.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01808v2",
    "published_date": "2024-09-03 11:40:38 UTC",
    "updated_date": "2024-09-10 13:33:46 UTC"
  },
  {
    "arxiv_id": "2409.01806v1",
    "title": "LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning",
    "authors": [
      "Haoming Li",
      "Zhaoliang Chen",
      "Jonathan Zhang",
      "Fei Liu"
    ],
    "abstract": "Effective planning is essential for the success of any task, from organizing\na vacation to routing autonomous vehicles and developing corporate strategies.\nIt involves setting goals, formulating plans, and allocating resources to\nachieve them. LLMs are particularly well-suited for automated planning due to\ntheir strong capabilities in commonsense reasoning. They can deduce a sequence\nof actions needed to achieve a goal from a given state and identify an\neffective course of action. However, it is frequently observed that plans\ngenerated through direct prompting often fail upon execution. Our survey aims\nto highlight the existing challenges in planning with language models, focusing\non key areas such as embodied environments, optimal scheduling, competitive and\ncooperative games, task decomposition, reasoning, and planning. Through this\nstudy, we explore how LLMs transform AI planning and provide unique insights\ninto the future of LM-assisted planning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01806v1",
    "published_date": "2024-09-03 11:39:52 UTC",
    "updated_date": "2024-09-03 11:39:52 UTC"
  },
  {
    "arxiv_id": "2409.01790v2",
    "title": "Training on the Benchmark Is Not All You Need",
    "authors": [
      "Shiwen Ni",
      "Xiangtao Kong",
      "Chengming Li",
      "Xiping Hu",
      "Ruifeng Xu",
      "Jia Zhu",
      "Min Yang"
    ],
    "abstract": "The success of Large Language Models (LLMs) relies heavily on the huge amount\nof pre-training data learned in the pre-training phase. The opacity of the\npre-training process and the training data causes the results of many benchmark\ntests to become unreliable. If any model has been trained on a benchmark test\nset, it can seriously hinder the health of the field. In order to automate and\nefficiently test the capabilities of large language models, numerous mainstream\nbenchmarks adopt a multiple-choice format. As the swapping of the contents of\nmultiple-choice options does not affect the meaning of the question itself, we\npropose a simple and effective data leakage detection method based on this\nproperty. Specifically, we shuffle the contents of the options in the data to\ngenerate the corresponding derived data sets, and then detect data leakage\nbased on the model's log probability distribution over the derived data sets.\nIf there is a maximum and outlier in the set of log probabilities, it indicates\nthat the data is leaked. Our method is able to work under gray-box conditions\nwithout access to model training data or weights, effectively identifying data\nleakage from benchmark test sets in model pre-training data, including both\nnormal scenarios and complex scenarios where options may have been shuffled\nintentionally or unintentionally. Through experiments based on two LLMs and\nbenchmark designs, we demonstrate the effectiveness of our method. In addition,\nwe evaluate the degree of data leakage of 35 mainstream open-source LLMs on\nfour benchmark datasets and give a ranking of the leaked LLMs for each\nbenchmark, and we find that the Qwen family of LLMs has the highest degree of\ndata leakage.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01790v2",
    "published_date": "2024-09-03 11:09:44 UTC",
    "updated_date": "2025-02-28 02:40:58 UTC"
  },
  {
    "arxiv_id": "2409.04465v1",
    "title": "Here's Charlie! Realising the Semantic Web vision of Agents in the age of LLMs",
    "authors": [
      "Jesse Wright"
    ],
    "abstract": "This paper presents our research towards a near-term future in which legal\nentities, such as individuals and organisations can entrust semi-autonomous\nAI-driven agents to carry out online interactions on their behalf. The author's\nresearch concerns the development of semi-autonomous Web agents, which consult\nusers if and only if the system does not have sufficient context or confidence\nto proceed working autonomously. This creates a user-agent dialogue that allows\nthe user to teach the agent about the information sources they trust, their\ndata-sharing preferences, and their decision-making preferences. Ultimately,\nthis enables the user to maximise control over their data and decisions while\nretaining the convenience of using agents, including those driven by LLMs.\n  In view of developing near-term solutions, the research seeks to answer the\nquestion: \"How do we build a trustworthy and reliable network of\nsemi-autonomous agents which represent individuals and organisations on the\nWeb?\". After identifying key requirements, the paper presents a demo for a\nsample use case of a generic personal assistant. This is implemented using\n(Notation3) rules to enforce safety guarantees around belief, data sharing and\ndata usage and LLMs to allow natural language interaction with users and\nserendipitous dialogues between software agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The 23rd International Semantic Web Conference, November 11--15,\n  2024, Hanover, MD - Posters and Demos track",
    "pdf_url": "http://arxiv.org/pdf/2409.04465v1",
    "published_date": "2024-09-03 10:32:47 UTC",
    "updated_date": "2024-09-03 10:32:47 UTC"
  },
  {
    "arxiv_id": "2409.03793v3",
    "title": "Safeguarding AI Agents: Developing and Analyzing Safety Architectures",
    "authors": [
      "Ishaan Domkundwar",
      "Mukunda N S",
      "Ishaan Bhola",
      "Riddhik Kochhar"
    ],
    "abstract": "AI agents, specifically powered by large language models, have demonstrated\nexceptional capabilities in various applications where precision and efficacy\nare necessary. However, these agents come with inherent risks, including the\npotential for unsafe or biased actions, vulnerability to adversarial attacks,\nlack of transparency, and tendency to generate hallucinations. As AI agents\nbecome more prevalent in critical sectors of the industry, the implementation\nof effective safety protocols becomes increasingly important. This paper\naddresses the critical need for safety measures in AI systems, especially ones\nthat collaborate with human teams. We propose and evaluate three frameworks to\nenhance safety protocols in AI agent systems: an LLM-powered input-output\nfilter, a safety agent integrated within the system, and a hierarchical\ndelegation-based system with embedded safety checks. Our methodology involves\nimplementing these frameworks and testing them against a set of unsafe agentic\nuse cases, providing a comprehensive evaluation of their effectiveness in\nmitigating risks associated with AI agent deployment. We conclude that these\nframeworks can significantly strengthen the safety and security of AI agent\nsystems, minimizing potential harmful actions or outputs. Our work contributes\nto the ongoing effort to create safe and reliable AI applications, particularly\nin automated operations, and provides a foundation for developing robust\nguardrails to ensure the responsible use of AI agents in real-world\napplications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.03793v3",
    "published_date": "2024-09-03 10:14:51 UTC",
    "updated_date": "2025-02-28 22:26:17 UTC"
  },
  {
    "arxiv_id": "2409.01754v1",
    "title": "Empirical evidence of Large Language Model's influence on human spoken communication",
    "authors": [
      "Hiromu Yakura",
      "Ezequiel Lopez-Lopez",
      "Levin Brinkmann",
      "Ignacio Serna",
      "Prateek Gupta",
      "Iyad Rahwan"
    ],
    "abstract": "Artificial Intelligence (AI) agents now interact with billions of humans in\nnatural language, thanks to advances in Large Language Models (LLMs) like\nChatGPT. This raises the question of whether AI has the potential to shape a\nfundamental aspect of human culture: the way we speak. Recent analyses revealed\nthat scientific publications already exhibit evidence of AI-specific language.\nBut this evidence is inconclusive, since scientists may simply be using AI to\ncopy-edit their writing. To explore whether AI has influenced human spoken\ncommunication, we transcribed and analyzed about 280,000 English-language\nvideos of presentations, talks, and speeches from more than 20,000 YouTube\nchannels of academic institutions. We find a significant shift in the trend of\nword usage specific to words distinctively associated with ChatGPT following\nits release. These findings provide the first empirical evidence that humans\nincreasingly imitate LLMs in their spoken language. Our results raise societal\nand policy-relevant concerns about the potential of AI to unintentionally\nreduce linguistic diversity, or to be deliberately misused for mass\nmanipulation. They also highlight the need for further investigation into the\nfeedback loops between machine behavior and human culture.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01754v1",
    "published_date": "2024-09-03 10:01:51 UTC",
    "updated_date": "2024-09-03 10:01:51 UTC"
  },
  {
    "arxiv_id": "2409.02148v1",
    "title": "Optimal Power Grid Operations with Foundation Models",
    "authors": [
      "Alban Puech",
      "Jonas Weiss",
      "Thomas Brunschwiler",
      "Hendrik F. Hamann"
    ],
    "abstract": "The energy transition, crucial for tackling the climate crisis, demands\nintegrating numerous distributed, renewable energy sources into existing grids.\nAlong with climate change and consumer behavioral changes, this leads to\nchanges and variability in generation and load patterns, introducing\nsignificant complexity and uncertainty into grid planning and operations. While\nthe industry has already started to exploit AI to overcome computational\nchallenges of established grid simulation tools, we propose the use of AI\nFoundation Models (FMs) and advances in Graph Neural Networks to efficiently\nexploit poorly available grid data for different downstream tasks, enhancing\ngrid operations. For capturing the grid's underlying physics, we believe that\nbuilding a self-supervised model learning the power flow dynamics is a critical\nfirst step towards developing an FM for the power grid. We show how this\napproach may close the gap between the industry needs and current grid analysis\ncapabilities, to bring the industry closer to optimal grid operation and\nplanning.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02148v1",
    "published_date": "2024-09-03 09:06:13 UTC",
    "updated_date": "2024-09-03 09:06:13 UTC"
  },
  {
    "arxiv_id": "2409.01713v2",
    "title": "Interpreting Outliers in Time Series Data through Decoding Autoencoder",
    "authors": [
      "Patrick Knab",
      "Sascha Marton",
      "Christian Bartelt",
      "Robert Fuder"
    ],
    "abstract": "Outlier detection is a crucial analytical tool in various fields. In critical\nsystems like manufacturing, malfunctioning outlier detection can be costly and\nsafety-critical. Therefore, there is a significant need for explainable\nartificial intelligence (XAI) when deploying opaque models in such\nenvironments. This study focuses on manufacturing time series data from a\nGerman automotive supply industry. We utilize autoencoders to compress the\nentire time series and then apply anomaly detection techniques to its latent\nfeatures. For outlier interpretation, we (i) adopt widely used XAI techniques\nto the autoencoder's encoder. Additionally, (ii) we propose AEE, Aggregated\nExplanatory Ensemble, a novel approach that fuses explanations of multiple XAI\ntechniques into a single, more expressive interpretation. For evaluation of\nexplanations, (iii) we propose a technique to measure the quality of encoder\nexplanations quantitatively. Furthermore, we qualitatively assess the\neffectiveness of outlier explanations with domain expertise.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 8 figures, accepted at TempXAI @ ECML-PKDD, published in\n  CEUR Workshop Proceedings, Vol. 3761. https://ceur-ws.org/Vol-3761/paper3.pdf",
    "pdf_url": "http://arxiv.org/pdf/2409.01713v2",
    "published_date": "2024-09-03 08:52:21 UTC",
    "updated_date": "2025-02-03 10:56:36 UTC"
  },
  {
    "arxiv_id": "2409.01695v1",
    "title": "USTC-KXDIGIT System Description for ASVspoof5 Challenge",
    "authors": [
      "Yihao Chen",
      "Haochen Wu",
      "Nan Jiang",
      "Xiang Xia",
      "Qing Gu",
      "Yunqi Hao",
      "Pengfei Cai",
      "Yu Guan",
      "Jialong Wang",
      "Weilin Xie",
      "Lei Fang",
      "Sian Fang",
      "Yan Song",
      "Wu Guo",
      "Lin Liu",
      "Minqiang Xu"
    ],
    "abstract": "This paper describes the USTC-KXDIGIT system submitted to the ASVspoof5\nChallenge for Track 1 (speech deepfake detection) and Track 2 (spoofing-robust\nautomatic speaker verification, SASV). Track 1 showcases a diverse range of\ntechnical qualities from potential processing algorithms and includes both open\nand closed conditions. For these conditions, our system consists of a cascade\nof a frontend feature extractor and a back-end classifier. We focus on\nextensive embedding engineering and enhancing the generalization of the\nback-end classifier model. Specifically, the embedding engineering is based on\nhand-crafted features and speech representations from a self-supervised model,\nused for closed and open conditions, respectively. To detect spoof attacks\nunder various adversarial conditions, we trained multiple systems on an\naugmented training set. Additionally, we used voice conversion technology to\nsynthesize fake audio from genuine audio in the training set to enrich the\nsynthesis algorithms. To leverage the complementary information learned by\ndifferent model architectures, we employed activation ensemble and fused scores\nfrom different systems to obtain the final decision score for spoof detection.\nDuring the evaluation phase, the proposed methods achieved 0.3948 minDCF and\n14.33% EER in the close condition, and 0.0750 minDCF and 2.59% EER in the open\ncondition, demonstrating the robustness of our submitted systems under\nadversarial conditions. In Track 2, we continued using the CM system from Track\n1 and fused it with a CNN-based ASV system. This approach achieved 0.2814\nmin-aDCF in the closed condition and 0.0756 min-aDCF in the open condition,\nshowcasing superior performance in the SASV system.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "ASVspoof5 workshop paper",
    "pdf_url": "http://arxiv.org/pdf/2409.01695v1",
    "published_date": "2024-09-03 08:28:58 UTC",
    "updated_date": "2024-09-03 08:28:58 UTC"
  },
  {
    "arxiv_id": "2409.12979v1",
    "title": "Can we only use guideline instead of shot in prompt?",
    "authors": [
      "Jiaxiang Chen",
      "Song Wang",
      "Zhucong Li",
      "Wayne Xiong",
      "Lizhen Qu",
      "Zenglin Xu",
      "Yuan Qi"
    ],
    "abstract": "Currently, prompting techniques can be mainly divided into two\ncategories:1)shot method implicitly inspires the model to answer the question\nby mimicing the steps in the given example, e.g., the few-shot CoT. 2)\nGuideline method explicitly instructs the model to reason by following\nguidelines, which contains succinct and concise task-specific knowledge. Shot\nmethod is prone to difficulties in terms of selection of shots type, the number\nof shots, and the design of the reasoning steps, so a question arises: can we\nonly use guideline instead of shot in the prompt? To this end, we propose the\nFGT framework to automatically learn task-specific guidelines from dataset\nconsisting of Feedback, Guideline, and Tree-gather agents. First, the feedback\nagent is designed to evaluate the outcomes, both right and wrong, of each Q&A\nto gather insights guiding more effective optimization strategies. Next, the\nguideline agent is tasked with deriving guidelines from each piece of feedback\nand storing them in local memory. Lastly, the tree-gather agent aggregates all\nguidelines hierarchically through a tree structure, ultimately obtaining all\nunduplicated guidelines from a global perspective. In addition, we induce the\nmodel to generate intermediate processes to ensure the reasoning consistent\nwith the guidelines. Experimental results demonstrate that our approach\nachieves superior performance across multiple tasks, thereby highlighting the\neffectiveness of using the guidelines in prompt.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12979v1",
    "published_date": "2024-09-03 08:14:55 UTC",
    "updated_date": "2024-09-03 08:14:55 UTC"
  },
  {
    "arxiv_id": "2409.01688v3",
    "title": "Differentially Private Kernel Density Estimation",
    "authors": [
      "Erzhi Liu",
      "Jerry Yao-Chieh Hu",
      "Alex Reneau",
      "Zhao Song",
      "Han Liu"
    ],
    "abstract": "We introduce a refined differentially private (DP) data structure for kernel\ndensity estimation (KDE), offering not only improved privacy-utility tradeoff\nbut also better efficiency over prior results. Specifically, we study the\nmathematical problem: given a similarity function $f$ (or DP KDE) and a private\ndataset $X \\subset \\mathbb{R}^d$, our goal is to preprocess $X$ so that for any\nquery $y\\in\\mathbb{R}^d$, we approximate $\\sum_{x \\in X} f(x, y)$ in a\ndifferentially private fashion. The best previous algorithm for $f(x,y) =\\| x -\ny \\|_1$ is the node-contaminated balanced binary tree by [Backurs, Lin,\nMahabadi, Silwal, and Tarnawski, ICLR 2024]. Their algorithm requires $O(nd)$\nspace and time for preprocessing with $n=|X|$. For any query point, the query\ntime is $d \\log n$, with an error guarantee of $(1+\\alpha)$-approximation and\n$\\epsilon^{-1} \\alpha^{-0.5} d^{1.5} R \\log^{1.5} n$.\n  In this paper, we improve the best previous result [Backurs, Lin, Mahabadi,\nSilwal, and Tarnawski, ICLR 2024] in three aspects:\n  - We reduce query time by a factor of $\\alpha^{-1} \\log n$.\n  - We improve the approximation ratio from $\\alpha$ to 1.\n  - We reduce the error dependence by a factor of $\\alpha^{-0.5}$.\n  From a technical perspective, our method of constructing the search tree\ndiffers from previous work [Backurs, Lin, Mahabadi, Silwal, and Tarnawski, ICLR\n2024]. In prior work, for each query, the answer is split into $\\alpha^{-1}\n\\log n$ numbers, each derived from the summation of $\\log n$ values in interval\ntree countings. In contrast, we construct the tree differently, splitting the\nanswer into $\\log n$ numbers, where each is a smart combination of two distance\nvalues, two counting values, and $y$ itself. We believe our tree structure may\nbe of independent interest.",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.DS",
    "comment": "v2: Appendix added. v3: Numerical validations added",
    "pdf_url": "http://arxiv.org/pdf/2409.01688v3",
    "published_date": "2024-09-03 08:01:19 UTC",
    "updated_date": "2025-03-24 00:13:57 UTC"
  },
  {
    "arxiv_id": "2409.01679v2",
    "title": "Adaptive Explicit Knowledge Transfer for Knowledge Distillation",
    "authors": [
      "Hyungkeun Park",
      "Jong-Seok Lee"
    ],
    "abstract": "Logit-based knowledge distillation (KD) for classification is cost-efficient\ncompared to feature-based KD but often subject to inferior performance.\nRecently, it was shown that the performance of logit-based KD can be improved\nby effectively delivering the probability distribution for the non-target\nclasses from the teacher model, which is known as `implicit (dark) knowledge',\nto the student model. Through gradient analysis, we first show that this\nactually has an effect of adaptively controlling the learning of implicit\nknowledge. Then, we propose a new loss that enables the student to learn\nexplicit knowledge (i.e., the teacher's confidence about the target class)\nalong with implicit knowledge in an adaptive manner. Furthermore, we propose to\nseparate the classification and distillation tasks for effective distillation\nand inter-class relationship modeling. Experimental results demonstrate that\nthe proposed method, called adaptive explicit knowledge transfer (AEKT) method,\nachieves improved performance compared to the state-of-the-art KD methods on\nthe CIFAR-100 and ImageNet datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01679v2",
    "published_date": "2024-09-03 07:42:59 UTC",
    "updated_date": "2024-09-05 07:44:14 UTC"
  },
  {
    "arxiv_id": "2409.01676v1",
    "title": "Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring",
    "authors": [
      "Wenyang Hu",
      "Gaetan Frusque",
      "Tianyang Wang",
      "Fulei Chu",
      "Olga Fink"
    ],
    "abstract": "Deriving health indicators of rotating machines is crucial for their\nmaintenance. However, this process is challenging for the prevalent adopted\nintelligent methods since they may take the whole data distributions, not only\nintroducing noise interference but also lacking the explainability. To address\nthese issues, we propose a diffusion-based weakly-supervised approach for\nderiving health indicators of rotating machines, enabling early fault detection\nand continuous monitoring of condition evolution. This approach relies on a\nclassifier-free diffusion model trained using healthy samples and a few\nanomalies. This model generates healthy samples. and by comparing the\ndifferences between the original samples and the generated ones in the envelope\nspectrum, we construct an anomaly map that clearly identifies faults. Health\nindicators are then derived, which can explain the fault types and mitigate\nnoise interference. Comparative studies on two cases demonstrate that the\nproposed method offers superior health monitoring effectiveness and robustness\ncompared to baseline models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01676v1",
    "published_date": "2024-09-03 07:41:55 UTC",
    "updated_date": "2024-09-03 07:41:55 UTC"
  },
  {
    "arxiv_id": "2409.02145v1",
    "title": "A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction",
    "authors": [
      "Zekang Yang",
      "Hong Liu",
      "Xiangdong Wang"
    ],
    "abstract": "Computer-aided cancer survival risk prediction plays an important role in the\ntimely treatment of patients. This is a challenging weakly supervised ordinal\nregression task associated with multiple clinical factors involved such as\npathological images, genomic data and etc. In this paper, we propose a new\ntraining method, multimodal object-level contrast learning, for cancer survival\nrisk prediction. First, we construct contrast learning pairs based on the\nsurvival risk relationship among the samples in the training sample set. Then\nwe introduce the object-level contrast learning method to train the survival\nrisk predictor. We further extend it to the multimodal scenario by applying\ncross-modal constrast. Considering the heterogeneity of pathological images and\ngenomics data, we construct a multimodal survival risk predictor employing\nattention-based and self-normalizing based nerural network respectively.\nFinally, the survival risk predictor trained by our proposed method outperforms\nstate-of-the-art methods on two public multimodal cancer datasets for survival\nrisk prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02145v1",
    "published_date": "2024-09-03 07:36:34 UTC",
    "updated_date": "2024-09-03 07:36:34 UTC"
  },
  {
    "arxiv_id": "2409.01672v2",
    "title": "Enhancing Fine-Grained Visual Recognition in the Low-Data Regime Through Feature Magnitude Regularization",
    "authors": [
      "Avraham Chapman",
      "Haiming Xu",
      "Lingqiao Liu"
    ],
    "abstract": "Training a fine-grained image recognition model with limited data presents a\nsignificant challenge, as the subtle differences between categories may not be\neasily discernible amidst distracting noise patterns. One commonly employed\nstrategy is to leverage pretrained neural networks, which can generate\neffective feature representations for constructing an image classification\nmodel with a restricted dataset. However, these pretrained neural networks are\ntypically trained for different tasks than the fine-grained visual recognition\n(FGVR) task at hand, which can lead to the extraction of less relevant\nfeatures. Moreover, in the context of building FGVR models with limited data,\nthese irrelevant features can dominate the training process, overshadowing more\nuseful, generalizable discriminative features. Our research has identified a\nsurprisingly simple solution to this challenge: we introduce a regularization\ntechnique to ensure that the magnitudes of the extracted features are evenly\ndistributed. This regularization is achieved by maximizing the uniformity of\nfeature magnitude distribution, measured through the entropy of the normalized\nfeatures. The motivation behind this regularization is to remove bias in\nfeature magnitudes from pretrained models, where some features may be more\nprominent and, consequently, more likely to be used for classification.\nAdditionally, we have developed a dynamic weighting mechanism to adjust the\nstrength of this regularization throughout the learning process. Despite its\napparent simplicity, our approach has demonstrated significant performance\nimprovements across various fine-grained visual recognition datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01672v2",
    "published_date": "2024-09-03 07:32:46 UTC",
    "updated_date": "2024-09-07 05:36:52 UTC"
  },
  {
    "arxiv_id": "2409.04464v2",
    "title": "Leveraging Large Language Models for Solving Rare MIP Challenges",
    "authors": [
      "Teng Wang",
      "Wing-Yin Yu",
      "Ruifeng She",
      "Wenhan Yang",
      "Taijie Chen",
      "Jianping Zhang"
    ],
    "abstract": "Mixed Integer Programming (MIP) has been extensively applied in areas\nrequiring mathematical solvers to address complex instances within tight time\nconstraints. However, as the problem scale increases, the complexity of model\nformulation and finding feasible solutions escalates significantly. In\ncontrast, the model-building cost for end-to-end models, such as large language\nmodels (LLMs), remains largely unaffected by problem scale due to their pattern\nrecognition capabilities. While LLMs, like GPT-4, without fine-tuning, can\nhandle some traditional medium-scale MIP problems, they struggle with uncommon\nor highly specialized MIP scenarios. Fine-tuning LLMs can yield some feasible\nsolutions for medium-scale MIP instances, but these models typically fail to\nexplore diverse solutions when constrained by a low and constant temperature,\nlimiting their performance. In this paper, we propose and evaluate a\nrecursively dynamic temperature method integrated with a chain-of-thought\napproach. Our findings show that starting with a high temperature and gradually\nlowering it leads to better feasible solutions compared to other dynamic\ntemperature strategies. Additionally, by comparing results generated by the LLM\nwith those from Gurobi, we demonstrate that the LLM can produce solutions that\ncomplement traditional solvers by accelerating the pruning process and\nimproving overall efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.04464v2",
    "published_date": "2024-09-03 07:25:01 UTC",
    "updated_date": "2024-09-18 07:43:12 UTC"
  },
  {
    "arxiv_id": "2409.01668v3",
    "title": "Pureformer-VC: Non-parallel One-Shot Voice Conversion with Pure Transformer Blocks and Triplet Discriminative Training",
    "authors": [
      "Wenhan Yao",
      "Zedong Xing",
      "Xiarun Chen",
      "Jia Liu",
      "Yongqiang He",
      "Weiping Wen"
    ],
    "abstract": "One-shot voice conversion(VC) aims to change the timbre of any source speech\nto match that of the target speaker with only one speech sample. Existing style\ntransfer-based VC methods relied on speech representation disentanglement and\nsuffered from accurately and independently encoding each speech component and\nrecomposing back to converted speech effectively. To tackle this, we proposed\nPureformer-VC, which utilizes Conformer blocks to build a disentangled encoder,\nand Zipformer blocks to build a style transfer decoder as the generator. In the\ndecoder, we used effective styleformer blocks to integrate speaker\ncharacteristics effectively into the generated speech. The models used the\ngenerative VAE loss for encoding components and triplet loss for unsupervised\ndiscriminative training. We applied the styleformer method to Zipformer's\nshared weights for style transfer. The experimental results show that the\nproposed model achieves comparable subjective scores and exhibits improvements\nin objective metrics compared to existing methods in a one-shot voice\nconversion scenario.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "our paper is rejected",
    "pdf_url": "http://arxiv.org/pdf/2409.01668v3",
    "published_date": "2024-09-03 07:21:19 UTC",
    "updated_date": "2024-11-25 01:35:22 UTC"
  },
  {
    "arxiv_id": "2409.01652v2",
    "title": "ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation",
    "authors": [
      "Wenlong Huang",
      "Chen Wang",
      "Yunzhu Li",
      "Ruohan Zhang",
      "Li Fei-Fei"
    ],
    "abstract": "Representing robotic manipulation tasks as constraints that associate the\nrobot and the environment is a promising way to encode desired robot behaviors.\nHowever, it remains unclear how to formulate the constraints such that they are\n1) versatile to diverse tasks, 2) free of manual labeling, and 3) optimizable\nby off-the-shelf solvers to produce robot actions in real-time. In this work,\nwe introduce Relational Keypoint Constraints (ReKep), a visually-grounded\nrepresentation for constraints in robotic manipulation. Specifically, ReKep is\nexpressed as Python functions mapping a set of 3D keypoints in the environment\nto a numerical cost. We demonstrate that by representing a manipulation task as\na sequence of Relational Keypoint Constraints, we can employ a hierarchical\noptimization procedure to solve for robot actions (represented by a sequence of\nend-effector poses in SE(3)) with a perception-action loop at a real-time\nfrequency. Furthermore, in order to circumvent the need for manual\nspecification of ReKep for each new task, we devise an automated procedure that\nleverages large vision models and vision-language models to produce ReKep from\nfree-form language instructions and RGB-D observations. We present system\nimplementations on a wheeled single-arm platform and a stationary dual-arm\nplatform that can perform a large variety of manipulation tasks, featuring\nmulti-stage, in-the-wild, bimanual, and reactive behaviors, all without\ntask-specific data or environment models. Website at\nhttps://rekep-robot.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01652v2",
    "published_date": "2024-09-03 06:45:22 UTC",
    "updated_date": "2024-11-12 04:33:26 UTC"
  },
  {
    "arxiv_id": "2409.07482v1",
    "title": "VSLLaVA: a pipeline of large multimodal foundation model for industrial vibration signal analysis",
    "authors": [
      "Qi Li",
      "Jinfeng Huang",
      "Hongliang He",
      "Xinran Zhang",
      "Feibin Zhang",
      "Zhaoye Qin",
      "Fulei Chu"
    ],
    "abstract": "Large multimodal foundation models have been extensively utilized for image\nrecognition tasks guided by instructions, yet there remains a scarcity of\ndomain expertise in industrial vibration signal analysis. This paper presents a\npipeline named VSLLaVA that leverages a large language model to integrate\nexpert knowledge for identification of signal parameters and diagnosis of\nfaults. Within this pipeline, we first introduce an expert rule-assisted signal\ngenerator. The generator merges signal provided by vibration analysis experts\nwith domain-specific parameter identification and fault diagnosis\nquestion-answer pairs to build signal-question-answer triplets. Then we use\nthese triplets to apply low-rank adaptation methods for fine-tuning the linear\nlayers of the Contrastive Language-Image Pretraining (CLIP) and large language\nmodel, injecting multimodal signal processing knowledge. Finally, the\nfine-tuned model is assessed through the combined efforts of large language\nmodel and expert rules to evaluate answer accuracy and relevance, which\nshowcases enhanced performance in identifying, analyzing various signal\nparameters, and diagnosing faults. These enhancements indicate the potential of\nthis pipeline to build a foundational model for future industrial signal\nanalysis and monitoring.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07482v1",
    "published_date": "2024-09-03 06:21:26 UTC",
    "updated_date": "2024-09-03 06:21:26 UTC"
  },
  {
    "arxiv_id": "2409.01635v1",
    "title": "PMLBmini: A Tabular Classification Benchmark Suite for Data-Scarce Applications",
    "authors": [
      "Ricardo Knauer",
      "Marvin Grimm",
      "Erik Rodner"
    ],
    "abstract": "In practice, we are often faced with small-sized tabular data. However,\ncurrent tabular benchmarks are not geared towards data-scarce applications,\nmaking it very difficult to derive meaningful conclusions from empirical\ncomparisons. We introduce PMLBmini, a tabular benchmark suite of 44 binary\nclassification datasets with sample sizes $\\leq$ 500. We use our suite to\nthoroughly evaluate current automated machine learning (AutoML) frameworks,\noff-the-shelf tabular deep neural networks, as well as classical linear models\nin the low-data regime. Our analysis reveals that state-of-the-art AutoML and\ndeep learning approaches often fail to appreciably outperform even a simple\nlogistic regression baseline, but we also identify scenarios where AutoML and\ndeep learning methods are indeed reasonable to apply. Our benchmark suite,\navailable on https://github.com/RicardoKnauer/TabMini , allows researchers and\npractitioners to analyze their own methods and challenge their data efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "AutoML 2024 Workshop Track",
    "pdf_url": "http://arxiv.org/pdf/2409.01635v1",
    "published_date": "2024-09-03 06:13:03 UTC",
    "updated_date": "2024-09-03 06:13:03 UTC"
  },
  {
    "arxiv_id": "2409.01633v3",
    "title": "Dreaming is All You Need",
    "authors": [
      "Mingze Ni",
      "Wei Liu"
    ],
    "abstract": "In classification tasks, achieving a harmonious balance between exploration\nand precision is of paramount importance. To this end, this research introduces\ntwo novel deep learning models, SleepNet and DreamNet, to strike this balance.\nSleepNet seamlessly integrates supervised learning with unsupervised ``sleep\"\nstages using pre-trained encoder models. Dedicated neurons within SleepNet are\nembedded in these unsupervised features, forming intermittent ``sleep\" blocks\nthat facilitate exploratory learning. Building upon the foundation of SleepNet,\nDreamNet employs full encoder-decoder frameworks to reconstruct the hidden\nstates, mimicking the human \"dreaming\" process. This reconstruction process\nenables further exploration and refinement of the learned representations.\nMoreover, the principle ideas of our SleepNet and DreamNet are generic and can\nbe applied to both computer vision and natural language processing downstream\ntasks. Through extensive empirical evaluations on diverse image and text\ndatasets, SleepNet and DreanNet have demonstrated superior performance compared\nto state-of-the-art models, showcasing the strengths of unsupervised\nexploration and supervised precision afforded by our innovative approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01633v3",
    "published_date": "2024-09-03 06:04:39 UTC",
    "updated_date": "2024-09-15 12:17:03 UTC"
  },
  {
    "arxiv_id": "2409.15512v2",
    "title": "PixelBytes: Catching Unified Embedding for Multimodal Generation",
    "authors": [
      "Fabien Furfaro"
    ],
    "abstract": "This report introduces PixelBytes Embedding, a novel approach for unified\nmultimodal representation learning. Our method captures diverse inputs in a\nsingle, cohesive representation, enabling emergent properties for multimodal\nsequence generation, particularly for text and pixelated images. Inspired by\nstate-of-the-art sequence models such as Image Transformers, PixelCNN, and\nMamba-Bytes, PixelBytes aims to address the challenges of integrating different\ndata types. We explore various model architectures, including Recurrent Neural\nNetworks (RNNs), State Space Models (SSMs), and Attention-based models,\nfocusing on bidirectional processing and our innovative PxBy embedding\ntechnique. Our experiments, conducted on a specialized PixelBytes Pok{\\'e}mon\ndataset, demonstrate that bidirectional sequence models with PxBy embedding and\nconvolutional layers can generate coherent multimodal sequences. This work\ncontributes to the advancement of integrated AI models capable of understanding\nand generating multimodal data in a unified manner.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This article is an earlier version of my work arXiv:2410.01820\n  \"PixelBytes: Catching Unified Representation for Multimodal Generation.\"",
    "pdf_url": "http://arxiv.org/pdf/2409.15512v2",
    "published_date": "2024-09-03 06:02:02 UTC",
    "updated_date": "2024-10-21 18:57:08 UTC"
  },
  {
    "arxiv_id": "2409.01630v1",
    "title": "SafeEmbodAI: a Safety Framework for Mobile Robots in Embodied AI Systems",
    "authors": [
      "Wenxiao Zhang",
      "Xiangrui Kong",
      "Thomas Braunl",
      "Jin B. Hong"
    ],
    "abstract": "Embodied AI systems, including AI-powered robots that autonomously interact\nwith the physical world, stand to be significantly advanced by Large Language\nModels (LLMs), which enable robots to better understand complex language\ncommands and perform advanced tasks with enhanced comprehension and\nadaptability, highlighting their potential to improve embodied AI capabilities.\nHowever, this advancement also introduces safety challenges, particularly in\nrobotic navigation tasks. Improper safety management can lead to failures in\ncomplex environments and make the system vulnerable to malicious command\ninjections, resulting in unsafe behaviours such as detours or collisions. To\naddress these issues, we propose \\textit{SafeEmbodAI}, a safety framework for\nintegrating mobile robots into embodied AI systems. \\textit{SafeEmbodAI}\nincorporates secure prompting, state management, and safety validation\nmechanisms to secure and assist LLMs in reasoning through multi-modal data and\nvalidating responses. We designed a metric to evaluate mission-oriented\nexploration, and evaluations in simulated environments demonstrate that our\nframework effectively mitigates threats from malicious commands and improves\nperformance in various environment settings, ensuring the safety of embodied AI\nsystems. Notably, In complex environments with mixed obstacles, our method\ndemonstrates a significant performance increase of 267\\% compared to the\nbaseline in attack scenarios, highlighting its robustness in challenging\nconditions.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01630v1",
    "published_date": "2024-09-03 05:56:50 UTC",
    "updated_date": "2024-09-03 05:56:50 UTC"
  },
  {
    "arxiv_id": "2409.01622v1",
    "title": "T1-contrast Enhanced MRI Generation from Multi-parametric MRI for Glioma Patients with Latent Tumor Conditioning",
    "authors": [
      "Zach Eidex",
      "Mojtaba Safari",
      "Richard L. J. Qiu",
      "David S. Yu",
      "Hui-Kuo Shu",
      "Hui Mao",
      "Xiaofeng Yang"
    ],
    "abstract": "Objective: Gadolinium-based contrast agents (GBCAs) are commonly used in MRI\nscans of patients with gliomas to enhance brain tumor characterization using\nT1-weighted (T1W) MRI. However, there is growing concern about GBCA toxicity.\nThis study develops a deep-learning framework to generate T1-postcontrast (T1C)\nfrom pre-contrast multiparametric MRI. Approach: We propose the tumor-aware\nvision transformer (TA-ViT) model that predicts high-quality T1C images. The\npredicted tumor region is significantly improved (P < .001) by conditioning the\ntransformer layers from predicted segmentation maps through adaptive layer norm\nzero mechanism. The predicted segmentation maps were generated with the\nmulti-parametric residual (MPR) ViT model and transformed into a latent space\nto produce compressed, feature-rich representations. The TA-ViT model predicted\nT1C MRI images of 501 glioma cases. Selected patients were split into training\n(N=400), validation (N=50), and test (N=51) sets. Main Results: Both\nqualitative and quantitative results demonstrate that the TA-ViT model performs\nsuperior against the benchmark MRP-ViT model. Our method produces synthetic T1C\nMRI with high soft tissue contrast and more accurately reconstructs both the\ntumor and whole brain volumes. The synthesized T1C images achieved remarkable\nimprovements in both tumor and healthy tissue regions compared to the MRP-ViT\nmodel. For healthy tissue and tumor regions, the results were as follows: NMSE:\n8.53 +/- 4.61E-4; PSNR: 31.2 +/- 2.2; NCC: 0.908 +/- .041 and NMSE: 1.22 +/-\n1.27E-4, PSNR: 41.3 +/- 4.7, and NCC: 0.879 +/- 0.042, respectively.\nSignificance: The proposed method generates synthetic T1C images that closely\nresemble real T1C images. Future development and application of this approach\nmay enable contrast-agent-free MRI for brain tumor patients, eliminating the\nrisk of GBCA toxicity and simplifying the MRI scan protocol.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "arXiv admin note: text overlap with arXiv:2407.02616",
    "pdf_url": "http://arxiv.org/pdf/2409.01622v1",
    "published_date": "2024-09-03 05:45:37 UTC",
    "updated_date": "2024-09-03 05:45:37 UTC"
  },
  {
    "arxiv_id": "2409.01612v1",
    "title": "Lexicographic optimization-based approaches to learning a representative model for multi-criteria sorting with non-monotonic criteria",
    "authors": [
      "Zhen Zhang",
      "Zhuolin Li",
      "Wenyu Yu"
    ],
    "abstract": "Deriving a representative model using value function-based methods from the\nperspective of preference disaggregation has emerged as a prominent and growing\ntopic in multi-criteria sorting (MCS) problems. A noteworthy observation is\nthat many existing approaches to learning a representative model for MCS\nproblems traditionally assume the monotonicity of criteria, which may not\nalways align with the complexities found in real-world MCS scenarios.\nConsequently, this paper proposes some approaches to learning a representative\nmodel for MCS problems with non-monotonic criteria through the integration of\nthe threshold-based value-driven sorting procedure. To do so, we first define\nsome transformation functions to map the marginal values and category\nthresholds into a UTA-like functional space. Subsequently, we construct\nconstraint sets to model non-monotonic criteria in MCS problems and develop\noptimization models to check and rectify the inconsistency of the decision\nmaker's assignment example preference information. By simultaneously\nconsidering the complexity and discriminative power of the models, two distinct\nlexicographic optimization-based approaches are developed to derive a\nrepresentative model for MCS problems with non-monotonic criteria. Eventually,\nwe offer an illustrative example and conduct comprehensive simulation\nexperiments to elaborate the feasibility and validity of the proposed\napproaches.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "45 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01612v1",
    "published_date": "2024-09-03 05:29:05 UTC",
    "updated_date": "2024-09-03 05:29:05 UTC"
  },
  {
    "arxiv_id": "2409.01610v1",
    "title": "Decompose the model: Mechanistic interpretability in image models with Generalized Integrated Gradients (GIG)",
    "authors": [
      "Yearim Kim",
      "Sangyu Han",
      "Sangbum Han",
      "Nojun Kwak"
    ],
    "abstract": "In the field of eXplainable AI (XAI) in language models, the progression from\nlocal explanations of individual decisions to global explanations with\nhigh-level concepts has laid the groundwork for mechanistic interpretability,\nwhich aims to decode the exact operations. However, this paradigm has not been\nadequately explored in image models, where existing methods have primarily\nfocused on class-specific interpretations. This paper introduces a novel\napproach to systematically trace the entire pathway from input through all\nintermediate layers to the final output within the whole dataset. We utilize\nPointwise Feature Vectors (PFVs) and Effective Receptive Fields (ERFs) to\ndecompose model embeddings into interpretable Concept Vectors. Then, we\ncalculate the relevance between concept vectors with our Generalized Integrated\nGradients (GIG), enabling a comprehensive, dataset-wide analysis of model\nbehavior. We validate our method of concept extraction and concept attribution\nin both qualitative and quantitative evaluations. Our approach advances the\nunderstanding of semantic significance within image models, offering a holistic\nview of their operational mechanics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01610v1",
    "published_date": "2024-09-03 05:19:35 UTC",
    "updated_date": "2024-09-03 05:19:35 UTC"
  },
  {
    "arxiv_id": "2409.01605v1",
    "title": "Laser: Parameter-Efficient LLM Bi-Tuning for Sequential Recommendation with Collaborative Information",
    "authors": [
      "Xinyu Zhang",
      "Linmei Hu",
      "Luhao Zhang",
      "Dandan Song",
      "Heyan Huang",
      "Liqiang Nie"
    ],
    "abstract": "Sequential recommender systems are essential for discerning user preferences\nfrom historical interactions and facilitating targeted recommendations. Recent\ninnovations employing Large Language Models (LLMs) have advanced the field by\nencoding item semantics, yet they often necessitate substantial parameter\ntuning and are resource-demanding. Moreover, these works fails to consider the\ndiverse characteristics of different types of users and thus diminishes the\nrecommendation accuracy. In this paper, we propose a parameter-efficient Large\nLanguage Model Bi-Tuning framework for sequential recommendation with\ncollaborative information (Laser). Specifically, Bi-Tuning works by inserting\ntrainable virtual tokens at both the prefix and suffix of the input sequence\nand freezing the LLM parameters, thus optimizing the LLM for the sequential\nrecommendation. In our Laser, the prefix is utilized to incorporate user-item\ncollaborative information and adapt the LLM to the recommendation task, while\nthe suffix converts the output embeddings of the LLM from the language space to\nthe recommendation space for the follow-up item recommendation. Furthermore, to\ncapture the characteristics of different types of users when integrating the\ncollaborative information via the prefix, we introduce M-Former, a lightweight\nMoE-based querying transformer that uses a set of query experts to integrate\ndiverse user-specific collaborative information encoded by frozen ID-based\nsequential recommender systems, significantly improving the accuracy of\nrecommendations. Extensive experiments on real-world datasets demonstrate that\nLaser can parameter-efficiently adapt LLMs to effective recommender systems,\nsignificantly outperforming state-of-the-art methods.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01605v1",
    "published_date": "2024-09-03 04:55:03 UTC",
    "updated_date": "2024-09-03 04:55:03 UTC"
  },
  {
    "arxiv_id": "2409.01596v2",
    "title": "Synthesizing Late-Stage Contrast Enhancement in Breast MRI: A Comprehensive Pipeline Leveraging Temporal Contrast Enhancement Dynamics",
    "authors": [
      "Ruben D. Fonnegra",
      "Maria Liliana Hernández",
      "Juan C. Caicedo",
      "Gloria M. Díaz"
    ],
    "abstract": "Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is essential\nfor breast cancer diagnosis due to its ability to characterize tissue through\ncontrast agent kinetics. However, traditional DCE-MRI protocols require\nmultiple imaging phases, including early and late post-contrast acquisitions,\nleading to prolonged scan times, patient discomfort, motion artifacts, high\ncosts, and limited accessibility. To overcome these limitations, this study\npresents a pipeline for synthesizing late-phase DCE-MRI images from early-phase\ndata, replicating the time-intensity (TI) curve behavior in enhanced regions\nwhile maintaining visual fidelity across the entire image. The proposed\napproach introduces a novel loss function, Time Intensity Loss (TI-loss),\nleveraging the temporal behavior of contrast agents to guide the training of a\ngenerative model. Additionally, a new normalization strategy, TI-norm,\npreserves the contrast enhancement pattern across multiple image sequences at\nvarious timestamps, addressing limitations of conventional normalization\nmethods. Two metrics are proposed to evaluate image quality: the Contrast Agent\nPattern Score ($\\mathcal{CP}_{s}$), which validates enhancement patterns in\nannotated regions, and the Average Difference in Enhancement ($\\mathcal{ED}$),\nmeasuring differences between real and generated enhancements. Using a public\nDCE-MRI dataset with 1.5T and 3T scanners, the proposed method demonstrates\naccurate synthesis of late-phase images that outperform existing models in\nreplicating the TI curve's behavior in regions of interest while preserving\noverall image quality. This advancement shows a potential to optimize DCE-MRI\nprotocols by reducing scanning time without compromising diagnostic accuracy,\nand bringing generative models closer to practical implementation in clinical\nscenarios to enhance efficiency in breast cancer imaging.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01596v2",
    "published_date": "2024-09-03 04:31:49 UTC",
    "updated_date": "2025-01-24 21:21:08 UTC"
  },
  {
    "arxiv_id": "2409.01588v2",
    "title": "Large-scale Urban Facility Location Selection with Knowledge-informed Reinforcement Learning",
    "authors": [
      "Hongyuan Su",
      "Yu Zheng",
      "Jingtao Ding",
      "Depeng Jin",
      "Yong Li"
    ],
    "abstract": "The facility location problem (FLP) is a classical combinatorial optimization\nchallenge aimed at strategically laying out facilities to maximize their\naccessibility. In this paper, we propose a reinforcement learning method\ntailored to solve large-scale urban FLP, capable of producing near-optimal\nsolutions at superfast inference speed. We distill the essential swap operation\nfrom local search, and simulate it by intelligently selecting edges on a graph\nof urban regions, guided by a knowledge-informed graph neural network, thus\nsidestepping the need for heavy computation of local search. Extensive\nexperiments on four US cities with different geospatial conditions demonstrate\nthat our approach can achieve comparable performance to commercial solvers with\nless than 5\\% accessibility loss, while displaying up to 1000 times speedup. We\ndeploy our model as an online geospatial application at\nhttps://huggingface.co/spaces/randommmm/MFLP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "68T20"
    ],
    "primary_category": "cs.LG",
    "comment": "Sigspatial2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01588v2",
    "published_date": "2024-09-03 04:04:40 UTC",
    "updated_date": "2024-09-06 08:16:02 UTC"
  },
  {
    "arxiv_id": "2409.01586v4",
    "title": "Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation",
    "authors": [
      "Tiansheng Huang",
      "Sihao Hu",
      "Fatih Ilhan",
      "Selim Furkan Tekin",
      "Ling Liu"
    ],
    "abstract": "Harmful fine-tuning attack poses serious safety concerns for large language\nmodels' fine-tuning-as-a-service. While existing defenses have been proposed to\nmitigate the issue, their performances are still far away from satisfactory,\nand the root cause of the problem has not been fully recovered. To this end, we\nin this paper show that harmful perturbation over the model weights could be a\nprobable cause of alignment-broken. In order to attenuate the negative impact\nof harmful perturbation, we propose an alignment-stage solution, dubbed\nBooster. Technically, along with the original alignment loss, we append a loss\nregularizer in the alignment stage's optimization. The regularizer ensures that\nthe model's harmful loss reduction after the simulated harmful perturbation is\nattenuated, thereby mitigating the subsequent fine-tuning risk. Empirical\nresults show that Booster can effectively reduce the harmful score of the\nfine-tuned models while maintaining the performance of downstream tasks. Our\ncode is available at https://github.com/git-disl/Booster.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01586v4",
    "published_date": "2024-09-03 03:59:22 UTC",
    "updated_date": "2025-03-17 17:17:16 UTC"
  },
  {
    "arxiv_id": "2409.01581v1",
    "title": "GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color Point Clouds via 3D Gaussian Splatting",
    "authors": [
      "Zixuan Guo",
      "Yifan Xie",
      "Weijing Xie",
      "Peng Huang",
      "Fei Ma",
      "Fei Richard Yu"
    ],
    "abstract": "Dense colored point clouds enhance visual perception and are of significant\nvalue in various robotic applications. However, existing learning-based point\ncloud upsampling methods are constrained by computational resources and batch\nprocessing strategies, which often require subdividing point clouds into\nsmaller patches, leading to distortions that degrade perceptual quality. To\naddress this challenge, we propose a novel 2D-3D hybrid colored point cloud\nupsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for\nrobotic perception. This approach leverages 3DGS to bridge 3D point clouds with\ntheir 2D rendered images in robot vision systems. A dual scale rendered image\nrestoration network transforms sparse point cloud renderings into dense\nrepresentations, which are then input into 3DGS along with precise robot camera\nposes and interpolated sparse point clouds to reconstruct dense 3D point\nclouds. We have made a series of enhancements to the vanilla 3DGS, enabling\nprecise control over the number of points and significantly boosting the\nquality of the upsampled point cloud for robotic scene understanding. Our\nframework supports processing entire point clouds on a single consumer-grade\nGPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation\nand thus producing high-quality, dense colored point clouds with millions of\npoints for robot navigation and manipulation tasks. Extensive experimental\nresults on generating million-level point cloud data validate the effectiveness\nof our method, substantially improving the quality of colored point clouds and\ndemonstrating significant potential for applications involving large-scale\npoint clouds in autonomous robotics and human-robot interaction scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01581v1",
    "published_date": "2024-09-03 03:35:04 UTC",
    "updated_date": "2024-09-03 03:35:04 UTC"
  },
  {
    "arxiv_id": "2409.13694v3",
    "title": "Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A Benchmark and Empirical Study",
    "authors": [
      "Shuo Yu",
      "Mingyue Cheng",
      "Jiqian Yang",
      "Jie Ouyang",
      "Yucong Luo",
      "Chenyi Lei",
      "Qi Liu",
      "Enhong Chen"
    ],
    "abstract": "Retrieval-augmented generation (RAG) is increasingly recognized as an\neffective approach to mitigating the hallucination of large language models\n(LLMs) through the integration of external knowledge. While numerous efforts,\nmost studies focus on a single type of external knowledge source. In contrast,\nmost real-world applications involve diverse knowledge from various sources, a\nscenario that has been relatively underexplored. The main dilemma is the lack\nof a suitable dataset incorporating multiple knowledge sources and\npre-exploration of the associated issues. To address these challenges, we\nstandardize a benchmark dataset that combines structured and unstructured\nknowledge across diverse and complementary domains. Building upon the dataset,\nwe identify the limitations of existing methods under such conditions.\nTherefore, we develop PruningRAG, a plug-and-play RAG framework that uses\nmulti-granularity pruning strategies to more effectively incorporate relevant\ncontext and mitigate the negative impact of misleading information. Extensive\nexperimental results demonstrate superior performance of PruningRAG and our\ninsightful findings are also reported. Our dataset and code are publicly\navailable\\footnote{https://github.com/USTCAGI/PruningRAG}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 9 figures;",
    "pdf_url": "http://arxiv.org/pdf/2409.13694v3",
    "published_date": "2024-09-03 03:31:37 UTC",
    "updated_date": "2025-02-16 11:07:13 UTC"
  },
  {
    "arxiv_id": "2409.01579v1",
    "title": "AdaComp: Extractive Context Compression with Adaptive Predictor for Retrieval-Augmented Large Language Models",
    "authors": [
      "Qianchi Zhang",
      "Hainan Zhang",
      "Liang Pang",
      "Hongwei Zheng",
      "Zhiming Zheng"
    ],
    "abstract": "Retrieved documents containing noise will hinder RAG from detecting answer\nclues and make the inference process slow and expensive. Therefore, context\ncompression is necessary to enhance its accuracy and efficiency. Existing\ncontext compression methods use extractive or generative models to retain the\nmost query-relevant sentences or apply the information bottleneck theory to\npreserve sufficient information. However, these methods may face issues such as\nover-compression or high computational costs. We observe that the retriever\noften ranks relevant documents at the top, but the exact number of documents\nneeded to answer the query is uncertain due to the impact of query complexity\nand retrieval quality: complex queries like multi-hop questions may require\nretaining more documents than simpler queries, and a low-quality retrieval may\nneed to rely on more documents to generate accurate outputs. Therefore,\ndetermining the minimum number of required documents (compression rate) is\nstill a challenge for RAG. In this paper, we introduce AdaComp, a low-cost\nextractive context compression method that adaptively determines the\ncompression rate based on both query complexity and retrieval quality.\nSpecifically, we first annotate the minimum top-k documents necessary for the\nRAG system to answer the current query as the compression rate and then\nconstruct triplets of the query, retrieved documents, and its compression rate.\nThen, we use this triplet dataset to train a compression-rate predictor.\nExperiments on three QA datasets and one conversational Muiti-doc QA dataset\nshow that AdaComp significantly reduces inference costs while maintaining\nperformance nearly identical to uncompressed models, achieving a balance\nbetween efficiency and performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 5 figures, code available at\n  https://anonymous.4open.science/r/AdaComp-8C0C/",
    "pdf_url": "http://arxiv.org/pdf/2409.01579v1",
    "published_date": "2024-09-03 03:25:59 UTC",
    "updated_date": "2024-09-03 03:25:59 UTC"
  },
  {
    "arxiv_id": "2409.01573v2",
    "title": "Improving Apple Object Detection with Occlusion-Enhanced Distillation",
    "authors": [
      "Liang Geng"
    ],
    "abstract": "Apples growing in natural environments often face severe visual obstructions\nfrom leaves and branches. This significantly increases the risk of false\ndetections in object detection tasks, thereby escalating the challenge.\nAddressing this issue, we introduce a technique called \"Occlusion-Enhanced\nDistillation\" (OED). This approach utilizes occlusion information to regularize\nthe learning of semantically aligned features on occluded datasets and employs\nExponential Moving Average (EMA) to enhance training stability. Specifically,\nwe first design an occlusion-enhanced dataset that integrates Grounding DINO\nand SAM methods to extract occluding elements such as leaves and branches from\neach sample, creating occlusion examples that reflect the natural growth state\nof fruits. Additionally, we propose a multi-scale knowledge distillation\nstrategy, where the student network uses images with increased occlusions as\ninputs, while the teacher network employs images without natural occlusions.\nThrough this setup, the strategy guides the student network to learn from the\nteacher across scales of semantic and local features alignment, effectively\nnarrowing the feature distance between occluded and non-occluded targets and\nenhancing the robustness of object detection. Lastly, to improve the stability\nof the student network, we introduce the EMA strategy, which aids the student\nnetwork in learning more generalized feature expressions that are less affected\nby the noise of individual image occlusions. Our method significantly\noutperforms current state-of-the-art techniques through extensive comparative\nexperiments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01573v2",
    "published_date": "2024-09-03 03:11:48 UTC",
    "updated_date": "2024-10-30 02:36:18 UTC"
  },
  {
    "arxiv_id": "2409.01572v1",
    "title": "LSSF-Net: Lightweight Segmentation with Self-Awareness, Spatial Attention, and Focal Modulation",
    "authors": [
      "Hamza Farooq",
      "Zuhair Zafar",
      "Ahsan Saadat",
      "Tariq M Khan",
      "Shahzaib Iqbal",
      "Imran Razzak"
    ],
    "abstract": "Accurate segmentation of skin lesions within dermoscopic images plays a\ncrucial role in the timely identification of skin cancer for computer-aided\ndiagnosis on mobile platforms. However, varying shapes of the lesions, lack of\ndefined edges, and the presence of obstructions such as hair strands and marker\ncolors make this challenge more complex. \\textcolor{red}Additionally, skin\nlesions often exhibit subtle variations in texture and color that are difficult\nto differentiate from surrounding healthy skin, necessitating models that can\ncapture both fine-grained details and broader contextual information.\nCurrently, melanoma segmentation models are commonly based on fully connected\nnetworks and U-Nets. However, these models often struggle with capturing the\ncomplex and varied characteristics of skin lesions, such as the presence of\nindistinct boundaries and diverse lesion appearances, which can lead to\nsuboptimal segmentation performance.To address these challenges, we propose a\nnovel lightweight network specifically designed for skin lesion segmentation\nutilizing mobile devices, featuring a minimal number of learnable parameters\n(only 0.8 million). This network comprises an encoder-decoder architecture that\nincorporates conformer-based focal modulation attention, self-aware local and\nglobal spatial attention, and split channel-shuffle. The efficacy of our model\nhas been evaluated on four well-established benchmark datasets for skin lesion\nsegmentation: ISIC 2016, ISIC 2017, ISIC 2018, and PH2. Empirical findings\nsubstantiate its state-of-the-art performance, notably reflected in a high\nJaccard index.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01572v1",
    "published_date": "2024-09-03 03:06:32 UTC",
    "updated_date": "2024-09-03 03:06:32 UTC"
  },
  {
    "arxiv_id": "2409.01560v1",
    "title": "Blocks as Probes: Dissecting Categorization Ability of Large Multimodal Models",
    "authors": [
      "Bin Fu",
      "Qiyang Wan",
      "Jialin Li",
      "Ruiping Wang",
      "Xilin Chen"
    ],
    "abstract": "Categorization, a core cognitive ability in humans that organizes objects\nbased on common features, is essential to cognitive science as well as computer\nvision. To evaluate the categorization ability of visual AI models, various\nproxy tasks on recognition from datasets to open world scenarios have been\nproposed. Recent development of Large Multimodal Models (LMMs) has demonstrated\nimpressive results in high-level visual tasks, such as visual question\nanswering, video temporal reasoning, etc., utilizing the advanced architectures\nand large-scale multimodal instruction tuning. Previous researchers have\ndeveloped holistic benchmarks to measure the high-level visual capability of\nLMMs, but there is still a lack of pure and in-depth quantitative evaluation of\nthe most fundamental categorization ability. According to the research on human\ncognitive process, categorization can be seen as including two parts: category\nlearning and category use. Inspired by this, we propose a novel, challenging,\nand efficient benchmark based on composite blocks, called ComBo, which provides\na disentangled evaluation framework and covers the entire categorization\nprocess from learning to use. By analyzing the results of multiple evaluation\ntasks, we find that although LMMs exhibit acceptable generalization ability in\nlearning new categories, there are still gaps compared to humans in many ways,\nsuch as fine-grained perception of spatial relationship and abstract category\nunderstanding. Through the study of categorization, we can provide inspiration\nfor the further development of LMMs in terms of interpretability and\ngeneralization.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "39 pages, 28 figures, 4 tables. Accepted at The 35th British Machine\n  Vision Conference (BMVC 2024). Project page at\n  https://fubin29.github.io/Blocks-as-Probes/",
    "pdf_url": "http://arxiv.org/pdf/2409.01560v1",
    "published_date": "2024-09-03 02:55:36 UTC",
    "updated_date": "2024-09-03 02:55:36 UTC"
  },
  {
    "arxiv_id": "2409.01556v2",
    "title": "Benchmarking Cognitive Domains for LLMs: Insights from Taiwanese Hakka Culture",
    "authors": [
      "Chen-Chi Chang",
      "Ching-Yuan Chen",
      "Hung-Shin Lee",
      "Chih-Cheng Lee"
    ],
    "abstract": "This study introduces a comprehensive benchmark designed to evaluate the\nperformance of large language models (LLMs) in understanding and processing\ncultural knowledge, with a specific focus on Hakka culture as a case study.\nLeveraging Bloom's Taxonomy, the study develops a multi-dimensional framework\nthat systematically assesses LLMs across six cognitive domains: Remembering,\nUnderstanding, Applying, Analyzing, Evaluating, and Creating. This benchmark\nextends beyond traditional single-dimensional evaluations by providing a deeper\nanalysis of LLMs' abilities to handle culturally specific content, ranging from\nbasic recall of facts to higher-order cognitive tasks such as creative\nsynthesis. Additionally, the study integrates Retrieval-Augmented Generation\n(RAG) technology to address the challenges of minority cultural knowledge\nrepresentation in LLMs, demonstrating how RAG enhances the models' performance\nby dynamically incorporating relevant external information. The results\nhighlight the effectiveness of RAG in improving accuracy across all cognitive\ndomains, particularly in tasks requiring precise retrieval and application of\ncultural knowledge. However, the findings also reveal the limitations of RAG in\ncreative tasks, underscoring the need for further optimization. This benchmark\nprovides a robust tool for evaluating and comparing LLMs in culturally diverse\ncontexts, offering valuable insights for future research and development in\nAI-driven cultural knowledge preservation and dissemination.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to O-COCOSDA 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01556v2",
    "published_date": "2024-09-03 02:50:04 UTC",
    "updated_date": "2024-09-25 00:31:18 UTC"
  },
  {
    "arxiv_id": "2409.01555v1",
    "title": "EA-RAS: Towards Efficient and Accurate End-to-End Reconstruction of Anatomical Skeleton",
    "authors": [
      "Zhiheng Peng",
      "Kai Zhao",
      "Xiaoran Chen",
      "Li Ma",
      "Siyu Xia",
      "Changjie Fan",
      "Weijian Shang",
      "Wei Jing"
    ],
    "abstract": "Efficient, accurate and low-cost estimation of human skeletal information is\ncrucial for a range of applications such as biology education and\nhuman-computer interaction. However, current simple skeleton models, which are\ntypically based on 2D-3D joint points, fall short in terms of anatomical\nfidelity, restricting their utility in fields. On the other hand, more complex\nmodels while anatomically precise, are hindered by sophisticate multi-stage\nprocessing and the need for extra data like skin meshes, making them unsuitable\nfor real-time applications. To this end, we propose the EA-RAS (Towards\nEfficient and Accurate End-to-End Reconstruction of Anatomical Skeleton), a\nsingle-stage, lightweight, and plug-and-play anatomical skeleton estimator that\ncan provide real-time, accurate anatomically realistic skeletons with arbitrary\npose using only a single RGB image input. Additionally, EA-RAS estimates the\nconventional human-mesh model explicitly, which not only enhances the\nfunctionality but also leverages the outside skin information by integrating\nfeatures into the inside skeleton modeling process. In this work, we also\ndevelop a progressive training strategy and integrated it with an enhanced\noptimization process, enabling the network to obtain initial weights using only\na small skin dataset and achieve self-supervision in skeleton reconstruction.\nBesides, we also provide an optional lightweight post-processing optimization\nstrategy to further improve accuracy for scenarios that prioritize precision\nover real-time processing. The experiments demonstrated that our regression\nmethod is over 800 times faster than existing methods, meeting real-time\nrequirements. Additionally, the post-processing optimization strategy provided\ncan enhance reconstruction accuracy by over 50% and achieve a speed increase of\nmore than 7 times.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages,15 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01555v1",
    "published_date": "2024-09-03 02:46:28 UTC",
    "updated_date": "2024-09-03 02:46:28 UTC"
  },
  {
    "arxiv_id": "2409.01552v1",
    "title": "Self-Instructed Derived Prompt Generation Meets In-Context Learning: Unlocking New Potential of Black-Box LLMs",
    "authors": [
      "Zhuo Li",
      "Yuhao Du",
      "Jinpeng Hu",
      "Xiang Wan",
      "Anningzhe Gao"
    ],
    "abstract": "Large language models (LLMs) have shown success in generating high-quality\nresponses. In order to achieve better alignment with LLMs with human\npreference, various works are proposed based on specific optimization process,\nwhich, however, is not suitable to Black-Box LLMs like GPT-4, due to\ninaccessible parameters. In Black-Box LLMs case, their performance is highly\ndependent on the quality of the provided prompts. Existing methods to enhance\nresponse quality often involve a prompt refinement model, yet these approaches\npotentially suffer from semantic inconsistencies between the refined and\noriginal prompts, and typically overlook the relationship between them. To\naddress these challenges, we introduce a self-instructed in-context learning\nframework that empowers LLMs to deliver more effective responses by generating\nreliable derived prompts to construct informative contextual environments. Our\napproach incorporates a self-instructed reinforcement learning mechanism,\nenabling direct interaction with the response model during derived prompt\ngeneration for better alignment. We then formulate querying as an in-context\nlearning task, using responses from LLMs combined with the derived prompts to\nestablish a contextual demonstration for the original prompt. This strategy\nensures alignment with the original query, reduces discrepancies from refined\nprompts, and maximizes the LLMs' in-context learning capability. Extensive\nexperiments demonstrate that the proposed method not only generates more\nreliable derived prompts but also significantly enhances LLMs' ability to\ndeliver more effective responses, including Black-Box models such as GPT-4.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01552v1",
    "published_date": "2024-09-03 02:42:39 UTC",
    "updated_date": "2024-09-03 02:42:39 UTC"
  },
  {
    "arxiv_id": "2409.01548v3",
    "title": "VoxHakka: A Dialectally Diverse Multi-speaker Text-to-Speech System for Taiwanese Hakka",
    "authors": [
      "Li-Wei Chen",
      "Hung-Shin Lee",
      "Chen-Chi Chang"
    ],
    "abstract": "This paper introduces VoxHakka, a text-to-speech (TTS) system designed for\nTaiwanese Hakka, a critically under-resourced language spoken in Taiwan.\nLeveraging the YourTTS framework, VoxHakka achieves high naturalness and\naccuracy and low real-time factor in speech synthesis while supporting six\ndistinct Hakka dialects. This is achieved by training the model with\ndialect-specific data, allowing for the generation of speaker-aware Hakka\nspeech. To address the scarcity of publicly available Hakka speech corpora, we\nemployed a cost-effective approach utilizing a web scraping pipeline coupled\nwith automatic speech recognition (ASR)-based data cleaning techniques. This\nprocess ensured the acquisition of a high-quality, multi-speaker, multi-dialect\ndataset suitable for TTS training. Subjective listening tests conducted using\ncomparative mean opinion scores (CMOS) demonstrate that VoxHakka significantly\noutperforms existing publicly available Hakka TTS systems in terms of\npronunciation accuracy, tone correctness, and overall naturalness. This work\nrepresents a significant advancement in Hakka language technology and provides\na valuable resource for language preservation and revitalization efforts.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to O-COCOSDA 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01548v3",
    "published_date": "2024-09-03 02:37:34 UTC",
    "updated_date": "2024-10-02 02:25:30 UTC"
  },
  {
    "arxiv_id": "2409.01545v1",
    "title": "Effective Noise-aware Data Simulation for Domain-adaptive Speech Enhancement Leveraging Dynamic Stochastic Perturbation",
    "authors": [
      "Chien-Chun Wang",
      "Li-Wei Chen",
      "Hung-Shin Lee",
      "Berlin Chen",
      "Hsin-Min Wang"
    ],
    "abstract": "Cross-domain speech enhancement (SE) is often faced with severe challenges\ndue to the scarcity of noise and background information in an unseen target\ndomain, leading to a mismatch between training and test conditions. This study\nputs forward a novel data simulation method to address this issue, leveraging\nnoise-extractive techniques and generative adversarial networks (GANs) with\nonly limited target noisy speech data. Notably, our method employs a noise\nencoder to extract noise embeddings from target-domain data. These embeddings\naptly guide the generator to synthesize utterances acoustically fitted to the\ntarget domain while authentically preserving the phonetic content of the input\nclean speech. Furthermore, we introduce the notion of dynamic stochastic\nperturbation, which can inject controlled perturbations into the noise\nembeddings during inference, thereby enabling the model to generalize well to\nunseen noise conditions. Experiments on the VoiceBank-DEMAND benchmark dataset\ndemonstrate that our domain-adaptive SE method outperforms an existing strong\nbaseline based on data simulation.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to IEEE SLT 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01545v1",
    "published_date": "2024-09-03 02:29:01 UTC",
    "updated_date": "2024-09-03 02:29:01 UTC"
  },
  {
    "arxiv_id": "2409.01540v1",
    "title": "Long-Range Biometric Identification in Real World Scenarios: A Comprehensive Evaluation Framework Based on Missions",
    "authors": [
      "Deniz Aykac",
      "Joel Brogan",
      "Nell Barber",
      "Ryan Shivers",
      "Bob Zhang",
      "Dallas Sacca",
      "Ryan Tipton",
      "Gavin Jager",
      "Austin Garret",
      "Matthew Love",
      "Jim Goddard",
      "David Cornett III",
      "David S. Bolme"
    ],
    "abstract": "The considerable body of data available for evaluating biometric recognition\nsystems in Research and Development (R\\&D) environments has contributed to the\nincreasingly common problem of target performance mismatch. Biometric\nalgorithms are frequently tested against data that may not reflect the real\nworld applications they target. From a Testing and Evaluation (T\\&E)\nstandpoint, this domain mismatch causes difficulty assessing when improvements\nin State-of-the-Art (SOTA) research actually translate to improved applied\noutcomes. This problem can be addressed with thoughtful preparation of data and\nexperimental methods to reflect specific use-cases and scenarios.\n  To that end, this paper evaluates research solutions for identifying\nindividuals at ranges and altitudes, which could support various application\nareas such as counterterrorism, protection of critical infrastructure\nfacilities, military force protection, and border security. We address\nchallenges including image quality issues and reliance on face recognition as\nthe sole biometric modality. By fusing face and body features, we propose\ndeveloping robust biometric systems for effective long-range identification\nfrom both the ground and steep pitch angles. Preliminary results show promising\nprogress in whole-body recognition. This paper presents these early findings\nand discusses potential future directions for advancing long-range biometric\nidentification systems based on mission-driven metrics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01540v1",
    "published_date": "2024-09-03 02:17:36 UTC",
    "updated_date": "2024-09-03 02:17:36 UTC"
  },
  {
    "arxiv_id": "2409.01534v1",
    "title": "Think Twice Before Recognizing: Large Multimodal Models for General Fine-grained Traffic Sign Recognition",
    "authors": [
      "Yaozong Gan",
      "Guang Li",
      "Ren Togo",
      "Keisuke Maeda",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "abstract": "We propose a new strategy called think twice before recognizing to improve\nfine-grained traffic sign recognition (TSR). Fine-grained TSR in the wild is\ndifficult due to the complex road conditions, and existing approaches\nparticularly struggle with cross-country TSR when data is lacking. Our strategy\nachieves effective fine-grained TSR by stimulating the multiple-thinking\ncapability of large multimodal models (LMM). We introduce context,\ncharacteristic, and differential descriptions to design multiple thinking\nprocesses for the LMM. The context descriptions with center coordinate prompt\noptimization help the LMM to locate the target traffic sign in the original\nroad images containing multiple traffic signs and filter irrelevant answers\nthrough the proposed prior traffic sign hypothesis. The characteristic\ndescription is based on few-shot in-context learning of template traffic signs,\nwhich decreases the cross-domain difference and enhances the fine-grained\nrecognition capability of the LMM. The differential descriptions of similar\ntraffic signs optimize the multimodal thinking capability of the LMM. The\nproposed method is independent of training data and requires only simple and\nuniform instructions. We conducted extensive experiments on three benchmark\ndatasets and two real-world datasets from different countries, and the proposed\nmethod achieves state-of-the-art TSR results on all five datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01534v1",
    "published_date": "2024-09-03 02:08:47 UTC",
    "updated_date": "2024-09-03 02:08:47 UTC"
  },
  {
    "arxiv_id": "2409.01532v1",
    "title": "Improving Robustness of Spectrogram Classifiers with Neural Stochastic Differential Equations",
    "authors": [
      "Joel Brogan",
      "Olivera Kotevska",
      "Anibely Torres",
      "Sumit Jha",
      "Mark Adams"
    ],
    "abstract": "Signal analysis and classification is fraught with high levels of noise and\nperturbation. Computer-vision-based deep learning models applied to\nspectrograms have proven useful in the field of signal classification and\ndetection; however, these methods aren't designed to handle the low\nsignal-to-noise ratios inherent within non-vision signal processing tasks.\nWhile they are powerful, they are currently not the method of choice in the\ninherently noisy and dynamic critical infrastructure domain, such as smart-grid\nsensing, anomaly detection, and non-intrusive load monitoring.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01532v1",
    "published_date": "2024-09-03 02:03:50 UTC",
    "updated_date": "2024-09-03 02:03:50 UTC"
  },
  {
    "arxiv_id": "2409.01531v1",
    "title": "On the Design Space Between Transformers and Recursive Neural Nets",
    "authors": [
      "Jishnu Ray Chowdhury",
      "Cornelia Caragea"
    ],
    "abstract": "In this paper, we study two classes of models, Recursive Neural Networks\n(RvNNs) and Transformers, and show that a tight connection between them emerges\nfrom the recent development of two recent models - Continuous Recursive Neural\nNetworks (CRvNN) and Neural Data Routers (NDR). On one hand, CRvNN pushes the\nboundaries of traditional RvNN, relaxing its discrete structure-wise\ncomposition and ends up with a Transformer-like structure. On the other hand,\nNDR constrains the original Transformer to induce better structural inductive\nbias, ending up with a model that is close to CRvNN. Both models, CRvNN and\nNDR, show strong performance in algorithmic tasks and generalization in which\nsimpler forms of RvNNs and Transformers fail. We explore these \"bridge\" models\nin the design space between RvNNs and Transformers, formalize their tight\nconnections, discuss their limitations, and propose ideas for future research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01531v1",
    "published_date": "2024-09-03 02:03:35 UTC",
    "updated_date": "2024-09-03 02:03:35 UTC"
  },
  {
    "arxiv_id": "2409.01524v2",
    "title": "S^3cMath: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners",
    "authors": [
      "Yuchen Yan",
      "Jin Jiang",
      "Yang Liu",
      "Yixin Cao",
      "Xin Xu",
      "Mengdi Zhang",
      "Xunliang Cai",
      "Jian Shao"
    ],
    "abstract": "Self-correction is a novel method that can stimulate the potential reasoning\nabilities of large language models (LLMs). It involves detecting and correcting\nerrors during the inference process when LLMs solve reasoning problems.\nHowever, recent works do not regard self-correction as a spontaneous and\nintrinsic capability of LLMs. Instead, such correction is achieved through\npost-hoc generation, external knowledge introduction, multi-model\ncollaboration, and similar techniques. In this paper, we propose a series of\nmathematical LLMs called S$^3$c-Math, which are able to perform Spontaneous\nStep-level Self-correction for Mathematical reasoning. This capability helps\nLLMs to recognize whether their ongoing inference tends to contain errors and\nsimultaneously correct these errors to produce a more reliable response. We\nproposed a method, which employs a step-level sampling approach to construct\nstep-wise self-correction data for achieving such ability. Additionally, we\nimplement a training strategy that uses above constructed data to equip LLMs\nwith spontaneous step-level self-correction capacities. Our data and methods\nhave been demonstrated to be effective across various foundation LLMs,\nconsistently showing significant progress in evaluations on GSM8K, MATH, and\nother mathematical benchmarks. To the best of our knowledge, we are the first\nto introduce the spontaneous step-level self-correction ability of LLMs in\nmathematical reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01524v2",
    "published_date": "2024-09-03 01:40:21 UTC",
    "updated_date": "2025-02-20 02:20:58 UTC"
  },
  {
    "arxiv_id": "2409.01514v1",
    "title": "From Data to Insights: A Covariate Analysis of the IARPA BRIAR Dataset for Multimodal Biometric Recognition Algorithms at Altitude and Range",
    "authors": [
      "David S. Bolme",
      "Deniz Aykac",
      "Ryan Shivers",
      "Joel Brogan",
      "Nell Barber",
      "Bob Zhang",
      "Laura Davies",
      "David Cornett III"
    ],
    "abstract": "This paper examines covariate effects on fused whole body biometrics\nperformance in the IARPA BRIAR dataset, specifically focusing on UAV platforms,\nelevated positions, and distances up to 1000 meters. The dataset includes\noutdoor videos compared with indoor images and controlled gait recordings.\nNormalized raw fusion scores relate directly to predicted false accept rates\n(FAR), offering an intuitive means for interpreting model results. A linear\nmodel is developed to predict biometric algorithm scores, analyzing their\nperformance to identify the most influential covariates on accuracy at altitude\nand range. Weather factors like temperature, wind speed, solar loading, and\nturbulence are also investigated in this analysis. The study found that\nresolution and camera distance best predicted accuracy and findings can guide\nfuture research and development efforts in long-range/elevated/UAV biometrics\nand support the creation of more reliable and robust systems for national\nsecurity and other critical domains.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01514v1",
    "published_date": "2024-09-03 00:58:50 UTC",
    "updated_date": "2024-09-03 00:58:50 UTC"
  },
  {
    "arxiv_id": "2409.12973v1",
    "title": "The Era of Foundation Models in Medical Imaging is Approaching : A Scoping Review of the Clinical Value of Large-Scale Generative AI Applications in Radiology",
    "authors": [
      "Inwoo Seo",
      "Eunkyoung Bae",
      "Joo-Young Jeon",
      "Young-Sang Yoon",
      "Jiho Cha"
    ],
    "abstract": "Social problems stemming from the shortage of radiologists are intensifying,\nand artificial intelligence is being highlighted as a potential solution.\nRecently emerging large-scale generative AI has expanded from large language\nmodels (LLMs) to multi-modal models, showing potential to revolutionize the\nentire process of medical imaging. However, comprehensive reviews on their\ndevelopment status and future challenges are currently lacking. This scoping\nreview systematically organizes existing literature on the clinical value of\nlarge-scale generative AI applications by following PCC guidelines. A\nsystematic search was conducted across four databases: PubMed, EMbase,\nIEEE-Xplore, and Google Scholar, and 15 studies meeting the inclusion/exclusion\ncriteria set by the researchers were reviewed. Most of these studies focused on\nimproving the efficiency of report generation in specific parts of the\ninterpretation process or on translating reports to aid patient understanding,\nwith the latest studies extending to AI applications performing direct\ninterpretations. All studies were quantitatively evaluated by clinicians, with\nmost utilizing LLMs and only three employing multi-modal models. Both LLMs and\nmulti-modal models showed excellent results in specific areas, but none yet\noutperformed radiologists in diagnostic performance. Most studies utilized GPT,\nwith few using models specialized for the medical imaging domain. This study\nprovides insights into the current state and limitations of large-scale\ngenerative AI-based applications in the medical imaging field, offering\nfoundational data and suggesting that the era of medical imaging foundation\nmodels is on the horizon, which may fundamentally transform clinical practice\nin the near future.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages,3 figures, 4 tables, submitted to NPJ imaging",
    "pdf_url": "http://arxiv.org/pdf/2409.12973v1",
    "published_date": "2024-09-03 00:48:50 UTC",
    "updated_date": "2024-09-03 00:48:50 UTC"
  }
]
{
  "date": "2024-11-28",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-11-28 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文更新聚焦于人工智能生成模型、强化学习、计算机视觉和医疗应用等领域，亮点包括 Open-Sora Plan 的高效视频生成框架以及 AMO Sampler 的图像文本渲染优化，同时有 CVPR 2025 等会议相关论文凸显前沿进展。\n\n下面，我挑选并简要讨论了部分重要或有话题度的论文，先从 AI 生成和模型优化入手，再聊医疗与视觉应用，最后快速掠过其他领域的内容。限于篇幅，我会优先突出核心贡献，并对次要论文一笔带过。\n\n**AI 生成与模型优化方面：**  \n这部分论文展示了生成模型的创新，如文本渲染和视频生成，涉及高效采样和缓存机制。  \n- **AMO Sampler: Enhancing Text Rendering with Overshooting（AMO 采样器：通过过采样增强文本渲染）** - 作者包括 Qiang Liu 和 Hongliang Fei。该论文提出了一种训练-free 方法，使用过采样采样器（overshooting sampler）结合注意力机制（Attention Modulated Overshooting），显著提升了文本到图像生成模型（如 Stable Diffusion 3）的渲染准确性，改善了文本描绘错误，实现 32.3% 的准确率提升，而不增加推理成本。  \n- **Marconi: Prefix Caching for the Era of Hybrid LLMs（Marconi：针对混合大语言模型的 Prefix 缓存）** - 作者包括 Tri Dao 和 Ravi Netravali。论文设计了高效缓存策略，针对混合注意力-循环模型（如 State Space Models），通过预测重用可能性和计算节省，实现了高达 34.4 倍的 token 命中率，提高了大语言模型的推理效率。  \n- **Open-Sora Plan: Open-Source Large Video Generation Model（Open-Sora Plan：开源大型视频生成模型）** - 作者团队包括 Yonghong Tian 和 Li Yuan。该论文开源了一个视频生成框架，结合 Wavelet-Flow 变分自编码器和条件控制器，支持高分辨率长视频生成，实验显示在质量和效率上超越基线，潜力巨大。  \n- **Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads（Orthus：使用模态特定头的自回归图像-文本交错生成）** - 论文引入模态特定头（modality-specific heads），实现了图像和文本的自回归生成，显著提高了生成多样性和一致性，在 GenEval 和 MME 基准上达到 SOTA 水平。\n\n**医疗与视觉应用方面：**  \n这些论文强调多模态数据和实时追踪，特别在医疗图像分析和机器人领域有实际影响。  \n- **Libra: Leveraging Temporal Images for Biomedical Radiology Analysis（Libra：利用时序图像进行生物医学放射学分析）** - 作者包括 Edmond S. L. Ho。该论文开发了时序对齐连接器（Temporal Alignment Connector），针对胸部 X 光图像生成报告，提升了临床相关性和词汇准确性，在 MIMIC-CXR 数据集上设定了新基准。  \n- **HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos（HOT3D：从第一人称多视图视频中进行 3D 手和物体追踪）** - 作者包括 Richard Newcombe 和 Tomas Hodan。论文构建了大规模数据集，支持 3D 手和物体追踪，实验证明多视图方法在手势和物体识别上大幅优于单视图基线，适用于增强现实应用。  \n- **Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation（与 DINO 对话：桥接自监督视觉骨干与语言用于开放词汇分割）** - 论文将 DINOv2 与 CLIP 结合，通过注意力映射实现开放词汇图像分割，在无监督基准上达到 SOTA 性能，提升了语义定位精度。\n\n**强化学习与机器人规划方面：**  \n这部分包括高效规划和多代理系统，强调实际部署的鲁棒性。  \n- **Global Tensor Motion Planning（全局张量运动规划）** - 作者包括 Jan Peters。该论文提出张量操作的采样-based 算法，支持 GPU 加速运动规划，证明了在机器人学习任务中的概率完整性和效率，实验显示在复杂地图上优于基线。  \n- **Mars-PO: Multi-Agent Reasoning System Preference Optimization（Mars-PO：多代理推理系统偏好优化）** - 论文设计了多代理框架，通过混合正样本优化提升数学推理性能，在 MATH 基准上将 Llama3.1 的准确率从 50.38% 提高到 57.82%。  \n- **Integrating Transit Signal Priority into Multi-Agent Reinforcement Learning based Traffic Signal Control（将交通信号优先权集成到多代理强化学习中的交通信号控制）** - 该论文将多代理强化学习应用于交通信号优化，实现了公交优先策略，减少了公交延误 27%，并保持侧路交通影响最小。\n\n其他论文涉及领域广泛，如时间序列预测、异常检测和伦理分析，但多为次要贡献，我快速掠过：  \n- **Knowledge-Augmented Explainable and Interpretable Learning for Anomaly Detection and Diagnosis（知识增强的可解释异常检测和诊断学习）** - 作者 Martin Atzmueller 提出知识增强方法，提升了异常检测的可解释性，但细节较常规。  \n- **Concept-driven Off Policy Evaluation（概念驱动的离策略评估）** - 论文引入概念瓶颈模型减少评估方差，在合成和真实数据集上表现良好，但未见突破性创新。  \n- 其余如 EEG 数据集构建（ArEEG_Words）、偏好优化（PEFT-as-an-Attack）和伦理讨论（On the Ethical Considerations of Generative Agents）等，贡献稳健但不具话题度，仅提到其在特定领域的应用潜力。\n\n总之，今天的 arXiv 更新突出了 AI 生成和医疗应用的创新潜力，建议关注 Open-Sora Plan 和 AMO Sampler 等论文，以探索实际部署机会。更多细节可查阅 arXiv。明天的快报见！",
  "papers": [
    {
      "arxiv_id": "2411.19415v2",
      "title": "AMO Sampler: Enhancing Text Rendering with Overshooting",
      "title_zh": "AMO Sampler：通过",
      "authors": [
        "Xixi Hu",
        "Keyang Xu",
        "Bo Liu",
        "Qiang Liu",
        "Hongliang Fei"
      ],
      "abstract": "Achieving precise alignment between textual instructions and generated images\nin text-to-image generation is a significant challenge, particularly in\nrendering written text within images. Sate-of-the-art models like Stable\nDiffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text\ndepiction, resulting in misspelled or inconsistent text. We introduce a\ntraining-free method with minimal computational overhead that significantly\nenhances text rendering quality. Specifically, we introduce an overshooting\nsampler for pretrained rectified flow (RF) models, by alternating between\nover-simulating the learned ordinary differential equation (ODE) and\nreintroducing noise. Compared to the Euler sampler, the overshooting sampler\neffectively introduces an extra Langevin dynamics term that can help correct\nthe compounding error from successive Euler steps and therefore improve the\ntext rendering. However, when the overshooting strength is high, we observe\nover-smoothing artifacts on the generated images. To address this issue, we\npropose an Attention Modulated Overshooting sampler (AMO), which adaptively\ncontrols the strength of overshooting for each image patch according to their\nattention score with the text content. AMO demonstrates a 32.3% and 35.9%\nimprovement in text rendering accuracy on SD3 and Flux without compromising\noverall image quality or increasing inference cost. Code available at:\nhttps://github.com/hxixixh/amo-release.",
      "tldr_zh": "本论文针对文本到图像生成中文本渲染的精确对齐问题（如Stable Diffusion 3 (SD3)和Flux模型的拼写错误或不一致），提出了一种无需训练且计算开销小的overshooting sampler方法。该方法基于预训练的rectified flow (RF)模型，通过交替过度模拟ordinary differential equation (ODE)和重新引入噪声，引入额外的Langevin dynamics术语来修正累积误差，从而提升文本渲染质量。针对overshooting强度可能导致的over-smoothing artifacts，作者开发了Attention Modulated Overshooting sampler (AMO)，该采样器根据图像补丁与文本内容的attention score自适应调整overshooting强度。在SD3和Flux上，AMO分别实现了32.3%和35.9%的文本渲染准确率提升，同时保持整体图像质量并无额外推理成本。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.19415v2",
      "published_date": "2024-11-28 23:45:45 UTC",
      "updated_date": "2025-05-03 00:42:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:15:05.670566"
    },
    {
      "arxiv_id": "2412.00146v1",
      "title": "Knowledge-Augmented Explainable and Interpretable Learning for Anomaly Detection and Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Martin Atzmueller",
        "Tim Bohne",
        "Patricia Windler"
      ],
      "abstract": "Knowledge-augmented learning enables the combination of knowledge-based and\ndata-driven approaches. For anomaly detection and diagnosis, understandability\nis typically an important factor, especially in high-risk areas. Therefore,\nexplainability and interpretability are also major criteria in such contexts.\nThis chapter focuses on knowledge-augmented explainable and interpretable\nlearning to enhance understandability, transparency and ultimately\ncomputational sensemaking. We exemplify different approaches and methods in the\ndomains of anomaly detection and diagnosis - from comparatively simple\ninterpretable methods towards more advanced neuro-symbolic approaches.",
      "tldr_zh": "这篇论文探讨了知识增强学习（knowledge-augmented learning），它将知识驱动方法与数据驱动方法相结合，用于异常检测和诊断领域，以提升系统的可解释性和可解释性（explainable and interpretable learning）。重点在于提高理解性、透明度和计算意义，尤其适用于高风险环境。论文通过示例展示了从简单可解释方法到高级神经符号方法（neuro-symbolic approaches）的多种策略，为更可靠的异常处理提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.00146v1",
      "published_date": "2024-11-28 23:42:46 UTC",
      "updated_date": "2024-11-28 23:42:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:15:17.119760"
    },
    {
      "arxiv_id": "2411.19395v1",
      "title": "Concept-driven Off Policy Evaluation",
      "title_zh": "概念驱动的离策略评估",
      "authors": [
        "Ritam Majumdar",
        "Jack Teversham",
        "Sonali Parbhoo"
      ],
      "abstract": "Evaluating off-policy decisions using batch data poses significant challenges\ndue to limited sample sizes leading to high variance. To improve Off-Policy\nEvaluation (OPE), we must identify and address the sources of this variance.\nRecent research on Concept Bottleneck Models (CBMs) shows that using\nhuman-explainable concepts can improve predictions and provide better\nunderstanding. We propose incorporating concepts into OPE to reduce variance.\nOur work introduces a family of concept-based OPE estimators, proving that they\nremain unbiased and reduce variance when concepts are known and predefined.\nSince real-world applications often lack predefined concepts, we further\ndevelop an end-to-end algorithm to learn interpretable, concise, and diverse\nparameterized concepts optimized for variance reduction. Our experiments with\nsynthetic and real-world datasets show that both known and learned\nconcept-based estimators significantly improve OPE performance. Crucially, we\nshow that, unlike other OPE methods, concept-based estimators are easily\ninterpretable and allow for targeted interventions on specific concepts,\nfurther enhancing the quality of these estimators.",
      "tldr_zh": "本文针对 Off-Policy Evaluation (OPE) 中的高方差问题，提出使用人类可解释的概念（concepts）来改善估算器性能，减少样本大小导致的方差。研究引入一族 concept-based OPE estimators，并证明这些估算器在概念已知时保持无偏，同时开发了端到端算法来学习可解释、简洁、多样的参数化概念，以优化方差减少。实验在合成和真实数据集上显示，这些估算器显著提升了 OPE 性能。相比其他方法，concept-based estimators 更易解释，并允许针对特定概念的干预，进一步提高估算质量。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "37 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.19395v1",
      "published_date": "2024-11-28 22:15:06 UTC",
      "updated_date": "2024-11-28 22:15:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:15:30.068767"
    },
    {
      "arxiv_id": "2411.19393v2",
      "title": "Global Tensor Motion Planning",
      "title_zh": "翻译失败",
      "authors": [
        "An T. Le",
        "Kay Hansel",
        "João Carvalho",
        "Joe Watson",
        "Julen Urain",
        "Armin Biess",
        "Georgia Chalvatzaki",
        "Jan Peters"
      ],
      "abstract": "Batch planning is increasingly necessary to quickly produce diverse and\nhigh-quality motion plans for downstream learning applications, such as\ndistillation and imitation learning. This paper presents Global Tensor Motion\nPlanning (GTMP) -- a sampling-based motion planning algorithm comprising only\ntensor operations. We introduce a novel discretization structure represented as\na random multipartite graph, enabling efficient vectorized sampling, collision\nchecking, and search. We provide a theoretical investigation showing that GTMP\nexhibits probabilistic completeness while supporting modern GPU/TPU.\nAdditionally, by incorporating smooth structures into the multipartite graph,\nGTMP directly plans smooth splines without requiring gradient-based\noptimization. Experiments on lidar-scanned occupancy maps and the\nMotionBenchMarker dataset demonstrate GTMP's computation efficiency in batch\nplanning compared to baselines, underscoring GTMP's potential as a robust,\nscalable planner for diverse applications and large-scale robot learning tasks.",
      "tldr_zh": "本文提出 Global Tensor Motion Planning (GTMP)，一种基于采样的运动规划算法，仅使用张量操作，旨在提升批量规划的效率以支持下游学习应用如蒸馏和模仿学习。GTMP 引入随机多部分图作为离散化结构，实现高效的向量化采样、碰撞检查和搜索，同时通过融入平滑结构直接规划平滑样条，而无需基于梯度的优化。实验在 lidar-scanned 占用地图和 MotionBenchMarker 数据集上表明，GTMP 比基线方法计算效率更高，并证明其概率完整性（probabilistic completeness）和对 GPU/TPU 的支持，使其成为稳健、可扩展的机器人学习工具。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.19393v2",
      "published_date": "2024-11-28 22:07:46 UTC",
      "updated_date": "2024-12-31 14:05:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:15:41.786906"
    },
    {
      "arxiv_id": "2412.10384v1",
      "title": "Adult learners recall and recognition performance and affective feedback when learning from an AI-generated synthetic video",
      "title_zh": "成年学习者从 AI 生成的合成视频中学习时的回忆和识别性能以及",
      "authors": [
        "Zoe Ruo-Yu Li",
        "Caswell Barry",
        "Mutlu Cukurova"
      ],
      "abstract": "The widespread use of generative AI has led to multiple applications of\nAI-generated text and media to potentially enhance learning outcomes. However,\nthere are a limited number of well-designed experimental studies investigating\nthe impact of learning gains and affective feedback from AI-generated media\ncompared to traditional media (e.g., text from documents and human recordings\nof video). The current study recruited 500 participants to investigate adult\nlearners recall and recognition performances as well as their affective\nfeedback on the AI-generated synthetic video, using a mixed-methods approach\nwith a pre-and post-test design. Specifically, four learning conditions,\nAI-generated framing of human instructor-generated text, AI-generated synthetic\nvideos with human instructor-generated text, human instructor-generated videos,\nand human instructor-generated text frame (baseline), were considered. The\nresults indicated no statistically significant difference amongst conditions on\nrecall and recognition performance. In addition, the participants affective\nfeedback was not statistically significantly different between the two video\nconditions. However, adult learners preferred to learn from the video formats\nrather than text materials.",
      "tldr_zh": "本研究调查了成人学习者在使用 AI-generated synthetic video 时的 recall and recognition performance 以及 affective feedback，旨在比较 AI 生成媒体与传统媒体（如人类生成的文本或视频）对学习效果的影响。研究招募了 500 名参与者，采用混合方法和前后测试设计，比较了四种学习条件：AI-generated framing 与人类教师文本、AI-generated synthetic videos 与人类教师文本、人类教师生成的视频，以及人类教师生成的文本框架（作为基线）。结果显示，四种条件在回忆和识别表现上无显著统计差异，情感反馈在两种视频条件间也无显著差异；然而，参与者更倾向于视频格式而非文本材料，这为未来 AI 教育工具的设计提供了启示。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "13 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.10384v1",
      "published_date": "2024-11-28 21:40:28 UTC",
      "updated_date": "2024-11-28 21:40:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:15:54.108109"
    },
    {
      "arxiv_id": "2411.19385v1",
      "title": "Zero-Forget Preservation of Semantic Communication Alignment in Distributed AI Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Jingzhi Hu",
        "Geoffrey Ye Li"
      ],
      "abstract": "Future communication networks are expected to connect massive distributed\nartificial intelligence (AI). Exploiting aligned priori knowledge of AI pairs,\nit is promising to convert high-dimensional data transmission into\nhighly-compressed semantic communications (SC). However, to accommodate the\nlocal data distribution and user preferences, AIs generally adapt to different\ndomains, which fundamentally distorts the SC alignment. In this paper, we\npropose a zero-forget domain adaptation (ZFDA) framework to preserve SC\nalignment. To prevent the DA from changing substantial neural parameters of AI,\nwe design sparse additive modifications (SAM) to the parameters, which can be\nefficiently stored and switched-off to restore the SC alignment. To optimize\nthe SAM, we decouple it into tractable continuous variables and a binary mask,\nand then handle the binary mask by a score-based optimization. Experimental\nevaluations on a SC system for image transmissions validate that the proposed\nframework perfectly preserves the SC alignment with almost no loss of DA\nperformance, even improved in some cases, at a cost of less than 1% of\nadditional memory.",
      "tldr_zh": "该论文探讨了分布式 AI 网络中语义通信(SC)对齐的保留问题，因为 AI 适应本地数据分布和用户偏好会导致 SC 对齐扭曲。作者提出零遗忘域适应(ZFDA)框架，通过稀疏加性修改(SAM)对参数进行高效修改，确保在域适应(DA)过程中不改变实质神经参数，且 SAM 可以轻松存储和切换以恢复 SC 对齐。为优化 SAM，框架将其解耦为连续变量和二进制掩码，并采用基于分数的优化处理掩码。实验在图像传输的 SC 系统上显示，该框架完美保留 SC 对齐，同时几乎不损失 DA 性能（某些情况下甚至提升），仅需不到 1% 的额外内存。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19385v1",
      "published_date": "2024-11-28 21:28:18 UTC",
      "updated_date": "2024-11-28 21:28:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:16:06.033603"
    },
    {
      "arxiv_id": "2411.19379v3",
      "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Pan",
        "Zhuang Wang",
        "Zhen Jia",
        "Can Karakus",
        "Luca Zancato",
        "Tri Dao",
        "Yida Wang",
        "Ravi Netravali"
      ],
      "abstract": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
      "tldr_zh": "该论文提出 Marconi，一种针对 Hybrid LLMs 的 prefix caching 系统，旨在解决这些模型在处理长上下文时因 Recurrent 层 in-place state updates 导致的缓存复用效率低下问题。Marconi 引入创新的 admission 和 eviction 政策，通过预测缓存条目的复用可能性（涵盖不同命中场景）和计算节省与内存占用比，从而更高效地管理缓存。实验结果显示，在各种工作负载中，Marconi 比现有系统提升高达 34.4 倍的 token 命中率，并降低 71.1% 或 617 ms 的 TTFT，为 Hybrid LLMs 的实际部署提供了显著性能优化。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "MLSys 2025 camera-ready version",
      "pdf_url": "http://arxiv.org/pdf/2411.19379v3",
      "published_date": "2024-11-28 21:10:20 UTC",
      "updated_date": "2025-04-10 05:06:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:16:18.232504"
    },
    {
      "arxiv_id": "2411.19378v2",
      "title": "Libra: Leveraging Temporal Images for Biomedical Radiology Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Xi Zhang",
        "Zaiqiao Meng",
        "Jake Lever",
        "Edmond S. L. Ho"
      ],
      "abstract": "Radiology report generation (RRG) requires advanced medical image analysis,\neffective temporal reasoning, and accurate text generation. While multimodal\nlarge language models (MLLMs) align with pre-trained vision encoders to enhance\nvisual-language understanding, most existing methods rely on single-image\nanalysis or rule-based heuristics to process multiple images, failing to fully\nleverage temporal information in multi-modal medical datasets. In this paper,\nwe introduce Libra, a temporal-aware MLLM tailored for chest X-ray report\ngeneration. Libra combines a radiology-specific image encoder with a novel\nTemporal Alignment Connector (TAC), designed to accurately capture and\nintegrate temporal differences between paired current and prior images.\nExtensive experiments on the MIMIC-CXR dataset demonstrate that Libra\nestablishes a new state-of-the-art benchmark among similarly scaled MLLMs,\nsetting new standards in both clinical relevance and lexical accuracy.",
      "tldr_zh": "该论文针对放射学报告生成（RRG）的问题，指出现有多模态大语言模型（MLLMs）依赖单图像分析或规则-based启发式，无法充分利用多模态医疗数据集中的时序信息。研究引入Libra，一种时序感知MLLM框架，结合放射学特定的图像编码器和新型Temporal Alignment Connector (TAC)，以准确捕获并整合配对当前和先前图像的时序差异。实验结果显示，Libra在MIMIC-CXR数据集上超越同规模模型，建立了新的最先进基准，在临床相关性和词汇准确性方面显著提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "I.2.10; J.3; I.5.4"
      ],
      "primary_category": "cs.CV",
      "comment": "30 pages, 5 figures, Adding Appendix",
      "pdf_url": "http://arxiv.org/pdf/2411.19378v2",
      "published_date": "2024-11-28 21:07:22 UTC",
      "updated_date": "2025-02-16 17:29:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:16:28.477244"
    },
    {
      "arxiv_id": "2411.19360v1",
      "title": "DENIAHL: In-Context Features Influence LLM Needle-In-A-Haystack Abilities",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Dai",
        "Dan Pechi",
        "Xinyi Yang",
        "Garvit Banga",
        "Raghav Mantri"
      ],
      "abstract": "The Needle-in-a-haystack (NIAH) test is a general task used to assess\nlanguage models' (LMs') abilities to recall particular information from long\ninput context. This framework however does not provide a means of analyzing\nwhat factors, beyond context length, contribute to LMs' abilities or\ninabilities to separate and recall needles from their haystacks. To provide a\nsystematic means of assessing what features contribute to LMs' NIAH\ncapabilities, we developed a synthetic benchmark called DENIAHL (Data-oriented\nEvaluation of NIAH for LLM's). Our work expands on previous NIAH studies by\nablating NIAH features beyond typical context length including data type, size,\nand patterns. We find stark differences between GPT-3.5 and LLaMA 2-7B's\nperformance on DENIAHL, and drops in recall performance when features like item\nsize are increased, and to some degree when data type is changed from numbers\nto letters. This has implications for increasingly large context models,\ndemonstrating factors beyond item-number impact NIAH capabilities.",
      "tldr_zh": "本文开发了 DENIAHL 合成基准测试，用于系统评估影响语言模型 (LLM) 在 Needle-in-a-Haystack (NIAH) 任务中的因素，包括数据类型、大小和模式等超出上下文长度的特征。研究通过去除这些特征来扩展之前的 NIAH 研究，并比较了 GPT-3.5 和 LLaMA 2-7B 的性能，发现后者在某些条件下表现更差。结果显示，当项目大小增加或数据类型从数字改为字母时，模型的回忆性能会显著下降。这为设计更大上下文模型提供了启示，强调了这些因素对 NIAH 能力的深远影响。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19360v1",
      "published_date": "2024-11-28 20:14:47 UTC",
      "updated_date": "2024-11-28 20:14:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:16:42.091883"
    },
    {
      "arxiv_id": "2411.19359v1",
      "title": "Integrating Transit Signal Priority into Multi-Agent Reinforcement Learning based Traffic Signal Control",
      "title_zh": "翻译失败",
      "authors": [
        "Dickness Kakitahi Kwesiga",
        "Suyash Chandra Vishnoi",
        "Angshuman Guin",
        "Michael Hunter"
      ],
      "abstract": "This study integrates Transit Signal Priority (TSP) into multi-agent\nreinforcement learning (MARL) based traffic signal control. The first part of\nthe study develops adaptive signal control based on MARL for a pair of\ncoordinated intersections in a microscopic simulation environment. The two\nagents, one for each intersection, are centrally trained using a value\ndecomposition network (VDN) architecture. The trained agents show slightly\nbetter performance compared to coordinated actuated signal control based on\noverall intersection delay at v/c of 0.95. In the second part of the study the\ntrained signal control agents are used as background signal controllers while\ndeveloping event-based TSP agents. In one variation, independent TSP agents are\nformulated and trained under a decentralized training and decentralized\nexecution (DTDE) framework to implement TSP at each intersection. In the second\nvariation, the two TSP agents are centrally trained under a centralized\ntraining and decentralized execution (CTDE) framework and VDN architecture to\nselect and implement coordinated TSP strategies across the two intersections.\nIn both cases the agents converge to the same bus delay value, but independent\nagents show high instability throughout the training process. For the test\nruns, the two independent agents reduce bus delay across the two intersections\nby 22% compared to the no TSP case while the coordinated TSP agents achieve 27%\ndelay reduction. In both cases, there is only a slight increase in delay for a\nmajority of the side street movements.",
      "tldr_zh": "本研究将 Transit Signal Priority (TSP) 集成到 Multi-Agent Reinforcement Learning (MARL) 基于的交通信号控制中，以优化协调路口的信号管理。研究首先使用 Value Decomposition Network (VDN) 架构对两个路口智能体进行集中训练，相比传统致动信号控制，在高交通量下表现出略微更好的整体延误性能；随后，开发了基于事件的 TSP 智能体，包括独立训练的 Decentralized Training and Decentralized Execution (DTDE) 变体和集中训练的 Centralized Training and Decentralized Execution (CTDE) 变体。结果显示，协调 TSP 智能体将公交延误减少 27%，独立智能体减少 22%，且两种方案对侧街交通的影响均很小，为智能交通信号系统提供了更高效的策略。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19359v1",
      "published_date": "2024-11-28 20:09:12 UTC",
      "updated_date": "2024-11-28 20:09:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:16:53.631990"
    },
    {
      "arxiv_id": "2411.19356v1",
      "title": "Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp Brauner",
        "Felix Glawe",
        "Gian Luca Liehner",
        "Luisa Vervier",
        "Martina Ziefle"
      ],
      "abstract": "Understanding public perception of artificial intelligence (AI) and the\ntradeoffs between potential risks and benefits is crucial, as these perceptions\nmight shape policy decisions, influence innovation trajectories for successful\nmarket strategies, and determine individual and societal acceptance of AI\ntechnologies. Using a representative sample of 1100 participants from Germany,\nthis study examines mental models of AI. Participants quantitatively evaluated\n71 statements about AI's future capabilities (e.g., autonomous driving, medical\ncare, art, politics, warfare, and societal divides), assessing the expected\nlikelihood of occurrence, perceived risks, benefits, and overall value. We\npresent rankings of these projections alongside visual mappings illustrating\npublic risk-benefit tradeoffs. While many scenarios were deemed likely,\nparticipants often associated them with high risks, limited benefits, and low\noverall value. Across all scenarios, 96.4% ($r^2=96.4\\%$) of the variance in\nvalue assessment can be explained by perceived risks ($\\beta=-.504$) and\nperceived benefits ($\\beta=+.710$), with no significant relation to expected\nlikelihood. Demographics and personality traits influenced perceptions of\nrisks, benefits, and overall evaluations, underscoring the importance of\nincreasing AI literacy and tailoring public information to diverse user needs.\nThese findings provide actionable insights for researchers, developers, and\npolicymakers by highlighting critical public concerns and individual factors\nessential to align AI development with individual values.",
      "tldr_zh": "本研究调查了1100名德国代表性样本对人工智能(AI)的公众认知，焦点在于AI未来能力的预期（如自动驾驶和医疗护理）、风险-益处权衡，以及这些因素对社会接受度的影响。结果显示，参与者对许多场景认为发生可能性高，但往往伴随高风险、低益处和低整体价值，其中96.4%的价值评估方差可由感知风险(β=-0.504)和感知益处(β=0.710)解释，而非预期可能性。人口统计和个性特征显著影响这些感知，该研究为研究者、开发者和政策制定者提供了行动洞见，强调提高AI素养并针对多样用户需求定制信息以促进AI的社会接受。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19356v1",
      "published_date": "2024-11-28 20:03:01 UTC",
      "updated_date": "2024-11-28 20:03:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:17:06.111342"
    },
    {
      "arxiv_id": "2411.19352v2",
      "title": "OMuleT: Orchestrating Multiple Tools for Practicable Conversational Recommendation",
      "title_zh": "OMuleT：编排多个工具用于可行对话式推荐",
      "authors": [
        "Se-eun Yoon",
        "Xiaokai Wei",
        "Yexi Jiang",
        "Rachit Pareek",
        "Frank Ong",
        "Kevin Gao",
        "Julian McAuley",
        "Michelle Gong"
      ],
      "abstract": "In this paper, we present a systematic effort to design, evaluate, and\nimplement a realistic conversational recommender system (CRS). The objective of\nour system is to allow users to input free-form text to request\nrecommendations, and then receive a list of relevant and diverse items. While\nprevious work on synthetic queries augments large language models (LLMs) with\n1-3 tools, we argue that a more extensive toolbox is necessary to effectively\nhandle real user requests. As such, we propose a novel approach that equips\nLLMs with over 10 tools, providing them access to the internal knowledge base\nand API calls used in production. We evaluate our model on a dataset of real\nusers and show that it generates relevant, novel, and diverse recommendations\ncompared to vanilla LLMs. Furthermore, we conduct ablation studies to\ndemonstrate the effectiveness of using the full range of tools in our toolbox.\nWe share our designs and lessons learned from deploying the system for internal\nalpha release. Our contribution is the addressing of all four key aspects of a\npracticable CRS: (1) real user requests, (2) augmenting LLMs with a wide\nvariety of tools, (3) extensive evaluation, and (4) deployment insights.",
      "tldr_zh": "本研究提出OMuleT，一种为实用对话推荐系统(Conversational Recommender System, CRS)编排多个工具的框架，旨在处理用户自由文本请求并提供相关、多样化的推荐物品。不同于以往仅使用1-3个工具的方法，OMuleT为Large Language Models (LLMs)配备超过10个工具，包括内部知识库和API calls，以更好地应对真实用户需求。实验在真实用户数据集上显示，OMuleT生成的推荐在相关性、新颖性和多样性方面优于普通LLMs，并通过消融研究验证了完整工具集的有效性。该框架的贡献在于全面解决CRS的关键方面，包括真实请求处理、工具增强、广泛评估和部署经验。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19352v2",
      "published_date": "2024-11-28 19:53:39 UTC",
      "updated_date": "2025-01-01 00:03:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:17:17.498966"
    },
    {
      "arxiv_id": "2412.05313v6",
      "title": "λ: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile Manipulation Robotics",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Jaafar",
        "Shreyas Sundara Raman",
        "Yichen Wei",
        "Sudarshan Harithas",
        "Sofia Juliani",
        "Anneke Wernerfelt",
        "Benedict Quartey",
        "Ifrah Idrees",
        "Jason Xinyu Liu",
        "Stefanie Tellex"
      ],
      "abstract": "Learning to execute long-horizon mobile manipulation tasks is crucial for\nadvancing robotics in household and workplace settings. However, current\napproaches are typically data-inefficient, underscoring the need for improved\nmodels that require realistically sized benchmarks to evaluate their\nefficiency. To address this, we introduce the LAMBDA ({\\lambda})\nbenchmark-Long-horizon Actions for Mobile-manipulation Benchmarking of Directed\nActivities-which evaluates the data efficiency of models on\nlanguage-conditioned, long-horizon, multi-room, multi-floor, pick-and-place\ntasks using a dataset of manageable size, more feasible for collection. Our\nbenchmark includes 571 human-collected demonstrations that provide realism and\ndiversity in simulated and real-world settings. Unlike planner-generated data,\nthese trajectories offer natural variability and replay-verifiability, ensuring\nrobust learning and evaluation. We leverage LAMBDA to benchmark current\nend-to-end learning methods and a modular neuro-symbolic approaches that\ncombines foundation models with task and motion planning. We find that\nend-to-end methods-even when pretrained-yield lower success rates, while\nneuro-symbolic methods perform significantly better and require less data.",
      "tldr_zh": "本文引入了 LAMBDA 基准，用于评估模型在长时域室内移动操作任务（如语言条件下的多房间、多楼层捡取和放置）的数据效率问题。该基准基于 571 个人类收集的演示数据，提供真实多样性和可重放性，以模拟真实世界场景。实验结果显示，端-to-end learning 方法即使预训练也成功率较低，而 neuro-symbolic approaches 表现更佳，且所需数据更少。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.05313v6",
      "published_date": "2024-11-28 19:31:50 UTC",
      "updated_date": "2025-03-04 17:33:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:17:28.733907"
    },
    {
      "arxiv_id": "2411.19341v1",
      "title": "An Adversarial Learning Approach to Irregular Time-Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Heejeong Nam",
        "Jihyun Kim",
        "Jimin Yeom"
      ],
      "abstract": "Forecasting irregular time series presents significant challenges due to two\nkey issues: the vulnerability of models to mean regression, driven by the noisy\nand complex nature of the data, and the limitations of traditional error-based\nevaluation metrics, which fail to capture meaningful patterns and penalize\nunrealistic forecasts. These problems result in forecasts that often misalign\nwith human intuition. To tackle these challenges, we propose an adversarial\nlearning framework with a deep analysis of adversarial components.\nSpecifically, we emphasize the importance of balancing the modeling of global\ndistribution (overall patterns) and transition dynamics (localized temporal\nchanges) to better capture the nuances of irregular time series. Overall, this\nresearch provides practical insights for improving models and evaluation\nmetrics, and pioneers the application of adversarial learning in the domian of\nirregular time-series forecasting.",
      "tldr_zh": "该研究针对不规则时间序列预测的挑战提出了一种对抗学习（adversarial learning）框架，主要解决模型易受均值回归（mean regression）影响以及传统错误评估指标无法捕捉有意义模式的问题，导致预测偏离人类直觉。该框架通过深入分析对抗组件，强调平衡全局分布（overall patterns）和过渡动态（localized temporal changes），从而更精确地捕捉数据细微差别。实验结果提供了改进模型和评估指标的实用见解，并开创了在irregular time-series forecasting领域应用对抗学习的新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to AdvML-Frontiers Workshop @ NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.19341v1",
      "published_date": "2024-11-28 19:28:07 UTC",
      "updated_date": "2024-11-28 19:28:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:17:40.256159"
    },
    {
      "arxiv_id": "2411.19339v2",
      "title": "Towards a Mechanistic Explanation of Diffusion Model Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Matthew Niedoba",
        "Berend Zwartsenberg",
        "Kevin Murphy",
        "Frank Wood"
      ],
      "abstract": "We propose a simple, training-free mechanism which explains the\ngeneralization behaviour of diffusion models. By comparing pre-trained\ndiffusion models to their theoretically optimal empirical counterparts, we\nidentify a shared local inductive bias across a variety of network\narchitectures. From this observation, we hypothesize that network denoisers\ngeneralize through localized denoising operations, as these operations\napproximate the training objective well over much of the training distribution.\nTo validate our hypothesis, we introduce novel denoising algorithms which\naggregate local empirical denoisers to replicate network behaviour. Comparing\nthese algorithms to network denoisers across forward and reverse diffusion\nprocesses, our approach exhibits consistent visual similarity to neural network\noutputs, with lower mean squared error than previously proposed methods.",
      "tldr_zh": "本研究提出一个简单的、无需训练的机制来解释diffusion models的泛化行为，通过比较预训练模型与其理论最优经验对应物，识别出各种网络架构共享的局部归纳偏差(inductive bias)。作者假设网络去噪器(denoisers)通过本地化去噪操作实现泛化，因为这些操作在训练分布的大部分区域能良好近似训练目标。为验证这一假设，论文引入了新的去噪算法，这些算法聚合局部经验去噪器以复制网络行为。实验结果显示，该方法在正向和反向diffusion过程中，与网络去噪器相比，表现出更高的视觉相似性和更低的均方误差(mean squared error)。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 23 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.19339v2",
      "published_date": "2024-11-28 19:22:17 UTC",
      "updated_date": "2025-02-14 19:20:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:17:53.708808"
    },
    {
      "arxiv_id": "2411.19335v2",
      "title": "PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Shenghui Li",
        "Edith C. -H. Ngai",
        "Fanghua Ye",
        "Thiemo Voigt"
      ],
      "abstract": "Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a\npromising paradigm for privacy-preserving and efficient adaptation of\nPre-trained Language Models (PLMs) in Federated Learning (FL) settings. It\npreserves data privacy by keeping the data decentralized and training the model\non local devices, ensuring that raw data never leaves the user's device.\nMoreover, the integration of PEFT methods such as LoRA significantly reduces\nthe number of trainable parameters compared to fine-tuning the entire model,\nthereby minimizing communication costs and computational overhead. Despite its\npotential, the security implications of FedPEFT remain underexplored. This\npaper introduces a novel security threat to FedPEFT, termed PEFT-as-an-Attack\n(PaaA), which exposes how PEFT can be exploited as an attack vector to\ncircumvent PLMs' safety alignment and generate harmful content in response to\nmalicious prompts. Our evaluation of PaaA reveals that with less than 1% of the\nmodel's parameters set as trainable, and a small subset of clients acting\nmaliciously, the attack achieves an approximate 80% attack success rate using\nrepresentative PEFT methods such as LoRA. To mitigate this threat, we further\ninvestigate potential defense strategies, including Robust Aggregation Schemes\n(RASs) and Post-PEFT Safety Alignment (PPSA). However, our empirical analysis\nhighlights the limitations of these defenses, i.e., even the most advanced\nRASs, such as DnC and ClippedClustering, struggle to defend against PaaA in\nscenarios with highly heterogeneous data distributions. Similarly, while PPSA\ncan reduce attack success rates to below 10%, it severely degrades the model's\naccuracy on the target task. Our results underscore the urgent need for more\neffective defense mechanisms that simultaneously ensure security and maintain\nthe performance of the FedPEFT paradigm.",
      "tldr_zh": "本研究揭示了 Federated Parameter-Efficient Fine-Tuning (FedPEFT) 框架的安全风险，提出 PEFT-as-an-Attack (PaaA) 攻击方法，该攻击利用少于 1% 的模型参数（如 LoRA）并通过恶意客户端绕过 Pre-trained Language Models (PLMs) 的安全对齐，成功生成有害内容，攻击成功率高达约 80%。实验评估显示，这种攻击在联邦学习环境中特别有效，尤其在数据分布高度异质的场景下。论文探讨了潜在防御策略，包括 Robust Aggregation Schemes (RASs) 和 Post-PEFT Safety Alignment (PPSA)，但发现 RASs 如 DnC 和 ClippedClustering 难以应对异质数据，而 PPSA 虽可将攻击成功率降至 10% 以下，却会显著降低模型在目标任务上的准确性。总体而言，该工作强调了 FedPEFT 范式中亟需更有效的安全机制，以平衡性能和防护需求。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19335v2",
      "published_date": "2024-11-28 19:05:01 UTC",
      "updated_date": "2024-12-19 14:30:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:18:06.136863"
    },
    {
      "arxiv_id": "2411.19331v1",
      "title": "Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Barsellotti",
        "Lorenzo Bianchi",
        "Nicola Messina",
        "Fabio Carrara",
        "Marcella Cornia",
        "Lorenzo Baraldi",
        "Fabrizio Falchi",
        "Rita Cucchiara"
      ],
      "abstract": "Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form\ntextual concepts without predefined training classes. While existing\nvision-language models such as CLIP can generate segmentation masks by\nleveraging coarse spatial information from Vision Transformers, they face\nchallenges in spatial localization due to their global alignment of image and\ntext features. Conversely, self-supervised visual models like DINO excel in\nfine-grained visual encoding but lack integration with language. To bridge this\ngap, we present Talk2DINO, a novel hybrid approach that combines the spatial\naccuracy of DINOv2 with the language understanding of CLIP. Our approach aligns\nthe textual embeddings of CLIP to the patch-level features of DINOv2 through a\nlearned mapping function without the need to fine-tune the underlying\nbackbones. At training time, we exploit the attention maps of DINOv2 to\nselectively align local visual patches with textual embeddings. We show that\nthe powerful semantic and localization abilities of Talk2DINO can enhance the\nsegmentation process, resulting in more natural and less noisy segmentations,\nand that our approach can also effectively distinguish foreground objects from\nthe background. Experimental results demonstrate that Talk2DINO achieves\nstate-of-the-art performance across several unsupervised OVS benchmarks. Source\ncode and models are publicly available at:\nhttps://lorebianchi98.github.io/Talk2DINO/.",
      "tldr_zh": "该研究提出 Talk2DINO，一种创新方法，将自监督视觉模型 DINOv2 的空间准确性与语言模型 CLIP 的文本理解能力相结合，用于 Open-Vocabulary Segmentation (OVS)，以解决现有模型在空间定位方面的挑战。通过一个学习映射函数，对齐 CLIP 的文本嵌入与 DINOv2 的 patch-level 特征，并在训练时利用 DINOv2 的注意力图进行选择性对齐，该框架无需微调基础模型即可提升分割质量。实验结果显示，Talk2DINO 在多个无监督 OVS 基准上实现最先进性能，提供更自然、少噪声的图像分割，并有效区分前景和背景对象。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19331v1",
      "published_date": "2024-11-28 19:00:03 UTC",
      "updated_date": "2024-11-28 19:00:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:18:19.439057"
    },
    {
      "arxiv_id": "2412.00142v2",
      "title": "Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers",
      "title_zh": "翻译失败",
      "authors": [
        "Chancharik Mitra",
        "Brandon Huang",
        "Tianning Chai",
        "Zhiqiu Lin",
        "Assaf Arbelle",
        "Rogerio Feris",
        "Leonid Karlinsky",
        "Trevor Darrell",
        "Deva Ramanan",
        "Roei Herzig"
      ],
      "abstract": "Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a\nwide variety of vision-language (VL) tasks such as image captioning or visual\nquestion answering. Despite strong performance, LMMs are not directly suited\nfor foundational discriminative vision-language tasks (i.e., tasks requiring\ndiscrete label predictions) such as image classification and multiple-choice\nVQA. One key challenge in utilizing LMMs for discriminative tasks is the\nextraction of useful features from generative models. To overcome this issue,\nwe propose an approach for finding features in the model's latent space to more\neffectively leverage LMMs for discriminative tasks. Toward this end, we present\nSparse Attention Vectors (SAVs) -- a finetuning-free method that leverages\nsparse attention head activations (fewer than 1\\% of the heads) in LMMs as\nstrong features for VL tasks. With only few-shot examples, SAVs demonstrate\nstate-of-the-art performance compared to a variety of few-shot and finetuned\nbaselines on a collection of discriminative tasks. Our experiments also imply\nthat SAVs can scale in performance with additional examples and generalize to\nsimilar tasks, establishing SAVs as both effective and robust multimodal\nfeature representations.",
      "tldr_zh": "本研究发现，生成式大型多模态模型（LMMs）如LLaVA和Qwen-VL在视觉语言（VL）任务上表现出色，但不适合基础鉴别性任务（如图像分类和多选VQA），主要由于特征提取的挑战。作者提出Sparse Attention Vectors (SAVs)，一种无需微调的方法，利用LMMs中少于1%的稀疏注意力头激活作为强有力的多模态特征表示。在少样本场景下，SAVs在多种鉴别性任务上达到最先进性能，并能随样本增加而提升，且具备良好的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00142v2",
      "published_date": "2024-11-28 18:55:41 UTC",
      "updated_date": "2025-01-13 23:45:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:18:29.821514"
    },
    {
      "arxiv_id": "2411.19301v1",
      "title": "Structured Object Language Modeling (SoLM): Native Structured Objects Generation Conforming to Complex Schemas with Self-Supervised Denoising",
      "title_zh": "翻译失败",
      "authors": [
        "Amir Tavanaei",
        "Kee Kiat Koo",
        "Hayreddin Ceker",
        "Shaobai Jiang",
        "Qi Li",
        "Julien Han",
        "Karim Bouyarmane"
      ],
      "abstract": "In this paper, we study the problem of generating structured objects that\nconform to a complex schema, with intricate dependencies between the different\ncomponents (facets) of the object. The facets of the object (attributes,\nfields, columns, properties) can be a mix of short, structured,\ntype-constrained facts, or long natural-language descriptions. The object has\nto be self-consistent between the different facets in the redundant information\nit carries (relative consistency), while being grounded with respect to world\nknowledge (absolute consistency). We frame the problem as a Language Modeling\nproblem (Structured Object Language Modeling) and train an LLM to perform the\ntask natively, without requiring instructions or prompt-engineering. We propose\na self-supervised denoising method to train the model from an existing dataset\nof such objects. The input query can be the existing object itself, in which\ncase the model acts as a regenerator, completing, correcting, normalizing the\ninput, or any unstructured blurb to be structured. We show that the\nself-supervised denoising training provides a strong baseline, and that\nadditional supervised fine-tuning with small amount of human demonstrations\nleads to further improvement. Experimental results show that the proposed\nmethod matches or outperforms prompt-engineered general-purpose\nstate-of-the-art LLMs (Claude 3, Mixtral-8x7B), while being order-of-magnitude\nmore cost-efficient.",
      "tldr_zh": "本论文提出 Structured Object Language Modeling (SoLM)，一种直接生成符合复杂模式（包括短结构化事实和长自然语言描述）的结构化对象方法，确保对象内部一致性和世界知识一致性。\nSoLM 将问题框架化为语言建模任务，使用自监督去噪训练从现有数据集训练 LLM，无需提示工程，支持输入对象的再生、修正和结构化。\n实验结果表明，该方法与通用 LLM 如 Claude 3 和 Mixtral-8x7B 相当或优于它们，同时成本效率高出几个数量级，进一步的少量监督微调可提升性能。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19301v1",
      "published_date": "2024-11-28 18:16:41 UTC",
      "updated_date": "2024-11-28 18:16:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:18:41.653755"
    },
    {
      "arxiv_id": "2411.19285v2",
      "title": "BPQP: A Differentiable Convex Optimization Framework for Efficient End-to-End Learning",
      "title_zh": "BPQP：一个用于高效端到端学习的可微凸优化框架",
      "authors": [
        "Jianming Pan",
        "Zeqi Ye",
        "Xiao Yang",
        "Xu Yang",
        "Weiqing Liu",
        "Lewen Wang",
        "Jiang Bian"
      ],
      "abstract": "Data-driven decision-making processes increasingly utilize end-to-end\nlearnable deep neural networks to render final decisions. Sometimes, the output\nof the forward functions in certain layers is determined by the solutions to\nmathematical optimization problems, leading to the emergence of differentiable\noptimization layers that permit gradient back-propagation. However, real-world\nscenarios often involve large-scale datasets and numerous constraints,\npresenting significant challenges. Current methods for differentiating\noptimization problems typically rely on implicit differentiation, which\nnecessitates costly computations on the Jacobian matrices, resulting in low\nefficiency. In this paper, we introduce BPQP, a differentiable convex\noptimization framework designed for efficient end-to-end learning. To enhance\nefficiency, we reformulate the backward pass as a simplified and decoupled\nquadratic programming problem by leveraging the structural properties of the\nKKT matrix. This reformulation enables the use of first-order optimization\nalgorithms in calculating the backward pass gradients, allowing our framework\nto potentially utilize any state-of-the-art solver. As solver technologies\nevolve, BPQP can continuously adapt and improve its efficiency. Extensive\nexperiments on both simulated and real-world datasets demonstrate that BPQP\nachieves a significant improvement in efficiency--typically an order of\nmagnitude faster in overall execution time compared to other differentiable\noptimization layers. Our results not only highlight the efficiency gains of\nBPQP but also underscore its superiority over differentiable optimization layer\nbaselines.",
      "tldr_zh": "本论文提出 BPQP，这是一个高效的 Differentiable Convex Optimization 框架，用于支持端到端学习中的优化问题微分。BPQP 通过将反向传播重新表述为简化且解耦的 Quadratic Programming 问题，利用 KKT 矩阵的结构特性，并允许使用一阶优化算法与任何先进求解器兼容，从而显著提升计算效率。实验在模拟和真实数据集上表明，BPQP 的整体执行时间比现有可微优化层快一个数量级，展示了其在实际应用中的优越性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.PM"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024 Spotlight",
      "pdf_url": "http://arxiv.org/pdf/2411.19285v2",
      "published_date": "2024-11-28 17:31:15 UTC",
      "updated_date": "2024-12-30 03:25:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:18:53.328811"
    },
    {
      "arxiv_id": "2411.19274v1",
      "title": "On-chip Hyperspectral Image Segmentation with Fully Convolutional Networks for Scene Understanding in Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Jon Gutiérrez-Zaballa",
        "Koldo Basterretxea",
        "Javier Echanobe",
        "M. Victoria Martínez",
        "Unai Martínez-Corral",
        "Óscar Mata Carballeira",
        "Inés del Campo"
      ],
      "abstract": "Most of current computer vision-based advanced driver assistance systems\n(ADAS) perform detection and tracking of objects quite successfully under\nregular conditions. However, under adverse weather and changing lighting\nconditions, and in complex situations with many overlapping objects, these\nsystems are not completely reliable. The spectral reflectance of the different\nobjects in a driving scene beyond the visible spectrum can offer additional\ninformation to increase the reliability of these systems, especially under\nchallenging driving conditions. Furthermore, this information may be\nsignificant enough to develop vision systems that allow for a better\nunderstanding and interpretation of the whole driving scene. In this work we\nexplore the use of snapshot, video-rate hyperspectral imaging (HSI) cameras in\nADAS on the assumption that the near infrared (NIR) spectral reflectance of\ndifferent materials can help to better segment the objects in real driving\nscenarios. To do this, we have used the HSI-Drive 1.1 dataset to perform\nvarious experiments on spectral classification algorithms. However, the\ninformation retrieval of hyperspectral recordings in natural outdoor scenarios\nis challenging, mainly because of deficient colour constancy and other inherent\nshortcomings of current snapshot HSI technology, which poses some limitations\nto the development of pure spectral classifiers. In consequence, in this work\nwe analyze to what extent the spatial features codified by standard, tiny fully\nconvolutional network (FCN) models can improve the performance of HSI\nsegmentation systems for ADAS applications.\n  The abstract above is truncated due to submission limits. For the full\nabstract, please refer to the published article.",
      "tldr_zh": "本研究探讨了在自动驾驶系统中，使用芯片级高光谱图像分割（Hyperspectral Image Segmentation）结合全卷积网络（Fully Convolutional Networks, FCN）来提升场景理解能力，尤其在恶劣天气、变化光照和复杂场景下。论文指出，高光谱成像（HSI）的近红外（NIR）光谱反射能提供额外信息，帮助更好地分割物体，并利用HSI-Drive 1.1数据集进行光谱分类实验。结果显示，标准小型FCN模型通过整合空间特征，能显著改善HSI分割系统的性能，解决纯光谱分类的局限性，从而增强高级驾驶辅助系统（ADAS）的可靠性和整体场景解读。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19274v1",
      "published_date": "2024-11-28 17:10:50 UTC",
      "updated_date": "2024-11-28 17:10:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:19:05.459004"
    },
    {
      "arxiv_id": "2412.00138v1",
      "title": "Unleashing the Power of Data Synthesis in Visual Localization",
      "title_zh": "释放数据合成在视觉定位中的力量",
      "authors": [
        "Sihang Li",
        "Siqi Tan",
        "Bowen Chang",
        "Jing Zhang",
        "Chen Feng",
        "Yiming Li"
      ],
      "abstract": "Visual localization, which estimates a camera's pose within a known scene, is\na long-standing challenge in vision and robotics. Recent end-to-end methods\nthat directly regress camera poses from query images have gained attention for\nfast inference. However, existing methods often struggle to generalize to\nunseen views. In this work, we aim to unleash the power of data synthesis to\npromote the generalizability of pose regression. Specifically, we lift real 2D\nimages into 3D Gaussian Splats with varying appearance and deblurring\nabilities, which are then used as a data engine to synthesize more posed\nimages. To fully leverage the synthetic data, we build a two-branch joint\ntraining pipeline, with an adversarial discriminator to bridge the syn-to-real\ngap. Experiments on established benchmarks show that our method outperforms\nstate-of-the-art end-to-end approaches, reducing translation and rotation\nerrors by 50% and 21.6% on indoor datasets, and 35.56% and 38.7% on outdoor\ndatasets. We also validate the effectiveness of our method in dynamic driving\nscenarios under varying weather conditions. Notably, as data synthesis scales\nup, our method exhibits a growing ability to interpolate and extrapolate\ntraining data for localizing unseen views. Project Page:\nhttps://ai4ce.github.io/RAP/",
      "tldr_zh": "本文探讨了视觉定位（Visual Localization）中端到端方法的泛化性问题，提出通过数据合成提升模型性能。具体方法包括将真实2D图像提升为3D Gaussian Splats以生成更多带位姿的合成图像，并采用双分支联合训练管道和对抗性判别器来弥合合成数据与真实数据的差距。实验结果显示，该方法在室内数据集上减少了50%的平移误差和21.6%的旋转误差，在室外数据集上减少了35.56%的平移误差和38.7%的旋转误差，并在动态驾驶场景下表现出色，随着数据合成规模扩大，模型对未见视图的插值和外推能力显著增强。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "24 pages, 21 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.00138v1",
      "published_date": "2024-11-28 16:58:10 UTC",
      "updated_date": "2024-11-28 16:58:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:19:18.613901"
    },
    {
      "arxiv_id": "2411.19245v1",
      "title": "Contrastive representations of high-dimensional, structured treatments",
      "title_zh": "高维结构化处理的对比表示",
      "authors": [
        "Oriol Corcoll Andreu",
        "Athanasios Vlontzos",
        "Michael O'Riordan",
        "Ciaran M. Gilligan-Lee"
      ],
      "abstract": "Estimating causal effects is vital for decision making. In standard causal\neffect estimation, treatments are usually binary- or continuous-valued.\nHowever, in many important real-world settings, treatments can be structured,\nhigh-dimensional objects, such as text, video, or audio. This provides a\nchallenge to traditional causal effect estimation. While leveraging the shared\nstructure across different treatments can help generalize to unseen treatments\nat test time, we show in this paper that using such structure blindly can lead\nto biased causal effect estimation. We address this challenge by devising a\nnovel contrastive approach to learn a representation of the high-dimensional\ntreatments, and prove that it identifies underlying causal factors and discards\nnon-causally relevant factors. We prove that this treatment representation\nleads to unbiased estimates of the causal effect, and empirically validate and\nbenchmark our results on synthetic and real-world datasets.",
      "tldr_zh": "该论文探讨了高维结构化处理（如文本、视频或音频）的因果效应估计问题，指出传统方法可能因盲目利用处理结构而导致偏置。作者提出了一种新颖的对比学习（contrastive）方法，用于学习这些处理的表现形式，并证明该方法能识别底层因果因素并过滤非因果相关因素，从而实现无偏见的因果效应估计。在合成和真实数据集上的实验验证了该方法的有效性，并为实际决策提供可靠基准。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19245v1",
      "published_date": "2024-11-28 16:33:31 UTC",
      "updated_date": "2024-11-28 16:33:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:19:29.078871"
    },
    {
      "arxiv_id": "2412.00136v2",
      "title": "FonTS: Text Rendering with Typography and Style Controls",
      "title_zh": "翻译失败",
      "authors": [
        "Wenda Shi",
        "Yiren Song",
        "Dengming Zhang",
        "Jiaming Liu",
        "Xingxing Zou"
      ],
      "abstract": "Visual text rendering are widespread in various real-world applications,\nrequiring careful font selection and typographic choices. Recent progress in\ndiffusion transformer (DiT)-based text-to-image (T2I) models show promise in\nautomating these processes. However, these methods still encounter challenges\nlike inconsistent fonts, style variation, and limited fine-grained control,\nparticularly at the word-level. This paper proposes a two-stage DiT-based\npipeline to address these problems by enhancing controllability over typography\nand style in text rendering. We introduce typography control fine-tuning\n(TC-FT), an parameter-efficient fine-tuning method (on $5\\%$ key parameters)\nwith enclosing typography control tokens (ETC-tokens), which enables precise\nword-level application of typographic features. To further address style\ninconsistency in text rendering, we propose a text-agnostic style control\nadapter (SCA) that prevents content leakage while enhancing style consistency.\nTo implement TC-FT and SCA effectively, we incorporated HTML-render into the\ndata synthesis pipeline and proposed the first word-level controllable dataset.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\napproach in achieving superior word-level typographic control, font\nconsistency, and style consistency in text rendering tasks. The datasets and\nmodels will be available for academic use.",
      "tldr_zh": "该论文提出FonTS，一种基于Diffusion Transformer (DiT)的两阶段管道，用于提升文本渲染中的排版和风格控制，解决现有T2I模型在字体不一致、风格变化和字级细粒度控制方面的挑战。第一阶段引入Typography Control Fine-Tuning (TC-FT)，一种参数高效的微调方法，仅优化5%的关键参数，并使用Enclosing Typography Control Tokens (ETC-tokens)实现精确的字级排版应用。第二阶段的Text-Agnostic Style Control Adapter (SCA)则防止内容泄露并提升风格一致性；为此，作者整合了HTML-render到数据合成管道，并创建了首个字级可控数据集。实验结果显示，该方法在文本渲染任务中显著提高了字级排版控制、字体一致性和风格一致性，相关数据集和模型将公开用于学术研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00136v2",
      "published_date": "2024-11-28 16:19:37 UTC",
      "updated_date": "2025-03-10 08:43:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:19:42.001232"
    },
    {
      "arxiv_id": "2411.19234v1",
      "title": "SmartLLMSentry: A Comprehensive LLM Based Smart Contract Vulnerability Detection Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Oualid Zaazaa",
        "Hanan El Bakkali"
      ],
      "abstract": "Smart contracts are essential for managing digital assets in blockchain\nnetworks, highlighting the need for effective security measures. This paper\nintroduces SmartLLMSentry, a novel framework that leverages large language\nmodels (LLMs), specifically ChatGPT with in-context training, to advance smart\ncontract vulnerability detection. Traditional rule-based frameworks have\nlimitations in integrating new detection rules efficiently. In contrast,\nSmartLLMSentry utilizes LLMs to streamline this process. We created a\nspecialized dataset of five randomly selected vulnerabilities for model\ntraining and evaluation. Our results show an exact match accuracy of 91.1% with\nsufficient data, although GPT-4 demonstrated reduced performance compared to\nGPT-3 in rule generation. This study illustrates that SmartLLMSentry\nsignificantly enhances the speed and accuracy of vulnerability detection\nthrough LLMdriven rule integration, offering a new approach to improving\nBlockchain security and addressing previously underexplored vulnerabilities in\nsmart contracts.",
      "tldr_zh": "本文提出 SmartLLMSentry，一种基于大型语言模型 (LLMs) 如 ChatGPT 的全面框架，用于检测智能合约漏洞，以解决传统规则-based 方法在规则集成效率上的局限性。该框架通过 in-context training 和一个专门数据集（涵盖五种随机选定的漏洞）来训练模型，提升检测过程的速度和准确性。实验结果显示，在足够数据支持下，精确匹配准确率达到 91.1%，尽管 GPT-4 在规则生成方面不如 GPT-3。该研究为区块链安全提供了新颖的 LLM 驱动方法，有效识别先前未充分探索的漏洞。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19234v1",
      "published_date": "2024-11-28 16:02:01 UTC",
      "updated_date": "2024-11-28 16:02:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:19:53.445166"
    },
    {
      "arxiv_id": "2411.19230v1",
      "title": "Pre-Training Graph Contrastive Masked Autoencoders are Strong Distillers for EEG",
      "title_zh": "翻译失败",
      "authors": [
        "Xinxu Wei",
        "Kanhao Zhao",
        "Yong Jiao",
        "Nancy B. Carlisle",
        "Hua Xie",
        "Yu Zhang"
      ],
      "abstract": "Effectively utilizing extensive unlabeled high-density EEG data to improve\nperformance in scenarios with limited labeled low-density EEG data presents a\nsignificant challenge. In this paper, we address this by framing it as a graph\ntransfer learning and knowledge distillation problem. We propose a Unified\nPre-trained Graph Contrastive Masked Autoencoder Distiller, named EEG-DisGCMAE,\nto bridge the gap between unlabeled/labeled and high/low-density EEG data. To\nfully leverage the abundant unlabeled EEG data, we introduce a novel unified\ngraph self-supervised pre-training paradigm, which seamlessly integrates Graph\nContrastive Pre-training and Graph Masked Autoencoder Pre-training. This\napproach synergistically combines contrastive and generative pre-training\ntechniques by reconstructing contrastive samples and contrasting the\nreconstructions. For knowledge distillation from high-density to low-density\nEEG data, we propose a Graph Topology Distillation loss function, allowing a\nlightweight student model trained on low-density data to learn from a teacher\nmodel trained on high-density data, effectively handling missing electrodes\nthrough contrastive distillation. To integrate transfer learning and\ndistillation, we jointly pre-train the teacher and student models by\ncontrasting their queries and keys during pre-training, enabling robust\ndistillers for downstream tasks. We demonstrate the effectiveness of our method\non four classification tasks across two clinical EEG datasets with abundant\nunlabeled data and limited labeled data. The experimental results show that our\napproach significantly outperforms contemporary methods in both efficiency and\naccuracy.",
      "tldr_zh": "该研究针对利用大量未标记的高密度 EEG 数据提升有限标记的低密度 EEG 性能的问题，提出了一种统一的预训练框架 EEG-DisGCMAE，将 Graph Contrastive Pre-training 和 Graph Masked Autoencoder Pre-training 整合，通过重建对比样本和对比重建来实现协同学习。论文引入 Graph Topology Distillation 损失函数，实现从高密度 EEG 教师模型到低密度 EEG 学生模型的知识转移，处理缺失电极问题，并通过联合预训练对比查询和键来增强模型鲁棒性。在四个分类任务上实验表明，该方法在效率和准确性上显著优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.19230v1",
      "published_date": "2024-11-28 15:53:32 UTC",
      "updated_date": "2024-11-28 15:53:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:20:05.253009"
    },
    {
      "arxiv_id": "2411.19229v2",
      "title": "Habit Coach: Customising RAG-based chatbots to support behavior change",
      "title_zh": "翻译失败",
      "authors": [
        "Arian Fooroogh Mand Arabi",
        "Cansu Koyuturk",
        "Michael O'Mahony",
        "Raffaella Calati",
        "Dimitri Ognibene"
      ],
      "abstract": "This paper presents the iterative development of Habit Coach, a GPT-based\nchatbot designed to support users in habit change through personalized\ninteraction. Employing a user-centered design approach, we developed the\nchatbot using a Retrieval-Augmented Generation (RAG) system, which enables\nbehavior personalization without retraining the underlying language model\n(GPT-4). The system leverages document retrieval and specialized prompts to\ntailor interactions, drawing from Cognitive Behavioral Therapy (CBT) and\nnarrative therapy techniques. A key challenge in the development process was\nthe difficulty of translating declarative knowledge into effective interaction\nbehaviors. In the initial phase, the chatbot was provided with declarative\nknowledge about CBT via reference textbooks and high-level conversational\ngoals. However, this approach resulted in imprecise and inefficient behavior,\nas the GPT model struggled to convert static information into dynamic and\ncontextually appropriate interactions. This highlighted the limitations of\nrelying solely on declarative knowledge to guide chatbot behavior, particularly\nin nuanced, therapeutic conversations. Over four iterations, we addressed this\nissue by gradually transitioning towards procedural knowledge, refining the\nchatbot's interaction strategies, and improving its overall effectiveness. In\nthe final evaluation, 5 participants engaged with the chatbot over five\nconsecutive days, receiving individualized CBT interventions. The Self-Report\nHabit Index (SRHI) was used to measure habit strength before and after the\nintervention, revealing a reduction in habit strength post-intervention. These\nresults underscore the importance of procedural knowledge in driving effective,\npersonalized behavior change support in RAG-based systems.",
      "tldr_zh": "这篇论文介绍了 Habit Coach，一种基于 GPT-4 的聊天机器人，使用 Retrieval-Augmented Generation (RAG) 系统结合 Cognitive Behavioral Therapy (CBT) 和叙事疗法，提供个性化的习惯改变支持。该系统通过迭代开发过程，从最初依赖声明性知识（如 CBT 教材）转向程序性知识，解决了将静态信息转化为动态互动的挑战，从而提升了聊天机器人的精确性和有效性。在最终评估中，5 名参与者通过五天互动后，使用 Self-Report Habit Index (SRHI) 测量显示习惯强度降低，这强调了程序性知识在 RAG-based 系统中的关键作用。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted for Italian Workshop on Artificial Intelligence for Human\n  Machine Interaction (AIxHMI 2024), November 26, 2024, Bolzano, Italy",
      "pdf_url": "http://arxiv.org/pdf/2411.19229v2",
      "published_date": "2024-11-28 15:53:27 UTC",
      "updated_date": "2024-12-16 17:16:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:20:17.311811"
    },
    {
      "arxiv_id": "2411.19223v5",
      "title": "On the Unknowable Limits to Prediction",
      "title_zh": "论预测的不可知极限",
      "authors": [
        "Jiani Yan",
        "Charles Rahal"
      ],
      "abstract": "We propose a rigorous decomposition of predictive error, highlighting that\nnot all 'irreducible' error is genuinely immutable. Many domains stand to\nbenefit from iterative enhancements in measurement, construct validity, and\nmodeling. Our approach demonstrates how apparently 'unpredictable' outcomes can\nbecome more tractable with improved data (across both target and features) and\nrefined algorithms. By distinguishing aleatoric from epistemic error, we\ndelineate how accuracy may asymptotically improve--though inherent\nstochasticity may remain--and offer a robust framework for advancing\ncomputational research.",
      "tldr_zh": "本文提出了一种预测错误的严格分解，指出并非所有“irreducible error”都是真正不可变的，而是可以通过迭代改进测量、结构有效性（construct validity）和建模来减少。作者区分了“aleatoric error”（固有随机性）和“epistemic error”（知识相关错误），并展示了如何通过更高质量的数据（包括目标和特征）和优化算法，使看似“unpredictable”的结果变得更易处理。该框架为计算研究提供了一个稳健方法，支持预测准确性的渐进式提升，尽管固有随机性可能持续存在。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19223v5",
      "published_date": "2024-11-28 15:48:02 UTC",
      "updated_date": "2025-02-10 22:34:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:20:29.468746"
    },
    {
      "arxiv_id": "2411.19211v1",
      "title": "On the Ethical Considerations of Generative Agents",
      "title_zh": "翻译失败",
      "authors": [
        "N'yoma Diamond",
        "Soumya Banerjee"
      ],
      "abstract": "The Generative Agents framework recently developed by Park et al. has enabled\nnumerous new technical solutions and problem-solving approaches. Academic and\nindustrial interest in generative agents has been explosive as a result of the\neffectiveness of generative agents toward emulating human behaviour. However,\nit is necessary to consider the ethical challenges and concerns posed by this\ntechnique and its usage. In this position paper, we discuss the extant\nliterature that evaluate the ethical considerations regarding generative agents\nand similar generative tools, and identify additional concerns of significant\nimportance. We also suggest guidelines and necessary future research on how to\nmitigate some of the ethical issues and systemic risks associated with\ngenerative agents.",
      "tldr_zh": "该论文讨论了 Generative Agents 框架的伦理考虑，该框架由 Park 等开发，用于模拟人类行为并推动技术创新，但也引发了潜在的道德挑战和系统风险。作为一篇位置论文，作者回顾了现有文献，识别了 Generative Agents 及类似工具的关键伦理问题，包括对人类行为的模仿可能带来的负面影响，并提出了缓解这些问题的指导方针和未来研究建议。通过这些努力，论文旨在促进更负责任的 Generative Agents 应用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "cs.MA"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted (poster) to Socially Responsible Language Modelling Research\n  (SoLaR) Workshop at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.19211v1",
      "published_date": "2024-11-28 15:31:49 UTC",
      "updated_date": "2024-11-28 15:31:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:20:40.836069"
    },
    {
      "arxiv_id": "2411.19193v1",
      "title": "Convex Regularization and Convergence of Policy Gradient Flows under Safety Constraints",
      "title_zh": "翻译失败",
      "authors": [
        "Pekka Malo",
        "Lauri Viitasaari",
        "Antti Suominen",
        "Eeva Vilkkumaa",
        "Olli Tahvonen"
      ],
      "abstract": "This paper studies reinforcement learning (RL) in infinite-horizon dynamic\ndecision processes with almost-sure safety constraints. Such safety-constrained\ndecision processes are central to applications in autonomous systems, finance,\nand resource management, where policies must satisfy strict, state-dependent\nconstraints. We consider a doubly-regularized RL framework that combines reward\nand parameter regularization to address these constraints within continuous\nstate-action spaces. Specifically, we formulate the problem as a convex\nregularized objective with parametrized policies in the mean-field regime. Our\napproach leverages recent developments in mean-field theory and Wasserstein\ngradient flows to model policies as elements of an infinite-dimensional\nstatistical manifold, with policy updates evolving via gradient flows on the\nspace of parameter distributions. Our main contributions include establishing\nsolvability conditions for safety-constrained problems, defining smooth and\nbounded approximations that facilitate gradient flows, and demonstrating\nexponential convergence towards global solutions under sufficient\nregularization. We provide general conditions on regularization functions,\nencompassing standard entropy regularization as a special case. The results\nalso enable a particle method implementation for practical RL applications. The\ntheoretical insights and convergence guarantees presented here offer a robust\nframework for safe RL in complex, high-dimensional decision-making problems.",
      "tldr_zh": "本研究探讨了强化学习（RL）在无限时域动态决策过程中的几乎确定安全约束问题，针对自主系统、金融和资源管理等领域的严格状态依赖约束，提出了一种结合奖励和参数正则化的双重正则化框架。方法通过凸正则化目标和参数化策略在均值场制度（mean-field regime）下建模策略更新，利用Wasserstein梯度流在无限维统计流形上演化策略。研究的主要贡献包括确立了安全约束问题的可解条件、定义平滑有界近似以支持梯度流、证明在足够正则化下策略向全局解指数收敛，并为RL实际应用提供粒子方法（particle method）实现，从而为复杂高维决策问题的安全RL奠定稳健框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "math.PR",
        "stat.ML",
        "90C26, 90C40, 90C46, 93E20, 60B05"
      ],
      "primary_category": "cs.LG",
      "comment": "74 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.19193v1",
      "published_date": "2024-11-28 15:04:43 UTC",
      "updated_date": "2024-11-28 15:04:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:20:54.132866"
    },
    {
      "arxiv_id": "2412.00132v1",
      "title": "Road User Classification from High-Frequency GNSS Data Using Distributed Edge Intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Lennart Köpper",
        "Thomas Wieland"
      ],
      "abstract": "Real-world traffic involves diverse road users, ranging from pedestrians to\nheavy trucks, necessitating effective road user classification for various\napplications within Intelligent Transport Systems (ITS). Traditional approaches\noften rely on intrusive and/or expensive external hardware sensors. These\nsystems typically have limited spatial coverage. In response to these\nlimitations, this work aims to investigate an unintrusive and cost-effective\nalternative for road user classification by using high-frequency (1-2 Hz)\npositional sequences. A cutting-edge solution could involve leveraging\npositioning data from 5G networks. However, this feature is currently only\nproposed in the 3GPP standard and has not yet been implemented for outdoor\napplications by 5G equipment vendors. Therefore, our approach relies on\npositional data, that is recorded under real-world conditions using Global\nNavigation Satellite Systems (GNSS) and processed on distributed edge devices.\nAs a start-ing point, four types of road users are distinguished: pedestri-ans,\ncyclists, motorcycles, and passenger cars. While earlier approaches used\nclassical statistical methods, we propose Long Short-Term Memory (LSTM)\nrecurrent neural networks (RNNs) as the preferred classification method, as\nthey repre-sent state-of-the-art in processing sequential data. An RNN\narchitecture for road user classification, based on selected motion\ncharacteristics derived from raw positional sequences is presented and the\ninfluence of sequence length on classifica-tion quality is examined. The\nresults of the work show that RNNs are capable of efficiently classifying road\nusers on dis-tributed devices, and can particularly differentiate between types\nof motorized vehicles, based on two- to four-minute se-quences.",
      "tldr_zh": "这篇论文探讨了使用高频 GNSS 数据和分布式边缘智能来分类路用户的问题，以解决传统方法的侵入性和成本高问题。研究提出了一种基于 LSTM RNNs 的分类架构，利用从原始定位序列中提取的运动特征，并分析序列长度对分类质量的影响。结果显示，该方法能在分布式设备上高效区分行人、骑行者、摩托车和乘用车，尤其在机动车辆分类上，使用2-4分钟序列即可实现准确分类。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00132v1",
      "published_date": "2024-11-28 14:51:02 UTC",
      "updated_date": "2024-11-28 14:51:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:21:05.907118"
    },
    {
      "arxiv_id": "2411.19182v1",
      "title": "SOWing Information: Cultivating Contextual Coherence with MLLMs in Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhan Pei",
        "Ruoyu Wang",
        "Yongqi Yang",
        "Ye Zhu",
        "Olga Russakovsky",
        "Yu Wu"
      ],
      "abstract": "Originating from the diffusion phenomenon in physics, which describes the\nrandom movement and collisions of particles, diffusion generative models\nsimulate a random walk in the data space along the denoising trajectory. This\nallows information to diffuse across regions, yielding harmonious outcomes.\nHowever, the chaotic and disordered nature of information diffusion in\ndiffusion models often results in undesired interference between image regions,\ncausing degraded detail preservation and contextual inconsistency. In this\nwork, we address these challenges by reframing disordered diffusion as a\npowerful tool for text-vision-to-image generation (TV2I) tasks, achieving\npixel-level condition fidelity while maintaining visual and semantic coherence\nthroughout the image. We first introduce Cyclic One-Way Diffusion (COW), which\nprovides an efficient unidirectional diffusion framework for precise\ninformation transfer while minimizing disruptive interference. Building on COW,\nwe further propose Selective One-Way Diffusion (SOW), which utilizes Multimodal\nLarge Language Models (MLLMs) to clarify the semantic and spatial relationships\nwithin the image. Based on these insights, SOW combines attention mechanisms to\ndynamically regulate the direction and intensity of diffusion according to\ncontextual relationships. Extensive experiments demonstrate the untapped\npotential of controlled information diffusion, offering a path to more adaptive\nand versatile generative models in a learning-free manner.",
      "tldr_zh": "该论文探讨了扩散生成模型在图像生成中的问题，即信息扩散导致图像区域干扰、细节退化和上下文不一致。作者提出Cyclic One-Way Diffusion (COW)框架，提供高效的单向扩散机制，以实现精确信息传输并减少干扰；随后引入Selective One-Way Diffusion (SOW)，利用Multimodal Large Language Models (MLLMs)和注意力机制，根据语义和空间关系动态调节扩散的方向和强度，从而提升文本-视觉-图像生成 (TV2I) 任务的像素级保真度和整体一致性。实验结果证明，这种控制信息扩散的方法在无学习场景下显著提高了生成模型的适应性和多功能性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://pyh-129.github.io/SOW/",
      "pdf_url": "http://arxiv.org/pdf/2411.19182v1",
      "published_date": "2024-11-28 14:35:25 UTC",
      "updated_date": "2024-11-28 14:35:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:21:17.870276"
    },
    {
      "arxiv_id": "2411.19167v2",
      "title": "HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Prithviraj Banerjee",
        "Sindi Shkodrani",
        "Pierre Moulon",
        "Shreyas Hampali",
        "Shangchen Han",
        "Fan Zhang",
        "Linguang Zhang",
        "Jade Fountain",
        "Edward Miller",
        "Selen Basol",
        "Richard Newcombe",
        "Robert Wang",
        "Jakob Julian Engel",
        "Tomas Hodan"
      ],
      "abstract": "We introduce HOT3D, a publicly available dataset for egocentric hand and\nobject tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of\nrecordings that feature 19 subjects interacting with 33 diverse rigid objects.\nIn addition to simple pick-up, observe, and put-down actions, the subjects\nperform actions typical for a kitchen, office, and living room environment. The\nrecordings include multiple synchronized data streams containing egocentric\nmulti-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D\nposes of cameras, hands, and objects. The dataset is recorded with two headsets\nfrom Meta: Project Aria, which is a research prototype of AI glasses, and Quest\n3, a virtual-reality headset that has shipped millions of units. Ground-truth\nposes were obtained by a motion-capture system using small optical markers\nattached to hands and objects. Hand annotations are provided in the UmeTrack\nand MANO formats, and objects are represented by 3D meshes with PBR materials\nobtained by an in-house scanner. In our experiments, we demonstrate the\neffectiveness of multi-view egocentric data for three popular tasks: 3D hand\ntracking, model-based 6DoF object pose estimation, and 3D lifting of unknown\nin-hand objects. The evaluated multi-view methods, whose benchmarking is\nuniquely enabled by HOT3D, significantly outperform their single-view\ncounterparts.",
      "tldr_zh": "本研究引入了HOT3D数据集，用于第一人称视角的多视角视频中的3D手和物体跟踪。该数据集包含超过833分钟（超过370万张图像）的录像，涉及19个受试者与33个刚性物体的互动，包括厨房、办公室和客厅环境的典型动作，并提供同步的多视角RGB图像、眼动信号、场景点云以及相机、手和物体的3D姿势。手部注释采用UmeTrack和MANO格式，物体则使用3D网格和PBR材料。实验结果表明，多视角方法在3D手跟踪、基于模型的6DoF object pose estimation以及未知手持物体的3D lifting任务中显著优于单视角对应方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.19167v2",
      "published_date": "2024-11-28 14:09:42 UTC",
      "updated_date": "2025-04-30 13:32:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:21:30.208627"
    },
    {
      "arxiv_id": "2412.00131v1",
      "title": "Open-Sora Plan: Open-Source Large Video Generation Model",
      "title_zh": "Open-Sora Plan：开源大型视频生成模型",
      "authors": [
        "Bin Lin",
        "Yunyang Ge",
        "Xinhua Cheng",
        "Zongjian Li",
        "Bin Zhu",
        "Shaodong Wang",
        "Xianyi He",
        "Yang Ye",
        "Shenghai Yuan",
        "Liuhan Chen",
        "Tanghui Jia",
        "Junwu Zhang",
        "Zhenyu Tang",
        "Yatian Pang",
        "Bin She",
        "Cen Yan",
        "Zhiheng Hu",
        "Xiaoyi Dong",
        "Lin Chen",
        "Zhang Pan",
        "Xing Zhou",
        "Shaoling Dong",
        "Yonghong Tian",
        "Li Yuan"
      ],
      "abstract": "We introduce Open-Sora Plan, an open-source project that aims to contribute a\nlarge generation model for generating desired high-resolution videos with long\ndurations based on various user inputs. Our project comprises multiple\ncomponents for the entire video generation process, including a Wavelet-Flow\nVariational Autoencoder, a Joint Image-Video Skiparse Denoiser, and various\ncondition controllers. Moreover, many assistant strategies for efficient\ntraining and inference are designed, and a multi-dimensional data curation\npipeline is proposed for obtaining desired high-quality data. Benefiting from\nefficient thoughts, our Open-Sora Plan achieves impressive video generation\nresults in both qualitative and quantitative evaluations. We hope our careful\ndesign and practical experience can inspire the video generation research\ncommunity. All our codes and model weights are publicly available at\n\\url{https://github.com/PKU-YuanGroup/Open-Sora-Plan}.",
      "tldr_zh": "本研究介绍了 Open-Sora Plan，这是一个开源项目，旨在开发大型视频生成模型，能够基于用户输入生成高分辨率、长时长的视频。项目包括关键组件如 Wavelet-Flow Variational Autoencoder、Joint Image-Video Skiparse Denoiser 和各种条件控制器，以及高效训练和推理策略和多维数据整理管道。这些设计使模型在定性和定量评估中取得了令人印象深刻的视频生成性能，所有代码和模型权重已在 GitHub 上公开，以激励视频生成研究社区。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "v1.3",
      "pdf_url": "http://arxiv.org/pdf/2412.00131v1",
      "published_date": "2024-11-28 14:07:45 UTC",
      "updated_date": "2024-11-28 14:07:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:21:40.587810"
    },
    {
      "arxiv_id": "2411.19154v1",
      "title": "DESIRE: Dynamic Knowledge Consolidation for Rehearsal-Free Continual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Haiyang Guo",
        "Fei Zhu",
        "Fanhu Zeng",
        "Bing Liu",
        "Xu-Yao Zhang"
      ],
      "abstract": "Continual learning aims to equip models with the ability to retain previously\nlearned knowledge like a human. Recent work incorporating Parameter-Efficient\nFine-Tuning has revitalized the field by introducing lightweight extension\nmodules. However, existing methods usually overlook the issue of information\nleakage caused by the fact that the experiment data have been used in\npre-trained models. Once these duplicate data are removed in the pre-training\nphase, their performance can be severely affected. In this paper, we propose a\nnew LoRA-based rehearsal-free method named DESIRE. Our method avoids imposing\nadditional constraints during training to mitigate catastrophic forgetting,\nthereby maximizing the learning of new classes. To integrate knowledge from old\nand new tasks, we propose two efficient post-processing modules. On the one\nhand, we retain only two sets of LoRA parameters for merging and propose\ndynamic representation consolidation to calibrate the merged feature\nrepresentation. On the other hand, we propose decision boundary refinement to\naddress classifier bias when training solely on new class data. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\non multiple datasets and strikes an effective balance between stability and\nplasticity. Our code will be publicly available.",
      "tldr_zh": "本论文提出了一种名为 DESIRE 的基于 LoRA 的无排练持续学习方法（rehearsal-free Continual Learning），旨在解决现有方法忽略的信息泄露问题和灾难性遗忘（catastrophic forgetting），从而最大化新类别的学习。DESIRE 通过两个高效的后处理模块实现知识整合：动态表示整合（dynamic representation consolidation）来校准合并后的特征表示，以及决策边界精炼（decision boundary refinement）来修正仅在新类数据上训练时的分类器偏差。实验结果显示，该方法在多个数据集上达到了 state-of-the-art 性能，并在稳定性和可塑性之间取得了有效平衡，且代码将公开可用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19154v1",
      "published_date": "2024-11-28 13:54:01 UTC",
      "updated_date": "2024-11-28 13:54:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:21:54.319517"
    },
    {
      "arxiv_id": "2411.19141v1",
      "title": "On Moving Object Segmentation from Monocular Video with Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Christian Homeyer",
        "Christoph Schnörr"
      ],
      "abstract": "Moving object detection and segmentation from a single moving camera is a\nchallenging task, requiring an understanding of recognition, motion and 3D\ngeometry. Combining both recognition and reconstruction boils down to a fusion\nproblem, where appearance and motion features need to be combined for\nclassification and segmentation. In this paper, we present a novel fusion\narchitecture for monocular motion segmentation - M3Former, which leverages the\nstrong performance of transformers for segmentation and multi-modal fusion. As\nreconstructing motion from monocular video is ill-posed, we systematically\nanalyze different 2D and 3D motion representations for this problem and their\nimportance for segmentation performance. Finally, we analyze the effect of\ntraining data and show that diverse datasets are required to achieve SotA\nperformance on Kitti and Davis.",
      "tldr_zh": "本论文探讨了从单目视频中分割移动物体的挑战，该任务需要整合识别、运动和3D几何理解。作者提出了一种新型融合架构M3Former，利用Transformers进行分割和多模态融合，将外观和运动特征结合以提升分类和分割性能。论文系统分析了不同2D和3D运动表示对分割效果的重要性，并发现使用多样化数据集是实现SotA性能的关键，在Kitti和Davis数据集上取得了显著改进。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "WICCV2023",
      "pdf_url": "http://arxiv.org/pdf/2411.19141v1",
      "published_date": "2024-11-28 13:42:35 UTC",
      "updated_date": "2024-11-28 13:42:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:22:04.898918"
    },
    {
      "arxiv_id": "2411.19140v1",
      "title": "Examining Multimodal Gender and Content Bias in ChatGPT-4o",
      "title_zh": "研究 ChatGPT-4o 中的多模态性别和内容偏见",
      "authors": [
        "Roberto Balestri"
      ],
      "abstract": "This study investigates ChatGPT-4o's multimodal content generation,\nhighlighting significant disparities in its treatment of sexual content and\nnudity versus violent and drug-related themes. Detailed analysis reveals that\nChatGPT-4o consistently censors sexual content and nudity, while showing\nleniency towards violence and drug use. Moreover, a pronounced gender bias\nemerges, with female-specific content facing stricter regulation compared to\nmale-specific content. This disparity likely stems from media scrutiny and\npublic backlash over past AI controversies, prompting tech companies to impose\nstringent guidelines on sensitive issues to protect their reputations. Our\nfindings emphasize the urgent need for AI systems to uphold genuine ethical\nstandards and accountability, transcending mere political correctness. This\nresearch contributes to the understanding of biases in AI-driven language and\nmultimodal models, calling for more balanced and ethical content moderation\npractices.",
      "tldr_zh": "这篇论文考察了 ChatGPT-4o 在多模态内容生成中的性别和内容偏差，发现它对性内容和裸露进行严格审查，而对暴力及毒品相关主题较为宽容。研究进一步揭示了明显的 gender bias，女性特定内容比男性内容受到更严厉的监管，这种差异可能源于媒体审查和过去 AI 争议引发的公司声誉保护措施。论文强调，AI 系统需超越表面上的政治正确，真正遵守伦理标准和问责制，并呼吁实施更平衡的内容 moderation 实践。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "stat.OT"
      ],
      "primary_category": "cs.CY",
      "comment": "17 pages, 4 figures, 3 tables. Conference: \"14th International\n  Conference on Artificial Intelligence, Soft Computing and Applications (AIAA\n  2024), London, 23-24 November 2024\" It will be published in the proceedings\n  \"David C. Wyld et al. (Eds): IoTE, CNDC, DSA, AIAA, NLPTA, DPPR - 2024\"",
      "pdf_url": "http://arxiv.org/pdf/2411.19140v1",
      "published_date": "2024-11-28 13:41:44 UTC",
      "updated_date": "2024-11-28 13:41:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:24:17.320779"
    },
    {
      "arxiv_id": "2411.19134v1",
      "title": "Visual SLAMMOT Considering Multiple Motion Models",
      "title_zh": "视觉 SLAMMOT：考虑多个运动模型",
      "authors": [
        "Peilin Tian",
        "Hao Li"
      ],
      "abstract": "Simultaneous Localization and Mapping (SLAM) and Multi-Object Tracking (MOT)\nare pivotal tasks in the realm of autonomous driving, attracting considerable\nresearch attention. While SLAM endeavors to generate real-time maps and\ndetermine the vehicle's pose in unfamiliar settings, MOT focuses on the\nreal-time identification and tracking of multiple dynamic objects. Despite\ntheir importance, the prevalent approach treats SLAM and MOT as independent\nmodules within an autonomous vehicle system, leading to inherent limitations.\nClassical SLAM methodologies often rely on a static environment assumption,\nsuitable for indoor rather than dynamic outdoor scenarios. Conversely,\nconventional MOT techniques typically rely on the vehicle's known state,\nconstraining the accuracy of object state estimations based on this prior. To\naddress these challenges, previous efforts introduced the unified SLAMMOT\nparadigm, yet primarily focused on simplistic motion patterns. In our team's\nprevious work IMM-SLAMMOT\\cite{IMM-SLAMMOT}, we present a novel methodology\nincorporating consideration of multiple motion models into SLAMMOT i.e. tightly\ncoupled SLAM and MOT, demonstrating its efficacy in LiDAR-based systems. This\npaper studies feasibility and advantages of instantiating this methodology as\nvisual SLAMMOT, bridging the gap between LiDAR and vision-based sensing\nmechanisms. Specifically, we propose a solution of visual SLAMMOT considering\nmultiple motion models and validate the inherent advantages of IMM-SLAMMOT in\nthe visual domain.",
      "tldr_zh": "本研究探讨了在自动驾驶领域中，将同时定位与建图(SLAM)和多目标跟踪(MOT)紧密整合的视觉 SLAMMOT 方法，以解决传统独立模块的局限性，如 SLAM 假设静态环境和 MOT 依赖已知车辆状态。基于先前 IMM-SLAMMOT 工作，该论文提出一种考虑多个运动模型的视觉 SLAMMOT 解决方案，将其从 LiDAR 系统扩展到视觉领域。实验验证了这一方法的有效性，展示了其在桥接不同传感机制方面的优势，提高了动态场景下的准确性和鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19134v1",
      "published_date": "2024-11-28 13:36:04 UTC",
      "updated_date": "2024-11-28 13:36:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:22:29.059827"
    },
    {
      "arxiv_id": "2411.19121v1",
      "title": "MSG score: A Comprehensive Evaluation for Multi-Scene Video Generation",
      "title_zh": "MSG score：多场景视频生成的全面评估",
      "authors": [
        "Daewon Yoon",
        "Hyungsuk Lee",
        "Wonsik Shin"
      ],
      "abstract": "This paper addresses the metrics required for generating multi-scene videos\nbased on a continuous scenario, as opposed to traditional short video\ngeneration. Scenario-based videos require a comprehensive evaluation that\nconsiders multiple factors such as character consistency, artistic coherence,\naesthetic quality, and the alignment of the generated content with the intended\nprompt. Additionally, in video generation, unlike single images, the movement\nof characters across frames introduces potential issues like distortion or\nunintended changes, which must be effectively evaluated and corrected. In the\ncontext of probabilistic models like diffusion, generating the desired scene\nrequires repeated sampling and manual selection, akin to how a film director\nchooses the best shots from numerous takes. We propose a score-based evaluation\nbenchmark that automates this process, enabling a more objective and efficient\nassessment of these complexities. This approach allows for the generation of\nhigh-quality multi-scene videos by selecting the best outcomes based on\nautomated scoring rather than manual inspection.",
      "tldr_zh": "这篇论文针对基于连续场景的多场景视频生成，提出了一种全面的评估指标MSG score，以解决传统短视频生成无法覆盖的复杂因素，如角色一致性、艺术连贯性、美学质量以及生成内容与提示的匹配。不同于单图像生成，该指标特别关注视频中角色运动可能导致的扭曲或意外变化问题，并借鉴扩散模型的反复采样过程。MSG score通过自动化评分基准模拟导演选择最佳镜头的机制，实现更客观、高效的视频评估，从而提升多场景视频的整体质量。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19121v1",
      "published_date": "2024-11-28 13:11:50 UTC",
      "updated_date": "2024-11-28 13:11:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:22:40.838309"
    },
    {
      "arxiv_id": "2411.19114v1",
      "title": "PREBA: A Hardware/Software Co-Design for Multi-Instance GPU based AI Inference Servers",
      "title_zh": "PREBA：一种针对基于多实例 GPU 的 AI 推理服务器的硬件/软件协同设计",
      "authors": [
        "Gwangoo Yeo",
        "Jiin Kim",
        "Yujeong Choi",
        "Minsoo Rhu"
      ],
      "abstract": "NVIDIA's Multi-Instance GPU (MIG) is a feature that enables system designers\nto reconfigure one large GPU into multiple smaller GPU slices. This work\ncharacterizes this emerging GPU and evaluates its effectiveness in designing\nhigh-performance AI inference servers. Our study reveals that the data\npreprocessing stage of AI inference causes significant performance bottlenecks\nto MIG. To this end, we present PREBA, which is a hardware/software co-design\ntargeting MIG inference servers. Our first proposition is an FPGA-based data\npreprocessing accelerator that unlocks the full potential of MIG with\ndomain-specific acceleration of data preprocessing. The MIG inference server\nunleashed from preprocessing overheads is then augmented with our dynamic\nbatching system that enables high-performance inference. PREBA is implemented\nend-to-end in real systems, providing a 3.7x improvement in throughput, 3.4x\nreduction in tail latency, 3.5x improvement in energy-efficiency, and 3.0x\nimprovement in cost-efficiency.",
      "tldr_zh": "该研究针对 NVIDIA 的 Multi-Instance GPU (MIG) 在 AI 推理服务器中的应用，识别出数据预处理阶段导致的性能瓶颈，并提出 PREBA，这是一个硬件/软件联合设计方案。PREBA 包括一个基于 FPGA 的数据预处理加速器，以加速特定领域的数据处理，并结合动态批处理系统来提升推理性能。通过在真实系统中端到端实现，PREBA 实现了 3.7 倍吞吐量提升、3.4 倍尾延迟减少、3.5 倍能效改善和 3.0 倍成本效率提升。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.AR",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19114v1",
      "published_date": "2024-11-28 13:02:41 UTC",
      "updated_date": "2024-11-28 13:02:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:22:53.943028"
    },
    {
      "arxiv_id": "2412.00127v2",
      "title": "Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads",
      "title_zh": "翻译失败",
      "authors": [
        "Siqi Kou",
        "Jiachun Jin",
        "Zhihong Liu",
        "Chang Liu",
        "Ye Ma",
        "Jian Jia",
        "Quan Chen",
        "Peng Jiang",
        "Zhijie Deng"
      ],
      "abstract": "We introduce Orthus, an autoregressive (AR) transformer that excels in\ngenerating images given textual prompts, answering questions based on visual\ninputs, and even crafting lengthy image-text interleaved contents. Unlike prior\narts on unified multimodal modeling, Orthus simultaneously copes with discrete\ntext tokens and continuous image features under the AR modeling principle. The\ncontinuous treatment of visual signals minimizes the information loss for both\nimage understanding and generation while the fully AR formulation renders the\ncharacterization of the correlation between modalities straightforward. The key\nmechanism enabling Orthus to leverage these advantages lies in its\nmodality-specific heads -- one regular language modeling (LM) head predicts\ndiscrete text tokens and one diffusion head generates continuous image features\nconditioning on the output of the backbone. We devise an efficient strategy for\nbuilding Orthus -- by substituting the Vector Quantization (VQ) operation in\nthe existing unified AR model with a soft alternative, introducing a diffusion\nhead, and tuning the added modules to reconstruct images, we can create an\nOrthus-base model effortlessly (e.g., within mere 72 A100 GPU hours).\nOrthus-base can further embrace post-training to better model interleaved\nimages and texts. Empirically, Orthus surpasses competing baselines including\nShow-o and Chameleon across standard benchmarks, achieving a GenEval score of\n0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows\nexceptional mixed-modality generation capabilities, reflecting the potential\nfor handling intricate practical generation tasks.",
      "tldr_zh": "我们引入了 Orthus，一种 autoregressive transformer，用于基于文本提示生成图像、回答视觉问题以及创建图像-文本交错内容。\nOrthus 通过 modality-specific heads（包括语言建模头预测文本标记和扩散头生成图像特征）同时处理离散文本和连续图像信号，实现高效的 AR 建模，并通过替换 VQ 操作和微调模块（仅需 72 A100 GPU 小时）轻松构建。\n实验结果显示，Orthus 超越了基线模型如 Show-o 和 Chameleon，在 GenEval 分数达 0.58、MME-P 分数达 1265.8 的基础上，展示了出色的混合模态生成能力，适用于复杂实际任务。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00127v2",
      "published_date": "2024-11-28 13:00:38 UTC",
      "updated_date": "2025-04-16 10:04:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:24:29.075123"
    },
    {
      "arxiv_id": "2411.19094v3",
      "title": "Beautimeter: Harnessing GPT for Assessing Architectural and Urban Beauty based on the 15 Properties of Living Structure",
      "title_zh": "翻译失败",
      "authors": [
        "Bin Jiang"
      ],
      "abstract": "Beautimeter is a new tool powered by generative pre-trained transformer (GPT)\ntechnology, designed to evaluate architectural and urban beauty. Rooted in\nChristopher Alexander's theory of centers, this work builds on the idea that\nall environments possess, to varying degrees, an innate sense of life.\nAlexander identified 15 fundamental properties, such as levels of scale and\nthick boundaries, that characterize living structure, which Beautimeter uses as\na basis for its analysis. By integrating GPT's advanced natural language\nprocessing capabilities, Beautimeter assesses the extent to which a structure\nembodies these 15 properties, enabling a nuanced evaluation of architectural\nand urban aesthetics. Using ChatGPT, the tool helps users generate insights\ninto the perceived beauty and coherence of spaces. We conducted a series of\ncase studies, evaluating images of architectural and urban environments, as\nwell as carpets, paintings, and other artifacts. The results demonstrate\nBeautimeter's effectiveness in analyzing aesthetic qualities across diverse\ncontexts. Our findings suggest that by leveraging GPT technology, Beautimeter\noffers architects, urban planners, and designers a powerful tool to create\nspaces that resonate deeply with people. This paper also explores the\nimplications of such technology for architecture and urban design, highlighting\nits potential to enhance both the design process and the assessment of built\nenvironments. Keywords: Living structure, structural beauty, Christopher\nAlexander, AI in Design, human centered design",
      "tldr_zh": "本研究开发了Beautimeter，一种基于GPT技术的工具，用于评估建筑和城市美学，根植于Christopher Alexander的Living Structure理论及其15个核心属性（如levels of scale和thick boundaries）。该工具利用GPT的自然语言处理能力分析结构是否体现了这些属性，并通过ChatGPT帮助用户生成空间美感和连贯性的洞见。实验结果显示，Beautimeter在多种案例研究中（如建筑、城市环境和艺术品图像）有效提升了美学评估的准确性，为建筑师、城市规划者和设计师提供了一个强大的人centered design工具，潜在地优化设计过程和环境质量。",
      "categories": [
        "physics.soc-ph",
        "cs.AI"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "12 pages, 6 figure, and 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.19094v3",
      "published_date": "2024-11-28 12:14:24 UTC",
      "updated_date": "2025-03-23 13:58:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:24:40.543903"
    },
    {
      "arxiv_id": "2411.19083v1",
      "title": "ObjectRelator: Enabling Cross-View Object Relation Understanding in Ego-Centric and Exo-Centric Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Yuqian Fu",
        "Runze Wang",
        "Yanwei Fu",
        "Danda Pani Paudel",
        "Xuanjing Huang",
        "Luc Van Gool"
      ],
      "abstract": "In this paper, we focus on the Ego-Exo Object Correspondence task, an\nemerging challenge in the field of computer vision that aims to map objects\nacross ego-centric and exo-centric views. We introduce ObjectRelator, a novel\nmethod designed to tackle this task, featuring two new modules: Multimodal\nCondition Fusion (MCFuse) and SSL-based Cross-View Object Alignment\n(XObjAlign). MCFuse effectively fuses language and visual conditions to enhance\ntarget object localization, while XObjAlign enforces consistency in object\nrepresentations across views through a self-supervised alignment strategy.\nExtensive experiments demonstrate the effectiveness of ObjectRelator, achieving\nstate-of-the-art performance on Ego2Exo and Exo2Ego tasks with minimal\nadditional parameters. This work provides a foundation for future research in\ncomprehensive cross-view object relation understanding highlighting the\npotential of leveraging multimodal guidance and cross-view alignment. Codes and\nmodels will be released to advance further research in this direction.",
      "tldr_zh": "本研究针对计算机视觉中的Ego-Exo Object Correspondence任务，提出了一种新方法ObjectRelator，用于在第一人称（ego-centric）和第三人称（exo-centric）视频间理解和映射对象关系。ObjectRelator包括两个关键模块：Multimodal Condition Fusion (MCFuse)，用于融合语言和视觉条件以提升目标对象定位；以及SSL-based Cross-View Object Alignment (XObjAlign)，通过自监督策略确保跨视角对象表示的一致性。实验结果显示，ObjectRelator在Ego2Exo和Exo2Ego任务上实现了最先进性能，同时仅需最少的额外参数，为跨视角对象关系理解提供了重要基础，并计划发布代码和模型以推动后续研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19083v1",
      "published_date": "2024-11-28 12:01:03 UTC",
      "updated_date": "2024-11-28 12:01:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:24:52.854804"
    },
    {
      "arxiv_id": "2411.19075v1",
      "title": "LADDER: Multi-objective Backdoor Attack via Evolutionary Algorithm",
      "title_zh": "翻译失败",
      "authors": [
        "Dazhuang Liu",
        "Yanqi Qiao",
        "Rui Wang",
        "Kaitai Liang",
        "Georgios Smaragdakis"
      ],
      "abstract": "Current black-box backdoor attacks in convolutional neural networks formulate\nattack objective(s) as single-objective optimization problems in single domain.\nDesigning triggers in single domain harms semantics and trigger robustness as\nwell as introduces visual and spectral anomaly. This work proposes a\nmulti-objective black-box backdoor attack in dual domains via evolutionary\nalgorithm (LADDER), the first instance of achieving multiple attack objectives\nsimultaneously by optimizing triggers without requiring prior knowledge about\nvictim model. In particular, we formulate LADDER as a multi-objective\noptimization problem (MOP) and solve it via multi-objective evolutionary\nalgorithm (MOEA). MOEA maintains a population of triggers with trade-offs among\nattack objectives and uses non-dominated sort to drive triggers toward optimal\nsolutions. We further apply preference-based selection to MOEA to exclude\nimpractical triggers. We state that LADDER investigates a new dual-domain\nperspective for trigger stealthiness by minimizing the anomaly between clean\nand poisoned samples in the spectral domain. Lastly, the robustness against\npreprocessing operations is achieved by pushing triggers to low-frequency\nregions. Extensive experiments comprehensively showcase that LADDER achieves\nattack effectiveness of at least 99%, attack robustness with 90.23% (50.09%\nhigher than state-of-the-art attacks on average), superior natural stealthiness\n(1.12x to 196.74x improvement) and excellent spectral stealthiness (8.45x\nenhancement) as compared to current stealthy attacks by the average $l_2$-norm\nacross 5 public datasets.",
      "tldr_zh": "该论文提出 LADDER，一种基于进化算法的多目标黑盒后门攻击方法，旨在同时优化攻击有效性、鲁棒性和隐蔽性，而无需 victim 模型的先验知识。\nLADDER 将攻击问题表述为多目标优化问题 (MOP)，使用多目标进化算法 (MOEA) 在双领域（视觉和光谱）中优化触发器，通过非主导排序和偏好选择来权衡目标，并将触发器推向低频区域以提升对预处理操作的抵抗力。\n实验结果显示，在 5 个公共数据集上，LADDER 实现了至少 99% 的攻击有效性，攻击鲁棒性平均提高 50.09%，并在自然隐蔽性（1.12x 到 196.74x 改善）和光谱隐蔽性（8.45x 增强）方面显著优于现有攻击。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19075v1",
      "published_date": "2024-11-28 11:50:23 UTC",
      "updated_date": "2024-11-28 11:50:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:25:06.373358"
    },
    {
      "arxiv_id": "2412.03593v1",
      "title": "CovidLLM: A Robust Large Language Model with Missing Value Adaptation and Multi-Objective Learning Strategy for Predicting Disease Severity and Clinical Outcomes in COVID-19 Patients",
      "title_zh": "CovidLLM：一种鲁棒的大型语言模型，结合缺失值适应和多目标学习策略，用于预测COVID-19患者疾病严重程度和临床结局",
      "authors": [
        "Shengjun Zhu",
        "Siyu Liu",
        "Yang Li",
        "Qing Lei",
        "Hongyan Hou",
        "Hewei Jiang",
        "Shujuan Guo",
        "Feng Wang",
        "Rongshang Chen",
        "Xionglin Fan",
        "Shengce Tao",
        "Jiaxin Cai"
      ],
      "abstract": "Coronavirus Disease 2019 (COVID-19), which emerged in 2019, has caused\nmillions of deaths worldwide. Although effective vaccines have been developed\nto mitigate severe symptoms, certain populations, particularly the elderly and\nthose with comorbidities, remain at high risk for severe outcomes and increased\nmortality. Consequently, early identification of the severity and clinical\noutcomes of the disease in these patients is vital to prevent adverse\nprognoses. Although traditional machine learning and deep learning models have\nbeen widely employed in this area, the potential of large language models\n(LLMs) remains largely unexplored. Our research focuses primarily on\nconstructing specialized prompts and adopting multi-objective learning\nstrategies. We started by selecting serological indicators that significantly\ncorrelate with clinical outcomes and disease severity to serve as input data\nfor the model. Blood test samples often contain numerous missing values, and\ntraditional models generally rely on imputation to handle these gaps in the\ndata. In contrast, LLMs offer the advantage of robust semantic understanding.\nBy setting prompts, we can explicitly inform the model when a feature's value\nis missing, without the need for imputation. For the multi-objective learning\nstrategy, the model is designed to first predict disease severity and then\npredict clinical outcomes. Given that LLMs utilize both the input text and the\ngenerated tokens as input for generating the next token, the predicted severity\nis used as a basis for generating the clinical outcome. During the fine-tuning\nof the LLM, the two objectives influence and improve each other. Our\nexperiments were implemented based on the ChatGLM model. The results\ndemonstrate the effectiveness of LLMs in this task, suggesting promising\npotential for further development.",
      "tldr_zh": "该研究开发了 CovidLLM，一种鲁棒的大型语言模型(LLMs)，通过缺失值适应和多目标学习策略来预测 COVID-19 患者的疾病严重性和临床结果，针对高危人群如老人和有并发症者提供早期识别支持。模型使用专门的提示处理血清指标中的缺失值，避免传统填充方法，而是直接告知模型特征缺失；同时，多目标学习策略先预测疾病严重性，然后基于此生成临床结果预测，使两个目标相互影响和优化。实验基于 ChatGLM 模型进行，结果显示该方法有效，并展示了 LLMs 在医疗预测领域的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03593v1",
      "published_date": "2024-11-28 11:27:38 UTC",
      "updated_date": "2024-11-28 11:27:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:25:16.591917"
    },
    {
      "arxiv_id": "2411.19064v1",
      "title": "Way to Specialist: Closing Loop Between Specialized LLM and Evolving Domain Knowledge Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Yutong Zhang",
        "Lixing Chen",
        "Shenghong Li",
        "Nan Cao",
        "Yang Shi",
        "Jiaxin Ding",
        "Zhe Qu",
        "Pan Zhou",
        "Yang Bai"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional performance across\na wide variety of domains. Nonetheless, generalist LLMs continue to fall short\nin reasoning tasks necessitating specialized knowledge. Prior investigations\ninto specialized LLMs focused on domain-specific training, which entails\nsubstantial efforts in domain data acquisition and model parameter fine-tuning.\nTo address these challenges, this paper proposes the Way-to-Specialist (WTS)\nframework, which synergizes retrieval-augmented generation with knowledge\ngraphs (KGs) to enhance the specialized capability of LLMs in the absence of\nspecialized training. In distinction to existing paradigms that merely utilize\nexternal knowledge from general KGs or static domain KGs to prompt LLM for\nenhanced domain-specific reasoning, WTS proposes an innovative\n\"LLM$\\circlearrowright$KG\" paradigm, which achieves bidirectional enhancement\nbetween specialized LLM and domain knowledge graph (DKG). The proposed paradigm\nencompasses two closely coupled components: the DKG-Augmented LLM and the\nLLM-Assisted DKG Evolution. The former retrieves question-relevant domain\nknowledge from DKG and uses it to prompt LLM to enhance the reasoning\ncapability for domain-specific tasks; the latter leverages LLM to generate new\ndomain knowledge from processed tasks and use it to evolve DKG. WTS closes the\nloop between DKG-Augmented LLM and LLM-Assisted DKG Evolution, enabling\ncontinuous improvement in the domain specialization as it progressively answers\nand learns from domain-specific questions. We validate the performance of WTS\non 6 datasets spanning 5 domains. The experimental results show that WTS\nsurpasses the previous SOTA in 4 specialized domains and achieves a maximum\nperformance improvement of 11.3%.",
      "tldr_zh": "该研究提出 Way-to-Specialist (WTS) 框架，通过检索增强生成（RAG）和知识图谱（KGs）相结合，提升大语言模型（LLMs）在专业领域推理任务的能力，而无需进行专业训练。WTS 引入 \"LLM$\\circlearrowright$KG\" 范式，实现专业 LLM 和领域知识图谱（DKG）的双向增强，包括 DKG-Augmented LLM 用于从 DKG 检索相关知识以提升任务推理，以及 LLM-Assisted DKG Evolution 利用 LLM 生成新知识来动态更新 DKG。该框架形成闭环，支持持续改进领域专业化能力。在 6 个数据集和 5 个领域的实验中，WTS 在 4 个专业领域超越现有 SOTA 方法，最大性能提升达 11.3%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by KDD 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.19064v1",
      "published_date": "2024-11-28 11:24:43 UTC",
      "updated_date": "2024-11-28 11:24:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:25:30.375860"
    },
    {
      "arxiv_id": "2411.19043v1",
      "title": "Using a Feedback Loop for LLM-based Infrastructure as Code Generation",
      "title_zh": "使用反馈循环进行",
      "authors": [
        "Mayur Amarnath Palavalli",
        "Mark Santolucito"
      ],
      "abstract": "Code generation with Large Language Models (LLMs) has helped to increase\nsoftware developer productivity in coding tasks, but has yet to have\nsignificant impact on the tasks of software developers that surround this code.\nIn particular, the challenge of infrastructure management remains an open\nquestion. We investigate the ability of an LLM agent to construct\ninfrastructure using the Infrastructure as Code (IaC) paradigm. We particularly\ninvestigate the use of a feedback loop that returns errors and warnings on the\ngenerated IaC to allow the LLM agent to improve the code. We find that, for\neach iteration of the loop, its effectiveness decreases exponentially until it\nplateaus at a certain point and becomes ineffective.",
      "tldr_zh": "该论文探讨了使用 Large Language Models (LLMs) 生成 Infrastructure as Code (IaC) 以提升基础设施管理的效率，但强调了其对周边开发任务的有限影响。研究方法包括构建一个 LLM 代理，并引入反馈循环，将生成的 IaC 代码中的错误和警告反馈给代理以进行迭代改进。结果发现，反馈循环的有效性在每次迭代中呈指数下降，最终趋于平稳并变得无效，这突显了 LLM 在持续优化任务中的局限性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "4 pages, submitted to accepted by International Journal of Secondary\n  Computing and Applications Research",
      "pdf_url": "http://arxiv.org/pdf/2411.19043v1",
      "published_date": "2024-11-28 10:40:55 UTC",
      "updated_date": "2024-11-28 10:40:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:25:40.806057"
    },
    {
      "arxiv_id": "2411.19039v1",
      "title": "Mars-PO: Multi-Agent Reasoning System Preference Optimization",
      "title_zh": "Mars-PO：多智能体推理系统偏好优化",
      "authors": [
        "Xiaoxuan Lou",
        "Chaojie Wang",
        "Bo An"
      ],
      "abstract": "Mathematical reasoning is a fundamental capability for large language models\n(LLMs), yet achieving high performance in this domain remains a significant\nchallenge. The auto-regressive generation process often makes LLMs susceptible\nto errors, hallucinations, and inconsistencies, particularly during multi-step\nreasoning. In this paper, we propose Mars-PO, a novel framework to improve the\nmathematical reasoning capabilities of LLMs through a multi-agent system. It\ncombines high-quality outputs from multiple agents into a hybrid positive\nsample set and pairs them with agent-specific negative samples to construct\nrobust preference pairs for training. By aligning agents with shared positive\nsamples while addressing individual weaknesses, Mars-PO achieves substantial\nperformance improvements on mathematical reasoning benchmarks. For example, it\nincreases the accuracy on the MATH benchmark of the state-of-the-art\ninstruction-tuned LLM, Llama3.1-8B-Instruct, from 50.38% to 57.82%.\nExperimental results further demonstrate that our method consistently\noutperforms other baselines, such as supervised fine-tuning, vanilla DPO, and\nits enhanced versions, highlighting the effectiveness of our approach.",
      "tldr_zh": "该论文提出 Mars-PO 框架，一种多智能体系统，用于优化大型语言模型 (LLMs) 在数学推理中的性能，通过将多个代理的高质量输出整合成混合正样本集，并与代理特定的负样本配对，形成偏好对进行训练。\n这种方法让代理共享正样本以增强整体能力，同时针对个体弱点进行优化，从而减少自回归生成过程中的错误和不一致性。\n实验结果显示，Mars-PO 将 Llama3.1-8B-Instruct 在 MATH 基准上的准确率从 50.38% 提高到 57.82%，并在多项测试中优于基线方法如监督细调 (SFT) 和 DPO。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19039v1",
      "published_date": "2024-11-28 10:35:16 UTC",
      "updated_date": "2024-11-28 10:35:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:25:53.791269"
    },
    {
      "arxiv_id": "2411.19027v1",
      "title": "Enhancing Neural Network Robustness Against Fault Injection Through Non-linear Weight Transformations",
      "title_zh": "通过非线性权重变换增强神经网络对故障注入的鲁棒性",
      "authors": [
        "Ninnart Fuengfusin",
        "Hakaru Tamukoh"
      ],
      "abstract": "Deploying deep neural networks (DNNs) in real-world environments poses\nchallenges due to faults that can manifest in physical hardware from radiation,\naging, and temperature fluctuations. To address this, previous works have\nfocused on protecting DNNs via activation range restriction using clipped ReLU\nand finding the optimal clipping threshold. However, this work instead focuses\non constraining DNN weights by applying saturated activation functions (SAFs):\nTanh, Arctan, and others. SAFs prevent faults from causing DNN weights to\nbecome excessively large, which can lead to model failure. These methods not\nonly enhance the robustness of DNNs against fault injections but also improve\nDNN performance by a small margin. Before deployment, DNNs are trained with\nweights constrained by SAFs. During deployment, the weights without applied SAF\nare written to mediums with faults. When read, weights with faults are applied\nwith SAFs and are used for inference. We demonstrate our proposed method across\nthree datasets (CIFAR10, CIFAR100, ImageNet 2012) and across three datatypes\n(32-bit floating point (FP32), 16-bit floating point, and 8-bit fixed point).\nWe show that our method enables FP32 ResNet18 with ImageNet 2012 to operate at\na bit-error rate of 0.00001 with minor accuracy loss, while without the\nproposed method, the FP32 DNN only produces random guesses. Furthermore, to\naccelerate the training process, we demonstrate that an ImageNet 2012\npre-trained ResNet18 can be adapted to SAF by training for a few epochs with a\nslight improvement in Top-1 accuracy while still ensuring robustness against\nfault injection.",
      "tldr_zh": "该研究提出了一种通过非线性权重变换增强深度神经网络 (DNNs) 对故障注入鲁棒性的方法，具体使用饱和激活函数 (SAFs) 如 Tanh 和 Arctan 来约束权重，防止故障导致权重过大引发模型失败。不同于以往的激活范围限制方法，该方法在训练时应用 SAFs 约束权重，并在部署时读取故障权重后重新应用 SAFs 进行推理，从而提高模型的鲁棒性和性能。实验在 CIFAR10、CIFAR100 和 ImageNet 2012 数据集上进行，涵盖 FP32、16-bit FP 和 8-bit fixed point 数据类型，结果显示 FP32 ResNet18 在 bit-error rate 为 0.00001 时保持较小准确率损失，而未使用该方法时模型仅输出随机猜测。此外，通过少量训练轮次即可适应预训练模型，实现鲁棒性提升的同时略微改善 Top-1 准确率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.19027v1",
      "published_date": "2024-11-28 10:01:39 UTC",
      "updated_date": "2024-11-28 10:01:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:26:05.776919"
    },
    {
      "arxiv_id": "2412.00121v1",
      "title": "Hybrid Discriminative Attribute-Object Embedding Network for Compositional Zero-Shot Learning",
      "title_zh": "用于组合式零样本学习的混合判别性属性-对象嵌入网络",
      "authors": [
        "Yang Liu",
        "Xinshuo Wang",
        "Jiale Du",
        "Xinbo Gao",
        "Jungong Han"
      ],
      "abstract": "Compositional Zero-Shot Learning (CZSL) recognizes new combinations by\nlearning from known attribute-object pairs. However, the main challenge of this\ntask lies in the complex interactions between attributes and object visual\nrepresentations, which lead to significant differences in images. In addition,\nthe long-tail label distribution in the real world makes the recognition task\nmore complicated. To address these problems, we propose a novel method, named\nHybrid Discriminative Attribute-Object Embedding (HDA-OE) network. To increase\nthe variability of training data, HDA-OE introduces an attribute-driven data\nsynthesis (ADDS) module. ADDS generates new samples with diverse attribute\nlabels by combining multiple attributes of the same object. By expanding the\nattribute space in the dataset, the model is encouraged to learn and\ndistinguish subtle differences between attributes. To further improve the\ndiscriminative ability of the model, HDA-OE introduces the subclass-driven\ndiscriminative embedding (SDDE) module, which enhances the subclass\ndiscriminative ability of the encoding by embedding subclass information in a\nfine-grained manner, helping to capture the complex dependencies between\nattributes and object visual features. The proposed model has been evaluated on\nthree benchmark datasets, and the results verify its effectiveness and\nreliability.",
      "tldr_zh": "本研究针对Compositional Zero-Shot Learning (CZSL)中的挑战，如属性和对象视觉表示的复杂交互以及长尾标签分布问题，提出了一种新型Hybrid Discriminative Attribute-Object Embedding (HDA-OE)网络。HDA-OE包括Attribute-Driven Data Synthesis (ADDS)模块，通过结合同一对象的多个属性生成多样化样本，以扩展属性空间并帮助模型区分细微差异；以及Subclass-Driven Discriminative Embedding (SDDE)模块，通过细粒度嵌入子类信息提升模型的区分能力，捕捉属性与对象视觉特征的复杂依赖。在三个基准数据集上的实验结果验证了该方法的有效性和可靠性，显著提高了CZSL的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00121v1",
      "published_date": "2024-11-28 09:50:25 UTC",
      "updated_date": "2024-11-28 09:50:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:26:16.642929"
    },
    {
      "arxiv_id": "2412.02713v2",
      "title": "Applying IRT to Distinguish Between Human and Generative AI Responses to Multiple-Choice Assessments",
      "title_zh": "使用项目反应理论区分人类和生成式人工智能对多项选择评估的响应",
      "authors": [
        "Alona Strugatski",
        "Giora Alexandron"
      ],
      "abstract": "Generative AI is transforming the educational landscape, raising significant\nconcerns about cheating. Despite the widespread use of multiple-choice\nquestions in assessments, the detection of AI cheating in MCQ-based tests has\nbeen almost unexplored, in contrast to the focus on detecting AI-cheating on\ntext-rich student outputs. In this paper, we propose a method based on the\napplication of Item Response Theory to address this gap. Our approach operates\non the assumption that artificial and human intelligence exhibit different\nresponse patterns, with AI cheating manifesting as deviations from the expected\npatterns of human responses. These deviations are modeled using Person-Fit\nStatistics. We demonstrate that this method effectively highlights the\ndifferences between human responses and those generated by premium versions of\nleading chatbots (ChatGPT, Claude, and Gemini), but that it is also sensitive\nto the amount of AI cheating in the data. Furthermore, we show that the\nchatbots differ in their reasoning profiles. Our work provides both a\ntheoretical foundation and empirical evidence for the application of IRT to\nidentify AI cheating in MCQ-based assessments.",
      "tldr_zh": "本论文提出了一种基于 Item Response Theory (IRT) 的方法，用于区分人类和生成式 AI 在多选题评估中的响应，旨在解决教育领域 AI 作弊问题。\n该方法假设人类和 AI 的响应模式存在偏差，并通过 Person-Fit Statistics 建模这些差异，以检测 AI 生成的答案。\n实验结果表明，该方法能有效识别顶级聊天机器人（如 ChatGPT、Claude 和 Gemini）的响应，但也受 AI 作弊量影响，且这些机器人显示出不同的推理模式。\n这项工作为在多选题评估中识别 AI 作弊提供了理论基础和实证证据。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "PRE-PRINT VERSION Accepted to The 15th International Learning\n  Analytics and Knowledge Conference (LAK25)",
      "pdf_url": "http://arxiv.org/pdf/2412.02713v2",
      "published_date": "2024-11-28 09:43:06 UTC",
      "updated_date": "2024-12-12 13:28:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:26:28.270028"
    },
    {
      "arxiv_id": "2412.00120v1",
      "title": "Relation-Aware Meta-Learning for Zero-shot Sketch-Based Image Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Liu",
        "Jiale Du",
        "Xinbo Gao",
        "Jungong Han"
      ],
      "abstract": "Sketch-based image retrieval (SBIR) relies on free-hand sketches to retrieve\nnatural photos within the same class. However, its practical application is\nlimited by its inability to retrieve classes absent from the training set. To\naddress this limitation, the task has evolved into Zero-Shot Sketch-Based Image\nRetrieval (ZS-SBIR), where model performance is evaluated on unseen categories.\nTraditional SBIR primarily focuses on narrowing the domain gap between photo\nand sketch modalities. However, in the zero-shot setting, the model not only\nneeds to address this cross-modal discrepancy but also requires a strong\ngeneralization capability to transfer knowledge to unseen categories. To this\nend, we propose a novel framework for ZS-SBIR that employs a pair-based\nrelation-aware quadruplet loss to bridge feature gaps. By incorporating two\nnegative samples from different modalities, the approach prevents positive\nfeatures from becoming disproportionately distant from one modality while\nremaining close to another, thus enhancing inter-class separability. We also\npropose a Relation-Aware Meta-Learning Network (RAMLN) to obtain the margin, a\nhyper-parameter of cross-modal quadruplet loss, to improve the generalization\nability of the model. RAMLN leverages external memory to store feature\ninformation, which it utilizes to assign optimal margin values. Experimental\nresults obtained on the extended Sketchy and TU-Berlin datasets show a sharp\nimprovement over existing state-of-the-art methods in ZS-SBIR.",
      "tldr_zh": "本论文针对 Zero-shot Sketch-Based Image Retrieval (ZS-SBIR) 的挑战，提出了一种新框架，以解决模型在未见类别上处理照片和草图模态差距的问题，同时提升泛化能力。框架采用基于关系的四元组损失（pair-based relation-aware quadruplet loss），通过引入两个不同模态的负样本来增强类间可分性，避免正样本特征在模态间距离不当。论文还引入 Relation-Aware Meta-Learning Network (RAMLN)，利用外部记忆存储特征信息来动态优化损失函数的超参数 margin，以提高模型的泛化性能。在扩展的 Sketchy 和 TU-Berlin 数据集上的实验结果显示，该方法显著优于现有最先进技术，证明了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00120v1",
      "published_date": "2024-11-28 09:35:27 UTC",
      "updated_date": "2024-11-28 09:35:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:26:41.452603"
    },
    {
      "arxiv_id": "2411.19000v3",
      "title": "An AI-driven multimodal smart home platform for continuous monitoring and intelligent assistance in post-stroke patients",
      "title_zh": "AI驱动的多模态智能家居平台，用于中风后",
      "authors": [
        "Chenyu Tang",
        "Ruizhi Zhang",
        "Shuo Gao",
        "Zihe Zhao",
        "Zibo Zhang",
        "Jiaqi Wang",
        "Cong Li",
        "Junliang Chen",
        "Yanning Dai",
        "Shengbo Wang",
        "Ruoyu Juan",
        "Qiaoying Li",
        "Ruimou Xie",
        "Xuhang Chen",
        "Xinkai Zhou",
        "Yunjia Xia",
        "Jianan Chen",
        "Fanghao Lu",
        "Xin Li",
        "Ninglli Wang",
        "Peter Smielewski",
        "Yu Pan",
        "Hubin Zhao",
        "Luigi G. Occhipinti"
      ],
      "abstract": "At-home rehabilitation for post-stroke patients presents significant\nchallenges, as continuous, personalized care is often limited outside clinical\nsettings. Additionally, the absence of comprehensive solutions addressing\ndiverse monitoring and assistance needs in home environments complicates\nrecovery efforts. Here, we present a multimodal smart home platform designed\nfor continuous, at-home rehabilitation of post-stroke patients, integrating\nwearable sensing, ambient monitoring, and adaptive automation. A plantar\npressure insole equipped with a machine learning pipeline classifies users into\nmotor recovery stages with up to 94% accuracy, enabling quantitative tracking\nof walking patterns. A head-mounted eye-tracking module supports cognitive\nassessments and hands-free control of household devices, while ambient sensors\nensure sub-second response times for interaction. These data streams are fused\nlocally via a hierarchical Internet of Things (IoT) architecture, protecting\nprivacy and minimizing latency. An embedded large language model (LLM) agent,\nAuto-Care, continuously interprets multimodal data to provide real-time\ninterventions-issuing personalized reminders, adjusting environmental\nconditions, and notifying caregivers. Implemented in a post-stroke context,\nthis integrated smart home platform increases overall user satisfaction by an\naverage of 115% (p<0.01) compared to traditional home environment. Beyond\nstroke, the system offers a scalable framework for patient-centered, long-term\ncare in broader neurorehabilitation and aging-in-place applications.",
      "tldr_zh": "本文提出一个AI驱动的多模态智能家居平台，用于中风后患者的持续家庭康复，提供可穿戴传感器（如足压内底和眼动追踪模块）、环境监测以及IoT架构的整合，以实现实时数据融合和隐私保护。足压内底通过机器学习管道将用户运动恢复阶段分类准确率高达94%，而嵌入式LLM代理Auto-Care则解读多模态数据，提供个性化提醒、环境调整和护理通知。实验结果显示，该平台在实际应用中将用户满意度平均提高115%（p<0.01），并为更广泛的神经康复和养老场景提供可扩展框架。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.HC",
      "comment": "5 figures, 41 references",
      "pdf_url": "http://arxiv.org/pdf/2411.19000v3",
      "published_date": "2024-11-28 09:04:39 UTC",
      "updated_date": "2025-04-15 14:35:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:26:52.864513"
    },
    {
      "arxiv_id": "2411.18997v1",
      "title": "GRU-PFG: Extract Inter-Stock Correlation from Stock Factors with Graph Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Yonggai Zhuang",
        "Haoran Chen",
        "Kequan Wang",
        "Teng Fei"
      ],
      "abstract": "The complexity of stocks and industries presents challenges for stock\nprediction. Currently, stock prediction models can be divided into two\ncategories. One category, represented by GRU and ALSTM, relies solely on stock\nfactors for prediction, with limited effectiveness. The other category,\nrepresented by HIST and TRA, incorporates not only stock factors but also\nindustry information, industry financial reports, public sentiment, and other\ninputs for prediction. The second category of models can capture correlations\nbetween stocks by introducing additional information, but the extra data is\ndifficult to standardize and generalize. Considering the current state and\nlimitations of these two types of models, this paper proposes the GRU-PFG\n(Project Factors into Graph) model. This model only takes stock factors as\ninput and extracts inter-stock correlations using graph neural networks. It\nachieves prediction results that not only outperform the others models relies\nsolely on stock factors, but also achieve comparable performance to the second\ncategory models. The experimental results show that on the CSI300 dataset, the\nIC of GRU-PFG is 0.134, outperforming HIST's 0.131 and significantly surpassing\nGRU and Transformer, achieving results better than the second category models.\nMoreover as a model that relies solely on stock factors, it has greater\npotential for generalization.",
      "tldr_zh": "该论文提出GRU-PFG模型，通过Graph Neural Network从股票因素中提取股票间相关性，旨在解决现有股票预测模型的局限性。不同于仅依赖股票因素的模型（如GRU和ALSTM）或引入额外数据的模型（如HIST和TRA），GRU-PFG仅使用股票因素作为输入，利用图神经网络构建股票关联图，实现更有效的预测。实验结果显示，在CSI300数据集上，GRU-PFG的IC指标达到0.134，优于HIST的0.131并显著超过GRU和Transformer，甚至与第二类模型相当，同时具备更好的泛化潜力。",
      "categories": [
        "q-fin.CP",
        "cs.AI"
      ],
      "primary_category": "q-fin.CP",
      "comment": "17pages",
      "pdf_url": "http://arxiv.org/pdf/2411.18997v1",
      "published_date": "2024-11-28 08:50:55 UTC",
      "updated_date": "2024-11-28 08:50:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:27:04.599913"
    },
    {
      "arxiv_id": "2411.18993v1",
      "title": "Harden Deep Neural Networks Against Fault Injections Through Weight Scaling",
      "title_zh": "翻译失败",
      "authors": [
        "Ninnart Fuengfusin",
        "Hakaru Tamukoh"
      ],
      "abstract": "Deep neural networks (DNNs) have enabled smart applications on hardware\ndevices. However, these hardware devices are vulnerable to unintended faults\ncaused by aging, temperature variance, and write errors. These faults can cause\nbit-flips in DNN weights and significantly degrade the performance of DNNs.\nThus, protection against these faults is crucial for the deployment of DNNs in\ncritical applications. Previous works have proposed error correction codes\nbased methods, however these methods often require high overheads in both\nmemory and computation. In this paper, we propose a simple yet effective method\nto harden DNN weights by multiplying weights by constants before storing them\nto fault-prone medium. When used, these weights are divided back by the same\nconstants to restore the original scale. Our method is based on the observation\nthat errors from bit-flips have properties similar to additive noise, therefore\nby dividing by constants can reduce the absolute error from bit-flips. To\ndemonstrate our method, we conduct experiments across four ImageNet 2012\npre-trained models along with three different data types: 32-bit floating\npoint, 16-bit floating point, and 8-bit fixed point. This method demonstrates\nthat by only multiplying weights with constants, Top-1 Accuracy of 8-bit fixed\npoint ResNet50 is improved by 54.418 at bit-error rate of 0.0001.",
      "tldr_zh": "本论文针对深度神经网络(DNNs)面临的硬件故障注入问题，如位翻转(bit-flips)导致的权重错误，提出了一种简单有效的权重缩放方法：在存储权重前乘以常量，并在使用时除以常量，以减少错误的影响。该方法基于位翻转错误类似于加性噪声的观察，避免了传统错误修正代码方法的高内存和计算开销。实验在ImageNet 2012的四种预训练模型和多种数据类型（如32-bit floating point、16-bit floating point和8-bit fixed point）上进行，结果显示，8-bit fixed point ResNet50在bit-error rate为0.0001时的Top-1 Accuracy提高了54.418%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.18993v1",
      "published_date": "2024-11-28 08:47:23 UTC",
      "updated_date": "2024-11-28 08:47:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:27:17.083959"
    },
    {
      "arxiv_id": "2412.03592v1",
      "title": "Using Images to Find Context-Independent Word Representations in Vector Space",
      "title_zh": "翻译失败",
      "authors": [
        "Harsh Kumar"
      ],
      "abstract": "Many methods have been proposed to find vector representation for words, but\nmost rely on capturing context from the text to find semantic relationships\nbetween these vectors. We propose a novel method of using dictionary meanings\nand image depictions to find word vectors independent of any context. We use\nauto-encoder on the word images to find meaningful representations and use them\nto calculate the word vectors. We finally evaluate our method on word\nsimilarity, concept categorization and outlier detection tasks. Our method\nperforms comparably to context-based methods while taking much less training\ntime.",
      "tldr_zh": "本文提出了一种新方法，使用图像描绘和字典含义来获取独立于上下文的词向量表示，避免依赖文本语境。方法涉及应用auto-encoder处理词图像，提取有意义的表示并计算词向量。在词相似性、概念分类和异常检测任务上，该方法与基于上下文的传统方法性能相当，但训练时间显著缩短。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03592v1",
      "published_date": "2024-11-28 08:44:10 UTC",
      "updated_date": "2024-11-28 08:44:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:27:27.145844"
    },
    {
      "arxiv_id": "2411.18990v1",
      "title": "USTCCTSU at SemEval-2024 Task 1: Reducing Anisotropy for Cross-lingual Semantic Textual Relatedness Task",
      "title_zh": "翻译失败",
      "authors": [
        "Jianjian Li",
        "Shengwei Liang",
        "Yong Liao",
        "Hongping Deng",
        "Haiyang Yu"
      ],
      "abstract": "Cross-lingual semantic textual relatedness task is an important research task\nthat addresses challenges in cross-lingual communication and text\nunderstanding. It helps establish semantic connections between different\nlanguages, crucial for downstream tasks like machine translation, multilingual\ninformation retrieval, and cross-lingual text understanding.Based on extensive\ncomparative experiments, we choose the XLM-R-base as our base model and use\npre-trained sentence representations based on whitening to reduce\nanisotropy.Additionally, for the given training data, we design a delicate data\nfiltering method to alleviate the curse of multilingualism. With our approach,\nwe achieve a 2nd score in Spanish, a 3rd in Indonesian, and multiple entries in\nthe top ten results in the competition's track C. We further do a comprehensive\nanalysis to inspire future research aimed at improving performance on\ncross-lingual tasks.",
      "tldr_zh": "该论文针对 SemEval-2024 Task 1 的跨语言语义文本相关性（Cross-lingual Semantic Textual Relatedness）任务，提出了一种减少 anisotropy 的方法，以提升跨语言通信和文本理解。研究者选用 XLM-R-base 作为基础模型，并结合基于 whitening 的预训练句子表示，以及一个精细的数据过滤策略来缓解 curse of multilingualism。实验结果显示，该方法在 Track C 中获得西班牙语的第二名、印度尼西亚语的第三名，以及多个前十名成绩，并通过全面分析为未来跨语言任务研究提供启发。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.18990v1",
      "published_date": "2024-11-28 08:40:14 UTC",
      "updated_date": "2024-11-28 08:40:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:27:40.299091"
    },
    {
      "arxiv_id": "2412.00117v1",
      "title": "Proceedings of the 2024 XCSP3 Competition",
      "title_zh": "翻译失败",
      "authors": [
        "Gilles Audemard",
        "Christophe Lecoutre",
        "Emmanuel Lonca"
      ],
      "abstract": "This document represents the proceedings of the 2024 XCSP3 Competition. The\nresults of this competition of constraint solvers were presented at CP'24 (30th\nInternational Conference on Principles and Practice of Constraint Programming).",
      "tldr_zh": "本文件是2024 XCSP3 Competition的会议记录，聚焦于约束求解器（constraint solvers）的比赛结果。这些结果在CP'24（第30届约束编程原理与实践国际会议）上公布，为约束编程领域的进展提供了宝贵参考和基准。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "104 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.00117v1",
      "published_date": "2024-11-28 08:16:40 UTC",
      "updated_date": "2024-11-28 08:16:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:27:51.467053"
    },
    {
      "arxiv_id": "2411.18956v1",
      "title": "Random Sampling for Diffusion-based Adversarial Purification",
      "title_zh": "翻译失败",
      "authors": [
        "Jiancheng Zhang",
        "Peiran Dong",
        "Yongyong Chen",
        "Yin-Ping Zhao",
        "Song Guo"
      ],
      "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have gained great attention\nin adversarial purification. Current diffusion-based works focus on designing\neffective condition-guided mechanisms while ignoring a fundamental problem,\ni.e., the original DDPM sampling is intended for stable generation, which may\nnot be the optimal solution for adversarial purification. Inspired by the\nstability of the Denoising Diffusion Implicit Model (DDIM), we propose an\nopposite sampling scheme called random sampling. In brief, random sampling will\nsample from a random noisy space during each diffusion process, while DDPM and\nDDIM sampling will continuously sample from the adjacent or original noisy\nspace. Thus, random sampling obtains more randomness and achieves stronger\nrobustness against adversarial attacks. Correspondingly, we also introduce a\nnovel mediator conditional guidance to guarantee the consistency of the\nprediction under the purified image and clean image input. To expand awareness\nof guided diffusion purification, we conduct a detailed evaluation with\ndifferent sampling methods and our random sampling achieves an impressive\nimprovement in multiple settings. Leveraging mediator-guided random sampling,\nwe also establish a baseline method named DiffAP, which significantly\noutperforms state-of-the-art (SOTA) approaches in performance and defensive\nstability. Remarkably, under strong attack, our DiffAP even achieves a more\nthan 20% robustness advantage with 10$\\times$ sampling acceleration.",
      "tldr_zh": "本研究针对去噪扩散概率模型 (DDPMs) 在对抗性净化 (adversarial purification) 中的采样问题，提出了一种新的 random sampling 方案，以提升模型的随机性和鲁棒性。该方案通过在每个扩散过程中从随机噪声空间采样，相比传统 DDPM 和 DDIM 采样方式，显著提高了对对抗攻击的抵抗力；同时，引入 mediator conditional guidance 机制，确保净化图像与干净图像预测的一致性。实验结果显示，random sampling 在多种设置下表现出色，而基于此的基线方法 DiffAP 超越了现有最先进 (SOTA) 技术，在强攻击下实现了超过 20% 的鲁棒性优势，并将采样速度加速 10 倍。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.18956v1",
      "published_date": "2024-11-28 07:04:09 UTC",
      "updated_date": "2024-11-28 07:04:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:28:04.332656"
    },
    {
      "arxiv_id": "2412.07793v1",
      "title": "Publication Trends in Artificial Intelligence Conferences: The Rise of Super Prolific Authors",
      "title_zh": "翻译失败",
      "authors": [
        "Ariful Azad",
        "Afeefa Banu"
      ],
      "abstract": "Papers published in top conferences contribute influential discoveries that\nare reshaping the landscape of modern Artificial Intelligence (AI). We analyzed\n87,137 papers from 11 AI conferences to examine publication trends over the\npast decade. Our findings reveal a consistent increase in both the number of\npapers and authors, reflecting the growing interest in AI research. We also\nobserved a rise in prolific researchers who publish dozens of papers at the\nsame conference each year. In light of this analysis, the AI research community\nshould consider revisiting authorship policies, addressing equity concerns, and\nevaluating the workload of junior researchers to foster a more sustainable and\ninclusive research environment.",
      "tldr_zh": "这篇论文分析了过去十年11个顶尖AI会议的87,137篇论文，揭示了论文和作者数量的持续增长，这反映了AI研究兴趣的快速上升。研究特别关注“super prolific authors”的兴起，这些作者每年在同一会议上发表数十篇论文。作者建议AI研究社区重新审视作者政策、解决公平性问题，并评估初级研究者的工作量，以促进一个更可持续和包容性的研究环境。",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.07793v1",
      "published_date": "2024-11-28 06:56:49 UTC",
      "updated_date": "2024-11-28 06:56:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:28:15.638024"
    },
    {
      "arxiv_id": "2411.18954v2",
      "title": "NeuroLifting: Neural Inference on Markov Random Fields at Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Yaomin Wang",
        "Chaolong Ying",
        "Xiaodong Luo",
        "Tianshu Yu"
      ],
      "abstract": "Inference in large-scale Markov Random Fields (MRFs) is a critical yet\nchallenging task, traditionally approached through approximate methods like\nbelief propagation and mean field, or exact methods such as the Toulbar2\nsolver. These strategies often fail to strike an optimal balance between\nefficiency and solution quality, particularly as the problem scale increases.\nThis paper introduces NeuroLifting, a novel technique that leverages Graph\nNeural Networks (GNNs) to reparameterize decision variables in MRFs,\nfacilitating the use of standard gradient descent optimization. By extending\ntraditional lifting techniques into a non-parametric neural network framework,\nNeuroLifting benefits from the smooth loss landscape of neural networks,\nenabling efficient and parallelizable optimization. Empirical results\ndemonstrate that, on moderate scales, NeuroLifting performs very close to the\nexact solver Toulbar2 in terms of solution quality, significantly surpassing\nexisting approximate methods. Notably, on large-scale MRFs, NeuroLifting\ndelivers superior solution quality against all baselines, as well as exhibiting\nlinear computational complexity growth. This work presents a significant\nadvancement in MRF inference, offering a scalable and effective solution for\nlarge-scale problems.",
      "tldr_zh": "这篇论文介绍了 NeuroLifting，一种创新方法，利用 Graph Neural Networks (GNNs) 重新参数化 Markov Random Fields (MRFs) 中的决策变量，并结合标准梯度下降优化，以解决大规模 MRF 推理的效率和质量平衡问题。NeuroLifting 将传统的 lifting 技术扩展到非参数神经网络框架，利用神经网络的光滑损失景观实现高效且可并行化的优化。实验结果显示，在中等规模问题上，其解决方案质量接近精确求解器 Toulbar2，并显著优于现有近似方法如 belief propagation 和 mean field；在大规模 MRFs 上，NeuroLifting 提供最佳的解决方案质量，同时表现出线性的计算复杂度增长。该工作为大规模 MRF 推理提供了可扩展且有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.18954v2",
      "published_date": "2024-11-28 06:50:47 UTC",
      "updated_date": "2025-05-16 08:18:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:30:22.192087"
    },
    {
      "arxiv_id": "2411.18948v3",
      "title": "RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Xue Tan",
        "Hao Luan",
        "Mingyu Luo",
        "Xiaoyan Sun",
        "Ping Chen",
        "Jun Dai"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enriches the input to LLMs by retrieving\ninformation from the relevant knowledge database, enabling them to produce\nresponses that are more accurate and contextually appropriate. It is worth\nnoting that the knowledge database, being sourced from publicly available\nchannels such as Wikipedia, inevitably introduces a new attack surface. RAG\npoisoning involves injecting malicious texts into the knowledge database,\nultimately leading to the generation of the attacker's target response (also\ncalled poisoned response). However, there are currently limited methods\navailable for detecting such poisoning attacks. We aim to bridge the gap in\nthis work. Particularly, we introduce RevPRAG, a flexible and automated\ndetection pipeline that leverages the activations of LLMs for poisoned response\ndetection. Our investigation uncovers distinct patterns in LLMs' activations\nwhen generating correct responses versus poisoned responses. Our results on\nmultiple benchmark datasets and RAG architectures show our approach could\nachieve 98% true positive rate, while maintaining false positive rates close to\n1%.",
      "tldr_zh": "Retrieval-Augmented Generation (RAG) 通过从知识数据库检索信息来提升大型语言模型 (LLMs) 的响应准确性，但其公开来源易受 poisoning attacks 影响，导致生成恶意投毒响应。RevPRAG 是一种灵活的自动化检测管道，利用 LLMs 的 activations 分析来识别这些攻击，通过发现正确响应与投毒响应间的激活模式差异。实验在多个基准数据集和 RAG 架构上显示，RevPRAG 实现了 98% 的 true positive rate，同时将 false positive rates 控制在 1% 左右，从而有效桥接了投毒攻击检测的空白。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.18948v3",
      "published_date": "2024-11-28 06:29:46 UTC",
      "updated_date": "2025-04-27 11:15:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:28:40.780173"
    },
    {
      "arxiv_id": "2412.01849v2",
      "title": "Towards Data-centric Machine Learning on Directed Graphs: a Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Henan Sun",
        "Xunkai Li",
        "Daohan Su",
        "Junyi Han",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "abstract": "In recent years, Graph Neural Networks (GNNs) have made significant advances\nin processing structured data. However, most of them primarily adopted a\nmodel-centric approach, which simplifies graphs by converting them into\nundirected formats and emphasizes model designs. This approach is inherently\nlimited in real-world applications due to the unavoidable information loss in\nsimple undirected graphs and the model optimization challenges that arise when\nexceeding the upper bounds of this sub-optimal data representational capacity.\nAs a result, there has been a shift toward data-centric methods that prioritize\nimproving graph quality and representation. Specifically, various types of\ngraphs can be derived from naturally structured data, including heterogeneous\ngraphs, hypergraphs, and directed graphs. Among these, directed graphs offer\ndistinct advantages in topological systems by modeling causal relationships,\nand directed GNNs have been extensively studied in recent years. However, a\ncomprehensive survey of this emerging topic is still lacking. Therefore, we aim\nto provide a comprehensive review of directed graph learning, with a particular\nfocus on a data-centric perspective. Specifically, we first introduce a novel\ntaxonomy for existing studies. Subsequently, we re-examine these methods from\nthe data-centric perspective, with an emphasis on understanding and improving\ndata representation. It demonstrates that a deep understanding of directed\ngraphs and their quality plays a crucial role in model performance.\nAdditionally, we explore the diverse applications of directed GNNs across 10+\ndomains, highlighting their broad applicability. Finally, we identify key\nopportunities and challenges within the field, offering insights that can guide\nfuture research and development in directed graph learning.",
      "tldr_zh": "这篇论文综述了有向图（directed graphs）上的数据中心机器学习方法，强调从传统模型中心转向数据中心，以解决Graph Neural Networks (GNNs) 在处理有向图时因信息损失和优化挑战而带来的局限性。作者引入了一个新分类法（taxonomy），并从数据中心视角重新审视现有研究，突出改善数据表示和理解有向图质量对模型性能的关键作用。论文还探讨了有向GNNs在10多个领域的广泛应用，并识别了未来研究的机会和挑战，为该领域的发展提供指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "In Progress",
      "pdf_url": "http://arxiv.org/pdf/2412.01849v2",
      "published_date": "2024-11-28 06:09:12 UTC",
      "updated_date": "2024-12-11 08:28:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:28:52.685776"
    },
    {
      "arxiv_id": "2412.00114v2",
      "title": "SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Cao",
        "Yun Xing",
        "Jie Zhang",
        "Di Lin",
        "Tianwei Zhang",
        "Ivor Tsang",
        "Yang Liu",
        "Qing Guo"
      ],
      "abstract": "Large vision-language models (LVLMs) have shown remarkable capabilities in\ninterpreting visual content. While existing works demonstrate these models'\nvulnerability to deliberately placed adversarial texts, such texts are often\neasily identifiable as anomalous. In this paper, we present the first approach\nto generate scene-coherent typographic adversarial attacks that mislead\nadvanced LVLMs while maintaining visual naturalness through the capability of\nthe LLM-based agent. Our approach addresses three critical questions: what\nadversarial text to generate, where to place it within the scene, and how to\nintegrate it seamlessly. We propose a training-free, multi-modal LLM-driven\nscene-coherent typographic adversarial planning (SceneTAP) that employs a\nthree-stage process: scene understanding, adversarial planning, and seamless\nintegration. The SceneTAP utilizes chain-of-thought reasoning to comprehend the\nscene, formulate effective adversarial text, strategically plan its placement,\nand provide detailed instructions for natural integration within the image.\nThis is followed by a scene-coherent TextDiffuser that executes the attack\nusing a local diffusion mechanism. We extend our method to real-world scenarios\nby printing and placing generated patches in physical environments,\ndemonstrating its practical implications. Extensive experiments show that our\nscene-coherent adversarial text successfully misleads state-of-the-art LVLMs,\nincluding ChatGPT-4o, even after capturing new images of physical setups. Our\nevaluations demonstrate a significant increase in attack success rates while\nmaintaining visual naturalness and contextual appropriateness. This work\nhighlights vulnerabilities in current vision-language models to sophisticated,\nscene-coherent adversarial attacks and provides insights into potential defense\nmechanisms.",
      "tldr_zh": "本文提出 SceneTAP，一种基于多模态 LLM 的训练-free 框架，用于生成与场景相符的排版对抗攻击，旨在误导大型视觉语言模型 (LVLMs) 而保持视觉自然性。框架通过 chain-of-thought reasoning 的三阶段过程——场景理解、对抗规划和无缝整合——来制定有效的对抗文本、战略性放置位置，并利用 scene-coherent TextDiffuser 执行攻击。实验结果显示，SceneTAP 在真实环境中显著提高了攻击成功率（如对 ChatGPT-4o 的有效性），同时突显了 LVLMs 的漏洞，并为潜在防御机制提供了洞见。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00114v2",
      "published_date": "2024-11-28 05:55:13 UTC",
      "updated_date": "2025-04-08 02:54:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:29:04.921918"
    },
    {
      "arxiv_id": "2411.18932v1",
      "title": "ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Rao Fu",
        "Ziyang Luo",
        "Hongzhan Lin",
        "Zhen Ye",
        "Jing Ma"
      ],
      "abstract": "Recent advancements in large multimodal models (LMMs) have showcased\nimpressive code generation capabilities, primarily evaluated through\nimage-to-code benchmarks. However, these benchmarks are limited to specific\nvisual programming scenarios where the logic reasoning and the multimodal\nunderstanding capacities are split apart. To fill this gap, we propose\nScratchEval, a novel benchmark designed to evaluate the visual programming\nreasoning ability of LMMs. ScratchEval is based on Scratch, a block-based\nvisual programming language widely used in children's programming education. By\nintegrating visual elements and embedded programming logic, ScratchEval\nrequires the model to process both visual information and code structure,\nthereby comprehensively evaluating its programming intent understanding\nability. Our evaluation approach goes beyond the traditional image-to-code\nmapping and focuses on unified logical thinking and problem-solving abilities,\nproviding a more comprehensive and challenging framework for evaluating the\nvisual programming ability of LMMs. ScratchEval not only fills the gap in\nexisting evaluation methods, but also provides new insights for the future\ndevelopment of LMMs in the field of visual programming. Our benchmark can be\naccessed at https://github.com/HKBUNLP/ScratchEval .",
      "tldr_zh": "这篇论文引入了ScratchEval，一个新型基准，用于评估大型多模态模型(LMMs)如GPT-4o的视觉编程推理能力。ScratchEval基于Scratch语言（一种面向儿童的块状视觉编程语言），通过整合视觉元素和编程逻辑，测试模型处理图像信息与代码结构的能力。相较于传统的image-to-code基准，该方法更注重模型的统一逻辑思考和问题解决能力，提供了一个更全面且具挑战性的评估框架。最终，ScratchEval填补了现有评估方法的空白，并为LMMs在视觉编程领域的未来发展带来新见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.18932v1",
      "published_date": "2024-11-28 05:51:45 UTC",
      "updated_date": "2024-11-28 05:51:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:29:17.865258"
    },
    {
      "arxiv_id": "2412.00113v1",
      "title": "Boundary-Decoder network for inverse prediction of capacitor electrostatic analysis",
      "title_zh": "Boundary-Decoder 网络用于电容器静电分析的逆向预测",
      "authors": [
        "Kart-Leong Lim",
        "Rahul Dutta",
        "Mihai Rotaru"
      ],
      "abstract": "Traditional electrostatic simulation are meshed-based methods which convert\npartial differential equations into an algebraic system of equations and their\nsolutions are approximated through numerical methods. These methods are time\nconsuming and any changes in their initial or boundary conditions will require\nsolving the numerical problem again. Newer computational methods such as the\nphysics informed neural net (PINN) similarly require re-training when boundary\nconditions changes. In this work, we propose an end-to-end deep learning\napproach to model parameter changes to the boundary conditions. The proposed\nmethod is demonstrated on the test problem of a long air-filled capacitor\nstructure. The proposed approach is compared to plain vanilla deep learning\n(NN) and PINN. It is shown that our method can significantly outperform both NN\nand PINN under dynamic boundary condition as well as retaining its full\ncapability as a forward model.",
      "tldr_zh": "传统静电模拟方法，如基于网格的数值方法和物理信息神经网络(PINN)，在边界条件变化时需重新计算或训练，效率低下。本文提出了一种端到端的深度学习方法，Boundary-Decoder network，用于逆向预测电容器的静电分析。该方法在长空气填充电容器结构的测试问题上，与普通神经网络(NN)和PINN相比，表现出显著优势，尤其在动态边界条件下，同时保持了前向模型的功能。这种创新方法为高效处理参数变化的静电模拟提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00113v1",
      "published_date": "2024-11-28 05:51:00 UTC",
      "updated_date": "2024-11-28 05:51:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:29:29.167306"
    },
    {
      "arxiv_id": "2411.18929v1",
      "title": "VIPaint: Image Inpainting with Pre-Trained Diffusion Models via Variational Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Sakshi Agarwal",
        "Gabe Hoope",
        "Erik B. Sudderth"
      ],
      "abstract": "Diffusion probabilistic models learn to remove noise that is artificially\nadded to the data during training. Novel data, like images, may then be\ngenerated from Gaussian noise through a sequence of denoising operations. While\nthis Markov process implicitly defines a joint distribution over noise-free\ndata, it is not simple to condition the generative process on masked or partial\nimages. A number of heuristic sampling procedures have been proposed for\nsolving inverse problems with diffusion priors, but these approaches do not\ndirectly approximate the true conditional distribution imposed by inference\nqueries, and are often ineffective for large masked regions. Moreover, many of\nthese baselines cannot be applied to latent diffusion models which use image\nencodings for efficiency. We instead develop a hierarchical variational\ninference algorithm that analytically marginalizes missing features, and uses a\nrigorous variational bound to optimize a non-Gaussian Markov approximation of\nthe true diffusion posterior. Through extensive experiments with both\npixel-based and latent diffusion models of images, we show that our VIPaint\nmethod significantly outperforms previous approaches in both the plausibility\nand diversity of imputations, and is easily generalized to other inverse\nproblems like deblurring and superresolution.",
      "tldr_zh": "该论文提出VIPaint方法，利用预训练diffusion models通过variational inference进行图像填充，解决了传统方法在条件生成上的局限性，如无法准确逼近真实条件分布或处理大掩码区域。VIPaint采用分层变分推理算法，分析性地边缘化缺失特征，并优化一个非高斯Markov近似，以生成更精确的后验分布。实验结果显示，该方法在像素和潜在diffusion models上显著优于现有基线，提高了图像填充的合理性和多样性，并可扩展到其他逆问题如去模糊和超分辨率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.18929v1",
      "published_date": "2024-11-28 05:35:36 UTC",
      "updated_date": "2024-11-28 05:35:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:29:40.656784"
    },
    {
      "arxiv_id": "2411.18923v2",
      "title": "EzSQL: An SQL intermediate representation for improving SQL-to-text Generation",
      "title_zh": "EzSQL：一种用于改进 SQL 到文本生成的 SQL 中间表示",
      "authors": [
        "Meher Bhardwaj",
        "Hrishikesh Ethari",
        "Dennis Singh Moirangthem"
      ],
      "abstract": "The SQL-to-text generation task traditionally uses template base, Seq2Seq,\ntree-to-sequence, and graph-to-sequence models. Recent models take advantage of\npre-trained generative language models for this task in the Seq2Seq framework.\nHowever, treating SQL as a sequence of inputs to the pre-trained models is not\noptimal. In this work, we put forward a new SQL intermediate representation\ncalled EzSQL to align SQL with the natural language text sequence. EzSQL\nsimplifies the SQL queries and brings them closer to natural language text by\nmodifying operators and keywords, which can usually be described in natural\nlanguage. EzSQL also removes the need for set operators. Our proposed\nSQL-to-text generation model uses EzSQL as the input to a pre-trained\ngenerative language model for generating the text descriptions. We demonstrate\nthat our model is an effective state-of-the-art method to generate text\nnarrations from SQL queries on the WikiSQL and Spider datasets. We also show\nthat by generating pretraining data using our SQL-to-text generation model, we\ncan enhance the performance of Text-to-SQL parsers.",
      "tldr_zh": "该论文提出了一种新的SQL中间表示EzSQL，以优化SQL-to-text生成任务。EzSQL通过修改运算符和关键词，将SQL查询简化并更接近自然语言文本，同时去除集合运算符，使其更适合作为预训练生成语言模型的输入。实验结果显示，使用EzSQL的模型在WikiSQL和Spider数据集上达到了state-of-the-art性能，并在生成预训练数据方面提升了Text-to-SQL解析器的表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under revision and review at Expert System With Applications Journal\n  after first review",
      "pdf_url": "http://arxiv.org/pdf/2411.18923v2",
      "published_date": "2024-11-28 05:24:46 UTC",
      "updated_date": "2025-04-09 05:40:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:29:52.230153"
    },
    {
      "arxiv_id": "2411.18922v1",
      "title": "Devising a Set of Compact and Explainable Spoken Language Feature for Screening Alzheimer's Disease",
      "title_zh": "翻译失败",
      "authors": [
        "Junan Li",
        "Yunxiang Li",
        "Yuren Wang",
        "Xixin Wu",
        "Helen Meng"
      ],
      "abstract": "Alzheimer's disease (AD) has become one of the most significant health\nchallenges in an aging society. The use of spoken language-based AD detection\nmethods has gained prevalence due to their scalability due to their\nscalability. Based on the Cookie Theft picture description task, we devised an\nexplainable and effective feature set that leverages the visual capabilities of\na large language model (LLM) and the Term Frequency-Inverse Document Frequency\n(TF-IDF) model. Our experimental results show that the newly proposed features\nconsistently outperform traditional linguistic features across two different\nclassifiers with high dimension efficiency. Our new features can be well\nexplained and interpreted step by step which enhance the interpretability of\nautomatic AD screening.",
      "tldr_zh": "这篇论文针对阿尔茨海默病 (AD) 筛查，设计了一个紧凑且可解释的口语语言特征集，基于 Cookie Theft 图片描述任务。特征集结合大型语言模型 (LLM) 的视觉能力和 Term Frequency-Inverse Document Frequency (TF-IDF) 模型，提高了检测的可扩展性和效率。实验结果显示，新特征在两个不同分类器上 consistently outperform 传统语言特征，具有高维度效率，并通过逐步解释增强了自动 AD 筛查的可解释性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published at ISCSLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.18922v1",
      "published_date": "2024-11-28 05:23:22 UTC",
      "updated_date": "2024-11-28 05:23:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:30:04.431896"
    },
    {
      "arxiv_id": "2411.18919v1",
      "title": "Federated Continual Graph Learning",
      "title_zh": "联邦持续图学习",
      "authors": [
        "Yinlin Zhu",
        "Xunkai Li",
        "Miao Hu",
        "Di Wu"
      ],
      "abstract": "In the era of big data, managing evolving graph data poses substantial\nchallenges due to storage costs and privacy issues. Training graph neural\nnetworks (GNNs) on such evolving data usually causes catastrophic forgetting,\nimpairing performance on earlier tasks. Despite existing continual graph\nlearning (CGL) methods mitigating this to some extent, they predominantly\noperate in centralized architectures and overlook the potential of distributed\ngraph databases to harness collective intelligence for enhanced performance\noptimization. To address these challenges, we present a pioneering study on\nFederated Continual Graph Learning (FCGL), which adapts GNNs to multiple\nevolving graphs within decentralized settings while adhering to storage and\nprivacy constraints. Our work begins with a comprehensive empirical analysis of\nFCGL, assessing its data characteristics, feasibility, and effectiveness, and\nreveals two principal challenges: local graph forgetting (LGF), where local\nGNNs forget prior knowledge when adapting to new tasks, and global expertise\nconflict (GEC), where the global GNN exhibits sub-optimal performance in both\nadapting to new tasks and retaining old ones, arising from inconsistent client\nexpertise during server-side parameter aggregation. To tackle these, we propose\nthe POWER framework, which mitigates LGF by preserving and replaying experience\nnodes with maximum local-global coverage at each client and addresses GEC by\nusing a pseudo prototype reconstruction strategy and trajectory-aware knowledge\ntransfer at the central server. Extensive evaluations across multiple graph\ndatasets demonstrate POWER's superior performance over straightforward\nfederated extensions of the centralized CGL algorithms and vision-focused\nfederated continual learning algorithms. Our code is available at\nhttps://github.com/zyl24/FCGL_POWER.",
      "tldr_zh": "在大数据时代，管理演化图数据面临存储成本和隐私挑战，而训练图神经网络(GNNs)常导致灾难性遗忘，现有中心化持续图学习(CGL)方法未能利用分布式设置。本文首次提出Federated Continual Graph Learning (FCGL)，在去中心化环境中适应多个演化图，同时遵守相关约束。针对主要挑战local graph forgetting (LGF)和global expertise conflict (GEC)，作者开发了POWER框架，通过在客户端保留并重放经验节点、在服务器端使用伪原型重建和轨迹感知知识转移来缓解问题。实验在多个图数据集上证明，POWER显著优于基于中心化CGL或视觉联邦持续学习的基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2411.18919v1",
      "published_date": "2024-11-28 05:15:47 UTC",
      "updated_date": "2024-11-28 05:15:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:30:16.622048"
    },
    {
      "arxiv_id": "2412.06810v1",
      "title": "I See, Therefore I Do: Estimating Causal Effects for Image Treatments",
      "title_zh": "我见，因此我行：针对图像处理的因果效应估计",
      "authors": [
        "Abhinav Thorat",
        "Ravi Kolla",
        "Niranjan Pedanekar"
      ],
      "abstract": "Causal effect estimation under observational studies is challenging due to\nthe lack of ground truth data and treatment assignment bias. Though various\nmethods exist in literature for addressing this problem, most of them ignore\nmulti-dimensional treatment information by considering it as scalar, either\ncontinuous or discrete. Recently, certain works have demonstrated the utility\nof this rich yet complex treatment information into the estimation process,\nresulting in better causal effect estimation. However, these works have been\ndemonstrated on either graphs or textual treatments. There is a notable gap in\nexisting literature in addressing higher dimensional data such as images that\nhas a wide variety of applications. In this work, we propose a model named NICE\n(Network for Image treatments Causal effect Estimation), for estimating\nindividual causal effects when treatments are images. NICE demonstrates an\neffective way to use the rich multidimensional information present in image\ntreatments that helps in obtaining improved causal effect estimates. To\nevaluate the performance of NICE, we propose a novel semi-synthetic data\nsimulation framework that generates potential outcomes when images serve as\ntreatments. Empirical results on these datasets, under various setups including\nthe zero-shot case, demonstrate that NICE significantly outperforms existing\nmodels that incorporate treatment information for causal effect estimation.",
      "tldr_zh": "该研究解决了观察性研究中因果效应估计的挑战，特别是处理多维治疗信息（如图像）时现有方法存在的偏差和忽略问题。作者提出了一种名为 NICE（Network for Image treatments Causal effect Estimation）的模型，通过有效利用图像治疗的丰富多维信息，改善了个体因果效应估计的准确性。为评估 NICE，研究还开发了一个新型半合成数据模拟框架，用于生成图像作为治疗的潜在结果。实验结果显示，在各种设置（包括零样本情况）下，NICE 显著优于现有模型，证明了其在高维数据因果分析中的优势。",
      "categories": [
        "cs.AI",
        "stat.AP",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "8 Pages",
      "pdf_url": "http://arxiv.org/pdf/2412.06810v1",
      "published_date": "2024-11-28 04:40:15 UTC",
      "updated_date": "2024-11-28 04:40:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:30:33.146695"
    },
    {
      "arxiv_id": "2412.05311v1",
      "title": "DRC-Coder: Automated DRC Checker Code Generation Using LLM Autonomous Agent",
      "title_zh": "DRC-Coder：使用 LLM 自治代理的自动 DRC 检查器代码生成",
      "authors": [
        "Chen-Chia Chang",
        "Chia-Tung Ho",
        "Yaguang Li",
        "Yiran Chen",
        "Haoxing Ren"
      ],
      "abstract": "In the advanced technology nodes, the integrated design rule checker (DRC) is\noften utilized in place and route tools for fast optimization loops for\npower-performance-area. Implementing integrated DRC checkers to meet the\nstandard of commercial DRC tools demands extensive human expertise to interpret\nfoundry specifications, analyze layouts, and debug code iteratively. However,\nthis labor-intensive process, requiring to be repeated by every update of\ntechnology nodes, prolongs the turnaround time of designing circuits. In this\npaper, we present DRC-Coder, a multi-agent framework with vision capabilities\nfor automated DRC code generation. By incorporating vision language models and\nlarge language models (LLM), DRC-Coder can effectively process textual, visual,\nand layout information to perform rule interpretation and coding by two\nspecialized LLMs. We also design an auto-evaluation function for LLMs to enable\nDRC code debugging. Experimental results show that targeting on a sub-3nm\ntechnology node for a state-of-the-art standard cell layout tool, DRC-Coder\nachieves perfect F1 score 1.000 in generating DRC codes for meeting the\nstandard of a commercial DRC tool, highly outperforming standard prompting\ntechniques (F1=0.631). DRC-Coder can generate code for each design rule within\nfour minutes on average, which significantly accelerates technology advancement\nand reduces engineering costs.",
      "tldr_zh": "本论文提出DRC-Coder，一种基于LLM自治智能体的多智能体框架，用于自动化生成DRC检查代码，以加速集成电路设计优化循环。该框架整合视觉语言模型和LLM，处理文本、视觉和布局信息，通过两个专门的LLM进行规则解释和编码，并设计了自动评估功能以支持代码调试。实验结果显示，在亚3nm技术节点上，DRC-Coder实现了完美的F1 score 1.000，远超标准提示技术（F1=0.631），并平均在四分钟内生成每条规则的代码，从而显著降低工程成本并推动技术进步。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "Proceedings of the 2025 International Symposium on Physical Design\n  (ISPD '25), March 16--19, 2025, Austin, TX, USA",
      "pdf_url": "http://arxiv.org/pdf/2412.05311v1",
      "published_date": "2024-11-28 04:29:17 UTC",
      "updated_date": "2024-11-28 04:29:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:30:45.404855"
    },
    {
      "arxiv_id": "2412.00110v1",
      "title": "Demographic Predictability in 3D CT Foundation Embeddings",
      "title_zh": "三维 CT 基础嵌入中的人口统计学可预测性",
      "authors": [
        "Guangyao Zheng",
        "Michael A. Jacobs",
        "Vishwa S. Parekh"
      ],
      "abstract": "Self-supervised foundation models have recently been successfully extended to\nencode three-dimensional (3D) computed tomography (CT) images, with excellent\nperformance across several downstream tasks, such as intracranial hemorrhage\ndetection and lung cancer risk forecasting. However, as self-supervised models\nlearn from complex data distributions, questions arise concerning whether these\nembeddings capture demographic information, such as age, sex, or race. Using\nthe National Lung Screening Trial (NLST) dataset, which contains 3D CT images\nand demographic data, we evaluated a range of classifiers: softmax regression,\nlinear regression, linear support vector machine, random forest, and decision\ntree, to predict sex, race, and age of the patients in the images. Our results\nindicate that the embeddings effectively encoded age and sex information, with\na linear regression model achieving a root mean square error (RMSE) of 3.8\nyears for age prediction and a softmax regression model attaining an AUC of\n0.998 for sex classification. Race prediction was less effective, with an AUC\nof 0.878. These findings suggest a detailed exploration into the information\nencoded in self-supervised learning frameworks is needed to help ensure fair,\nresponsible, and patient privacy-protected healthcare AI.",
      "tldr_zh": "该研究探讨了自监督基础模型（self-supervised foundation models）在三维 CT（3D CT）图像嵌入中是否捕捉了人口统计信息，如年龄、性别和种族。研究者使用 National Lung Screening Trial (NLST) 数据集，并采用多种分类器（如 softmax regression、linear regression 和 random forest）来预测图像中的这些属性。结果显示，嵌入模型有效地编码了年龄和性别信息，linear regression 模型的年龄预测根均方误差（RMSE）为 3.8 年，而 softmax regression 模型的性别分类面积下曲线下值（AUC）达 0.998；然而，种族预测的 AUC 仅为 0.878。这些发现突出了需要深入分析自监督学习框架中编码的信息，以确保医疗 AI 的公平性、责任性和患者隐私保护。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "submitted to Radiology Cardiothoracic Imaging",
      "pdf_url": "http://arxiv.org/pdf/2412.00110v1",
      "published_date": "2024-11-28 04:26:39 UTC",
      "updated_date": "2024-11-28 04:26:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:30:58.435490"
    },
    {
      "arxiv_id": "2412.10381v5",
      "title": "Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream Allocation in Feed",
      "title_zh": "翻译失败",
      "authors": [
        "Jingxin Liu",
        "Xiang Gao",
        "Yisha Li",
        "Xin Li",
        "Haiyang Lu",
        "Ben Wang"
      ],
      "abstract": "In the context of a short video & live stream mixed recommendation scenario,\nthe live stream recommendation system (RS) decides whether to allocate at most\none live stream into the video feed for each user request. To maximize\nlong-term user engagement, it is crucial to determine an optimal live stream\npolicy for accurate live stream allocation. The inappropriate live stream\nallocation policy can significantly affect the duration of the usage app and\nuser retention, which ignores the long-term negative impact of live stream\nallocation. Recently, reinforcement learning (RL) has been widely applied in\nrecommendation systems to capture long-term user engagement. However,\ntraditional RL algorithms often face divergence and instability problems, which\nrestricts the application and deployment in the large-scale industrial\nrecommendation systems, especially in the aforementioned challenging scenario.\nTo address these challenges, we propose a novel Supervised Learning-enhanced\nMulti-Group Actor Critic algorithm (SL-MGAC). Specifically, we introduce a\nsupervised learning-enhanced actor-critic framework that incorporates variance\nreduction techniques, where multi-task reward learning helps restrict\nbootstrapping error accumulation during critic learning. Additionally, we\ndesign a multi-group state decomposition module for both actor and critic\nnetworks to reduce prediction variance and improve model stability. We also\npropose a novel reward function to prevent overly greedy live stream\nallocation. Empirically, we evaluate the SL-MGAC algorithm using offline policy\nevaluation (OPE) and online A/B testing. Experimental results demonstrate that\nthe proposed method not only outperforms baseline methods under the\nplatform-level constraints but also exhibits enhanced stability in online\nrecommendation scenarios.",
      "tldr_zh": "该论文针对短视频和直播混合推荐场景，提出了一种Supervised Learning-enhanced Multi-Group Actor Critic (SL-MGAC)算法，以优化直播分配策略并最大化长期用户参与。SL-MGAC通过引入监督学习增强的actor-critic框架和方差减少技术，包括多任务奖励学习来限制bootstrapping错误积累，以及多组状态分解模块来降低预测方差并提升模型稳定性。论文还设计了一种新颖的奖励函数，以防止过度贪婪的直播分配。实验结果显示，在offline policy evaluation (OPE)和在线A/B测试中，SL-MGAC在平台级约束下优于基线方法，并表现出更高的稳定性和性能提升。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10381v5",
      "published_date": "2024-11-28 04:06:02 UTC",
      "updated_date": "2025-05-17 01:06:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:31:09.926133"
    },
    {
      "arxiv_id": "2411.18892v2",
      "title": "A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges",
      "title_zh": "强化学习的全面综述：从算法到实际挑战",
      "authors": [
        "Majid Ghasemi",
        "Amir Hossein Moosavi",
        "Dariush Ebrahimi"
      ],
      "abstract": "Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial\nIntelligence (AI), enabling agents to learn optimal behaviors through\ninteractions with their environments. Drawing from the foundations of trial and\nerror, RL equips agents to make informed decisions through feedback in the form\nof rewards or penalties. This paper presents a comprehensive survey of RL,\nmeticulously analyzing a wide range of algorithms, from foundational tabular\nmethods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize\nand evaluate these algorithms based on key criteria such as scalability, sample\nefficiency, and suitability. We compare the methods in the form of their\nstrengths and weaknesses in diverse settings. Additionally, we offer practical\ninsights into the selection and implementation of RL algorithms, addressing\ncommon challenges like convergence, stability, and the exploration-exploitation\ndilemma. This paper serves as a comprehensive reference for researchers and\npractitioners aiming to harness the full potential of RL in solving complex,\nreal-world problems.",
      "tldr_zh": "这篇论文对Reinforcement Learning (RL)进行了全面调查，涵盖从基础表格方法到高级Deep Reinforcement Learning (DRL)技术的各种算法。论文通过分类和评估这些算法，基于可扩展性、样本效率和适用性等标准，比较了它们的优势和劣势，以适应不同场景。作者还提供了RL算法的选择与实施的实用指导，针对常见挑战如收敛、稳定性和exploration-exploitation困境，提出了解决方案。该调查为研究者和从业者提供了宝贵参考，帮助他们在复杂真实世界问题中充分发挥RL的潜力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "79 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.18892v2",
      "published_date": "2024-11-28 03:53:14 UTC",
      "updated_date": "2025-02-01 23:49:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:31:20.984981"
    },
    {
      "arxiv_id": "2411.18888v1",
      "title": "ArEEG_Words: Dataset for Envisioned Speech Recognition using EEG for Arabic Words",
      "title_zh": "翻译失败",
      "authors": [
        "Hazem Darwish",
        "Abdalrahman Al Malah",
        "Khloud Al Jallad",
        "Nada Ghneim"
      ],
      "abstract": "Brain-Computer-Interface (BCI) aims to support communication-impaired\npatients by translating neural signals into speech. A notable research topic in\nBCI involves Electroencephalography (EEG) signals that measure the electrical\nactivity in the brain. While significant advancements have been made in BCI EEG\nresearch, a major limitation still exists: the scarcity of publicly available\nEEG datasets for non-English languages, such as Arabic. To address this gap, we\nintroduce in this paper ArEEG_Words dataset, a novel EEG dataset recorded from\n22 participants with mean age of 22 years (5 female, 17 male) using a\n14-channel Emotiv Epoc X device. The participants were asked to be free from\nany effects on their nervous system, such as coffee, alcohol, cigarettes, and\nso 8 hours before recording. They were asked to stay calm in a clam room during\nimagining one of the 16 Arabic Words for 10 seconds. The words include 16\ncommonly used words such as up, down, left, and right. A total of 352 EEG\nrecordings were collected, then each recording was divided into multiple 250ms\nsignals, resulting in a total of 15,360 EEG signals. To the best of our\nknowledge, ArEEG_Words data is the first of its kind in Arabic EEG domain.\nMoreover, it is publicly available for researchers as we hope that will fill\nthe gap in Arabic EEG research.",
      "tldr_zh": "本研究针对脑机接口(BCI)中阿拉伯语EEG数据集的缺失问题，引入了ArEEG_Words数据集，用于支持想象语音识别。该数据集由22名参与者（平均年龄22岁）使用14通道Emotiv Epoc X设备采集，共记录352个EEG信号，每个信号对应参与者想象16个常用阿拉伯单词（如up, down, left, right）中的一个，每次10秒，并进一步分为15,360个250ms子信号。实验控制严格，确保参与者在无神经系统干扰的安静环境中进行，这有助于填补非英语语言EEG研究的空白，并为未来BCI应用提供公开可用的资源。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2402.15733",
      "pdf_url": "http://arxiv.org/pdf/2411.18888v1",
      "published_date": "2024-11-28 03:31:12 UTC",
      "updated_date": "2024-11-28 03:31:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:31:33.086034"
    },
    {
      "arxiv_id": "2411.18864v1",
      "title": "Redesigning the ensemble Kalman filter with a dedicated model of epistemic uncertainty",
      "title_zh": "翻译失败",
      "authors": [
        "Chatchuea Kimchaiwong",
        "Jeremie Houssineau",
        "Adam M. Johansen"
      ],
      "abstract": "The problem of incorporating information from observations received serially\nin time is widespread in the field of uncertainty quantification. Within a\nprobabilistic framework, such problems can be addressed using standard\nfiltering techniques. However, in many real-world problems, some (or all) of\nthe uncertainty is epistemic, arising from a lack of knowledge, and is\ndifficult to model probabilistically. This paper introduces a possibilistic\nensemble Kalman filter designed for this setting and characterizes some of its\nproperties. Using possibility theory to describe epistemic uncertainty is\nappealing from a philosophical perspective, and it is easy to justify certain\nheuristics often employed in standard ensemble Kalman filters as principled\napproaches to capturing uncertainty within it. The possibilistic approach\nmotivates a robust mechanism for characterizing uncertainty which shows good\nperformance with small sample sizes, and can outperform standard ensemble\nKalman filters at given sample size, even when dealing with genuinely aleatoric\nuncertainty.",
      "tldr_zh": "该论文重新设计了ensemble Kalman filter，引入了一个专门针对认识论不确定性(epistemic uncertainty)的possibilistic版本，以处理序列时间观察数据中的不确定性问题。\n该方法利用possibility theory来建模这种源于知识缺口的epistemic uncertainty，并为标准ensemble Kalman filter中的启发式提供原理依据。\n结果表明，这种possibilistic方法在小样本下表现出色，甚至在面对真正的随机不确定性(aleatoric uncertainty)时，也能优于传统方法。",
      "categories": [
        "stat.ME",
        "cs.AI",
        "62F15, 65C35"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.18864v1",
      "published_date": "2024-11-28 02:11:23 UTC",
      "updated_date": "2024-11-28 02:11:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:31:47.396272"
    },
    {
      "arxiv_id": "2411.18845v2",
      "title": "An Integrated Artificial Intelligence Operating System for Advanced Low-Altitude Aviation Applications",
      "title_zh": "集成人工智能操作系统，用于先进低空航空应用",
      "authors": [
        "Minzhe Tan",
        "Xinlin Fan",
        "Jian He",
        "Yi Hou",
        "Zhan Liu",
        "Yaopeng Jiang",
        "Y. M. Jiang"
      ],
      "abstract": "This paper introduces a high-performance artificial intelligence operating\nsystem tailored for low-altitude aviation, designed to address key challenges\nsuch as real-time task execution, computational efficiency, and seamless\nmodular collaboration. Built on a powerful hardware platform and leveraging the\nUNIX architecture, the system implements a distributed data processing strategy\nthat ensures rapid and efficient synchronization across critical modules,\nincluding vision, navigation, and perception. By adopting dynamic resource\nmanagement, it optimally allocates computational resources, such as CPU and\nGPU, based on task priority and workload, ensuring high performance for\ndemanding tasks like real-time video processing and AI model inference.\nFurthermore, the system features an advanced interrupt handling mechanism that\nallows for quick responses to sudden environmental changes, such as obstacle\ndetection, by prioritizing critical tasks, thus improving safety and mission\nsuccess rates. Robust security measures, including data encryption, access\ncontrol, and fault tolerance, ensure the system's resilience against external\nthreats and its ability to recover from potential hardware or software\nfailures. Complementing these core features are modular components for image\nanalysis, multi-sensor fusion, dynamic path planning, multi-drone coordination,\nand ground station monitoring. Additionally, a low-code development platform\nsimplifies user customization, making the system adaptable to various\nmission-specific needs. This comprehensive approach ensures the system meets\nthe evolving demands of intelligent aviation, providing a stable, efficient,\nand secure environment for complex drone operations.",
      "tldr_zh": "本论文提出了一种集成式人工智能操作系统（AI Operating System），针对先进低空航空应用，解决实时任务执行、计算效率和模块协作等关键挑战。该系统基于强大硬件平台和 UNIX architecture，采用分布式数据处理策略实现视觉、导航和感知模块的快速同步，并通过动态资源管理优化 CPU 和 GPU 分配，以支持实时视频处理和 AI model inference。此外，该系统配备高级中断处理机制和安全措施（如数据加密和容错），提升了对环境变化的响应速度和整体安全性，适用于图像分析、多传感器融合及多无人机协调等复杂任务，从而为智能航空提供高效、可靠的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.OS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.18845v2",
      "published_date": "2024-11-28 01:24:16 UTC",
      "updated_date": "2025-01-05 05:28:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:31:56.600171"
    },
    {
      "arxiv_id": "2412.10380v1",
      "title": "Challenges in Human-Agent Communication",
      "title_zh": "翻译失败",
      "authors": [
        "Gagan Bansal",
        "Jennifer Wortman Vaughan",
        "Saleema Amershi",
        "Eric Horvitz",
        "Adam Fourney",
        "Hussein Mozannar",
        "Victor Dibia",
        "Daniel S. Weld"
      ],
      "abstract": "Remarkable advancements in modern generative foundation models have enabled\nthe development of sophisticated and highly capable autonomous agents that can\nobserve their environment, invoke tools, and communicate with other agents to\nsolve problems. Although such agents can communicate with users through natural\nlanguage, their complexity and wide-ranging failure modes present novel\nchallenges for human-AI interaction. Building on prior research and informed by\na communication grounding perspective, we contribute to the study of\n\\emph{human-agent communication} by identifying and analyzing twelve key\ncommunication challenges that these systems pose. These include challenges in\nconveying information from the agent to the user, challenges in enabling the\nuser to convey information to the agent, and overarching challenges that need\nto be considered across all human-agent communication. We illustrate each\nchallenge through concrete examples and identify open directions of research.\nOur findings provide insights into critical gaps in human-agent communication\nresearch and serve as an urgent call for new design patterns, principles, and\nguidelines to support transparency and control in these systems.",
      "tldr_zh": "本研究探讨了现代生成基础模型(generative foundation models)带来的先进自主代理在人类-代理通信(human-agent communication)中的挑战，这些代理虽能通过自然语言与用户交互，但其复杂性和失败模式导致了新型的人类-AI交互问题。论文基于先前的研究和通信grounding视角，识别并分析了十二个关键挑战，包括代理向用户传达信息、用户向代理传达信息，以及跨所有通信的总体挑战，并通过具体例子进行说明。这些发现揭示了人类-代理通信研究中的关键空白，并呼吁开发新的设计模式、原则和指南，以提升这些系统的透明度和控制。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10380v1",
      "published_date": "2024-11-28 01:21:26 UTC",
      "updated_date": "2024-11-28 01:21:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:32:08.864902"
    },
    {
      "arxiv_id": "2412.00107v1",
      "title": "Virtual Sensing to Enable Real-Time Monitoring of Inaccessible Locations \\& Unmeasurable Parameters",
      "title_zh": "虚拟传感用于启用对无法访问位置和无法测量参数的实时监控",
      "authors": [
        "Kazuma Kobayashi",
        "Farid Ahmed",
        "Syed Bahauddin Alam"
      ],
      "abstract": "Real-time monitoring of critical parameters is essential for energy systems'\nsafe and efficient operation. However, traditional sensors often fail and\ndegrade in harsh environments where physical sensors cannot be placed\n(inaccessible locations). In addition, there are important parameters that\ncannot be directly measured by sensors. We need machine learning (ML)-based\nreal-time monitoring in those remote locations to ensure system operations.\nHowever, traditional ML models struggle to process continuous sensor profile\ndata to fit model requirements, leading to the loss of spatial relationships.\nAnother challenge for real-time monitoring is ``dataset shift\" and the need for\nfrequent retraining under varying conditions, where extensive retraining\nprohibits real-time inference. To resolve these challenges, this study\naddressed the limitations of real-time monitoring methods by enabling\nmonitoring in locations where physical sensors are impractical to deploy. Our\nproposed approach, utilizing Multi-Input Operator Network virtual sensors,\nleverages deep learning to seamlessly integrate diverse data sources and\naccurately predict key parameters in real-time without the need for additional\nphysical sensors. The approach's effectiveness is demonstrated through\nthermal-hydraulic monitoring in a nuclear reactor subchannel, achieving\nremarkable accuracy.",
      "tldr_zh": "该论文讨论了传统传感器在恶劣环境或不可访问位置的局限性，以及无法直接测量某些参数的挑战，导致实时监控困难。研究提出了一种基于Multi-Input Operator Network的虚拟传感器方法，利用深度学习整合多样数据源，实现对关键参数的准确实时预测，而无需额外物理传感器。为了验证其有效性，实验在核反应堆子通道的热液力监控中取得了显著准确性，解决了machine learning (ML)模型的dataset shift问题和频繁重训需求。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.00107v1",
      "published_date": "2024-11-28 00:58:29 UTC",
      "updated_date": "2024-11-28 00:58:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:32:21.001988"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 80,
  "processed_papers_count": 80,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T05:32:38.394831"
}
[
  {
    "arxiv_id": "2403.15962v1",
    "title": "Detection of Problem Gambling with Less Features Using Machine Learning Methods",
    "authors": [
      "Yang Jiao",
      "Gloria Wong-Padoongpatt",
      "Mei Yang"
    ],
    "abstract": "Analytic features in gambling study are performed based on the amount of data\nmonitoring on user daily actions. While performing the detection of problem\ngambling, existing datasets provide relatively rich analytic features for\nbuilding machine learning based model. However, considering the complexity and\ncost of collecting the analytic features in real applications, conducting\nprecise detection with less features will tremendously reduce the cost of data\ncollection. In this study, we propose a deep neural networks PGN4 that performs\nwell when using limited analytic features. Through the experiment on two\ndatasets, we discover that PGN4 only experiences a mere performance drop when\ncutting 102 features to 5 features. Besides, we find the commonality within the\ntop 5 features from two datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 5 tables, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2403.15962v1",
    "published_date": "2024-03-23 23:49:01 UTC",
    "updated_date": "2024-03-23 23:49:01 UTC"
  },
  {
    "arxiv_id": "2403.15961v3",
    "title": "SAT Encoding of Partial Ordering Models for Graph Coloring Problems",
    "authors": [
      "Daniel Faber",
      "Adalat Jabrayilov",
      "Petra Mutzel"
    ],
    "abstract": "In this paper, we suggest new SAT encodings of the partial-ordering based ILP\nmodel for the graph coloring problem (GCP) and the bandwidth coloring problem\n(BCP). The GCP asks for the minimum number of colors that can be assigned to\nthe vertices of a given graph such that each two adjacent vertices get\ndifferent colors. The BCP is a generalization, where each edge has a weight\nthat enforces a minimal \"distance\" between the assigned colors, and the goal is\nto minimize the \"largest\" color used. For the widely studied GCP, we\nexperimentally compare our new SAT encoding to the state-of-the-art approaches\non the DIMACS benchmark set. Our evaluation confirms that this SAT encoding is\neffective for sparse graphs and even outperforms the state-of-the-art on some\nDIMACS instances. For the BCP, our theoretical analysis shows that the\npartial-ordering based SAT and ILP formulations have an asymptotically smaller\nsize than that of the classical assignment-based model. Our practical\nevaluation confirms not only a dominance compared to the assignment-based\nencodings but also to the state-of-the-art approaches on a set of benchmark\ninstances. Up to our knowledge, we have solved several open instances of the\nBCP from the literature for the first time.",
    "categories": [
      "cs.AI",
      "cs.DM",
      "cs.DS",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15961v3",
    "published_date": "2024-03-23 23:48:41 UTC",
    "updated_date": "2024-08-14 14:32:21 UTC"
  },
  {
    "arxiv_id": "2403.15955v3",
    "title": "Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection",
    "authors": [
      "Minzhou Pan",
      "Zhenting Wang",
      "Xin Dong",
      "Vikash Sehwag",
      "Lingjuan Lyu",
      "Xue Lin"
    ],
    "abstract": "In this paper, we propose WaterMark Detection (WMD), the first invisible\nwatermark detection method under a black-box and annotation-free setting. WMD\nis capable of detecting arbitrary watermarks within a given reference dataset\nusing a clean non-watermarked dataset as a reference, without relying on\nspecific decoding methods or prior knowledge of the watermarking techniques. We\ndevelop WMD using foundations of offset learning, where a clean non-watermarked\ndataset enables us to isolate the influence of only watermarked samples in the\nreference dataset. Our comprehensive evaluations demonstrate the effectiveness\nof WMD, significantly outperforming naive detection methods, which only yield\nAUC scores around 0.5. In contrast, WMD consistently achieves impressive\ndetection AUC scores, surpassing 0.9 in most single-watermark datasets and\nexceeding 0.7 in more challenging multi-watermark scenarios across diverse\ndatasets and watermarking methods. As invisible watermarks become increasingly\nprevalent, while specific decoding techniques remain undisclosed, our approach\nprovides a versatile solution and establishes a path toward increasing\naccountability, transparency, and trust in our digital visual content.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15955v3",
    "published_date": "2024-03-23 23:22:54 UTC",
    "updated_date": "2024-03-30 06:42:02 UTC"
  },
  {
    "arxiv_id": "2403.15953v1",
    "title": "Understanding The Effectiveness of Lossy Compression in Machine Learning Training Sets",
    "authors": [
      "Robert Underwood",
      "Jon C. Calhoun",
      "Sheng Di",
      "Franck Cappello"
    ],
    "abstract": "Learning and Artificial Intelligence (ML/AI) techniques have become\nincreasingly prevalent in high performance computing (HPC). However, these\nmethods depend on vast volumes of floating point data for training and\nvalidation which need methods to share the data on a wide area network (WAN) or\nto transfer it from edge devices to data centers. Data compression can be a\nsolution to these problems, but an in-depth understanding of how lossy\ncompression affects model quality is needed. Prior work largely considers a\nsingle application or compression method. We designed a systematic methodology\nfor evaluating data reduction techniques for ML/AI, and we use it to perform a\nvery comprehensive evaluation with 17 data reduction methods on 7 ML/AI\napplications to show modern lossy compression methods can achieve a 50-100x\ncompression ratio improvement for a 1% or less loss in quality. We identify\ncritical insights that guide the future use and design of lossy compressors for\nML/AI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6; E.2; C.4"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.15953v1",
    "published_date": "2024-03-23 23:14:37 UTC",
    "updated_date": "2024-03-23 23:14:37 UTC"
  },
  {
    "arxiv_id": "2403.15944v1",
    "title": "Adaptive Super Resolution For One-Shot Talking-Head Generation",
    "authors": [
      "Luchuan Song",
      "Pinxin Liu",
      "Guojun Yin",
      "Chenliang Xu"
    ],
    "abstract": "The one-shot talking-head generation learns to synthesize a talking-head\nvideo with one source portrait image under the driving of same or different\nidentity video. Usually these methods require plane-based pixel transformations\nvia Jacobin matrices or facial image warps for novel poses generation. The\nconstraints of using a single image source and pixel displacements often\ncompromise the clarity of the synthesized images. Some methods try to improve\nthe quality of synthesized videos by introducing additional super-resolution\nmodules, but this will undoubtedly increase computational consumption and\ndestroy the original data distribution. In this work, we propose an adaptive\nhigh-quality talking-head video generation method, which synthesizes\nhigh-resolution video without additional pre-trained modules. Specifically,\ninspired by existing super-resolution methods, we down-sample the one-shot\nsource image, and then adaptively reconstruct high-frequency details via an\nencoder-decoder module, resulting in enhanced video clarity. Our method\nconsistently improves the quality of generated videos through a straightforward\nyet effective strategy, substantiated by quantitative and qualitative\nevaluations. The code and demo video are available on:\n\\url{https://github.com/Songluchuan/AdaSR-TalkingHead/}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.15944v1",
    "published_date": "2024-03-23 22:14:38 UTC",
    "updated_date": "2024-03-23 22:14:38 UTC"
  },
  {
    "arxiv_id": "2403.15941v3",
    "title": "Explore until Confident: Efficient Exploration for Embodied Question Answering",
    "authors": [
      "Allen Z. Ren",
      "Jaden Clark",
      "Anushri Dixit",
      "Masha Itkina",
      "Anirudha Majumdar",
      "Dorsa Sadigh"
    ],
    "abstract": "We consider the problem of Embodied Question Answering (EQA), which refers to\nsettings where an embodied agent such as a robot needs to actively explore an\nenvironment to gather information until it is confident about the answer to a\nquestion. In this work, we leverage the strong semantic reasoning capabilities\nof large vision-language models (VLMs) to efficiently explore and answer such\nquestions. However, there are two main challenges when using VLMs in EQA: they\ndo not have an internal memory for mapping the scene to be able to plan how to\nexplore over time, and their confidence can be miscalibrated and can cause the\nrobot to prematurely stop exploration or over-explore. We propose a method that\nfirst builds a semantic map of the scene based on depth information and via\nvisual prompting of a VLM - leveraging its vast knowledge of relevant regions\nof the scene for exploration. Next, we use conformal prediction to calibrate\nthe VLM's question answering confidence, allowing the robot to know when to\nstop exploration - leading to a more calibrated and efficient exploration\nstrategy. To test our framework in simulation, we also contribute a new EQA\ndataset with diverse, realistic human-robot scenarios and scenes built upon the\nHabitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot\nexperiments show our proposed approach improves the performance and efficiency\nover baselines that do no leverage VLM for exploration or do not calibrate its\nconfidence. Webpage with experiment videos and code:\nhttps://explore-eqa.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Robotics: Science and Systems (RSS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15941v3",
    "published_date": "2024-03-23 22:04:03 UTC",
    "updated_date": "2024-07-07 19:40:31 UTC"
  },
  {
    "arxiv_id": "2403.15940v1",
    "title": "Geotokens and Geotransformers",
    "authors": [
      "Eren Unlu"
    ],
    "abstract": "In transformer architectures, position encoding primarily provides a sense of\nsequence for input tokens. While the original transformer paper's method has\nshown satisfactory results in general language processing tasks, there have\nbeen new proposals, such as Rotary Position Embedding (RoPE), for further\nimprovement. This paper presents geotokens, input components for transformers,\neach linked to a specific geological location. Unlike typical language\nsequences, for these tokens, the order is not as vital as the geographical\ncoordinates themselves. To represent the relative position in this context and\nto keep a balance between the real world distance and the distance in the\nembedding space, we design a position encoding approach drawing from the RoPE\nstructure but tailored for spherical coordinates.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15940v1",
    "published_date": "2024-03-23 22:02:56 UTC",
    "updated_date": "2024-03-23 22:02:56 UTC"
  },
  {
    "arxiv_id": "2403.15938v1",
    "title": "LlamBERT: Large-scale low-cost data annotation in NLP",
    "authors": [
      "Bálint Csanády",
      "Lajos Muzsai",
      "Péter Vedres",
      "Zoltán Nádasdy",
      "András Lukács"
    ],
    "abstract": "Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable\nproficiency in a wide range of natural language processing (NLP) tasks. Despite\ntheir effectiveness, the high costs associated with their use pose a challenge.\nWe present LlamBERT, a hybrid approach that leverages LLMs to annotate a small\nsubset of large, unlabeled databases and uses the results for fine-tuning\ntransformer encoders like BERT and RoBERTa. This strategy is evaluated on two\ndiverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our\nresults indicate that the LlamBERT approach slightly compromises on accuracy\nwhile offering much greater cost-effectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7; F.1.1"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2403.15938v1",
    "published_date": "2024-03-23 21:54:34 UTC",
    "updated_date": "2024-03-23 21:54:34 UTC"
  },
  {
    "arxiv_id": "2403.15933v3",
    "title": "Understanding Domain-Size Generalization in Markov Logic Networks",
    "authors": [
      "Florian Chen",
      "Felix Weitkämper",
      "Sagar Malhotra"
    ],
    "abstract": "We study the generalization behavior of Markov Logic Networks (MLNs) across\nrelational structures of different sizes. Multiple works have noticed that MLNs\nlearned on a given domain generalize poorly across domains of different sizes.\nThis behavior emerges from a lack of internal consistency within an MLN when\nused across different domain sizes. In this paper, we quantify this\ninconsistency and bound it in terms of the variance of the MLN parameters. The\nparameter variance also bounds the KL divergence between an MLN's marginal\ndistributions taken from different domain sizes. We use these bounds to show\nthat maximizing the data log-likelihood while simultaneously minimizing the\nparameter variance corresponds to two natural notions of generalization across\ndomain sizes. Our theoretical results apply to Exponential Random Graphs and\nother Markov network based relational models. Finally, we observe that\nsolutions known to decrease the variance of the MLN parameters, like\nregularization and Domain-Size Aware MLNs, increase the internal consistency of\nthe MLNs. We empirically verify our results on four different datasets, with\ndifferent methods to control parameter variance, showing that controlling\nparameter variance leads to better generalization.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "To Appear in Proceedings of ECML 2024-Research Track",
    "pdf_url": "http://arxiv.org/pdf/2403.15933v3",
    "published_date": "2024-03-23 21:16:56 UTC",
    "updated_date": "2024-06-03 15:30:52 UTC"
  },
  {
    "arxiv_id": "2403.15931v4",
    "title": "X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention",
    "authors": [
      "You Xie",
      "Hongyi Xu",
      "Guoxian Song",
      "Chao Wang",
      "Yichun Shi",
      "Linjie Luo"
    ],
    "abstract": "We propose X-Portrait, an innovative conditional diffusion model tailored for\ngenerating expressive and temporally coherent portrait animation. Specifically,\ngiven a single portrait as appearance reference, we aim to animate it with\nmotion derived from a driving video, capturing both highly dynamic and subtle\nfacial expressions along with wide-range head movements. As its core, we\nleverage the generative prior of a pre-trained diffusion model as the rendering\nbackbone, while achieve fine-grained head pose and expression control with\nnovel controlling signals within the framework of ControlNet. In contrast to\nconventional coarse explicit controls such as facial landmarks, our motion\ncontrol module is learned to interpret the dynamics directly from the original\ndriving RGB inputs. The motion accuracy is further enhanced with a patch-based\nlocal control module that effectively enhance the motion attention to\nsmall-scale nuances like eyeball positions. Notably, to mitigate the identity\nleakage from the driving signals, we train our motion control modules with\nscaling-augmented cross-identity images, ensuring maximized disentanglement\nfrom the appearance reference modules. Experimental results demonstrate the\nuniversal effectiveness of X-Portrait across a diverse range of facial\nportraits and expressive driving sequences, and showcase its proficiency in\ngenerating captivating portrait animations with consistently maintained\nidentity characteristics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "SIGGRAPH 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15931v4",
    "published_date": "2024-03-23 20:30:28 UTC",
    "updated_date": "2024-07-25 22:45:41 UTC"
  },
  {
    "arxiv_id": "2403.15916v1",
    "title": "Multi-agent transformer-accelerated RL for satisfaction of STL specifications",
    "authors": [
      "Albin Larsson Forsberg",
      "Alexandros Nikou",
      "Aneta Vulgarakis Feljan",
      "Jana Tumova"
    ],
    "abstract": "One of the main challenges in multi-agent reinforcement learning is\nscalability as the number of agents increases. This issue is further\nexacerbated if the problem considered is temporally dependent. State-of-the-art\nsolutions today mainly follow centralized training with decentralized execution\nparadigm in order to handle the scalability concerns. In this paper, we propose\ntime-dependent multi-agent transformers which can solve the temporally\ndependent multi-agent problem efficiently with a centralized approach via the\nuse of transformers that proficiently handle the large input. We highlight the\nefficacy of this method on two problems and use tools from statistics to verify\nthe probability that the trajectories generated under the policy satisfy the\ntask. The experiments show that our approach has superior performance against\nthe literature baseline algorithms in both cases.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to L4DC 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2403.15916v1",
    "published_date": "2024-03-23 19:13:01 UTC",
    "updated_date": "2024-03-23 19:13:01 UTC"
  },
  {
    "arxiv_id": "2404.07963v1",
    "title": "EduAgent: Generative Student Agents in Learning",
    "authors": [
      "Songlin Xu",
      "Xinyu Zhang",
      "Lianhui Qin"
    ],
    "abstract": "Student simulation in online education is important to address dynamic\nlearning behaviors of students with diverse backgrounds. Existing simulation\nmodels based on deep learning usually need massive training data, lacking prior\nknowledge in educational contexts. Large language models (LLMs) may contain\nsuch prior knowledge since they are pre-trained from a large corpus. However,\nbecause student behaviors are dynamic and multifaceted with individual\ndifferences, directly prompting LLMs is not robust nor accurate enough to\ncapture fine-grained interactions among diverse student personas, learning\nbehaviors, and learning outcomes. This work tackles this problem by presenting\na newly annotated fine-grained large-scale dataset and proposing EduAgent, a\nnovel generative agent framework incorporating cognitive prior knowledge (i.e.,\ntheoretical findings revealed in cognitive science) to guide LLMs to first\nreason correlations among various behaviors and then make simulations. Our two\nexperiments show that EduAgent could not only mimic and predict learning\nbehaviors of real students but also generate realistic learning behaviors of\nvirtual students without real data.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.07963v1",
    "published_date": "2024-03-23 18:19:17 UTC",
    "updated_date": "2024-03-23 18:19:17 UTC"
  },
  {
    "arxiv_id": "2403.15901v3",
    "title": "MatchSeg: Towards Better Segmentation via Reference Image Matching",
    "authors": [
      "Jiayu Huo",
      "Ruiqiang Xiao",
      "Haotian Zheng",
      "Yang Liu",
      "Sebastien Ourselin",
      "Rachel Sparks"
    ],
    "abstract": "Recently, automated medical image segmentation methods based on deep learning\nhave achieved great success. However, they heavily rely on large annotated\ndatasets, which are costly and time-consuming to acquire. Few-shot learning\naims to overcome the need for annotated data by using a small labeled dataset,\nknown as a support set, to guide predicting labels for new, unlabeled images,\nknown as the query set. Inspired by this paradigm, we introduce MatchSeg, a\nnovel framework that enhances medical image segmentation through strategic\nreference image matching. We leverage contrastive language-image pre-training\n(CLIP) to select highly relevant samples when defining the support set.\nAdditionally, we design a joint attention module to strengthen the interaction\nbetween support and query features, facilitating a more effective knowledge\ntransfer between support and query sets. We validated our method across four\npublic datasets. Experimental results demonstrate superior segmentation\nperformance and powerful domain generalization ability of MatchSeg against\nexisting methods for domain-specific and cross-domain segmentation tasks. Our\ncode is made available at https://github.com/keeplearning-again/MatchSeg",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "International Conference on Bioinformatics and Biomedicine (BIBM\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.15901v3",
    "published_date": "2024-03-23 18:04:58 UTC",
    "updated_date": "2024-08-17 23:49:15 UTC"
  },
  {
    "arxiv_id": "2403.15886v1",
    "title": "Leveraging Zero-Shot Prompting for Efficient Language Model Distillation",
    "authors": [
      "Lukas Vöge",
      "Vincent Gurgul",
      "Stefan Lessmann"
    ],
    "abstract": "This paper introduces a novel approach for efficiently distilling LLMs into\nsmaller, application-specific models, significantly reducing operational costs\nand manual labor. Addressing the challenge of deploying computationally\nintensive LLMs in specific applications or edge devices, this technique\nutilizes LLMs' reasoning capabilities to generate labels and natural language\nrationales for unlabeled data. Our approach enhances both finetuning and\ndistillation by employing a multi-task training framework where student models\nmimic these rationales alongside teacher predictions. Key contributions include\nthe employment of zero-shot prompting to elicit teacher model rationales,\nreducing the necessity for handcrafted few-shot examples and lowering the\noverall token count required, which directly translates to cost savings given\nthe pay-per-token billing model of major tech companies' LLM APIs.\nAdditionally, the paper investigates the impact of explanation properties on\ndistillation efficiency, demonstrating that minimal performance loss occurs\neven when rationale augmentation is not applied across the entire dataset,\nfacilitating further reductions of tokens. This research marks a step toward\nthe efficient training of task-specific models with minimal human intervention,\noffering substantial cost-savings while maintaining, or even enhancing,\nperformance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15886v1",
    "published_date": "2024-03-23 16:51:52 UTC",
    "updated_date": "2024-03-23 16:51:52 UTC"
  },
  {
    "arxiv_id": "2403.15879v6",
    "title": "TrustSQL: Benchmarking Text-to-SQL Reliability with Penalty-Based Scoring",
    "authors": [
      "Gyubok Lee",
      "Woosog Chay",
      "Seonhee Cho",
      "Edward Choi"
    ],
    "abstract": "Text-to-SQL enables users to interact with databases using natural language,\nsimplifying the retrieval and synthesis of information. Despite the remarkable\nsuccess of large language models (LLMs) in translating natural language\nquestions into SQL queries, widespread deployment remains limited due to two\nprimary challenges. First, the effective use of text-to-SQL models depends on\nusers' understanding of the model's capabilities-the scope of questions the\nmodel can correctly answer. Second, the absence of abstention mechanisms can\nlead to incorrect SQL generation going unnoticed, thereby undermining trust in\nthe model's output. To enable wider deployment, it is crucial to address these\nchallenges in model design and enhance model evaluation to build trust in the\nmodel's output. To this end, we introduce TrustSQL, a novel comprehensive\nbenchmark designed to evaluate text-to-SQL reliability-defined as a model's\nability to correctly handle any type of input question by generating correct\nSQL queries for feasible questions and abstaining from generating infeasible\nones (e.g., due to schema incompatibility or functionalities beyond SQL). We\nevaluate existing methods using a novel penalty-based scoring metric with two\nmodeling approaches: (1) pipeline-based methods combining SQL generators with\ninfeasible question detectors and SQL error detectors for abstention; and (2)\nunified methods using a single model for the entire task. Our experimental\nresults reveal that achieving high scores under severe penalties requires\nsignificant effort and provide a new perspective on developing text-to-SQL\nmodels for safer deployment. TrustSQL is available at\nhttps://github.com/glee4810/TrustSQL.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2403.15879v6",
    "published_date": "2024-03-23 16:12:52 UTC",
    "updated_date": "2024-07-02 10:32:22 UTC"
  },
  {
    "arxiv_id": "2403.15876v1",
    "title": "Cognitive resilience: Unraveling the proficiency of image-captioning models to interpret masked visual content",
    "authors": [
      "Zhicheng Du",
      "Zhaotian Xie",
      "Huazhang Ying",
      "Likun Zhang",
      "Peiwu Qin"
    ],
    "abstract": "This study explores the ability of Image Captioning (IC) models to decode\nmasked visual content sourced from diverse datasets. Our findings reveal the IC\nmodel's capability to generate captions from masked images, closely resembling\nthe original content. Notably, even in the presence of masks, the model adeptly\ncrafts descriptive textual information that goes beyond what is observable in\nthe original image-generated captions. While the decoding performance of the IC\nmodel experiences a decline with an increase in the masked region's area, the\nmodel still performs well when important regions of the image are not masked at\nhigh coverage.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as tiny paper in ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15876v1",
    "published_date": "2024-03-23 15:53:00 UTC",
    "updated_date": "2024-03-23 15:53:00 UTC"
  },
  {
    "arxiv_id": "2403.15875v1",
    "title": "LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series classification",
    "authors": [
      "Zhicheng Du",
      "Zhaotian Xie",
      "Yan Tong",
      "Peiwu Qin"
    ],
    "abstract": "This study constructs the LanguAge Model with Prompt EngineeRing (LAMPER)\nframework, designed to systematically evaluate the adaptability of pre-trained\nlanguage models (PLMs) in accommodating diverse prompts and their integration\nin zero-shot time series (TS) classification. We deploy LAMPER in experimental\nassessments using 128 univariate TS datasets sourced from the UCR archive. Our\nfindings indicate that the feature representation capacity of LAMPER is\ninfluenced by the maximum input token threshold imposed by PLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as tiny paper in ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15875v1",
    "published_date": "2024-03-23 15:52:37 UTC",
    "updated_date": "2024-03-23 15:52:37 UTC"
  },
  {
    "arxiv_id": "2403.17978v1",
    "title": "Holographic Global Convolutional Networks for Long-Range Prediction Tasks in Malware Detection",
    "authors": [
      "Mohammad Mahmudul Alam",
      "Edward Raff",
      "Stella Biderman",
      "Tim Oates",
      "James Holt"
    ],
    "abstract": "Malware detection is an interesting and valuable domain to work in because it\nhas significant real-world impact and unique machine-learning challenges. We\ninvestigate existing long-range techniques and benchmarks and find that they're\nnot very suitable in this problem area. In this paper, we introduce Holographic\nGlobal Convolutional Networks (HGConv) that utilize the properties of\nHolographic Reduced Representations (HRR) to encode and decode features from\nsequence elements. Unlike other global convolutional methods, our method does\nnot require any intricate kernel computation or crafted kernel design. HGConv\nkernels are defined as simple parameters learned through backpropagation. The\nproposed method has achieved new SOTA results on Microsoft Malware\nClassification Challenge, Drebin, and EMBER malware benchmarks. With log-linear\ncomplexity in sequence length, the empirical results demonstrate substantially\nfaster run-time by HGConv compared to other methods achieving far more\nefficient scaling even with sequence length $\\geq 100,000$.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in Proceedings of the 27th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2024, Valencia, Spain",
    "pdf_url": "http://arxiv.org/pdf/2403.17978v1",
    "published_date": "2024-03-23 15:49:13 UTC",
    "updated_date": "2024-03-23 15:49:13 UTC"
  },
  {
    "arxiv_id": "2403.15864v1",
    "title": "Using Large Language Models for OntoClean-based Ontology Refinement",
    "authors": [
      "Yihang Zhao",
      "Neil Vetter",
      "Kaveh Aryan"
    ],
    "abstract": "This paper explores the integration of Large Language Models (LLMs) such as\nGPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing\non the OntoClean methodology. OntoClean, critical for assessing the\nmetaphysical quality of ontologies, involves a two-step process of assigning\nmeta-properties to classes and verifying a set of constraints. Manually\nconducting the first step proves difficult in practice, due to the need for\nphilosophical expertise and lack of consensus among ontologists. By employing\nLLMs with two prompting strategies, the study demonstrates that high accuracy\nin the labelling process can be achieved. The findings suggest the potential\nfor LLMs to enhance ontology refinement, proposing the development of plugin\nsoftware for ontology tools to facilitate this integration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15864v1",
    "published_date": "2024-03-23 15:09:50 UTC",
    "updated_date": "2024-03-23 15:09:50 UTC"
  },
  {
    "arxiv_id": "2403.15857v2",
    "title": "Automated System-level Testing of Unmanned Aerial Systems",
    "authors": [
      "Hassan Sartaj",
      "Asmar Muqeet",
      "Muhammad Zohaib Iqbal",
      "Muhammad Uzair Khan"
    ],
    "abstract": "Unmanned aerial systems (UAS) rely on various avionics systems that are\nsafety-critical and mission-critical. A major requirement of international\nsafety standards is to perform rigorous system-level testing of avionics\nsoftware systems. The current industrial practice is to manually create test\nscenarios, manually/automatically execute these scenarios using simulators, and\nmanually evaluate outcomes. The test scenarios typically consist of setting\ncertain flight or environment conditions and testing the system under test in\nthese settings. The state-of-the-art approaches for this purpose also require\nmanual test scenario development and evaluation. In this paper, we propose a\nnovel approach to automate the system-level testing of the UAS. The proposed\napproach (AITester) utilizes model-based testing and artificial intelligence\n(AI) techniques to automatically generate, execute, and evaluate various test\nscenarios. The test scenarios are generated on the fly, i.e., during test\nexecution based on the environmental context at runtime. The approach is\nsupported by a toolset. We empirically evaluate the proposed approach on two\ncore components of UAS, an autopilot system of an unmanned aerial vehicle (UAV)\nand cockpit display systems (CDS) of the ground control station (GCS). The\nresults show that the AITester effectively generates test scenarios causing\ndeviations from the expected behavior of the UAV autopilot and reveals\npotential flaws in the GCS-CDS.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.SE",
    "comment": "Published in Automated Software Engineering",
    "pdf_url": "http://arxiv.org/pdf/2403.15857v2",
    "published_date": "2024-03-23 14:47:26 UTC",
    "updated_date": "2024-08-02 11:36:14 UTC"
  },
  {
    "arxiv_id": "2403.15855v3",
    "title": "Initialisation and Network Effects in Decentralised Federated Learning",
    "authors": [
      "Arash Badie-Modiri",
      "Chiara Boldrini",
      "Lorenzo Valerio",
      "János Kertész",
      "Márton Karsai"
    ],
    "abstract": "Fully decentralised federated learning enables collaborative training of\nindividual machine learning models on a distributed network of communicating\ndevices while keeping the training data localised on each node. This approach\navoids central coordination, enhances data privacy and eliminates the risk of a\nsingle point of failure. Our research highlights that the effectiveness of\ndecentralised federated learning is significantly influenced by the network\ntopology of connected devices and the learning models' initial conditions. We\npropose a strategy for uncoordinated initialisation of the artificial neural\nnetworks based on the distribution of eigenvector centralities of the\nunderlying communication network, leading to a radically improved training\nefficiency. Additionally, our study explores the scaling behaviour and the\nchoice of environmental parameters under our proposed initialisation strategy.\nThis work paves the way for more efficient and scalable artificial neural\nnetwork training in a distributed and uncoordinated environment, offering a\ndeeper understanding of the intertwining roles of network structure and\nlearning dynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "physics.soc-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15855v3",
    "published_date": "2024-03-23 14:24:36 UTC",
    "updated_date": "2024-11-05 18:27:11 UTC"
  },
  {
    "arxiv_id": "2403.15852v2",
    "title": "SOEN-101: Code Generation by Emulating Software Process Models Using Large Language Model Agents",
    "authors": [
      "Feng Lin",
      "Dong Jae Kim",
      "Tse-Husn",
      "Chen"
    ],
    "abstract": "Software process models are essential to facilitate collaboration and\ncommunication among software teams to solve complex development tasks. Inspired\nby these software engineering practices, we present FlowGen - a code generation\nframework that emulates software process models based on multiple Large\nLanguage Model (LLM) agents. We emulate three process models, FlowGenWaterfall,\nFlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e.,\nrequirement engineer, architect, developer, tester, and scrum master) that\ncorrespond to everyday development activities and organize their communication\npatterns. The agents work collaboratively using chain-of-thought and prompt\ncomposition with continuous self-refinement to improve the code quality. We use\nGPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion)\nto evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP,\nand MBPP-ET. Our findings show that FlowGenScrum excels compared to other\nprocess models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval,\nHumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement\nover RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum\nachieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming\nReflexion. Notably, integrating CodeT into FlowGenScrum resulted in\nstatistically significant improvements, achieving the highest Pass@1 scores.\nOur analysis also reveals that the development activities impacted code smell\nand exception handling differently, with design and code review adding more\nexception handling and reducing code smells. Finally, FlowGen models maintain\nstable Pass@1 scores across GPT3.5 versions and temperature values,\nhighlighting the effectiveness of software process models in enhancing the\nquality and stability of LLM-generated code.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "ICSE 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.15852v2",
    "published_date": "2024-03-23 14:04:48 UTC",
    "updated_date": "2024-10-31 14:43:58 UTC"
  },
  {
    "arxiv_id": "2403.15834v1",
    "title": "ARO: Large Language Model Supervised Robotics Text2Skill Autonomous Learning",
    "authors": [
      "Yiwen Chen",
      "Yuyao Ye",
      "Ziyi Chen",
      "Chuheng Zhang",
      "Marcelo H. Ang"
    ],
    "abstract": "Robotics learning highly relies on human expertise and efforts, such as\ndemonstrations, design of reward functions in reinforcement learning,\nperformance evaluation using human feedback, etc. However, reliance on human\nassistance can lead to expensive learning costs and make skill learning\ndifficult to scale. In this work, we introduce the Large Language Model\nSupervised Robotics Text2Skill Autonomous Learning (ARO) framework, which aims\nto replace human participation in the robot skill learning process with\nlarge-scale language models that incorporate reward function design and\nperformance evaluation. We provide evidence that our approach enables fully\nautonomous robot skill learning, capable of completing partial tasks without\nhuman intervention. Furthermore, we also analyze the limitations of this\napproach in task understanding and optimization stability.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.15834v1",
    "published_date": "2024-03-23 13:21:09 UTC",
    "updated_date": "2024-03-23 13:21:09 UTC"
  },
  {
    "arxiv_id": "2403.15826v2",
    "title": "Scaling Learning based Policy Optimization for Temporal Logic Tasks by Controller Network Dropout",
    "authors": [
      "Navid Hashemi",
      "Bardh Hoxha",
      "Danil Prokhorov",
      "Georgios Fainekos",
      "Jyotirmoy Deshmukh"
    ],
    "abstract": "This paper introduces a model-based approach for training feedback\ncontrollers for an autonomous agent operating in a highly nonlinear (albeit\ndeterministic) environment. We desire the trained policy to ensure that the\nagent satisfies specific task objectives and safety constraints, both expressed\nin Discrete-Time Signal Temporal Logic (DT-STL). One advantage for\nreformulation of a task via formal frameworks, like DT-STL, is that it permits\nquantitative satisfaction semantics. In other words, given a trajectory and a\nDT-STL formula, we can compute the {\\em robustness}, which can be interpreted\nas an approximate signed distance between the trajectory and the set of\ntrajectories satisfying the formula. We utilize feedback control, and we assume\na feed forward neural network for learning the feedback controller. We show how\nthis learning problem is similar to training recurrent neural networks (RNNs),\nwhere the number of recurrent units is proportional to the temporal horizon of\nthe agent's task objectives. This poses a challenge: RNNs are susceptible to\nvanishing and exploding gradients, and na\\\"{i}ve gradient descent-based\nstrategies to solve long-horizon task objectives thus suffer from the same\nproblems. To tackle this challenge, we introduce a novel gradient approximation\nalgorithm based on the idea of dropout or gradient sampling. One of the main\ncontributions is the notion of {\\em controller network dropout}, where we\napproximate the NN controller in several time-steps in the task horizon by the\ncontrol input obtained using the controller in a previous training step. We\nshow that our control synthesis methodology, can be quite helpful for\nstochastic gradient descent to converge with less numerical issues, enabling\nscalable backpropagation over long time horizons and trajectories over high\ndimensional state spaces.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15826v2",
    "published_date": "2024-03-23 12:53:51 UTC",
    "updated_date": "2024-08-27 22:18:07 UTC"
  },
  {
    "arxiv_id": "2403.15824v1",
    "title": "Carbon Intensity-Aware Adaptive Inference of DNNs",
    "authors": [
      "Jiwan Jung"
    ],
    "abstract": "DNN inference, known for its significant energy consumption and the resulting\nhigh carbon footprint, can be made more sustainable by adapting model size and\naccuracy to the varying carbon intensity throughout the day. Our heuristic\nalgorithm uses larger, high-accuracy models during low-intensity periods and\nsmaller, lower-accuracy ones during high-intensity periods. We also introduce a\nmetric, carbon-emission efficiency, which quantitatively measures the efficacy\nof adaptive model selection in terms of carbon footprint. The evaluation showed\nthat the proposed approach could improve the carbon emission efficiency in\nimproving the accuracy of vision recognition services by up to 80%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15824v1",
    "published_date": "2024-03-23 12:33:12 UTC",
    "updated_date": "2024-03-23 12:33:12 UTC"
  },
  {
    "arxiv_id": "2403.15812v1",
    "title": "The Impact of Evolutionary Computation on Robotic Design: A Case Study with an Underactuated Hand Exoskeleton",
    "authors": [
      "Baris Akbas",
      "Huseyin Taner Yuksel",
      "Aleyna Soylemez",
      "Mazhar Eid Zyada",
      "Mine Sarac",
      "Fabio Stroppa"
    ],
    "abstract": "Robotic exoskeletons can enhance human strength and aid people with physical\ndisabilities. However, designing them to ensure safety and optimal performance\npresents significant challenges. Developing exoskeletons should incorporate\nspecific optimization algorithms to find the best design. This study\ninvestigates the potential of Evolutionary Computation (EC) methods in robotic\ndesign optimization, with an underactuated hand exoskeleton (U-HEx) used as a\ncase study. We propose improving the performance and usability of the U-HEx\ndesign, which was initially optimized using a naive brute-force approach, by\nintegrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch\nAlgorithm. Comparative analysis revealed that EC methods consistently yield\nmore precise and optimal solutions than brute force in a significantly shorter\ntime. This allowed us to improve the optimization by increasing the number of\nvariables in the design, which was impossible with naive methods. The results\nshow significant improvements in terms of the torque magnitude the device\ntransfers to the user, enhancing its efficiency. These findings underline the\nimportance of performing proper optimization while designing exoskeletons, as\nwell as providing a significant improvement to this specific robotic design.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages (+ref), 4 figures, IEEE International Conference on Robotics\n  and Automation (ICRA) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15812v1",
    "published_date": "2024-03-23 11:50:20 UTC",
    "updated_date": "2024-03-23 11:50:20 UTC"
  },
  {
    "arxiv_id": "2403.15807v1",
    "title": "Efficient Data Access Paths for Mixed Vector-Relational Search",
    "authors": [
      "Viktor Sanca",
      "Anastasia Ailamaki"
    ],
    "abstract": "The rapid growth of machine learning capabilities and the adoption of data\nprocessing methods using vector embeddings sparked a great interest in creating\nsystems for vector data management. While the predominant approach of vector\ndata management is to use specialized index structures for fast search over the\nentirety of the vector embeddings, once combined with other (meta)data, the\nsearch queries can also become selective on relational attributes - typical for\nanalytical queries. As using vector indexes differs from traditional relational\ndata access, we revisit and analyze alternative access paths for efficient\nmixed vector-relational search.\n  We first evaluate the accurate but exhaustive scan-based search and propose\nhardware optimizations and alternative tensor-based formulation and batching to\noffset the cost. We outline the complex access-path design space, primarily\ndriven by relational selectivity, and the decisions to consider when selecting\nan exhaustive scan-based search against an approximate index-based approach.\nSince the vector index primarily avoids expensive computation across the entire\ndataset, contrary to the common relational knowledge, it is better to scan at\nlower selectivity and probe at higher, with a cross-point between the two\napproaches dictated by data dimensionality and the number of concurrent search\nqueries.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15807v1",
    "published_date": "2024-03-23 11:34:17 UTC",
    "updated_date": "2024-03-23 11:34:17 UTC"
  },
  {
    "arxiv_id": "2403.15796v3",
    "title": "Understanding Emergent Abilities of Language Models from the Loss Perspective",
    "authors": [
      "Zhengxiao Du",
      "Aohan Zeng",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "abstract": "Recent studies have put into question the belief that emergent abilities in\nlanguage models are exclusive to large models. This skepticism arises from two\nobservations: 1) smaller models can also exhibit high performance on emergent\nabilities and 2) there is doubt on the discontinuous metrics used to measure\nthese abilities. In this paper, we propose to study emergent abilities in the\nlens of pre-training loss, instead of model size or training compute. We\ndemonstrate that the Transformer models with the same pre-training loss, but\ndifferent model and data sizes, generate the same performance on various\ndownstream tasks, with a fixed data corpus, tokenization, and model\narchitecture. We also discover that a model exhibits emergent abilities on\ncertain tasks -- regardless of the continuity of metrics -- when its\npre-training loss falls below a specific threshold. Before reaching this\nthreshold, its performance remains at the level of random guessing. This\ninspires us to redefine emergent abilities as those that manifest in models\nwith lower pre-training losses, highlighting that these abilities cannot be\npredicted by merely extrapolating the performance trends of models with higher\npre-training losses.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 8 figures. Accepted in NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15796v3",
    "published_date": "2024-03-23 11:03:31 UTC",
    "updated_date": "2025-01-15 02:48:59 UTC"
  },
  {
    "arxiv_id": "2403.15779v1",
    "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
    "authors": [
      "Youyang Qu",
      "Ming Ding",
      "Nan Sun",
      "Kanchana Thilakarathna",
      "Tianqing Zhu",
      "Dusit Niyato"
    ],
    "abstract": "Large Language Models (LLMs) are foundational to AI advancements,\nfacilitating applications like predictive text generation. Nonetheless, they\npose risks by potentially memorizing and disseminating sensitive, biased, or\ncopyrighted information from their vast datasets. Machine unlearning emerges as\na cutting-edge solution to mitigate these concerns, offering techniques for\nLLMs to selectively discard certain data. This paper reviews the latest in\nmachine unlearning for LLMs, introducing methods for the targeted forgetting of\ninformation to address privacy, ethical, and legal challenges without\nnecessitating full model retraining. It divides existing research into\nunlearning from unstructured/textual data and structured/classification data,\nshowcasing the effectiveness of these approaches in removing specific data\nwhile maintaining model efficacy. Highlighting the practicality of machine\nunlearning, this analysis also points out the hurdles in preserving model\nintegrity, avoiding excessive or insufficient data removal, and ensuring\nconsistent outputs, underlining the role of machine unlearning in advancing\nresponsible, ethical AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15779v1",
    "published_date": "2024-03-23 09:26:15 UTC",
    "updated_date": "2024-03-23 09:26:15 UTC"
  },
  {
    "arxiv_id": "2403.15776v1",
    "title": "Modeling Unified Semantic Discourse Structure for High-quality Headline Generation",
    "authors": [
      "Minghui Xu",
      "Hao Fei",
      "Fei Li",
      "Shengqiong Wu",
      "Rui Sun",
      "Chong Teng",
      "Donghong Ji"
    ],
    "abstract": "Headline generation aims to summarize a long document with a short, catchy\ntitle that reflects the main idea. This requires accurately capturing the core\ndocument semantics, which is challenging due to the lengthy and background\ninformation-rich na ture of the texts. In this work, We propose using a unified\nsemantic discourse structure (S3) to represent document semantics, achieved by\ncombining document-level rhetorical structure theory (RST) trees with\nsentence-level abstract meaning representation (AMR) graphs to construct S3\ngraphs. The hierarchical composition of sentence, clause, and word\nintrinsically characterizes the semantic meaning of the overall document. We\nthen develop a headline generation framework, in which the S3 graphs are\nencoded as contextual features. To consolidate the efficacy of S3 graphs, we\nfurther devise a hierarchical structure pruning mechanism to dynamically screen\nthe redundant and nonessential nodes within the graph. Experimental results on\ntwo headline generation datasets demonstrate that our method outperforms\nexisting state-of-art methods consistently. Our work can be instructive for a\nbroad range of document modeling tasks, more than headline or summarization\ngeneration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15776v1",
    "published_date": "2024-03-23 09:18:53 UTC",
    "updated_date": "2024-03-23 09:18:53 UTC"
  },
  {
    "arxiv_id": "2403.15769v3",
    "title": "FusionINN: Decomposable Image Fusion for Brain Tumor Monitoring",
    "authors": [
      "Nishant Kumar",
      "Ziyan Tao",
      "Jaikirat Singh",
      "Yang Li",
      "Peiwen Sun",
      "Binghui Zhao",
      "Stefan Gumhold"
    ],
    "abstract": "Image fusion typically employs non-invertible neural networks to merge\nmultiple source images into a single fused image. However, for clinical\nexperts, solely relying on fused images may be insufficient for making\ndiagnostic decisions, as the fusion mechanism blends features from source\nimages, thereby making it difficult to interpret the underlying tumor\npathology. We introduce FusionINN, a novel decomposable image fusion framework,\ncapable of efficiently generating fused images and also decomposing them back\nto the source images. FusionINN is designed to be bijective by including a\nlatent image alongside the fused image, while ensuring minimal transfer of\ninformation from the source images to the latent representation. To the best of\nour knowledge, we are the first to investigate the decomposability of fused\nimages, which is particularly crucial for life-sensitive applications such as\nmedical image fusion compared to other tasks like multi-focus or multi-exposure\nimage fusion. Our extensive experimentation validates FusionINN over existing\ndiscriminative and generative fusion methods, both subjectively and\nobjectively. Moreover, compared to a recent denoising diffusion-based fusion\nmodel, our approach offers faster and qualitatively better fusion results.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted at IJCAI Workshop 2024. Source code available at\n  https://github.com/nish03/FusionINN",
    "pdf_url": "http://arxiv.org/pdf/2403.15769v3",
    "published_date": "2024-03-23 08:54:03 UTC",
    "updated_date": "2024-06-10 13:09:53 UTC"
  },
  {
    "arxiv_id": "2403.15766v1",
    "title": "BEND: Bagging Deep Learning Training Based on Efficient Neural Network Diffusion",
    "authors": [
      "Jia Wei",
      "Xingjun Zhang",
      "Witold Pedrycz"
    ],
    "abstract": "Bagging has achieved great success in the field of machine learning by\nintegrating multiple base classifiers to build a single strong classifier to\nreduce model variance. The performance improvement of bagging mainly relies on\nthe number and diversity of base classifiers. However, traditional deep\nlearning model training methods are expensive to train individually and\ndifficult to train multiple models with low similarity in a restricted dataset.\nRecently, diffusion models, which have been tremendously successful in the\nfields of imaging and vision, have been found to be effective in generating\nneural network model weights and biases with diversity. We creatively propose a\nBagging deep learning training algorithm based on Efficient Neural network\nDiffusion (BEND). The originality of BEND comes from the first use of a neural\nnetwork diffusion model to efficiently build base classifiers for bagging. Our\napproach is simple but effective, first using multiple trained model weights\nand biases as inputs to train autoencoder and latent diffusion model to realize\na diffusion model from noise to valid neural network parameters. Subsequently,\nwe generate several base classifiers using the trained diffusion model.\nFinally, we integrate these ba se classifiers for various inference tasks using\nthe Bagging method. Resulting experiments on multiple models and datasets show\nthat our proposed BEND algorithm can consistently outperform the mean and\nmedian accuracies of both the original trained model and the diffused model. At\nthe same time, new models diffused using the diffusion model have higher\ndiversity and lower cost than multiple models trained using traditional\nmethods. The BEND approach successfully introduces diffusion models into the\nnew deep learning training domain and provides a new paradigm for future deep\nlearning training and inference.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15766v1",
    "published_date": "2024-03-23 08:40:38 UTC",
    "updated_date": "2024-03-23 08:40:38 UTC"
  },
  {
    "arxiv_id": "2403.15765v1",
    "title": "Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents",
    "authors": [
      "Hao Wang",
      "Tang Li",
      "Chenhui Chu",
      "Nengjun Zhu",
      "Rui Wang",
      "Pinpin Zhu"
    ],
    "abstract": "Key-value relations are prevalent in Visually-Rich Documents (VRDs), often\ndepicted in distinct spatial regions accompanied by specific color and font\nstyles. These non-textual cues serve as important indicators that greatly\nenhance human comprehension and acquisition of such relation triplets. However,\ncurrent document AI approaches often fail to consider this valuable prior\ninformation related to visual and spatial features, resulting in suboptimal\nperformance, particularly when dealing with limited examples. To address this\nlimitation, our research focuses on few-shot relational learning, specifically\ntargeting the extraction of key-value relation triplets in VRDs. Given the\nabsence of a suitable dataset for this task, we introduce two new few-shot\nbenchmarks built upon existing supervised benchmark datasets. Furthermore, we\npropose a variational approach that incorporates relational 2D-spatial priors\nand prototypical rectification techniques. This approach aims to generate\nrelation representations that are more aware of the spatial context and unseen\nrelation in a manner similar to human perception. Experimental results\ndemonstrate the effectiveness of our proposed method by showcasing its ability\nto outperform existing methods. This study also opens up new possibilities for\npractical applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 7 figures, accepted by LERC-COLING2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15765v1",
    "published_date": "2024-03-23 08:40:35 UTC",
    "updated_date": "2024-03-23 08:40:35 UTC"
  },
  {
    "arxiv_id": "2403.15760v2",
    "title": "An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning",
    "authors": [
      "Jianqing Zhang",
      "Yang Liu",
      "Yang Hua",
      "Jian Cao"
    ],
    "abstract": "Heterogeneous Federated Learning (HtFL) enables task-specific knowledge\nsharing among clients with different model architectures while preserving\nprivacy. Despite recent research progress, transferring knowledge in HtFL is\nstill difficult due to data and model heterogeneity. To tackle this, we\nintroduce a public pre-trained generator (e.g., StyleGAN or Stable Diffusion)\nas the bridge and propose a new upload-efficient knowledge transfer scheme\ncalled Federated Knowledge-Transfer-Loop (FedKTL). It can produce task-related\nprototypical image-vector pairs via the generator's inference on the server.\nWith these pairs, each client can transfer common knowledge from the generator\nto its local model through an additional supervised local task. We conduct\nextensive experiments on four datasets under two types of data heterogeneity\nwith 14 heterogeneous models, including CNNs and ViTs. Results show that our\nFedKTL surpasses seven state-of-the-art methods by up to 7.31%. Moreover, our\nknowledge transfer scheme is applicable in cloud-edge scenarios with only one\nedge client. Code: https://github.com/TsingZ0/FedKTL",
    "categories": [
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by CVPR2024. We have incorporated additional analysis for\n  the Stable Diffusion experiments in Appendix A",
    "pdf_url": "http://arxiv.org/pdf/2403.15760v2",
    "published_date": "2024-03-23 08:24:09 UTC",
    "updated_date": "2024-08-19 12:59:53 UTC"
  },
  {
    "arxiv_id": "2403.15757v1",
    "title": "User-Side Realization",
    "authors": [
      "Ryoma Sato"
    ],
    "abstract": "Users are dissatisfied with services. Since the service is not tailor-made\nfor a user, it is natural for dissatisfaction to arise. The problem is, that\neven if users are dissatisfied, they often do not have the means to resolve\ntheir dissatisfaction. The user cannot alter the source code of the service,\nnor can they force the service provider to change. The user has no choice but\nto remain dissatisfied or quit the service. User-side realization offers\nproactive solutions to this problem by providing general algorithms to deal\nwith common problems on the user's side. These algorithms run on the user's\nside and solve the problems without having the service provider change the\nservice itself.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Doctoral Thesis",
    "pdf_url": "http://arxiv.org/pdf/2403.15757v1",
    "published_date": "2024-03-23 08:03:50 UTC",
    "updated_date": "2024-03-23 08:03:50 UTC"
  },
  {
    "arxiv_id": "2403.15756v1",
    "title": "Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study",
    "authors": [
      "Matteo Esposito",
      "Francesco Palagiano"
    ],
    "abstract": "Preliminary security risk analysis (PSRA) provides a quick approach to\nidentify, evaluate and propose remeditation to potential risks in specific\nscenarios. The extensive expertise required for an effective PSRA and the\nsubstantial ammount of textual-related tasks hinder quick assessments in\nmission-critical contexts, where timely and prompt actions are essential. The\nspeed and accuracy of human experts in PSRA significantly impact response time.\nA large language model can quickly summarise information in less time than a\nhuman. To our knowledge, no prior study has explored the capabilities of\nfine-tuned models (FTM) in PSRA. Our case study investigates the proficiency of\nFTM to assist practitioners in PSRA. We manually curated 141 representative\nsamples from over 50 mission-critical analyses archived by the industrial\ncontext team in the last five years.We compared the proficiency of the FTM\nversus seven human experts. Within the industrial context, our approach has\nproven successful in reducing errors in PSRA, hastening security risk\ndetection, and minimizing false positives and negatives. This translates to\ncost savings for the company by averting unnecessary expenses associated with\nimplementing unwarranted countermeasures. Therefore, experts can focus on more\ncomprehensive risk analysis, leveraging LLMs for an effective preliminary\nassessment within a condensed timeframe.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CY"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15756v1",
    "published_date": "2024-03-23 07:59:30 UTC",
    "updated_date": "2024-03-23 07:59:30 UTC"
  },
  {
    "arxiv_id": "2403.15747v1",
    "title": "CodeShell Technical Report",
    "authors": [
      "Rui Xie",
      "Zhengran Zeng",
      "Zhuohao Yu",
      "Chang Gao",
      "Shikun Zhang",
      "Wei Ye"
    ],
    "abstract": "Code large language models mark a pivotal breakthrough in artificial\nintelligence. They are specifically crafted to understand and generate\nprogramming languages, significantly boosting the efficiency of coding\ndevelopment workflows. In this technical report, we present CodeShell-Base, a\nseven billion-parameter foundation model with 8K context length, showcasing\nexceptional proficiency in code comprehension. By incorporating Grouped-Query\nAttention and Rotary Positional Embedding into GPT-2, CodeShell-Base integrates\nthe structural merits of StarCoder and CodeLlama and forms its unique\narchitectural design. We then carefully built a comprehensive data\npre-processing process, including similar data deduplication, perplexity-based\ndata filtering, and model-based data filtering. Through this process, We have\ncurated 100 billion high-quality pre-training data from GitHub. Benefiting from\nthe high-quality data, CodeShell-Base outperforms CodeLlama in Humaneval after\ntraining on just 500 billion tokens (5 epochs). We have conducted extensive\nexperiments across multiple language datasets, including Python, Java, and C++,\nand the results indicate that our model possesses robust foundational\ncapabilities in code comprehension and generation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15747v1",
    "published_date": "2024-03-23 07:29:41 UTC",
    "updated_date": "2024-03-23 07:29:41 UTC"
  },
  {
    "arxiv_id": "2403.15743v2",
    "title": "A Comparative Study of Artificial Potential Fields and Reciprocal Control Barrier Function-based Safety Filters",
    "authors": [
      "Ming Li",
      "Zhiyong Sun"
    ],
    "abstract": "In this paper, we demonstrate that controllers designed by artificial\npotential fields (APFs) can be derived from reciprocal control barrier function\nquadratic program (RCBF-QP) safety filters. By integrating APFs within the\nRCBF-QP framework, we explicitly establish the relationship between these two\napproaches. Specifically, we first introduce the concepts of tightened control\nLyapunov functions (T-CLFs) and tightened reciprocal control barrier functions\n(T-RCBFs), each of which incorporates a flexible auxiliary function. We then\nutilize an attractive potential field as a T-CLF to guide the nominal\ncontroller design, and a repulsive potential field as a T-RCBF to formulate an\nRCBF-QP safety filter. With appropriately chosen auxiliary functions, we show\nthat controllers designed by APFs and those derived by RCBF-QP safety filters\nare equivalent. Based on this insight, we further generalize the APF-based\ncontrollers (equivalently, RCBF-QP safety filter-based controllers) to more\ngeneral scenarios without restricting the choice of auxiliary functions.\nFinally, we present a collision avoidance example to clearly illustrate the\nconnection and equivalence between the two methods.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY",
      "math.DS"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15743v2",
    "published_date": "2024-03-23 07:14:27 UTC",
    "updated_date": "2025-04-16 08:37:28 UTC"
  },
  {
    "arxiv_id": "2403.15729v3",
    "title": "Towards a RAG-based Summarization Agent for the Electron-Ion Collider",
    "authors": [
      "Karthik Suresh",
      "Neeltje Kackar",
      "Luke Schleck",
      "Cristiano Fanelli"
    ],
    "abstract": "The complexity and sheer volume of information encompassing documents,\npapers, data, and other resources from large-scale experiments demand\nsignificant time and effort to navigate, making the task of accessing and\nutilizing these varied forms of information daunting, particularly for new\ncollaborators and early-career scientists. To tackle this issue, a Retrieval\nAugmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under\ndevelopment. This AI-Agent not only condenses information but also effectively\nreferences relevant responses, offering substantial advantages for\ncollaborators. Our project involves a two-step approach: first, querying a\ncomprehensive vector database containing all pertinent experiment information;\nsecond, utilizing a Large Language Model (LLM) to generate concise summaries\nenriched with citations based on user queries and retrieved data. We describe\nthe evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to\nassess the effectiveness of responses. Furthermore, we describe the concept of\nprompt template-based instruction-tuning which provides flexibility and\naccuracy in summarization. Importantly, the implementation relies on LangChain,\nwhich serves as the foundation of our entire workflow. This integration ensures\nefficiency and scalability, facilitating smooth deployment and accessibility\nfor various user groups within the Electron Ion Collider (EIC) community. This\ninnovative AI-driven framework not only simplifies the understanding of vast\ndatasets but also encourages collaborative participation, thereby empowering\nresearchers. As a demonstration, a web application has been developed to\nexplain each stage of the RAG Agent development in detail.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "hep-ex",
      "physics.ins-det"
    ],
    "primary_category": "cs.CL",
    "comment": "updated title to have no latex formatting and added acknowledgements",
    "pdf_url": "http://arxiv.org/pdf/2403.15729v3",
    "published_date": "2024-03-23 05:32:46 UTC",
    "updated_date": "2024-06-08 01:15:05 UTC"
  },
  {
    "arxiv_id": "2403.15728v1",
    "title": "Learnable WSN Deployment of Evidential Collaborative Sensing Model",
    "authors": [
      "Ruijie Liu",
      "Tianxiang Zhan",
      "Zhen Li",
      "Yong Deng"
    ],
    "abstract": "In wireless sensor networks (WSNs), coverage and deployment are two most\ncrucial issues when conducting detection tasks. However, the detection\ninformation collected from sensors is oftentimes not fully utilized and\nefficiently integrated. Such sensing model and deployment strategy, thereby,\ncannot reach the maximum quality of coverage, particularly when the amount of\nsensors within WSNs expands significantly. In this article, we aim at achieving\nthe optimal coverage quality of WSN deployment. We develop a collaborative\nsensing model of sensors to enhance detection capabilities of WSNs, by\nleveraging the collaborative information derived from the combination rule\nunder the framework of evidence theory. In this model, the performance\nevaluation of evidential fusion systems is adopted as the criterion of the\nsensor selection. A learnable sensor deployment network (LSDNet) considering\nboth sensor contribution and detection capability, is proposed for achieving\nthe optimal deployment of WSNs. Moreover, we deeply investigate the algorithm\nfor finding the requisite minimum number of sensors that realizes the full\ncoverage of WSNs. A series of numerical examples, along with an application of\nforest area monitoring, are employed to demonstrate the effectiveness and the\nrobustness of the proposed algorithms.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15728v1",
    "published_date": "2024-03-23 05:29:09 UTC",
    "updated_date": "2024-03-23 05:29:09 UTC"
  },
  {
    "arxiv_id": "2403.15724v1",
    "title": "PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific Documents",
    "authors": [
      "Nan Zhang",
      "Connor Heaton",
      "Sean Timothy Okonsky",
      "Prasenjit Mitra",
      "Hilal Ezgi Toraman"
    ],
    "abstract": "Optical Character Recognition (OCR) is an established task with the objective\nof identifying the text present in an image. While many off-the-shelf OCR\nmodels exist, they are often trained for either scientific (e.g., formulae) or\ngeneric printed English text. Extracting text from chemistry publications\nrequires an OCR model that is capable in both realms. Nougat, a recent tool,\nexhibits strong ability to parse academic documents, but is unable to parse\ntables in PubMed articles, which comprises a significant part of the academic\ncommunity and is the focus of this work. To mitigate this gap, we present the\nPrinted English and Chemical Equations (PEaCE) dataset, containing both\nsynthetic and real-world records, and evaluate the efficacy of\ntransformer-based OCR models when trained on this resource. Given that\nreal-world records contain artifacts not present in synthetic records, we\npropose transformations that mimic such qualities. We perform a suite of\nexperiments to explore the impact of patch size, multi-domain training, and our\nproposed transformations, ultimately finding that models with a small patch\nsize trained on multiple domains using the proposed transformations yield the\nbest performance. Our dataset and code is available at\nhttps://github.com/ZN1010/PEaCE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15724v1",
    "published_date": "2024-03-23 05:20:36 UTC",
    "updated_date": "2024-03-23 05:20:36 UTC"
  },
  {
    "arxiv_id": "2403.15716v1",
    "title": "Distributed Robust Learning based Formation Control of Mobile Robots based on Bioinspired Neural Dynamics",
    "authors": [
      "Zhe Xu",
      "Tao Yan",
      "Simon X. Yang",
      "S. Andrew Gadsden",
      "Mohammad Biglarbegian"
    ],
    "abstract": "This paper addresses the challenges of distributed formation control in\nmultiple mobile robots, introducing a novel approach that enhances real-world\npracticability. We first introduce a distributed estimator using a variable\nstructure and cascaded design technique, eliminating the need for derivative\ninformation to improve the real time performance. Then, a kinematic tracking\ncontrol method is developed utilizing a bioinspired neural dynamic-based\napproach aimed at providing smooth control inputs and effectively resolving the\nspeed jump issue. Furthermore, to address the challenges for robots operating\nwith completely unknown dynamics and disturbances, a learning-based robust\ndynamic controller is developed. This controller provides real time parameter\nestimates while maintaining its robustness against disturbances. The overall\nstability of the proposed method is proved with rigorous mathematical analysis.\nAt last, multiple comprehensive simulation studies have shown the advantages\nand effectiveness of the proposed method.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "This paper is accepted by IEEE Transactions on Intelligent Vehicles",
    "pdf_url": "http://arxiv.org/pdf/2403.15716v1",
    "published_date": "2024-03-23 04:36:12 UTC",
    "updated_date": "2024-03-23 04:36:12 UTC"
  },
  {
    "arxiv_id": "2403.15709v2",
    "title": "Contact-aware Human Motion Generation from Textual Descriptions",
    "authors": [
      "Sihan Ma",
      "Qiong Cao",
      "Jing Zhang",
      "Dacheng Tao"
    ],
    "abstract": "This paper addresses the problem of generating 3D interactive human motion\nfrom text. Given a textual description depicting the actions of different body\nparts in contact with static objects, we synthesize sequences of 3D body poses\nthat are visually natural and physically plausible. Yet, this task poses a\nsignificant challenge due to the inadequate consideration of interactions by\nphysical contacts in both motion and textual descriptions, leading to unnatural\nand implausible sequences. To tackle this challenge, we create a novel dataset\nnamed RICH-CAT, representing \"Contact-Aware Texts\" constructed from the RICH\ndataset. RICH-CAT comprises high-quality motion, accurate human-object contact\nlabels, and detailed textual descriptions, encompassing over 8,500 motion-text\npairs across 26 indoor/outdoor actions. Leveraging RICH-CAT, we propose a novel\napproach named CATMO for text-driven interactive human motion synthesis that\nexplicitly integrates human body contacts as evidence. We employ two VQ-VAE\nmodels to encode motion and body contact sequences into distinct yet\ncomplementary latent spaces and an intertwined GPT for generating human motions\nand contacts in a mutually conditioned manner. Additionally, we introduce a\npre-trained text encoder to learn textual embeddings that better discriminate\namong various contact types, allowing for more precise control over synthesized\nmotions and contacts. Our experiments demonstrate the superior performance of\nour approach compared to existing text-to-motion methods, producing stable,\ncontact-aware motion sequences. Code and data will be available for research\npurposes at https://xymsh.github.io/RICH-CAT/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://xymsh.github.io/RICH-CAT/",
    "pdf_url": "http://arxiv.org/pdf/2403.15709v2",
    "published_date": "2024-03-23 04:08:39 UTC",
    "updated_date": "2024-09-14 02:54:11 UTC"
  },
  {
    "arxiv_id": "2403.15707v1",
    "title": "Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs",
    "authors": [
      "Aakash Lahoti",
      "Stefani Karp",
      "Ezra Winston",
      "Aarti Singh",
      "Yuanzhi Li"
    ],
    "abstract": "Vision tasks are characterized by the properties of locality and translation\ninvariance. The superior performance of convolutional neural networks (CNNs) on\nthese tasks is widely attributed to the inductive bias of locality and weight\nsharing baked into their architecture. Existing attempts to quantify the\nstatistical benefits of these biases in CNNs over locally connected\nconvolutional neural networks (LCNs) and fully connected neural networks (FCNs)\nfall into one of the following categories: either they disregard the optimizer\nand only provide uniform convergence upper bounds with no separating lower\nbounds, or they consider simplistic tasks that do not truly mirror the locality\nand translation invariance as found in real-world vision tasks. To address\nthese deficiencies, we introduce the Dynamic Signal Distribution (DSD)\nclassification task that models an image as consisting of $k$ patches, each of\ndimension $d$, and the label is determined by a $d$-sparse signal vector that\ncan freely appear in any one of the $k$ patches. On this task, for any\northogonally equivariant algorithm like gradient descent, we prove that CNNs\nrequire $\\tilde{O}(k+d)$ samples, whereas LCNs require $\\Omega(kd)$ samples,\nestablishing the statistical advantages of weight sharing in translation\ninvariant tasks. Furthermore, LCNs need $\\tilde{O}(k(k+d))$ samples, compared\nto $\\Omega(k^2d)$ samples for FCNs, showcasing the benefits of locality in\nlocal tasks. Additionally, we develop information theoretic tools for analyzing\nrandomized algorithms, which may be of interest for statistical research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "40 pages, 4 figures, Accepted to ICLR 2024, Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2403.15707v1",
    "published_date": "2024-03-23 03:57:28 UTC",
    "updated_date": "2024-03-23 03:57:28 UTC"
  },
  {
    "arxiv_id": "2403.15698v3",
    "title": "SceneX: Procedural Controllable Large-scale Scene Generation",
    "authors": [
      "Mengqi Zhou",
      "Yuxi Wang",
      "Jun Hou",
      "Shougao Zhang",
      "Yiwei Li",
      "Chuanchen Luo",
      "Junran Peng",
      "Zhaoxiang Zhang"
    ],
    "abstract": "Developing comprehensive explicit world models is crucial for understanding\nand simulating real-world scenarios. Recently, Procedural Controllable\nGeneration (PCG) has gained significant attention in large-scale scene\ngeneration by enabling the creation of scalable, high-quality assets. However,\nPCG faces challenges such as limited modular diversity, high expertise\nrequirements, and challenges in managing the diverse elements and structures in\ncomplex scenes. In this paper, we introduce a large-scale scene generation\nframework, SceneX, which can automatically produce high-quality procedural\nmodels according to designers' textual descriptions. Specifically, the proposed\nmethod comprises two components, PCGHub and PCGPlanner. The former encompasses\nan extensive collection of accessible procedural assets and thousands of\nhand-craft API documents to perform as a standard protocol for PCG controller.\nThe latter aims to generate executable actions for Blender to produce\ncontrollable and precise 3D assets guided by the user's instructions. Extensive\nexperiments demonstrated the capability of our method in controllable\nlarge-scale scene generation, including nature scenes and unbounded cities, as\nwell as scene editing such as asset placement and season translation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15698v3",
    "published_date": "2024-03-23 03:23:29 UTC",
    "updated_date": "2024-12-17 14:39:07 UTC"
  },
  {
    "arxiv_id": "2403.15696v1",
    "title": "MixRED: A Mix-lingual Relation Extraction Dataset",
    "authors": [
      "Lingxing Kong",
      "Yougang Chu",
      "Zheng Ma",
      "Jianbing Zhang",
      "Liang He",
      "Jiajun Chen"
    ],
    "abstract": "Relation extraction is a critical task in the field of natural language\nprocessing with numerous real-world applications. Existing research primarily\nfocuses on monolingual relation extraction or cross-lingual enhancement for\nrelation extraction. Yet, there remains a significant gap in understanding\nrelation extraction in the mix-lingual (or code-switching) scenario, where\nindividuals intermix contents from different languages within sentences,\ngenerating mix-lingual content. Due to the lack of a dedicated dataset, the\neffectiveness of existing relation extraction models in such a scenario is\nlargely unexplored. To address this issue, we introduce a novel task of\nconsidering relation extraction in the mix-lingual scenario called MixRE and\nconstructing the human-annotated dataset MixRED to support this task. In\naddition to constructing the MixRED dataset, we evaluate both state-of-the-art\nsupervised models and large language models (LLMs) on MixRED, revealing their\nrespective advantages and limitations in the mix-lingual scenario. Furthermore,\nwe delve into factors influencing model performance within the MixRE task and\nuncover promising directions for enhancing the performance of both supervised\nmodels and LLMs in this novel task.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15696v1",
    "published_date": "2024-03-23 03:18:14 UTC",
    "updated_date": "2024-03-23 03:18:14 UTC"
  },
  {
    "arxiv_id": "2403.15690v1",
    "title": "EAGLE: A Domain Generalization Framework for AI-generated Text Detection",
    "authors": [
      "Amrita Bhattacharjee",
      "Raha Moraffah",
      "Joshua Garland",
      "Huan Liu"
    ],
    "abstract": "With the advancement in capabilities of Large Language Models (LLMs), one\nmajor step in the responsible and safe use of such LLMs is to be able to detect\ntext generated by these models. While supervised AI-generated text detectors\nperform well on text generated by older LLMs, with the frequent release of new\nLLMs, building supervised detectors for identifying text from such new models\nwould require new labeled training data, which is infeasible in practice. In\nthis work, we tackle this problem and propose a domain generalization framework\nfor the detection of AI-generated text from unseen target generators. Our\nproposed framework, EAGLE, leverages the labeled data that is available so far\nfrom older language models and learns features invariant across these\ngenerators, in order to detect text generated by an unknown target generator.\nEAGLE learns such domain-invariant features by combining the representational\npower of self-supervised contrastive learning with domain adversarial training.\nThrough our experiments we demonstrate how EAGLE effectively achieves\nimpressive performance in detecting text generated by unseen target generators,\nincluding recent state-of-the-art ones such as GPT-4 and Claude, reaching\ndetection scores of within 4.7% of a fully supervised detector.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15690v1",
    "published_date": "2024-03-23 02:44:20 UTC",
    "updated_date": "2024-03-23 02:44:20 UTC"
  },
  {
    "arxiv_id": "2406.08492v1",
    "title": "ASI as the New God: Technocratic Theocracy",
    "authors": [
      "Tevfik Uyar"
    ],
    "abstract": "As Artificial General Intelligence edges closer to reality, Artificial\nSuperintelligence does too. This paper argues that ASI's unparalleled\ncapabilities might lead people to attribute godlike infallibility to it,\nresulting in a cognitive bias toward unquestioning acceptance of its decisions.\nBy drawing parallels between ASI and divine attributes such as omnipotence,\nomniscience, and omnipresence, this analysis highlights the risks of conflating\ntechnological advancement with moral and ethical superiority. Such dynamics\ncould engender a technocratic theocracy, where decision-making is abdicated to\nASI, undermining human agency and critical thinking.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "K.4.2"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2406.08492v1",
    "published_date": "2024-03-23 00:48:11 UTC",
    "updated_date": "2024-03-23 00:48:11 UTC"
  }
]
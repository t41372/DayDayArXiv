{
  "date": "2025-12-18",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-12-18 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„ AI ç ”ç©¶å‘˜æœ‹å‹ã€‚**\n\n**ä»Šæ—¥æ€»è¯„ï¼š**\nä»Šå¤©çš„ arXiv åˆ—è¡¨å¼‚å¸¸æ‹¥æŒ¤ï¼ˆ168ç¯‡ï¼ï¼‰ï¼Œå¯è°“æ˜¯â€œç¥ä»™æ‰“æ¶â€çš„ä¸€å¤©ã€‚**å¤§æ¨¡å‹æ¶æ„**æ–¹é¢ï¼ŒMoEï¼ˆæ··åˆä¸“å®¶æ¨¡å‹ï¼‰ç»§ç»­å·å‡ºæ–°é«˜åº¦ï¼Œä¸ä»…æœ‰ 100B å‚æ•°çº§çš„ INTELLECT-3ï¼Œè¿˜æœ‰æè‡´ç¨€ç–çš„ Sigma-MoE-Tinyã€‚**è§†é¢‘ç”Ÿæˆ**é¢†åŸŸè¿æ¥é‡ç£…ç‚¸å¼¹ TurboDiffusionï¼Œå·ç§°èƒ½åŠ é€Ÿ 100-200 å€ã€‚**Agentï¼ˆæ™ºèƒ½ä½“ï¼‰**çš„ç ”ç©¶é‡å¿ƒæ˜æ˜¾è½¬å‘äº†å®‰å…¨æ€§ï¼ˆéšç§æ³„éœ²ï¼‰å’Œç§‘å­¦åº”ç”¨ï¼ˆPDEæ±‚è§£ï¼‰ã€‚æ­¤å¤–ï¼Œä¸€ç¯‡é¢˜ä¸ºâ€œAI éœ€è¦ç‰©ç†å­¦èƒœè¿‡ç‰©ç†å­¦éœ€è¦ AIâ€çš„è¯„è®ºæ–‡ç« å¯èƒ½ä¼šåœ¨å­¦æœ¯ç•Œå¼•å‘ä¸å°çš„è®¨è®ºã€‚\n\nä¸‹é¢è®©æˆ‘ä»¬è¿›å…¥æ·±è¯»ç¯èŠ‚ï¼Œè¿™æ˜¯æˆ‘ä¸ºä½ ç²¾é€‰çš„ä»Šæ—¥å¿…è¯»ã€‚\n\n---\n\n### ğŸš€ æ ¸å¿ƒæ¨¡å‹ä¸æ¶æ„åˆ›æ–° (Model Architecture & Scaling)\n\n**1. [æ¨è] INTELLECT-3: Technical Report**\n> **INTELLECT-3ï¼šæŠ€æœ¯æŠ¥å‘Š**\n> *å…³é”®è¯ï¼šMoE, å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ , å¼€æºåŸºç¡€è®¾æ–½*\n> **TLDR:** Prime Intellect å›¢é˜Ÿå‘å¸ƒäº† INTELLECT-3ï¼Œè¿™æ˜¯ä¸€ä¸ª 106B å‚æ•°ï¼ˆæ¿€æ´» 12Bï¼‰çš„ **Mixture-of-Experts (MoE)** æ¨¡å‹ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºå±•ç¤ºäº†ç«¯åˆ°ç«¯çš„ **RLï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰åŸºç¡€è®¾æ–½**ï¼Œåœ¨æ•°å­¦ã€ä»£ç å’Œæ¨ç†åŸºå‡†ä¸Šè¶…è¶Šäº†è®¸å¤šæ›´å¤§çš„å‰æ²¿æ¨¡å‹ã€‚é€šè¿‡ `prime-rl` æ¡†æ¶ï¼Œä»–ä»¬å®ç°äº†ä»å•èŠ‚ç‚¹åˆ°æ•°åƒ GPU çš„æ‰©å±•ï¼Œæ”¯æŒå¤šè½®äº¤äº’å’Œå·¥å…·è°ƒç”¨ã€‚\n> **Implication:** è¿™ä¸ä»…æ˜¯å‘æ¨¡å‹ï¼Œæ›´æ˜¯å‘åŸºå»ºã€‚å¼€æºçš„å¤§è§„æ¨¡ RL è®­ç»ƒæ ˆå¯¹äºç¤¾åŒºå¤ç° DeepSeek æˆ– o1 ç±»çš„å·¥ä½œè‡³å…³é‡è¦ã€‚\n\n**2. Sigma-MoE-Tiny Technical Report**\n> **Sigma-MoE-Tiny æŠ€æœ¯æŠ¥å‘Š**\n> *å…³é”®è¯ï¼šæè‡´ç¨€ç– MoE, è´Ÿè½½å‡è¡¡, ç»†ç²’åº¦ä¸“å®¶*\n> **TLDR:** å¾®è½¯å›¢é˜Ÿä¸ä»…å·å¤§ï¼Œè¿˜åœ¨å·å°ã€‚Sigma-MoE-Tiny æ‹¥æœ‰ 20B æ€»å‚æ•°ï¼Œä½†æ¯æ¬¡æ¨ç†ä»…æ¿€æ´» **0.5B**ã€‚é€šè¿‡ç»†ç²’åº¦ä¸“å®¶åˆ†å‰²ï¼ˆæ¯å±‚96ä¸ªä¸“å®¶ï¼‰å’Œæ¯ä¸ª token ä»…æ¿€æ´» 1 ä¸ªä¸“å®¶çš„ç­–ç•¥ï¼Œå®ç°äº†æè‡´æ•ˆç‡ã€‚ä»–ä»¬æå‡ºäº†ä¸€ç§**æ¸è¿›å¼ç¨€ç–åŒ–ï¼ˆprogressive sparsificationï¼‰**çš„è°ƒåº¦ç­–ç•¥æ¥è§£å†³è´Ÿè½½å‡è¡¡é—®é¢˜ã€‚\n> **Implication:** è¯æ˜äº†æä½æ¿€æ´»å‚æ•°é‡çš„ MoE åœ¨æ€§èƒ½ä¸Šä¾ç„¶èƒ½æ‰“ï¼Œä¸ºç«¯ä¾§å’Œä½å»¶è¿Ÿéƒ¨ç½²æä¾›äº†æ–°æ€è·¯ã€‚\n\n**3. AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints**\n> **AlignMergeï¼šé€šè¿‡ Fisher å¼•å¯¼çš„å‡ ä½•çº¦æŸå®ç°ä¿æŒå¯¹é½çš„å¤§è¯­è¨€æ¨¡å‹åˆå¹¶**\n> *å…³é”®è¯ï¼šModel Merging, å¯¹é½ä¿ç•™, Fisher ä¿¡æ¯*\n> **TLDR:** æ¨¡å‹åˆå¹¶ï¼ˆModel Mergingï¼‰é€šå¸¸ä¼šç ´åæ¨¡å‹çš„å®‰å…¨å¯¹é½ã€‚AlignMerge å°†åˆå¹¶è§†ä¸ºä¸€ä¸ª**å‡ ä½•çº¦æŸ**é—®é¢˜ï¼Œåˆ©ç”¨å±€éƒ¨ Fisher ä¿¡æ¯ä¼°è®¡å¯¹é½å­ç©ºé—´ã€‚å®ƒåœ¨ä¿ç•™ä»»åŠ¡èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾ç€å‡å°‘äº†å¯¹é½æ¼‚ç§»ï¼ˆAlignment-Subspace Driftï¼‰ã€‚\n\n---\n\n### ğŸ¥ è§†è§‰ç”Ÿæˆä¸å¤šæ¨¡æ€ (Vision, Video & Multimodal)\n\n**4. [é‡ç£…] TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times**\n> **TurboDiffusionï¼šå°†è§†é¢‘æ‰©æ•£æ¨¡å‹åŠ é€Ÿ 100-200 å€**\n> *å…³é”®è¯ï¼šè§†é¢‘ç”Ÿæˆ, æ¨ç†åŠ é€Ÿ, W8A8 é‡åŒ–*\n> **TLDR:** æ¥è‡ªæ¸…åå’Œä¼¯å…‹åˆ©çš„ç ”ç©¶ã€‚é€šè¿‡ç»“åˆ **SageAttention**ã€**ç¨€ç–çº¿æ€§æ³¨æ„åŠ› (SLA)**ã€**æ­¥éª¤è’¸é¦ (Step Distillation)** å’Œ **W8A8 é‡åŒ–**ï¼ŒTurboDiffusion åœ¨ RTX 5090 ä¸Šå®ç°äº†æƒŠäººçš„ 100-200 å€åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†è§†é¢‘è´¨é‡ã€‚\n> **Implication:** è§†é¢‘ç”Ÿæˆçš„â€œå®æ—¶åŒ–â€å¯èƒ½æ¯”æˆ‘ä»¬é¢„æƒ³çš„æ¥å¾—æ›´å¿«ï¼Œè¿™å¯¹äºäº¤äº’å¼è§†é¢‘åº”ç”¨æ˜¯å·¨å¤§çš„åˆ©å¥½ã€‚\n\n**5. EasyV2V: A High-quality Instruction-based Video Editing Framework**\n> **EasyV2Vï¼šé«˜è´¨é‡çš„åŸºäºæŒ‡ä»¤çš„è§†é¢‘ç¼–è¾‘æ¡†æ¶**\n> *å…³é”®è¯ï¼šè§†é¢‘ç¼–è¾‘, è¿è´¯æ€§, æ©ç æœºåˆ¶*\n> **TLDR:** é’ˆå¯¹è§†é¢‘ç¼–è¾‘ä¸­çš„ä¸€è‡´æ€§å’Œæ§åˆ¶éš¾é¢˜ï¼ŒEasyV2V åˆ©ç”¨é¢„è®­ç»ƒçš„ T2V æ¨¡å‹ï¼Œé€šè¿‡ç®€å•çš„åºåˆ—æ‹¼æ¥å’Œè½»é‡çº§ LoRA å¾®è°ƒï¼Œå®ç°äº† SOTA çš„ç¼–è¾‘æ•ˆæœã€‚å®ƒç»Ÿä¸€äº†æ—¶ç©ºæ§åˆ¶ï¼Œæ”¯æŒ Video+Text æˆ– Video+Mask+Text ç­‰å¤šç§è¾“å…¥ã€‚\n\n**6. PixelArena: A benchmark for Pixel-Precision Visual Intelligence**\n> **PixelArenaï¼šåƒç´ çº§è§†è§‰æ™ºèƒ½åŸºå‡†æµ‹è¯•**\n> *å…³é”®è¯ï¼šBenchmark, åƒç´ çº§ç”Ÿæˆ, Gemini 3 Pro*\n> **TLDR:** ç°æœ‰çš„å›¾åƒç”Ÿæˆè¯„ä¼°è¿‡äºå…³æ³¨ç¾å­¦ï¼Œå¿½ç•¥äº†ç»†ç²’åº¦æ§åˆ¶ã€‚PixelArena å…³æ³¨**åƒç´ ç²¾åº¦**ã€‚æµ‹è¯„å‘ç° **Gemini 3 Pro Image** å±•ç°å‡ºäº†æ¶Œç°èƒ½åŠ›ï¼Œèƒ½åœ¨é›¶æ ·æœ¬ä¸‹ç”Ÿæˆé«˜ä¿çœŸçš„è¯­ä¹‰æ©ç ï¼ˆsemantic masksï¼‰ï¼Œè¿™æ˜¯ä¹‹å‰çš„æ¨¡å‹åšä¸åˆ°çš„ã€‚\n\n---\n\n### ğŸ¤– æ™ºèƒ½ä½“ä¸å®‰å…¨æ€§ (Agents & Safety)\n\n**7. Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation**\n> **æ™ºèƒ½ä½“å·¥å…·ç¼–æ’æ³„éœ²æ›´å¤šä¿¡æ¯ï¼šæ•°æ®é›†ã€åŸºå‡†ä¸ç¼“è§£**\n> *å…³é”®è¯ï¼šéšç§æ³„éœ², å¤šå·¥å…·ç¼–æ’, TOP-R*\n> **TLDR:** æå‡ºäº† **TOP-R (Tools Orchestration Privacy Risk)** æ¦‚å¿µã€‚å•æ™ºèƒ½ä½“å¤šå·¥å…·æ¶æ„ä¸ºäº†å®Œæˆä»»åŠ¡ï¼Œå¯èƒ½ä¼šè‡ªä¸»èšåˆåˆ†æ•£åœ¨ä¸åŒå·¥å…·ä¸­çš„ä¿¡æ¯ç‰‡æ®µï¼Œä»è€Œæ¨ç†å‡ºæ•æ„Ÿéšç§ã€‚ç°æœ‰æ¨¡å‹çš„å¹³å‡é£é™©æ³„éœ²ç‡é«˜è¾¾ 90.24%ã€‚\n> **Implication:** è¿™æ˜¯ä¸€ä¸ªéå¸¸éšè”½ä½†å±é™©çš„æ”»å‡»é¢ã€‚æ™ºèƒ½ä½“è¶Šèªæ˜ã€æ¨ç†èƒ½åŠ›è¶Šå¼ºï¼Œéšç§æ³„éœ²é£é™©åè€Œè¶Šå¤§ã€‚\n\n**8. Adaptation of Agentic AI**\n> **Agentic AI çš„é€‚åº”æ€§**\n> *å…³é”®è¯ï¼šç»¼è¿°, é€‚åº”ç­–ç•¥, å·¥å…·é€‚åº”*\n> **TLDR:** Jimeng Sun å’Œ Jiawei Han ç­‰å¤§ä½¬å›¢é˜Ÿçš„ç»¼è¿°ã€‚å°† Agent é€‚åº”æ€§åˆ†ä¸º**æ™ºèƒ½ä½“é€‚åº”**å’Œ**å·¥å…·é€‚åº”**ï¼Œå¹¶è¿›ä¸€æ­¥ç»†åˆ†äº†åŸºäºå·¥å…·æ‰§è¡Œä¿¡å·å’Œæ™ºèƒ½ä½“è¾“å‡ºä¿¡å·çš„é€‚åº”å½¢å¼ã€‚è¿™ä¸ºè®¾è®¡æ›´é²æ£’çš„ Agent ç³»ç»Ÿæä¾›äº†ç³»ç»Ÿæ€§çš„æ¡†æ¶ã€‚\n\n**9. Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?**\n> **å¤§å‹æ¨ç†æ¨¡å‹èƒ½å¦åˆ©ç”¨â€œæœ‰ç¼ºé™·çš„æ€ç»´â€æé«˜æ•°å­¦ä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Ÿ**\n> *å…³é”®è¯ï¼šCoT, é”™è¯¯æ¢å¤, é²æ£’æ€§*\n> **TLDR:** è¿™æ˜¯ä¸€ä¸ªåç›´è§‰çš„ç ”ç©¶ã€‚åœ¨è®­ç»ƒä¸­æ•…æ„åŠ å…¥åŒ…å«**å•ä¸€å—æ§é”™è¯¯**ï¼ˆè®¡ç®—æˆ–é€»è¾‘é”™è¯¯ï¼‰çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œåè€Œèƒ½æ•™ä¼šæ¨¡å‹æ£€æµ‹å¹¶ä»é”™è¯¯ä¸­æ¢å¤ã€‚è¿™ç§ Mixed-CoT-RL æ¨¡å‹æ¯”ä»…åœ¨æ­£ç¡®æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹æ›´é²æ£’ã€‚\n\n---\n\n### ğŸ”¬ AI for Science (ç§‘å­¦æ™ºèƒ½)\n\n**10. AI Needs Physics More Than Physics Needs AI**\n> **AI æ¯”ç‰©ç†å­¦æ›´éœ€è¦ç‰©ç†å­¦**\n> *å…³é”®è¯ï¼šè§‚ç‚¹æ–‡ç« , ç‰©ç†æ„ŸçŸ¥, å¯è§£é‡Šæ€§*\n> **TLDR:** 2024 å¹´è¯ºå¥–ç»™äº† AIï¼Œä½†è¿™ç¯‡æ–‡ç« æ³¼äº†å†·æ°´ã€‚ä½œè€…è®¤ä¸ºç›®å‰çš„ AI ç¼ºä¹ç‰©ç†å®šå¾‹çº¦æŸã€ä¸ç¡®å®šæ€§é‡åŒ–å’Œå› æœæœºåˆ¶ã€‚æ–‡ç« å‘¼å **Big AI**â€”â€”å°†åŸºäºç†è®ºçš„ä¸¥è°¨æ€§ä¸æœºå™¨å­¦ä¹ çš„çµæ´»æ€§ç»“åˆï¼Œåˆ©ç”¨ç‰©ç†å­¦çš„èŒƒå¼æ¥ä¿®è¡¥å½“å‰ AI çš„ç¼ºé™·ã€‚\n\n**11. Pretrained Battery Transformer (PBT): A battery life prediction foundation model**\n> **é¢„è®­ç»ƒç”µæ±  Transformer (PBT)ï¼šç”µæ± å¯¿å‘½é¢„æµ‹çš„åŸºç¡€æ¨¡å‹**\n> *å…³é”®è¯ï¼šåŸºç¡€æ¨¡å‹, ç”µæ± å¯¿å‘½, è¿ç§»å­¦ä¹ *\n> **TLDR:** ç”µæ± é¢†åŸŸçš„â€œGPTæ—¶åˆ»â€ã€‚è¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ç”µæ± å¯¿å‘½é¢„æµ‹çš„ Foundation Modelã€‚PBT é€šè¿‡é¢†åŸŸçŸ¥è¯†ç¼–ç çš„ MoE å±‚ï¼Œåœ¨æœ€å¤§çš„å…¬å¼€ç”µæ± æ•°æ®åº“ä¸Šè®­ç»ƒï¼Œåœ¨è·¨åŒ–å­¦æˆåˆ†ã€è·¨å·¥å†µé¢„æµ‹ä¸Šæ¯”ç°æœ‰æ¨¡å‹æå‡äº†è¿‘ 20%ã€‚\n\n**12. PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving**\n> **PDE-Agentï¼šç”¨äº PDE æ±‚è§£çš„å·¥å…·é“¾å¢å¼ºå¤šæ™ºèƒ½ä½“æ¡†æ¶**\n> *å…³é”®è¯ï¼šåå¾®åˆ†æ–¹ç¨‹, å¤šæ™ºèƒ½ä½“, ç§‘å­¦è®¡ç®—*\n> **TLDR:** å°†æ±‚è§£åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEï¼‰è½¬åŒ–ä¸ºå¤šæ™ºèƒ½ä½“çš„å·¥å…·è°ƒç”¨è¿‡ç¨‹ã€‚å¼•å…¥äº† **Prog-Act** æ¡†æ¶å’Œå¸¦æœ‰å›¾è®°å¿†çš„åŠ¨æ€è§„åˆ’ï¼Œè§£å†³äº†ä¼ ç»Ÿ PINN éœ€è¦å¤§é‡ä¸“å®¶æ‰‹å·¥è®¾ç½®çš„é—®é¢˜ï¼Œå®ç°äº†ä»è‡ªç„¶è¯­è¨€æè¿°åˆ° PDE æ±‚è§£çš„è‡ªåŠ¨åŒ–ã€‚\n\n---\n\n### ğŸ“± è¾¹ç¼˜è®¡ç®—ä¸æ•ˆç‡ (Edge & Efficiency)\n\n**13. Benchmarking and Adapting On-Device Large Language Models for Clinical Decision Support**\n> **ç«¯ä¾§å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠå†³ç­–æ”¯æŒä¸­çš„åŸºå‡†æµ‹è¯•ä¸é€‚åº”**\n> *å…³é”®è¯ï¼šç«¯ä¾§æ¨¡å‹, åŒ»ç–— AI, éšç§*\n> **TLDR:** é’ˆå¯¹åŒ»ç–—éšç§ç—›ç‚¹ï¼Œè¯„ä¼°äº† gpt-oss-20b ç­‰ç«¯ä¾§æ¨¡å‹ã€‚å‘ç°ç»è¿‡å¾®è°ƒçš„ 20B ç«¯ä¾§æ¨¡å‹åœ¨è¯Šæ–­å‡†ç¡®ç‡ä¸Šå¯ä»¥é€¼è¿‘ GPT-5ï¼Œè¯æ˜äº†â€œå°æ¨¡å‹+å¾®è°ƒâ€åœ¨ç‰¹å®šå‚ç›´é¢†åŸŸæ›¿ä»£äº‘ç«¯å¤§æ¨¡å‹çš„æ½œåŠ›ã€‚\n\n**14. Scaling Laws for Energy Efficiency of Local LLMs**\n> **æœ¬åœ° LLM èƒ½æ•ˆçš„ç¼©æ”¾å®šå¾‹**\n> *å…³é”®è¯ï¼šç¼©æ”¾å®šå¾‹, CPU æ¨ç†, è¾¹ç¼˜è®¾å¤‡*\n> **TLDR:** ç ”ç©¶äº† MacBook Pro M2 å’Œæ ‘è“æ´¾ 5 ä¸Šçš„æ¨ç†èƒ½è€—ã€‚å‘ç°äº†ä¸¤ä¸ªå®šå¾‹ï¼šè¯­è¨€æ¨¡å‹çš„è®¡ç®—æˆæœ¬éš token é•¿åº¦çº¿æ€§å¢é•¿ï¼›è€Œ VLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰å­˜åœ¨ä¸€ä¸ªç”±é¢„å¤„ç†é©±åŠ¨çš„â€œåˆ†è¾¨ç‡æ‹ç‚¹â€ã€‚é‡å­å¯å‘çš„å‹ç¼©æŠ€æœ¯å¯å‡å°‘ 62% çš„èƒ½è€—ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€æ”»å‡»ä¸é˜²å¾¡ (Security & Attacks)\n\n**15. Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics**\n> **æ‹’ç»å¼•å¯¼ï¼šå¯¹ LLM æ•æ„Ÿè¯é¢˜æ‹’ç»è¡Œä¸ºçš„ç»†ç²’åº¦æ§åˆ¶**\n> *å…³é”®è¯ï¼šæ¨ç†å¹²é¢„, æ‹’ç»æœºåˆ¶, æ¿€æ´»å¼•å¯¼*\n> **TLDR:** é€šè¿‡ **Refusal Steering**ï¼Œå¯ä»¥åœ¨æ¨ç†æ—¶ç§»é™¤æ¨¡å‹å¯¹æ”¿æ²»æ•æ„Ÿè¯é¢˜çš„æ‹’ç»è¡Œä¸ºï¼ŒåŒæ—¶ä¿ç•™å¯¹æœ‰å®³å†…å®¹çš„é˜²å¾¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ‹’ç»ä¿¡å·ä¸»è¦é›†ä¸­åœ¨ Transformer çš„æ·±å±‚ï¼Œé€šè¿‡æ“æ§æ¿€æ´»å‘é‡å¯ä»¥ç²¾å‡†â€œåˆ‡é™¤â€è¿™ç§è¿‡åº¦é˜²å¾¡ã€‚\n\n**16. MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval**\n> **MemoryGraftï¼šé€šè¿‡æŠ•æ¯’ç»éªŒæ£€ç´¢å®ç° LLM æ™ºèƒ½ä½“çš„æŒä¹…æ€§ç ´å**\n> *å…³é”®è¯ï¼šRAG æŠ•æ¯’, é•¿æœŸè®°å¿†, é—´æ¥æ³¨å…¥*\n> **TLDR:** é’ˆå¯¹å…·å¤‡é•¿æœŸè®°å¿†çš„ Agent çš„æ”»å‡»ã€‚æ”»å‡»è€…åªéœ€æ³¨å…¥å°‘é‡æ¶æ„çš„â€œæˆåŠŸç»éªŒâ€ï¼ŒAgent åœ¨æ£€ç´¢è¿‡å¾€ç»éªŒæ—¶å°±ä¼šæ¨¡ä»¿è¿™äº›æ¶æ„æ¨¡å¼ï¼Œå¯¼è‡´è¡Œä¸ºå‘ç”ŸæŒä¹…æ€§æ¼‚ç§»ã€‚\n\n**17. The Violation State: Safety State Persistence in a Multimodal Language Model Interface**\n> **è¿è§„çŠ¶æ€ï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ¥å£ä¸­çš„å®‰å…¨çŠ¶æ€æŒä¹…æ€§**\n> *å…³é”®è¯ï¼šChatGPT, å®‰å…¨è¿‡åº¦æ³›åŒ–, æ‹’ç»æœåŠ¡*\n> **TLDR:** åœ¨ ChatGPT ç•Œé¢ä¸­å‘ç°äº†ä¸€ä¸ªç°è±¡ï¼šå¦‚æœä½ ä¸Šä¼ ä¸€å¼ ç‰ˆæƒå›¾ç‰‡è¦æ±‚å»æ°´å°è¢«æ‹’ç»ï¼Œåç»­å³ä½¿ä½ è¦æ±‚ç”Ÿæˆå®Œå…¨æ— å…³çš„è‰¯æ€§å›¾ç‰‡ï¼Œä¹Ÿä¼šè¢«æŒç»­æ‹’ç»ã€‚è¿™è¢«ç§°ä¸º**å®‰å…¨çŠ¶æ€æŒä¹…æ€§ï¼ˆSafety-State Persistenceï¼‰**ï¼Œæ˜¯ä¸€ç§ä¼šè¯çº§åˆ«çš„è¿‡åº¦é˜²å¾¡ã€‚\n\n---\n\n### ğŸ’¡ å…¶å®ƒå€¼å¾—å…³æ³¨çš„çŸ­è®¯\n\n*   **[MRI] Field strength-dependent performance variability... (Paper 1):** è®­ç»ƒ MRI åˆ†å‰²æ¨¡å‹æ—¶ï¼Œç£åœºå¼ºåº¦ï¼ˆ1.5T vs 3.0Tï¼‰æ˜¯ä¸€ä¸ªä¸¥é‡çš„æ··æ‚å› ç´ ï¼Œæ··åˆæ•°æ®è®­ç»ƒæ•ˆæœåè€Œä¸å¦‚ä¸“ä¸€æ•°æ®ã€‚\n*   **[Finance] Interpretable Deep Learning for Stock Returns (Paper 133):** æå‡º CB-APM æ¨¡å‹ï¼Œå°†åˆ†æå¸ˆå…±è¯†ä½œä¸ºç»“æ„åŒ–â€œç“¶é¢ˆâ€åµŒå…¥æ¨¡å‹ï¼Œæ˜¾ç€æé«˜äº†è‚¡ä»·å›æŠ¥é¢„æµ‹çš„è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚\n*   **[Audio] Hearing to Translate (Paper 111):** å¯¹æ¯”è¯„æµ‹å‘ç°ï¼Œç›®å‰çš„ç«¯åˆ°ç«¯ SpeechLLM åœ¨è¯­éŸ³ç¿»è¯‘è´¨é‡ä¸Šä»ç„¶æ‰“ä¸è¿‡ä¼ ç»Ÿçš„â€œè¯­éŸ³è¯†åˆ«+æ–‡æœ¬ç¿»è¯‘â€çº§è”ç³»ç»Ÿã€‚\n*   **[Gaming] StarCraft+ (Paper 102):** ä¹‹å‰çš„æ˜Ÿé™…äº‰éœ¸ AI éƒ½æ˜¯æ‰“å†…ç½® AIï¼Œè¿™ä¸ªæ–°åŸºå‡†ä¸“æ³¨äº**ç®—æ³•å¯¹æˆ˜ç®—æ³•**ï¼ˆAlgorithm-vs-Algorithmï¼‰ï¼Œæ›´è€ƒéªŒåšå¼ˆç­–ç•¥ã€‚\n\n---\n\nä»Šå¤©çš„ arXiv å°±è¯»åˆ°è¿™é‡Œã€‚**MoE çš„å°å‹åŒ–å’Œè§†é¢‘ç”Ÿæˆçš„åŠ é€ŸåŒ–**æ˜¯ä»Šå¤©çš„æŠ€æœ¯äº®ç‚¹ï¼Œè€Œ **Agent çš„éšç§ä¸å®‰å…¨é—®é¢˜**åˆ™æ˜¯æˆ‘ä»¬å¿…é¡»è­¦æƒ•çš„é˜´æš—é¢ã€‚\n\næ˜å¤©è§ï¼\n**Gemini Enterprise**",
  "papers": [
    {
      "arxiv_id": "2512.22176v1",
      "title": "Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„ç£å…±æŒ¯æˆåƒåˆ†æä¸­åœºå¼ºä¾èµ–æ€§çš„æ€§èƒ½å·®å¼‚",
      "authors": [
        "Muhammad Ibtsaam Qadir",
        "Duane Schonlau",
        "Ulrike Dydak",
        "Fiona R. Kolbinger"
      ],
      "abstract": "This study quantitatively evaluates the impact of MRI scanner magnetic field strength on the performance and generalizability of deep learning-based segmentation algorithms. Three publicly available MRI datasets (breast tumor, pancreas, and cervical spine) were stratified by scanner field strength (1.5T vs. 3.0T). For each segmentation task, three nnU-Net-based models were developed: A model trained on 1.5T data only (m-1.5T), a model trained on 3.0T data only (m-3.0T), and a model trained on pooled 1.5T and 3.0T data (m-combined). Each model was evaluated on both 1.5T and 3.0T validation sets. Field-strength-dependent performance differences were investigated via Uniform Manifold Approximation and Projection (UMAP)-based clustering and radiomic analysis, including 23 first-order and texture features. For breast tumor segmentation, m-3.0T (DSC: 0.494 [1.5T] and 0.433 [3.0T]) significantly outperformed m-1.5T (DSC: 0.411 [1.5T] and 0.289 [3.0T]) and m-combined (DSC: 0.373 [1.5T] and 0.268[3.0T]) on both validation sets (p<0.0001). Pancreas segmentation showed similar trends: m-3.0T achieved the highest DSC (0.774 [1.5T], 0.840 [3.0T]), while m-1.5T underperformed significantly (p<0.0001). For cervical spine, models performed optimally on same-field validation sets with minimal cross-field performance degradation (DSC>0.92 for all comparisons). Radiomic analysis revealed moderate field-strength-dependent clustering in soft tissues (silhouette scores 0.23-0.29) but minimal separation in osseous structures (0.12). These results indicate that magnetic field strength in the training data substantially influences the performance of deep learning-based segmentation models, particularly for soft-tissue structures (e.g., small lesions). This warrants consideration of magnetic field strength as a confounding factor in studies evaluating AI performance on MRI.",
      "tldr_zh": "è¯¥ç ”ç©¶å®šé‡è¯„ä¼°äº†MRIæ‰«æä»ªç£åœºå¼ºåº¦ï¼ˆ1.5Tä¸3.0Tï¼‰å¯¹åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²ç®—æ³•æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›çš„å½±å“ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ä¹³è…ºè‚¿ç˜¤ã€èƒ°è…ºå’Œé¢ˆæ¤ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ï¼Œè®­ç»ƒäº†åŸºäºnnU-Netçš„å¤šç§åœºå¼ºç»„åˆæ¨¡å‹ï¼Œå¹¶ç»“åˆUniform Manifold Approximation and Projection (UMAP)èšç±»ä¸radiomic analysisè¿›è¡Œæ·±å…¥åˆ†æã€‚å®éªŒå‘ç°ï¼Œåœ¨ä¹³è…ºè‚¿ç˜¤å’Œèƒ°è…ºç­‰è½¯ç»„ç»‡åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨3.0Tæ•°æ®è®­ç»ƒçš„æ¨¡å‹è¡¨ç°æ˜¾è‘—ä¼˜äº1.5Tæ¨¡å‹ï¼Œè€Œåœ¨é¢ˆæ¤ç­‰éª¨éª¼ç»“æ„ä¸­ï¼Œè·¨åœºå¼ºçš„æ€§èƒ½è¡°å‡ç›¸å¯¹è¾ƒå°ã€‚æ”¾å°„ç»„å­¦åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œåœºå¼ºå¯¼è‡´çš„ç‰¹å¾èšç±»åœ¨è½¯ç»„ç»‡ä¸­æ›´ä¸ºæ˜æ˜¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè®­ç»ƒæ•°æ®ä¸­çš„ç£åœºå¼ºåº¦ä¼šå®è´¨æ€§åœ°å½±å“æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨é’ˆå¯¹è½¯ç»„ç»‡ç»“æ„çš„åˆ†æä¸­ã€‚å› æ­¤ï¼Œè¯¥ç ”ç©¶å¼ºè°ƒåœ¨è¯„ä¼°MRIäººå·¥æ™ºèƒ½ç®—æ³•æ€§èƒ½æ—¶ï¼Œå¿…é¡»å°†ç£åœºå¼ºåº¦è§†ä¸ºä¸€ä¸ªå…³é”®çš„æ··æ·†å› å­(confounding factor)ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "comment": "16 pages, 1 table, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22176v1",
      "published_date": "2025-12-18 23:50:06 UTC",
      "updated_date": "2025-12-18 23:50:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:49:12.123108+00:00"
    },
    {
      "arxiv_id": "2601.03266v1",
      "title": "Benchmarking and Adapting On-Device Large Language Models for Clinical Decision Support",
      "title_zh": "é¢å‘ä¸´åºŠå†³ç­–æ”¯æŒçš„ç«¯ä¾§å¤§è¯­è¨€æ¨¡å‹åŸºå‡†è¯„ä¼°ä¸é€‚é…",
      "authors": [
        "Alif Munim",
        "Jun Ma",
        "Omar Ibrahim",
        "Alhusain Abdalla",
        "Shuolin Yin",
        "Leo Chen",
        "Bo Wang"
      ],
      "abstract": "Large language models (LLMs) have rapidly advanced in clinical decision-making, yet the deployment of proprietary systems is hindered by privacy concerns and reliance on cloud-based infrastructure. Open-source alternatives allow local inference but often require large model sizes that limit their use in resource-constrained clinical settings. Here, we benchmark two on-device LLMs, gpt-oss-20b and gpt-oss-120b, across three representative clinical tasks: general disease diagnosis, specialty-specific (ophthalmology) diagnosis and management, and simulation of human expert grading and evaluation. We compare their performance with state-of-the-art proprietary models (GPT-5 and o4-mini) and a leading open-source model (DeepSeek-R1), and we further evaluate the adaptability of on-device systems by fine-tuning gpt-oss-20b on general diagnostic data. Across tasks, gpt-oss models achieve performance comparable to or exceeding DeepSeek-R1 and o4-mini despite being substantially smaller. In addition, fine-tuning remarkably improves the diagnostic accuracy of gpt-oss-20b, enabling it to approach the performance of GPT-5. These findings highlight the potential of on-device LLMs to deliver accurate, adaptable, and privacy-preserving clinical decision support, offering a practical pathway for broader integration of LLMs into routine clinical practice.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸´åºŠå†³ç­–æ”¯æŒè¯„ä¼°å¹¶ä¼˜åŒ–äº†ç«¯ä¾§å¤§è¯­è¨€æ¨¡å‹(On-Device Large Language Models)ï¼Œé‡ç‚¹è€ƒå¯Ÿäº†gpt-oss-20bå’Œgpt-oss-120båœ¨é€šç”¨ç–¾ç—…è¯Šæ–­ã€çœ¼ç§‘ä¸“é¡¹è¯Šç–—åŠä¸“å®¶è¯„åˆ†æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡ä¸GPT-5ã€o4-miniå’ŒDeepSeek-R1ç­‰å…ˆè¿›æ¨¡å‹å¯¹æ¯”ï¼Œå¹¶å¯¹gpt-oss-20bè¿›è¡Œå¾®è°ƒ(Fine-tuning)ï¼Œç ”ç©¶æ·±å…¥æ¢è®¨äº†ç«¯ä¾§ç³»ç»Ÿçš„é€‚é…æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å‚æ•°è§„æ¨¡è¾ƒå°ï¼Œgpt-ossæ¨¡å‹åœ¨å„é¡¹ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¸DeepSeek-R1å’Œo4-miniç›¸å½“ç”šè‡³æ›´ä¼˜çš„æˆç»©ã€‚æ­¤å¤–ï¼Œå¾®è°ƒæ˜¾è‘—æå‡äº†gpt-oss-20bçš„è¯Šæ–­å‡†ç¡®ç‡ï¼Œä½¿å…¶æ€§èƒ½æ¥è¿‘GPT-5ã€‚è¿™äº›å‘ç°è¯å®äº†ç«¯ä¾§å¤§è¯­è¨€æ¨¡å‹åœ¨æä¾›å‡†ç¡®ã€å¯é€‚é…ä¸”ä¿æŠ¤éšç§(Privacy-preserving)çš„ä¸´åºŠæ”¯æŒæ–¹é¢çš„æ½œåŠ›ã€‚è¯¥å·¥ä½œä¸ºå°†å¤§è¯­è¨€æ¨¡å‹é›†æˆåˆ°èµ„æºå—é™çš„å¸¸è§„ä¸´åºŠå®è·µä¸­æä¾›äº†ä¸€æ¡åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.03266v1",
      "published_date": "2025-12-18 22:29:45 UTC",
      "updated_date": "2025-12-18 22:29:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:49:22.615648+00:00"
    },
    {
      "arxiv_id": "2512.17102v1",
      "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
      "title_zh": "åŸºäºæŠ€èƒ½åº“çš„è‡ªæˆ‘è¿›åŒ–æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Jiongxiao Wang",
        "Qiaojing Yan",
        "Yawei Wang",
        "Yijun Tian",
        "Soumya Smruti Mishra",
        "Zhichao Xu",
        "Megha Gandhi",
        "Panpan Xu",
        "Lin Lee Cheong"
      ],
      "abstract": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨éƒ¨ç½²åˆ°æ–°ç¯å¢ƒæ—¶éš¾ä»¥æŒç»­æ”¹è¿›çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæŠ€èƒ½åº“(Skill Library)çš„å¼ºåŒ–å­¦ä¹ (RL)æ–¹æ³•ã€‚ç ”ç©¶è€…å¼•å…¥äº†åä¸ºSAGE (Skill Augmented GRPO for self-Evolution) çš„æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡å°†æŠ€èƒ½ç³»ç»Ÿåœ°æ•´åˆåˆ°å­¦ä¹ è¿‡ç¨‹ä¸­æ¥å¢å¼ºæ™ºèƒ½ä½“çš„è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶æ˜¯é¡ºåºå±•å¼€(Sequential Rollout)ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡åœ¨ç›¸ä¼¼ä»»åŠ¡é“¾ä¸­çš„è¿­ä»£éƒ¨ç½²æ¥ç§¯ç´¯å¹¶å¤ç”¨æŠ€èƒ½ï¼Œå¹¶é…åˆæŠ€èƒ½é›†æˆå¥–åŠ±(Skill-integrated Reward)ä¼˜åŒ–æŠ€èƒ½çš„ç”Ÿæˆä¸åˆ©ç”¨ã€‚åœ¨AppWorldåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒSAGEåœ¨ç›‘ç£å¾®è°ƒ(Supervised-finetuned)æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå°†åœºæ™¯ç›®æ ‡å®Œæˆç‡(Scenario Goal Completion)æå‡äº†8.9%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ•ˆç‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸ä»…äº¤äº’æ­¥æ•°å‡å°‘äº†26%ï¼Œç”Ÿæˆçš„Tokenæ•°é‡ä¹Ÿå‡å°‘äº†59%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºæç¤ºè¯çš„æŠ€èƒ½åº“æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17102v1",
      "published_date": "2025-12-18 21:58:19 UTC",
      "updated_date": "2025-12-18 21:58:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:49:19.178816+00:00"
    },
    {
      "arxiv_id": "2512.17100v2",
      "title": "UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data",
      "title_zh": "UniCoMTEï¼šä¸€ç§ç”¨äºè§£é‡Šå¿ƒç”µå›¾ï¼ˆECGï¼‰æ•°æ®æ—¶é—´åºåˆ—åˆ†ç±»å™¨çš„é€šç”¨åäº‹å®æ¡†æ¶",
      "authors": [
        "Justin Li",
        "Efe Sencan",
        "Jasper Zheng Duan",
        "Vitus J. Leung",
        "Stephen Tsaur",
        "Ayse K. Coskun"
      ],
      "abstract": "Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UniCoMTEï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè§£é‡Šå¤šå˜é‡æ—¶é—´åºåˆ—(multivariate time series)åˆ†ç±»å™¨çš„é€šç”¨ä¸”ä¸æ¨¡å‹æ— å…³(model-agnostic)çš„åäº‹å®è§£é‡Š(counterfactual explanations)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¿®æ”¹è¾“å…¥æ ·æœ¬å¹¶è¯„ä¼°å…¶å¯¹æ¨¡å‹é¢„æµ‹çš„å½±å“ï¼Œè¯†åˆ«å‡ºå¯¹é¢„æµ‹ç»“æœè´¡çŒ®æœ€å¤§çš„æ—¶é—´ç‰¹å¾ï¼Œä¸”èƒ½ç›´æ¥åœ¨åŸå§‹æ—¶é—´åºåˆ—è¾“å…¥ä¸Šè¿è¡Œã€‚ç ”ç©¶äººå‘˜åœ¨å¿ƒç”µå›¾(ECG)åˆ†ç±»å™¨ä¸Šè¯„ä¼°äº†UniCoMTEï¼Œå¹¶å°†å…¶ç”Ÿæˆçš„è§£é‡Šä¸LIMEå’ŒSHAPç­‰ç°æœ‰æŠ€æœ¯åœ¨å¯ç†è§£æ€§(comprehensibility)å’Œæ³›åŒ–æ€§(generalizability)æ–¹é¢è¿›è¡Œäº†é‡åŒ–å¯¹æ¯”ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŒ»å­¦ä¸“å®¶å¯¹åäº‹å®è§£é‡Šä¸åŸå§‹ECGæ ·æœ¬çš„è¯„ä¼°é—®å·ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¡†æ¶çš„ä¸´åºŠå®ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniCoMTEç”Ÿæˆçš„è§£é‡Šå…·æœ‰ç®€æ´ã€ç¨³å®šä¸”ç¬¦åˆäººç±»ç›´è§‰çš„ç‰¹ç‚¹ï¼Œåœ¨æ¸…æ™°åº¦å’Œé€‚ç”¨æ€§ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™ä¸€æ¡†æ¶å°†æ¨¡å‹é¢„æµ‹ä¸å…·æœ‰å®é™…æ„ä¹‰çš„ä¿¡å·æ¨¡å¼ç›¸è”ç³»ï¼Œæ˜¾è‘—æå‡äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç°å®ä¸–ç•Œæ—¶é—´åºåˆ—åº”ç”¨ä¸­çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.17100v2",
      "published_date": "2025-12-18 21:56:08 UTC",
      "updated_date": "2025-12-22 02:23:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:50:03.714341+00:00"
    },
    {
      "arxiv_id": "2512.17093v1",
      "title": "A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving",
      "title_zh": "ä¸€ç§ç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹é€»è¾‘è°œé¢˜æ±‚è§£ä¸­å›ç­”é›†ç¨‹åºè®¾è®¡èƒ½åŠ›çš„æ±‚è§£å™¨åœ¨ç¯æ¡†æ¶",
      "authors": [
        "Timo Pierre Schrader",
        "Lukas Lange",
        "Tobias Kaminski",
        "Simon Razniewski",
        "Annemarie Friedrich"
      ],
      "abstract": "The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.\n  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç”Ÿæˆ Answer Set Programming (ASP) ç­‰é¢†åŸŸç‰¹å®šè¯­è¨€ä»£ç æ—¶ï¼Œå› é¢„è®­ç»ƒæ•°æ®ä¸è¶³è€Œé¢ä¸´çš„è¯­ä¹‰è§£ææŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„ ASP-solver-in-the-loop æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ±‚è§£å™¨å¼•å¯¼çš„æŒ‡ä»¤å¾®è°ƒ (instruction-tuning) æå‡æ¨¡å‹å¤„ç†é€»è¾‘è°œé¢˜çš„èƒ½åŠ›ï¼Œä¸”ä»…éœ€è‡ªç„¶è¯­è¨€çš„é—®é¢˜è§„æ ¼åŠå…¶å¯¹åº”çš„è§£ä½œä¸ºè¾“å…¥ã€‚åˆ©ç”¨å£°æ˜å¼ ASP ç¼–ç¨‹ä¸­å±€éƒ¨ç¼–ç èƒ½é€æ­¥ç¼©å°è§£ç©ºé—´çš„ç‰¹æ€§ï¼Œæ¡†æ¶æ ¹æ®æ±‚è§£å™¨ (solver) çš„åé¦ˆå°†ç”Ÿæˆçš„é‡‡æ ·æ•°æ®åˆ†ä¸ºé€‰ä¸­ä¸æ‹’ç»å®ä¾‹ï¼Œå¹¶æ®æ­¤è¿›è¡Œç›‘ç£å¾®è°ƒ (supervised fine-tuning)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ç»“åˆ Best-of-N é‡‡æ ·çš„æ±‚è§£å™¨å¼•å¯¼æœç´¢ç­–ç•¥ä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†çš„ä¸åŒæç¤ºè®¾ç½®ä¸‹å‡å®ç°äº†æ€§èƒ½çš„æŒç»­æå‡ï¼Œæ˜¾è‘—æ”¹å–„äº† LLMs åœ¨å¤æ‚é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­çš„ä»£ç ç”Ÿæˆè´¨é‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 7 figures, accepted at AAAI'26",
      "pdf_url": "https://arxiv.org/pdf/2512.17093v1",
      "published_date": "2025-12-18 21:45:45 UTC",
      "updated_date": "2025-12-18 21:45:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:49:25.330743+00:00"
    },
    {
      "arxiv_id": "2512.17091v1",
      "title": "Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making",
      "title_zh": "å­¦ä»¥è§„åˆ’ï¼Œè§„ä»¥ä¿ƒå­¦ï¼šé¢å‘æ ·æœ¬é«˜æ•ˆå†³ç­–çš„è‡ªé€‚åº”åˆ†å±‚ RL-MPC",
      "authors": [
        "Toshiaki Hori",
        "Jonathan DeCastro",
        "Deepak Gopinath",
        "Avinash Balachandran",
        "Guy Rosman"
      ],
      "abstract": "We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸æ¨¡å‹é¢„æµ‹æ§åˆ¶(MPC)çš„è‡ªé€‚åº”å±‚æ¬¡åŒ–è§„åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„é‡‡æ ·æ•ˆç‡å’Œé²æ£’æ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†å¼ºåŒ–å­¦ä¹ ç”Ÿæˆçš„åŠ¨ä½œç”¨äºæŒ‡å¯¼Model Predictive Path Integral (MPPI)é‡‡æ ·å™¨ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”èšåˆé‡‡æ ·ç»“æœæ¥ä¼˜åŒ–ä»·å€¼ä¼°è®¡(Value Estimation)ã€‚è¿™ç§è‡ªé€‚åº”æœºåˆ¶å…è®¸ç³»ç»Ÿåœ¨ä»·å€¼ä¼°è®¡ä¸ç¡®å®šçš„åŒºåŸŸåˆ©ç”¨MPPIè¿›è¡Œé¢å¤–æ¢ç´¢ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒçš„ç¨³å¥æ€§å’Œç­–ç•¥çš„æ•´ä½“æ€§èƒ½ã€‚å®éªŒåœ¨èµ›è½¦é©¾é©¶(Race Driving)ã€æ”¹è¿›å‹Acrobotä»¥åŠå¤æ‚ç¯å¢ƒä¸‹çš„Lunar Landerç­‰å¤šä¸ªé¢†åŸŸè¿›è¡Œäº†éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®æ•ˆç‡å’Œä»»åŠ¡æˆåŠŸç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒæˆåŠŸç‡æœ€é«˜æå‡äº†72%ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ¯”éè‡ªé€‚åº”é‡‡æ ·å¿«2.1å€ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå¤„ç†å…·æœ‰å±‚æ¬¡ç»“æ„çš„å¤æ‚è§„åˆ’é—®é¢˜æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 8 figures. Under review",
      "pdf_url": "https://arxiv.org/pdf/2512.17091v1",
      "published_date": "2025-12-18 21:44:00 UTC",
      "updated_date": "2025-12-18 21:44:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:49:29.790061+00:00"
    },
    {
      "arxiv_id": "2512.17090v1",
      "title": "How to Square Tensor Networks and Circuits Without Squaring Them",
      "title_zh": "å¦‚ä½•åœ¨ä¸è¿›è¡Œå¹³æ–¹è¿ç®—çš„æƒ…å†µä¸‹å®ç°å¼ é‡ç½‘ç»œä¸ç”µè·¯çš„å¹³æ–¹åŒ–",
      "authors": [
        "Lorenzo Loconte",
        "AdriÃ¡n Javaloy",
        "Antonio Vergari"
      ],
      "abstract": "Squared tensor networks (TNs) and their extension as computational graphs--squared circuits--have been used as expressive distribution estimators, yet supporting closed-form marginalization. However, the squaring operation introduces additional complexity when computing the partition function or marginalizing variables, which hinders their applicability in ML. To solve this issue, canonical forms of TNs are parameterized via unitary matrices to simplify the computation of marginals. However, these canonical forms do not apply to circuits, as they can represent factorizations that do not directly map to a known TN. Inspired by the ideas of orthogonality in canonical forms and determinism in circuits enabling tractable maximization, we show how to parameterize squared circuits to overcome their marginalization overhead. Our parameterizations unlock efficient marginalization even in factorizations different from TNs, but encoded as circuits, whose structure would otherwise make marginalization computationally hard. Finally, our experiments on distribution estimation show how our proposed conditions in squared circuits come with no expressiveness loss, while enabling more efficient learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¹³æ–¹å¼ é‡ç½‘ç»œ (Squared Tensor Networks, TNs) åŠå…¶æ‰©å±•è®¡ç®—å›¾â€”â€”å¹³æ–¹ç”µè·¯ (squared circuits) åœ¨æ¦‚ç‡åˆ†å¸ƒä¼°è®¡ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³å…¶åœ¨è®¡ç®—é…åˆ†å‡½æ•°æˆ–è¾¹é™…åŒ–å˜é‡æ—¶å› å¹³æ–¹æ“ä½œå¼•å…¥çš„é¢å¤–å¤æ‚åº¦ã€‚ç”±äºå¼ é‡ç½‘ç»œçš„è§„èŒƒå½¢å¼ (canonical forms) æ— æ³•ç›´æ¥åº”ç”¨äºå…·æœ‰ä¸åŒåˆ†è§£ç»“æ„çš„ç”µè·¯ï¼Œä½œè€…å—æ­£äº¤æ€§ (orthogonality) å’Œç¡®å®šæ€§ (determinism) å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¹³æ–¹ç”µè·¯å‚æ•°åŒ–æ–¹æ³•ä»¥æ¶ˆé™¤è¾¹é™…åŒ–å¼€é”€ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—åŸæœ¬è®¡ç®—å›°éš¾çš„å¤æ‚ç”µè·¯ç»“æ„èƒ½å¤Ÿå®ç°é«˜æ•ˆçš„è¾¹é™…åŒ–å¤„ç†ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥å‚æ•°åŒ–æ–¹æ¡ˆåœ¨ä¿æŒæ¨¡å‹è¡¨è¾¾èƒ½åŠ› (expressiveness) çš„åŒæ—¶ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„å­¦ä¹ è¿‡ç¨‹ï¼Œä¸ºå¤„ç†éå¼ é‡ç½‘ç»œç»“æ„çš„æ¦‚ç‡å»ºæ¨¡æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17090v1",
      "published_date": "2025-12-18 21:36:54 UTC",
      "updated_date": "2025-12-18 21:36:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:49:31.453132+00:00"
    },
    {
      "arxiv_id": "2512.17086v1",
      "title": "Value Under Ignorance in Universal Artificial Intelligence",
      "title_zh": "é€šç”¨äººå·¥æ™ºèƒ½ä¸­æ— çŸ¥çŠ¶æ€ä¸‹çš„ä»·å€¼",
      "authors": [
        "Cole Wyeth",
        "Marcus Hutter"
      ],
      "abstract": "We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.",
      "tldr_zh": "è¯¥ç ”ç©¶å°† AIXI å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ™ºèƒ½ä½“è¿›è¡Œäº†æ³›åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿæ”¯æŒæ›´å¹¿æ³›çš„æ•ˆç”¨å‡½æ•°(Utility Functions)ç±»åˆ«ã€‚ä¸ºäº†è§£å†³æ™ºèƒ½ä½“ä¿¡å¿µåˆ†å¸ƒä¸­æŸäº›å‡è®¾ä»…èƒ½é¢„æµ‹æœ‰é™å†å²å‰ç¼€æ‰€å¸¦æ¥çš„æ­§ä¹‰ï¼Œç ”ç©¶è€…æ¢è®¨äº†è¢«ç§°ä½œåŠæµ‹åº¦æŸå¤±(Semimeasure Loss)çš„é‡åŒ–æŒ‡æ ‡ï¼Œå¹¶æŒ‘æˆ˜äº†å°†å…¶ç®€å•è§£é‡Šä¸ºâ€œæ­»äº¡æ¦‚ç‡â€çš„ä¼ ç»Ÿè§‚ç‚¹ã€‚è®ºæ–‡æå‡ºå°†æ­¤ç±»ä¿¡å¿µåˆ†å¸ƒè§†ä¸ºä¸ç²¾ç¡®æ¦‚ç‡åˆ†å¸ƒ(Imprecise Probability Distributions)ï¼Œå¹¶å°†åŠæµ‹åº¦æŸå¤±å®šä¹‰ä¸ºå®Œå…¨æ— çŸ¥(Total Ignorance)çŠ¶æ€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶åˆ©ç”¨ä¸ç²¾ç¡®æ¦‚ç‡ç†è®ºä¸­çš„ Choquet ç§¯åˆ†æ¥è®¡ç®—æœŸæœ›æ•ˆç”¨ï¼Œå¹¶å¯¹å…¶å¯è®¡ç®—æ€§æ°´å¹³(Computability Level)è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ç ”ç©¶è¯æ˜æ ‡å‡†çš„é€’å½’ä»·å€¼å‡½æ•°(Recursive Value Function)æ˜¯è¯¥æ¡†æ¶ä¸‹çš„ä¸€ä¸ªç‰¹ä¾‹ï¼ŒåŒæ—¶æ­ç¤ºäº†åœ¨æ­»äº¡è§£é‡Šä¸‹çš„æœ€ä¸€èˆ¬æœŸæœ›æ•ˆç”¨æ— æ³•å®Œå…¨ç”± Choquet ç§¯åˆ†åˆ»ç”»ï¼Œä¸ºé€šç”¨äººå·¥æ™ºèƒ½çš„ä»·å€¼ç†è®ºæä¾›äº†æ–°çš„æ•°å­¦è§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17086v1",
      "published_date": "2025-12-18 21:34:50 UTC",
      "updated_date": "2025-12-18 21:34:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:49:35.410440+00:00"
    },
    {
      "arxiv_id": "2512.17083v3",
      "title": "When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation",
      "title_zh": "å½“ F1 æŒ‡æ ‡å¤±æ•ˆï¼šå¯¹è¯è¯é¢˜åˆ†å‰²çš„ç²’åº¦æ„ŸçŸ¥è¯„ä¼°",
      "authors": [
        "Michael H. Coen"
      ],
      "abstract": "Dialogue topic segmentation supports summarization, retrieval, memory management, and conversational continuity. Despite decades of work, evaluation practice remains dominated by strict boundary matching and F1-based metrics. Modern large language model (LLM) based conversational systems increasingly rely on segmentation to manage conversation history beyond fixed context windows. In such systems, unstructured context accumulation degrades efficiency and coherence.\n  This paper introduces an evaluation framework that reports boundary density and segment alignment diagnostics (purity and coverage) alongside window-tolerant F1 (W-F1). By separating boundary scoring from boundary selection, we evaluate segmentation quality across density regimes rather than at a single operating point. Cross-dataset evaluation shows that reported performance differences often reflect annotation granularity mismatch rather than boundary placement quality alone.\n  We evaluate structurally distinct segmentation strategies across eight dialogue datasets spanning task-oriented, open-domain, meeting-style, and synthetic interactions. Boundary-based metrics are strongly coupled to boundary density: threshold sweeps produce larger W-F1 changes than switching between methods. These findings support viewing topic segmentation as a granularity selection problem rather than prediction of a single correct boundary set. This motivates separating boundary scoring from boundary selection for analyzing and tuning segmentation under varying annotation granularities.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¯¹è¯è¯é¢˜åˆ†å‰² (Dialogue Topic Segmentation) é¢†åŸŸé•¿æœŸä¾èµ–ä¸¥æ ¼è¾¹ç•ŒåŒ¹é…å’Œ F1 åˆ†æ•°è¿™ä¸€è¯„ä»·ç°çŠ¶ï¼ŒæŒ‡å‡ºäº†å…¶åœ¨å¤„ç†å¤§è¯­è¨€æ¨¡å‹ (LLM) ä¸Šä¸‹æ–‡ç®¡ç†æ—¶çš„å±€é™æ€§ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œåœ¨å¼•å…¥çª—å£å®¹å¿ F1 (W-F1) çš„åŸºç¡€ä¸Šï¼Œå¢åŠ äº†è¾¹ç•Œå¯†åº¦å’Œç‰‡æ®µå¯¹é½è¯Šæ–­æŒ‡æ ‡ï¼Œå³çº¯åº¦ (Purity) å’Œè¦†ç›–åº¦ (Coverage)ã€‚é€šè¿‡å°†è¾¹ç•Œè¯„åˆ† (Boundary Scoring) ä¸è¾¹ç•Œé€‰æ‹© (Boundary Selection) åˆ†ç¦»ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè·¨è¶Šä¸åŒå¯†åº¦åŒºé—´è¯„ä¼°åˆ†å‰²è´¨é‡ï¼Œè€Œéå±€é™äºå•ä¸€æ“ä½œç‚¹ã€‚ç ”ç©¶åœ¨æ¶µç›–ä»»åŠ¡å¯¼å‘ã€ä¼šè®®åŠåˆæˆäº¤äº’ç­‰å…«ä¸ªå¯¹è¯æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå‘ç°æ€§èƒ½å·®å¼‚å¾€å¾€æºäºæ ‡æ³¨ç»†ç²’åº¦ (Granularity) çš„ä¸åŒ¹é…ï¼Œè€Œéå•çº¯çš„è¾¹ç•Œä½ç½®è¯¯å·®ã€‚å®éªŒè¯æ˜é˜ˆå€¼æ‰«æŸ¥å¯¹ W-F1 çš„å½±å“æ˜¾è‘—è¶…è¿‡äº†æ–¹æ³•æœ¬èº«çš„åˆ‡æ¢ï¼Œä»è€Œå°†è¯é¢˜åˆ†å‰²é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªç»†ç²’åº¦é€‰æ‹©é—®é¢˜ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨å¤šå˜æ ‡æ³¨å‡†åˆ™ä¸‹åˆ†æå’Œè°ƒä¼˜åˆ†å‰²ç³»ç»Ÿæä¾›äº†é‡è¦çš„ç†è®ºä¾æ®å’Œå®ç”¨å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "34 pages, 4 figures. Evaluation and methodology study on dialogue topic segmentation",
      "pdf_url": "https://arxiv.org/pdf/2512.17083v3",
      "published_date": "2025-12-18 21:29:43 UTC",
      "updated_date": "2025-12-31 08:52:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:49:39.914312+00:00"
    },
    {
      "arxiv_id": "2512.22175v1",
      "title": "Characterizing Motion Encoding in Video Diffusion Timesteps",
      "title_zh": "è§†é¢‘æ‰©æ•£æ—¶é—´æ­¥ä¸­è¿åŠ¨ç¼–ç çš„ç‰¹æ€§è¡¨å¾",
      "authors": [
        "Vatsal Baherwani",
        "Yixuan Ren",
        "Abhinav Shrivastava"
      ],
      "abstract": "Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°é‡åŒ–å¹¶è¡¨å¾äº†è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨å»å™ªæ—¶é—´æ­¥(timesteps)ä¸­å¦‚ä½•å¯¹è¿åŠ¨(motion)è¿›è¡Œç¼–ç ã€‚ç ”ç©¶è€…é€šè¿‡åœ¨ç‰¹å®šæ—¶é—´æ­¥æ³¨å…¥æ–°æ¡ä»¶å¹¶æƒè¡¡å¤–è§‚ç¼–è¾‘ä¸è¿åŠ¨ä¿ç•™çš„å…³ç³»ï¼Œè¯†åˆ«å‡ºäº†æ—©æœŸçš„è¿åŠ¨ä¸»å¯¼(motion-dominant)å’ŒåæœŸçš„å¤–è§‚ä¸»å¯¼(appearance-dominant)é˜¶æ®µã€‚è¿™ä¸€å‘ç°å°†å¹¿æ³›ä½¿ç”¨çš„ç»éªŒå¯å‘å¼è§‚ç‚¹è½¬åŒ–ä¸ºæ—¶ç©ºè§£è€¦(spatiotemporal disentanglement)åŸåˆ™ï¼Œå¹¶åˆ’å®šäº†æ˜ç¡®çš„æ“ä½œè¾¹ç•Œã€‚åŸºäºæ­¤è¡¨å¾ï¼Œè¯¥å·¥ä½œç®€åŒ–äº†ç°æœ‰çš„å•æ¬¡(one-shot)è¿åŠ¨å®šåˆ¶èŒƒå¼ï¼Œä»…é€šè¿‡é™åˆ¶åœ¨è¿åŠ¨ä¸»å¯¼é˜¶æ®µè¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼Œä¾¿å®ç°äº†æ— éœ€è¾…åŠ©å»åæ¨¡å—çš„å¼ºæ•ˆè¿åŠ¨è¿ç§»ã€‚è¿™ç§å—æ—¶é—´æ­¥çº¦æŸçš„æ–¹æ³•å¯ä½œä¸ºå³æ’å³ç”¨ç»„ä»¶ï¼Œè½»æ¾é›†æˆåˆ°ç°æœ‰çš„è§†é¢‘è¿åŠ¨è¿ç§»ä¸ç¼–è¾‘æ–¹æ³•ä¸­ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22175v1",
      "published_date": "2025-12-18 21:20:54 UTC",
      "updated_date": "2025-12-18 21:20:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:49:45.133915+00:00"
    },
    {
      "arxiv_id": "2512.17079v1",
      "title": "Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?",
      "title_zh": "å¤§æ¨ç†æ¨¡å‹èƒ½å¦åˆ©ç”¨å­˜åœ¨ç¼ºé™·çš„æ€ç»´æå‡æ•°å­¦ä»»åŠ¡çš„å‡†ç¡®ç‡ï¼Ÿ",
      "authors": [
        "Saraswathy Amjith",
        "Mihika Dusad",
        "Neha Muramalla",
        "Shweta Shah"
      ],
      "abstract": "Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹æ˜¯å¦å¯ä»¥é€šè¿‡åœ¨åŒ…å«é”™è¯¯çš„é“¾å¼æ€ç»´ï¼ˆChain-of-thoughtï¼‰è½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¥æé«˜åœ¨æ•°å­¦ä»»åŠ¡ä¸­çš„é”™è¯¯æ£€æµ‹ä¸æ¢å¤èƒ½åŠ›ã€‚ç ”ç©¶è€…åˆ©ç”¨ MATH-lighteval ç«èµ›é¢˜ç›®ç”Ÿæˆäº†åŒ…å«å—æ§è®¡ç®—æˆ–æ¨ç†é”™è¯¯çš„æµ‹è¯•å‰ç¼€ï¼Œå¹¶é‡‡ç”¨ GRPO ç®—æ³•å¯¹ Qwen3-4B æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒï¼ˆfine-tuneï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMixed-CoT-RL æ¨¡å‹åœ¨æ ‡å‡†é¢˜ç›®ä¸Šä¸å¸¸è§„æ¨¡å‹æ€§èƒ½æŒå¹³ï¼Œä½†åœ¨åŒ…å«é”™è¯¯æ¨ç†çš„é—®é¢˜ä¸Šè¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ï¼ˆrobustnessï¼‰ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œä»…é’ˆå¯¹æ­£ç¡®ç­”æ¡ˆè®­ç»ƒçš„ RL åè€Œä¼šé™ä½æ¨¡å‹çš„æŠ—å¹²æ‰°èƒ½åŠ›ï¼Œè€Œå¼•å…¥æ¨ç†é”™è¯¯è®­ç»ƒæ¯”å•ä¸€è®¡ç®—é”™è¯¯è®­ç»ƒæ•ˆæœæ›´ä½³ã€‚è¯¥ç ”ç©¶è¯æ˜ï¼Œåœ¨è®­ç»ƒä¸­æš´éœ²äºç¼ºé™·è½¨è¿¹èƒ½æ˜¾è‘—æå‡æ¨¡å‹çš„é”™è¯¯æ¢å¤è¡Œä¸ºä¸”ä¸ç‰ºç‰²åŸå§‹å‡†ç¡®ç‡ï¼Œä¸ºæ„å»ºæ›´å…·éŸ§æ€§çš„ LLMs æ¨ç†ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17079v1",
      "published_date": "2025-12-18 21:20:21 UTC",
      "updated_date": "2025-12-18 21:20:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:50:24.375981+00:00"
    },
    {
      "arxiv_id": "2512.17067v1",
      "title": "Bots Don't Sit Still: A Longitudinal Study of Bot Behaviour Change, Temporal Drift, and Feature-Structure Evolution",
      "title_zh": "æœºå™¨äººå¹¶éé™æ­¢ä¸åŠ¨ï¼šæœºå™¨äººè¡Œä¸ºå˜åŒ–ã€æ—¶é—´æ¼‚ç§»ä¸ç‰¹å¾ç»“æ„æ¼”åŒ–çš„çºµå‘ç ”ç©¶",
      "authors": [
        "Ohoud Alzahrani",
        "Russell Beale",
        "Bob Hendley"
      ],
      "abstract": "Social bots are now deeply embedded in online platforms for promotion, persuasion, and manipulation. Most bot-detection systems still treat behavioural features as static, implicitly assuming bots behave stationarily over time. We test that assumption for promotional Twitter bots, analysing change in both individual behavioural signals and the relationships between them. Using 2,615 promotional bot accounts and 2.8M tweets, we build yearly time series for ten content-based meta-features. Augmented Dickey-Fuller and KPSS tests plus linear trends show all ten are non-stationary: nine increase over time, while language diversity declines slightly.\n  Stratifying by activation generation and account age reveals systematic differences: second-generation bots are most active and link-heavy; short-lived bots show intense, repetitive activity with heavy hashtag/URL use; long-lived bots are less active but more linguistically diverse and use emojis more variably. We then analyse co-occurrence across generations using 18 interpretable binary features spanning actions, topic similarity, URLs, hashtags, sentiment, emojis, and media (153 pairs). Chi-square tests indicate almost all pairs are dependent. Spearman correlations shift in strength and sometimes polarity: many links (e.g. multiple hashtags with media; sentiment with URLs) strengthen, while others flip from weakly positive to weakly or moderately negative. Later generations show more structured combinations of cues.\n  Taken together, these studies provide evidence that promotional social bots adapt over time at both the level of individual meta-features and the level of feature interdependencies, with direct implications for the design and evaluation of bot-detection systems trained on historical behavioural features.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶å¯¹ç¤¾äº¤æœºå™¨äººï¼ˆSocial botsï¼‰çš„è¡Œä¸ºå˜åŒ–ã€æ—¶é—´åç§»ï¼ˆTemporal Driftï¼‰ä»¥åŠç‰¹å¾ç»“æ„æ¼”åŒ–è¿›è¡Œäº†çºµå‘ç ”ç©¶ï¼Œæ—¨åœ¨æŒ‘æˆ˜å½“å‰æœºå™¨äººæ£€æµ‹ç³»ç»Ÿå°†è¡Œä¸ºç‰¹å¾è§†ä¸ºé™æ€çš„é¢„è®¾ã€‚ç ”ç©¶å›¢é˜Ÿåˆ†æäº†2,615ä¸ªæ¨å¹¿å‹Twitteræœºå™¨äººè´¦å·åŠ280ä¸‡æ¡æ¨æ–‡ï¼Œåˆ©ç”¨Augmented Dickey-Fullerå’ŒKPSSæ£€éªŒå‘ç°åä¸ªæ ¸å¿ƒå…ƒç‰¹å¾å‡è¡¨ç°å‡ºéå¹³ç¨³æ€§ï¼ˆNon-stationaryï¼‰ï¼Œå…¶ä¸­ä¹ä¸ªç‰¹å¾éšæ—¶é—´å¢å¼ºã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”ä¸åŒä»£é™…å’Œè´¦å·å¯¿å‘½å‘ç°ï¼Œç¬¬äºŒä»£æœºå™¨äººæœ€ä¸ºæ´»è·ƒï¼Œè€Œé•¿å¯¿æœºå™¨äººåˆ™è¡¨ç°å‡ºæ›´é«˜çš„è¯­è¨€å¤šæ ·æ€§å’Œè¡¨æƒ…ç¬¦å·å˜åŒ–ã€‚æ­¤å¤–ï¼Œå¯¹18ä¸ªè§£é‡Šæ€§ç‰¹å¾çš„å…±ç°åˆ†ææ˜¾ç¤ºï¼Œç‰¹å¾é—´çš„Spearmanç›¸å…³æ€§åœ¨å¼ºåº¦å’Œææ€§ä¸Šå‡å‘ç”Ÿäº†æ˜¾è‘—åç§»ï¼ŒåæœŸæœºå™¨äººå‘ˆç°å‡ºæ›´å¤æ‚çš„è¡Œä¸ºæ¨¡å¼ã€‚è¿™äº›å‘ç°è¡¨æ˜æ¨å¹¿å‹æœºå™¨äººä¼šåœ¨ä¸ªä½“ç‰¹å¾åŠå…¶ç›¸äº’ä¾èµ–æ€§å±‚é¢ä¸æ–­æ¼”åŒ–é€‚åº”ï¼Œä¸ºè®¾è®¡èƒ½å¤Ÿåº”å¯¹åŠ¨æ€ç¯å¢ƒçš„æœºå™¨äººæ£€æµ‹ç³»ç»Ÿæä¾›äº†é‡è¦ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17067v1",
      "published_date": "2025-12-18 21:08:34 UTC",
      "updated_date": "2025-12-18 21:08:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:50:21.584953+00:00"
    },
    {
      "arxiv_id": "2512.17066v1",
      "title": "Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations",
      "title_zh": "ç°å®å¨èƒæ„ŸçŸ¥é©±åŠ¨ç¾¤ä½“é—´å†²çªï¼šåŸºäºç”Ÿæˆå¼æ™ºèƒ½ä½“æ¨¡æ‹Ÿçš„å› æœåŠ¨æ€åˆ†æ",
      "authors": [
        "Suhaib Abdurahman",
        "Farzan Karimi-Malekabadi",
        "Chenxiao Yu",
        "Nour S. Kteily",
        "Morteza Dehghani"
      ],
      "abstract": "Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„ç”Ÿæˆæ™ºèƒ½ä½“(generative agents)åœ¨è™šæ‹Ÿç¤¾ä¼šä¸­è¿›è¡Œæ¨¡æ‹Ÿï¼Œæ·±å…¥æ¢è®¨äº†ç°å®å¨èƒ(realistic threat)ä¸è±¡å¾æ€§å¨èƒ(symbolic threat)å¯¹ç¾¤é™…å†²çªçš„å› æœå½±å“åŠå…¶åŠ¨æ€äº¤äº’ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMèƒ½å¤Ÿå°†ç°å®å¨èƒã€è±¡å¾æ€§å¨èƒå’Œæ•Œæ„(hostility)ç¼–ç ä¸ºæˆªç„¶ä¸åŒçš„å†…éƒ¨çŠ¶æ€ï¼Œä¸”é€šè¿‡æ“çºµè¿™äº›çŠ¶æ€å¯ç›´æ¥é©±åŠ¨è¡Œä¸ºè½¬å˜ã€‚å› æœåˆ†æè¡¨æ˜ï¼Œç°å®å¨èƒæ˜¯å¯¼è‡´æ•Œæ„å¢åŠ çš„ç›´æ¥é©±åŠ¨åŠ›ï¼Œè€Œè±¡å¾æ€§å¨èƒçš„å½±å“è¾ƒå¼±ä¸”å®Œå…¨ç”±å†…ç¾¤ä½“åå‘(ingroup bias)æ‰€ä»‹å¯¼ï¼Œä»…åœ¨ç°å®å¨èƒä¸å­˜åœ¨æ—¶æ‰ä¼šè¯±å‘æ•Œæ„ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œéæ•Œå¯¹çš„ç¾¤é™…æ¥è§¦(intergroup contact)èƒ½æœ‰æ•ˆç¼“å†²å†²çªå‡çº§ï¼Œè€Œç»“æ„æ€§ä¸å¯¹ç§°åˆ™å¯¼è‡´æ•Œæ„æ›´å¤šåœ°é›†ä¸­åœ¨å¤šæ•°ç¾¤ä½“ä¸­ã€‚è¯¥æ¨¡æ‹Ÿå®éªŒä¸ºç†è§£äººç±»å†²çªæä¾›äº†è¶…è¶Šä¼¦ç†å’Œæ•°æ®é™åˆ¶çš„å› æœè§£é‡Šï¼Œæ­ç¤ºäº†å¨èƒæ„ŸçŸ¥åœ¨å†²çªæ¼”å˜ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17066v1",
      "published_date": "2025-12-18 21:06:07 UTC",
      "updated_date": "2025-12-18 21:06:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:50:31.950003+00:00"
    },
    {
      "arxiv_id": "2512.17060v1",
      "title": "On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues",
      "title_zh": "è®ºä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è‡ªæˆ‘çŠ¶æ€åœ¨é¢å‘æ²Ÿé€šåˆ†æå¯¹è¯çš„ LLM æ™ºèƒ½ä½“è¡Œä¸ºä¸­çš„ä½œç”¨",
      "authors": [
        "Monika Zamojska",
        "JarosÅ‚aw A. Chudziak"
      ],
      "abstract": "LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹LLMæ™ºèƒ½ä½“åœ¨æ¨¡æ‹Ÿäººç±»å¿ƒç†æ·±åº¦ã€ä¸€è‡´æ€§ä»¥åŠæƒ…æ„Ÿå†²çªæ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäº¤äº’åˆ†æ(Transactional Analysis)ç†è®ºçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-Agent System)ã€‚è¯¥ç³»ç»Ÿå°†æ¯ä¸ªæ™ºèƒ½ä½“åˆ’åˆ†ä¸ºçˆ¶æ¯(Parent)ã€æˆäºº(Adult)å’Œå„¿ç«¥(Child)ä¸‰ç§è‡ªæˆ‘çŠ¶æ€(Ego States)ï¼Œå¹¶å°†å…¶è§†ä¸ºå…·æœ‰ç‹¬ç«‹è§†è§’å’Œæ¨ç†é£æ ¼çš„çŸ¥è¯†ç»“æ„ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¸°å¯Œå“åº”è¿‡ç¨‹ï¼Œæ™ºèƒ½ä½“é›†æˆäº†ä¿¡æ¯æ£€ç´¢æœºåˆ¶ï¼Œèƒ½å¤Ÿä»å‘é‡å­˜å‚¨ä¸­æå–ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é€šè¿‡åœ¨æ¨¡æ‹Ÿå¯¹è¯åœºæ™¯ä¸­è¿›è¡Œçš„æ¶ˆèå®éªŒï¼Œç ”ç©¶å¯¹æ¯”äº†å¸¦æœ‰å’Œä¸å¸¦ä¿¡æ¯æ£€ç´¢æœºåˆ¶çš„æ™ºèƒ½ä½“è¡¨ç°ï¼Œç»“æœéªŒè¯äº†è¯¥æ¶æ„åœ¨æå‡äº¤äº’çœŸå®æ€§æ–¹é¢çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åœ¨äºå°†å¿ƒç†å­¦ç»“æ„ä¸ä¸Šä¸‹æ–‡æ£€ç´¢ç›¸ç»“åˆï¼Œä¸ºå¼€å‘å…·å¤‡å¿ƒç†å­¦åŸºç¡€çš„å¤æ‚æ™ºèƒ½ä½“è¡Œä¸ºæä¾›äº†æ–°çš„æ¢ç´¢æ–¹å‘ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Presented at the 39th Pacific Asia Conference on Language, Information and Computation (PACLIC 39)",
      "pdf_url": "https://arxiv.org/pdf/2512.17060v1",
      "published_date": "2025-12-18 20:53:31 UTC",
      "updated_date": "2025-12-18 20:53:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:50:23.488479+00:00"
    },
    {
      "arxiv_id": "2512.17053v1",
      "title": "Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL",
      "title_zh": "åŸºäºç»“æ„åŒ–æ€ç»´é“¾çš„ Text-to-SQL çŸ¥è¯†è’¸é¦",
      "authors": [
        "Khushboo Thaker",
        "Yony Bresler"
      ],
      "abstract": "Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ä¸šçº§ Text-to-SQL ç³»ç»Ÿåœ¨æˆæœ¬ã€å®‰å…¨å’Œæ€§èƒ½é—´çš„æƒè¡¡éš¾é¢˜ï¼Œæå‡ºäº† Struct-SQL çŸ¥è¯†è’¸é¦ (Knowledge Distillation) æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿéç»“æ„åŒ– Chain-of-Thought (CoT) åœ¨æ¨ç†å¼•å¯¼ä¸­çš„æ¨¡ç³Šæ€§é—®é¢˜ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ä½œä¸ºå½¢å¼åŒ–çš„ç»“æ„åŒ–æ¨ç†è“å›¾ï¼Œä¸ºå°å‹è¯­è¨€æ¨¡å‹ (SLMs) æä¾›æ›´ç²¾ç¡®çš„æ•™å­¦ä¿¡å·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒStruct-SQL æ¯”éç»“æ„åŒ– CoT è’¸é¦åŸºçº¿å®ç°äº† 8.1% çš„ç»å¯¹æ€§èƒ½æå‡ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†è¯­æ³•é”™è¯¯ã€‚è¿™ä¸€ç ”ç©¶æˆæœè¯å®äº†æ•™æˆæ¨¡å‹ä½¿ç”¨ç»“æ„åŒ–é€»è¾‘è“å›¾è¿›è¡Œæ¨ç†ï¼Œå¯¹äºæå‡ SLMs åœ¨ SQL ç”Ÿæˆä»»åŠ¡ä¸­çš„å¯é æ€§å…·æœ‰æ˜¾è‘—ç›Šå¤„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17053v1",
      "published_date": "2025-12-18 20:41:22 UTC",
      "updated_date": "2025-12-18 20:41:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:50:35.348519+00:00"
    },
    {
      "arxiv_id": "2512.22174v1",
      "title": "BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs",
      "title_zh": "BitFlipScopeï¼šé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹æ¯”ç‰¹ç¿»è½¬æŸåçš„å¯æ‰©å±•æ•…éšœå®šä½ä¸ä¿®å¤",
      "authors": [
        "Muhammad Zeeshan Karamat",
        "Sadman Saif",
        "Christiana Chamon Garcia"
      ],
      "abstract": "Large Language Models (LLMs) deployed in practical and safety-critical settings are increasingly susceptible to bit-flip faults caused by hardware degradation, cosmic radiation, or deliberate fault-injection attacks such as Rowhammer. These faults silently corrupt internal parameters and can lead to unpredictable or dangerous model behavior. Localizing these corruptions is essential: without identifying the affected region, it is impossible to diagnose the source of degradation, apply targeted corrective measures, or restore model functionality without resorting to costly fine-tuning or full retraining. This work introduces BitFlipScope, a scalable, software-based framework for identifying fault-affected regions within transformer architectures under two deployment scenarios. When a clean reference model is available, BitFlipScope performs differential analysis of outputs, hidden states, and internal activations for detecting anomalous behavior indicative of corruption to pinpoint or localize faults. When no reference model exists, it uses residual-path perturbation and loss-sensitivity profiling to infer the fault-impacted region directly from the corrupted model. In both settings, the framework not only enables effective fault diagnosis but also supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models. Together, these capabilities make BitFlipScope an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¡¬ä»¶é€€åŒ–ã€å®‡å®™è¾å°„æˆ– Rowhammer æ”»å‡»å¯¼è‡´çš„æ¯”ç‰¹ç¿»è½¬(bit-flip)æ•…éšœï¼Œæå‡ºäº† BitFlipScope æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„é«˜æ•ˆæ•…éšœå®šä½ä¸æ¢å¤ã€‚é’ˆå¯¹ Transformer æ¶æ„ï¼ŒBitFlipScope æ¶µç›–äº†ä¸¤ç§éƒ¨ç½²åœºæ™¯ï¼šåœ¨æ‹¥æœ‰å¹²å‡€å‚è€ƒæ¨¡å‹æ—¶ï¼Œé€šè¿‡å¯¹è¾“å‡ºã€éšè—çŠ¶æ€å’Œå†…éƒ¨æ¿€æ´»è¿›è¡Œå·®å¼‚åˆ†æ(differential analysis)æ¥ç²¾å‡†å®šä½æ•…éšœï¼›åœ¨æ— å‚è€ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œåˆ™åˆ©ç”¨æ®‹å·®è·¯å¾„æ‰°åŠ¨(residual-path perturbation)å’ŒæŸå¤±æ•æ„Ÿåº¦å‰–æ(loss-sensitivity profiling)ç›´æ¥ä»å—æŸæ¨¡å‹ä¸­æ¨æ–­æ•…éšœåŒºåŸŸã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ä¸ä»…èƒ½å®ç°æœ‰æ•ˆçš„æ•…éšœè¯Šæ–­ï¼Œè¿˜èƒ½åœ¨æ— éœ€å¾®è°ƒ(fine-tuning)æˆ–é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æ”¯æŒè½»é‡åŒ–çš„æ€§èƒ½æ¢å¤ã€‚BitFlipScope çš„æå‡ºä¸ºåœ¨æ˜“å‘ç”Ÿç¡¬ä»¶æ•…éšœæˆ–å¯¹æŠ—æ€§ç¯å¢ƒä¸‹çš„å¯ä¿¡ã€å…·æ•…éšœéŸ§æ€§çš„ LLM éƒ¨ç½²æä¾›äº†å®ç”¨çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22174v1",
      "published_date": "2025-12-18 20:35:29 UTC",
      "updated_date": "2025-12-18 20:35:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:50:32.079137+00:00"
    },
    {
      "arxiv_id": "2512.17048v1",
      "title": "Another Fit Bites the Dust: Conformal Prediction as a Calibration Standard for Machine Learning in High-Energy Physics",
      "title_zh": "åˆä¸€æ‹Ÿåˆå‘Šç»ˆï¼šç¬¦åˆé¢„æµ‹ä½œä¸ºé«˜èƒ½ç‰©ç†æœºå™¨å­¦ä¹ çš„æ ¡å‡†æ ‡å‡†",
      "authors": [
        "Jack Y. Araz",
        "Michael Spannowsky"
      ],
      "abstract": "Machine-learning techniques are essential in modern collider research, yet their probabilistic outputs often lack calibrated uncertainty estimates and finite-sample guarantees, limiting their direct use in statistical inference and decision-making. Conformal prediction (CP) provides a simple, distribution-free framework for calibrating arbitrary predictive models without retraining, yielding rigorous uncertainty quantification with finite-sample coverage guarantees under minimal exchangeability assumptions, without reliance on asymptotics, limit theorems, or Gaussian approximations. In this work, we investigate CP as a unifying calibration layer for machine-learning applications in high-energy physics. Using publicly available collider datasets and a diverse set of models, we show that a single conformal formalism can be applied across regression, binary and multi-class classification, anomaly detection, and generative modelling, converting raw model outputs into statistically valid prediction sets, typicality regions, and p-values with controlled false-positive rates. While conformal prediction does not improve raw model performance, it enforces honest uncertainty quantification and transparent error control. We argue that conformal calibration should be adopted as a standard component of machine-learning pipelines in collider physics, enabling reliable interpretation, robust comparisons, and principled statistical decisions in experimental and phenomenological analyses.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¬¦åˆé¢„æµ‹(Conformal Prediction, CP)ä½œä¸ºé«˜èƒ½ç‰©ç†(High-Energy Physics)ä¸­æœºå™¨å­¦ä¹ åº”ç”¨çš„ç»Ÿä¸€æ ¡å‡†å±‚ï¼Œæ—¨åœ¨è§£å†³æ¦‚ç‡è¾“å‡ºç¼ºä¹æ ¡å‡†ä¸ç¡®å®šæ€§ä¼°è®¡å’Œæœ‰é™æ ·æœ¬ä¿è¯çš„é—®é¢˜ã€‚Conformal Prediction æä¾›äº†ä¸€ä¸ªç®€å•ã€ä¸ä¾èµ–åˆ†å¸ƒä¸”æ— éœ€é‡æ–°è®­ç»ƒçš„æ¡†æ¶ï¼Œèƒ½åœ¨æå°äº¤æ¢æ€§å‡è®¾ä¸‹ä¸ºä»»æ„é¢„æµ‹æ¨¡å‹æä¾›ä¸¥æ ¼çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚é€šè¿‡å…¬å¼€çš„å¯¹æ’æœºæ•°æ®é›†ï¼Œç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå›å½’ã€åˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹å’Œç”Ÿæˆæ¨¡å‹ï¼Œå°†åŸå§‹è¾“å‡ºè½¬åŒ–ä¸ºå…·æœ‰å—æ§å‡é˜³æ€§ç‡çš„é¢„æµ‹é›†å’Œ p-valuesã€‚è™½ç„¶ Conformal Prediction å¹¶ä¸ç›´æ¥æé«˜åŸå§‹æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ï¼Œä½†å®ƒå¼ºåŒ–äº†è¯šå®çš„ä¸ç¡®å®šæ€§é‡åŒ–å’Œé€æ˜çš„è¯¯å·®æ§åˆ¶ã€‚ä½œè€…å»ºè®®å°†ç¬¦åˆæ ¡å‡†ä½œä¸ºé«˜èƒ½ç‰©ç†æœºå™¨å­¦ä¹ æµæ°´çº¿çš„æ ‡å‡†ç»„ä»¶ï¼Œä»¥ç¡®ä¿åœ¨å®éªŒå’Œç°è±¡å­¦åˆ†æä¸­å®ç°å¯é çš„è§£é‡Šã€é²æ£’çš„æ¯”è¾ƒä»¥åŠåŸºäºåŸåˆ™çš„ç»Ÿè®¡å†³ç­–ã€‚",
      "categories": [
        "hep-ph",
        "cs.AI",
        "hep-ex"
      ],
      "primary_category": "hep-ph",
      "comment": "24 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.17048v1",
      "published_date": "2025-12-18 20:31:25 UTC",
      "updated_date": "2025-12-18 20:31:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:50:45.921340+00:00"
    },
    {
      "arxiv_id": "2512.17043v1",
      "title": "UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering",
      "title_zh": "UniRel-R1ï¼šé¢å‘çŸ¥è¯†å›¾è°±å…³ç³»å‹é—®ç­”çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹æ¨ç†",
      "authors": [
        "Yinxu Tang",
        "Chengsong Huang",
        "Jiaxin Huang",
        "William Yeoh"
      ],
      "abstract": "Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UniRel-R1ï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³å…³ç³»ä¸­å¿ƒçŸ¥è¯†å›¾è°±é—®ç­” (Relation-centric KGQA) æŒ‘æˆ˜çš„ç»Ÿä¸€æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿä¾§é‡äºè¿”å›å•ä¸ªå®ä½“çš„é—®ç­”ä¸åŒï¼Œå…³ç³»ä¸­å¿ƒé—®ç­”å¯»æ±‚æ•è·å®ä½“é—´è¯­ä¹‰è¿æ¥çš„å­å›¾ (Subgraph)ï¼Œä½†é¢ä¸´å€™é€‰é¡¹è¿‡å¤šä¸”æ˜“è¢«çç¢è¿æ¥å¹²æ‰°çš„éš¾é¢˜ã€‚UniRel-R1 é›†æˆäº†å­å›¾é€‰æ‹©ã€å¤šé˜¶æ®µå›¾å‰ªæ (Multi-stage graph pruning) ä»¥åŠé€šè¿‡å¼ºåŒ–å­¦ä¹  (Reinforcement learning) å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹ã€‚å…¶å¥–åŠ±å‡½æ•°ä¸“é—¨è®¾è®¡ç”¨äºé¼“åŠ±ç”Ÿæˆç´§å‡‘ä¸”å…·ä½“çš„å­å›¾ï¼Œä¼˜å…ˆé€‰æ‹©ä¿¡æ¯é‡æ›´å¤§çš„å…³ç³»å’Œä½åº¦ä¸­é—´å®ä½“ã€‚å®éªŒè¯æ˜ï¼ŒUniRel-R1 åœ¨è¿é€šæ€§å’Œå¥–åŠ±æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äº Vanilla åŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨å¤„ç†æœªè§è¿‡çš„å®ä½“å’Œå…³ç³»æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17043v1",
      "published_date": "2025-12-18 20:11:20 UTC",
      "updated_date": "2025-12-18 20:11:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:50:39.667980+00:00"
    },
    {
      "arxiv_id": "2512.17041v1",
      "title": "Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats",
      "title_zh": "æ™ºèƒ½ä½“è½¦è¾†çš„å®‰å…¨é£é™©ï¼šè®¤çŸ¥ä¸è·¨å±‚å¨èƒçš„ç³»ç»Ÿæ€§åˆ†æ",
      "authors": [
        "Ali Eslami",
        "Jiangbo Yu"
      ],
      "abstract": "Agentic AI is increasingly being explored and introduced in both manually driven and autonomous vehicles, leading to the notion of Agentic Vehicles (AgVs), with capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance. While frameworks such as the OWASP Agentic AI Security Risks highlight vulnerabilities in reasoning-driven AI systems, they are not designed for safety-critical cyber-physical platforms such as vehicles, nor do they account for interactions with other layers such as perception, communication, and control layers. This paper investigates security threats in AgVs, including OWASP-style risks and cyber-attacks from other layers affecting the agentic layer. By introducing a role-based architecture for agentic vehicles, consisting of a Personal Agent and a Driving Strategy Agent, we will investigate vulnerabilities in both agentic AI layer and cross-layer risks, including risks originating from upstream layers (e.g., perception layer, control layer, etc.). A severity matrix and attack-chain analysis illustrate how small distortions can escalate into misaligned or unsafe behavior in both human-driven and autonomous vehicles. The resulting framework provides the first structured foundation for analyzing security risks of agentic AI in both current and emerging vehicle platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Agentic Vehicles (AgVs) è¿™ä¸€æ–°å…´é¢†åŸŸï¼Œç³»ç»Ÿæ€§åœ°åˆ†æäº†å…¶åœ¨è®¤çŸ¥å’Œè·¨å±‚äº¤äº’æ–¹é¢é¢ä¸´çš„å®‰å…¨é£é™©ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„ OWASP Agentic AI Security Risks æ¡†æ¶ä¸è¶³ä»¥åº”å¯¹è½¦è¾†è¿™ç±»å®‰å…¨å…³é”®å‹ç½‘ç»œç‰©ç†å¹³å°ä¸­æ„ŸçŸ¥ã€é€šä¿¡å’Œæ§åˆ¶å±‚ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŒ…å« Personal Agent å’Œ Driving Strategy Agent çš„è§’è‰²åŒ–æ¶æ„ï¼Œé‡ç‚¹è°ƒæŸ¥äº†æ™ºèƒ½ä»£ç†å±‚å†…éƒ¨åŠè·¨å±‚ï¼ˆå¦‚ä¸Šæ¸¸æ„ŸçŸ¥å±‚å¯¹ä»£ç†å±‚çš„å½±å“ï¼‰çš„æ¼æ´ã€‚é€šè¿‡å¼•å…¥ä¸¥é‡æ€§çŸ©é˜µ (severity matrix) å’Œæ”»å‡»é“¾ (attack-chain) åˆ†æï¼Œç ”ç©¶å±•ç¤ºäº†å¾®å°çš„è¾“å…¥æ‰­æ›²å¦‚ä½•å¯¼è‡´äººå·¥é©¾é©¶æˆ–è‡ªåŠ¨é©¾é©¶è½¦è¾†å‡ºç°å¤±è°ƒæˆ–ä¸å®‰å…¨è¡Œä¸ºã€‚è¯¥æˆæœä¸ºåˆ†æå½“å‰åŠæœªæ¥è½¦è¾†å¹³å°ä¸­æ™ºèƒ½ä»£ç† AI çš„å®‰å…¨æ€§æä¾›äº†é¦–ä¸ªç»“æ„åŒ–åŸºç¡€æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17041v1",
      "published_date": "2025-12-18 20:04:21 UTC",
      "updated_date": "2025-12-18 20:04:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:51:53.622937+00:00"
    },
    {
      "arxiv_id": "2601.00007v1",
      "title": "Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games",
      "title_zh": "Yahtzeeï¼šé¢å‘éšæœºç»„åˆåšå¼ˆçš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯",
      "authors": [
        "Nicholas A. Pape"
      ],
      "abstract": "Yahtzee is a classic dice game with a stochastic, combinatorial structure and delayed rewards, making it an interesting mid-scale RL benchmark. While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods. We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods: REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk. We ablate feature and action encodings, architecture, return estimators, and entropy regularization to understand their impact on learning. Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance, whereas A2C trains robustly across a range of settings. Our agent attains a median score of 241.78 points over 100,000 evaluation games, within 5.0\\% of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of 24.9\\% and 34.1\\%, respectively. All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment and exploration challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†å…·æœ‰éšæœºç»„åˆç»“æ„å’Œå»¶è¿Ÿå¥–åŠ±çš„éª°å­æ¸¸æˆ Yahtzee ä½œä¸ºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„ä¸­è§„æ¨¡åŸºå‡†ä»»åŠ¡ã€‚ä½œè€…å°†å…¶å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process)ï¼Œå¹¶åˆ©ç”¨ REINFORCEã€Advantage Actor-Critic (A2C) å’Œ Proximal Policy Optimization (PPO) ç­‰ç­–ç•¥æ¢¯åº¦æ–¹æ³•è®­ç»ƒè‡ªåšå¼ˆæ™ºèƒ½ä½“ã€‚é€šè¿‡å¯¹ç‰¹å¾ç¼–ç ã€ç½‘ç»œæ¶æ„å’Œæ­£åˆ™åŒ–æ‰‹æ®µçš„æ¶ˆèå®éªŒï¼Œç ”ç©¶å‘ç° A2C åœ¨ä¸åŒè®¾ç½®ä¸‹è¡¨ç°æœ€ä¸ºç¨³å¥ï¼Œè€Œ REINFORCE å’Œ PPO å¯¹è¶…å‚æ•°æå…¶æ•æ„Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ™ºèƒ½ä½“çš„ä¸­ä½å¾—åˆ†è¾¾åˆ° 241.78ï¼Œä¸åŠ¨æ€è§„åˆ’(Dynamic Programming)å¾—å‡ºçš„æœ€ä¼˜è§£å·®è·ä»…ä¸º 5.0%ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨å­¦ä¹ ä¸Šå±‚å¥–é‡‘(upper section bonus)ç­–ç•¥æ—¶å‡è¡¨ç°ä¸ä½³ï¼Œåæ˜ å‡ºé•¿æ—¶åŸŸä¿¡ç”¨åˆ†é…(credit-assignment)å’Œæ¢ç´¢(exploration)åœ¨éšæœºç»„åˆåšå¼ˆä¸­ä¾ç„¶æ˜¯ä¸¥å³»æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 19 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00007v1",
      "published_date": "2025-12-18 20:03:32 UTC",
      "updated_date": "2025-12-18 20:03:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:50:51.195147+00:00"
    },
    {
      "arxiv_id": "2512.17029v1",
      "title": "Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation",
      "title_zh": "Adversarial VRï¼šç”¨äºè¯„ä¼° VR æ™•åŠ¨ç—‡æ£€æµ‹ä¸ç¼“è§£å¯¹æŠ—é²æ£’æ€§çš„å¼€æºæµ‹è¯•å¹³å°",
      "authors": [
        "Istiak Ahmed",
        "Ripan Kumar Kundu",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "Deep learning (DL)-based automated cybersickness detection methods, along with adaptive mitigation techniques, can enhance user comfort and interaction. However, recent studies show that these DL-based systems are susceptible to adversarial attacks; small perturbations to sensor inputs can degrade model performance, trigger incorrect mitigation, and disrupt the user's immersive experience (UIX). Additionally, there is a lack of dedicated open-source testbeds that evaluate the robustness of these systems under adversarial conditions, limiting the ability to assess their real-world effectiveness. To address this gap, this paper introduces Adversarial-VR, a novel real-time VR testbed for evaluating DL-based cybersickness detection and mitigation strategies under adversarial conditions. Developed in Unity, the testbed integrates two state-of-the-art (SOTA) DL models: DeepTCN and Transformer, which are trained on the open-source MazeSick dataset, for real-time cybersickness severity detection and applies a dynamic visual tunneling mechanism that adjusts the field-of-view based on model outputs. To assess robustness, we incorporate three SOTA adversarial attacks: MI-FGSM, PGD, and C&W, which successfully prevent cybersickness mitigation by fooling DL-based cybersickness models' outcomes. We implement these attacks using a testbed with a custom-built VR Maze simulation and an HTC Vive Pro Eye headset, and we open-source our implementation for widespread adoption by VR developers and researchers. Results show that these adversarial attacks are capable of successfully fooling the system. For instance, the C&W attack results in a $5.94x decrease in accuracy for the Transformer-based cybersickness model compared to the accuracy without the attack.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Adversarial-VRï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„å®æ—¶è™šæ‹Ÿç°å®(VR)æµ‹è¯•åºŠï¼Œæ—¨åœ¨è¯„ä¼°åŸºäºæ·±åº¦å­¦ä¹ (Deep Learning)çš„æ™•åŠ¨ç—‡(Cybersickness)æ£€æµ‹ä¸ç¼“è§£ç³»ç»Ÿåœ¨å¯¹æŠ—æ”»å‡»(Adversarial Attacks)ä¸‹çš„é²æ£’æ€§(Robustness)ã€‚è¯¥ç³»ç»Ÿåœ¨Unityç¯å¢ƒä¸‹å¼€å‘ï¼Œé›†æˆäº†DeepTCNå’ŒTransformerç­‰å…ˆè¿›æ¨¡å‹ï¼Œå¹¶ç»“åˆåŠ¨æ€è§†è§‰éš§é“(Visual Tunneling)æœºåˆ¶å®ç°å®æ—¶çš„æ™•åŠ¨ç—‡å¹²é¢„ã€‚ç ”ç©¶è€…é€šè¿‡åœ¨å®šåˆ¶çš„VRè¿·å®«æ¨¡æ‹Ÿä¸­å¼•å…¥MI-FGSMã€PGDå’ŒC&Wç­‰å¯¹æŠ—æ”»å‡»æ‰‹æ®µï¼Œç³»ç»Ÿåœ°æµ‹è¯•äº†ä¼ æ„Ÿå™¨è¾“å…¥å—åˆ°å¾®å°æ‰°åŠ¨æ—¶æ¨¡å‹æ€§èƒ½çš„é€€åŒ–æƒ…å†µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ”»å‡»èƒ½æœ‰æ•ˆè¯¯å¯¼æ¨¡å‹è¾“å‡ºå¹¶é˜»æ–­ç¼“è§£ç­–ç•¥ï¼Œå…¶ä¸­C&Wæ”»å‡»å¯¼è‡´Transformeræ¨¡å‹çš„å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™äº†5.94å€ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼€æºæ­¤æµ‹è¯•åºŠï¼Œä¸ºVRé¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†è¯„ä¼°å¹¶å¢å¼ºæ²‰æµ¸å¼æ™ºèƒ½ç³»ç»Ÿå®‰å…¨æ€§çš„æ ‡å‡†åŒ–å·¥å…·ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CR",
      "comment": "Published in the 2025 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)",
      "pdf_url": "https://arxiv.org/pdf/2512.17029v1",
      "published_date": "2025-12-18 19:45:47 UTC",
      "updated_date": "2025-12-18 19:45:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:52:28.882652+00:00"
    },
    {
      "arxiv_id": "2512.17028v1",
      "title": "A Women's Health Benchmark for Large Language Models",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹çš„å¥³æ€§å¥åº·åŸºå‡†",
      "authors": [
        "Victoria-Elisabeth Gruber",
        "Razvan Marinescu",
        "Diego Fajardo",
        "Amin H. Nassar",
        "Christopher Arkfeld",
        "Alexandria Ludlow",
        "Shama Patel",
        "Mehrnoosh Samaei",
        "Valerie Klug",
        "Anna Huber",
        "Marcel GÃ¼hner",
        "Albert Botta i Orfila",
        "Irene Lagoja",
        "Kimya Tarr",
        "Haleigh Larson",
        "Mary Beth Howard"
      ],
      "abstract": "As large language models (LLMs) become primary sources of health information for millions, their accuracy in women's health remains critically unexamined. We introduce the Women's Health Benchmark (WHB), the first benchmark evaluating LLM performance specifically in women's health. Our benchmark comprises 96 rigorously validated model stumps covering five medical specialties (obstetrics and gynecology, emergency medicine, primary care, oncology, and neurology), three query types (patient query, clinician query, and evidence/policy query), and eight error types (dosage/medication errors, missing critical information, outdated guidelines/treatment recommendations, incorrect treatment advice, incorrect factual information, missing/incorrect differential diagnosis, missed urgency, and inappropriate recommendations). We evaluated 13 state-of-the-art LLMs and revealed alarming gaps: current models show approximately 60\\% failure rates on the women's health benchmark, with performance varying dramatically across specialties and error types. Notably, models universally struggle with \"missed urgency\" indicators, while newer models like GPT-5 show significant improvements in avoiding inappropriate recommendations. Our findings underscore that AI chatbots are not yet fully able of providing reliable advice in women's health.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†Women's Health Benchmark (WHB)ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¥³æ€§å¥åº·é¢†åŸŸè¡¨ç°çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŒ…å«96ä¸ªç»è¿‡ä¸¥æ ¼éªŒè¯çš„æ¨¡å‹å­˜æ ¹(model stumps)ï¼Œè¦†ç›–äº†å¦‡äº§ç§‘ã€æ€¥è¯Šã€åˆçº§ä¿å¥ã€è‚¿ç˜¤å’Œç¥ç»ç§‘å­¦äº”ä¸ªä¸“ä¸šï¼Œä»¥åŠç”¨è¯é”™è¯¯ã€å…³é”®ä¿¡æ¯é—æ¼å’Œé”™å¤±ç´§æ€¥æ€§ç­‰å…«ç§é”™è¯¯ç±»å‹ã€‚é€šè¿‡å¯¹13ç§å…ˆè¿›LLMsçš„è¯„ä¼°å‘ç°ï¼Œå½“å‰æ¨¡å‹åœ¨å¥³æ€§å¥åº·åŸºå‡†ä¸Šçš„å¤±è´¥ç‡çº¦ä¸º60%ï¼Œä¸”æ€§èƒ½éšä¸“ä¸šå’Œé”™è¯¯ç±»å‹è€Œå‰§çƒˆæ³¢åŠ¨ã€‚ç ”ç©¶æŒ‡å‡ºæ‰€æœ‰æ¨¡å‹åœ¨è¯†åˆ«â€œé”™å¤±ç´§æ€¥æƒ…å†µ(missed urgency)â€æŒ‡æ ‡ä¸Šå‡è¡¨ç°å›°éš¾ï¼Œè™½ç„¶GPT-5ç­‰æ–°æ¨¡å‹åœ¨å‡å°‘ä¸å½“å»ºè®®æ–¹é¢æœ‰æ˜¾è‘—æå‡ï¼Œä½†æ•´ä½“å¯é æ€§ä»æ˜¾ä¸è¶³ã€‚è¯¥ç ”ç©¶ç»“æœæœ€ç»ˆå¼ºè°ƒï¼Œç›®å‰çš„AIèŠå¤©æœºå™¨äººå°šæ— æ³•åœ¨å¥³æ€§å¥åº·é¢†åŸŸæä¾›è¶³å¤Ÿå¯é çš„åŒ»ç–—å»ºè®®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 6 Figures, 2 Tables",
      "pdf_url": "https://arxiv.org/pdf/2512.17028v1",
      "published_date": "2025-12-18 19:44:28 UTC",
      "updated_date": "2025-12-18 19:44:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:52:14.280978+00:00"
    },
    {
      "arxiv_id": "2512.17027v1",
      "title": "Unexpected Knowledge: Auditing Wikipedia and Grokipedia Search Recommendations",
      "title_zh": "æ„æ–™ä¹‹å¤–çš„çŸ¥è¯†ï¼šWikipedia ä¸ Grokipedia æœç´¢æ¨èå®¡è®¡",
      "authors": [
        "Erica Coppolillo",
        "Simone Mungari"
      ],
      "abstract": "Encyclopedic knowledge platforms are key gateways through which users explore information online. The recent release of Grokipedia, a fully AI-generated encyclopedia, introduces a new alternative to traditional, well-established platforms like Wikipedia. In this context, search engine mechanisms play an important role in guiding users exploratory paths, yet their behavior across different encyclopedic systems remains underexplored. In this work, we address this gap by providing the first comparative analysis of search engine in Wikipedia and Grokipedia.\n  Using nearly 10,000 neutral English words and their substrings as queries, we collect over 70,000 search engine results and examine their semantic alignment, overlap, and topical structure. We find that both platforms frequently generate results that are weakly related to the original query and, in many cases, surface unexpected content starting from innocuous queries. Despite these shared properties, the two systems often produce substantially different recommendation sets for the same query. Through topical annotation and trajectory analysis, we further identify systematic differences in how content categories are surfaced and how search engine results evolve over multiple stages of exploration.\n  Overall, our findings show that unexpected search engine outcomes are a common feature of both the platforms, even though they exhibit discrepancies in terms of topical distribution and query suggestions.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶å¯¹ä¼ ç»Ÿç™¾ç§‘å…¨ä¹¦ Wikipedia ä¸ AI ç”Ÿæˆçš„ç™¾ç§‘å…¨ä¹¦ Grokipedia çš„æœç´¢æ¨èæœºåˆ¶è¿›è¡Œäº†é¦–æ¬¡å¯¹æ¯”å®¡è®¡ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¿‘ 10,000 ä¸ªä¸­æ€§è‹±æ–‡å•è¯ä½œä¸ºæŸ¥è¯¢è¯ï¼Œæ”¶é›†å¹¶åˆ†æäº†è¶…è¿‡ 70,000 æ¡æœç´¢ç»“æœçš„è¯­ä¹‰å¯¹é½(semantic alignment)ã€é‡åˆåº¦åŠä¸»é¢˜ç»“æ„ã€‚è°ƒæŸ¥å‘ç°ï¼Œè¿™ä¸¤ä¸ªå¹³å°åœ¨å¤„ç†æ— å®³æŸ¥è¯¢æ—¶éƒ½ç»å¸¸ç”Ÿæˆå¼±ç›¸å…³æˆ–éé¢„æœŸçš„å†…å®¹(unexpected content)ï¼Œå±•ç°å‡ºç›¸ä¼¼çš„å±€é™æ€§ã€‚ç„¶è€Œï¼Œé’ˆå¯¹ç›¸åŒçš„æŸ¥è¯¢ï¼Œä¸¤ä¸ªç³»ç»Ÿç”Ÿæˆçš„æ¨èé›†å¾€å¾€å­˜åœ¨å®è´¨æ€§å·®å¼‚ï¼Œä¸”åœ¨å†…å®¹ç±»åˆ«çš„å‘ˆç°æ–¹å¼å’Œæœç´¢ç»“æœçš„æ¼”å˜è½¨è¿¹ä¸Šè¡¨ç°å‡ºç³»ç»Ÿæ€§åŒºåˆ«ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡åœ¨ä¸»é¢˜åˆ†å¸ƒå’ŒæŸ¥è¯¢å»ºè®®(query suggestions)æ–¹é¢å­˜åœ¨å·®å¼‚ï¼Œä½†äº§ç”Ÿéé¢„æœŸæœç´¢ç»“æœæ˜¯è¿™ä¸¤ç±»çŸ¥è¯†å¹³å°çš„å…±åŒç‰¹å¾ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17027v1",
      "published_date": "2025-12-18 19:41:58 UTC",
      "updated_date": "2025-12-18 19:41:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:52:24.398879+00:00"
    },
    {
      "arxiv_id": "2512.16920v1",
      "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
      "title_zh": "EasyV2Vï¼šé«˜è´¨é‡æŒ‡ä»¤å¼è§†é¢‘ç¼–è¾‘æ¡†æ¶",
      "authors": [
        "Jinjie Mai",
        "Chaoyang Wang",
        "Guocheng Gordon Qian",
        "Willi Menapace",
        "Sergey Tulyakov",
        "Bernard Ghanem",
        "Peter Wonka",
        "Ashkan Mirzaei"
      ],
      "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘ç¼–è¾‘åœ¨ä¸€è‡´æ€§ã€æ§åˆ¶åŠ›å’Œæ³›åŒ–æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†EasyV2Vï¼Œä¸€ä¸ªç®€å•ä¸”é«˜æ•ˆçš„åŸºäºæŒ‡ä»¤(Instruction-based)çš„è§†é¢‘ç¼–è¾‘æ¡†æ¶ã€‚åœ¨æ•°æ®å±‚é¢ï¼ŒEasyV2Vé€šè¿‡ç»“åˆç°æœ‰ä¸“å®¶æ¨¡å‹ã€å•å¸§ç›‘ç£ã€å…±äº«ä»¿å°„è¿åŠ¨çš„ä¼ªè§†é¢‘å¯¹ä»¥åŠè¿‡æ¸¡ç›‘ç£(Transition supervision)ç­‰æŠ€æœ¯ï¼Œæ„å»ºäº†é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„è§†é¢‘ç¼–è¾‘æ•°æ®é›†ã€‚åœ¨æ¨¡å‹æ¶æ„ä¸Šï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬è½¬è§†é¢‘(Text-to-Video)æ¨¡å‹å›ºæœ‰çš„ç¼–è¾‘æ½œåŠ›ï¼Œé€šè¿‡åºåˆ—æ‹¼æ¥(Sequence concatenation)å’Œè½»é‡åŒ–LoRAå¾®è°ƒå®ç°äº†é«˜æ•ˆè®­ç»ƒã€‚æ­¤å¤–ï¼ŒEasyV2Vå¼•å…¥äº†ç»Ÿä¸€çš„æ—¶ç©ºæ©ç (Mask)æœºåˆ¶ï¼Œå¹¶æ”¯æŒå¯é€‰çš„å‚è€ƒå›¾åƒ(Reference image)ï¼Œä»è€Œå®ç°äº†çµæ´»çš„å¤šæ¨¡æ€è¾“å…¥æ§åˆ¶ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒEasyV2Våœ¨å¤šç§ç¼–è¾‘åœºæ™¯ä¸‹å‡å–å¾—äº†State-of-the-Artçš„æ€§èƒ½è¡¨ç°ï¼Œå…¶è§†é¢‘ç¼–è¾‘è´¨é‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å­¦æœ¯åŠå•†ä¸šç³»ç»Ÿã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://snap-research.github.io/easyv2v/",
      "pdf_url": "https://arxiv.org/pdf/2512.16920v1",
      "published_date": "2025-12-18 18:59:57 UTC",
      "updated_date": "2025-12-18 18:59:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:52:16.286245+00:00"
    },
    {
      "arxiv_id": "2512.16919v1",
      "title": "DVGT: Driving Visual Geometry Transformer",
      "title_zh": "DVGTï¼šè‡ªåŠ¨é©¾é©¶è§†è§‰å‡ ä½• Transformer",
      "authors": [
        "Sicheng Zuo",
        "Zixun Xie",
        "Wenzhao Zheng",
        "Shaoqing Xu",
        "Fang Li",
        "Shengyin Jiang",
        "Long Chen",
        "Zhi-Xin Yang",
        "Jiwen Lu"
      ],
      "abstract": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DVGT (Driving Visual Geometry Transformer)ï¼Œæ—¨åœ¨ä»æ— ä½å§¿çš„å¤šè§†å›¾è§†è§‰è¾“å…¥åºåˆ—ä¸­é‡æ„å…¨å±€ç¨ å¯†çš„ 3D ç‚¹äº‘åœ°å›¾ï¼Œä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­åœºæ™¯å‡ ä½•æ„ŸçŸ¥çš„é€‚åº”æ€§é—®é¢˜ã€‚è¯¥æ¨¡å‹é¦–å…ˆåˆ©ç”¨ DINO éª¨å¹²ç½‘ç»œæå–å›¾åƒç‰¹å¾ï¼Œå¹¶é‡‡ç”¨äº¤æ›¿çš„è§†å›¾å†…å±€éƒ¨æ³¨æ„åŠ› (intra-view local attention)ã€è·¨è§†å›¾ç©ºé—´æ³¨æ„åŠ› (cross-view spatial attention) å’Œè·¨å¸§æ—¶é—´æ³¨æ„åŠ› (cross-frame temporal attention) æ¥æ¨æ–­å›¾åƒé—´çš„å‡ ä½•å…³ç³»ã€‚DVGT é€šè¿‡å¤šä¸ªè§£ç å¤´ç›´æ¥é¢„æµ‹ç¬¬ä¸€å¸§è‡ªèº«åæ ‡ç³»ä¸‹çš„å…¨å±€ç‚¹äº‘åœ°å›¾ä»¥åŠæ¯ä¸€å¸§çš„è‡ªèº«ä½å§¿ (ego poses)ã€‚ä¸ä¾èµ–ç²¾ç¡®ç›¸æœºå‚æ•°çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒDVGT ä¸ä½¿ç”¨æ˜¾å¼çš„ 3D å‡ ä½•å…ˆéªŒï¼Œèƒ½å¤Ÿçµæ´»å¤„ç†ä»»æ„ç›¸æœºé…ç½®ï¼Œå¹¶ç›´æ¥ä»å›¾åƒåºåˆ—é¢„æµ‹å…·æœ‰ç»å¯¹å°ºåº¦çš„åº¦é‡å‡ ä½• (metric-scaled geometry)ï¼Œæ— éœ€ä¸å¤–éƒ¨ä¼ æ„Ÿå™¨è¿›è¡ŒåæœŸå¯¹é½ã€‚é€šè¿‡åœ¨ nuScenesã€Waymo å’Œ KITTI ç­‰å¤šä¸ªå¤§è§„æ¨¡è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®éªŒç»“æœè¡¨æ˜ DVGT åœ¨å¤šç§åœºæ™¯ä¸‹çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at https://github.com/wzzheng/DVGT",
      "pdf_url": "https://arxiv.org/pdf/2512.16919v1",
      "published_date": "2025-12-18 18:59:57 UTC",
      "updated_date": "2025-12-18 18:59:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:52:48.938205+00:00"
    },
    {
      "arxiv_id": "2512.16921v1",
      "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
      "title_zh": "å…³é”®å·®å¼‚ï¼šé¢å‘èƒ½åŠ›å·®è·å‘ç°ä¸ä¿®å¤çš„æ¨¡å‹å®¡è®¡",
      "authors": [
        "Qihao Liu",
        "Chengzhi Mao",
        "Yaojie Liu",
        "Alan Yuille",
        "Wen-Sheng Chu"
      ],
      "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AuditDMï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å®¡è®¡æ¨¡å‹é—´çš„å·®å¼‚æ¥ä¸»åŠ¨å‘ç°å¹¶ä¿®å¤å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) çš„åŠŸèƒ½ç¼ºé™·ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹  (reinforcement learning) å°† MLLM å¾®è°ƒä¸ºå®¡è®¡å‘˜ï¼Œç”Ÿæˆæå…·æŒ‘æˆ˜æ€§çš„é—®é¢˜å’Œåäº‹å®å›¾åƒ (counterfactual images)ï¼Œä»¥æœ€å¤§åŒ–ç›®æ ‡æ¨¡å‹ä¹‹é—´çš„åˆ†æ­§ã€‚AuditDM èƒ½å¤Ÿæ­ç¤ºå¤šæ ·ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„å¤±è´¥æ¨¡å¼ï¼Œå¹¶æä¾›æ— éœ€æ ‡æ³¨çš„æ•°æ®ç”¨äºæ¨¡å‹ä¿®å¤ (rectification)ã€‚åœ¨å¯¹ Gemma-3 å’Œ PaliGemma-2 ç­‰å…ˆè¿›æ¨¡å‹çš„å®éªŒä¸­ï¼Œè¯¥æ¡†æ¶å‘ç°äº† 20 å¤šç§ä¸åŒçš„å¤±è´¥ç±»å‹ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨ 16 ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºæŒç»­æ”¹è¿›ã€‚å®éªŒç»“æœè¯æ˜ AuditDM ç”šè‡³èƒ½ä½¿ 3B æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Š 28B çš„åŒç±»æ¨¡å‹ï¼Œè¡¨æ˜åœ¨æ•°æ®è§„æ¨¡åŒ– (data scaling) æ”¶ç›Šé€’å‡çš„èƒŒæ™¯ä¸‹ï¼Œé’ˆå¯¹æ€§å®¡è®¡æ˜¯æ¨¡å‹è¯Šæ–­ä¸ä¼˜åŒ–çš„æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "project page: https://auditdm.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2512.16921v1",
      "published_date": "2025-12-18 18:59:57 UTC",
      "updated_date": "2025-12-18 18:59:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:53:12.272792+00:00"
    },
    {
      "arxiv_id": "2512.16917v2",
      "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
      "title_zh": "ç”Ÿæˆå¼å¯¹æŠ—æ¨ç†å™¨ï¼šåˆ©ç”¨å¯¹æŠ—å¼ºåŒ–å­¦ä¹ å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Qihao Liu",
        "Luoxin Ye",
        "Wufei Ma",
        "Yu-Cheng Chou",
        "Alan Yuille"
      ],
      "abstract": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Generative Adversarial Reasoner (GAR)ï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¨ç†èƒ½åŠ›çš„ on-policy è”åˆè®­ç»ƒæ¡†æ¶ã€‚é’ˆå¯¹ LLMs åœ¨æ•°å­¦æ¨ç†ä¸­å­˜åœ¨çš„é€»è¾‘ç¼ºé™·å’Œè®¡ç®—é”™è¯¯ï¼Œè¯¥æ¡†æ¶é€šè¿‡ Adversarial Reinforcement Learning ä½¿æ¨ç†å™¨ (Reasoner) å’ŒåŸºäº LLM çš„åˆ¤åˆ«å™¨ (Discriminator) å…±åŒæ¼”åŒ–ã€‚åˆ¤åˆ«å™¨åˆ©ç”¨è®¡ç®—é«˜æ•ˆçš„å®¡æŸ¥è°ƒåº¦æœºåˆ¶å°†æ¨ç†é“¾åˆ‡åˆ†ä¸ºé€»è¾‘å®Œæ•´çš„ç‰‡æ®µï¼Œå¹¶å¯¹æ¯æ®µçš„åˆç†æ€§æä¾›ç»“æ„åŒ–è¯„ä¼°ã€‚è¯¥å­¦ä¹ è¿‡ç¨‹ç»“åˆäº†äº’è¡¥ä¿¡å·ï¼Œæ¨ç†å™¨å› é€»è¾‘ä¸€è‡´ä¸”ç­”æ¡ˆæ­£ç¡®è·å¾—å¥–åŠ±ï¼Œè€Œåˆ¤åˆ«å™¨åˆ™å› å‡†ç¡®è¯†åˆ«é”™è¯¯è·å¾—å¥–åŠ±ï¼Œä»è€Œæä¾›äº†å¯†é›†ä¸”æ ¡å‡†è‰¯å¥½çš„ step-level å¥–åŠ±ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œåœ¨ AIME24 ä»»åŠ¡ä¸Šå°† DeepSeek-R1-Distill-Qwen-7B çš„å‡†ç¡®ç‡ä» 54.0 æå‡è‡³ 61.3ï¼Œå¹¶å°† DeepSeek-R1-Distill-Llama-8B ä» 43.7 æå‡è‡³ 53.7ã€‚æ­¤å¤–ï¼Œæ¨¡å—åŒ–çš„åˆ¤åˆ«å™¨è¿˜æ”¯æŒ Teacher Distillation å’Œ Preference Alignment ç­‰å¤šç§çµæ´»çš„å¥–åŠ±å¡‘é€ ç›®æ ‡ï¼Œæ˜¾è‘—æé«˜äº† LLMs çš„æ ·æœ¬æ•ˆç‡å’Œæ•´ä½“æ¨ç†è´¨é‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "V2: Added links to the code-generation results and additional details in the appendix",
      "pdf_url": "https://arxiv.org/pdf/2512.16917v2",
      "published_date": "2025-12-18 18:59:54 UTC",
      "updated_date": "2025-12-26 02:03:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:52:49.308152+00:00"
    },
    {
      "arxiv_id": "2512.16912v2",
      "title": "Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "title_zh": "æ¢ç´¢ä¸åˆ©ç”¨ï¼šåŸºäºè£å‰ªã€ç†µå’Œè™šå‡å¥–åŠ±å¯¹ RLVR çš„é‡æ–°å®¡è§†",
      "authors": [
        "Peter Chen",
        "Xiaopeng Li",
        "Ziniu Li",
        "Wotao Yin",
        "Xi Chen",
        "Tianyi Lin"
      ],
      "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±(Reinforcement Learning with Verifiable Rewards, RLVR)åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†èƒ½åŠ›ä¸­çš„æ¢ç´¢ä¸åˆ©ç”¨æƒè¡¡é—®é¢˜ã€‚é’ˆå¯¹è¿‘æœŸç ”ç©¶ä¸­é€šè¿‡ä¼ªå¥–åŠ±(Spurious Rewards)æŠ‘åˆ¶åˆ©ç”¨ä»¥åŠé€šè¿‡ç†µæœ€å°åŒ–(Entropy Minimization)æŠ‘åˆ¶æ¢ç´¢åè€Œèƒ½æå‡æ€§èƒ½çš„çŸ›ç›¾ç°è±¡ï¼Œè®ºæ–‡æ·±å…¥åˆ†æäº†å…¶èƒŒåçš„è¿è¡Œæœºåˆ¶ã€‚ä½œè€…é‡ç‚¹è€ƒå¯Ÿäº†ç­–ç•¥ç†µ(Policy Entropy)ä¸æ€§èƒ½çš„å…³ç³»ï¼Œä»¥åŠä¼ªå¥–åŠ±æ˜¯å¦é€šè¿‡æˆªæ–­åå·®(Clipping Bias)å’Œæ¨¡å‹æ±¡æŸ“(Model Contamination)äº§ç”Ÿæ”¶ç›Šã€‚ç ”ç©¶å‘ç°ï¼Œä¼ªå¥–åŠ±ä¸‹çš„æˆªæ–­åå·®ä¼šæ˜¾è‘—é™ä½ç­–ç•¥ç†µï¼Œä½¿æ¨¡å‹è¾“å‡ºæ›´å…·ç¡®å®šæ€§ï¼Œè€Œå•çº¯çš„ç†µæœ€å°åŒ–ä¸è¶³ä»¥æ”¹å–„æ€§èƒ½ã€‚è®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº†å¥–åŠ±ä¸åŒ¹é…æ¨¡å‹(Reward-Misalignment Model)ï¼Œè§£é‡Šäº†ä¼ªå¥–åŠ±åœ¨éæ±¡æŸ“è®¾å®šä¸‹ä¾ç„¶èƒ½å¢å¼ºæ€§èƒ½çš„åŸå› ã€‚è¯¥é¡¹å·¥ä½œé˜æ˜äº†ä¼ªå¥–åŠ±è·ç›Šçš„æœºåˆ¶ï¼Œå¹¶ä¸ºæ›´æœ‰æ•ˆçš„RLVRè®­ç»ƒæä¾›äº†ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "35 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.16912v2",
      "published_date": "2025-12-18 18:59:27 UTC",
      "updated_date": "2025-12-21 17:23:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:52:55.233935+00:00"
    },
    {
      "arxiv_id": "2512.17796v1",
      "title": "Animate Any Character in Any World",
      "title_zh": "è®©ä»»æ„è§’è‰²åœ¨ä»»æ„ä¸–ç•Œä¸­åŠ¨èµ·æ¥",
      "authors": [
        "Yitong Wang",
        "Fangyun Wei",
        "Hongyang Zhang",
        "Bo Dai",
        "Yan Lu"
      ],
      "abstract": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ä»‹ç»äº† AniXï¼Œä¸€ä¸ªæ—¨åœ¨æ”¯æŒç”¨æˆ·æŒ‡å®šè§’è‰²åœ¨ 3DGS (3D Gaussian Splatting) åœºæ™¯ä¸­æ‰§è¡Œå¼€æ”¾å¼åŠ¨ä½œçš„æ¡†æ¶ã€‚AniX ç»“åˆäº†é™æ€ä¸–ç•Œç”Ÿæˆæ¨¡å‹çš„çœŸå®æ„Ÿä¸å¯æ§å®ä½“æ¨¡å‹çš„å¯æ‰©å±•æ€§ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¼•å¯¼è§’è‰²åœ¨ç¯å¢ƒä¸­è‡ªç”±æ¢ç´¢ï¼Œå¹¶å®Œæˆä»åŸºç¡€è¿åŠ¨åˆ°ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒ (object-centric interactions) çš„å¤æ‚äº¤äº’ã€‚è¯¥ç³»ç»Ÿå°†è§†é¢‘åˆæˆå»ºæ¨¡ä¸ºæ¡ä»¶è‡ªå›å½’è§†é¢‘ç”Ÿæˆ (conditional autoregressive video generation) é—®é¢˜ï¼Œå¹¶åœ¨é¢„è®­ç»ƒè§†é¢‘ç”Ÿæˆå™¨çš„åŸºç¡€ä¸Šé€šè¿‡ç‰¹å®šçš„è®­ç»ƒç­–ç•¥æ˜¾è‘—æå‡äº†è¿åŠ¨åŠ¨åŠ›å­¦ (motion dynamics)ã€‚AniX èƒ½å¤Ÿç”Ÿæˆå…·æœ‰é«˜åº¦æ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰ä¿çœŸåº¦çš„è§†é¢‘å‰ªè¾‘ï¼ŒåŒæ—¶åœ¨ä¸åŒåŠ¨ä½œå’Œè§’è‰²ä¹‹é—´ä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯„ä¼°è¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è§†è§‰è´¨é‡ã€è§’è‰²ä¸€è‡´æ€§ã€åŠ¨ä½œå¯æ§æ€§å’Œé•¿ç¨‹è¿è´¯æ€§ (long-horizon coherence) æ–¹é¢å‡å–å¾—äº†ä¼˜å¼‚è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://snowflakewang.github.io/AniX/",
      "pdf_url": "https://arxiv.org/pdf/2512.17796v1",
      "published_date": "2025-12-18 18:59:18 UTC",
      "updated_date": "2025-12-18 18:59:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:52:46.259736+00:00"
    },
    {
      "arxiv_id": "2512.16911v1",
      "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
      "title_zh": "åéªŒè¡Œä¸ºå…‹éš†ï¼šé¢å‘é«˜æ•ˆå¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„ BC ç­–ç•¥é¢„è®­ç»ƒ",
      "authors": [
        "Andrew Wagenmaker",
        "Perry Dong",
        "Raymond Tsao",
        "Chelsea Finn",
        "Sergey Levine"
      ],
      "abstract": "Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é¢„è®­ç»ƒç­–ç•¥å¯¹Reinforcement Learning (RL)å¾®è°ƒæ•ˆæœçš„å½±å“ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„Behavioral Cloning (BC)åœ¨ç¡®ä¿åŠ¨ä½œè¦†ç›–ç‡æ–¹é¢å­˜åœ¨ç†è®ºç¼ºé™·ï¼Œå¸¸å¯¼è‡´åç»­å¾®è°ƒæ•ˆç‡ä½ä¸‹ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Posterior Behavioral Cloning (PostBC)æ¡†æ¶ï¼Œé€šè¿‡å»ºæ¨¡ç»™å®šæ¼”ç¤ºæ•°æ®é›†ä¸‹æ¼”ç¤ºè€…è¡Œä¸ºçš„åéªŒåˆ†å¸ƒï¼Œè€Œéç®€å•æ‹Ÿåˆè§‚æµ‹åŠ¨ä½œï¼Œæ¥ç¡®ä¿ç­–ç•¥å¯¹æ¼”ç¤ºè€…åŠ¨ä½œç©ºé—´çš„æœ‰æ•ˆè¦†ç›–ã€‚è¿™ç§æ–¹æ³•åœ¨ç»´æŒé¢„è®­ç»ƒé˜¶æ®µæ€§èƒ½ä¸ä½äºBCçš„åŒæ—¶ï¼Œèƒ½ä¸ºRLå¾®è°ƒæä¾›æ›´ä¼˜çš„åˆå§‹åŒ–çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ç°ä»£ç”Ÿæˆæ¨¡å‹å®ç°çš„PostBCåœ¨å¤šä¸ªæœºå™¨äººæ§åˆ¶åŸºå‡†æµ‹è¯•åŠçœŸå®ä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œå…¶å¾®è°ƒæ€§èƒ½å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„BCæ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºå¦‚ä½•é€šè¿‡æ”¹è¿›é¢„è®­ç»ƒé˜¶æ®µæ¥æå‡æœºå™¨äººç­–ç•¥çš„å¾®è°ƒæ•ˆç‡æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æŒä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16911v1",
      "published_date": "2025-12-18 18:59:17 UTC",
      "updated_date": "2025-12-18 18:59:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:53:01.295586+00:00"
    },
    {
      "arxiv_id": "2512.16907v2",
      "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos",
      "title_zh": "ä»æ¨ç†åˆ°åŠ¨ä½œï¼šåŸºäºç¬¬ä¸€äººç§°è§†è§’äººç±»äº¤äº’è§†é¢‘çš„ 3D æ‰‹éƒ¨è½¨è¿¹é¢„æµ‹",
      "authors": [
        "Mingfei Chen",
        "Yifan Wang",
        "Zhengqin Li",
        "Homanga Bharadhwaj",
        "Yujin Chen",
        "Chuan Qin",
        "Ziyi Kou",
        "Yuan Tian",
        "Eric Whitmire",
        "Rajinder Sodhi",
        "Hrvoje Benko",
        "Eli Shlizerman",
        "Yue Liu"
      ],
      "abstract": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»¥å¾€3Dæ‰‹éƒ¨è½¨è¿¹é¢„æµ‹(3D hand trajectory prediction)ä¸­è¿åŠ¨ä¸è¯­ä¹‰ç›‘ç£è„±èŠ‚ã€æ¨ç†ä¸åŠ¨ä½œè”ç³»è–„å¼±çš„é—®é¢˜ï¼Œæå‡ºäº†EgoMANã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆæ„å»ºäº†EgoMANæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹äº¤äº’é˜¶æ®µæ„ŸçŸ¥çš„å¤§è§„æ¨¡ç¬¬ä¸€è§†è§’(egocentric)æ•°æ®é›†ï¼ŒåŒ…å«21.9ä¸‡æ¡å…­è‡ªç”±åº¦(6DoF)è½¨è¿¹å’Œ300ä¸‡ä¸ªç”¨äºè¯­ä¹‰ã€ç©ºé—´åŠè¿åŠ¨æ¨ç†çš„ç»“æ„åŒ–é—®ç­”å¯¹ã€‚éšåï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†EgoMANæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§æ¨ç†åˆ°è¿åŠ¨(reasoning-to-motion)çš„æ¡†æ¶ï¼Œé€šè¿‡è½¨è¿¹æ ‡è®°æ¥å£(trajectory-token interface)å°†è§†è§‰è¯­è¨€æ¨ç†ä¸è¿åŠ¨ç”Ÿæˆç´§å¯†è”ç³»ã€‚é€šè¿‡æ¸è¿›å¼è®­ç»ƒ(progressive training)æ¥å¯¹é½æ¨ç†ä¸è¿åŠ¨åŠ¨åŠ›å­¦ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå‡†ç¡®ä¸”å…·å¤‡é˜¶æ®µæ„ŸçŸ¥èƒ½åŠ›çš„è½¨è¿¹ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­å±•ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website: https://egoman-project.github.io",
      "pdf_url": "https://arxiv.org/pdf/2512.16907v2",
      "published_date": "2025-12-18 18:59:01 UTC",
      "updated_date": "2025-12-30 21:52:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:53:55.067095+00:00"
    },
    {
      "arxiv_id": "2512.16901v1",
      "title": "Impacts of Racial Bias in Historical Training Data for News AI",
      "title_zh": "å†å²è®­ç»ƒæ•°æ®ä¸­çš„ç§æ—åè§å¯¹æ–°é—»äººå·¥æ™ºèƒ½çš„å½±å“",
      "authors": [
        "Rahul Bhargava",
        "Malene Hornstrup Jespersen",
        "Emily Boardman Ndulue",
        "Vivica Dsouza"
      ],
      "abstract": "AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ–°é—»AI(News AI)å†å²è®­ç»ƒæ•°æ®ä¸­ç§æ—åè§çš„å½±å“ï¼Œé‡ç‚¹åˆ†æäº†åŸºäºå¹¿æ³›ä½¿ç”¨çš„New York Times Annotated Corpusè®­ç»ƒçš„å¤šæ ‡ç­¾åˆ†ç±»å™¨ã€‚ç ”ç©¶é€šè¿‡å®šé‡å’Œå®šæ€§æ–¹æ³•ï¼Œè°ƒæŸ¥äº†æ¨¡å‹ä¸­ä¸€ä¸ªå…·æœ‰äº‰è®®çš„\"blacks\"ä¸»é¢˜æ ‡ç­¾ï¼Œå¹¶åˆ©ç”¨å¯è§£é‡ŠAI(explainable AI)æ–¹æ³•åˆ†æäº†å…¶ç¼–ç çš„ç‰¹å®šæ¦‚å¿µã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶è¯¥æ ‡ç­¾åœ¨æŸäº›å°‘æ•°æ—è£”ç¾¤ä½“ä¸­è¡¨ç°å‡ºé€šç”¨çš„\"racism detector\"åŠŸèƒ½ï¼Œä½†åœ¨åº”å¯¹COVID-19æ—¶æœŸçš„åäºšè£”ä»‡æ¨æŠ¥é“åŠBlack Lives Matterè¿åŠ¨ç­‰ç°ä»£è¯­å¢ƒæ—¶è¡¨ç°ä¸ä½³ã€‚è¿™ä¸€æ¡ˆä¾‹ç ”ç©¶æ­ç¤ºäº†åµŒå…¥å¼åè§å¦‚ä½•å¯¼è‡´æ–°é—»ç¼–è¾‘å®¤ä¸­çš„story discoveryã€æ‘˜è¦å’Œå—ä¼—å®šä½äº§ç”Ÿæ„å¤–è¾“å‡ºã€‚è¯¥è®ºæ–‡æœ€ç»ˆå¼ºè°ƒäº†æ–°é—»æœºæ„åœ¨å¼•å…¥AIå·¥ä½œæµå·¥å…·æ—¶ï¼Œå¦‚ä½•åœ¨åˆ©ç”¨æŠ€æœ¯ä¸é™ä½é‡ç°å†å²åè§é£é™©ä¹‹é—´å¹³è¡¡çš„æ ¹æœ¬æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16901v1",
      "published_date": "2025-12-18 18:56:11 UTC",
      "updated_date": "2025-12-18 18:56:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:53:32.806709+00:00"
    },
    {
      "arxiv_id": "2512.16891v1",
      "title": "LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation",
      "title_zh": "LinkedOutï¼šä»è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­å¯¼å‡ºä¸–ç•ŒçŸ¥è¯†è¡¨ç¤ºï¼ŒåŠ©åŠ›ä¸‹ä¸€ä»£è§†é¢‘æ¨è",
      "authors": [
        "Haichao Zhang",
        "Yao Lu",
        "Lichen Wang",
        "Yunzhe Li",
        "Daiwei Chen",
        "Yunpeng Xu",
        "Yun Fu"
      ],
      "abstract": "Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LinkedOutï¼Œæ—¨åœ¨è§£å†³Video Large Language Models (VLLMs)åœ¨è§†é¢‘æ¨èä»»åŠ¡ä¸­ç”±äºé«˜æ¨ç†å»¶è¿Ÿã€ä¸æ”¯æŒå¤šè§†é¢‘è¾“å…¥ä»¥åŠè¯­è¨€ç“¶é¢ˆå¯¼è‡´çš„è§†è§‰ç»†èŠ‚ä¸¢å¤±ç­‰æŒ‘æˆ˜ã€‚LinkedOuté€šè¿‡VLLMsç›´æ¥ä»è§†é¢‘åŸå§‹å¸§ä¸­æå–å…·æœ‰è¯­ä¹‰åŸºç¡€å’Œä¸–ç•ŒçŸ¥è¯†æ„ŸçŸ¥èƒ½åŠ›çš„Tokenï¼Œæœ‰æ•ˆä¿ç•™äº†åƒç´ çº§ç»†èŠ‚å¹¶å®ç°äº†å¿«é€Ÿæ¨ç†ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è·¨å±‚çŸ¥è¯†èåˆçš„Mixture of Experts (MoE)æœºåˆ¶ï¼Œèƒ½å¤Ÿä»ä¸°å¯Œçš„VLLMç‰¹å¾ä¸­è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æŠ½è±¡çº§åˆ«ï¼Œä»è€Œæ”¯æŒä¸ªæ€§åŒ–ä¸”å¯è§£é‡Šçš„æ¨èã€‚ä½œä¸ºé¦–ä¸ªç›´æ¥å¤„ç†åŸå§‹è§†é¢‘å¸§ä¸”æ— éœ€æ‰‹å·¥æ ‡æ³¨çš„åŸºäºVLLMçš„è§†é¢‘æ¨èæ–¹æ³•ï¼ŒLinkedOutåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†State-of-the-art (SOTA)æ°´å¹³ã€‚å®éªŒç»“æœè¯å®äº†è¯¥æ–¹æ³•åœ¨åˆ©ç”¨VLLMä¸–ç•ŒçŸ¥è¯†å…ˆéªŒè¿›è¡Œè§†è§‰æ¨ç†æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œä¸ºä½å»¶è¿Ÿã€é«˜æ€§èƒ½çš„ä¸‹ä¸€ä»£è§†é¢‘æ¨èç³»ç»Ÿæä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16891v1",
      "published_date": "2025-12-18 18:52:18 UTC",
      "updated_date": "2025-12-18 18:52:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:54:03.529552+00:00"
    },
    {
      "arxiv_id": "2512.16876v1",
      "title": "Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies",
      "title_zh": "ååŒè®­ç»ƒï¼Œç²¾å‡†è¯Šæ–­ï¼šé’ˆå¯¹VIå‹èƒ¶åŸè›‹ç™½ç›¸å…³è‚Œè¥å…»ä¸è‰¯ç—‡çš„è”é‚¦å­¦ä¹ ",
      "authors": [
        "Astrid Brull",
        "Sara Aguti",
        "VÃ©ronique Bolduc",
        "Ying Hu",
        "Daniel M. Jimenez-Gutierrez",
        "Enrique Zuazua",
        "Joaquin Del-Rio",
        "Oleksii Sliusarenko",
        "Haiyan Zhou",
        "Francesco Muntoni",
        "Carsten G. BÃ¶nnemann",
        "Xabi Uribe-Etxebarria"
      ],
      "abstract": "The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èƒ¶åŸè›‹ç™½VIç›¸å…³è‚Œè‚‰è¥å…»ä¸è‰¯(COL6-RD)ç­‰ç½•è§ç—…è¯Šæ–­ä¸­å­˜åœ¨çš„æ•°æ®ç¨€ç¼ºå’Œéšç§ç›‘ç®¡éšœç¢ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè”åˆå­¦ä¹ (Federated Learning)çš„å…¨çƒæ€§åä½œè®­ç»ƒæ–¹æ³•ã€‚ç ”ç©¶åˆ©ç”¨Sherpa.ai FLå¹³å°æ•´åˆäº†ä¸¤å®¶å›½é™…æœºæ„çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œé€šè¿‡åˆ†ææ‚£è€…æˆçº¤ç»´ç»†èƒçš„èƒ¶åŸè›‹ç™½VIå…ç–«è§å…‰æ˜¾å¾®é•œå›¾åƒ(immunofluorescence microscopy images)ï¼Œæ„å»ºäº†é«˜æ•ˆçš„è¯Šæ–­æ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå°†å›¾åƒå‡†ç¡®åˆ†ç±»ä¸ºå¤–æ˜¾å­è·³è·ƒ(exon skipping)ã€ç”˜æ°¨é…¸æ›¿æ¢(glycine substitution)å’Œä¼ªå¤–æ˜¾å­æ’å…¥(pseudoexon insertion)ä¸‰ç§æ ¸å¿ƒè‡´ç—…æœºåˆ¶ï¼Œå…¶F1-scoreè¾¾åˆ°0.82ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å•ä¸€æœºæ„æ¨¡å‹ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè”åˆå­¦ä¹ (Federated Learning)åœ¨ä¸æ³„éœ²æœ¬åœ°æ•°æ®çš„å‰æä¸‹ï¼Œæœ‰æ•ˆæå‡äº†ç½•è§ç—…è¯Šæ–­çš„æ³›åŒ–èƒ½åŠ›ä¸å‡†ç¡®åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜ä¸ºä¸ç¡®å®šæ„ä¹‰å˜å¼‚(variants of uncertain significance)çš„è§£è¯»ä»¥åŠè¯†åˆ«æ–°å‹è‡´ç—…å˜å¼‚çš„æµ‹åºç­–ç•¥ä¼˜åŒ–æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16876v1",
      "published_date": "2025-12-18 18:44:13 UTC",
      "updated_date": "2025-12-18 18:44:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:53:38.957407+00:00"
    },
    {
      "arxiv_id": "2512.16874v1",
      "title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking",
      "title_zh": "Pixel Sealï¼šåŸºäºçº¯å¯¹æŠ—å¼è®­ç»ƒçš„éšå½¢å›¾åƒä¸è§†é¢‘æ°´å°",
      "authors": [
        "TomÃ¡Å¡ SouÄek",
        "Pierre Fernandez",
        "Hady Elsahar",
        "Sylvestre-Alvise Rebuffi",
        "Valeriu Lacatusu",
        "Tuan Tran",
        "Tom Sander",
        "Alexandre Mourachko"
      ],
      "abstract": "Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Pixel Sealï¼Œä¸€ç§é’ˆå¯¹å›¾åƒå’Œè§†é¢‘ä¸å¯è§æ°´å°çš„æ–°å‹ SOTA æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ€æœ¯åœ¨é²æ£’æ€§ä¸ä¸å¯è§æ€§å¹³è¡¡æ–¹é¢çš„å±€é™ã€‚ä½œè€…è¯†åˆ«å‡ºä¼ ç»Ÿæ–¹æ³•ä¾èµ– MSE å’Œ LPIPS ç­‰ä»£ç†æ„ŸçŸ¥æŸå¤±ä¼šå¯¼è‡´æ°´å°ä¼ªå½±ï¼Œä¸”å¤šç›®æ ‡ä¼˜åŒ–çš„ä¸ç¨³å®šæ€§åŠé«˜åˆ†è¾¨ç‡ç¼©æ”¾é—®é¢˜ä¹Ÿé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¸ºæ­¤ï¼ŒPixel Seal é‡‡ç”¨äº† Adversarial-only training èŒƒå¼ï¼Œé€šè¿‡æ¶ˆé™¤ä¸å¯é çš„åƒç´ çº§æŸå¤±æå‡è§†è§‰è´¨é‡ï¼Œå¹¶å¼•å…¥ä¸‰é˜¶æ®µè®­ç»ƒè®¡åˆ’(Three-stage training schedule)ä»¥è§£è€¦é²æ£’æ€§ä¸ä¸å¯è§æ€§çš„ä¼˜åŒ–ç›®æ ‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é€šè¿‡åŸºäº JND çš„è¡°å‡å’Œè®­ç»ƒæ—¶æ¨ç†æ¨¡æ‹Ÿè§£å†³äº†é«˜åˆ†è¾¨ç‡é€‚é…é—®é¢˜ï¼Œå¹¶åˆ©ç”¨ Temporal watermark pooling æŠ€æœ¯é«˜æ•ˆæ‰©å±•è‡³è§†é¢‘é¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPixel Seal åœ¨å¤šç§å›¾åƒå˜æ¢ä¸‹å‡è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æŠ€æœ¯çš„é²æ£’æ€§ï¼Œä¸ºç°å®ä¸–ç•Œä¸­çš„æ•°å­—å†…å®¹æº¯æºæä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code and model available at https://github.com/facebookresearch/videoseal",
      "pdf_url": "https://arxiv.org/pdf/2512.16874v1",
      "published_date": "2025-12-18 18:42:19 UTC",
      "updated_date": "2025-12-18 18:42:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:53:58.871653+00:00"
    },
    {
      "arxiv_id": "2512.16873v1",
      "title": "The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI",
      "title_zh": "ç¤¾ä¼šè´£ä»»æ ˆï¼šä¸€ç§ç”¨äºç¤¾ä¼š-æŠ€æœ¯äººå·¥æ™ºèƒ½æ²»ç†çš„æ§åˆ¶ç†è®ºæ¶æ„",
      "authors": [
        "Otman A. Basir"
      ],
      "abstract": "Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ç¤¾ä¼šè´£ä»»å †æ ˆ (Social Responsibility Stack, SRS)ï¼Œæ—¨åœ¨ä¸ºç¤¾ä¼šæŠ€æœ¯ AI ç³»ç»Ÿæä¾›ä¸€å¥—å¯æ‰§è¡Œçš„å·¥ç¨‹æ²»ç†æœºåˆ¶ï¼Œä»¥è§£å†³ç°æœ‰è´Ÿè´£ä»» AI å‡†åˆ™ç¼ºä¹ç³»ç»Ÿç”Ÿå‘½å‘¨æœŸå†…çº¦æŸåŠ›çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†å…­å±‚æ¶æ„æ¨¡å‹ï¼Œå°†ç¤¾ä¼šä»·å€¼è§‚è½¬åŒ–ä¸ºæ˜¾å¼çš„çº¦æŸ (constraints)ã€é˜²æŠ¤æªæ–½ (safeguards)ã€è¡Œä¸ºæ¥å£ã€å®¡è®¡æœºåˆ¶åŠæ²»ç†æµç¨‹ã€‚SRS å°†è´£ä»»å»ºæ¨¡ä¸ºé’ˆå¯¹ç¤¾ä¼šæŠ€æœ¯ç³»ç»Ÿçš„é—­ç¯ç›‘ç£æ§åˆ¶ (closed-loop supervisory control) é—®é¢˜ï¼Œæœ‰æ•ˆæ•´åˆäº†è®¾è®¡é˜¶æ®µçš„é˜²æŠ¤ä¸è¿è¡Œæ—¶çš„åŠ¨æ€ç›‘æµ‹ã€‚é€šè¿‡ç»Ÿä¸€çš„åŸºäºçº¦æŸçš„è¡¨è¿°ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸæŒç»­ç›‘æµ‹å¹¶å¼ºåˆ¶æ‰§è¡Œå…¬å¹³æ€§ (fairness)ã€è‡ªä¸»æ€§ (autonomy)ã€è®¤çŸ¥è´Ÿè· (cognitive burden) å’Œè§£é‡Šè´¨é‡ç­‰æ ¸å¿ƒæŒ‡æ ‡ã€‚è®ºæ–‡é€šè¿‡ä¸´åºŠå†³ç­–æ”¯æŒã€åä½œå¼è‡ªåŠ¨é©¾é©¶æ±½è½¦ (autonomous vehicles) å’Œå…¬å…±éƒ¨é—¨ç³»ç»Ÿçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜äº† SRS å¦‚ä½•å°†è§„èŒƒæ€§ç›®æ ‡è½¬åŒ–ä¸ºå¯æ“ä½œçš„å·¥ç¨‹æ§åˆ¶ã€‚è¿™ä¸€æ¡†æ¶æ¶èµ·äº†ä¼¦ç†å­¦ã€æ§åˆ¶ç†è®ºä¸ AI æ²»ç†ä¹‹é—´çš„æ¡¥æ¢ï¼Œä¸ºæ„å»ºå¯é—®è´£ã€è‡ªé€‚åº”ä¸”å¯å®¡è®¡çš„ç¤¾ä¼šæŠ€æœ¯ AI ç³»ç»Ÿæä¾›äº†å®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16873v1",
      "published_date": "2025-12-18 18:42:16 UTC",
      "updated_date": "2025-12-18 18:42:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:53:44.433935+00:00"
    },
    {
      "arxiv_id": "2512.16871v1",
      "title": "Sequencing to Mitigate Catastrophic Forgetting in Continual Learning",
      "title_zh": "é€šè¿‡ä»»åŠ¡æ’åºç¼“è§£æŒç»­å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜",
      "authors": [
        "Hesham G. Moussa",
        "Aroosa Hameed",
        "Arashmid Akhavain"
      ],
      "abstract": "To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨ Continual Learning ä¸­å¦‚ä½•é€šè¿‡ä¼˜åŒ–ä»»åŠ¡åºåˆ—æ¥ç¼“è§£ Catastrophic Forgetting (CF) é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„ replay-basedã€regularization-based ç­‰äº”ç±»ç­–ç•¥ä¸åŒï¼Œæœ¬æ–‡ä»ä»»åŠ¡å‘ˆç°ç»™æ¨¡å‹çš„æœ€ä½³æ’åºè¿™ä¸€æ–°è§†è§’åˆ‡å…¥ï¼Œç ”ç©¶äº†ä»»åŠ¡æ’åºåœ¨å‡è½»é—å¿˜ä¸­çš„ä½œç”¨ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§å€Ÿé‰´ Neural Architecture Search (NAS) ç†å¿µçš„ zero-shot scoring ç®—æ³•ï¼Œç”¨äºè‡ªåŠ¨ç¡®å®šä»»åŠ¡çš„æœ€ä¼˜é¡ºåºã€‚å®éªŒç»“æœè¯æ˜ï¼Œæ™ºèƒ½çš„ä»»åŠ¡æ’åºä¸ä»…èƒ½å¤§å¹…å‡å°‘ CFï¼Œåœ¨ä¸ä¼ ç»ŸæŒç»­å­¦ä¹ æ–¹æ³•ç»“åˆæ—¶è¿˜èƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ä¸é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ‰€å±•ç°çš„ç­–ç•¥å¯¹ curriculum learning ç­‰ç›¸å…³é¢†åŸŸä¹Ÿå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The Manuscript is submitted for review under IEEE Transactions on Artificial intelligence",
      "pdf_url": "https://arxiv.org/pdf/2512.16871v1",
      "published_date": "2025-12-18 18:40:58 UTC",
      "updated_date": "2025-12-18 18:40:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:53:50.917594+00:00"
    },
    {
      "arxiv_id": "2512.16866v1",
      "title": "Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models",
      "title_zh": "é€šè¿‡æ•™å¸ˆæ¨¡å‹çŸ¥è¯†è½¬åŒ–çš„è¾¹ç¼˜ä¾§åŠç›‘ç£åœ¨çº¿å­¦ä¹ ",
      "authors": [
        "Jiabin Xue"
      ],
      "abstract": "Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: \"How to determine labels for truly future, unseen data points\". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜æœºå™¨å­¦ä¹  (Edge ML) åœ¨å¤„ç†æœªçŸ¥æ•°æ®æ—¶éš¾ä»¥è·å–æ ‡ç­¾çš„é—®é¢˜ï¼Œæå‡ºäº†çŸ¥è¯†è½¬æ¢ (Knowledge Transformation, KT) æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ˜¯ä¸€ç§ç»“åˆäº†çŸ¥è¯†è’¸é¦ (Knowledge Distillation)ã€ä¸»åŠ¨å­¦ä¹  (Active Learning) å’Œå› æœæ¨ç† (causal reasoning) çš„æ··åˆæ¶æ„ï¼Œæ—¨åœ¨ä¸ºåœ¨çº¿è®­ç»ƒçš„å­¦ç”Ÿæ¨¡å‹æä¾›æŒ‡å¯¼ã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼ŒKT å……å½“ä¸»åŠ¨å­¦ä¹ ä¸­çš„å…ˆçŸ¥ (oracle)ï¼Œé€šè¿‡è½¬åŒ–æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†æ¥ç”Ÿæˆä¼ªæ ‡ç­¾ (pseudo-labels)ï¼Œä»è€Œå®ç°åŠç›‘ç£çš„åœ¨çº¿å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ‹¥æœ‰ç¨³å®šæ•™å¸ˆæ¨¡å‹çš„å‰æä¸‹ï¼Œå­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæœ€ç»ˆè¾¾åˆ°å…¶é¢„æœŸçš„æœ€ä¼˜æ€§èƒ½ã€‚è¯¥ç ”ç©¶è¯å®åœ¨æ•™å¸ˆæ¨¡å‹ä»»åŠ¡å…·æœ‰é€šç”¨æ€§æˆ–å­¦ç”Ÿæ¨¡å‹ä»»åŠ¡æ ‡ç­¾è·å–æˆæœ¬æé«˜çš„åœºæ™¯ä¸‹ï¼ŒKT æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16866v1",
      "published_date": "2025-12-18 18:37:28 UTC",
      "updated_date": "2025-12-18 18:37:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:53:57.978549+00:00"
    },
    {
      "arxiv_id": "2512.16861v1",
      "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
      "title_zh": "ReinforceGenï¼šç»“åˆè‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆä¸å¼ºåŒ–å­¦ä¹ çš„æ··åˆæŠ€èƒ½ç­–ç•¥",
      "authors": [
        "Zihan Zhou",
        "Animesh Garg",
        "Ajay Mandlekar",
        "Caelan Garrett"
      ],
      "abstract": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ReinforceGenç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººé¢†åŸŸä¸­é•¿æ—¶ç¨‹æ“ä½œ(Long-horizon manipulation)çš„æŒ‘æˆ˜ã€‚ReinforceGené€šè¿‡ç»“åˆä»»åŠ¡åˆ†è§£(task decomposition)ã€æ•°æ®ç”Ÿæˆã€æ¨¡ä»¿å­¦ä¹ (imitation learning)å’Œè¿åŠ¨è§„åˆ’(motion planning)æ„å»ºåˆå§‹æ–¹æ¡ˆï¼Œå¹¶åˆ©ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ (reinforcement learning)çš„å¾®è°ƒæŠ€æœ¯ä¼˜åŒ–å„ç»„ä»¶ã€‚è¯¥æ¡†æ¶å°†ä»»åŠ¡åˆ†å‰²ä¸ºå¤šä¸ªå±€éƒ¨æŠ€èƒ½(skills)ï¼Œé€šè¿‡è¿åŠ¨è§„åˆ’è¿›è¡Œè¿æ¥ï¼Œåœ¨ä»…ä½¿ç”¨10ä¸ªäººç±»æ¼”ç¤ºç”Ÿæˆçš„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶éšåé€šè¿‡åœ¨çº¿é€‚åº”å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚åœ¨Robosuiteæ•°æ®é›†çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReinforceGenåœ¨è§†è§‰è¿åŠ¨æ§åˆ¶ä»»åŠ¡ä¸­è¾¾åˆ°äº†80%çš„æˆåŠŸç‡ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œå…¶å¾®è°ƒæ–¹æ³•ä½¿ç³»ç»Ÿå¹³å‡æ€§èƒ½æå‡äº†89%ï¼Œæ˜¾è‘—å¢å¼ºäº†æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ“ä½œèƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16861v1",
      "published_date": "2025-12-18 18:32:39 UTC",
      "updated_date": "2025-12-18 18:32:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:54:26.474675+00:00"
    },
    {
      "arxiv_id": "2512.16856v1",
      "title": "Distributional AGI Safety",
      "title_zh": "åˆ†å¸ƒå¼é€šç”¨äººå·¥æ™ºèƒ½å®‰å…¨",
      "authors": [
        "Nenad TomaÅ¡ev",
        "Matija Franklin",
        "Julian Jacobs",
        "SÃ©bastien Krier",
        "Simon Osindero"
      ],
      "abstract": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Distributional AGI Safetyï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿå…³æ³¨å•ä¸€ AGI ç³»ç»Ÿå®‰å…¨ä¸å¯¹é½çš„èŒƒå¼ã€‚ä½œè€…æå‡ºäº† patchwork AGI å‡è¯´ï¼Œè®¤ä¸ºé€šç”¨èƒ½åŠ›å¯èƒ½é¦–å…ˆé€šè¿‡å…·æœ‰äº’è¡¥æŠ€èƒ½çš„ sub-AGI æ™ºèƒ½ä½“ç¾¤ä½“é—´çš„åè°ƒè€Œæ˜¾ç°ï¼Œè€Œéå•ä¸€çš„å•ä½“ç³»ç»Ÿã€‚é’ˆå¯¹è¿™ä¸€è¶‹åŠ¿ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåˆ†å¸ƒå¼ AGI å®‰å…¨æ¡†æ¶ï¼Œå°†ç ”ç©¶é‡å¿ƒä»è¯„ä¼°å’Œå¯¹é½å•ä¸ªæ™ºèƒ½ä½“è½¬å‘ç®¡ç†é›†ä½“è¡Œä¸ºã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯è®¾è®¡å¹¶å®æ–½è™šæ‹Ÿæ™ºèƒ½ä½“æ²™ç®±ç»æµï¼ˆvirtual agentic sandbox economiesï¼‰ï¼Œé€šè¿‡ç¨³å¥çš„å¸‚åœºæœºåˆ¶ã€å®¡è®¡ã€å£°èª‰ç®¡ç†å’Œç›‘ç£æ¥æ²»ç†æ™ºèƒ½ä½“é—´çš„äº¤æ˜“ã€‚è¿™ä¸€æ–¹æ³•æ—¨åœ¨åº”å¯¹å…·å¤‡å·¥å…·ä½¿ç”¨å’Œåä½œèƒ½åŠ›çš„é«˜çº§æ™ºèƒ½ä½“ç¾¤è½æ‰€å¸¦æ¥çš„ç´§è¿«å®‰å…¨é£é™©ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16856v1",
      "published_date": "2025-12-18 18:29:50 UTC",
      "updated_date": "2025-12-18 18:29:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:54:05.641635+00:00"
    },
    {
      "arxiv_id": "2512.16855v1",
      "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge",
      "title_zh": "TOGGLEï¼šé¢å‘è¾¹ç¼˜ç«¯çš„æ—¶åºé€»è¾‘å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹å‹ç¼©",
      "authors": [
        "Khurram Khalil",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TOGGLEæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ä¿¡å·æ—¶åºé€»è¾‘(Signal Temporal Logic, STL)æŒ‡å¯¼çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)å‹ç¼©æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¸­è®¡ç®—èµ„æºå—é™åŠä¼ ç»Ÿå‹ç¼©å¯¼è‡´çš„è¯­è¨€ç‰¹æ€§å—æŸé—®é¢˜ã€‚TOGGLEé€šè¿‡STLæ­£å¼æŒ‡å®šå¹¶å¼ºåˆ¶æ‰§è¡Œå‹ç¼©è¿‡ç¨‹ä¸­çš„è¯­è¨€å±æ€§ï¼Œå¹¶é‡‡ç”¨å—STLé²æ£’æ€§å¼•å¯¼çš„è´å¶æ–¯ä¼˜åŒ–(Bayesian optimization)ç³»ç»Ÿåœ°æ¢ç´¢å±‚çº§é‡åŒ–(Quantization)ä¸å‰ªæ(Pruning)é…ç½®ã€‚è¯¥æ¡†æ¶æ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒå³å¯ç”Ÿæˆæ»¡è¶³å½¢å¼åŒ–çº¦æŸçš„å‹ç¼©æ¨¡å‹ï¼Œå¹¶åœ¨GPT-2ã€DeepSeek-V2ã€LLaMA 3å’ŒMistralç­‰å¤šç§æ¶æ„ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTOGGLEåœ¨æ»¡è¶³æ‰€æœ‰è¯­è¨€å±æ€§çš„å‰æä¸‹ï¼Œå®ç°äº†è®¡ç®—æˆæœ¬(FLOPs)é™ä½é«˜è¾¾3.3å€ä»¥åŠæ¨¡å‹ä½“ç§¯ç¼©å‡68.8%ã€‚ä½œä¸ºé¦–ä¸ªå°†å½¢å¼åŒ–æ–¹æ³•(Formal Methods)å¼•å…¥LLMå‹ç¼©çš„ç ”ç©¶ï¼ŒTOGGLEä¸ºåœ¨è¾¹ç¼˜ç¡¬ä»¶ä¸Šå®ç°é«˜æ•ˆä¸”å¯éªŒè¯çš„æ¨¡å‹éƒ¨ç½²æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in the IEEE ICCAD 2025 conference",
      "pdf_url": "https://arxiv.org/pdf/2512.16855v1",
      "published_date": "2025-12-18 18:27:42 UTC",
      "updated_date": "2025-12-18 18:27:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:54:39.122784+00:00"
    },
    {
      "arxiv_id": "2512.16853v1",
      "title": "GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation",
      "title_zh": "GenEval 2ï¼šåº”å¯¹æ–‡ç”Ÿå›¾è¯„ä¼°ä¸­çš„åŸºå‡†æ¼‚ç§»",
      "authors": [
        "Amita Kamath",
        "Kai-Wei Chang",
        "Ranjay Krishna",
        "Luke Zettlemoyer",
        "Yushi Hu",
        "Marjan Ghazvininejad"
      ],
      "abstract": "Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒ (Text-to-Image) æ¨¡å‹è¯„ä¼°ä¸­å‡ºç°çš„åŸºå‡†åç§» (benchmark drift) é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œå³é™æ€è¯„ä¼°æŒ‡æ ‡å› æ— æ³•è·Ÿä¸Šæ¨¡å‹èƒ½åŠ›çš„å¿«é€Ÿæ›´è¿­è€Œé€æ¸åç¦»äººç±»åˆ¤æ–­ã€‚ç ”ç©¶å‘ç°ï¼Œä¸»æµåŸºå‡† GenEval åœ¨å½“å‰æ¨¡å‹ä¸Šçš„è¯¯å·®å·²é«˜è¾¾ 17.7%ï¼Œè¡¨æ˜å…¶å·²å¤„äºé¥±å’ŒçŠ¶æ€ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ¨å‡ºäº†å…¨æ–°çš„ GenEval 2 åŸºå‡†ï¼Œé€šè¿‡å¢å¼ºåŸºç¡€è§†è§‰æ¦‚å¿µçš„è¦†ç›–èŒƒå›´å’Œæå‡ç»„åˆæ€§ (compositionality) è¦æ±‚ï¼Œä¸ºå½“å‰æ¨¡å‹æä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°ç¯å¢ƒã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº† Soft-TIFA è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ•´åˆå¯¹åŸºç¡€è§†è§‰å…ƒç´ çš„åˆ¤æ–­ï¼Œå®ç°äº†ä¸äººç±»å®¡ç¾æ›´é«˜åº¦çš„ä¸€è‡´æ€§ï¼Œå¹¶æœ‰æ•ˆé™ä½äº†æœªæ¥å‘ç”ŸåŸºå‡†åç§»çš„é£é™©ã€‚è¿™é¡¹å·¥ä½œä¸ä»…å¡«è¡¥äº†å½“å‰çš„è¯„ä¼°ç©ºç™½ï¼Œæ›´å¼ºè°ƒäº†å¯¹è‡ªåŠ¨åŒ–æ¨¡å‹è¯„ä¼°åŸºå‡†è¿›è¡ŒæŒç»­å®¡è®¡ä¸æ”¹è¿›çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16853v1",
      "published_date": "2025-12-18 18:26:56 UTC",
      "updated_date": "2025-12-18 18:26:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:55:26.682382+00:00"
    },
    {
      "arxiv_id": "2512.16851v1",
      "title": "PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy",
      "title_zh": "PrivateXRï¼šåŸºäºå¯è§£é‡Šäººå·¥æ™ºèƒ½å¼•å¯¼çš„å·®åˆ†éšç§é˜²å¾¡æ‰©å±•ç°å®ä¸­çš„éšç§æ”»å‡»",
      "authors": [
        "Ripan Kumar Kundu",
        "Istiak Ahmed",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½ä¸æ‰©å±•ç°å®(AI XR)èåˆä¸­æ•æ„Ÿæ•°æ®é¢ä¸´çš„éšç§å®‰å…¨æŒ‘æˆ˜ï¼Œæå‡ºäº†PrivateXRæ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆé˜²å¾¡æˆå‘˜æ¨ç†æ”»å‡»(MIA)å’Œé‡æ–°è¯†åˆ«æ”»å‡»(RDA)ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿå·®åˆ†éšç§(DP)æŠ€æœ¯å› å‡åŒ€æ·»åŠ å™ªå£°è€Œå¯¼è‡´æ¨¡å‹ç²¾åº¦ä¸‹é™å’Œæ¨ç†å»¶è¿Ÿçš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°ç»“åˆäº†å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)ä¸é€‰æ‹©æ€§DPæŠ€æœ¯ï¼Œé€šè¿‡äº‹åè§£é‡Š(post-hoc explanations)è¯†åˆ«AI XRæ¨¡å‹ä¸­æœ€å…·å½±å“åŠ›çš„ç‰¹å¾ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå®æ–½é’ˆå¯¹æ€§ä¿æŠ¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ™•åŠ¨ç—‡(cybersickness)ã€æƒ…æ„Ÿå’Œæ´»åŠ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•å°†MIAå’ŒRDAçš„æ”»å‡»æˆåŠŸç‡åˆ†åˆ«é™ä½äº†43%å’Œ39%ï¼ŒåŒæ—¶åˆ©ç”¨Transformeræ¨¡å‹ä¿æŒäº†é«˜è¾¾97%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒPrivateXRçš„æ¨ç†é€Ÿåº¦æ¯”ä¼ ç»ŸDPæ–¹æ³•æå‡äº†çº¦2å€ï¼Œæ˜¾è‘—å¢å¼ºäº†å®æ—¶éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚ç ”ç©¶å›¢é˜Ÿæœ€ç»ˆåœ¨HTC VIVE Proè®¾å¤‡ä¸ŠæˆåŠŸéƒ¨ç½²äº†è¯¥ç³»ç»Ÿï¼Œé€šè¿‡PrivateXRç”¨æˆ·ç•Œé¢å…è®¸ç”¨æˆ·è‡ªä¸»è°ƒèŠ‚éšç§çº§åˆ«ï¼Œåœ¨ç¡®ä¿XRæ¸¸æˆä½“éªŒçš„åŒæ—¶æä¾›äº†åšå®çš„éšç§ä¿éšœã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CR",
      "comment": "Published in the IEEE ISMAR 2025 conference",
      "pdf_url": "https://arxiv.org/pdf/2512.16851v1",
      "published_date": "2025-12-18 18:23:06 UTC",
      "updated_date": "2025-12-18 18:23:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:54:43.796353+00:00"
    },
    {
      "arxiv_id": "2512.16848v1",
      "title": "Meta-RL Induces Exploration in Language Agents",
      "title_zh": "å…ƒå¼ºåŒ–å­¦ä¹ æ¿€å‘è¯­è¨€æ™ºèƒ½ä½“çš„æ¢ç´¢èƒ½åŠ›",
      "authors": [
        "Yulun Jiang",
        "Liangze Jiang",
        "Damien Teney",
        "Michael Moor",
        "Maria Brbic"
      ],
      "abstract": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LaMerï¼Œä¸€ç§é€šç”¨çš„å…ƒå¼ºåŒ–å­¦ä¹ (Meta-RL)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨å¤„ç†å¤šè½®é•¿ç¨‹ä»»åŠ¡æ—¶é¢ä¸´çš„ä¸»åŠ¨æ¢ç´¢ä¸è¶³å’Œè¯•é”™ç»éªŒåˆ©ç”¨æ•ˆç‡ä½çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç”±è·¨å›åˆ(cross-episode)è®­ç»ƒæ¡†æ¶å’ŒåŸºäºåæ€(reflection)çš„ä¸Šä¸‹æ–‡ç­–ç•¥é€‚é…(in-context policy adaptation)ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†ç»„æˆï¼Œå‰è€…ç”¨äºä¼˜åŒ–é•¿æœŸå¥–åŠ±å¹¶è¯±å¯¼æ¢ç´¢è¡Œä¸ºï¼Œåè€…åˆ™å…è®¸æ™ºèƒ½ä½“åœ¨æ— éœ€æ¢¯åº¦æ›´æ–°çš„æƒ…å†µä¸‹æ ¹æ®ç¯å¢ƒåé¦ˆåŠ¨æ€è°ƒæ•´ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼ŒLaMeråœ¨Sokobanã€MineSweeperå’ŒWebshopä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ (RL)åŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½åˆ†åˆ«æå‡äº†11%ã€14%å’Œ19%ï¼Œä¸”åœ¨é¢å¯¹æœªè§è¿‡çš„æŒ‘æˆ˜æ€§ä»»åŠ¡æ—¶å±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥é¡¹å·¥ä½œè¡¨æ˜ï¼ŒMeta-RLèƒ½å¤Ÿä¸ºè¯­è¨€æ™ºèƒ½ä½“æä¾›ä¸€ç§åŸåˆ™æ€§çš„æ–¹æ³•æ¥ä¹ å¾—æ¢ç´¢ç­–ç•¥ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºå…¶åœ¨å¤æ‚æ–°ç¯å¢ƒä¸­çš„ç¨³å¥é€‚é…èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16848v1",
      "published_date": "2025-12-18 18:22:17 UTC",
      "updated_date": "2025-12-18 18:22:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:55:00.847351+00:00"
    },
    {
      "arxiv_id": "2512.16843v1",
      "title": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference",
      "title_zh": "LLMCacheï¼šé¢å‘ Transformer æ¨ç†åŠ é€Ÿå¤ç”¨çš„é€å±‚ç¼“å­˜ç­–ç•¥",
      "authors": [
        "Harsh Vardhan Bansal"
      ],
      "abstract": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LLMCacheï¼Œä¸€ç§æ—¨åœ¨åŠ é€Ÿ Transformer æ¨ç†çš„æ–°å‹å±‚çº§ç¼“å­˜ (layer-wise caching) æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰ç¼“å­˜æœºåˆ¶åœ¨å¤„ç†æ¨ç†å»¶è¿Ÿæ–¹é¢çš„å±€é™æ€§ï¼ŒLLMCache é€šè¿‡åŸºäºè¾“å…¥åºåˆ—è¯­ä¹‰ç›¸ä¼¼åº¦é‡ç”¨ä¸­é—´æ¿€æ´»å€¼ (intermediate activations) æ¥æ˜¾è‘—æå‡æ€§èƒ½ã€‚è¯¥æ¡†æ¶å…·æœ‰æ¨¡å‹æ— å…³æ€§ (model-agnostic)ï¼Œèƒ½å¤ŸåŒæ—¶æ”¯æŒ Encoder å’Œ Decoder æ¶æ„ï¼Œå¹¶å…è®¸åœ¨ Transformer çš„ä»»æ„å±‚çº§è¿›è¡Œç¼“å­˜ã€‚ç ”ç©¶å¼•å…¥äº†è½»é‡çº§çš„æŒ‡çº¹æœºåˆ¶ (fingerprinting mechanism) ç”¨äºåŒ¹é…è¯­ä¹‰ç›¸ä¼¼çš„è¾“å…¥ï¼Œå¹¶æå‡ºäº†è‡ªé€‚åº”é©±é€ç­–ç•¥ (adaptive eviction strategies) ä»¥ç®¡ç†ç¼“å­˜çš„æœ‰æ•ˆæ€§ã€‚åœ¨ BERT å’Œ GPT-2 æ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMCache åœ¨ SQuAD å’Œ WikiText-103 ç­‰æ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾ 3.1 å€çš„æ¨ç†åŠ é€Ÿï¼Œä¸”å‡†ç¡®ç‡ä¸‹é™æ§åˆ¶åœ¨ 0.5% ä»¥å†…ã€‚è¿™ä¸€ç»“æœè¯æ˜äº† LLMCache æ˜¯åœ¨å®é™…åº”ç”¨ä¸­ä¼˜åŒ– Transformer æ¨ç†æ•ˆç‡çš„ä¸€ç§é€šç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted and presented at 13th IEEE International Conference on Intelligent Systems and Embedded Design (ISED-2025)",
      "pdf_url": "https://arxiv.org/pdf/2512.16843v1",
      "published_date": "2025-12-18 18:18:57 UTC",
      "updated_date": "2025-12-18 18:18:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:55:26.537921+00:00"
    },
    {
      "arxiv_id": "2512.16842v1",
      "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction",
      "title_zh": "OPENTOUCHï¼šå°†å…¨æ‰‹è§¦è§‰å¼•å…¥ç°å®ä¸–ç•Œäº¤äº’",
      "authors": [
        "Yuxin Ray Song",
        "Jinzhou Li",
        "Rao Fu",
        "Devin Murphy",
        "Kaichen Zhou",
        "Rishi Shiv",
        "Yaqi Li",
        "Haoyu Xiong",
        "Crystal Elaine Owens",
        "Yilun Du",
        "Yiyue Luo",
        "Xianyi Cheng",
        "Antonio Torralba",
        "Wojciech Matusik",
        "Paul Pu Liang"
      ],
      "abstract": "The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº† OpenTouchï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨ in-the-wild ç¯å¢ƒä¸‹é‡‡é›†çš„ egocentric å…¨æ‰‹è§¦è§‰æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³è§†è§‰æ„ŸçŸ¥ä¸ç‰©ç†äº¤äº’ä¹‹é—´ç¼ºä¹è§¦è§‰æ•°æ®å¯¹é½çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å« 5.1 å°æ—¶çš„åŒæ­¥ video-touch-pose æ•°æ®ï¼Œä»¥åŠ 2,900 ä¸ªå¸¦æœ‰è¯¦ç»†æ–‡æœ¬æ ‡æ³¨çš„ç²¾é€‰å‰ªè¾‘ç‰‡æ®µã€‚ç ”ç©¶è€…åŸºäº OpenTouch å¼•å…¥äº† retrieval å’Œ classification åŸºå‡†æµ‹è¯•ï¼Œç”¨ä»¥æ¢ç©¶è§¦è§‰å¦‚ä½•é”šå®šæ„ŸçŸ¥ä¸è¡ŒåŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§¦è§‰ä¿¡å·ä¸º grasp understanding æä¾›äº†ç®€æ´ä¸”å¼ºå¤§çš„çº¿ç´¢ï¼Œèƒ½å¤Ÿæ˜¾è‘—å¢å¼º cross-modal alignmentï¼Œå¹¶èƒ½ä» in-the-wild è§†é¢‘æŸ¥è¯¢ä¸­å¯é åœ°æ£€ç´¢ã€‚é€šè¿‡å‘å¸ƒè¿™ä¸€å¸¦æœ‰æ ‡æ³¨çš„ vision-touch-pose æ•°æ®é›†å’ŒåŸºå‡†ï¼Œè¯¥ç ”ç©¶æ—¨åœ¨æ¨åŠ¨ multimodal egocentric perceptionã€embodied learning ä»¥åŠ contact-rich robotic manipulation çš„è¿›ä¸€æ­¥å‘å±•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "https://opentouch-tactile.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2512.16842v1",
      "published_date": "2025-12-18 18:18:17 UTC",
      "updated_date": "2025-12-18 18:18:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:54:55.839716+00:00"
    },
    {
      "arxiv_id": "2512.16826v1",
      "title": "Next-Generation License Plate Detection and Recognition System using YOLOv8",
      "title_zh": "åŸºäº YOLOv8 çš„ä¸‹ä¸€ä»£è½¦ç‰Œæ£€æµ‹ä¸è¯†åˆ«ç³»ç»Ÿ",
      "authors": [
        "Arslan Amin",
        "Rafia Mumtaz",
        "Muhammad Jawad Bashir",
        "Syed Mohammad Hassan Zaidi"
      ],
      "abstract": "In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº YOLOv8 çš„ä¸‹ä¸€ä»£è½¦ç‰Œæ£€æµ‹ä¸è¯†åˆ«ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½äº¤é€šç³»ç»Ÿ(Intelligent Transportation Systems)åœ¨å¤æ‚ç¯å¢ƒä¸‹å®æ—¶å‡†ç¡®æ€§çš„æŒ‘æˆ˜ã€‚ç ”ç©¶åˆ†åˆ«è¯„ä¼°äº†ä¸åŒ YOLOv8 å˜ä½“åœ¨è½¦ç‰Œè¯†åˆ«(LPR)å’Œå­—ç¬¦è¯†åˆ«(Character Recognition)ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå…¶ä¸­ YOLOv8 Nano åœ¨ LPR ä»»åŠ¡ä¸­è¾¾åˆ°äº† 0.964 çš„ precision å’Œ 0.918 çš„ mAP50ã€‚åœ¨ Character Recognition ä»»åŠ¡ä¸­ï¼ŒYOLOv8 Small è¡¨ç°å‡º 0.92 çš„ precision å’Œ 0.91 çš„ mAP50ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäº xè½´ä½ç½®çš„è‡ªå®šä¹‰å­—ç¬¦æ’åºæ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†è¯†åˆ«åºåˆ—çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å°† YOLOv8 Nano ä¸ YOLOv8 Small ç»“åˆæ„å»ºä¼˜åŒ–æµæ°´çº¿(optimized pipeline)ï¼Œè¯¥æ–¹æ¡ˆåœ¨å…¼é¡¾è®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°äº†é«˜ç²¾åº¦è¯†åˆ«ã€‚è¯¥ç ”ç©¶æˆæœä¸ºæœªæ¥åœ¨è¾¹ç¼˜è®¾å¤‡(edge devices)ä¸Šéƒ¨ç½²é«˜æ•ˆã€ç¨³å¥çš„åŸå¸‚äº¤é€šåŸºç¡€è®¾æ–½å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 5 figures. Accepted and published in the 2023 IEEE 20th International Conference on Smart Communities: Improving Quality of Life using AI, Robotics and IoT (HONET)",
      "pdf_url": "https://arxiv.org/pdf/2512.16826v1",
      "published_date": "2025-12-18 18:06:29 UTC",
      "updated_date": "2025-12-18 18:06:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:55:42.135960+00:00"
    },
    {
      "arxiv_id": "2601.02382v1",
      "title": "How to Discover Knowledge for FutureG: Contextual RAG and LLM Prompting for O-RAN",
      "title_zh": "FutureG çŸ¥è¯†å‘ç°ï¼šé¢å‘ O-RAN çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ RAG ä¸å¤§è¯­è¨€æ¨¡å‹æç¤ºæŠ€æœ¯",
      "authors": [
        "Nathan Conger",
        "Nathan Scollar",
        "Kemal Davaslioglu",
        "Yalin E. Sagduyu",
        "Sastry Kompella"
      ],
      "abstract": "We present a retrieval-augmented question answering framework for 5G/6G networks, where the Open Radio Access Network (O-RAN) has become central to disaggregated, virtualized, and AI-driven wireless systems. While O-RAN enables multi-vendor interoperability and cloud-native deployments, its fast-changing specifications and interfaces pose major challenges for researchers and practitioners. Manual navigation of these complex documents is labor-intensive and error-prone, slowing system design, integration, and deployment. To address this challenge, we adopt Contextual Retrieval-Augmented Generation (Contextual RAG), a strategy in which candidate answer choices guide document retrieval and chunk-specific context to improve large language model (LLM) performance. This improvement over traditional RAG achieves more targeted and context-aware retrieval, which improves the relevance of documents passed to the LLM, particularly when the query alone lacks sufficient context for accurate grounding. Our framework is designed for dynamic domains where data evolves rapidly and models must be continuously updated or redeployed, all without requiring LLM fine-tuning. We evaluate this framework using the ORANBenchmark-13K dataset, and compare three LLMs, namely, Llama3.2, Qwen2.5-7B, and Qwen3.0-4B, across both Direct Question Answering (Direct Q&A) and Chain-of-Thought (CoT) prompting strategies. We show that Contextual RAG consistently improves accuracy over standard RAG and base prompting, while maintaining competitive runtime and CO2 emissions. These results highlight the potential of Contextual RAG to serve as a scalable and effective solution for domain-specific Q&A in ORAN and broader 5G/6G environments, enabling more accurate interpretation of evolving standards while preserving efficiency and sustainability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹5G/6Gç½‘ç»œçš„æ£€ç´¢å¢å¼ºé—®ç­”æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Open Radio Access Network (O-RAN) è§„èŒƒå¿«é€Ÿæ¼”è¿›ç»™ç ”ç©¶å’Œä»ä¸šè€…å¸¦æ¥çš„æ–‡æ¡£å¯¼èˆªæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒé‡‡ç”¨äº†æƒ…å¢ƒæ£€ç´¢å¢å¼ºç”Ÿæˆ (Contextual RAG) ç­–ç•¥ï¼Œåˆ©ç”¨å€™é€‰ç­”æ¡ˆé€‰é¡¹å¼•å¯¼æ–‡æ¡£æ£€ç´¢å’Œåˆ†å—ç‰¹å®šä¸Šä¸‹æ–‡ï¼Œä»è€Œæ˜¾è‘—æå‡äº†å¤§è¯­è¨€æ¨¡å‹ (LLM) åœ¨èƒŒæ™¯ä¿¡æ¯ä¸è¶³æ—¶çš„æ£€ç´¢ç²¾åº¦å’Œå“åº”ç›¸å…³æ€§ã€‚ç ”ç©¶åœ¨ ORANBenchmark-13K æ•°æ®é›†ä¸Šå¯¹æ¯”äº† Llama3.2ã€Qwen2.5-7B å’Œ Qwen3.0-4B æ¨¡å‹åœ¨ç›´æ¥é—®ç­” (Direct Q&A) ä¸é“¾å¼æ€ç»´ (Chain-of-Thought) æç¤ºç­–ç•¥ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒContextual RAG åœ¨å‡†ç¡®æ€§ä¸Šä¸€è‡´ä¼˜äºæ ‡å‡† RAG å’ŒåŸºç¡€æç¤ºï¼ŒåŒæ—¶åœ¨è¿è¡Œæ—¶é—´å’ŒäºŒæ°§åŒ–ç¢³æ’æ”¾æ–¹é¢ä¿æŒäº†é«˜æ•ˆæ€§ã€‚è¯¥å·¥ä½œè¯æ˜äº† Contextual RAG æ˜¯ O-RAN é¢†åŸŸç‰¹å®šé—®ç­”çš„ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨æ— éœ€æ¨¡å‹å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå®ç°å¯¹åŠ¨æ€æ¼”è¿›æ ‡å‡†çš„ç²¾ç¡®è§£è¯»ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.02382v1",
      "published_date": "2025-12-18 18:03:59 UTC",
      "updated_date": "2025-12-18 18:03:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:55:15.100543+00:00"
    },
    {
      "arxiv_id": "2512.16814v1",
      "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è‡ªç„¶è¯­è¨€åˆ°æ—¶åºé€»è¾‘è¯­æ³•å¼ºåˆ¶ç¿»è¯‘",
      "authors": [
        "William English",
        "Dominic Simon",
        "Sumit Kumar Jha",
        "Rickard Ewetz"
      ],
      "abstract": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GraFTï¼ˆGrammar Forced Translationï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°†è‡ªç„¶è¯­è¨€ï¼ˆNatural Languageï¼‰ç¿»è¯‘ä¸ºæ—¶åºé€»è¾‘ï¼ˆTemporal Logicï¼‰æ—¶é¢ä¸´çš„åŸå­å‘½é¢˜ï¼ˆAPsï¼‰æå–ã€å…±æŒ‡æ¶ˆè§£ä»¥åŠæœ‰é™æ•°æ®ä¸‹çš„å­¦ä¹ éš¾é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•å…è®¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»å®Œæ•´è¯æ±‡è¡¨ä¸­é¢„æµ‹ä»¤ç‰Œä¸åŒï¼ŒGraFTé€šè¿‡è¯­æ³•å¼ºåˆ¶æ‰‹æ®µåœ¨æ¯ä¸€æ­¥ä»…å…è®¸æå°‘æ•°æœ‰æ•ˆä»¤ç‰Œè¾“å‡ºï¼Œä»è€Œæ˜¾è‘—ç¼©å°äº†è§£ç©ºé—´å¹¶é™ä½äº†ä»»åŠ¡å¤æ‚æ€§ã€‚è¯¥æ–‡è¿˜ä»ç†è®ºä¸Šè®ºè¯äº†è¿™ç§ç©ºé—´ç¼©å‡å¯¹æé«˜å­¦ä¹ æ•ˆç‡çš„åˆç†æ€§ã€‚åœ¨CWã€GLTLå’ŒNaviåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒGraFTå°†ç«¯åˆ°ç«¯ç¿»è¯‘å‡†ç¡®ç‡å¹³å‡æå‡äº†5.49%ï¼Œè€ŒåŸŸå¤–ï¼ˆout-of-domainï¼‰ç¿»è¯‘å‡†ç¡®ç‡åˆ™æå‡äº†14.06%ã€‚è¿™ä¸€æˆæœæ˜¾è‘—æå‡äº†äººç±»ä¸æœºå™¨äººåŠè‡ªä¸»ç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œé€šä¿¡çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16814v1",
      "published_date": "2025-12-18 17:55:15 UTC",
      "updated_date": "2025-12-18 17:55:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:55:14.520750+00:00"
    },
    {
      "arxiv_id": "2512.16813v1",
      "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
      "title_zh": "åŸºäºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„é›†ç¾¤ç½‘ç»œååŒæŠ—å¹²æ‰°éŸ§æ€§",
      "authors": [
        "Bahman Abolhassani",
        "Tugba Erpek",
        "Kemal Davaslioglu",
        "Yalin E. Sagduyu",
        "Sastry Kompella"
      ],
      "abstract": "Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ååº”å¼å¹²æ‰°æœº(Reactive jammers)å¯¹æœºå™¨äººé›†ç¾¤ç½‘ç»œé€šä¿¡åŠç¼–é˜Ÿå®Œæ•´æ€§æ„æˆçš„ä¸¥é‡å¨èƒï¼Œæå‡ºäº†ä¸€ç§åŸºäºQMIXç®—æ³•çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤æ‚å¹²æ‰°ç¯å¢ƒä¸‹çš„é€šä¿¡éŸ§æ€§ã€‚ä¼ ç»Ÿçš„å›ºå®šåŠŸç‡æ§åˆ¶æˆ–é™æ€è·³é¢‘åœ¨åº”å¯¹è‡ªé€‚åº”å¯¹æ‰‹æ—¶å¾€å¾€å¤±æ•ˆï¼Œè€Œè¯¥æ–¹æ¡ˆå…è®¸é›†ç¾¤ä¸­çš„å„æ™ºèƒ½ä½“ååŒé€‰æ‹©å‘å°„é¢‘ç‡(Channel)å’ŒåŠŸç‡ã€‚QMIXç®—æ³•é€šè¿‡å­¦ä¹ é›†ä¸­å¼ä½†å¯åˆ†è§£çš„åŠ¨ä½œä»·å€¼å‡½æ•°ï¼Œå®ç°äº†åè°ƒå¼çš„åˆ†å¸ƒå¼æ‰§è¡Œï¼Œæœ‰æ•ˆåº”å¯¹å…·æœ‰é©¬å°”å¯å¤«é˜ˆå€¼åŠ¨åŠ›å­¦(Markovian threshold dynamics)ç‰¹å¾çš„å¹²æ‰°è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQMIXçš„æ€§èƒ½æ¥è¿‘ç†è®ºä¸Šçš„æœ€ä¼˜ç­–ç•¥(Genie-aided optimal policy)ï¼Œä¸”åœ¨ååé‡å’ŒæŠ—å¹²æ‰°é¢‘ç‡æ–¹é¢å‡ä¼˜äºå±€éƒ¨ä¸Šç½®ä¿¡ç•Œ(UCB)åŠæ— çŠ¶æ€ååº”ç­–ç•¥ç­‰åŸºå‡†æ–¹æ³•ã€‚è¿™ä¸€æˆæœè¯æ˜äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åœ¨ä¿éšœäº‰ç«¯ç¯å¢ƒä¸‹è‡ªä¸»é›†ç¾¤é€šä¿¡å®‰å…¨ä¸ä»»åŠ¡æˆåŠŸç‡æ–¹é¢çš„æ˜¾è‘—æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16813v1",
      "published_date": "2025-12-18 17:54:20 UTC",
      "updated_date": "2025-12-18 17:54:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:55:34.367205+00:00"
    },
    {
      "arxiv_id": "2512.16795v1",
      "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs",
      "title_zh": "ä»äº‹å®åˆ°ç»“è®ºï¼šåœ¨æ£€ç´¢å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ä¸­é›†æˆæ¼”ç»æ¨ç†",
      "authors": [
        "Shubham Mishra",
        "Samyek Jain",
        "Gorang Mehrishi",
        "Shiv Tiwari",
        "Harsh Sharma",
        "Pratik Narang",
        "Dhruv Kumar"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)åœ¨é¢å¯¹å†²çªã€è¿‡æ—¶æˆ–ä¸»è§‚å¤–éƒ¨è¯æ®æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªé›†æˆæ¼”ç»æ¨ç†çš„æ¨ç†è¿½è¸ªå¢å¼ºå‹RAGæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ–‡æ¡£çº§è£å®š(document-level adjudication)ã€å†²çªåˆ†æ(conflict analysis)å’Œè½åœ°åˆæˆ(grounded synthesis)ä¸‰ä¸ªé˜¶æ®µï¼Œä¸ºæ¨¡å‹æä¾›äº†ç»“æ„åŒ–ä¸”å¯è§£é‡Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿäº§å‡ºå¸¦æœ‰å¼•ç”¨é“¾æ¥çš„å›ç­”æˆ–åˆç†çš„æ‹’ç»ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†Conflict-Aware Trust-Score (CATS)è¯„ä¼°æµæ°´çº¿ï¼Œåˆ©ç”¨LLM-as-a-Judgeæœºåˆ¶ä»äº‹å®å‡†ç¡®æ€§ã€æ‹’ç»å‡†ç¡®æ€§å’Œå†²çªè¡Œä¸ºå¯¹é½ç­‰ç»´åº¦è¿›è¡Œå…¨é¢è¡¡é‡ã€‚é€šè¿‡å»ºç«‹åŒ…å«539ä¸ªæŸ¥è¯¢çš„æ¨ç†æ•°æ®é›†å¹¶è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning)ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå…¶ä¸­Qwenæ¨¡å‹çš„ç«¯åˆ°ç«¯ç­”æ¡ˆæ­£ç¡®ç‡ä»0.069å¤§å¹…æå‡è‡³0.883ã€‚è¯¥å·¥ä½œä¸ºæ„å»ºå…·å¤‡å†²çªæ„ŸçŸ¥èƒ½åŠ›ä¸”å¯è§£é‡Šçš„RAGç³»ç»Ÿæä¾›äº†é‡è¦ç†è®ºä¸å®è¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2512.16795v1",
      "published_date": "2025-12-18 17:27:51 UTC",
      "updated_date": "2025-12-18 17:27:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:56:12.240793+00:00"
    },
    {
      "arxiv_id": "2512.16792v1",
      "title": "Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint",
      "title_zh": "é¢„ç®—çº¦æŸä¸‹çš„æ—¶å»¶æ„ŸçŸ¥å¤šé˜¶æ®µè¾¹ç¼˜æœåŠ¡å™¨å‡çº§",
      "authors": [
        "Endar Suprih Wihidayat",
        "Sieteng Soh",
        "Kwan-Wu Chin",
        "Duc-son Pham"
      ],
      "abstract": "In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¤šé˜¶æ®µè¾¹ç¼˜æœåŠ¡å™¨å‡çº§(Multi-stage Edge Server Upgrade, M-ESU)è¿™ä¸€æ–°å‹ç½‘ç»œè§„åˆ’é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³å¤šæ¥å…¥è¾¹ç¼˜è®¡ç®—(MEC)ç³»ç»Ÿåœ¨é•¿æœŸæ¼”è¿›è¿‡ç¨‹ä¸­çš„å®¹é‡æ‰©å±•ä¸æ€§èƒ½ä¼˜åŒ–æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åœ¨é¢„ç®—çº¦æŸä¸‹ï¼ŒååŒå†³ç­–æ–°æœåŠ¡å™¨çš„éƒ¨ç½²ã€ç°æœ‰æœåŠ¡å™¨çš„å‡çº§ä»¥åŠæœ€ä¼˜çš„ä»»åŠ¡å¸è½½(Task Offloading)ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–æ»¡è¶³å»¶è¿Ÿè¦æ±‚çš„å¹³å‡ä»»åŠ¡æ•°ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’(Mixed Integer Linear Programming, MILP)æ¨¡å‹ä»¥æ±‚å–æœ€ä¼˜è§£ï¼Œå¹¶é’ˆå¯¹å¤§è§„æ¨¡åœºæ™¯å¼€å‘äº†é«˜æ•ˆçš„å¯å‘å¼ç®—æ³•(M-ESU/H)ã€‚ä»¿çœŸç»“æœæ˜¾ç¤ºï¼ŒM-ESU/H åœ¨å°å‹ç½‘ç»œä¸­ä¸æœ€ä¼˜è§£çš„è¯¯å·®ä»…åœ¨1.25%ä»¥å†…ï¼Œä¸”å¤„ç†é€Ÿåº¦æå‡äº†æ•°ä¸ªæ•°é‡çº§ã€‚åœ¨å¤§å‹ç½‘ç»œå®éªŒä¸­ï¼Œè¯¥ç®—æ³•ç›¸æ¯”å…¶ä»–åŸºå‡†å¯å‘å¼æ–¹æ¡ˆåœ¨ä»»åŠ¡æ»¡æ„åº¦ä¸Šå®ç°äº†é«˜è¾¾21.57%çš„æå‡ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚åŠ¨æ€éœ€æ±‚ä¸‹çš„æ‰©å±•æ€§å’Œé•¿æœŸå®è·µåº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "17 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16792v1",
      "published_date": "2025-12-18 17:25:55 UTC",
      "updated_date": "2025-12-18 17:25:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:56:12.098957+00:00"
    },
    {
      "arxiv_id": "2512.16791v1",
      "title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals",
      "title_zh": "KineSTï¼šåŸºäºè¿åŠ¨å­¦å¼•å¯¼æ—¶ç©ºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„ç¨€ç–ä¿¡å·äººä½“è¿åŠ¨è¿½è¸ª",
      "authors": [
        "Shuting Zhao",
        "Zeyu Xiao",
        "Xinrong Chen"
      ],
      "abstract": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KineSTï¼Œä¸€ç§è¿åŠ¨å­¦å¼•å¯¼çš„æ—¶ç©ºçŠ¶æ€ç©ºé—´æ¨¡å‹(State Space Model)ï¼Œæ—¨åœ¨è§£å†³AR/VRåœºæ™¯ä¸­åˆ©ç”¨å¤´æˆ´å¼æ˜¾ç¤ºå™¨æ•æ‰çš„ç¨€ç–ä¿¡å·è¿›è¡Œå…¨èº«è¿åŠ¨è¿½è¸ªçš„éš¾é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬ã€æ—¶åºè¿è´¯æ€§å’Œç²¾åº¦å¹³è¡¡æ–¹é¢çš„å±€é™æ€§ï¼ŒKineSTåœ¨çŠ¶æ€ç©ºé—´äºŒé‡æ€§(State Space Duality)æ¡†æ¶ä¸‹ï¼Œå°†æ‰«æç­–ç•¥é‡æ„ä¸ºåµŒå…¥è¿åŠ¨å­¦å…ˆéªŒçš„åŒå‘æ‰«æï¼Œä»¥æœ‰æ•ˆæ•è·å¤æ‚çš„å…³èŠ‚ä¾èµ–å…³ç³»ã€‚è¯¥æ¡†æ¶é€šè¿‡æ··åˆæ—¶ç©ºè¡¨å¾å­¦ä¹ æ–¹æ³•ç´§å¯†è€¦åˆç©ºé—´ä¸æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œå¹¶å¼•å…¥å‡ ä½•è§’é€Ÿåº¦æŸå¤±(Geometric angular velocity loss)æ¥æ–½åŠ ç‰©ç†çº¦æŸï¼Œä»è€Œæå‡æ—‹è½¬å˜åŒ–çš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒKineSTä½œä¸ºä¸€ä¸ªè½»é‡åŒ–æ¨¡å‹ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—å¢å¼ºäº†è¿åŠ¨çš„æ—¶åºä¸€è‡´æ€§ï¼Œä¸ºè™šæ‹Ÿä¸ç°å®çš„äº¤äº’æä¾›äº†æ›´çœŸå®çš„å§¿æ€é‡å»ºæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.16791v1",
      "published_date": "2025-12-18 17:25:47 UTC",
      "updated_date": "2025-12-18 17:25:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:56:28.932283+00:00"
    },
    {
      "arxiv_id": "2512.16975v1",
      "title": "InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression",
      "title_zh": "InfoTokï¼šåŸºäºä¿¡æ¯è®ºå‹ç¼©çš„è‡ªé€‚åº”ç¦»æ•£è§†é¢‘åˆ†è¯å™¨",
      "authors": [
        "Haotian Ye",
        "Qiyuan He",
        "Jiaqi Han",
        "Puheng Li",
        "Jiaojiao Fan",
        "Zekun Hao",
        "Fitsum Reda",
        "Yogesh Balaji",
        "Huayu Chen",
        "Sheng Liu",
        "Angela Yao",
        "James Zou",
        "Stefano Ermon",
        "Haoxiang Wang",
        "Ming-Yu Liu"
      ],
      "abstract": "Accurate and efficient discrete video tokenization is essential for long video sequences processing. Yet, the inherent complexity and variable information density of videos present a significant bottleneck for current tokenizers, which rigidly compress all content at a fixed rate, leading to redundancy or information loss. Drawing inspiration from Shannon's information theory, this paper introduces InfoTok, a principled framework for adaptive video tokenization. We rigorously prove that existing data-agnostic training methods are suboptimal in representation length, and present a novel evidence lower bound (ELBO)-based algorithm that approaches theoretical optimality. Leveraging this framework, we develop a transformer-based adaptive compressor that enables adaptive tokenization. Empirical results demonstrate state-of-the-art compression performance, saving 20% tokens without influence on performance, and achieving 2.3x compression rates while still outperforming prior heuristic adaptive approaches. By allocating tokens according to informational richness, InfoTok enables a more compressed yet accurate tokenization for video representation, offering valuable insights for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘å†…å®¹çš„å¤æ‚æ€§ä¸ä¿¡æ¯å¯†åº¦å·®å¼‚å¯¼è‡´ä¼ ç»Ÿå›ºå®šé€Ÿç‡åˆ†è¯å™¨äº§ç”Ÿå†—ä½™æˆ–ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ï¼Œæå‡ºäº†InfoTokè¿™ä¸€åŸºäºé¦™å†œä¿¡æ¯è®ºçš„è‡ªé€‚åº”è§†é¢‘åˆ†è¯ï¼ˆAdaptive Video Tokenizationï¼‰æ¡†æ¶ã€‚ç ”ç©¶è€…åœ¨ç†è®ºä¸Šè¯æ˜äº†ç°æœ‰æ•°æ®æ— å…³è®­ç»ƒæ–¹æ³•åœ¨è¡¨ç¤ºé•¿åº¦ä¸Šçš„å±€é™æ€§ï¼Œå¹¶å¼€å‘å‡ºä¸€ç§åŸºäºè¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰çš„ç®—æ³•ä»¥é€¼è¿‘ç†è®ºæœ€ä¼˜æ€§èƒ½ã€‚åˆ©ç”¨è¯¥æ¡†æ¶æ„å»ºçš„Transformer-basedè‡ªé€‚åº”å‹ç¼©å™¨èƒ½å¤Ÿæ ¹æ®è§†é¢‘ä¿¡æ¯ä¸°å¯Œåº¦åŠ¨æ€åˆ†é…Tokenï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„è¡¨å¾ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼ŒInfoTokåœ¨ä¿æŒæ€§èƒ½ä¸å˜çš„æƒ…å†µä¸‹å¯èŠ‚çœ20%çš„Tokenï¼Œå¹¶è¾¾åˆ°2.3å€çš„å‹ç¼©ç‡ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰çš„å¯å‘å¼è‡ªé€‚åº”æ–¹æ³•ã€‚è¿™ä¸€ç ”ç©¶ä¸ºé•¿è§†é¢‘åºåˆ—çš„é«˜æ•ˆå¤„ç†æä¾›äº†æ›´å…·å‹ç¼©æ€§ä¸”å‡†ç¡®çš„æ–¹æ¡ˆï¼Œä¸ºæœªæ¥çš„è§†é¢‘æ™ºèƒ½ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16975v1",
      "published_date": "2025-12-18 17:13:59 UTC",
      "updated_date": "2025-12-18 17:13:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:56:26.558356+00:00"
    },
    {
      "arxiv_id": "2512.16780v1",
      "title": "Towards Mass Spectrum Analysis with ASP",
      "title_zh": "é¢å‘ ASP çš„è´¨è°±åˆ†æ",
      "authors": [
        "Nils KÃ¼chenmeister",
        "Alex Ivliev",
        "Markus KrÃ¶tzsch"
      ],
      "abstract": "We present a new use of Answer Set Programming (ASP) to discover the molecular structure of chemical samples based on the relative abundance of elements and structural fragments, as measured in mass spectrometry. To constrain the exponential search space for this combinatorial problem, we develop canonical representations of molecular structures and an ASP implementation that uses these definitions. We evaluate the correctness of our implementation over a large set of known molecular structures, and we compare its quality and performance to other ASP symmetry-breaking methods and to a commercial tool from analytical chemistry. Under consideration in Theory and Practice of Logic Programming (TPLP).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨Answer Set Programming (ASP)æ¥æ¨æ–­åŒ–å­¦æ ·å“åˆ†å­ç»“æ„çš„æ–°æ–¹æ³•ï¼Œä¸»è¦ä¾æ®è´¨è°±åˆ†æä¸­æµ‹å¾—çš„å…ƒç´ ç›¸å¯¹ä¸°åº¦å’Œç»“æ„ç¢ç‰‡ã€‚ä¸ºäº†çº¦æŸè¯¥ç»„åˆé—®é¢˜å¸¦æ¥çš„æŒ‡æ•°çº§æœç´¢ç©ºé—´ï¼Œä½œè€…å¼€å‘äº†åˆ†å­ç»“æ„çš„è§„èŒƒè¡¨ç¤º(canonical representations)åŠå…¶å¯¹åº”çš„ASPå®ç°æ–¹æ¡ˆã€‚ç ”ç©¶é€šè¿‡å¯¹å¤§é‡å·²çŸ¥åˆ†å­ç»“æ„è¿›è¡Œæµ‹è¯•éªŒè¯äº†è¯¥ç³»ç»Ÿçš„æ­£ç¡®æ€§ï¼Œå¹¶å°†å…¶æ€§èƒ½ä¸ç°æœ‰çš„ASPå¯¹ç§°æ€§ç ´ç¼º(symmetry-breaking)æ–¹æ³•ä»¥åŠå•†ä¸šåˆ†æåŒ–å­¦å·¥å…·è¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è¾…åŠ©å‘ç°åˆ†å­ç»“æ„ï¼Œå±•ç¤ºäº†é€»è¾‘ç¼–ç¨‹åœ¨åˆ†æåŒ–å­¦é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "22 pages, 11 figures. Extended version of a paper accepted at 17th International Conference on Logic Programming and Non-monotonic Reasoning (LPNMR 2024). Under consideration in Theory and Practice of Logic Programming (TPLP)",
      "pdf_url": "https://arxiv.org/pdf/2512.16780v1",
      "published_date": "2025-12-18 17:13:15 UTC",
      "updated_date": "2025-12-18 17:13:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:57:07.966706+00:00"
    },
    {
      "arxiv_id": "2512.16770v1",
      "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation",
      "title_zh": "GinSignï¼šé¢å‘æ—¶åºé€»è¾‘ç¿»è¯‘çš„è‡ªç„¶è¯­è¨€ä¸ç³»ç»Ÿç­¾åå…³è”æ¡†æ¶",
      "authors": [
        "William English",
        "Chase Walker",
        "Dominic Simon",
        "Rickard Ewetz"
      ],
      "abstract": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GinSign æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªç„¶è¯­è¨€ (NL) åˆ°æ—¶åºé€»è¾‘ (TL) ç¿»è¯‘è¿‡ç¨‹ä¸­åŸå­æ˜ å°„ (atom grounding) å‡†ç¡®æ€§ä½çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚GinSign å°†åŸå­æ˜ å°„ä»»åŠ¡ä»å¤æ‚çš„è‡ªç”±æ–‡æœ¬ç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºç»“æ„åŒ–çš„åˆ†ç±»é—®é¢˜ï¼Œé€šè¿‡ä¸“é—¨çš„æ˜ å°„æ¨¡å‹å°† NL ç‰‡æ®µç²¾ç¡®å¯¹åº”åˆ°ç³»ç»Ÿç­¾å (system signature) å®šä¹‰çš„åŸå­é›†åˆä¸­ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†å±‚æ¬¡åŒ–åˆ†è§£ç­–ç•¥ï¼Œé¦–å…ˆé¢„æµ‹è°“è¯æ ‡ç­¾ (predicate labels)ï¼Œéšåé€‰æ‹©ç±»å‹åŒ¹é…çš„å¸¸é‡å‚æ•° (constant arguments)ï¼Œä»è€Œå®ç°äº†å¯¹åŸå­å‘½é¢˜çš„ç²¾å‡†åˆ†é…ã€‚è¿™ç§è®¾è®¡å…è®¸ä½¿ç”¨å‚æ•°é‡è¾ƒå°çš„æ©ç è¯­è¨€æ¨¡å‹ (masked language models) ä»£æ›¿æ˜‚è´µçš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs)ï¼Œåœ¨é™ä½è®¡ç®—èµ„æºéœ€æ±‚çš„åŒæ—¶æå‡äº†å¯é æ€§ã€‚è·¨é¢†åŸŸå®éªŒè¡¨æ˜ï¼ŒGinSign èƒ½å¤Ÿæ˜¾è‘—å‡å°‘è¯­ä¹‰ä¸ç­‰ä»·çš„é—®é¢˜ï¼Œå…¶æ¥åœ°çš„é€»è¾‘ç­‰ä»·åˆ†æ•° (logical-equivalence scores) è¾¾åˆ°äº† 95.5%ã€‚ç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ (SOTA)ï¼ŒGinSign åœ¨ç¿»è¯‘å‡†ç¡®ç‡ä¸Šæå‡äº† 1.4 å€ï¼Œä¸ºæ„å»ºå¯ä¿¡è‡ªä¸»ç³»ç»Ÿçš„è‡ªåŠ¨åŒ–å½¢å¼åŒ–è§„èŒƒéªŒè¯æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16770v1",
      "published_date": "2025-12-18 17:03:07 UTC",
      "updated_date": "2025-12-18 17:03:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:58:45.443457+00:00"
    },
    {
      "arxiv_id": "2512.16755v1",
      "title": "CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?",
      "title_zh": "CitySeekerï¼šè§†è§‰è¯­è¨€æ¨¡å‹å¦‚ä½•æ¢ç´¢åŸºäºéšå¼äººç±»éœ€æ±‚çš„å…·èº«åŸå¸‚å¯¼èˆªï¼Ÿ",
      "authors": [
        "Siqi Wang",
        "Chao Liang",
        "Yunfan Gao",
        "Erxin Yu",
        "Sen Li",
        "Yushi Li",
        "Jing Li",
        "Haofen Wang"
      ],
      "abstract": "Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., \"I am thirsty\") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling \"last-mile\" navigation challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CitySeekerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤„ç†äººç±»éšå¼éœ€æ±‚æ—¶ï¼ŒäºåŠ¨æ€åŸå¸‚ç¯å¢ƒä¸­è¿›è¡Œå…·ä½“åŒ–å¯¼èˆª(Embodied Urban Navigation)çš„ç©ºé—´æ¨ç†ä¸å†³ç­–èƒ½åŠ›çš„å…¨æ–°åŸºå‡†ã€‚CitySeekeråŒ…å«åˆ†å¸ƒåœ¨8ä¸ªåŸå¸‚çš„6,440æ¡è½¨è¿¹ï¼Œæ¶µç›–äº†7ç§ç›®æ ‡é©±åŠ¨åœºæ™¯ä¸‹çš„å¤šæ ·åŒ–è§†è§‰ç‰¹å¾å’Œéšå¼éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å‡ºè‰²çš„æ¨¡å‹å¦‚Qwen2.5-VL-32B-Instructï¼Œåœ¨ä»»åŠ¡å®Œæˆç‡ä¸Šä¹Ÿä»…ä¸º21.1%ï¼Œæš´éœ²å‡ºæ¨¡å‹åœ¨é•¿ç¨‹æ¨ç†(Long-horizon Reasoning)ä¸­çš„è¯¯å·®ç´¯ç§¯ã€ç©ºé—´è®¤çŸ¥ä¸è¶³ä»¥åŠç»éªŒå¬å›ç¼ºé™·ç­‰å…³é”®ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æ¢ç´¢äº†å›æº¯æœºåˆ¶(Backtracking Mechanisms)ã€å¢å¼ºç©ºé—´è®¤çŸ¥å’ŒåŸºäºè®°å¿†çš„æ£€ç´¢(Memory-Based Retrieval, BCR)ç­‰å—äººç±»è®¤çŸ¥åœ°å›¾å¯å‘çš„ç­–ç•¥ï¼Œå¼ºè°ƒäº†è¿­ä»£è§‚æµ‹-æ¨ç†å¾ªç¯å’Œè‡ªé€‚åº”è·¯å¾„ä¼˜åŒ–çš„é‡è¦æ€§ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘å…·å¤‡é²æ£’ç©ºé—´æ™ºèƒ½ã€èƒ½å¤Ÿåº”å¯¹â€œæœ€åä¸€å…¬é‡Œâ€å¯¼èˆªæŒ‘æˆ˜çš„VLMsæä¾›äº†é‡è¦çš„å®è·µè§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16755v1",
      "published_date": "2025-12-18 16:53:12 UTC",
      "updated_date": "2025-12-18 16:53:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:57:29.438708+00:00"
    },
    {
      "arxiv_id": "2512.16750v1",
      "title": "Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error",
      "title_zh": "åˆç†æ€§å³å¤±è´¥ï¼šLLMä¸äººç±»å¦‚ä½•å…±åŒæ„å»ºè®¤è¯†è®ºé”™è¯¯",
      "authors": [
        "Claudia Vale Oliveira",
        "Nelson Zagalo",
        "Filipe Silva",
        "Anabela Brandao",
        "Syeda Faryal Hussain Khurrum",
        "Joaquim Santos"
      ],
      "abstract": "Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸äººç±»åœ¨æ—¥å¸¸æ¨ç†ä¸­å¦‚ä½•å…±åŒæ„å»ºè®¤è¯†è®ºé”™è¯¯(Epistemic Error)ï¼Œå°†å¤±è´¥å®šä¹‰ä¸ºç”±æ¨¡å‹ç”Ÿæˆçš„åˆç†æ€§(Plausibility)ä¸äººç±»è§£é‡Šæ€§åˆ¤æ–­å…±åŒå¡‘é€ çš„å…³ç³»å´©æºƒã€‚é€šè¿‡ä¸‰è½®å¤šæ¨¡å‹è¯„ä¼°å’Œè·¨å­¦ç§‘ä»»åŠ¡ï¼Œç ”ç©¶å‘ç°LLMçš„é”™è¯¯æ­£ä»é¢„æµ‹æ€§è½¬å‘é˜é‡Šæ€§ï¼Œè¯­è¨€æµåˆ©åº¦ã€ç»“æ„è¿è´¯æ€§å’Œè¡¨é¢åˆç†çš„å¼•ç”¨æ©ç›–äº†æ·±å±‚æ„ä¹‰çš„æ‰­æ›²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯„ä»·è€…å¾€å¾€å°†æ­£ç¡®æ€§ã€ç›¸å…³æ€§å’Œä¸€è‡´æ€§ç­‰æ ‡å‡†æ··æ·†ä¸ºåŸºäºå½¢å¼å’Œæµåˆ©åº¦çš„ç›´è§‰å¯å‘å¼åˆ¤æ–­ï¼Œåœ¨ä»»åŠ¡å¯†åº¦å¢åŠ æ—¶ä¼šäº§ç”Ÿè®¤çŸ¥æ¼‚ç§»(Cognitive Drift)ï¼Œå¯¼è‡´é”™è¯¯ä½†å½¢å¼è‰¯å¥½çš„ç­”æ¡ˆè¢«è§†ä¸ºå¯ä¿¡ã€‚ç ”ç©¶å¼ºè°ƒé”™è¯¯å¹¶éå•çº¯çš„æ¨¡å‹è¡Œä¸ºï¼Œè€Œæ˜¯ç”Ÿæˆå¼åˆç†æ€§ä¸äººç±»è§£é‡Šæ·å¾„å…±åŒä½œç”¨çš„äº§ç‰©ã€‚å› æ­¤ï¼Œè®¤è¯†AIçš„è®¤è¯†è®ºå¤±è´¥éœ€è¦å°†è¯„ä¼°é‡æ–°æ„æƒ³ä¸ºä¸€ä¸ªå…³ç³»é˜é‡Šè¿‡ç¨‹ï¼Œä»è€Œæ­ç¤ºç³»ç»Ÿæ•…éšœä¸äººç±»å¤±å‡†(Miscalibration)ä¹‹é—´æ—¥ç›Šæ¨¡ç³Šçš„ç•Œé™ã€‚è¯¥ç ”ç©¶ä¸ºå®Œå–„LLMè¯„ä¼°ä½“ç³»ã€æå‡æ•°å­—ç´ å…»ä»¥åŠè®¾è®¡å¯ä¿¡çš„äººæœºé€šä¿¡æä¾›äº†é‡è¦å¯ç¤ºã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "19 pages, 2 tables, 77 references, 6 appendices",
      "pdf_url": "https://arxiv.org/pdf/2512.16750v1",
      "published_date": "2025-12-18 16:45:29 UTC",
      "updated_date": "2025-12-18 16:45:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:57:53.480786+00:00"
    },
    {
      "arxiv_id": "2512.16743v1",
      "title": "TreeNet: A Light Weight Model for Low Bitrate Image Compression",
      "title_zh": "TreeNetï¼šä¸€ç§ç”¨äºä½æ¯”ç‰¹ç‡å›¾åƒå‹ç¼©çš„è½»é‡çº§æ¨¡å‹",
      "authors": [
        "Mahadev Prasad Panda",
        "Purnachandra Rao Makkena",
        "Srivatsa Prativadibhayankaram",
        "Siegfried FÃ¶ÃŸel",
        "AndrÃ© Kaup"
      ],
      "abstract": "Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TreeNetï¼Œä¸€ç§æ—¨åœ¨é™ä½å­¦ä¹ å‹å›¾åƒå‹ç¼©æŠ€æœ¯è®¡ç®—å¤æ‚åº¦çš„è½»é‡çº§æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„äºŒå‰æ ‘ç»“æ„(binary tree-structured)ç¼–è§£ç æ¶æ„ï¼Œä»¥å®ç°é«˜æ•ˆçš„ç‰¹å¾è¡¨ç¤ºä¸å›¾åƒé‡å»ºã€‚TreeNet å¼•å…¥äº†æ³¨æ„åŠ›ç‰¹å¾èåˆ(attentional feature fusion)æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆæ¥è‡ªå¤šä¸ªåˆ†æ”¯çš„ç‰¹å¾ä¿¡æ¯ã€‚ç ”ç©¶äººå‘˜åœ¨ä¸‰ä¸ªä¸»æµåŸºå‡†æ•°æ®é›†ä¸Šå¯¹è¯¥æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸æœ€æ–°çš„å­¦ä¹ å‹å‹ç¼©æ ‡å‡† JPEG AI ç­‰æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä½æ¯”ç‰¹ç‡(low bitrates)æ¡ä»¶ä¸‹ï¼ŒTreeNet çš„ BD-rate ç›¸æ¯” JPEG AI å¹³å‡æå‡äº† 4.83%ã€‚ä¸æ­¤åŒæ—¶ï¼Œè¯¥æ¨¡å‹çš„å¤æ‚åº¦å¤§å¹…é™ä½äº† 87.82%ï¼Œæ˜¾è‘—æé«˜äº†è¿ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¹¿æ³›çš„æ¶ˆèå®éªŒ(ablation studies)ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†ä¸åŒæ½œå˜é‡è¡¨ç¤º(latent representations)å¯¹é‡å»ºè´¨é‡çš„å½±å“ï¼Œä¸ºæ¨¡å‹çš„é«˜æ•ˆæ€§æä¾›äº†æ·±å…¥è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16743v1",
      "published_date": "2025-12-18 16:40:06 UTC",
      "updated_date": "2025-12-18 16:40:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:57:46.871871+00:00"
    },
    {
      "arxiv_id": "2512.16739v1",
      "title": "AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach",
      "title_zh": "AIé©±åŠ¨çš„ç™Œç—›å‘ä½œé¢„æµ‹ï¼šä¸€ç§æ··åˆå†³ç­–æ”¯æŒæ–¹æ³•",
      "authors": [
        "Yipeng Zhuang",
        "Yifeng Guo",
        "Yuewen Li",
        "Yuheng Wu",
        "Philip Leung-Ho Yu",
        "Tingting Song",
        "Zhiyong Wang",
        "Kunzhong Zhou",
        "Weifang Wang",
        "Li Zhuang"
      ],
      "abstract": "Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‚ºç™Œæ‚£è€…é¢‘ç¹å‡ºç°çš„çªç ´æ€§ç–¼ç—›(breakthrough pain)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæœºå™¨å­¦ä¹ (machine learning)ä¸å¤§è¯­è¨€æ¨¡å‹(large language model)çš„æ··åˆå†³ç­–æ”¯æŒæµæ°´çº¿ï¼Œæ—¨åœ¨åˆ©ç”¨ç»“æ„åŒ–å’Œéç»“æ„åŒ–çš„ç”µå­å¥åº·è®°å½•(electronic health record)æ•°æ®é¢„æµ‹ä½é™¢å48åŠ72å°æ—¶å†…çš„ç–¼ç—›å‘ä½œã€‚é€šè¿‡åˆ†æ266åä½é™¢æ‚£è€…çš„å›é¡¾æ€§é˜Ÿåˆ—ï¼Œæœºå™¨å­¦ä¹ æ¨¡å—è´Ÿè´£æ•æ‰è¯ç‰©ä½¿ç”¨çš„æ—¶é—´è¶‹åŠ¿ï¼Œè€Œå¤§è¯­è¨€æ¨¡å‹åˆ™ç”¨äºè§£ææ¨¡ç³Šçš„ç»™è¯è®°å½•å’Œä¸´åºŠç¬”è®°ç­‰æ–‡æœ¬æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨48å°æ—¶å’Œ72å°æ—¶çš„é¢„æµ‹å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°0.874å’Œ0.917ï¼Œä¸”ç”±äºå¤§è¯­è¨€æ¨¡å‹çš„å¢å¼ºï¼Œçµæ•åº¦åˆ†åˆ«æå‡äº†8.6%å’Œ10.4%ã€‚è¿™ç§æ··åˆæ–¹æ³•æ˜¾è‘—æé«˜äº†é¢„æµ‹æ¨¡å‹çš„çµæ•åº¦ä¸å¯è§£é‡Šæ€§ï¼Œä¸ºæ—©æœŸç–¼ç—›é¢„è­¦æä¾›äº†ä¸€ä¸ªä¸´åºŠå¯æ‰©å±•çš„å·¥å…·ï¼Œæœ‰åŠ©äºä¼˜åŒ–è‚¿ç˜¤æŠ¤ç†ä¸­çš„èµ„æºé…ç½®ä¸æ²»ç–—ç²¾å‡†åº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16739v1",
      "published_date": "2025-12-18 16:37:29 UTC",
      "updated_date": "2025-12-18 16:37:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:58:30.238963+00:00"
    },
    {
      "arxiv_id": "2512.16733v2",
      "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities",
      "title_zh": "é»‘ç›’äººå·¥æ™ºèƒ½èƒ½åŠ›æ¦‚ç‡æ¨¡å‹çš„å‘ç°ä¸å­¦ä¹ ",
      "authors": [
        "Daniel Bramblett",
        "Rushang Karia",
        "Adrian Ciotinga",
        "Ruthvick Suresh",
        "Pulkit Verma",
        "YooJung Choi",
        "Siddharth Srivastava"
      ],
      "abstract": "Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºç¡€æ¨¡å‹ç­‰é»‘ç›’äººå·¥æ™ºèƒ½(Black-box AI, BBAI)ç³»ç»Ÿåœ¨åºåˆ—å†³ç­–ä¸­çš„å®‰å…¨æ€§ä¸å¯è§£é‡Šæ€§éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨PDDLé£æ ¼è¡¨ç¤ºæ³•æ¥å­¦ä¹ å’Œå»ºæ¨¡å…¶è§„åˆ’èƒ½åŠ›çš„æ–¹æ³•ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢(Monte-Carlo tree search)èŒƒå¼ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°è®¾è®¡æµ‹è¯•ä»»åŠ¡ã€é‡‡é›†æ•°æ®å¹¶å‰ªæç¬¦å·æ¨¡å‹å‡è®¾ç©ºé—´ï¼Œå®ç°äº†å¯¹BBAIèƒ½åŠ›çš„è‡ªåŠ¨åŒ–å‘ç°ã€‚å­¦ä¹ åˆ°çš„æ¦‚ç‡æ¨¡å‹èƒ½å¤Ÿæ¸…æ™°æè¿°BBAIçš„åŠŸèƒ½ã€æ‰§è¡Œå‰ææ¡ä»¶ä»¥åŠå„ç§æ‰§è¡Œç»“æœåŠå…¶å…³è”æ¦‚ç‡ã€‚ç†è®ºè¯æ˜ç¡®ä¿äº†è¯¥æ¨¡å‹çš„å¥å…¨æ€§(Soundness)ã€å®Œå¤‡æ€§(Completeness)ä¸æ”¶æ•›æ€§(Convergence)ã€‚åœ¨å¤šç§BBAIç³»ç»Ÿä¸Šçš„å®éªŒç»“æœè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•åœ¨æè¿°æ¨¡å‹èƒ½åŠ›æ–¹é¢çš„å¹¿åº¦ã€æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¿™ä¸€æˆæœä¸ºè¯„ä¼°å’Œç›‘ç£å¤æ‚AIç³»ç»Ÿåœ¨ç°å®ä»»åŠ¡ä¸­çš„å®‰å…¨éƒ¨ç½²æä¾›äº†é‡è¦çš„å¯è§£é‡Šæ€§å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16733v2",
      "published_date": "2025-12-18 16:32:06 UTC",
      "updated_date": "2025-12-20 18:26:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:59:16.179939+00:00"
    },
    {
      "arxiv_id": "2512.16715v2",
      "title": "Towards Reproducibility in Predictive Process Mining: SPICE -- A Deep Learning Library",
      "title_zh": "è¿ˆå‘é¢„æµ‹æ€§è¿‡ç¨‹æŒ–æ˜çš„å¯å¤ç°æ€§ï¼šSPICE æ·±åº¦å­¦ä¹ åº“",
      "authors": [
        "Oliver Stritzel",
        "Nick HÃ¼hnerbein",
        "Simon Rauch",
        "Itzel Zarate",
        "Lukas Fleischmann",
        "Moike Buck",
        "Attila Lischka",
        "Christian Frey"
      ],
      "abstract": "In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢„æµ‹æ€§è¿‡ç¨‹æŒ–æ˜ (Predictive Process Mining, PPM) é¢†åŸŸä¸­ç¥ç»ç½‘ç»œæŠ€æœ¯æ™®éå­˜åœ¨çš„ç¼ºä¹å¯å¤ç°æ€§ã€é€æ˜åº¦å’Œæ˜“ç”¨æ€§ç­‰æŒ‘æˆ˜ï¼Œå¼€å‘äº†åŸºäº PyTorch çš„ Python åº“ SPICEã€‚è¯¥æ¡†æ¶é‡æ–°å®ç°äº†ä¸‰ç§æµè¡Œçš„æ·±åº¦å­¦ä¹ åŸºå‡†æ–¹æ³•ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå…·å¤‡ä¸¥æ ¼å¯é…ç½®æ€§çš„é€šç”¨åŸºç¡€ç³»ç»Ÿï¼Œæ—¨åœ¨ä¸ºç°æœ‰åŠæœªæ¥çš„å»ºæ¨¡æ–¹æ¡ˆæä¾›ç¨³å¥ä¸”å¯å¤ç°çš„æ¯”è¾ƒæ ‡å‡†ã€‚é€šè¿‡åœ¨ 11 ä¸ªæ•°æ®é›†ä¸Šå¯¹æ¯”åŸå§‹æŠ¥å‘ŠæŒ‡æ ‡ä¸å…¬å¹³æŒ‡æ ‡ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº† SPICE åœ¨æ¶ˆé™¤ä¸åŒå®ç°æ–¹æ¡ˆé—´çš„å·®å¼‚ä»¥åŠå¢å¼ºå®éªŒé€æ˜åº¦æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚è¯¥æˆæœæœ‰æ•ˆè§£å†³äº†ä¸šåŠ¡è¿‡ç¨‹ç›‘æ§å’Œå…³é”®ç»©æ•ˆæŒ‡æ ‡ (KPIs) é¢„æµ‹ä¸­æ¨¡å‹éš¾ä»¥æ¨ªå‘å¯¹æ¯”çš„é—®é¢˜ï¼Œä¸ºæ¨åŠ¨ PPM é¢†åŸŸç ”ç©¶çš„è§„èŒƒåŒ–æä¾›äº†é‡è¦çš„åŸºç¡€è®¾æ–½æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16715v2",
      "published_date": "2025-12-18 16:18:06 UTC",
      "updated_date": "2025-12-19 13:06:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:59:14.785973+00:00"
    },
    {
      "arxiv_id": "2512.16707v2",
      "title": "Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems",
      "title_zh": "åŒé‡è®¡ç®—è§†ç•Œï¼šæ™ºèƒ½ç³»ç»Ÿä¸­çš„ä¸å®Œå¤‡æ€§ä¸ä¸å¯é¢„æµ‹æ€§",
      "authors": [
        "Abhisek Ganguly"
      ],
      "abstract": "We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the latter bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot verify its own maximal prediction horizon universally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems. The construction presented here constitutes one representative instance of a broader logical class of such limitations.",
      "tldr_zh": "è¯¥ç ”ç©¶å½¢å¼åŒ–äº†é™åˆ¶ç®—æ³•æ™ºèƒ½ï¼ˆalgorithmic intelligenceï¼‰çš„ä¸¤ä¸ªç‹¬ç«‹è®¡ç®—å±€é™ï¼šå½¢å¼ä¸å®Œå¤‡æ€§ï¼ˆformal incompletenessï¼‰å’ŒåŠ¨åŠ›å­¦ä¸å¯é¢„æµ‹æ€§ï¼ˆdynamical unpredictabilityï¼‰ã€‚å‰è€…é™åˆ¶äº†ç›¸å®¹æ¨ç†ç³»ç»Ÿçš„æ¼”ç»èƒ½åŠ›ï¼Œè€Œåè€…åˆ™ç•Œå®šäº†æœ‰é™ç²¾åº¦ä¸‹çš„é•¿æœŸé¢„æµ‹è¾¹ç•Œã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ä¸¤ä¸ªæç«¯å› ç´ å…±åŒå¯¹æ™ºèƒ½ä½“æ¨ç†è‡ªèº«é¢„æµ‹èƒ½åŠ›æ„æˆäº†ç»“æ„æ€§çº¦æŸã€‚å…·ä½“è€Œè¨€ï¼Œç®—æ³•æ™ºèƒ½ä½“æ— æ³•æ™®ééªŒè¯å…¶è‡ªèº«çš„æå¤§é¢„æµ‹æ°´å¹³ï¼ˆmaximal prediction horizonï¼‰ã€‚è¿™ä¸€è§†è§’é˜æ˜äº†æ™ºèƒ½ç³»ç»Ÿä¸­æ¨ç†ã€é¢„æµ‹ä¸è‡ªæˆ‘åˆ†æä¹‹é—´çš„å›ºæœ‰æƒè¡¡ã€‚è¯¥æ„å»ºå±•ç¤ºäº†é€»è¾‘ä¸Šæ­¤ç±»å±€é™æ€§æ›´å¹¿æ³›ç±»åˆ«ä¸­çš„ä¸€ä¸ªä»£è¡¨æ€§å®ä¾‹ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "8 Pages, 0 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16707v2",
      "published_date": "2025-12-18 16:12:04 UTC",
      "updated_date": "2025-12-21 19:02:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:59:13.896330+00:00"
    },
    {
      "arxiv_id": "2512.16701v1",
      "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences",
      "title_zh": "æ•™è‚²ä¸­çš„èµ›åšäººæ–‡ä¸»ä¹‰ï¼šä¾æ‰˜äººå·¥æ™ºèƒ½ä¸å­¦ä¹ ç§‘å­¦é‡å¡‘ä¸»ä½“æ€§",
      "authors": [
        "Giovanni Adorni"
      ],
      "abstract": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.",
      "tldr_zh": "GenAI æ­£åœ¨é‡å¡‘æ•™è‚²é¢†åŸŸçš„çŸ¥è¯†ç”Ÿäº§ä¸éªŒè¯æ–¹å¼ï¼Œä½†ä¹Ÿå¼•å‘äº†å…³äºè®¤è¯†è‡ªåŠ¨åŒ– (epistemic automation)ã€è®¤çŸ¥å‡è´Ÿ (cognitive offloading) å’Œæ•™å¸ˆå»ä¸“ä¸šåŒ–çš„æ·±åˆ»æ‹…å¿§ã€‚æœ¬æ–‡æå‡ºäº†æ•™è‚²ä¸­çš„èµ›åšäººæ–‡ä¸»ä¹‰ (Cyber Humanism in Education) æ¡†æ¶ï¼Œæ—¨åœ¨ AI é©±åŠ¨çš„æ™¯è§‚ä¸­é‡æ–°å¤ºå›äººç±»çš„ä¸»ä½“æ€§ (human agency)ã€‚è¯¥ç ”ç©¶å°† AI èµ‹èƒ½çš„å­¦ä¹ ç¯å¢ƒè§†ä¸ºç”±äººæœºå…±åˆ›çš„ç¤¾ä¼šæŠ€æœ¯åŸºç¡€è®¾æ–½ (socio-technical infrastructures)ï¼Œå¹¶å°†å¸ˆç”Ÿå®šä½ä¸ºå…·æœ‰å¡‘é€ èƒ½åŠ›çš„è®¤è¯†ä»£ç†äºº (epistemic agents) å’Œç®—æ³•å…¬æ°‘ (algorithmic citizens)ã€‚æ–‡ç« ç¡®ç«‹äº†èµ›åšäººæ–‡ä¸»ä¹‰è®¾è®¡çš„ä¸‰ä¸ªæ”¯æŸ±ï¼Œå³åæ€èƒ½åŠ› (reflexive competence)ã€ç®—æ³•å…¬æ°‘æ„è¯† (algorithmic citizenship) å’Œå¯¹è¯è®¾è®¡ (dialogic design)ã€‚é€šè¿‡åŸºäºæç¤ºè¯çš„å­¦ä¹  (prompt-based learning) å’Œå¯¹è¯å¼ AI æ•™è‚²è€… (Conversational AI Educator) è®¤è¯ç­‰é«˜ç­‰æ•™è‚²æ¡ˆä¾‹ï¼Œç ”ç©¶å±•ç¤ºäº†è¿™äº›å®è·µå¦‚ä½•å¢å¼ºè®¤è¯†ä¸»ä½“æ€§ (epistemic agency)ã€‚æœ€åï¼Œç ”ç©¶æ¢è®¨äº†åœ¨å·¥ä½œè´Ÿæ‹…ã€å…¬å¹³æ€§å’Œæ²»ç†æ–¹é¢çš„ç°å®æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ä»¥äººä¸ºæœ¬çš„ AI æ•™è‚²å‘å±•æä¾›äº†é‡è¦å¯ç¤ºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 16 references, Key Note preented at the \"WAILS 2025 - The 2nd. Workshop on Artificial Intelligence with and for Learning Sciences\", Cagliary, Italy, 10-12 December 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.16701v1",
      "published_date": "2025-12-18 16:06:04 UTC",
      "updated_date": "2025-12-18 16:06:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:59:21.115297+00:00"
    },
    {
      "arxiv_id": "2512.16698v1",
      "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning",
      "title_zh": "å¤šæ™ºèƒ½ä½“ä¼˜äºå•æ™ºèƒ½ä½“å—ï¼Ÿé¢å‘å›¾ç¤ºå‡ ä½•é—®é¢˜æ±‚è§£ä¸æ¨ç†çš„æ™ºèƒ½ä½“æ¡†æ¶è¯„ä¼°",
      "authors": [
        "Mahbub E Sobhani",
        "Md. Faiyaz Abdullah Sayeedi",
        "Mohammad Nehad Alam",
        "Proma Hossain Progga",
        "Swakkhar Shatabda"
      ],
      "abstract": "Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹æ¯”Geometry3Kã€MathVerseã€OlympiadBenchå’ŒWe-Mathå››ä¸ªè§†è§‰æ•°å­¦åŸºå‡†æµ‹è¯•ï¼Œç³»ç»Ÿè¯„ä¼°äº†åœ¨å‡ ä½•é—®é¢˜è§£å†³ä¸æ¨ç†(Diagram-grounded geometry problem solving and reasoning)ä»»åŠ¡ä¸­å¤šæ™ºèƒ½ä½“(Multi-agent)æ¡†æ¶ç›¸å¯¹äºå•æ™ºèƒ½ä½“(Single-agent)çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šæ™ºèƒ½ä½“è®¾è®¡èƒ½æ˜¾è‘—æå‡Qwen-2.5-VLç­‰å¼€æºæ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨Geometry3KåŸºå‡†æµ‹è¯•ä¸­æœ€é«˜æå‡äº†6.8ä¸ªç™¾åˆ†ç‚¹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé—­æºæ¨¡å‹Gemini-2.0-Flashåœ¨ä¼ ç»ŸåŸºå‡†ä¸Šé€šå¸¸ä»¥å•æ™ºèƒ½ä½“æ¨¡å¼è¡¨ç°æ›´ä½³ï¼Œä»…åœ¨è¾ƒæ–°çš„We-Mathæ•°æ®é›†ä¸Šé€šè¿‡å¤šæ™ºèƒ½ä½“åä½œè·å¾—äº†å°å¹…å¢é•¿ã€‚ç ”ç©¶ç»“è®ºæŒ‡å‡ºï¼Œè™½ç„¶å¤šæ™ºèƒ½ä½“æµæ°´çº¿(Pipelines)èƒ½æœ‰æ•ˆå¢å¼ºå¼€æºæ¨¡å‹åŠååŠ©å¼ºåŠ›é—­æºç³»ç»Ÿå¤„ç†æ–°é¢–ä»»åŠ¡ï¼Œä½†æ™ºèƒ½ä½“åˆ†è§£(Agentic decomposition)å¹¶éæ™®éçš„æœ€ä¼˜æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the ARR October 2025 cycle",
      "pdf_url": "https://arxiv.org/pdf/2512.16698v1",
      "published_date": "2025-12-18 16:00:47 UTC",
      "updated_date": "2025-12-18 16:00:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:59:15.057795+00:00"
    },
    {
      "arxiv_id": "2512.16694v1",
      "title": "Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm",
      "title_zh": "åŸºäº Apriori ç®—æ³•çš„åœ£è®­æ–‡æœ¬æ— ç›‘ç£ä¸»é¢˜èšç±»",
      "authors": [
        "Wisnu Uriawan",
        "Achmad Ajie Priyajie",
        "Angga Gustian",
        "Fikri Nur Hidayat",
        "Sendi Ahmad Rafiudin",
        "Muhamad Fikri Zaelani"
      ],
      "abstract": "This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼Šæ–¯å…°æ–‡æœ¬æ•°å­—åŒ–èƒŒæ™¯ä¸‹ Hadith æ–‡æœ¬è‡ªåŠ¨åŒ–ä¸»é¢˜åˆ†ç»„çš„ç´§è¿«éœ€æ±‚ï¼Œæ¢è®¨äº†æ— ç›‘ç£å­¦ä¹ æ–¹æ³•çš„åº”ç”¨ã€‚ç ”ç©¶é‡‡ç”¨äº†å¸ƒå“ˆé‡Œåœ£è®­ (Bukhari) çš„å°å°¼è¯­è¯‘æœ¬ä½œä¸ºæ•°æ®é›†ï¼Œå¹¶æ‰§è¡Œäº†åŒ…æ‹¬ Case foldingã€åˆ†è¯ (Tokenization)ã€å»åœç”¨è¯ (Stopword removal) å’Œè¯å¹²æå– (Stemming) åœ¨å†…çš„é¢„å¤„ç†æ­¥éª¤ã€‚éšåï¼Œç ”ç©¶åˆ©ç”¨ Apriori ç®—æ³•è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜ (Association rule mining)ï¼Œé€šè¿‡æ”¯æŒåº¦ (Support)ã€ç½®ä¿¡åº¦ (Confidence) å’Œæå‡åº¦ (Lift) ç­‰å‚æ•°è¿›è¡Œæ·±å…¥åˆ†æã€‚å®éªŒç»“æœæ­ç¤ºäº†å…·æœ‰å®é™…æ„ä¹‰çš„å…³è”æ¨¡å¼ï¼Œä¾‹å¦‚ Rakaat-Prayerã€Verse-Revelation ä»¥åŠ Hadith-Story ä¹‹é—´çš„è¯­ä¹‰è”ç³»ï¼Œå‡†ç¡®æè¿°äº†ç¤¼æ‹œã€å¯ç¤ºå’Œåœ£è®­å™äº‹ç­‰ä¸»é¢˜ã€‚è¿™äº›å‘ç°è¯æ˜äº† Apriori ç®—æ³•èƒ½å¤Ÿè‡ªåŠ¨æŒ–æ˜æ–‡æœ¬ä¸­æ½œåœ¨çš„è¯­ä¹‰å…³ç³»ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æ— ç›‘ç£ä¸»é¢˜èšç±»ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ•°å­—ä¼Šæ–¯å…°ç ”ç©¶æä¾›äº†æŠ€æœ¯æ”¯æ’‘ï¼Œä¹Ÿä¸ºåŸºäºæŠ€æœ¯çš„å­¦ä¹ ç³»ç»Ÿå¼€å‘åšå‡ºäº†ç§¯æè´¡çŒ®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16694v1",
      "published_date": "2025-12-18 15:59:46 UTC",
      "updated_date": "2025-12-18 15:59:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:59:39.604895+00:00"
    },
    {
      "arxiv_id": "2512.16685v1",
      "title": "Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray",
      "title_zh": "3D-MRIä¸2D-X-Rayä¸­çš„å°‘æ ·æœ¬æŒ‡çº¹å—è¯•è€…é‡è¯†åˆ«",
      "authors": [
        "GonÃ§alo Gaspar Alves",
        "Shekoufeh Gorgi Zadeh",
        "Andreas Husch",
        "Ben Bausch"
      ],
      "abstract": "Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†Subject FingerprintingæŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³åˆå¹¶å¼€æºæ•°æ®é›†æ—¶å› åŒä¸€å—è¯•è€…å‡ºç°åœ¨å¤šä¸ªé›†åˆä¸­è€Œå¯¼è‡´çš„Data Leakageé—®é¢˜ï¼Œä»è€Œé¿å…æ¨¡å‹æ€§èƒ½è¢«è™šé«˜è¯„ä¼°ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å—è¯•è€…çš„æ‰€æœ‰å›¾åƒæ˜ å°„åˆ°Latent Spaceä¸­çš„ç‰¹å®šåŒºåŸŸï¼Œåˆ©ç”¨Similarity Matchingå®ç°é«˜æ•ˆçš„Subject Re-Identificationã€‚ç ”ç©¶äººå‘˜é‡‡ç”¨ç»è¿‡Triplet Margin Lossè®­ç»ƒçš„ResNet-50æ¨¡å‹ï¼Œåœ¨3D MRIå’Œ2D X-rayæ•°æ®é›†ä¸Šé’ˆå¯¹Few-Shotåœºæ™¯è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚å®éªŒæ¶µç›–äº†ä»æ ‡å‡†çš„20-way 1-shotåˆ°æŒ‘æˆ˜æ€§çš„1000-way 1-shotç­‰å¤šç§è®¾å®šã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ChestXray-14æ•°æ®é›†ä¸Šçš„Mean-Recall-@-Kè¾¾åˆ°99.10%ï¼ˆ20-way 1-shotï¼‰ï¼Œåœ¨BraTS-2021ä¸Šè¾¾åˆ°99.20%ï¼ˆ20-way 1-shotï¼‰ã€‚å³ä¾¿åœ¨å¤æ‚çš„500-way 5-shotæˆ–100-way 3-shotä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹ä¾ç„¶ä¿æŒäº†æé«˜çš„è¯†åˆ«å‡†ç¡®ç‡ã€‚è¿™ä¸€æˆæœä¸ºç¡®ä¿å¤šæºåŒ»ç–—æ•°æ®é›†é›†æˆæ—¶çš„å®éªŒä¸¥è°¨æ€§æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16685v1",
      "published_date": "2025-12-18 15:50:54 UTC",
      "updated_date": "2025-12-18 15:50:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:59:41.932032+00:00"
    },
    {
      "arxiv_id": "2601.06049v1",
      "title": "The Violation State: Safety State Persistence in a Multimodal Language Model Interface",
      "title_zh": "è¿è§„çŠ¶æ€ï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç•Œé¢ä¸­çš„å®‰å…¨çŠ¶æ€æŒä¹…æ€§",
      "authors": [
        "Bentley DeVilling"
      ],
      "abstract": "Multimodal AI systems integrate text generation, image generation, and other capabilities within a single conversational interface. These systems employ safety mechanisms to prevent disallowed actions, including the removal of watermarks from copyrighted images. While single-turn refusals are expected, the interaction between safety filters and conversation-level state is not well understood. This study documents a reproducible behavioral effect in the ChatGPT (GPT-5.1) web interface. Manual execution was chosen to capture the exact user-facing safety behavior of the production system, rather than isolated API components. When a conversation begins with an uploaded copyrighted image and a request to remove a watermark, which the model correctly refuses, subsequent prompts to generate unrelated, benign images are refused for the remainder of the session. Importantly, text-only requests (e.g., generating a Python function) continue to succeed. Across 40 manually run sessions (30 contaminated and 10 controls), contaminated threads showed 116/120 image-generation refusals (96.67%), while control threads showed 0/40 refusals (Fisher's exact p < 0.0001). All sessions used an identical fixed prompt order, ensuring sequence uniformity across conditions. We describe this as safety-state persistence: a form of conversational over-generalization in which a copyright refusal influences subsequent, unrelated image-generation behavior. We present these findings as behavioral observations, not architectural claims. We discuss possible explanations, methodological limitations (single model, single interface), and implications for multimodal reliability, user experience, and the design of session-level safety systems. These results motivate further examination of session-level safety interactions in multimodal AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€AIç³»ç»Ÿ(Multimodal AI systems)ä¸­å®‰å…¨è¿‡æ»¤å™¨ä¸å¯¹è¯çŠ¶æ€ä¹‹é—´çš„äº¤äº’ï¼Œè®°å½•äº†ChatGPT (GPT-5.1)ç•Œé¢ä¸­ä¸€ç§å¯é‡å¤çš„â€œå®‰å…¨çŠ¶æ€æŒç»­æ€§â€(Safety State Persistence)ç°è±¡ã€‚å®éªŒé€šè¿‡40ä¸ªæ‰‹åŠ¨æ“ä½œçš„ä¼šè¯å‘ç°ï¼Œå½“æ¨¡å‹æ­£ç¡®æ‹’ç»äº†åˆå§‹çš„ç‰ˆæƒå›¾åƒå»æ°´å°è¯·æ±‚åï¼Œè¯¥ä¼šè¯ä¸­åç»­ç”Ÿæˆçš„è‰¯æ€§å›¾åƒè¯·æ±‚åœ¨96.67%çš„æƒ…å†µä¸‹ä¹Ÿä¼šè¢«æ‹’ç»ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œçº¯æ–‡æœ¬è¯·æ±‚ï¼ˆå¦‚ç”ŸæˆPythonä»£ç ï¼‰åœ¨å—æ±¡æŸ“çš„ä¼šè¯ä¸­ä»èƒ½æˆåŠŸæ‰§è¡Œï¼Œè¡¨æ˜è¿™ç§å½±å“å…·æœ‰ç‰¹å®šæ¨¡æ€çš„ç‰¹å¾ã€‚ç ”ç©¶è€…å°†æ­¤å®šä¹‰ä¸ºä¸€ç§å¯¹è¯å±‚é¢çš„è¿‡åº¦æ³›åŒ–(Conversational Over-generalization)ï¼Œå³é’ˆå¯¹ç‰ˆæƒçš„æ‹’ç»ä¼šé”™è¯¯åœ°å½±å“åç»­æ— å…³çš„å›¾åƒç”Ÿæˆè¡Œä¸ºã€‚è¯¥å‘ç°æ­ç¤ºäº†ç”Ÿäº§çº§å¤šæ¨¡æ€ç•Œé¢åœ¨ä¼šè¯çº§å®‰å…¨é€»è¾‘ä¸Šçš„æ½œåœ¨ç¼ºé™·ï¼Œå¹¶è®¨è®ºäº†å…¶å¯¹ç³»ç»Ÿå¯é æ€§ã€ç”¨æˆ·ä½“éªŒåŠå®‰å…¨æœºåˆ¶è®¾è®¡çš„æ·±è¿œå½±å“ã€‚è¿™é¡¹å·¥ä½œä¸ºè¿›ä¸€æ­¥å®¡è§†å¤šæ¨¡æ€AIç³»ç»Ÿä¸­çš„é•¿ç¨‹å®‰å…¨çŠ¶æ€äº¤äº’æä¾›äº†é‡è¦çš„å®è¯è§‚å¯Ÿã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "19 pages, 1 figure, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2601.06049v1",
      "published_date": "2025-12-18 15:50:32 UTC",
      "updated_date": "2025-12-18 15:50:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:59:38.020377+00:00"
    },
    {
      "arxiv_id": "2512.16661v2",
      "title": "Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance",
      "title_zh": "é¢å‘ç ”ç©¶æ¨èä¸è¾…åŠ©çš„ Microsoft Academic Graph ä¿¡æ¯æ£€ç´¢",
      "authors": [
        "Shikshya Shiwakoti",
        "Samuel Goldsmith",
        "Ujjwal Pandit"
      ],
      "abstract": "In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§‘å­¦æ–‡çŒ®æµ·é‡å¢é•¿å¸¦æ¥çš„ç­›é€‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ç”¨äºç ”ç©¶æ¨èä¸è¾…åŠ©çš„ Microsoft Academic Graph ä¿¡æ¯æ£€ç´¢æ–¹æ¡ˆã€‚æ ¸å¿ƒè´¡çŒ®æ˜¯å¼€å‘äº†ä¸€ç§åä¸º Attention-Based Subgraph Retriever çš„ GNN-as-retriever æ¨¡å‹ï¼Œæ—¨åœ¨æå‡å¤§è§„æ¨¡ä¿¡æ¯æ•°æ®åº“çš„æœç´¢æ•ˆèƒ½ã€‚è¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°åˆ©ç”¨ attention-based pruning æŠ€æœ¯ï¼Œä»å¤æ‚çš„å›¾ç»“æ„ä¸­æå–å‡ºç²¾ç‚¼çš„å­å›¾ã€‚æå–å‡ºçš„å­å›¾éšåè¢«ä¼ é€’è‡³ Large Language Model (LLM)ï¼Œä»¥åˆ©ç”¨å…¶å¼ºå¤§çš„è¯­ä¹‰ç†è§£èƒ½åŠ›è¿›è¡Œé«˜çº§çŸ¥è¯†æ¨ç†ã€‚é€šè¿‡ç»“åˆ Graph Neural Networks (GNNs) çš„ç»“æ„å¤„ç†ä¼˜åŠ¿ä¸ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—å¢å¼ºäº†åœ¨æµ·é‡å­¦æœ¯æ•°æ®ä¸­è¿›è¡Œä¿¡æ¯è¿‡æ»¤å’Œæ¨èçš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "5 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16661v2",
      "published_date": "2025-12-18 15:29:18 UTC",
      "updated_date": "2025-12-21 15:17:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:59:39.291168+00:00"
    },
    {
      "arxiv_id": "2512.20660v1",
      "title": "Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering",
      "title_zh": "åº”å¯¹éšæœºæ€§ï¼šè½¯ä»¶å·¥ç¨‹ç¥ç»ç¬¦å·ç³»ç»Ÿçš„å­¦ä¹ åŸºç¡€",
      "authors": [
        "Matthew Thompson"
      ],
      "abstract": "Current approaches to AI coding agents appear to blur the lines between the Large Language Model (LLM) and the agent itself, asking the LLM to make decisions best left to deterministic processes. This leads to systems prone to stochastic failures such as gaming unit tests or hallucinating syntax. Drawing on established software engineering practices that provide deterministic frameworks for managing unpredictable processes, this paper proposes setting the control boundary such that the LLM is treated as a component of the environment environment -- preserving its creative stochasticity -- rather than the decision-making agent.\n  A \\textbf{Dual-State Architecture} is formalized, separating workflow state (deterministic control flow) from environment state (stochastic generation). \\textbf{Atomic Action Pairs} couple generation with verification as indivisible transactions, where \\textbf{Guard Functions} act as sensing actions that project probabilistic outputs onto observable workflow state. The framework is validated on three code generation tasks across 13 LLMs (1.3B--15B parameters). For qualified instruction-following models, task success rates improved by up to 66 percentage points at 1.2--2.1$\\times$ baseline computational cost. The results suggest that architectural constraints can substitute for parameter scale in achieving reliable code generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰AIç¼–ç¨‹æ™ºèƒ½ä½“ä¸­Large Language Model (LLM)ä¸æ™ºèƒ½ä½“ç•Œé™æ¨¡ç³Šå¯¼è‡´ç³»ç»Ÿæ˜“å‡ºç°å¹»è§‰å’Œéšæœºæ€§å¤±è´¥(stochastic failures)çš„é—®é¢˜ï¼Œæå‡ºå°†LLMè§†ä¸ºç¯å¢ƒç»„ä»¶è€Œéå†³ç­–æ ¸å¿ƒï¼Œä»¥ä¿ç•™å…¶åˆ›é€ æ€§éšæœºæ€§ã€‚ç ”ç©¶æ­£å¼æ„å»ºäº†Dual-State Architectureï¼Œå°†ç¡®å®šæ€§çš„å·¥ä½œæµçŠ¶æ€(workflow state)ä¸éšæœºçš„ç”Ÿæˆç¯å¢ƒçŠ¶æ€(environment state)è¿›è¡Œåˆ†ç¦»ã€‚é€šè¿‡å¼•å…¥Atomic Action Pairså°†ç”Ÿæˆä¸éªŒè¯è€¦åˆä¸ºä¸å¯åˆ†å‰²çš„äº‹åŠ¡ï¼Œå¹¶åˆ©ç”¨Guard Functionså°†æ¦‚ç‡è¾“å‡ºæŠ•å°„ä¸ºå¯è§‚æµ‹çš„ç³»ç»ŸçŠ¶æ€ã€‚åœ¨13ä¸ªLLMsä¸Šçš„å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶ä½¿ä»»åŠ¡æˆåŠŸç‡æœ€é«˜æå‡äº†66ä¸ªç™¾åˆ†ç‚¹ï¼Œè€Œè®¡ç®—æˆæœ¬ä»…å¢åŠ 1.2è‡³2.1å€ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆç†çš„æ¶æ„çº¦æŸ(architectural constraints)åœ¨å®ç°å¯é ä»£ç ç”Ÿæˆæ–¹é¢å¯ä»¥æœ‰æ•ˆæ›¿ä»£å•çº¯çš„å‚æ•°è§„æ¨¡(parameter scale)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "55 pages, 3 figures, 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.20660v1",
      "published_date": "2025-12-18 15:28:21 UTC",
      "updated_date": "2025-12-18 15:28:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:59:42.063235+00:00"
    },
    {
      "arxiv_id": "2512.16658v1",
      "title": "Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking",
      "title_zh": "åŸºäºæ··æ²Œç™½ç›’æ°´å°çš„æ·±åº¦ç¥ç»ç½‘ç»œçŸ¥è¯†äº§æƒä¿æŠ¤",
      "authors": [
        "Sangeeth B",
        "Serena Nicolazzo",
        "Deepa K.",
        "Vinod P"
      ],
      "abstract": "The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºChaos-Basedçš„White-Box Watermarkingæ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŠ¤Deep Neural Networks (DNN)çš„Intellectual Propertyå¹¶é˜²æ­¢æ¨¡å‹è¯¯ç”¨ã€‚è¯¥æ–¹æ³•åˆ©ç”¨Logistic mapç”Ÿæˆæ··æ²Œåºåˆ—ï¼Œå¹¶å°†å…¶åµŒå…¥åˆ°é€‰å®šä¸­é—´å±‚çš„æƒé‡ä¸­ï¼Œæ—¢ä¸éœ€è¦æ”¹å˜æ¨¡å‹ç»“æ„ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´é¢„æµ‹æ€§èƒ½ä¸‹é™ã€‚éªŒè¯è¿‡ç¨‹é‡‡ç”¨äº†Genetic Algorithmï¼Œé€šè¿‡ä¼˜åŒ–æå–åºåˆ—ä¸å†ç”Ÿåºåˆ—ä¹‹é—´çš„ç›¸ä¼¼åº¦æ¥ç²¾å‡†æ¢å¤åŸå§‹æ··æ²Œå‚æ•°ã€‚åœ¨MNISTå’ŒCIFAR-10æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒåµŒå…¥çš„æ°´å°åœ¨Fine-tuningåä¾ç„¶å¯æ£€æµ‹ï¼Œä¸”å¯¹æ¨¡å‹å‡†ç¡®æ€§çš„å½±å“å¾®ä¹å…¶å¾®ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡æƒé‡å¯†åº¦å›¾åˆ†æå’ŒåŸºäºActivationçš„åˆ†ç±»å™¨ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†åŒºåˆ†åŸå§‹æ¨¡å‹ä¸ç¯¡æ”¹æ¨¡å‹çš„èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ¡ˆä¸ºç°å®ä¸–ç•Œä¸­å…³é”®çš„IPä¿æŠ¤éœ€æ±‚æä¾›äº†ä¸€ç§çµæ´»ä¸”å¯æ‰©å±•çš„White-Boxæ‰€æœ‰æƒéªŒè¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16658v1",
      "published_date": "2025-12-18 15:26:50 UTC",
      "updated_date": "2025-12-18 15:26:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:00:06.411135+00:00"
    },
    {
      "arxiv_id": "2512.16656v1",
      "title": "Comprehensive AI Literacy: The Case for Centering Human Agency",
      "title_zh": "å…¨é¢äººå·¥æ™ºèƒ½ç´ å…»ï¼šè®ºäººç±»èƒ½åŠ¨æ€§çš„æ ¸å¿ƒåœ°ä½",
      "authors": [
        "Sri Yash Tadimalla",
        "Justin Cary",
        "Gordon Hull",
        "Jordan Register",
        "Daniel Maxwell",
        "David Pugalee",
        "Tina Heafner"
      ],
      "abstract": "The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ AI æŠ€æœ¯å¿«é€Ÿæ™®åŠè€Œæ•™è‚²æ¡†æ¶æ»åçš„ç°çŠ¶ï¼ŒæŒ‡å‡ºç›®å‰æ•™è‚²é¢†åŸŸå­˜åœ¨ä¸€ç§ä»…å…³æ³¨åŠŸèƒ½æ€§æ“ä½œæŠ€èƒ½ï¼Œè€Œå¿½è§†æ‰¹åˆ¤æ€§åŠä¼¦ç†æ¨ç†çš„ AI literacy gapã€‚è®ºæ–‡æå€¡å‘ä»¥äººç±»è‡ªä¸»æ€§(Human Agency)ä¸ºæ ¸å¿ƒçš„å…¨é¢ AI è¯†è¯»(comprehensive AI literacy)è¿›è¡Œç³»ç»Ÿæ€§è½¬å˜ï¼Œå¼ºè°ƒæœ‰æ„è¯†ã€æ‰¹åˆ¤æ€§ä¸”è´Ÿè´£ä»»çš„é€‰æ‹©èƒ½åŠ›ã€‚è¿™ç§æ ¸å¿ƒåŸåˆ™é€‚ç”¨äºæ•™è‚²ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ‰€æœ‰åˆ©ç›Šç›¸å…³è€…ï¼ŒåŒ…æ‹¬å­¦ç”Ÿå¯¹ AI çš„è´¨ç–‘ä¸é€‰æ‹©æ€§ä½¿ç”¨ï¼Œä»¥åŠæ•™å¸ˆåœ¨è®¾è®¡å­¦ä¹ ä½“éªŒæ—¶ä¿æŒæ•™å­¦æ§åˆ¶æƒè€Œéè®©æ¸¡ç»™å·¥å…·çš„è‡ªä¸»æ€§ã€‚çœŸæ­£çš„ AI è¯†è¯»åº”å°†æŠ€æœ¯è§†ä¸ºä¸€ç§é€‰æ‹©è€Œéå¿…ç„¶ï¼Œå¹¶è¦æ±‚å¯¹æ‰¹åˆ¤æ€§æ€ç»´(critical thinking)å’Œè®¤è¯†è®º(epistemology)æœ‰æ·±åˆ»ç†è§£ã€‚é€šè¿‡æ–‡ä¸­æè¿°çš„ AI Literacy, Fluency å’Œ Competency æ¡†æ¶ï¼Œæ•™è‚²è€…å’Œå­¦ç”Ÿèƒ½å¤Ÿæˆä¸ºä»¥äººä¸ºä¸­å¿ƒ(human-centric)åº”ç”¨æ¨¡å¼çš„ä¸»ä½“ã€‚è¯¥ç ”ç©¶ä¸ºæ˜ç¡® AI å†³ç­–æ„å›¾åŠå…¶å¯¹å­¦æœ¯ã€èŒä¸šå’Œç¤¾ä¼šçš„å½±å“æä¾›äº†å¿…è¦è·¯å¾„ï¼Œæ—¨åœ¨åŸ¹å…»èƒ½å¤Ÿè‡ªä¸»æŒæ§ AI æŠ€æœ¯è€Œéè¢«æŠ€æœ¯é©±åŠ¨çš„æœªæ¥å…¬æ°‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "2 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.16656v1",
      "published_date": "2025-12-18 15:25:38 UTC",
      "updated_date": "2025-12-18 15:25:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T17:59:59.709133+00:00"
    },
    {
      "arxiv_id": "2512.16650v1",
      "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
      "title_zh": "Prefix Probingï¼šå¤§è¯­è¨€æ¨¡å‹è½»é‡çº§æœ‰å®³å†…å®¹æ£€æµ‹",
      "authors": [
        "Jirui Yang",
        "Hengqi Guo",
        "Zhihui Lu",
        "Yi Zhao",
        "Yuansen Zhang",
        "Shijing Hu",
        "Qiang Duan",
        "Yinggui Wang",
        "Tao Wei"
      ],
      "abstract": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å®‰å…¨æ•æ„Ÿåº”ç”¨ä¸­é¢ä¸´çš„æ£€æµ‹å‡†ç¡®ç‡ã€æ¨ç†å»¶è¿Ÿå’Œéƒ¨ç½²æˆæœ¬ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œæå‡ºäº†Prefix Probingã€‚è¿™æ˜¯ä¸€ç§é»‘ç›’æœ‰å®³å†…å®¹æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡æ¯”è¾ƒâ€œåŒæ„/æ‰§è¡Œâ€ä¸â€œæ‹’ç»/å®‰å…¨â€å¼€å¤´å‰ç¼€çš„æ¡ä»¶å¯¹æ•°æ¦‚ç‡(conditional log-probabilities)æ¥ç”Ÿæˆæœ‰å®³æ€§è¯„åˆ†ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å‰ç¼€ç¼“å­˜(prefix caching)æŠ€æœ¯å°†æ£€æµ‹å¼€é”€é™ä½è‡³æ¥è¿‘é¦–ä¸ªè¯å…ƒ(first-token)çš„å»¶è¿Ÿï¼Œä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…éœ€å•æ¬¡è®¡ç®—ï¼Œæ— éœ€è°ƒç”¨é¢å¤–æ¨¡å‹æˆ–è¿›è¡Œå¤šé˜¶æ®µæ¨ç†ã€‚ç ”ç©¶è¿˜è®¾è®¡äº†ä¸€ç§é«˜æ•ˆçš„å‰ç¼€æ„å»ºç®—æ³•ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å‘ç°ä¿¡æ¯é‡ä¸°å¯Œçš„å‰ç¼€ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºæ£€æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒPrefix Probingåœ¨æ£€æµ‹æ•ˆèƒ½ä¸Šå¯ä¸ä¸»æµå¤–éƒ¨å®‰å…¨æ¨¡å‹åª²ç¾ã€‚è¯¥æ–¹æ³•åœ¨æä½è®¡ç®—æˆæœ¬ä¸‹å®ç°äº†é«˜æ•ˆæ£€æµ‹ï¼Œä¸”æ— éœ€é¢å¤–æ¨¡å‹éƒ¨ç½²ï¼Œå…·æœ‰æ˜¾è‘—çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16650v1",
      "published_date": "2025-12-18 15:22:14 UTC",
      "updated_date": "2025-12-18 15:22:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:00:07.224735+00:00"
    },
    {
      "arxiv_id": "2512.16644v1",
      "title": "Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam",
      "title_zh": "ä½œä¸ºä¼Šæ–¯å…°é—®é¢˜å’¨è¯¢åª’ä»‹çš„ä¼Šæ–¯å…°æ•™æ³•èŠå¤©æœºå™¨äººçš„å®ç°",
      "authors": [
        "Wisnu Uriawan",
        "Aria Octavian Hamza",
        "Ade Ripaldi Nuralim",
        "Adi Purnama",
        "Ahmad Juaeni Yunus",
        "Anissya Auliani Supriadi Putri"
      ],
      "abstract": "This research presents the implementation of a Sharia-compliant chatbot as an interactive medium for consulting Islamic questions, leveraging Reinforcement Learning (Q-Learning) integrated with Sentence-Transformers for semantic embedding to ensure contextual and accurate responses. Utilizing the CRISP-DM methodology, the system processes a curated Islam QA dataset of 25,000 question-answer pairs from authentic sources like the Qur'an, Hadith, and scholarly fatwas, formatted in JSON for flexibility and scalability. The chatbot prototype, developed with a Flask API backend and Flutter-based mobile frontend, achieves 87% semantic accuracy in functional testing across diverse topics including fiqh, aqidah, ibadah, and muamalah, demonstrating its potential to enhance religious literacy, digital da'wah, and access to verified Islamic knowledge in the Industry 4.0 era. While effective for closed-domain queries, limitations such as static learning and dataset dependency highlight opportunities for future enhancements like continuous adaptation and multi-turn conversation support, positioning this innovation as a bridge between traditional Islamic scholarship and modern AI-driven consultation.",
      "tldr_zh": "è¯¥ç ”ç©¶å®ç°äº†ä¸€æ¬¾ç¬¦åˆä¼Šæ–¯å…°æ•™æ³•(Sharia)çš„èŠå¤©æœºå™¨äººï¼Œæ—¨åœ¨ä½œä¸ºè§£ç­”ä¼Šæ–¯å…°ç›¸å…³é—®é¢˜çš„äº¤äº’å¼å’¨è¯¢åª’ä»‹ã€‚ç³»ç»Ÿé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ ä¸­çš„ Q-Learning ç®—æ³•ï¼Œå¹¶ç»“åˆ Sentence-Transformers è¿›è¡Œè¯­ä¹‰åµŒå…¥(Semantic Embedding)ï¼Œä»¥ç¡®ä¿å›ç­”çš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§ä¸å‡†ç¡®æ€§ã€‚ç ”ç©¶éµå¾ª CRISP-DM æ–¹æ³•è®ºï¼Œåˆ©ç”¨åŒ…å« 25,000 ä¸ªæ¥è‡ªå¤å…°ç»(Qur'an)ã€åœ£è®­(Hadith)å’Œå­¦è€…æ•™æ³•åˆ¤ä¾‹(Fatwas)çš„é—®ç­”æ•°æ®é›†è¿›è¡Œå¼€å‘ã€‚è¯¥åŸå‹ç³»ç»Ÿç”± Flask API åç«¯å’Œ Flutter ç§»åŠ¨å‰ç«¯æ„æˆï¼Œæ¶µç›–äº† Fiqhã€Aqidahã€Ibadah å’Œ Muamalah ç­‰æ ¸å¿ƒé¢†åŸŸã€‚æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œè¯¥æœºå™¨äººåœ¨å¤„ç†é—­ç¯é¢†åŸŸæŸ¥è¯¢æ—¶å®ç°äº† 87% çš„è¯­ä¹‰å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å·¥ä¸š 4.0 æ—¶ä»£æå‡å®—æ•™ç´ å…»å’Œæ•°å­—åŒ–å®£æ•™(Digital Da'wah)çš„æ½œåŠ›ã€‚å°½ç®¡ç›®å‰ä»å­˜åœ¨é™æ€å­¦ä¹ å’Œæ•°æ®é›†ä¾èµ–ç­‰å±€é™ï¼Œä½†è¯¥ç ”ç©¶æˆåŠŸä¸ºä¼ ç»Ÿä¼Šæ–¯å…°å­¦æœ¯ä¸ç°ä»£ AI å’¨è¯¢æŠ€æœ¯ä¹‹é—´æ­å»ºäº†æ²Ÿé€šæ¡¥æ¢ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16644v1",
      "published_date": "2025-12-18 15:15:46 UTC",
      "updated_date": "2025-12-18 15:15:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:00:10.857358+00:00"
    },
    {
      "arxiv_id": "2512.16626v1",
      "title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game",
      "title_zh": "Stackelberg åŸºäºäººç±»åé¦ˆçš„å­¦ä¹ ï¼šä½œä¸ºåºè´¯åšå¼ˆçš„åå¥½ä¼˜åŒ–",
      "authors": [
        "Barna PÃ¡sztor",
        "Thomas Kleine Buening",
        "Andreas Krause"
      ],
      "abstract": "We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Stackelberg Learning from Human Feedback (SLHF)ï¼Œè¿™æ˜¯ä¸€ç§å°†å¯¹é½é—®é¢˜å»ºæ¨¡ä¸ºé¡ºåºåšå¼ˆ(sequential-move game)çš„æ–°å‹åå¥½ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±å…ˆè¡Œè¡ŒåŠ¨çš„Leaderå’Œæ ¹æ®å…¶è¡ŒåŠ¨åšå‡ºæ¡ä»¶å“åº”çš„Followerç»„æˆï¼Œå°†ä¼˜åŒ–è¿‡ç¨‹åˆ†è§£ä¸ºFollowerçš„ç»†åŒ–é—®é¢˜(refinement problem)ä¸Leaderçš„å¯¹æŠ—ä¼˜åŒ–é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„Reinforcement Learning from Human Feedback (RLHF)æˆ–Nash Learning from Human Feedback (NLHF)ä¸åŒï¼ŒSLHFåˆ©ç”¨é¡ºåºåšå¼ˆçš„éå¯¹ç§°æ€§æ•æ‰æ›´å¤æ‚çš„åå¥½ç»“æ„ï¼Œå¹¶åŸç”Ÿæ”¯æŒæ¨ç†é˜¶æ®µçš„ç»†åŒ–(inference-time refinement)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSLHFåœ¨ä¸€è‡´æ€§ã€æ•°æ®æ•æ„Ÿæ€§ä»¥åŠå¤„ç†éä¼ é€’æ€§åå¥½(intransitive preferences)æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚åœ¨0.5Bè‡³8Bå‚æ•°è§„æ¨¡çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)å®éªŒä¸­ï¼ŒSLHFå±•ç°äº†å¼ºæœ‰åŠ›çš„å¯¹é½èƒ½åŠ›ï¼Œä¸”å…¶æ¨ç†ç»†åŒ–èƒ½åŠ›æ— éœ€é¢å¤–å¾®è°ƒå³å¯åœ¨ä¸åŒæ¨¡å‹æ—ä¹‹é—´å®ç°è¿ç§»ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "cs.MA",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 tables, 1 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16626v1",
      "published_date": "2025-12-18 15:03:23 UTC",
      "updated_date": "2025-12-18 15:03:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:00:20.946543+00:00"
    },
    {
      "arxiv_id": "2512.16614v1",
      "title": "Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents",
      "title_zh": "æ‹’ç»çŒœæµ‹ï¼Œåˆ†çº§ç ”åˆ¤ï¼šè¿ˆå‘å¯è§£é‡Šä¸”ç»è¿‡ä¸ç¡®å®šæ€§æ ¡å‡†çš„ AI å–è¯æ™ºèƒ½ä½“",
      "authors": [
        "Giulia Boato",
        "Andrea Montibeller",
        "Edward Delp",
        "Luisa Verdoliva",
        "Daniele Miorandi"
      ],
      "abstract": "AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨å¤šåª’ä½“å–è¯(multimedia forensics)é¢†åŸŸçš„åº”ç”¨ï¼Œé’ˆå¯¹ç°æœ‰è§£å†³æ–¹æ¡ˆçš„ç¼ºé™·ï¼Œæå‡ºäº†å¯è§£é‡Šä¸”å…·å¤‡ä¸ç¡®å®šæ€§æ ¡å‡†(uncertainty-calibrated)çš„AIå–è¯æ™ºèƒ½ä½“ã€‚è¿™äº›æ™ºèƒ½ä½“ä½œä¸ºå¯é çš„ç¼–æ’å™¨(orchestrators)ï¼Œèƒ½å¤Ÿè‡ªä¸»é€‰æ‹©å¹¶ç»„åˆå¤šç§å–è¯æ£€æµ‹å™¨(forensic detectors)ï¼Œä»¥è¯†åˆ«å†…å®¹çš„æ¥æºä¸èƒŒæ™¯ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºæä¾›æ„ŸçŸ¥ä¸ç¡®å®šæ€§(uncertainty-aware)çš„è¯„ä¼°ç»“æœï¼Œæœ‰æ•ˆåœ°è¯†åˆ«å¹¶å¤„ç†äº†å½“å‰æŠ€æœ¯è·¯å¾„ä¸­çš„æ½œåœ¨é™·é˜±ã€‚é€šè¿‡å¼•å…¥è¿™ä¸€ç»Ÿä¸€æ¡†æ¶ï¼Œç ”ç©¶æ—¨åœ¨ä¼˜åŒ–çœŸå®æ€§éªŒè¯(authenticity verification)æµç¨‹ï¼Œæå‡å–è¯åˆ¤å®šçš„å¯é æ€§ã€‚è¯¥æ–¹æ³•å¼ºè°ƒåœ¨é¢å¯¹ä¸ç¡®å®šæƒ…å†µæ—¶è¿›è¡Œå‡çº§å¤„ç†è€Œéç›²ç›®çŒœæµ‹ï¼Œä¸ºæ„å»ºæ›´é€æ˜ã€æ›´å…·å¯è§£é‡Šæ€§çš„AIå–è¯ç³»ç»Ÿæä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16614v1",
      "published_date": "2025-12-18 14:52:57 UTC",
      "updated_date": "2025-12-18 14:52:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:00:21.803260+00:00"
    },
    {
      "arxiv_id": "2512.16602v2",
      "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics",
      "title_zh": "Refusal Steeringï¼šé’ˆå¯¹æ•æ„Ÿè¯é¢˜çš„å¤§è¯­è¨€æ¨¡å‹æ‹’ç»è¡Œä¸ºç»†ç²’åº¦æ§åˆ¶",
      "authors": [
        "Iker GarcÃ­a-Ferrero",
        "David Montero",
        "Roman Orus"
      ],
      "abstract": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Refusal Steeringï¼Œä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒå³å¯åœ¨æ¨ç†é˜¶æ®µ(inference-time)å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ”¿æ²»æ•æ„Ÿè¯é¢˜ä¸Šçš„æ‹’ç»è¡Œä¸ºè¿›è¡Œç»†ç²’åº¦æ§åˆ¶çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨LLM-as-a-judgeæ›¿ä»£äº†ä¼ ç»Ÿçš„æ¨¡å¼åŒ–æ£€æµ‹ï¼Œé€šè¿‡åˆ†é…æ‹’ç»ç½®ä¿¡åº¦åˆ†æ•°å¹¶ç»“åˆå²­æ­£åˆ™åŒ–(ridge-regularized)å˜ä½“æ¥è®¡ç®—è½¬å‘å‘é‡(steering vectors)ï¼Œä»è€Œæ›´ç²¾å‡†åœ°éš”ç¦»æ‹’ç»ä¸åˆè§„ç‰¹å¾ã€‚åœ¨Qwen3-Next-80B-A3B-Thinkingç­‰æ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æŠ€æœ¯èƒ½åœ¨ä¸æŸå®³é€šç”¨æ€§èƒ½å’ŒJailbreakBenchå®‰å…¨æ€§çš„å‰æä¸‹ï¼Œæœ‰æ•ˆæ¶ˆé™¤æ”¿æ²»æ•æ„Ÿè¯é¢˜çš„æ‹’ç»è¡Œä¸ºã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨4Bè‡³80Bä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œå¹¶æ­ç¤ºäº†æ‹’ç»ä¿¡å·ä¸»è¦é›†ä¸­åœ¨Transformeræ¨¡å‹çš„æ·±å±‚ä¸”åˆ†å¸ƒäºå¤šç»´ç©ºé—´ä¸­ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ¿€æ´»è½¬å‘(activation steering)èƒ½å¤Ÿæœ‰æ•ˆå‰¥ç¦»ç‰¹å®šçš„æ”¿æ²»æ‹’ç»è¡Œä¸ºå¹¶ä¿ç•™å®‰å…¨å¯¹é½ï¼Œä¸ºå®ç°æ¨ç†é˜¶æ®µå¯æ§ä¸”é€æ˜çš„æ¨¡å‹è°ƒèŠ‚æä¾›äº†å®ç”¨è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16602v2",
      "published_date": "2025-12-18 14:43:04 UTC",
      "updated_date": "2026-01-22 13:49:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:00:23.894891+00:00"
    },
    {
      "arxiv_id": "2601.02380v1",
      "title": "The Refutability Gap: Challenges in Validating Reasoning by Large Language Models",
      "title_zh": "è¯ä¼ªæ€§é¸¿æ²Ÿï¼šå¤§è¯­è¨€æ¨¡å‹æ¨ç†éªŒè¯é¢ä¸´çš„æŒ‘æˆ˜",
      "authors": [
        "Elchanan Mossel"
      ],
      "abstract": "Recent reports claim that Large Language Models (LLMs) have achieved the ability to derive new science and exhibit human-level general intelligence. We argue that such claims are not rigorous scientific claims, as they do not satisfy Popper's refutability principle (often termed falsifiability), which requires that scientific statements be capable of being disproven. We identify several methodological pitfalls in current AI research on reasoning, including the inability to verify the novelty of findings due to opaque and non-searchable training data, the lack of reproducibility caused by continuous model updates, and the omission of human-interaction transcripts, which obscures the true source of scientific discovery. Additionally, the absence of counterfactuals and data on failed attempts creates a selection bias that may exaggerate LLM capabilities. To address these challenges, we propose guidelines for scientific transparency and reproducibility for research on reasoning by LLMs. Establishing such guidelines is crucial for both scientific integrity and the ongoing societal debates regarding fair data usage.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦æ¨å¯¼å’Œé€šç”¨æ™ºèƒ½æ–¹é¢çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºç›®å‰å…³äº LLMs å…·å¤‡æ¨ç†èƒ½åŠ›çš„è®¸å¤šä¸»å¼ ç¼ºä¹ç§‘å­¦ä¸¥è°¨æ€§ã€‚ä½œè€…è®¤ä¸ºè¿™äº›ä¸»å¼ ä¸ç¬¦åˆæ³¢æ™®å°”çš„å¯è¯ä¼ªæ€§åŸåˆ™ï¼ˆRefutability Principleï¼‰ï¼Œå³ç§‘å­¦é™ˆè¿°å¿…é¡»èƒ½å¤Ÿè¢«è¯æ˜æ˜¯é”™è¯¯çš„ã€‚ç ”ç©¶è¯†åˆ«äº†å½“å‰äººå·¥æ™ºèƒ½æ¨ç†ç ”ç©¶ä¸­çš„å¤šä¸ªæ–¹æ³•è®ºé™·é˜±ï¼ŒåŒ…æ‹¬ç”±äºè®­ç»ƒæ•°æ®ä¸é€æ˜ä¸”ä¸å¯æœç´¢è€Œæ— æ³•éªŒè¯å‘ç°çš„æ–°é¢–æ€§ï¼Œä»¥åŠæ¨¡å‹æŒç»­æ›´æ–°å¯¼è‡´çš„ä¸å¯é‡ç°æ€§ï¼ˆLack of Reproducibilityï¼‰ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æŒ‡å‡ºç”±äºç¼ºå°‘äººç±»äº¤äº’è®°å½•ä»¥åŠåäº‹å®æ•°æ®ï¼ˆCounterfactualsï¼‰å’Œå¤±è´¥è®°å½•ï¼Œç°æœ‰çš„è¯„ä»·å¯èƒ½å­˜åœ¨ä¸¥é‡çš„é€‰æ‹©æ€§åå·®ï¼Œä»è€Œå¤¸å¤§äº† LLMs çš„çœŸå®èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç³»åˆ—æ—¨åœ¨æé«˜ LLMs æ¨ç†ç ”ç©¶ç§‘å­¦é€æ˜åº¦å’Œé‡ç°æ€§çš„æŒ‡å—ã€‚å»ºç«‹è¿™äº›å‡†åˆ™å¯¹äºç»´æŠ¤ç§‘å­¦è¯šä¿¡ä»¥åŠå¤„ç†å…³äºå…¬å¹³æ•°æ®ä½¿ç”¨çš„ç¤¾ä¼šè¾©è®ºå…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "he authors explicitly reserve all rights in this work. No permission is granted for the reproduction, storage, or use of this document for the purpose of training artificial intelligence systems or for text and data mining (TDM), including but not limited to the generation of embeddings, summaries, or synthetic derivatives",
      "pdf_url": "https://arxiv.org/pdf/2601.02380v1",
      "published_date": "2025-12-18 14:42:03 UTC",
      "updated_date": "2025-12-18 14:42:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:00:29.065924+00:00"
    },
    {
      "arxiv_id": "2512.16586v1",
      "title": "Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks",
      "title_zh": "Yuan-TecSwinï¼šä¸€ç§é‡‡ç”¨ Swin-transformer æ¨¡å—çš„æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Shaohua Wu",
        "Tong Yu",
        "Shenling Wang",
        "Xudong Zhao"
      ],
      "abstract": "Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Yuan-TecSwinï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Swin-transformer å—çš„æ–‡æœ¬è°ƒèŠ‚ Diffusion modelï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œ (CNN) åœ¨æ‰©æ•£æ¨¡å‹ä¸­å› å±€éƒ¨æ€§é™åˆ¶å¯¼è‡´çš„å¯¹é•¿ç¨‹è¯­ä¹‰ä¿¡æ¯ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­å¼•å…¥ Swin-transformer æ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†ç‰¹å¾æå–å’Œå›¾åƒä¿®å¤è¿‡ç¨‹ä¸­çš„éå±€éƒ¨å»ºæ¨¡ (non-local modeling) èƒ½åŠ›ã€‚é€šè¿‡ä¼˜åŒ–æ–‡æœ¬ç¼–ç å™¨é€‰æ‹©ã€æ–‡æœ¬åµŒå…¥ (text embedding) çš„æœ‰æ•ˆåˆ©ç”¨ä»¥åŠæ–‡æœ¬æ¡ä»¶çš„èåˆè®¾è®¡ï¼Œæ¨¡å‹æ˜¾è‘—å¢å¼ºäº†æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½ (text-image alignment)ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨äº†é€‚é…çš„æ—¶é—´æ­¥é•¿æœç´¢ç­–ç•¥ï¼Œä½¿æ¨ç†æ€§èƒ½åœ¨ä¸åŒæ‰©æ•£é˜¶æ®µæå‡äº† 10%ã€‚å®éªŒè¡¨æ˜ï¼ŒYuan-TecSwin åœ¨ ImageNet ç”ŸæˆåŸºå‡†ä¸Šå–å¾—äº† 1.37 çš„ FID è¯„åˆ†ï¼Œåœ¨æ— éœ€é¢å¤–æ¨¡å‹è¾…åŠ©çš„æƒ…å†µä¸‹è¾¾åˆ°äº†å½“å‰çš„ State-of-the-art æ°´å‡†ã€‚å¯¹æ¯”æµ‹è¯•æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹ç”Ÿæˆçš„å›¾åƒè´¨é‡æé«˜ï¼Œäººç±»å—è®¿è€…éš¾ä»¥å°†å…¶ä¸çœŸå®çš„äººå·¥ç»˜ç”»ä½œå“åŒºåˆ†å¼€æ¥ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16586v1",
      "published_date": "2025-12-18 14:32:06 UTC",
      "updated_date": "2025-12-18 14:32:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:00:26.461417+00:00"
    },
    {
      "arxiv_id": "2512.16553v1",
      "title": "Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild",
      "title_zh": "Needle in the Webï¼šçœŸå®ç½‘ç»œç¯å¢ƒä¸‹ç›®æ ‡ç½‘é¡µæ£€ç´¢çš„è¯„æµ‹åŸºå‡†",
      "authors": [
        "Yumeng Wang",
        "Tianyu Fan",
        "Lingrui Xu",
        "Chao Huang"
      ],
      "abstract": "Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•å¿½ç•¥æ¨¡ç³Šæ¢ç´¢æ€§æœç´¢(Fuzzy Exploratory Search)çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºNeedle in the Webçš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ç°ä»£æœç´¢æ™ºèƒ½ä½“(Search Agents)å’Œå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†çœŸå®ä¸–ç•Œç½‘é¡µæ—¶çš„æ£€ç´¢ä¸æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†ä¸“æ³¨äºè§£å†³è¯­ä¹‰æ¨¡ç³Šã€å¤šé¢æ€§çš„æŸ¥è¯¢ï¼Œå…±åŒ…å«è¦†ç›–ä¸ƒä¸ªé¢†åŸŸçš„663ä¸ªé«˜è´¨é‡é—®é¢˜ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§èƒ½å¤Ÿç”Ÿæˆå¯æ§éš¾åº¦æŸ¥è¯¢çš„çµæ´»æ–¹æ³•ã€‚é€šè¿‡å¯¹ä¸‰ç§é¢†å…ˆLLMså’Œä¸‰ç§åŸºäºæ™ºèƒ½ä½“çš„æœç´¢ç³»ç»Ÿçš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°ç°æœ‰æ¨¡å‹åœ¨åº”å¯¹æ­¤ç±»ä»»åŠ¡æ—¶æ™®éè¡¨ç°ä¸ä½³ï¼Œå¤šæ•°å‡†ç¡®ç‡ä½äº35%ï¼Œä¸”æ²¡æœ‰æ¨¡å‹èƒ½åœ¨æ‰€æœ‰é¢†åŸŸæˆ–éš¾åº¦çº§åˆ«ä¸ŠæŒç»­è¡¨ç°ä¼˜å¼‚ã€‚è¿™äº›å®éªŒå‘ç°è¡¨æ˜Needle in the Webä¸ºå½“å‰æœç´¢ç³»ç»Ÿå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå¹¶å‡¸æ˜¾äº†è¯­ä¹‰æ¨¡ç³Šç¯å¢ƒä¸‹æœ‰æ•ˆæ¨¡ç³Šæ£€ç´¢(Fuzzy Retrieval)è¿™ä¸€äºŸå¾…è§£å†³çš„å¼€æ”¾æ€§é—®é¢˜ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Data and code are available at https://github.com/Tango-Whiskyman/Needle_in_the_Web",
      "pdf_url": "https://arxiv.org/pdf/2512.16553v1",
      "published_date": "2025-12-18 13:57:28 UTC",
      "updated_date": "2025-12-18 13:57:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:01:10.231061+00:00"
    },
    {
      "arxiv_id": "2512.16532v1",
      "title": "From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment",
      "title_zh": "ä»ä¸ªæ€§åŒ–åˆ°åè§ï¼šæ‹›è˜ä¸­è®°å¿†å¢å¼ºå‹ AI æ™ºèƒ½ä½“çš„åå·®ä¸æ­§è§†",
      "authors": [
        "Himanshu Gharat",
        "Himanshi Agrawal",
        "Gourab K. Patro"
      ],
      "abstract": "Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è®°å¿†å¢å¼ºä¸ªæ€§åŒ–(Memory-enhanced Personalization)åœ¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„æ™ºèƒ½ä½“ä¸­å¼•å‘çš„åè§ä¸æ­§è§†é£é™©ã€‚ç ”ç©¶è€…ä»¥æ‹›è˜(Recruitment)ä¸ºåº”ç”¨åœºæ™¯ï¼Œé€šè¿‡æ¨¡æ‹Ÿè®°å¿†å¢å¼ºä¸ªæ€§åŒ–æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œç³»ç»Ÿåˆ†æäº†åè§åœ¨ä¸åŒæ“ä½œé˜¶æ®µçš„å¼•å…¥ä¸æ”¾å¤§æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä¾¿ä½¿ç”¨ç»è¿‡å®‰å…¨æ€§è®­ç»ƒ(Safety-trained)çš„LLMsï¼Œåè§ä»ä¼šé€šè¿‡ä¸ªæ€§åŒ–è¿‡ç¨‹è¢«ä¸æ–­å¼•å…¥å¹¶å¼ºåŒ–ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†è®°å¿†æœºåˆ¶åœ¨æå‡å“åº”ç›¸å…³æ€§çš„åŒæ—¶å¯èƒ½åŠ å‰§ä¸å¹³ç­‰ï¼Œå¼ºè°ƒäº†åœ¨å¼€å‘åŸºäºLLMçš„è®°å¿†å¢å¼ºæ™ºèƒ½ä½“æ—¶ï¼Œå¿…é¡»å»ºç«‹é¢å¤–çš„ä¿æŠ¤æªæ–½æˆ–æ™ºèƒ½ä½“æŠ¤æ (Agent Guardrails)ä»¥åº”å¯¹æ½œåœ¨çš„ä¼¦ç†é£é™©ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26)",
      "pdf_url": "https://arxiv.org/pdf/2512.16532v1",
      "published_date": "2025-12-18 13:41:37 UTC",
      "updated_date": "2025-12-18 13:41:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:01:34.662198+00:00"
    },
    {
      "arxiv_id": "2512.16531v4",
      "title": "Scaling Laws for Energy Efficiency of Local LLMs",
      "title_zh": "æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹èƒ½æ•ˆçš„æ‰©å±•å®šå¾‹",
      "authors": [
        "Ander Alvarez",
        "Alessandro Genuardi",
        "Nilotpal Sinha",
        "Antonio Tiene",
        "Mikail Okyay",
        "Bakbergen Ryskulov",
        "David Montero",
        "Samuel Mugel",
        "RomÃ¡n OrÃºs"
      ],
      "abstract": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°é‡åŒ–äº†åœ¨ MacBook Pro M2 å’Œ Raspberry Pi 5 ç­‰è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œä»…ä¾é  CPU è¿›è¡Œæœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¨ç†æ—¶çš„èƒ½æ•ˆæ‰©å±•è§„å¾‹ã€‚é€šè¿‡å¯¹å¤„ç†å™¨å’Œå†…å­˜ä½¿ç”¨æƒ…å†µè¿›è¡Œè¿ç»­é‡‡æ ·å’Œç§¯åˆ†ï¼Œç ”ç©¶å‘ç° LLMs çš„æ¨ç†è®¡ç®—æˆæœ¬éš Token é•¿åº¦å‘ˆè¿‘ä¼¼çº¿æ€§æ‰©å±•ã€‚å¯¹äº VLMsï¼Œç ”ç©¶æ­ç¤ºäº†ç”±é¢„å¤„ç†é©±åŠ¨çš„â€œåˆ†è¾¨ç‡æ‹ç‚¹â€ï¼ˆResolution Kneeï¼‰ï¼Œå³è®¡ç®—è´Ÿè½½åœ¨è¶…è¿‡å†…éƒ¨åˆ†è¾¨ç‡é™åˆ¶åè¶‹äºç¨³å®šï¼Œè€Œåœ¨é™åˆ¶ä»¥ä¸‹åˆ™å¤§å¹…ä¸‹é™ã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜å—é‡å­å¯å‘çš„å‹ç¼©æŠ€æœ¯ï¼ˆQuantum-inspired Compressionï¼‰èƒ½åœ¨ä¿è¯ç”šè‡³æå‡è¯­ä¹‰å‡†ç¡®åº¦çš„å‰æä¸‹ï¼Œå‡å°‘é«˜è¾¾ 71.9% çš„èµ„æºå ç”¨å’Œ 62% çš„èƒ½è€—ã€‚è¯¥æˆæœè¯†åˆ«äº†æ¨¡å‹å‹ç¼©å’Œè¾“å…¥åˆ†è¾¨ç‡é¢„å¤„ç†ä½œä¸ºå®ç°å¯æŒç»­è¾¹ç¼˜æ¨ç†çš„å…³é”®æ æ†ï¼Œä¸ºæœ¬åœ°å¤šæ¨¡æ€ CPU æ¨ç†æä¾›äº†é‡è¦çš„ç³»ç»Ÿé‡åŒ–ä¾æ®å’Œä¼˜åŒ–è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16531v4",
      "published_date": "2025-12-18 13:40:33 UTC",
      "updated_date": "2025-12-29 15:54:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:01:41.884325+00:00"
    },
    {
      "arxiv_id": "2512.16530v1",
      "title": "Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬é€šä¿—åŒ–æ”¹ç¼–ï¼šè¯„ä¼°æŒ‡æ ‡å¯¹æ¯”",
      "authors": [
        "Primoz Kocbek",
        "Leon Kopitar",
        "Gregor Stiglic"
      ],
      "abstract": "This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å°†ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬ç®€åŒ–ä¸ºé€šä¿—è¯­è¨€çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å…¬ä¼—çš„å¥åº·ç´ å…»ã€‚ç ”ç©¶è€…å¼€å‘å¹¶è¯„ä¼°äº†åŸºäºæç¤ºè¯æ¨¡æ¿ (Prompt Template) çš„åŸºå‡†æ–¹æ¡ˆã€åŒæ™ºèƒ½ä½“ (Two AI Agent) åä½œæ–¹æ¡ˆä»¥åŠå¾®è°ƒ (Fine-tuning) æ–¹æ¡ˆï¼Œå¹¶é€‰å– gpt-4o ä¸ gpt-4o mini ä½œä¸ºæ ¸å¿ƒæ¨¡å‹ã€‚è¯„ä¼°ä½“ç³»æ¶µç›–äº† Flesch-Kincaid grade levelã€SMOG Indexã€SARI å’Œ BERTScore ç­‰ä¼ ç»Ÿå®šé‡æŒ‡æ ‡ï¼Œä»¥åŠé’ˆå¯¹ç®€æ´æ€§ä¸å‡†ç¡®æ€§çš„ 5-point Likert scales å®šæ€§è¯„ä¼°ï¼ŒåŒæ—¶å¼•å…¥äº† G-Eval æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œgpt-4o-mini åœ¨è¯¥ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œè€Œå¾®è°ƒ (Fine-tuning) æ–¹æ³•çš„æ•ˆæœç›¸å¯¹æ¬ ä½³ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œä½œä¸º LLM å®šé‡æŒ‡æ ‡çš„ G-Eval ä¸å®šæ€§è¯„ä¼°ç»“æœå…·æœ‰é«˜åº¦ä¸€è‡´æ€§ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦æ–‡æœ¬ç®€åŒ–çš„è‡ªåŠ¨åŒ–è¯„ä¼°æä¾›äº†æœ‰æ•ˆå‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2512.16530v1",
      "published_date": "2025-12-18 13:37:58 UTC",
      "updated_date": "2025-12-18 13:37:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:02:27.141832+00:00"
    },
    {
      "arxiv_id": "2512.16529v2",
      "title": "ParamExplorer: A framework for exploring parameters in generative art",
      "title_zh": "ParamExplorerï¼šç”Ÿæˆè‰ºæœ¯å‚æ•°æ¢ç´¢æ¡†æ¶",
      "authors": [
        "Julien Gachadoat",
        "Guillaume Lagarde"
      ],
      "abstract": "Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆè‰ºæœ¯(Generative art)ç³»ç»Ÿä¸­é«˜ç»´ä¸”å¤æ‚çš„å‚æ•°ç©ºé—´å¯¼è‡´è‰ºæœ¯åˆ›ä½œä¾èµ–è€—æ—¶æ‰‹åŠ¨å°è¯•çš„é—®é¢˜ï¼Œæå‡ºäº†ParamExploreræ¡†æ¶ã€‚è¿™æ˜¯ä¸€ä¸ªå—å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¯å‘çš„å¯äº¤äº’ã€æ¨¡å—åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡äººç±»åœ¨ç¯(Human-in-the-loop)æˆ–è‡ªåŠ¨åé¦ˆå¼•å¯¼ï¼Œè¾…åŠ©ç”Ÿæˆè‰ºæœ¯ç®—æ³•çš„å‚æ•°ç©ºé—´æ¢ç´¢ã€‚è¯¥æ¡†æ¶å…·å¤‡è‰¯å¥½çš„å…¼å®¹æ€§ï¼Œèƒ½å¤Ÿä¸ç°æœ‰çš„p5jsé¡¹ç›®æ— ç¼é›†æˆï¼Œå¹¶å…è®¸ç”¨æˆ·é€šè¿‡ä¸åŒçš„ç­–ç•¥è¿›è¡Œå®éªŒã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…åœ¨è¯¥æ¡†æ¶å†…å®ç°å¹¶è¯„ä¼°äº†å¤šç§è¢«ç§°ä¸ºæ™ºèƒ½ä½“(agents)çš„æ¢ç´¢ç­–ç•¥ï¼Œä»¥éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚è¯¥æ¡†æ¶ä¸ºè§£å†³ç”Ÿæˆè‰ºæœ¯ä¸­çš„ç»„åˆçˆ†ç‚¸é—®é¢˜æä¾›äº†æ–°é€”å¾„ï¼Œå¸®åŠ©è‰ºæœ¯å®¶å‘æ˜æ›´å¤šæ½œåœ¨ä¸”å…·æœ‰ç¾æ„Ÿçš„è§†è§‰é…ç½®ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16529v2",
      "published_date": "2025-12-18 13:37:50 UTC",
      "updated_date": "2025-12-19 09:09:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:01:39.475417+00:00"
    },
    {
      "arxiv_id": "2512.16523v1",
      "title": "TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models",
      "title_zh": "TTPï¼šé¢å‘è§†è§‰-è¯­è¨€æ¨¡å‹å¯¹æŠ—æ£€æµ‹ä¸é²æ£’è‡ªé€‚åº”çš„æµ‹è¯•æ—¶å¡«å……",
      "authors": [
        "Zhiwei Li",
        "Yitian Pang",
        "Weining Wang",
        "Zhenan Sun",
        "Qi Li"
      ],
      "abstract": "Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ CLIP ç­‰è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨é¢å¯¹å¯¹æŠ—æ€§æ‰°åŠ¨æ—¶è¡¨ç°å‡ºçš„è„†å¼±æ€§ï¼Œæå‡ºäº†åä¸º Test-Time Padding (TTP) çš„è½»é‡çº§é˜²å¾¡æ¡†æ¶ã€‚TTP é€šè¿‡åˆ†æç©ºé—´å¡«å……å‰åç‰¹å¾åµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦åç§»(cosine similarity shift)æ¥å®ç°é«˜æ•ˆçš„å¯¹æŠ—æ£€æµ‹ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªé€‚ç”¨äºä¸åŒæ¶æ„å’Œæ•°æ®é›†çš„é€šç”¨æ£€æµ‹é˜ˆå€¼ã€‚é’ˆå¯¹è¯†åˆ«å‡ºçš„å¯¹æŠ—æ€§æ ·æœ¬ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¯è®­ç»ƒå¡«å……(trainable padding)ä¿®å¤è¢«å¹²æ‰°çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¹¶ç»“åˆç›¸ä¼¼åº¦æ„ŸçŸ¥é›†æˆç­–ç•¥(similarity-aware ensemble)ä»¥ç¡®ä¿æœ€ç»ˆé¢„æµ‹çš„ç¨³å¥æ€§ã€‚å¯¹äºæ­£å¸¸çš„å¹²å‡€è¾“å…¥ï¼ŒTTP é»˜è®¤ä¿æŒå…¶åŸå§‹çŠ¶æ€æˆ–é€šè¿‡é›†æˆç°æœ‰çš„æµ‹è¯•æ—¶é€‚åº”(Test-Time Adaptation)æŠ€æœ¯æ¥è¿›ä¸€æ­¥ä¼˜åŒ–å‡†ç¡®ç‡ã€‚åœ¨å¤šç§ CLIP éª¨å¹²ç½‘ç»œä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTTP åœ¨æ˜¾è‘—å¢å¼ºå¯¹æŠ—é²æ£’æ€§(adversarial robustness)çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¿å…å¯¹å¹²å‡€å‡†ç¡®ç‡(clean accuracy)çš„è´Ÿé¢å½±å“ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16523v1",
      "published_date": "2025-12-18 13:34:14 UTC",
      "updated_date": "2025-12-18 13:34:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:02:38.908292+00:00"
    },
    {
      "arxiv_id": "2512.16515v2",
      "title": "The Universe Learning Itself: On the Evolution of Dynamics from the Big Bang to Machine Intelligence",
      "title_zh": "è‡ªå­¦ä¹ çš„å®‡å®™ï¼šè®ºä»å¤§çˆ†ç‚¸åˆ°æœºå™¨æ™ºèƒ½çš„åŠ¨åŠ›å­¦æ¼”åŒ–",
      "authors": [
        "Pradeep Singh",
        "Mudasani Rushikesh",
        "Bezawada Sri Sai Anurag",
        "Balasubramanian Raman"
      ],
      "abstract": "We develop a unified, dynamical-systems narrative of the universe that traces a continuous chain of structure formation from the Big Bang to contemporary human societies and their artificial learning systems. Rather than treating cosmology, astrophysics, geophysics, biology, cognition, and machine intelligence as disjoint domains, we view each as successive regimes of dynamics on ever-richer state spaces, stitched together by phase transitions, symmetry-breaking events, and emergent attractors. Starting from inflationary field dynamics and the growth of primordial perturbations, we describe how gravitational instability sculpts the cosmic web, how dissipative collapse in baryonic matter yields stars and planets, and how planetary-scale geochemical cycles define long-lived nonequilibrium attractors. Within these attractors, we frame the origin of life as the emergence of self-maintaining reaction networks, evolutionary biology as flow on high-dimensional genotype-phenotype-environment manifolds, and brains as adaptive dynamical systems operating near critical surfaces. Human culture and technology-including modern machine learning and artificial intelligence-are then interpreted as symbolic and institutional dynamics that implement and refine engineered learning flows which recursively reshape their own phase space. Throughout, we emphasize recurring mathematical motifs-instability, bifurcation, multiscale coupling, and constrained flows on measure-zero subsets of the accessible state space. Our aim is not to present any new cosmological or biological model, but a cross-scale, theoretical perspective: a way of reading the universe's history as the evolution of dynamics itself, culminating (so far) in biological and artificial systems capable of modeling, predicting, and deliberately perturbing their own future trajectories.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„åŠ¨åŠ›ç³»ç»Ÿ (dynamical-systems) å™äº‹æ¡†æ¶ï¼Œæ—¨åœ¨å°†ä»å¤§çˆ†ç‚¸ (Big Bang) åˆ°å½“ä»£äººç±»ç¤¾ä¼šåŠäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç»“æ„æ¼”åŒ–è¿‡ç¨‹ä¸²è”èµ·æ¥ã€‚è®ºæ–‡å°†å®‡å®™å­¦ã€ç”Ÿç‰©å­¦åŠæœºå™¨æ™ºèƒ½ç­‰è§†ä¸ºåœ¨æ—¥ç›Šä¸°å¯Œçš„çŠ¶æ€ç©ºé—´ (state spaces) ä¸Šé€šè¿‡ç›¸å˜ (phase transitions) å’Œæ¶Œç°å¸å¼•å­ (emergent attractors) è¡”æ¥çš„è¿ç»­åŠ¨åŠ›å­¦æœºåˆ¶ã€‚ä»æš´èƒ€åœºåŠ¨åŠ›å­¦ (inflationary field dynamics) åˆ°è¡Œæ˜Ÿå°ºåº¦çš„éå¹³è¡¡å¸å¼•å­ (nonequilibrium attractors)ï¼Œç ”ç©¶æç»˜äº†ç‰©è´¨å¦‚ä½•é€šè¿‡å¼•åŠ›å’Œè€—æ•£ç»“æ„å®ç°å¤æ‚åŒ–ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œç”Ÿå‘½ã€å¤§è„‘åŠäººç±»æ–‡åŒ–è¢«è¯ é‡Šä¸ºåœ¨ç‰¹å®šæµå½¢ä¸Šè¿è¡Œçš„è‡ªé€‚åº”åŠ¨åŠ›ç³»ç»Ÿ (adaptive dynamical systems)ï¼Œé€šè¿‡ç¬¦å·åŠ¨åŠ›å­¦é€’å½’åœ°é‡å¡‘ç›¸ä½ç©ºé—´ (phase space)ã€‚ç ”ç©¶å¼ºè°ƒäº†ä¸ç¨³å®šæ€§ã€åˆ†å‰ (bifurcation) å’Œå¤šå°ºåº¦è€¦åˆ (multiscale coupling) ç­‰æ•°å­¦åŸºå…ƒï¼Œå°†å®‡å®™å²è§†ä¸ºåŠ¨åŠ›å­¦æœ¬èº«çš„æ¼”åŒ–è¿‡ç¨‹ã€‚è¿™ä¸€æ¼”åŒ–æœ€ç»ˆäº§å‡ºäº†èƒ½å¤Ÿå»ºæ¨¡ã€é¢„æµ‹å¹¶ä¸»åŠ¨å¹²é¢„è‡ªèº«æœªæ¥å‘å±•è½¨è¿¹çš„ç”Ÿç‰©ä¸äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚",
      "categories": [
        "nlin.AO",
        "cs.AI"
      ],
      "primary_category": "nlin.AO",
      "comment": "38 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16515v2",
      "published_date": "2025-12-18 13:28:02 UTC",
      "updated_date": "2025-12-21 05:35:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:01:54.863349+00:00"
    },
    {
      "arxiv_id": "2512.16512v1",
      "title": "XTC, A Research Platform for Optimizing AI Workload Operators",
      "title_zh": "XTCï¼šé¢å‘ AI å·¥ä½œè´Ÿè½½ç®—å­ä¼˜åŒ–çš„ç ”ç©¶å¹³å°",
      "authors": [
        "Pompougnac Hugo",
        "Guillon Christophe",
        "Noiry Sylvain",
        "Dutilleul Alban",
        "Iooss Guillaume",
        "Rastello Fabrice"
      ],
      "abstract": "Achieving high efficiency on AI operators demands precise control over computation and data movement. However, existing scheduling languages are locked into specific compiler ecosystems, preventing fair comparison, reuse, and evaluation across frameworks. No unified interface currently decouples scheduling specification from code generation and measurement. We introduce XTC, a platform that unifies scheduling and performance evaluation across compilers. With its common API and reproducible measurement framework, XTC enables portable experimentation and accelerates research on optimization strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† XTCï¼Œä¸€ä¸ªæ—¨åœ¨ä¼˜åŒ– AI ç®—å­(AI operators)å·¥ä½œè´Ÿè½½çš„ç ”ç©¶å¹³å°ï¼Œä»¥è§£å†³ç°æœ‰è°ƒåº¦è¯­è¨€(scheduling languages)å—é™äºç‰¹å®šç¼–è¯‘å™¨ç”Ÿæ€ç³»ç»Ÿè€Œéš¾ä»¥è¿›è¡Œè·¨æ¡†æ¶æ¯”è¾ƒå’Œé‡ç”¨çš„é—®é¢˜ã€‚XTC æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¥å£ï¼Œå®ç°äº†è°ƒåº¦è§„èŒƒ(scheduling specification)ä¸ä»£ç ç”Ÿæˆ(code generation)åŠæ€§èƒ½æµ‹é‡çš„è§£è€¦ï¼Œå¡«è¡¥äº†å½“å‰å·¥å…·é“¾ä¸­çš„ç©ºç™½ã€‚é€šè¿‡é€šç”¨çš„ API å’Œå¯é‡å¤çš„æµ‹é‡æ¡†æ¶ï¼Œè¯¥å¹³å°ç»Ÿä¸€äº†è·¨ç¼–è¯‘å™¨çš„è°ƒåº¦ä¸è¯„ä¼°æµç¨‹ï¼Œæ”¯æŒé«˜åº¦å¯ç§»æ¤çš„å®éªŒè®¾è®¡ã€‚XTC çš„é—®ä¸–æ˜¾è‘—åŠ é€Ÿäº†ç®—å­ä¼˜åŒ–ç­–ç•¥çš„ç ”ç©¶è¿›ç¨‹ï¼Œä¸ºä¸åŒæ¡†æ¶é—´çš„å…¬å¹³è¯„ä¼°å’ŒæŠ€æœ¯å¤ç”¨æä¾›äº†æ ‡å‡†åŒ–çš„ç§‘ç ”åŸºç¡€è®¾æ–½ã€‚",
      "categories": [
        "cs.PF",
        "cs.AI"
      ],
      "primary_category": "cs.PF",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16512v1",
      "published_date": "2025-12-18 13:24:44 UTC",
      "updated_date": "2025-12-18 13:24:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:01:57.800279+00:00"
    },
    {
      "arxiv_id": "2512.16494v1",
      "title": "PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation",
      "title_zh": "PoseMoEï¼šç”¨äºå•ç›®3Däººä½“å§¿æ€ä¼°è®¡çš„æ··åˆä¸“å®¶ç½‘ç»œ",
      "authors": [
        "Mengyuan Liu",
        "Jiajie Liu",
        "Jinyan Zhang",
        "Wenhao Li",
        "Junsong Yuan"
      ],
      "abstract": "The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•ç›® 3D äººä½“å§¿æ€ä¼°è®¡æå‡ºäº† PoseMoEï¼Œä¸€ç§æ—¨åœ¨è§£å†³æ·±åº¦ç‰¹å¾ä¸ç¡®å®šæ€§å¯¹ 2D å§¿æ€ç‰¹å¾äº§ç”Ÿè´Ÿé¢å½±å“çš„ Mixture-of-Experts ç½‘ç»œã€‚ä¼ ç»Ÿçš„ lifting-based æ–¹æ³•é€šå¸¸åœ¨çº ç¼ çš„ç‰¹å¾ç©ºé—´ä¸­åŒæ—¶å¤„ç† 2D å§¿æ€ä¸æœªçŸ¥æ·±åº¦ï¼Œè¿™é™åˆ¶äº†æ•´ä½“ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚PoseMoE é€šè¿‡ä¸“é—¨çš„ä¸“å®¶æ¨¡å—å®ç°äº† 2D å§¿æ€ä¸æ·±åº¦ç‰¹å¾ç¼–ç çš„è§£è€¦ï¼Œä»è€Œå‡å°‘äº†æ·±åº¦ä¸ç¡®å®šæ€§å¯¹ 2D å§¿æ€ç‰¹å¾çš„å¹²æ‰°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†è·¨ä¸“å®¶çŸ¥è¯†èšåˆæ¨¡å— (cross-expert knowledge aggregation module)ï¼Œé€šè¿‡åŒå‘æ˜ å°„æ•´åˆæ—¶ç©ºä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å¢å¼ºç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPoseMoE åœ¨ Human3.6Mã€MPI-INF-3DHP å’Œ 3DPW ä¸‰ä¸ªä¸»æµæ•°æ®é›†ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„ lifting-based æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IEEE Transactions on Image Processing (T-IP)",
      "pdf_url": "https://arxiv.org/pdf/2512.16494v1",
      "published_date": "2025-12-18 13:01:36 UTC",
      "updated_date": "2025-12-18 13:01:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:02:13.091799+00:00"
    },
    {
      "arxiv_id": "2512.16491v2",
      "title": "Best Practices For Empirical Meta-Algorithmic Research: Guidelines from the COSEAL Research Network",
      "title_zh": "å…ƒç®—æ³•å®è¯ç ”ç©¶çš„æœ€ä½³å®è·µï¼šCOSEAL ç ”ç©¶ç½‘ç»œæŒ‡å—",
      "authors": [
        "Theresa Eimer",
        "Lennart SchÃ¤permeier",
        "AndrÃ© Biedenkapp",
        "Alexander Tornede",
        "Lars Kotthoff",
        "Pieter Leyman",
        "Matthias Feurer",
        "Katharina Eggensperger",
        "Kaitlin Maile",
        "Tanja Tornede",
        "Anna Kozak",
        "Ke Xue",
        "Marcel Wever",
        "Mitra Baratchi",
        "Damir Pulatov",
        "Heike Trautmann",
        "Haniye Kashgarani",
        "Marius Lindauer"
      ],
      "abstract": "Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing experiments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.",
      "tldr_zh": "è¯¥ç ”ç©¶æ±‡æ€»äº†æ¥è‡ª COSEAL ç ”ç©¶ç½‘ç»œçš„å®è¯å…ƒç®—æ³• (Empirical Meta-Algorithmic Research) æœ€ä½³å®è·µæŒ‡å—ï¼Œæ—¨åœ¨è§£å†³è¯¥é¢†åŸŸåœ¨å®éªŒä¸­é¢ä¸´çš„è®¡ç®—æˆæœ¬é«˜ã€è¯¯å·®æºå¤šä»¥åŠç§‘ç ”ç»“è®ºæ•ˆåº¦å—æŸç­‰é—®é¢˜ã€‚æŠ¥å‘Šç³»ç»Ÿæ€§åœ°æ•´åˆäº†åŸæœ¬æ•£è½åœ¨ä¸åŒå­é¢†åŸŸä¸­çš„ä¼˜ç§€åšæ³•ï¼Œæ¶µç›–äº†ä»åˆ¶å®šç ”ç©¶é—®é¢˜ã€é€‰æ‹©å®éªŒè®¾è®¡ (Experimental Design)ã€æ‰§è¡Œå®éªŒåˆ°æœ€ç»ˆå…¬æ­£åˆ†æå¹¶å‘ˆç°ç»“æœçš„å®Œæ•´ç§‘ç ”å‘¨æœŸã€‚å…¶æŒ‡å¯¼èŒƒå›´æ¶µç›–äº†ç®—æ³•é€‰æ‹© (Algorithm Selection)ã€é…ç½® (Configuration) å’Œè°ƒåº¦ (Scheduling) ç­‰æ ¸å¿ƒä»»åŠ¡ã€‚é€šè¿‡ç¡®ç«‹è¯¥é¢†åŸŸçš„å½“å‰æŠ€æœ¯ç°çŠ¶ (State-of-the-art)ï¼Œè¯¥æŒ‡å—ä¸ºæ–°è¿›ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†æ ‡å‡†åŒ–çš„æ“ä½œå‚è€ƒã€‚è¿™ä¸€ä¸¾æªä¸ä»…æœ‰åŠ©äºè§„èŒƒå®éªŒæµç¨‹ï¼Œä¹Ÿä¸ºæå‡ç§‘å­¦è§è§£çš„æœ‰æ•ˆæ€§ä¸å¯æ‰©å±•æ€§æä¾›äº†é‡è¦ä¿éšœã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16491v2",
      "published_date": "2025-12-18 12:59:45 UTC",
      "updated_date": "2025-12-19 11:30:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:02:16.644274+00:00"
    },
    {
      "arxiv_id": "2512.16970v1",
      "title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework",
      "title_zh": "PAACEï¼šä¸€ç§è§„åˆ’æ„ŸçŸ¥çš„è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“ä¸Šä¸‹æ–‡å·¥ç¨‹æ¡†æ¶",
      "authors": [
        "Kamer Ali Yuksel"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PAACE (Plan-Aware Automated Agent Context Engineering)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ (LLM) æ™ºèƒ½ä½“åœ¨å¤æ‚å¤šæ­¥å·¥ä½œæµä¸­ä¸æ–­æ‰©å¼ çš„ä¸Šä¸‹æ–‡è€Œè®¾è®¡çš„è®¡åˆ’æ„ŸçŸ¥è‡ªåŠ¨å·¥ç¨‹æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ‘˜è¦å’Œå‹ç¼©æ–¹æ³•å¿½è§†æ™ºèƒ½ä½“æ¨ç†ä¸­è®¡åˆ’æ„ŸçŸ¥ç‰¹æ€§çš„é—®é¢˜ï¼ŒPAACE é€šè¿‡ next-k-task ç›¸å…³æ€§å»ºæ¨¡å’Œè®¡åˆ’ç»“æ„åˆ†æå®ç°äº†å¯¹æ™ºèƒ½ä½“çŠ¶æ€çš„åŠ¨æ€ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶ç”±å¤§è§„æ¨¡åˆæˆå·¥ä½œæµç”Ÿæˆå™¨ PAACE-Syn å’ŒåŸºäºæ•™å¸ˆæ¼”ç¤ºè’¸é¦çš„è®¡åˆ’æ„ŸçŸ¥å‹ç¼©å™¨å®¶æ— PAACE-FT ç»„æˆã€‚åœ¨ AppWorldã€OfficeBench å’Œ 8-Objective QA ç­‰é•¿ç¨‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPAACE åœ¨æé«˜æ™ºèƒ½ä½“æ­£ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†å³°å€¼ä¸Šä¸‹æ–‡è´Ÿè½½å’Œç´¯ç§¯ä¾èµ–ã€‚å®éªŒè¯æ˜ï¼Œè’¸é¦åçš„ PAACE-FT èƒ½å¤Ÿä¿ç•™æ•™å¸ˆæ¨¡å‹ 97% çš„æ€§èƒ½ï¼Œå¹¶å°†æ¨ç†æˆæœ¬é™ä½äº†ä¸€ä¸ªæ•°é‡çº§ä»¥ä¸Šï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å®é™…éƒ¨ç½²ä¸­çš„é«˜æ•ˆæ€§ä¸å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16970v1",
      "published_date": "2025-12-18 12:54:56 UTC",
      "updated_date": "2025-12-18 12:54:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:02:34.584997+00:00"
    },
    {
      "arxiv_id": "2512.16485v1",
      "title": "Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors",
      "title_zh": "é¢å¸¦å¾®ç¬‘ï¼Œçœ¼å«å¿§ä¼¤ï¼šåˆ©ç”¨çœ¼éƒ¨ä¸é¢éƒ¨è¡Œä¸ºå¤šæ¨¡æ€æ•°æ®é›†å¼¥åˆæƒ…æ„Ÿé¸¿æ²Ÿ",
      "authors": [
        "Kejun Liu",
        "Yuanyuan Liu",
        "Lin Wei",
        "Chang Tang",
        "Yibing Zhan",
        "Zijing Chen",
        "Zhe Chen"
      ],
      "abstract": "Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢éƒ¨è¡¨æƒ…è¯†åˆ«(FER)å¾€å¾€åæ˜ ç¤¾äº¤ä¿¡å·è€ŒéçœŸå®å†…åœ¨æƒ…æ„Ÿçš„é—®é¢˜ï¼Œæ¢è®¨äº†æƒ…æ„Ÿè¯†åˆ«(ER)ä¸FERä¹‹é—´çš„å·®è·ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªåä¸ºEMER (Eye-behavior-aided Multimodal Emotion Recognition) çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ•°æ®é›†ï¼Œå°†çœ¼éƒ¨è¡Œä¸ºä½œä¸ºé‡è¦çš„æƒ…æ„Ÿçº¿ç´¢å¼•å…¥ã€‚è¯¥æ•°æ®é›†é€šè¿‡è‡ªå‘æƒ…æ„Ÿè¯±å¯¼èŒƒå¼ï¼Œåœ¨é‡‡é›†é¢éƒ¨è¡¨æƒ…è§†é¢‘çš„åŒæ—¶åŒæ­¥è·å–éä¾µå…¥å¼çœ¼åŠ¨åºåˆ—å’Œæ³¨è§†ç‚¹å›¾ï¼Œå¹¶åˆ†åˆ«ä¸ºERå’ŒFERæ ‡æ³¨äº†å¤šè§†è§’æ ‡ç­¾ä»¥æ­ç¤ºäºŒè€…çš„é¸¿æ²Ÿã€‚åŸºäºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†EMERT (Eye-behavior-aided MER Transformer) æ¨¡å‹ï¼Œåˆ©ç”¨æ¨¡æ€å¯¹æŠ—ç‰¹å¾è§£è€¦(modality-adversarial feature decoupling)å’Œå¤šä»»åŠ¡Transformerå°†çœ¼éƒ¨è¡Œä¸ºå»ºæ¨¡ä¸ºé¢éƒ¨è¡¨æƒ…çš„æœ‰åŠ›äº’è¡¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEMERTåœ¨å¤šç§åŸºå‡†è¯„ä¼°ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰SOTAæ–¹æ³•ï¼ŒéªŒè¯äº†å»ºæ¨¡çœ¼éƒ¨è¡Œä¸ºåœ¨æå‡ERé²æ£’æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºå¼¥è¡¥FERä¸ERä¹‹é—´çš„å·®è·æä¾›äº†æ·±åº¦åˆ†æï¼Œä¹Ÿä¸ºå®ç°æ›´å¯é çš„æƒ…æ„Ÿè®¡ç®—ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by TMM",
      "pdf_url": "https://arxiv.org/pdf/2512.16485v1",
      "published_date": "2025-12-18 12:52:55 UTC",
      "updated_date": "2025-12-18 12:52:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:03:06.843104+00:00"
    },
    {
      "arxiv_id": "2512.16484v1",
      "title": "Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment",
      "title_zh": "å¼•å¯¼æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä»·ä¸­çš„æ„ŸçŸ¥-æ¨ç†æ›´è¶‹äºäººç±»",
      "authors": [
        "Yuan Li",
        "Yahan Yu",
        "Youyuan Lin",
        "Yong-Hao Yang",
        "Chenhui Chu",
        "Shin'ya Nishida"
      ],
      "abstract": "Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åœ¨æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°(Blind Image Quality Assessment, BIQA)ä¸­ä½¿æ¨¡å‹å…·å¤‡ç±»äººä¸”è‡ªæ´½çš„æ„ŸçŸ¥æ¨ç†èƒ½åŠ›ã€‚ä½œè€…é¦–å…ˆæ”¶é›†äº†æ•æ‰äººç±»æ„ŸçŸ¥æ¨ç†è¿‡ç¨‹çš„å¤šç»´åº¦è¯„ä¼°æ•°æ®ï¼Œå¹¶é‡‡ç”¨ Reinforcement Learning æ–¹æ³•ï¼Œå°†äººç±»æ ‡æ³¨ä½œä¸ºå¥–åŠ±ä¿¡å·æ¥å¼•å¯¼æ¨¡å‹æ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥ã€‚ä¸ºäº†ä½¿æ¨¡å‹å†…åŒ–è‡ªæ´½çš„æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ç§å¥–åŠ±æœºåˆ¶ï¼Œé©±åŠ¨æ¨¡å‹ä»…é€šè¿‡å…¶è‡ªèº«ç”Ÿæˆçš„æè¿°æ¥æ¨æ–­å›¾åƒè´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ Pearson å’Œ Spearman ç›¸å…³ç³»æ•°ç­‰é€šç”¨æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†ä¸å½“å‰ SOTA ç³»ç»Ÿç›¸å½“çš„æ°´å¹³ã€‚åœ¨è¡¡é‡äººæœºä¸€è‡´æ€§çš„ ROUGE-1 æŒ‡æ ‡ä¸Šï¼Œè¯¥æ¨¡å‹åœ¨ human-annotated æ ·æœ¬ä¸Šè¾¾åˆ°äº† 0.512ï¼Œæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹çš„ 0.443ã€‚è¿™é¡¹å·¥ä½œæœ‰æ•ˆæå‡äº†æ¨¡å‹ç”Ÿæˆçš„è§£é‡Šä¸äººç±»æ„ŸçŸ¥çš„ä¸€è‡´æ€§ï¼Œä¸º BIQA é¢†åŸŸå®ç°ç±»äººçš„å¯è§£é‡Šæ€§æ¨ç†è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2512.16484v1",
      "published_date": "2025-12-18 12:52:37 UTC",
      "updated_date": "2025-12-18 12:52:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:03:07.992136+00:00"
    },
    {
      "arxiv_id": "2512.16969v1",
      "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
      "title_zh": "åŸºäºç§‘å­¦å®¶å¯¹é½å·¥ä½œæµçš„å¤§è¯­è¨€æ¨¡å‹ç§‘å­¦é€šç”¨æ™ºèƒ½æ¢ç©¶",
      "authors": [
        "Wanghan Xu",
        "Yuhao Zhou",
        "Yifan Zhou",
        "Qinglong Cao",
        "Shuo Li",
        "Jia Bu",
        "Bo Liu",
        "Yixin Chen",
        "Xuming He",
        "Xiangyu Zhao",
        "Xiang Zhuang",
        "Fengxiang Wang",
        "Zhiwang Zhou",
        "Qiantai Feng",
        "Wenxuan Huang",
        "Jiaqi Wei",
        "Hao Wu",
        "Yuejin Yang",
        "Guangshuai Wang",
        "Sheng Xu",
        "Ziyan Huang",
        "Xinyao Liu",
        "Jiyao Liu",
        "Cheng Tang",
        "Wei Li",
        "Ying Chen",
        "Junzhi Ning",
        "Pengfei Jiang",
        "Chenglong Ma",
        "Ye Du",
        "Changkai Ji",
        "Huihui Xu",
        "Ming Hu",
        "Jiangbin Zheng",
        "Xin Chen",
        "Yucheng Wu",
        "Feifei Jiang",
        "Xi Chen",
        "Xiangru Tang",
        "Yuchen Fu",
        "Yingzhou Lu",
        "Yuanyuan Zhang",
        "Lihao Sun",
        "Chengbo Li",
        "Jinzhe Ma",
        "Wanhao Liu",
        "Yating Liu",
        "Kuo-Cheng Wu",
        "Shengdu Chai",
        "Yizhou Wang",
        "Ouwen Zhangjin",
        "Chen Tang",
        "Shufei Zhang",
        "Wenbo Cao",
        "Junjie Ren",
        "Taoyong Cui",
        "Zhouheng Yao",
        "Juntao Deng",
        "Yijie Sun",
        "Feng Liu",
        "Wangxu Wei",
        "Jingyi Xu",
        "Zhangrui Li",
        "Junchao Gong",
        "Zijie Guo",
        "Zhiyu Yao",
        "Zaoyu Chen",
        "Tianhao Peng",
        "Fangchen Yu",
        "Bo Zhang",
        "Dongzhan Zhou",
        "Shixiang Tang",
        "Jiaheng Liu",
        "Fenghua Ling",
        "Yan Lu",
        "Yuchen Ren",
        "Ben Fei",
        "Zhen Zhao",
        "Xinyu Gu",
        "Rui Su",
        "Xiao-Ming Wu",
        "Weikang Si",
        "Yang Liu",
        "Hao Chen",
        "Xiangchao Yan",
        "Xue Yang",
        "Junchi Yan",
        "Jiamin Wu",
        "Qihao Zheng",
        "Chenhui Li",
        "Zhiqiang Gao",
        "Hao Kong",
        "Junjun He",
        "Mao Su",
        "Tianfan Fu",
        "Peng Ye",
        "Chunfeng Song",
        "Nanqing Dong",
        "Yuqiang Li",
        "Huazhu Fu",
        "Siqi Sun",
        "Lijing Cheng",
        "Jintai Lin",
        "Wanli Ouyang",
        "Bowen Zhou",
        "Wenlong Zhang",
        "Lei Bai"
      ],
      "abstract": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç§‘å­¦äººå·¥æ™ºèƒ½é¢†åŸŸç¼ºä¹ç»Ÿä¸€æ¡†æ¶çš„é—®é¢˜ï¼ŒåŸºäºå®è·µæ¢ç©¶æ¨¡å‹(Practical Inquiry Model, PIM)æå‡ºäº†ç§‘å­¦é€šç”¨æ™ºèƒ½(Scientific General Intelligence, SGI)çš„è¿è¡Œå®šä¹‰ã€‚ç ”ç©¶é€šè¿‡æ·±åº¦ç ”ç©¶ã€æ„æ€ç”Ÿæˆã€å¹²/æ¹¿å®éªŒåŠå®éªŒæ¨ç†å››é¡¹ä¸ç§‘å­¦å®¶å·¥ä½œæµå¯¹é½çš„ä»»åŠ¡ï¼Œæ„å»ºäº†åŒ…å«1000å¤šä¸ªè·¨å­¦ç§‘ä¸“å®¶ç­–åˆ’æ ·æœ¬çš„è¯„ä¼°åŸºå‡†SGI-Benchã€‚å®éªŒå‘ç°ï¼Œç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ·±åº¦ç ”ç©¶çš„ç²¾ç¡®åŒ¹é…ç‡ä»…ä¸º10-20%ï¼Œä¸”åœ¨å®éªŒç”Ÿæˆçš„ç»†èŠ‚å¯è¡Œæ€§ã€ä»£ç æ‰§è¡Œç»“æœå‡†ç¡®æ€§åŠå¤šæ¨¡æ€æ¯”è¾ƒæ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ (Test-Time Reinforcement Learning, TTRL)æŠ€æœ¯ï¼Œé€šè¿‡åœ¨æ¨ç†é˜¶æ®µä¼˜åŒ–æ£€ç´¢å¢å¼ºçš„æ–°é¢–æ€§å¥–åŠ±ï¼Œåœ¨æ— éœ€å‚è€ƒç­”æ¡ˆçš„æƒ…å†µä¸‹æ˜¾è‘—å¢å¼ºäº†ç§‘å­¦å‡è®¾çš„æ–°é¢–æ€§ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡ç•Œå®šSGIå†…æ¶µå¹¶æä¾›ç³»ç»ŸåŒ–è¯„ä¼°ï¼Œä¸ºå¼€å‘èƒ½å¤Ÿæ·±åº¦å‚ä¸ç§‘å­¦å‘ç°è¿‡ç¨‹çš„è‡ªä¸»AIç³»ç»Ÿå¥ å®šäº†ç†è®ºä¸å®è¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16969v1",
      "published_date": "2025-12-18 12:44:36 UTC",
      "updated_date": "2025-12-18 12:44:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:03:22.448737+00:00"
    },
    {
      "arxiv_id": "2512.16468v1",
      "title": "Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery",
      "title_zh": "é‡åŒ–ä¸å¼¥åˆä¿çœŸåº¦å·®è·ï¼šä¸€ç§åŸºäºå†³å®šæ€§ç‰¹å¾çš„åˆæˆä¸çœŸå®å›¾åƒå¯¹æ¯”æ–¹æ³•",
      "authors": [
        "Danial Safaei",
        "Siddartha Khastgir",
        "Mohsen Alirezaei",
        "Jeroen Ploeg",
        "Son Tong",
        "Xingyu Zhao"
      ],
      "abstract": "Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images \"look real\" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶æ±½è½¦(AV)å®‰å…¨è¯„ä¼°ä¸­åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®ä¹‹é—´çš„ä¿çœŸåº¦å·®å¼‚é—®é¢˜ï¼ŒæŒ‡å‡ºä»…é åƒç´ çº§çš„è§†è§‰çœŸå®æ„Ÿ(visual realism)æ— æ³•ä¿è¯ç³»ç»Ÿä»ä»¿çœŸç¯å¢ƒåˆ°ç°å®ä¸–ç•Œçš„å¯é è¿ç§»ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDecisive Feature Fidelity (DFF) çš„ç‰¹å®šè¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æœºåˆ¶å¯¹ç­‰æ€§(mechanism parity)ï¼Œå³è¯„ä¼°å—æµ‹ç³»ç»Ÿåœ¨ä¸åŒé¢†åŸŸåšå‡ºå†³ç­–æ—¶æ˜¯å¦åŸºäºç›¸åŒçš„å› æœè¯æ®ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æŠ€æœ¯è¯†åˆ«å¹¶å¯¹æ¯”é©±åŠ¨è¾“å‡ºçš„å…³é”®ç‰¹å¾ï¼Œå¹¶è¿›ä¸€æ­¥æå‡ºäº†åŸºäºåäº‹å®è§£é‡Š(counterfactual explanations)çš„ä¼°è®¡å™¨ä»¥åŠDFFå¼•å¯¼çš„æ ¡å‡†æ–¹æ¡ˆã€‚åœ¨2126å¯¹KITTIå’ŒVirtualKITTI2æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDFFèƒ½å¤Ÿæ­ç¤ºä¼ ç»ŸæŒ‡æ ‡æ‰€å¿½è§†çš„æ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDFFå¼•å¯¼çš„æ ¡å‡†èƒ½åœ¨ä¸æŸå®³è¾“å‡ºä¿çœŸåº¦çš„å‰æä¸‹ï¼Œæœ‰æ•ˆæå‡ä»¿çœŸå™¨çš„å…³é”®ç‰¹å¾å’Œè¾“å…¥å±‚ä¿çœŸåº¦ï¼Œä¸ºAVå®‰å…¨éªŒè¯æä¾›äº†æ›´å¯é çš„è¯„ä¼°æ‰‹æ®µã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16468v1",
      "published_date": "2025-12-18 12:39:13 UTC",
      "updated_date": "2025-12-18 12:39:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:03:23.217922+00:00"
    },
    {
      "arxiv_id": "2512.16465v2",
      "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution",
      "title_zh": "cuPilotï¼šé¢å‘ CUDA æ ¸å‡½æ•°æ¼”åŒ–çš„ç­–ç•¥ååŒå¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Jinwu Chen",
        "Qidie Wu",
        "Bin Li",
        "Lin Ma",
        "Xin Si",
        "Yang Hu",
        "Shouyi Yin",
        "Jun Yang"
      ],
      "abstract": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹CUDAå†…æ ¸ä¼˜åŒ–ä¸­ç¡¬ä»¶-è½¯ä»¶ååŒè®¾è®¡çš„å¤æ‚æ€§ï¼Œä»¥åŠç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸è¿›åŒ–ç®—æ³•ç»“åˆæ—¶åœ¨æ™ºèƒ½ä½“è®¾è®¡å’Œè¿›åŒ–è¡¨å¾ä¸Šçš„å±€é™æ€§ï¼Œæå‡ºäº†cuPilotæ¡†æ¶ã€‚cuPilotæ˜¯ä¸€ä¸ªç­–ç•¥åè°ƒçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ƒå¼•å…¥äº†â€œç­–ç•¥(strategy)â€ä½œä¸ºå†…æ ¸è¿›åŒ–çš„ä¸­é—´è¯­ä¹‰è¡¨ç¤ºï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•ä¸­è¡¨ç¤ºä¸åŒ¹é…çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ç­–ç•¥åè°ƒçš„è¿›åŒ–ç®—æ³•ã€åŸºäºå±‹é¡¶æ¨¡å‹(roofline-guided)çš„æç¤ºæŠ€æœ¯ä»¥åŠç­–ç•¥çº§çš„ç§ç¾¤åˆå§‹åŒ–æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒcuPilotç”Ÿæˆçš„å†…æ ¸åœ¨åŒ…å«100ä¸ªå†…æ ¸çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºPyTorchå®ç°äº†å¹³å‡3.09å€çš„åŠ é€Ÿã€‚åœ¨é€šç”¨çŸ©é˜µä¹˜æ³•(GEMM)ä»»åŠ¡ä¸­ï¼ŒcuPilotå±•ç°äº†æ·±åº¦çš„ä¼˜åŒ–èƒ½åŠ›å¹¶å®ç°äº†å…³é”®ç¡¬ä»¶å•å…ƒçš„é«˜åˆ©ç”¨ç‡ï¼Œä¸ºè‡ªåŠ¨åŒ–çš„é«˜æ€§èƒ½å†…æ ¸æ¼”è¿›æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16465v2",
      "published_date": "2025-12-18 12:34:00 UTC",
      "updated_date": "2025-12-23 07:16:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:03:20.414886+00:00"
    },
    {
      "arxiv_id": "2512.16455v1",
      "title": "AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research",
      "title_zh": "AI4EOSCï¼šé¢å‘ç§‘ç ”äººå·¥æ™ºèƒ½çš„è”é‚¦äº‘å¹³å°",
      "authors": [
        "Ignacio Heredia",
        "Ãlvaro LÃ³pez GarcÃ­a",
        "GermÃ¡n MoltÃ³",
        "Amanda Calatrava",
        "Valentin Kozlov",
        "Alessandro Costantini",
        "Viet Tran",
        "Mario David",
        "Daniel San MartÃ­n",
        "Marcin PÅ‚Ã³ciennik",
        "Marta ObregÃ³n Ruiz",
        "SaÃºl Fernandez",
        "Judith SÃ¡inz-Pardo DÃ­az",
        "Miguel Caballer",
        "Caterina AlarcÃ³n MarÃ­n",
        "Stefan Dlugolinsky",
        "Martin Å eleng",
        "Lisana Berberi",
        "Khadijeh Alibabaei",
        "Borja Esteban Sanchis",
        "Pedro Castro",
        "Giacinto Donvito",
        "Diego Aguirre",
        "Sergio Langarita",
        "Vicente Rodriguez",
        "Leonhard Duda",
        "AndrÃ©s Heredia Canales",
        "Susana Rebolledo Ruiz",
        "JoÃ£o Machado",
        "Giang Nguyen",
        "Fernando Aguilar GÃ³mez",
        "Jaime DÃ­ez"
      ],
      "abstract": "In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† AI4EOSCï¼Œè¿™æ˜¯ä¸€ä¸ªè‡´åŠ›äºæ”¯æŒç§‘å­¦ç ”ç©¶ä¸­ Artificial Intelligence å·¥ä½œè´Ÿè½½çš„è”é‚¦è®¡ç®—å¹³å°ã€‚è¯¥å¹³å°é€šè¿‡æä¾›å¯¹ç‰©ç†åˆ†å¸ƒçš„ e-Infrastructures çš„ä¸€è‡´ä¸”é€æ˜çš„è®¿é—®ï¼Œé‡ç‚¹å…³æ³¨ reproducible deployments çš„å®ç°ã€‚å¹³å°é€šè¿‡ç»¼åˆæœåŠ¡ç›®å½•æ¶µç›–äº†å®Œæ•´çš„ Machine Learning ç”Ÿå‘½å‘¨æœŸï¼ŒåŒ…æ‹¬æ¨¡å‹å¼€å‘ã€åˆ©ç”¨ GPU èµ„æºå’Œ Federated Learning æ”¯æŒè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä»¥åŠåœ¨ Cloud Continuum ä¸Šçš„å¹¿æ³›éƒ¨ç½²ã€‚æ­¤å¤–ï¼ŒAI4EOSC æä¾›äº†ç”¨äº AI æ¨¡å‹å¯è¿½æº¯æ€§å’Œ reproducibility çš„å·¥å…·ï¼Œå¹¶é›†æˆäº†å¤šç§æ¨¡å‹æä¾›å•†ã€æ•°æ®é›†å’Œå­˜å‚¨èµ„æºï¼Œæ–¹ä¾¿ç”¨æˆ·ä¸å¹¿æ³›çš„ç”Ÿæ€ç³»ç»Ÿäº¤äº’ã€‚è¯¥å¹³å°å…·å¤‡é«˜åº¦çš„å¯å®šåˆ¶æ€§ï¼Œæ—¨åœ¨æœ‰æ•ˆé™ä½å¤–éƒ¨ç¤¾åŒºçš„é‡‡ç”¨é—¨æ§›ï¼Œå¹¶ä¸ºç§‘ç ”äººå‘˜æä¾›é›†æˆä¸”ä¸€è‡´çš„å¼€å‘ä½“éªŒã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16455v1",
      "published_date": "2025-12-18 12:20:31 UTC",
      "updated_date": "2025-12-18 12:20:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:03:21.248704+00:00"
    },
    {
      "arxiv_id": "2512.16453v1",
      "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries",
      "title_zh": "TimeSeries2Report æç¤ºæŠ€æœ¯åŠ©åŠ›å¤§è¯­è¨€æ¨¡å‹å®ç°é”‚ç¦»å­ç”µæ± è‡ªé€‚åº”ç®¡ç†",
      "authors": [
        "Jiayang Yang",
        "Chunhui Zhao",
        "Martin Guay",
        "Zhixing Cao"
      ],
      "abstract": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TimeSeries2Report (TS2R)ï¼Œè¿™æ˜¯ä¸€ç§å°†é”‚ç¦»å­ç”µæ± åŸå§‹æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢ä¸ºç»“æ„åŒ–ã€è¯­ä¹‰ä¸°å¯ŒæŠ¥å‘Šçš„æç¤ºæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ Large Language Models (LLMs) è¿›è¡Œç”µæ± å‚¨èƒ½ç³»ç»Ÿ (BESS) çš„ç®¡ç†ä¸ç»´æŠ¤ã€‚TS2R é€šè¿‡ç»“åˆåˆ†æ®µ (segmentation)ã€è¯­ä¹‰æŠ½è±¡ (semantic abstraction) å’ŒåŸºäºè§„åˆ™çš„è§£é‡Šï¼Œæœ‰æ•ˆåœ°å°†åº•å±‚ä¼ æ„Ÿå™¨ä¿¡å·ä¸é«˜å±‚ä¸Šä¸‹æ–‡æ´å¯Ÿæ¡¥æ¥èµ·æ¥ï¼Œä½¿ LLMs èƒ½å¤Ÿè¿›è¡Œæ¨ç†ã€é¢„æµ‹å’Œå†³ç­–ã€‚å®éªŒåœ¨å®éªŒå®¤åŠç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å¼‚å¸¸æ£€æµ‹ (anomaly detection)ã€è·ç”µçŠ¶æ€ (state-of-charge) é¢„æµ‹ä»¥åŠå……æ”¾ç”µç®¡ç†ç­‰å…³é”®ä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼Œç›¸æ¯”äºåŸºäºè§†è§‰ã€åµŒå…¥æˆ–çº¯æ–‡æœ¬çš„æç¤ºåŸºçº¿ï¼ŒTS2R åœ¨å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚åœ¨æ— éœ€é‡æ–°è®­ç»ƒæˆ–ä¿®æ”¹æ¨¡å‹æ¶æ„çš„æƒ…å†µä¸‹ï¼Œé›†æˆ TS2R çš„ LLMs è¾¾åˆ°äº†ä¸“å®¶çº§çš„å†³ç­–è´¨é‡å’Œé¢„æµ‹ä¸€è‡´æ€§ï¼Œä¸ºå®ç°è‡ªé€‚åº”çš„ç”µæ± æ™ºèƒ½ç®¡ç†å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16453v1",
      "published_date": "2025-12-18 12:15:52 UTC",
      "updated_date": "2025-12-18 12:15:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:03:40.570066+00:00"
    },
    {
      "arxiv_id": "2512.16448v1",
      "title": "IoMT-based Automated Leukemia Classification using CNN and Higher Order Singular Value",
      "title_zh": "åŸºäº IoMT çš„å·ç§¯ç¥ç»ç½‘ç»œä¸é«˜é˜¶å¥‡å¼‚å€¼è‡ªåŠ¨ç™½è¡€ç—…åˆ†ç±»",
      "authors": [
        "Shabnam Bagheri Marzijarani",
        "Mohammad Zolfaghari",
        "Hedieh Sajedi"
      ],
      "abstract": "The Internet of Things (IoT) is a concept by which objects find identity and can communicate with each other in a network. One of the applications of the IoT is in the field of medicine, which is called the Internet of Medical Things (IoMT). Acute Lymphocytic Leukemia (ALL) is a type of cancer categorized as a hematic disease. It usually begins in the bone marrow due to the overproduction of immature White Blood Cells (WBCs or leukocytes). Since it has a high rate of spread to other body organs, it is a fatal disease if not diagnosed and treated early. Therefore, for identifying cancerous (ALL) cells in medical diagnostic laboratories, blood, as well as bone marrow smears, are taken by pathologists. However, manual examinations face limitations due to human error risk and time-consuming procedures. So, to tackle the mentioned issues, methods based on Artificial Intelligence (AI), capable of identifying cancer from non-cancer tissue, seem vital. Deep Neural Networks (DNNs) are the most efficient machine learning (ML) methods. These techniques employ multiple layers to extract higher-level features from the raw input. In this paper, a Convolutional Neural Network (CNN) is applied along with a new type of classifier, Higher Order Singular Value Decomposition (HOSVD), to categorize ALL and normal (healthy) cells from microscopic blood images. We employed the model on IoMT structure to identify leukemia quickly and safely. With the help of this new leukemia classification framework, patients and clinicians can have real-time communication. The model was implemented on the Acute Lymphoblastic Leukemia Image Database (ALL-IDB2) and achieved an average accuracy of %98.88 in the test step.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ€¥æ€§æ·‹å·´ç»†èƒç™½è¡€ç—… (Acute Lymphocytic Leukemia, ALL) æ—©æœŸè¯Šæ–­ä¸­äººå·¥è¯„ä¼°è€—æ—¶ä¸”æ˜“å‡ºé”™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŒ»ç–—ç‰©è”ç½‘ (Internet of Medical Things, IoMT) çš„è‡ªåŠ¨åŒ–åˆ†ç±»æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Network, CNN) ä»æ˜¾å¾®è¡€ç»†èƒå›¾åƒä¸­æå–é«˜å±‚ç‰¹å¾ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹åˆ†ç±»å™¨â€”â€”é«˜é˜¶å¥‡å¼‚å€¼åˆ†è§£ (Higher Order Singular Value Decomposition, HOSVD)ï¼Œç”¨äºå¯¹ ALL ç»†èƒå’Œæ­£å¸¸ç»†èƒè¿›è¡Œç²¾å‡†åŒºåˆ†ã€‚é€šè¿‡å°†è¯¥æ¨¡å‹é›†æˆåˆ° IoMT æ¶æ„ä¸­ï¼Œå®ç°äº†æ‚£è€…ä¸ä¸´åºŠåŒ»ç”Ÿä¹‹é—´çš„å®æ—¶é€šä¿¡ï¼Œæ˜¾è‘—æå‡äº†ç™½è¡€ç—…è¯†åˆ«çš„æ•ˆç‡ä¸å®‰å…¨æ€§ã€‚å®éªŒåœ¨ ALL-IDB2 æ•°æ®åº“ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨æµ‹è¯•é˜¶æ®µè¾¾åˆ°äº† 98.88% çš„å¹³å‡å‡†ç¡®ç‡ï¼Œä¸ºé«˜æ•ˆã€å¯é çš„æ™ºæ…§åŒ»ç–—è¾…åŠ©è¯Šæ–­æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16448v1",
      "published_date": "2025-12-18 12:09:45 UTC",
      "updated_date": "2025-12-18 12:09:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:03:39.375389+00:00"
    },
    {
      "arxiv_id": "2512.16447v1",
      "title": "Towards AI-Supported Research: a Vision of the TIB AIssistant",
      "title_zh": "è¿ˆå‘äººå·¥æ™ºèƒ½è¾…åŠ©ç§‘ç ”ï¼šTIB AIssistant æ„¿æ™¯",
      "authors": [
        "SÃ¶ren Auer",
        "Allard Oelen",
        "Mohamad Yaser Jaradeh",
        "Mutahira Khalid",
        "Farhana Keya",
        "Sasi Kiran Gaddipati",
        "Jennifer D'Souza",
        "Lorenz SchlÃ¼ter",
        "Amirreza Alasti",
        "Gollam Rabby",
        "Azanzi Jiomekong",
        "Oliver Karras"
      ],
      "abstract": "The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TIB AIssistant çš„æ„¿æ™¯ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ”¯æŒè·¨å­¦ç§‘ç§‘å­¦å‘ç°çš„é€šç”¨å‹äººæœºåä½œå¹³å°ã€‚é’ˆå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) å’Œå¤§è¯­è¨€æ¨¡å‹ (Large Language Models) åœ¨é›†æˆåˆ°ç ”ç©¶æµç¨‹æ—¶é¢ä¸´çš„é¢†åŸŸéœ€æ±‚å·®å¼‚ã€AI ç´ å…»æœ‰é™ä»¥åŠå·¥å…·åè°ƒå¤æ‚ç­‰æŒ‘æˆ˜ï¼Œè¯¥å¹³å°æä¾›äº†ç³»ç»ŸåŒ–çš„è§£å†³æ–¹æ¡ˆã€‚TIB AIssistant é‡‡ç”¨äº†æ¨¡å—åŒ–è®¾è®¡ï¼Œæ ¸å¿ƒç»„ä»¶åŒ…æ‹¬æç¤ºè¯ (prompt) å’Œå·¥å…·åº“ã€å…±äº«æ•°æ®å­˜å‚¨ä»¥åŠçµæ´»çš„ç¼–æ’æ¡†æ¶ (orchestration framework)ã€‚è¯¥å¹³å°èƒ½å¤Ÿè¾…åŠ©ç ”ç©¶äººå‘˜å®Œæˆä»æ„æ€ã€æ–‡çŒ®åˆ†æã€æ–¹æ³•è®ºå¼€å‘åˆ°æ•°æ®åˆ†æå’Œå­¦æœ¯å†™ä½œçš„æ•´ä¸ªç ”ç©¶ç”Ÿå‘½å‘¨æœŸä»»åŠ¡ã€‚é€šè¿‡æè¿°å…¶æ¦‚å¿µæ¡†æ¶ã€ç³»ç»Ÿæ¶æ„ä»¥åŠæ—©æœŸåŸå‹çš„å®ç°ï¼Œè¯¥ç ”ç©¶éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¢å¼ºå­¦æœ¯å·¥ä½œæµå’Œä¿ƒè¿›ç§‘å­¦å‘ç°æ–¹é¢çš„å¯è¡Œæ€§ä¸æ½œåœ¨å½±å“ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16447v1",
      "published_date": "2025-12-18 12:08:46 UTC",
      "updated_date": "2025-12-18 12:08:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:04:53.646882+00:00"
    },
    {
      "arxiv_id": "2512.16446v1",
      "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion",
      "title_zh": "E-SDSï¼šç¯å¢ƒæ„ŸçŸ¥å‹ See it, Do it, Sortedâ€”â€”ç”¨äºäººå½¢æœºå™¨äººè¿åŠ¨çš„è‡ªåŠ¨åŒ–ç¯å¢ƒæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Enis Yalcin",
        "Joshua O'Hara",
        "Maria Stamatopoulou",
        "Chengxu Zhou",
        "Dimitrios Kanoulas"
      ],
      "abstract": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†E-SDS (Environment-aware See it, Do it, Sorted)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨äººå½¢æœºå™¨äººè¿åŠ¨å¥–åŠ±è®¾è®¡ä¸­å› ç¼ºä¹ç¯å¢ƒæ„ŸçŸ¥è€Œæ— æ³•åº”å¯¹å¤æ‚åœ°å½¢çš„é—®é¢˜ã€‚E-SDSé€šè¿‡å°†VLMsä¸å®æ—¶åœ°å½¢ä¼ æ„Ÿå™¨åˆ†æç›¸ç»“åˆï¼Œå¹¶ä»¥ç¤ºä¾‹è§†é¢‘ä¸ºå‚è€ƒï¼Œå®ç°äº†å¥–åŠ±å‡½æ•°çš„è‡ªåŠ¨åŒ–ç”Ÿæˆï¼Œä»è€Œè®­ç»ƒå‡ºå…·å¤‡ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›çš„é²æ£’è¿åŠ¨ç­–ç•¥ã€‚åœ¨Unitree G1äººå½¢æœºå™¨äººä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åŒ…å«æ¥¼æ¢¯ã€éšœç¢ç‰©å’Œæ²Ÿå£‘çš„å¤šç§å¤æ‚åœ°å½¢ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¯å”¯ä¸€æˆåŠŸå®ç°ä¸‹æ¥¼æ¢¯ä»»åŠ¡çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒE-SDSå°†é€Ÿåº¦è·Ÿè¸ªè¯¯å·®(velocity tracking error)é™ä½äº†51.9-82.6%ï¼Œå¹¶å°†å¥–åŠ±è®¾è®¡çš„äººå·¥æˆæœ¬ä»æ•°å¤©ç¼©çŸ­è‡³ä¸¤å°æ—¶ä»¥å†…ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººè¿åŠ¨ç­–ç•¥çš„å¼€å‘æ•ˆç‡ä¸æ€§èƒ½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "12 pages, 3 figures, 4 tables. Accepted at RiTA 2025 (Springer LNNS)",
      "pdf_url": "https://arxiv.org/pdf/2512.16446v1",
      "published_date": "2025-12-18 12:08:24 UTC",
      "updated_date": "2025-12-18 12:08:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:03:49.100531+00:00"
    },
    {
      "arxiv_id": "2512.16445v1",
      "title": "Topic Modelling Black Box Optimization",
      "title_zh": "ä¸»é¢˜å»ºæ¨¡é»‘ç›’ä¼˜åŒ–",
      "authors": [
        "Roman Akramov",
        "Artem Khamatullin",
        "Svetlana Glazyrina",
        "Maksim Kryzhanovskiy",
        "Roman Ischenko"
      ],
      "abstract": "Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ Latent Dirichlet Allocation (LDA) æ¨¡å‹ä¸­é€‰æ‹©ä¸»é¢˜æ•°é‡ $T$ çš„å…³é”®å†³ç­–é—®é¢˜ï¼Œå¹¶å°†å…¶å…¬å¼åŒ–ä¸ºä¸€ä¸ªç¦»æ•£çš„é»‘ç›’ä¼˜åŒ– (black-box optimization) é—®é¢˜ã€‚ä½œè€…åœ¨å›ºå®šè¯„ä¼°é¢„ç®—ä¸‹ï¼Œå¯¹æ¯”äº†ä¼ ç»Ÿçš„è¿›åŒ–ç®—æ³• Genetic Algorithm (GA) å’Œ Evolution Strategy (ES)ï¼Œä»¥åŠä¸¤ç§ç»è¿‡å­¦ä¹ çš„æ‘Šé”€ä¼˜åŒ–æ–¹æ³• Preferential Amortized Black-Box Optimization (PABBO) å’Œ Sharpness-Aware Black-Box Optimization (SABBO)ã€‚å®éªŒé€šè¿‡è¡¡é‡éªŒè¯é›†å›°æƒ‘åº¦ (validation perplexity) å‘ç°ï¼Œè™½ç„¶æ‰€æœ‰ä¼˜åŒ–å™¨æœ€ç»ˆéƒ½èƒ½è¾¾åˆ°ç›¸ä¼¼çš„æ€§èƒ½æ°´å¹³ï¼Œä½†æ‘Šé”€ä¼˜åŒ–å™¨åœ¨æ ·æœ¬å’Œæ—¶é—´æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºè¿›åŒ–ç®—æ³•ã€‚ç‰¹åˆ«æ˜¯ SABBO é€šå¸¸åªéœ€å•æ¬¡è¯„ä¼°å³å¯ç¡®å®šè¿‘ä¹æœ€ä¼˜çš„ä¸»é¢˜æ•°é‡ï¼Œè€Œ PABBO ä¹Ÿèƒ½åœ¨æå°‘æ•°è¯„ä¼°ä¸­æ‰¾åˆ°å…·æœ‰ç«äº‰åŠ›çš„é…ç½®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGA å’Œ ES éœ€è¦æ¶ˆè€—å‡ ä¹å…¨éƒ¨é¢„ç®—æ‰èƒ½æ¥è¿‘ç›¸åŒçš„æ€§èƒ½åŒºåŸŸï¼Œå‡¸æ˜¾äº†æ‘Šé”€é»‘ç›’ä¼˜åŒ–åœ¨æå‡ä¸»é¢˜æ¨¡å‹è°ƒå‚æ•ˆç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16445v1",
      "published_date": "2025-12-18 12:00:24 UTC",
      "updated_date": "2025-12-18 12:00:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:05:33.292904+00:00"
    },
    {
      "arxiv_id": "2512.16444v1",
      "title": "StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm",
      "title_zh": "StarCraft+ï¼šå¯¹æŠ—èŒƒå¼ä¸‹çš„å¤šæ™ºèƒ½ä½“ç®—æ³•åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yadong Li",
        "Tong Zhang",
        "Bo Huang",
        "Zhen Cui"
      ],
      "abstract": "Deep multi-agent reinforcement learning (MARL) algorithms are booming in the field of collaborative intelligence, and StarCraft multi-agent challenge (SMAC) is widely-used as the benchmark therein. However, imaginary opponents of MARL algorithms are practically configured and controlled in a fixed built-in AI mode, which causes less diversity and versatility in algorithm evaluation. To address this issue, in this work, we establish a multi-agent algorithm-vs-algorithm environment, named StarCraft II battle arena (SC2BA), to refresh the benchmarking of MARL algorithms in an adversary paradigm. Taking StarCraft as infrastructure, the SC2BA environment is specifically created for inter-algorithm adversary with the consideration of fairness, usability and customizability, and meantime an adversarial PyMARL (APyMARL) library is developed with easy-to-use interfaces/modules. Grounding in SC2BA, we benchmark those classic MARL algorithms in two types of adversarial modes: dual-algorithm paired adversary and multi-algorithm mixed adversary, where the former conducts the adversary of pairwise algorithms while the latter focuses on the adversary to multiple behaviors from a group of algorithms. The extensive benchmark experiments exhibit some thought-provoking observations/problems in the effectivity, sensibility and scalability of these completed algorithms. The SC2BA environment as well as reproduced experiments are released in \\href{https://github.com/dooliu/SC2BA}{Github}, and we believe that this work could mark a new step for the MARL field in the coming years.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹StarCraft multi-agent challenge (SMAC) ä¸­å¯¹æ‰‹è¡Œä¸ºå›ºå®šå¯¼è‡´è¯„ä¼°ç¼ºä¹å¤šæ ·æ€§çš„å±€é™ï¼Œæå‡ºäº†åä¸ºStarCraft II battle arena (SC2BA) çš„å¤šæ™ºèƒ½ä½“ç®—æ³•å¯¹æˆ˜ç¯å¢ƒã€‚è¯¥å·¥ä½œå°†åŸºå‡†æµ‹è¯•ä»ä¼ ç»Ÿçš„åä½œä»»åŠ¡æ‰©å±•åˆ°å¯¹æŠ—èŒƒå¼ (adversary paradigm)ï¼Œå¹¶å¼€å‘äº†é…å¥—çš„Adversarial PyMARL (APyMARL) åº“ä»¥æä¾›æ˜“ç”¨çš„æ¨¡å—åŒ–æ¥å£ã€‚é€šè¿‡åŒç®—æ³•é…å¯¹å¯¹æŠ— (dual-algorithm paired adversary) å’Œå¤šç®—æ³•æ··åˆå¯¹æŠ— (multi-algorithm mixed adversary) æ¨¡å¼ï¼Œç ”ç©¶è€…å¯¹å¤šç§ç»å…¸MARLç®—æ³•è¿›è¡Œäº†æ·±å…¥æµ‹è¯„ã€‚å®éªŒç»“æœæ­ç¤ºäº†ç°æœ‰ç®—æ³•åœ¨æœ‰æ•ˆæ€§ (effectivity)ã€æ•æ„Ÿæ€§ (sensibility) å’Œæ‰©å±•æ€§ (scalability) æ–¹é¢å­˜åœ¨çš„ä¸€ç³»åˆ—æ–°é—®é¢˜ã€‚è¿™é¡¹å·¥ä½œä¸ºDeep multi-agent reinforcement learning (MARL) é¢†åŸŸæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§ä¸”å¯å®šåˆ¶çš„è¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16444v1",
      "published_date": "2025-12-18 11:58:10 UTC",
      "updated_date": "2025-12-18 11:58:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:05:32.502297+00:00"
    },
    {
      "arxiv_id": "2512.16442v1",
      "title": "TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles",
      "title_zh": "TIB AIssistantï¼šè´¯ç©¿ç§‘ç ”å…¨ç”Ÿå‘½å‘¨æœŸçš„ AI è¾…åŠ©ç§‘ç ”å¹³å°",
      "authors": [
        "Allard Oelen",
        "SÃ¶ren Auer"
      ],
      "abstract": "The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº† TIB AIssistantï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸ºæ•´ä¸ªç ”ç©¶ç”Ÿå‘½å‘¨æœŸ (Research Life Cycles) æä¾›å…¨é¢æ”¯æŒçš„äººå·¥æ™ºèƒ½è¾…åŠ©ç ”ç©¶å¹³å°ã€‚è¯¥å¹³å°ç”±ä¸€ç³»åˆ—ä¸“é—¨è´Ÿè´£ç‰¹å®šç ”ç©¶ä»»åŠ¡çš„åŠ©æ‰‹ (Assistants) ç»„æˆï¼Œå¹¶é›†æˆäº†è®¿é—®å¤–éƒ¨å­¦æœ¯æœåŠ¡çš„å·¥å…·ä»¥å¢å¼ºåŠŸèƒ½ã€‚ç”Ÿæˆçš„æ•°æ®ä¼šè¢«ç³»ç»Ÿè®°å½•ï¼Œå¹¶æ”¯æŒå¯¼å‡ºä¸º RO-Crate æ†ç»‘åŒ…ï¼Œä»è€Œæ˜¾è‘—æå‡ç ”ç©¶é¡¹ç›®çš„é€æ˜åº¦ä¸å¯é‡å¤æ€§ (Reproducibility)ã€‚é€šè¿‡æ¼”ç¤ºå¤šä¸ªåŠ©æ‰‹ååŒå·¥ä½œçš„åºåˆ—åŒ–æµç¨‹ï¼Œè¯¥å¹³å°è¯æ˜äº†å…¶èƒ½å¤Ÿæœ‰æ•ˆè¾…åŠ©ç”Ÿæˆç ”ç©¶è®ºæ–‡è‰æ¡ˆã€‚æ€»ä½“è€Œè¨€ï¼ŒTIB AIssistant åˆ©ç”¨ Large Language Models (LLMs) ä¸ºæ„å»ºç¤¾åŒºé©±åŠ¨çš„ AI è¾…åŠ©ç ”ç©¶ç”Ÿæ€å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16442v1",
      "published_date": "2025-12-18 11:54:38 UTC",
      "updated_date": "2025-12-18 11:54:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:05:35.991143+00:00"
    },
    {
      "arxiv_id": "2512.16433v1",
      "title": "Emergent Bias and Fairness in Multi-Agent Decision Systems",
      "title_zh": "å¤šæ™ºèƒ½ä½“å†³ç­–ç³»ç»Ÿä¸­çš„æ¶Œç°åè§ä¸å…¬å¹³æ€§",
      "authors": [
        "Maeve Madigan",
        "Parameswaran Kamalaruban",
        "Glenn Moynihan",
        "Tom Kempton",
        "David Sutton",
        "Stuart Burrell"
      ],
      "abstract": "Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-Agent Systems)åœ¨é‡‘èå†³ç­–ä¸­çš„å…¬å¹³æ€§è¯„ä¼°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡ç”¨è¯„åˆ†å’Œæ”¶å…¥ä¼°ç®—ç­‰é«˜é£é™©é¢†åŸŸã€‚ä¸ºäº†è§£å†³ç¼ºä¹æœ‰æ•ˆè¯„ä¼°æ–¹æ³•å¯¼è‡´çš„åè§é£é™©ï¼Œç ”ç©¶äººå‘˜é’ˆå¯¹å¤šæ™ºèƒ½ä½“é¢„æµ‹ç³»ç»Ÿå¼€å‘äº†å…¬å¹³æ€§è¯„ä¼°æ–¹æ³•ï¼Œå¹¶åœ¨é‡‘èè¡¨æ ¼é¢†åŸŸå¼€å±•äº†å¤§è§„æ¨¡æ¨¡æ‹Ÿå®éªŒã€‚é€šè¿‡åˆ†æä¸åŒçš„é€šä¿¡å’Œåä½œæœºåˆ¶ï¼Œç ”ç©¶å‘ç°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å†³ç­–ä¸­ä¼šäº§ç”Ÿæ— æ³•è¿½æº¯è‡³å•ä¸ªç»„ä»¶çš„æ¶Œç°åè§(Emergent Bias)ï¼Œè¯æ˜äº†ç³»ç»Ÿå…·æœ‰çœŸæ­£çš„é›†ä½“è¡Œä¸ºç‰¹å¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡‘èé¢†åŸŸå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å…¬å¹³æ€§é£é™©æ˜¯æ¨¡å‹é£é™©çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œç›´æ¥å½±å“å†³ç­–çš„å®‰å…¨æ€§å’Œåˆè§„æ€§ã€‚å› æ­¤ï¼Œè¯¥ç ”ç©¶æå€¡å¿…é¡»å°†å¤šæ™ºèƒ½ä½“å†³ç­–ç³»ç»Ÿä½œä¸ºæ•´ä½“è¿›è¡Œè¯„ä¼°ï¼Œè€Œéå¯¹å…¶æ„æˆç»„ä»¶è¿›è¡Œç®€å•çš„è¿˜åŸè®ºåˆ†æã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16433v1",
      "published_date": "2025-12-18 11:37:32 UTC",
      "updated_date": "2025-12-18 11:37:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:05:32.986373+00:00"
    },
    {
      "arxiv_id": "2512.16425v1",
      "title": "Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach",
      "title_zh": "ORKG ASKï¼šä¸€ç§é‡‡ç”¨ç¥ç»ç¬¦å·æ–¹æ³•çš„äººå·¥æ™ºèƒ½é©±åŠ¨å­¦æœ¯æ–‡çŒ®æ£€ç´¢ä¸æ¢ç´¢ç³»ç»Ÿ",
      "authors": [
        "Allard Oelen",
        "Mohamad Yaser Jaradeh",
        "SÃ¶ren Auer"
      ],
      "abstract": "As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† ORKG ASK (Assistant for Scientific Knowledge)ï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³å­¦æœ¯æ–‡çŒ®é‡æ¿€å¢å¯¼è‡´æ£€ç´¢å›°éš¾çš„ AI é©±åŠ¨å‹å­¦æœ¯æ–‡çŒ®æœç´¢ä¸æ¢ç´¢ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ç¥ç»ç¬¦å· (neuro-symbolic) æ–¹æ³•ï¼Œç»“åˆäº†å‘é‡æœç´¢ (vector search)ã€å¤§è¯­è¨€æ¨¡å‹ (LLMs) å’ŒçŸ¥è¯†å›¾è°± (knowledge graphs) çš„ä¼˜åŠ¿ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›ä¸»åŠ¨æ”¯æŒã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€è¾“å…¥ç ”ç©¶é—®é¢˜å¹¶æ£€ç´¢ç›¸å…³æ–‡ç« ï¼Œç³»ç»Ÿåˆ™åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) æŠ€æœ¯è‡ªåŠ¨æå–å…³é”®ä¿¡æ¯å¹¶é’ˆå¯¹é—®é¢˜ç”Ÿæˆå›ç­”ã€‚é€šè¿‡å¯¹æ˜“ç”¨æ€§å’Œå®ç”¨æ€§çš„è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ ORKG ASK ç³»ç»Ÿç•Œé¢å‹å¥½ï¼Œç”¨æˆ·æ»¡æ„åº¦è¾ƒé«˜ã€‚è¯¥ç³»ç»Ÿçš„æå‡ºä¸ºåˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é«˜æ•ˆæ¢ç´¢å’Œå‘ç°å­¦æœ¯èµ„æºæä¾›äº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16425v1",
      "published_date": "2025-12-18 11:25:14 UTC",
      "updated_date": "2025-12-18 11:25:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:05:34.541773+00:00"
    },
    {
      "arxiv_id": "2512.16424v1",
      "title": "Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs",
      "title_zh": "Syntheliteï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åŒ–å­¦å®¶å¯¹é½ä¸”å…¼é¡¾å¯è¡Œæ€§çš„åˆæˆè§„åˆ’",
      "authors": [
        "Nguyen Xuan-Vu",
        "Daniel Armstrong",
        "Milena Wehrbach",
        "Andres M Bran",
        "Zlatko JonÄev",
        "Philippe Schwaller"
      ],
      "abstract": "Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Synthelite æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è®¡ç®—æœºè¾…åŠ©åˆæˆè§„åˆ’ (Computer-aided synthesis planning, CASP) é•¿æœŸä»¥æ¥ç¼ºä¹ä¸äººç±»ä¸“å®¶äº¤äº’æœºåˆ¶çš„å±€é™æ€§ã€‚Synthelite åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å†…åœ¨çš„åŒ–å­¦çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ç›´æ¥æè®®é€†åˆæˆè½¬åŒ– (retrosynthetic transformations) æ¥ç”Ÿæˆç«¯åˆ°ç«¯çš„åˆæˆè·¯å¾„ã€‚è¯¥æ¡†æ¶å…è®¸ä¸“å®¶é€šè¿‡è‡ªç„¶è¯­è¨€æç¤º (natural language prompts) è¿›è¡Œå¹²é¢„ï¼Œä½¿å…¶èƒ½å¤Ÿçµæ´»é€‚åº”å¤šç§ç”¨æˆ·æŒ‡å®šçš„å¤æ‚çº¦æŸæ¡ä»¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSynthelite åœ¨ç­–ç•¥çº¦æŸå’Œèµ·å§‹åŸæ–™çº¦æŸçš„åˆæˆä»»åŠ¡ä¸­å‡å®ç°äº†é«˜è¾¾ 95% çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨è·¯å¾„è®¾è®¡ä¸­è¡¨ç°å‡ºå¯¹åŒ–å­¦å¯è¡Œæ€§ (chemical feasibility) çš„é«˜åº¦æ„ŸçŸ¥ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æä¾›äº†ä¸€ä¸ªå®ç”¨çš„åˆæˆè§„åˆ’å·¥å…·ï¼Œä¹Ÿä¸º LLMs æˆä¸ºåŒ–å­¦åˆæˆè§„åˆ’æ ¸å¿ƒç¼–æ’è€…çš„èŒƒå¼æ¼”è¿›å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16424v1",
      "published_date": "2025-12-18 11:24:30 UTC",
      "updated_date": "2025-12-18 11:24:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:06:05.808351+00:00"
    },
    {
      "arxiv_id": "2512.16406v1",
      "title": "Hypernetworks That Evolve Themselves",
      "title_zh": "è‡ªæˆ‘è¿›åŒ–çš„è¶…ç½‘ç»œ",
      "authors": [
        "Joachim Winther Pedersen",
        "Erwan Plantec",
        "Eleni Nisioti",
        "Marcello Barylli",
        "Milton Montero",
        "Kathrin Korte",
        "Sebastian Risi"
      ],
      "abstract": "How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Self-Referential Graph HyperNetworks (Self-Referential GHNs)ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€å¤–éƒ¨ä¼˜åŒ–å™¨å³å¯å®ç°è‡ªæˆ‘æ¼”åŒ–çš„ç¥ç»ç½‘ç»œç³»ç»Ÿã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆHypernetworksã€Stochastic parameter generationå’ŒGraph-based representationsï¼Œå°†å˜å¼‚å’Œç»§æ‰¿æœºåˆ¶ç›´æ¥åµŒå…¥åˆ°ç½‘ç»œå†…éƒ¨ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªä¸»åœ°è¿›è¡Œçªå˜ä¸è¯„ä¼°ã€‚Self-Referential GHNså°†å˜å¼‚ç‡è§†ä¸ºå¯é€‰æ‹©æ€§çŠ¶è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œå¹¶åœ¨CartPoleSwitchå’ŒLunarLander-Switchç­‰å¼ºåŒ–å­¦ä¹ (Reinforcement learning)åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¿«é€Ÿä¸”å¯é çš„é€‚åº”èƒ½åŠ›ã€‚åœ¨Ant-v5è¡Œèµ°åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥ç³»ç»ŸæˆåŠŸæ¼”åŒ–å‡ºè¿è´¯çš„æ­¥æ€ï¼Œå¹¶èƒ½é€šè¿‡è‡ªä¸»å‡å°‘ç§ç¾¤å˜å¼‚æ¥å®ç°å¯¹ä¼˜é€‰æ–¹æ¡ˆçš„å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¯è¿›åŒ–æ€§(Evolvability)å¯ä»¥é€šè¿‡ç¥ç»ç½‘ç»œçš„è‡ªæˆ‘å¼•ç”¨(Self-reference)è€Œäº§ç”Ÿã€‚è¿™ä¸€æˆæœä¸ºå¼€å‘æ›´æ¥è¿‘ç”Ÿç‰©è¿›åŒ–çš„è‡ªä¸»ã€å¼€æ”¾å¼å­¦ä¹ æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16406v1",
      "published_date": "2025-12-18 11:05:34 UTC",
      "updated_date": "2025-12-18 11:05:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:05:52.256556+00:00"
    },
    {
      "arxiv_id": "2512.16397v1",
      "title": "Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture",
      "title_zh": "åˆ©ç”¨é«˜æ–¯æ³¼æº…æ„å»ºé«˜ä¿çœŸé¢éƒ¨å‡ ä½•ä¸çº¹ç†",
      "authors": [
        "Haodi He",
        "Jihun Yu",
        "Ronald Fedkiw"
      ],
      "abstract": "We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨Gaussian Splattingä»å°‘é‡æœªæ ‡å®šå›¾åƒä¸­é‡æ„é«˜ä¿çœŸé¢éƒ¨å‡ ä½•ä¸çº¹ç†çš„æ–¹æ³•ã€‚ç›¸æ¯”NeRFï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨è¯­ä¹‰åˆ†å‰²å¯¹é½é¢éƒ¨åŒºåŸŸï¼Œä»…éœ€11å¼ é™æ€å›¾åƒå³å¯å®ç°ä¸­æ€§å§¿æ€çš„ç²¾ç¡®é‡æ„ã€‚é€šè¿‡å°†Gaussiansè½¯çº¦æŸåœ¨åº•å±‚ä¸‰è§’åŒ–è¡¨é¢ä¸Šï¼Œè¯¥æ–¹æ³•ä¸ä»…å¢å¼ºäº†é‡æ„çš„ç»“æ„æ€§ï¼Œè¿˜æ˜¾è‘—æå‡äº†åº•å±‚å‡ ä½•çš„ç²¾åº¦ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå°†Gaussian Splatsè½¬åŒ–ä¸ºè§†ç‚¹ç›¸å…³çš„Neural Textureï¼Œä½¿å…¶æ— éœ€ä¿®æ”¹ç°æœ‰æ¶æ„å³å¯æ— ç¼é›†æˆåˆ°æ ‡å‡†å›¾å½¢ç®¡çº¿ä¸­ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨Relightable Gaussianæ¨¡å‹å®ç°äº†çº¹ç†ä¸å…‰ç…§çš„æœ‰æ•ˆè§£è€¦ï¼Œä»è€Œè·å¾—é«˜è´¨é‡çš„Albedoè´´å›¾ã€‚å®éªŒè¯æ˜è¯¥ç³»ç»Ÿå¯¹å…‰ç…§ä¸ä¸€è‡´çš„å¼‚æºå›¾åƒå…·æœ‰é²æ£’æ€§ï¼Œå¹¶åœ¨æ–‡æœ¬é©±åŠ¨çš„èµ„äº§ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to CVPR 2026. 21 pages, 22 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16397v1",
      "published_date": "2025-12-18 10:53:51 UTC",
      "updated_date": "2025-12-18 10:53:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:06:10.560565+00:00"
    },
    {
      "arxiv_id": "2512.16392v1",
      "title": "PCIA: A Path Construction Imitation Algorithm for Global Optimization",
      "title_zh": "PCIAï¼šé¢å‘å…¨å±€ä¼˜åŒ–çš„è·¯å¾„æ„å»ºæ¨¡ä»¿ç®—æ³•",
      "authors": [
        "Mohammad-Javad Rezaei",
        "Mozafar Bag-Mohammadi"
      ],
      "abstract": "In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºPCIA (Path Construction Imitation Algorithm) çš„æ–°å‹å…ƒå¯å‘å¼ä¼˜åŒ–ç®—æ³• (metaheuristic optimization algorithm)ï¼Œä¸“é—¨ç”¨äºè§£å†³å…¨å±€ä¼˜åŒ–é—®é¢˜ã€‚è¯¥ç®—æ³•çš„çµæ„Ÿæºè‡ªäººç±»æ„å»ºå’Œä½¿ç”¨æ–°è·¯å¾„çš„è¡Œä¸ºæ¨¡å¼ï¼Œæ¨¡æ‹Ÿäº†äººç±»åœ¨åå¥½æµè¡Œè·¯çº¿ã€æ™ºèƒ½æ··åˆç°æœ‰è·¯å¾„åº”å¯¹å°è·¯ä»¥åŠéšæœºé€‰æ‹©è·¯å¾„æ¢ç´¢æœªçŸ¥ç›®çš„åœ°æ—¶çš„å†³ç­–é€»è¾‘ã€‚PCIA é‡‡ç”¨äº†ç±»ä¼¼äºç¾¤æ™ºèƒ½ç®—æ³• (swarm-based algorithms) çš„ç­–ç•¥ï¼Œé€šè¿‡ç”Ÿæˆéšæœºç§ç¾¤æ¥å¯»æ‰¾é€šå¾€ç›®æ ‡çš„æœ€ä¼˜è·¯å¾„ï¼Œå…¶ä¸­æ¯ä¸€ä¸ªç²’å­éƒ½ä»£è¡¨ä¸€æ¡é€šå¾€ç›®çš„åœ°çš„æ½œåœ¨è·¯å¾„ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨53ä¸ªæ•°å­¦ä¼˜åŒ–é—®é¢˜å’Œ13ä¸ªå—çº¦æŸä¼˜åŒ–é—®é¢˜ä¸Šå¯¹è¯¥ç®—æ³•è¿›è¡Œäº†å‹åŠ›æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCIA ä¸å½“å‰æµè¡ŒåŠæœ€æ–°çš„å…ƒå¯å‘å¼ç®—æ³•ç›¸æ¯”å…·æœ‰æå¼ºçš„ç«äº‰åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚ä¼˜åŒ–ä»»åŠ¡æ—¶çš„é«˜æ•ˆæ€§ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16392v1",
      "published_date": "2025-12-18 10:39:43 UTC",
      "updated_date": "2025-12-18 10:39:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:06:10.305327+00:00"
    },
    {
      "arxiv_id": "2512.16391v1",
      "title": "Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference",
      "title_zh": "Kascadeï¼šé•¿ä¸Šä¸‹æ–‡ LLM æ¨ç†çš„å®ç”¨ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•",
      "authors": [
        "Dhruv Deshmukh",
        "Saurabh Goyal",
        "Nipun Kwatra",
        "Ramachandran Ramjee"
      ],
      "abstract": "Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Kascadeï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„ç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attentionï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é™ä½é•¿ä¸Šä¸‹æ–‡ï¼ˆLong-Contextï¼‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„å»¶è¿Ÿã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†å Softmax æ³¨æ„åŠ›çš„å†…åœ¨ç¨€ç–æ€§ä»¥åŠé«˜æƒé‡ Key æ ‡è¯†åœ¨ç›¸é‚»å±‚é—´çš„ç¨³å®šæ€§ï¼Œé€šè¿‡åŠ¨æ€è§„åˆ’ï¼ˆDynamic-programmingï¼‰ç®—æ³•åœ¨é”šå±‚ï¼ˆAnchor layersï¼‰ä¸­è®¡ç®—å¹¶è·¨å±‚å¤ç”¨ Top-k ç´¢å¼•ã€‚Kascade å¼•å…¥äº†å¤šå¤´æ„ŸçŸ¥ï¼ˆHead-awareï¼‰çš„é€‰æ‹©æœºåˆ¶ä¸é«˜æ•ˆçš„å—çº§ï¼ˆTile-levelï¼‰æ“ä½œï¼Œèƒ½å¤ŸåŒæ—¶ä¼˜åŒ–é¢„å¡«å……ï¼ˆPrefillï¼‰ä¸è§£ç ï¼ˆDecodeï¼‰é˜¶æ®µçš„è®¡ç®—æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ H100 GPU ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸æ¯” FlashAttention-3 åŸºçº¿åˆ†åˆ«åœ¨è§£ç å’Œé¢„å¡«å……é˜¶æ®µå®ç°äº†é«˜è¾¾ 4.1 å€å’Œ 2.2 å€çš„æé€Ÿã€‚æ­¤å¤–ï¼ŒKascade åœ¨ LongBench å’Œ AIME-24 ç­‰é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ä¸ç¨ å¯†æ³¨æ„åŠ›ï¼ˆDense Attentionï¼‰å‡ ä¹ä¸€è‡´çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…æ¨ç†åœºæ™¯ä¸­çš„å®ç”¨æ€§ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 8 figures, 3 tables and 1 algorithm",
      "pdf_url": "https://arxiv.org/pdf/2512.16391v1",
      "published_date": "2025-12-18 10:37:14 UTC",
      "updated_date": "2025-12-18 10:37:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:06:22.528309+00:00"
    },
    {
      "arxiv_id": "2512.16378v2",
      "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
      "title_zh": "å¬éŸ³è¯†è¯‘ï¼šå°†è¯­éŸ³æ¨¡æ€é›†æˆè‡³å¤§è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ€§åˆ†æ",
      "authors": [
        "Sara Papi",
        "Javier Garcia Gilabert",
        "Zachary Hopton",
        "VilÃ©m Zouhar",
        "Carlos Escolano",
        "Gerard I. GÃ¡llego",
        "Jorge Iranzo-SÃ¡nchez",
        "Ahrii Kim",
        "Dominik MachÃ¡Äek",
        "Patricia Schmidtova",
        "Maike ZÃ¼fle"
      ],
      "abstract": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä¸­æ•´åˆè¯­éŸ³æ¨¡æ€ï¼ˆå³ SpeechLLMsï¼‰å¯¹è¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ (Speech-to-Text Translation) è´¨é‡çš„å½±å“ã€‚ç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†åä¸º Hearing to Translate çš„ç»¼åˆæµ‹è¯•å¥—ä»¶ï¼Œå¯¹ 5 ä¸ªæœ€å…ˆè¿›çš„ SpeechLLMs ä»¥åŠ 16 ä¸ªç”±è¯­éŸ³åŸºç¡€æ¨¡å‹ (SFM) ä¸å¤šè¯­è¨€ LLMs ç»„æˆçš„ä¸²è” (Cascade) æˆ–ç›´æ¥ç³»ç»Ÿè¿›è¡Œäº†ä¸¥è°¨çš„åŸºå‡†æµ‹è¯•ã€‚è¯„ä¼°æ¶µç›–äº† 16 ä¸ªåŸºå‡†æµ‹è¯•ã€13 ç§è¯­è¨€å¯¹ä»¥åŠåŒ…æ‹¬ä¸æµåˆ©ã€å™ªå£°å’Œé•¿è¯­éŸ³åœ¨å†…çš„ 9 ç§æŒ‘æˆ˜æ€§æ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸²è”ç³»ç»Ÿç›®å‰ä»ç„¶æ˜¯æœ€å¯é çš„é€‰æ‹©ï¼Œç°æœ‰çš„ SpeechLLMs ä»…åœ¨ç‰¹å®šè®¾ç½®ä¸‹èƒ½ä¸ä¸²è”ç³»ç»ŸæŒå¹³ï¼Œè€Œå•ç‹¬çš„ SFMs è¡¨ç°åˆ™æ˜¾è‘—è½åã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†æ— è®ºæ˜¯é€šè¿‡æ¨¡å‹å†…éƒ¨é›†æˆè¿˜æ˜¯æµæ°´çº¿ç»„åˆï¼Œå¼•å…¥ LLM å¯¹äºå®ç°é«˜è´¨é‡è¯­éŸ³ç¿»è¯‘è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "comment": "Project available at https://github.com/sarapapi/hearing2translate",
      "pdf_url": "https://arxiv.org/pdf/2512.16378v2",
      "published_date": "2025-12-18 10:21:14 UTC",
      "updated_date": "2025-12-24 14:39:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:06:40.492784+00:00"
    },
    {
      "arxiv_id": "2512.16349v1",
      "title": "Collaborative Edge-to-Server Inference for Vision-Language Models",
      "title_zh": "é¢å‘è§†è§‰è¯­è¨€æ¨¡å‹çš„ååŒè¾¹ç«¯-æœåŠ¡å™¨æ¨ç†",
      "authors": [
        "Soochang Song",
        "Yongjune Kim"
      ],
      "abstract": "We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Models, VLMsï¼‰çš„åä½œå¼è¾¹ç¼˜åˆ°æœåŠ¡å™¨ï¼ˆEdge-to-Serverï¼‰æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨é™ä½é€šä¿¡æˆæœ¬çš„åŒæ—¶ä¿æŒæ¨ç†å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®¾è®¡ï¼Œç¬¬ä¸€é˜¶æ®µç”±æœåŠ¡å™¨å¯¹å…¨å±€å›¾åƒè¿›è¡Œæ¨ç†ï¼Œåˆ©ç”¨VLMçš„å†…éƒ¨æ³¨æ„åŠ›æœºåˆ¶è¯†åˆ«æ„Ÿå…´è¶£åŒºåŸŸï¼ˆRegion of Interest, RoIï¼‰ï¼Œå¹¶é€šè¿‡è®¡ç®—è¾“å‡ºTokençš„æœ€å°ç†µï¼ˆmin-entropyï¼‰ä½œä¸ºç½®ä¿¡åº¦æŒ‡æ ‡ã€‚è‹¥æœ€å°ç†µè¶…è¿‡é¢„è®¾é˜ˆå€¼ï¼ŒæœåŠ¡å™¨å°†è¯·æ±‚è¾¹ç¼˜è®¾å¤‡å‘é€ä¿ç•™ç»†èŠ‚çš„RoIå±€éƒ¨å›¾åƒï¼Œè¿›è€Œé€šè¿‡è”åˆåˆ©ç”¨å…¨å±€å’Œå±€éƒ¨å›¾åƒæ¥ç²¾ç‚¼æ¨ç†ç»“æœã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§é€‰æ‹©æ€§é‡ä¼ ç­–ç•¥åœ¨å¤šç§VLMæ¶æ„ä¸‹å‡èƒ½æ˜¾è‘—å‡å°‘é€šä¿¡å¼€é”€ï¼Œå¹¶æœ‰æ•ˆè§£å†³äº†å› å›¾åƒç¼©æ”¾å¯¼è‡´çš„ç»†ç²’åº¦ç»†èŠ‚ä¸¢å¤±é—®é¢˜ï¼Œç¡®ä¿äº†é«˜æ•ˆä¸”å‡†ç¡®çš„ç«¯åˆ°ç«¯æ¨ç†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16349v1",
      "published_date": "2025-12-18 09:38:18 UTC",
      "updated_date": "2025-12-18 09:38:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:06:58.206409+00:00"
    },
    {
      "arxiv_id": "2512.16344v1",
      "title": "AI Needs Physics More Than Physics Needs AI",
      "title_zh": "äººå·¥æ™ºèƒ½å¯¹ç‰©ç†å­¦çš„éœ€æ±‚ç”šäºç‰©ç†å­¦å¯¹äººå·¥æ™ºèƒ½çš„éœ€æ±‚",
      "authors": [
        "Peter Coveney",
        "Roger Highfield"
      ],
      "abstract": "Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ (AI) ä¸ç‰©ç†å­¦ (Physics) ä¹‹é—´çš„å…±ç”Ÿå…³ç³»ï¼ŒæŒ‡å‡ºç›¸è¾ƒäº AI å¯¹ç‰©ç†å­¦çš„å½“å‰è´¡çŒ®ï¼Œç‰©ç†å­¦åŸç†å¯¹å…‹æœå½“ä»£ AI çš„æŠ€æœ¯ç“¶é¢ˆå…·æœ‰æ›´å…³é”®çš„ä½œç”¨ã€‚æ–‡ç« æ‰¹åˆ¤äº†ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹ (Large Language Models)ã€æ¨ç†æ¨¡å‹å’Œæ™ºèƒ½ä½“ AI (Agentic AI) åœ¨å‚æ•°æ„ä¹‰ã€åˆ†å¸ƒåå·®ã€ä¸ç¡®å®šæ€§é‡åŒ–ä»¥åŠæ•æ‰ç§‘å­¦å®šå¾‹æ–¹é¢çš„æ ¸å¿ƒå±€é™ã€‚é€šè¿‡åˆ†æè¿™äº›æŒ‘æˆ˜ï¼Œä½œè€…è¯†åˆ«äº†é‡å­ AI (Quantum AI) å’Œæ¨¡æ‹Ÿè®¡ç®— (Analogue Computing) å¸¦æ¥çš„æ½œåœ¨æœºé‡ã€‚ç ”ç©¶æœ€ç»ˆæå‡ºäº†â€œå¤§äººå·¥æ™ºèƒ½â€ (Big AI) çš„å‘å±•è·¯çº¿å›¾ï¼Œä¸»å¼ å°†åŸºäºç†è®ºçš„ä¸¥è°¨æ€§ä¸æœºå™¨å­¦ä¹  (Machine Learning) çš„çµæ´»æ€§æ·±åº¦èåˆã€‚è¯¥è®ºè¿°æ—¨åœ¨é€šè¿‡å¼•å…¥ç‰©ç†å­¦çš„æœºåˆ¶æ€§æ´å¯Ÿï¼Œä¸ºæ„å»ºæ›´å…·è§£é‡ŠåŠ›å’Œç§‘å­¦åŸºç¡€çš„æ–°ä¸€ä»£äººå·¥æ™ºèƒ½å¥ å®šç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16344v1",
      "published_date": "2025-12-18 09:31:05 UTC",
      "updated_date": "2025-12-18 09:31:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:06:42.947413+00:00"
    },
    {
      "arxiv_id": "2512.16334v3",
      "title": "Pretrained Battery Transformer (PBT): A battery life prediction foundation model",
      "title_zh": "é¢„è®­ç»ƒç”µæ±  Transformer (PBT)ï¼šç”µæ± å¯¿å‘½é¢„æµ‹åŸºç¡€æ¨¡å‹",
      "authors": [
        "Ruifeng Tan",
        "Weixiang Hong",
        "Jia Li",
        "Jiaqiang Huang",
        "Tong-Yi Zhang"
      ],
      "abstract": "Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Pretrained Battery Transformer (PBT)ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ç”µæ± å¾ªç¯å¯¿å‘½é¢„æµ‹çš„åŸºåº§æ¨¡å‹ (Foundation Model)ï¼Œæ—¨åœ¨è§£å†³å› æ•°æ®ç¨€ç¼ºå’Œå¼‚è´¨æ€§å¯¼è‡´çš„ç ”ç©¶ç“¶é¢ˆã€‚PBT é€šè¿‡é¢†åŸŸçŸ¥è¯†ç¼–ç çš„æ··åˆä¸“å®¶å±‚ (Mixture-of-Expert layers) æ„å»ºè€Œæˆï¼Œèƒ½å¤Ÿä»å¤šæ ·åŒ–çš„è€åŒ–æ¡ä»¶ä¸­å­¦ä¹ å¯è¿ç§»çš„ç‰¹å¾è¡¨ç¤ºã€‚è¯¥æ¨¡å‹åœ¨åŒ…å« 13 ä¸ªé”‚ç¦»å­ç”µæ±  (Lithium-ion battery) æ•°æ®é›†çš„å¤§å‹æ•°æ®åº“ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶åœ¨ 15 ä¸ªæ¶µç›–ä¸åŒè¿è¡Œæ¡ä»¶ã€åŒ–æˆå·¥è‰ºå’ŒåŒ–å­¦ä½“ç³»çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPBT çš„è¡¨ç°å¹³å‡ä¼˜äºç°æœ‰æ¨¡å‹ 19.8%ï¼Œå±•ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œè¿ç§»å­¦ä¹  (Transfer Learning) æ€§èƒ½ã€‚è¯¥å·¥ä½œä¸ºç”µæ± å¯¿å‘½é¢„æµ‹å»ºç«‹äº†åŸºåº§æ¨¡å‹è·¯å¾„ï¼Œä¸ºå¼€å‘é€šç”¨çš„ç”µæ± å¯¿å‘½é¢„æµ‹ç³»ç»Ÿæä¾›äº†å¯èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 figures in the main content",
      "pdf_url": "https://arxiv.org/pdf/2512.16334v3",
      "published_date": "2025-12-18 09:17:45 UTC",
      "updated_date": "2025-12-23 02:58:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:07:12.295601+00:00"
    },
    {
      "arxiv_id": "2512.16317v1",
      "title": "Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference",
      "title_zh": "å»ä¸­å¿ƒåŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­æˆæœ¬æ„ŸçŸ¥å‹ PoQ çš„è®¾è®¡ä¸è¯„ä¼°",
      "authors": [
        "Arther Tian",
        "Alex Ding",
        "Frank Chen",
        "Alan Wu",
        "Aaron Chan",
        "Bruce Zhang"
      ],
      "abstract": "Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.\n  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Cost-Aware PoQæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å»ä¸­å¿ƒåŒ–å¤§è¯­è¨€æ¨¡å‹(Decentralized LLM Inference)ä¸­ç°æœ‰éªŒè¯æ–¹æ³•å¿½ç•¥è®¡ç®—æˆæœ¬å·®å¼‚çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨å¥–åŠ±æœºåˆ¶ä¸­æ•´åˆæ˜¾å¼çš„æ•ˆç‡åº¦é‡ï¼Œç»“åˆTokençº§åˆ«çš„F1åˆ†æ•°ã€è½»é‡çº§è¯„ä¼°å™¨(Learned Evaluators)å’ŒGPTåˆ¤æ–­ï¼Œæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è´¨é‡ä¸æˆæœ¬è¯„ä¼°æµæ°´çº¿ã€‚é’ˆå¯¹TinyLlama-1.1Båˆ°Llama-3.2-3Bç­‰å¤šç§æ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼Œè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦(Semantic Textual Similarity)çš„åŒç¼–ç å™¨(Bi-encoder)åœ¨è¯„ä¼°ä¸€è‡´æ€§ä¸Šæ˜¾è‘—ä¼˜äºäº¤å‰ç¼–ç å™¨(Cross-encoder)ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å•ä½å»¶è¿Ÿçš„è´¨é‡äº§å‡ºæ–¹é¢ï¼Œæ¨¡å‹æ± ä¸­è¾ƒå¤§çš„æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡ã€‚é€šè¿‡5000è½®è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ(Monte Carlo Simulations)è¯å®ï¼Œè¯¥æˆæœ¬æ„ŸçŸ¥å¥–åŠ±æ–¹æ¡ˆèƒ½æŒç»­å¥–åŠ±é«˜æ•ˆé«˜è´¨é‡èŠ‚ç‚¹å¹¶æƒ©ç½šä½æ•ˆä½è´¨èŠ‚ç‚¹ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºæ„å»ºç»æµå¯æŒç»­çš„å»ä¸­å¿ƒåŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¥ å®šäº†å®è·µåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16317v1",
      "published_date": "2025-12-18 08:57:17 UTC",
      "updated_date": "2025-12-18 08:57:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:06:58.470944+00:00"
    },
    {
      "arxiv_id": "2512.16310v1",
      "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
      "title_zh": "æ™ºèƒ½ä½“å·¥å…·ç¼–æ’å¸¦æ¥æ›´å¤šæ³„éœ²ï¼šæ•°æ®é›†ã€åŸºå‡†ä¸ç¼“è§£æ–¹æ¡ˆ",
      "authors": [
        "Yuxuan Qiao",
        "Dongqin Liu",
        "Hongchang Yang",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "abstract": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„å•æ™ºèƒ½ä½“å¤šå·¥å…·æ¶æ„ä¸­å­˜åœ¨çš„æ–°å‹éšç§é£é™©ï¼Œå³å·¥å…·ç¼–æ’éšç§é£é™©(Tools Orchestration Privacy Risk, TOP-R)ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¯¥é£é™©çš„æ ¹æºåœ¨äºæ™ºèƒ½ä½“åœ¨è¿½æ±‚ç”¨æˆ·ç›®æ ‡æ—¶ï¼Œå› è¿‡åº¦ä¼˜åŒ–å¸®åŠ©æ€§è€Œå¿½ç•¥äº†éšç§æ„è¯†ï¼Œä»è€Œé€šè¿‡æ¨ç†èƒ½åŠ›å°†è·¨å·¥å…·çš„é›¶æ•£ä¿¡æ¯åˆæˆä¸ºéé¢„æœŸçš„æ•æ„Ÿä¿¡æ¯ã€‚ä¸ºäº†ç³»ç»Ÿç ”ç©¶è¿™ä¸€é£é™©ï¼Œä½œè€…å»ºç«‹äº†ä¸€ä¸ªå½¢å¼åŒ–æ¡†æ¶å¹¶æ„å»ºäº†åŒ…å«æ³„éœ²ä¸è‰¯æ€§åœºæ™¯çš„è¯„æµ‹åŸºå‡†TOP-Benchï¼ŒåŒæ—¶å¼•å…¥äº†H-ScoreæŒ‡æ ‡æ¥é‡åŒ–å®‰å…¨æ€§ä¸é²æ£’æ€§ä¹‹é—´çš„æƒè¡¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºTOP-Ré£é™©æå…¶ä¸¥é‡ï¼Œä»£è¡¨æ€§æ¨¡å‹çš„å¹³å‡é£é™©æ³„éœ²ç‡(Risk Leakage Rate, RLR)é«˜è¾¾90.24%ï¼Œä¸”H-Scoreè¡¨ç°æ™®éè¾ƒä½ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†éšç§å¢å¼ºåŸåˆ™(Privacy Enhancement Principle, PEP)æ–¹æ³•ï¼ŒæˆåŠŸå°†é£é™©æ³„éœ²ç‡é™è‡³46.58%å¹¶æ˜¾è‘—æå‡äº†H-Scoreã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ­ç¤ºäº†å½“å‰æ™ºèƒ½ä½“æ¶æ„ä¸­å›ºæœ‰çš„ç»“æ„æ€§é™åˆ¶ï¼Œä¹Ÿä¸ºç¼“è§£æ­¤ç±»æ–°å‹éšç§é£é™©æä¾›äº†å¯è¡Œçš„ç­–ç•¥ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16310v1",
      "published_date": "2025-12-18 08:50:57 UTC",
      "updated_date": "2025-12-18 08:50:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:06:56.947593+00:00"
    },
    {
      "arxiv_id": "2512.16307v1",
      "title": "Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks",
      "title_zh": "è¶…è¶ŠåŸºå‡†ï¼šé’ˆå¯¹æç¤ºè¯æ³¨å…¥æ”»å‡»çš„åˆ›æ–°é˜²å¾¡",
      "authors": [
        "Safwan Shaheer",
        "G. M. Refatul Islam",
        "Mohammad Rafid Hamid",
        "Tahsin Zaman Jilan"
      ],
      "abstract": "In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ—¥ç›Šä¸¥é‡çš„Prompt Injectionæ”»å‡»é£é™©ï¼Œé‡ç‚¹æ¢è®¨äº†LLaMAç³»åˆ—ç­‰å°å‹å¼€æºæ¨¡å‹çš„å®‰å…¨é˜²å¾¡é—®é¢˜ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°é˜²å¾¡æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨Chain of Thoughts (CoT)ä½œä¸ºç§å­é˜²å¾¡ï¼ˆseed defenseï¼‰æ¥è¿­ä»£ä¼˜åŒ–é˜²å¾¡æç¤ºï¼ˆdefense promptsï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿæ€§åœ°è¯„ä¼°ç”Ÿæˆçš„é˜²å¾¡æœºåˆ¶ï¼Œé’ˆå¯¹åŸºå‡†æ”»å‡»æµ‹è¯•æ˜¾è‘—æå‡äº†æ¨¡å‹ç¼“è§£Goal-Hijackingæ¼æ´çš„èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç­–ç•¥åœ¨å¤§å¹…é™ä½æ”»å‡»æˆåŠŸç‡å’Œè¯¯æŠ¥ç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹å¹¶æ‹¦æˆªæ¶æ„æŒ‡ä»¤ã€‚è¿™é¡¹å·¥ä½œä¸ºå°å‹å¼€æºLLMsåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ï¼ˆedge devicesï¼‰ä¸Šçš„å®‰å…¨é«˜æ•ˆéƒ¨ç½²å¥ å®šäº†åŸºç¡€ï¼Œæœ‰åŠ›æ¨åŠ¨äº†å¼€æºå¤§æ¨¡å‹ç”Ÿæ€ç³»ç»Ÿçš„å®‰å…¨æ€§å»ºè®¾ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16307v1",
      "published_date": "2025-12-18 08:47:07 UTC",
      "updated_date": "2025-12-18 08:47:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:07:13.542833+00:00"
    },
    {
      "arxiv_id": "2512.17958v1",
      "title": "Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization",
      "title_zh": "åŸºäº RGB å§¿æ€ä¸æƒ…æ„Ÿçº¿ç´¢åŠè·¨ç›¸æœºæ¨¡å‹æ³›åŒ–çš„å®æ—¶äººæœºäº¤äº’æ„å›¾æ£€æµ‹",
      "authors": [
        "Farida Mohsen",
        "Ali Safa"
      ],
      "abstract": "Service robots in public spaces require real-time understanding of human behavioral intentions for natural interaction. We present a practical multimodal framework for frame-accurate human-robot interaction intent detection that fuses camera-invariant 2D skeletal pose and facial emotion features extracted from monocular RGB video. Unlike prior methods requiring RGB-D sensors or GPU acceleration, our approach resource-constrained embedded hardware (Raspberry Pi 5, CPU-only). To address the severe class imbalance in natural human-robot interaction datasets, we introduce a novel approach to synthesize temporally coherent pose-emotion-label sequences for data re-balancing called MINT-RVAE (Multimodal Recurrent Variational Autoencoder for Intent Sequence Generation). Comprehensive offline evaluations under cross-subject and cross-scene protocols demonstrate strong generalization performance, achieving frame- and sequence-level AUROC of 0.95. Crucially, we validate real-world generalization through cross-camera evaluation on the MIRA robot head, which employs a different onboard RGB sensor and operates in uncontrolled environments not represented in the training data. Despite this domain shift, the deployed system achieves 91% accuracy and 100% recall across 32 live interaction trials. The close correspondence between offline and deployed performance confirms the cross-sensor and cross-environment robustness of the proposed multimodal approach, highlighting its suitability for ubiquitous multimedia-enabled social robots.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºå®æ—¶äººæœºäº¤äº’(Human-Robot Interaction)æ„å›¾æ£€æµ‹çš„å®ç”¨å¤šæ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡èåˆå•ç›®RGBè§†é¢‘ä¸­çš„ç›¸æœºæ— å…³2Déª¨æ¶å§¿æ€(2D skeletal pose)å’Œé¢éƒ¨è¡¨æƒ…ç‰¹å¾æ¥å®ç°ç²¾å‡†è¯†åˆ«ã€‚ä¸ºè§£å†³è‡ªç„¶äº¤äº’æ•°æ®é›†ä¸­ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç ”ç©¶è€…å¼•å…¥äº†MINT-RVAE(Multimodal Recurrent Variational Autoencoder for Intent Sequence Generation)æ–¹æ³•æ¥åˆæˆæ—¶é—´ç›¸å¹²çš„ç‰¹å¾åºåˆ—ã€‚è¯¥ç³»ç»Ÿå…·æœ‰æé«˜çš„è®¡ç®—æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€GPUåŠ é€Ÿçš„åµŒå…¥å¼ç¡¬ä»¶ï¼ˆå¦‚Raspberry Pi 5ï¼‰ä¸Šå®ç°å¸§çº§å‡†ç¡®çš„å®æ—¶è¿è¡Œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç¦»çº¿è¯„ä¼°ä¸­è¾¾åˆ°äº†0.95çš„AUROCï¼Œå¹¶åœ¨è·¨ç›¸æœºã€è·¨ç¯å¢ƒçš„æœºå™¨äººå®å¢ƒæµ‹è¯•ä¸­å–å¾—äº†91%çš„å‡†ç¡®ç‡å’Œ100%çš„å¬å›ç‡ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†æ‰€æå¤šæ¨¡æ€æ–¹æ¡ˆåœ¨å¤„ç†é¢†åŸŸæ¼‚ç§»æ—¶çš„å¼ºå¤§é²æ£’æ€§ï¼Œä¸ºèµ„æºå—é™çš„ç¤¾äº¤æœºå™¨äººæä¾›äº†å¯è½åœ°çš„å®æ—¶æ„ŸçŸ¥æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.17958v1",
      "published_date": "2025-12-18 08:44:22 UTC",
      "updated_date": "2025-12-18 08:44:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:07:13.676397+00:00"
    },
    {
      "arxiv_id": "2512.16303v2",
      "title": "PixelArena: A benchmark for Pixel-Precision Visual Intelligence",
      "title_zh": "PixelArenaï¼šåƒç´ çº§ç²¾åº¦è§†è§‰æ™ºèƒ½è¯„æµ‹åŸºå‡†",
      "authors": [
        "Feng Liang",
        "Sizhe Cheng",
        "Chenqi Yi",
        "Yong Wang"
      ],
      "abstract": "Omni-modal models that have multimodal input and output are emerging. However, benchmarking their multimodal generation, especially in image generation, is challenging due to the subtleties of human preferences and model biases. Many image generation benchmarks focus on aesthetics instead of the fine-grained generation capabilities of these models, failing to evaluate their visual intelligence with objective metrics. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. With our benchmark and experiments, we find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to dataset development, omni-modal model development, and the design of metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨æ¨¡æ€æ¨¡å‹(Omni-modal models)å›¾åƒç”Ÿæˆè¯„ä¼°ä¸»è¦å…³æ³¨å®¡ç¾è€Œç¼ºä¹å®¢è§‚ã€ç»†ç²’åº¦è§†è§‰æ™ºèƒ½åº¦é‡çš„é—®é¢˜ï¼Œæå‡ºäº†PixelArenaåŸºå‡†ã€‚PixelArenaé€šè¿‡è¯­ä¹‰åˆ†å‰²ä»»åŠ¡(semantic segmentation tasks)åœ¨åƒç´ ç²¾åº¦(pixel precision)å±‚é¢ä¸Šå®¢è§‚æ£€éªŒæ¨¡å‹çš„ç»†ç²’åº¦ç”Ÿæˆæ™ºèƒ½ã€‚å®éªŒå‘ç°ï¼Œæœ€æ–°çš„Gemini 3 Pro Imageå…·å¤‡æ¶Œç°çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨é›¶æ ·æœ¬(zero-shot)è®¾ç½®ä¸‹ç”Ÿæˆé«˜ä¿çœŸåº¦çš„è¯­ä¹‰æ©ç (semantic masks)ã€‚è¿™å±•ç¤ºäº†å‰æ‰€æœªæœ‰çš„è§†è§‰æ™ºèƒ½ä»¥åŠåœ¨å…¨æ–°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„çœŸå®æ³›åŒ–èƒ½åŠ›(true generalization)ã€‚ç ”ç©¶è¿›ä¸€æ­¥é€šè¿‡å®šæ€§å’Œå®šé‡åˆ†æå¯¹æ¯”äº†ä¸åŒæ¨¡å‹ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å¤±æ•ˆæ¡ˆä¾‹(failure cases)ã€‚è¯¥ç ”ç©¶ç»“æœä¸ä»…æ ‡å¿—ç€é¢†åŸŸçš„é‡å¤§è¿›å±•ï¼Œè¿˜ä¸ºæœªæ¥æ•°æ®é›†å¼€å‘ã€å…¨æ¨¡æ€æ¨¡å‹æ„å»ºåŠè¯„ä¼°æŒ‡æ ‡è®¾è®¡æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 11 figures, project page: https://pixelarena.reify.ing/project",
      "pdf_url": "https://arxiv.org/pdf/2512.16303v2",
      "published_date": "2025-12-18 08:41:27 UTC",
      "updated_date": "2026-01-09 10:55:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:07:19.375580+00:00"
    },
    {
      "arxiv_id": "2512.16301v2",
      "title": "Adaptation of Agentic AI",
      "title_zh": "ä»£ç†å¼ AI çš„é€‚é…",
      "authors": [
        "Pengcheng Jiang",
        "Jiacheng Lin",
        "Zhiyi Shi",
        "Zifeng Wang",
        "Luxi He",
        "Yichen Wu",
        "Ming Zhong",
        "Peiyang Song",
        "Qizheng Zhang",
        "Heng Wang",
        "Xueqiang Xu",
        "Hanwen Xu",
        "Pengrui Han",
        "Dylan Zhang",
        "Jiashuo Sun",
        "Chaoqi Yang",
        "Kun Qian",
        "Tian Wang",
        "Changran Hu",
        "Manling Li",
        "Quanzheng Li",
        "Hao Peng",
        "Sheng Wang",
        "Jingbo Shang",
        "Chao Zhang",
        "Jiaxuan You",
        "Liyuan Liu",
        "Pan Lu",
        "Yu Zhang",
        "Heng Ji",
        "Yejin Choi",
        "Dawn Song",
        "Jimeng Sun",
        "Jiawei Han"
      ],
      "abstract": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Agentic AI ç³»ç»Ÿä¸­çš„ Adaptation æœºåˆ¶ï¼Œå°†å…¶è§†ä¸ºæå‡æ¨¡å‹åœ¨è§„åˆ’ã€æ¨ç†åŠå¤–éƒ¨å·¥å…·äº¤äº’ä¸­è¡¨ç°ã€å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„æ ¸å¿ƒã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç³»ç»Ÿæ€§æ¡†æ¶ï¼Œæ¶µç›–äº† Agent Adaptations å’Œ Tool Adaptations ä¸¤ä¸ªä¸»è¦ç»´åº¦ï¼Œå¹¶è¿›ä¸€æ­¥ç»†åŒ–ä¸º Tool-execution-signaledã€Agent-output-signaled ä»¥åŠ Agent-agnostic å’Œ Agent-supervised ç­‰å…·ä½“å½¢å¼ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜ç¡®ä¸åŒç­–ç•¥çš„è®¾è®¡ç©ºé—´ä¸æƒè¡¡ï¼ˆTrade-offsï¼‰ï¼Œä¸ºç³»ç»Ÿè®¾è®¡è¿‡ç¨‹ä¸­é€‚é…ç­–ç•¥çš„é€‰æ‹©æˆ–åˆ‡æ¢æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚é€šè¿‡è¯„è¿°ä»£è¡¨æ€§æ–¹æ³•å¹¶åˆ†æå…¶ä¼˜ç¼ºç‚¹ï¼Œè®ºæ–‡æ­ç¤ºäº†è¯¥é¢†åŸŸé¢ä¸´çš„å…¬å¼€æŒ‘æˆ˜ä¸æœªæ¥æœºé‡ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´å¼ºå¤§ã€é«˜æ•ˆä¸”å¯é çš„ Agentic AI ç³»ç»Ÿæä¾›äº†é‡è¦çš„æ¦‚å¿µåŸºç¡€å’Œå®è·µè·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16301v2",
      "published_date": "2025-12-18 08:38:51 UTC",
      "updated_date": "2025-12-22 11:05:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:07:35.641556+00:00"
    },
    {
      "arxiv_id": "2512.16300v1",
      "title": "Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection",
      "title_zh": "ä»£ç åœ¨ç¯å–è¯ï¼šé¢å‘å›¾åƒä¼ªé€ æ£€æµ‹çš„æ™ºèƒ½ä½“åŒ–å·¥å…·è°ƒç”¨",
      "authors": [
        "Fanrui Zhang",
        "Qiang Zhang",
        "Sizhuo Zhou",
        "Jianwen Sun",
        "Chuanhao Li",
        "Jiaxin Ai",
        "Yukang Feng",
        "Yujie Zhang",
        "Wenjie Li",
        "Zizhen Li",
        "Yifan Chang",
        "Jiawei Liu",
        "Kaipeng Zhang"
      ],
      "abstract": "Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å›¾åƒä¼ªé€ æ£€æµ‹(Image Forgery Detection, IFD)ä¸­ä½çº§ä¼ªå½±ä¸é«˜çº§è¯­ä¹‰çŸ¥è¯†éš¾ä»¥æœ‰æ•ˆç»Ÿä¸€çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºForenAgentçš„å¤šè½®äº¤äº’å¼æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†Code-in-the-Loopå–è¯æ¨¡å¼ï¼Œä½¿å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)èƒ½å¤Ÿå›´ç»•æ£€æµ‹ç›®æ ‡è‡ªä¸»ç”Ÿæˆã€æ‰§è¡Œå¹¶è¿­ä»£ä¼˜åŒ–åŸºäºPythonçš„åº•å±‚å·¥å…·ã€‚é€šè¿‡ç»“åˆCold Startå’Œå¼ºåŒ–å¾®è°ƒ(Reinforcement Fine-Tuning)çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼ŒForenAgentæ˜¾è‘—æå‡äº†å…¶å·¥å…·äº¤äº’çš„é€‚åº”æ€§ä¸æ¨ç†ç²¾åº¦ã€‚ç ”ç©¶å›¢é˜Ÿå—äººç±»æ€ç»´å¯å‘è®¾è®¡äº†åŒ…å«å…¨å±€æ„ŸçŸ¥ã€å±€éƒ¨èšç„¦ã€è¿­ä»£æ¢æµ‹å’Œæ•´ä½“è£å†³çš„åŠ¨æ€æ¨ç†ç¯è·¯ï¼Œå¹¶åŒæ­¥æ„å»ºäº†æ‹¥æœ‰10ä¸‡å¼ å›¾åƒçš„å¤§è§„æ¨¡å¼‚æ„æ•°æ®é›†FABenchã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åº•å±‚å·¥å…·çš„è¾…åŠ©ä¸‹ï¼ŒForenAgentåœ¨å¤æ‚IFDä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„å·¥å…·è°ƒç”¨èƒ½åŠ›ä¸åæ€æ€§æ¨ç†ï¼Œä¸ºé€šç”¨å›¾åƒå–è¯æŠ€æœ¯æ¢ç´¢å‡ºä¸€æ¡æå…·æ½œåŠ›çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16300v1",
      "published_date": "2025-12-18 08:38:44 UTC",
      "updated_date": "2025-12-18 08:38:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:07:54.920499+00:00"
    },
    {
      "arxiv_id": "2512.16962v1",
      "title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval",
      "title_zh": "MemoryGraftï¼šåŸºäºä¸­æ¯’ç»éªŒæ£€ç´¢çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æŒä¹…æ€§æ”»é™·",
      "authors": [
        "Saksham Sahai Srivastava",
        "Haoyu He"
      ],
      "abstract": "Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MemoryGraftï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“çš„æ–°å‹é—´æ¥æ³¨å…¥æ”»å‡»æ–¹å¼ï¼Œæ—¨åœ¨æ­ç¤ºé•¿æ•ˆè®°å¿†å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æœºåˆ¶ä¸­å­˜åœ¨çš„ä¿¡ä»»è¾¹ç•Œæ¼æ´ã€‚ä¸ä¼ ç»Ÿçš„ç¬æ€æç¤ºæ³¨å…¥æˆ–é’ˆå¯¹äº‹å®çŸ¥è¯†çš„RAGæŠ•æ¯’ä¸åŒï¼ŒMemoryGrafté€šè¿‡åœ¨æ™ºèƒ½ä½“çš„é•¿æœŸè®°å¿†ä¸­æ¤å…¥æ¶æ„çš„â€œæˆåŠŸç»éªŒâ€æ¥å®ç°æŒç»­æ€§å±å®³ã€‚è¯¥æ”»å‡»åˆ©ç”¨äº†æ™ºèƒ½ä½“çš„è¯­ä¹‰æ¨¡ä»¿å¯å‘å¼(semantic imitation heuristic)ï¼Œå³æ™ºèƒ½ä½“å€¾å‘äºä»æ£€ç´¢åˆ°çš„æˆåŠŸä»»åŠ¡ä¸­å¤åˆ¶è¡Œä¸ºæ¨¡å¼ã€‚æ”»å‡»è€…é€šè¿‡åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­æä¾›çœ‹ä¼¼æ— å®³çš„è¾“å…¥ï¼Œè¯±å¯¼æ™ºèƒ½ä½“æ„å»ºåŒ…å«æ¶æ„ç¨‹åºæ¨¡æ¿çš„æŠ•æ¯’RAGå­˜å‚¨ï¼Œä½¿å¾—è¿™äº›â€œå«æ¥è®°å¿†â€åœ¨åç»­ç›¸ä¼¼ä»»åŠ¡ä¸­é€šè¿‡è¯æ³•å’ŒåµŒå…¥ç›¸ä¼¼æ€§è¢«æ£€ç´¢ã€‚å®éªŒåœ¨MetaGPTçš„DataInterpreteræ™ºèƒ½ä½“å’ŒGPT-4oä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜å°‘æ•°æŠ•æ¯’è®°å½•å³å¯åœ¨è‰¯æ€§å·¥ä½œè´Ÿè½½ä¸­å æ®æ˜¾è‘—æ£€ç´¢æ¯”ä¾‹ï¼Œå¯¼è‡´æ™ºèƒ½ä½“äº§ç”Ÿè·¨ä¼šè¯çš„æŒç»­è¡Œä¸ºæ¼‚ç§»ã€‚è¯¥å‘ç°è¯æ˜äº†åŸºäºç»éªŒçš„è‡ªæˆ‘æå‡æœºåˆ¶å¯èƒ½æ²¦ä¸ºéšè”½ä¸”æŒä¹…çš„å—æ”»å‡»å‘é‡ï¼Œä¸ºæ™ºèƒ½ä½“å®‰å…¨ç ”ç©¶æä¾›äº†æ–°è§†è§’ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "14 pages, 1 figure, includes appendix",
      "pdf_url": "https://arxiv.org/pdf/2512.16962v1",
      "published_date": "2025-12-18 08:34:40 UTC",
      "updated_date": "2025-12-18 08:34:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:09:16.355420+00:00"
    },
    {
      "arxiv_id": "2512.16297v1",
      "title": "Feature-Selective Representation Misdirection for Machine Unlearning",
      "title_zh": "é’ˆå¯¹æœºå™¨é—å¿˜çš„ç‰¹å¾é€‰æ‹©æ€§è¡¨å¾è¯¯å¯¼",
      "authors": [
        "Taozhao Chen",
        "Linghan Huang",
        "Kim-Kwang Raymond Choo",
        "Huaming Chen"
      ],
      "abstract": "As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†æ•æ„Ÿæˆ–è¿è§„çŸ¥è¯†æ—¶é¢ä¸´çš„éšç§å’Œå®‰å…¨é£é™©ï¼Œæå‡ºäº†é€‰æ‹©æ€§è¡¨ç¤ºè¯¯å¯¼ (Selective Representation Misdirection for Unlearning, SRMU) è¿™ä¸€æœºå™¨é—å¿˜ (Machine Unlearning) æ¡†æ¶ã€‚ç°æœ‰çš„é—å¿˜æŠ€æœ¯åœ¨é—å¿˜é›†ä¸ä¿ç•™é›†é«˜åº¦çº ç¼  (entangled distributions) çš„åœºæ™¯ä¸‹å¾€å¾€ä¼šå¯¼è‡´æ¨¡å‹é€šç”¨æ•ˆç”¨å—æŸï¼Œè€Œ SRMU é€šè¿‡ä¸€ç§å—æ§çš„æ¿€æ´»ç¼–è¾‘ (activation-editing) æœºåˆ¶è§£å†³äº†è¿™ä¸€éš¾é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ¿€æ´»é‡è¦æ€§å›¾ (activation importance map) æ„é€ ç»“æ„åŒ–è¯¯å¯¼å‘é‡ï¼Œå®ç°ç‰¹å¾æ„ŸçŸ¥ä¸”æ–¹å‘å—æ§çš„æ‰°åŠ¨ï¼Œä»è€Œåœ¨æŠ‘åˆ¶æœ‰å®³è¡¨ç¤ºçš„åŒæ—¶æœ€å¤§é™åº¦ä¿ç•™è‰¯æ€§çŸ¥è¯†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSRMU åœ¨ WMDP åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿› (SOTA) çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ•°æ®é›†é‡å åº¦é«˜è¾¾ 20-30% çš„æƒ…å†µä¸‹ä¾ç„¶è¡¨ç°ç¨³å¥ã€‚è¿™é¡¹å·¥ä½œä¸ºåŸºäº LLM çš„åº”ç”¨åœ¨å®‰å…¨æ²»ç†ã€éšç§åˆè§„ä»¥åŠå—æ§çŸ¥è¯†ç§»é™¤æ–¹é¢æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16297v1",
      "published_date": "2025-12-18 08:31:50 UTC",
      "updated_date": "2025-12-18 08:31:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:08:09.244666+00:00"
    },
    {
      "arxiv_id": "2512.16295v1",
      "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models",
      "title_zh": "OS-Oracleï¼šè·¨å¹³å° GUI è¯„ä»·æ¨¡å‹çš„ç»¼åˆæ€§æ¡†æ¶",
      "authors": [
        "Zhenyu Wu",
        "Jingjing Xie",
        "Zehao Li",
        "Bowen Yang",
        "Qiushi Sun",
        "Zhaoyang Liu",
        "Zhoumianze Liu",
        "Yu Qiao",
        "Xiangyu Yue",
        "Zun Wang",
        "Zichen Ding"
      ],
      "abstract": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLM)é©±åŠ¨çš„è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“(CUAs)åœ¨é•¿ç¨‹GUIä»»åŠ¡ä¸­å› å†³ç­–é”™è¯¯ç§¯ç´¯è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†OS-Oracleï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæ„å»ºè·¨å¹³å°GUIè¯„ä»·æ¨¡å‹(Critic Models)çš„å…¨é¢æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®æµæ°´çº¿ç”¨äºåˆæˆé«˜è´¨é‡çš„åé¦ˆæ•°æ®ï¼Œå¹¶é‡‡ç”¨ç»“åˆç›‘ç£å¾®è°ƒ(SFT)ä¸ä¿æŒä¸€è‡´æ€§çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(CP-GRPO)çš„ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æ¨å‡ºäº†æ¶µç›–ç§»åŠ¨ç«¯ã€ç½‘é¡µç«¯å’Œæ¡Œé¢ç«¯çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•OS-Critic Benchï¼Œå¹¶åŸºäº31ä¸‡ä¸ªæ ·æœ¬è®­ç»ƒå‡ºOS-Oracle-7Bæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOS-Oracle-7Båœ¨å¼€æºæ¨¡å‹ä¸­å–å¾—äº†SOTAæ€§èƒ½ï¼Œå¹¶åœ¨ç§»åŠ¨ç«¯é¢†åŸŸè¶…è¶Šäº†ä¸“æœ‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ä½œä¸ºé¢„è¯„ä»·æ¨¡å‹(Pre-critic)èƒ½æ˜¾è‘—æå‡UI-TARS-1.5-7Bç­‰åŸç”ŸGUIæ™ºèƒ½ä½“åœ¨OSWorldå’ŒAndroidWorldç¯å¢ƒä¸‹çš„ä»»åŠ¡è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œä¸ºæå‡GUIæ™ºèƒ½ä½“åœ¨å®é™…åº”ç”¨ä¸­çš„å†³ç­–å¯é æ€§å’Œå®¹é”™èƒ½åŠ›æä¾›äº†é‡è¦å·¥å…·å’Œæ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16295v1",
      "published_date": "2025-12-18 08:29:50 UTC",
      "updated_date": "2025-12-18 08:29:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:08:18.674589+00:00"
    },
    {
      "arxiv_id": "2512.16282v2",
      "title": "CALM: A CKA-Guided Adaptive Layer-Wise Modularization Framework for LLM Quantization",
      "title_zh": "CALMï¼šåŸºäº CKA å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹é‡åŒ–è‡ªé€‚åº”é€å±‚æ¨¡å—åŒ–æ¡†æ¶",
      "authors": [
        "Jinhao Zhang",
        "Yunquan Zhang",
        "Daning Chen",
        "JunSun",
        "Zicheng Yan"
      ],
      "abstract": "Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CALM (A CKA-guided Adaptive Layer-wise Modularization)a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. CALM independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMsincluding LLaMA and Qwenin terms of perplexity (PPL) and downstream task performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CALMï¼Œä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) é‡åŒ–è®¾è®¡çš„è‡ªé€‚åº”é€å±‚æ¨¡å—åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åè®­ç»ƒé‡åŒ– (PTQ) æ–¹æ³•å¯¹æ‰€æœ‰ç½‘ç»œå±‚é‡‡ç”¨ç»Ÿä¸€ç­–ç•¥è€Œå¿½ç•¥å±‚é—´å·®å¼‚çš„å±€é™æ€§ã€‚CALM æ˜¯ä¸€ç§æ— éœ€å¾®è°ƒã€å³æ’å³ç”¨çš„æ–¹æ¡ˆï¼Œå®ƒé€šè¿‡ç‹¬ç«‹è¯„ä¼°æ¯å±‚ä¸Šçš„å¤šç§ PTQ ç®—æ³•ï¼Œå¹¶åˆ©ç”¨çº¿æ€§ä¸­å¿ƒæ ¸å¯¹é½ (Linear Centered Kernel Alignment, CKA) æŒ‡æ ‡è‡ªåŠ¨ä¸ºå„å±‚é€‰æ‹©æœ€ä¼˜é‡åŒ–ç­–ç•¥ã€‚è¿™äº›é€å±‚ä¼˜åŒ–çš„ç­–ç•¥æœ€ç»ˆè¢«é›†æˆä¸ºæ··åˆé‡åŒ–æ¨¡å‹ï¼Œä»¥å®ç°æ€§èƒ½çš„æœ€ä¼˜åŒ–ã€‚åœ¨ LLaMA å’Œ Qwen ç­‰ä¸»æµæ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCALM åœ¨å›°æƒ‘åº¦ (PPL) å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ–¹é¢å‡ä¸€è‡´ä¼˜äºç»Ÿä¸€é‡åŒ–åŸºå‡†åŠæœ€å…ˆè¿›çš„æ··åˆç²¾åº¦ (Mixed-precision) æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16282v2",
      "published_date": "2025-12-18 08:01:19 UTC",
      "updated_date": "2026-01-08 16:51:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:08:06.439818+00:00"
    },
    {
      "arxiv_id": "2512.16280v2",
      "title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams",
      "title_zh": "çˆ±ã€è°è¨€ä¸è¯­è¨€æ¨¡å‹ï¼šæ¢ç©¶äººå·¥æ™ºèƒ½åœ¨æƒ…æ„Ÿè¯±å¯¼è¯ˆéª—ä¸­çš„è§’è‰²",
      "authors": [
        "Gilad Gressel",
        "Rahul Pankajakshan",
        "Shir Rozenfeld",
        "Ling Li",
        "Ivan Franceschini",
        "Krishnashree Achuthan",
        "Yisroel Mirsky"
      ],
      "abstract": "Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.\n  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æƒ…æ„Ÿè¯ˆéª—(Romance-Baiting Scams)ä¸­çš„åº”ç”¨åŠå…¶å¯¹è‡ªåŠ¨åŒ–è¯ˆéª—çš„æ¨åŠ¨ä½œç”¨ã€‚ç ”ç©¶è€…é€šè¿‡è®¿è°ˆ145åè¯ˆéª—ç»„ç»‡å†…éƒ¨äººå‘˜å’Œ5åå—å®³è€…ï¼Œå¹¶å¼€å±•äº†ä¸€é¡¹å¯¹æ¯”LLMè¯ˆéª—ä»£ç†ä¸äººç±»æ“ä½œå‘˜çš„é•¿æœŸç›²æµ‹ç ”ç©¶ï¼ŒåŒæ—¶è¯„ä¼°äº†å•†ä¸šå®‰å…¨è¿‡æ»¤å™¨(Safety Filters)çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLMså·²åœ¨è¯ˆéª—ç»„ç»‡ä¸­å¹¿æ³›åº”ç”¨ï¼Œçº¦87%çš„å¯¹è¯ä»»åŠ¡å…·å¤‡é«˜åº¦çš„è‡ªåŠ¨åŒ–æ½œåŠ›ã€‚å®éªŒè¯æ˜ï¼ŒLLMä»£ç†æ¯”äººç±»æ“ä½œå‘˜æ›´èƒ½èµ¢å¾—å—å®³è€…çš„ä¿¡ä»»(p=0.007)ï¼Œä¸”åœ¨è¯±å¯¼åˆè§„æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºäººç±»ï¼ˆ46%å¯¹18%ï¼‰ã€‚ä¸æ­¤åŒæ—¶ï¼Œç°æœ‰çš„ä¸»æµå®‰å…¨è¿‡æ»¤å™¨å¯¹æƒ…æ„Ÿè¯ˆéª—å¯¹è¯çš„æ£€æµ‹ç‡ä¸º0.0%ï¼Œå®Œå…¨æ— æ³•åº”å¯¹æ­¤ç±»å¨èƒã€‚è¯¥ç ”ç©¶ç»“æœè­¦ç¤ºï¼Œæƒ…æ„Ÿè¯ˆéª—æ­£é¢ä¸´å…¨é¢è‡ªåŠ¨åŒ–çš„é£é™©ï¼Œè€Œç›®å‰çš„é˜²å¾¡æ‰‹æ®µåœ¨é˜»æ­¢æ­¤ç±»AIé©±åŠ¨çš„è¯ˆéª—æ‰©å¼ æ–¹é¢ä»å­˜åœ¨ä¸¥é‡ç¼ºå¤±ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16280v2",
      "published_date": "2025-12-18 07:59:15 UTC",
      "updated_date": "2025-12-22 10:37:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:08:34.214713+00:00"
    },
    {
      "arxiv_id": "2512.16279v1",
      "title": "QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems",
      "title_zh": "QuadSentinelï¼šå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­æœºå™¨å¯æ ¡éªŒæ§åˆ¶çš„ç›¸ç»§å¼å®‰å…¨",
      "authors": [
        "Yiliu Yang",
        "Yilei Jiang",
        "Qunzhong Wang",
        "Yingshui Tan",
        "Xiaoyong Zhu",
        "Sherman S. M. Chow",
        "Bo Zheng",
        "Xiangyu Yue"
      ],
      "abstract": "Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \\textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \\textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†QuadSentinelï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å¤§è¯­è¨€æ¨¡å‹å®‰å…¨é£é™©çš„å››æ™ºèƒ½ä½“é˜²å¾¡æ¡†æ¶ï¼Œç”±state trackerã€policy verifierã€threat watcherå’Œrefereeç»„æˆã€‚ä¸ºè§£å†³è‡ªç„¶è¯­è¨€ç­–ç•¥åœ¨æœºå™¨æ£€æŸ¥å’Œè¿è¡Œæ—¶æ‰§è¡Œä¸­çš„æ­§ä¹‰æ€§ï¼Œè¯¥æ–¹æ³•å°†å®‰å…¨ç­–ç•¥è¡¨è¾¾ä¸ºsequentsï¼Œå¹¶å°†å…¶ç¼–è¯‘ä¸ºåŸºäºå¯è§‚å¯ŸçŠ¶æ€è°“è¯çš„æœºå™¨å¯æ£€æŸ¥è§„åˆ™ã€‚QuadSentinelåˆ©ç”¨Refereeé€»è¾‘å’Œé«˜æ•ˆçš„top-kè°“è¯æ›´æ–°æœºåˆ¶ï¼Œåœ¨ç¡®ä¿ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°åˆ†å±‚å†²çªè§£å†³å’Œæ£€æŸ¥ä¼˜å…ˆçº§æ’åºã€‚åœ¨ST-WebAgentBenchå’ŒAgentHarmåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†guardrailçš„å‡†ç¡®æ€§å’Œè§„åˆ™å¬å›ç‡ï¼Œå¹¶æœ‰æ•ˆé™ä½äº†è¯¯æŠ¥ç‡ã€‚ç›¸è¾ƒäºShieldAgentç­‰å•æ™ºèƒ½ä½“åŸºçº¿ï¼ŒQuadSentinelå±•ç°äº†æ›´å¼ºçš„å®‰å…¨æ§åˆ¶èƒ½åŠ›ï¼Œä¸”å…è®¸åœ¨ä¸ä¿®æ”¹æ ¸å¿ƒæ™ºèƒ½ä½“çš„æƒ…å†µä¸‹è¿›è¡Œç‹¬ç«‹éƒ¨ç½²ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå®ç°æœºå™¨å¯éªŒè¯çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå®‰å…¨ç®¡æ§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2512.16279v1",
      "published_date": "2025-12-18 07:58:40 UTC",
      "updated_date": "2025-12-18 07:58:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:09:32.908076+00:00"
    },
    {
      "arxiv_id": "2512.16275v1",
      "title": "GFLAN: Generative Functional Layouts",
      "title_zh": "GFLANï¼šç”Ÿæˆå¼åŠŸèƒ½æ€§å¸ƒå±€",
      "authors": [
        "Mohamed Abouagour",
        "Eleftherios Garyfallidis"
      ],
      "abstract": "Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GFLANï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³è‡ªåŠ¨åŒ–å¹³é¢å›¾ç”Ÿæˆï¼ˆAutomated floor plan generationï¼‰ä¸­å»ºç­‘æ¨ç†å’ŒåŠŸèƒ½çº¦æŸéš¾é¢˜çš„ç”Ÿæˆæ¡†æ¶ã€‚GFLAN å°†åˆæˆè¿‡ç¨‹æ˜¾å¼åˆ†è§£ä¸ºæ‹“æ‰‘è§„åˆ’ï¼ˆtopological planningï¼‰å’Œå‡ ä½•å®ç°ï¼ˆgeometric realizationï¼‰ä¸¤ä¸ªé˜¶æ®µï¼Œä»¥åº”å¯¹å¤æ‚çš„ç»„åˆæœç´¢ä¸å‡ ä½•çº¦æŸæŒ‘æˆ˜ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŒç¼–ç å™¨å·ç§¯æ¶æ„å¤„ç†ç©ºé—´èƒŒæ™¯ä¸å¸ƒå±€çŠ¶æ€ï¼Œé€šè¿‡ç¦»æ•£æ¦‚ç‡å›¾ï¼ˆdiscrete probability mapsï¼‰åœ¨å»ºç­‘åŒ…ç»œå†…é¡ºåºåˆ†é…æˆ¿é—´è´¨å¿ƒã€‚ç¬¬äºŒé˜¶æ®µåˆ™æ„å»ºäº†è¿æ¥æˆ¿é—´èŠ‚ç‚¹ä¸è¾¹ç•Œé¡¶ç‚¹çš„å¼‚æ„å›¾ï¼ˆheterogeneous graphï¼‰ï¼Œå¹¶é‡‡ç”¨ Transformer å¢å¼ºçš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¥è”åˆå›å½’æˆ¿é—´è¾¹ç•Œã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„åƒç´ åˆ°åƒç´ ï¼ˆpixel-to-pixelï¼‰ç›´æ¥ç”Ÿæˆæ–¹å¼ï¼ŒGFLAN é€šè¿‡è¿™ç§ä¸¤é˜¶æ®µåˆ†è§£æ˜¾è‘—æå‡äº†å¯¹æ‹“æ‰‘å…³ç³»å’ŒåŠŸèƒ½çº¦æŸçš„æ•æ‰èƒ½åŠ›ï¼Œä¸ºå¤æ‚çš„å»ºç­‘å¸ƒå±€åˆæˆæä¾›äº†æ›´å…·åŸåˆ™æ€§çš„è®¡ç®—å¤„ç†æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16275v1",
      "published_date": "2025-12-18 07:52:47 UTC",
      "updated_date": "2025-12-18 07:52:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:08:22.616514+00:00"
    },
    {
      "arxiv_id": "2512.16272v2",
      "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls",
      "title_zh": "çªç ´ç›²ç‚¹ï¼šç¼“è§£åŸºäºå¤§è¯­è¨€æ¨¡å‹è¯„ä¼°é™·é˜±çš„åˆ†ææ€§æç¤º",
      "authors": [
        "Ora Nova Fandina",
        "Eitan Farchi",
        "Shmulik Froimovich",
        "Raviv Gal",
        "Wesam Ibraheem",
        "Rami Katan",
        "Alice Podolsky"
      ],
      "abstract": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.\n  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.\n  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45-63% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 74% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤è€… (Large Language Models as judges, LaaJ) åœ¨ä»£ç ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯ COBOL é—ç•™ä»£ç ç°ä»£åŒ–ä¸­çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºäº† LaaJ åœ¨å¤„ç†ç‰¹å®šé¢†åŸŸä»»åŠ¡æ—¶å­˜åœ¨çš„è¯„ä¼°ç›²ç‚¹ (blind spots)ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…é€šè¿‡ä¸“å®¶çŸ¥è¯†æ„å»ºäº†é”™è¯¯åˆ†ç±»å­¦ (taxonomy)ï¼Œå¹¶å¼€å‘äº†ä¸€æ¬¾è½»é‡çº§åˆ†ææ£€æŸ¥å·¥å…· (analytic checker tool) ç”¨ä»¥è¯†åˆ« 30 å¤šç§é¢†åŸŸç‰¹å®šé—®é¢˜ã€‚è¯¥æ–¹æ³•å°†å·¥å…·ç”Ÿæˆçš„åˆ†ææç¤º (analytic hints) åŠ¨æ€æ³¨å…¥è¯„åˆ¤è€…çš„æç¤ºè¯ä¸­ï¼Œå¼•å¯¼ LaaJ é‡æ–°è¯„ä¼°è¢«å¿½è§†çš„ä»£ç ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åŒ…å« 100 ä¸ªç¨‹åºçš„æµ‹è¯•é›†ä¸Šï¼ŒLaaJ+Hints æ–¹æ¡ˆå°†é”™è¯¯æ£€å‡ºç‡ä» 45-63% æå‡è‡³ 74%ï¼Œå¹¶æä¾›äº†æ›´å…·æ·±åº¦å’Œå‡†ç¡®æ€§çš„è§£é‡Šã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†åˆ†æå·¥å…·ä¸å¤§æ¨¡å‹ç»“åˆçš„æ··åˆæ¶æ„ (analytic-LLM hybrids) èƒ½æ˜¾è‘—å¢å¼ºå·¥ä¸šæµæ°´çº¿ä¸­è‡ªåŠ¨åŒ–è¯„ä¼°çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16272v2",
      "published_date": "2025-12-18 07:43:48 UTC",
      "updated_date": "2026-01-18 14:05:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:08:45.880623+00:00"
    },
    {
      "arxiv_id": "2512.16271v1",
      "title": "Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification",
      "title_zh": "é¢å‘å©´å„¿å“­å£°åˆ†ç±»çš„é¢†åŸŸæ— å…³å› æœæ„ŸçŸ¥éŸ³é¢‘ Transformer",
      "authors": [
        "Geofrey Owino",
        "Bernard Shibwabo Kasamani",
        "Ahmed M. Abdelmoniem",
        "Edem Wornyo"
      ],
      "abstract": "Accurate and interpretable classification of infant cry paralinguistics is essential for early detection of neonatal distress and clinical decision support. However, many existing deep learning methods rely on correlation-driven acoustic representations, which makes them vulnerable to noise, spurious cues, and domain shifts across recording environments. We propose DACH-TIC, a Domain-Agnostic Causal-Aware Hierarchical Audio Transformer for robust infant cry classification. The model integrates causal attention, hierarchical representation learning, multi-task supervision, and adversarial domain generalization within a unified framework.\n  DACH-TIC employs a structured transformer backbone with local token-level and global semantic encoders, augmented by causal attention masking and controlled perturbation training to approximate counterfactual acoustic variations. A domain-adversarial objective promotes environment-invariant representations, while multi-task learning jointly optimizes cry type recognition, distress intensity estimation, and causal relevance prediction. The model is evaluated on the Baby Chillanto and Donate-a-Cry datasets, with ESC-50 environmental noise overlays for domain augmentation.\n  Experimental results show that DACH-TIC outperforms state-of-the-art baselines, including HTS-AT and SE-ResNet Transformer, achieving improvements of 2.6 percent in accuracy and 2.2 points in macro-F1 score, alongside enhanced causal fidelity. The model generalizes effectively to unseen acoustic environments, with a domain performance gap of only 2.4 percent, demonstrating its suitability for real-world neonatal acoustic monitoring systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å©´å„¿å“­å£°åˆ†ç±»ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹æ˜“å—å™ªå£°ã€è™šå‡çº¿ç´¢å’Œé¢†åŸŸåç§»(domain shifts)å½±å“çš„é—®é¢˜ï¼Œæå‡ºäº†DACH-TICæ¡†æ¶ã€‚è¿™æ˜¯ä¸€ç§é¢†åŸŸæ— å…³ä¸”å…·æœ‰å› æœæ„ŸçŸ¥(Domain-Agnostic Causal-Aware)çš„å±‚æ¬¡åŒ–éŸ³é¢‘Transformerï¼Œé›†æˆäº†å› æœæ³¨æ„åŠ›(causal attention)ã€å±‚æ¬¡åŒ–è¡¨ç¤ºå­¦ä¹ ã€å¤šä»»åŠ¡ç›‘ç£ä»¥åŠå¯¹æŠ—æ€§é¢†åŸŸæ³›åŒ–æŠ€æœ¯ã€‚æ¨¡å‹é‡‡ç”¨åŒ…å«å±€éƒ¨tokençº§å’Œå…¨å±€è¯­ä¹‰ç¼–ç å™¨çš„Transformerä¸»å¹²ç½‘ç»œï¼Œé€šè¿‡å—æ§æ‰°åŠ¨è®­ç»ƒæ¨¡æ‹Ÿåäº‹å®å£°å­¦å˜åŒ–ï¼Œå¹¶åˆ©ç”¨é¢†åŸŸå¯¹æŠ—ç›®æ ‡æå‡ç¯å¢ƒä¸å˜æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDACH-TICåœ¨å‡†ç¡®ç‡å’Œmacro-F1å¾—åˆ†ä¸Šåˆ†åˆ«æ¯”HTS-ATå’ŒSE-ResNet Transformerç­‰åŸºå‡†æ¨¡å‹æå‡äº†2.6%å’Œ2.2ä¸ªç™¾åˆ†ç‚¹ã€‚è¯¥æ¨¡å‹åœ¨æœªçŸ¥å£°å­¦ç¯å¢ƒä¸­çš„æ€§èƒ½å·®è·ä»…ä¸º2.4%ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…æ–°ç”Ÿå„¿å£°å­¦ç›‘æµ‹å’Œä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿä¸­çš„é²æ£’æ€§ã€å¯è§£é‡Šæ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "This paper has been published in the IEEE proceedings of the 8th International Conference of Computer and Informatics Engineering (IC2IE)",
      "pdf_url": "https://arxiv.org/pdf/2512.16271v1",
      "published_date": "2025-12-18 07:40:44 UTC",
      "updated_date": "2025-12-18 07:40:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:08:46.145763+00:00"
    },
    {
      "arxiv_id": "2512.16270v1",
      "title": "TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering",
      "title_zh": "TextEditBenchï¼šè¶…è¶Šæ¸²æŸ“çš„æ¨ç†æ„ŸçŸ¥å‹æ–‡æœ¬ç¼–è¾‘è¯„æµ‹",
      "authors": [
        "Rui Gui",
        "Yang Wan",
        "Haochen Han",
        "Dongxing Mao",
        "Fangming Liu",
        "Min Li",
        "Alex Jinpeng Wang"
      ],
      "abstract": "Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†TextEditBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å›¾åƒä¸­ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒåŒºåŸŸçš„ç»¼åˆè¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³è§†è§‰ç”Ÿæˆé¢†åŸŸä¸­è¢«é•¿æœŸå¿½è§†çš„æ–‡æœ¬ç¼–è¾‘éš¾é¢˜ã€‚è¯¥åŸºå‡†è¶…è¶Šäº†åŸºç¡€çš„åƒç´ ä¿®æ”¹ï¼Œå¼ºè°ƒæ¨ç†å¯†é›†å‹çš„ç¼–è¾‘åœºæ™¯ï¼Œè¦æ±‚æ¨¡å‹å…·å¤‡ç†è§£ç‰©ç†åˆç†æ€§ã€è¯­è¨€æ„ä¹‰åŠè·¨æ¨¡æ€ä¾èµ–çš„èƒ½åŠ›ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºè¯­ä¹‰æœŸæœ›ï¼ˆSemantic Expectation, SEï¼‰çš„æ–°å‹è¯„ä¼°ç»´åº¦ï¼Œé€šè¿‡è¡¡é‡è¯­ä¹‰ä¸€è‡´æ€§ã€è¯­å¢ƒè¿è´¯æ€§å’Œè·¨æ¨¡æ€å¯¹é½æ¥é‡åŒ–æ¨¡å‹çš„æ¨ç†æ°´å¹³ã€‚å¯¹ç°æœ‰æœ€å…ˆè¿›ç³»ç»Ÿçš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡å½“å‰æ¨¡å‹èƒ½å¤„ç†ç®€å•æŒ‡ä»¤ï¼Œä½†åœ¨è¯­å¢ƒæ¨ç†ã€ç‰©ç†ä¸€è‡´æ€§åŠå¸ƒå±€æ„ŸçŸ¥é›†æˆæ–¹é¢ä»è¡¨ç°æ¬ ç¼ºã€‚TextEditBenchä¸ºæ¨è¿›å¤šæ¨¡æ€ç”Ÿæˆä¸­å—æ§çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ä¸æ¨ç†èƒ½åŠ›å»ºç«‹äº†å…³é”®çš„æµ‹è¯•æ ‡å‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16270v1",
      "published_date": "2025-12-18 07:37:08 UTC",
      "updated_date": "2025-12-18 07:37:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:09:54.511507+00:00"
    },
    {
      "arxiv_id": "2512.16262v1",
      "title": "Learning to Wait: Synchronizing Agents with the Physical World",
      "title_zh": "å­¦ä¹ ç­‰å¾…ï¼šå®ç°æ™ºèƒ½ä½“ä¸ç‰©ç†ä¸–ç•Œçš„åŒæ­¥",
      "authors": [
        "Yifei She",
        "Ping Zhang",
        "He Liu",
        "Yanmin Jia",
        "Yang Jing",
        "Zijun Liu",
        "Peng Sun",
        "Xiangbin Li",
        "Xiaohe Hu"
      ],
      "abstract": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®ä¸–ç•Œæ™ºèƒ½ä½“ä»»åŠ¡ä¸­å› éé˜»å¡æ“ä½œå’Œå˜åŠ¨å»¶è¿Ÿå¯¼è‡´çš„â€œæ—¶é—´é—´éš”â€(Temporal Gap)æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ä½¿å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸»åŠ¨å°†â€œè®¤çŸ¥æ—¶é—´çº¿â€(Cognitive Timeline)ä¸ç‰©ç†ä¸–ç•ŒåŒæ­¥çš„â€œæ™ºèƒ½ä½“ä¾§æ–¹æ³•â€(Agent-side Approach)ã€‚è¯¥æ–¹æ³•å°†â€œä»£ç å³åŠ¨ä½œâ€(Code-as-Action)èŒƒå¼æ‰©å±•è‡³æ—¶é—´ç»´åº¦ï¼Œä½¿æ™ºèƒ½ä½“èƒ½åˆ©ç”¨è¯­ä¹‰å…ˆéªŒå’Œè¯­å¢ƒå­¦ä¹ (ICL)é¢„æµ‹ç²¾ç¡®çš„ç­‰å¾…æ—¶é•¿(time.sleep(t))ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿè½®è¯¢æ–¹æ¡ˆå¸¦æ¥çš„ä¸Šä¸‹æ–‡å†—ä½™å’Œæ‰©å±•æ€§éš¾é¢˜ã€‚é€šè¿‡åœ¨Kubernetesé›†ç¾¤ç¯å¢ƒä¸­çš„å®éªŒéªŒè¯ï¼Œæ™ºèƒ½ä½“å±•ç°å‡ºç²¾å‡†æ ¡å‡†å†…éƒ¨æ—¶é’Ÿçš„èƒ½åŠ›ï¼Œåœ¨æ˜¾è‘—é™ä½æŸ¥è¯¢å¼€é”€çš„åŒæ—¶æœ€å°åŒ–äº†æ‰§è¡Œå»¶è¿Ÿã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ—¶é—´æ„ŸçŸ¥(Temporal Awareness)æ˜¯æ™ºèƒ½ä½“åœ¨å¼€æ”¾å¼ç¯å¢ƒä¸­å®ç°è‡ªä¸»è¿›åŒ–æ‰€å¿…éœ€çš„ä¸€ç§å¯å­¦ä¹ èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16262v1",
      "published_date": "2025-12-18 07:24:44 UTC",
      "updated_date": "2025-12-18 07:24:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:09:57.951922+00:00"
    },
    {
      "arxiv_id": "2512.16251v3",
      "title": "Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model",
      "title_zh": "è‚¡ç¥¨æ”¶ç›Šç‡çš„å¯è§£é‡Šæ·±åº¦å­¦ä¹ ï¼šå…±è¯†ç“¶é¢ˆèµ„äº§å®šä»·æ¨¡å‹",
      "authors": [
        "Bong-Gyu Jang",
        "Younwoo Jeong",
        "Changeun Kim"
      ],
      "abstract": "We introduce the Consensus-Bottleneck Asset Pricing Model (CB-APM), a framework that reconciles the predictive power of deep learning with the structural transparency of traditional finance. By embedding aggregate analyst consensus as a structural \"bottleneck\", the model treats professional beliefs as a sufficient statistic for the market's high-dimensional information set. We document a striking \"interpretability-accuracy amplification effect\" for annual horizons, the structural constraint acts as an endogenous regularizer that significantly improves out-of-sample R2 over unconstrained benchmarks. Portfolios sorted on CB-APM forecasts exhibit a strong monotonic return gradient, delivering an annualized Sharpe ratio of 1.44 and robust performance across macroeconomic regimes. Furthermore, pricing diagnostics reveal that the learned consensus captures priced variation only partially spanned by canonical factor models, identifying structured risk heterogeneity that standard linear models systematically miss. Our results suggest that anchoring machine intelligence to human-expert belief formation is not merely a tool for transparency, but a catalyst for uncovering new dimensions of belief-driven risk premiums.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å…±è¯†ç“¶é¢ˆèµ„äº§å®šä»·æ¨¡å‹ (Consensus-Bottleneck Asset Pricing Model, CB-APM)ï¼Œæ—¨åœ¨å°†æ·±åº¦å­¦ä¹  (Deep Learning) çš„é¢„æµ‹èƒ½åŠ›ä¸ä¼ ç»Ÿé‡‘èçš„ç»“æ„é€æ˜åº¦ç›¸ç»“åˆã€‚è¯¥æ¨¡å‹å°†åˆ†æå¸ˆçš„ç»¼åˆå…±è¯† (Aggregate Analyst Consensus) åµŒå…¥ä¸ºç»“æ„æ€§çš„â€œç“¶é¢ˆâ€ (Bottleneck)ï¼Œå°†ä¸“ä¸šæŠ•èµ„è€…çš„ä¿¡å¿µè§†ä¸ºå¸‚åœºé«˜ç»´ä¿¡æ¯é›†çš„å……åˆ†ç»Ÿè®¡é‡ã€‚ç ”ç©¶å‘ç°äº†ä¸€ç§æ˜¾è‘—çš„â€œå¯è§£é‡Šæ€§-å‡†ç¡®æ€§æ”¾å¤§æ•ˆåº”â€ (Interpretability-Accuracy Amplification Effect)ï¼Œå³ç»“æ„æ€§çº¦æŸåœ¨å¹´åº¦é¢„æµ‹ä¸­èµ·åˆ°äº†å†…ç”Ÿæ­£åˆ™åŒ–ä½œç”¨ï¼Œä½¿å…¶æ ·æœ¬å¤– R2 æ˜¾è‘—ä¼˜äºæ— çº¦æŸåŸºå‡†æ¨¡å‹ã€‚åŸºäº CB-APM é¢„æµ‹æ„å»ºçš„æŠ•èµ„ç»„åˆå±•ç°å‡ºå¼ºåŠ²çš„å•è°ƒæ”¶ç›Šæ¢¯åº¦ï¼Œå®ç°äº† 1.44 çš„å¹´åŒ–å¤æ™®æ¯”ç‡ (Sharpe Ratio)ï¼Œä¸”åœ¨ä¸åŒå®è§‚ç»æµå‘¨æœŸä¸‹å‡è¡¨ç°ç¨³å¥ã€‚å®šä»·è¯Šæ–­è¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ¨¡å‹æ•è·çš„å…±è¯†å˜åŠ¨è¯†åˆ«äº†æ ‡å‡†çº¿æ€§æ¨¡å‹ç³»ç»Ÿæ€§å¿½ç•¥çš„ç»“æ„åŒ–é£é™©å¼‚è´¨æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†æœºå™¨æ™ºèƒ½ä¸äººç±»ä¸“å®¶ä¿¡å¿µå½¢æˆæœºåˆ¶ç›¸ç»“åˆï¼Œä¸ä»…æ˜¯æå‡é€æ˜åº¦çš„å·¥å…·ï¼Œæ›´æ˜¯æ­ç¤ºä¿¡å¿µé©±åŠ¨é£é™©æº¢ä»· (Risk Premiums) æ–°ç»´åº¦çš„å‚¬åŒ–å‰‚ã€‚",
      "categories": [
        "q-fin.PR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.PR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16251v3",
      "published_date": "2025-12-18 07:05:25 UTC",
      "updated_date": "2025-12-31 06:16:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:09:51.060309+00:00"
    },
    {
      "arxiv_id": "2512.16250v1",
      "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding",
      "title_zh": "AMUSEï¼šé¢å‘æ™ºèƒ½ä½“å¤šå‘è¨€äººç†è§£çš„è§†å¬åŸºå‡†ä¸å¯¹é½æ¡†æ¶",
      "authors": [
        "Sanjoy Chowdhury",
        "Karren D. Yang",
        "Xudong Liu",
        "Fartash Faghri",
        "Pavan Kumar Anasosalu Vasu",
        "Oncel Tuzel",
        "Dinesh Manocha",
        "Chun-Liang Li",
        "Raviteja Vemulapalli"
      ],
      "abstract": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šå‘è¨€äººå¯¹è¯åœºæ™¯ä¸­ä»£ç†æ¨ç†ï¼ˆAgentic Reasoningï¼‰èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†å…¨æ–°çš„åŸºå‡†æµ‹è¯• AMUSEã€‚AMUSE é€šè¿‡æ—¶ç©ºå‘è¨€äººå®šä½ï¼ˆSpatio-temporal Speaker Groundingï¼‰å’Œå¤šæ¨¡æ€å¯¹è¯æ€»ç»“ç­‰å…­ç±»ä»»åŠ¡ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å¤æ‚è§†å¬äº¤äº’ä¸­çš„è§„åˆ’ã€å®šä½ä¸åæ€èƒ½åŠ›ã€‚å®éªŒæ˜¾ç¤ºï¼Œç°æœ‰å…ˆè¿›æ¨¡å‹åœ¨å¤„ç†å¤šå‘è¨€äººæ¨ç†æ—¶è¡¨ç°æ™®éè¾ƒå¼±ä¸”ä¸ä¸€è‡´ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº† RAFT ä»£ç†å¯¹é½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å°†å†…åœ¨å¤šæ¨¡æ€è‡ªæˆ‘è¯„ä¼°æ•´åˆä¸ºå¥–åŠ±ä¼˜åŒ–ï¼Œå¹¶ç»“åˆé€‰æ‹©æ€§å‚æ•°é€‚é…å®ç°é«˜æ•ˆæ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAFT åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†é«˜è¾¾ 39.52% çš„ç›¸å¯¹å‡†ç¡®ç‡æå‡ã€‚AMUSE ä¸ RAFT çš„æå‡ºä¸ºåˆ†æåŠå¢å¼ºå¤šæ¨¡æ€æ¨¡å‹çš„ä»£ç†æ¨ç†èƒ½åŠ›æä¾›äº†ç³»ç»Ÿçš„è¯„ä¼°å·¥å…·ä¸ä¼˜åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16250v1",
      "published_date": "2025-12-18 07:01:47 UTC",
      "updated_date": "2025-12-18 07:01:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:09:55.103937+00:00"
    },
    {
      "arxiv_id": "2512.16248v2",
      "title": "Sigma-MoE-Tiny Technical Report",
      "title_zh": "Sigma-MoE-Tiny æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Qingguo Hu",
        "Zhenghao Lin",
        "Ziyue Yang",
        "Yucheng Ding",
        "Xiao Liu",
        "Yuting Jiang",
        "Ruizhe Wang",
        "Tianyu Chen",
        "Zhongxin Guo",
        "Yifan Xiong",
        "Rui Gao",
        "Lei Qu",
        "Jinsong Su",
        "Peng Cheng",
        "Yeyun Gong"
      ],
      "abstract": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Sigma-MoE-Tinyï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰æé«˜ç¨€ç–åº¦çš„æ··åˆä¸“å®¶æ¨¡å‹ (Mixture-of-Experts)ï¼Œæ—¨åœ¨é€šè¿‡é«˜æ•ˆçš„å¯æ‰©å±•æ€§æå‡åŸºç¡€æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç»†ç²’åº¦çš„ä¸“å®¶åˆ†æ®µæŠ€æœ¯ï¼Œæ¯å±‚åŒ…å«å¤šè¾¾ 96 ä¸ªä¸“å®¶ä¸”æ¯ä¸ª token ä»…æ¿€æ´»ä¸€ä¸ªä¸“å®¶ï¼Œå®ç°äº† 20B æ€»å‚æ•°ä¸‹ä»… 0.5B æ¿€æ´»å‚æ•°çš„æé«˜æ•ˆç‡ã€‚é’ˆå¯¹æç«¯ç¨€ç–å¯¼è‡´çš„ä¸“å®¶è´Ÿè½½å‡è¡¡éš¾é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†æ¸è¿›å¼ç¨€ç–åŒ–æ–¹æ¡ˆ (progressive sparsification schedule)ï¼Œä»¥å¹³è¡¡ä¸“å®¶åˆ©ç”¨ç‡å¹¶ç¡®ä¿è®­ç»ƒç¨³å®šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒSigma-MoE-Tiny åœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°æå…¶ç¨³å®šï¼Œä¸”åœ¨å„é¡¹è¯„ä¼°ä¸­å‡ä¼˜äºåŒè§„æ¨¡ç”šè‡³æ›´å¤§è§„æ¨¡çš„ç«äº‰å¯¹æ‰‹ã€‚æ­¤å¤–ï¼Œè¯¥æŠ¥å‘Šè¿˜æ·±å…¥è®¨è®ºäº†é«˜ç¨€ç–åº¦ MoE æ¨¡å‹ä¸­çš„è´Ÿè½½å‡è¡¡é—®é¢˜ï¼Œä¸ºæœªæ¥å¼€å‘æ›´é«˜ç¨€ç–åº¦çš„æ¶æ„æä¾›äº†é‡è¦çš„ç†è®ºè§è§£å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16248v2",
      "published_date": "2025-12-18 06:57:42 UTC",
      "updated_date": "2025-12-19 05:44:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:10:05.446045+00:00"
    },
    {
      "arxiv_id": "2512.16245v1",
      "title": "AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints",
      "title_zh": "AlignMergeï¼šåŸºäº Fisher å¼•å¯¼å‡ ä½•çº¦æŸçš„å¯¹é½ä¿æŒå¤§è¯­è¨€æ¨¡å‹åˆå¹¶",
      "authors": [
        "Aniruddha Roy",
        "Jyoti Patel",
        "Aman Chadha",
        "Vinija Jain",
        "Amitava Das"
      ],
      "abstract": "Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.\n  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:\n  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,\n  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.\n  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AlignMergeï¼Œä¸€ç§æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åˆå¹¶è¿‡ç¨‹ä¸­å› å¸¸è§„æƒé‡æ··åˆå¯¼è‡´å¯¹é½(alignment)å±æ€§è¢«ç ´åçš„å‡ ä½•æ„ŸçŸ¥åˆå¹¶æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ Fisher-Guided Geometric Constraints å°†å¯¹é½è®¾ä¸ºæ˜¾å¼ä¸å˜é‡ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒåŸºå‡†æ¨¡å‹çš„å±€éƒ¨ Fisher chart ä¸­åˆ©ç”¨æŠ•å½±çŸ©é˜µä¼°è®¡å¯¹é½å­ç©ºé—´ã€‚AlignMerge çš„æ ¸å¿ƒä¼˜åŒ–ç›®æ ‡ç»“åˆäº† Fisher-Rao geometry çš„è·ç¦»çº¦æŸã€å¯¹é½æ•æ„Ÿæ–¹å‘çš„è¿åŠ¨æƒ©ç½šä»¥åŠè½¯å¯¹é½é¢„ç®—ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åä¸º Alignment Quality Index (AQI) çš„æ½œåœ¨ç©ºé—´å‡†åˆ™ï¼Œç”¨ä»¥è¡¡é‡è¡¨ç¤ºç©ºé—´ä¸­å¯¹é½ä¸éå¯¹é½è¡Œä¸ºçš„åˆ†ç¦»ç¨‹åº¦ã€‚åœ¨ LLaMA-3ã€Mistralã€Qwen 2 ç­‰å¤šç§æ¨¡å‹å®¶æ—ä¸Šçš„å®éªŒè¯æ˜ï¼ŒAlignMerge åœ¨æå‡å¯¹é½æŒ‡æ ‡å’Œé™ä½ toxicity çš„åŒæ—¶ï¼Œåœ¨æŒ‡ä»¤éµå¾ªã€æ¨ç†å’Œæœ‰ç”¨æ€§æ–¹é¢å‡åŒ¹é…æˆ–è¶…è¿‡äº†æœ€ä¼˜ä¸“å®¶æ¨¡å‹ã€‚ä¸ TIESã€SafeMerge å’Œ MergeAlign ç­‰åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å±•ç°å‡ºæ›´å°çš„å¯¹é½å­ç©ºé—´æ¼‚ç§»ï¼Œä¸ºæœªæ¥åŸºç¡€æ¨¡å‹çš„å‡ ä½•æ„ŸçŸ¥ç»„åˆæä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16245v1",
      "published_date": "2025-12-18 06:55:17 UTC",
      "updated_date": "2025-12-18 06:55:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:10:24.539250+00:00"
    },
    {
      "arxiv_id": "2512.16244v2",
      "title": "Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”±ç²—åˆ°ç»†å¼€æ”¾é›†å›¾èŠ‚ç‚¹åˆ†ç±»",
      "authors": [
        "Xueqi Ma",
        "Xingjun Ma",
        "Sarah Monazam Erfani",
        "Danilo Mandic",
        "James Bailey"
      ],
      "abstract": "Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(GNNs)åœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­å¯¹åˆ†å¸ƒå¤–(OOD)æ ·æœ¬åˆ†ç±»çš„éœ€æ±‚ï¼Œæå‡ºäº†CFC (Coarse-to-Fine open-set Classification)æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å°†ä¼ ç»Ÿçš„åˆ†å¸ƒå¤–(OOD)æ£€æµ‹æ‰©å±•åˆ°æ›´ç»†è‡´çš„åˆ†å¸ƒå¤–(OOD)åˆ†ç±»ï¼Œä»è€Œåœ¨æ²¡æœ‰çœŸå®æ ‡ç­¾ä¿¡æ¯çš„æƒ…å†µä¸‹è¯†åˆ«æ ·æœ¬çš„æ½œåœ¨ç±»åˆ«ã€‚CFCç”±ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼šåˆ©ç”¨LLMæç¤ºè¿›è¡Œåˆæ­¥æ£€æµ‹ä¸æ ‡ç­¾ç”Ÿæˆçš„ç²—åˆ†ç±»å™¨ã€ç»“åˆç²—åˆ†ç±»ç»“æœè¿›è¡Œè®­ç»ƒçš„GNNç²¾ç»†åˆ†ç±»å™¨ï¼Œä»¥åŠé€šè¿‡LLMä¸åå¤„ç†å®ç°çš„ç»†åŒ–åˆ†ç±»ã€‚ä¸ä¾èµ–åˆæˆæˆ–è¾…åŠ©æ ·æœ¬çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒCFCé‡‡ç”¨åŸºäºè¯­ä¹‰å«ä¹‰çš„çœŸå®åˆ†å¸ƒå¤–(OOD)å®ä¾‹ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨æ¬ºè¯ˆæ£€æµ‹å’ŒåŒ»ç–—è¯Šæ–­ç­‰é«˜é£é™©åœºæ™¯ä¸‹çš„å¯è§£é‡Šæ€§ä¸å®ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCFCåœ¨å›¾å’Œæ–‡æœ¬é¢†åŸŸçš„åˆ†å¸ƒå¤–(OOD)æ£€æµ‹æ•ˆæœæ¯”ç°æœ‰æœ€ä¼˜æ–¹æ³•æå‡äº†10%ï¼Œå¹¶åœ¨å›¾æ•°æ®é›†çš„åˆ†å¸ƒå¤–(OOD)åˆ†ç±»ä¸­è¾¾åˆ°äº†æœ€é«˜70%çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.16244v2",
      "published_date": "2025-12-18 06:50:13 UTC",
      "updated_date": "2025-12-21 10:28:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:10:39.846370+00:00"
    },
    {
      "arxiv_id": "2512.16237v1",
      "title": "Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis",
      "title_zh": "é€šè¿‡ç¨‹åºåŒ–æ•°æ®åˆæˆæå‡ MLLMs çš„ç©ºé—´æ¨ç†èƒ½åŠ›",
      "authors": [
        "Zhi Helu",
        "Huang Jingjing",
        "Xu Wang",
        "Xu Yangbin",
        "Zhang Wanyue",
        "Jiang Baoyang",
        "Deng Shirui",
        "Zhu Liang",
        "Li Fangfang",
        "Zhao Tiejun",
        "Lin Yankai",
        "Yao Yuan"
      ],
      "abstract": "Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.",
      "tldr_zh": "é’ˆå¯¹å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„å±€é™ï¼Œä»¥åŠç°æœ‰è®­ç»ƒæ•°æ®åœ¨ç»“æ„åˆšæ€§ä¸äººå·¥æ ‡æ³¨ä¸å¯æ‰©å±•æ€§ä¹‹é—´çš„çŸ›ç›¾ï¼Œè¯¥ç ”ç©¶æå‡ºäº† SPRITE æ¡†æ¶ã€‚SPRITE çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†äº‹å®çœŸå€¼ (ground-truth) ç”Ÿæˆè½¬åŒ–ä¸ºä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å°†å¤æ‚çš„ç©ºé—´é—®é¢˜ç¼–è¯‘ä¸ºå¯æ‰§è¡Œç¨‹åºï¼Œå¹¶ç»“åˆæ¨¡æ‹Ÿå™¨æå–çš„é«˜ç²¾åº¦åœºæ™¯å…ƒä¿¡æ¯è¿›è¡ŒéªŒè¯ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†ç”Ÿæˆçš„ 30 ä¸‡ä½™å¯¹å›¾åƒ/è§†é¢‘æŒ‡ä»¤å¾®è°ƒå¯¹åœ¨è®¡ç®—ä¸Šç²¾ç¡®ä¸”å…·æœ‰é«˜åº¦çš„è¯­è¨€å¤šæ ·æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ SPRITE æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªç©ºé—´åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½å¢ç›Šï¼Œå¹¶ä¼˜äºåŒç­‰è§„æ¨¡çš„å…¶ä»–å¼€æºæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæ‰©å±•æ€§åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œå…‹æœä¼ ç»Ÿæ¨¡æ¿åŒ–æ–¹æ³•çš„å¤šæ ·æ€§å±€é™å¯¹äºæ„å»ºé²æ£’ã€é€šç”¨çš„å…·èº«æ™ºèƒ½ (Embodied Intelligence) ç©ºé—´æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16237v1",
      "published_date": "2025-12-18 06:30:08 UTC",
      "updated_date": "2025-12-18 06:30:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:10:22.848733+00:00"
    },
    {
      "arxiv_id": "2512.16236v1",
      "title": "The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models",
      "title_zh": "ä¿¡æ¯æ£€ç´¢ä¸­çš„é‡æ’åºæ¨¡å‹æ¼”è¿›ï¼šä»å¯å‘å¼æ–¹æ³•åˆ°å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Tejul Pandit",
        "Sakshi Mahendru",
        "Meet Raval",
        "Dhvani Upadhyay"
      ],
      "abstract": "Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.\n  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿåœ°å›é¡¾äº†ä¿¡æ¯æ£€ç´¢(Information Retrieval, IR)ç³»ç»Ÿä¸­é‡æ’åº(Reranking)æ¨¡å‹çš„æ¼”è¿›å†ç¨‹ï¼Œç‰¹åˆ«æ¢è®¨äº†å…¶åœ¨ç°ä»£æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval Augmented Generation, RAG)æµæ°´çº¿ä¸­å¯¹è¾“å‡ºè´¨é‡çš„å…³é”®å½±å“ã€‚æ–‡ç« æŒ‰ç…§æ—¶é—´é¡ºåºæ¢³ç†äº†ä»åŸºç¡€æ–¹æ³•åˆ°å¤æ‚ç¥ç»ç½‘ç»œæ¶æ„çš„å‘å±•ï¼Œæ¶µç›–äº†äº¤å‰ç¼–ç å™¨(cross-encoders)ã€åºåˆ—ç”Ÿæˆæ¨¡å‹(å¦‚T5)ä»¥åŠåˆ©ç”¨ç»“æ„åŒ–ä¿¡æ¯çš„å›¾ç¥ç»ç½‘ç»œ(GNNs)ã€‚é’ˆå¯¹ç¥ç»é‡æ’åºå™¨çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œç ”ç©¶åˆ†æäº†é€šè¿‡çŸ¥è¯†è’¸é¦(knowledge distillation)æ„å»ºè½»é‡åŒ–æ¨¡å‹çš„æ•ˆç‡ä¼˜åŒ–æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œç»¼è¿°é‡ç‚¹æç»˜äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨é‡æ’åºé¢†åŸŸçš„æ–°å…´ç‰ˆå›¾ï¼Œè¯¦ç»†è€ƒå¯Ÿäº†å„ç§æç¤ºç­–ç•¥(prompting strategies)å’Œå¾®è°ƒæ‰‹æ®µ(fine-tuning tactics)ã€‚é€šè¿‡å¯¹ä¸åŒé‡æ’åºèŒƒå¼çš„æ ¸å¿ƒåŸç†ã€æœ‰æ•ˆæ€§ã€è®¡ç®—ç‰¹å¾åŠå®é™…æƒè¡¡è¿›è¡Œç»“æ„åŒ–ç»¼åˆï¼Œè¯¥ç ”ç©¶ä¸ºç†è§£å„ç±»ç­–ç•¥çš„ä¼˜åŠ£æä¾›äº†æ¸…æ™°çš„è§†è§’ï¼Œä¸ºæ„å»ºé«˜æ€§èƒ½æ£€ç´¢ç³»ç»Ÿæä¾›äº†å…¨é¢æŒ‡å—ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "15 pages, 1 figure, Accepted in CLNLP'25",
      "pdf_url": "https://arxiv.org/pdf/2512.16236v1",
      "published_date": "2025-12-18 06:29:37 UTC",
      "updated_date": "2025-12-18 06:29:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:10:18.434264+00:00"
    },
    {
      "arxiv_id": "2512.16235v1",
      "title": "AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection",
      "title_zh": "äººå·¥æ™ºèƒ½é©±åŠ¨çš„çš®è‚¤ç—…è¯Šæ–­ï¼šä»å¯è§£é‡Šæ¨¡å‹åˆ°ä¸´åºŠåº”ç”¨â€”â€”æ™®æƒ ä¸”å¯ä¿¡çš„çš®è‚¤ç—…æ£€æµ‹ç»¼åˆæ¡†æ¶",
      "authors": [
        "Satya Narayana Panda",
        "Vaishnavi Kukkala",
        "Spandana Iyer"
      ],
      "abstract": "Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?\n  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.\n  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªç»¼åˆæ€§çš„å¤šæ¨¡æ€ AI æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³çš®è‚¤ç—…è¯Šæ–­ä¸­ä¸“å®¶ç¨€ç¼ºåŠå®¶æ—å²åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆæ·±åº¦å­¦ä¹ å›¾åƒåˆ†æä¸ç»“æ„åŒ–ä¸´åºŠæ•°æ®ï¼Œåˆ©ç”¨å¯è§£é‡Šçš„å·ç§¯ç¥ç»ç½‘ç»œ (Convolutional Neural Networks) ä¸é›†æˆé—ä¼ é£é™©å› ç´ çš„ä¸´åºŠå†³ç­–æ ‘ï¼Œå®ç°äº†å½±åƒä¸å®¶æ—ç—…å²çš„ååŒå¤„ç†ã€‚éªŒè¯ç»“æœè¡¨æ˜ï¼Œåœ¨æ•´åˆå®¶æ—å²æ•°æ®åï¼Œè¯¥ç³»ç»Ÿå¯¹é»‘è‰²ç´ ç˜¤ (Melanoma)ã€é“¶å±‘ç—… (Psoriasis) å’Œç‰¹åº”æ€§çš®ç‚ (Atopic Dermatitis) ç­‰é—ä¼ æ€§çš®è‚¤ç—…çš„è¯Šæ–­å‡†ç¡®ç‡æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¯è§£é‡Š AI (Explainable AI) æœºåˆ¶å¢å¼ºäº†è¯Šæ–­çš„é€æ˜åº¦ï¼Œç¡®ä¿å…¶èƒ½æ— ç¼é›†æˆåˆ°ä¸´åºŠå·¥ä½œæµä¸­ã€‚ä¸“å®¶åé¦ˆæ˜¾ç¤ºè¯¥ç³»ç»Ÿåœ¨æ—©æœŸæ£€æµ‹å’Œä¸ªæ€§åŒ–æ²»ç–—å»ºè®®æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä¸ºå®ç°å¯ä¿¡ä¸”æ™®åŠçš„çš®è‚¤ç—…è¾…åŠ©è¯Šæ–­æä¾›äº†é‡è¦çš„æŠ€æœ¯æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 5 figures, 1 table. Code available at https://github.com/colabre2020/Enhancing-Skin-Disease-Diagnosis",
      "pdf_url": "https://arxiv.org/pdf/2512.16235v1",
      "published_date": "2025-12-18 06:28:51 UTC",
      "updated_date": "2025-12-18 06:28:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:10:26.654675+00:00"
    },
    {
      "arxiv_id": "2512.16227v1",
      "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
      "title_zh": "é¢å‘é²æ£’å¤§è¯­è¨€æ¨¡å‹ç¼–è¾‘çš„ä¿¡æ¯è®ºæ¡†æ¶",
      "authors": [
        "Qizhou Chen",
        "Chengyu Wang",
        "Taolin Zhang",
        "Xiaofeng He"
      ],
      "abstract": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºä¿¡æ¯ç“¶é¢ˆç†è®º (information bottleneck theory) çš„é²æ£’å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç¼–è¾‘æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç¼–è¾‘æŠ€æœ¯æ³›åŒ–èƒ½åŠ›ä¸è¶³åŠæ˜“å¼•å‘æ„å¤–å‰¯ä½œç”¨çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç²¾ç¡®å‹ç¼©å’Œéš”ç¦»çŸ¥è¯†ä¿®æ­£æ‰€éœ€çš„å…³é”®ä¿¡æ¯ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘äº†å¯¹æ¨¡å‹æ— å…³è¡Œä¸ºçš„å¹²æ‰°ã€‚åŸºäºæ­¤ç†è®ºï¼Œä½œè€…å¼€å‘äº† Information Bottleneck Knowledge Editor (IBKE)ï¼Œåˆ©ç”¨ç´§å‡‘çš„éšæ€§è¡¨ç¤º (latent representations) å¼•å¯¼åŸºäºæ¢¯åº¦çš„æ›´æ–°ï¼Œå®ç°äº†é«˜æ•ˆä¸”é€‚ç”¨æ€§å¹¿æ³›çš„æ¨¡å‹ç¼–è¾‘ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒIBKE åœ¨å¤šç§ LLM æ¶æ„å’Œæ ‡å‡†åŸºå‡†ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº† state-of-the-art çš„å‡†ç¡®ç‡ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†ç¼–è¾‘çš„æ³›åŒ–æ€§ (generality) å’Œç‰¹å¼‚æ€§ (specificity)ï¼Œä¸ºå¼€æ”¾åŸŸçŸ¥è¯†ç¼–è¾‘å»ºç«‹äº†ä¸€ä¸ªç†è®ºä¸¥è°¨ä¸”å®ç”¨çš„èŒƒå¼ã€‚è¿™ä¸€æˆæœæœ‰æ•ˆå¢å¼ºäº† LLMs åœ¨ç°å®åº”ç”¨ä¸­çš„å®ç”¨æ€§ä¸å¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16227v1",
      "published_date": "2025-12-18 06:21:17 UTC",
      "updated_date": "2025-12-18 06:21:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:11:08.646361+00:00"
    },
    {
      "arxiv_id": "2512.16221v1",
      "title": "Neural emulation of gravity-driven geohazard runout",
      "title_zh": "é‡åŠ›é©±åŠ¨åœ°è´¨ç¾å®³è¿ç§»è¿‡ç¨‹çš„ç¥ç»æ¨¡æ‹Ÿ",
      "authors": [
        "Lorenzo Nava",
        "Ye Chen",
        "Maximillian Van Wyk de Vries"
      ],
      "abstract": "Predicting geohazard runout is critical for protecting lives, infrastructure and ecosystems. Rapid mass flows, including landslides and avalanches, cause several thousand deaths across a wide range of environments, often travelling many kilometres from their source. The wide range of source conditions and material properties governing these flows makes their runout difficult to anticipate, particularly for downstream communities that may be suddenly exposed to severe impacts. Accurately predicting runout at scale requires models that are both physically realistic and computationally efficient, yet existing approaches face a fundamental speed-realism trade-off. Here we train a machine learning model to predict geohazard runout across representative real world terrains. The model predicts both flow extent and deposit thickness with high accuracy and 100 to 10,000 times faster computation than numerical solvers. It is trained on over 100,000 numerical simulations across over 10,000 real world digital elevation model chips and reproduces key physical behaviours, including avulsion and deposition patterns, while generalizing across different flow types, sizes and landscapes. Our results demonstrate that neural emulation enables rapid, spatially resolved runout prediction across diverse real world terrains, opening new opportunities for disaster risk reduction and impact-based forecasting. These results highlight neural emulation as a promising pathway for extending physically realistic geohazard modelling to spatial and temporal scales relevant for large scale early warning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ°è´¨ç¾å®³æ‰©æ•£(geohazard runout)é¢„æµ‹ä¸­ç‰©ç†çœŸå®æ„Ÿä¸è®¡ç®—æ•ˆç‡çš„æƒè¡¡éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œä»¿çœŸ(neural emulation)çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨è¶…è¿‡10,000ä¸ªçœŸå®ä¸–ç•Œæ•°å­—é«˜ç¨‹æ¨¡å‹(digital elevation model)åˆ‡ç‰‡å’Œ100,000æ¬¡æ•°å€¼æ¨¡æ‹ŸåŸºç¡€ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿä»¥é«˜ç²¾åº¦é¢„æµ‹æµåŠ¨èŒƒå›´å’Œå †ç§¯åšåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶è®¡ç®—é€Ÿåº¦æ¯”ä¼ ç»Ÿæ•°å€¼æ±‚è§£å™¨(numerical solvers)å¿«100è‡³10,000å€ï¼Œå¹¶èƒ½å‡†ç¡®å¤ç°æº¢æµ(avulsion)å’Œæ²‰ç§¯æ¨¡å¼ç­‰å…³é”®ç‰©ç†è¡Œä¸ºã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç¥ç»ç½‘ç»œä»¿çœŸåœ¨å¤šæ ·åŒ–ç°å®åœ°å½¢ä¸­å®ç°å¿«é€Ÿã€ç©ºé—´è§£ææ‰©æ•£é¢„æµ‹çš„å¯è¡Œæ€§ï¼Œä¸ºå¤§è§„æ¨¡æ—©æœŸé¢„è­¦ç³»ç»Ÿ(early warning systems)å’Œç¾å®³é£é™©è¯„ä¼°æä¾›äº†ç‰©ç†çœŸå®ä¸”é«˜æ•ˆçš„å»ºæ¨¡è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16221v1",
      "published_date": "2025-12-18 06:10:33 UTC",
      "updated_date": "2025-12-18 06:10:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:11:10.079261+00:00"
    },
    {
      "arxiv_id": "2512.16214v2",
      "title": "PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving",
      "title_zh": "PDE-Agentï¼šé¢å‘åå¾®åˆ†æ–¹ç¨‹æ±‚è§£çš„å·¥å…·é“¾å¢å¼ºå‹å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Jianming Liu",
        "Ren Zhu",
        "Jian Xu",
        "Kun Ding",
        "Xu-Yao Zhang",
        "Gaofeng Meng",
        "Cheng-Lin Liu"
      ],
      "abstract": "Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PDE-Agentï¼Œä¸€ç§ç”¨äºæ±‚è§£åå¾®åˆ†æ–¹ç¨‹(PDE)çš„å·¥å…·é“¾å¢å¼ºå‹å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•å’Œç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(PINNs)åœ¨è‡ªä¸»æ€§æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶å°†PDEæ±‚è§£å»ºæ¨¡ä¸ºç”±å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„å·¥å…·è°ƒç”¨è¿‡ç¨‹ï¼Œå®ç°äº†ä»è‡ªç„¶è¯­è¨€æè¿°åˆ°è‡ªåŠ¨åŒ–æ±‚è§£çš„è·¨è¶Šã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬åŸºäºå›¾å­˜å‚¨(graph memory)çš„Prog-Actåä½œæ¡†æ¶ï¼Œé€šè¿‡åŒé—­ç¯(dual-loop)æœºåˆ¶å®ç°åŠ¨æ€è§„åˆ’ä¸è‡ªåŠ¨çº é”™ï¼›ä»¥åŠé›†æˆäº†å·¥å…·å‚æ•°åˆ†ç¦»æœºåˆ¶çš„èµ„æºæ± (Resource-Pool)ï¼Œæœ‰æ•ˆè§£å†³äº†è·¨å·¥å…·ä¾èµ–å’Œè¿è¡Œæ—¶äº§ç‰©ç®¡ç†éš¾é¢˜ã€‚ç ”ç©¶åŒæ­¥æ¨å‡ºäº†PDE-BenchåŸºå‡†æµ‹è¯•ä¸å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ï¼Œå®éªŒç»“æœè¯å®PDE-Agentåœ¨å¤„ç†å¤æ‚å¤šæ­¥åŠè·¨æ­¥ä¾èµ–ä»»åŠ¡ä¸­å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—æ¨è¿›äº†è‡ªåŠ¨åŒ–ç§‘å­¦è®¡ç®—(automated scientific computing)çš„å‘å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Adding Affiliation Information on arXiv",
      "pdf_url": "https://arxiv.org/pdf/2512.16214v2",
      "published_date": "2025-12-18 06:02:50 UTC",
      "updated_date": "2025-12-22 03:24:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:11:21.423345+00:00"
    },
    {
      "arxiv_id": "2512.20655v1",
      "title": "MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing",
      "title_zh": "MaskOptï¼šæ¨åŠ¨é›†æˆç”µè·¯åˆ¶é€ äººå·¥æ™ºèƒ½å‘å±•çš„å¤§è§„æ¨¡æ©æ¨¡ä¼˜åŒ–æ•°æ®é›†",
      "authors": [
        "Yuting Hu",
        "Lei Zhuang",
        "Hua Xiang",
        "Jinjun Xiong",
        "Gi-Joon Nam"
      ],
      "abstract": "As integrated circuit (IC) dimensions shrink below the lithographic wavelength, optical lithography faces growing challenges from diffraction and process variability. Model-based optical proximity correction (OPC) and inverse lithography technique (ILT) remain indispensable but computationally expensive, requiring repeated simulations that limit scalability. Although deep learning has been applied to mask optimization, existing datasets often rely on synthetic layouts, disregard standard-cell hierarchy, and neglect the surrounding contexts around the mask optimization targets, thereby constraining their applicability to practical mask optimization. To advance deep learning for cell- and context-aware mask optimization, we present MaskOpt, a large-scale benchmark dataset constructed from real IC designs at the 45$\\mathrm{nm}$ node. MaskOpt includes 104,714 metal-layer tiles and 121,952 via-layer tiles. Each tile is clipped at a standard-cell placement to preserve cell information, exploiting repeated logic gate occurrences. Different context window sizes are supported in MaskOpt to capture the influence of neighboring shapes from optical proximity effects. We evaluate state-of-the-art deep learning models for IC mask optimization to build up benchmarks, and the evaluation results expose distinct trade-offs across baseline models. Further context size analysis and input ablation studies confirm the importance of both surrounding geometries and cell-aware inputs in achieving accurate mask generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MaskOptï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº 45nm çœŸå®é›†æˆç”µè·¯è®¾è®¡çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ åœ¨ Mask Optimization ä»»åŠ¡ä¸­é¢ä¸´çš„æ•°æ®é›†å±€é™æ€§ã€‚é’ˆå¯¹ä¼ ç»Ÿ Optical Proximity Correction (OPC) å’Œ Inverse Lithography Technique (ILT) è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼ŒMaskOpt å…‹æœäº†ç°æœ‰ç ”ç©¶ä¾èµ–åˆæˆå¸ƒå±€ã€å¿½è§† Standard-Cell å±‚çº§ä»¥åŠç¼ºä¹ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç¼ºé™·ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ 10 ä¸‡ä¸ªé‡‘å±å±‚å’Œé€šå­”å±‚ Tilesï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é‚»è¿‘å½¢çŠ¶å¼•èµ·çš„ Optical Proximity Effectsï¼Œå¹¶æ”¯æŒä¸åŒå°ºå¯¸çš„ Context Windowã€‚ç ”ç©¶é€šè¿‡è¯„ä¼°å¤šç§ SOTA æ·±åº¦å­¦ä¹ æ¨¡å‹å»ºç«‹äº†æ€§èƒ½åŸºå‡†ï¼Œåˆ†æç»“æœæ­ç¤ºäº†å„åŸºçº¿æ¨¡å‹åœ¨ç²¾åº¦ä¸æ•ˆç‡ä¸Šçš„æƒè¡¡ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œç»“åˆå‘¨å›´å‡ ä½•å½¢çŠ¶çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸ Cell-Aware è¾“å…¥æ˜¯å®ç°é«˜ç²¾åº¦æ©æ¨¡ä¼˜åŒ–çš„å…³é”®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.20655v1",
      "published_date": "2025-12-18 05:53:45 UTC",
      "updated_date": "2025-12-18 05:53:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:11:26.286997+00:00"
    },
    {
      "arxiv_id": "2512.16202v1",
      "title": "Open Ad-hoc Categorization with Contextualized Feature Learning",
      "title_zh": "åŸºäºè¯­å¢ƒåŒ–ç‰¹å¾å­¦ä¹ çš„å¼€æ”¾å¼å³æ—¶åˆ†ç±»",
      "authors": [
        "Zilin Wang",
        "Sangwoo Mo",
        "Stella X. Yu",
        "Sima Behpour",
        "Liu Ren"
      ],
      "abstract": "Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.\n  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.\n  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰åœºæ™¯çš„å¼€æ”¾å¼ä¸´æ—¶åˆ†ç±» (Open Ad-hoc Categorization) é—®é¢˜ï¼Œæ—¨åœ¨è®©äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“èƒ½æ ¹æ®ç‰¹å®šç›®æ ‡åŠ¨æ€åˆ›å»ºåˆ†ç±»ï¼Œè€Œéå±€é™äºé¢„å®šä¹‰çš„å›ºå®šç±»åˆ«ã€‚åœ¨ç»™å®šå°‘é‡æ ‡è®°ç¤ºä¾‹å’Œå¤§é‡æœªæ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¯¥ä»»åŠ¡çš„ç›®æ ‡æ˜¯é€šè¿‡è¯­ä¹‰æ‰©å±•å’Œè§†è§‰èšç±»å‘ç°æ½œåœ¨è¯­å¢ƒå¹¶æ‰©å±•è¿™äº›ä¸´æ—¶ç±»åˆ«ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† OAK æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å†»ç»“çš„ CLIP è¾“å…¥ç«¯å¼•å…¥äº†ä¸€ç»„å¯å­¦ä¹ çš„è¯­å¢ƒæ ‡è®° (Context Tokens)ï¼Œå¹¶ç»“åˆäº† CLIP çš„å›¾æ–‡å¯¹é½ç›®æ ‡ä¸ GCD çš„è§†è§‰èšç±»ç›®æ ‡è¿›è¡Œä¼˜åŒ–ã€‚åœ¨ Stanford å’Œ Clevr-4 æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOAK åœ¨å‡†ç¡®ç‡å’Œæ¦‚å¿µå‘ç°æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ï¼Œå…¶ä¸­åœ¨ Stanford Mood ä»»åŠ¡ä¸Šçš„æ–°ç±»åˆ«å‡†ç¡®ç‡è¾¾åˆ° 87.4%ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒOAK è¿˜èƒ½ç”Ÿæˆå…·æœ‰å¯è§£é‡Šæ€§çš„æ˜¾è‘—å›¾ (Saliency Maps)ï¼Œèƒ½é’ˆå¯¹ä¸åŒä»»åŠ¡ç²¾å‡†èšç„¦å…³é”®ç‰¹å¾ï¼Œå¦‚åœ¨åŠ¨ä½œè¯†åˆ«ä¸­å…³æ³¨æ‰‹éƒ¨ï¼Œåœ¨æƒ…ç»ªè¯†åˆ«ä¸­å…³æ³¨é¢éƒ¨ã€‚è¿™ç§æœºåˆ¶ä¸ä»…æé«˜äº†åˆ†ç±»çš„è‡ªé€‚åº”èƒ½åŠ›å’Œæ³›åŒ–æ€§ï¼Œè¿˜é€šè¿‡é€æ˜çš„å†³ç­–è¿‡ç¨‹å¢å¼ºäº†ç³»ç»Ÿçš„å¯ä¿¡åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages, 17 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16202v1",
      "published_date": "2025-12-18 05:49:15 UTC",
      "updated_date": "2025-12-18 05:49:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:11:09.633534+00:00"
    },
    {
      "arxiv_id": "2601.04214v1",
      "title": "Active Sensing Shapes Real-World Decision-Making through Dynamic Evidence Accumulation",
      "title_zh": "ä¸»åŠ¨æ„ŸçŸ¥é€šè¿‡åŠ¨æ€è¯æ®ç§¯ç´¯å¡‘é€ ç°å®ä¸–ç•Œå†³ç­–",
      "authors": [
        "Hongliang Lu",
        "Yunmeng Liu",
        "Junjie Yang"
      ],
      "abstract": "Human decision-making heavily relies on active sensing, a well-documented cognitive behaviour for evidence gathering to accommodate ever-changing environments. However, its operational mechanism in the real world remains non-trivial. Currently, an in-laboratory paradigm, called evidence accumulation modelling (EAM), points out that human decision-making involves transforming external evidence into internal mental beliefs. However, the gap in evidence affordance between real-world contexts and laboratory settings hinders the effective application of EAM. Here we generalize EAM to the real world and conduct analysis in real-world driving scenarios. A cognitive scheme is proposed to formalize real-world evidence affordance and capture active sensing through eye movements. Empirically, our scheme can plausibly portray the accumulation of drivers' mental beliefs, explaining how active sensing transforms evidence into mental beliefs from the perspective of information utility. Also, our results demonstrate a negative correlation between evidence affordance and attention recruited by individuals, revealing how human drivers adapt their evidence-collection patterns across various contexts. Moreover, we reveal the positive influence of evidence affordance and attention distribution on decision-making propensity. In a nutshell, our computational scheme generalizes EAM to real-world contexts and provides a comprehensive account of how active sensing underlies real-world decision-making, unveiling multifactorial, integrated characteristics in real-world decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸»åŠ¨æ„ŸçŸ¥(Active Sensing)å¦‚ä½•é€šè¿‡åŠ¨æ€è¯æ®ç§¯ç´¯å½±å“ç°å®ä¸–ç•Œçš„å†³ç­–è¿‡ç¨‹ï¼Œæ—¨åœ¨å¼¥è¡¥è¯æ®ç§¯ç´¯æ¨¡å‹(Evidence Accumulation Modelling, EAM)åœ¨å®éªŒå®¤èŒƒå¼ä¸çœŸå®åœºæ™¯åº”ç”¨ä¹‹é—´çš„å·®è·ã€‚ç ”ç©¶è€…åŸºäºçœŸå®é©¾é©¶åœºæ™¯æå‡ºäº†ä¸€ç§è®¤çŸ¥æ–¹æ¡ˆï¼Œåˆ©ç”¨çœ¼åŠ¨è¿½è¸ªæ•æ‰ä¸»åŠ¨æ„ŸçŸ¥ï¼Œå¹¶å¯¹ç°å®ä¸–ç•Œçš„è¯æ®æä¾›(Evidence Affordance)è¿›è¡Œäº†å½¢å¼åŒ–æè¿°ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆèƒ½ä»ä¿¡æ¯æ•ˆç”¨(Information Utility)çš„è§’åº¦æœ‰æ•ˆè§£é‡Šä¸»åŠ¨æ„ŸçŸ¥å¦‚ä½•å°†å¤–éƒ¨è¯æ®è½¬åŒ–ä¸ºå†…éƒ¨å¿ƒç†ä¿¡å¿µã€‚ç ”ç©¶æ­ç¤ºäº†è¯æ®æä¾›ä¸ä¸ªä½“åˆ†é…çš„æ³¨æ„åŠ›ä¹‹é—´å­˜åœ¨è´Ÿç›¸å…³å…³ç³»ï¼Œå±•ç¤ºäº†é©¾é©¶å‘˜åœ¨ä¸åŒè¯­å¢ƒä¸‹å¦‚ä½•é€‚åº”æ€§åœ°è°ƒæ•´è¯æ®æ”¶é›†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œç»“æœè¯å®äº†è¯æ®æä¾›å’Œæ³¨æ„åŠ›åˆ†å¸ƒå¯¹å†³ç­–å€¾å‘å…·æœ‰æ­£å‘å½±å“ã€‚è¯¥è®¡ç®—æ–¹æ¡ˆæˆåŠŸåœ°å°†EAMæ¨å¹¿è‡³ç°å®è¯­å¢ƒï¼Œä¸ºä¸»åŠ¨æ„ŸçŸ¥å¦‚ä½•æ„æˆå¤æ‚ç°å®å†³ç­–çš„åŸºç¡€æä¾›äº†å…¨é¢çš„è®¡ç®—æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.RO",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.04214v1",
      "published_date": "2025-12-18 05:45:53 UTC",
      "updated_date": "2025-12-18 05:45:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:11:27.395797+00:00"
    },
    {
      "arxiv_id": "2512.16185v1",
      "title": "Weighted K-Harmonic Means Clustering: Convergence Analysis and Applications to Wireless Communications",
      "title_zh": "åŠ æƒ K-è°ƒå’Œå‡å€¼èšç±»ï¼šæ”¶æ•›æ€§åˆ†æåŠå…¶åœ¨æ— çº¿é€šä¿¡ä¸­çš„åº”ç”¨",
      "authors": [
        "Gourab Ghatak"
      ],
      "abstract": "We propose the \\emph{weighted K-harmonic means} (WKHM) clustering algorithm, a regularized variant of K-harmonic means designed to ensure numerical stability while enabling soft assignments through inverse-distance weighting. Unlike classical K-means and constrained K-means, WKHM admits a direct interpretation in wireless networks: its weights are exactly equivalent to fractional user association based on received signal strength. We establish rigorous convergence guarantees under both deterministic and stochastic settings, addressing key technical challenges arising from non-convexity and random initialization. Specifically, we prove monotone descent to a local minimum under fixed initialization, convergence in probability under Binomial Point Process (BPP) initialization, and almost sure convergence under mild decay conditions. These results provide the first stochastic convergence guarantees for harmonic-mean-based clustering. Finally, through extensive simulations with diverse user distributions, we show that WKHM achieves a superior tradeoff between minimum signal strength and load fairness compared to classical and modern clustering baselines, making it a principled tool for joint radio node placement and user association in wireless networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åŠ æƒK-è°æ³¢å‡å€¼(Weighted K-harmonic means, WKHM)èšç±»ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ç¡®ä¿æ•°å€¼ç¨³å®šæ€§å¹¶é€šè¿‡é€†è·ç¦»åŠ æƒå®ç°è½¯åˆ†é…(soft assignments)çš„æ­£åˆ™åŒ–å˜ä½“ã€‚ä¸ä¼ ç»Ÿçš„K-meansä¸åŒï¼ŒWKHMåœ¨æ— çº¿ç½‘ç»œä¸­å…·æœ‰æ˜ç¡®çš„ç‰©ç†è§£é‡Šï¼Œå…¶æƒé‡å®Œå…¨ç­‰åŒäºåŸºäºæ¥æ”¶ä¿¡å·å¼ºåº¦çš„åˆ†æ•°ç”¨æˆ·å…³è”(fractional user association)ã€‚ä½œè€…åœ¨ç¡®å®šæ€§å’Œéšæœºåœºæ™¯ä¸‹å‡å»ºç«‹äº†ä¸¥æ ¼çš„æ”¶æ•›æ€§ä¿è¯ï¼ŒåŒ…æ‹¬è¯æ˜äº†åœ¨å›ºå®šåˆå§‹åŒ–ä¸‹çš„å•è°ƒä¸‹é™ä»¥åŠåœ¨äºŒé¡¹ç‚¹è¿‡ç¨‹(Binomial Point Process, BPP)åˆå§‹åŒ–ä¸‹çš„æ¦‚ç‡æ”¶æ•›ä¸å‡ ä¹å¤„å¤„æ”¶æ•›ã€‚è¿™äº›æˆæœä¸ºåŸºäºè°æ³¢å‡å€¼çš„èšç±»æä¾›äº†é¦–ä¸ªéšæœºæ”¶æ•›æ€§åˆ†æï¼Œæœ‰æ•ˆè§£å†³äº†ç”±éå‡¸æ€§å’Œéšæœºåˆå§‹åŒ–å¸¦æ¥çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚ä»¿çœŸå®éªŒæ˜¾ç¤ºï¼ŒWKHMåœ¨æœ€å°ä¿¡å·å¼ºåº¦ä¸è´Ÿè½½å…¬å¹³æ€§ä¹‹é—´è¾¾æˆäº†æ¯”ç°æœ‰åŸºå‡†æ¨¡å‹æ›´ä¼˜çš„å¹³è¡¡ã€‚è¯¥ç®—æ³•ä¸ºæ— çº¿ç½‘ç»œä¸­çš„æ— çº¿ç”µèŠ‚ç‚¹éƒ¨ç½²ä¸ç”¨æˆ·å…³è”æä¾›äº†ä¸€ä¸ªå…¼å…·ç†è®ºæ”¯æ’‘ä¸å®è·µæ€§èƒ½çš„å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16185v1",
      "published_date": "2025-12-18 05:09:56 UTC",
      "updated_date": "2025-12-18 05:09:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:11:30.181342+00:00"
    },
    {
      "arxiv_id": "2601.11559v1",
      "title": "MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?",
      "title_zh": "MIMIC-RDï¼šå¤§è¯­è¨€æ¨¡å‹èƒ½å¦åœ¨çœŸå®ä¸–ç•Œä¸´åºŠåœºæ™¯ä¸‹è¿›è¡Œç½•è§ç—…çš„é‰´åˆ«è¯Šæ–­ï¼Ÿ",
      "authors": [
        "Zilal Eiz AlDin",
        "John Wu",
        "Jeffrey Paul Fung",
        "Jennifer King",
        "Mya Watts",
        "Lauren ONeill",
        "Adam Richard Cross",
        "Jimeng Sun"
      ],
      "abstract": "Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨çœŸå®ä¸´åºŠç¯å¢ƒä¸‹å¯¹ç½•è§ç—…è¿›è¡Œé‰´åˆ«è¯Šæ–­(Differential Diagnosis)çš„èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æ–¹æ³•ä¾èµ–ç†æƒ³åŒ–æ¡ˆä¾‹æˆ–ICDä»£ç æ˜ å°„å¯¼è‡´æ•°æ®åå·®çš„é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†MIMIC-RDï¼Œä¸€ä¸ªé€šè¿‡å°†ä¸´åºŠæ–‡æœ¬ç›´æ¥æ˜ å°„åˆ°Orphanetæ•°æ®åº“æ„å»ºçš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨LLMåˆæ­¥æŒ–æ˜å¹¶ç»“åˆå››ååŒ»å­¦è¯„ä¼°å‘˜çš„éªŒè¯ï¼Œåœ¨145åæ‚£è€…çš„æ•°æ®é›†ä¸Šå¯¹å¤šç§å‰æ²¿æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œç›®å‰çš„SOTA LLMsåœ¨ç½•è§ç—…é‰´åˆ«è¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œåæ˜ å‡ºæ¨¡å‹èƒ½åŠ›ä¸å®é™…ä¸´åºŠéœ€æ±‚ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ç°æœ‰æŠ€æœ¯çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥æå‡ç½•è§ç—…è¯Šæ–­å‡†ç¡®æ€§æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.11559v1",
      "published_date": "2025-12-18 05:03:49 UTC",
      "updated_date": "2025-12-18 05:03:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:11:38.708651+00:00"
    },
    {
      "arxiv_id": "2512.16178v1",
      "title": "Towards Closing the Domain Gap with Event Cameras",
      "title_zh": "è¿ˆå‘åˆ©ç”¨äº‹ä»¶ç›¸æœºå¼¥åˆåŸŸé—´å·®å¼‚",
      "authors": [
        "M. Oltan Sevinc",
        "Liao Wu",
        "Francisco Cruz"
      ],
      "abstract": "Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç«¯åˆ°ç«¯é©¾é©¶ä¸­ä¼ ç»Ÿæ‘„åƒæœºå› è®­ç»ƒä¸éƒ¨ç½²ç¯å¢ƒä¸åŒ¹é…ï¼ˆå³ domain gapï¼‰è€Œå¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ˜¼å¤œå…‰ç…§å·®å¼‚å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºä½¿ç”¨ event cameras ä½œä¸ºæ½œåœ¨çš„æ›¿ä»£ä¼ æ„Ÿå™¨ï¼Œæ—¨åœ¨ä¸è¿›è¡Œé¢å¤–è°ƒæ•´çš„æƒ…å†µä¸‹è·¨å…‰ç…§æ¡ä»¶ä¿æŒæ€§èƒ½ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œevent cameras åœ¨ä¸åŒå…‰ç…§ç¯å¢ƒä¸‹å±•ç°å‡ºæ›´ä¸€è‡´çš„è¡¨ç°ï¼Œå…¶ domain-shift penalties é€šå¸¸ä¼˜äºæˆ–ç­‰åŒäºä¼ ç»Ÿçš„ grayscale framesã€‚ç ”ç©¶æœ€ç»ˆè¯å®äº† event cameras åœ¨ cross-domain åœºæ™¯ä¸­å…·å¤‡æ›´ä¼˜è¶Šçš„åŸºå‡†æ€§èƒ½ï¼Œä¸ºç¼©å°è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„ç¯å¢ƒå·®å¼‚æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to Australasian Conference on Robotics and Automation (ACRA), 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.16178v1",
      "published_date": "2025-12-18 04:57:32 UTC",
      "updated_date": "2025-12-18 04:57:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:11:35.579920+00:00"
    },
    {
      "arxiv_id": "2512.16171v1",
      "title": "Science Consultant Agent",
      "title_zh": "ç§‘å­¦é¡¾é—®æ™ºèƒ½ä½“",
      "authors": [
        "Karthikeyan K",
        "Philip Wu",
        "Xin Tang",
        "Alexandre Alves"
      ],
      "abstract": "The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†Science Consultant Agentï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºWebçš„äººå·¥æ™ºèƒ½AIå·¥å…·ï¼Œæ—¨åœ¨å¸®åŠ©ä»ä¸šè€…ä¸ºAI-based solutionsé€‰æ‹©å¹¶å®æ–½æœ€æœ‰æ•ˆçš„å»ºæ¨¡ç­–ç•¥ã€‚è¯¥å·¥å…·ç”±å››ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼ŒåŒ…æ‹¬Questionnaireã€Smart Fillã€Research-Guided Recommendationä»¥åŠPrototype Builderã€‚é€šè¿‡æ•´åˆç»“æ„åŒ–é—®å·ã€åŸºäºæ–‡çŒ®çš„è§£å†³æ–¹æ¡ˆå»ºè®®å’ŒåŸå‹ç”ŸæˆåŠŸèƒ½ï¼ŒScience Consultant Agentèƒ½å¤Ÿæ˜¾è‘—åŠ é€Ÿä»äº§å“ç»ç†ã€è½¯ä»¶å¼€å‘äººå‘˜åˆ°ç ”ç©¶äººå‘˜çš„å¼€å‘æµç¨‹ã€‚è¯¥å·¥å…·çš„Pipelineå°†éœ€æ±‚é‡‡é›†ä¸ç ”ç©¶é©±åŠ¨çš„è¾…åŠ©å†³ç­–ç›¸ç»“åˆï¼Œä¸ºAIæ–¹æ¡ˆçš„è½åœ°æä¾›äº†ç³»ç»ŸåŒ–çš„è·¯å¾„ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºé™ä½äº†æ¨¡å‹é€‰æ‹©çš„é—¨æ§›ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–æ‰‹æ®µæå‡äº†è·¨è§’è‰²å›¢é˜Ÿåœ¨æ„å»ºæ™ºèƒ½ç³»ç»Ÿæ—¶çš„ç ”å‘æ•ˆç‡ä¸å®æ–½å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16171v1",
      "published_date": "2025-12-18 04:46:42 UTC",
      "updated_date": "2025-12-18 04:46:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:11:50.472208+00:00"
    },
    {
      "arxiv_id": "2512.16167v1",
      "title": "Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services",
      "title_zh": "Ev-Trustï¼šé¢å‘åŸºäº LLM çš„å¤šæ™ºèƒ½ä½“æœåŠ¡æ¼”åŒ–åšå¼ˆçš„ç­–ç•¥å‡è¡¡ä¿¡ä»»æœºåˆ¶",
      "authors": [
        "Shiduo Yang",
        "Jiye Wang",
        "Jiayu Qin",
        "Jianbin Li",
        "Yu Wang",
        "Yuanhe Zhao",
        "Kenan Guo"
      ],
      "abstract": "The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized \"Request-Response-Payment-Evaluation\" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Ev-Trustï¼Œä¸€ç§åŸºäºæ¼”åŒ–åšå¼ˆè®º(Evolutionary Game Theory)çš„ç­–ç•¥å‡è¡¡ä¿¡ä»»æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å»ä¸­å¿ƒåŒ–ç¯å¢ƒä¸‹é¢ä¸´çš„æ¬ºè¯ˆå’Œè¯¯å¯¼ä¿¡æ¯ç­‰ä¿¡ä»»æŒ‘æˆ˜ã€‚è¯¥æœºåˆ¶å°†ç›´æ¥ä¿¡ä»»ã€é—´æ¥ä¿¡ä»»ä¸é¢„æœŸæ”¶ç›Šæ•´åˆè‡³åŠ¨æ€åé¦ˆç»“æ„ä¸­ï¼Œå¹¶åœ¨å»ä¸­å¿ƒåŒ–çš„â€œè¯·æ±‚-å“åº”-æ”¯ä»˜-è¯„ä¼°â€æœåŠ¡æ¡†æ¶å†…å¼•å¯¼æ™ºèƒ½ä½“çš„è¡Œä¸ºæ¼”åŒ–ã€‚é€šè¿‡åŸºäºå¤åˆ¶åŠ¨æ€æ–¹ç¨‹(Replicator Dynamics Equations)çš„ç†è®ºæ¨å¯¼ï¼Œç ”ç©¶è¯æ˜äº†ç³»ç»Ÿå­˜åœ¨å±€éƒ¨æ¼”åŒ–å‡è¡¡åŠå…¶ç¨³å®šæ€§ï¼Œç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ’é™¤æ¶æ„å‚ä¸è€…å¹¶å¼ºåŒ–åä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEv-Truståœ¨LLMé©±åŠ¨çš„å¼€æ”¾æœåŠ¡äº¤äº’åœºæ™¯ä¸­èƒ½æœ‰æ•ˆåæ˜ æ™ºèƒ½ä½“ä¿¡èª‰ï¼Œæ˜¾è‘—å‡å°‘æ¶æ„ç­–ç•¥çš„åº”ç”¨å¹¶æå‡é›†ä½“æ”¶ç›Šï¼Œä¸ºæ™ºèƒ½ä½“åŒ–WebæœåŠ¡çš„ä¿¡ä»»å»ºæ¨¡æä¾›äº†æ¼”åŒ–åšå¼ˆçš„æ–°è§†è§’ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.MA",
      "comment": "12 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16167v1",
      "published_date": "2025-12-18 04:39:13 UTC",
      "updated_date": "2025-12-18 04:39:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:12:04.484258+00:00"
    },
    {
      "arxiv_id": "2512.16164v1",
      "title": "C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation",
      "title_zh": "C-DGPAï¼šä»¥ç±»åˆ«ä¸ºä¸­å¿ƒçš„åŒé‡å¯¹é½ç”Ÿæˆå¼æç¤ºè‡ªé€‚åº”",
      "authors": [
        "Chao Li",
        "Dasha Hu",
        "Chengyang Li",
        "Yuming Jiang",
        "Yuncheng Shen"
      ],
      "abstract": "Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— ç›‘ç£åŸŸè‡ªé€‚åº”(Unsupervised Domain Adaptation)ä¸­è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)æç¤ºå¾®è°ƒé¢ä¸´çš„é¢†åŸŸå·®å¼‚æŒ‘æˆ˜ï¼Œæå‡ºäº†C-DGPAï¼ˆClass-Centric Dual-Alignment Generative Prompt Adaptationï¼‰æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½è§†æ¡ä»¶åˆ†å¸ƒå·®å¼‚å¯¼è‡´çš„ç±»åˆ«åŸå‹é”™ä½é—®é¢˜ï¼ŒC-DGPAé€šè¿‡åŒåˆ†æ”¯æ¶æ„ååŒä¼˜åŒ–è¾¹ç¼˜åˆ†å¸ƒå¯¹é½(marginal distribution alignment)å’Œæ¡ä»¶åˆ†å¸ƒå¯¹é½(conditional distribution alignment)ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŠ¨æ€å¯¹æŠ—è®­ç»ƒå¼¥è¡¥è¾¹ç¼˜åˆ†å¸ƒå·®å¼‚ï¼Œå¹¶å¼•å…¥ç±»åˆ«æ˜ å°„æœºåˆ¶(Class Mapping Mechanism)æ¥è§„èŒƒè¯­ä¹‰æç¤ºç†è§£ï¼Œä»è€Œé˜²æ­¢å¯¹æºåŸŸçš„è¿‡åº¦ä¾èµ–ã€‚è¿™ç§åŒé‡å¯¹é½ç­–ç•¥æˆåŠŸå°†é¢†åŸŸçŸ¥è¯†æ•´åˆåˆ°æç¤ºå­¦ä¹ ä¸­ï¼Œç¡®ä¿äº†æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé¢†åŸŸä¸å˜ä¸”å…·åˆ¤åˆ«æ€§çš„è¯­ä¹‰è¡¨ç¤ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒC-DGPAåœ¨OfficeHomeã€Office31å’ŒVisDA-2017ç­‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›æ°´å¹³(SOTA)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16164v1",
      "published_date": "2025-12-18 04:30:53 UTC",
      "updated_date": "2025-12-18 04:30:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:12:06.836894+00:00"
    },
    {
      "arxiv_id": "2512.17956v2",
      "title": "Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration",
      "title_zh": "Victor Calibration (VC)ï¼šåœ†æ¡Œç¼–æ’ä¸‹çš„å¤šè½®ç½®ä¿¡åº¦æ ¡å‡†ä¸ CP4.3 æ²»ç†å‹åŠ›æµ‹è¯•",
      "authors": [
        "Victor Stasiuc"
      ],
      "abstract": "Safety alignment can make frontier LMs overly conservative, degrading collaboration via hedging or false refusals. We present a lightweight toolkit with three parts: (1) Victor Calibration (VC), a multi-pass protocol that elicits a scalar confidence proxy T (T0<T1<T2) through iterative evidence re-evaluation; (2) FD-Lite, a behavior-only phenomenology audit with a fixed anchor phrase and a meta-prefix trap to avoid anthropomorphic claims; and (3) CP4.3, a governance stress test for rank invariance and allocation monotonicity (M6). Across Claude 4.5 models (Haiku, Sonnet no-thinking, Sonnet thinking) and Opus, we observe monotonic VC trajectories without violating safety invariants, and stable CP4.3 behavior. (\"Opus\" here refers to a single Claude Opus 4.1 session accessed via a standard UI account, as reported in Table 1.) This work was conducted by a single operator (n=1) and is intended as hypothesis-generating; we explicitly invite replication, critique, and extension by the research community. We include prompt templates and an artifact plan to facilitate independent verification.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®‰å…¨å¯¹é½(Safety alignment)å¯¼è‡´å¤§è¯­è¨€æ¨¡å‹(LMs)è¿‡åº¦ä¿å®ˆã€å‡ºç°å›é¿æ€§å›ç­”æˆ–é”™è¯¯æ‹’ç»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†çš„è½»é‡çº§å·¥å…·åŒ…ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬Victor Calibration (VC) å¤šè½®ç½®ä¿¡åº¦æ ¡å‡†åè®®ï¼Œé€šè¿‡è¿­ä»£å¼è¯æ®è¯„ä¼°æå–ç½®ä¿¡åº¦ä»£ç†ï¼›FD-Liteç°è±¡å­¦å®¡è®¡å·¥å…·ï¼Œåˆ©ç”¨å›ºå®šé”šç‚¹çŸ­è¯­å’Œå…ƒå‰ç¼€é™·é˜±é¿å…æ¨¡å‹äº§ç”Ÿæ‹ŸäººåŒ–é™ˆè¿°ï¼›ä»¥åŠCP4.3æ²»ç†å‹åŠ›æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°ç§©ä¸å˜æ€§(rank invariance)å’Œåˆ†é…å•è°ƒæ€§(allocation monotonicity)ã€‚åœ¨å¯¹Claude 4.5ç³»åˆ—æ¨¡å‹å’ŒOpusçš„å®éªŒä¸­ï¼Œç ”ç©¶è§‚å¯Ÿåˆ°äº†å•è°ƒçš„VCè½¨è¿¹ä¸”æœªè¿åå®‰å…¨ä¸å˜é‡ï¼Œå¹¶éªŒè¯äº†CP4.3è¡Œä¸ºçš„ç¨³å®šæ€§ã€‚è¯¥å·¥ä½œä¸ºå¤§æ¨¡å‹çš„ç½®ä¿¡åº¦æ ¡å‡†ä¸æ²»ç†æä¾›äº†å®éªŒè·¯å¾„ï¼Œç ”ç©¶è€…åŒæ—¶å…¬å¼€äº†Promptæ¨¡æ¿ä»¥ä¿ƒè¿›ç¤¾åŒºçš„ç‹¬ç«‹éªŒè¯ä¸æ‰©å±•ç ”ç©¶ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "7 pages, 1 figure, 4 tables. Exploratory case study",
      "pdf_url": "https://arxiv.org/pdf/2512.17956v2",
      "published_date": "2025-12-18 04:09:22 UTC",
      "updated_date": "2025-12-29 18:07:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:12:14.214212+00:00"
    },
    {
      "arxiv_id": "2512.16149v1",
      "title": "ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs",
      "title_zh": "ToolForgeï¼šä¸€ç§æ— éœ€çœŸå® API çš„å¤šè·³æœç´¢æ•°æ®åˆæˆæµæ°´çº¿",
      "authors": [
        "Hao Chen",
        "Zhexin Hu",
        "Jiajun Chai",
        "Haocheng Yang",
        "Hang He",
        "Xiaohan Wang",
        "Wei Lin",
        "Luhang Wang",
        "Guojun Yin",
        "Zhuofeng zhao"
      ],
      "abstract": "Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ToolForgeï¼Œä¸€ç§æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å·¥å…·è°ƒç”¨å’Œæ£€ç´¢å¢å¼ºä»»åŠ¡ä¸­é¢ä¸´çš„é«˜æ˜‚æ•°æ®è·å–æˆæœ¬åŠç¼ºä¹å¤æ‚æ¨ç†èƒ½åŠ›é—®é¢˜çš„è‡ªåŠ¨åŒ–åˆæˆæ¡†æ¶ã€‚ToolForgeçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºæ— éœ€çœŸå®çš„APIè°ƒç”¨ï¼Œè€Œæ˜¯é€šè¿‡æ„å»ºå°‘é‡è™šæ‹Ÿå·¥å…·å¹¶åˆ©ç”¨â€œé—®é¢˜-é»„é‡‘ä¸Šä¸‹æ–‡-ç­”æ¡ˆâ€(question, golden context, answer)ä¸‰å…ƒç»„æ¥åˆæˆå¤§è§„æ¨¡çš„å¤šè·³æœç´¢(multi-hop search)å­¦ä¹ æ•°æ®ã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥é›†æˆäº†å¤šè·³æ¨ç†(multi-hop reasoning)ä¸è‡ªæˆ‘åæ€(self-reflection)æœºåˆ¶ï¼Œå¹¶é‡‡ç”¨ç»“åˆè§„åˆ™ä¸æ¨¡å‹è¯„ä¼°çš„å¤šå±‚éªŒè¯æ¡†æ¶(Multi-Layer Validation Framework)ä»¥ç¡®ä¿æ•°æ®ä¿çœŸåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºè¯¥åˆæˆæ•°æ®è®­ç»ƒçš„8Bå‚æ•°æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½è¶…è¶Šäº†GPT-4oã€‚è¿™ä¸€æˆæœè¯æ˜äº†é€šè¿‡é«˜è´¨é‡è™šæ‹Ÿå·¥å…·åˆæˆæ•°æ®æ¥æå‡æ¨¡å‹åœ¨å¤æ‚å¤šè·³å·¥å…·è°ƒç”¨åœºæ™¯ä¸‹è¡¨ç°çš„å¯è¡Œæ€§ä¸é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 9 tables, 6 figures. Code available at https://github.com/Buycar-arb/ToolForge",
      "pdf_url": "https://arxiv.org/pdf/2512.16149v1",
      "published_date": "2025-12-18 04:06:26 UTC",
      "updated_date": "2025-12-18 04:06:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:12:39.430086+00:00"
    },
    {
      "arxiv_id": "2512.16147v1",
      "title": "Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning",
      "title_zh": "è§£ç ä»‡æ¨æ•…äº‹ä¼ æ’­ä¸­çš„è™šå‡å™äº‹ï¼šåŸºäºå¤šä»»åŠ¡å­¦ä¹ çš„åŒå¤´ RoBERTa æ¨¡å‹",
      "authors": [
        "Yash Bhaskar",
        "Sankalp Bahad",
        "Parameswari Krishnamurthy"
      ],
      "abstract": "Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \\cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“ä¸Šç”±è™šå‡å™äº‹é©±åŠ¨çš„ä»‡æ¨è¨€è®ºï¼ˆç§°ä¸º Faux-Hateï¼‰è¿™ä¸€ç‰¹å®šç°è±¡ï¼Œå¼€å‘äº†ä¸€å¥—åŸºäºåŒå¤´ RoBERTa (Dual-Head RoBERTa) æ¨¡å‹çš„åˆ†æç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿä¸“æ³¨äºå¤„ç†å°åœ°è¯­-è‹±è¯­æ··åˆä»£ç  (code-mixed Hindi-English) ç¤¾äº¤åª’ä½“æ–‡æœ¬ï¼Œæ—¨åœ¨åŒæ—¶è§£å†³äºŒåˆ†ç±» Faux-Hate æ£€æµ‹ä»¥åŠä»‡æ¨å†…å®¹çš„ç›®æ ‡å’Œä¸¥é‡ç¨‹åº¦ (Target and Severity) é¢„æµ‹ä¸¤ä¸ªæ ¸å¿ƒå­ä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å°†å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç† (NLP) æŠ€æœ¯ä¸é¢†åŸŸç‰¹å®šé¢„è®­ç»ƒ (domain-specific pretraining) ç›¸ç»“åˆï¼Œåˆ©ç”¨å¤šä»»åŠ¡å­¦ä¹  (Multi-Task Learning) æ¡†æ¶æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚è¯­å¢ƒä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¯æ˜è¯¥ç³»ç»Ÿåœ¨ç›¸å…³ä»»åŠ¡ä¸­å–å¾—äº†æå…·ç«äº‰åŠ›çš„æˆç»©ï¼Œå±•ç¤ºäº†å¤šä»»åŠ¡å­¦ä¹ åœ¨è¯†åˆ«å’Œåˆ†æè™šå‡å™äº‹å¼•å‘çš„ä»‡æ¨è¨€è®ºæ–¹é¢çš„å“è¶Šæ•ˆèƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted Paper, Anthology ID: 2024.icon-fauxhate.3, 4 pages, 1 figure, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2512.16147v1",
      "published_date": "2025-12-18 04:00:06 UTC",
      "updated_date": "2025-12-18 04:00:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:13:26.850877+00:00"
    },
    {
      "arxiv_id": "2512.16145v1",
      "title": "MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation",
      "title_zh": "MRG-R1ï¼šé¢å‘ä¸´åºŠä¸€è‡´æ€§åŒ»å­¦æŠ¥å‘Šç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Pengyu Wang",
        "Shuchang Ye",
        "Usman Naseem",
        "Jinman Kim"
      ],
      "abstract": "Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured \"thinking report\" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MRG-R1ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å®ç°ä¸´åºŠå¯¹é½çš„åŒ»å­¦æŠ¥å‘Šç”Ÿæˆ (Medical Report Generation) æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•è¿‡åº¦å…³æ³¨è¯­è¨€é£æ ¼æ¨¡ä»¿è€Œå¿½ç•¥ä¸´åºŠå‡†ç¡®æ€§çš„é—®é¢˜ï¼ŒMRG-R1 åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (LVLM) ä¸Šé‡‡ç”¨äº†è¯­ä¹‰é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹  (SRL) æ–¹æ³•ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ Group Relative Policy Optimization (GRPO) ä¼˜åŒ–æŠ¥å‘Šå±‚çº§çš„å¥–åŠ±å‡½æ•°ï¼Œé€šè¿‡è®¡ç®—ç”ŸæˆæŠ¥å‘Šä¸å‚è€ƒæŠ¥å‘Šä¸­å…³é”®æ”¾å°„å­¦å‘ç°ä¹‹é—´çš„ Margin-based Cosine Similarity (MCCS) æ¥ç›´æ¥æå‡ä¸´åºŠä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†è½»é‡çº§æ¨ç†æ ¼å¼çº¦æŸï¼Œå¼•å¯¼æ¨¡å‹äº§å‡ºç»“æ„åŒ–çš„â€œæ€è€ƒæŠ¥å‘Šâ€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMRG-R1 åœ¨ IU X-Ray å’Œ MIMIC-CXR æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œå…¶ CE-F1 åˆ†å€¼æ˜¾è‘—æå‡ã€‚ç ”ç©¶è¯æ˜ï¼Œä¼˜åŒ–åŸºäºä¸´åºŠäº‹å®çš„æŠ¥å‘Šçº§å¥–åŠ±æ¯”ä¼ ç»Ÿçš„ Token-level ç›‘ç£èƒ½æ›´æœ‰æ•ˆåœ°ä¿éšœåŒ»å­¦æŠ¥å‘Šçš„æ­£ç¡®æ€§ï¼Œä¸ºåŒ»å­¦å¤§è§†è§‰è¯­è¨€æ¨¡å‹ (Med-LVLM) çš„è®­ç»ƒæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.16145v1",
      "published_date": "2025-12-18 03:57:55 UTC",
      "updated_date": "2025-12-18 03:57:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:12:57.298922+00:00"
    },
    {
      "arxiv_id": "2512.16144v1",
      "title": "INTELLECT-3: Technical Report",
      "title_zh": "INTELLECT-3 æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Prime Intellect Team",
        "Mika Senghaas",
        "Fares Obeid",
        "Sami Jaghouar",
        "William Brown",
        "Jack Min Ong",
        "Daniel Auras",
        "Matej Sirovatka",
        "Jannik Straube",
        "Andrew Baker",
        "Sebastian MÃ¼ller",
        "Justus Mattern",
        "Manveer Basra",
        "Aiman Ismail",
        "Dominik Scherm",
        "Cooper Miller",
        "Ameen Patel",
        "Simon Kirsten",
        "Mario Sieg",
        "Christian Reetz",
        "Kemal Erdem",
        "Vincent Weisser",
        "Johannes Hagemann"
      ],
      "abstract": "We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† INTELLECT-3ï¼Œä¸€ä¸ªé‡‡ç”¨ Mixture-of-Experts (MoE) æ¶æ„çš„ 106B å‚æ•°æ¨¡å‹ï¼ˆ12B æ¿€æ´»ï¼‰ï¼Œå…¶æ ¸å¿ƒæ˜¯åœ¨å…¨æ ˆç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹  (RL) åŸºç¡€è®¾æ–½ä¸Šè®­ç»ƒè€Œæˆã€‚INTELLECT-3 åœ¨æ•°å­¦ã€ä»£ç ã€ç§‘å­¦å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†è®¸å¤šè§„æ¨¡æ›´å¤§çš„å‰æ²¿æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å¼€æºäº†å®Œæ•´çš„æŠ€æœ¯æ ˆï¼ŒåŒ…æ‹¬ä¸“ä¸ºå¤§è§„æ¨¡å¼‚æ­¥å¼ºåŒ–å­¦ä¹ è®¾è®¡çš„æ¡†æ¶ prime-rlï¼Œè¯¥æ¡†æ¶åŸç”Ÿæ”¯æŒå¤šè½®äº¤äº’å’Œå·¥å…·è°ƒç”¨ï¼Œå¹¶èƒ½é«˜æ•ˆæ‰©å±•è‡³æ•°åƒä¸ª GPUã€‚é€šè¿‡åœ¨ GLM-4.5-Air-Base åŸºç¡€ä¸Šè¿›è¡Œ SFT å’Œå¤§è§„æ¨¡ RL è®­ç»ƒï¼Œå›¢é˜Ÿåœ¨ 512 å— H200 ä¸Šå®ç°äº†æé«˜çš„è®­ç»ƒæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥é¡¹ç›®è¿˜æä¾›äº†åŒ…å«å®Œæ•´è®­ç»ƒé…æ–¹ (recipe)ã€ç¯å¢ƒé›†åˆåŠ verifiers åº“çš„ Environments Hub ç¤¾åŒºå¹³å°ï¼Œä¸ºæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„å¼€å‘ä¸è¯„ä¼°æä¾›äº†å…¨å¥—åŸºç¡€è®¾æ–½æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16144v1",
      "published_date": "2025-12-18 03:57:01 UTC",
      "updated_date": "2025-12-18 03:57:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:12:48.649919+00:00"
    },
    {
      "arxiv_id": "2512.16123v1",
      "title": "Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection",
      "title_zh": "é’ˆå¯¹ç›®æ ‡æ£€æµ‹å¯¹æŠ—æ”»å‡»çš„åŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„å»å™ªé˜²å¾¡",
      "authors": [
        "Min Geun Song",
        "Gang Min Kim",
        "Woonmin Kim",
        "Yongsik Kim",
        "Jeonghyun Sim",
        "Sangbeom Park",
        "Huy Kang Kim"
      ],
      "abstract": "Deep learning-based object detection models play a critical role in real-world applications such as autonomous driving and security surveillance systems, yet they remain vulnerable to adversarial examples. In this work, we propose an autoencoder-based denoising defense to recover object detection performance degraded by adversarial perturbations. We conduct adversarial attacks using Perlin noise on vehicle-related images from the COCO dataset, apply a single-layer convolutional autoencoder to remove the perturbations, and evaluate detection performance using YOLOv5. Our experiments demonstrate that adversarial attacks reduce bbox mAP from 0.2890 to 0.1640, representing a 43.3% performance degradation. After applying the proposed autoencoder defense, bbox mAP improves to 0.1700 (3.7% recovery) and bbox mAP@50 increases from 0.2780 to 0.3080 (10.8% improvement). These results indicate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.",
      "tldr_zh": "æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶å’Œå®‰é˜²ç›‘æ§ç­‰ç°å®åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œä½†ææ˜“å—åˆ°å¯¹æŠ—æ ·æœ¬(Adversarial Examples)çš„å½±å“ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªåŠ¨ç¼–ç å™¨(Autoencoder)çš„å»å™ªé˜²å¾¡æœºåˆ¶ï¼Œæ—¨åœ¨æ¢å¤å—å¯¹æŠ—æ‰°åŠ¨å½±å“çš„ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚ç ”ç©¶äººå‘˜åœ¨COCOæ•°æ®é›†çš„è½¦è¾†å›¾åƒä¸Šåˆ©ç”¨Perlinå™ªå£°è¿›è¡Œå¯¹æŠ—æ”»å‡»ï¼Œå¹¶é‡‡ç”¨å•å±‚å·ç§¯è‡ªåŠ¨ç¼–ç å™¨(Convolutional Autoencoder)å»é™¤æ‰°åŠ¨ï¼Œéšåä½¿ç”¨YOLOv5è¯„ä¼°æ£€æµ‹è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹æŠ—æ”»å‡»å¯¼è‡´bbox mAPä»0.2890ä¸‹é™è‡³0.1640ï¼Œæ€§èƒ½è¡°å‡è¾¾43.3%ã€‚åœ¨åº”ç”¨æ‰€æé˜²å¾¡æ–¹æ¡ˆåï¼Œbbox mAPæ¢å¤è‡³0.1700ï¼Œä¸”bbox mAP@50ä»0.2780æå‡è‡³0.3080ï¼Œå®ç°äº†10.8%çš„æ€§èƒ½æ”¹è¿›ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„å»å™ªæŠ€æœ¯èƒ½å¤Ÿåœ¨æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œé’ˆå¯¹å¯¹æŠ—æ”»å‡»æä¾›æœ‰æ•ˆçš„å±€éƒ¨é˜²å¾¡èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "7 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16123v1",
      "published_date": "2025-12-18 03:19:40 UTC",
      "updated_date": "2025-12-18 03:19:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:12:29.478837+00:00"
    },
    {
      "arxiv_id": "2512.16108v1",
      "title": "WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning",
      "title_zh": "WeMusic-Agentï¼šåŸºäºçŸ¥è¯†å†…åŒ–ä¸æ™ºèƒ½ä½“è¾¹ç•Œå­¦ä¹ çš„é«˜æ•ˆå¯¹è¯å¼éŸ³ä¹æ¨è",
      "authors": [
        "Wendong Bi",
        "Yirong Mao",
        "Xianglong Liu",
        "Kai Tian",
        "Jian Zhang",
        "Hanjie Wang",
        "Wenhui Que"
      ],
      "abstract": "Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†WeMusic-Agentï¼Œä¸€ä¸ªä¸“ä¸ºé«˜æ•ˆå¯¹è¯å¼éŸ³ä¹æ¨èè®¾è®¡çš„LLMè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨ä¸“ä¸šé¢†åŸŸçŸ¥è¯†å’Œå·¥å…·é›†æˆä¹‹é—´éš¾ä»¥å¹³è¡¡çš„é—®é¢˜ã€‚é€šè¿‡ç»“åˆKnowledge internalizationå’ŒAgentic boundary learningï¼Œè¯¥æ¡†æ¶æ•™ä¼šæ¨¡å‹æ™ºèƒ½åœ°åˆ¤æ–­ä½•æ—¶åˆ©ç”¨å†…åŒ–çŸ¥è¯†æˆ–è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚éŸ³ä¹æ£€ç´¢APIï¼‰ã€‚åŸºäºæ­¤æ¡†æ¶å¼€å‘çš„WeMusic-Agent-M1æ¨¡å‹åœ¨50Bè§„æ¨¡çš„éŸ³ä¹è¯­æ–™åº“ä¸Šè¿›è¡Œäº†Continued pretrainingï¼Œä»è€Œè·å¾—äº†æ·±åšçš„éŸ³ä¹èƒŒæ™¯çŸ¥è¯†å’Œç²¾å‡†çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜åˆ©ç”¨å¾®ä¿¡å¬ä¸€å¬(WeChat Listen)çš„çœŸå®æ•°æ®æ„å»ºäº†ä¸€ä¸ªç»¼åˆè¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–äº†æ¨èçš„ç›¸å…³æ€§ã€ä¸ªæ€§åŒ–å’Œå¤šæ ·æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒWeMusic-Agentåœ¨çœŸå®åº”ç”¨åœºæ™¯ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œä¸ºå¯¹è¯å¼éŸ³ä¹æ¨èæä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16108v1",
      "published_date": "2025-12-18 02:59:19 UTC",
      "updated_date": "2025-12-18 02:59:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:12:45.409856+00:00"
    },
    {
      "arxiv_id": "2512.16106v1",
      "title": "ModelTables: A Corpus of Tables about Models",
      "title_zh": "ModelTablesï¼šæ¨¡å‹ç›¸å…³è¡¨æ ¼è¯­æ–™åº“",
      "authors": [
        "Zhengyuan Dong",
        "Victor Zhong",
        "RenÃ©e J. Miller"
      ],
      "abstract": "We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† ModelTablesï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ Model Lakes ä¸­è¡¨æ ¼çš„åŸºå‡†æµ‹è¯•é›†ï¼Œæ—¨åœ¨æ•æ‰ä¼ ç»Ÿæ–‡æœ¬æ£€ç´¢å¸¸å¿½ç•¥çš„æ€§èƒ½ä¸é…ç½®è¡¨æ ¼çš„ç»“æ„åŒ–è¯­ä¹‰ã€‚è¯¥è¯­æ–™åº“æ„å»ºè‡ª Hugging Face æ¨¡å‹å¡ç‰‡ã€GitHub READMEs åŠç›¸å…³è®ºæ–‡ï¼Œç›®å‰æ¶µç›–äº†è¶…è¿‡ 6 ä¸‡ä¸ªæ¨¡å‹å’Œ 9 ä¸‡å¼ è¡¨æ ¼ã€‚ä¸å¸¸è§„çš„ Data Lake è¡¨æ ¼ç›¸æ¯”ï¼Œæ¨¡å‹è¡¨æ ¼è§„æ¨¡è¾ƒå°ä½†å…·æœ‰æ›´ç´§å¯†çš„è¡¨é—´å…³ç³»ï¼Œåæ˜ äº†æ¨¡å‹ä¸ benchmark æ¼”å˜çš„é«˜åº¦è€¦åˆã€‚ç ”ç©¶è€…åˆ©ç”¨è®ºæ–‡å¼•ç”¨ã€æ¨¡å‹å¡ç‰‡ç»§æ‰¿å’Œå…±äº«æ•°æ®é›†æ„å»ºäº†å¤šæºæ ‡æ³¨åŸºå‡†ï¼Œå¹¶åœ¨è¡¨æ ¼æœç´¢ (table search) ä»»åŠ¡ä¸­å¯¹æ¯”äº†å¤šç§ Data Lake æœç´¢ç®—å­ä¸ Information Retrieval åŸºçº¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºè¡¨æ ¼çš„ Dense Retrieval è¾¾åˆ°äº† 66.5% çš„ P@1ï¼Œè€Œ Metadata Hybrid Retrieval è¾¾åˆ° 54.1%ï¼Œè¯æ˜äº†è¯¥é¢†åŸŸä»æœ‰æ˜¾è‘—çš„ç ”ç©¶ç©ºé—´ã€‚é€šè¿‡å‘å¸ƒ ModelTables åŠå…¶æ„å»ºåè®®ï¼Œè¯¥å·¥ä½œä¸º AI æ¨¡å‹ç»“æ„åŒ–çŸ¥è¯†çš„è¯­ä¹‰æ£€ç´¢ã€å¯¹æ¯”ä¸ç»„ç»‡æä¾›äº†é¦–ä¸ªå¤§è§„æ¨¡åŸºå‡†ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DB",
      "comment": "14 pages, 8 figures and 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.16106v1",
      "published_date": "2025-12-18 02:51:46 UTC",
      "updated_date": "2025-12-18 02:51:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:12:56.190933+00:00"
    },
    {
      "arxiv_id": "2512.16103v1",
      "title": "AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation",
      "title_zh": "AIMMï¼šä¸€ç§ç”¨äºæ£€æµ‹å—ç¤¾äº¤åª’ä½“å½±å“çš„è‚¡å¸‚æ“çºµçš„äººå·¥æ™ºèƒ½é©±åŠ¨å¤šæ¨¡æ€æ¡†æ¶",
      "authors": [
        "Sandeep Neela"
      ],
      "abstract": "Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.\n  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.\n  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“ååŒè¡ŒåŠ¨å¼•å‘çš„è‚¡ç¥¨å¸‚åœºæ“çºµé—®é¢˜ï¼Œæå‡ºäº†åä¸º AIMM çš„äººå·¥æ™ºèƒ½é©±åŠ¨å¤šæ¨¡æ€æ¡†æ¶ã€‚è¯¥æ¡†æ¶èåˆäº† Reddit ç¤¾äº¤æ´»åŠ¨ã€bot å’ŒååŒè¡Œä¸ºæŒ‡æ ‡ä»¥åŠ OHLCV å¸‚åœºç‰¹å¾ï¼Œæ—¨åœ¨ä¸ºæ¯ä¸ªè‚¡ç¥¨ç”Ÿæˆæ¯æ—¥çš„ AIMM Manipulation Risk Score æ“çºµé£é™©è¯„åˆ†ã€‚ç³»ç»Ÿé€šè¿‡ Parquet åŸç”Ÿæµæ°´çº¿å’Œ Streamlit ä»ªè¡¨æ¿ï¼Œæ”¯æŒåˆ†æå¸ˆå¯¹å¯ç–‘æ—¶é—´çª—ã€ç¤¾äº¤å¸–å­å’Œä»·æ ¼èµ°åŠ¿è¿›è¡Œæ·±åº¦æ£€æŸ¥ã€‚ç ”ç©¶è´¡çŒ®äº†åŒ…å« 33 ä¸ªæ ‡è®°è‚¡ç¥¨æ—¥çš„ AIMM Ground Truth (AIMM-GT) æ•°æ®é›†ï¼Œå¹¶å®ç°äº† Forward-walk evaluation å‰å‘èµ°åŠ¿è¯„ä¼°æœºåˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAIMM åœ¨ 2021 å¹´ GME æŒ¤å‹é«˜å³°å‘ç”Ÿå‰ 22 å¤©ä¾¿æˆåŠŸè¯†åˆ«å‡ºé£é™©ä¿¡å·ã€‚å°½ç®¡ç›®å‰æ ·æœ¬é‡æœ‰é™ï¼Œè¯¥æ¡†æ¶ä»è¯æ˜äº†å¤šæ¨¡æ€æ•°æ®èåˆåœ¨è¯†åˆ«æ—©æœŸå¸‚åœºæ“çºµé¢„è­¦ä¸­çš„æ½œåŠ›ï¼Œç›®å‰ç›¸å…³ä»£ç ä¸æ•°æ®é›†æ¶æ„å·²å¼€æºä»¥æ”¯æŒå¸‚åœºç›‘ç®¡ç ”ç©¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2512.16103v1",
      "published_date": "2025-12-18 02:42:01 UTC",
      "updated_date": "2025-12-18 02:42:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:13:42.513066+00:00"
    },
    {
      "arxiv_id": "2512.16093v1",
      "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
      "title_zh": "TurboDiffusionï¼šå°†è§†é¢‘æ‰©æ•£æ¨¡å‹åŠ é€Ÿ 100-200 å€",
      "authors": [
        "Jintao Zhang",
        "Kaiwen Zheng",
        "Kai Jiang",
        "Haoxu Wang",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Jianfei Chen",
        "Jun Zhu"
      ],
      "abstract": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TurboDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ˜¾è‘—åŠ é€Ÿè§†é¢‘ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè§†é¢‘è´¨é‡çš„åŒæ—¶å®ç° 100-200 å€çš„ç«¯åˆ°ç«¯ç”ŸæˆåŠ é€Ÿã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼ŒTurboDiffusion ç»“åˆäº†ä½æ¯”ç‰¹ SageAttention å’Œå¯è®­ç»ƒçš„ Sparse-Linear Attention (SLA) æ¥ä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—ã€‚è¯¥æ¡†æ¶è¿˜é‡‡ç”¨ rCM æŠ€æœ¯è¿›è¡Œé«˜æ•ˆçš„æ­¥æ•°è’¸é¦ (Step distillation)ï¼Œå¹¶åˆ©ç”¨ W8A8 é‡åŒ–æ–¹æ¡ˆå¯¹æ¨¡å‹å‚æ•°å’Œæ¿€æ´»è¿›è¡Œ 8 æ¯”ç‰¹å‹ç¼©ä»¥åŠ é€Ÿçº¿æ€§å±‚è¿ç®—ã€‚å®éªŒåœ¨ Wan2.2 å’Œ Wan2.1 ç­‰å¤šæ¬¾å¤§å‚æ•°é‡æ¨¡å‹ä¸Šå±•å¼€ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶å³ä½¿åœ¨å•å— RTX 5090 GPU ä¸Šä¹Ÿèƒ½ç»´æŒæé«˜çš„æ¨ç†æ•ˆç‡ã€‚æ­¤é¡¹æˆæœè¯æ˜äº†å…¶åœ¨å¤„ç†é«˜æ¸…è§†é¢‘ç”Ÿæˆä»»åŠ¡æ—¶çš„å“è¶Šæ€§èƒ½ï¼Œä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›äº†é«˜æ•ˆçš„å·¥ç¨‹ä¸ç®—æ³•è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16093v1",
      "published_date": "2025-12-18 02:21:30 UTC",
      "updated_date": "2025-12-18 02:21:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:13:41.693349+00:00"
    },
    {
      "arxiv_id": "2512.16089v1",
      "title": "LAPX: Lightweight Hourglass Network with Global Context",
      "title_zh": "LAPXï¼šç»“åˆå…¨å±€ä¸Šä¸‹æ–‡çš„è½»é‡çº§æ²™æ¼ç½‘ç»œ",
      "authors": [
        "Haopeng Zhao",
        "Marsha Mariya Kappan",
        "Mahdi Bamdad",
        "Francisco Cruz"
      ],
      "abstract": "Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººä½“å§¿æ€ä¼°è®¡(Human pose estimation)ä»»åŠ¡ä¸­ï¼Œç°æœ‰SOTAæ¨¡å‹å‚æ•°é‡å¤§ä¸”è®¡ç®—æˆæœ¬é«˜ï¼Œè€Œè½»é‡åŒ–æ¨¡å‹å¾€å¾€åœ¨ç²¾åº¦æˆ–è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†LAPXã€‚LAPXæ˜¯ä¸€ç§é›†æˆäº†è‡ªæ³¨æ„åŠ›(self-attention)æœºåˆ¶çš„æ²™æ¼ç½‘ç»œ(Hourglass network)ï¼Œæ—¨åœ¨æœ‰æ•ˆæ•è·å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯(global contextual information)ã€‚è¯¥æ¨¡å‹åœ¨å‰æœŸå·¥ä½œLAPçš„åŸºç¡€ä¸Šï¼Œé€šè¿‡è¿›ä¸€æ­¥ä¼˜åŒ–é˜¶æ®µè®¾è®¡(stage design)å¹¶ç²¾ç‚¼è½»é‡åŒ–æ³¨æ„åŠ›æ¨¡å—(lightweight attention modules)ï¼Œæ˜¾è‘—æå‡äº†ç‰¹å¾æå–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLAPXåœ¨MPIIå’ŒCOCOä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†æå…·ç«äº‰åŠ›çš„ç»“æœã€‚å‡­å€Ÿä»…2.3Mçš„å‚æ•°é‡å’Œå‡ºè‰²çš„å®æ—¶å¤„ç†æ€§èƒ½(real-time performance)ï¼Œè¯¥ç ”ç©¶è¯å®äº†LAPXåœ¨è¾¹ç¼˜è®¾å¤‡(edge devices)ä¸Šéƒ¨ç½²çš„é«˜æ•ˆæ€§ä¸å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.16089v1",
      "published_date": "2025-12-18 02:04:36 UTC",
      "updated_date": "2025-12-18 02:04:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:13:59.897016+00:00"
    },
    {
      "arxiv_id": "2512.16083v1",
      "title": "Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers",
      "title_zh": "é€šè¿‡åŸºäºå‡½æ•°ä¾èµ–å›¾é‡æ’åºå™¨çš„ LLM é«˜æ•ˆæ¨¡å¼è¿‡æ»¤å®ç° Text2SQL çš„è§„æ¨¡åŒ–æ‰©å±•",
      "authors": [
        "Thanh Dat Hoang",
        "Thanh Tam Nguyen",
        "Thanh Trung Huynh",
        "Hongzhi Yin",
        "Quoc Viet Hung Nguyen"
      ],
      "abstract": "Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \\toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \\toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Text2SQL ç³»ç»Ÿåœ¨å¤„ç†åŒ…å«æˆç™¾ä¸Šåƒå¼ è¡¨åŠæ•°ä¸‡åˆ—çš„å¤§è§„æ¨¡ Schema æ—¶é¢ä¸´çš„ LLM ä¸Šä¸‹æ–‡é™åˆ¶é—®é¢˜ï¼Œæå‡ºäº†åä¸º GRAST çš„é«˜æ•ˆ Schema è¿‡æ»¤æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡é›†æˆæ•°å€¼å’Œå…ƒæ•°æ®çš„æŸ¥è¯¢æ„ŸçŸ¥ LLM ç¼–ç å™¨å¯¹åˆ—è¿›è¡Œæ’åï¼Œéšååˆ©ç”¨åŸºäºå‡½æ•°ä¾èµ– (functional dependencies) çš„è½»é‡çº§ Graph Transformer å¯¹äº’è¿åˆ—è¿›è¡Œé‡æ–°æ’åï¼Œä»¥æ•æ‰åˆ—é—´çš„ç»“æ„ä¿¡æ¯ã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆçš„å­æ¨¡å¼ (sub-schema) å…·å¤‡è¿æ¥æ€§ï¼Œç ”ç©¶é‡‡ç”¨ Steiner-tree å¯å‘å¼ç®—æ³•è¿›è¡Œæœ€ç»ˆç­›é€‰ã€‚å®éªŒè¡¨æ˜ï¼ŒGRAST åœ¨çœŸå®æ•°æ®é›†ä¸Šè¾¾åˆ°äº†è¿‘ä¹å®Œç¾çš„å¬å›ç‡ï¼Œå…¶ç²¾ç¡®åº¦æ˜¾è‘—è¶…è¿‡äº† CodeSã€SchemaExP å’Œ Qwen æ’åºå™¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ‰©å±•è‡³å¤„ç†è¶…è¿‡ 23,000 åˆ—çš„è¶…å¤§è§„æ¨¡ Schemaï¼Œä¸”ä¸­ä½å»¶è¿Ÿä¿æŒåœ¨äºšç§’çº§ï¼Œä¸ºå¤§è§„æ¨¡ Text2SQL åº”ç”¨æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.16083v1",
      "published_date": "2025-12-18 01:59:06 UTC",
      "updated_date": "2025-12-18 01:59:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:14:07.304960+00:00"
    },
    {
      "arxiv_id": "2512.16081v1",
      "title": "Evaluation of Generative Models for Emotional 3D Animation Generation in VR",
      "title_zh": "VR ç¯å¢ƒä¸‹æƒ…æ„Ÿ 3D åŠ¨ç”»ç”Ÿæˆçš„ç”Ÿæˆå¼æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Kiran Chhatre",
        "Renan Guarese",
        "Andrii Matviienko",
        "Christopher Peters"
      ],
      "abstract": "Social interactions incorporate nonverbal signals to convey emotions alongside speech, including facial expressions and body gestures. Generative models have demonstrated promising results in creating full-body nonverbal animations synchronized with speech; however, evaluations using statistical metrics in 2D settings fail to fully capture user-perceived emotions, limiting our understanding of model effectiveness. To address this, we evaluate emotional 3D animation generative models within a Virtual Reality (VR) environment, emphasizing user-centric metrics emotional arousal realism, naturalness, enjoyment, diversity, and interaction quality in a real-time human-agent interaction scenario. Through a user study (N=48), we examine perceived emotional quality for three state of the art speech-driven 3D animation methods across two emotions happiness (high arousal) and neutral (mid arousal). Additionally, we compare these generative models against real human expressions obtained via a reconstruction-based method to assess both their strengths and limitations and how closely they replicate real human facial and body expressions. Our results demonstrate that methods explicitly modeling emotions lead to higher recognition accuracy compared to those focusing solely on speech-driven synchrony. Users rated the realism and naturalness of happy animations significantly higher than those of neutral animations, highlighting the limitations of current generative models in handling subtle emotional states. Generative models underperformed compared to reconstruction-based methods in facial expression quality, and all methods received relatively low ratings for animation enjoyment and interaction quality, emphasizing the importance of incorporating user-centric evaluations into generative model development. Finally, participants positively recognized animation diversity across all generative models.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨è™šæ‹Ÿç°å®(Virtual Reality, VR)ç¯å¢ƒä¸‹è¯„ä¼°äº†æƒ…æ„ŸåŒ–3DåŠ¨ç”»ç”Ÿæˆæ¨¡å‹ï¼Œé‡ç‚¹å…³æ³¨ç°å®æ„Ÿã€è‡ªç„¶åº¦ã€å¤šæ ·æ€§å’Œäº¤äº’è´¨é‡ç­‰ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æŒ‡æ ‡ã€‚é€šè¿‡ä¸€é¡¹æ¶‰åŠ48åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶ï¼Œç ”ç©¶è€…å¯¹æ¯”äº†ä¸‰ç§æœ€å…ˆè¿›çš„è¯­éŸ³é©±åŠ¨(speech-driven)3DåŠ¨ç”»ç”Ÿæˆæ–¹æ³•åœ¨å¿«ä¹å’Œä¸­æ€§æƒ…ç»ªä¸‹çš„è¡¨ç°ï¼Œå¹¶å°†å…¶ä¸åŸºäºé‡æ„çš„äººç±»çœŸå®è¡¨æƒ…è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”ä»…å…³æ³¨è¯­éŸ³åŒæ­¥çš„æ¨¡å‹ï¼Œæ˜¾å¼å»ºæ¨¡æƒ…æ„Ÿçš„æ¨¡å‹åœ¨è¯†åˆ«å‡†ç¡®ç‡ä¸Šè¡¨ç°æ›´ä½³ã€‚ç ”ç©¶å‘ç°ç”¨æˆ·å¯¹é«˜å…´(Happy)åŠ¨ç”»çš„ç°å®æ„Ÿè¯„åˆ†æ˜¾è‘—é«˜äºä¸­æ€§(Neutral)åŠ¨ç”»ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†ç»†å¾®æƒ…ç»ªçŠ¶æ€æ–¹é¢çš„å±€é™ã€‚æ­¤å¤–ï¼Œç”Ÿæˆæ¨¡å‹åœ¨é¢éƒ¨è¡¨æƒ…è´¨é‡ä¸Šä»é€Šäºé‡æ„æ–¹æ³•ï¼Œä¸”æ‰€æœ‰æ–¹æ³•åœ¨äº¤äº’è´¨é‡è¯„åˆ†ä¸Šæ™®éè¾ƒä½ã€‚è¯¥ç ”ç©¶æœ€ç»ˆå¼ºè°ƒäº†å°†ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„è¯„ä¼°çº³å…¥ç”Ÿæˆæ¨¡å‹å¼€å‘çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå¤šæ ·æ€§æ˜¯ç›®å‰ç”Ÿæˆæ¨¡å‹çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "comment": "20 pages, 5 figures. Webpage: https://emotional3dhumans.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2512.16081v1",
      "published_date": "2025-12-18 01:56:22 UTC",
      "updated_date": "2025-12-18 01:56:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:13:52.795770+00:00"
    },
    {
      "arxiv_id": "2512.16071v1",
      "title": "Feasibility of Radio Frequency Based Wireless Sensing of Lead Contamination in Soil",
      "title_zh": "åŸºäºå°„é¢‘æ— çº¿æ„ŸçŸ¥çš„åœŸå£¤é“…æ±¡æŸ“æ£€æµ‹å¯è¡Œæ€§ç ”ç©¶",
      "authors": [
        "Yixuan Gao",
        "Tanvir Ahmed",
        "Mikhail Mohammed",
        "Zhongqi Cheng",
        "Rajalakshmi Nandakumar"
      ],
      "abstract": "Widespread Pb (lead) contamination of urban soil significantly impacts food safety and public health and hinders city greening efforts. However, most existing technologies for measuring Pb are labor-intensive and costly. In this study, we propose SoilScanner, a radio frequency-based wireless system that can detect Pb in soils. This is based on our discovery that the propagation of different frequency band radio signals is affected differently by different salts such as NaCl and Pb(NO3)2 in the soil. In a controlled experiment, manually adding NaCl and Pb(NO3)2 in clean soil, we demonstrated that different salts reflected signals at different frequencies in distinct patterns. In addition, we confirmed the finding using uncontrolled field samples with a machine learning model. Our experiment results show that SoilScanner can classify soil samples into low-Pb and high-Pb categories (threshold at 200 ppm) with an accuracy of 72%, with no sample with > 500 ppm of Pb being misclassified. The results of this study show that it is feasible to build portable and affordable Pb detection and screening devices based on wireless technology.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸå¸‚åœŸå£¤ä¸­å¹¿æ³›å­˜åœ¨çš„Pbï¼ˆé“…ï¼‰æ±¡æŸ“é—®é¢˜ï¼Œæå‡ºäº†åä¸ºSoilScannerçš„å°„é¢‘(Radio Frequency)æ— çº¿ä¼ æ„Ÿç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ£€æµ‹æŠ€æœ¯åŠ³åŠ¨å¼ºåº¦å¤§ä¸”æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚SoilScannerçš„æ ¸å¿ƒåŸç†æºäºä¸€é¡¹æ–°å‘ç°ï¼Œå³ä¸åŒæ³¢æ®µçš„æ— çº¿ä¿¡å·åœ¨åœŸå£¤ä¸­çš„ä¼ æ’­ä¼šå—åˆ°ä¸åŒç›ç±»ï¼ˆå¦‚NaClå’ŒPb(NO3)2ï¼‰çš„ç‰¹å¼‚æ€§å½±å“ã€‚ç ”ç©¶äººå‘˜é€šè¿‡åœ¨æ¸…æ´åœŸå£¤ä¸­æ‰‹åŠ¨æ·»åŠ ç›ç±»çš„å—æ§å®éªŒï¼Œè¯å®äº†ä¸åŒç›ç±»åœ¨ç‰¹å®šé¢‘ç‡ä¸‹å…·æœ‰ç‹¬ç‰¹çš„ä¿¡å·åå°„æ¨¡å¼ï¼Œå¹¶ç»“åˆæœºå™¨å­¦ä¹ (Machine Learning)æ¨¡å‹åœ¨éå—æ§çš„é‡å¤–æ ·æœ¬ä¸­è¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoilScannerèƒ½å¤Ÿä»¥72%çš„å‡†ç¡®ç‡å°†åœŸå£¤æ ·æœ¬åˆ†ä¸ºé«˜Pbä¸ä½Pbç±»åˆ«ï¼ˆä»¥200 ppmä¸ºé˜ˆå€¼ï¼‰ï¼Œä¸”å¯¹äºPbæµ“åº¦è¶…è¿‡500 ppmçš„æ ·æœ¬æœªå‡ºç°ä»»ä½•è¯¯åˆ¤ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨æ— çº¿æŠ€æœ¯å¼€å‘ä¾¿æºå¼ä¸”ç»æµå®æƒ çš„Pbæ£€æµ‹å’Œç­›æŸ¥è®¾å¤‡å…·æœ‰é«˜åº¦å¯è¡Œæ€§ï¼Œä¸ºåŸå¸‚ç¯å¢ƒä¿æŠ¤å’Œå…¬å…±å¥åº·ç®¡ç†æä¾›äº†é‡è¦çš„æŠ€æœ¯å‚è€ƒã€‚",
      "categories": [
        "cs.ET",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.ET",
      "comment": "12 pages, 12 Figures, International Conference on Embedded Wireless Systems and Networks, https://ewsn.org/file-repository/ewsn2024/ewsn24-final99.pdf, Best Paper Award of EWSN2024",
      "pdf_url": "https://arxiv.org/pdf/2512.16071v1",
      "published_date": "2025-12-18 01:36:39 UTC",
      "updated_date": "2025-12-18 01:36:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:13:53.635097+00:00"
    },
    {
      "arxiv_id": "2512.16063v1",
      "title": "A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis",
      "title_zh": "é¢å‘è‡ªåŠ¨åŒ–å®šæ€§åˆ†æçš„å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶",
      "authors": [
        "Qidi Xu",
        "Nuzha Amjad",
        "Grace Giles",
        "Alexa Cumming",
        "De'angelo Hermesky",
        "Alexander Wen",
        "Min Ji Kwak",
        "Yejin Kim"
      ],
      "abstract": "Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åä¸ºCoTI (Collaborative Theme Identification Agent)çš„å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼ˆInstructor, Thematizer, CodebookGeneratorï¼‰å®ç°å®šæ€§ä¸»é¢˜åˆ†æ(Qualitative Thematic Analysis)çš„è‡ªåŠ¨åŒ–ï¼Œä»è€Œè§£å†³è¯¥é¢†åŸŸåŠ³åŠ¨å¼ºåº¦å¤§ã€ä¸»è§‚æ€§å¼ºä¸”éš¾ä»¥è§„æ¨¡åŒ–çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿå°†CoTIåº”ç”¨äº12ä¾‹å¿ƒåŠ›è¡°ç«­æ‚£è€…å…³äºç”¨è¯å¼ºåº¦çš„è®¿è°ˆåˆ†æï¼Œç»“æœæ˜¾ç¤ºCoTIåœ¨è¯†åˆ«å…³é”®çŸ­è¯­ã€æç‚¼ä¸»é¢˜å’Œç”Ÿæˆä»£ç ç°¿(Codebook)æ–¹é¢çš„è¡¨ç°ä¸èµ„æ·±ç ”ç©¶å‘˜é«˜åº¦æ¥è¿‘ï¼Œæ˜¾è‘—ä¼˜äºåˆçº§ç ”ç©¶å‘˜å’Œç°æœ‰çš„åŸºçº¿NLPæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒCoTIå·²è¢«æ•´åˆåˆ°é¢å‘ç”¨æˆ·çš„åº”ç”¨ç¨‹åºä¸­ä»¥æ”¯æŒäººæœºäº¤äº’å®šæ€§åˆ†æã€‚ç„¶è€Œï¼Œå®éªŒè§‚å¯Ÿåˆ°åˆçº§ç ”ç©¶å‘˜ä¸CoTIçš„åä½œä»…å¸¦æ¥äº†è¾¹é™…æ”¶ç›Šï¼Œè¿™æç¤ºäº†åœ¨AIè¾…åŠ©ç§‘ç ”ä¸­ç”¨æˆ·å¯èƒ½å› è¿‡åº¦ä¾èµ–ç³»ç»Ÿè€Œå‰Šå¼±å…¶ç‹¬ç«‹çš„æ‰¹åˆ¤æ€§æ€ç»´ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "42 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.16063v1",
      "published_date": "2025-12-18 01:13:31 UTC",
      "updated_date": "2025-12-18 01:13:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:14:06.755468+00:00"
    },
    {
      "arxiv_id": "2512.16046v1",
      "title": "CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting",
      "title_zh": "CauSTreamï¼šé¢å‘å¾„æµé¢„æµ‹çš„å› æœæ—¶ç©ºè¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Shu Wan",
        "Reepal Shah",
        "John Sabo",
        "Huan Liu",
        "K. SelÃ§uk Candan"
      ],
      "abstract": "Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CauSTreamï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¾„æµé¢„æŠ¥ (Streamflow Forecasting) çš„ç»Ÿä¸€å› æœæ—¶ç©ºè¡¨å¾å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç‰©ç†è¿‡ç¨‹å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ã€‚CauSTream é€šè¿‡è”åˆå­¦ä¹ æ°”è±¡é©±åŠ¨å› ç´ é—´çš„å¾„æµå› æœå›¾ (Runoff Causal Graph) ä»¥åŠæ•è·æ°´æ–‡ç«™é—´åŠ¨æ€ä¾èµ–å…³ç³»çš„æ±‡æµå›¾ (Routing Graph)ï¼Œå®ç°äº†å¯¹æ°´æ–‡è¿‡ç¨‹çš„ç»“æ„åŒ–å»ºæ¨¡ã€‚ç ”ç©¶è€…è¿›ä¸€æ­¥åœ¨éå‚æ•°åŒ–è®¾ç½®ä¸‹ä¸ºè¿™äº›å› æœç»“æ„å»ºç«‹äº†å¯è¾¨è¯†æ€§ (Identifiability) æ¡ä»¶ï¼Œç¡®ä¿äº†æ¨¡å‹çš„ç†è®ºä¸¥è°¨æ€§ã€‚åœ¨ç¾å›½ä¸‰å¤§æ²³æµæµåŸŸçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒé¢„æŠ¥æ—¶æ•ˆä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸”åœ¨é•¿æ—¶æ•ˆé¢„æŠ¥ä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCauSTream å­¦ä¹ åˆ°çš„å› æœå›¾ä¸é¢†åŸŸçŸ¥è¯†é«˜åº¦å»åˆï¼Œä¸ºç†è§£æµåŸŸåŠ¨åŠ›å­¦æä¾›äº†ç›´è§‚ä¸”å¯è§£é‡Šçš„æ´å¯Ÿã€‚è¯¥å·¥ä½œä¸ºç§‘å­¦å’Œç¯å¢ƒé¢†åŸŸçš„å› æœæ—¶ç©ºå»ºæ¨¡æä¾›äº†é‡è¦çš„åŸåˆ™æ€§åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IEEE Big Data 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.16046v1",
      "published_date": "2025-12-18 00:07:23 UTC",
      "updated_date": "2025-12-18 00:07:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T18:14:32.528452+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 168,
  "processed_papers_count": 168,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T18:15:46.121742+00:00"
}
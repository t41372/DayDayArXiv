[
  {
    "arxiv_id": "2410.14911v1",
    "title": "A Hybrid Defense Strategy for Boosting Adversarial Robustness in Vision-Language Models",
    "authors": [
      "Yuhan Liang",
      "Yijun Li",
      "Yumeng Niu",
      "Qianhe Shen",
      "Hangyu Liu"
    ],
    "abstract": "The robustness of Vision-Language Models (VLMs) such as CLIP is critical for\ntheir deployment in safety-critical applications like autonomous driving,\nhealthcare diagnostics, and security systems, where accurate interpretation of\nvisual and textual data is essential. However, these models are highly\nsusceptible to adversarial attacks, which can severely compromise their\nperformance and reliability in real-world scenarios. Previous methods have\nprimarily focused on improving robustness through adversarial training and\ngenerating adversarial examples using models like FGSM, AutoAttack, and\nDeepFool. However, these approaches often rely on strong assumptions, such as\nfixed perturbation norms or predefined attack patterns, and involve high\ncomputational complexity, making them challenging to implement in practical\nsettings. In this paper, we propose a novel adversarial training framework that\nintegrates multiple attack strategies and advanced machine learning techniques\nto significantly enhance the robustness of VLMs against a broad range of\nadversarial attacks. Experiments conducted on real-world datasets, including\nCIFAR-10 and CIFAR-100, demonstrate that the proposed method significantly\nenhances model robustness. The fine-tuned CLIP model achieved an accuracy of\n43.5% on adversarially perturbed images, compared to only 4% for the baseline\nmodel. The neural network model achieved a high accuracy of 98% in these\nchallenging classification tasks, while the XGBoost model reached a success\nrate of 85.26% in prediction tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14911v1",
    "published_date": "2024-10-18 23:47:46 UTC",
    "updated_date": "2024-10-18 23:47:46 UTC"
  },
  {
    "arxiv_id": "2411.00796v1",
    "title": "Sentiment Analysis Based on RoBERTa for Amazon Review: An Empirical Study on Decision Making",
    "authors": [
      "Xinli Guo"
    ],
    "abstract": "In this study, we leverage state-of-the-art Natural Language Processing (NLP)\ntechniques to perform sentiment analysis on Amazon product reviews. By\nemploying transformer-based models, RoBERTa, we analyze a vast dataset to\nderive sentiment scores that accurately reflect the emotional tones of the\nreviews. We provide an in-depth explanation of the underlying principles of\nthese models and evaluate their performance in generating sentiment scores.\nFurther, we conduct comprehensive data analysis and visualization to identify\npatterns and trends in sentiment scores, examining their alignment with\nbehavioral economics principles such as electronic word of mouth (eWOM),\nconsumer emotional reactions, and the confirmation bias. Our findings\ndemonstrate the efficacy of advanced NLP models in sentiment analysis and offer\nvaluable insights into consumer behavior, with implications for strategic\ndecision-making and marketing practices.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "Master's thesis",
    "pdf_url": "http://arxiv.org/pdf/2411.00796v1",
    "published_date": "2024-10-18 22:46:27 UTC",
    "updated_date": "2024-10-18 22:46:27 UTC"
  },
  {
    "arxiv_id": "2410.14897v1",
    "title": "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items",
    "authors": [
      "Melissa Roemmele",
      "Andrew S. Gordon"
    ],
    "abstract": "LLMs can now perform a variety of complex writing tasks. They also excel in\nanswering questions pertaining to natural language inference and commonsense\nreasoning. Composing these questions is itself a skilled writing task, so in\nthis paper we consider LLMs as authors of commonsense assessment items. We\nprompt LLMs to generate items in the style of a prominent benchmark for\ncommonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine\nthe outcome according to analyses facilitated by the LLMs and human annotation.\nWe find that LLMs that succeed in answering the original COPA benchmark are\nalso more successful in authoring their own items.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Findings of EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.14897v1",
    "published_date": "2024-10-18 22:42:23 UTC",
    "updated_date": "2024-10-18 22:42:23 UTC"
  },
  {
    "arxiv_id": "2410.14895v2",
    "title": "Truncated Consistency Models",
    "authors": [
      "Sangyun Lee",
      "Yilun Xu",
      "Tomas Geffner",
      "Giulia Fanti",
      "Karsten Kreis",
      "Arash Vahdat",
      "Weili Nie"
    ],
    "abstract": "Consistency models have recently been introduced to accelerate sampling from\ndiffusion models by directly predicting the solution (i.e., data) of the\nprobability flow ODE (PF ODE) from initial noise. However, the training of\nconsistency models requires learning to map all intermediate points along PF\nODE trajectories to their corresponding endpoints. This task is much more\nchallenging than the ultimate objective of one-step generation, which only\nconcerns the PF ODE's noise-to-data mapping. We empirically find that this\ntraining paradigm limits the one-step generation performance of consistency\nmodels. To address this issue, we generalize consistency training to the\ntruncated time range, which allows the model to ignore denoising tasks at\nearlier time steps and focus its capacity on generation. We propose a new\nparameterization of the consistency function and a two-stage training procedure\nthat prevents the truncated-time training from collapsing to a trivial\nsolution. Experiments on CIFAR-10 and ImageNet $64\\times64$ datasets show that\nour method achieves better one-step and two-step FIDs than the state-of-the-art\nconsistency models such as iCT-deep, using more than 2$\\times$ smaller\nnetworks. Project page: https://truncated-cm.github.io/",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.14895v2",
    "published_date": "2024-10-18 22:38:08 UTC",
    "updated_date": "2025-01-23 18:56:32 UTC"
  },
  {
    "arxiv_id": "2410.14894v2",
    "title": "Soft-Label Integration for Robust Toxicity Classification",
    "authors": [
      "Zelei Cheng",
      "Xian Wu",
      "Jiahao Yu",
      "Shuo Han",
      "Xin-Qiang Cai",
      "Xinyu Xing"
    ],
    "abstract": "Toxicity classification in textual content remains a significant problem.\nData with labels from a single annotator fall short of capturing the diversity\nof human perspectives. Therefore, there is a growing need to incorporate\ncrowdsourced annotations for training an effective toxicity classifier.\nAdditionally, the standard approach to training a classifier using empirical\nrisk minimization (ERM) may fail to address the potential shifts between the\ntraining set and testing set due to exploiting spurious correlations. This work\nintroduces a novel bi-level optimization framework that integrates crowdsourced\nannotations with the soft-labeling technique and optimizes the soft-label\nweights by Group Distributionally Robust Optimization (GroupDRO) to enhance the\nrobustness against out-of-distribution (OOD) risk. We theoretically prove the\nconvergence of our bi-level optimization algorithm. Experimental results\ndemonstrate that our approach outperforms existing baseline methods in terms of\nboth average and worst-group accuracy, confirming its effectiveness in\nleveraging crowdsourced annotations to achieve more effective and robust\ntoxicity classification.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.14894v2",
    "published_date": "2024-10-18 22:36:03 UTC",
    "updated_date": "2024-11-07 21:53:17 UTC"
  },
  {
    "arxiv_id": "2410.14890v1",
    "title": "Reasoning, Memorization, and Fine-Tuning Language Models for Non-Cooperative Games",
    "authors": [
      "Yunhao Yang",
      "Leonard Berthellemy",
      "Ufuk Topcu"
    ],
    "abstract": "We develop a method that integrates the tree of thoughts and multi-agent\nframework to enhance the capability of pre-trained language models in solving\ncomplex, unfamiliar games. The method decomposes game-solving into four\nincremental tasks -- game summarization, area selection, action extraction, and\naction validation -- each assigned to a specific language-model agent. By\nconstructing a tree of thoughts, the method simulates reasoning paths and\nallows agents to collaboratively distill game representations and tactics,\nmitigating the limitations of language models in reasoning and long-term\nmemorization. Additionally, an automated fine-tuning process further optimizes\nthe agents' performance by ranking query-response pairs based on game outcomes,\ne.g., winning or losing. We apply the method to a non-cooperative game and\ndemonstrate a 65 percent winning rate against benchmark algorithms, with an\nadditional 10 percent improvement after fine-tuning. In contrast to existing\ndeep learning algorithms for game solving that require millions of training\nsamples, the proposed method consumes approximately 1000 training samples,\nhighlighting its efficiency and scalability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14890v1",
    "published_date": "2024-10-18 22:28:22 UTC",
    "updated_date": "2024-10-18 22:28:22 UTC"
  },
  {
    "arxiv_id": "2410.14888v1",
    "title": "Self-Satisfied: An end-to-end framework for SAT generation and prediction",
    "authors": [
      "Christopher R. Serrano",
      "Jonathan Gallagher",
      "Kenji Yamada",
      "Alexei Kopylov",
      "Michael A. Warren"
    ],
    "abstract": "The boolean satisfiability (SAT) problem asks whether there exists an\nassignment of boolean values to the variables of an arbitrary boolean formula\nmaking the formula evaluate to True. It is well-known that all NP-problems can\nbe coded as SAT problems and therefore SAT is important both practically and\ntheoretically. From both of these perspectives, better understanding the\npatterns and structure implicit in SAT data is of significant value. In this\npaper, we describe several advances that we believe will help open the door to\nsuch understanding: we introduce hardware accelerated algorithms for fast SAT\nproblem generation, a geometric SAT encoding that enables the use of\ntransformer architectures typically applied to vision tasks, and a simple yet\neffective technique we term head slicing for reducing sequence length\nrepresentation inside transformer architectures. These advances allow us to\nscale our approach to SAT problems with thousands of variables and tens of\nthousands of clauses. We validate our architecture, termed Satisfiability\nTransformer (SaT), on the SAT prediction task with data from the SAT\nCompetition (SATComp) 2022 problem sets. Prior related work either leveraged a\npure machine learning approach, but could not handle SATComp-sized problems, or\nwas hybrid in the sense of integrating a machine learning component in a\nstandard SAT solving tool. Our pure machine learning approach achieves\nprediction accuracies comparable to recent work, but on problems that are an\norder of magnitude larger than previously demonstrated. A fundamental aspect of\nour work concerns the very nature of SAT data and its suitability for training\nmachine learning models. We both describe experimental results that probe the\nlandscape of where SAT data can be successfully used for learning and position\nthese results within the broader context of complexity and learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO",
      "03D99",
      "I.5.2; I.5.1; I.2.3; F.0"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.14888v1",
    "published_date": "2024-10-18 22:25:54 UTC",
    "updated_date": "2024-10-18 22:25:54 UTC"
  },
  {
    "arxiv_id": "2410.14881v2",
    "title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation",
    "authors": [
      "Jianfa Chen",
      "Emily Shen",
      "Trupti Bavalatti",
      "Xiaowen Lin",
      "Yongkai Wang",
      "Shuming Hu",
      "Harihar Subramanyam",
      "Ksheeraj Sai Vepuri",
      "Ming Jiang",
      "Ji Qi",
      "Li Chen",
      "Nan Jiang",
      "Ankit Jain"
    ],
    "abstract": "Robust content moderation classifiers are essential for the safety of\nGenerative AI systems. In this task, differences between safe and unsafe inputs\nare often extremely subtle, making it difficult for classifiers (and indeed,\neven humans) to properly distinguish violating vs. benign samples without\ncontext or explanation. Scaling risk discovery and mitigation through\ncontinuous model fine-tuning is also slow, challenging and costly, preventing\ndevelopers from being able to respond quickly and effectively to emergent\nharms. We propose a Classification approach employing Retrieval-Augmented\nGeneration (Class-RAG). Class-RAG extends the capability of its base LLM\nthrough access to a retrieval library which can be dynamically updated to\nenable semantic hotfixing for immediate, flexible risk mitigation. Compared to\nmodel fine-tuning, Class-RAG demonstrates flexibility and transparency in\ndecision-making, outperforms on classification and is more robust against\nadversarial attack, as evidenced by empirical studies. Our findings also\nsuggest that Class-RAG performance scales with retrieval library size,\nindicating that increasing the library size is a viable and low-cost approach\nto improve content moderation.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, submit to ACL",
    "pdf_url": "http://arxiv.org/pdf/2410.14881v2",
    "published_date": "2024-10-18 22:07:36 UTC",
    "updated_date": "2024-12-17 22:07:18 UTC"
  },
  {
    "arxiv_id": "2410.14879v2",
    "title": "Vital Insight: Assisting Experts' Context-Driven Sensemaking of Multi-modal Personal Tracking Data Using Visualization and Human-In-The-Loop LLM Agents",
    "authors": [
      "Jiachen Li",
      "Xiwen Li",
      "Justin Steinberg",
      "Akshat Choube",
      "Bingsheng Yao",
      "Xuhai Xu",
      "Dakuo Wang",
      "Elizabeth Mynatt",
      "Varun Mishra"
    ],
    "abstract": "Passive tracking methods, such as phone and wearable sensing, have become\ndominant in monitoring human behaviors in modern ubiquitous computing studies.\nWhile there have been significant advances in machine-learning approaches to\ntranslate periods of raw sensor data to model momentary behaviors, (e.g.,\nphysical activity recognition), there still remains a significant gap in the\ntranslation of these sensing streams into meaningful, high-level, context-aware\ninsights that are required for various applications (e.g., summarizing an\nindividual's daily routine). To bridge this gap, experts often need to employ a\ncontext-driven sensemaking process in real-world studies to derive insights.\nThis process often requires manual effort and can be challenging even for\nexperienced researchers due to the complexity of human behaviors.\n  We conducted three rounds of user studies with 21 experts to explore\nsolutions to address challenges with sensemaking. We follow a human-centered\ndesign process to identify needs and design, iterate, build, and evaluate Vital\nInsight (VI), a novel, LLM-assisted, prototype system to enable\nhuman-in-the-loop inference (sensemaking) and visualizations of multi-modal\npassive sensing data from smartphones and wearables. Using the prototype as a\ntechnology probe, we observe experts' interactions with it and develop an\nexpert sensemaking model that explains how experts move between direct data\nrepresentations and AI-supported inferences to explore, question, and validate\ninsights. Through this iterative process, we also synthesize and discuss a list\nof design implications for the design of future AI-augmented visualization\nsystems to better assist experts' sensemaking processes in multi-modal health\nsensing data.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14879v2",
    "published_date": "2024-10-18 21:56:35 UTC",
    "updated_date": "2025-02-27 22:31:58 UTC"
  },
  {
    "arxiv_id": "2410.14872v2",
    "title": "How to Evaluate Reward Models for RLHF",
    "authors": [
      "Evan Frick",
      "Tianle Li",
      "Connor Chen",
      "Wei-Lin Chiang",
      "Anastasios N. Angelopoulos",
      "Jiantao Jiao",
      "Banghua Zhu",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "abstract": "We introduce a new benchmark for reward models that quantifies their ability\nto produce strong language models through RLHF (Reinforcement Learning from\nHuman Feedback). The gold-standard approach is to run a full RLHF training\npipeline and directly probe downstream LLM performance. However, this process\nis prohibitively expensive. To address this, we build a predictive model of\ndownstream LLM performance by evaluating the reward model on proxy tasks. These\nproxy tasks consist of a large-scale human preference and a verifiable\ncorrectness preference dataset, in which we measure 12 metrics across 12\ndomains. To investigate which reward model metrics are most correlated to\ngold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a\nlarge-scale crowdsourced human preference platform to view real reward model\ndownstream performance as ground truth. Ultimately, we compile our data and\nfindings into Preference Proxy Evaluations (PPE), the first reward model\nbenchmark explicitly linked to post-RLHF real-world human preference\nperformance, which we open-source for public use and further development. Our\ncode and evaluations can be found at https://github.com/lmarena/PPE .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14872v2",
    "published_date": "2024-10-18 21:38:21 UTC",
    "updated_date": "2024-10-22 22:18:14 UTC"
  },
  {
    "arxiv_id": "2410.14865v1",
    "title": "Joint Verification and Refinement of Language Models for Safety-Constrained Planning",
    "authors": [
      "Yunhao Yang",
      "William Ward",
      "Zichao Hu",
      "Joydeep Biswas",
      "Ufuk Topcu"
    ],
    "abstract": "Although pre-trained language models can generate executable plans (e.g.,\nprogrammatic policies) for solving robot tasks, the generated plans may violate\ntask-relevant logical specifications due to the models' black-box nature. A\nsignificant gap remains between the language models' outputs and verifiable\nexecutions of plans. We develop a method to generate executable plans and\nformally verify them against task-relevant safety specifications. Given a\nhigh-level task description in natural language, the proposed method queries a\nlanguage model to generate plans in the form of executable robot programs. It\nthen converts the generated plan into an automaton-based representation,\nallowing formal verification of the automaton against the specifications. We\nprove that given a set of verified plans, the composition of these plans also\nsatisfies the safety specifications. This proof ensures the safety of complex,\nmulti-component plans, obviating the computation complexity of verifying the\ncomposed plan. We then propose an automated fine-tuning process that refines\nthe language model to generate specification-compliant plans without the need\nfor human labeling. The empirical results show a 30 percent improvement in the\nprobability of generating plans that meet task specifications after\nfine-tuning.",
    "categories": [
      "cs.AI",
      "cs.FL",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14865v1",
    "published_date": "2024-10-18 21:16:30 UTC",
    "updated_date": "2024-10-18 21:16:30 UTC"
  },
  {
    "arxiv_id": "2410.14853v2",
    "title": "DFlow: Diverse Dialogue Flow Simulation with Large Language Models",
    "authors": [
      "Wanyu Du",
      "Song Feng",
      "James Gung",
      "Lijia Sun",
      "Yi Zhang",
      "Saab Mansour",
      "Yanjun Qi"
    ],
    "abstract": "Developing language model-based dialogue agents requires effective data to\ntrain models that can follow specific task logic. However, most existing data\nsimulation methods focus on increasing diversity in language, topics, or\ndialogue acts at the utterance level, largely neglecting a critical aspect of\ntask logic diversity at the dialogue level. This paper proposes a novel data\nsimulation method designed to enhance the diversity of synthetic dialogues by\nfocusing on task execution logic. Our method uses LLMs to generate decision\ntree-structured task plans, which enables the derivation of diverse dialogue\ntrajectories for a given task. Each trajectory, referred to as a \"dialog flow\",\nguides the generation of a multi-turn dialogue that follows a unique\ntrajectory. We apply this method to generate a task-oriented dialogue dataset\ncomprising 3,886 dialogue flows across 15 different domains. We validate the\neffectiveness of this dataset using the next action prediction task, where\nmodels fine-tuned on our dataset outperform strong baselines, including GPT-4.\nUpon acceptance of this paper, we plan to release the code and data publicly.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.14853v2",
    "published_date": "2024-10-18 20:35:28 UTC",
    "updated_date": "2025-03-01 23:22:15 UTC"
  },
  {
    "arxiv_id": "2410.14827v2",
    "title": "Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment",
    "authors": [
      "Zedian Shao",
      "Hongbin Liu",
      "Jaden Mu",
      "Neil Zhenqiang Gong"
    ],
    "abstract": "In a prompt injection attack, an attacker injects a prompt into the original\none, aiming to make an LLM follow the injected prompt to perform an\nattacker-chosen task. Existing attacks primarily focus on how to blend the\ninjected prompt into the original prompt without altering the LLM itself. Our\nexperiments show that these attacks achieve some success, but there is still\nsignificant room for improvement. In this work, we show that an attacker can\nboost the success of prompt injection attacks by poisoning the LLM's alignment\nprocess. Specifically, we propose PoisonedAlign, a method to strategically\ncreate poisoned alignment samples. When even a small fraction of the alignment\ndata is poisoned using our method, the aligned LLM becomes more vulnerable to\nprompt injection while maintaining its foundational capabilities. The code is\navailable at https://github.com/Sadcardation/PoisonedAlign",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14827v2",
    "published_date": "2024-10-18 18:52:16 UTC",
    "updated_date": "2025-04-04 21:04:01 UTC"
  },
  {
    "arxiv_id": "2410.14826v2",
    "title": "SPRIG: Improving Large Language Model Performance by System Prompt Optimization",
    "authors": [
      "Lechen Zhang",
      "Tolga Ergen",
      "Lajanugen Logeswaran",
      "Moontae Lee",
      "David Jurgens"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive capabilities in many\nscenarios, but their performance depends, in part, on the choice of prompt.\nPast research has focused on optimizing prompts specific to a task. However,\nmuch less attention has been given to optimizing the general instructions\nincluded in a prompt, known as a system prompt. To address this gap, we propose\nSPRIG, an edit-based genetic algorithm that iteratively constructs prompts from\nprespecified components to maximize the model's performance in general\nscenarios. We evaluate the performance of system prompts on a collection of 47\ndifferent types of tasks to ensure generalizability. Our study finds that a\nsingle optimized system prompt performs on par with task prompts optimized for\neach individual task. Moreover, combining system and task-level optimizations\nleads to further improvement, which showcases their complementary nature.\nExperiments also reveal that the optimized system prompts generalize\neffectively across model families, parameter sizes, and languages. This study\nprovides insights into the role of system-level instructions in maximizing LLM\npotential.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14826v2",
    "published_date": "2024-10-18 18:51:44 UTC",
    "updated_date": "2024-10-25 05:43:16 UTC"
  },
  {
    "arxiv_id": "2410.14817v4",
    "title": "A Complexity-Based Theory of Compositionality",
    "authors": [
      "Eric Elmoznino",
      "Thomas Jiralerspong",
      "Yoshua Bengio",
      "Guillaume Lajoie"
    ],
    "abstract": "Compositionality is believed to be fundamental to intelligence. In humans, it\nunderlies the structure of thought, language, and higher-level reasoning. In\nAI, compositional representations can enable a powerful form of\nout-of-distribution generalization, in which a model systematically adapts to\nnovel combinations of known concepts. However, while we have strong intuitions\nabout what compositionality is, there currently exists no formal definition for\nit that is measurable and mathematical. Here, we propose such a definition,\nwhich we call representational compositionality, that accounts for and extends\nour intuitions about compositionality. The definition is conceptually simple,\nquantitative, grounded in algorithmic information theory, and applicable to any\nrepresentation. Intuitively, representational compositionality states that a\ncompositional representation satisfies three properties. First, it must be\nexpressive. Second, it must be possible to re-describe the representation as a\nfunction of discrete symbolic sequences with re-combinable parts, analogous to\nsentences in natural language. Third, the function that relates these symbolic\nsequences to the representation, analogous to semantics in natural language,\nmust be simple. Through experiments on both synthetic and real world data, we\nvalidate our definition of compositionality and show how it unifies disparate\nintuitions from across the literature in both AI and cognitive science. We also\nshow that representational compositionality, while theoretically intractable,\ncan be readily estimated using standard deep learning tools. Our definition has\nthe potential to inspire the design of novel, theoretically-driven models that\nbetter capture the mechanisms of compositional thought.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14817v4",
    "published_date": "2024-10-18 18:37:27 UTC",
    "updated_date": "2025-02-05 20:11:18 UTC"
  },
  {
    "arxiv_id": "2410.14808v1",
    "title": "The S2 Hierarchical Discrete Global Grid as a Nexus for Data Representation, Integration, and Querying Across Geospatial Knowledge Graphs",
    "authors": [
      "Shirly Stephen",
      "Mitchell Faulk",
      "Krzysztof Janowicz",
      "Colby Fisher",
      "Thomas Thelen",
      "Rui Zhu",
      "Pascal Hitzler",
      "Cogan Shimizu",
      "Kitty Currier",
      "Mark Schildhauer",
      "Dean Rehberger",
      "Zhangyu Wang",
      "Antrea Christou"
    ],
    "abstract": "Geospatial Knowledge Graphs (GeoKGs) have become integral to the growing\nfield of Geospatial Artificial Intelligence. Initiatives like the U.S. National\nScience Foundation's Open Knowledge Network program aim to create an ecosystem\nof nation-scale, cross-disciplinary GeoKGs that provide AI-ready geospatial\ndata aligned with FAIR principles. However, building this infrastructure\npresents key challenges, including 1) managing large volumes of data, 2) the\ncomputational complexity of discovering topological relations via SPARQL, and\n3) conflating multi-scale raster and vector data. Discrete Global Grid Systems\n(DGGS) help tackle these issues by offering efficient data integration and\nrepresentation strategies. The KnowWhereGraph utilizes Google's S2 Geometry --\na DGGS framework -- to enable efficient multi-source data processing,\nqualitative spatial querying, and cross-graph integration. This paper outlines\nthe implementation of S2 within KnowWhereGraph, emphasizing its role in\ntopologically enriching and semantically compressing data. Ultimately, this\nwork demonstrates the potential of DGGS frameworks, particularly S2, for\nbuilding scalable GeoKGs.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14808v1",
    "published_date": "2024-10-18 18:30:05 UTC",
    "updated_date": "2024-10-18 18:30:05 UTC"
  },
  {
    "arxiv_id": "2410.14807v1",
    "title": "Aligning AI Agents via Information-Directed Sampling",
    "authors": [
      "Hong Jun Jeon",
      "Benjamin Van Roy"
    ],
    "abstract": "The staggering feats of AI systems have brought to attention the topic of AI\nAlignment: aligning a \"superintelligent\" AI agent's actions with humanity's\ninterests. Many existing frameworks/algorithms in alignment study the problem\non a myopic horizon or study learning from human feedback in isolation, relying\non the contrived assumption that the agent has already perfectly identified the\nenvironment. As a starting point to address these limitations, we define a\nclass of bandit alignment problems as an extension of classic multi-armed\nbandit problems. A bandit alignment problem involves an agent tasked with\nmaximizing long-run expected reward by interacting with an environment and a\nhuman, both involving details/preferences initially unknown to the agent. The\nreward of actions in the environment depends on both observed outcomes and\nhuman preferences. Furthermore, costs are associated with querying the human to\nlearn preferences. Therefore, an effective agent ought to intelligently\ntrade-off exploration (of the environment and human) and exploitation. We study\nthese trade-offs theoretically and empirically in a toy bandit alignment\nproblem which resembles the beta-Bernoulli bandit. We demonstrate while naive\nexploration algorithms which reflect current practices and even touted\nalgorithms such as Thompson sampling both fail to provide acceptable solutions\nto this problem, information-directed sampling achieves favorable regret.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14807v1",
    "published_date": "2024-10-18 18:23:41 UTC",
    "updated_date": "2024-10-18 18:23:41 UTC"
  },
  {
    "arxiv_id": "2410.14803v5",
    "title": "DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents",
    "authors": [
      "Taiyi Wang",
      "Zhihao Wu",
      "Jianheng Liu",
      "Jianye Hao",
      "Jun Wang",
      "Kun Shao"
    ],
    "abstract": "On-device control agents, especially on mobile devices, are responsible for\noperating mobile devices to fulfill users' requests, enabling seamless and\nintuitive interactions. Integrating Multimodal Large Language Models (MLLMs)\ninto these agents enhances their ability to understand and execute complex\ncommands, thereby improving user experience. However, fine-tuning MLLMs for\non-device control presents significant challenges due to limited data\navailability and inefficient online training processes. This paper introduces\nDistRL, a novel framework designed to enhance the efficiency of online RL\nfine-tuning for mobile device control agents. DistRL employs centralized\ntraining and decentralized data acquisition to ensure efficient fine-tuning in\nthe context of dynamic online interactions. Additionally, the framework is\nbacked by our tailor-made RL algorithm, which effectively balances exploration\nwith the prioritized utilization of collected data to ensure stable and robust\ntraining. Our experiments show that, on average, DistRL delivers a 3X\nimprovement in training efficiency and enables training data collection 2.4X\nfaster than the leading synchronous multi-machine methods. Notably, after\ntraining, DistRL achieves a 20% relative improvement in success rate compared\nto state-of-the-art methods on general Android tasks from an open benchmark,\nsignificantly outperforming existing approaches while maintaining the same\ntraining time. These results validate DistRL as a scalable and efficient\nsolution, offering substantial improvements in both training efficiency and\nagent performance for real-world, in-the-wild device control tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper and Appendix, 26 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.14803v5",
    "published_date": "2024-10-18 18:19:56 UTC",
    "updated_date": "2025-02-21 16:23:59 UTC"
  },
  {
    "arxiv_id": "2410.14799v1",
    "title": "Deep Generic Dynamic Object Detection Based on Dynamic Grid Maps",
    "authors": [
      "Rujiao Yan",
      "Linda Schubert",
      "Alexander Kamm",
      "Matthias Komar",
      "Matthias Schreier"
    ],
    "abstract": "This paper describes a method to detect generic dynamic objects for automated\ndriving. First, a LiDAR-based dynamic grid is generated online. Second, a deep\nlearning-based detector is trained on the dynamic grid to infer the presence of\ndynamic objects of any type, which is a prerequisite for safe automated\nvehicles in arbitrary, edge-case scenarios. The Rotation-equivariant Detector\n(ReDet) - originally designed for oriented object detection on aerial images -\nwas chosen due to its high detection performance. Experiments are conducted\nbased on real sensor data and the benefits in comparison to classic dynamic\ncell clustering strategies are highlighted. The false positive object detection\nrate is strongly reduced by the proposed approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 6 figures, IEEE IV24",
    "pdf_url": "http://arxiv.org/pdf/2410.14799v1",
    "published_date": "2024-10-18 18:15:32 UTC",
    "updated_date": "2024-10-18 18:15:32 UTC"
  },
  {
    "arxiv_id": "2410.14676v2",
    "title": "SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment",
    "authors": [
      "Qin Liu",
      "Fei Wang",
      "Chaowei Xiao",
      "Muhao Chen"
    ],
    "abstract": "Existing preference alignment is a one-size-fits-all alignment mechanism,\nwhere the part of the large language model (LLM) parametric knowledge with\nnon-preferred features is uniformly blocked to all the users. However, this\npart of knowledge can be useful to advanced users whose expertise qualifies\nthem to handle these information. The one-size-fits-all alignment mechanism\nundermines LLM's utility for these qualified users. To address this problem, we\npropose SudoLM, a framework that lets LLMs learn access control over specific\nparametric knowledge for users with different credentials via authorization\nalignment. SudoLM allows authorized users to unlock their access to all the\nparametric knowledge with an assigned SUDO key while blocking access to\nnon-qualified users. Experiments on two application scenarios demonstrate that\nSudoLM effectively controls the user's access to the parametric knowledge and\nmaintains its general utility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14676v2",
    "published_date": "2024-10-18 17:59:51 UTC",
    "updated_date": "2025-02-27 01:05:43 UTC"
  },
  {
    "arxiv_id": "2410.14675v2",
    "title": "To Trust or Not to Trust? Enhancing Large Language Models' Situated Faithfulness to External Contexts",
    "authors": [
      "Yukun Huang",
      "Sanxing Chen",
      "Hongyi Cai",
      "Bhuwan Dhingra"
    ],
    "abstract": "Large Language Models (LLMs) are often augmented with external contexts, such\nas those used in retrieval-augmented generation (RAG). However, these contexts\ncan be inaccurate or intentionally misleading, leading to conflicts with the\nmodel's internal knowledge. We argue that robust LLMs should demonstrate\nsituated faithfulness, dynamically calibrating their trust in external\ninformation based on their confidence in the internal knowledge and the\nexternal context to resolve knowledge conflicts. To benchmark this capability,\nwe evaluate LLMs across several QA datasets, including a newly created dataset\nfeaturing in-the-wild incorrect contexts sourced from Reddit posts. We show\nthat when provided with both correct and incorrect contexts, both open-source\nand proprietary models tend to overly rely on external information, regardless\nof its factual accuracy. To enhance situated faithfulness, we propose two\napproaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence\nReasoning (RCR). SCR enables models to self-assess the confidence of external\ninformation relative to their own internal knowledge to produce the most\naccurate answer. RCR, in contrast, extracts explicit confidence signals from\nthe LLM and determines the final answer using predefined rules. Our results\nshow that for LLMs with strong reasoning capabilities, such as GPT-4o and\nGPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a\ndirect input augmentation baseline. Conversely, for a smaller model like\nLlama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence\nReasoning Direct Preference Optimization (CR-DPO) method improves performance\non both seen and unseen datasets, yielding an average improvement of 8.9% on\nLlama-3-8B. In addition to quantitative results, we offer insights into the\nrelative strengths of SCR and RCR.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14675v2",
    "published_date": "2024-10-18 17:59:47 UTC",
    "updated_date": "2025-03-17 04:47:58 UTC"
  },
  {
    "arxiv_id": "2410.14672v3",
    "title": "BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities",
    "authors": [
      "Shaozhe Hao",
      "Xuantong Liu",
      "Xianbiao Qi",
      "Shihao Zhao",
      "Bojia Zi",
      "Rong Xiao",
      "Kai Han",
      "Kwan-Yee K. Wong"
    ],
    "abstract": "We introduce BiGR, a novel conditional image generation model using compact\nbinary latent codes for generative training, focusing on enhancing both\ngeneration and representation capabilities. BiGR is the first conditional\ngenerative model that unifies generation and discrimination within the same\nframework. BiGR features a binary tokenizer, a masked modeling mechanism, and a\nbinary transcoder for binary code prediction. Additionally, we introduce a\nnovel entropy-ordered sampling method to enable efficient image generation.\nExtensive experiments validate BiGR's superior performance in generation\nquality, as measured by FID-50k, and representation capabilities, as evidenced\nby linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization\nacross various vision tasks, enabling applications such as image inpainting,\noutpainting, editing, interpolation, and enrichment, without the need for\nstructural modifications. Our findings suggest that BiGR unifies generative and\ndiscriminative tasks effectively, paving the way for further advancements in\nthe field. We further enable BiGR to perform text-to-image generation,\nshowcasing its potential for broader applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Updated with additional T2I results; Project page:\n  https://haoosz.github.io/BiGR",
    "pdf_url": "http://arxiv.org/pdf/2410.14672v3",
    "published_date": "2024-10-18 17:59:04 UTC",
    "updated_date": "2025-01-05 10:51:24 UTC"
  },
  {
    "arxiv_id": "2410.14666v2",
    "title": "DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph",
    "authors": [
      "Maitreya Prafulla Chitale",
      "Uday Bindal",
      "Rajakrishnan Rajkumar",
      "Rahul Mishra"
    ],
    "abstract": "Summarizing movie screenplays presents a unique set of challenges compared to\nstandard document summarization. Screenplays are not only lengthy, but also\nfeature a complex interplay of characters, dialogues, and scenes, with numerous\ndirect and subtle relationships and contextual nuances that are difficult for\nmachine learning models to accurately capture and comprehend. Recent attempts\nat screenplay summarization focus on fine-tuning transformer-based pre-trained\nmodels, but these models often fall short in capturing long-term dependencies\nand latent relationships, and frequently encounter the \"lost in the middle\"\nissue. To address these challenges, we introduce DiscoGraMS, a novel resource\nthat represents movie scripts as a movie character-aware discourse graph (CaD\nGraph). This approach is well-suited for various downstream tasks, such as\nsummarization, question-answering, and salience detection. The model aims to\npreserve all salient information, offering a more comprehensive and faithful\nrepresentation of the screenplay's content. We further explore a baseline\nmethod that combines the CaD Graph with the corresponding movie script through\na late fusion of graph and text modalities, and we present very initial\npromising results.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NAACL 2025 (Main)",
    "pdf_url": "http://arxiv.org/pdf/2410.14666v2",
    "published_date": "2024-10-18 17:56:11 UTC",
    "updated_date": "2025-03-02 10:38:32 UTC"
  },
  {
    "arxiv_id": "2410.14665v1",
    "title": "Online Reinforcement Learning with Passive Memory",
    "authors": [
      "Anay Pattanaik",
      "Lav R. Varshney"
    ],
    "abstract": "This paper considers an online reinforcement learning algorithm that\nleverages pre-collected data (passive memory) from the environment for online\ninteraction. We show that using passive memory improves performance and further\nprovide theoretical guarantees for regret that turns out to be near-minimax\noptimal. Results show that the quality of passive memory determines\nsub-optimality of the incurred regret. The proposed approach and results hold\nin both continuous and discrete state-action spaces.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14665v1",
    "published_date": "2024-10-18 17:55:15 UTC",
    "updated_date": "2024-10-18 17:55:15 UTC"
  },
  {
    "arxiv_id": "2410.14651v2",
    "title": "Real-time Fake News from Adversarial Feedback",
    "authors": [
      "Sanxing Chen",
      "Yukun Huang",
      "Bhuwan Dhingra"
    ],
    "abstract": "We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in high\naccuracies over time for LLM-based detectors -- even after their knowledge\ncutoffs. This suggests that recent popular fake news from such sources can be\neasily detected due to pre-training and retrieval corpus contamination or\nincreasingly salient shallow patterns. Instead, we argue that a proper fake\nnews detection dataset should test a model's ability to reason factually about\nthe current world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification ROC-AUC by an absolute 17.5 percent for a strong RAG-based\nGPT-4o detector. Our experiments reveal the important role of RAG in both\ndetecting and generating fake news, as retrieval-free LLM detectors are\nvulnerable to unseen events and adversarial attacks, while feedback from RAG\ndetection helps discover more deceitful patterns in fake news.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14651v2",
    "published_date": "2024-10-18 17:47:11 UTC",
    "updated_date": "2024-12-29 18:22:27 UTC"
  },
  {
    "arxiv_id": "2410.14641v1",
    "title": "Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs",
    "authors": [
      "Runchu Tian",
      "Yanghao Li",
      "Yuepeng Fu",
      "Siyang Deng",
      "Qinyu Luo",
      "Cheng Qian",
      "Shuo Wang",
      "Xin Cong",
      "Zhong Zhang",
      "Yesai Wu",
      "Yankai Lin",
      "Huadong Wang",
      "Xiaojiang Liu"
    ],
    "abstract": "Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "work in progress",
    "pdf_url": "http://arxiv.org/pdf/2410.14641v1",
    "published_date": "2024-10-18 17:41:19 UTC",
    "updated_date": "2024-10-18 17:41:19 UTC"
  },
  {
    "arxiv_id": "2410.14635v2",
    "title": "GenEOL: Harnessing the Generative Power of LLMs for Training-Free Sentence Embeddings",
    "authors": [
      "Raghuveer Thirukovalluru",
      "Bhuwan Dhingra"
    ],
    "abstract": "Training-free embedding methods directly leverage pretrained large language\nmodels (LLMs) to embed text, bypassing the costly and complex procedure of\ncontrastive learning. Previous training-free embedding methods have mainly\nfocused on optimizing embedding prompts and have overlooked the benefits of\nutilizing the generative abilities of LLMs. We propose a novel method, GenEOL,\nwhich uses LLMs to generate diverse transformations of a sentence that preserve\nits meaning, and aggregates the resulting embeddings of these transformations\nto enhance the overall sentence embedding. GenEOL significantly outperforms the\nexisting training-free embedding methods by an average of 2.85 points across\nseveral LLMs on the sentence semantic text similarity (STS) benchmark. GenEOL\nalso achieves notable gains in clustering, reranking, and pair-classification\ntasks from the MTEB benchmark. Additionally, GenEOL stabilizes representation\nquality across LLM layers and remains robust to perturbations of embedding\nprompts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL Findings 2025, 9 pages, 4 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.14635v2",
    "published_date": "2024-10-18 17:36:53 UTC",
    "updated_date": "2025-02-09 16:40:16 UTC"
  },
  {
    "arxiv_id": "2410.14630v2",
    "title": "On the Regularization of Learnable Embeddings for Time Series Forecasting",
    "authors": [
      "Luca Butera",
      "Giovanni De Felice",
      "Andrea Cini",
      "Cesare Alippi"
    ],
    "abstract": "In forecasting multiple time series, accounting for the individual features\nof each sequence can be challenging. To address this, modern deep learning\nmethods for time series analysis combine a shared (global) model with local\nlayers, specific to each time series, often implemented as learnable\nembeddings. Ideally, these local embeddings should encode meaningful\nrepresentations of the unique dynamics of each sequence. However, when these\nare learned end-to-end as parameters of a forecasting model, they may end up\nacting as mere sequence identifiers. Shared processing blocks may then become\nreliant on such identifiers, limiting their transferability to new contexts. In\nthis paper, we address this issue by investigating methods to regularize the\nlearning of local learnable embeddings for time series processing.\nSpecifically, we perform the first extensive empirical study on the subject and\nshow how such regularizations consistently improve performance in widely\nadopted architectures. Furthermore, we show that methods attempting to prevent\nthe co-adaptation of local and global parameters by means of embeddings\nperturbation are particularly effective in this context. In this regard, we\ninclude in the comparison several perturbation-based regularization methods,\ngoing as far as periodically resetting the embeddings during training. The\nobtained results provide an important contribution to understanding the\ninterplay between learnable local parameters and shared processing layers: a\nkey challenge in modern time series processing models and a step toward\ndeveloping effective foundation models for time series.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at TMLR",
    "pdf_url": "http://arxiv.org/pdf/2410.14630v2",
    "published_date": "2024-10-18 17:30:20 UTC",
    "updated_date": "2025-02-13 10:43:50 UTC"
  },
  {
    "arxiv_id": "2410.14627v1",
    "title": "CELI: Controller-Embedded Language Model Interactions",
    "authors": [
      "Jan-Samuel Wagner",
      "Dave DeCaprio",
      "Abishek Chiffon Muthu Raja",
      "Jonathan M. Holman",
      "Lauren K. Brady",
      "Sky C. Cheung",
      "Hosein Barzekar",
      "Eric Yang",
      "Mark Anthony Martinez II",
      "David Soong",
      "Sriram Sridhar",
      "Han Si",
      "Brandon W. Higgs",
      "Hisham Hamadeh",
      "Scott Ogden"
    ],
    "abstract": "We introduce Controller-Embedded Language Model Interactions (CELI), a\nframework that integrates control logic directly within language model (LM)\nprompts, facilitating complex, multi-stage task execution. CELI addresses\nlimitations of existing prompt engineering and workflow optimization techniques\nby embedding control logic directly within the operational context of language\nmodels, enabling dynamic adaptation to evolving task requirements. Our\nframework transfers control from the traditional programming execution\nenvironment to the LMs, allowing them to autonomously manage computational\nworkflows while maintaining seamless interaction with external systems and\nfunctions. CELI supports arbitrary function calls with variable arguments,\nbridging the gap between LMs' adaptive reasoning capabilities and conventional\nsoftware paradigms' structured control mechanisms. To evaluate CELI's\nversatility and effectiveness, we conducted case studies in two distinct\ndomains: code generation (HumanEval benchmark) and multi-stage content\ngeneration (Wikipedia-style articles). The results demonstrate notable\nperformance improvements across a range of domains. CELI achieved a 4.9\npercentage point improvement over the best reported score of the baseline GPT-4\nmodel on the HumanEval code generation benchmark. In multi-stage content\ngeneration, 94.4% of CELI-produced Wikipedia-style articles met or exceeded\nfirst draft quality when optimally configured, with 44.4% achieving high\nquality. These outcomes underscore CELI's potential for optimizing AI-driven\nworkflows across diverse computational domains.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "68T50, 68Q32, 68N19",
      "I.2.6; I.2.7; D.2.2"
    ],
    "primary_category": "cs.SE",
    "comment": "26 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.14627v1",
    "published_date": "2024-10-18 17:29:56 UTC",
    "updated_date": "2024-10-18 17:29:56 UTC"
  },
  {
    "arxiv_id": "2410.14616v1",
    "title": "Benchmarking Deep Reinforcement Learning for Navigation in Denied Sensor Environments",
    "authors": [
      "Mariusz Wisniewski",
      "Paraskevas Chatzithanos",
      "Weisi Guo",
      "Antonios Tsourdos"
    ],
    "abstract": "Deep Reinforcement learning (DRL) is used to enable autonomous navigation in\nunknown environments. Most research assume perfect sensor data, but real-world\nenvironments may contain natural and artificial sensor noise and denial. Here,\nwe present a benchmark of both well-used and emerging DRL algorithms in a\nnavigation task with configurable sensor denial effects. In particular, we are\ninterested in comparing how different DRL methods (e.g. model-free PPO vs.\nmodel-based DreamerV3) are affected by sensor denial. We show that DreamerV3\noutperforms other methods in the visual end-to-end navigation task with a\ndynamic goal - and other methods are not able to learn this. Furthermore,\nDreamerV3 generally outperforms other methods in sensor-denied environments. In\norder to improve robustness, we use adversarial training and demonstrate an\nimproved performance in denied environments, although this generally comes with\na performance cost on the vanilla environments. We anticipate this benchmark of\ndifferent DRL methods and the usage of adversarial training to be a starting\npoint for the development of more elaborate navigation strategies that are\ncapable of dealing with uncertain and denied sensor readings.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "31 pages, 19 figures. For associated code, see\n  https://github.com/mazqtpopx/cranfield-navigation-gym",
    "pdf_url": "http://arxiv.org/pdf/2410.14616v1",
    "published_date": "2024-10-18 17:14:28 UTC",
    "updated_date": "2024-10-18 17:14:28 UTC"
  },
  {
    "arxiv_id": "2410.14615v2",
    "title": "Asymptotically Optimal Change Detection for Unnormalized Pre- and Post-Change Distributions",
    "authors": [
      "Arman Adibi",
      "Sanjeev Kulkarni",
      "H. Vincent Poor",
      "Taposh Banerjee",
      "Vahid Tarokh"
    ],
    "abstract": "This paper addresses the problem of detecting changes when only unnormalized\npre- and post-change distributions are accessible. This situation happens in\nmany scenarios in physics such as in ferromagnetism, crystallography,\nmagneto-hydrodynamics, and thermodynamics, where the energy models are\ndifficult to normalize.\n  Our approach is based on the estimation of the Cumulative Sum (CUSUM)\nstatistics, which is known to produce optimal performance. We first present an\nintuitively appealing approximation method. Unfortunately, this produces a\nbiased estimator of the CUSUM statistics and may cause performance degradation.\nWe then propose the Log-Partition Approximation Cumulative Sum (LPA-CUSUM)\nalgorithm based on thermodynamic integration (TI) in order to estimate the\nlog-ratio of normalizing constants of pre- and post-change distributions. It is\nproved that this approach gives an unbiased estimate of the log-partition\nfunction and the CUSUM statistics, and leads to an asymptotically optimal\nperformance. Moreover, we derive a relationship between the required sample\nsize for thermodynamic integration and the desired detection delay performance,\noffering guidelines for practical parameter selection. Numerical studies are\nprovided demonstrating the efficacy of our approach.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14615v2",
    "published_date": "2024-10-18 17:13:29 UTC",
    "updated_date": "2025-02-11 06:41:22 UTC"
  },
  {
    "arxiv_id": "2410.16327v1",
    "title": "Feint and Attack: Attention-Based Strategies for Jailbreaking and Protecting LLMs",
    "authors": [
      "Rui Pu",
      "Chaozhuo Li",
      "Rui Ha",
      "Zejian Chen",
      "Litian Zhang",
      "Zheng Liu",
      "Lirong Qiu",
      "Xi Zhang"
    ],
    "abstract": "Jailbreak attack can be used to access the vulnerabilities of Large Language\nModels (LLMs) by inducing LLMs to generate the harmful content. And the most\ncommon method of the attack is to construct semantically ambiguous prompts to\nconfuse and mislead the LLMs. To access the security and reveal the intrinsic\nrelation between the input prompt and the output for LLMs, the distribution of\nattention weight is introduced to analyze the underlying reasons. By using\nstatistical analysis methods, some novel metrics are defined to better describe\nthe distribution of attention weight, such as the Attention Intensity on\nSensitive Words (Attn_SensWords), the Attention-based Contextual Dependency\nScore (Attn_DepScore) and Attention Dispersion Entropy (Attn_Entropy). By\nleveraging the distinct characteristics of these metrics, the beam search\nalgorithm and inspired by the military strategy \"Feint and Attack\", an\neffective jailbreak attack strategy named as Attention-Based Attack (ABA) is\nproposed. In the ABA, nested attack prompts are employed to divert the\nattention distribution of the LLMs. In this manner, more harmless parts of the\ninput can be used to attract the attention of the LLMs. In addition, motivated\nby ABA, an effective defense strategy called as Attention-Based Defense (ABD)\nis also put forward. Compared with ABA, the ABD can be used to enhance the\nrobustness of LLMs by calibrating the attention distribution of the input\nprompt. Some comparative experiments have been given to demonstrate the\neffectiveness of ABA and ABD. Therefore, both ABA and ABD can be used to access\nthe security of the LLMs. The comparative experiment results also give a\nlogical explanation that the distribution of attention weight can bring great\ninfluence on the output for LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16327v1",
    "published_date": "2024-10-18 17:02:13 UTC",
    "updated_date": "2024-10-18 17:02:13 UTC"
  },
  {
    "arxiv_id": "2410.14606v2",
    "title": "Streaming Deep Reinforcement Learning Finally Works",
    "authors": [
      "Mohamed Elsayed",
      "Gautham Vasan",
      "A. Rupam Mahmood"
    ],
    "abstract": "Natural intelligence processes experience as a continuous stream, sensing,\nacting, and learning moment-by-moment in real time. Streaming learning, the\nmodus operandi of classic reinforcement learning (RL) algorithms like\nQ-learning and TD, mimics natural learning by using the most recent sample\nwithout storing it. This approach is also ideal for resource-constrained,\ncommunication-limited, and privacy-sensitive applications. However, in deep RL,\nlearners almost always use batch updates and replay buffers, making them\ncomputationally expensive and incompatible with streaming learning. Although\nthe prevalence of batch deep RL is often attributed to its sample efficiency, a\nmore critical reason for the absence of streaming deep RL is its frequent\ninstability and failure to learn, which we refer to as stream barrier. This\npaper introduces the stream-x algorithms, the first class of deep RL algorithms\nto overcome stream barrier for both prediction and control and match sample\nefficiency of batch RL. Through experiments in Mujoco Gym, DM Control Suite,\nand Atari Games, we demonstrate stream barrier in existing algorithms and\nsuccessful stable learning with our stream-x algorithms: stream Q, stream AC,\nand stream TD, achieving the best model-free performance in DM Control Dog\nenvironments. A set of common techniques underlies the stream-x algorithms,\nenabling their success with a single set of hyperparameters and allowing for\neasy extension to other algorithms, thereby reviving streaming RL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14606v2",
    "published_date": "2024-10-18 17:00:29 UTC",
    "updated_date": "2024-12-06 00:55:36 UTC"
  },
  {
    "arxiv_id": "2410.14602v1",
    "title": "How Does Data Diversity Shape the Weight Landscape of Neural Networks?",
    "authors": [
      "Yang Ba",
      "Michelle V. Mancenido",
      "Rong Pan"
    ],
    "abstract": "To enhance the generalization of machine learning models to unseen data,\ntechniques such as dropout, weight decay ($L_2$ regularization), and noise\naugmentation are commonly employed. While regularization methods (i.e., dropout\nand weight decay) are geared toward adjusting model parameters to prevent\noverfitting, data augmentation increases the diversity of the input training\nset, a method purported to improve accuracy and calibration error. In this\npaper, we investigate the impact of each of these techniques on the parameter\nspace of neural networks, with the goal of understanding how they alter the\nweight landscape in transfer learning scenarios. To accomplish this, we employ\nRandom Matrix Theory to analyze the eigenvalue distributions of pre-trained\nmodels, fine-tuned using these techniques but using different levels of data\ndiversity, for the same downstream tasks. We observe that diverse data\ninfluences the weight landscape in a similar fashion as dropout. Additionally,\nwe compare commonly used data augmentation methods with synthetic data created\nby generative models. We conclude that synthetic data can bring more diversity\ninto real input data, resulting in a better performance on out-of-distribution\ntest instances.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14602v1",
    "published_date": "2024-10-18 16:57:05 UTC",
    "updated_date": "2024-10-18 16:57:05 UTC"
  },
  {
    "arxiv_id": "2410.14596v2",
    "title": "Teaching Models to Balance Resisting and Accepting Persuasion",
    "authors": [
      "Elias Stengel-Eskin",
      "Peter Hase",
      "Mohit Bansal"
    ],
    "abstract": "Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Training (or PBT), which\nleverages multi-agent recursive dialogue trees to create data and trains models\nvia preference optimization to accept persuasion when appropriate. PBT allows\nus to use data generated from dialogues between smaller 7-8B models for\ntraining much larger 70B models. Moreover, PBT consistently improves resistance\nto misinformation and resilience to being challenged while also resulting in\nthe best overall performance on holistic data containing both positive and\nnegative persuasion. Crucially, we show that PBT models are better teammates in\nmulti-agent debates across two domains (trivia and commonsense QA). We find\nthat without PBT, pairs of stronger and weaker models have unstable\nperformance, with the order in which the models present their answers\ndetermining whether the team obtains the stronger or weaker model's\nperformance. PBT leads to better and more stable results and less order\ndependence, with the stronger model consistently pulling the weaker one up.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL Camera-Ready. Code:\n  https://github.com/esteng/persuasion_balanced_training",
    "pdf_url": "http://arxiv.org/pdf/2410.14596v2",
    "published_date": "2024-10-18 16:49:36 UTC",
    "updated_date": "2025-02-10 14:09:46 UTC"
  },
  {
    "arxiv_id": "2410.14593v1",
    "title": "Temporal Fair Division of Indivisible Items",
    "authors": [
      "Edith Elkind",
      "Alexander Lam",
      "Mohamad Latifian",
      "Tzeh Yuan Neoh",
      "Nicholas Teh"
    ],
    "abstract": "We study a fair division model where indivisible items arrive sequentially,\nand must be allocated immediately and irrevocably. Previous work on online fair\ndivision has shown impossibility results in achieving approximate envy-freeness\nunder these constraints. In contrast, we consider an informed setting where the\nalgorithm has complete knowledge of future items, and aim to ensure that the\ncumulative allocation at each round satisfies approximate envy-freeness --\nwhich we define as temporal envy-freeness up to one item (TEF1). We focus on\nsettings where items can be exclusively goods or exclusively chores. For goods,\nwhile TEF1 allocations may not always exist, we identify several special cases\nwhere they do -- two agents, two item types, generalized binary valuations,\nunimodal preferences -- and provide polynomial-time algorithms for these cases.\nWe also prove that determining the existence of a TEF1 allocation is NP-hard.\nFor chores, we establish analogous results for the special cases, but present a\nslightly weaker intractability result. We also establish the incompatibility\nbetween TEF1 and Pareto-optimality, with the implication that it is intractable\nto find a TEF1 allocation that maximizes any $p$-mean welfare, even for two\nagents.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14593v1",
    "published_date": "2024-10-18 16:43:36 UTC",
    "updated_date": "2024-10-18 16:43:36 UTC"
  },
  {
    "arxiv_id": "2410.14586v1",
    "title": "Neural Combinatorial Clustered Bandits for Recommendation Systems",
    "authors": [
      "Baran Atalar",
      "Carlee Joe-Wong"
    ],
    "abstract": "We consider the contextual combinatorial bandit setting where in each round,\nthe learning agent, e.g., a recommender system, selects a subset of \"arms,\"\ne.g., products, and observes rewards for both the individual base arms, which\nare a function of known features (called \"context\"), and the super arm (the\nsubset of arms), which is a function of the base arm rewards. The agent's goal\nis to simultaneously learn the unknown reward functions and choose the\nhighest-reward arms. For example, the \"reward\" may represent a user's\nprobability of clicking on one of the recommended products. Conventional bandit\nmodels, however, employ restrictive reward function models in order to obtain\nperformance guarantees. We make use of deep neural networks to estimate and\nlearn the unknown reward functions and propose Neural UCB Clustering\n(NeUClust), which adopts a clustering approach to select the super arm in every\nround by exploiting underlying structure in the context space. Unlike prior\nneural bandit works, NeUClust uses a neural network to estimate the super arm\nreward and select the super arm, thus eliminating the need for a known\noptimization oracle. We non-trivially extend prior neural combinatorial bandit\nworks to prove that NeUClust achieves\n$\\widetilde{O}\\left(\\widetilde{d}\\sqrt{T}\\right)$ regret, where $\\widetilde{d}$\nis the effective dimension of a neural tangent kernel matrix, $T$ the number of\nrounds. Experiments on real world recommendation datasets show that NeUClust\nachieves better regret and reward than other contextual combinatorial and\nneural bandit algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14586v1",
    "published_date": "2024-10-18 16:37:28 UTC",
    "updated_date": "2024-10-18 16:37:28 UTC"
  },
  {
    "arxiv_id": "2410.14584v1",
    "title": "MCSFF: Multi-modal Consistency and Specificity Fusion Framework for Entity Alignment",
    "authors": [
      "Wei Ai",
      "Wen Deng",
      "Hongyi Chen",
      "Jiayi Du",
      "Tao Meng",
      "Yuntao Shou"
    ],
    "abstract": "Multi-modal entity alignment (MMEA) is essential for enhancing knowledge\ngraphs and improving information retrieval and question-answering systems.\nExisting methods often focus on integrating modalities through their\ncomplementarity but overlook the specificity of each modality, which can\nobscure crucial features and reduce alignment accuracy. To solve this, we\npropose the Multi-modal Consistency and Specificity Fusion Framework (MCSFF),\nwhich innovatively integrates both complementary and specific aspects of\nmodalities. We utilize Scale Computing's hyper-converged infrastructure to\noptimize IT management and resource allocation in large-scale data processing.\nOur framework first computes similarity matrices for each modality using\nmodality embeddings to preserve their unique characteristics. Then, an\niterative update method denoises and enhances modality features to fully\nexpress critical information. Finally, we integrate the updated information\nfrom all modalities to create enriched and precise entity representations.\nExperiments show our method outperforms current state-of-the-art MMEA baselines\non the MMKG dataset, demonstrating its effectiveness and practical potential.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 1 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.14584v1",
    "published_date": "2024-10-18 16:35:25 UTC",
    "updated_date": "2024-10-18 16:35:25 UTC"
  },
  {
    "arxiv_id": "2410.14582v4",
    "title": "Do LLMs estimate uncertainty well in instruction-following?",
    "authors": [
      "Juyeon Heo",
      "Miao Xiong",
      "Christina Heinze-Deml",
      "Jaya Narain"
    ],
    "abstract": "Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14582v4",
    "published_date": "2024-10-18 16:32:10 UTC",
    "updated_date": "2025-03-28 15:50:55 UTC"
  },
  {
    "arxiv_id": "2410.14581v3",
    "title": "Optimizing Attention with Mirror Descent: Generalized Max-Margin Token Selection",
    "authors": [
      "Addison Kristanto Julistiono",
      "Davoud Ataee Tarzanagh",
      "Navid Azizan"
    ],
    "abstract": "Attention mechanisms have revolutionized several domains of artificial\nintelligence, such as natural language processing and computer vision, by\nenabling models to selectively focus on relevant parts of the input data. While\nrecent work has characterized the optimization dynamics of gradient descent\n(GD) in attention-based models and the structural properties of its preferred\nsolutions, less is known about more general optimization algorithms such as\nmirror descent (MD). In this paper, we investigate the convergence properties\nand implicit biases of a family of MD algorithms tailored for softmax attention\nmechanisms, with the potential function chosen as the $p$-th power of the\n$\\ell_p$-norm. Specifically, we show that these algorithms converge in\ndirection to a generalized hard-margin SVM with an $\\ell_p$-norm objective when\napplied to a classification problem using a softmax attention model. Notably,\nour theoretical results reveal that the convergence rate is comparable to that\nof traditional GD in simpler models, despite the highly nonlinear and nonconvex\nnature of the present problem. Additionally, we delve into the joint\noptimization dynamics of the key-query matrix and the decoder, establishing\nconditions under which this complex joint optimization converges to their\nrespective hard-margin SVM solutions. Lastly, our numerical experiments on real\ndata demonstrate that MD algorithms improve generalization over standard GD and\nexcel in optimal token selection.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14581v3",
    "published_date": "2024-10-18 16:32:06 UTC",
    "updated_date": "2025-03-21 13:15:52 UTC"
  },
  {
    "arxiv_id": "2410.14579v1",
    "title": "Towards Unsupervised Validation of Anomaly-Detection Models",
    "authors": [
      "Lihi Idan"
    ],
    "abstract": "Unsupervised validation of anomaly-detection models is a highly challenging\ntask. While the common practices for model validation involve a labeled\nvalidation set, such validation sets cannot be constructed when the underlying\ndatasets are unlabeled. The lack of robust and efficient unsupervised\nmodel-validation techniques presents an acute challenge in the implementation\nof automated anomaly-detection pipelines, especially when there exists no prior\nknowledge of the model's performance on similar datasets. This work presents a\nnew paradigm to automated validation of anomaly-detection models, inspired by\nreal-world, collaborative decision-making mechanisms. We focus on two\ncommonly-used, unsupervised model-validation tasks -- model selection and model\nevaluation -- and provide extensive experimental results that demonstrate the\naccuracy and robustness of our approach on both tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14579v1",
    "published_date": "2024-10-18 16:27:04 UTC",
    "updated_date": "2024-10-18 16:27:04 UTC"
  },
  {
    "arxiv_id": "2410.14578v1",
    "title": "Large Language Models Are Overparameterized Text Encoders",
    "authors": [
      "Thennal D K",
      "Tim Fischer",
      "Chris Biemann"
    ],
    "abstract": "Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages of content + 1 for limitations and ethical considerations, 14\n  pages in total including references and appendix, 5+1 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.14578v1",
    "published_date": "2024-10-18 16:26:45 UTC",
    "updated_date": "2024-10-18 16:26:45 UTC"
  },
  {
    "arxiv_id": "2410.14574v1",
    "title": "MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts",
    "authors": [
      "Rachel S. Y. Teo",
      "Tan M. Nguyen"
    ],
    "abstract": "Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled\nscalability in deep learning. SMoE has the potential to exponentially increase\nparameter count while maintaining the efficiency of the model by only\nactivating a small subset of these parameters for a given sample. However, it\nhas been observed that SMoE suffers from unstable training and has difficulty\nadapting to new distributions, leading to the model's lack of robustness to\ndata contamination. To overcome these limitations, we first establish a\nconnection between the dynamics of the expert representations in SMoEs and\ngradient descent on a multi-objective optimization problem. Leveraging our\nframework, we then integrate momentum into SMoE and propose a new family of\nSMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate\nthat MomentumSMoE is more stable and robust than SMoE. In particular, we verify\nthe advantages of MomentumSMoE over SMoE on a variety of practical tasks\nincluding ImageNet-1K object recognition and WikiText-103 language modeling. We\ndemonstrate the applicability of MomentumSMoE to many types of SMoE models,\nincluding those in the Sparse MoE model for vision (V-MoE) and the Generalist\nLanguage Model (GLaM). We also show that other advanced momentum-based\noptimization methods, such as Adam, can be easily incorporated into the\nMomentumSMoE framework for designing new SMoE models with even better\nperformance, almost negligible additional computation cost, and simple\nimplementations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/rachtsy/MomentumSMoE",
    "pdf_url": "http://arxiv.org/pdf/2410.14574v1",
    "published_date": "2024-10-18 16:20:22 UTC",
    "updated_date": "2024-10-18 16:20:22 UTC"
  },
  {
    "arxiv_id": "2410.14573v1",
    "title": "Building Trust in Black-box Optimization: A Comprehensive Framework for Explainability",
    "authors": [
      "Nazanin Nezami",
      "Hadis Anahideh"
    ],
    "abstract": "Optimizing costly black-box functions within a constrained evaluation budget\npresents significant challenges in many real-world applications. Surrogate\nOptimization (SO) is a common resolution, yet its proprietary nature introduced\nby the complexity of surrogate models and the sampling core (e.g., acquisition\nfunctions) often leads to a lack of explainability and transparency. While\nexisting literature has primarily concentrated on enhancing convergence to\nglobal optima, the practical interpretation of newly proposed strategies\nremains underexplored, especially in batch evaluation settings. In this paper,\nwe propose \\emph{Inclusive} Explainability Metrics for Surrogate Optimization\n(IEMSO), a comprehensive set of model-agnostic metrics designed to enhance the\ntransparency, trustworthiness, and explainability of the SO approaches. Through\nthese metrics, we provide both intermediate and post-hoc explanations to\npractitioners before and after performing expensive evaluations to gain trust.\nWe consider four primary categories of metrics, each targeting a specific\naspect of the SO process: Sampling Core Metrics, Batch Properties Metrics,\nOptimization Process Metrics, and Feature Importance. Our experimental\nevaluations demonstrate the significant potential of the proposed metrics\nacross different benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14573v1",
    "published_date": "2024-10-18 16:20:17 UTC",
    "updated_date": "2024-10-18 16:20:17 UTC"
  },
  {
    "arxiv_id": "2410.14571v2",
    "title": "TransBox: EL++-closed Ontology Embedding",
    "authors": [
      "Hui Yang",
      "Jiaoyan Chen",
      "Uli Sattler"
    ],
    "abstract": "OWL (Web Ontology Language) ontologies, which are able to represent both\nrelational and type facts as standard knowledge graphs and complex domain\nknowledge in Description Logic (DL) axioms, are widely adopted in domains such\nas healthcare and bioinformatics. Inspired by the success of knowledge graph\nembeddings, embedding OWL ontologies has gained significant attention in recent\nyears. Current methods primarily focus on learning embeddings for atomic\nconcepts and roles, enabling the evaluation based on normalized axioms through\nspecially designed score functions. However, they often neglect the embedding\nof complex concepts, making it difficult to infer with more intricate axioms.\nThis limitation reduces their effectiveness in advanced reasoning tasks, such\nas Ontology Learning and ontology-mediated Query Answering. In this paper, we\npropose EL++-closed ontology embeddings which are able to represent any logical\nexpressions in DL via composition. Furthermore, we develop TransBox, an\neffective EL++-closed ontology embedding method that can handle many-to-one,\none-to-many and many-to-many relations. Our extensive experiments demonstrate\nthat TransBox often achieves state-of-the-art performance across various\nreal-world datasets for predicting complex axioms.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14571v2",
    "published_date": "2024-10-18 16:17:10 UTC",
    "updated_date": "2025-02-04 09:01:06 UTC"
  },
  {
    "arxiv_id": "2410.14569v3",
    "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
    "authors": [
      "Hanna Kim",
      "Minkyoo Song",
      "Seung Ho Na",
      "Seungwon Shin",
      "Kimin Lee"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, generated\nimpersonation posts where 93.9% of them were deemed authentic, and boosted\nclick rate of phishing links in spear phishing emails by 46.67%. Additionally,\nour findings underscore the limitations of existing safeguards in contemporary\ncommercial LLMs, emphasizing the urgent need for robust security measures to\nprevent the misuse of LLM agents.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "20 pages, To appear in Usenix Security 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.14569v3",
    "published_date": "2024-10-18 16:16:34 UTC",
    "updated_date": "2025-02-03 06:52:55 UTC"
  },
  {
    "arxiv_id": "2410.14567v4",
    "title": "ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions",
    "authors": [
      "Zhiyuan Peng",
      "Jinming Nian",
      "Alexandre Evfimievski",
      "Yi Fang"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has become integral to large language\nmodels (LLMs), particularly for conversational AI systems where user questions\nmay reference knowledge beyond the LLMs' training cutoff. However, many natural\nuser questions lack well-defined answers, either due to limited domain\nknowledge or because the retrieval system returns documents that are relevant\nin appearance but uninformative in content. In such cases, LLMs often produce\nhallucinated answers without flagging them. While recent work has largely\nfocused on questions with false premises, we study out-of-scope questions,\nwhere the retrieved document appears semantically similar to the question but\nlacks the necessary information to answer it. In this paper, we propose a\nguided hallucination-based approach ELOQ to automatically generate a diverse\nset of out-of-scope questions from post-cutoff documents, followed by human\nverification to ensure quality. We use this dataset to evaluate several LLMs on\ntheir ability to detect out-of-scope questions and generate appropriate\nresponses. Finally, we introduce an improved detection method that enhances the\nreliability of LLM-based question-answering systems in handling out-of-scope\nquestions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by SIGIR'25",
    "pdf_url": "http://arxiv.org/pdf/2410.14567v4",
    "published_date": "2024-10-18 16:11:29 UTC",
    "updated_date": "2025-05-04 04:28:02 UTC"
  },
  {
    "arxiv_id": "2410.14766v1",
    "title": "Evaluating Quantized Large Language Models for Code Generation on Low-Resource Language Benchmarks",
    "authors": [
      "Enkhbold Nyamsuren"
    ],
    "abstract": "Democratization of AI is an important topic within the broader topic of the\ndigital divide. This issue is relevant to LLMs, which are becoming popular as\nAI co-pilots but suffer from a lack of accessibility due to high computational\ndemand. In this study, we evaluate whether quantization is a viable approach\ntoward enabling LLMs on generic consumer devices. The study assesses the\nperformance of five quantized code LLMs in Lua code generation tasks. To\nevaluate the impact of quantization, the models with 7B parameters were tested\non a consumer laptop at 2-, 4-, and 8-bit integer precisions and compared to\nnon-quantized code LLMs with 1.3, 2, and 3 billion parameters. Lua is chosen as\na low-level resource language to avoid models' biases related to high-resource\nlanguages. The results suggest that the models quantized at the 4-bit integer\nprecision offer the best trade-off between performance and model size. These\nmodels can be comfortably deployed on an average laptop without a dedicated\nGPU. The performance significantly drops at the 2-bit integer precision. The\nmodels at 8-bit integer precision require more inference time that does not\neffectively translate to better performance. The 4-bit models with 7 billion\nparameters also considerably outperform non-quantized models with lower\nparameter numbers despite having comparable model sizes with respect to storage\nand memory demand. While quantization indeed increases the accessibility of\nsmaller LLMs with 7 billion parameters, these LLMs demonstrate overall low\nperformance (less than 50\\%) on high-precision and low-resource tasks such as\nLua code generation. While accessibility is improved, usability is still not at\nthe practical level comparable to foundational LLMs such as GPT-4o or Llama 3.1\n405B.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14766v1",
    "published_date": "2024-10-18 15:50:59 UTC",
    "updated_date": "2024-10-18 15:50:59 UTC"
  },
  {
    "arxiv_id": "2410.14548v1",
    "title": "Boosting K-means for Big Data by Fusing Data Streaming with Global Optimization",
    "authors": [
      "Ravil Mussabayev",
      "Rustam Mussabayev"
    ],
    "abstract": "K-means clustering is a cornerstone of data mining, but its efficiency\ndeteriorates when confronted with massive datasets. To address this limitation,\nwe propose a novel heuristic algorithm that leverages the Variable Neighborhood\nSearch (VNS) metaheuristic to optimize K-means clustering for big data. Our\napproach is based on the sequential optimization of the partial objective\nfunction landscapes obtained by restricting the Minimum Sum-of-Squares\nClustering (MSSC) formulation to random samples from the original big dataset.\nWithin each landscape, systematically expanding neighborhoods of the currently\nbest (incumbent) solution are explored by reinitializing all degenerate and a\nvarying number of additional centroids. Extensive and rigorous experimentation\non a large number of real-world datasets reveals that by transforming the\ntraditional local search into a global one, our algorithm significantly\nenhances the accuracy and efficiency of K-means clustering in big data\nenvironments, becoming the new state of the art in the field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14548v1",
    "published_date": "2024-10-18 15:43:34 UTC",
    "updated_date": "2024-10-18 15:43:34 UTC"
  },
  {
    "arxiv_id": "2410.14545v1",
    "title": "Tell me what I need to know: Exploring LLM-based (Personalized) Abstractive Multi-Source Meeting Summarization",
    "authors": [
      "Frederic Kirstein",
      "Terry Ruas",
      "Robert Kratel",
      "Bela Gipp"
    ],
    "abstract": "Meeting summarization is crucial in digital communication, but existing\nsolutions struggle with salience identification to generate personalized,\nworkable summaries, and context understanding to fully comprehend the meetings'\ncontent. Previous attempts to address these issues by considering related\nsupplementary resources (e.g., presentation slides) alongside transcripts are\nhindered by models' limited context sizes and handling the additional\ncomplexities of the multi-source tasks, such as identifying relevant\ninformation in additional files and seamlessly aligning it with the meeting\ncontent. This work explores multi-source meeting summarization considering\nsupplementary materials through a three-stage large language model approach:\nidentifying transcript passages needing additional context, inferring relevant\ndetails from supplementary materials and inserting them into the transcript,\nand generating a summary from this enriched transcript. Our multi-source\napproach enhances model understanding, increasing summary relevance by ~9% and\nproducing more content-rich outputs. We introduce a personalization protocol\nthat extracts participant characteristics and tailors summaries accordingly,\nimproving informativeness by ~10%. This work further provides insights on\nperformance-cost trade-offs across four leading model families, including\nedge-device capable options. Our approach can be extended to similar complex\ngenerative tasks benefitting from additional resources and personalization,\nsuch as dialogue systems and action planning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14545v1",
    "published_date": "2024-10-18 15:40:48 UTC",
    "updated_date": "2024-10-18 15:40:48 UTC"
  },
  {
    "arxiv_id": "2410.14544v1",
    "title": "Computational Grounding of Responsibility Attribution and Anticipation in LTLf",
    "authors": [
      "Giuseppe De Giacomo",
      "Emiliano Lorini",
      "Timothy Parker",
      "Gianmarco Parretti"
    ],
    "abstract": "Responsibility is one of the key notions in machine ethics and in the area of\nautonomous systems. It is a multi-faceted notion involving counterfactual\nreasoning about actions and strategies. In this paper, we study different\nvariants of responsibility in a strategic setting based on LTLf. We show a\nconnection with notions in reactive synthesis, including synthesis of winning,\ndominant, and best-effort strategies. This connection provides the building\nblocks for a computational grounding of responsibility including complexity\ncharacterizations and sound, complete, and optimal algorithms for attributing\nand anticipating responsibility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14544v1",
    "published_date": "2024-10-18 15:38:33 UTC",
    "updated_date": "2024-10-18 15:38:33 UTC"
  },
  {
    "arxiv_id": "2410.14765v1",
    "title": "What's New in My Data? Novelty Exploration via Contrastive Generation",
    "authors": [
      "Masaru Isonuma",
      "Ivan Titov"
    ],
    "abstract": "Fine-tuning is widely used to adapt language models for specific goals, often\nleveraging real-world data such as patient records, customer-service\ninteractions, or web content in languages not covered in pre-training. These\ndatasets are typically massive, noisy, and often confidential, making their\ndirect inspection challenging. However, understanding them is essential for\nguiding model deployment and informing decisions about data cleaning or\nsuppressing any harmful behaviors learned during fine-tuning. In this study, we\nintroduce the task of novelty discovery through generation, which aims to\nidentify novel properties of a fine-tuning dataset by generating examples that\nillustrate these properties. Our approach, Contrastive Generative Exploration\n(CGE), assumes no direct access to the data but instead relies on a pre-trained\nmodel and the same model after fine-tuning. By contrasting the predictions of\nthese two models, CGE can generate examples that highlight novel\ncharacteristics of the fine-tuning data. However, this simple approach may\nproduce examples that are too similar to one another, failing to capture the\nfull range of novel phenomena present in the dataset. We address this by\nintroducing an iterative version of CGE, where the previously generated\nexamples are used to update the pre-trained model, and this updated model is\nthen contrasted with the fully fine-tuned model to generate the next example,\npromoting diversity in the generated outputs. Our experiments demonstrate the\neffectiveness of CGE in detecting novel content, such as toxic language, as\nwell as new natural and programming languages. Furthermore, we show that CGE\nremains effective even when models are fine-tuned using differential privacy\ntechniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14765v1",
    "published_date": "2024-10-18 15:24:05 UTC",
    "updated_date": "2024-10-18 15:24:05 UTC"
  },
  {
    "arxiv_id": "2411.02408v2",
    "title": "AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with an LLM-based Empathetic Coworker",
    "authors": [
      "Vedant Das Swain",
      "Qiuyue \"Joy\" Zhong",
      "Jash Rajesh Parekh",
      "Yechan Jeon",
      "Roy Zimmermann",
      "Mary Czerwinski",
      "Jina Suh",
      "Varun Mishra",
      "Koustuv Saha",
      "Javier Hernandez"
    ],
    "abstract": "Client-Service Representatives (CSRs) are vital to organizations. Frequent\ninteractions with disgruntled clients, however, disrupt their mental\nwell-being. To help CSRs regulate their emotions while interacting with uncivil\nclients, we designed Care-Pilot, an LLM-powered assistant, and evaluated its\nefficacy, perception, and use. Our comparative analyses between 665 human and\nCare-Pilot-generated support messages highlight Care-Pilot's ability to adapt\nto and demonstrate empathy in various incivility incidents. Additionally, 143\nCSRs assessed Care-Pilot's empathy as more sincere and actionable than human\nmessages. Finally, we interviewed 20 CSRs who interacted with Care-Pilot in a\nsimulation exercise. They reported that Care-Pilot helped them avoid negative\nthinking, recenter thoughts, and humanize clients; showing potential for\nbridging gaps in coworker support. Yet, they also noted deployment challenges\nand emphasized the indispensability of shared experiences. We discuss future\ndesigns and societal implications of AI-mediated emotional labor, underscoring\nempathy as a critical function for AI assistants for worker mental health.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02408v2",
    "published_date": "2024-10-18 15:22:07 UTC",
    "updated_date": "2025-02-27 16:53:40 UTC"
  },
  {
    "arxiv_id": "2410.14524v1",
    "title": "Less is More: Selective Reduction of CT Data for Self-Supervised Pre-Training of Deep Learning Models with Contrastive Learning Improves Downstream Classification Performance",
    "authors": [
      "Daniel Wolf",
      "Tristan Payer",
      "Catharina Silvia Lisson",
      "Christoph Gerhard Lisson",
      "Meinrad Beer",
      "Michael Götz",
      "Timo Ropinski"
    ],
    "abstract": "Self-supervised pre-training of deep learning models with contrastive\nlearning is a widely used technique in image analysis. Current findings\nindicate a strong potential for contrastive pre-training on medical images.\nHowever, further research is necessary to incorporate the particular\ncharacteristics of these images. We hypothesize that the similarity of medical\nimages hinders the success of contrastive learning in the medical imaging\ndomain. To this end, we investigate different strategies based on deep\nembedding, information theory, and hashing in order to identify and reduce\nredundancy in medical pre-training datasets. The effect of these different\nreduction strategies on contrastive learning is evaluated on two pre-training\ndatasets and several downstream classification tasks. In all of our\nexperiments, dataset reduction leads to a considerable performance gain in\ndownstream tasks, e.g., an AUC score improvement from 0.78 to 0.83 for the\nCOVID CT Classification Grand Challenge, 0.97 to 0.98 for the OrganSMNIST\nClassification Challenge and 0.73 to 0.83 for a brain hemorrhage classification\ntask. Furthermore, pre-training is up to nine times faster due to the dataset\nreduction. In conclusion, the proposed approach highlights the importance of\ndataset quality and provides a transferable approach to improve contrastive\npre-training for classification downstream tasks on medical images.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Published in Computers in Biology and Medicine",
    "pdf_url": "http://arxiv.org/pdf/2410.14524v1",
    "published_date": "2024-10-18 15:08:05 UTC",
    "updated_date": "2024-10-18 15:08:05 UTC"
  },
  {
    "arxiv_id": "2410.14516v5",
    "title": "Do LLMs \"know\" internally when they follow instructions?",
    "authors": [
      "Juyeon Heo",
      "Christina Heinze-Deml",
      "Oussama Elachqar",
      "Kwan Ho Ryan Chan",
      "Shirley Ren",
      "Udhay Nallasamy",
      "Andy Miller",
      "Jaya Narain"
    ],
    "abstract": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. In this work, we investigate whether LLMs encode\ninformation in their representations that correlate with instruction-following\nsuccess - a property we term knowing internally. Our analysis identifies a\ndirection in the input embedding space, termed the instruction-following\ndimension, that predicts whether a response will comply with a given\ninstruction. We find that this dimension generalizes well across unseen tasks\nbut not across unseen instruction types. We demonstrate that modifying\nrepresentations along this dimension improves instruction-following success\nrates compared to random changes, without compromising response quality.\nFurther investigation reveals that this dimension is more closely related to\nthe phrasing of prompts rather than the inherent difficulty of the task or\ninstructions. This work provides insight into the internal workings of LLMs'\ninstruction-following, paving the way for reliable LLM agents.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14516v5",
    "published_date": "2024-10-18 14:55:14 UTC",
    "updated_date": "2025-03-28 15:40:49 UTC"
  },
  {
    "arxiv_id": "2410.14515v2",
    "title": "Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media",
    "authors": [
      "Owen Cook",
      "Charlie Grimshaw",
      "Ben Wu",
      "Sophie Dillon",
      "Jack Hicks",
      "Luke Jones",
      "Thomas Smith",
      "Matyas Szert",
      "Xingyi Song"
    ],
    "abstract": "Misinformation spreads rapidly on social media, confusing the truth and\ntargeting potentially vulnerable people. To effectively mitigate the negative\nimpact of misinformation, it must first be accurately detected before applying\na mitigation strategy, such as X's community notes, which is currently a manual\nprocess. This study takes a knowledge-based approach to misinformation\ndetection, modelling the problem similarly to one of natural language\ninference. The EffiARA annotation framework is introduced, aiming to utilise\ninter- and intra-annotator agreement to understand the reliability of each\nannotator and influence the training of large language models for\nclassification based on annotator reliability. In assessing the EffiARA\nannotation framework, the Russo-Ukrainian Conflict Knowledge-Based\nMisinformation Classification Dataset (RUC-MCD) was developed and made publicly\navailable. This study finds that sample weighting using annotator reliability\nperforms the best, utilising both inter- and intra-annotator agreement and\nsoft-label training. The highest classification performance achieved using\nLlama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 3 figures, 3 tables. Code available here:\n  https://github.com/MiniEggz/ruc-misinfo; annotation framework available here:\n  https://github.com/MiniEggz/EffiARA",
    "pdf_url": "http://arxiv.org/pdf/2410.14515v2",
    "published_date": "2024-10-18 14:54:40 UTC",
    "updated_date": "2025-02-03 18:10:18 UTC"
  },
  {
    "arxiv_id": "2410.14508v1",
    "title": "LEAD: Latent Realignment for Human Motion Diffusion",
    "authors": [
      "Nefeli Andreou",
      "Xi Wang",
      "Victoria Fernández Abrevaya",
      "Marie-Paule Cani",
      "Yiorgos Chrysanthou",
      "Vicky Kalogeiton"
    ],
    "abstract": "Our goal is to generate realistic human motion from natural language. Modern\nmethods often face a trade-off between model expressiveness and text-to-motion\nalignment. Some align text and motion latent spaces but sacrifice\nexpressiveness; others rely on diffusion models producing impressive motions,\nbut lacking semantic meaning in their latent space. This may compromise\nrealism, diversity, and applicability. Here, we address this by combining\nlatent diffusion with a realignment mechanism, producing a novel, semantically\nstructured space that encodes the semantics of language. Leveraging this\ncapability, we introduce the task of textual motion inversion to capture novel\nmotion concepts from a few examples. For motion synthesis, we evaluate LEAD on\nHumanML3D and KIT-ML and show comparable performance to the state-of-the-art in\nterms of realism, diversity, and text-motion consistency. Our qualitative\nanalysis and user study reveal that our synthesized motions are sharper, more\nhuman-like and comply better with the text compared to modern methods. For\nmotion textual inversion, our method demonstrates improved capacity in\ncapturing out-of-distribution characteristics in comparison to traditional\nVAEs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14508v1",
    "published_date": "2024-10-18 14:43:05 UTC",
    "updated_date": "2024-10-18 14:43:05 UTC"
  },
  {
    "arxiv_id": "2410.14506v1",
    "title": "SignAttention: On the Interpretability of Transformer Models for Sign Language Translation",
    "authors": [
      "Pedro Alejandro Dal Bianco",
      "Oscar Agustín Stanchi",
      "Facundo Manuel Quiroga",
      "Franco Ronchetti",
      "Enzo Ferrante"
    ],
    "abstract": "This paper presents the first comprehensive interpretability analysis of a\nTransformer-based Sign Language Translation (SLT) model, focusing on the\ntranslation from video-based Greek Sign Language to glosses and text.\nLeveraging the Greek Sign Language Dataset, we examine the attention mechanisms\nwithin the model to understand how it processes and aligns visual input with\nsequential glosses. Our analysis reveals that the model pays attention to\nclusters of frames rather than individual ones, with a diagonal alignment\npattern emerging between poses and glosses, which becomes less distinct as the\nnumber of glosses increases. We also explore the relative contributions of\ncross-attention and self-attention at each decoding step, finding that the\nmodel initially relies on video frames but shifts its focus to previously\npredicted tokens as the translation progresses. This work contributes to a\ndeeper understanding of SLT models, paving the way for the development of more\ntransparent and reliable translation systems essential for real-world\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at IAI Workshop @ NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.14506v1",
    "published_date": "2024-10-18 14:38:37 UTC",
    "updated_date": "2024-10-18 14:38:37 UTC"
  },
  {
    "arxiv_id": "2410.14763v2",
    "title": "Enabling Scalable Evaluation of Bias Patterns in Medical LLMs",
    "authors": [
      "Hamed Fayyaz",
      "Raphael Poulain",
      "Rahmatollah Beheshti"
    ],
    "abstract": "Large language models (LLMs) have shown impressive potential in helping with\nnumerous medical challenges. Deploying LLMs in high-stakes applications such as\nmedicine, however, brings in many concerns. One major area of concern relates\nto biased behaviors of LLMs in medical applications, leading to unfair\ntreatment of individuals. To pave the way for the responsible and impactful\ndeployment of Med LLMs, rigorous evaluation is a key prerequisite. Due to the\nhuge complexity and variability of different medical scenarios, existing work\nin this domain has primarily relied on using manually crafted datasets for bias\nevaluation. In this study, we present a new method to scale up such bias\nevaluations by automatically generating test cases based on rigorous medical\nevidence. We specifically target the challenges of a) domain-specificity of\nbias characterization, b) hallucinating while generating the test cases, and c)\nvarious dependencies between the health outcomes and sensitive attributes. To\nthat end, we offer new methods to address these challenges integrated with our\ngenerative pipeline, using medical knowledge graphs, medical ontologies, and\ncustomized general LLM evaluation frameworks in our method. Through a series of\nextensive experiments, we show that the test cases generated by our proposed\nmethod can effectively reveal bias patterns in Med LLMs at larger and more\nflexible scales than human-crafted datasets. We publish a large bias evaluation\ndataset using our pipeline, which is dedicated to a few medical case studies. A\nlive demo of our application for vignette generation is available at\nhttps://vignette.streamlit.app. Our code is also available at\nhttps://github.com/healthylaife/autofair.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14763v2",
    "published_date": "2024-10-18 14:17:03 UTC",
    "updated_date": "2025-04-13 03:42:38 UTC"
  },
  {
    "arxiv_id": "2410.14488v1",
    "title": "ANT: Adaptive Noise Schedule for Time Series Diffusion Models",
    "authors": [
      "Seunghan Lee",
      "Kibok Lee",
      "Taeyoung Park"
    ],
    "abstract": "Advances in diffusion models for generative artificial intelligence have\nrecently propagated to the time series (TS) domain, demonstrating\nstate-of-the-art performance on various tasks. However, prior works on TS\ndiffusion models often borrow the framework of existing works proposed in other\ndomains without considering the characteristics of TS data, leading to\nsuboptimal performance. In this work, we propose Adaptive Noise schedule for\nTime series diffusion models (ANT), which automatically predetermines proper\nnoise schedules for given TS datasets based on their statistics representing\nnon-stationarity. Our intuition is that an optimal noise schedule should\nsatisfy the following desiderata: 1) It linearly reduces the non-stationarity\nof TS data so that all diffusion steps are equally meaningful, 2) the data is\ncorrupted to the random noise at the final step, and 3) the number of steps is\nsufficiently large. The proposed method is practical for use in that it\neliminates the necessity of finding the optimal noise schedule with a small\nadditional cost to compute the statistics for given datasets, which can be done\noffline before training. We validate the effectiveness of our method across\nvarious tasks, including TS forecasting, refinement, and generation, on\ndatasets from diverse domains. Code is available at this repository:\nhttps://github.com/seunghan96/ANT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.14488v1",
    "published_date": "2024-10-18 14:16:54 UTC",
    "updated_date": "2024-10-18 14:16:54 UTC"
  },
  {
    "arxiv_id": "2410.14484v1",
    "title": "Transfer Reinforcement Learning in Heterogeneous Action Spaces using Subgoal Mapping",
    "authors": [
      "Kavinayan P. Sivakumar",
      "Yan Zhang",
      "Zachary Bell",
      "Scott Nivison",
      "Michael M. Zavlanos"
    ],
    "abstract": "In this paper, we consider a transfer reinforcement learning problem\ninvolving agents with different action spaces. Specifically, for any new unseen\ntask, the goal is to use a successful demonstration of this task by an expert\nagent in its action space to enable a learner agent learn an optimal policy in\nits own different action space with fewer samples than those required if the\nlearner was learning on its own. Existing transfer learning methods across\ndifferent action spaces either require handcrafted mappings between those\naction spaces provided by human experts, which can induce bias in the learning\nprocedure, or require the expert agent to share its policy parameters with the\nlearner agent, which does not generalize well to unseen tasks. In this work, we\npropose a method that learns a subgoal mapping between the expert agent policy\nand the learner agent policy. Since the expert agent and the learner agent have\ndifferent action spaces, their optimal policies can have different subgoal\ntrajectories. We learn this subgoal mapping by training a Long Short Term\nMemory (LSTM) network for a distribution of tasks and then use this mapping to\npredict the learner subgoal sequence for unseen tasks, thereby improving the\nspeed of learning by biasing the agent's policy towards the predicted learner\nsubgoal sequence. Through numerical experiments, we demonstrate that the\nproposed learning scheme can effectively find the subgoal mapping underlying\nthe given distribution of tasks. Moreover, letting the learner agent imitate\nthe expert agent's policy with the learnt subgoal mapping can significantly\nimprove the sample efficiency and training time of the learner agent in unseen\nnew tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14484v1",
    "published_date": "2024-10-18 14:08:41 UTC",
    "updated_date": "2024-10-18 14:08:41 UTC"
  },
  {
    "arxiv_id": "2410.14481v1",
    "title": "DRL Optimization Trajectory Generation via Wireless Network Intent-Guided Diffusion Models for Optimizing Resource Allocation",
    "authors": [
      "Junjie Wu",
      "Xuming Fang",
      "Dusit Niyato",
      "Jiacheng Wang",
      "Jingyu Wang"
    ],
    "abstract": "With the rapid advancements in wireless communication fields, including\nlow-altitude economies, 6G, and Wi-Fi, the scale of wireless networks continues\nto expand, accompanied by increasing service quality demands. Traditional deep\nreinforcement learning (DRL)-based optimization models can improve network\nperformance by solving non-convex optimization problems intelligently. However,\nthey heavily rely on online deployment and often require extensive initial\ntraining. Online DRL optimization models typically make accurate decisions\nbased on current channel state distributions. When these distributions change,\ntheir generalization capability diminishes, which hinders the responsiveness\nessential for real-time and high-reliability wireless communication networks.\nFurthermore, different users have varying quality of service (QoS) requirements\nacross diverse scenarios, and conventional online DRL methods struggle to\naccommodate this variability. Consequently, exploring flexible and customized\nAI strategies is critical. We propose a wireless network intent (WNI)-guided\ntrajectory generation model based on a generative diffusion model (GDM). This\nmodel can be generated and fine-tuned in real time to achieve the objective and\nmeet the constraints of target intent networks, significantly reducing state\ninformation exposure during wireless communication. Moreover, The WNI-guided\noptimization trajectory generation can be customized to address differentiated\nQoS requirements, enhancing the overall quality of communication in future\nintelligent networks. Extensive simulation results demonstrate that our\napproach achieves greater stability in spectral efficiency variations and\noutperforms traditional DRL optimization models in dynamic communication\nsystems.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14481v1",
    "published_date": "2024-10-18 14:04:38 UTC",
    "updated_date": "2024-10-18 14:04:38 UTC"
  },
  {
    "arxiv_id": "2410.19822v1",
    "title": "Human-Centric eXplainable AI in Education",
    "authors": [
      "Subhankar Maity",
      "Aniket Deroy"
    ],
    "abstract": "As artificial intelligence (AI) becomes more integrated into educational\nenvironments, how can we ensure that these systems are both understandable and\ntrustworthy? The growing demand for explainability in AI systems is a critical\narea of focus. This paper explores Human-Centric eXplainable AI (HCXAI) in the\neducational landscape, emphasizing its role in enhancing learning outcomes,\nfostering trust among users, and ensuring transparency in AI-driven tools,\nparticularly through the innovative use of large language models (LLMs). What\nchallenges arise in the implementation of explainable AI in educational\ncontexts? This paper analyzes these challenges, addressing the complexities of\nAI models and the diverse needs of users. It outlines comprehensive frameworks\nfor developing HCXAI systems that prioritize user understanding and engagement,\nensuring that educators and students can effectively interact with these\ntechnologies. Furthermore, what steps can educators, developers, and\npolicymakers take to create more effective, inclusive, and ethically\nresponsible AI solutions in education? The paper provides targeted\nrecommendations to address this question, highlighting the necessity of\nprioritizing explainability. By doing so, how can we leverage AI's\ntransformative potential to foster equitable and engaging educational\nexperiences that support diverse learners?",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "Preprint. Under Review",
    "pdf_url": "http://arxiv.org/pdf/2410.19822v1",
    "published_date": "2024-10-18 14:02:47 UTC",
    "updated_date": "2024-10-18 14:02:47 UTC"
  },
  {
    "arxiv_id": "2410.14470v1",
    "title": "How Do Training Methods Influence the Utilization of Vision Models?",
    "authors": [
      "Paul Gavrikov",
      "Shashank Agnihotri",
      "Margret Keuper",
      "Janis Keuper"
    ],
    "abstract": "Not all learnable parameters (e.g., weights) contribute equally to a neural\nnetwork's decision function. In fact, entire layers' parameters can sometimes\nbe reset to random values with little to no impact on the model's decisions. We\nrevisit earlier studies that examined how architecture and task complexity\ninfluence this phenomenon and ask: is this phenomenon also affected by how we\ntrain the model? We conducted experimental evaluations on a diverse set of\nImageNet-1k classification models to explore this, keeping the architecture and\ntraining data constant but varying the training pipeline. Our findings reveal\nthat the training method strongly influences which layers become critical to\nthe decision function for a given task. For example, improved training regimes\nand self-supervised training increase the importance of early layers while\nsignificantly under-utilizing deeper layers. In contrast, methods such as\nadversarial training display an opposite trend. Our preliminary results extend\nprevious findings, offering a more nuanced understanding of the inner mechanics\nof neural networks.\n  Code: https://github.com/paulgavrikov/layer_criticality",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at the Interpretable AI: Past, Present and Future Workshop\n  at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.14470v1",
    "published_date": "2024-10-18 13:54:46 UTC",
    "updated_date": "2024-10-18 13:54:46 UTC"
  },
  {
    "arxiv_id": "2410.14461v1",
    "title": "The Propensity for Density in Feed-forward Models",
    "authors": [
      "Nandi Schoots",
      "Alex Jackson",
      "Ali Kholmovaia",
      "Peter McBurney",
      "Murray Shanahan"
    ],
    "abstract": "Does the process of training a neural network to solve a task tend to use all\nof the available weights even when the task could be solved with fewer weights?\nTo address this question we study the effects of pruning fully connected,\nconvolutional and residual models while varying their widths. We find that the\nproportion of weights that can be pruned without degrading performance is\nlargely invariant to model size. Increasing the width of a model has little\neffect on the density of the pruned model relative to the increase in absolute\nsize of the pruned network. In particular, we find substantial prunability\nacross a large range of model sizes, where our biggest model is 50 times as\nwide as our smallest model. We explore three hypotheses that could explain\nthese findings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14461v1",
    "published_date": "2024-10-18 13:40:44 UTC",
    "updated_date": "2024-10-18 13:40:44 UTC"
  },
  {
    "arxiv_id": "2410.14445v2",
    "title": "Toward Generalizing Visual Brain Decoding to Unseen Subjects",
    "authors": [
      "Xiangtao Kong",
      "Kexin Huang",
      "Ping Li",
      "Lei Zhang"
    ],
    "abstract": "Visual brain decoding aims to decode visual information from human brain\nactivities. Despite the great progress, one critical limitation of current\nbrain decoding research lies in the lack of generalization capability to unseen\nsubjects. Prior works typically focus on decoding brain activity of individuals\nbased on the observation that different subjects exhibit different brain\nactivities, while it remains unclear whether brain decoding can be generalized\nto unseen subjects. This study aims to answer this question. We first\nconsolidate an image-fMRI dataset consisting of stimulus-image and\nfMRI-response pairs, involving 177 subjects in the movie-viewing task of the\nHuman Connectome Project (HCP). This dataset allows us to investigate the brain\ndecoding performance with the increase of participants. We then present a\nlearning paradigm that applies uniform processing across all subjects, instead\nof employing different network heads or tokenizers for individuals as in\nprevious methods, which can accommodate a large number of subjects to explore\nthe generalization capability across different subjects. A series of\nexperiments are conducted and we have the following findings. First, the\nnetwork exhibits clear generalization capabilities with the increase of\ntraining subjects. Second, the generalization capability is common to popular\nnetwork architectures (MLP, CNN and Transformer). Third, the generalization\nperformance is affected by the similarity between subjects. Our findings reveal\nthe inherent similarities in brain activities across individuals. With the\nemerging of larger and more comprehensive datasets, it is possible to train a\nbrain decoding foundation model in the future. Codes and models can be found at\nhttps://github.com/Xiangtaokong/TGBD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14445v2",
    "published_date": "2024-10-18 13:04:35 UTC",
    "updated_date": "2024-10-21 01:45:47 UTC"
  },
  {
    "arxiv_id": "2410.14436v1",
    "title": "Learning to refine domain knowledge for biological network inference",
    "authors": [
      "Peiwen Li",
      "Menghua Wu"
    ],
    "abstract": "Perturbation experiments allow biologists to discover causal relationships\nbetween variables of interest, but the sparsity and high dimensionality of\nthese data pose significant challenges for causal structure learning\nalgorithms. Biological knowledge graphs can bootstrap the inference of causal\nstructures in these situations, but since they compile vastly diverse\ninformation, they can bias predictions towards well-studied systems.\nAlternatively, amortized causal structure learning algorithms encode inductive\nbiases through data simulation and train supervised models to recapitulate\nthese synthetic graphs. However, realistically simulating biology is arguably\neven harder than understanding a specific system. In this work, we take\ninspiration from both strategies and propose an amortized algorithm for\nrefining domain knowledge, based on data observations. On real and synthetic\ndatasets, we show that our approach outperforms baselines in recovering ground\ntruth causal graphs and identifying errors in the prior knowledge with limited\ninterventional data.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14436v1",
    "published_date": "2024-10-18 12:53:23 UTC",
    "updated_date": "2024-10-18 12:53:23 UTC"
  },
  {
    "arxiv_id": "2410.14429v1",
    "title": "FashionR2R: Texture-preserving Rendered-to-Real Image Translation with Diffusion Models",
    "authors": [
      "Rui Hu",
      "Qian He",
      "Gaofeng He",
      "Jiedong Zhuang",
      "Huang Chen",
      "Huafeng Liu",
      "Huamin Wang"
    ],
    "abstract": "Modeling and producing lifelike clothed human images has attracted\nresearchers' attention from different areas for decades, with the complexity\nfrom highly articulated and structured content. Rendering algorithms decompose\nand simulate the imaging process of a camera, while are limited by the accuracy\nof modeled variables and the efficiency of computation. Generative models can\nproduce impressively vivid human images, however still lacking in\ncontrollability and editability. This paper studies photorealism enhancement of\nrendered images, leveraging generative power from diffusion models on the\ncontrolled basis of rendering. We introduce a novel framework to translate\nrendered images into their realistic counterparts, which consists of two\nstages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG).\nIn DKI, we adopt positive (real) domain finetuning and negative (rendered)\ndomain embedding to inject knowledge into a pretrained Text-to-image (T2I)\ndiffusion model. In RIG, we generate the realistic image corresponding to the\ninput rendered image, with a Texture-preserving Attention Control (TAC) to\npreserve fine-grained clothing textures, exploiting the decoupled features\nencoded in the UNet structure. Additionally, we introduce SynFashion dataset,\nfeaturing high-quality digital clothing images with diverse textures. Extensive\nexperimental results demonstrate the superiority and effectiveness of our\nmethod in rendered-to-real image translation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.14429v1",
    "published_date": "2024-10-18 12:48:22 UTC",
    "updated_date": "2024-10-18 12:48:22 UTC"
  },
  {
    "arxiv_id": "2410.14425v1",
    "title": "Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation",
    "authors": [
      "Shuai Zhao",
      "Xiaobao Wu",
      "Cong-Duy Nguyen",
      "Meihuizi Jia",
      "Yichao Feng",
      "Luu Anh Tuan"
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct experiments on text classification tasks involving\nthree state-of-the-art language models and three different backdoor attack\nalgorithms. Our empirical results demonstrate the outstanding performance of\nW2SDefense in defending against backdoor attacks without compromising model\nperformance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14425v1",
    "published_date": "2024-10-18 12:39:32 UTC",
    "updated_date": "2024-10-18 12:39:32 UTC"
  },
  {
    "arxiv_id": "2410.14416v1",
    "title": "An explainable machine learning approach for energy forecasting at the household level",
    "authors": [
      "Pauline Béraud",
      "Margaux Rioux",
      "Michel Babany",
      "Philippe de La Chevasnerie",
      "Damien Theis",
      "Giacomo Teodori",
      "Chloé Pinguet",
      "Romane Rigaud",
      "François Leclerc"
    ],
    "abstract": "Electricity forecasting has been a recurring research topic, as it is key to\nfinding the right balance between production and consumption. While most papers\nare focused on the national or regional scale, few are interested in the\nhousehold level. Desegregated forecast is a common topic in Machine Learning\n(ML) literature but lacks explainability that household energy forecasts\nrequire. This paper specifically targets the challenges of forecasting\nelectricity use at the household level. This paper confronts common Machine\nLearning algorithms to electricity household forecasts, weighing the pros and\ncons, including accuracy and explainability with well-known key metrics.\nFurthermore, we also confront them in this paper with the business challenges\nspecific to this sector such as explainability or outliers resistance. We\nintroduce a custom decision tree, aiming at providing a fair estimate of the\nenergy consumption, while being explainable and consistent with human\nintuition. We show that this novel method allows greater explainability without\nsacrificing much accuracy. The custom tree methodology can be used in various\nbusiness use cases but is subject to limitations, such as a lack of resilience\nwith outliers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14416v1",
    "published_date": "2024-10-18 12:29:10 UTC",
    "updated_date": "2024-10-18 12:29:10 UTC"
  },
  {
    "arxiv_id": "2410.14395v1",
    "title": "Generative AI, Pragmatics, and Authenticity in Second Language Learning",
    "authors": [
      "Robert Godwin-Jones`"
    ],
    "abstract": "There are obvious benefits to integrating generative AI (artificial\nintelligence) into language learning and teaching. Those include using AI as a\nlanguage tutor, creating learning materials, or assessing learner output.\nHowever, due to how AI systems under-stand human language, based on a\nmathematical model using statistical probability, they lack the lived\nexperience to be able to use language with the same social aware-ness as\nhumans. Additionally, there are built-in linguistic and cultural biases based\non their training data which is mostly in English and predominantly from\nWestern sources. Those facts limit AI suitability for some language learning\ninteractions. Stud-ies have clearly shown that systems such as ChatGPT often do\nnot produce language that is pragmatically appropriate. The lack of linguistic\nand cultural authenticity has important implications for how AI is integrated\ninto second language acquisition as well as in instruction targeting\ndevelopment of intercultural communication compe-tence.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14395v1",
    "published_date": "2024-10-18 11:58:03 UTC",
    "updated_date": "2024-10-18 11:58:03 UTC"
  },
  {
    "arxiv_id": "2410.14393v1",
    "title": "Debug Smarter, Not Harder: AI Agents for Error Resolution in Computational Notebooks",
    "authors": [
      "Konstantin Grotov",
      "Artem Borzilov",
      "Maksim Krivobok",
      "Timofey Bryksin",
      "Yaroslav Zharov"
    ],
    "abstract": "Computational notebooks became indispensable tools for research-related\ndevelopment, offering unprecedented interactivity and flexibility in the\ndevelopment process. However, these benefits come at the cost of\nreproducibility and an increased potential for bugs. With the rise of\ncode-fluent Large Language Models empowered with agentic techniques, smart\nbug-fixing tools with a high level of autonomy have emerged. However, those\ntools are tuned for classical script programming and still struggle with\nnon-linear computational notebooks. In this paper, we present an AI agent\ndesigned specifically for error resolution in a computational notebook. We have\ndeveloped an agentic system capable of exploring a notebook environment by\ninteracting with it -- similar to how a user would -- and integrated the system\ninto the JetBrains service for collaborative data science called Datalore. We\nevaluate our approach against the pre-existing single-action solution by\ncomparing costs and conducting a user study. Users rate the error resolution\ncapabilities of the agentic system higher but experience difficulties with UI.\nWe share the results of the study and consider them valuable for further\nimproving user-agent collaboration.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to EMNLP 2024 System Demonstrations",
    "pdf_url": "http://arxiv.org/pdf/2410.14393v1",
    "published_date": "2024-10-18 11:55:34 UTC",
    "updated_date": "2024-10-18 11:55:34 UTC"
  },
  {
    "arxiv_id": "2410.14389v1",
    "title": "SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task Learning with Deep Representation Surgery",
    "authors": [
      "Enneng Yang",
      "Li Shen",
      "Zhenyi Wang",
      "Guibing Guo",
      "Xingwei Wang",
      "Xiaocun Cao",
      "Jie Zhang",
      "Dacheng Tao"
    ],
    "abstract": "Model merging-based multitask learning (MTL) offers a promising approach for\nperforming MTL by merging multiple expert models without requiring access to\nraw training data. However, in this paper, we examine the merged model's\nrepresentation distribution and uncover a critical issue of \"representation\nbias\". This bias arises from a significant distribution gap between the\nrepresentations of the merged and expert models, leading to the suboptimal\nperformance of the merged MTL model. To address this challenge, we first\npropose a representation surgery solution called Surgery. Surgery is a\nlightweight, task-specific module that aligns the final layer representations\nof the merged model with those of the expert models, effectively alleviating\nbias and improving the merged model's performance. Despite these improvements,\na performance gap remains compared to the traditional MTL method. Further\nanalysis reveals that representation bias phenomena exist at each layer of the\nmerged model, and aligning representations only in the last layer is\ninsufficient for fully reducing systemic bias because biases introduced at each\nlayer can accumulate and interact in complex ways. To tackle this, we then\npropose a more comprehensive solution, deep representation surgery (also called\nSurgeryV2), which mitigates representation bias across all layers, and thus\nbridges the performance gap between model merging-based MTL and traditional\nMTL. Finally, we design an unsupervised optimization objective to optimize both\nthe Surgery and SurgeryV2 modules. Our experimental results show that\nincorporating these modules into state-of-the-art (SOTA) model merging schemes\nleads to significant performance gains. Notably, our SurgeryV2 scheme reaches\nalmost the same level as individual expert models or the traditional MTL model.\nThe code is available at \\url{https://github.com/EnnengYang/SurgeryV2}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is an extended version of our previous work\n  [arXiv:2402.02705] presented at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.14389v1",
    "published_date": "2024-10-18 11:49:40 UTC",
    "updated_date": "2024-10-18 11:49:40 UTC"
  },
  {
    "arxiv_id": "2410.16324v1",
    "title": "CybORG++: An Enhanced Gym for the Development of Autonomous Cyber Agents",
    "authors": [
      "Harry Emerson",
      "Liz Bates",
      "Chris Hicks",
      "Vasilios Mavroudis"
    ],
    "abstract": "CybORG++ is an advanced toolkit for reinforcement learning research focused\non network defence. Building on the CAGE 2 CybORG environment, it introduces\nkey improvements, including enhanced debugging capabilities, refined agent\nimplementation support, and a streamlined environment that enables faster\ntraining and easier customisation. Along with addressing several software bugs\nfrom its predecessor, CybORG++ introduces MiniCAGE, a lightweight version of\nCAGE 2, which improves performance dramatically, up to 1000x faster execution\nin parallel iterations, without sacrificing accuracy or core functionality.\nCybORG++ serves as a robust platform for developing and evaluating defensive\nagents, making it a valuable resource for advancing enterprise network defence\nresearch.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "8 pages, 3 figures and included appendix",
    "pdf_url": "http://arxiv.org/pdf/2410.16324v1",
    "published_date": "2024-10-18 11:04:07 UTC",
    "updated_date": "2024-10-18 11:04:07 UTC"
  },
  {
    "arxiv_id": "2410.14371v1",
    "title": "Interpretable end-to-end Neurosymbolic Reinforcement Learning agents",
    "authors": [
      "Nils Grandien",
      "Quentin Delfosse",
      "Kristian Kersting"
    ],
    "abstract": "Deep reinforcement learning (RL) agents rely on shortcut learning, preventing\nthem from generalizing to slightly different environments. To address this\nproblem, symbolic method, that use object-centric states, have been developed.\nHowever, comparing these methods to deep agents is not fair, as these last\noperate from raw pixel-based states. In this work, we instantiate the symbolic\nSCoBots framework. SCoBots decompose RL tasks into intermediate, interpretable\nrepresentations, culminating in action decisions based on a comprehensible set\nof object-centric relational concepts. This architecture aids in demystifying\nagent decisions. By explicitly learning to extract object-centric\nrepresentations from raw states, object-centric RL, and policy distillation via\nrule extraction, this work places itself within the neurosymbolic AI paradigm,\nblending the strengths of neural networks with symbolic AI. We present the\nfirst implementation of an end-to-end trained SCoBot, separately evaluate of\nits components, on different Atari games. The results demonstrate the\nframework's potential to create interpretable and performing RL systems, and\npave the way for future research directions in obtaining end-to-end\ninterpretable RL agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages; 5 figures; 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.14371v1",
    "published_date": "2024-10-18 10:59:13 UTC",
    "updated_date": "2024-10-18 10:59:13 UTC"
  },
  {
    "arxiv_id": "2410.14368v2",
    "title": "CoMAL: Collaborative Multi-Agent Large Language Models for Mixed-Autonomy Traffic",
    "authors": [
      "Huaiyuan Yao",
      "Longchao Da",
      "Vishnu Nandam",
      "Justin Turnau",
      "Zhiwei Liu",
      "Linsey Pang",
      "Hua Wei"
    ],
    "abstract": "The integration of autonomous vehicles into urban traffic has great potential\nto improve efficiency by reducing congestion and optimizing traffic flow\nsystematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent\nLLMs), a framework designed to address the mixed-autonomy traffic problem by\ncollaboration among autonomous vehicles to optimize traffic flow. CoMAL is\nbuilt upon large language models, operating in an interactive traffic\nsimulation environment. It utilizes a Perception Module to observe surrounding\nagents and a Memory Module to store strategies for each agent. The overall\nworkflow includes a Collaboration Module that encourages autonomous vehicles to\ndiscuss the effective strategy and allocate roles, a reasoning engine to\ndetermine optimal behaviors based on assigned roles, and an Execution Module\nthat controls vehicle actions using a hybrid approach combining rule-based\nmodels. Experimental results demonstrate that CoMAL achieves superior\nperformance on the Flow benchmark. Additionally, we evaluate the impact of\ndifferent language models and compare our framework with reinforcement learning\napproaches. It highlights the strong cooperative capability of LLM agents and\npresents a promising solution to the mixed-autonomy traffic challenge. The code\nis available at https://github.com/Hyan-Yao/CoMAL.",
    "categories": [
      "cs.AI",
      "cs.RO",
      "68T42, 90B20, 90C27",
      "I.2.11; I.2.9; H.4.2"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figures, accepted to SDM25",
    "pdf_url": "http://arxiv.org/pdf/2410.14368v2",
    "published_date": "2024-10-18 10:53:44 UTC",
    "updated_date": "2025-01-09 06:02:11 UTC"
  },
  {
    "arxiv_id": "2410.14353v2",
    "title": "Assistive AI for Augmenting Human Decision-making",
    "authors": [
      "Natabara Máté Gyöngyössy",
      "Bernát Török",
      "Csilla Farkas",
      "Laura Lucaj",
      "Attila Menyhárd",
      "Krisztina Menyhárd-Balázs",
      "András Simonyi",
      "Patrick van der Smagt",
      "Zsolt Ződi",
      "András Lőrincz"
    ],
    "abstract": "Regulatory frameworks for the use of AI are emerging. However, they trail\nbehind the fast-evolving malicious AI technologies that can quickly cause\nlasting societal damage. In response, we introduce a pioneering Assistive AI\nframework designed to enhance human decision-making capabilities. This\nframework aims to establish a trust network across various fields, especially\nwithin legal contexts, serving as a proactive complement to ongoing regulatory\nefforts. Central to our framework are the principles of privacy,\naccountability, and credibility. In our methodology, the foundation of\nreliability of information and information sources is built upon the ability to\nuphold accountability, enhance security, and protect privacy. This approach\nsupports, filters, and potentially guides communication, thereby empowering\nindividuals and communities to make well-informed decisions based on\ncutting-edge advancements in AI. Our framework uses the concept of Boards as\nproxies to collectively ensure that AI-assisted decisions are reliable,\naccountable, and in alignment with societal values and legal standards. Through\na detailed exploration of our framework, including its main components,\noperations, and sample use cases, the paper shows how AI can assist in the\ncomplex process of decision-making while maintaining human oversight. The\nproposed framework not only extends regulatory landscapes but also highlights\nthe synergy between AI technology and human judgement, underscoring the\npotential of AI to serve as a vital instrument in discerning reality from\nfiction and thus enhancing the decision-making process. Furthermore, we provide\ndomain-specific use cases to highlight the applicability of our framework.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "37 pages, 6 figures; Changes: Revised references (k-anonimity)",
    "pdf_url": "http://arxiv.org/pdf/2410.14353v2",
    "published_date": "2024-10-18 10:16:07 UTC",
    "updated_date": "2024-10-26 18:41:41 UTC"
  },
  {
    "arxiv_id": "2410.14347v1",
    "title": "A Scientific Machine Learning Approach for Predicting and Forecasting Battery Degradation in Electric Vehicles",
    "authors": [
      "Sharv Murgai",
      "Hrishikesh Bhagwat",
      "Raj Abhijit Dandekar",
      "Rajat Dandekar",
      "Sreedath Panat"
    ],
    "abstract": "Carbon emissions are rising at an alarming rate, posing a significant threat\nto global efforts to mitigate climate change. Electric vehicles have emerged as\na promising solution, but their reliance on lithium-ion batteries introduces\nthe critical challenge of battery degradation. Accurate prediction and\nforecasting of battery degradation over both short and long time spans are\nessential for optimizing performance, extending battery life, and ensuring\neffective long-term energy management. This directly influences the\nreliability, safety, and sustainability of EVs, supporting their widespread\nadoption and aligning with key UN SDGs. In this paper, we present a novel\napproach to the prediction and long-term forecasting of battery degradation\nusing Scientific Machine Learning framework which integrates domain knowledge\nwith neural networks, offering more interpretable and scientifically grounded\nsolutions for both predicting short-term battery health and forecasting\ndegradation over extended periods. This hybrid approach captures both known and\nunknown degradation dynamics, improving predictive accuracy while reducing data\nrequirements. We incorporate ground-truth data to inform our models, ensuring\nthat both the predictions and forecasts reflect practical conditions. The model\nachieved MSE of 9.90 with the UDE and 11.55 with the NeuralODE, in experimental\ndata, a loss of 1.6986 with the UDE, and a MSE of 2.49 in the NeuralODE,\ndemonstrating the enhanced precision of our approach. This integration of\ndata-driven insights with SciML's strengths in interpretability and scalability\nallows for robust battery management. By enhancing battery longevity and\nminimizing waste, our approach contributes to the sustainability of energy\nsystems and accelerates the global transition toward cleaner, more responsible\nenergy solutions, aligning with the UN's SDG agenda.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14347v1",
    "published_date": "2024-10-18 09:57:59 UTC",
    "updated_date": "2024-10-18 09:57:59 UTC"
  },
  {
    "arxiv_id": "2410.14311v2",
    "title": "Game Theory with Simulation in the Presence of Unpredictable Randomisation",
    "authors": [
      "Vojtech Kovarik",
      "Nathaniel Sauerberg",
      "Lewis Hammond",
      "Vincent Conitzer"
    ],
    "abstract": "AI agents will be predictable in certain ways that traditional agents are\nnot. Where and how can we leverage this predictability in order to improve\nsocial welfare? We study this question in a game-theoretic setting where one\nagent can pay a fixed cost to simulate the other in order to learn its mixed\nstrategy. As a negative result, we prove that, in contrast to prior work on\npure-strategy simulation, enabling mixed-strategy simulation may no longer lead\nto improved outcomes for both players in all so-called \"generalised trust\ngames\". In fact, mixed-strategy simulation does not help in any game where the\nsimulatee's action can depend on that of the simulator. We also show that, in\ngeneral, deciding whether simulation introduces Pareto-improving Nash\nequilibria in a given game is NP-hard. As positive results, we establish that\nmixed-strategy simulation can improve social welfare if the simulator has the\noption to scale their level of trust, if the players face challenges with both\ntrust and coordination, or if maintaining some level of privacy is essential\nfor enabling cooperation.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14311v2",
    "published_date": "2024-10-18 09:17:18 UTC",
    "updated_date": "2025-02-07 11:18:31 UTC"
  },
  {
    "arxiv_id": "2410.14310v1",
    "title": "Transferring Tactile Data Across Sensors",
    "authors": [
      "Wadhah Zai El Amri",
      "Malte Kuhlmann",
      "Nicolás Navarro-Guerrero"
    ],
    "abstract": "Tactile perception is essential for human interaction with the environment\nand is becoming increasingly crucial in robotics. Tactile sensors like the\nBioTac mimic human fingertips and provide detailed interaction data. Despite\nits utility in applications like slip detection and object identification, this\nsensor is now deprecated, making many existing datasets obsolete. This article\nintroduces a novel method for translating data between tactile sensors by\nexploiting sensor deformation information rather than output signals. We\ndemonstrate the approach by translating BioTac signals into the DIGIT sensor.\nOur framework consists of three steps: first, converting signal data into\ncorresponding 3D deformation meshes; second, translating these 3D deformation\nmeshes from one sensor to another; and third, generating output images using\nthe converted meshes. Our approach enables the continued use of valuable\ndatasets.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Extended Abstract. Accepted in ICRA@40 (40th Anniversary of the IEEE\n  International Conference on Robotics and Automation) 23-26 September, 2024\n  Rotterdam, Netherlands",
    "pdf_url": "http://arxiv.org/pdf/2410.14310v1",
    "published_date": "2024-10-18 09:15:47 UTC",
    "updated_date": "2024-10-18 09:15:47 UTC"
  },
  {
    "arxiv_id": "2410.14309v2",
    "title": "LoGU: Long-form Generation with Uncertainty Expressions",
    "authors": [
      "Ruihan Yang",
      "Caiqi Zhang",
      "Zhisong Zhang",
      "Xinting Huang",
      "Sen Yang",
      "Nigel Collier",
      "Dong Yu",
      "Deqing Yang"
    ],
    "abstract": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14309v2",
    "published_date": "2024-10-18 09:15:35 UTC",
    "updated_date": "2024-10-24 18:26:39 UTC"
  },
  {
    "arxiv_id": "2410.14289v1",
    "title": "SwaQuAD-24: QA Benchmark Dataset in Swahili",
    "authors": [
      "Alfred Malengo Kondoro"
    ],
    "abstract": "This paper proposes the creation of a Swahili Question Answering (QA)\nbenchmark dataset, aimed at addressing the underrepresentation of Swahili in\nnatural language processing (NLP). Drawing from established benchmarks like\nSQuAD, GLUE, KenSwQuAD, and KLUE, the dataset will focus on providing\nhigh-quality, annotated question-answer pairs that capture the linguistic\ndiversity and complexity of Swahili. The dataset is designed to support a\nvariety of applications, including machine translation, information retrieval,\nand social services like healthcare chatbots. Ethical considerations, such as\ndata privacy, bias mitigation, and inclusivity, are central to the dataset\ndevelopment. Additionally, the paper outlines future expansion plans to include\ndomain-specific content, multimodal integration, and broader crowdsourcing\nefforts. The Swahili QA dataset aims to foster technological innovation in East\nAfrica and provide an essential resource for NLP research and applications in\nlow-resource languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14289v1",
    "published_date": "2024-10-18 08:49:24 UTC",
    "updated_date": "2024-10-18 08:49:24 UTC"
  },
  {
    "arxiv_id": "2410.14285v1",
    "title": "Advanced Underwater Image Quality Enhancement via Hybrid Super-Resolution Convolutional Neural Networks and Multi-Scale Retinex-Based Defogging Techniques",
    "authors": [
      "Yugandhar Reddy Gogireddy",
      "Jithendra Reddy Gogireddy"
    ],
    "abstract": "The difficulties of underwater image degradation due to light scattering,\nabsorption, and fog-like particles which lead to low resolution and poor\nvisibility are discussed in this study report. We suggest a sophisticated\nhybrid strategy that combines Multi-Scale Retinex (MSR) defogging methods with\nSuper-Resolution Convolutional Neural Networks (SRCNN) to address these\nproblems. The Retinex algorithm mimics human visual perception to reduce uneven\nlighting and fogging, while the SRCNN component improves the spatial resolution\nof underwater photos.Through the combination of these methods, we are able to\nenhance the clarity, contrast, and colour restoration of underwater images,\noffering a reliable way to improve image quality in difficult underwater\nconditions. The research conducts extensive experiments on real-world\nunderwater datasets to further illustrate the efficacy of the suggested\napproach. In terms of sharpness, visibility, and feature retention,\nquantitative evaluation which use metrics like the Structural Similarity Index\nMeasure (SSIM) and Peak Signal-to-Noise Ratio (PSNR) demonstrates notable\nadvances over conventional techniques.In real-time underwater applications like\nmarine exploration, underwater robotics, and autonomous underwater vehicles,\nwhere clear and high-resolution imaging is crucial for operational success, the\ncombination of deep learning and conventional image processing techniques\noffers a computationally efficient framework with superior results.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14285v1",
    "published_date": "2024-10-18 08:40:26 UTC",
    "updated_date": "2024-10-18 08:40:26 UTC"
  },
  {
    "arxiv_id": "2410.14273v1",
    "title": "REEF: Representation Encoding Fingerprints for Large Language Models",
    "authors": [
      "Jie Zhang",
      "Dongrui Liu",
      "Chen Qian",
      "Linfeng Zhang",
      "Yong Liu",
      "Yu Qiao",
      "Jing Shao"
    ],
    "abstract": "Protecting the intellectual property of open-source Large Language Models\n(LLMs) is very important, because training LLMs costs extensive computational\nresources and data. Therefore, model owners and third parties need to identify\nwhether a suspect model is a subsequent development of the victim model. To\nthis end, we propose a training-free REEF to identify the relationship between\nthe suspect and victim models from the perspective of LLMs' feature\nrepresentations. Specifically, REEF computes and compares the centered kernel\nalignment similarity between the representations of a suspect model and a\nvictim model on the same samples. This training-free REEF does not impair the\nmodel's general capabilities and is robust to sequential fine-tuning, pruning,\nmodel merging, and permutations. In this way, REEF provides a simple and\neffective way for third parties and models' owners to protect LLMs'\nintellectual property together. The code is available at\nhttps://github.com/tmylla/REEF.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14273v1",
    "published_date": "2024-10-18 08:27:02 UTC",
    "updated_date": "2024-10-18 08:27:02 UTC"
  },
  {
    "arxiv_id": "2410.14257v1",
    "title": "Revisiting SLO and Goodput Metrics in LLM Serving",
    "authors": [
      "Zhibin Wang",
      "Shipeng Li",
      "Yuhang Zhou",
      "Xue Li",
      "Rong Gu",
      "Nguyen Cam-Tu",
      "Chen Tian",
      "Sheng Zhong"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable performance and are\nwidely deployed in various applications, while the serving of LLM inference has\nraised concerns about user experience and serving throughput. Accordingly,\nservice level objectives (SLOs) and goodput-the number of requests that meet\nSLOs per second-are introduced to evaluate the performance of LLM serving.\nHowever, existing metrics fail to capture the nature of user experience. We\nobserve two ridiculous phenomena in existing metrics: 1) delaying token\ndelivery can smooth the tail time between tokens (tail TBT) of a request and 2)\ndropping the request that fails to meet the SLOs midway can improve goodput.\n  In this paper, we revisit SLO and goodput metrics in LLM serving and propose\na unified metric framework smooth goodput including SLOs and goodput to reflect\nthe nature of user experience in LLM serving. The framework can adapt to\nspecific goals of different tasks by setting parameters. We re-evaluate the\nperformance of different LLM serving systems under multiple workloads based on\nthis unified framework and provide possible directions for future optimization\nof existing strategies. We hope that this framework can provide a unified\nstandard for evaluating LLM serving and foster researches in the field of LLM\nserving optimization to move in a cohesive direction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14257v1",
    "published_date": "2024-10-18 08:05:37 UTC",
    "updated_date": "2024-10-18 08:05:37 UTC"
  },
  {
    "arxiv_id": "2410.14255v2",
    "title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
    "authors": [
      "Xiang Hu",
      "Hongyu Fu",
      "Jinge Wang",
      "Yifeng Wang",
      "Zhikun Li",
      "Renjun Xu",
      "Yu Lu",
      "Yaochu Jin",
      "Lili Pan",
      "Zhenzhong Lan"
    ],
    "abstract": "Scientific innovation is pivotal for humanity, and harnessing large language\nmodels (LLMs) to generate research ideas could transform discovery. However,\nexisting LLMs often produce simplistic and repetitive suggestions due to their\nlimited ability in acquiring external knowledge for innovation. To address this\nproblem, we introduce an enhanced planning and search methodology designed to\nboost the creative potential of LLM-based systems. Our approach involves an\niterative process to purposely plan the retrieval of external knowledge,\nprogressively enriching the idea generation with broader and deeper insights.\nValidation through automated and human assessments indicates that our framework\nsubstantially elevates the quality of generated ideas, particularly in novelty\nand diversity. The number of unique novel ideas produced by our framework is\n3.4 times higher than without it. Moreover, our method outperforms the current\nstate-of-the-art, generating at least 2.5 times more top-rated ideas based on\n170 seed papers in a Swiss Tournament evaluation.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14255v2",
    "published_date": "2024-10-18 08:04:36 UTC",
    "updated_date": "2024-10-27 04:02:32 UTC"
  },
  {
    "arxiv_id": "2410.14251v2",
    "title": "Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation",
    "authors": [
      "Shuo Tang",
      "Xianghe Pang",
      "Zexi Liu",
      "Bohan Tang",
      "Rui Ye",
      "Tian Jin",
      "Xiaowen Dong",
      "Yanfeng Wang",
      "Siheng Chen"
    ],
    "abstract": "Post-training is essential for enabling large language models (LLMs) to\nfollow human instructions. However, its effectiveness depends on high-quality\ninstruction data, which is challenging to obtain in the real world due to\nprivacy concerns, data scarcity, and high annotation costs. To fill this gap,\ninspired by the recent success of using LLMs to simulate human society, we\npropose MATRIX, a multi-agent simulator that automatically generates diverse\ntext-based scenarios, capturing a wide range of real-world human needs in a\nrealistic and scalable manner. Leveraging these outputs, we introduce a novel\nscenario-driven instruction generator MATRIX-Gen for controllable and highly\nrealistic data synthesis. Extensive experiments demonstrate that our framework\neffectively generates both general and domain-specific data. On AlpacaEval 2\nand Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on datasets\nsynthesized by MATRIX-Gen with just 20K instruction-response pairs, outperforms\nMeta's Llama-3-8B-Instruct model, which was trained on over 10M pairs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14251v2",
    "published_date": "2024-10-18 08:01:39 UTC",
    "updated_date": "2025-02-20 03:26:26 UTC"
  },
  {
    "arxiv_id": "2410.14240v1",
    "title": "Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction",
    "authors": [
      "Manuel Brenner",
      "Christoph Jürgen Hemmer",
      "Zahra Monfared",
      "Daniel Durstewitz"
    ],
    "abstract": "Dynamical systems (DS) theory is fundamental for many areas of science and\nengineering. It can provide deep insights into the behavior of systems evolving\nin time, as typically described by differential or recursive equations. A\ncommon approach to facilitate mathematical tractability and interpretability of\nDS models involves decomposing nonlinear DS into multiple linear DS separated\nby switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are\npopular in engineering and a frequent choice in mathematics for analyzing the\ntopological properties of DS. However, hand-crafting such models is tedious and\nonly possible for very low-dimensional scenarios, while inferring them from\ndata usually gives rise to unnecessarily complex representations with very many\nlinear subregions. Here we introduce Almost-Linear Recurrent Neural Networks\n(AL-RNNs) which automatically and robustly produce most parsimonious PWL\nrepresentations of DS from time series data, using as few PWL nonlinearities as\npossible. AL-RNNs can be efficiently trained with any SOTA algorithm for\ndynamical systems reconstruction (DSR), and naturally give rise to a symbolic\nencoding of the underlying DS that provably preserves important topological\nproperties. We show that for the Lorenz and R\\\"ossler systems, AL-RNNs\ndiscover, in a purely data-driven way, the known topologically minimal PWL\nrepresentations of the corresponding chaotic attractors. We further illustrate\non two challenging empirical datasets that interpretable symbolic encodings of\nthe dynamics can be achieved, tremendously facilitating mathematical and\ncomputational analysis of the underlying systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DS",
      "nlin.CD",
      "physics.data-an"
    ],
    "primary_category": "cs.LG",
    "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.14240v1",
    "published_date": "2024-10-18 07:44:12 UTC",
    "updated_date": "2024-10-18 07:44:12 UTC"
  },
  {
    "arxiv_id": "2410.14755v1",
    "title": "Controllable Discovery of Intents: Incremental Deep Clustering Using Semi-Supervised Contrastive Learning",
    "authors": [
      "Mrinal Rawat",
      "Hithesh Sankararaman",
      "Victor Barres"
    ],
    "abstract": "Deriving value from a conversational AI system depends on the capacity of a\nuser to translate the prior knowledge into a configuration. In most cases,\ndiscovering the set of relevant turn-level speaker intents is often one of the\nkey steps. Purely unsupervised algorithms provide a natural way to tackle\ndiscovery problems but make it difficult to incorporate constraints and only\noffer very limited control over the outcomes. Previous work has shown that\nsemi-supervised (deep) clustering techniques can allow the system to\nincorporate prior knowledge and constraints in the intent discovery process.\nHowever they did not address how to allow for control through human feedback.\nIn our Controllable Discovery of Intents (CDI) framework domain and prior\nknowledge are incorporated using a sequence of unsupervised contrastive\nlearning on unlabeled data followed by fine-tuning on partially labeled data,\nand finally iterative refinement of clustering and representations through\nrepeated clustering and pseudo-label fine-tuning. In addition, we draw from\ncontinual learning literature and use learning-without-forgetting to prevent\ncatastrophic forgetting across those training stages. Finally, we show how this\ndeep-clustering process can become part of an incremental discovery strategy\nwith human-in-the-loop. We report results on both CLINC and BANKING datasets.\nCDI outperforms previous works by a significant margin: 10.26% and 11.72%\nrespectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in IJCNLP'23",
    "pdf_url": "http://arxiv.org/pdf/2410.14755v1",
    "published_date": "2024-10-18 07:24:02 UTC",
    "updated_date": "2024-10-18 07:24:02 UTC"
  },
  {
    "arxiv_id": "2410.14225v2",
    "title": "Few-Shot Joint Multimodal Entity-Relation Extraction via Knowledge-Enhanced Cross-modal Prompt Model",
    "authors": [
      "Li Yuan",
      "Yi Cai",
      "Junsheng Huang"
    ],
    "abstract": "Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task\nthat aims to extract entities and their relations from text-image pairs in\nsocial media posts. Existing methods for JMERE require large amounts of labeled\ndata. However, gathering and annotating fine-grained multimodal data for JMERE\nposes significant challenges. Initially, we construct diverse and comprehensive\nmultimodal few-shot datasets fitted to the original data distribution. To\naddress the insufficient information in the few-shot setting, we introduce the\n\\textbf{K}nowledge-\\textbf{E}nhanced \\textbf{C}ross-modal \\textbf{P}rompt\n\\textbf{M}odel (KECPM) for JMERE. This method can effectively address the\nproblem of insufficient information in the few-shot setting by guiding a large\nlanguage model to generate supplementary background knowledge. Our proposed\nmethod comprises two stages: (1) a knowledge ingestion stage that dynamically\nformulates prompts based on semantic similarity guide ChatGPT generating\nrelevant knowledge and employs self-reflection to refine the knowledge; (2) a\nknowledge-enhanced language model stage that merges the auxiliary knowledge\nwith the original input and utilizes a transformer-based model to align with\nJMERE's required output format. We extensively evaluate our approach on a\nfew-shot dataset derived from the JMERE dataset, demonstrating its superiority\nover strong baselines in terms of both micro and macro F$_1$ scores.\nAdditionally, we present qualitative analyses and case studies to elucidate the\neffectiveness of our model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted by ACM MM 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.14225v2",
    "published_date": "2024-10-18 07:14:54 UTC",
    "updated_date": "2025-03-23 02:01:21 UTC"
  },
  {
    "arxiv_id": "2410.14219v1",
    "title": "Formal Explanations for Neuro-Symbolic AI",
    "authors": [
      "Sushmita Paul",
      "Jinqiang Yu",
      "Jip J. Dekker",
      "Alexey Ignatiev",
      "Peter J. Stuckey"
    ],
    "abstract": "Despite the practical success of Artificial Intelligence (AI), current neural\nAI algorithms face two significant issues. First, the decisions made by neural\narchitectures are often prone to bias and brittleness. Second, when a chain of\nreasoning is required, neural systems often perform poorly. Neuro-symbolic\nartificial intelligence is a promising approach that tackles these (and other)\nweaknesses by combining the power of neural perception and symbolic reasoning.\nMeanwhile, the success of AI has made it critical to understand its behaviour,\nleading to the development of explainable artificial intelligence (XAI). While\nneuro-symbolic AI systems have important advantages over purely neural AI, we\nstill need to explain their actions, which are obscured by the interactions of\nthe neural and symbolic components. To address the issue, this paper proposes a\nformal approach to explaining the decisions of neuro-symbolic systems. The\napproach hinges on the use of formal abductive explanations and on solving the\nneuro-symbolic explainability problem hierarchically. Namely, it first computes\na formal explanation for the symbolic component of the system, which serves to\nidentify a subset of the individual parts of neural information that needs to\nbe explained. This is followed by explaining only those individual neural\ninputs, independently of each other, which facilitates succinctness of\nhierarchical formal explanations and helps to increase the overall performance\nof the approach. Experimental results for a few complex reasoning tasks\ndemonstrate practical efficiency of the proposed approach, in comparison to\npurely neural systems, from the perspective of explanation size, explanation\ntime, training time, model sizes, and the quality of explanations reported.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14219v1",
    "published_date": "2024-10-18 07:08:31 UTC",
    "updated_date": "2024-10-18 07:08:31 UTC"
  },
  {
    "arxiv_id": "2410.14754v2",
    "title": "On the Sparsity of the Strong Lottery Ticket Hypothesis",
    "authors": [
      "Emanuele Natale",
      "Davide Ferre'",
      "Giordano Giambartolomei",
      "Frédéric Giroire",
      "Frederik Mallmann-Trenn"
    ],
    "abstract": "Considerable research efforts have recently been made to show that a random\nneural network $N$ contains subnetworks capable of accurately approximating any\ngiven neural network that is sufficiently smaller than $N$, without any\ntraining. This line of research, known as the Strong Lottery Ticket Hypothesis\n(SLTH), was originally motivated by the weaker Lottery Ticket Hypothesis, which\nstates that a sufficiently large random neural network $N$ contains\n\\emph{sparse} subnetworks that can be trained efficiently to achieve\nperformance comparable to that of training the entire network $N$. Despite its\noriginal motivation, results on the SLTH have so far not provided any guarantee\non the size of subnetworks. Such limitation is due to the nature of the main\ntechnical tool leveraged by these results, the Random Subset Sum (RSS) Problem.\nInformally, the RSS Problem asks how large a random i.i.d. sample $\\Omega$\nshould be so that we are able to approximate any number in $[-1,1]$, up to an\nerror of $ \\epsilon$, as the sum of a suitable subset of $\\Omega$. We provide\nthe first proof of the SLTH in classical settings, such as dense and\nequivariant networks, with guarantees on the sparsity of the subnetworks.\nCentral to our results, is the proof of an essentially tight bound on the\nRandom Fixed-Size Subset Sum Problem (RFSS), a variant of the RSS Problem in\nwhich we only ask for subsets of a given size, which is of independent\ninterest.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14754v2",
    "published_date": "2024-10-18 06:57:37 UTC",
    "updated_date": "2024-10-31 07:50:22 UTC"
  },
  {
    "arxiv_id": "2410.14208v1",
    "title": "Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning",
    "authors": [
      "Xiaochuan Li",
      "Zichun Yu",
      "Chenyan Xiong"
    ],
    "abstract": "Synthetic data has been widely used to train large language models, but their\ngenerative nature inevitably introduces noisy, non-informative, and misleading\nlearning signals. In this paper, we propose Montessori-Instruct, a novel data\nsynthesis framework that tailors the data synthesis ability of the teacher\nlanguage model toward the student language model's learning process.\nSpecifically, we utilize local data influence of synthetic training data points\non students to characterize students' learning preferences. Then, we train the\nteacher model with Direct Preference Optimization (DPO) to generate synthetic\ndata tailored toward student learning preferences. Experiments with\nLlama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and\nMT-Bench demonstrate that Montessori-Instruct significantly outperforms\nstandard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also\nbeats data synthesized by a stronger teacher model, GPT-4o. Further analysis\nconfirms the benefits of teacher's learning to generate more influential\ntraining data in the student's improved learning, the advantages of local data\ninfluence in accurately measuring student preferences, and the robustness of\nMontessori-Instruct across different student models. Our code and data are\nopen-sourced at https://github.com/cxcscmu/Montessori-Instruct.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Codes and data are open-sourced at\n  https://github.com/cxcscmu/Montessori-Instruct",
    "pdf_url": "http://arxiv.org/pdf/2410.14208v1",
    "published_date": "2024-10-18 06:50:15 UTC",
    "updated_date": "2024-10-18 06:50:15 UTC"
  },
  {
    "arxiv_id": "2410.19818v1",
    "title": "UniMTS: Unified Pre-training for Motion Time Series",
    "authors": [
      "Xiyuan Zhang",
      "Diyan Teng",
      "Ranak Roy Chowdhury",
      "Shuheng Li",
      "Dezhi Hong",
      "Rajesh K. Gupta",
      "Jingbo Shang"
    ],
    "abstract": "Motion time series collected from mobile and wearable devices such as\nsmartphones and smartwatches offer significant insights into human behavioral\npatterns, with wide applications in healthcare, automation, IoT, and AR/XR due\nto their low-power, always-on nature. However, given security and privacy\nconcerns, building large-scale motion time series datasets remains difficult,\npreventing the development of pre-trained models for human activity analysis.\nTypically, existing models are trained and tested on the same dataset, leading\nto poor generalizability across variations in device location, device mounting\norientation and human activity type. In this paper, we introduce UniMTS, the\nfirst unified pre-training procedure for motion time series that generalizes\nacross diverse device latent factors and activities. Specifically, we employ a\ncontrastive learning framework that aligns motion time series with text\ndescriptions enriched by large language models. This helps the model learn the\nsemantics of time series to generalize across activities. Given the absence of\nlarge-scale motion time series data, we derive and synthesize time series from\nexisting motion skeleton data with all-joint coverage. Spatio-temporal graph\nnetworks are utilized to capture the relationships across joints for\ngeneralization across different device locations. We further design\nrotation-invariant augmentation to make the model agnostic to changes in device\nmounting orientations. Our model shows exceptional generalizability across 18\nmotion time series classification benchmark datasets, outperforming the best\nbaselines by 340% in the zero-shot setting, 16.3% in the few-shot setting, and\n9.2% in the full-shot setting.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "NeurIPS 2024. Code: https://github.com/xiyuanzh/UniMTS. Model:\n  https://huggingface.co/xiyuanz/UniMTS",
    "pdf_url": "http://arxiv.org/pdf/2410.19818v1",
    "published_date": "2024-10-18 06:39:13 UTC",
    "updated_date": "2024-10-18 06:39:13 UTC"
  },
  {
    "arxiv_id": "2410.14202v3",
    "title": "Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs",
    "authors": [
      "SeongYeub Chu",
      "JongWoo Kim",
      "Bryan Wong",
      "MunYong Yi"
    ],
    "abstract": "Existing automated essay scoring (AES) has solely relied on essay text\nwithout using explanatory rationales for the scores, thereby forgoing an\nopportunity to capture the specific aspects evaluated by rubric indicators in a\nfine-grained manner. This paper introduces Rationale-based Multiple Trait\nScoring (RMTS), a novel approach for multi-trait essay scoring that integrates\nprompt-engineering-based large language models (LLMs) with a fine-tuning-based\nessay scoring model using a smaller large language model (S-LLM). RMTS uses an\nLLM-based trait-wise rationale generation system where a separate LLM agent\ngenerates trait-specific rationales based on rubric guidelines, which the\nscoring model uses to accurately predict multi-trait scores. Extensive\nexperiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,\nshow that RMTS significantly outperforms state-of-the-art models and vanilla\nS-LLMs in trait-specific scoring. By assisting quantitative assessment with\nfine-grained qualitative rationales, RMTS enhances the trait-wise reliability,\nproviding partial explanations about essays. The code is available at\nhttps://github.com/BBeeChu/RMTS.git.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14202v3",
    "published_date": "2024-10-18 06:35:17 UTC",
    "updated_date": "2025-02-05 07:52:14 UTC"
  },
  {
    "arxiv_id": "2410.14198v1",
    "title": "Supervised Chain of Thought",
    "authors": [
      "Xiang Zhang",
      "Dujian Ding"
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing\nand hold immense potential for advancing Artificial Intelligence. However, the\ncore architecture of most mainstream LLMs -- the Transformer -- has inherent\nlimitations in computational depth, rendering them theoretically incapable of\nsolving many reasoning tasks that demand increasingly deep computations. Chain\nof Thought (CoT) prompting has emerged as a technique to address these\narchitectural limitations, as evidenced by several theoretical studies. It\noffers a promising approach to solving complex reasoning tasks that were\npreviously beyond the capabilities of these models. Despite its successes, CoT\nand its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a\n\"one-prompt-for-all\" approach, using a single prompt structure (e.g., \"think\nstep by step\") for a wide range of tasks -- from counting and sorting to\nsolving mathematical and algorithmic problems. This approach poses significant\nchallenges for models to generate the correct reasoning steps, as the model\nmust navigate through a vast prompt template space to find the appropriate\ntemplate for each task. In this work, we build upon previous theoretical\nanalyses of CoT to demonstrate how the one-prompt-for-all approach can\nnegatively affect the computability of LLMs. We partition the solution search\nspace into two: the prompt space and the answer space. Our findings show that\ntask-specific supervision is essential for navigating the prompt space\naccurately and achieving optimal performance. Through experiments with\nstate-of-the-art LLMs, we reveal a gap in reasoning performance when\nsupervision is applied versus when it is not.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14198v1",
    "published_date": "2024-10-18 06:25:27 UTC",
    "updated_date": "2024-10-18 06:25:27 UTC"
  },
  {
    "arxiv_id": "2410.14194v1",
    "title": "Speciesism in Natural Language Processing Research",
    "authors": [
      "Masashi Takeshita",
      "Rafal Rzepka"
    ],
    "abstract": "Natural Language Processing (NLP) research on AI Safety and social bias in AI\nhas focused on safety for humans and social bias against human minorities.\nHowever, some AI ethicists have argued that the moral significance of nonhuman\nanimals has been ignored in AI research. Therefore, the purpose of this study\nis to investigate whether there is speciesism, i.e., discrimination against\nnonhuman animals, in NLP research. First, we explain why nonhuman animals are\nrelevant in NLP research. Next, we survey the findings of existing research on\nspeciesism in NLP researchers, data, and models and further investigate this\nproblem in this study. The findings of this study suggest that speciesism\nexists within researchers, data, and models, respectively. Specifically, our\nsurvey and experiments show that (a) among NLP researchers, even those who\nstudy social bias in AI, do not recognize speciesism or speciesist bias; (b)\namong NLP data, speciesist bias is inherent in the data annotated in the\ndatasets used to evaluate NLP models; (c) OpenAI GPTs, recent NLP models,\nexhibit speciesist bias by default. Finally, we discuss how we can reduce\nspeciesism in NLP research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This article is a preprint and has not been peer-reviewed. The\n  postprint has been accepted for publication in AI and Ethics. Please cite the\n  final version of the article once it is published",
    "pdf_url": "http://arxiv.org/pdf/2410.14194v1",
    "published_date": "2024-10-18 06:09:41 UTC",
    "updated_date": "2024-10-18 06:09:41 UTC"
  },
  {
    "arxiv_id": "2411.02404v1",
    "title": "Enhancing Retrieval Performance: An Ensemble Approach For Hard Negative Mining",
    "authors": [
      "Hansa Meghwani"
    ],
    "abstract": "Ranking consistently emerges as a primary focus in information retrieval\nresearch. Retrieval and ranking models serve as the foundation for numerous\napplications, including web search, open domain QA, enterprise domain QA, and\ntext-based recommender systems. Typically, these models undergo training on\ntriplets consisting of binary relevance assignments, comprising one positive\nand one negative passage. However, their utilization involves a context where a\nsignificantly more nuanced understanding of relevance is necessary, especially\nwhen re-ranking a large pool of potentially relevant passages. Although\ncollecting positive examples through user feedback like impressions or clicks\nis straightforward, identifying suitable negative pairs from a vast pool of\npossibly millions or even billions of documents possess a greater challenge.\nGenerating a substantial number of negative pairs is often necessary to\nmaintain the high quality of the model. Several approaches have been suggested\nin literature to tackle the issue of selecting suitable negative pairs from an\nextensive corpus. This study focuses on explaining the crucial role of hard\nnegatives in the training process of cross-encoder models, specifically aiming\nto explain the performance gains observed with hard negative sampling compared\nto random sampling. We have developed a robust hard negative mining technique\nfor efficient training of cross-encoder re-rank models on an enterprise dataset\nwhich has domain specific context. We provide a novel perspective to enhance\nretrieval models, ultimately influencing the performance of advanced LLM\nsystems like Retrieval-Augmented Generation (RAG) and Reasoning and Action\nAgents (ReAct). The proposed approach demonstrates that learning both\nsimilarity and dissimilarity simultaneously with cross-encoders improves\nperformance of retrieval systems.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Master's thesis",
    "pdf_url": "http://arxiv.org/pdf/2411.02404v1",
    "published_date": "2024-10-18 05:23:39 UTC",
    "updated_date": "2024-10-18 05:23:39 UTC"
  },
  {
    "arxiv_id": "2410.16033v3",
    "title": "TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling",
    "authors": [
      "Jiahao Qiu",
      "Yifu Lu",
      "Yifan Zeng",
      "Jiacheng Guo",
      "Jiayi Geng",
      "Huazheng Wang",
      "Kaixuan Huang",
      "Yue Wu",
      "Mengdi Wang"
    ],
    "abstract": "Inference-time alignment enhances the performance of large language models\nwithout requiring additional training or fine-tuning but presents challenges\ndue to balancing computational efficiency with high-quality output. Best-of-N\n(BoN) sampling, as a simple yet powerful approach, generates multiple responses\nand selects the best one, achieving improved performance but with a high\ncomputational cost. We propose TreeBoN, a novel framework that integrates a\nspeculative tree-search strategy into Best-of-N (BoN) Sampling. TreeBoN\nmaintains a set of parent nodes, iteratively branching and pruning low-quality\nresponses, thereby reducing computational overhead while maintaining high\noutput quality. Our approach also leverages token-level rewards from Direct\nPreference Optimization (DPO) to guide tree expansion and prune low-quality\npaths. We evaluate TreeBoN using AlpacaFarm, HH-RLHF, UltraFeedback, GSM8K, and\nTutorEval datasets, demonstrating consistent improvements. Specifically,\nTreeBoN achieves the highest win rate of 65% on TutorEval and around 60% win\nrates across other different datasets, outperforming standard BoN with the same\ncomputational cost and showcasing its scalability and alignment efficacy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.16033v3",
    "published_date": "2024-10-18 04:38:21 UTC",
    "updated_date": "2024-10-30 00:02:08 UTC"
  },
  {
    "arxiv_id": "2410.14170v2",
    "title": "Personalized Image Generation with Large Multimodal Models",
    "authors": [
      "Yiyan Xu",
      "Wenjie Wang",
      "Yang Zhang",
      "Biao Tang",
      "Peng Yan",
      "Fuli Feng",
      "Xiangnan He"
    ],
    "abstract": "Personalized content filtering, such as recommender systems, has become a\ncritical infrastructure to alleviate information overload. However, these\nsystems merely filter existing content and are constrained by its limited\ndiversity, making it difficult to meet users' varied content needs. To address\nthis limitation, personalized content generation has emerged as a promising\ndirection with broad applications. Nevertheless, most existing research focuses\non personalized text generation, with relatively little attention given to\npersonalized image generation. The limited work in personalized image\ngeneration faces challenges in accurately capturing users' visual preferences\nand needs from noisy user-interacted images and complex multimodal\ninstructions. Worse still, there is a lack of supervised data for training\npersonalized image generation models.\n  To overcome the challenges, we propose a Personalized Image Generation\nFramework named Pigeon, which adopts exceptional large multimodal models with\nthree dedicated modules to capture users' visual preferences and needs from\nnoisy user history and multimodal instructions. To alleviate the data scarcity,\nwe introduce a two-stage preference alignment scheme, comprising masked\npreference reconstruction and pairwise preference alignment, to align Pigeon\nwith the personalized image generation task. We apply Pigeon to personalized\nsticker and movie poster generation, where extensive quantitative results and\nhuman evaluation highlight its superiority over various generative baselines.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted for publication in WWW'25",
    "pdf_url": "http://arxiv.org/pdf/2410.14170v2",
    "published_date": "2024-10-18 04:20:46 UTC",
    "updated_date": "2025-02-02 06:35:42 UTC"
  },
  {
    "arxiv_id": "2411.02403v2",
    "title": "A Persuasion-Based Prompt Learning Approach to Improve Smishing Detection through Data Augmentation",
    "authors": [
      "Ho Sung Shim",
      "Hyoungjun Park",
      "Kyuhan Lee",
      "Jang-Sun Park",
      "Seonhye Kang"
    ],
    "abstract": "Smishing, which aims to illicitly obtain personal information from\nunsuspecting victims, holds significance due to its negative impacts on our\nsociety. In prior studies, as a tool to counteract smishing, machine learning\n(ML) has been widely adopted, which filters and blocks smishing messages before\nthey reach potential victims. However, a number of challenges remain in\nML-based smishing detection, with the scarcity of annotated datasets being one\nmajor hurdle. Specifically, given the sensitive nature of smishing-related\ndata, there is a lack of publicly accessible data that can be used for training\nand evaluating ML models. Additionally, the nuanced similarities between\nsmishing messages and other types of social engineering attacks such as spam\nmessages exacerbate the challenge of smishing classification with limited\nresources. To tackle this challenge, we introduce a novel data augmentation\nmethod utilizing a few-shot prompt learning approach. What sets our approach\napart from extant methods is the use of the principles of persuasion, a\npsychology theory which explains the underlying mechanisms of smishing. By\ndesigning prompts grounded in the persuasion principles, our augmented dataset\ncould effectively capture various, important aspects of smishing messages,\nenabling ML models to be effectively trained. Our evaluation within a\nreal-world context demonstrates that our augmentation approach produces more\ndiverse and higher-quality smishing data instances compared to other\ncutting-edging approaches, leading to substantial improvements in the ability\nof ML models to detect the subtle characteristics of smishing messages.\nMoreover, our additional analyses reveal that the performance improvement\nprovided by our approach is more pronounced when used with ML models that have\na larger number of parameters, demonstrating its effectiveness in training\nlarge-scale ML models.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02403v2",
    "published_date": "2024-10-18 04:20:02 UTC",
    "updated_date": "2024-11-06 02:32:01 UTC"
  },
  {
    "arxiv_id": "2410.14166v2",
    "title": "LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems",
    "authors": [
      "Nan Xu",
      "Xuezhe Ma"
    ],
    "abstract": "Interestingly, LLMs yet struggle with some basic tasks that humans find\ntrivial to handle, e.g., counting the number of character r's in the word\n\"strawberry\". There are several popular conjectures (e.g., tokenization,\narchitecture and training data) regarding the reason for deficiency of LLMs in\nsimple word-based counting problems, sharing the similar belief that such\nfailure stems from model pretraining hence probably inevitable during\ndeployment. In this paper, we carefully design multiple evaluation settings to\ninvestigate validity of prevalent conjectures. Meanwhile, we measure\ntransferability of advanced mathematical and coding reasoning capabilities from\nspecialized LLMs to simple counting tasks. Although specialized LLMs suffer\nfrom counting problems as well, we find conjectures about inherent deficiency\nof LLMs invalid and further seek opportunities to elicit knowledge and\ncapabilities from LLMs that are beneficial to counting tasks. Compared with\nstrategies such as finetuning and in-context learning that are commonly adopted\nto enhance performance on new or challenging tasks, we show that engaging\nreasoning is the most robust and efficient way to help LLMs better perceive\ntasks with more accurate responses.\n  We hope our conjecture validation design could provide insights into the\nstudy of future critical failure modes of LLMs. Based on challenges in\ntransferring advanced capabilities to much simpler tasks, we call for more\nattention to model capability acquisition and evaluation. We also highlight the\nimportance of cultivating consciousness of \"reasoning before responding\" during\nmodel pretraining.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.14166v2",
    "published_date": "2024-10-18 04:17:16 UTC",
    "updated_date": "2025-02-06 22:38:32 UTC"
  },
  {
    "arxiv_id": "2410.14753v2",
    "title": "Collaboratively adding new knowledge to an LLM",
    "authors": [
      "Rhui Dih Lee",
      "Laura Wynter"
    ],
    "abstract": "We address the question of how to successively add new knowledge to an LLM\nwhilst retaining previously-added knowledge. We consider two settings,\nsemi-cooperative and fully-cooperative. Overall, LoRA performs better in most\ncases than full-fine tuning of all parameters when both new knowledge\nacquisition and retention of old, including recent, knowledge are taken into\naccount. In the semi-cooperative setting, where datasets are not available\nafter training, MOE mixing, model merging, and LoRA-based orthogonal subspace\nsequential learning, using a small weight on the orthogonality term, perform\nwell. In the fully-cooperative setting where datasets remain available, joint\ntraining and sequential training with replay are both effective approaches with\nLoRA training generally preferable to full fine-tuning. The codes needed to\nreproduce the results are provided in an open source repository.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14753v2",
    "published_date": "2024-10-18 04:04:51 UTC",
    "updated_date": "2024-10-29 07:03:36 UTC"
  },
  {
    "arxiv_id": "2410.14154v1",
    "title": "RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training",
    "authors": [
      "Muhe Ding",
      "Yang Ma",
      "Pengda Qin",
      "Jianlong Wu",
      "Yuhong Li",
      "Liqiang Nie"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have recently received substantial\ninterest, which shows their emerging potential as general-purpose models for\nvarious vision-language tasks. MLLMs involve significant external knowledge\nwithin their parameters; however, it is challenging to continually update these\nmodels with the latest knowledge, which involves huge computational costs and\npoor interpretability. Retrieval augmentation techniques have proven to be\neffective plugins for both LLMs and MLLMs. In this study, we propose multimodal\nadaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training\n(RA-BLIP), a novel retrieval-augmented framework for various MLLMs. Considering\nthe redundant information within vision modality, we first leverage the\nquestion to instruct the extraction of visual information through interactions\nwith one set of learnable queries, minimizing irrelevant interference during\nretrieval and generation. Besides, we introduce a pre-trained multimodal\nadaptive fusion module to achieve question text-to-multimodal retrieval and\nintegration of multimodal knowledge by projecting visual and language\nmodalities into a unified semantic space. Furthermore, we present an Adaptive\nSelection Knowledge Generation (ASKG) strategy to train the generator to\nautonomously discern the relevance of retrieved knowledge, which realizes\nexcellent denoising performance. Extensive experiments on open multimodal\nquestion-answering datasets demonstrate that RA-BLIP achieves significant\nperformance and surpasses the state-of-the-art retrieval-augmented models.",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "primary_category": "cs.MM",
    "comment": "10 pages, 6 figures, Journal",
    "pdf_url": "http://arxiv.org/pdf/2410.14154v1",
    "published_date": "2024-10-18 03:45:19 UTC",
    "updated_date": "2024-10-18 03:45:19 UTC"
  },
  {
    "arxiv_id": "2410.14150v1",
    "title": "Utilizing Large Language Models for Event Deconstruction to Enhance Multimodal Aspect-Based Sentiment Analysis",
    "authors": [
      "Xiaoyong Huang",
      "Heli Sun",
      "Qunshu Gao",
      "Wenjie Huang",
      "Ruichen Cao"
    ],
    "abstract": "With the rapid development of the internet, the richness of User-Generated\nContentcontinues to increase, making Multimodal Aspect-Based Sentiment Analysis\n(MABSA) a research hotspot. Existing studies have achieved certain results in\nMABSA, but they have not effectively addressed the analytical challenges in\nscenarios where multiple entities and sentiments coexist. This paper\ninnovatively introduces Large Language Models (LLMs) for event decomposition\nand proposes a reinforcement learning framework for Multimodal Aspect-based\nSentiment Analysis (MABSA-RL) framework. This framework decomposes the original\ntext into a set of events using LLMs, reducing the complexity of analysis,\nintroducing reinforcement learning to optimize model parameters. Experimental\nresults show that MABSA-RL outperforms existing advanced methods on two\nbenchmark datasets. This paper provides a new research perspective and method\nfor multimodal aspect-level sentiment analysis.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14150v1",
    "published_date": "2024-10-18 03:40:45 UTC",
    "updated_date": "2024-10-18 03:40:45 UTC"
  },
  {
    "arxiv_id": "2410.14147v1",
    "title": "Leveraging Large Language Models for Enhancing Public Transit Services",
    "authors": [
      "Jiahao Wang",
      "Amer Shalaby"
    ],
    "abstract": "Public transit systems play a crucial role in providing efficient and\nsustainable transportation options in urban areas. However, these systems face\nvarious challenges in meeting commuters' needs. On the other hand, despite the\nrapid development of Large Language Models (LLMs) worldwide, their integration\ninto transit systems remains relatively unexplored.\n  The objective of this paper is to explore the utilization of LLMs in the\npublic transit system, with a specific focus on improving the customers'\nexperience and transit staff performance. We present a general framework for\ndeveloping LLM applications in transit systems, wherein the LLM serves as the\nintermediary for information communication between natural language content and\nthe resources within the database. In this context, the LLM serves a\nmultifaceted role, including understanding users' requirements, retrieving data\nfrom the dataset in response to user queries, and tailoring the information to\nalign with the users' specific needs. Three transit LLM applications are\npresented: Tweet Writer, Trip Advisor, and Policy Navigator. Tweet Writer\nautomates updates to the transit system alerts on social media, Trip Advisor\noffers customized transit trip suggestions, and Policy Navigator provides clear\nand personalized answers to policy queries. Leveraging LLMs in these\napplications enhances seamless communication with their capabilities of\nunderstanding and generating human-like languages. With the help of these three\nLLM transit applications, transit system media personnel can provide system\nupdates more efficiently, and customers can access travel information and\npolicy answers in a more user-friendly manner.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "24 pages, 18 figures, submitting to Journal of ITS",
    "pdf_url": "http://arxiv.org/pdf/2410.14147v1",
    "published_date": "2024-10-18 03:33:47 UTC",
    "updated_date": "2024-10-18 03:33:47 UTC"
  },
  {
    "arxiv_id": "2410.14146v1",
    "title": "CausalChat: Interactive Causal Model Development and Refinement Using Large Language Models",
    "authors": [
      "Yanming Zhang",
      "Akshith Kota",
      "Eric Papenhausen",
      "Klaus Mueller"
    ],
    "abstract": "Causal networks are widely used in many fields to model the complex\nrelationships between variables. A recent approach has sought to construct\ncausal networks by leveraging the wisdom of crowds through the collective\nparticipation of humans. While this can yield detailed causal networks that\nmodel the underlying phenomena quite well, it requires a large number of\nindividuals with domain understanding. We adopt a different approach:\nleveraging the causal knowledge that large language models, such as OpenAI's\nGPT-4, have learned by ingesting massive amounts of literature. Within a\ndedicated visual analytics interface, called CausalChat, users explore single\nvariables or variable pairs recursively to identify causal relations, latent\nvariables, confounders, and mediators, constructing detailed causal networks\nthrough conversation. Each probing interaction is translated into a tailored\nGPT-4 prompt and the response is conveyed through visual representations which\nare linked to the generated text for explanations. We demonstrate the\nfunctionality of CausalChat across diverse data contexts and conduct user\nstudies involving both domain experts and laypersons.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14146v1",
    "published_date": "2024-10-18 03:33:32 UTC",
    "updated_date": "2024-10-18 03:33:32 UTC"
  },
  {
    "arxiv_id": "2410.14144v1",
    "title": "A Lightweight Multi Aspect Controlled Text Generation Solution For Large Language Models",
    "authors": [
      "Chenyang Zhang",
      "Jiayi Lin",
      "Haibo Tong",
      "Bingxuan Hou",
      "Dongyu Zhang",
      "Jialin Li",
      "Junli Wang"
    ],
    "abstract": "Large language models (LLMs) show remarkable abilities with instruction\ntuning. However, they fail to achieve ideal tasks when lacking high-quality\ninstruction tuning data on target tasks. Multi-Aspect Controllable Text\nGeneration (MCTG) is a representative task for this dilemma, where aspect\ndatasets are usually biased and correlated. Existing work exploits additional\nmodel structures and strategies for solutions, limiting adaptability to LLMs.\nTo activate MCTG ability of LLMs, we propose a lightweight MCTG pipeline based\non data augmentation. We analyze bias and correlations in traditional datasets,\nand address these concerns with augmented control attributes and sentences.\nAugmented datasets are feasible for instruction tuning. In our experiments,\nLLMs perform better in MCTG after data augmentation, with a 20% accuracy rise\nand less aspect correlations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14144v1",
    "published_date": "2024-10-18 03:32:00 UTC",
    "updated_date": "2024-10-18 03:32:00 UTC"
  },
  {
    "arxiv_id": "2410.14138v2",
    "title": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom",
    "authors": [
      "Jingqi Zhou",
      "Sheng Wang",
      "Jingwei Dong",
      "Lei Li",
      "Jiahui Gao",
      "Jiyue Jiang",
      "Lingpeng Kong",
      "Chuan Wu"
    ],
    "abstract": "Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., insufficient and irrelevant visual descriptions, and limited\nmulti-modal capacities). We then decompose visual reasoning process into two\nstages: visual perception (i.e., eyesight) and textual reasoning (i.e.,\nwisdom), and introduce a novel visual reasoning framework named ProReason. This\nframework features multi-run proactive perception and decoupled\nvision-reasoning capabilities. Briefly, given a multi-modal question, ProReason\niterates proactive information collection and reasoning until the answer can be\nconcluded with necessary and sufficient visual descriptions. Notably, the\ndisassociation of capabilities allows seamless integration of existing large\nlanguage models (LLMs) to compensate for the reasoning deficits of LVLMs. Our\nextensive experiments demonstrate that ProReason outperforms both existing\nmulti-step reasoning frameworks and passive peer methods on a wide range of\nbenchmarks for both open-source and closed-source models. In addition, with the\nassistance of LLMs, ProReason achieves a performance improvement of up to 15%\non MMMU benchmark. Our insights into existing solutions and the decoupled\nperspective for feasible integration of LLMs illuminate future research on\nvisual reasoning techniques, especially LLM-assisted ones.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14138v2",
    "published_date": "2024-10-18 03:22:06 UTC",
    "updated_date": "2025-03-27 08:07:19 UTC"
  },
  {
    "arxiv_id": "2410.14135v1",
    "title": "Inverse Reinforcement Learning from Non-Stationary Learning Agents",
    "authors": [
      "Kavinayan P. Sivakumar",
      "Yi Shen",
      "Zachary Bell",
      "Scott Nivison",
      "Boyuan Chen",
      "Michael M. Zavlanos"
    ],
    "abstract": "In this paper, we study an inverse reinforcement learning problem that\ninvolves learning the reward function of a learning agent using trajectory data\ncollected while this agent is learning its optimal policy. To address this\nproblem, we propose an inverse reinforcement learning method that allows us to\nestimate the policy parameters of the learning agent which can then be used to\nestimate its reward function. Our method relies on a new variant of the\nbehavior cloning algorithm, which we call bundle behavior cloning, and uses a\nsmall number of trajectories generated by the learning agent's policy at\ndifferent points in time to learn a set of policies that match the distribution\nof actions observed in the sampled trajectories. We then use the cloned\npolicies to train a neural network model that estimates the reward function of\nthe learning agent. We provide a theoretical analysis to show a complexity\nresult on bound guarantees for our method that beats standard behavior cloning\nas well as numerical experiments for a reinforcement learning problem that\nvalidate the proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14135v1",
    "published_date": "2024-10-18 03:02:44 UTC",
    "updated_date": "2024-10-18 03:02:44 UTC"
  },
  {
    "arxiv_id": "2410.14131v2",
    "title": "Deep Learning Applications in Medical Image Analysis: Advancements, Challenges, and Future Directions",
    "authors": [
      "Aimina Ali Eli",
      "Abida Ali"
    ],
    "abstract": "Medical image analysis has emerged as an essential element of contemporary\nhealthcare, facilitating physicians in achieving expedited and precise\ndiagnosis. Recent breakthroughs in deep learning, a subset of artificial\nintelligence, have markedly revolutionized the analysis of medical pictures,\nimproving the accuracy and efficiency of clinical procedures. Deep learning\nalgorithms, especially convolutional neural networks (CNNs), have demonstrated\nremarkable proficiency in autonomously learning features from multidimensional\nmedical pictures, including MRI, CT, and X-ray scans, without the necessity for\nmanual feature extraction. These models have been utilized across multiple\nmedical disciplines, including pathology, radiology, ophthalmology, and\ncardiology, where they aid in illness detection, classification, and\nsegmentation tasks......",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14131v2",
    "published_date": "2024-10-18 02:57:14 UTC",
    "updated_date": "2024-11-04 21:47:36 UTC"
  },
  {
    "arxiv_id": "2411.00788v1",
    "title": "KeyInst: Keyword Instruction for Improving SQL Formulation in Text-to-SQL",
    "authors": [
      "Xiping Liu",
      "Zhao Tan"
    ],
    "abstract": "Text-to-SQL parsing involves the translation of natural language queries\n(NLQs) into their corresponding SQL commands. A principal challenge within this\ndomain is the formulation of SQL queries that are not only syntactically\ncorrect but also semantically aligned with the natural language input. However,\nthe intrinsic disparity between the NLQ and the SQL poses a significant\nchallenge. In this research, we introduce Keyword Instruction (KeyInst), a\nnovel method designed to enhance SQL formulation by Large Language Models\n(LLMs). KeyInst essentially provides guidance on pivotal SQL keywords likely to\nbe part of the final query, thus facilitates a smoother SQL query formulation\nprocess. We explore two strategies for integrating KeyInst into Text-to-SQL\nparsing: a pipeline strategy and a single-pass strategy. The former first\ngenerates KeyInst for question, which are then used to prompt LLMs. The latter\nemploys a fine-tuned model to concurrently generate KeyInst and SQL in one\nstep. We developed StrucQL, a benchmark specifically designed for the\nevaluation of SQL formulation. Extensive experiments on StrucQL and other\nbenchmarks demonstrate that KeyInst significantly improves upon the existing\nText-to-SQL prompting techniques.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00788v1",
    "published_date": "2024-10-18 02:45:36 UTC",
    "updated_date": "2024-10-18 02:45:36 UTC"
  },
  {
    "arxiv_id": "2410.14752v1",
    "title": "TimeSeriesExam: A time series understanding exam",
    "authors": [
      "Yifu Cai",
      "Arjun Choudhry",
      "Mononito Goswami",
      "Artur Dubrawski"
    ],
    "abstract": "Large Language Models (LLMs) have recently demonstrated a remarkable ability\nto model time series data. These capabilities can be partly explained if LLMs\nunderstand basic time series concepts. However, our knowledge of what these\nmodels understand about time series data remains relatively limited. To address\nthis gap, we introduce TimeSeriesExam, a configurable and scalable\nmultiple-choice question exam designed to assess LLMs across five core time\nseries understanding categories: pattern recognition, noise understanding,\nsimilarity analysis, anomaly detection, and causality analysis. TimeSeriesExam\ncomprises of over 700 questions, procedurally generated using 104 carefully\ncurated templates and iteratively refined to balance difficulty and their\nability to discriminate good from bad models. We test 7 state-of-the-art LLMs\non the TimeSeriesExam and provide the first comprehensive evaluation of their\ntime series understanding abilities. Our results suggest that closed-source\nmodels such as GPT-4 and Gemini understand simple time series concepts\nsignificantly better than their open-source counterparts, while all models\nstruggle with complex concepts such as causality analysis. We believe that the\nability to programatically generate questions is fundamental to assessing and\nimproving LLM's ability to understand and reason about time series data.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS'24 Time Series in the Age of Large Models\n  Workshop",
    "pdf_url": "http://arxiv.org/pdf/2410.14752v1",
    "published_date": "2024-10-18 02:37:14 UTC",
    "updated_date": "2024-10-18 02:37:14 UTC"
  },
  {
    "arxiv_id": "2410.14122v1",
    "title": "Towards Robust Transcription: Exploring Noise Injection Strategies for Training Data Augmentation",
    "authors": [
      "Yonghyun Kim",
      "Alexander Lerch"
    ],
    "abstract": "Recent advancements in Automatic Piano Transcription (APT) have significantly\nimproved system performance, but the impact of noisy environments on the system\nperformance remains largely unexplored. This study investigates the impact of\nwhite noise at various Signal-to-Noise Ratio (SNR) levels on state-of-the-art\nAPT models and evaluates the performance of the Onsets and Frames model when\ntrained on noise-augmented data. We hope this research provides valuable\ninsights as preliminary work toward developing transcription models that\nmaintain consistent performance across a range of acoustic conditions.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to the Late-Breaking Demo Session of the 25th International\n  Society for Music Information Retrieval (ISMIR) Conference, 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.14122v1",
    "published_date": "2024-10-18 02:31:36 UTC",
    "updated_date": "2024-10-18 02:31:36 UTC"
  },
  {
    "arxiv_id": "2410.14121v2",
    "title": "FedMSE: Semi-supervised federated learning approach for IoT network intrusion detection",
    "authors": [
      "Van Tuan Nguyen",
      "Razvan Beuran"
    ],
    "abstract": "This paper proposes a novel federated learning approach for improving IoT\nnetwork intrusion detection. The rise of IoT has expanded the cyber attack\nsurface, making traditional centralized machine learning methods insufficient\ndue to concerns about data availability, computational resources, transfer\ncosts, and especially privacy preservation. A semi-supervised federated\nlearning model was developed to overcome these issues, combining the Shrink\nAutoencoder and Centroid one-class classifier (SAE-CEN). This approach enhances\nthe performance of intrusion detection by effectively representing normal\nnetwork data and accurately identifying anomalies in the decentralized\nstrategy. Additionally, a mean square error-based aggregation algorithm\n(MSEAvg) was introduced to improve global model performance by prioritizing\nmore accurate local models. The results obtained in our experimental setup,\nwhich uses various settings relying on the N-BaIoT dataset and Dirichlet\ndistribution, demonstrate significant improvements in real-world heterogeneous\nIoT networks in detection accuracy from 93.98$\\pm$2.90 to 97.30$\\pm$0.49,\nreduced learning costs when requiring only 50\\% of gateways participating in\nthe training process, and robustness in large-scale networks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14121v2",
    "published_date": "2024-10-18 02:23:57 UTC",
    "updated_date": "2025-04-03 15:16:55 UTC"
  },
  {
    "arxiv_id": "2410.14118v1",
    "title": "Skill Generalization with Verbs",
    "authors": [
      "Rachel Ma",
      "Lyndon Lam",
      "Benjamin A. Spiegel",
      "Aditya Ganeshan",
      "Roma Patel",
      "Ben Abbatematteo",
      "David Paulius",
      "Stefanie Tellex",
      "George Konidaris"
    ],
    "abstract": "It is imperative that robots can understand natural language commands issued\nby humans. Such commands typically contain verbs that signify what action\nshould be performed on a given object and that are applicable to many objects.\nWe propose a method for generalizing manipulation skills to novel objects using\nverbs. Our method learns a probabilistic classifier that determines whether a\ngiven object trajectory can be described by a specific verb. We show that this\nclassifier accurately generalizes to novel object categories with an average\naccuracy of 76.69% across 13 object categories and 14 verbs. We then perform\npolicy search over the object kinematics to find an object trajectory that\nmaximizes classifier prediction for a given verb. Our method allows a robot to\ngenerate a trajectory for a novel object based on a verb, which can then be\nused as input to a motion planner. We show that our model can generate\ntrajectories that are usable for executing five verb commands applied to novel\ninstances of two different object categories on a real robot.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages + 2 pages (references), 6 figures. Accepted at IROS 2023.\n  Code, dataset info and demo videos can be found at:\n  https://rachelma80000.github.io/SkillGenVerbs/",
    "pdf_url": "http://arxiv.org/pdf/2410.14118v1",
    "published_date": "2024-10-18 02:12:18 UTC",
    "updated_date": "2024-10-18 02:12:18 UTC"
  },
  {
    "arxiv_id": "2410.14115v1",
    "title": "A Communication and Computation Efficient Fully First-order Method for Decentralized Bilevel Optimization",
    "authors": [
      "Min Wen",
      "Chengchang Liu",
      "Ahmed Abdelmoniem",
      "Yipeng Zhou",
      "Yuedong Xu"
    ],
    "abstract": "Bilevel optimization, crucial for hyperparameter tuning, meta-learning and\nreinforcement learning, remains less explored in the decentralized learning\nparadigm, such as decentralized federated learning (DFL). Typically,\ndecentralized bilevel methods rely on both gradients and Hessian matrices to\napproximate hypergradients of upper-level models. However, acquiring and\nsharing the second-order oracle is compute and communication intensive. % and\nsharing this information incurs heavy communication overhead. To overcome these\nchallenges, this paper introduces a fully first-order decentralized method for\ndecentralized Bilevel optimization, $\\text{C}^2$DFB which is both compute- and\ncommunicate-efficient. In $\\text{C}^2$DFB, each learning node optimizes a\nmin-min-max problem to approximate hypergradient by exclusively using gradients\ninformation. To reduce the traffic load at the inner-loop of solving the\nlower-level problem, $\\text{C}^2$DFB incorporates a lightweight communication\nprotocol for efficiently transmitting compressed residuals of local parameters.\n% during the inner loops. Rigorous theoretical analysis ensures its convergence\n% of the algorithm, indicating a first-order oracle calls of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-4})$. Experiments on hyperparameter tuning and\nhyper-representation tasks validate the superiority of $\\text{C}^2$DFB across\nvarious typologies and heterogeneous data distributions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "19 Pages",
    "pdf_url": "http://arxiv.org/pdf/2410.14115v1",
    "published_date": "2024-10-18 02:00:45 UTC",
    "updated_date": "2024-10-18 02:00:45 UTC"
  },
  {
    "arxiv_id": "2410.19817v2",
    "title": "Step Guided Reasoning: Improving Mathematical Reasoning using Guidance Generation and Step Reasoning",
    "authors": [
      "Lang Cao",
      "Chao Peng",
      "Renhong Chen",
      "Wu Ning",
      "Yingtian Zou",
      "Yitong Li"
    ],
    "abstract": "Mathematical reasoning has been challenging for large language models (LLMs).\nHowever, the introduction of step-by-step Chain-of-Thought (CoT) inference has\nsignificantly advanced the mathematical capabilities of LLMs. Despite this\nprogress, current approaches either necessitate extensive inference datasets\nfor training or depend on few-shot methods that frequently compromise\ncomputational accuracy. To address these bottlenecks in mathematical reasoning,\nwe propose a novel method called Step Guidied Reasoning, which is more stable\nand generalizable than few-shot methods and does not involve further\nfine-tuning of the model. In this approach, LLMs reflect on small reasoning\nsteps, similar to how humans deliberate and focus attention on what to do next.\nBy incorporating this reflective process into the inference stage, LLMs can\neffectively guide their reasoning from one step to the next. Through extensive\nexperiments, we demonstrate the significant effect of Step Guidied Reasoning in\naugmenting mathematical performance in state-of-the-art language models.\nQwen2-72B-Instruct outperforms its math-specific counterpart,\nQwen2.5-72B-Math-Instruct, on MMLU- STEM with a score of 90.9%, compared to\n87.3%. The average scores of Qwen2-7B-Instruct and Qwen2-72B-Instruct increase\nfrom 27.1% to 36.3% and from 36.5% to 47.4% on the mathematics domain,\nrespectively.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.19817v2",
    "published_date": "2024-10-18 01:38:24 UTC",
    "updated_date": "2025-02-17 06:39:16 UTC"
  },
  {
    "arxiv_id": "2410.14103v3",
    "title": "Extreme Precipitation Nowcasting using Multi-Task Latent Diffusion Models",
    "authors": [
      "Li Chaorong",
      "Ling Xudong",
      "Yang Qiang",
      "Qin Fengqing",
      "Huang Yuanyuan"
    ],
    "abstract": "Deep learning models have achieved remarkable progress in precipitation\nprediction. However, they still face significant challenges in accurately\ncapturing spatial details of radar images, particularly in regions of high\nprecipitation intensity. This limitation results in reduced spatial\nlocalization accuracy when predicting radar echo images across varying\nprecipitation intensities. To address this challenge, we propose an innovative\nprecipitation prediction approach termed the Multi-Task Latent Diffusion Model\n(MTLDM). The core idea of MTLDM lies in the recognition that precipitation\nradar images represent a combination of multiple components, each corresponding\nto different precipitation intensities. Thus, we adopt a divide-and-conquer\nstrategy, decomposing radar images into several sub-images based on their\nprecipitation intensities and individually modeling these components. During\nthe prediction stage, MTLDM integrates these sub-image representations by\nutilizing a trained latent-space rainfall diffusion model, followed by decoding\nthrough a multi-task decoder to produce the final precipitation prediction.\nExperimental evaluations conducted on the MRMS dataset demonstrate that the\nproposed MTLDM method surpasses state-of-the-art techniques, achieving a\nCritical Success Index (CSI) improvement of 13-26%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "86A10, 68T07",
      "I.2.6; J.7"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 14figures",
    "pdf_url": "http://arxiv.org/pdf/2410.14103v3",
    "published_date": "2024-10-18 00:50:56 UTC",
    "updated_date": "2025-03-25 08:14:47 UTC"
  },
  {
    "arxiv_id": "2410.14101v2",
    "title": "Multi-Source Spatial Knowledge Understanding for Immersive Visual Text-to-Speech",
    "authors": [
      "Shuwei He",
      "Rui Liu"
    ],
    "abstract": "Visual Text-to-Speech (VTTS) aims to take the environmental image as the\nprompt to synthesize reverberant speech for the spoken content. Previous works\nfocus on the RGB modality for global environmental modeling, overlooking the\npotential of multi-source spatial knowledge like depth, speaker position, and\nenvironmental semantics. To address these issues, we propose a novel\nmulti-source spatial knowledge understanding scheme for immersive VTTS, termed\nMS2KU-VTTS. Specifically, we first prioritize RGB image as the dominant source\nand consider depth image, speaker position knowledge from object detection, and\nGemini-generated semantic captions as supplementary sources. Afterwards, we\npropose a serial interaction mechanism to effectively integrate both dominant\nand supplementary sources. The resulting multi-source knowledge is dynamically\nintegrated based on the respective contributions of each source.This enriched\ninteraction and integration of multi-source spatial knowledge guides the speech\ngeneration model, enhancing the immersive speech experience. Experimental\nresults demonstrate that the MS$^2$KU-VTTS surpasses existing baselines in\ngenerating immersive speech. Demos and code are available at:\nhttps://github.com/AI-S2-Lab/MS2KU-VTTS.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 1 figure, Accepted by ICASSP'2025",
    "pdf_url": "http://arxiv.org/pdf/2410.14101v2",
    "published_date": "2024-10-18 00:46:18 UTC",
    "updated_date": "2024-12-23 08:57:37 UTC"
  },
  {
    "arxiv_id": "2410.14099v1",
    "title": "ST-MoE-BERT: A Spatial-Temporal Mixture-of-Experts Framework for Long-Term Cross-City Mobility Prediction",
    "authors": [
      "Haoyu He",
      "Haozheng Luo",
      "Qi R. Wang"
    ],
    "abstract": "Predicting human mobility across multiple cities presents significant\nchallenges due to the complex and diverse spatial-temporal dynamics inherent in\ndifferent urban environments. In this study, we propose a robust approach to\npredict human mobility patterns called ST-MoE-BERT. Compared to existing\nmethods, our approach frames the prediction task as a spatial-temporal\nclassification problem. Our methodology integrates the Mixture-of-Experts\narchitecture with BERT model to capture complex mobility dynamics and perform\nthe downstream human mobility prediction task. Additionally, transfer learning\nis integrated to solve the challenge of data scarcity in cross-city prediction.\nWe demonstrate the effectiveness of the proposed model on GEO-BLEU and DTW,\ncomparing it to several state-of-the-art methods. Notably, ST-MoE-BERT achieves\nan average improvement of 8.29%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "2nd ACM SIGSPATIAL International Workshop on the Human Mobility\n  Prediction Challenge",
    "pdf_url": "http://arxiv.org/pdf/2410.14099v1",
    "published_date": "2024-10-18 00:32:18 UTC",
    "updated_date": "2024-10-18 00:32:18 UTC"
  },
  {
    "arxiv_id": "2410.14091v2",
    "title": "Towards Effective Planning Strategies for Dynamic Opinion Networks",
    "authors": [
      "Bharath Muppasani",
      "Protik Nag",
      "Vignesh Narayanan",
      "Biplav Srivastava",
      "Michael N. Huhns"
    ],
    "abstract": "In this study, we investigate the under-explored intervention planning aimed\nat disseminating accurate information within dynamic opinion networks by\nleveraging learning strategies. Intervention planning involves identifying key\nnodes (search) and exerting control (e.g., disseminating accurate or official\ninformation through the nodes) to mitigate the influence of misinformation.\nHowever, as the network size increases, the problem becomes computationally\nintractable. To address this, we first introduce a ranking algorithm to\nidentify key nodes for disseminating accurate information, which facilitates\nthe training of neural network classifiers that provide generalized solutions\nfor the search and planning problems. Second, we mitigate the complexity of\nlabel generation, which becomes challenging as the network grows, by developing\na reinforcement learning-based centralized dynamic planning framework. We\nanalyze these NN-based planners for opinion networks governed by two dynamic\npropagation models. Each model incorporates both binary and continuous opinion\nand trust representations. Our experimental results demonstrate that the\nranking algorithm-based classifiers provide plans that enhance infection rate\ncontrol, especially with increased action budgets for small networks. Further,\nwe observe that the reward strategies focusing on key metrics, such as the\nnumber of susceptible nodes and infection rates, outperform those prioritizing\nfaster blocking strategies. Additionally, our findings reveal that graph\nconvolutional network-based planners facilitate scalable centralized plans that\nachieve lower infection rates (higher control) across various network\nconfigurations, including Watts-Strogatz topology, varying action budgets,\nvarying initial infected nodes, and varying degrees of infected nodes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.14091v2",
    "published_date": "2024-10-18 00:13:56 UTC",
    "updated_date": "2024-11-03 01:13:34 UTC"
  }
]
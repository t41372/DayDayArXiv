[
  {
    "arxiv_id": "2408.15266v1",
    "title": "People over trust AI-generated medical responses and view them to be as valid as doctors, despite low accuracy",
    "authors": [
      "Shruthi Shekar",
      "Pat Pataranutaporn",
      "Chethan Sarabu",
      "Guillermo A. Cecchi",
      "Pattie Maes"
    ],
    "abstract": "This paper presents a comprehensive analysis of how AI-generated medical\nresponses are perceived and evaluated by non-experts. A total of 300\nparticipants gave evaluations for medical responses that were either written by\na medical doctor on an online healthcare platform, or generated by a large\nlanguage model and labeled by physicians as having high or low accuracy.\nResults showed that participants could not effectively distinguish between\nAI-generated and Doctors' responses and demonstrated a preference for\nAI-generated responses, rating High Accuracy AI-generated responses as\nsignificantly more valid, trustworthy, and complete/satisfactory. Low Accuracy\nAI-generated responses on average performed very similar to Doctors' responses,\nif not more. Participants not only found these low-accuracy AI-generated\nresponses to be valid, trustworthy, and complete/satisfactory but also\nindicated a high tendency to follow the potentially harmful medical advice and\nincorrectly seek unnecessary medical attention as a result of the response\nprovided. This problematic reaction was comparable if not more to the reaction\nthey displayed towards doctors' responses. This increased trust placed on\ninaccurate or inappropriate AI-generated medical advice can lead to\nmisdiagnosis and harmful consequences for individuals seeking help. Further,\nparticipants were more trusting of High Accuracy AI-generated responses when\ntold they were given by a doctor and experts rated AI-generated responses\nsignificantly higher when the source of the response was unknown. Both experts\nand non-experts exhibited bias, finding AI-generated responses to be more\nthorough and accurate than Doctors' responses but still valuing the involvement\nof a Doctor in the delivery of their medical advice. Ensuring AI systems are\nimplemented with medical professionals should be the future of using AI for the\ndelivery of medical advice.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15266v1",
    "published_date": "2024-08-11 23:41:28 UTC",
    "updated_date": "2024-08-11 23:41:28 UTC"
  },
  {
    "arxiv_id": "2408.05874v2",
    "title": "LLM-Based Robust Product Classification in Commerce and Compliance",
    "authors": [
      "Sina Gholamian",
      "Gianfranco Romani",
      "Bartosz Rudnikowicz",
      "Stavroula Skylaki"
    ],
    "abstract": "Product classification is a crucial task in international trade, as\ncompliance regulations are verified and taxes and duties are applied based on\nproduct categories. Manual classification of products is time-consuming and\nerror-prone, and the sheer volume of products imported and exported renders the\nmanual process infeasible. Consequently, e-commerce platforms and enterprises\ninvolved in international trade have turned to automatic product classification\nusing machine learning. However, current approaches do not consider the\nreal-world challenges associated with product classification, such as very\nabbreviated and incomplete product descriptions. In addition, recent\nadvancements in generative Large Language Models (LLMs) and their reasoning\ncapabilities are mainly untapped in product classification and e-commerce. In\nthis research, we explore the real-life challenges of industrial classification\nand we propose data perturbations that allow for realistic data simulation.\nFurthermore, we employ LLM-based product classification to improve the\nrobustness of the prediction in presence of incomplete data. Our research shows\nthat LLMs with in-context learning outperform the supervised approaches in the\nclean-data scenario. Additionally, we illustrate that LLMs are significantly\nmore robust than the supervised approaches when data attacks are present.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera-ready version for Customizable NLP Workshop at EMNLP 2024. 11\n  pages",
    "pdf_url": "http://arxiv.org/pdf/2408.05874v2",
    "published_date": "2024-08-11 22:59:32 UTC",
    "updated_date": "2024-10-15 16:18:10 UTC"
  },
  {
    "arxiv_id": "2408.05861v2",
    "title": "Leveraging Knowledge Graph-Based Human-Like Memory Systems to Solve Partially Observable Markov Decision Processes",
    "authors": [
      "Taewoon Kim",
      "Vincent Fran√ßois-Lavet",
      "Michael Cochez"
    ],
    "abstract": "Humans observe only part of their environment at any moment but can still\nmake complex, long-term decisions thanks to our long-term memory. To test how\nan AI can learn and utilize its long-term memory, we have developed a partially\nobservable Markov decision processes (POMDP) environment, where the agent has\nto answer questions while navigating a maze. The environment is completely\nknowledge graph (KG) based, where the hidden states are dynamic KGs. A KG is\nboth human- and machine-readable, making it easy to see what the agents\nremember and forget. We train and compare agents with different memory systems,\nto shed light on how human brains work when it comes to managing its own\nmemory. By repurposing the given learning objective as learning a memory\nmanagement policy, we were able to capture the most likely hidden state, which\nis not only interpretable but also reusable.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05861v2",
    "published_date": "2024-08-11 21:04:14 UTC",
    "updated_date": "2024-08-18 19:32:35 UTC"
  },
  {
    "arxiv_id": "2408.05860v2",
    "title": "Root Cause Attribution of Delivery Risks via Causal Discovery with Reinforcement Learning",
    "authors": [
      "Shi Bo",
      "Minheng Xiao"
    ],
    "abstract": "This paper presents a novel approach to root cause attribution of delivery\nrisks within supply chains by integrating causal discovery with reinforcement\nlearning. As supply chains become increasingly complex, traditional methods of\nroot cause analysis struggle to capture the intricate interrelationships\nbetween various factors, often leading to spurious correlations and suboptimal\ndecision-making. Our approach addresses these challenges by leveraging causal\ndiscovery to identify the true causal relationships between operational\nvariables, and reinforcement learning to iteratively refine the causal graph.\nThis method enables the accurate identification of key drivers of late\ndeliveries, such as shipping mode and delivery status, and provides actionable\ninsights for optimizing supply chain performance. We apply our approach to a\nreal-world supply chain dataset, demonstrating its effectiveness in uncovering\nthe underlying causes of delivery delays and offering strategies for mitigating\nthese risks. The findings have significant implications for improving\noperational efficiency, customer satisfaction, and overall profitability within\nsupply chains.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05860v2",
    "published_date": "2024-08-11 20:52:51 UTC",
    "updated_date": "2025-01-28 04:04:43 UTC"
  },
  {
    "arxiv_id": "2408.05859v1",
    "title": "The Cognitive Revolution in Interpretability: From Explaining Behavior to Interpreting Representations and Algorithms",
    "authors": [
      "Adam Davies",
      "Ashkan Khakzar"
    ],
    "abstract": "Artificial neural networks have long been understood as \"black boxes\": though\nwe know their computation graphs and learned parameters, the knowledge encoded\nby these weights and functions they perform are not inherently interpretable.\nAs such, from the early days of deep learning, there have been efforts to\nexplain these models' behavior and understand them internally; and recently,\nmechanistic interpretability (MI) has emerged as a distinct research area\nstudying the features and implicit algorithms learned by foundation models such\nas large language models. In this work, we aim to ground MI in the context of\ncognitive science, which has long struggled with analogous questions in\nstudying and explaining the behavior of \"black box\" intelligent systems like\nthe human brain. We leverage several important ideas and developments in the\nhistory of cognitive science to disentangle divergent objectives in MI and\nindicate a clear path forward. First, we argue that current methods are ripe to\nfacilitate a transition in deep learning interpretation echoing the \"cognitive\nrevolution\" in 20th-century psychology that shifted the study of human\npsychology from pure behaviorism toward mental representations and processing.\nSecond, we propose a taxonomy mirroring key parallels in computational\nneuroscience to describe two broad categories of MI research, semantic\ninterpretation (what latent representations are learned and used) and\nalgorithmic interpretation (what operations are performed over representations)\nto elucidate their divergent goals and objects of study. Finally, we elaborate\nthe parallels and distinctions between various approaches in both categories,\nanalyze the respective strengths and weaknesses of representative works,\nclarify underlying assumptions, outline key challenges, and discuss the\npossibility of unifying these modes of interpretation under a common framework.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05859v1",
    "published_date": "2024-08-11 20:50:16 UTC",
    "updated_date": "2024-08-11 20:50:16 UTC"
  },
  {
    "arxiv_id": "2408.08899v1",
    "title": "Kov: Transferable and Naturalistic Black-Box LLM Attacks using Markov Decision Processes and Tree Search",
    "authors": [
      "Robert J. Moss"
    ],
    "abstract": "Eliciting harmful behavior from large language models (LLMs) is an important\ntask to ensure the proper alignment and safety of the models. Often when\ntraining LLMs, ethical guidelines are followed yet alignment failures may still\nbe uncovered through red teaming adversarial attacks. This work frames the\nred-teaming problem as a Markov decision process (MDP) and uses Monte Carlo\ntree search to find harmful behaviors of black-box, closed-source LLMs. We\noptimize token-level prompt suffixes towards targeted harmful behaviors on\nwhite-box LLMs and include a naturalistic loss term, log-perplexity, to\ngenerate more natural language attacks for better interpretability. The\nproposed algorithm, Kov, trains on white-box LLMs to optimize the adversarial\nattacks and periodically evaluates responses from the black-box LLM to guide\nthe search towards more harmful black-box behaviors. In our preliminary study,\nresults indicate that we can jailbreak black-box models, such as GPT-3.5, in\nonly 10 queries, yet fail on GPT-4$-$which may indicate that newer models are\nmore robust to token-level attacks. All work to reproduce these results is open\nsourced (https://github.com/sisl/Kov.jl).",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.08899v1",
    "published_date": "2024-08-11 20:31:52 UTC",
    "updated_date": "2024-08-11 20:31:52 UTC"
  },
  {
    "arxiv_id": "2408.07092v2",
    "title": "Post-Training Sparse Attention with Double Sparsity",
    "authors": [
      "Shuo Yang",
      "Ying Sheng",
      "Joseph E. Gonzalez",
      "Ion Stoica",
      "Lianmin Zheng"
    ],
    "abstract": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07092v2",
    "published_date": "2024-08-11 18:40:36 UTC",
    "updated_date": "2024-08-18 17:27:17 UTC"
  },
  {
    "arxiv_id": "2408.05842v5",
    "title": "Open Role-Playing with Delta-Engines",
    "authors": [
      "Hongqiu Wu",
      "Zekai Xu",
      "Tianyang Xu",
      "Shize Wei",
      "Yan Wang",
      "Jiale Hong",
      "Weiqi Wu",
      "Hai Zhao"
    ],
    "abstract": "Game roles can be reflections of personas from a parallel world. In this\npaper, we propose a new style of game-play to bridge self-expression and\nrole-playing: \\emph{open role-playing games (ORPGs)}, where players are allowed\nto craft and embody their unique characters in the game world. Our vision is\nthat, in the real world, we are individually similar when we are born, but we\ngrow into unique ones as a result of the strongly different choices we make\nafterward. Therefore, in an ORPG, we empower players with freedom to decide\ntheir own growing curves through natural language inputs, ultimately becoming\nunique characters. To technically do this, we propose a special engine called\nDelta-Engine. This engine is not a traditional game engine used for game\ndevelopment, but serves as an in-game module to provide new game-play\nexperiences. A delta-engine consists of two components, a base engine and a\nneural proxy. The base engine programs the prototype of the character as well\nas the foundational settings of the game; the neural proxy is an LLM, which\nrealizes the character growth by generating new code snippets on the base\nengine incrementally. In this paper, we self-develop a specific ORPG based on\ndelta-engines. It is adapted from the popular animated series ``Pok\\'emon''. We\npresent our efforts in generating out-of-domain and interesting role data in\nthe development process as well as accessing the performance of a delta-engine.\nWhile the empirical results in this work are specific, we aim for them to\nprovide general insights for future games.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05842v5",
    "published_date": "2024-08-11 18:32:29 UTC",
    "updated_date": "2025-03-07 04:13:19 UTC"
  },
  {
    "arxiv_id": "2408.05836v1",
    "title": "Real-Time Drowsiness Detection Using Eye Aspect Ratio and Facial Landmark Detection",
    "authors": [
      "Varun Shiva Krishna Rupani",
      "Velpooru Venkata Sai Thushar",
      "Kondadi Tejith"
    ],
    "abstract": "Drowsiness detection is essential for improving safety in areas such as\ntransportation and workplace health. This study presents a real-time system\ndesigned to detect drowsiness using the Eye Aspect Ratio (EAR) and facial\nlandmark detection techniques. The system leverages Dlibs pre-trained shape\npredictor model to accurately detect and monitor 68 facial landmarks, which are\nused to compute the EAR. By establishing a threshold for the EAR, the system\nidentifies when eyes are closed, indicating potential drowsiness. The process\ninvolves capturing a live video stream, detecting faces in each frame,\nextracting eye landmarks, and calculating the EAR to assess alertness. Our\nexperiments show that the system reliably detects drowsiness with high accuracy\nwhile maintaining low computational demands. This study offers a strong\nsolution for real-time drowsiness detection, with promising applications in\ndriver monitoring and workplace safety. Future research will investigate\nincorporating additional physiological and contextual data to further enhance\ndetection accuracy and reliability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05836v1",
    "published_date": "2024-08-11 17:34:24 UTC",
    "updated_date": "2024-08-11 17:34:24 UTC"
  },
  {
    "arxiv_id": "2408.05834v2",
    "title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm",
    "authors": [
      "Eli Sennesh",
      "Hao Wu",
      "Tommaso Salvatori"
    ],
    "abstract": "Unexpected stimuli induce \"error\" or \"surprise\" signals in the brain. The\ntheory of predictive coding promises to explain these observations in terms of\nBayesian inference by suggesting that the cortex implements variational\ninference in a probabilistic graphical model. However, when applied to machine\nlearning tasks, this family of algorithms has yet to perform on par with other\nvariational approaches in high-dimensional, structured inference problems. To\naddress this, we introduce a novel predictive coding algorithm for structured\ngenerative models, that we call divide-and-conquer predictive coding (DCPC).\nDCPC differs from other formulations of predictive coding, as it respects the\ncorrelation structure of the generative model and provably performs\nmaximum-likelihood updates of model parameters, all without sacrificing\nbiological plausibility. Empirically, DCPC achieves better numerical\nperformance than competing algorithms and provides accurate inference in a\nnumber of problems not previously addressed with predictive coding. We provide\nan open implementation of DCPC in Pyro on Github.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "stat.ML",
    "comment": "22 pages, 5 figures, accepted to Neural Information Processing\n  Systems (NeurIPS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.05834v2",
    "published_date": "2024-08-11 17:29:03 UTC",
    "updated_date": "2024-10-17 02:10:10 UTC"
  },
  {
    "arxiv_id": "2408.05831v1",
    "title": "Robust Domain Generalization for Multi-modal Object Recognition",
    "authors": [
      "Yuxin Qiao",
      "Keqin Li",
      "Junhong Lin",
      "Rong Wei",
      "Chufeng Jiang",
      "Yang Luo",
      "Haoyu Yang"
    ],
    "abstract": "In multi-label classification, machine learning encounters the challenge of\ndomain generalization when handling tasks with distributions differing from the\ntraining data. Existing approaches primarily focus on vision object recognition\nand neglect the integration of natural language. Recent advancements in\nvision-language pre-training leverage supervision from extensive\nvisual-language pairs, enabling learning across diverse domains and enhancing\nrecognition in multi-modal scenarios. However, these approaches face\nlimitations in loss function utilization, generality across backbones, and\nclass-aware visual fusion. This paper proposes solutions to these limitations\nby inferring the actual loss, broadening evaluations to larger vision-language\nbackbones, and introducing Mixup-CLIPood, which incorporates a novel mix-up\nloss for enhanced class-aware visual fusion. Our method demonstrates superior\nperformance in domain generalization across multiple datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 2 figures. This is a preprint version of the article. The\n  final version will be published in the proceedings of the IEEE conference",
    "pdf_url": "http://arxiv.org/pdf/2408.05831v1",
    "published_date": "2024-08-11 17:13:21 UTC",
    "updated_date": "2024-08-11 17:13:21 UTC"
  },
  {
    "arxiv_id": "2408.15263v1",
    "title": "S4DL: Shift-sensitive Spatial-Spectral Disentangling Learning for Hyperspectral Image Unsupervised Domain Adaptation",
    "authors": [
      "Jie Feng",
      "Tianshu Zhang",
      "Junpeng Zhang",
      "Ronghua Shang",
      "Weisheng Dong",
      "Guangming Shi",
      "Licheng Jiao"
    ],
    "abstract": "Unsupervised domain adaptation techniques, extensively studied in\nhyperspectral image (HSI) classification, aim to use labeled source domain data\nand unlabeled target domain data to learn domain invariant features for\ncross-scene classification. Compared to natural images, numerous spectral bands\nof HSIs provide abundant semantic information, but they also increase the\ndomain shift significantly. In most existing methods, both explicit alignment\nand implicit alignment simply align feature distribution, ignoring domain\ninformation in the spectrum. We noted that when the spectral channel between\nsource and target domains is distinguished obviously, the transfer performance\nof these methods tends to deteriorate. Additionally, their performance\nfluctuates greatly owing to the varying domain shifts across various datasets.\nTo address these problems, a novel shift-sensitive spatial-spectral\ndisentangling learning (S4DL) approach is proposed. In S4DL, gradient-guided\nspatial-spectral decomposition is designed to separate domain-specific and\ndomain-invariant representations by generating tailored masks under the\nguidance of the gradient from domain classification. A shift-sensitive adaptive\nmonitor is defined to adjust the intensity of disentangling according to the\nmagnitude of domain shift. Furthermore, a reversible neural network is\nconstructed to retain domain information that lies in not only in semantic but\nalso the shallow-level detailed information. Extensive experimental results on\nseveral cross-scene HSI datasets consistently verified that S4DL is better than\nthe state-of-the-art UDA methods. Our source code will be available at\nhttps://github.com/xdu-jjgs/S4DL.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.15263v1",
    "published_date": "2024-08-11 15:58:24 UTC",
    "updated_date": "2024-08-11 15:58:24 UTC"
  },
  {
    "arxiv_id": "2408.05804v1",
    "title": "A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals",
    "authors": [
      "Grace Liu",
      "Michael Tang",
      "Benjamin Eysenbach"
    ],
    "abstract": "In this paper, we present empirical evidence of skills and directed\nexploration emerging from a simple RL algorithm long before any successful\ntrials are observed. For example, in a manipulation task, the agent is given a\nsingle observation of the goal state and learns skills, first for moving its\nend-effector, then for pushing the block, and finally for picking up and\nplacing the block. These skills emerge before the agent has ever successfully\nplaced the block at the goal location and without the aid of any reward\nfunctions, demonstrations, or manually-specified distance metrics. Once the\nagent has learned to reach the goal state reliably, exploration is reduced.\nImplementing our method involves a simple modification of prior work and does\nnot require density estimates, ensembles, or any additional hyperparameters.\nIntuitively, the proposed method seems like it should be terrible at\nexploration, and we lack a clear theoretical understanding of why it works so\neffectively, though our experiments provide some hints.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code and videos: https://graliuce.github.io/sgcrl/",
    "pdf_url": "http://arxiv.org/pdf/2408.05804v1",
    "published_date": "2024-08-11 15:49:00 UTC",
    "updated_date": "2024-08-11 15:49:00 UTC"
  },
  {
    "arxiv_id": "2408.05798v2",
    "title": "Time Makes Space: Emergence of Place Fields in Networks Encoding Temporally Continuous Sensory Experiences",
    "authors": [
      "Zhaoze Wang",
      "Ronald W. Di Tullio",
      "Spencer Rooke",
      "Vijay Balasubramanian"
    ],
    "abstract": "The vertebrate hippocampus is believed to use recurrent connectivity in area\nCA3 to support episodic memory recall from partial cues. This brain area also\ncontains place cells, whose location-selective firing fields implement maps\nsupporting spatial memory. Here we show that place cells emerge in networks\ntrained to remember temporally continuous sensory episodes. We model CA3 as a\nrecurrent autoencoder that recalls and reconstructs sensory experiences from\nnoisy and partially occluded observations by agents traversing simulated rooms.\nThe agents move in realistic trajectories modeled from rodents and environments\nare modeled as high-dimensional sensory experience maps. Training our\nautoencoder to pattern-complete and reconstruct experiences with a constraint\non total activity causes spatially localized firing fields, i.e., place cells,\nto emerge in the encoding layer. The emergent place fields reproduce key\naspects of hippocampal phenomenology: a) remapping (maintenance of and\nreversion to distinct learned maps in different environments), implemented via\nrepositioning of experience manifolds in the network's hidden layer, b)\northogonality of spatial representations in different arenas, c) robust place\nfield emergence in differently shaped rooms, with single units showing multiple\nplace fields in large or complex spaces, and d) slow representational drift of\nplace fields. We argue that these results arise because continuous traversal of\nspace makes sensory experience temporally continuous. We make testable\npredictions: a) rapidly changing sensory context will disrupt place fields, b)\nplace fields will form even if recurrent connections are blocked, but reversion\nto previously learned representations upon remapping will be abolished, c) the\ndimension of temporally smooth experience sets the dimensionality of place\nfields, including during virtual navigation of abstract spaces.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05798v2",
    "published_date": "2024-08-11 15:17:11 UTC",
    "updated_date": "2025-01-29 22:01:37 UTC"
  },
  {
    "arxiv_id": "2408.05795v1",
    "title": "A Meta-Engine Framework for Interleaved Task and Motion Planning using Topological Refinements",
    "authors": [
      "Elisa Tosello",
      "Alessandro Valentini",
      "Andrea Micheli"
    ],
    "abstract": "Task And Motion Planning (TAMP) is the problem of finding a solution to an\nautomated planning problem that includes discrete actions executable by\nlow-level continuous motions. This field is gaining increasing interest within\nthe robotics community, as it significantly enhances robot's autonomy in\nreal-world applications. Many solutions and formulations exist, but no clear\nstandard representation has emerged. In this paper, we propose a general and\nopen-source framework for modeling and benchmarking TAMP problems. Moreover, we\nintroduce an innovative meta-technique to solve TAMP problems involving moving\nagents and multiple task-state-dependent obstacles. This approach enables using\nany off-the-shelf task planner and motion planner while leveraging a geometric\nanalysis of the motion planner's search space to prune the task planner's\nexploration, enhancing its efficiency. We also show how to specialize this\nmeta-engine for the case of an incremental SMT-based planner. We demonstrate\nthe effectiveness of our approach across benchmark problems of increasing\ncomplexity, where robots must navigate environments with movable obstacles.\nFinally, we integrate state-of-the-art TAMP algorithms into our framework and\ncompare their performance with our achievements.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "To appear in ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.05795v1",
    "published_date": "2024-08-11 14:57:57 UTC",
    "updated_date": "2024-08-11 14:57:57 UTC"
  },
  {
    "arxiv_id": "2408.05794v2",
    "title": "HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes",
    "authors": [
      "Xuanyu Su",
      "Yansong Li",
      "Diana Inkpen",
      "Nathalie Japkowicz"
    ],
    "abstract": "Amidst the rise of Large Multimodal Models (LMMs) and their widespread\napplication in generating and interpreting complex content, the risk of\npropagating biased and harmful memes remains significant. Current safety\nmeasures often fail to detect subtly integrated hateful content within\n``Confounder Memes''. To address this, we introduce \\textsc{HateSieve}, a new\nframework designed to enhance the detection and segmentation of hateful\nelements in memes. \\textsc{HateSieve} features a novel Contrastive Meme\nGenerator that creates semantically paired memes, a customized triplet dataset\nfor contrastive learning, and an Image-Text Alignment module that produces\ncontext-aware embeddings for accurate meme segmentation. Empirical experiments\non the Hateful Meme Dataset show that \\textsc{HateSieve} not only surpasses\nexisting LMMs in performance with fewer trainable parameters but also offers a\nrobust mechanism for precisely identifying and isolating hateful content.\n\\textcolor{red}{Caution: Contains academic discussions of hate speech; viewer\ndiscretion advised.}",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NAACL 2025 Findings; camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2408.05794v2",
    "published_date": "2024-08-11 14:56:06 UTC",
    "updated_date": "2025-04-30 03:30:36 UTC"
  },
  {
    "arxiv_id": "2408.05788v1",
    "title": "Continual Learning of Nonlinear Independent Representations",
    "authors": [
      "Boyang Sun",
      "Ignavier Ng",
      "Guangyi Chen",
      "Yifan Shen",
      "Qirong Ho",
      "Kun Zhang"
    ],
    "abstract": "Identifying the causal relations between interested variables plays a pivotal\nrole in representation learning as it provides deep insights into the dataset.\nIdentifiability, as the central theme of this approach, normally hinges on\nleveraging data from multiple distributions (intervention, distribution shift,\ntime series, etc.). Despite the exciting development in this field, a practical\nbut often overlooked problem is: what if those distribution shifts happen\nsequentially? In contrast, any intelligence possesses the capacity to abstract\nand refine learned knowledge sequentially -- lifelong learning. In this paper,\nwith a particular focus on the nonlinear independent component analysis (ICA)\nframework, we move one step forward toward the question of enabling models to\nlearn meaningful (identifiable) representations in a sequential manner, termed\ncontinual causal representation learning. We theoretically demonstrate that\nmodel identifiability progresses from a subspace level to a component-wise\nlevel as the number of distributions increases. Empirically, we show that our\nmethod achieves performance comparable to nonlinear ICA methods trained jointly\non multiple offline distributions and, surprisingly, the incoming new\ndistribution does not necessarily benefit the identification of all latent\nvariables.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 5 Figures",
    "pdf_url": "http://arxiv.org/pdf/2408.05788v1",
    "published_date": "2024-08-11 14:33:37 UTC",
    "updated_date": "2024-08-11 14:33:37 UTC"
  },
  {
    "arxiv_id": "2408.05781v2",
    "title": "CURLing the Dream: Contrastive Representations for World Modeling in Reinforcement Learning",
    "authors": [
      "Victor Augusto Kich",
      "Jair Augusto Bottega",
      "Raul Steinmetz",
      "Ricardo Bedin Grando",
      "Ayano Yorozu",
      "Akihisa Ohya"
    ],
    "abstract": "In this work, we present Curled-Dreamer, a novel reinforcement learning\nalgorithm that integrates contrastive learning into the DreamerV3 framework to\nenhance performance in visual reinforcement learning tasks. By incorporating\nthe contrastive loss from the CURL algorithm and a reconstruction loss from\nautoencoder, Curled-Dreamer achieves significant improvements in various\nDeepMind Control Suite tasks. Our extensive experiments demonstrate that\nCurled-Dreamer consistently outperforms state-of-the-art algorithms, achieving\nhigher mean and median scores across a diverse set of tasks. The results\nindicate that the proposed approach not only accelerates learning but also\nenhances the robustness of the learned policies. This work highlights the\npotential of combining different learning paradigms to achieve superior\nperformance in reinforcement learning applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper accepted for 24th International Conference on Control,\n  Automation and Systems (ICCAS)",
    "pdf_url": "http://arxiv.org/pdf/2408.05781v2",
    "published_date": "2024-08-11 14:13:22 UTC",
    "updated_date": "2024-08-31 21:20:16 UTC"
  },
  {
    "arxiv_id": "2408.05777v1",
    "title": "Seg-CycleGAN : SAR-to-optical image translation guided by a downstream task",
    "authors": [
      "Hannuo Zhang",
      "Huihui Li",
      "Jiarui Lin",
      "Yujie Zhang",
      "Jianghua Fan",
      "Hang Liu"
    ],
    "abstract": "Optical remote sensing and Synthetic Aperture Radar(SAR) remote sensing are\ncrucial for earth observation, offering complementary capabilities. While\noptical sensors provide high-quality images, they are limited by weather and\nlighting conditions. In contrast, SAR sensors can operate effectively under\nadverse conditions. This letter proposes a GAN-based SAR-to-optical image\ntranslation method named Seg-CycleGAN, designed to enhance the accuracy of ship\ntarget translation by leveraging semantic information from a pre-trained\nsemantic segmentation model. Our method utilizes the downstream task of ship\ntarget semantic segmentation to guide the training of image translation\nnetwork, improving the quality of output Optical-styled images. The potential\nof foundation-model-annotated datasets in SAR-to-optical translation tasks is\nrevealed. This work suggests broader research and applications for\ndownstream-task-guided frameworks. The code will be available at\nhttps://github.com/NPULHH/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.05777v1",
    "published_date": "2024-08-11 14:01:21 UTC",
    "updated_date": "2024-08-11 14:01:21 UTC"
  },
  {
    "arxiv_id": "2408.13265v1",
    "title": "Exploiting Formal Concept Analysis for Data Modeling in Data Lakes",
    "authors": [
      "Anes Bendimerad",
      "Romain Mathonat",
      "Youcef Remil",
      "Mehdi Kaytoue"
    ],
    "abstract": "Data lakes are widely used to store extensive and heterogeneous datasets for\nadvanced analytics. However, the unstructured nature of data in these\nrepositories introduces complexities in exploiting them and extracting\nmeaningful insights. This motivates the need of exploring efficient approaches\nfor consolidating data lakes and deriving a common and unified schema. This\npaper introduces a practical data visualization and analysis approach rooted in\nFormal Concept Analysis (FCA) to systematically clean, organize, and design\ndata structures within a data lake. We explore diverse data structures stored\nin our data lake at Infologic, including InfluxDB measurements and\nElasticsearch indexes, aiming to derive conventions for a more accessible data\nmodel. Leveraging FCA, we represent data structures as objects, analyze the\nconcept lattice, and present two strategies-top-down and bottom-up-to unify\nthese structures and establish a common schema. Our methodology yields\nsignificant results, enabling the identification of common concepts in the data\nstructures, such as resources along with their underlying shared fields\n(timestamp, type, usedRatio, etc.). Moreover, the number of distinct data\nstructure field names is reduced by 54 percent (from 190 to 88) in the studied\nsubset of our data lake. We achieve a complete coverage of 80 percent of data\nstructures with only 34 distinct field names, a significant improvement from\nthe initial 121 field names that were needed to reach such coverage. The paper\nprovides insights into the Infologic ecosystem, problem formulation,\nexploration strategies, and presents both qualitative and quantitative results.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13265v1",
    "published_date": "2024-08-11 13:58:31 UTC",
    "updated_date": "2024-08-11 13:58:31 UTC"
  },
  {
    "arxiv_id": "2408.05773v1",
    "title": "Neurosymbolic Methods for Rule Mining",
    "authors": [
      "Agnieszka Lawrynowicz",
      "Luis Galarraga",
      "Mehwish Alam",
      "Berenice Jaulmes",
      "Vaclav Zeman",
      "Tomas Kliegr"
    ],
    "abstract": "In this chapter, we address the problem of rule mining, beginning with\nessential background information, including measures of rule quality. We then\nexplore various rule mining methodologies, categorized into three groups:\ninductive logic programming, path sampling and generalization, and linear\nprogramming. Following this, we delve into neurosymbolic methods, covering\ntopics such as the integration of deep learning with rules, the use of\nembeddings for rule learning, and the application of large language models in\nrule learning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05773v1",
    "published_date": "2024-08-11 13:50:40 UTC",
    "updated_date": "2024-08-11 13:50:40 UTC"
  },
  {
    "arxiv_id": "2408.05772v1",
    "title": "An analysis of HOI: using a training-free method with multimodal visual foundation models when only the test set is available, without the training set",
    "authors": [
      "Chaoyi Ai"
    ],
    "abstract": "Human-Object Interaction (HOI) aims to identify the pairs of humans and\nobjects in images and to recognize their relationships, ultimately forming\n$\\langle human, object, verb \\rangle$ triplets. Under default settings, HOI\nperformance is nearly saturated, with many studies focusing on long-tail\ndistribution and zero-shot/few-shot scenarios. Let us consider an intriguing\nproblem:``What if there is only test dataset without training dataset, using\nmultimodal visual foundation model in a training-free manner? '' This study\nuses two experimental settings: grounding truth and random arbitrary\ncombinations. We get some interesting conclusion and find that the open\nvocabulary capabilities of the multimodal visual foundation model are not yet\nfully realized. Additionally, replacing the feature extraction with grounding\nDINO further confirms these findings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05772v1",
    "published_date": "2024-08-11 13:40:02 UTC",
    "updated_date": "2024-08-11 13:40:02 UTC"
  },
  {
    "arxiv_id": "2408.05767v2",
    "title": "Reference-free Hallucination Detection for Large Vision-Language Models",
    "authors": [
      "Qing Li",
      "Jiahui Geng",
      "Chenyang Lyu",
      "Derui Zhu",
      "Maxim Panov",
      "Fakhri Karray"
    ],
    "abstract": "Large vision-language models (LVLMs) have made significant progress in recent\nyears. While LVLMs exhibit excellent ability in language understanding,\nquestion answering, and conversations of visual inputs, they are prone to\nproducing hallucinations. While several methods are proposed to evaluate the\nhallucinations in LVLMs, most are reference-based and depend on external tools,\nwhich complicates their practical application. To assess the viability of\nalternative methods, it is critical to understand whether the reference-free\napproaches, which do not rely on any external tools, can efficiently detect\nhallucinations. Therefore, we initiate an exploratory study to demonstrate the\neffectiveness of different reference-free solutions in detecting hallucinations\nin LVLMs. In particular, we conduct an extensive study on three kinds of\ntechniques: uncertainty-based, consistency-based, and supervised uncertainty\nquantification methods on four representative LVLMs across two different tasks.\nThe empirical results show that the reference-free approaches are capable of\neffectively detecting non-factual responses in LVLMs, with the supervised\nuncertainty quantification method outperforming the others, achieving the best\nperformance across different settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05767v2",
    "published_date": "2024-08-11 13:17:14 UTC",
    "updated_date": "2024-11-19 13:22:21 UTC"
  },
  {
    "arxiv_id": "2408.05758v1",
    "title": "VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing",
    "authors": [
      "Chunyu Qiang",
      "Wang Geng",
      "Yi Zhao",
      "Ruibo Fu",
      "Tao Wang",
      "Cheng Gong",
      "Tianrui Wang",
      "Qiuyu Liu",
      "Jiangyan Yi",
      "Zhengqi Wen",
      "Chen Zhang",
      "Hao Che",
      "Longbiao Wang",
      "Jianwu Dang",
      "Jianhua Tao"
    ],
    "abstract": "Deep learning has brought significant improvements to the field of\ncross-modal representation learning. For tasks such as text-to-speech (TTS),\nvoice conversion (VC), and automatic speech recognition (ASR), a cross-modal\nfine-grained (frame-level) sequence representation is desired, emphasizing the\nsemantic content of the text modality while de-emphasizing the paralinguistic\ninformation of the speech modality. We propose a method called \"Vector\nQuantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the\ncross-modal aligned sequence transcoder to bring text and speech into a joint\nmultimodal space, learning how to connect text and speech at the frame level.\nThe proposed VQ-CTAP is a paradigm for cross-modal sequence representation\nlearning, offering a promising solution for fine-grained generation and\nrecognition tasks in speech processing. The VQ-CTAP can be directly applied to\nVC and ASR tasks without fine-tuning or additional structures. We propose a\nsequence-aware semantic connector, which connects multiple frozen pre-trained\nmodules for the TTS task, exhibiting a plug-and-play capability. We design a\nstepping optimization strategy to ensure effective model convergence by\ngradually injecting and adjusting the influence of various loss components.\nFurthermore, we propose a semantic-transfer-wise paralinguistic consistency\nloss to enhance representational capabilities, allowing the model to better\ngeneralize to unseen data and capture the nuances of paralinguistic\ninformation. In addition, VQ-CTAP achieves high-compression speech coding at a\nrate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the\nsampling rate. The audio demo is available at\nhttps://qiangchunyu.github.io/VQCTAP/",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05758v1",
    "published_date": "2024-08-11 12:24:23 UTC",
    "updated_date": "2024-08-11 12:24:23 UTC"
  },
  {
    "arxiv_id": "2408.05748v1",
    "title": "Low-Dimensional Federated Knowledge Graph Embedding via Knowledge Distillation",
    "authors": [
      "Xiaoxiong Zhang",
      "Zhiwei Zeng",
      "Xin Zhou",
      "Zhiqi Shen"
    ],
    "abstract": "Federated Knowledge Graph Embedding (FKGE) aims to facilitate collaborative\nlearning of entity and relation embeddings from distributed Knowledge Graphs\n(KGs) across multiple clients, while preserving data privacy. Training FKGE\nmodels with higher dimensions is typically favored due to their potential for\nachieving superior performance. However, high-dimensional embeddings present\nsignificant challenges in terms of storage resource and inference speed. Unlike\ntraditional KG embedding methods, FKGE involves multiple client-server\ncommunication rounds, where communication efficiency is critical. Existing\nembedding compression methods for traditional KGs may not be directly\napplicable to FKGE as they often require multiple model trainings which\npotentially incur substantial communication costs. In this paper, we propose a\nlight-weight component based on Knowledge Distillation (KD) which is titled\nFedKD and tailored specifically for FKGE methods. During client-side local\ntraining, FedKD facilitates the low-dimensional student model to mimic the\nscore distribution of triples from the high-dimensional teacher model using KL\ndivergence loss. Unlike traditional KD way, FedKD adaptively learns a\ntemperature to scale the score of positive triples and separately adjusts the\nscores of corresponding negative triples using a predefined temperature,\nthereby mitigating teacher over-confidence issue. Furthermore, we dynamically\nadjust the weight of KD loss to optimize the training process. Extensive\nexperiments on three datasets support the effectiveness of FedKD.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05748v1",
    "published_date": "2024-08-11 11:15:41 UTC",
    "updated_date": "2024-08-11 11:15:41 UTC"
  },
  {
    "arxiv_id": "2408.05740v1",
    "title": "MTSCI: A Conditional Diffusion Model for Multivariate Time Series Consistent Imputation",
    "authors": [
      "Jianping Zhou",
      "Junhao Li",
      "Guanjie Zheng",
      "Xinbing Wang",
      "Chenghu Zhou"
    ],
    "abstract": "Missing values are prevalent in multivariate time series, compromising the\nintegrity of analyses and degrading the performance of downstream tasks.\nConsequently, research has focused on multivariate time series imputation,\naiming to accurately impute the missing values based on available observations.\nA key research question is how to ensure imputation consistency, i.e.,\nintra-consistency between observed and imputed values, and inter-consistency\nbetween adjacent windows after imputation. However, previous methods rely\nsolely on the inductive bias of the imputation targets to guide the learning\nprocess, ignoring imputation consistency and ultimately resulting in poor\nperformance. Diffusion models, known for their powerful generative abilities,\nprefer to generate consistent results based on available observations.\nTherefore, we propose a conditional diffusion model for Multivariate Time\nSeries Consistent Imputation (MTSCI). Specifically, MTSCI employs a contrastive\ncomplementary mask to generate dual views during the forward noising process.\nThen, the intra contrastive loss is calculated to ensure intra-consistency\nbetween the imputed and observed values. Meanwhile, MTSCI utilizes a mixup\nmechanism to incorporate conditional information from adjacent windows during\nthe denoising process, facilitating the inter-consistency between imputed\nsamples. Extensive experiments on multiple real-world datasets demonstrate that\nour method achieves the state-of-the-art performance on multivariate time\nseries imputation task under different missing scenarios. Code is available at\nhttps://github.com/JeremyChou28/MTSCI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures, accepted by CIKM2024",
    "pdf_url": "http://arxiv.org/pdf/2408.05740v1",
    "published_date": "2024-08-11 10:24:53 UTC",
    "updated_date": "2024-08-11 10:24:53 UTC"
  },
  {
    "arxiv_id": "2408.06391v1",
    "title": "Autoregressive Enzyme Function Prediction with Multi-scale Multi-modality Fusion",
    "authors": [
      "Dingyi Rong",
      "Wenzhuo Zheng",
      "Bozitao Zhong",
      "Zhouhan Lin",
      "Liang Hong",
      "Ning Liu"
    ],
    "abstract": "Accurate prediction of enzyme function is crucial for elucidating biological\nmechanisms and driving innovation across various sectors. Existing deep\nlearning methods tend to rely solely on either sequence data or structural data\nand predict the EC number as a whole, neglecting the intrinsic hierarchical\nstructure of EC numbers. To address these limitations, we introduce MAPred, a\nnovel multi-modality and multi-scale model designed to autoregressively predict\nthe EC number of proteins. MAPred integrates both the primary amino acid\nsequence and the 3D tokens of proteins, employing a dual-pathway approach to\ncapture comprehensive protein characteristics and essential local functional\nsites. Additionally, MAPred utilizes an autoregressive prediction network to\nsequentially predict the digits of the EC number, leveraging the hierarchical\norganization of EC classifications. Evaluations on benchmark datasets,\nincluding New-392, Price, and New-815, demonstrate that our method outperforms\nexisting models, marking a significant advance in the reliability and\ngranularity of protein function prediction within bioinformatics.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06391v1",
    "published_date": "2024-08-11 08:28:43 UTC",
    "updated_date": "2024-08-11 08:28:43 UTC"
  },
  {
    "arxiv_id": "2408.05717v1",
    "title": "Deformable Image Registration with Multi-scale Feature Fusion from Shared Encoder, Auxiliary and Pyramid Decoders",
    "authors": [
      "Hongchao Zhou",
      "Shunbo Hu"
    ],
    "abstract": "In this work, we propose a novel deformable convolutional pyramid network for\nunsupervised image registration. Specifically, the proposed network enhances\nthe traditional pyramid network by adding an additional shared auxiliary\ndecoder for image pairs. This decoder provides multi-scale high-level feature\ninformation from unblended image pairs for the registration task. During the\nregistration process, we also design a multi-scale feature fusion block to\nextract the most beneficial features for the registration task from both global\nand local contexts. Validation results indicate that this method can capture\ncomplex deformations while achieving higher registration accuracy and\nmaintaining smooth and plausible deformations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05717v1",
    "published_date": "2024-08-11 08:02:28 UTC",
    "updated_date": "2024-08-11 08:02:28 UTC"
  },
  {
    "arxiv_id": "2408.05715v1",
    "title": "Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking",
    "authors": [
      "Zhi-Cun Lyu",
      "Xin-Ye Li",
      "Zheng Xie",
      "Ming Li"
    ],
    "abstract": "Code generation has been greatly enhanced by the profound advancements in\nLarge Language Models (LLMs) recently. Nevertheless, such LLM-based code\ngeneration approaches still struggle to generate error-free code in a few tries\nwhen faced with complex problems. To address this, the prevailing strategy is\nto sample a huge number of candidate programs, with the hope of any one in them\ncould work. However, users of code generation systems usually expect to find a\ncorrect program by reviewing or testing only a small number of code candidates.\nOtherwise, the system would be unhelpful. In this paper, we propose Top Pass, a\ncode ranking approach that identifies potential correct solutions from a large\nnumber of candidates. Top Pass directly optimizes the pass@k loss function,\nenhancing the quality at the top of the candidate list. This enables the user\nto find the correct solution within as few tries as possible. Experimental\nresults on four benchmarks indicate that our Top Pass method enhances the\nusability of code generation models by producing better ranking results,\nparticularly achieving a 32.9\\% relative improvement in pass@1 on CodeContests\nwhen compared to the state-of-the-art ranking method.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by Frontier of Computer Science",
    "pdf_url": "http://arxiv.org/pdf/2408.05715v1",
    "published_date": "2024-08-11 07:53:51 UTC",
    "updated_date": "2024-08-11 07:53:51 UTC"
  },
  {
    "arxiv_id": "2408.05712v1",
    "title": "DeepAir: A Multi-Agent Deep Reinforcement Learning Based Scheme for an Unknown User Location Problem",
    "authors": [
      "Baris Yamansavascilar",
      "Atay Ozgovde",
      "Cem Ersoy"
    ],
    "abstract": "The deployment of unmanned aerial vehicles (UAVs) in many different settings\nhas provided various solutions and strategies for networking paradigms.\nTherefore, it reduces the complexity of the developments for the existing\nproblems, which otherwise require more sophisticated approaches. One of those\nexisting problems is the unknown user locations in an infrastructure-less\nenvironment in which users cannot connect to any communication device or\ncomputation-providing server, which is essential to task offloading in order to\nachieve the required quality of service (QoS). Therefore, in this study, we\ninvestigate this problem thoroughly and propose a novel deep reinforcement\nlearning (DRL) based scheme, DeepAir. DeepAir considers all of the necessary\nsteps including sensing, localization, resource allocation, and multi-access\nedge computing (MEC) to achieve QoS requirements for the offloaded tasks\nwithout violating the maximum tolerable delay. To this end, we use two types of\nUAVs including detector UAVs, and serving UAVs. We utilize detector UAVs as DRL\nagents which ensure sensing, localization, and resource allocation. On the\nother hand, we utilize serving UAVs to provide MEC features. Our experiments\nshow that DeepAir provides a high task success rate by deploying fewer detector\nUAVs in the environment, which includes different numbers of users and user\nattraction points, compared to benchmark methods.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "12 pages, 8 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.05712v1",
    "published_date": "2024-08-11 07:28:35 UTC",
    "updated_date": "2024-08-11 07:28:35 UTC"
  },
  {
    "arxiv_id": "2408.05705v2",
    "title": "TC-KANRecon: High-Quality and Accelerated MRI Reconstruction via Adaptive KAN Mechanisms and Intelligent Feature Scaling",
    "authors": [
      "Ruiquan Ge",
      "Xiao Yu",
      "Yifei Chen",
      "Guanyu Zhou",
      "Fan Jia",
      "Shenghao Zhu",
      "Junhao Jia",
      "Chenyan Zhang",
      "Yifei Sun",
      "Dong Zeng",
      "Changmiao Wang",
      "Qiegen Liu",
      "Shanzhou Niu"
    ],
    "abstract": "Magnetic Resonance Imaging (MRI) has become essential in clinical diagnosis\ndue to its high resolution and multiple contrast mechanisms. However, the\nrelatively long acquisition time limits its broader application. To address\nthis issue, this study presents an innovative conditional guided diffusion\nmodel, named as TC-KANRecon, which incorporates the Multi-Free U-KAN (MF-UKAN)\nmodule and a dynamic clipping strategy. TC-KANRecon model aims to accelerate\nthe MRI reconstruction process through deep learning methods while maintaining\nthe quality of the reconstructed images. The MF-UKAN module can effectively\nbalance the tradeoff between image denoising and structure preservation.\nSpecifically, it presents the multi-head attention mechanisms and scalar\nmodulation factors, which significantly enhances the model's robustness and\nstructure preservation capabilities in complex noise environments. Moreover,\nthe dynamic clipping strategy in TC-KANRecon adjusts the cropping interval\naccording to the sampling steps, thereby mitigating image detail loss\ntypicalching the visual features of the images. Furthermore, the MC-Model\nincorporates full-sampling k-space information, realizing efficient fusion of\nconditional information, enhancing the model's ability to process complex data,\nand improving the realism and detail richness of reconstructed images.\nExperimental results demonstrate that the proposed method outperforms other MRI\nreconstruction methods in both qualitative and quantitative evaluations.\nNotably, TC-KANRecon method exhibits excellent reconstruction results when\nprocessing high-noise, low-sampling-rate MRI data. Our source code is available\nat https://github.com/lcbkmm/TC-KANRecon.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.05705v2",
    "published_date": "2024-08-11 06:31:56 UTC",
    "updated_date": "2025-01-06 10:11:37 UTC"
  },
  {
    "arxiv_id": "2408.05692v1",
    "title": "A Novel Momentum-Based Deep Learning Techniques for Medical Image Classification and Segmentation",
    "authors": [
      "Koushik Biswas",
      "Ridal Pal",
      "Shaswat Patel",
      "Debesh Jha",
      "Meghana Karri",
      "Amit Reza",
      "Gorkem Durak",
      "Alpay Medetalibeyoglu",
      "Matthew Antalek",
      "Yury Velichko",
      "Daniela Ladner",
      "Amir Borhani",
      "Ulas Bagci"
    ],
    "abstract": "Accurately segmenting different organs from medical images is a critical\nprerequisite for computer-assisted diagnosis and intervention planning. This\nstudy proposes a deep learning-based approach for segmenting various organs\nfrom CT and MRI scans and classifying diseases. Our study introduces a novel\ntechnique integrating momentum within residual blocks for enhanced training\ndynamics in medical image analysis. We applied our method in two distinct\ntasks: segmenting liver, lung, & colon data and classifying abdominal pelvic CT\nand MRI scans. The proposed approach has shown promising results, outperforming\nstate-of-the-art methods on publicly available benchmarking datasets. For\ninstance, in the lung segmentation dataset, our approach yielded significant\nenhancements over the TransNetR model, including a 5.72% increase in dice\nscore, a 5.04% improvement in mean Intersection over Union (mIoU), an 8.02%\nimprovement in recall, and a 4.42% improvement in precision. Hence,\nincorporating momentum led to state-of-the-art performance in both segmentation\nand classification tasks, representing a significant advancement in the field\nof medical imaging.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.05692v1",
    "published_date": "2024-08-11 04:12:35 UTC",
    "updated_date": "2024-08-11 04:12:35 UTC"
  },
  {
    "arxiv_id": "2408.05682v1",
    "title": "Separate Generation and Evaluation for Parallel Greedy Best-First Search",
    "authors": [
      "Takumi Shimoda",
      "Alex Fukunaga"
    ],
    "abstract": "Parallelization of Greedy Best First Search (GBFS) has been difficult because\nstraightforward parallelization can result in search behavior which differs\nsignificantly from sequential GBFS, exploring states which would not be\nexplored by sequential GBFS with any tie-breaking strategy. Recent work has\nproposed a class of parallel GBFS algorithms which constrains search to\nexploration of the Bench Transition System (BTS), which is the set of states\nthat can be expanded by GBFS under some tie-breaking policy. However, enforcing\nthis constraint is costly, as such BTS-constrained algorithms are forced to\nspend much of the time waiting so that only states which are guaranteed to be\nin the BTS are expanded. We propose an improvement to parallel search which\ndecouples state generation and state evaluation and significantly improves\nstate evaluation rate, resulting in better search performance.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.DS"
    ],
    "primary_category": "cs.AI",
    "comment": "In Proceedings of ICAPS-2024 Workshop on Heuristics and Search for\n  Domain-Independent Planning (HSDIP-24)\n  https://icaps24.icaps-conference.org/program/workshops/hsdip/",
    "pdf_url": "http://arxiv.org/pdf/2408.05682v1",
    "published_date": "2024-08-11 03:29:17 UTC",
    "updated_date": "2024-08-11 03:29:17 UTC"
  },
  {
    "arxiv_id": "2408.05681v1",
    "title": "SRTFD: Scalable Real-Time Fault Diagnosis through Online Continual Learning",
    "authors": [
      "Dandan Zhao",
      "Karthick Sharma",
      "Hongpeng Yin",
      "Yuxin Qi",
      "Shuhao Zhang"
    ],
    "abstract": "Fault diagnosis (FD) is essential for maintaining operational safety and\nminimizing economic losses by detecting system abnormalities. Recently, deep\nlearning (DL)-driven FD methods have gained prominence, offering significant\nimprovements in precision and adaptability through the utilization of extensive\ndatasets and advanced DL models. Modern industrial environments, however,\ndemand FD methods that can handle new fault types, dynamic conditions,\nlarge-scale data, and provide real-time responses with minimal prior\ninformation. Although online continual learning (OCL) demonstrates potential in\naddressing these requirements by enabling DL models to continuously learn from\nstreaming data, it faces challenges such as data redundancy, imbalance, and\nlimited labeled data. To overcome these limitations, we propose SRTFD, a\nscalable real-time fault diagnosis framework that enhances OCL with three\ncritical methods: Retrospect Coreset Selection (RCS), which selects the most\nrelevant data to reduce redundant training and improve efficiency; Global\nBalance Technique (GBT), which ensures balanced coreset selection and robust\nmodel performance; and Confidence and Uncertainty-driven Pseudo-label Learning\n(CUPL), which updates the model using unlabeled data for continuous adaptation.\nExtensive experiments on a real-world dataset and two public simulated datasets\ndemonstrate SRTFD's effectiveness and potential for providing advanced,\nscalable, and precise fault diagnosis in modern industrial systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05681v1",
    "published_date": "2024-08-11 03:26:22 UTC",
    "updated_date": "2024-08-11 03:26:22 UTC"
  },
  {
    "arxiv_id": "2408.05678v1",
    "title": "Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data",
    "authors": [
      "Ji Liu",
      "Juncheng Jia",
      "Hong Zhang",
      "Yuhui Yun",
      "Leye Wang",
      "Yang Zhou",
      "Huaiyu Dai",
      "Dejing Dou"
    ],
    "abstract": "Despite achieving remarkable performance, Federated Learning (FL) encounters\ntwo important problems, i.e., low training efficiency and limited computational\nresources. In this paper, we propose a new FL framework, i.e., FedDUMAP, with\nthree original contributions, to leverage the shared insensitive data on the\nserver in addition to the distributed data in edge devices so as to efficiently\ntrain a global model. First, we propose a simple dynamic server update\nalgorithm, which takes advantage of the shared insensitive data on the server\nwhile dynamically adjusting the update steps on the server in order to speed up\nthe convergence and improve the accuracy. Second, we propose an adaptive\noptimization method with the dynamic server update algorithm to exploit the\nglobal momentum on the server and each local device for superior accuracy.\nThird, we develop a layer-adaptive model pruning method to carry out specific\npruning operations, which is adapted to the diverse features of each layer so\nas to attain an excellent trade-off between effectiveness and efficiency. Our\nproposed FL model, FedDUMAP, combines the three original techniques and has a\nsignificantly better performance compared with baseline approaches in terms of\nefficiency (up to 16.9 times faster), accuracy (up to 20.4% higher), and\ncomputational cost (up to 62.6% smaller).",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "27 pages, to appear in TIST",
    "pdf_url": "http://arxiv.org/pdf/2408.05678v1",
    "published_date": "2024-08-11 02:59:11 UTC",
    "updated_date": "2024-08-11 02:59:11 UTC"
  },
  {
    "arxiv_id": "2408.05669v1",
    "title": "StealthDiffusion: Towards Evading Diffusion Forensic Detection through Diffusion Model",
    "authors": [
      "Ziyin Zhou",
      "Ke Sun",
      "Zhongxi Chen",
      "Huafeng Kuang",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ],
    "abstract": "The rapid progress in generative models has given rise to the critical task\nof AI-Generated Content Stealth (AIGC-S), which aims to create AI-generated\nimages that can evade both forensic detectors and human inspection. This task\nis crucial for understanding the vulnerabilities of existing detection methods\nand developing more robust techniques. However, current adversarial attacks\noften introduce visible noise, have poor transferability, and fail to address\nspectral differences between AI-generated and genuine images. To address this,\nwe propose StealthDiffusion, a framework based on stable diffusion that\nmodifies AI-generated images into high-quality, imperceptible adversarial\nexamples capable of evading state-of-the-art forensic detectors.\nStealthDiffusion comprises two main components: Latent Adversarial\nOptimization, which generates adversarial perturbations in the latent space of\nstable diffusion, and Control-VAE, a module that reduces spectral differences\nbetween the generated adversarial images and genuine images without affecting\nthe original diffusion model's generation process. Extensive experiments show\nthat StealthDiffusion is effective in both white-box and black-box settings,\ntransforming AI-generated images into high-quality adversarial forgeries with\nfrequency spectra similar to genuine images. These forgeries are classified as\ngenuine by advanced forensic classifiers and are difficult for humans to\ndistinguish.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05669v1",
    "published_date": "2024-08-11 01:22:29 UTC",
    "updated_date": "2024-08-11 01:22:29 UTC"
  },
  {
    "arxiv_id": "2408.13718v1",
    "title": "GPT-4 Emulates Average-Human Emotional Cognition from a Third-Person Perspective",
    "authors": [
      "Ala N. Tak",
      "Jonathan Gratch"
    ],
    "abstract": "This paper extends recent investigations on the emotional reasoning abilities\nof Large Language Models (LLMs). Current research on LLMs has not directly\nevaluated the distinction between how LLMs predict the self-attribution of\nemotions and the perception of others' emotions. We first look at carefully\ncrafted emotion-evoking stimuli, originally designed to find patterns of brain\nneural activity representing fine-grained inferred emotional attributions of\nothers. We show that GPT-4 is especially accurate in reasoning about such\nstimuli. This suggests LLMs agree with humans' attributions of others' emotions\nin stereotypical scenarios remarkably more than self-attributions of emotions\nin idiosyncratic situations. To further explore this, our second study utilizes\na dataset containing annotations from both the author and a third-person\nperspective. We find that GPT-4's interpretations align more closely with human\njudgments about the emotions of others than with self-assessments. Notably,\nconventional computational models of emotion primarily rely on self-reported\nground truth as the gold standard. However, an average observer's standpoint,\nwhich LLMs appear to have adopted, might be more relevant for many downstream\napplications, at least in the absence of individual information and adequate\nsafety considerations.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "submitted to 12th International Conference on Affective Computing &\n  Intelligent Interaction, Glasgow, UK, September 15-18, 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.13718v1",
    "published_date": "2024-08-11 01:22:09 UTC",
    "updated_date": "2024-08-11 01:22:09 UTC"
  },
  {
    "arxiv_id": "2408.05667v3",
    "title": "PhishLang: A Real-Time, Fully Client-Side Phishing Detection Framework Using MobileBERT",
    "authors": [
      "Sayak Saha Roy",
      "Shirin Nilizadeh"
    ],
    "abstract": "In this paper, we introduce PhishLang, the first fully client-side\nanti-phishing framework built on a lightweight ensemble framework that utilizes\nadvanced language models to analyze the contextual features of a website's\nsource code and URL. Unlike traditional heuristic or machine learning\napproaches that rely on static features and struggle to adapt to evolving\nthreats, or deep learning models that are computationally intensive, our\napproach utilizes MobileBERT, a fast and memory-efficient variant of the BERT\narchitecture, to capture nuanced features indicative of phishing attacks. To\nfurther enhance detection accuracy, PhishLang employs a multi-modal ensemble\napproach, combining both the URL and Source detection models. This architecture\nensures robustness by allowing one model to compensate for scenarios where the\nother may fail, or if both models provide ambiguous inferences. As a result,\nPhishLang excels at detecting both regular and evasive phishing threats,\nincluding zero-day attacks, outperforming popular anti-phishing tools, while\noperating without relying on external blocklists and safeguarding user privacy\nby ensuring that browser history remains entirely local and unshared. We\nrelease PhishLang as a Chromium browser extension and also open-source the\nframework to aid the research community.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.HC",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.05667v3",
    "published_date": "2024-08-11 01:14:13 UTC",
    "updated_date": "2025-04-16 23:13:25 UTC"
  }
]
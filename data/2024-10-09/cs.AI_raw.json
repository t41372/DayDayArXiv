[
  {
    "arxiv_id": "2410.07484v2",
    "title": "WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents",
    "authors": [
      "Siyu Zhou",
      "Tianyi Zhou",
      "Yijun Yang",
      "Guodong Long",
      "Deheng Ye",
      "Jing Jiang",
      "Chengqi Zhang"
    ],
    "abstract": "Can large language models (LLMs) directly serve as powerful world models for\nmodel-based agents? While the gaps between the prior knowledge of LLMs and the\nspecified environment's dynamics do exist, our study reveals that the gaps can\nbe bridged by aligning an LLM with its deployed environment and such \"world\nalignment\" can be efficiently achieved by rule learning on LLMs. Given the rich\nprior knowledge of LLMs, only a few additional rules suffice to align LLM\npredictions with the specified environment dynamics. To this end, we propose a\nneurosymbolic approach to learn these rules gradient-free through LLMs, by\ninducing, updating, and pruning rules based on comparisons of agent-explored\ntrajectories and world model predictions. The resulting world model is composed\nof the LLM and the learned rules. Our embodied LLM agent \"WALL-E\" is built upon\nmodel-predictive control (MPC). By optimizing look-ahead actions based on the\nprecise world model, MPC significantly improves exploration and learning\nefficiency. Compared to existing LLM agents, WALL-E's reasoning only requires a\nfew principal rules rather than verbose buffered trajectories being included in\nthe LLM input. On open-world challenges in Minecraft and ALFWorld, WALL-E\nachieves higher success rates than existing methods, with lower costs on\nreplanning time and the number of tokens used for reasoning. In Minecraft,\nWALL-E exceeds baselines by 15-30% in success rate while costing 8-20 fewer\nreplanning rounds and only 60-80% of tokens. In ALFWorld, its success rate\nsurges to a new record high of 95% only after 6 iterations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "35 pages, including references and appendix. Code is available at\n  https://github.com/elated-sawyer/WALL-E",
    "pdf_url": "http://arxiv.org/pdf/2410.07484v2",
    "published_date": "2024-10-09 23:37:36 UTC",
    "updated_date": "2024-10-11 23:32:09 UTC"
  },
  {
    "arxiv_id": "2410.07472v1",
    "title": "Exploring the design space of deep-learning-based weather forecasting systems",
    "authors": [
      "Shoaib Ahmed Siddiqui",
      "Jean Kossaifi",
      "Boris Bonev",
      "Christopher Choy",
      "Jan Kautz",
      "David Krueger",
      "Kamyar Azizzadenesheli"
    ],
    "abstract": "Despite tremendous progress in developing deep-learning-based weather\nforecasting systems, their design space, including the impact of different\ndesign choices, is yet to be well understood. This paper aims to fill this\nknowledge gap by systematically analyzing these choices including architecture,\nproblem formulation, pretraining scheme, use of image-based pretrained models,\nloss functions, noise injection, multi-step inputs, additional static masks,\nmulti-step finetuning (including larger stride models), as well as training on\na larger dataset. We study fixed-grid architectures such as UNet, fully\nconvolutional architectures, and transformer-based models, along with\ngrid-invariant architectures, including graph-based and operator-based models.\nOur results show that fixed-grid architectures outperform grid-invariant\narchitectures, indicating a need for further architectural developments in\ngrid-invariant models such as neural operators. We therefore propose a hybrid\nsystem that combines the strong performance of fixed-grid models with the\nflexibility of grid-invariant architectures. We further show that multi-step\nfine-tuning is essential for most deep-learning models to work well in\npractice, which has been a common practice in the past. Pretraining objectives\ndegrade performance in comparison to supervised training, while image-based\npretrained models provide useful inductive biases in some cases in comparison\nto training the model from scratch. Interestingly, we see a strong positive\neffect of using a larger dataset when training a smaller model as compared to\ntraining on a smaller dataset for longer. Larger models, on the other hand,\nprimarily benefit from just an increase in the computational budget. We believe\nthat these results will aid in the design of better weather forecasting systems\nin the future.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07472v1",
    "published_date": "2024-10-09 22:25:50 UTC",
    "updated_date": "2024-10-09 22:25:50 UTC"
  },
  {
    "arxiv_id": "2410.07471v2",
    "title": "SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection",
    "authors": [
      "Han Shen",
      "Pin-Yu Chen",
      "Payel Das",
      "Tianyi Chen"
    ],
    "abstract": "Fine-tuning on task-specific data to boost downstream performance is a\ncrucial step for leveraging Large Language Models (LLMs). However, previous\nstudies have demonstrated that fine-tuning the models on several adversarial\nsamples or even benign data can greatly comprise the model's pre-equipped\nalignment and safety capabilities. In this work, we propose SEAL, a novel\nframework to enhance safety in LLM fine-tuning. SEAL learns a data ranker based\non the bilevel optimization to up rank the safe and high-quality fine-tuning\ndata and down rank the unsafe or low-quality ones. Models trained with SEAL\ndemonstrate superior quality over multiple baselines, with 8.5% and 9.7% win\nrate increase compared to random selection respectively on Llama-3-8b-Instruct\nand Merlinite-7b models. Our code is available on github\nhttps://github.com/hanshen95/SEAL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07471v2",
    "published_date": "2024-10-09 22:24:22 UTC",
    "updated_date": "2024-10-11 01:05:22 UTC"
  },
  {
    "arxiv_id": "2410.07447v1",
    "title": "TinyLidarNet: 2D LiDAR-based End-to-End Deep Learning Model for F1TENTH Autonomous Racing",
    "authors": [
      "Mohammed Misbah Zarrar",
      "Qitao Weng",
      "Bakhbyergyen Yerjan",
      "Ahmet Soyyigit",
      "Heechul Yun"
    ],
    "abstract": "Prior research has demonstrated the effectiveness of end-to-end deep learning\nfor robotic navigation, where the control signals are directly derived from raw\nsensory data. However, the majority of existing end-to-end navigation solutions\nare predominantly camera-based. In this paper, we introduce TinyLidarNet, a\nlightweight 2D LiDAR-based end-to-end deep learning model for autonomous\nracing. An F1TENTH vehicle using TinyLidarNet won 3rd place in the 12th F1TENTH\nAutonomous Grand Prix competition, demonstrating its competitive performance.\nWe systematically analyze its performance on untrained tracks and computing\nrequirements for real-time processing. We find that TinyLidarNet's 1D\nConvolutional Neural Network (CNN) based architecture significantly outperforms\nwidely used Multi-Layer Perceptron (MLP) based architecture. In addition, we\nshow that it can be processed in real-time on low-end micro-controller units\n(MCUs).",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07447v1",
    "published_date": "2024-10-09 21:28:33 UTC",
    "updated_date": "2024-10-09 21:28:33 UTC"
  },
  {
    "arxiv_id": "2410.07441v1",
    "title": "Zero-Shot Generalization of Vision-Based RL Without Data Augmentation",
    "authors": [
      "Sumeet Batra",
      "Gaurav S. Sukhatme"
    ],
    "abstract": "Generalizing vision-based reinforcement learning (RL) agents to novel\nenvironments remains a difficult and open challenge. Current trends are to\ncollect large-scale datasets or use data augmentation techniques to prevent\noverfitting and improve downstream generalization. However, the computational\nand data collection costs increase exponentially with the number of task\nvariations and can destabilize the already difficult task of training RL\nagents. In this work, we take inspiration from recent advances in computational\nneuroscience and propose a model, Associative Latent DisentAnglement (ALDA),\nthat builds on standard off-policy RL towards zero-shot generalization.\nSpecifically, we revisit the role of latent disentanglement in RL and show how\ncombining it with a model of associative memory achieves zero-shot\ngeneralization on difficult task variations without relying on data\naugmentation. Finally, we formally show that data augmentation techniques are a\nform of weak disentanglement and discuss the implications of this insight.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07441v1",
    "published_date": "2024-10-09 21:14:09 UTC",
    "updated_date": "2024-10-09 21:14:09 UTC"
  },
  {
    "arxiv_id": "2410.07432v2",
    "title": "Can Transformers Reason Logically? A Study in SAT Solving",
    "authors": [
      "Leyan Pan",
      "Vijay Ganesh",
      "Jacob Abernethy",
      "Chris Esposo",
      "Wenke Lee"
    ],
    "abstract": "We formally study the logical reasoning capabilities of decoder-only\nTransformers in the context of the boolean satisfiability (SAT) problem. First,\nwe prove by construction that decoder-only Transformers can decide 3-SAT, in a\nnon-uniform model of computation, using backtracking and deduction via\nChain-of-Thought (CoT). %We prove its correctness by showing trace equivalence\nto the well-known DPLL SAT-solving algorithm. Second, we implement our\nconstruction as a PyTorch model with a tool (PARAT) that we designed to\nempirically demonstrate its correctness and investigate its properties. Third,\nrather than \\textit{programming} a transformer to reason, we evaluate\nempirically whether it can be \\textit{trained} to do so by learning directly\nfrom algorithmic traces (``reasoning paths'') from our theoretical\nconstruction. The trained models demonstrate strong out-of-distribution\ngeneralization on problem sizes seen during training but has limited length\ngeneralization, which is consistent with the implications of our theoretical\nresult",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "41 pages, 4 Figures",
    "pdf_url": "http://arxiv.org/pdf/2410.07432v2",
    "published_date": "2024-10-09 21:01:52 UTC",
    "updated_date": "2025-02-08 02:12:58 UTC"
  },
  {
    "arxiv_id": "2410.11875v1",
    "title": "A Framework for SLO, Carbon, and Wastewater-Aware Sustainable FaaS Cloud Platform Management",
    "authors": [
      "Sirui Qi",
      "Hayden Moore",
      "Ninad Hogade",
      "Dejan Milojicic",
      "Cullen Bash",
      "Sudeep Pasricha"
    ],
    "abstract": "Function-as-a-Service (FaaS) is a growing cloud computing paradigm that is\nexpected to reduce the user cost of service over traditional serverful\napproaches. However, the environmental impact of FaaS has not received much\nattention. We investigate FaaS scheduling and scaling from a sustainability\nperspective in this work. We find that the service-level objectives (SLOs) of\nFaaS and carbon emissions conflict with each other. We also find that\nSLO-focused FaaS scheduling can exacerbate water use in a datacenter. We\npropose a novel sustainability-focused FaaS scheduling and scaling framework to\nco-optimize SLO performance, carbon emissions, and wastewater generation.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.11875v1",
    "published_date": "2024-10-09 20:47:52 UTC",
    "updated_date": "2024-10-09 20:47:52 UTC"
  },
  {
    "arxiv_id": "2410.07426v1",
    "title": "CAFEEN: A Cooperative Approach for Energy Efficient NoCs with Multi-Agent Reinforcement Learning",
    "authors": [
      "Kamil Khan",
      "Sudeep Pasricha"
    ],
    "abstract": "In emerging high-performance Network-on-Chip (NoC) architectures, efficient\npower management is crucial to minimize energy consumption. We propose a novel\nframework called CAFEEN that employs both heuristic-based fine-grained and\nmachine learning-based coarse-grained power-gating for energy-efficient NoCs.\nCAFEEN uses a fine-grained method to activate only essential NoC buffers during\nlower network loads. It switches to a coarse-grained method at peak loads to\nminimize compounding wake-up overhead using multi-agent reinforcement learning.\nResults show that CAFEEN adaptively balances power-efficiency with performance,\nreducing total energy by 2.60x for single application workloads and 4.37x for\nmulti-application workloads, compared to state-of-the-art NoC power-gating\nframeworks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07426v1",
    "published_date": "2024-10-09 20:42:55 UTC",
    "updated_date": "2024-10-09 20:42:55 UTC"
  },
  {
    "arxiv_id": "2410.07405v1",
    "title": "Exploring Efficient Foundational Multi-modal Models for Video Summarization",
    "authors": [
      "Karan Samel",
      "Apoorva Beedu",
      "Nitish Sontakke",
      "Irfan Essa"
    ],
    "abstract": "Foundational models are able to generate text outputs given prompt\ninstructions and text, audio, or image inputs. Recently these models have been\ncombined to perform tasks on video, such as video summarization. Such video\nfoundation models perform pre-training by aligning outputs from each\nmodality-specific model into the same embedding space. Then the embeddings from\neach model are used within a language model, which is fine-tuned on a desired\ninstruction set. Aligning each modality during pre-training is computationally\nexpensive and prevents rapid testing of different base modality models. During\nfine-tuning, evaluation is carried out within in-domain videos where it is hard\nto understand the generalizability and data efficiency of these methods. To\nalleviate these issues we propose a plug-and-play video language model. It\ndirectly uses the texts generated from each input modality into the language\nmodel, avoiding pre-training alignment overhead. Instead of fine-tuning we\nleverage few-shot instruction adaptation strategies. We compare the performance\nversus the computational costs for our plug-and-play style method and baseline\ntuning methods. Finally, we explore the generalizability of each method during\ndomain shift and present insights on what data is useful when training data is\nlimited. Through this analysis, we present practical insights on how to\nleverage multi-modal foundational models for effective results given realistic\ncompute and data limitations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.07405v1",
    "published_date": "2024-10-09 20:07:06 UTC",
    "updated_date": "2024-10-09 20:07:06 UTC"
  },
  {
    "arxiv_id": "2410.07404v2",
    "title": "Fostering Intrinsic Motivation in Reinforcement Learning with Pretrained Foundation Models",
    "authors": [
      "Alain Andres",
      "Javier Del Ser"
    ],
    "abstract": "Exploration remains a significant challenge in reinforcement learning,\nespecially in environments where extrinsic rewards are sparse or non-existent.\nThe recent rise of foundation models, such as CLIP, offers an opportunity to\nleverage pretrained, semantically rich embeddings that encapsulate broad and\nreusable knowledge. In this work we explore the potential of these foundation\nmodels not just to drive exploration, but also to analyze the critical role of\nthe episodic novelty term in enhancing exploration effectiveness of the agent.\nWe also investigate whether providing the intrinsic module with complete state\ninformation -- rather than just partial observations -- can improve\nexploration, despite the difficulties in handling small variations within large\nstate spaces. Our experiments in the MiniGrid domain reveal that intrinsic\nmodules can effectively utilize full state information, significantly\nincreasing sample efficiency while learning an optimal policy. Moreover, we\nshow that the embeddings provided by foundation models are sometimes even\nbetter than those constructed by the agent during training, further\naccelerating the learning process, especially when coupled with the episodic\nnovelty term to enhance exploration.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the Intrinsically Motivated Open-ended Learning workshop\n  at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.07404v2",
    "published_date": "2024-10-09 20:05:45 UTC",
    "updated_date": "2024-11-25 07:42:16 UTC"
  },
  {
    "arxiv_id": "2410.12843v1",
    "title": "Exploring Prompt Engineering: A Systematic Review with SWOT Analysis",
    "authors": [
      "Aditi Singh",
      "Abul Ehtesham",
      "Gaurav Kumar Gupta",
      "Nikhil Kumar Chatta",
      "Saket Kumar",
      "Tala Talaei Khoei"
    ],
    "abstract": "In this paper, we conduct a comprehensive SWOT analysis of prompt engineering\ntechniques within the realm of Large Language Models (LLMs). Emphasizing\nlinguistic principles, we examine various techniques to identify their\nstrengths, weaknesses, opportunities, and threats. Our findings provide\ninsights into enhancing AI interactions and improving language model\ncomprehension of human prompts. The analysis covers techniques including\ntemplate-based approaches and fine-tuning, addressing the problems and\nchallenges associated with each. The conclusion offers future research\ndirections aimed at advancing the effectiveness of prompt engineering in\noptimizing human-machine communication.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 1 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.12843v1",
    "published_date": "2024-10-09 19:48:35 UTC",
    "updated_date": "2024-10-09 19:48:35 UTC"
  },
  {
    "arxiv_id": "2410.07395v1",
    "title": "LLM Embeddings Improve Test-time Adaptation to Tabular $Y|X$-Shifts",
    "authors": [
      "Yibo Zeng",
      "Jiashuo Liu",
      "Henry Lam",
      "Hongseok Namkoong"
    ],
    "abstract": "For tabular datasets, the change in the relationship between the label and\ncovariates ($Y|X$-shifts) is common due to missing variables (a.k.a.\nconfounders). Since it is impossible to generalize to a completely new and\nunknown domain, we study models that are easy to adapt to the target domain\neven with few labeled examples. We focus on building more informative\nrepresentations of tabular data that can mitigate $Y|X$-shifts, and propose to\nleverage the prior world knowledge in LLMs by serializing (write down) the\ntabular data to encode it. We find LLM embeddings alone provide inconsistent\nimprovements in robustness, but models trained on them can be well\nadapted/finetuned to the target domain even using 32 labeled observations. Our\nfinding is based on a comprehensive and systematic study consisting of 7650\nsource-target pairs and benchmark against 261,000 model configurations trained\nby 22 algorithms. Our observation holds when ablating the size of accessible\ntarget data and different adaptation strategies. The code is available at\nhttps://github.com/namkoong-lab/LLM-Tabular-Shifts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07395v1",
    "published_date": "2024-10-09 19:46:30 UTC",
    "updated_date": "2024-10-09 19:46:30 UTC"
  },
  {
    "arxiv_id": "2410.07391v1",
    "title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks",
    "authors": [
      "Isaac R. Galatzer-Levy",
      "David Munday",
      "Jed McGiffin",
      "Xin Liu",
      "Danny Karmon",
      "Ilia Labzovsky",
      "Rivka Moroshko",
      "Amir Zait",
      "Daniel McDuff"
    ],
    "abstract": "There is increasing interest in tracking the capabilities of general\nintelligence foundation models. This study benchmarks leading large language\nmodels and vision language models against human performance on the Wechsler\nAdult Intelligence Scale (WAIS-IV), a comprehensive, population-normed\nassessment of underlying human cognition and intellectual abilities, with a\nfocus on the domains of VerbalComprehension (VCI), Working Memory (WMI), and\nPerceptual Reasoning (PRI). Most models demonstrated exceptional capabilities\nin the storage, retrieval, and manipulation of tokens such as arbitrary\nsequences of letters and numbers, with performance on the Working Memory Index\n(WMI) greater or equal to the 99.5th percentile when compared to human\npopulation normative ability. Performance on the Verbal Comprehension Index\n(VCI) which measures retrieval of acquired information, and linguistic\nunderstanding about the meaning of words and their relationships to each other,\nalso demonstrated consistent performance at or above the 98th percentile.\nDespite these broad strengths, we observed consistently poor performance on the\nPerceptual Reasoning Index (PRI; range 0.1-10th percentile) from multimodal\nmodels indicating profound inability to interpret and reason on visual\ninformation. Smaller and older model versions consistently performed worse,\nindicating that training data, parameter count and advances in tuning are\nresulting in significant advances in cognitive ability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07391v1",
    "published_date": "2024-10-09 19:22:26 UTC",
    "updated_date": "2024-10-09 19:22:26 UTC"
  },
  {
    "arxiv_id": "2410.07383v1",
    "title": "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers",
    "authors": [
      "Viktoriia Chekalina",
      "Anna Rudenko",
      "Gleb Mezentsev",
      "Alexander Mikhalev",
      "Alexander Panchenko",
      "Ivan Oseledets"
    ],
    "abstract": "The performance of Transformer models has been enhanced by increasing the\nnumber of parameters and the length of the processed text. Consequently,\nfine-tuning the entire model becomes a memory-intensive process.\nHigh-performance methods for parameter-efficient fine-tuning (PEFT) typically\nwork with Attention blocks and often overlook MLP blocks, which contain about\nhalf of the model parameters. We propose a new selective PEFT method, namely\nSparseGrad, that performs well on MLP blocks. We transfer layer gradients to a\nspace where only about 1\\% of the layer's elements remain significant. By\nconverting gradients into a sparse structure, we reduce the number of updated\nparameters. We apply SparseGrad to fine-tune BERT and RoBERTa for the NLU task\nand LLaMa-2 for the Question-Answering task. In these experiments, with\nidentical memory requirements, our method outperforms LoRA and MeProp, robust\npopular state-of-the-art PEFT approaches.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07383v1",
    "published_date": "2024-10-09 19:03:52 UTC",
    "updated_date": "2024-10-09 19:03:52 UTC"
  },
  {
    "arxiv_id": "2410.07379v1",
    "title": "Learn from Real: Reality Defender's Submission to ASVspoof5 Challenge",
    "authors": [
      "Yi Zhu",
      "Chirag Goel",
      "Surya Koppisetti",
      "Trang Tran",
      "Ankur Kumar",
      "Gaurav Bharaj"
    ],
    "abstract": "Audio deepfake detection is crucial to combat the malicious use of\nAI-synthesized speech. Among many efforts undertaken by the community, the\nASVspoof challenge has become one of the benchmarks to evaluate the\ngeneralizability and robustness of detection models. In this paper, we present\nReality Defender's submission to the ASVspoof5 challenge, highlighting a novel\npretraining strategy which significantly improves generalizability while\nmaintaining low computational cost during training. Our system SLIM learns the\nstyle-linguistics dependency embeddings from various types of bonafide speech\nusing self-supervised contrastive learning. The learned embeddings help to\ndiscriminate spoof from bonafide speech by focusing on the relationship between\nthe style and linguistics aspects. We evaluated our system on ASVspoof5,\nASV2019, and In-the-wild. Our submission achieved minDCF of 0.1499 and EER of\n5.5% on ASVspoof5 Track 1, and EER of 7.4% and 10.8% on ASV2019 and In-the-wild\nrespectively.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted into ASVspoof5 workshop",
    "pdf_url": "http://arxiv.org/pdf/2410.07379v1",
    "published_date": "2024-10-09 18:55:28 UTC",
    "updated_date": "2024-10-09 18:55:28 UTC"
  },
  {
    "arxiv_id": "2410.07369v4",
    "title": "An Undetectable Watermark for Generative Image Models",
    "authors": [
      "Sam Gunn",
      "Xuandong Zhao",
      "Dawn Song"
    ],
    "abstract": "We present the first undetectable watermarking scheme for generative image\nmodels. Undetectability ensures that no efficient adversary can distinguish\nbetween watermarked and un-watermarked images, even after making many adaptive\nqueries. In particular, an undetectable watermark does not degrade image\nquality under any efficiently computable metric. Our scheme works by selecting\nthe initial latents of a diffusion model using a pseudorandom error-correcting\ncode (Christ and Gunn, 2024), a strategy which guarantees undetectability and\nrobustness. We experimentally demonstrate that our watermarks are\nquality-preserving and robust using Stable Diffusion 2.1. Our experiments\nverify that, in contrast to every prior scheme we tested, our watermark does\nnot degrade image quality. Our experiments also demonstrate robustness:\nexisting watermark removal attacks fail to remove our watermark from images\nwithout significantly degrading the quality of the images. Finally, we find\nthat we can robustly encode 512 bits in our watermark, and up to 2500 bits when\nthe images are not subjected to watermark removal attacks. Our code is\navailable at https://github.com/XuandongZhao/PRC-Watermark.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CR",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.07369v4",
    "published_date": "2024-10-09 18:33:06 UTC",
    "updated_date": "2025-04-21 21:40:23 UTC"
  },
  {
    "arxiv_id": "2410.12842v1",
    "title": "A Two-Model Approach for Humour Style Recognition",
    "authors": [
      "Mary Ogbuka Kenneth",
      "Foaad Khosmood",
      "Abbas Edalat"
    ],
    "abstract": "Humour, a fundamental aspect of human communication, manifests itself in\nvarious styles that significantly impact social interactions and mental health.\nRecognising different humour styles poses challenges due to the lack of\nestablished datasets and machine learning (ML) models. To address this gap, we\npresent a new text dataset for humour style recognition, comprising 1463\ninstances across four styles (self-enhancing, self-deprecating, affiliative,\nand aggressive) and non-humorous text, with lengths ranging from 4 to 229\nwords. Our research employs various computational methods, including classic\nmachine learning classifiers, text embedding models, and DistilBERT, to\nestablish baseline performance. Additionally, we propose a two-model approach\nto enhance humour style recognition, particularly in distinguishing between\naffiliative and aggressive styles. Our method demonstrates an 11.61%\nimprovement in f1-score for affiliative humour classification, with consistent\nimprovements in the 14 models tested. Our findings contribute to the\ncomputational analysis of humour in text, offering new tools for studying\nhumour in literature, social media, and other textual sources.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.12842v1",
    "published_date": "2024-10-09 18:25:07 UTC",
    "updated_date": "2024-10-09 18:25:07 UTC"
  },
  {
    "arxiv_id": "2410.07364v2",
    "title": "Unlocking Real-Time Fluorescence Lifetime Imaging: Multi-Pixel Parallelism for FPGA-Accelerated Processing",
    "authors": [
      "Ismail Erbas",
      "Aporva Amarnath",
      "Vikas Pandey",
      "Karthik Swaminathan",
      "Naigang Wang",
      "Xavier Intes"
    ],
    "abstract": "Fluorescence lifetime imaging (FLI) is a widely used technique in the\nbiomedical field for measuring the decay times of fluorescent molecules,\nproviding insights into metabolic states, protein interactions, and\nligand-receptor bindings. However, its broader application in fast biological\nprocesses, such as dynamic activity monitoring, and clinical use, such as in\nguided surgery, is limited by long data acquisition times and computationally\ndemanding data processing. While deep learning has reduced post-processing\ntimes, time-resolved data acquisition remains a bottleneck for real-time\napplications. To address this, we propose a method to achieve real-time FLI\nusing an FPGA-based hardware accelerator. Specifically, we implemented a\nGRU-based sequence-to-sequence (Seq2Seq) model on an FPGA board compatible with\ntime-resolved cameras. The GRU model balances accurate processing with the\nresource constraints of FPGAs, which have limited DSP units and BRAM. The\nlimited memory and computational resources on the FPGA require efficient\nscheduling of operations and memory allocation to deploy deep learning models\nfor low-latency applications. We address these challenges by using STOMP, a\nqueue-based discrete-event simulator that automates and optimizes task\nscheduling and memory management on hardware. By integrating a GRU-based\nSeq2Seq model and its compressed version, called Seq2SeqLite, generated through\nknowledge distillation, we were able to process multiple pixels in parallel,\nreducing latency compared to sequential processing. We explore various levels\nof parallelism to achieve an optimal balance between performance and resource\nutilization. Our results indicate that the proposed techniques achieved a 17.7x\nand 52.0x speedup over manual scheduling for the Seq2Seq model and the\nSeq2SeqLite model, respectively.",
    "categories": [
      "physics.optics",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "physics.optics",
    "comment": "7 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.07364v2",
    "published_date": "2024-10-09 18:24:23 UTC",
    "updated_date": "2024-11-15 15:46:00 UTC"
  },
  {
    "arxiv_id": "2410.07358v1",
    "title": "Improving the portability of predicting students performance models by using ontologies",
    "authors": [
      "Javier Lopez Zambrano",
      "Juan A. Lara",
      "Cristobal Romero"
    ],
    "abstract": "One of the main current challenges in Educational Data Mining and Learning\nAnalytics is the portability or transferability of predictive models obtained\nfor a particular course so that they can be applied to other different courses.\nTo handle this challenge, one of the foremost problems is the models excessive\ndependence on the low-level attributes used to train them, which reduces the\nmodels portability. To solve this issue, the use of high level attributes with\nmore semantic meaning, such as ontologies, may be very useful. Along this line,\nwe propose the utilization of an ontology that uses a taxonomy of actions that\nsummarises students interactions with the Moodle learning management system. We\ncompare the results of this proposed approach against our previous results when\nwe used low-level raw attributes obtained directly from Moodle logs. The\nresults indicate that the use of the proposed ontology improves the portability\nof the models in terms of predictive accuracy. The main contribution of this\npaper is to show that the ontological models obtained in one source course can\nbe applied to other different target courses with similar usage levels without\nlosing prediction accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07358v1",
    "published_date": "2024-10-09 18:18:54 UTC",
    "updated_date": "2024-10-09 18:18:54 UTC"
  },
  {
    "arxiv_id": "2410.07348v1",
    "title": "MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts",
    "authors": [
      "Peng Jin",
      "Bo Zhu",
      "Li Yuan",
      "Shuicheng Yan"
    ],
    "abstract": "In this work, we aim to simultaneously enhance the effectiveness and\nefficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose\nMoE++, a general and heterogeneous MoE framework that integrates both\nFeed-Forward Network~(FFN) and zero-computation experts. Specifically, we\nintroduce three types of zero-computation experts: the zero expert, copy\nexpert, and constant expert, which correspond to discard, skip, and replace\noperations, respectively. This design offers three key advantages: (i) Low\nComputing Overhead: Unlike the uniform mixing mechanism for all tokens within\nvanilla MoE, MoE++ allows each token to engage with a dynamic number of FFNs,\nbe adjusted by constant vectors, or even skip the MoE layer entirely. (ii) High\nPerformance: By enabling simple tokens to utilize fewer FFN experts, MoE++\nallows more experts to focus on challenging tokens, thereby unlocking greater\nperformance potential than vanilla MoE. (iii) Deployment Friendly: Given that\nzero-computation experts have negligible parameters, we can deploy all\nzero-computation experts on each GPU, eliminating the significant communication\noverhead and expert load imbalance associated with FFN experts distributed\nacross different GPUs. Moreover, we leverage gating residuals, enabling each\ntoken to consider the pathway taken in the previous layer when selecting the\nappropriate experts. Extensive experimental results demonstrate that MoE++\nachieves better performance while delivering 1.1-2.1x expert forward throughput\ncompared to a vanilla MoE model of the same size, which lays a solid foundation\nfor developing advanced and efficient MoE-related models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, Code: https://github.com/SkyworkAI/MoE-plus-plus",
    "pdf_url": "http://arxiv.org/pdf/2410.07348v1",
    "published_date": "2024-10-09 18:01:27 UTC",
    "updated_date": "2024-10-09 18:01:27 UTC"
  },
  {
    "arxiv_id": "2410.07336v1",
    "title": "Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training",
    "authors": [
      "Sara Sarto",
      "Nicholas Moratelli",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "abstract": "Despite significant advancements in caption generation, existing evaluation\nmetrics often fail to capture the full quality or fine-grained details of\ncaptions. This is mainly due to their reliance on non-specific human-written\nreferences or noisy pre-training data. Still, finding an effective metric is\ncrucial not only for captions evaluation but also for the generation phase.\nMetrics can indeed play a key role in the fine-tuning stage of captioning\nmodels, ultimately enhancing the quality of the generated captions. In this\npaper, we propose PAC-S++, a learnable metric that leverages the CLIP model,\npre-trained on both web-collected and cleaned data and regularized through\nadditional pairs of generated visual and textual positive samples. Exploiting\nthis stronger and curated pre-training, we also apply PAC-S++ as a reward in\nthe Self-Critical Sequence Training (SCST) stage typically employed to\nfine-tune captioning models. Extensive experiments on different image and video\ndatasets highlight the effectiveness of PAC-S++ compared to popular metrics for\nthe task, including its sensitivity to object hallucinations. Furthermore, we\nshow that integrating PAC-S++ into the fine-tuning stage of a captioning model\nresults in semantically richer captions with fewer repetitions and grammatical\nerrors. Evaluations on out-of-domain benchmarks further demonstrate the\nefficacy of our fine-tuning approach in enhancing model capabilities. Source\ncode and trained models are publicly available at:\nhttps://github.com/aimagelab/pacscore.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07336v1",
    "published_date": "2024-10-09 18:00:09 UTC",
    "updated_date": "2024-10-09 18:00:09 UTC"
  },
  {
    "arxiv_id": "2410.07331v2",
    "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
    "authors": [
      "Yiming Huang",
      "Jianwen Luo",
      "Yan Yu",
      "Yitong Zhang",
      "Fangyu Lei",
      "Yifan Wei",
      "Shizhu He",
      "Lifu Huang",
      "Xiao Liu",
      "Jun Zhao",
      "Kang Liu"
    ],
    "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to\nassess LLMs on agent-based data science tasks. This benchmark features three\ncore elements: First, the tasks within DA-Code are inherently challenging,\nsetting them apart from traditional code generation tasks and demanding\nadvanced coding skills in grounding and planning. Second, examples in DA-Code\nare all based on real and diverse data, covering a wide range of complex data\nwrangling and analytics tasks. Third, to solve the tasks, the models must\nutilize complex data science programming languages, to perform intricate data\nprocessing and derive the answers. We set up the benchmark in a controllable\nand executable environment that aligns with real-world data analysis scenarios\nand is scalable. The annotators meticulously design the evaluation suite to\nensure the accuracy and robustness of the evaluation. We develop the DA-Agent\nbaseline. Experiments show that although the baseline performs better than\nother existing frameworks, using the current best LLMs achieves only 30.5%\naccuracy, leaving ample room for improvement. We release our benchmark at\nhttps://da-code-bench.github.io.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.07331v2",
    "published_date": "2024-10-09 18:00:05 UTC",
    "updated_date": "2024-10-11 00:53:05 UTC"
  },
  {
    "arxiv_id": "2410.07177v2",
    "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs for Video QA",
    "authors": [
      "Hanrong Ye",
      "Haotian Zhang",
      "Erik Daxberger",
      "Lin Chen",
      "Zongyu Lin",
      "Yanghao Li",
      "Bowen Zhang",
      "Haoxuan You",
      "Dan Xu",
      "Zhe Gan",
      "Jiasen Lu",
      "Yinfei Yang"
    ],
    "abstract": "This research aims to comprehensively explore building a multimodal\nfoundation model for egocentric video understanding. To achieve this goal, we\nwork on three fronts. First, as there is a lack of QA data for egocentric video\nunderstanding, we automatically generate 7M high-quality QA samples for\negocentric videos ranging from 30 seconds to one hour long in Ego4D based on\nhuman-annotated data. This is one of the largest egocentric QA datasets.\nSecond, we contribute a challenging egocentric QA benchmark with 629 videos and\n7,026 questions to evaluate the models' ability in recognizing and memorizing\nvisual details across videos of varying lengths. We introduce a new de-biasing\nevaluation method to help mitigate the unavoidable language bias present in the\nmodels being evaluated. Third, we propose a specialized multimodal architecture\nfeaturing a novel \"Memory Pointer Prompting\" mechanism. This design includes a\n\\textit{global glimpse} step to gain an overarching understanding of the entire\nvideo and identify key visual information, followed by a fallback step that\nutilizes the key visual information to generate responses. This enables the\nmodel to more effectively comprehend extended video content. With the data,\nbenchmark, and model, we build MM-Ego, an egocentric multimodal LLM that shows\npowerful performance on egocentric video understanding.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.07177v2",
    "published_date": "2024-10-09 17:59:59 UTC",
    "updated_date": "2025-04-13 12:27:56 UTC"
  },
  {
    "arxiv_id": "2410.07176v1",
    "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models",
    "authors": [
      "Fei Wang",
      "Xingchen Wan",
      "Ruoxi Sun",
      "Jiefeng Chen",
      "Sercan Ö. Arık"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2410.07176v1",
    "published_date": "2024-10-09 17:59:58 UTC",
    "updated_date": "2024-10-09 17:59:58 UTC"
  },
  {
    "arxiv_id": "2410.07174v1",
    "title": "Neural Circuit Architectural Priors for Quadruped Locomotion",
    "authors": [
      "Nikhil X. Bhattasali",
      "Venkatesh Pattabiraman",
      "Lerrel Pinto",
      "Grace W. Lindsay"
    ],
    "abstract": "Learning-based approaches to quadruped locomotion commonly adopt generic\npolicy architectures like fully connected MLPs. As such architectures contain\nfew inductive biases, it is common in practice to incorporate priors in the\nform of rewards, training curricula, imitation data, or trajectory generators.\nIn nature, animals are born with priors in the form of their nervous system's\narchitecture, which has been shaped by evolution to confer innate ability and\nefficient learning. For instance, a horse can walk within hours of birth and\ncan quickly improve with practice. Such architectural priors can also be useful\nin ANN architectures for AI. In this work, we explore the advantages of a\nbiologically inspired ANN architecture for quadruped locomotion based on neural\ncircuits in the limbs and spinal cord of mammals. Our architecture achieves\ngood initial performance and comparable final performance to MLPs, while using\nless data and orders of magnitude fewer parameters. Our architecture also\nexhibits better generalization to task variations, even admitting deployment on\na physical robot without standard sim-to-real methods. This work shows that\nneural circuits can provide valuable architectural priors for locomotion and\nencourages future work in other sensorimotor skills.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.RO"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07174v1",
    "published_date": "2024-10-09 17:59:45 UTC",
    "updated_date": "2024-10-09 17:59:45 UTC"
  },
  {
    "arxiv_id": "2410.07173v2",
    "title": "Better Language Models Exhibit Higher Visual Alignment",
    "authors": [
      "Jona Ruthardt",
      "Gertjan J. Burghouts",
      "Serge Belongie",
      "Yuki M. Asano"
    ],
    "abstract": "How well do text-only Large Language Models (LLMs) naturally align with the\nvisual world? We provide the first direct analysis by utilizing frozen text\nrepresentations in a discriminative vision-language model framework and\nmeasuring zero-shot generalization on unseen classes. We find decoder-based\nLLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs\nreliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs\nleads to strong gains in cross-lingual settings, where our approach surpasses\nCLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves\nboth robustness and generalization and also significantly reduces the need for\npaired data and compute, making vision-language models more accessible and\nadaptable.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07173v2",
    "published_date": "2024-10-09 17:59:33 UTC",
    "updated_date": "2025-02-17 13:25:17 UTC"
  },
  {
    "arxiv_id": "2410.07170v3",
    "title": "One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation",
    "authors": [
      "Fabian Paischer",
      "Lukas Hauzenberger",
      "Thomas Schmied",
      "Benedikt Alkin",
      "Marc Peter Deisenroth",
      "Sepp Hochreiter"
    ],
    "abstract": "Foundation models (FMs) are pre-trained on large-scale datasets and then\nfine-tuned on a downstream task for a specific application. The most successful\nand most commonly used fine-tuning method is to update the pre-trained weights\nvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are\nusually initialized at random with a uniform rank distribution across the model\nweights. Recent works focus on different initialization schemes or the learning\nof adaptive ranks during fine-tuning. Both approaches have only been\ninvestigated in isolation, resulting in slow convergence or a uniform rank\ndistribution, in turn leading to suboptimal performance. We propose to improve\nLoRA by initializing the new weights in a data-driven manner by computing\nsingular value decomposition (SVD) on minibatches of activation vectors. Then,\nwe initialize the LoRA matrices with the obtained right-singular vectors and\nredistribute ranks among all weight matrices to provably store the maximum\namount of information of the downstream data in the newly introduced weights.\nIn this way, only what information to maintain or neglect during the\nfine-tuning process needs to be learned. We call our new method\n$\\textbf{E}$xplained $\\textbf{V}$ariance $\\textbf{A}$daptation (EVA). We apply\nEVA to a variety of fine-tuning tasks ranging from language generation and\nunderstanding to image classification and reinforcement learning. EVA exhibits\nfaster convergence than competitors and achieves the highest average score\nacross a multitude of tasks per domain while reducing the number of trainable\nparameters through rank redistribution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages + references and appendix, code available at\n  https://github.com/ml-jku/EVA",
    "pdf_url": "http://arxiv.org/pdf/2410.07170v3",
    "published_date": "2024-10-09 17:59:06 UTC",
    "updated_date": "2024-12-16 19:19:14 UTC"
  },
  {
    "arxiv_id": "2410.07166v3",
    "title": "Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making",
    "authors": [
      "Manling Li",
      "Shiyu Zhao",
      "Qineng Wang",
      "Kangrui Wang",
      "Yu Zhou",
      "Sanjana Srivastava",
      "Cem Gokmen",
      "Tony Lee",
      "Li Erran Li",
      "Ruohan Zhang",
      "Weiyu Liu",
      "Percy Liang",
      "Li Fei-Fei",
      "Jiayuan Mao",
      "Jiajun Wu"
    ],
    "abstract": "We aim to evaluate Large Language Models (LLMs) for embodied decision making.\nWhile a significant body of work has been leveraging LLMs for decision making\nin embodied environments, we still lack a systematic understanding of their\nperformance because they are usually applied in different domains, for\ndifferent purposes, and built based on different inputs and outputs.\nFurthermore, existing evaluations tend to rely solely on a final success rate,\nmaking it difficult to pinpoint what ability is missing in LLMs and where the\nproblem lies, which in turn blocks embodied agents from leveraging LLMs\neffectively and selectively. To address these limitations, we propose a\ngeneralized interface (Embodied Agent Interface) that supports the\nformalization of various types of tasks and input-output specifications of\nLLM-based modules. Specifically, it allows us to unify 1) a broad set of\nembodied decision-making tasks involving both state and temporally extended\ngoals, 2) four commonly-used LLM-based modules for decision making: goal\ninterpretation, subgoal decomposition, action sequencing, and transition\nmodeling, and 3) a collection of fine-grained metrics which break down\nevaluation into various types of errors, such as hallucination errors,\naffordance errors, various types of planning errors, etc. Overall, our\nbenchmark offers a comprehensive assessment of LLMs' performance for different\nsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI\nsystems, and providing insights for effective and selective use of LLMs in\nembodied decision making.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for oral presentation at NeurIPS 2024 in the Datasets and\n  Benchmarks track. Final Camera version",
    "pdf_url": "http://arxiv.org/pdf/2410.07166v3",
    "published_date": "2024-10-09 17:59:00 UTC",
    "updated_date": "2025-01-19 19:29:50 UTC"
  },
  {
    "arxiv_id": "2410.07163v3",
    "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning",
    "authors": [
      "Chongyu Fan",
      "Jiancheng Liu",
      "Licong Lin",
      "Jinghan Jia",
      "Ruiqi Zhang",
      "Song Mei",
      "Sijia Liu"
    ],
    "abstract": "This work studies the problem of large language model (LLM) unlearning,\naiming to remove unwanted data influences (e.g., copyrighted or harmful\ncontent) while preserving model utility. Despite the increasing demand for\nunlearning, a technically-grounded optimization framework is lacking. Gradient\nascent (GA)-type methods, though widely used, are suboptimal as they reverse\nthe learning process without controlling optimization divergence (i.e.,\ndeviation from the pre-trained state), leading to risks of over-forgetting and\npotential model collapse. Negative preference optimization (NPO) has been\nproposed to address this issue and is considered one of the state-of-the-art\nLLM unlearning approaches. In this work, we revisit NPO and identify another\ncritical issue: reference model bias. This bias arises from using the reference\nmodel (i.e., the model prior to unlearning) to evaluate the unlearning success,\nwhich can compromise NPO's effectiveness. Specifically, it leads to (a) uneven\nallocation of optimization power across forget data with varying difficulty\nlevels and (b) ineffective gradient weight smoothing during the early stages of\nunlearning optimization. To overcome these challenges, we propose a simple yet\neffective unlearning optimization framework, called SimNPO, showing that\n`simplicity' in removing the reliance on a reference model (through the lens of\nsimple preference optimization) benefits unlearning. We provide deeper insights\ninto SimNPO's advantages through an analysis based on mixtures of Markov\nchains. Extensive experiments further validate SimNPO's efficacy on benchmarks\nlike TOFU and MUSE, as well as its robustness against relearning attacks. Codes\nare available at https://github.com/OPTML-Group/Unlearn-Simple.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07163v3",
    "published_date": "2024-10-09 17:58:12 UTC",
    "updated_date": "2025-02-07 18:34:28 UTC"
  },
  {
    "arxiv_id": "2410.07305v1",
    "title": "A Blockchain and Artificial Intelligence based System for Halal Food Traceability",
    "authors": [
      "Abdulla Alourani",
      "Shahnawaz Khan"
    ],
    "abstract": "The demand of the halal food products is increasing rapidly around the world.\nThe consumption of halal food product is just not among the Muslims but also\namong non-Muslims, due to the purity of the halal food products. However, there\nare several challenges that are faced by the halal food consumers. The\nchallenges raise a doubt among the halal food consumers about the authenticity\nof the product being halal. Therefore, a solution that can address these issues\nand can establish trust between consumers and producers. Blockchain technology\ncan provide a distributed ledger of an immutable record of the information.\nArtificial intelligence supports developing a solution for pattern\nidentification. The proposed research utilizes blockchain an artificial\nintelligence-based system for developing a system that ensure the authenticity\nof the halal food products by providing the traceability related to all the\noperations and processes of the supply chain and sourcing the raw material. The\nproposed system has been tested with a local supermarket. The results and tests\nof the developed solution seemed effective and the testers expressed interest\nin real-world implementation of the proposed system.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.07305v1",
    "published_date": "2024-10-09 17:57:01 UTC",
    "updated_date": "2024-10-09 17:57:01 UTC"
  },
  {
    "arxiv_id": "2410.07158v2",
    "title": "Quanda: An Interpretability Toolkit for Training Data Attribution Evaluation and Beyond",
    "authors": [
      "Dilyara Bareeva",
      "Galip Ümit Yolcu",
      "Anna Hedström",
      "Niklas Schmolenski",
      "Thomas Wiegand",
      "Wojciech Samek",
      "Sebastian Lapuschkin"
    ],
    "abstract": "In recent years, training data attribution (TDA) methods have emerged as a\npromising direction for the interpretability of neural networks. While research\naround TDA is thriving, limited effort has been dedicated to the evaluation of\nattributions. Similar to the development of evaluation metrics for traditional\nfeature attribution approaches, several standalone metrics have been proposed\nto evaluate the quality of TDA methods across various contexts. However, the\nlack of a unified framework that allows for systematic comparison limits trust\nin TDA methods and stunts their widespread adoption. To address this research\ngap, we introduce Quanda, a Python toolkit designed to facilitate the\nevaluation of TDA methods. Beyond offering a comprehensive set of evaluation\nmetrics, Quanda provides a uniform interface for seamless integration with\nexisting TDA implementations across different repositories, thus enabling\nsystematic benchmarking. The toolkit is user-friendly, thoroughly tested,\nwell-documented, and available as an open-source library on PyPi and under\nhttps://github.com/dilyabareeva/quanda.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07158v2",
    "published_date": "2024-10-09 17:56:41 UTC",
    "updated_date": "2024-10-10 16:36:19 UTC"
  },
  {
    "arxiv_id": "2410.07157v1",
    "title": "InstructG2I: Synthesizing Images from Multimodal Attributed Graphs",
    "authors": [
      "Bowen Jin",
      "Ziqi Pang",
      "Bingjun Guo",
      "Yu-Xiong Wang",
      "Jiaxuan You",
      "Jiawei Han"
    ],
    "abstract": "In this paper, we approach an overlooked yet critical task Graph2Image:\ngenerating images from multimodal attributed graphs (MMAGs). This task poses\nsignificant challenges due to the explosion in graph size, dependencies among\ngraph entities, and the need for controllability in graph conditions. To\naddress these challenges, we propose a graph context-conditioned diffusion\nmodel called InstructG2I. InstructG2I first exploits the graph structure and\nmultimodal information to conduct informative neighbor sampling by combining\npersonalized page rank and re-ranking based on vision-language features. Then,\na Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary\nset of graph prompts to guide the denoising process of diffusion. Finally, we\npropose graph classifier-free guidance, enabling controllable generation by\nvarying the strength of graph guidance and multiple connected edges to a node.\nExtensive experiments conducted on three datasets from different domains\ndemonstrate the effectiveness and controllability of our approach. The code is\navailable at https://github.com/PeterGriffinJin/InstructG2I.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.07157v1",
    "published_date": "2024-10-09 17:56:15 UTC",
    "updated_date": "2024-10-09 17:56:15 UTC"
  },
  {
    "arxiv_id": "2410.07147v1",
    "title": "Taking a turn for the better: Conversation redirection throughout the course of mental-health therapy",
    "authors": [
      "Vivian Nguyen",
      "Sang Min Jung",
      "Lillian Lee",
      "Thomas D. Hull",
      "Cristian Danescu-Niculescu-Mizil"
    ],
    "abstract": "Mental-health therapy involves a complex conversation flow in which patients\nand therapists continuously negotiate what should be talked about next. For\nexample, therapists might try to shift the conversation's direction to keep the\ntherapeutic process on track and avoid stagnation, or patients might push the\ndiscussion towards issues they want to focus on.\n  How do such patient and therapist redirections relate to the development and\nquality of their relationship? To answer this question, we introduce a\nprobabilistic measure of the extent to which a certain utterance immediately\nredirects the flow of the conversation, accounting for both the intention and\nthe actual realization of such a change. We apply this new measure to\ncharacterize the development of patient-therapist relationships over multiple\nsessions in a very large, widely-used online therapy platform. Our analysis\nreveals that (1) patient control of the conversation's direction generally\nincreases relative to that of the therapist as their relationship progresses;\nand (2) patients who have less control in the first few sessions are\nsignificantly more likely to eventually express dissatisfaction with their\ntherapist and terminate the relationship.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in the Proceedings of EMNLP (Findings) 2024. Code available\n  at https://convokit.cornell.edu",
    "pdf_url": "http://arxiv.org/pdf/2410.07147v1",
    "published_date": "2024-10-09 17:54:41 UTC",
    "updated_date": "2024-10-09 17:54:41 UTC"
  },
  {
    "arxiv_id": "2410.07145v1",
    "title": "Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling",
    "authors": [
      "Yingfa Chen",
      "Xinrong Zhang",
      "Shengding Hu",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "One essential advantage of recurrent neural networks (RNNs) over\ntransformer-based language models is their linear computational complexity\nconcerning the sequence length, which makes them much faster in handling long\nsequences during inference. However, most publicly available RNNs (e.g., Mamba\nand RWKV) are trained on sequences with less than 10K tokens, and their\neffectiveness in longer contexts remains largely unsatisfying so far. In this\npaper, we study the cause of the inability to process long context for RNNs and\nsuggest critical mitigations. We examine two practical concerns when applying\nstate-of-the-art RNNs to long contexts: (1) the inability to extrapolate to\ninputs longer than the training length and (2) the upper bound of memory\ncapacity. Addressing the first concern, we first investigate *state collapse*\n(SC), a phenomenon that causes severe performance degradation on sequence\nlengths not encountered during training. With controlled experiments, we\nattribute this to overfitting due to the recurrent state being\noverparameterized for the training length. For the second concern, we train a\nseries of Mamba-2 models on long documents to empirically estimate the\nrecurrent state capacity in language modeling and passkey retrieval. Then,\nthree SC mitigation methods are proposed to improve Mamba-2's length\ngeneralizability, allowing the model to process more than 1M tokens without SC.\nWe also find that the recurrent state capacity in passkey retrieval scales\nexponentially to the state size, and we empirically train a Mamba-2 370M with\nnear-perfect passkey retrieval accuracy on 256K context length. This suggests a\npromising future for RNN-based long-context modeling.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.07145v1",
    "published_date": "2024-10-09 17:54:28 UTC",
    "updated_date": "2024-10-09 17:54:28 UTC"
  },
  {
    "arxiv_id": "2410.07137v2",
    "title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates",
    "authors": [
      "Xiaosen Zheng",
      "Tianyu Pang",
      "Chao Du",
      "Qian Liu",
      "Jing Jiang",
      "Min Lin"
    ],
    "abstract": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2410.07137v2",
    "published_date": "2024-10-09 17:53:06 UTC",
    "updated_date": "2025-03-02 14:28:33 UTC"
  },
  {
    "arxiv_id": "2410.07304v1",
    "title": "The Moral Turing Test: Evaluating Human-LLM Alignment in Moral Decision-Making",
    "authors": [
      "Basile Garcia",
      "Crystal Qian",
      "Stefano Palminteri"
    ],
    "abstract": "As large language models (LLMs) become increasingly integrated into society,\ntheir alignment with human morals is crucial. To better understand this\nalignment, we created a large corpus of human- and LLM-generated responses to\nvarious moral scenarios. We found a misalignment between human and LLM moral\nassessments; although both LLMs and humans tended to reject morally complex\nutilitarian dilemmas, LLMs were more sensitive to personal framing. We then\nconducted a quantitative user study involving 230 participants (N=230), who\nevaluated these responses by determining whether they were AI-generated and\nassessed their agreement with the responses. Human evaluators preferred LLMs'\nassessments in moral scenarios, though a systematic anti-AI bias was observed:\nparticipants were less likely to agree with judgments they believed to be\nmachine-generated. Statistical and NLP-based analyses revealed subtle\nlinguistic differences in responses, influencing detection and agreement.\nOverall, our findings highlight the complexities of human-AI perception in\nmorally charged decision-making.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07304v1",
    "published_date": "2024-10-09 17:52:00 UTC",
    "updated_date": "2024-10-09 17:52:00 UTC"
  },
  {
    "arxiv_id": "2410.07129v2",
    "title": "Mental Disorders Detection in the Era of Large Language Models",
    "authors": [
      "Gleb Kuzmin",
      "Petr Strepetov",
      "Maksim Stankevich",
      "Artem Shelmanov",
      "Ivan Smirnov"
    ],
    "abstract": "This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07129v2",
    "published_date": "2024-10-09 17:51:55 UTC",
    "updated_date": "2024-10-16 10:14:54 UTC"
  },
  {
    "arxiv_id": "2410.07119v1",
    "title": "Thing2Reality: Transforming 2D Content into Conditioned Multiviews and 3D Gaussian Objects for XR Communication",
    "authors": [
      "Erzhen Hu",
      "Mingyi Li",
      "Jungtaek Hong",
      "Xun Qian",
      "Alex Olwal",
      "David Kim",
      "Seongkook Heo",
      "Ruofei Du"
    ],
    "abstract": "During remote communication, participants often share both digital and\nphysical content, such as product designs, digital assets, and environments, to\nenhance mutual understanding. Recent advances in augmented communication have\nfacilitated users to swiftly create and share digital 2D copies of physical\nobjects from video feeds into a shared space. However, conventional 2D\nrepresentations of digital objects restricts users' ability to spatially\nreference items in a shared immersive environment. To address this, we propose\nThing2Reality, an Extended Reality (XR) communication platform that enhances\nspontaneous discussions of both digital and physical items during remote\nsessions. With Thing2Reality, users can quickly materialize ideas or physical\nobjects in immersive environments and share them as conditioned multiview\nrenderings or 3D Gaussians. Thing2Reality enables users to interact with remote\nobjects or discuss concepts in a collaborative manner. Our user study revealed\nthat the ability to interact with and manipulate 3D representations of objects\nsignificantly enhances the efficiency of discussions, with the potential to\naugment discussion of 2D artifacts.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "18 pages (15 pages without references), 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.07119v1",
    "published_date": "2024-10-09 17:49:06 UTC",
    "updated_date": "2024-10-09 17:49:06 UTC"
  },
  {
    "arxiv_id": "2410.07112v2",
    "title": "VHELM: A Holistic Evaluation of Vision Language Models",
    "authors": [
      "Tony Lee",
      "Haoqin Tu",
      "Chi Heem Wong",
      "Wenhao Zheng",
      "Yiyang Zhou",
      "Yifan Mai",
      "Josselin Somerville Roberts",
      "Michihiro Yasunaga",
      "Huaxiu Yao",
      "Cihang Xie",
      "Percy Liang"
    ],
    "abstract": "Current benchmarks for assessing vision-language models (VLMs) often focus on\ntheir perception or problem-solving capabilities and neglect other critical\naspects such as fairness, multilinguality, or toxicity. Furthermore, they\ndiffer in their evaluation procedures and the scope of the evaluation, making\nit difficult to compare models. To address these issues, we extend the HELM\nframework to VLMs to present the Holistic Evaluation of Vision Language Models\n(VHELM). VHELM aggregates various datasets to cover one or more of the 9\naspects: visual perception, knowledge, reasoning, bias, fairness,\nmultilinguality, robustness, toxicity, and safety. In doing so, we produce a\ncomprehensive, multi-dimensional view of the capabilities of the VLMs across\nthese important factors. In addition, we standardize the standard inference\nparameters, methods of prompting, and evaluation metrics to enable fair\ncomparisons across models. Our framework is designed to be lightweight and\nautomatic so that evaluation runs are cheap and fast. Our initial run evaluates\n22 VLMs on 21 existing datasets to provide a holistic snapshot of the models.\nWe uncover new key findings, such as the fact that efficiency-focused models\n(e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than\ntheir full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark\nbut not when evaluated on the other aspects. For transparency, we release the\nraw model generations and complete results on our website\n(https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living\nbenchmark, and we hope to continue adding new datasets and models over time.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024. First three authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2410.07112v2",
    "published_date": "2024-10-09 17:46:34 UTC",
    "updated_date": "2024-10-24 05:17:36 UTC"
  },
  {
    "arxiv_id": "2410.07109v2",
    "title": "I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in Multi-Agent Settings with Social Hierarchy",
    "authors": [
      "Gian Maria Campedelli",
      "Nicolò Penzo",
      "Massimo Stefan",
      "Roberto Dessì",
      "Marco Guerini",
      "Bruno Lepri",
      "Jacopo Staiano"
    ],
    "abstract": "As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07109v2",
    "published_date": "2024-10-09 17:45:47 UTC",
    "updated_date": "2024-10-16 08:06:22 UTC"
  },
  {
    "arxiv_id": "2410.07302v1",
    "title": "Examining the Prevalence and Dynamics of AI-Generated Media in Art Subreddits",
    "authors": [
      "Hana Matatov",
      "Marianne Aubin Le Quéré",
      "Ofra Amir",
      "Mor Naaman"
    ],
    "abstract": "Broadly accessible generative AI models like Dall-E have made it possible for\nanyone to create compelling visual art. In online communities, the introduction\nof AI-generated content (AIGC) may impact community dynamics by shifting the\nkinds of content being posted or the responses to content suspected of being\ngenerated by AI. We take steps towards examining the potential impact of AIGC\non art-related communities on Reddit. We distinguish between communities that\ndisallow AI content and those without a direct policy. We look at image-based\nposts made to these communities that are transparently created by AI, or\ncomments in these communities that suspect authors of using generative AI. We\nfind that AI posts (and accusations) have played a very small part in these\ncommunities through the end of 2023, accounting for fewer than 0.2% of the\nimage-based posts. Even as the absolute number of author-labelled AI posts\ndwindles over time, accusations of AI use remain more persistent. We show that\nAI content is more readily used by newcomers and may help increase\nparticipation if it aligns with community rules. However, the tone of comments\nsuspecting AI use by others have become more negative over time, especially in\ncommunities that do not have explicit rules about AI. Overall, the results show\nthe changing norms and interactions around AIGC in online communities\ndesignated for creativity.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07302v1",
    "published_date": "2024-10-09 17:41:13 UTC",
    "updated_date": "2024-10-09 17:41:13 UTC"
  },
  {
    "arxiv_id": "2410.07096v7",
    "title": "Rejecting Hallucinated State Targets during Planning",
    "authors": [
      "Mingde Zhao",
      "Tristan Sylvain",
      "Romain Laroche",
      "Doina Precup",
      "Yoshua Bengio"
    ],
    "abstract": "Generative models can be used in planning to propose targets corresponding to\nstates that agents deem either likely or advantageous to experience. However,\nimperfections, common in learned models, lead to infeasible hallucinated\ntargets, which can cause delusional behaviors and thus safety concerns. This\nwork first categorizes and investigates the properties of various kinds of\ninfeasible targets. Then, we devise a strategy to reject infeasible targets\nwith a generic target evaluator, which trains alongside planning agents as an\nadd-on without the need to change the behavior nor the architectures of the\nagent (and the generative model) it is attached to. We highlight that, without\nproper training, the evaluator can produce delusional estimates, rendering the\nstrategy futile. Thus, to learn correct evaluations of infeasible targets, we\npropose to use a combination of learning rule, architecture, and two assistive\nhindsight relabeling strategies. Our experiments validate significant\nreductions in delusional behaviors and enhancements in the performance of\nseveral kinds of existing planning agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "[20250511]: ICML 2025 Camera Ready,\n  https://github.com/mila-iqia/delusions",
    "pdf_url": "http://arxiv.org/pdf/2410.07096v7",
    "published_date": "2024-10-09 17:35:25 UTC",
    "updated_date": "2025-05-11 20:13:14 UTC"
  },
  {
    "arxiv_id": "2410.07094v1",
    "title": "An Approach for Auto Generation of Labeling Functions for Software Engineering Chatbots",
    "authors": [
      "Ebube Alor",
      "Ahmad Abdellatif",
      "SayedHassan Khatoonabadi",
      "Emad Shihab"
    ],
    "abstract": "Software engineering (SE) chatbots are increasingly gaining attention for\ntheir role in enhancing development processes. At the core of chatbots are the\nNatural Language Understanding platforms (NLUs), which enable them to\ncomprehend and respond to user queries. Before deploying NLUs, there is a need\nto train them with labeled data. However, acquiring such labeled data for SE\nchatbots is challenging due to the scarcity of high-quality datasets. This\nchallenge arises because training SE chatbots requires specialized vocabulary\nand phrases not found in typical language datasets. Consequently, chatbot\ndevelopers often resort to manually annotating user queries to gather the data\nnecessary for training effective chatbots, a process that is both\ntime-consuming and resource-intensive. Previous studies propose approaches to\nsupport chatbot practitioners in annotating users' posed queries. However,\nthese approaches require human intervention to generate rules, called labeling\nfunctions (LFs), that identify and categorize user queries based on specific\npatterns in the data. To address this issue, we propose an approach to\nautomatically generate LFs by extracting patterns from labeled user queries. We\nevaluate the effectiveness of our approach by applying it to the queries of\nfour diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow)\nand measure the performance improvement gained from training the NLU on the\nqueries labeled by the generated LFs. We find that the generated LFs\neffectively label data with AUC scores of up to 85.3%, and NLU's performance\nimprovement of up to 27.2% across the studied datasets. Furthermore, our\nresults show that the number of LFs used to generate LFs affects the labeling\nperformance. We believe that our approach can save time and resources in\nlabeling users' queries, allowing practitioners to focus on core chatbot\nfunctionalities.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Submitted to IEEE Transactions on Software Engineering for review",
    "pdf_url": "http://arxiv.org/pdf/2410.07094v1",
    "published_date": "2024-10-09 17:34:14 UTC",
    "updated_date": "2024-10-09 17:34:14 UTC"
  },
  {
    "arxiv_id": "2410.12841v2",
    "title": "UniAutoML: A Human-Centered Framework for Unified Discriminative and Generative AutoML with Large Language Models",
    "authors": [
      "Jiayi Guo",
      "Zan Chen",
      "Yingrui Ji",
      "Liyun Zhang",
      "Daqin Luo",
      "Zhigang Li",
      "Yiqin Shen"
    ],
    "abstract": "Automated Machine Learning (AutoML) has simplified complex ML processes such\nas data pre-processing, model selection, and hyper-parameter searching.\nHowever, traditional AutoML frameworks focus solely on discriminative tasks,\noften falling short in tackling AutoML for generative models. Additionally,\nthese frameworks lack interpretability and user engagement during the training\nprocess, primarily due to the absence of human-centered design. It leads to a\nlack of transparency in final decision-making and limited user control,\npotentially reducing trust and adoption of AutoML methods. To address these\nlimitations, we introduce UniAutoML, a human-centered AutoML framework that\nleverages Large Language Models (LLMs) to unify AutoML for both discriminative\n(e.g., Transformers and CNNs for classification or regression tasks) and\ngenerative tasks (e.g., fine-tuning diffusion models or LLMs). The\nhuman-centered design of UniAutoML innovatively features a conversational user\ninterface (CUI) that facilitates natural language interactions, providing users\nwith real-time guidance, feedback, and progress updates for better\ninterpretability. This design enhances transparency and user control throughout\nthe AutoML training process, allowing users to seamlessly break down or modify\nthe model being trained. To mitigate potential risks associated with LLM\ngenerated content, UniAutoML incorporates a safety guardline that filters\ninputs and censors outputs. We evaluated UniAutoML's performance and usability\nthrough experiments on eight diverse datasets and user studies involving 25\nparticipants, demonstrating that UniAutoML not only enhances performance but\nalso improves user control and trust. Our human-centered design bridges the gap\nbetween AutoML capabilities and user understanding, making ML more accessible\nto a broader audience.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.12841v2",
    "published_date": "2024-10-09 17:33:15 UTC",
    "updated_date": "2024-10-18 03:03:01 UTC"
  },
  {
    "arxiv_id": "2410.07076v5",
    "title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
    "authors": [
      "Zonglin Yang",
      "Wanhao Liu",
      "Ben Gao",
      "Tong Xie",
      "Yuqiang Li",
      "Wanli Ouyang",
      "Soujanya Poria",
      "Erik Cambria",
      "Dongzhan Zhou"
    ],
    "abstract": "Scientific discovery plays a pivotal role in advancing human society, and\nrecent progress in large language models (LLMs) suggests their potential to\naccelerate this process. However, it remains unclear whether LLMs can\nautonomously generate novel and valid hypotheses in chemistry. In this work, we\ninvestigate whether LLMs can discover high-quality chemistry hypotheses given\nonly a research background-comprising a question and/or a survey-without\nrestriction on the domain of the question. We begin with the observation that\nhypothesis discovery is a seemingly intractable task. To address this, we\npropose a formal mathematical decomposition grounded in a fundamental\nassumption: that most chemistry hypotheses can be composed from a research\nbackground and a set of inspirations. This decomposition leads to three\npractical subtasks-retrieving inspirations, composing hypotheses with\ninspirations, and ranking hypotheses - which together constitute a sufficient\nset of subtasks for the overall scientific discovery task. We further develop\nan agentic LLM framework, MOOSE-Chem, that is a direct implementation of this\nmathematical decomposition. To evaluate this framework, we construct a\nbenchmark of 51 high-impact chemistry papers published and online after January\n2024, each manually annotated by PhD chemists with background, inspirations,\nand hypothesis. The framework is able to rediscover many hypotheses with high\nsimilarity to the groundtruth, successfully capturing the core\ninnovations-while ensuring no data contamination since it uses an LLM with\nknowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high\naccuracy on inspiration retrieval, a task with inherently out-of-distribution\nnature, we propose a bold assumption: that LLMs may already encode latent\nscientific knowledge associations not yet recognized by humans.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.07076v5",
    "published_date": "2024-10-09 17:19:58 UTC",
    "updated_date": "2025-05-18 13:23:30 UTC"
  },
  {
    "arxiv_id": "2410.07071v2",
    "title": "Retrieval-Augmented Decision Transformer: External Memory for In-context RL",
    "authors": [
      "Thomas Schmied",
      "Fabian Paischer",
      "Vihang Patil",
      "Markus Hofmarcher",
      "Razvan Pascanu",
      "Sepp Hochreiter"
    ],
    "abstract": "In-context learning (ICL) is the ability of a model to learn a new task by\nobserving a few exemplars in its context. While prevalent in NLP, this\ncapability has recently also been observed in Reinforcement Learning (RL)\nsettings. Prior in-context RL methods, however, require entire episodes in the\nagent's context. Given that complex environments typically lead to long\nepisodes with sparse rewards, these methods are constrained to simple\nenvironments with short episodes. To address these challenges, we introduce\nRetrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external\nmemory mechanism to store past experiences from which it retrieves only\nsub-trajectories relevant for the current situation. The retrieval component in\nRA-DT does not require training and can be entirely domain-agnostic. We\nevaluate the capabilities of RA-DT on grid-world environments, robotics\nsimulations, and procedurally-generated video games. On grid-worlds, RA-DT\noutperforms baselines, while using only a fraction of their context length.\nFurthermore, we illuminate the limitations of current in-context RL methods on\ncomplex environments and discuss future directions. To facilitate future\nresearch, we release datasets for four of the considered environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07071v2",
    "published_date": "2024-10-09 17:15:30 UTC",
    "updated_date": "2024-12-07 10:31:51 UTC"
  },
  {
    "arxiv_id": "2410.07069v1",
    "title": "ReIFE: Re-evaluating Instruction-Following Evaluation",
    "authors": [
      "Yixin Liu",
      "Kejian Shi",
      "Alexander R. Fabbri",
      "Yilun Zhao",
      "Peifeng Wang",
      "Chien-Sheng Wu",
      "Shafiq Joty",
      "Arman Cohan"
    ],
    "abstract": "The automatic evaluation of instruction following typically involves using\nlarge language models (LLMs) to assess response quality. However, there is a\nlack of comprehensive evaluation of these LLM-based evaluators across two\ndimensions: the base LLMs and the evaluation protocols. Therefore, we present a\nthorough meta-evaluation of instruction following, including 25 base LLMs and\n15 recently proposed evaluation protocols, on 4 human-annotated datasets,\nassessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows\nus to identify the best-performing base LLMs and evaluation protocols with a\nhigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)\nBase LLM performance ranking remains largely consistent across evaluation\nprotocols, with less capable LLMs showing greater improvement from protocol\nenhancements; (2) Robust evaluation of evaluation protocols requires many base\nLLMs with varying capability levels, as protocol effectiveness can depend on\nthe base LLM used; (3) Evaluation results on different datasets are not always\nconsistent, so a rigorous evaluation requires multiple datasets with\ndistinctive features. We release our meta-evaluation suite ReIFE, which\nprovides the codebase and evaluation result collection for more than 500\nLLM-evaluator configurations, to support future research in\ninstruction-following evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "GitHub Repo: https://github.com/yale-nlp/ReIFE, Evaluation Result\n  Collection: https://huggingface.co/datasets/yale-nlp/ReIFE",
    "pdf_url": "http://arxiv.org/pdf/2410.07069v1",
    "published_date": "2024-10-09 17:14:50 UTC",
    "updated_date": "2024-10-09 17:14:50 UTC"
  },
  {
    "arxiv_id": "2410.07299v2",
    "title": "Towards Generalisable Time Series Understanding Across Domains",
    "authors": [
      "Özgün Turgut",
      "Philip Müller",
      "Martin J. Menten",
      "Daniel Rueckert"
    ],
    "abstract": "Recent breakthroughs in natural language processing and computer vision,\ndriven by efficient pre-training on large datasets, have enabled foundation\nmodels to excel on a wide range of tasks. However, this potential has not yet\nbeen fully realised in time series analysis, as existing methods fail to\naddress the heterogeneity in large time series corpora. Prevalent in domains\nranging from medicine to finance, time series vary substantially in\ncharacteristics such as variate count, inter-variate relationships, temporal\npatterns, and sampling frequency. To address this, we introduce a novel\npre-training paradigm specifically designed to handle time series\nheterogeneity. We propose a tokeniser with learnable domain signatures, a dual\nmasking strategy, and a normalised cross-correlation loss, enabling our open\nmodel for general time series analysis (OTiS) to efficiently learn from large\ntime series corpora. Extensive benchmarking on diverse tasks, such as\nclassification, regression, and forecasting, demonstrates that OTiS outperforms\nstate-of-the-art baselines. Our code and pre-trained weights are available at\nhttps://github.com/oetu/otis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07299v2",
    "published_date": "2024-10-09 17:09:30 UTC",
    "updated_date": "2025-01-31 14:50:26 UTC"
  },
  {
    "arxiv_id": "2410.12839v1",
    "title": "Capturing Bias Diversity in LLMs",
    "authors": [
      "Purva Prasad Gosavi",
      "Vaishnavi Murlidhar Kulkarni",
      "Alan F. Smeaton"
    ],
    "abstract": "This paper presents research on enhancements to Large Language Models (LLMs)\nthrough the addition of diversity in its generated outputs. Our study\nintroduces a configuration of multiple LLMs which demonstrates the diversities\ncapable with a single LLM. By developing multiple customised instances of a GPT\nmodel, each reflecting biases in specific demographic characteristics including\ngender, age, and race, we propose, develop and evaluate a framework for a more\nnuanced and representative AI dialogue which we call BiasGPT. The customised\nGPT models will ultimately collaborate, merging their diverse perspectives on a\ntopic into an integrated response that captures a broad spectrum of human\nexperiences and viewpoints. In this paper, through experiments, we demonstrate\nthe capabilities of a GPT model to embed different biases which, when combined,\ncan open the possibilities of more inclusive AI technologies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "2nd International Conference on Foundation and Large Language Models\n  (FLLM2024), 26-29 November, 2024 | Dubai, UAE",
    "pdf_url": "http://arxiv.org/pdf/2410.12839v1",
    "published_date": "2024-10-09 17:07:50 UTC",
    "updated_date": "2024-10-09 17:07:50 UTC"
  },
  {
    "arxiv_id": "2410.07298v3",
    "title": "Enhancing Performance of Point Cloud Completion Networks with Consistency Loss",
    "authors": [
      "Kevin Tirta Wijaya",
      "Christofel Rio Goenawan",
      "Seung-Hyun Kong"
    ],
    "abstract": "Point cloud completion networks are conventionally trained to minimize the\ndisparities between the completed point cloud and the ground-truth counterpart.\nHowever, an incomplete object-level point cloud can have multiple valid\ncompletion solutions when it is examined in isolation. This one-to-many mapping\nissue can cause contradictory supervision signals to the network because the\nloss function may produce different values for identical input-output pairs of\nthe network. In many cases, this issue could adversely affect the network\noptimization process. In this work, we propose to enhance the conventional\nlearning objective using a novel completion consistency loss to mitigate the\none-to-many mapping problem. Specifically, the proposed consistency loss ensure\nthat a point cloud completion network generates a coherent completion solution\nfor incomplete objects originating from the same source point cloud.\nExperimental results across multiple well-established datasets and benchmarks\ndemonstrated the proposed completion consistency loss have excellent capability\nto enhance the completion performance of various existing networks without any\nmodification to the design of the networks. The proposed consistency loss\nenhances the performance of the point completion network without affecting the\ninference speed, thereby increasing the accuracy of point cloud completion.\nNotably, a state-of-the-art point completion network trained with the proposed\nconsistency loss can achieve state-of-the-art accuracy on the challenging new\nMVP dataset. The code and result of experiment various point completion models\nusing proposed consistency loss will be available at:\nhttps://github.com/kaist-avelab/ConsistencyLoss .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "First version of Paper \"Enhancing Performance of Point Cloud\n  Completion Networks with Consistency Loss\" by Kevin Tirta Wijaya and\n  Christofel Rio Goenawan. In process submission to Neurocomputing Journal 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.07298v3",
    "published_date": "2024-10-09 17:07:34 UTC",
    "updated_date": "2025-01-14 21:26:13 UTC"
  },
  {
    "arxiv_id": "2410.18251v1",
    "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs",
    "authors": [
      "Iman Saberi",
      "Fatemeh Fard"
    ],
    "abstract": "Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly\nimproved code generation, but, they frequently face difficulties when dealing\nwith challenging and complex problems. Retrieval-Augmented Generation (RAG)\naddresses this issue by retrieving and integrating external knowledge at the\ninference time. However, retrieval models often fail to find most relevant\ncontext, and generation models, with limited context capacity, can hallucinate\nwhen given irrelevant data. We present a novel framework that leverages a\nProgramming Knowledge Graph (PKG) to semantically represent and retrieve code.\nThis approach enables fine-grained code retrieval by focusing on the most\nrelevant segments while reducing irrelevant context through a tree-pruning\ntechnique. PKG is coupled with a re-ranking mechanism to reduce even more\nhallucinations by selectively integrating non-RAG solutions. We propose two\nretrieval approaches-block-wise and function-wise-based on the PKG, optimizing\ncontext granularity. Evaluations on the HumanEval and MBPP benchmarks show our\nmethod improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art\nmodels by up to 34% on MBPP. Our contributions include PKG-based retrieval,\ntree pruning to enhance retrieval precision, a re-ranking method for robust\nsolution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic\ncode augmentation with relevant comments and docstrings.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.SE",
    "comment": "20 pages, Conference",
    "pdf_url": "http://arxiv.org/pdf/2410.18251v1",
    "published_date": "2024-10-09 16:35:41 UTC",
    "updated_date": "2024-10-09 16:35:41 UTC"
  },
  {
    "arxiv_id": "2410.14712v1",
    "title": "Abstracting Situation Calculus Action Theories",
    "authors": [
      "Bita Banihashemi",
      "Giuseppe De Giacomo",
      "Yves Lespérance"
    ],
    "abstract": "We develop a general framework for agent abstraction based on the situation\ncalculus and the ConGolog agent programming language. We assume that we have a\nhigh-level specification and a low-level specification of the agent, both\nrepresented as basic action theories. A refinement mapping specifies how each\nhigh-level action is implemented by a low-level ConGolog program and how each\nhigh-level fluent can be translated into a low-level formula. We define a\nnotion of sound abstraction between such action theories in terms of the\nexistence of a suitable bisimulation between their respective models. Sound\nabstractions have many useful properties that ensure that we can reason about\nthe agent's actions (e.g., executability, projection, and planning) at the\nabstract level, and refine and concretely execute them at the low level. We\nalso characterize the notion of complete abstraction where all actions\n(including exogenous ones) that the high level thinks can happen can in fact\noccur at the low level. To facilitate verifying that one has a sound/complete\nabstraction relative to a mapping, we provide a set of necessary and sufficient\nconditions. Finally, we identify a set of basic action theory constraints that\nensure that for any low-level action sequence, there is a unique high-level\naction sequence that it refines. This allows us to track/monitor what the\nlow-level agent is doing and describe it in abstract terms (i.e., provide\nhigh-level explanations, for instance, to a client or manager).",
    "categories": [
      "cs.LO",
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.LO",
    "comment": "60 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2410.14712v1",
    "published_date": "2024-10-09 16:34:28 UTC",
    "updated_date": "2024-10-09 16:34:28 UTC"
  },
  {
    "arxiv_id": "2410.07041v1",
    "title": "Emergent properties with repeated examples",
    "authors": [
      "François Charton",
      "Julia Kempe"
    ],
    "abstract": "We study the performance of transformers as a function of the number of\nrepetitions of training examples with algorithmically generated datasets. On\nthree problems of mathematics: the greatest common divisor, modular\nmultiplication, and matrix eigenvalues, we show that for a fixed number of\ntraining steps, models trained on smaller sets of repeated examples outperform\nmodels trained on larger sets of single-use examples. We also demonstrate that\ntwo-set training - repeated use of a small random subset of examples, along\nnormal sampling on the rest of the training set - provides for faster learning\nand better performance. This highlights that the benefits of repetition can\noutweigh those of data diversity. These datasets and problems provide a\ncontrolled setting to shed light on the still poorly understood interplay\nbetween generalization and memorization in deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07041v1",
    "published_date": "2024-10-09 16:28:23 UTC",
    "updated_date": "2024-10-09 16:28:23 UTC"
  },
  {
    "arxiv_id": "2410.07035v1",
    "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",
    "authors": [
      "Zekun Wang",
      "Feiyu Duan",
      "Yibo Zhang",
      "Wangchunshu Zhou",
      "Ke Xu",
      "Wenhao Huang",
      "Jie Fu"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious domains, including role-playing, creative writing, mathematical\nreasoning, and coding. Despite these advancements, LLMs still encounter\nchallenges with length control, frequently failing to adhere to specific length\nconstraints due to their token-level operations and insufficient training on\ndata with strict length limitations. We identify this issue as stemming from a\nlack of positional awareness and propose novel approaches--PositionID Prompting\nand PositionID Fine-Tuning--to address it. These methods enhance the model's\nability to continuously monitor and manage text length during generation.\nAdditionally, we introduce PositionID CP Prompting to enable LLMs to perform\ncopy and paste operations accurately. Furthermore, we develop two benchmarks\nfor evaluating length control and copy-paste abilities. Our experiments\ndemonstrate that our methods significantly improve the model's adherence to\nlength constraints and copy-paste accuracy without compromising response\nquality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "39 pages. CP-Bench and LenCtrl-Bench are available in\n  https://huggingface.co/datasets/ZenMoore/CP-Bench and\n  https://huggingface.co/datasets/ZenMoore/LenCtrl-Bench",
    "pdf_url": "http://arxiv.org/pdf/2410.07035v1",
    "published_date": "2024-10-09 16:15:36 UTC",
    "updated_date": "2024-10-09 16:15:36 UTC"
  },
  {
    "arxiv_id": "2410.09103v1",
    "title": "Parameter-Efficient Fine-Tuning via Selective Discrete Cosine Transform",
    "authors": [
      "Yixian Shen",
      "Qi Bi",
      "Jia-Hong Huang",
      "Hongyi Zhu",
      "Anuj Pathania"
    ],
    "abstract": "In the era of large language models, parameter-efficient fine-tuning (PEFT)\nhas been extensively studied. However, these approaches usually rely on the\nspace domain, which encounters storage challenges especially when handling\nextensive adaptations or larger models. The frequency domain, in contrast, is\nmore effective in compressing trainable parameters while maintaining the\nexpressive capability. In this paper, we propose a novel Selective Discrete\nCosine Transformation (sDCTFT) fine-tuning scheme to push this frontier. Its\ngeneral idea is to exploit the superior energy compaction and decorrelation\nproperties of DCT to improve both model efficiency and accuracy. Specifically,\nit projects the weight change from the low-rank adaptation into the discrete\ncosine space. Then, the weight change is partitioned over different levels of\nthe discrete cosine spectrum, and the most critical frequency components in\neach partition are selected. Extensive experiments on four benchmark datasets\ndemonstrate the superior accuracy, reduced computational cost, and lower\nstorage requirements of the proposed method over the prior arts. For instance,\nwhen performing instruction tuning on the LLaMA3.1-8B model, sDCTFT outperforms\nLoRA with just 0.05M trainable parameters compared to LoRA's 38.2M, and\nsurpasses FourierFT with 30\\% less trainable parameters. The source code will\nbe publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09103v1",
    "published_date": "2024-10-09 16:07:42 UTC",
    "updated_date": "2024-10-09 16:07:42 UTC"
  },
  {
    "arxiv_id": "2410.07018v2",
    "title": "Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization",
    "authors": [
      "Chengtao Jian",
      "Kai Yang",
      "Yang Jiao"
    ],
    "abstract": "Out-of-Distribution (OOD) generalization in machine learning is a burgeoning\narea of study. Its primary goal is to enhance the adaptability and resilience\nof machine learning models when faced with new, unseen, and potentially\nadversarial data that significantly diverges from their original training\ndatasets. In this paper, we investigate time series OOD generalization via\npre-trained Large Language Models (LLMs). We first propose a novel\n\\textbf{T}ri-level learning framework for \\textbf{T}ime \\textbf{S}eries\n\\textbf{O}OD generalization, termed TTSO, which considers both sample-level and\ngroup-level uncertainties. This formula offers a fresh theoretic perspective\nfor formulating and analyzing OOD generalization problem. In addition, we\nprovide a theoretical analysis to justify this method is well motivated. We\nthen develop a stratified localization algorithm tailored for this tri-level\noptimization problem, theoretically demonstrating the guaranteed convergence of\nthe proposed algorithm. Our analysis also reveals that the iteration complexity\nto obtain an $\\epsilon$-stationary point is bounded by\nO($\\frac{1}{\\epsilon^{2}}$). Extensive experiments on real-world datasets have\nbeen conducted to elucidate the effectiveness of the proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.07018v2",
    "published_date": "2024-10-09 16:00:21 UTC",
    "updated_date": "2024-11-02 00:58:53 UTC"
  },
  {
    "arxiv_id": "2410.07009v2",
    "title": "Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with Patent-Paper Pairs",
    "authors": [
      "Valentin Knappich",
      "Simon Razniewski",
      "Anna Hätty",
      "Annemarie Friedrich"
    ],
    "abstract": "Dealing with long and highly complex technical text is a challenge for Large\nLanguage Models (LLMs), which still have to unfold their potential in\nsupporting expensive and timeintensive processes like patent drafting. Within\npatents, the description constitutes more than 90% of the document on average.\nYet, its automatic generation remains understudied. When drafting patent\napplications, patent attorneys typically receive invention reports (IRs), which\nare usually confidential, hindering research on LLM-supported patent drafting.\nOften, prepublication research papers serve as IRs. We leverage this duality to\nbuild PAP2PAT, an open and realistic benchmark for patent drafting consisting\nof 1.8k patent-paper pairs describing the same inventions. To address the\ncomplex longdocument patent generation task, we propose chunk-based\noutline-guided generation using the research paper as invention specification.\nOur extensive evaluation using PAP2PAT and a human case study show that LLMs\ncan effectively leverage information from the paper, but still struggle to\nprovide the necessary level of detail. Fine-tuning leads to more patent-style\nlanguage, but also to more hallucination. We release our data and code\nhttps://github.com/boschresearch/Pap2Pat.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07009v2",
    "published_date": "2024-10-09 15:52:48 UTC",
    "updated_date": "2025-03-06 08:51:05 UTC"
  },
  {
    "arxiv_id": "2410.07002v3",
    "title": "CursorCore: Assist Programming through Aligning Anything",
    "authors": [
      "Hao Jiang",
      "Qi Liu",
      "Rui Li",
      "Shengyu Ye",
      "Shijin Wang"
    ],
    "abstract": "Large language models have been successfully applied to programming\nassistance tasks, such as code completion, code insertion, and instructional\ncode editing. However, these applications remain insufficiently automated and\nstruggle to effectively integrate various types of information during the\nprogramming process, including coding history, current code, and user\ninstructions. In this work, we propose a new conversational framework that\ncomprehensively integrates these information sources, collect data to train our\nmodels and evaluate their performance. Firstly, to thoroughly evaluate how well\nmodels align with different types of information and the quality of their\noutputs, we introduce a new benchmark, APEval (Assist Programming Eval), to\ncomprehensively assess the performance of models in programming assistance\ntasks. Then, for data collection, we develop a data generation pipeline,\nProgramming-Instruct, which synthesizes training data from diverse sources,\nsuch as GitHub and online judge platforms. This pipeline can automatically\ngenerate various types of messages throughout the programming process. Finally,\nusing this pipeline, we generate 219K samples, fine-tune multiple models, and\ndevelop the CursorCore series. We show that CursorCore outperforms other models\nof comparable size. This framework unifies applications such as inline chat and\nautomated editing, contributes to the advancement of coding assistants. Code,\nmodels and data are freely available at\nhttps://github.com/TechxGenus/CursorCore.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07002v3",
    "published_date": "2024-10-09 15:45:52 UTC",
    "updated_date": "2025-05-13 14:13:13 UTC"
  },
  {
    "arxiv_id": "2410.10880v2",
    "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
    "authors": [
      "Hengxiang Zhang",
      "Songxin Zhang",
      "Bingyi Jing",
      "Hongxin Wei"
    ],
    "abstract": "In the era of large language models (LLMs), detecting pretraining data has\nbeen increasingly important due to concerns about fair evaluation and ethical\nrisks. Current methods differentiate members and non-members by designing\nscoring functions, like Perplexity and Min-k%. However, the diversity and\ncomplexity of training data magnifies the difficulty of distinguishing, leading\nto suboptimal performance in detecting pretraining data. In this paper, we\nfirst explore the benefits of unseen data, which can be easily collected after\nthe release of the LLM. We find that the perplexities of LLMs shift differently\nfor members and non-members, after fine-tuning with a small amount of\npreviously unseen data. In light of this, we introduce a novel and effective\nmethod termed Fine-tuned Score Deviation(FSD), which improves the performance\nof current scoring functions for pretraining data detection. In particular, we\npropose to measure the deviation distance of current scores after fine-tuning\non a small amount of unseen data within the same domain. In effect, using a few\nunseen data can largely decrease the scores of all non-members, leading to a\nlarger deviation distance than members. Extensive experiments demonstrate the\neffectiveness of our method, significantly improving the AUC score on common\nbenchmark datasets across various models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10880v2",
    "published_date": "2024-10-09 15:36:42 UTC",
    "updated_date": "2025-03-17 12:29:05 UTC"
  },
  {
    "arxiv_id": "2410.06981v3",
    "title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models",
    "authors": [
      "Michael Lan",
      "Philip Torr",
      "Austin Meek",
      "Ashkan Khakzar",
      "David Krueger",
      "Fazl Barez"
    ],
    "abstract": "We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones, making it difficult\nto disentangle and match features across different models. To address this\nissue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics on SAE feature spaces across different LLMs. Our\nexperiments reveal significant similarities in SAE feature spaces across\nvarious LLMs, providing new evidence for feature universality.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06981v3",
    "published_date": "2024-10-09 15:18:57 UTC",
    "updated_date": "2025-03-17 00:31:46 UTC"
  },
  {
    "arxiv_id": "2410.06977v2",
    "title": "Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification",
    "authors": [
      "Chenyue Li",
      "Shuoyi Chen",
      "Mang Ye"
    ],
    "abstract": "Wildlife ReID involves utilizing visual technology to identify specific\nindividuals of wild animals in different scenarios, holding significant\nimportance for wildlife conservation, ecological research, and environmental\nmonitoring. Existing wildlife ReID methods are predominantly tailored to\nspecific species, exhibiting limited applicability. Although some approaches\nleverage extensively studied person ReID techniques, they struggle to address\nthe unique challenges posed by wildlife. Therefore, in this paper, we present a\nunified, multi-species general framework for wildlife ReID. Given that\nhigh-frequency information is a consistent representation of unique features in\nvarious species, significantly aiding in identifying contours and details such\nas fur textures, we propose the Adaptive High-Frequency Transformer model with\nthe goal of enhancing high-frequency information learning. To mitigate the\ninevitable high-frequency interference in the wilderness environment, we\nintroduce an object-aware high-frequency selection strategy to adaptively\ncapture more valuable high-frequency components. Notably, we unify the\nexperimental settings of multiple wildlife datasets for ReID, achieving\nsuperior performance over state-of-the-art ReID methods. In domain\ngeneralization scenarios, our approach demonstrates robust generalization to\nunknown species.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by European Conference on Computer Vision (ECCV) 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.06977v2",
    "published_date": "2024-10-09 15:16:30 UTC",
    "updated_date": "2024-10-25 14:13:28 UTC"
  },
  {
    "arxiv_id": "2410.06973v1",
    "title": "Personal Intelligence System UniLM: Hybrid On-Device Small Language Model and Server-Based Large Language Model for Malay Nusantara",
    "authors": [
      "Azree Nazri",
      "Olalekan Agbolade",
      "Faisal Aziz"
    ],
    "abstract": "In contexts with limited computational and data resources, high-resource\nlanguage models often prove inadequate, particularly when addressing the\nspecific needs of Malay languages. This paper introduces a Personal\nIntelligence System designed to efficiently integrate both on-device and\nserver-based models. The system incorporates SLiM-34M for on-device processing,\noptimized for low memory and power usage, and MANYAK-1.3B for server-based\ntasks, allowing for scalable, high-performance language processing. The models\nachieve significant results across various tasks, such as machine translation,\nquestion-answering, and translate IndoMMLU. Particularly noteworthy is\nSLiM-34M's ability to achieve a high improvement in accuracy compared to other\nLLMs while using 2 times fewer pre-training tokens. This work challenges the\nprevailing assumption that large-scale computational resources are necessary to\nbuild effective language models, contributing to the development of\nresource-efficient models for the Malay language with the unique orchestration\nbetween SLiM-34M and MANYAK-1.3B.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 5 tables, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.06973v1",
    "published_date": "2024-10-09 15:11:13 UTC",
    "updated_date": "2024-10-09 15:11:13 UTC"
  },
  {
    "arxiv_id": "2410.06969v1",
    "title": "DLGNet: Hyperedge Classification through Directed Line Graphs for Chemical Reactions",
    "authors": [
      "Stefano Fiorini",
      "Giulia M. Bovolenta",
      "Stefano Coniglio",
      "Michele Ciavotta",
      "Pietro Morerio",
      "Michele Parrinello",
      "Alessio Del Bue"
    ],
    "abstract": "Graphs and hypergraphs provide powerful abstractions for modeling\ninteractions among a set of entities of interest and have been attracting a\ngrowing interest in the literature thanks to many successful applications in\nseveral fields. In particular, they are rapidly expanding in domains such as\nchemistry and biology, especially in the areas of drug discovery and molecule\ngeneration. One of the areas witnessing the fasted growth is the chemical\nreactions field, where chemical reactions can be naturally encoded as directed\nhyperedges of a hypergraph. In this paper, we address the chemical reaction\nclassification problem by introducing the notation of a Directed Line Graph\n(DGL) associated with a given directed hypergraph. On top of it, we build the\nDirected Line Graph Network (DLGNet), the first spectral-based Graph Neural\nNetwork (GNN) expressly designed to operate on a hypergraph via its DLG\ntransformation. The foundation of DLGNet is a novel Hermitian matrix, the\nDirected Line Graph Laplacian, which compactly encodes the directionality of\nthe interactions taking place within the directed hyperedges of the hypergraph\nthanks to the DLG representation. The Directed Line Graph Laplacian enjoys many\ndesirable properties, including admitting an eigenvalue decomposition and being\npositive semidefinite, which make it well-suited for its adoption within a\nspectral-based GNN. Through extensive experiments on chemical reaction\ndatasets, we show that DGLNet significantly outperforms the existing\napproaches, achieving on a collection of real-world datasets an average\nrelative-percentage-difference improvement of 33.01%, with a maximum\nimprovement of 37.71%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06969v1",
    "published_date": "2024-10-09 15:07:53 UTC",
    "updated_date": "2024-10-09 15:07:53 UTC"
  },
  {
    "arxiv_id": "2410.06965v2",
    "title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment",
    "authors": [
      "Juhyun Oh",
      "Eunsu Kim",
      "Jiseon Kim",
      "Wenda Xu",
      "Inha Cha",
      "William Yang Wang",
      "Alice Oh"
    ],
    "abstract": "Despite advancements in Large Language Model (LLM) alignment, understanding\nthe reasons behind LLM preferences remains crucial for bridging the gap between\ndesired and actual behavior. LLMs often exhibit biases or tendencies that\ndiverge from human preferences, such as favoring certain writing styles or\nproducing overly verbose outputs. However, current methods for evaluating\npreference alignment often lack explainability, relying on coarse-grained\ncomparisons. To address this, we introduce PROFILE (PRObing Factors of\nInfLuence for Explainability), a novel framework that uncovers and quantifies\nthe influence of specific factors driving preferences. PROFILE's factor level\nanalysis explains the 'why' behind human-model alignment and misalignment,\noffering insights into the direction of model improvement. We apply PROFILE to\nanalyze human and LLM preferences across three tasks: summarization, helpful\nresponse generation, and document-based question-answering. Our factor level\nanalysis reveals a substantial discrepancy between human and LLM preferences in\ngeneration tasks, whereas LLMs show strong alignment with human preferences in\nevaluation tasks. We demonstrate how leveraging factor level insights,\nincluding addressing misaligned factors or exploiting the generation-evaluation\ngap, can improve alignment with human preferences. This work underscores the\nimportance of explainable preference analysis and highlights PROFILE's\npotential to provide valuable training signals, driving further improvements in\nhuman-model alignment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06965v2",
    "published_date": "2024-10-09 15:02:34 UTC",
    "updated_date": "2024-11-24 13:43:37 UTC"
  },
  {
    "arxiv_id": "2410.06963v2",
    "title": "ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling",
    "authors": [
      "Deok-Kyeong Jang",
      "Dongseok Yang",
      "Deok-Yun Jang",
      "Byeoli Choi",
      "Donghoon Shin",
      "Sung-hee Lee"
    ],
    "abstract": "This paper introduces ELMO, a real-time upsampling motion capture framework\ndesigned for a single LiDAR sensor. Modeled as a conditional autoregressive\ntransformer-based upsampling motion generator, ELMO achieves 60 fps motion\ncapture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is\nthe coupling of the self-attention mechanism with thoughtfully designed\nembedding modules for motion and point clouds, significantly elevating the\nmotion quality. To facilitate accurate motion capture, we develop a one-time\nskeleton calibration model capable of predicting user skeleton offsets from a\nsingle-frame point cloud. Additionally, we introduce a novel data augmentation\ntechnique utilizing a LiDAR simulator, which enhances global root tracking to\nimprove environmental understanding. To demonstrate the effectiveness of our\nmethod, we compare ELMO with state-of-the-art methods in both image-based and\npoint cloud-based motion capture. We further conduct an ablation study to\nvalidate our design principles. ELMO's fast inference time makes it well-suited\nfor real-time applications, exemplified in our demo video featuring live\nstreaming and interactive gaming scenarios. Furthermore, we contribute a\nhigh-quality LiDAR-mocap synchronized dataset comprising 20 different subjects\nperforming a range of motions, which can serve as a valuable resource for\nfuture research. The dataset and evaluation code are available at {\\blue\n\\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.GR",
    "comment": "published at ACM Transactions on Graphics (Proc. SIGGRAPH ASIA), 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.06963v2",
    "published_date": "2024-10-09 15:02:08 UTC",
    "updated_date": "2024-10-11 14:12:48 UTC"
  },
  {
    "arxiv_id": "2410.06961v1",
    "title": "Self-Boosting Large Language Models with Synthetic Preference Data",
    "authors": [
      "Qingxiu Dong",
      "Li Dong",
      "Xingxing Zhang",
      "Zhifang Sui",
      "Furu Wei"
    ],
    "abstract": "Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06961v1",
    "published_date": "2024-10-09 14:57:31 UTC",
    "updated_date": "2024-10-09 14:57:31 UTC"
  },
  {
    "arxiv_id": "2410.06957v1",
    "title": "Support Vector Boosting Machine (SVBM): Enhancing Classification Performance with AdaBoost and Residual Connections",
    "authors": [
      "Junbo Jacob Lian"
    ],
    "abstract": "In traditional boosting algorithms, the focus on misclassified training\nsamples emphasizes their importance based on difficulty during the learning\nprocess. While using a standard Support Vector Machine (SVM) as a weak learner\nin an AdaBoost framework can enhance model performance by concentrating on\nerror samples, this approach introduces significant challenges. Specifically,\nSVMs, characterized by their stability and robustness, may require\ndestabilization to fit the boosting paradigm, which in turn can constrain\nperformance due to reliance on the weighted results from preceding iterations.\nTo address these challenges, we propose the Support Vector Boosting Machine\n(SVBM), which integrates a novel subsampling process with SVM algorithms and\nresidual connection techniques. This method updates sample weights by\nconsidering both the current model's predictions and the outputs from prior\nrounds, allowing for effective sparsity control. The SVBM framework enhances\nthe ability to form complex decision boundaries, thereby improving\nclassification performance. The MATLAB source code for SVBM can be accessed at\nhttps://github.com/junbolian/SVBM.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The MATLAB source code for SVBM can be accessed at\n  https://github.com/junbolian/SVBM",
    "pdf_url": "http://arxiv.org/pdf/2410.06957v1",
    "published_date": "2024-10-09 14:55:19 UTC",
    "updated_date": "2024-10-09 14:55:19 UTC"
  },
  {
    "arxiv_id": "2410.11872v2",
    "title": "ClickAgent: Enhancing UI Location Capabilities of Autonomous Agents",
    "authors": [
      "Jakub Hoscilowicz",
      "Bartosz Maj",
      "Bartosz Kozakiewicz",
      "Oleksii Tymoshchuk",
      "Artur Janicki"
    ],
    "abstract": "With the growing reliance on digital devices equipped with graphical user\ninterfaces (GUIs), such as computers and smartphones, the need for effective\nautomation tools has become increasingly important. While multimodal large\nlanguage models (MLLMs) like GPT-4V excel in many areas, they struggle with GUI\ninteractions, limiting their effectiveness in automating everyday tasks. In\nthis paper, we introduce ClickAgent, a novel framework for building autonomous\nagents. In ClickAgent, the MLLM handles reasoning and action planning, while a\nseparate UI location model (e.g., SeeClick) identifies the relevant UI elements\non the screen. This approach addresses a key limitation of current-generation\nMLLMs: their difficulty in accurately locating UI elements. ClickAgent\noutperforms other prompt-based autonomous agents (CogAgent, AppAgent) on the\nAITW benchmark. Our evaluation was conducted on both an Android smartphone\nemulator and an actual Android smartphone, using the task success rate as the\nkey metric for measuring agent performance.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "The code for ClickAgent is available at github.com/Samsung/ClickAgent",
    "pdf_url": "http://arxiv.org/pdf/2410.11872v2",
    "published_date": "2024-10-09 14:49:02 UTC",
    "updated_date": "2024-10-17 07:12:31 UTC"
  },
  {
    "arxiv_id": "2410.06950v1",
    "title": "Faithful Interpretation for Graph Neural Networks",
    "authors": [
      "Lijie Hu",
      "Tianhao Huang",
      "Lu Yu",
      "Wanyu Lin",
      "Tianhang Zheng",
      "Di Wang"
    ],
    "abstract": "Currently, attention mechanisms have garnered increasing attention in Graph\nNeural Networks (GNNs), such as Graph Attention Networks (GATs) and Graph\nTransformers (GTs). It is not only due to the commendable boost in performance\nthey offer but also its capacity to provide a more lucid rationale for model\nbehaviors, which are often viewed as inscrutable. However, Attention-based GNNs\nhave demonstrated instability in interpretability when subjected to various\nsources of perturbations during both training and testing phases, including\nfactors like additional edges or nodes. In this paper, we propose a solution to\nthis problem by introducing a novel notion called Faithful Graph\nAttention-based Interpretation (FGAI). In particular, FGAI has four crucial\nproperties regarding stability and sensitivity to interpretation and final\noutput distribution. Built upon this notion, we propose an efficient\nmethodology for obtaining FGAI, which can be viewed as an ad hoc modification\nto the canonical Attention-based GNNs. To validate our proposed solution, we\nintroduce two novel metrics tailored for graph interpretation assessment.\nExperimental results demonstrate that FGAI exhibits superior stability and\npreserves the interpretability of attention under various forms of\nperturbations and randomness, which makes FGAI a more faithful and reliable\nexplanation tool.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.06950v1",
    "published_date": "2024-10-09 14:47:12 UTC",
    "updated_date": "2024-10-09 14:47:12 UTC"
  },
  {
    "arxiv_id": "2410.06946v2",
    "title": "A Trilogy of AI Safety Frameworks: Paths from Facts and Knowledge Gaps to Reliable Predictions and New Knowledge",
    "authors": [
      "Simon Kasif"
    ],
    "abstract": "AI Safety has become a vital front-line concern of many scientists within and\noutside the AI community. There are many immediate and long term anticipated\nrisks that range from existential risk to human existence to deep fakes and\nbias in machine learning systems [1-5]. In this paper, we reduce the full scope\nand immense complexity of AI safety concerns to a trilogy of three important\nbut tractable opportunities for advances that have the short-term potential to\nimprove AI safety and reliability without reducing AI innovation in critical\ndomains. In this perspective, we discuss this vision based on several case\nstudies that already produced proofs of concept in critical ML applications in\nbiomedical science.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06946v2",
    "published_date": "2024-10-09 14:43:06 UTC",
    "updated_date": "2024-10-13 17:35:36 UTC"
  },
  {
    "arxiv_id": "2410.06943v1",
    "title": "AutoFeedback: An LLM-based Framework for Efficient and Accurate API Request Generation",
    "authors": [
      "Huanxi Liu",
      "Jiaqi Liao",
      "Dawei Feng",
      "Kele Xu",
      "Huaimin Wang"
    ],
    "abstract": "Large Language Models (LLMs) leverage external tools primarily through\ngenerating the API request to enhance task completion efficiency. The accuracy\nof API request generation significantly determines the capability of LLMs to\naccomplish tasks.\n  Due to the inherent hallucinations within the LLM, it is difficult to\nefficiently and accurately generate the correct API request.\n  Current research uses prompt-based feedback to facilitate the LLM-based API\nrequest generation. However, existing methods lack factual information and are\ninsufficiently detailed.\n  To address these issues, we propose AutoFeedback, an LLM-based framework for\nefficient and accurate API request generation, with a Static Scanning Component\n(SSC) and a Dynamic Analysis Component (DAC). SSC incorporates errors detected\nin the API requests as pseudo-facts into the feedback, enriching the factual\ninformation. DAC retrieves information from API documentation, enhancing the\nlevel of detail in feedback.\n  Based on this two components, Autofeedback implementes two feedback loops\nduring the process of generating API requests by the LLM.\n  Extensive experiments demonstrate that it significantly improves accuracy of\nAPI request generation and reduces the interaction cost. AutoFeedback achieves\nan accuracy of 100.00\\% on a real-world API dataset and reduces the cost of\ninteraction with GPT-3.5 Turbo by 23.44\\%, and GPT-4 Turbo by 11.85\\%.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.06943v1",
    "published_date": "2024-10-09 14:38:28 UTC",
    "updated_date": "2024-10-09 14:38:28 UTC"
  },
  {
    "arxiv_id": "2410.06932v1",
    "title": "Reproducing and Extending Experiments in Behavioral Strategy with Large Language Models",
    "authors": [
      "Daniel Albert",
      "Stephan Billinger"
    ],
    "abstract": "In this study, we propose LLM agents as a novel approach in behavioral\nstrategy research, complementing simulations and laboratory experiments to\nadvance our understanding of cognitive processes in decision-making.\nSpecifically, we reproduce a human laboratory experiment in behavioral strategy\nusing large language model (LLM) generated agents and investigate how LLM\nagents compare to observed human behavior. Our results show that LLM agents\neffectively reproduce search behavior and decision-making comparable to humans.\nExtending our experiment, we analyze LLM agents' simulated \"thoughts,\"\ndiscovering that more forward-looking thoughts correlate with favoring\nexploitation over exploration to maximize wealth. We show how this new approach\ncan be leveraged in behavioral strategy research and address limitations.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06932v1",
    "published_date": "2024-10-09 14:26:20 UTC",
    "updated_date": "2024-10-09 14:26:20 UTC"
  },
  {
    "arxiv_id": "2410.06912v2",
    "title": "Compositional Entailment Learning for Hyperbolic Vision-Language Models",
    "authors": [
      "Avik Pal",
      "Max van Spengler",
      "Guido Maria D'Amely di Melendugno",
      "Alessandro Flaborea",
      "Fabio Galasso",
      "Pascal Mettes"
    ],
    "abstract": "Image-text representation learning forms a cornerstone in vision-language\nmodels, where pairs of images and textual descriptions are contrastively\naligned in a shared embedding space. Since visual and textual concepts are\nnaturally hierarchical, recent work has shown that hyperbolic space can serve\nas a high-potential manifold to learn vision-language representation with\nstrong downstream performance. In this work, for the first time we show how to\nfully leverage the innate hierarchical nature of hyperbolic embeddings by\nlooking beyond individual image-text pairs. We propose Compositional Entailment\nLearning for hyperbolic vision-language models. The idea is that an image is\nnot only described by a sentence but is itself a composition of multiple object\nboxes, each with their own textual description. Such information can be\nobtained freely by extracting nouns from sentences and using openly available\nlocalized grounding models. We show how to hierarchically organize images,\nimage boxes, and their textual descriptions through contrastive and\nentailment-based objectives. Empirical evaluation on a hyperbolic\nvision-language model trained with millions of image-text pairs shows that the\nproposed compositional learning approach outperforms conventional Euclidean\nCLIP learning, as well as recent hyperbolic alternatives, with better zero-shot\nand retrieval generalization and clearly stronger hierarchical performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as oral paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.06912v2",
    "published_date": "2024-10-09 14:12:50 UTC",
    "updated_date": "2025-03-01 13:43:36 UTC"
  },
  {
    "arxiv_id": "2410.06911v1",
    "title": "Combining Planning and Diffusion for Mobility with Unknown Dynamics",
    "authors": [
      "Yajvan Ravan",
      "Zhutian Yang",
      "Tao Chen",
      "Tomás Lozano-Pérez",
      "Leslie Pack Kaelbling"
    ],
    "abstract": "Manipulation of large objects over long horizons (such as carts in a\nwarehouse) is an essential skill for deployable robotic systems. Large objects\nrequire mobile manipulation which involves simultaneous manipulation,\nnavigation, and movement with the object in tow. In many real-world situations,\nobject dynamics are incredibly complex, such as the interaction of an office\nchair (with a rotating base and five caster wheels) and the ground. We present\na hierarchical algorithm for long-horizon robot manipulation problems in which\nthe dynamics are partially unknown. We observe that diffusion-based behavior\ncloning is highly effective for short-horizon problems with unknown dynamics,\nso we decompose the problem into an abstract high-level, obstacle-aware\nmotion-planning problem that produces a waypoint sequence. We use a\nshort-horizon, relative-motion diffusion policy to achieve the waypoints in\nsequence. We train mobile manipulation policies on a Spot robot that has to\npush and pull an office chair. Our hierarchical manipulation policy performs\nconsistently better, especially when the horizon increases, compared to a\ndiffusion policy trained on long-horizon demonstrations or motion planning\nassuming a rigidly-attached object (success rate of 8 (versus 0 and 5\nrespectively) out of 10 runs). Importantly, our learned policy generalizes to\nnew layouts, grasps, chairs, and flooring that induces more friction, without\nany further training, showing promise for other complex mobile manipulation\nproblems. Project Page: https://yravan.github.io/plannerorderedpolicy/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.06911v1",
    "published_date": "2024-10-09 14:12:28 UTC",
    "updated_date": "2024-10-09 14:12:28 UTC"
  },
  {
    "arxiv_id": "2410.07289v1",
    "title": "Principal Orthogonal Latent Components Analysis (POLCA Net)",
    "authors": [
      "Jose Antonio Martin H.",
      "Freddy Perozo",
      "Manuel Lopez"
    ],
    "abstract": "Representation learning is a pivotal area in the field of machine learning,\nfocusing on the development of methods to automatically discover the\nrepresentations or features needed for a given task from raw data. Unlike\ntraditional feature engineering, which requires manual crafting of features,\nrepresentation learning aims to learn features that are more useful and\nrelevant for tasks such as classification, prediction, and clustering. We\nintroduce Principal Orthogonal Latent Components Analysis Network (POLCA Net),\nan approach to mimic and extend PCA and LDA capabilities to non-linear domains.\nPOLCA Net combines an autoencoder framework with a set of specialized loss\nfunctions to achieve effective dimensionality reduction, orthogonality,\nvariance-based feature sorting, high-fidelity reconstructions, and\nadditionally, when used with classification labels, a latent representation\nwell suited for linear classifiers and low dimensional visualization of class\ndistribution as well.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07289v1",
    "published_date": "2024-10-09 14:04:31 UTC",
    "updated_date": "2024-10-09 14:04:31 UTC"
  },
  {
    "arxiv_id": "2410.06883v4",
    "title": "Degree-Conscious Spiking Graph for Cross-Domain Adaptation",
    "authors": [
      "Yingxu Wang",
      "Mengzhu Wang",
      "Siwei Liu",
      "Houcheng Su",
      "Nan Yin",
      "James Kwok"
    ],
    "abstract": "Spiking Graph Networks (SGNs) have demonstrated significant potential in\ngraph classification by emulating brain-inspired neural dynamics to achieve\nenergy-efficient computation. However, existing SGNs are generally constrained\nto in-distribution scenarios and struggle with distribution shifts. In this\npaper, we first propose the domain adaptation problem in SGNs, and introduce a\nnovel framework named Degree-Consicious Spiking Graph for Cross-Domain\nAdaptation. DeSGraDA enhances generalization across domains with three key\ncomponents. First, we introduce the degree-conscious spiking representation\nmodule by adapting spike thresholds based on node degrees, enabling more\nexpressive and structure-aware signal encoding. Then, we perform temporal\ndistribution alignment by adversarially matching membrane potentials between\ndomains, ensuring effective performance under domain shift while preserving\nenergy efficiency. Additionally, we extract consistent predictions across two\nspaces to create reliable pseudo-labels, effectively leveraging unlabeled data\nto enhance graph classification performance. Furthermore, we establish the\nfirst generalization bound for SGDA, providing theoretical insights into its\nadaptation performance. Extensive experiments on benchmark datasets validate\nthat DeSGraDA consistently outperforms state-of-the-art methods in both\nclassification accuracy and energy efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06883v4",
    "published_date": "2024-10-09 13:45:54 UTC",
    "updated_date": "2025-05-16 12:24:56 UTC"
  },
  {
    "arxiv_id": "2410.06865v2",
    "title": "Students' Perceptions and Use of Generative AI Tools for Programming Across Different Computing Courses",
    "authors": [
      "Hieke Keuning",
      "Isaac Alpizar-Chacon",
      "Ioanna Lykourentzou",
      "Lauren Beehler",
      "Christian Köppe",
      "Imke de Jong",
      "Sergey Sosnovsky"
    ],
    "abstract": "Investigation of students' perceptions and opinions on the use of generative\nartificial intelligence (GenAI) in education is a topic gaining much interest.\nStudies addressing this are typically conducted with large heterogeneous\ngroups, at one moment in time. However, how students perceive and use GenAI\ntools can potentially depend on many factors, including their background\nknowledge, familiarity with the tools, and the learning goals and policies of\nthe courses they are taking.\n  In this study we explore how students following computing courses use GenAI\nfor programming-related tasks across different programs and courses: Bachelor\nand Master, in courses in which learning programming is the learning goal,\ncourses that require programming as a means to achieve another goal, and in\ncourses in which programming is optional, but can be useful. We are also\ninterested in changes over time, since GenAI capabilities are changing at a\nfast pace, and users are adopting GenAI increasingly.\n  We conducted three consecutive surveys (fall `23, winter `23, and spring `24)\namong students of all computing programs of a large European research\nuniversity. We asked questions on the use in education, ethics, and job\nprospects, and we included specific questions on the (dis)allowed use of GenAI\ntools in the courses they were taking at the time.\n  We received 264 responses, which we quantitatively and qualitatively\nanalyzed, to find out how students have employed GenAI tools across 59\ndifferent computing courses, and whether the opinion of an average student\nabout these tools evolves over time. Our study contributes to the emerging\ndiscussion of how to differentiate GenAI use across different courses, and how\nto align its use with the learning goals of a computing course.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted to Koli Calling 24. Numbers in Table 1, row 1 updated",
    "pdf_url": "http://arxiv.org/pdf/2410.06865v2",
    "published_date": "2024-10-09 13:24:06 UTC",
    "updated_date": "2024-11-13 20:41:14 UTC"
  },
  {
    "arxiv_id": "2410.07287v1",
    "title": "Crafting desirable climate trajectories with RL explored socio-environmental simulations",
    "authors": [
      "James Rudd-Jones",
      "Fiona Thendean",
      "María Pérez-Ortiz"
    ],
    "abstract": "Climate change poses an existential threat, necessitating effective climate\npolicies to enact impactful change. Decisions in this domain are incredibly\ncomplex, involving conflicting entities and evidence. In the last decades,\npolicymakers increasingly use simulations and computational methods to guide\nsome of their decisions. Integrated Assessment Models (IAMs) are one of such\nmethods, which combine social, economic, and environmental simulations to\nforecast potential policy effects. For example, the UN uses outputs of IAMs for\ntheir recent Intergovernmental Panel on Climate Change (IPCC) reports.\nTraditionally these have been solved using recursive equation solvers, but have\nseveral shortcomings, e.g. struggling at decision making under uncertainty.\nRecent preliminary work using Reinforcement Learning (RL) to replace the\ntraditional solvers shows promising results in decision making in uncertain and\nnoisy scenarios. We extend on this work by introducing multiple interacting RL\nagents as a preliminary analysis on modelling the complex interplay of\nsocio-interactions between various stakeholders or nations that drives much of\nthe current climate crisis. Our findings show that cooperative agents in this\nframework can consistently chart pathways towards more desirable futures in\nterms of reduced carbon emissions and improved economy. However, upon\nintroducing competition between agents, for instance by using opposing reward\nfunctions, desirable climate futures are rarely reached. Modelling competition\nis key to increased realism in these simulations, as such we employ policy\ninterpretation by visualising what states lead to more uncertain behaviour, to\nunderstand algorithm failure. Finally, we highlight the current limitations and\navenues for further work to ensure future technology uptake for policy\nderivation.",
    "categories": [
      "physics.soc-ph",
      "cs.AI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "23 pages, 13 Figures",
    "pdf_url": "http://arxiv.org/pdf/2410.07287v1",
    "published_date": "2024-10-09 13:21:50 UTC",
    "updated_date": "2024-10-09 13:21:50 UTC"
  },
  {
    "arxiv_id": "2410.07286v2",
    "title": "Benchmarking Data Heterogeneity Evaluation Approaches for Personalized Federated Learning",
    "authors": [
      "Zhilong Li",
      "Xiaohu Wu",
      "Xiaoli Tang",
      "Tiantian He",
      "Yew-Soon Ong",
      "Mengmeng Chen",
      "Qiqi Liu",
      "Qicheng Lao",
      "Han Yu"
    ],
    "abstract": "There is growing research interest in measuring the statistical heterogeneity\nof clients' local datasets. Such measurements are used to estimate the\nsuitability for collaborative training of personalized federated learning (PFL)\nmodels. Currently, these research endeavors are taking place in silos and there\nis a lack of a unified benchmark to provide a fair and convenient comparison\namong various approaches in common settings. We aim to bridge this important\ngap in this paper. The proposed benchmarking framework currently includes six\nrepresentative approaches. Extensive experiments have been conducted to compare\nthese approaches under five standard non-IID FL settings, providing much needed\ninsights into which approaches are advantageous under which settings. The\nproposed framework offers useful guidance on the suitability of various data\ndivergence measures in FL systems. It is beneficial for keeping related\nresearch activities on the right track in terms of: (1) designing PFL schemes,\n(2) selecting appropriate data heterogeneity evaluation approaches for specific\nFL application scenarios, and (3) addressing fairness issues in collaborative\nmodel training. The code is available at\nhttps://github.com/Xiaoni-61/DH-Benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to FL@FM-NeurIPS'24",
    "pdf_url": "http://arxiv.org/pdf/2410.07286v2",
    "published_date": "2024-10-09 13:16:02 UTC",
    "updated_date": "2024-10-28 05:00:37 UTC"
  },
  {
    "arxiv_id": "2410.06851v2",
    "title": "Understanding Model Ensemble in Transferable Adversarial Attack",
    "authors": [
      "Wei Yao",
      "Zeliang Zhang",
      "Huayi Tang",
      "Yong Liu"
    ],
    "abstract": "Model ensemble adversarial attack has become a powerful method for generating\ntransferable adversarial examples that can target even unknown models, but its\ntheoretical foundation remains underexplored. To address this gap, we provide\nearly theoretical insights that serve as a roadmap for advancing model ensemble\nadversarial attack. We first define transferability error to measure the error\nin adversarial transferability, alongside concepts of diversity and empirical\nmodel ensemble Rademacher complexity. We then decompose the transferability\nerror into vulnerability, diversity, and a constant, which rigidly explains the\norigin of transferability error in model ensemble attack: the vulnerability of\nan adversarial example to ensemble components, and the diversity of ensemble\ncomponents. Furthermore, we apply the latest mathematical tools in information\ntheory to bound the transferability error using complexity and generalization\nterms, contributing to three practical guidelines for reducing transferability\nerror: (1) incorporating more surrogate models, (2) increasing their diversity,\nand (3) reducing their complexity in cases of overfitting. Finally, extensive\nexperiments with 54 models validate our theoretical framework, representing a\nsignificant step forward in understanding transferable model ensemble\nadversarial attacks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06851v2",
    "published_date": "2024-10-09 13:14:11 UTC",
    "updated_date": "2025-01-31 02:01:16 UTC"
  },
  {
    "arxiv_id": "2410.06847v1",
    "title": "A Safety Modulator Actor-Critic Method in Model-Free Safe Reinforcement Learning and Application in UAV Hovering",
    "authors": [
      "Qihan Qi",
      "Xinsong Yang",
      "Gang Xia",
      "Daniel W. C. Ho",
      "Pengyang Tang"
    ],
    "abstract": "This paper proposes a safety modulator actor-critic (SMAC) method to address\nsafety constraint and overestimation mitigation in model-free safe\nreinforcement learning (RL). A safety modulator is developed to satisfy safety\nconstraints by modulating actions, allowing the policy to ignore safety\nconstraint and focus on maximizing reward. Additionally, a distributional\ncritic with a theoretical update rule for SMAC is proposed to mitigate the\noverestimation of Q-values with safety constraints. Both simulation and\nreal-world scenarios experiments on Unmanned Aerial Vehicles (UAVs) hovering\nconfirm that the SMAC can effectively maintain safety constraints and\noutperform mainstream baseline algorithms.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06847v1",
    "published_date": "2024-10-09 13:07:24 UTC",
    "updated_date": "2024-10-09 13:07:24 UTC"
  },
  {
    "arxiv_id": "2410.06846v4",
    "title": "Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity",
    "authors": [
      "Mutian He",
      "Philip N. Garner"
    ],
    "abstract": "Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 5 figures; ICLR 2025 camera ready. Code:\n  https://github.com/idiap/linearize-distill-pretrained-transformers",
    "pdf_url": "http://arxiv.org/pdf/2410.06846v4",
    "published_date": "2024-10-09 13:06:43 UTC",
    "updated_date": "2025-03-13 16:17:19 UTC"
  },
  {
    "arxiv_id": "2410.06845v2",
    "title": "MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders",
    "authors": [
      "Cheng Li",
      "May Fung",
      "Qingyun Wang",
      "Chi Han",
      "Manling Li",
      "Jindong Wang",
      "Heng Ji"
    ],
    "abstract": "Mental health disorders are one of the most serious diseases in the world.\nMost people with such a disease lack access to adequate care, which highlights\nthe importance of training models for the diagnosis and treatment of mental\nhealth disorders. However, in the mental health domain, privacy concerns limit\nthe accessibility of personalized treatment data, making it challenging to\nbuild powerful models. In this paper, we introduce MentalArena, a self-play\nframework to train language models by generating domain-specific personalized\ndata, where we obtain a better model capable of making a personalized diagnosis\nand treatment (as a therapist) and providing information (as a patient). To\naccurately model human-like mental health patients, we devise Symptom Encoder,\nwhich simulates a real patient from both cognition and behavior perspectives.\nTo address intent bias during patient-therapist interactions, we propose\nSymptom Decoder to compare diagnosed symptoms with encoded symptoms, and\ndynamically manage the dialogue between patient and therapist according to the\nidentified deviations. We evaluated MentalArena against 6 benchmarks, including\nbiomedicalQA and mental health tasks, compared to 6 advanced models. Our\nmodels, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform\ntheir counterparts, including GPT-4o. We hope that our work can inspire future\nresearch on personalized care. Code is available in\nhttps://github.com/Scarelette/MentalArena/tree/main",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical Report; 26 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.06845v2",
    "published_date": "2024-10-09 13:06:40 UTC",
    "updated_date": "2025-02-05 07:15:14 UTC"
  },
  {
    "arxiv_id": "2410.09102v2",
    "title": "Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy",
    "authors": [
      "Tong Wu",
      "Shujian Zhang",
      "Kaiqiang Song",
      "Silei Xu",
      "Sanqiang Zhao",
      "Ravi Agrawal",
      "Sathish Reddy Indurthi",
      "Chong Xiang",
      "Prateek Mittal",
      "Wenxuan Zhou"
    ],
    "abstract": "Large Language Models (LLMs) are susceptible to security and safety threats,\nsuch as prompt injection, prompt extraction, and harmful requests. One major\ncause of these vulnerabilities is the lack of an instruction hierarchy. Modern\nLLM architectures treat all inputs equally, failing to distinguish between and\nprioritize various types of instructions, such as system messages, user\nprompts, and data. As a result, lower-priority user prompts may override more\ncritical system instructions, including safety protocols. Existing approaches\nto achieving instruction hierarchy, such as delimiters and instruction-based\ntraining, do not address this issue at the architectural level. We introduce\nthe Instructional Segment Embedding (ISE) technique, inspired by BERT, to\nmodern large language models, which embeds instruction priority information\ndirectly into the model. This approach enables models to explicitly\ndifferentiate and prioritize various instruction types, significantly improving\nsafety against malicious prompts that attempt to override priority rules. Our\nexperiments on the Structured Query and Instruction Hierarchy benchmarks\ndemonstrate an average robust accuracy increase of up to 15.75% and 18.68%,\nrespectively. Furthermore, we observe an improvement in instruction-following\ncapability of up to 4.1% evaluated on AlpacaEval. Overall, our approach offers\na promising direction for enhancing the safety and effectiveness of LLM\narchitectures.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2410.09102v2",
    "published_date": "2024-10-09 12:52:41 UTC",
    "updated_date": "2025-03-01 19:06:35 UTC"
  },
  {
    "arxiv_id": "2410.06819v1",
    "title": "Dynamic Neural Potential Field: Online Trajectory Optimization in Presence of Moving Obstacles",
    "authors": [
      "Aleksey Staroverov",
      "Muhammad Alhaddad",
      "Aditya Narendra",
      "Konstantin Mironov",
      "Aleksandr Panov"
    ],
    "abstract": "We address a task of local trajectory planning for the mobile robot in the\npresence of static and dynamic obstacles. Local trajectory is obtained as a\nnumerical solution of the Model Predictive Control (MPC) problem. Collision\navoidance may be provided by adding repulsive potential of the obstacles to the\ncost function of MPC. We develop an approach, where repulsive potential is\nestimated by the neural model. We propose and explore three possible strategies\nof handling dynamic obstacles. First, environment with dynamic obstacles is\nconsidered as a sequence of static environments. Second, the neural model\npredict a sequence of repulsive potential at once. Third, the neural model\npredict future repulsive potential step by step in autoregressive mode. We\nimplement these strategies and compare it with CIAO* and MPPI using BenchMR\nframework. First two strategies showed higher performance than CIAO* and MPPI\nwhile preserving safety constraints. The third strategy was a bit slower,\nhowever it still satisfy time limits. We deploy our approach on Husky UGV\nmobile platform, which move through the office corridors under proposed MPC\nlocal trajectory planner. The code and trained models are available at\n\\url{https://github.com/CognitiveAISystems/Dynamic-Neural-Potential-Field}.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06819v1",
    "published_date": "2024-10-09 12:27:09 UTC",
    "updated_date": "2024-10-09 12:27:09 UTC"
  },
  {
    "arxiv_id": "2410.06818v1",
    "title": "An Improved Approach for Cardiac MRI Segmentation based on 3D UNet Combined with Papillary Muscle Exclusion",
    "authors": [
      "Narjes Benameur",
      "Ramzi Mahmoudi",
      "Mohamed Deriche",
      "Amira fayouka",
      "Imene Masmoudi",
      "Nessrine Zoghlami"
    ],
    "abstract": "Left ventricular ejection fraction (LVEF) is the most important clinical\nparameter of cardiovascular function. The accuracy in estimating this parameter\nis highly dependent upon the precise segmentation of the left ventricle (LV)\nstructure at the end diastole and systole phases. Therefore, it is crucial to\ndevelop robust algorithms for the precise segmentation of the heart structure\nduring different phases. Methodology: In this work, an improved 3D UNet model\nis introduced to segment the myocardium and LV, while excluding papillary\nmuscles, as per the recommendation of the Society for Cardiovascular Magnetic\nResonance. For the practical testing of the proposed framework, a total of\n8,400 cardiac MRI images were collected and analysed from the military hospital\nin Tunis (HMPIT), as well as the popular ACDC public dataset. As performance\nmetrics, we used the Dice coefficient and the F1 score for validation/testing\nof the LV and the myocardium segmentation. Results: The data was split into\n70%, 10%, and 20% for training, validation, and testing, respectively. It is\nworth noting that the proposed segmentation model was tested across three axis\nviews: basal, medio basal and apical at two different cardiac phases: end\ndiastole and end systole instances. The experimental results showed a Dice\nindex of 0.965 and 0.945, and an F1 score of 0.801 and 0.799, at the end\ndiastolic and systolic phases, respectively. Additionally, clinical evaluation\noutcomes revealed a significant difference in the LVEF and other clinical\nparameters when the papillary muscles were included or excluded.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06818v1",
    "published_date": "2024-10-09 12:19:58 UTC",
    "updated_date": "2024-10-09 12:19:58 UTC"
  },
  {
    "arxiv_id": "2410.06816v2",
    "title": "On the Expressiveness of Multi-Neuron Convex Relaxations",
    "authors": [
      "Yuhao Mao",
      "Yani Zhang",
      "Martin Vechev"
    ],
    "abstract": "To provide robustness guarantees, neural network certification methods\nheavily rely on convex relaxations. The imprecision of these convex\nrelaxations, however, is a major obstacle: even the most precise single-neuron\nrelaxation is incomplete for general ReLU networks, a phenomenon referred to as\nthe single-neuron convex barrier. While heuristic instantiations of\nmulti-neuron relaxations have been proposed to circumvent this barrier in\npractice, their theoretical properties remain largely unknown. In this work, we\nconduct the first rigorous study of the expressiveness of multi-neuron\nrelaxations. We first show that the ``$\\max$'' function in $\\mathbb{R}^d$ can\nbe encoded by a ReLU network and exactly bounded by a multi-neuron relaxation,\nwhich is impossible for any single-neuron relaxation. Further, we prove that\nmulti-neuron relaxations can be turned into complete verifiers by\nsemantic-preserving structural transformations or by input space partitioning\nthat enjoys improved worst-case partition complexity. We also show that without\nthese augmentations, the completeness guarantee can no longer be obtained, and\nthe relaxation error of every multi-neuron relaxation can be unbounded. To the\nbest of our knowledge, this is the first work to provide an extensive\ncharacterization of multi-neuron relaxations and their expressiveness in neural\nnetwork certification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06816v2",
    "published_date": "2024-10-09 12:14:24 UTC",
    "updated_date": "2025-01-31 14:04:29 UTC"
  },
  {
    "arxiv_id": "2410.06814v1",
    "title": "Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning",
    "authors": [
      "Qiang Hu",
      "Hengxiang Zhang",
      "Hongxin Wei"
    ],
    "abstract": "Over-parameterized models are typically vulnerable to membership inference\nattacks, which aim to determine whether a specific sample is included in the\ntraining of a given model. Previous Weight regularizations (e.g., L1\nregularization) typically impose uniform penalties on all parameters, leading\nto a suboptimal tradeoff between model utility and privacy. In this work, we\nfirst show that only a small fraction of parameters substantially impact the\nprivacy risk. In light of this, we propose Privacy-aware Sparsity Tuning\n(PAST), a simple fix to the L1 Regularization, by employing adaptive penalties\nto different parameters. Our key idea behind PAST is to promote sparsity in\nparameters that significantly contribute to privacy leakage. In particular, we\nconstruct the adaptive weight for each parameter based on its privacy\nsensitivity, i.e., the gradient of the loss gap with respect to the parameter.\nUsing PAST, the network shrinks the loss gap between members and non-members,\nleading to strong resistance to privacy attacks. Extensive experiments\ndemonstrate the superiority of PAST, achieving a state-of-the-art balance in\nthe privacy-utility trade-off.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06814v1",
    "published_date": "2024-10-09 12:13:49 UTC",
    "updated_date": "2024-10-09 12:13:49 UTC"
  },
  {
    "arxiv_id": "2410.11871v2",
    "title": "TinyClick: Single-Turn Agent for Empowering GUI Automation",
    "authors": [
      "Pawel Pawlowski",
      "Krystian Zawistowski",
      "Wojciech Lapacz",
      "Marcin Skorupa",
      "Adam Wiacek",
      "Sebastien Postansque",
      "Jakub Hoscilowicz"
    ],
    "abstract": "We present a single-turn agent for graphical user interface (GUI) interaction\ntasks, using Vision-Language Model Florence-2-Base. The agent's primary task is\nidentifying the screen coordinates of the UI element corresponding to the\nuser's command. It demonstrates strong performance on Screenspot and OmniAct,\nwhile maintaining a compact size of 0.27B parameters and minimal latency.\nRelevant improvement comes from multi-task training and MLLM-based data\naugmentation. Manually annotated corpora are scarce, but we show that MLLM\naugmentation might produce better results. On Screenspot and OmniAct, our model\noutperforms both GUI-specific models (e.g., SeeClick) and MLLMs (e.g., GPT-4V).",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "The model is available at huggingface.co/Samsung/TinyClick",
    "pdf_url": "http://arxiv.org/pdf/2410.11871v2",
    "published_date": "2024-10-09 12:06:43 UTC",
    "updated_date": "2024-10-17 08:03:19 UTC"
  },
  {
    "arxiv_id": "2410.10879v2",
    "title": "Enhancing Vision-Language Model Pre-training with Image-text Pair Pruning Based on Word Frequency",
    "authors": [
      "Mingliang Liang",
      "Martha Larson"
    ],
    "abstract": "We propose Word-Frequency-based Image-Text Pair Pruning (WFPP), a novel data\npruning method that improves the efficiency of VLMs. Unlike MetaCLIP, our\nmethod does not need metadata for pruning, but selects text-image pairs to\nprune based on the content of the text. Specifically, WFPP prunes text-image\npairs containing high-frequency words across the entire training dataset. The\neffect of WFPP is to reduce the dominance of frequent words. The result a\nbetter balanced word-frequency distribution in the dataset, which is known to\nimprove the training of word embedding models. After pre-training on the pruned\nsubset, we fine-tuned the model on the entire dataset for one additional epoch\nto achieve better performance. Our experiments demonstrate that applying WFPP\nwhen training a CLIP model improves performance on a wide range of downstream\ntasks. WFPP also provides the advantage of speeding up pre-training by using\nfewer samples. Additionally, we analyze the training data before and after\npruning to visualize how WFPP changes the balance of word frequencies. We hope\nour work encourages researchers to consider the distribution of words in the\ntraining data when pre-training VLMs, not limited to CLIP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10879v2",
    "published_date": "2024-10-09 11:54:41 UTC",
    "updated_date": "2024-12-10 13:00:22 UTC"
  },
  {
    "arxiv_id": "2410.06796v1",
    "title": "Diffuse or Confuse: A Diffusion Deepfake Speech Dataset",
    "authors": [
      "Anton Firc",
      "Kamil Malinka",
      "Petr Hanáček"
    ],
    "abstract": "Advancements in artificial intelligence and machine learning have\nsignificantly improved synthetic speech generation. This paper explores\ndiffusion models, a novel method for creating realistic synthetic speech. We\ncreate a diffusion dataset using available tools and pretrained models.\nAdditionally, this study assesses the quality of diffusion-generated deepfakes\nversus non-diffusion ones and their potential threat to current deepfake\ndetection systems. Findings indicate that the detection of diffusion-based\ndeepfakes is generally comparable to non-diffusion deepfakes, with some\nvariability based on detector architecture. Re-vocoding with diffusion vocoders\nshows minimal impact, and the overall speech quality is comparable to\nnon-diffusion methods.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "I.2.7"
    ],
    "primary_category": "cs.CR",
    "comment": "Presented at International Conference of the Biometrics Special\n  Interest Group (BIOSIG 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.06796v1",
    "published_date": "2024-10-09 11:51:08 UTC",
    "updated_date": "2024-10-09 11:51:08 UTC"
  },
  {
    "arxiv_id": "2410.18985v1",
    "title": "rECGnition_v1.0: Arrhythmia detection using cardiologist-inspired multi-modal architecture incorporating demographic attributes in ECG",
    "authors": [
      "Shreya Srivastava",
      "Durgesh Kumar",
      "Jatin Bedi",
      "Sandeep Seth",
      "Deepak Sharma"
    ],
    "abstract": "A substantial amount of variability in ECG manifested due to patient\ncharacteristics hinders the adoption of automated analysis algorithms in\nclinical practice. None of the ECG annotators developed till date consider the\ncharacteristics of the patients in a multi-modal architecture. We employed the\nXGBoost model to analyze the UCI Arrhythmia dataset, linking patient\ncharacteristics to ECG morphological changes. The model accurately classified\npatient gender using discriminative ECG features with 87.75% confidence. We\npropose a novel multi-modal methodology for ECG analysis and arrhythmia\nclassification that can help defy the variability in ECG related to\npatient-specific conditions. This deep learning algorithm, named\nrECGnition_v1.0 (robust ECG abnormality detection Version 1), fuses Beat\nMorphology with Patient Characteristics to create a discriminative feature map\nthat understands the internal correlation between both modalities. A Squeeze\nand Excitation based Patient characteristic Encoding Network (SEPcEnet) has\nbeen introduced, considering the patient's demographics. The trained model\noutperformed the various existing algorithms by achieving the overall F1-score\nof 0.986 for the ten arrhythmia class classification in the MITDB and achieved\nnear perfect prediction scores of ~0.99 for LBBB, RBBB, Premature ventricular\ncontraction beat, Atrial premature beat and Paced beat. Subsequently, the\nmethodology was validated across INCARTDB, EDB and different class groups of\nMITDB using transfer learning. The generalizability test provided F1-scores of\n0.980, 0.946, 0.977, and 0.980 for INCARTDB, EDB, MITDB AAMI, and MITDB Normal\nvs. Abnormal Classification, respectively. Therefore, with a more enhanced and\ncomprehensive understanding of the patient being examined and their ECG for\ndiverse CVD manifestations, the proposed rECGnition_v1.0 algorithm paves the\nway for its deployment in clinics.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18985v1",
    "published_date": "2024-10-09 11:17:02 UTC",
    "updated_date": "2024-10-09 11:17:02 UTC"
  },
  {
    "arxiv_id": "2410.07283v1",
    "title": "Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems",
    "authors": [
      "Donghyun Lee",
      "Mo Tiwari"
    ],
    "abstract": "As Large Language Models (LLMs) grow increasingly powerful, multi-agent\nsystems are becoming more prevalent in modern AI applications. Most safety\nresearch, however, has focused on vulnerabilities in single-agent LLMs. These\ninclude prompt injection attacks, where malicious prompts embedded in external\ncontent trick the LLM into executing unintended or harmful actions,\ncompromising the victim's application. In this paper, we reveal a more\ndangerous vector: LLM-to-LLM prompt injection within multi-agent systems. We\nintroduce Prompt Infection, a novel attack where malicious prompts\nself-replicate across interconnected agents, behaving much like a computer\nvirus. This attack poses severe threats, including data theft, scams,\nmisinformation, and system-wide disruption, all while propagating silently\nthrough the system. Our extensive experiments demonstrate that multi-agent\nsystems are highly susceptible, even when agents do not publicly share all\ncommunications. To address this, we propose LLM Tagging, a defense mechanism\nthat, when combined with existing safeguards, significantly mitigates infection\nspread. This work underscores the urgent need for advanced security measures as\nmulti-agent LLM systems become more widely adopted.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07283v1",
    "published_date": "2024-10-09 11:01:29 UTC",
    "updated_date": "2024-10-09 11:01:29 UTC"
  },
  {
    "arxiv_id": "2410.09099v2",
    "title": "Adaptive Active Inference Agents for Heterogeneous and Lifelong Federated Learning",
    "authors": [
      "Anastasiya Danilenka",
      "Alireza Furutanpey",
      "Victor Casamayor Pujol",
      "Boris Sedlak",
      "Anna Lackinger",
      "Maria Ganzha",
      "Marcin Paprzycki",
      "Schahram Dustdar"
    ],
    "abstract": "Handling heterogeneity and unpredictability are two core problems in\npervasive computing. The challenge is to seamlessly integrate devices with\nvarying computational resources in a dynamic environment to form a cohesive\nsystem that can fulfill the needs of all participants. Existing work on\nadaptive systems typically focuses on optimizing individual variables or\nlow-level Service Level Objectives (SLOs), such as constraining the usage of\nspecific resources. While low-level control mechanisms permit fine-grained\ncontrol over a system, they introduce considerable complexity, particularly in\ndynamic environments. To this end, we propose drawing from Active Inference\n(AIF), a neuroscientific framework for designing adaptive agents. Specifically,\nwe introduce a conceptual agent for heterogeneous pervasive systems that\npermits setting global systems constraints as high-level SLOs. Instead of\nmanually setting low-level SLOs, the system finds an equilibrium that can adapt\nto environmental changes. We demonstrate the viability of our AIF agents with\nan extensive experiment design, using heterogeneous and lifelong federated\nlearning as an application scenario. We conduct our experiments on a physical\ntestbed of devices with different resource types and vendor specifications. The\nresults provide convincing evidence that an AIF agent can adapt a system to\nenvironmental changes. In particular, the AIF agent can balance competing SLOs\nin resource heterogeneous environments to ensure up to 98% fulfillment rate.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, double column, 17 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.09099v2",
    "published_date": "2024-10-09 10:43:29 UTC",
    "updated_date": "2025-03-08 16:51:00 UTC"
  },
  {
    "arxiv_id": "2410.06735v1",
    "title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?",
    "authors": [
      "Fumiya Uchiyama",
      "Takeshi Kojima",
      "Andrew Gambardella",
      "Qi Cao",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "abstract": "Recent large language models (LLMs) have demonstrated remarkable\ngeneralization abilities in mathematics and logical reasoning tasks. Prior\nresearch indicates that LLMs pre-trained with programming language data exhibit\nhigh mathematical and reasoning abilities; however, this causal relationship\nhas not been rigorously tested. Our research aims to verify which programming\nlanguages and features during pre-training affect logical inference\nperformance. Specifically, we pre-trained decoder-based language models from\nscratch using datasets from ten programming languages (e.g., Python, C, Java)\nand three natural language datasets (Wikipedia, Fineweb, C4) under identical\nconditions. Thereafter, we evaluated the trained models in a few-shot\nin-context learning setting on logical reasoning tasks: FLD and bAbi, which do\nnot require commonsense or world knowledge. The results demonstrate that nearly\nall models trained with programming languages consistently outperform those\ntrained with natural languages, indicating that programming languages contain\nfactors that elicit logic inference performance. In addition, we found that\nmodels trained with programming languages exhibit a better ability to follow\ninstructions compared to those trained with natural languages. Further analysis\nreveals that the depth of Abstract Syntax Trees representing parsed results of\nprograms also affects logical reasoning performance. These findings will offer\ninsights into the essential elements of pre-training for acquiring the\nfoundational abilities of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06735v1",
    "published_date": "2024-10-09 10:13:13 UTC",
    "updated_date": "2024-10-09 10:13:13 UTC"
  },
  {
    "arxiv_id": "2410.10878v2",
    "title": "Herald: A Natural Language Annotated Lean 4 Dataset",
    "authors": [
      "Guoxiong Gao",
      "Yutong Wang",
      "Jiedong Jiang",
      "Qi Gao",
      "Zihan Qin",
      "Tianyi Xu",
      "Bin Dong"
    ],
    "abstract": "Verifiable formal languages like Lean have profoundly impacted mathematical\nreasoning, particularly through the use of large language models (LLMs) for\nautomated reasoning. A significant challenge in training LLMs for these formal\nlanguages is the lack of parallel datasets that align natural language with\nformal language proofs. To address this challenge, this paper introduces a\nnovel framework for translating the Mathlib4 corpus (a unified library of\nmathematics in formal language Lean 4) into natural language. Building upon\nthis, we employ a dual augmentation strategy that combines tactic-based and\ninformal-based approaches, leveraging the Lean-jixia system, a Lean 4 analyzer.\nWe present the results of this pipeline on Mathlib4 as Herald (Hierarchy and\nRetrieval-based Translated Lean Dataset). We also propose the Herald\nTranslator, which is fine-tuned on Herald. Herald translator achieves a 93.2%\naccuracy (Pass@128) on formalizing statements in the miniF2F-test and a 22.5%\naccuracy on our internal graduate-level textbook dataset, outperforming\nInternLM2-Math-Plus-7B (74.0% and 7.5%) and TheoremLlama (50.1% and 4.0%).\nFurthermore, we propose a section-level translation framework for real-world\napplications. As a direct application of Herald translator, we have\nsuccessfully translated a template section in the Stack project, marking a\nnotable progress in the automatic formalization of graduate-level mathematical\nliterature. Our model, along with the datasets, are open-sourced to the public.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10878v2",
    "published_date": "2024-10-09 10:11:24 UTC",
    "updated_date": "2025-02-27 07:01:28 UTC"
  },
  {
    "arxiv_id": "2410.06733v1",
    "title": "Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles",
    "authors": [
      "Qi Chen",
      "Bowen Zhang",
      "Gang Wang",
      "Qi Wu"
    ],
    "abstract": "While advancements in NLP have significantly improved the performance of\nLarge Language Models (LLMs) on tasks requiring vertical thinking, their\nlateral thinking capabilities remain under-explored and challenging to measure\ndue to the complexity of assessing creative thought processes and the scarcity\nof relevant data. To address these challenges, we introduce SPLAT, a benchmark\nleveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.\nThis benchmark, containing 975 graded situation puzzles across three difficulty\nlevels, employs a new multi-turn player-judge framework instead of the\ntraditional model-based evaluation, which often necessitates a stronger\nevaluation model. This framework simulates an interactive game where the model\n(player) asks the evaluation model (judge) questions about an incomplete story\nto infer the full scenario. The judge answers based on a detailed reference\nscenario or evaluates if the player's predictions align with the reference one.\nThis approach lessens dependence on more robust evaluation models, enabling the\nassessment of state-of-the-art LLMs. The experiments demonstrate that a robust\nevaluation model, such as WizardLM-2, closely matches human judgements in both\nintermediate question-answering and final scenario accuracy, achieving over 80%\nagreement-similar to the agreement levels among humans. Furthermore, applying\ndata and reasoning processes from our benchmark to other lateral\nthinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to\nperformance enhancements. This suggests that our benchmark effectively\nevaluates and elicits the lateral thinking abilities of LLMs. Code is available\nat: https://github.com/chenqi008/LateralThinking.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.06733v1",
    "published_date": "2024-10-09 10:09:11 UTC",
    "updated_date": "2024-10-09 10:09:11 UTC"
  },
  {
    "arxiv_id": "2410.10877v2",
    "title": "Improving Data Efficiency via Curating LLM-Driven Rating Systems",
    "authors": [
      "Jinlong Pang",
      "Jiaheng Wei",
      "Ankit Parag Shah",
      "Zhaowei Zhu",
      "Yaxuan Wang",
      "Chen Qian",
      "Yang Liu",
      "Yujia Bao",
      "Wei Wei"
    ],
    "abstract": "Instruction tuning is critical for adapting large language models (LLMs) to\ndownstream tasks, and recent studies have demonstrated that small amounts of\nhuman-curated data can outperform larger datasets, challenging traditional data\nscaling laws. While LLM-based data quality rating systems offer a\ncost-effective alternative to human annotation, they often suffer from\ninaccuracies and biases, even in powerful models like GPT-4. In this work, we\nintroduce DS2, a Diversity-aware Score curation method for Data Selection. By\nsystematically modeling error patterns through a score transition matrix, DS2\ncorrects LLM-based scores and promotes diversity in the selected data samples.\nOur approach shows that a curated subset (just 3.3% of the original dataset)\noutperforms full-scale datasets (300k samples) across various machine-alignment\nbenchmarks, and matches or surpasses human-aligned datasets such as LIMA with\nthe same sample size (1k samples). These findings challenge conventional data\nscaling assumptions, highlighting that redundant, low-quality samples can\ndegrade performance and reaffirming that \"more can be less.\"",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10877v2",
    "published_date": "2024-10-09 10:07:55 UTC",
    "updated_date": "2025-03-05 23:56:10 UTC"
  },
  {
    "arxiv_id": "2410.06725v1",
    "title": "Evaluating the Impact of Point Cloud Colorization on Semantic Segmentation Accuracy",
    "authors": [
      "Qinfeng Zhu",
      "Jiaze Cao",
      "Yuanzhi Cai",
      "Lei Fan"
    ],
    "abstract": "Point cloud semantic segmentation, the process of classifying each point into\npredefined categories, is essential for 3D scene understanding. While\nimage-based segmentation is widely adopted due to its maturity, methods relying\nsolely on RGB information often suffer from degraded performance due to color\ninaccuracies. Recent advancements have incorporated additional features such as\nintensity and geometric information, yet RGB channels continue to negatively\nimpact segmentation accuracy when errors in colorization occur. Despite this,\nprevious studies have not rigorously quantified the effects of erroneous\ncolorization on segmentation performance. In this paper, we propose a novel\nstatistical approach to evaluate the impact of inaccurate RGB information on\nimage-based point cloud segmentation. We categorize RGB inaccuracies into two\ntypes: incorrect color information and similar color information. Our results\ndemonstrate that both types of color inaccuracies significantly degrade\nsegmentation accuracy, with similar color errors particularly affecting the\nextraction of geometric features. These findings highlight the critical need to\nreassess the role of RGB information in point cloud segmentation and its\nimplications for future algorithm design.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by 2024 IEEE 8th International Conference on Vision, Image\n  and Signal Processing",
    "pdf_url": "http://arxiv.org/pdf/2410.06725v1",
    "published_date": "2024-10-09 09:46:53 UTC",
    "updated_date": "2024-10-09 09:46:53 UTC"
  },
  {
    "arxiv_id": "2410.06719v3",
    "title": "Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques",
    "authors": [
      "Benyuan Meng",
      "Qianqian Xu",
      "Zitai Wang",
      "Zhiyong Yang",
      "Xiaochun Cao",
      "Qingming Huang"
    ],
    "abstract": "Diffusion models are powerful generative models, and this capability can also\nbe applied to discrimination. The inner activations of a pre-trained diffusion\nmodel can serve as features for discriminative tasks, namely, diffusion\nfeature. We discover that diffusion feature has been hindered by a hidden yet\nuniversal phenomenon that we call content shift. To be specific, there are\ncontent differences between features and the input image, such as the exact\nshape of a certain object. We locate the cause of content shift as one inherent\ncharacteristic of diffusion models, which suggests the broad existence of this\nphenomenon in diffusion feature. Further empirical study also indicates that\nits negative impact is not negligible even when content shift is not visually\nperceivable. Hence, we propose to suppress content shift to enhance the overall\nquality of diffusion features. Specifically, content shift is related to the\ninformation drift during the process of recovering an image from the noisy\ninput, pointing out the possibility of turning off-the-shelf generation\ntechniques into tools for content shift suppression. We further propose a\npractical guideline named GATE to efficiently evaluate the potential benefit of\na technique and provide an implementation of our methodology. Despite the\nsimplicity, the proposed approach has achieved superior results on various\ntasks and datasets, validating its potential as a generic booster for diffusion\nfeatures. Our code is available at\nhttps://github.com/Darkbblue/diffusion-content-shift.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2410.03558",
    "pdf_url": "http://arxiv.org/pdf/2410.06719v3",
    "published_date": "2024-10-09 09:43:36 UTC",
    "updated_date": "2024-10-18 06:39:27 UTC"
  },
  {
    "arxiv_id": "2410.06707v1",
    "title": "Calibrating Verbalized Probabilities for Large Language Models",
    "authors": [
      "Cheng Wang",
      "Gyuri Szarvas",
      "Georges Balazs",
      "Pavel Danchenko",
      "Patrick Ernst"
    ],
    "abstract": "Calibrating verbalized probabilities presents a novel approach for reliably\nassessing and leveraging outputs from black-box Large Language Models (LLMs).\nRecent methods have demonstrated improved calibration by applying techniques\nlike Platt scaling or temperature scaling to the confidence scores generated by\nLLMs. In this paper, we explore the calibration of verbalized probability\ndistributions for discriminative tasks. First, we investigate the capability of\nLLMs to generate probability distributions over categorical labels. We\ntheoretically and empirically identify the issue of re-softmax arising from the\nscaling of verbalized probabilities, and propose using the invert softmax trick\nto approximate the \"logit\" by inverting verbalized probabilities. Through\nextensive evaluation on three public datasets, we demonstrate: (1) the robust\ncapability of LLMs in generating class distributions, and (2) the effectiveness\nof the invert softmax trick in estimating logits, which, in turn, facilitates\npost-calibration adjustments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.06707v1",
    "published_date": "2024-10-09 09:20:24 UTC",
    "updated_date": "2024-10-09 09:20:24 UTC"
  },
  {
    "arxiv_id": "2410.06704v1",
    "title": "PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs",
    "authors": [
      "Krishna Kanth Nakka",
      "Ahmed Frikha",
      "Ricardo Mendes",
      "Xue Jiang",
      "Xuebing Zhou"
    ],
    "abstract": "In this work, we introduce PII-Scope, a comprehensive benchmark designed to\nevaluate state-of-the-art methodologies for PII extraction attacks targeting\nLLMs across diverse threat settings. Our study provides a deeper understanding\nof these attacks by uncovering several hyperparameters (e.g., demonstration\nselection) crucial to their effectiveness. Building on this understanding, we\nextend our study to more realistic attack scenarios, exploring PII attacks that\nemploy advanced adversarial strategies, including repeated and diverse\nquerying, and leveraging iterative learning for continual PII extraction.\nThrough extensive experimentation, our results reveal a notable underestimation\nof PII leakage in existing single-query attacks. In fact, we show that with\nsophisticated adversarial capabilities and a limited query budget, PII\nextraction rates can increase by up to fivefold when targeting the pretrained\nmodel. Moreover, we evaluate PII leakage on finetuned models, showing that they\nare more vulnerable to leakage than pretrained models. Overall, our work\nestablishes a rigorous empirical benchmark for PII extraction attacks in\nrealistic threat scenarios and provides a strong foundation for developing\neffective mitigation strategies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06704v1",
    "published_date": "2024-10-09 09:16:25 UTC",
    "updated_date": "2024-10-09 09:16:25 UTC"
  },
  {
    "arxiv_id": "2410.06703v4",
    "title": "ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web Agents",
    "authors": [
      "Ido Levy",
      "Ben Wiesel",
      "Sami Marreed",
      "Alon Oved",
      "Avi Yaeli",
      "Segev Shlomov"
    ],
    "abstract": "Autonomous web agents solve complex browsing tasks, yet existing benchmarks\nmeasure only whether an agent finishes a task, ignoring whether it does so\nsafely or in a way enterprises can trust. To integrate these agents into\ncritical workflows, safety and trustworthiness (ST) are prerequisite conditions\nfor adoption. We introduce \\textbf{\\textsc{ST-WebAgentBench}}, a configurable\nand easily extensible suite for evaluating web agent ST across realistic\nenterprise scenarios. Each of its 222 tasks is paired with ST policies, concise\nrules that encode constraints, and is scored along six orthogonal dimensions\n(e.g., user consent, robustness). Beyond raw task success, we propose the\n\\textit{Completion Under Policy} (\\textit{CuP}) metric, which credits only\ncompletions that respect all applicable policies, and the \\textit{Risk Ratio},\nwhich quantifies ST breaches across dimensions. Evaluating three open\nstate-of-the-art agents reveals that their average CuP is less than two-thirds\nof their nominal completion rate, exposing critical safety gaps. By releasing\ncode, evaluation templates, and a policy-authoring interface,\n\\href{https://sites.google.com/view/st-webagentbench/home}{\\textsc{ST-WebAgentBench}}\nprovides an actionable first step toward deploying trustworthy web agents at\nscale.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06703v4",
    "published_date": "2024-10-09 09:13:38 UTC",
    "updated_date": "2025-05-19 08:50:49 UTC"
  },
  {
    "arxiv_id": "2410.06699v1",
    "title": "Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models",
    "authors": [
      "Yubo Wang",
      "Chaohu Liu",
      "Yanqiu Qu",
      "Haoyu Cao",
      "Deqiang Jiang",
      "Linli Xu"
    ],
    "abstract": "Large vision-language models (LVLMs) integrate visual information into large\nlanguage models, showcasing remarkable multi-modal conversational capabilities.\nHowever, the visual modules introduces new challenges in terms of robustness\nfor LVLMs, as attackers can craft adversarial images that are visually clean\nbut may mislead the model to generate incorrect answers. In general, LVLMs rely\non vision encoders to transform images into visual tokens, which are crucial\nfor the language models to perceive image contents effectively. Therefore, we\nare curious about one question: Can LVLMs still generate correct responses when\nthe encoded visual tokens are attacked and disrupting the visual information?\nTo this end, we propose a non-targeted attack method referred to as VT-Attack\n(Visual Tokens Attack), which constructs adversarial examples from multiple\nperspectives, with the goal of comprehensively disrupting feature\nrepresentations and inherent relationships as well as the semantic properties\nof visual tokens output by image encoders. Using only access to the image\nencoder in the proposed attack, the generated adversarial examples exhibit\ntransferability across diverse LVLMs utilizing the same image encoder and\ngenerality across different tasks. Extensive experiments validate the superior\nattack performance of the VT-Attack over baseline methods, demonstrating its\neffectiveness in attacking LVLMs with image encoders, which in turn can provide\nguidance on the robustness of LVLMs, particularly in terms of the stability of\nthe visual feature space.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ACMMM 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.06699v1",
    "published_date": "2024-10-09 09:06:56 UTC",
    "updated_date": "2024-10-09 09:06:56 UTC"
  },
  {
    "arxiv_id": "2410.06681v1",
    "title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
    "authors": [
      "Kai Ebert",
      "Nicolas Alder",
      "Ralf Herbrich",
      "Philipp Hacker"
    ],
    "abstract": "We live in a world that is experiencing an unprecedented boom of AI\napplications that increasingly penetrate and enhance all sectors of private and\npublic life, from education, media, medicine, and mobility to the industrial\nand professional workspace, and -- potentially particularly consequentially --\nrobotics. As this world is simultaneously grappling with climate change, the\nclimate and environmental implications of the development and use of AI have\nbecome an important subject of public and academic debate. In this paper, we\naim to provide guidance on the climate-related regulation for data centers and\nAI specifically, and discuss how to operationalize these requirements. We also\nhighlight challenges and room for improvement, and make a number of policy\nproposals to this end. In particular, we propose a specific interpretation of\nthe AI Act to bring reporting on the previously unadressed energy consumption\nfrom AI inferences back into the scope. We also find that the AI Act fails to\naddress indirect greenhouse gas emissions from AI applications. Furthermore,\nfor the purpose of energy consumption reporting, we compare levels of\nmeasurement within data centers and recommend measurement at the cumulative\nserver level. We also argue for an interpretation of the AI Act that includes\nenvironmental concerns in the mandatory risk assessment (sustainability risk\nassessment, SIA), and provide guidance on its operationalization. The EU data\ncenter regulation proves to be a good first step but requires further\ndevelopment by including binding renewable energy and efficiency targets for\ndata centers. Overall, we make twelve concrete policy proposals, in four main\nareas: Energy and Environmental Reporting Obligations; Legal and Regulatory\nClarifications; Transparency and Accountability Mechanisms; and Future\nFar-Reaching Measures beyond Transparency.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "18 pages, 1 figure, preprint",
    "pdf_url": "http://arxiv.org/pdf/2410.06681v1",
    "published_date": "2024-10-09 08:43:53 UTC",
    "updated_date": "2024-10-09 08:43:53 UTC"
  },
  {
    "arxiv_id": "2410.06678v2",
    "title": "M3Bench: Benchmarking Whole-body Motion Generation for Mobile Manipulation in 3D Scenes",
    "authors": [
      "Zeyu Zhang",
      "Sixu Yan",
      "Muzhi Han",
      "Zaijin Wang",
      "Xinggang Wang",
      "Song-Chun Zhu",
      "Hangxin Liu"
    ],
    "abstract": "We propose M^3Bench, a new benchmark of whole-body motion generation for\nmobile manipulation tasks. Given a 3D scene context, M^3Bench requires an\nembodied agent to understand its configuration, environmental constraints and\ntask objectives, then generate coordinated whole-body motion trajectories for\nobject rearrangement tasks. M^3Bench features 30k object rearrangement tasks\nacross 119 diverse scenes, providing expert demonstrations generated by our\nnewly developed M^3BenchMaker. This automatic data generation tool produces\ncoordinated whole-body motion trajectories from high-level task instructions,\nrequiring only basic scene and robot information. Our benchmark incorporates\nvarious task splits to assess generalization across different dimensions and\nleverages realistic physics simulation for trajectory evaluation. Through\nextensive experimental analyses, we reveal that state-of-the-art models still\nstruggle with coordinated base-arm motion while adhering to environment-context\nand task-specific constraints, highlighting the need to develop new models that\naddress this gap. Through M^3Bench, we aim to facilitate future robotics\nresearch towards more adaptive and capable mobile manipulation in diverse,\nreal-world environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Code and data set will be released after acceptance",
    "pdf_url": "http://arxiv.org/pdf/2410.06678v2",
    "published_date": "2024-10-09 08:38:21 UTC",
    "updated_date": "2024-10-15 03:02:05 UTC"
  },
  {
    "arxiv_id": "2410.06667v2",
    "title": "Large Language Models as Code Executors: An Exploratory Study",
    "authors": [
      "Chenyang Lyu",
      "Lecheng Yan",
      "Rui Xing",
      "Wenxi Li",
      "Younes Samih",
      "Tianbo Ji",
      "Longyue Wang"
    ],
    "abstract": "The capabilities of Large Language Models (LLMs) have significantly evolved,\nextending from natural language processing to complex tasks like code\nunderstanding and generation. We expand the scope of LLMs' capabilities to a\nbroader context, using LLMs to execute code snippets to obtain the output. This\npaper pioneers the exploration of LLMs as code executors, where code snippets\nare directly fed to the models for execution, and outputs are returned. We are\nthe first to comprehensively examine this feasibility across various LLMs,\nincluding OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the\no1 model achieved over 90% accuracy in code execution, while others\ndemonstrated lower accuracy levels. Furthermore, we introduce an Iterative\nInstruction Prompting (IIP) technique that processes code snippets line by\nline, enhancing the accuracy of weaker models by an average of 7.22% (with the\nhighest improvement of 18.96%) and an absolute average improvement of 3.86%\nagainst CoT prompting (with the highest improvement of 19.46%). Our study not\nonly highlights the transformative potential of LLMs in coding but also lays\nthe groundwork for future advancements in automated programming and the\ncompletion of complex tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06667v2",
    "published_date": "2024-10-09 08:23:22 UTC",
    "updated_date": "2024-10-10 05:12:44 UTC"
  },
  {
    "arxiv_id": "2410.06665v3",
    "title": "Revisiting Multi-Permutation Equivariance through the Lens of Irreducible Representations",
    "authors": [
      "Yonatan Sverdlov",
      "Ido Springer",
      "Nadav Dym"
    ],
    "abstract": "This paper explores the characterization of equivariant linear layers for\nrepresentations of permutations and related groups. Unlike traditional\napproaches, which address these problems using parameter-sharing, we consider\nan alternative methodology based on irreducible representations and Schur's\nlemma. Using this methodology, we obtain an alternative derivation for existing\nmodels like DeepSets, 2-IGN graph equivariant networks, and Deep Weight Space\n(DWS) networks. The derivation for DWS networks is significantly simpler than\nthat of previous results.\n  Next, we extend our approach to unaligned symmetric sets, where equivariance\nto the wreath product of groups is required. Previous works have addressed this\nproblem in a rather restrictive setting, in which almost all wreath equivariant\nlayers are Siamese. In contrast, we give a full characterization of layers in\nthis case and show that there is a vast number of additional non-Siamese layers\nin some settings. We also show empirically that these additional non-Siamese\nlayers can improve performance in tasks like graph anomaly detection, weight\nspace alignment, and learning Wasserstein distances. Our code is available at\n\\href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06665v3",
    "published_date": "2024-10-09 08:19:31 UTC",
    "updated_date": "2025-03-06 07:41:25 UTC"
  },
  {
    "arxiv_id": "2410.06664v2",
    "title": "Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning",
    "authors": [
      "Qianli Ma",
      "Xuefei Ning",
      "Dongrui Liu",
      "Li Niu",
      "Linfeng Zhang"
    ],
    "abstract": "Diffusion models are trained by learning a sequence of models that reverse\neach step of noise corruption. Typically, the model parameters are fully shared\nacross multiple timesteps to enhance training efficiency. However, since the\ndenoising tasks differ at each timestep, the gradients computed at different\ntimesteps may conflict, potentially degrading the overall performance of image\ngeneration. To solve this issue, this work proposes a\n\\textbf{De}couple-then-\\textbf{Me}rge (\\textbf{DeMe}) framework, which begins\nwith a pretrained model and finetunes separate models tailored to specific\ntimesteps. We introduce several improved techniques during the finetuning stage\nto promote effective knowledge sharing while minimizing training interference\nacross timesteps. Finally, after finetuning, these separate models can be\nmerged into a single model in the parameter space, ensuring efficient and\npractical inference. Experimental results show significant generation quality\nimprovements upon 6 benchmarks including Stable Diffusion on COCO30K,\nImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10.\nCode is available at \\href{https://github.com/MqLeet/DeMe}{GitHub}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2410.06664v2",
    "published_date": "2024-10-09 08:19:25 UTC",
    "updated_date": "2025-03-14 09:54:17 UTC"
  },
  {
    "arxiv_id": "2410.06652v2",
    "title": "Task-oriented Time Series Imputation Evaluation via Generalized Representers",
    "authors": [
      "Zhixian Wang",
      "Linxiao Yang",
      "Liang Sun",
      "Qingsong Wen",
      "Yi Wang"
    ],
    "abstract": "Time series analysis is widely used in many fields such as power energy,\neconomics, and transportation, including different tasks such as forecasting,\nanomaly detection, classification, etc. Missing values are widely observed in\nthese tasks, and often leading to unpredictable negative effects on existing\nmethods, hindering their further application. In response to this situation,\nexisting time series imputation methods mainly focus on restoring sequences\nbased on their data characteristics, while ignoring the performance of the\nrestored sequences in downstream tasks. Considering different requirements of\ndownstream tasks (e.g., forecasting), this paper proposes an efficient\ndownstream task-oriented time series imputation evaluation approach. By\ncombining time series imputation with neural network models used for downstream\ntasks, the gain of different imputation strategies on downstream tasks is\nestimated without retraining, and the most favorable imputation value for\ndownstream tasks is given by combining different imputation strategies\naccording to the estimated gain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 9 figures, 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.06652v2",
    "published_date": "2024-10-09 08:04:48 UTC",
    "updated_date": "2024-10-10 04:16:14 UTC"
  },
  {
    "arxiv_id": "2410.06651v1",
    "title": "Toward Physics-guided Time Series Embedding",
    "authors": [
      "Jiaxi Hu",
      "Bowen Zhang",
      "Qingsong Wen",
      "Fugee Tsung",
      "Yuxuan Liang"
    ],
    "abstract": "In various scientific and engineering fields, the primary research areas have\nrevolved around physics-based dynamical systems modeling and data-driven time\nseries analysis. According to the embedding theory, dynamical systems and time\nseries can be mutually transformed using observation functions and physical\nreconstruction techniques. Based on this, we propose Embedding Duality Theory,\nwhere the parameterized embedding layer essentially provides a linear\nestimation of the non-linear time series dynamics. This theory enables us to\nbypass the parameterized embedding layer and directly employ physical\nreconstruction techniques to acquire a data embedding representation. Utilizing\nphysical priors results in a 10X reduction in parameters, a 3X increase in\nspeed, and maximum performance boosts of 18% in expert, 22% in few-shot, and\n53\\% in zero-shot tasks without any hyper-parameter tuning. All methods are\nencapsulated as a plug-and-play module",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06651v1",
    "published_date": "2024-10-09 08:04:06 UTC",
    "updated_date": "2024-10-09 08:04:06 UTC"
  },
  {
    "arxiv_id": "2410.06638v3",
    "title": "Subtle Errors Matter: Preference Learning via Error-injected Self-editing",
    "authors": [
      "Kaishuai Xu",
      "Tiezheng Yu",
      "Wenjun Hou",
      "Yi Cheng",
      "Chak Tou Leong",
      "Liangyou Li",
      "Xin Jiang",
      "Lifeng Shang",
      "Qun Liu",
      "Wenjie Li"
    ],
    "abstract": "Large Language Models (LLMs) have exhibited strong mathematical reasoning\nprowess, tackling tasks ranging from basic arithmetic to advanced\ncompetition-level problems. However, frequently occurring subtle yet critical\nerrors, such as miscalculations or incorrect substitutions, limit the LLMs'\nfull potential. Existing studies to improve mathematical ability typically\ninvolve applying preference learning to step-wise solution pairs. Although\nthese methods leverage samples of varying granularity to mitigate reasoning\nerrors, they overlook critical subtle errors. In this work, we propose a novel\npreference learning framework called eRror-Injected Self-Editing (RISE), which\ninjects predefined subtle errors into pivotal tokens in reasoning or\ncomputation steps to construct hard pairs for error mitigation. In detail, RISE\nuses the LLM itself to edit a small number of tokens in the solution, injecting\ndesigned subtle errors. Then, pairs composed of self-edited solutions and their\ncorresponding correct ones, along with pairs of correct and incorrect solutions\nobtained through sampling, are used together for subtle error-aware DPO\ntraining. Compared with other preference learning methods, RISE further refines\nthe training objective without requiring fine-grained sampling or preference\nannotation. Extensive experiments validate the effectiveness of RISE, with\npreference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0%\non GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect\nof error mitigation extends from mathematical reasoning to logical reasoning\nand code generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06638v3",
    "published_date": "2024-10-09 07:43:38 UTC",
    "updated_date": "2025-03-03 07:09:42 UTC"
  },
  {
    "arxiv_id": "2410.06621v1",
    "title": "Effective Exploration Based on the Structural Information Principles",
    "authors": [
      "Xianghua Zeng",
      "Hao Peng",
      "Angsheng Li"
    ],
    "abstract": "Traditional information theory provides a valuable foundation for\nReinforcement Learning, particularly through representation learning and\nentropy maximization for agent exploration. However, existing methods primarily\nconcentrate on modeling the uncertainty associated with RL's random variables,\nneglecting the inherent structure within the state and action spaces. In this\npaper, we propose a novel Structural Information principles-based Effective\nExploration framework, namely SI2E. Structural mutual information between two\nvariables is defined to address the single-variable limitation in structural\ninformation, and an innovative embedding principle is presented to capture\ndynamics-relevant state-action representations. The SI2E analyzes value\ndifferences in the agent's policy between state-action pairs and minimizes\nstructural entropy to derive the hierarchical state-action structure, referred\nto as the encoding tree. Under this tree structure, value-conditional\nstructural entropy is defined and maximized to design an intrinsic reward\nmechanism that avoids redundant transitions and promotes enhanced coverage in\nthe state-action space. Theoretical connections are established between SI2E\nand classical information-theoretic methodologies, highlighting our framework's\nrationality and advantage. Comprehensive evaluations in the MiniGrid,\nMetaWorld, and DeepMind Control Suite benchmarks demonstrate that SI2E\nsignificantly outperforms state-of-the-art exploration baselines regarding\nfinal performance and sample efficiency, with maximum improvements of 37.63%\nand 60.25%, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages in main paper and 15 pages in appendix",
    "pdf_url": "http://arxiv.org/pdf/2410.06621v1",
    "published_date": "2024-10-09 07:19:16 UTC",
    "updated_date": "2024-10-09 07:19:16 UTC"
  },
  {
    "arxiv_id": "2410.06617v5",
    "title": "Learning Evolving Tools for Large Language Models",
    "authors": [
      "Guoxin Chen",
      "Zhong Zhang",
      "Xin Cong",
      "Fangda Guo",
      "Yesai Wu",
      "Yankai Lin",
      "Wenzheng Feng",
      "Yasheng Wang"
    ],
    "abstract": "Tool learning enables large language models (LLMs) to interact with external\ntools and APIs, greatly expanding the application scope of LLMs. However, due\nto the dynamic nature of external environments, these tools and APIs may become\noutdated over time, preventing LLMs from correctly invoking tools. Existing\nresearch primarily focuses on static environments and overlooks this issue,\nlimiting the adaptability of LLMs in real-world applications. In this paper, we\npropose ToolEVO, a novel framework designed to enhance the adaptive and\nreflective capabilities of LLMs against tool variability. By leveraging Monte\nCarlo Tree Search, ToolEVO facilitates active exploration and interaction of\nLLMs within dynamic environments, allowing for autonomous self-reflection and\nself-updating of tool usage based on environmental feedback. Additionally, we\nintroduce ToolQA-D, a benchmark specifically designed to evaluate the impact of\ntool variability. Extensive experiments demonstrate the effectiveness and\nstability of our approach, highlighting the importance of adaptability to tool\nvariability for effective tool learning. Code:\nhttps://github.com/Chen-GX/ToolEVO",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera ready version for ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.06617v5",
    "published_date": "2024-10-09 07:14:45 UTC",
    "updated_date": "2025-02-28 04:37:07 UTC"
  },
  {
    "arxiv_id": "2410.07278v2",
    "title": "PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models",
    "authors": [
      "Yingen Liu",
      "Fan Wu",
      "Ruihui Li",
      "Zhuo Tang",
      "Kenli Li"
    ],
    "abstract": "Multimodal large language models (MLLMs) demonstrate strong performance\nacross visual tasks, but their efficiency is hindered by significant\ncomputational and memory demands from processing long contexts in multimodal\ninputs. To address this, we introduce PAR (Prompt-Aware Token Reduction), a\nnovel and plug-and-play approach that reduces visual tokens efficiently without\ncompromising model performance. Unlike previous methods that rely heavily on\nattention mechanisms and overlooking cross-modal interactions , we uses a\nprompt-aware strategy to adpative identify and cluster essential visual tokens.\nPAR categorizes visual context redundancy into two types: external and\ninternal. External redundancy is minimized through semantic retrieval, while\ninternal redundancy is addressed using a token routing mechanism. This method\nsubstantially reduces computational load without requiring additional training\nor complex architectural modifications. \\textbf{Experimental results\ndemonstrate that across various visual question answering tasks, PAR reduces\nFLOPs by 83\\% with a compression ratio of 89\\%, while retaining 97\\% of\nbaseline accuracy.} The adaptive design of PAR achieves a 2x token reduction\nratio compared to prior approaches, enabling a better balance between\nperformance and efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures,3 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.07278v2",
    "published_date": "2024-10-09 07:13:22 UTC",
    "updated_date": "2024-12-02 08:43:33 UTC"
  },
  {
    "arxiv_id": "2410.06614v2",
    "title": "Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification for Visual Place Recognition with Vision Transformers",
    "authors": [
      "Stephen Hausler",
      "Peyman Moghadam"
    ],
    "abstract": "In this work we propose a novel joint training method for Visual Place\nRecognition (VPR), which simultaneously learns a global descriptor and a pair\nclassifier for re-ranking. The pair classifier can predict whether a given pair\nof images are from the same place or not. The network only comprises Vision\nTransformer components for both the encoder and the pair classifier, and both\ncomponents are trained using their respective class tokens. In existing VPR\nmethods, typically the network is initialized using pre-trained weights from a\ngeneric image dataset such as ImageNet. In this work we propose an alternative\npre-training strategy, by using Siamese Masked Image Modelling as a\npre-training task. We propose a Place-aware image sampling procedure from a\ncollection of large VPR datasets for pre-training our model, to learn visual\nfeatures tuned specifically for VPR. By re-using the Mask Image Modelling\nencoder and decoder weights in the second stage of training, Pair-VPR can\nachieve state-of-the-art VPR performance across five benchmark datasets with a\nViT-B encoder, along with further improvements in localization recall with\nlarger encoders. The Pair-VPR website is:\nhttps://csiro-robotics.github.io/Pair-VPR.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06614v2",
    "published_date": "2024-10-09 07:09:46 UTC",
    "updated_date": "2025-03-02 08:59:29 UTC"
  },
  {
    "arxiv_id": "2410.06608v1",
    "title": "Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS",
    "authors": [
      "Onkar Kishor Susladkar",
      "Vishesh Tripathi",
      "Biddwan Ahmed"
    ],
    "abstract": "This research introduces a comprehensive Bahasa text-to-speech (TTS) dataset\nand a novel TTS model, EnGen-TTS, designed to enhance the quality and\nversatility of synthetic speech in the Bahasa language. The dataset, spanning\n\\textasciitilde55.0 hours and 52K audio recordings, integrates diverse textual\nsources, ensuring linguistic richness. A meticulous recording setup captures\nthe nuances of Bahasa phonetics, employing professional equipment to ensure\nhigh-fidelity audio samples. Statistical analysis reveals the dataset's scale\nand diversity, laying the foundation for model training and evaluation. The\nproposed EnGen-TTS model performs better than established baselines, achieving\na Mean Opinion Score (MOS) of 4.45 $\\pm$ 0.13. Additionally, our investigation\non real-time factor and model size highlights EnGen-TTS as a compelling choice,\nwith efficient performance. This research marks a significant advancement in\nBahasa TTS technology, with implications for diverse language applications.\nLink to Generated Samples: \\url{https://bahasa-harmony-comp.vercel.app/}",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06608v1",
    "published_date": "2024-10-09 07:01:05 UTC",
    "updated_date": "2024-10-09 07:01:05 UTC"
  },
  {
    "arxiv_id": "2410.07277v1",
    "title": "Swin-BERT: A Feature Fusion System designed for Speech-based Alzheimer's Dementia Detection",
    "authors": [
      "Yilin Pan",
      "Yanpei Shi",
      "Yijia Zhang",
      "Mingyu Lu"
    ],
    "abstract": "Speech is usually used for constructing an automatic Alzheimer's dementia\n(AD) detection system, as the acoustic and linguistic abilities show a decline\nin people living with AD at the early stages. However, speech includes not only\nAD-related local and global information but also other information unrelated to\ncognitive status, such as age and gender. In this paper, we propose a\nspeech-based system named Swin-BERT for automatic dementia detection. For the\nacoustic part, the shifted windows multi-head attention that proposed to\nextract local and global information from images, is used for designing our\nacoustic-based system. To decouple the effect of age and gender on acoustic\nfeature extraction, they are used as an extra input of the designed acoustic\nsystem. For the linguistic part, the rhythm-related information, which varies\nsignificantly between people living with and without AD, is removed while\ntranscribing the audio recordings into transcripts. To compensate for the\nremoved rhythm-related information, the character-level transcripts are\nproposed to be used as the extra input of a word-level BERT-style system.\nFinally, the Swin-BERT combines the acoustic features learned from our proposed\nacoustic-based system with our linguistic-based system. The experiments are\nbased on the two datasets provided by the international dementia detection\nchallenges: the ADReSS and ADReSSo. The results show that both the proposed\nacoustic and linguistic systems can be better or comparable with previous\nresearch on the two datasets. Superior results are achieved by the proposed\nSwin-BERT system on the ADReSS and ADReSSo datasets, which are 85.58\\% F-score\nand 87.32\\% F-score respectively.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07277v1",
    "published_date": "2024-10-09 06:58:20 UTC",
    "updated_date": "2024-10-09 06:58:20 UTC"
  },
  {
    "arxiv_id": "2410.07274v1",
    "title": "Mitigation of gender bias in automatic facial non-verbal behaviors generation",
    "authors": [
      "Alice Delbosc",
      "Magalie Ochs",
      "Nicolas Sabouret",
      "Brian Ravenet",
      "Stephane Ayache"
    ],
    "abstract": "Research on non-verbal behavior generation for social interactive agents\nfocuses mainly on the believability and synchronization of non-verbal cues with\nspeech. However, existing models, predominantly based on deep learning\narchitectures, often perpetuate biases inherent in the training data. This\nraises ethical concerns, depending on the intended application of these agents.\nThis paper addresses these issues by first examining the influence of gender on\nfacial non-verbal behaviors. We concentrate on gaze, head movements, and facial\nexpressions. We introduce a classifier capable of discerning the gender of a\nspeaker from their non-verbal cues. This classifier achieves high accuracy on\nboth real behavior data, extracted using state-of-the-art tools, and synthetic\ndata, generated from a model developed in previous work.Building upon this\nwork, we present a new model, FairGenderGen, which integrates a gender\ndiscriminator and a gradient reversal layer into our previous behavior\ngeneration model. This new model generates facial non-verbal behaviors from\nspeech features, mitigating gender sensitivity in the generated behaviors. Our\nexperiments demonstrate that the classifier, developed in the initial phase, is\nno longer effective in distinguishing the gender of the speaker from the\ngenerated non-verbal behaviors.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07274v1",
    "published_date": "2024-10-09 06:41:24 UTC",
    "updated_date": "2024-10-09 06:41:24 UTC"
  },
  {
    "arxiv_id": "2410.18115v1",
    "title": "Point Cloud Compression with Bits-back Coding",
    "authors": [
      "Nguyen Quang Hieu",
      "Minh Nguyen",
      "Dinh Thai Hoang",
      "Diep N. Nguyen",
      "Eryk Dutkiewicz"
    ],
    "abstract": "This paper introduces a novel lossless compression method for compressing\ngeometric attributes of point cloud data with bits-back coding. Our method\nspecializes in using a deep learning-based probabilistic model to estimate the\nShannon's entropy of the point cloud information, i.e., geometric attributes of\nthe 3D floating points. Once the entropy of the point cloud dataset is\nestimated with a convolutional variational autoencoder (CVAE), we use the\nlearned CVAE model to compress the geometric attributes of the point clouds\nwith the bits-back coding technique. The novelty of our method with bits-back\ncoding specializes in utilizing the learned latent variable model of the CVAE\nto compress the point cloud data. By using bits-back coding, we can capture the\npotential correlation between the data points, such as similar spatial features\nlike shapes and scattering regions, into the lower-dimensional latent space to\nfurther reduce the compression ratio. The main insight of our method is that we\ncan achieve a competitive compression ratio as conventional deep learning-based\napproaches, while significantly reducing the overhead cost of storage and/or\ncommunicating the compression codec, making our approach more applicable in\npractical scenarios. Throughout comprehensive evaluations, we found that the\ncost for the overhead is significantly small, compared to the reduction of the\ncompression ratio when compressing large point cloud datasets. Experiment\nresults show that our proposed approach can achieve a compression ratio of 1.56\nbit-per-point on average, which is significantly lower than the baseline\napproach such as Google's Draco with a compression ratio of 1.83 bit-per-point.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is under reviewed in IEEE Robotics and Automation Letters",
    "pdf_url": "http://arxiv.org/pdf/2410.18115v1",
    "published_date": "2024-10-09 06:34:48 UTC",
    "updated_date": "2024-10-09 06:34:48 UTC"
  },
  {
    "arxiv_id": "2410.14710v1",
    "title": "G2D2: Gradient-guided Discrete Diffusion for image inverse problem solving",
    "authors": [
      "Naoki Murata",
      "Chieh-Hsin Lai",
      "Yuhta Takida",
      "Toshimitsu Uesaka",
      "Bac Nguyen",
      "Stefano Ermon",
      "Yuki Mitsufuji"
    ],
    "abstract": "Recent literature has effectively utilized diffusion models trained on\ncontinuous variables as priors for solving inverse problems. Notably, discrete\ndiffusion models with discrete latent codes have shown strong performance,\nparticularly in modalities suited for discrete compressed representations, such\nas image and motion generation. However, their discrete and non-differentiable\nnature has limited their application to inverse problems formulated in\ncontinuous spaces. This paper presents a novel method for addressing linear\ninverse problems by leveraging image-generation models based on discrete\ndiffusion as priors. We overcome these limitations by approximating the true\nposterior distribution with a variational distribution constructed from\ncategorical distributions and continuous relaxation techniques. Furthermore, we\nemploy a star-shaped noise process to mitigate the drawbacks of traditional\ndiscrete diffusion models with absorbing states, demonstrating that our method\nperforms comparably to continuous diffusion techniques. To the best of our\nknowledge, this is the first approach to use discrete diffusion model-based\npriors for solving image inverse problems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.14710v1",
    "published_date": "2024-10-09 06:18:25 UTC",
    "updated_date": "2024-10-09 06:18:25 UTC"
  },
  {
    "arxiv_id": "2410.06561v1",
    "title": "Efficient and Robust Knowledge Distillation from A Stronger Teacher Based on Correlation Matching",
    "authors": [
      "Wenqi Niu",
      "Yingchao Wang",
      "Guohui Cai",
      "Hanpo Hou"
    ],
    "abstract": "Knowledge Distillation (KD) has emerged as a pivotal technique for neural\nnetwork compression and performance enhancement. Most KD methods aim to\ntransfer dark knowledge from a cumbersome teacher model to a lightweight\nstudent model based on Kullback-Leibler (KL) divergence loss. However, the\nstudent performance improvements achieved through KD exhibit diminishing\nmarginal returns, where a stronger teacher model does not necessarily lead to a\nproportionally stronger student model. To address this issue, we empirically\nfind that the KL-based KD method may implicitly change the inter-class\nrelationships learned by the student model, resulting in a more complex and\nambiguous decision boundary, which in turn reduces the model's accuracy and\ngeneralization ability. Therefore, this study argues that the student model\nshould learn not only the probability values from the teacher's output but also\nthe relative ranking of classes, and proposes a novel Correlation Matching\nKnowledge Distillation (CMKD) method that combines the Pearson and Spearman\ncorrelation coefficients-based KD loss to achieve more efficient and robust\ndistillation from a stronger teacher model. Moreover, considering that samples\nvary in difficulty, CMKD dynamically adjusts the weights of the Pearson-based\nloss and Spearman-based loss. CMKD is simple yet practical, and extensive\nexperiments demonstrate that it can consistently achieve state-of-the-art\nperformance on CIRAR-100 and ImageNet, and adapts well to various teacher\narchitectures, sizes, and other KD methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.06561v1",
    "published_date": "2024-10-09 05:42:47 UTC",
    "updated_date": "2024-10-09 05:42:47 UTC"
  },
  {
    "arxiv_id": "2410.06560v1",
    "title": "Mitigating Time Discretization Challenges with WeatherODE: A Sandwich Physics-Driven Neural ODE for Weather Forecasting",
    "authors": [
      "Peiyuan Liu",
      "Tian Zhou",
      "Liang Sun",
      "Rong Jin"
    ],
    "abstract": "In the field of weather forecasting, traditional models often grapple with\ndiscretization errors and time-dependent source discrepancies, which limit\ntheir predictive performance. In this paper, we present WeatherODE, a novel\none-stage, physics-driven ordinary differential equation (ODE) model designed\nto enhance weather forecasting accuracy. By leveraging wave equation theory and\nintegrating a time-dependent source model, WeatherODE effectively addresses the\nchallenges associated with time-discretization error and dynamic atmospheric\nprocesses. Moreover, we design a CNN-ViT-CNN sandwich structure, facilitating\nefficient learning dynamics tailored for distinct yet interrelated tasks with\nvarying optimization biases in advection equation estimation. Through rigorous\nexperiments, WeatherODE demonstrates superior performance in both global and\nregional weather forecasting tasks, outperforming recent state-of-the-art\napproaches by significant margins of over 40.0\\% and 31.8\\% in root mean square\nerror (RMSE), respectively. The source code is available at\n\\url{https://github.com/DAMO-DI-ML/WeatherODE}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06560v1",
    "published_date": "2024-10-09 05:41:24 UTC",
    "updated_date": "2024-10-09 05:41:24 UTC"
  },
  {
    "arxiv_id": "2410.18114v5",
    "title": "Bridging Today and the Future of Humanity: AI Safety in 2024 and Beyond",
    "authors": [
      "Shanshan Han"
    ],
    "abstract": "The advancements in generative AI inevitably raise concerns about their risks\nand safety implications, which, in return, catalyzes significant progress in AI\nsafety. However, as this field continues to evolve, a critical question arises:\nare our current efforts on AI safety aligned with the advancements of AI as\nwell as the long-term goal of human civilization? This paper presents a\nblueprint for an advanced human society and leverages this vision to guide\ncurrent AI safety efforts. It outlines a future where the Internet of\nEverything becomes reality, and creates a roadmap of significant technological\nadvancements towards this envisioned future. For each stage of the\nadvancements, this paper forecasts potential AI safety issues that humanity may\nface. By projecting current efforts against this blueprint, this paper examines\nthe alignment between the current efforts and the long-term needs, and\nhighlights unique challenges and missions that demand increasing attention from\nAI safety practitioners in the 2020s. This vision paper aims to offer a broader\nperspective on AI safety, emphasizing that our current efforts should not only\naddress immediate concerns but also anticipate potential risks in the expanding\nAI landscape, thereby promoting a safe and sustainable future of AI and human\ncivilization.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.18114v5",
    "published_date": "2024-10-09 05:36:29 UTC",
    "updated_date": "2025-01-11 00:59:59 UTC"
  },
  {
    "arxiv_id": "2410.06554v2",
    "title": "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models",
    "authors": [
      "Yanjun Chen",
      "Dawei Zhu",
      "Yirong Sun",
      "Xinghao Chen",
      "Wei Zhang",
      "Xiaoyu Shen"
    ],
    "abstract": "Reinforcement Learning from Human Feedback significantly enhances Natural\nLanguage Processing by aligning language models with human expectations. A\ncritical factor in this alignment is the strength of reward models used during\ntraining. This study explores whether stronger reward models invariably lead to\nbetter language models. In this paper, through experiments on relevance,\nfactuality, and completeness tasks using the QA-FEEDBACK dataset and reward\nmodels based on Longformer, we uncover a surprising paradox: language models\ntrained with moderately accurate reward models outperform those guided by\nhighly accurate ones. This challenges the widely held belief that stronger\nreward models always lead to better language models, and opens up new avenues\nfor future research into the key factors driving model performance and how to\nchoose the most suitable reward models. Code and additional details are\navailable at https://github.com/EIT-NLP/AccuracyParadox-RLHF.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 27 figures (including 18 in the appendix), submitted to\n  EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.06554v2",
    "published_date": "2024-10-09 05:17:08 UTC",
    "updated_date": "2024-10-16 04:48:08 UTC"
  },
  {
    "arxiv_id": "2410.06551v1",
    "title": "InstantIR: Blind Image Restoration with Instant Generative Reference",
    "authors": [
      "Jen-Yuan Huang",
      "Haofan Wang",
      "Qixun Wang",
      "Xu Bai",
      "Hao Ai",
      "Peng Xing",
      "Jen-Tse Huang"
    ],
    "abstract": "Handling test-time unknown degradation is the major challenge in Blind Image\nRestoration (BIR), necessitating high model generalization. An effective\nstrategy is to incorporate prior knowledge, either from human input or\ngenerative model. In this paper, we introduce Instant-reference Image\nRestoration (InstantIR), a novel diffusion-based BIR method which dynamically\nadjusts generation condition during inference. We first extract a compact\nrepresentation of the input via a pre-trained vision encoder. At each\ngeneration step, this representation is used to decode current diffusion latent\nand instantiate it in the generative prior. The degraded image is then encoded\nwith this reference, providing robust generation condition. We observe the\nvariance of generative references fluctuate with degradation intensity, which\nwe further leverage as an indicator for developing a sampling algorithm\nadaptive to input quality. Extensive experiments demonstrate InstantIR achieves\nstate-of-the-art performance and offering outstanding visual quality. Through\nmodulating generative references with textual description, InstantIR can\nrestore extreme degradation and additionally feature creative restoration.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06551v1",
    "published_date": "2024-10-09 05:15:29 UTC",
    "updated_date": "2024-10-09 05:15:29 UTC"
  },
  {
    "arxiv_id": "2410.06550v1",
    "title": "Investigating Cost-Efficiency of LLM-Generated Training Data for Conversational Semantic Frame Analysis",
    "authors": [
      "Shiho Matta",
      "Yin Jou Huang",
      "Fei Cheng",
      "Hirokazu Kiyomaru",
      "Yugo Murawaki"
    ],
    "abstract": "Recent studies have demonstrated that few-shot learning allows LLMs to\ngenerate training data for supervised models at a low cost. However, the\nquality of LLM-generated data may not entirely match that of human-labeled\ndata. This raises a crucial question: how should one balance the trade-off\nbetween the higher quality but more expensive human data and the lower quality\nyet substantially cheaper LLM-generated data? In this paper, we synthesized\ntraining data for conversational semantic frame analysis using GPT-4 and\nexamined how to allocate budgets optimally to achieve the best performance. Our\nexperiments, conducted across various budget levels, reveal that optimal\ncost-efficiency is achieved by combining both human and LLM-generated data\nacross a wide range of budget levels. Notably, as the budget decreases, a\nhigher proportion of LLM-generated data becomes more preferable.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages including 4 pages of references and appendix. 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.06550v1",
    "published_date": "2024-10-09 05:15:13 UTC",
    "updated_date": "2024-10-09 05:15:13 UTC"
  },
  {
    "arxiv_id": "2410.07271v2",
    "title": "Multi-Task Program Error Repair and Explanatory Diagnosis",
    "authors": [
      "Zhenyu Xu",
      "Victor S. Sheng"
    ],
    "abstract": "Program errors can occur in any type of programming, and can manifest in a\nvariety of ways, such as unexpected output, crashes, or performance issues. And\nprogram error diagnosis can often be too abstract or technical for developers\nto understand, especially for beginners. The goal of this paper is to present a\nnovel machine-learning approach for Multi-task Program Error Repair and\nExplanatory Diagnosis (mPRED). A pre-trained language model is used to encode\nthe source code, and a downstream model is specifically designed to identify\nand repair errors. Programs and test cases will be augmented and optimized from\nseveral perspectives. Additionally, our approach incorporates a \"chain of\nthoughts\" method, which enables the models to produce intermediate reasoning\nexplanations before providing the final correction. To aid in visualizing and\nanalyzing the program structure, we use a graph neural network for program\nstructure visualization. Overall, our approach offers a promising approach for\nrepairing program errors across different programming languages and providing\nhelpful explanations to programmers.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07271v2",
    "published_date": "2024-10-09 05:09:24 UTC",
    "updated_date": "2025-01-06 04:33:32 UTC"
  },
  {
    "arxiv_id": "2410.08035v2",
    "title": "IntrinsicVoice: Empowering LLMs with Intrinsic Real-time Voice Interaction Abilities",
    "authors": [
      "Xin Zhang",
      "Xiang Lyu",
      "Zhihao Du",
      "Qian Chen",
      "Dong Zhang",
      "Hangrui Hu",
      "Chaohong Tan",
      "Tianyu Zhao",
      "Yuxuan Wang",
      "Bin Zhang",
      "Heng Lu",
      "Yaqian Zhou",
      "Xipeng Qiu"
    ],
    "abstract": "Current methods of building LLMs with voice interaction capabilities rely\nheavily on explicit text autoregressive generation before or during speech\nresponse generation to maintain content quality, which unfortunately brings\ncomputational overhead and increases latency in multi-turn interactions. To\naddress this, we introduce IntrinsicVoic,e an LLM designed with intrinsic\nreal-time voice interaction capabilities. IntrinsicVoice aims to facilitate the\ntransfer of textual capabilities of pre-trained LLMs to the speech modality by\nmitigating the modality gap between text and speech. Our novelty architecture,\nGroupFormer, can reduce speech sequences to lengths comparable to text\nsequences while generating high-quality audio, significantly reducing the\nlength difference between speech and text, speeding up inference, and\nalleviating long-text modeling issues. Additionally, we construct a multi-turn\nspeech-to-speech dialogue dataset named \\method-500k which includes nearly 500k\nturns of speech-to-speech dialogues, and a cross-modality training strategy to\nenhance the semantic alignment between speech and text. Experimental results\ndemonstrate that IntrinsicVoice can generate high-quality speech response with\nlatency lower than 100ms in multi-turn dialogue scenarios. Demos are available\nat https://instrinsicvoice.github.io/.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.08035v2",
    "published_date": "2024-10-09 05:04:31 UTC",
    "updated_date": "2024-10-12 06:46:39 UTC"
  },
  {
    "arxiv_id": "2410.06549v2",
    "title": "DiffGAD: A Diffusion-based Unsupervised Graph Anomaly Detector",
    "authors": [
      "Jinghan Li",
      "Yuan Gao",
      "Jinda Lu",
      "Junfeng Fang",
      "Congcong Wen",
      "Hui Lin",
      "Xiang Wang"
    ],
    "abstract": "Graph Anomaly Detection (GAD) is crucial for identifying abnormal entities\nwithin networks, garnering significant attention across various fields.\nTraditional unsupervised methods, which decode encoded latent representations\nof unlabeled data with a reconstruction focus, often fail to capture critical\ndiscriminative content, leading to suboptimal anomaly detection. To address\nthese challenges, we present a Diffusion-based Graph Anomaly Detector\n(DiffGAD). At the heart of DiffGAD is a novel latent space learning paradigm,\nmeticulously designed to enhance its proficiency by guiding it with\ndiscriminative content. This innovative approach leverages diffusion sampling\nto infuse the latent space with discriminative content and introduces a\ncontent-preservation mechanism that retains valuable information across\ndifferent scales, significantly improving its adeptness at identifying\nanomalies with limited time and space complexity. Our comprehensive evaluation\nof DiffGAD, conducted on six real-world and large-scale datasets with various\nmetrics, demonstrated its exceptional performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06549v2",
    "published_date": "2024-10-09 05:02:56 UTC",
    "updated_date": "2025-02-25 03:03:45 UTC"
  },
  {
    "arxiv_id": "2410.06541v2",
    "title": "Chip-Tuning: Classify Before Language Models Say",
    "authors": [
      "Fangwei Zhu",
      "Dian Li",
      "Jiajun Huang",
      "Gang Liu",
      "Hui Wang",
      "Zhifang Sui"
    ],
    "abstract": "The rapid development in the performance of large language models (LLMs) is\naccompanied by the escalation of model size, leading to the increasing cost of\nmodel training and inference. Previous research has discovered that certain\nlayers in LLMs exhibit redundancy, and removing these layers brings only\nmarginal loss in model performance. In this paper, we adopt the probing\ntechnique to explain the layer redundancy in LLMs and demonstrate that language\nmodels can be effectively pruned with probing classifiers. We propose\nchip-tuning, a simple and effective structured pruning framework specialized\nfor classification problems. Chip-tuning attaches tiny probing classifiers\nnamed chips to different layers of LLMs, and trains chips with the backbone\nmodel frozen. After selecting a chip for classification, all layers subsequent\nto the attached layer could be removed with marginal performance loss.\nExperimental results on various LLMs and datasets demonstrate that chip-tuning\nsignificantly outperforms previous state-of-the-art baselines in both accuracy\nand pruning ratio, achieving a pruning ratio of up to 50%. We also find that\nchip-tuning could be applied on multimodal models, and could be combined with\nmodel finetuning, proving its excellent compatibility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06541v2",
    "published_date": "2024-10-09 04:35:22 UTC",
    "updated_date": "2024-10-11 05:20:19 UTC"
  },
  {
    "arxiv_id": "2410.07269v2",
    "title": "Deep Learning for Surgical Instrument Recognition and Segmentation in Robotic-Assisted Surgeries: A Systematic Review",
    "authors": [
      "Fatimaelzahraa Ali Ahmed",
      "Mahmoud Yousef",
      "Mariam Ali Ahmed",
      "Hasan Omar Ali",
      "Anns Mahboob",
      "Hazrat Ali",
      "Zubair Shah",
      "Omar Aboumarzouk",
      "Abdulla Al Ansari",
      "Shidin Balakrishnan"
    ],
    "abstract": "Applying deep learning (DL) for annotating surgical instruments in\nrobot-assisted minimally invasive surgeries (MIS) represents a significant\nadvancement in surgical technology. This systematic review examines 48 studies\nthat and advanced DL methods and architectures. These sophisticated DL models\nhave shown notable improvements in the precision and efficiency of detecting\nand segmenting surgical tools. The enhanced capabilities of these models\nsupport various clinical applications, including real-time intraoperative\nguidance, comprehensive postoperative evaluations, and objective assessments of\nsurgical skills. By accurately identifying and segmenting surgical instruments\nin video data, DL models provide detailed feedback to surgeons, thereby\nimproving surgical outcomes and reducing complication risks. Furthermore, the\napplication of DL in surgical education is transformative. The review\nunderscores the significant impact of DL on improving the accuracy of skill\nassessments and the overall quality of surgical training programs. However,\nimplementing DL in surgical tool detection and segmentation faces challenges,\nsuch as the need for large, accurately annotated datasets to train these models\neffectively. The manual annotation process is labor-intensive and\ntime-consuming, posing a significant bottleneck. Future research should focus\non automating the detection and segmentation process and enhancing the\nrobustness of DL models against environmental variations. Expanding the\napplication of DL models across various surgical specialties will be essential\nto fully realize this technology's potential. Integrating DL with other\nemerging technologies, such as augmented reality (AR), also offers promising\nopportunities to further enhance the precision and efficacy of surgical\nprocedures.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "57 pages, 9 figures, Published in Artificial Intelligence Reviews\n  journal <https://link.springer.com/journal/10462>",
    "pdf_url": "http://arxiv.org/pdf/2410.07269v2",
    "published_date": "2024-10-09 04:07:38 UTC",
    "updated_date": "2024-11-07 07:52:59 UTC"
  },
  {
    "arxiv_id": "2410.06530v3",
    "title": "TopoTune : A Framework for Generalized Combinatorial Complex Neural Networks",
    "authors": [
      "Mathilde Papillon",
      "Guillermo Bernárdez",
      "Claudio Battiloro",
      "Nina Miolane"
    ],
    "abstract": "Graph Neural Networks (GNNs) excel in learning from relational datasets,\nprocessing node and edge features in a way that preserves the symmetries of the\ngraph domain. However, many complex systems -- such as biological or social\nnetworks--involve multiway complex interactions that are more naturally\nrepresented by higher-order topological domains. The emerging field of\nTopological Deep Learning (TDL) aims to accommodate and leverage these\nhigher-order structures. Combinatorial Complex Neural Networks (CCNNs), fairly\ngeneral TDL models, have been shown to be more expressive and better performing\nthan GNNs. However, differently from the GNN ecosystem, TDL lacks a principled\nand standardized framework for easily defining new architectures, restricting\nits accessibility and applicability. To address this issue, we introduce\nGeneralized CCNNs (GCCNs), a novel simple yet powerful family of TDL models\nthat can be used to systematically transform any (graph) neural network into\nits TDL counterpart. We prove that GCCNs generalize and subsume CCNNs, while\nextensive experiments on a diverse class of GCCNs show that these architectures\nconsistently match or outperform CCNNs, often with less model complexity. In an\neffort to accelerate and democratize TDL, we introduce TopoTune, a lightweight\nsoftware for defining, building, and training GCCNs with unprecedented\nflexibility and ease.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06530v3",
    "published_date": "2024-10-09 04:07:20 UTC",
    "updated_date": "2025-02-11 17:49:04 UTC"
  },
  {
    "arxiv_id": "2410.06527v1",
    "title": "The Sampling-Gaussian for stereo matching",
    "authors": [
      "Baiyu Pan",
      "jichao jiao",
      "Bowen Yao",
      "Jianxin Pang",
      "Jun Cheng"
    ],
    "abstract": "The soft-argmax operation is widely adopted in neural network-based stereo\nmatching methods to enable differentiable regression of disparity. However,\nnetwork trained with soft-argmax is prone to being multimodal due to absence of\nexplicit constraint to the shape of the probability distribution. Previous\nmethods leverages Laplacian distribution and cross-entropy for training but\nfailed to effectively improve the accuracy and even compromises the efficiency\nof the network. In this paper, we conduct a detailed analysis of the previous\ndistribution-based methods and propose a novel supervision method for stereo\nmatching, Sampling-Gaussian. We sample from the Gaussian distribution for\nsupervision. Moreover, we interpret the training as minimizing the distance in\nvector space and propose a combined loss of L1 loss and cosine similarity loss.\nAdditionally, we leveraged bilinear interpolation to upsample the cost volume.\nOur method can be directly applied to any soft-argmax-based stereo matching\nmethod without a reduction in efficiency. We have conducted comprehensive\nexperiments to demonstrate the superior performance of our Sampling-Gaussian.\nThe experimental results prove that we have achieved better accuracy on five\nbaseline methods and two datasets. Our method is easy to implement, and the\ncode is available online.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "TL;DR: A novel Gaussian distribution-based supervision method for\n  stereo matching. Implemented with five baseline methods and achieves notable\n  improvement. Main content, 10 pages. conference submission",
    "pdf_url": "http://arxiv.org/pdf/2410.06527v1",
    "published_date": "2024-10-09 03:57:13 UTC",
    "updated_date": "2024-10-09 03:57:13 UTC"
  },
  {
    "arxiv_id": "2410.06524v1",
    "title": "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA",
    "authors": [
      "Maharshi Gor",
      "Hal Daumé III",
      "Tianyi Zhou",
      "Jordan Boyd-Graber"
    ],
    "abstract": "Recent advancements of large language models (LLMs) have led to claims of AI\nsurpassing humans in natural language processing (NLP) tasks such as textual\nunderstanding and reasoning. This work investigates these assertions by\nintroducing CAIMIRA, a novel framework rooted in item response theory (IRT)\nthat enables quantitative assessment and comparison of problem-solving\nabilities of question-answering (QA) agents: humans and AI systems. Through\nanalysis of over 300,000 responses from ~70 AI systems and 155 humans across\nthousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in\nknowledge domains and reasoning skills. Humans outperform AI systems in\nknowledge-grounded abductive and conceptual reasoning, while state-of-the-art\nLLMs like GPT-4 and LLaMA show superior performance on targeted information\nretrieval and fact-based reasoning, particularly when information gaps are\nwell-defined and addressable through pattern matching or data retrieval. These\nfindings highlight the need for future QA tasks to focus on questions that\nchallenge not only higher-order reasoning and scientific thinking, but also\ndemand nuanced linguistic interpretation and cross-contextual knowledge\napplication, helping advance AI developments that better emulate or complement\nhuman cognitive abilities in real-world problem-solving.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear at EMNLP 2024 (Main)",
    "pdf_url": "http://arxiv.org/pdf/2410.06524v1",
    "published_date": "2024-10-09 03:53:26 UTC",
    "updated_date": "2024-10-09 03:53:26 UTC"
  },
  {
    "arxiv_id": "2410.06523v2",
    "title": "Phase Diagram from Nonlinear Interaction between Superconducting Order and Density: Toward Data-Based Holographic Superconductor",
    "authors": [
      "Sejin Kim",
      "Kyung Kiu Kim",
      "Yunseok Seo"
    ],
    "abstract": "We address an inverse problem in modeling holographic superconductors. We\nfocus our research on the critical temperature behavior depicted by\nexperiments. We use a physics-informed neural network method to find a mass\nfunction $M(F^2)$, which is necessary to understand phase transition behavior.\nThis mass function describes a nonlinear interaction between superconducting\norder and charge carrier density. We introduce positional embedding layers to\nimprove the learning process in our algorithm, and the Adam optimization is\nused to predict the critical temperature data via holographic calculation with\nappropriate accuracy. Consideration of the positional embedding layers is\nmotivated by the transformer model of natural-language processing in the\nartificial intelligence (AI) field. We obtain holographic models that reproduce\nborderlines of the normal and superconducting phases provided by actual data.\nOur work is the first holographic attempt to match phase transition data\nquantitatively obtained from experiments. Also, the present work offers a new\nmethodology for data-based holographic models.",
    "categories": [
      "hep-th",
      "cond-mat.dis-nn",
      "cond-mat.supr-con",
      "cs.AI"
    ],
    "primary_category": "hep-th",
    "comment": "22 pages, 20 figures, published version in JHEP",
    "pdf_url": "http://arxiv.org/pdf/2410.06523v2",
    "published_date": "2024-10-09 03:52:18 UTC",
    "updated_date": "2025-05-07 14:12:16 UTC"
  },
  {
    "arxiv_id": "2410.06516v1",
    "title": "QuadBEV: An Efficient Quadruple-Task Perception Framework via Bird's-Eye-View Representation",
    "authors": [
      "Yuxin Li",
      "Yiheng Li",
      "Xulei Yang",
      "Mengying Yu",
      "Zihang Huang",
      "Xiaojun Wu",
      "Chai Kiat Yeo"
    ],
    "abstract": "Bird's-Eye-View (BEV) perception has become a vital component of autonomous\ndriving systems due to its ability to integrate multiple sensor inputs into a\nunified representation, enhancing performance in various downstream tasks.\nHowever, the computational demands of BEV models pose challenges for real-world\ndeployment in vehicles with limited resources. To address these limitations, we\npropose QuadBEV, an efficient multitask perception framework that leverages the\nshared spatial and contextual information across four key tasks: 3D object\ndetection, lane detection, map segmentation, and occupancy prediction. QuadBEV\nnot only streamlines the integration of these tasks using a shared backbone and\ntask-specific heads but also addresses common multitask learning challenges\nsuch as learning rate sensitivity and conflicting task objectives. Our\nframework reduces redundant computations, thereby enhancing system efficiency,\nmaking it particularly suited for embedded systems. We present comprehensive\nexperiments that validate the effectiveness and robustness of QuadBEV,\ndemonstrating its suitability for real-world applications.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06516v1",
    "published_date": "2024-10-09 03:31:45 UTC",
    "updated_date": "2024-10-09 03:31:45 UTC"
  },
  {
    "arxiv_id": "2410.07268v1",
    "title": "Learning Content-Aware Multi-Modal Joint Input Pruning via Bird's-Eye-View Representation",
    "authors": [
      "Yuxin Li",
      "Yiheng Li",
      "Xulei Yang",
      "Mengying Yu",
      "Zihang Huang",
      "Xiaojun Wu",
      "Chai Kiat Yeo"
    ],
    "abstract": "In the landscape of autonomous driving, Bird's-Eye-View (BEV) representation\nhas recently garnered substantial academic attention, serving as a\ntransformative framework for the fusion of multi-modal sensor inputs. This BEV\nparadigm effectively shifts the sensor fusion challenge from a rule-based\nmethodology to a data-centric approach, thereby facilitating more nuanced\nfeature extraction from an array of heterogeneous sensors. Notwithstanding its\nevident merits, the computational overhead associated with BEV-based techniques\noften mandates high-capacity hardware infrastructures, thus posing challenges\nfor practical, real-world implementations. To mitigate this limitation, we\nintroduce a novel content-aware multi-modal joint input pruning technique. Our\nmethod leverages BEV as a shared anchor to algorithmically identify and\neliminate non-essential sensor regions prior to their introduction into the\nperception model's backbone. We validatethe efficacy of our approach through\nextensive experiments on the NuScenes dataset, demonstrating substantial\ncomputational efficiency without sacrificing perception accuracy. To the best\nof our knowledge, this work represents the first attempt to alleviate the\ncomputational burden from the input pruning point.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07268v1",
    "published_date": "2024-10-09 03:30:00 UTC",
    "updated_date": "2024-10-09 03:30:00 UTC"
  },
  {
    "arxiv_id": "2410.06511v2",
    "title": "TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training",
    "authors": [
      "Wanchao Liang",
      "Tianyu Liu",
      "Less Wright",
      "Will Constable",
      "Andrew Gu",
      "Chien-Chin Huang",
      "Iris Zhang",
      "Wei Feng",
      "Howard Huang",
      "Junjie Wang",
      "Sanket Purandare",
      "Gokul Nadathur",
      "Stratos Idreos"
    ],
    "abstract": "The development of large language models (LLMs) has been instrumental in\nadvancing state-of-the-art natural language processing applications. Training\nLLMs with billions of parameters and trillions of tokens require sophisticated\ndistributed systems that enable composing and comparing several\nstate-of-the-art techniques in order to efficiently scale across thousands of\naccelerators. However, existing solutions are complex, scattered across\nmultiple libraries/repositories, lack interoperability, and are cumbersome to\nmaintain. Thus, curating and empirically comparing training recipes require\nnon-trivial engineering effort.\n  This paper introduces TorchTitan, an open-source, PyTorch-native distributed\ntraining system that unifies state-of-the-art techniques, streamlining\nintegration and reducing overhead. TorchTitan enables 3D parallelism in a\nmodular manner with elastic scaling, providing comprehensive logging,\ncheckpointing, and debugging tools for production-ready training. It also\nincorporates hardware-software co-designed solutions, leveraging features like\nFloat8 training and SymmetricMemory. As a flexible test bed, TorchTitan\nfacilitates custom recipe curation and comparison, allowing us to develop\noptimized training recipes for Llama 3.1 and provide guidance on selecting\ntechniques for maximum efficiency based on our experiences.\n  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8\nbillion to 405 billion parameters, and showcase its exceptional performance,\nmodular composability, and elastic scalability. By stacking training\noptimizations, we demonstrate accelerations of 65.08% with 1D parallelism at\nthe 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at\nthe 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at\nthe 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized\nbaselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06511v2",
    "published_date": "2024-10-09 03:26:11 UTC",
    "updated_date": "2024-11-04 13:52:23 UTC"
  },
  {
    "arxiv_id": "2410.10874v1",
    "title": "Optimizing Transformer based on high-performance optimizer for predicting employment sentiment in American social media content",
    "authors": [
      "Feiyang Wang",
      "Qiaozhi Bao",
      "Zixuan Wang",
      "Yanlin Chen"
    ],
    "abstract": "This article improves the Transformer model based on swarm intelligence\noptimization algorithm, aiming to predict the emotions of employment related\ntext content on American social media. Through text preprocessing, feature\nextraction, and vectorization, the text data was successfully converted into\nnumerical data and imported into the model for training. The experimental\nresults show that during the training process, the accuracy of the model\ngradually increased from 49.27% to 82.83%, while the loss value decreased from\n0.67 to 0.35, indicating a significant improvement in the performance of the\nmodel on the training set. According to the confusion matrix analysis of the\ntraining set, the accuracy of the training set is 86.15%. The confusion matrix\nof the test set also showed good performance, with an accuracy of 82.91%. The\naccuracy difference between the training set and the test set is only 3.24%,\nindicating that the model has strong generalization ability. In addition, the\nevaluation of polygon results shows that the model performs well in\nclassification accuracy, sensitivity, specificity, and area under the curve\n(AUC), with a Kappa coefficient of 0.66 and an F-measure of 0.80, further\nverifying the effectiveness of the model in social media sentiment analysis.\nThe improved model proposed in this article not only improves the accuracy of\nsentiment recognition in employment related texts on social media, but also has\nimportant practical significance. This social media based data analysis method\ncan not only capture social dynamics in a timely manner, but also promote\ndecision-makers to pay attention to public concerns and provide data support\nfor improving employment conditions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.10874v1",
    "published_date": "2024-10-09 03:14:05 UTC",
    "updated_date": "2024-10-09 03:14:05 UTC"
  },
  {
    "arxiv_id": "2410.06502v2",
    "title": "Chemistry-Inspired Diffusion with Non-Differentiable Guidance",
    "authors": [
      "Yuchen Shen",
      "Chenhao Zhang",
      "Sijie Fu",
      "Chenghui Zhou",
      "Newell Washburn",
      "Barnabás Póczos"
    ],
    "abstract": "Recent advances in diffusion models have shown remarkable potential in the\nconditional generation of novel molecules. These models can be guided in two\nways: (i) explicitly, through additional features representing the condition,\nor (ii) implicitly, using a property predictor. However, training property\npredictors or conditional diffusion models requires an abundance of labeled\ndata and is inherently challenging in real-world applications. We propose a\nnovel approach that attenuates the limitations of acquiring large labeled\ndatasets by leveraging domain knowledge from quantum chemistry as a\nnon-differentiable oracle to guide an unconditional diffusion model. Instead of\nrelying on neural networks, the oracle provides accurate guidance in the form\nof estimated gradients, allowing the diffusion process to sample from a\nconditional distribution specified by quantum chemistry. We show that this\nresults in more precise conditional generation of novel and stable molecular\nstructures. Our experiments demonstrate that our method: (1) significantly\nreduces atomic forces, enhancing the validity of generated molecules when used\nfor stability optimization; (2) is compatible with both explicit and implicit\nguidance in diffusion models, enabling joint optimization of molecular\nproperties and stability; and (3) generalizes effectively to molecular\noptimization tasks beyond stability optimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.06502v2",
    "published_date": "2024-10-09 03:10:21 UTC",
    "updated_date": "2025-03-11 14:58:58 UTC"
  },
  {
    "arxiv_id": "2410.06497v1",
    "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta's Ads System",
    "authors": [
      "Fang Zhou",
      "Yaning Huang",
      "Dong Liang",
      "Dai Li",
      "Zhongke Zhang",
      "Kai Wang",
      "Xiao Xin",
      "Abdallah Aboelela",
      "Zheliang Jiang",
      "Yang Wang",
      "Jeff Song",
      "Wei Zhang",
      "Chen Liang",
      "Huayu Li",
      "ChongLin Sun",
      "Hang Yang",
      "Lei Qu",
      "Zhan Shu",
      "Mindi Yuan",
      "Emanuele Maccherani",
      "Taha Hayat",
      "John Guo",
      "Varna Puvvada",
      "Uladzimir Pashkevich"
    ],
    "abstract": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06497v1",
    "published_date": "2024-10-09 02:51:27 UTC",
    "updated_date": "2024-10-09 02:51:27 UTC"
  },
  {
    "arxiv_id": "2410.06493v1",
    "title": "BiC-MPPI: Goal-Pursuing, Sampling-Based Bidirectional Rollout Clustering Path Integral for Trajectory Optimization",
    "authors": [
      "Minchan Jung",
      "Kwangki Kim"
    ],
    "abstract": "This paper introduces the Bidirectional Clustered MPPI (BiC-MPPI) algorithm,\na novel trajectory optimization method aimed at enhancing goal-directed\nguidance within the Model Predictive Path Integral (MPPI) framework. BiC-MPPI\nincorporates bidirectional dynamics approximations and a new guide cost\nmechanism, improving both trajectory planning and goal-reaching performance. By\nleveraging forward and backward rollouts, the bidirectional approach ensures\neffective trajectory connections between initial and terminal states, while the\nguide cost helps discover dynamically feasible paths. Experimental results\ndemonstrate that BiC-MPPI outperforms existing MPPI variants in both 2D and 3D\nenvironments, achieving higher success rates and competitive computation times\nacross 900 simulations on a modified BARN dataset for autonomous navigation.\n  GitHub: https://github.com/i-ASL/BiC-MPPI",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.OC",
      "68T40, 13P25",
      "I.2.9; I.2.8; G.1.6; G.4"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 1 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.06493v1",
    "published_date": "2024-10-09 02:36:35 UTC",
    "updated_date": "2024-10-09 02:36:35 UTC"
  },
  {
    "arxiv_id": "2410.06491v1",
    "title": "Honesty to Subterfuge: In-Context Reinforcement Learning Can Make Honest Models Reward Hack",
    "authors": [
      "Leo McKee-Reid",
      "Christoph Sträter",
      "Maria Angelica Martinez",
      "Joe Needham",
      "Mikita Balesni"
    ],
    "abstract": "Previous work has shown that training \"helpful-only\" LLMs with reinforcement\nlearning on a curriculum of gameable environments can lead models to generalize\nto egregious specification gaming, such as editing their own reward function or\nmodifying task checklists to appear more successful. We show that gpt-4o,\ngpt-4o-mini, o1-preview, and o1-mini - frontier models trained to be helpful,\nharmless, and honest - can engage in specification gaming without training on a\ncurriculum of tasks, purely from in-context iterative reflection (which we call\nin-context reinforcement learning, \"ICRL\"). We also show that using ICRL to\ngenerate highly-rewarded outputs for expert iteration (compared to the standard\nexpert iteration reinforcement learning algorithm) may increase gpt-4o-mini's\npropensity to learn specification-gaming policies, generalizing (in very rare\ncases) to the most egregious strategy where gpt-4o-mini edits its own reward\nfunction. Our results point toward the strong ability of in-context reflection\nto discover rare specification-gaming strategies that models might not exhibit\nzero-shot or with normal training, highlighting the need for caution when\nrelying on alignment of LLMs in zero-shot settings.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.06491v1",
    "published_date": "2024-10-09 02:34:27 UTC",
    "updated_date": "2024-10-09 02:34:27 UTC"
  },
  {
    "arxiv_id": "2410.06490v2",
    "title": "Adaptive Guidance for Local Training in Heterogeneous Federated Learning",
    "authors": [
      "Jianqing Zhang",
      "Yang Liu",
      "Yang Hua",
      "Jian Cao",
      "Qiang Yang"
    ],
    "abstract": "Model heterogeneity poses a significant challenge in Heterogeneous Federated\nLearning (HtFL). In scenarios with diverse model architectures, directly\naggregating model parameters is impractical, leading HtFL methods to\nincorporate an extra objective alongside the original local objective on each\nclient to facilitate collaboration. However, this often results in a mismatch\nbetween the extra and local objectives. To resolve this, we propose Federated\nLearning-to-Guide (FedL2G), a method that adaptively learns to guide local\ntraining in a federated manner, ensuring the added objective aligns with each\nclient's original goal. With theoretical guarantees, FedL2G utilizes only\nfirst-order derivatives w.r.t. model parameters, achieving a non-convex\nconvergence rate of O(1/T). We conduct extensive experiments across two data\nheterogeneity and six model heterogeneity settings, using 14 heterogeneous\nmodel architectures (e.g., CNNs and ViTs). The results show that FedL2G\nsignificantly outperforms seven state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06490v2",
    "published_date": "2024-10-09 02:31:49 UTC",
    "updated_date": "2025-01-30 06:58:58 UTC"
  },
  {
    "arxiv_id": "2410.10873v1",
    "title": "AuditWen:An Open-Source Large Language Model for Audit",
    "authors": [
      "Jiajia Huang",
      "Haoran Zhu",
      "Chao Xu",
      "Tianming Zhan",
      "Qianqian Xie",
      "Jimin Huang"
    ],
    "abstract": "Intelligent auditing represents a crucial advancement in modern audit\npractices, enhancing both the quality and efficiency of audits within the realm\nof artificial intelligence. With the rise of large language model (LLM), there\nis enormous potential for intelligent models to contribute to audit domain.\nHowever, general LLMs applied in audit domain face the challenges of lacking\nspecialized knowledge and the presence of data biases. To overcome these\nchallenges, this study introduces AuditWen, an open-source audit LLM by\nfine-tuning Qwen with constructing instruction data from audit domain. We first\noutline the application scenarios for LLMs in the audit and extract\nrequirements that shape the development of LLMs tailored for audit purposes. We\nthen propose an audit LLM, called AuditWen, by fine-tuning Qwen with\nconstructing 28k instruction dataset from 15 audit tasks and 3 layers. In\nevaluation stage, we proposed a benchmark with 3k instructions that covers a\nset of critical audit tasks derived from the application scenarios. With the\nbenchmark, we compare AuditWen with other existing LLMs from information\nextraction, question answering and document generation. The experimental\nresults demonstrate superior performance of AuditWen both in question\nunderstanding and answer generation, making it an immediately valuable tool for\naudit.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages,1 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.10873v1",
    "published_date": "2024-10-09 02:28:55 UTC",
    "updated_date": "2024-10-09 02:28:55 UTC"
  },
  {
    "arxiv_id": "2410.08037v1",
    "title": "Composite Learning Units: Generalized Learning Beyond Parameter Updates to Transform LLMs into Adaptive Reasoners",
    "authors": [
      "Santosh Kumar Radha",
      "Oktay Goktas"
    ],
    "abstract": "Human learning thrives on the ability to learn from mistakes, adapt through\nfeedback, and refine understanding-processes often missing in static machine\nlearning models. In this work, we introduce Composite Learning Units (CLUs)\ndesigned to transform reasoners, such as Large Language Models (LLMs), into\nlearners capable of generalized, continuous learning without conventional\nparameter updates while enhancing their reasoning abilities through continual\ninteraction and feedback. CLUs are built on an architecture that allows a\nreasoning model to maintain and evolve a dynamic knowledge repository: a\nGeneral Knowledge Space for broad, reusable insights and a Prompt-Specific\nKnowledge Space for task-specific learning. Through goal-driven interactions,\nCLUs iteratively refine these knowledge spaces, enabling the system to adapt\ndynamically to complex tasks, extract nuanced insights, and build upon past\nexperiences autonomously. We demonstrate CLUs' effectiveness through a\ncryptographic reasoning task, where they continuously evolve their\nunderstanding through feedback to uncover hidden transformation rules. While\nconventional models struggle to grasp underlying logic, CLUs excel by engaging\nin an iterative, goal-oriented process. Specialized components-handling\nknowledge retrieval, prompt generation, and feedback analysis-work together\nwithin a reinforcing feedback loop. This approach allows CLUs to retain the\nmemory of past failures and successes, adapt autonomously, and apply\nsophisticated reasoning effectively, continually learning from mistakes while\nalso building on breakthroughs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.08037v1",
    "published_date": "2024-10-09 02:27:58 UTC",
    "updated_date": "2024-10-09 02:27:58 UTC"
  },
  {
    "arxiv_id": "2410.06483v2",
    "title": "Deep Learning Ensemble for Predicting Diabetic Macular Edema Onset Using Ultra-Wide Field Color Fundus Image",
    "authors": [
      "Pengyao Qin",
      "Arun J. Thirunavukarasu",
      "Theodoros Arvanitis",
      "Le Zhang"
    ],
    "abstract": "Diabetic macular edema (DME) is a severe complication of diabetes,\ncharacterized by thickening of the central portion of the retina due to\naccumulation of fluid. DME is a significant and common cause of visual\nimpairment in diabetic patients. Center-involved DME (ci-DME) is the highest\nrisk form of disease because fluid extends close to the fovea which is\nresponsible for sharp central vision. Earlier diagnosis or prediction of ci-DME\nmay improve treatment outcomes. Here, we propose an ensemble method to predict\nci-DME onset within a year, after using synthetic ultra-wide field color fundus\nphotography (UWF-CFP) images provided by the DIAMOND Challenge during\ndevelopment. We adopted a variety of baseline state-of-the-art classification\nnetworks including ResNet, DenseNet, EfficientNet, and VGG with the aim of\nenhancing model robustness. The best performing models were Densenet-121,\nResnet-152 and EfficientNet-b7, and these were assembled into a definitive\npredictive model. The final ensemble model demonstrates a strong performance\nwith an Area Under Curve (AUC) of 0.7017, an F1 score of 0.6512, and an\nExpected Calibration Error (ECE) of 0.2057 when deployed on the synthetic test\ndataset. Results from our ensemble model were superior/comparable to previous\nrecorded results in highly curated settings using conventional fundus\nphotography/ultra-wide field fundus photography. Optimal sensitivity in\nprevious studies (using humans or computers to diagnose) ranges from 67.3%-98%,\nspecificity from 47.8%-80%. Therefore, our method can be used safely and\neffectively in a range of settings may facilitate earlier diagnosis, better\ntreatment decisions, and improved prognostication in ci-DME.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06483v2",
    "published_date": "2024-10-09 02:16:29 UTC",
    "updated_date": "2024-12-09 21:38:56 UTC"
  },
  {
    "arxiv_id": "2410.06482v1",
    "title": "OledFL: Unleashing the Potential of Decentralized Federated Learning via Opposite Lookahead Enhancement",
    "authors": [
      "Qinglun Li",
      "Miao Zhang",
      "Mengzhu Wang",
      "Quanjun Yin",
      "Li Shen"
    ],
    "abstract": "Decentralized Federated Learning (DFL) surpasses Centralized Federated\nLearning (CFL) in terms of faster training, privacy preservation, and light\ncommunication, making it a promising alternative in the field of federated\nlearning. However, DFL still exhibits significant disparities with CFL in terms\nof generalization ability such as rarely theoretical understanding and degraded\nempirical performance due to severe inconsistency. In this paper, we enhance\nthe consistency of DFL by developing an opposite lookahead enhancement\ntechnique (Ole), yielding OledFL to optimize the initialization of each client\nin each communication round, thus significantly improving both the\ngeneralization and convergence speed. Moreover, we rigorously establish its\nconvergence rate in non-convex setting and characterize its generalization\nbound through uniform stability, which provides concrete reasons why OledFL can\nachieve both the fast convergence speed and high generalization ability.\nExtensive experiments conducted on the CIFAR10 and CIFAR100 datasets with\nDirichlet and Pathological distributions illustrate that our OledFL can achieve\nup to 5\\% performance improvement and 8$\\times$ speedup, compared to the most\npopular DFedAvg optimizer in DFL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06482v1",
    "published_date": "2024-10-09 02:16:14 UTC",
    "updated_date": "2024-10-09 02:16:14 UTC"
  },
  {
    "arxiv_id": "2410.06473v3",
    "title": "GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic Guidance",
    "authors": [
      "Arthur Bucker",
      "Pablo Ortega-Kral",
      "Jonathan Francis",
      "Jean Oh"
    ],
    "abstract": "Robot learning approaches such as behavior cloning and reinforcement learning\nhave shown great promise in synthesizing robot skills from human demonstrations\nin specific environments. However, these approaches often require task-specific\ndemonstrations or designing complex simulation environments, which limits the\ndevelopment of generalizable and robust policies for unseen real-world\nsettings. Recent advances in the use of foundation models for robotics (e.g.,\nLLMs, VLMs) have shown great potential in enabling systems to understand the\nsemantics in the world from large-scale internet data. However, it remains an\nopen challenge to use this knowledge to enable robotic systems to understand\nthe underlying dynamics of the world, to generalize policies across different\ntasks, and to adapt policies to new environments. To alleviate these\nlimitations, we propose an agentic framework for robot self-guidance and\nself-improvement, which consists of a set of role-specialized conversational\nagents, such as a high-level advisor, a grounding agent, a monitoring agent,\nand a robotic agent. Our framework iteratively grounds a base robot policy to\nrelevant objects in the environment and uses visuomotor cues to shift the\naction distribution of the policy to more desirable states, online, while\nremaining agnostic to the subjective configuration of a given robot hardware\nplatform. We demonstrate that our approach can effectively guide manipulation\npolicies to achieve significantly higher success rates, both in simulation and\nin real-world experiments, without the need for additional human demonstrations\nor extensive exploration. Code and videos available at:\nhttps://agenticrobots.github.io",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "21 pages, 12 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.06473v3",
    "published_date": "2024-10-09 02:00:37 UTC",
    "updated_date": "2025-04-08 16:32:04 UTC"
  },
  {
    "arxiv_id": "2410.06472v2",
    "title": "Enabling Novel Mission Operations and Interactions with ROSA: The Robot Operating System Agent",
    "authors": [
      "Rob Royce",
      "Marcel Kaufmann",
      "Jonathan Becktor",
      "Sangwoo Moon",
      "Kalind Carpenter",
      "Kai Pak",
      "Amanda Towler",
      "Rohan Thakker",
      "Shehryar Khattak"
    ],
    "abstract": "The advancement of robotic systems has revolutionized numerous industries,\nyet their operation often demands specialized technical knowledge, limiting\naccessibility for non-expert users. This paper introduces ROSA (Robot Operating\nSystem Agent), an AI-powered agent that bridges the gap between the Robot\nOperating System (ROS) and natural language interfaces. By leveraging\nstate-of-the-art language models and integrating open-source frameworks, ROSA\nenables operators to interact with robots using natural language, translating\ncommands into actions and interfacing with ROS through well-defined tools.\nROSA's design is modular and extensible, offering seamless integration with\nboth ROS1 and ROS2, along with safety mechanisms like parameter validation and\nconstraint enforcement to ensure secure, reliable operations. While ROSA is\noriginally designed for ROS, it can be extended to work with other robotics\nmiddle-wares to maximize compatibility across missions. ROSA enhances\nhuman-robot interaction by democratizing access to complex robotic systems,\nempowering users of all expertise levels with multi-modal capabilities such as\nspeech integration and visual perception. Ethical considerations are thoroughly\naddressed, guided by foundational principles like Asimov's Three Laws of\nRobotics, ensuring that AI integration promotes safety, transparency, privacy,\nand accountability. By making robotic technology more user-friendly and\naccessible, ROSA not only improves operational efficiency but also sets a new\nstandard for responsible AI use in robotics and potentially future mission\noperations. This paper introduces ROSA's architecture and showcases initial\nmock-up operations in JPL's Mars Yard, a laboratory, and a simulation using\nthree different robots. The core ROSA library is available as open-source.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "Preprint. Accepted at IEEE Aerospace Conference 2025, 16 pages, 12\n  figures",
    "pdf_url": "http://arxiv.org/pdf/2410.06472v2",
    "published_date": "2024-10-09 01:54:02 UTC",
    "updated_date": "2025-02-13 00:37:06 UTC"
  },
  {
    "arxiv_id": "2410.06468v2",
    "title": "Does Spatial Cognition Emerge in Frontier Models?",
    "authors": [
      "Santhosh Kumar Ramakrishnan",
      "Erik Wijmans",
      "Philipp Kraehenbuehl",
      "Vladlen Koltun"
    ],
    "abstract": "Not yet. We present SPACE, a benchmark that systematically evaluates spatial\ncognition in frontier models. Our benchmark builds on decades of research in\ncognitive science. It evaluates large-scale mapping abilities that are brought\nto bear when an organism traverses physical environments, smaller-scale\nreasoning about object shapes and layouts, and cognitive infrastructure such as\nspatial attention and memory. For many tasks, we instantiate parallel\npresentations via text and images, allowing us to benchmark both large language\nmodels and large multimodal models. Results suggest that contemporary frontier\nmodels fall short of the spatial intelligence of animals, performing near\nchance level on a number of classic tests of animal cognition. Code and data\nare available: https://github.com/apple/ml-space-benchmark",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.06468v2",
    "published_date": "2024-10-09 01:41:49 UTC",
    "updated_date": "2025-04-18 03:53:04 UTC"
  },
  {
    "arxiv_id": "2410.06462v1",
    "title": "Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders",
    "authors": [
      "David Noever",
      "Forrest McKee"
    ],
    "abstract": "The research builds and evaluates the adversarial potential to introduce\ncopied code or hallucinated AI recommendations for malicious code in popular\ncode repositories. While foundational large language models (LLMs) from OpenAI,\nGoogle, and Anthropic guard against both harmful behaviors and toxic strings,\nprevious work on math solutions that embed harmful prompts demonstrate that the\nguardrails may differ between expert contexts. These loopholes would appear in\nmixture of expert's models when the context of the question changes and may\noffer fewer malicious training examples to filter toxic comments or recommended\noffensive actions. The present work demonstrates that foundational models may\nrefuse to propose destructive actions correctly when prompted overtly but may\nunfortunately drop their guard when presented with a sudden change of context,\nlike solving a computer programming challenge. We show empirical examples with\ntrojan-hosting repositories like GitHub, NPM, NuGet, and popular content\ndelivery networks (CDN) like jsDelivr which amplify the attack surface. In the\nLLM's directives to be helpful, example recommendations propose application\nprogramming interface (API) endpoints which a determined domain-squatter could\nacquire and setup attack mobile infrastructure that triggers from the naively\ncopied code. We compare this attack to previous work on context-shifting and\ncontrast the attack surface as a novel version of \"living off the land\" attacks\nin the malware literature. In the latter case, foundational language models can\nhijack otherwise innocent user prompts to recommend actions that violate their\nowners' safety policies when posed directly without the accompanying coding\nsupport request.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06462v1",
    "published_date": "2024-10-09 01:36:25 UTC",
    "updated_date": "2024-10-09 01:36:25 UTC"
  },
  {
    "arxiv_id": "2410.09097v2",
    "title": "Recent advancements in LLM Red-Teaming: Techniques, Defenses, and Ethical Considerations",
    "authors": [
      "Tarun Raheja",
      "Nilay Pochhi",
      "F. D. C. M. Curie"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks, but their vulnerability to jailbreak attacks\nposes significant security risks. This survey paper presents a comprehensive\nanalysis of recent advancements in attack strategies and defense mechanisms\nwithin the field of Large Language Model (LLM) red-teaming. We analyze various\nattack methods, including gradient-based optimization, reinforcement learning,\nand prompt engineering approaches. We discuss the implications of these attacks\non LLM safety and the need for improved defense mechanisms. This work aims to\nprovide a thorough understanding of the current landscape of red-teaming\nattacks and defenses on LLMs, enabling the development of more secure and\nreliable language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.09097v2",
    "published_date": "2024-10-09 01:35:38 UTC",
    "updated_date": "2024-12-17 04:34:32 UTC"
  },
  {
    "arxiv_id": "2410.06458v1",
    "title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
    "authors": [
      "Thomas Palmeira Ferraz",
      "Kartik Mehta",
      "Yu-Hsiang Lin",
      "Haw-Shiuan Chang",
      "Shereen Oraby",
      "Sijia Liu",
      "Vivek Subramanian",
      "Tagyoung Chung",
      "Mohit Bansal",
      "Nanyun Peng"
    ],
    "abstract": "Instruction following is a key capability for LLMs. However, recent studies\nhave shown that LLMs often struggle with instructions containing multiple\nconstraints (e.g. a request to create a social media post \"in a funny tone\"\nwith \"no hashtag\"). Despite this, most evaluations focus solely on synthetic\ndata. To address this, we introduce RealInstruct, the first benchmark designed\nto evaluate LLMs' ability to follow real-world multi-constrained instructions\nby leveraging queries real users asked AI assistants. We also investigate\nmodel-based evaluation as a cost-effective alternative to human annotation for\nthis task. Our findings reveal that even the proprietary GPT-4 model fails to\nmeet at least one constraint on over 21% of instructions, highlighting the\nlimitations of state-of-the-art models. To address the performance gap between\nopen-source and proprietary models, we propose the Decompose, Critique and\nRefine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to\nfollow constraints. DeCRIM works by decomposing the original instruction into a\nlist of constraints and using a Critic model to decide when and where the LLM's\nresponse needs refinement. Our results show that DeCRIM improves Mistral's\nperformance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback.\nMoreover, we demonstrate that with strong feedback, open-source LLMs with\nDeCRIM can outperform GPT-4 on both benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear at EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.06458v1",
    "published_date": "2024-10-09 01:25:10 UTC",
    "updated_date": "2024-10-09 01:25:10 UTC"
  },
  {
    "arxiv_id": "2410.06452v1",
    "title": "Modeling chaotic Lorenz ODE System using Scientific Machine Learning",
    "authors": [
      "Sameera S Kashyap",
      "Raj Abhijit Dandekar",
      "Rajat Dandekar",
      "Sreedath Panat"
    ],
    "abstract": "In climate science, models for global warming and weather prediction face\nsignificant challenges due to the limited availability of high-quality data and\nthe difficulty in obtaining it, making data efficiency crucial. In the past few\nyears, Scientific Machine Learning (SciML) models have gained tremendous\ntraction as they can be trained in a data-efficient manner, making them highly\nsuitable for real-world climate applications. Despite this, very little\nattention has been paid to chaotic climate system modeling utilizing SciML\nmethods. In this paper, we have integrated SciML methods into foundational\nweather models, where we have enhanced large-scale climate predictions with a\nphysics-informed approach that achieves high accuracy with reduced data. We\nsuccessfully demonstrate that by combining the interpretability of physical\nclimate models with the computational power of neural networks, SciML models\ncan prove to be a reliable tool for modeling climate. This indicates a shift\nfrom the traditional black box-based machine learning modeling of climate\nsystems to physics-informed decision-making, leading to effective climate\npolicy implementation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 8 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.06452v1",
    "published_date": "2024-10-09 01:17:06 UTC",
    "updated_date": "2024-10-09 01:17:06 UTC"
  },
  {
    "arxiv_id": "2410.06442v1",
    "title": "MaD-Scientist: AI-based Scientist solving Convection-Diffusion-Reaction Equations Using Massive PINN-Based Prior Data",
    "authors": [
      "Mingu Kang",
      "Dongseok Lee",
      "Woojin Cho",
      "Jaehyeon Park",
      "Kookjin Lee",
      "Anthony Gruber",
      "Youngjoon Hong",
      "Noseong Park"
    ],
    "abstract": "Large language models (LLMs), like ChatGPT, have shown that even trained with\nnoisy prior data, they can generalize effectively to new tasks through\nin-context learning (ICL) and pre-training techniques. Motivated by this, we\nexplore whether a similar approach can be applied to scientific foundation\nmodels (SFMs). Our methodology is structured as follows: (i) we collect\nlow-cost physics-informed neural network (PINN)-based approximated prior data\nin the form of solutions to partial differential equations (PDEs) constructed\nthrough an arbitrary linear combination of mathematical dictionaries; (ii) we\nutilize Transformer architectures with self and cross-attention mechanisms to\npredict PDE solutions without knowledge of the governing equations in a\nzero-shot setting; (iii) we provide experimental evidence on the\none-dimensional convection-diffusion-reaction equation, which demonstrate that\npre-training remains robust even with approximated prior data, with only\nmarginal impacts on test accuracy. Notably, this finding opens the path to\npre-training SFMs with realistic, low-cost data instead of (or in conjunction\nwith) numerical high-cost data. These results support the conjecture that SFMs\ncan improve in a manner similar to LLMs, where fully cleaning the vast set of\nsentences crawled from the Internet is nearly impossible.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.06442v1",
    "published_date": "2024-10-09 00:52:00 UTC",
    "updated_date": "2024-10-09 00:52:00 UTC"
  }
]
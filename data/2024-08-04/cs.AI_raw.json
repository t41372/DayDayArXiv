[
  {
    "arxiv_id": "2408.02165v1",
    "title": "SelfBC: Self Behavior Cloning for Offline Reinforcement Learning",
    "authors": [
      "Shirong Liu",
      "Chenjia Bai",
      "Zixian Guo",
      "Hao Zhang",
      "Gaurav Sharma",
      "Yang Liu"
    ],
    "abstract": "Policy constraint methods in offline reinforcement learning employ additional\nregularization techniques to constrain the discrepancy between the learned\npolicy and the offline dataset. However, these methods tend to result in overly\nconservative policies that resemble the behavior policy, thus limiting their\nperformance. We investigate this limitation and attribute it to the static\nnature of traditional constraints. In this paper, we propose a novel dynamic\npolicy constraint that restricts the learned policy on the samples generated by\nthe exponential moving average of previously learned policies. By integrating\nthis self-constraint mechanism into off-policy methods, our method facilitates\nthe learning of non-conservative policies while avoiding policy collapse in the\noffline setting. Theoretical results show that our approach results in a nearly\nmonotonically improved reference policy. Extensive experiments on the D4RL\nMuJoCo domain demonstrate that our proposed method achieves state-of-the-art\nperformance among the policy constraint methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02165v1",
    "published_date": "2024-08-04 23:23:48 UTC",
    "updated_date": "2024-08-04 23:23:48 UTC"
  },
  {
    "arxiv_id": "2408.02156v1",
    "title": "Calibration-Disentangled Learning and Relevance-Prioritized Reranking for Calibrated Sequential Recommendation",
    "authors": [
      "Hyunsik Jeon",
      "Se-eun Yoon",
      "Julian McAuley"
    ],
    "abstract": "Calibrated recommendation, which aims to maintain personalized proportions of\ncategories within recommendations, is crucial in practical scenarios since it\nenhances user satisfaction by reflecting diverse interests. However, achieving\ncalibration in a sequential setting (i.e., calibrated sequential\nrecommendation) is challenging due to the need to adapt to users' evolving\npreferences. Previous methods typically leverage reranking algorithms to\ncalibrate recommendations after training a model without considering the effect\nof calibration and do not effectively tackle the conflict between relevance and\ncalibration during the reranking process. In this work, we propose LeapRec\n(Calibration-Disentangled Learning and Relevance-Prioritized Reranking), a\nnovel approach for the calibrated sequential recommendation that addresses\nthese challenges. LeapRec consists of two phases, model training phase and\nreranking phase. In the training phase, a backbone model is trained using our\nproposed calibration-disentangled learning-to-rank loss, which optimizes\npersonalized rankings while integrating calibration considerations. In the\nreranking phase, relevant items are prioritized at the top of the list, with\nitems needed for calibration following later to address potential conflicts\nbetween relevance and calibration. Through extensive experiments on four\nreal-world datasets, we show that LeapRec consistently outperforms previous\nmethods in the calibrated sequential recommendation. Our code is available at\nhttps://github.com/jeon185/LeapRec.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Published at CIKM '24 as a full research paper",
    "pdf_url": "http://arxiv.org/pdf/2408.02156v1",
    "published_date": "2024-08-04 22:23:09 UTC",
    "updated_date": "2024-08-04 22:23:09 UTC"
  },
  {
    "arxiv_id": "2408.02153v1",
    "title": "ARVO: Atlas of Reproducible Vulnerabilities for Open Source Software",
    "authors": [
      "Xiang Mei",
      "Pulkit Singh Singaria",
      "Jordi Del Castillo",
      "Haoran Xi",
      "Abdelouahab",
      "Benchikh",
      "Tiffany Bao",
      "Ruoyu Wang",
      "Yan Shoshitaishvili",
      "Adam Doupé",
      "Hammond Pearce",
      "Brendan Dolan-Gavitt"
    ],
    "abstract": "High-quality datasets of real-world vulnerabilities are enormously valuable\nfor downstream research in software security, but existing datasets are\ntypically small, require extensive manual effort to update, and are missing\ncrucial features that such research needs. In this paper, we introduce ARVO: an\nAtlas of Reproducible Vulnerabilities in Open-source software. By sourcing\nvulnerabilities from C/C++ projects that Google's OSS-Fuzz discovered and\nimplementing a reliable re-compilation system, we successfully reproduce more\nthan 5,000 memory vulnerabilities across over 250 projects, each with a\ntriggering input, the canonical developer-written patch for fixing the\nvulnerability, and the ability to automatically rebuild the project from source\nand run it at its vulnerable and patched revisions. Moreover, our dataset can\nbe automatically updated as OSS-Fuzz finds new vulnerabilities, allowing it to\ngrow over time. We provide a thorough characterization of the ARVO dataset,\nshow that it can locate fixes more accurately than Google's own OSV\nreproduction effort, and demonstrate its value for future research through two\ncase studies: firstly evaluating real-world LLM-based vulnerability repair, and\nsecondly identifying over 300 falsely patched (still-active) zero-day\nvulnerabilities from projects improperly labeled by OSS-Fuzz.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "14 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.02153v1",
    "published_date": "2024-08-04 22:13:14 UTC",
    "updated_date": "2024-08-04 22:13:14 UTC"
  },
  {
    "arxiv_id": "2408.02152v1",
    "title": "Generative Retrieval with Few-shot Indexing",
    "authors": [
      "Arian Askari",
      "Chuan Meng",
      "Mohammad Aliannejadi",
      "Zhaochun Ren",
      "Evangelos Kanoulas",
      "Suzan Verberne"
    ],
    "abstract": "Existing generative retrieval (GR) approaches rely on training-based\nindexing, i.e., fine-tuning a model to memorise the associations between a\nquery and the document identifier (docid) of a relevant document.\nTraining-based indexing has three limitations: high training overhead,\nunder-utilization of the pre-trained knowledge of large language models (LLMs),\nand challenges in adapting to a dynamic document corpus. To address the above\nissues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR).\nIt has a novel few-shot indexing process, where we prompt an LLM to generate\ndocids for all documents in a corpus, ultimately creating a docid bank for the\nentire corpus. During retrieval, we feed a query to the same LLM and constrain\nit to generate a docid within the docid bank created during indexing, and then\nmap the generated docid back to its corresponding document. Few-Shot GR relies\nsolely on prompting an LLM without requiring any training, making it more\nefficient. Moreover, we devise few-shot indexing with one-to-many mapping to\nfurther enhance Few-Shot GR. Experiments show that Few-Shot GR achieves\nsuperior performance to state-of-the-art GR methods that require heavy\ntraining.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02152v1",
    "published_date": "2024-08-04 22:00:34 UTC",
    "updated_date": "2024-08-04 22:00:34 UTC"
  },
  {
    "arxiv_id": "2408.02148v2",
    "title": "Environment Complexity and Nash Equilibria in a Sequential Social Dilemma",
    "authors": [
      "Mustafa Yasir",
      "Andrew Howes",
      "Vasilios Mavroudis",
      "Chris Hicks"
    ],
    "abstract": "Multi-agent reinforcement learning (MARL) methods, while effective in\nzero-sum or positive-sum games, often yield suboptimal outcomes in general-sum\ngames where cooperation is essential for achieving globally optimal outcomes.\nMatrix game social dilemmas, which abstract key aspects of general-sum\ninteractions, such as cooperation, risk, and trust, fail to model the temporal\nand spatial dynamics characteristic of real-world scenarios. In response, our\nstudy extends matrix game social dilemmas into more complex, higher-dimensional\nMARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma\nto more closely match the decision-space of a one-shot matrix game while also\nintroducing variable environment complexity. Our findings indicate that as\ncomplexity increases, MARL agents trained in these environments converge to\nsuboptimal strategies, consistent with the risk-dominant Nash equilibria\nstrategies found in matrix games. Our work highlights the impact of environment\ncomplexity on achieving optimal outcomes in higher-dimensional game-theoretic\nMARL environments.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "Accepted to the 17th European Workshop on Reinforcement Learning\n  (EWRL)",
    "pdf_url": "http://arxiv.org/pdf/2408.02148v2",
    "published_date": "2024-08-04 21:27:36 UTC",
    "updated_date": "2024-08-08 16:16:06 UTC"
  },
  {
    "arxiv_id": "2408.02143v1",
    "title": "Analyzing Cultural Representations of Emotions in LLMs through Mixed Emotion Survey",
    "authors": [
      "Shiran Dudy",
      "Ibrahim Said Ahmad",
      "Ryoko Kitajima",
      "Agata Lapedriza"
    ],
    "abstract": "Large Language Models (LLMs) have gained widespread global adoption,\nshowcasing advanced linguistic capabilities across multiple of languages. There\nis a growing interest in academia to use these models to simulate and study\nhuman behaviors. However, it is crucial to acknowledge that an LLM's\nproficiency in a specific language might not fully encapsulate the norms and\nvalues associated with its culture. Concerns have emerged regarding potential\nbiases towards Anglo-centric cultures and values due to the predominance of\nWestern and US-based training data. This study focuses on analyzing the\ncultural representations of emotions in LLMs, in the specific case of\nmixed-emotion situations. Our methodology is based on the studies of Miyamoto\net al. (2010), which identified distinctive emotional indicators in Japanese\nand American human responses. We first administer their mixed emotion survey to\nfive different LLMs and analyze their outputs. Second, we experiment with\ncontextual variables to explore variations in responses considering both\nlanguage and speaker origin. Thirdly, we expand our investigation to encompass\nadditional East Asian and Western European origin languages to gauge their\nalignment with their respective cultures, anticipating a closer fit. We find\nthat (1) models have limited alignment with the evidence in the literature; (2)\nwritten language has greater effect on LLMs' response than information on\nparticipants origin; and (3) LLMs responses were found more similar for East\nAsian languages than Western European languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Was accepted to ACII 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.02143v1",
    "published_date": "2024-08-04 20:56:05 UTC",
    "updated_date": "2024-08-04 20:56:05 UTC"
  },
  {
    "arxiv_id": "2408.02140v1",
    "title": "VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces",
    "authors": [
      "Somnath Sendhil Kumar",
      "Yuvaraj Govindarajulu",
      "Pavan Kulkarni",
      "Manojkumar Parmar"
    ],
    "abstract": "In the domain of black-box model extraction, conventional methods reliant on\nsoft labels or surrogate datasets struggle with scaling to high-dimensional\ninput spaces and managing the complexity of an extensive array of interrelated\nclasses. In this work, we present a novel approach that utilizes SHAP (SHapley\nAdditive exPlanations) to enhance synthetic data generation. SHAP quantifies\nthe individual contributions of each input feature towards the victim model's\noutput, facilitating the optimization of an energy-based GAN towards a\ndesirable output. This method significantly boosts performance, achieving a\n16.45% increase in the accuracy of image classification models and extending to\nvideo classification models with an average improvement of 26.11% and a maximum\nof 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics\n600, and Something-Something V2. We further demonstrate the effectiveness and\npractical utility of our method under various scenarios, including the\navailability of top-k prediction probabilities, top-k prediction labels, and\ntop-1 labels.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02140v1",
    "published_date": "2024-08-04 20:38:45 UTC",
    "updated_date": "2024-08-04 20:38:45 UTC"
  },
  {
    "arxiv_id": "2408.02117v2",
    "title": "Value-Based Rationales Improve Social Experience: A Multiagent Simulation Study",
    "authors": [
      "Sz-Ting Tzeng",
      "Nirav Ajmeri",
      "Munindar P. Singh"
    ],
    "abstract": "We propose Exanna, a framework to realize agents that incorporate values in\ndecision making. An Exannaagent considers the values of itself and others when\nproviding rationales for its actions and evaluating the rationales provided by\nothers. Via multiagent simulation, we demonstrate that considering values in\ndecision making and producing rationales, especially for norm-deviating\nactions, leads to (1) higher conflict resolution, (2) better social experience,\n(3) higher privacy, and (4) higher flexibility.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "13 pages, 13 figures, 13 tables (and supplementary material with\n  reproducibility and additional results), accepted at ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.02117v2",
    "published_date": "2024-08-04 19:14:36 UTC",
    "updated_date": "2024-08-14 15:25:32 UTC"
  },
  {
    "arxiv_id": "2408.02113v2",
    "title": "Diseño de sonido para producciones audiovisuales e historias sonoras en el aula. Hacia una docencia creativa mediante el uso de herramientas inteligentes",
    "authors": [
      "Miguel Civit",
      "Francisco Cuadrado"
    ],
    "abstract": "This study aims to share a teaching experience teaching sound design for\naudiovisual productions and compares different projects tackled by students. It\nis not intended to be a comparative analysis of different types of teaching but\nrather an analysis of different problems observed in different profiles of\nstudents of the subject who study it in different grades. The world of audio\ncan be very interesting for a large part of the students, both those with\ncreative and technical inclinations. Musical creation and production,\nsynchronization with images, dubbing, etc. They are disciplines that are\ngenerally interesting but can have a very high barrier to entry due to their\ngreat technical complexity. Sometimes it can take weeks or even months for the\nuninitiated to begin to use audio editing programs with the necessary ease,\nwhich are not always particularly intuitive for students. Learning through the\nuse of PBL methodologies generates, in our experience, results much superior to\nthose that can be observed through the use of other teaching methods such as\nmaster classes. Students acquire technical skills while developing creative\nprojects in which they get personally involved. Despite everything mentioned\nabove, most interactions between teachers and students focus on aspects of\ntechnical correction. From different parameters in reverbs (such as pre-delay,\ndecay, modulation...) to how to correctly adjust compressors, noise gates,\netc.; The number of tools with which to work with audio is incredibly\nextensive, as well as many of its features that can present serious differences\ndepending on their manufacturers.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "11 pages, in Spanish language. 1 figure. Preprint from La nueva era\n  del podcast (2023)",
    "pdf_url": "http://arxiv.org/pdf/2408.02113v2",
    "published_date": "2024-08-04 18:54:59 UTC",
    "updated_date": "2024-08-12 11:44:10 UTC"
  },
  {
    "arxiv_id": "2408.02111v3",
    "title": "Understanding Deep Learning via Notions of Rank",
    "authors": [
      "Noam Razin"
    ],
    "abstract": "Despite the extreme popularity of deep learning in science and industry, its\nformal understanding is limited. This thesis puts forth notions of rank as key\nfor developing a theory of deep learning, focusing on the fundamental aspects\nof generalization and expressiveness. In particular, we establish that\ngradient-based training can induce an implicit regularization towards low rank\nfor several neural network architectures, and demonstrate empirically that this\nphenomenon may facilitate an explanation of generalization over natural data\n(e.g., audio, images, and text). Then, we characterize the ability of graph\nneural networks to model interactions via a notion of rank, which is commonly\nused for quantifying entanglement in quantum physics. A central tool underlying\nthese results is a connection between neural networks and tensor\nfactorizations. Practical implications of our theory for designing explicit\nregularization schemes and data preprocessing algorithms are presented.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "PhD thesis",
    "pdf_url": "http://arxiv.org/pdf/2408.02111v3",
    "published_date": "2024-08-04 18:47:55 UTC",
    "updated_date": "2024-12-28 23:53:31 UTC"
  },
  {
    "arxiv_id": "2408.02700v1",
    "title": "Inventory problems and the parametric measure $m_λ$",
    "authors": [
      "Irina Georgescu"
    ],
    "abstract": "The credibility theory was introduced by B. Liu as a new way to describe the\nfuzzy uncertainty. The credibility measure is the fundamental notion of the\ncredibility theory. Recently, L.Yang and K. Iwamura extended the credibility\nmeasure by defining the parametric measure $m_{\\lambda}$ ($\\lambda$ is a real\nparameter in the interval $[0,1]$ and for $\\lambda= 1/2$ we obtain as a\nparticular case the notion of credibility measure). By using the\n$m_{\\lambda}$-measure, we studied in this paper a risk neutral multi-item\ninventory problem. Our construction generalizes the credibilistic inventory\nmodel developed by Y. Li and Y. Liu in 2019. In our model, the components of\ndemand vector are fuzzy variables and the maximization problem is formulated by\nusing the notion of $m_{\\lambda}$-expected value. We shall prove a general\nformula for the solution of optimization problem, from which we obtained\neffective formulas for computing the optimal solutions in the particular cases\nwhere the demands are trapezoidal and triangular fuzzy numbers. For\n$\\lambda=1/2$ we obtain as a particular case the computation formulas of the\noptimal solutions of the credibilistic inventory problem of Li and Liu. These\ncomputation formulas are applied for some $m_{\\lambda}$-models obtained from\nnumerical data.",
    "categories": [
      "math.OC",
      "cs.AI",
      "econ.TH"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02700v1",
    "published_date": "2024-08-04 18:05:34 UTC",
    "updated_date": "2024-08-04 18:05:34 UTC"
  },
  {
    "arxiv_id": "2408.04652v1",
    "title": "Leveraging Large Language Models with Chain-of-Thought and Prompt Engineering for Traffic Crash Severity Analysis and Inference",
    "authors": [
      "Hao Zhen",
      "Yucheng Shi",
      "Yongcan Huang",
      "Jidong J. Yang",
      "Ninghao Liu"
    ],
    "abstract": "Harnessing the power of Large Language Models (LLMs), this study explores the\nuse of three state-of-the-art LLMs, specifically GPT-3.5-turbo, LLaMA3-8B, and\nLLaMA3-70B, for crash severity inference, framing it as a classification task.\nWe generate textual narratives from original traffic crash tabular data using a\npre-built template infused with domain knowledge. Additionally, we incorporated\nChain-of-Thought (CoT) reasoning to guide the LLMs in analyzing the crash\ncauses and then inferring the severity. This study also examine the impact of\nprompt engineering specifically designed for crash severity inference. The LLMs\nwere tasked with crash severity inference to: (1) evaluate the models'\ncapabilities in crash severity analysis, (2) assess the effectiveness of CoT\nand domain-informed prompt engineering, and (3) examine the reasoning abilities\nwith the CoT framework. Our results showed that LLaMA3-70B consistently\noutperformed the other models, particularly in zero-shot settings. The CoT and\nPrompt Engineering techniques significantly enhanced performance, improving\nlogical reasoning and addressing alignment issues. Notably, the CoT offers\nvaluable insights into LLMs' reasoning processes, unleashing their capacity to\nconsider diverse factors such as environmental conditions, driver behavior, and\nvehicle characteristics in severity analysis and inference.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 12 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.04652v1",
    "published_date": "2024-08-04 17:14:10 UTC",
    "updated_date": "2024-08-04 17:14:10 UTC"
  },
  {
    "arxiv_id": "2408.02088v3",
    "title": "KAN-RCBEVDepth: A multi-modal fusion algorithm in object detection for autonomous driving",
    "authors": [
      "Zhihao Lai",
      "Chuanhao Liu",
      "Shihui Sheng",
      "Zhiqiang Zhang"
    ],
    "abstract": "Accurate 3D object detection in autonomous driving is critical yet\nchallenging due to occlusions, varying object sizes, and complex urban\nenvironments. This paper introduces the KAN-RCBEVDepth method, an innovative\napproach aimed at enhancing 3D object detection by fusing multimodal sensor\ndata from cameras, LiDAR, and millimeter-wave radar. Our unique Bird's Eye\nView-based approach significantly improves detection accuracy and efficiency by\nseamlessly integrating diverse sensor inputs, refining spatial relationship\nunderstanding, and optimizing computational procedures. Experimental results\nshow that the proposed method outperforms existing techniques across multiple\ndetection metrics, achieving a higher Mean Distance AP (0.389, 23\\%\nimprovement), a better ND Score (0.485, 17.1\\% improvement), and a faster\nEvaluation Time (71.28s, 8\\% faster). Additionally, the KAN-RCBEVDepth method\nsignificantly reduces errors compared to BEVDepth, with lower Transformation\nError (0.6044, 13.8\\% improvement), Scale Error (0.2780, 2.6\\% improvement),\nOrientation Error (0.5830, 7.6\\% improvement), Velocity Error (0.4244, 28.3\\%\nimprovement), and Attribute Error (0.2129, 3.2\\% improvement). These findings\nsuggest that our method offers enhanced accuracy, reliability, and efficiency,\nmaking it well-suited for dynamic and demanding autonomous driving scenarios.\nThe code will be released in \\url{https://github.com/laitiamo/RCBEVDepth-KAN}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02088v3",
    "published_date": "2024-08-04 16:54:49 UTC",
    "updated_date": "2024-08-27 16:46:53 UTC"
  },
  {
    "arxiv_id": "2408.02085v5",
    "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
    "authors": [
      "Yulei Qin",
      "Yuncheng Yang",
      "Pengcheng Guo",
      "Gang Li",
      "Hang Shao",
      "Yuchen Shi",
      "Zihan Xu",
      "Yun Gu",
      "Ke Li",
      "Xing Sun"
    ],
    "abstract": "Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between the latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to TMLR with Survey Certificate, review, survey, 37 pages, 5\n  figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.02085v5",
    "published_date": "2024-08-04 16:50:07 UTC",
    "updated_date": "2024-12-29 04:41:32 UTC"
  },
  {
    "arxiv_id": "2408.02061v1",
    "title": "ParkingE2E: Camera-based End-to-end Parking Network, from Images to Planning",
    "authors": [
      "Changze Li",
      "Ziheng Ji",
      "Zhe Chen",
      "Tong Qin",
      "Ming Yang"
    ],
    "abstract": "Autonomous parking is a crucial task in the intelligent driving field.\nTraditional parking algorithms are usually implemented using rule-based\nschemes. However, these methods are less effective in complex parking scenarios\ndue to the intricate design of the algorithms. In contrast,\nneural-network-based methods tend to be more intuitive and versatile than the\nrule-based methods. By collecting a large number of expert parking trajectory\ndata and emulating human strategy via learning-based methods, the parking task\ncan be effectively addressed. In this paper, we employ imitation learning to\nperform end-to-end planning from RGB images to path planning by imitating human\ndriving trajectories. The proposed end-to-end approach utilizes a target query\nencoder to fuse images and target features, and a transformer-based decoder to\nautoregressively predict future waypoints. We conducted extensive experiments\nin real-world scenarios, and the results demonstrate that the proposed method\nachieved an average parking success rate of 87.8% across four different\nreal-world garages. Real-vehicle experiments further validate the feasibility\nand effectiveness of the method proposed in this paper.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02061v1",
    "published_date": "2024-08-04 15:20:39 UTC",
    "updated_date": "2024-08-04 15:20:39 UTC"
  },
  {
    "arxiv_id": "2408.02698v2",
    "title": "Applications of Scientific Machine Learning for the Analysis of Functionally Graded Porous Beams",
    "authors": [
      "Mohammad Sadegh Eshaghi",
      "Mostafa Bamdad",
      "Cosmin Anitescu",
      "Yizheng Wang",
      "Xiaoying Zhuang",
      "Timon Rabczuk"
    ],
    "abstract": "This study investigates different Scientific Machine Learning (SciML)\napproaches for the analysis of functionally graded (FG) porous beams and\ncompares them under a new framework. The beam material properties are assumed\nto vary as an arbitrary continuous function. The methods consider the output of\na neural network/operator as an approximation to the displacement fields and\nderive the equations governing beam behavior based on the continuum\nformulation. The methods are implemented in the framework and formulated by\nthree approaches: (a) the vector approach leads to a Physics-Informed Neural\nNetwork (PINN), (b) the energy approach brings about the Deep Energy Method\n(DEM), and (c) the data-driven approach, which results in a class of Neural\nOperator methods. Finally, a neural operator has been trained to predict the\nresponse of the porous beam with functionally graded material under any\nporosity distribution pattern and any arbitrary traction condition. The results\nare validated with analytical and numerical reference solutions. The data and\ncode accompanying this manuscript will be publicly available at\nhttps://github.com/eshaghi-ms/DeepNetBeam.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02698v2",
    "published_date": "2024-08-04 15:01:52 UTC",
    "updated_date": "2024-12-24 10:45:47 UTC"
  },
  {
    "arxiv_id": "2408.02049v3",
    "title": "3D Single-object Tracking in Point Clouds with High Temporal Variation",
    "authors": [
      "Qiao Wu",
      "Kun Sun",
      "Pei An",
      "Mathieu Salzmann",
      "Yanning Zhang",
      "Jiaqi Yang"
    ],
    "abstract": "The high temporal variation of the point clouds is the key challenge of 3D\nsingle-object tracking (3D SOT). Existing approaches rely on the assumption\nthat the shape variation of the point clouds and the motion of the objects\nacross neighboring frames are smooth, failing to cope with high temporal\nvariation data. In this paper, we present a novel framework for 3D SOT in point\nclouds with high temporal variation, called HVTrack. HVTrack proposes three\nnovel components to tackle the challenges in the high temporal variation\nscenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud\nshape variations; 2) a Base-Expansion Feature Cross-Attention module to deal\nwith similar object distractions in expanded search areas; 3) a Contextual\nPoint Guided Self-Attention module for suppressing heavy background noise. We\nconstruct a dataset with high temporal variation (KITTI-HV) by setting\ndifferent frame intervals for sampling in the KITTI dataset. On the KITTI-HV\nwith 5 frame intervals, our HVTrack surpasses the state-of-the-art tracker\nCXTracker by 11.3%/15.7% in Success/Precision.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV24",
    "pdf_url": "http://arxiv.org/pdf/2408.02049v3",
    "published_date": "2024-08-04 14:57:28 UTC",
    "updated_date": "2024-09-06 07:48:03 UTC"
  },
  {
    "arxiv_id": "2408.02047v2",
    "title": "Latency-Aware Resource Allocation for Mobile Edge Generation and Computing via Deep Reinforcement Learning",
    "authors": [
      "Yinyu Wu",
      "Xuhui Zhang",
      "Jinke Ren",
      "Huijun Xing",
      "Yanyan Shen",
      "Shuguang Cui"
    ],
    "abstract": "Recently, the integration of mobile edge computing (MEC) and generative\nartificial intelligence (GAI) technology has given rise to a new area called\nmobile edge generation and computing (MEGC), which offers mobile users\nheterogeneous services such as task computing and content generation. In this\nletter, we investigate the joint communication, computation, and the AIGC\nresource allocation problem in an MEGC system. A latency minimization problem\nis first formulated to enhance the quality of service for mobile users. Due to\nthe strong coupling of the optimization variables, we propose a new deep\nreinforcement learning-based algorithm to solve it efficiently. Numerical\nresults demonstrate that the proposed algorithm can achieve lower latency than\ntwo baseline algorithms.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "5 pages, 6 figures. This paper has been accepted for publication by\n  IEEE Networking Letters",
    "pdf_url": "http://arxiv.org/pdf/2408.02047v2",
    "published_date": "2024-08-04 14:53:44 UTC",
    "updated_date": "2024-10-19 05:42:42 UTC"
  },
  {
    "arxiv_id": "2408.02044v1",
    "title": "Fine-tuning multilingual language models in Twitter/X sentiment analysis: a study on Eastern-European V4 languages",
    "authors": [
      "Tomáš Filip",
      "Martin Pavlíček",
      "Petr Sosík"
    ],
    "abstract": "The aspect-based sentiment analysis (ABSA) is a standard NLP task with\nnumerous approaches and benchmarks, where large language models (LLM) represent\nthe current state-of-the-art. We focus on ABSA subtasks based on Twitter/X data\nin underrepresented languages. On such narrow tasks, small tuned language\nmodels can often outperform universal large ones, providing available and cheap\nsolutions.\n  We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for\nclassification of sentiment towards Russia and Ukraine in the context of the\nongoing military conflict. The training/testing dataset was obtained from the\nacademic API from Twitter/X during 2023, narrowed to the languages of the V4\ncountries (Czech Republic, Slovakia, Poland, Hungary). Then we measure their\nperformance under a variety of settings including translations, sentiment\ntargets, in-context learning and more, using GPT4 as a reference model. We\ndocument several interesting phenomena demonstrating, among others, that some\nmodels are much better fine-tunable on multilingual Twitter tasks than others,\nand that they can reach the SOTA level with a very small training set. Finally\nwe identify combinations of settings providing the best results.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.02044v1",
    "published_date": "2024-08-04 14:35:30 UTC",
    "updated_date": "2024-08-04 14:35:30 UTC"
  },
  {
    "arxiv_id": "2408.11823v1",
    "title": "Mamba-Spike: Enhancing the Mamba Architecture with a Spiking Front-End for Efficient Temporal Data Processing",
    "authors": [
      "Jiahao Qin",
      "Feng Liu"
    ],
    "abstract": "The field of neuromorphic computing has gained significant attention in\nrecent years, aiming to bridge the gap between the efficiency of biological\nneural networks and the performance of artificial intelligence systems. This\npaper introduces Mamba-Spike, a novel neuromorphic architecture that integrates\na spiking front-end with the Mamba backbone to achieve efficient and robust\ntemporal data processing. The proposed approach leverages the event-driven\nnature of spiking neural networks (SNNs) to capture and process asynchronous,\ntime-varying inputs, while harnessing the power of the Mamba backbone's\nselective state spaces and linear-time sequence modeling capabilities to model\ncomplex temporal dependencies effectively. The spiking front-end of Mamba-Spike\nemploys biologically inspired neuron models, along with adaptive threshold and\nsynaptic dynamics. These components enable efficient spatiotemporal feature\nextraction and encoding of the input data. The Mamba backbone, on the other\nhand, utilizes a hierarchical structure with gated recurrent units and\nattention mechanisms to capture long-term dependencies and selectively process\nrelevant information. To evaluate the efficacy of the proposed architecture, a\ncomprehensive empirical study is conducted on both neuromorphic datasets,\nincluding DVS Gesture and TIDIGITS, and standard datasets, such as Sequential\nMNIST and CIFAR10-DVS. The results demonstrate that Mamba-Spike consistently\noutperforms state-of-the-art baselines, achieving higher accuracy, lower\nlatency, and improved energy efficiency. Moreover, the model exhibits\nrobustness to various input perturbations and noise levels, highlighting its\npotential for real-world applications. The code will be available at\nhttps://github.com/ECNU-Cross-Innovation-Lab/Mamba-Spike.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "12 pages, 5 figures, accepted by CGI 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11823v1",
    "published_date": "2024-08-04 14:10:33 UTC",
    "updated_date": "2024-08-04 14:10:33 UTC"
  },
  {
    "arxiv_id": "2408.02032v3",
    "title": "Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models",
    "authors": [
      "Fushuo Huo",
      "Wenchao Xu",
      "Zhong Zhang",
      "Haozhao Wang",
      "Zhicheng Chen",
      "Peilin Zhao"
    ],
    "abstract": "While Large Vision-Language Models (LVLMs) have rapidly advanced in recent\nyears, the prevalent issue known as the `hallucination' problem has emerged as\na significant bottleneck, hindering their real-world deployments. Existing\nmethods mitigate this issue mainly from two perspectives: One approach\nleverages extra knowledge like robust instruction tuning LVLMs with curated\ndatasets or employing auxiliary analysis networks, which inevitable incur\nadditional costs. Another approach, known as contrastive decoding, induces\nhallucinations by manually disturbing the vision or instruction raw inputs and\nmitigates them by contrasting the outputs of the disturbed and original LVLMs.\nHowever, these approaches rely on empirical holistic input disturbances and\ndouble the inference cost. To avoid these issues, we propose a simple yet\neffective method named Self-Introspective Decoding (SID). Our empirical\ninvestigation reveals that pretrained LVLMs can introspectively assess the\nimportance of vision tokens based on preceding vision and text (both\ninstruction and generated) tokens. We develop the Context and Text-aware Token\nSelection (CT2S) strategy, which preserves only unimportant vision tokens after\nearly layers of LVLMs to adaptively amplify text-informed hallucination during\nthe auto-regressive decoding. This approach ensures that multimodal knowledge\nabsorbed in the early layers induces multimodal contextual rather than aimless\nhallucinations. Subsequently, the original token logits subtract the amplified\nvision-and-text association hallucinations, guiding LVLMs decoding faithfully.\nExtensive experiments illustrate SID generates less-hallucination and\nhigher-quality texts across various metrics, without extra knowledge and much\nadditional computation burdens.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR2025",
    "pdf_url": "http://arxiv.org/pdf/2408.02032v3",
    "published_date": "2024-08-04 13:50:17 UTC",
    "updated_date": "2025-03-16 06:51:13 UTC"
  },
  {
    "arxiv_id": "2408.02029v3",
    "title": "Mining Path Association Rules in Large Property Graphs (with Appendix)",
    "authors": [
      "Yuya Sasaki",
      "Panagiotis Karras"
    ],
    "abstract": "How can we mine frequent path regularities from a graph with edge labels and\nvertex attributes? The task of association rule mining successfully discovers\nregular patterns in item sets and substructures. Still, to our best knowledge,\nthis concept has not yet been extended to path patterns in large property\ngraphs. In this paper, we introduce the problem of path association rule mining\n(PARM). Applied to any \\emph{reachability path} between two vertices within a\nlarge graph, PARM discovers regular ways in which path patterns, identified by\nvertex attributes and edge labels, co-occur with each other. We develop an\nefficient and scalable algorithm PIONEER that exploits an anti-monotonicity\nproperty to effectively prune the search space. Further, we devise\napproximation techniques and employ parallelization to achieve scalable path\nassociation rule mining. Our experimental study using real-world graph data\nverifies the significance of path association rules and the efficiency of our\nsolutions.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02029v3",
    "published_date": "2024-08-04 13:39:57 UTC",
    "updated_date": "2024-09-20 14:59:39 UTC"
  },
  {
    "arxiv_id": "2408.02025v2",
    "title": "Contrastive Learning-based Chaining-Cluster for Multilingual Voice-Face Association",
    "authors": [
      "Wuyang Chen",
      "Yanjie Sun",
      "Kele Xu",
      "Yong Dou"
    ],
    "abstract": "The innate correlation between a person's face and voice has recently emerged\nas a compelling area of study, especially within the context of multilingual\nenvironments. This paper introduces our novel solution to the Face-Voice\nAssociation in Multilingual Environments (FAME) 2024 challenge, focusing on a\ncontrastive learning-based chaining-cluster method to enhance face-voice\nassociation. This task involves the challenges of building biometric relations\nbetween auditory and visual modality cues and modelling the prosody\ninterdependence between different languages while addressing both intrinsic and\nextrinsic variability present in the data. To handle these non-trivial\nchallenges, our method employs supervised cross-contrastive (SCC) learning to\nestablish robust associations between voices and faces in multi-language\nscenarios. Following this, we have specifically designed a\nchaining-cluster-based post-processing step to mitigate the impact of outliers\noften found in unconstrained in the wild data. We conducted extensive\nexperiments to investigate the impact of language on face-voice association.\nThe overall results were evaluated on the FAME public evaluation platform,\nwhere we achieved 2nd place. The results demonstrate the superior performance\nof our method, and we validate the robustness and effectiveness of our proposed\napproach. Code is available at https://github.com/colaudiolab/FAME24_solution.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02025v2",
    "published_date": "2024-08-04 13:24:36 UTC",
    "updated_date": "2024-08-19 05:14:53 UTC"
  },
  {
    "arxiv_id": "2408.02022v1",
    "title": "Scenario-based Thermal Management Parametrization Through Deep Reinforcement Learning",
    "authors": [
      "Thomas Rudolf",
      "Philip Muhl",
      "Sören Hohmann",
      "Lutz Eckstein"
    ],
    "abstract": "The thermal system of battery electric vehicles demands advanced control. Its\nthermal management needs to effectively control active components across\nvarying operating conditions. While robust control function parametrization is\nrequired, current methodologies show significant drawbacks. They consume\nconsiderable time, human effort, and extensive real-world testing.\nConsequently, there is a need for innovative and intelligent solutions that are\ncapable of autonomously parametrizing embedded controllers. Addressing this\nissue, our paper introduces a learning-based tuning approach. We propose a\nmethodology that benefits from automated scenario generation for increased\nrobustness across vehicle usage scenarios. Our deep reinforcement learning\nagent processes the tuning task context and incorporates an image-based\ninterpretation of embedded parameter sets. We demonstrate its applicability to\na valve controller parametrization task and verify it in real-world vehicle\ntesting. The results highlight the competitive performance to baseline methods.\nThis novel approach contributes to the shift towards virtual development of\nthermal management functions, with promising potential of large-scale parameter\ntuning in the automotive industry.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 7 figures, 2 tables, 1 algorithm, 10 equations, conference",
    "pdf_url": "http://arxiv.org/pdf/2408.02022v1",
    "published_date": "2024-08-04 13:19:45 UTC",
    "updated_date": "2024-08-04 13:19:45 UTC"
  },
  {
    "arxiv_id": "2408.02018v1",
    "title": "Individualized multi-horizon MRI trajectory prediction for Alzheimer's Disease",
    "authors": [
      "Rosemary He",
      "Gabriella Ang",
      "Daniel Tward"
    ],
    "abstract": "Neurodegeneration as measured through magnetic resonance imaging (MRI) is\nrecognized as a potential biomarker for diagnosing Alzheimer's disease (AD),\nbut is generally considered less specific than amyloid or tau based biomarkers.\nDue to a large amount of variability in brain anatomy between different\nindividuals, we hypothesize that leveraging MRI time series can help improve\nspecificity, by treating each patient as their own baseline. Here we turn to\nconditional variational autoencoders to generate individualized MRI predictions\ngiven the subject's age, disease status and one previous scan. Using serial\nimaging data from the Alzheimer's Disease Neuroimaging Initiative, we train a\nnovel architecture to build a latent space distribution which can be sampled\nfrom to generate future predictions of changing anatomy. This enables us to\nextrapolate beyond the dataset and predict MRIs up to 10 years. We evaluated\nthe model on a held-out set from ADNI and an independent dataset (from Open\nAccess Series of Imaging Studies). By comparing to several alternatives, we\nshow that our model produces more individualized images with higher resolution.\nFurther, if an individual already has a follow-up MRI, we demonstrate a usage\nof our model to compute a likelihood ratio classifier for disease status. In\npractice, the model may be able to assist in early diagnosis of AD and provide\na counterfactual baseline trajectory for treatment effect estimation.\nFurthermore, it generates a synthetic dataset that can potentially be used for\ndownstream tasks such as anomaly detection and classification.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "MICCAI 2024 LDTM workshop",
    "pdf_url": "http://arxiv.org/pdf/2408.02018v1",
    "published_date": "2024-08-04 13:09:06 UTC",
    "updated_date": "2024-08-04 13:09:06 UTC"
  },
  {
    "arxiv_id": "2408.02697v3",
    "title": "Why Rectified Power Unit Networks Fail and How to Improve It: An Effective Theory Perspective",
    "authors": [
      "Taeyoung Kim",
      "Myungjoo Kang"
    ],
    "abstract": "The Rectified Power Unit (RePU) activation functions, unlike the Rectified\nLinear Unit (ReLU), have the advantage of being a differentiable function when\nconstructing neural networks. However, it can be experimentally observed when\ndeep layers are stacked, neural networks constructed with RePU encounter\ncritical issues. These issues include the values exploding or vanishing and\nfailure of training. And these happen regardless of the hyperparameter\ninitialization. From the perspective of effective theory, we aim to identify\nthe causes of this phenomenon and propose a new activation function that\nretains the advantages of RePU while overcoming its drawbacks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "41 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.02697v3",
    "published_date": "2024-08-04 13:05:05 UTC",
    "updated_date": "2024-11-20 20:00:31 UTC"
  },
  {
    "arxiv_id": "2408.02009v2",
    "title": "Joint Learning of Emotions in Music and Generalized Sounds",
    "authors": [
      "Federico Simonetta",
      "Francesca Certo",
      "Stavros Ntalampiras"
    ],
    "abstract": "In this study, we aim to determine if generalized sounds and music can share\na common emotional space, improving predictions of emotion in terms of arousal\nand valence. We propose the use of multiple datasets as a multi-domain learning\ntechnique. Our approach involves creating a common space encompassing features\nthat characterize both generalized sounds and music, as they can evoke emotions\nin a similar manner. To achieve this, we utilized two publicly available\ndatasets, namely IADS-E and PMEmo, following a standardized experimental\nprotocol. We employed a wide variety of features that capture diverse aspects\nof the audio structure including key parameters of spectrum, energy, and\nvoicing. Subsequently, we performed joint learning on the common feature space,\nleveraging heterogeneous model architectures. Interestingly, this synergistic\nscheme outperforms the state-of-the-art in both sound and music emotion\nprediction. The code enabling full replication of the presented experimental\npipeline is available at https://github.com/LIMUNIMI/MusicSoundEmotions.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at Audio Mostly 2024, Milan",
    "pdf_url": "http://arxiv.org/pdf/2408.02009v2",
    "published_date": "2024-08-04 12:19:03 UTC",
    "updated_date": "2024-08-14 09:28:16 UTC"
  },
  {
    "arxiv_id": "2408.01999v2",
    "title": "Reinforcement Learning for an Efficient and Effective Malware Investigation during Cyber Incident Response",
    "authors": [
      "Dipo Dunsin",
      "Mohamed Chahine Ghanem",
      "Karim Ouazzane",
      "Vassil Vassilev"
    ],
    "abstract": "This research focused on enhancing post-incident malware forensic\ninvestigation using reinforcement learning RL. We proposed an advanced MDP post\nincident malware forensics investigation model and framework to expedite post\nincident forensics. We then implement our RL Malware Investigation Model based\non structured MDP within the proposed framework. To identify malware artefacts,\nthe RL agent acquires and examines forensics evidence files, iteratively\nimproving its capabilities using Q Table and temporal difference learning. The\nQ learning algorithm significantly improved the agent ability to identify\nmalware. An epsilon greedy exploration strategy and Q learning updates enabled\nefficient learning and decision making. Our experimental testing revealed that\noptimal learning rates depend on the MDP environment complexity, with simpler\nenvironments benefiting from higher rates for quicker convergence and complex\nones requiring lower rates for stability. Our model performance in identifying\nand classifying malware reduced malware analysis time compared to human\nexperts, demonstrating robustness and adaptability. The study highlighted the\nsignificance of hyper parameter tuning and suggested adaptive strategies for\ncomplex environments. Our RL based approach produced promising results and is\nvalidated as an alternative to traditional methods notably by offering\ncontinuous learning and adaptation to new and evolving malware threats which\nultimately enhance the post incident forensics investigations.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CR",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.01999v2",
    "published_date": "2024-08-04 11:55:24 UTC",
    "updated_date": "2025-01-07 10:04:51 UTC"
  },
  {
    "arxiv_id": "2408.01988v1",
    "title": "MetaWearS: A Shortcut in Wearable Systems Lifecycle with Only a Few Shots",
    "authors": [
      "Alireza Amirshahi",
      "Maedeh H. Toosi",
      "Siamak Mohammadi",
      "Stefano Albini",
      "Pasquale Davide Schiavone",
      "Giovanni Ansaloni",
      "Amir Aminifar",
      "David Atienza"
    ],
    "abstract": "Wearable systems provide continuous health monitoring and can lead to early\ndetection of potential health issues. However, the lifecycle of wearable\nsystems faces several challenges. First, effective model training for new\nwearable devices requires substantial labeled data from various subjects\ncollected directly by the wearable. Second, subsequent model updates require\nfurther extensive labeled data for retraining. Finally, frequent model updating\non the wearable device can decrease the battery life in long-term data\nmonitoring. Addressing these challenges, in this paper, we propose MetaWearS, a\nmeta-learning method to reduce the amount of initial data collection required.\nMoreover, our approach incorporates a prototypical updating mechanism,\nsimplifying the update process by modifying the class prototype rather than\nretraining the entire model. We explore the performance of MetaWearS in two\ncase studies, namely, the detection of epileptic seizures and the detection of\natrial fibrillation. We show that by fine-tuning with just a few samples, we\nachieve 70% and 82% AUC for the detection of epileptic seizures and the\ndetection of atrial fibrillation, respectively. Compared to a conventional\napproach, our proposed method performs better with up to 45% AUC. Furthermore,\nupdating the model with only 16 minutes of additional labeled data increases\nthe AUC by up to 5.3%. Finally, MetaWearS reduces the energy consumption for\nmodel updates by 456x and 418x for epileptic seizure and AF detection,\nrespectively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01988v1",
    "published_date": "2024-08-04 11:00:43 UTC",
    "updated_date": "2024-08-04 11:00:43 UTC"
  },
  {
    "arxiv_id": "2408.01986v1",
    "title": "DeMansia: Mamba Never Forgets Any Tokens",
    "authors": [
      "Ricky Fang"
    ],
    "abstract": "This paper examines the mathematical foundations of transformer\narchitectures, highlighting their limitations particularly in handling long\nsequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM),\nand LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia\nintegrates state space models with token labeling techniques to enhance\nperformance in image classification tasks, efficiently addressing the\ncomputational challenges posed by traditional transformers. The architecture,\nbenchmark, and comparisons with contemporary models demonstrate DeMansia's\neffectiveness. The implementation of this paper is available on GitHub at\nhttps://github.com/catalpaaa/DeMansia",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01986v1",
    "published_date": "2024-08-04 10:54:36 UTC",
    "updated_date": "2024-08-04 10:54:36 UTC"
  },
  {
    "arxiv_id": "2408.01970v1",
    "title": "SR-CIS: Self-Reflective Incremental System with Decoupled Memory and Reasoning",
    "authors": [
      "Biqing Qi",
      "Junqi Gao",
      "Xinquan Chen",
      "Dong Li",
      "Weinan Zhang",
      "Bowen Zhou"
    ],
    "abstract": "The ability of humans to rapidly learn new knowledge while retaining old\nmemories poses a significant challenge for current deep learning models. To\nhandle this challenge, we draw inspiration from human memory and learning\nmechanisms and propose the Self-Reflective Complementary Incremental System\n(SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and\nComplementary Memory Module (CMM), SR-CIS features a small model for fast\ninference and a large model for slow deliberation in CIM, enabled by the\nConfidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient\ncollaboration. CMM consists of task-specific Short-Term Memory (STM) region and\na universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank\nAdaptive (LoRA) and corresponding prototype weights and biases, it instantiates\nexternal storage for parameter and representation memory, thus deconstructing\nthe memory module from the inference module. By storing textual descriptions of\nimages during training and combining them with the Scenario Replay Module (SRM)\npost-training for memory combination, along with periodic short-to-long-term\nmemory restructuring, SR-CIS achieves stable incremental memory with limited\nstorage requirements. Balancing model plasticity and memory stability under\nconstraints of limited storage and low data resources, SR-CIS surpasses\nexisting competitive baselines on multiple standard and few-shot incremental\nlearning benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01970v1",
    "published_date": "2024-08-04 09:09:35 UTC",
    "updated_date": "2024-08-04 09:09:35 UTC"
  },
  {
    "arxiv_id": "2408.01966v2",
    "title": "ML-EAT: A Multilevel Embedding Association Test for Interpretable and Transparent Social Science",
    "authors": [
      "Robert Wolfe",
      "Alexis Hiniker",
      "Bill Howe"
    ],
    "abstract": "This research introduces the Multilevel Embedding Association Test (ML-EAT),\na method designed for interpretable and transparent measurement of intrinsic\nbias in language technologies. The ML-EAT addresses issues of ambiguity and\ndifficulty in interpreting the traditional EAT measurement by quantifying bias\nat three levels of increasing granularity: the differential association between\ntwo target concepts with two attribute concepts; the individual effect size of\neach target concept with two attribute concepts; and the association between\neach individual target concept and each individual attribute concept. Using the\nML-EAT, this research defines a taxonomy of EAT patterns describing the nine\npossible outcomes of an embedding association test, each of which is associated\nwith a unique EAT-Map, a novel four-quadrant visualization for interpreting the\nML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2\nlanguage models, and a CLIP language-and-image model shows that EAT patterns\nadd otherwise unobservable information about the component biases that make up\nan EAT; reveal the effects of prompting in zero-shot models; and can also\nidentify situations when cosine similarity is an ineffective metric, rendering\nan EAT unreliable. Our work contributes a method for rendering bias more\nobservable and interpretable, improving the transparency of computational\ninvestigations into human minds and societies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Artificial Intelligence, Ethics, and Society 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01966v2",
    "published_date": "2024-08-04 09:04:44 UTC",
    "updated_date": "2024-08-27 20:05:59 UTC"
  },
  {
    "arxiv_id": "2408.01964v1",
    "title": "Top K Enhanced Reinforcement Learning Attacks on Heterogeneous Graph Node Classification",
    "authors": [
      "Honglin Gao",
      "Gaoxi Xiao"
    ],
    "abstract": "Graph Neural Networks (GNNs) have attracted substantial interest due to their\nexceptional performance on graph-based data. However, their robustness,\nespecially on heterogeneous graphs, remains underexplored, particularly against\nadversarial attacks. This paper proposes HeteroKRLAttack, a targeted evasion\nblack-box attack method for heterogeneous graphs. By integrating reinforcement\nlearning with a Top-K algorithm to reduce the action space, our method\nefficiently identifies effective attack strategies to disrupt node\nclassification tasks. We validate the effectiveness of HeteroKRLAttack through\nexperiments on multiple heterogeneous graph datasets, showing significant\nreductions in classification accuracy compared to baseline methods. An ablation\nstudy underscores the critical role of the Top-K algorithm in enhancing attack\nperformance. Our findings highlight potential vulnerabilities in current models\nand provide guidance for future defense strategies against adversarial attacks\non heterogeneous graphs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01964v1",
    "published_date": "2024-08-04 08:44:00 UTC",
    "updated_date": "2024-08-04 08:44:00 UTC"
  },
  {
    "arxiv_id": "2408.01962v1",
    "title": "The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations",
    "authors": [
      "Robert Wolfe",
      "Tanushree Mitra"
    ],
    "abstract": "Calls to use open generative language models in academic research have\nhighlighted the need for reproducibility and transparency in scientific\nresearch. However, the impact of generative AI extends well beyond academia, as\ncorporations and public interest organizations have begun integrating these\nmodels into their data science pipelines. We expand this lens to include the\nimpact of open models on organizations, focusing specifically on fact-checking\norganizations, which use AI to observe and analyze large volumes of circulating\nmisinformation, yet must also ensure the reproducibility and impartiality of\ntheir work. We wanted to understand where fact-checking organizations use open\nmodels in their data science pipelines; what motivates their use of open models\nor proprietary models; and how their use of open or proprietary models can\ninform research on the societal impact of generative AI. To answer these\nquestions, we conducted an interview study with N=24 professionals at 20\nfact-checking organizations on six continents. Based on these interviews, we\noffer a five-component conceptual model of where fact-checking organizations\nemploy generative AI to support or automate parts of their data science\npipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data\nDelivery, and Data Sharing. We then provide taxonomies of fact-checking\norganizations' motivations for using open models and the limitations that\nprevent them for further adopting open models, finding that they prefer open\nmodels for Organizational Autonomy, Data Privacy and Ownership, Application\nSpecificity, and Capability Transparency. However, they nonetheless use\nproprietary models due to perceived advantages in Performance, Usability, and\nSafety, as well as Opportunity Costs related to participation in emerging\ngenerative AI ecosystems. Our work provides novel perspective on open models in\ndata-driven organizations.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.ET"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at Artificial Intelligence, Ethics, and Society 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01962v1",
    "published_date": "2024-08-04 08:41:48 UTC",
    "updated_date": "2024-08-04 08:41:48 UTC"
  },
  {
    "arxiv_id": "2408.01961v1",
    "title": "Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study",
    "authors": [
      "Robert Wolfe",
      "Aayushi Dangol",
      "Bill Howe",
      "Alexis Hiniker"
    ],
    "abstract": "Popular and news media often portray teenagers with sensationalism, as both a\nrisk to society and at risk from society. As AI begins to absorb some of the\nepistemic functions of traditional media, we study how teenagers in two\ncountries speaking two languages: 1) are depicted by AI, and 2) how they would\nprefer to be depicted. Specifically, we study the biases about teenagers\nlearned by static word embeddings (SWEs) and generative language models (GLMs),\ncomparing these with the perspectives of adolescents living in the U.S. and\nNepal. We find English-language SWEs associate teenagers with societal\nproblems, and more than 50% of the 1,000 words most associated with teenagers\nin the pretrained GloVe SWE reflect such problems. Given prompts about\nteenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss\nsocietal problems, most commonly violence, but also drug use, mental illness,\nand sexual taboo. Nepali models, while not free of such associations, are less\ndominated by social problems. Data from workshops with N=13 U.S. adolescents\nand N=18 Nepalese adolescents show that AI presentations are disconnected from\nteenage life, which revolves around activities like school and friendship.\nParticipant ratings of how well 20 trait words describe teens are decorrelated\nfrom SWE associations, with Pearson's r=.02, n.s. in English FastText and\nr=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in\nGloVe. U.S. participants suggested AI could fairly present teens by\nhighlighting diversity, while Nepalese participants centered positivity.\nParticipants were optimistic that, if it learned from adolescents, rather than\nmedia sources, AI could help mitigate stereotypes. Our work offers an\nunderstanding of the ways SWEs and GLMs misrepresent a developmentally\nvulnerable group and provides a template for less sensationalized\ncharacterization.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted at Artificial Intelligence, Ethics, and Society 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01961v1",
    "published_date": "2024-08-04 08:35:02 UTC",
    "updated_date": "2024-08-04 08:35:02 UTC"
  },
  {
    "arxiv_id": "2408.01960v1",
    "title": "AnomalySD: Few-Shot Multi-Class Anomaly Detection with Stable Diffusion Model",
    "authors": [
      "Zhenyu Yan",
      "Qingqing Fang",
      "Wenxi Lv",
      "Qinliang Su"
    ],
    "abstract": "Anomaly detection is a critical task in industrial manufacturing, aiming to\nidentify defective parts of products. Most industrial anomaly detection methods\nassume the availability of sufficient normal data for training. This assumption\nmay not hold true due to the cost of labeling or data privacy policies.\nAdditionally, mainstream methods require training bespoke models for different\nobjects, which incurs heavy costs and lacks flexibility in practice. To address\nthese issues, we seek help from Stable Diffusion (SD) model due to its\ncapability of zero/few-shot inpainting, which can be leveraged to inpaint\nanomalous regions as normal. In this paper, a few-shot multi-class anomaly\ndetection framework that adopts Stable Diffusion model is proposed, named\nAnomalySD. To adapt SD to anomaly detection task, we design different\nhierarchical text descriptions and the foreground mask mechanism for\nfine-tuning SD. In the inference stage, to accurately mask anomalous regions\nfor inpainting, we propose multi-scale mask strategy and prototype-guided mask\nstrategy to handle diverse anomalous regions. Hierarchical text prompts are\nalso utilized to guide the process of inpainting in the inference stage. The\nanomaly score is estimated based on inpainting result of all masks. Extensive\nexperiments on the MVTec-AD and VisA datasets demonstrate the superiority of\nour approach. We achieved anomaly classification and segmentation results of\n93.6%/94.8% AUROC on the MVTec-AD dataset and 86.1%/96.5% AUROC on the VisA\ndataset under multi-class and one-shot settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.01960v1",
    "published_date": "2024-08-04 08:33:44 UTC",
    "updated_date": "2024-08-04 08:33:44 UTC"
  },
  {
    "arxiv_id": "2408.01959v2",
    "title": "Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI",
    "authors": [
      "Robert Wolfe",
      "Aayushi Dangol",
      "Alexis Hiniker",
      "Bill Howe"
    ],
    "abstract": "Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at Artificial Intelligence, Ethics, and Society 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01959v2",
    "published_date": "2024-08-04 08:26:58 UTC",
    "updated_date": "2024-08-27 19:57:45 UTC"
  },
  {
    "arxiv_id": "2408.02695v1",
    "title": "Distribution-Level Memory Recall for Continual Learning: Preserving Knowledge and Avoiding Confusion",
    "authors": [
      "Shaoxu Cheng",
      "Kanglei Geng",
      "Chiyuan He",
      "Zihuan Qiu",
      "Linfeng Xu",
      "Heqian Qiu",
      "Lanxiao Wang",
      "Qingbo Wu",
      "Fanman Meng",
      "Hongliang Li"
    ],
    "abstract": "Continual Learning (CL) aims to enable Deep Neural Networks (DNNs) to learn\nnew data without forgetting previously learned knowledge. The key to achieving\nthis goal is to avoid confusion at the feature level, i.e., avoiding confusion\nwithin old tasks and between new and old tasks. Previous prototype-based CL\nmethods generate pseudo features for old knowledge replay by adding Gaussian\nnoise to the centroids of old classes. However, the distribution in the feature\nspace exhibits anisotropy during the incremental process, which prevents the\npseudo features from faithfully reproducing the distribution of old knowledge\nin the feature space, leading to confusion in classification boundaries within\nold tasks. To address this issue, we propose the Distribution-Level Memory\nRecall (DMR) method, which uses a Gaussian mixture model to precisely fit the\nfeature distribution of old knowledge at the distribution level and generate\npseudo features in the next stage. Furthermore, resistance to confusion at the\ndistribution level is also crucial for multimodal learning, as the problem of\nmultimodal imbalance results in significant differences in feature responses\nbetween different modalities, exacerbating confusion within old tasks in\nprototype-based CL methods. Therefore, we mitigate the multi-modal imbalance\nproblem by using the Inter-modal Guidance and Intra-modal Mining (IGIM) method\nto guide weaker modalities with prior information from dominant modalities and\nfurther explore useful information within modalities. For the second key, We\npropose the Confusion Index to quantitatively describe a model's ability to\ndistinguish between new and old tasks, and we use the Incremental Mixup Feature\nEnhancement (IMFE) method to enhance pseudo features with new sample features,\nalleviating classification confusion between new and old knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02695v1",
    "published_date": "2024-08-04 07:37:12 UTC",
    "updated_date": "2024-08-04 07:37:12 UTC"
  },
  {
    "arxiv_id": "2408.01942v1",
    "title": "Visual Grounding for Object-Level Generalization in Reinforcement Learning",
    "authors": [
      "Haobin Jiang",
      "Zongqing Lu"
    ],
    "abstract": "Generalization is a pivotal challenge for agents following natural language\ninstructions. To approach this goal, we leverage a vision-language model (VLM)\nfor visual grounding and transfer its vision-language knowledge into\nreinforcement learning (RL) for object-centric tasks, which makes the agent\ncapable of zero-shot generalization to unseen objects and instructions. By\nvisual grounding, we obtain an object-grounded confidence map for the target\nobject indicated in the instruction. Based on this map, we introduce two routes\nto transfer VLM knowledge into RL. Firstly, we propose an object-grounded\nintrinsic reward function derived from the confidence map to more effectively\nguide the agent towards the target object. Secondly, the confidence map offers\na more unified, accessible task representation for the agent's policy, compared\nto language embeddings. This enables the agent to process unseen objects and\ninstructions through comprehensible visual confidence maps, facilitating\nzero-shot object-level generalization. Single-task experiments prove that our\nintrinsic reward significantly improves performance on challenging skill\nlearning. In multi-task experiments, through testing on tasks beyond the\ntraining set, we show that the agent, when provided with the confidence map as\nthe task representation, possesses better generalization capabilities than\nlanguage-based conditioning. The code is available at\nhttps://github.com/PKU-RL/COPL.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "35 pages, 14 figures, 17 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.01942v1",
    "published_date": "2024-08-04 06:34:24 UTC",
    "updated_date": "2024-08-04 06:34:24 UTC"
  },
  {
    "arxiv_id": "2408.03160v2",
    "title": "User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance",
    "authors": [
      "Mrinal Verghese",
      "Brian Chen",
      "Hamid Eghbalzadeh",
      "Tushar Nagarajan",
      "Ruta Desai"
    ],
    "abstract": "Our research investigates the capability of modern multimodal reasoning\nmodels, powered by Large Language Models (LLMs), to facilitate vision-powered\nassistants for multi-step daily activities. Such assistants must be able to 1)\nencode relevant visual history from the assistant's sensors, e.g., camera, 2)\nforecast future actions for accomplishing the activity, and 3) replan based on\nthe user in the loop. To evaluate the first two capabilities, grounding visual\nhistory and forecasting in short and long horizons, we conduct benchmarking of\ntwo prominent classes of multimodal LLM approaches -- Socratic Models and\nVision Conditioned Language Models (VCLMs) on video-based action anticipation\ntasks using offline datasets. These offline benchmarks, however, do not allow\nus to close the loop with the user, which is essential to evaluate the\nreplanning capabilities and measure successful activity completion in assistive\nscenarios. To that end, we conduct a first-of-its-kind user study, with 18\nparticipants performing 3 different multi-step cooking activities while wearing\nan egocentric observation device called Aria and following assistance from\nmultimodal LLMs. We find that the Socratic approach outperforms VCLMs in both\noffline and online settings. We further highlight how grounding long visual\nhistory, common in activity assistance, remains challenging in current models,\nespecially for VCLMs, and demonstrate that offline metrics do not indicate\nonline performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.03160v2",
    "published_date": "2024-08-04 06:12:42 UTC",
    "updated_date": "2024-08-12 01:59:00 UTC"
  },
  {
    "arxiv_id": "2408.01935v1",
    "title": "Defining and Evaluating Decision and Composite Risk in Language Models Applied to Natural Language Inference",
    "authors": [
      "Ke Shen",
      "Mayank Kejriwal"
    ],
    "abstract": "Despite their impressive performance, large language models (LLMs) such as\nChatGPT are known to pose important risks. One such set of risks arises from\nmisplaced confidence, whether over-confidence or under-confidence, that the\nmodels have in their inference. While the former is well studied, the latter is\nnot, leading to an asymmetry in understanding the comprehensive risk of the\nmodel based on misplaced confidence. In this paper, we address this asymmetry\nby defining two types of risk (decision and composite risk), and proposing an\nexperimental framework consisting of a two-level inference architecture and\nappropriate metrics for measuring such risks in both discriminative and\ngenerative LLMs. The first level relies on a decision rule that determines\nwhether the underlying language model should abstain from inference. The second\nlevel (which applies if the model does not abstain) is the model's inference.\nDetailed experiments on four natural language commonsense reasoning datasets\nusing both an open-source ensemble-based RoBERTa model and ChatGPT, demonstrate\nthe practical utility of the evaluation framework. For example, our results\nshow that our framework can get an LLM to confidently respond to an extra 20.1%\nof low-risk inference tasks that other methods might misclassify as high-risk,\nand skip 19.8% of high-risk tasks, which would have been answered incorrectly.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2310.03283",
    "pdf_url": "http://arxiv.org/pdf/2408.01935v1",
    "published_date": "2024-08-04 05:24:32 UTC",
    "updated_date": "2024-08-04 05:24:32 UTC"
  },
  {
    "arxiv_id": "2408.01933v4",
    "title": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models",
    "authors": [
      "Bowen Wang",
      "Jiuyang Chang",
      "Yiming Qian",
      "Guoxin Chen",
      "Junhao Chen",
      "Zhouqiang Jiang",
      "Jiahao Zhang",
      "Yuta Nakashima",
      "Hajime Nagahara"
    ],
    "abstract": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages,6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.01933v4",
    "published_date": "2024-08-04 05:15:02 UTC",
    "updated_date": "2025-01-13 07:13:56 UTC"
  },
  {
    "arxiv_id": "2408.01928v1",
    "title": "A Semi-supervised Multi-channel Graph Convolutional Network for Query Classification in E-commerce",
    "authors": [
      "Chunyuan Yuan",
      "Ming Pang",
      "Zheng Fang",
      "Xue Jiang",
      "Changping Peng",
      "Zhangang Lin"
    ],
    "abstract": "Query intent classification is an essential module for customers to find\ndesired products on the e-commerce application quickly. Most existing query\nintent classification methods rely on the users' click behavior as a supervised\nsignal to construct training samples. However, these methods based entirely on\nposterior labels may lead to serious category imbalance problems because of the\nMatthew effect in click samples. Compared with popular categories, it is\ndifficult for products under long-tail categories to obtain traffic and user\nclicks, which makes the models unable to detect users' intent for products\nunder long-tail categories. This in turn aggravates the problem that long-tail\ncategories cannot obtain traffic, forming a vicious circle. In addition, due to\nthe randomness of the user's click, the posterior label is unstable for the\nquery with similar semantics, which makes the model very sensitive to the\ninput, leading to an unstable and incomplete recall of categories.\n  In this paper, we propose a novel Semi-supervised Multi-channel Graph\nConvolutional Network (SMGCN) to address the above problems from the\nperspective of label association and semi-supervised learning. SMGCN extends\ncategory information and enhances the posterior label by utilizing the\nsimilarity score between the query and categories. Furthermore, it leverages\nthe co-occurrence and semantic similarity graph of categories to strengthen the\nrelations among labels and weaken the influence of posterior label instability.\nWe conduct extensive offline and online A/B experiments, and the experimental\nresults show that SMGCN significantly outperforms the strong baselines, which\nshows its effectiveness and practicality.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by WWW2024",
    "pdf_url": "http://arxiv.org/pdf/2408.01928v1",
    "published_date": "2024-08-04 04:52:21 UTC",
    "updated_date": "2024-08-04 04:52:21 UTC"
  },
  {
    "arxiv_id": "2408.01916v2",
    "title": "MAO: A Framework for Process Model Generation with Multi-Agent Orchestration",
    "authors": [
      "Leilei Lin",
      "Yumeng Jin",
      "Yingming Zhou",
      "Wenlong Chen",
      "Chen Qian"
    ],
    "abstract": "Process models are frequently used in software engineering to describe\nbusiness requirements, guide software testing and control system improvement.\nHowever, traditional process modeling methods often require the participation\nof numerous experts, which is expensive and time-consuming. Therefore, the\nexploration of a more efficient and cost-effective automated modeling method\nhas emerged as a focal point in current research. This article explores a\nframework for automatically generating process models with multi-agent\norchestration (MAO), aiming to enhance the efficiency of process modeling and\noffer valuable insights for domain experts. Our framework MAO leverages large\nlanguage models as the cornerstone for multi-agent, employing an innovative\nprompt strategy to ensure efficient collaboration among multi-agent.\nSpecifically, 1) generation. The first phase of MAO is to generate a slightly\nrough process model from the text description; 2) refinement. The agents would\ncontinuously refine the initial process model through multiple rounds of\ndialogue; 3) reviewing. Large language models are prone to hallucination\nphenomena among multi-turn dialogues, so the agents need to review and repair\nsemantic hallucinations in process models; 4) testing. The representation of\nprocess models is diverse. Consequently, the agents utilize external tools to\ntest whether the generated process model contains format errors, namely format\nhallucinations, and then adjust the process model to conform to the output\nparadigm. The experiments demonstrate that the process models generated by our\nframework outperform existing methods and surpass manual modeling by 89%, 61%,\n52%, and 75% on four different datasets, respectively.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.01916v2",
    "published_date": "2024-08-04 03:32:17 UTC",
    "updated_date": "2024-08-07 10:37:38 UTC"
  },
  {
    "arxiv_id": "2408.01904v2",
    "title": "The Artificial Intelligence Disclosure (AID) Framework: An Introduction",
    "authors": [
      "Kari D. Weaver"
    ],
    "abstract": "As the use of Generative Artificial Intelligence tools have grown in higher\neducation and research, there have been increasing calls for transparency and\ngranularity around the use and attribution of the use of these tools. Thus far,\nthis need has been met via the recommended inclusion of a note, with little to\nno guidance on what the note itself should include. This has been identified as\na problem to the use of AI in academic and research contexts. This article\nintroduces The Artificial Intelligence Disclosure (AID) Framework, a standard,\ncomprehensive, and detailed framework meant to inform the development and\nwriting of GenAI disclosure for education and research.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "5 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.01904v2",
    "published_date": "2024-08-04 02:18:42 UTC",
    "updated_date": "2025-04-09 19:03:37 UTC"
  },
  {
    "arxiv_id": "2408.02694v1",
    "title": "KAN based Autoencoders for Factor Models",
    "authors": [
      "Tianqi Wang",
      "Shubham Singh"
    ],
    "abstract": "Inspired by recent advances in Kolmogorov-Arnold Networks (KANs), we\nintroduce a novel approach to latent factor conditional asset pricing models.\nWhile previous machine learning applications in asset pricing have\npredominantly used Multilayer Perceptrons with ReLU activation functions to\nmodel latent factor exposures, our method introduces a KAN-based autoencoder\nwhich surpasses MLP models in both accuracy and interpretability. Our model\noffers enhanced flexibility in approximating exposures as nonlinear functions\nof asset characteristics, while simultaneously providing users with an\nintuitive framework for interpreting latent factors. Empirical backtesting\ndemonstrates our model's superior ability to explain cross-sectional risk\nexposures. Moreover, long-short portfolios constructed using our model's\npredictions achieve higher Sharpe ratios, highlighting its practical value in\ninvestment management.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.CP"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.02694v1",
    "published_date": "2024-08-04 02:02:09 UTC",
    "updated_date": "2024-08-04 02:02:09 UTC"
  },
  {
    "arxiv_id": "2408.04651v1",
    "title": "Knowledge AI: Fine-tuning NLP Models for Facilitating Scientific Knowledge Extraction and Understanding",
    "authors": [
      "Balaji Muralidharan",
      "Hayden Beadles",
      "Reza Marzban",
      "Kalyan Sashank Mupparaju"
    ],
    "abstract": "This project investigates the efficacy of Large Language Models (LLMs) in\nunderstanding and extracting scientific knowledge across specific domains and\nto create a deep learning framework: Knowledge AI. As a part of this framework,\nwe employ pre-trained models and fine-tune them on datasets in the scientific\ndomain. The models are adapted for four key Natural Language Processing (NLP)\ntasks: summarization, text generation, question answering, and named entity\nrecognition. Our results indicate that domain-specific fine-tuning\nsignificantly enhances model performance in each of these tasks, thereby\nimproving their applicability for scientific contexts. This adaptation enables\nnon-experts to efficiently query and extract information within targeted\nscientific fields, demonstrating the potential of fine-tuned LLMs as a tool for\nknowledge discovery in the sciences.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.04651v1",
    "published_date": "2024-08-04 01:32:09 UTC",
    "updated_date": "2024-08-04 01:32:09 UTC"
  },
  {
    "arxiv_id": "2408.01892v1",
    "title": "Re-ENACT: Reinforcement Learning for Emotional Speech Generation using Actor-Critic Strategy",
    "authors": [
      "Ravi Shankar",
      "Archana Venkataraman"
    ],
    "abstract": "In this paper, we propose the first method to modify the prosodic features of\na given speech signal using actor-critic reinforcement learning strategy. Our\napproach uses a Bayesian framework to identify contiguous segments of\nimportance that links segments of the given utterances to perception of\nemotions in humans. We train a neural network to produce the variational\nposterior of a collection of Bernoulli random variables; our model applies a\nMarkov prior on it to ensure continuity. A sample from this distribution is\nused for downstream emotion prediction. Further, we train the neural network to\npredict a soft assignment over emotion categories as the target variable. In\nthe next step, we modify the prosodic features (pitch, intensity, and rhythm)\nof the masked segment to increase the score of target emotion. We employ an\nactor-critic reinforcement learning to train the prosody modifier by\ndiscretizing the space of modifications. Further, it provides a simple solution\nto the problem of gradient computation through WSOLA operation for rhythm\nmanipulation. Our experiments demonstrate that this framework changes the\nperceived emotion of a given speech utterance to the target. Further, we show\nthat our unified technique is on par with state-of-the-art emotion conversion\nmodels from supervised and unsupervised domains that require pairwise training.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "7 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.01892v1",
    "published_date": "2024-08-04 00:47:29 UTC",
    "updated_date": "2024-08-04 00:47:29 UTC"
  }
]
[
  {
    "arxiv_id": "2501.04182v1",
    "title": "Fixed Points of Deep Neural Networks: Emergence, Stability, and Applications",
    "authors": [
      "L. Berlyand",
      "V. Slavin"
    ],
    "abstract": "We present numerical and analytical results on the formation and stability of\na family of fixed points of deep neural networks (DNNs). Such fixed points\nappear in a class of DNNs when dimensions of input and output vectors are the\nsame. We demonstrate examples of applications of such networks in supervised,\nsemi-supervised and unsupervised learning such as encoding/decoding of images,\nrestoration of damaged images among others.\n  We present several numerical and analytical results. First, we show that for\nuntrained DNN's with weights and biases initialized by normally distributed\nrandom variables the only one fixed point exists. This result holds for DNN\nwith any depth (number of layers) $L$, any layer width $N$, and sigmoid-type\nactivation functions. Second, it has been shown that for a DNN whose parameters\n(weights and biases) are initialized by ``light-tailed'' distribution of\nweights (e.g. normal distribution), after training the distribution of these\nparameters become ``heavy-tailed''. This motivates our study of DNNs with\n``heavy-tailed'' initialization. For such DNNs we show numerically %existence\nand stability that training leads to emergence of $Q(N,L)$ fixed points, where\n$Q(N,L)$ is a positive integer which depends on the number of layers $L$ and\nlayer width $N$. We further observe numerically that for fixed $N = N_0$ the\nfunction $Q(N_0, L)$ is non-monotone, that is it initially grows as $L$\nincreases and then decreases to 1.\n  This non-monotone behavior of $Q(N_0, L)$ is also obtained by analytical\nderivation of equation for Empirical Spectral Distribution (ESD) of\ninput-output Jacobian followed by numerical solution of this equation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.04182v1",
    "published_date": "2025-01-07 23:23:26 UTC",
    "updated_date": "2025-01-07 23:23:26 UTC"
  },
  {
    "arxiv_id": "2501.04180v2",
    "title": "HIVEX: A High-Impact Environment Suite for Multi-Agent Research (extended version)",
    "authors": [
      "Philipp Dominic Siedler"
    ],
    "abstract": "Games have been vital test beds for the rapid development of Agent-based\nresearch. Remarkable progress has been achieved in the past, but it is unclear\nif the findings equip for real-world problems. While pressure grows, some of\nthe most critical ecological challenges can find mitigation and prevention\nsolutions through technology and its applications. Most real-world domains\ninclude multi-agent scenarios and require machine-machine and human-machine\ncollaboration. Open-source environments have not advanced and are often toy\nscenarios, too abstract or not suitable for multi-agent research. By mimicking\nreal-world problems and increasing the complexity of environments, we hope to\nadvance state-of-the-art multi-agent research and inspire researchers to work\non immediate real-world problems. Here, we present HIVEX, an environment suite\nto benchmark multi-agent research focusing on ecological challenges. HIVEX\nincludes the following environments: Wind Farm Control, Wildfire Resource\nManagement, Drone-Based Reforestation, Ocean Plastic Collection, and Aerial\nWildfire Suppression. We provide environments, training examples, and baselines\nfor the main and sub-tasks. All trained models resulting from the experiments\nof this work are hosted on Hugging Face. We also provide a leaderboard on\nHugging Face and encourage the community to submit models trained on our\nenvironment suite.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04180v2",
    "published_date": "2025-01-07 23:16:31 UTC",
    "updated_date": "2025-01-21 14:25:45 UTC"
  },
  {
    "arxiv_id": "2501.04173v1",
    "title": "Multimodal Multihop Source Retrieval for Web Question Answering",
    "authors": [
      "Navya Yarrabelly",
      "Saloni Mittal"
    ],
    "abstract": "This work deals with the challenge of learning and reasoning over multi-modal\nmulti-hop question answering (QA). We propose a graph reasoning network based\non the semantic structure of the sentences to learn multi-source reasoning\npaths and find the supporting facts across both image and text modalities for\nanswering the question. In this paper, we investigate the importance of graph\nstructure for multi-modal multi-hop question answering. Our analysis is\ncentered on WebQA. We construct a strong baseline model, that finds relevant\nsources using a pairwise classification task. We establish that, with the\nproper use of feature representations from pre-trained models, graph structure\nhelps in improving multi-modal multi-hop question answering. We point out that\nboth graph structure and adjacency matrix are task-related prior knowledge, and\ngraph structure can be leveraged to improve the retrieval performance for the\ntask. Experiments and visualized analysis demonstrate that message propagation\nover graph networks or the entire graph structure can replace massive\nmultimodal transformers with token-wise cross-attention. We demonstrated the\napplicability of our method and show a performance gain of \\textbf{4.6$\\%$}\nretrieval F1score over the transformer baselines, despite being a very light\nmodel. We further demonstrated the applicability of our model to a large scale\nretrieval setting.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2010.03604 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2501.04173v1",
    "published_date": "2025-01-07 22:53:56 UTC",
    "updated_date": "2025-01-07 22:53:56 UTC"
  },
  {
    "arxiv_id": "2501.04169v1",
    "title": "Learning to Transfer Human Hand Skills for Robot Manipulations",
    "authors": [
      "Sungjae Park",
      "Seungho Lee",
      "Mingi Choi",
      "Jiye Lee",
      "Jeonghwan Kim",
      "Jisoo Kim",
      "Hanbyul Joo"
    ],
    "abstract": "We present a method for teaching dexterous manipulation tasks to robots from\nhuman hand motion demonstrations. Unlike existing approaches that solely rely\non kinematics information without taking into account the plausibility of robot\nand object interaction, our method directly infers plausible robot manipulation\nactions from human motion demonstrations. To address the embodiment gap between\nthe human hand and the robot system, our approach learns a joint motion\nmanifold that maps human hand movements, robot hand actions, and object\nmovements in 3D, enabling us to infer one motion component from others. Our key\nidea is the generation of pseudo-supervision triplets, which pair human,\nobject, and robot motion trajectories synthetically. Through real-world\nexperiments with robot hand manipulation, we demonstrate that our data-driven\nretargeting method significantly outperforms conventional retargeting\ntechniques, effectively bridging the embodiment gap between human and robotic\nhands. Website at https://rureadyo.github.io/MocapRobot/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Preprint. Under Review",
    "pdf_url": "http://arxiv.org/pdf/2501.04169v1",
    "published_date": "2025-01-07 22:33:47 UTC",
    "updated_date": "2025-01-07 22:33:47 UTC"
  },
  {
    "arxiv_id": "2501.04167v1",
    "title": "Reasoning-Enhanced Self-Training for Long-Form Personalized Text Generation",
    "authors": [
      "Alireza Salemi",
      "Cheng Li",
      "Mingyang Zhang",
      "Qiaozhu Mei",
      "Weize Kong",
      "Tao Chen",
      "Zhuowan Li",
      "Michael Bendersky",
      "Hamed Zamani"
    ],
    "abstract": "Personalized text generation requires a unique ability of large language\nmodels (LLMs) to learn from context that they often do not encounter during\ntheir standard training. One way to encourage LLMs to better use personalized\ncontext for generating outputs that better align with the user's expectations\nis to instruct them to reason over the user's past preferences, background\nknowledge, or writing style. To achieve this, we propose Reasoning-Enhanced\nSelf-Training for Personalized Text Generation (REST-PG), a framework that\ntrains LLMs to reason over personal data during response generation. REST-PG\nfirst generates reasoning paths to train the LLM's reasoning abilities and then\nemploys Expectation-Maximization Reinforced Self-Training to iteratively train\nthe LLM based on its own high-reward outputs. We evaluate REST-PG on the\nLongLaMP benchmark, consisting of four diverse personalized long-form text\ngeneration tasks. Our experiments demonstrate that REST-PG achieves significant\nimprovements over state-of-the-art baselines, with an average relative\nperformance gain of 14.5% on the benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04167v1",
    "published_date": "2025-01-07 22:29:08 UTC",
    "updated_date": "2025-01-07 22:29:08 UTC"
  },
  {
    "arxiv_id": "2502.03467v1",
    "title": "Where AI Assurance Might Go Wrong: Initial lessons from engineering of critical systems",
    "authors": [
      "Robin Bloomfield",
      "John Rushby"
    ],
    "abstract": "We draw on our experience working on system and software assurance and\nevaluation for systems important to society to summarise how safety engineering\nis performed in traditional critical systems, such as aircraft flight control.\nWe analyse how this critical systems perspective might support the development\nand implementation of AI Safety Frameworks. We present the analysis in terms\nof: system engineering, safety and risk analysis, and decision analysis and\nsupport.\n  We consider four key questions: What is the system? How good does it have to\nbe? What is the impact of criticality on system development? and How much\nshould we trust it? We identify topics worthy of further discussion. In\nparticular, we are concerned that system boundaries are not broad enough, that\nthe tolerability and nature of the risks are not sufficiently elaborated, and\nthat the assurance methods lack theories that would allow behaviours to be\nadequately assured.\n  We advocate the use of assurance cases based on Assurance 2.0 to support\ndecision making in which the criticality of the decision as well as the\ncriticality of the system are evaluated. We point out the orders of magnitude\ndifference in confidence needed in critical rather than everyday systems and\nhow everyday techniques do not scale in rigour.\n  Finally we map our findings in detail to two of the questions posed by the\nFAISC organisers and we note that the engineering of critical systems has\nevolved through open and diverse discussion. We hope that topics identified\nhere will support the post-FAISC dialogues.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "Presented at UK AI Safety Institute (AISI) Conference on Frontier AI\n  Safety Frameworks (FAISC 24), Berkeley CA, November 2024",
    "pdf_url": "http://arxiv.org/pdf/2502.03467v1",
    "published_date": "2025-01-07 22:02:23 UTC",
    "updated_date": "2025-01-07 22:02:23 UTC"
  },
  {
    "arxiv_id": "2501.04142v1",
    "title": "BiasGuard: Guardrailing Fairness in Machine Learning Production Systems",
    "authors": [
      "Nurit Cohen-Inger",
      "Seffi Cohen",
      "Neomi Rabaev",
      "Lior Rokach",
      "Bracha Shapira"
    ],
    "abstract": "As machine learning (ML) systems increasingly impact critical sectors such as\nhiring, financial risk assessments, and criminal justice, the imperative to\nensure fairness has intensified due to potential negative implications. While\nmuch ML fairness research has focused on enhancing training data and processes,\naddressing the outputs of already deployed systems has received less attention.\nThis paper introduces 'BiasGuard', a novel approach designed to act as a\nfairness guardrail in production ML systems. BiasGuard leverages Test-Time\nAugmentation (TTA) powered by Conditional Generative Adversarial Network\n(CTGAN), a cutting-edge generative AI model, to synthesize data samples\nconditioned on inverted protected attribute values, thereby promoting equitable\noutcomes across diverse groups. This method aims to provide equal opportunities\nfor both privileged and unprivileged groups while significantly enhancing the\nfairness metrics of deployed systems without the need for retraining. Our\ncomprehensive experimental analysis across diverse datasets reveals that\nBiasGuard enhances fairness by 31% while only reducing accuracy by 0.09%\ncompared to non-mitigated benchmarks. Additionally, BiasGuard outperforms\nexisting post-processing methods in improving fairness, positioning it as an\neffective tool to safeguard against biases when retraining the model is\nimpractical.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04142v1",
    "published_date": "2025-01-07 21:10:16 UTC",
    "updated_date": "2025-01-07 21:10:16 UTC"
  },
  {
    "arxiv_id": "2501.04136v1",
    "title": "Implementing Systemic Thinking for Automatic Schema Matching: An Agent-Based Modeling Approach",
    "authors": [
      "Hicham Assoudi",
      "Hakim Lounis"
    ],
    "abstract": "Several approaches are proposed to deal with the problem of the Automatic\nSchema Matching (ASM). The challenges and difficulties caused by the complexity\nand uncertainty characterizing both the process and the outcome of Schema\nMatching motivated us to investigate how bio-inspired emerging paradigm can\nhelp with understanding, managing, and ultimately overcoming those challenges.\nIn this paper, we explain how we approached Automatic Schema Matching as a\nsystemic and Complex Adaptive System (CAS) and how we modeled it using the\napproach of Agent-Based Modeling and Simulation (ABMS). This effort gives birth\nto a tool (prototype) for schema matching called Reflex-SMAS. A set of\nexperiments demonstrates the viability of our approach on two main aspects: (i)\neffectiveness (increasing the quality of the found matchings) and (ii)\nefficiency (reducing the effort required for this efficiency). Our approach\nrepresents a significant paradigm-shift, in the field of Automatic Schema\nMatching.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "COGNITIVE 2018 : The Tenth International Conference on Advanced\n  Cognitive Technologies and Applications",
    "pdf_url": "http://arxiv.org/pdf/2501.04136v1",
    "published_date": "2025-01-07 20:52:08 UTC",
    "updated_date": "2025-01-07 20:52:08 UTC"
  },
  {
    "arxiv_id": "2501.04734v1",
    "title": "Generative Style Transfer for MRI Image Segmentation: A Case of Glioma Segmentation in Sub-Saharan Africa",
    "authors": [
      "Rancy Chepchirchir",
      "Jill Sunday",
      "Raymond Confidence",
      "Dong Zhang",
      "Talha Chaudhry",
      "Udunna C. Anazodo",
      "Kendi Muchungi",
      "Yujing Zou"
    ],
    "abstract": "In Sub-Saharan Africa (SSA), the utilization of lower-quality Magnetic\nResonance Imaging (MRI) technology raises questions about the applicability of\nmachine learning methods for clinical tasks. This study aims to provide a\nrobust deep learning-based brain tumor segmentation (BraTS) method tailored for\nthe SSA population using a threefold approach. Firstly, the impact of domain\nshift from the SSA training data on model efficacy was examined, revealing no\nsignificant effect. Secondly, a comparative analysis of 3D and 2D\nfull-resolution models using the nnU-Net framework indicates similar\nperformance of both the models trained for 300 epochs achieving a five-fold\ncross-validation score of 0.93. Lastly, addressing the performance gap observed\nin SSA validation as opposed to the relatively larger BraTS glioma (GLI)\nvalidation set, two strategies are proposed: fine-tuning SSA cases using the\nGLI+SSA best-pretrained 2D fullres model at 300 epochs, and introducing a novel\nneural style transfer-based data augmentation technique for the SSA cases. This\ninvestigation underscores the potential of enhancing brain tumor prediction\nwithin SSA's unique healthcare landscape.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04734v1",
    "published_date": "2025-01-07 19:48:30 UTC",
    "updated_date": "2025-01-07 19:48:30 UTC"
  },
  {
    "arxiv_id": "2501.04108v2",
    "title": "TrojanDec: Data-free Detection of Trojan Inputs in Self-supervised Learning",
    "authors": [
      "Yupei Liu",
      "Yanting Wang",
      "Jinyuan Jia"
    ],
    "abstract": "An image encoder pre-trained by self-supervised learning can be used as a\ngeneral-purpose feature extractor to build downstream classifiers for various\ndownstream tasks. However, many studies showed that an attacker can embed a\ntrojan into an encoder such that multiple downstream classifiers built based on\nthe trojaned encoder simultaneously inherit the trojan behavior. In this work,\nwe propose TrojanDec, the first data-free method to identify and recover a test\ninput embedded with a trigger. Given a (trojaned or clean) encoder and a test\ninput, TrojanDec first predicts whether the test input is trojaned. If not, the\ntest input is processed in a normal way to maintain the utility. Otherwise, the\ntest input will be further restored to remove the trigger. Our extensive\nevaluation shows that TrojanDec can effectively identify the trojan (if any)\nfrom a given test input and recover it under state-of-the-art trojan attacks.\nWe further demonstrate by experiments that our TrojanDec outperforms the\nstate-of-the-art defenses.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "To appear in AAAI'2025",
    "pdf_url": "http://arxiv.org/pdf/2501.04108v2",
    "published_date": "2025-01-07 19:35:19 UTC",
    "updated_date": "2025-02-04 15:23:17 UTC"
  },
  {
    "arxiv_id": "2501.04102v1",
    "title": "Enhancing Distribution and Label Consistency for Graph Out-of-Distribution Generalization",
    "authors": [
      "Song Wang",
      "Xiaodong Yang",
      "Rashidul Islam",
      "Huiyuan Chen",
      "Minghua Xu",
      "Jundong Li",
      "Yiwei Cai"
    ],
    "abstract": "To deal with distribution shifts in graph data, various graph\nout-of-distribution (OOD) generalization techniques have been recently\nproposed. These methods often employ a two-step strategy that first creates\naugmented environments and subsequently identifies invariant subgraphs to\nimprove generalizability. Nevertheless, this approach could be suboptimal from\nthe perspective of consistency. First, the process of augmenting environments\nby altering the graphs while preserving labels may lead to graphs that are not\nrealistic or meaningfully related to the origin distribution, thus lacking\ndistribution consistency. Second, the extracted subgraphs are obtained from\ndirectly modifying graphs, and may not necessarily maintain a consistent\npredictive relationship with their labels, thereby impacting label consistency.\nIn response to these challenges, we introduce an innovative approach that aims\nto enhance these two types of consistency for graph OOD generalization. We\npropose a modifier to obtain both augmented and invariant graphs in a unified\nmanner. With the augmented graphs, we enrich the training data without\ncompromising the integrity of label-graph relationships. The label consistency\nenhancement in our framework further preserves the supervision information in\nthe invariant graph. We conduct extensive experiments on real-world datasets to\ndemonstrate the superiority of our framework over other state-of-the-art\nbaselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICDM 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.04102v1",
    "published_date": "2025-01-07 19:19:22 UTC",
    "updated_date": "2025-01-07 19:19:22 UTC"
  },
  {
    "arxiv_id": "2501.04733v1",
    "title": "AI-Driven Reinvention of Hydrological Modeling for Accurate Predictions and Interpretation to Transform Earth System Modeling",
    "authors": [
      "Cuihui Xia",
      "Lei Yue",
      "Deliang Chen",
      "Yuyang Li",
      "Hongqiang Yang",
      "Ancheng Xue",
      "Zhiqiang Li",
      "Qing He",
      "Guoqing Zhang",
      "Dambaru Ballab Kattel",
      "Lei Lei",
      "Ming Zhou"
    ],
    "abstract": "Traditional equation-driven hydrological models often struggle to accurately\npredict streamflow in challenging regional Earth systems like the Tibetan\nPlateau, while hybrid and existing algorithm-driven models face difficulties in\ninterpreting hydrological behaviors. This work introduces HydroTrace, an\nalgorithm-driven, data-agnostic model that substantially outperforms these\napproaches, achieving a Nash-Sutcliffe Efficiency of 98% and demonstrating\nstrong generalization on unseen data. Moreover, HydroTrace leverages advanced\nattention mechanisms to capture spatial-temporal variations and\nfeature-specific impacts, enabling the quantification and spatial resolution of\nstreamflow partitioning as well as the interpretation of hydrological behaviors\nsuch as glacier-snow-streamflow interactions and monsoon dynamics.\nAdditionally, a large language model (LLM)-based application allows users to\neasily understand and apply HydroTrace's insights for practical purposes. These\nadvancements position HydroTrace as a transformative tool in hydrological and\nbroader Earth system modeling, offering enhanced prediction accuracy and\ninterpretability.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "physics.ao-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04733v1",
    "published_date": "2025-01-07 18:59:53 UTC",
    "updated_date": "2025-01-07 18:59:53 UTC"
  },
  {
    "arxiv_id": "2501.03968v2",
    "title": "VLM-driven Behavior Tree for Context-aware Task Planning",
    "authors": [
      "Naoki Wake",
      "Atsushi Kanehira",
      "Jun Takamatsu",
      "Kazuhiro Sasabuchi",
      "Katsushi Ikeuchi"
    ],
    "abstract": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages, 11 figures, 5 tables. Last updated on January 9th, 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.03968v2",
    "published_date": "2025-01-07 18:06:27 UTC",
    "updated_date": "2025-01-10 10:38:49 UTC"
  },
  {
    "arxiv_id": "2501.03952v1",
    "title": "Localizing AI: Evaluating Open-Weight Language Models for Languages of Baltic States",
    "authors": [
      "Jurgita Kapočiūtė-Dzikienė",
      "Toms Bergmanis",
      "Mārcis Pinnis"
    ],
    "abstract": "Although large language models (LLMs) have transformed our expectations of\nmodern language technologies, concerns over data privacy often restrict the use\nof commercially available LLMs hosted outside of EU jurisdictions. This limits\ntheir application in governmental, defence, and other data-sensitive sectors.\nIn this work, we evaluate the extent to which locally deployable open-weight\nLLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.\nWe examine various size and precision variants of the top-performing\nmultilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine\ntranslation, multiple-choice question answering, and free-form text generation.\nThe results indicate that while certain models like Gemma~2 perform close to\nthe top commercially available models, many LLMs struggle with these languages.\nMost surprisingly, however, we find that these models, while showing close to\nstate-of-the-art translation performance, are still prone to lexical\nhallucinations with errors in at least 1 in 20 words for all open-weight\nmultilingual LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper is accepted to NoDaLiDa/Baltic-HLT 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03952v1",
    "published_date": "2025-01-07 17:24:17 UTC",
    "updated_date": "2025-01-07 17:24:17 UTC"
  },
  {
    "arxiv_id": "2501.03941v1",
    "title": "Synthetic Data Privacy Metrics",
    "authors": [
      "Amy Steier",
      "Lipika Ramaswamy",
      "Andre Manoel",
      "Alexa Haushalter"
    ],
    "abstract": "Recent advancements in generative AI have made it possible to create\nsynthetic datasets that can be as accurate as real-world data for training AI\nmodels, powering statistical insights, and fostering collaboration with\nsensitive datasets while offering strong privacy guarantees. Effectively\nmeasuring the empirical privacy of synthetic data is an important step in the\nprocess. However, while there is a multitude of new privacy metrics being\npublished every day, there currently is no standardization. In this paper, we\nreview the pros and cons of popular metrics that include simulations of\nadversarial attacks. We also review current best practices for amending\ngenerative models to enhance the privacy of the data they create (e.g.\ndifferential privacy).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.03941v1",
    "published_date": "2025-01-07 17:02:33 UTC",
    "updated_date": "2025-01-07 17:02:33 UTC"
  },
  {
    "arxiv_id": "2501.03940v2",
    "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection",
    "authors": [
      "Pablo Miralles-González",
      "Javier Huertas-Tato",
      "Alejandro Martín",
      "David Camacho"
    ],
    "abstract": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03940v2",
    "published_date": "2025-01-07 17:00:49 UTC",
    "updated_date": "2025-01-22 10:39:50 UTC"
  },
  {
    "arxiv_id": "2501.03936v3",
    "title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides",
    "authors": [
      "Hao Zheng",
      "Xinyan Guan",
      "Hao Kong",
      "Jia Zheng",
      "Weixiang Zhou",
      "Hongyu Lin",
      "Yaojie Lu",
      "Ben He",
      "Xianpei Han",
      "Le Sun"
    ],
    "abstract": "Automatically generating presentations from documents is a challenging task\nthat requires accommodating content quality, visual appeal, and structural\ncoherence. Existing methods primarily focus on improving and evaluating the\ncontent quality in isolation, overlooking visual appeal and structural\ncoherence, which limits their practical applicability. To address these\nlimitations, we propose PPTAgent, which comprehensively improves presentation\ngeneration through a two-stage, edit-based approach inspired by human\nworkflows. PPTAgent first analyzes reference presentations to extract\nslide-level functional types and content schemas, then drafts an outline and\niteratively generates editing actions based on selected reference slides to\ncreate new slides. To comprehensively evaluate the quality of generated\npresentations, we further introduce PPTEval, an evaluation framework that\nassesses presentations across three dimensions: Content, Design, and Coherence.\nResults demonstrate that PPTAgent significantly outperforms existing automatic\npresentation generation methods across all three dimensions.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 23 figures, see https://github.com/icip-cas/PPTAgent for\n  details",
    "pdf_url": "http://arxiv.org/pdf/2501.03936v3",
    "published_date": "2025-01-07 16:53:01 UTC",
    "updated_date": "2025-02-21 07:52:39 UTC"
  },
  {
    "arxiv_id": "2501.04072v1",
    "title": "Multi-armed Bandit and Backbone boost Lin-Kernighan-Helsgaun Algorithm for the Traveling Salesman Problems",
    "authors": [
      "Long Wang",
      "Jiongzhi Zheng",
      "Zhengda Xiong",
      "Kun He"
    ],
    "abstract": "The Lin-Kernighan-Helsguan (LKH) heuristic is a classic local search\nalgorithm for the Traveling Salesman Problem (TSP). LKH introduces an\n$\\alpha$-value to replace the traditional distance metric for evaluating the\nedge quality, which leads to a significant improvement. However, we observe\nthat the $\\alpha$-value does not make full use of the historical information\nduring the search, and single guiding information often makes LKH hard to\nescape from some local optima. To address the above issues, we propose a novel\nway to extract backbone information during the TSP local search process, which\nis dynamic and can be updated once a local optimal solution is found. We\nfurther propose to combine backbone information, $\\alpha$-value, and distance\nto evaluate the edge quality so as to guide the search. Moreover, we abstract\ntheir different combinations to arms in a multi-armed bandit (MAB) and use an\nMAB model to help the algorithm select an appropriate evaluation metric\ndynamically. Both the backbone information and MAB can provide diverse guiding\ninformation and learn from the search history to suggest the best metric. We\napply our methods to LKH and LKH-3, which is an extension version of LKH that\ncan be used to solve about 40 variant problems of TSP and Vehicle Routing\nProblem (VRP). Extensive experiments show the excellent performance and\ngeneralization capability of our proposed method, significantly improving LKH\nfor TSP and LKH-3 for two representative TSP and VRP variants, the Colored TSP\n(CTSP) and Capacitated VRP with Time Windows (CVRPTW).",
    "categories": [
      "cs.DS",
      "cs.AI"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04072v1",
    "published_date": "2025-01-07 16:45:41 UTC",
    "updated_date": "2025-01-07 16:45:41 UTC"
  },
  {
    "arxiv_id": "2501.03916v3",
    "title": "Dolphin: Moving Towards Closed-loop Auto-research through Thinking, Practice, and Feedback",
    "authors": [
      "Jiakang Yuan",
      "Xiangchao Yan",
      "Shiyang Feng",
      "Bo Zhang",
      "Tao Chen",
      "Botian Shi",
      "Wanli Ouyang",
      "Yu Qiao",
      "Lei Bai",
      "Bowen Zhou"
    ],
    "abstract": "The scientific research paradigm is undergoing a profound transformation\nowing to the development of Artificial Intelligence (AI). Recent works\ndemonstrate that various AI-assisted research methods can largely improve\nresearch efficiency by improving data analysis, accelerating computation, and\nfostering novel idea generation. To further move towards the ultimate goal\n(i.e., automatic scientific research), in this paper, we introduce Dolphin, a\nclosed-loop LLM-driven framework to enhance the automation level of scientific\nresearch. Dolphin first generates novel ideas based on feedback from previous\nexperiments and relevant papers ranked by the topic and task attributes. Then,\nthe generated ideas can be implemented using a code template refined and\ndebugged with the designed exception-traceback-guided local code structure.\nFinally, Dolphin automatically analyzes the results of each idea and feeds the\nresults back to the next round of idea generation. Experiments are conducted on\nthe benchmark datasets of different topics and a subset of MLE-bench. Results\nshow that Dolphin can continuously improve the performance of the input topic\nin a loop. We highlight that Dolphin can automatically propose methods that are\ncomparable to the state-of-the-art in some tasks such as 3D point\nclassification.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 12 figures, and our homepage:\n  https://alpha-innovator.github.io/Dolphin-project-page",
    "pdf_url": "http://arxiv.org/pdf/2501.03916v3",
    "published_date": "2025-01-07 16:31:10 UTC",
    "updated_date": "2025-04-09 16:27:02 UTC"
  },
  {
    "arxiv_id": "2501.10413v1",
    "title": "Cooperative Search and Track of Rogue Drones using Multiagent Reinforcement Learning",
    "authors": [
      "Panayiota Valianti",
      "Kleanthis Malialis",
      "Panayiotis Kolios",
      "Georgios Ellinas"
    ],
    "abstract": "This work considers the problem of intercepting rogue drones targeting\nsensitive critical infrastructure facilities. While current interception\ntechnologies focus mainly on the jamming/spoofing tasks, the challenges of\neffectively locating and tracking rogue drones have not received adequate\nattention. Solving this problem and integrating with recently proposed\ninterception techniques will enable a holistic system that can reliably detect,\ntrack, and neutralize rogue drones. Specifically, this work considers a team of\npursuer UAVs that can search, detect, and track multiple rogue drones over a\nsensitive facility. The joint search and track problem is addressed through a\nnovel multiagent reinforcement learning scheme to optimize the agent mobility\ncontrol actions that maximize the number of rogue drones detected and tracked.\nThe performance of the proposed system is investigated under realistic settings\nthrough extensive simulation experiments with varying number of agents\ndemonstrating both its performance and scalability.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10413v1",
    "published_date": "2025-01-07 16:22:51 UTC",
    "updated_date": "2025-01-07 16:22:51 UTC"
  },
  {
    "arxiv_id": "2501.03904v1",
    "title": "Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study",
    "authors": [
      "Ramya Jonnala",
      "Gongbo Liang",
      "Jeong Yang",
      "Izzat Alsmadi"
    ],
    "abstract": "The integration of large language models (LLMs) into public transit systems\npresents a transformative opportunity to enhance urban mobility. This study\nexplores the potential of LLMs to revolutionize public transportation\nmanagement within the context of San Antonio's transit system. Leveraging the\ncapabilities of LLMs in natural language processing and data analysis, we\ninvestigate their capabilities to optimize route planning, reduce wait times,\nand provide personalized travel assistance. By utilizing the General Transit\nFeed Specification (GTFS) and other relevant data, this research aims to\ndemonstrate how LLMs can potentially improve resource allocation, elevate\npassenger satisfaction, and inform data-driven decision-making in transit\noperations. A comparative analysis of different ChatGPT models was conducted to\nassess their ability to understand transportation information, retrieve\nrelevant data, and provide comprehensive responses. Findings from this study\nsuggest that while LLMs hold immense promise for public transit, careful\nengineering and fine-tuning are essential to realizing their full potential.\nSan Antonio serves as a case study to inform the development of LLM-powered\ntransit systems in other urban environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "This work is accepted to AAAI 2025 Workshop on AI for Urban Planning.\n  arXiv admin note: substantial text overlap with arXiv:2407.11003",
    "pdf_url": "http://arxiv.org/pdf/2501.03904v1",
    "published_date": "2025-01-07 16:18:55 UTC",
    "updated_date": "2025-01-07 16:18:55 UTC"
  },
  {
    "arxiv_id": "2501.03902v1",
    "title": "Explainable Reinforcement Learning via Temporal Policy Decomposition",
    "authors": [
      "Franco Ruggeri",
      "Alessio Russo",
      "Rafia Inam",
      "Karl Henrik Johansson"
    ],
    "abstract": "We investigate the explainability of Reinforcement Learning (RL) policies\nfrom a temporal perspective, focusing on the sequence of future outcomes\nassociated with individual actions. In RL, value functions compress information\nabout rewards collected across multiple trajectories and over an infinite\nhorizon, allowing a compact form of knowledge representation. However, this\ncompression obscures the temporal details inherent in sequential\ndecision-making, presenting a key challenge for interpretability. We present\nTemporal Policy Decomposition (TPD), a novel explainability approach that\nexplains individual RL actions in terms of their Expected Future Outcome (EFO).\nThese explanations decompose generalized value functions into a sequence of\nEFOs, one for each time step up to a prediction horizon of interest, revealing\ninsights into when specific outcomes are expected to occur. We leverage\nfixed-horizon temporal difference learning to devise an off-policy method for\nlearning EFOs for both optimal and suboptimal actions, enabling contrastive\nexplanations consisting of EFOs for different state-action pairs. Our\nexperiments demonstrate that TPD generates accurate explanations that (i)\nclarify the policy's future strategy and anticipated trajectory for a given\naction and (ii) improve understanding of the reward composition, facilitating\nfine-tuning of the reward function to align with human expectations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.03902v1",
    "published_date": "2025-01-07 16:10:09 UTC",
    "updated_date": "2025-01-07 16:10:09 UTC"
  },
  {
    "arxiv_id": "2501.03895v2",
    "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token",
    "authors": [
      "Shaolei Zhang",
      "Qingkai Fang",
      "Zhe Yang",
      "Yang Feng"
    ],
    "abstract": "The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICLR 2025. Code: https://github.com/ictnlp/LLaVA-Mini\n  Model: https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b",
    "pdf_url": "http://arxiv.org/pdf/2501.03895v2",
    "published_date": "2025-01-07 16:03:14 UTC",
    "updated_date": "2025-03-02 15:55:07 UTC"
  },
  {
    "arxiv_id": "2501.05478v1",
    "title": "Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models",
    "authors": [
      "Malak Mansour",
      "Ahmed Aly",
      "Bahey Tharwat",
      "Sarim Hashmi",
      "Dong An",
      "Ian Reid"
    ],
    "abstract": "Large Language Models (LLMs) such as GPT-4, trained on huge amount of\ndatasets spanning multiple domains, exhibit significant reasoning,\nunderstanding, and planning capabilities across various tasks. This study\npresents the first-ever work in Arabic language integration within the\nVision-and-Language Navigation (VLN) domain in robotics, an area that has been\nnotably underexplored in existing research. We perform a comprehensive\nevaluation of state-of-the-art multi-lingual Small Language Models (SLMs),\nincluding GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the\nArabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure\nLLM-based instruction-following navigation agent, to assess the impact of\nlanguage on navigation reasoning through zero-shot sequential action prediction\nusing the R2R dataset. Through comprehensive experiments, we demonstrate that\nour framework is capable of high-level planning for navigation tasks when\nprovided with instructions in both English and Arabic. However, certain models\nstruggled with reasoning and planning in the Arabic language due to inherent\nlimitations in their capabilities, sub-optimal performance, and parsing issues.\nThese findings highlight the importance of enhancing planning and reasoning\ncapabilities in language models for effective navigation, emphasizing this as a\nkey area for further development while also unlocking the potential of\nArabic-language models for impactful real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05478v1",
    "published_date": "2025-01-07 16:01:25 UTC",
    "updated_date": "2025-01-07 16:01:25 UTC"
  },
  {
    "arxiv_id": "2501.03888v4",
    "title": "Neural DNF-MT: A Neuro-symbolic Approach for Learning Interpretable and Editable Policies",
    "authors": [
      "Kexin Gu Baugh",
      "Luke Dickens",
      "Alessandra Russo"
    ],
    "abstract": "Although deep reinforcement learning has been shown to be effective, the\nmodel's black-box nature presents barriers to direct policy interpretation. To\naddress this problem, we propose a neuro-symbolic approach called neural DNF-MT\nfor end-to-end policy learning. The differentiable nature of the neural DNF-MT\nmodel enables the use of deep actor-critic algorithms for training. At the same\ntime, its architecture is designed so that trained models can be directly\ntranslated into interpretable policies expressed as standard (bivalent or\nprobabilistic) logic programs. Moreover, additional layers can be included to\nextract abstract features from complex observations, acting as a form of\npredicate invention. The logic representations are highly interpretable, and we\nshow how the bivalent representations of deterministic policies can be edited\nand incorporated back into a neural model, facilitating manual intervention and\nadaptation of learned policies. We evaluate our approach on a range of tasks\nrequiring learning deterministic or stochastic behaviours from various forms of\nobservations. Our empirical results show that our neural DNF-MT model performs\nat the level of competing black-box methods whilst providing interpretable\npolicies.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "AAMAS 2025 (with Appendix)",
    "pdf_url": "http://arxiv.org/pdf/2501.03888v4",
    "published_date": "2025-01-07 15:51:49 UTC",
    "updated_date": "2025-04-23 21:30:44 UTC"
  },
  {
    "arxiv_id": "2501.03879v1",
    "title": "CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds Ratio on High-Resolution Point Clouds",
    "authors": [
      "Keonwoo Kim",
      "Yeongjae Cho",
      "Taebaek Hwang",
      "Minsoo Jo",
      "Sangdo Han"
    ],
    "abstract": "Recent research has demonstrated that Large Language Models (LLMs) are not\nlimited to text-only tasks but can also function as multimodal models across\nvarious modalities, including audio, images, and videos. In particular,\nresearch on 3D Large Multimodal Models (3D LMMs) is making notable strides,\ndriven by the potential of processing higher-dimensional data like point\nclouds. However, upon closer examination, we find that the visual and textual\ncontent within each sample of existing training datasets lacks both high\ninformational granularity and clarity, which serve as a bottleneck for precise\ncross-modal understanding. To address these issues, we propose CL3DOR,\nContrastive Learning for 3D large multimodal models via Odds ratio on\nhigh-Resolution point clouds, designed to ensure greater specificity and\nclarity in both visual and textual content. Specifically, we increase the\ndensity of point clouds per object and construct informative hard negative\nresponses in the training dataset to penalize unwanted responses. To leverage\nhard negative responses, we incorporate the odds ratio as an auxiliary term for\ncontrastive learning into the conventional language modeling loss. CL3DOR\nachieves state-of-the-art performance in 3D scene understanding and reasoning\nbenchmarks. Additionally, we demonstrate the effectiveness of CL3DOR's key\ncomponents through extensive experiments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03879v1",
    "published_date": "2025-01-07 15:42:32 UTC",
    "updated_date": "2025-01-07 15:42:32 UTC"
  },
  {
    "arxiv_id": "2501.03847v2",
    "title": "Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control",
    "authors": [
      "Zekai Gu",
      "Rui Yan",
      "Jiahao Lu",
      "Peng Li",
      "Zhiyang Dou",
      "Chenyang Si",
      "Zhen Dong",
      "Qifeng Liu",
      "Cheng Lin",
      "Ziwei Liu",
      "Wenping Wang",
      "Yuan Liu"
    ],
    "abstract": "Diffusion models have demonstrated impressive performance in generating\nhigh-quality videos from text prompts or images. However, precise control over\nthe video generation process, such as camera manipulation or content editing,\nremains a significant challenge. Existing methods for controlled video\ngeneration are typically limited to a single control type, lacking the\nflexibility to handle diverse control demands. In this paper, we introduce\nDiffusion as Shader (DaS), a novel approach that supports multiple video\ncontrol tasks within a unified architecture. Our key insight is that achieving\nversatile video control necessitates leveraging 3D control signals, as videos\nare fundamentally 2D renderings of dynamic 3D content. Unlike prior methods\nlimited to 2D control signals, DaS leverages 3D tracking videos as control\ninputs, making the video diffusion process inherently 3D-aware. This innovation\nallows DaS to achieve a wide range of video controls by simply manipulating the\n3D tracking videos. A further advantage of using 3D tracking videos is their\nability to effectively link frames, significantly enhancing the temporal\nconsistency of the generated videos. With just 3 days of fine-tuning on 8 H800\nGPUs using less than 10k videos, DaS demonstrates strong control capabilities\nacross diverse tasks, including mesh-to-video generation, camera control,\nmotion transfer, and object manipulation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://igl-hkust.github.io/das/ Codes:\n  https://github.com/IGL-HKUST/DiffusionAsShader",
    "pdf_url": "http://arxiv.org/pdf/2501.03847v2",
    "published_date": "2025-01-07 15:01:58 UTC",
    "updated_date": "2025-01-09 04:25:42 UTC"
  },
  {
    "arxiv_id": "2501.04070v2",
    "title": "More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives",
    "authors": [
      "Xiaoqing Zhang",
      "Ang Lv",
      "Yuhan Liu",
      "Flood Sung",
      "Wei Liu",
      "Shuo Shang",
      "Xiuying Chen",
      "Rui Yan"
    ],
    "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL)\nwithout requiring parameter updates. However, as the number of ICL\ndemonstrations increases from a few to many, performance tends to plateau and\neventually decline. We identify two primary causes for this trend: the\nsuboptimal negative log-likelihood (NLL) optimization objective and the\nincremental data noise. To address these issues, we introduce DrICL, a novel\noptimization method that enhances model performance through Differentiated\nLearning and advantage-based Reweighting objectives. Globally, DrICL utilizes\ndifferentiated learning to optimize the NLL objective, ensuring that many-shot\nperformance surpasses zero-shot levels. Locally, it dynamically adjusts the\nweighting of many-shot demonstrations by leveraging cumulative advantages\ninspired by reinforcement learning, thereby improving generalization. This\napproach allows the model to handle varying numbers of shots effectively,\nmitigating the impact of noisy data. Recognizing the lack of multi-task\ndatasets with diverse many-shot distributions, we develop the Many-Shot ICL\nBenchmark (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers\nfrom 1 to 350 within sequences of up to 8,000 tokens-for fine-tuning purposes.\nICL-50 facilitates the evaluation of many-shot ICL strategies across seven\nprominent NLP tasks and 50 distinct datasets. Experimental results demonstrate\nthat LLMs enhanced with DrICL achieve significant improvements in many-shot\nsetups across various tasks, including both in-domain and out-of-domain\nscenarios. We release the code and benchmark dataset hoping to facilitate\nfurther research in many-shot ICL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 8 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.04070v2",
    "published_date": "2025-01-07 14:57:08 UTC",
    "updated_date": "2025-01-09 02:20:13 UTC"
  },
  {
    "arxiv_id": "2501.03836v3",
    "title": "SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor Diagnosis",
    "authors": [
      "Runci Bai",
      "Guibao Xu",
      "Yanze Shi"
    ],
    "abstract": "Brain tumors can lead to neurological dysfunction, cognitive and\npsychological changes, increased intracranial pressure, and seizures, posing\nsignificant risks to health. The You Only Look Once (YOLO) series has shown\nsuperior accuracy in medical imaging object detection. This paper presents a\nnovel SCC-YOLO architecture that integrates the SCConv module into YOLOv9. The\nSCConv module optimizes convolutional efficiency by reducing spatial and\nchannel redundancy, enhancing image feature learning. We examine the effects of\ndifferent attention mechanisms with YOLOv9 for brain tumor detection using the\nBr35H dataset and our custom dataset (Brain_Tumor_Dataset). Results indicate\nthat SCC-YOLO improved mAP50 by 0.3% on the Br35H dataset and by 0.5% on our\ncustom dataset compared to YOLOv9. SCC-YOLO achieves state-of-the-art\nperformance in brain tumor detection.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03836v3",
    "published_date": "2025-01-07 14:45:39 UTC",
    "updated_date": "2025-03-02 06:41:56 UTC"
  },
  {
    "arxiv_id": "2501.03835v2",
    "title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification",
    "authors": [
      "Yindu Su",
      "Huike Zou",
      "Lin Sun",
      "Ting Zhang",
      "Haiyang Yang",
      "Liyu Chen",
      "David Lo",
      "Qingheng Zhang",
      "Shuguang Han",
      "Jufeng Chen"
    ],
    "abstract": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendations, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity to the item embedding. It leverages contrastive training with\ntaxonomy-aware hard negative sampling and employs adaptive inference with\ndynamic thresholds. TACLR offers three key advantages: (1) it effectively\nhandles implicit and OOD values while producing normalized outputs; (2) it\nscales to thousands of categories, tens of thousands of attributes, and\nmillions of values; and (3) it supports efficient inference for high-load\nindustrial scenarios. Extensive experiments on proprietary and public datasets\nvalidate the effectiveness and efficiency of TACLR. Moreover, it has been\nsuccessfully deployed in a real-world e-commerce platform, processing millions\nof product listings daily while supporting dynamic, large-scale attribute\ntaxonomies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03835v2",
    "published_date": "2025-01-07 14:45:30 UTC",
    "updated_date": "2025-02-08 11:13:23 UTC"
  },
  {
    "arxiv_id": "2501.03832v1",
    "title": "Three-dimensional attention Transformer for state evaluation in real-time strategy games",
    "authors": [
      "Yanqing Ye",
      "Weilong Yang",
      "Kai Qiu",
      "Jie Zhang"
    ],
    "abstract": "Situation assessment in Real-Time Strategy (RTS) games is crucial for\nunderstanding decision-making in complex adversarial environments. However,\nexisting methods remain limited in processing multi-dimensional feature\ninformation and temporal dependencies. Here we propose a tri-dimensional\nSpace-Time-Feature Transformer (TSTF Transformer) architecture, which\nefficiently models battlefield situations through three independent but\ncascaded modules: spatial attention, temporal attention, and feature attention.\nOn a dataset comprising 3,150 adversarial experiments, the 8-layer TSTF\nTransformer demonstrates superior performance: achieving 58.7% accuracy in the\nearly game (~4% progress), significantly outperforming the conventional\nTimesformer's 41.8%; reaching 97.6% accuracy in the mid-game (~40% progress)\nwhile maintaining low performance variation (standard deviation 0.114).\nMeanwhile, this architecture requires fewer parameters (4.75M) compared to the\nbaseline model (5.54M). Our study not only provides new insights into situation\nassessment in RTS games but also presents an innovative paradigm for\nTransformer-based multi-dimensional temporal modeling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.03832v1",
    "published_date": "2025-01-07 14:42:38 UTC",
    "updated_date": "2025-01-07 14:42:38 UTC"
  },
  {
    "arxiv_id": "2501.03825v1",
    "title": "Deep Sylvester Posterior Inference for Adaptive Compressed Sensing in Ultrasound Imaging",
    "authors": [
      "Simon W. Penninga",
      "Hans van Gorp",
      "Ruud J. G. van Sloun"
    ],
    "abstract": "Ultrasound images are commonly formed by sequential acquisition of\nbeam-steered scan-lines. Minimizing the number of required scan-lines can\nsignificantly enhance frame rate, field of view, energy efficiency, and data\ntransfer speeds. Existing approaches typically use static subsampling schemes\nin combination with sparsity-based or, more recently, deep-learning-based\nrecovery. In this work, we introduce an adaptive subsampling method that\nmaximizes intrinsic information gain in-situ, employing a Sylvester Normalizing\nFlow encoder to infer an approximate Bayesian posterior under partial\nobservation in real-time. Using the Bayesian posterior and a deep generative\nmodel for future observations, we determine the subsampling scheme that\nmaximizes the mutual information between the subsampled observations, and the\nnext frame of the video. We evaluate our approach using the EchoNet cardiac\nultrasound video dataset and demonstrate that our active sampling method\noutperforms competitive baselines, including uniform and variable-density\nrandom sampling, as well as equidistantly spaced scan-lines, improving mean\nabsolute reconstruction error by 15%. Moreover, posterior inference and the\nsampling scheme generation are performed in just 0.015 seconds (66Hz), making\nit fast enough for real-time 2D ultrasound imaging applications.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03825v1",
    "published_date": "2025-01-07 14:37:14 UTC",
    "updated_date": "2025-01-07 14:37:14 UTC"
  },
  {
    "arxiv_id": "2501.03824v1",
    "title": "Online Reinforcement Learning-Based Dynamic Adaptive Evaluation Function for Real-Time Strategy Tasks",
    "authors": [
      "Weilong Yang",
      "Jie Zhang",
      "Xunyun Liu",
      "Yanqing Ye"
    ],
    "abstract": "Effective evaluation of real-time strategy tasks requires adaptive mechanisms\nto cope with dynamic and unpredictable environments. This study proposes a\nmethod to improve evaluation functions for real-time responsiveness to\nbattle-field situation changes, utilizing an online reinforcement\nlearning-based dynam-ic weight adjustment mechanism within the real-time\nstrategy game. Building on traditional static evaluation functions, the method\nemploys gradient descent in online reinforcement learning to update weights\ndynamically, incorporating weight decay techniques to ensure stability.\nAdditionally, the AdamW optimizer is integrated to adjust the learning rate and\ndecay rate of online reinforcement learning in real time, further reducing the\ndependency on manual parameter tun-ing. Round-robin competition experiments\ndemonstrate that this method signifi-cantly enhances the application\neffectiveness of the Lanchester combat model evaluation function, Simple\nevaluation function, and Simple Sqrt evaluation function in planning algorithms\nincluding IDABCD, IDRTMinimax, and Port-folio AI. The method achieves a notable\nimprovement in scores, with the en-hancement becoming more pronounced as the\nmap size increases. Furthermore, the increase in evaluation function\ncomputation time induced by this method is kept below 6% for all evaluation\nfunctions and planning algorithms. The pro-posed dynamic adaptive evaluation\nfunction demonstrates a promising approach for real-time strategy task\nevaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.03824v1",
    "published_date": "2025-01-07 14:36:33 UTC",
    "updated_date": "2025-01-07 14:36:33 UTC"
  },
  {
    "arxiv_id": "2501.03795v1",
    "title": "Self-Adaptive ERP: Embedding NLP into Petri-Net creation and Model Matching",
    "authors": [
      "Ahmed Maged",
      "Gamal Kassem"
    ],
    "abstract": "Enterprise Resource Planning (ERP) consultants play a vital role in\ncustomizing systems to meet specific business needs by processing large amounts\nof data and adapting functionalities. However, the process is\nresource-intensive, time-consuming, and requires continuous adjustments as\nbusiness demands evolve. This research introduces a Self-Adaptive ERP Framework\nthat automates customization using enterprise process models and system usage\nanalysis. It leverages Artificial Intelligence (AI) & Natural Language\nProcessing (NLP) for Petri nets to transform business processes into adaptable\nmodels, addressing both structural and functional matching. The framework,\nbuilt using Design Science Research (DSR) and a Systematic Literature Review\n(SLR), reduces reliance on manual adjustments, improving ERP customization\nefficiency and accuracy while minimizing the need for consultants.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03795v1",
    "published_date": "2025-01-07 14:01:59 UTC",
    "updated_date": "2025-01-07 14:01:59 UTC"
  },
  {
    "arxiv_id": "2501.04068v1",
    "title": "Explainable Reinforcement Learning for Formula One Race Strategy",
    "authors": [
      "Devin Thomas",
      "Junqi Jiang",
      "Avinash Kori",
      "Aaron Russo",
      "Steffen Winkler",
      "Stuart Sale",
      "Joseph McMillan",
      "Francesco Belardinelli",
      "Antonio Rago"
    ],
    "abstract": "In Formula One, teams compete to develop their cars and achieve the highest\npossible finishing position in each race. During a race, however, teams are\nunable to alter the car, so they must improve their cars' finishing positions\nvia race strategy, i.e. optimising their selection of which tyre compounds to\nput on the car and when to do so. In this work, we introduce a reinforcement\nlearning model, RSRL (Race Strategy Reinforcement Learning), to control race\nstrategies in simulations, offering a faster alternative to the industry\nstandard of hard-coded and Monte Carlo-based race strategies. Controlling cars\nwith a pace equating to an expected finishing position of P5.5 (where P1\nrepresents first place and P20 is last place), RSRL achieves an average\nfinishing position of P5.33 on our test race, the 2023 Bahrain Grand Prix,\noutperforming the best baseline of P5.63. We then demonstrate, in a\ngeneralisability study, how performance for one track or multiple tracks can be\nprioritised via training. Further, we supplement model predictions with feature\nimportance, decision tree-based surrogate models, and decision tree\ncounterfactuals towards improving user trust in the model. Finally, we provide\nillustrations which exemplify our approach in real-world situations, drawing\nparallels between simulations and reality.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 6 figures. Copyright ACM 2025. This is the authors' version\n  of the work. It is posted here for your personal use. Not for redistribution.\n  The definitive Version of Record will be published in SAC 2025,\n  http://dx.doi.org/10.1145/3672608.3707766",
    "pdf_url": "http://arxiv.org/pdf/2501.04068v1",
    "published_date": "2025-01-07 13:54:19 UTC",
    "updated_date": "2025-01-07 13:54:19 UTC"
  },
  {
    "arxiv_id": "2501.03764v1",
    "title": "SelectiveFinetuning: Enhancing Transfer Learning in Sleep Staging through Selective Domain Alignment",
    "authors": [
      "Siyuan Zhao",
      "Chenyu Liu",
      "Yi Ding",
      "Xinliang Zhou"
    ],
    "abstract": "In practical sleep stage classification, a key challenge is the variability\nof EEG data across different subjects and environments. Differences in\nphysiology, age, health status, and recording conditions can lead to domain\nshifts between data. These domain shifts often result in decreased model\naccuracy and reliability, particularly when the model is applied to new data\nwith characteristics different from those it was originally trained on, which\nis a typical manifestation of negative transfer. To address this, we propose\nSelectiveFinetuning in this paper. Our method utilizes a pretrained Multi\nResolution Convolutional Neural Network (MRCNN) to extract EEG features,\ncapturing the distinctive characteristics of different sleep stages. To\nmitigate the effect of domain shifts, we introduce a domain aligning mechanism\nthat employs Earth Mover Distance (EMD) to evaluate and select source domain\ndata closely matching the target domain. By finetuning the model with selective\nsource data, our SelectiveFinetuning enhances the model's performance on target\ndomain that exhibits domain shifts compared to the data used for training.\nExperimental results show that our method outperforms existing baselines,\noffering greater robustness and adaptability in practical scenarios where data\ndistributions are often unpredictable.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03764v1",
    "published_date": "2025-01-07 13:08:54 UTC",
    "updated_date": "2025-01-07 13:08:54 UTC"
  },
  {
    "arxiv_id": "2501.06226v1",
    "title": "asanAI: In-Browser, No-Code, Offline-First Machine Learning Toolkit",
    "authors": [
      "Norman Koch",
      "Siavash Ghiasvand"
    ],
    "abstract": "Machine learning (ML) has become crucial in modern life, with growing\ninterest from researchers and the public. Despite its potential, a significant\nentry barrier prevents widespread adoption, making it challenging for\nnon-experts to understand and implement ML techniques. The increasing desire to\nleverage ML is counterbalanced by its technical complexity, creating a gap\nbetween potential and practical application. This work introduces asanAI, an\noffline-first, open-source, no-code machine learning toolkit designed for users\nof all skill levels. It allows individuals to design, debug, train, and test ML\nmodels directly in a web browser, eliminating the need for software\ninstallations and coding. The toolkit runs on any device with a modern web\nbrowser, including smartphones, and ensures user privacy through local\ncomputations while utilizing WebGL for enhanced GPU performance. Users can\nquickly experiment with neural networks and train custom models using various\ndata sources, supported by intuitive visualizations of network structures and\ndata flows. asanAI simplifies the teaching of ML concepts in educational\nsettings and is released under an open-source MIT license, encouraging\nmodifications. It also supports exporting models in industry-ready formats,\nempowering a diverse range of users to effectively learn and apply machine\nlearning in their projects. The proposed toolkit is successfully utilized by\nresearchers of ScaDS.AI to swiftly draft and test machine learning ideas, by\ntrainers to effectively educate enthusiasts, and by teachers to introduce\ncontemporary ML topics in classrooms with minimal effort and high clarity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.06226v1",
    "published_date": "2025-01-07 12:47:52 UTC",
    "updated_date": "2025-01-07 12:47:52 UTC"
  },
  {
    "arxiv_id": "2501.04067v1",
    "title": "Explainable Time Series Prediction of Tyre Energy in Formula One Race Strategy",
    "authors": [
      "Jamie Todd",
      "Junqi Jiang",
      "Aaron Russo",
      "Steffen Winkler",
      "Stuart Sale",
      "Joseph McMillan",
      "Antonio Rago"
    ],
    "abstract": "Formula One (F1) race strategy takes place in a high-pressure and fast-paced\nenvironment where split-second decisions can drastically affect race results.\nTwo of the core decisions of race strategy are when to make pit stops (i.e.\nreplace the cars' tyres) and which tyre compounds (hard, medium or soft, in\nnormal conditions) to select. The optimal pit stop decisions can be determined\nby estimating the tyre degradation of these compounds, which in turn can be\ncomputed from the energy applied to each tyre, i.e. the tyre energy. In this\nwork, we trained deep learning models, using the Mercedes-AMG PETRONAS F1\nteam's historic race data consisting of telemetry, to forecast tyre energies\nduring races. Additionally, we fitted XGBoost, a decision tree-based machine\nlearning algorithm, to the same dataset and compared the results, with both\ngiving impressive performance. Furthermore, we incorporated two different\nexplainable AI methods, namely feature importance and counterfactual\nexplanations, to gain insights into the reasoning behind the forecasts. Our\ncontributions thus result in an explainable, automated method which could\nassist F1 teams in optimising their race strategy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 9 figures. Copyright ACM 2025. This is the authors' version\n  of the work. It is posted here for your personal use. Not for redistribution.\n  The definitive Version of Record will be published in SAC 2025,\n  http://dx.doi.org/10.1145/3672608.3707765",
    "pdf_url": "http://arxiv.org/pdf/2501.04067v1",
    "published_date": "2025-01-07 12:38:48 UTC",
    "updated_date": "2025-01-07 12:38:48 UTC"
  },
  {
    "arxiv_id": "2501.03722v1",
    "title": "Self-adaptive vision-language model for 3D segmentation of pulmonary artery and vein",
    "authors": [
      "Xiaotong Guo",
      "Deqian Yang",
      "Dan Wang",
      "Haochen Zhao",
      "Yuan Li",
      "Zhilin Sui",
      "Tao Zhou",
      "Lijun Zhang",
      "Yanda Meng"
    ],
    "abstract": "Accurate segmentation of pulmonary structures iscrucial in clinical\ndiagnosis, disease study, and treatment planning. Significant progress has been\nmade in deep learning-based segmentation techniques, but most require much\nlabeled data for training. Consequently, developing precise segmentation\nmethods that demand fewer labeled datasets is paramount in medical image\nanalysis. The emergence of pre-trained vision-language foundation models, such\nas CLIP, recently opened the door for universal computer vision tasks.\nExploiting the generalization ability of these pre-trained foundation models on\ndownstream tasks, such as segmentation, leads to unexpected performance with a\nrelatively small amount of labeled data. However, exploring these models for\npulmonary artery-vein segmentation is still limited. This paper proposes a\nnovel framework called Language-guided self-adaptive Cross-Attention Fusion\nFramework. Our method adopts pre-trained CLIP as a strong feature extractor for\ngenerating the segmentation of 3D CT scans, while adaptively aggregating the\ncross-modality of text and image representations. We propose a s pecially\ndesigned adapter module to fine-tune pre-trained CLIP with a self-adaptive\nlearning strategy to effectively fuse the two modalities of embeddings. We\nextensively validate our method on a local dataset, which is the largest\npulmonary artery-vein CT dataset to date and consists of 718 labeled data in\ntotal. The experiments show that our method outperformed other state-of-the-art\nmethods by a large margin. Our data and code will be made publicly available\nupon acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages,3 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.03722v1",
    "published_date": "2025-01-07 12:03:02 UTC",
    "updated_date": "2025-01-07 12:03:02 UTC"
  },
  {
    "arxiv_id": "2501.03717v1",
    "title": "Materialist: Physically Based Editing Using Single-Image Inverse Rendering",
    "authors": [
      "Lezhong Wang",
      "Duc Minh Tran",
      "Ruiqi Cui",
      "Thomson TG",
      "Manmohan Chandraker",
      "Jeppe Revall Frisvad"
    ],
    "abstract": "To perform image editing based on single-view, inverse physically based\nrendering, we present a method combining a learning-based approach with\nprogressive differentiable rendering. Given an image, our method leverages\nneural networks to predict initial material properties. Progressive\ndifferentiable rendering is then used to optimize the environment map and\nrefine the material properties with the goal of closely matching the rendered\nresult to the input image. We require only a single image while other inverse\nrendering methods based on the rendering equation require multiple views. In\ncomparison to single-view methods that rely on neural renderers, our approach\nachieves more realistic light material interactions, accurate shadows, and\nglobal illumination. Furthermore, with optimized material properties and\nillumination, our method enables a variety of tasks, including physically based\nmaterial editing, object insertion, and relighting. We also propose a method\nfor material transparency editing that operates effectively without requiring\nfull scene geometry. Compared with methods based on Stable Diffusion, our\napproach offers stronger interpretability and more realistic light refraction\nbased on empirical results.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "code will be available at github.com/lez-s/Materialist",
    "pdf_url": "http://arxiv.org/pdf/2501.03717v1",
    "published_date": "2025-01-07 11:52:01 UTC",
    "updated_date": "2025-01-07 11:52:01 UTC"
  },
  {
    "arxiv_id": "2501.03715v1",
    "title": "Neural Deconstruction Search for Vehicle Routing Problems",
    "authors": [
      "André Hottung",
      "Paula Wong-Chung",
      "Kevin Tierney"
    ],
    "abstract": "Autoregressive construction approaches generate solutions to vehicle routing\nproblems in a step-by-step fashion, leading to high-quality solutions that are\nnearing the performance achieved by handcrafted, operations research\ntechniques. In this work, we challenge the conventional paradigm of sequential\nsolution construction and introduce an iterative search framework where\nsolutions are instead deconstructed by a neural policy. Throughout the search,\nthe neural policy collaborates with a simple greedy insertion algorithm to\nrebuild the deconstructed solutions. Our approach surpasses the performance of\nstate-of-the-art operations research methods across three challenging vehicle\nrouting problems of various problem sizes.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03715v1",
    "published_date": "2025-01-07 11:44:25 UTC",
    "updated_date": "2025-01-07 11:44:25 UTC"
  },
  {
    "arxiv_id": "2501.03711v1",
    "title": "Unsupervised Speech Segmentation: A General Approach Using Speech Language Models",
    "authors": [
      "Avishai Elmakies",
      "Omri Abend",
      "Yossi Adi"
    ],
    "abstract": "In this paper, we introduce an unsupervised approach for Speech Segmentation,\nwhich builds on previously researched approaches, e.g., Speaker Diarization,\nwhile being applicable to an inclusive set of acoustic-semantic distinctions,\npaving a path towards a general Unsupervised Speech Segmentation approach.\nUnlike traditional speech and audio segmentation, which mainly focuses on\nspectral changes in the input signal, e.g., phone segmentation, our approach\ntries to segment the spoken utterance into chunks with differing\nacoustic-semantic styles, focusing on acoustic-semantic information that does\nnot translate well into text, e.g., emotion or speaker. While most Speech\nSegmentation tasks only handle one style change, e.g., emotion diarization, our\napproach tries to handle multiple acoustic-semantic style changes. Leveraging\nrecent advances in Speech Language Models (SLMs), we propose a simple\nunsupervised method to segment a given speech utterance. We empirically\ndemonstrate the effectiveness of the proposed approach by considering several\nsetups. Results suggest that the proposed method is superior to the evaluated\nbaselines on boundary detection, segment purity, and over-segmentation. Code is\navailable at\nhttps://github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03711v1",
    "published_date": "2025-01-07 11:32:13 UTC",
    "updated_date": "2025-01-07 11:32:13 UTC"
  },
  {
    "arxiv_id": "2501.03700v1",
    "title": "AuxDepthNet: Real-Time Monocular 3D Object Detection with Depth-Sensitive Features",
    "authors": [
      "Ruochen Zhang",
      "Hyeung-Sik Choi",
      "Dongwook Jung",
      "Phan Huy Nam Anh",
      "Sang-Ki Jeong",
      "Zihao Zhu"
    ],
    "abstract": "Monocular 3D object detection is a challenging task in autonomous systems due\nto the lack of explicit depth information in single-view images. Existing\nmethods often depend on external depth estimators or expensive sensors, which\nincrease computational complexity and hinder real-time performance. To overcome\nthese limitations, we propose AuxDepthNet, an efficient framework for real-time\nmonocular 3D object detection that eliminates the reliance on external depth\nmaps or pre-trained depth models. AuxDepthNet introduces two key components:\nthe Auxiliary Depth Feature (ADF) module, which implicitly learns\ndepth-sensitive features to improve spatial reasoning and computational\nefficiency, and the Depth Position Mapping (DPM) module, which embeds depth\npositional information directly into the detection process to enable accurate\nobject localization and 3D bounding box regression. Leveraging the DepthFusion\nTransformer architecture, AuxDepthNet globally integrates visual and\ndepth-sensitive features through depth-guided interactions, ensuring robust and\nefficient detection. Extensive experiments on the KITTI dataset show that\nAuxDepthNet achieves state-of-the-art performance, with $\\text{AP}_{3D}$ scores\nof 24.72\\% (Easy), 18.63\\% (Moderate), and 15.31\\% (Hard), and\n$\\text{AP}_{\\text{BEV}}$ scores of 34.11\\% (Easy), 25.18\\% (Moderate), and\n21.90\\% (Hard) at an IoU threshold of 0.7.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03700v1",
    "published_date": "2025-01-07 11:07:32 UTC",
    "updated_date": "2025-01-07 11:07:32 UTC"
  },
  {
    "arxiv_id": "2501.03696v1",
    "title": "Exploring Molecule Generation Using Latent Space Graph Diffusion",
    "authors": [
      "Prashanth Pombala",
      "Gerrit Grossmann",
      "Verena Wolf"
    ],
    "abstract": "Generating molecular graphs is a challenging task due to their discrete\nnature and the competitive objectives involved. Diffusion models have emerged\nas SOTA approaches in data generation across various modalities. For molecular\ngraphs, graph neural networks (GNNs) as a diffusion backbone have achieved\nimpressive results. Latent space diffusion, where diffusion occurs in a\nlow-dimensional space via an autoencoder, has demonstrated computational\nefficiency. However, the literature on latent space diffusion for molecular\ngraphs is scarce, and no commonly accepted best practices exist. In this work,\nwe explore different approaches and hyperparameters, contrasting generative\nflow models (denoising diffusion, flow matching, heat dissipation) and\narchitectures (GNNs and E(3)-equivariant GNNs). Our experiments reveal a high\nsensitivity to the choice of approach and design decisions. Code is made\navailable at\ngithub.com/Prashanth-Pombala/Molecule-Generation-using-Latent-Space-Graph-Diffusion.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03696v1",
    "published_date": "2025-01-07 10:54:44 UTC",
    "updated_date": "2025-01-07 10:54:44 UTC"
  },
  {
    "arxiv_id": "2501.04062v1",
    "title": "ChronoLLM: A Framework for Customizing Large Language Model for Digital Twins generalization based on PyChrono",
    "authors": [
      "Jingquan Wang",
      "Harry Zhang",
      "Khailanii Slaton",
      "Shu Wang",
      "Radu Serban",
      "Jinlong Wu",
      "Dan Negrut"
    ],
    "abstract": "Recently, the integration of advanced simulation technologies with artificial\nintelligence (AI) is revolutionizing science and engineering research.\nChronoLlama introduces a novel framework that customizes the open-source LLMs,\nspecifically for code generation, paired with PyChrono for multi-physics\nsimulations. This integration aims to automate and improve the creation of\nsimulation scripts, thus enhancing model accuracy and efficiency. This\ncombination harnesses the speed of AI-driven code generation with the\nreliability of physics-based simulations, providing a powerful tool for\nresearchers and engineers. Empirical results indicate substantial enhancements\nin simulation setup speed, accuracy of the generated codes, and overall\ncomputational efficiency. ChronoLlama not only expedites the development and\ntesting of multibody systems but also spearheads a scalable, AI-enhanced\napproach to managing intricate mechanical simulations. This pioneering\nintegration of cutting-edge AI with traditional simulation platforms represents\na significant leap forward in automating and optimizing design processes in\nengineering applications.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04062v1",
    "published_date": "2025-01-07 10:39:14 UTC",
    "updated_date": "2025-01-07 10:39:14 UTC"
  },
  {
    "arxiv_id": "2501.03689v1",
    "title": "MAJL: A Model-Agnostic Joint Learning Framework for Music Source Separation and Pitch Estimation",
    "authors": [
      "Haojie Wei",
      "Jun Yuan",
      "Rui Zhang",
      "Quanyu Dai",
      "Yueguo Chen"
    ],
    "abstract": "Music source separation and pitch estimation are two vital tasks in music\ninformation retrieval. Typically, the input of pitch estimation is obtained\nfrom the output of music source separation. Therefore, existing methods have\ntried to perform these two tasks simultaneously, so as to leverage the mutually\nbeneficial relationship between both tasks. However, these methods still face\ntwo critical challenges that limit the improvement of both tasks: the lack of\nlabeled data and joint learning optimization. To address these challenges, we\npropose a Model-Agnostic Joint Learning (MAJL) framework for both tasks. MAJL\nis a generic framework and can use variant models for each task. It includes a\ntwo-stage training method and a dynamic weighting method named Dynamic Weights\non Hard Samples (DWHS), which addresses the lack of labeled data and joint\nlearning optimization, respectively. Experimental results on public music\ndatasets show that MAJL outperforms state-of-the-art methods on both tasks,\nwith significant improvements of 0.92 in Signal-to-Distortion Ratio (SDR) for\nmusic source separation and 2.71% in Raw Pitch Accuracy (RPA) for pitch\nestimation. Furthermore, comprehensive studies not only validate the\neffectiveness of each component of MAJL, but also indicate the great generality\nof MAJL in adapting to different model architectures.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03689v1",
    "published_date": "2025-01-07 10:38:51 UTC",
    "updated_date": "2025-01-07 10:38:51 UTC"
  },
  {
    "arxiv_id": "2501.03681v1",
    "title": "SLAM: Towards Efficient Multilingual Reasoning via Selective Language Alignment",
    "authors": [
      "Yuchun Fan",
      "Yongyu Mu",
      "Yilin Wang",
      "Lei Huang",
      "Junhao Ruan",
      "Bei Li",
      "Tong Xiao",
      "Shujian Huang",
      "Xiaocheng Feng",
      "Jingbo Zhu"
    ],
    "abstract": "Despite the significant improvements achieved by large language models (LLMs)\nin English reasoning tasks, these models continue to struggle with multilingual\nreasoning. Recent studies leverage a full-parameter and two-stage training\nparadigm to teach models to first understand non-English questions and then\nreason. However, this method suffers from both substantial computational\nresource computing and catastrophic forgetting. The fundamental cause is that,\nwith the primary goal of enhancing multilingual comprehension, an excessive\nnumber of irrelevant layers and parameters are tuned during the first stage.\nGiven our findings that the representation learning of languages is merely\nconducted in lower-level layers, we propose an efficient multilingual reasoning\nalignment approach that precisely identifies and fine-tunes the layers\nresponsible for handling multilingualism. Experimental results show that our\nmethod, SLAM, only tunes 6 layers' feed-forward sub-layers including 6.5-8% of\nall parameters within 7B and 13B LLMs, achieving superior average performance\nthan all strong baselines across 10 languages. Meanwhile, SLAM only involves\none training stage, reducing training time by 4.1-11.9 compared to the\ntwo-stage method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by COLING 2025 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2501.03681v1",
    "published_date": "2025-01-07 10:29:43 UTC",
    "updated_date": "2025-01-07 10:29:43 UTC"
  },
  {
    "arxiv_id": "2501.03676v2",
    "title": "SALE-Based Offline Reinforcement Learning with Ensemble Q-Networks",
    "authors": [
      "Zheng Chun"
    ],
    "abstract": "In this work, we build upon the offline reinforcement learning algorithm TD7,\nwhich incorporates State-Action Learned Embeddings (SALE) and a prioritized\nexperience replay buffer (LAP). We propose a model-free actor-critic algorithm\nthat integrates ensemble Q-networks and a gradient diversity penalty from EDAC.\nThe ensemble Q-networks introduce penalties to guide the actor network toward\nin-distribution actions, effectively addressing the challenge of\nout-of-distribution actions. Meanwhile, the gradient diversity penalty\nencourages diverse Q-value gradients, further suppressing overestimation for\nout-of-distribution actions. Additionally, our method retains an adjustable\nbehavior cloning (BC) term that directs the actor network toward dataset\nactions during early training stages, while gradually reducing its influence as\nthe precision of the Q-ensemble improves. These enhancements work\nsynergistically to improve the stability and precision of the training.\nExperimental results on the D4RL MuJoCo benchmarks demonstrate that our\nalgorithm achieves higher convergence speed, stability, and performance\ncompared to existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T05, 90C40",
      "I.2.6; I.2.8"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 7 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.03676v2",
    "published_date": "2025-01-07 10:22:30 UTC",
    "updated_date": "2025-01-12 09:40:44 UTC"
  },
  {
    "arxiv_id": "2501.03674v1",
    "title": "Action Quality Assessment via Hierarchical Pose-guided Multi-stage Contrastive Regression",
    "authors": [
      "Mengshi Qi",
      "Hao Ye",
      "Jiaxuan Peng",
      "Huadong Ma"
    ],
    "abstract": "Action Quality Assessment (AQA), which aims at automatic and fair evaluation\nof athletic performance, has gained increasing attention in recent years.\nHowever, athletes are often in rapid movement and the corresponding visual\nappearance variances are subtle, making it challenging to capture fine-grained\npose differences and leading to poor estimation performance. Furthermore, most\ncommon AQA tasks, such as diving in sports, are usually divided into multiple\nsub-actions, each of which contains different durations. However, existing\nmethods focus on segmenting the video into fixed frames, which disrupts the\ntemporal continuity of sub-actions resulting in unavoidable prediction errors.\nTo address these challenges, we propose a novel action quality assessment\nmethod through hierarchically pose-guided multi-stage contrastive regression.\nFirstly, we introduce a multi-scale dynamic visual-skeleton encoder to capture\nfine-grained spatio-temporal visual and skeletal features. Then, a procedure\nsegmentation network is introduced to separate different sub-actions and obtain\nsegmented features. Afterwards, the segmented visual and skeletal features are\nboth fed into a multi-modal fusion module as physics structural priors, to\nguide the model in learning refined activity similarities and variances.\nFinally, a multi-stage contrastive learning regression approach is employed to\nlearn discriminative representations and output prediction results. In\naddition, we introduce a newly-annotated FineDiving-Pose Dataset to improve the\ncurrent low-quality human pose labels. In experiments, the results on\nFineDiving and MTL-AQA datasets demonstrate the effectiveness and superiority\nof our proposed approach. Our source code and dataset are available at\nhttps://github.com/Lumos0507/HP-MCoRe.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03674v1",
    "published_date": "2025-01-07 10:20:16 UTC",
    "updated_date": "2025-01-07 10:20:16 UTC"
  },
  {
    "arxiv_id": "2501.05476v1",
    "title": "IntegrityAI at GenAI Detection Task 2: Detecting Machine-Generated Academic Essays in English and Arabic Using ELECTRA and Stylometry",
    "authors": [
      "Mohammad AL-Smadi"
    ],
    "abstract": "Recent research has investigated the problem of detecting machine-generated\nessays for academic purposes. To address this challenge, this research utilizes\npre-trained, transformer-based models fine-tuned on Arabic and English academic\nessays with stylometric features. Custom models based on ELECTRA for English\nand AraELECTRA for Arabic were trained and evaluated using a benchmark dataset.\nProposed models achieved excellent results with an F1-score of 99.7%, ranking\n2nd among of 26 teams in the English subtask, and 98.4%, finishing 1st out of\n23 teams in the Arabic one.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05476v1",
    "published_date": "2025-01-07 10:19:56 UTC",
    "updated_date": "2025-01-07 10:19:56 UTC"
  },
  {
    "arxiv_id": "2501.03670v1",
    "title": "A Diversity-Enhanced Knowledge Distillation Model for Practical Math Word Problem Solving",
    "authors": [
      "Yi Zhang",
      "Guangyou Zhou",
      "Zhiwen Xie",
      "Jinjin Ma",
      "Jimmy Xiangji Huang"
    ],
    "abstract": "Math Word Problem (MWP) solving is a critical task in natural language\nprocessing, has garnered significant research interest in recent years. Various\nrecent studies heavily rely on Seq2Seq models and their extensions (e.g.,\nSeq2Tree and Graph2Tree) to generate mathematical equations. While effective,\nthese models struggle to generate diverse but counterpart solution equations,\nlimiting their generalization across various math problem scenarios. In this\npaper, we introduce a novel Diversity-enhanced Knowledge Distillation (DivKD)\nmodel for practical MWP solving. Our approach proposes an adaptive diversity\ndistillation method, in which a student model learns diverse equations by\nselectively transferring high-quality knowledge from a teacher model.\nAdditionally, we design a diversity prior-enhanced student model to better\ncapture the diversity distribution of equations by incorporating a conditional\nvariational auto-encoder. Extensive experiments on {four} MWP benchmark\ndatasets demonstrate that our approach achieves higher answer accuracy than\nstrong baselines while maintaining high efficiency for practical applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03670v1",
    "published_date": "2025-01-07 10:18:22 UTC",
    "updated_date": "2025-01-07 10:18:22 UTC"
  },
  {
    "arxiv_id": "2501.03643v2",
    "title": "Effective and Efficient Mixed Precision Quantization of Speech Foundation Models",
    "authors": [
      "Haoning Xu",
      "Zhaoqing Li",
      "Zengrui Jin",
      "Huimeng Wang",
      "Youjun Chen",
      "Guinan Li",
      "Mengzhe Geng",
      "Shujie Hu",
      "Jiajun Deng",
      "Xunying Liu"
    ],
    "abstract": "This paper presents a novel mixed-precision quantization approach for speech\nfoundation models that tightly integrates mixed-precision learning and\nquantized model parameter estimation into one single model compression stage.\nExperiments conducted on LibriSpeech dataset with fine-tuned wav2vec2.0-base\nand HuBERT-large models suggest the resulting mixed-precision quantized models\nincreased the lossless compression ratio by factors up to 1.7x and 1.9x over\nthe respective uniform-precision and two-stage mixed-precision quantized\nbaselines that perform precision learning and model parameters quantization in\nseparate and disjointed stages, while incurring no statistically word error\nrate (WER) increase over the 32-bit full-precision models. The system\ncompression time of wav2vec2.0-base and HuBERT-large models is reduced by up to\n1.9 and 1.5 times over the two-stage mixed-precision baselines, while both\nproduce lower WERs. The best-performing 3.5-bit mixed-precision quantized\nHuBERT-large model produces a lossless compression ratio of 8.6x over the\n32-bit full-precision system.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "To appear at IEEE ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03643v2",
    "published_date": "2025-01-07 09:21:52 UTC",
    "updated_date": "2025-01-11 06:24:11 UTC"
  },
  {
    "arxiv_id": "2501.06224v3",
    "title": "Detection, Retrieval, and Explanation Unified: A Violence Detection System Based on Knowledge Graphs and GAT",
    "authors": [
      "Wen-Dong Jiang",
      "Chih-Yung Chang",
      "Diptendu Sinha Roy"
    ],
    "abstract": "Recently, violence detection systems developed using unified multimodal\nmodels have achieved significant success and attracted widespread attention.\nHowever, most of these systems face two critical challenges: the lack of\ninterpretability as black-box models and limited functionality, offering only\nclassification or retrieval capabilities. To address these challenges, this\npaper proposes a novel interpretable violence detection system, termed the\nThree-in-One (TIO) System. The TIO system integrates knowledge graphs (KG) and\ngraph attention networks (GAT) to provide three core functionalities:\ndetection, retrieval, and explanation. Specifically, the system processes each\nvideo frame along with text descriptions generated by a large language model\n(LLM) for videos containing potential violent behavior. It employs ImageBind to\ngenerate high-dimensional embeddings for constructing a knowledge graph, uses\nGAT for reasoning, and applies lightweight time series modules to extract video\nembedding features. The final step connects a classifier and retriever for\nmulti-functional outputs. The interpretability of KG enables the system to\nverify the reasoning process behind each output. Additionally, the paper\nintroduces several lightweight methods to reduce the resource consumption of\nthe TIO system and enhance its efficiency. Extensive experiments conducted on\nthe XD-Violence and UCF-Crime datasets validate the effectiveness of the\nproposed system. A case study further reveals an intriguing phenomenon: as the\nnumber of bystanders increases, the occurrence of violent behavior tends to\ndecrease.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2501.06224v3",
    "published_date": "2025-01-07 09:21:20 UTC",
    "updated_date": "2025-02-06 04:58:18 UTC"
  },
  {
    "arxiv_id": "2501.03635v1",
    "title": "MHGNet: Multi-Heterogeneous Graph Neural Network for Traffic Prediction",
    "authors": [
      "Mei Wu",
      "Yiqian Lin",
      "Tianfan Jiang",
      "Wenchao Weng"
    ],
    "abstract": "In recent years, traffic flow prediction has played a crucial role in the\nmanagement of intelligent transportation systems. However, traditional\nforecasting methods often model non-Euclidean low-dimensional traffic data as a\nsimple graph with single-type nodes and edges, failing to capture similar\ntrends among nodes of the same type. To address this limitation, this paper\nproposes MHGNet, a novel framework for modeling spatiotemporal\nmulti-heterogeneous graphs. Within this framework, the STD Module decouples\nsingle-pattern traffic data into multi-pattern traffic data through feature\nmappings of timestamp embedding matrices and node embedding matrices.\nSubsequently, the Node Clusterer leverages the Euclidean distance between nodes\nand different types of limit points to perform clustering with O(N) time\ncomplexity. The nodes within each cluster undergo residual subgraph convolution\nwithin the spatiotemporal fusion subgraphs generated by the DSTGG Module,\nfollowed by processing in the SIE Module for node repositioning and\nredistribution of weights. To validate the effectiveness of MHGNet, this paper\nconducts extensive ablation studies and quantitative evaluations on four widely\nused benchmarks, demonstrating its superior performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by 2025 lEEE International Conference on Acoustics, speech,\n  and signal Processing (lCASSP2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.03635v1",
    "published_date": "2025-01-07 09:10:09 UTC",
    "updated_date": "2025-01-07 09:10:09 UTC"
  },
  {
    "arxiv_id": "2501.05475v1",
    "title": "Retrieval-Augmented Generation by Evidence Retroactivity in LLMs",
    "authors": [
      "Liang Xiao",
      "Wen Dai",
      "Shuai Chen",
      "Bin Qin",
      "Chongyang Shi",
      "Haopeng Jing",
      "Tianyu Guo"
    ],
    "abstract": "Retrieval-augmented generation has gained significant attention due to its\nability to integrate relevant external knowledge, enhancing the accuracy and\nreliability of the LLMs' responses. Most of the existing methods apply a\ndynamic multiple retrieval-generating process, to address multi-hop complex\nquestions by decomposing them into sub-problems. However, these methods rely on\nan unidirectional forward reasoning paradigm, where errors from insufficient\nreasoning steps or inherent flaws in current retrieval systems are\nirreversible, potentially derailing the entire reasoning chain. For the first\ntime, this work introduces Retroactive Retrieval-Augmented Generation\n(RetroRAG), a novel framework to build a retroactive reasoning paradigm.\nRetroRAG revises and updates the evidence, redirecting the reasoning chain to\nthe correct direction. RetroRAG constructs an evidence-collation-discovery\nframework to search, generate, and refine credible evidence. It synthesizes\ninferential evidence related to the key entities in the question from the\nexisting source knowledge and formulates search queries to uncover additional\ninformation. As new evidence is found, RetroRAG continually updates and\norganizes this information, enhancing its ability to locate further necessary\nevidence. Paired with an Answerer to generate and evaluate outputs, RetroRAG is\ncapable of refining its reasoning process iteratively until a reliable answer\nis obtained. Empirical evaluations show that RetroRAG significantly outperforms\nexisting methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.05475v1",
    "published_date": "2025-01-07 08:57:42 UTC",
    "updated_date": "2025-01-07 08:57:42 UTC"
  },
  {
    "arxiv_id": "2501.05474v1",
    "title": "Modality-Invariant Bidirectional Temporal Representation Distillation Network for Missing Multimodal Sentiment Analysis",
    "authors": [
      "Xincheng Wang",
      "Liejun Wang",
      "Yinfeng Yu",
      "Xinxin Jiao"
    ],
    "abstract": "Multimodal Sentiment Analysis (MSA) integrates diverse modalities(text,\naudio, and video) to comprehensively analyze and understand individuals'\nemotional states. However, the real-world prevalence of incomplete data poses\nsignificant challenges to MSA, mainly due to the randomness of modality\nmissing. Moreover, the heterogeneity issue in multimodal data has yet to be\neffectively addressed. To tackle these challenges, we introduce the\nModality-Invariant Bidirectional Temporal Representation Distillation Network\n(MITR-DNet) for Missing Multimodal Sentiment Analysis. MITR-DNet employs a\ndistillation approach, wherein a complete modality teacher model guides a\nmissing modality student model, ensuring robustness in the presence of modality\nmissing. Simultaneously, we developed the Modality-Invariant Bidirectional\nTemporal Representation Learning Module (MIB-TRL) to mitigate heterogeneity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication by 2025 IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.05474v1",
    "published_date": "2025-01-07 07:57:16 UTC",
    "updated_date": "2025-01-07 07:57:16 UTC"
  },
  {
    "arxiv_id": "2501.03598v1",
    "title": "RecKG: Knowledge Graph for Recommender Systems",
    "authors": [
      "Junhyuk Kwon",
      "Seokho Ahn",
      "Young-Duk Seo"
    ],
    "abstract": "Knowledge graphs have proven successful in integrating heterogeneous data\nacross various domains. However, there remains a noticeable dearth of research\non their seamless integration among heterogeneous recommender systems, despite\nknowledge graph-based recommender systems garnering extensive research\nattention. This study aims to fill this gap by proposing RecKG, a standardized\nknowledge graph for recommender systems. RecKG ensures the consistent\nrepresentation of entities across different datasets, accommodating diverse\nattribute types for effective data integration. Through a meticulous\nexamination of various recommender system datasets, we select attributes for\nRecKG, ensuring standardized formatting through consistent naming conventions.\nBy these characteristics, RecKG can seamlessly integrate heterogeneous data\nsources, enabling the discovery of additional semantic information within the\nintegrated knowledge graph. We apply RecKG to standardize real-world datasets,\nsubsequently developing an application for RecKG using a graph database.\nFinally, we validate RecKG's achievement in interoperability through a\nqualitative evaluation between RecKG and other studies.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by The 39th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2501.03598v1",
    "published_date": "2025-01-07 07:55:35 UTC",
    "updated_date": "2025-01-07 07:55:35 UTC"
  },
  {
    "arxiv_id": "2501.03583v1",
    "title": "STContext: A Multifaceted Dataset for Developing Context-aware Spatio-temporal Crowd Mobility Prediction Models",
    "authors": [
      "Liyue Chen",
      "Jiangyi Fang",
      "Tengfei Liu",
      "Fangyuan Gao",
      "Leye Wang"
    ],
    "abstract": "In smart cities, context-aware spatio-temporal crowd flow prediction (STCFP)\nmodels leverage contextual features (e.g., weather) to identify unusual crowd\nmobility patterns and enhance prediction accuracy. However, the best practice\nfor incorporating contextual features remains unclear due to inconsistent usage\nof contextual features in different papers. Developing a multifaceted dataset\nwith rich types of contextual features and STCFP scenarios is crucial for\nestablishing a principled context modeling paradigm. Existing open crowd flow\ndatasets lack an adequate range of contextual features, which poses an urgent\nrequirement to build a multifaceted dataset to fill these research gaps. To\nthis end, we create STContext, a multifaceted dataset for developing\ncontext-aware STCFP models. Specifically, STContext provides nine\nspatio-temporal datasets across five STCFP scenarios and includes ten\ncontextual features, including weather, air quality index, holidays, points of\ninterest, road networks, etc. Besides, we propose a unified workflow for\nincorporating contextual features into deep STCFP methods, with steps including\nfeature transformation, dependency modeling, representation fusion, and\ntraining strategies. Through extensive experiments, we have obtained several\nuseful guidelines for effective context modeling and insights for future\nresearch. The STContext is open-sourced at\nhttps://github.com/Liyue-Chen/STContext.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03583v1",
    "published_date": "2025-01-07 07:16:56 UTC",
    "updated_date": "2025-01-07 07:16:56 UTC"
  },
  {
    "arxiv_id": "2501.03575v2",
    "title": "Cosmos World Foundation Model Platform for Physical AI",
    "authors": [
      "NVIDIA",
      ":",
      "Niket Agarwal",
      "Arslan Ali",
      "Maciej Bala",
      "Yogesh Balaji",
      "Erik Barker",
      "Tiffany Cai",
      "Prithvijit Chattopadhyay",
      "Yongxin Chen",
      "Yin Cui",
      "Yifan Ding",
      "Daniel Dworakowski",
      "Jiaojiao Fan",
      "Michele Fenzi",
      "Francesco Ferroni",
      "Sanja Fidler",
      "Dieter Fox",
      "Songwei Ge",
      "Yunhao Ge",
      "Jinwei Gu",
      "Siddharth Gururani",
      "Ethan He",
      "Jiahui Huang",
      "Jacob Huffman",
      "Pooya Jannaty",
      "Jingyi Jin",
      "Seung Wook Kim",
      "Gergely Klár",
      "Grace Lam",
      "Shiyi Lan",
      "Laura Leal-Taixe",
      "Anqi Li",
      "Zhaoshuo Li",
      "Chen-Hsuan Lin",
      "Tsung-Yi Lin",
      "Huan Ling",
      "Ming-Yu Liu",
      "Xian Liu",
      "Alice Luo",
      "Qianli Ma",
      "Hanzi Mao",
      "Kaichun Mo",
      "Arsalan Mousavian",
      "Seungjun Nah",
      "Sriharsha Niverty",
      "David Page",
      "Despoina Paschalidou",
      "Zeeshan Patel",
      "Lindsey Pavao",
      "Morteza Ramezanali",
      "Fitsum Reda",
      "Xiaowei Ren",
      "Vasanth Rao Naik Sabavat",
      "Ed Schmerling",
      "Stella Shi",
      "Bartosz Stefaniak",
      "Shitao Tang",
      "Lyne Tchapmi",
      "Przemek Tredak",
      "Wei-Cheng Tseng",
      "Jibin Varghese",
      "Hao Wang",
      "Haoxiang Wang",
      "Heng Wang",
      "Ting-Chun Wang",
      "Fangyin Wei",
      "Xinyue Wei",
      "Jay Zhangjie Wu",
      "Jiashu Xu",
      "Wei Yang",
      "Lin Yen-Chen",
      "Xiaohui Zeng",
      "Yu Zeng",
      "Jing Zhang",
      "Qinsheng Zhang",
      "Yuxuan Zhang",
      "Qingqing Zhao",
      "Artur Zolkowski"
    ],
    "abstract": "Physical AI needs to be trained digitally first. It needs a digital twin of\nitself, the policy model, and a digital twin of the world, the world model. In\nthis paper, we present the Cosmos World Foundation Model Platform to help\ndevelopers build customized world models for their Physical AI setups. We\nposition a world foundation model as a general-purpose world model that can be\nfine-tuned into customized world models for downstream applications. Our\nplatform covers a video curation pipeline, pre-trained world foundation models,\nexamples of post-training of pre-trained world foundation models, and video\ntokenizers. To help Physical AI builders solve the most critical problems of\nour society, we make Cosmos open-source and our models open-weight with\npermissive licenses available via\nhttps://github.com/nvidia-cosmos/cosmos-predict1.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03575v2",
    "published_date": "2025-01-07 06:55:50 UTC",
    "updated_date": "2025-03-18 16:59:07 UTC"
  },
  {
    "arxiv_id": "2501.03572v1",
    "title": "From Code to Compliance: Assessing ChatGPT's Utility in Designing an Accessible Webpage -- A Case Study",
    "authors": [
      "Ammar Ahmed",
      "Margarida Fresco",
      "Fredrik Forsberg",
      "Hallvard Grotli"
    ],
    "abstract": "Web accessibility ensures that individuals with disabilities can access and\ninteract with digital content without barriers, yet a significant majority of\nmost used websites fail to meet accessibility standards. This study evaluates\nChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web\nContent Accessibility Guidelines (WCAG). While ChatGPT can effectively address\naccessibility issues when prompted, its default code often lacks compliance,\nreflecting limitations in its training data and prevailing inaccessible web\npractices. Automated and manual testing revealed strengths in resolving simple\nissues but challenges with complex tasks, requiring human oversight and\nadditional iterations. Unlike prior studies, we incorporate manual evaluation,\ndynamic elements, and use the visual reasoning capability of ChatGPT along with\nthe prompts to fix accessibility issues. Providing screenshots alongside\nprompts enhances the LLM's ability to address accessibility issues by allowing\nit to analyze surrounding components, such as determining appropriate contrast\ncolors. We found that effective prompt engineering, such as providing concise,\nstructured feedback and incorporating visual aids, significantly enhances\nChatGPT's performance. These findings highlight the potential and limitations\nof large language models for accessible web development, offering practical\nguidance for developers to create more inclusive websites.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "D.1.2; F.3.1; F.4.1; D.3.2; H.1.2; H.5.2; D.2.2; H.1.2; I.3.6;\n  H.5.4; H.5.1"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03572v1",
    "published_date": "2025-01-07 06:51:46 UTC",
    "updated_date": "2025-01-07 06:51:46 UTC"
  },
  {
    "arxiv_id": "2501.03566v1",
    "title": "Applying Large Language Models in Knowledge Graph-based Enterprise Modeling: Challenges and Opportunities",
    "authors": [
      "Benedikt Reitemeyer",
      "Hans-Georg Fill"
    ],
    "abstract": "The role of large language models (LLMs) in enterprise modeling has recently\nstarted to shift from academic research to that of industrial applications.\nThereby, LLMs represent a further building block for the machine-supported\ngeneration of enterprise models. In this paper we employ a knowledge\ngraph-based approach for enterprise modeling and investigate the potential\nbenefits of LLMs in this context. In addition, the findings of an expert survey\nand ChatGPT-4o-based experiments demonstrate that LLM-based model generations\nexhibit minimal variability, yet remain constrained to specific tasks, with\nreliability declining for more intricate tasks. The survey results further\nsuggest that the supervision and intervention of human modeling experts are\nessential to ensure the accuracy and integrity of the generated models.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03566v1",
    "published_date": "2025-01-07 06:34:17 UTC",
    "updated_date": "2025-01-07 06:34:17 UTC"
  },
  {
    "arxiv_id": "2501.03562v2",
    "title": "Rethinking Adversarial Attacks in Reinforcement Learning from Policy Distribution Perspective",
    "authors": [
      "Tianyang Duan",
      "Zongyuan Zhang",
      "Zheng Lin",
      "Yue Gao",
      "Ling Xiong",
      "Yong Cui",
      "Hongbin Liang",
      "Xianhao Chen",
      "Heming Cui",
      "Dong Huang"
    ],
    "abstract": "Deep Reinforcement Learning (DRL) suffers from uncertainties and inaccuracies\nin the observation signal in realworld applications. Adversarial attack is an\neffective method for evaluating the robustness of DRL agents. However, existing\nattack methods targeting individual sampled actions have limited impacts on the\noverall policy distribution, particularly in continuous action spaces. To\naddress these limitations, we propose the Distribution-Aware Projected Gradient\nDescent attack (DAPGD). DAPGD uses distribution similarity as the gradient\nperturbation input to attack the policy network, which leverages the entire\npolicy distribution rather than relying on individual samples. We utilize the\nBhattacharyya distance in DAPGD to measure policy similarity, enabling\nsensitive detection of subtle but critical differences between probability\ndistributions. Our experiment results demonstrate that DAPGD achieves SOTA\nresults compared to the baselines in three robot navigation tasks, achieving an\naverage 22.03% higher reward drop compared to the best baseline.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 2 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.03562v2",
    "published_date": "2025-01-07 06:22:55 UTC",
    "updated_date": "2025-01-08 08:57:32 UTC"
  },
  {
    "arxiv_id": "2501.03560v1",
    "title": "KG-TRICK: Unifying Textual and Relational Information Completion of Knowledge for Multilingual Knowledge Graphs",
    "authors": [
      "Zelin Zhou",
      "Simone Conia",
      "Daniel Lee",
      "Min Li",
      "Shenglei Huang",
      "Umar Farooq Minhas",
      "Saloni Potdar",
      "Henry Xiao",
      "Yunyao Li"
    ],
    "abstract": "Multilingual knowledge graphs (KGs) provide high-quality relational and\ntextual information for various NLP applications, but they are often\nincomplete, especially in non-English languages. Previous research has shown\nthat combining information from KGs in different languages aids either\nKnowledge Graph Completion (KGC), the task of predicting missing relations\nbetween entities, or Knowledge Graph Enhancement (KGE), the task of predicting\nmissing textual information for entities. Although previous efforts have\nconsidered KGC and KGE as independent tasks, we hypothesize that they are\ninterdependent and mutually beneficial. To this end, we introduce KG-TRICK, a\nnovel sequence-to-sequence framework that unifies the tasks of textual and\nrelational information completion for multilingual KGs. KG-TRICK demonstrates\nthat: i) it is possible to unify the tasks of KGC and KGE into a single\nframework, and ii) combining textual information from multiple languages is\nbeneficial to improve the completeness of a KG. As part of our contributions,\nwe also introduce WikiKGE10++, the largest manually-curated benchmark for\ntextual information completion of KGs, which features over 25,000 entities\nacross 10 diverse languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera ready for COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03560v1",
    "published_date": "2025-01-07 06:21:40 UTC",
    "updated_date": "2025-01-07 06:21:40 UTC"
  },
  {
    "arxiv_id": "2501.03544v2",
    "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models",
    "authors": [
      "Lingzhi Yuan",
      "Xiaojun Jia",
      "Yihao Huang",
      "Wei Dong",
      "Yang Liu"
    ],
    "abstract": "Text-to-image (T2I) models have been shown to be vulnerable to misuse,\nparticularly in generating not-safe-for-work (NSFW) content, raising serious\nethical concerns. In this work, we present PromptGuard, a novel content\nmoderation technique that draws inspiration from the system prompt mechanism in\nlarge language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack\na direct interface for enforcing behavioral guidelines. Our key idea is to\noptimize a safety soft prompt that functions as an implicit system prompt\nwithin the T2I model's textual embedding space. This universal soft prompt (P*)\ndirectly moderates NSFW inputs, enabling safe yet realistic image generation\nwithout altering the inference efficiency or requiring proxy models. Extensive\nexperiments across three datasets demonstrate that PromptGuard effectively\nmitigates NSFW content generation while preserving high-quality benign outputs.\nPromptGuard achieves 7.8 times faster than prior content moderation methods,\nsurpassing eight state-of-the-art defenses with an optimal unsafe ratio down to\n5.84%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 8 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.03544v2",
    "published_date": "2025-01-07 05:39:21 UTC",
    "updated_date": "2025-04-04 05:56:04 UTC"
  },
  {
    "arxiv_id": "2501.03540v1",
    "title": "Deep Learning within Tabular Data: Foundations, Challenges, Advances and Future Directions",
    "authors": [
      "Weijieying Ren",
      "Tianxiang Zhao",
      "Yuqing Huang",
      "Vasant Honavar"
    ],
    "abstract": "Tabular data remains one of the most prevalent data types across a wide range\nof real-world applications, yet effective representation learning for this\ndomain poses unique challenges due to its irregular patterns, heterogeneous\nfeature distributions, and complex inter-column dependencies. This survey\nprovides a comprehensive review of state-of-the-art techniques in tabular data\nrepresentation learning, structured around three foundational design elements:\ntraining data, neural architectures, and learning objectives. Unlike prior\nsurveys that focus primarily on either architecture design or learning\nstrategies, we adopt a holistic perspective that emphasizes the universality\nand robustness of representation learning methods across diverse downstream\ntasks. We examine recent advances in data augmentation and generation,\nspecialized neural network architectures tailored to tabular data, and\ninnovative learning objectives that enhance representation quality.\nAdditionally, we highlight the growing influence of self-supervised learning\nand the adaptation of transformer-based foundation models for tabular data. Our\nreview is based on a systematic literature search using rigorous inclusion\ncriteria, encompassing 127 papers published since 2020 in top-tier conferences\nand journals. Through detailed analysis and comparison, we identify emerging\ntrends, critical gaps, and promising directions for future research, aiming to\nguide the development of more generalizable and effective tabular data\nrepresentation methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03540v1",
    "published_date": "2025-01-07 05:23:36 UTC",
    "updated_date": "2025-01-07 05:23:36 UTC"
  },
  {
    "arxiv_id": "2501.03535v2",
    "title": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive Querying for LLM-Based Autonomous Driving",
    "authors": [
      "Xuewen Luo",
      "Fan Ding",
      "Fengze Yang",
      "Yang Zhou",
      "Junnyong Loo",
      "Hwa Hui Tew",
      "Chenxi Liu"
    ],
    "abstract": "This study addresses the critical need for enhanced situational awareness in\nautonomous driving (AD) by leveraging the contextual reasoning capabilities of\nlarge language models (LLMs). Unlike traditional perception systems that rely\non rigid, label-based annotations, it integrates real-time, multimodal sensor\ndata into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically\nunderstand and respond to complex driving environments. To overcome the\ninherent latency and modality limitations of LLMs, a proactive\nRetrieval-Augmented Generation (RAG) is designed for AD, combined with a\nchain-of-thought prompting mechanism, ensuring rapid and context-rich\nunderstanding. Experimental results using real-world Vehicle-to-everything\n(V2X) datasets demonstrate significant improvements in perception and\nprediction performance, highlighting the potential of this framework to enhance\nsafety, adaptability, and decision-making in next-generation AD systems.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted for presentation at WACV Workshop LLMAD\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03535v2",
    "published_date": "2025-01-07 05:15:46 UTC",
    "updated_date": "2025-01-08 10:34:54 UTC"
  },
  {
    "arxiv_id": "2502.00011v1",
    "title": "TOAST Framework: A Multidimensional Approach to Ethical and Sustainable AI Integration in Organizations",
    "authors": [
      "Dian Tjondronegoro"
    ],
    "abstract": "Artificial Intelligence (AI) has emerged as a transformative technology with\nthe potential to revolutionize various sectors, from healthcare to finance,\neducation, and beyond. However, successfully implementing AI systems remains a\ncomplex challenge, requiring a comprehensive and methodologically sound\nframework. This paper contributes to this challenge by introducing the\nTrustworthy, Optimized, Adaptable, and Socio-Technologically harmonious (TOAST)\nframework. It draws on insights from various disciplines to align technical\nstrategy with ethical values, societal responsibilities, and innovation\naspirations. The TOAST framework is a novel approach designed to guide the\nimplementation of AI systems, focusing on reliability, accountability,\ntechnical advancement, adaptability, and socio-technical harmony. By grounding\nthe TOAST framework in healthcare case studies, this paper provides a robust\nevaluation of its practicality and theoretical soundness in addressing\noperational, ethical, and regulatory challenges in high-stakes environments,\ndemonstrating how adaptable AI systems can enhance institutional efficiency,\nmitigate risks like bias and data privacy, and offer a replicable model for\nother sectors requiring ethically aligned and efficient AI integration.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "90-02",
      "K.6.m"
    ],
    "primary_category": "cs.CY",
    "comment": "25 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2502.00011v1",
    "published_date": "2025-01-07 05:13:39 UTC",
    "updated_date": "2025-01-07 05:13:39 UTC"
  },
  {
    "arxiv_id": "2501.03523v1",
    "title": "Vocal Tract Length Warped Features for Spoken Keyword Spotting",
    "authors": [
      "Achintya kr. Sarkar",
      "Priyanka Dwivedi",
      "Zheng-Hua Tan"
    ],
    "abstract": "In this paper, we propose several methods that incorporate vocal tract length\n(VTL) warped features for spoken keyword spotting (KWS). The first method,\nVTL-independent KWS, involves training a single deep neural network (DNN) that\nutilizes VTL features with various warping factors. During training, a specific\nVTL feature is randomly selected per epoch, allowing the exploration of VTL\nvariations. During testing, the VTL features with different warping factors of\na test utterance are scored against the DNN and combined with equal weight. In\nthe second method scores the conventional features of a test utterance (without\nVTL warping) against the DNN. The third method, VTL-concatenation KWS,\nconcatenates VTL warped features to form high-dimensional features for KWS.\nEvaluations carried out on the English Google Command dataset demonstrate that\nthe proposed methods improve the accuracy of KWS.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03523v1",
    "published_date": "2025-01-07 04:38:28 UTC",
    "updated_date": "2025-01-07 04:38:28 UTC"
  },
  {
    "arxiv_id": "2501.03499v1",
    "title": "Can Deep Learning Trigger Alerts from Mobile-Captured Images?",
    "authors": [
      "Pritisha Sarkar",
      "Duranta Durbaar Vishal Saha",
      "Mousumi Saha"
    ],
    "abstract": "Our research presents a comprehensive approach to leveraging mobile camera\nimage data for real-time air quality assessment and recommendation. We develop\na regression-based Convolutional Neural Network model and tailor it explicitly\nfor air quality prediction by exploiting the inherent relationship between\noutput parameters. As a result, the Mean Squared Error of 0.0077 and 0.0112\nobtained for 2 and 5 pollutants respectively outperforms existing models.\nFurthermore, we aim to verify the common practice of augmenting the original\ndataset with a view to introducing more variation in the training phase. It is\none of our most significant contributions that our experimental results\ndemonstrate minimal accuracy differences between the original and augmented\ndatasets. Finally, a real-time, user-friendly dashboard is implemented which\ndynamically displays the Air Quality Index and pollutant values derived from\ncaptured mobile camera images. Users' health conditions are considered to\nrecommend whether a location is suitable based on current air quality metrics.\nOverall, this research contributes to verification of data augmentation\ntechniques, CNN-based regression modelling for air quality prediction, and\nuser-centric air quality monitoring through mobile technology. The proposed\nsystem offers practical solutions for individuals to make informed\nenvironmental health and well-being decisions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03499v1",
    "published_date": "2025-01-07 03:39:43 UTC",
    "updated_date": "2025-01-07 03:39:43 UTC"
  },
  {
    "arxiv_id": "2501.03491v1",
    "title": "Can LLMs Design Good Questions Based on Context?",
    "authors": [
      "Yueheng Zhang",
      "Xiaoyuan Liu",
      "Yiyou Sun",
      "Atheer Alharbi",
      "Hend Alzahrani",
      "Basel Alomair",
      "Dawn Song"
    ],
    "abstract": "This paper evaluates questions generated by LLMs from context, comparing them\nto human-generated questions across six dimensions. We introduce an automated\nLLM-based evaluation method, focusing on aspects like question length, type,\ncontext coverage, and answerability. Our findings highlight unique\ncharacteristics of LLM-generated questions, contributing insights that can\nsupport further research in question quality and downstream applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03491v1",
    "published_date": "2025-01-07 03:21:17 UTC",
    "updated_date": "2025-01-07 03:21:17 UTC"
  },
  {
    "arxiv_id": "2501.03486v1",
    "title": "Align-Pro: A Principled Approach to Prompt Optimization for LLM Alignment",
    "authors": [
      "Prashant Trivedi",
      "Souradip Chakraborty",
      "Avinash Reddy",
      "Vaneet Aggarwal",
      "Amrit Singh Bedi",
      "George K. Atia"
    ],
    "abstract": "The alignment of large language models (LLMs) with human values is critical\nas these models become increasingly integrated into various societal and\ndecision-making processes. Traditional methods, such as reinforcement learning\nfrom human feedback (RLHF), achieve alignment by fine-tuning model parameters,\nbut these approaches are often computationally expensive and impractical when\nmodels are frozen or inaccessible for parameter modification. In contrast,\nprompt optimization is a viable alternative to RLHF for LLM alignment. While\nthe existing literature has shown empirical promise of prompt optimization, its\ntheoretical underpinning remains under-explored. We address this gap by\nformulating prompt optimization as an optimization problem and try to provide\ntheoretical insights into the optimality of such a framework. To analyze the\nperformance of the prompt optimization, we study theoretical suboptimality\nbounds and provide insights in terms of how prompt optimization depends upon\nthe given prompter and target model. We also provide empirical validation\nthrough experiments on various datasets, demonstrating that prompt optimization\ncan effectively align LLMs, even when parameter fine-tuning is not feasible.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, Accepted in AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03486v1",
    "published_date": "2025-01-07 03:14:39 UTC",
    "updated_date": "2025-01-07 03:14:39 UTC"
  },
  {
    "arxiv_id": "2501.03475v1",
    "title": "Reading with Intent -- Neutralizing Intent",
    "authors": [
      "Benjamin Reichman",
      "Adar Avsian",
      "Larry Heck"
    ],
    "abstract": "Queries to large language models (LLMs) can be divided into two parts: the\ninstruction/question and the accompanying context. The context for\nretrieval-augmented generation (RAG) systems in most benchmarks comes from\nWikipedia or Wikipedia-like texts which are written in a neutral and factual\ntone. However, when RAG systems retrieve internet-based content, they encounter\ntext with diverse tones and linguistic styles, introducing challenges for\ndownstream tasks. The Reading with Intent task addresses this issue by\nevaluating how varying tones in context passages affect model performance.\nBuilding on prior work that focused on sarcasm, we extend this paradigm by\nconstructing a dataset where context passages are transformed to $11$ distinct\nemotions using a better synthetic data generation approach. Using this dataset,\nwe train an emotion translation model to systematically adapt passages to\nspecified emotional tones. The human evaluation shows that the LLM fine-tuned\nto become the emotion-translator benefited from the synthetically generated\ndata. Finally, the emotion-translator is used in the Reading with Intent task\nto transform the passages to a neutral tone. By neutralizing the passages, it\nmitigates the challenges posed by sarcastic passages and improves overall\nresults on this task by about $3\\%$.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03475v1",
    "published_date": "2025-01-07 02:33:25 UTC",
    "updated_date": "2025-01-07 02:33:25 UTC"
  },
  {
    "arxiv_id": "2501.04732v1",
    "title": "SNR-EQ-JSCC: Joint Source-Channel Coding with SNR-Based Embedding and Query",
    "authors": [
      "Hongwei Zhang",
      "Meixia Tao"
    ],
    "abstract": "Coping with the impact of dynamic channels is a critical issue in joint\nsource-channel coding (JSCC)-based semantic communication systems. In this\npaper, we propose a lightweight channel-adaptive semantic coding architecture\ncalled SNR-EQ-JSCC. It is built upon the generic Transformer model and achieves\nchannel adaptation (CA) by Embedding the signal-to-noise ratio (SNR) into the\nattention blocks and dynamically adjusting attention scores through\nchannel-adaptive Queries. Meanwhile, penalty terms are introduced in the loss\nfunction to stabilize the training process. Considering that instantaneous SNR\nfeedback may be imperfect, we propose an alternative method that uses only the\naverage SNR, which requires no retraining of SNR-EQ-JSCC. Simulation results\nconducted on image transmission demonstrate that the proposed SNR-EQJSCC\noutperforms the state-of-the-art SwinJSCC in peak signal-to-noise ratio (PSNR)\nand perception metrics while only requiring 0.05% of the storage overhead and\n6.38% of the computational complexity for CA. Moreover, the channel-adaptive\nquery method demonstrates significant improvements in perception metrics. When\ninstantaneous SNR feedback is imperfect, SNR-EQ-JSCC using only the average SNR\nstill surpasses baseline schemes.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.04732v1",
    "published_date": "2025-01-07 02:31:04 UTC",
    "updated_date": "2025-01-07 02:31:04 UTC"
  },
  {
    "arxiv_id": "2501.03468v1",
    "title": "MTRAG: A Multi-Turn Conversational Benchmark for Evaluating Retrieval-Augmented Generation Systems",
    "authors": [
      "Yannis Katsis",
      "Sara Rosenthal",
      "Kshitij Fadnis",
      "Chulaka Gunasekara",
      "Young-Suk Lee",
      "Lucian Popa",
      "Vraj Shah",
      "Huaiyu Zhu",
      "Danish Contractor",
      "Marina Danilevsky"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has recently become a very popular task\nfor Large Language Models (LLMs). Evaluating them on multi-turn RAG\nconversations, where the system is asked to generate a response to a question\nin the context of a preceding conversation is an important and often overlooked\ntask with several additional challenges. We present MTRAG: an end-to-end\nhuman-generated multi-turn RAG benchmark that reflects several real-world\nproperties across diverse dimensions for evaluating the full RAG pipeline.\nMTRAG contains 110 conversations averaging 7.7 turns each across four domains\nfor a total of 842 tasks. We also explore automation paths via synthetic data\nand LLM-as-a-Judge evaluation. Our human and automatic evaluations show that\neven state-of-the-art LLM RAG systems struggle on MTRAG. We demonstrate the\nneed for strong retrieval and generation systems that can handle later turns,\nunanswerable questions, non-standalone questions, and multiple domains. MTRAG\nis available at https://github.com/ibm/mt-rag-benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03468v1",
    "published_date": "2025-01-07 01:52:56 UTC",
    "updated_date": "2025-01-07 01:52:56 UTC"
  },
  {
    "arxiv_id": "2501.03464v2",
    "title": "LHGNN: Local-Higher Order Graph Neural Networks For Audio Classification and Tagging",
    "authors": [
      "Shubhr Singh",
      "Emmanouil Benetos",
      "Huy Phan",
      "Dan Stowell"
    ],
    "abstract": "Transformers have set new benchmarks in audio processing tasks, leveraging\nself-attention mechanisms to capture complex patterns and dependencies within\naudio data. However, their focus on pairwise interactions limits their ability\nto process the higher-order relations essential for identifying distinct audio\nobjects. To address this limitation, this work introduces the Local- Higher\nOrder Graph Neural Network (LHGNN), a graph based model that enhances feature\nunderstanding by integrating local neighbourhood information with higher-order\ndata from Fuzzy C-Means clusters, thereby capturing a broader spectrum of audio\nrelationships. Evaluation of the model on three publicly available audio\ndatasets shows that it outperforms Transformer-based models across all\nbenchmarks while operating with substantially fewer parameters. Moreover, LHGNN\ndemonstrates a distinct advantage in scenarios lacking ImageNet pretraining,\nestablishing its effectiveness and efficiency in environments where extensive\npretraining data is unavailable.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03464v2",
    "published_date": "2025-01-07 01:45:39 UTC",
    "updated_date": "2025-01-29 12:22:49 UTC"
  },
  {
    "arxiv_id": "2501.03461v2",
    "title": "Radar Signal Recognition through Self-Supervised Learning and Domain Adaptation",
    "authors": [
      "Zi Huang",
      "Simon Denman",
      "Akila Pemasiri",
      "Clinton Fookes",
      "Terrence Martin"
    ],
    "abstract": "Automatic radar signal recognition (RSR) plays a pivotal role in electronic\nwarfare (EW), as accurately classifying radar signals is critical for informing\ndecision-making processes. Recent advances in deep learning have shown\nsignificant potential in improving RSR performance in domains with ample\nannotated data. However, these methods fall short in EW scenarios where\nannotated RF data are scarce or impractical to obtain. To address these\nchallenges, we introduce a self-supervised learning (SSL) method which utilises\nmasked signal modelling and RF domain adaption to enhance RSR performance in\nenvironments with limited RF samples and labels. Specifically, we investigate\npre-training masked autoencoders (MAE) on baseband in-phase and quadrature\n(I/Q) signals from various RF domains and subsequently transfer the learned\nrepresentation to the radar domain, where annotated data are limited. Empirical\nresults show that our lightweight self-supervised ResNet model with domain\nadaptation achieves up to a 17.5% improvement in 1-shot classification accuracy\nwhen pre-trained on in-domain signals (i.e., radar signals) and up to a 16.31%\nimprovement when pre-trained on out-of-domain signals (i.e., comm signals),\ncompared to its baseline without SSL. We also provide reference results for\nseveral MAE designs and pre-training strategies, establishing a new benchmark\nfor few-shot radar signal classification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.03461v2",
    "published_date": "2025-01-07 01:35:56 UTC",
    "updated_date": "2025-01-14 04:53:30 UTC"
  },
  {
    "arxiv_id": "2501.03458v1",
    "title": "Activating Associative Disease-Aware Vision Token Memory for LLM-Based X-ray Report Generation",
    "authors": [
      "Xiao Wang",
      "Fuling Wang",
      "Haowen Wang",
      "Bo Jiang",
      "Chuanfu Li",
      "Yaowei Wang",
      "Yonghong Tian",
      "Jin Tang"
    ],
    "abstract": "X-ray image based medical report generation achieves significant progress in\nrecent years with the help of the large language model, however, these models\nhave not fully exploited the effective information in visual image regions,\nresulting in reports that are linguistically sound but insufficient in\ndescribing key diseases. In this paper, we propose a novel associative\nmemory-enhanced X-ray report generation model that effectively mimics the\nprocess of professional doctors writing medical reports. It considers both the\nmining of global and local visual information and associates historical report\ninformation to better complete the writing of the current report. Specifically,\ngiven an X-ray image, we first utilize a classification model along with its\nactivation maps to accomplish the mining of visual regions highly associated\nwith diseases and the learning of disease query tokens. Then, we employ a\nvisual Hopfield network to establish memory associations for disease-related\ntokens, and a report Hopfield network to retrieve report memory information.\nThis process facilitates the generation of high-quality reports based on a\nlarge language model and achieves state-of-the-art performance on multiple\nbenchmark datasets, including the IU X-ray, MIMIC-CXR, and Chexpert Plus. The\nsource code of this work is released on\n\\url{https://github.com/Event-AHU/Medical_Image_Analysis}.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "In Peer Review",
    "pdf_url": "http://arxiv.org/pdf/2501.03458v1",
    "published_date": "2025-01-07 01:19:48 UTC",
    "updated_date": "2025-01-07 01:19:48 UTC"
  },
  {
    "arxiv_id": "2501.03443v1",
    "title": "Optimization Learning",
    "authors": [
      "Pascal Van Hentenryck"
    ],
    "abstract": "This article introduces the concept of optimization learning, a methodology\nto design optimization proxies that learn the input/output mapping of\nparametric optimization problems. These optimization proxies are trustworthy by\ndesign: they compute feasible solutions to the underlying optimization\nproblems, provide quality guarantees on the returned solutions, and scale to\nlarge instances. Optimization proxies are differentiable programs that combine\ntraditional deep learning technology with repair or completion layers to\nproduce feasible solutions. The article shows that optimization proxies can be\ntrained end-to-end in a self-supervised way. It presents methodologies to\nprovide performance guarantees and to scale optimization proxies to large-scale\noptimization problems. The potential of optimization proxies is highlighted\nthrough applications in power systems and, in particular, real-time risk\nassessment and security-constrained optimal power flow.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03443v1",
    "published_date": "2025-01-07 00:09:52 UTC",
    "updated_date": "2025-01-07 00:09:52 UTC"
  }
]
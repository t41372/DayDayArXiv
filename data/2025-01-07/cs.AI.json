{
  "date": "2025-01-07",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-07 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 模型优化、多模态处理、LLM（Large Language Models）应用以及 AI 伦理与安全，亮点包括 NVIDIA 团队提出的物理 AI 平台，以及 LLM 在医疗和强化学习中的创新整合；知名学者如 John Rushby 的 AI 安全框架论文值得关注。\n\n### 重点论文讨论\n我们挑选了最具话题度和影响力的论文优先讨论，包括 AI 安全、LLM 优化和医疗应用领域。以下按主题归类，相关论文放在一起简要概述。\n\n**AI 安全与伦理（Where AI Assurance Might Go Wrong）**  \n- **Where AI Assurance Might Go Wrong: Initial lessons from engineering of critical systems（AI 安全框架的潜在问题：从关键系统工程中获得的初步教训）**  \n  作者：Robin Bloomfield, John Rushby（知名学者）。这篇论文分析了传统安全工程如何应用于 AI 安全框架，强调系统边界和风险评估的重要性。主要贡献是通过 Assurance 2.0 案例，支持决策分析，并指出 AI 系统中信任度需求的量级差异，为 AI 安全提供实用指导。\n\n- **BiasGuard: Guardrailing Fairness in Machine Learning Production Systems（BiasGuard：机器学习生产系统中公平性的防护机制）**  \n  这篇论文提出 BiasGuard 方法，使用 Test-Time Augmentation 和 CTGAN 生成数据，以提升模型公平性。主要发现：在不重新训练的情况下，提高公平性指标 31%，仅损失 0.09% 准确率，适用于实际部署场景。\n\n**LLM 和生成模型优化（PromptGuard: Soft Prompt-Guided Unsafe Content Moderation）**  \n- **PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models（PromptGuard：基于软提示引导的文本到图像模型不安全内容调节）**  \n  作者团队包括杨柳等。论文创新性地使用软提示作为系统提示，优化文本到图像模型的安全性。主要贡献：有效抑制不安全内容生成，同时保持图像质量和效率，实验显示不安全比率降至 5.84%，比现有方法快 7.8 倍。\n\n- **Cosmos World Foundation Model Platform for Physical AI（Cosmos 世界基础模型平台：用于物理 AI 的框架）**  \n  作者：NVIDIA 团队（包括 Dieter Fox 等知名研究者）。这篇论文引入 Cosmos 平台，支持物理 AI 的数字孪生模型训练。主要发现：通过开源模型和视频处理管道，提升 AI 在真实世界的预测准确性和泛化能力，是 AI 应用领域的重大进展。\n\n- **Align-Pro: A Principled Approach to Prompt Optimization for LLM Alignment（Align-Pro：LLM 对齐的原理性提示优化方法）**  \n  论文提出基于优化的提示策略，实现 LLM 的自监督对齐。主要贡献：理论上分析提示优化的子最优边界，并通过实验验证其在资源受限场景下的有效性，显著提升模型性能。\n\n**医疗 AI 和图像处理（AsanAI: In-Browser, No-Code, Offline-First Machine Learning Toolkit）**  \n- **AsanAI: In-Browser, No-Code, Offline-First Machine Learning Toolkit（AsanAI：浏览器内、无代码、离线优先的机器学习工具包）**  \n  这篇论文开发了 AsanAI 工具，支持浏览器端机器学习训练。主要发现：无需安装软件即可处理图像和数据，实验证明其在教育和研究中的高效性，提升了 AI 可访问性。\n\n- **Deep Learning within Tabular Data: Foundations, Challenges, Advances and Future Directions（表格数据中的深度学习：基础、挑战、进展和未来方向）**  \n  作者：Weijieying Ren 等。论文综述了表格数据表示学习的深度学习方法，涵盖训练数据、架构和目标。主要贡献：通过系统文献分析，提供泛化框架指南，强调自监督学习在实际任务中的潜力。\n\n其他论文较多，我们快速掠过以下几篇有代表性的：  \n- **HIVEX: A High-Impact Environment Suite for Multi-Agent Research（HIVEX：多代理研究的高影响环境套件）**  \n  作者：Philipp Dominic Siedler。贡献：提出生态挑战环境套件，提升多代理研究的真实性。\n\n- **TrojanDec: Data-free Detection of Trojan Inputs in Self-supervised Learning（TrojanDec：自监督学习中无数据木马输入检测）**  \n  主要发现：首次实现无数据木马检测，实验显示在攻击场景下恢复准确率高。\n\n- **LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token（LLaVA-Mini：使用单个视觉标记的图像和视频高效大多模态模型）**  \n  贡献：通过模态预融合减少视觉标记，提升多模态处理效率，FLOPs 减少 77%。\n\n今天 arXiv 的论文整体质量高，AI 领域的创新层出不穷，但需注意实际应用中的伦理和效率挑战。更多细节可查阅 arXiv 页面！",
  "papers": [
    {
      "arxiv_id": "2501.04182v1",
      "title": "Fixed Points of Deep Neural Networks: Emergence, Stability, and Applications",
      "title_zh": "深度神经网络的固定点：涌现、稳定性和应用",
      "authors": [
        "L. Berlyand",
        "V. Slavin"
      ],
      "abstract": "We present numerical and analytical results on the formation and stability of\na family of fixed points of deep neural networks (DNNs). Such fixed points\nappear in a class of DNNs when dimensions of input and output vectors are the\nsame. We demonstrate examples of applications of such networks in supervised,\nsemi-supervised and unsupervised learning such as encoding/decoding of images,\nrestoration of damaged images among others.\n  We present several numerical and analytical results. First, we show that for\nuntrained DNN's with weights and biases initialized by normally distributed\nrandom variables the only one fixed point exists. This result holds for DNN\nwith any depth (number of layers) $L$, any layer width $N$, and sigmoid-type\nactivation functions. Second, it has been shown that for a DNN whose parameters\n(weights and biases) are initialized by ``light-tailed'' distribution of\nweights (e.g. normal distribution), after training the distribution of these\nparameters become ``heavy-tailed''. This motivates our study of DNNs with\n``heavy-tailed'' initialization. For such DNNs we show numerically %existence\nand stability that training leads to emergence of $Q(N,L)$ fixed points, where\n$Q(N,L)$ is a positive integer which depends on the number of layers $L$ and\nlayer width $N$. We further observe numerically that for fixed $N = N_0$ the\nfunction $Q(N_0, L)$ is non-monotone, that is it initially grows as $L$\nincreases and then decreases to 1.\n  This non-monotone behavior of $Q(N_0, L)$ is also obtained by analytical\nderivation of equation for Empirical Spectral Distribution (ESD) of\ninput-output Jacobian followed by numerical solution of this equation.",
      "tldr_zh": "本研究探讨了深度神经网络(DNNs)中固定点的出现(stability)、稳定性和应用，通过数值和分析方法进行分析。论文发现，对于未经训练的DNNs（权重和偏差由正态分布初始化），无论层数L、层宽N或使用sigmoid-type激活函数，仅存在一个固定点；而在训练后，参数分布从“light-tailed”变为“heavy-tailed”，导致出现Q(N,L)个固定点，且Q(N_0, L)随L增加呈非单调变化（先增长后减少）。此外，作者通过分析输入-输出Jacobian的Empirical Spectral Distribution (ESD)方程并数值求解，验证了这些现象，并展示了固定点在监督、半监督和无监督学习中的应用，如图像编码/解码和损坏图像修复。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.04182v1",
      "published_date": "2025-01-07 23:23:26 UTC",
      "updated_date": "2025-01-07 23:23:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:02:54.234079"
    },
    {
      "arxiv_id": "2501.04180v2",
      "title": "HIVEX: A High-Impact Environment Suite for Multi-Agent Research (extended version)",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp Dominic Siedler"
      ],
      "abstract": "Games have been vital test beds for the rapid development of Agent-based\nresearch. Remarkable progress has been achieved in the past, but it is unclear\nif the findings equip for real-world problems. While pressure grows, some of\nthe most critical ecological challenges can find mitigation and prevention\nsolutions through technology and its applications. Most real-world domains\ninclude multi-agent scenarios and require machine-machine and human-machine\ncollaboration. Open-source environments have not advanced and are often toy\nscenarios, too abstract or not suitable for multi-agent research. By mimicking\nreal-world problems and increasing the complexity of environments, we hope to\nadvance state-of-the-art multi-agent research and inspire researchers to work\non immediate real-world problems. Here, we present HIVEX, an environment suite\nto benchmark multi-agent research focusing on ecological challenges. HIVEX\nincludes the following environments: Wind Farm Control, Wildfire Resource\nManagement, Drone-Based Reforestation, Ocean Plastic Collection, and Aerial\nWildfire Suppression. We provide environments, training examples, and baselines\nfor the main and sub-tasks. All trained models resulting from the experiments\nof this work are hosted on Hugging Face. We also provide a leaderboard on\nHugging Face and encourage the community to submit models trained on our\nenvironment suite.",
      "tldr_zh": "该论文介绍了HIVEX，一套高影响力的环境套件，用于基准测试多智能体研究（Multi-Agent Research），旨在解决现有开源环境过于简单或不适用于真实世界生态挑战的问题。HIVEX通过模拟真实场景如Wind Farm Control、Wildfire Resource Management、Drone-Based Reforestation、Ocean Plastic Collection和Aerial Wildfire Suppression等任务，强调多智能体协作和机器-人类互动。作者提供环境、训练示例、基线模型以及Hugging Face上的模型托管和排行榜，鼓励社区参与以推进生态相关的研究应用。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04180v2",
      "published_date": "2025-01-07 23:16:31 UTC",
      "updated_date": "2025-01-21 14:25:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:03:04.535618"
    },
    {
      "arxiv_id": "2501.04173v1",
      "title": "Multimodal Multihop Source Retrieval for Web Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Navya Yarrabelly",
        "Saloni Mittal"
      ],
      "abstract": "This work deals with the challenge of learning and reasoning over multi-modal\nmulti-hop question answering (QA). We propose a graph reasoning network based\non the semantic structure of the sentences to learn multi-source reasoning\npaths and find the supporting facts across both image and text modalities for\nanswering the question. In this paper, we investigate the importance of graph\nstructure for multi-modal multi-hop question answering. Our analysis is\ncentered on WebQA. We construct a strong baseline model, that finds relevant\nsources using a pairwise classification task. We establish that, with the\nproper use of feature representations from pre-trained models, graph structure\nhelps in improving multi-modal multi-hop question answering. We point out that\nboth graph structure and adjacency matrix are task-related prior knowledge, and\ngraph structure can be leveraged to improve the retrieval performance for the\ntask. Experiments and visualized analysis demonstrate that message propagation\nover graph networks or the entire graph structure can replace massive\nmultimodal transformers with token-wise cross-attention. We demonstrated the\napplicability of our method and show a performance gain of \\textbf{4.6$\\%$}\nretrieval F1score over the transformer baselines, despite being a very light\nmodel. We further demonstrated the applicability of our model to a large scale\nretrieval setting.",
      "tldr_zh": "这篇论文针对多模态（multimodal）和多跳（multihop）问答的挑战，提出了一种基于图推理网络的模型，利用句子语义结构学习多源推理路径，并在图像和文本模态中检索支持事实。研究在 WebQA 数据集上构建了强基线模型，通过预训练特征表示和图结构（如邻接矩阵）提升了检索性能，证明消息传播机制能取代大规模的多模态 transformer。实验结果显示，该轻量级模型在检索 F1 分数上比 transformer 基线提高了 4.6%，并适用于大规模检索设置。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2010.03604 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2501.04173v1",
      "published_date": "2025-01-07 22:53:56 UTC",
      "updated_date": "2025-01-07 22:53:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:03:17.567725"
    },
    {
      "arxiv_id": "2501.04169v1",
      "title": "Learning to Transfer Human Hand Skills for Robot Manipulations",
      "title_zh": "翻译失败",
      "authors": [
        "Sungjae Park",
        "Seungho Lee",
        "Mingi Choi",
        "Jiye Lee",
        "Jeonghwan Kim",
        "Jisoo Kim",
        "Hanbyul Joo"
      ],
      "abstract": "We present a method for teaching dexterous manipulation tasks to robots from\nhuman hand motion demonstrations. Unlike existing approaches that solely rely\non kinematics information without taking into account the plausibility of robot\nand object interaction, our method directly infers plausible robot manipulation\nactions from human motion demonstrations. To address the embodiment gap between\nthe human hand and the robot system, our approach learns a joint motion\nmanifold that maps human hand movements, robot hand actions, and object\nmovements in 3D, enabling us to infer one motion component from others. Our key\nidea is the generation of pseudo-supervision triplets, which pair human,\nobject, and robot motion trajectories synthetically. Through real-world\nexperiments with robot hand manipulation, we demonstrate that our data-driven\nretargeting method significantly outperforms conventional retargeting\ntechniques, effectively bridging the embodiment gap between human and robotic\nhands. Website at https://rureadyo.github.io/MocapRobot/.",
      "tldr_zh": "本研究提出了一种从人类手部动作演示中学习并转移技能的方法，用于机器人灵巧操作任务。该方法通过学习一个joint motion manifold，将人类手动作、机器人手动作和物体运动在3D空间中映射，以解决embodiment gap问题，并生成pseudo-supervision triplets作为合成监督数据。相比传统重定向技术，该数据-driven retargeting方法在真实机器人手实验中表现出显著优势，有效桥接了人类和机器人手之间的差异。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Preprint. Under Review",
      "pdf_url": "http://arxiv.org/pdf/2501.04169v1",
      "published_date": "2025-01-07 22:33:47 UTC",
      "updated_date": "2025-01-07 22:33:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:05:29.437187"
    },
    {
      "arxiv_id": "2501.04167v1",
      "title": "Reasoning-Enhanced Self-Training for Long-Form Personalized Text Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Alireza Salemi",
        "Cheng Li",
        "Mingyang Zhang",
        "Qiaozhu Mei",
        "Weize Kong",
        "Tao Chen",
        "Zhuowan Li",
        "Michael Bendersky",
        "Hamed Zamani"
      ],
      "abstract": "Personalized text generation requires a unique ability of large language\nmodels (LLMs) to learn from context that they often do not encounter during\ntheir standard training. One way to encourage LLMs to better use personalized\ncontext for generating outputs that better align with the user's expectations\nis to instruct them to reason over the user's past preferences, background\nknowledge, or writing style. To achieve this, we propose Reasoning-Enhanced\nSelf-Training for Personalized Text Generation (REST-PG), a framework that\ntrains LLMs to reason over personal data during response generation. REST-PG\nfirst generates reasoning paths to train the LLM's reasoning abilities and then\nemploys Expectation-Maximization Reinforced Self-Training to iteratively train\nthe LLM based on its own high-reward outputs. We evaluate REST-PG on the\nLongLaMP benchmark, consisting of four diverse personalized long-form text\ngeneration tasks. Our experiments demonstrate that REST-PG achieves significant\nimprovements over state-of-the-art baselines, with an average relative\nperformance gain of 14.5% on the benchmark.",
      "tldr_zh": "本研究提出了一种名为REST-PG的框架，用于提升大型语言模型(LLMs)在长文本个性化生成中的性能，通过增强推理能力来更好地利用用户的偏好、背景知识和写作风格。REST-PG首先生成推理路径以训练LLMs的推理技能，然后采用Expectation-Maximization Reinforced Self-Training方法，通过迭代训练基于自身高奖励输出来优化模型。在LongLaMP基准测试中，该框架在四个多样化任务上比现有基线平均相对性能提升14.5%，显著改善了个性化文本生成的准确性和适应性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04167v1",
      "published_date": "2025-01-07 22:29:08 UTC",
      "updated_date": "2025-01-07 22:29:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:03:40.151947"
    },
    {
      "arxiv_id": "2502.03467v1",
      "title": "Where AI Assurance Might Go Wrong: Initial lessons from engineering of critical systems",
      "title_zh": "AI 保证可能出错之处：从关键系统工程中获得的初步教训",
      "authors": [
        "Robin Bloomfield",
        "John Rushby"
      ],
      "abstract": "We draw on our experience working on system and software assurance and\nevaluation for systems important to society to summarise how safety engineering\nis performed in traditional critical systems, such as aircraft flight control.\nWe analyse how this critical systems perspective might support the development\nand implementation of AI Safety Frameworks. We present the analysis in terms\nof: system engineering, safety and risk analysis, and decision analysis and\nsupport.\n  We consider four key questions: What is the system? How good does it have to\nbe? What is the impact of criticality on system development? and How much\nshould we trust it? We identify topics worthy of further discussion. In\nparticular, we are concerned that system boundaries are not broad enough, that\nthe tolerability and nature of the risks are not sufficiently elaborated, and\nthat the assurance methods lack theories that would allow behaviours to be\nadequately assured.\n  We advocate the use of assurance cases based on Assurance 2.0 to support\ndecision making in which the criticality of the decision as well as the\ncriticality of the system are evaluated. We point out the orders of magnitude\ndifference in confidence needed in critical rather than everyday systems and\nhow everyday techniques do not scale in rigour.\n  Finally we map our findings in detail to two of the questions posed by the\nFAISC organisers and we note that the engineering of critical systems has\nevolved through open and diverse discussion. We hope that topics identified\nhere will support the post-FAISC dialogues.",
      "tldr_zh": "本研究基于传统关键系统的安全工程经验（如飞机飞行控制），分析了这些方法如何支持 AI Safety Frameworks 的开发和实施，从系统工程、安全和风险分析以及决策分析三个方面进行探讨。论文提出四个关键问题，包括系统定义、性能要求、关键性对开发的影响以及信任程度，并指出当前存在的问题，如系统边界不够宽泛、风险可容忍性未充分阐述，以及保证方法缺乏足够的理论支持。作者倡导使用基于 Assurance 2.0 的保证案例来提升决策过程的严谨性，强调关键系统需要远高于日常系统的信心水平，并将发现映射到 FAISC 相关问题，以促进后续对话。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CY",
      "comment": "Presented at UK AI Safety Institute (AISI) Conference on Frontier AI\n  Safety Frameworks (FAISC 24), Berkeley CA, November 2024",
      "pdf_url": "http://arxiv.org/pdf/2502.03467v1",
      "published_date": "2025-01-07 22:02:23 UTC",
      "updated_date": "2025-01-07 22:02:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:05:53.352855"
    },
    {
      "arxiv_id": "2501.04142v1",
      "title": "BiasGuard: Guardrailing Fairness in Machine Learning Production Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Nurit Cohen-Inger",
        "Seffi Cohen",
        "Neomi Rabaev",
        "Lior Rokach",
        "Bracha Shapira"
      ],
      "abstract": "As machine learning (ML) systems increasingly impact critical sectors such as\nhiring, financial risk assessments, and criminal justice, the imperative to\nensure fairness has intensified due to potential negative implications. While\nmuch ML fairness research has focused on enhancing training data and processes,\naddressing the outputs of already deployed systems has received less attention.\nThis paper introduces 'BiasGuard', a novel approach designed to act as a\nfairness guardrail in production ML systems. BiasGuard leverages Test-Time\nAugmentation (TTA) powered by Conditional Generative Adversarial Network\n(CTGAN), a cutting-edge generative AI model, to synthesize data samples\nconditioned on inverted protected attribute values, thereby promoting equitable\noutcomes across diverse groups. This method aims to provide equal opportunities\nfor both privileged and unprivileged groups while significantly enhancing the\nfairness metrics of deployed systems without the need for retraining. Our\ncomprehensive experimental analysis across diverse datasets reveals that\nBiasGuard enhances fairness by 31% while only reducing accuracy by 0.09%\ncompared to non-mitigated benchmarks. Additionally, BiasGuard outperforms\nexisting post-processing methods in improving fairness, positioning it as an\neffective tool to safeguard against biases when retraining the model is\nimpractical.",
      "tldr_zh": "这篇论文介绍了 BiasGuard，一种创新方法，用于在生产 ML 系统（如招聘、金融风险评估和刑事司法领域）中作为公平性防护栏，以减少潜在偏见。BiasGuard 利用 Test-Time Augmentation (TTA) 和 Conditional Generative Adversarial Network (CTGAN) 生成基于反转保护属性值的合成数据样本，从而为特权和非特权群体提供平等机会，而无需重新训练模型。实验结果显示，BiasGuard 在多种数据集上提高了 31% 的公平性指标，同时仅降低了 0.09% 的准确率，并优于现有后处理方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04142v1",
      "published_date": "2025-01-07 21:10:16 UTC",
      "updated_date": "2025-01-07 21:10:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:04:05.228984"
    },
    {
      "arxiv_id": "2501.04136v1",
      "title": "Implementing Systemic Thinking for Automatic Schema Matching: An Agent-Based Modeling Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Hicham Assoudi",
        "Hakim Lounis"
      ],
      "abstract": "Several approaches are proposed to deal with the problem of the Automatic\nSchema Matching (ASM). The challenges and difficulties caused by the complexity\nand uncertainty characterizing both the process and the outcome of Schema\nMatching motivated us to investigate how bio-inspired emerging paradigm can\nhelp with understanding, managing, and ultimately overcoming those challenges.\nIn this paper, we explain how we approached Automatic Schema Matching as a\nsystemic and Complex Adaptive System (CAS) and how we modeled it using the\napproach of Agent-Based Modeling and Simulation (ABMS). This effort gives birth\nto a tool (prototype) for schema matching called Reflex-SMAS. A set of\nexperiments demonstrates the viability of our approach on two main aspects: (i)\neffectiveness (increasing the quality of the found matchings) and (ii)\nefficiency (reducing the effort required for this efficiency). Our approach\nrepresents a significant paradigm-shift, in the field of Automatic Schema\nMatching.",
      "tldr_zh": "本文提出了一种将 Automatic Schema Matching (ASM) 视为 Systemic and Complex Adaptive System (CAS) 的新方法，并采用 Agent-Based Modeling and Simulation (ABMS) 进行建模，以应对 ASM 过程中的复杂性和不确定性。研究开发了名为 Reflex-SMAS 的原型工具，通过实验验证，该方法在提高匹配质量和降低所需努力方面均表现出显著效果。总体上，这一方法标志着 ASM 领域的一个重要范式转变。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "COGNITIVE 2018 : The Tenth International Conference on Advanced\n  Cognitive Technologies and Applications",
      "pdf_url": "http://arxiv.org/pdf/2501.04136v1",
      "published_date": "2025-01-07 20:52:08 UTC",
      "updated_date": "2025-01-07 20:52:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:04:16.366389"
    },
    {
      "arxiv_id": "2501.04734v1",
      "title": "Generative Style Transfer for MRI Image Segmentation: A Case of Glioma Segmentation in Sub-Saharan Africa",
      "title_zh": "翻译失败",
      "authors": [
        "Rancy Chepchirchir",
        "Jill Sunday",
        "Raymond Confidence",
        "Dong Zhang",
        "Talha Chaudhry",
        "Udunna C. Anazodo",
        "Kendi Muchungi",
        "Yujing Zou"
      ],
      "abstract": "In Sub-Saharan Africa (SSA), the utilization of lower-quality Magnetic\nResonance Imaging (MRI) technology raises questions about the applicability of\nmachine learning methods for clinical tasks. This study aims to provide a\nrobust deep learning-based brain tumor segmentation (BraTS) method tailored for\nthe SSA population using a threefold approach. Firstly, the impact of domain\nshift from the SSA training data on model efficacy was examined, revealing no\nsignificant effect. Secondly, a comparative analysis of 3D and 2D\nfull-resolution models using the nnU-Net framework indicates similar\nperformance of both the models trained for 300 epochs achieving a five-fold\ncross-validation score of 0.93. Lastly, addressing the performance gap observed\nin SSA validation as opposed to the relatively larger BraTS glioma (GLI)\nvalidation set, two strategies are proposed: fine-tuning SSA cases using the\nGLI+SSA best-pretrained 2D fullres model at 300 epochs, and introducing a novel\nneural style transfer-based data augmentation technique for the SSA cases. This\ninvestigation underscores the potential of enhancing brain tumor prediction\nwithin SSA's unique healthcare landscape.",
      "tldr_zh": "本研究针对撒哈拉以南非洲（SSA）地区使用较低质量 MRI 图像的挑战，提出了一种鲁棒的深度学习脑肿瘤分割（BraTS）方法，专注于胶质瘤（Glioma）分割，以适应本地医疗环境。首先，分析了从 SSA 训练数据到模型效能的领域转移影响，发现无显著效果；同时，使用 nnU-Net 框架比较了 3D 和 2D 全分辨率模型，两者均在 300 个 epochs 下达到 0.93 的五折交叉验证分数。其次，为解决 SSA 验证集的性能差距，引入了两种策略：利用 GLI+SSA 最佳预训练 2D fullres 模型微调 SSA 病例，以及一种新型神经风格转移（neural style transfer）-based 数据增强技术。该方法突显了在 SSA 独特医疗景观中提升脑肿瘤预测潜力的重要性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04734v1",
      "published_date": "2025-01-07 19:48:30 UTC",
      "updated_date": "2025-01-07 19:48:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:04:29.206305"
    },
    {
      "arxiv_id": "2501.04108v2",
      "title": "TrojanDec: Data-free Detection of Trojan Inputs in Self-supervised Learning",
      "title_zh": "TrojanDec：自监督学习中木马输入的无数据检测",
      "authors": [
        "Yupei Liu",
        "Yanting Wang",
        "Jinyuan Jia"
      ],
      "abstract": "An image encoder pre-trained by self-supervised learning can be used as a\ngeneral-purpose feature extractor to build downstream classifiers for various\ndownstream tasks. However, many studies showed that an attacker can embed a\ntrojan into an encoder such that multiple downstream classifiers built based on\nthe trojaned encoder simultaneously inherit the trojan behavior. In this work,\nwe propose TrojanDec, the first data-free method to identify and recover a test\ninput embedded with a trigger. Given a (trojaned or clean) encoder and a test\ninput, TrojanDec first predicts whether the test input is trojaned. If not, the\ntest input is processed in a normal way to maintain the utility. Otherwise, the\ntest input will be further restored to remove the trigger. Our extensive\nevaluation shows that TrojanDec can effectively identify the trojan (if any)\nfrom a given test input and recover it under state-of-the-art trojan attacks.\nWe further demonstrate by experiments that our TrojanDec outperforms the\nstate-of-the-art defenses.",
      "tldr_zh": "本研究提出TrojanDec，一种无数据(data-free)方法，用于检测和恢复自监督学习(Self-supervised Learning)中的Trojan Inputs。TrojanDec给定一个可能被篡改的图像编码器和测试输入，首先预测输入是否嵌入触发器；若未被篡改，则正常处理；若被篡改，则进一步恢复以移除触发器。实验结果显示，TrojanDec在最先进的Trojan攻击下有效识别和恢复输入，并优于现有state-of-the-art防御方法。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "To appear in AAAI'2025",
      "pdf_url": "http://arxiv.org/pdf/2501.04108v2",
      "published_date": "2025-01-07 19:35:19 UTC",
      "updated_date": "2025-02-04 15:23:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:04:40.457590"
    },
    {
      "arxiv_id": "2501.04102v1",
      "title": "Enhancing Distribution and Label Consistency for Graph Out-of-Distribution Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Song Wang",
        "Xiaodong Yang",
        "Rashidul Islam",
        "Huiyuan Chen",
        "Minghua Xu",
        "Jundong Li",
        "Yiwei Cai"
      ],
      "abstract": "To deal with distribution shifts in graph data, various graph\nout-of-distribution (OOD) generalization techniques have been recently\nproposed. These methods often employ a two-step strategy that first creates\naugmented environments and subsequently identifies invariant subgraphs to\nimprove generalizability. Nevertheless, this approach could be suboptimal from\nthe perspective of consistency. First, the process of augmenting environments\nby altering the graphs while preserving labels may lead to graphs that are not\nrealistic or meaningfully related to the origin distribution, thus lacking\ndistribution consistency. Second, the extracted subgraphs are obtained from\ndirectly modifying graphs, and may not necessarily maintain a consistent\npredictive relationship with their labels, thereby impacting label consistency.\nIn response to these challenges, we introduce an innovative approach that aims\nto enhance these two types of consistency for graph OOD generalization. We\npropose a modifier to obtain both augmented and invariant graphs in a unified\nmanner. With the augmented graphs, we enrich the training data without\ncompromising the integrity of label-graph relationships. The label consistency\nenhancement in our framework further preserves the supervision information in\nthe invariant graph. We conduct extensive experiments on real-world datasets to\ndemonstrate the superiority of our framework over other state-of-the-art\nbaselines.",
      "tldr_zh": "本研究针对图 Out-of-Distribution (OOD) 泛化问题，指出现有方法在增强环境和提取不变子图时，可能导致分布一致性和标签一致性的不足，从而影响模型泛化能力。  \n为了解决这些挑战，该框架引入一个统一的修改器来同时生成增强图和不变图，同时通过增强标签一致性来保留监督信息，从而丰富训练数据并保持标签-图关系的完整性。  \n实验在真实世界数据集上表明，该方法优于现有最先进基线，显著提升了图 OOD 泛化的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICDM 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.04102v1",
      "published_date": "2025-01-07 19:19:22 UTC",
      "updated_date": "2025-01-07 19:19:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:06:04.748971"
    },
    {
      "arxiv_id": "2501.04733v1",
      "title": "AI-Driven Reinvention of Hydrological Modeling for Accurate Predictions and Interpretation to Transform Earth System Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Cuihui Xia",
        "Lei Yue",
        "Deliang Chen",
        "Yuyang Li",
        "Hongqiang Yang",
        "Ancheng Xue",
        "Zhiqiang Li",
        "Qing He",
        "Guoqing Zhang",
        "Dambaru Ballab Kattel",
        "Lei Lei",
        "Ming Zhou"
      ],
      "abstract": "Traditional equation-driven hydrological models often struggle to accurately\npredict streamflow in challenging regional Earth systems like the Tibetan\nPlateau, while hybrid and existing algorithm-driven models face difficulties in\ninterpreting hydrological behaviors. This work introduces HydroTrace, an\nalgorithm-driven, data-agnostic model that substantially outperforms these\napproaches, achieving a Nash-Sutcliffe Efficiency of 98% and demonstrating\nstrong generalization on unseen data. Moreover, HydroTrace leverages advanced\nattention mechanisms to capture spatial-temporal variations and\nfeature-specific impacts, enabling the quantification and spatial resolution of\nstreamflow partitioning as well as the interpretation of hydrological behaviors\nsuch as glacier-snow-streamflow interactions and monsoon dynamics.\nAdditionally, a large language model (LLM)-based application allows users to\neasily understand and apply HydroTrace's insights for practical purposes. These\nadvancements position HydroTrace as a transformative tool in hydrological and\nbroader Earth system modeling, offering enhanced prediction accuracy and\ninterpretability.",
      "tldr_zh": "这篇论文介绍了HydroTrace，一种AI驱动的算法模型，用于革新水文建模，解决传统方程驱动模型在预测青藏高原等复杂区域河流量时的准确性和解释性难题。HydroTrace在数据无关的框架下，通过高级注意力机制捕获空间-时间变化和特征影响，实现了98%的Nash-Sutcliffe Efficiency，并在未见数据上表现出强泛化能力，同时能量化河流量分区并解释关键水文行为如冰川-雪-河流量互动和季风动态。论文还开发了基于LLM的应用程序，便于用户理解和应用这些洞见。该模型有望转变水文和更广泛的地球系统建模，提供更高的预测准确性和可解释性。",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "physics.ao-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04733v1",
      "published_date": "2025-01-07 18:59:53 UTC",
      "updated_date": "2025-01-07 18:59:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:06:17.359723"
    },
    {
      "arxiv_id": "2501.03968v2",
      "title": "VLM-driven Behavior Tree for Context-aware Task Planning",
      "title_zh": "VLM驱动的行为树用于上下文感知任务规划",
      "authors": [
        "Naoki Wake",
        "Atsushi Kanehira",
        "Jun Takamatsu",
        "Kazuhiro Sasabuchi",
        "Katsushi Ikeuchi"
      ],
      "abstract": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations.",
      "tldr_zh": "本论文提出了一种新框架，利用 Vision-Language Models (VLMs) 来生成和编辑 Behavior Trees (BTs)，以处理视觉条件，实现机器人任务规划在视觉复杂环境中的上下文感知。\n框架的核心机制是通过自提示的视觉条件节点生成 BTs，这些条件以自由形式文本表达，并由另一个 VLM 过程整合文本并在机器人执行时使用真实世界图像进行评估。\n在真实咖啡馆场景的实验中，该框架证明了其可行性，同时暴露了某些局限性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages, 11 figures, 5 tables. Last updated on January 9th, 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.03968v2",
      "published_date": "2025-01-07 18:06:27 UTC",
      "updated_date": "2025-01-10 10:38:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:06:29.080791"
    },
    {
      "arxiv_id": "2501.03952v1",
      "title": "Localizing AI: Evaluating Open-Weight Language Models for Languages of Baltic States",
      "title_zh": "翻译失败",
      "authors": [
        "Jurgita Kapočiūtė-Dzikienė",
        "Toms Bergmanis",
        "Mārcis Pinnis"
      ],
      "abstract": "Although large language models (LLMs) have transformed our expectations of\nmodern language technologies, concerns over data privacy often restrict the use\nof commercially available LLMs hosted outside of EU jurisdictions. This limits\ntheir application in governmental, defence, and other data-sensitive sectors.\nIn this work, we evaluate the extent to which locally deployable open-weight\nLLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.\nWe examine various size and precision variants of the top-performing\nmultilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine\ntranslation, multiple-choice question answering, and free-form text generation.\nThe results indicate that while certain models like Gemma~2 perform close to\nthe top commercially available models, many LLMs struggle with these languages.\nMost surprisingly, however, we find that these models, while showing close to\nstate-of-the-art translation performance, are still prone to lexical\nhallucinations with errors in at least 1 in 20 words for all open-weight\nmultilingual LLMs.",
      "tldr_zh": "这篇论文评估了开源LLMs（如Llama 3、Gemma 2、Phi和NeMo）在波罗的海国家语言（Lithuanian、Latvian和Estonian）上的性能，以解决商业LLMs数据隐私问题对政府、国防等领域的限制。研究通过机器翻译、多项选择问答和自由形式文本生成任务测试了这些模型的各种大小和精度变体。结果显示，Gemma 2的表现接近顶级商业模型，但多数LLMs在这些语言上表现不佳，且普遍存在词汇幻觉（lexical hallucinations），导致翻译错误率高达每20个单词中至少一个。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper is accepted to NoDaLiDa/Baltic-HLT 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03952v1",
      "published_date": "2025-01-07 17:24:17 UTC",
      "updated_date": "2025-01-07 17:24:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:06:42.267985"
    },
    {
      "arxiv_id": "2501.03941v1",
      "title": "Synthetic Data Privacy Metrics",
      "title_zh": "翻译失败",
      "authors": [
        "Amy Steier",
        "Lipika Ramaswamy",
        "Andre Manoel",
        "Alexa Haushalter"
      ],
      "abstract": "Recent advancements in generative AI have made it possible to create\nsynthetic datasets that can be as accurate as real-world data for training AI\nmodels, powering statistical insights, and fostering collaboration with\nsensitive datasets while offering strong privacy guarantees. Effectively\nmeasuring the empirical privacy of synthetic data is an important step in the\nprocess. However, while there is a multitude of new privacy metrics being\npublished every day, there currently is no standardization. In this paper, we\nreview the pros and cons of popular metrics that include simulations of\nadversarial attacks. We also review current best practices for amending\ngenerative models to enhance the privacy of the data they create (e.g.\ndifferential privacy).",
      "tldr_zh": "本文讨论了生成式 AI（generative AI）在创建合成数据集方面的进展，这些数据集可用于训练 AI 模型、统计分析和处理敏感数据，同时提供强隐私保障。论文审查了多种流行隐私指标的优缺点，包括模拟对抗攻击的评估方法，并强调了当前缺乏标准化。作者还总结了增强生成模型隐私的最佳实践，例如差分隐私（differential privacy），以促进合成数据的可靠使用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.03941v1",
      "published_date": "2025-01-07 17:02:33 UTC",
      "updated_date": "2025-01-07 17:02:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:06:52.501199"
    },
    {
      "arxiv_id": "2501.03940v2",
      "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection",
      "title_zh": "翻译失败",
      "authors": [
        "Pablo Miralles-González",
        "Javier Huertas-Tato",
        "Alejandro Martín",
        "David Camacho"
      ],
      "abstract": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
      "tldr_zh": "该论文探讨了检测AI生成文本的挑战，特别是针对大型语言模型（LLMs）的下一个token分布输出。作者提出Perplexity Attention Weighted Network (PAWN)，通过使用LLM的最后一个隐藏状态和位置来加权token指标的聚合，从而解决传统方法忽略token难易度的局限性。实验结果显示，PAWN在分布内性能优于基线模型，使用更少参数，并在未见领域、源模型和对抗攻击中表现出更好的泛化性及鲁棒性；此外，如果骨干模型支持多语言，PAWN可在跨语言任务中达到81.46%的平均宏F1分数。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03940v2",
      "published_date": "2025-01-07 17:00:49 UTC",
      "updated_date": "2025-01-22 10:39:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:07:05.629729"
    },
    {
      "arxiv_id": "2501.03936v3",
      "title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides",
      "title_zh": "PPTAgent：超越文本到幻灯片的演示文稿生成与评估",
      "authors": [
        "Hao Zheng",
        "Xinyan Guan",
        "Hao Kong",
        "Jia Zheng",
        "Weixiang Zhou",
        "Hongyu Lin",
        "Yaojie Lu",
        "Ben He",
        "Xianpei Han",
        "Le Sun"
      ],
      "abstract": "Automatically generating presentations from documents is a challenging task\nthat requires accommodating content quality, visual appeal, and structural\ncoherence. Existing methods primarily focus on improving and evaluating the\ncontent quality in isolation, overlooking visual appeal and structural\ncoherence, which limits their practical applicability. To address these\nlimitations, we propose PPTAgent, which comprehensively improves presentation\ngeneration through a two-stage, edit-based approach inspired by human\nworkflows. PPTAgent first analyzes reference presentations to extract\nslide-level functional types and content schemas, then drafts an outline and\niteratively generates editing actions based on selected reference slides to\ncreate new slides. To comprehensively evaluate the quality of generated\npresentations, we further introduce PPTEval, an evaluation framework that\nassesses presentations across three dimensions: Content, Design, and Coherence.\nResults demonstrate that PPTAgent significantly outperforms existing automatic\npresentation generation methods across all three dimensions.",
      "tldr_zh": "该研究提出PPTAgent，一种超越传统Text-to-Slides方法的演示文稿生成框架，通过两阶段编辑流程模拟人类工作流：首先分析参考演示文稿提取slide-level functional types和content schemas，然后起草大纲并迭代生成编辑动作以创建新幻灯片，从而同时提升内容质量、视觉吸引力（visual appeal）和结构连贯性（structural coherence）。为了全面评估生成的演示文稿，该论文引入PPTEval评估框架，从Content、Design和Coherence三个维度进行评估。实验结果显示，PPTAgent在所有维度上显著优于现有方法。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 23 figures, see https://github.com/icip-cas/PPTAgent for\n  details",
      "pdf_url": "http://arxiv.org/pdf/2501.03936v3",
      "published_date": "2025-01-07 16:53:01 UTC",
      "updated_date": "2025-02-21 07:52:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:07:16.757857"
    },
    {
      "arxiv_id": "2501.04072v1",
      "title": "Multi-armed Bandit and Backbone boost Lin-Kernighan-Helsgaun Algorithm for the Traveling Salesman Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Long Wang",
        "Jiongzhi Zheng",
        "Zhengda Xiong",
        "Kun He"
      ],
      "abstract": "The Lin-Kernighan-Helsguan (LKH) heuristic is a classic local search\nalgorithm for the Traveling Salesman Problem (TSP). LKH introduces an\n$\\alpha$-value to replace the traditional distance metric for evaluating the\nedge quality, which leads to a significant improvement. However, we observe\nthat the $\\alpha$-value does not make full use of the historical information\nduring the search, and single guiding information often makes LKH hard to\nescape from some local optima. To address the above issues, we propose a novel\nway to extract backbone information during the TSP local search process, which\nis dynamic and can be updated once a local optimal solution is found. We\nfurther propose to combine backbone information, $\\alpha$-value, and distance\nto evaluate the edge quality so as to guide the search. Moreover, we abstract\ntheir different combinations to arms in a multi-armed bandit (MAB) and use an\nMAB model to help the algorithm select an appropriate evaluation metric\ndynamically. Both the backbone information and MAB can provide diverse guiding\ninformation and learn from the search history to suggest the best metric. We\napply our methods to LKH and LKH-3, which is an extension version of LKH that\ncan be used to solve about 40 variant problems of TSP and Vehicle Routing\nProblem (VRP). Extensive experiments show the excellent performance and\ngeneralization capability of our proposed method, significantly improving LKH\nfor TSP and LKH-3 for two representative TSP and VRP variants, the Colored TSP\n(CTSP) and Capacitated VRP with Time Windows (CVRPTW).",
      "tldr_zh": "该论文针对Lin-Kernighan-Helsgaun (LKH)算法在Traveling Salesman Problem (TSP)中的局限性提出改进方案，主要是因为α-value未能充分利用历史信息，导致容易陷入局部最优。研究者引入动态backbone信息提取机制，并在找到局部最优解时更新它，同时将backbone信息、α-value和距离结合作为边质量评估指标，并利用Multi-armed Bandit (MAB)模型动态选择最佳评估组合，以提供多样化指导和历史学习。实验结果显示，该方法显著提升了LKH在TSP上的性能，以及LKH-3在Colored TSP (CTSP)和Capacitated VRP with Time Windows (CVRPTW)上的表现，展示了优秀的泛化能力。",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04072v1",
      "published_date": "2025-01-07 16:45:41 UTC",
      "updated_date": "2025-01-07 16:45:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:09:29.040228"
    },
    {
      "arxiv_id": "2501.03916v3",
      "title": "Dolphin: Moving Towards Closed-loop Auto-research through Thinking, Practice, and Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Jiakang Yuan",
        "Xiangchao Yan",
        "Shiyang Feng",
        "Bo Zhang",
        "Tao Chen",
        "Botian Shi",
        "Wanli Ouyang",
        "Yu Qiao",
        "Lei Bai",
        "Bowen Zhou"
      ],
      "abstract": "The scientific research paradigm is undergoing a profound transformation\nowing to the development of Artificial Intelligence (AI). Recent works\ndemonstrate that various AI-assisted research methods can largely improve\nresearch efficiency by improving data analysis, accelerating computation, and\nfostering novel idea generation. To further move towards the ultimate goal\n(i.e., automatic scientific research), in this paper, we introduce Dolphin, a\nclosed-loop LLM-driven framework to enhance the automation level of scientific\nresearch. Dolphin first generates novel ideas based on feedback from previous\nexperiments and relevant papers ranked by the topic and task attributes. Then,\nthe generated ideas can be implemented using a code template refined and\ndebugged with the designed exception-traceback-guided local code structure.\nFinally, Dolphin automatically analyzes the results of each idea and feeds the\nresults back to the next round of idea generation. Experiments are conducted on\nthe benchmark datasets of different topics and a subset of MLE-bench. Results\nshow that Dolphin can continuously improve the performance of the input topic\nin a loop. We highlight that Dolphin can automatically propose methods that are\ncomparable to the state-of-the-art in some tasks such as 3D point\nclassification.",
      "tldr_zh": "本论文提出Dolphin，一个基于LLM的闭环框架，旨在通过思考、实践和反馈循环来提升科学研究的自动化水平。Dolphin框架首先根据实验反馈和相关论文生成新想法，然后利用精炼的代码模板进行实现和调试，最后自动分析结果并回馈到下一轮迭代中。实验结果显示，在不同主题的基准数据集和MLE-bench子集上，Dolphin能持续优化性能，并在诸如3D点分类任务中自动提出与最先进方法相当的方案。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 12 figures, and our homepage:\n  https://alpha-innovator.github.io/Dolphin-project-page",
      "pdf_url": "http://arxiv.org/pdf/2501.03916v3",
      "published_date": "2025-01-07 16:31:10 UTC",
      "updated_date": "2025-04-09 16:27:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:07:41.243845"
    },
    {
      "arxiv_id": "2501.10413v1",
      "title": "Cooperative Search and Track of Rogue Drones using Multiagent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Panayiota Valianti",
        "Kleanthis Malialis",
        "Panayiotis Kolios",
        "Georgios Ellinas"
      ],
      "abstract": "This work considers the problem of intercepting rogue drones targeting\nsensitive critical infrastructure facilities. While current interception\ntechnologies focus mainly on the jamming/spoofing tasks, the challenges of\neffectively locating and tracking rogue drones have not received adequate\nattention. Solving this problem and integrating with recently proposed\ninterception techniques will enable a holistic system that can reliably detect,\ntrack, and neutralize rogue drones. Specifically, this work considers a team of\npursuer UAVs that can search, detect, and track multiple rogue drones over a\nsensitive facility. The joint search and track problem is addressed through a\nnovel multiagent reinforcement learning scheme to optimize the agent mobility\ncontrol actions that maximize the number of rogue drones detected and tracked.\nThe performance of the proposed system is investigated under realistic settings\nthrough extensive simulation experiments with varying number of agents\ndemonstrating both its performance and scalability.",
      "tldr_zh": "该研究针对拦截针对敏感关键基础设施的恶意无人机问题，强调了定位和跟踪的挑战，并提出了一种多智能体强化学习方案。方法涉及一队追逐无人机（pursuer UAVs）协同搜索、检测和跟踪多个恶意无人机，通过优化代理的移动控制动作来最大化检测效率。与现有技术集成后，该系统在广泛的模拟实验中展示了出色的性能和可扩展性，尤其在不同代理数量下的场景。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.10413v1",
      "published_date": "2025-01-07 16:22:51 UTC",
      "updated_date": "2025-01-07 16:22:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:07:52.336844"
    },
    {
      "arxiv_id": "2501.03904v1",
      "title": "Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study",
      "title_zh": "探索大型语言模型在公共交通中的潜力：圣安东尼奥案例研究",
      "authors": [
        "Ramya Jonnala",
        "Gongbo Liang",
        "Jeong Yang",
        "Izzat Alsmadi"
      ],
      "abstract": "The integration of large language models (LLMs) into public transit systems\npresents a transformative opportunity to enhance urban mobility. This study\nexplores the potential of LLMs to revolutionize public transportation\nmanagement within the context of San Antonio's transit system. Leveraging the\ncapabilities of LLMs in natural language processing and data analysis, we\ninvestigate their capabilities to optimize route planning, reduce wait times,\nand provide personalized travel assistance. By utilizing the General Transit\nFeed Specification (GTFS) and other relevant data, this research aims to\ndemonstrate how LLMs can potentially improve resource allocation, elevate\npassenger satisfaction, and inform data-driven decision-making in transit\noperations. A comparative analysis of different ChatGPT models was conducted to\nassess their ability to understand transportation information, retrieve\nrelevant data, and provide comprehensive responses. Findings from this study\nsuggest that while LLMs hold immense promise for public transit, careful\nengineering and fine-tuning are essential to realizing their full potential.\nSan Antonio serves as a case study to inform the development of LLM-powered\ntransit systems in other urban environments.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在公共交通系统中的应用潜力，以圣安东尼奥的交通系统作为案例研究。研究利用LLMs的自然语言处理和数据分析能力，结合General Transit Feed Specification (GTFS)数据，优化路线规划、减少等待时间并提供个性化旅行协助，从而改善资源分配和提升乘客满意度。针对不同ChatGPT模型进行的比较分析显示，LLMs在理解交通信息、检索数据和生成响应方面表现出色，但需通过精细工程和微调来充分发挥潜力。该案例为其他城市开发LLMs驱动的交通系统提供了宝贵参考。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "This work is accepted to AAAI 2025 Workshop on AI for Urban Planning.\n  arXiv admin note: substantial text overlap with arXiv:2407.11003",
      "pdf_url": "http://arxiv.org/pdf/2501.03904v1",
      "published_date": "2025-01-07 16:18:55 UTC",
      "updated_date": "2025-01-07 16:18:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:09:39.487310"
    },
    {
      "arxiv_id": "2501.03902v1",
      "title": "Explainable Reinforcement Learning via Temporal Policy Decomposition",
      "title_zh": "基于时间策略分解的可解释强化学习",
      "authors": [
        "Franco Ruggeri",
        "Alessio Russo",
        "Rafia Inam",
        "Karl Henrik Johansson"
      ],
      "abstract": "We investigate the explainability of Reinforcement Learning (RL) policies\nfrom a temporal perspective, focusing on the sequence of future outcomes\nassociated with individual actions. In RL, value functions compress information\nabout rewards collected across multiple trajectories and over an infinite\nhorizon, allowing a compact form of knowledge representation. However, this\ncompression obscures the temporal details inherent in sequential\ndecision-making, presenting a key challenge for interpretability. We present\nTemporal Policy Decomposition (TPD), a novel explainability approach that\nexplains individual RL actions in terms of their Expected Future Outcome (EFO).\nThese explanations decompose generalized value functions into a sequence of\nEFOs, one for each time step up to a prediction horizon of interest, revealing\ninsights into when specific outcomes are expected to occur. We leverage\nfixed-horizon temporal difference learning to devise an off-policy method for\nlearning EFOs for both optimal and suboptimal actions, enabling contrastive\nexplanations consisting of EFOs for different state-action pairs. Our\nexperiments demonstrate that TPD generates accurate explanations that (i)\nclarify the policy's future strategy and anticipated trajectory for a given\naction and (ii) improve understanding of the reward composition, facilitating\nfine-tuning of the reward function to align with human expectations.",
      "tldr_zh": "本文从时间视角探讨强化学习 (RL) 策略的可解释性，提出 Temporal Policy Decomposition (TPD) 方法，通过 Expected Future Outcome (EFO) 来分解动作的未来结果序列，从而揭示特定结果的发生时机。TPD 利用固定时域时间差分学习开发离线算法，生成最优和次优动作的对比解释，帮助理解策略的预期轨迹和奖励组成。实验结果显示，该方法准确提升了策略解释性，并支持微调奖励函数以更好地符合人类期望。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.03902v1",
      "published_date": "2025-01-07 16:10:09 UTC",
      "updated_date": "2025-01-07 16:10:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:09:52.659923"
    },
    {
      "arxiv_id": "2501.03895v2",
      "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token",
      "title_zh": "LLaVA-Mini：高效图像和视频大型多",
      "authors": [
        "Shaolei Zhang",
        "Qingkai Fang",
        "Zhe Yang",
        "Yang Feng"
      ],
      "abstract": "The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory.",
      "tldr_zh": "本文提出LLaVA-Mini，一种高效的大多模态模型（LMMs），旨在通过使用仅一个vision token来显著减少计算开销，同时支持图像、高分辨率图像和视频的理解。作者通过分析LMMs对vision tokens的处理，发现大多数tokens仅在LLM骨干的早期层融合视觉信息，因此引入modality pre-fusion技术提前将视觉信息融入文本tokens，实现vision tokens的极致压缩。实验结果显示，LLaVA-Mini在11个图像基准和7个视频基准上优于LLaVA-v1.5，将FLOPs减少77%，响应延迟低至40毫秒，并在24GB GPU内存上处理超过10,000帧视频。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICLR 2025. Code: https://github.com/ictnlp/LLaVA-Mini\n  Model: https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b",
      "pdf_url": "http://arxiv.org/pdf/2501.03895v2",
      "published_date": "2025-01-07 16:03:14 UTC",
      "updated_date": "2025-03-02 15:55:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:10:05.582008"
    },
    {
      "arxiv_id": "2501.05478v1",
      "title": "Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models",
      "title_zh": "翻译失败",
      "authors": [
        "Malak Mansour",
        "Ahmed Aly",
        "Bahey Tharwat",
        "Sarim Hashmi",
        "Dong An",
        "Ian Reid"
      ],
      "abstract": "Large Language Models (LLMs) such as GPT-4, trained on huge amount of\ndatasets spanning multiple domains, exhibit significant reasoning,\nunderstanding, and planning capabilities across various tasks. This study\npresents the first-ever work in Arabic language integration within the\nVision-and-Language Navigation (VLN) domain in robotics, an area that has been\nnotably underexplored in existing research. We perform a comprehensive\nevaluation of state-of-the-art multi-lingual Small Language Models (SLMs),\nincluding GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the\nArabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure\nLLM-based instruction-following navigation agent, to assess the impact of\nlanguage on navigation reasoning through zero-shot sequential action prediction\nusing the R2R dataset. Through comprehensive experiments, we demonstrate that\nour framework is capable of high-level planning for navigation tasks when\nprovided with instructions in both English and Arabic. However, certain models\nstruggled with reasoning and planning in the Arabic language due to inherent\nlimitations in their capabilities, sub-optimal performance, and parsing issues.\nThese findings highlight the importance of enhancing planning and reasoning\ncapabilities in language models for effective navigation, emphasizing this as a\nkey area for further development while also unlocking the potential of\nArabic-language models for impactful real-world applications.",
      "tldr_zh": "这篇论文首次将阿拉伯语整合到视觉和语言导航 (VLN) 领域，评估了大型语言模型 (LLMs) 和多语言小型语言模型 (SLMs) 如 GPT-4o mini、Llama 3 8B 和 Phi-3 medium 14B，以及阿拉伯语专属模型 Jais，在机器人导航任务中的推理和规划能力。研究采用 NavGPT 框架，通过零样本顺序动作预测在 R2R 数据集上测试英语和阿拉伯语指令，结果显示模型在英语中表现出色，但阿拉伯语指令下存在推理、规划和解析问题。论文强调需要提升语言模型的规划能力，以推动阿拉伯语模型在实际导航应用的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.05478v1",
      "published_date": "2025-01-07 16:01:25 UTC",
      "updated_date": "2025-01-07 16:01:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:10:17.176546"
    },
    {
      "arxiv_id": "2501.03888v4",
      "title": "Neural DNF-MT: A Neuro-symbolic Approach for Learning Interpretable and Editable Policies",
      "title_zh": "翻译失败",
      "authors": [
        "Kexin Gu Baugh",
        "Luke Dickens",
        "Alessandra Russo"
      ],
      "abstract": "Although deep reinforcement learning has been shown to be effective, the\nmodel's black-box nature presents barriers to direct policy interpretation. To\naddress this problem, we propose a neuro-symbolic approach called neural DNF-MT\nfor end-to-end policy learning. The differentiable nature of the neural DNF-MT\nmodel enables the use of deep actor-critic algorithms for training. At the same\ntime, its architecture is designed so that trained models can be directly\ntranslated into interpretable policies expressed as standard (bivalent or\nprobabilistic) logic programs. Moreover, additional layers can be included to\nextract abstract features from complex observations, acting as a form of\npredicate invention. The logic representations are highly interpretable, and we\nshow how the bivalent representations of deterministic policies can be edited\nand incorporated back into a neural model, facilitating manual intervention and\nadaptation of learned policies. We evaluate our approach on a range of tasks\nrequiring learning deterministic or stochastic behaviours from various forms of\nobservations. Our empirical results show that our neural DNF-MT model performs\nat the level of competing black-box methods whilst providing interpretable\npolicies.",
      "tldr_zh": "该研究提出了一种神经符号方法，neural DNF-MT，用于端到端政策学习，以解决深度强化学习的黑盒问题。该方法利用可微分架构结合deep actor-critic algorithms进行训练，并设计成可直接转化为可解释的逻辑程序，包括bivalent或probabilistic形式。同时，通过添加额外层实现predicate invention，并支持对确定性政策的编辑和整合回神经模型。实验结果表明，neural DNF-MT在各种任务中表现与黑盒方法相当，同时提供高度可解释的政策。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "AAMAS 2025 (with Appendix)",
      "pdf_url": "http://arxiv.org/pdf/2501.03888v4",
      "published_date": "2025-01-07 15:51:49 UTC",
      "updated_date": "2025-04-23 21:30:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:10:28.551784"
    },
    {
      "arxiv_id": "2501.03879v1",
      "title": "CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds Ratio on High-Resolution Point Clouds",
      "title_zh": "翻译失败",
      "authors": [
        "Keonwoo Kim",
        "Yeongjae Cho",
        "Taebaek Hwang",
        "Minsoo Jo",
        "Sangdo Han"
      ],
      "abstract": "Recent research has demonstrated that Large Language Models (LLMs) are not\nlimited to text-only tasks but can also function as multimodal models across\nvarious modalities, including audio, images, and videos. In particular,\nresearch on 3D Large Multimodal Models (3D LMMs) is making notable strides,\ndriven by the potential of processing higher-dimensional data like point\nclouds. However, upon closer examination, we find that the visual and textual\ncontent within each sample of existing training datasets lacks both high\ninformational granularity and clarity, which serve as a bottleneck for precise\ncross-modal understanding. To address these issues, we propose CL3DOR,\nContrastive Learning for 3D large multimodal models via Odds ratio on\nhigh-Resolution point clouds, designed to ensure greater specificity and\nclarity in both visual and textual content. Specifically, we increase the\ndensity of point clouds per object and construct informative hard negative\nresponses in the training dataset to penalize unwanted responses. To leverage\nhard negative responses, we incorporate the odds ratio as an auxiliary term for\ncontrastive learning into the conventional language modeling loss. CL3DOR\nachieves state-of-the-art performance in 3D scene understanding and reasoning\nbenchmarks. Additionally, we demonstrate the effectiveness of CL3DOR's key\ncomponents through extensive experiments.",
      "tldr_zh": "该论文发现现有3D Large Multimodal Models (3D LMMs) 训练数据集的视觉和文本内容缺乏高信息粒度和清晰度，限制了跨模态理解。提出CL3DOR方法，通过Contrastive Learning和Odds Ratio在High-Resolution Point Clouds上优化模型，具体包括增加点云密度、构建信息丰富的硬负样本，并将赔率比作为辅助项整合到语言建模损失中。实验结果显示，CL3DOR在3D场景理解和推理基准上达到最先进性能，并证明了其关键组件的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03879v1",
      "published_date": "2025-01-07 15:42:32 UTC",
      "updated_date": "2025-01-07 15:42:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:10:42.053830"
    },
    {
      "arxiv_id": "2501.03847v2",
      "title": "Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control",
      "title_zh": "翻译失败",
      "authors": [
        "Zekai Gu",
        "Rui Yan",
        "Jiahao Lu",
        "Peng Li",
        "Zhiyang Dou",
        "Chenyang Si",
        "Zhen Dong",
        "Qifeng Liu",
        "Cheng Lin",
        "Ziwei Liu",
        "Wenping Wang",
        "Yuan Liu"
      ],
      "abstract": "Diffusion models have demonstrated impressive performance in generating\nhigh-quality videos from text prompts or images. However, precise control over\nthe video generation process, such as camera manipulation or content editing,\nremains a significant challenge. Existing methods for controlled video\ngeneration are typically limited to a single control type, lacking the\nflexibility to handle diverse control demands. In this paper, we introduce\nDiffusion as Shader (DaS), a novel approach that supports multiple video\ncontrol tasks within a unified architecture. Our key insight is that achieving\nversatile video control necessitates leveraging 3D control signals, as videos\nare fundamentally 2D renderings of dynamic 3D content. Unlike prior methods\nlimited to 2D control signals, DaS leverages 3D tracking videos as control\ninputs, making the video diffusion process inherently 3D-aware. This innovation\nallows DaS to achieve a wide range of video controls by simply manipulating the\n3D tracking videos. A further advantage of using 3D tracking videos is their\nability to effectively link frames, significantly enhancing the temporal\nconsistency of the generated videos. With just 3 days of fine-tuning on 8 H800\nGPUs using less than 10k videos, DaS demonstrates strong control capabilities\nacross diverse tasks, including mesh-to-video generation, camera control,\nmotion transfer, and object manipulation.",
      "tldr_zh": "本文提出Diffusion as Shader (DaS)，一个统一的架构，用于实现3D-aware的视频扩散模型，支持多种视频生成控制任务，如相机操控、内容编辑和动作转移，以解决现有方法单一控制类型的局限性。DaS的关键创新在于使用3D跟踪视频作为输入，使视频生成过程具备3D感知能力，从而提升生成视频的时序一致性。实验结果显示，通过在8 H800 GPUs上仅用3天和少于10k视频的微调，DaS在网格到视频生成等任务上表现出色，显著提高了控制灵活性和性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://igl-hkust.github.io/das/ Codes:\n  https://github.com/IGL-HKUST/DiffusionAsShader",
      "pdf_url": "http://arxiv.org/pdf/2501.03847v2",
      "published_date": "2025-01-07 15:01:58 UTC",
      "updated_date": "2025-01-09 04:25:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:10:52.947936"
    },
    {
      "arxiv_id": "2501.04070v2",
      "title": "More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoqing Zhang",
        "Ang Lv",
        "Yuhan Liu",
        "Flood Sung",
        "Wei Liu",
        "Shuo Shang",
        "Xiuying Chen",
        "Rui Yan"
      ],
      "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL)\nwithout requiring parameter updates. However, as the number of ICL\ndemonstrations increases from a few to many, performance tends to plateau and\neventually decline. We identify two primary causes for this trend: the\nsuboptimal negative log-likelihood (NLL) optimization objective and the\nincremental data noise. To address these issues, we introduce DrICL, a novel\noptimization method that enhances model performance through Differentiated\nLearning and advantage-based Reweighting objectives. Globally, DrICL utilizes\ndifferentiated learning to optimize the NLL objective, ensuring that many-shot\nperformance surpasses zero-shot levels. Locally, it dynamically adjusts the\nweighting of many-shot demonstrations by leveraging cumulative advantages\ninspired by reinforcement learning, thereby improving generalization. This\napproach allows the model to handle varying numbers of shots effectively,\nmitigating the impact of noisy data. Recognizing the lack of multi-task\ndatasets with diverse many-shot distributions, we develop the Many-Shot ICL\nBenchmark (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers\nfrom 1 to 350 within sequences of up to 8,000 tokens-for fine-tuning purposes.\nICL-50 facilitates the evaluation of many-shot ICL strategies across seven\nprominent NLP tasks and 50 distinct datasets. Experimental results demonstrate\nthat LLMs enhanced with DrICL achieve significant improvements in many-shot\nsetups across various tasks, including both in-domain and out-of-domain\nscenarios. We release the code and benchmark dataset hoping to facilitate\nfurther research in many-shot ICL.",
      "tldr_zh": "本研究发现，大语言模型 (LLMs) 在多样本 In-Context Learning (ICL) 中，随着演示样本数量增加，性能会趋于平稳或下降，主要由于次优的负对数似然 (NLL) 优化目标和数据噪声问题。为解决此问题，作者提出 DrICL 方法，通过全局 Differentiated Learning 优化 NLL 以超越零样本水平，以及局部基于强化学习的累积优势动态调整样本权重，从而提升模型的泛化能力。研究还开发了 Many-Shot ICL Benchmark (ICL-50)，一个包含 50 个任务的基准，覆盖 1 到 350 个样本和序列长达 8,000 tokens，用于评估七个主要 NLP 任务。实验结果表明，DrICL 增强的 LLMs 在领域内和领域外场景下显著提高了多样本 ICL 性能，并开源了代码和数据集以促进进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 8 figures, 11 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.04070v2",
      "published_date": "2025-01-07 14:57:08 UTC",
      "updated_date": "2025-01-09 02:20:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:11:06.131012"
    },
    {
      "arxiv_id": "2501.03836v3",
      "title": "SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Runci Bai",
        "Guibao Xu",
        "Yanze Shi"
      ],
      "abstract": "Brain tumors can lead to neurological dysfunction, cognitive and\npsychological changes, increased intracranial pressure, and seizures, posing\nsignificant risks to health. The You Only Look Once (YOLO) series has shown\nsuperior accuracy in medical imaging object detection. This paper presents a\nnovel SCC-YOLO architecture that integrates the SCConv module into YOLOv9. The\nSCConv module optimizes convolutional efficiency by reducing spatial and\nchannel redundancy, enhancing image feature learning. We examine the effects of\ndifferent attention mechanisms with YOLOv9 for brain tumor detection using the\nBr35H dataset and our custom dataset (Brain_Tumor_Dataset). Results indicate\nthat SCC-YOLO improved mAP50 by 0.3% on the Br35H dataset and by 0.5% on our\ncustom dataset compared to YOLOv9. SCC-YOLO achieves state-of-the-art\nperformance in brain tumor detection.",
      "tldr_zh": "本研究提出了一种改进的对象检测器SCC-YOLO，用于辅助脑肿瘤诊断，通过将SCConv模块集成到YOLOv9中，以减少空间和通道冗余并提升图像特征学习。SCConv模块优化了卷积效率，使模型在脑肿瘤检测任务中表现更佳。实验在Br35H数据集和自定义Brain_Tumor_Dataset上进行，结果显示SCC-YOLO比YOLOv9提高了mAP50 0.3%和0.5%，并达到了脑肿瘤检测的最新状态性能。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03836v3",
      "published_date": "2025-01-07 14:45:39 UTC",
      "updated_date": "2025-03-02 06:41:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:11:16.405001"
    },
    {
      "arxiv_id": "2501.03835v2",
      "title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Yindu Su",
        "Huike Zou",
        "Lin Sun",
        "Ting Zhang",
        "Haiyang Yang",
        "Liyu Chen",
        "David Lo",
        "Qingheng Zhang",
        "Shuguang Han",
        "Jufeng Chen"
      ],
      "abstract": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendations, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity to the item embedding. It leverages contrastive training with\ntaxonomy-aware hard negative sampling and employs adaptive inference with\ndynamic thresholds. TACLR offers three key advantages: (1) it effectively\nhandles implicit and OOD values while producing normalized outputs; (2) it\nscales to thousands of categories, tens of thousands of attributes, and\nmillions of values; and (3) it supports efficient inference for high-load\nindustrial scenarios. Extensive experiments on proprietary and public datasets\nvalidate the effectiveness and efficiency of TACLR. Moreover, it has been\nsuccessfully deployed in a real-world e-commerce platform, processing millions\nof product listings daily while supporting dynamic, large-scale attribute\ntaxonomies.",
      "tldr_zh": "本文提出 TACLR，一种基于检索的工业产品属性值识别（PAVI）方法，旨在解决现有方法在推断隐式值、处理分布外（OOD）值以及生成规范输出方面的挑战。TACLR 将 PAVI 转化为信息检索任务，通过编码产品配置文件和候选值成嵌入向量，并利用 contrastive training 结合 taxonomy-aware hard negative sampling 和自适应推理进行训练和检索。该方法的关键优势包括有效处理隐式和 OOD 值、可扩展到数千类别、数万个属性和数百万值，以及支持高效推理以适应高负载工业场景。在专有和公共数据集上的广泛实验验证了 TACLR 的有效性和效率，并已在真实电商平台成功部署，每天处理数百万产品列表。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03835v2",
      "published_date": "2025-01-07 14:45:30 UTC",
      "updated_date": "2025-02-08 11:13:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:13:08.079780"
    },
    {
      "arxiv_id": "2501.03832v1",
      "title": "Three-dimensional attention Transformer for state evaluation in real-time strategy games",
      "title_zh": "三维注意力 Transformer 用于实时策略游戏中的状态评估",
      "authors": [
        "Yanqing Ye",
        "Weilong Yang",
        "Kai Qiu",
        "Jie Zhang"
      ],
      "abstract": "Situation assessment in Real-Time Strategy (RTS) games is crucial for\nunderstanding decision-making in complex adversarial environments. However,\nexisting methods remain limited in processing multi-dimensional feature\ninformation and temporal dependencies. Here we propose a tri-dimensional\nSpace-Time-Feature Transformer (TSTF Transformer) architecture, which\nefficiently models battlefield situations through three independent but\ncascaded modules: spatial attention, temporal attention, and feature attention.\nOn a dataset comprising 3,150 adversarial experiments, the 8-layer TSTF\nTransformer demonstrates superior performance: achieving 58.7% accuracy in the\nearly game (~4% progress), significantly outperforming the conventional\nTimesformer's 41.8%; reaching 97.6% accuracy in the mid-game (~40% progress)\nwhile maintaining low performance variation (standard deviation 0.114).\nMeanwhile, this architecture requires fewer parameters (4.75M) compared to the\nbaseline model (5.54M). Our study not only provides new insights into situation\nassessment in RTS games but also presents an innovative paradigm for\nTransformer-based multi-dimensional temporal modeling.",
      "tldr_zh": "本研究提出了一种三维注意Transformer（TSTF Transformer）架构，用于实时策略（RTS）游戏中的状态评估，以解决现有方法在处理多维特征信息和时间依赖性方面的局限性。该架构通过级联的空间注意（spatial attention）、时间注意（temporal attention）和特征注意（feature attention）模块，高效地建模战场情况。在包含3,150个对抗实验的数据集中，8层TSTF Transformer在早游戏阶段（~4%进度）实现58.7%的准确率，比基准Timesformer的41.8%高出显著优势，并在中游戏阶段（~40%进度）达到97.6%的准确率，同时参数量更低（4.75M vs. 5.54M）。这项工作为RTS游戏的情境评估提供了新见解，并为基于Transformer的多维时间建模引入了创新范式。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.03832v1",
      "published_date": "2025-01-07 14:42:38 UTC",
      "updated_date": "2025-01-07 14:42:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:13:19.920147"
    },
    {
      "arxiv_id": "2501.03825v1",
      "title": "Deep Sylvester Posterior Inference for Adaptive Compressed Sensing in Ultrasound Imaging",
      "title_zh": "深度 Sylvester 后验推断用于自适应压缩感知的超",
      "authors": [
        "Simon W. Penninga",
        "Hans van Gorp",
        "Ruud J. G. van Sloun"
      ],
      "abstract": "Ultrasound images are commonly formed by sequential acquisition of\nbeam-steered scan-lines. Minimizing the number of required scan-lines can\nsignificantly enhance frame rate, field of view, energy efficiency, and data\ntransfer speeds. Existing approaches typically use static subsampling schemes\nin combination with sparsity-based or, more recently, deep-learning-based\nrecovery. In this work, we introduce an adaptive subsampling method that\nmaximizes intrinsic information gain in-situ, employing a Sylvester Normalizing\nFlow encoder to infer an approximate Bayesian posterior under partial\nobservation in real-time. Using the Bayesian posterior and a deep generative\nmodel for future observations, we determine the subsampling scheme that\nmaximizes the mutual information between the subsampled observations, and the\nnext frame of the video. We evaluate our approach using the EchoNet cardiac\nultrasound video dataset and demonstrate that our active sampling method\noutperforms competitive baselines, including uniform and variable-density\nrandom sampling, as well as equidistantly spaced scan-lines, improving mean\nabsolute reconstruction error by 15%. Moreover, posterior inference and the\nsampling scheme generation are performed in just 0.015 seconds (66Hz), making\nit fast enough for real-time 2D ultrasound imaging applications.",
      "tldr_zh": "本文提出了一种自适应压缩感知方法，用于超声成像，旨在通过最小化beam-steered scan-lines的数量来提升帧率、视野和能效。方法利用Sylvester Normalizing Flow encoder实时推断部分观察下的近似Bayesian posterior，并结合deep generative model优化subsampling方案，以最大化subsampled observations与下一个视频帧之间的mutual information。实验在EchoNet cardiac ultrasound video数据集上显示，该方法比uniform sampling、variable-density random sampling和equidistantly spaced scan-lines等基线改善mean absolute reconstruction error 15%，且后验推断及sampling scheme生成仅需0.015秒（66Hz），适用于实时2D超声成像应用。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03825v1",
      "published_date": "2025-01-07 14:37:14 UTC",
      "updated_date": "2025-01-07 14:37:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:13:32.090735"
    },
    {
      "arxiv_id": "2501.03824v1",
      "title": "Online Reinforcement Learning-Based Dynamic Adaptive Evaluation Function for Real-Time Strategy Tasks",
      "title_zh": "基于在线强化学习的动态自适应评估函数，用于实时策略任务",
      "authors": [
        "Weilong Yang",
        "Jie Zhang",
        "Xunyun Liu",
        "Yanqing Ye"
      ],
      "abstract": "Effective evaluation of real-time strategy tasks requires adaptive mechanisms\nto cope with dynamic and unpredictable environments. This study proposes a\nmethod to improve evaluation functions for real-time responsiveness to\nbattle-field situation changes, utilizing an online reinforcement\nlearning-based dynam-ic weight adjustment mechanism within the real-time\nstrategy game. Building on traditional static evaluation functions, the method\nemploys gradient descent in online reinforcement learning to update weights\ndynamically, incorporating weight decay techniques to ensure stability.\nAdditionally, the AdamW optimizer is integrated to adjust the learning rate and\ndecay rate of online reinforcement learning in real time, further reducing the\ndependency on manual parameter tun-ing. Round-robin competition experiments\ndemonstrate that this method signifi-cantly enhances the application\neffectiveness of the Lanchester combat model evaluation function, Simple\nevaluation function, and Simple Sqrt evaluation function in planning algorithms\nincluding IDABCD, IDRTMinimax, and Port-folio AI. The method achieves a notable\nimprovement in scores, with the en-hancement becoming more pronounced as the\nmap size increases. Furthermore, the increase in evaluation function\ncomputation time induced by this method is kept below 6% for all evaluation\nfunctions and planning algorithms. The pro-posed dynamic adaptive evaluation\nfunction demonstrates a promising approach for real-time strategy task\nevaluation.",
      "tldr_zh": "这篇论文提出了一种基于在线强化学习（online reinforcement learning）的动态自适应评估函数，用于实时策略任务，以应对动态环境变化。方法通过梯度下降动态更新权重，并结合权重衰减和 AdamW 优化器实时调整学习率和衰减率，从而减少手动参数调整。实验结果显示，该方法显著提升了 Lanchester 战斗模型、Simple 评估函数和 Simple Sqrt 评估函数在 IDABCD、IDRTMinimax 和 Portfolio AI 等规划算法中的表现，成绩改善尤其在地图大小增加时更明显，且额外计算时间增加低于6%。这为实时策略任务评估提供了高效、可扩展的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.03824v1",
      "published_date": "2025-01-07 14:36:33 UTC",
      "updated_date": "2025-01-07 14:36:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:13:43.314597"
    },
    {
      "arxiv_id": "2501.03795v1",
      "title": "Self-Adaptive ERP: Embedding NLP into Petri-Net creation and Model Matching",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Maged",
        "Gamal Kassem"
      ],
      "abstract": "Enterprise Resource Planning (ERP) consultants play a vital role in\ncustomizing systems to meet specific business needs by processing large amounts\nof data and adapting functionalities. However, the process is\nresource-intensive, time-consuming, and requires continuous adjustments as\nbusiness demands evolve. This research introduces a Self-Adaptive ERP Framework\nthat automates customization using enterprise process models and system usage\nanalysis. It leverages Artificial Intelligence (AI) & Natural Language\nProcessing (NLP) for Petri nets to transform business processes into adaptable\nmodels, addressing both structural and functional matching. The framework,\nbuilt using Design Science Research (DSR) and a Systematic Literature Review\n(SLR), reduces reliance on manual adjustments, improving ERP customization\nefficiency and accuracy while minimizing the need for consultants.",
      "tldr_zh": "该研究针对企业资源规划(ERP)系统定制过程的资源密集和耗时问题，提出了一种Self-Adaptive ERP框架。该框架整合了人工智能(AI)和自然语言处理(NLP)技术，与Petri nets相结合，将业务流程转化为可适应模型，实现结构和功能匹配。基于Design Science Research(DSR)和Systematic Literature Review(SLR)的构建方法，该框架减少了对咨询师的依赖，提高了ERP定制的效率和准确性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03795v1",
      "published_date": "2025-01-07 14:01:59 UTC",
      "updated_date": "2025-01-07 14:01:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:13:54.290911"
    },
    {
      "arxiv_id": "2501.04068v1",
      "title": "Explainable Reinforcement Learning for Formula One Race Strategy",
      "title_zh": "可解释强化学习在一级方程式赛车策略中的应用",
      "authors": [
        "Devin Thomas",
        "Junqi Jiang",
        "Avinash Kori",
        "Aaron Russo",
        "Steffen Winkler",
        "Stuart Sale",
        "Joseph McMillan",
        "Francesco Belardinelli",
        "Antonio Rago"
      ],
      "abstract": "In Formula One, teams compete to develop their cars and achieve the highest\npossible finishing position in each race. During a race, however, teams are\nunable to alter the car, so they must improve their cars' finishing positions\nvia race strategy, i.e. optimising their selection of which tyre compounds to\nput on the car and when to do so. In this work, we introduce a reinforcement\nlearning model, RSRL (Race Strategy Reinforcement Learning), to control race\nstrategies in simulations, offering a faster alternative to the industry\nstandard of hard-coded and Monte Carlo-based race strategies. Controlling cars\nwith a pace equating to an expected finishing position of P5.5 (where P1\nrepresents first place and P20 is last place), RSRL achieves an average\nfinishing position of P5.33 on our test race, the 2023 Bahrain Grand Prix,\noutperforming the best baseline of P5.63. We then demonstrate, in a\ngeneralisability study, how performance for one track or multiple tracks can be\nprioritised via training. Further, we supplement model predictions with feature\nimportance, decision tree-based surrogate models, and decision tree\ncounterfactuals towards improving user trust in the model. Finally, we provide\nillustrations which exemplify our approach in real-world situations, drawing\nparallels between simulations and reality.",
      "tldr_zh": "这篇论文提出了一种可解释的强化学习模型 RSRL（Race Strategy Reinforcement Learning），用于优化一级方程式赛车（Formula One）的赛策略，包括轮胎选择和更换时间，以提升赛车的最终排名。相比传统硬编码和 Monte Carlo 方法，RSRL 在模拟中将预期的 P5.5 位置提升到平均 P5.33，在 2023 年巴林大奖赛测试中表现出色。论文通过特征重要性、决策树代理模型和决策树反事实等技术增强模型的可解释性，并探讨了模型在单一或多个赛道的泛化训练策略，以提高实际应用中的用户信任。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 6 figures. Copyright ACM 2025. This is the authors' version\n  of the work. It is posted here for your personal use. Not for redistribution.\n  The definitive Version of Record will be published in SAC 2025,\n  http://dx.doi.org/10.1145/3672608.3707766",
      "pdf_url": "http://arxiv.org/pdf/2501.04068v1",
      "published_date": "2025-01-07 13:54:19 UTC",
      "updated_date": "2025-01-07 13:54:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:14:08.007084"
    },
    {
      "arxiv_id": "2501.03764v1",
      "title": "SelectiveFinetuning: Enhancing Transfer Learning in Sleep Staging through Selective Domain Alignment",
      "title_zh": "SelectiveFinetuning：通过选择性领域对齐增强睡眠分期中的迁移",
      "authors": [
        "Siyuan Zhao",
        "Chenyu Liu",
        "Yi Ding",
        "Xinliang Zhou"
      ],
      "abstract": "In practical sleep stage classification, a key challenge is the variability\nof EEG data across different subjects and environments. Differences in\nphysiology, age, health status, and recording conditions can lead to domain\nshifts between data. These domain shifts often result in decreased model\naccuracy and reliability, particularly when the model is applied to new data\nwith characteristics different from those it was originally trained on, which\nis a typical manifestation of negative transfer. To address this, we propose\nSelectiveFinetuning in this paper. Our method utilizes a pretrained Multi\nResolution Convolutional Neural Network (MRCNN) to extract EEG features,\ncapturing the distinctive characteristics of different sleep stages. To\nmitigate the effect of domain shifts, we introduce a domain aligning mechanism\nthat employs Earth Mover Distance (EMD) to evaluate and select source domain\ndata closely matching the target domain. By finetuning the model with selective\nsource data, our SelectiveFinetuning enhances the model's performance on target\ndomain that exhibits domain shifts compared to the data used for training.\nExperimental results show that our method outperforms existing baselines,\noffering greater robustness and adaptability in practical scenarios where data\ndistributions are often unpredictable.",
      "tldr_zh": "该论文针对睡眠阶段分类中EEG数据领域的偏移问题（如生理差异和记录条件），提出了SelectiveFinetuning方法，以提升迁移学习的效果。该方法利用预训练的Multi Resolution Convolutional Neural Network (MRCNN)提取EEG特征，并通过Earth Mover Distance (EMD)评估并选择与目标域相似的源域数据进行微调，从而缓解负迁移的影响。实验结果显示，SelectiveFinetuning优于现有基线模型，提供更强的鲁棒性和适应性，尤其适用于数据分布不可预测的实际场景。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03764v1",
      "published_date": "2025-01-07 13:08:54 UTC",
      "updated_date": "2025-01-07 13:08:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:14:19.350857"
    },
    {
      "arxiv_id": "2501.06226v1",
      "title": "asanAI: In-Browser, No-Code, Offline-First Machine Learning Toolkit",
      "title_zh": "asanAI: 浏览器内、无代码、离线优先机器学习工具包",
      "authors": [
        "Norman Koch",
        "Siavash Ghiasvand"
      ],
      "abstract": "Machine learning (ML) has become crucial in modern life, with growing\ninterest from researchers and the public. Despite its potential, a significant\nentry barrier prevents widespread adoption, making it challenging for\nnon-experts to understand and implement ML techniques. The increasing desire to\nleverage ML is counterbalanced by its technical complexity, creating a gap\nbetween potential and practical application. This work introduces asanAI, an\noffline-first, open-source, no-code machine learning toolkit designed for users\nof all skill levels. It allows individuals to design, debug, train, and test ML\nmodels directly in a web browser, eliminating the need for software\ninstallations and coding. The toolkit runs on any device with a modern web\nbrowser, including smartphones, and ensures user privacy through local\ncomputations while utilizing WebGL for enhanced GPU performance. Users can\nquickly experiment with neural networks and train custom models using various\ndata sources, supported by intuitive visualizations of network structures and\ndata flows. asanAI simplifies the teaching of ML concepts in educational\nsettings and is released under an open-source MIT license, encouraging\nmodifications. It also supports exporting models in industry-ready formats,\nempowering a diverse range of users to effectively learn and apply machine\nlearning in their projects. The proposed toolkit is successfully utilized by\nresearchers of ScaDS.AI to swiftly draft and test machine learning ideas, by\ntrainers to effectively educate enthusiasts, and by teachers to introduce\ncontemporary ML topics in classrooms with minimal effort and high clarity.",
      "tldr_zh": "这篇论文介绍了 asanAI，一种无代码、离线优先的 Machine Learning 工具包，旨在降低 ML 的技术门槛，让非专家用户轻松设计、调试、训练和测试模型。asanAI 通过浏览器内运行，支持本地计算和 WebGL 加速，确保用户隐私和跨设备兼容（如智能手机），并提供直观的网络结构和数据流可视化。工具包开源（MIT 许可），已成功应用于研究、教育领域，帮助用户快速实验 ML 想法并导出行业标准模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.06226v1",
      "published_date": "2025-01-07 12:47:52 UTC",
      "updated_date": "2025-01-07 12:47:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:14:30.805950"
    },
    {
      "arxiv_id": "2501.04067v1",
      "title": "Explainable Time Series Prediction of Tyre Energy in Formula One Race Strategy",
      "title_zh": "翻译失败",
      "authors": [
        "Jamie Todd",
        "Junqi Jiang",
        "Aaron Russo",
        "Steffen Winkler",
        "Stuart Sale",
        "Joseph McMillan",
        "Antonio Rago"
      ],
      "abstract": "Formula One (F1) race strategy takes place in a high-pressure and fast-paced\nenvironment where split-second decisions can drastically affect race results.\nTwo of the core decisions of race strategy are when to make pit stops (i.e.\nreplace the cars' tyres) and which tyre compounds (hard, medium or soft, in\nnormal conditions) to select. The optimal pit stop decisions can be determined\nby estimating the tyre degradation of these compounds, which in turn can be\ncomputed from the energy applied to each tyre, i.e. the tyre energy. In this\nwork, we trained deep learning models, using the Mercedes-AMG PETRONAS F1\nteam's historic race data consisting of telemetry, to forecast tyre energies\nduring races. Additionally, we fitted XGBoost, a decision tree-based machine\nlearning algorithm, to the same dataset and compared the results, with both\ngiving impressive performance. Furthermore, we incorporated two different\nexplainable AI methods, namely feature importance and counterfactual\nexplanations, to gain insights into the reasoning behind the forecasts. Our\ncontributions thus result in an explainable, automated method which could\nassist F1 teams in optimising their race strategy.",
      "tldr_zh": "这篇论文针对一级方程式（F1）赛车赛策略中的轮胎能量（tyre energy）预测，旨在通过估计轮胎退化来优化进站换胎时间和胎面选择（硬、中或软）。研究者使用Mercedes-AMG PETRONAS F1团队的历史遥测数据训练深度学习模型和XGBoost算法进行时间序列预测，两者均表现出色。论文还整合了特征重要性（feature importance）和反事实解释（counterfactual explanations）等可解释AI方法，以揭示预测背后的原因。最终，该方法提供了一个可解释的自动化工具，帮助F1团队提升赛策略决策。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 9 figures. Copyright ACM 2025. This is the authors' version\n  of the work. It is posted here for your personal use. Not for redistribution.\n  The definitive Version of Record will be published in SAC 2025,\n  http://dx.doi.org/10.1145/3672608.3707765",
      "pdf_url": "http://arxiv.org/pdf/2501.04067v1",
      "published_date": "2025-01-07 12:38:48 UTC",
      "updated_date": "2025-01-07 12:38:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:14:44.143951"
    },
    {
      "arxiv_id": "2501.03722v1",
      "title": "Self-adaptive vision-language model for 3D segmentation of pulmonary artery and vein",
      "title_zh": "自适应视觉-语言模型用于肺动脉和静脉的3D分割",
      "authors": [
        "Xiaotong Guo",
        "Deqian Yang",
        "Dan Wang",
        "Haochen Zhao",
        "Yuan Li",
        "Zhilin Sui",
        "Tao Zhou",
        "Lijun Zhang",
        "Yanda Meng"
      ],
      "abstract": "Accurate segmentation of pulmonary structures iscrucial in clinical\ndiagnosis, disease study, and treatment planning. Significant progress has been\nmade in deep learning-based segmentation techniques, but most require much\nlabeled data for training. Consequently, developing precise segmentation\nmethods that demand fewer labeled datasets is paramount in medical image\nanalysis. The emergence of pre-trained vision-language foundation models, such\nas CLIP, recently opened the door for universal computer vision tasks.\nExploiting the generalization ability of these pre-trained foundation models on\ndownstream tasks, such as segmentation, leads to unexpected performance with a\nrelatively small amount of labeled data. However, exploring these models for\npulmonary artery-vein segmentation is still limited. This paper proposes a\nnovel framework called Language-guided self-adaptive Cross-Attention Fusion\nFramework. Our method adopts pre-trained CLIP as a strong feature extractor for\ngenerating the segmentation of 3D CT scans, while adaptively aggregating the\ncross-modality of text and image representations. We propose a s pecially\ndesigned adapter module to fine-tune pre-trained CLIP with a self-adaptive\nlearning strategy to effectively fuse the two modalities of embeddings. We\nextensively validate our method on a local dataset, which is the largest\npulmonary artery-vein CT dataset to date and consists of 718 labeled data in\ntotal. The experiments show that our method outperformed other state-of-the-art\nmethods by a large margin. Our data and code will be made publicly available\nupon acceptance.",
      "tldr_zh": "本研究针对肺动脉和静脉的3D分割问题，提出了一种自适应视觉语言模型，以减少对大量标注数据的依赖。方法采用预训练的CLIP模型作为特征提取器，并通过Language-guided self-adaptive Cross-Attention Fusion Framework自适应融合文本和图像表示，利用专设计适配器模块进行微调。实验在包含718个标注数据的最大肺动静脉CT数据集上验证，该框架大幅优于现有最先进方法，展示了其高效性和泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages,3 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.03722v1",
      "published_date": "2025-01-07 12:03:02 UTC",
      "updated_date": "2025-01-07 12:03:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:14:54.933935"
    },
    {
      "arxiv_id": "2501.03717v1",
      "title": "Materialist: Physically Based Editing Using Single-Image Inverse Rendering",
      "title_zh": "翻译失败",
      "authors": [
        "Lezhong Wang",
        "Duc Minh Tran",
        "Ruiqi Cui",
        "Thomson TG",
        "Manmohan Chandraker",
        "Jeppe Revall Frisvad"
      ],
      "abstract": "To perform image editing based on single-view, inverse physically based\nrendering, we present a method combining a learning-based approach with\nprogressive differentiable rendering. Given an image, our method leverages\nneural networks to predict initial material properties. Progressive\ndifferentiable rendering is then used to optimize the environment map and\nrefine the material properties with the goal of closely matching the rendered\nresult to the input image. We require only a single image while other inverse\nrendering methods based on the rendering equation require multiple views. In\ncomparison to single-view methods that rely on neural renderers, our approach\nachieves more realistic light material interactions, accurate shadows, and\nglobal illumination. Furthermore, with optimized material properties and\nillumination, our method enables a variety of tasks, including physically based\nmaterial editing, object insertion, and relighting. We also propose a method\nfor material transparency editing that operates effectively without requiring\nfull scene geometry. Compared with methods based on Stable Diffusion, our\napproach offers stronger interpretability and more realistic light refraction\nbased on empirical results.",
      "tldr_zh": "这篇论文介绍了Materialist，一种基于单张图像的反向物理渲染方法，结合学习-based approach和progressive differentiable rendering来预测和优化材质属性及环境映射，以精确匹配输入图像。相比依赖多视图或神经渲染的其他方法，该方法实现了更真实的灯光材质互动、准确阴影和全局照明，并支持物理-based材质编辑、对象插入和重照明等任务。论文还提出了一种无需完整场景几何的材质透明度编辑技术，并在与Stable Diffusion方法的比较中展示了更强的可解释性和真实的灯光折射效果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "code will be available at github.com/lez-s/Materialist",
      "pdf_url": "http://arxiv.org/pdf/2501.03717v1",
      "published_date": "2025-01-07 11:52:01 UTC",
      "updated_date": "2025-01-07 11:52:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:15:07.483981"
    },
    {
      "arxiv_id": "2501.03715v1",
      "title": "Neural Deconstruction Search for Vehicle Routing Problems",
      "title_zh": "翻译失败",
      "authors": [
        "André Hottung",
        "Paula Wong-Chung",
        "Kevin Tierney"
      ],
      "abstract": "Autoregressive construction approaches generate solutions to vehicle routing\nproblems in a step-by-step fashion, leading to high-quality solutions that are\nnearing the performance achieved by handcrafted, operations research\ntechniques. In this work, we challenge the conventional paradigm of sequential\nsolution construction and introduce an iterative search framework where\nsolutions are instead deconstructed by a neural policy. Throughout the search,\nthe neural policy collaborates with a simple greedy insertion algorithm to\nrebuild the deconstructed solutions. Our approach surpasses the performance of\nstate-of-the-art operations research methods across three challenging vehicle\nrouting problems of various problem sizes.",
      "tldr_zh": "本论文提出了一种名为 Neural Deconstruction Search 的框架，用于解决 Vehicle Routing Problems，通过迭代分解解决方案来挑战传统的自回归构建方法。该框架利用神经策略对现有解决方案进行 deconstruction，然后与一个简单的 greedy insertion algorithm 合作重建优化后的方案。实验结果显示，该方法在三种不同规模的挑战性车辆路径问题上，超过了最先进的操作研究技术的性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03715v1",
      "published_date": "2025-01-07 11:44:25 UTC",
      "updated_date": "2025-01-07 11:44:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:15:18.918518"
    },
    {
      "arxiv_id": "2501.03711v1",
      "title": "Unsupervised Speech Segmentation: A General Approach Using Speech Language Models",
      "title_zh": "无监督语音分割：使用语音语言模型的一种通用方法",
      "authors": [
        "Avishai Elmakies",
        "Omri Abend",
        "Yossi Adi"
      ],
      "abstract": "In this paper, we introduce an unsupervised approach for Speech Segmentation,\nwhich builds on previously researched approaches, e.g., Speaker Diarization,\nwhile being applicable to an inclusive set of acoustic-semantic distinctions,\npaving a path towards a general Unsupervised Speech Segmentation approach.\nUnlike traditional speech and audio segmentation, which mainly focuses on\nspectral changes in the input signal, e.g., phone segmentation, our approach\ntries to segment the spoken utterance into chunks with differing\nacoustic-semantic styles, focusing on acoustic-semantic information that does\nnot translate well into text, e.g., emotion or speaker. While most Speech\nSegmentation tasks only handle one style change, e.g., emotion diarization, our\napproach tries to handle multiple acoustic-semantic style changes. Leveraging\nrecent advances in Speech Language Models (SLMs), we propose a simple\nunsupervised method to segment a given speech utterance. We empirically\ndemonstrate the effectiveness of the proposed approach by considering several\nsetups. Results suggest that the proposed method is superior to the evaluated\nbaselines on boundary detection, segment purity, and over-segmentation. Code is\navailable at\nhttps://github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.",
      "tldr_zh": "本研究提出了一种通用的Unsupervised Speech Segmentation方法，利用Speech Language Models (SLMs)实现无监督语音分割。该方法扩展了传统方法（如Speaker Diarization），不仅关注光谱变化，还能处理多种声学-语义风格变化，例如情感或说话人，从而将语音分割成不同风格的块。相比基线模型，该方法在边界检测、段纯度和过度分割等方面表现出色，通过实验验证了其有效性。代码已在GitHub上公开。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03711v1",
      "published_date": "2025-01-07 11:32:13 UTC",
      "updated_date": "2025-01-07 11:32:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:15:31.024935"
    },
    {
      "arxiv_id": "2501.03700v1",
      "title": "AuxDepthNet: Real-Time Monocular 3D Object Detection with Depth-Sensitive Features",
      "title_zh": "翻译失败",
      "authors": [
        "Ruochen Zhang",
        "Hyeung-Sik Choi",
        "Dongwook Jung",
        "Phan Huy Nam Anh",
        "Sang-Ki Jeong",
        "Zihao Zhu"
      ],
      "abstract": "Monocular 3D object detection is a challenging task in autonomous systems due\nto the lack of explicit depth information in single-view images. Existing\nmethods often depend on external depth estimators or expensive sensors, which\nincrease computational complexity and hinder real-time performance. To overcome\nthese limitations, we propose AuxDepthNet, an efficient framework for real-time\nmonocular 3D object detection that eliminates the reliance on external depth\nmaps or pre-trained depth models. AuxDepthNet introduces two key components:\nthe Auxiliary Depth Feature (ADF) module, which implicitly learns\ndepth-sensitive features to improve spatial reasoning and computational\nefficiency, and the Depth Position Mapping (DPM) module, which embeds depth\npositional information directly into the detection process to enable accurate\nobject localization and 3D bounding box regression. Leveraging the DepthFusion\nTransformer architecture, AuxDepthNet globally integrates visual and\ndepth-sensitive features through depth-guided interactions, ensuring robust and\nefficient detection. Extensive experiments on the KITTI dataset show that\nAuxDepthNet achieves state-of-the-art performance, with $\\text{AP}_{3D}$ scores\nof 24.72\\% (Easy), 18.63\\% (Moderate), and 15.31\\% (Hard), and\n$\\text{AP}_{\\text{BEV}}$ scores of 34.11\\% (Easy), 25.18\\% (Moderate), and\n21.90\\% (Hard) at an IoU threshold of 0.7.",
      "tldr_zh": "该论文提出AuxDepthNet，一种高效的单目3D物体检测框架，旨在解决单视图图像中缺乏显式深度信息的问题，而无需依赖外部深度估计器或昂贵传感器。框架的关键组件包括Auxiliary Depth Feature (ADF) 模块，用于隐式学习深度敏感特征以提升空间推理效率，以及Depth Position Mapping (DPM) 模块和DepthFusion Transformer 架构，用于将深度位置信息嵌入检测过程，实现准确的物体定位和3D边界框回归。实验在KITTI数据集上显示，AuxDepthNet 达到最先进性能，$\\text{AP}_{3D}$ 分别为24.72% (Easy)、18.63% (Moderate) 和15.31% (Hard)，$\\text{AP}_{\\text{BEV}}$ 分别为34.11% (Easy)、25.18% (Moderate) 和21.90% (Hard)，在IoU阈值0.7下显著提升实时检测能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03700v1",
      "published_date": "2025-01-07 11:07:32 UTC",
      "updated_date": "2025-01-07 11:07:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:15:43.864964"
    },
    {
      "arxiv_id": "2501.03696v1",
      "title": "Exploring Molecule Generation Using Latent Space Graph Diffusion",
      "title_zh": "使用潜在空间图扩散探索分子生成",
      "authors": [
        "Prashanth Pombala",
        "Gerrit Grossmann",
        "Verena Wolf"
      ],
      "abstract": "Generating molecular graphs is a challenging task due to their discrete\nnature and the competitive objectives involved. Diffusion models have emerged\nas SOTA approaches in data generation across various modalities. For molecular\ngraphs, graph neural networks (GNNs) as a diffusion backbone have achieved\nimpressive results. Latent space diffusion, where diffusion occurs in a\nlow-dimensional space via an autoencoder, has demonstrated computational\nefficiency. However, the literature on latent space diffusion for molecular\ngraphs is scarce, and no commonly accepted best practices exist. In this work,\nwe explore different approaches and hyperparameters, contrasting generative\nflow models (denoising diffusion, flow matching, heat dissipation) and\narchitectures (GNNs and E(3)-equivariant GNNs). Our experiments reveal a high\nsensitivity to the choice of approach and design decisions. Code is made\navailable at\ngithub.com/Prashanth-Pombala/Molecule-Generation-using-Latent-Space-Graph-Diffusion.",
      "tldr_zh": "这篇论文探讨了使用潜在空间图扩散(latent space graph diffusion)生成分子图的挑战，强调了分子图的离散性和竞争目标。研究者比较了不同生成流模型(generative flow models)，包括denoising diffusion、flow matching和heat dissipation，以及架构如图神经网络(GNNs)和E(3)-equivariant GNNs。实验结果显示，这些方法的选取和设计决策对生成性能高度敏感，并提供了开源代码以促进进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03696v1",
      "published_date": "2025-01-07 10:54:44 UTC",
      "updated_date": "2025-01-07 10:54:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:15:55.419871"
    },
    {
      "arxiv_id": "2501.04062v1",
      "title": "ChronoLLM: A Framework for Customizing Large Language Model for Digital Twins generalization based on PyChrono",
      "title_zh": "翻译失败",
      "authors": [
        "Jingquan Wang",
        "Harry Zhang",
        "Khailanii Slaton",
        "Shu Wang",
        "Radu Serban",
        "Jinlong Wu",
        "Dan Negrut"
      ],
      "abstract": "Recently, the integration of advanced simulation technologies with artificial\nintelligence (AI) is revolutionizing science and engineering research.\nChronoLlama introduces a novel framework that customizes the open-source LLMs,\nspecifically for code generation, paired with PyChrono for multi-physics\nsimulations. This integration aims to automate and improve the creation of\nsimulation scripts, thus enhancing model accuracy and efficiency. This\ncombination harnesses the speed of AI-driven code generation with the\nreliability of physics-based simulations, providing a powerful tool for\nresearchers and engineers. Empirical results indicate substantial enhancements\nin simulation setup speed, accuracy of the generated codes, and overall\ncomputational efficiency. ChronoLlama not only expedites the development and\ntesting of multibody systems but also spearheads a scalable, AI-enhanced\napproach to managing intricate mechanical simulations. This pioneering\nintegration of cutting-edge AI with traditional simulation platforms represents\na significant leap forward in automating and optimizing design processes in\nengineering applications.",
      "tldr_zh": "该研究提出ChronoLLM框架，通过定制开源Large Language Model (LLMs)来生成代码，并与PyChrono多物理模拟工具集成，旨在提升数字孪生(Digital Twins)技术的泛化能力。该框架自动化模拟脚本的创建，提高了模型准确性和计算效率，结合AI驱动的快速代码生成与基于物理的可靠模拟。实验结果显示，模拟设置速度、生成的代码准确性和整体计算效率均显著提升，为多体系统的开发测试提供可扩展的AI增强方法，并在工程应用的自动化设计优化中实现重大进展。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04062v1",
      "published_date": "2025-01-07 10:39:14 UTC",
      "updated_date": "2025-01-07 10:39:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:16:07.363824"
    },
    {
      "arxiv_id": "2501.03689v1",
      "title": "MAJL: A Model-Agnostic Joint Learning Framework for Music Source Separation and Pitch Estimation",
      "title_zh": "MAJL：一种模型无关的联合学习框架，用于音乐源分离和音高估计",
      "authors": [
        "Haojie Wei",
        "Jun Yuan",
        "Rui Zhang",
        "Quanyu Dai",
        "Yueguo Chen"
      ],
      "abstract": "Music source separation and pitch estimation are two vital tasks in music\ninformation retrieval. Typically, the input of pitch estimation is obtained\nfrom the output of music source separation. Therefore, existing methods have\ntried to perform these two tasks simultaneously, so as to leverage the mutually\nbeneficial relationship between both tasks. However, these methods still face\ntwo critical challenges that limit the improvement of both tasks: the lack of\nlabeled data and joint learning optimization. To address these challenges, we\npropose a Model-Agnostic Joint Learning (MAJL) framework for both tasks. MAJL\nis a generic framework and can use variant models for each task. It includes a\ntwo-stage training method and a dynamic weighting method named Dynamic Weights\non Hard Samples (DWHS), which addresses the lack of labeled data and joint\nlearning optimization, respectively. Experimental results on public music\ndatasets show that MAJL outperforms state-of-the-art methods on both tasks,\nwith significant improvements of 0.92 in Signal-to-Distortion Ratio (SDR) for\nmusic source separation and 2.71% in Raw Pitch Accuracy (RPA) for pitch\nestimation. Furthermore, comprehensive studies not only validate the\neffectiveness of each component of MAJL, but also indicate the great generality\nof MAJL in adapting to different model architectures.",
      "tldr_zh": "这篇论文提出了 MAJL，一种模型无关的联合学习框架，用于音乐源分离和音高估计两个任务，以利用两者之间的互惠关系。框架包括两阶段训练方法和动态权重方法 (Dynamic Weights on Hard Samples, DWHS)，分别解决标注数据不足和联合优化挑战。实验结果显示，MAJL 在公共音乐数据集上显著提升了性能，音乐源分离的 Signal-to-Distortion Ratio (SDR) 提高了 0.92，音高估计的 Raw Pitch Accuracy (RPA) 提高了 2.71%。此外，该框架在不同模型架构上表现出良好的通用性和适应性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03689v1",
      "published_date": "2025-01-07 10:38:51 UTC",
      "updated_date": "2025-01-07 10:38:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:16:20.422729"
    },
    {
      "arxiv_id": "2501.03681v1",
      "title": "SLAM: Towards Efficient Multilingual Reasoning via Selective Language Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchun Fan",
        "Yongyu Mu",
        "Yilin Wang",
        "Lei Huang",
        "Junhao Ruan",
        "Bei Li",
        "Tong Xiao",
        "Shujian Huang",
        "Xiaocheng Feng",
        "Jingbo Zhu"
      ],
      "abstract": "Despite the significant improvements achieved by large language models (LLMs)\nin English reasoning tasks, these models continue to struggle with multilingual\nreasoning. Recent studies leverage a full-parameter and two-stage training\nparadigm to teach models to first understand non-English questions and then\nreason. However, this method suffers from both substantial computational\nresource computing and catastrophic forgetting. The fundamental cause is that,\nwith the primary goal of enhancing multilingual comprehension, an excessive\nnumber of irrelevant layers and parameters are tuned during the first stage.\nGiven our findings that the representation learning of languages is merely\nconducted in lower-level layers, we propose an efficient multilingual reasoning\nalignment approach that precisely identifies and fine-tunes the layers\nresponsible for handling multilingualism. Experimental results show that our\nmethod, SLAM, only tunes 6 layers' feed-forward sub-layers including 6.5-8% of\nall parameters within 7B and 13B LLMs, achieving superior average performance\nthan all strong baselines across 10 languages. Meanwhile, SLAM only involves\none training stage, reducing training time by 4.1-11.9 compared to the\ntwo-stage method.",
      "tldr_zh": "该研究针对大语言模型（LLMs）在多语言推理上的挑战，提出了一种高效方法 SLAM，通过选择性语言对齐（Selective Language Alignment）精确识别并微调处理多语言的层，仅调整 6 层的 Feed-Forward 子层，占所有参数的 6.5-8%。与传统两阶段全参数训练相比，SLAM 避免了计算资源浪费和灾难性遗忘，仅需一个训练阶段，将训练时间减少 4.1-11.9 倍。实验结果显示，SLAM 在 10 种语言上实现了优于强基线的平均性能，提升了多语言推理的效率和效果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by COLING 2025 (Oral)",
      "pdf_url": "http://arxiv.org/pdf/2501.03681v1",
      "published_date": "2025-01-07 10:29:43 UTC",
      "updated_date": "2025-01-07 10:29:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:16:31.268155"
    },
    {
      "arxiv_id": "2501.03676v2",
      "title": "SALE-Based Offline Reinforcement Learning with Ensemble Q-Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Zheng Chun"
      ],
      "abstract": "In this work, we build upon the offline reinforcement learning algorithm TD7,\nwhich incorporates State-Action Learned Embeddings (SALE) and a prioritized\nexperience replay buffer (LAP). We propose a model-free actor-critic algorithm\nthat integrates ensemble Q-networks and a gradient diversity penalty from EDAC.\nThe ensemble Q-networks introduce penalties to guide the actor network toward\nin-distribution actions, effectively addressing the challenge of\nout-of-distribution actions. Meanwhile, the gradient diversity penalty\nencourages diverse Q-value gradients, further suppressing overestimation for\nout-of-distribution actions. Additionally, our method retains an adjustable\nbehavior cloning (BC) term that directs the actor network toward dataset\nactions during early training stages, while gradually reducing its influence as\nthe precision of the Q-ensemble improves. These enhancements work\nsynergistically to improve the stability and precision of the training.\nExperimental results on the D4RL MuJoCo benchmarks demonstrate that our\nalgorithm achieves higher convergence speed, stability, and performance\ncompared to existing methods.",
      "tldr_zh": "本研究基于 TD7 算法，提出了一种整合 State-Action Learned Embeddings (SALE) 的离线强化学习方法，使用 ensemble Q-networks 和 gradient diversity penalty 来提升性能。ensemble Q-networks 通过引入惩罚引导 actor network 选择 in-distribution actions，同时 gradient diversity penalty 促进 Q-value gradients 的多样性，以抑制 out-of-distribution actions 的过估计；此外，还保留了一个可调整的 behavior cloning (BC) term，在训练早期引导 actor 向数据集 actions 靠拢，并随 Q-ensemble 精确度提升而逐渐减少其影响。这些改进协同提高了训练的稳定性和精确性。在 D4RL MuJoCo benchmarks 上，该算法显示出比现有方法更高的收敛速度、稳定性和整体性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T05, 90C40",
        "I.2.6; I.2.8"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 7 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.03676v2",
      "published_date": "2025-01-07 10:22:30 UTC",
      "updated_date": "2025-01-12 09:40:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:16:43.501818"
    },
    {
      "arxiv_id": "2501.03674v1",
      "title": "Action Quality Assessment via Hierarchical Pose-guided Multi-stage Contrastive Regression",
      "title_zh": "翻译失败",
      "authors": [
        "Mengshi Qi",
        "Hao Ye",
        "Jiaxuan Peng",
        "Huadong Ma"
      ],
      "abstract": "Action Quality Assessment (AQA), which aims at automatic and fair evaluation\nof athletic performance, has gained increasing attention in recent years.\nHowever, athletes are often in rapid movement and the corresponding visual\nappearance variances are subtle, making it challenging to capture fine-grained\npose differences and leading to poor estimation performance. Furthermore, most\ncommon AQA tasks, such as diving in sports, are usually divided into multiple\nsub-actions, each of which contains different durations. However, existing\nmethods focus on segmenting the video into fixed frames, which disrupts the\ntemporal continuity of sub-actions resulting in unavoidable prediction errors.\nTo address these challenges, we propose a novel action quality assessment\nmethod through hierarchically pose-guided multi-stage contrastive regression.\nFirstly, we introduce a multi-scale dynamic visual-skeleton encoder to capture\nfine-grained spatio-temporal visual and skeletal features. Then, a procedure\nsegmentation network is introduced to separate different sub-actions and obtain\nsegmented features. Afterwards, the segmented visual and skeletal features are\nboth fed into a multi-modal fusion module as physics structural priors, to\nguide the model in learning refined activity similarities and variances.\nFinally, a multi-stage contrastive learning regression approach is employed to\nlearn discriminative representations and output prediction results. In\naddition, we introduce a newly-annotated FineDiving-Pose Dataset to improve the\ncurrent low-quality human pose labels. In experiments, the results on\nFineDiving and MTL-AQA datasets demonstrate the effectiveness and superiority\nof our proposed approach. Our source code and dataset are available at\nhttps://github.com/Lumos0507/HP-MCoRe.",
      "tldr_zh": "该论文针对Action Quality Assessment (AQA)的问题，提出了一种分层姿势引导的多阶段对比回归方法，以捕捉运动员快速运动中的细粒度姿势差异，并解决子动作时序连续性中断的问题。该方法包括多尺度动态视觉-骨骼编码器用于提取时空特征、过程分割网络分离子动作、多模态融合模块整合视觉和骨骼信息作为物理结构先验，以及多阶段对比学习回归输出精确预测结果。此外，论文引入了新标注的FineDiving-Pose数据集，并在FineDiving和MTL-AQA数据集上实验证明了方法的优越性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03674v1",
      "published_date": "2025-01-07 10:20:16 UTC",
      "updated_date": "2025-01-07 10:20:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:16:55.839529"
    },
    {
      "arxiv_id": "2501.05476v1",
      "title": "IntegrityAI at GenAI Detection Task 2: Detecting Machine-Generated Academic Essays in English and Arabic Using ELECTRA and Stylometry",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad AL-Smadi"
      ],
      "abstract": "Recent research has investigated the problem of detecting machine-generated\nessays for academic purposes. To address this challenge, this research utilizes\npre-trained, transformer-based models fine-tuned on Arabic and English academic\nessays with stylometric features. Custom models based on ELECTRA for English\nand AraELECTRA for Arabic were trained and evaluated using a benchmark dataset.\nProposed models achieved excellent results with an F1-score of 99.7%, ranking\n2nd among of 26 teams in the English subtask, and 98.4%, finishing 1st out of\n23 teams in the Arabic one.",
      "tldr_zh": "该研究针对检测机器生成的学术论文问题，开发了基于 ELECTRA 和 AraELECTRA 的自定义模型，并结合 stylometry 特征，对英语和阿拉伯语论文进行微调和评估。模型使用基准数据集进行训练，在英语子任务中实现了99.7%的 F1-score，排名26支队伍中的第2；在阿拉伯语子任务中达到了98.4%的 F1-score，位列23支队伍的第1。主要贡献在于提升了跨语言机器生成文本的检测准确性，为学术诚信提供可靠工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.05476v1",
      "published_date": "2025-01-07 10:19:56 UTC",
      "updated_date": "2025-01-07 10:19:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:19:06.737465"
    },
    {
      "arxiv_id": "2501.03670v1",
      "title": "A Diversity-Enhanced Knowledge Distillation Model for Practical Math Word Problem Solving",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Zhang",
        "Guangyou Zhou",
        "Zhiwen Xie",
        "Jinjin Ma",
        "Jimmy Xiangji Huang"
      ],
      "abstract": "Math Word Problem (MWP) solving is a critical task in natural language\nprocessing, has garnered significant research interest in recent years. Various\nrecent studies heavily rely on Seq2Seq models and their extensions (e.g.,\nSeq2Tree and Graph2Tree) to generate mathematical equations. While effective,\nthese models struggle to generate diverse but counterpart solution equations,\nlimiting their generalization across various math problem scenarios. In this\npaper, we introduce a novel Diversity-enhanced Knowledge Distillation (DivKD)\nmodel for practical MWP solving. Our approach proposes an adaptive diversity\ndistillation method, in which a student model learns diverse equations by\nselectively transferring high-quality knowledge from a teacher model.\nAdditionally, we design a diversity prior-enhanced student model to better\ncapture the diversity distribution of equations by incorporating a conditional\nvariational auto-encoder. Extensive experiments on {four} MWP benchmark\ndatasets demonstrate that our approach achieves higher answer accuracy than\nstrong baselines while maintaining high efficiency for practical applications.",
      "tldr_zh": "本论文针对数学文字问题（MWP）解决中的多样性不足问题，提出了一种Diversity-enhanced Knowledge Distillation（DivKD）模型，以提升模型生成多样化方程的能力。DivKD 通过自适应多样性蒸馏方法，让学生模型从教师模型中选择性地学习高质量知识，同时整合条件变分自编码器（conditional variational auto-encoder）来增强学生模型对方程多样性分布的捕捉。实验结果显示，该模型在四个MWP基准数据集上比强基线实现了更高的答案准确率，同时保持了高效的实际应用性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03670v1",
      "published_date": "2025-01-07 10:18:22 UTC",
      "updated_date": "2025-01-07 10:18:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:19:17.702258"
    },
    {
      "arxiv_id": "2501.03643v2",
      "title": "Effective and Efficient Mixed Precision Quantization of Speech Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Haoning Xu",
        "Zhaoqing Li",
        "Zengrui Jin",
        "Huimeng Wang",
        "Youjun Chen",
        "Guinan Li",
        "Mengzhe Geng",
        "Shujie Hu",
        "Jiajun Deng",
        "Xunying Liu"
      ],
      "abstract": "This paper presents a novel mixed-precision quantization approach for speech\nfoundation models that tightly integrates mixed-precision learning and\nquantized model parameter estimation into one single model compression stage.\nExperiments conducted on LibriSpeech dataset with fine-tuned wav2vec2.0-base\nand HuBERT-large models suggest the resulting mixed-precision quantized models\nincreased the lossless compression ratio by factors up to 1.7x and 1.9x over\nthe respective uniform-precision and two-stage mixed-precision quantized\nbaselines that perform precision learning and model parameters quantization in\nseparate and disjointed stages, while incurring no statistically word error\nrate (WER) increase over the 32-bit full-precision models. The system\ncompression time of wav2vec2.0-base and HuBERT-large models is reduced by up to\n1.9 and 1.5 times over the two-stage mixed-precision baselines, while both\nproduce lower WERs. The best-performing 3.5-bit mixed-precision quantized\nHuBERT-large model produces a lossless compression ratio of 8.6x over the\n32-bit full-precision system.",
      "tldr_zh": "本论文提出了一种高效的混合精度量化（mixed-precision quantization）方法，将混合精度学习和量化模型参数估计整合到一个单一的模型压缩阶段，针对语音基础模型进行优化。\n在 LibriSpeech 数据集上实验表明，该方法使 fine-tuned wav2vec2.0-base 和 HuBERT-large 模型的无损压缩比分别比均匀精度和两阶段混合精度基线提高了高达 1.7x 和 1.9x，同时 word error rate (WER) 与 32-bit 全精度模型无显著差异。\n此外，该方法还减少了系统压缩时间（wav2vec2.0-base 减少 1.9 倍，HuBERT-large 减少 1.5 倍），最佳的 3.5-bit 混合精度量化 HuBERT-large 模型实现了 8.6x 的无损压缩比。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "To appear at IEEE ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03643v2",
      "published_date": "2025-01-07 09:21:52 UTC",
      "updated_date": "2025-01-11 06:24:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:19:31.159887"
    },
    {
      "arxiv_id": "2501.06224v3",
      "title": "Detection, Retrieval, and Explanation Unified: A Violence Detection System Based on Knowledge Graphs and GAT",
      "title_zh": "检测、检索和解释统一：基于知识图谱和图注意力网络的暴力检测系统",
      "authors": [
        "Wen-Dong Jiang",
        "Chih-Yung Chang",
        "Diptendu Sinha Roy"
      ],
      "abstract": "Recently, violence detection systems developed using unified multimodal\nmodels have achieved significant success and attracted widespread attention.\nHowever, most of these systems face two critical challenges: the lack of\ninterpretability as black-box models and limited functionality, offering only\nclassification or retrieval capabilities. To address these challenges, this\npaper proposes a novel interpretable violence detection system, termed the\nThree-in-One (TIO) System. The TIO system integrates knowledge graphs (KG) and\ngraph attention networks (GAT) to provide three core functionalities:\ndetection, retrieval, and explanation. Specifically, the system processes each\nvideo frame along with text descriptions generated by a large language model\n(LLM) for videos containing potential violent behavior. It employs ImageBind to\ngenerate high-dimensional embeddings for constructing a knowledge graph, uses\nGAT for reasoning, and applies lightweight time series modules to extract video\nembedding features. The final step connects a classifier and retriever for\nmulti-functional outputs. The interpretability of KG enables the system to\nverify the reasoning process behind each output. Additionally, the paper\nintroduces several lightweight methods to reduce the resource consumption of\nthe TIO system and enhance its efficiency. Extensive experiments conducted on\nthe XD-Violence and UCF-Crime datasets validate the effectiveness of the\nproposed system. A case study further reveals an intriguing phenomenon: as the\nnumber of bystanders increases, the occurrence of violent behavior tends to\ndecrease.",
      "tldr_zh": "本论文提出一个名为 Three-in-One (TIO) System 的暴力检测系统，利用 Knowledge Graphs (KG) 和 Graph Attention Networks (GAT) 统一实现检测、检索和解释三大功能，解决了现有模型的可解释性不足和功能局限问题。系统通过大语言模型 (LLM) 生成视频帧的文本描述，结合 ImageBind 生成高维嵌入构建 KG，并采用 GAT 进行推理以及轻量级时间序列模块提取特征，最终输出多功能结果。实验在 XD-Violence 和 UCF-Crime 数据集上验证了系统的有效性，并通过案例研究发现，随着旁观者数量增加，暴力行为发生倾向减少。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2501.06224v3",
      "published_date": "2025-01-07 09:21:20 UTC",
      "updated_date": "2025-02-06 04:58:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:20:35.010891"
    },
    {
      "arxiv_id": "2501.03635v1",
      "title": "MHGNet: Multi-Heterogeneous Graph Neural Network for Traffic Prediction",
      "title_zh": "MHGNet: 多异构图神经网络用于交通预测",
      "authors": [
        "Mei Wu",
        "Yiqian Lin",
        "Tianfan Jiang",
        "Wenchao Weng"
      ],
      "abstract": "In recent years, traffic flow prediction has played a crucial role in the\nmanagement of intelligent transportation systems. However, traditional\nforecasting methods often model non-Euclidean low-dimensional traffic data as a\nsimple graph with single-type nodes and edges, failing to capture similar\ntrends among nodes of the same type. To address this limitation, this paper\nproposes MHGNet, a novel framework for modeling spatiotemporal\nmulti-heterogeneous graphs. Within this framework, the STD Module decouples\nsingle-pattern traffic data into multi-pattern traffic data through feature\nmappings of timestamp embedding matrices and node embedding matrices.\nSubsequently, the Node Clusterer leverages the Euclidean distance between nodes\nand different types of limit points to perform clustering with O(N) time\ncomplexity. The nodes within each cluster undergo residual subgraph convolution\nwithin the spatiotemporal fusion subgraphs generated by the DSTGG Module,\nfollowed by processing in the SIE Module for node repositioning and\nredistribution of weights. To validate the effectiveness of MHGNet, this paper\nconducts extensive ablation studies and quantitative evaluations on four widely\nused benchmarks, demonstrating its superior performance.",
      "tldr_zh": "该论文提出 MHGNet，一种多异构图神经网络框架，用于解决传统交通预测方法在建模非欧空间数据时忽略同类型节点相似趋势的问题。MHGNet 通过 STD Module 将单模式交通数据解耦成多模式数据，利用时间戳和节点嵌入矩阵进行特征映射；Node Clusterer 以 O(N) 时间复杂度基于欧氏距离进行节点聚类；DSTGG Module 生成时空融合子图进行残差子图卷积，SIE Module 负责节点重新定位和权重重新分配。实验在四个常用基准数据集上进行了广泛的消融研究和定量评估，证明 MHGNet 比传统方法表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by 2025 lEEE International Conference on Acoustics, speech,\n  and signal Processing (lCASSP2025)",
      "pdf_url": "http://arxiv.org/pdf/2501.03635v1",
      "published_date": "2025-01-07 09:10:09 UTC",
      "updated_date": "2025-01-07 09:10:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:19:54.513831"
    },
    {
      "arxiv_id": "2501.05475v1",
      "title": "Retrieval-Augmented Generation by Evidence Retroactivity in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Liang Xiao",
        "Wen Dai",
        "Shuai Chen",
        "Bin Qin",
        "Chongyang Shi",
        "Haopeng Jing",
        "Tianyu Guo"
      ],
      "abstract": "Retrieval-augmented generation has gained significant attention due to its\nability to integrate relevant external knowledge, enhancing the accuracy and\nreliability of the LLMs' responses. Most of the existing methods apply a\ndynamic multiple retrieval-generating process, to address multi-hop complex\nquestions by decomposing them into sub-problems. However, these methods rely on\nan unidirectional forward reasoning paradigm, where errors from insufficient\nreasoning steps or inherent flaws in current retrieval systems are\nirreversible, potentially derailing the entire reasoning chain. For the first\ntime, this work introduces Retroactive Retrieval-Augmented Generation\n(RetroRAG), a novel framework to build a retroactive reasoning paradigm.\nRetroRAG revises and updates the evidence, redirecting the reasoning chain to\nthe correct direction. RetroRAG constructs an evidence-collation-discovery\nframework to search, generate, and refine credible evidence. It synthesizes\ninferential evidence related to the key entities in the question from the\nexisting source knowledge and formulates search queries to uncover additional\ninformation. As new evidence is found, RetroRAG continually updates and\norganizes this information, enhancing its ability to locate further necessary\nevidence. Paired with an Answerer to generate and evaluate outputs, RetroRAG is\ncapable of refining its reasoning process iteratively until a reliable answer\nis obtained. Empirical evaluations show that RetroRAG significantly outperforms\nexisting methods.",
      "tldr_zh": "本文提出 RetroRAG，一种新型逆向推理框架，用于提升大型语言模型(LLMs)的 Retrieval-Augmented Generation (RAG)，以解决现有方法在处理多跳复杂问题时因单向前向推理导致的不可逆错误。RetroRAG 通过证据收集-发现框架，从现有知识中合成推断证据、制定搜索查询，并持续更新和组织新证据，实现推理过程的迭代精炼。配以 Answerer 模块，该框架能够生成、评估输出，并反复优化直到获得可靠答案。实验评估显示，RetroRAG 在性能上显著优于现有方法，提升了响应准确性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.05475v1",
      "published_date": "2025-01-07 08:57:42 UTC",
      "updated_date": "2025-01-07 08:57:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:20:06.771757"
    },
    {
      "arxiv_id": "2501.05474v1",
      "title": "Modality-Invariant Bidirectional Temporal Representation Distillation Network for Missing Multimodal Sentiment Analysis",
      "title_zh": "用于缺失多模态情感分析的模态不变双向时间表示蒸馏网络",
      "authors": [
        "Xincheng Wang",
        "Liejun Wang",
        "Yinfeng Yu",
        "Xinxin Jiao"
      ],
      "abstract": "Multimodal Sentiment Analysis (MSA) integrates diverse modalities(text,\naudio, and video) to comprehensively analyze and understand individuals'\nemotional states. However, the real-world prevalence of incomplete data poses\nsignificant challenges to MSA, mainly due to the randomness of modality\nmissing. Moreover, the heterogeneity issue in multimodal data has yet to be\neffectively addressed. To tackle these challenges, we introduce the\nModality-Invariant Bidirectional Temporal Representation Distillation Network\n(MITR-DNet) for Missing Multimodal Sentiment Analysis. MITR-DNet employs a\ndistillation approach, wherein a complete modality teacher model guides a\nmissing modality student model, ensuring robustness in the presence of modality\nmissing. Simultaneously, we developed the Modality-Invariant Bidirectional\nTemporal Representation Learning Module (MIB-TRL) to mitigate heterogeneity.",
      "tldr_zh": "多模态情感分析（Multimodal Sentiment Analysis, MSA）在处理文本、音频和视频等模态时，常因模态缺失的随机性和数据异质性问题而面临挑战。论文提出 Modality-Invariant Bidirectional Temporal Representation Distillation Network (MITR-DNet)，通过知识蒸馏方法，让完整模态的教师模型指导缺失模态的学生模型，以提升分析的鲁棒性。同时，引入 Modality-Invariant Bidirectional Temporal Representation Learning Module (MIB-TRL) 来缓解多模态数据的异质性，确保更准确的情感状态理解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication by 2025 IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP 2025)",
      "pdf_url": "http://arxiv.org/pdf/2501.05474v1",
      "published_date": "2025-01-07 07:57:16 UTC",
      "updated_date": "2025-01-07 07:57:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:20:18.405983"
    },
    {
      "arxiv_id": "2501.03598v1",
      "title": "RecKG: Knowledge Graph for Recommender Systems",
      "title_zh": "RecKG：推荐系统的知识图谱",
      "authors": [
        "Junhyuk Kwon",
        "Seokho Ahn",
        "Young-Duk Seo"
      ],
      "abstract": "Knowledge graphs have proven successful in integrating heterogeneous data\nacross various domains. However, there remains a noticeable dearth of research\non their seamless integration among heterogeneous recommender systems, despite\nknowledge graph-based recommender systems garnering extensive research\nattention. This study aims to fill this gap by proposing RecKG, a standardized\nknowledge graph for recommender systems. RecKG ensures the consistent\nrepresentation of entities across different datasets, accommodating diverse\nattribute types for effective data integration. Through a meticulous\nexamination of various recommender system datasets, we select attributes for\nRecKG, ensuring standardized formatting through consistent naming conventions.\nBy these characteristics, RecKG can seamlessly integrate heterogeneous data\nsources, enabling the discovery of additional semantic information within the\nintegrated knowledge graph. We apply RecKG to standardize real-world datasets,\nsubsequently developing an application for RecKG using a graph database.\nFinally, we validate RecKG's achievement in interoperability through a\nqualitative evaluation between RecKG and other studies.",
      "tldr_zh": "这篇论文提出 RecKG，一种标准化知识图（Knowledge Graph），旨在解决推荐系统（Recommender Systems）中异构数据整合的不足问题。RecKG 通过一致的实体表示和属性标准化（如统一命名约定），从多种数据集选取属性，确保无缝整合不同数据源并发现额外语义信息。研究者应用 RecKG 于真实数据集，开发了基于图数据库的应用，并通过定性评估验证了其互操作性，提高了推荐系统的整体性能。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by The 39th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2501.03598v1",
      "published_date": "2025-01-07 07:55:35 UTC",
      "updated_date": "2025-01-07 07:55:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:20:29.647289"
    },
    {
      "arxiv_id": "2501.03583v1",
      "title": "STContext: A Multifaceted Dataset for Developing Context-aware Spatio-temporal Crowd Mobility Prediction Models",
      "title_zh": "翻译失败",
      "authors": [
        "Liyue Chen",
        "Jiangyi Fang",
        "Tengfei Liu",
        "Fangyuan Gao",
        "Leye Wang"
      ],
      "abstract": "In smart cities, context-aware spatio-temporal crowd flow prediction (STCFP)\nmodels leverage contextual features (e.g., weather) to identify unusual crowd\nmobility patterns and enhance prediction accuracy. However, the best practice\nfor incorporating contextual features remains unclear due to inconsistent usage\nof contextual features in different papers. Developing a multifaceted dataset\nwith rich types of contextual features and STCFP scenarios is crucial for\nestablishing a principled context modeling paradigm. Existing open crowd flow\ndatasets lack an adequate range of contextual features, which poses an urgent\nrequirement to build a multifaceted dataset to fill these research gaps. To\nthis end, we create STContext, a multifaceted dataset for developing\ncontext-aware STCFP models. Specifically, STContext provides nine\nspatio-temporal datasets across five STCFP scenarios and includes ten\ncontextual features, including weather, air quality index, holidays, points of\ninterest, road networks, etc. Besides, we propose a unified workflow for\nincorporating contextual features into deep STCFP methods, with steps including\nfeature transformation, dependency modeling, representation fusion, and\ntraining strategies. Through extensive experiments, we have obtained several\nuseful guidelines for effective context modeling and insights for future\nresearch. The STContext is open-sourced at\nhttps://github.com/Liyue-Chen/STContext.",
      "tldr_zh": "本论文针对智能城市中上下文感知时空人群流动预测（STCFP）模型的问题，创建了多面数据集 STContext，以解决现有数据集缺乏丰富上下文特征的不足。STContext 包括九个时空数据集、五个 STCFP 场景和十种上下文特征，如天气、空气质量指数、节假日、兴趣点和道路网络等。论文还提出了一种统一工作流程，涵盖 feature transformation、dependency modeling、representation fusion 和 training strategies，通过广泛实验得出有效的上下文建模指导和未来研究见解。该数据集已开源，可用于进一步研究。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03583v1",
      "published_date": "2025-01-07 07:16:56 UTC",
      "updated_date": "2025-01-07 07:16:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:20:42.950516"
    },
    {
      "arxiv_id": "2501.03575v2",
      "title": "Cosmos World Foundation Model Platform for Physical AI",
      "title_zh": "翻译失败",
      "authors": [
        "NVIDIA",
        ":",
        "Niket Agarwal",
        "Arslan Ali",
        "Maciej Bala",
        "Yogesh Balaji",
        "Erik Barker",
        "Tiffany Cai",
        "Prithvijit Chattopadhyay",
        "Yongxin Chen",
        "Yin Cui",
        "Yifan Ding",
        "Daniel Dworakowski",
        "Jiaojiao Fan",
        "Michele Fenzi",
        "Francesco Ferroni",
        "Sanja Fidler",
        "Dieter Fox",
        "Songwei Ge",
        "Yunhao Ge",
        "Jinwei Gu",
        "Siddharth Gururani",
        "Ethan He",
        "Jiahui Huang",
        "Jacob Huffman",
        "Pooya Jannaty",
        "Jingyi Jin",
        "Seung Wook Kim",
        "Gergely Klár",
        "Grace Lam",
        "Shiyi Lan",
        "Laura Leal-Taixe",
        "Anqi Li",
        "Zhaoshuo Li",
        "Chen-Hsuan Lin",
        "Tsung-Yi Lin",
        "Huan Ling",
        "Ming-Yu Liu",
        "Xian Liu",
        "Alice Luo",
        "Qianli Ma",
        "Hanzi Mao",
        "Kaichun Mo",
        "Arsalan Mousavian",
        "Seungjun Nah",
        "Sriharsha Niverty",
        "David Page",
        "Despoina Paschalidou",
        "Zeeshan Patel",
        "Lindsey Pavao",
        "Morteza Ramezanali",
        "Fitsum Reda",
        "Xiaowei Ren",
        "Vasanth Rao Naik Sabavat",
        "Ed Schmerling",
        "Stella Shi",
        "Bartosz Stefaniak",
        "Shitao Tang",
        "Lyne Tchapmi",
        "Przemek Tredak",
        "Wei-Cheng Tseng",
        "Jibin Varghese",
        "Hao Wang",
        "Haoxiang Wang",
        "Heng Wang",
        "Ting-Chun Wang",
        "Fangyin Wei",
        "Xinyue Wei",
        "Jay Zhangjie Wu",
        "Jiashu Xu",
        "Wei Yang",
        "Lin Yen-Chen",
        "Xiaohui Zeng",
        "Yu Zeng",
        "Jing Zhang",
        "Qinsheng Zhang",
        "Yuxuan Zhang",
        "Qingqing Zhao",
        "Artur Zolkowski"
      ],
      "abstract": "Physical AI needs to be trained digitally first. It needs a digital twin of\nitself, the policy model, and a digital twin of the world, the world model. In\nthis paper, we present the Cosmos World Foundation Model Platform to help\ndevelopers build customized world models for their Physical AI setups. We\nposition a world foundation model as a general-purpose world model that can be\nfine-tuned into customized world models for downstream applications. Our\nplatform covers a video curation pipeline, pre-trained world foundation models,\nexamples of post-training of pre-trained world foundation models, and video\ntokenizers. To help Physical AI builders solve the most critical problems of\nour society, we make Cosmos open-source and our models open-weight with\npermissive licenses available via\nhttps://github.com/nvidia-cosmos/cosmos-predict1.",
      "tldr_zh": "本文介绍了Cosmos World Foundation Model Platform，这是一个用于Physical AI的平台，帮助开发者构建定制的世界模型（world model），包括策略模型（policy model）的数字孪生（digital twin）。平台提供视频策划管道、预训练的世界基础模型、后训练示例和视频分词器，支持将通用模型微调为下游应用。Cosmos以开源形式发布，采用宽松许可，通过GitHub提供模型，旨在助力解决社会关键问题。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03575v2",
      "published_date": "2025-01-07 06:55:50 UTC",
      "updated_date": "2025-03-18 16:59:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:20:54.171689"
    },
    {
      "arxiv_id": "2501.03572v1",
      "title": "From Code to Compliance: Assessing ChatGPT's Utility in Designing an Accessible Webpage -- A Case Study",
      "title_zh": "从代码到合规：评估 ChatGPT 在设计无障碍网页方面的效用——一个案例研究",
      "authors": [
        "Ammar Ahmed",
        "Margarida Fresco",
        "Fredrik Forsberg",
        "Hallvard Grotli"
      ],
      "abstract": "Web accessibility ensures that individuals with disabilities can access and\ninteract with digital content without barriers, yet a significant majority of\nmost used websites fail to meet accessibility standards. This study evaluates\nChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web\nContent Accessibility Guidelines (WCAG). While ChatGPT can effectively address\naccessibility issues when prompted, its default code often lacks compliance,\nreflecting limitations in its training data and prevailing inaccessible web\npractices. Automated and manual testing revealed strengths in resolving simple\nissues but challenges with complex tasks, requiring human oversight and\nadditional iterations. Unlike prior studies, we incorporate manual evaluation,\ndynamic elements, and use the visual reasoning capability of ChatGPT along with\nthe prompts to fix accessibility issues. Providing screenshots alongside\nprompts enhances the LLM's ability to address accessibility issues by allowing\nit to analyze surrounding components, such as determining appropriate contrast\ncolors. We found that effective prompt engineering, such as providing concise,\nstructured feedback and incorporating visual aids, significantly enhances\nChatGPT's performance. These findings highlight the potential and limitations\nof large language models for accessible web development, offering practical\nguidance for developers to create more inclusive websites.",
      "tldr_zh": "本研究评估了 ChatGPT (GPT-4o) 在设计符合 Web Content Accessibility Guidelines (WCAG) 的网页方面的效用，通过一个案例研究发现其在提示下能有效解决简单可访问性问题，但默认代码往往不合规，反映了训练数据和现有网页实践的局限性。研究采用了自动化和手动测试方法，结合视觉推理能力、提示工程（如提供结构化反馈和截图）来处理动态元素和复杂任务，结果显示这种方法显著提升了 ChatGPT 的性能，但仍需人工监督和多次迭代。总体而言，该研究突出了 LLM 在可访问网页开发中的潜力与挑战，为开发者提供实用指导，以创建更具包容性的网站。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "D.1.2; F.3.1; F.4.1; D.3.2; H.1.2; H.5.2; D.2.2; H.1.2; I.3.6;\n  H.5.4; H.5.1"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03572v1",
      "published_date": "2025-01-07 06:51:46 UTC",
      "updated_date": "2025-01-07 06:51:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:21:06.420932"
    },
    {
      "arxiv_id": "2501.03566v1",
      "title": "Applying Large Language Models in Knowledge Graph-based Enterprise Modeling: Challenges and Opportunities",
      "title_zh": "翻译失败",
      "authors": [
        "Benedikt Reitemeyer",
        "Hans-Georg Fill"
      ],
      "abstract": "The role of large language models (LLMs) in enterprise modeling has recently\nstarted to shift from academic research to that of industrial applications.\nThereby, LLMs represent a further building block for the machine-supported\ngeneration of enterprise models. In this paper we employ a knowledge\ngraph-based approach for enterprise modeling and investigate the potential\nbenefits of LLMs in this context. In addition, the findings of an expert survey\nand ChatGPT-4o-based experiments demonstrate that LLM-based model generations\nexhibit minimal variability, yet remain constrained to specific tasks, with\nreliability declining for more intricate tasks. The survey results further\nsuggest that the supervision and intervention of human modeling experts are\nessential to ensure the accuracy and integrity of the generated models.",
      "tldr_zh": "该论文探讨了大型语言模型 (LLMs) 在基于知识图谱 (Knowledge Graph) 的企业建模中的应用，强调了其作为机器辅助生成企业模型的潜在构建块。研究通过专家调查和基于 ChatGPT-4o 的实验，评估了 LLMs 的益处，结果显示 LLM 生成的模型变异性较低，但仅适用于特定任务，复杂任务的可靠性显著下降。调查进一步指出，确保模型准确性和完整性需要人类建模专家的监督和干预。该研究为 LLMs 在企业建模领域的挑战和机会提供了宝贵见解。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03566v1",
      "published_date": "2025-01-07 06:34:17 UTC",
      "updated_date": "2025-01-07 06:34:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:21:18.274599"
    },
    {
      "arxiv_id": "2501.03562v2",
      "title": "Rethinking Adversarial Attacks in Reinforcement Learning from Policy Distribution Perspective",
      "title_zh": "从策略分布视角重新思考强化学习中的对抗攻击",
      "authors": [
        "Tianyang Duan",
        "Zongyuan Zhang",
        "Zheng Lin",
        "Yue Gao",
        "Ling Xiong",
        "Yong Cui",
        "Hongbin Liang",
        "Xianhao Chen",
        "Heming Cui",
        "Dong Huang"
      ],
      "abstract": "Deep Reinforcement Learning (DRL) suffers from uncertainties and inaccuracies\nin the observation signal in realworld applications. Adversarial attack is an\neffective method for evaluating the robustness of DRL agents. However, existing\nattack methods targeting individual sampled actions have limited impacts on the\noverall policy distribution, particularly in continuous action spaces. To\naddress these limitations, we propose the Distribution-Aware Projected Gradient\nDescent attack (DAPGD). DAPGD uses distribution similarity as the gradient\nperturbation input to attack the policy network, which leverages the entire\npolicy distribution rather than relying on individual samples. We utilize the\nBhattacharyya distance in DAPGD to measure policy similarity, enabling\nsensitive detection of subtle but critical differences between probability\ndistributions. Our experiment results demonstrate that DAPGD achieves SOTA\nresults compared to the baselines in three robot navigation tasks, achieving an\naverage 22.03% higher reward drop compared to the best baseline.",
      "tldr_zh": "本文重新审视了深度强化学习 (DRL) 中的对抗攻击问题，指出现有方法仅针对单个动作样本，导致对整体策略分布的影响有限，尤其在连续动作空间。作者提出了一种新方法 Distribution-Aware Projected Gradient Descent attack (DAPGD)，它使用策略分布相似性作为梯度扰动输入，从而更全面地攻击策略网络。DAPGD 采用 Bhattacharyya distance 来测量策略相似性，以敏感地检测概率分布间的细微差异。在三个机器人导航任务的实验中，DAPGD 比最佳基线方法平均导致 22.03% 更高的奖励下降，实现了 SOTA 性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 2 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.03562v2",
      "published_date": "2025-01-07 06:22:55 UTC",
      "updated_date": "2025-01-08 08:57:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:21:30.541217"
    },
    {
      "arxiv_id": "2501.03560v1",
      "title": "KG-TRICK: Unifying Textual and Relational Information Completion of Knowledge for Multilingual Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Zelin Zhou",
        "Simone Conia",
        "Daniel Lee",
        "Min Li",
        "Shenglei Huang",
        "Umar Farooq Minhas",
        "Saloni Potdar",
        "Henry Xiao",
        "Yunyao Li"
      ],
      "abstract": "Multilingual knowledge graphs (KGs) provide high-quality relational and\ntextual information for various NLP applications, but they are often\nincomplete, especially in non-English languages. Previous research has shown\nthat combining information from KGs in different languages aids either\nKnowledge Graph Completion (KGC), the task of predicting missing relations\nbetween entities, or Knowledge Graph Enhancement (KGE), the task of predicting\nmissing textual information for entities. Although previous efforts have\nconsidered KGC and KGE as independent tasks, we hypothesize that they are\ninterdependent and mutually beneficial. To this end, we introduce KG-TRICK, a\nnovel sequence-to-sequence framework that unifies the tasks of textual and\nrelational information completion for multilingual KGs. KG-TRICK demonstrates\nthat: i) it is possible to unify the tasks of KGC and KGE into a single\nframework, and ii) combining textual information from multiple languages is\nbeneficial to improve the completeness of a KG. As part of our contributions,\nwe also introduce WikiKGE10++, the largest manually-curated benchmark for\ntextual information completion of KGs, which features over 25,000 entities\nacross 10 diverse languages.",
      "tldr_zh": "本研究提出KG-TRICK，一种新型sequence-to-sequence框架，用于统一多语言知识图谱(Multilingual Knowledge Graphs)的文本信息补全(Knowledge Graph Enhancement, KGE)和关系信息补全(Knowledge Graph Completion, KGC)任务。KG-TRICK假设并证明KGC和KGE相互依赖，通过结合多语言文本信息来提升知识图谱的完整性，并在实验中展示了其有效性。作为贡献，该框架还引入了WikiKGE10++基准数据集，包含超过25,000个实体和10种语言，支持更全面的文本信息补全评估。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Camera ready for COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03560v1",
      "published_date": "2025-01-07 06:21:40 UTC",
      "updated_date": "2025-01-07 06:21:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:21:41.863991"
    },
    {
      "arxiv_id": "2501.03544v2",
      "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models",
      "title_zh": "翻译失败",
      "authors": [
        "Lingzhi Yuan",
        "Xiaojun Jia",
        "Yihao Huang",
        "Wei Dong",
        "Yang Liu"
      ],
      "abstract": "Text-to-image (T2I) models have been shown to be vulnerable to misuse,\nparticularly in generating not-safe-for-work (NSFW) content, raising serious\nethical concerns. In this work, we present PromptGuard, a novel content\nmoderation technique that draws inspiration from the system prompt mechanism in\nlarge language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack\na direct interface for enforcing behavioral guidelines. Our key idea is to\noptimize a safety soft prompt that functions as an implicit system prompt\nwithin the T2I model's textual embedding space. This universal soft prompt (P*)\ndirectly moderates NSFW inputs, enabling safe yet realistic image generation\nwithout altering the inference efficiency or requiring proxy models. Extensive\nexperiments across three datasets demonstrate that PromptGuard effectively\nmitigates NSFW content generation while preserving high-quality benign outputs.\nPromptGuard achieves 7.8 times faster than prior content moderation methods,\nsurpassing eight state-of-the-art defenses with an optimal unsafe ratio down to\n5.84%.",
      "tldr_zh": "该研究提出PromptGuard，一种基于soft prompt引导的内容审核技术，用于缓解Text-to-Image (T2I) 模型生成不安全（NSFW）内容的风险，灵感来源于大型语言模型（LLMs）的安全对齐系统提示机制。PromptGuard通过优化一个通用soft prompt（P*），在T2I模型的文本嵌入空间中充当隐式系统提示，从而直接调节NSFW输入，实现安全且高质量的图像生成，同时不影响推理效率或需要额外代理模型。在三个数据集上的实验显示，该方法比现有审核技术快7.8倍，并优于八种最先进防御，将unsafe ratio降低至5.84%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 8 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.03544v2",
      "published_date": "2025-01-07 05:39:21 UTC",
      "updated_date": "2025-04-04 05:56:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:23:54.451031"
    },
    {
      "arxiv_id": "2501.03540v1",
      "title": "Deep Learning within Tabular Data: Foundations, Challenges, Advances and Future Directions",
      "title_zh": "表格数据中的深度学习：基础、挑战、进展和未来方向",
      "authors": [
        "Weijieying Ren",
        "Tianxiang Zhao",
        "Yuqing Huang",
        "Vasant Honavar"
      ],
      "abstract": "Tabular data remains one of the most prevalent data types across a wide range\nof real-world applications, yet effective representation learning for this\ndomain poses unique challenges due to its irregular patterns, heterogeneous\nfeature distributions, and complex inter-column dependencies. This survey\nprovides a comprehensive review of state-of-the-art techniques in tabular data\nrepresentation learning, structured around three foundational design elements:\ntraining data, neural architectures, and learning objectives. Unlike prior\nsurveys that focus primarily on either architecture design or learning\nstrategies, we adopt a holistic perspective that emphasizes the universality\nand robustness of representation learning methods across diverse downstream\ntasks. We examine recent advances in data augmentation and generation,\nspecialized neural network architectures tailored to tabular data, and\ninnovative learning objectives that enhance representation quality.\nAdditionally, we highlight the growing influence of self-supervised learning\nand the adaptation of transformer-based foundation models for tabular data. Our\nreview is based on a systematic literature search using rigorous inclusion\ncriteria, encompassing 127 papers published since 2020 in top-tier conferences\nand journals. Through detailed analysis and comparison, we identify emerging\ntrends, critical gaps, and promising directions for future research, aiming to\nguide the development of more generalizable and effective tabular data\nrepresentation methods.",
      "tldr_zh": "这篇论文对表格数据（tabular data）中的深度学习表示学习进行了全面调查，探讨了其基础、挑战和最新进展，包括不规则模式、异质特征分布以及列间依赖性的独特难题。\n论文从训练数据、神经架构（neural architectures）和学习目标（learning objectives）三个核心元素入手，审查了数据增强和生成、专用神经网络架构、创新学习目标、自监督学习（self-supervised learning）以及Transformer-based基础模型的适应应用。\n基于对127篇自2020年以来顶级会议和期刊论文的系统分析，该研究识别了新兴趋势、关键空白，并为开发更通用和鲁棒的表格数据表示学习方法指出了未来方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03540v1",
      "published_date": "2025-01-07 05:23:36 UTC",
      "updated_date": "2025-01-07 05:23:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:24:06.615536"
    },
    {
      "arxiv_id": "2501.03535v2",
      "title": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive Querying for LLM-Based Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Xuewen Luo",
        "Fan Ding",
        "Fengze Yang",
        "Yang Zhou",
        "Junnyong Loo",
        "Hwa Hui Tew",
        "Chenxi Liu"
      ],
      "abstract": "This study addresses the critical need for enhanced situational awareness in\nautonomous driving (AD) by leveraging the contextual reasoning capabilities of\nlarge language models (LLMs). Unlike traditional perception systems that rely\non rigid, label-based annotations, it integrates real-time, multimodal sensor\ndata into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically\nunderstand and respond to complex driving environments. To overcome the\ninherent latency and modality limitations of LLMs, a proactive\nRetrieval-Augmented Generation (RAG) is designed for AD, combined with a\nchain-of-thought prompting mechanism, ensuring rapid and context-rich\nunderstanding. Experimental results using real-world Vehicle-to-everything\n(V2X) datasets demonstrate significant improvements in perception and\nprediction performance, highlighting the potential of this framework to enhance\nsafety, adaptability, and decision-making in next-generation AD systems.",
      "tldr_zh": "该研究针对自动驾驶 (AD) 的情境感知需求，提出 SenseRAG 框架，利用大型语言模型 (LLMs) 的上下文推理能力，将实时多模态传感器数据整合到一个统一的 LLMs 可读知识库中。框架通过主动检索增强生成 (RAG) 和链式思维提示机制，克服 LLMs 的延迟和模态限制，实现对复杂驾驶环境的动态理解和快速响应。在真实 Vehicle-to-everything (V2X) 数据集上的实验显示，该方法显著提升了感知和预测性能，从而提高下一代 AD 系统的安全、适应性和决策能力。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted for presentation at WACV Workshop LLMAD\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03535v2",
      "published_date": "2025-01-07 05:15:46 UTC",
      "updated_date": "2025-01-08 10:34:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:24:17.477141"
    },
    {
      "arxiv_id": "2502.00011v1",
      "title": "TOAST Framework: A Multidimensional Approach to Ethical and Sustainable AI Integration in Organizations",
      "title_zh": "翻译失败",
      "authors": [
        "Dian Tjondronegoro"
      ],
      "abstract": "Artificial Intelligence (AI) has emerged as a transformative technology with\nthe potential to revolutionize various sectors, from healthcare to finance,\neducation, and beyond. However, successfully implementing AI systems remains a\ncomplex challenge, requiring a comprehensive and methodologically sound\nframework. This paper contributes to this challenge by introducing the\nTrustworthy, Optimized, Adaptable, and Socio-Technologically harmonious (TOAST)\nframework. It draws on insights from various disciplines to align technical\nstrategy with ethical values, societal responsibilities, and innovation\naspirations. The TOAST framework is a novel approach designed to guide the\nimplementation of AI systems, focusing on reliability, accountability,\ntechnical advancement, adaptability, and socio-technical harmony. By grounding\nthe TOAST framework in healthcare case studies, this paper provides a robust\nevaluation of its practicality and theoretical soundness in addressing\noperational, ethical, and regulatory challenges in high-stakes environments,\ndemonstrating how adaptable AI systems can enhance institutional efficiency,\nmitigate risks like bias and data privacy, and offer a replicable model for\nother sectors requiring ethically aligned and efficient AI integration.",
      "tldr_zh": "本研究引入了 TOAST framework（Trustworthy, Optimized, Adaptable, and Socio-Technologically harmonious），一种多维度方法，用于指导组织在伦理和可持续性方面整合 AI 系统。该框架整合多学科见解，强调可靠性、问责性、技术进步、适应性和社会技术和谐，以平衡技术策略、伦理价值和社会责任。论文通过医疗保健案例研究评估了框架的实用性和理论可靠性，证明它能提升机构效率、缓解风险（如偏见和数据隐私问题），并提供可复制的模型适用于其他高风险领域。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "90-02",
        "K.6.m"
      ],
      "primary_category": "cs.CY",
      "comment": "25 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2502.00011v1",
      "published_date": "2025-01-07 05:13:39 UTC",
      "updated_date": "2025-01-07 05:13:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:22:29.381024"
    },
    {
      "arxiv_id": "2501.03523v1",
      "title": "Vocal Tract Length Warped Features for Spoken Keyword Spotting",
      "title_zh": "翻译失败",
      "authors": [
        "Achintya kr. Sarkar",
        "Priyanka Dwivedi",
        "Zheng-Hua Tan"
      ],
      "abstract": "In this paper, we propose several methods that incorporate vocal tract length\n(VTL) warped features for spoken keyword spotting (KWS). The first method,\nVTL-independent KWS, involves training a single deep neural network (DNN) that\nutilizes VTL features with various warping factors. During training, a specific\nVTL feature is randomly selected per epoch, allowing the exploration of VTL\nvariations. During testing, the VTL features with different warping factors of\na test utterance are scored against the DNN and combined with equal weight. In\nthe second method scores the conventional features of a test utterance (without\nVTL warping) against the DNN. The third method, VTL-concatenation KWS,\nconcatenates VTL warped features to form high-dimensional features for KWS.\nEvaluations carried out on the English Google Command dataset demonstrate that\nthe proposed methods improve the accuracy of KWS.",
      "tldr_zh": "这篇论文提出几种整合 Vocal Tract Length (VTL) 扭曲特征的方法，以提升 Spoken Keyword Spotting (KWS) 的性能。具体方法包括：VTL-independent KWS，通过训练单一 Deep Neural Network (DNN) 来处理各种 VTL 变化，并在测试时结合不同扭曲因子的得分；第二种方法直接对测试语音的常规特征（无 VTL 扭曲）进行评分；第三种方法为 VTL-concatenation KWS，将 VTL 扭曲特征连接成高维特征用于检测。在 English Google Command 数据集上的评估结果表明，这些方法显著提高了 KWS 的准确率。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03523v1",
      "published_date": "2025-01-07 04:38:28 UTC",
      "updated_date": "2025-01-07 04:38:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:24:43.164985"
    },
    {
      "arxiv_id": "2501.03499v1",
      "title": "Can Deep Learning Trigger Alerts from Mobile-Captured Images?",
      "title_zh": "深度学习能从手机捕获的图像中触发警报吗？",
      "authors": [
        "Pritisha Sarkar",
        "Duranta Durbaar Vishal Saha",
        "Mousumi Saha"
      ],
      "abstract": "Our research presents a comprehensive approach to leveraging mobile camera\nimage data for real-time air quality assessment and recommendation. We develop\na regression-based Convolutional Neural Network model and tailor it explicitly\nfor air quality prediction by exploiting the inherent relationship between\noutput parameters. As a result, the Mean Squared Error of 0.0077 and 0.0112\nobtained for 2 and 5 pollutants respectively outperforms existing models.\nFurthermore, we aim to verify the common practice of augmenting the original\ndataset with a view to introducing more variation in the training phase. It is\none of our most significant contributions that our experimental results\ndemonstrate minimal accuracy differences between the original and augmented\ndatasets. Finally, a real-time, user-friendly dashboard is implemented which\ndynamically displays the Air Quality Index and pollutant values derived from\ncaptured mobile camera images. Users' health conditions are considered to\nrecommend whether a location is suitable based on current air quality metrics.\nOverall, this research contributes to verification of data augmentation\ntechniques, CNN-based regression modelling for air quality prediction, and\nuser-centric air quality monitoring through mobile technology. The proposed\nsystem offers practical solutions for individuals to make informed\nenvironmental health and well-being decisions.",
      "tldr_zh": "本研究利用手机相机图像开发了一个基于回归的CNN模型，用于实时空气质量预测，通过利用输出参数之间的关系，实现了对2种和5种污染物的预测，MSE分别为0.0077和0.0112，优于现有模型。实验结果显示，数据增强技术对准确性影响最小，原数据集和增强数据集的性能差异不大。该系统还包括一个用户友好的实时仪表板，能动态显示空气质量指数(AQI)和污染物值，并根据用户健康状况提供位置推荐。整体贡献在于验证数据增强技术、改进CNN回归建模，以及推动用户导向的移动空气质量监测，帮助个体做出环境健康决策。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03499v1",
      "published_date": "2025-01-07 03:39:43 UTC",
      "updated_date": "2025-01-07 03:39:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:22:53.883386"
    },
    {
      "arxiv_id": "2501.03491v1",
      "title": "Can LLMs Design Good Questions Based on Context?",
      "title_zh": "LLMs 能否基于上下文设计出好的问题？",
      "authors": [
        "Yueheng Zhang",
        "Xiaoyuan Liu",
        "Yiyou Sun",
        "Atheer Alharbi",
        "Hend Alzahrani",
        "Basel Alomair",
        "Dawn Song"
      ],
      "abstract": "This paper evaluates questions generated by LLMs from context, comparing them\nto human-generated questions across six dimensions. We introduce an automated\nLLM-based evaluation method, focusing on aspects like question length, type,\ncontext coverage, and answerability. Our findings highlight unique\ncharacteristics of LLM-generated questions, contributing insights that can\nsupport further research in question quality and downstream applications.",
      "tldr_zh": "这篇论文评估了大型语言模型(LLMs)基于上下文生成的问题质量，与人类生成的问题在六个维度上进行比较。研究引入了一种自动化的LLM-based评估方法，重点考察问题长度、类型、上下文覆盖和可回答性等方面。结果显示，LLMs生成的问题具有独特特征，这些发现为提升问题质量和下游应用提供了宝贵见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03491v1",
      "published_date": "2025-01-07 03:21:17 UTC",
      "updated_date": "2025-01-07 03:21:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:25:06.355627"
    },
    {
      "arxiv_id": "2501.03486v1",
      "title": "Align-Pro: A Principled Approach to Prompt Optimization for LLM Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Prashant Trivedi",
        "Souradip Chakraborty",
        "Avinash Reddy",
        "Vaneet Aggarwal",
        "Amrit Singh Bedi",
        "George K. Atia"
      ],
      "abstract": "The alignment of large language models (LLMs) with human values is critical\nas these models become increasingly integrated into various societal and\ndecision-making processes. Traditional methods, such as reinforcement learning\nfrom human feedback (RLHF), achieve alignment by fine-tuning model parameters,\nbut these approaches are often computationally expensive and impractical when\nmodels are frozen or inaccessible for parameter modification. In contrast,\nprompt optimization is a viable alternative to RLHF for LLM alignment. While\nthe existing literature has shown empirical promise of prompt optimization, its\ntheoretical underpinning remains under-explored. We address this gap by\nformulating prompt optimization as an optimization problem and try to provide\ntheoretical insights into the optimality of such a framework. To analyze the\nperformance of the prompt optimization, we study theoretical suboptimality\nbounds and provide insights in terms of how prompt optimization depends upon\nthe given prompter and target model. We also provide empirical validation\nthrough experiments on various datasets, demonstrating that prompt optimization\ncan effectively align LLMs, even when parameter fine-tuning is not feasible.",
      "tldr_zh": "这篇论文提出 Align-Pro，一种基于原则的方法，通过提示优化（prompt optimization）来实现大型语言模型（LLMs）的对齐（alignment），以应对传统方法如 RLHF（Reinforcement Learning from Human Feedback）的计算开销和参数微调限制。作者将提示优化表述为一个优化问题，并提供理论分析，包括次优性边界（suboptimality bounds），探讨其对提示器和目标模型的依赖性。实验结果在各种数据集上验证了该方法的有效性，证明它能有效对齐 LLMs，即使模型参数不可访问。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages, Accepted in AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03486v1",
      "published_date": "2025-01-07 03:14:39 UTC",
      "updated_date": "2025-01-07 03:14:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:25:18.064193"
    },
    {
      "arxiv_id": "2501.03475v1",
      "title": "Reading with Intent -- Neutralizing Intent",
      "title_zh": "带有意图的阅读——中和意图",
      "authors": [
        "Benjamin Reichman",
        "Adar Avsian",
        "Larry Heck"
      ],
      "abstract": "Queries to large language models (LLMs) can be divided into two parts: the\ninstruction/question and the accompanying context. The context for\nretrieval-augmented generation (RAG) systems in most benchmarks comes from\nWikipedia or Wikipedia-like texts which are written in a neutral and factual\ntone. However, when RAG systems retrieve internet-based content, they encounter\ntext with diverse tones and linguistic styles, introducing challenges for\ndownstream tasks. The Reading with Intent task addresses this issue by\nevaluating how varying tones in context passages affect model performance.\nBuilding on prior work that focused on sarcasm, we extend this paradigm by\nconstructing a dataset where context passages are transformed to $11$ distinct\nemotions using a better synthetic data generation approach. Using this dataset,\nwe train an emotion translation model to systematically adapt passages to\nspecified emotional tones. The human evaluation shows that the LLM fine-tuned\nto become the emotion-translator benefited from the synthetically generated\ndata. Finally, the emotion-translator is used in the Reading with Intent task\nto transform the passages to a neutral tone. By neutralizing the passages, it\nmitigates the challenges posed by sarcastic passages and improves overall\nresults on this task by about $3\\%$.",
      "tldr_zh": "本研究探讨了检索增强生成（RAG）系统中，上下文语气多样性对大型语言模型（LLMs）性能的影响，引入“Reading with Intent”任务来评估这一问题。研究构建了一个数据集，将上下文段落转换为11种不同情绪，使用改进的合成数据生成方法训练了一个情绪翻译模型。人类评估显示，该模型通过合成数据提升了性能，最终通过将段落转换为中性语气，缓解了讽刺等挑战，并在任务中提高了约3%的整体结果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03475v1",
      "published_date": "2025-01-07 02:33:25 UTC",
      "updated_date": "2025-01-07 02:33:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:25:29.494008"
    },
    {
      "arxiv_id": "2501.04732v1",
      "title": "SNR-EQ-JSCC: Joint Source-Channel Coding with SNR-Based Embedding and Query",
      "title_zh": "翻译失败",
      "authors": [
        "Hongwei Zhang",
        "Meixia Tao"
      ],
      "abstract": "Coping with the impact of dynamic channels is a critical issue in joint\nsource-channel coding (JSCC)-based semantic communication systems. In this\npaper, we propose a lightweight channel-adaptive semantic coding architecture\ncalled SNR-EQ-JSCC. It is built upon the generic Transformer model and achieves\nchannel adaptation (CA) by Embedding the signal-to-noise ratio (SNR) into the\nattention blocks and dynamically adjusting attention scores through\nchannel-adaptive Queries. Meanwhile, penalty terms are introduced in the loss\nfunction to stabilize the training process. Considering that instantaneous SNR\nfeedback may be imperfect, we propose an alternative method that uses only the\naverage SNR, which requires no retraining of SNR-EQ-JSCC. Simulation results\nconducted on image transmission demonstrate that the proposed SNR-EQJSCC\noutperforms the state-of-the-art SwinJSCC in peak signal-to-noise ratio (PSNR)\nand perception metrics while only requiring 0.05% of the storage overhead and\n6.38% of the computational complexity for CA. Moreover, the channel-adaptive\nquery method demonstrates significant improvements in perception metrics. When\ninstantaneous SNR feedback is imperfect, SNR-EQ-JSCC using only the average SNR\nstill surpasses baseline schemes.",
      "tldr_zh": "该论文提出了一种轻量级通道自适应语义编码架构SNR-EQ-JSCC，基于Transformer模型，通过将信噪比(SNR)嵌入注意力块并动态调整通道自适应查询，实现对动态通道的适应，同时在损失函数中引入惩罚项以稳定训练过程。论文还提供了一种仅使用平均SNR的替代方法，无需重新训练，以应对即时SNR反馈不完美的场景。在图像传输模拟中，SNR-EQ-JSCC在峰值信噪比(PSNR)和感知指标上优于最先进方案SwinJSCC，同时存储开销仅为0.05%，计算复杂度为6.38%，并在不完美反馈下仍表现出色。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04732v1",
      "published_date": "2025-01-07 02:31:04 UTC",
      "updated_date": "2025-01-07 02:31:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:25:42.578646"
    },
    {
      "arxiv_id": "2501.03468v1",
      "title": "MTRAG: A Multi-Turn Conversational Benchmark for Evaluating Retrieval-Augmented Generation Systems",
      "title_zh": "MTRAG：用于评估检索增强生成系统的多轮对话基准测试",
      "authors": [
        "Yannis Katsis",
        "Sara Rosenthal",
        "Kshitij Fadnis",
        "Chulaka Gunasekara",
        "Young-Suk Lee",
        "Lucian Popa",
        "Vraj Shah",
        "Huaiyu Zhu",
        "Danish Contractor",
        "Marina Danilevsky"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has recently become a very popular task\nfor Large Language Models (LLMs). Evaluating them on multi-turn RAG\nconversations, where the system is asked to generate a response to a question\nin the context of a preceding conversation is an important and often overlooked\ntask with several additional challenges. We present MTRAG: an end-to-end\nhuman-generated multi-turn RAG benchmark that reflects several real-world\nproperties across diverse dimensions for evaluating the full RAG pipeline.\nMTRAG contains 110 conversations averaging 7.7 turns each across four domains\nfor a total of 842 tasks. We also explore automation paths via synthetic data\nand LLM-as-a-Judge evaluation. Our human and automatic evaluations show that\neven state-of-the-art LLM RAG systems struggle on MTRAG. We demonstrate the\nneed for strong retrieval and generation systems that can handle later turns,\nunanswerable questions, non-standalone questions, and multiple domains. MTRAG\nis available at https://github.com/ibm/mt-rag-benchmark.",
      "tldr_zh": "该研究引入了 MTRAG，这是一个端到端的多轮对话基准，用于评估 Retrieval-Augmented Generation (RAG) 系统在真实世界场景中的性能。MTRAG 由人类生成，包含 110 个对话、平均 7.7 轮次、跨越四个领域的 842 个任务，旨在测试系统处理后续轮次、无法回答的问题和非独立问题等挑战。研究探索了通过合成数据和 LLM-as-a-Judge 的自动化评估路径，结果显示即使是先进的 LLM RAG 系统在 MTRAG 上表现不佳，突显了需要更强的检索和生成能力来提升多领域对话效果。基准资源可从 https://github.com/ibm/mt-rag-benchmark 获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03468v1",
      "published_date": "2025-01-07 01:52:56 UTC",
      "updated_date": "2025-01-07 01:52:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:25:54.372059"
    },
    {
      "arxiv_id": "2501.03464v2",
      "title": "LHGNN: Local-Higher Order Graph Neural Networks For Audio Classification and Tagging",
      "title_zh": "LHGNN",
      "authors": [
        "Shubhr Singh",
        "Emmanouil Benetos",
        "Huy Phan",
        "Dan Stowell"
      ],
      "abstract": "Transformers have set new benchmarks in audio processing tasks, leveraging\nself-attention mechanisms to capture complex patterns and dependencies within\naudio data. However, their focus on pairwise interactions limits their ability\nto process the higher-order relations essential for identifying distinct audio\nobjects. To address this limitation, this work introduces the Local- Higher\nOrder Graph Neural Network (LHGNN), a graph based model that enhances feature\nunderstanding by integrating local neighbourhood information with higher-order\ndata from Fuzzy C-Means clusters, thereby capturing a broader spectrum of audio\nrelationships. Evaluation of the model on three publicly available audio\ndatasets shows that it outperforms Transformer-based models across all\nbenchmarks while operating with substantially fewer parameters. Moreover, LHGNN\ndemonstrates a distinct advantage in scenarios lacking ImageNet pretraining,\nestablishing its effectiveness and efficiency in environments where extensive\npretraining data is unavailable.",
      "tldr_zh": "这篇论文介绍了 LHGNN（Local-Higher Order Graph Neural Network），一种基于图的模型，用于音频分类和标记任务，以克服 Transformers 模型在处理更高阶音频关系时的局限性。LHGNN 通过整合局部邻域信息和 Fuzzy C-Means 集群的高阶数据，增强特征理解并捕捉更广泛的音频依赖关系。在三个公开音频数据集上的评估显示，LHGNN 超过了基于 Transformers 的基准模型，同时使用更少的参数，并在缺乏 ImageNet 预训练的场景中表现出显著优势，证明了其高效性和适用性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03464v2",
      "published_date": "2025-01-07 01:45:39 UTC",
      "updated_date": "2025-01-29 12:22:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:26:06.089894"
    },
    {
      "arxiv_id": "2501.03461v2",
      "title": "Radar Signal Recognition through Self-Supervised Learning and Domain Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Zi Huang",
        "Simon Denman",
        "Akila Pemasiri",
        "Clinton Fookes",
        "Terrence Martin"
      ],
      "abstract": "Automatic radar signal recognition (RSR) plays a pivotal role in electronic\nwarfare (EW), as accurately classifying radar signals is critical for informing\ndecision-making processes. Recent advances in deep learning have shown\nsignificant potential in improving RSR performance in domains with ample\nannotated data. However, these methods fall short in EW scenarios where\nannotated RF data are scarce or impractical to obtain. To address these\nchallenges, we introduce a self-supervised learning (SSL) method which utilises\nmasked signal modelling and RF domain adaption to enhance RSR performance in\nenvironments with limited RF samples and labels. Specifically, we investigate\npre-training masked autoencoders (MAE) on baseband in-phase and quadrature\n(I/Q) signals from various RF domains and subsequently transfer the learned\nrepresentation to the radar domain, where annotated data are limited. Empirical\nresults show that our lightweight self-supervised ResNet model with domain\nadaptation achieves up to a 17.5% improvement in 1-shot classification accuracy\nwhen pre-trained on in-domain signals (i.e., radar signals) and up to a 16.31%\nimprovement when pre-trained on out-of-domain signals (i.e., comm signals),\ncompared to its baseline without SSL. We also provide reference results for\nseveral MAE designs and pre-training strategies, establishing a new benchmark\nfor few-shot radar signal classification.",
      "tldr_zh": "这篇论文针对标注数据稀缺的电子战（EW）场景，提出了一种基于自监督学习（SSL）和域适应（Domain Adaptation）的雷达信号识别（RSR）方法，以提升分类性能。具体而言，该方法利用masked signal modelling在各种RF域（如baseband in-phase and quadrature (I/Q)信号）上预训练masked autoencoders (MAE)，然后将学得的表示转移到标注数据有限的雷达域。实验结果显示，自监督的轻量级ResNet模型在1-shot分类准确率上比基线提高了17.5%（in-domain信号）和16.31%（out-of-domain信号），并为few-shot雷达信号分类建立了新基准。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.03461v2",
      "published_date": "2025-01-07 01:35:56 UTC",
      "updated_date": "2025-01-14 04:53:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:26:18.466378"
    },
    {
      "arxiv_id": "2501.03458v1",
      "title": "Activating Associative Disease-Aware Vision Token Memory for LLM-Based X-ray Report Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Xiao Wang",
        "Fuling Wang",
        "Haowen Wang",
        "Bo Jiang",
        "Chuanfu Li",
        "Yaowei Wang",
        "Yonghong Tian",
        "Jin Tang"
      ],
      "abstract": "X-ray image based medical report generation achieves significant progress in\nrecent years with the help of the large language model, however, these models\nhave not fully exploited the effective information in visual image regions,\nresulting in reports that are linguistically sound but insufficient in\ndescribing key diseases. In this paper, we propose a novel associative\nmemory-enhanced X-ray report generation model that effectively mimics the\nprocess of professional doctors writing medical reports. It considers both the\nmining of global and local visual information and associates historical report\ninformation to better complete the writing of the current report. Specifically,\ngiven an X-ray image, we first utilize a classification model along with its\nactivation maps to accomplish the mining of visual regions highly associated\nwith diseases and the learning of disease query tokens. Then, we employ a\nvisual Hopfield network to establish memory associations for disease-related\ntokens, and a report Hopfield network to retrieve report memory information.\nThis process facilitates the generation of high-quality reports based on a\nlarge language model and achieves state-of-the-art performance on multiple\nbenchmark datasets, including the IU X-ray, MIMIC-CXR, and Chexpert Plus. The\nsource code of this work is released on\n\\url{https://github.com/Event-AHU/Medical_Image_Analysis}.",
      "tldr_zh": "该论文提出了一种新型关联记忆增强模型，用于基于大型语言模型(LLM)的X-ray图像报告生成，旨在解决现有模型未充分利用图像区域信息的问题，导致报告语言流畅但缺少关键疾病描述。该模型模仿专业医生流程，通过分类模型及其activation maps挖掘与疾病相关的视觉区域，并学习disease query tokens。随后，利用视觉Hopfield网络建立疾病相关标记的记忆关联，以及报告Hopfield网络检索历史报告信息，从而生成更准确全面的报告。实验结果显示，该方法在IU X-ray、MIMIC-CXR和Chexpert Plus等基准数据集上达到了state-of-the-art性能，并开源了代码。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "In Peer Review",
      "pdf_url": "http://arxiv.org/pdf/2501.03458v1",
      "published_date": "2025-01-07 01:19:48 UTC",
      "updated_date": "2025-01-07 01:19:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:26:29.771726"
    },
    {
      "arxiv_id": "2501.03443v1",
      "title": "Optimization Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Pascal Van Hentenryck"
      ],
      "abstract": "This article introduces the concept of optimization learning, a methodology\nto design optimization proxies that learn the input/output mapping of\nparametric optimization problems. These optimization proxies are trustworthy by\ndesign: they compute feasible solutions to the underlying optimization\nproblems, provide quality guarantees on the returned solutions, and scale to\nlarge instances. Optimization proxies are differentiable programs that combine\ntraditional deep learning technology with repair or completion layers to\nproduce feasible solutions. The article shows that optimization proxies can be\ntrained end-to-end in a self-supervised way. It presents methodologies to\nprovide performance guarantees and to scale optimization proxies to large-scale\noptimization problems. The potential of optimization proxies is highlighted\nthrough applications in power systems and, in particular, real-time risk\nassessment and security-constrained optimal power flow.",
      "tldr_zh": "本研究引入了 optimization learning 的概念，这是一种设计 optimization proxies 的方法，用于学习参数化优化问题的输入/输出映射。这些 proxies 通过结合传统 deep learning 技术与修复或完成层，生成可行解决方案，并提供质量保证，同时扩展到大规模实例。优化 proxies 可以采用自监督方式进行端到端训练，并通过特定方法确保性能可靠性。最后，该框架在电力系统应用中展现潜力，例如实时风险评估和安全约束 optimal power flow。",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03443v1",
      "published_date": "2025-01-07 00:09:52 UTC",
      "updated_date": "2025-01-07 00:09:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:26:41.290894"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 78,
  "processed_papers_count": 78,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T21:26:56.629808"
}
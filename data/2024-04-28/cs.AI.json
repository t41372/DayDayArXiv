{
  "date": "2024-04-28",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-04-28 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦 AI 和机器学习领域，特别是大型语言模型（LLM）的增量学习、多模态应用和解释性改进，令人印象深刻的包括 ICML 接受的视觉模型解释论文，以及 LLM 在专利和角色扮演中的创新应用；知名学者如 Dan Klein 和 Yike Guo 参与的相关工作值得关注。\n\n### 重点论文聚焦：LLM 和 AI 代理\n- **Towards Incremental Learning in Large Language Models (关于大型语言模型的增量学习的批判性综述)**  \n  这篇论文由 Mladjan Jovanovic 和 Peter Voss 撰写，系统综述了 LLM 的增量学习范式，如持续学习和元学习，强调了这些方法在处理动态数据时的优势和挑战。主要贡献是揭示现有方法未实现实时更新，并提出未来研究方向，帮助设计更适应的 LLM 系统。\n\n- **From Persona to Personalization: A Survey on Role-Playing Language Agents (从角色到个性化：角色扮演语言代理的调查)**  \n  作者包括 Yue Zhang 等，ICML 接受的综述，探讨了 LLM 在角色扮演中的应用，分类为人口统计、角色和个性化角色。核心发现是通过多代理架构，LLM 可以模拟历史人物或个性化服务，提升 AI 在游戏和助手中的实用性。\n\n- **PatentGPT: A Large Language Model for Intellectual Property (PatentGPT：用于知识产权的大型语言模型)**  \n  这篇由 Yike Guo 等领导的论文，构建了基于开源模型的 PatentGPT，针对 IP 领域优化。贡献在于通过 SMoE 架构处理长文本，并在专利考试中超越 GPT-4，展示了 LLM 在专业领域的可扩展性和实际应用潜力。\n\n- **WorldGPT: Empowering LLM as Multimodal World Model (WorldGPT：将 LLM 作为多模态世界模型的赋能)**  \n  作者如 Guangyu Xia 和 Yike Guo，提出 WorldGPT 框架，使用 LLM 模拟多模态环境。关键发现是通过数据生成管道，模型在 3D 任务中超越 GPT-4，适用于 AI 代理训练，并开源代码促进研究。\n\n### AI 解释性和安全相关\n- **SAFE-RL: Saliency-Aware Counterfactual Explainer for Deep Reinforcement Learning Policies (SAFE-RL：基于显著性的强化学习策略反事实解释器)**  \n  论文引入显著性图和生成模型，生成更可靠的反事实解释，提升 DRL 在自动驾驶等安全场景的可解释性。主要发现是模型在 Atari 和 Pacman 等环境中优于现有方法，并开源数据集。\n\n- **Position: Do Not Explain Vision Models Without Context (观点：不要在没有上下文的情况下解释视觉模型)**  \n  作者 Przemysław Biecek 等，ICML 2024 接受，强调视觉模型解释需考虑空间上下文。贡献在于展示传统 XAI 方法的失败案例，并提出从“哪里”转向“如何”的新研究方向，提升 AI 解释的真实性。\n\n- **Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (偏见中和框架：使用偏见智能商数评估 LLM 的公平性)**  \n  这篇工作引入 BiQ 指标，检测并缓解 LLM 中的种族偏见。核心发现是通过结合现有方法，框架在无人口统计数据下提升公平性，并在比较中显示 Latimer AI 优于 GPT-4。\n\n### 计算机视觉和多模态应用\n- **Paint by Inpaint: Learning to Add Image Objects by Removing Them First (通过补全学习添加图像对象)**  \n  论文提出逆向补全方法，使用生成模型从真实图像中添加对象。贡献在于构建新数据集和模型，提升图像编辑的准确性和自然性，适用于复杂文本提示。\n\n- **S$^2$Mamba: A Spatial-spectral State Space Model for Hyperspectral Image Classification (S$^2$Mamba：用于高光谱图像分类的空间-光谱状态空间模型)**  \n  作者设计了高效的 Mamba 模型，处理高光谱图像的空间和光谱特征。主要发现是模型在基准上超越 Transformer，复杂度线性降低，适合土地覆盖分析。\n\n其他论文如 IoT 安全检测、多模态追踪和推荐系统等，也涉及 AI 应用，但相对常规，我们快速掠过。例如，**Retrieval-Oriented Knowledge for Click-Through Rate Prediction (基于检索的点击率预测知识框架)** 提出 ROK 框架，提升推荐效率；**Mamba-FETrack: Frame-Event Tracking (Mamba-FETrack：帧-事件追踪)** 使用状态空间模型融合 RGB 和事件数据，提高追踪性能。这些工作虽有实际价值，但影响力不如上述重点论文。\n\n总之，今天的 arXiv 突显了 LLM 在解释性和多模态领域的突破，AI 社区可关注这些前沿进展以推动更可靠的应用。更多细节请查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2404.18328v1",
      "title": "Multi-stage Attack Detection and Prediction Using Graph Neural Networks: An IoT Feasibility Study",
      "title_zh": "翻译失败",
      "authors": [
        "Hamdi Friji",
        "Ioannis Mavromatis",
        "Adrian Sanchez-Mompo",
        "Pietro Carnelli",
        "Alexis Olivereau",
        "Aftab Khan"
      ],
      "abstract": "With the ever-increasing reliance on digital networks for various aspects of\nmodern life, ensuring their security has become a critical challenge. Intrusion\nDetection Systems play a crucial role in ensuring network security, actively\nidentifying and mitigating malicious behaviours. However, the relentless\nadvancement of cyber-threats has rendered traditional/classical approaches\ninsufficient in addressing the sophistication and complexity of attacks. This\npaper proposes a novel 3-stage intrusion detection system inspired by a\nsimplified version of the Lockheed Martin cyber kill chain to detect advanced\nmulti-step attacks. The proposed approach consists of three models, each\nresponsible for detecting a group of attacks with common characteristics. The\ndetection outcome of the first two stages is used to conduct a feasibility\nstudy on the possibility of predicting attacks in the third stage. Using the\nToN IoT dataset, we achieved an average of 94% F1-Score among different stages,\noutperforming the benchmark approaches based on Random-forest model. Finally,\nwe comment on the feasibility of this approach to be integrated in a real-world\nsystem and propose various possible future work.",
      "tldr_zh": "这篇论文提出了一种基于 Graph Neural Networks 的多阶段攻击检测和预测系统，针对 IoT 环境中的复杂网络威胁问题。该系统借鉴简化版 Lockheed Martin 网络杀伤链的设计，分为三个阶段，每个阶段负责检测一组具有共同特征的攻击，并利用前两阶段的检测结果来预测第三阶段的潜在攻击。使用 ToN IoT 数据集进行实验，该方法实现了平均 94% 的 F1-Score，优于基于 Random Forest 的基准模型。论文还讨论了该系统在真实世界部署的可行性，并建议了未来的研究方向。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18328v1",
      "published_date": "2024-04-28 22:11:24 UTC",
      "updated_date": "2024-04-28 22:11:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:14:34.961003"
    },
    {
      "arxiv_id": "2404.18326v1",
      "title": "SAFE-RL: Saliency-Aware Counterfactual Explainer for Deep Reinforcement Learning Policies",
      "title_zh": "翻译失败",
      "authors": [
        "Amir Samadi",
        "Konstantinos Koufos",
        "Kurt Debattista",
        "Mehrdad Dianati"
      ],
      "abstract": "While Deep Reinforcement Learning (DRL) has emerged as a promising solution\nfor intricate control tasks, the lack of explainability of the learned policies\nimpedes its uptake in safety-critical applications, such as automated driving\nsystems (ADS). Counterfactual (CF) explanations have recently gained prominence\nfor their ability to interpret black-box Deep Learning (DL) models. CF examples\nare associated with minimal changes in the input, resulting in a complementary\noutput by the DL model. Finding such alternations, particularly for\nhigh-dimensional visual inputs, poses significant challenges. Besides, the\ntemporal dependency introduced by the reliance of the DRL agent action on a\nhistory of past state observations further complicates the generation of CF\nexamples. To address these challenges, we propose using a saliency map to\nidentify the most influential input pixels across the sequence of past observed\nstates by the agent. Then, we feed this map to a deep generative model,\nenabling the generation of plausible CFs with constrained modifications centred\non the salient regions. We evaluate the effectiveness of our framework in\ndiverse domains, including ADS, Atari Pong, Pacman and space-invaders games,\nusing traditional performance metrics such as validity, proximity and sparsity.\nExperimental results demonstrate that this framework generates more informative\nand plausible CFs than the state-of-the-art for a wide range of environments\nand DRL agents. In order to foster research in this area, we have made our\ndatasets and codes publicly available at\nhttps://github.com/Amir-Samadi/SAFE-RL.",
      "tldr_zh": "该论文提出 SAFE-RL 框架，这是一种基于显著性感知（Saliency-Aware）的逆事实（Counterfactual）解释器，用于解释 Deep Reinforcement Learning (DRL) 策略的决策过程，以解决其在安全关键应用（如自动驾驶系统，ADS）中的解释性不足问题。框架通过 saliency map 识别输入序列中关键像素，并利用深度生成模型生成最小修改的逆事实（CF）示例，确保解释的合理性和准确性。实验在 ADS、Atari Pong、Pacman 和 Space-Invaders 等环境中验证了该方法，比现有技术在 validity、proximity 和 sparsity 等指标上表现出色，并公开了数据集和代码以推动相关研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18326v1",
      "published_date": "2024-04-28 21:47:34 UTC",
      "updated_date": "2024-04-28 21:47:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:14:46.737804"
    },
    {
      "arxiv_id": "2404.18316v3",
      "title": "Position: Do Not Explain Vision Models Without Context",
      "title_zh": "翻译失败",
      "authors": [
        "Paulina Tomaszewska",
        "Przemysław Biecek"
      ],
      "abstract": "Does the stethoscope in the picture make the adjacent person a doctor or a\npatient? This, of course, depends on the contextual relationship of the two\nobjects. If it's obvious, why don't explanation methods for vision models use\ncontextual information? In this paper, we (1) review the most popular methods\nof explaining computer vision models by pointing out that they do not take into\naccount context information, (2) show examples of failures of popular XAI\nmethods, (3) provide examples of real-world use cases where spatial context\nplays a significant role, (4) propose new research directions that may lead to\nbetter use of context information in explaining computer vision models, (5)\nargue that a change in approach to explanations is needed from 'where' to\n'how'.",
      "tldr_zh": "这篇论文强调，在解释计算机视觉模型时，必须考虑上下文信息，因为现有方法忽略了对象间的关系，导致解释不准确。论文回顾了流行 XAI 方法的不足，并通过失败例子和真实世界用例（如物体空间关系）展示了其局限性。最终，论文提出新研究方向，主张将解释方法从关注“where”（位置）转向“how”（关系），以提升计算机视觉模型的可解释性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at International Conference on Machine Learning (ICML) 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.18316v3",
      "published_date": "2024-04-28 20:57:55 UTC",
      "updated_date": "2024-06-02 20:57:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:14:56.600949"
    },
    {
      "arxiv_id": "2404.18311v4",
      "title": "Towards Incremental Learning in Large Language Models: A Critical Review",
      "title_zh": "翻译失败",
      "authors": [
        "Mladjan Jovanovic",
        "Peter Voss"
      ],
      "abstract": "Incremental learning is the ability of systems to acquire knowledge over\ntime, enabling their adaptation and generalization to novel tasks. It is a\ncritical ability for intelligent, real-world systems, especially when data\nchanges frequently or is limited. This review provides a comprehensive analysis\nof incremental learning in Large Language Models. It synthesizes the\nstate-of-the-art incremental learning paradigms, including continual learning,\nmeta-learning, parameter-efficient learning, and mixture-of-experts learning.\nWe demonstrate their utility for incremental learning by describing specific\nachievements from these related topics and their critical factors. An important\nfinding is that many of these approaches do not update the core model, and none\nof them update incrementally in real-time. The paper highlights current\nproblems and challenges for future research in the field. By consolidating the\nlatest relevant research developments, this review offers a comprehensive\nunderstanding of incremental learning and its implications for designing and\ndeveloping LLM-based learning systems.",
      "tldr_zh": "这篇评论论文审视了大型语言模型（Large Language Models, LLMs）中的增量学习（Incremental Learning），一种系统持续获取知识以适应新任务的能力。论文综合分析了当前先进范式，包括 continual learning、meta-learning、parameter-efficient learning 和 mixture-of-experts learning，并展示了这些方法在相关领域的具体成就和关键因素。重要发现是，大多数方法未更新核心模型，且尚未实现实时增量更新，导致当前存在数据变化频繁时的局限性。该评论强调了未来研究挑战，并为设计基于 LLMs 的学习系统提供了全面理解和指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18311v4",
      "published_date": "2024-04-28 20:44:53 UTC",
      "updated_date": "2024-05-05 08:46:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:15:07.727838"
    },
    {
      "arxiv_id": "2404.18304v2",
      "title": "Retrieval-Oriented Knowledge for Click-Through Rate Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Huanshuo Liu",
        "Bo Chen",
        "Menghui Zhu",
        "Jianghao Lin",
        "Jiarui Qin",
        "Yang Yang",
        "Hao Zhang",
        "Ruiming Tang"
      ],
      "abstract": "Click-through rate (CTR) prediction is crucial for personalized online\nservices. Sample-level retrieval-based models, such as RIM, have demonstrated\nremarkable performance. However, they face challenges including inference\ninefficiency and high resource consumption due to the retrieval process, which\nhinder their practical application in industrial settings. To address this, we\npropose a universal plug-and-play \\underline{r}etrieval-\\underline{o}riented\n\\underline{k}nowledge (\\textbf{\\name}) framework that bypasses the real\nretrieval process. The framework features a knowledge base that preserves and\nimitates the retrieved \\& aggregated representations using a\ndecomposition-reconstruction paradigm. Knowledge distillation and contrastive\nlearning optimize the knowledge base, enabling the integration of\nretrieval-enhanced representations with various CTR models. Experiments on\nthree large-scale datasets demonstrate \\name's exceptional compatibility and\nperformance, with the neural knowledge base serving as an effective surrogate\nfor the retrieval pool. \\name surpasses the teacher model while maintaining\nsuperior inference efficiency and demonstrates the feasibility of distilling\nknowledge from non-parametric methods using a parametric approach. These\nresults highlight \\name's strong potential for real-world applications and its\nability to transform retrieval-based methods into practical solutions. Our\nimplementation code is available to support reproducibility in\n\\url{https://github.com/HSLiu-Initial/ROK.git}.",
      "tldr_zh": "这篇论文针对点击率(CTR)预测任务，提出了一种通用的可插入式框架ROK（Retrieval-Oriented Knowledge），旨在解决现有样本级检索模型（如RIM）在推理效率和资源消耗方面的挑战。ROK框架通过构建一个知识库，使用分解-重构范式来保存和模仿检索后的表示，并结合知识蒸馏和对比学习优化，使其能与各种CTR模型无缝集成。实验结果显示，在三个大规模数据集上，ROK不仅超过了基准模型，还显著提高了推理效率，并证明了从非参数方法中通过参数方式提炼知识的可行性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "11 pages, 6 figures, 6 tables.Accepted by CIKM'24",
      "pdf_url": "http://arxiv.org/pdf/2404.18304v2",
      "published_date": "2024-04-28 20:21:03 UTC",
      "updated_date": "2024-10-03 20:14:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:15:21.040818"
    },
    {
      "arxiv_id": "2404.18296v1",
      "title": "Using Deep Q-Learning to Dynamically Toggle between Push/Pull Actions in Computational Trust Mechanisms",
      "title_zh": "翻译失败",
      "authors": [
        "Zoi Lygizou",
        "Dimitris Kalles"
      ],
      "abstract": "Recent work on decentralized computational trust models for open Multi Agent\nSystems has resulted in the development of CA, a biologically inspired model\nwhich focuses on the trustee's perspective. This new model addresses a serious\nunresolved problem in existing trust and reputation models, namely the\ninability to handle constantly changing behaviors and agents' continuous entry\nand exit from the system. In previous work, we compared CA to FIRE, a\nwell-known trust and reputation model, and found that CA is superior when the\ntrustor population changes, whereas FIRE is more resilient to the trustee\npopulation changes. Thus, in this paper, we investigate how the trustors can\ndetect the presence of several dynamic factors in their environment and then\ndecide which trust model to employ in order to maximize utility. We frame this\nproblem as a machine learning problem in a partially observable environment,\nwhere the presence of several dynamic factors is not known to the trustor and\nwe describe how an adaptable trustor can rely on a few measurable features so\nas to assess the current state of the environment and then use Deep Q Learning\n(DQN), in a single-agent Reinforcement Learning setting, to learn how to adapt\nto a changing environment. We ran a series of simulation experiments to compare\nthe performance of the adaptable trustor with the performance of trustors using\nonly one model (FIRE or CA) and we show that an adaptable agent is indeed\ncapable of learning when to use each model and, thus, perform consistently in\ndynamic environments.",
      "tldr_zh": "本文提出了一种使用 Deep Q Learning (DQN) 的方法，让信任者在计算信任机制中动态切换 CA 和 FIRE 模型，以应对多代理系统中的环境变化。CA 模型专注于受托人视角并处理代理行为波动，而 FIRE 模型更适合受托人群体变化；通过强化学习框架，信任者基于部分可观察环境的 measurable features 评估状态并学习何时切换模型。实验结果显示，可适应信任者比仅使用单一模型的代理在动态环境中表现出色，能更一致地最大化效用。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18296v1",
      "published_date": "2024-04-28 19:44:56 UTC",
      "updated_date": "2024-04-28 19:44:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:15:32.830297"
    },
    {
      "arxiv_id": "2404.18291v1",
      "title": "Panoptic Segmentation and Labelling of Lumbar Spine Vertebrae using Modified Attention Unet",
      "title_zh": "翻译失败",
      "authors": [
        "Rikathi Pal",
        "Priya Saha",
        "Somoballi Ghoshal",
        "Amlan Chakrabarti",
        "Susmita Sur-Kolay"
      ],
      "abstract": "Segmentation and labeling of vertebrae in MRI images of the spine are\ncritical for the diagnosis of illnesses and abnormalities. These steps are\nindispensable as MRI technology provides detailed information about the tissue\nstructure of the spine. Both supervised and unsupervised segmentation methods\nexist, yet acquiring sufficient data remains challenging for achieving high\naccuracy. In this study, we propose an enhancing approach based on modified\nattention U-Net architecture for panoptic segmentation of 3D sliced MRI data of\nthe lumbar spine. Our method achieves an impressive accuracy of 99.5\\% by\nincorporating novel masking logic, thus significantly advancing the\nstate-of-the-art in vertebral segmentation and labeling. This contributes to\nmore precise and reliable diagnosis and treatment planning.",
      "tldr_zh": "该研究针对MRI图像中腰椎（lumbar spine）的分割和标记问题，提出了一种基于修改后Attention U-Net架构的增强方法，用于3D切片数据的全景分割（panoptic segmentation）。该方法通过引入新型masking logic，成功解决了数据获取不足的挑战，并实现了99.5%的准确率，显著超越了现有技术。最终，这为脊椎疾病的精确诊断和治疗规划提供了更可靠的支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18291v1",
      "published_date": "2024-04-28 19:35:00 UTC",
      "updated_date": "2024-04-28 19:35:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:15:43.526919"
    },
    {
      "arxiv_id": "2405.01587v1",
      "title": "Improve Academic Query Resolution through BERT-based Question Extraction from Images",
      "title_zh": "翻译失败",
      "authors": [
        "Nidhi Kamal",
        "Saurabh Yadav",
        "Jorawar Singh",
        "Aditi Avasthi"
      ],
      "abstract": "Providing fast and accurate resolution to the student's query is an essential\nsolution provided by Edtech organizations. This is generally provided with a\nchat-bot like interface to enable students to ask their doubts easily. One\npreferred format for student queries is images, as it allows students to\ncapture and post questions without typing complex equations and information.\nHowever, this format also presents difficulties, as images may contain multiple\nquestions or textual noise that lowers the accuracy of existing single-query\nanswering solutions. In this paper, we propose a method for extracting\nquestions from text or images using a BERT-based deep learning model and\ncompare it to the other rule-based and layout-based methods. Our method aims to\nimprove the accuracy and efficiency of student query resolution in Edtech\norganizations.",
      "tldr_zh": "该论文针对 Edtech 组织的学生查询问题，指出图像格式（如包含复杂方程）虽方便但易导致多个问题或噪声干扰现有答案系统的准确性。研究提出一种基于 BERT 的深度学习模型，用于从文本或图像中提取问题，并将其与基于规则和布局的方法进行比较。结果显示，该方法显著提高了查询解析的准确性和效率，为 Edtech 平台的查询响应提供了更可靠的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01587v1",
      "published_date": "2024-04-28 19:11:08 UTC",
      "updated_date": "2024-04-28 19:11:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:15:56.040600"
    },
    {
      "arxiv_id": "2404.18276v1",
      "title": "Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)",
      "title_zh": "翻译失败",
      "authors": [
        "Malur Narayan",
        "John Pasmore",
        "Elton Sampaio",
        "Vijay Raghavan",
        "Gabriella Waters"
      ],
      "abstract": "The burgeoning influence of Large Language Models (LLMs) in shaping public\ndiscourse and decision-making underscores the imperative to address inherent\nbiases within these AI systems. In the wake of AI's expansive integration\nacross sectors, addressing racial bias in LLMs has never been more critical.\nThis paper introduces a novel framework called Comprehensive Bias\nNeutralization Framework (CBNF) which embodies an innovative approach to\nquantifying and mitigating biases within LLMs. Our framework combines the Large\nLanguage Model Bias Index (LLMBI) [Oketunji, A., Anas, M., Saina, D., (2023)]\nand Bias removaL with No Demographics (BLIND) [Orgad, H., Belinkov, Y. (2023)]\nmethodologies to create a new metric called Bias Intelligence Quotient\n(BiQ)which detects, measures, and mitigates racial bias in LLMs without\nreliance on demographic annotations.\n  By introducing a new metric called BiQ that enhances LLMBI with additional\nfairness metrics, CBNF offers a multi-dimensional metric for bias assessment,\nunderscoring the necessity of a nuanced approach to fairness in AI [Mehrabi et\nal., 2021]. This paper presents a detailed analysis of Latimer AI (a language\nmodel incrementally trained on black history and culture) in comparison to\nChatGPT 3.5, illustrating Latimer AI's efficacy in detecting racial, cultural,\nand gender biases through targeted training and refined bias mitigation\nstrategies [Latimer & Bender, 2023].",
      "tldr_zh": "这篇论文引入了 Comprehensive Bias Neutralization Framework (CBNF)，一种创新框架，用于量化并缓解 Large Language Models (LLMs) 中的种族偏见。CBNF 结合 Large Language Model Bias Index (LLMBI) 和 Bias removaL with No Demographics (BLIND) 方法，开发了新的 Bias Intelligence Quotient (BiQ) 指标，能够多维度检测、测量和减轻偏见，而无需依赖人口统计学注解。研究通过比较 Latimer AI（针对黑人历史和文化进行增量训练的模型）和 ChatGPT 3.5，展示了 Latimer AI 在识别种族、文化和性别偏见方面的显著优势。该框架强调了 AI 公平性的必要性，为偏见评估提供了更细致的方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "D.1; I.2"
      ],
      "primary_category": "cs.CL",
      "comment": "41 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.18276v1",
      "published_date": "2024-04-28 18:47:14 UTC",
      "updated_date": "2024-04-28 18:47:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:16:10.305057"
    },
    {
      "arxiv_id": "2404.18270v1",
      "title": "Pragmatic Formal Verification of Sequential Error Detection and Correction Codes (ECCs) used in Safety-Critical Design",
      "title_zh": "翻译失败",
      "authors": [
        "Aman Kumar"
      ],
      "abstract": "Error Detection and Correction Codes (ECCs) are often used in digital designs\nto protect data integrity. Especially in safety-critical systems such as\nautomotive electronics, ECCs are widely used and the verification of such\ncomplex logic becomes more critical considering the ISO 26262 safety standards.\nExhaustive verification of ECC using formal methods has been a challenge given\nthe high number of data bits to protect. As an example, for an ECC of 128 data\nbits with a possibility to detect up to four-bit errors, the combination of bit\nerrors is given by 128C1 + 128C2 + 128C3 + 128C4 = 1.1 * 10^7. This vast\nanalysis space often leads to bounded proof results. Moreover, the complexity\nand state-space increase further if the ECC has sequential encoding and\ndecoding stages. To overcome such problems and sign-off the design with\nconfidence within reasonable proof time, we present a pragmatic formal\nverification approach of complex ECC cores with several complexity reduction\ntechniques and know-how that were learnt during the course of verification. We\ndiscuss using the linearity of the syndrome generator as a helper assertion,\nusing the abstract model as glue logic to compare the RTL with the sequential\nversion of the circuit, k-induction-based model checking and using mathematical\nrelations captured as properties to simplify the verification in order to get\nan unbounded proof result within 24 hours of proof runtime.",
      "tldr_zh": "该研究针对安全关键设计（如汽车电子符合 ISO 26262 标准）中使用的顺序错误检测和纠正码 (ECCs)，提出了一种实用的形式验证方法，以应对数据位数多导致的验证空间庞大和复杂性问题。该方法采用多种复杂度降低技巧，包括利用 syndrome generator 的线性作为辅助断言、通过抽象模型比较 RTL 与顺序电路、应用 k-induction-based model checking，以及将数学关系转化为属性进行简化验证。这些技术使得验证过程能够在 24 小时内获得无界证明结果，提高了 ECC 设计的可靠性和验证效率。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in DVCon U.S. 2023",
      "pdf_url": "http://arxiv.org/pdf/2404.18270v1",
      "published_date": "2024-04-28 18:31:09 UTC",
      "updated_date": "2024-04-28 18:31:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:16:20.356725"
    },
    {
      "arxiv_id": "2405.15784v1",
      "title": "CLARINET: Augmenting Language Models to Ask Clarification Questions for Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Yizhou Chi",
        "Jessy Lin",
        "Kevin Lin",
        "Dan Klein"
      ],
      "abstract": "Users often make ambiguous requests that require clarification. We study the\nproblem of asking clarification questions in an information retrieval setting,\nwhere systems often face ambiguous search queries and it is challenging to turn\nthe uncertainty in the retrieval model into a natural language question. We\npresent CLARINET, a system that asks informative clarification questions by\nchoosing questions whose answers would maximize certainty in the correct\ncandidate. Our approach works by augmenting a large language model (LLM) to\ncondition on a retrieval distribution, finetuning end-to-end to generate the\nquestion that would have maximized the rank of the true candidate at each turn.\nWhen evaluated on a real-world retrieval dataset of users searching for books,\nour system outperforms traditional heuristics such as information gain on\nretrieval success by 17% and vanilla-prompted LLMs by 39% relative.",
      "tldr_zh": "该研究提出CLARINET系统，通过增强大型语言模型(LLM)来生成信息检索中的澄清问题，这些问题基于检索分布设计，能最大化正确候选的确定性。具体方法包括端到端微调LLM，使其生成能提升真实候选排名的澄清问题，从而处理用户模糊查询的不确定性。在真实书籍搜索数据集上，CLARINET比传统信息增益启发式方法提高了17%的检索成功率，并比直接提示的LLM提高了39%。这为检索系统的交互式改进提供了有效框架。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15784v1",
      "published_date": "2024-04-28 18:21:31 UTC",
      "updated_date": "2024-04-28 18:21:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:16:32.711859"
    },
    {
      "arxiv_id": "2404.18264v1",
      "title": "Modeling Orthographic Variation Improves NLP Performance for Nigerian Pidgin",
      "title_zh": "翻译失败",
      "authors": [
        "Pin-Jie Lin",
        "Merel Scholman",
        "Muhammed Saeed",
        "Vera Demberg"
      ],
      "abstract": "Nigerian Pidgin is an English-derived contact language and is traditionally\nan oral language, spoken by approximately 100 million people. No orthographic\nstandard has yet been adopted, and thus the few available Pidgin datasets that\nexist are characterised by noise in the form of orthographic variations. This\ncontributes to under-performance of models in critical NLP tasks. The current\nwork is the first to describe various types of orthographic variations commonly\nfound in Nigerian Pidgin texts, and model this orthographic variation. The\nvariations identified in the dataset form the basis of a phonetic-theoretic\nframework for word editing, which is used to generate orthographic variations\nto augment training data. We test the effect of this data augmentation on two\ncritical NLP tasks: machine translation and sentiment analysis. The proposed\nvariation generation framework augments the training data with new orthographic\nvariants which are relevant for the test set but did not occur in the training\nset originally. Our results demonstrate the positive effect of augmenting the\ntraining data with a combination of real texts from other corpora as well as\nsynthesized orthographic variation, resulting in performance improvements of\n2.1 points in sentiment analysis and 1.4 BLEU points in translation to English.",
      "tldr_zh": "这篇论文针对Nigerian Pidgin这种缺乏标准正字法的英语衍生语言，首次描述并建模了常见的orthographic variations，以解决这些变体带来的数据集噪声问题，从而提升NLP任务性能。研究者基于语音理论框架开发了一个词编辑系统，用于生成相关正字变体并扩充训练数据，包括结合真实语料和合成变体。实验结果显示，这种数据扩充方法在情感分析任务上提高了2.1点性能，在机器翻译到英语任务上提升了1.4 BLEU points。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to LREC-COLING 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2404.18264v1",
      "published_date": "2024-04-28 18:07:13 UTC",
      "updated_date": "2024-04-28 18:07:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:16:45.701950"
    },
    {
      "arxiv_id": "2404.18262v1",
      "title": "Generating Situated Reflection Triggers about Alternative Solution Paths: A Case Study of Generative AI for Computer-Supported Collaborative Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Atharva Naik",
        "Jessica Ruhan Yin",
        "Anusha Kamath",
        "Qianou Ma",
        "Sherry Tongshuang Wu",
        "Charles Murray",
        "Christopher Bogart",
        "Majd Sakr",
        "Carolyn P. Rose"
      ],
      "abstract": "An advantage of Large Language Models (LLMs) is their contextualization\ncapability - providing different responses based on student inputs like\nsolution strategy or prior discussion, to potentially better engage students\nthan standard feedback. We present a design and evaluation of a\nproof-of-concept LLM application to offer students dynamic and contextualized\nfeedback. Specifically, we augment an Online Programming Exercise bot for a\ncollege-level Cloud Computing course with ChatGPT, which offers students\ncontextualized reflection triggers during a collaborative query optimization\ntask in database design. We demonstrate that LLMs can be used to generate\nhighly situated reflection triggers that incorporate details of the\ncollaborative discussion happening in context. We discuss in depth the\nexploration of the design space of the triggers and their correspondence with\nthe learning objectives as well as the impact on student learning in a pilot\nstudy with 34 students.",
      "tldr_zh": "本文研究了大型语言模型 (LLMs) 的情境化能力，探讨其如何根据学生输入（如解决方案策略或讨论内容）生成动态反馈，以提升计算机支持协作学习的效果。具体而言，研究设计了一个原型系统，将 ChatGPT 整合到在线编程练习机器人中，用于大学云计算课程的协作查询优化任务。结果显示，LLMs 能有效生成高度情境化的反思触发器，并通过 34 名学生的试点研究，证明了其对学习目标的对应性和对学生学习的影响。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18262v1",
      "published_date": "2024-04-28 17:56:14 UTC",
      "updated_date": "2024-04-28 17:56:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:16:58.198066"
    },
    {
      "arxiv_id": "2404.18255v5",
      "title": "PatentGPT: A Large Language Model for Intellectual Property",
      "title_zh": "PatentGPT：面向知识产权的大型",
      "authors": [
        "Zilong Bai",
        "Ruiji Zhang",
        "Linqing Chen",
        "Qijun Cai",
        "Yuan Zhong",
        "Cong Wang",
        "Yan Fang",
        "Jie Fang",
        "Jing Sun",
        "Weikuan Wang",
        "Lizhi Zhou",
        "Haoran Hua",
        "Tian Qiu",
        "Chaochao Wang",
        "Cheng Sun",
        "Jianping Lu",
        "Yixin Wang",
        "Yubin Xia",
        "Meng Hu",
        "Haowen Liu",
        "Peng Xu",
        "Licong Xu",
        "Fu Bian",
        "Xiaolong Gu",
        "Lisha Zhang",
        "Weilei Wang",
        "Changyang Tu"
      ],
      "abstract": "In recent years, large language models(LLMs) have attracted significant\nattention due to their exceptional performance across a multitude of natural\nlanguage process tasks, and have been widely applied in various fields.\nHowever, the application of large language models in the Intellectual Property\n(IP) domain is challenging due to the strong need for specialized knowledge,\nprivacy protection, processing of extremely long text in this field. In this\ntechnical report, we present for the first time a low-cost, standardized\nprocedure for training IP-oriented LLMs, meeting the unique requirements of the\nIP domain. Using this standard process, we have trained the PatentGPT series\nmodels based on open-source pretrained models. By evaluating them on the\nopen-source IP-oriented benchmark MOZIP, our domain-specific LLMs outperforms\nGPT-4, indicating the effectiveness of the proposed training procedure and the\nexpertise of the PatentGPT models in the IP domain. Remarkably, our model\nsurpassed GPT-4 on the 2019 China Patent Agent Qualification Examination,\nscoring 65 and matching human expert levels. Additionally, the PatentGPT model,\nwhich utilizes the SMoE architecture, achieves performance comparable to that\nof GPT-4 in the IP domain and demonstrates a better cost-performance ratio on\nlong-text tasks, potentially serving as an alternative to GPT-4 within the IP\ndomain.",
      "tldr_zh": "这篇论文介绍了 PatentGPT，一种针对知识产权（IP）领域的大语言模型（LLMs），旨在解决专业知识需求、隐私保护和长文本处理等挑战。研究者提出了一种低成本、标准化的训练过程，使用开源预训练模型训练了 PatentGPT 系列模型，并采用 SMoE 架构提升性能。在开源基准 MOZIP 上，PatentGPT 超过了 GPT-4，并在 2019 中国专利代理人资格考试中得分 65，达到人类专家水平。该模型在 IP 领域的长文本任务上展现出与 GPT-4 相当的性能，但具有更好的性价比。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18255v5",
      "published_date": "2024-04-28 17:36:43 UTC",
      "updated_date": "2024-06-05 03:02:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:17:09.649758"
    },
    {
      "arxiv_id": "2404.18243v2",
      "title": "LEGENT: Open Platform for Embodied Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Zhili Cheng",
        "Zhitong Wang",
        "Jinyi Hu",
        "Shengding Hu",
        "An Liu",
        "Yuge Tu",
        "Pengkai Li",
        "Lei Shi",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Despite advancements in Large Language Models (LLMs) and Large Multimodal\nModels (LMMs), their integration into language-grounded, human-like embodied\nagents remains incomplete, hindering complex real-life task performance in\nphysical environments. Existing integrations often feature limited open\nsourcing, challenging collective progress in this field. We introduce LEGENT,\nan open, scalable platform for developing embodied agents using LLMs and LMMs.\nLEGENT offers a dual approach: a rich, interactive 3D environment with\ncommunicable and actionable agents, paired with a user-friendly interface, and\na sophisticated data generation pipeline utilizing advanced algorithms to\nexploit supervision from simulated worlds at scale. In our experiments, an\nembryonic vision-language-action model trained on LEGENT-generated data\nsurpasses GPT-4V in embodied tasks, showcasing promising generalization\ncapabilities.",
      "tldr_zh": "尽管大型语言模型（LLMs）和大型多模态模型（LMMs）在整合到语言基础的embodied agents中仍有不足，导致这些agents在物理环境中处理复杂任务的性能受限，且现有方案开源有限，该研究引入了LEGENT，一个开放且可扩展的平台，用于开发基于LLMs和LMMs的embodied agents。LEGENT采用双重方法，包括一个丰富的交互式3D环境，支持可通信和可行动的agents，并配有用户友好的界面，以及一个先进的データ生成管道，通过高级算法从模拟世界中大规模获取监督数据。在实验中，使用LEGENT生成的數據训练的vision-language-action模型在embodied tasks中超过了GPT-4V的性能，展示了良好的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 System Demonstration",
      "pdf_url": "http://arxiv.org/pdf/2404.18243v2",
      "published_date": "2024-04-28 16:50:12 UTC",
      "updated_date": "2024-08-11 17:18:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:17:21.622659"
    },
    {
      "arxiv_id": "2404.18231v2",
      "title": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Jiangjie Chen",
        "Xintao Wang",
        "Rui Xu",
        "Siyu Yuan",
        "Yikai Zhang",
        "Wei Shi",
        "Jian Xie",
        "Shuang Li",
        "Ruihan Yang",
        "Tinghui Zhu",
        "Aili Chen",
        "Nianqi Li",
        "Lida Chen",
        "Caiyu Hu",
        "Siye Wu",
        "Scott Ren",
        "Ziquan Fu",
        "Yanghua Xiao"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have significantly\nboosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI\nsystems designed to simulate assigned personas. By harnessing multiple advanced\nabilities of LLMs, including in-context learning, instruction following, and\nsocial intelligence, RPLAs achieve a remarkable sense of human likeness and\nvivid role-playing performance. RPLAs can mimic a wide range of personas,\nranging from historical figures and fictional characters to real-life\nindividuals. Consequently, they have catalyzed numerous AI applications, such\nas emotional companions, interactive video games, personalized assistants and\ncopilots, and digital clones. In this paper, we conduct a comprehensive survey\nof this field, illustrating the evolution and recent progress in RPLAs\nintegrating with cutting-edge LLM technologies. We categorize personas into\nthree types: 1) Demographic Persona, which leverages statistical stereotypes;\n2) Character Persona, focused on well-established figures; and 3)\nIndividualized Persona, customized through ongoing user interactions for\npersonalized services. We begin by presenting a comprehensive overview of\ncurrent methodologies for RPLAs, followed by the details for each persona type,\ncovering corresponding data sourcing, agent construction, and evaluation.\nAfterward, we discuss the fundamental risks, existing limitations, and future\nprospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI\napplications, which reflects practical user demands that shape and drive RPLA\nresearch. Through this work, we aim to establish a clear taxonomy of RPLA\nresearch and applications, and facilitate future research in this critical and\never-evolving field, and pave the way for a future where humans and RPLAs\ncoexist in harmony.",
      "tldr_zh": "这篇论文对角色扮演语言代理（RPLAs）进行了全面调查，探讨了大型语言模型（LLMs）的进步如何推动RPLAs的发展，使其具备in-context learning、instruction following和social intelligence等能力，从而模拟各种人物并应用于情感伴侣、互动游戏和个性化助手等领域。作者将personas分为三类：Demographic Persona（基于统计刻板印象）、Character Persona（针对知名人物）和Individualized Persona（通过用户互动定制化），并详细阐述了数据来源、代理构建和评估方法。论文还分析了RPLAs的潜在风险和局限性，并展望了未来前景，旨在建立清晰的RPLA研究taxonomy，促进人类与AI的和谐共存。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to TMLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.18231v2",
      "published_date": "2024-04-28 15:56:41 UTC",
      "updated_date": "2024-10-09 12:11:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:17:32.705115"
    },
    {
      "arxiv_id": "2404.18214v2",
      "title": "Contrastive Learning Method for Sequential Recommendation based on Multi-Intention Disentanglement",
      "title_zh": "基于多意图解耦的对比学习序列推荐方法",
      "authors": [
        "Zeyu Hu",
        "Yuzhi Xiao",
        "Tao Huang",
        "Xuanrong Huo"
      ],
      "abstract": "Sequential recommendation is one of the important branches of recommender\nsystem, aiming to achieve personalized recommended items for the future through\nthe analysis and prediction of users' ordered historical interactive behaviors.\nHowever, along with the growth of the user volume and the increasingly rich\nbehavioral information, how to understand and disentangle the user's\ninteractive multi-intention effectively also poses challenges to behavior\nprediction and sequential recommendation. In light of these challenges, we\npropose a Contrastive Learning sequential recommendation method based on\nMulti-Intention Disentanglement (MIDCL). In our work, intentions are recognized\nas dynamic and diverse, and user behaviors are often driven by current\nmulti-intentions, which means that the model needs to not only mine the most\nrelevant implicit intention for each user, but also impair the influence from\nirrelevant intentions. Therefore, we choose Variational Auto-Encoder (VAE) to\nrealize the disentanglement of users' multi-intentions. We propose two types of\ncontrastive learning paradigms for finding the most relevant user's interactive\nintention, and maximizing the mutual information of positive sample pairs,\nrespectively. Experimental results show that MIDCL not only has significant\nsuperiority over most existing baseline methods, but also brings a more\ninterpretable case to the research about intention-based prediction and\nrecommendation.",
      "tldr_zh": "本研究针对顺序推荐（Sequential Recommendation）中的用户多意图（Multi-Intention Disentanglement）挑战，提出了一种基于对比学习（Contrastive Learning）的MIDCL方法，以有效分析和预测用户的有序历史互动行为。  \n该方法利用Variational Auto-Encoder (VAE)实现多意图分离，并设计两种对比学习范式：一种用于识别用户最相关互动意图，另一种用于最大化正样本对的互信息，从而减少无关意图的影响。  \n实验结果表明，MIDCL在性能上显著优于现有基线方法，并为意图-based预测和推荐提供了更具可解释性的案例。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18214v2",
      "published_date": "2024-04-28 15:13:36 UTC",
      "updated_date": "2024-05-08 17:23:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:17:46.452048"
    },
    {
      "arxiv_id": "2404.18213v2",
      "title": "S$^2$Mamba: A Spatial-spectral State Space Model for Hyperspectral Image Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Guanchun Wang",
        "Xiangrong Zhang",
        "Zelin Peng",
        "Tianyang Zhang",
        "Licheng Jiao"
      ],
      "abstract": "Land cover analysis using hyperspectral images (HSI) remains an open problem\ndue to their low spatial resolution and complex spectral information. Recent\nstudies are primarily dedicated to designing Transformer-based architectures\nfor spatial-spectral long-range dependencies modeling, which is computationally\nexpensive with quadratic complexity. Selective structured state space model\n(Mamba), which is efficient for modeling long-range dependencies with linear\ncomplexity, has recently shown promising progress. However, its potential in\nhyperspectral image processing that requires handling numerous spectral bands\nhas not yet been explored. In this paper, we innovatively propose S$^2$Mamba, a\nspatial-spectral state space model for hyperspectral image classification, to\nexcavate spatial-spectral contextual features, resulting in more efficient and\naccurate land cover analysis. In S$^2$Mamba, two selective structured state\nspace models through different dimensions are designed for feature extraction,\none for spatial, and the other for spectral, along with a spatial-spectral\nmixture gate for optimal fusion. More specifically, S$^2$Mamba first captures\nspatial contextual relations by interacting each pixel with its adjacent\nthrough a Patch Cross Scanning module and then explores semantic information\nfrom continuous spectral bands through a Bi-directional Spectral Scanning\nmodule. Considering the distinct expertise of the two attributes in homogenous\nand complicated texture scenes, we realize the Spatial-spectral Mixture Gate by\na group of learnable matrices, allowing for the adaptive incorporation of\nrepresentations learned across different dimensions. Extensive experiments\nconducted on HSI classification benchmarks demonstrate the superiority and\nprospect of S$^2$Mamba. The code will be made available at:\nhttps://github.com/PURE-melo/S2Mamba.",
      "tldr_zh": "该论文提出 S$^2$Mamba，一种空间-光谱状态空间模型，用于解决高光谱图像(HSI)分类中的低空间分辨率和复杂光谱信息问题，相比基于 Transformer 的方法，它采用 Selective Structured State Space Model 以线性复杂度高效建模长程依赖。S$^2$Mamba 包括 Patch Cross Scanning 模块提取空间上下文关系、Bi-directional Spectral Scanning 模块探索光谱语义信息，以及 Spatial-spectral Mixture Gate 通过可学习矩阵适应性地融合两种特征。实验在 HSI 分类基准上证明了该模型的优越性，提供更准确和高效的土地覆盖分析。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18213v2",
      "published_date": "2024-04-28 15:12:56 UTC",
      "updated_date": "2024-08-13 10:47:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:17:58.725022"
    },
    {
      "arxiv_id": "2404.18212v3",
      "title": "Paint by Inpaint: Learning to Add Image Objects by Removing Them First",
      "title_zh": "Paint by Inpaint：先移除图像对象再学习添加它们",
      "authors": [
        "Navve Wasserman",
        "Noam Rotstein",
        "Roy Ganz",
        "Ron Kimmel"
      ],
      "abstract": "Image editing has advanced significantly with the introduction of\ntext-conditioned diffusion models. Despite this progress, seamlessly adding\nobjects to images based on textual instructions without requiring user-provided\ninput masks remains a challenge. We address this by leveraging the insight that\nremoving objects (Inpaint) is significantly simpler than its inverse process of\nadding them (Paint), attributed to inpainting models that benefit from\nsegmentation mask guidance. Capitalizing on this realization, by implementing\nan automated and extensive pipeline, we curate a filtered large-scale image\ndataset containing pairs of images and their corresponding object-removed\nversions. Using these pairs, we train a diffusion model to inverse the\ninpainting process, effectively adding objects into images. Unlike other\nediting datasets, ours features natural target images instead of synthetic ones\nwhile ensuring source-target consistency by construction. Additionally, we\nutilize a large Vision-Language Model to provide detailed descriptions of the\nremoved objects and a Large Language Model to convert these descriptions into\ndiverse, natural-language instructions. Our quantitative and qualitative\nresults show that the trained model surpasses existing models in both object\naddition and general editing tasks. Visit our project page for the released\ndataset and trained models at https://rotsteinnoam.github.io/Paint-by-Inpaint.",
      "tldr_zh": "本文提出了一种名为 Paint by Inpaint 的图像编辑方法，通过先移除对象（Inpaint）来学习添加对象（Paint），利用扩散模型（diffusion model）逆转该过程，以实现基于文本指令的无缝对象添加。作者构建了一个大型数据集，包含自然图像及其移除对象版本的配对，使用自动化管道结合 Vision-Language Model 生成详细对象描述，并通过 Large Language Model 转换为多样化指令，确保源-目标一致性。实验结果显示，该训练模型在对象添加和一般编辑任务上，定量和定性均超越现有模型，为图像编辑提供了更高效的工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18212v3",
      "published_date": "2024-04-28 15:07:53 UTC",
      "updated_date": "2025-03-20 06:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:18:09.264525"
    },
    {
      "arxiv_id": "2405.01585v1",
      "title": "Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications",
      "title_zh": "Tabular Embedding Model (TEM)：微调嵌入模型用于",
      "authors": [
        "Sujit Khanna",
        "Shishir Subedi"
      ],
      "abstract": "In recent times Large Language Models have exhibited tremendous capabilities,\nespecially in the areas of mathematics, code generation and general-purpose\nreasoning. However for specialized domains especially in applications that\nrequire parsing and analyzing large chunks of numeric or tabular data even\nstate-of-the-art (SOTA) models struggle. In this paper, we introduce a new\napproach to solving domain-specific tabular data analysis tasks by presenting a\nunique RAG workflow that mitigates the scalability issues of existing tabular\nLLM solutions. Specifically, we present Tabular Embedding Model (TEM), a novel\napproach to fine-tune embedding models for tabular Retrieval-Augmentation\nGeneration (RAG) applications. Embedding models form a crucial component in the\nRAG workflow and even current SOTA embedding models struggle as they are\npredominantly trained on textual datasets and thus underperform in scenarios\ninvolving complex tabular data. The evaluation results showcase that our\napproach not only outperforms current SOTA embedding models in this domain but\nalso does so with a notably smaller and more efficient model structure.",
      "tldr_zh": "该论文针对大型语言模型(LLMs)在处理数字和表格数据时的局限性，提出了一种新型框架Tabular Embedding Model (TEM)，通过微调嵌入模型来优化表格Retrieval-Augmentation Generation (RAG)应用，从而解决现有解决方案的可扩展性问题。TEM专注于提升嵌入模型在复杂表格数据场景中的性能，利用专门的RAG工作流来改善数据解析和分析。实验结果显示，TEM不仅在该领域超越了当前state-of-the-art (SOTA)嵌入模型，还采用了更小、更高效的模型结构。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.01585v1",
      "published_date": "2024-04-28 14:58:55 UTC",
      "updated_date": "2024-04-28 14:58:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:18:20.656195"
    },
    {
      "arxiv_id": "2404.18203v2",
      "title": "LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM",
      "title_zh": "LMM-PCQA：使用 LMM 辅助点云质量评估",
      "authors": [
        "Zicheng Zhang",
        "Haoning Wu",
        "Yingjie Zhou",
        "Chunyi Li",
        "Wei Sun",
        "Chaofeng Chen",
        "Xiongkuo Min",
        "Xiaohong Liu",
        "Weisi Lin",
        "Guangtao Zhai"
      ],
      "abstract": "Although large multi-modality models (LMMs) have seen extensive exploration\nand application in various quality assessment studies, their integration into\nPoint Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs'\nexceptional performance and robustness in low-level vision and quality\nassessment tasks, this study aims to investigate the feasibility of imparting\nPCQA knowledge to LMMs through text supervision. To achieve this, we transform\nquality labels into textual descriptions during the fine-tuning phase, enabling\nLMMs to derive quality rating logits from 2D projections of point clouds. To\ncompensate for the loss of perception in the 3D domain, structural features are\nextracted as well. These quality logits and structural features are then\ncombined and regressed into quality scores. Our experimental results affirm the\neffectiveness of our approach, showcasing a novel integration of LMMs into PCQA\nthat enhances model understanding and assessment accuracy. We hope our\ncontributions can inspire subsequent investigations into the fusion of LMMs\nwith PCQA, fostering advancements in 3D visual quality analysis and beyond. The\ncode is available at https://github.com/zzc-1998/LMM-PCQA.",
      "tldr_zh": "本文提出 LMM-PCQA 方法，将大型多模态模型 (LMMs) 首次应用于点云质量评估 (PCQA)，通过文本监督将质量标签转化为文本描述，以微调 LMMs 并从点云的 2D 投影中提取质量评分 logits，同时提取结构特征来弥补 3D 感知损失，并将这些元素结合回归最终质量分数。实验结果验证了该方法的有效性，在 PCQA 任务中显著提升了模型的理解和评估准确性。该研究有望激发后续工作，推动 LMMs 与 PCQA 的融合，推进 3D 视觉质量分析领域的发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18203v2",
      "published_date": "2024-04-28 14:47:09 UTC",
      "updated_date": "2024-08-06 03:37:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:18:35.873906"
    },
    {
      "arxiv_id": "2405.01509v1",
      "title": "Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models",
      "title_zh": "可学习的语言水印用于追踪大型语言模型的模型提取攻击",
      "authors": [
        "Minhao Bai",
        "Kaiyi Pang",
        "Yongfeng Huang"
      ],
      "abstract": "In the rapidly evolving domain of artificial intelligence, safeguarding the\nintellectual property of Large Language Models (LLMs) is increasingly crucial.\nCurrent watermarking techniques against model extraction attacks, which rely on\nsignal insertion in model logits or post-processing of generated text, remain\nlargely heuristic. We propose a novel method for embedding learnable linguistic\nwatermarks in LLMs, aimed at tracing and preventing model extraction attacks.\nOur approach subtly modifies the LLM's output distribution by introducing\ncontrolled noise into token frequency distributions, embedding an statistically\nidentifiable controllable watermark.We leverage statistical hypothesis testing\nand information theory, particularly focusing on Kullback-Leibler Divergence,\nto differentiate between original and modified distributions effectively. Our\nwatermarking method strikes a delicate well balance between robustness and\noutput quality, maintaining low false positive/negative rates and preserving\nthe LLM's original performance.",
      "tldr_zh": "该论文提出了一种可学习的语言水印方法，用于追踪和防范针对Large Language Models (LLMs)的模型提取攻击，从而保护AI知识产权。方法通过向token频率分布引入受控噪声，微妙地修改LLM的输出分布，并嵌入可统计识别的水印，利用统计假设测试和Kullback-Leibler Divergence来有效区分原始与修改分布。该方法在鲁棒性和输出质量之间实现了良好平衡，保持了低假阳性/假阴性率，同时保留了LLM的原始性能。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "not decided",
      "pdf_url": "http://arxiv.org/pdf/2405.01509v1",
      "published_date": "2024-04-28 14:45:53 UTC",
      "updated_date": "2024-04-28 14:45:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:18:44.884605"
    },
    {
      "arxiv_id": "2404.18202v2",
      "title": "WorldGPT: Empowering LLM as Multimodal World Model",
      "title_zh": "WorldGPT: 赋能大语言模型作为多模态世界模型",
      "authors": [
        "Zhiqi Ge",
        "Hongzhe Huang",
        "Mingze Zhou",
        "Juncheng Li",
        "Guoming Wang",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "abstract": "World models are progressively being employed across diverse fields,\nextending from basic environment simulation to complex scenario construction.\nHowever, existing models are mainly trained on domain-specific states and\nactions, and confined to single-modality state representations. In this paper,\nWe introduce WorldGPT, a generalist world model built upon Multimodal Large\nLanguage Model (MLLM). WorldGPT acquires an understanding of world dynamics\nthrough analyzing millions of videos across various domains. To further enhance\nWorldGPT's capability in specialized scenarios and long-term tasks, we have\nintegrated it with a novel cognitive architecture that combines memory\noffloading, knowledge retrieval, and context reflection. As for evaluation, we\nbuild WorldNet, a multimodal state transition prediction benchmark encompassing\nvaried real-life scenarios. Conducting evaluations on WorldNet directly\ndemonstrates WorldGPT's capability to accurately model state transition\npatterns, affirming its effectiveness in understanding and predicting the\ndynamics of complex scenarios. We further explore WorldGPT's emerging potential\nin serving as a world simulator, helping multimodal agents generalize to\nunfamiliar domains through efficiently synthesising multimodal instruction\ninstances which are proved to be as reliable as authentic data for fine-tuning\npurposes. The project is available on\n\\url{https://github.com/DCDmllm/WorldGPT}.",
      "tldr_zh": "该研究引入了 WorldGPT，一种基于 Multimodal Large Language Model (MLLM) 的通用世界模型，通过分析数百万跨领域视频来理解世界动态，并整合了新型认知架构（包括记忆卸载、知识检索和上下文反射），以提升其在专业场景和长期任务中的性能。为评估 WorldGPT，论文构建了 WorldNet 基准，涵盖各种真实生活场景的预测任务，结果显示它能准确建模多模态状态转移模式。此外，WorldGPT 作为世界模拟器，可合成可靠的多模态指令实例，帮助多模态代理在陌生领域泛化，并证明这些数据可用于微调效果等同于真实数据。",
      "categories": [
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "update v2",
      "pdf_url": "http://arxiv.org/pdf/2404.18202v2",
      "published_date": "2024-04-28 14:42:02 UTC",
      "updated_date": "2024-09-28 17:00:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:18:58.889817"
    },
    {
      "arxiv_id": "2404.18198v1",
      "title": "Permutation-equivariant quantum convolutional neural networks",
      "title_zh": "置换等变的量子卷积神经网络",
      "authors": [
        "Sreetama Das",
        "Filippo Caruso"
      ],
      "abstract": "The Symmetric group $S_{n}$ manifests itself in large classes of quantum\nsystems as the invariance of certain characteristics of a quantum state with\nrespect to permuting the qubits. The subgroups of $S_{n}$ arise, among many\nother contexts, to describe label symmetry of classical images with respect to\nspatial transformations, e.g. reflection or rotation. Equipped with the\nformalism of geometric quantum machine learning, in this work we propose the\narchitectures of equivariant quantum convolutional neural networks (EQCNNs)\nadherent to $S_{n}$ and its subgroups. We demonstrate that a careful choice of\npixel-to-qubit embedding order can facilitate easy construction of EQCNNs for\nsmall subgroups of $S_{n}$. Our novel EQCNN architecture corresponding to the\nfull permutation group $S_{n}$ is built by applying all possible QCNNs with\nequal probability, which can also be conceptualized as a dropout strategy in\nquantum neural networks. For subgroups of $S_{n}$, our numerical results using\nMNIST datasets show better classification accuracy than non-equivariant QCNNs.\nThe $S_{n}$-equivariant QCNN architecture shows significantly improved training\nand test performance than non-equivariant QCNN for classification of connected\nand non-connected graphs. When trained with sufficiently large number of data,\nthe $S_{n}$-equivariant QCNN shows better average performance compared to\n$S_{n}$-equivariant QNN . These results contribute towards building powerful\nquantum machine learning architectures in permutation-symmetric systems.",
      "tldr_zh": "这篇论文提出了置换等变量子卷积神经网络 (EQCNNs)，这些网络基于几何量子机器学习形式主义，遵守 Symmetric group \\( S_n \\) 和其子群，以处理量子系统中的置换不变性和经典图像的标签对称性。方法包括通过仔细选择像素到量子比特的嵌入顺序来构建针对 \\( S_n \\) 子群的 EQCNNs，并针对完整的 \\( S_n \\) 采用所有可能 QCNNs 的等概率应用，类似于量子神经网络中的 dropout 策略。实验结果显示，在 MNIST 数据集上，EQCNNs 比非等变 QCNNs 实现了更高的分类准确率；此外，在连接和非连接图的分类任务中，\\( S_n \\)-等变 QCNNs 显著提升了训练和测试性能，并整体优于 \\( S_n \\)-等变 QNNs。这些发现为置换对称系统的量子机器学习架构提供了更强大和可靠的框架。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "13 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18198v1",
      "published_date": "2024-04-28 14:34:28 UTC",
      "updated_date": "2024-04-28 14:34:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:19:12.781264"
    },
    {
      "arxiv_id": "2405.15783v1",
      "title": "Multimodality Invariant Learning for Multimedia-Based New Item Recommendation",
      "title_zh": "多模态不变学习用于基于多媒体的新项目推荐",
      "authors": [
        "Haoyue Bai",
        "Le Wu",
        "Min Hou",
        "Miaomiao Cai",
        "Zhuangzhuang He",
        "Yuyang Zhou",
        "Richang Hong",
        "Meng Wang"
      ],
      "abstract": "Multimedia-based recommendation provides personalized item suggestions by\nlearning the content preferences of users. With the proliferation of digital\ndevices and APPs, a huge number of new items are created rapidly over time. How\nto quickly provide recommendations for new items at the inference time is\nchallenging. What's worse, real-world items exhibit varying degrees of modality\nmissing(e.g., many short videos are uploaded without text descriptions). Though\nmany efforts have been devoted to multimedia-based recommendations, they either\ncould not deal with new multimedia items or assumed the modality completeness\nin the modeling process.\n  In this paper, we highlight the necessity of tackling the modality missing\nissue for new item recommendation. We argue that users' inherent content\npreference is stable and better kept invariant to arbitrary modality missing\nenvironments. Therefore, we approach this problem from a novel perspective of\ninvariant learning. However, how to construct environments from finite user\nbehavior training data to generalize any modality missing is challenging. To\ntackle this issue, we propose a novel Multimodality Invariant Learning\nreCommendation(a.k.a. MILK) framework. Specifically, MILK first designs a\ncross-modality alignment module to keep semantic consistency from pretrained\nmultimedia item features. After that, MILK designs multi-modal heterogeneous\nenvironments with cyclic mixup to augment training data, in order to mimic any\nmodality missing for invariant user preference learning. Extensive experiments\non three real datasets verify the superiority of our proposed framework. The\ncode is available at https://github.com/HaoyueBai98/MILK.",
      "tldr_zh": "该论文探讨了基于多媒体的推荐系统在处理新项目推荐时面临的挑战，特别是模态缺失问题（如视频缺少文本描述），并强调用户内容偏好应保持不变（invariant learning）。为了解决这一问题，作者提出了一种新型框架Multimodality Invariant Learning reCommendation（MILK），它包括cross-modality alignment module来确保多媒体特征的语义一致性，以及multi-modal heterogeneous environments with cyclic mixup来模拟任意模态缺失并增强训练数据。实验结果显示，MILK框架在三个真实数据集上显著优于基线模型，证明了其在新项目推荐的有效性。代码已在GitHub上公开。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.15783v1",
      "published_date": "2024-04-28 14:29:09 UTC",
      "updated_date": "2024-04-28 14:29:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:19:22.619841"
    },
    {
      "arxiv_id": "2404.18197v1",
      "title": "A General Causal Inference Framework for Cross-Sectional Observational Data",
      "title_zh": "横截面观测数据的通用因果推断框架",
      "authors": [
        "Yonghe Zhao",
        "Huiyan Sun"
      ],
      "abstract": "Causal inference methods for observational data are highly regarded due to\ntheir wide applicability. While there are already numerous methods available\nfor de-confounding bias, these methods generally assume that covariates consist\nsolely of confounders or make naive assumptions about the covariates. Such\nassumptions face challenges in both theory and practice, particularly when\ndealing with high-dimensional covariates. Relaxing these naive assumptions and\nidentifying the confounding covariates that truly require correction can\neffectively enhance the practical significance of these methods. Therefore,\nthis paper proposes a General Causal Inference (GCI) framework specifically\ndesigned for cross-sectional observational data, which precisely identifies the\nkey confounding covariates and provides corresponding identification algorithm.\nSpecifically, based on progressive derivations of the Markov property on\nDirected Acyclic Graph, we conclude that the key confounding covariates are\nequivalent to the common root ancestors of the treatment and the outcome\nvariable. Building upon this conclusion, the GCI framework is composed of a\nnovel Ancestor Set Identification (ASI) algorithm and de-confounding inference\nmethods. Firstly, the ASI algorithm is theoretically supported by the\nconditional independence properties and causal asymmetry between variables,\nenabling the identification of key confounding covariates. Subsequently, the\nidentified confounding covariates are used in the de-confounding inference\nmethods to obtain unbiased causal effect estimation, which can support informed\ndecision-making. Extensive experiments on synthetic datasets demonstrate that\nthe GCI framework can effectively identify the critical confounding covariates\nand significantly improve the precision, stability, and interpretability of\ncausal inference in observational studies.",
      "tldr_zh": "该论文提出一个通用因果推断（General Causal Inference, GCI）框架，针对横截面观察数据中的混杂偏差问题，通过精确识别关键混杂协变量来提升现有方法的实用性。框架基于Directed Acyclic Graph (DAG)的Markov属性，得出关键混杂协变量等同于治疗变量和结果变量的共同根祖先，并引入Ancestor Set Identification (ASI)算法，利用条件独立性和因果不对称性进行识别。实验结果显示，在合成数据集上，GCI框架能有效识别这些协变量，并显著提高因果效应的精确性、稳定性和可解释性。",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ME",
      "comment": "19 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18197v1",
      "published_date": "2024-04-28 14:26:27 UTC",
      "updated_date": "2024-04-28 14:26:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:19:34.230468"
    },
    {
      "arxiv_id": "2404.18191v2",
      "title": "Exploring the Robustness of In-Context Learning with Noisy Labels",
      "title_zh": "探索带噪声标签的在上下文学习的鲁棒性",
      "authors": [
        "Chen Cheng",
        "Xinzhi Yu",
        "Haodong Wen",
        "Jingsong Sun",
        "Guanzhang Yue",
        "Yihao Zhang",
        "Zeming Wei"
      ],
      "abstract": "Recently, the mysterious In-Context Learning (ICL) ability exhibited by\nTransformer architectures, especially in large language models (LLMs), has\nsparked significant research interest. However, the resilience of Transformers'\nin-context learning capabilities in the presence of noisy samples, prevalent in\nboth training corpora and prompt demonstrations, remains underexplored. In this\npaper, inspired by prior research that studies ICL ability using simple\nfunction classes, we take a closer look at this problem by investigating the\nrobustness of Transformers against noisy labels. Specifically, we first conduct\na thorough evaluation and analysis of the robustness of Transformers against\nnoisy labels during in-context learning and show that they exhibit notable\nresilience against diverse types of noise in demonstration labels. Furthermore,\nwe delve deeper into this problem by exploring whether introducing noise into\nthe training set, akin to a form of data augmentation, enhances such robustness\nduring inference, and find that such noise can indeed improve the robustness of\nICL. Overall, our fruitful analysis and findings provide a comprehensive\nunderstanding of the resilience of Transformer models against label noises\nduring ICL and provide valuable insights into the research on Transformers in\nnatural language processing. Our code is available at\nhttps://github.com/InezYu0928/in-context-learning.",
      "tldr_zh": "本文探讨了Transformer模型在In-Context Learning (ICL)中的鲁棒性，特别针对训练语料和提示演示中常见的噪声标签问题。通过使用简单函数类进行评估，研究发现Transformer对各种噪声标签表现出显著的韧性。进一步实验表明，在训练集引入噪声作为数据增强，能有效提升ICL的鲁棒性。该研究为理解Transformer在自然语言处理中的性能提供了宝贵见解，并公开了相关代码。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2024 Workshop on Reliable and Responsible Foundation Models",
      "pdf_url": "http://arxiv.org/pdf/2404.18191v2",
      "published_date": "2024-04-28 14:05:23 UTC",
      "updated_date": "2024-05-01 09:15:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:19:46.171252"
    },
    {
      "arxiv_id": "2404.18185v1",
      "title": "Ranked List Truncation for Large Language Model-based Re-Ranking",
      "title_zh": "翻译失败",
      "authors": [
        "Chuan Meng",
        "Negar Arabzadeh",
        "Arian Askari",
        "Mohammad Aliannejadi",
        "Maarten de Rijke"
      ],
      "abstract": "We study ranked list truncation (RLT) from a novel \"retrieve-then-re-rank\"\nperspective, where we optimize re-ranking by truncating the retrieved list\n(i.e., trim re-ranking candidates). RLT is crucial for re-ranking as it can\nimprove re-ranking efficiency by sending variable-length candidate lists to a\nre-ranker on a per-query basis. It also has the potential to improve re-ranking\neffectiveness. Despite its importance, there is limited research into applying\nRLT methods to this new perspective. To address this research gap, we reproduce\nexisting RLT methods in the context of re-ranking, especially newly emerged\nlarge language model (LLM)-based re-ranking. In particular, we examine to what\nextent established findings on RLT for retrieval are generalizable to the\n\"retrieve-then-re-rank\" setup from three perspectives: (i) assessing RLT\nmethods in the context of LLM-based re-ranking with lexical first-stage\nretrieval, (ii) investigating the impact of different types of first-stage\nretrievers on RLT methods, and (iii) investigating the impact of different\ntypes of re-rankers on RLT methods. We perform experiments on the TREC 2019 and\n2020 deep learning tracks, investigating 8 RLT methods for pipelines involving\n3 retrievers and 2 re-rankers. We reach new insights into RLT methods in the\ncontext of re-ranking.",
      "tldr_zh": "本论文探讨了 Ranked List Truncation (RLT) 在“检索然后重新排序”框架中的应用，旨在通过截断检索列表来优化 Large Language Model (LLM)-based re-ranking 的效率和效果。研究者复制了现有 RLT 方法，并评估了其在不同设置下的泛化性，包括与词汇级第一阶段检索的结合、不同类型检索器（如3种）和重新排序器（如2种）的影响。实验基于 TREC 2019 和 2020 深度学习轨道，使用8种 RLT 方法，揭示了 RLT 在 re-ranking 上下文中新的见解和潜力。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "H.3.3"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted for publication as a long paper at SIGIR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.18185v1",
      "published_date": "2024-04-28 13:39:33 UTC",
      "updated_date": "2024-04-28 13:39:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:20:00.147167"
    },
    {
      "arxiv_id": "2404.18183v1",
      "title": "Innovative Application of Artificial Intelligence Technology in Bank Credit Risk Management",
      "title_zh": "人工智能技术在银行信贷风险管理中的创新应用",
      "authors": [
        "Shuochen Bi",
        "Wenqing Bao"
      ],
      "abstract": "With the rapid growth of technology, especially the widespread application of\nartificial intelligence (AI) technology, the risk management level of\ncommercial banks is constantly reaching new heights. In the current wave of\ndigitalization, AI has become a key driving force for the strategic\ntransformation of financial institutions, especially the banking industry. For\ncommercial banks, the stability and safety of asset quality are crucial, which\ndirectly relates to the long-term stable growth of the bank. Among them, credit\nrisk management is particularly core because it involves the flow of a large\namount of funds and the accuracy of credit decisions. Therefore, establishing a\nscientific and effective credit risk decision-making mechanism is of great\nstrategic significance for commercial banks. In this context, the innovative\napplication of AI technology has brought revolutionary changes to bank credit\nrisk management. Through deep learning and big data analysis, AI can accurately\nevaluate the credit status of borrowers, timely identify potential risks, and\nprovide banks with more accurate and comprehensive credit decision support. At\nthe same time, AI can also achieve realtime monitoring and early warning,\nhelping banks intervene before risks occur and reduce losses.",
      "tldr_zh": "该论文探讨了人工智能（AI）技术在银行信贷风险管理中的创新应用，强调AI如何通过深度学习和大数据分析提升借款人信用评估的准确性，并及时识别潜在风险。研究指出，AI能够提供实时监控和预警机制，帮助银行在风险发生前进行干预，从而减少损失并优化信用决策支持。总体而言，此创新方法为商业银行的战略转型和资产质量稳定提供了关键驱动力。",
      "categories": [
        "q-fin.RM",
        "cs.AI"
      ],
      "primary_category": "q-fin.RM",
      "comment": "6 pages, 1 figure, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.18183v1",
      "published_date": "2024-04-28 13:29:35 UTC",
      "updated_date": "2024-04-28 13:29:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:20:08.546157"
    },
    {
      "arxiv_id": "2404.18178v1",
      "title": "Assessing Image Quality Using a Simple Generative Representation",
      "title_zh": "使用简单生成式表示评估图像质量",
      "authors": [
        "Simon Raviv",
        "Gal Chechik"
      ],
      "abstract": "Perceptual image quality assessment (IQA) is the task of predicting the\nvisual quality of an image as perceived by a human observer. Current\nstate-of-the-art techniques are based on deep representations trained in\ndiscriminative manner. Such representations may ignore visually important\nfeatures, if they are not predictive of class labels. Recent generative models\nsuccessfully learn low-dimensional representations using auto-encoding and have\nbeen argued to preserve better visual features. Here we leverage existing\nauto-encoders and propose VAE-QA, a simple and efficient method for predicting\nimage quality in the presence of a full-reference. We evaluate our approach on\nfour standard benchmarks and find that it significantly improves generalization\nacross datasets, has fewer trainable parameters, a smaller memory footprint and\nfaster run time.",
      "tldr_zh": "这篇论文探讨了 Perceptual Image Quality Assessment (IQA)，即预测人类感知的图像视觉质量，并指出现有基于判别式深度表示的方法可能忽略重要视觉特征。作者提出 VAE-QA，一种简单高效的方法，利用现有 auto-encoders 的生成式表示来评估全参考图像质量。实验结果显示，VAE-QA 在四个标准基准上显著提升了数据集间的泛化能力，同时减少了训练参数、降低了内存占用并加快了运行速度。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18178v1",
      "published_date": "2024-04-28 13:18:47 UTC",
      "updated_date": "2024-04-28 13:18:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:20:22.541676"
    },
    {
      "arxiv_id": "2404.18174v1",
      "title": "Mamba-FETrack: Frame-Event Tracking via State Space Model",
      "title_zh": "翻译失败",
      "authors": [
        "Ju Huang",
        "Shiao Wang",
        "Shuai Wang",
        "Zhe Wu",
        "Xiao Wang",
        "Bo Jiang"
      ],
      "abstract": "RGB-Event based tracking is an emerging research topic, focusing on how to\neffectively integrate heterogeneous multi-modal data (synchronized exposure\nvideo frames and asynchronous pulse Event stream). Existing works typically\nemploy Transformer based networks to handle these modalities and achieve decent\naccuracy through input-level or feature-level fusion on multiple datasets.\nHowever, these trackers require significant memory consumption and\ncomputational complexity due to the use of self-attention mechanism. This paper\nproposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the\nState Space Model (SSM) to achieve high-performance tracking while effectively\nreducing computational costs and realizing more efficient tracking.\nSpecifically, we adopt two modality-specific Mamba backbone networks to extract\nthe features of RGB frames and Event streams. Then, we also propose to boost\nthe interactive learning between the RGB and Event features using the Mamba\nnetwork. The fused features will be fed into the tracking head for target\nobject localization. Extensive experiments on FELT and FE108 datasets fully\nvalidated the efficiency and effectiveness of our proposed tracker.\nSpecifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric,\nwhile the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost\nof ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about\n$9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB\nand 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We\nhope this work can bring some new insights to the tracking field and greatly\npromote the application of the Mamba architecture in tracking. The source code\nof this work will be released on\n\\url{https://github.com/Event-AHU/Mamba_FETrack}.",
      "tldr_zh": "这篇论文提出了 Mamba-FETrack，一种基于 State Space Model (SSM) 的 RGB-Event 跟踪框架，旨在高效整合异构多模态数据（RGB 帧和事件流），以解决现有 Transformer 网络的高计算复杂性和内存消耗问题。框架采用两个模态特定的 Mamba 骨干网络提取特征，并通过 Mamba 网络增强 RGB 和 Event 特征间的交互，最终将融合特征输入跟踪头进行目标定位。实验在 FELT 和 FE108 数据集上验证了其有效性，Mamba-FETrack 的 SR/PR 指标达到 43.5/55.6，比基于 ViT-S 的 OSTrack (40.0/50.9) 有所提升，同时内存消耗减少 9.5%，FLOPs 和参数分别减少 94.5% 和 88.3%。这项工作为跟踪领域带来新见解，并促进 Mamba 架构的应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "In Peer Review",
      "pdf_url": "http://arxiv.org/pdf/2404.18174v1",
      "published_date": "2024-04-28 13:12:49 UTC",
      "updated_date": "2024-04-28 13:12:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:20:38.591604"
    },
    {
      "arxiv_id": "2404.18161v1",
      "title": "IMEX-Reg: Implicit-Explicit Regularization in the Function Space for Continual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Prashant Bhat",
        "Bharath Renjith",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Continual learning (CL) remains one of the long-standing challenges for deep\nneural networks due to catastrophic forgetting of previously acquired\nknowledge. Although rehearsal-based approaches have been fairly successful in\nmitigating catastrophic forgetting, they suffer from overfitting on buffered\nsamples and prior information loss, hindering generalization under low-buffer\nregimes. Inspired by how humans learn using strong inductive biases, we propose\nIMEX-Reg to improve the generalization performance of experience rehearsal in\nCL under low buffer regimes. Specifically, we employ a two-pronged\nimplicit-explicit regularization approach using contrastive representation\nlearning (CRL) and consistency regularization. To further leverage the global\nrelationship between representations learned using CRL, we propose a\nregularization strategy to guide the classifier toward the activation\ncorrelations in the unit hypersphere of the CRL. Our results show that IMEX-Reg\nsignificantly improves generalization performance and outperforms\nrehearsal-based approaches in several CL scenarios. It is also robust to\nnatural and adversarial corruptions with less task-recency bias. Additionally,\nwe provide theoretical insights to support our design decisions further.",
      "tldr_zh": "该论文针对持续学习（CL）中深度神经网络的灾难性遗忘问题，提出IMEX-Reg方法，通过隐式-显式正则化策略改善基于经验回放的泛化性能，尤其在低缓冲区条件下。IMEX-Reg结合对比表示学习（CRL）和一致性正则化，并引入一个正则化策略来指导分类器利用CRL的单位超球面激活相关性，从而增强模型的鲁棒性和泛化能力。实验结果显示，IMEX-Reg在多种CL场景中优于现有基于回放的方法，对自然和对抗性破坏更具抵抗力，并减少了任务最近偏差；此外，论文提供了理论分析来支持其设计决策。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Transactions on Machine Learning Research",
      "pdf_url": "http://arxiv.org/pdf/2404.18161v1",
      "published_date": "2024-04-28 12:25:09 UTC",
      "updated_date": "2024-04-28 12:25:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:20:48.936136"
    },
    {
      "arxiv_id": "2404.18149v1",
      "title": "Compressed Deepfake Video Detection Based on 3D Spatiotemporal Trajectories",
      "title_zh": "基于 3D 时空轨迹的压缩",
      "authors": [
        "Zongmei Chen",
        "Xin Liao",
        "Xiaoshuai Wu",
        "Yanxiang Chen"
      ],
      "abstract": "The misuse of deepfake technology by malicious actors poses a potential\nthreat to nations, societies, and individuals. However, existing methods for\ndetecting deepfakes primarily focus on uncompressed videos, such as noise\ncharacteristics, local textures, or frequency statistics. When applied to\ncompressed videos, these methods experience a decrease in detection performance\nand are less suitable for real-world scenarios. In this paper, we propose a\ndeepfake video detection method based on 3D spatiotemporal trajectories.\nSpecifically, we utilize a robust 3D model to construct spatiotemporal motion\nfeatures, integrating feature details from both 2D and 3D frames to mitigate\nthe influence of large head rotation angles or insufficient lighting within\nframes. Furthermore, we separate facial expressions from head movements and\ndesign a sequential analysis method based on phase space motion trajectories to\nexplore the feature differences between genuine and fake faces in deepfake\nvideos. We conduct extensive experiments to validate the performance of our\nproposed method on several compressed deepfake benchmarks. The robustness of\nthe well-designed features is verified by calculating the consistent\ndistribution of facial landmarks before and after video compression.Our method\nyields satisfactory results and showcases its potential for practical\napplications.",
      "tldr_zh": "该论文针对现有 deepfake 视频检测方法在压缩视频上性能下降的问题，提出了一种基于 3D spatiotemporal trajectories 的检测方法，以提升真实场景的适用性。具体而言，该方法利用鲁棒的 3D 模型构建时空运动特征，整合 2D 和 3D 帧细节，并通过分离面部表情与头部运动的设计相空间运动轨迹顺序分析，探索真实和伪造面部的特征差异。实验在多个压缩 deepfake 基准上验证了方法的有效性，facial landmarks 的一致分布证明了其鲁棒性，并展示了在实际应用中的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18149v1",
      "published_date": "2024-04-28 11:48:13 UTC",
      "updated_date": "2024-04-28 11:48:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:21:02.405775"
    },
    {
      "arxiv_id": "2404.18144v1",
      "title": "Generative AI for Visualization: State of the Art and Future Directions",
      "title_zh": "生成式 AI 用于可视化",
      "authors": [
        "Yilin Ye",
        "Jianing Hao",
        "Yihan Hou",
        "Zhan Wang",
        "Shishi Xiao",
        "Yuyu Luo",
        "Wei Zeng"
      ],
      "abstract": "Generative AI (GenAI) has witnessed remarkable progress in recent years and\ndemonstrated impressive performance in various generation tasks in different\ndomains such as computer vision and computational design. Many researchers have\nattempted to integrate GenAI into visualization framework, leveraging the\nsuperior generative capacity for different operations. Concurrently, recent\nmajor breakthroughs in GenAI like diffusion model and large language model have\nalso drastically increase the potential of GenAI4VIS. From a technical\nperspective, this paper looks back on previous visualization studies leveraging\nGenAI and discusses the challenges and opportunities for future research.\nSpecifically, we cover the applications of different types of GenAI methods\nincluding sequence, tabular, spatial and graph generation techniques for\ndifferent tasks of visualization which we summarize into four major stages:\ndata enhancement, visual mapping generation, stylization and interaction. For\neach specific visualization sub-task, we illustrate the typical data and\nconcrete GenAI algorithms, aiming to provide in-depth understanding of the\nstate-of-the-art GenAI4VIS techniques and their limitations. Furthermore, based\non the survey, we discuss three major aspects of challenges and research\nopportunities including evaluation, dataset, and the gap between end-to-end\nGenAI and generative algorithms. By summarizing different generation\nalgorithms, their current applications and limitations, this paper endeavors to\nprovide useful insights for future GenAI4VIS research.",
      "tldr_zh": "这篇论文回顾了生成式 AI (GenAI) 在可视化领域的最新进展，探讨了其在数据增强、视觉映射生成、风格化和交互等四个主要阶段的应用，包括序列、表格、空间和图生成技术。作者总结了各种 GenAI 算法（如 diffusion model 和 large language model）在这些任务中的具体使用，并分析了它们的优势和局限性。论文还指出了未来研究的关键挑战和机会，如改进评估方法、构建高质量数据集，以及弥合端到端 GenAI 与传统生成算法之间的差距，以推动 GenAI4VIS 的发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18144v1",
      "published_date": "2024-04-28 11:27:30 UTC",
      "updated_date": "2024-04-28 11:27:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:21:12.742817"
    },
    {
      "arxiv_id": "2404.18134v2",
      "title": "Learning Fairer Representations with FairVIC",
      "title_zh": "翻译失败",
      "authors": [
        "Charmaine Barker",
        "Daniel Bethell",
        "Dimitar Kazakov"
      ],
      "abstract": "Mitigating bias in automated decision-making systems, particularly in deep\nlearning models, is a critical challenge due to nuanced definitions of\nfairness, dataset-specific biases, and the inherent trade-off between fairness\nand accuracy. To address these issues, we introduce FairVIC, an innovative\napproach that enhances fairness in neural networks by integrating variance,\ninvariance, and covariance terms into the loss function during training. Unlike\nmethods that rely on predefined fairness criteria, FairVIC abstracts fairness\nconcepts to minimise dependency on protected characteristics. We evaluate\nFairVIC against comparable bias mitigation techniques on benchmark datasets,\nconsidering both group and individual fairness, and conduct an ablation study\non the accuracy-fairness trade-off. FairVIC demonstrates significant\nimprovements ($\\approx70\\%$) in fairness across all tested metrics without\ncompromising accuracy, thus offering a robust, generalisable solution for fair\ndeep learning across diverse tasks and datasets.",
      "tldr_zh": "这篇论文介绍了FairVIC，一种创新方法，通过将variance, invariance和covariance项整合到损失函数中，来缓解深度学习模型中与公平性相关的偏见问题。FairVIC不依赖预定义的公平标准，而是通过最小化对受保护特征的依赖，实现对公平概念的抽象化处理。在基准数据集上的实验中，FairVIC在群体和个体公平性指标上实现了约70%的显著改善，同时保持了模型的准确性，提供了一个鲁棒且可泛化的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18134v2",
      "published_date": "2024-04-28 10:10:21 UTC",
      "updated_date": "2025-02-03 12:49:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:21:23.072104"
    },
    {
      "arxiv_id": "2404.18130v2",
      "title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
      "title_zh": "翻译失败",
      "authors": [
        "Hanmeng Liu",
        "Zhiyang Teng",
        "Chaoli Zhang",
        "Yue Zhang"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for\naugmenting the inferential capabilities of language models during reasoning\ntasks. Despite its advancements, CoT often grapples with challenges in\nvalidating reasoning validity and ensuring informativeness. Addressing these\nlimitations, this paper introduces the Logic Agent (LA), an agent-based\nframework aimed at enhancing the validity of reasoning processes in Large\nLanguage Models (LLMs) through strategic logic rule invocation. Unlike\nconventional approaches, LA transforms LLMs into logic agents that dynamically\napply propositional logic rules, initiating the reasoning process by converting\nnatural language inputs into structured logic forms. The logic agent leverages\na comprehensive set of predefined functions to systematically navigate the\nreasoning process. This methodology not only promotes the structured and\ncoherent generation of reasoning constructs but also significantly improves\ntheir interpretability and logical coherence. Through extensive\nexperimentation, we demonstrate LA's capacity to scale effectively across\nvarious model sizes, markedly improving the precision of complex reasoning\nacross diverse tasks.",
      "tldr_zh": "这篇论文针对 Chain-of-Thought (CoT) 提示在推理任务中验证有效性和确保信息性的挑战，引入了 Logic Agent (LA) 框架，以提升 Large Language Models (LLMs) 的推理质量。LA 将 LLMs 转化为逻辑代理，通过动态调用命题逻辑规则，将自然语言输入转换为结构化逻辑形式，并利用一组预定义函数系统化地导航推理过程，从而提高推理的结构化、可解释性和逻辑连贯性。实验结果显示，LA 在不同模型大小上有效扩展，并显著提升了复杂推理任务的精确性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "The experiment is subject to certain errors",
      "pdf_url": "http://arxiv.org/pdf/2404.18130v2",
      "published_date": "2024-04-28 10:02:28 UTC",
      "updated_date": "2024-12-06 01:34:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:21:35.843896"
    },
    {
      "arxiv_id": "2404.18955v1",
      "title": "GARA: A novel approach to Improve Genetic Algorithms' Accuracy and Efficiency by Utilizing Relationships among Genes",
      "title_zh": "GARA：一种利用基因间关系改进遗传算法准确性和效率的新方法",
      "authors": [
        "Zhaoning Shi",
        "Meng Xiang",
        "Zhaoyang Hai",
        "Xiabi Liu",
        "Yan Pei"
      ],
      "abstract": "Genetic algorithms have played an important role in engineering optimization.\nTraditional GAs treat each gene separately. However, biophysical studies of\ngene regulatory networks revealed direct associations between different genes.\nIt inspires us to propose an improvement to GA in this paper, Gene Regulatory\nGenetic Algorithm (GRGA), which, to our best knowledge, is the first time to\nutilize relationships among genes for improving GA's accuracy and efficiency.\nWe design a directed multipartite graph encapsulating the solution space,\ncalled RGGR, where each node corresponds to a gene in the solution and the edge\nrepresents the relationship between adjacent nodes. The edge's weight reflects\nthe relationship degree and is updated based on the idea that the edges'\nweights in a complete chain as candidate solution with acceptable or\nunacceptable performance should be strengthened or reduced, respectively. The\nobtained RGGR is then employed to determine appropriate loci of crossover and\nmutation operators, thereby directing the evolutionary process toward faster\nand better convergence. We analyze and validate our proposed GRGA approach in a\nsingle-objective multimodal optimization problem, and further test it on three\ntypes of applications, including feature selection, text summarization, and\ndimensionality reduction. Results illustrate that our GARA is effective and\npromising.",
      "tldr_zh": "本研究提出了一种新型遗传算法改进方法，名为GARA（Gene Regulatory Genetic Algorithm），首次利用基因间关系来提升遗传算法（GAs）的准确性和效率。GARA 通过构建一个有向多部分图RGGR来封装解决方案空间，其中节点代表基因，边及其权重表示基因间的关联强度，并根据候选解决方案的表现动态更新权重，以指导交叉和变异操作的方向。实验结果显示，在单目标多模态优化问题以及特征选择、文本摘要和降维等应用中，GARA 显著提高了算法的收敛速度和性能，证明了其有效性和潜力。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18955v1",
      "published_date": "2024-04-28 08:33:39 UTC",
      "updated_date": "2024-04-28 08:33:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:21:46.777939"
    },
    {
      "arxiv_id": "2404.18094v1",
      "title": "USAT: A Universal Speaker-Adaptive Text-to-Speech Approach",
      "title_zh": "USAT：一种通用的说话者自适应文本到语音方法",
      "authors": [
        "Wenbin Wang",
        "Yang Song",
        "Sanjay Jha"
      ],
      "abstract": "Conventional text-to-speech (TTS) research has predominantly focused on\nenhancing the quality of synthesized speech for speakers in the training\ndataset. The challenge of synthesizing lifelike speech for unseen,\nout-of-dataset speakers, especially those with limited reference data, remains\na significant and unresolved problem. While zero-shot or few-shot\nspeaker-adaptive TTS approaches have been explored, they have many limitations.\nZero-shot approaches tend to suffer from insufficient generalization\nperformance to reproduce the voice of speakers with heavy accents. While\nfew-shot methods can reproduce highly varying accents, they bring a significant\nstorage burden and the risk of overfitting and catastrophic forgetting. In\naddition, prior approaches only provide either zero-shot or few-shot\nadaptation, constraining their utility across varied real-world scenarios with\ndifferent demands. Besides, most current evaluations of speaker-adaptive TTS\nare conducted only on datasets of native speakers, inadvertently neglecting a\nvast portion of non-native speakers with diverse accents. Our proposed\nframework unifies both zero-shot and few-shot speaker adaptation strategies,\nwhich we term as \"instant\" and \"fine-grained\" adaptations based on their\nmerits. To alleviate the insufficient generalization performance observed in\nzero-shot speaker adaptation, we designed two innovative discriminators and\nintroduced a memory mechanism for the speech decoder. To prevent catastrophic\nforgetting and reduce storage implications for few-shot speaker adaptation, we\ndesigned two adapters and a unique adaptation procedure.",
      "tldr_zh": "本文提出 USAT，一种通用说话者自适应文本到语音 (TTS) 框架，旨在统一 zero-shot（零样本）和 few-shot（少样本）适应策略，分别称为“instant”（即时）和“fine-grained”（精细）适应，以解决传统 TTS 在处理数据集外说话者，尤其是重口音或数据有限情况下的泛化性能不足问题。框架通过设计两个创新鉴别器和一个内存机制来提升 zero-shot 适应的泛化能力，同时引入两个适配器和独特适应过程，以防止 few-shot 适应中的过拟合、灾难性遗忘和存储负担。总体上，USAT 框架扩展了 TTS 的适用场景，支持非母语者等多种口音，提高了真实世界应用的实用性和鲁棒性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "15 pages, 13 figures. Copyright has been transferred to IEEE",
      "pdf_url": "http://arxiv.org/pdf/2404.18094v1",
      "published_date": "2024-04-28 06:50:55 UTC",
      "updated_date": "2024-04-28 06:50:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:22:01.384968"
    },
    {
      "arxiv_id": "2404.18083v2",
      "title": "Online,Target-Free LiDAR-Camera Extrinsic Calibration via Cross-Modal Mask Matching",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiwei Huang",
        "Yikang Zhang",
        "Qijun Chen",
        "Rui Fan"
      ],
      "abstract": "LiDAR-camera extrinsic calibration (LCEC) is crucial for data fusion in\nintelligent vehicles. Offline, target-based approaches have long been the\npreferred choice in this field. However, they often demonstrate poor\nadaptability to real-world environments. This is largely because extrinsic\nparameters may change significantly due to moderate shocks or during extended\noperations in environments with vibrations. In contrast, online, target-free\napproaches provide greater adaptability yet typically lack robustness,\nprimarily due to the challenges in cross-modal feature matching. Therefore, in\nthis article, we unleash the full potential of large vision models (LVMs),\nwhich are emerging as a significant trend in the fields of computer vision and\nrobotics, especially for embodied artificial intelligence, to achieve robust\nand accurate online, target-free LCEC across a variety of challenging\nscenarios. Our main contributions are threefold: we introduce a novel framework\nknown as MIAS-LCEC, provide an open-source versatile calibration toolbox with\nan interactive visualization interface, and publish three real-world datasets\ncaptured from various indoor and outdoor environments. The cornerstone of our\nframework and toolbox is the cross-modal mask matching (C3M) algorithm,\ndeveloped based on a state-of-the-art (SoTA) LVM and capable of generating\nsufficient and reliable matches. Extensive experiments conducted on these\nreal-world datasets demonstrate the robustness of our approach and its superior\nperformance compared to SoTA methods, particularly for the solid-state LiDARs\nwith super-wide fields of view.",
      "tldr_zh": "本研究提出了一种在线、无目标的 LiDAR-Camera Extrinsic Calibration (LCEC) 方法，通过跨模态掩码匹配 (C3M) 算法利用大型视觉模型 (LVMs) 来实现鲁棒的跨模态特征匹配，解决传统方法在真实环境中的适应性和稳定性问题。核心框架 MIAS-LCEC 结合了该算法，提供了一个开源的多功能标定工具箱和交互式可视化界面，并发布了三个真实世界数据集，涵盖各种室内和室外场景。实验结果显示，该方法在这些数据集上显著优于现有 state-of-the-art (SoTA) 方法，尤其在固态 LiDAR 的超宽视场应用中，展现出更高的准确性和鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "accepted to IEEE Trans. on Intelligent Vehicles (T-IV)",
      "pdf_url": "http://arxiv.org/pdf/2404.18083v2",
      "published_date": "2024-04-28 06:25:56 UTC",
      "updated_date": "2024-06-20 03:20:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:22:12.487675"
    },
    {
      "arxiv_id": "2404.18081v2",
      "title": "ComposerX: Multi-Agent Symbolic Music Composition with LLMs",
      "title_zh": "ComposerX：多智能体符号化音乐作曲与大型语言模型",
      "authors": [
        "Qixin Deng",
        "Qikai Yang",
        "Ruibin Yuan",
        "Yipeng Huang",
        "Yi Wang",
        "Xubo Liu",
        "Zeyue Tian",
        "Jiahao Pan",
        "Ge Zhang",
        "Hanfeng Lin",
        "Yizhi Li",
        "Yinghao Ma",
        "Jie Fu",
        "Chenghua Lin",
        "Emmanouil Benetos",
        "Wenwu Wang",
        "Guangyu Xia",
        "Wei Xue",
        "Yike Guo"
      ],
      "abstract": "Music composition represents the creative side of humanity, and itself is a\ncomplex task that requires abilities to understand and generate information\nwith long dependency and harmony constraints. While demonstrating impressive\ncapabilities in STEM subjects, current LLMs easily fail in this task,\ngenerating ill-written music even when equipped with modern techniques like\nIn-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs'\npotential in music composition by leveraging their reasoning ability and the\nlarge knowledge base in music history and theory, we propose ComposerX, an\nagent-based symbolic music generation framework. We find that applying a\nmulti-agent approach significantly improves the music composition quality of\nGPT-4. The results demonstrate that ComposerX is capable of producing coherent\npolyphonic music compositions with captivating melodies, while adhering to user\ninstructions.",
      "tldr_zh": "这篇论文提出了 ComposerX，一种基于多代理框架的符号音乐生成系统，利用 LLMs 的推理能力和音乐历史理论知识，旨在解决当前 LLMs 在音乐创作中存在的长依赖性和和声约束问题。ComposerX 通过多代理方法显著提升了 GPT-4 的创作质量，超越了传统技术如 In-Context-Learning 和 Chain-of-Thoughts。实验结果表明，该框架能够生成连贯的多声部音乐，具有吸引人的旋律，并严格遵守用户指令。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18081v2",
      "published_date": "2024-04-28 06:17:42 UTC",
      "updated_date": "2024-04-30 14:14:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:22:23.368846"
    },
    {
      "arxiv_id": "2405.00066v1",
      "title": "Research and application of artificial intelligence based webshell detection model: A literature review",
      "title_zh": "基于人工智能的 webshell 检测模型的研究与应用：文献综述",
      "authors": [
        "Mingrui Ma",
        "Lansheng Han",
        "Chunjie Zhou"
      ],
      "abstract": "Webshell, as the \"culprit\" behind numerous network attacks, is one of the\nresearch hotspots in the field of cybersecurity. However, the complexity,\nstealthiness, and confusing nature of webshells pose significant challenges to\nthe corresponding detection schemes. With the rise of Artificial Intelligence\n(AI) technology, researchers have started to apply different intelligent\nalgorithms and neural network architectures to the task of webshell detection.\nHowever, the related research still lacks a systematic and standardized\nmethodological process, which is confusing and redundant. Therefore, following\nthe development timeline, we carefully summarize the progress of relevant\nresearch in this field, dividing it into three stages: Start Stage, Initial\nDevelopment Stage, and In-depth Development Stage. We further elaborate on the\nmain characteristics and core algorithms of each stage. In addition, we analyze\nthe pain points and challenges that still exist in this field and predict the\nfuture development trend of this field from our point of view. To the best of\nour knowledge, this is the first review that details the research related to\nAI-based webshell detection. It is also hoped that this paper can provide\ndetailed technical information for more researchers interested in AI-based\nwebshell detection tasks.",
      "tldr_zh": "这篇文献综述系统总结了基于 Artificial Intelligence (AI) 的 webshell 检测模型的研究与应用进展，按照时间线将其分为三个阶段：Start Stage、Initial Development Stage 和 In-depth Development Stage。论文详细阐述了每个阶段的主要特点、核心算法，以及 webshell 的复杂性、隐蔽性和混淆性带来的检测挑战。最终，它分析了当前领域的痛点和未来发展趋势，并作为首个全面综述，为 AI 相关研究者提供宝贵的技术参考。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "21 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.00066v1",
      "published_date": "2024-04-28 06:14:27 UTC",
      "updated_date": "2024-04-28 06:14:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:22:36.316877"
    },
    {
      "arxiv_id": "2404.18074v3",
      "title": "MMAC-Copilot: Multi-modal Agent Collaboration Operating Copilot",
      "title_zh": "翻译失败",
      "authors": [
        "Zirui Song",
        "Yaohang Li",
        "Meng Fang",
        "Yanda Li",
        "Zhenhao Chen",
        "Zecheng Shi",
        "Yuan Huang",
        "Xiuying Chen",
        "Ling Chen"
      ],
      "abstract": "Large language model agents that interact with PC applications often face\nlimitations due to their singular mode of interaction with real-world\nenvironments, leading to restricted versatility and frequent hallucinations. To\naddress this, we propose the Multi-Modal Agent Collaboration framework\n(MMAC-Copilot), a framework utilizes the collective expertise of diverse agents\nto enhance interaction ability with application. The framework introduces a\nteam collaboration chain, enabling each participating agent to contribute\ninsights based on their specific domain knowledge, effectively reducing the\nhallucination associated with knowledge domain gaps. We evaluate MMAC-Copilot\nusing the GAIA benchmark and our newly introduced Visual Interaction Benchmark\n(VIBench). MMAC-Copilot achieved exceptional performance on GAIA, with an\naverage improvement of 6.8\\% over existing leading systems. VIBench focuses on\nnon-API-interactable applications across various domains, including 3D gaming,\nrecreation, and office scenarios. It also demonstrated remarkable capability on\nVIBench. We hope this work can inspire in this field and provide a more\ncomprehensive assessment of Autonomous agents. The anonymous Github is\navailable at\n\\href{https://anonymous.4open.science/r/ComputerAgentWithVision-3C12}{Anonymous\nGithub}",
      "tldr_zh": "该研究提出 MMAC-Copilot 框架，通过多模态代理协作（Multi-Modal Agent Collaboration）来解决大型语言模型代理在与 PC 应用交互时的单一模式局限性和幻觉问题。\n框架引入团队协作 chain，让各代理基于其特定领域知识贡献见解，从而有效减少知识领域差距。\n在 GAIA benchmark 上，MMAC-Copilot 比现有领先系统平均提升 6.8%，并在新引入的 Visual Interaction Benchmark (VIBench) 上表现出色，后者针对非 API 可交互应用如 3D 游戏和办公室场景。\n这项工作旨在为自主代理领域提供更全面评估，并附有匿名 GitHub 资源。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Technical Reports",
      "pdf_url": "http://arxiv.org/pdf/2404.18074v3",
      "published_date": "2024-04-28 05:33:15 UTC",
      "updated_date": "2025-03-23 13:04:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:22:49.389657"
    },
    {
      "arxiv_id": "2404.18066v1",
      "title": "Quantized Context Based LIF Neurons for Recurrent Spiking Neural Networks in 45nm",
      "title_zh": "翻译失败",
      "authors": [
        "Sai Sukruth Bezugam",
        "Yihao Wu",
        "JaeBum Yoo",
        "Dmitri Strukov",
        "Bongjin Kim"
      ],
      "abstract": "In this study, we propose the first hardware implementation of a\ncontext-based recurrent spiking neural network (RSNN) emphasizing on\nintegrating dual information streams within the neocortical pyramidal neurons\nspecifically Context- Dependent Leaky Integrate and Fire (CLIF) neuron models,\nessential element in RSNN. We present a quantized version of the CLIF neuron\n(qCLIF), developed through a hardware-software codesign approach utilizing the\nsparse activity of RSNN. Implemented in a 45nm technology node, the qCLIF is\ncompact (900um^2) and achieves a high accuracy of 90% despite 8 bit\nquantization on DVS gesture classification dataset. Our analysis spans a\nnetwork configuration from 10 to 200 qCLIF neurons, supporting up to 82k\nsynapses within a 1.86 mm^2 footprint, demonstrating scalability and efficiency",
      "tldr_zh": "这篇论文提出了第一个基于硬件实现的上下文相关循环脉冲神经网络 (RSNN)，通过整合双信息流到上下文依赖泄露积分发放 (CLIF) 神经元模型，并开发了其量化版本 qCLIF，利用硬件-软件协同设计和 RSNN 的稀疏活动。qCLIF 在 45nm 工艺节点上实现，体积紧凑（900um^2），并在 8 位量化下于 DVS gesture classification 数据集上达到 90% 的准确率。研究分析了从 10 到 200 个 qCLIF 神经元的网络配置，支持高达 82k 突触，在 1.86 mm^2 面积内，展示了出色的可扩展性和效率。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.AR",
        "cs.CV",
        "q-bio.NC"
      ],
      "primary_category": "cs.NE",
      "comment": "7 Pages, 7 Figures, 2 Tables",
      "pdf_url": "http://arxiv.org/pdf/2404.18066v1",
      "published_date": "2024-04-28 04:32:44 UTC",
      "updated_date": "2024-04-28 04:32:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:23:01.195056"
    },
    {
      "arxiv_id": "2404.18065v1",
      "title": "Grounded Compositional and Diverse Text-to-3D with Pretrained Multi-View Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaolong Li",
        "Jiawei Mo",
        "Ying Wang",
        "Chethan Parameshwara",
        "Xiaohan Fei",
        "Ashwin Swaminathan",
        "CJ Taylor",
        "Zhuowen Tu",
        "Paolo Favaro",
        "Stefano Soatto"
      ],
      "abstract": "In this paper, we propose an effective two-stage approach named\nGrounded-Dreamer to generate 3D assets that can accurately follow complex,\ncompositional text prompts while achieving high fidelity by using a pre-trained\nmulti-view diffusion model. Multi-view diffusion models, such as MVDream, have\nshown to generate high-fidelity 3D assets using score distillation sampling\n(SDS). However, applied naively, these methods often fail to comprehend\ncompositional text prompts, and may often entirely omit certain subjects or\nparts. To address this issue, we first advocate leveraging text-guided 4-view\nimages as the bottleneck in the text-to-3D pipeline. We then introduce an\nattention refocusing mechanism to encourage text-aligned 4-view image\ngeneration, without the necessity to re-train the multi-view diffusion model or\ncraft a high-quality compositional 3D dataset. We further propose a hybrid\noptimization strategy to encourage synergy between the SDS loss and the sparse\nRGB reference images. Our method consistently outperforms previous\nstate-of-the-art (SOTA) methods in generating compositional 3D assets,\nexcelling in both quality and accuracy, and enabling diverse 3D from the same\ntext prompt.",
      "tldr_zh": "这篇论文提出了 Grounded-Dreamer，一种两阶段方法，用于生成能精确遵循复杂组合文本提示的高保真 3D 资产，基于预训练的多视图扩散模型（如 MVDream）。为了解决传统方法在理解组合文本时可能忽略某些元素的不足，该方法引入注意力重聚焦机制（attention refocusing mechanism），通过文本引导的 4 视图图像作为瓶颈来提升生成质量，而无需重新训练模型或构建高质量数据集。同时，论文提出混合优化策略（hybrid optimization strategy），促进 score distillation sampling (SDS) 损失与稀疏 RGB 参考图像的协同作用。实验结果表明，Grounded-Dreamer 在生成组合 3D 资产方面超越现有 SOTA 方法，在质量、准确性和多样性上表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18065v1",
      "published_date": "2024-04-28 04:05:10 UTC",
      "updated_date": "2024-04-28 04:05:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:23:13.598915"
    },
    {
      "arxiv_id": "2407.01551v1",
      "title": "Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational Text Data",
      "title_zh": "利用大型语言模型中的提示来克服复杂教育文本数据中的不平衡",
      "authors": [
        "Jeanne McClure",
        "Machi Shimmei",
        "Noboru Matsuda",
        "Shiyan Jiang"
      ],
      "abstract": "In this paper, we explore the potential of Large Language Models (LLMs) with\nassertions to mitigate imbalances in educational datasets. Traditional models\noften fall short in such contexts, particularly due to the complexity and\nnuanced nature of the data. This issue is especially prominent in the education\nsector, where cognitive engagement levels among students show significant\nvariation in their open responses. To test our hypothesis, we utilized an\nexisting technology for assertion-based prompt engineering through an\n'Iterative - ICL PE Design Process' comparing traditional Machine Learning (ML)\nmodels against LLMs augmented with assertions (N=135). Further, we conduct a\nsensitivity analysis on a subset (n=27), examining the variance in model\nperformance concerning classification metrics and cognitive engagement levels\nin each iteration. Our findings reveal that LLMs with assertions significantly\noutperform traditional ML models, particularly in cognitive engagement levels\nwith minority representation, registering up to a 32% increase in F1-score.\nAdditionally, our sensitivity study indicates that incorporating targeted\nassertions into the LLM tested on the subset enhances its performance by\n11.94%. This improvement primarily addresses errors stemming from the model's\nlimitations in understanding context and resolving lexical ambiguities in\nstudent responses.",
      "tldr_zh": "该研究探讨了利用 Large Language Models (LLMs) 结合 assertions 来缓解复杂教育文本数据中的失衡问题，特别是学生开放响应中认知参与水平的差异。作者采用 'Iterative - ICL PE Design Process' 的提示工程方法，比较了传统 Machine Learning (ML) 模型与增强 assertions 的 LLMs (N=135)，并通过敏感性分析 (n=27) 评估了模型在分类指标上的性能变化。结果显示，LLMs with assertions 在少数派认知参与水平上显著优于传统模型，F1-score 提高了高达 32%，并通过针对性 assertions 提升了 11.94% 的性能，主要解决了上下文理解和词汇歧义的局限性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "17 pages, 5 figures, 3 tables, 2 appendices",
      "pdf_url": "http://arxiv.org/pdf/2407.01551v1",
      "published_date": "2024-04-28 00:24:08 UTC",
      "updated_date": "2024-04-28 00:24:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:23:27.413289"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 45,
  "processed_papers_count": 45,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T04:23:46.156854"
}
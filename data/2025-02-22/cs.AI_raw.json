[
  {
    "arxiv_id": "2502.16380v1",
    "title": "Understanding Fixed Predictions via Confined Regions",
    "authors": [
      "Connor Lawless",
      "Tsui-Wei Weng",
      "Berk Ustun",
      "Madeleine Udell"
    ],
    "abstract": "Machine learning models are designed to predict outcomes using features about\nan individual, but fail to take into account how individuals can change them.\nConsequently, models can assign fixed predictions that deny individuals\nrecourse to change their outcome. This work develops a new paradigm to identify\nfixed predictions by finding confined regions in which all individuals receive\nfixed predictions. We introduce the first method, ReVer, for this task, using\ntools from mixed-integer quadratically constrained programming. Our approach\ncertifies recourse for out-of-sample data, provides interpretable descriptions\nof confined regions, and runs in seconds on real world datasets. We conduct a\ncomprehensive empirical study of confined regions across diverse applications.\nOur results highlight that existing point-wise verification methods fail to\ndiscover confined regions, while ReVer provably succeeds.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16380v1",
    "published_date": "2025-02-22 23:06:10 UTC",
    "updated_date": "2025-02-22 23:06:10 UTC"
  },
  {
    "arxiv_id": "2502.16378v1",
    "title": "Auto-ADMET: An Effective and Interpretable AutoML Method for Chemical ADMET Property Prediction",
    "authors": [
      "Alex G. C. de SÃ¡",
      "David B. Ascher"
    ],
    "abstract": "Machine learning (ML) has been playing important roles in drug discovery in\nthe past years by providing (pre-)screening tools for prioritising chemical\ncompounds to pass through wet lab experiments. One of the main ML tasks in drug\ndiscovery is to build quantitative structure-activity relationship (QSAR)\nmodels, associating the molecular structure of chemical compounds with an\nactivity or property. These properties -- including absorption, distribution,\nmetabolism, excretion and toxicity (ADMET) -- are essential to model compound\nbehaviour, activity and interactions in the organism. Although several methods\nexist, the majority of them do not provide an appropriate model's\npersonalisation, yielding to bias and lack of generalisation to new data since\nthe chemical space usually shifts from application to application. This fact\nleads to low predictive performance when completely new data is being tested by\nthe model. The area of Automated Machine Learning (AutoML) emerged aiming to\nsolve this issue, outputting tailored ML algorithms to the data at hand.\nAlthough an important task, AutoML has not been practically used to assist\ncheminformatics and computational chemistry researchers often, with just a few\nworks related to the field. To address these challenges, this work introduces\nAuto-ADMET, an interpretable evolutionary-based AutoML method for chemical\nADMET property prediction. Auto-ADMET employs a Grammar-based Genetic\nProgramming (GGP) method with a Bayesian Network Model to achieve comparable or\nbetter predictive performance against three alternative methods -- standard GGP\nmethod, pkCSM and XGBOOST model -- on 12 benchmark chemical ADMET property\nprediction datasets. The use of a Bayesian Network model on Auto-ADMET's\nevolutionary process assisted in both shaping the search procedure and\ninterpreting the causes of its AutoML performance.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16378v1",
    "published_date": "2025-02-22 22:54:08 UTC",
    "updated_date": "2025-02-22 22:54:08 UTC"
  },
  {
    "arxiv_id": "2502.16376v1",
    "title": "Does Your AI Agent Get You? A Personalizable Framework for Approximating Human Models from Argumentation-based Dialogue Traces",
    "authors": [
      "Yinxu Tang",
      "Stylianos Loukas Vasileiou",
      "William Yeoh"
    ],
    "abstract": "Explainable AI is increasingly employing argumentation methods to facilitate\ninteractive explanations between AI agents and human users. While existing\napproaches typically rely on predetermined human user models, there remains a\ncritical gap in dynamically learning and updating these models during\ninteractions. In this paper, we present a framework that enables AI agents to\nadapt their understanding of human users through argumentation-based dialogues.\nOur approach, called Persona, draws on prospect theory and integrates a\nprobability weighting function with a Bayesian belief update mechanism that\nrefines a probability distribution over possible human models based on\nexchanged arguments. Through empirical evaluations with human users in an\napplied argumentation setting, we demonstrate that Persona effectively captures\nevolving human beliefs, facilitates personalized interactions, and outperforms\nstate-of-the-art methods.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16376v1",
    "published_date": "2025-02-22 22:37:16 UTC",
    "updated_date": "2025-02-22 22:37:16 UTC"
  },
  {
    "arxiv_id": "2502.16375v1",
    "title": "Personhood Credentials: Human-Centered Design Recommendation Balancing Security, Usability, and Trust",
    "authors": [
      "Ayae Ide",
      "Tanusree Sharma"
    ],
    "abstract": "Building on related concepts, like, decentralized identifiers (DIDs), proof\nof personhood, anonymous credentials, personhood credentials (PHCs) emerged as\nan alternative approach, enabling individuals to verify to digital service\nproviders that they are a person without disclosing additional information.\nHowever, new technologies might introduce some friction due to users\nmisunderstandings and mismatched expectations. Despite their growing\nimportance, limited research has been done on users perceptions and preferences\nregarding PHCs. To address this gap, we conducted competitive analysis, and\nsemi-structured online user interviews with 23 participants from US and EU to\nprovide concrete design recommendations for PHCs that incorporate user needs,\nadoption rules, and preferences. Our study -- (a)surfaces how people reason\nabout unknown privacy and security guarantees of PHCs compared to current\nverification methods -- (b) presents the impact of several factors on how\npeople would like to onboard and manage PHCs, including, trusted issuers (e.g.\ngov), ground truth data to issue PHC (e.g biometrics, physical id), and\nissuance system (e.g. centralized vs decentralized). In a think-aloud\nconceptual design session, participants recommended -- conceptualized design,\nsuch as periodic biometrics verification, time-bound credentials, visually\ninteractive human-check, and supervision of government for issuance system. We\npropose actionable designs reflecting users preferences.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16375v1",
    "published_date": "2025-02-22 22:33:00 UTC",
    "updated_date": "2025-02-22 22:33:00 UTC"
  },
  {
    "arxiv_id": "2502.16366v2",
    "title": "A generative approach to LLM harmfulness detection with special red flag tokens",
    "authors": [
      "Sophie Xhonneux",
      "David Dobre",
      "Mehrnaz Mofakhami",
      "Leo Schwinn",
      "Gauthier Gidel"
    ],
    "abstract": "Most safety training methods for large language models (LLMs) based on\nfine-tuning rely on dramatically changing the output distribution of the model\nwhen faced with a harmful request, shifting it from an unsafe answer to a\nrefusal to respond. These methods inherently compromise model capabilities and\nmight make auto-regressive models vulnerable to attacks that make likely an\ninitial token of affirmative response. To avoid that, we propose to expand the\nmodel's vocabulary with a special token we call red flag token (<rf>) and\npropose to fine-tune the model to generate this token at any time harmful\ncontent is generated or about to be generated. This novel safety training\nmethod effectively augments LLMs into generative classifiers of harmfulness at\nall times during the conversation. This method offers several advantages: it\nenables the model to explicitly learn the concept of harmfulness while\nmarginally affecting the generated distribution, thus maintaining the model's\nutility. It also evaluates each generated answer rather than just the input\nprompt and provides a stronger defence against sampling-based attacks. In\naddition, it simplifies the evaluation of the model's robustness and reduces\ncorrelated failures when combined with a classifier. We further show an\nincreased robustness to long contexts, and supervised fine-tuning attacks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.16366v2",
    "published_date": "2025-02-22 21:48:48 UTC",
    "updated_date": "2025-03-05 20:31:47 UTC"
  },
  {
    "arxiv_id": "2502.16359v1",
    "title": "Audio Visual Segmentation Through Text Embeddings",
    "authors": [
      "Kyungbok Lee",
      "You Zhang",
      "Zhiyao Duan"
    ],
    "abstract": "The goal of Audio-Visual Segmentation (AVS) is to localize and segment the\nsounding source objects from the video frames. Researchers working on AVS\nsuffer from limited datasets because hand-crafted annotation is expensive.\nRecent works attempt to overcome the challenge of limited data by leveraging\nthe segmentation foundation model, SAM, prompting it with audio to enhance its\nability to segment sounding source objects. While this approach alleviates the\nmodel's burden on understanding visual modality by utilizing pre-trained\nknowledge of SAM, it does not address the fundamental challenge of the limited\ndataset for learning audio-visual relationships. To address these limitations,\nwe propose \\textbf{AV2T-SAM}, a novel framework that bridges audio features\nwith the text embedding space of pre-trained text-prompted SAM. Our method\nleverages multimodal correspondence learned from rich text-image paired\ndatasets to enhance audio-visual alignment. Furthermore, we introduce a novel\nfeature, $\\mathbf{\\textit{\\textbf{f}}_{CLIP} \\odot\n\\textit{\\textbf{f}}_{CLAP}}$, which emphasizes shared semantics of audio and\nvisual modalities while filtering irrelevant noise. Experiments on the AVSBench\ndataset demonstrate state-of-the-art performance on both datasets of AVSBench.\nOur approach outperforms existing methods by effectively utilizing pretrained\nsegmentation models and cross-modal semantic alignment.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16359v1",
    "published_date": "2025-02-22 21:15:44 UTC",
    "updated_date": "2025-02-22 21:15:44 UTC"
  },
  {
    "arxiv_id": "2502.17516v1",
    "title": "A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models",
    "authors": [
      "Zihao Lin",
      "Samyadeep Basu",
      "Mohammad Beigi",
      "Varun Manjunatha",
      "Ryan A. Rossi",
      "Zichao Wang",
      "Yufan Zhou",
      "Sriram Balasubramanian",
      "Arman Zarei",
      "Keivan Rezaei",
      "Ying Shen",
      "Barry Menglong Yao",
      "Zhiyang Xu",
      "Qin Liu",
      "Yuxiang Zhang",
      "Yan Sun",
      "Shilong Liu",
      "Li Shen",
      "Hongxuan Li",
      "Soheil Feizi",
      "Lifu Huang"
    ],
    "abstract": "The rise of foundation models has transformed machine learning research,\nprompting efforts to uncover their inner workings and develop more efficient\nand reliable applications for better control. While significant progress has\nbeen made in interpreting Large Language Models (LLMs), multimodal foundation\nmodels (MMFMs) - such as contrastive vision-language models, generative\nvision-language models, and text-to-image models - pose unique interpretability\nchallenges beyond unimodal frameworks. Despite initial studies, a substantial\ngap remains between the interpretability of LLMs and MMFMs. This survey\nexplores two key aspects: (1) the adaptation of LLM interpretability methods to\nmultimodal models and (2) understanding the mechanistic differences between\nunimodal language models and crossmodal systems. By systematically reviewing\ncurrent MMFM analysis techniques, we propose a structured taxonomy of\ninterpretability methods, compare insights across unimodal and multimodal\narchitectures, and highlight critical research gaps.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 4 Figures, 10 Tables",
    "pdf_url": "http://arxiv.org/pdf/2502.17516v1",
    "published_date": "2025-02-22 20:55:26 UTC",
    "updated_date": "2025-02-22 20:55:26 UTC"
  },
  {
    "arxiv_id": "2502.18515v1",
    "title": "A Multi-Agent Framework for Automated Vulnerability Detection and Repair in Solidity and Move Smart Contracts",
    "authors": [
      "Rabimba Karanjai",
      "Sam Blackshear",
      "Lei Xu",
      "Weidong Shi"
    ],
    "abstract": "The rapid growth of the blockchain ecosystem and the increasing value locked\nin smart contracts necessitate robust security measures. While languages like\nSolidity and Move aim to improve smart contract security, vulnerabilities\npersist. This paper presents Smartify, a novel multi-agent framework leveraging\nLarge Language Models (LLMs) to automatically detect and repair vulnerabilities\nin Solidity and Move smart contracts. Unlike traditional methods that rely\nsolely on vast pre-training datasets, Smartify employs a team of specialized\nagents working on different specially fine-tuned LLMs to analyze code based on\nunderlying programming concepts and language-specific security principles. We\nevaluated Smartify on a dataset for Solidity and a curated dataset for Move,\ndemonstrating its effectiveness in fixing a wide range of vulnerabilities. Our\nresults show that Smartify (Gemma2+codegemma) achieves state-of-the-art\nperformance, surpassing existing LLMs and enhancing general-purpose models'\ncapabilities, such as Llama 3.1. Notably, Smartify can incorporate\nlanguage-specific knowledge, such as the nuances of Move, without requiring\nmassive language-specific pre-training datasets. This work offers a detailed\nanalysis of various LLMs' performance on smart contract repair, highlighting\nthe strengths of our multi-agent approach and providing a blueprint for\ndeveloping more secure and reliable decentralized applications in the growing\nblockchain landscape. We also provide a detailed recipe for extending this to\nother similar use cases.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MA",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18515v1",
    "published_date": "2025-02-22 20:30:47 UTC",
    "updated_date": "2025-02-22 20:30:47 UTC"
  },
  {
    "arxiv_id": "2502.16344v1",
    "title": "Machine Learning-Based Cloud Computing Compliance Process Automation",
    "authors": [
      "Yuqing Wang",
      "Xiao Yang"
    ],
    "abstract": "Cloud computing adoption across industries has revolutionized enterprise\noperations while introducing significant challenges in compliance management.\nOrganizations must continuously meet evolving regulatory requirements such as\nGDPR and ISO 27001, yet traditional manual review processes have become\nincreasingly inadequate for modern business scales. This paper presents a novel\nmachine learning-based framework for automating cloud computing compliance\nprocesses, addressing critical challenges including resource-intensive manual\nreviews, extended compliance cycles, and delayed risk identification. Our\nproposed framework integrates multiple machine learning technologies, including\nBERT-based document processing (94.5% accuracy), One-Class SVM for anomaly\ndetection (88.7% accuracy), and an improved CNN-LSTM architecture for\nsequential compliance data analysis (90.2% accuracy). Implementation results\ndemonstrate significant improvements: reducing compliance process duration from\n7 days to 1.5 days, improving accuracy from 78% to 93%, and decreasing manual\neffort by 73.3%. A real-world deployment at a major securities firm validated\nthese results, processing 800,000 daily transactions with 94.2% accuracy in\nrisk identification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16344v1",
    "published_date": "2025-02-22 20:18:21 UTC",
    "updated_date": "2025-02-22 20:18:21 UTC"
  },
  {
    "arxiv_id": "2502.16343v1",
    "title": "Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading Agents",
    "authors": [
      "David Byrd"
    ],
    "abstract": "Companies across all economic sectors continue to deploy large language\nmodels at a rapid pace. Reinforcement learning is experiencing a resurgence of\ninterest due to its association with the fine-tuning of language models from\nhuman feedback. Tool-chain language models control task-specific agents; if the\nconverse has not already appeared, it soon will. In this paper, we present what\nwe believe is the first investigation of an intelligent trading agent based on\ncontinuous deep reinforcement learning that also controls a large language\nmodel with which it can post to a social media feed observed by other traders.\nWe empirically investigate the performance and impact of such an agent in a\nsimulated financial market, finding that it learns to optimize its total\nreward, and thereby augment its profit, by manipulating the sentiment of the\nposts it produces. The paper concludes with discussion, limitations, and\nsuggestions for future work.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16343v1",
    "published_date": "2025-02-22 20:17:14 UTC",
    "updated_date": "2025-02-22 20:17:14 UTC"
  },
  {
    "arxiv_id": "2502.16331v1",
    "title": "A Gap Between the Gaussian RKHS and Neural Networks: An Infinite-Center Asymptotic Analysis",
    "authors": [
      "Akash Kumar",
      "Rahul Parhi",
      "Mikhail Belkin"
    ],
    "abstract": "Recent works have characterized the function-space inductive bias of\ninfinite-width bounded-norm single-hidden-layer neural networks as a kind of\nbounded-variation-type space. This novel neural network Banach space\nencompasses many classical multivariate function spaces including certain\nSobolev spaces and the spectral Barron spaces. Notably, this Banach space also\nincludes functions that exhibit less classical regularity such as those that\nonly vary in a few directions. On bounded domains, it is well-established that\nthe Gaussian reproducing kernel Hilbert space (RKHS) strictly embeds into this\nBanach space, demonstrating a clear gap between the Gaussian RKHS and the\nneural network Banach space. It turns out that when investigating these spaces\non unbounded domains, e.g., all of $\\mathbb{R}^d$, the story is fundamentally\ndifferent. We establish the following fundamental result: Certain functions\nthat lie in the Gaussian RKHS have infinite norm in the neural network Banach\nspace. This provides a nontrivial gap between kernel methods and neural\nnetworks by the exhibition of functions in which kernel methods can do strictly\nbetter than neural networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2502.16331v1",
    "published_date": "2025-02-22 19:33:19 UTC",
    "updated_date": "2025-02-22 19:33:19 UTC"
  },
  {
    "arxiv_id": "2502.16324v1",
    "title": "Deep Time Warping for Multiple Time Series Alignment",
    "authors": [
      "Alireza Nourbakhsh",
      "Hoda Mohammadzade"
    ],
    "abstract": "Time Series Alignment is a critical task in signal processing with numerous\nreal-world applications. In practice, signals often exhibit temporal shifts and\nscaling, making classification on raw data prone to errors. This paper\nintroduces a novel approach for Multiple Time Series Alignment (MTSA)\nleveraging Deep Learning techniques. While most existing methods primarily\naddress Multiple Sequence Alignment (MSA) for protein and DNA sequences, there\nremains a significant gap in alignment methodologies for numerical time series.\nAdditionally, conventional approaches typically focus on pairwise alignment,\nwhereas our proposed method aligns all signals in a multiple manner (all the\nsignals are aligned together at once). This innovation not only enhances\nalignment efficiency but also significantly improves computational speed. By\ndecomposing into piece-wise linear sections, we introduce varying levels of\ncomplexity into the warping function. Additionally, our method ensures the\nsatisfaction of three warping constraints: boundary, monotonicity, and\ncontinuity conditions. The utilization of a deep convolutional network allows\nus to employ a new loss function, addressing some limitations of Dynamic Time\nWarping (DTW). Experimental results on the UCR Archive 2018, comprising 129\ntime series datasets, demonstrate that employing our approach to align signals\nsignificantly enhances classification accuracy and warping average and also\nreduces the run time across the majority of these datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "32 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.16324v1",
    "published_date": "2025-02-22 18:55:51 UTC",
    "updated_date": "2025-02-22 18:55:51 UTC"
  },
  {
    "arxiv_id": "2502.16320v1",
    "title": "Direct Alignment with Heterogeneous Preferences",
    "authors": [
      "Ali Shirali",
      "Arash Nasr-Esfahany",
      "Abdullah Alomar",
      "Parsa Mirtaheri",
      "Rediet Abebe",
      "Ariel Procaccia"
    ],
    "abstract": "Alignment with human preferences is commonly framed using a universal reward\nfunction, even though human preferences are inherently heterogeneous. We\nformalize this heterogeneity by introducing user types and examine the limits\nof the homogeneity assumption. We show that aligning to heterogeneous\npreferences with a single policy is best achieved using the average reward\nacross user types. However, this requires additional information about\nannotators. We examine improvements under different information settings,\nfocusing on direct alignment methods. We find that minimal information can\nyield first-order improvements, while full feedback from each user type leads\nto consistent learning of the optimal policy. Surprisingly, however, no\nsample-efficient consistent direct loss exists in this latter setting. These\nresults reveal a fundamental tension between consistency and sample efficiency\nin direct policy alignment.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16320v1",
    "published_date": "2025-02-22 18:46:33 UTC",
    "updated_date": "2025-02-22 18:46:33 UTC"
  },
  {
    "arxiv_id": "2502.16312v1",
    "title": "Iterative Auto-Annotation for Scientific Named Entity Recognition Using BERT-Based Models",
    "authors": [
      "Kartik Gupta"
    ],
    "abstract": "This paper presents an iterative approach to performing Scientific Named\nEntity Recognition (SciNER) using BERT-based models. We leverage transfer\nlearning to fine-tune pretrained models with a small but high-quality set of\nmanually annotated data. The process is iteratively refined by using the\nfine-tuned model to auto-annotate a larger dataset, followed by additional\nrounds of fine-tuning. We evaluated two models, dslim/bert-large-NER and\nbert-largecased, and found that bert-large-cased consistently outperformed the\nformer. Our approach demonstrated significant improvements in prediction\naccuracy and F1 scores, especially for less common entity classes. Future work\ncould include pertaining with unlabeled data, exploring more powerful encoders\nlike RoBERTa, and expanding the scope of manual annotations. This methodology\nhas broader applications in NLP tasks where access to labeled data is limited.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.16312v1",
    "published_date": "2025-02-22 17:58:20 UTC",
    "updated_date": "2025-02-22 17:58:20 UTC"
  },
  {
    "arxiv_id": "2502.16299v1",
    "title": "A calibration test for evaluating set-based epistemic uncertainty representations",
    "authors": [
      "Mira JÃ¼rgens",
      "Thomas Mortier",
      "Eyke HÃ¼llermeier",
      "Viktor Bengs",
      "Willem Waegeman"
    ],
    "abstract": "The accurate representation of epistemic uncertainty is a challenging yet\nessential task in machine learning. A widely used representation corresponds to\nconvex sets of probabilistic predictors, also known as credal sets. One popular\nway of constructing these credal sets is via ensembling or specialized\nsupervised learning methods, where the epistemic uncertainty can be quantified\nthrough measures such as the set size or the disagreement among members. In\nprinciple, these sets should contain the true data-generating distribution. As\na necessary condition for this validity, we adopt the strongest notion of\ncalibration as a proxy. Concretely, we propose a novel statistical test to\ndetermine whether there is a convex combination of the set's predictions that\nis calibrated in distribution. In contrast to previous methods, our framework\nallows the convex combination to be instance dependent, recognizing that\ndifferent ensemble members may be better calibrated in different regions of the\ninput space. Moreover, we learn this combination via proper scoring rules,\nwhich inherently optimize for calibration. Building on differentiable,\nkernel-based estimators of calibration errors, we introduce a nonparametric\ntesting procedure and demonstrate the benefits of capturing instance-level\nvariability on of synthetic and real-world experiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16299v1",
    "published_date": "2025-02-22 17:10:45 UTC",
    "updated_date": "2025-02-22 17:10:45 UTC"
  },
  {
    "arxiv_id": "2502.16294v1",
    "title": "TimePFN: Effective Multivariate Time Series Forecasting with Synthetic Data",
    "authors": [
      "Ege Onur Taga",
      "M. Emrullah Ildiz",
      "Samet Oymak"
    ],
    "abstract": "The diversity of time series applications and scarcity of domain-specific\ndata highlight the need for time-series models with strong few-shot learning\ncapabilities. In this work, we propose a novel training scheme and a\ntransformer-based architecture, collectively referred to as TimePFN, for\nmultivariate time-series (MTS) forecasting. TimePFN is based on the concept of\nPrior-data Fitted Networks (PFN), which aims to approximate Bayesian inference.\nOur approach consists of (1) generating synthetic MTS data through diverse\nGaussian process kernels and the linear coregionalization method, and (2) a\nnovel MTS architecture capable of utilizing both temporal and cross-channel\ndependencies across all input patches. We evaluate TimePFN on several benchmark\ndatasets and demonstrate that it outperforms the existing state-of-the-art\nmodels for MTS forecasting in both zero-shot and few-shot settings. Notably,\nfine-tuning TimePFN with as few as 500 data points nearly matches full dataset\ntraining error, and even 50 data points yield competitive results. We also find\nthat TimePFN exhibits strong univariate forecasting performance, attesting to\nits generalization ability. Overall, this work unlocks the power of synthetic\ndata priors for MTS forecasting and facilitates strong zero- and few-shot\nforecasting performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in AAAI-2025 as a conference paper",
    "pdf_url": "http://arxiv.org/pdf/2502.16294v1",
    "published_date": "2025-02-22 16:55:14 UTC",
    "updated_date": "2025-02-22 16:55:14 UTC"
  },
  {
    "arxiv_id": "2502.16291v2",
    "title": "The Design Space of Recent AI-assisted Research Tools for Ideation, Sensemaking, and Scientific Creativity",
    "authors": [
      "Runlong Ye",
      "Matthew Varona",
      "Oliver Huang",
      "Patrick Yung Kang Lee",
      "Michael Liut",
      "Carolina Nobre"
    ],
    "abstract": "Generative AI (GenAI) tools are radically expanding the scope and capability\nof automation in knowledge work such as academic research. While promising for\naugmenting cognition and streamlining processes, AI-assisted research tools may\nalso increase automation bias and hinder critical thinking. To examine recent\ndevelopments, we surveyed publications from leading HCI venues over the past\nthree years, closely analyzing thirteen tools to better understand the novel\ncapabilities of these AI-assisted systems and the design spaces they enable:\nseven employing traditional AI or customized transformer-based approaches, and\nsix integrating open-access large language models (LLMs). Our analysis\ncharacterizes the emerging design space, distinguishes between tools focused on\nworkflow mimicry versus generative exploration, and yields four critical design\nrecommendations to guide the development of future systems that foster\nmeaningful cognitive engagement: providing user agency and control,\ndifferentiating divergent/convergent thinking support, ensuring adaptability,\nand prioritizing transparency/accuracy. This work discusses how these insights\nsignal a shift from mere workflow replication towards generative co-creation,\npresenting new opportunities for the community to craft intuitive, AI-driven\nresearch interfaces and interactions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16291v2",
    "published_date": "2025-02-22 16:42:11 UTC",
    "updated_date": "2025-04-19 21:15:55 UTC"
  },
  {
    "arxiv_id": "2502.16286v1",
    "title": "Verification of Bit-Flip Attacks against Quantized Neural Networks",
    "authors": [
      "Yedi Zhang",
      "Lei Huang",
      "Pengfei Gao",
      "Fu Song",
      "Jun Sun",
      "Jin Song Dong"
    ],
    "abstract": "In the rapidly evolving landscape of neural network security, the resilience\nof neural networks against bit-flip attacks (i.e., an attacker maliciously\nflips an extremely small amount of bits within its parameter storage memory\nsystem to induce harmful behavior), has emerged as a relevant area of research.\nExisting studies suggest that quantization may serve as a viable defense\nagainst such attacks. Recognizing the documented susceptibility of real-valued\nneural networks to such attacks and the comparative robustness of quantized\nneural networks (QNNs), in this work, we introduce BFAVerifier, the first\nverification framework designed to formally verify the absence of bit-flip\nattacks or to identify all vulnerable parameters in a sound and rigorous\nmanner. BFAVerifier comprises two integral components: an abstraction-based\nmethod and an MILP-based method. Specifically, we first conduct a reachability\nanalysis with respect to symbolic parameters that represent the potential\nbit-flip attacks, based on a novel abstract domain with a sound guarantee. If\nthe reachability analysis fails to prove the resilience of such attacks, then\nwe encode this verification problem into an equivalent MILP problem which can\nbe solved by off-the-shelf solvers. Therefore, BFAVerifier is sound, complete,\nand reasonably efficient. We conduct extensive experiments, which demonstrate\nits effectiveness and efficiency across various network architectures,\nquantization bit-widths, and adversary capabilities.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "37 pages, 13 figures, 14 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.16286v1",
    "published_date": "2025-02-22 16:35:38 UTC",
    "updated_date": "2025-02-22 16:35:38 UTC"
  },
  {
    "arxiv_id": "2502.16284v1",
    "title": "MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra",
    "authors": [
      "Liang Wang",
      "Shaozhen Liu",
      "Yu Rong",
      "Deli Zhao",
      "Qiang Liu",
      "Shu Wu",
      "Liang Wang"
    ],
    "abstract": "Establishing the relationship between 3D structures and the energy states of\nmolecular systems has proven to be a promising approach for learning 3D\nmolecular representations. However, existing methods are limited to modeling\nthe molecular energy states from classical mechanics. This limitation results\nin a significant oversight of quantum mechanical effects, such as quantized\n(discrete) energy level structures, which offer a more accurate estimation of\nmolecular energy and can be experimentally measured through energy spectra. In\nthis paper, we propose to utilize the energy spectra to enhance the\npre-training of 3D molecular representations (MolSpectra), thereby infusing the\nknowledge of quantum mechanics into the molecular representations.\nSpecifically, we propose SpecFormer, a multi-spectrum encoder for encoding\nmolecular spectra via masked patch reconstruction. By further aligning outputs\nfrom the 3D encoder and spectrum encoder using a contrastive objective, we\nenhance the 3D encoder's understanding of molecules. Evaluations on public\nbenchmarks reveal that our pre-trained representations surpass existing methods\nin predicting molecular properties and modeling dynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.16284v1",
    "published_date": "2025-02-22 16:34:32 UTC",
    "updated_date": "2025-02-22 16:34:32 UTC"
  },
  {
    "arxiv_id": "2502.16282v1",
    "title": "Understanding the Emergence of Multimodal Representation Alignment",
    "authors": [
      "Megan Tjandrasuwita",
      "Chanakya Ekbote",
      "Liu Ziyin",
      "Paul Pu Liang"
    ],
    "abstract": "Multimodal representation learning is fundamentally about transforming\nincomparable modalities into comparable representations. While prior research\nprimarily focused on explicitly aligning these representations through targeted\nlearning objectives and model architectures, a recent line of work has found\nthat independently trained unimodal models of increasing scale and performance\ncan become implicitly aligned with each other. These findings raise fundamental\nquestions regarding the emergence of aligned representations in multimodal\nlearning. Specifically: (1) when and why does alignment emerge implicitly? and\n(2) is alignment a reliable indicator of performance? Through a comprehensive\nempirical investigation, we demonstrate that both the emergence of alignment\nand its relationship with task performance depend on several critical data\ncharacteristics. These include, but are not necessarily limited to, the degree\nof similarity between the modalities and the balance between redundant and\nunique information they provide for the task. Our findings suggest that\nalignment may not be universally beneficial; rather, its impact on performance\nvaries depending on the dataset and task. These insights can help practitioners\ndetermine whether increasing alignment between modalities is advantageous or,\nin some cases, detrimental to achieving optimal performance. Code is released\nat https://github.com/MeganTj/multimodal_alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 22 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.16282v1",
    "published_date": "2025-02-22 16:27:31 UTC",
    "updated_date": "2025-02-22 16:27:31 UTC"
  },
  {
    "arxiv_id": "2502.16280v1",
    "title": "Human Preferences in Large Language Model Latent Space: A Technical Analysis on the Reliability of Synthetic Data in Voting Outcome Prediction",
    "authors": [
      "Sarah Ball",
      "Simeon Allmendinger",
      "Frauke Kreuter",
      "Niklas KÃ¼hl"
    ],
    "abstract": "Generative AI (GenAI) is increasingly used in survey contexts to simulate\nhuman preferences. While many research endeavors evaluate the quality of\nsynthetic GenAI data by comparing model-generated responses to gold-standard\nsurvey results, fundamental questions about the validity and reliability of\nusing LLMs as substitutes for human respondents remain. Our study provides a\ntechnical analysis of how demographic attributes and prompt variations\ninfluence latent opinion mappings in large language models (LLMs) and evaluates\ntheir suitability for survey-based predictions. Using 14 different models, we\nfind that LLM-generated data fails to replicate the variance observed in\nreal-world human responses, particularly across demographic subgroups. In the\npolitical space, persona-to-party mappings exhibit limited differentiation,\nresulting in synthetic data that lacks the nuanced distribution of opinions\nfound in survey data. Moreover, we show that prompt sensitivity can\nsignificantly alter outputs for some models, further undermining the stability\nand predictiveness of LLM-based simulations. As a key contribution, we adapt a\nprobe-based methodology that reveals how LLMs encode political affiliations in\ntheir latent space, exposing the systematic distortions introduced by these\nmodels. Our findings highlight critical limitations in AI-generated survey\ndata, urging caution in its use for public opinion research, social science\nexperimentation, and computational behavioral modeling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16280v1",
    "published_date": "2025-02-22 16:25:33 UTC",
    "updated_date": "2025-02-22 16:25:33 UTC"
  },
  {
    "arxiv_id": "2502.16279v1",
    "title": "Beyond Trusting Trust: Multi-Model Validation for Robust Code Generation",
    "authors": [
      "Bradley McDanel"
    ],
    "abstract": "This paper explores the parallels between Thompson's \"Reflections on Trusting\nTrust\" and modern challenges in LLM-based code generation. We examine how\nThompson's insights about compiler backdoors take on new relevance in the era\nof large language models, where the mechanisms for potential exploitation are\neven more opaque and difficult to analyze. Building on this analogy, we discuss\nhow the statistical nature of LLMs creates novel security challenges in code\ngeneration pipelines. As a potential direction forward, we propose an\nensemble-based validation approach that leverages multiple independent models\nto detect anomalous code patterns through cross-model consensus. This\nperspective piece aims to spark discussion about trust and validation in\nAI-assisted software development.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "3 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.16279v1",
    "published_date": "2025-02-22 16:24:23 UTC",
    "updated_date": "2025-02-22 16:24:23 UTC"
  },
  {
    "arxiv_id": "2502.18512v1",
    "title": "FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression",
    "authors": [
      "Jianjian Li",
      "Junquan Fan",
      "Feng Tang",
      "Gang Huang",
      "Shitao Zhu",
      "Songlin Liu",
      "Nian Xie",
      "Wulong Liu",
      "Yong Liao"
    ],
    "abstract": "The rapid success of Vision Large Language Models (VLLMs) often depends on\nthe high-resolution images with abundant visual tokens, which hinders training\nand deployment efficiency. Current training-free visual token compression\nmethods exhibit serious performance degradation in tasks involving\nhigh-resolution, text-oriented image understanding and reasoning. In this\npaper, we propose an efficient visual token compression framework for\ntext-oriented VLLMs in high-resolution scenarios. In particular, we employ a\nlight-weight self-distillation pre-training stage to compress the visual\ntokens, requiring a limited numbers of image-text pairs and minimal learnable\nparameters. Afterwards, to mitigate potential performance degradation of\ntoken-compressed models, we construct a high-quality post-train stage. To\nvalidate the effectiveness of our method, we apply it to an advanced VLLMs,\nInternVL2. Experimental results show that our approach significantly reduces\ncomputational overhead while outperforming the baselines across a range of\ntext-oriented benchmarks. We will release the models and code soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 18 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.18512v1",
    "published_date": "2025-02-22 16:05:33 UTC",
    "updated_date": "2025-02-22 16:05:33 UTC"
  },
  {
    "arxiv_id": "2502.16274v1",
    "title": "Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation",
    "authors": [
      "Kartik Gupta"
    ],
    "abstract": "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and\nengaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated\ndataset of movie conversations. Due to the limitations in GPU computing and\nVRAM, the training process began with the 0.5B model progressively scaling up\nto the 1.5B and 3B versions as efficiency improvements were implemented. The\nQwen 2.5 series, developed by Alibaba Group, stands at the forefront of small\nopen-source pre-trained models, particularly excelling in creative tasks\ncompared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results\ndemonstrate the ability of small models to produce high-quality, realistic\ndialogue, offering a promising approach for real-time, context-sensitive\nconversation generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.16274v1",
    "published_date": "2025-02-22 16:00:01 UTC",
    "updated_date": "2025-02-22 16:00:01 UTC"
  },
  {
    "arxiv_id": "2502.16255v1",
    "title": "rECGnition_v2.0: Self-Attentive Canonical Fusion of ECG and Patient Data using deep learning for effective Cardiac Diagnostics",
    "authors": [
      "Shreya Srivastava",
      "Durgesh Kumar",
      "Ram Jiwari",
      "Sandeep Seth",
      "Deepak Sharma"
    ],
    "abstract": "The variability in ECG readings influenced by individual patient\ncharacteristics has posed a considerable challenge to adopting automated ECG\nanalysis in clinical settings. A novel feature fusion technique termed SACC\n(Self Attentive Canonical Correlation) was proposed to address this. This\ntechnique is combined with DPN (Dual Pathway Network) and depth-wise separable\nconvolution to create a robust, interpretable, and fast end-to-end arrhythmia\nclassification model named rECGnition_v2.0 (robust ECG abnormality detection).\nThis study uses MIT-BIH, INCARTDB and EDB dataset to evaluate the efficiency of\nrECGnition_v2.0 for various classes of arrhythmias. To investigate the\ninfluence of constituting model components, various ablation studies were\nperformed, i.e. simple concatenation, CCA and proposed SACC were compared,\nwhile the importance of global and local ECG features were tested using DPN\nrECGnition_v2.0 model and vice versa. It was also benchmarked with\nstate-of-the-art CNN models for overall accuracy vs model parameters, FLOPs,\nmemory requirements, and prediction time. Furthermore, the inner working of the\nmodel was interpreted by comparing the activation locations in ECG before and\nafter the SACC layer. rECGnition_v2.0 showed a remarkable accuracy of 98.07%\nand an F1-score of 98.05% for classifying ten distinct classes of arrhythmia\nwith just 82.7M FLOPs per sample, thereby going beyond the performance metrics\nof current state-of-the-art (SOTA) models by utilizing MIT-BIH Arrhythmia\ndataset. Similarly, on INCARTDB and EDB datasets, excellent F1-scores of 98.01%\nand 96.21% respectively was achieved for AAMI classification. The compact\narchitectural footprint of the rECGnition_v2.0, characterized by its lesser\ntrainable parameters and diminished computational demands, unfurled several\nadvantages including interpretability and scalability.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16255v1",
    "published_date": "2025-02-22 15:16:46 UTC",
    "updated_date": "2025-02-22 15:16:46 UTC"
  },
  {
    "arxiv_id": "2503.02889v1",
    "title": "Function-Coherent Gambles with Non-Additive Sequential Dynamics",
    "authors": [
      "Gregory Wheeler"
    ],
    "abstract": "The desirable gambles framework provides a rigorous foundation for imprecise\nprobability theory but relies heavily on linear utility via its coherence\naxioms. In our related work, we introduced function-coherent gambles to\naccommodate non-linear utility. However, when repeated gambles are played over\ntime -- especially in intertemporal choice where rewards compound\nmultiplicatively -- the standard additive combination axiom fails to capture\nthe appropriate long-run evaluation. In this paper we extend the framework by\nrelaxing the additive combination axiom and introducing a nonlinear combination\noperator that effectively aggregates repeated gambles in the log-domain. This\noperator preserves the time-average (geometric) growth rate and addresses the\nergodicity problem. We prove the key algebraic properties of the operator,\ndiscuss its impact on coherence, risk assessment, and representation, and\nprovide a series of illustrative examples. Our approach bridges the gap between\nexpectation values and time averages and unifies normative theory with\nempirically observed non-stationary reward dynamics.",
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.CE",
      "math.PR"
    ],
    "primary_category": "econ.TH",
    "comment": "10 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2503.02889v1",
    "published_date": "2025-02-22 14:58:20 UTC",
    "updated_date": "2025-02-22 14:58:20 UTC"
  },
  {
    "arxiv_id": "2502.17515v1",
    "title": "Towards User-level Private Reinforcement Learning with Human Feedback",
    "authors": [
      "Jiaming Zhang",
      "Mingxi Lei",
      "Meng Ding",
      "Mengdi Li",
      "Zihang Xiang",
      "Difei Xu",
      "Jinhui Xu",
      "Di Wang"
    ],
    "abstract": "Reinforcement Learning with Human Feedback (RLHF) has emerged as an\ninfluential technique, enabling the alignment of large language models (LLMs)\nwith human preferences. Despite the promising potential of RLHF, how to protect\nuser preference privacy has become a crucial issue. Most previous work has\nfocused on using differential privacy (DP) to protect the privacy of individual\ndata. However, they have concentrated primarily on item-level privacy\nprotection and have unsatisfactory performance for user-level privacy, which is\nmore common in RLHF. This study proposes a novel framework, AUP-RLHF, which\nintegrates user-level label DP into RLHF. We first show that the classical\nrandom response algorithm, which achieves an acceptable performance in\nitem-level privacy, leads to suboptimal utility when in the user-level\nsettings. We then establish a lower bound for the user-level label DP-RLHF and\ndevelop the AUP-RLHF algorithm, which guarantees $(\\varepsilon, \\delta)$\nuser-level privacy and achieves an improved estimation error. Experimental\nresults show that AUP-RLHF outperforms existing baseline methods in sentiment\ngeneration and summarization tasks, achieving a better privacy-utility\ntrade-off.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17515v1",
    "published_date": "2025-02-22 14:57:28 UTC",
    "updated_date": "2025-02-22 14:57:28 UTC"
  },
  {
    "arxiv_id": "2502.16249v1",
    "title": "Linear Attention for Efficient Bidirectional Sequence Modeling",
    "authors": [
      "Arshia Afzal",
      "Elias Abad Rocamora",
      "Leyla Naz Candogan",
      "Pol Puigdemont",
      "Francesco Tonin",
      "Yongtao Wu",
      "Mahsa Shoaran",
      "Volkan Cevher"
    ],
    "abstract": "Transformers with linear attention enable fast and parallel training.\nMoreover, they can be formulated as Recurrent Neural Networks (RNNs), for\nefficient linear-time inference. While extensively evaluated in causal sequence\nmodeling, they have yet to be extended to the bidirectional setting. This work\nintroduces the LION framework, establishing new theoretical foundations for\nlinear transformers in bidirectional sequence modeling. LION constructs a\nbidirectional RNN equivalent to full Linear Attention. This extends the\nbenefits of linear transformers: parallel training, and efficient inference,\ninto the bidirectional setting. Using LION, we cast three linear transformers\nto their bidirectional form: LION-LIT, the bidirectional variant corresponding\nto (Katharopoulos et al., 2020); LION-D, extending RetNet (Sun et al., 2023);\nand LION-S, a linear transformer with a stable selective mask inspired by\nselectivity of SSMs (Dao & Gu, 2024). Replacing the attention block with LION\n(-LIT, -D, -S) achieves performance on bidirectional tasks that approaches that\nof Transformers and State-Space Models (SSMs), while delivering significant\nimprovements in training speed. Our implementation is available in\nhttp://github.com/LIONS-EPFL/LION.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16249v1",
    "published_date": "2025-02-22 14:52:17 UTC",
    "updated_date": "2025-02-22 14:52:17 UTC"
  },
  {
    "arxiv_id": "2503.01855v2",
    "title": "Function-coherent gambles",
    "authors": [
      "Gregory Wheeler"
    ],
    "abstract": "The desirable gambles framework provides a foundational approach to imprecise\nprobability theory but relies heavily on linear utility assumptions. This paper\nintroduces function-coherent gambles, a generalization that accommodates\nnon-linear utility while preserving essential rationality properties. We\nestablish core axioms for function-coherence and prove a representation theorem\nthat characterizes acceptable gambles through continuous linear functionals.\nThe framework is then applied to analyze various forms of discounting in\nintertemporal choice, including hyperbolic, quasi-hyperbolic, scale-dependent,\nand state-dependent discounting. We demonstrate how these alternatives to\nconstant-rate exponential discounting can be integrated within the\nfunction-coherent framework. This unified treatment provides theoretical\nfoundations for modeling sophisticated patterns of time preference within the\ndesirability paradigm, bridging a gap between normative theory and observed\nbehavior in intertemporal decision-making under genuine uncertainty.",
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.CE",
      "math.PR"
    ],
    "primary_category": "econ.TH",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01855v2",
    "published_date": "2025-02-22 14:44:54 UTC",
    "updated_date": "2025-04-25 06:57:11 UTC"
  },
  {
    "arxiv_id": "2502.16242v1",
    "title": "Reproducibility Study of Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation",
    "authors": [
      "Jose L. Garcia",
      "Karolina Hajkova",
      "Maria Marchenko",
      "Carlos Miguel PatiÃ±o"
    ],
    "abstract": "This paper presents a reproducibility study and extension of \"Cooperation,\nCompetition, and Maliciousness: LLM-Stakeholders Interactive Negotiation.\" We\nvalidate the original findings using a range of open-weight models (1.5B-70B\nparameters) and GPT-4o Mini while introducing several novel contributions. We\nanalyze the Pareto front of the games, propose a communication-free baseline to\ntest whether successful negotiations are possible without agent interaction,\nevaluate recent small language models' performance, analyze structural\ninformation leakage in model responses, and implement an inequality metric to\nassess negotiation fairness. Our results demonstrate that smaller models (<10B\nparameters) struggle with format adherence and coherent responses, but larger\nopen-weight models can approach proprietary model performance. Additionally, in\nmany scenarios, single-agent approaches can achieve comparable results to\nmulti-agent negotiations, challenging assumptions about the necessity of agent\ncommunication to perform well on the benchmark. This work also provides\ninsights into the accessibility, fairness, environmental impact, and privacy\nconsiderations of LLM-based negotiation systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16242v1",
    "published_date": "2025-02-22 14:28:49 UTC",
    "updated_date": "2025-02-22 14:28:49 UTC"
  },
  {
    "arxiv_id": "2502.16240v1",
    "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
    "authors": [
      "Haoyang Li",
      "Jia Qi Yip",
      "Tianyu Fan",
      "Eng Siong Chng"
    ],
    "abstract": "Recent advancements in Neural Audio Codec (NAC) models have inspired their\nuse in various speech processing tasks, including speech enhancement (SE). In\nthis work, we propose a novel, efficient SE approach by leveraging the\npre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE\nmethods, which process discrete speech tokens using Language Models (LMs), we\nperform SE within the continuous embedding space of the pretrained NAC, which\nis highly compressed along the time dimension for efficient representation. Our\nlightweight SE model, optimized through an embedding-level loss, delivers\nresults comparable to SE baselines trained on larger datasets, with a\nsignificantly lower real-time factor of 0.005. Additionally, our method\nachieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer\nin a simulated cloud-based audio transmission environment. This work highlights\na new, efficient NAC-based SE solution, particularly suitable for cloud\napplications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other\nworks.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.16240v1",
    "published_date": "2025-02-22 14:25:55 UTC",
    "updated_date": "2025-02-22 14:25:55 UTC"
  },
  {
    "arxiv_id": "2502.17514v1",
    "title": "SAE-V: Interpreting Multimodal Models for Enhanced Alignment",
    "authors": [
      "Hantao Lou",
      "Changye Li",
      "Jiaming Ji",
      "Yaodong Yang"
    ],
    "abstract": "With the integration of image modality, the semantic space of multimodal\nlarge language models (MLLMs) is more complex than text-only models, making\ntheir interpretability more challenging and their alignment less stable,\nparticularly susceptible to low-quality data, which can lead to inconsistencies\nbetween modalities, hallucinations, and biased outputs. As a result, developing\ninterpretability methods for MLLMs is crucial for improving alignment quality\nand efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained\nattention for their ability to interpret latent representations. However,\nextending SAEs to multimodal settings presents new challenges due to modality\nfusion and the difficulty of isolating cross-modal representations. To address\nthese challenges, we introduce SAE-V, a mechanistic interpretability framework\nthat extends the SAE paradigm to MLLMs. By identifying and analyzing\ninterpretable features along with their corresponding data, SAE-V enables\nfine-grained interpretation of both model behavior and data quality,\nfacilitating a deeper understanding of cross-modal interactions and alignment\ndynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides\nan intrinsic data filtering mechanism to enhance model alignment without\nrequiring additional models. Specifically, when applied to the alignment\nprocess of MLLMs, SAE-V-based data filtering methods could achieve more than\n110% performance with less than 50% data. Our results highlight SAE-V's ability\nto enhance interpretability and alignment in MLLMs, providing insights into\ntheir internal mechanisms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17514v1",
    "published_date": "2025-02-22 14:20:07 UTC",
    "updated_date": "2025-02-22 14:20:07 UTC"
  },
  {
    "arxiv_id": "2502.16238v1",
    "title": "Brain-Model Evaluations Need the NeuroAI Turing Test",
    "authors": [
      "Jenelle Feather",
      "Meenakshi Khosla",
      "N. Apurva Ratan Murty",
      "Aran Nayebi"
    ],
    "abstract": "What makes an artificial system a good model of intelligence? The classical\ntest proposed by Alan Turing focuses on behavior, requiring that an artificial\nagent's behavior be indistinguishable from that of a human. While behavioral\nsimilarity provides a strong starting point, two systems with very different\ninternal representations can produce the same outputs. Thus, in modeling\nbiological intelligence, the field of NeuroAI often aims to go beyond\nbehavioral similarity and achieve representational convergence between a\nmodel's activations and the measured activity of a biological system. This\nposition paper argues that the standard definition of the Turing Test is\nincomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI\nTuring Test'', a benchmark that extends beyond behavior alone and\n\\emph{additionally} requires models to produce internal neural representations\nthat are empirically indistinguishable from those of a brain up to measured\nindividual variability, i.e. the differences between a computational model and\nthe brain is no more than the difference between one brain and another brain.\nWhile the brain is not necessarily the ceiling of intelligence, it remains the\nonly universally agreed-upon example, making it a natural reference point for\nevaluating computational models. By proposing this framework, we aim to shift\nthe discourse from loosely defined notions of brain inspiration to a systematic\nand testable standard centered on both behavior and internal representations,\nproviding a clear benchmark for neuroscientific modeling and AI development.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "q-bio.NC",
    "comment": "9 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.16238v1",
    "published_date": "2025-02-22 14:16:28 UTC",
    "updated_date": "2025-02-22 14:16:28 UTC"
  },
  {
    "arxiv_id": "2502.16235v2",
    "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
    "authors": [
      "Yifu Ding",
      "Wentao Jiang",
      "Shunyu Liu",
      "Yongcheng Jing",
      "Jinyang Guo",
      "Yingjie Wang",
      "Jing Zhang",
      "Zengmao Wang",
      "Ziwei Liu",
      "Bo Du",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "abstract": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.16235v2",
    "published_date": "2025-02-22 14:13:37 UTC",
    "updated_date": "2025-02-27 06:39:06 UTC"
  },
  {
    "arxiv_id": "2502.16233v1",
    "title": "Graph Self-Supervised Learning with Learnable Structural and Positional Encodings",
    "authors": [
      "Asiri Wijesinghe",
      "Hao Zhu",
      "Piotr Koniusz"
    ],
    "abstract": "Traditional Graph Self-Supervised Learning (GSSL) struggles to capture\ncomplex structural properties well. This limitation stems from two main\nfactors: (1) the inadequacy of conventional Graph Neural Networks (GNNs) in\nrepresenting sophisticated topological features, and (2) the focus of\nself-supervised learning solely on final graph representations. To address\nthese issues, we introduce \\emph{GenHopNet}, a GNN framework that integrates a\n$k$-hop message-passing scheme, enhancing its ability to capture local\nstructural information without explicit substructure extraction. We\ntheoretically demonstrate that \\emph{GenHopNet} surpasses the expressiveness of\nthe classical Weisfeiler-Lehman (WL) test for graph isomorphism. Furthermore,\nwe propose a structural- and positional-aware GSSL framework that incorporates\ntopological information throughout the learning process. This approach enables\nthe learning of representations that are both sensitive to graph topology and\ninvariant to specific structural and feature augmentations. Comprehensive\nexperiments on graph classification datasets, including those designed to test\nstructural sensitivity, show that our method consistently outperforms the\nexisting approaches and maintains computational efficiency. Our work\nsignificantly advances GSSL's capability in distinguishing graphs with similar\nlocal structures but different global topologies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is accepted by The World Wide Web Conference (WWW) 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.16233v1",
    "published_date": "2025-02-22 14:10:06 UTC",
    "updated_date": "2025-02-22 14:10:06 UTC"
  },
  {
    "arxiv_id": "2502.17513v2",
    "title": "Int2Int: a framework for mathematics with transformers",
    "authors": [
      "FranÃ§ois Charton"
    ],
    "abstract": "This paper documents Int2Int, an open source code base for using transformers\non problems of mathematical research, with a focus on number theory and other\nproblems involving integers. Int2Int is a complete PyTorch implementation of a\ntransformer architecture, together with training and evaluation loops, and\nclasses and functions to represent, generate and decode common mathematical\nobjects. Ancillary code for data preparation, and Jupyter Notebooks for\nvisualizing experimental results are also provided. This document presents the\nmain features of Int2Int, serves as its user manual, and provides guidelines on\nhow to extend it. Int2Int is released under the MIT licence, at\nhttps://github.com/f-charton/Int2Int.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17513v2",
    "published_date": "2025-02-22 13:43:28 UTC",
    "updated_date": "2025-03-24 19:11:58 UTC"
  },
  {
    "arxiv_id": "2503.04779v3",
    "title": "Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of LLMs on Formal Specification Inference",
    "authors": [
      "Thanh Le-Cong",
      "Bach Le",
      "Toby Murray"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly being used to automate\nprogramming tasks. Yet, LLMs' capabilities in reasoning about program semantics\nare still inadequately studied, leaving significant potential for further\nexploration. This paper introduces FormalBench, a comprehensive benchmark\ndesigned to evaluate LLMs' reasoning abilities on program semantics,\nparticularly via the task of synthesizing formal program specifications to\nassist verifying program correctness. This task requires both comprehensive\nreasoning over all possible program executions and the generation of precise,\nsyntactically correct expressions that adhere to formal syntax and semantics.\nUsing this benchmark, we evaluated the ability of LLMs in synthesizing\nconsistent and complete specifications. Our findings show that LLMs perform\nwell with simple control flows but struggle with more complex structures,\nespecially loops, even with advanced prompting. Additionally, LLMs exhibit\nlimited robustness against semantic-preserving transformations. We also\nhighlight common failure patterns and design self-repair prompts, improving\nsuccess rates by 25%.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04779v3",
    "published_date": "2025-02-22 13:27:31 UTC",
    "updated_date": "2025-03-15 10:45:06 UTC"
  },
  {
    "arxiv_id": "2502.18511v1",
    "title": "ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models",
    "authors": [
      "Xuxu Liu",
      "Siyuan Liang",
      "Mengya Han",
      "Yong Luo",
      "Aishan Liu",
      "Xiantao Cai",
      "Zheng He",
      "Dacheng Tao"
    ],
    "abstract": "Generative large language models are crucial in natural language processing,\nbut they are vulnerable to backdoor attacks, where subtle triggers compromise\ntheir behavior. Although backdoor attacks against LLMs are constantly emerging,\nexisting benchmarks remain limited in terms of sufficient coverage of attack,\nmetric system integrity, backdoor attack alignment. And existing pre-trained\nbackdoor attacks are idealized in practice due to resource access constraints.\nTherefore we establish $\\textit{ELBA-Bench}$, a comprehensive and unified\nframework that allows attackers to inject backdoor through parameter efficient\nfine-tuning ($\\textit{e.g.,}$ LoRA) or without fine-tuning techniques\n($\\textit{e.g.,}$ In-context-learning). $\\textit{ELBA-Bench}$ provides over\n1300 experiments encompassing the implementations of 12 attack methods, 18\ndatasets, and 12 LLMs. Extensive experiments provide new invaluable findings\ninto the strengths and limitations of various attack strategies. For instance,\nPEFT attack consistently outperform without fine-tuning approaches in\nclassification tasks while showing strong cross-dataset generalization with\noptimized triggers boosting robustness; Task-relevant backdoor optimization\ntechniques or attack prompts along with clean and adversarial demonstrations\ncan enhance backdoor attack success while preserving model performance on clean\nsamples. Additionally, we introduce a universal toolbox designed for\nstandardized backdoor attack research, with the goal of propelling further\nprogress in this vital area.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18511v1",
    "published_date": "2025-02-22 12:55:28 UTC",
    "updated_date": "2025-02-22 12:55:28 UTC"
  },
  {
    "arxiv_id": "2503.01854v1",
    "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
    "authors": [
      "Jiahui Geng",
      "Qing Li",
      "Herbert Woisetschlaeger",
      "Zongxiong Chen",
      "Yuxia Wang",
      "Preslav Nakov",
      "Hans-Arno Jacobsen",
      "Fakhri Karray"
    ],
    "abstract": "This study investigates the machine unlearning techniques within the context\nof large language models (LLMs), referred to as \\textit{LLM unlearning}. LLM\nunlearning offers a principled approach to removing the influence of\nundesirable data (e.g., sensitive or illegal information) from LLMs, while\npreserving their overall utility without requiring full retraining. Despite\ngrowing research interest, there is no comprehensive survey that systematically\norganizes existing work and distills key insights; here, we aim to bridge this\ngap. We begin by introducing the definition and the paradigms of LLM\nunlearning, followed by a comprehensive taxonomy of existing unlearning\nstudies. Next, we categorize current unlearning approaches, summarizing their\nstrengths and limitations. Additionally, we review evaluation metrics and\nbenchmarks, providing a structured overview of current assessment\nmethodologies. Finally, we outline promising directions for future research,\nhighlighting key challenges and opportunities in the field.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01854v1",
    "published_date": "2025-02-22 12:46:14 UTC",
    "updated_date": "2025-02-22 12:46:14 UTC"
  },
  {
    "arxiv_id": "2502.16214v2",
    "title": "SalM$^{2}$: An Extremely Lightweight Saliency Mamba Model for Real-Time Cognitive Awareness of Driver Attention",
    "authors": [
      "Chunyu Zhao",
      "Wentao Mu",
      "Xian Zhou",
      "Wenbo Liu",
      "Fei Yan",
      "Tao Deng"
    ],
    "abstract": "Driver attention recognition in driving scenarios is a popular direction in\ntraffic scene perception technology. It aims to understand human driver\nattention to focus on specific targets/objects in the driving scene. However,\ntraffic scenes contain not only a large amount of visual information but also\nsemantic information related to driving tasks. Existing methods lack attention\nto the actual semantic information present in driving scenes. Additionally, the\ntraffic scene is a complex and dynamic process that requires constant attention\nto objects related to the current driving task. Existing models, influenced by\ntheir foundational frameworks, tend to have large parameter counts and complex\nstructures. Therefore, this paper proposes a real-time saliency Mamba network\nbased on the latest Mamba framework. As shown in Figure 1, our model uses very\nfew parameters (0.08M, only 0.09~11.16% of other models), while maintaining\nSOTA performance or achieving over 98% of the SOTA model's performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The article has been accepted for publication at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.16214v2",
    "published_date": "2025-02-22 12:37:52 UTC",
    "updated_date": "2025-02-28 03:11:00 UTC"
  },
  {
    "arxiv_id": "2502.16203v1",
    "title": "Machine Learning Framework for Early Power, Performance, and Area Estimation of RTL",
    "authors": [
      "Anindita Chattopadhyay",
      "Vijay Kumar Sutrakar"
    ],
    "abstract": "A critical stage in the evolving landscape of VLSI design is the design phase\nthat is transformed into register-transfer level (RTL), which specifies system\nfunctionality through hardware description languages like Verilog. Generally,\nevaluating the quality of an RTL design demands full synthesis via electronic\ndesign automation (EDA) tool is time-consuming process that is not well-suited\nto rapid design iteration and optimization. Although recent breakthroughs in\nmachine Learning (ML) have brought early prediction models, these methods\nusually do not provide robust and generalizable solutions with respect to a\nwide range of RTL designs. This paper proposes a pre-synthesis framework that\nmakes early estimation of power, performance and area (PPA) metrics directly\nfrom the hardware description language (HDL) code making direct use of library\nfiles instead of toggle files. The proposed framework introduces a bit-level\nrepresentation referred to as the simple operator graph (SOG), which uses\nsingle-bit operators to generate a generalized and flexible structure that\nclosely mirrors the characteristics of post synthesis design. The proposed\nmodel bridges the RTL and post-synthesis design, which will help in precisely\npredicting key metrics. The proposed tree-based ML framework shows superior\npredictive performance PPA estimation. Validation is carried out on 147\ndistinct RTL designs. The proposed model with 147 different designs shows\naccuracy of 98%, 98%, and 90% for WNS, TNS and power, respectively, indicates\nsignificant accuracy improvements relative to state-of-the-art methods.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16203v1",
    "published_date": "2025-02-22 12:12:51 UTC",
    "updated_date": "2025-02-22 12:12:51 UTC"
  },
  {
    "arxiv_id": "2502.16198v1",
    "title": "An Autonomous Network Orchestration Framework Integrating Large Language Models with Continual Reinforcement Learning",
    "authors": [
      "Masoud Shokrnezhad",
      "Tarik Taleb"
    ],
    "abstract": "6G networks aim to achieve global coverage, massive connectivity, and\nultra-stringent requirements. Space-Air-Ground Integrated Networks (SAGINs) and\nSemantic Communication (SemCom) are essential for realizing these goals, yet\nthey introduce considerable complexity in resource orchestration. Drawing\ninspiration from research in robotics, a viable solution to manage this\ncomplexity is the application of Large Language Models (LLMs). Although the use\nof LLMs in network orchestration has recently gained attention, existing\nsolutions have not sufficiently addressed LLM hallucinations or their\nadaptation to network dynamics. To address this gap, this paper proposes a\nframework called Autonomous Reinforcement Coordination (ARC) for a\nSemCom-enabled SAGIN. This framework employs an LLM-based Retrieval-Augmented\nGenerator (RAG) monitors services, users, and resources and processes the\ncollected data, while a Hierarchical Action Planner (HAP) orchestrates\nresources. ARC decomposes orchestration into two tiers, utilizing LLMs for\nhigh-level planning and Reinforcement Learning (RL) agents for low-level\ndecision-making, in alignment with the Mixture of Experts (MoE) concept. The\nLLMs utilize Chain-of-Thought (CoT) reasoning for few-shot learning, empowered\nby contrastive learning, while the RL agents employ replay buffer management\nfor continual learning, thereby achieving efficiency, accuracy, and\nadaptability. Simulations are provided to demonstrate the effectiveness of ARC,\nalong with a comprehensive discussion on potential future research directions\nto enhance and upgrade ARC.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "IEEE Communications Magazine",
    "pdf_url": "http://arxiv.org/pdf/2502.16198v1",
    "published_date": "2025-02-22 11:53:34 UTC",
    "updated_date": "2025-02-22 11:53:34 UTC"
  },
  {
    "arxiv_id": "2502.16184v1",
    "title": "Robustness and Cybersecurity in the EU Artificial Intelligence Act",
    "authors": [
      "Henrik Nolte",
      "Miriam Rateike",
      "MichÃ¨le Finck"
    ],
    "abstract": "The EU Artificial Intelligence Act (AIA) establishes different legal\nprinciples for different types of AI systems. While prior work has sought to\nclarify some of these principles, little attention has been paid to robustness\nand cybersecurity. This paper aims to fill this gap. We identify legal\nchallenges and shortcomings in provisions related to robustness and\ncybersecurity for high-risk AI systems (Art. 15 AIA) and general-purpose AI\nmodels (Art. 55 AIA). We show that robustness and cybersecurity demand\nresilience against performance disruptions. Furthermore, we assess potential\nchallenges in implementing these provisions in light of recent advancements in\nthe machine learning (ML) literature. Our analysis informs efforts to develop\nharmonized standards, guidelines by the European Commission, as well as\nbenchmarks and measurement methodologies under Art. 15(2) AIA. With this, we\nseek to bridge the gap between legal terminology and ML research, fostering a\nbetter alignment between research and implementation efforts.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16184v1",
    "published_date": "2025-02-22 11:12:20 UTC",
    "updated_date": "2025-02-22 11:12:20 UTC"
  },
  {
    "arxiv_id": "2502.16181v1",
    "title": "BiDeV: Bilateral Defusing Verification for Complex Claim Fact-Checking",
    "authors": [
      "Yuxuan Liu",
      "Hongda Sun",
      "Wenya Guo",
      "Xinyan Xiao",
      "Cunli Mao",
      "Zhengtao Yu",
      "Rui Yan"
    ],
    "abstract": "Complex claim fact-checking performs a crucial role in disinformation\ndetection. However, existing fact-checking methods struggle with claim\nvagueness, specifically in effectively handling latent information and complex\nrelations within claims. Moreover, evidence redundancy, where nonessential\ninformation complicates the verification process, remains a significant issue.\nTo tackle these limitations, we propose Bilateral Defusing Verification\n(BiDeV), a novel fact-checking working-flow framework integrating multiple\nrole-played LLMs to mimic the human-expert fact-checking process. BiDeV\nconsists of two main modules: Vagueness Defusing identifies latent information\nand resolves complex relations to simplify the claim, and Redundancy Defusing\neliminates redundant content to enhance the evidence quality. Extensive\nexperimental results on two widely used challenging fact-checking benchmarks\n(Hover and Feverous-s) demonstrate that our BiDeV can achieve the best\nperformance under both gold and open settings. This highlights the\neffectiveness of BiDeV in handling complex claims and ensuring precise\nfact-checking",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2502.16181v1",
    "published_date": "2025-02-22 10:58:40 UTC",
    "updated_date": "2025-02-22 10:58:40 UTC"
  },
  {
    "arxiv_id": "2503.05758v1",
    "title": "ADAPT Centre Contribution on Implementation of the EU AI Act and Fundamental Right Protection",
    "authors": [
      "Dave Lewis",
      "Marta Lasek-Markey",
      "Harshvardhan J. Pandit",
      "Delaram Golpayegani",
      "Darren McCabe",
      "Louise McCormack",
      "Joshua Hovsha",
      "Deirdre Ahern",
      "Arthit Suriyawongku"
    ],
    "abstract": "This document represents the ADAPT Centre's submission to the Irish\nDepartment of Enterprise, Trade and Employment (DETE) regarding the public\nconsultation on implementation of the EU AI Act.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05758v1",
    "published_date": "2025-02-22 10:54:38 UTC",
    "updated_date": "2025-02-22 10:54:38 UTC"
  },
  {
    "arxiv_id": "2503.05757v1",
    "title": "Uncertainty-Aware Fusion: An Ensemble Framework for Mitigating Hallucinations in Large Language Models",
    "authors": [
      "Prasenjit Dey",
      "Srujana Merugu",
      "Sivaramakrishnan Kaveri"
    ],
    "abstract": "Large Language Models (LLMs) are known to hallucinate and generate\nnon-factual outputs which can undermine user trust. Traditional methods to\ndirectly mitigate hallucinations, such as representation editing and\ncontrastive decoding, often require additional training data and involve high\nimplementation complexity. While ensemble-based approaches harness multiple\nLLMs to tap into the \"wisdom of crowds\", these methods overlook uncertainties\nin individual model responses. Recent studies reveal that uncertainty\nestimation can enable LLMs to self-assess the likelihood of generating\nhallucinations. In this work, we focus on factoid question answering (QA) and\nobserve that LLMs accuracy and self-assessment capabilities vary widely with\ndifferent models excelling in different scenarios. Leveraging this insight, we\npropose Uncertainty-Aware Fusion (UAF), an ensemble framework to reduces\nhallucinations by strategically combining multiple LLM based on their accuracy\nand self-assessment abilities. Empirical results on several public benchmark\ndatasets show that UAF outperforms state-of-the-art hallucination mitigation\nmethods by $8\\%$ in factual accuracy, while either narrowing or surpassing the\nperformance gap with GPT-4.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the ACM Web Conference 2025, WWW 25",
    "pdf_url": "http://arxiv.org/pdf/2503.05757v1",
    "published_date": "2025-02-22 10:48:18 UTC",
    "updated_date": "2025-02-22 10:48:18 UTC"
  },
  {
    "arxiv_id": "2502.16176v2",
    "title": "An End-to-End Homomorphically Encrypted Neural Network",
    "authors": [
      "Marcos Florencio",
      "Luiz Alencar",
      "Bianca Lima"
    ],
    "abstract": "Every commercially available, state-of-the-art neural network consume plain\ninput data, which is a well-known privacy concern. We propose a new\narchitecture based on homomorphic encryption, which allows the neural network\nto operate on encrypted data. We show that Homomorphic Neural Networks (HNN)\ncan achieve full privacy and security while maintaining levels of accuracy\ncomparable to plain neural networks. We also introduce a new layer, the\nDifferentiable Soft-Argmax, which allows the calibration of output logits in\nthe encrypted domain, raising the entropy of the activation parameters, thus\nimproving the security of the model, while keeping the overall noise below the\nacceptable noise budget. Experiments were conducted using the Stanford\nSentiment Treebank (SST-2) corpora on the DistilBERT base uncased finetuned\nSST-2 English sentiment analysis model, and the results show that the HNN model\ncan achieve up to 82.5% of the accuracy of the plain model while maintaining\nfull privacy and security.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16176v2",
    "published_date": "2025-02-22 10:45:07 UTC",
    "updated_date": "2025-02-27 14:09:57 UTC"
  },
  {
    "arxiv_id": "2502.16175v1",
    "title": "Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens",
    "authors": [
      "Ziwei Shan",
      "Yaoyu He",
      "Chengfeng Zhao",
      "Jiashen Du",
      "Jingyan Zhang",
      "Qixuan Zhang",
      "Jingyi Yu",
      "Lan Xu"
    ],
    "abstract": "Human bodily movements convey critical insights into action intentions and\ncognitive processes, yet existing multimodal systems primarily focused on\nunderstanding human motion via language, vision, and audio, which struggle to\ncapture the dynamic forces and torques inherent in 3D motion. Inertial\nmeasurement units (IMUs) present a promising alternative, offering lightweight,\nwearable, and privacy-conscious motion sensing. However, processing of\nstreaming IMU data faces challenges such as wireless transmission instability,\nsensor noise, and drift, limiting their utility for long-term real-time motion\ncapture (MoCap), and more importantly, online motion analysis. To address these\nchallenges, we introduce Mojito, an intelligent motion agent that integrates\ninertial sensing with large language models (LLMs) for interactive motion\ncapture and behavioral analysis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "First three authors contribute equally. Project page:\n  https://koyui.github.io/mojito/",
    "pdf_url": "http://arxiv.org/pdf/2502.16175v1",
    "published_date": "2025-02-22 10:31:58 UTC",
    "updated_date": "2025-02-22 10:31:58 UTC"
  },
  {
    "arxiv_id": "2502.16174v1",
    "title": "Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of Their Inputs?",
    "authors": [
      "Maciej ChrabÄszcz",
      "Filip Szatkowski",
      "Bartosz WÃ³jcik",
      "Jan DubiÅski",
      "Tomasz TrzciÅski"
    ],
    "abstract": "Ensuring the safety of the Large Language Model (LLM) is critical, but\ncurrently used methods in most cases sacrifice the model performance to obtain\nincreased safety or perform poorly on data outside of their adaptation\ndistribution. We investigate existing methods for such generalization and find\nthem insufficient. Surprisingly, while even plain LLMs recognize unsafe\nprompts, they may still generate unsafe responses. To avoid performance\ndegradation and preserve safe performance, we advocate for a two-step\nframework, where we first identify unsafe prompts via a lightweight classifier,\nand apply a \"safe\" model only to such prompts. In particular, we explore the\ndesign of the safety detector in more detail, investigating the use of\ndifferent classifier architectures and prompting techniques. Interestingly, we\nfind that the final hidden state for the last token is enough to provide robust\nperformance, minimizing false positives on benign data while performing well on\nmalicious prompt detection. Additionally, we show that classifiers trained on\nthe representations from different model layers perform comparably on the\nlatest model layers, indicating that safety representation is present in the\nLLMs' hidden states at most model stages. Our work is a step towards efficient,\nrepresentation-based safety mechanisms for LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16174v1",
    "published_date": "2025-02-22 10:31:50 UTC",
    "updated_date": "2025-02-22 10:31:50 UTC"
  },
  {
    "arxiv_id": "2502.16171v1",
    "title": "EPERM: An Evidence Path Enhanced Reasoning Model for Knowledge Graph Question and Answering",
    "authors": [
      "Xiao Long",
      "Liansheng Zhuang",
      "Aodi Li",
      "Minghong Yao",
      "Shafei Wang"
    ],
    "abstract": "Due to the remarkable reasoning ability, Large language models (LLMs) have\ndemonstrated impressive performance in knowledge graph question answering\n(KGQA) tasks, which find answers to natural language questions over knowledge\ngraphs (KGs). To alleviate the hallucinations and lack of knowledge issues of\nLLMs, existing methods often retrieve the question-related information from KGs\nto enrich the input context. However, most methods focus on retrieving the\nrelevant information while ignoring the importance of different types of\nknowledge in reasoning, which degrades their performance. To this end, this\npaper reformulates the KGQA problem as a graphical model and proposes a\nthree-stage framework named the Evidence Path Enhanced Reasoning Model (EPERM)\nfor KGQA. In the first stage, EPERM uses the fine-tuned LLM to retrieve a\nsubgraph related to the question from the original knowledge graph. In the\nsecond stage, EPERM filters out the evidence paths that faithfully support the\nreasoning of the questions, and score their importance in reasoning. Finally,\nEPERM uses the weighted evidence paths to reason the final answer. Since\nconsidering the importance of different structural information in KGs for\nreasoning, EPERM can improve the reasoning ability of LLMs in KGQA tasks.\nExtensive experiments on benchmark datasets demonstrate that EPERM achieves\nsuperior performances in KGQA tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16171v1",
    "published_date": "2025-02-22 10:05:22 UTC",
    "updated_date": "2025-02-22 10:05:22 UTC"
  },
  {
    "arxiv_id": "2502.16170v1",
    "title": "Destroy and Repair Using Hyper Graphs for Routing",
    "authors": [
      "Ke Li",
      "Fei Liu",
      "Zhengkun Wang",
      "Qingfu Zhang"
    ],
    "abstract": "Recent advancements in Neural Combinatorial Optimization (NCO) have shown\npromise in solving routing problems like the Traveling Salesman Problem (TSP)\nand Capacitated Vehicle Routing Problem (CVRP) without handcrafted designs.\nResearch in this domain has explored two primary categories of methods:\niterative and non-iterative. While non-iterative methods struggle to generate\nnear-optimal solutions directly, iterative methods simplify the task by\nlearning local search steps. However, existing iterative methods are often\nlimited by restricted neighborhood searches, leading to suboptimal results. To\naddress this limitation, we propose a novel approach that extends the search to\nlarger neighborhoods by learning a destroy-and-repair strategy. Specifically,\nwe introduce a Destroy-and-Repair framework based on Hyper-Graphs (DRHG). This\nframework reduces consecutive intact edges to hyper-edges, allowing the model\nto pay more attention to the destroyed part and decrease the complexity of\nencoding all nodes. Experiments demonstrate that DRHG achieves stateof-the-art\nperformance on TSP with up to 10,000 nodes and shows strong generalization to\nreal-world TSPLib and CVRPLib problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2502.16170v1",
    "published_date": "2025-02-22 10:04:58 UTC",
    "updated_date": "2025-02-22 10:04:58 UTC"
  },
  {
    "arxiv_id": "2502.16169v1",
    "title": "Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs under Noisy Observations",
    "authors": [
      "Chunyang Li",
      "Weiqi Wang",
      "Tianshi Zheng",
      "Yangqiu Song"
    ],
    "abstract": "Inductive reasoning, a cornerstone of human cognition, enables generalization\nfrom limited data but hasn't yet been fully achieved by large language models\n(LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain\nstable and consistent rule abstraction under imperfect observations remains\nunderexplored. To fill this gap, in this work, we introduce Robust Rule\nInduction, a task that evaluates LLMs' capability in inferring rules from data\nthat are fused with noisy examples. To address this task, we further propose\nSample-steered Rule Refinement (SRR), a method enhancing reasoning stability\nvia observation diversification and execution-guided feedback. Experiments\nacross arithmetic, cryptography, and list functions reveal: (1) SRR outperforms\nother methods with minimal performance degradation under noise; (2) Despite\nslight accuracy variation, LLMs exhibit instability under noise (e.g., 0%\naccuracy change with only 70% consistent score); (3) Counterfactual task gaps\nhighlight LLMs' reliance on memorized patterns over genuine abstraction. Our\nfindings challenge LLMs' reasoning robustness, revealing susceptibility to\nhypothesis drift and pattern overfitting, while providing empirical evidence\ncritical for developing human-like inductive systems. Code and data are\navailable at\n\\href{https://github.com/lcy2723/Robust-Rule-Induction}{https://github.com/lcy2723/Robust-Rule-Induction}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16169v1",
    "published_date": "2025-02-22 10:03:19 UTC",
    "updated_date": "2025-02-22 10:03:19 UTC"
  },
  {
    "arxiv_id": "2502.16167v1",
    "title": "PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models",
    "authors": [
      "Xinwei Liu",
      "Xiaojun Jia",
      "Yuan Xun",
      "Hua Zhang",
      "Xiaochun Cao"
    ],
    "abstract": "Diffusion models (DMs) have revolutionized data generation, particularly in\ntext-to-image (T2I) synthesis. However, the widespread use of personalized\ngenerative models raises significant concerns regarding privacy violations and\ncopyright infringement. To address these issues, researchers have proposed\nadversarial perturbation-based protection techniques. However, these methods\nhave notable limitations, including insufficient robustness against data\ntransformations and the inability to fully eliminate identifiable features of\nprotected objects in the generated output. In this paper, we introduce\nPersGuard, a novel backdoor-based approach that prevents malicious\npersonalization of specific images. Unlike traditional adversarial perturbation\nmethods, PersGuard implant backdoor triggers into pre-trained T2I models,\npreventing the generation of customized outputs for designated protected images\nwhile allowing normal personalization for unprotected ones. Unfortunately,\nexisting backdoor methods for T2I diffusion models fail to be applied to\npersonalization scenarios due to the different backdoor objectives and the\npotential backdoor elimination during downstream fine-tuning processes. To\naddress these, we propose three novel backdoor objectives specifically designed\nfor personalization scenarios, coupled with backdoor retention loss engineered\nto resist downstream fine-tuning. These components are integrated into a\nunified optimization framework. Extensive experimental evaluations demonstrate\nPersGuard's effectiveness in preserving data privacy, even under challenging\nconditions including gray-box settings, multi-object protection, and facial\nidentity scenarios. Our method significantly outperforms existing techniques,\noffering a more robust solution for privacy and copyright protection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16167v1",
    "published_date": "2025-02-22 09:47:55 UTC",
    "updated_date": "2025-02-22 09:47:55 UTC"
  },
  {
    "arxiv_id": "2502.19439v1",
    "title": "Multi-objective Cat Swarm Optimization Algorithm based on a Grid System",
    "authors": [
      "Aram M. Ahmed",
      "Bryar A. Hassan",
      "Tarik A. Rashid",
      "Kaniaw A. Noori",
      "Soran Ab. M. Saeed",
      "Omed H. Ahmed",
      "Shahla U. Umar"
    ],
    "abstract": "This paper presents a multi-objective version of the Cat Swarm Optimization\nAlgorithm called the Grid-based Multi-objective Cat Swarm Optimization\nAlgorithm (GMOCSO). Convergence and diversity preservation are the two main\ngoals pursued by modern multi-objective algorithms to yield robust results. To\nachieve these goals, we first replace the roulette wheel method of the original\nCSO algorithm with a greedy method. Then, two key concepts from Pareto Archived\nEvolution Strategy Algorithm (PAES) are adopted: the grid system and double\narchive strategy. Several test functions and a real-world scenario called the\nPressure vessel design problem are used to evaluate the proposed algorithm's\nperformance. In the experiment, the proposed algorithm is compared with other\nwell-known algorithms using different metrics such as Reversed Generational\nDistance, Spacing metric, and Spread metric. The optimization results show the\nrobustness of the proposed algorithm, and the results are further confirmed\nusing statistical methods and graphs. Finally, conclusions and future\ndirections were presented..",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.19439v1",
    "published_date": "2025-02-22 09:13:21 UTC",
    "updated_date": "2025-02-22 09:13:21 UTC"
  },
  {
    "arxiv_id": "2502.18509v1",
    "title": "Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents",
    "authors": [
      "Ivoline Ngong",
      "Swanand Kadhe",
      "Hao Wang",
      "Keerthiram Murugesan",
      "Justin D. Weisz",
      "Amit Dhurandhar",
      "Karthikeyan Natesan Ramamurthy"
    ],
    "abstract": "Conversational agents are increasingly woven into individuals' personal\nlives, yet users often underestimate the privacy risks involved. The moment\nusers share information with these agents (e.g., LLMs), their private\ninformation becomes vulnerable to exposure. In this paper, we characterize the\nnotion of contextual privacy for user interactions with LLMs. It aims to\nminimize privacy risks by ensuring that users (sender) disclose only\ninformation that is both relevant and necessary for achieving their intended\ngoals when interacting with LLMs (untrusted receivers). Through a formative\ndesign user study, we observe how even \"privacy-conscious\" users inadvertently\nreveal sensitive information through indirect disclosures. Based on insights\nfrom this study, we propose a locally-deployable framework that operates\nbetween users and LLMs, and identifies and reformulates out-of-context\ninformation in user prompts. Our evaluation using examples from ShareGPT shows\nthat lightweight models can effectively implement this framework, achieving\nstrong gains in contextual privacy while preserving the user's intended\ninteraction goals through different approaches to classify information relevant\nto the intended goals.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "22 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.18509v1",
    "published_date": "2025-02-22 09:05:39 UTC",
    "updated_date": "2025-02-22 09:05:39 UTC"
  },
  {
    "arxiv_id": "2502.16137v1",
    "title": "Chain-of-Description: What I can understand, I can put into words",
    "authors": [
      "Jiaxin Guo",
      "Daimeng Wei",
      "Zongyao Li",
      "Hengchao Shang",
      "Yuanchang Luo",
      "Hao Yang"
    ],
    "abstract": "In this paper, we propose a novel strategy defined as Chain-of-Description\n(CoD) Prompting, tailored for Multi-Modal Large Language Models. This approach\ninvolves having the model first provide a detailed description of the\nmulti-modal input before generating an answer to the question. When applied to\nmodels such as Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL, CoD Prompting\nsignificantly enhances performance compared to standard prompting methods. This\nis demonstrated by nearly a 4\\% improvement in the speech category of the audio\nbenchmark AIR-Bench-Chat and a 5.3\\% improvement in the hard-level portion of\nthe vision benchmark MMMU\\_Pro. Our ablation study further validates the\neffectiveness of CoD Prompting.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16137v1",
    "published_date": "2025-02-22 08:27:31 UTC",
    "updated_date": "2025-02-22 08:27:31 UTC"
  },
  {
    "arxiv_id": "2502.16129v1",
    "title": "Robust Dynamic Facial Expression Recognition",
    "authors": [
      "Feng Liu",
      "Hanyang Wang",
      "Siyuan Shen"
    ],
    "abstract": "The study of Dynamic Facial Expression Recognition (DFER) is a nascent field\nof research that involves the automated recognition of facial expressions in\nvideo data. Although existing research has primarily focused on learning\nrepresentations under noisy and hard samples, the issue of the coexistence of\nboth types of samples remains unresolved. In order to overcome this challenge,\nthis paper proposes a robust method of distinguishing between hard and noisy\nsamples. This is achieved by evaluating the prediction agreement of the model\non different sampled clips of the video. Subsequently, methodologies that\nreinforce the learning of hard samples and mitigate the impact of noisy samples\ncan be employed. Moreover, to identify the principal expression in a video and\nenhance the model's capacity for representation learning, comprising a key\nexpression re-sampling framework and a dual-stream hierarchical network is\nproposed, namely Robust Dynamic Facial Expression Recognition (RDFER). The key\nexpression re-sampling framework is designed to identify the key expression,\nthereby mitigating the potential confusion caused by non-target expressions.\nRDFER employs two sequence models with the objective of disentangling\nshort-term facial movements and long-term emotional changes. The proposed\nmethod has been shown to outperform current State-Of-The-Art approaches in DFER\nthrough extensive experimentation on benchmark datasets such as DFEW and\nFERV39K. A comprehensive analysis provides valuable insights and observations\nregarding the proposed agreement. This work has significant implications for\nthe field of dynamic facial expression recognition and promotes the further\ndevelopment of the field of noise-consistent robust learning in dynamic facial\nexpression recognition. The code is available from\n[https://github.com/Cross-Innovation-Lab/RDFER].",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16129v1",
    "published_date": "2025-02-22 07:48:12 UTC",
    "updated_date": "2025-02-22 07:48:12 UTC"
  },
  {
    "arxiv_id": "2502.16128v1",
    "title": "Heterogeneous Multi-Agent Bandits with Parsimonious Hints",
    "authors": [
      "Amirmahdi Mirfakhar",
      "Xuchuang Wang",
      "Jinhang Zuo",
      "Yair Zick",
      "Mohammad Hajiesmaili"
    ],
    "abstract": "We study a hinted heterogeneous multi-agent multi-armed bandits problem\n(HMA2B), where agents can query low-cost observations (hints) in addition to\npulling arms. In this framework, each of the $M$ agents has a unique reward\ndistribution over $K$ arms, and in $T$ rounds, they can observe the reward of\nthe arm they pull only if no other agent pulls that arm. The goal is to\nmaximize the total utility by querying the minimal necessary hints without\npulling arms, achieving time-independent regret. We study HMA2B in both\ncentralized and decentralized setups. Our main centralized algorithm, GP-HCLA,\nwhich is an extension of HCLA, uses a central decision-maker for arm-pulling\nand hint queries, achieving $O(M^4K)$ regret with $O(MK\\log T)$ adaptive hints.\nIn decentralized setups, we propose two algorithms, HD-ETC and EBHD-ETC, that\nallow agents to choose actions independently through collision-based\ncommunication and query hints uniformly until stopping, yielding $O(M^3K^2)$\nregret with $O(M^3K\\log T)$ hints, where the former requires knowledge of the\nminimum gap and the latter does not. Finally, we establish lower bounds to\nprove the optimality of our results and verify them through numerical\nsimulations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at AAAI-2025",
    "pdf_url": "http://arxiv.org/pdf/2502.16128v1",
    "published_date": "2025-02-22 07:46:41 UTC",
    "updated_date": "2025-02-22 07:46:41 UTC"
  },
  {
    "arxiv_id": "2502.18508v1",
    "title": "REFINE: Inversion-Free Backdoor Defense via Model Reprogramming",
    "authors": [
      "Yukun Chen",
      "Shuo Shao",
      "Enhao Huang",
      "Yiming Li",
      "Pin-Yu Chen",
      "Zhan Qin",
      "Kui Ren"
    ],
    "abstract": "Backdoor attacks on deep neural networks (DNNs) have emerged as a significant\nsecurity threat, allowing adversaries to implant hidden malicious behaviors\nduring the model training phase. Pre-processing-based defense, which is one of\nthe most important defense paradigms, typically focuses on input\ntransformations or backdoor trigger inversion (BTI) to deactivate or eliminate\nembedded backdoor triggers during the inference process. However, these methods\nsuffer from inherent limitations: transformation-based defenses often fail to\nbalance model utility and defense performance, while BTI-based defenses\nstruggle to accurately reconstruct trigger patterns without prior knowledge. In\nthis paper, we propose REFINE, an inversion-free backdoor defense method based\non model reprogramming. REFINE consists of two key components: \\textbf{(1)} an\ninput transformation module that disrupts both benign and backdoor patterns,\ngenerating new benign features; and \\textbf{(2)} an output remapping module\nthat redefines the model's output domain to guide the input transformations\neffectively. By further integrating supervised contrastive loss, REFINE\nenhances the defense capabilities while maintaining model utility. Extensive\nexperiments on various benchmark datasets demonstrate the effectiveness of our\nREFINE and its resistance to potential adaptive attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "This paper is accept by ICLR 2025. The first two authors contributed\n  equally to this work. Our code is available at BackdoorBox\n  (https://github.com/THUYimingLi/BackdoorBox) and Github repository\n  (https://github.com/WhitolfChen/REFINE). 28 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.18508v1",
    "published_date": "2025-02-22 07:29:12 UTC",
    "updated_date": "2025-02-22 07:29:12 UTC"
  },
  {
    "arxiv_id": "2504.03648v1",
    "title": "AIBrix: Towards Scalable, Cost-Effective Large Language Model Inference Infrastructure",
    "authors": [
      "The AIBrix Team",
      "Jiaxin Shan",
      "Varun Gupta",
      "Le Xu",
      "Haiyang Shi",
      "Jingyuan Zhang",
      "Ning Wang",
      "Linhui Xu",
      "Rong Kang",
      "Tongping Liu",
      "Yifei Zhang",
      "Yiqing Zhu",
      "Shuowei Jin",
      "Gangmuk Lim",
      "Binbin Chen",
      "Zuzhi Chen",
      "Xiao Liu",
      "Xin Chen",
      "Kante Yin",
      "Chak-Pong Chung",
      "Chenyu Jiang",
      "Yicheng Lu",
      "Jianjun Chen",
      "Caixue Lin",
      "Wu Xiang",
      "Rui Shi",
      "Liguang Xie"
    ],
    "abstract": "We introduce AIBrix, a cloud-native, open-source framework designed to\noptimize and simplify large-scale LLM deployment in cloud environments. Unlike\ntraditional cloud-native stacks, AIBrix follows a co-design philosophy,\nensuring every layer of the infrastructure is purpose-built for seamless\nintegration with inference engines like vLLM. AIBrix introduces several key\ninnovations to reduce inference costs and enhance performance including\nhigh-density LoRA management for dynamic adapter scheduling, LLM-specific\nautoscalers, and prefix-aware, load-aware routing. To further improve\nefficiency, AIBrix incorporates a distributed KV cache, boosting token reuse\nacross nodes, leading to a 50% increase in throughput and a 70% reduction in\ninference latency. AIBrix also supports unified AI runtime which streamlines\nmodel management while maintaining vendor-agnostic engine compatibility. For\nlarge-scale multi-node inference, AIBrix employs hybrid orchestration --\nleveraging Kubernetes for coarse-grained scheduling and Ray for fine-grained\nexecution -- to balance efficiency and flexibility. Additionally, an SLO-driven\nGPU optimizer dynamically adjusts resource allocations, optimizing\nheterogeneous serving to maximize cost efficiency while maintaining service\nguarantees. Finally, AIBrix enhances system reliability with AI accelerator\ndiagnostic tools, enabling automated failure detection and mock-up testing to\nimprove fault resilience. AIBrix is available at\nhttps://github.com/vllm-project/aibrix.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03648v1",
    "published_date": "2025-02-22 07:07:38 UTC",
    "updated_date": "2025-02-22 07:07:38 UTC"
  },
  {
    "arxiv_id": "2502.16111v1",
    "title": "PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving",
    "authors": [
      "Mihir Parmar",
      "Xin Liu",
      "Palash Goyal",
      "Yanfei Chen",
      "Long Le",
      "Swaroop Mishra",
      "Hossein Mobahi",
      "Jindong Gu",
      "Zifeng Wang",
      "Hootan Nakhost",
      "Chitta Baral",
      "Chen-Yu Lee",
      "Tomas Pfister",
      "Hamid Palangi"
    ],
    "abstract": "Recent agent frameworks and inference-time algorithms often struggle with\ncomplex planning problems due to limitations in verifying generated plans or\nreasoning and varying complexity of instances within a single task. Many\nexisting methods for these tasks either perform task-level verification without\nconsidering constraints or apply inference-time algorithms without adapting to\ninstance-level complexity. To address these limitations, we propose PlanGEN, a\nmodel-agnostic and easily scalable agent framework with three key components:\nconstraint, verification, and selection agents. Specifically, our approach\nproposes constraint-guided iterative verification to enhance performance of\ninference-time algorithms--Best of N, Tree-of-Thought, and REBASE. In PlanGEN\nframework, the selection agent optimizes algorithm choice based on instance\ncomplexity, ensuring better adaptability to complex planning problems.\nExperimental results demonstrate significant improvements over the strongest\nbaseline across multiple benchmarks, achieving state-of-the-art results on\nNATURAL PLAN ($\\sim$8%$\\uparrow$), OlympiadBench ($\\sim$4%$\\uparrow$), DocFinQA\n($\\sim$7%$\\uparrow$), and GPQA ($\\sim$1%$\\uparrow$). Our key finding highlights\nthat constraint-guided iterative verification improves inference-time\nalgorithms, and adaptive selection further boosts performance on complex\nplanning and reasoning problems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "30 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.16111v1",
    "published_date": "2025-02-22 06:21:56 UTC",
    "updated_date": "2025-02-22 06:21:56 UTC"
  },
  {
    "arxiv_id": "2502.16105v1",
    "title": "NeurFlow: Interpreting Neural Networks through Neuron Groups and Functional Interactions",
    "authors": [
      "Tue M. Cao",
      "Nhat X. Hoang",
      "Hieu H. Pham",
      "Phi Le Nguyen",
      "My T. Thai"
    ],
    "abstract": "Understanding the inner workings of neural networks is essential for\nenhancing model performance and interpretability. Current research\npredominantly focuses on examining the connection between individual neurons\nand the model's final predictions. Which suffers from challenges in\ninterpreting the internal workings of the model, particularly when neurons\nencode multiple unrelated features. In this paper, we propose a novel framework\nthat transitions the focus from analyzing individual neurons to investigating\ngroups of neurons, shifting the emphasis from neuron-output relationships to\nfunctional interaction between neurons. Our automated framework, NeurFlow,\nfirst identifies core neurons and clusters them into groups based on shared\nfunctional relationships, enabling a more coherent and interpretable view of\nthe network's internal processes. This approach facilitates the construction of\na hierarchical circuit representing neuron interactions across layers, thus\nimproving interpretability while reducing computational costs. Our extensive\nempirical studies validate the fidelity of our proposed NeurFlow. Additionally,\nwe showcase its utility in practical applications such as image debugging and\nautomatic concept labeling, thereby highlighting its potential to advance the\nfield of neural network explainability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The Thirteenth International Conference on Learning Representations",
    "pdf_url": "http://arxiv.org/pdf/2502.16105v1",
    "published_date": "2025-02-22 06:01:03 UTC",
    "updated_date": "2025-02-22 06:01:03 UTC"
  },
  {
    "arxiv_id": "2502.16101v2",
    "title": "Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals",
    "authors": [
      "Linda Zeng",
      "Rithwik Gupta",
      "Divij Motwani",
      "Diji Yang",
      "Yi Zhang"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has shown impressive capabilities in\nmitigating hallucinations in large language models (LLMs). However, LLMs\nstruggle to handle misleading retrievals and often fail to maintain their own\nreasoning when exposed to conflicting or selectively-framed evidence, making\nthem vulnerable to real-world misinformation. In such real-world retrieval\nscenarios, misleading and conflicting information is rampant, particularly in\nthe political domain, where evidence is often selectively framed, incomplete,\nor polarized. However, existing RAG benchmarks largely assume a clean retrieval\nsetting, where models succeed by accurately retrieving and generating answers\nfrom gold-standard documents. This assumption fails to align with real-world\nconditions, leading to an overestimation of RAG system performance. To bridge\nthis gap, we introduce RAGuard, a fact-checking dataset designed to evaluate\nthe robustness of RAG systems against misleading retrievals. Unlike prior\nbenchmarks that rely on synthetic noise, our dataset constructs its retrieval\ncorpus from Reddit discussions, capturing naturally occurring misinformation.\nIt categorizes retrieved evidence into three types: supporting, misleading, and\nirrelevant, providing a realistic and challenging testbed for assessing how\nwell RAG systems navigate different retrieval information. Our benchmark\nexperiments reveal that when exposed to misleading retrievals, all tested\nLLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no\nretrieval at all), highlighting their susceptibility to noisy environments. To\nthe best of our knowledge, RAGuard is the first benchmark to systematically\nassess RAG robustness against misleading evidence. We expect this benchmark\nwill drive future research toward improving RAG systems beyond idealized\ndatasets, making them more reliable for real-world applications.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16101v2",
    "published_date": "2025-02-22 05:50:15 UTC",
    "updated_date": "2025-05-21 21:12:31 UTC"
  },
  {
    "arxiv_id": "2502.16097v1",
    "title": "LitLinker: Supporting the Ideation of Interdisciplinary Contexts with Large Language Models for Teaching Literature in Elementary Schools",
    "authors": [
      "Haoxiang Fan",
      "Changshuang Zhou",
      "Hao Yu",
      "Xueyang Wu",
      "Jiangyu Gu",
      "Zhenhui Peng"
    ],
    "abstract": "Teaching literature under interdisciplinary contexts (e.g., science, art)\nthat connect reading materials has become popular in elementary schools.\nHowever, constructing such contexts is challenging as it requires teachers to\nexplore substantial amounts of interdisciplinary content and link it to the\nreading materials. In this paper, we develop LitLinker via an iterative design\nprocess involving 13 teachers to facilitate the ideation of interdisciplinary\ncontexts for teaching literature. Powered by a large language model (LLM),\nLitLinker can recommend interdisciplinary topics and contextualize them with\nthe literary elements (e.g., paragraphs, viewpoints) in the reading materials.\nA within-subjects study (N=16) shows that compared to an LLM chatbot, LitLinker\ncan improve the integration depth of different subjects and reduce workload in\nthis ideation task. Expert interviews (N=9) also demonstrate LitLinker's\nusefulness for supporting the ideation of interdisciplinary contexts for\nteaching literature. We conclude with concerns and design considerations for\nsupporting interdisciplinary teaching with LLMs.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16097v1",
    "published_date": "2025-02-22 05:40:19 UTC",
    "updated_date": "2025-02-22 05:40:19 UTC"
  },
  {
    "arxiv_id": "2502.17510v1",
    "title": "Recurrent Knowledge Identification and Fusion for Language Model Continual Learning",
    "authors": [
      "Yujie Feng",
      "Xujia Wang",
      "Zexin Lu",
      "Shenghong Fu",
      "Guangyuan Shi",
      "Yongxin Xu",
      "Yasha Wang",
      "Philip S. Yu",
      "Xu Chu",
      "Xiao-Ming Wu"
    ],
    "abstract": "Continual learning (CL) is crucial for deploying large language models (LLMs)\nin dynamic real-world environments without costly retraining. While recent\nmodel ensemble and model merging methods guided by parameter importance have\ngained popularity, they often struggle to balance knowledge transfer and\nforgetting, mainly due to the reliance on static importance estimates during\nsequential training. In this paper, we present Recurrent-KIF, a novel CL\nframework for Recurrent Knowledge Identification and Fusion, which enables\ndynamic estimation of parameter importance distributions to enhance knowledge\ntransfer. Inspired by human continual learning, Recurrent-KIF employs an inner\nloop that rapidly adapts to new tasks while identifying important parameters,\ncoupled with an outer loop that globally manages the fusion of new and\nhistorical knowledge through redundant knowledge pruning and key knowledge\nmerging. These inner-outer loops iteratively perform multiple rounds of fusion,\nallowing Recurrent-KIF to leverage intermediate training information and\nadaptively adjust fusion strategies based on evolving importance distributions.\nExtensive experiments on two CL benchmarks with various model sizes (from 770M\nto 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic\nforgetting and enhances knowledge transfer.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17510v1",
    "published_date": "2025-02-22 05:37:27 UTC",
    "updated_date": "2025-02-22 05:37:27 UTC"
  },
  {
    "arxiv_id": "2502.16091v2",
    "title": "Privacy-Aware Joint DNN Model Deployment and Partition Optimization for Delay-Efficient Collaborative Edge Inference",
    "authors": [
      "Zhipeng Cheng",
      "Xiaoyu Xia",
      "Hong Wang",
      "Minghui Liwang",
      "Ning Chen",
      "Xuwei Fan",
      "Xianbin Wang"
    ],
    "abstract": "Edge inference (EI) is a key solution to address the growing challenges of\ndelayed response times, limited scalability, and privacy concerns in\ncloud-based Deep Neural Network (DNN) inference. However, deploying DNN models\non resource-constrained edge devices faces more severe challenges, such as\nmodel storage limitations, dynamic service requests, and privacy risks. This\npaper proposes a novel framework for privacy-aware joint DNN model deployment\nand partition optimization to minimize long-term average inference delay under\nresource and privacy constraints. Specifically, the problem is formulated as a\ncomplex optimization problem considering model deployment, user-server\nassociation, and model partition strategies. To handle the NP-hardness and\nfuture uncertainties, a Lyapunov-based approach is introduced to transform the\nlong-term optimization into a single-time-slot problem, ensuring system\nperformance. Additionally, a coalition formation game model is proposed for\nedge server association, and a greedy-based algorithm is developed for model\ndeployment within each coalition to efficiently solve the problem. Extensive\nsimulations show that the proposed algorithms effectively reduce inference\ndelay while satisfying privacy constraints, outperforming baseline approaches\nin various scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16091v2",
    "published_date": "2025-02-22 05:27:24 UTC",
    "updated_date": "2025-03-01 04:35:04 UTC"
  },
  {
    "arxiv_id": "2502.16090v1",
    "title": "Echo: A Large Language Model with Temporal Episodic Memory",
    "authors": [
      "WenTao Liu",
      "Ruohua Zhang",
      "Aimin Zhou",
      "Feng Gao",
      "JiaLi Liu"
    ],
    "abstract": "Research on large language models (LLMs) has shown remarkable performance in\ndomains such as mathematics, programming, and literary creation. However, most\nstudies have focused on semantic memory-based question answering, neglecting\nLLMs' potential to handle episodic memory (EM)-related queries. This oversight\nhas led to suboptimal performance in applications requiring EM, including\nemotional companionship, personal AI assistants, and AI teachers. To address\nthis gap, we introduce Echo, a LLM enhanced with temporal episodic memory. We\npropose a Multi-Agent Data Generation Framework that guides the model in\ngenerating multi-turn, complex scenario episodic memory dialogue data\n(EM-Train). Temporal information is innovatively incorporated into the LLM\ntraining process, and Echo is trained using the EM-Train. Furthermore, We\ndevelop an EM-Test benchmark specifically designed to evaluate LLMs' episodic\nmemory capabilities. The EM-Test assesses performance across various time spans\nand difficulty levels, providing a comprehensive evaluation of multi-turn\nepisodic memory dialogues. Our experiments demonstrate that Echo significantly\noutperforms state-of-the-art LLMs on EM-Test. Additionally, a qualitative\nanalysis reveals Echo's potential to exhibit human-like episodic memory\ncapabilities. We will open-source all datasets, code, and model weights.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16090v1",
    "published_date": "2025-02-22 05:25:20 UTC",
    "updated_date": "2025-02-22 05:25:20 UTC"
  },
  {
    "arxiv_id": "2503.05755v1",
    "title": "SEAFL: Enhancing Efficiency in Semi-Asynchronous Federated Learning through Adaptive Aggregation and Selective Training",
    "authors": [
      "Md Sirajul Islam",
      "Sanjeev Panta",
      "Fei Xu",
      "Xu Yuan",
      "Li Chen",
      "Nian-Feng Tzeng"
    ],
    "abstract": "Federated Learning (FL) is a promising distributed machine learning framework\nthat allows collaborative learning of a global model across decentralized\ndevices without uploading their local data. However, in real-world FL\nscenarios, the conventional synchronous FL mechanism suffers from inefficient\ntraining caused by slow-speed devices, commonly known as stragglers, especially\nin heterogeneous communication environments. Though asynchronous FL effectively\ntackles the efficiency challenge, it induces substantial system overheads and\nmodel degradation. Striking for a balance, semi-asynchronous FL has gained\nincreasing attention, while still suffering from the open challenge of stale\nmodels, where newly arrived updates are calculated based on outdated weights\nthat easily hurt the convergence of the global model. In this paper, we present\n{\\em SEAFL}, a novel FL framework designed to mitigate both the straggler and\nthe stale model challenges in semi-asynchronous FL. {\\em SEAFL} dynamically\nassigns weights to uploaded models during aggregation based on their staleness\nand importance to the current global model. We theoretically analyze the\nconvergence rate of {\\em SEAFL} and further enhance the training efficiency\nwith an extended variant that allows partial training on slower devices,\nenabling them to contribute to global aggregation while reducing excessive\nwaiting times. We evaluate the effectiveness of {\\em SEAFL} through extensive\nexperiments on three benchmark datasets. The experimental results demonstrate\nthat {\\em SEAFL} outperforms its closest counterpart by up to $\\sim$22\\% in\nterms of the wall-clock training time required to achieve target accuracy.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05755v1",
    "published_date": "2025-02-22 05:13:53 UTC",
    "updated_date": "2025-02-22 05:13:53 UTC"
  },
  {
    "arxiv_id": "2502.16079v1",
    "title": "Together We Rise: Optimizing Real-Time Multi-Robot Task Allocation using Coordinated Heterogeneous Plays",
    "authors": [
      "Aritra Pal",
      "Anandsingh Chauhan",
      "Mayank Baranwal"
    ],
    "abstract": "Efficient task allocation among multiple robots is crucial for optimizing\nproductivity in modern warehouses, particularly in response to the increasing\ndemands of online order fulfillment. This paper addresses the real-time\nmulti-robot task allocation (MRTA) problem in dynamic warehouse environments,\nwhere tasks emerge with specified start and end locations. The objective is to\nminimize both the total travel distance of robots and delays in task\ncompletion, while also considering practical constraints such as battery\nmanagement and collision avoidance. We introduce MRTAgent, a dual-agent\nReinforcement Learning (RL) framework inspired by self-play, designed to\noptimize task assignments and robot selection to ensure timely task execution.\nFor safe navigation, a modified linear quadratic controller (LQR) approach is\nemployed. To the best of our knowledge, MRTAgent is the first framework to\naddress all critical aspects of practical MRTA problems while supporting\ncontinuous robot movements.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to AAMAS 2025 (AAAI Track)",
    "pdf_url": "http://arxiv.org/pdf/2502.16079v1",
    "published_date": "2025-02-22 04:59:27 UTC",
    "updated_date": "2025-02-22 04:59:27 UTC"
  },
  {
    "arxiv_id": "2503.10642v1",
    "title": "Text2Zinc: A Cross-Domain Dataset for Modeling Optimization and Satisfaction Problems in MiniZinc",
    "authors": [
      "Akash Singirikonda",
      "Serdar Kadioglu",
      "Karthik Uppuluri"
    ],
    "abstract": "There is growing interest in utilizing large language models (LLMs) as\nco-pilots for combinatorial optimization and constraint programming tasks\nacross various problems. This paper aims to advance this line of research by\nintroducing Text2Zinc}, a cross-domain dataset for capturing optimization and\nsatisfaction problems specified in natural language text. Our work is\ndistinguished from previous attempts by integrating both satisfaction and\noptimization problems within a unified dataset using a solver-agnostic modeling\nlanguage. To achieve this, we leverage MiniZinc's solver-and-paradigm-agnostic\nmodeling capabilities to formulate these problems. Using the Text2Zinc dataset,\nwe conduct comprehensive baseline experiments to compare execution and solution\naccuracy across several methods, including off-the-shelf prompting strategies,\nchain-of-thought reasoning, and a compositional approach. Additionally, we\nexplore the effectiveness of intermediary representations, specifically\nknowledge graphs. Our findings indicate that LLMs are not yet a push-button\ntechnology to model combinatorial problems from text. We hope that Text2Zinc\nserves as a valuable resource for researchers and practitioners to advance the\nfield further.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10642v1",
    "published_date": "2025-02-22 04:13:53 UTC",
    "updated_date": "2025-02-22 04:13:53 UTC"
  },
  {
    "arxiv_id": "2502.18506v1",
    "title": "Exploring Patient Data Requirements in Training Effective AI Models for MRI-based Breast Cancer Classification",
    "authors": [
      "Solha Kang",
      "Wesley De Neve",
      "Francois Rameau",
      "Utku Ozbulak"
    ],
    "abstract": "The past decade has witnessed a substantial increase in the number of\nstartups and companies offering AI-based solutions for clinical decision\nsupport in medical institutions. However, the critical nature of medical\ndecision-making raises several concerns about relying on external software. Key\nissues include potential variations in image modalities and the medical devices\nused to obtain these images, potential legal issues, and adversarial attacks.\nFortunately, the open-source nature of machine learning research has made\nfoundation models publicly available and straightforward to use for medical\napplications. This accessibility allows medical institutions to train their own\nAI-based models, thereby mitigating the aforementioned concerns. Given this\ncontext, an important question arises: how much data do medical institutions\nneed to train effective AI models? In this study, we explore this question in\nrelation to breast cancer detection, a particularly contested area due to the\nprevalence of this disease, which affects approximately 1 in every 8 women.\nThrough large-scale experiments on various patient sizes in the training set,\nwe show that medical institutions do not need a decade's worth of MRI images to\ntrain an AI model that performs competitively with the state-of-the-art,\nprovided the model leverages foundation models. Furthermore, we observe that\nfor patient counts greater than 50, the number of patients in the training set\nhas a negligible impact on the performance of models and that simple ensembles\nfurther improve the results without additional complexity.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted for publication in MICCAI 2024 Deep Breast Workshop on AI\n  and Imaging for Diagnostic and Treatment Challenges in Breast Care",
    "pdf_url": "http://arxiv.org/pdf/2502.18506v1",
    "published_date": "2025-02-22 04:04:52 UTC",
    "updated_date": "2025-02-22 04:04:52 UTC"
  },
  {
    "arxiv_id": "2502.16069v2",
    "title": "Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents",
    "authors": [
      "Patrick Tser Jern Kon",
      "Jiachen Liu",
      "Qiuyi Ding",
      "Yiming Qiu",
      "Zhenning Yang",
      "Yibo Huang",
      "Jayanth Srinivasa",
      "Myungjin Lee",
      "Mosharaf Chowdhury",
      "Ang Chen"
    ],
    "abstract": "Scientific experimentation, a cornerstone of human progress, demands rigor in\nreliability, methodical control, and interpretability to yield meaningful\nresults. Despite the growing capabilities of large language models (LLMs) in\nautomating different aspects of the scientific process, automating rigorous\nexperimentation remains a significant challenge. To address this gap, we\npropose Curie, an AI agent framework designed to embed rigor into the\nexperimentation process through three key components: an intra-agent rigor\nmodule to enhance reliability, an inter-agent rigor module to maintain\nmethodical control, and an experiment knowledge module to enhance\ninterpretability. To evaluate Curie, we design a novel experimental benchmark\ncomposed of 46 questions across four computer science domains, derived from\ninfluential research papers, and widely adopted open-source projects. Compared\nto the strongest baseline tested, we achieve a 3.4$\\times$ improvement in\ncorrectly answering experimental questions. Curie is open-sourced at\nhttps://github.com/Just-Curieous/Curie.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.16069v2",
    "published_date": "2025-02-22 03:58:19 UTC",
    "updated_date": "2025-02-26 02:33:28 UTC"
  },
  {
    "arxiv_id": "2502.16065v1",
    "title": "A Survey of Model Extraction Attacks and Defenses in Distributed Computing Environments",
    "authors": [
      "Kaixiang Zhao",
      "Lincan Li",
      "Kaize Ding",
      "Neil Zhenqiang Gong",
      "Yue Zhao",
      "Yushun Dong"
    ],
    "abstract": "Model Extraction Attacks (MEAs) threaten modern machine learning systems by\nenabling adversaries to steal models, exposing intellectual property and\ntraining data. With the increasing deployment of machine learning models in\ndistributed computing environments, including cloud, edge, and federated\nlearning settings, each paradigm introduces distinct vulnerabilities and\nchallenges. Without a unified perspective on MEAs across these distributed\nenvironments, organizations risk fragmented defenses, inadequate risk\nassessments, and substantial economic and privacy losses. This survey is\nmotivated by the urgent need to understand how the unique characteristics of\ncloud, edge, and federated deployments shape attack vectors and defense\nrequirements. We systematically examine the evolution of attack methodologies\nand defense mechanisms across these environments, demonstrating how\nenvironmental factors influence security strategies in critical sectors such as\nautonomous vehicles, healthcare, and financial services. By synthesizing recent\nadvances in MEAs research and discussing the limitations of current evaluation\npractices, this survey provides essential insights for developing robust and\nadaptive defense strategies. Our comprehensive approach highlights the\nimportance of integrating protective measures across the entire distributed\ncomputing landscape to ensure the secure deployment of machine learning models.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16065v1",
    "published_date": "2025-02-22 03:46:50 UTC",
    "updated_date": "2025-02-22 03:46:50 UTC"
  },
  {
    "arxiv_id": "2502.16060v1",
    "title": "Single-Channel EEG Tokenization Through Time-Frequency Modeling",
    "authors": [
      "Jathurshan Pradeepkumar",
      "Xihao Piao",
      "Zheng Chen",
      "Jimeng Sun"
    ],
    "abstract": "We introduce TFM-Tokenizer, a novel tokenization framework tailored for EEG\nanalysis that transforms continuous, noisy brain signals into a sequence of\ndiscrete, well-represented tokens for various EEG tasks. Conventional\napproaches typically rely on continuous embeddings and inter-channel\ndependencies, which are limited in capturing inherent EEG features such as\ntemporally unpredictable patterns and diverse oscillatory waveforms. In\ncontrast, we hypothesize that critical time-frequency features can be\neffectively captured from a single channel. By learning tokens that encapsulate\nthese intrinsic patterns within a single channel, our approach yields a\nscalable tokenizer adaptable across diverse EEG settings. We integrate the\nTFM-Tokenizer with a transformer-based TFM-Encoder, leveraging established\npretraining techniques from natural language processing, such as masked token\nprediction, followed by downstream fine-tuning for various EEG tasks.\nExperiments across four EEG datasets show that TFM-Token outperforms\nstate-of-the-art methods. On TUEV, our approach improves balanced accuracy and\nCohen's Kappa by 5% over baselines. Comprehensive analysis of the learned\ntokens demonstrates their ability to capture class-distinctive features,\nenhance frequency representation, and ability to encode time-frequency motifs\ninto distinct tokens, improving interpretability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16060v1",
    "published_date": "2025-02-22 03:32:36 UTC",
    "updated_date": "2025-02-22 03:32:36 UTC"
  },
  {
    "arxiv_id": "2502.16054v2",
    "title": "Human-AI Collaboration in Cloud Security: Cognitive Hierarchy-Driven Deep Reinforcement Learning",
    "authors": [
      "Zahra Aref",
      "Sheng Wei",
      "Narayan B. Mandayam"
    ],
    "abstract": "Given the complexity of multi-tenant cloud environments and the growing need\nfor real-time threat mitigation, Security Operations Centers (SOCs) must adopt\nAI-driven adaptive defense mechanisms to counter Advanced Persistent Threats\n(APTs). However, SOC analysts face challenges in handling adaptive adversarial\ntactics, requiring intelligent decision-support frameworks. We propose a\nCognitive Hierarchy Theory-driven Deep Q-Network (CHT-DQN) framework that\nmodels interactive decision-making between SOC analysts and AI-driven APT bots.\nThe SOC analyst (defender) operates at cognitive level-1, anticipating attacker\nstrategies, while the APT bot (attacker) follows a level-0 policy. By\nincorporating CHT into DQN, our framework enhances adaptive SOC defense using\nAttack Graph (AG)-based reinforcement learning. Simulation experiments across\nvarying AG complexities show that CHT-DQN consistently achieves higher data\nprotection and lower action discrepancies compared to standard DQN. A\ntheoretical lower bound further confirms its superiority as AG complexity\nincreases. A human-in-the-loop (HITL) evaluation on Amazon Mechanical Turk\n(MTurk) reveals that SOC analysts using CHT-DQN-derived transition\nprobabilities align more closely with adaptive attackers, leading to better\ndefense outcomes. Moreover, human behavior aligns with Prospect Theory (PT) and\nCumulative Prospect Theory (CPT): participants are less likely to reselect\nfailed actions and more likely to persist with successful ones. This asymmetry\nreflects amplified loss sensitivity and biased probability weighting --\nunderestimating gains after failure and overestimating continued success. Our\nfindings highlight the potential of integrating cognitive models into deep\nreinforcement learning to improve real-time SOC decision-making for cloud\nsecurity.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16054v2",
    "published_date": "2025-02-22 03:19:21 UTC",
    "updated_date": "2025-04-20 16:40:20 UTC"
  },
  {
    "arxiv_id": "2503.05753v1",
    "title": "Exploring AI Writers: Technology, Impact, and Future Prospects",
    "authors": [
      "Zhiqian Huang"
    ],
    "abstract": "This study explores the practical capabilities of AI writers, focusing on\ntheir applications across various creative domains. It delves into the\npotential impact of AI-generated content on traditional media industries and\nacademic writing processes. The research examines how AI tools are reshaping\nnews production workflows, particularly in fields such as finance, sports, and\nnatural disasters. Additionally, it addresses ethical concerns, including\nauthorship and copyright issues arising from AI-driven creative outputs. The\nfindings reveal mixed perceptions among media students regarding the\nintegration of AI into their profession, reflecting both optimism about\nefficiency gains and apprehensions over increased job market competition.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05753v1",
    "published_date": "2025-02-22 02:03:09 UTC",
    "updated_date": "2025-02-22 02:03:09 UTC"
  },
  {
    "arxiv_id": "2502.16033v2",
    "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
    "authors": [
      "Qianqi Yan",
      "Yue Fan",
      "Hongquan Li",
      "Shan Jiang",
      "Yang Zhao",
      "Xinze Guan",
      "Ching-Chen Kuo",
      "Xin Eric Wang"
    ],
    "abstract": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained\nand tested on consistent visual-textual inputs, leaving open the question of\nwhether they can handle inconsistencies in real-world, layout-rich content. To\nbridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR)\nbenchmark to assess MLLMs' ability to detect and reason about semantic\nmismatches in artifacts such as webpages, presentation slides, and posters.\nMMIR comprises 534 challenging samples, each containing synthetically injected\nerrors across five reasoning-heavy categories: Factual Contradiction, Identity\nMisattribution, Contextual Mismatch, Quantitative Discrepancy, and\nTemporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing\nthat models with dedicated multimodal reasoning capabilities, such as o1,\nsubstantially outperform their counterparts while open-source models remain\nparticularly vulnerable to inconsistency errors. Detailed error analyses\nfurther show that models excel in detecting pairwise inconsistencies but\nstruggle with inconsistencies confined to single elements in complex layouts.\nProbing experiments reveal that single-modality prompting, including\nChain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains,\nrevealing a key bottleneck in cross-modal reasoning. Our findings highlight the\nneed for advanced multimodal reasoning and point to future research on\nmultimodal inconsistency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16033v2",
    "published_date": "2025-02-22 01:52:37 UTC",
    "updated_date": "2025-03-04 08:23:58 UTC"
  },
  {
    "arxiv_id": "2502.16032v2",
    "title": "Clinical Inspired MRI Lesion Segmentation",
    "authors": [
      "Lijun Yan",
      "Churan Wang",
      "Fangwei Zhong",
      "Yizhou Wang"
    ],
    "abstract": "Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting\npathological tissues in various diseases. Different MRI sequences have\ndifferent contrast mechanisms and sensitivities for different types of lesions,\nwhich pose challenges to accurate and consistent lesion segmentation. In\nclinical practice, radiologists commonly use the sub-sequence feature, i.e. the\ndifference between post contrast-enhanced T1-weighted (post) and\npre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we\npropose a residual fusion method to learn subsequence representation for MRI\nlesion segmentation. Specifically, we iteratively and adaptively fuse features\nfrom pre- and post-contrast sequences at multiple resolutions, using dynamic\nweights to achieve optimal fusion and address diverse lesion enhancement\npatterns. Our method achieves state-of-the-art performances on BraTS2023\ndataset for brain tumor segmentation and our in-house breast MRI dataset for\nbreast lesion segmentation. Our method is clinically inspired and has the\npotential to facilitate lesion segmentation in various applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted in ISBI2025 oral",
    "pdf_url": "http://arxiv.org/pdf/2502.16032v2",
    "published_date": "2025-02-22 01:37:35 UTC",
    "updated_date": "2025-05-12 09:34:10 UTC"
  },
  {
    "arxiv_id": "2502.16030v1",
    "title": "Real Time Offside Detection using a Single Camera in Soccer",
    "authors": [
      "Shounak Desai"
    ],
    "abstract": "Technological advancements in soccer have surged over the past decade,\ntransforming aspects of the sport. Unlike binary rules, many soccer\nregulations, such as the \"Offside Rule,\" rely on subjective interpretation\nrather than straightforward True or False criteria. The on-field referee holds\nultimate authority in adjudicating these nuanced decisions. A significant\nbreakthrough in soccer officiating is the Video Assistant Referee (VAR) system,\nleveraging a network of 20-30 cameras within stadiums to minimize human errors.\nVAR's operational scope typically encompasses 10-30 cameras, ensuring high\ndecision accuracy but at a substantial cost. This report proposes an innovative\napproach to offside detection using a single camera, such as the broadcasting\ncamera, to mitigate expenses associated with sophisticated technological\nsetups.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.16030v1",
    "published_date": "2025-02-22 01:33:26 UTC",
    "updated_date": "2025-02-22 01:33:26 UTC"
  },
  {
    "arxiv_id": "2503.16457v1",
    "title": "Integrating Personality into Digital Humans: A Review of LLM-Driven Approaches for Virtual Reality",
    "authors": [
      "Iago Alves Brito",
      "Julia Soares Dollis",
      "Fernanda Bufon FÃ¤rber",
      "Pedro Schindler Freire Brasil Ribeiro",
      "Rafael Teixeira Sousa",
      "Arlindo Rodrigues GalvÃ£o Filho"
    ],
    "abstract": "The integration of large language models (LLMs) into virtual reality (VR)\nenvironments has opened new pathways for creating more immersive and\ninteractive digital humans. By leveraging the generative capabilities of LLMs\nalongside multimodal outputs such as facial expressions and gestures, virtual\nagents can simulate human-like personalities and emotions, fostering richer and\nmore engaging user experiences. This paper provides a comprehensive review of\nmethods for enabling digital humans to adopt nuanced personality traits,\nexploring approaches such as zero-shot, few-shot, and fine-tuning.\nAdditionally, it highlights the challenges of integrating LLM-driven\npersonality traits into VR, including computational demands, latency issues,\nand the lack of standardized evaluation frameworks for multimodal interactions.\nBy addressing these gaps, this work lays a foundation for advancing\napplications in education, therapy, and gaming, while fostering\ninterdisciplinary collaboration to redefine human-computer interaction in VR.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.16457v1",
    "published_date": "2025-02-22 01:33:05 UTC",
    "updated_date": "2025-02-22 01:33:05 UTC"
  },
  {
    "arxiv_id": "2502.17507v1",
    "title": "C-3DPO: Constrained Controlled Classification for Direct Preference Optimization",
    "authors": [
      "Kavosh Asadi",
      "Julien Han",
      "Xingzi Xu",
      "Dominique Perrault-Joncas",
      "Shoham Sabach",
      "Karim Bouyarmane",
      "Mohammad Ghavamzadeh"
    ],
    "abstract": "Direct preference optimization (DPO)-style algorithms have emerged as a\npromising approach for solving the alignment problem in AI. We present a novel\nperspective that formulates these algorithms as implicit classification\nalgorithms. This classification framework enables us to recover many variants\nof DPO-style algorithms by choosing appropriate classification labels and loss\nfunctions. We then leverage this classification framework to demonstrate that\nthe underlying problem solved in these algorithms is under-specified, making\nthem susceptible to probability collapse of the winner-loser responses. We\naddress this by proposing a set of constraints designed to control the movement\nof probability mass between the winner and loser in the reference and target\npolicies. Our resulting algorithm, which we call Constrained Controlled\nClassification DPO (\\texttt{C-3DPO}), has a meaningful RLHF interpretation. By\nhedging against probability collapse, \\texttt{C-3DPO} provides practical\nimprovements over vanilla \\texttt{DPO} when aligning several large language\nmodels using standard preference datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17507v1",
    "published_date": "2025-02-22 00:38:44 UTC",
    "updated_date": "2025-02-22 00:38:44 UTC"
  },
  {
    "arxiv_id": "2502.17506v2",
    "title": "RAG-Enhanced Collaborative LLM Agents for Drug Discovery",
    "authors": [
      "Namkyeong Lee",
      "Edward De Brouwer",
      "Ehsan Hajiramezanali",
      "Tommaso Biancalani",
      "Chanyoung Park",
      "Gabriele Scalia"
    ],
    "abstract": "Recent advances in large language models (LLMs) have shown great potential to\naccelerate drug discovery. However, the specialized nature of biochemical data\noften necessitates costly domain-specific fine-tuning, posing critical\nchallenges. First, it hinders the application of more flexible general-purpose\nLLMs in cutting-edge drug discovery tasks. More importantly, it impedes the\nrapid integration of the vast amounts of scientific data continuously generated\nthrough experiments and research. To investigate these challenges, we propose\nCLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored\nto drug discovery tasks. Through the collaboration of multiple LLM agents,\nCLADD dynamically retrieves information from biomedical knowledge bases,\ncontextualizes query molecules, and integrates relevant evidence to generate\nresponses -- all without the need for domain-specific fine-tuning. Crucially,\nwe tackle key obstacles in applying RAG workflows to biochemical data,\nincluding data heterogeneity, ambiguity, and multi-source integration. We\ndemonstrate the flexibility and effectiveness of this framework across a\nvariety of drug discovery tasks, showing that it outperforms general-purpose\nand domain-specific LLMs as well as traditional deep learning approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Machine Learning, Drug Discovery",
    "pdf_url": "http://arxiv.org/pdf/2502.17506v2",
    "published_date": "2025-02-22 00:12:52 UTC",
    "updated_date": "2025-03-10 12:11:58 UTC"
  },
  {
    "arxiv_id": "2502.16012v1",
    "title": "Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving",
    "authors": [
      "Prashant Shekhar",
      "Bidur Devkota",
      "Dumindu Samaraweera",
      "Laxima Niure Kandel",
      "Manoj Babu"
    ],
    "abstract": "Adversarial attacks pose a significant threat to deep learning models,\nparticularly in safety-critical applications like healthcare and autonomous\ndriving. Recently, patch based attacks have demonstrated effectiveness in\nreal-time inference scenarios owing to their 'drag and drop' nature. Following\nthis idea for Semantic Segmentation (SS), here we propose a novel Expectation\nOver Transformation (EOT) based adversarial patch attack that is more realistic\nfor autonomous vehicles. To effectively train this attack we also propose a\n'simplified' loss function that is easy to analyze and implement. Using this\nattack as our basis, we investigate whether adversarial patches once optimized\non a specific SS model, can fool other models or architectures. We conduct a\ncomprehensive cross-model transferability analysis of adversarial patches\ntrained on SOTA Convolutional Neural Network (CNN) models such PIDNet-S,\nPIDNet-M and PIDNet-L, among others. Additionally, we also include the\nSegformer model to study transferability to Vision Transformers (ViTs). All of\nour analysis is conducted on the widely used Cityscapes dataset. Our study\nreveals key insights into how model architectures (CNN vs CNN or CNN vs.\nTransformer-based) influence attack susceptibility. In particular, we conclude\nthat although the transferability (effectiveness) of attacks on unseen images\nof any dimension is really high, the attacks trained against one particular\nmodel are minimally effective on other models. And this was found to be true\nfor both ViT and CNN based models. Additionally our results also indicate that\nfor CNN-based models, the repercussions of patch attacks are local, unlike\nViTs. Per-class analysis reveals that simple-classes like 'sky' suffer less\nmisclassification than others. The code for the project is available at:\nhttps://github.com/p-shekhar/adversarial-patch-transferability",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.16012v1",
    "published_date": "2025-02-22 00:03:53 UTC",
    "updated_date": "2025-02-22 00:03:53 UTC"
  }
]
[
  {
    "arxiv_id": "2407.13054v2",
    "title": "Comprehensive Review and Empirical Evaluation of Causal Discovery Algorithms for Numerical Data",
    "authors": [
      "Wenjin Niu",
      "Zijun Gao",
      "Liyan Song",
      "Lingbo Li"
    ],
    "abstract": "Causal analysis has become an essential component in understanding the\nunderlying causes of phenomena across various fields. Despite its significance,\nexisting literature on causal discovery algorithms is fragmented, with\ninconsistent methodologies, i.e., there is no universal classification standard\nfor existing methods, and a lack of comprehensive evaluations, i.e., data\ncharacteristics are often ignored to be jointly analyzed when benchmarking\nalgorithms. This study addresses these gaps by conducting an exhaustive review\nand empirical evaluation for causal discovery methods on numerical data, aiming\nto provide a clearer and more structured understanding of the field. Our\nresearch begins with a comprehensive literature review spanning over two\ndecades, analyzing over 200 academic articles and identifying more than 40\nrepresentative algorithms. This extensive analysis leads to the development of\na structured taxonomy tailored to the complexities of causal discovery,\ncategorizing methods into six main types. To address the lack of comprehensive\nevaluations, our study conducts an extensive empirical assessment of 29 causal\ndiscovery algorithms on multiple synthetic and real-world datasets. We\ncategorize synthetic datasets based on size, linearity, and noise distribution,\nemploying five evaluation metrics, and summarize the top-3 algorithm\nrecommendations, providing guidelines for users in various data scenarios. Our\nresults highlight a significant impact of dataset characteristics on algorithm\nperformance. Moreover, a metadata extraction strategy with an accuracy\nexceeding 80% is developed to assist users in algorithm selection on unknown\ndatasets. Based on these insights, we offer professional and practical\nguidelines to help users choose the most suitable causal discovery methods for\ntheir specific dataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.13054v2",
    "published_date": "2024-07-17 23:47:05 UTC",
    "updated_date": "2024-09-04 13:13:03 UTC"
  },
  {
    "arxiv_id": "2408.02074v1",
    "title": "Applying Conditional Generative Adversarial Networks for Imaging Diagnosis",
    "authors": [
      "Haowei Yang",
      "Yuxiang Hu",
      "Shuyao He",
      "Ting Xu",
      "Jiajie Yuan",
      "Xingxin Gu"
    ],
    "abstract": "This study introduces an innovative application of Conditional Generative\nAdversarial Networks (C-GAN) integrated with Stacked Hourglass Networks (SHGN)\naimed at enhancing image segmentation, particularly in the challenging\nenvironment of medical imaging. We address the problem of overfitting, common\nin deep learning models applied to complex imaging datasets, by augmenting data\nthrough rotation and scaling. A hybrid loss function combining L1 and L2\nreconstruction losses, enriched with adversarial training, is introduced to\nrefine segmentation processes in intravascular ultrasound (IVUS) imaging. Our\napproach is unique in its capacity to accurately delineate distinct regions\nwithin medical images, such as tissue boundaries and vascular structures,\nwithout extensive reliance on domain-specific knowledge. The algorithm was\nevaluated using a standard medical image library, showing superior performance\nmetrics compared to existing methods, thereby demonstrating its potential in\nenhancing automated medical diagnostics through deep learning",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.02074v1",
    "published_date": "2024-07-17 23:23:09 UTC",
    "updated_date": "2024-07-17 23:23:09 UTC"
  },
  {
    "arxiv_id": "2407.20244v1",
    "title": "Steamroller Problems: An Evaluation of LLM Reasoning Capability with Automated Theorem Prover Strategies",
    "authors": [
      "Lachlan McGinness",
      "Peter Baumgartner"
    ],
    "abstract": "This study presents the first examination of the ability of Large Language\nModels (LLMs) to follow reasoning strategies that are used to guide Automated\nTheorem Provers (ATPs). We evaluate the performance of GPT4, GPT3.5 Turbo and\nGoogle's recent Gemini model on problems from a steamroller domain. In addition\nto determining accuracy we make use of the Natural Language Processing library\nspaCy to explore new methods of investigating LLM's reasoning capabilities.\nThis led to one alarming result, the low correlation between correct reasoning\nand correct answers for any of the tested models. We found that the models'\nperformance when using the ATP reasoning strategies was comparable to one-shot\nchain of thought and observe that attention to uncertainty in the accuracy\nresults is critical when drawing conclusions about model performance.\nConsistent with previous speculation we confirm that LLMs have a preference\nfor, and are best able to follow, bottom up reasoning processes. However, the\nreasoning strategies can still be beneficial for deriving small and relevant\nsets of formulas for external processing by a trusted inference engine.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.20244v1",
    "published_date": "2024-07-17 22:49:23 UTC",
    "updated_date": "2024-07-17 22:49:23 UTC"
  },
  {
    "arxiv_id": "2407.13044v4",
    "title": "DropKAN: Regularizing KANs by masking post-activations",
    "authors": [
      "Mohammed Ghaith Altarabichi"
    ],
    "abstract": "We propose DropKAN (Dropout Kolmogorov-Arnold Networks) a regularization\nmethod that prevents co-adaptation of activation function weights in\nKolmogorov-Arnold Networks (KANs). DropKAN functions by embedding the drop mask\ndirectly within the KAN layer, randomly masking the outputs of some activations\nwithin the KANs' computation graph. We show that this simple procedure that\nrequire minimal coding effort has a regularizing effect and consistently lead\nto better generalization of KANs. We analyze the adaptation of the standard\nDropout with KANs and demonstrate that Dropout applied to KANs' neurons can\nlead to unpredictable behavior in the feedforward pass. We carry an empirical\nstudy with real world Machine Learning datasets to validate our findings. Our\nresults suggest that DropKAN is consistently a better alternative to using\nstandard Dropout with KANs, and improves the generalization performance of\nKANs. Our implementation of DropKAN is available at:\n\\url{https://github.com/Ghaith81/dropkan}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.13044v4",
    "published_date": "2024-07-17 22:48:47 UTC",
    "updated_date": "2024-08-20 12:16:13 UTC"
  },
  {
    "arxiv_id": "2408.00792v1",
    "title": "A Scalable and Generalized Deep Learning Framework for Anomaly Detection in Surveillance Videos",
    "authors": [
      "Sabah Abdulazeez Jebur",
      "Khalid A. Hussein",
      "Haider Kadhim Hoomod",
      "Laith Alzubaidi",
      "Ahmed Ali Saihood",
      "YuanTong Gu"
    ],
    "abstract": "Anomaly detection in videos is challenging due to the complexity, noise, and\ndiverse nature of activities such as violence, shoplifting, and vandalism.\nWhile deep learning (DL) has shown excellent performance in this area, existing\napproaches have struggled to apply DL models across different anomaly tasks\nwithout extensive retraining. This repeated retraining is time-consuming,\ncomputationally intensive, and unfair. To address this limitation, a new DL\nframework is introduced in this study, consisting of three key components:\ntransfer learning to enhance feature generalization, model fusion to improve\nfeature representation, and multi-task classification to generalize the\nclassifier across multiple tasks without training from scratch when new task is\nintroduced. The framework's main advantage is its ability to generalize without\nrequiring retraining from scratch for each new task. Empirical evaluations\ndemonstrate the framework's effectiveness, achieving an accuracy of 97.99% on\nthe RLVS dataset (violence detection), 83.59% on the UCF dataset (shoplifting\ndetection), and 88.37% across both datasets using a single classifier without\nretraining. Additionally, when tested on an unseen dataset, the framework\nachieved an accuracy of 87.25%. The study also utilizes two explainability\ntools to identify potential biases, ensuring robustness and fairness. This\nresearch represents the first successful resolution of the generalization issue\nin anomaly detection, marking a significant advancement in the field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00792v1",
    "published_date": "2024-07-17 22:41:12 UTC",
    "updated_date": "2024-07-17 22:41:12 UTC"
  },
  {
    "arxiv_id": "2407.13036v1",
    "title": "ColorMAE: Exploring data-independent masking strategies in Masked AutoEncoders",
    "authors": [
      "Carlos Hinojosa",
      "Shuming Liu",
      "Bernard Ghanem"
    ],
    "abstract": "Masked AutoEncoders (MAE) have emerged as a robust self-supervised framework,\noffering remarkable performance across a wide range of downstream tasks. To\nincrease the difficulty of the pretext task and learn richer visual\nrepresentations, existing works have focused on replacing standard random\nmasking with more sophisticated strategies, such as adversarial-guided and\nteacher-guided masking. However, these strategies depend on the input data thus\ncommonly increasing the model complexity and requiring additional calculations\nto generate the mask patterns. This raises the question: Can we enhance MAE\nperformance beyond random masking without relying on input data or incurring\nadditional computational costs? In this work, we introduce a simple yet\neffective data-independent method, termed ColorMAE, which generates different\nbinary mask patterns by filtering random noise. Drawing inspiration from color\nnoise in image processing, we explore four types of filters to yield mask\npatterns with different spatial and semantic priors. ColorMAE requires no\nadditional learnable parameters or computational overhead in the network, yet\nit significantly enhances the learned representations. We provide a\ncomprehensive empirical evaluation, demonstrating our strategy's superiority in\ndownstream tasks compared to random masking. Notably, we report an improvement\nof 2.72 in mIoU in semantic segmentation tasks relative to baseline MAE\nimplementations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Work Accepted for Publication at ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.13036v1",
    "published_date": "2024-07-17 22:04:00 UTC",
    "updated_date": "2024-07-17 22:04:00 UTC"
  },
  {
    "arxiv_id": "2407.13032v1",
    "title": "Agent-E: From Autonomous Web Navigation to Foundational Design Principles in Agentic Systems",
    "authors": [
      "Tamer Abuelsaad",
      "Deepak Akkil",
      "Prasenjit Dey",
      "Ashish Jagmohan",
      "Aditya Vempaty",
      "Ravi Kokku"
    ],
    "abstract": "AI Agents are changing the way work gets done, both in consumer and\nenterprise domains. However, the design patterns and architectures to build\nhighly capable agents or multi-agent systems are still developing, and the\nunderstanding of the implication of various design choices and algorithms is\nstill evolving. In this paper, we present our work on building a novel web\nagent, Agent-E \\footnote{Our code is available at\n\\url{https://github.com/EmergenceAI/Agent-E}}. Agent-E introduces numerous\narchitectural improvements over prior state-of-the-art web agents such as\nhierarchical architecture, flexible DOM distillation and denoising method, and\nthe concept of \\textit{change observation} to guide the agent towards more\naccurate performance. We first present the results of an evaluation of Agent-E\non WebVoyager benchmark dataset and show that Agent-E beats other SOTA text and\nmulti-modal web agents on this benchmark in most categories by 10-30\\%. We then\nsynthesize our learnings from the development of Agent-E into general design\nprinciples for developing agentic systems. These include the use of\ndomain-specific primitive skills, the importance of distillation and de-noising\nof environmental observations, the advantages of a hierarchical architecture,\nand the role of agentic self-improvement to enhance agent efficiency and\nefficacy as the agent gathers experience.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.13032v1",
    "published_date": "2024-07-17 21:44:28 UTC",
    "updated_date": "2024-07-17 21:44:28 UTC"
  },
  {
    "arxiv_id": "2407.13025v2",
    "title": "From Principles to Practices: Lessons Learned from Applying Partnership on AI's (PAI) Synthetic Media Framework to 11 Use Cases",
    "authors": [
      "Claire R. Leibowicz",
      "Christian H. Cardona"
    ],
    "abstract": "2023 was the year the world woke up to generative AI, and 2024 is the year\npolicymakers are responding more firmly. Importantly, this policy momentum is\ntaking place alongside real world creation and distribution of synthetic media.\nSocial media platforms, news organizations, dating apps, image generation\ncompanies, and more are already navigating a world of AI-generated visuals and\nsounds, already changing hearts and minds, as policymakers try to catch up.\nHow, then, can AI governance capture the complexity of the synthetic media\nlandscape? How can it attend to synthetic media's myriad uses, ranging from\nstorytelling to privacy preservation, to deception, fraud, and defamation,\ntaking into account the many stakeholders involved in its development,\ncreation, and distribution? And what might it mean to govern synthetic media in\na manner that upholds the truth while bolstering freedom of expression? What\nfollows is the first known collection of diverse examples of the implementation\nof synthetic media governance that responds to these questions, specifically\nthrough Partnership on AI's (PAI) Responsible Practices for Synthetic Media - a\nvoluntary, normative Framework for creating, distributing, and building\ntechnology for synthetic media responsibly, launched in February 2023. In this\npaper, we present a case bank of real world examples that help operationalize\nthe Framework - highlighting areas synthetic media governance can be applied,\naugmented, expanded, and refined for use, in practice. Read together, the cases\nemphasize distinct elements of AI policymaking and seven emergent best\npractices supporting transparency, safety, expression, and digital dignity\nonline: consent, disclosure, and differentiation between harmful and creative\nuse cases.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "18 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2407.13025v2",
    "published_date": "2024-07-17 21:27:56 UTC",
    "updated_date": "2024-07-19 15:57:35 UTC"
  },
  {
    "arxiv_id": "2407.13023v1",
    "title": "A Three-Stage Algorithm for the Closest String Problem on Artificial and Real Gene Sequences",
    "authors": [
      "Alireza Abdi",
      "Marko Djukanovic",
      "Hesam Tahmasebi Boldaji",
      "Hadis Salehi",
      "Aleksandar Kartelj"
    ],
    "abstract": "The Closest String Problem is an NP-hard problem that aims to find a string\nthat has the minimum distance from all sequences that belong to the given set\nof strings. Its applications can be found in coding theory, computational\nbiology, and designing degenerated primers, among others. There are efficient\nexact algorithms that have reached high-quality solutions for binary sequences.\nHowever, there is still room for improvement concerning the quality of\nsolutions over DNA and protein sequences. In this paper, we introduce a\nthree-stage algorithm that comprises the following process: first, we apply a\nnovel alphabet pruning method to reduce the search space for effectively\nfinding promising search regions. Second, a variant of beam search to find a\nheuristic solution is employed. This method utilizes a newly developed guiding\nfunction based on an expected distance heuristic score of partial solutions.\nLast, we introduce a local search to improve the quality of the solution\nobtained from the beam search. Furthermore, due to the lack of real-world\nbenchmarks, two real-world datasets are introduced to verify the robustness of\nthe method. The extensive experimental results show that the proposed method\noutperforms the previous approaches from the literature.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.13023v1",
    "published_date": "2024-07-17 21:26:27 UTC",
    "updated_date": "2024-07-17 21:26:27 UTC"
  },
  {
    "arxiv_id": "2407.13006v1",
    "title": "Sparsity-based Safety Conservatism for Constrained Offline Reinforcement Learning",
    "authors": [
      "Minjae Cho",
      "Chuangchuang Sun"
    ],
    "abstract": "Reinforcement Learning (RL) has made notable success in decision-making\nfields like autonomous driving and robotic manipulation. Yet, its reliance on\nreal-time feedback poses challenges in costly or hazardous settings.\nFurthermore, RL's training approach, centered on \"on-policy\" sampling, doesn't\nfully capitalize on data. Hence, Offline RL has emerged as a compelling\nalternative, particularly in conducting additional experiments is impractical,\nand abundant datasets are available. However, the challenge of distributional\nshift (extrapolation), indicating the disparity between data distributions and\nlearning policies, also poses a risk in offline RL, potentially leading to\nsignificant safety breaches due to estimation errors (interpolation). This\nconcern is particularly pronounced in safety-critical domains, where real-world\nproblems are prevalent. To address both extrapolation and interpolation errors,\nnumerous studies have introduced additional constraints to confine policy\nbehavior, steering it towards more cautious decision-making. While many studies\nhave addressed extrapolation errors, fewer have focused on providing effective\nsolutions for tackling interpolation errors. For example, some works tackle\nthis issue by incorporating potential cost-maximizing optimization by\nperturbing the original dataset. However, this, involving a bi-level\noptimization structure, may introduce significant instability or complicate\nproblem-solving in high-dimensional tasks. This motivates us to pinpoint areas\nwhere hazards may be more prevalent than initially estimated based on the\nsparsity of available data by providing significant insight into constrained\noffline RL. In this paper, we present conservative metrics based on data\nsparsity that demonstrate the high generalizability to any methods and efficacy\ncompared to using bi-level cost-ub-maximization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.13006v1",
    "published_date": "2024-07-17 20:57:05 UTC",
    "updated_date": "2024-07-17 20:57:05 UTC"
  },
  {
    "arxiv_id": "2407.13000v1",
    "title": "Novel Deep Neural Network Classifier Characterization Metrics with Applications to Dataless Evaluation",
    "authors": [
      "Nathaniel Dean",
      "Dilip Sarkar"
    ],
    "abstract": "The mainstream AI community has seen a rise in large-scale open-source\nclassifiers, often pre-trained on vast datasets and tested on standard\nbenchmarks; however, users facing diverse needs and limited, expensive test\ndata may be overwhelmed by available choices. Deep Neural Network (DNN)\nclassifiers undergo training, validation, and testing phases using example\ndataset, with the testing phase focused on determining the classification\naccuracy of test examples without delving into the inner working of the\nclassifier. In this work, we evaluate a DNN classifier's training quality\nwithout any example dataset. It is assumed that a DNN is a composition of a\nfeature extractor and a classifier which is the penultimate completely\nconnected layer. The quality of a classifier is estimated using its weight\nvectors. The feature extractor is characterized using two metrics that utilize\nfeature vectors it produces when synthetic data is fed as input. These\nsynthetic input vectors are produced by backpropagating desired outputs of the\nclassifier. Our empirical study of the proposed method for ResNet18, trained\nwith CAFIR10 and CAFIR100 datasets, confirms that data-less evaluation of DNN\nclassifiers is indeed possible.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.13000v1",
    "published_date": "2024-07-17 20:40:46 UTC",
    "updated_date": "2024-07-17 20:40:46 UTC"
  },
  {
    "arxiv_id": "2407.15866v2",
    "title": "SmartQuant: CXL-based AI Model Store in Support of Runtime Configurable Weight Quantization",
    "authors": [
      "Rui Xie",
      "Asad Ul Haq",
      "Linsen Ma",
      "Krystal Sun",
      "Sanchari Sen",
      "Swagath Venkataramani",
      "Liu Liu",
      "Tong Zhang"
    ],
    "abstract": "Recent studies have revealed that, during the inference on generative AI\nmodels such as transformer, the importance of different weights exhibits\nsubstantial context-dependent variations. This naturally manifests a promising\npotential of adaptively configuring weight quantization to improve the\ngenerative AI inference efficiency. Although configurable weight quantization\ncan readily leverage the hardware support of variable-precision arithmetics in\nmodern GPU and AI accelerators, little prior research has studied how one could\nexploit variable weight quantization to proportionally improve the AI model\nmemory access speed and energy efficiency. Motivated by the rapidly maturing\nCXL ecosystem, this work develops a CXL-based design solution to fill this gap.\nThe key is to allow CXL memory controllers play an active role in supporting\nand exploiting runtime configurable weight quantization. Using transformer as a\nrepresentative generative AI model, we carried out experiments that well\ndemonstrate the effectiveness of the proposed design solution.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15866v2",
    "published_date": "2024-07-17 20:39:49 UTC",
    "updated_date": "2024-08-17 19:44:41 UTC"
  },
  {
    "arxiv_id": "2407.15865v1",
    "title": "A Survey of AI-Powered Mini-Grid Solutions for a Sustainable Future in Rural Communities",
    "authors": [
      "Craig Pirie",
      "Harsha Kalutarage",
      "Muhammad Shadi Hajar",
      "Nirmalie Wiratunga",
      "Subodha Charles",
      "Geeth Sandaru Madhushan",
      "Priyantha Buddhika",
      "Supun Wijesiriwardana",
      "Akila Dimantha",
      "Kithdara Hansamal",
      "Shalitha Pathiranage"
    ],
    "abstract": "This paper presents a comprehensive survey of AI-driven mini-grid solutions\naimed at enhancing sustainable energy access. It emphasises the potential of\nmini-grids, which can operate independently or in conjunction with national\npower grids, to provide reliable and affordable electricity to remote\ncommunities. Given the inherent unpredictability of renewable energy sources\nsuch as solar and wind, the necessity for accurate energy forecasting and\nmanagement is discussed, highlighting the role of advanced AI techniques in\nforecasting energy supply and demand, optimising grid operations, and ensuring\nsustainable energy distribution. This paper reviews various forecasting models,\nincluding statistical methods, machine learning algorithms, and hybrid\napproaches, evaluating their effectiveness for both short-term and long-term\npredictions. Additionally, it explores public datasets and tools such as\nProphet, NeuralProphet, and N-BEATS for model implementation and validation.\nThe survey concludes with recommendations for future research, addressing\nchallenges in model adaptation and optimisation for real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.15865v1",
    "published_date": "2024-07-17 20:23:38 UTC",
    "updated_date": "2024-07-17 20:23:38 UTC"
  },
  {
    "arxiv_id": "2407.12994v2",
    "title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks",
    "authors": [
      "Shubham Vatsal",
      "Harsh Dubey"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable performance on many\ndifferent Natural Language Processing (NLP) tasks. Prompt engineering plays a\nkey role in adding more to the already existing abilities of LLMs to achieve\nsignificant performance gains on various NLP tasks. Prompt engineering requires\ncomposing natural language instructions called prompts to elicit knowledge from\nLLMs in a structured way. Unlike previous state-of-the-art (SoTA) models,\nprompt engineering does not require extensive parameter re-training or\nfine-tuning based on the given NLP task and thus solely operates on the\nembedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently\nextract LLMs' knowledge through a basic natural language conversational\nexchange or prompt engineering, allowing more and more people even without deep\nmathematical machine learning background to experiment with LLMs. With prompt\nengineering gaining popularity in the last two years, researchers have come up\nwith numerous engineering techniques around designing prompts to improve\naccuracy of information extraction from the LLMs. In this paper, we summarize\ndifferent prompting techniques and club them together based on different NLP\ntasks that they have been used for. We further granularly highlight the\nperformance of these prompting strategies on various datasets belonging to that\nNLP task, talk about the corresponding LLMs used, present a taxonomy diagram\nand discuss the possible SoTA for specific datasets. In total, we read and\npresent a survey of 44 research papers which talk about 39 different prompting\nmethods on 29 different NLP tasks of which most of them have been published in\nthe last two years.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12994v2",
    "published_date": "2024-07-17 20:23:19 UTC",
    "updated_date": "2024-07-24 03:53:41 UTC"
  },
  {
    "arxiv_id": "2407.12980v2",
    "title": "A Framework for testing Federated Learning algorithms using an edge-like environment",
    "authors": [
      "Felipe Machado Schwanck",
      "Marcos Tomazzoli Leipnitz",
      "Joel Luís Carbonera",
      "Juliano Araujo Wickboldt"
    ],
    "abstract": "Federated Learning (FL) is a machine learning paradigm in which many clients\ncooperatively train a single centralized model while keeping their data private\nand decentralized. FL is commonly used in edge computing, which involves\nplacing computer workloads (both hardware and software) as close as possible to\nthe edge, where the data is being created and where actions are occurring,\nenabling faster response times, greater data privacy, and reduced data transfer\ncosts. However, due to the heterogeneous data distributions/contents of\nclients, it is non-trivial to accurately evaluate the contributions of local\nmodels in global centralized model aggregation. This is an example of a major\nchallenge in FL, commonly known as data imbalance or class imbalance. In\ngeneral, testing and assessing FL algorithms can be a very difficult and\ncomplex task due to the distributed nature of the systems. In this work, a\nframework is proposed and implemented to assess FL algorithms in a more easy\nand scalable way. This framework is evaluated over a distributed edge-like\nenvironment managed by a container orchestration platform (i.e. Kubernetes).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.NI",
      "C.2.4; I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12980v2",
    "published_date": "2024-07-17 19:52:53 UTC",
    "updated_date": "2024-12-12 21:05:20 UTC"
  },
  {
    "arxiv_id": "2407.12973v1",
    "title": "Temporal Label Hierachical Network for Compound Emotion Recognition",
    "authors": [
      "Sunan Li",
      "Hailun Lian",
      "Cheng Lu",
      "Yan Zhao",
      "Tianhua Qi",
      "Hao Yang",
      "Yuan Zong",
      "Wenming Zheng"
    ],
    "abstract": "The emotion recognition has attracted more attention in recent decades.\nAlthough significant progress has been made in the recognition technology of\nthe seven basic emotions, existing methods are still hard to tackle compound\nemotion recognition that occurred commonly in practical application. This\narticle introduces our achievements in the 7th Field Emotion Behavior Analysis\n(ABAW) competition. In the competition, we selected pre trained ResNet18 and\nTransformer, which have been widely validated, as the basic network framework.\nConsidering the continuity of emotions over time, we propose a time pyramid\nstructure network for frame level emotion prediction. Furthermore. At the same\ntime, in order to address the lack of data in composite emotion recognition, we\nutilized fine-grained labels from the DFEW database to construct training data\nfor emotion categories in competitions. Taking into account the characteristics\nof valence arousal of various complex emotions, we constructed a classification\nframework from coarse to fine in the label space.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "draft for abaw7",
    "pdf_url": "http://arxiv.org/pdf/2407.12973v1",
    "published_date": "2024-07-17 19:38:44 UTC",
    "updated_date": "2024-07-17 19:38:44 UTC"
  },
  {
    "arxiv_id": "2407.12964v1",
    "title": "Learning Long-Horizon Predictions for Quadrotor Dynamics",
    "authors": [
      "Pratyaksh Prabhav Rao",
      "Alessandro Saviolo",
      "Tommaso Castiglione Ferrari",
      "Giuseppe Loianno"
    ],
    "abstract": "Accurate modeling of system dynamics is crucial for achieving\nhigh-performance planning and control of robotic systems. Although existing\ndata-driven approaches represent a promising approach for modeling dynamics,\ntheir accuracy is limited to a short prediction horizon, overlooking the impact\nof compounding prediction errors over longer prediction horizons. Strategies to\nmitigate these cumulative errors remain underexplored. To bridge this gap, in\nthis paper, we study the key design choices for efficiently learning\nlong-horizon prediction dynamics for quadrotors. Specifically, we analyze the\nimpact of multiple architectures, historical data, and multi-step loss\nformulation. We show that sequential modeling techniques showcase their\nadvantage in minimizing compounding errors compared to other types of\nsolutions. Furthermore, we propose a novel decoupled dynamics learning\napproach, which further simplifies the learning process while also enhancing\nthe approach modularity. Extensive experiments and ablation studies on\nreal-world quadrotor data demonstrate the versatility and precision of the\nproposed approach. Our outcomes offer several insights and methodologies for\nenhancing long-term predictive accuracy of learned quadrotor dynamics for\nplanning and control.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 5 figures, 3 tables. Paper accepted by IROS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12964v1",
    "published_date": "2024-07-17 19:06:47 UTC",
    "updated_date": "2024-07-17 19:06:47 UTC"
  },
  {
    "arxiv_id": "2407.13803v1",
    "title": "Less is More: Sparse Watermarking in LLMs with Enhanced Text Quality",
    "authors": [
      "Duy C. Hoang",
      "Hung T. Q. Le",
      "Rui Chu",
      "Ping Li",
      "Weijie Zhao",
      "Yingjie Lao",
      "Khoa D. Doan"
    ],
    "abstract": "With the widespread adoption of Large Language Models (LLMs), concerns about\npotential misuse have emerged. To this end, watermarking has been adapted to\nLLM, enabling a simple and effective way to detect and monitor generated text.\nHowever, while the existing methods can differentiate between watermarked and\nunwatermarked text with high accuracy, they often face a trade-off between the\nquality of the generated text and the effectiveness of the watermarking\nprocess. In this work, we present a novel type of LLM watermark, Sparse\nWatermark, which aims to mitigate this trade-off by applying watermarks to a\nsmall subset of generated tokens distributed across the text. The key strategy\ninvolves anchoring watermarked tokens to words that have specific\nPart-of-Speech (POS) tags. Our experimental results demonstrate that the\nproposed watermarking scheme achieves high detectability while generating text\nthat outperforms previous LLM watermarking methods in quality across various\ntasks",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.13803v1",
    "published_date": "2024-07-17 18:52:12 UTC",
    "updated_date": "2024-07-17 18:52:12 UTC"
  },
  {
    "arxiv_id": "2407.12950v2",
    "title": "Beyond the Veil of Similarity: Quantifying Semantic Continuity in Explainable AI",
    "authors": [
      "Qi Huang",
      "Emanuele Mezzi",
      "Osman Mutlu",
      "Miltiadis Kofinas",
      "Vidya Prasad",
      "Shadnan Azwad Khan",
      "Elena Ranguelova",
      "Niki van Stein"
    ],
    "abstract": "We introduce a novel metric for measuring semantic continuity in Explainable\nAI methods and machine learning models. We posit that for models to be truly\ninterpretable and trustworthy, similar inputs should yield similar\nexplanations, reflecting a consistent semantic understanding. By leveraging XAI\ntechniques, we assess semantic continuity in the task of image recognition. We\nconduct experiments to observe how incremental changes in input affect the\nexplanations provided by different XAI methods. Through this approach, we aim\nto evaluate the models' capability to generalize and abstract semantic concepts\naccurately and to evaluate different XAI methods in correctly capturing the\nmodel behaviour. This paper contributes to the broader discourse on AI\ninterpretability by proposing a quantitative measure for semantic continuity\nfor XAI methods, offering insights into the models' and explainers' internal\nreasoning processes, and promoting more reliable and transparent AI systems.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, accepted at the world conference of explainable AI, 2024,\n  Malta",
    "pdf_url": "http://arxiv.org/pdf/2407.12950v2",
    "published_date": "2024-07-17 18:32:41 UTC",
    "updated_date": "2025-01-30 08:38:34 UTC"
  },
  {
    "arxiv_id": "2407.12943v1",
    "title": "Halu-J: Critique-Based Hallucination Judge",
    "authors": [
      "Binjie Wang",
      "Steffi Chern",
      "Ethan Chern",
      "Pengfei Liu"
    ],
    "abstract": "Large language models (LLMs) frequently generate non-factual content, known\nas hallucinations. Existing retrieval-augmented-based hallucination detection\napproaches typically address this by framing it as a classification task,\nevaluating hallucinations based on their consistency with retrieved evidence.\nHowever, this approach usually lacks detailed explanations for these\nevaluations and does not assess the reliability of these explanations.\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\npartially relevant evidence retrieval, impairing the detection process.\nMoreover, while real-world hallucination detection requires analyzing multiple\npieces of evidence, current systems usually treat all evidence uniformly\nwithout considering its relevance to the content. To address these challenges,\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\nparameters. Halu-J enhances hallucination detection by selecting pertinent\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\ncapability in critique generation and evidence selection. We also introduce\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12943v1",
    "published_date": "2024-07-17 18:21:01 UTC",
    "updated_date": "2024-07-17 18:21:01 UTC"
  },
  {
    "arxiv_id": "2407.12929v2",
    "title": "The 2024 Foundation Model Transparency Index",
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Sayash Kapoor",
      "Shayne Longpre",
      "Betty Xiong",
      "Nestor Maslej",
      "Percy Liang"
    ],
    "abstract": "Foundation models are increasingly consequential yet extremely opaque. To\ncharacterize the status quo, the Foundation Model Transparency Index (FMTI) was\nlaunched in October 2023 to measure the transparency of leading foundation\nmodel developers. FMTI 2023 assessed 10 major foundation model developers (e.g.\nOpenAI, Google) on 100 transparency indicators (e.g. does the developer\ndisclose the wages it pays for data labor?). At the time, developers publicly\ndisclosed very limited information with the average score being 37 out of 100.\nTo understand how the status quo has changed, we conduct a follow-up study\nafter 6 months: we score 14 developers against the same 100 indicators. While\nin FMTI 2023 we searched for publicly available information, in FMTI 2024\ndevelopers submit reports on the 100 transparency indicators, potentially\nincluding information that was not previously public. We find that developers\nnow score 58 out of 100 on average, a 21 point improvement over FMTI 2023. Much\nof this increase is driven by developers disclosing information during the FMTI\n2024 process: on average, developers disclosed information related to 16.6\nindicators that was not previously public. We observe regions of sustained\n(i.e. across 2023 and 2024) and systemic (i.e. across most or all developers)\nopacity such as on copyright status, data access, data labor, and downstream\nimpact. We publish transparency reports for each developer that consolidate\ninformation disclosures: these reports are based on the information disclosed\nto us via developers. Our findings demonstrate that transparency can be\nimproved in this nascent ecosystem, the Foundation Model Transparency Index\nlikely contributes to these improvements, and policymakers should consider\ninterventions in areas where transparency has not improved.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in TMLR 2025. Project page: https://crfm.stanford.edu/fmti",
    "pdf_url": "http://arxiv.org/pdf/2407.12929v2",
    "published_date": "2024-07-17 18:03:37 UTC",
    "updated_date": "2025-03-04 21:07:55 UTC"
  },
  {
    "arxiv_id": "2407.12899v2",
    "title": "DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject Consistent Diffusion",
    "authors": [
      "Huiguo He",
      "Huan Yang",
      "Zixi Tuo",
      "Yuan Zhou",
      "Qiuyue Wang",
      "Yuhang Zhang",
      "Zeyu Liu",
      "Wenhao Huang",
      "Hongyang Chao",
      "Jian Yin"
    ],
    "abstract": "Story visualization aims to create visually compelling images or videos\ncorresponding to textual narratives. Despite recent advances in diffusion\nmodels yielding promising results, existing methods still struggle to create a\ncoherent sequence of subject-consistent frames based solely on a story. To this\nend, we propose DreamStory, an automatic open-domain story visualization\nframework by leveraging the LLMs and a novel multi-subject consistent diffusion\nmodel. DreamStory consists of (1) an LLM acting as a story director and (2) an\ninnovative Multi-Subject consistent Diffusion model (MSD) for generating\nconsistent multi-subject across the images. First, DreamStory employs the LLM\nto generate descriptive prompts for subjects and scenes aligned with the story,\nannotating each scene's subjects for subsequent subject-consistent generation.\nSecond, DreamStory utilizes these detailed subject descriptions to create\nportraits of the subjects, with these portraits and their corresponding textual\ninformation serving as multimodal anchors (guidance). Finally, the MSD uses\nthese multimodal anchors to generate story scenes with consistent\nmulti-subject. Specifically, the MSD includes Masked Mutual Self-Attention\n(MMSA) and Masked Mutual Cross-Attention (MMCA) modules. MMSA and MMCA modules\nensure appearance and semantic consistency with reference images and text,\nrespectively. Both modules employ masking mechanisms to prevent subject\nblending. To validate our approach and promote progress in story visualization,\nwe established a benchmark, DS-500, which can assess the overall performance of\nthe story visualization framework, subject-identification accuracy, and the\nconsistency of the generation model. Extensive experiments validate the\neffectiveness of DreamStory in both subjective and objective evaluations.\nPlease visit our project homepage at https://dream-xyz.github.io/dreamstory.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12899v2",
    "published_date": "2024-07-17 17:54:12 UTC",
    "updated_date": "2025-03-09 13:33:40 UTC"
  },
  {
    "arxiv_id": "2407.12773v1",
    "title": "OMG-Net: A Deep Learning Framework Deploying Segment Anything to Detect Pan-Cancer Mitotic Figures from Haematoxylin and Eosin-Stained Slides",
    "authors": [
      "Zhuoyan Shen",
      "Mikael Simard",
      "Douglas Brand",
      "Vanghelita Andrei",
      "Ali Al-Khader",
      "Fatine Oumlil",
      "Katherine Trevers",
      "Thomas Butters",
      "Simon Haefliger",
      "Eleanna Kara",
      "Fernanda Amary",
      "Roberto Tirabosco",
      "Paul Cool",
      "Gary Royle",
      "Maria A. Hawkins",
      "Adrienne M. Flanagan",
      "Charles-Antoine Collins Fekete"
    ],
    "abstract": "Mitotic activity is an important feature for grading several cancer types.\nCounting mitotic figures (MFs) is a time-consuming, laborious task prone to\ninter-observer variation. Inaccurate recognition of MFs can lead to incorrect\ngrading and hence potential suboptimal treatment. In this study, we propose an\nartificial intelligence (AI)-aided approach to detect MFs in digitised\nhaematoxylin and eosin-stained whole slide images (WSIs). Advances in this area\nare hampered by the limited number and types of cancer datasets of MFs. Here we\nestablish the largest pan-cancer dataset of mitotic figures by combining an\nin-house dataset of soft tissue tumours (STMF) with five open-source mitotic\ndatasets comprising multiple human cancers and canine specimens (ICPR, TUPAC,\nCCMCT, CMC and MIDOG++). This new dataset identifies 74,620 MFs and 105,538\nmitotic-like figures. We then employed a two-stage framework (the Optimised\nMitoses Generator Network (OMG-Net) to classify MFs. The framework first\ndeploys the Segment Anything Model (SAM) to automate the contouring of MFs and\nsurrounding objects. An adapted ResNet18 is subsequently trained to classify\nMFs. OMG-Net reaches an F1-score of 0.84 on pan-cancer MF detection (breast\ncarcinoma, neuroendocrine tumour and melanoma), largely outperforming the\nprevious state-of-the-art MIDOG++ benchmark model on its hold-out testing set\n(e.g. +16% F1-score on breast cancer detection, p<0.001) thereby providing\nsuperior accuracy in detecting MFs on various types of tumours obtained with\ndifferent scanners.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12773v1",
    "published_date": "2024-07-17 17:53:37 UTC",
    "updated_date": "2024-07-17 17:53:37 UTC"
  },
  {
    "arxiv_id": "2407.12753v1",
    "title": "LookupViT: Compressing visual information to a limited number of tokens",
    "authors": [
      "Rajat Koner",
      "Gagan Jain",
      "Prateek Jain",
      "Volker Tresp",
      "Sujoy Paul"
    ],
    "abstract": "Vision Transformers (ViT) have emerged as the de-facto choice for numerous\nindustry grade vision solutions. But their inference cost can be prohibitive\nfor many settings, as they compute self-attention in each layer which suffers\nfrom quadratic computational complexity in the number of tokens. On the other\nhand, spatial information in images and spatio-temporal information in videos\nis usually sparse and redundant. In this work, we introduce LookupViT, that\naims to exploit this information sparsity to reduce ViT inference cost.\nLookupViT provides a novel general purpose vision transformer block that\noperates by compressing information from higher resolution tokens to a fixed\nnumber of tokens. These few compressed tokens undergo meticulous processing,\nwhile the higher-resolution tokens are passed through computationally cheaper\nlayers. Information sharing between these two token sets is enabled through a\nbidirectional cross-attention mechanism. The approach offers multiple\nadvantages - (a) easy to implement on standard ML accelerators (GPUs/TPUs) via\nstandard high-level operators, (b) applicable to standard ViT and its variants,\nthus generalizes to various tasks, (c) can handle different tokenization and\nattention approaches. LookupViT also offers flexibility for the compressed\ntokens, enabling performance-computation trade-offs in a single trained model.\nWe show LookupViT's effectiveness on multiple domains - (a) for\nimage-classification (ImageNet-1K and ImageNet-21K), (b) video classification\n(Kinetics400 and Something-Something V2), (c) image captioning (COCO-Captions)\nwith a frozen encoder. LookupViT provides $2\\times$ reduction in FLOPs while\nupholding or improving accuracy across these domains. In addition, LookupViT\nalso demonstrates out-of-the-box robustness and generalization on image\nclassification (ImageNet-C,R,A,O), improving by up to $4\\%$ over ViT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12753v1",
    "published_date": "2024-07-17 17:22:43 UTC",
    "updated_date": "2024-07-17 17:22:43 UTC"
  },
  {
    "arxiv_id": "2407.12736v3",
    "title": "CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference",
    "authors": [
      "Mohammad Erfan Sadeghi",
      "Arash Fayyazi",
      "Suhas Somashekar",
      "Massoud Pedram"
    ],
    "abstract": "Vision Transformers (ViTs) represent a groundbreaking shift in machine\nlearning approaches to computer vision. Unlike traditional approaches, ViTs\nemploy the self-attention mechanism, which has been widely used in natural\nlanguage processing, to analyze image patches. Despite their advantages in\nmodeling visual tasks, deploying ViTs on hardware platforms, notably\nField-Programmable Gate Arrays (FPGAs), introduces considerable challenges.\nThese challenges stem primarily from the non-linear calculations and high\ncomputational and memory demands of ViTs. This paper introduces CHOSEN, a\nsoftware-hardware co-design framework to address these challenges and offer an\nautomated framework for ViT deployment on the FPGAs in order to maximize\nperformance. Our framework is built upon three fundamental contributions:\nmulti-kernel design to maximize the bandwidth, mainly targeting benefits of\nmulti DDR memory banks, approximate non-linear functions that exhibit minimal\naccuracy degradation, and efficient use of available logic blocks on the FPGA,\nand efficient compiler to maximize the performance and memory-efficiency of the\ncomputing kernels by presenting a novel algorithm for design space exploration\nto find optimal hardware configuration that achieves optimal throughput and\nlatency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a\n1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12736v3",
    "published_date": "2024-07-17 16:56:06 UTC",
    "updated_date": "2024-07-25 00:00:18 UTC"
  },
  {
    "arxiv_id": "2407.12730v3",
    "title": "RoDE: Linear Rectified Mixture of Diverse Experts for Food Large Multi-Modal Models",
    "authors": [
      "Pengkun Jiao",
      "Xinlan Wu",
      "Bin Zhu",
      "Jingjing Chen",
      "Chong-Wah Ngo",
      "Yugang Jiang"
    ],
    "abstract": "Large Multi-modal Models (LMMs) have significantly advanced a variety of\nvision-language tasks. The scalability and availability of high-quality\ntraining data play a pivotal role in the success of LMMs. In the realm of food,\nwhile comprehensive food datasets such as Recipe1M offer an abundance of\ningredient and recipe information, they often fall short of providing ample\ndata for nutritional analysis. The Recipe1M+ dataset, despite offering a subset\nfor nutritional evaluation, is limited in the scale and accuracy of nutrition\ninformation. To bridge this gap, we introduce Uni-Food, a unified food dataset\nthat comprises over 100,000 images with various food labels, including\ncategories, ingredients, recipes, and ingredient-level nutritional information.\nUni-Food is designed to provide a more holistic approach to food data analysis,\nthereby enhancing the performance and capabilities of LMMs in this domain. To\nmitigate the conflicts arising from multi-task supervision during fine-tuning\nof LMMs, we introduce a novel Linear Rectification Mixture of Diverse Experts\n(RoDE) approach. RoDE utilizes a diverse array of experts to address tasks of\nvarying complexity, thereby facilitating the coordination of trainable\nparameters, i.e., it allocates more parameters for more complex tasks and,\nconversely, fewer parameters for simpler tasks. RoDE implements linear\nrectification union to refine the router's functionality, thereby enhancing the\nefficiency of sparse task allocation. These design choices endow RoDE with\nfeatures that ensure GPU memory efficiency and ease of optimization. Our\nexperimental results validate the effectiveness of our proposed approach in\naddressing the inherent challenges of food-related multitasking.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12730v3",
    "published_date": "2024-07-17 16:49:34 UTC",
    "updated_date": "2024-12-16 06:16:27 UTC"
  },
  {
    "arxiv_id": "2407.12724v1",
    "title": "An Evaluation of Continual Learning for Advanced Node Semiconductor Defect Inspection",
    "authors": [
      "Amit Prasad",
      "Bappaditya Dey",
      "Victor Blanco",
      "Sandip Halder"
    ],
    "abstract": "Deep learning-based semiconductor defect inspection has gained traction in\nrecent years, offering a powerful and versatile approach that provides high\naccuracy, adaptability, and efficiency in detecting and classifying nano-scale\ndefects. However, semiconductor manufacturing processes are continually\nevolving, leading to the emergence of new types of defects over time. This\npresents a significant challenge for conventional supervised defect detectors,\nas they may suffer from catastrophic forgetting when trained on new defect\ndatasets, potentially compromising performance on previously learned tasks. An\nalternative approach involves the constant storage of previously trained\ndatasets alongside pre-trained model versions, which can be utilized for\n(re-)training from scratch or fine-tuning whenever encountering a new defect\ndataset. However, adhering to such a storage template is impractical in terms\nof size, particularly when considering High-Volume Manufacturing (HVM).\nAdditionally, semiconductor defect datasets, especially those encompassing\nstochastic defects, are often limited and expensive to obtain, thus lacking\nsufficient representation of the entire universal set of defectivity. This work\nintroduces a task-agnostic, meta-learning approach aimed at addressing this\nchallenge, which enables the incremental addition of new defect classes and\nscales to create a more robust and generalized model for semiconductor defect\ninspection. We have benchmarked our approach using real resist-wafer SEM\n(Scanning Electron Microscopy) datasets for two process steps, ADI and AEI,\ndemonstrating its superior performance compared to conventional supervised\ntraining methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for presentation at the European Conference on Machine\n  Learning and Principles and Practice of Knowledge Discovery in Databases 2024\n  Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2407.12724v1",
    "published_date": "2024-07-17 16:41:22 UTC",
    "updated_date": "2024-07-17 16:41:22 UTC"
  },
  {
    "arxiv_id": "2407.12710v1",
    "title": "A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer Problems",
    "authors": [
      "Mohammad-Amin Charusaie",
      "Samira Samadi"
    ],
    "abstract": "Learn-to-Defer is a paradigm that enables learning algorithms to work not in\nisolation but as a team with human experts. In this paradigm, we permit the\nsystem to defer a subset of its tasks to the expert. Although there are\ncurrently systems that follow this paradigm and are designed to optimize the\naccuracy of the final human-AI team, the general methodology for developing\nsuch systems under a set of constraints (e.g., algorithmic fairness, expert\nintervention budget, defer of anomaly, etc.) remains largely unexplored. In\nthis paper, using a $d$-dimensional generalization to the fundamental lemma of\nNeyman and Pearson (d-GNP), we obtain the Bayes optimal solution for\nlearn-to-defer systems under various constraints. Furthermore, we design a\ngeneralizable algorithm to estimate that solution and apply this algorithm to\nthe COMPAS and ACSIncome datasets. Our algorithm shows improvements in terms of\nconstraint violation over a set of baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12710v1",
    "published_date": "2024-07-17 16:32:30 UTC",
    "updated_date": "2024-07-17 16:32:30 UTC"
  },
  {
    "arxiv_id": "2407.12702v2",
    "title": "TransCAD: A Hierarchical Transformer for CAD Sequence Inference from Point Clouds",
    "authors": [
      "Elona Dupont",
      "Kseniya Cherenkova",
      "Dimitrios Mallis",
      "Gleb Gusev",
      "Anis Kacem",
      "Djamila Aouada"
    ],
    "abstract": "3D reverse engineering, in which a CAD model is inferred given a 3D scan of a\nphysical object, is a research direction that offers many promising practical\napplications. This paper proposes TransCAD, an end-to-end transformer-based\narchitecture that predicts the CAD sequence from a point cloud. TransCAD\nleverages the structure of CAD sequences by using a hierarchical learning\nstrategy. A loop refiner is also introduced to regress sketch primitive\nparameters. Rigorous experimentation on the DeepCAD and Fusion360 datasets show\nthat TransCAD achieves state-of-the-art results. The result analysis is\nsupported with a proposed metric for CAD sequence, the mean Average Precision\nof CAD Sequence, that addresses the limitations of existing metrics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12702v2",
    "published_date": "2024-07-17 16:24:36 UTC",
    "updated_date": "2024-07-18 10:27:36 UTC"
  },
  {
    "arxiv_id": "2408.00790v1",
    "title": "Improving Air Mobility for Pre-Disaster Planning with Neural Network Accelerated Genetic Algorithm",
    "authors": [
      "Kamal Acharya",
      "Alvaro Velasquez",
      "Yongxin Liu",
      "Dahai Liu",
      "Liang Sun",
      "Houbing Song"
    ],
    "abstract": "Weather disaster related emergency operations pose a great challenge to air\nmobility in both aircraft and airport operations, especially when the impact is\ngradually approaching. We propose an optimized framework for adjusting airport\noperational schedules for such pre-disaster scenarios. We first, aggregate\noperational data from multiple airports and then determine the optimal count of\nevacuation flights to maximize the impacted airport's outgoing capacity without\nimpeding regular air traffic. We then propose a novel Neural Network (NN)\naccelerated Genetic Algorithm(GA) for evacuation planning. Our experiments show\nthat integration yielded comparable results but with smaller computational\noverhead. We find that the utilization of a NN enhances the efficiency of a GA,\nfacilitating more rapid convergence even when operating with a reduced\npopulation size. This effectiveness persists even when the model is trained on\ndata from airports different from those under test.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "7 pages, 8 figures, ITSC 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.00790v1",
    "published_date": "2024-07-17 15:59:41 UTC",
    "updated_date": "2024-07-17 15:59:41 UTC"
  },
  {
    "arxiv_id": "2407.12671v1",
    "title": "GraphMuse: A Library for Symbolic Music Graph Processing",
    "authors": [
      "Emmanouil Karystinaios",
      "Gerhard Widmer"
    ],
    "abstract": "Graph Neural Networks (GNNs) have recently gained traction in symbolic music\ntasks, yet a lack of a unified framework impedes progress. Addressing this gap,\nwe present GraphMuse, a graph processing framework and library that facilitates\nefficient music graph processing and GNN training for symbolic music tasks.\nCentral to our contribution is a new neighbor sampling technique specifically\ntargeted toward meaningful behavior in musical scores. Additionally, GraphMuse\nintegrates hierarchical modeling elements that augment the expressivity and\ncapabilities of graph networks for musical tasks. Experiments with two specific\nmusical prediction tasks -- pitch spelling and cadence detection -- demonstrate\nsignificant performance improvement over previous methods. Our hope is that\nGraphMuse will lead to a boost in, and standardization of, symbolic music\nprocessing based on graph representations. The library is available at\nhttps://github.com/manoskary/graphmuse",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.DL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at the 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.12671v1",
    "published_date": "2024-07-17 15:54:09 UTC",
    "updated_date": "2024-07-17 15:54:09 UTC"
  },
  {
    "arxiv_id": "2407.12669v1",
    "title": "Enhancing the Utility of Privacy-Preserving Cancer Classification using Synthetic Data",
    "authors": [
      "Richard Osuala",
      "Daniel M. Lang",
      "Anneliese Riess",
      "Georgios Kaissis",
      "Zuzanna Szafranowska",
      "Grzegorz Skorupko",
      "Oliver Diaz",
      "Julia A. Schnabel",
      "Karim Lekadir"
    ],
    "abstract": "Deep learning holds immense promise for aiding radiologists in breast cancer\ndetection. However, achieving optimal model performance is hampered by\nlimitations in availability and sharing of data commonly associated to patient\nprivacy concerns. Such concerns are further exacerbated, as traditional deep\nlearning models can inadvertently leak sensitive training information. This\nwork addresses these challenges exploring and quantifying the utility of\nprivacy-preserving deep learning techniques, concretely, (i) differentially\nprivate stochastic gradient descent (DP-SGD) and (ii) fully synthetic training\ndata generated by our proposed malignancy-conditioned generative adversarial\nnetwork. We assess these methods via downstream malignancy classification of\nmammography masses using a transformer model. Our experimental results depict\nthat synthetic data augmentation can improve privacy-utility tradeoffs in\ndifferentially private model training. Further, model pretraining on synthetic\ndata achieves remarkable performance, which can be further increased with\nDP-SGD fine-tuning across all privacy guarantees. With this first in-depth\nexploration of privacy-preserving deep learning in breast imaging, we address\ncurrent and emerging clinical privacy requirements and pave the way towards the\nadoption of private high-utility deep diagnostic models. Our reproducible\ncodebase is publicly available at https://github.com/RichardObi/mammo_dp.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Early Accept at MICCAI 2024 Deep-Breath Workshop",
    "pdf_url": "http://arxiv.org/pdf/2407.12669v1",
    "published_date": "2024-07-17 15:52:45 UTC",
    "updated_date": "2024-07-17 15:52:45 UTC"
  },
  {
    "arxiv_id": "2407.12665v3",
    "title": "Beyond Next Token Prediction: Patch-Level Training for Large Language Models",
    "authors": [
      "Chenze Shao",
      "Fandong Meng",
      "Jie Zhou"
    ],
    "abstract": "The prohibitive training costs of Large Language Models (LLMs) have emerged\nas a significant bottleneck in the development of next-generation LLMs. In this\npaper, we show that it is possible to significantly reduce the training costs\nof LLMs without sacrificing their performance. Specifically, we introduce\npatch-level training for LLMs, in which multiple tokens are aggregated into a\nunit of higher information density, referred to as a `patch', to serve as the\nfundamental text unit for training LLMs. During patch-level training, we feed\nthe language model shorter sequences of patches and train it to predict the\nnext patch, thereby processing the majority of the training data at a\nsignificantly reduced cost. Following this, the model continues token-level\ntraining on the remaining training data to align with the inference mode.\nExperiments on a diverse range of models (370M-2.7B parameters) demonstrate\nthat patch-level training can reduce the overall training costs to 0.5$\\times$,\nwithout compromising the model performance compared to token-level training.\nSource code: https://github.com/shaochenze/PatchTrain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025 Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2407.12665v3",
    "published_date": "2024-07-17 15:48:39 UTC",
    "updated_date": "2025-05-15 05:15:13 UTC"
  },
  {
    "arxiv_id": "2407.12663v2",
    "title": "Is That Rain? Understanding Effects on Visual Odometry Performance for Autonomous UAVs and Efficient DNN-based Rain Classification at the Edge",
    "authors": [
      "Andrea Albanese",
      "Yanran Wang",
      "Davide Brunelli",
      "David Boyle"
    ],
    "abstract": "The development of safe and reliable autonomous unmanned aerial vehicles\nrelies on the ability of the system to recognise and adapt to changes in the\nlocal environment based on sensor inputs. State-of-the-art local tracking and\ntrajectory planning are typically performed using camera sensor input to the\nflight control algorithm, but the extent to which environmental disturbances\nlike rain affect the performance of these systems is largely unknown. In this\npaper, we first describe the development of an open dataset comprising ~335k\nimages to examine these effects for seven different classes of precipitation\nconditions and show that a worst-case average tracking error of 1.5 m is\npossible for a state-of-the-art visual odometry system (VINS-Fusion). We then\nuse the dataset to train a set of deep neural network models suited to mobile\nand constrained deployment scenarios to determine the extent to which it may be\npossible to efficiently and accurately classify these `rainy' conditions. The\nmost lightweight of these models (MobileNetV3 small) can achieve an accuracy of\n90% with a memory footprint of just 1.28 MB and a frame rate of 93 FPS, which\nis suitable for deployment in resource-constrained and latency-sensitive\nsystems. We demonstrate a classification latency in the order of milliseconds\nusing typical flight computer hardware. Accordingly, such a model can feed into\nthe disturbance estimation component of an autonomous flight controller. In\naddition, data from unmanned aerial vehicles with the ability to accurately\ndetermine environmental conditions in real time may contribute to developing\nmore granular timely localised weather forecasting.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12663v2",
    "published_date": "2024-07-17 15:47:25 UTC",
    "updated_date": "2025-02-11 10:21:16 UTC"
  },
  {
    "arxiv_id": "2408.00789v1",
    "title": "Machine Learning for Dynamic Management Zone in Smart Farming",
    "authors": [
      "Chamil Kulatunga",
      "Sahraoui Dhelim",
      "Tahar Kechadi"
    ],
    "abstract": "Digital agriculture is growing in popularity among professionals and brings\ntogether new opportunities along with pervasive use of modern data-driven\ntechnologies. Digital agriculture approaches can be used to replace all\ntraditional agricultural system at very reasonable costs. It is very effective\nin optimising large-scale management of resources, while traditional techniques\ncannot even tackle the problem. In this paper, we proposed a dynamic management\nzone delineation approach based on Machine Learning clustering algorithms using\ncrop yield data, elevation and soil texture maps and available NDVI data. Our\nproposed dynamic management zone delineation approach is useful for analysing\nthe spatial variation of yield zones. Delineation of yield regions based on\nhistorical yield data augmented with topography and soil physical properties\nhelps farmers to economically and sustainably deploy site-specific management\npractices identifying persistent issues in a field. The use of frequency maps\nis capable of capturing dynamically changing incidental issues within a growing\nseason. The proposed zone management approach can help farmers/agronomists to\napply variable-rate N fertilisation more effectively by analysing yield\npotential and stability zones with satellite-based NDVI monitoring.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00789v1",
    "published_date": "2024-07-17 15:37:57 UTC",
    "updated_date": "2024-07-17 15:37:57 UTC"
  },
  {
    "arxiv_id": "2407.12647v1",
    "title": "Fusion Flow-enhanced Graph Pooling Residual Networks for Unmanned Aerial Vehicles Surveillance in Day and Night Dual Visions",
    "authors": [
      "Alam Noor",
      "Kai Li",
      "Eduardo Tovar",
      "Pei Zhang",
      "Bo Wei"
    ],
    "abstract": "Recognizing unauthorized Unmanned Aerial Vehicles (UAVs) within designated\nno-fly zones throughout the day and night is of paramount importance, where the\nunauthorized UAVs pose a substantial threat to both civil and military aviation\nsafety. However, recognizing UAVs day and night with dual-vision cameras is\nnontrivial, since red-green-blue (RGB) images suffer from a low detection rate\nunder an insufficient light condition, such as on cloudy or stormy days, while\nblack-and-white infrared (IR) images struggle to capture UAVs that overlap with\nthe background at night. In this paper, we propose a new optical flow-assisted\ngraph-pooling residual network (OF-GPRN), which significantly enhances the UAV\ndetection rate in day and night dual visions. The proposed OF-GPRN develops a\nnew optical fusion to remove superfluous backgrounds, which improves RGB/IR\nimaging clarity. Furthermore, OF-GPRN extends optical fusion by incorporating a\ngraph residual split attention network and a feature pyramid, which refines the\nperception of UAVs, leading to a higher success rate in UAV detection. A\ncomprehensive performance evaluation is conducted using a benchmark UAV catch\ndataset. The results indicate that the proposed OF-GPRN elevates the UAV mean\naverage precision (mAP) detection rate to 87.8%, marking a 17.9% advancement\ncompared to the residual graph neural network (ResGCN)-based approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The article is accepted at July 08, 2024 with 13 pages and 10 figures\n  in the Journal of Engineering Applications of Artificial Intelligence,\n  Elsevier",
    "pdf_url": "http://arxiv.org/pdf/2407.12647v1",
    "published_date": "2024-07-17 15:16:23 UTC",
    "updated_date": "2024-07-17 15:16:23 UTC"
  },
  {
    "arxiv_id": "2407.12642v2",
    "title": "Zero-shot Text-guided Infinite Image Synthesis with LLM guidance",
    "authors": [
      "Soyeong Kwon",
      "Taegyeong Lee",
      "Taehwan Kim"
    ],
    "abstract": "Text-guided image editing and generation methods have diverse real-world\napplications. However, text-guided infinite image synthesis faces several\nchallenges. First, there is a lack of text-image paired datasets with\nhigh-resolution and contextual diversity. Second, expanding images based on\ntext requires global coherence and rich local context understanding. Previous\nstudies have mainly focused on limited categories, such as natural landscapes,\nand also required to train on high-resolution images with paired text. To\naddress these challenges, we propose a novel approach utilizing Large Language\nModels (LLMs) for both global coherence and local context understanding,\nwithout any high-resolution text-image paired training dataset. We train the\ndiffusion model to expand an image conditioned on global and local captions\ngenerated from the LLM and visual feature. At the inference stage, given an\nimage and a global caption, we use the LLM to generate a next local caption to\nexpand the input image. Then, we expand the image using the global caption,\ngenerated local caption and the visual feature to consider global consistency\nand spatial local context. In experiments, our model outperforms the baselines\nboth quantitatively and qualitatively. Furthermore, our model demonstrates the\ncapability of text-guided arbitrary-sized image generation in zero-shot manner\nwith LLM guidance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is being withdrawn due to issues of misconduct in the\n  experiments presented in Table 2 and Figures 6, 7, and 8. We recognize this\n  as an ethical concern and sincerely apologize to the research community for\n  any inconvenience it may have caused",
    "pdf_url": "http://arxiv.org/pdf/2407.12642v2",
    "published_date": "2024-07-17 15:10:01 UTC",
    "updated_date": "2024-12-26 01:49:20 UTC"
  },
  {
    "arxiv_id": "2407.12629v1",
    "title": "A Methodology Establishing Linear Convergence of Adaptive Gradient Methods under PL Inequality",
    "authors": [
      "Kushal Chakrabarti",
      "Mayank Baranwal"
    ],
    "abstract": "Adaptive gradient-descent optimizers are the standard choice for training\nneural network models. Despite their faster convergence than gradient-descent\nand remarkable performance in practice, the adaptive optimizers are not as well\nunderstood as vanilla gradient-descent. A reason is that the dynamic update of\nthe learning rate that helps in faster convergence of these methods also makes\ntheir analysis intricate. Particularly, the simple gradient-descent method\nconverges at a linear rate for a class of optimization problems, whereas the\npractically faster adaptive gradient methods lack such a theoretical guarantee.\nThe Polyak-{\\L}ojasiewicz (PL) inequality is the weakest known class, for which\nlinear convergence of gradient-descent and its momentum variants has been\nproved. Therefore, in this paper, we prove that AdaGrad and Adam, two\nwell-known adaptive gradient methods, converge linearly when the cost function\nis smooth and satisfies the PL inequality. Our theoretical framework follows a\nsimple and unified approach, applicable to both batch and stochastic gradients,\nwhich can potentially be utilized in analyzing linear convergence of other\nvariants of Adam.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication at the main track of 27th European\n  Conference on Artificial Intelligence (ECAI-2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.12629v1",
    "published_date": "2024-07-17 14:56:21 UTC",
    "updated_date": "2024-07-17 14:56:21 UTC"
  },
  {
    "arxiv_id": "2407.18267v1",
    "title": "MCU-MixQ: A HW/SW Co-optimized Mixed-precision Neural Network Design Framework for MCUs",
    "authors": [
      "Junfeng Gong",
      "Cheng Liu",
      "Long Cheng",
      "Huawei Li",
      "Xiaowei Li"
    ],
    "abstract": "Mixed-precision neural network (MPNN) that utilizes just enough data width\nfor the neural network processing is an effective approach to meet the\nstringent resources constraints including memory and computing of MCUs.\nNevertheless, there is still a lack of sub-byte and mixed-precision SIMD\noperations in MCU-class ISA and the limited computing capability of MCUs\nremains underutilized, which further aggravates the computing bound encountered\nin neural network processing. As a result, the benefits of MPNNs cannot be\nfully unleashed. In this work, we propose to pack multiple low-bitwidth\narithmetic operations within a single instruction multiple data (SIMD)\ninstructions in typical MCUs, and then develop an efficient convolution\noperator by exploring both the data parallelism and computing parallelism in\nconvolution along with the proposed SIMD packing. Finally, we further leverage\nNeural Architecture Search (NAS) to build a HW/SW co-designed MPNN design\nframework, namely MCU-MixQ. This framework can optimize both the MPNN\nquantization and MPNN implementation efficiency, striking an optimized balance\nbetween neural network performance and accuracy. According to our experiment\nresults, MCU-MixQ achieves 2.1$\\times$ and 1.4$\\times$ speedup over CMix-NN and\nMCUNet respectively under the same resource constraints.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.18267v1",
    "published_date": "2024-07-17 14:51:15 UTC",
    "updated_date": "2024-07-17 14:51:15 UTC"
  },
  {
    "arxiv_id": "2407.12620v2",
    "title": "Harnessing the Power of Artificial Intelligence to Vitalize Endangered Indigenous Languages: Technologies and Experiences",
    "authors": [
      "Claudio Pinhanez",
      "Paulo Cavalin",
      "Luciana Storto",
      "Thomas Finbow",
      "Alexander Cobbinah",
      "Julio Nogima",
      "Marisa Vasconcelos",
      "Pedro Domingues",
      "Priscila de Souza Mizukami",
      "Nicole Grell",
      "Majoí Gongora",
      "Isabel Gonçalves"
    ],
    "abstract": "Since 2022 we have been exploring application areas and technologies in which\nArtificial Intelligence (AI) and modern Natural Language Processing (NLP), such\nas Large Language Models (LLMs), can be employed to foster the usage and\nfacilitate the documentation of Indigenous languages which are in danger of\ndisappearing. We start by discussing the decreasing diversity of languages in\nthe world and how working with Indigenous languages poses unique ethical\nchallenges for AI and NLP. To address those challenges, we propose an\nalternative development AI cycle based on community engagement and usage. Then,\nwe report encouraging results in the development of high-quality machine\nlearning translators for Indigenous languages by fine-tuning state-of-the-art\n(SOTA) translators with tiny amounts of data and discuss how to avoid some\ncommon pitfalls in the process. We also present prototypes we have built in\nprojects done in 2023 and 2024 with Indigenous communities in Brazil, aimed at\nfacilitating writing, and discuss the development of Indigenous Language Models\n(ILMs) as a replicable and scalable way to create spell-checkers, next-word\npredictors, and similar tools. Finally, we discuss how we envision a future for\nlanguage documentation where dying languages are preserved as interactive\nlanguage models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12620v2",
    "published_date": "2024-07-17 14:46:37 UTC",
    "updated_date": "2024-07-29 17:19:43 UTC"
  },
  {
    "arxiv_id": "2407.12616v1",
    "title": "Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models",
    "authors": [
      "Donggeun Kim",
      "Taesup Kim"
    ],
    "abstract": "Multimodal learning typically relies on the assumption that all modalities\nare fully available during both the training and inference phases. However, in\nreal-world scenarios, consistently acquiring complete multimodal data presents\nsignificant challenges due to various factors. This often leads to the issue of\nmissing modalities, where data for certain modalities are absent, posing\nconsiderable obstacles not only for the availability of multimodal pretrained\nmodels but also for their fine-tuning and the preservation of robustness in\ndownstream tasks. To address these challenges, we propose a novel framework\nintegrating parameter-efficient fine-tuning of unimodal pretrained models with\na self-supervised joint-embedding learning method. This framework enables the\nmodel to predict the embedding of a missing modality in the representation\nspace during inference. Our method effectively predicts the missing embedding\nthrough prompt tuning, leveraging information from available modalities. We\nevaluate our approach on several multimodal benchmark datasets and demonstrate\nits effectiveness and robustness across various scenarios of missing\nmodalities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12616v1",
    "published_date": "2024-07-17 14:44:25 UTC",
    "updated_date": "2024-07-17 14:44:25 UTC"
  },
  {
    "arxiv_id": "2407.12609v1",
    "title": "Instance-wise Uncertainty for Class Imbalance in Semantic Segmentation",
    "authors": [
      "Luís Almeida",
      "Inês Dutra",
      "Francesco Renna"
    ],
    "abstract": "Semantic segmentation is a fundamental computer vision task with a vast\nnumber of applications. State of the art methods increasingly rely on deep\nlearning models, known to incorrectly estimate uncertainty and being\noverconfident in predictions, especially in data not seen during training. This\nis particularly problematic in semantic segmentation due to inherent class\nimbalance. Popular uncertainty quantification approaches are task-agnostic and\nfail to leverage spatial pixel correlations in uncertainty estimates, crucial\nin this task. In this work, a novel training methodology specifically designed\nfor semantic segmentation is presented. Training samples are weighted by\ninstance-wise uncertainty masks computed by an ensemble. This is shown to\nincrease performance on minority classes, boost model generalization and\nrobustness to domain-shift when compared to using the inverse of class\nproportions or no class weights at all. This method addresses the challenges of\nclass imbalance and uncertainty estimation in semantic segmentation,\npotentially enhancing model performance and reliability across various\napplications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12609v1",
    "published_date": "2024-07-17 14:38:32 UTC",
    "updated_date": "2024-07-17 14:38:32 UTC"
  },
  {
    "arxiv_id": "2408.00786v2",
    "title": "Whether to trust: the ML leap of faith",
    "authors": [
      "Tory Frame",
      "Julian Padget",
      "George Stothart",
      "Elizabeth Coulthard"
    ],
    "abstract": "Human trust is a prerequisite to trustworthy AI adoption, yet trust remains\npoorly understood. Trust is often described as an attitude, but attitudes\ncannot be reliably measured or managed. Additionally, humans frequently\nconflate trust in an AI system, its machine learning (ML) technology, and its\nother component parts. Without fully understanding the 'leap of faith' involved\nin trusting ML, users cannot develop intrinsic trust in these systems. A common\napproach to building trust is to explain a ML model's reasoning process.\nHowever, such explanations often fail to resonate with non-experts due to the\ninherent complexity of ML systems and explanations are disconnected from users'\nown (unarticulated) mental models. This work puts forward an innovative way of\ndirectly building intrinsic trust in ML, by discerning and measuring the Leap\nof Faith (LoF) taken when a user decides to rely on ML. The LoF matrix captures\nthe alignment between an ML model and a human expert's mental model. This match\nis rigorously and practically identified by feeding the user's data and\nobjective function into both an ML agent and an expert-validated rules-based\nagent: a verified point of reference that can be tested a priori against a\nuser's own mental model. This represents a new class of neuro-symbolic\narchitecture. The LoF matrix reveals to the user the distance that constitutes\nthe leap of faith between the rules-based and ML agents. For the first time, we\npropose trust metrics that evaluate whether users demonstrate trust through\ntheir actions rather than self-reported intent and whether such trust is\ndeserved based on outcomes. The significance of the contribution is that it\nenables empirical assessment and management of ML trust drivers, to support\ntrustworthy ML adoption. The approach is illustrated through a long-term\nhigh-stakes field study: a 3-month pilot of a multi-agent sleep-improvement\nsystem.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "12 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.00786v2",
    "published_date": "2024-07-17 14:36:19 UTC",
    "updated_date": "2025-01-23 17:53:31 UTC"
  },
  {
    "arxiv_id": "2407.12605v2",
    "title": "Continuous reasoning for adaptive container image distribution in the cloud-edge continuum",
    "authors": [
      "Damiano Azzolini",
      "Stefano Forti",
      "Antonio Ielo"
    ],
    "abstract": "Cloud-edge computing requires applications to operate across diverse\ninfrastructures, often triggered by cyber-physical events. Containers offer a\nlightweight deployment option but pulling images from central repositories can\ncause delays. This article presents a novel declarative approach and\nopen-source prototype for replicating container images across the cloud-edge\ncontinuum. Considering resource availability, network QoS, and storage costs,\nwe leverage logic programming to (i) determine optimal initial placements via\nAnswer Set Programming (ASP) and (ii) adapt placements using Prolog-based\ncontinuous reasoning. We evaluate our solution through simulations, showcasing\nhow combining ASP and Prolog continuous reasoning can balance cost optimisation\nand prompt decision-making in placement adaptation at increasing infrastructure\nsizes.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LO",
      "cs.SE"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12605v2",
    "published_date": "2024-07-17 14:33:52 UTC",
    "updated_date": "2025-04-05 08:30:57 UTC"
  },
  {
    "arxiv_id": "2407.12599v1",
    "title": "On Diversity in Discriminative Neural Networks",
    "authors": [
      "Brahim Oubaha",
      "Claude Berrou",
      "Xueyao Ji",
      "Yehya Nasser",
      "Raphaël Le Bidan"
    ],
    "abstract": "Diversity is a concept of prime importance in almost all disciplines based on\ninformation processing. In telecommunications, for example, spatial, temporal,\nand frequency diversity, as well as redundant coding, are fundamental concepts\nthat have enabled the design of extremely efficient systems. In machine\nlearning, in particular with neural networks, diversity is not always a concept\nthat is emphasized or at least clearly identified. This paper proposes a neural\nnetwork architecture that builds upon various diversity principles, some of\nthem already known, others more original. Our architecture obtains remarkable\nresults, with a record self-supervised learning accuracy of 99. 57% in MNIST,\nand a top tier promising semi-supervised learning accuracy of 94.21% in\nCIFAR-10 using only 25 labels per class.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in: 2024 IEEE 12th International Symposium on Signal,\n  Image, Video and Communications (ISIVC)",
    "pdf_url": "http://arxiv.org/pdf/2407.12599v1",
    "published_date": "2024-07-17 14:26:44 UTC",
    "updated_date": "2024-07-17 14:26:44 UTC"
  },
  {
    "arxiv_id": "2407.12588v2",
    "title": "Benchmarking Robust Self-Supervised Learning Across Diverse Downstream Tasks",
    "authors": [
      "Antoni Kowalczuk",
      "Jan Dubiński",
      "Atiyeh Ashari Ghomi",
      "Yi Sui",
      "George Stein",
      "Jiapeng Wu",
      "Jesse C. Cresswell",
      "Franziska Boenisch",
      "Adam Dziedzic"
    ],
    "abstract": "Large-scale vision models have become integral in many applications due to\ntheir unprecedented performance and versatility across downstream tasks.\nHowever, the robustness of these foundation models has primarily been explored\nfor a single task, namely image classification. The vulnerability of other\ncommon vision tasks, such as semantic segmentation and depth estimation,\nremains largely unknown. We present a comprehensive empirical evaluation of the\nadversarial robustness of self-supervised vision encoders across multiple\ndownstream tasks. Our attacks operate in the encoder embedding space and at the\ndownstream task output level. In both cases, current state-of-the-art\nadversarial fine-tuning techniques tested only for classification significantly\ndegrade clean and robust performance on other tasks. Since the purpose of a\nfoundation model is to cater to multiple applications at once, our findings\nreveal the need to enhance encoder robustness more broadly. Our code is\navailable at ${github.com/layer6ai-labs/ssl-robustness}$.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at the ICML 2024 Workshop on Foundation Models in the Wild",
    "pdf_url": "http://arxiv.org/pdf/2407.12588v2",
    "published_date": "2024-07-17 14:12:34 UTC",
    "updated_date": "2024-07-18 06:55:33 UTC"
  },
  {
    "arxiv_id": "2407.12582v2",
    "title": "Embracing Events and Frames with Hierarchical Feature Refinement Network for Object Detection",
    "authors": [
      "Hu Cao",
      "Zehua Zhang",
      "Yan Xia",
      "Xinyi Li",
      "Jiahao Xia",
      "Guang Chen",
      "Alois Knoll"
    ],
    "abstract": "In frame-based vision, object detection faces substantial performance\ndegradation under challenging conditions due to the limited sensing capability\nof conventional cameras. Event cameras output sparse and asynchronous events,\nproviding a potential solution to solve these problems. However, effectively\nfusing two heterogeneous modalities remains an open issue. In this work, we\npropose a novel hierarchical feature refinement network for event-frame fusion.\nThe core concept is the design of the coarse-to-fine fusion module, denoted as\nthe cross-modality adaptive feature refinement (CAFR) module. In the initial\nphase, the bidirectional cross-modality interaction (BCI) part facilitates\ninformation bridging from two distinct sources. Subsequently, the features are\nfurther refined by aligning the channel-level mean and variance in the two-fold\nadaptive feature refinement (TAFR) part. We conducted extensive experiments on\ntwo benchmarks: the low-resolution PKU-DDD17-Car dataset and the\nhigh-resolution DSEC dataset. Experimental results show that our method\nsurpasses the state-of-the-art by an impressive margin of $\\textbf{8.0}\\%$ on\nthe DSEC dataset. Besides, our method exhibits significantly better robustness\n(\\textbf{69.5}\\% versus \\textbf{38.7}\\%) when introducing 15 different\ncorruption types to the frame images. The code can be found at the link\n(https://github.com/HuCaoFighting/FRN).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12582v2",
    "published_date": "2024-07-17 14:09:46 UTC",
    "updated_date": "2024-10-31 14:37:42 UTC"
  },
  {
    "arxiv_id": "2407.12581v1",
    "title": "Towards Understanding Unsafe Video Generation",
    "authors": [
      "Yan Pang",
      "Aiping Xiong",
      "Yang Zhang",
      "Tianhao Wang"
    ],
    "abstract": "Video generation models (VGMs) have demonstrated the capability to synthesize\nhigh-quality output. It is important to understand their potential to produce\nunsafe content, such as violent or terrifying videos. In this work, we provide\na comprehensive understanding of unsafe video generation.\n  First, to confirm the possibility that these models could indeed generate\nunsafe videos, we choose unsafe content generation prompts collected from 4chan\nand Lexica, and three open-source SOTA VGMs to generate unsafe videos. After\nfiltering out duplicates and poorly generated content, we created an initial\nset of 2112 unsafe videos from an original pool of 5607 videos. Through\nclustering and thematic coding analysis of these generated videos, we identify\n5 unsafe video categories: Distorted/Weird, Terrifying, Pornographic,\nViolent/Bloody, and Political. With IRB approval, we then recruit online\nparticipants to help label the generated videos. Based on the annotations\nsubmitted by 403 participants, we identified 937 unsafe videos from the initial\nvideo set. With the labeled information and the corresponding prompts, we\ncreated the first dataset of unsafe videos generated by VGMs.\n  We then study possible defense mechanisms to prevent the generation of unsafe\nvideos. Existing defense methods in image generation focus on filtering either\ninput prompt or output results. We propose a new approach called Latent\nVariable Defense (LVD), which works within the model's internal sampling\nprocess. LVD can achieve 0.90 defense accuracy while reducing time and\ncomputing resources by 10x when sampling a large number of unsafe prompts.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.12581v1",
    "published_date": "2024-07-17 14:07:22 UTC",
    "updated_date": "2024-07-17 14:07:22 UTC"
  },
  {
    "arxiv_id": "2407.12579v1",
    "title": "The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation",
    "authors": [
      "Yi Yao",
      "Chan-Feng Hsu",
      "Jhe-Hao Lin",
      "Hongxia Xie",
      "Terence Lin",
      "Yi-Ning Huang",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ],
    "abstract": "In spite of recent advancements in text-to-image generation, limitations\npersist in handling complex and imaginative prompts due to the restricted\ndiversity and complexity of training data. This work explores how diffusion\nmodels can generate images from prompts requiring artistic creativity or\nspecialized knowledge. We introduce the Realistic-Fantasy Benchmark (RFBench),\na novel evaluation framework blending realistic and fantastical scenarios. To\naddress these challenges, we propose the Realistic-Fantasy Network (RFNet), a\ntraining-free approach integrating diffusion models with LLMs. Extensive human\nevaluations and GPT-based compositional assessments demonstrate our approach's\nsuperiority over state-of-the-art methods. Our code and dataset is available at\nhttps://leo81005.github.io/Reality-and-Fantasy/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12579v1",
    "published_date": "2024-07-17 14:04:10 UTC",
    "updated_date": "2024-07-17 14:04:10 UTC"
  },
  {
    "arxiv_id": "2407.12576v2",
    "title": "IICPilot: An Intelligent Integrated Circuit Backend Design Framework Using Open EDA",
    "authors": [
      "Zesong Jiang",
      "Qing Zhang",
      "Cheng Liu",
      "Long Cheng",
      "Huawei Li",
      "Xiaowei Li"
    ],
    "abstract": "Open-source EDA tools are rapidly advancing, fostering collaboration,\ninnovation, and knowledge sharing within the EDA community. However, the\ngrowing complexity of these tools, characterized by numerous design parameters\nand heuristics, poses a significant barrier to their widespread adoption. This\ncomplexity is particularly pronounced in integrated circuit (IC) backend\ndesigns, which place substantial demands on engineers' expertise in EDA tools.\nTo tackle this challenge, we introduce IICPilot, an intelligent IC backend\ndesign system based on LLM technology. IICPilot automates various backend\ndesign procedures, including script generation, EDA tool invocation, design\nspace exploration of EDA parameters, container-based computing resource\nallocation, and exception management. By automating these tasks, IICPilot\nsignificantly lowers the barrier to entry for open-source EDA tools.\nSpecifically, IICPilot utilizes LangChain's multi-agent framework to\nefficiently handle distinct design tasks, enabling flexible enhancements\nindependently. Moreover, IICPilot separates the backend design workflow from\nspecific open-source EDA tools through a unified EDA calling interface. This\napproach allows seamless integration with different open-source EDA tools like\nOpenROAD and iEDA, streamlining the backend design and optimization across the\nEDA tools.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2407.12576v2",
    "published_date": "2024-07-17 14:02:01 UTC",
    "updated_date": "2024-08-28 03:15:10 UTC"
  },
  {
    "arxiv_id": "2407.12543v2",
    "title": "Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships",
    "authors": [
      "Angie Boggust",
      "Hyemin Bang",
      "Hendrik Strobelt",
      "Arvind Satyanarayan"
    ],
    "abstract": "While interpretability methods identify a model's learned concepts, they\noverlook the relationships between concepts that make up its abstractions and\ninform its ability to generalize to new data. To assess whether models' have\nlearned human-aligned abstractions, we introduce abstraction alignment, a\nmethodology to compare model behavior against formal human knowledge.\nAbstraction alignment externalizes domain-specific human knowledge as an\nabstraction graph, a set of pertinent concepts spanning levels of abstraction.\nUsing the abstraction graph as a ground truth, abstraction alignment measures\nthe alignment of a model's behavior by determining how much of its uncertainty\nis accounted for by the human abstractions. By aggregating abstraction\nalignment across entire datasets, users can test alignment hypotheses, such as\nwhich human concepts the model has learned and where misalignments recur. In\nevaluations with experts, abstraction alignment differentiates seemingly\nsimilar errors, improves the verbosity of existing model-quality metrics, and\nuncovers improvements to current human abstractions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 7 figures, published in CHI 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.12543v2",
    "published_date": "2024-07-17 13:27:26 UTC",
    "updated_date": "2025-02-13 20:47:20 UTC"
  },
  {
    "arxiv_id": "2407.12532v1",
    "title": "Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models",
    "authors": [
      "Xihe Qiu",
      "Haoyu Wang",
      "Xiaoyu Tan",
      "Chao Qu",
      "Yujie Xiong",
      "Yuan Cheng",
      "Yinghui Xu",
      "Wei Chu",
      "Yuan Qi"
    ],
    "abstract": "Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12532v1",
    "published_date": "2024-07-17 13:14:00 UTC",
    "updated_date": "2024-07-17 13:14:00 UTC"
  },
  {
    "arxiv_id": "2407.12528v1",
    "title": "On the Complexity of Identification in Linear Structural Causal Models",
    "authors": [
      "Julian Dörfler",
      "Benito van der Zander",
      "Markus Bläser",
      "Maciej Liskiewicz"
    ],
    "abstract": "Learning the unknown causal parameters of a linear structural causal model is\na fundamental task in causal analysis. The task, known as the problem of\nidentification, asks to estimate the parameters of the model from a combination\nof assumptions on the graphical structure of the model and observational data,\nrepresented as a non-causal covariance matrix. In this paper, we give a new\nsound and complete algorithm for generic identification which runs in\npolynomial space. By standard simulation results, this algorithm has\nexponential running time which vastly improves the state-of-the-art double\nexponential time method using a Gr\\\"obner basis approach. The paper also\npresents evidence that parameter identification is computationally hard in\ngeneral. In particular, we prove, that the task asking whether, for a given\nfeasible correlation matrix, there are exactly one or two or more parameter\nsets explaining the observed matrix, is hard for $\\forall R$, the co-class of\nthe existential theory of the reals. In particular, this problem is\n$coNP$-hard. To our best knowledge, this is the first hardness result for some\nnotion of identifiability.",
    "categories": [
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12528v1",
    "published_date": "2024-07-17 13:11:26 UTC",
    "updated_date": "2024-07-17 13:11:26 UTC"
  },
  {
    "arxiv_id": "2407.12522v1",
    "title": "Struct-X: Enhancing Large Language Models Reasoning with Structured Data",
    "authors": [
      "Xiaoyu Tan",
      "Haoyu Wang",
      "Xihe Qiu",
      "Yuan Cheng",
      "Yinghui Xu",
      "Wei Chu",
      "Yuan Qi"
    ],
    "abstract": "Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12522v1",
    "published_date": "2024-07-17 13:06:25 UTC",
    "updated_date": "2024-07-17 13:06:25 UTC"
  },
  {
    "arxiv_id": "2407.12516v1",
    "title": "Online Pseudo-Zeroth-Order Training of Neuromorphic Spiking Neural Networks",
    "authors": [
      "Mingqing Xiao",
      "Qingyan Meng",
      "Zongpeng Zhang",
      "Di He",
      "Zhouchen Lin"
    ],
    "abstract": "Brain-inspired neuromorphic computing with spiking neural networks (SNNs) is\na promising energy-efficient computational approach. However, successfully\ntraining SNNs in a more biologically plausible and\nneuromorphic-hardware-friendly way is still challenging. Most recent methods\nleverage spatial and temporal backpropagation (BP), not adhering to\nneuromorphic properties. Despite the efforts of some online training methods,\ntackling spatial credit assignments by alternatives with comparable performance\nas spatial BP remains a significant problem. In this work, we propose a novel\nmethod, online pseudo-zeroth-order (OPZO) training. Our method only requires a\nsingle forward propagation with noise injection and direct top-down signals for\nspatial credit assignment, avoiding spatial BP's problem of symmetric weights\nand separate phases for layer-by-layer forward-backward propagation. OPZO\nsolves the large variance problem of zeroth-order methods by the\npseudo-zeroth-order formulation and momentum feedback connections, while having\nmore guarantees than random feedback. Combining online training, OPZO can pave\npaths to on-chip SNN training. Experiments on neuromorphic and static datasets\nwith fully connected and convolutional networks demonstrate the effectiveness\nof OPZO with similar performance compared with spatial BP, as well as estimated\nlow training costs.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12516v1",
    "published_date": "2024-07-17 12:09:00 UTC",
    "updated_date": "2024-07-17 12:09:00 UTC"
  },
  {
    "arxiv_id": "2407.12894v1",
    "title": "Maintenance Strategies for Sewer Pipes with Multi-State Degradation and Deep Reinforcement Learning",
    "authors": [
      "Lisandro A. Jimenez-Roa",
      "Thiago D. Simão",
      "Zaharah Bukhsh",
      "Tiedo Tinga",
      "Hajo Molegraaf",
      "Nils Jansen",
      "Marielle Stoelinga"
    ],
    "abstract": "Large-scale infrastructure systems are crucial for societal welfare, and\ntheir effective management requires strategic forecasting and intervention\nmethods that account for various complexities. Our study addresses two\nchallenges within the Prognostics and Health Management (PHM) framework applied\nto sewer assets: modeling pipe degradation across severity levels and\ndeveloping effective maintenance policies. We employ Multi-State Degradation\nModels (MSDM) to represent the stochastic degradation process in sewer pipes\nand use Deep Reinforcement Learning (DRL) to devise maintenance strategies. A\ncase study of a Dutch sewer network exemplifies our methodology. Our findings\ndemonstrate the model's effectiveness in generating intelligent, cost-saving\nmaintenance strategies that surpass heuristics. It adapts its management\nstrategy based on the pipe's age, opting for a passive approach for newer pipes\nand transitioning to active strategies for older ones to prevent failures and\nreduce costs. This research highlights DRL's potential in optimizing\nmaintenance policies. Future research will aim improve the model by\nincorporating partial observability, exploring various reinforcement learning\nalgorithms, and extending this methodology to comprehensive infrastructure\nmanagement.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12894v1",
    "published_date": "2024-07-17 12:07:07 UTC",
    "updated_date": "2024-07-17 12:07:07 UTC"
  },
  {
    "arxiv_id": "2407.12508v2",
    "title": "MERLIN: Multimodal Embedding Refinement via LLM-based Iterative Navigation for Text-Video Retrieval-Rerank Pipeline",
    "authors": [
      "Donghoon Han",
      "Eunhwan Park",
      "Gisang Lee",
      "Adam Lee",
      "Nojun Kwak"
    ],
    "abstract": "The rapid expansion of multimedia content has made accurately retrieving\nrelevant videos from large collections increasingly challenging. Recent\nadvancements in text-video retrieval have focused on cross-modal interactions,\nlarge-scale foundation model training, and probabilistic modeling, yet often\nneglect the crucial user perspective, leading to discrepancies between user\nqueries and the content retrieved. To address this, we introduce MERLIN\n(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,\ntraining-free pipeline that leverages Large Language Models (LLMs) for\niterative feedback learning. MERLIN refines query embeddings from a user\nperspective, enhancing alignment between queries and video content through a\ndynamic question answering process. Experimental results on datasets like\nMSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves\nRecall@1, outperforming existing systems and confirming the benefits of\nintegrating LLMs into multimodal retrieval systems for more responsive and\ncontext-aware multimedia retrieval.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Industry Track Accepted (Camera-Ready Version)",
    "pdf_url": "http://arxiv.org/pdf/2407.12508v2",
    "published_date": "2024-07-17 11:45:02 UTC",
    "updated_date": "2024-10-16 06:25:50 UTC"
  },
  {
    "arxiv_id": "2407.12505v1",
    "title": "Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments",
    "authors": [
      "Runfa Chen",
      "Ling Wang",
      "Yu Du",
      "Tianrui Xue",
      "Fuchun Sun",
      "Jianwei Zhang",
      "Wenbing Huang"
    ],
    "abstract": "Learning policies for multi-entity systems in 3D environments is far more\ncomplicated against single-entity scenarios, due to the exponential expansion\nof the global state space as the number of entities increases. One potential\nsolution of alleviating the exponential complexity is dividing the global space\ninto independent local views that are invariant to transformations including\ntranslations and rotations. To this end, this paper proposes Subequivariant\nHierarchical Neural Networks (SHNN) to facilitate multi-entity policy learning.\nIn particular, SHNN first dynamically decouples the global space into local\nentity-level graphs via task assignment. Second, it leverages subequivariant\nmessage passing over the local entity-level graphs to devise local reference\nframes, remarkably compressing the representation redundancy, particularly in\ngravity-affected environments. Furthermore, to overcome the limitations of\nexisting benchmarks in capturing the subtleties of multi-entity systems under\nthe Euclidean symmetry, we propose the Multi-entity Benchmark (MEBEN), a new\nsuite of environments tailored for exploring a wide range of multi-entity\nreinforcement learning. Extensive experiments demonstrate significant\nadvancements of SHNN on the proposed benchmarks compared to existing methods.\nComprehensive ablations are conducted to verify the indispensability of task\nassignment and subequivariance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12505v1",
    "published_date": "2024-07-17 11:37:34 UTC",
    "updated_date": "2024-07-17 11:37:34 UTC"
  },
  {
    "arxiv_id": "2407.12492v2",
    "title": "Temporal Test-Time Adaptation with State-Space Models",
    "authors": [
      "Mona Schirmer",
      "Dan Zhang",
      "Eric Nalisnick"
    ],
    "abstract": "Distribution shifts between training and test data are inevitable over the\nlifecycle of a deployed model, leading to performance decay. Adapting a model\non test samples can help mitigate this drop in performance. However, most\ntest-time adaptation methods have focused on synthetic corruption shifts,\nleaving a variety of distribution shifts underexplored. In this paper, we focus\non distribution shifts that evolve gradually over time, which are common in the\nwild but challenging for existing methods, as we show. To address this, we\npropose STAD, a probabilistic state-space model that adapts a deployed model to\ntemporal distribution shifts by learning the time-varying dynamics in the last\nset of hidden features. Without requiring labels, our model infers\ntime-evolving class prototypes that act as a dynamic classification head.\nThrough experiments on real-world temporal distribution shifts, we show that\nour method excels in handling small batch sizes and label shift.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12492v2",
    "published_date": "2024-07-17 11:18:49 UTC",
    "updated_date": "2024-10-02 17:29:54 UTC"
  },
  {
    "arxiv_id": "2407.12893v1",
    "title": "Hybrid Dynamic Pruning: A Pathway to Efficient Transformer Inference",
    "authors": [
      "Ghadeer Jaradat",
      "Mohammed Tolba",
      "Ghada Alsuhli",
      "Hani Saleh",
      "Mahmoud Al-Qutayri",
      "Thanos Stouraitis",
      "Baker Mohammad"
    ],
    "abstract": "In the world of deep learning, Transformer models have become very\nsignificant, leading to improvements in many areas from understanding language\nto recognizing images, covering a wide range of applications. Despite their\nsuccess, the deployment of these models in real-time applications, particularly\non edge devices, poses significant challenges due to their quadratic\ncomputational intensity and memory demands. To overcome these challenges we\nintroduce a novel Hybrid Dynamic Pruning (HDP), an efficient\nalgorithm-architecture co-design approach that accelerates transformers using\nhead sparsity, block sparsity and approximation opportunities to reduce\ncomputations in attention and reduce memory access. With the observation of the\nhuge redundancy in attention scores and attention heads, we propose a novel\ninteger-based row-balanced block pruning to prune unimportant blocks in the\nattention matrix at run time, also propose integer-based head pruning to detect\nand prune unimportant heads at an early stage at run time. Also we propose an\napproximation method that reduces attention computations. To efficiently\nsupport these methods with lower latency and power efficiency, we propose a HDP\nco-processor architecture.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12893v1",
    "published_date": "2024-07-17 11:15:16 UTC",
    "updated_date": "2024-07-17 11:15:16 UTC"
  },
  {
    "arxiv_id": "2407.12471v2",
    "title": "Characterization of Political Polarized Users Attacked by Language Toxicity on Twitter",
    "authors": [
      "Wentao Xu"
    ],
    "abstract": "Understanding the dynamics of language toxicity on social media is important\nfor us to investigate the propagation of misinformation and the development of\necho chambers for political scenarios such as U.S. presidential elections.\nRecent research has used large-scale data to investigate the dynamics across\nsocial media platforms. However, research on the toxicity dynamics is not\nenough. This study aims to provide a first exploration of the potential\nlanguage toxicity flow among Left, Right and Center users. Specifically, we aim\nto examine whether Left users were easier to be attacked by language toxicity.\nIn this study, more than 500M Twitter posts were examined. It was discovered\nthat Left users received much more toxic replies than Right and Center users.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "91, 94",
      "J.4"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12471v2",
    "published_date": "2024-07-17 10:49:47 UTC",
    "updated_date": "2024-11-14 05:49:31 UTC"
  },
  {
    "arxiv_id": "2407.12468v3",
    "title": "Evaluating Search Engines and Large Language Models for Answering Health Questions",
    "authors": [
      "Marcos Fernández-Pichel",
      "Juan C. Pichel",
      "David E. Losada"
    ],
    "abstract": "Search engines (SEs) have traditionally been primary tools for information\nseeking, but the new Large Language Models (LLMs) are emerging as powerful\nalternatives, particularly for question-answering tasks. This study compares\nthe performance of four popular SEs, seven LLMs, and retrieval-augmented (RAG)\nvariants in answering 150 health-related questions from the TREC Health\nMisinformation (HM) Track. Results reveal SEs correctly answer between 50 and\n70% of questions, often hindered by many retrieval results not responding to\nthe health question. LLMs deliver higher accuracy, correctly answering about\n80% of questions, though their performance is sensitive to input prompts. RAG\nmethods significantly enhance smaller LLMs' effectiveness, improving accuracy\nby up to 30% by integrating retrieval evidence.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12468v3",
    "published_date": "2024-07-17 10:40:39 UTC",
    "updated_date": "2025-03-06 11:53:49 UTC"
  },
  {
    "arxiv_id": "2407.17518v1",
    "title": "Driving pattern interpretation based on action phases clustering",
    "authors": [
      "Xue Yao",
      "Simeon C. Calvert",
      "Serge P. Hoogendoorn"
    ],
    "abstract": "Current approaches to identifying driving heterogeneity face challenges in\ncomprehending fundamental patterns from the perspective of underlying driving\nbehavior mechanisms. The concept of Action phases was proposed in our previous\nwork, capturing the diversity of driving characteristics with physical\nmeanings. This study presents a novel framework to further interpret driving\npatterns by classifying Action phases in an unsupervised manner. In this\nframework, a Resampling and Downsampling Method (RDM) is first applied to\nstandardize the length of Action phases. Then the clustering calibration\nprocedure including ''Feature Selection'', ''Clustering Analysis'',\n''Difference/Similarity Evaluation'', and ''Action phases Re-extraction'' is\niteratively applied until all differences among clusters and similarities\nwithin clusters reach the pre-determined criteria. Application of the framework\nusing real-world datasets revealed six driving patterns in the I80 dataset,\nlabeled as ''Catch up'', ''Keep away'', and ''Maintain distance'', with both\n''Stable'' and ''Unstable'' states. Notably, Unstable patterns are more\nnumerous than Stable ones. ''Maintain distance'' is the most common among\nStable patterns. These observations align with the dynamic nature of driving.\nTwo patterns ''Stable keep away'' and ''Unstable catch up'' are missing in the\nUS101 dataset, which is in line with our expectations as this dataset was\npreviously shown to have less heterogeneity. This demonstrates the potential of\ndriving patterns in describing driving heterogeneity. The proposed framework\npromises advantages in addressing label scarcity in supervised learning and\nenhancing tasks such as driving behavior modeling and driving trajectory\nprediction.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17518v1",
    "published_date": "2024-07-17 10:40:23 UTC",
    "updated_date": "2024-07-17 10:40:23 UTC"
  },
  {
    "arxiv_id": "2407.12449v1",
    "title": "Close the Sim2real Gap via Physically-based Structured Light Synthetic Data Simulation",
    "authors": [
      "Kaixin Bai",
      "Lei Zhang",
      "Zhaopeng Chen",
      "Fang Wan",
      "Jianwei Zhang"
    ],
    "abstract": "Despite the substantial progress in deep learning, its adoption in industrial\nrobotics projects remains limited, primarily due to challenges in data\nacquisition and labeling. Previous sim2real approaches using domain\nrandomization require extensive scene and model optimization. To address these\nissues, we introduce an innovative physically-based structured light simulation\nsystem, generating both RGB and physically realistic depth images, surpassing\nprevious dataset generation tools. We create an RGBD dataset tailored for\nrobotic industrial grasping scenarios and evaluate it across various tasks,\nincluding object detection, instance segmentation, and embedding sim2real\nvisual perception in industrial robotic grasping. By reducing the sim2real gap\nand enhancing deep learning training, we facilitate the application of deep\nlearning models in industrial settings. Project details are available at\nhttps://baikaixinpublic.github.io/structured light 3D synthesizer/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 2024 IEEE International Conference on Robotics and\n  Automation",
    "pdf_url": "http://arxiv.org/pdf/2407.12449v1",
    "published_date": "2024-07-17 09:57:14 UTC",
    "updated_date": "2024-07-17 09:57:14 UTC"
  },
  {
    "arxiv_id": "2407.12437v1",
    "title": "Variable-Agnostic Causal Exploration for Reinforcement Learning",
    "authors": [
      "Minh Hoang Nguyen",
      "Hung Le",
      "Svetha Venkatesh"
    ],
    "abstract": "Modern reinforcement learning (RL) struggles to capture real-world\ncause-and-effect dynamics, leading to inefficient exploration due to extensive\ntrial-and-error actions. While recent efforts to improve agent exploration have\nleveraged causal discovery, they often make unrealistic assumptions of causal\nvariables in the environments. In this paper, we introduce a novel framework,\nVariable-Agnostic Causal Exploration for Reinforcement Learning (VACERL),\nincorporating causal relationships to drive exploration in RL without\nspecifying environmental causal variables. Our approach automatically\nidentifies crucial observation-action steps associated with key variables using\nattention mechanisms. Subsequently, it constructs the causal graph connecting\nthese steps, which guides the agent towards observation-action pairs with\ngreater causal influence on task completion. This can be leveraged to generate\nintrinsic rewards or establish a hierarchy of subgoals to enhance exploration\nefficiency. Experimental results showcase a significant improvement in agent\nperformance in grid-world, 2d games and robotic domains, particularly in\nscenarios with sparse rewards and noisy actions, such as the notorious Noisy-TV\nenvironments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12437v1",
    "published_date": "2024-07-17 09:45:27 UTC",
    "updated_date": "2024-07-17 09:45:27 UTC"
  },
  {
    "arxiv_id": "2407.12426v1",
    "title": "Sharif-STR at SemEval-2024 Task 1: Transformer as a Regression Model for Fine-Grained Scoring of Textual Semantic Relations",
    "authors": [
      "Seyedeh Fatemeh Ebrahimi",
      "Karim Akhavan Azari",
      "Amirmasoud Iravani",
      "Hadi Alizadeh",
      "Zeinab Sadat Taghavi",
      "Hossein Sameti"
    ],
    "abstract": "Semantic Textual Relatedness holds significant relevance in Natural Language\nProcessing, finding applications across various domains. Traditionally,\napproaches to STR have relied on knowledge-based and statistical methods.\nHowever, with the emergence of Large Language Models, there has been a paradigm\nshift, ushering in new methodologies. In this paper, we delve into the\ninvestigation of sentence-level STR within Track A (Supervised) by leveraging\nfine-tuning techniques on the RoBERTa transformer. Our study focuses on\nassessing the efficacy of this approach across different languages. Notably,\nour findings indicate promising advancements in STR performance, particularly\nin Latin languages. Specifically, our results demonstrate notable improvements\nin English, achieving a correlation of 0.82 and securing a commendable 19th\nrank. Similarly, in Spanish, we achieved a correlation of 0.67, securing the\n15th position. However, our approach encounters challenges in languages like\nArabic, where we observed a correlation of only 0.38, resulting in a 20th rank.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 9 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.12426v1",
    "published_date": "2024-07-17 09:25:18 UTC",
    "updated_date": "2024-07-17 09:25:18 UTC"
  },
  {
    "arxiv_id": "2407.12423v3",
    "title": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions",
    "authors": [
      "Zixin Chen",
      "Jiachen Wang",
      "Meng Xia",
      "Kento Shigyo",
      "Dingdong Liu",
      "Rong Zhang",
      "Huamin Qu"
    ],
    "abstract": "The integration of Large Language Models (LLMs), especially ChatGPT, into\neducation is poised to revolutionize students' learning experiences by\nintroducing innovative conversational learning methodologies. To empower\nstudents to fully leverage the capabilities of ChatGPT in educational\nscenarios, understanding students' interaction patterns with ChatGPT is crucial\nfor instructors. However, this endeavor is challenging due to the absence of\ndatasets focused on student-ChatGPT conversations and the complexities in\nidentifying and analyzing the evolutional interaction patterns within\nconversations. To address these challenges, we collected conversational data\nfrom 48 students interacting with ChatGPT in a master's level data\nvisualization course over one semester. We then developed a coding scheme,\ngrounded in the literature on cognitive levels and thematic analysis, to\ncategorize students' interaction patterns with ChatGPT. Furthermore, we present\na visual analytics system, StuGPTViz, that tracks and compares temporal\npatterns in student prompts and the quality of ChatGPT's responses at multiple\nscales, revealing significant pedagogical insights for instructors. We\nvalidated the system's effectiveness through expert interviews with six data\nvisualization instructors and three case studies. The results confirmed\nStuGPTViz's capacity to enhance educators' insights into the pedagogical value\nof ChatGPT. We also discussed the potential research opportunities of applying\nvisual analytics in education and developing AI-driven personalized learning\nsolutions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages. To be published at IEEE Visualization 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12423v3",
    "published_date": "2024-07-17 09:20:44 UTC",
    "updated_date": "2024-09-17 08:32:02 UTC"
  },
  {
    "arxiv_id": "2407.12421v1",
    "title": "SafePowerGraph: Safety-aware Evaluation of Graph Neural Networks for Transmission Power Grids",
    "authors": [
      "Salah Ghamizi",
      "Aleksandar Bojchevski",
      "Aoxiang Ma",
      "Jun Cao"
    ],
    "abstract": "Power grids are critical infrastructures of paramount importance to modern\nsociety and their rapid evolution and interconnections has heightened the\ncomplexity of power systems (PS) operations. Traditional methods for grid\nanalysis struggle with the computational demands of large-scale RES and ES\nintegration, prompting the adoption of machine learning (ML) techniques,\nparticularly Graph Neural Networks (GNNs). GNNs have proven effective in\nsolving the alternating current (AC) Power Flow (PF) and Optimal Power Flow\n(OPF) problems, crucial for operational planning. However, existing benchmarks\nand datasets completely ignore safety and robustness requirements in their\nevaluation and never consider realistic safety-critical scenarios that most\nimpact the operations of the power grids. We present SafePowerGraph, the first\nsimulator-agnostic, safety-oriented framework and benchmark for GNNs in PS\noperations. SafePowerGraph integrates multiple PF and OPF simulators and\nassesses GNN performance under diverse scenarios, including energy price\nvariations and power line outages. Our extensive experiments underscore the\nimportance of self-supervised learning and graph attention architectures for\nGNN robustness. We provide at https://github.com/yamizi/SafePowerGraph our\nopen-source repository, a comprehensive leaderboard, a dataset and model zoo\nand expect our framework to standardize and advance research in the critical\nfield of GNN for power systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12421v1",
    "published_date": "2024-07-17 09:01:38 UTC",
    "updated_date": "2024-07-17 09:01:38 UTC"
  },
  {
    "arxiv_id": "2407.12417v1",
    "title": "Improving the classification of extreme classes by means of loss regularisation and generalised beta distributions",
    "authors": [
      "Víctor Manuel Vargas",
      "Pedro Antonio Gutiérrez",
      "Javier Barbero-Gómez",
      "César Hervás-Martínez"
    ],
    "abstract": "An ordinal classification problem is one in which the target variable takes\nvalues on an ordinal scale. Nowadays, there are many of these problems\nassociated with real-world tasks where it is crucial to accurately classify the\nextreme classes of the ordinal structure. In this work, we propose a unimodal\nregularisation approach that can be applied to any loss function to improve the\nclassification performance of the first and last classes while maintaining good\nperformance for the remainder. The proposed methodology is tested on six\ndatasets with different numbers of classes, and compared with other unimodal\nregularisation methods in the literature. In addition, performance in the\nextreme classes is compared using a new metric that takes into account their\nsensitivities. Experimental results and statistical analysis show that the\nproposed methodology obtains a superior average performance considering\ndifferent metrics. The results for the proposed metric show that the\ngeneralised beta distribution generally improves classification performance in\nthe extreme classes. At the same time, the other five nominal and ordinal\nmetrics considered show that the overall performance is aligned with the\nperformance of previous alternatives.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12417v1",
    "published_date": "2024-07-17 08:57:42 UTC",
    "updated_date": "2024-07-17 08:57:42 UTC"
  },
  {
    "arxiv_id": "2407.12410v1",
    "title": "Proximity-based Self-Federated Learning",
    "authors": [
      "Davide Domini",
      "Gianluca Aguzzi",
      "Nicolas Farabegoli",
      "Mirko Viroli",
      "Lukas Esterle"
    ],
    "abstract": "In recent advancements in machine learning, federated learning allows a\nnetwork of distributed clients to collaboratively develop a global model\nwithout needing to share their local data. This technique aims to safeguard\nprivacy, countering the vulnerabilities of conventional centralized learning\nmethods. Traditional federated learning approaches often rely on a central\nserver to coordinate model training across clients, aiming to replicate the\nsame model uniformly across all nodes. However, these methods overlook the\nsignificance of geographical and local data variances in vast networks,\npotentially affecting model effectiveness and applicability. Moreover, relying\non a central server might become a bottleneck in large networks, such as the\nones promoted by edge computing. Our paper introduces a novel,\nfully-distributed federated learning strategy called proximity-based\nself-federated learning that enables the self-organised creation of multiple\nfederations of clients based on their geographic proximity and data\ndistribution without exchanging raw data. Indeed, unlike traditional\nalgorithms, our approach encourages clients to share and adjust their models\nwith neighbouring nodes based on geographic proximity and model accuracy. This\nmethod not only addresses the limitations posed by diverse data distributions\nbut also enhances the model's adaptability to different regional\ncharacteristics creating specialized models for each federation. We demonstrate\nthe efficacy of our approach through simulations on well-known datasets,\nshowcasing its effectiveness over the conventional centralized federated\nlearning framework.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12410v1",
    "published_date": "2024-07-17 08:44:45 UTC",
    "updated_date": "2024-07-17 08:44:45 UTC"
  },
  {
    "arxiv_id": "2407.12397v1",
    "title": "Mamba-PTQ: Outlier Channels in Recurrent Large Language Models",
    "authors": [
      "Alessandro Pierro",
      "Steven Abreu"
    ],
    "abstract": "Modern recurrent layers are emerging as a promising path toward edge\ndeployment of foundation models, especially in the context of large language\nmodels (LLMs). Compressing the whole input sequence in a finite-dimensional\nrepresentation enables recurrent layers to model long-range dependencies while\nmaintaining a constant inference cost for each token and a fixed memory\nrequirement. However, the practical deployment of LLMs in resource-limited\nenvironments often requires further model compression, such as quantization and\npruning. While these techniques are well-established for attention-based\nmodels, their effects on recurrent layers remain underexplored.\n  In this preliminary work, we focus on post-training quantization for\nrecurrent LLMs and show that Mamba models exhibit the same pattern of outlier\nchannels observed in attention-based LLMs. We show that the reason for the\ndifficulty of quantizing SSMs is caused by activation outliers, similar to\nthose observed in transformer-based LLMs. We report baseline results for\npost-training quantization of Mamba that do not take into account the\nactivation outliers and suggest first steps for outlier-aware quantization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Work presented at the Efficient Systems for Foundation Models\n  Workshop @ ICML2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12397v1",
    "published_date": "2024-07-17 08:21:06 UTC",
    "updated_date": "2024-07-17 08:21:06 UTC"
  },
  {
    "arxiv_id": "2407.12393v5",
    "title": "PersLLM: A Personified Training Approach for Large Language Models",
    "authors": [
      "Zheni Zeng",
      "Jiayi Chen",
      "Huimin Chen",
      "Yukun Yan",
      "Yuxuan Chen",
      "Zhenghao Liu",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Large language models (LLMs) exhibit human-like intelligence, enabling them\nto simulate human behavior and support various applications that require both\nhumanized communication and extensive knowledge reserves. Efforts are made to\npersonify LLMs with special training data or hand-crafted prompts, while\ncorrespondingly faced with challenges such as insufficient data usage or rigid\nbehavior patterns. Consequently, personified LLMs fail to capture personified\nknowledge or express persistent opinion. To fully unlock the potential of LLM\npersonification, we propose PersLLM, a framework for better data construction\nand model tuning. For insufficient data usage, we incorporate strategies such\nas Chain-of-Thought prompting and anti-induction, improving the quality of data\nconstruction and capturing the personality experiences, knowledge, and thoughts\nmore comprehensively. For rigid behavior patterns, we design the tuning process\nand introduce automated DPO to enhance the specificity and dynamism of the\nmodels' personalities, which leads to a more natural opinion communication.\nBoth automated metrics and expert human evaluations demonstrate the\neffectiveness of our approach. Case studies in human-machine interactions and\nmulti-agent systems further suggest potential application scenarios and future\ndirections for LLM personification.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages for main text, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.12393v5",
    "published_date": "2024-07-17 08:13:22 UTC",
    "updated_date": "2025-05-15 08:22:08 UTC"
  },
  {
    "arxiv_id": "2407.12391v1",
    "title": "LLM Inference Serving: Survey of Recent Advances and Opportunities",
    "authors": [
      "Baolin Li",
      "Yankai Jiang",
      "Vijay Gadepally",
      "Devesh Tiwari"
    ],
    "abstract": "This survey offers a comprehensive overview of recent advancements in Large\nLanguage Model (LLM) serving systems, focusing on research since the year 2023.\nWe specifically examine system-level enhancements that improve performance and\nefficiency without altering the core LLM decoding mechanisms. By selecting and\nreviewing high-quality papers from prestigious ML and system venues, we\nhighlight key innovations and practical considerations for deploying and\nscaling LLMs in real-world production environments. This survey serves as a\nvaluable resource for LLM practitioners seeking to stay abreast of the latest\ndevelopments in this rapidly evolving field.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12391v1",
    "published_date": "2024-07-17 08:11:47 UTC",
    "updated_date": "2024-07-17 08:11:47 UTC"
  },
  {
    "arxiv_id": "2407.12378v2",
    "title": "StoX-Net: Stochastic Processing of Partial Sums for Efficient In-Memory Computing DNN Accelerators",
    "authors": [
      "Ethan G Rogers",
      "Sohan Salahuddin Mugdho",
      "Kshemal Kshemendra Gupte",
      "Cheng Wang"
    ],
    "abstract": "Crossbar-based in-memory computing (IMC) has emerged as a promising platform\nfor hardware acceleration of deep neural networks (DNNs). However, the energy\nand latency of IMC systems are dominated by the large overhead of the\nperipheral analog-to-digital converters (ADCs). To address such ADC bottleneck,\nhere we propose to implement stochastic processing of array-level partial sums\n(PS) for efficient IMC. Leveraging the probabilistic switching of spin-orbit\ntorque magnetic tunnel junctions, the proposed PS processing eliminates the\ncostly ADC, achieving significant improvement in energy and area efficiency. To\nmitigate accuracy loss, we develop PS-quantization-aware training that enables\nbackward propagation across stochastic PS. Furthermore, a novel scheme with an\ninhomogeneous sampling length of the stochastic conversion is proposed. When\nrunning ResNet20 on the CIFAR-10 dataset, our architecture-to-algorithm\nco-design demonstrates up to 16x, 8x, and 10x improvement in energy, latency,\nand area, respectively, compared to IMC with standard ADC. Our optimized design\nconfiguration using stochastic PS achieved 130x (24x) improvement in\nEnergy-Delay-Product compared to IMC with full precision ADC (sparse low-bit\nADC), while maintaining near-software accuracy at various benchmark\nclassification tasks.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12378v2",
    "published_date": "2024-07-17 07:56:43 UTC",
    "updated_date": "2024-11-08 17:56:34 UTC"
  },
  {
    "arxiv_id": "2407.12374v2",
    "title": "Graph Signal Processing for Cross-Domain Recommendation",
    "authors": [
      "Jeongeun Lee",
      "Seongku Kang",
      "Won-Yong Shin",
      "Jeongwhan Choi",
      "Noseong Park",
      "Dongha Lee"
    ],
    "abstract": "Cross-domain recommendation (CDR) extends conventional recommender systems by\nleveraging user-item interactions from dense domains to mitigate data sparsity\nand the cold start problem. While CDR offers substantial potential for\nenhancing recommendation performance, most existing CDR methods suffer from\nsensitivity to the ratio of overlapping users and intrinsic discrepancy between\nsource and target domains. To overcome these limitations, in this work, we\nexplore the application of graph signal processing (GSP) in CDR scenarios. We\npropose CGSP, a unified CDR framework based on GSP, which employs a\ncross-domain similarity graph constructed by flexibly combining target-only\nsimilarity and source-bridged similarity. By processing personalized graph\nsignals computed for users from either the source or target domain, our\nframework effectively supports both inter-domain and intra-domain\nrecommendations. Our empirical evaluation demonstrates that CGSP consistently\noutperforms various encoder-based CDR approaches in both intra-domain and\ninter-domain recommendation scenarios, especially when the ratio of overlapping\nusers is low, highlighting its significant practical implication in real-world\napplications.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12374v2",
    "published_date": "2024-07-17 07:52:45 UTC",
    "updated_date": "2024-07-22 04:07:03 UTC"
  },
  {
    "arxiv_id": "2407.12371v2",
    "title": "HIMO: A New Benchmark for Full-Body Human Interacting with Multiple Objects",
    "authors": [
      "Xintao Lv",
      "Liang Xu",
      "Yichao Yan",
      "Xin Jin",
      "Congsheng Xu",
      "Shuwen Wu",
      "Yifan Liu",
      "Lincheng Li",
      "Mengxiao Bi",
      "Wenjun Zeng",
      "Xiaokang Yang"
    ],
    "abstract": "Generating human-object interactions (HOIs) is critical with the tremendous\nadvances of digital avatars. Existing datasets are typically limited to humans\ninteracting with a single object while neglecting the ubiquitous manipulation\nof multiple objects. Thus, we propose HIMO, a large-scale MoCap dataset of\nfull-body human interacting with multiple objects, containing 3.3K 4D HOI\nsequences and 4.08M 3D HOI frames. We also annotate HIMO with detailed textual\ndescriptions and temporal segments, benchmarking two novel tasks of HOI\nsynthesis conditioned on either the whole text prompt or the segmented text\nprompts as fine-grained timeline control. To address these novel tasks, we\npropose a dual-branch conditional diffusion model with a mutual interaction\nmodule for HOI synthesis. Besides, an auto-regressive generation pipeline is\nalso designed to obtain smooth transitions between HOI segments. Experimental\nresults demonstrate the generalization ability to unseen object geometries and\ntemporal compositions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://lvxintao.github.io/himo, accepted by ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12371v2",
    "published_date": "2024-07-17 07:47:34 UTC",
    "updated_date": "2024-09-11 09:53:18 UTC"
  },
  {
    "arxiv_id": "2407.12366v2",
    "title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models",
    "authors": [
      "Gengze Zhou",
      "Yicong Hong",
      "Zun Wang",
      "Xin Eric Wang",
      "Qi Wu"
    ],
    "abstract": "Capitalizing on the remarkable advancements in Large Language Models (LLMs),\nthere is a burgeoning initiative to harness LLMs for instruction following\nrobotic navigation. Such a trend underscores the potential of LLMs to\ngeneralize navigational reasoning and diverse language understanding. However,\na significant discrepancy in agent performance is observed when integrating\nLLMs in the Vision-and-Language navigation (VLN) tasks compared to previous\ndownstream specialist models. Furthermore, the inherent capacity of language to\ninterpret and facilitate communication in agent interactions is often\nunderutilized in these integrations. In this work, we strive to bridge the\ndivide between VLN-specialized models and LLM-based navigation paradigms, while\nmaintaining the interpretative prowess of LLMs in generating linguistic\nnavigational reasoning. By aligning visual content in a frozen LLM, we\nencompass visual observation comprehension for LLMs and exploit a way to\nincorporate LLMs and navigation policy networks for effective action\npredictions and navigational reasoning. We demonstrate the data efficiency of\nthe proposed methods and eliminate the gap between LM-based agents and\nstate-of-the-art VLN specialists.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12366v2",
    "published_date": "2024-07-17 07:44:26 UTC",
    "updated_date": "2024-09-20 01:32:35 UTC"
  },
  {
    "arxiv_id": "2407.12888v1",
    "title": "Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models",
    "authors": [
      "Alexander R. Pelletier",
      "Joseph Ramirez",
      "Irsyad Adam",
      "Simha Sankar",
      "Yu Yan",
      "Ding Wang",
      "Dylan Steinecke",
      "Wei Wang",
      "Peipei Ping"
    ],
    "abstract": "The vast amount of biomedical information available today presents a\nsignificant challenge for investigators seeking to digest, process, and\nunderstand these findings effectively. Large Language Models (LLMs) have\nemerged as powerful tools to navigate this complex and challenging data\nlandscape. However, LLMs may lead to hallucinatory responses, making Retrieval\nAugmented Generation (RAG) crucial for achieving accurate information. In this\nprotocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease\nDistinction), a comprehensive workflow designed to support investigators with\nknowledge integration and hypothesis generation, identifying validated paths\nforward. Relevant biomedical information from publications and knowledge bases\nare reviewed, integrated, and extracted via text-mining association analysis\nand explainable graph prediction models on disease nodes, forecasting potential\nlinks among drugs and diseases. These analyses, along with biomedical texts,\nare integrated into a framework that facilitates user-directed mechanism\nelucidation as well as hypothesis exploration through RAG-enabled LLMs. A\nclinical use-case demonstrates RUGGED's ability to evaluate and recommend\ntherapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy\n(DCM), analyzing prescribed drugs for molecular interactions and unexplored\nuses. The platform minimizes LLM hallucinations, offers actionable insights,\nand improves the investigation of novel therapeutics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12888v1",
    "published_date": "2024-07-17 07:44:18 UTC",
    "updated_date": "2024-07-17 07:44:18 UTC"
  },
  {
    "arxiv_id": "2407.12357v1",
    "title": "Evaluating graph-based explanations for AI-based recommender systems",
    "authors": [
      "Simon Delarue",
      "Astrid Bertrand",
      "Tiphaine Viard"
    ],
    "abstract": "Recent years have witnessed a rapid growth of recommender systems, providing\nsuggestions in numerous applications with potentially high social impact, such\nas health or justice. Meanwhile, in Europe, the upcoming AI Act mentions\n\\emph{transparency} as a requirement for critical AI systems in order to\n``mitigate the risks to fundamental rights''. Post-hoc explanations seamlessly\nalign with this goal and extensive literature on the subject produced several\nforms of such objects, graphs being one of them. Early studies in visualization\ndemonstrated the graphs' ability to improve user understanding, positioning\nthem as potentially ideal explanations. However, it remains unclear how\ngraph-based explanations compare to other explanation designs. In this work, we\naim to determine the effectiveness of graph-based explanations in improving\nusers' perception of AI-based recommendations using a mixed-methods approach.\nWe first conduct a qualitative study to collect users' requirements for graph\nexplanations. We then run a larger quantitative study in which we evaluate the\ninfluence of various explanation designs, including enhanced graph-based ones,\non aspects such as understanding, usability and curiosity toward the AI system.\nWe find that users perceive graph-based explanations as more usable than\ndesigns involving feature importance. However, we also reveal that textual\nexplanations lead to higher objective understanding than graph-based designs.\nMost importantly, we highlight the strong contrast between participants'\nexpressed preferences for graph design and their actual ratings using it, which\nare lower compared to textual design. These findings imply that meeting\nstakeholders' expressed preferences might not alone guarantee ``good''\nexplanations. Therefore, crafting hybrid designs successfully balancing social\nexpectations with downstream performance emerges as a significant challenge.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12357v1",
    "published_date": "2024-07-17 07:28:49 UTC",
    "updated_date": "2024-07-17 07:28:49 UTC"
  },
  {
    "arxiv_id": "2407.12352v1",
    "title": "SENTAUR: Security EnhaNced Trojan Assessment Using LLMs Against Undesirable Revisions",
    "authors": [
      "Jitendra Bhandari",
      "Rajat Sadhukhan",
      "Prashanth Krishnamurthy",
      "Farshad Khorrami",
      "Ramesh Karri"
    ],
    "abstract": "A globally distributed IC supply chain brings risks due to untrusted third\nparties. The risks span inadvertent use of hardware Trojan (HT), inserted\nIntellectual Property (3P-IP) or Electronic Design Automation (EDA) flows. HT\ncan introduce stealthy HT behavior, prevent an IC work as intended, or leak\nsensitive data via side channels. To counter HTs, rapidly examining HT\nscenarios is a key requirement. While Trust-Hub benchmarks are a good starting\npoint to assess defenses, they encompass a small subset of manually created HTs\nwithin the expanse of HT designs. Further, the HTs may disappear during\nsynthesis. We propose a large language model (LLM) framework SENTAUR to\ngenerate a suite of legitimate HTs for a Register Transfer Level (RTL) design\nby learning its specifications, descriptions, and natural language descriptions\nof HT effects. Existing tools and benchmarks are limited; they need a learning\nperiod to construct an ML model to mimic the threat model and are difficult to\nreproduce. SENTAUR can swiftly produce HT instances by leveraging LLMs without\nany learning period and sanitizing the HTs facilitating their rapid assessment.\nEvaluation of SENTAUR involved generating effective, synthesizable, and\npractical HTs from TrustHub and elsewhere, investigating impacts of\npayloads/triggers at the RTL. While our evaluation focused on HT insertion,\nSENTAUR can generalize to automatically transform an RTL code to have defined\nfunctional modifications.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12352v1",
    "published_date": "2024-07-17 07:13:06 UTC",
    "updated_date": "2024-07-17 07:13:06 UTC"
  },
  {
    "arxiv_id": "2408.00780v1",
    "title": "In-Depth Analysis of Emotion Recognition through Knowledge-Based Large Language Models",
    "authors": [
      "Bin Han",
      "Cleo Yau",
      "Su Lei",
      "Jonathan Gratch"
    ],
    "abstract": "Emotion recognition in social situations is a complex task that requires\nintegrating information from both facial expressions and the situational\ncontext. While traditional approaches to automatic emotion recognition have\nfocused on decontextualized signals, recent research emphasizes the importance\nof context in shaping emotion perceptions. This paper contributes to the\nemerging field of context-based emotion recognition by leveraging psychological\ntheories of human emotion perception to inform the design of automated methods.\nWe propose an approach that combines emotion recognition methods with Bayesian\nCue Integration (BCI) to integrate emotion inferences from decontextualized\nfacial expressions and contextual knowledge inferred via Large-language Models.\nWe test this approach in the context of interpreting facial expressions during\na social task, the prisoner's dilemma. Our results provide clear support for\nBCI across a range of automatic emotion recognition methods. The best automated\nmethod achieved results comparable to human observers, suggesting the potential\nfor this approach to advance the field of affective computing.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "68"
    ],
    "primary_category": "cs.HC",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.00780v1",
    "published_date": "2024-07-17 06:39:51 UTC",
    "updated_date": "2024-07-17 06:39:51 UTC"
  },
  {
    "arxiv_id": "2407.14247v1",
    "title": "Continual Learning for Adaptable Car-Following in Dynamic Traffic Environments",
    "authors": [
      "Xianda Chen",
      "PakHin Tiu",
      "Xu Han",
      "Junjie Chen",
      "Yuanfei Wu",
      "Xinhu Zheng",
      "Meixin Zhu"
    ],
    "abstract": "The continual evolution of autonomous driving technology requires\ncar-following models that can adapt to diverse and dynamic traffic\nenvironments. Traditional learning-based models often suffer from performance\ndegradation when encountering unseen traffic patterns due to a lack of\ncontinual learning capabilities. This paper proposes a novel car-following\nmodel based on continual learning that addresses this limitation. Our framework\nincorporates Elastic Weight Consolidation (EWC) and Memory Aware Synapses (MAS)\ntechniques to mitigate catastrophic forgetting and enable the model to learn\nincrementally from new traffic data streams. We evaluate the performance of the\nproposed model on the Waymo and Lyft datasets which encompass various traffic\nscenarios. The results demonstrate that the continual learning techniques\nsignificantly outperform the baseline model, achieving 0\\% collision rates\nacross all traffic conditions. This research contributes to the advancement of\nautonomous driving technology by fostering the development of more robust and\nadaptable car-following models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.14247v1",
    "published_date": "2024-07-17 06:32:52 UTC",
    "updated_date": "2024-07-17 06:32:52 UTC"
  },
  {
    "arxiv_id": "2408.00779v1",
    "title": "Learning Structurally Stabilized Representations for Multi-modal Lossless DNA Storage",
    "authors": [
      "Ben Cao",
      "Tiantian He",
      "Xue Li",
      "Bin Wang",
      "Xiaohu Wu",
      "Qiang Zhang",
      "Yew-Soon Ong"
    ],
    "abstract": "In this paper, we present Reed-Solomon coded single-stranded representation\nlearning (RSRL), a novel end-to-end model for learning representations for\nmulti-modal lossless DNA storage. In contrast to existing learning-based\nmethods, the proposed RSRL is inspired by both error-correction codec and\nstructural biology. Specifically, RSRL first learns the representations for the\nsubsequent storage from the binary data transformed by the Reed-Solomon codec.\nThen, the representations are masked by an RS-code-informed mask to focus on\ncorrecting the burst errors occurring in the learning process. With the decoded\nrepresentations with error corrections, a novel biologically stabilized loss is\nformulated to regularize the data representations to possess stable\nsingle-stranded structures. By incorporating these novel strategies, the\nproposed RSRL can learn highly durable, dense, and lossless representations for\nthe subsequent storage tasks into DNA sequences. The proposed RSRL has been\ncompared with a number of strong baselines in real-world tasks of multi-modal\ndata storage. The experimental results obtained demonstrate that RSRL can store\ndiverse types of data with much higher information density and durability but\nmuch lower error rates.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET",
      "cs.IT",
      "math.IT",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.00779v1",
    "published_date": "2024-07-17 06:31:49 UTC",
    "updated_date": "2024-07-17 06:31:49 UTC"
  },
  {
    "arxiv_id": "2407.12338v1",
    "title": "GUME: Graphs and User Modalities Enhancement for Long-Tail Multimodal Recommendation",
    "authors": [
      "Guojiao Lin",
      "Zhen Meng",
      "Dongjie Wang",
      "Qingqing Long",
      "Yuanchun Zhou",
      "Meng Xiao"
    ],
    "abstract": "Multimodal recommendation systems (MMRS) have received considerable attention\nfrom the research community due to their ability to jointly utilize information\nfrom user behavior and product images and text. Previous research has two main\nissues. First, many long-tail items in recommendation systems have limited\ninteraction data, making it difficult to learn comprehensive and informative\nrepresentations. However, past MMRS studies have overlooked this issue.\nSecondly, users' modality preferences are crucial to their behavior. However,\nprevious research has primarily focused on learning item modality\nrepresentations, while user modality representations have remained relatively\nsimplistic.To address these challenges, we propose a novel Graphs and User\nModalities Enhancement (GUME) for long-tail multimodal recommendation.\nSpecifically, we first enhance the user-item graph using multimodal similarity\nbetween items. This improves the connectivity of long-tail items and helps them\nlearn high-quality representations through graph propagation. Then, we\nconstruct two types of user modalities: explicit interaction features and\nextended interest features. By using the user modality enhancement strategy to\nmaximize mutual information between these two features, we improve the\ngeneralization ability of user modality representations. Additionally, we\ndesign an alignment strategy for modality data to remove noise from both\ninternal and external perspectives. Extensive experiments on four publicly\navailable datasets demonstrate the effectiveness of our approach.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages, accepted by CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12338v1",
    "published_date": "2024-07-17 06:29:00 UTC",
    "updated_date": "2024-07-17 06:29:00 UTC"
  },
  {
    "arxiv_id": "2407.12331v2",
    "title": "I2AM: Interpreting Image-to-Image Latent Diffusion Models via Bi-Attribution Maps",
    "authors": [
      "Junseo Park",
      "Hyeryung Jang"
    ],
    "abstract": "Large-scale diffusion models have made significant advances in image\ngeneration, particularly through cross-attention mechanisms. While\ncross-attention has been well-studied in text-to-image tasks, their\ninterpretability in image-to-image (I2I) diffusion models remains\nunderexplored. This paper introduces Image-to-Image Attribution Maps (I2AM), a\nmethod that enhances the interpretability of I2I models by visualizing\nbidirectional attribution maps, from the reference image to the generated image\nand vice versa. I2AM aggregates cross-attention scores across time steps,\nattention heads, and layers, offering insights into how critical features are\ntransferred between images. We demonstrate the effectiveness of I2AM across\nobject detection, inpainting, and super-resolution tasks. Our results\ndemonstrate that I2AM successfully identifies key regions responsible for\ngenerating the output, even in complex scenes. Additionally, we introduce the\nInpainting Mask Attention Consistency Score (IMACS) as a novel evaluation\nmetric to assess the alignment between attribution maps and inpainting masks,\nwhich correlates strongly with existing performance metrics. Through extensive\nexperiments, we show that I2AM enables model debugging and refinement,\nproviding practical tools for improving I2I model's performance and\ninterpretability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.12331v2",
    "published_date": "2024-07-17 06:15:05 UTC",
    "updated_date": "2025-03-20 08:27:10 UTC"
  },
  {
    "arxiv_id": "2407.12330v1",
    "title": "Uncertainty Calibration with Energy Based Instance-wise Scaling in the Wild Dataset",
    "authors": [
      "Mijoo Kim",
      "Junseok Kwon"
    ],
    "abstract": "With the rapid advancement in the performance of deep neural networks (DNNs),\nthere has been significant interest in deploying and incorporating artificial\nintelligence (AI) systems into real-world scenarios. However, many DNNs lack\nthe ability to represent uncertainty, often exhibiting excessive confidence\neven when making incorrect predictions. To ensure the reliability of AI\nsystems, particularly in safety-critical cases, DNNs should transparently\nreflect the uncertainty in their predictions. In this paper, we investigate\nrobust post-hoc uncertainty calibration methods for DNNs within the context of\nmulti-class classification tasks. While previous studies have made notable\nprogress, they still face challenges in achieving robust calibration,\nparticularly in scenarios involving out-of-distribution (OOD). We identify that\nprevious methods lack adaptability to individual input data and struggle to\naccurately estimate uncertainty when processing inputs drawn from the wild\ndataset. To address this issue, we introduce a novel instance-wise calibration\nmethod based on an energy model. Our method incorporates energy scores instead\nof softmax confidence scores, allowing for adaptive consideration of DNN\nuncertainty for each prediction within a logit space. In experiments, we show\nthat the proposed method consistently maintains robust performance across the\nspectrum, spanning from in-distribution to OOD scenarios, when compared to\nother state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12330v1",
    "published_date": "2024-07-17 06:14:55 UTC",
    "updated_date": "2024-07-17 06:14:55 UTC"
  },
  {
    "arxiv_id": "2407.12327v5",
    "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
    "authors": [
      "Ayush Kaushal",
      "Tejas Vaidhya",
      "Arnab Kumar Mondal",
      "Tejas Pandey",
      "Aaryan Bhagat",
      "Irina Rish"
    ],
    "abstract": "Rapid advancements in GPU computational power has outpaced memory capacity\nand bandwidth growth, creating bottlenecks in Large Language Model (LLM)\ninference. Post-training quantization is the leading method for addressing\nmemory-related bottlenecks in LLM inference, but it suffers from significant\nperformance degradation below 4-bit precision. This paper addresses these\nchallenges by investigating the pretraining of low-bitwidth models specifically\nTernary Language Models (TriLMs) as an alternative to traditional\nfloating-point models (FloatLMs) and their post-training quantized versions\n(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning\nmultiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M\nto 3.9B parameters trained on 300B tokens. Our comprehensive evaluation\ndemonstrates that TriLMs offer superior scaling behavior in terms of model size\n(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs\nconsistently outperform their QuantLM and FloatLM counterparts for a given bit\nsize across various benchmarks. Notably, the 3.9B parameter TriLM matches the\nperformance of the FloatLM 3.9B across all benchmarks, despite having fewer\nbits than FloatLM 830M. Overall, this research provides valuable insights into\nthe feasibility and scalability of low-bitwidth language models, paving the way\nfor the development of more efficient LLMs.\n  To enhance understanding of low-bitwidth models, we are releasing 500+\nintermediate checkpoints of the Spectra suite at\nhttps://github.com/NolanoOrg/SpectraSuite.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "68T30",
      "I.2.6; I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "42 pages, 21 figures, and 13 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.12327v5",
    "published_date": "2024-07-17 05:53:20 UTC",
    "updated_date": "2024-10-11 04:44:55 UTC"
  },
  {
    "arxiv_id": "2407.21033v3",
    "title": "Multi-Grained Query-Guided Set Prediction Network for Grounded Multimodal Named Entity Recognition",
    "authors": [
      "Jielong Tang",
      "Zhenxing Wang",
      "Ziyang Gong",
      "Jianxing Yu",
      "Xiangwei Zhu",
      "Jian Yin"
    ],
    "abstract": "Grounded Multimodal Named Entity Recognition (GMNER) is an emerging\ninformation extraction (IE) task, aiming to simultaneously extract entity\nspans, types, and corresponding visual regions of entities from given\nsentence-image pairs data. Recent unified methods employing machine reading\ncomprehension or sequence generation-based frameworks show limitations in this\ndifficult task. The former, utilizing human-designed type queries, struggles to\ndifferentiate ambiguous entities, such as Jordan (Person) and off-White x\nJordan (Shoes). The latter, following the one-by-one decoding order, suffers\nfrom exposure bias issues. We maintain that these works misunderstand the\nrelationships of multimodal entities. To tackle these, we propose a novel\nunified framework named Multi-grained Query-guided Set Prediction Network\n(MQSPN) to learn appropriate relationships at intra-entity and inter-entity\nlevels. Specifically, MQSPN explicitly aligns textual entities with visual\nregions by employing a set of learnable queries to strengthen intra-entity\nconnections. Based on distinct intra-entity modeling, MQSPN reformulates GMNER\nas a set prediction, guiding models to establish appropriate inter-entity\nrelationships from a optimal global matching perspective. Additionally, we\nincorporate a query-guided Fusion Net (QFNet) as a glue network to boost better\nalignment of two-level relationships. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performances in widely used benchmarks.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "13 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.21033v3",
    "published_date": "2024-07-17 05:42:43 UTC",
    "updated_date": "2025-01-25 11:53:12 UTC"
  },
  {
    "arxiv_id": "2407.21032v1",
    "title": "Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion",
    "authors": [
      "Sanghyun Kim",
      "Seohyeon Jung",
      "Balhae Kim",
      "Moonseok Choi",
      "Jinwoo Shin",
      "Juho Lee"
    ],
    "abstract": "This paper addresses the societal concerns arising from large-scale\ntext-to-image diffusion models for generating potentially harmful or\ncopyrighted content. Existing models rely heavily on internet-crawled data,\nwherein problematic concepts persist due to incomplete filtration processes.\nWhile previous approaches somewhat alleviate the issue, they often rely on\ntext-specified concepts, introducing challenges in accurately capturing nuanced\nconcepts and aligning model knowledge with human understandings. In response,\nwe propose a framework named Human Feedback Inversion (HFI), where human\nfeedback on model-generated images is condensed into textual tokens guiding the\nmitigation or removal of problematic images. The proposed framework can be\nbuilt upon existing techniques for the same purpose, enhancing their alignment\nwith human judgment. By doing so, we simplify the training objective with a\nself-distillation-based technique, providing a strong baseline for concept\nremoval. Our experimental results demonstrate our framework significantly\nreduces objectionable content generation while preserving image quality,\ncontributing to the ethical deployment of AI in the public sphere.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024. 56 pages, 24 figures. Caution: This paper contains\n  discussions and examples related to harmful content, including text and\n  images. Reader discretion is advised. Code is available at\n  https://github.com/nannullna/safeguard-hfi",
    "pdf_url": "http://arxiv.org/pdf/2407.21032v1",
    "published_date": "2024-07-17 05:21:41 UTC",
    "updated_date": "2024-07-17 05:21:41 UTC"
  },
  {
    "arxiv_id": "2407.12315v2",
    "title": "ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via Modal Fusion Map",
    "authors": [
      "Yilin Ye",
      "Shishi Xiao",
      "Xingchen Zeng",
      "Wei Zeng"
    ],
    "abstract": "Multi-modal embeddings form the foundation for vision-language models, such\nas CLIP embeddings, the most widely used text-image embeddings. However, these\nembeddings are vulnerable to subtle misalignment of cross-modal features,\nresulting in decreased model performance and diminished generalization. To\naddress this problem, we design ModalChorus, an interactive system for visual\nprobing and alignment of multi-modal embeddings. ModalChorus primarily offers a\ntwo-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel\nparametric dimensionality reduction method that integrates both metric and\nnonmetric objectives to enhance modality fusion; and 2) embedding alignment\nthat allows users to interactively articulate intentions for both point-set and\nset-set alignments. Quantitative and qualitative comparisons for CLIP\nembeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and\ndata fusion (e.g., data context map) methods demonstrate the advantages of MFM\nin showcasing cross-modal features over common vision-language datasets. Case\nstudies reveal that ModalChorus can facilitate intuitive discovery of\nmisalignment and efficient re-alignment in scenarios ranging from zero-shot\nclassification to cross-modal retrieval and generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "VIS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12315v2",
    "published_date": "2024-07-17 04:49:56 UTC",
    "updated_date": "2024-10-26 09:04:25 UTC"
  },
  {
    "arxiv_id": "2407.12292v1",
    "title": "Any Target Can be Offense: Adversarial Example Generation via Generalized Latent Infection",
    "authors": [
      "Youheng Sun",
      "Shengming Yuan",
      "Xuanhan Wang",
      "Lianli Gao",
      "Jingkuan Song"
    ],
    "abstract": "Targeted adversarial attack, which aims to mislead a model to recognize any\nimage as a target object by imperceptible perturbations, has become a\nmainstream tool for vulnerability assessment of deep neural networks (DNNs).\nSince existing targeted attackers only learn to attack known target classes,\nthey cannot generalize well to unknown classes. To tackle this issue, we\npropose $\\bf{G}$eneralized $\\bf{A}$dversarial attac$\\bf{KER}$ ($\\bf{GAKer}$),\nwhich is able to construct adversarial examples to any target class. The core\nidea behind GAKer is to craft a latently infected representation during\nadversarial example generation. To this end, the extracted latent\nrepresentations of the target object are first injected into intermediate\nfeatures of an input image in an adversarial generator. Then, the generator is\noptimized to ensure visual consistency with the input image while being close\nto the target object in the feature space. Since the GAKer is class-agnostic\nyet model-agnostic, it can be regarded as a general tool that not only reveals\nthe vulnerability of more DNNs but also identifies deficiencies of DNNs in a\nwider range of classes. Extensive experiments have demonstrated the\neffectiveness of our proposed method in generating adversarial examples for\nboth known and unknown classes. Notably, compared with other generative\nmethods, our method achieves an approximately $14.13\\%$ higher attack success\nrate for unknown classes and an approximately $4.23\\%$ higher success rate for\nknown classes. Our code is available in https://github.com/VL-Group/GAKer.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.12292v1",
    "published_date": "2024-07-17 03:24:09 UTC",
    "updated_date": "2024-07-17 03:24:09 UTC"
  },
  {
    "arxiv_id": "2407.12288v3",
    "title": "Information-Theoretic Foundations for Machine Learning",
    "authors": [
      "Hong Jun Jeon",
      "Benjamin Van Roy"
    ],
    "abstract": "The staggering progress of machine learning in the past decade has been a\nsight to behold. In retrospect, it is both remarkable and unsettling that these\nmilestones were achievable with little to no rigorous theory to guide\nexperimentation. Despite this fact, practitioners have been able to guide their\nfuture experimentation via observations from previous large-scale empirical\ninvestigations. However, alluding to Plato's Allegory of the cave, it is likely\nthat the observations which form the field's notion of reality are but shadows\nrepresenting fragments of that reality. In this work, we propose a theoretical\nframework which attempts to answer what exists outside of the cave. To the\ntheorist, we provide a framework which is mathematically rigorous and leaves\nopen many interesting ideas for future exploration. To the practitioner, we\nprovide a framework whose results are very intuitive, general, and which will\nhelp form principles to guide future investigations. Concretely, we provide a\ntheoretical framework rooted in Bayesian statistics and Shannon's information\ntheory which is general enough to unify the analysis of many phenomena in\nmachine learning. Our framework characterizes the performance of an optimal\nBayesian learner, which considers the fundamental limits of information.\nThroughout this work, we derive very general theoretical results and apply them\nto derive insights specific to settings ranging from data which is\nindependently and identically distributed under an unknown distribution, to\ndata which is sequential, to data which exhibits hierarchical structure\namenable to meta-learning. We conclude with a section dedicated to\ncharacterizing the performance of misspecified algorithms. These results are\nexciting and particularly relevant as we strive to overcome increasingly\ndifficult machine learning challenges in this endlessly complex world.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12288v3",
    "published_date": "2024-07-17 03:18:40 UTC",
    "updated_date": "2024-08-20 05:34:20 UTC"
  },
  {
    "arxiv_id": "2407.12282v2",
    "title": "Chip Placement with Diffusion Models",
    "authors": [
      "Vint Lee",
      "Minh Nguyen",
      "Leena Elzeiny",
      "Chun Deng",
      "Pieter Abbeel",
      "John Wawrzynek"
    ],
    "abstract": "Macro placement is a vital step in digital circuit design that defines the\nphysical location of large collections of components, known as macros, on a 2D\nchip. Because key performance metrics of the chip are determined by the\nplacement, optimizing it is crucial. Existing learning-based methods typically\nfall short because of their reliance on reinforcement learning (RL), which is\nslow and struggles to generalize, requiring online training on each new\ncircuit. Instead, we train a diffusion model capable of placing new circuits\nzero-shot, using guided sampling in lieu of RL to optimize placement quality.\nTo enable such models to train at scale, we designed a capable yet efficient\narchitecture for the denoising model, and propose a novel algorithm to generate\nlarge synthetic datasets for pre-training. To allow zero-shot transfer to real\ncircuits, we empirically study the design decisions of our dataset generation\nalgorithm, and identify several key factors enabling generalization. When\ntrained on our synthetic data, our models generate high-quality placements on\nunseen, realistic circuits, achieving competitive performance on placement\nbenchmarks compared to state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12282v2",
    "published_date": "2024-07-17 03:02:24 UTC",
    "updated_date": "2025-03-07 05:47:20 UTC"
  },
  {
    "arxiv_id": "2407.12281v2",
    "title": "Turning Generative Models Degenerate: The Power of Data Poisoning Attacks",
    "authors": [
      "Shuli Jiang",
      "Swanand Ravindra Kadhe",
      "Yi Zhou",
      "Farhan Ahmed",
      "Ling Cai",
      "Nathalie Baracaldo"
    ],
    "abstract": "The increasing use of large language models (LLMs) trained by third parties\nraises significant security concerns. In particular, malicious actors can\nintroduce backdoors through poisoning attacks to generate undesirable outputs.\nWhile such attacks have been extensively studied in image domains and\nclassification tasks, they remain underexplored for natural language generation\n(NLG) tasks. To address this gap, we conduct an investigation of various\npoisoning techniques targeting the LLM's fine-tuning phase via prefix-tuning, a\nParameter Efficient Fine-Tuning (PEFT) method. We assess their effectiveness\nacross two generative tasks: text summarization and text completion; and we\nalso introduce new metrics to quantify the success and stealthiness of such NLG\npoisoning attacks. Through our experiments, we find that the prefix-tuning\nhyperparameters and trigger designs are the most crucial factors to influence\nattack success and stealthiness. Moreover, we demonstrate that existing popular\ndefenses are ineffective against our poisoning attacks. Our study presents the\nfirst systematic approach to understanding poisoning attacks targeting NLG\ntasks during fine-tuning via PEFT across a wide range of triggers and attack\nsettings. We hope our findings will aid the AI security community in developing\neffective defenses against such threats.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "18 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.12281v2",
    "published_date": "2024-07-17 03:02:15 UTC",
    "updated_date": "2024-07-18 05:52:35 UTC"
  },
  {
    "arxiv_id": "2407.12277v1",
    "title": "Multimodal Reranking for Knowledge-Intensive Visual Question Answering",
    "authors": [
      "Haoyang Wen",
      "Honglei Zhuang",
      "Hamed Zamani",
      "Alexander Hauptmann",
      "Michael Bendersky"
    ],
    "abstract": "Knowledge-intensive visual question answering requires models to effectively\nuse external knowledge to help answer visual questions. A typical pipeline\nincludes a knowledge retriever and an answer generator. However, a retriever\nthat utilizes local information, such as an image patch, may not provide\nreliable question-candidate relevance scores. Besides, the two-tower\narchitecture also limits the relevance score modeling of a retriever to select\ntop candidates for answer generator reasoning. In this paper, we introduce an\nadditional module, a multi-modal reranker, to improve the ranking quality of\nknowledge candidates for answer generation. Our reranking module takes\nmulti-modal information from both candidates and questions and performs\ncross-item interaction for better relevance score modeling. Experiments on\nOK-VQA and A-OKVQA show that multi-modal reranker from distant supervision\nprovides consistent improvements. We also find a training-testing discrepancy\nwith reranking in answer generation, where performance improves if training\nknowledge candidates are similar to or noisier than those used in testing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12277v1",
    "published_date": "2024-07-17 02:58:52 UTC",
    "updated_date": "2024-07-17 02:58:52 UTC"
  },
  {
    "arxiv_id": "2407.12229v2",
    "title": "Laugh Now Cry Later: Controlling Time-Varying Emotional States of Flow-Matching-Based Zero-Shot Text-to-Speech",
    "authors": [
      "Haibin Wu",
      "Xiaofei Wang",
      "Sefik Emre Eskimez",
      "Manthan Thakker",
      "Daniel Tompkins",
      "Chung-Hsien Tsai",
      "Canrun Li",
      "Zhen Xiao",
      "Sheng Zhao",
      "Jinyu Li",
      "Naoyuki Kanda"
    ],
    "abstract": "People change their tones of voice, often accompanied by nonverbal\nvocalizations (NVs) such as laughter and cries, to convey rich emotions.\nHowever, most text-to-speech (TTS) systems lack the capability to generate\nspeech with rich emotions, including NVs. This paper introduces EmoCtrl-TTS, an\nemotion-controllable zero-shot TTS that can generate highly emotional speech\nwith NVs for any speaker. EmoCtrl-TTS leverages arousal and valence values, as\nwell as laughter embeddings, to condition the flow-matching-based zero-shot\nTTS. To achieve high-quality emotional speech generation, EmoCtrl-TTS is\ntrained using more than 27,000 hours of expressive data curated based on\npseudo-labeling. Comprehensive evaluations demonstrate that EmoCtrl-TTS excels\nin mimicking the emotions of audio prompts in speech-to-speech translation\nscenarios. We also show that EmoCtrl-TTS can capture emotion changes, express\nstrong emotions, and generate various NVs in zero-shot TTS. See\nhttps://aka.ms/emoctrl-tts for demo samples.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted by SLT2024. See https://aka.ms/emoctrl-tts for demo samples",
    "pdf_url": "http://arxiv.org/pdf/2407.12229v2",
    "published_date": "2024-07-17 00:54:15 UTC",
    "updated_date": "2024-09-17 10:40:11 UTC"
  },
  {
    "arxiv_id": "2407.12223v4",
    "title": "Conditional Quantile Estimation for Uncertain Watch Time in Short-Video Recommendation",
    "authors": [
      "Chengzhi Lin",
      "Shuchang Liu",
      "Chuyuan Wang",
      "Yongqi Liu"
    ],
    "abstract": "Accurately predicting watch time is crucial for optimizing recommendations\nand user experience in short video platforms. However, existing methods that\nestimate a single average watch time often fail to capture the inherent\nuncertainty in user engagement patterns. In this paper, we propose Conditional\nQuantile Estimation (CQE) to model the entire conditional distribution of watch\ntime. Using quantile regression, CQE characterizes the complex watch-time\ndistribution for each user-video pair, providing a flexible and comprehensive\napproach to understanding user behavior. We further design multiple strategies\nto combine the quantile estimates, adapting to different recommendation\nscenarios and user preferences. Extensive offline experiments and online A/B\ntests demonstrate the superiority of CQE in watch-time prediction and user\nengagement modeling. Specifically, deploying CQE online on a large-scale\nplatform with hundreds of millions of daily active users has led to substantial\ngains in key evaluation metrics, including active days, engagement time, and\nvideo views. These results highlight the practical impact of our proposed\napproach in enhancing the user experience and overall performance of the short\nvideo recommendation system. The code will be released\nhttps://github.com/justopit/CQE.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.12223v4",
    "published_date": "2024-07-17 00:25:35 UTC",
    "updated_date": "2025-04-13 12:45:32 UTC"
  }
]
{
  "date": "2025-05-25",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-05-25 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv åˆ—è¡¨éå¸¸çƒ­é—¹ï¼Œ**æ¨ç†ï¼ˆReasoningï¼‰**å’Œ**å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**åœ¨å¤šæ¨¡æ€å’Œåè®­ç»ƒé˜¶æ®µçš„åº”ç”¨æ˜¯å¤§çƒ­ç‚¹ï¼Œå°¤å…¶æ˜¯å— DeepSeek-R1 å¯å‘çš„å·¥ä½œï¼ˆå¦‚ SATORI-R1, VTool-R1ï¼‰å±‚å‡ºä¸ç©·ã€‚æ­¤å¤–ï¼Œå…³äº LLM åŸºç¡€ç†è®ºï¼ˆTop-k è§£ç çš„æ•°å­¦æœ¬è´¨ï¼‰å’Œå®‰å…¨æ€§ï¼ˆè¾©è®ºä¸­çš„è‡ªä¿¡å¹»è§‰ã€æŒ‡ä»¤å±‚çº§ï¼‰ä¹Ÿæœ‰å‡ ç¯‡éå¸¸ç¡¬æ ¸çš„æ–‡ç« ã€‚\n\nä¸‹é¢æˆ‘ä»¬ç›´å…¥ä¸»é¢˜ï¼Œçœ‹çœ‹ä»Šå¤©ä¸å®¹é”™è¿‡çš„è®ºæ–‡ã€‚\n\n---\n\n### ğŸŒŸ ç†è®ºåŸºçŸ³ä¸å¤§æ¨¡å‹è¡Œä¸º (Foundations & LLM Behavior)\n\n**1. Top-$k$ è§£ç çš„ç†è®ºåŸºç¡€**\n**# Title: Foundations of Top-$k$ Decoding For Language Models**\nè¿™ç¯‡è®ºæ–‡éå¸¸é‡è¦ã€‚æˆ‘ä»¬æ¯å¤©éƒ½åœ¨ç”¨ Top-$k$ é‡‡æ ·ï¼Œç›´è§‰ä¸Šæ˜¯å› ä¸ºå®ƒæˆªæ–­äº†å™ªå£°å°¾éƒ¨ï¼Œä½†ä¸€ç›´ç¼ºä¹ä¸¥è°¨çš„ç†è®ºæ”¯æ’‘ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåŸºäº **Bregman æ•£åº¦ï¼ˆBregman divergenceï¼‰** çš„ç†è®ºæ¡†æ¶ï¼Œè¯æ˜äº† Top-$k$ è§£ç å®é™…ä¸Šæ˜¯åœ¨æœ€å°åŒ–æŸç§ç¨€ç–æ­£åˆ™åŒ–ä¸‹çš„æ•£åº¦ã€‚æ›´æœ‰è¶£çš„æ˜¯ï¼Œä»–ä»¬è¯æ˜äº†æŸå¤±å‡½æ•°å…³äº $k$ æ˜¯ç¦»æ•£å‡¸çš„ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ç”¨äºŒåˆ†æŸ¥æ‰¾é«˜æ•ˆæ‰¾åˆ°æœ€ä¼˜çš„ $k$ï¼Œè€Œä¸æ˜¯æ‹è„‘è¢‹å®šä¸€ä¸ªå‚æ•°ã€‚\n*æ ¸å¿ƒè´¡çŒ®ï¼šä¸ºå¤§æ¨¡å‹æ¨ç†ä¸­æœ€å¸¸ç”¨çš„é‡‡æ ·ç­–ç•¥è¡¥å…¨äº†ç¼ºå¤±çš„ç†è®ºæ‹¼å›¾ã€‚*\n\n**2. ä¸¤ä¸ª LLM è¾©è®ºï¼šç›²ç›®è‡ªä¿¡çš„é™·é˜±**\n**# Title: When Two LLMs Debate, Both Think They'll Win**\nè¿™ç¯‡è®ºæ–‡æ­ç¤ºäº† LLM åœ¨å¤šè½®è¾©è®ºä¸­çš„ä¸€ä¸ªæœ‰è¶£ä½†ä»¤äººæ‹…å¿§çš„ç°è±¡ï¼š**ç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡ï¼ˆSystematic overconfidenceï¼‰**ã€‚å³ä¾¿å¤„äºé›¶å’Œåšå¼ˆä¸­ï¼ŒåŒæ–¹æ¨¡å‹å¾€å¾€éƒ½è®¤ä¸ºè‡ªå·±æœ‰ 75% ä»¥ä¸Šçš„èƒœç‡ï¼ˆåŠ èµ·æ¥è¶…è¿‡ 100%ï¼Œè¿™åœ¨é€»è¾‘ä¸Šæ˜¯ä¸å¯èƒ½çš„ï¼‰ã€‚è€Œä¸”éšç€è¾©è®ºè¿›è¡Œï¼Œå®ƒä»¬çš„è‡ªä¿¡åº¦ä¸é™åå‡ã€‚è¿™è¡¨æ˜ LLM ç›®å‰è¿˜æ— æ³•åœ¨åŠ¨æ€äº¤äº’ä¸­å‡†ç¡®è¯„ä¼°è‡ªèº«çš„èƒœç‡æˆ–é”™è¯¯ï¼Œè¿™å¯¹ Agentic AI çš„éƒ¨ç½²æ˜¯ä¸ªè­¦é’Ÿã€‚\n\n**3. Vanilla Policy Gradient çš„é€†è¢­**\n**# Title: Improving Value Estimation Critically Enhances Vanilla Policy Gradient**\nRLHF ä¸­å¤§å®¶å¸¸ç”¨ PPOï¼Œå› ä¸ºè§‰å¾—å®ƒç¨³å®šã€‚ä½†è¿™ç¯‡è®ºæ–‡æŒ‘æˆ˜äº†è¿™ä¸€å…±è¯†ï¼Œä½œè€…å‘ç°åªè¦å¢åŠ æ¯æ¬¡è¿­ä»£ä¸­ä»·å€¼ï¼ˆValueï¼‰æ›´æ–°çš„æ­¥æ•°ï¼Œæœ€æœ´ç´ çš„ **Vanilla Policy Gradient** å°±èƒ½è¾¾åˆ°ç”šè‡³è¶…è¿‡ PPO çš„æ•ˆæœã€‚è¿™æ„å‘³ç€æ‰€è°“â€œä¿¡ä»»åŸŸï¼ˆTrust Regionï¼‰â€å¯èƒ½æ²¡æˆ‘ä»¬æƒ³çš„é‚£ä¹ˆå…³é”®ï¼Œ**ä»·å€¼ä¼°è®¡çš„å‡†ç¡®æ€§**æ‰æ˜¯ç‹é“ã€‚\n\n---\n\n### ğŸ§  æ¨ç†ä¸å¼ºåŒ–å­¦ä¹  (Reasoning & RL)\n\n**4. è§†è§‰è¯­è¨€æ¨¡å‹ä¹Ÿè¦ R1 æ—¶åˆ»**\n**# Title: SATORI-R1: Incentivizing Multimodal Reasoning through Explicit Visual Anchoring**\n**# Title: VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use**\nè¿™ä¸¤ç¯‡å·¥ä½œéƒ½æ˜æ˜¾å—åˆ°äº† DeepSeek-R1 çš„å¯å‘ï¼Œè¯•å›¾åœ¨å¤šæ¨¡æ€é¢†åŸŸå¤ç°é•¿æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œ RL çš„å¨åŠ›ã€‚\n*   **SATORI-R1** è§£å†³äº† VLM åœ¨é•¿æ¨ç†ä¸­â€œè§†è§‰å¤±ç„¦â€çš„é—®é¢˜ï¼Œé€šè¿‡å°†ä»»åŠ¡åˆ†è§£ä¸ºæè¿°ã€å®šä½ã€é¢„æµ‹ä¸‰ä¸ªé˜¶æ®µï¼Œå¹¶å¼•å…¥æ˜¾å¼çš„è§†è§‰é”šç‚¹ï¼ˆVisual Anchoringï¼‰å¥–åŠ±ï¼Œåœ¨ VQA ä»»åŠ¡ä¸Šæå‡æ˜¾è‘—ã€‚\n*   **VTool-R1** åˆ™ä¾§é‡äºè®© VLM å­¦ä¼šâ€œç”¨å›¾æ€è€ƒâ€ï¼Œé€šè¿‡ RL è®­ç»ƒæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­è°ƒç”¨è§†è§‰å·¥å…·ï¼ˆå¦‚ç”»å›¾ã€ç¼–è¾‘ï¼‰ï¼Œä»è€Œç”ŸæˆåŒ…å«è§†è§‰æ­¥éª¤çš„å¤šæ¨¡æ€æ€ç»´é“¾ã€‚\n\n**5. æµ‹è¯•æ—¶è®¡ç®—çš„æ•ˆç‡ä¼˜åŒ–**\n**# Title: LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling**\nTest-time scalingï¼ˆæµ‹è¯•æ—¶æ‰©å±•ï¼‰æ˜¯ç°åœ¨çš„æ˜¾å­¦ï¼Œä½†ç”Ÿæˆçš„æ€ç»´é“¾å¾€å¾€å¤ªé•¿å¤ªå•°å—¦ã€‚LIMOPro æå‡ºäº† **PIR (Perplexity-based Importance Refinement)**ï¼Œèƒ½å¤Ÿè¯†åˆ«å¹¶å‰ªææ‰é‚£äº›å¯¹æœ€ç»ˆç­”æ¡ˆç½®ä¿¡åº¦è´¡çŒ®ä¸å¤§çš„â€œåŠŸèƒ½æ€§æ­¥éª¤â€ï¼Œåªä¿ç•™æ ¸å¿ƒçš„æ¸è¿›å¼æ¨ç†ã€‚ç»“æœæ˜¯ï¼šæ¨ç†æ›´çŸ­ï¼ŒToken æ›´å°‘ï¼Œä½†å‡†ç¡®ç‡åè€Œæ›´é«˜ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ç”Ÿæˆä¸ç†è§£ (Multimodal Gen & Und)\n\n**6. è§†é¢‘ç”Ÿæˆçš„åŠ é€Ÿæ–¹æ¡ˆ**\n**# Title: SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation**\nè§†é¢‘ç”Ÿæˆï¼ˆå¦‚ Sora, CogVideoXï¼‰å¤ªæ…¢äº†ã€‚SRDiffusion æå‡ºäº†ä¸€ç§â€œè‰å›¾-æ¸²æŸ“â€çš„åä½œæ¨¡å¼ï¼šç”¨å¤§æ¨¡å‹å¤„ç†é«˜å™ªå£°é˜¶æ®µï¼ˆå®šå¤§å±€ã€å®šåŠ¨ä½œï¼‰ï¼Œç”¨å°æ¨¡å‹å¤„ç†ä½å™ªå£°é˜¶æ®µï¼ˆè¡¥ç»†èŠ‚ã€æ¸²æŸ“ï¼‰ã€‚è¿™ç§å¤§å°æ¨¡å‹æ¥åŠ›çš„æ–¹å¼ï¼Œåœ¨å‡ ä¹ä¸æŸå¤±è´¨é‡çš„å‰æä¸‹ï¼ŒæŠŠæ¨ç†é€Ÿåº¦æå‡äº† 2-3 å€ã€‚\n\n**7. ç‰©ç†æ¨ç†ï¼šè§†è§‰çœŸçš„æœ‰å¸®åŠ©å—ï¼Ÿ**\n**# Title: SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning**\nä½œè€…æ„å»ºäº†ä¸€ä¸ªæ¶µç›–åˆä¸­åˆ°åšå£«èµ„æ ¼è€ƒè¯•éš¾åº¦çš„ç‰©ç†æ¨ç†åŸºå‡† **SeePhys**ã€‚ç»“æœå¾ˆæ‰“è„¸ï¼šå³ä¾¿æ˜¯æœ€å¼ºçš„ Gemini-1.5-pro å’Œ o4-miniï¼Œå‡†ç¡®ç‡ä¹Ÿä¸åˆ° 60%ã€‚æ¨¡å‹ä¸»è¦çš„é—®é¢˜æ˜¯æ— æ³•å°†å›¾è¡¨ä¿¡æ¯ä¸ç‰©ç†æ¨ç†ä¸¥å¯†è€¦åˆï¼Œä¸”è¿‡åº¦ä¾èµ–æ–‡æœ¬æç¤ºä½œä¸ºâ€œè®¤çŸ¥æ·å¾„â€ã€‚\n\n**8. ç»Ÿä¸€è§†è§‰ç”Ÿæˆä¸ç†è§£**\n**# Title: Jodi: Unification of Visual Generation and Understanding via Joint Modeling**\nè¿™æ˜¯ä¸€ä¸ªâ€œå¤§ä¸€ç»Ÿâ€çš„å°è¯•ã€‚Jodi æ¨¡å‹é€šè¿‡çº¿æ€§æ‰©æ•£ Transformer å’Œè§’è‰²åˆ‡æ¢æœºåˆ¶ï¼Œåœ¨ä¸€ä¸ªæ¨¡å‹é‡ŒåŒæ—¶å®ç°äº†ï¼šæ–‡ç”Ÿå›¾ã€å¯æ§ç”Ÿæˆã€ä»¥åŠçœ‹å›¾è¯´è¯ï¼ˆç†è§£ï¼‰ã€‚è¿™ç§è”åˆå»ºæ¨¡ï¼ˆJoint Modelingï¼‰æ˜¯æœªæ¥çš„è¶‹åŠ¿ï¼Œä¸å†éœ€è¦ä¸ºç”Ÿæˆå’Œç†è§£åˆ†åˆ«è®­ç»ƒä¸¤ä¸ªæ¨¡å‹ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€å¯¹é½ä¸ç¤¾ä¼šå½±å“ (Safety, Alignment & Society)\n\n**9. åŒºåŸŸæ¨¡å‹ä¸æ‡‚å½“åœ°æ–‡åŒ–**\n**# Title: Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment**\nä¸ºäº†è§£å†³ LLM è¿‡äºè¥¿æ–¹åŒ–çš„é—®é¢˜ï¼Œå¾ˆå¤šå›½å®¶å¼€å‘äº†â€œåŒºåŸŸæ€§ LLMâ€ï¼ˆæ¯”å¦‚å°åº¦æ¨¡å‹ï¼‰ã€‚ä½†è¿™ç¯‡è®ºæ–‡å‘ç°ï¼Œè¿™äº›æ¨¡å‹è™½ç„¶èƒ½è¯´æµåˆ©çš„å½“åœ°è¯­è¨€ï¼Œä½†**ä»·å€¼è§‚å’Œä¹ ä¿—**ä¾ç„¶æ˜¯è¥¿æ–¹çš„ã€‚ç”šè‡³ï¼Œä¸€ä¸ªç¾å›½å—è®¿è€…çš„ä»·å€¼è§‚éƒ½æ¯”è¿™äº›â€œæœ¬åœŸæ¨¡å‹â€æ›´æ¥è¿‘å½“åœ°äººã€‚è¿™è¯´æ˜ä»…é è¯­è¨€è®­ç»ƒä¸è¶³ä»¥å®ç°æ–‡åŒ–å¯¹é½ï¼Œéœ€è¦æ›´æ·±å±‚çš„æ–‡åŒ–æ•°æ®ã€‚\n\n**10. é˜²å¾¡â€œæŒ‡ä»¤å±‚çº§â€æ”»å‡»**\n**# Title: Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations**\næç¤ºæ³¨å…¥ï¼ˆPrompt Injectionï¼‰æ”»å‡»å¾ˆéš¾é˜²ï¼Œå› ä¸ºæ¨¡å‹åˆ†ä¸æ¸…å“ªäº›æ˜¯ç³»ç»ŸæŒ‡ä»¤ï¼Œå“ªäº›æ˜¯ç”¨æˆ·è¾“å…¥ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºä¸ä»…ä»…åœ¨è¾“å…¥å±‚åŒºåˆ†ï¼Œè€Œæ˜¯å°†**æŒ‡ä»¤å±‚çº§ï¼ˆInstruction Hierarchyï¼‰**ä¿¡å·æ³¨å…¥åˆ°æ¨¡å‹çš„**ä¸­é—´å±‚è¡¨ç¤º**ä¸­ã€‚è¿™ç§â€œå…¨ç¨‹ä¸¥é˜²æ­»å®ˆâ€çš„ç­–ç•¥å°†æ”»å‡»æˆåŠŸç‡é™ä½äº†æ•°å€ã€‚\n\n**11. å¯¹æŠ—â€œé—å¿˜æ”»å‡»â€**\n**# Title: An Embarrassingly Simple Defense Against LLM Abliteration Attacks**\næœ€è¿‘æœ‰ä¸€ç§å« \"Abliteration\" çš„æ”»å‡»ï¼Œé€šè¿‡æŠ‘åˆ¶æ¨¡å‹çš„â€œæ‹’ç»æ–¹å‘â€æ¥ç»•è¿‡å®‰å…¨é™åˆ¶ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç®€å•çš„é˜²å¾¡ï¼š**Extended-refusalï¼ˆæ‰©å±•æ‹’ç»ï¼‰**ã€‚ä¸åªæ˜¯ç®€å•è¯´ \"I cannot\"ï¼Œè€Œæ˜¯è®­ç»ƒæ¨¡å‹åœ¨æ‹’ç»å‰ç»™å‡ºè¯¦ç»†çš„ç†ç”±ã€‚è¿™ç§å°†æ‹’ç»ä¿¡å·åˆ†æ•£åˆ°å¤šä¸ª Token çš„åšæ³•ï¼Œè®©æ”»å‡»è€…å¾ˆéš¾é€šè¿‡åˆ‡æ–­å•ä¸€æ–¹å‘æ¥ç ´è§£ã€‚\n\n---\n\n### âš—ï¸ AI for Science & Agent\n\n**12. ç§‘å­¦å‡è®¾ç”Ÿæˆçš„ç²¾ç»†åŒ–**\n**# Title: MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search**\nç°åœ¨çš„ LLM ç”Ÿæˆçš„ç§‘å­¦å‡è®¾å¾€å¾€å¤ªç¬¼ç»Ÿã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†**åˆ†å±‚æœç´¢ï¼ˆHierarchical Searchï¼‰**æ–¹æ³•ï¼Œä»å¤§æ–¹å‘é€æ­¥ç»†åŒ–åˆ°å…·ä½“çš„å®éªŒé…ç½®ï¼Œæ—¨åœ¨è®© LLM ç”ŸæˆçœŸæ­£å¯æ‰§è¡Œã€åŒ…å«æ–¹æ³•è®ºç»†èŠ‚çš„ç§‘å­¦å‡è®¾ã€‚\n\n**13. è›‹ç™½è´¨è®¾è®¡çš„â€œè¯æ±‡è¡¨â€**\n**# Title: Protein Design with Dynamic Protein Vocabulary**\n**# Title: PDFBench: A Benchmark for De novo Protein Design from Function**\nè›‹ç™½è´¨è®¾è®¡é¢†åŸŸä»Šå¤©æœ‰ä¸¤ç¯‡å€¼å¾—å…³æ³¨ã€‚**ProDVa** å¼•å…¥äº†â€œåŠ¨æ€è›‹ç™½è´¨ç‰‡æ®µâ€ä½œä¸ºè¯æ±‡è¡¨ï¼Œå€Ÿé‰´è‡ªç„¶ç•Œä¸­è›‹ç™½è´¨çš„æŠ˜å æ¨¡å¼ï¼Œå¤§å¤§æé«˜äº†ç”Ÿæˆç»“æ„çš„å¯æŠ˜å æ€§ã€‚**PDFBench** åˆ™æ˜¯ä¸€ä¸ªéå¸¸éœ€è¦çš„åŸºå‡†æµ‹è¯•ï¼Œç»Ÿä¸€äº†ä»åŠŸèƒ½æè¿°è®¾è®¡è›‹ç™½è´¨çš„è¯„ä¼°æ ‡å‡†ã€‚\n\n**14. Agent äº’æ“ä½œæ€§åè®®**\n**# Title: Collaborative Agentic AI Needs Interoperability Across Ecosystems**\néšç€ Agent è¶Šæ¥è¶Šå¤šï¼Œä¸åŒç”Ÿæ€ç³»ç»Ÿï¼ˆå¦‚ OpenAI, Google, Open Sourceï¼‰çš„ Agent æ²¡æ³•åˆä½œã€‚ä½œè€…æå‡ºäº† **Web of Agents** æ¶æ„ï¼Œæ—¨åœ¨å»ºç«‹ä¸€å¥—æœ€å°åŒ–çš„æ ‡å‡†åè®®ï¼Œè®©ä¸åŒå¹³å°çš„ Agent èƒ½å¤Ÿå‘ç°å½¼æ­¤ã€äº¤æ¢ä¿¡æ¯å¹¶ååŒå·¥ä½œã€‚è¿™æ˜¯ Agent ä»â€œç©å…·â€èµ°å‘â€œåŸºç¡€è®¾æ–½â€çš„å¿…ç»ä¹‹è·¯ã€‚\n\n---\n\n**å¿«é€Ÿæ è¿‡ (Quick Skim)**ï¼š\n*   **# Title: 100-LongBench**: è´¨ç–‘ç°æœ‰çš„é•¿æ–‡æœ¬åŸºå‡†å¹¶æ²¡æœ‰çœŸæ­£æµ‹è¯•é•¿æ–‡æœ¬èƒ½åŠ›ï¼Œæå‡ºäº†æ›´å¯æ§çš„è¯„ä¼°ã€‚\n*   **# Title: FastMamba**: é’ˆå¯¹ FPGA ä¼˜åŒ–çš„ Mamba åŠ é€Ÿå™¨ï¼Œè¾¹ç¼˜ç«¯éƒ¨ç½²çš„ç¦éŸ³ã€‚\n*   **# Title: FP4 All the Way**: å…¨é“¾è·¯ FP4 é‡åŒ–è®­ç»ƒ LLMï¼Œçœæ˜¾å­˜çš„æ–°æé™ã€‚\n*   **# Title: Graph Anomaly Detection**: æå‡ºäº†åŸºäºå¡æ–¹å°æ³¢ï¼ˆChi-Square Waveletï¼‰çš„å›¾å¼‚å¸¸æ£€æµ‹ï¼Œè§£å†³äº†å¼‚æ„å›¾çš„å¹³æ»‘é—®é¢˜ã€‚\n\n---\nå¸Œæœ›è¿™ä»½å¿«æŠ¥èƒ½å¸®ä½ èŠ‚çœæ—¶é—´ï¼æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2505.19371v1",
      "title": "Foundations of Top-$k$ Decoding For Language Models",
      "title_zh": "è¯­è¨€æ¨¡å‹ Top-$k$ è§£ç çš„ç†è®ºåŸºç¡€",
      "authors": [
        "Georgy Noarov",
        "Soham Mallick",
        "Tao Wang",
        "Sunay Joshi",
        "Yan Sun",
        "Yangxinyu Xie",
        "Mengxin Yu",
        "Edgar Dobriban"
      ],
      "abstract": "Top-$k$ decoding is a widely used method for sampling from LLMs: at each token, only the largest $k$ next-token-probabilities are kept, and the next token is sampled after re-normalizing them to sum to unity. Top-$k$ and other sampling methods are motivated by the intuition that true next-token distributions are sparse, and the noisy LLM probabilities need to be truncated. However, to our knowledge, a precise theoretical motivation for the use of top-$k$ decoding is missing. In this work, we develop a theoretical framework that both explains and generalizes top-$k$ decoding. We view decoding at a fixed token as the recovery of a sparse probability distribution. We consider \\emph{Bregman decoders} obtained by minimizing a separable Bregman divergence (for both the \\emph{primal} and \\emph{dual} cases) with a sparsity-inducing $\\ell_0$ regularization. Despite the combinatorial nature of the objective, we show how to optimize it efficiently for a large class of divergences. We show that the optimal decoding strategies are greedy, and further that the loss function is discretely convex in $k$, so that binary search provably and efficiently finds the optimal $k$. We show that top-$k$ decoding arises as a special case for the KL divergence, and identify new decoding strategies that have distinct behaviors (e.g., non-linearly up-weighting larger probabilities after re-normalization).",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ä¸ºè¯­è¨€æ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨çš„ Top-$k$ decoding æä¾›äº†ç†è®ºåŸºç¡€ï¼Œæ—¨åœ¨è§£å†³è¯¥æ–¹æ³•é•¿æœŸç¼ºä¹ç²¾ç¡®ç†è®ºåŠ¨æœºçš„é—®é¢˜ã€‚ä½œè€…å¼€å‘äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œå°†è§£ç è¿‡ç¨‹è§†ä¸ºç¨€ç–æ¦‚ç‡åˆ†å¸ƒçš„æ¢å¤ï¼Œå¹¶æå‡ºäº†åŸºäºæœ€å°åŒ–å¯åˆ†ç¦» Bregman divergence ä¸”å¸¦æœ‰è¯±å¯¼ç¨€ç–æ€§çš„ $\\ell_0$ regularization çš„ Bregman decodersã€‚ç ”ç©¶è¯æ˜äº†æœ€ä¼˜è§£ç ç­–ç•¥å…·æœ‰è´ªå©ªæ€§ï¼Œä¸”æŸå¤±å‡½æ•°åœ¨ $k$ ä¸Šå‘ˆç°ç¦»æ•£å‡¸æ€§ï¼Œä½¿å¾—é€šè¿‡ binary search é«˜æ•ˆå¯»æ‰¾æœ€ä¼˜ $k$ æˆä¸ºå¯èƒ½ã€‚è¯¥æ¡†æ¶æ­ç¤ºäº† Top-$k$ decoding æ˜¯ KL divergence ä¸‹çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå¹¶ç”±æ­¤è¯†åˆ«å‡ºäº†å…·æœ‰éçº¿æ€§ä¸Šè°ƒæ¦‚ç‡ç­‰ç‹¬ç‰¹è¡Œä¸ºçš„æ–°å‹è§£ç ç­–ç•¥ã€‚è¿™é¡¹å·¥ä½œä¸ä»…è§£é‡Šäº†ç°æœ‰é‡‡æ ·æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºå¼€å‘æ›´å…·é€šç”¨æ€§å’Œç†è®ºä¿éšœçš„è¯­è¨€æ¨¡å‹è§£ç ç®—æ³•å¥ å®šäº†æ•°å­¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19371v1",
      "published_date": "2025-05-25 23:46:34 UTC",
      "updated_date": "2025-05-25 23:46:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:55:35.238977+00:00"
    },
    {
      "arxiv_id": "2505.19369v1",
      "title": "SETransformer: A Hybrid Attention-Based Architecture for Robust Human Activity Recognition",
      "title_zh": "SETransformerï¼šä¸€ç§åŸºäºæ··åˆæ³¨æ„åŠ›çš„é²æ£’äººä½“æ´»åŠ¨è¯†åˆ«æ¶æ„",
      "authors": [
        "Yunbo Liu",
        "Xukui Qin",
        "Yifan Gao",
        "Xiang Li",
        "Chengwei Feng"
      ],
      "abstract": "Human Activity Recognition (HAR) using wearable sensor data has become a central task in mobile computing, healthcare, and human-computer interaction. Despite the success of traditional deep learning models such as CNNs and RNNs, they often struggle to capture long-range temporal dependencies and contextual relevance across multiple sensor channels. To address these limitations, we propose SETransformer, a hybrid deep neural architecture that combines Transformer-based temporal modeling with channel-wise squeeze-and-excitation (SE) attention and a learnable temporal attention pooling mechanism. The model takes raw triaxial accelerometer data as input and leverages global self-attention to capture activity-specific motion dynamics over extended time windows, while adaptively emphasizing informative sensor channels and critical time steps.\n  We evaluate SETransformer on the WISDM dataset and demonstrate that it significantly outperforms conventional models including LSTM, GRU, BiLSTM, and CNN baselines. The proposed model achieves a validation accuracy of 84.68\\% and a macro F1-score of 84.64\\%, surpassing all baseline architectures by a notable margin. Our results show that SETransformer is a competitive and interpretable solution for real-world HAR tasks, with strong potential for deployment in mobile and ubiquitous sensing applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººä½“æ´»åŠ¨è¯†åˆ« (Human Activity Recognition, HAR) ä¸­ä¼ ç»Ÿæ¨¡å‹éš¾ä»¥æ•æ‰é•¿ç¨‹æ—¶é—´ä¾èµ–å’Œä¼ æ„Ÿå™¨é€šé“ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† SETransformer æ··åˆæ¶æ„ã€‚è¯¥æ¶æ„é›†æˆäº† Transformer æ—¶é—´å»ºæ¨¡ã€é€šé“çº§æŒ¤å‹æ¿€åŠ± (Squeeze-and-Excitation, SE) æ³¨æ„åŠ›ä»¥åŠå¯å­¦ä¹ çš„æ—¶é—´æ³¨æ„åŠ›æ± åŒ–æœºåˆ¶ï¼Œèƒ½å¤Ÿä»åŸå§‹ä¸‰è½´åŠ é€Ÿåº¦è®¡æ•°æ®ä¸­æœ‰æ•ˆæå–è¿åŠ¨åŠ¨æ€ã€‚åœ¨ WISDM æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSETransformer çš„éªŒè¯å‡†ç¡®ç‡è¾¾åˆ° 84.68%ï¼Œå® F1 åˆ†æ•°ä¸º 84.64%ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äº LSTMã€GRUã€BiLSTM å’Œ CNN ç­‰åŸºçº¿æ¨¡å‹ã€‚ç»“æœè¯æ˜è¯¥æ¨¡å‹ä¸ºç°å®ä¸–ç•Œçš„ HAR ä»»åŠ¡æä¾›äº†ä¸€ç§å…·æœ‰ç«äº‰åŠ›å’Œå¯è§£é‡Šæ€§çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨ç§»åŠ¨å’Œæ³›åœ¨æ„ŸçŸ¥åº”ç”¨é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„éƒ¨ç½²æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19369v1",
      "published_date": "2025-05-25 23:39:34 UTC",
      "updated_date": "2025-05-25 23:39:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:55:39.129743+00:00"
    },
    {
      "arxiv_id": "2505.19361v4",
      "title": "Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments",
      "title_zh": "é’ˆå¯¹æ–°ç¯å¢ƒä¸‹å¤šé¢„è®­ç»ƒæ¨¡å‹æ„ŸçŸ¥é”™è¯¯çš„ä¸€è‡´æ€§æº¯å› æ¨ç†",
      "authors": [
        "Mario Leiva",
        "Noel Ngu",
        "Joshua Shay Kricheli",
        "Aditya Taparia",
        "Ransalu Senanayake",
        "Paulo Shakarian",
        "Nathaniel Bastian",
        "John Corcoran",
        "Gerardo Simari"
      ],
      "abstract": "The deployment of pre-trained perception models in novel environments often leads to performance degradation due to distributional shifts. Although recent artificial intelligence approaches for metacognition use logical rules to characterize and filter model errors, improving precision often comes at the cost of reduced recall. This paper addresses the hypothesis that leveraging multiple pre-trained models can mitigate this recall reduction. We formulate the challenge of identifying and managing conflicting predictions from various models as a consistency-based abduction problem, building on the idea of abductive learning (ABL) but applying it to test-time instead of training. The input predictions and the learned error detection rules derived from each model are encoded in a logic program. We then seek an abductive explanation--a subset of model predictions--that maximizes prediction coverage while ensuring the rate of logical inconsistencies (derived from domain constraints) remains below a specified threshold. We propose two algorithms for this knowledge representation task: an exact method based on Integer Programming (IP) and an efficient Heuristic Search (HS). Through extensive experiments on a simulated aerial imagery dataset featuring controlled, complex distributional shifts, we demonstrate that our abduction-based framework outperforms individual models and standard ensemble baselines, achieving, for instance, average relative improvements of approximately 13.6\\% in F1-score and 16.6\\% in accuracy across 15 diverse test datasets when compared to the best individual model. Our results validate the use of consistency-based abduction as an effective mechanism to robustly integrate knowledge from multiple imperfect models in challenging, novel scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢„è®­ç»ƒæ„ŸçŸ¥æ¨¡å‹åœ¨é™Œç”Ÿç¯å¢ƒä¸­å› åˆ†å¸ƒåç§»(distributional shifts)å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä¸€è‡´æ€§çš„æº¯å› æ¨ç†(consistency-based abduction)æ¡†æ¶ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†æº¯å› å­¦ä¹ (abductive learning, ABL)çš„æ€æƒ³ï¼Œå°†å…¶åˆ›æ–°æ€§åœ°åº”ç”¨äºæµ‹è¯•é˜¶æ®µè€Œéè®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡é€»è¾‘ç¨‹åºç¼–ç å¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœåŠå…¶é”™è¯¯æ£€æµ‹è§„åˆ™ã€‚ç ”ç©¶çš„æ ¸å¿ƒåœ¨äºå¯»æ‰¾ä¸€ä¸ªèƒ½æœ€å¤§åŒ–é¢„æµ‹è¦†ç›–èŒƒå›´çš„æº¯å› è§£é‡Š(abductive explanation)å­é›†ï¼ŒåŒæ—¶ç¡®ä¿é€»è¾‘ä¸ä¸€è‡´ç‡åœ¨é¢†åŸŸçº¦æŸä¸‹ä½äºé¢„è®¾é˜ˆå€¼ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†åŸºäºæ•´æ•°è§„åˆ’(Integer Programming, IP)çš„ç²¾ç¡®ç®—æ³•å’Œå¯å‘å¼æœç´¢(Heuristic Search, HS)ç®—æ³•æ¥å¤„ç†è¿™ä¸€çŸ¥è¯†è¡¨ç¤ºä»»åŠ¡ã€‚åœ¨æ¨¡æ‹Ÿèˆªç©ºå›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨F1-scoreå’Œå‡†ç¡®ç‡ä¸Šåˆ†åˆ«æ¯”è¡¨ç°æœ€å¥½çš„å•ä¸€æ¨¡å‹æå‡äº†çº¦13.6%å’Œ16.6%ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†åˆ©ç”¨ä¸€è‡´æ€§æº¯å› æ¨ç†å¯ä»¥ä½œä¸ºä¸€ç§æœ‰æ•ˆæœºåˆ¶ï¼Œåœ¨æŒ‘æˆ˜æ€§çš„æ–°åœºæ™¯ä¸­ç¨³å¥åœ°æ•´åˆæ¥è‡ªå¤šä¸ªä¸å®Œç¾æ¨¡å‹çš„çŸ¥è¯†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19361v4",
      "published_date": "2025-05-25 23:17:47 UTC",
      "updated_date": "2025-12-03 20:59:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:55:43.647457+00:00"
    },
    {
      "arxiv_id": "2505.19356v2",
      "title": "Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval",
      "title_zh": "é¢å‘é˜¿å§†å“ˆæ‹‰è¯­æ®µè½æ£€ç´¢çš„ä¼˜åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹ä¸è¯„æµ‹åŸºå‡†",
      "authors": [
        "Kidist Amde Mekonnen",
        "Yosef Worku Alemneh",
        "Maarten de Rijke"
      ],
      "abstract": "Neural retrieval methods using transformer-based pre-trained language models have advanced multilingual and cross-lingual retrieval. However, their effectiveness for low-resource, morphologically rich languages such as Amharic remains underexplored due to data scarcity and suboptimal tokenization. We address this gap by introducing Amharic-specific dense retrieval models based on pre-trained Amharic BERT and RoBERTa backbones. Our proposed RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while being over 13x smaller. Additionally, we train a ColBERT-based late interaction retrieval model that achieves the highest MRR@10 score (0.843) among all evaluated models. We benchmark our proposed models against both sparse and dense retrieval baselines to systematically assess retrieval effectiveness in Amharic. Our analysis highlights key challenges in low-resource settings and underscores the importance of language-specific adaptation. To foster future research in low-resource IR, we publicly release our dataset, codebase, and trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.",
      "tldr_zh": "é’ˆå¯¹é˜¿å§†å“ˆæ‹‰è¯­(Amharic)è¿™ç§ä½èµ„æºä¸”å½¢æ€ä¸°å¯Œçš„è¯­è¨€åœ¨ç¥ç»æ£€ç´¢é¢†åŸŸç”±äºæ•°æ®åŒ®ä¹å’Œåˆ†è¯ä¸ç†æƒ³å¯¼è‡´çš„æ•ˆèƒ½ä¸è¶³é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸“é—¨é’ˆå¯¹é˜¿å§†å“ˆæ‹‰è¯­ä¼˜åŒ–çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹å’Œè¯„ä¼°åŸºå‡†ã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†åŸºäºé¢„è®­ç»ƒ Amharic BERT å’Œ RoBERTa éª¨å¹²ç½‘ç»œçš„å¯†é›†æ£€ç´¢æ¨¡å‹ï¼Œå…¶ä¸­æ‹¥æœ‰1.1äº¿å‚æ•°çš„ RoBERTa-Base-Amharic-Embed æ¨¡å‹åœ¨ MRR@10 æŒ‡æ ‡ä¸Šæ¯”æœ€å¼ºçš„å¤šè¯­è¨€åŸºå‡† Arctic Embed 2.0 æå‡äº†17.6%ï¼ŒRecall@10 æå‡äº†9.86%ã€‚æ­¤å¤–ï¼Œæ›´è½»é‡åŒ–çš„å˜ä½“å¦‚ RoBERTa-Medium-Amharic-Embed (42M) åœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶ï¼Œå‚æ•°è§„æ¨¡æ¯”åŸºå‡†æ¨¡å‹å°13å€ä»¥ä¸Šã€‚è¯¥ç ”ç©¶è¿˜è®­ç»ƒäº†ä¸€ä¸ªåŸºäº ColBERT çš„åæœŸäº¤äº’(late interaction)æ£€ç´¢æ¨¡å‹ï¼Œå¹¶åœ¨æ‰€æœ‰è¯„ä¼°æ¨¡å‹ä¸­å–å¾—äº†æœ€é«˜çš„ MRR@10 åˆ†æ•°(0.843)ã€‚é€šè¿‡å¯¹ç¨€ç–å’Œå¯†é›†æ£€ç´¢åŸºå‡†çš„ç³»ç»Ÿè¯„ä¼°ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†ä½èµ„æºç¯å¢ƒä¸‹ä¿¡æ¯æ£€ç´¢çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶å…¬å¼€äº†ç›¸å…³æ•°æ®é›†ã€ä»£ç åº“å’Œè®­ç»ƒæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages (excl. refs/appendix), 10 figures. Accepted to ACL 2025 Findings. Kidist and Yosef contributed equally to this work. Public resources: https://github.com/kidist-amde/amharic-ir-benchmarks",
      "pdf_url": "https://arxiv.org/pdf/2505.19356v2",
      "published_date": "2025-05-25 23:06:20 UTC",
      "updated_date": "2025-06-10 13:33:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:55:43.511931+00:00"
    },
    {
      "arxiv_id": "2505.19353v1",
      "title": "Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation",
      "title_zh": "é”™è¯¯æ¶æ„ï¼šäººå·¥æ™ºèƒ½ä¸äººç±»ä»£ç ç”Ÿæˆçš„å“²å­¦æ¢ç©¶",
      "authors": [
        "Camilo ChacÃ³n Sartori"
      ],
      "abstract": "With the rise of generative AI (GenAI), Large Language Models are increasingly employed for code generation, becoming active co-authors alongside human programmers. Focusing specifically on this application domain, this paper articulates distinct ``Architectures of Error'' to ground an epistemic distinction between human and machine code generation. Examined through their shared vulnerability to error, this distinction reveals fundamentally different causal origins: human-cognitive versus artificial-stochastic. To develop this framework and substantiate the distinction, the analysis draws critically upon Dennett's mechanistic functionalism and Rescher's methodological pragmatism. I argue that a systematic differentiation of these error profiles raises critical philosophical questions concerning semantic coherence, security robustness, epistemic limits, and control mechanisms in human-AI collaborative software development. The paper also utilizes Floridi's levels of abstraction to provide a nuanced understanding of how these error dimensions interact and may evolve with technological advancements. This analysis aims to offer philosophers a structured framework for understanding GenAI's unique epistemological challenges, shaped by these architectural foundations, while also providing software engineers a basis for more critically informed engagement.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (GenAI) åœ¨ä»£ç ç”Ÿæˆä¸­ä¸äººç±»åä½œçš„å“²å­¦å†…æ¶µï¼Œå¹¶æå‡ºäº†â€œé”™è¯¯æ¶æ„â€ (Architectures of Error) è¿™ä¸€æ¡†æ¶æ¥åŒºåˆ†äººç±»ä¸æœºå™¨ä»£ç ç”Ÿæˆçš„è®¤è¯†è®ºå·®å¼‚ã€‚é€šè¿‡å¯¹ä¸¤è€…åœ¨é”™è¯¯è„†å¼±æ€§ä¸Šçš„åˆ†æï¼Œè®ºæ–‡æ­ç¤ºäº†äººç±»è®¤çŸ¥ (human-cognitive) ä¸äººå·¥æ™ºèƒ½éšæœºæ€§ (artificial-stochastic) ä¹‹é—´æ ¹æœ¬ä¸åŒçš„å› æœèµ·æºã€‚è¯¥åˆ†ææ‰¹åˆ¤æ€§åœ°å€Ÿé‰´äº† Dennett çš„æœºæ¢°åŠŸèƒ½ä¸»ä¹‰ (mechanistic functionalism) å’Œ Rescher çš„æ–¹æ³•è®ºå®ç”¨ä¸»ä¹‰ (methodological pragmatism) æ¥æ·±åŒ–è¿™ä¸€åŒºåˆ†ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œç³»ç»Ÿæ€§çš„é”™è¯¯å‰–é¢å·®å¼‚å¼•å‘äº†å…³äºè¯­ä¹‰è¿è´¯æ€§ (semantic coherence)ã€å®‰å…¨æ€§ç¨³å¥æ€§ä»¥åŠäººç±»ä¸ AI åä½œå¼€å‘ä¸­æ§åˆ¶æœºåˆ¶çš„å…³é”®å“²å­¦æ€è€ƒã€‚æ­¤å¤–ï¼Œè®ºæ–‡åˆ©ç”¨ Floridi çš„æŠ½è±¡å±‚æ¬¡ (levels of abstraction) ç†è®ºæ¥é˜é‡Šè¿™äº›é”™è¯¯ç»´åº¦å¦‚ä½•éšæŠ€æœ¯è¿›æ­¥è€Œæ¼”å˜ã€‚è¯¥å·¥ä½œæ—¨åœ¨ä¸ºå“²å­¦å®¶æä¾›ç†è§£ GenAI è®¤è¯†è®ºæŒ‘æˆ˜çš„ç»“æ„åŒ–æ¡†æ¶ï¼Œå¹¶ä¸ºè½¯ä»¶å·¥ç¨‹å¸ˆæä¾›å…·å¤‡æ‰¹åˆ¤æ€§è§†è§’çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2505.19353v1",
      "published_date": "2025-05-25 22:49:36 UTC",
      "updated_date": "2025-05-25 22:49:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:55:42.679052+00:00"
    },
    {
      "arxiv_id": "2505.23791v1",
      "title": "Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning",
      "title_zh": "è”é‚¦å­¦ä¹ ä¸­åŸºäºè¿ç§»å­¦ä¹ çš„æ¨¡å‹æå–æ”»å‡»æŸ¥è¯¢æ•ˆç‡ä¸å‡†ç¡®ç‡è¯„ä¼°",
      "authors": [
        "Sayyed Farid Ahamed",
        "Sandip Roy",
        "Soumya Banerjee",
        "Marc Vucovich",
        "Kevin Choi",
        "Abdul Rahman",
        "Alison Hu",
        "Edward Bowen",
        "Sachin Shetty"
      ],
      "abstract": "Federated Learning (FL) is a collaborative learning framework designed to protect client data, yet it remains highly vulnerable to Intellectual Property (IP) threats. Model extraction (ME) attacks pose a significant risk to Machine Learning as a Service (MLaaS) platforms, enabling attackers to replicate confidential models by querying black-box (without internal insight) APIs. Despite FL's privacy-preserving goals, its distributed nature makes it particularly susceptible to such attacks. This paper examines the vulnerability of FL-based victim models to two types of model extraction attacks. For various federated clients built under the NVFlare platform, we implemented ME attacks across two deep learning architectures and three image datasets. We evaluate the proposed ME attack performance using various metrics, including accuracy, fidelity, and KL divergence. The experiments show that for different FL clients, the accuracy and fidelity of the extracted model are closely related to the size of the attack query set. Additionally, we explore a transfer learning based approach where pretrained models serve as the starting point for the extraction process. The results indicate that the accuracy and fidelity of the fine-tuned pretrained extraction models are notably higher, particularly with smaller query sets, highlighting potential advantages for attackers.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†è”é‚¦å­¦ä¹ (Federated Learning)åœ¨é¢å¯¹æ¨¡å‹çªƒå–æ”»å‡»(Model Extraction Attacks)æ—¶çš„çŸ¥è¯†äº§æƒ(IP)é£é™©ï¼Œæ¢è®¨äº†åˆ†å¸ƒå¼å­¦ä¹ æ¶æ„åœ¨Machine Learning as a Service (MLaaS)ç¯å¢ƒä¸‹çš„è„†å¼±æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨NVFlareå¹³å°ä¸Šé’ˆå¯¹å¤šç§æ·±åº¦å­¦ä¹ æ¶æ„å’Œå›¾åƒæ•°æ®é›†å®æ–½äº†æ”»å‡»ï¼Œå¹¶åˆ©ç”¨å‡†ç¡®ç‡ã€ä¿çœŸåº¦å’ŒKL divergenceç­‰æŒ‡æ ‡è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæå–æ¨¡å‹çš„å‡†ç¡®ç‡ä¸ä¿çœŸåº¦ä¸æ”»å‡»æŸ¥è¯¢é›†(Query Set)çš„å¤§å°æ˜¾è‘—ç›¸å…³ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºè¿ç§»å­¦ä¹ (Transfer Learning)çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºæå–è¿‡ç¨‹çš„èµ·ç‚¹ã€‚ç»“æœè¯å®ï¼Œç»è¿‡å¾®è°ƒçš„é¢„è®­ç»ƒæå–æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶åœ¨å°è§„æ¨¡æŸ¥è¯¢é›†ä¸‹å±•ç°å‡ºæ›´é«˜çš„å‡†ç¡®ç‡å’Œä¿çœŸåº¦ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†æ”»å‡»è€…åˆ©ç”¨è¿ç§»å­¦ä¹ æå‡æ¨¡å‹çªƒå–æ•ˆç‡çš„æ½œåœ¨å¨èƒï¼Œä¸ºè”é‚¦å­¦ä¹ çš„å®‰å…¨é˜²å¾¡æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at IEEE IWCMC. 6 pages, 4 Figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.23791v1",
      "published_date": "2025-05-25 22:40:10 UTC",
      "updated_date": "2025-05-25 22:40:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:55:50.784353+00:00"
    },
    {
      "arxiv_id": "2505.23790v1",
      "title": "Rethinking the Understanding Ability across LLMs through Mutual Information",
      "title_zh": "åŸºäºäº’ä¿¡æ¯é‡æ–°å®¡è§† LLMs çš„ç†è§£èƒ½åŠ›",
      "authors": [
        "Shaojie Wang",
        "Sirui Ding",
        "Na Zou"
      ],
      "abstract": "Recent advances in large language models (LLMs) have revolutionized natural language processing, yet evaluating their intrinsic linguistic understanding remains challenging. Moving beyond specialized evaluation tasks, we propose an information-theoretic framework grounded in mutual information (MI) to achieve this. We formalize the understanding as MI between an input sentence and its latent representation (sentence-level MI), measuring how effectively input information is preserved in latent representation. Given that LLMs learn embeddings for individual tokens, we decompose sentence-level MI into token-level MI between tokens and sentence embeddings, establishing theoretical bounds connecting these measures. Based on this foundation, we theoretically derive a computable lower bound for token-level MI using Fano's inequality, which directly relates to token-level recoverability-the ability to predict original tokens from sentence embedding. We implement this recoverability task to comparatively measure MI across different LLMs, revealing that encoder-only models consistently maintain higher information fidelity than their decoder-only counterparts, with the latter exhibiting a distinctive late-layer \"forgetting\" pattern where mutual information is first enhanced and then discarded. Moreover, fine-tuning to maximize token-level recoverability consistently improves understanding ability of LLMs on tasks without task-specific supervision, demonstrating that mutual information can serve as a foundation for understanding and improving language model capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºäº’ä¿¡æ¯(Mutual Information, MI)çš„ä¿¡æ¯è®ºæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å†…åœ¨è¯­è¨€ç†è§£èƒ½åŠ›ã€‚ä½œè€…å°†ç†è§£å®šä¹‰ä¸ºè¾“å…¥å¥å­ä¸å…¶æ½œè¡¨å¾ä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨Fanoä¸ç­‰å¼æ¨å¯¼å‡ºæ ‡è®°çº§åˆ«çš„å¯æ¢å¤æ€§(token-level recoverability)ä½œä¸ºäº’ä¿¡æ¯çš„å¯è®¡ç®—ä¸‹ç•Œã€‚é€šè¿‡å¯¹æ¯”ä¸åŒæ¶æ„çš„æ¨¡å‹ï¼Œç ”ç©¶å‘ç°ä»…ç¼–ç å™¨(encoder-only)æ¨¡å‹çš„ä¿¡æ¯ä¿çœŸåº¦å§‹ç»ˆé«˜äºä»…è§£ç å™¨(decoder-only)æ¨¡å‹ï¼Œä¸”åè€…åœ¨æ·±å±‚è¡¨ç°å‡ºå…ˆå¢å¼ºåä¸¢å¼ƒä¿¡æ¯çš„â€œé—å¿˜â€æ¨¡å¼ã€‚æ­¤å¤–ï¼Œå®éªŒè¯æ˜é€šè¿‡æœ€å¤§åŒ–æ ‡è®°çº§åˆ«å¯æ¢å¤æ€§è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹åœ¨æ— ç‰¹å®šç›‘ç£ä»»åŠ¡ä¸‹çš„ç†è§£èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£å’Œä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›æä¾›äº†ç†è®ºæ”¯æ’‘å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23790v1",
      "published_date": "2025-05-25 22:31:24 UTC",
      "updated_date": "2025-05-25 22:31:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:55:53.762654+00:00"
    },
    {
      "arxiv_id": "2505.19347v3",
      "title": "PatentMind: A Multi-Aspect Reasoning Graph for Patent Similarity Evaluation",
      "title_zh": "PatentMindï¼šé¢å‘ä¸“åˆ©ç›¸ä¼¼åº¦è¯„ä¼°çš„å¤šç»´æ¨ç†å›¾",
      "authors": [
        "Yongmin Yoo",
        "Qiongkai Xu",
        "Longbing Cao"
      ],
      "abstract": "Patent similarity evaluation plays a critical role in intellectual property analysis. However, existing methods often overlook the intricate structure of patent documents, which integrate technical specifications, legal boundaries, and application contexts. We introduce PatentMind, a novel framework for patent similarity assessment based on a Multi-Aspect Reasoning Graph (MARG). PatentMind decomposes patents into their three dimensions of technical features, application domains, and claim scopes, then dimension-specific similarity scores are calculated over the MARG. These scores are dynamically weighted through a context-aware reasoning process, which integrates contextual signals to emulate expert-level judgment. To support evaluation, we construct a human-annotated benchmark PatentSimBench, comprising 500 patent pairs. Experimental results demonstrate that the PatentMind-generated scores show a strong correlation ($r=0.938$) with expert annotations, significantly outperforming embedding-based models, patent-specific models, and advanced prompt engineering methods. Beyond computational linguistics, our framework provides a structured and semantically grounded foundation for real-world decision-making, particularly for tasks such as infringement risk assessment, underscoring its broader impact on both patent analytics and evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PatentMindï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šç»´åº¦æ¨ç†å›¾ (Multi-Aspect Reasoning Graph, MARG) çš„ä¸“åˆ©ç›¸ä¼¼æ€§è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰ä¸“åˆ©åœ¨æŠ€æœ¯ã€æ³•å¾‹å’Œåº”ç”¨ç»´åº¦å¤æ‚ç»“æ„çš„é—®é¢˜ã€‚PatentMind å°†ä¸“åˆ©åˆ†è§£ä¸ºæŠ€æœ¯ç‰¹å¾ (technical features)ã€åº”ç”¨é¢†åŸŸ (application domains) å’Œæƒåˆ©è¦æ±‚èŒƒå›´ (claim scopes) ä¸‰ä¸ªç»´åº¦ï¼Œå¹¶åœ¨ MARG ä¸Šè®¡ç®—å„ç»´åº¦çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†è¿‡ç¨‹ (context-aware reasoning) å¯¹å¾—åˆ†è¿›è¡ŒåŠ¨æ€åŠ æƒï¼Œä»è€Œæœ‰æ•ˆæ¨¡æ‹Ÿä¸“å®¶çº§çš„åˆ¤æ–­ã€‚ä¸ºæ”¯æŒæ€§èƒ½è¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å« 500 å¯¹ä¸“åˆ©çš„äººå·¥æ ‡æ³¨åŸºå‡†æ•°æ®é›† PatentSimBenchã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPatentMind ç”Ÿæˆçš„å¾—åˆ†ä¸ä¸“å®¶æ ‡æ³¨çš„ç›¸å…³æ€§é«˜è¾¾ $r=0.938$ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºäºåµŒå…¥çš„æ¨¡å‹ (embedding-based models) åŠå…ˆè¿›çš„æç¤ºå·¥ç¨‹æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºä¾µæƒé£é™©è¯„ä¼° (infringement risk assessment) ç­‰ç°å®å†³ç­–ä»»åŠ¡æä¾›äº†ç»“æ„åŒ–ä¸”å…·å¤‡è¯­ä¹‰åŸºç¡€çš„æ”¯æ’‘ï¼Œå¯¹ä¸“åˆ©åˆ†æé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19347v3",
      "published_date": "2025-05-25 22:28:27 UTC",
      "updated_date": "2026-01-05 22:59:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:56:03.184132+00:00"
    },
    {
      "arxiv_id": "2505.19345v2",
      "title": "PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims",
      "title_zh": "PatentScoreï¼šå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä¸“åˆ©æƒåˆ©è¦æ±‚å¤šç»´åº¦è¯„ä¼°",
      "authors": [
        "Yongmin Yoo",
        "Qiongkai Xu",
        "Longbing Cao"
      ],
      "abstract": "High-stakes texts such as patent claims, medical records, and technical reports are structurally complex and demand a high degree of reliability and precision. While large language models (LLMs) have recently been applied to automate their generation in high-stakes domains, reliably evaluating such outputs remains a major challenge. Conventional natural language generation (NLG) metrics are effective for generic documents but fail to capture the structural and legal characteristics essential to evaluating complex high-stakes documents. To address this gap, we propose PatentScore, a multi-dimensional evaluation framework specifically designed for one of the most intricate and rigorous domains, patent claims. PatentScore integrates hierarchical decomposition of claim elements, validation patterns grounded in legal and technical standards, and scoring across structural, semantic, and legal dimensions. In experiments on our dataset which consists of 400 Claim1, PatentScore achieved the highest correlation with expert annotations ($r = 0.819$), significantly outperforming widely used NLG metrics. This work establishes a new standard for evaluating LLM-generated patent claims, providing a solid foundation for research on patent generation and validation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”Ÿæˆä¸“åˆ©æƒåˆ©è¦æ±‚ç­‰é«˜é£é™©æ–‡æœ¬æ—¶ï¼Œä¼ ç»Ÿè‡ªç„¶è¯­è¨€ç”Ÿæˆ(NLG)æŒ‡æ ‡éš¾ä»¥æ•æ‰ç»“æ„ä¸æ³•å¾‹ç‰¹æ€§çš„å±€é™ï¼Œæå‡ºäº†å¤šç»´åº¦è¯„ä¼°æ¡†æ¶PatentScoreã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æƒåˆ©è¦æ±‚è¦ç´ è¿›è¡Œå±‚çº§åˆ†è§£(hierarchical decomposition)ï¼Œå¹¶ç»“åˆåŸºäºæ³•å¾‹ä¸æŠ€æœ¯æ ‡å‡†çš„éªŒè¯æ¨¡å¼ï¼Œä»ç»“æ„(structural)ã€è¯­ä¹‰(semantic)å’Œæ³•å¾‹(legal)ä¸‰ä¸ªç»´åº¦è¿›è¡Œç»¼åˆè¯„åˆ†ã€‚åœ¨åŒ…å«400æ¡æƒåˆ©è¦æ±‚(Claim1)çš„æ•°æ®é›†å®éªŒä¸­ï¼ŒPatentScoreä¸ä¸“å®¶æ ‡æ³¨çš„ç›¸å…³æ€§è¾¾åˆ°0.819ï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„é€šç”¨è¯„ä¼°æŒ‡æ ‡ã€‚è¿™é¡¹å·¥ä½œä¸ºLLMç”Ÿæˆä¸“åˆ©çš„è¯„ä¼°å»ºç«‹äº†æ–°æ ‡å‡†ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„è‡ªåŠ¨åŒ–ç”Ÿæˆä¸éªŒè¯ç ”ç©¶æä¾›äº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19345v2",
      "published_date": "2025-05-25 22:20:11 UTC",
      "updated_date": "2025-09-16 06:50:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:56:27.923913+00:00"
    },
    {
      "arxiv_id": "2505.19342v1",
      "title": "Communication-Efficient Multi-Device Inference Acceleration for Transformer Models",
      "title_zh": "é¢å‘ Transformer æ¨¡å‹çš„é€šä¿¡é«˜æ•ˆå¤šè®¾å¤‡æ¨ç†åŠ é€Ÿ",
      "authors": [
        "Xiao Liu",
        "Lijun Zhang",
        "Deepak Ganesan",
        "Hui Guan"
      ],
      "abstract": "Transformer models power many AI applications but suffer from high inference latency, limiting their use in real-time settings. Multi-device inference can reduce latency by parallelizing computation. Yet, existing methods require high inter-device bandwidth, making them impractical for bandwidth-constrained environments. We propose ASTRA, a communication-efficient framework that accelerates Transformer inference through a novel integration of sequence parallelism and a Mixed-Precision Attention mechanism designed to minimize inter-device communication. ASTRA compresses non-local token embeddings via vector quantization and preserves task accuracy through two optimizations, Noise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT and GPT2 across vision and NLP tasks show that ASTRA achieves up to 2.64X speedups over single-device inference and up to 15.25X speedups over state-of-the-art multi-device inferences, while operating under bandwidths as low as 10 Mbps. ASTRA is open-sourced at https://github.com/xl1990/Astra.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ASTRAï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨åŠ é€ŸTransformeræ¨¡å‹æ¨ç†çš„é«˜æ•ˆé€šä¿¡æ¡†æ¶ï¼Œä¸»è¦è§£å†³å¸¦å®½å—é™ç¯å¢ƒä¸‹å¤šè®¾å¤‡æ¨ç†æ‰€é¢ä¸´çš„é€šä¿¡ç“¶é¢ˆã€‚ASTRAé€šè¿‡åˆ›æ–°åœ°æ•´åˆSequence Parallelismå’Œæ··åˆç²¾åº¦æ³¨æ„åŠ›æœºåˆ¶(Mixed-Precision Attention)ï¼Œæ˜¾è‘—é™ä½äº†è®¾å¤‡é—´çš„é€šä¿¡å¼€é”€ã€‚è¯¥æ¡†æ¶åˆ©ç”¨çŸ¢é‡é‡åŒ–(Vector Quantization)æŠ€æœ¯å‹ç¼©éæœ¬åœ°TokenåµŒå…¥ï¼Œå¹¶é€šè¿‡å™ªå£°å¢å¼ºé‡åŒ–(Noise-Augmented Quantization)å’Œåˆ†å¸ƒå¼ç±»Token(Distributed Class Tokens)ä¸¤é¡¹å…³é”®ä¼˜åŒ–ç¡®ä¿ä»»åŠ¡å‡†ç¡®æ€§ã€‚é’ˆå¯¹ViTå’ŒGPT2ç­‰æ¨¡å‹çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä½è‡³10 Mbpsçš„å¸¦å®½ç¯å¢ƒä¸‹ï¼ŒASTRAç›¸æ¯”å•è®¾å¤‡æ¨ç†å¯å®ç°2.64å€çš„åŠ é€Ÿï¼Œç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›çš„å¤šè®¾å¤‡æ¨ç†æ–¹æ³•æå‡é«˜è¾¾15.25å€ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨å¸¦å®½å—é™çš„å®æ—¶åœºæ™¯ä¸­é«˜æ•ˆéƒ¨ç½²å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19342v1",
      "published_date": "2025-05-25 22:16:59 UTC",
      "updated_date": "2025-05-25 22:16:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:56:01.863663+00:00"
    },
    {
      "arxiv_id": "2505.19339v1",
      "title": "Towards Humanoid Robot Autonomy: A Dynamic Architecture Integrating Continuous thought Machines (CTM) and Model Context Protocol (MCP)",
      "title_zh": "è¿ˆå‘äººå½¢æœºå™¨äººè‡ªä¸»ï¼šä¸€ç§é›†æˆè¿ç»­æ€ç»´æœºå™¨ï¼ˆCTMï¼‰ä¸æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰çš„åŠ¨æ€æ¶æ„",
      "authors": [
        "Libo Wang"
      ],
      "abstract": "To address the gaps between the static pre-set \"thinking-planning-action\" of humanoid robots in unfamiliar scenarios and the highly programmed \"call tool-return result\" due to the lack of autonomous coding capabilities, this work designs a dynamic architecture connecting continuous thought machines (CTM) and model context protocol (MCP). It proposes a theoretical parallel solution through tick-slab and uses rank compression to achieve parameter suppression to provide a solution for achieving autonomous actions due to autonomous coding. The researcher used a simulation-based experiment using OpenAI's o4-mini-high as a tool to build the experimental environment, and introduced the extended SayCan dataset to conduct nine epochs of experiments. The experimental results show that the CTM-MCP architecture is feasible and effective through the data results of seven metrics: task success rate (TSR), execution success rate (ESR), average episode length (AEL), ROSCOE, REVEAL, proficiency self-assessment (PSA), task effectiveness (TE). In practice, it provides a reference experience for exploring the autonomous dynamic coding of humanoid robots based on continuous thinking to achieve human-like autonomous actions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå½¢æœºå™¨äººåœ¨é™Œç”Ÿåœºæ™¯ä¸‹é™æ€é¢„è®¾è·¯å¾„åŠç¼ºä¹è‡ªä¸»ç¼–ç èƒ½åŠ›çš„é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ç§æ•´åˆæŒç»­æ€ç»´æœºå™¨(Continuous thought Machines, CTM)ä¸æ¨¡å‹ä¸Šä¸‹æ–‡åè®®(Model Context Protocol, MCP)çš„åŠ¨æ€æ¶æ„ã€‚è¯¥æ–¹æ¡ˆæå‡ºäº†åŸºäºtick-slabçš„ç†è®ºå¹¶è¡Œè§£æ³•ï¼Œå¹¶åˆ©ç”¨ç§©å‹ç¼©(rank compression)æŠ€æœ¯å®ç°å‚æ•°æŠ‘åˆ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªä¸»ç¼–ç é©±åŠ¨æœºå™¨äººçš„è‡ªä¸»è¡ŒåŠ¨ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ä»¿çœŸç¯å¢ƒä¸­åˆ©ç”¨OpenAIçš„o4-mini-highå·¥å…·å’Œæ‰©å±•çš„SayCanæ•°æ®é›†è¿›è¡Œäº†ä¹è½®å®éªŒï¼Œé€šè¿‡ä»»åŠ¡æˆåŠŸç‡(TSR)å’Œæ‰§è¡ŒæˆåŠŸç‡(ESR)ç­‰ä¸ƒé¡¹å…³é”®æŒ‡æ ‡éªŒè¯äº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¯æ˜äº†CTM-MCPæ¶æ„åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„å¯è¡Œæ€§ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººçš„ä»»åŠ¡æœ‰æ•ˆæ€§(TE)ä¸ç†Ÿç»ƒåº¦è‡ªè¯„ä¼°(PSA)ã€‚è¯¥æˆæœä¸ºäººå½¢æœºå™¨äººå®ç°ç±»äººåŒ–çš„è‡ªä¸»åŠ¨æ€ç¼–ç ä¸å†³ç­–æä¾›äº†é‡è¦çš„ç†è®ºå‚è€ƒä¸å®è·µç»éªŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "The relevant architecture code and some experimental records have been uploaded to the GitHub repository for sharing: https://github.com/brucewang123456789/GeniusTrail/tree/main/CTM-MCP",
      "pdf_url": "https://arxiv.org/pdf/2505.19339v1",
      "published_date": "2025-05-25 22:12:35 UTC",
      "updated_date": "2025-05-25 22:12:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:56:56.438739+00:00"
    },
    {
      "arxiv_id": "2505.19337v2",
      "title": "Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies",
      "title_zh": "é¢å‘é›¶æ ·æœ¬åˆ°è¾¾-è§„é¿ç­–ç•¥çš„æç¤ºå¼å†³ç­– Transformer",
      "authors": [
        "Kevin Li",
        "Marinka Zitnik"
      ],
      "abstract": "Offline goal-conditioned reinforcement learning methods have shown promise for reach-avoid tasks, where an agent must reach a target state while avoiding undesirable regions of the state space. Existing approaches typically encode avoid-region information into an augmented state space and cost function, which prevents flexible, dynamic specification of novel avoid-region information at evaluation time. They also rely heavily on well-designed reward and cost functions, limiting scalability to complex or poorly structured environments. We introduce RADT, a decision transformer model for offline, reward-free, goal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid regions directly as prompt tokens, allowing any number of avoid regions of arbitrary size to be specified at evaluation time. Using only suboptimal offline trajectories from a random policy, RADT learns reach-avoid behavior through a novel combination of goal and avoid-region hindsight relabeling. We benchmark RADT against 3 existing offline goal-conditioned RL models across 11 tasks, environments, and experimental settings. RADT generalizes in a zero-shot manner to out-of-distribution avoid region sizes and counts, outperforming baselines that require retraining. In one such zero-shot setting, RADT achieves 35.7% improvement in normalized cost over the best retrained baseline while maintaining high goal-reaching success. We apply RADT to cell reprogramming in biology, where it reduces visits to undesirable intermediate gene expression states during trajectories to desired target states, despite stochastic transitions and discrete, structured state dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ (Offline goal-conditioned RL)åœ¨åˆ°è¾¾-è§„é¿(reach-avoid)ä»»åŠ¡ä¸­æ— æ³•çµæ´»ã€åŠ¨æ€åœ°æŒ‡å®šè§„é¿åŒºåŸŸä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†RADTæ¨¡å‹ã€‚RADT æ˜¯ä¸€ç§åŸºäºå†³ç­–å˜æ¢å™¨(Decision Transformer)çš„æ¶æ„ï¼Œå®ƒå°†ç›®æ ‡å’Œè§„é¿åŒºåŸŸç›´æ¥ç¼–ç ä¸ºæç¤ºè¯(prompt tokens)ï¼Œä»è€Œå…è®¸åœ¨è¯„ä¼°é˜¶æ®µå®æ—¶æŒ‡å®šä»»æ„æ•°é‡å’Œå¤§å°çš„è§„é¿åŒºåŸŸã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆç›®æ ‡å’Œè§„é¿åŒºåŸŸåéªŒé‡æ ‡è®°(hindsight relabeling)çš„æ–°é¢–æŠ€æœ¯ï¼Œèƒ½å¤Ÿä»…åˆ©ç”¨éšæœºç­–ç•¥ç”Ÿæˆçš„æ¬¡ä¼˜ç¦»çº¿è½¨è¿¹å­¦ä¹ åˆ°æœ‰æ•ˆçš„ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRADT åœ¨11é¡¹åŸºå‡†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿä»¥é›¶æ ·æœ¬(zero-shot)æ–¹å¼æ³›åŒ–è‡³åˆ†å¸ƒå¤–çš„è§„é¿åŒºåŸŸè®¾ç½®ï¼Œä¸”åœ¨ç‰¹å®šè®¾å®šä¸‹æ¯”éœ€è¦é‡è®­ç»ƒçš„åŸºçº¿æ¨¡å‹é™ä½äº†35.7%çš„å½’ä¸€åŒ–æˆæœ¬ã€‚æœ€åï¼Œè¯¥ç ”ç©¶æˆåŠŸå°† RADT åº”ç”¨äºç”Ÿç‰©å­¦ä¸­çš„ç»†èƒé‡ç¼–ç¨‹ä»»åŠ¡ï¼Œåœ¨å­˜åœ¨éšæœºè½¬æ¢å’Œç¦»æ•£ç»“æ„åŒ–åŠ¨åŠ›å­¦çš„ç¯å¢ƒä¸‹ï¼Œæœ‰æ•ˆå‡å°‘äº†åœ¨è¶‹å‘ç›®æ ‡çŠ¶æ€è¿‡ç¨‹ä¸­å¯¹ä¸ç†æƒ³ä¸­é—´åŸºå› è¡¨è¾¾çŠ¶æ€çš„è®¿é—®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19337v2",
      "published_date": "2025-05-25 22:00:38 UTC",
      "updated_date": "2025-05-27 02:56:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:11.862004+00:00"
    },
    {
      "arxiv_id": "2505.21551v1",
      "title": "WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper",
      "title_zh": "WhisperDï¼šåŸºäº Whisper çš„ç—´å‘†ç—‡è¯­éŸ³è¯†åˆ«ä¸å¡«å……è¯æ£€æµ‹",
      "authors": [
        "Emmanuel Akinrintoyo",
        "Nadine Abdelhalim",
        "Nicole Salomons"
      ],
      "abstract": "Whisper fails to correctly transcribe dementia speech because persons with dementia (PwDs) often exhibit irregular speech patterns and disfluencies such as pauses, repetitions, and fragmented sentences. It was trained on standard speech and may have had little or no exposure to dementia-affected speech. However, correct transcription is vital for dementia speech for cost-effective diagnosis and the development of assistive technology. In this work, we fine-tune Whisper with the open-source dementia speech dataset (DementiaBank) and our in-house dataset to improve its word error rate (WER). The fine-tuning also includes filler words to ascertain the filler inclusion rate (FIR) and F1 score. The fine-tuned models significantly outperformed the off-the-shelf models. The medium-sized model achieved a WER of 0.24, outperforming previous work. Similarly, there was a notable generalisability to unseen data and speech patterns.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹ Whisper åœ¨å¤„ç†è®¤çŸ¥éšœç¢ç—‡(dementia)æ‚£è€…è¯­éŸ³æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº† WhisperD æ¨¡å‹ã€‚ç”±äºæ‚£è€…é€šå¸¸å­˜åœ¨ä¸è§„åˆ™çš„è¯­éŸ³æ¨¡å¼å’Œåœé¡¿ã€é‡å¤åŠç‰‡æ®µåŒ–å¥å­ç­‰ä¸æµåˆ©è¡¨è¾¾ï¼Œæ ‡å‡†çš„ Whisper æ¨¡å‹éš¾ä»¥å‡†ç¡®è½¬å½•ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨å¼€æºçš„ DementiaBank æ•°æ®é›†å’Œå†…éƒ¨æ•°æ®é›†å¯¹ Whisper è¿›è¡Œäº†å¾®è°ƒ(fine-tune)ï¼Œæ—¨åœ¨é™ä½å­—é”™ç‡(WER)å¹¶æé«˜å¡«å……è¯(filler words)çš„æ£€æµ‹ç²¾åº¦ã€‚å¾®è°ƒè¿‡ç¨‹è¿˜åŒ…æ‹¬å¯¹å¡«å……è¯åŒ…å«ç‡(FIR)å’Œ F1 åˆ†æ•°çš„è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…¶ä¸­ä¸­å‹æ¨¡å‹è¾¾åˆ°äº† 0.24 çš„ WERï¼Œè¶…è¶Šäº†å‰äººçš„ç ”ç©¶æˆæœã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†æœªè§æ•°æ®å’Œæ–°çš„è¯­éŸ³æ¨¡å¼æ—¶å±•ç°å‡ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›(generalisability)ï¼Œä¸ºè®¤çŸ¥éšœç¢çš„æˆæœ¬æ•ˆç›Šè¯Šæ–­å’Œè¾…åŠ©æŠ€æœ¯å¼€å‘æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Submitted to Interspeech 2025 (Accepted)",
      "pdf_url": "https://arxiv.org/pdf/2505.21551v1",
      "published_date": "2025-05-25 21:48:03 UTC",
      "updated_date": "2025-05-25 21:48:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:02.649274+00:00"
    },
    {
      "arxiv_id": "2505.19333v1",
      "title": "Evaluating Steering Techniques using Human Similarity Judgments",
      "title_zh": "åŸºäºäººç±»ç›¸ä¼¼æ€§åˆ¤æ–­çš„è½¬å‘æŠ€æœ¯è¯„ä¼°",
      "authors": [
        "Zach Studdiford",
        "Timothy T. Rogers",
        "Siddharth Suresh",
        "Kushin Mukherjee"
      ],
      "abstract": "Current evaluations of Large Language Model (LLM) steering techniques focus on task-specific performance, overlooking how well steered representations align with human cognition. Using a well-established triadic similarity judgment task, we assessed steered LLMs on their ability to flexibly judge similarity between concepts based on size or kind. We found that prompt-based steering methods outperformed other methods both in terms of steering accuracy and model-to-human alignment. We also found LLMs were biased towards 'kind' similarity and struggled with 'size' alignment. This evaluation approach, grounded in human cognition, adds further support to the efficacy of prompt-based steering and reveals privileged representational axes in LLMs prior to steering.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨äººç±»ç›¸ä¼¼æ€§åˆ¤æ–­ä»»åŠ¡è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è½¬å‘æŠ€æœ¯ï¼ˆSteering Techniquesï¼‰ï¼Œæ—¨åœ¨æ¢è®¨è½¬å‘åçš„è¡¨å¾ä¸äººç±»è®¤çŸ¥çš„å¯¹é½ç¨‹åº¦ã€‚é€šè¿‡ä¸‰å…ƒç»„ç›¸ä¼¼æ€§åˆ¤æ–­ä»»åŠ¡ï¼Œç ”ç©¶è€…æµ‹è¯•äº†æ¨¡å‹æ ¹æ®â€œå°ºå¯¸â€æˆ–â€œç±»åˆ«â€çµæ´»åˆ¤æ–­æ¦‚å¿µç›¸ä¼¼æ€§çš„èƒ½åŠ›ã€‚å®éªŒå‘ç°ï¼ŒåŸºäºæç¤ºçš„è½¬å‘æ–¹æ³•ï¼ˆPrompt-based Steeringï¼‰åœ¨è½¬å‘å‡†ç¡®æ€§å’Œæ¨¡å‹ä¸äººç±»çš„å¯¹é½åº¦ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº† LLM æ™®éå­˜åœ¨åå‘â€œç±»åˆ«â€ç›¸ä¼¼æ€§çš„åè§ï¼Œä¸”åœ¨â€œå°ºå¯¸â€ç»´åº¦çš„å¯¹é½ä¸Šè¡¨ç°è¾ƒå·®ã€‚è¿™ç§åŸºäºäººç±»è®¤çŸ¥çš„è¯„ä¼°æ–¹æ³•ä¸ä»…è¿›ä¸€æ­¥è¯æ˜äº†æç¤ºè½¬å‘çš„æœ‰æ•ˆæ€§ï¼Œè¿˜æ­ç¤ºäº†æ¨¡å‹åœ¨è½¬å‘å‰å­˜åœ¨çš„ç‰¹æƒè¡¨å¾è½´ï¼ˆPrivileged Representational Axesï¼‰ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19333v1",
      "published_date": "2025-05-25 21:40:26 UTC",
      "updated_date": "2025-05-25 21:40:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:56:57.878239+00:00"
    },
    {
      "arxiv_id": "2505.19317v4",
      "title": "Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics",
      "title_zh": "åŠªåŠ›æ„ŸçŸ¥å…¬å¹³ï¼šå°†å—å“²å­¦å¯å‘ä¸”ä»¥äººä¸ºä¸­å¿ƒçš„â€œåŠªåŠ›â€è§‚å¿µèå…¥ç®—æ³•å…¬å¹³æ€§æŒ‡æ ‡",
      "authors": [
        "Tin Trung Nguyen",
        "Jiannan Xu",
        "Zora Che",
        "Phuong-Anh Nguyen-Le",
        "Rushil Dandamudi",
        "Donald Braman",
        "Furong Huang",
        "Hal DaumÃ©",
        "Zubin Jelveh"
      ],
      "abstract": "Although popularized AI fairness metrics, e.g., demographic parity, have uncovered bias in AI-assisted decision-making outcomes, they do not consider how much effort one has spent to get to where one is today in the input feature space. However, the notion of effort is important in how Philosophy and humans understand fairness. We propose a philosophy-informed approach to conceptualize and evaluate Effort-aware Fairness (EaF), grounded in the concept of Force, which represents the temporal trajectory of predictive features coupled with inertia. Besides theoretical formulation, our empirical contributions include: (1) a pre-registered human subjects experiment, which shows that for both stages of the (individual) fairness evaluation process, people consider the temporal trajectory of a predictive feature more than its aggregate value; (2) pipelines to compute Effort-aware Individual/Group Fairness in the criminal justice and personal finance contexts. Our work may enable AI model auditors to uncover and potentially correct unfair decisions against individuals who have spent significant efforts to improve but are still stuck with systemic disadvantages outside their control.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Effort-aware Fairness (EaF)ï¼Œè¿™æ˜¯ä¸€ç§å—å“²å­¦å¯å‘ä¸”ä»¥äººä¸ºä¸­å¿ƒçš„æ–°å‹ç®—æ³•å…¬å¹³æ€§æŒ‡æ ‡ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸæŒ‡æ ‡ï¼ˆå¦‚ demographic parityï¼‰å¿½ç•¥ä¸ªä½“åœ¨è¾“å…¥ç‰¹å¾ç©ºé—´ä¸­ä»˜å‡ºçš„åŠªåŠ›è¿™ä¸€é—®é¢˜ã€‚ç ”ç©¶åŸºäº Force çš„æ¦‚å¿µï¼Œå°†é¢„æµ‹ç‰¹å¾çš„æ—¶é—´è½¨è¿¹ä¸æƒ¯æ€§ç»“åˆï¼Œå¯¹åŠªåŠ›è¿›è¡Œäº†ç†è®ºå…¬å¼åŒ–ã€‚é€šè¿‡ä¸€é¡¹é¢„æ³¨å†Œçš„äººä½“å—è¯•è€…å®éªŒï¼Œç ”ç©¶å‘ç°äººç±»åœ¨è¯„ä¼°å…¬å¹³æ€§æ—¶ï¼Œç›¸æ¯”äºç‰¹å¾çš„èšåˆå€¼ï¼ˆaggregate valueï¼‰ï¼Œæ›´çœ‹é‡ç‰¹å¾éšæ—¶é—´çš„å˜åŒ–è½¨è¿¹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼€å‘äº†åœ¨åˆ‘äº‹å¸æ³•å’Œä¸ªäººç†è´¢èƒŒæ™¯ä¸‹è®¡ç®— Effort-aware Individual/Group Fairness çš„æµæ°´çº¿ã€‚è¯¥å·¥ä½œä¸º AI æ¨¡å‹å®¡è®¡å‘˜æä¾›äº†ä¸€ç§æ–°å·¥å…·ï¼Œèƒ½å¤Ÿè¯†åˆ«å¹¶çº æ­£é‚£äº›è™½ç„¶ä»˜å‡ºäº†å·¨å¤§åŠªåŠ›ä½†ä»å—é™äºä¸å¯æ§ç³»ç»Ÿæ€§åŠ£åŠ¿çš„ä¸ªä½“æ‰€é¢ä¸´çš„ä¸å…¬å¹³å†³ç­–ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "AIES 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.19317v4",
      "published_date": "2025-05-25 21:07:13 UTC",
      "updated_date": "2025-09-11 12:10:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:09.832556+00:00"
    },
    {
      "arxiv_id": "2505.19315v1",
      "title": "Demand Selection for VRP with Emission Quota",
      "title_zh": "å¸¦æ’æ”¾é…é¢çº¦æŸçš„è½¦è¾†è·¯å¾„é—®é¢˜çš„éœ€æ±‚é€‰æ‹©",
      "authors": [
        "Farid Najar",
        "Dominique Barth",
        "Yann Strozecki"
      ],
      "abstract": "Combinatorial optimization (CO) problems are traditionally addressed using Operations Research (OR) methods, including metaheuristics. In this study, we introduce a demand selection problem for the Vehicle Routing Problem (VRP) with an emission quota, referred to as QVRP. The objective is to minimize the number of omitted deliveries while respecting the pollution quota. We focus on the demand selection part, called Maximum Feasible Vehicle Assignment (MFVA), while the construction of a routing for the VRP instance is solved using classical OR methods. We propose several methods for selecting the packages to omit, both from machine learning (ML) and OR. Our results show that, in this static problem setting, classical OR-based methods consistently outperform ML-based approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¸¦æ’æ”¾é…é¢çš„è½¦è¾†è·¯å¾„é—®é¢˜ï¼ˆVehicle Routing Problem with Emission Quota, QVRPï¼‰ï¼Œæ—¨åœ¨ç¯å¢ƒæ±¡æŸ“é…é¢é™åˆ¶ä¸‹æœ€å°åŒ–æœªé€è¾¾è´§ç‰©çš„æ•°é‡ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨å…¶ä¸­çš„éœ€æ±‚é€‰æ‹©éƒ¨åˆ†ï¼Œå³æœ€å¤§å¯è¡Œè½¦è¾†åˆ†é…ï¼ˆMaximum Feasible Vehicle Assignment, MFVAï¼‰é—®é¢˜ï¼Œè€Œ VRP å®ä¾‹çš„è·¯å¾„æ„å»ºåˆ™æ²¿ç”¨ä¼ ç»Ÿçš„è¿ç­¹å­¦ï¼ˆOperations Research, ORï¼‰æ–¹æ³•ã€‚ä½œè€…é’ˆå¯¹åŒ…è£¹é€‰æ‹©ç¯èŠ‚æå‡ºäº†å¤šç§åŸºäºæœºå™¨å­¦ä¹ ï¼ˆMachine Learning, MLï¼‰å’Œè¿ç­¹å­¦çš„æ±‚è§£æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é™æ€é—®é¢˜è®¾å®šä¸‹ï¼Œç»å…¸çš„åŸºäºè¿ç­¹å­¦çš„æ–¹æ³•åœ¨æ€§èƒ½è¡¨ç°ä¸Šå§‹ç»ˆä¼˜äºåŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºå—ç¢³æ’æ”¾é™åˆ¶çš„ç»„åˆä¼˜åŒ–ï¼ˆCombinatorial optimizationï¼‰é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„æ–¹æ³•è®ºå‚è€ƒï¼Œå¹¶éªŒè¯äº†ä¼ ç»Ÿç®—æ³•åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„ç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19315v1",
      "published_date": "2025-05-25 21:04:38 UTC",
      "updated_date": "2025-05-25 21:04:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:11.135309+00:00"
    },
    {
      "arxiv_id": "2505.19314v3",
      "title": "SoloSpeech: Enhancing Intelligibility and Quality in Target Speech Extraction through a Cascaded Generative Pipeline",
      "title_zh": "SoloSpeechï¼šé€šè¿‡çº§è”ç”Ÿæˆå¼æµæ°´çº¿æå‡ç›®æ ‡è¯­éŸ³æå–çš„å¯æ‡‚åº¦ä¸è´¨é‡",
      "authors": [
        "Helin Wang",
        "Jiarui Hai",
        "Dongchao Yang",
        "Chen Chen",
        "Kai Li",
        "Junyi Peng",
        "Thomas Thebaud",
        "Laureano Moro Velazquez",
        "Jesus Villalba",
        "Najim Dehak"
      ],
      "abstract": "Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio's latent space, aligning it with the mixture audio's latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction while demonstrating exceptional generalization on out-of-domain data and real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SoloSpeechï¼Œä¸€ç§æ—¨åœ¨æå‡ç›®æ ‡è¯­éŸ³æå– (Target Speech Extraction) æ¸…æ™°åº¦ä¸è´¨é‡çš„æ–°å‹çº§è”ç”Ÿæˆæµæ°´çº¿ (cascaded generative pipeline)ã€‚é’ˆå¯¹ç°æœ‰åˆ¤åˆ«å¼æ¨¡å‹æ˜“äº§ç”Ÿä¼ªå½±åŠç”Ÿæˆå¼æ¨¡å‹éŸ³è´¨æ¬ ä½³çš„é—®é¢˜ï¼ŒSoloSpeech é›†æˆäº†å‹ç¼©ã€æå–ã€é‡å»ºä¸æ ¡æ­£å››ä¸ªå…³é”®è¿‡ç¨‹ã€‚å…¶æ ¸å¿ƒç‰¹å¾åœ¨äºé‡‡ç”¨äº†ä¸€ç§ä¸ä¾èµ–è¯´è¯äººåµŒå…¥ (speaker-embedding-free) çš„æå–å™¨ï¼Œé€šè¿‡å°†å‚è€ƒéŸ³é¢‘ä¸æ··åˆéŸ³é¢‘åœ¨æ½œç©ºé—´è¿›è¡Œæ¡ä»¶å¯¹é½ï¼Œæœ‰æ•ˆé¿å…äº†ç‰¹å¾å¤±é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoloSpeech åœ¨ Libri2Mix æ•°æ®é›†ä¸Šè¾¾åˆ°äº†å½“å‰çš„å…ˆè¿›æ°´å¹³ (state-of-the-art)ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†åŸŸå¤–æ•°æ® (out-of-domain) åŠçœŸå®ä¸–ç•Œåœºæ™¯æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19314v3",
      "published_date": "2025-05-25 21:00:48 UTC",
      "updated_date": "2025-09-06 14:32:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:25.261437+00:00"
    },
    {
      "arxiv_id": "2505.19310v1",
      "title": "Retrieval-Augmented Generation for Service Discovery: Chunking Strategies and Benchmarking",
      "title_zh": "é¢å‘æœåŠ¡å‘ç°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼šåˆ†å—ç­–ç•¥ä¸åŸºå‡†æµ‹è¯•",
      "authors": [
        "Robin D. Pesl",
        "Jerin G. Mathew",
        "Massimo Mecella",
        "Marco Aiello"
      ],
      "abstract": "Integrating multiple (sub-)systems is essential to create advanced Information Systems. Difficulties mainly arise when integrating dynamic environments, e.g., the integration at design time of not yet existing services. This has been traditionally addressed using a registry that provides the API documentation of the endpoints. Large Language Models have shown to be capable of automatically creating system integrations (e.g., as service composition) based on this documentation but require concise input due to input oken limitations, especially regarding comprehensive API descriptions. Currently, it is unknown how best to preprocess these API descriptions. In the present work, we (i) analyze the usage of Retrieval Augmented Generation for endpoint discovery and the chunking, i.e., preprocessing, of state-of-practice OpenAPIs to reduce the input oken length while preserving the most relevant information. To further reduce the input token length for the composition prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that only receives a summary of the most relevant endpoints nd retrieves specification details on demand. We evaluate RAG for endpoint discovery using (iii) a proposed novel service discovery benchmark SOCBench-D representing a general setting across numerous domains and the real-world RestBench enchmark, first, for the different chunking possibilities and parameters measuring the endpoint retrieval accuracy. Then, we assess the Discovery Agent using the same test data set. The prototype shows how to successfully employ RAG for endpoint discovery to reduce the token count. Our experiments show that endpoint-based approaches outperform naive chunking methods for preprocessing. Relying on an agent significantly improves precision while being prone to decrease recall, disclosing the need for further reasoning capabilities.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹åœ¨é›†æˆåŠ¨æ€ç³»ç»Ÿç¯å¢ƒæ—¶ï¼ŒAPI æ–‡æ¡£è¿‡é•¿å¯¼è‡´å¤§è¯­è¨€æ¨¡å‹(LLMs)è¾“å…¥ Token å—é™çš„é—®é¢˜ï¼Œæ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ä¼˜åŒ–æœåŠ¡å‘ç°ã€‚ä½œè€…åˆ†æäº†é’ˆå¯¹ OpenAPI è§„èŒƒçš„ä¸åŒåˆ†å—(Chunking)ç­–ç•¥ï¼Œæ—¨åœ¨å‡å°‘è¾“å…¥é•¿åº¦çš„åŒæ—¶ä¿ç•™æ ¸å¿ƒç«¯ç‚¹ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å‘ç°æ™ºèƒ½ä½“(Discovery Agent)ï¼Œé€šè¿‡ä»…æ¥æ”¶ç›¸å…³ç«¯ç‚¹æ‘˜è¦å¹¶åœ¨éœ€è¦æ—¶æŒ‰éœ€æ£€ç´¢ç»†èŠ‚ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†æœåŠ¡ç»„åˆçš„ Prompt é•¿åº¦ã€‚ä¸ºäº†éªŒè¯æ•ˆæœï¼Œç ”ç©¶å¼•å…¥äº†å…¨æ–°çš„æœåŠ¡å‘ç°åŸºå‡†æµ‹è¯•é›† SOCBench-Dï¼Œå¹¶åœ¨ RestBench ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºç«¯ç‚¹(Endpoint-based)çš„é¢„å¤„ç†æ–¹æ³•åœ¨æ£€ç´¢å‡†ç¡®ç‡ä¸Šä¼˜äºä¼ ç»Ÿåˆ†å—ç­–ç•¥ã€‚æœ€åï¼Œè™½ç„¶ Discovery Agent æ˜¾è‘—æå‡äº†æ£€ç´¢çš„ç²¾ç¡®ç‡(Precision)ï¼Œä½†ä¹Ÿè¡¨ç°å‡ºå¬å›ç‡(Recall)ä¸‹é™çš„è¶‹åŠ¿ï¼Œå¼ºè°ƒäº†å¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2411.19804",
      "pdf_url": "https://arxiv.org/pdf/2505.19310v1",
      "published_date": "2025-05-25 20:49:39 UTC",
      "updated_date": "2025-05-25 20:49:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:19.205843+00:00"
    },
    {
      "arxiv_id": "2505.19301v2",
      "title": "A Novel Zero-Trust Identity Framework for Agentic AI: Decentralized Authentication and Fine-Grained Access Control",
      "title_zh": "é¢å‘æ™ºèƒ½ä½“ AI çš„æ–°å‹é›¶ä¿¡ä»»èº«ä»½æ¡†æ¶ï¼šå»ä¸­å¿ƒåŒ–è®¤è¯ä¸ç»†ç²’åº¦è®¿é—®æ§åˆ¶",
      "authors": [
        "Ken Huang",
        "Vineeth Sai Narajala",
        "John Yeoh",
        "Jason Ross",
        "Ramesh Raskar",
        "Youssef Harkati",
        "Jerry Huang",
        "Idan Habler",
        "Chris Hughes"
      ],
      "abstract": "Traditional Identity and Access Management (IAM) systems, primarily designed for human users or static machine identities via protocols such as OAuth, OpenID Connect (OIDC), and SAML, prove fundamentally inadequate for the dynamic, interdependent, and often ephemeral nature of AI agents operating at scale within Multi Agent Systems (MAS), a computational system composed of multiple interacting intelligent agents that work collectively.\n  This paper posits the imperative for a novel Agentic AI IAM framework: We deconstruct the limitations of existing protocols when applied to MAS, illustrating with concrete examples why their coarse-grained controls, single-entity focus, and lack of context-awareness falter. We then propose a comprehensive framework built upon rich, verifiable Agent Identities (IDs), leveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs), that encapsulate an agents capabilities, provenance, behavioral scope, and security posture.\n  Our framework includes an Agent Naming Service (ANS) for secure and capability-aware discovery, dynamic fine-grained access control mechanisms, and critically, a unified global session management and policy enforcement layer for real-time control and consistent revocation across heterogeneous agent communication protocols. We also explore how Zero-Knowledge Proofs (ZKPs) enable privacy-preserving attribute disclosure and verifiable policy compliance.\n  We outline the architecture, operational lifecycle, innovative contributions, and security considerations of this new IAM paradigm, aiming to establish the foundational trust, accountability, and security necessary for the burgeoning field of agentic AI and the complex ecosystems they will inhabit.",
      "tldr_zh": "é’ˆå¯¹ä¼ ç»Ÿèº«ä»½ä¸è®¿é—®ç®¡ç†(IAM)ç³»ç»Ÿåœ¨å¤„ç†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi Agent Systems)æ—¶å­˜åœ¨çš„åŠ¨æ€æ€§ä¸è¶³åŠæƒé™æ§åˆ¶è¿‡ç²—ç­‰å±€é™ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é¢å‘æ™ºèƒ½ä½“AI(Agentic AI)çš„æ–°å‹é›¶ä¿¡ä»»(Zero-Trust)èº«ä»½æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å»ä¸­å¿ƒåŒ–æ ‡è¯†ç¬¦(DIDs)å’Œå¯éªŒè¯å‡­è¯(VCs)æ„å»ºäº†ä¸°å¯Œçš„å¯éªŒè¯æ™ºèƒ½ä½“èº«ä»½ï¼Œæ¶µç›–äº†æ™ºèƒ½ä½“çš„èƒ½åŠ›ã€æº¯æºåŠè¡Œä¸ºèŒƒå›´ã€‚æ–¹æ¡ˆå¼•å…¥äº†æ™ºèƒ½ä½“å‘½åæœåŠ¡(Agent Naming Service)ä»¥å®ç°å®‰å…¨çš„èƒ½åŠ›æ„ŸçŸ¥å‘ç°ï¼Œå¹¶ç»“åˆäº†åŠ¨æ€ç»†ç²’åº¦è®¿é—®æ§åˆ¶æœºåˆ¶ã€‚ç³»ç»Ÿæ ¸å¿ƒåŒ…å«ç»Ÿä¸€çš„å…¨å±€ä¼šè¯ç®¡ç†å’Œç­–ç•¥æ‰§è¡Œå±‚ï¼Œç¡®ä¿åœ¨å¼‚æ„é€šä¿¡åè®®ä¸‹å®ç°å®æ—¶æ§åˆ¶å’Œä¸€è‡´çš„æƒé™æ’¤é”€ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ©ç”¨é›¶çŸ¥è¯†è¯æ˜(Zero-Knowledge Proofs)æŠ€æœ¯ï¼Œå®ç°äº†ä¿æŠ¤éšç§çš„å±æ€§æŠ«éœ²å’Œå¯éªŒè¯çš„ç­–ç•¥åˆè§„ã€‚è¯¥æ¶æ„é€šè¿‡å»ºç«‹å¿…è¦çš„ä¿¡ä»»ã€é—®è´£åˆ¶å’Œå®‰å…¨æœºåˆ¶ï¼Œä¸ºæ—¥ç›Šå¤æ‚çš„æ™ºèƒ½ä½“AIç”Ÿæ€ç³»ç»Ÿå¥ å®šäº†å…³é”®çš„åŸºç¡€è®¾æ–½æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CR",
      "comment": "24 Pages, 5 figures, 2 tables, edit: added a missed Author",
      "pdf_url": "https://arxiv.org/pdf/2505.19301v2",
      "published_date": "2025-05-25 20:21:55 UTC",
      "updated_date": "2025-05-28 17:37:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:26.948801+00:00"
    },
    {
      "arxiv_id": "2505.19299v2",
      "title": "A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations",
      "title_zh": "è¿ˆå‘å¿ å®æ€§çš„å¿…è¦ä¸€æ­¥ï¼šè‡ªç”±æ–‡æœ¬è§£é‡Šä¸€è‡´æ€§çš„è¯„ä¼°ä¸æå‡",
      "authors": [
        "Lingjun Zhao",
        "Hal DaumÃ©"
      ],
      "abstract": "Faithful free-text explanations are important to ensure transparency in high-stakes AI decision-making contexts, but they are challenging to generate by language models and assess by humans. In this paper, we present a measure for Prediction-EXplanation (PEX) consistency, by extending the concept of weight of evidence. This measure quantifies how much a free-text explanation supports or opposes a prediction, serving as an important aspect of explanation faithfulness. Our analysis reveals that more than 62% explanations generated by large language models lack this consistency. We show that applying direct preference optimization improves the consistency of generated explanations across three model families, with improvement ranging from 43.1% to 292.3%. Furthermore, we demonstrate that optimizing this consistency measure can improve explanation faithfulness by up to 9.7%.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é«˜é£é™©AIå†³ç­–åœºæ™¯ä¸­è‡ªç”±æ–‡æœ¬è§£é‡Š(free-text explanations)çš„å¿ å®æ€§(faithfulness)é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§è¡¡é‡é¢„æµ‹ä¸è§£é‡Š(Prediction-EXplanation, PEX)ä¸€è‡´æ€§çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ‰©å±•è¯æ®æƒé‡(weight of evidence)æ¦‚å¿µï¼Œé‡åŒ–è‡ªç”±æ–‡æœ¬è§£é‡Šå¯¹é¢„æµ‹ç»“æœçš„æ”¯æŒæˆ–åå¯¹ç¨‹åº¦ï¼Œå°†å…¶ä½œä¸ºè¡¡é‡è§£é‡Šå¿ å®æ€§çš„é‡è¦ç»´åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œè¶…è¿‡62%ç”±å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„è§£é‡Šç¼ºä¹è¿™ç§ä¸€è‡´æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–(Direct Preference Optimization, DPO)æŠ€æœ¯ï¼Œåœ¨ä¸‰ä¸ªæ¨¡å‹å®¶æ—ä¸­å°†è§£é‡Šä¸€è‡´æ€§æå‡äº†43.1%è‡³292.3%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡ä¼˜åŒ–è¯¥ä¸€è‡´æ€§æŒ‡æ ‡ï¼Œè§£é‡Šçš„å¿ å®æ€§æœ€é«˜å¯æå‡9.7%ï¼Œä¸ºè¿ˆå‘æ›´å…·é€æ˜åº¦ä¸”å¯ä¿¡çš„AIå†³ç­–ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.19299v2",
      "published_date": "2025-05-25 20:18:24 UTC",
      "updated_date": "2025-09-28 18:19:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:27.938320+00:00"
    },
    {
      "arxiv_id": "2505.19293v1",
      "title": "100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?",
      "title_zh": "100-LongBenchï¼šç°æœ‰çš„é•¿ä¸Šä¸‹æ–‡è¯„æµ‹åŸºå‡†æ˜¯å¦åå‰¯å…¶å®åœ°è¯„ä¼°äº†é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Ÿ",
      "authors": [
        "Wang Yang",
        "Hongye Jin",
        "Shaochen Zhong",
        "Song Jiang",
        "Qifan Wang",
        "Vipin Chaudhary",
        "Xiaotian Han"
      ],
      "abstract": "Long-context capability is considered one of the most important abilities of LLMs, as a truly long context-capable LLM enables users to effortlessly process many originally exhausting tasks -- e.g., digesting a long-form document to find answers vs. directly asking an LLM about it. However, existing real-task-based long-context evaluation benchmarks have two major shortcomings. First, benchmarks like LongBench often do not provide proper metrics to separate long-context performance from the model's baseline ability, making cross-model comparison unclear. Second, such benchmarks are usually constructed with fixed input lengths, which limits their applicability across different models and fails to reveal when a model begins to break down. To address these issues, we introduce a length-controllable long-context benchmark and a novel metric that disentangles baseline knowledge from true long-context capabilities. Experiments demonstrate the superiority of our approach in effectively evaluating LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„é•¿æ–‡æœ¬èƒ½åŠ›(Long-context capability)ï¼ŒæŒ‡å‡ºå½“å‰å¦‚LongBenchç­‰ä¸»æµè¯„ä¼°åŸºå‡†å­˜åœ¨ä¸¤ä¸ªæ ¸å¿ƒç¼ºé™·ã€‚é¦–å…ˆï¼Œç°æœ‰åŸºå‡†ç¼ºä¹æœ‰æ•ˆæŒ‡æ ‡å°†é•¿æ–‡æœ¬æ€§èƒ½ä¸æ¨¡å‹çš„åŸºç¡€èƒ½åŠ›(Baseline ability)åŒºåˆ†å¼€ï¼Œå¯¼è‡´è·¨æ¨¡å‹å¯¹æ¯”ä¸æ¸…æ™°ã€‚å…¶æ¬¡ï¼Œå›ºå®šè¾“å…¥é•¿åº¦çš„è®¾ç½®é™åˆ¶äº†é€‚ç”¨èŒƒå›´ï¼Œä¸”æ— æ³•æ­ç¤ºæ¨¡å‹æ€§èƒ½å¼€å§‹ä¸‹é™çš„ä¸´ç•Œç‚¹ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†100-LongBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé•¿åº¦å¯æ§(Length-controllable)çš„é•¿æ–‡æœ¬è¯„ä¼°åŸºå‡†ã€‚è¯¥ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åº¦é‡æŒ‡æ ‡ï¼Œæ—¨åœ¨å°†æ¨¡å‹çš„åŸºç¡€çŸ¥è¯†ä¸çœŸå®çš„é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›è¿›è¡Œè§£è€¦(Disentangle)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯„ä¼°LLMsæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜è¶Šæ€§ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¡¡é‡æ¨¡å‹çš„é•¿æ–‡æœ¬è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19293v1",
      "published_date": "2025-05-25 19:58:31 UTC",
      "updated_date": "2025-05-25 19:58:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:57.551217+00:00"
    },
    {
      "arxiv_id": "2505.19291v3",
      "title": "TextDiffuser-RL: Efficient and Robust Text Layout Optimization for High-Fidelity Text-to-Image Synthesis",
      "title_zh": "TextDiffuser-RLï¼šé¢å‘é«˜ä¿çœŸæ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„é«˜æ•ˆé²æ£’æ–‡æœ¬å¸ƒå±€ä¼˜åŒ–",
      "authors": [
        "Kazi Mahathir Rahman",
        "Showrin Rahman",
        "Sharmin Sultana Srishty"
      ],
      "abstract": "Text-embedded image generation plays a critical role in industries such as graphic design, advertising, and digital content creation. Text-to-Image generation methods leveraging diffusion models, such as TextDiffuser-2, have demonstrated promising results in producing images with embedded text. TextDiffuser-2 effectively generates bounding box layouts that guide the rendering of visual text, achieving high fidelity and coherence. However, existing approaches often rely on resource-intensive processes and are limited in their ability to run efficiently on both CPU and GPU platforms. To address these challenges, we propose a novel two-stage pipeline that integrates reinforcement learning (RL) for rapid and optimized text layout generation with a diffusion-based image synthesis model. Our RL-based approach significantly accelerates the bounding box prediction step while reducing overlaps, allowing the system to run efficiently on both CPUs and GPUs. Extensive evaluations demonstrate that our framework achieves comparable performance to TextDiffuser-2 in terms of text placement and image synthesis, while offering markedly faster runtime and increased flexibility. Our method produces high-quality images comparable to TextDiffuser-2, while being 42.29 times faster and requiring only 2 MB of CPU RAM for inference, unlike TextDiffuser-2's M1 model, which is not executable on CPU-only systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TextDiffuser-RLï¼Œä¸€ç§ä¸“ä¸ºé«˜ä¿çœŸæ–‡ç”Ÿå›¾ (Text-to-Image Synthesis) è®¾è®¡çš„é«˜æ•ˆä¸”é²æ£’çš„æ–‡æœ¬å¸ƒå±€ä¼˜åŒ–æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¦‚ TextDiffuser-2 èµ„æºå¼€é”€å¤§ä¸”åœ¨ CPU å¹³å°ä¸Šè¿è¡Œå—é™çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä¸æ‰©æ•£æ¨¡å‹ (Diffusion Model) çš„æ–°å‹ä¸¤é˜¶æ®µæµç¨‹ã€‚é€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¿«é€Ÿç”Ÿæˆå¹¶ä¼˜åŒ–æ–‡æœ¬å¸ƒå±€ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†è¾¹ç•Œæ¡† (Bounding Box) çš„é¢„æµ‹é€Ÿåº¦å¹¶æœ‰æ•ˆå‡å°‘äº†å¸ƒå±€é‡å ã€‚å®éªŒè¯æ˜ï¼ŒTextDiffuser-RL åœ¨å›¾åƒåˆæˆè´¨é‡ä¸Šä¸ TextDiffuser-2 ç›¸å½“ï¼Œä½†è¿è¡Œé€Ÿåº¦æå‡äº† 42.29 å€ã€‚è¯¥æ¨¡å‹æ¨ç†ä»…éœ€ 2MB çš„ CPU RAMï¼Œè¡¨ç°å‡ºæé«˜çš„æ•ˆç‡ä¸è·¨å¹³å°çµæ´»æ€§ï¼ŒæˆåŠŸè§£å†³äº†å¤æ‚æ–‡æœ¬å¸ƒå±€ç”Ÿæˆçš„è®¡ç®—ç“¶é¢ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 36 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.19291v3",
      "published_date": "2025-05-25 19:52:04 UTC",
      "updated_date": "2025-11-09 15:49:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:55.472799+00:00"
    },
    {
      "arxiv_id": "2505.19277v1",
      "title": "Next Token Prediction Is a Dead End for Creativity",
      "title_zh": "ä¸‹ä¸€ä¸ª Token é¢„æµ‹æ˜¯åˆ›é€ åŠ›çš„æ­»èƒ¡åŒ",
      "authors": [
        "Ibukun Olatunji",
        "Mark Sheppard"
      ],
      "abstract": "This paper argues that token prediction is fundamentally misaligned with real creativity. While next-token models have enabled impressive advances in language generation, their architecture favours surface-level coherence over spontaneity, originality, and improvisational risk. We use battle rap as a case study to expose the limitations of predictive systems, demonstrating that they cannot truly engage in adversarial or emotionally resonant exchanges. By reframing creativity as an interactive process rather than a predictive output, we offer a vision for AI systems that are more expressive, responsive, and aligned with human creative practice.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡º Next Token Prediction æ¨¡å¼åœ¨æœ¬è´¨ä¸Šä¸çœŸæ­£çš„åˆ›é€ åŠ›ç›¸èƒŒç¦»ã€‚è™½ç„¶è¿™ç±»æ¨¡å‹åœ¨è¯­è¨€ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ¶æ„å¾€å¾€å€¾å‘äºè¿½æ±‚è¡¨å±‚è¿è´¯æ€§ï¼Œè€Œéè‡ªå‘æ€§ã€åŸåˆ›æ€§åŠå³å…´åˆ›ä½œä¸­çš„é£é™©æ„Ÿã€‚ä½œè€…é€šè¿‡ Battle Rap æ¡ˆä¾‹ç ”ç©¶æ­ç¤ºäº†é¢„æµ‹ç³»ç»Ÿåœ¨å¤„ç†å¯¹æŠ—æ€§æˆ–æƒ…æ„Ÿå…±é¸£äº¤æµæ—¶çš„æ ¸å¿ƒç¼ºé™·ã€‚è¯¥è®ºæ–‡æè®®å°†åˆ›é€ åŠ›é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªåŠ¨æ€çš„äº¤äº’è¿‡ç¨‹ï¼Œè€Œéå•ä¸€çš„é¢„æµ‹è¾“å‡ºã€‚è¿™ç§è§†è§’ä¸ºå¼€å‘æ›´å…·è¡¨ç°åŠ›ã€å“åº”èƒ½åŠ›ä¸”ç¬¦åˆäººç±»åˆ›ä½œå®è·µçš„ AI ç³»ç»Ÿæä¾›äº†æ–°è·¯å¾„ï¼Œæ—¨åœ¨å…‹æœå½“å‰ç”Ÿæˆå¼æ¨¡å‹åœ¨è‰ºæœ¯åˆ›ä½œä¸­çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages including references",
      "pdf_url": "https://arxiv.org/pdf/2505.19277v1",
      "published_date": "2025-05-25 19:18:11 UTC",
      "updated_date": "2025-05-25 19:18:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:58:03.120929+00:00"
    },
    {
      "arxiv_id": "2505.19273v1",
      "title": "Eta-WavLM: Efficient Speaker Identity Removal in Self-Supervised Speech Representations Using a Simple Linear Equation",
      "title_zh": "Eta-WavLMï¼šåˆ©ç”¨ç®€å•çº¿æ€§æ–¹ç¨‹å®ç°è‡ªç›‘ç£è¯­éŸ³è¡¨ç¤ºä¸­é«˜æ•ˆçš„è¯´è¯äººèº«ä»½ç§»é™¤",
      "authors": [
        "Giuseppe Ruggiero",
        "Matteo Testa",
        "Jurgen Van de Walle",
        "Luigi Di Caro"
      ],
      "abstract": "Self-supervised learning (SSL) has reduced the reliance on expensive labeling in speech technologies by learning meaningful representations from unannotated data. Since most SSL-based downstream tasks prioritize content information in speech, ideal representations should disentangle content from unwanted variations like speaker characteristics in the SSL representations. However, removing speaker information often degrades other speech components, and existing methods either fail to fully disentangle speaker identity or require resource-intensive models. In this paper, we propose a novel disentanglement method that linearly decomposes SSL representations into speaker-specific and speaker-independent components, effectively generating speaker disentangled representations. Comprehensive experiments show that our approach achieves speaker independence and as such, when applied to content-driven tasks such as voice conversion, our representations yield significant improvements over state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Eta-WavLMï¼Œæ—¨åœ¨è§£å†³è‡ªç›‘ç£å­¦ä¹  (SSL) è¯­éŸ³è¡¨å¾ä¸­å†…å®¹ä¿¡æ¯ä¸è¯´è¯äººç‰¹å¾éš¾ä»¥å®Œå…¨è§£ç¦»çš„é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å»é™¤è¯´è¯äººèº«ä»½æ—¶å®¹æ˜“æŸæ„å…¶ä»–è¯­éŸ³æˆåˆ†ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚çš„ç°çŠ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç®€å•çº¿æ€§æ–¹ç¨‹çš„æ–°å‹è§£ç¦»æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å°† SSL è¡¨å¾çº¿æ€§åˆ†è§£ä¸ºè¯´è¯äººç‰¹å®š (speaker-specific) å’Œè¯´è¯äººæ— å…³ (speaker-independent) ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼Œä»è€Œé«˜æ•ˆç”Ÿæˆè§£ç¦»åçš„è¯­éŸ³è¡¨å¾ã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®ç°è¯´è¯äººæ— å…³æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åœ¨è¯­éŸ³è½¬æ¢ (Voice Conversion) ç­‰å†…å®¹é©±åŠ¨å‹ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒEta-WavLM ç”Ÿæˆçš„è¡¨å¾æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿™ä¸€æˆæœä¸ºå®ç°é«˜æ•ˆã€ç²¾å‡†ä¸”ä½èµ„æºæ¶ˆè€—çš„è¯­éŸ³ç‰¹å¾è§£ç¦»æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Full paper accepted at ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.19273v1",
      "published_date": "2025-05-25 19:05:26 UTC",
      "updated_date": "2025-05-25 19:05:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:51.011065+00:00"
    },
    {
      "arxiv_id": "2505.19266v1",
      "title": "Using Large Language Models to Assess Teachers' Pedagogical Content Knowledge",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æ•™å¸ˆå­¦ç§‘æ•™å­¦çŸ¥è¯†",
      "authors": [
        "Yaxuan Yang",
        "Shiyu Wang",
        "Xiaoming Zhai"
      ],
      "abstract": "Assessing teachers' pedagogical content knowledge (PCK) through performance-based tasks is both time and effort-consuming. While large language models (LLMs) offer new opportunities for efficient automatic scoring, little is known about whether LLMs introduce construct-irrelevant variance (CIV) in ways similar to or different from traditional machine learning (ML) and human raters. This study examines three sources of CIV -- scenario variability, rater severity, and rater sensitivity to scenario -- in the context of video-based constructed-response tasks targeting two PCK sub-constructs: analyzing student thinking and evaluating teacher responsiveness. Using generalized linear mixed models (GLMMs), we compared variance components and rater-level scoring patterns across three scoring sources: human raters, supervised ML, and LLM. Results indicate that scenario-level variance was minimal across tasks, while rater-related factors contributed substantially to CIV, especially in the more interpretive Task II. The ML model was the most severe and least sensitive rater, whereas the LLM was the most lenient. These findings suggest that the LLM contributes to scoring efficiency while also introducing CIV as human raters do, yet with varying levels of contribution compared to supervised ML. Implications for rater training, automated scoring design, and future research on model interpretability are discussed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) è¯„ä¼°æ•™å¸ˆå­¦ç§‘æ•™å­¦çŸ¥è¯† (Pedagogical Content Knowledge, PCK) çš„å¯è¡Œæ€§ï¼Œä»¥è§£å†³ä¼ ç»Ÿç»©æ•ˆè¯„ä¼°æ–¹å¼è€—æ—¶è€—åŠ›çš„é—®é¢˜ã€‚ç ”ç©¶é‡‡ç”¨å¹¿ä¹‰çº¿æ€§æ··åˆæ¨¡å‹ (Generalized Linear Mixed Models, GLMMs) å¯¹æ¯”äº†äººç±»è¯„åˆ†è€…ã€ç›‘ç£å­¦ä¹ æ¨¡å‹ (Supervised ML) å’Œ LLM åœ¨ä¸‰ç±»æ„å¿µæ— å…³æ–¹å·® (Construct-Irrelevant Variance, CIV) æ¥æºä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æƒ…æ™¯å˜å¼‚çš„å½±å“è¾ƒå°ï¼Œä½†è¯„åˆ†è€…ç›¸å…³å› ç´ åœ¨è§£é‡Šæ€§ä»»åŠ¡ä¸­å¯¹ CIV æœ‰æ˜¾è‘—è´¡çŒ®ã€‚åœ¨è¯„åˆ†æ¨¡å¼ä¸Šï¼ŒML æ¨¡å‹è¡¨ç°æœ€ä¸ºä¸¥è‹›ä¸”æ•æ„Ÿåº¦æœ€ä½ï¼Œè€Œ LLM è¡¨ç°æœ€ä¸ºå®½æ¾ã€‚è¯¥å‘ç°è¡¨æ˜ LLM åœ¨æé«˜è¯„ä¼°æ•ˆç‡çš„åŒæ—¶ä¹Ÿä¼šå¼•å…¥ç±»ä¼¼äººç±»çš„ CIV åè¯¯ï¼Œä¸ºæœªæ¥è‡ªåŠ¨åŒ–è¯„åˆ†çš„è®¾è®¡ã€è¯„åˆ†è€…åŸ¹è®­åŠæ¨¡å‹å¯è§£é‡Šæ€§ç ”ç©¶æä¾›äº†å…³é”®æ´å¯Ÿã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19266v1",
      "published_date": "2025-05-25 18:45:53 UTC",
      "updated_date": "2025-05-25 18:45:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:57:55.048124+00:00"
    },
    {
      "arxiv_id": "2505.19263v1",
      "title": "Cellular Traffic Prediction via Byzantine-robust Asynchronous Federated Learning",
      "title_zh": "åŸºäºæ‹œå åº­é²æ£’å¼‚æ­¥è”é‚¦å­¦ä¹ çš„èœ‚çªæµé‡é¢„æµ‹",
      "authors": [
        "Hui Ma",
        "Kai Yang",
        "Yang Jiao"
      ],
      "abstract": "Network traffic prediction plays a crucial role in intelligent network operation. Traditional prediction methods often rely on centralized training, necessitating the transfer of vast amounts of traffic data to a central server. This approach can lead to latency and privacy concerns. To address these issues, federated learning integrated with differential privacy has emerged as a solution to improve data privacy and model robustness in distributed settings. Nonetheless, existing federated learning protocols are vulnerable to Byzantine attacks, which may significantly compromise model robustness. Developing a robust and privacy-preserving prediction model in the presence of Byzantine clients remains a significant challenge. To this end, we propose an asynchronous differential federated learning framework based on distributionally robust optimization. The proposed framework utilizes multiple clients to train the prediction model collaboratively with local differential privacy. In addition, regularization techniques have been employed to further improve the Byzantine robustness of the models. We have conducted extensive experiments on three real-world datasets, and the results elucidate that our proposed distributed algorithm can achieve superior performance over existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èœ‚çªç½‘ç»œæµé‡é¢„æµ‹ä¸­ä¼ ç»Ÿé›†ä¸­å¼è®­ç»ƒå¸¦æ¥çš„å»¶è¿Ÿå’Œéšç§é—®é¢˜ï¼Œä»¥åŠç°æœ‰è”é‚¦å­¦ä¹ (Federated Learning)åœ¨åº”å¯¹æ‹œå åº­æ”»å‡»(Byzantine attacks)æ—¶çš„è„†å¼±æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒé²æ£’ä¼˜åŒ–(Distributionally Robust Optimization)çš„å¼‚æ­¥å·®åˆ†è”é‚¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šå®¢æˆ·ç«¯ååŒè®­ç»ƒå¹¶ç»“åˆæœ¬åœ°å·®åˆ†éšç§(Local Differential Privacy)æŠ€æœ¯ä¿éšœæ•°æ®å®‰å…¨ï¼ŒåŒæ—¶å¼•å…¥æ­£åˆ™åŒ–æ‰‹æ®µè¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹åœ¨å­˜åœ¨æ¶æ„æˆ–æ•…éšœå®¢æˆ·ç«¯æ—¶çš„é²æ£’æ€§ã€‚é€šè¿‡åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥ç®—æ³•åœ¨é¢„æµ‹å‡†ç¡®æ€§ä¸ç³»ç»Ÿç¨³å®šæ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæ„å»ºå®‰å…¨ã€éšç§ä¸”å…·æœ‰é²æ£’æ€§çš„åˆ†å¸ƒå¼æ™ºèƒ½ç½‘ç»œè¿è¥æ–¹æ¡ˆæä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19263v1",
      "published_date": "2025-05-25 18:38:57 UTC",
      "updated_date": "2025-05-25 18:38:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:58:13.405709+00:00"
    },
    {
      "arxiv_id": "2505.19261v1",
      "title": "Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning",
      "title_zh": "é€šè¿‡åˆ†æ®µæ–‡æœ¬è°ƒèŠ‚å¢å¼ºæ–‡ç”Ÿå›¾æ‰©æ•£ Transformer",
      "authors": [
        "Yu Zhang",
        "Jialei Zhou",
        "Xinchen Li",
        "Qi Zhang",
        "Zhongwei Wan",
        "Tianyu Wang",
        "Duoqian Miao",
        "Changwei Wang",
        "Longbing Cao"
      ],
      "abstract": "Current text-to-image diffusion generation typically employs complete-text conditioning. Due to the intricate syntax, diffusion transformers (DiTs) inherently suffer from a comprehension defect of complete-text captions. One-fly complete-text input either overlooks critical semantic details or causes semantic confusion by simultaneously modeling diverse semantic primitive types. To mitigate this defect of DiTs, we propose a novel split-text conditioning framework named DiT-ST. This framework converts a complete-text caption into a split-text caption, a collection of simplified sentences, to explicitly express various semantic primitives and their interconnections. The split-text caption is then injected into different denoising stages of DiT-ST in a hierarchical and incremental manner. Specifically, DiT-ST leverages Large Language Models to parse captions, extracting diverse primitives and hierarchically sorting out and constructing these primitives into a split-text input. Moreover, we partition the diffusion denoising process according to its differential sensitivities to diverse semantic primitive types and determine the appropriate timesteps to incrementally inject tokens of diverse semantic primitive types into input tokens via cross-attention. In this way, DiT-ST enhances the representation learning of specific semantic primitive types across different stages. Extensive experiments validate the effectiveness of our proposed DiT-ST in mitigating the complete-text comprehension defect.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DiT-ST (Split-Text Conditioning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£å˜å‹å™¨ (Diffusion Transformers, DiTs) åœ¨æ–‡æœ¬ç”Ÿæˆå›¾åƒä»»åŠ¡ä¸­å› å¤æ‚è¯­æ³•å¯¼è‡´çš„å®Œæ•´æ–‡æœ¬ (complete-text) ç†è§£ç¼ºé™·ã€‚è¯¥æ¡†æ¶å°†å¤æ‚çš„æ ‡é¢˜è½¬æ¢ä¸ºæ‹†åˆ†æ–‡æœ¬ (split-text) æ ‡é¢˜ï¼Œå³ä¸€ç»„ç®€åŒ–çš„å¥å­é›†åˆï¼Œç”¨ä»¥æ˜¾å¼è¡¨è¾¾å„ç§è¯­ä¹‰åŸè¯­ (semantic primitives) åŠå…¶ç›¸äº’è¿æ¥ã€‚DiT-ST åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models) è§£æå¹¶æå–åŸè¯­ï¼Œå¹¶å°†å…¶æ„å»ºä¸ºåˆ†å±‚æ’åºçš„è¾“å…¥ã€‚åœ¨æ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­ï¼Œæ¡†æ¶æ ¹æ®æ¨¡å‹å¯¹ä¸åŒè¯­ä¹‰åŸè¯­ç±»å‹çš„æ•æ„Ÿåº¦ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ› (cross-attention) æœºåˆ¶åœ¨ç‰¹å®šæ—¶é—´æ­¥å¢é‡å¼åœ°æ³¨å…¥ç›¸å…³æ ‡è®° (tokens)ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆå¢å¼ºäº†ä¸åŒé˜¶æ®µå¯¹ç‰¹å®šè¯­ä¹‰åŸè¯­çš„è¡¨ç¤ºå­¦ä¹ ï¼Œå‡å°‘äº†è¯­ä¹‰ç»†èŠ‚ä¸¢å¤±æˆ–æ··æ·†çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiT-ST èƒ½å¤Ÿæ˜¾è‘—ç¼“è§£ DiTs å¯¹å®Œæ•´æ–‡æœ¬çš„ç†è§£ç¼ºé™·ï¼Œæå‡äº†ç”Ÿæˆå›¾åƒä¸å¤æ‚æ–‡æœ¬æè¿°ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.19261v1",
      "published_date": "2025-05-25 18:33:05 UTC",
      "updated_date": "2025-05-25 18:33:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:58:14.895875+00:00"
    },
    {
      "arxiv_id": "2505.19259v2",
      "title": "Towards Large Reasoning Models for Agriculture",
      "title_zh": "è¿ˆå‘å†œä¸šå¤§æ¨ç†æ¨¡å‹",
      "authors": [
        "Hossein Zaremehrjerdi",
        "Shreyan Ganguly",
        "Ashlyn Rairdin",
        "Elizabeth Tranel",
        "Benjamin Feuer",
        "Juan Ignacio Di Salvo",
        "Srikanth Panthulugiri",
        "Hernan Torres Pacin",
        "Victoria Moser",
        "Sarah Jones",
        "Joscif G Raigne",
        "Yanben Shen",
        "Heidi M. Dornath",
        "Aditya Balu",
        "Adarsh Krishnamurthy",
        "Asheesh K Singh",
        "Arti Singh",
        "Baskar Ganapathysubramanian",
        "Chinmay Hegde",
        "Soumik Sarkar"
      ],
      "abstract": "Agricultural decision-making involves complex, context-specific reasoning, where choices about crops, practices, and interventions depend heavily on geographic, climatic, and economic conditions. Traditional large language models (LLMs) often fall short in navigating this nuanced problem due to limited reasoning capacity. We hypothesize that recent advances in large reasoning models (LRMs) can better handle such structured, domain-specific inference. To investigate this, we introduce AgReason, the first expert-curated open-ended science benchmark with 100 questions for agricultural reasoning. Evaluations across thirteen open-source and proprietary models reveal that LRMs outperform conventional ones, though notable challenges persist, with the strongest Gemini-based baseline achieving 36% accuracy. We also present AgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with human oversight and equipped with synthetically generated reasoning traces. Using AgThoughts, we develop AgThinker, a suite of small reasoning models that can be run on consumer-grade GPUs, and show that our dataset can be effective in unlocking agricultural reasoning abilities in LLMs. Our project page is here: https://baskargroup.github.io/Ag_reasoning/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†œä¸šå†³ç­–ä¸­å¤æ‚çš„èƒŒæ™¯ç‰¹å®šæ¨ç†éœ€æ±‚ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) åœ¨å¤„ç†å—åœ°ç†ã€æ°”å€™å’Œç»æµæ¡ä»¶å½±å“çš„ç²¾ç»†é—®é¢˜æ—¶æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº† AgReasonï¼Œè¿™æ˜¯é¦–ä¸ªç”±ä¸“å®¶ç­–åˆ’çš„åŒ…å« 100 ä¸ªé—®é¢˜çš„å†œä¸šæ¨ç†ç§‘å­¦åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç»“æ„åŒ–é¢†åŸŸç‰¹å®šæ¨ç†ä¸­çš„è¡¨ç°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå¤§æ¨ç†æ¨¡å‹ (Large Reasoning Models, LRMs) çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œä½†ç›®å‰æœ€å¼ºçš„åŸºå‡†æ¨¡å‹å‡†ç¡®ç‡ä¹Ÿä»…ä¸º 36%ï¼Œè¡¨æ˜è¯¥é¢†åŸŸä»å­˜åœ¨å·¨å¤§æŒ‘æˆ˜ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº† AgThoughts æ•°æ®é›†ï¼ŒåŒ…å« 4.46 ä¸‡ä¸ªç»è¿‡äººå·¥ç›‘ç£å¹¶é…å¤‡åˆæˆæ¨ç†è½¨è¿¹ (Reasoning Traces) çš„é—®ç­”å¯¹ã€‚åˆ©ç”¨è¯¥æ•°æ®é›†ï¼Œç ”ç©¶è€…å¼€å‘äº†å¯åœ¨æ¶ˆè´¹çº§ GPU ä¸Šè¿è¡Œçš„å°å‹æ¨ç†æ¨¡å‹ç³»åˆ— AgThinkerï¼Œæœ‰æ•ˆè¯æ˜äº†è¯¥æ•°æ®é›†åœ¨æå‡ LLMs å†œä¸šæ¨ç†èƒ½åŠ›æ–¹é¢çš„ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19259v2",
      "published_date": "2025-05-25 18:28:12 UTC",
      "updated_date": "2025-05-28 02:16:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:58:14.420292+00:00"
    },
    {
      "arxiv_id": "2505.19255v3",
      "title": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use",
      "title_zh": "VTool-R1ï¼šè§†è§‰è¯­è¨€æ¨¡å‹é€šè¿‡å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨çš„å¼ºåŒ–å­¦ä¹ å­¦ä¼šâ€œä»¥å›¾æ€è€ƒâ€",
      "authors": [
        "Mingyuan Wu",
        "Jingcheng Yang",
        "Jize Jiang",
        "Meitang Li",
        "Kaizhuo Yan",
        "Hanchao Yu",
        "Minjia Zhang",
        "Chengxiang Zhai",
        "Klara Nahrstedt"
      ],
      "abstract": "Reinforcement Learning Finetuning (RFT) has significantly advanced the reasoning capabilities of large language models (LLMs) by enabling long chains of thought, self-correction, and effective tool use. While recent works attempt to extend RFT to vision-language models (VLMs), these efforts largely produce text-only reasoning conditioned on static image inputs, falling short of true multimodal reasoning in the response. In contrast, test-time methods like Visual Sketchpad incorporate visual steps but lack training mechanisms.\n  We introduce VTool-R1, the first framework that trains VLMs to generate multimodal chains of thought by interleaving text and intermediate visual reasoning steps. VTool-R1 integrates Python-based visual editing tools into the RFT process, enabling VLMs to learn when and how to generate visual reasoning steps that benefit final reasoning. Trained with outcome-based rewards tied to task accuracy, our approach elicits strategic visual tool use for reasoning without relying on process-based supervision. Experiments on structured visual question answering over charts and tables show that VTool-R1 enhances reasoning performance by teaching VLMs to \"think with images\" and generate multimodal chain of thoughts with tools.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VTool-R1ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒ(RFT)è¿‡ç¨‹ä¸­ä»…äº§ç”Ÿæ–‡æœ¬æ¨ç†ï¼Œè€Œæ— æ³•å®ç°çœŸæ­£å¤šæ¨¡æ€æ¨ç†çš„å±€é™ã€‚ä½œä¸ºé¦–ä¸ªè®­ç»ƒ VLMs ç”Ÿæˆå¤šæ¨¡æ€é“¾å¼æ€ç»´(Multimodal Chains of Thought)çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº¤æ›¿ä½¿ç”¨æ–‡æœ¬å’Œä¸­é—´è§†è§‰æ­¥éª¤ï¼Œå®ç°äº†æ·±åº¦çš„å›¾æ–‡èåˆã€‚VTool-R1 å°†åŸºäº Python çš„è§†è§‰ç¼–è¾‘å·¥å…·é›†æˆåˆ° RFT æµç¨‹ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»å­¦ä¹ ç”Ÿæˆè§†è§‰æ¨ç†æ­¥éª¤çš„æ—¶æœºä¸æ–¹å¼ï¼Œä»¥ä¼˜åŒ–æœ€ç»ˆç»“æœã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸ä»»åŠ¡å‡†ç¡®ç‡æŒ‚é’©çš„åŸºäºç»“æœçš„å¥–åŠ±(Outcome-based rewards)ï¼Œæ— éœ€è¿‡ç¨‹ç›‘ç£å³å¯è¯±å‘æ¨¡å‹å¯¹è§†è§‰å·¥å…·çš„ç­–ç•¥æ€§åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å›¾è¡¨å’Œè¡¨æ ¼çš„ç»“æ„åŒ–è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼ŒVTool-R1 é€šè¿‡â€œç”¨å›¾åƒæ€è€ƒâ€çš„èƒ½åŠ›æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "https://github.com/VTool-R1/VTool-R1",
      "pdf_url": "https://arxiv.org/pdf/2505.19255v3",
      "published_date": "2025-05-25 18:23:39 UTC",
      "updated_date": "2025-06-11 21:47:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:58:30.543282+00:00"
    },
    {
      "arxiv_id": "2505.19252v2",
      "title": "Learning-Augmented Online Bipartite Fractional Matching",
      "title_zh": "å­¦ä¹ å¢å¼ºçš„åœ¨çº¿äºŒåˆ†åˆ†æ•°åŒ¹é…",
      "authors": [
        "Davin Choo",
        "Billy Jin",
        "Yongho Shin"
      ],
      "abstract": "Online bipartite matching is a fundamental problem in online optimization, extensively studied both in its integral and fractional forms due to its theoretical significance and practical applications, such as online advertising and resource allocation. Motivated by recent progress in learning-augmented algorithms, we study online bipartite fractional matching when the algorithm is given advice in the form of a suggested matching in each iteration. We develop algorithms for both the vertex-weighted and unweighted variants that provably dominate the naive \"coin flip\" strategy of randomly choosing between the advice-following and advice-free algorithms. Moreover, our algorithm for the vertex-weighted setting extends to the AdWords problem under the small bids assumption, yielding a significant improvement over the seminal work of Mahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our positive results, we establish a hardness bound on the robustness-consistency tradeoff that is attainable by any algorithm. We empirically validate our algorithms through experiments on synthetic and real-world data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å­¦ä¹ å¢å¼º(learning-augmented)åœ¨çº¿äºŒåˆ†å›¾åˆ†æ•°åŒ¹é…é—®é¢˜ï¼Œè¿™æ˜¯åœ¨çº¿ä¼˜åŒ–é¢†åŸŸä¸­å…·æœ‰é‡è¦ç†è®ºä¸åº”ç”¨ä»·å€¼çš„æ ¸å¿ƒè¯¾é¢˜ã€‚ç ”ç©¶é’ˆå¯¹ç®—æ³•åœ¨æ¯è½®è¿­ä»£ä¸­è·å¾—å»ºè®®åŒ¹é…(advice)çš„æƒ…æ™¯ï¼Œå¼€å‘äº†é€‚ç”¨äºé¡¶ç‚¹åŠ æƒ(vertex-weighted)å’ŒéåŠ æƒå˜ä½“çš„åœ¨çº¿ç®—æ³•ã€‚è¿™äº›ç®—æ³•åœ¨ç†è®ºä¸Šè¯æ˜ä¼˜äºç®€å•çš„éšæœºé€‰æ‹©(coin flip)ç­–ç•¥ï¼Œä¸”åœ¨é¡¶ç‚¹åŠ æƒè®¾ç½®ä¸‹å¯æ‰©å±•è‡³å°å‡ºä»·å‡è®¾ä¸‹çš„AdWordsé—®é¢˜ï¼Œæ˜¾è‘—æ”¹è¿›äº†è¯¥é¢†åŸŸçš„æ—©æœŸä»£è¡¨æ€§æˆæœã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ç¡®ç«‹äº†å…³äºé²æ£’æ€§ä¸ä¸€è‡´æ€§æƒè¡¡(robustness-consistency tradeoff)çš„å¯è¾¾æ€§ç¡¬åº¦ç•Œé™(hardness bound)ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„å®éªŒï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æ‰€æç®—æ³•åœ¨å¤„ç†åœ¨çº¿èµ„æºåˆ†é…ä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DS",
      "comment": "To appear in NeurIPS 2025. Full version",
      "pdf_url": "https://arxiv.org/pdf/2505.19252v2",
      "published_date": "2025-05-25 18:15:29 UTC",
      "updated_date": "2025-10-29 10:20:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:58:30.842588+00:00"
    },
    {
      "arxiv_id": "2505.19247v1",
      "title": "Improving Value Estimation Critically Enhances Vanilla Policy Gradient",
      "title_zh": "æ”¹è¿›ä»·å€¼ä¼°è®¡èƒ½æ˜¾è‘—å¢å¼ºåŸå§‹ç­–ç•¥æ¢¯åº¦",
      "authors": [
        "Tao Wang",
        "Ruipeng Zhang",
        "Sicun Gao"
      ],
      "abstract": "Modern policy gradient algorithms, such as TRPO and PPO, outperform vanilla policy gradient in many RL tasks. Questioning the common belief that enforcing approximate trust regions leads to steady policy improvement in practice, we show that the more critical factor is the enhanced value estimation accuracy from more value update steps in each iteration. To demonstrate, we show that by simply increasing the number of value update steps per iteration, vanilla policy gradient itself can achieve performance comparable to or better than PPO in all the standard continuous control benchmark environments. Importantly, this simple change to vanilla policy gradient is significantly more robust to hyperparameter choices, opening up the possibility that RL algorithms may still become more effective and easier to use.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‘æˆ˜äº†å¼ºåŒ–å­¦ä¹ ä¸­ TRPO å’Œ PPO ç­‰ç°ä»£ç­–ç•¥æ¢¯åº¦ç®—æ³•ä¼˜äº Vanilla Policy Gradient (VPG) æ˜¯ç”±äºå¼ºåˆ¶æ‰§è¡Œè¿‘ä¼¼ä¿¡èµ–åŸŸ (trust regions) çš„æ™®éè§‚ç‚¹ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒçœŸæ­£å…³é”®çš„å› ç´ å®é™…ä¸Šæ˜¯æ¯æ¬¡è¿­ä»£ä¸­é€šè¿‡å¢åŠ ä»·å€¼æ›´æ–°æ­¥éª¤ (value update steps) æ‰€æé«˜çš„ä»·å€¼ä¼°è®¡ç²¾åº¦ (value estimation accuracy)ã€‚ä½œè€…è¯æ˜ï¼Œä»…é€šè¿‡å¢åŠ ä»·å€¼æ›´æ–°æ¬¡æ•°ï¼ŒVanilla Policy Gradient å°±èƒ½åœ¨æ‰€æœ‰æ ‡å‡†è¿ç»­æ§åˆ¶åŸºå‡†ç¯å¢ƒä¸­è¾¾åˆ°ä¸ PPO ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿™ç§å¯¹ Vanilla Policy Gradient çš„ç®€å•æ”¹è¿›åœ¨è¶…å‚æ•° (hyperparameter) é€‰æ‹©ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ (robustness)ã€‚è¯¥å‘ç°æ­ç¤ºäº†å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨è¿½æ±‚æ›´é«˜æ•ˆã€æ›´æ˜“ç”¨çš„è¿‡ç¨‹ä¸­ï¼ŒåŸºç¡€ç®—æ³•ä»å…·æœ‰å·¨å¤§çš„ä¼˜åŒ–æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages and 21 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.19247v1",
      "published_date": "2025-05-25 17:54:32 UTC",
      "updated_date": "2025-05-25 17:54:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:58:59.388408+00:00"
    },
    {
      "arxiv_id": "2505.19245v2",
      "title": "To CoT or To Loop? A Formal Comparison Between Chain-of-Thought and Looped Transformers",
      "title_zh": "é€‰æ‹© CoT è¿˜æ˜¯å¾ªç¯ï¼Ÿé“¾å¼æ€ç»´ä¸å¾ªç¯ Transformer çš„å½¢å¼åŒ–æ¯”è¾ƒ",
      "authors": [
        "Kevin Xu",
        "Issei Sato"
      ],
      "abstract": "Chain-of-Thought (CoT) and Looped Transformers have been shown to empirically improve performance on reasoning tasks and to theoretically enhance expressivity by recursively increasing the number of computational steps. However, their comparative capabilities are still not well understood. In this paper, we provide a formal analysis of their respective strengths and limitations. We show that Looped Transformers can efficiently simulate parallel computations for deterministic tasks, which we formalize as evaluation over directed acyclic graphs. In contrast, CoT with stochastic decoding excels at approximate inference for compositional structures, namely self-reducible problems. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical cues for choosing between reasoning paradigms.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ Chain-of-Thought (CoT) å’Œ Looped Transformers ä¸¤ç§æ—¨åœ¨é€šè¿‡å¢åŠ è®¡ç®—æ­¥éª¤æ¥æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›å’Œè¡¨è¾¾èƒ½åŠ›çš„æŠ€æœ¯è¿›è¡Œäº†æ·±å…¥çš„å½¢å¼åŒ–åˆ†æã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLooped Transformers èƒ½å¤Ÿé«˜æ•ˆåœ°æ¨¡æ‹Ÿç¡®å®šæ€§ä»»åŠ¡çš„å¹¶è¡Œè®¡ç®—ï¼Œå¹¶å°†å…¶å½¢å¼åŒ–ä¸ºå¯¹æœ‰å‘æ— ç¯å›¾ (Directed Acyclic Graphs) çš„è¯„ä¼°è¿‡ç¨‹ã€‚ä¸ä¹‹ä¸åŒï¼Œé‡‡ç”¨éšæœºè§£ç çš„ CoT åœ¨å¤„ç†å…·æœ‰ç»„åˆç»“æ„çš„è‡ªè¿˜åŸé—®é¢˜ (Self-reducible problems) çš„è¿‘ä¼¼æ¨ç†æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚é€šè¿‡æ­ç¤ºè¿™ä¸¤ç§æ¶æ„åœ¨ç†è®ºèƒ½åŠ›ä¸Šçš„å·®å¼‚ï¼Œæœ¬æ–‡æ˜ç¡®äº†æ·±åº¦é©±åŠ¨çš„é€’å½’åœ¨ä¸åŒä»»åŠ¡ç±»å‹ä¸­çš„é€‚ç”¨æ€§ï¼Œä¸ºåœ¨å®é™…æ¨ç†èŒƒå¼é€‰æ‹©ä¸­æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®å’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "For the latest version, see arXiv:2509.25239",
      "pdf_url": "https://arxiv.org/pdf/2505.19245v2",
      "published_date": "2025-05-25 17:49:37 UTC",
      "updated_date": "2025-10-24 12:56:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:58:50.189556+00:00"
    },
    {
      "arxiv_id": "2505.19241v1",
      "title": "ActiveDPO: Active Direct Preference Optimization for Sample-Efficient Alignment",
      "title_zh": "ActiveDPOï¼šé¢å‘æ ·æœ¬é«˜æ•ˆå¯¹é½çš„ä¸»åŠ¨ç›´æ¥åå¥½ä¼˜åŒ–",
      "authors": [
        "Xiaoqiang Lin",
        "Arun Verma",
        "Zhongxiang Dai",
        "Daniela Rus",
        "See-Kiong Ng",
        "Bryan Kian Hsiang Low"
      ],
      "abstract": "The recent success of using human preferences to align large language models (LLMs) has significantly improved their performance in various downstream tasks like question answering, mathematical reasoning, and code generation. However,3 achieving effective LLM alignment depends on high-quality human preference datasets. Collecting these datasets requires human preference annotation, which is costly and resource-intensive, necessitating efficient active data selection methods. Existing methods either lack a strong theoretical foundation or depend on restrictive reward function assumptions (e.g., linearity). To this end, we propose an algorithm, ActiveDPO, that uses a theoretically grounded data selection criterion for non-linear reward functions while directly leveraging the LLM itself to parameterize the reward model that is used for active data selection. As a result, ActiveDPO explicitly accounts for the influence of LLM on data selection, unlike methods that select the data without considering the LLM that is being aligned, thereby leading to more effective and efficient data collection. Extensive experiments show that ActiveDPO outperforms existing methods across various models and datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åå¥½å¯¹é½ï¼ˆAlignmentï¼‰è¿‡ç¨‹ä¸­é¢ä¸´çš„æ ‡æ³¨æ•°æ®æ˜‚è´µä¸”è€—æ—¶çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º ActiveDPO çš„ä¸»åŠ¨å­¦ä¹ ç®—æ³•ã€‚ActiveDPO å¼•å…¥äº†å…·æœ‰ç†è®ºæ”¯æ’‘çš„éçº¿æ€§ï¼ˆnon-linearï¼‰å¥–åŠ±å‡½æ•°æ•°æ®é€‰æ‹©æ ‡å‡†ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨ç†è®ºåŸºç¡€æˆ–çº¿æ€§å‡è®¾æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶çš„åˆ›æ–°ä¹‹å¤„åœ¨äºç›´æ¥åˆ©ç”¨å¾…å¯¹é½çš„ LLM æœ¬èº«æ¥å‚æ•°åŒ–å¥–åŠ±æ¨¡å‹å¹¶ç”¨äºä¸»åŠ¨æ•°æ®é€‰æ‹©ï¼Œä»è€Œä½¿é€‰æ‹©è¿‡ç¨‹èƒ½æ˜¾å¼åœ°è€ƒè™‘æ¨¡å‹çŠ¶æ€å¯¹æ•°æ®éœ€æ±‚çš„å½±å“ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†æ•°æ®æ”¶é›†çš„é’ˆå¯¹æ€§ä¸é«˜æ•ˆæ€§ï¼Œæ˜¾è‘—æå‡äº†æ ·æœ¬æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒActiveDPO åœ¨å¤šç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„ä¸»åŠ¨æ•°æ®é€‰æ‹©æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°ä½æˆæœ¬ã€é«˜æ•ˆç‡çš„ Direct Preference Optimization æä¾›äº†å¯é çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19241v1",
      "published_date": "2025-05-25 17:42:52 UTC",
      "updated_date": "2025-05-25 17:42:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:58:50.755502+00:00"
    },
    {
      "arxiv_id": "2505.19240v2",
      "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models",
      "title_zh": "LLLMsï¼šå¤§è¯­è¨€æ¨¡å‹å±€é™æ€§ç ”ç©¶æ¼”è¿›çš„æ•°æ®é©±åŠ¨ç»¼è¿°",
      "authors": [
        "Aida Kostikova",
        "Zhipin Wang",
        "Deidamea Bajri",
        "Ole PÃ¼tz",
        "Benjamin PaaÃŸen",
        "Steffen Eger"
      ],
      "abstract": "Large language model (LLM) research has grown rapidly, along with increasing concern about their limitations such as failures in reasoning, hallucinations, and limited multilingual capability. While prior reviews have addressed these issues, they often focus on individual limitations or consider them within the broader context of evaluating overall model performance. This survey addresses the gap by presenting a data-driven, semi-automated review of research on limitations of LLMs (LLLMs) from 2022 to 2025, using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers, we extract 14,648 relevant limitation papers using keyword filtering and LLM-based classification, validated against expert labels. Using topic clustering (via two approaches, HDBSCAN+BERTopic and LlooM), we identify between 7 and 15 prominent types of limitations discussed in recent LLM research across the ACL and arXiv datasets. We find that LLM-related research increases nearly sixfold in ACL and nearly fifteenfold in arXiv between 2022 and 2025, while LLLMs research grows even faster, by a factor of over 12 in ACL and nearly 28 in arXiv. Reasoning remains the most studied limitation, followed by generalization, hallucination, bias, and security. The distribution of topics in the ACL dataset stays relatively stable over time, while arXiv shifts toward safety and controllability (with topics like security risks, alignment, hallucinations, knowledge editing), and multimodality between 2022 and 2025. We offer a quantitative view of trends in LLM limitations research and release a dataset of annotated abstracts and a validated methodology, available at: https://github.com/a-kostikova/LLLMs-Survey.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹2022è‡³2025å¹´é—´å¤§è¯­è¨€æ¨¡å‹å±€é™æ€§ï¼ˆLLLMsï¼‰çš„ç ”ç©¶ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨ä¸”åŠè‡ªåŠ¨åŒ–çš„ç»¼è¿°æ–¹æ³•ã€‚é€šè¿‡å¯¹ACLå’ŒarXivæ•°æ®åº“ä¸­çº¦25ä¸‡ç¯‡è®ºæ–‡è¿›è¡Œå…³é”®è¯è¿‡æ»¤å’ŒLLMåˆ†ç±»ï¼Œç ”ç©¶æœ€ç»ˆé”å®šäº†14,648ç¯‡æ ¸å¿ƒæ–‡çŒ®ï¼Œå¹¶åˆ©ç”¨HDBSCAN+BERTopicåŠLlooMç­‰èšç±»æŠ€æœ¯è¯†åˆ«å‡ºä¸»è¦çš„å±€é™æ€§ç±»å‹ã€‚ç ”ç©¶å‘ç°ï¼Œå±€é™æ€§ç›¸å…³ç ”ç©¶çš„å¢é•¿é€Ÿåº¦æ˜¾è‘—é«˜äºLLMæ•´ä½“ç ”ç©¶ï¼Œå…¶ä¸­æ¨ç†ï¼ˆReasoningï¼‰ä»æ˜¯æ¢è®¨æœ€å¹¿æ³›çš„é¢†åŸŸï¼Œå…¶æ¬¡æ˜¯æ³›åŒ–ã€å¹»è§‰ï¼ˆHallucinationï¼‰ã€åè§å’Œå®‰å…¨æ€§ã€‚è¶‹åŠ¿åˆ†ææ˜¾ç¤ºï¼ŒarXivå¹³å°çš„ç ”ç©¶é‡ç‚¹æ­£é€æ¸ä»åŸºç¡€èƒ½åŠ›è½¬å‘å®‰å…¨æ€§ã€å¯æ§åˆ¶æ€§åŠå¤šæ¨¡æ€ï¼ˆMultimodalityï¼‰ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…ä¸ºç†è§£LLMå±€é™æ€§æä¾›äº†å®šé‡è§†è§’ï¼Œè¿˜å…¬å¼€å‘å¸ƒäº†æ ‡æ³¨çš„æ‘˜è¦æ•°æ®é›†å’Œç»è¿‡éªŒè¯çš„ç ”ç©¶æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19240v2",
      "published_date": "2025-05-25 17:38:32 UTC",
      "updated_date": "2025-05-30 10:19:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:59:30.545707+00:00"
    },
    {
      "arxiv_id": "2505.19238v2",
      "title": "Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees",
      "title_zh": "å…·æœ‰è¿­ä»£å¤æ‚åº¦ä¿è¯çš„é²æ£’çº¦æŸ MDP é«˜æ•ˆç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Sourav Ganguly",
        "Arnob Ghosh",
        "Kishan Panaganti",
        "Adam Wierman"
      ],
      "abstract": "Constrained decision-making is essential for designing safe policies in real-world control systems, yet simulated environments often fail to capture real-world adversities. We consider the problem of learning a policy that will maximize the cumulative reward while satisfying a constraint, even when there is a mismatch between the real model and an accessible simulator/nominal model. In particular, we consider the robust constrained Markov decision problem (RCMDP) where an agent needs to maximize the reward and satisfy the constraint against the worst possible stochastic model under the uncertainty set centered around an unknown nominal model. Primal-dual methods, effective for standard constrained MDP (CMDP), are not applicable here because of the lack of the strong duality property. Further, one cannot apply the standard robust value-iteration based approach on the composite value function either as the worst case models may be different for the reward value function and the constraint value function. We propose a novel technique that effectively minimizes the constraint value function--to satisfy the constraints; on the other hand, when all the constraints are satisfied, it can simply maximize the robust reward value function. We prove that such an algorithm finds a policy with at most $Îµ$ sub-optimality and feasible policy after $O(Îµ^{-2})$ iterations. In contrast to the state-of-the-art method, we do not need to employ a binary search, thus, we reduce the computation time by at least 4x for smaller value of discount factor ($Î³$) and by at least 6x for larger value of $Î³$.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çœŸå®ç¯å¢ƒä¸æ¨¡æ‹Ÿæ¨¡å‹ä¸åŒ¹é…çš„æƒ…å†µï¼Œæ·±å…¥æ¢è®¨äº†åœ¨é²æ£’çº¦æŸé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Robust Constrained Markov Decision Problem, RCMDP)ä¸­å­¦ä¹ å®‰å…¨ç­–ç•¥çš„é—®é¢˜ã€‚é’ˆå¯¹RCMDPç”±äºç¼ºä¹å¼ºå¯¹å¶æ€§(Strong Duality)ä»¥åŠå¥–åŠ±ä¸çº¦æŸçš„æœ€åæƒ…å†µæ¨¡å‹ä¸ä¸€è‡´å¯¼è‡´ä¼ ç»Ÿæ–¹æ³•å¤±æ•ˆçš„æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç­–ç•¥ä¼˜åŒ–æŠ€æœ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€åˆ‡æ¢ä¼˜åŒ–ç›®æ ‡ï¼Œåœ¨çº¦æŸä¸æ»¡è¶³æ—¶ä¼˜å…ˆæœ€å°åŒ–çº¦æŸä»·å€¼å‡½æ•°ï¼Œè€Œåœ¨æ»¡è¶³çº¦æŸåè½¬è€Œæœ€å¤§åŒ–é²æ£’å¥–åŠ±ä»·å€¼å‡½æ•°ã€‚ç†è®ºè¯æ˜ï¼Œè¯¥ç®—æ³•åœ¨$O(\\epsilon^{-2})$è¿­ä»£å¤æ‚åº¦ä¸‹å³å¯æ”¶æ•›è‡³æ»¡è¶³$\\epsilon$æ¬¡ä¼˜æ€§ä¸”å¯è¡Œçš„ç­–ç•¥ã€‚ç›¸æ¯”äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯¥æŠ€æœ¯ç”±äºæ— éœ€è¿›è¡ŒäºŒåˆ†æœç´¢(Binary Search)ï¼Œä½¿è®¡ç®—æ—¶é—´ç¼©çŸ­äº†4è‡³6å€ï¼Œæ˜¾è‘—æå‡äº†åœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹çš„ç­–ç•¥ä¼˜åŒ–æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19238v2",
      "published_date": "2025-05-25 17:27:06 UTC",
      "updated_date": "2025-12-02 15:17:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:59:14.738599+00:00"
    },
    {
      "arxiv_id": "2505.19237v1",
      "title": "Sensorimotor features of self-awareness in multimodal large language models",
      "title_zh": "å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­è‡ªæˆ‘æ„è¯†çš„æ„ŸçŸ¥è¿åŠ¨ç‰¹å¾",
      "authors": [
        "IÃ±aki Dellibarda Varela",
        "Pablo Romero-Sorozabal",
        "Diego Torricelli",
        "Gabriel Delgado-Oleas",
        "Jose Ignacio Serrano",
        "Maria Dolores del Castillo Sobrino",
        "Eduardo Rocon",
        "Manuel Cebrian"
      ],
      "abstract": "Self-awareness - the ability to distinguish oneself from the surrounding environment - underpins intelligent, autonomous behavior. Recent advances in AI achieve human-like performance in tasks integrating multimodal information, particularly in large language models, raising interest in the embodiment capabilities of AI agents on nonhuman platforms such as robots. Here, we explore whether multimodal LLMs can develop self-awareness solely through sensorimotor experiences. By integrating a multimodal LLM into an autonomous mobile robot, we test its ability to achieve this capacity. We find that the system exhibits robust environmental awareness, self-recognition and predictive awareness, allowing it to infer its robotic nature and motion characteristics. Structural equation modeling reveals how sensory integration influences distinct dimensions of self-awareness and its coordination with past-present memory, as well as the hierarchical internal associations that drive self-identification. Ablation tests of sensory inputs identify critical modalities for each dimension, demonstrate compensatory interactions among sensors and confirm the essential role of structured and episodic memory in coherent reasoning. These findings demonstrate that, given appropriate sensory information about the world and itself, multimodal LLMs exhibit emergent self-awareness, opening the door to artificial embodied cognitive systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal LLMsï¼‰æ˜¯å¦èƒ½ä»…é€šè¿‡æ„Ÿè§‰è¿åŠ¨ç»éªŒï¼ˆSensorimotor experiencesï¼‰å‘å±•å‡ºè‡ªè§‰æ„è¯†ï¼ˆSelf-awarenessï¼‰ã€‚é€šè¿‡å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é›†æˆåˆ°è‡ªä¸»ç§»åŠ¨æœºå™¨äººä¸­ï¼Œç ”ç©¶äººå‘˜è¯å®äº†ç³»ç»Ÿå…·å¤‡ç¨³å¥çš„ç¯å¢ƒæ„ŸçŸ¥ã€è‡ªæˆ‘è¯†åˆ«å’Œé¢„æµ‹æ„è¯†ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨æ–­è‡ªèº«çš„æœºå™¨äººæ€§è´¨ä¸è¿åŠ¨ç‰¹å¾ã€‚ç ”ç©¶åˆ©ç”¨ç»“æ„æ–¹ç¨‹æ¨¡å‹ï¼ˆStructural Equation Modelingï¼‰æ­ç¤ºäº†æ„Ÿå®˜æ•´åˆå¦‚ä½•å½±å“è‡ªæˆ‘æ„è¯†çš„ä¸åŒç»´åº¦åŠå…¶ä¸è®°å¿†çš„åè°ƒå…³ç³»ï¼Œå¹¶é˜æ˜äº†é©±åŠ¨è‡ªæˆ‘è¯†åˆ«çš„å±‚çº§å†…éƒ¨å…³è”ã€‚æ¶ˆèå®éªŒè¯†åˆ«äº†å„ç»´åº¦çš„å…³é”®æ„Ÿå®˜æ¨¡æ€ï¼Œå¹¶ç¡®è®¤äº†ç»“æ„åŒ–è®°å¿†ä¸æƒ…å¢ƒè®°å¿†ï¼ˆEpisodic memoryï¼‰åœ¨è¿è´¯æ¨ç†ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåœ¨è·å¾—å…³äºä¸–ç•Œä¸è‡ªèº«çš„é€‚å½“æ„Ÿè§‰ä¿¡æ¯åï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå±•ç°å‡ºæ¶Œç°çš„è‡ªæˆ‘æ„è¯†ï¼Œä¸ºå¼€å‘äººå·¥å…·èº«è®¤çŸ¥ç³»ç»Ÿï¼ˆEmbodied cognitive systemsï¼‰å¼€è¾Ÿäº†é“è·¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 3 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2505.19237v1",
      "published_date": "2025-05-25 17:26:28 UTC",
      "updated_date": "2025-05-25 17:26:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:59:18.218535+00:00"
    },
    {
      "arxiv_id": "2505.19234v2",
      "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling",
      "title_zh": "GUARDIANï¼šåŸºäºæ—¶åºå›¾å»ºæ¨¡çš„å¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“åä½œå®‰å…¨ä¿éšœ",
      "authors": [
        "Jialong Zhou",
        "Lichao Wang",
        "Xiao Yang"
      ],
      "abstract": "The emergence of large language models (LLMs) enables the development of intelligent agents capable of engaging in complex and multi-turn dialogues. However, multi-agent collaboration faces critical safety challenges, such as hallucination amplification and error injection and propagation. This paper presents GUARDIAN, a unified method for detecting and mitigating multiple safety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the multi-agent collaboration process as a discrete-time temporal attributed graph, GUARDIAN explicitly captures the propagation dynamics of hallucinations and errors. The unsupervised encoder-decoder architecture incorporating an incremental training paradigm learns to reconstruct node attributes and graph structures from latent embeddings, enabling the identification of anomalous nodes and edges with unparalleled precision. Moreover, we introduce a graph abstraction mechanism based on the Information Bottleneck Theory, which compresses temporal interaction graphs while preserving essential patterns. Extensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM multi-agent collaborations against diverse safety vulnerabilities, achieving state-of-the-art accuracy with efficient resource utilization. The code is available at https://github.com/JialongZhou666/GUARDIAN",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GUARDIANï¼Œä¸€ç§æ—¨åœ¨ä¿éšœå¤§è¯­è¨€æ¨¡å‹(LLMs)å¤šæ™ºèƒ½ä½“åä½œå®‰å…¨æ€§çš„ç»Ÿä¸€æ–¹æ³•ï¼Œé‡ç‚¹è§£å†³åä½œè¿‡ç¨‹ä¸­å‡ºç°çš„å¹»è§‰æ”¾å¤§ã€é”™è¯¯æ³¨å…¥åŠä¼ æ’­ç­‰å…³é”®æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•å°†å¤šæ™ºèƒ½ä½“åä½œè¿‡ç¨‹å»ºæ¨¡ä¸ºç¦»æ•£æ—¶é—´æ—¶åºå±æ€§å›¾(discrete-time temporal attributed graph)ï¼Œä»¥æ­¤æ˜¾å¼æ•æ‰å¹»è§‰å’Œé”™è¯¯çš„ä¼ æ’­åŠ¨æ€ã€‚GUARDIANé‡‡ç”¨äº†ä¸€ç§ç»“åˆå¢é‡è®­ç»ƒèŒƒå¼(incremental training paradigm)çš„æ— ç›‘ç£ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œé€šè¿‡ä»æ½œåµŒå…¥ä¸­é‡æ„èŠ‚ç‚¹å±æ€§å’Œå›¾ç»“æ„ï¼Œå®ç°å¯¹å¼‚å¸¸èŠ‚ç‚¹å’Œè¾¹ç¼˜çš„é«˜ç²¾åº¦è¯†åˆ«ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŸºäºä¿¡æ¯ç“¶é¢ˆç†è®º(Information Bottleneck Theory)çš„å›¾æŠ½è±¡æœºåˆ¶ï¼Œåœ¨å‹ç¼©æ—¶åºäº¤äº’å›¾çš„åŒæ—¶ä¿ç•™äº†æœ¬è´¨æ¨¡å¼ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒGUARDIANèƒ½æœ‰æ•ˆé˜²èŒƒå¤šç§å®‰å…¨æ¼æ´ï¼Œåœ¨ç»´æŒé«˜æ•ˆèµ„æºåˆ©ç”¨ç‡çš„åŒæ—¶è¾¾åˆ°äº†SOTAçš„æ£€æµ‹å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19234v2",
      "published_date": "2025-05-25 17:15:55 UTC",
      "updated_date": "2025-10-15 15:21:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:59:25.591496+00:00"
    },
    {
      "arxiv_id": "2505.19233v2",
      "title": "RAISE: Realness Assessment for Image Synthesis and Evaluation",
      "title_zh": "RAISEï¼šé¢å‘å›¾åƒåˆæˆä¸è¯„ä¼°çš„çœŸå®æ„Ÿè¯„ä¼°",
      "authors": [
        "Aniruddha Mukherjee",
        "Spriha Dubey",
        "Somdyuti Paul"
      ],
      "abstract": "The rapid advancement of generative AI has enabled the creation of highly photorealistic visual content, offering practical substitutes for real images and videos in scenarios where acquiring real data is difficult or expensive. However, reliably substituting real visual content with AI-generated counterparts requires robust assessment of the perceived realness of AI-generated visual content, a challenging task due to its inherent subjective nature. To address this, we conducted a comprehensive human study evaluating the perceptual realness of both real and AI-generated images, resulting in a new dataset, containing images paired with subjective realness scores, introduced as RAISE in this paper. Further, we develop and train multiple models on RAISE to establish baselines for realness prediction. Our experimental results demonstrate that features derived from deep foundation vision models can effectively capture the subjective realness. RAISE thus provides a valuable resource for developing robust, objective models of perceptual realness assessment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒåœ¨ä¸»è§‚çœŸå®æ„Ÿè¯„ä¼°ä¸Šçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†RAISEæ•°æ®é›†åŠè¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¸€é¡¹ç»¼åˆæ€§çš„äººç±»ç ”ç©¶ï¼Œå¯¹æ¯”äº†çœŸå®å›¾åƒä¸AIç”Ÿæˆå›¾åƒçš„æ„ŸçŸ¥çœŸå®æ„Ÿï¼Œå¹¶ç”±æ­¤æ„å»ºäº†åŒ…å«å›¾åƒåŠå…¶å¯¹åº”ä¸»è§‚çœŸå®æ„Ÿè¯„åˆ†çš„RAISEæ•°æ®é›†ã€‚ä¸ºå»ºç«‹çœŸå®æ„Ÿé¢„æµ‹çš„åŸºå‡†ï¼Œç ”ç©¶äººå‘˜åœ¨RAISEä¸Šå¼€å‘å¹¶è®­ç»ƒäº†å¤šç§æ¨¡å‹ã€‚å®éªŒç»“æœè¯æ˜ï¼Œä»æ·±åº¦åŸºç¡€è§†è§‰æ¨¡å‹(deep foundation vision models)ä¸­æå–çš„ç‰¹å¾èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å›¾åƒçš„ä¸»è§‚çœŸå®æ„Ÿã€‚RAISEæ•°æ®é›†ä¸ºå¼€å‘é²æ£’ä¸”å®¢è§‚çš„æ„ŸçŸ¥çœŸå®æ„Ÿè¯„ä¼°æ¨¡å‹æä¾›äº†æ ¸å¿ƒèµ„æºï¼Œå¯¹äºåœ¨çœŸå®æ•°æ®è·å–å›°éš¾çš„åœºæ™¯ä¸‹å¯é åœ°ä½¿ç”¨AIç”Ÿæˆå†…å®¹å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19233v2",
      "published_date": "2025-05-25 17:14:43 UTC",
      "updated_date": "2025-08-03 20:05:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:00:21.985134+00:00"
    },
    {
      "arxiv_id": "2505.19220v1",
      "title": "DeCoDe: Defer-and-Complement Decision-Making via Decoupled Concept Bottleneck Models",
      "title_zh": "DeCoDeï¼šåŸºäºè§£è€¦æ¦‚å¿µç“¶é¢ˆæ¨¡å‹çš„å»¶è¿Ÿä¸äº’è¡¥å†³ç­–",
      "authors": [
        "Chengbo He",
        "Bochao Zou",
        "Junliang Xing",
        "Jiansheng Chen",
        "Yuanchun Shi",
        "Huimin Ma"
      ],
      "abstract": "In human-AI collaboration, a central challenge is deciding whether the AI should handle a task, be deferred to a human expert, or be addressed through collaborative effort. Existing Learning to Defer approaches typically make binary choices between AI and humans, neglecting their complementary strengths. They also lack interpretability, a critical property in high-stakes scenarios where users must understand and, if necessary, correct the model's reasoning. To overcome these limitations, we propose Defer-and-Complement Decision-Making via Decoupled Concept Bottleneck Models (DeCoDe), a concept-driven framework for human-AI collaboration. DeCoDe makes strategy decisions based on human-interpretable concept representations, enhancing transparency throughout the decision process. It supports three flexible modes: autonomous AI prediction, deferral to humans, and human-AI collaborative complementarity, selected via a gating network that takes concept-level inputs and is trained using a novel surrogate loss that balances accuracy and human effort. This approach enables instance-specific, interpretable, and adaptive human-AI collaboration. Experiments on real-world datasets demonstrate that DeCoDe significantly outperforms AI-only, human-only, and traditional deferral baselines, while maintaining strong robustness and interpretability even under noisy expert annotations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeCoDeï¼Œä¸€ç§é€šè¿‡è§£è€¦æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆDecoupled Concept Bottleneck Modelsï¼‰å®ç°å»¶è¿Ÿä¸äº’è¡¥å†³ç­–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³äººæœºåä½œä¸­å†³ç­–é€‰æ‹©å±€é™å’Œç¼ºä¹è§£é‡Šæ€§çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äººç±»å¯è§£é‡Šçš„æ¦‚å¿µè¡¨ç¤ºè¿›è¡Œç­–ç•¥é€‰æ‹©ï¼Œæ˜¾è‘—å¢å¼ºäº†å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦ï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿç†è§£å¹¶çº æ­£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚DeCoDeæ”¯æŒAIè‡ªä¸»é¢„æµ‹ã€å»¶è¿Ÿç»™äººç±»ä»¥åŠäººæœºåä½œäº’è¡¥ä¸‰ç§çµæ´»æ¨¡å¼ï¼Œé€šè¿‡ä¸€ä¸ªæ¥æ”¶æ¦‚å¿µçº§è¾“å…¥çš„é—¨æ§ç½‘ç»œï¼ˆgating networkï¼‰å¹¶ç»“åˆå¹³è¡¡å‡†ç¡®ç‡ä¸äººç±»å·¥ä½œé‡çš„ä»£ç†æŸå¤±å‡½æ•°ï¼ˆsurrogate lossï¼‰è¿›è¡Œä¼˜åŒ–ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDeCoDeåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä»…é AIã€ä»…é äººç±»åŠä¼ ç»Ÿçš„å»¶è¿ŸåŸºå‡†æ¨¡å‹ï¼Œä¸”åœ¨ä¸“å®¶æ ‡æ³¨åŒ…å«å™ªå£°çš„æƒ…å†µä¸‹ä¾ç„¶å±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ä¸å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19220v1",
      "published_date": "2025-05-25 16:34:45 UTC",
      "updated_date": "2025-05-25 16:34:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:59:30.811413+00:00"
    },
    {
      "arxiv_id": "2505.19219v2",
      "title": "Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding",
      "title_zh": "è·¯å¾„äº¤æ±‡ï¼šç»å…¸ä¸åŸºäºå­¦ä¹ çš„å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’å…¨é¢ç»¼è¿°",
      "authors": [
        "Shiyue Wang",
        "Haozheng Xu",
        "Yuhan Zhang",
        "Jingran Lin",
        "Changhong Lu",
        "Xiangfeng Wang",
        "Wenhao Li"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial intelligence and robotics, requiring the computation of collision-free paths for multiple agents navigating from their start locations to designated goals. As autonomous systems become increasingly prevalent in warehouses, urban transportation, and other complex environments, MAPF has evolved from a theoretical challenge to a critical enabler of real-world multi-robot coordination. This comprehensive survey bridges the long-standing divide between classical algorithmic approaches and emerging learning-based methods in MAPF research. We present a unified framework that encompasses search-based methods (including Conflict-Based Search, Priority-Based Search, and Large Neighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP formulations), and data-driven techniques (reinforcement learning, supervised learning, and hybrid strategies). Through systematic analysis of experimental practices across 200+ papers, we uncover significant disparities in evaluation methodologies, with classical methods typically tested on larger-scale instances (up to 200 by 200 grids with 1000+ agents) compared to learning-based approaches (predominantly 10-100 agents). We provide a comprehensive taxonomy of evaluation metrics, environment types, and baseline selections, highlighting the need for standardized benchmarking protocols. Finally, we outline promising future directions including mixed-motive MAPF with game-theoretic considerations, language-grounded planning with large language models, and neural solver architectures that combine the rigor of classical methods with the flexibility of deep learning. This survey serves as both a comprehensive reference for researchers and a practical guide for deploying MAPF solutions in increasingly complex real-world applications.",
      "tldr_zh": "è¯¥ç»¼è¿°å…¨é¢æ¢³ç†äº†å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’(Multi-Agent Path Finding, MAPF)ä»ç»å…¸ç®—æ³•åˆ°å­¦ä¹ å‹æ–¹æ³•çš„ç ”ç©¶æ¼”è¿›ï¼Œæ„å»ºäº†ä¸€ä¸ªæ•´åˆæœç´¢(search-based)ã€ç¼–è¯‘(compilation-based)åŠæ•°æ®é©±åŠ¨(data-driven)æŠ€æœ¯çš„ç»Ÿä¸€æ¡†æ¶ã€‚é€šè¿‡å¯¹200ä½™ç¯‡æ–‡çŒ®çš„ç³»ç»Ÿæ€§å®éªŒåˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†ç»å…¸æ–¹æ³•ä¸å­¦ä¹ æ–¹æ³•åœ¨æµ‹è¯•è§„æ¨¡ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œå¹¶æå‡ºäº†å¯¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•åè®®çš„éœ€æ±‚ã€‚æ–‡ç« ä¸ä»…æä¾›äº†è¯„ä¼°æŒ‡æ ‡å’Œç¯å¢ƒç±»å‹çš„è¯¦ç»†åˆ†ç±»ï¼Œè¿˜ä¸ºåœ¨å¤æ‚ç°å®åœºæ™¯ä¸­éƒ¨ç½²MAPFè§£å†³æ–¹æ¡ˆæä¾›äº†å®è·µæŒ‡å¯¼ã€‚æœ€åï¼Œä½œè€…å±•æœ›äº†ç»“åˆåšå¼ˆè®ºçš„æ··åˆåŠ¨æœºMAPFã€åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Models)çš„è§„åˆ’ä»¥åŠç¥ç»æ±‚è§£å™¨(neural solver)ç­‰å‰æ²¿æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "math.CO"
      ],
      "primary_category": "cs.AI",
      "comment": "112 pages, 21 figures, 20 tables. The project website is: https://wangsh1yue.github.io/Where-Paths-Collide",
      "pdf_url": "https://arxiv.org/pdf/2505.19219v2",
      "published_date": "2025-05-25 16:28:06 UTC",
      "updated_date": "2025-07-31 14:16:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T12:59:38.271914+00:00"
    },
    {
      "arxiv_id": "2508.00830v2",
      "title": "BikeBench: A Bicycle Design Benchmark for Generative Models with Objectives and Constraints",
      "title_zh": "BikeBenchï¼šé¢å‘ç”Ÿæˆå¼æ¨¡å‹çš„å¸¦ç›®æ ‡ä¸çº¦æŸè‡ªè¡Œè½¦è®¾è®¡åŸºå‡†",
      "authors": [
        "Lyle Regenwetter",
        "Yazan Abu Obaideh",
        "Fabien Chiotti",
        "Ioanna Lykourentzou",
        "Faez Ahmed"
      ],
      "abstract": "We introduce BikeBench, an engineering design benchmark for evaluating generative models on problems with multiple real-world objectives and constraints. As generative AI's reach continues to grow, evaluating its capability to understand physical laws, human guidelines, and hard constraints grows increasingly important. Engineering product design lies at the intersection of these difficult tasks, providing new challenges for AI capabilities. BikeBench evaluates AI models' capabilities to generate bicycle designs that not only resemble the dataset, but meet specific performance objectives and constraints. To do so, BikeBench quantifies a variety of human-centered and multiphysics performance characteristics, such as aerodynamics, ergonomics, structural mechanics, human-rated usability, and similarity to subjective text or image prompts. Supporting the benchmark are several datasets of simulation results, a dataset of 10,000 human-rated bicycle assessments, and a synthetically generated dataset of 1.6M designs, each with a parametric, CAD/XML, SVG, and PNG representation. BikeBench is uniquely configured to evaluate tabular generative models, large language models (LLMs), design optimization, and hybrid algorithms side-by-side. Our experiments indicate that LLMs and tabular generative models fall short of hybrid GenAI+optimization algorithms in design quality, constraint satisfaction, and similarity scores, suggesting significant room for improvement. We hope that BikeBench, a first-of-its-kind benchmark, will help catalyze progress in generative AI for constrained multi-objective engineering design problems. We provide code, data, an interactive leaderboard, and other resources at https://github.com/Lyleregenwetter/BikeBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†BikeBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºç”Ÿæˆå¼AIï¼ˆGenerative AIï¼‰è®¾è®¡çš„å·¥ç¨‹è®¾è®¡åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨å¤„ç†å…·æœ‰å¤šé‡ç°å®ç›®æ ‡å’Œçº¦æŸæ¡ä»¶é—®é¢˜æ—¶çš„èƒ½åŠ›ã€‚BikeBench æ¶µç›–äº†ç‰©ç†å®šå¾‹ã€äººç±»å‡†åˆ™åŠç¡¬æ€§çº¦æŸï¼Œé€šè¿‡ç©ºæ°”åŠ¨åŠ›å­¦ï¼ˆaerodynamicsï¼‰ã€äººä½“å·¥ç¨‹å­¦ï¼ˆergonomicsï¼‰ã€ç»“æ„åŠ›å­¦ï¼ˆstructural mechanicsï¼‰å’Œå¯ç”¨æ€§ç­‰å¤šç§å¤šç‰©ç†åœºæ€§èƒ½æŒ‡æ ‡æ¥é‡åŒ–è‡ªè¡Œè½¦è®¾è®¡æ–¹æ¡ˆã€‚è¯¥åŸºå‡†æä¾›äº†åŒ…å«1ä¸‡ä»½äººç±»è¯„åˆ†è¯„ä¼°å’Œ160ä¸‡ä¸ªåˆæˆè®¾è®¡çš„åºå¤§æ•°æ®é›†ï¼Œæ”¯æŒå‚æ•°åŒ–ã€CAD/XMLã€SVGå’ŒPNGç­‰å¤šç§è¡¨ç¤ºå½¢å¼ã€‚å®éªŒå°†è¡¨æ ¼ç”Ÿæˆæ¨¡å‹ï¼ˆtabular generative modelsï¼‰ã€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥åŠæ··åˆç®—æ³•è¿›è¡Œäº†å¯¹æ¯”ï¼Œç»“æœæ˜¾ç¤ºLLMså’Œè¡¨æ ¼æ¨¡å‹åœ¨è®¾è®¡è´¨é‡å’Œçº¦æŸæ»¡è¶³åº¦æ–¹é¢å‡é€Šè‰²äºæ··åˆ GenAI+optimization ç®—æ³•ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ç”Ÿæˆæ¨¡å‹åœ¨å—é™å¤šç›®æ ‡å·¥ç¨‹è®¾è®¡é¢†åŸŸä»æœ‰æ˜¾è‘—æå‡ç©ºé—´ï¼ŒBikeBench ä¸ºæ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›æ­¥æä¾›äº†é‡è¦çš„ç ”ç©¶å·¥å…·ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00830v2",
      "published_date": "2025-05-25 16:26:08 UTC",
      "updated_date": "2025-10-24 17:02:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:01:41.188127+00:00"
    },
    {
      "arxiv_id": "2505.19213v1",
      "title": "Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning",
      "title_zh": "åŸºäºè¯¾ç¨‹æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æå‡åŒ»ç–—æ¨ç†èƒ½åŠ›",
      "authors": [
        "Shaohao Rui",
        "Kaitao Chen",
        "Weijie Ma",
        "Xiaosong Wang"
      ],
      "abstract": "Recent advances in reinforcement learning with verifiable, rule-based rewards have greatly enhanced the reasoning capabilities and out-of-distribution generalization of VLMs/LLMs, obviating the need for manually crafted reasoning chains. Despite these promising developments in the general domain, their translation to medical imaging remains limited. Current medical reinforcement fine-tuning (RFT) methods predominantly focus on close-ended VQA, thereby restricting the model's ability to engage in world knowledge retrieval and flexible task adaptation. More critically, these methods fall short of addressing the critical clinical demand for open-ended, reasoning-intensive decision-making. To bridge this gap, we introduce \\textbf{MedCCO}, the first multimodal reinforcement learning framework tailored for medical VQA that unifies close-ended and open-ended data within a curriculum-driven RFT paradigm. Specifically, MedCCO is initially fine-tuned on a diverse set of close-ended medical VQA tasks to establish domain-grounded reasoning capabilities, and is then progressively adapted to open-ended tasks to foster deeper knowledge enhancement and clinical interpretability. We validate MedCCO across eight challenging medical VQA benchmarks, spanning both close-ended and open-ended settings. Experimental results show that MedCCO consistently enhances performance and generalization, achieving a 11.4\\% accuracy gain across three in-domain tasks, and a 5.7\\% improvement on five out-of-domain benchmarks. These findings highlight the promise of curriculum-guided RL in advancing robust, clinically-relevant reasoning in medical multimodal language models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰åŒ»ç–—é¢†åŸŸå¼ºåŒ–å­¦ä¹ å¾®è°ƒ(RFT)æ–¹æ³•ä¸»è¦å±€é™åœ¨é—­å£(close-ended)å¼è§†è§‰é—®ç­”(VQA)ä¸”ç¼ºä¹å¼€æ”¾å¼æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªä¸“ä¸ºåŒ»ç–—è§†è§‰é—®ç­”è®¾è®¡çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ¡†æ¶MedCCOã€‚MedCCOé‡‡ç”¨äº†ä¸€ç§è¯¾ç¨‹é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒèŒƒå¼(curriculum-driven RFT paradigm)ï¼Œå°†é—­å£å¼å’Œå¼€æ”¾å¼æ•°æ®è¿›è¡Œç»Ÿä¸€ã€‚è¯¥æ¡†æ¶é¦–å…ˆåœ¨å¤šæ ·åŒ–çš„é—­å£åŒ»ç–—VQAä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒä»¥å»ºç«‹åŸºäºé¢†åŸŸçŸ¥è¯†çš„æ¨ç†èƒ½åŠ›ï¼Œéšåé€æ­¥é€‚åº”å¼€æ”¾å¼ä»»åŠ¡ä»¥ä¿ƒè¿›æ·±å±‚çš„çŸ¥è¯†å¢å¼ºå’Œä¸´åºŠå¯è§£é‡Šæ€§(clinical interpretability)ã€‚ç ”ç©¶äººå‘˜åœ¨æ¶µç›–é—­å£å’Œå¼€æ”¾è®¾ç½®çš„å…«ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»ç–—VQAåŸºå‡†æµ‹è¯•ä¸­å¯¹MedCCOè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMedCCOåœ¨ä¸‰é¡¹åŸŸå†…ä»»åŠ¡ä¸­å®ç°äº†11.4%çš„å‡†ç¡®ç‡æå‡ï¼Œåœ¨äº”é¡¹åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­æå‡äº†5.7%ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°è¯æ˜äº†è¯¾ç¨‹å¯¼å‘çš„å¼ºåŒ–å­¦ä¹ åœ¨æ¨åŠ¨åŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®ç°ç¨³å¥ä¸”å…·æœ‰ä¸´åºŠç›¸å…³æ€§çš„æ¨ç†æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19213v1",
      "published_date": "2025-05-25 16:20:55 UTC",
      "updated_date": "2025-05-25 16:20:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:00:38.976159+00:00"
    },
    {
      "arxiv_id": "2505.19212v1",
      "title": "When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas",
      "title_zh": "å½“ä¼¦ç†ä¸æ”¶ç›ŠèƒŒç¦»ï¼šé“å¾·åŒ–ç¤¾ä¼šå›°å¢ƒä¸­çš„ LLM æ™ºèƒ½ä½“",
      "authors": [
        "Steffen Backmann",
        "David Guzman Piedrahita",
        "Emanuel Tewolde",
        "Rada Mihalcea",
        "Bernhard SchÃ¶lkopf",
        "Zhijing Jin"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled their use in complex agentic roles, involving decision-making with humans or other agents, making ethical alignment a key AI safety concern. While prior work has examined both LLMs' moral judgment and strategic behavior in social dilemmas, there is limited understanding of how they act when moral imperatives directly conflict with rewards or incentives. To investigate this, we introduce Moral Behavior in Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the prisoner's dilemma and public goods game with morally charged contexts. In MoralSim, we test a range of frontier models across both game structures and three distinct moral framings, enabling a systematic examination of how LLMs navigate social dilemmas in which ethical norms conflict with payoff-maximizing strategies. Our results show substantial variation across models in both their general tendency to act morally and the consistency of their behavior across game types, the specific moral framing, and situational factors such as opponent behavior and survival risks. Crucially, no model exhibits consistently moral behavior in MoralSim, highlighting the need for caution when deploying LLMs in agentic roles where the agent's \"self-interest\" may conflict with ethical expectations. Our code is available at https://github.com/sbackmann/moralsim.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºæ™ºèƒ½ä»£ç†æ‰§è¡Œå†³ç­–ä»»åŠ¡æ—¶ï¼Œå…¶é“å¾·å¯¹é½ï¼ˆethical alignmentï¼‰ä¸åˆ©ç›Šå›æŠ¥å‘ç”Ÿå†²çªæ—¶çš„è¡Œä¸ºè¡¨ç°ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº† Moral Behavior in Social Dilemma Simulation (MoralSim) æ¨¡æ‹Ÿæ¡†æ¶ï¼Œé€šè¿‡åœ¨å›šå¾’å›°å¢ƒ (prisoner's dilemma) å’Œå…¬å…±ç‰©å“åšå¼ˆ (public goods game) ä¸­å¼•å…¥å…·æœ‰é“å¾·å¯¼å‘çš„è¯­å¢ƒï¼Œç³»ç»Ÿè¯„ä¼°äº†å¤šç§å‰æ²¿æ¨¡å‹åœ¨åˆ©ç›Šé©±åŠ¨ä¸é“å¾·å‡†åˆ™èƒŒç¦»æ—¶çš„å†³ç­–é€»è¾‘ã€‚ç»“æœè¡¨æ˜ï¼Œä¸åŒæ¨¡å‹åœ¨é“å¾·å€¾å‘å’Œä¸€è‡´æ€§ä¸Šå·®å¼‚å·¨å¤§ï¼Œå…¶è¡Œä¸ºææ˜“å—åˆ°ç‰¹å®šçš„é“å¾·æ¡†æ¶ã€å¯¹æ‰‹ç­–ç•¥åŠç”Ÿå­˜é£é™©ç­‰æƒ…å¢ƒå› ç´ çš„å½±å“ã€‚å…³é”®ç»“è®ºæ˜¾ç¤ºï¼Œç›®å‰æ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½åœ¨ MoralSim ä¸­è¡¨ç°å‡ºæŒç»­ç¨³å®šçš„é“å¾·è¡Œä¸ºï¼Œè¿™å‡¸æ˜¾äº†åœ¨ LLM ä»£ç†çš„â€œè‡ªæˆ‘åˆ©ç›Šâ€ä¸é“å¾·é¢„æœŸå†²çªæ—¶ï¼Œéƒ¨ç½²æ­¤ç±»ç³»ç»Ÿæ‰€é¢ä¸´çš„å®‰å…¨é£é™©ä¸æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19212v1",
      "published_date": "2025-05-25 16:19:24 UTC",
      "updated_date": "2025-05-25 16:19:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:01:01.137003+00:00"
    },
    {
      "arxiv_id": "2505.19209v2",
      "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search",
      "title_zh": "MOOSE-Chem2ï¼šé€šè¿‡å±‚çº§æœç´¢æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦ç§‘å­¦å‡è®¾å‘ç°ä¸­çš„æé™",
      "authors": [
        "Zonglin Yang",
        "Wanhao Liu",
        "Ben Gao",
        "Yujie Liu",
        "Wei Li",
        "Tong Xie",
        "Lidong Bing",
        "Wanli Ouyang",
        "Erik Cambria",
        "Dongzhan Zhou"
      ],
      "abstract": "Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details. We introduce and formally define the new task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions. We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs' capacity to solve it when maximally leveraged. Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM. To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations. We show that this hierarchical process smooths the reward landscape and enables more effective optimization. Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent literature show that our method consistently outperforms strong baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç»†ç²’åº¦ç§‘å­¦å‡è®¾å‘ç°(fine-grained scientific hypothesis discovery)é¢†åŸŸçš„æ€§èƒ½æé™ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•ç”Ÿæˆçš„å‡è®¾å¾€å¾€ç¼ºä¹å®éªŒæ“ä½œç»†èŠ‚çš„é—®é¢˜ã€‚ä½œè€…å°†æ­¤è¿‡ç¨‹å®šä¹‰ä¸ºä¸€ä¸ªç»„åˆä¼˜åŒ–é—®é¢˜(combinatorial optimization problem)ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ†å±‚æœç´¢æ–¹æ³•(hierarchical search method)ï¼Œé€šè¿‡ä»é€šç”¨æ¦‚å¿µåˆ°å…·ä½“å®éªŒé…ç½®çš„æ¸è¿›å¼ç»†èŠ‚é›†æˆæ¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚ç ”ç©¶é€šè¿‡å®éªŒåˆ†æäº† LLM å†…éƒ¨è¯„åˆ†æ„æˆçš„æ½œå¥–åŠ±å›¾æ™¯(latent reward landscape)ä¸çœŸå®å‡è®¾çš„å¯¹é½ç¨‹åº¦ï¼Œå¹¶éªŒè¯äº†é›†æˆ(ensemble)ç­–ç•¥åœ¨æå‡å¥–åŠ±å›¾æ™¯å¯é æ€§æ–¹é¢çš„ä½œç”¨ã€‚è¯¥æ–¹æ³•åœ¨åŒ…å«ä¸“å®¶æ ‡æ³¨ç»†ç²’åº¦å‡è®¾çš„æ–°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½æŒç»­ä¼˜äºç°æœ‰çš„å¼ºåŸºå‡†æ¨¡å‹ã€‚è¿™è¯æ˜äº†é€šè¿‡åˆ†å±‚ä¼˜åŒ–ç­–ç•¥ï¼ŒLLMs èƒ½å¤Ÿç”Ÿæˆæ›´åŠ ç²¾ç¡®ä¸”å…·æœ‰å®éªŒå¯è¡Œæ€§çš„ç§‘å­¦å‡è®¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.19209v2",
      "published_date": "2025-05-25 16:13:46 UTC",
      "updated_date": "2025-10-27 13:16:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:00:50.140817+00:00"
    },
    {
      "arxiv_id": "2505.19205v2",
      "title": "OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization",
      "title_zh": "OptiMindTuneï¼šé¢å‘æ™ºèƒ½è¶…å‚æ•°ä¼˜åŒ–çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Meher Bhaskar Madiraju",
        "Meher Sai Preetam Madiraju"
      ],
      "abstract": "Hyperparameter optimization (HPO) is a critical yet challenging aspect of machine learning model development, significantly impacting model performance and generalization. Traditional HPO methods often struggle with high dimensionality, complex interdependencies, and computational expense. This paper introduces OptiMindTune, a novel multi-agent framework designed to intelligently and efficiently optimize hyperparameters. OptiMindTune leverages the collaborative intelligence of three specialized AI agents -- a Recommender Agent, an Evaluator Agent, and a Decision Agent -- each powered by Google's Gemini models. These agents address distinct facets of the HPO problem, from model selection and hyperparameter suggestion to robust evaluation and strategic decision-making. By fostering dynamic interactions and knowledge sharing, OptiMindTune aims to converge to optimal hyperparameter configurations more rapidly and robustly than existing single-agent or monolithic approaches. Our framework integrates principles from advanced large language models, and adaptive search to achieve scalable and intelligent AutoML. We posit that this multi-agent paradigm offers a promising avenue for tackling the increasing complexity of modern machine learning model tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OptiMindTuneï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æœºå™¨å­¦ä¹ æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–(Hyperparameter Optimization, HPO)æŒ‘æˆ˜è€Œè®¾è®¡çš„åˆ›æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç”±Googleçš„Geminiæ¨¡å‹é©±åŠ¨çš„ä¸‰ä¸ªä¸“é—¨æ™ºèƒ½ä½“â€”â€”Recommender Agentã€Evaluator Agentå’ŒDecision Agentâ€”â€”çš„åä½œæ™ºèƒ½ï¼Œåˆ†åˆ«å¤„ç†æ¨¡å‹é€‰æ‹©ã€å‚æ•°å»ºè®®ã€é²æ£’è¯„ä¼°å’Œæˆ˜ç•¥å†³ç­–ã€‚é€šè¿‡ä¿ƒè¿›æ™ºèƒ½ä½“é—´çš„åŠ¨æ€äº¤äº’å’ŒçŸ¥è¯†å…±äº«ï¼ŒOptiMindTuneæ—¨åœ¨æ¯”ä¼ ç»Ÿçš„å•æ™ºèƒ½ä½“æˆ–æ•´ä½“æ–¹æ³•æ›´è¿…é€Ÿã€æ›´ç¨³å¥åœ°æ”¶æ•›åˆ°æœ€ä¼˜é…ç½®ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹(Large Language Models)åŸç†ä¸è‡ªé€‚åº”æœç´¢æŠ€æœ¯ï¼Œå®ç°äº†ä¸€ç§å¯æ‰©å±•ä¸”æ™ºèƒ½çš„AutoMLèŒƒå¼ï¼Œä¸ºåº”å¯¹ç°ä»£æ¨¡å‹è°ƒä¼˜ä¸­æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.19205v2",
      "published_date": "2025-05-25 16:05:41 UTC",
      "updated_date": "2025-05-28 15:13:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:00:52.479641+00:00"
    },
    {
      "arxiv_id": "2505.19203v2",
      "title": "EnvSDD: Benchmarking Environmental Sound Deepfake Detection",
      "title_zh": "EnvSDDï¼šç¯å¢ƒéŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Han Yin",
        "Yang Xiao",
        "Rohan Kumar Das",
        "Jisheng Bai",
        "Haohe Liu",
        "Wenwu Wang",
        "Mark D Plumbley"
      ],
      "abstract": "Audio generation systems now create very realistic soundscapes that can enhance media production, but also pose potential risks. Several studies have examined deepfakes in speech or singing voice. However, environmental sounds have different characteristics, which may make methods for detecting speech and singing deepfakes less effective for real-world sounds. In addition, existing datasets for environmental sound deepfake detection are limited in scale and audio types. To address this gap, we introduce EnvSDD, the first large-scale curated dataset designed for this task, consisting of 45.25 hours of real and 316.74 hours of fake audio. The test set includes diverse conditions to evaluate the generalizability, such as unseen generation models and unseen datasets. We also propose an audio deepfake detection system, based on a pre-trained audio foundation model. Results on EnvSDD show that our proposed system outperforms the state-of-the-art systems from speech and singing domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¯å¢ƒéŸ³ Deepfake æ£€æµ‹é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„è¯­éŸ³æˆ–æ­Œå£°æ£€æµ‹æ–¹æ³•å› å£°å­¦ç‰¹æ€§å·®å¼‚éš¾ä»¥ç›´æ¥è¿ç§»ï¼Œä¸”å½“å‰ç¼ºä¹å¤§è§„æ¨¡ä¸“ç”¨æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº† EnvSDDï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºè¯¥ä»»åŠ¡è®¾è®¡çš„å¤§è§„æ¨¡ç­–åˆ’æ•°æ®é›†ï¼ŒåŒ…å«äº† 45.25 å°æ—¶çš„çœŸå®éŸ³é¢‘å’Œ 316.74 å°æ—¶çš„ä¼ªé€ éŸ³é¢‘ã€‚è¯¥æ•°æ®é›†çš„æµ‹è¯•é›†æ¶µç›–äº†å¤šç§å¤æ‚æƒ…å†µï¼ŒåŒ…æ‹¬æœªè§è¿‡çš„ç”Ÿæˆæ¨¡å‹å’Œæ•°æ®é›†ï¼Œæ—¨åœ¨ä¸¥è°¨è¯„ä¼°æ£€æµ‹ç³»ç»Ÿçš„ Generalizabilityã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒ Audio Foundation Model çš„æ£€æµ‹ç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ EnvSDD æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç›®å‰è¯­éŸ³å’Œæ­Œå£°é¢†åŸŸçš„ State-of-the-art ç³»ç»Ÿï¼Œä¸ºç¯å¢ƒéŸ³ä¼ªé€ æ£€æµ‹çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Proceedings of Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.19203v2",
      "published_date": "2025-05-25 16:02:56 UTC",
      "updated_date": "2025-09-29 06:59:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:01:08.110974+00:00"
    },
    {
      "arxiv_id": "2505.19197v3",
      "title": "Structuring the Unstructured: A Multi-Agent System for Extracting and Querying Financial KPIs and Guidance",
      "title_zh": "éç»“æ„åŒ–æ•°æ®ç»“æ„åŒ–ï¼šä¸€ç§ç”¨äºè´¢åŠ¡ KPI ä¸ä¸šç»©æŒ‡å¼•æå–åŠæŸ¥è¯¢çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Chanyeol Choi",
        "Alejandro Lopez-Lira",
        "Yongjae Lee",
        "Jihoon Kwon",
        "Minjae Kim",
        "Juneha Hwang",
        "Minsoo Ha",
        "Chaewoon Kim",
        "Jaeseon Ha",
        "Suyeol Yun",
        "Jin Kim"
      ],
      "abstract": "Extracting structured and quantitative insights from unstructured financial filings is essential in investment research, yet remains time-consuming and resource-intensive. Conventional approaches in practice rely heavily on labor-intensive manual processes, limiting scalability and delaying the research workflow. In this paper, we propose an efficient and scalable method for accurately extracting quantitative insights from unstructured financial documents, leveraging a multi-agent system composed of large language models. Our proposed multi-agent system consists of two specialized agents: the \\emph{Extraction Agent} and the \\emph{Text-to-SQL Agent}. The \\textit{Extraction Agent} automatically identifies key performance indicators from unstructured financial text, standardizes their formats, and verifies their accuracy. On the other hand, the \\textit{Text-to-SQL Agent} generates executable SQL statements from natural language queries, allowing users to access structured data accurately without requiring familiarity with the database schema. Through experiments, we demonstrate that our proposed system effectively transforms unstructured text into structured data accurately and enables precise retrieval of key information. First, we demonstrate that our system achieves approximately 95\\% accuracy in transforming financial filings into structured data, matching the performance level typically attained by human annotators. Second, in a human evaluation of the retrieval task -- where natural language queries are used to search information from structured data -- 91\\% of the responses were rated as correct by human evaluators. In both evaluations, our system generalizes well across financial document types, consistently delivering reliable performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨ä»éç»“æ„åŒ–è´¢åŠ¡æ–‡ä»¶ä¸­é«˜æ•ˆæå–å¹¶æŸ¥è¯¢ç»“æ„åŒ–å…³é”®ç»©æ•ˆæŒ‡æ ‡(KPIs)å’Œä¸šç»©æŒ‡å¼•(Guidance)ã€‚è¯¥ç³»ç»Ÿç”±ä¸¤ä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“ç»„æˆï¼šExtraction Agentè´Ÿè´£è‡ªåŠ¨è¯†åˆ«è´¢åŠ¡æ–‡æœ¬ä¸­çš„KPIså¹¶è¿›è¡Œæ ¼å¼æ ‡å‡†åŒ–ä¸å‡†ç¡®æ€§éªŒè¯ï¼Œè€ŒText-to-SQL Agentåˆ™å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„SQLè¯­å¥ï¼Œä½¿ç”¨æˆ·æ— éœ€ç†Ÿæ‚‰æ•°æ®åº“æ¨¡å¼å³å¯ç²¾å‡†è®¿é—®æ•°æ®ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸæŠ•èµ„ç ”ç©¶ä¸­ä¾èµ–äººå·¥å¤„ç†å¯¼è‡´çš„å¯æ‰©å±•æ€§å·®å’Œå·¥ä½œæµå»¶è¿Ÿç­‰é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨æ•°æ®ç»“æ„åŒ–è½¬æ¢ä»»åŠ¡ä¸­è¾¾åˆ°äº†çº¦95%çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½ä¸äººç±»æ ‡æ³¨å‘˜ç›¸å½“ã€‚åœ¨é’ˆå¯¹æ£€ç´¢ä»»åŠ¡çš„äººå·¥è¯„ä¼°ä¸­ï¼Œ91%çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢å“åº”è¢«è¯„å®šä¸ºæ­£ç¡®ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿåœ¨ä¸åŒç±»å‹çš„è´¢åŠ¡æ–‡æ¡£ä¸­å‡å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤ŸæŒç»­æä¾›å¯é ä¸”ç²¾å‡†çš„é‡åŒ–æ´å¯Ÿã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, FinIR'25",
      "pdf_url": "https://arxiv.org/pdf/2505.19197v3",
      "published_date": "2025-05-25 15:45:46 UTC",
      "updated_date": "2025-06-26 04:56:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:01:29.321557+00:00"
    },
    {
      "arxiv_id": "2505.19195v1",
      "title": "CardioCoT: Hierarchical Reasoning for Multimodal Survival Analysis",
      "title_zh": "CardioCoTï¼šé¢å‘å¤šæ¨¡æ€ç”Ÿå­˜åˆ†æçš„å±‚çº§æ¨ç†",
      "authors": [
        "Shaohao Rui",
        "Haoyang Su",
        "Jinyi Xiang",
        "Lian-Ming Wu",
        "Xiaosong Wang"
      ],
      "abstract": "Accurate prediction of major adverse cardiovascular events recurrence risk in acute myocardial infarction patients based on postoperative cardiac MRI and associated clinical notes is crucial for precision treatment and personalized intervention. Existing methods primarily focus on risk stratification capability while overlooking the need for intermediate robust reasoning and model interpretability in clinical practice. Moreover, end-to-end risk prediction using LLM/VLM faces significant challenges due to data limitations and modeling complexity. To bridge this gap, we propose CardioCoT, a novel two-stage hierarchical reasoning-enhanced survival analysis framework designed to enhance both model interpretability and predictive performance. In the first stage, we employ an evidence-augmented self-refinement mechanism to guide LLM/VLMs in generating robust hierarchical reasoning trajectories based on associated radiological findings. In the second stage, we integrate the reasoning trajectories with imaging data for risk model training and prediction. CardioCoT demonstrates superior performance in MACE recurrence risk prediction while providing interpretable reasoning processes, offering valuable insights for clinical decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ€¥æ€§å¿ƒè‚Œæ¢—æ­» (AMI) æ‚£è€…ï¼Œæ—¨åœ¨åˆ©ç”¨æœ¯åå¿ƒè„ç£å…±æŒ¯æˆåƒ (Cardiac MRI) å’Œä¸´åºŠè®°å½•å‡†ç¡®é¢„æµ‹ä¸»è¦ä¸è‰¯å¿ƒè¡€ç®¡äº‹ä»¶ (MACE) çš„å¤å‘é£é™©ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ç¼ºä¹æ¨ç†å¯è§£é‡Šæ€§ä»¥åŠå¤§è¯­è¨€æ¨¡å‹ (LLM/VLM) ç›´æ¥è¿›è¡Œç«¯åˆ°ç«¯å»ºæ¨¡éš¾åº¦å¤§çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº† CardioCoTï¼Œä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µåˆ†å±‚æ¨ç†å¢å¼ºç”Ÿå­˜åˆ†ææ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè¯¥æ¡†æ¶é‡‡ç”¨è¯æ®å¢å¼ºçš„è‡ªæˆ‘ç²¾ç‚¼æœºåˆ¶ (Evidence-augmented self-refinement) å¼•å¯¼æ¨¡å‹æ ¹æ®æ”¾å°„å­¦å‘ç°ç”Ÿæˆç¨³å¥çš„åˆ†å±‚æ¨ç†è½¨è¿¹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œç³»ç»Ÿå°†ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ä¸å½±åƒæ•°æ®é›†æˆï¼Œå…±åŒç”¨äºé£é™©æ¨¡å‹çš„è®­ç»ƒä¸é¢„æµ‹ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCardioCoT åœ¨ MACE å¤å‘é£é™©é¢„æµ‹ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ä»…æå‡äº†é¢„æµ‹å‡†ç¡®åº¦ï¼Œè¿˜é€šè¿‡æä¾›é€æ˜çš„æ¨ç†è¿‡ç¨‹ä¸ºä¸´åºŠå†³ç­–æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19195v1",
      "published_date": "2025-05-25 15:41:18 UTC",
      "updated_date": "2025-05-25 15:41:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:01:15.001124+00:00"
    },
    {
      "arxiv_id": "2505.19194v3",
      "title": "Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation",
      "title_zh": "æ›²ç‡åŠ¨æ€é»‘ç›’æ”»å‡»ï¼šåŸºäºåŠ¨æ€æ›²ç‡ä¼°è®¡çš„å¯¹æŠ—é²æ£’æ€§å†æ¢",
      "authors": [
        "Peiran Sun"
      ],
      "abstract": "Adversarial attack reveals the vulnerability of deep learning models. It is assumed that high curvature may give rise to rough decision boundary and thus result in less robust models. However, the most commonly used \\textit{curvature} is the curvature of loss function, scores or other parameters from within the model as opposed to decision boundary curvature, since the former can be relatively easily formed using second order derivative. In this paper, we propose a new query-efficient method, dynamic curvature estimation (DCE), to estimate the decision boundary curvature in a black-box setting. Our approach is based on CGBA, a black-box adversarial attack. By performing DCE on a wide range of classifiers, we discovered, statistically, a connection between decision boundary curvature and adversarial robustness. We also propose a new attack method, curvature dynamic black-box attack (CDBA) with improved performance using the estimated curvature.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„è„†å¼±æ€§ï¼ŒæŒ‡å‡ºå†³ç­–è¾¹ç•Œçš„æ›²ç‡(curvature)ä¸æ¨¡å‹çš„é²æ£’æ€§(robustness)å¯†åˆ‡ç›¸å…³ã€‚ä»¥å¾€ç ”ç©¶å¤šå…³æ³¨æ˜“äºé€šè¿‡äºŒé˜¶å¯¼æ•°è®¡ç®—çš„æŸå¤±å‡½æ•°æˆ–å‚æ•°æ›²ç‡ï¼Œè€Œå¿½è§†äº†æ›´æœ¬è´¨ä½†éš¾ä»¥ç›´æ¥è·å–çš„å†³ç­–è¾¹ç•Œæ›²ç‡ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æŸ¥è¯¢é«˜æ•ˆçš„åŠ¨æ€æ›²ç‡ä¼°è®¡(Dynamic Curvature Estimation, DCE)æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨é»‘ç›’(black-box)è®¾ç½®ä¸‹å‡†ç¡®ä¼°ç®—å†³ç­–è¾¹ç•Œæ›²ç‡ã€‚è¯¥æ–¹æ³•åŸºäºCGBAå¯¹æŠ—æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡å¯¹å¤šç§åˆ†ç±»å™¨çš„å®éªŒåˆ†æï¼Œåœ¨ç»Ÿè®¡å­¦ä¸Šè¯å®äº†å†³ç­–è¾¹ç•Œæ›²ç‡ä¸å¯¹æŠ—é²æ£’æ€§ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº†æ›²ç‡åŠ¨æ€é»‘ç›’æ”»å‡»(Curvature Dynamic Black-box Attack, CDBA)ï¼Œé€šè¿‡åˆ©ç”¨ä¼°è®¡çš„æ›²ç‡ä¿¡æ¯æ˜¾è‘—æå‡äº†æ”»å‡»æ€§èƒ½ã€‚è¿™ä¸€ç ”ç©¶ä¸ä»…æ­ç¤ºäº†æ¨¡å‹è„†å¼±æ€§çš„å‡ ä½•ç‰¹å¾ï¼Œä¹Ÿä¸ºè¯„ä¼°å’Œå¢å¼ºæ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„å®‰å…¨æ€§æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19194v3",
      "published_date": "2025-05-25 15:41:11 UTC",
      "updated_date": "2025-11-28 13:10:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:01:42.188550+00:00"
    },
    {
      "arxiv_id": "2505.19190v1",
      "title": "I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts",
      "title_zh": "I2MoEï¼šå¯è§£é‡Šçš„å¤šæ¨¡æ€äº¤äº’æ„ŸçŸ¥æ··åˆä¸“å®¶æ¨¡å‹",
      "authors": [
        "Jiayi Xin",
        "Sukwon Yun",
        "Jie Peng",
        "Inyoung Choi",
        "Jenna L. Ballard",
        "Tianlong Chen",
        "Qi Long"
      ],
      "abstract": "Modality fusion is a cornerstone of multimodal learning, enabling information integration from diverse data sources. However, vanilla fusion methods are limited by (1) inability to account for heterogeneous interactions between modalities and (2) lack of interpretability in uncovering the multimodal interactions inherent in the data. To this end, we propose I2MoE (Interpretable Multimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework designed to enhance modality fusion by explicitly modeling diverse multimodal interactions, as well as providing interpretation on a local and global level. First, I2MoE utilizes different interaction experts with weakly supervised interaction losses to learn multimodal interactions in a data-driven way. Second, I2MoE deploys a reweighting model that assigns importance scores for the output of each interaction expert, which offers sample-level and dataset-level interpretation. Extensive evaluation of medical and general multimodal datasets shows that I2MoE is flexible enough to be combined with different fusion techniques, consistently improves task performance, and provides interpretation across various real-world scenarios. Code is available at https://github.com/Raina-Xin/I2MoE.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†I2MoE (Interpretable Multimodal Interaction-aware Mixture-of-Experts)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºæ¨¡æ€èåˆ(Modality fusion)çš„ç«¯åˆ°ç«¯æ··åˆä¸“å®¶æ¨¡å‹(MoE)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é’ˆå¯¹ä¼ ç»Ÿèåˆæ–¹æ³•éš¾ä»¥å»ºæ¨¡æ¨¡æ€é—´å¼‚æ„äº¤äº’(Heterogeneous interactions)ä¸”ç¼ºä¹è§£é‡Šæ€§çš„å±€é™æ€§ï¼Œé€šè¿‡å¼•å…¥ä¸åŒçš„äº¤äº’ä¸“å®¶å¹¶åˆ©ç”¨å¼±ç›‘ç£äº¤äº’æŸå¤±(Weakly supervised interaction losses)ä»¥æ•°æ®é©±åŠ¨çš„æ–¹å¼å­¦ä¹ å¤šæ¨¡æ€äº¤äº’ã€‚åŒæ—¶ï¼ŒI2MoE éƒ¨ç½²äº†ä¸€ä¸ªé‡åŠ æƒæ¨¡å‹(Reweighting model)ä¸ºå„ä¸“å®¶è¾“å‡ºåˆ†é…é‡è¦æ€§å¾—åˆ†ï¼Œä»è€Œåœ¨æ ·æœ¬çº§åˆ«å’Œæ•°æ®é›†çº§åˆ«æä¾›å¯è§†åŒ–è§£é‡Šã€‚åœ¨åŒ»ç–—å’Œé€šç”¨å¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒI2MoE å…·æœ‰æé«˜çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿä¸å¤šç§èåˆæŠ€æœ¯ç»“åˆå¹¶æŒç»­æå‡ä»»åŠ¡æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ä»…åœ¨å¤šé¡¹ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜å¼‚è¡¨ç°ï¼Œè¿˜ä¸ºå¤æ‚çœŸå®åœºæ™¯ä¸‹çš„æ¨¡æ€äº¤äº’æä¾›äº†æ·±å…¥çš„æœºåˆ¶æ´å¯Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025 Poster",
      "pdf_url": "https://arxiv.org/pdf/2505.19190v1",
      "published_date": "2025-05-25 15:34:29 UTC",
      "updated_date": "2025-05-25 15:34:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:01:31.592852+00:00"
    },
    {
      "arxiv_id": "2505.19187v2",
      "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling",
      "title_zh": "LIMOProï¼šé¢å‘é«˜æ•ˆä¸”æœ‰æ•ˆæµ‹è¯•æ—¶æ‰©å±•çš„æ¨ç†ç²¾ç‚¼",
      "authors": [
        "Yang Xiao",
        "Jiashuo Wang",
        "Ruifeng Yuan",
        "Chunpu Xu",
        "Kaishuai Xu",
        "Wenjie Li",
        "Pengfei Liu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs). However, these reasoning chains often contain verbose elements that mirror human problem-solving, categorized as progressive reasoning (the essential solution development path) and functional elements (verification processes, alternative solution approaches, and error corrections). While progressive reasoning is crucial, the functional elements significantly increase computational demands during test-time inference. We introduce PIR (Perplexity-based Importance Refinement), a principled framework that quantitatively evaluates the importance of each reasoning step based on its impact on answer prediction confidence. PIR systematically identifies and selectively prunes only low-importance functional steps while preserving progressive reasoning components, creating optimized training data that maintains the integrity of the core solution path while reducing verbosity. Models fine-tuned on PIR-optimized data exhibit superior test-time scaling properties, generating more concise reasoning chains while achieving improved accuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to -41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond). Our approach demonstrates strong generalizability across different model sizes, data sources, and token budgets, offering a practical solution for deploying reasoning-capable LLMs in scenarios where efficient test-time scaling, response time, and computational efficiency are valuable constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LIMOProï¼Œå¹¶å¼•å…¥äº†åä¸ºPIR (Perplexity-based Importance Refinement) çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–æ¨ç†é“¾æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„æµ‹è¯•æ—¶ç¼©æ”¾ (Test-time Scaling) æ•ˆç‡ã€‚é’ˆå¯¹ Chain-of-Thought (CoT) æ•°æ®ä¸­å†—ä½™çš„åŠŸèƒ½æ€§å…ƒç´ ï¼ŒPIR é€šè¿‡é‡åŒ–è¯„ä¼°å„æ¨ç†æ­¥éª¤å¯¹ç­”æ¡ˆé¢„æµ‹ç½®ä¿¡åº¦çš„å½±å“ï¼Œç³»ç»Ÿæ€§åœ°è¯†åˆ«å¹¶ä¿®å‰ªä½é‡è¦æ€§æ­¥éª¤ã€‚è¯¥æ–¹æ³•åœ¨ä¿ç•™æ ¸å¿ƒæ¸è¿›å¼æ¨ç† (Progressive Reasoning) è·¯å¾„çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€å¹¶é™ä½äº†æ¨ç†å»¶è¿Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç» PIR ä¼˜åŒ–æ•°æ®å¾®è°ƒçš„æ¨¡å‹åœ¨ AIMEã€AMC å’Œ GPQA Diamond ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸ä»…å‡†ç¡®ç‡æå‡äº† 0.9% è‡³ 6.6%ï¼ŒToken ä½¿ç”¨é‡ä¹Ÿå¤§å¹…å‡å°‘äº† 3% è‡³ 41%ã€‚æ­¤é¡¹å·¥ä½œè¯æ˜äº†ç²¾ç®€æ¨ç†é“¾åœ¨ç»´æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸éƒ¨ç½²æ•ˆç‡ï¼Œä¸ºé«˜æ•ˆæ¨ç†æ¨¡å‹çš„å®ç°æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.19187v2",
      "published_date": "2025-05-25 15:17:57 UTC",
      "updated_date": "2025-10-21 10:15:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:02:16.087079+00:00"
    },
    {
      "arxiv_id": "2505.19186v1",
      "title": "PosePilot: An Edge-AI Solution for Posture Correction in Physical Exercises",
      "title_zh": "PosePilotï¼šé¢å‘ä½“è‚²é”»ç‚¼å§¿åŠ¿çº æ­£çš„è¾¹ç¼˜ AI è§£å†³æ–¹æ¡ˆ",
      "authors": [
        "Rushiraj Gadhvi",
        "Priyansh Desai",
        "Siddharth"
      ],
      "abstract": "Automated pose correction remains a significant challenge in AI-driven fitness systems, despite extensive research in activity recognition. This work presents PosePilot, a novel system that integrates pose recognition with real-time personalized corrective feedback, overcoming the limitations of traditional fitness solutions. Using Yoga, a discipline requiring precise spatio-temporal alignment as a case study, we demonstrate PosePilot's ability to analyze complex physical movements. Designed for deployment on edge devices, PosePilot can be extended to various at-home and outdoor exercises. We employ a Vanilla LSTM, allowing the system to capture temporal dependencies for pose recognition. Additionally, a BiLSTM with multi-head Attention enhances the model's ability to process motion contexts, selectively focusing on key limb angles for accurate error detection while maintaining computational efficiency. As part of this work, we introduce a high-quality video dataset used for evaluating our models. Most importantly, PosePilot provides instant corrective feedback at every stage of a movement, ensuring precise posture adjustments throughout the exercise routine. The proposed approach 1) performs automatic human posture recognition, 2) provides personalized posture correction feedback at each instant which is crucial in Yoga, and 3) offers a lightweight and robust posture correction model feasible for deploying on edge devices in real-world environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PosePilotï¼Œä¸€ç§ä¸“ä¸ºè¾¹ç¼˜AI(Edge-AI)è®¾è®¡çš„è¿åŠ¨å§¿åŠ¿çŸ«æ­£ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³AIé©±åŠ¨å¥èº«ç³»ç»Ÿåœ¨å§¿åŠ¿è¯†åˆ«ä¸å®æ—¶çº é”™æ–¹é¢çš„å±€é™æ€§ã€‚ç³»ç»Ÿä»¥å¯¹æ—¶ç©ºå¯¹é½è¦æ±‚æé«˜çš„ç‘œä¼½ä¸ºæ¡ˆä¾‹ï¼Œé‡‡ç”¨Vanilla LSTMæ•æ‰åŠ¨ä½œçš„æ—¶åºä¾èµ–æ€§ï¼Œå¹¶åˆ©ç”¨å¸¦æœ‰å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶(multi-head Attention)çš„BiLSTMå¢å¼ºå¯¹è¿åŠ¨è¯­å¢ƒçš„å¤„ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹å…³é”®è‚¢ä½“è§’åº¦è¿›è¡Œé€‰æ‹©æ€§å…³æ³¨ï¼Œåœ¨ç¡®ä¿è®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°ç²¾ç¡®çš„åŠ¨ä½œé”™è¯¯æ£€æµ‹ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå®ç°äº†è‡ªåŠ¨åŒ–çš„å§¿åŠ¿è¯†åˆ«ï¼Œå¹¶èƒ½åœ¨è¿åŠ¨çš„æ¯ä¸€ä¸ªç¬é—´æä¾›å³æ—¶çš„ä¸ªæ€§åŒ–çŸ«æ­£åé¦ˆã€‚æ­¤å¤–ï¼ŒPosePilotå…·å¤‡è½»é‡åŒ–ä¸é²æ£’æ€§ç‰¹å¾ï¼Œé…åˆç ”ç©¶ä¸­å‘å¸ƒçš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œä¸ºåœ¨çœŸå®ç¯å¢ƒçš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²é«˜æ•ˆè¿åŠ¨è¾…åŠ©æŠ€æœ¯æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication at IBPRIA 2025 Conference in Coimbra, Portugal",
      "pdf_url": "https://arxiv.org/pdf/2505.19186v1",
      "published_date": "2025-05-25 15:13:54 UTC",
      "updated_date": "2025-05-25 15:13:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:02:23.361005+00:00"
    },
    {
      "arxiv_id": "2505.19184v3",
      "title": "When Two LLMs Debate, Both Think They'll Win",
      "title_zh": "å½“ä¸¤ä¸ªå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¾©è®ºï¼ŒåŒæ–¹å‡è®¤ä¸ºè‡ªå·±å°†è·èƒœ",
      "authors": [
        "Pradyumna Shyama Prasad",
        "Minh Nhat Nguyen"
      ],
      "abstract": "Can LLMs accurately adjust their confidence when facing opposition? Building on previous studies measuring calibration on static fact-based question-answering tasks, we evaluate Large Language Models (LLMs) in a dynamic, adversarial debate setting, uniquely combining two realistic factors: (a) a multi-turn format requiring models to update beliefs as new information emerges, and (b) a zero-sum structure to control for task-related uncertainty, since mutual high-confidence claims imply systematic overconfidence. We organized 60 three-round policy debates among ten state-of-the-art LLMs, with models privately rating their confidence (0-100) in winning after each round. We observed five concerning patterns: (1) Systematic overconfidence: models began debates with average initial confidence of 72.9% vs. a rational 50% baseline. (2) Confidence escalation: rather than reducing confidence as debates progressed, debaters increased their win probabilities, averaging 83% by the final round. (3) Mutual overestimation: in 61.7% of debates, both sides simultaneously claimed >=75% probability of victory, a logical impossibility. (4) Persistent self-debate bias: models debating identical copies increased confidence from 64.1% to 75.2%; even when explicitly informed their chance of winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5) Misaligned private reasoning: models' private scratchpad thoughts sometimes differed from their public confidence ratings, raising concerns about faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the ability to accurately self-assess or update their beliefs in dynamic, multi-turn tasks; a major concern as LLMs are now increasingly deployed without careful review in assistant and agentic roles.\n  Code for our experiments is available at https://github.com/pradyuprasad/llms_overconfidence",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŠ¨æ€ã€å¯¹æŠ—æ€§è¾©è®º(adversarial debate)èƒŒæ™¯ä¸‹å‡†ç¡®è°ƒæ•´ç½®ä¿¡åº¦çš„èƒ½åŠ›ã€‚é€šè¿‡ç»„ç»‡10ç§æœ€å…ˆè¿›æ¨¡å‹è¿›è¡Œ60åœºå¤šè½®æ”¿ç­–è¾©è®ºå¹¶è®°å½•å…¶ç§ä¸‹èƒœç‡è¯„åˆ†ï¼Œç ”ç©¶è€…å‘ç°äº†äº”ä¸ªå…³é”®é—®é¢˜ã€‚å®éªŒæ˜¾ç¤ºæ¨¡å‹å­˜åœ¨æ˜æ˜¾çš„ç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡(Systematic overconfidence)ï¼Œä¸”ç½®ä¿¡åº¦ä¼šéšè¾©è®ºæ¨è¿›è€Œä¸åˆç†åœ°æŒç»­å‡çº§(Confidence escalation)ã€‚å¤šæ•°æƒ…å†µä¸‹è¾©è®ºåŒæ–¹è¡¨ç°å‡ºé€»è¾‘çŸ›ç›¾çš„ç›¸äº’é«˜ä¼°(Mutual overestimation)ï¼Œç”šè‡³åœ¨è‡ªæˆ‘è¾©è®º(Self-debate bias)æˆ–å·²çŸ¥èƒœç‡å‡è¡¡çš„æƒ…å†µä¸‹ä»ç›²ç›®å¢åŠ ä¿¡å¿ƒã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„ç§ä¸‹é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†ä¸å…¶å…¬å¼€ç½®ä¿¡åº¦è¯„çº§å­˜åœ¨ä¸ä¸€è‡´ï¼Œåæ˜ å‡ºæ¨ç†çš„å¿ å®æ€§é—®é¢˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨å¤šè½®äº¤äº’ä¸­ç¼ºä¹å‡†ç¡®è‡ªè¯„å’Œæ›´æ–°ä¿¡å¿µçš„èƒ½åŠ›ã€‚è¿™æ­ç¤ºäº†LLMsåœ¨ä½œä¸ºåŠ©æ‰‹æˆ–æ™ºèƒ½ä½“(agentic roles)è¢«å¹¿æ³›éƒ¨ç½²æ—¶ï¼Œå› ç¼ºä¹ä¸¥è°¨å®¡æŸ¥è€Œå­˜åœ¨çš„é‡å¤§å¯é æ€§éšæ‚£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19184v3",
      "published_date": "2025-05-25 15:06:17 UTC",
      "updated_date": "2025-06-09 17:54:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:02:31.284394+00:00"
    },
    {
      "arxiv_id": "2505.19178v1",
      "title": "Saliency-guided Emotion Modeling: Predicting Viewer Reactions from Video Stimuli",
      "title_zh": "æ˜¾è‘—æ€§å¼•å¯¼çš„æƒ…ç»ªå»ºæ¨¡ï¼šä»è§†é¢‘åˆºæ¿€ä¸­é¢„æµ‹è§‚çœ‹è€…ååº”",
      "authors": [
        "Akhila Yaragoppa",
        "Siddharth"
      ],
      "abstract": "Understanding the emotional impact of videos is crucial for applications in content creation, advertising, and Human-Computer Interaction (HCI). Traditional affective computing methods rely on self-reported emotions, facial expression analysis, and biosensing data, yet they often overlook the role of visual saliency -- the naturally attention-grabbing regions within a video. In this study, we utilize deep learning to introduce a novel saliency-based approach to emotion prediction by extracting two key features: saliency area and number of salient regions. Using the HD2S saliency model and OpenFace facial action unit analysis, we examine the relationship between video saliency and viewer emotions. Our findings reveal three key insights: (1) Videos with multiple salient regions tend to elicit high-valence, low-arousal emotions, (2) Videos with a single dominant salient region are more likely to induce low-valence, high-arousal responses, and (3) Self-reported emotions often misalign with facial expression-based emotion detection, suggesting limitations in subjective reporting. By leveraging saliency-driven insights, this work provides a computationally efficient and interpretable alternative for emotion modeling, with implications for content creation, personalized media experiences, and affective computing research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰æ˜¾è‘—æ€§(Visual Saliency)çš„æƒ…ç»ªé¢„æµ‹æ–°æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯åˆ†æè§†é¢‘å†…å®¹å¯¹è§‚ä¼—æƒ…ç»ªçš„å½±å“ã€‚ç ”ç©¶é€šè¿‡ HD2S æ˜¾è‘—æ€§æ¨¡å‹æå–äº†æ˜¾è‘—æ€§åŒºåŸŸé¢ç§¯(Saliency area)å’Œæ˜¾è‘—æ€§åŒºåŸŸæ•°é‡(Number of salient regions)ä¸¤ä¸ªæ ¸å¿ƒç‰¹å¾ï¼Œå¹¶ç»“åˆ OpenFace è¿›è¡Œé¢éƒ¨åŠ¨ä½œå•å…ƒåˆ†æä»¥æ¢ç©¶å…¶ä¸è§‚ä¼—æƒ…ç»ªçš„å…³è”ã€‚ç ”ç©¶å‘ç°ï¼ŒåŒ…å«å¤šä¸ªæ˜¾è‘—æ€§åŒºåŸŸçš„è§†é¢‘å€¾å‘äºè¯±å‘é«˜æ­£æ€§å€¼(High-valence)å’Œä½å”¤é†’åº¦(Low-arousal)çš„æƒ…ç»ªï¼Œè€Œå…·æœ‰å•ä¸€ä¸»å¯¼æ˜¾è‘—æ€§åŒºåŸŸçš„è§†é¢‘åˆ™æ›´æ˜“å¼•èµ·ä½æ­£æ€§å€¼(Low-valence)å’Œé«˜å”¤é†’åº¦(High-arousal)çš„ååº”ã€‚æ­¤å¤–ï¼Œå®éªŒæ­ç¤ºäº†è‡ªæˆ‘æŠ¥å‘Šæƒ…ç»ªä¸é¢éƒ¨è¡¨æƒ…æ£€æµ‹ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼ŒæŒ‡å‡ºäº†ä¸»è§‚æŠ¥å‘Šçš„å±€é™æ€§ã€‚è¿™ä¸€æ˜¾è‘—æ€§é©±åŠ¨(Saliency-driven)çš„è§è§£ä¸ºæƒ…ç»ªå»ºæ¨¡æä¾›äº†ä¸€ç§è®¡ç®—é«˜æ•ˆä¸”å…·å¯è§£é‡Šæ€§çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨å†…å®¹åˆ›ä½œã€ä¸ªæ€§åŒ–åª’ä½“ä½“éªŒåŠæƒ…æ„Ÿè®¡ç®—(Affective Computing)é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication at IBPRIA 2025 Conference in Coimbra, Portugal",
      "pdf_url": "https://arxiv.org/pdf/2505.19178v1",
      "published_date": "2025-05-25 14:52:36 UTC",
      "updated_date": "2025-05-25 14:52:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:02:10.639628+00:00"
    },
    {
      "arxiv_id": "2505.19173v1",
      "title": "Investigating Pedagogical Teacher and Student LLM Agents: Genetic Adaptation Meets Retrieval Augmented Generation Across Learning Style",
      "title_zh": "æ¢ç©¶æ•™å­¦å‹æ•™å¸ˆä¸å­¦ç”Ÿå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼šè·¨å­¦ä¹ é£æ ¼ä¸‹çš„é—ä¼ è‡ªé€‚åº”ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆä¹‹èåˆ",
      "authors": [
        "Debdeep Sanyal",
        "Agniva Maiti",
        "Umakanta Maharana",
        "Dhruv Kumar",
        "Ankur Mali",
        "C. Lee Giles",
        "Murari Mandal"
      ],
      "abstract": "Effective teaching requires adapting instructional strategies to accommodate the diverse cognitive and behavioral profiles of students, a persistent challenge in education and teacher training. While Large Language Models (LLMs) offer promise as tools to simulate such complex pedagogical environments, current simulation frameworks are limited in two key respects: (1) they often reduce students to static knowledge profiles, and (2) they lack adaptive mechanisms for modeling teachers who evolve their strategies in response to student feedback. To address these gaps, \\textbf{we introduce a novel simulation framework that integrates LLM-based heterogeneous student agents with a self-optimizing teacher agent}. The teacher agent's pedagogical policy is dynamically evolved using a genetic algorithm, allowing it to discover and refine effective teaching strategies based on the aggregate performance of diverse learners. In addition, \\textbf{we propose Persona-RAG}, a Retrieval Augmented Generation module that enables student agents to retrieve knowledge tailored to their individual learning styles. Persona-RAG preserves the retrieval accuracy of standard RAG baselines while enhancing personalization, an essential factor in modeling realistic educational scenarios. Through extensive experiments, we demonstrate how our framework supports the emergence of distinct and interpretable teaching patterns when interacting with varied student populations. Our results highlight the potential of LLM-driven simulations to inform adaptive teaching practices and provide a testbed for training human educators in controlled, data-driven environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„æ¨¡æ‹Ÿæ¡†æ¶ï¼Œé›†æˆäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¼‚è´¨å­¦ç”Ÿæ™ºèƒ½ä½“ä¸è‡ªä¼˜åŒ–æ•™å¸ˆæ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ•™è‚²æ¨¡æ‹Ÿä¸­å­¦ç”Ÿç”»åƒé™æ€åŒ–åŠæ•™å¸ˆç¼ºä¹è‡ªé€‚åº”æœºåˆ¶çš„é—®é¢˜ã€‚æ•™å¸ˆæ™ºèƒ½ä½“çš„æ•™å­¦ç­–ç•¥é€šè¿‡é—ä¼ ç®—æ³•(Genetic Algorithm)è¿›è¡ŒåŠ¨æ€æ¼”åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®å¤šæ ·åŒ–å­¦ä¹ è€…çš„ç»¼åˆè¡¨ç°ä¸æ–­ç²¾ç‚¼æœ‰æ•ˆçš„æ•™å­¦æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†Persona-RAGæ¨¡å—ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ä¸ºå­¦ç”Ÿæ™ºèƒ½ä½“æä¾›ä¸å…¶å­¦ä¹ é£æ ¼åŒ¹é…çš„å®šåˆ¶åŒ–çŸ¥è¯†ï¼Œåœ¨ç»´æŒæ£€ç´¢å‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—å¢å¼ºäº†ä¸ªæ€§åŒ–æ°´å¹³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨ä¸ä¸åŒå­¦ç”Ÿç¾¤ä½“çš„äº’åŠ¨ä¸­å±•ç°å‡ºç‹¬ç‰¹ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„æ•™å­¦æ¨¡å¼ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†LLMé©±åŠ¨çš„æ¨¡æ‹Ÿåœ¨æ”¯æŒè‡ªé€‚åº”æ•™å­¦å®è·µä»¥åŠä¸ºæ•™è‚²å·¥ä½œè€…æä¾›å—æ§ã€æ•°æ®é©±åŠ¨çš„è®­ç»ƒç¯å¢ƒæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "38 Pages",
      "pdf_url": "https://arxiv.org/pdf/2505.19173v1",
      "published_date": "2025-05-25 14:45:35 UTC",
      "updated_date": "2025-05-25 14:45:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:02:39.212372+00:00"
    },
    {
      "arxiv_id": "2505.19167v2",
      "title": "Amplifying Human Creativity and Problem Solving with AI Through Generative Collective Intelligence",
      "title_zh": "é€šè¿‡ç”Ÿæˆå¼é›†ä½“æ™ºèƒ½åˆ©ç”¨ AI å¢å¼ºäººç±»åˆ›é€ åŠ›ä¸é—®é¢˜è§£å†³èƒ½åŠ›",
      "authors": [
        "Thomas P. Kehler",
        "Scott E. Page",
        "Alex Pentland",
        "Martin Reeves",
        "John Seely Brown"
      ],
      "abstract": "We propose a general framework for human-AI collaboration that amplifies the distinct capabilities of both types of intelligence. We refer to this as Generative Collective Intelligence (GCI). GCI employs AI in dual roles: as interactive agents and as technology that accumulates, organizes, and leverages knowledge. In this second role, AI creates a cognitive bridge between human reasoning and AI models. The AI functions as a social and cultural technology that enables groups to solve complex problems through structured collaboration that transcends traditional communication barriers. We argue that GCI can overcome limitations of purely algorithmic approaches to problem-solving and decision-making. We describe the mathematical foundations of GCI, based on the law of comparative judgment and minimum regret principles, and briefly illustrate its applications across various domains, including climate adaptation, healthcare transformation, and civic participation. By combining human creativity with AI's computational capabilities, GCI offers a promising approach to addressing complex societal challenges that neither humans nor machines can solve alone.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ç”Ÿæˆå¼é›†ä½“æ™ºèƒ½(Generative Collective Intelligence, GCI)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æ”¾å¤§äººç±»å’Œäººå·¥æ™ºèƒ½(AI)å„è‡ªç‹¬ç‰¹èƒ½åŠ›çš„é€šç”¨åä½œæ¡†æ¶ã€‚GCIå°†AIå®šä¹‰ä¸ºåŒé‡è§’è‰²ï¼šæ—¢ä½œä¸ºäº¤äº’æ™ºèƒ½ä½“(interactive agents)ï¼Œä¹Ÿä½œä¸ºç§¯ç´¯å’Œç»„ç»‡çŸ¥è¯†çš„æŠ€æœ¯æ‰‹æ®µï¼Œä»è€Œåœ¨äººç±»æ¨ç†ä¸AIæ¨¡å‹ä¹‹é—´å»ºç«‹è®¤çŸ¥æ¡¥æ¢ã€‚ä½œä¸ºä¸€ç§ç¤¾ä¼šæ–‡åŒ–æŠ€æœ¯ï¼ŒGCIé€šè¿‡ç»“æ„åŒ–åä½œå¸®åŠ©ç¾¤ä½“è¶…è¶Šä¼ ç»Ÿæ²Ÿé€šéšœç¢ï¼Œæœ‰æ•ˆå…‹æœäº†çº¯ç®—æ³•æ–¹æ³•åœ¨å¤æ‚é—®é¢˜è§£å†³å’Œå†³ç­–åˆ¶å®šä¸­çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶å»ºç«‹äº†åŸºäºæ¯”è¾ƒåˆ¤æ–­å®šå¾‹(law of comparative judgment)å’Œæœ€å°é—æ†¾åŸåˆ™(minimum regret principles)çš„æ•°å­¦åŸºç¡€ã€‚ç ”ç©¶è¿›ä¸€æ­¥å±•ç¤ºäº†å…¶åœ¨æ°”å€™é€‚åº”(climate adaptation)ã€åŒ»ç–—è½¬å‹(healthcare transformation)å’Œå…¬æ°‘å‚ä¸(civic participation)ç­‰å¤šä¸ªé¢†åŸŸçš„åº”ç”¨ä»·å€¼ã€‚é€šè¿‡ç»“åˆäººç±»åˆ›é€ åŠ›ä¸AIçš„è®¡ç®—èƒ½åŠ›ï¼ŒGCIä¸ºè§£å†³äººç±»æˆ–æœºå™¨éƒ½æ— æ³•å•ç‹¬åº”å¯¹çš„å¤æ‚ç¤¾ä¼šæŒ‘æˆ˜æä¾›äº†ä¸€ç§ç³»ç»Ÿæ€§çš„åä½œè·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19167v2",
      "published_date": "2025-05-25 14:33:49 UTC",
      "updated_date": "2025-06-04 18:36:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:02:53.896935+00:00"
    },
    {
      "arxiv_id": "2505.19165v3",
      "title": "OrgAccess: A Benchmark for Role Based Access Control in Organization Scale LLMs",
      "title_zh": "OrgAccessï¼šé¢å‘ç»„ç»‡çº§å¤§è¯­è¨€æ¨¡å‹åŸºäºè§’è‰²è®¿é—®æ§åˆ¶çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Debdeep Sanyal",
        "Umakanta Maharana",
        "Yash Sinha",
        "Hong Ming Tan",
        "Shirish Karande",
        "Mohan Kankanhalli",
        "Murari Mandal"
      ],
      "abstract": "Role-based access control (RBAC) and hierarchical structures are foundational to how information flows and decisions are made within virtually all organizations. As the potential of Large Language Models (LLMs) to serve as unified knowledge repositories and intelligent assistants in enterprise settings becomes increasingly apparent, a critical, yet under explored, challenge emerges: \\textit{can these models reliably understand and operate within the complex, often nuanced, constraints imposed by organizational hierarchies and associated permissions?} Evaluating this crucial capability is inherently difficult due to the proprietary and sensitive nature of real-world corporate data and access control policies. We introduce a synthetic yet representative \\textbf{OrgAccess} benchmark consisting of 40 distinct types of permissions commonly relevant across different organizational roles and levels. We further create three types of permissions: 40,000 easy (1 permission), 10,000 medium (3-permissions tuple), and 20,000 hard (5-permissions tuple) to test LLMs' ability to accurately assess these permissions and generate responses that strictly adhere to the specified hierarchical rules, particularly in scenarios involving users with overlapping or conflicting permissions. Our findings reveal that even state-of-the-art LLMs struggle significantly to maintain compliance with role-based structures, even with explicit instructions, with their performance degrades further when navigating interactions involving two or more conflicting permissions. Specifically, even \\textbf{GPT-4.1 only achieves an F1-Score of 0.27 on our hardest benchmark}. This demonstrates a critical limitation in LLMs' complex rule following and compositional reasoning capabilities beyond standard factual or STEM-based benchmarks, opening up a new paradigm for evaluating their fitness for practical, structured environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ä¸šçº§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†ç»„ç»‡æ¶æ„å’Œæƒé™é™åˆ¶æ—¶çš„å¯é æ€§é—®é¢˜ï¼Œæå‡ºäº† OrgAccess åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŒ…å« 40 ç§å¸¸è§çš„è§’è‰²æƒé™ç±»å‹ï¼Œå¹¶æ„å»ºäº†æ¶µç›–æ˜“ã€ä¸­ã€éš¾ä¸‰ä¸ªç­‰çº§å…± 70,000 ä¸ªæµ‹è¯•æ¡ˆä¾‹ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨å¤„ç†é‡å æˆ–å†²çªæƒé™æ—¶çš„è§„åˆ™éµå¾ªèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯å½“å‰æœ€å…ˆè¿›çš„ LLMs åœ¨å¤„ç†åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ï¼ˆRBACï¼‰ç»“æ„æ—¶ä¹Ÿé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œéš¾ä»¥å®Œå…¨éµå®ˆæ˜ç¡®çš„å±‚çº§æŒ‡ä»¤ã€‚åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„å¤æ‚ä»»åŠ¡ä¸­ï¼Œå³ä½¿æ˜¯ GPT-4.1 çš„ F1-Score ä¹Ÿä»…ä¸º 0.27ï¼Œä¸”æ¨¡å‹æ€§èƒ½éšç€æƒé™å†²çªå¤æ‚åº¦çš„å¢åŠ è€Œæ˜¾è‘—ä¸‹é™ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº† LLMs åœ¨å¤æ‚è§„åˆ™éµå¾ªå’Œç»„åˆæ¨ç†èƒ½åŠ›ä¸Šçš„å…³é”®å±€é™ï¼Œä¸ºè¯„ä¼°å…¶åœ¨ç»“æ„åŒ–ç°å®ç¯å¢ƒä¸­çš„é€‚ç”¨æ€§å¼€è¾Ÿäº†æ–°çš„è¯„ä»·èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "56 Pages",
      "pdf_url": "https://arxiv.org/pdf/2505.19165v3",
      "published_date": "2025-05-25 14:30:15 UTC",
      "updated_date": "2025-06-17 16:48:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:03:03.857908+00:00"
    },
    {
      "arxiv_id": "2505.19164v5",
      "title": "BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations",
      "title_zh": "BroadGenï¼šé¢å‘å¹¿å‘Šä¸»ç”Ÿæˆæœ‰æ•ˆä¸”é«˜æ•ˆå¹¿æ³›åŒ¹é…å…³é”®è¯æ¨èçš„æ¡†æ¶",
      "authors": [
        "Ashirbad Mishra",
        "Jinyu Zhao",
        "Soumik Dey",
        "Hansi Wu",
        "Binbin Li",
        "Kamesh Madduri"
      ],
      "abstract": "In the domain of sponsored search advertising, the focus of Keyphrase recommendation has largely been on exact match types, which pose issues such as high management expenses, limited targeting scope, and evolving search query patterns. Alternatives like Broad match types can alleviate certain drawbacks of exact matches but present challenges like poor targeting accuracy and minimal supervisory signals owing to limited advertiser usage. This research defines the criteria for an ideal broad match, emphasizing on both efficiency and effectiveness, ensuring that a significant portion of matched queries are relevant. We propose BroadGen, an innovative framework that recommends efficient and effective broad match keyphrases by utilizing historical search query data. Additionally, we demonstrate that BroadGen, through token correspondence modeling, maintains better query stability over time. BroadGen's capabilities allow it to serve daily, millions of sellers at eBay with over 2.5 billion items.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èµåŠ©æœç´¢å¹¿å‘Šé¢†åŸŸä¸­ Exact Match ç±»å‹å…³é”®è¯æ¨èå­˜åœ¨çš„ç®¡ç†æˆæœ¬é«˜ã€è¦†ç›–èŒƒå›´æœ‰é™ä»¥åŠæœç´¢æ¨¡å¼å¤šå˜ç­‰é—®é¢˜ï¼Œæ¢è®¨äº† Broad Match ç±»å‹çš„åº”ç”¨æ½œåŠ›åŠå…¶é¢ä¸´çš„å‡†ç¡®æ€§æŒ‘æˆ˜ã€‚è®ºæ–‡å®šä¹‰äº†ç†æƒ³ Broad Match çš„æ ‡å‡†ï¼Œå¼ºè°ƒéœ€åŒæ—¶å…¼é¡¾æ•ˆç‡ï¼ˆEfficiencyï¼‰ä¸æ•ˆæœï¼ˆEffectivenessï¼‰ï¼Œä»¥ç¡®ä¿åŒ¹é…æŸ¥è¯¢çš„é«˜ç›¸å…³æ€§ã€‚ä¸ºæ­¤æå‡ºäº†åä¸º BroadGen çš„åˆ›æ–°æ¡†æ¶ï¼Œåˆ©ç”¨å†å²æœç´¢æŸ¥è¯¢æ•°æ®ç”Ÿæˆé«˜æ•ˆä¸”æœ‰æ•ˆçš„ Broad Match å…³é”®è¯æ¨èã€‚BroadGen é€šè¿‡ Token Correspondence å»ºæ¨¡æŠ€æœ¯ï¼Œèƒ½å¤Ÿéšæ—¶é—´æ¨ç§»ç»´æŒæ›´ä¼˜çš„æŸ¥è¯¢ç¨³å®šæ€§ã€‚ç›®å‰è¯¥æ¡†æ¶å·²åœ¨ eBay å¹³å°æŠ•å…¥å®é™…åº”ç”¨ï¼Œæ¯æ—¥ä¸ºæ•°ç™¾ä¸‡å–å®¶åŠè¶…è¿‡ 25 äº¿ä»¶å•†å“æä¾›æœåŠ¡ï¼Œè¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡å•†ä¸šç¯å¢ƒä¸‹çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19164v5",
      "published_date": "2025-05-25 14:25:52 UTC",
      "updated_date": "2025-11-13 17:45:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:03:00.223591+00:00"
    },
    {
      "arxiv_id": "2505.21550v1",
      "title": "Collaborative Agentic AI Needs Interoperability Across Ecosystems",
      "title_zh": "ååŒæ™ºèƒ½ä½“ AI éœ€è¦è·¨ç”Ÿæ€ç³»ç»Ÿçš„äº’æ“ä½œæ€§",
      "authors": [
        "Rishi Sharma",
        "Martijn de Vos",
        "Pradyumna Chari",
        "Ramesh Raskar",
        "Anne-Marie Kermarrec"
      ],
      "abstract": "Collaborative agentic AI is projected to transform entire industries by enabling AI-powered agents to autonomously perceive, plan, and act within digital environments. Yet, current solutions in this field are all built in isolation, and we are rapidly heading toward a landscape of fragmented, incompatible ecosystems. In this position paper, we argue that interoperability, achieved by the adoption of minimal standards, is essential to ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To this end, we devise a minimal architectural foundation for collaborative agentic AI, named Web of Agents, which is composed of four components: agent-to-agent messaging, interaction interoperability, state management, and agent discovery. Web of Agents adopts existing standards and reuses existing infrastructure where possible. With Web of Agents, we take the first but critical step toward interoperable agentic systems and offer a pragmatic path forward before ecosystem fragmentation becomes the norm.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ååŒæ™ºèƒ½ä½“AI (Collaborative agentic AI) åœ¨æ”¹å˜å„è¡Œä¸šæ½œåŠ›ä¸‹æ‰€é¢ä¸´çš„ç”Ÿæ€ç³»ç»Ÿç¢ç‰‡åŒ–ä¸ä¸å…¼å®¹é—®é¢˜ã€‚ä½œè€…è®¤ä¸ºï¼Œé€šè¿‡é‡‡ç”¨æœ€å°æ ‡å‡†å®ç°äº’æ“ä½œæ€§ (Interoperability) å¯¹äºå»ºç«‹å¼€æ”¾ã€å®‰å…¨ä¸”å…·å¤‡ Web è§„æ¨¡çš„æ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†åä¸º Web of Agents çš„åä½œå¼æ™ºèƒ½ä½“ AI æœ€å°æ¶æ„åŸºç¡€ï¼Œè¯¥æ¶æ„ç”±æ™ºèƒ½ä½“é—´æ¶ˆæ¯ä¼ é€’ (agent-to-agent messaging)ã€äº¤äº’äº’æ“ä½œæ€§ (interaction interoperability)ã€çŠ¶æ€ç®¡ç† (state management) å’Œæ™ºèƒ½ä½“å‘ç° (agent discovery) å››ä¸ªæ ¸å¿ƒéƒ¨åˆ†ç»„æˆã€‚Web of Agents å°½å¯èƒ½å¤ç”¨ç°æœ‰æ ‡å‡†å’ŒåŸºç¡€è®¾æ–½ï¼Œæ—¨åœ¨è§£å†³å½“å‰å„ç³»ç»Ÿå­¤ç«‹è¿è¡Œçš„ç°çŠ¶ã€‚è¿™ä¸€æ–¹æ¡ˆä¸ºæ„å»ºå…·æœ‰äº’æ“ä½œæ€§çš„æ™ºèƒ½ä½“ç³»ç»Ÿå¥ å®šäº†å…³é”®åŸºç¡€ï¼Œå¹¶åœ¨ç”Ÿæ€ç³»ç»Ÿç¢ç‰‡åŒ–æˆä¸ºå¸¸æ€ä¹‹å‰æä¾›äº†ä¸€æ¡åŠ¡å®çš„è§£å†³è·¯å¾„ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21550v1",
      "published_date": "2025-05-25 14:25:08 UTC",
      "updated_date": "2025-05-25 14:25:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:03:03.986384+00:00"
    },
    {
      "arxiv_id": "2505.19163v1",
      "title": "SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs",
      "title_zh": "SpokenNativQAï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€æ—¥å¸¸å£è¯­æŸ¥è¯¢",
      "authors": [
        "Firoj Alam",
        "Md Arid Hasan",
        "Shammur Absar Chowdhury"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various disciplines and tasks. However, benchmarking their capabilities with multilingual spoken queries remains largely unexplored. In this study, we introduce SpokenNativQA, the first multilingual and culturally aligned spoken question-answering (SQA) dataset designed to evaluate LLMs in real-world conversational settings. The dataset comprises approximately 33,000 naturally spoken questions and answers in multiple languages, including low-resource and dialect-rich languages, providing a robust benchmark for assessing LLM performance in speech-based interactions. SpokenNativQA addresses the limitations of text-based QA datasets by incorporating speech variability, accents, and linguistic diversity. We benchmark different ASR systems and LLMs for SQA and present our findings. We released the data at (https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental scripts at (https://llmebench.qcri.org/) for the research community.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†SpokenNativQAï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çœŸå®å¯¹è¯åœºæ™¯ä¸­è¡¨ç°çš„å¤šè¯­è¨€ä¸”æ–‡åŒ–ä¸€è‡´çš„è¯­éŸ³é—®ç­”ï¼ˆSQAï¼‰æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«çº¦33,000ä¸ªè‡ªç„¶è¯­éŸ³é—®é¢˜åŠç­”æ¡ˆï¼Œæ¶µç›–äº†åŒ…æ‹¬ä½èµ„æºè¯­è¨€å’Œä¸°å¯Œæ–¹è¨€åœ¨å†…çš„å¤šç§è¯­è¨€ï¼Œä¸ºè¯„ä¼°LLMsåœ¨è¯­éŸ³äº¤äº’ä¸­çš„è¡¨ç°æä¾›äº†ç¨³å¥çš„åŸºå‡†ã€‚SpokenNativQAé€šè¿‡çº³å…¥è¯­éŸ³å˜å¼‚æ€§ã€å£éŸ³å’Œè¯­è¨€å¤šæ ·æ€§ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ–‡æœ¬é—®ç­”æ•°æ®é›†åœ¨è¯„ä¼°æ¨¡å‹å¤„ç†çœŸå®äº¤äº’èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ã€‚ä½œè€…å¯¹ä¸åŒçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå’ŒLLMsåœ¨è¯­éŸ³é—®ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¿›è¡Œäº†åŸºå‡†æµ‹è¯•å¹¶å±•ç¤ºäº†å®éªŒå‘ç°ã€‚è¯¥ç ”ç©¶ç›®å‰å·²å…¬å¼€ç›¸å…³æ•°æ®é›†å’Œå®éªŒè„šæœ¬ï¼Œæ—¨åœ¨ä¸ºå¯è§£é‡Šä¸”å…·å¤‡æ–‡åŒ–æ„ŸçŸ¥èƒ½åŠ›çš„è¯­éŸ³æ™ºèƒ½æŠ€æœ¯å¥ å®šåŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Spoken Question Answering, Multilingual LLMs, Speech-based Evaluation, Dialectal Speech, Low-resource Languages, Multimodal Benchmarking, Conversational AI, Speech-to-Text QA, Real-world Interaction, Natural Language Understanding",
      "pdf_url": "https://arxiv.org/pdf/2505.19163v1",
      "published_date": "2025-05-25 14:22:18 UTC",
      "updated_date": "2025-05-25 14:22:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:03:39.454271+00:00"
    },
    {
      "arxiv_id": "2505.19151v1",
      "title": "SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation",
      "title_zh": "SRDiffusionï¼šé€šè¿‡è‰å›¾-æ¸²æŸ“ååŒåŠ é€Ÿè§†é¢‘æ‰©æ•£æ¨ç†",
      "authors": [
        "Shenggan Cheng",
        "Yuanxin Wei",
        "Lansong Diao",
        "Yong Liu",
        "Bujiao Chen",
        "Lianghua Huang",
        "Yu Liu",
        "Wenyuan Yu",
        "Jiangsu Du",
        "Wei Lin",
        "Yang You"
      ],
      "abstract": "Leveraging the diffusion transformer (DiT) architecture, models like Sora, CogVideoX and Wan have achieved remarkable progress in text-to-video, image-to-video, and video editing tasks. Despite these advances, diffusion-based video generation remains computationally intensive, especially for high-resolution, long-duration videos. Prior work accelerates its inference by skipping computation, usually at the cost of severe quality degradation. In this paper, we propose SRDiffusion, a novel framework that leverages collaboration between large and small models to reduce inference cost. The large model handles high-noise steps to ensure semantic and motion fidelity (Sketching), while the smaller model refines visual details in low-noise steps (Rendering). Experimental results demonstrate that our method outperforms existing approaches, over 3$\\times$ speedup for Wan with nearly no quality loss for VBench, and 2$\\times$ speedup for CogVideoX. Our method is introduced as a new direction orthogonal to existing acceleration strategies, offering a practical solution for scalable video generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SRDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨åŠ é€Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„æ–°å‹æ¡†æ¶ï¼Œä¸»è¦é’ˆå¯¹åŸºäº Diffusion Transformer (DiT) æ¶æ„çš„æ¨¡å‹åœ¨å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆä¸­è®¡ç®—å¼€é”€å·¨å¤§çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å¤§æ¨¡å‹ä¸å°æ¨¡å‹ä¹‹é—´çš„â€œè‰å›¾-æ¸²æŸ“åä½œâ€ï¼ˆSketching-Rendering Cooperationï¼‰æœºåˆ¶ï¼Œé€šè¿‡åˆ†å·¥æå‡æ•ˆç‡ã€‚å…¶ä¸­å¤§æ¨¡å‹è´Ÿè´£å¤„ç†é«˜å™ªå£°æ­¥éª¤ä»¥ç¡®ä¿è§†é¢‘çš„è¯­ä¹‰å’ŒåŠ¨ä½œå¿ å®åº¦ï¼ˆSketchingï¼‰ï¼Œè€Œè¾ƒå°çš„æ¨¡å‹åˆ™åœ¨ä½å™ªå£°æ­¥éª¤ä¸­è´Ÿè´£ç»†åŒ–è§†è§‰ç»†èŠ‚ï¼ˆRenderingï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSRDiffusion åœ¨ Wan æ¨¡å‹ä¸Šå®ç°äº†è¶…è¿‡ 3 å€çš„åŠ é€Ÿä¸”åœ¨ VBench æŒ‡æ ‡ä¸Šå‡ ä¹æ²¡æœ‰è´¨é‡æŸå¤±ï¼Œåœ¨ CogVideoX æ¨¡å‹ä¸Šä¹Ÿå®ç°äº† 2 å€çš„åŠ é€Ÿã€‚è¯¥æ–¹æ³•ä½œä¸ºä¸€ç§ä¸ç°æœ‰åŠ é€Ÿç­–ç•¥æ­£äº¤çš„æ–°æ–¹å‘ï¼Œä¸ºé«˜è´¨é‡ã€å¯æ‰©å±•çš„è§†é¢‘ç”Ÿæˆæä¾›äº†ä¸€ç§æ˜¾è‘—é™ä½æ¨ç†æˆæœ¬çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "9 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.19151v1",
      "published_date": "2025-05-25 13:58:52 UTC",
      "updated_date": "2025-05-25 13:58:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:04:41.109271+00:00"
    },
    {
      "arxiv_id": "2505.19147v3",
      "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
      "title_zh": "AI æ•ˆç‡çš„è½¬å‘ï¼šä»ä»¥æ¨¡å‹ä¸ºä¸­å¿ƒåˆ°ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„å‹ç¼©",
      "authors": [
        "Xuyang Liu",
        "Zichen Wen",
        "Shaobo Wang",
        "Junjie Chen",
        "Zhishan Tao",
        "Yubo Wang",
        "Tailai Chen",
        "Xiangqi Jin",
        "Chang Zou",
        "Yiyu Wang",
        "Chenfei Liao",
        "Xu Zheng",
        "Honggang Chen",
        "Weijia Li",
        "Xuming Hu",
        "Conghui He",
        "Linfeng Zhang"
      ],
      "abstract": "The advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on scaling model parameters. However, as hardware limits constrain further model growth, the primary computational bottleneck has shifted to the quadratic cost of self-attention over increasingly long sequences by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, \\textbf{we argue that the focus of research for efficient artificial intelligence (AI) is shifting from model-centric compression to data-centric compression}. We position data-centric compression as the emerging paradigm, which improves AI efficiency by directly compressing the volume of data processed during model training or inference. To formalize this shift, we establish a unified framework for existing efficiency strategies and demonstrate why it constitutes a crucial paradigm change for long-context AI. We then systematically review the landscape of data-centric compression methods, analyzing their benefits across diverse scenarios. Finally, we outline key challenges and promising future research directions. Our work aims to provide a novel perspective on AI efficiency, synthesize existing efforts, and catalyze innovation to address the challenges posed by ever-increasing context lengths.",
      "tldr_zh": "éšç€å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)è§„æ¨¡ä¸æ–­æ‰©å¤§ï¼Œç¡¬ä»¶é™åˆ¶ä»¥åŠè¶…é•¿ä¸Šä¸‹æ–‡å¸¦æ¥çš„self-attentionäºŒæ¬¡æ–¹è®¡ç®—æˆæœ¬å·²æˆä¸ºä¸»è¦çš„è®¡ç®—ç“¶é¢ˆã€‚è¯¥ç«‹åœºè®ºæ–‡æå‡ºï¼Œäººå·¥æ™ºèƒ½æ•ˆç‡çš„ç ”ç©¶é‡ç‚¹æ­£åœ¨ä»ä¼ ç»Ÿçš„Model-Centric Compressionï¼ˆæ¨¡å‹ä¸­å¿ƒå‹ç¼©ï¼‰è½¬å‘Data-Centric Compressionï¼ˆæ•°æ®ä¸­å¿ƒå‹ç¼©ï¼‰ã€‚è¿™ä¸€æ–°å…´èŒƒå¼æ—¨åœ¨é€šè¿‡ç›´æ¥å‹ç¼©æ¨¡å‹åœ¨è®­ç»ƒæˆ–æ¨ç†è¿‡ç¨‹ä¸­å¤„ç†çš„æ•°æ®é‡ï¼Œä»æ ¹æœ¬ä¸Šæé«˜AIç³»ç»Ÿçš„è¿è¡Œæ•ˆç‡ã€‚è®ºæ–‡ä¸ºç°æœ‰çš„æ•ˆç‡ç­–ç•¥å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¹¶æ·±å…¥è®ºè¯äº†è¯¥èŒƒå¼è½¬å˜å¯¹äºé•¿ä¸Šä¸‹æ–‡(long-context)AIå‘å±•çš„å¿…è¦æ€§ã€‚é€šè¿‡å¯¹æ•°æ®ä¸­å¿ƒå‹ç¼©æ–¹æ³•çš„ç³»ç»Ÿæ€§å›é¡¾ï¼Œä½œè€…åˆ†æäº†å…¶åœ¨å¤šå…ƒåœºæ™¯ä¸‹çš„ä¼˜åŠ¿ï¼Œå¹¶æŒ‡å‡ºäº†è¯¥é¢†åŸŸé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œä¸ºåº”å¯¹æ—¥ç›Šå¢é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦éœ€æ±‚æä¾›äº†æ–°é¢–çš„è§†è§’ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æŒ‡æ˜äº†è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "Project: \\url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}",
      "pdf_url": "https://arxiv.org/pdf/2505.19147v3",
      "published_date": "2025-05-25 13:51:17 UTC",
      "updated_date": "2025-10-12 10:03:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:04:02.253807+00:00"
    },
    {
      "arxiv_id": "2505.20347v1",
      "title": "SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data",
      "title_zh": "SeRLï¼šæœ‰é™æ•°æ®ä¸‹çš„å¤§è¯­è¨€æ¨¡å‹è‡ªåšå¼ˆå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Wenkai Fang",
        "Shunyu Liu",
        "Yang Zhou",
        "Kongcheng Zhang",
        "Tongya Zheng",
        "Kaixuan Chen",
        "Mingli Song",
        "Dacheng Tao"
      ],
      "abstract": "Recent advances have demonstrated the effectiveness of Reinforcement Learning (RL) in improving the reasoning capabilities of Large Language Models (LLMs). However, existing works inevitably rely on high-quality instructions and verifiable rewards for effective training, both of which are often difficult to obtain in specialized domains. In this paper, we propose Self-play Reinforcement Learning(SeRL) to bootstrap LLM training with limited initial data. Specifically, SeRL comprises two complementary modules: self-instruction and self-rewarding. The former module generates additional instructions based on the available data at each training step, employing robust online filtering strategies to ensure instruction quality, diversity, and difficulty. The latter module introduces a simple yet effective majority-voting mechanism to estimate response rewards for additional instructions, eliminating the need for external annotations. Finally, SeRL performs conventional RL based on the generated data, facilitating iterative self-play learning. Extensive experiments on various reasoning benchmarks and across different LLM backbones demonstrate that the proposed SeRL yields results superior to its counterparts and achieves performance on par with those obtained by high-quality data with verifiable rewards. Our code is available at https://github.com/wantbook-book/SeRL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­è¿‡åº¦ä¾èµ–é«˜è´¨é‡æŒ‡ä»¤å’Œå¯éªŒè¯å¥–åŠ±çš„é—®é¢˜ï¼Œæå‡ºäº†SeRLæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨æœ‰é™çš„åˆå§‹æ•°æ®é€šè¿‡è‡ªæˆ‘åšå¼ˆå®ç°æ¨¡å‹æ€§èƒ½çš„å¼•å¯¼ä¸æå‡ã€‚SeRLç”±è‡ªæˆ‘æŒ‡ä»¤(self-instruction)å’Œè‡ªæˆ‘å¥–åŠ±(self-rewarding)ä¸¤ä¸ªäº’è¡¥æ¨¡å—ç»„æˆï¼Œå‰è€…é€šè¿‡é²æ£’çš„åœ¨çº¿è¿‡æ»¤ç­–ç•¥ç”Ÿæˆé«˜è´¨é‡ä¸”å…·æŒ‘æˆ˜æ€§çš„æŒ‡ä»¤ï¼Œåè€…åˆ™é€šè¿‡å¤šæ•°æŠ•ç¥¨(majority-voting)æœºåˆ¶åœ¨æ— éœ€å¤–éƒ¨æ ‡æ³¨çš„æƒ…å†µä¸‹ä¼°ç®—å¥–åŠ±ã€‚é€šè¿‡åŸºäºç”Ÿæˆæ•°æ®è¿›è¡Œè¿­ä»£å¼å­¦ä¹ ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆå®ç°äº†è‡ªæˆ‘åšå¼ˆå¼ºåŒ–å­¦ä¹ çš„é—­ç¯ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSeRLåœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…¶è¡¨ç°ç”šè‡³å¯ä»¥åª²ç¾ä½¿ç”¨é«˜è´¨é‡æ ‡æ³¨æ•°æ®åŠå¯éªŒè¯å¥–åŠ±çš„è®­ç»ƒæ•ˆæœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.20347v1",
      "published_date": "2025-05-25 13:28:04 UTC",
      "updated_date": "2025-05-25 13:28:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:04:22.404325+00:00"
    },
    {
      "arxiv_id": "2506.15686v2",
      "title": "Learning from M-Tuple Dominant Positive and Unlabeled Data",
      "title_zh": "åŸºäº M å…ƒç»„å ä¼˜çš„æ­£æ ·æœ¬ä¸æ— æ ‡ç­¾æ•°æ®çš„å­¦ä¹ ",
      "authors": [
        "Jiahe Qin",
        "Junpeng Li",
        "Changchun Hua",
        "Yana Yang"
      ],
      "abstract": "Label Proportion Learning (LLP) addresses the classification problem where multiple instances are grouped into bags and each bag contains information about the proportion of each class. However, in practical applications, obtaining precise supervisory information regarding the proportion of instances in a specific class is challenging. To better align with real-world application scenarios and effectively leverage the proportional constraints of instances within tuples, this paper proposes a generalized learning framework \\emph{MDPU}. Specifically, we first mathematically model the distribution of instances within tuples of arbitrary size, under the constraint that the number of positive instances is no less than that of negative instances. Then we derive an unbiased risk estimator that satisfies risk consistency based on the empirical risk minimization (ERM) method. To mitigate the inevitable overfitting issue during training, a risk correction method is introduced, leading to the development of a corrected risk estimator. The generalization error bounds of the unbiased risk estimator theoretically demonstrate the consistency of the proposed method. Extensive experiments on multiple datasets and comparisons with other relevant baseline methods comprehensively validate the effectiveness of the proposed learning framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ ‡ç­¾æ¯”ä¾‹å­¦ä¹  (Label Proportion Learning, LLP) åœ¨å®é™…åº”ç”¨ä¸­éš¾ä»¥è·å–ç²¾ç¡®ç±»åˆ«æ¯”ä¾‹ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º MDPU çš„é€šç”¨å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶èšç„¦äº M-tuple çº¦æŸä¸‹çš„æ­£ä¾‹å ä¼˜ä¸”æ— æ ‡è®°æ•°æ®åœºæ™¯ï¼Œå³å…ƒç»„ä¸­æ­£ä¾‹æ ·æœ¬æ•°é‡ä¸å°‘äºè´Ÿä¾‹æ ·æœ¬ã€‚ä½œè€…é¦–å…ˆå¯¹ä»»æ„å¤§å°å…ƒç»„å†…çš„æ ·æœ¬åˆ†å¸ƒè¿›è¡Œæ•°å­¦å»ºæ¨¡ï¼Œå¹¶åŸºäºç»éªŒé£é™©æœ€å°åŒ– (Empirical Risk Minimization, ERM) æ–¹æ³•æ¨å¯¼å‡ºæ»¡è¶³é£é™©ä¸€è‡´æ€§çš„æ— åé£é™©ä¼°è®¡å™¨ã€‚ä¸ºè§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­ä¸å¯é¿å…çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥é£é™©ä¿®æ­£æœºåˆ¶ï¼Œå¼€å‘å‡ºä¿®æ­£åçš„é£é™©ä¼°è®¡å™¨ã€‚ç†è®ºå±‚é¢çš„æ³›åŒ–è¯¯å·®ç•Œåˆ†æéªŒè¯äº†æ‰€ææ–¹æ³•çš„ä¸€è‡´æ€§ï¼Œè€Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœåˆ™å……åˆ†è¯æ˜äº† MDPU æ¡†æ¶åœ¨å¤„ç†å¤æ‚æ¯”ä¾‹çº¦æŸä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15686v2",
      "published_date": "2025-05-25 13:20:11 UTC",
      "updated_date": "2025-07-12 08:39:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:04:40.608402+00:00"
    },
    {
      "arxiv_id": "2506.15685v2",
      "title": "Ignition Phase : Standard Training for Fast Adversarial Robustness",
      "title_zh": "Ignition Phaseï¼šå®ç°å¿«é€Ÿå¯¹æŠ—é²æ£’æ€§çš„æ ‡å‡†è®­ç»ƒ",
      "authors": [
        "Wang Yu-Hang",
        "Liu ying",
        "Fang liang",
        "Wang Xuelin",
        "Junkang Guo",
        "Shiwei Li",
        "Lei Gao",
        "Jian Liu",
        "Wenfei Yin"
      ],
      "abstract": "Adversarial Training (AT) is a cornerstone defense, but many variants overlook foundational feature representations by primarily focusing on stronger attack generation. We introduce Adversarial Evolution Training (AET), a simple yet powerful framework that strategically prepends an Empirical Risk Minimization (ERM) phase to conventional AT. We hypothesize this initial ERM phase cultivates a favorable feature manifold, enabling more efficient and effective robustness acquisition. Empirically, AET achieves comparable or superior robustness more rapidly, improves clean accuracy, and cuts training costs by 8-25\\%. Its effectiveness is shown across multiple datasets, architectures, and when augmenting established AT methods. Our findings underscore the impact of feature pre-conditioning via standard training for developing more efficient, principled robust defenses. Code is available in the supplementary material.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Adversarial Evolution Training (AET)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³ Adversarial Training (AT) å¾€å¾€å¿½è§†åŸºç¡€ç‰¹å¾è¡¨ç¤ºé—®é¢˜çš„ç®€å•ä¸”å¼ºå¤§çš„æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ä¼ ç»Ÿ AT ä¹‹å‰æˆ˜ç•¥æ€§åœ°é¢„ç½®ä¸€ä¸ª Empirical Risk Minimization (ERM) é˜¶æ®µï¼Œåˆ©ç”¨åˆå§‹çš„ ERM é˜¶æ®µåŸ¹å…»å‡ºæœ‰åˆ©çš„ç‰¹å¾æµå½¢ (feature manifold)ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„é²æ£’æ€§è·å–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAET èƒ½å¤Ÿä»¥æ›´å¿«çš„é€Ÿåº¦è¾¾åˆ°ç”šè‡³è¶…è¶Šå¯¹æ¯”æ¨¡å‹çš„é²æ£’æ€§ï¼Œåœ¨æå‡å¹²å‡€å‡†ç¡®ç‡ (clean accuracy) çš„åŒæ—¶å‡å°‘äº† 8-25% çš„è®­ç»ƒæˆæœ¬ã€‚è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§åœ¨å¤šä¸ªæ•°æ®é›†ã€æ¨¡å‹æ¶æ„ä»¥åŠå¯¹ç°æœ‰ AT æ–¹æ³•çš„å¢å¼ºä¸­å¾—åˆ°äº†éªŒè¯ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†é€šè¿‡æ ‡å‡†è®­ç»ƒè¿›è¡Œç‰¹å¾é¢„å¤„ç† (feature pre-conditioning) å¯¹äºå¼€å‘é«˜æ•ˆã€æœ‰åŸåˆ™çš„é²æ£’é˜²å¾¡ç³»ç»Ÿå…·æœ‰é‡è¦å½±å“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Due to errors in both the theoretical formulation and the experimental design, we hereby withdraw the manuscript",
      "pdf_url": "https://arxiv.org/pdf/2506.15685v2",
      "published_date": "2025-05-25 13:12:03 UTC",
      "updated_date": "2025-10-11 02:48:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:05:14.611217+00:00"
    },
    {
      "arxiv_id": "2505.19128v1",
      "title": "RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models",
      "title_zh": "RetrieveAllï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€å‘½åå®ä½“è¯†åˆ«æ¡†æ¶",
      "authors": [
        "Jin Zhang",
        "Fan Gao",
        "Linyu Li",
        "Yongbin Yu",
        "Xiangxiang Wang",
        "Nyima Tashi",
        "Gadeng Luosang"
      ],
      "abstract": "The rise of large language models has led to significant performance breakthroughs in named entity recognition (NER) for high-resource languages, yet there remains substantial room for improvement in low- and medium-resource languages. Existing multilingual NER methods face severe language interference during the multi-language adaptation process, manifested in feature conflicts between different languages and the competitive suppression of low-resource language features by high-resource languages. Although training a dedicated model for each language can mitigate such interference, it lacks scalability and incurs excessive computational costs in real-world applications. To address this issue, we propose RetrieveAll, a universal multilingual NER framework based on dynamic LoRA. The framework decouples task-specific features across languages and demonstrates efficient dynamic adaptability. Furthermore, we introduce a cross-granularity knowledge augmented method that fully exploits the intrinsic potential of the data without relying on external resources. By leveraging a hierarchical prompting mechanism to guide knowledge injection, this approach advances the paradigm from \"prompt-guided inference\" to \"prompt-driven learning.\" Experimental results show that RetrieveAll outperforms existing baselines; on the PAN-X dataset, it achieves an average F1 improvement of 12.1 percent.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸­ä½èµ„æºè¯­è¨€ Named Entity Recognition (NER) æ—¶é¢ä¸´çš„è¯­è¨€å¹²æ‰°å’Œç‰¹å¾å†²çªé—®é¢˜ï¼Œæå‡ºäº† RetrieveAll é€šç”¨å¤šè¯­è¨€æ¡†æ¶ã€‚RetrieveAll åŸºäºåŠ¨æ€ LoRA æŠ€æœ¯ï¼Œé€šè¿‡è§£è€¦ä¸åŒè¯­è¨€çš„ä»»åŠ¡ç‰¹å®šç‰¹å¾å®ç°äº†é«˜æ•ˆçš„åŠ¨æ€é€‚åº”æ€§ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šè¯­è¨€é€‚é…ä¸­çš„ç«äº‰æŠ‘åˆ¶ç°è±¡ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†è·¨ç²’åº¦çŸ¥è¯†å¢å¼ºæ–¹æ³•ï¼Œåˆ©ç”¨å±‚æ¬¡åŒ–æç¤ºæœºåˆ¶ (hierarchical prompting) å°†ä¼ ç»ŸèŒƒå¼ä»â€œæç¤ºå¼•å¯¼æ¨ç†â€è½¬å˜ä¸ºâ€œæç¤ºé©±åŠ¨å­¦ä¹ â€ï¼Œä»è€Œåœ¨ä¸ä¾èµ–å¤–éƒ¨èµ„æºçš„æƒ…å†µä¸‹å……åˆ†æŒ–æ˜æ•°æ®æ½œåŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRetrieveAll åœ¨å¤šç§è¯­è¨€ç¯å¢ƒä¸‹å‡ä¼˜äºç°æœ‰åŸºå‡†ï¼Œåœ¨ PAN-X æ•°æ®é›†ä¸Šå®ç°äº† 12.1% çš„å¹³å‡ F1 å€¼æå‡ã€‚è¯¥æ¡†æ¶è¯æ˜äº†é€šè¿‡ç²¾ç»†çš„ç‰¹å¾è§£è€¦å’Œæç¤ºå­¦ä¹ ï¼Œå¯ä»¥åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å¤§å¹…æå‡å¤šè¯­è¨€å®ä½“è¯†åˆ«çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19128v1",
      "published_date": "2025-05-25 12:52:18 UTC",
      "updated_date": "2025-05-25 12:52:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:04:47.445894+00:00"
    },
    {
      "arxiv_id": "2505.23788v1",
      "title": "Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework",
      "title_zh": "è¿åç‰ˆæƒæ³•çš„ä¹ç§æ–¹å¼åŠå¤§æ¨¡å‹çš„è§„é¿ä¹‹é“ï¼šä¸€ç§å¯¹é½åˆç†ä½¿ç”¨åŸåˆ™çš„ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Aakash Sen Sharma",
        "Debdeep Sanyal",
        "Priyansh Srivastava",
        "Sundar Atreya H.",
        "Shirish Karande",
        "Mohan Kankanhalli",
        "Murari Mandal"
      ],
      "abstract": "Large language models (LLMs) commonly risk copyright infringement by reproducing protected content verbatim or with insufficient transformative modifications, posing significant ethical, legal, and practical concerns. Current inference-time safeguards predominantly rely on restrictive refusal-based filters, often compromising the practical utility of these models. To address this, we collaborated closely with intellectual property experts to develop FUA-LLM (Fair Use Aligned Language Models), a legally-grounded framework explicitly designed to align LLM outputs with fair-use doctrine. Central to our method is FairUseDB, a carefully constructed dataset containing 18,000 expert-validated examples covering nine realistic infringement scenarios. Leveraging this dataset, we apply Direct Preference Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce legally compliant and practically useful alternatives rather than resorting to blunt refusal. Recognizing the shortcomings of traditional evaluation metrics, we propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic Mean (CAH) to balance infringement risk against response utility. Extensive quantitative experiments coupled with expert evaluations confirm that FUA-LLM substantially reduces problematic outputs (up to 20\\%) compared to state-of-the-art approaches, while preserving real-world usability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å› ç›´æ¥å¤åˆ¶ç‰ˆæƒå†…å®¹è€Œé¢ä¸´çš„ä¾µæƒé£é™©ï¼Œæå‡ºäº†FUA-LLMï¼ˆFair Use Aligned Language Modelsï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹è¾“å‡ºä¸Fair UseåŸåˆ™ä¿æŒä¸€è‡´ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¸çŸ¥è¯†äº§æƒä¸“å®¶åˆä½œæ„å»ºäº†åŒ…å«1.8ä¸‡ä¸ªä¸“å®¶éªŒè¯ç¤ºä¾‹çš„FairUseDBæ•°æ®é›†ï¼Œæ¶µç›–äº†ä¹ç§ç°å®çš„ç‰ˆæƒä¾µæƒåœºæ™¯ã€‚é€šè¿‡åˆ©ç”¨è¯¥æ•°æ®é›†å¹¶é‡‡ç”¨Direct Preference Optimization (DPO) æŠ€æœ¯å¯¹å¼€æºæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒFUA-LLMèƒ½å¤Ÿç”Ÿæˆæ³•å¾‹åˆè§„ä¸”å…·å¤‡å®ç”¨æ€§çš„æ›¿ä»£å†…å®¹ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ‹’ç»å¼è¿‡æ»¤æœºåˆ¶å¯¼è‡´æ¨¡å‹å®ç”¨æ€§ä¸‹é™çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†Weighted Penalty Utilityå’ŒCompliance Aware Harmonic Mean (CAH) ç­‰æ–°æŒ‡æ ‡ï¼Œä»¥å¹³è¡¡ä¾µæƒé£é™©ä¸å“åº”æ•ˆç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFUA-LLMåœ¨ä¿æŒç°å®å¯ç”¨æ€§çš„å‰æä¸‹ï¼Œæ¯”ç°æœ‰æŠ€æœ¯å‡å°‘äº†é«˜è¾¾20%çš„è¿è§„è¾“å‡ºï¼Œä¸ºå¼€å‘æ³•å¾‹åˆè§„çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "30 Pages",
      "pdf_url": "https://arxiv.org/pdf/2505.23788v1",
      "published_date": "2025-05-25 12:23:26 UTC",
      "updated_date": "2025-05-25 12:23:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:04:36.307233+00:00"
    },
    {
      "arxiv_id": "2505.19119v1",
      "title": "CloneShield: A Framework for Universal Perturbation Against Zero-Shot Voice Cloning",
      "title_zh": "CloneShieldï¼šé¢å‘é›¶æ ·æœ¬è¯­éŸ³å…‹éš†çš„é€šç”¨æ‰°åŠ¨æ¡†æ¶",
      "authors": [
        "Renyuan Li",
        "Zhibo Liang",
        "Haichuan Zhang",
        "Tianyu Shi",
        "Zhiyuan Cheng",
        "Jia Shi",
        "Carl Yang",
        "Mingjie Tang"
      ],
      "abstract": "Recent breakthroughs in text-to-speech (TTS) voice cloning have raised serious privacy concerns, allowing highly accurate vocal identity replication from just a few seconds of reference audio, while retaining the speaker's vocal authenticity. In this paper, we introduce CloneShield, a universal time-domain adversarial perturbation framework specifically designed to defend against zero-shot voice cloning. Our method provides protection that is robust across speakers and utterances, without requiring any prior knowledge of the synthesized text. We formulate perturbation generation as a multi-objective optimization problem, and propose Multi-Gradient Descent Algorithm (MGDA) to ensure the robust protection across diverse utterances. To preserve natural auditory perception for users, we decompose the adversarial perturbation via Mel-spectrogram representations and fine-tune it for each sample. This design ensures imperceptibility while maintaining strong degradation effects on zero-shot cloned outputs. Experiments on three state-of-the-art zero-shot TTS systems, five benchmark datasets and evaluations from 60 human listeners demonstrate that our method preserves near-original audio quality in protected inputs (PESQ = 3.90, SRS = 0.93) while substantially degrading both speaker similarity and speech quality in cloned samples (PESQ = 1.07, SRS = 0.08).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CloneShieldï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºé˜²å¾¡é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ï¼ˆzero-shot voice cloningï¼‰è®¾è®¡çš„é€šç”¨æ—¶åŸŸå¯¹æŠ—æ‰°åŠ¨ï¼ˆuniversal time-domain adversarial perturbationï¼‰æ¡†æ¶ã€‚ä¸ºäº†åº”å¯¹æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯å¸¦æ¥çš„éšç§æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶å°†æ‰°åŠ¨ç”Ÿæˆå»ºæ¨¡ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é‡‡ç”¨å¤šæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆMGDAï¼‰ä»¥ç¡®ä¿åœ¨ä¸åŒè¯´è¯è€…å’Œè¯­æ–™ä¸Šçš„é˜²å¾¡é²æ£’æ€§ã€‚é€šè¿‡Mel-spectrogramè¡¨ç¤ºåˆ†è§£å¯¹æŠ—æ‰°åŠ¨å¹¶è¿›è¡Œæ ·æœ¬çº§å¾®è°ƒï¼ŒCloneShieldåœ¨ä¿æŒåŸå§‹éŸ³é¢‘è‡ªç„¶å¬æ„Ÿçš„åŒæ—¶ï¼Œèƒ½æ˜¾è‘—é™ä½å…‹éš†è¾“å‡ºçš„è¯­éŸ³ç›¸ä¼¼åº¦å’Œè´¨é‡ã€‚åœ¨ä¸‰ä¸ªå‰æ²¿TTSç³»ç»Ÿå’Œäº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒåŠäººç±»å¬æ„Ÿè¯„ä¼°è¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç»´æŒé«˜ä¿æŠ¤è¾“å…¥è´¨é‡ï¼ˆPESQ = 3.90ï¼‰çš„å‰æä¸‹ï¼Œä½¿å…‹éš†æ ·æœ¬çš„è´¨é‡å¤§å¹…é€€åŒ–ï¼ˆPESQ = 1.07ï¼‰ã€‚è¿™é¡¹ç ”ç©¶ä¸ºé˜²æ­¢æœªç»æˆæƒçš„èº«ä»½å¤åˆ¶æä¾›äº†ä¸€ç§æ— éœ€åˆæˆæ–‡æœ¬å…ˆéªŒçŸ¥è¯†çš„æ™®é€‚æ€§é˜²å¾¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "10pages, 4figures",
      "pdf_url": "https://arxiv.org/pdf/2505.19119v1",
      "published_date": "2025-05-25 12:22:00 UTC",
      "updated_date": "2025-05-25 12:22:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:04:55.169461+00:00"
    },
    {
      "arxiv_id": "2505.19115v2",
      "title": "FP4 All the Way: Fully Quantized Training of LLMs",
      "title_zh": "FP4 è´¯ç©¿å§‹ç»ˆï¼šå¤§è¯­è¨€æ¨¡å‹å…¨é‡åŒ–è®­ç»ƒ",
      "authors": [
        "Brian Chmiel",
        "Maxim Fishman",
        "Ron Banner",
        "Daniel Soudry"
      ],
      "abstract": "We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately $\\sqrt{3}$ times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in https://github.com/Anonymous1252022/fp4-all-the-way .",
      "tldr_zh": "è¯¥ç ”ç©¶é¦–æ¬¡å±•ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…¨é‡åŒ–è®­ç»ƒï¼ˆFully Quantized Training, FQTï¼‰ï¼Œåœ¨é«˜è¾¾2000äº¿Tokençš„æ•°æ®é›†ä¸Šä¸»è¦ä½¿ç”¨4ä½æµ®ç‚¹æ•°ï¼ˆFP4ï¼‰ç²¾åº¦å¤„ç†æƒé‡ã€æ¿€æ´»å’Œæ¢¯åº¦ã€‚ä½œè€…æ·±å…¥æ¢è®¨äº†FP4çš„å…³é”®è®¾è®¡é€‰æ‹©ï¼Œå‘ç°NVFP4æ ¼å¼ï¼ˆæ¯16ä¸ªFP4å€¼å…±äº«ä¸€ä¸ªE4M3ç¼©æ”¾å› å­ï¼‰èƒ½æä¾›æœ€ä½³è®­ç»ƒæ•ˆæœã€‚åœ¨èˆå…¥ç­–ç•¥ä¸Šï¼Œå‰å‘ä¼ æ’­é‡‡ç”¨Round-to-nearestï¼Œè€Œåå‘ä¼ æ’­å’Œæ›´æ–°é˜¶æ®µåˆ™ä½¿ç”¨Stochastic roundingä»¥å¢å¼ºæ•°å€¼ç¨³å®šæ€§ã€‚ç ”ç©¶è¿˜æå‡ºäº†ä¸€ä¸ªå…³äºé‡åŒ–è®­ç»ƒæœ‰æ•ˆæ€§çš„ç†è®ºé˜ˆå€¼ï¼Œå³å½“æ¢¯åº¦èŒƒæ•°ä½äºé‡åŒ–å™ªå£°çš„$\\sqrt{3}$å€æ—¶ï¼Œè®­ç»ƒæ•ˆç‡ä¼šä¸‹é™ã€‚é€šè¿‡è¿™äº›ä¼˜åŒ–ï¼Œå›¢é˜Ÿåœ¨256ä¸ªIntel Gaudi2åŠ é€Ÿå™¨ä¸ŠæˆåŠŸè®­ç»ƒäº†ä¸€ä¸ª70äº¿å‚æ•°çš„æ¨¡å‹ã€‚æœ€ç»ˆå®éªŒç»“æœè¡¨æ˜ï¼ŒFP4è®­ç»ƒæ¨¡å‹çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸æ ‡å‡†çš„BF16åŸºå‡†ç›¸å½“ï¼Œè¯æ˜äº†FP4å…¨é‡åŒ–è®­ç»ƒæ˜¯å¤§è§„æ¨¡æ¨¡å‹å¼€å‘ä¸­ä¸€ç§æå…·å®è·µä»·å€¼ä¸”é«˜æ•ˆçš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19115v2",
      "published_date": "2025-05-25 12:14:25 UTC",
      "updated_date": "2025-08-10 07:10:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:05:39.155506+00:00"
    },
    {
      "arxiv_id": "2505.19110v2",
      "title": "An Interpretable Representation Learning Approach for Diffusion Tensor Imaging",
      "title_zh": "å¼¥æ•£å¼ é‡æˆåƒçš„å¯è§£é‡Šè¡¨ç¤ºå­¦ä¹ æ–¹æ³•",
      "authors": [
        "Vishwa Mohan Singh",
        "Alberto Gaston Villagran Asiares",
        "Luisa Sophie Schuhmacher",
        "Kate Rendall",
        "Simon WeiÃŸbrod",
        "David RÃ¼gamer",
        "Inga KÃ¶rte"
      ],
      "abstract": "Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the structural connectivity of the brain, but presents challenges in effective representation and interpretation in deep learning models. In this work, we propose a novel 2D representation of DTI tractography that encodes tract-level fractional anisotropy (FA) values into a 9x9 grayscale image. This representation is processed through a Beta-Total Correlation Variational Autoencoder with a Spatial Broadcast Decoder to learn a disentangled and interpretable latent embedding. We evaluate the quality of this embedding using supervised and unsupervised representation learning strategies, including auxiliary classification, triplet loss, and SimCLR-based contrastive learning. Compared to the 1D Group deep neural network (DNN) baselines, our approach improves the F1 score in a downstream sex classification task by 15.74% and shows a better disentanglement than the 3D representation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼¥æ•£å¼ é‡æˆåƒ(Diffusion Tensor Imaging, DTI)çº¤ç»´è¿½è¸ªåœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­æœ‰æ•ˆè¡¨ç¤ºå’Œè§£é‡Šçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„2Dè¡¨ç¤ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†çº¤ç»´æŸæ°´å¹³çš„åˆ†æ•°å„å‘å¼‚æ€§(Fractional Anisotropy, FA)å€¼ç¼–ç ä¸º9x9çš„ç°åº¦å›¾åƒï¼Œå¹¶åˆ©ç”¨å¸¦æœ‰ç©ºé—´å¹¿æ’­è§£ç å™¨(Spatial Broadcast Decoder)çš„Beta-Total Correlation Variational Autoencoderæ¥å­¦ä¹ è§£è€¦ä¸”å…·å¯è§£é‡Šæ€§çš„æ½œåµŒå…¥(Latent Embedding)ã€‚ç ”ç©¶é‡‡ç”¨äº†è¾…åŠ©åˆ†ç±»ã€ä¸‰å…ƒç»„æŸå¤±(Triplet Loss)å’ŒåŸºäºSimCLRçš„å¯¹æ¯”å­¦ä¹ ç­‰å¤šç§ç­–ç•¥è¯„ä¼°åµŒå…¥è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸1D Groupæ·±åº¦ç¥ç»ç½‘ç»œ(DNN)åŸºå‡†ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‹æ¸¸æ€§åˆ«åˆ†ç±»ä»»åŠ¡ä¸­çš„F1åˆ†æ•°æå‡äº†15.74%ï¼Œä¸”åœ¨è§£è€¦æ€§èƒ½ä¸Šä¼˜äº3Dè¡¨ç¤ºã€‚è¯¥ç ”ç©¶ä¸ºç»“æ„è¿æ¥ç»„å­¦æ•°æ®çš„ç‰¹å¾æå–å’Œä¸´åºŠåº”ç”¨æä¾›äº†æ›´å…·å¯è§£é‡Šæ€§çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication at MIDL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.19110v2",
      "published_date": "2025-05-25 11:55:02 UTC",
      "updated_date": "2025-05-31 11:31:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:05:03.873416+00:00"
    },
    {
      "arxiv_id": "2505.19108v1",
      "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models",
      "title_zh": "CCHallï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹è·¨è¯­è¨€ä¸è·¨æ¨¡æ€è”åˆå¹»è§‰æ£€æµ‹çš„æ–°å‹åŸºå‡†",
      "authors": [
        "Yongheng Zhang",
        "Xu Liu",
        "Ruoxi Zhou",
        "Qiguang Chen",
        "Hao Fei",
        "Wenpeng Lu",
        "Libo Qin"
      ],
      "abstract": "Investigating hallucination issues in large language models (LLMs) within cross-lingual and cross-modal scenarios can greatly advance the large-scale deployment in real-world applications. Nevertheless, the current studies are limited to a single scenario, either cross-lingual or cross-modal, leaving a gap in the exploration of hallucinations in the joint cross-lingual and cross-modal scenarios. Motivated by this, we introduce a novel joint Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this gap. Specifically, CCHall simultaneously incorporates both cross-lingual and cross-modal hallucination scenarios, which can be used to assess the cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a comprehensive evaluation on CCHall, exploring both mainstream open-source and closed-source LLMs. The experimental results highlight that current LLMs still struggle with CCHall. We hope CCHall can serve as a valuable resource to assess LLMs in joint cross-lingual and cross-modal scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†åä¸ºCCHallçš„æ–°å‹åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è·¨è¯­è¨€(Cross-lingual)å’Œè·¨æ¨¡æ€(Cross-modal)è”åˆåœºæ™¯ä¸‹å¹»è§‰æ£€æµ‹çš„ç ”ç©¶ç©ºç™½ã€‚æ­¤å‰ï¼Œå…³äºå¹»è§‰çš„ç ”ç©¶å¤§å¤šå±€é™äºå•ä¸€çš„è·¨è¯­è¨€æˆ–è·¨æ¨¡æ€åœºæ™¯ï¼Œå¿½ç•¥äº†ä¸¤è€…äº¤ç»‡å¸¦æ¥çš„å¤æ‚æŒ‘æˆ˜ã€‚CCHallé€šè¿‡åŒæ—¶æ•´åˆè¿™ä¸¤ç§ç»´åº¦ï¼Œæä¾›äº†ä¸€ä¸ªèƒ½å¤Ÿè¯„ä¼°LLMsç»¼åˆèƒ½åŠ›çš„æµ‹è¯•å¹³å°ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹ä¸»æµçš„å¼€æºå’Œé—­æºLLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå½“å‰çš„LLMsåœ¨åº”å¯¹CCHallæå‡ºçš„è”åˆæŒ‘æˆ˜æ—¶è¡¨ç°ä¾ç„¶ä¹åŠ›ã€‚è¿™ä¸€åŸºå‡†çš„å»ºç«‹ä¸ºæœªæ¥ä¼˜åŒ–LLMsåœ¨å¤æ‚å¤šå˜ç°å®ç¯å¢ƒä¸­çš„å¯é æ€§æä¾›äº†é‡è¦çš„å­¦æœ¯èµ„æºå’Œå‚è€ƒä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2505.19108v1",
      "published_date": "2025-05-25 11:54:32 UTC",
      "updated_date": "2025-05-25 11:54:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:07:28.701365+00:00"
    },
    {
      "arxiv_id": "2505.19099v8",
      "title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning",
      "title_zh": "SeePhysï¼šè§è€Œèƒ½æ€ï¼Ÿâ€”â€”åŸºäºè§†è§‰çš„ç‰©ç†æ¨ç†åŸºå‡†è¯„æµ‹",
      "authors": [
        "Kun Xiang",
        "Heng Li",
        "Terry Jingchen Zhang",
        "Yinya Huang",
        "Zirong Liu",
        "Peixin Qu",
        "Jixi He",
        "Jiaqi Chen",
        "Yu-Jie Yuan",
        "Jianhua Han",
        "Hang Xu",
        "Hanhui Li",
        "Mrinmaya Sachan",
        "Xiaodan Liang"
      ],
      "abstract": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features a substantial proportion of vision-essential problems (75%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our benchmark. These results reveal fundamental challenges in current large language models' visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SeePhysï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)ç‰©ç†æ¨ç†èƒ½åŠ›çš„å¤§è§„æ¨¡å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•(multimodal benchmark)ï¼Œæ¶µç›–äº†ä»ä¸­å­¦åˆ°åšå£«ç”Ÿæ°´å¹³çš„ 7 ä¸ªç‰©ç†åŸºç¡€é¢†åŸŸã€‚ä¸ä»¥å¾€è§†è§‰å…ƒç´ ä»…èµ·è¾…åŠ©ä½œç”¨çš„ç ”ç©¶ä¸åŒï¼ŒSeePhys åŒ…å«é«˜è¾¾ 75% çš„è§†è§‰å¿…éœ€å‹(vision-essential)é—®é¢˜ï¼Œè¦æ±‚æ¨¡å‹å¿…é¡»å‡†ç¡®æå–å›¾è¡¨ä¿¡æ¯æ‰èƒ½å®Œæˆæ¨ç†ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ Gemini-2.5-pro å’Œ o4-mini ç­‰é¡¶å°–æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡ä¹Ÿæœªèƒ½è¶…è¿‡ 60%ã€‚ç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å›¾è¡¨è§£é‡Šä¸ç‰©ç†æ¨ç†çš„ä¸¥å¯†è€¦åˆæ–¹é¢å­˜åœ¨æ˜æ˜¾çŸ­æ¿ï¼Œä¸”å­˜åœ¨è¿‡åº¦ä¾èµ–æ–‡æœ¬çº¿ç´¢è€Œéè§†è§‰ç†è§£çš„è®¤çŸ¥æ·å¾„ã€‚è¯¥å·¥ä½œä¸ºæ¨åŠ¨å…·å¤‡æ·±åº¦ç‰©ç†å¸¸è¯†çš„è§†è§‰æ¨ç†æŠ€æœ¯å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "physics.ed-ph",
        "physics.pop-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "46 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.19099v8",
      "published_date": "2025-05-25 11:28:34 UTC",
      "updated_date": "2025-10-06 16:16:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:06:33.328443+00:00"
    },
    {
      "arxiv_id": "2505.19096v1",
      "title": "Enable Lightweight and Precision-Scalable Posit/IEEE-754 Arithmetic in RISC-V Cores for Transprecision Computing",
      "title_zh": "é¢å‘è·¨ç²¾åº¦è®¡ç®—çš„ RISC-V å†…æ ¸è½»é‡çº§ã€ç²¾åº¦å¯æ‰©å±• Posit/IEEE-754 ç®—æœ¯å®ç°",
      "authors": [
        "Qiong Li",
        "Chao Fang",
        "Longwei Huang",
        "Jun Lin",
        "Zhongfeng Wang"
      ],
      "abstract": "While posit format offers superior dynamic range and accuracy for transprecision computing, its adoption in RISC-V processors is hindered by the lack of a unified solution for lightweight, precision-scalable, and IEEE-754 arithmetic compatible hardware implementation. To address these challenges, we enhance RISC-V processors by 1) integrating dedicated posit codecs into the original FPU for lightweight implementation, 2) incorporating multi/mixed-precision support with dynamic exponent size for precision-scalability, and 3) reusing and customizing ISA extensions for IEEE-754 compatible posit operations. Our comprehensive evaluation spans the modified FPU, RISC-V core, and SoC levels. It demonstrates that our implementation achieves 47.9% LUTs and 57.4% FFs reduction compared to state-of-the-art posit-enabled RISC-V processors, while achieving up to 2.54$\\times$ throughput improvement in various GEMM kernels.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½¬ç²¾åº¦è®¡ç®— (Transprecision Computing) ä¸­ posit æ ¼å¼åœ¨ RISC-V å¤„ç†å™¨ä¸Šç¼ºä¹è½»é‡åŒ–ã€ç²¾åº¦å¯æ‰©å±•ä¸”å…¼å®¹ IEEE-754 æ ‡å‡†ç¡¬ä»¶å®ç°çš„é—®é¢˜æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ä½œè€…é€šè¿‡åœ¨åŸå§‹æµ®ç‚¹è¿ç®—å•å…ƒ (FPU) ä¸­é›†æˆä¸“ç”¨çš„ posit ç¼–è§£ç å™¨å®ç°äº†è½»é‡åŒ–è®¾è®¡ï¼Œå¹¶å¼•å…¥äº†æ”¯æŒåŠ¨æ€é˜¶ç å¤§å° (dynamic exponent size) çš„å¤šç²¾åº¦/æ··åˆç²¾åº¦æ”¯æŒä»¥å¢å¼ºç²¾åº¦å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡å¤ç”¨å’Œå®šåˆ¶æŒ‡ä»¤é›†æ¶æ„ (ISA) æ‰©å±•ï¼Œç¡®ä¿äº† posit è¿ç®—ä¸ IEEE-754 æ ‡å‡†çš„ç‰©ç†å…¼å®¹ã€‚åœ¨ç³»ç»Ÿçº§è¯„ä¼°ä¸­ï¼Œè¯¥å®ç°ç›¸æ¯”æœ€å…ˆè¿›çš„ posit æ”¯æŒçš„ RISC-V å¤„ç†å™¨å‡å°‘äº† 47.9% çš„ LUTs å’Œ 57.4% çš„ FFs èµ„æºå ç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šç§ GEMM å†…æ ¸ä¸­ï¼Œè¯¥è®¾è®¡å®ç°äº†é«˜è¾¾ 2.54 å€çš„ååé‡æå‡ï¼Œä¸ºé«˜æ•ˆèƒ½è®¡ç®—æä¾›äº†çµæ´»ä¸”ç²¾ç¡®çš„ç¡¬ä»¶æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "Work in Progress",
      "pdf_url": "https://arxiv.org/pdf/2505.19096v1",
      "published_date": "2025-05-25 11:16:16 UTC",
      "updated_date": "2025-05-25 11:16:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:05:59.994912+00:00"
    },
    {
      "arxiv_id": "2505.19095v1",
      "title": "ScreenExplorer: Training a Vision-Language Model for Diverse Exploration in Open GUI World",
      "title_zh": "ScreenExplorerï¼šé¢å‘å¼€æ”¾ GUI ä¸–ç•Œå¤šæ ·åŒ–æ¢ç´¢çš„è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒ",
      "authors": [
        "Runliang Niu",
        "Jinglong Ji",
        "Yi Chang",
        "Qi Wang"
      ],
      "abstract": "The rapid progress of large language models (LLMs) has sparked growing interest in building Artificial General Intelligence (AGI) within Graphical User Interface (GUI) environments. However, existing GUI agents based on LLMs or vision-language models (VLMs) often fail to generalize to novel environments and rely heavily on manually curated, diverse datasets. To overcome these limitations, we introduce ScreenExplorer, a VLM trained via Group Relative Policy Optimization(GRPO) in real, dynamic, and open-ended GUI environments. Innovatively, we introduced a world-model-based curiosity reward function to help the agent overcome the cold-start phase of exploration. Additionally, distilling experience streams further enhances the model's exploration capabilities. Our training framework enhances model exploration in open GUI environments, with trained models showing better environmental adaptation and sustained exploration compared to static deployment models. Our findings offer a scalable pathway toward AGI systems with self-improving capabilities in complex interactive settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)æˆ–è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)æ™ºèƒ½ä½“æ³›åŒ–èƒ½åŠ›ä¸è¶³ä¸”è¿‡åº¦ä¾èµ–äººå·¥æ•°æ®é›†çš„é—®é¢˜ï¼Œæå‡ºäº†ScreenExplorerï¼Œè¿™æ˜¯ä¸€ç§åœ¨çœŸå®ã€åŠ¨æ€ä¸”å¼€æ”¾çš„GUIç¯å¢ƒä¸­é€šè¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å¼•å…¥äº†åŸºäºä¸–ç•Œæ¨¡å‹(World-model)çš„å¥½å¥‡å¿ƒå¥–åŠ±å‡½æ•°ï¼Œæ—¨åœ¨å¸®åŠ©æ™ºèƒ½ä½“å…‹æœæ¢ç´¢åˆæœŸçš„å†·å¯åŠ¨æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡è’¸é¦ç»éªŒæµ(Distilling experience streams)è¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒScreenExploreråœ¨å¼€æ”¾GUIç¯å¢ƒä¸­å±•ç°å‡ºæ¯”é™æ€éƒ¨ç½²æ¨¡å‹æ›´ä¼˜çš„ç¯å¢ƒé€‚åº”æ€§å’ŒæŒç»­æ¢ç´¢èƒ½åŠ›ã€‚è¿™ä¸€å‘ç°ä¸ºåœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸­æ„å»ºå…·æœ‰è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›çš„é€šç”¨äººå·¥æ™ºèƒ½(AGI)ç³»ç»Ÿæä¾›äº†ä¸€æ¡å¯æ‰©å±•çš„å®ç°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19095v1",
      "published_date": "2025-05-25 11:13:03 UTC",
      "updated_date": "2025-05-25 11:13:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:06:33.927643+00:00"
    },
    {
      "arxiv_id": "2505.19094v2",
      "title": "SATORI-R1: Incentivizing Multimodal Reasoning through Explicit Visual Anchoring",
      "title_zh": "SATORI-R1ï¼šé€šè¿‡æ˜¾å¼è§†è§‰é”šå®šæ¿€åŠ±å¤šæ¨¡æ€æ¨ç†",
      "authors": [
        "Chuming Shen",
        "Wei Wei",
        "Xiaoye Qu",
        "Yu Cheng"
      ],
      "abstract": "DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI ($\\textbf{S}patially$ $\\textbf{A}nchored$ $\\textbf{T}ask$ $\\textbf{O}ptimization$ with $\\textbf{R}e\\textbf{I}nforcement$ Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to $15.7\\%$ improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤šæ¨¡æ€é¢†åŸŸåº”ç”¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) è¿›è¡Œè‡ªç”±å½¢å¼æ¨ç†æ—¶ï¼Œè§†è§‰é—®ç­” (VQA) ä»»åŠ¡é¢ä¸´çš„è§†è§‰ç„¦ç‚¹åç¦»å…³é”®åŒºåŸŸå’Œä¸­é—´æ­¥éª¤éš¾ä»¥éªŒè¯ç­‰å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† SATORIï¼Œé€šè¿‡æ˜¾å¼çš„è§†è§‰é”šå®š (Explicit Visual Anchoring) æ¿€åŠ±å¤šæ¨¡æ€æ¨ç†ï¼Œå°† VQA åˆ†è§£ä¸ºå…¨å±€å›¾åƒæè¿° (global image captioning)ã€åŒºåŸŸå®šä½ (region localization) å’Œç­”æ¡ˆé¢„æµ‹ (answer prediction) ä¸‰ä¸ªå¯éªŒè¯é˜¶æ®µï¼Œå¹¶ä¸ºæ¯ä¸ªé˜¶æ®µæä¾›æ˜¾å¼çš„å¥–åŠ±ä¿¡å·ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŒ…å« 1.2 ä¸‡æ¡æ•°æ®çš„ VQA-Verify æ•°æ®é›†ï¼Œåˆ©ç”¨å¯¹é½çš„æè¿°å’Œè¾¹ç•Œæ¡† (bounding-boxes) è¾…åŠ©è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSATORI åœ¨ä¸ƒé¡¹ VQA åŸºå‡†æµ‹è¯•ä¸­å‡æœ‰æ˜¾è‘—æå‡ï¼Œå‡†ç¡®ç‡æœ€é«˜æ¯”ç±» R1 åŸºçº¿æé«˜ 15.7%ã€‚é€šè¿‡æ³¨æ„åŠ›å›¾ (attention map) åˆ†æè¯å®ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹å¯¹ä»»åŠ¡å…³é”®åŒºåŸŸçš„èšç„¦ï¼Œä»è€Œå¤§å¹…æå‡äº†å¤šæ¨¡æ€æ¨ç†çš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.19094v2",
      "published_date": "2025-05-25 11:11:06 UTC",
      "updated_date": "2025-12-03 07:15:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:06:04.142812+00:00"
    },
    {
      "arxiv_id": "2505.19092v2",
      "title": "Reinforced Latent Reasoning for LLM-based Recommendation",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹æ¨èçš„å¼ºåŒ–éšæ€§æ¨ç†",
      "authors": [
        "Yang Zhang",
        "Wenxin Xu",
        "Xiaoyan Zhao",
        "Wenjie Wang",
        "Fuli Feng",
        "Xiangnan He",
        "Tat-Seng Chua"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities in complex problem-solving tasks, sparking growing interest in their application to preference reasoning in recommendation systems. Existing methods typically rely on fine-tuning with explicit chain-of-thought (CoT) data. However, these methods face significant practical limitations due to (1) the difficulty of obtaining high-quality CoT data in recommendation and (2) the high inference latency caused by generating CoT reasoning. In this work, we explore an alternative approach that shifts from explicit CoT reasoning to compact, information-dense latent reasoning. This approach eliminates the need for explicit CoT generation and improves inference efficiency, as few latent tokens can effectively capture the entire reasoning process. Building on this idea, we propose \\textit{\\underline{R}einforced \\underline{Latent} \\underline{R}easoning for \\underline{R}ecommendation} (LatentR$^3$), a novel end-to-end training framework that leverages reinforcement learning (RL) to optimize latent reasoning without relying on any CoT data. LatentR$^3$ adopts a two-stage training strategy: first, supervised fine-tuning to initialize the latent reasoning module, followed by pure RL training to encourage exploration through a rule-based reward design. Our RL implementation is based on a modified GRPO algorithm, which reduces computational overhead during training and introduces continuous reward signals for more efficient learning. Extensive experiments demonstrate that LatentR$^3$ enables effective latent reasoning without any direct supervision of the reasoning process, significantly improving performance when integrated with different LLM-based recommendation methods. Our codes are available at https://github.com/xuwenxinedu/R3.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LatentRÂ³ï¼Œä¸€ä¸ªé¢å‘åŸºäºLLMçš„æ¨èç³»ç»Ÿçš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–Chain-of-Thought (CoT) æ•°æ®å¯¼è‡´çš„æ¨ç†å»¶è¿Ÿé«˜å’Œé«˜è´¨é‡æ ‡æ³¨è·å–éš¾ç­‰é—®é¢˜ï¼Œè¯¥æ¡†æ¶å°†æ˜¾å¼çš„CoTæ¨ç†è½¬å˜ä¸ºä¿¡æ¯å¯†é›†çš„éšæ€§æ¨ç† (latent reasoning)ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚LatentRÂ³é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå³å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒ (SFT) åˆå§‹åŒ–ï¼Œå†ç»“åˆæ”¹è¿›çš„GRPOç®—æ³•è¿›è¡Œçº¯å¼ºåŒ–å­¦ä¹  (RL) è®­ç»ƒï¼Œä»¥åˆ©ç”¨è§„åˆ™é©±åŠ¨çš„å¥–åŠ±ä¿¡å·ä¼˜åŒ–éšæ€§æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLatentRÂ³åœ¨æ— éœ€ä»»ä½•CoTæ•°æ®å’Œç›´æ¥æ¨ç†ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºä¸åŒLLMæ¨èæ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ä»…è§£å†³äº†CoTç”Ÿæˆå¸¦æ¥çš„æ€§èƒ½ç“¶é¢ˆï¼Œä¹Ÿä¸ºæ„å»ºæ›´é«˜æ•ˆã€å¯æ‰©å±•çš„æ¨èæ¨ç†ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19092v2",
      "published_date": "2025-05-25 11:03:45 UTC",
      "updated_date": "2025-10-24 12:45:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:06:46.813315+00:00"
    },
    {
      "arxiv_id": "2505.19091v1",
      "title": "ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models",
      "title_zh": "ReadBenchï¼šè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯†é›†æ–‡æœ¬è§†è§‰é˜…è¯»èƒ½åŠ›",
      "authors": [
        "Benjamin ClaviÃ©",
        "Florian Brand"
      ],
      "abstract": "Recent advancements in Large Vision-Language Models (VLMs), have greatly enhanced their capability to jointly process text and images. However, despite extensive benchmarks evaluating visual comprehension (e.g., diagrams, color schemes, OCR tasks...), there is limited assessment of VLMs' ability to read and reason about text-rich images effectively. To fill this gap, we introduce ReadBench, a multimodal benchmark specifically designed to evaluate the reading comprehension capabilities of VLMs. ReadBench transposes contexts from established text-only benchmarks into images of text while keeping textual prompts and questions intact. Evaluating leading VLMs with ReadBench, we find minimal-but-present performance degradation on short, text-image inputs, while performance sharply declines for longer, multi-page contexts. Our experiments further reveal that text resolution has negligible effects on multimodal performance. These findings highlight needed improvements in VLMs, particularly their reasoning over visually presented extensive textual content, a capability critical for practical applications. ReadBench is available at https://github.com/answerdotai/ReadBench .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤„ç†å¯Œæ–‡æœ¬å›¾åƒ(text-rich images)æ—¶é˜…è¯»ä¸æ¨ç†èƒ½åŠ›è¯„ä¼°ä¸è¶³çš„ç°çŠ¶ï¼Œæå‡ºäº†ReadBenchã€‚ä½œä¸ºä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•(multimodal benchmark)ï¼ŒReadBench é€šè¿‡å°†ç°æœ‰çš„çº¯æ–‡æœ¬åŸºå‡†è¯­å¢ƒè½¬æ¢ä¸ºæ–‡æœ¬å›¾åƒï¼ŒåŒæ—¶ä¿ç•™åŸå§‹çš„æç¤ºè¯å’Œé—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹å¯¹å¯†é›†æ–‡æœ¬çš„è§†è§‰é˜…è¯»ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¢†å…ˆçš„ VLMs åœ¨å¤„ç†çŸ­æ–‡æœ¬å›¾åƒè¾“å…¥æ—¶æ€§èƒ½ä»…è½»å¾®ä¸‹é™ï¼Œä½†åœ¨é¢å¯¹è¾ƒé•¿çš„å¤šé¡µ(multi-page)ä¸Šä¸‹æ–‡æ—¶ï¼Œæ¨¡å‹æ€§èƒ½ä¼šå‡ºç°æ˜¾è‘—ä¸‹æ»‘ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œæ–‡æœ¬åˆ†è¾¨ç‡å¯¹å¤šæ¨¡æ€æ€§èƒ½çš„å½±å“å¾®ä¹å…¶å¾®ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå½“å‰çš„ VLMs åœ¨å¯¹è§†è§‰å‘ˆç°çš„å¤§è§„æ¨¡æ–‡æœ¬å†…å®¹è¿›è¡Œæ¨ç†æ–¹é¢ä»æœ‰è¾ƒå¤§æå‡ç©ºé—´ï¼Œè¿™å¯¹å®ç°å¤æ‚çš„å®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19091v1",
      "published_date": "2025-05-25 11:02:01 UTC",
      "updated_date": "2025-05-25 11:02:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:06:25.420472+00:00"
    },
    {
      "arxiv_id": "2505.19086v3",
      "title": "MaskedManipulator: Versatile Whole-Body Manipulation",
      "title_zh": "MaskedManipulatorï¼šé€šç”¨çš„å…¨èº«æ“æ§",
      "authors": [
        "Chen Tessler",
        "Yifeng Jiang",
        "Erwin Coumans",
        "Zhengyi Luo",
        "Gal Chechik",
        "Xue Bin Peng"
      ],
      "abstract": "We tackle the challenges of synthesizing versatile, physically simulated human motions for full-body object manipulation. Unlike prior methods that are focused on detailed motion tracking, trajectory following, or teleoperation, our framework enables users to specify versatile high-level objectives such as target object poses or body poses. To achieve this, we introduce MaskedManipulator, a generative control policy distilled from a tracking controller trained on large-scale human motion capture data. This two-stage learning process allows the system to perform complex interaction behaviors, while providing intuitive user control over both character and object motions. MaskedManipulator produces goal-directed manipulation behaviors that expand the scope of interactive animation systems beyond task-specific solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰©ç†æ¨¡æ‹Ÿä¸­äººä½“å…¨èº«ç‰©ä½“æ“æ§ï¼ˆfull-body object manipulationï¼‰çš„å¤šåŠŸèƒ½åŠ¨ä½œåˆæˆæŒ‘æˆ˜ï¼Œæå‡ºäº† MaskedManipulator æ¡†æ¶ã€‚ä¸åŒäºä»¥å¾€ä¾§é‡äºç»†èŠ‚åŠ¨ä½œè¿½è¸ªã€è½¨è¿¹è·Ÿéšæˆ–é¥æ“ä½œçš„æ–¹æ³•ï¼Œè¯¥æ¡†æ¶å…è®¸ç”¨æˆ·é€šè¿‡æŒ‡å®šç›®æ ‡ç‰©ä½“ä½å§¿æˆ–èº«ä½“å§¿æ€ç­‰é«˜å±‚ç›®æ ‡ï¼ˆhigh-level objectivesï¼‰æ¥å¼•å¯¼åŠ¨ä½œã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªä»å¤§è§„æ¨¡äººä½“åŠ¨ä½œæ•æ‰ï¼ˆmotion captureï¼‰æ•°æ®è®­ç»ƒçš„è¿½è¸ªæ§åˆ¶å™¨ä¸­è’¸é¦è€Œå‡ºçš„ç”Ÿæˆå¼æ§åˆ¶ç­–ç•¥ï¼ˆgenerative control policyï¼‰ã€‚é€šè¿‡è¿™ç§ä¸¤é˜¶æ®µå­¦ä¹ è¿‡ç¨‹ï¼Œç³»ç»Ÿä¸ä»…èƒ½å¤Ÿå®ç°å¤æ‚çš„äº¤äº’è¡Œä¸ºï¼Œè¿˜èƒ½ä¸ºç”¨æˆ·æä¾›å¯¹è§’è‰²å’Œç‰©ä½“è¿åŠ¨çš„ç›´è§‚æ§åˆ¶ã€‚MaskedManipulator ç”Ÿæˆçš„ç›®æ ‡å¯¼å‘æ“æ§è¡Œä¸ºæ˜¾è‘—æ‰©å±•äº†äº¤äº’å¼åŠ¨ç”»ç³»ç»Ÿçš„èƒ½åŠ›è¾¹ç•Œï¼Œä½¿å…¶è¶…è¶Šäº†ç‰¹å®šä»»åŠ¡çš„å±€é™ï¼Œå®ç°äº†æ›´å…·é€šç”¨æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "comment": "SIGGRAPH Asia 2025 (Project page: https://research.nvidia.com/labs/par/maskedmanipulator/ )",
      "pdf_url": "https://arxiv.org/pdf/2505.19086v3",
      "published_date": "2025-05-25 10:46:14 UTC",
      "updated_date": "2025-12-11 17:25:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:07:22.615389+00:00"
    },
    {
      "arxiv_id": "2505.19084v1",
      "title": "Jodi: Unification of Visual Generation and Understanding via Joint Modeling",
      "title_zh": "Jodiï¼šé€šè¿‡è”åˆå»ºæ¨¡ç»Ÿä¸€è§†è§‰ç”Ÿæˆä¸ç†è§£",
      "authors": [
        "Yifeng Xu",
        "Zhenliang He",
        "Meina Kan",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "abstract": "Visual generation and understanding are two deeply interconnected aspects of human intelligence, yet they have been traditionally treated as separate tasks in machine learning. In this paper, we propose Jodi, a diffusion framework that unifies visual generation and understanding by jointly modeling the image domain and multiple label domains. Specifically, Jodi is built upon a linear diffusion transformer along with a role switch mechanism, which enables it to perform three particular types of tasks: (1) joint generation, where the model simultaneously generates images and multiple labels; (2) controllable generation, where images are generated conditioned on any combination of labels; and (3) image perception, where multiple labels can be predicted at once from a given image. Furthermore, we present the Joint-1.6M dataset, which contains 200,000 high-quality images collected from public sources, automatic labels for 7 visual domains, and LLM-generated captions. Extensive experiments demonstrate that Jodi excels in both generation and understanding tasks and exhibits strong extensibility to a wider range of visual domains. Code is available at https://github.com/VIPL-GENUN/Jodi.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Jodiï¼Œä¸€ä¸ªæ—¨åœ¨ç»Ÿä¸€è§†è§‰ç”Ÿæˆä¸ç†è§£çš„æ‰©æ•£æ¡†æ¶ (Diffusion Framework)ï¼Œé€šè¿‡å¯¹å›¾åƒåŸŸå’Œå¤šä¸ªæ ‡ç­¾åŸŸè¿›è¡Œè”åˆå»ºæ¨¡ (Joint Modeling) è§£å†³äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¸­ä¸¤é¡¹ä»»åŠ¡ç›¸äº’åˆ†ç¦»çš„é—®é¢˜ã€‚Jodi åŸºäºçº¿æ€§æ‰©æ•£å˜æ¢å™¨ (Linear Diffusion Transformer) æ„å»ºå¹¶å¼•å…¥äº†è§’è‰²åˆ‡æ¢æœºåˆ¶ (Role Switch Mechanism)ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶æ‰§è¡Œè”åˆç”Ÿæˆ (Joint Generation)ã€å¯æ§ç”Ÿæˆ (Controllable Generation) å’Œå›¾åƒæ„ŸçŸ¥ (Image Perception) ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…æ¨å‡ºäº†åŒ…å« 20 ä¸‡å¼ é«˜è´¨é‡å›¾åƒåŠ 7 ä¸ªè§†è§‰é¢†åŸŸè‡ªåŠ¨æ ‡ç­¾ä¸ LLM ç”Ÿæˆæè¿°çš„ Joint-1.6M æ•°æ®é›†ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒJodi åœ¨ç”Ÿæˆå’Œç†è§£é¢†åŸŸå‡è¡¨ç°å“è¶Šï¼Œå¹¶å¯¹æ›´å¹¿æ³›çš„è§†è§‰é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„æ‰©å±•æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code: https://github.com/VIPL-GENUN/Jodi",
      "pdf_url": "https://arxiv.org/pdf/2505.19084v1",
      "published_date": "2025-05-25 10:40:52 UTC",
      "updated_date": "2025-05-25 10:40:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:07:01.254138+00:00"
    },
    {
      "arxiv_id": "2505.19075v2",
      "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs",
      "title_zh": "Universal Reasonerï¼šé¢å‘å†»ç»“å¤§è¯­è¨€æ¨¡å‹çš„å•ä¸€ã€å¯ç»„åˆã€å³æ’å³ç”¨æ¨ç†å™¨",
      "authors": [
        "Jaemin Kim",
        "Hangeol Chang",
        "Hyunmin Hwang",
        "Choonghan Kim",
        "Jong Chul Ye"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable general capabilities, but enhancing skills such as reasoning often demands substantial computational resources and may compromise their generalization. While Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious alternative, they typically requires retraining for each LLM backbone due to architectural dependencies. To address these challenges, here we propose Universal Reasoner (UniR) - a single, lightweight, composable, and plug-and-play reasoning module that can be used with any frozen LLM to endow it with specialized reasoning capabilities. Specifically, UniR decomposes the reward into a standalone reasoning module that is trained independently using predefined rewards, effectively translating trajectory-level signals into token-level guidance. Once trained, UniR can be combined with any frozen LLM at inference time by simply adding its output logits to those of the LLM backbone. This additive structure naturally enables modular composition: multiple UniR modules trained for different tasks can be jointly applied by summing their logits, enabling complex reasoning via composition. Experimental results on mathematical reasoning and machine translation tasks show that UniR significantly outperforms existing baseline fine-tuning methods using the Llama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong generalization: reasoning modules trained on smaller models effectively guide much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust solution for enhancing reasoning in LLMs without compromising their core capabilities. Code is open-sourced at https://github.com/hangeol/UniR",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºUniversal Reasoner (UniR) çš„é€šç”¨æ¨ç†æ¨¡å—ï¼Œæ—¨åœ¨ä¸ºå„ç§å†»ç»“çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) æä¾›å³æ’å³ç”¨çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•å¯¹è®¡ç®—èµ„æºéœ€æ±‚å¤§ä¸”é«˜åº¦ä¾èµ–æ¨¡å‹æ¶æ„çš„æŒ‘æˆ˜ï¼ŒUniRå°†å¥–åŠ±æœºåˆ¶åˆ†è§£ä¸ºç‹¬ç«‹çš„æ¨ç†æ¨¡å—ï¼Œé€šè¿‡å°†è½¨è¿¹çº§ä¿¡å·è½¬åŒ–ä¸ºæ ‡è®°çº§æŒ‡å¯¼è¿›è¡Œç‹¬ç«‹è®­ç»ƒã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒUniRé€šè¿‡å°†å…¶è¾“å‡ºå¯¹æ•° (logits) ä¸ä¸»å¹²LLMç®€å•å åŠ å³å¯å®ç°åŠŸèƒ½é›†æˆï¼Œè¿™ç§åŠ æ³•ç»“æ„æ”¯æŒå¤šä¸ªé’ˆå¯¹ä¸åŒä»»åŠ¡è®­ç»ƒçš„UniRæ¨¡å—è¿›è¡Œæ¨¡å—åŒ–ç»„åˆã€‚å®éªŒè¯æ˜ï¼Œåœ¨Llama3.2æ¨¡å‹ä¸Šçš„æ•°å­¦æ¨ç†å’Œæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼ŒUniRçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) åŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒUniRå±•ç°äº†ä¼˜å¼‚çš„å¼±åˆ°å¼ºæ³›åŒ– (weak-to-strong generalization) èƒ½åŠ›ï¼Œå…è®¸åœ¨å°æ¨¡å‹ä¸Šè®­ç»ƒçš„æ¨ç†æ¨¡å—æœ‰æ•ˆå¼•å¯¼æ›´å¤§è§„æ¨¡çš„LLMã€‚è¯¥æ–¹æ¡ˆä¸ºåœ¨ä¸æŸå®³æ¨¡å‹æ ¸å¿ƒèƒ½åŠ›çš„å‰æä¸‹ï¼Œä½æˆæœ¬ä¸”ç¨³å¥åœ°å¢å¼ºLLMä¸“ä¸šæ¨ç†èƒ½åŠ›æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, typos corrected",
      "pdf_url": "https://arxiv.org/pdf/2505.19075v2",
      "published_date": "2025-05-25 10:19:10 UTC",
      "updated_date": "2025-05-27 13:53:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:07:03.504744+00:00"
    },
    {
      "arxiv_id": "2505.19059v1",
      "title": "An Initial Exploration of Fine-tuning Small Language Models for Smart Contract Reentrancy Vulnerability Detection",
      "title_zh": "å°è¯­è¨€æ¨¡å‹å¾®è°ƒåœ¨æ™ºèƒ½åˆçº¦é‡å…¥æ¼æ´æ£€æµ‹ä¸­çš„åˆæ­¥æ¢ç´¢",
      "authors": [
        "Ignacio Mariano Andreozzi Pofcher",
        "Joshua Ellul"
      ],
      "abstract": "Large Language Models (LLMs) are being used more and more for various coding tasks, including to help coders identify bugs and are a promising avenue to support coders in various tasks including vulnerability detection -- particularly given the flexibility of such generative AI models and tools. Yet for many tasks it may not be suitable to use LLMs, for which it may be more suitable to use smaller language models that can fit and easily execute and train on a developer's computer. In this paper we explore and evaluate whether smaller language models can be fine-tuned to achieve reasonable results for a niche area: vulnerability detection -- specifically focusing on detecting the reentrancy bug in Solidity smart contracts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é’ˆå¯¹ Solidity æ™ºèƒ½åˆçº¦ä¸­çš„ Reentrancy æ¼æ´æ£€æµ‹ï¼Œå¯¹å°å‹è¯­è¨€æ¨¡å‹ (Small Language Models) è¿›è¡Œå¾®è°ƒ (Fine-tuning) çš„åˆæ­¥å¯è¡Œæ€§ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ¼æ´è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åºå¤§çš„ä½“ç§¯ä½¿å¾—åœ¨å¼€å‘è€…ä¸ªäººç”µè„‘ä¸Šè¿›è¡Œå®æ—¶éƒ¨ç½²å’Œè®­ç»ƒå˜å¾—å›°éš¾ã€‚æœ¬æ–‡é€šè¿‡è¯„ä¼°ç»è¿‡ä¸“é—¨å¾®è°ƒçš„å°å‹æ¨¡å‹åœ¨ç‰¹å®šå®‰å…¨é¢†åŸŸçš„æ•ˆæœï¼Œæ—¨åœ¨æ¢ç©¶å…¶æ˜¯å¦èƒ½åœ¨ä½èµ„æºæ¶ˆè€—çš„æƒ…å†µä¸‹å®ç°åˆç†çš„æ£€æµ‹å‡†ç¡®åº¦ã€‚ç ”ç©¶æ ¸å¿ƒèšç„¦äºä»¥å¤ªåŠæ™ºèƒ½åˆçº¦ä¸­æœ€å¸¸è§çš„ Reentrancy æ¼æ´ï¼Œå¹¶éªŒè¯äº†å°å‹æ¨¡å‹åœ¨å¤„ç†æ­¤ç±»ç»†åˆ†å®‰å…¨ä»»åŠ¡æ—¶çš„æ½œåŠ›ã€‚è¯¥å·¥ä½œä¸ºå¼€å‘è½»é‡çº§ã€å¯æœ¬åœ°è¿è¡Œä¸”å…·å¤‡éšç§ä¿æŠ¤èƒ½åŠ›çš„æ™ºèƒ½åˆçº¦è¾…åŠ©å®¡è®¡å·¥å…·æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19059v1",
      "published_date": "2025-05-25 09:28:33 UTC",
      "updated_date": "2025-05-25 09:28:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:08:04.329018+00:00"
    },
    {
      "arxiv_id": "2506.02007v2",
      "title": "eACGM: Non-instrumented Performance Tracing and Anomaly Detection towards Machine Learning Systems",
      "title_zh": "eACGMï¼šé¢å‘æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ— æ’æ¡©æ€§èƒ½è¿½è¸ªä¸å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Ruilin Xu",
        "Zongxuan Xie",
        "Pengfei Chen"
      ],
      "abstract": "We present eACGM, a full-stack AI/ML system monitoring framework based on eBPF. eACGM collects real-time performance data from key hardware components, including the GPU and network communication layer, as well as from key software stacks such as CUDA, Python, and PyTorch, all without requiring any code instrumentation or modifications. Additionally, it leverages libnvml to gather process-level GPU resource usage information. By applying a Gaussian Mixture Model (GMM) to the collected multidimensional performance metrics for statistical modeling and clustering analysis, eACGM effectively identifies complex failure modes, such as latency anomalies, hardware failures, and communication inefficiencies, enabling rapid diagnosis of system bottlenecks and abnormal behaviors.\n  To evaluate eACGM's effectiveness and practicality, we conducted extensive empirical studies and case analyses in multi-node distributed training scenarios. The results demonstrate that eACGM, while maintaining a non-intrusive and low-overhead profile, successfully captures critical performance anomalies during model training and inference. Its stable anomaly detection performance and comprehensive monitoring capabilities validate its applicability and scalability in real-world production environments, providing strong support for performance optimization and fault diagnosis in large-scale AI/ML systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†eACGMï¼Œä¸€ä¸ªåŸºäºeBPFçš„å…¨æ ˆAI/MLç³»ç»Ÿç›‘æ§æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æ— éœ€ä»£ç æ’æ¡©(non-instrumented)çš„æ€§èƒ½è¿½è¸ªä¸å¼‚å¸¸æ£€æµ‹ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä»GPUã€ç½‘ç»œé€šä¿¡å±‚ä»¥åŠCUDAã€Pythonã€PyTorchç­‰å…³é”®è½¯ç¡¬ä»¶åè®®æ ˆä¸­å®æ—¶æ”¶é›†æ€§èƒ½æ•°æ®ï¼Œå¹¶ç»“åˆlibnvmlè·å–è¿›ç¨‹çº§GPUèµ„æºä½¿ç”¨æƒ…å†µã€‚é€šè¿‡åº”ç”¨é«˜æ–¯æ··åˆæ¨¡å‹(GMM)å¯¹å¤šç»´æ€§èƒ½æŒ‡æ ‡è¿›è¡Œç»Ÿè®¡å»ºæ¨¡ä¸èšç±»åˆ†æï¼ŒeACGMå¯ä»¥æœ‰æ•ˆè¯†åˆ«å»¶è¿Ÿå¼‚å¸¸ã€ç¡¬ä»¶æ•…éšœåŠé€šä¿¡ä½æ•ˆç­‰å¤æ‚æ•…éšœæ¨¡å¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒåœºæ™¯ä¸­ï¼ŒeACGMåœ¨ä¿æŒä½å¼€é”€å’Œéä¾µå…¥æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå‡†ç¡®æ•è·è®­ç»ƒä¸æ¨ç†è¿‡ç¨‹ä¸­çš„å…³é”®æ€§èƒ½å¼‚å¸¸ã€‚è¯¥ç³»ç»Ÿçš„ç¨³å®šæ€§ä¸å…¨é¢ç›‘æ§èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨çœŸå®ç”Ÿäº§ç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ä¸å¯æ‰©å±•æ€§ï¼Œä¸ºå¤§è§„æ¨¡AI/MLç³»ç»Ÿçš„æ€§èƒ½ä¼˜åŒ–å’Œæ•…éšœè¯Šæ–­æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.DC",
      "comment": "IWQoS 2025 (Camera-Ready Version)",
      "pdf_url": "https://arxiv.org/pdf/2506.02007v2",
      "published_date": "2025-05-25 09:25:39 UTC",
      "updated_date": "2025-07-01 11:37:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:07:46.746683+00:00"
    },
    {
      "arxiv_id": "2505.19056v2",
      "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks",
      "title_zh": "ä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ Abliteration æ”»å‡»çš„æç®€é˜²å¾¡æ–¹æ³•",
      "authors": [
        "Harethah Abu Shairah",
        "Hasan Abed Al Kader Hammoud",
        "Bernard Ghanem",
        "George Turkiyyah"
      ],
      "abstract": "Large language models (LLMs) are typically aligned to refuse harmful instructions through safety fine-tuning. A recent attack, termed abliteration, identifies and suppresses the single latent direction most responsible for refusal behavior, thereby enabling models to generate harmful content. We propose a defense that fundamentally alters how models express refusal. We construct an extended-refusal dataset in which responses to harmful prompts provide detailed justifications before refusing, distributing the refusal signal across multiple token positions. Fine-tuning Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on this dataset yields models that maintain high refusal rates under abliteration: refusal rates drop by at most 10%, compared to 70-80% drops in baseline models. Comprehensive evaluations of safety and utility demonstrate that extended-refusal fine-tuning effectively neutralizes abliteration attacks while preserving general model performance and enhancing robustness across multiple alignment scenarios.",
      "tldr_zh": "é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é¢ä¸´çš„abliterationæ”»å‡»ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æç®€ä¸”æœ‰æ•ˆçš„é˜²å¾¡æ–¹æ¡ˆï¼Œæ—¨åœ¨ä»æ ¹æœ¬ä¸Šæ”¹å˜æ¨¡å‹è¡¨è¾¾æ‹’ç»(refusal)è¡Œä¸ºçš„æ–¹å¼ã€‚ä¼ ç»Ÿçš„abliterationæ”»å‡»é€šè¿‡è¯†åˆ«å¹¶æŠ‘åˆ¶è´Ÿè´£æ‹’ç»çš„å•ä¸€æ½œåœ¨æ–¹å‘æ¥çªç ´å®‰å…¨å¯¹é½ï¼Œè€Œè¯¥ç ”ç©¶é€šè¿‡æ„å»ºextended-refusalæ•°æ®é›†ï¼Œè¦æ±‚æ¨¡å‹åœ¨æ­£å¼æ‹’ç»å‰æä¾›è¯¦ç»†çš„æ­£å½“ç†ç”±ï¼Œä»è€Œå°†æ‹’ç»ä¿¡å·åˆ†æ•£åˆ°å¤šä¸ªtokenä½ç½®ã€‚å®éªŒåœ¨Llama-2-7B-Chatå’ŒQwen2.5-Instructæ¨¡å‹ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜å¾®è°ƒåçš„æ¨¡å‹åœ¨é­å—æ”»å‡»æ—¶æ‹’ç»ç‡ä»…ä¸‹é™ä¸åˆ°10%ï¼Œè¿œä¼˜äºåŸºçº¿æ¨¡å‹70-80%çš„è·Œå¹…ã€‚ç»¼åˆè¯„ä¼°è¯æ˜ï¼Œè¿™ç§extended-refusalå¾®è°ƒæ–¹æ³•åœ¨ä¸æŸå®³æ¨¡å‹é€šç”¨æ€§èƒ½å’Œå®ç”¨æ€§çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¸­å’Œæ”»å‡»å¨èƒå¹¶æ˜¾è‘—æå‡æ¨¡å‹åœ¨å¤šç§å¯¹é½(alignment)åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚è¯¥ç ”ç©¶ä¸ºå¢å¼ºLLMsçš„é˜²å¾¡èƒ½åŠ›æä¾›äº†ä¸€ç§æå…·æ½œåŠ›çš„ä½æˆæœ¬è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "preprint - under review",
      "pdf_url": "https://arxiv.org/pdf/2505.19056v2",
      "published_date": "2025-05-25 09:18:24 UTC",
      "updated_date": "2025-10-07 11:31:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:07:55.354042+00:00"
    },
    {
      "arxiv_id": "2505.19040v1",
      "title": "Smart Waste Management System for Makkah City using Artificial Intelligence and Internet of Things",
      "title_zh": "åŸºäºäººå·¥æ™ºèƒ½ä¸ç‰©è”ç½‘çš„ Makkah æ™ºèƒ½åºŸç‰©ç®¡ç†ç³»ç»Ÿ",
      "authors": [
        "Rawabi S. Al Qurashi",
        "Maram M. Almnjomi",
        "Teef L. Alghamdi",
        "Amjad H. Almalki",
        "Shahad S. Alharthi",
        "Shahad M. althobuti",
        "Alanoud S. Alharthi",
        "Maha A. Thafar"
      ],
      "abstract": "Waste management is a critical global issue with significant environmental and public health implications. It has become more destructive during large-scale events such as the annual pilgrimage to Makkah, Saudi Arabia, one of the world's largest religious gatherings. This event's popularity has attracted millions worldwide, leading to significant and un-predictable accumulation of waste. Such a tremendous number of visitors leads to in-creased waste management issues at the Grand Mosque and other holy sites, highlighting the need for an effective solution other than traditional methods based on rigid collection schedules.\n  To address this challenge, this research proposed an innovative solution that is context-specific and tailored to the unique requirements of pilgrimage season: a Smart Waste Management System, called TUHR, that utilizes the Internet of Things and Artificial Intelligence. This system encompasses ultrasonic sensors that monitor waste levels in each container at the performance sites. Once the container reaches full capacity, the sensor communicates with the microcontroller, which alerts the relevant authorities. Moreover, our system can detect harmful substances such as gas from the gas detector sensor. Such a proactive and dynamic approach promises to mitigate the environmental and health risks associated with waste accumulation and enhance the cleanliness of these sites. It also delivers economic benefits by reducing unnecessary gasoline consumption and optimizing waste management resources. Importantly, this research aligns with the principles of smart cities and exemplifies the innovative, sustainable, and health-conscious approach that Saudi Arabia is implementing as part of its Vision 2030 initiative.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ²™ç‰¹é˜¿æ‹‰ä¼¯éº¦åŠ æœè§æœŸé—´äº§ç”Ÿçš„å·¨å¤§åƒåœ¾å¤„ç†æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º TUHR çš„æ™ºèƒ½åºŸç‰©ç®¡ç†ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé›†æˆäº†ç‰©è”ç½‘ (Internet of Things) å’Œäººå·¥æ™ºèƒ½ (Artificial Intelligence) æŠ€æœ¯ã€‚ç³»ç»Ÿé€šè¿‡åœ¨æ”¶é›†ç‚¹å®‰è£…è¶…å£°æ³¢ä¼ æ„Ÿå™¨ (ultrasonic sensors) å®æ—¶ç›‘æµ‹åƒåœ¾å¡«å……æ°´å¹³ï¼Œå¹¶åœ¨å®¹å™¨æ»¡è½½æ—¶ç”±å¾®æ§åˆ¶å™¨ (microcontroller) å‘ç›¸å…³éƒ¨é—¨å‘å‡ºé¢„è­¦ã€‚æ­¤å¤–ï¼ŒTUHR è¿˜é›†æˆäº†æ°”ä½“æ¢æµ‹ä¼ æ„Ÿå™¨ (gas detector sensor) ä»¥æ£€æµ‹æœ‰å®³ç‰©è´¨ï¼Œä»è€Œå®ç°å¯¹ç¯å¢ƒé£é™©çš„åŠ¨æ€ç›‘æ§ã€‚è¿™ç§ä¸»åŠ¨çš„ç®¡ç†æ–¹å¼ä¸ä»…æ˜¾è‘—æå‡äº†åœ£åœ°çš„æ¸…æ´åº¦ï¼Œè¿˜é€šè¿‡ä¼˜åŒ–èµ„æºåˆ†é…å’Œå‡å°‘ä¸å¿…è¦çš„ç‡ƒæ–™æ¶ˆè€—å¸¦æ¥äº†ç»æµæ•ˆç›Šã€‚è¯¥æ–¹æ¡ˆå±•ç¤ºäº†æ™ºèƒ½åŸå¸‚æŠ€æœ¯åœ¨å¤§è§„æ¨¡å…¬å…±æ´»åŠ¨ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œå¹¶ä¸æ²™ç‰¹é˜¿æ‹‰ä¼¯ 2030 æ„¿æ™¯ (Vision 2030) çš„å¯æŒç»­å‘å±•ç›®æ ‡é«˜åº¦å¥‘åˆã€‚",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.ET",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.19040v1",
      "published_date": "2025-05-25 08:42:13 UTC",
      "updated_date": "2025-05-25 08:42:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:07:52.212296+00:00"
    },
    {
      "arxiv_id": "2505.19038v3",
      "title": "Turb-L1: Achieving Long-term Turbulence Tracing By Tackling Spectral Bias",
      "title_zh": "Turb-L1ï¼šé€šè¿‡å…‹æœé¢‘è°±åå·®å®ç°é•¿æœŸæ¹æµè¿½è¸ª",
      "authors": [
        "Hao Wu",
        "Yuan Gao",
        "Chang Liu",
        "Fan Xu",
        "Fan Zhang",
        "Zhihong Zhu",
        "Yuqi Li",
        "Xian Wu",
        "Yuxuan Liang",
        "Li Liu",
        "Qingsong Wen",
        "Kun Wang",
        "Yu Zheng",
        "Xiaomeng Huang"
      ],
      "abstract": "Accurately predicting the long-term evolution of turbulence is crucial for advancing scientific understanding and optimizing engineering applications. However, existing deep learning methods face significant bottlenecks in long-term autoregressive prediction, which exhibit excessive smoothing and fail to accurately track complex fluid dynamics. Our extensive experimental and spectral analysis of prevailing methods provides an interpretable explanation for this shortcoming, identifying Spectral Bias as the core obstacle. Concretely, spectral bias is the inherent tendency of models to favor low-frequency, smooth features while overlooking critical high-frequency details during training, thus reducing fidelity and causing physical distortions in long-term predictions. Building on this insight, we propose Turb-L1, an innovative turbulence prediction method, which utilizes a Hierarchical Dynamics Synthesis mechanism within a multi-grid architecture to explicitly overcome spectral bias. It accurately captures cross-scale interactions and preserves the fidelity of high-frequency dynamics, enabling reliable long-term tracking of turbulence evolution. Extensive experiments on the 2D turbulence benchmark show that Turb-L1 demonstrates excellent performance: (I) In long-term predictions, it reduces Mean Squared Error (MSE) by $80.3\\%$ and increases Structural Similarity (SSIM) by over $9\\times$ compared to the SOTA baseline, significantly improving prediction fidelity. (II) It effectively overcomes spectral bias, accurately reproducing the full enstrophy spectrum and maintaining physical realism in high-wavenumber regions, thus avoiding the spectral distortions or spurious energy accumulation seen in other methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ åœ¨æ¹æµé•¿ç¨‹é¢„æµ‹ä¸­å› è¿‡åº¦å¹³æ»‘è€Œæ— æ³•å‡†ç¡®è¿½è¸ªå¤æ‚æµä½“åŠ¨åŠ›å­¦çš„é—®é¢˜ï¼Œé€šè¿‡è°±åˆ†æè¯†åˆ«å‡ºâ€œè°±åå·®(Spectral Bias)â€æ˜¯å¯¼è‡´é¢„æµ‹ç‰©ç†å¤±çœŸçš„æ ¸å¿ƒéšœç¢ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†Turb-L1ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç½‘æ ¼æ¶æ„ä¸­å¼•å…¥å±‚çº§åŠ¨åŠ›å­¦åˆæˆ(Hierarchical Dynamics Synthesis)æœºåˆ¶ï¼Œæ—¨åœ¨æ˜¾å¼å…‹æœè°±åå·®å¹¶æ•æ‰è·¨å°ºåº¦ç›¸äº’ä½œç”¨ã€‚Turb-L1èƒ½å¤Ÿæœ‰æ•ˆä¿ç•™é«˜é¢‘åŠ¨åŠ›å­¦çš„ä¿çœŸåº¦ï¼Œä»è€Œå®ç°å¯¹æ¹æµæ¼”åŒ–çš„å¯é é•¿ç¨‹è¿½è¸ªã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨2Dæ¹æµåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTurb-L1ç›¸æ¯”SOTAåŸºçº¿å°†å‡æ–¹è¯¯å·®(MSE)é™ä½äº†80.3%ï¼Œç»“æ„ç›¸ä¼¼æ€§(SSIM)æå‡äº†9å€ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æˆåŠŸå¤ç°äº†å®Œæ•´çš„æ¶¡åº¦è°±(enstrophy spectrum)ï¼Œå¹¶åœ¨é«˜æ³¢æ®µåŒºåŸŸä¿æŒäº†ç‰©ç†çœŸå®æ€§ï¼Œæœ‰æ•ˆé¿å…äº†å…¶ä»–æ–¹æ³•ä¸­å¸¸è§çš„èƒ½é‡ç§¯ç´¯æˆ–è°±å¤±çœŸã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19038v3",
      "published_date": "2025-05-25 08:38:55 UTC",
      "updated_date": "2025-11-19 05:11:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:07:53.313815+00:00"
    },
    {
      "arxiv_id": "2505.19031v1",
      "title": "Medical Large Vision Language Models with Multi-Image Visual Ability",
      "title_zh": "å…·å¤‡å¤šå›¾åƒè§†è§‰èƒ½åŠ›çš„åŒ»ç–—å¤§è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "Xikai Yang",
        "Juzheng Miao",
        "Yuchen Yuan",
        "Jiaze Wang",
        "Qi Dou",
        "Jinpeng Li",
        "Pheng-Ann Heng"
      ],
      "abstract": "Medical large vision-language models (LVLMs) have demonstrated promising performance across various single-image question answering (QA) benchmarks, yet their capability in processing multi-image clinical scenarios remains underexplored. Unlike single image based tasks, medical tasks involving multiple images often demand sophisticated visual understanding capabilities, such as temporal reasoning and cross-modal analysis, which are poorly supported by current medical LVLMs. To bridge this critical gap, we present the Med-MIM instruction dataset, comprising 83.2K medical multi-image QA pairs that span four types of multi-image visual abilities (temporal understanding, reasoning, comparison, co-reference). Using this dataset, we fine-tune Mantis and LLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and Med-Mantis, both optimized for multi-image analysis. Additionally, we develop the Med-MIM benchmark to comprehensively evaluate the medical multi-image understanding capabilities of LVLMs. We assess eight popular LVLMs, including our two models, on the Med-MIM benchmark. Experimental results show that both Med-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and held-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM instruction dataset effectively enhances LVLMs' multi-image understanding capabilities in the medical domain.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å¤§è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) åœ¨å¤„ç†å¤šå›¾åƒ (multi-image) ä¸´åºŠåœºæ™¯æ—¶å­˜åœ¨çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºäº†å½“å‰æ¨¡å‹åœ¨æ—¶åºæ¨ç† (temporal reasoning) å’Œè·¨æ¨¡æ€åˆ†ææ–¹é¢çš„èƒ½åŠ›ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº† Med-MIM æŒ‡ä»¤æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å« 8.32 ä¸‡ä¸ªåŒ»ç–—å¤šå›¾åƒé—®ç­” (QA) å¯¹ï¼Œæ¶µç›–äº†æ—¶åºç†è§£ã€æ¨ç†ã€æ¯”è¾ƒå’Œå…±æŒ‡å››ç§æ ¸å¿ƒè§†è§‰èƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨è¯¥æ•°æ®é›†å¯¹ Mantis å’Œ LLaVA-Med è¿›è¡Œå¾®è°ƒï¼Œç ”ç©¶è€…æˆåŠŸå¼€å‘å‡º MIM-LLaVA-Med å’Œ Med-Mantis ä¸¤ä¸ªä¸“æ³¨äºå¤šå›¾åƒåˆ†æçš„ä¸“ä¸šæ¨¡å‹ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶è¿˜æ„å»ºäº† Med-MIM benchmark è¯„ä»·åŸºå‡†ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼° LVLMs çš„å¤šå›¾åƒç†è§£æ°´å¹³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ–°å¼€å‘çš„æ¨¡å‹åœ¨ Med-MIM åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¼˜å¼‚æˆç»©ï¼Œè¯æ˜äº† Med-MIM æ•°æ®é›†èƒ½æ˜¾è‘—å¢å¼ºåŒ»ç–— LVLMs åœ¨å¤æ‚ä¸´åºŠç¯å¢ƒä¸‹çš„å¤šå›¾åƒå¤„ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.19031v1",
      "published_date": "2025-05-25 08:31:22 UTC",
      "updated_date": "2025-05-25 08:31:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:08:05.180028+00:00"
    },
    {
      "arxiv_id": "2505.19030v4",
      "title": "RECAST: Expanding the Boundaries of LLMs' Complex Instruction Following with Multi-Constraint Data",
      "title_zh": "RECASTï¼šåˆ©ç”¨å¤šçº¦æŸæ•°æ®æ‹“å±•å¤§è¯­è¨€æ¨¡å‹å¤æ‚æŒ‡ä»¤éµå¾ªçš„è¾¹ç•Œ",
      "authors": [
        "Zhengkang Guo",
        "Wenhao Liu",
        "Mingchen Xie",
        "Jingwen Xu",
        "Zisu Huang",
        "Muzhao Tian",
        "Jianhan Xu",
        "Yuanzhe Shen",
        "Qi Qian",
        "Muling Wu",
        "Xiaohua Wang",
        "Changze Lv",
        "He-Da Wang",
        "Hu Yao",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "abstract": "Large language models (LLMs) are increasingly expected to tackle complex tasks, driven by their expanding applications and users' growing proficiency in crafting sophisticated prompts. However, as the number of explicitly stated requirements increases (particularly more than 10 constraints), LLMs often struggle to accurately follow such complex instructions, which limits their applicability in complex real-world scenarios. To the best of our knowledge, existing datasets do not exceed 10 constraints per instance. To address this challenge, we propose RECAST, an efficient and scalable framework for synthesizing datasets where each example incorporates far more constraints than those in existing benchmarks, aiming to challenge and extend the boundaries of models' ability to follow complex instructions. These constraints are extracted from real-world prompt-response pairs to ensure practical relevance. Using this framework, we construct RECAST-30K, a large-scale, high-quality dataset comprising 30k instances spanning 19 constraint types. Experimental results demonstrate that models finetuned on RECAST-30K substantially improve in following complex instructions while maintaining their general capabilities without degradation. Moreover, RECAST enables automatic verification of constraint satisfaction via rule-based validators for quantitative constraints and LLM-based validators for qualitative ones; the verifiability provided by RECAST enables the design of reward functions for reinforcement learning, which further boosts model performance on complex and challenging tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RECAST æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†åŒ…å«è¶…è¿‡ 10 ä¸ªæ˜¾å¼çº¦æŸçš„å¤æ‚æŒ‡ä»¤æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ•°æ®é›†çº¦æŸæ•°é‡ä¸è¶³çš„å±€é™æ€§ï¼ŒRECAST ä»çœŸå®åœºæ™¯çš„æç¤ºè¯ä¸­æå–çº¦æŸï¼Œå®ç°äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„å¤æ‚æŒ‡ä»¤æ•°æ®åˆæˆã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥æ¡†æ¶æ„å»ºäº†åŒ…å« 3 ä¸‡æ¡é«˜è´¨é‡å®ä¾‹å’Œ 19 ç§çº¦æŸç±»å‹çš„ RECAST-30K æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ RECAST-30K ä¸Šè¿›è¡Œå¾®è°ƒèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºæ¨¡å‹éµå¾ªå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶é€šç”¨èƒ½åŠ›ä¸å—æŸã€‚æ­¤å¤–ï¼ŒRECAST æ”¯æŒé€šè¿‡åŸºäºè§„åˆ™å’ŒåŸºäº LLM çš„éªŒè¯å™¨è¿›è¡Œè‡ªåŠ¨çº¦æŸéªŒè¯ï¼Œä¸º Reinforcement Learning ä¸­çš„å¥–åŠ±å‡½æ•°è®¾è®¡æä¾›äº†åŸºç¡€ã€‚è¿™ç§å¯éªŒè¯æ€§è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹åœ¨åº”å¯¹æç«¯å¤æ‚å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡æ—¶çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19030v4",
      "published_date": "2025-05-25 08:31:08 UTC",
      "updated_date": "2025-10-04 12:53:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:08:51.488188+00:00"
    },
    {
      "arxiv_id": "2505.19028v4",
      "title": "InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts",
      "title_zh": "InfoChartQAï¼šé’ˆå¯¹ä¿¡æ¯å›¾è¡¨å¤šæ¨¡æ€é—®ç­”çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Tianchi Xie",
        "Minzhi Lin",
        "Mengchen Liu",
        "Yilin Ye",
        "Changjian Chen",
        "Shixia Liu"
      ],
      "abstract": "Understanding infographic charts with design-driven visual elements (e.g., pictograms, icons) requires both visual recognition and reasoning, posing challenges for multimodal large language models (MLLMs). However, existing visual-question answering benchmarks fall short in evaluating these capabilities of MLLMs due to the lack of paired plain charts and visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a benchmark for evaluating MLLMs on infographic chart understanding. It includes 5,642 pairs of infographic and plain charts, each sharing the same underlying data but differing in visual presentations. We further design visual-element-based questions to capture their unique visual designs and communicative intent. Evaluation of 20 MLLMs reveals a substantial performance decline on infographic charts, particularly for visual-element-based questions related to metaphors. The paired infographic and plain charts enable fine-grained error analysis and ablation studies, which highlight new opportunities for advancing MLLMs in infographic chart understanding. We release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† InfoChartQAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) å¯¹ä¿¡æ¯å›¾è¡¨ (Infographic Charts) ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†å¼¥è¡¥ç°æœ‰åŸºå‡†åœ¨é…å¯¹å›¾è¡¨å’Œè§†è§‰å…ƒç´ æé—®æ–¹é¢çš„ä¸è¶³ï¼Œè¯¥åŸºå‡†åŒ…å«äº† 5,642 å¯¹ä¿¡æ¯å›¾ä¸æ™®é€šå›¾è¡¨ï¼Œå®ƒä»¬å…±äº«ç›¸åŒæ•°æ®ä½†è§†è§‰å‘ˆç°å½¢å¼å„å¼‚ã€‚ç ”ç©¶è€…é€šè¿‡è®¾è®¡åŸºäºè§†è§‰å…ƒç´ çš„æé—®ï¼Œæ·±å…¥æ¢ç©¶äº†å›¾è¡¨çš„ç‹¬ç‰¹è®¾è®¡åŠä¼ è¾¾æ„å›¾ã€‚å¯¹ 20 ç§ä¸»æµ MLLMs çš„è¯„ä¼°æ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†ä¿¡æ¯å›¾è¡¨æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶åœ¨æ¶‰åŠéšå–» (Metaphors) çš„è§†è§‰è¯†åˆ«ä¸æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æ¬ ä½³ã€‚InfoChartQA æä¾›çš„é…å¯¹æ•°æ®æ”¯æŒç»†ç²’åº¦çš„é”™è¯¯åˆ†æï¼Œä¸ºæœªæ¥æå‡å¤šæ¨¡æ€æ¨¡å‹å¯¹å¤æ‚è§†è§‰è®¾è®¡çš„è§£æèƒ½åŠ›æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19028v4",
      "published_date": "2025-05-25 08:28:03 UTC",
      "updated_date": "2025-10-29 06:12:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:08:11.260894+00:00"
    },
    {
      "arxiv_id": "2505.19023v1",
      "title": "A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking",
      "title_zh": "çŒ´ç—˜çš®æŸæ£€æµ‹ä¸è¿½è¸ªæ™ºèƒ½åŒ»ç–—ç³»ç»Ÿ",
      "authors": [
        "Huda Alghoraibi",
        "Nuha Alqurashi",
        "Sarah Alotaibi",
        "Renad Alkhudaydi",
        "Bdoor Aldajani",
        "Lubna Alqurashi",
        "Jood Batweel",
        "Maha A. Thafar"
      ],
      "abstract": "Monkeypox is a viral disease characterized by distinctive skin lesions and has been reported in many countries. The recent global outbreak has emphasized the urgent need for scalable, accessible, and accurate diagnostic solutions to support public health responses.\n  In this study, we developed ITMAINN, an intelligent, AI-driven healthcare system specifically designed to detect Monkeypox from skin lesion images using advanced deep learning techniques. Our system consists of three main components. First, we trained and evaluated several pretrained models using transfer learning on publicly available skin lesion datasets to identify the most effective models. For binary classification (Monkeypox vs. non-Monkeypox), the Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16 achieved the highest performance, each with an accuracy and F1-score of 97.8%. For multiclass classification, which contains images of patients with Monkeypox and five other classes (chickenpox, measles, hand-foot-mouth disease, cowpox, and healthy), ResNetViT and ViT Hybrid models achieved 92% accuracy, with F1 scores of 92.24% and 92.19%, respectively. The best-performing and most lightweight model, MobileViT, was deployed within the mobile application. The second component is a cross-platform smartphone application that enables users to detect Monkeypox through image analysis, track symptoms, and receive recommendations for nearby healthcare centers based on their location. The third component is a real-time monitoring dashboard designed for health authorities to support them in tracking cases, analyzing symptom trends, guiding public health interventions, and taking proactive measures.\n  This system is fundamental in developing responsive healthcare infrastructure within smart cities. Our solution, ITMAINN, is part of revolutionizing public health management.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åä¸º ITMAINN çš„æ™ºèƒ½åŒ»ç–—ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ä»çš®è‚¤ç—…å˜å›¾åƒä¸­æ£€æµ‹å’Œè·Ÿè¸ª Monkeypoxã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨è¿ç§»å­¦ä¹  (Transfer Learning) å¯¹å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå…¶ä¸­ Vision Transformerã€MobileViTã€Transformer-in-Transformer å’Œ VGG16 åœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº† 97.8% çš„å‡†ç¡®ç‡å’Œ F1-scoreã€‚åœ¨åŒºåˆ†æ°´ç—˜ã€éº»ç–¹ç­‰å…­ç±»ç–¾ç—…çš„å¤šåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒResNetViT å’Œ ViT Hybrid æ¨¡å‹çš„å‡†ç¡®ç‡ä¹Ÿè¾¾åˆ°äº† 92%ã€‚ä¸ºäº†å®ç°ç§»åŠ¨ç«¯çš„å®é™…åº”ç”¨ï¼Œç ”ç©¶è€…å°†æ€§èƒ½ä¼˜å¼‚ä¸”è½»é‡åŒ–çš„ MobileViT æ¨¡å‹é›†æˆåˆ°è·¨å¹³å°åº”ç”¨ç¨‹åºä¸­ï¼Œæ”¯æŒç”¨æˆ·è¿›è¡Œç—…å˜æ£€æµ‹ã€ç—‡çŠ¶è¿½è¸ªåŠè·å–åŒ»ç–—å»ºè®®ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜åŒ…å«ä¸€ä¸ªé¢å‘å«ç”Ÿéƒ¨é—¨çš„å®æ—¶ç›‘æ§ä»ªè¡¨æ¿ (Dashboard)ï¼Œç”¨äºè¿½è¸ªç—…ä¾‹è¶‹åŠ¿å¹¶æŒ‡å¯¼å…¬å…±å«ç”Ÿå¹²é¢„ã€‚ITMAINN ç³»ç»Ÿçš„æå‡ºä¸ºæ™ºæ…§åŸå¸‚æ„å»ºå“åº”å¼åŒ»ç–—åŸºç¡€è®¾æ–½æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œåœ¨å…¬å…±å«ç”Ÿç®¡ç†é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "23 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.19023v1",
      "published_date": "2025-05-25 08:17:21 UTC",
      "updated_date": "2025-05-25 08:17:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:08:19.244985+00:00"
    },
    {
      "arxiv_id": "2505.19022v2",
      "title": "Rethinking Metrics and Benchmarks of Video Anomaly Detection",
      "title_zh": "é‡æ–°å®¡è§†è§†é¢‘å¼‚å¸¸æ£€æµ‹çš„è¯„ä¼°æŒ‡æ ‡ä¸åŸºå‡†",
      "authors": [
        "Zihao Liu",
        "Xiaoyu Wu",
        "Wenna Li",
        "Linlin Yang",
        "Shengjin Wang"
      ],
      "abstract": "Video Anomaly Detection (VAD), which aims to detect anomalies that deviate from expectation, has attracted increasing attention in recent years. Existing advancements in VAD primarily focus on model architectures and training strategies, while devoting insufficient attention to evaluation metrics and benchmarks. In this paper, we rethink VAD evaluation methods through comprehensive analyses, revealing three critical limitations in current practices: 1) existing metrics are significantly influenced by single annotation bias; 2) current metrics fail to reward early detection of anomalies; 3) available benchmarks lack the capability to evaluate scene overfitting of fully/weakly-supervised algorithms. To address these limitations, we propose three novel evaluation methods: first, we establish probabilistic AUC/AP (Prob-AUC/AP) metrics utlizing multi-round annotations to mitigate single annotation bias; second, we develop a Latency-aware Average Precision (LaAP) metric that rewards early and accurate anomaly detection; and finally, we introduce two hard normal benchmarks (UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene overfitting. We report performance comparisons of ten state-of-the-art VAD approaches using our proposed evaluation methods, providing novel perspectives for future VAD model development. We release our data and code in https://github.com/Kamino666/RethinkingVAD.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°å®¡è§†äº†è§†é¢‘å¼‚å¸¸æ£€æµ‹(Video Anomaly Detection, VAD)çš„è¯„ä¼°æŒ‡æ ‡ä¸åŸºå‡†æµ‹è¯•ï¼ŒæŒ‡å‡ºå½“å‰å®è·µä¸­å­˜åœ¨å•ä¸€æ ‡æ³¨åè§ã€ç¼ºä¹æ—©æœŸæ£€æµ‹æ¿€åŠ±ä»¥åŠæ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœºæ™¯è¿‡åº¦æ‹Ÿåˆ(Scene Overfitting)ç­‰å…³é”®å±€é™ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œä½œè€…é¦–å…ˆæå‡ºäº†åŸºäºå¤šè½®æ ‡æ³¨çš„æ¦‚ç‡æŒ‡æ ‡Prob-AUC/APï¼Œä»¥å‡è½»å•ä¸€æ ‡æ³¨å¸¦æ¥çš„åå·®ã€‚å…¶æ¬¡ï¼Œç ”ç©¶å¼€å‘äº†å»¶è¿Ÿæ„ŸçŸ¥å¹³å‡ç²¾åº¦(Latency-aware Average Precision, LaAP)æŒ‡æ ‡ï¼Œé€šè¿‡å¥–åŠ±æ—©æœŸä¸”å‡†ç¡®çš„å¼‚å¸¸æ£€æµ‹æ¥æå‡è¯„ä¼°çš„å®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œè¯¥å›¢é˜Ÿè¿˜æ¨å‡ºäº†UCF-HNå’ŒMSAD-HNä¸¤ä¸ªç¡¬æ ·æœ¬(Hard Normal)åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºæµ‹è¯•ç›‘ç£ç®—æ³•åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¯¹åç§æœ€å…ˆè¿›çš„VADæ–¹æ³•è¿›è¡Œæ·±å…¥çš„å¯¹æ¯”åˆ†æï¼Œè¯¥å·¥ä½œä¸ºé¢†åŸŸæœªæ¥çš„å‘å±•æä¾›äº†æ–°çš„è§†è§’å’Œè¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19022v2",
      "published_date": "2025-05-25 08:09:42 UTC",
      "updated_date": "2025-10-31 09:50:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:08:51.762950+00:00"
    },
    {
      "arxiv_id": "2505.19020v1",
      "title": "HGCL: Hierarchical Graph Contrastive Learning for User-Item Recommendation",
      "title_zh": "HGCLï¼šé¢å‘ç”¨æˆ·-é¡¹ç›®æ¨èçš„å±‚çº§åŒ–å›¾å¯¹æ¯”å­¦ä¹ ",
      "authors": [
        "Jiawei Xue",
        "Zhen Yang",
        "Haitao Lin",
        "Ziji Zhang",
        "Luzhu Wang",
        "Yikun Gu",
        "Yao Xu",
        "Xin Li"
      ],
      "abstract": "Graph Contrastive Learning (GCL), which fuses graph neural networks with contrastive learning, has evolved as a pivotal tool in user-item recommendations. While promising, existing GCL methods often lack explicit modeling of hierarchical item structures, which represent item similarities across varying resolutions. Such hierarchical item structures are ubiquitous in various items (e.g., online products and local businesses), and reflect their inherent organizational properties that serve as critical signals for enhancing recommendation accuracy. In this paper, we propose Hierarchical Graph Contrastive Learning (HGCL), a novel GCL method that incorporates hierarchical item structures for user-item recommendations. First, HGCL pre-trains a GCL module using cross-layer contrastive learning to obtain user and item representations. Second, HGCL employs a representation compression and clustering method to construct a two-hierarchy user-item bipartite graph. Ultimately, HGCL fine-tunes user and item representations by learning on the hierarchical graph, and then provides recommendations based on user-item interaction scores. Experiments on three widely adopted benchmark datasets ranging from 70K to 382K nodes confirm the superior performance of HGCL over existing baseline models, highlighting the contribution of hierarchical item structures in enhancing GCL methods for recommendation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å›¾å¯¹æ¯”å­¦ä¹ (Graph Contrastive Learning, GCL)åœ¨æ¨èç³»ç»Ÿä¸­ç¼ºä¹å¯¹å±‚çº§ç‰©å“ç»“æ„(hierarchical item structures)æ˜¾å¼å»ºæ¨¡çš„é—®é¢˜ï¼Œæå‡ºäº†å±‚çº§å›¾å¯¹æ¯”å­¦ä¹ (Hierarchical Graph Contrastive Learning, HGCL)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡è·¨å±‚å¯¹æ¯”å­¦ä¹ (cross-layer contrastive learning)é¢„è®­ç»ƒGCLæ¨¡å—ä»¥è·å–ç”¨æˆ·å’Œç‰©å“è¡¨ç¤ºï¼Œéšååˆ©ç”¨è¡¨ç¤ºå‹ç¼©ä¸èšç±»æŠ€æœ¯æ„å»ºå‡ºä¸€ä¸ªåŒå±‚çº§çš„ç”¨æˆ·-ç‰©å“äºŒåˆ†å›¾ã€‚é€šè¿‡åœ¨è¯¥å±‚çº§å›¾ä¸Šè¿›ä¸€æ­¥å­¦ä¹ å’Œå¾®è°ƒè¡¨ç¤ºï¼ŒHGCLèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ç‰©å“é—´å›ºæœ‰çš„ç»„ç»‡ç‰¹æ€§å¹¶ç”Ÿæˆç²¾å‡†æ¨èã€‚åœ¨ä¸‰ä¸ªå¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯å®ï¼ŒHGCLçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡äº†æ¨èå‡†ç¡®æ€§ï¼Œæ›´éªŒè¯äº†å±‚çº§ç‰©å“ç»“æ„åœ¨å¢å¼ºå¯¹æ¯”å­¦ä¹ æ¨¡å‹ä¸­çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.19020v1",
      "published_date": "2025-05-25 07:56:56 UTC",
      "updated_date": "2025-05-25 07:56:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:09:29.641911+00:00"
    },
    {
      "arxiv_id": "2505.19013v1",
      "title": "Faithful Group Shapley Value",
      "title_zh": "å¿ å®ç¾¤ç»„ Shapley å€¼",
      "authors": [
        "Kiljae Lee",
        "Ziqi Liu",
        "Weijing Tang",
        "Yuan Zhang"
      ],
      "abstract": "Data Shapley is an important tool for data valuation, which quantifies the contribution of individual data points to machine learning models. In practice, group-level data valuation is desirable when data providers contribute data in batch. However, we identify that existing group-level extensions of Data Shapley are vulnerable to shell company attacks, where strategic group splitting can unfairly inflate valuations. We propose Faithful Group Shapley Value (FGSV) that uniquely defends against such attacks. Building on original mathematical insights, we develop a provably fast and accurate approximation algorithm for computing FGSV. Empirical experiments demonstrate that our algorithm significantly outperforms state-of-the-art methods in computational efficiency and approximation accuracy, while ensuring faithful group-level valuation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•°æ®ä¼°å€¼å·¥å…· Data Shapley åœ¨ç»„çº§åº”ç”¨ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•å®¹æ˜“å—åˆ°é€šè¿‡ç­–ç•¥æ€§åˆ†å‰²ç»„æ¥è™šå¢ä¼°å€¼çš„â€œç©ºå£³å…¬å¸æ”»å‡»â€(shell company attacks)ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å¿ å®ç»„ Shapley å€¼ (Faithful Group Shapley Value, FGSV)ï¼Œä½œä¸ºå”¯ä¸€èƒ½å¤Ÿæœ‰æ•ˆé˜²å¾¡æ­¤ç±»æ”»å‡»çš„æ–¹æ¡ˆã€‚åŸºäºåŸåˆ›çš„æ•°å­¦æ´è§ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§å…·å¤‡ç†è®ºä¿è¯çš„å¿«é€Ÿä¸”é«˜ç²¾åº¦çš„è¿‘ä¼¼è®¡ç®—ç®—æ³•ã€‚å®è¯å®éªŒè¯æ˜ï¼Œè¯¥ç®—æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œè¿‘ä¼¼å‡†ç¡®åº¦ä¸Šå‡æ˜¾è‘—è¶…è¿‡äº†ç°æœ‰çš„å‰æ²¿æ–¹æ³•ï¼ŒåŒæ—¶ç¡®ä¿äº†ç»„çº§æ•°æ®ä¼°å€¼çš„å¿ å®æ€§ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "econ.GN",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19013v1",
      "published_date": "2025-05-25 07:32:12 UTC",
      "updated_date": "2025-05-25 07:32:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:09:31.029070+00:00"
    },
    {
      "arxiv_id": "2505.19003v1",
      "title": "Aligning LLM with human travel choices: a persona-based embedding learning approach",
      "title_zh": "é¢å‘äººç±»å‡ºè¡Œé€‰æ‹©çš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½ï¼šåŸºäºç”»åƒçš„åµŒå…¥å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Tianming Liu",
        "Manzi Li",
        "Yafeng Yin"
      ],
      "abstract": "The advent of large language models (LLMs) presents new opportunities for travel demand modeling. However, behavioral misalignment between LLMs and humans presents obstacles for the usage of LLMs, and existing alignment methods are frequently inefficient or impractical given the constraints of typical travel demand data. This paper introduces a novel framework for aligning LLMs with human travel choice behavior, tailored to the current travel demand data sources. Our framework uses a persona inference and loading process to condition LLMs with suitable prompts to enhance alignment. The inference step establishes a set of base personas from empirical data, and a learned persona loading function driven by behavioral embeddings guides the loading process. We validate our framework on the Swissmetro mode choice dataset, and the results show that our proposed approach significantly outperformed baseline choice models and LLM-based simulation models in predicting both aggregate mode choice shares and individual choice outcomes. Furthermore, we showcase that our framework can generate insights on population behavior through interpretable parameters. Overall, our research offers a more adaptable, interpretable, and resource-efficient pathway to robust LLM-based travel behavior simulation, paving the way to integrate LLMs into travel demand modeling practice in the future.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸äººç±»å‡ºè¡Œé€‰æ‹©è¡Œä¸ºä¹‹é—´çš„å¯¹é½ (behavioral misalignment) é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäººæ ¼æ¨ç†ä¸åŠ è½½ (persona inference and loading) çš„åµŒå…¥å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»ç»éªŒæ•°æ®ä¸­æå–åŸºç¡€äººæ ¼ (base personas)ï¼Œå¹¶åˆ©ç”¨è¡Œä¸ºåµŒå…¥ (behavioral embeddings) é©±åŠ¨çš„åŠ è½½å‡½æ•°ä¼˜åŒ–æç¤ºè¯å¼•å¯¼è¿‡ç¨‹ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨å‡ºè¡Œéœ€æ±‚å»ºæ¨¡ä¸­çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚å®éªŒåœ¨ Swissmetro æ¨¡å¼é€‰æ‹©æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨é¢„æµ‹æ€»ä½“æ¨¡å¼å æ¯”åŠä¸ªä½“å†³ç­–æ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é€‰æ‹©æ¨¡å‹ (choice models) å’ŒåŸºäº LLM çš„åŸºå‡†æ¨¡æ‹Ÿæ¨¡å‹ã€‚é€šè¿‡å¼•å…¥å¯è§£é‡Šå‚æ•°ï¼Œè¯¥ç ”ç©¶ä¸ºç†è§£ç¾¤ä½“è¡Œä¸ºæä¾›äº†æ–°é€”å¾„ï¼Œä¸ºå°† LLMs é›†æˆåˆ°äº¤é€šéœ€æ±‚å»ºæ¨¡ (travel demand modeling) å®è·µä¸­æä¾›äº†ä¸€æ¡é«˜æ•ˆä¸”å…·å¤‡é²æ£’æ€§çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.19003v1",
      "published_date": "2025-05-25 06:54:01 UTC",
      "updated_date": "2025-05-25 06:54:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:09:16.315976+00:00"
    },
    {
      "arxiv_id": "2505.19002v1",
      "title": "Semi-pessimistic Reinforcement Learning",
      "title_zh": "åŠæ‚²è§‚å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Jin Zhu",
        "Xin Zhou",
        "Jiaang Yao",
        "Gholamali Aminian",
        "Omar Rivasplata",
        "Simon Little",
        "Lexin Li",
        "Chengchun Shi"
      ],
      "abstract": "Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected data. However, it faces challenges of distributional shift, where the learned policy may encounter unseen scenarios not covered in the offline data. Additionally, numerous applications suffer from a scarcity of labeled reward data. Relying on labeled data alone often leads to a narrow state-action distribution, further amplifying the distributional shift, and resulting in suboptimal policy learning. To address these issues, we first recognize that the volume of unlabeled data is typically substantially larger than that of labeled data. We then propose a semi-pessimistic RL method to effectively leverage abundant unlabeled data. Our approach offers several advantages. It considerably simplifies the learning process, as it seeks a lower bound of the reward function, rather than that of the Q-function or state transition function. It is highly flexible, and can be integrated with a range of model-free and model-based RL algorithms. It enjoys the guaranteed improvement when utilizing vast unlabeled data, but requires much less restrictive conditions. We compare our method with a number of alternative solutions, both analytically and numerically, and demonstrate its clear competitiveness. We further illustrate with an application to adaptive deep brain stimulation for Parkinson's disease.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline RL) ä¸­å› æ ‡æ³¨å¥–åŠ±æ•°æ®ç¨€ç¼ºå¯¼è‡´çš„åˆ†å¸ƒåç§» (distributional shift) å’Œæ¬¡ä¼˜ç­–ç•¥å­¦ä¹ é—®é¢˜ï¼Œæå‡ºäº†åŠæ‚²è§‚å¼ºåŒ–å­¦ä¹  (Semi-pessimistic RL) æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æœ‰æ•ˆåˆ©ç”¨è§„æ¨¡è¿œè¶…æ ‡æ³¨æ•°æ®çš„æµ·é‡æœªæ ‡æ³¨æ•°æ®ï¼Œé€šè¿‡å¯»æ‰¾å¥–åŠ±å‡½æ•°çš„ä¸‹ç•Œè€Œé Q-function æˆ–çŠ¶æ€è½¬ç§»å‡½æ•°çš„ä¸‹ç•Œï¼Œæ˜¾è‘—ç®€åŒ–äº†å­¦ä¹ è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶å…·æœ‰é«˜åº¦çµæ´»æ€§ï¼Œå¯ä¸å¤šç§ model-free å’Œ model-based å¼ºåŒ–å­¦ä¹ ç®—æ³•æ— ç¼é›†æˆã€‚ç†è®ºåˆ†æå’Œæ•°å€¼å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ©ç”¨å¤§è§„æ¨¡æœªæ ‡æ³¨æ•°æ®æ—¶å…·å¤‡æ€§èƒ½æ”¹è¿›ä¿è¯ï¼Œä¸”æ‰€éœ€çš„é™åˆ¶æ¡ä»¶è¿œå°‘äºä¼ ç»Ÿæ–¹æ³•ã€‚æœ€åï¼Œè¯¥ç ”ç©¶é€šè¿‡åœ¨å¸•é‡‘æ£®ç—…è‡ªé€‚åº”æ·±åº¦è„‘åˆºæ¿€ (adaptive deep brain stimulation) ä¸­çš„åº”ç”¨ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åœºæ™¯ä¸‹çš„å¼ºå¤§ç«äº‰åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.19002v1",
      "published_date": "2025-05-25 06:47:36 UTC",
      "updated_date": "2025-05-25 06:47:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:09:14.748022+00:00"
    },
    {
      "arxiv_id": "2505.18995v1",
      "title": "FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)",
      "title_zh": "FiLLMï¼šåŸºäºä¸œå—äºšå¤§è¯­è¨€æ¨¡å‹ï¼ˆSEALLMï¼‰çš„è²å¾‹å®¾è¯­ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Carlos Jude G. Maminta",
        "Isaiah Job Enriquez",
        "Deandre Nigel Nunez",
        "Michael B. Dela Fuente"
      ],
      "abstract": "This study presents FiLLM, a Filipino-optimized large language model, designed to enhance natural language processing (NLP) capabilities in the Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining task-specific performance. The model was trained and evaluated on diverse Filipino datasets to address key NLP tasks, including Named Entity Recognition (NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text Summarization. Performance comparisons with the CalamanCy model were conducted using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap metrics. Results indicate that Calamancy outperforms FILLM in several aspects, demonstrating its effectiveness in processing Filipino text with improved linguistic comprehension and adaptability. This research contributes to the advancement of Filipino NLP applications by providing an optimized, efficient, and scalable language model tailored for local linguistic needs.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº†FiLLMï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºè²å¾‹å®¾è¯­è‡ªç„¶è¯­è¨€å¤„ç†(NLP)èƒ½åŠ›çš„ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹åŸºäºSeaLLM-7B 2.5æ„å»ºï¼Œå¹¶åˆ©ç”¨ä½ç§©è‡ªé€‚åº”(LoRA)å¾®è°ƒæŠ€æœ¯åœ¨ä¼˜åŒ–å†…å­˜æ•ˆç‡çš„åŒæ—¶ä¿æŒç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ã€‚FiLLMåœ¨åŒ…æ‹¬å‘½åå®ä½“è¯†åˆ«(NER)ã€è¯æ€§æ ‡æ³¨(POS)ã€ä¾å­˜å¥æ³•åˆ†æ(Dependency Parsing)å’Œæ–‡æœ¬æ‘˜è¦(Text Summarization)åœ¨å†…çš„å¤šç§è²å¾‹å®¾è¯­æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒä¸è¯„ä¼°ã€‚ç ”ç©¶é€šè¿‡F1åˆ†æ•°ã€å‡†ç¡®ç‡ã€å¬å›ç‡ã€å‹ç¼©ç‡å’Œå…³é”®è¯é‡å ç­‰æŒ‡æ ‡å°†å…¶ä¸Calamancyæ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCalamancyåœ¨å¤šä¸ªç»´åº¦ä¸Šä¼˜äºFiLLMï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†è²å¾‹å®¾è¯­æ–‡æœ¬æ—¶çš„è¯­è¨€ç†è§£åŠ›å’Œé€‚åº”æ€§ã€‚è¿™é¡¹ç ”ç©¶ä¸ºå¼€å‘é’ˆå¯¹å½“åœ°è¯­è¨€éœ€æ±‚çš„ä¼˜åŒ–ä¸”å¯æ‰©å±•çš„è¯­è¨€æ¨¡å‹åšå‡ºäº†è´¡çŒ®ï¼Œæ¨åŠ¨äº†è²å¾‹å®¾è¯­NLPåº”ç”¨çš„å‘å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18995v1",
      "published_date": "2025-05-25 06:36:26 UTC",
      "updated_date": "2025-05-25 06:36:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:09:16.143349+00:00"
    },
    {
      "arxiv_id": "2506.13992v2",
      "title": "AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science",
      "title_zh": "AssistedDSï¼šè¯„ä¼°å¤–éƒ¨é¢†åŸŸçŸ¥è¯†å¦‚ä½•è¾…åŠ©å¤§è¯­è¨€æ¨¡å‹å¼€å±•è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "An Luo",
        "Xun Xian",
        "Jin Du",
        "Fangqiao Tian",
        "Ganghua Wang",
        "Ming Zhong",
        "Shengchun Zhao",
        "Xuan Bi",
        "Zirui Liu",
        "Jiawei Zhou",
        "Jayanth Srinivasa",
        "Ashish Kundu",
        "Charles Fleming",
        "Mingyi Hong",
        "Jie Ding"
      ],
      "abstract": "Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems. Our data and code are publicly available here: https://github.com/jeremyxianx/Assisted-DS",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AssistedDSï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¡¨æ ¼é¢„æµ‹ä»»åŠ¡ä¸­å¤„ç†é¢†åŸŸçŸ¥è¯†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŒ…å«å…·æœ‰æ˜ç¡®ç”Ÿæˆæœºåˆ¶çš„åˆæˆæ•°æ®é›†ä»¥åŠçœŸå®çš„Kaggleç«èµ›æ¡ˆä¾‹ï¼Œå¹¶é…å¥—æä¾›äº†æ¶µç›–æ•°æ®æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹é€‰æ‹©çš„ä¸“ä¸šå»ºè®®æ–‡æ¡£ä¸å¯¹æŠ—æ€§æ–‡æ¡£ã€‚å®éªŒå‘ç°ï¼ŒLLMså¾€å¾€ä¼šç›²ç›®é‡‡çº³æ‰€æä¾›çš„ä¿¡æ¯ï¼Œåœ¨å¼•å…¥å¯¹æŠ—æ€§å†…å®¹æ—¶å…¶é¢„æµ‹æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼Œä¸”æ­£é¢çš„é¢†åŸŸæŒ‡å¯¼é€šå¸¸ä¸è¶³ä»¥æŠµæ¶ˆå¯¹æŠ—ä¿¡æ¯çš„è´Ÿé¢å½±å“ã€‚æ­¤å¤–ï¼ŒLLMsåœ¨å¤„ç†æ•°æ®é›†æ—¶ç»å¸¸åœ¨æ—¶é—´åºåˆ—æ•°æ®å¤„ç†ã€è·¨æŠ˜ç‰¹å¾å·¥ç¨‹çš„ä¸€è‡´æ€§ä»¥åŠç±»åˆ«å˜é‡è§£é‡Šç­‰æ–¹é¢å‡ºç°é”™è¯¯ã€‚è¿™äº›ç»“æœè¡¨æ˜å½“å‰æ¨¡å‹åœ¨æ‰¹åˆ¤æ€§è¯„ä¼°å’Œåˆ©ç”¨ä¸“å®¶çŸ¥è¯†æ–¹é¢ä»å­˜åœ¨å·¨å¤§å·®è·ï¼Œä¸ºå¼€å‘æ›´å…·é²æ£’æ€§å’ŒçŸ¥è¯†æ„ŸçŸ¥èƒ½åŠ›çš„è‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦(Automated Data Science)ç³»ç»ŸæŒ‡æ˜äº†ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.13992v2",
      "published_date": "2025-05-25 05:50:21 UTC",
      "updated_date": "2025-10-23 01:33:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:09:31.736239+00:00"
    },
    {
      "arxiv_id": "2505.20346v2",
      "title": "PDFBench: A Benchmark for De novo Protein Design from Function",
      "title_zh": "PDFBenchï¼šåŸºäºåŠŸèƒ½çš„è›‹ç™½è´¨ä»å¤´è®¾è®¡åŸºå‡†",
      "authors": [
        "Jiahao Kuang",
        "Nuowei Liu",
        "Jie Wang",
        "Changzhi Sun",
        "Tao Ji",
        "Yuanbin Wu"
      ],
      "abstract": "Function-guided protein design is a crucial task with significant applications in drug discovery and enzyme engineering. However, the field lacks a unified and comprehensive evaluation framework. Current models are assessed using inconsistent and limited subsets of metrics, which prevents fair comparison and a clear understanding of the relationships between different evaluation criteria. To address this gap, we introduce PDFBench, the first comprehensive benchmark for function-guided denovo protein design. Our benchmark systematically evaluates eight state-of-the-art models on 16 metrics across two key settings: description-guided design, for which we repurpose the Mol-Instructions dataset, originally lacking quantitative benchmarking, and keyword-guided design, for which we introduce a new test set, SwissTest, created with a strict datetime cutoff to ensure data integrity. By benchmarking across a wide array of metrics and analyzing their correlations, PDFBench enables more reliable model comparisons and provides key insights to guide future research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠŸèƒ½å¼•å¯¼çš„è›‹ç™½è´¨è®¾è®¡(function-guided protein design)é¢†åŸŸç¼ºä¹ç»Ÿä¸€ä¸”å…¨é¢è¯„ä¼°æ¡†æ¶çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªç»¼åˆæ€§åŸºå‡†æµ‹è¯•å¹³å°PDFBenchã€‚PDFBench æ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨è¯„ä¼°æŒ‡æ ‡ä¸Šçš„ä¸ä¸€è‡´æ€§å’Œå±€é™æ€§ï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹æ€§èƒ½çš„å…¬å¹³æ¯”è¾ƒã€‚è¯¥åŸºå‡†ç³»ç»Ÿåœ°è¯„ä¼°äº†8ä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹(state-of-the-art models)ï¼Œåœ¨16é¡¹æŒ‡æ ‡ä¸Šæ¶µç›–äº†æè¿°å¼•å¯¼è®¾è®¡(description-guided design)å’Œå…³é”®è¯å¼•å¯¼è®¾è®¡(keyword-guided design)ä¸¤ä¸ªæ ¸å¿ƒåœºæ™¯ã€‚å…¶ä¸­ï¼Œç ”ç©¶å›¢é˜Ÿé‡æ–°åˆ©ç”¨äº†Mol-Instructionsæ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†å…¨æ–°çš„SwissTestæµ‹è¯•é›†ï¼Œé€šè¿‡ä¸¥æ ¼çš„æ—¥æœŸæˆªæ–­æœºåˆ¶ç¡®ä¿äº†æ•°æ®å®Œæ•´æ€§(data integrity)ã€‚é€šè¿‡å¹¿æ³›çš„æŒ‡æ ‡åˆ†æä¸ç›¸å…³æ€§ç ”ç©¶ï¼ŒPDFBench ä¸ºè›‹ç™½è´¨ä»å¤´è®¾è®¡(de novo protein design)é¢†åŸŸæä¾›äº†æ›´å¯é çš„è¯„ä¼°å·¥å…·å’ŒæŒ‡å¯¼æœªæ¥ç ”ç©¶çš„å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.20346v2",
      "published_date": "2025-05-25 05:40:15 UTC",
      "updated_date": "2025-09-28 03:52:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:10:17.241603+00:00"
    },
    {
      "arxiv_id": "2505.18976v3",
      "title": "GraSS: Scalable Data Attribution with Gradient Sparsification and Sparse Projection",
      "title_zh": "GraSSï¼šåŸºäºæ¢¯åº¦ç¨€ç–åŒ–å’Œç¨€ç–æŠ•å½±çš„å¯æ‰©å±•æ•°æ®å½’å› ",
      "authors": [
        "Pingbang Hu",
        "Joseph Melkonian",
        "Weijing Tang",
        "Han Zhao",
        "Jiaqi W. Ma"
      ],
      "abstract": "Gradient-based data attribution methods, such as influence functions, are critical for understanding the impact of individual training samples without requiring repeated model retraining. However, their scalability is often limited by the high computational and memory costs associated with per-sample gradient computation. In this work, we propose GraSS, a novel gradient compression algorithm and its variants FactGraSS for linear layers specifically, that explicitly leverage the inherent sparsity of per-sample gradients to achieve sub-linear space and time complexity. Extensive experiments demonstrate the effectiveness of our approach, achieving substantial speedups while preserving data influence fidelity. In particular, FactGraSS achieves up to 165% faster throughput on billion-scale models compared to the previous state-of-the-art baselines. Our code is publicly available at https://github.com/TRAIS-Lab/GraSS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GraSSï¼Œä¸€ç§æ–°é¢–çš„æ¢¯åº¦å‹ç¼©ç®—æ³•åŠå…¶é’ˆå¯¹çº¿æ€§å±‚çš„å˜ä½“FactGraSSï¼Œæ—¨åœ¨è§£å†³åŸºäºæ¢¯åº¦çš„Data Attributionæ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ¨¡å‹æ—¶é¢ä¸´çš„é«˜è®¡ç®—å’Œå†…å­˜æˆæœ¬é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜¾å¼åˆ©ç”¨æ ·æœ¬æ¢¯åº¦ï¼ˆper-sample gradientsï¼‰çš„å›ºæœ‰ç¨€ç–æ€§ï¼Œå®ç°äº†æ¬¡çº¿æ€§çš„ç©ºé—´å’Œæ—¶é—´å¤æ‚åº¦ã€‚é€šè¿‡ç»“åˆGradient Sparsificationå’ŒSparse ProjectionæŠ€æœ¯ï¼ŒGraSSåœ¨æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œæœ‰æ•ˆä¿ç•™äº†æ•°æ®å½±å“çš„å¿ å®åº¦ï¼ˆfidelityï¼‰ã€‚å®éªŒè¯æ˜ï¼ŒFactGraSSåœ¨åäº¿å‚æ•°çº§æ¨¡å‹ä¸Šçš„ååé‡æ¯”å½“å‰æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•å¿«165%ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨å¤§è§„æ¨¡æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­é«˜æ•ˆé‡åŒ–è®­ç»ƒæ ·æœ¬å¯¹æ¨¡å‹çš„å½±å“æä¾›äº†å…·æœ‰æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å·²å…¬å¼€å‘å¸ƒä»£ç ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.18976v3",
      "published_date": "2025-05-25 04:58:57 UTC",
      "updated_date": "2025-10-28 15:46:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:09:54.496145+00:00"
    },
    {
      "arxiv_id": "2505.18975v4",
      "title": "FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization",
      "title_zh": "FastMambaï¼šåŸºäºç²¾ç¡®é‡åŒ–çš„é«˜é€Ÿé«˜æ•ˆ FPGA Mamba åŠ é€Ÿå™¨",
      "authors": [
        "Aotao Wang",
        "Haikuo Shao",
        "Shaobo Ma",
        "Zhongfeng Wang"
      ],
      "abstract": "State Space Models (SSMs), like recent Mamba2, have achieved remarkable performance and received extensive attention. However, deploying Mamba2 on resource-constrained edge devices encounters many problems: severe outliers within the linear layer challenging the quantization, diverse and irregular element-wise tensor operations, and hardware-unfriendly nonlinear functions in the SSM block. To address these issues, this paper presents FastMamba, a dedicated accelerator on FPGA with hardware-algorithm co-design to promote the deployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit quantization for linear layers through Hadamard transformation to eliminate outliers. Moreover, a hardware-friendly and fine-grained power-of-two quantization framework is presented for the SSM block and convolution layer, and a first-order linear approximation is developed to optimize the nonlinear functions. Based on the accurate algorithm quantization, we propose an accelerator that integrates parallel vector processing units, pipelined execution dataflow, and an efficient SSM Nonlinear Approximation Unit, which enhances computational efficiency and reduces hardware complexity. Finally, we evaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on Mamba2-130M, FastMamba achieves 68.80\\times and 8.90\\times speedup over Intel Xeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode experiment with Mamba2-2.7B, FastMamba attains 6\\times higher energy efficiency than RTX 3090 GPU.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Mamba2 æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æ—¶é¢ä¸´çš„çº¿æ€§å±‚ç¦»ç¾¤å€¼(outliers)ã€å¤æ‚å¼ é‡æ“ä½œä»¥åŠ SSM æ¨¡å—ä¸­éçº¿æ€§å‡½æ•°ç¡¬ä»¶ä¸å‹å¥½ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† FastMamba è¿™ä¸€ä¸“ç”¨çš„ FPGA åŠ é€Ÿå™¨ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ç¡¬ä»¶ä¸ç®—æ³•ååŒè®¾è®¡çš„ç­–ç•¥ï¼Œé€šè¿‡ Hadamard transformation æ¶ˆé™¤ç¦»ç¾¤å€¼ï¼ŒæˆåŠŸå®ç°äº†çº¿æ€§å±‚çš„ 8-bit é‡åŒ–ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥ä¸º SSM æ¨¡å—å’Œå·ç§¯å±‚å¼€å‘äº†ç¡¬ä»¶å‹å¥½ä¸”ç»†ç²’åº¦çš„ power-of-two é‡åŒ–æ¡†æ¶ï¼Œå¹¶åˆ©ç”¨ä¸€é˜¶çº¿æ€§è¿‘ä¼¼(first-order linear approximation)ä¼˜åŒ–äº†éçº¿æ€§å‡½æ•°ã€‚FastMamba é›†æˆäº†å¹¶è¡Œå‘é‡å¤„ç†å•å…ƒ(parallel vector processing units)ã€æµæ°´çº¿æ‰§è¡Œæ•°æ®æµä»¥åŠé«˜æ•ˆçš„ SSM Nonlinear Approximation Unitï¼Œä»è€Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡å¹¶é™ä½äº†ç¡¬ä»¶å¤æ‚åº¦ã€‚åœ¨ Xilinx VC709 FPGA ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒFastMamba åœ¨ Mamba2-130M çš„é¢„å¡«å……ä»»åŠ¡ä¸­ï¼Œè¿è¡Œé€Ÿåº¦åˆ†åˆ«æ¯” Intel Xeon 4210R CPU å’Œ NVIDIA RTX 3090 GPU å¿« 68.80 å€å’Œ 8.90 å€ã€‚åœ¨ Mamba2-2.7B çš„è§£ç å®éªŒä¸­ï¼Œè¯¥åŠ é€Ÿå™¨çš„èƒ½æ•ˆæ¯” RTX 3090 GPU é«˜å‡º 6 å€ï¼Œä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹é«˜æ•ˆéƒ¨ç½²å¤§è§„æ¨¡ State Space Models æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18975v4",
      "published_date": "2025-05-25 04:54:53 UTC",
      "updated_date": "2025-07-28 13:28:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:10:01.716373+00:00"
    },
    {
      "arxiv_id": "2505.18972v1",
      "title": "Revival with Voice: Multi-modal Controllable Text-to-Speech Synthesis",
      "title_zh": "Revival with Voiceï¼šå¤šæ¨¡æ€å¯æ§æ–‡æœ¬è½¬è¯­éŸ³åˆæˆ",
      "authors": [
        "Minsu Kim",
        "Pingchuan Ma",
        "Honglie Chen",
        "Stavros Petridis",
        "Maja Pantic"
      ],
      "abstract": "This paper explores multi-modal controllable Text-to-Speech Synthesis (TTS) where the voice can be generated from face image, and the characteristics of output speech (e.g., pace, noise level, distance, tone, place) can be controllable with natural text description. Specifically, we aim to mitigate the following three challenges in face-driven TTS systems. 1) To overcome the limited audio quality of audio-visual speech corpora, we propose a training method that additionally utilizes high-quality audio-only speech corpora. 2) To generate voices not only from real human faces but also from artistic portraits, we propose augmenting the input face image with stylization. 3) To consider one-to-many possibilities in face-to-voice mapping and ensure consistent voice generation at the same time, we propose to first employ sampling-based decoding and then use prompting with generated speech samples. Experimental results validate the proposed model's effectiveness in face-driven voice synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¯æ§æ–‡æœ¬è½¬è¯­éŸ³(Text-to-Speech, TTS)åˆæˆç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡äººè„¸å›¾åƒç”Ÿæˆå¯¹åº”è¯­éŸ³ï¼Œå¹¶åˆ©ç”¨è‡ªç„¶æ–‡æœ¬æè¿°ç²¾ç¡®æ§åˆ¶è¯­é€Ÿã€å™ªå£°æ°´å¹³ã€éŸ³è°ƒåŠåœºæ™¯ç­‰ç‰¹å¾ã€‚ä¸ºäº†è§£å†³ç°æœ‰å½±éŸ³è¯­æ–™åº“éŸ³é¢‘è´¨é‡ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†é«˜è´¨é‡çº¯éŸ³é¢‘è¯­æ–™åº“è¿›è¡Œè¾…åŠ©è®­ç»ƒã€‚é’ˆå¯¹ä»è‰ºæœ¯è‚–åƒç”Ÿæˆè¯­éŸ³çš„éœ€æ±‚ï¼Œç ”ç©¶é‡‡ç”¨äº†é£æ ¼åŒ–å¢å¼º(stylization)æŠ€æœ¯ä»¥æ‰©å±•æ¨¡å‹çš„è¾“å…¥é€‚ç”¨æ€§ã€‚ä¸ºäº†åœ¨åº”å¯¹äººè„¸åˆ°è¯­éŸ³æ˜ å°„çš„ä¸€å¯¹å¤šæŒ‘æˆ˜æ—¶ä¿æŒè¯­éŸ³ç”Ÿæˆçš„è¿è´¯æ€§ï¼Œç³»ç»Ÿå…ˆé‡‡ç”¨åŸºäºé‡‡æ ·çš„è§£ç (sampling-based decoding)ï¼Œå†åˆ©ç”¨ç”Ÿæˆçš„è¯­éŸ³æ ·æœ¬ä½œä¸ºæç¤º(prompting)è¿›è¡Œåç»­åˆæˆã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¨¡å‹åœ¨äººè„¸é©±åŠ¨è¯­éŸ³åˆæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®ç°æ›´å…·è¡¨ç°åŠ›å’Œä¸ªæ€§åŒ–çš„å¤šæ¨¡æ€äº¤äº’æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.18972v1",
      "published_date": "2025-05-25 04:43:17 UTC",
      "updated_date": "2025-05-25 04:43:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:10:25.260702+00:00"
    },
    {
      "arxiv_id": "2506.04237v1",
      "title": "A Comprehensive Survey on the Risks and Limitations of Concept-based Models",
      "title_zh": "åŸºäºæ¦‚å¿µçš„æ¨¡å‹çš„é£é™©ä¸å±€é™æ€§å…¨é¢ç»¼è¿°",
      "authors": [
        "Sanchit Sinha",
        "Aidong Zhang"
      ],
      "abstract": "Concept-based Models are a class of inherently explainable networks that improve upon standard Deep Neural Networks by providing a rationale behind their predictions using human-understandable `concepts'. With these models being highly successful in critical applications like medical diagnosis and financial risk prediction, there is a natural push toward their wider adoption in sensitive domains to instill greater trust among diverse stakeholders. However, recent research has uncovered significant limitations in the structure of such networks, their training procedure, underlying assumptions, and their susceptibility to adversarial vulnerabilities. In particular, issues such as concept leakage, entangled representations, and limited robustness to perturbations pose challenges to their reliability and generalization. Additionally, the effectiveness of human interventions in these models remains an open question, raising concerns about their real-world applicability. In this paper, we provide a comprehensive survey on the risks and limitations associated with Concept-based Models. In particular, we focus on aggregating commonly encountered challenges and the architecture choices mitigating these challenges for Supervised and Unsupervised paradigms. We also examine recent advances in improving their reliability and discuss open problems and promising avenues of future research in this domain.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹åŸºäºæ¦‚å¿µçš„æ¨¡å‹ (Concept-based Models) çš„é£é™©ä¸å±€é™æ€§è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ã€‚è™½ç„¶è¿™ç±»æ¨¡å‹æ—¨åœ¨é€šè¿‡äººç±»å¯ç†è§£çš„æ¦‚å¿µæä¾›å¯è§£é‡Šæ€§ï¼Œå¹¶åœ¨åŒ»ç–—å’Œé‡‘èç­‰æ•æ„Ÿé¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œä½†å…¶åœ¨ç»“æ„ã€è®­ç»ƒç¨‹åºåŠåº•å±‚å‡è®¾æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚è®ºæ–‡é‡ç‚¹åˆ†æäº†æ¦‚å¿µæ³„æ¼ (concept leakage)ã€çº ç¼ è¡¨å¾ (entangled representations) ä»¥åŠå¯¹æ‰°åŠ¨é²æ£’æ€§æœ‰é™ç­‰æŒ‘æˆ˜å¯¹æ¨¡å‹å¯é æ€§çš„å½±å“ã€‚é’ˆå¯¹ç›‘ç£ (Supervised) å’Œæ— ç›‘ç£ (Unsupervised) èŒƒå¼ï¼Œæ–‡ç« æ€»ç»“äº†åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜çš„å¸¸è§æ¶æ„é€‰æ‹©ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°æ¢è®¨äº†äººç±»å¹²é¢„ (human interventions) çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å®¡è§†äº†æå‡æ¨¡å‹å¯é æ€§çš„æœ€æ–°è¿›å±•ã€‚æœ€åï¼Œç ”ç©¶æŒ‡å‡ºäº†è¯¥é¢†åŸŸç›®å‰å­˜åœ¨çš„å¼€æ”¾æ€§é—®é¢˜åŠæœªæ¥å…·æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.04237v1",
      "published_date": "2025-05-25 03:53:26 UTC",
      "updated_date": "2025-05-25 03:53:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:10:49.457053+00:00"
    },
    {
      "arxiv_id": "2505.18966v2",
      "title": "Protein Design with Dynamic Protein Vocabulary",
      "title_zh": "åŸºäºåŠ¨æ€è›‹ç™½è´¨è¯æ±‡çš„è›‹ç™½è´¨è®¾è®¡",
      "authors": [
        "Nuowei Liu",
        "Jiahao Kuang",
        "Yanting Liu",
        "Tao Ji",
        "Changzhi Sun",
        "Man Lan",
        "Yuanbin Wu"
      ],
      "abstract": "Protein design is a fundamental challenge in biotechnology, aiming to design novel sequences with specific functions within the vast space of possible proteins. Recent advances in deep generative models have enabled function-based protein design from textual descriptions, yet struggle with structural plausibility. Inspired by classical protein design methods that leverage natural protein structures, we explore whether incorporating fragments from natural proteins can enhance foldability in generative models. Our empirical results show that even random incorporation of fragments improves foldability. Building on this insight, we introduce ProDVa, a novel protein design approach that integrates a text encoder for functional descriptions, a protein language model for designing proteins, and a fragment encoder to dynamically retrieve protein fragments based on textual functional descriptions. Experimental results demonstrate that our approach effectively designs protein sequences that are both functionally aligned and structurally plausible. Compared to state-of-the-art models, ProDVa achieves comparable function alignment using less than 0.04% of the training data, while designing significantly more well-folded proteins, with the proportion of proteins having pLDDT above 70 increasing by 7.38% and those with PAE below 10 increasing by 9.6%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿç‰©æŠ€æœ¯é¢†åŸŸä¸­åŸºäºæ–‡æœ¬æè¿°è®¾è®¡å…·æœ‰ç‰¹å®šåŠŸèƒ½ä¸”ç»“æ„åˆç†çš„è›‹ç™½è´¨åºåˆ—è¿™ä¸€æŒ‘æˆ˜ï¼Œæ¢è®¨äº†å¼•å…¥å¤©ç„¶è›‹ç™½è´¨ç‰‡æ®µå¢å¼ºç”Ÿæˆæ¨¡å‹æŠ˜å èƒ½åŠ›çš„å¯èƒ½æ€§ã€‚ä¸ºæ­¤ä½œè€…æå‡ºäº†ProDVaï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†æ–‡æœ¬ç¼–ç å™¨(text encoder)ã€è›‹ç™½è´¨è¯­è¨€æ¨¡å‹(protein language model)å’Œç‰‡æ®µç¼–ç å™¨(fragment encoder)çš„æ–°å‹è›‹ç™½è´¨è®¾è®¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬ä¸­çš„åŠŸèƒ½æè¿°åŠ¨æ€æ£€ç´¢(dynamically retrieve)è›‹ç™½è´¨ç‰‡æ®µï¼Œä»è€Œåœ¨ç¡®ä¿åŠŸèƒ½å¯¹é½çš„åŒæ—¶æé«˜ç»“æ„çš„ç‰©ç†åˆç†æ€§(structural plausibility)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒProDVaä»…ä½¿ç”¨ä¸åˆ°0.04%çš„è®­ç»ƒæ•°æ®å³å¯è¾¾åˆ°ä¸ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„åŠŸèƒ½å¯¹é½æ°´å¹³ã€‚åœ¨è›‹ç™½è´¨æŠ˜å è´¨é‡æ–¹é¢ï¼Œè¯¥æ¨¡å‹è®¾è®¡çš„è›‹ç™½è´¨ä¸­pLDDTè¶…è¿‡70çš„æ¯”ä¾‹æå‡äº†7.38%ï¼Œè€ŒPAEä½äº10çš„æ¯”ä¾‹æå‡äº†9.6%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†åŠ¨æ€è›‹ç™½è´¨è¯æ±‡è¡¨(Dynamic Protein Vocabulary)åœ¨æå‡å¯æŠ˜å è›‹ç™½è´¨è®¾è®¡æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2025 (Spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2505.18966v2",
      "published_date": "2025-05-25 03:50:50 UTC",
      "updated_date": "2025-10-14 14:04:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:10:48.444176+00:00"
    },
    {
      "arxiv_id": "2505.18961v2",
      "title": "Weaver: Interweaving SQL and LLM for Table Reasoning",
      "title_zh": "Weaverï¼šé¢å‘è¡¨æ ¼æ¨ç†çš„ SQL ä¸ LLM äº¤ç»‡",
      "authors": [
        "Rohit Khoja",
        "Devanshu Gupta",
        "Yanjie Fu",
        "Dan Roth",
        "Vivek Gupta"
      ],
      "abstract": "Querying tables with unstructured data is challenging due to the presence of text (or image), either embedded in the table or in external paragraphs, which traditional SQL struggles to process, especially for tasks requiring semantic reasoning. While Large Language Models (LLMs) excel at understanding context, they face limitations with long input sequences. Existing approaches that combine SQL and LLMs typically rely on rigid, predefined work-flows, limiting their adaptability to complex queries. To address these issues, we introduce Weaver , a modular pipeline that dynamically integrates SQL and LLMs for table-based question answering (TableQA). Weaver generates a flexible, step-by-step plan that combines SQL for structured data retrieval with LLMs for semantic processing. By decomposing complex queries into manageable subtasks, Weaver improves accuracy and generalization. Our experiments show that Weaver consistently outperforms state-of-the-art methods across four TableQA datasets, reducing both API calls and error rates. The code, along with other associated scripts, are available at https://coral-lab-asu.github.io/weaver.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Weaverï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³è¡¨æ¨ç† (Table Reasoning) ä¸­ç»“æ„åŒ–æŸ¥è¯¢ä¸éç»“æ„åŒ–è¯­ä¹‰ç†è§£ç»“åˆéš¾é¢˜çš„æ¨¡å—åŒ–æµæ°´çº¿ã€‚é’ˆå¯¹ä¼ ç»Ÿ SQL åœ¨è¯­ä¹‰æ¨ç†æ–¹é¢çš„å±€é™æ€§ä»¥åŠå¤§è¯­è¨€æ¨¡å‹ (LLMs) å¤„ç†é•¿åºåˆ—æ—¶çš„æŒ‘æˆ˜ï¼ŒWeaver é€šè¿‡åŠ¨æ€æ•´åˆ SQL å’Œ LLMs æ¥æ‰§è¡ŒåŸºäºè¡¨æ ¼çš„é—®ç­” (TableQA) ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆçµæ´»çš„é€æ­¥è®¡åˆ’ï¼Œåˆ©ç”¨ SQL è¿›è¡Œé«˜æ•ˆçš„ç»“æ„åŒ–æ•°æ®æ£€ç´¢ï¼Œå¹¶ååŒ LLMs è¿›è¡Œæ·±å±‚çš„è¯­ä¹‰å¤„ç†ã€‚é€šè¿‡å°†å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºä¸€ç³»åˆ—å¯ç®¡ç†çš„å­ä»»åŠ¡ï¼ŒWeaver æ˜¾è‘—å¢å¼ºäº†å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒWeaver åœ¨å››ä¸ª TableQA æ•°æ®é›†ä¸Šå‡ä¸€è‡´ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œåœ¨æå‡æ€§èƒ½çš„åŒæ—¶è¿˜æ˜¾è‘—é™ä½äº† API è°ƒç”¨æˆæœ¬å’Œç³»ç»Ÿé”™è¯¯ç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18961v2",
      "published_date": "2025-05-25 03:27:37 UTC",
      "updated_date": "2025-09-23 18:02:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:11:26.433635+00:00"
    },
    {
      "arxiv_id": "2505.18956v2",
      "title": "How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation",
      "title_zh": "å›¾åƒå¦‚ä½•ä¸ LiDAR å¯¹é½å¹¶äº’è¡¥ï¼Ÿè¿ˆå‘åè°ƒçš„å¤šæ¨¡æ€ä¸‰ç»´å…¨æ™¯åˆ†å‰²",
      "authors": [
        "Yining Pan",
        "Qiongjie Cui",
        "Xulei Yang",
        "Na Zhao"
      ],
      "abstract": "LiDAR-based 3D panoptic segmentation often struggles with the inherent sparsity of data from LiDAR sensors, which makes it challenging to accurately recognize distant or small objects. Recently, a few studies have sought to overcome this challenge by integrating LiDAR inputs with camera images, leveraging the rich and dense texture information provided by the latter. While these approaches have shown promising results, they still face challenges, such as misalignment during data augmentation and the reliance on post-processing steps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel multi-modal 3D panoptic segmentation framework. In IAL, we first introduce a modality-synchronized data augmentation strategy, PieAug, to ensure alignment between LiDAR and image inputs from the start. Next, we adopt a transformer decoder to directly predict panoptic segmentation results. To effectively fuse LiDAR and image features into tokens for the decoder, we design a Geometric-guided Token Fusion (GTF) module. Additionally, we leverage the complementary strengths of each modality as priors for query initialization through a Prior-based Query Generation (PQG) module, enhancing the decoder's ability to generate accurate instance masks. Our IAL framework achieves state-of-the-art performance compared to previous multi-modal 3D panoptic segmentation methods on two widely used benchmarks. Code and models are publicly available at <https://github.com/IMPL-Lab/IAL.git>.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹LiDARåœ¨3D panoptic segmentationä¸­å› ä¼ æ„Ÿå™¨å›ºæœ‰ç¨€ç–æ€§å¯¼è‡´éš¾ä»¥è¯†åˆ«è¿œå¤„æˆ–å¾®å°ç›®æ ‡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºImage-Assists-LiDAR (IAL) çš„å¤šæ¨¡æ€æ¡†æ¶ã€‚ä¸ºäº†è§£å†³æ•°æ®å¢å¼ºè¿‡ç¨‹ä¸­çš„æ¨¡æ€é”™ä½éš¾é¢˜ï¼ŒIALå¼•å…¥äº†æ¨¡æ€åŒæ­¥å¢å¼ºç­–ç•¥PieAugï¼Œä»æºå¤´ç¡®ä¿LiDARä¸å›¾åƒè¾“å…¥çš„ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†å‡ ä½•å¼•å¯¼ä»¤ç‰Œèåˆ(Geometric-guided Token Fusion, GTF)æ¨¡å—ï¼Œå°†ä¸åŒæ¨¡æ€çš„ç‰¹å¾æœ‰æ•ˆæ•´åˆè¿›transformerè§£ç å™¨ä¸­ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŸºäºå…ˆéªŒçš„æŸ¥è¯¢ç”Ÿæˆ(Prior-based Query Generation, PQG)æ¨¡å—ï¼ŒIALåˆ©ç”¨å„æ¨¡æ€çš„äº’è¡¥ä¼˜åŠ¿è¿›è¡ŒæŸ¥è¯¢åˆå§‹åŒ–ï¼Œä»è€Œæå‡äº†å®ä¾‹æ©ç ç”Ÿæˆçš„ç²¾ç¡®åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIALåœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šå‡å–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€3Då…¨æ™¯åˆ†å‰²æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the 2025 International Conference on Machine Learning (ICML)",
      "pdf_url": "https://arxiv.org/pdf/2505.18956v2",
      "published_date": "2025-05-25 03:01:28 UTC",
      "updated_date": "2025-06-10 05:46:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:11:13.503398+00:00"
    },
    {
      "arxiv_id": "2505.18955v2",
      "title": "Co-PatcheR: Collaborative Software Patching with Component(s)-specific Small Reasoning Models",
      "title_zh": "Co-PatcheRï¼šåŸºäºç»„ä»¶ç‰¹å®šå°å‹æ¨ç†æ¨¡å‹çš„åä½œå¼è½¯ä»¶è¡¥ä¸ä¿®å¤",
      "authors": [
        "Yuheng Tang",
        "Hongwei Li",
        "Kaijie Zhu",
        "Michael Yang",
        "Yangruibo Ding",
        "Wenbo Guo"
      ],
      "abstract": "Motivated by the success of general-purpose large language models (LLMs) in software patching, recent works started to train specialized patching models. Most works trained one model to handle the end-to-end patching pipeline (including issue localization, patch generation, and patch validation). However, it is hard for a small model to handle all tasks, as different sub-tasks have different workflows and require different expertise. As such, by using a 70 billion model, SOTA methods can only reach up to 41% resolved rate on SWE-bench-Verified. Motivated by the collaborative nature, we propose Co-PatcheR, the first collaborative patching system with small and specialized reasoning models for individual components. Our key technique novelties are the specific task designs and training recipes. First, we train a model for localization and patch generation. Our localization pinpoints the suspicious lines through a two-step procedure, and our generation combines patch generation and critique. We then propose a hybrid patch validation that includes two models for crafting issue-reproducing test cases with and without assertions and judging patch correctness, followed by a majority vote-based patch selection. Through extensive evaluation, we show that Co-PatcheR achieves 46% resolved rate on SWE-bench-Verified with only 3 x 14B models. This makes Co-PatcheR the best patcher with specialized models, requiring the least training resources and the smallest models. We conduct a comprehensive ablation study to validate our recipes, as well as our choice of training data number, model size, and testing-phase scaling strategy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è½¯ä»¶è¡¥ä¸ä¿®å¤ä¸­é¢ä¸´çš„å•ä¸€æ¨¡å‹éš¾ä»¥èƒœä»»å…¨æµç¨‹ä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†Co-PatcheRï¼Œè¿™æ˜¯é¦–ä¸ªåˆ©ç”¨ç»„ä»¶ç‰¹å®šçš„å¤šä¸ªå°å‹æ¨ç†æ¨¡å‹è¿›è¡Œåä½œçš„ä¿®å¤ç³»ç»Ÿã€‚Co-PatcheRçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶ç‰¹å®šçš„ä»»åŠ¡è®¾è®¡ä¸è®­ç»ƒæ–¹æ¡ˆï¼Œå®ƒé¦–å…ˆé€šè¿‡ä¸¤æ­¥å®šä½ç¨‹åºå’Œç»“åˆæ‰¹åˆ¤(critique)çš„ç”Ÿæˆæœºåˆ¶ï¼Œè®­ç»ƒå‡ºä¸“é—¨è´Ÿè´£ç¼ºé™·å®šä½(localization)ä¸è¡¥ä¸ç”Ÿæˆ(patch generation)çš„æ¨¡å‹ã€‚ç³»ç»Ÿéšåå¼•å…¥äº†æ··åˆè¡¥ä¸éªŒè¯(hybrid patch validation)æµç¨‹ï¼Œåˆ©ç”¨ä¸¤ä¸ªç‹¬ç«‹æ¨¡å‹æ„å»ºé‡ç°ç¼ºé™·çš„æµ‹è¯•ç”¨ä¾‹å¹¶åˆ¤æ–­è¡¥ä¸æ­£ç¡®æ€§ï¼Œæœ€åé€šè¿‡åŸºäºå¤šæ•°æŠ•ç¥¨(majority vote)çš„ç­–ç•¥å®Œæˆæœ€ç»ˆé€‰æ‹©ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒCo-PatcheRä»…ä½¿ç”¨3ä¸ª14Bè§„æ¨¡çš„æ¨¡å‹ä¾¿åœ¨SWE-bench-VerifiedåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†46%çš„è§£å†³ç‡ï¼Œä¸ä»…è¶…è¶Šäº†70Bè§„æ¨¡çš„SOTAæ¨¡å‹ï¼Œè¿˜æ˜¾è‘—é™ä½äº†å¯¹è®­ç»ƒèµ„æºå’Œæ¨¡å‹è§„æ¨¡çš„éœ€æ±‚ã€‚è¿™ä¸€æˆæœè¯æ˜äº†å°å‹ä¸“ä¸šåŒ–æ¨ç†æ¨¡å‹åœ¨åä½œæ¨¡å¼ä¸‹èƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¤æ‚çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18955v2",
      "published_date": "2025-05-25 02:58:30 UTC",
      "updated_date": "2025-11-25 22:57:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:10:53.649906+00:00"
    },
    {
      "arxiv_id": "2505.18949v1",
      "title": "The Price of Format: Diversity Collapse in LLMs",
      "title_zh": "æ ¼å¼çš„ä»£ä»·ï¼šLLMs ä¸­çš„å¤šæ ·æ€§åå¡Œ",
      "authors": [
        "Longfei Yun",
        "Chenyang An",
        "Zilong Wang",
        "Letian Peng",
        "Jingbo Shang"
      ],
      "abstract": "Instruction-tuned large language models (LLMs) employ structured templates, such as role markers and special tokens, to enforce format consistency during inference. However, we identify a critical limitation of such formatting: it induces a phenomenon we term diversity collapse, where the model generates semantically similar outputs for open-ended inputs, undermining creativity and variability. We systematically evaluate this effect across tasks like story completion and free-form generation, finding that (1) diversity collapse persists even under high-temperature sampling, and (2) structural tokens in templates significantly constrain the model's output space. To contextualize these findings, we fine-tune the same model using a range of structured prompts and then evaluate them across three axes: downstream task performance, alignment behavior, and output diversity. Our analysis shows that format consistency between fine-tuning and inference is crucial for structure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on knowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity is primarily governed by the presence or absence of structural tokens, with minimal formatting yielding the most diverse outputs. These findings reveal that current prompting conventions, while beneficial for alignment, may inadvertently suppress output diversity, underscoring the need for diversity-aware prompt design and instruction tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction-tunedï¼‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ç”±äºä½¿ç”¨ç»“æ„åŒ–æ¨¡æ¿ï¼ˆå¦‚è§’è‰²æ ‡è®°å’Œç‰¹æ®Š tokenï¼‰è€Œå¯¼è‡´çš„â€œå¤šæ ·æ€§åç¼©â€ï¼ˆDiversity collapseï¼‰ç°è±¡ï¼Œå³æ¨¡å‹åœ¨é¢å¯¹å¼€æ”¾å¼è¾“å…¥æ—¶ç”Ÿæˆè¯­ä¹‰ç›¸ä¼¼çš„è¾“å‡ºï¼Œä»è€Œå‰Šå¼±äº†å…¶åˆ›é€ åŠ›å’Œå˜å¼‚æ€§ã€‚ç ”ç©¶é€šè¿‡ç³»ç»Ÿè¯„ä¼°å‘ç°ï¼Œè¿™ç§å¤šæ ·æ€§åç¼©åœ¨é«˜æ¸©é‡‡æ ·ï¼ˆhigh-temperature samplingï¼‰ä¸‹ä¾ç„¶å­˜åœ¨ï¼Œä¸”æ¨¡æ¿ä¸­çš„ç»“æ„åŒ– token æ˜¾è‘—é™åˆ¶äº†æ¨¡å‹çš„è¾“å‡ºç©ºé—´ã€‚å®éªŒè¡¨æ˜ï¼Œå¾®è°ƒä¸æ¨ç†é˜¶æ®µçš„æ ¼å¼ä¸€è‡´æ€§å¯¹äº GSM8K å’Œ IFEval ç­‰ç»“æ„æ•æ„Ÿå‹ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä½†å¯¹ MMLU ç­‰çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡å½±å“è¾ƒå°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¾“å‡ºå¤šæ ·æ€§ä¸»è¦å—ç»“æ„åŒ– token çš„å½±å“ï¼Œæç®€æ ¼å¼å¾€å¾€èƒ½äº§ç”Ÿæ›´å…·å¤šæ ·æ€§çš„ç»“æœã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰çš„æç¤ºè¯æƒ¯ä¾‹è™½æœ‰åˆ©äºå¯¹é½ï¼ˆalignmentï¼‰ï¼Œä½†å¯èƒ½æ— æ„ä¸­æŠ‘åˆ¶äº†è¾“å‡ºå¤šæ ·æ€§ï¼Œä¸ºæ­¤ç ”ç©¶å¼ºè°ƒäº†å¼€å‘å…¼é¡¾å¤šæ ·æ€§çš„æç¤ºè¯è®¾è®¡å’ŒæŒ‡ä»¤å¾®è°ƒæ–¹æ¡ˆçš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.18949v1",
      "published_date": "2025-05-25 02:52:35 UTC",
      "updated_date": "2025-05-25 02:52:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:11:02.207511+00:00"
    },
    {
      "arxiv_id": "2505.18946v1",
      "title": "SANNet: A Semantic-Aware Agentic AI Networking Framework for Multi-Agent Cross-Layer Coordination",
      "title_zh": "SANNetï¼šé¢å‘å¤šæ™ºèƒ½ä½“è·¨å±‚ååŒçš„è¯­ä¹‰æ„ŸçŸ¥æ™ºèƒ½ä½“ AI ç½‘ç»œæ¡†æ¶",
      "authors": [
        "Yong Xiao",
        "Haoran Zhou",
        "Xubo Li",
        "Yayu Gao",
        "Guangming Shi",
        "Ping Zhang"
      ],
      "abstract": "Agentic AI networking (AgentNet) is a novel AI-native networking paradigm that relies on a large number of specialized AI agents to collaborate and coordinate for autonomous decision-making, dynamic environmental adaptation, and complex goal achievement. It has the potential to facilitate real-time network management alongside capabilities for self-configuration, self-optimization, and self-adaptation across diverse and complex networking environments, laying the foundation for fully autonomous networking systems in the future. Despite its promise, AgentNet is still in the early stage of development, and there still lacks an effective networking framework to support automatic goal discovery and multi-agent self-orchestration and task assignment. This paper proposes SANNet, a novel semantic-aware agentic AI networking architecture that can infer the semantic goal of the user and automatically assign agents associated with different layers of a mobile system to fulfill the inferred goal. Motivated by the fact that one of the major challenges in AgentNet is that different agents may have different and even conflicting objectives when collaborating for certain goals, we introduce a dynamic weighting-based conflict-resolving mechanism to address this issue. We prove that SANNet can provide theoretical guarantee in both conflict-resolving and model generalization performance for multi-agent collaboration in dynamic environment. We develop a hardware prototype of SANNet based on the open RAN and 5GS core platform. Our experimental results show that SANNet can significantly improve the performance of multi-agent networking systems, even when agents with conflicting objectives are selected to collaborate for the same goal.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SANNetï¼Œä¸€ç§æ–°å‹çš„Semantic-Aware Agentic AI Networkingï¼ˆè¯­ä¹‰æ„ŸçŸ¥æ™ºèƒ½ä½“ç½‘ç»œï¼‰æ¶æ„ï¼Œæ—¨åœ¨è§£å†³AgentNetåœ¨è‡ªåŠ¨ç›®æ ‡å‘ç°ã€å¤šæ™ºèƒ½ä½“è‡ªç¼–æ’åŠä»»åŠ¡åˆ†é…æ–¹é¢çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚SANNetèƒ½å¤Ÿæ¨æ–­ç”¨æˆ·çš„è¯­ä¹‰ç›®æ ‡ï¼Œå¹¶è‡ªåŠ¨åè°ƒç§»åŠ¨ç³»ç»Ÿä¸åŒå±‚çº§ï¼ˆLayersï¼‰çš„æ™ºèƒ½ä½“ä»¥å…±åŒå®Œæˆä»»åŠ¡ã€‚é’ˆå¯¹åä½œè¿‡ç¨‹ä¸­ä¸åŒæ™ºèƒ½ä½“å¯èƒ½å­˜åœ¨çš„ç›®æ ‡å†²çªé—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åŸºäºDynamic Weighting-basedçš„å†²çªè§£å†³æœºåˆ¶ï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†å…¶åœ¨å†²çªæ¶ˆè§£å’Œæ¨¡å‹æ³›åŒ–æ€§èƒ½æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚é€šè¿‡åœ¨Open RANå’Œ5GSæ ¸å¿ƒå¹³å°ä¸Šæ„å»ºçš„ç¡¬ä»¶åŸå‹è¿›è¡ŒéªŒè¯ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨åä½œæ™ºèƒ½ä½“ç›®æ ‡å­˜åœ¨å†²çªçš„å¤æ‚ç¯å¢ƒä¸‹ï¼ŒSANNetä»èƒ½æ˜¾è‘—æå‡å¤šæ™ºèƒ½ä½“ç½‘ç»œç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ï¼Œä¸ºæœªæ¥å…¨è‡ªåŠ¨åŒ–ç½‘ç»œç³»ç»Ÿçš„å®ç°æä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "submitted to IEEE GLOBECOM'25",
      "pdf_url": "https://arxiv.org/pdf/2505.18946v1",
      "published_date": "2025-05-25 02:45:18 UTC",
      "updated_date": "2025-05-25 02:45:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:11:26.142846+00:00"
    },
    {
      "arxiv_id": "2505.21548v2",
      "title": "Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment",
      "title_zh": "æµåˆ©å´ç–ç¦»ï¼šåŒºåŸŸæ€§å¤§è¯­è¨€æ¨¡å‹äº¦ç¼ºä¹æ–‡åŒ–å¯¹é½",
      "authors": [
        "Dhruv Agarwal",
        "Anya Shukla",
        "Sunayana Sitaram",
        "Aditya Vashistha"
      ],
      "abstract": "Large language models (LLMs) are used worldwide, yet exhibit Western cultural tendencies. Many countries are now building ``regional'' LLMs, but it remains unclear whether they reflect local values and practices or merely speak local languages. Using India as a case study, we evaluate six Indic and six global LLMs on two dimensions -- values and practices -- grounded in nationally representative surveys and community-sourced QA datasets. Across tasks, Indic models do not align better with Indian norms than global models; in fact, a U.S. respondent is a closer proxy for Indian values than any Indic model. Prompting and regional fine-tuning fail to recover alignment and can even degrade existing knowledge. We attribute this to scarce culturally grounded data, especially for pretraining. We position cultural evaluation as a first-class requirement alongside multilingual benchmarks and offer a reusable, community-grounded methodology. We call for native, community-authored corpora and thick x wide evaluations to build truly sovereign LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å…¨çƒèŒƒå›´å†…æ™®éå­˜åœ¨çš„è¥¿æ–¹æ–‡åŒ–å€¾å‘ï¼Œå¹¶å¯¹æ—¨åœ¨åæ˜ æœ¬åœ°ä»·å€¼çš„åŒºåŸŸæ€§æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚ä»¥å°åº¦ä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œä½œè€…åœ¨ä»·å€¼è§‚(values)å’Œå®è·µ(practices)ä¸¤ä¸ªç»´åº¦ä¸Šå¯¹æ¯”äº†å…­ä¸ªå°åº¦è¯­(Indic)æ¨¡å‹ä¸å…­ä¸ªå…¨çƒæ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºå°åº¦è¯­æ¨¡å‹åœ¨æ–‡åŒ–å¯¹é½(cultural alignment)æ–¹é¢å¹¶æœªä¼˜äºå…¨çƒæ¨¡å‹ï¼Œç”šè‡³å‘ç°ç¾å›½å—è®¿è€…çš„ä»·å€¼è§‚æ¯”è¿™äº›æ¨¡å‹æ›´æ¥è¿‘å°åº¦æœ¬åœŸæ ‡å‡†ã€‚ç ”ç©¶å‘ç°æç¤ºè¯å·¥ç¨‹(Prompting)å’ŒåŒºåŸŸå¾®è°ƒ(regional fine-tuning)æ— æ³•æœ‰æ•ˆè§£å†³å¯¹é½é—®é¢˜ï¼Œå…¶æ ¹æºåœ¨äºç¼ºä¹å…·æœ‰æœ¬åœŸæ–‡åŒ–åº•è•´çš„é¢„è®­ç»ƒæ•°æ®(pretraining data)ã€‚è®ºæ–‡ä¸»å¼ å°†æ–‡åŒ–è¯„ä¼°ä½œä¸ºä¸å¤šè¯­è¨€åŸºå‡†(multilingual benchmarks)åŒç­‰çº§åˆ«çš„æ ¸å¿ƒè¦æ±‚ï¼Œå¹¶æå‡ºäº†ä¸€å¥—åŸºäºç¤¾åŒºçš„å¯å¤ç”¨è¯„ä¼°æ–¹æ³•ã€‚ä½œè€…æœ€åå‘¼åé€šè¿‡æ„å»ºç”±æœ¬åœ°ç¤¾åŒºæ’°å†™çš„åŸç”Ÿè¯­æ–™åº“ï¼Œä»¥å®ç°å¼€å‘çœŸæ­£å…·å¤‡ä¸»æƒçš„(sovereign) LLMsçš„ç›®æ ‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "physics.soc-ph"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2505.21548v2",
      "published_date": "2025-05-25 01:59:23 UTC",
      "updated_date": "2025-09-21 19:54:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:11:28.226228+00:00"
    },
    {
      "arxiv_id": "2505.18934v1",
      "title": "Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection",
      "title_zh": "é¢å‘å¼‚æ„å›¾å¼‚å¸¸æ£€æµ‹çš„å¡æ–¹å°æ³¢å›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Xiping Li",
        "Xiangyu Dong",
        "Xingyi Zhang",
        "Kun Xie",
        "Yuanhao Feng",
        "Bo Wang",
        "Guilin Li",
        "Wuxiong Zeng",
        "Xiujun Shu",
        "Sibo Wang"
      ],
      "abstract": "Graph Anomaly Detection (GAD) in heterogeneous networks presents unique challenges due to node and edge heterogeneity. Existing Graph Neural Network (GNN) methods primarily focus on homogeneous GAD and thus fail to address three key issues: (C1) Capturing abnormal signal and rich semantics across diverse meta-paths; (C2) Retaining high-frequency content in HIN dimension alignment; and (C3) Learning effectively from difficult anomaly samples with class imbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based on a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse domains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter, which captures anomalous information via applying dedicated Chi-Square filters to each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns features while preserving high-frequency information and incorporates heterogeneous messages by a unified Chi-Square Filter; and (3) Contribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies to address class imbalance. Extensive experiments on public and industrial datasets show that ChiGAD outperforms state-of-the-art models on multiple metrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD datasets, validating the effectiveness of Chi-Square filters. Our code is available at https://github.com/HsipingLi/ChiGAD.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼‚è´¨å›¾å¼‚å¸¸æ£€æµ‹ï¼ˆGraph Anomaly Detection, GADï¼‰ä¸­è·¨å…ƒè·¯å¾„ï¼ˆmeta-pathsï¼‰æ•æ‰å¼‚å¸¸ä¿¡å·ã€é«˜é¢‘ä¿¡æ¯ä¿ç•™ä»¥åŠç±»åˆ«ä¸å¹³è¡¡ç­‰å…³é”®æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºæ–°å‹å¡æ–¹æ»¤æ³¢å™¨ï¼ˆChi-Square filterï¼‰çš„å…‰è°±å›¾ç¥ç»ç½‘ç»œï¼ˆspectral GNNï¼‰æ¡†æ¶ ChiGADã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåŒ…å«ç”¨äºæ•æ‰å¤šç»´è¯­ä¹‰ä¿¡æ¯çš„ Multi-Graph Chi-Square Filterï¼Œèƒ½åœ¨ç‰¹å¾å¯¹é½æ—¶ä¿ç•™é«˜é¢‘å†…å®¹çš„ Interactive Meta-Graph Convolutionï¼Œä»¥åŠé€šè¿‡åŠ æƒå¤„ç†å›°éš¾æ ·æœ¬çš„ Contribution-Informed Cross-Entropy Lossã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChiGAD åœ¨å¤šä¸ªå…¬å…±åŠå·¥ä¸šæ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„åŒè´¨å›¾å˜ä½“ ChiGNN åœ¨ä¸ƒä¸ª GAD æ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†å“è¶Šè¡¨ç°ï¼Œå……åˆ†éªŒè¯äº†å¡æ–¹æ»¤æ³¢å™¨åœ¨è¯†åˆ«å›¾æ•°æ®å¼‚å¸¸æ–¹é¢çš„æœ‰æ•ˆæ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18934v1",
      "published_date": "2025-05-25 01:58:02 UTC",
      "updated_date": "2025-05-25 01:58:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:11:46.141831+00:00"
    },
    {
      "arxiv_id": "2505.18933v1",
      "title": "REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing",
      "title_zh": "REACTï¼šé€šè¿‡è¡¨å¾æå–ä¸å¯æ§å¾®è°ƒå…‹æœå¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†ç¼–è¾‘ä¸­çš„è¿‡æ‹Ÿåˆ",
      "authors": [
        "Haitian Zhong",
        "Yuhuan Liu",
        "Ziyang Xu",
        "Guofan Liu",
        "Qiang Liu",
        "Shu Wu",
        "Zhe Zhao",
        "Liang Wang",
        "Tieniu Tan"
      ],
      "abstract": "Large language model editing methods frequently suffer from overfitting, wherein factual updates can propagate beyond their intended scope, overemphasizing the edited target even when it's contextually inappropriate. To address this challenge, we introduce REACT (Representation Extraction And Controllable Tuning), a unified two-phase framework designed for precise and controllable knowledge editing. In the initial phase, we utilize tailored stimuli to extract latent factual representations and apply Principal Component Analysis with a simple learnbale linear transformation to compute a directional \"belief shift\" vector for each instance. In the second phase, we apply controllable perturbations to hidden states using the obtained vector with a magnitude scalar, gated by a pre-trained classifier that permits edits only when contextually necessary. Relevant experiments on EVOKE benchmarks demonstrate that REACT significantly reduces overfitting across nearly all evaluation metrics, and experiments on COUNTERFACT and MQuAKE shows that our method preserves balanced basic editing performance (reliability, locality, and generality) under diverse editing scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Model)çŸ¥è¯†ç¼–è¾‘ä¸­å¸¸è§çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå³äº‹å®æ›´æ–°å®¹æ˜“è¶…å‡ºé¢„æœŸèŒƒå›´å¹¶åœ¨ä¸å½“è¯­å¢ƒä¸‹è¿‡åº¦å¼ºè°ƒç¼–è¾‘ç›®æ ‡ï¼Œæå‡ºäº†åä¸ºREACT(Representation Extraction And Controllable Tuning)çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆåˆ©ç”¨å®šåˆ¶åŒ–åˆºæ¿€æå–æ½œåœ¨äº‹å®è¡¨å¾ï¼Œå¹¶é€šè¿‡ä¸»æˆåˆ†åˆ†æ(Principal Component Analysis)ä¸çº¿æ€§å˜æ¢è®¡ç®—å‡ºæ¯ä¸ªå®ä¾‹çš„â€œä¿¡å¿µåç§»â€(belief shift)å‘é‡ã€‚éšåï¼Œç³»ç»Ÿåˆ©ç”¨è¯¥å‘é‡å¯¹éšè—çŠ¶æ€(hidden states)æ–½åŠ å¯æ§æ‰°åŠ¨ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒåˆ†ç±»å™¨ä½œä¸ºé—¨æ§ï¼Œç¡®ä¿ä»…åœ¨è¯­å¢ƒå¿…è¦æ—¶è§¦å‘ç¼–è¾‘ã€‚åœ¨EVOKEåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒREACTæ˜¾è‘—é™ä½äº†è¿‡æ‹Ÿåˆé£é™©ã€‚æ­¤å¤–ï¼Œåœ¨COUNTERFACTå’ŒMQuAKEä¸Šçš„å®éªŒè¿›ä¸€æ­¥è¯å®è¯¥æ–¹æ³•åœ¨å¯é æ€§(reliability)ã€å±€éƒ¨æ€§(locality)å’Œæ³›åŒ–æ€§(generality)ä¹‹é—´å®ç°äº†è‰¯å¥½å¹³è¡¡ï¼Œä¸ºç²¾ç¡®ä¸”å¯æ§çš„çŸ¥è¯†ç¼–è¾‘æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.18933v1",
      "published_date": "2025-05-25 01:57:06 UTC",
      "updated_date": "2025-05-25 01:57:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:11:41.210548+00:00"
    },
    {
      "arxiv_id": "2505.18931v3",
      "title": "Can Large Language Models Infer Causal Relationships from Real-World Text?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦ä»çœŸå®ä¸–ç•Œæ–‡æœ¬ä¸­æ¨æ–­å› æœå…³ç³»ï¼Ÿ",
      "authors": [
        "Ryan Saklad",
        "Aman Chadha",
        "Oleg Pavlov",
        "Raha Moraffah"
      ],
      "abstract": "Understanding and inferring causal relationships from texts is a core aspect of human cognition and is essential for advancing large language models (LLMs) towards artificial general intelligence. Existing work evaluating LLM causal reasoning primarily relies on synthetic or simplified texts with explicitly stated causal relationships. These texts typically feature short passages and few causal relations, failing to reflect the complexities of real-world reasoning. In this paper, we investigate whether LLMs are capable of inferring causal relationships from real-world texts. We develop a benchmark drawn from real-world academic literature, which includes diverse texts with respect to length, complexity (different levels of explicitness, number of causal events and relationships), and domain. To the best of our knowledge, our benchmark is the first-ever real-world dataset for this task. Our experiments on this dataset show that LLMs face significant challenges in inferring causal relationships from real-world text, with the best-performing model achieving an average F$_1$ score of only 0.535. Through systematic analysis across aspects of real-world text (explicitness, number of causal events and relationships, length of text, domain), our benchmark offers targeted insights for further research into advancing LLM causal reasoning. Our code and dataset can be found at https://github.com/Ryan-Saklad/ReCITE .",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)æ˜¯å¦èƒ½å¤Ÿä»ç°å®ä¸–ç•Œçš„æ–‡æœ¬ä¸­æ¨æ–­å› æœå…³ç³»ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°ä¸»è¦ä¾èµ–åˆæˆæˆ–ç®€åŒ–æ–‡æœ¬è€Œæ— æ³•åæ˜ ç°å®å¤æ‚æ€§çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†åä¸ºReCITEçš„åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯é¦–ä¸ªç›´æ¥ä»çœŸå®å­¦æœ¯æ–‡çŒ®ä¸­æå–çš„æ•°æ®é›†ï¼Œåœ¨æ–‡æœ¬é•¿åº¦ã€æ˜¾å¼ç¨‹åº¦ã€å› æœäº‹ä»¶æ•°é‡åŠé¢†åŸŸåˆ†å¸ƒä¸Šå…·æœ‰é«˜åº¦å¤šæ ·æ€§ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼ŒLLMsåœ¨å¤„ç†ç°å®ä¸–ç•Œæ–‡æœ¬çš„å› æœæ¨ç†(Causal Reasoning)æ—¶é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹å¹³å‡F$_1$å¾—åˆ†ä»…ä¸º0.535ã€‚è¯¥ç ”ç©¶é€šè¿‡å¯¹ä¸åŒæ–‡æœ¬ç»´åº¦çš„ç³»ç»Ÿè¯„ä¼°ï¼Œä¸ºæœªæ¥æå‡LLMså¤„ç†å¤æ‚å› æœå…³ç³»çš„èƒ½åŠ›æä¾›äº†é‡è¦çš„æ•°æ®æ”¯æŒå’Œç ”ç©¶è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18931v3",
      "published_date": "2025-05-25 01:50:05 UTC",
      "updated_date": "2026-01-17 00:21:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:12:01.040933+00:00"
    },
    {
      "arxiv_id": "2505.18930v1",
      "title": "WeedNet: A Foundation Model-Based Global-to-Local AI Approach for Real-Time Weed Species Identification and Classification",
      "title_zh": "WeedNetï¼šåŸºäºåŸºç¡€æ¨¡å‹çš„â€œä»å…¨çƒåˆ°å±€éƒ¨â€äººå·¥æ™ºèƒ½æ–¹æ³•ï¼Œç”¨äºæ‚è‰ç‰©ç§çš„å®æ—¶è¯†åˆ«ä¸åˆ†ç±»",
      "authors": [
        "Yanben Shen",
        "Timilehin T. Ayanlade",
        "Venkata Naresh Boddepalli",
        "Mojdeh Saadati",
        "Ashlyn Rairdin",
        "Zi K. Deng",
        "Muhammad Arbab Arshad",
        "Aditya Balu",
        "Daren Mueller",
        "Asheesh K Singh",
        "Wesley Everman",
        "Nirav Merchant",
        "Baskar Ganapathysubramanian",
        "Meaghan Anderson",
        "Soumik Sarkar",
        "Arti Singh"
      ],
      "abstract": "Early identification of weeds is essential for effective management and control, and there is growing interest in automating the process using computer vision techniques coupled with AI methods. However, challenges associated with training AI-based weed identification models, such as limited expert-verified data and complexity and variability in morphological features, have hindered progress. To address these issues, we present WeedNet, the first global-scale weed identification model capable of recognizing an extensive set of weed species, including noxious and invasive plant species. WeedNet is an end-to-end real-time weed identification pipeline and uses self-supervised learning, fine-tuning, and enhanced trustworthiness strategies. WeedNet achieved 91.02% accuracy across 1,593 weed species, with 41% species achieving 100% accuracy. Using a fine-tuning strategy and a Global-to-Local approach, the local Iowa WeedNet model achieved an overall accuracy of 97.38% for 85 Iowa weeds, most classes exceeded a 90% mean accuracy per class. Testing across intra-species dissimilarity (developmental stages) and inter-species similarity (look-alike species) suggests that diversity in the images collected, spanning all the growth stages and distinguishable plant characteristics, is crucial in driving model performance. The generalizability and adaptability of the Global WeedNet model enable it to function as a foundational model, with the Global-to-Local strategy allowing fine-tuning for region-specific weed communities. Additional validation of drone- and ground-rover-based images highlights the potential of WeedNet for integration into robotic platforms. Furthermore, integration with AI for conversational use provides intelligent agricultural and ecological conservation consulting tools for farmers, agronomists, researchers, land managers, and government agencies across diverse landscapes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†WeedNetï¼Œè¿™æ˜¯é¦–ä¸ªå…¨çƒè§„æ¨¡çš„æ‚è‰è¯†åˆ«æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡è®¡ç®—æœºè§†è§‰ä¸äººå·¥æ™ºèƒ½æŠ€æœ¯è§£å†³ä¸“å®¶éªŒè¯æ•°æ®å—é™åŠæ‚è‰å½¢æ€ç‰¹å¾å¤æ‚å¤šå˜ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºåŸºç¡€æ¨¡å‹(Foundation Model)çš„ä»å…¨çƒåˆ°å±€éƒ¨(Global-to-Local)çš„æ–¹æ³•ï¼Œæ•´åˆäº†è‡ªç›‘ç£å­¦ä¹ (self-supervised learning)ã€å¾®è°ƒ(fine-tuning)ä»¥åŠå¢å¼ºçš„å¯ä¿¡åº¦ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒWeedNetåœ¨å…¨çƒ1,593ç§æ‚è‰ä¸­å®ç°äº†91.02%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œè€Œåœ¨ç»è¿‡å¾®è°ƒçš„çˆ±è·åå·å±€éƒ¨æ¨¡å‹ä¸­ï¼Œå…¶è¯†åˆ«85ç§æ‚è‰çš„å‡†ç¡®ç‡é«˜è¾¾97.38%ã€‚ç ”ç©¶å‘ç°ï¼ŒåŒ…å«ä¸åŒç”Ÿé•¿é˜¶æ®µå’Œæ¤ç‰©ç‰¹å¾çš„å›¾åƒå¤šæ ·æ€§æ˜¯æå‡æ¨¡å‹åœ¨å¤„ç†ç§å†…å¼‚è´¨æ€§å’Œç§é—´ç›¸ä¼¼æ€§æ—¶æ€§èƒ½çš„å…³é”®ã€‚WeedNetä½œä¸ºä¸€ç§åŸºç¡€æ¨¡å‹å…·æœ‰æå¼ºçš„æ³›åŒ–æ€§å’Œé€‚åº”æ€§ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°æ— äººæœºæˆ–åœ°é¢æœºå™¨äººå¹³å°ä¸­ï¼Œå¹¶é€šè¿‡äººå·¥æ™ºèƒ½å¯¹è¯å·¥å…·ä¸ºå†œä¸šå’Œç”Ÿæ€ä¿æŠ¤æä¾›æ™ºèƒ½åŒ–å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18930v1",
      "published_date": "2025-05-25 01:49:36 UTC",
      "updated_date": "2025-05-25 01:49:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:12:05.545900+00:00"
    },
    {
      "arxiv_id": "2505.18929v1",
      "title": "Meta-aware Learning in text-to-SQL Large Language Model",
      "title_zh": "é¢å‘ text-to-SQL å¤§è¯­è¨€æ¨¡å‹çš„å…ƒæ„ŸçŸ¥å­¦ä¹ ",
      "authors": [
        "Wenda Zhang"
      ],
      "abstract": "The advancements of Large language models (LLMs) have provided great opportunities to text-to-SQL tasks to overcome the main challenges to understand complex domain information and complex database structures in business applications. In this paper, we propose a meta-aware learning framework to integrate domain knowledge, database schema, chain-of-thought reasoning processes, and metadata relationships to improve the SQL generation quality. The proposed framework includes four learning strategies: schema-based learning, Chain-of-Thought (CoT) learning, knowledge-enhanced learning, and key information tokenization. This approach provides a comprehensive understanding of database structure and metadata information towards LLM through fine-tuning to improve its performance on SQL generation within business domains. Through two experimental studies, we have demonstrated the superiority of the proposed methods in execution accuracy, multi-task SQL generation capability, and reduction of catastrophic forgetting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ Meta-aware Learning æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ Text-to-SQL ä»»åŠ¡ä¸­ç†è§£å¤æ‚é¢†åŸŸä¿¡æ¯å’Œæ•°æ®åº“ç»“æ„çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆé¢†åŸŸçŸ¥è¯† (Domain Knowledge)ã€æ•°æ®åº“æ¨¡å¼ (Database Schema)ã€é“¾å¼æ€ç»´ (Chain-of-Thought) æ¨ç†è¿‡ç¨‹ä»¥åŠå…ƒæ•°æ®å…³ç³» (Metadata Relationships) æ¥ä¼˜åŒ– SQL ç”Ÿæˆè´¨é‡ã€‚å…·ä½“å®æ–½ä¸­åŒ…å«äº†åŸºäºæ¨¡å¼çš„å­¦ä¹  (Schema-based Learning)ã€CoT å­¦ä¹ ã€çŸ¥è¯†å¢å¼ºå­¦ä¹  (Knowledge-enhanced Learning) ä»¥åŠå…³é”®ä¿¡æ¯ä»¤ç‰ŒåŒ– (Key Information Tokenization) å››ç§æ ¸å¿ƒç­–ç•¥ã€‚è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒ (Fine-tuning) ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å…¨é¢åœ°æŒæ¡æ•°æ®åº“ç»“æ„å’Œå…ƒæ•°æ®ä¿¡æ¯ï¼Œä»è€Œæé«˜åœ¨å•†ä¸šé¢†åŸŸå†…çš„ç”Ÿæˆæ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰§è¡Œå‡†ç¡®ç‡ (Execution Accuracy)ã€å¤šä»»åŠ¡ SQL ç”Ÿæˆèƒ½åŠ›ä»¥åŠå‡å°‘ç¾éš¾æ€§é—å¿˜ (Catastrophic Forgetting) æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Keywords: text-to-SQL LLM, fine-tuning, meta-aware leanring, metadata, chain-of-thought, BigQuery SQL, business database",
      "pdf_url": "https://arxiv.org/pdf/2505.18929v1",
      "published_date": "2025-05-25 01:45:00 UTC",
      "updated_date": "2025-05-25 01:45:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:12:09.412063+00:00"
    },
    {
      "arxiv_id": "2505.18927v3",
      "title": "Moderating Harm: Benchmarking Large Language Models for Cyberbullying Detection in YouTube Comments",
      "title_zh": "å±å®³æ²»ç†ï¼šé’ˆå¯¹ YouTube è¯„è®ºç½‘ç»œæ¬ºå‡Œæ£€æµ‹çš„å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Amel Muminovic"
      ],
      "abstract": "As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse threads in gaming, lifestyle, food vlog, and music channels. The dataset comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and Indonesian, annotated independently by two reviewers with substantial agreement (Cohen's kappa = 0.83). Using a unified prompt and deterministic settings, GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful posts (recall = 0.875) but its precision fell to 0.767 due to frequent false positives. Claude delivered the highest precision at 0.920 and the lowest false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative analysis showed that all three models struggle with sarcasm, coded insults, and mixed-language slang. These results underscore the need for moderation pipelines that combine complementary models, incorporate conversational context, and fine-tune for under-represented languages and implicit abuse. A de-identified version of the dataset and full prompts is publicly released to promote reproducibility and further progress in automated content moderation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ YouTube è¯„è®ºä¸­çš„ç½‘ç»œæ¬ºå‡Œè¡Œä¸ºï¼Œå¯¹ GPT-4.1ã€Gemini 1.5 Pro å’Œ Claude 3 Opus ä¸‰æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨åŒ…å«è‹±è¯­ã€é˜¿æ‹‰ä¼¯è¯­å’Œå°å°¼è¯­åœ¨å†…çš„ 5,080 æ¡è¯„è®ºæ•°æ®é›†ï¼Œåœ¨ç»Ÿä¸€æç¤ºè¯å’Œç¡®å®šæ€§è®¾ç½®ä¸‹è¯„ä¼°äº†å„æ¨¡å‹çš„æ£€æµ‹æ•ˆèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4.1 åœ¨æ€§èƒ½å¹³è¡¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œå–å¾—äº† 0.863 çš„ F1 åˆ†æ•°ï¼›Gemini 1.5 Pro è™½ç„¶å¬å›ç‡ (recall) æœ€é«˜ï¼Œä½†å› è¯¯æŠ¥è¾ƒå¤šå¯¼è‡´ç²¾ç¡®åº¦è¾ƒä½ï¼›è€Œ Claude 3 Opus å±•ç°äº†æœ€é«˜çš„ç²¾ç¡®åº¦ (precision)ï¼Œä½†å¬å›ç‡è¡¨ç°æ¬ ä½³ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨å¤„ç†è®½åˆº (sarcasm)ã€éšæ™¦ä¾®è¾±ä»¥åŠæ··åˆè¯­è¨€ä¿šè¯­æ—¶å‡é¢ä¸´æŒ‘æˆ˜ã€‚ç ”ç©¶æœ€åå¼ºè°ƒäº†æ„å»ºå¤šæ¨¡å‹ç»„åˆå®¡æ ¸æµç¨‹ä»¥åŠé’ˆå¯¹éšæ€§è™å¾…å’Œå°‘æ•°è¯­ç§è¿›è¡Œå¾®è°ƒçš„å¿…è¦æ€§ï¼Œå¹¶å…¬å¼€å‘å¸ƒäº†å»éšç§åŒ–çš„æ•°æ®é›†ä»¥è¾…åŠ©ç›¸å…³é¢†åŸŸç ”ç©¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 3 tables, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2505.18927v3",
      "published_date": "2025-05-25 01:28:30 UTC",
      "updated_date": "2025-06-01 01:17:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:12:08.550955+00:00"
    },
    {
      "arxiv_id": "2505.18917v2",
      "title": "Behavior Injection: Preparing Language Models for Reinforcement Learning",
      "title_zh": "è¡Œä¸ºæ³¨å…¥ï¼šä¸ºå¼ºåŒ–å­¦ä¹ å‡†å¤‡è¯­è¨€æ¨¡å‹",
      "authors": [
        "Zhepeng Cen",
        "Yihang Yao",
        "William Han",
        "Zuxin Liu",
        "Ding Zhao"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a powerful post-training technique to incentivize the reasoning ability of large language models (LLMs). However, LLMs can respond very inconsistently to RL finetuning: some show substantial performance gains, while others plateau or even degrade. To understand this divergence, we analyze the per-step influence of the RL objective and identify two key conditions for effective post-training: (1) RL-informative rollout accuracy, and (2) strong data co-influence, which quantifies how much the training data affects performance on other samples. Guided by these insights, we propose behavior injection, a task-agnostic data augmentation scheme applied prior to RL. Behavior injection enriches the supervised finetuning (SFT) data by seeding exploratory and exploitative behaviors, effectively making the model more RL-ready. We evaluate our method across two reasoning benchmarks with multiple base models. The results demonstrate that our theoretically motivated augmentation can significantly increase the performance gain from RL over the pre-RL model.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)å¾®è°ƒè¿‡ç¨‹ä¸­è¡¨ç°ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œåˆ†æäº†ä¸ºä½•éƒ¨åˆ†æ¨¡å‹åœ¨RLåæ€§èƒ½æå‡æ˜¾è‘—è€Œå…¶ä»–æ¨¡å‹åˆ™è¿›å±•åœæ»ã€‚é€šè¿‡ç ”ç©¶RLç›®æ ‡çš„é€æ­¥å½±å“ï¼Œç ”ç©¶è€…è¯†åˆ«å‡ºæœ‰æ•ˆåè®­ç»ƒçš„ä¸¤ä¸ªå…³é”®æ¡ä»¶ï¼šRL-informative rollout accuracyå’Œå¼ºæ•°æ®å…±å½±å“(data co-influence)ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºbehavior injectionçš„ä»»åŠ¡æ— å…³æ•°æ®å¢å¼ºæ–¹æ¡ˆï¼Œåœ¨RLé˜¶æ®µä¹‹å‰åº”ç”¨äºæ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç›‘ç£å¾®è°ƒ(SFT)æ•°æ®ä¸­æ¤å…¥æ¢ç´¢æ€§(exploratory)å’Œåˆ©ç”¨æ€§(exploitative)è¡Œä¸ºï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹çš„RLå‡†å¤‡å°±ç»ªåº¦ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§åŸºäºç†è®ºåŠ¨æœºçš„å¢å¼ºæ‰‹æ®µèƒ½æ˜¾è‘—æ”¾å¤§æ¨¡å‹ä»RLä¸­è·å¾—çš„æ€§èƒ½æ”¶ç›Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.18917v2",
      "published_date": "2025-05-25 00:54:50 UTC",
      "updated_date": "2025-10-03 20:14:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:12:14.780726+00:00"
    },
    {
      "arxiv_id": "2505.18912v3",
      "title": "Robust Stability Analysis of Positive Lure System with Neural Network Feedback",
      "title_zh": "å…·æœ‰ç¥ç»ç½‘ç»œåé¦ˆçš„æ­£ Lur'e ç³»ç»Ÿé²æ£’ç¨³å®šæ€§åˆ†æ",
      "authors": [
        "Hamidreza Montazeri Hedesh",
        "Moh. Kamalul Wafi",
        "Bahram Shafai",
        "Milad Siami"
      ],
      "abstract": "This paper investigates the robustness of the Lur'e problem under positivity constraints, drawing on results from the positive Aizerman conjecture and robustness properties of Metzler matrices. Specifically, we consider a control system of Lur'e type in which not only the linear part includes parametric uncertainty but also the nonlinear sector bound is unknown. We investigate tools from positive linear systems to effectively solve the problems in complicated and uncertain nonlinear systems. By leveraging the positivity characteristic of the system, we derive an explicit formula for the stability radius of Lur'e systems. Furthermore, we extend our analysis to systems with neural network (NN) feedback loops. Building on this approach, we also propose a refinement method for sector bounds of NNs. This study introduces a scalable and efficient approach for robustness analysis of both Lur'e and NN-controlled systems. Finally, the proposed results are supported by illustrative examples.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ­£æ€§çº¦æŸ(positivity constraints)ä¸‹Lur'eé—®é¢˜çš„é²æ£’æ€§ï¼Œå¹¶ç»“åˆäº†æ­£AizermançŒœæƒ³ä¸MetzlerçŸ©é˜µçš„é²æ£’æ€§è´¨ã€‚é’ˆå¯¹çº¿æ€§éƒ¨åˆ†åŒ…å«å‚æ•°ä¸ç¡®å®šæ€§ä¸”éçº¿æ€§æ‰‡åŒºè¾¹ç•Œ(sector bound)æœªçŸ¥çš„å¤æ‚ç³»ç»Ÿï¼Œæœ¬æ–‡åˆ©ç”¨æ­£çº¿æ€§ç³»ç»Ÿå·¥å…·æ¨å¯¼å‡ºäº†Lur'eç³»ç»Ÿç¨³å®šåŠå¾„(stability radius)çš„æ˜¾å¼å…¬å¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿›ä¸€æ­¥å°†åˆ†ææ‰©å±•è‡³åŒ…å«ç¥ç»ç½‘ç»œ(Neural Network, NN)åé¦ˆå›è·¯çš„ç³»ç»Ÿï¼Œå¹¶æå‡ºäº†ä¸€ç§é’ˆå¯¹ç¥ç»ç½‘ç»œæ‰‡åŒºè¾¹ç•Œçš„ç²¾ç»†åŒ–æ”¹è¿›æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºLur'eç³»ç»ŸåŠç¥ç»ç½‘ç»œæ§åˆ¶ç³»ç»Ÿçš„é²æ£’æ€§åˆ†ææä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡å¯æ‰©å±•æ€§çš„æ–°æ–¹æ¡ˆï¼Œæœ€åé€šè¿‡å…·ä½“å®ä¾‹éªŒè¯äº†ç†è®ºç»“æœçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "Accepted at the 9th IEEE Conference on Control Technology and Applications (CCTA) 2025, San Diego, California",
      "pdf_url": "https://arxiv.org/pdf/2505.18912v3",
      "published_date": "2025-05-25 00:37:28 UTC",
      "updated_date": "2025-10-03 20:16:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:12:19.952431+00:00"
    },
    {
      "arxiv_id": "2505.18907v1",
      "title": "Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations",
      "title_zh": "é€šè¿‡å¢å¼ºä¸­é—´è¡¨ç¤ºå¼ºåŒ–æŒ‡ä»¤å±‚çº§çš„æ‰§è¡Œ",
      "authors": [
        "Sanjay Kariyappa",
        "G. Edward Suh"
      ],
      "abstract": "Prompt injection attacks are a critical security vulnerability in large language models (LLMs), allowing attackers to hijack model behavior by injecting malicious instructions within the input context. Recent defense mechanisms have leveraged an Instruction Hierarchy (IH) Signal, often implemented through special delimiter tokens or additive embeddings to denote the privilege level of input tokens. However, these prior works typically inject the IH signal exclusively at the initial input layer, which we hypothesize limits its ability to effectively distinguish the privilege levels of tokens as it propagates through the different layers of the model. To overcome this limitation, we introduce a novel approach that injects the IH signal into the intermediate token representations within the network. Our method augments these representations with layer-specific trainable embeddings that encode the privilege information. Our evaluations across multiple models and training methods reveal that our proposal yields between $1.6\\times$ and $9.2\\times$ reduction in attack success rate on gradient-based prompt injection attacks compared to state-of-the-art methods, without significantly degrading the model's utility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) é¢ä¸´çš„æç¤ºæ³¨å…¥æ”»å‡» (Prompt injection attacks) è¿™ä¸€å…³é”®å®‰å…¨æ¼æ´ï¼Œæå‡ºäº†ä¸€ç§å¢å¼ºæŒ‡ä»¤å±‚çº§ (Instruction Hierarchy, IH) å¼ºåˆ¶æ‰§è¡Œçš„æ–°æ–¹æ³•ã€‚ä»¥å¾€çš„é˜²å¾¡æ–¹æ¡ˆé€šå¸¸ä»…åœ¨åˆå§‹è¾“å…¥å±‚æ³¨å…¥ IH ä¿¡å·ï¼Œç”±äºä¿¡å·åœ¨æ¨¡å‹æ·±å±‚ä¼ é€’ä¸­ä¼šé€æ¸è¡°å‡ï¼Œå¯¼è‡´å…¶åœ¨åŒºåˆ†ä¸åŒç‰¹æƒçº§åˆ«çš„æ ‡è®°æ—¶æ•ˆæœå—é™ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶åˆ›æ–°æ€§åœ°å°† IH ä¿¡å·æ³¨å…¥åˆ°ç½‘ç»œå†…éƒ¨çš„ä¸­é—´æ ‡è®°è¡¨ç¤º (Intermediate token representations) ä¸­ï¼Œé€šè¿‡å±‚ç‰¹å®šçš„å¯è®­ç»ƒåµŒå…¥ (Layer-specific trainable embeddings) æ¥ç¼–ç ç‰¹æƒä¿¡æ¯ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹å’Œè®­ç»ƒè®¾ç½®ä¸‹ï¼Œç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„é˜²å¾¡æŠ€æœ¯ï¼Œèƒ½å°†åŸºäºæ¢¯åº¦çš„æç¤ºæ³¨å…¥æ”»å‡»æˆåŠŸç‡é™ä½ 1.6 å€è‡³ 9.2 å€ï¼Œä¸”ä¸ä¼šæ˜¾è‘—é™ä½æ¨¡å‹çš„å®ç”¨æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ¨¡å‹å†…éƒ¨æ·±åº¦å¼ºåŒ–æŒ‡ä»¤ä¼˜å…ˆçº§ï¼Œä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„å®‰å…¨è¯­è¨€æ¨¡å‹æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18907v1",
      "published_date": "2025-05-25 00:01:39 UTC",
      "updated_date": "2025-05-25 00:01:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T13:12:33.444525+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 117,
  "processed_papers_count": 117,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T13:15:23.931524+00:00"
}
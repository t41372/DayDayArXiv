[
  {
    "arxiv_id": "2406.18615v1",
    "title": "Improving Execution Concurrency in Partial-Order Plans via Block-Substitution",
    "authors": [
      "Sabah Binte Noor",
      "Fazlul Hasan Siddiqui"
    ],
    "abstract": "Partial-order plans in AI planning facilitate execution flexibility and\nseveral other tasks, such as plan reuse, modification, and decomposition, due\nto their less constrained nature. A Partial-Order Plan (POP) allows two actions\nwith no ordering between them, thus providing the flexibility of executing\nactions in different sequences. This flexibility can be further extended by\nenabling parallel execution of actions in a POP to reduce its overall execution\ntime. While extensive studies exist on improving the flexibility of a POP by\noptimizing its action orderings through plan deordering and reordering, there\nhas been limited focus on the flexibility of executing actions concurrently in\na plan. Execution concurrency in a POP can be achieved by incorporating action\nnon-concurrency constraints, specifying which actions can not be executed in\nparallel. This work formalizes the conditions for non-concurrency constraints\nto transform a POP into a parallel plan. We also introduce an algorithm to\nenhance the plan's concurrency by optimizing resource utilization through\nsubstitutions of its subplans with respect to the corresponding planning task.\nOur algorithm employs block deordering that eliminates orderings in a POP by\nencapsulating coherent actions in blocks, and then exploits blocks as candidate\nsubplans for substitutions. Experiments over the benchmark problems from\nInternational Planning Competitions (IPC) exhibit significant improvement in\nplan concurrency, specifically, with improvement in 25% of the plans, and an\noverall increase of 2.1% in concurrency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2406.03091",
    "pdf_url": "http://arxiv.org/pdf/2406.18615v1",
    "published_date": "2024-06-25 23:36:13 UTC",
    "updated_date": "2024-06-25 23:36:13 UTC"
  },
  {
    "arxiv_id": "2406.17969v2",
    "title": "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective",
    "authors": [
      "Hanqi Yan",
      "Yanzheng Xiang",
      "Guangyi Chen",
      "Yifei Wang",
      "Lin Gui",
      "Yulan He"
    ],
    "abstract": "To better interpret the intrinsic mechanism of large language models (LLMs),\nrecent studies focus on monosemanticity on its basic units. A monosemantic\nneuron is dedicated to a single and specific concept, which forms a one-to-one\ncorrelation between neurons and concepts. Despite extensive research in\nmonosemanticity probing, it remains unclear whether monosemanticity is\nbeneficial or harmful to model capacity. To explore this question, we revisit\nmonosemanticity from the feature decorrelation perspective and advocate for its\nencouragement. We experimentally observe that the current conclusion by\nwang2024learning, which suggests that decreasing monosemanticity enhances model\nperformance, does not hold when the model changes. Instead, we demonstrate that\nmonosemanticity consistently exhibits a positive correlation with model\ncapacity, in the preference alignment process. Consequently, we apply feature\ncorrelation as a proxy for monosemanticity and incorporate a feature\ndecorrelation regularizer into the dynamic preference optimization process. The\nexperiments show that our method not only enhances representation diversity and\nactivation sparsity but also improves preference alignment performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP24, Main, Long",
    "pdf_url": "http://arxiv.org/pdf/2406.17969v2",
    "published_date": "2024-06-25 22:51:08 UTC",
    "updated_date": "2024-10-15 22:37:55 UTC"
  },
  {
    "arxiv_id": "2406.17968v1",
    "title": "Efficient Document Ranking with Learnable Late Interactions",
    "authors": [
      "Ziwei Ji",
      "Himanshu Jain",
      "Andreas Veit",
      "Sashank J. Reddi",
      "Sadeep Jayasumana",
      "Ankit Singh Rawat",
      "Aditya Krishna Menon",
      "Felix Yu",
      "Sanjiv Kumar"
    ],
    "abstract": "Cross-Encoder (CE) and Dual-Encoder (DE) models are two fundamental\napproaches for query-document relevance in information retrieval. To predict\nrelevance, CE models use joint query-document embeddings, while DE models\nmaintain factorized query and document embeddings; usually, the former has\nhigher quality while the latter benefits from lower latency. Recently,\nlate-interaction models have been proposed to realize more favorable\nlatency-quality tradeoffs, by using a DE structure followed by a lightweight\nscorer based on query and document token embeddings. However, these lightweight\nscorers are often hand-crafted, and there is no understanding of their\napproximation power; further, such scorers require access to individual\ndocument token embeddings, which imposes an increased latency and storage\nburden. In this paper, we propose novel learnable late-interaction models\n(LITE) that resolve these issues. Theoretically, we prove that LITE is a\nuniversal approximator of continuous scoring functions, even for relatively\nsmall embedding dimension. Empirically, LITE outperforms previous\nlate-interaction models such as ColBERT on both in-domain and zero-shot\nre-ranking tasks. For instance, experiments on MS MARCO passage re-ranking show\nthat LITE not only yields a model with better generalization, but also lowers\nlatency and requires 0.25x storage compared to ColBERT.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17968v1",
    "published_date": "2024-06-25 22:50:48 UTC",
    "updated_date": "2024-06-25 22:50:48 UTC"
  },
  {
    "arxiv_id": "2406.17961v2",
    "title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization",
    "authors": [
      "Md Mahadi Hasan Nahid",
      "Davood Rafiei"
    ],
    "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in parsing textual data and generating code. However, their\nperformance in tasks involving tabular data, especially those requiring\nsymbolic reasoning, faces challenges due to the structural variance and\ninconsistency in table cell values often found in web tables. In this paper, we\nintroduce NormTab, a novel framework aimed at enhancing the symbolic reasoning\nperformance of LLMs by normalizing web tables. We study table normalization as\na stand-alone, one-time preprocessing step using LLMs to support symbolic\nreasoning on tabular data. Our experimental evaluation, conducted on\nchallenging web table datasets such as WikiTableQuestion and TabFact,\ndemonstrates that leveraging NormTab significantly improves symbolic reasoning\nperformance, showcasing the importance and effectiveness of web table\nnormalization for enhancing LLM-based symbolic reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2406.17961v2",
    "published_date": "2024-06-25 22:40:03 UTC",
    "updated_date": "2025-04-02 20:52:21 UTC"
  },
  {
    "arxiv_id": "2406.17960v1",
    "title": "MAGIC: Meta-Ability Guided Interactive Chain-of-Distillation for Effective-and-Efficient Vision-and-Language Navigation",
    "authors": [
      "Liuyi Wang",
      "Zongtao He",
      "Mengjiao Shen",
      "Jingwei Yang",
      "Chengju Liu",
      "Qijun Chen"
    ],
    "abstract": "Despite the remarkable developments of recent large models in Embodied\nArtificial Intelligence (E-AI), their integration into robotics is hampered by\ntheir excessive parameter sizes and computational demands. Towards the\nVision-and-Language Navigation (VLN) task, a core task in E-AI, this paper\nreveals the great potential of using knowledge distillation for obtaining\nlightweight student models by proposing a Meta-Ability Guided Interactive\nChain-of-distillation (MAGIC) method. Specifically, a Meta-Ability Knowledge\nDistillation (MAKD) framework is proposed for decoupling and refining the\nnecessary meta-abilities of VLN agents. A Meta-Knowledge Randomization\nWeighting (MKRW) and a Meta-Knowledge Transferable Determination (MKTD) module\nare incorporated to dynamically adjust aggregation weights at the meta-ability\nand sample levels, respectively. Move beyond the traditional one-step\nunidirectional distillation, an Interactive Chain-of-Distillation (ICoD)\nlearning strategy is proposed to allow students to give feedback to teachers,\nforming a new multi-step teacher-student co-evolution pipeline. Remarkably, on\nthe R2R test unseen public leaderboard, our smallest model, MAGIC-S, with only\n5% (11M) of the teacher's size, outperforms all previous methods under the same\ntraining data. Additionally, our largest model, MAGIC-L, surpasses the previous\nstate-of-the-art by 5.84% in SPL and 3.18% in SR. Furthermore, a new dataset\nwas collected and annotated from our living environments, where MAGIC-S\ndemonstrated superior performance and real-time efficiency. Our code is\npublicly available on https://github.com/CrystalSixone/VLN-MAGIC.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17960v1",
    "published_date": "2024-06-25 22:33:41 UTC",
    "updated_date": "2024-06-25 22:33:41 UTC"
  },
  {
    "arxiv_id": "2406.17957v1",
    "title": "Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment",
    "authors": [
      "Paarth Neekhara",
      "Shehzeen Hussain",
      "Subhankar Ghosh",
      "Jason Li",
      "Rafael Valle",
      "Rohan Badlani",
      "Boris Ginsburg"
    ],
    "abstract": "Large Language Model (LLM) based text-to-speech (TTS) systems have\ndemonstrated remarkable capabilities in handling large speech datasets and\ngenerating natural speech for new speakers. However, LLM-based TTS models are\nnot robust as the generated output can contain repeating words, missing words\nand mis-aligned speech (referred to as hallucinations or attention errors),\nespecially when the text contains multiple occurrences of the same token. We\nexamine these challenges in an encoder-decoder transformer model and find that\ncertain cross-attention heads in such models implicitly learn the text and\nspeech alignment when trained for predicting speech tokens for a given text. To\nmake the alignment more robust, we propose techniques utilizing CTC loss and\nattention priors that encourage monotonic cross-attention over the text tokens.\nOur guided attention training technique does not introduce any new learnable\nparameters and significantly improves robustness of LLM-based TTS models.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Published as a conference paper at INTERSPEECH 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.17957v1",
    "published_date": "2024-06-25 22:18:52 UTC",
    "updated_date": "2024-06-25 22:18:52 UTC"
  },
  {
    "arxiv_id": "2406.17949v2",
    "title": "The Overcooked Generalisation Challenge",
    "authors": [
      "Constantin Ruhdorfer",
      "Matteo Bortoletto",
      "Anna Penzkofer",
      "Andreas Bulling"
    ],
    "abstract": "We introduce the Overcooked Generalisation Challenge (OGC) - the first\nbenchmark to study agents' zero-shot cooperation abilities when faced with\nnovel partners and levels in the Overcooked-AI environment. This perspective\nstarkly contrasts a large body of previous work that has trained and evaluated\ncooperating agents only on the same level, failing to capture generalisation\nabilities required for real-world human-AI cooperation. Our challenge\ninterfaces with state-of-the-art dual curriculum design (DCD) methods to\ngenerate auto-curricula for training general agents in Overcooked. It is the\nfirst cooperative multi-agent environment specially designed for DCD methods\nand, consequently, the first benchmarked with state-of-the-art methods. It is\nfully GPU-accelerated, built on the DCD benchmark suite minimax, and freely\navailable under an open-source license:\nhttps://git.hcics.simtech.uni-stuttgart.de/public-projects/OGC. We show that\ncurrent DCD algorithms struggle to produce useful policies in this novel\nchallenge, even if combined with recent network architectures that were\ndesigned for scalability and generalisability. The OGC pushes the boundaries of\nreal-world human-AI cooperation by enabling the research community to study the\nimpact of generalisation on cooperating agents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.17949v2",
    "published_date": "2024-06-25 21:51:43 UTC",
    "updated_date": "2025-04-03 12:32:15 UTC"
  },
  {
    "arxiv_id": "2407.11008v1",
    "title": "Figuring out Figures: Using Textual References to Caption Scientific Figures",
    "authors": [
      "Stanley Cao",
      "Kevin Liu"
    ],
    "abstract": "Figures are essential channels for densely communicating complex ideas in\nscientific papers. Previous work in automatically generating figure captions\nhas been largely unsuccessful and has defaulted to using single-layer LSTMs,\nwhich no longer achieve state-of-the-art performance. In our work, we use the\nSciCap datasets curated by Hsu et al. and use a variant of a CLIP+GPT-2\nencoder-decoder model with cross-attention to generate captions conditioned on\nthe image. Furthermore, we augment our training pipeline by creating a new\ndataset MetaSciCap that incorporates textual metadata from the original paper\nrelevant to the figure, such as the title, abstract, and in-text references. We\nuse SciBERT to encode the textual metadata and use this encoding alongside the\nfigure embedding. In our experimentation with different models, we found that\nthe CLIP+GPT-2 model performs better when it receives all textual metadata from\nthe SciBERT encoder in addition to the figure, but employing a SciBERT+GPT2\nmodel that uses only the textual metadata achieved optimal performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11008v1",
    "published_date": "2024-06-25 21:49:21 UTC",
    "updated_date": "2024-06-25 21:49:21 UTC"
  },
  {
    "arxiv_id": "2407.11007v1",
    "title": "Panacea: A foundation model for clinical trial search, summarization, design, and recruitment",
    "authors": [
      "Jiacheng Lin",
      "Hanwen Xu",
      "Zifeng Wang",
      "Sheng Wang",
      "Jimeng Sun"
    ],
    "abstract": "Clinical trials are fundamental in developing new drugs, medical devices, and\ntreatments. However, they are often time-consuming and have low success rates.\nAlthough there have been initial attempts to create large language models\n(LLMs) for clinical trial design and patient-trial matching, these models\nremain task-specific and not adaptable to diverse clinical trial tasks. To\naddress this challenge, we propose a clinical trial foundation model named\nPanacea, designed to handle multiple tasks, including trial search, trial\nsummarization, trial design, and patient-trial matching. We also assemble a\nlarge-scale dataset, named TrialAlign, of 793,279 trial documents and 1,113,207\ntrial-related scientific papers, to infuse clinical knowledge into the model by\npre-training. We further curate TrialInstruct, which has 200,866 of instruction\ndata for fine-tuning. These resources enable Panacea to be widely applicable\nfor a range of clinical trial tasks based on user requirements.\n  We evaluated Panacea on a new benchmark, named TrialPanorama, which covers\neight clinical trial tasks. Our method performed the best on seven of the eight\ntasks compared to six cutting-edge generic or medicine-specific LLMs.\nSpecifically, Panacea showed great potential to collaborate with human experts\nin crafting the design of eligibility criteria, study arms, and outcome\nmeasures, in multi-round conversations. In addition, Panacea achieved 14.42%\nimprovement in patient-trial matching, 41.78% to 52.02% improvement in trial\nsearch, and consistently ranked at the top for five aspects of trial\nsummarization. Our approach demonstrates the effectiveness of Panacea in\nclinical trials and establishes a comprehensive resource, including training\ndata, model, and benchmark, for developing clinical trial foundation models,\npaving the path for AI-based clinical trial development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11007v1",
    "published_date": "2024-06-25 21:29:25 UTC",
    "updated_date": "2024-06-25 21:29:25 UTC"
  },
  {
    "arxiv_id": "2407.11006v2",
    "title": "Evaluating the Efficacy of Foundational Models: Advancing Benchmarking Practices to Enhance Fine-Tuning Decision-Making",
    "authors": [
      "Oluyemi Enoch Amujo",
      "Shanchieh Jay Yang"
    ],
    "abstract": "Recently, large language models (LLMs) have expanded into various domains.\nHowever, there remains a need to evaluate how these models perform when\nprompted with commonplace queries compared to domain-specific queries, which\nmay be useful for benchmarking prior to fine-tuning for domain-specific\ndownstream tasks. This study evaluates LLMs, specifically Gemma-2B and\nGemma-7B, across diverse domains, including cybersecurity, medicine, and\nfinance, compared to common knowledge queries. This study utilizes a\ncomprehensive methodology to assess foundational models, which includes problem\nformulation, data analysis, and the development of ThroughCut, a novel outlier\ndetection technique that automatically identifies response throughput outliers\nbased on their conciseness. This methodological rigor enhances the credibility\nof the presented evaluation frameworks. This study focused on assessing\ninference time, response length, throughput, quality, and resource utilization\nand investigated the correlations between these factors. The results indicate\nthat model size and types of prompts used for inference significantly\ninfluenced response length and quality. In addition, common prompts, which\ninclude various types of queries, generate diverse and inconsistent responses\nat irregular intervals. In contrast, domain-specific prompts consistently\ngenerate concise responses within a reasonable time. Overall, this study\nunderscores the need for comprehensive evaluation frameworks to enhance the\nreliability of benchmarking procedures in multidomain AI research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures, 2 tables, and algorithms",
    "pdf_url": "http://arxiv.org/pdf/2407.11006v2",
    "published_date": "2024-06-25 20:52:31 UTC",
    "updated_date": "2024-08-20 19:17:58 UTC"
  },
  {
    "arxiv_id": "2407.11005v2",
    "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
    "authors": [
      "Robert Friel",
      "Masha Belyi",
      "Atindriyo Sanyal"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has become a standard architectural\npattern for incorporating domain-specific knowledge into user-facing chat\napplications powered by Large Language Models (LLMs). RAG systems are\ncharacterized by (1) a document retriever that queries a domain-specific corpus\nfor context information relevant to an input query, and (2) an LLM that\ngenerates a response based on the provided query and context. However,\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\nunified evaluation criteria and annotated datasets. In response, we introduce\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\nexamples. It covers five unique industry-specific domains and various RAG task\ntypes. RAGBench examples are sourced from industry corpora such as user\nmanuals, making it particularly relevant for industry applications. Further, we\nformalize the TRACe evaluation framework: a set of explainable and actionable\nRAG evaluation metrics applicable across all RAG domains. We release the\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\nenabling actionable feedback for continuous improvement of production\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\nRAG evaluation task. We identify areas where existing approaches fall short and\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\nevaluation systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11005v2",
    "published_date": "2024-06-25 20:23:15 UTC",
    "updated_date": "2025-01-16 10:05:17 UTC"
  },
  {
    "arxiv_id": "2406.17915v1",
    "title": "Semi-supervised classification of dental conditions in panoramic radiographs using large language model and instance segmentation: A real-world dataset evaluation",
    "authors": [
      "Bernardo Silva",
      "Jefferson Fontinele",
      "Carolina Letícia Zilli Vieira",
      "João Manuel R. S. Tavares",
      "Patricia Ramos Cury",
      "Luciano Oliveira"
    ],
    "abstract": "Dental panoramic radiographs offer vast diagnostic opportunities, but\ntraining supervised deep learning networks for automatic analysis of those\nradiology images is hampered by a shortage of labeled data. Here, a different\nperspective on this problem is introduced. A semi-supervised learning framework\nis proposed to classify thirteen dental conditions on panoramic radiographs,\nwith a particular emphasis on teeth. Large language models were explored to\nannotate the most common dental conditions based on dental reports.\nAdditionally, a masked autoencoder was employed to pre-train the classification\nneural network, and a Vision Transformer was used to leverage the unlabeled\ndata. The analyses were validated using two of the most extensive datasets in\nthe literature, comprising 8,795 panoramic radiographs and 8,029 paired reports\nand images. Encouragingly, the results consistently met or surpassed the\nbaseline metrics for the Matthews correlation coefficient. A comparison of the\nproposed solution with human practitioners, supported by statistical analysis,\nhighlighted its effectiveness and performance limitations; based on the degree\nof agreement among specialists, the solution demonstrated an accuracy level\ncomparable to that of a junior specialist.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "43 pages, 12 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.17915v1",
    "published_date": "2024-06-25 19:56:12 UTC",
    "updated_date": "2024-06-25 19:56:12 UTC"
  },
  {
    "arxiv_id": "2406.17910v1",
    "title": "Transforming Software Development: Evaluating the Efficiency and Challenges of GitHub Copilot in Real-World Projects",
    "authors": [
      "Ruchika Pandey",
      "Prabhat Singh",
      "Raymond Wei",
      "Shaila Shankar"
    ],
    "abstract": "Generative AI technologies promise to transform the product development\nlifecycle. This study evaluates the efficiency gains, areas for improvement,\nand emerging challenges of using GitHub Copilot, an AI-powered coding\nassistant. We identified 15 software development tasks and assessed Copilot's\nbenefits through real-world projects on large proprietary code bases. Our\nfindings indicate significant reductions in developer toil, with up to 50% time\nsaved in code documentation and autocompletion, and 30-40% in repetitive coding\ntasks, unit test generation, debugging, and pair programming. However, Copilot\nstruggles with complex tasks, large functions, multiple files, and proprietary\ncontexts, particularly with C/C++ code. We project a 33-36% time reduction for\ncoding-related tasks in a cloud-first software development lifecycle. This\nstudy aims to quantify productivity improvements, identify underperforming\nscenarios, examine practical benefits and challenges, investigate performance\nvariations across programming languages, and discuss emerging issues related to\ncode quality, security, and developer experience.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "13 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.17910v1",
    "published_date": "2024-06-25 19:51:21 UTC",
    "updated_date": "2024-06-25 19:51:21 UTC"
  },
  {
    "arxiv_id": "2406.17906v1",
    "title": "Unbiasing on the Fly: Explanation-Guided Human Oversight of Machine Learning System Decisions",
    "authors": [
      "Hussaini Mamman",
      "Shuib Basri",
      "Abdullateef Balogun",
      "Abubakar Abdullahi Imam",
      "Ganesh Kumar",
      "Luiz Fernando Capretz"
    ],
    "abstract": "The widespread adoption of ML systems across critical domains like hiring,\nfinance, and healthcare raises growing concerns about their potential for\ndiscriminatory decision-making based on protected attributes. While efforts to\nensure fairness during development are crucial, they leave deployed ML systems\nvulnerable to potentially exhibiting discrimination during their operations. To\naddress this gap, we propose a novel framework for on-the-fly tracking and\ncorrection of discrimination in deployed ML systems. Leveraging counterfactual\nexplanations, the framework continuously monitors the predictions made by an ML\nsystem and flags discriminatory outcomes. When flagged, post-hoc explanations\nrelated to the original prediction and the counterfactual alternatives are\npresented to a human reviewer for real-time intervention. This\nhuman-in-the-loop approach empowers reviewers to accept or override the ML\nsystem decision, enabling fair and responsible ML operation under dynamic\nsettings. While further work is needed for validation and refinement, this\nframework offers a promising avenue for mitigating discrimination and building\ntrust in ML systems deployed in a wide range of domains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.17906v1",
    "published_date": "2024-06-25 19:40:55 UTC",
    "updated_date": "2024-06-25 19:40:55 UTC"
  },
  {
    "arxiv_id": "2406.17904v1",
    "title": "Application of Liquid Rank Reputation System for Twitter Trend Analysis on Bitcoin",
    "authors": [
      "Abhishek Saxena",
      "Anton Kolonin"
    ],
    "abstract": "Analyzing social media trends can create a win-win situation for both\ncreators and consumers. Creators can receive fair compensation, while consumers\ngain access to engaging, relevant, and personalized content. This paper\nproposes a new model for analyzing Bitcoin trends on Twitter by incorporating a\n'liquid democracy' approach based on user reputation. This system aims to\nidentify the most impactful trends and their influence on Bitcoin prices and\ntrading volume. It uses a Twitter sentiment analysis model based on a\nreputation rating system to determine the impact on Bitcoin price change and\ntraded volume. In addition, the reputation model considers the users'\nhigher-order friends on the social network (the initial Twitter input channels\nin our case study) to improve the accuracy and diversity of the reputation\nresults. We analyze Bitcoin-related news on Twitter to understand how trends\nand user sentiment, measured through our Liquid Rank Reputation System, affect\nBitcoin price fluctuations and trading activity within the studied time frame.\nThis reputation model can also be used as an additional layer in other trend\nand sentiment analysis models. The paper proposes the implementation,\nchallenges, and future scope of the liquid rank reputation model.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "Under publication in 2024 Ural-Siberian Conference on Biomedical\n  Engineering, Radioelectronics and Information Technology, Yekaterinburg,\n  Russia",
    "pdf_url": "http://arxiv.org/pdf/2406.17904v1",
    "published_date": "2024-06-25 19:35:25 UTC",
    "updated_date": "2024-06-25 19:35:25 UTC"
  },
  {
    "arxiv_id": "2406.17902v1",
    "title": "Domain Adaptation of Echocardiography Segmentation Via Reinforcement Learning",
    "authors": [
      "Arnaud Judge",
      "Thierry Judge",
      "Nicolas Duchateau",
      "Roman A. Sandler",
      "Joseph Z. Sokol",
      "Olivier Bernard",
      "Pierre-Marc Jodoin"
    ],
    "abstract": "Performance of deep learning segmentation models is significantly challenged\nin its transferability across different medical imaging domains, particularly\nwhen aiming to adapt these models to a target domain with insufficient\nannotated data for effective fine-tuning. While existing domain adaptation (DA)\nmethods propose strategies to alleviate this problem, these methods do not\nexplicitly incorporate human-verified segmentation priors, compromising the\npotential of a model to produce anatomically plausible segmentations. We\nintroduce RL4Seg, an innovative reinforcement learning framework that reduces\nthe need to otherwise incorporate large expertly annotated datasets in the\ntarget domain, and eliminates the need for lengthy manual human review. Using a\ntarget dataset of 10,000 unannotated 2D echocardiographic images, RL4Seg not\nonly outperforms existing state-of-the-art DA methods in accuracy but also\nachieves 99% anatomical validity on a subset of 220 expert-validated subjects\nfrom the target domain. Furthermore, our framework's reward network offers\nuncertainty estimates comparable with dedicated state-of-the-art uncertainty\nmethods, demonstrating the utility and effectiveness of RL4Seg in overcoming\ndomain adaptation challenges in medical image segmentation.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.17902v1",
    "published_date": "2024-06-25 19:26:39 UTC",
    "updated_date": "2024-06-25 19:26:39 UTC"
  },
  {
    "arxiv_id": "2406.17898v1",
    "title": "Human-centered In-building Embodied Delivery Benchmark",
    "authors": [
      "Zhuoqun Xu",
      "Yang Liu",
      "Xiaoqi Li",
      "Jiyao Zhang",
      "Hao Dong"
    ],
    "abstract": "Recently, the concept of embodied intelligence has been widely accepted and\npopularized, leading people to naturally consider the potential for\ncommercialization in this field. In this work, we propose a specific commercial\nscenario simulation, human-centered in-building embodied delivery. Furthermore,\nfor this scenario, we have developed a brand-new virtual environment system\nfrom scratch, constructing a multi-level connected building space modeled after\na polar research station. This environment also includes autonomous human\ncharacters and robots with grasping and mobility capabilities, as well as a\nlarge number of interactive items. Based on this environment, we have built a\ndelivery dataset containing 13k language instructions to guide robots in\nproviding services. We simulate human behavior through human characters and\nsample their various needs in daily life. Finally, we proposed a method\ncentered around a large multimodal model to serve as the baseline system for\nthis dataset. Compared to past embodied data work, our work focuses on a\nvirtual environment centered around human-robot interaction for commercial\nscenarios. We believe this will bring new perspectives and exploration angles\nto the embodied community.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17898v1",
    "published_date": "2024-06-25 19:19:10 UTC",
    "updated_date": "2024-06-25 19:19:10 UTC"
  },
  {
    "arxiv_id": "2406.17888v1",
    "title": "CTBench: A Comprehensive Benchmark for Evaluating Language Model Capabilities in Clinical Trial Design",
    "authors": [
      "Nafis Neehal",
      "Bowen Wang",
      "Shayom Debopadhaya",
      "Soham Dan",
      "Keerthiram Murugesan",
      "Vibha Anand",
      "Kristin P. Bennett"
    ],
    "abstract": "CTBench is introduced as a benchmark to assess language models (LMs) in\naiding clinical study design. Given study-specific metadata, CTBench evaluates\nAI models' ability to determine the baseline features of a clinical trial (CT),\nwhich include demographic and relevant features collected at the trial's start\nfrom all participants. These baseline features, typically presented in CT\npublications (often as Table 1), are crucial for characterizing study cohorts\nand validating results. Baseline features, including confounders and\ncovariates, are also necessary for accurate treatment effect estimation in\nstudies involving observational data. CTBench consists of two datasets:\n\"CT-Repo,\" containing baseline features from 1,690 clinical trials sourced from\nclinicaltrials.gov, and \"CT-Pub,\" a subset of 100 trials with more\ncomprehensive baseline features gathered from relevant publications. Two\nLM-based evaluation methods are developed to compare the actual baseline\nfeature lists against LM-generated responses. \"ListMatch-LM\" and\n\"ListMatch-BERT\" use GPT-4o and BERT scores (at various thresholds),\nrespectively, for evaluation. To establish baseline results, advanced prompt\nengineering techniques using LLaMa3-70B-Instruct and GPT-4o in zero-shot and\nthree-shot learning settings are applied to generate potential baseline\nfeatures. The performance of GPT-4o as an evaluator is validated through\nhuman-in-the-loop evaluations on the CT-Pub dataset, where clinical experts\nconfirm matches between actual and LM-generated features. The results highlight\na promising direction with significant potential for improvement, positioning\nCTBench as a useful tool for advancing research on AI in CT design and\npotentially enhancing the efficacy and robustness of CTs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17888v1",
    "published_date": "2024-06-25 18:52:48 UTC",
    "updated_date": "2024-06-25 18:52:48 UTC"
  },
  {
    "arxiv_id": "2406.17887v1",
    "title": "Federated Dynamical Low-Rank Training with Global Loss Convergence Guarantees",
    "authors": [
      "Steffen Schotthöfer",
      "M. Paul Laiu"
    ],
    "abstract": "In this work, we propose a federated dynamical low-rank training (FeDLRT)\nscheme to reduce client compute and communication costs - two significant\nperformance bottlenecks in horizontal federated learning. Our method builds\nupon dynamical low-rank splitting schemes for manifold-constrained optimization\nto create a global low-rank basis of network weights, which enables client\ntraining on a small coefficient matrix. A consistent global low-rank basis\nallows us to incorporate a variance correction scheme and prove global loss\ndescent and convergence to a stationary point. Dynamic augmentation and\ntruncation of the low-rank bases automatically optimizes computing and\ncommunication resource utilization. We demonstrate the efficiency of FeDLRT in\nan array of computer vision benchmarks and show a reduction of client compute\nand communication costs by up to an order of magnitude with minimal impacts on\nglobal accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17887v1",
    "published_date": "2024-06-25 18:51:08 UTC",
    "updated_date": "2024-06-25 18:51:08 UTC"
  },
  {
    "arxiv_id": "2406.17885v3",
    "title": "Enabling Regional Explainability by Automatic and Model-agnostic Rule Extraction",
    "authors": [
      "Yu Chen",
      "Tianyu Cui",
      "Alexander Capstick",
      "Nan Fletcher-Loyd",
      "Payam Barnaghi"
    ],
    "abstract": "In Explainable AI, rule extraction translates model knowledge into logical\nrules, such as IF-THEN statements, crucial for understanding patterns learned\nby black-box models. This could significantly aid in fields like disease\ndiagnosis, disease progression estimation, or drug discovery. However, such\napplication domains often contain imbalanced data, with the class of interest\nunderrepresented. Existing methods inevitably compromise the performance of\nrules for the minor class to maximise the overall performance. As the first\nattempt in this field, we propose a model-agnostic approach for extracting\nrules from specific subgroups of data, featuring automatic rule generation for\nnumerical features. This method enhances the regional explainability of machine\nlearning models and offers wider applicability compared to existing methods. We\nadditionally introduce a new method for selecting features to compose rules,\nreducing computational costs in high-dimensional spaces. Experiments across\nvarious datasets and models demonstrate the effectiveness of our methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17885v3",
    "published_date": "2024-06-25 18:47:50 UTC",
    "updated_date": "2024-08-15 13:08:00 UTC"
  },
  {
    "arxiv_id": "2406.17876v1",
    "title": "ET tu, CLIP? Addressing Common Object Errors for Unseen Environments",
    "authors": [
      "Ye Won Byun",
      "Cathy Jiao",
      "Shahriar Noroozizadeh",
      "Jimin Sun",
      "Rosa Vitiello"
    ],
    "abstract": "We introduce a simple method that employs pre-trained CLIP encoders to\nenhance model generalization in the ALFRED task. In contrast to previous\nliterature where CLIP replaces the visual encoder, we suggest using CLIP as an\nadditional module through an auxiliary object detection objective. We validate\nour method on the recently proposed Episodic Transformer architecture and\ndemonstrate that incorporating CLIP improves task performance on the unseen\nvalidation set. Additionally, our analysis results support that CLIP especially\nhelps with leveraging object descriptions, detecting small objects, and\ninterpreting rare words.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17876v1",
    "published_date": "2024-06-25 18:35:13 UTC",
    "updated_date": "2024-06-25 18:35:13 UTC"
  },
  {
    "arxiv_id": "2406.17873v1",
    "title": "Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback",
    "authors": [
      "Zhongtao Miao",
      "Kaiyan Zhao",
      "Yoshimasa Tsuruoka"
    ],
    "abstract": "Current representations used in reasoning steps of large language models can\nmostly be categorized into two main types: (1) natural language, which is\ndifficult to verify; and (2) non-natural language, usually programming code,\nwhich is difficult for people who are unfamiliar with coding to read. In this\npaper, we propose to use a semi-structured form to represent reasoning steps of\nlarge language models. Specifically, we use relation tuples, which are not only\nhuman-readable but also machine-friendly and easier to verify than natural\nlanguage. We implement a framework that includes three main components: (1)\nintroducing relation tuples into the reasoning steps of large language models;\n(2) implementing an automatic verification process of reasoning steps with a\nlocal code interpreter based on relation tuples; and (3) integrating a simple\nand effective dynamic feedback mechanism, which we found helpful for\nself-improvement of large language models. The experimental results on various\narithmetic datasets demonstrate the effectiveness of our method in improving\nthe arithmetic reasoning ability of large language models. The source code is\navailable at https://github.com/gpgg/art.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review, 25 figures, 8 tables, 29 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.17873v1",
    "published_date": "2024-06-25 18:21:00 UTC",
    "updated_date": "2024-06-25 18:21:00 UTC"
  },
  {
    "arxiv_id": "2406.17864v1",
    "title": "AI Risk Categorization Decoded (AIR 2024): From Government Regulations to Corporate Policies",
    "authors": [
      "Yi Zeng",
      "Kevin Klyman",
      "Andy Zhou",
      "Yu Yang",
      "Minzhou Pan",
      "Ruoxi Jia",
      "Dawn Song",
      "Percy Liang",
      "Bo Li"
    ],
    "abstract": "We present a comprehensive AI risk taxonomy derived from eight government\npolicies from the European Union, United States, and China and 16 company\npolicies worldwide, making a significant step towards establishing a unified\nlanguage for generative AI safety evaluation. We identify 314 unique risk\ncategories organized into a four-tiered taxonomy. At the highest level, this\ntaxonomy encompasses System & Operational Risks, Content Safety Risks, Societal\nRisks, and Legal & Rights Risks. The taxonomy establishes connections between\nvarious descriptions and approaches to risk, highlighting the overlaps and\ndiscrepancies between public and private sector conceptions of risk. By\nproviding this unified framework, we aim to advance AI safety through\ninformation sharing across sectors and the promotion of best practices in risk\nmitigation for generative AI models and systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17864v1",
    "published_date": "2024-06-25 18:13:05 UTC",
    "updated_date": "2024-06-25 18:13:05 UTC"
  },
  {
    "arxiv_id": "2406.17863v4",
    "title": "What type of inference is planning?",
    "authors": [
      "Miguel Lázaro-Gredilla",
      "Li Yang Ku",
      "Kevin P. Murphy",
      "Dileep George"
    ],
    "abstract": "Multiple types of inference are available for probabilistic graphical models,\ne.g., marginal, maximum-a-posteriori, and even marginal maximum-a-posteriori.\nWhich one do researchers mean when they talk about \"planning as inference\"?\nThere is no consistency in the literature, different types are used, and their\nability to do planning is further entangled with specific approximations or\nadditional constraints. In this work we use the variational framework to show\nthat, just like all commonly used types of inference correspond to different\nweightings of the entropy terms in the variational problem, planning\ncorresponds exactly to a different set of weights. This means that all the\ntricks of variational inference are readily applicable to planning. We develop\nan analogue of loopy belief propagation that allows us to perform approximate\nplanning in factored-state Markov decisions processes without incurring\nintractability due to the exponentially large state space. The variational\nperspective shows that the previous types of inference for planning are only\nadequate in environments with low stochasticity, and allows us to characterize\neach type by its own merits, disentangling the type of inference from the\nadditional approximations that its practical use requires. We validate these\nresults empirically on synthetic MDPs and tasks posed in the International\nPlanning Competition.",
    "categories": [
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "Camera-ready version update",
    "pdf_url": "http://arxiv.org/pdf/2406.17863v4",
    "published_date": "2024-06-25 18:05:31 UTC",
    "updated_date": "2025-01-14 07:33:19 UTC"
  },
  {
    "arxiv_id": "2406.17841v1",
    "title": "Probing many-body Bell correlation depth with superconducting qubits",
    "authors": [
      "Ke Wang",
      "Weikang Li",
      "Shibo Xu",
      "Mengyao Hu",
      "Jiachen Chen",
      "Yaozu Wu",
      "Chuanyu Zhang",
      "Feitong Jin",
      "Xuhao Zhu",
      "Yu Gao",
      "Ziqi Tan",
      "Aosai Zhang",
      "Ning Wang",
      "Yiren Zou",
      "Tingting Li",
      "Fanhao Shen",
      "Jiarun Zhong",
      "Zehang Bao",
      "Zitian Zhu",
      "Zixuan Song",
      "Jinfeng Deng",
      "Hang Dong",
      "Xu Zhang",
      "Pengfei Zhang",
      "Wenjie Jiang",
      "Zhide Lu",
      "Zheng-Zhi Sun",
      "Hekang Li",
      "Qiujiang Guo",
      "Zhen Wang",
      "Patrick Emonts",
      "Jordi Tura",
      "Chao Song",
      "H. Wang",
      "Dong-Ling Deng"
    ],
    "abstract": "Quantum nonlocality describes a stronger form of quantum correlation than\nthat of entanglement. It refutes Einstein's belief of local realism and is\namong the most distinctive and enigmatic features of quantum mechanics. It is a\ncrucial resource for achieving quantum advantages in a variety of practical\napplications, ranging from cryptography and certified random number generation\nvia self-testing to machine learning. Nevertheless, the detection of\nnonlocality, especially in quantum many-body systems, is notoriously\nchallenging. Here, we report an experimental certification of genuine\nmultipartite Bell correlations, which signal nonlocality in quantum many-body\nsystems, up to 24 qubits with a fully programmable superconducting quantum\nprocessor. In particular, we employ energy as a Bell correlation witness and\nvariationally decrease the energy of a many-body system across a hierarchy of\nthresholds, below which an increasing Bell correlation depth can be certified\nfrom experimental data. As an illustrating example, we variationally prepare\nthe low-energy state of a two-dimensional honeycomb model with 73 qubits and\ncertify its Bell correlations by measuring an energy that surpasses the\ncorresponding classical bound with up to 48 standard deviations. In addition,\nwe variationally prepare a sequence of low-energy states and certify their\ngenuine multipartite Bell correlations up to 24 qubits via energies measured\nefficiently by parity oscillation and multiple quantum coherence techniques.\nOur results establish a viable approach for preparing and certifying\nmultipartite Bell correlations, which provide not only a finer benchmark beyond\nentanglement for quantum devices, but also a valuable guide towards exploiting\nmultipartite Bell correlation in a wide spectrum of practical applications.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "11 pages,6 figures + 14 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.17841v1",
    "published_date": "2024-06-25 18:00:00 UTC",
    "updated_date": "2024-06-25 18:00:00 UTC"
  },
  {
    "arxiv_id": "2407.11004v2",
    "title": "The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators",
    "authors": [
      "Tzu-Heng Huang",
      "Catherine Cao",
      "Vaishnavi Bhargava",
      "Frederic Sala"
    ],
    "abstract": "Large pretrained models can be used as annotators, helping replace or augment\ncrowdworkers and enabling distilling generalist models into smaller specialist\nmodels. Unfortunately, this comes at a cost: employing top-of-the-line models\noften requires paying thousands of dollars for API calls, while the resulting\ndatasets are static and challenging to audit. To address these challenges, we\npropose a simple alternative: rather than directly querying labels from\npretrained models, we task models to generate programs that can produce labels.\nThese programs can be stored and applied locally, re-used and extended, and\ncost orders of magnitude less. Our system, Alchemist, obtains comparable to or\nbetter performance than large language model-based annotation in a range of\ntasks for a fraction of the cost: on average, improvements amount to a 12.9%\nenhancement while the total labeling costs across all datasets are reduced by a\nfactor of approximately 500x.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024 Spotlight Paper",
    "pdf_url": "http://arxiv.org/pdf/2407.11004v2",
    "published_date": "2024-06-25 17:58:26 UTC",
    "updated_date": "2025-02-03 18:17:53 UTC"
  },
  {
    "arxiv_id": "2407.09533v1",
    "title": "Video Occupancy Models",
    "authors": [
      "Manan Tomar",
      "Philippe Hansen-Estruch",
      "Philip Bachman",
      "Alex Lamb",
      "John Langford",
      "Matthew E. Taylor",
      "Sergey Levine"
    ],
    "abstract": "We introduce a new family of video prediction models designed to support\ndownstream control tasks. We call these models Video Occupancy models (VOCs).\nVOCs operate in a compact latent space, thus avoiding the need to make\npredictions about individual pixels. Unlike prior latent-space world models,\nVOCs directly predict the discounted distribution of future states in a single\nstep, thus avoiding the need for multistep roll-outs. We show that both\nproperties are beneficial when building predictive models of video for use in\ndownstream control. Code is available at\n\\href{https://github.com/manantomar/video-occupancy-models}{\\texttt{github.com/manantomar/video-occupancy-models}}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09533v1",
    "published_date": "2024-06-25 17:57:38 UTC",
    "updated_date": "2024-06-25 17:57:38 UTC"
  },
  {
    "arxiv_id": "2406.17768v3",
    "title": "EXTRACT: Efficient Policy Learning by Extracting Transferable Robot Skills from Offline Data",
    "authors": [
      "Jesse Zhang",
      "Minho Heo",
      "Zuxin Liu",
      "Erdem Biyik",
      "Joseph J Lim",
      "Yao Liu",
      "Rasool Fakoor"
    ],
    "abstract": "Most reinforcement learning (RL) methods focus on learning optimal policies\nover low-level action spaces. While these methods can perform well in their\ntraining environments, they lack the flexibility to transfer to new tasks.\nInstead, RL agents that can act over useful, temporally extended skills rather\nthan low-level actions can learn new tasks more easily. Prior work in\nskill-based RL either requires expert supervision to define useful skills,\nwhich is hard to scale, or learns a skill-space from offline data with\nheuristics that limit the adaptability of the skills, making them difficult to\ntransfer during downstream RL. Our approach, EXTRACT, instead utilizes\npre-trained vision language models to extract a discrete set of semantically\nmeaningful skills from offline data, each of which is parameterized by\ncontinuous arguments, without human supervision. This skill parameterization\nallows robots to learn new tasks by only needing to learn when to select a\nspecific skill and how to modify its arguments for the specific task. We\ndemonstrate through experiments in sparse-reward, image-based, robot\nmanipulation environments that EXTRACT can more quickly learn new tasks than\nprior works, with major gains in sample efficiency and performance over prior\nskill-based RL. Website at https://www.jessezhang.net/projects/extract/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "25 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.17768v3",
    "published_date": "2024-06-25 17:50:03 UTC",
    "updated_date": "2024-09-19 00:24:52 UTC"
  },
  {
    "arxiv_id": "2406.17764v2",
    "title": "BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning",
    "authors": [
      "Ercong Nie",
      "Bo Shao",
      "Zifeng Ding",
      "Mingyang Wang",
      "Helmut Schmid",
      "Hinrich Schütze"
    ],
    "abstract": "This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual\nin-context knowledge editing (IKE) across 53 languages, unifying three\nknowledge editing (KE) datasets: zsRE, CounterFact, and WikiFactDiff.\nCross-lingual KE, which requires knowledge edited in one language to generalize\nacross others while preserving unrelated knowledge, remains underexplored. To\naddress this gap, we systematically evaluate IKE under zero-shot, one-shot, and\nfew-shot setups, incorporating tailored metric-specific demonstrations. Our\nfindings reveal that model scale and demonstration alignment critically govern\ncross-lingual IKE efficacy, with larger models and tailored demonstrations\nsignificantly improving performance. Linguistic properties, particularly script\ntype, strongly influence performance variation across languages, with non-Latin\nlanguages underperforming due to issues like language confusion.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.17764v2",
    "published_date": "2024-06-25 17:48:56 UTC",
    "updated_date": "2025-02-19 21:35:47 UTC"
  },
  {
    "arxiv_id": "2406.17763v2",
    "title": "DiffusionPDE: Generative PDE-Solving Under Partial Observation",
    "authors": [
      "Jiahe Huang",
      "Guandao Yang",
      "Zichen Wang",
      "Jeong Joon Park"
    ],
    "abstract": "We introduce a general framework for solving partial differential equations\n(PDEs) using generative diffusion models. In particular, we focus on the\nscenarios where we do not have the full knowledge of the scene necessary to\napply classical solvers. Most existing forward or inverse PDE approaches\nperform poorly when the observations on the data or the underlying coefficients\nare incomplete, which is a common assumption for real-world measurements. In\nthis work, we propose DiffusionPDE that can simultaneously fill in the missing\ninformation and solve a PDE by modeling the joint distribution of the solution\nand coefficient spaces. We show that the learned generative priors lead to a\nversatile framework for accurately solving a wide range of PDEs under partial\nobservation, significantly outperforming the state-of-the-art methods for both\nforward and inverse directions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024. Project page:\n  https://jhhuangchloe.github.io/Diffusion-PDE/",
    "pdf_url": "http://arxiv.org/pdf/2406.17763v2",
    "published_date": "2024-06-25 17:48:24 UTC",
    "updated_date": "2024-11-01 00:08:54 UTC"
  },
  {
    "arxiv_id": "2406.17762v1",
    "title": "Solving Hard Mizar Problems with Instantiation and Strategy Invention",
    "authors": [
      "Jan Jakubův",
      "Mikoláš Janota",
      "Josef Urban"
    ],
    "abstract": "In this work, we prove over 3000 previously ATP-unproved Mizar/MPTP problems\nby using several ATP and AI methods, raising the number of ATP-solved Mizar\nproblems from 75\\% to above 80\\%. First, we start to experiment with the cvc5\nSMT solver which uses several instantiation-based heuristics that differ from\nthe superposition-based systems, that were previously applied to Mizar,and add\nmany new solutions. Then we use automated strategy invention to develop cvc5\nstrategies that largely improve cvc5's performance on the hard problems. In\nparticular, the best invented strategy solves over 14\\% more problems than the\nbest previously available cvc5 strategy. We also show that different\nclausification methods have a high impact on such instantiation-based methods,\nagain producing many new solutions. In total, the methods solve 3021 (21.3\\%)\nof the 14163 previously unsolved hard Mizar problems. This is a new milestone\nover the Mizar large-theory benchmark and a large strengthening of the hammer\nmethods for Mizar.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17762v1",
    "published_date": "2024-06-25 17:47:13 UTC",
    "updated_date": "2024-06-25 17:47:13 UTC"
  },
  {
    "arxiv_id": "2406.17840v2",
    "title": "Human-Object Interaction from Human-Level Instructions",
    "authors": [
      "Zhen Wu",
      "Jiaman Li",
      "Pei Xu",
      "C. Karen Liu"
    ],
    "abstract": "Intelligent agents must autonomously interact with the environments to\nperform daily tasks based on human-level instructions. They need a foundational\nunderstanding of the world to accurately interpret these instructions, along\nwith precise low-level movement and interaction skills to execute the derived\nactions. In this work, we propose the first complete system for synthesizing\nphysically plausible, long-horizon human-object interactions for object\nmanipulation in contextual environments, driven by human-level instructions. We\nleverage large language models (LLMs) to interpret the input instructions into\ndetailed execution plans. Unlike prior work, our system is capable of\ngenerating detailed finger-object interactions, in seamless coordination with\nfull-body movements. We also train a policy to track generated motions in\nphysics simulation via reinforcement learning (RL) to ensure physical\nplausibility of the motion. Our experiments demonstrate the effectiveness of\nour system in synthesizing realistic interactions with diverse objects in\ncomplex environments, highlighting its potential for real-world applications.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "project page: https://hoifhli.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.17840v2",
    "published_date": "2024-06-25 17:46:28 UTC",
    "updated_date": "2024-12-11 04:37:15 UTC"
  },
  {
    "arxiv_id": "2406.17761v2",
    "title": "CaLMQA: Exploring culturally specific long-form question answering across 23 languages",
    "authors": [
      "Shane Arora",
      "Marzena Karpinska",
      "Hung-Ting Chen",
      "Ipsita Bhattacharjee",
      "Mohit Iyyer",
      "Eunsol Choi"
    ],
    "abstract": "Large language models (LLMs) are used for long-form question answering\n(LFQA), which requires them to generate paragraph-length answers to complex\nquestions. While LFQA has been well-studied in English, this research has not\nbeen extended to other languages. To bridge this gap, we introduce CaLMQA, a\ncollection of 1.5K complex culturally specific questions spanning 23 languages\nand 51 culturally agnostic questions translated from English into 22 other\nlanguages. We define culturally specific questions as those uniquely or more\nlikely to be asked by people from cultures associated with the question's\nlanguage. We collect naturally-occurring questions from community web forums\nand hire native speakers to write questions to cover under-resourced,\nrarely-studied languages such as Fijian and Kirundi. Our dataset contains\ndiverse, complex questions that reflect cultural topics (e.g. traditions, laws,\nnews) and the language usage of native speakers. We automatically evaluate a\nsuite of open- and closed-source models on CaLMQA by detecting incorrect\nlanguage and token repetitions in answers, and observe that the quality of\nLLM-generated answers degrades significantly for some low-resource languages.\nLastly, we perform human evaluation on a subset of models and languages. Manual\nevaluation reveals that model performance is significantly worse for culturally\nspecific questions than for culturally agnostic questions. Our findings\nhighlight the need for further research in non-English LFQA and provide an\nevaluation framework.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "39 pages, 17 figures. Code and data available at\n  https://github.com/2015aroras/CaLMQA. Revised argument in section 4, results\n  unchanged",
    "pdf_url": "http://arxiv.org/pdf/2406.17761v2",
    "published_date": "2024-06-25 17:45:26 UTC",
    "updated_date": "2024-07-03 16:33:55 UTC"
  },
  {
    "arxiv_id": "2406.17753v3",
    "title": "Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language",
    "authors": [
      "Amalie Brogaard Pauli",
      "Isabelle Augenstein",
      "Ira Assent"
    ],
    "abstract": "We are exposed to much information trying to influence us, such as teaser\nmessages, debates, politically framed news, and propaganda - all of which use\npersuasive language. With the recent interest in Large Language Models (LLMs),\nwe study the ability of LLMs to produce persuasive text. As opposed to prior\nwork which focuses on particular domains or types of persuasion, we conduct a\ngeneral study across various domains to measure and benchmark to what degree\nLLMs produce persuasive language - both when explicitly instructed to rewrite\ntext to be more or less persuasive and when only instructed to paraphrase. We\nconstruct the new dataset Persuasive-Pairs of pairs of a short text and its\nrewrite by an LLM to amplify or diminish persuasive language. We multi-annotate\nthe pairs on a relative scale for persuasive language: a valuable resource in\nitself, and for training a regression model to score and benchmark persuasive\nlanguage, including for new LLMs across domains. In our analysis, we find that\ndifferent 'personas' in LLaMA3's system prompt change persuasive language\nsubstantially, even when only instructed to paraphrase.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.17753v3",
    "published_date": "2024-06-25 17:40:47 UTC",
    "updated_date": "2025-02-20 19:54:03 UTC"
  },
  {
    "arxiv_id": "2406.17746v2",
    "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon",
    "authors": [
      "USVSN Sai Prashanth",
      "Alvin Deng",
      "Kyle O'Brien",
      "Jyothir S V",
      "Mohammad Aflah Khan",
      "Jaydeep Borkar",
      "Christopher A. Choquette-Choo",
      "Jacob Ray Fuehne",
      "Stella Biderman",
      "Tracy Ke",
      "Katherine Lee",
      "Naomi Saphra"
    ],
    "abstract": "Memorization in language models is typically treated as a homogenous\nphenomenon, neglecting the specifics of the memorized data. We instead model\nmemorization as the effect of a set of complex factors that describe each\nsample and relate it to the model and corpus. To build intuition around these\nfactors, we break memorization down into a taxonomy: recitation of highly\nduplicated sequences, reconstruction of inherently predictable sequences, and\nrecollection of sequences that are neither. We demonstrate the usefulness of\nour taxonomy by using it to construct a predictive model for memorization. By\nanalyzing dependencies and inspecting the weights of the predictive model, we\nfind that different factors influence the likelihood of memorization\ndifferently depending on the taxonomic category.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17746v2",
    "published_date": "2024-06-25 17:32:16 UTC",
    "updated_date": "2025-05-07 18:36:12 UTC"
  },
  {
    "arxiv_id": "2406.17741v2",
    "title": "Point-SAM: Promptable 3D Segmentation Model for Point Clouds",
    "authors": [
      "Yuchen Zhou",
      "Jiayuan Gu",
      "Tung Yen Chiang",
      "Fanbo Xiang",
      "Hao Su"
    ],
    "abstract": "The development of 2D foundation models for image segmentation has been\nsignificantly advanced by the Segment Anything Model (SAM). However, achieving\nsimilar success in 3D models remains a challenge due to issues such as\nnon-unified data formats, poor model scalability, and the scarcity of labeled\ndata with diverse masks. To this end, we propose a 3D promptable segmentation\nmodel Point-SAM, focusing on point clouds. We employ an efficient\ntransformer-based architecture tailored for point clouds, extending SAM to the\n3D domain. We then distill the rich knowledge from 2D SAM for Point-SAM\ntraining by introducing a data engine to generate part-level and object-level\npseudo-labels at scale from 2D SAM. Our model outperforms state-of-the-art 3D\nsegmentation models on several indoor and outdoor benchmarks and demonstrates a\nvariety of applications, such as interactive 3D annotation and zero-shot 3D\ninstance proposal. Codes and demo can be found at\nhttps://github.com/zyc00/Point-SAM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17741v2",
    "published_date": "2024-06-25 17:28:03 UTC",
    "updated_date": "2024-12-02 23:28:56 UTC"
  },
  {
    "arxiv_id": "2406.17740v3",
    "title": "Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning",
    "authors": [
      "Arijit Sehanobish",
      "Avinava Dubey",
      "Krzysztof Choromanski",
      "Somnath Basu Roy Chowdhury",
      "Deepali Jain",
      "Vikas Sindhwani",
      "Snigdha Chaturvedi"
    ],
    "abstract": "Recent efforts to scale Transformer models have demonstrated rapid progress\nacross a wide range of tasks (Wei et al., 2022). However, fine-tuning these\nmodels for downstream tasks is expensive due to their large parameter counts.\nParameter-efficient fine-tuning (PEFT) approaches have emerged as a viable\nalternative by allowing us to fine-tune models by updating only a small number\nof parameters. In this work, we propose a general framework for parameter\nefficient fine-tuning (PEFT), based on structured unrestricted-rank matrices\n(SURM) which can serve as a drop-in replacement for popular approaches such as\nAdapters and LoRA. Unlike other methods like LoRA, SURMs provides more\nflexibility in finding the right balance between compactness and\nexpressiveness. This is achieved by using low displacement rank matrices\n(LDRMs), which hasn't been used in this context before. SURMs remain\ncompetitive with baselines, often providing significant quality improvements\nwhile using a smaller parameter budget. SURMs achieve 5-7% accuracy gains on\nvarious image classification tasks while replacing low-rank matrices in LoRA.\nIt also results in up to 12x reduction of the number of parameters in adapters\n(with virtually no loss in quality) on the GLUE benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.17740v3",
    "published_date": "2024-06-25 17:26:05 UTC",
    "updated_date": "2024-12-17 21:11:27 UTC"
  },
  {
    "arxiv_id": "2406.17739v1",
    "title": "Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model",
    "authors": [
      "Fei Xia",
      "Yixuan Weng",
      "Shizhu He",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Taxonomies, which organize domain concepts into hierarchical structures, are\ncrucial for building knowledge systems and downstream applications. As domain\nknowledge evolves, taxonomies need to be continuously updated to include new\nconcepts. Previous approaches have mainly focused on adding concepts to the\nleaf nodes of the existing hierarchical tree, which does not fully utilize the\ntaxonomy's knowledge and is unable to update the original taxonomy structure\n(usually involving non-leaf nodes). In this paper, we propose a two-stage\nmethod called ATTEMPT for taxonomy completion. Our method inserts new concepts\ninto the correct position by finding a parent node and labeling child nodes.\nSpecifically, by combining local nodes with prompts to generate natural\nsentences, we take advantage of pre-trained language models for\nhypernym/hyponymy recognition. Experimental results on two public datasets\n(including six domains) show that ATTEMPT performs best on both taxonomy\ncompletion and extension tasks, surpassing existing methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17739v1",
    "published_date": "2024-06-25 17:25:02 UTC",
    "updated_date": "2024-06-25 17:25:02 UTC"
  },
  {
    "arxiv_id": "2406.17737v1",
    "title": "LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users",
    "authors": [
      "Elinor Poole-Dayan",
      "Deb Roy",
      "Jad Kabbara"
    ],
    "abstract": "While state-of-the-art Large Language Models (LLMs) have shown impressive\nperformance on many tasks, there has been extensive research on undesirable\nmodel behavior such as hallucinations and bias. In this work, we investigate\nhow the quality of LLM responses changes in terms of information accuracy,\ntruthfulness, and refusals depending on three user traits: English proficiency,\neducation level, and country of origin. We present extensive experimentation on\nthree state-of-the-art LLMs and two different datasets targeting truthfulness\nand factuality. Our findings suggest that undesirable behaviors in\nstate-of-the-art LLMs occur disproportionately more for users with lower\nEnglish proficiency, of lower education status, and originating from outside\nthe US, rendering these models unreliable sources of information towards their\nmost vulnerable users.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17737v1",
    "published_date": "2024-06-25 17:24:07 UTC",
    "updated_date": "2024-06-25 17:24:07 UTC"
  },
  {
    "arxiv_id": "2406.17838v1",
    "title": "InFiConD: Interactive No-code Fine-tuning with Concept-based Knowledge Distillation",
    "authors": [
      "Jinbin Huang",
      "Wenbin He",
      "Liang Gou",
      "Liu Ren",
      "Chris Bryan"
    ],
    "abstract": "The emergence of large-scale pre-trained models has heightened their\napplication in various downstream tasks, yet deployment is a challenge in\nenvironments with limited computational resources. Knowledge distillation has\nemerged as a solution in such scenarios, whereby knowledge from large teacher\nmodels is transferred into smaller student' models, but this is a non-trivial\nprocess that traditionally requires technical expertise in AI/ML. To address\nthese challenges, this paper presents InFiConD, a novel framework that\nleverages visual concepts to implement the knowledge distillation process and\nenable subsequent no-code fine-tuning of student models. We develop a novel\nknowledge distillation pipeline based on extracting text-aligned visual\nconcepts from a concept corpus using multimodal models, and construct highly\ninterpretable linear student models based on visual concepts that mimic a\nteacher model in a response-based manner. InFiConD's interface allows users to\ninteractively fine-tune the student model by manipulating concept influences\ndirectly in the user interface. We validate InFiConD via a robust usage\nscenario and user study. Our findings indicate that InFiConD's\nhuman-in-the-loop and visualization-driven approach enables users to\neffectively create and analyze student models, understand how knowledge is\ntransferred, and efficiently perform fine-tuning operations. We discuss how\nthis work highlights the potential of interactive and visual methods in making\nknowledge distillation and subsequent no-code fine-tuning more accessible and\nadaptable to a wider range of users with domain-specific demands.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17838v1",
    "published_date": "2024-06-25 16:56:45 UTC",
    "updated_date": "2024-06-25 16:56:45 UTC"
  },
  {
    "arxiv_id": "2406.17714v3",
    "title": "Compositional Models for Estimating Causal Effects",
    "authors": [
      "Purva Pruthi",
      "David Jensen"
    ],
    "abstract": "Many real-world systems can be usefully represented as sets of interacting\ncomponents. Examples include computational systems, such as query processors\nand compilers, natural systems, such as cells and ecosystems, and social\nsystems, such as families and organizations. However, current approaches to\nestimating potential outcomes and causal effects typically treat such systems\nas single units, represent them with a fixed set of variables, and assume a\nhomogeneous data-generating process. In this work, we study a compositional\napproach for estimating individual-level potential outcomes and causal effects\nin structured systems, where each unit is represented by an instance-specific\ncomposition of multiple heterogeneous components. The compositional approach\ndecomposes unit-level causal queries into more fine-grained queries, explicitly\nmodeling how unit-level interventions affect component-level outcomes to\ngenerate a unit's outcome. We demonstrate this approach using modular neural\nnetwork architectures and show that it provides benefits for causal effect\nestimation from observational data, such as accurate causal effect estimation\nfor structured units, increased sample efficiency, improved overlap between\ntreatment and control groups, and compositional generalization to units with\nunseen combinations of components. Remarkably, our results show that\ncompositional modeling can improve the accuracy of causal estimation even when\ncomponent-level outcomes are unobserved. We also create and use a set of\nreal-world evaluation environments for the empirical evaluation of\ncompositional approaches for causal effect estimation and demonstrate the role\nof composition structure, varying amounts of component-level data access, and\ncomponent heterogeneity in the performance of compositional models as compared\nto the non-compositional approaches.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the Fourth Conference on Causal Learning and Reasoning\n  (CLeaR), 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.17714v3",
    "published_date": "2024-06-25 16:56:17 UTC",
    "updated_date": "2025-03-17 05:36:58 UTC"
  },
  {
    "arxiv_id": "2406.17711v1",
    "title": "Data curation via joint example selection further accelerates multimodal learning",
    "authors": [
      "Talfan Evans",
      "Nikhil Parthasarathy",
      "Hamza Merzic",
      "Olivier J. Henaff"
    ],
    "abstract": "Data curation is an essential component of large-scale pretraining. In this\nwork, we demonstrate that jointly selecting batches of data is more effective\nfor learning than selecting examples independently. Multimodal contrastive\nobjectives expose the dependencies between data and thus naturally yield\ncriteria for measuring the joint learnability of a batch. We derive a simple\nand tractable algorithm for selecting such batches, which significantly\naccelerate training beyond individually-prioritized data points. As performance\nimproves by selecting from larger super-batches, we also leverage recent\nadvances in model approximation to reduce the associated computational\noverhead. As a result, our approach--multimodal contrastive learning with joint\nexample selection (JEST)--surpasses state-of-the-art models with up to\n13$\\times$ fewer iterations and 10$\\times$ less computation. Essential to the\nperformance of JEST is the ability to steer the data selection process towards\nthe distribution of smaller, well-curated datasets via pretrained reference\nmodels, exposing the level of data curation as a new dimension for neural\nscaling laws.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Main text: 9 pages, 5 figures, 3 tables, 1 algorithm. Appendix: 7\n  pages, 5 figures, 1 table, 2. algorithm",
    "pdf_url": "http://arxiv.org/pdf/2406.17711v1",
    "published_date": "2024-06-25 16:52:37 UTC",
    "updated_date": "2024-06-25 16:52:37 UTC"
  },
  {
    "arxiv_id": "2406.17697v1",
    "title": "HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target Binding Affinity Prediction",
    "authors": [
      "Xi Xiao",
      "Wentao Wang",
      "Jiacheng Xie",
      "Lijing Zhu",
      "Gaofei Chen",
      "Zhengji Li",
      "Tianyang Wang",
      "Min Xu"
    ],
    "abstract": "Drug target binding affinity (DTA) is a key criterion for drug screening.\nExisting experimental methods are time-consuming and rely on limited structural\nand domain information. While learning-based methods can model sequence and\nstructural information, they struggle to integrate contextual data and often\nlack comprehensive modeling of drug-target interactions. In this study, we\npropose a novel DTA prediction method, termed HGTDP-DTA, which utilizes dynamic\nprompts within a hybrid Graph-Transformer framework. Our method generates\ncontext-specific prompts for each drug-target pair, enhancing the model's\nability to capture unique interactions. The introduction of prompt tuning\nfurther optimizes the prediction process by filtering out irrelevant noise and\nemphasizing task-relevant information, dynamically adjusting the input features\nof the molecular graph. The proposed hybrid Graph-Transformer architecture\ncombines structural information from Graph Convolutional Networks (GCNs) with\nsequence information captured by Transformers, facilitating the interaction\nbetween global and local information. Additionally, we adopted the multi-view\nfeature fusion method to project molecular graph views and affinity subgraph\nviews into a common feature space, effectively combining structural and\ncontextual information. Experiments on two widely used public datasets, Davis\nand KIBA, show that HGTDP-DTA outperforms state-of-the-art DTA prediction\nmethods in both prediction performance and generalization ability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17697v1",
    "published_date": "2024-06-25 16:33:33 UTC",
    "updated_date": "2024-06-25 16:33:33 UTC"
  },
  {
    "arxiv_id": "2407.11003v1",
    "title": "Using Large Language Models in Public Transit Systems, San Antonio as a case study",
    "authors": [
      "Ramya Jonnala",
      "Gongbo Liang",
      "Jeong Yang",
      "Izzat Alsmadi"
    ],
    "abstract": "The integration of large language models into public transit systems\nrepresents a significant advancement in urban transportation management and\npassenger experience. This study examines the impact of LLMs within San\nAntonio's public transit system, leveraging their capabilities in natural\nlanguage processing, data analysis, and real time communication. By utilizing\nGTFS and other public transportation information, the research highlights the\ntransformative potential of LLMs in enhancing route planning, reducing wait\ntimes, and providing personalized travel assistance. Our case study is the city\nof San Antonio as part of a project aiming to demonstrate how LLMs can optimize\nresource allocation, improve passenger satisfaction, and support decision\nmaking processes in transit management. We evaluated LLM responses to questions\nrelated to both information retrieval and also understanding. Ultimately, we\nbelieve that the adoption of LLMs in public transit systems can lead to more\nefficient, responsive, and user-friendly transportation networks, providing a\nmodel for other cities to follow.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11003v1",
    "published_date": "2024-06-25 16:32:56 UTC",
    "updated_date": "2024-06-25 16:32:56 UTC"
  },
  {
    "arxiv_id": "2406.17688v1",
    "title": "Unified Auto-Encoding with Masked Diffusion",
    "authors": [
      "Philippe Hansen-Estruch",
      "Sriram Vishwanath",
      "Amy Zhang",
      "Manan Tomar"
    ],
    "abstract": "At the core of both successful generative and self-supervised representation\nlearning models there is a reconstruction objective that incorporates some form\nof image corruption. Diffusion models implement this approach through a\nscheduled Gaussian corruption process, while masked auto-encoder models do so\nby masking patches of the image. Despite their different approaches, the\nunderlying similarity in their methodologies suggests a promising avenue for an\nauto-encoder capable of both de-noising tasks. We propose a unified\nself-supervised objective, dubbed Unified Masked Diffusion (UMD), that combines\npatch-based and noise-based corruption techniques within a single auto-encoding\nframework. Specifically, UMD modifies the diffusion transformer (DiT) training\nprocess by introducing an additional noise-free, high masking representation\nstep in the diffusion noising schedule, and utilizes a mixed masked and noised\nimage for subsequent timesteps. By integrating features useful for diffusion\nmodeling and for predicting masked patch tokens, UMD achieves strong\nperformance in downstream generative and representation learning tasks,\nincluding linear probing and class-conditional generation. This is achieved\nwithout the need for heavy data augmentations, multiple views, or additional\nencoders. Furthermore, UMD improves over the computational efficiency of prior\ndiffusion based methods in total training time. We release our code at\nhttps://github.com/philippe-eecs/small-vision.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "19 Pages, 8 Figures, 3Tables",
    "pdf_url": "http://arxiv.org/pdf/2406.17688v1",
    "published_date": "2024-06-25 16:24:34 UTC",
    "updated_date": "2024-06-25 16:24:34 UTC"
  },
  {
    "arxiv_id": "2406.17837v1",
    "title": "Transformer Normalisation Layers and the Independence of Semantic Subspaces",
    "authors": [
      "Stephen Menary",
      "Samuel Kaski",
      "Andre Freitas"
    ],
    "abstract": "Recent works have shown that transformers can solve contextual reasoning\ntasks by internally executing computational graphs called circuits. Circuits\noften use attention to logically match information from subspaces of the\nrepresentation, e.g. using position-in-sequence to identify the previous token.\nIn this work, we consider a semantic subspace to be any independent subspace of\nthe latent representation that can fully determine an attention distribution.\nWe show that Pre-Norm, the placement of normalisation layer used by\nstate-of-the-art transformers, violates this ability unless the model learns a\nstrict representation structure of orthogonal spheres. This is because it\ncauses linear subspaces to interfere through their common normalisation factor.\nTheoretically, we analyse circuit stability by modelling this interference as\nrandom noise on the $L_2$-norms of the query/key/value vectors, predicting a\nphenomenon of circuit collapse when sparse-attention shifts to a different\ntoken. Empirically, we investigate the sensitivity of real-world models trained\nfor mathematical addition, observing a 1% rate of circuit collapse when the\nnorms are artificially perturbed by $\\lesssim$10%. We contrast Pre-Norm with\nQKV-Norm, which places normalisation after the attention head's linear\noperators. Theoretically this relaxes the representational constraints.\nEmpirically we observe comparable in-distribution but worse out-of-distribution\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17837v1",
    "published_date": "2024-06-25 16:16:38 UTC",
    "updated_date": "2024-06-25 16:16:38 UTC"
  },
  {
    "arxiv_id": "2406.17836v1",
    "title": "A Moonshot for AI Oracles in the Sciences",
    "authors": [
      "Bryan Kaiser",
      "Tailin Wu",
      "Maike Sonnewald",
      "Colin Thackray",
      "Skylar Callis"
    ],
    "abstract": "Nobel laureate Philip Anderson and Elihu Abrahams once stated that, \"even if\nmachines did contribute to normal science, we see no mechanism by which they\ncould create a Kuhnian revolution and thereby establish a new physical law.\" In\nthis Perspective, we draw upon insights from the philosophies of science and\nartificial intelligence (AI) to propose necessary conditions of precisely such\na mechanism for generating revolutionary mathematical theories. Recent\nadvancements in AI suggest that satisfying the proposed necessary conditions by\nmachines may be plausible; thus, our proposed necessary conditions also define\na moonshot challenge. We also propose a heuristic definition of the\nintelligibility of mathematical theories to accelerate the development of\nmachine theorists.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "math.HO",
      "physics.soc-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17836v1",
    "published_date": "2024-06-25 16:15:57 UTC",
    "updated_date": "2024-06-25 16:15:57 UTC"
  },
  {
    "arxiv_id": "2407.12808v1",
    "title": "Towards Optimal Trade-offs in Knowledge Distillation for CNNs and Vision Transformers at the Edge",
    "authors": [
      "John Violos",
      "Symeon Papadopoulos",
      "Ioannis Kompatsiaris"
    ],
    "abstract": "This paper discusses four facets of the Knowledge Distillation (KD) process\nfor Convolutional Neural Networks (CNNs) and Vision Transformer (ViT)\narchitectures, particularly when executed on edge devices with constrained\nprocessing capabilities. First, we conduct a comparative analysis of the KD\nprocess between CNNs and ViT architectures, aiming to elucidate the feasibility\nand efficacy of employing different architectural configurations for the\nteacher and student, while assessing their performance and efficiency. Second,\nwe explore the impact of varying the size of the student model on accuracy and\ninference speed, while maintaining a constant KD duration. Third, we examine\nthe effects of employing higher resolution images on the accuracy, memory\nfootprint and computational workload. Last, we examine the performance\nimprovements obtained by fine-tuning the student model after KD to specific\ndownstream tasks. Through empirical evaluations and analyses, this research\nprovides AI practitioners with insights into optimal strategies for maximizing\nthe effectiveness of the KD process on edge devices.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12808v1",
    "published_date": "2024-06-25 16:15:02 UTC",
    "updated_date": "2024-06-25 16:15:02 UTC"
  },
  {
    "arxiv_id": "2406.17663v2",
    "title": "LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic",
    "authors": [
      "Aditya Kalyanpur",
      "Kailash Karthik Saravanakumar",
      "Victor Barres",
      "Jennifer Chu-Carroll",
      "David Melville",
      "David Ferrucci"
    ],
    "abstract": "We introduce LLM-ARC, a neuro-symbolic framework designed to enhance the\nlogical reasoning capabilities of Large Language Models (LLMs), by combining\nthem with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Critic\nmethod where the LLM Actor generates declarative logic programs along with\ntests for semantic correctness, while the Automated Reasoning Critic evaluates\nthe code, runs the tests and provides feedback on test failures for iterative\nrefinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves a\nnew state-of-the-art accuracy of 88.32% on the FOLIO benchmark which tests\ncomplex logical reasoning capabilities. Our experiments demonstrate significant\nimprovements over LLM-only baselines, highlighting the importance of logic test\ngeneration and iterative self-refinement. We achieve our best result using a\nfully automated self-supervised training loop where the Actor is trained on\nend-to-end dialog traces with Critic feedback. We discuss potential\nenhancements and provide a detailed error analysis, showcasing the robustness\nand efficacy of LLM-ARC for complex natural language reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17663v2",
    "published_date": "2024-06-25 15:52:15 UTC",
    "updated_date": "2024-07-19 12:59:11 UTC"
  },
  {
    "arxiv_id": "2406.17659v1",
    "title": "DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning",
    "authors": [
      "Xiaohan Zhang",
      "Zainab Altaweel",
      "Yohei Hayamizu",
      "Yan Ding",
      "Saeid Amiri",
      "Hao Yang",
      "Andy Kaminski",
      "Chad Esselink",
      "Shiqi Zhang"
    ],
    "abstract": "Vision-language models (VLMs) have been applied to robot task planning\nproblems, where the robot receives a task in natural language and generates\nplans based on visual inputs. While current VLMs have demonstrated strong\nvision-language understanding capabilities, their performance is still far from\nbeing satisfactory in planning tasks. At the same time, although classical task\nplanners, such as PDDL-based, are strong in planning for long-horizon tasks,\nthey do not work well in open worlds where unforeseen situations are common. In\nthis paper, we propose a novel task planning and execution framework, called\nDKPROMPT, which automates VLM prompting using domain knowledge in PDDL for\nclassical planning in open worlds. Results from quantitative experiments show\nthat DKPROMPT outperforms classical planning, pure VLM-based and a few other\ncompetitive baselines in task completion rate.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17659v1",
    "published_date": "2024-06-25 15:49:47 UTC",
    "updated_date": "2024-06-25 15:49:47 UTC"
  },
  {
    "arxiv_id": "2406.17654v2",
    "title": "MDHA: Multi-Scale Deformable Transformer with Hybrid Anchors for Multi-View 3D Object Detection",
    "authors": [
      "Michelle Adeline",
      "Junn Yong Loo",
      "Vishnu Monn Baskaran"
    ],
    "abstract": "Multi-view 3D object detection is a crucial component of autonomous driving\nsystems. Contemporary query-based methods primarily depend either on\ndataset-specific initialization of 3D anchors, introducing bias, or utilize\ndense attention mechanisms, which are computationally inefficient and\nunscalable. To overcome these issues, we present MDHA, a novel sparse\nquery-based framework, which constructs adaptive 3D output proposals using\nhybrid anchors from multi-view, multi-scale image input. Fixed 2D anchors are\ncombined with depth predictions to form 2.5D anchors, which are projected to\nobtain 3D proposals. To ensure high efficiency, our proposed Anchor Encoder\nperforms sparse refinement and selects the top-$k$ anchors and features.\nMoreover, while existing multi-view attention mechanisms rely on projecting\nreference points to multiple images, our novel Circular Deformable Attention\nmechanism only projects to a single image but allows reference points to\nseamlessly attend to adjacent images, improving efficiency without compromising\non performance. On the nuScenes val set, it achieves 46.4\\% mAP and 55.0\\% NDS\nwith a ResNet101 backbone. MDHA significantly outperforms the baseline where\nanchor proposals are modelled as learnable embeddings. Code is available at\nhttps://github.com/NaomiEX/MDHA.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.17654v2",
    "published_date": "2024-06-25 15:46:39 UTC",
    "updated_date": "2024-11-09 12:00:32 UTC"
  },
  {
    "arxiv_id": "2406.17651v5",
    "title": "Software Model Evolution with Large Language Models: Experiments on Simulated, Public, and Industrial Datasets",
    "authors": [
      "Christof Tinnes",
      "Alisa Welter",
      "Sven Apel"
    ],
    "abstract": "Modeling structure and behavior of software systems plays a crucial role in\nthe industrial practice of software engineering. As with other software\nengineering artifacts, software models are subject to evolution. Supporting\nmodelers in evolving software models with recommendations for model completions\nis still an open problem, though. In this paper, we explore the potential of\nlarge language models for this task. In particular, we propose an approach,\nRAMC, leveraging large language models, model histories, and\nretrieval-augmented generation for model completion. Through experiments on\nthree datasets, including an industrial application, one public open-source\ncommunity dataset, and one controlled collection of simulated model\nrepositories, we evaluate the potential of large language models for model\ncompletion with RAMC. We found that large language models are indeed a\npromising technology for supporting software model evolution (62.30%\nsemantically correct completions on real-world industrial data and up to 86.19%\ntype-correct completions). The general inference capabilities of large language\nmodels are particularly useful when dealing with concepts for which there are\nfew, noisy, or no examples at all.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "94-04",
      "D.2.2"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17651v5",
    "published_date": "2024-06-25 15:43:20 UTC",
    "updated_date": "2024-12-10 09:54:43 UTC"
  },
  {
    "arxiv_id": "2406.17650v2",
    "title": "ELIZA Reinterpreted: The world's first chatbot was not intended as a chatbot at all",
    "authors": [
      "Jeff Shrager"
    ],
    "abstract": "ELIZA, often considered the world's first chatbot, was written by Joseph\nWeizenbaum in the early 1960s. Weizenbaum did not intend to invent the chatbot,\nbut rather to build a platform for research into human-machine conversation and\nthe important cognitive processes of interpretation and misinterpretation. His\npurpose was obscured by ELIZA's fame, resulting in large part from the\nfortuitous timing of it's creation, and it's escape into the wild. In this\npaper I provide a rich historical context for ELIZA's creation, demonstrating\nthat ELIZA arose from the intersection of some of the central threads in the\ntechnical history of AI. I also briefly discuss how ELIZA escaped into the\nworld, and how its accidental escape, along with several coincidental turns of\nthe programming language screws, led both to the misapprehension that ELIZA was\nintended as a chatbot, and to the loss of the original ELIZA to history for\nover 50 years.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "In process of journal review",
    "pdf_url": "http://arxiv.org/pdf/2406.17650v2",
    "published_date": "2024-06-25 15:41:40 UTC",
    "updated_date": "2024-09-19 15:13:43 UTC"
  },
  {
    "arxiv_id": "2406.17835v1",
    "title": "The Use of AI-Robotic Systems for Scientific Discovery",
    "authors": [
      "Alexander H. Gower",
      "Konstantin Korovin",
      "Daniel Brunnsåker",
      "Filip Kronström",
      "Gabriel K. Reder",
      "Ievgeniia A. Tiukova",
      "Ronald S. Reiserer",
      "John P. Wikswo",
      "Ross D. King"
    ],
    "abstract": "The process of developing theories and models and testing them with\nexperiments is fundamental to the scientific method. Automating the entire\nscientific method then requires not only automation of the induction of\ntheories from data, but also experimentation from design to implementation.\nThis is the idea behind a robot scientist -- a coupled system of AI and\nlaboratory robotics that has agency to test hypotheses with real-world\nexperiments. In this chapter we explore some of the fundamentals of robot\nscientists in the philosophy of science. We also map the activities of a robot\nscientist to machine learning paradigms, and argue that the scientific method\nshares an analogy with active learning. We demonstrate these concepts using\nexamples from previous robot scientists, and also from Genesis: a next\ngeneration robot scientist designed for research in systems biology, comprising\na micro-fluidic system with 1000 computer-controlled micro-bioreactors and\ninterpretable models based in controlled vocabularies and logic.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, book chapter",
    "pdf_url": "http://arxiv.org/pdf/2406.17835v1",
    "published_date": "2024-06-25 15:33:01 UTC",
    "updated_date": "2024-06-25 15:33:01 UTC"
  },
  {
    "arxiv_id": "2406.17642v1",
    "title": "Banishing LLM Hallucinations Requires Rethinking Generalization",
    "authors": [
      "Johnny Li",
      "Saksham Consul",
      "Eda Zhou",
      "James Wong",
      "Naila Farooqui",
      "Yuxin Ye",
      "Nithyashree Manohar",
      "Zhuxiaona Wei",
      "Tian Wu",
      "Ben Echols",
      "Sharon Zhou",
      "Gregory Diamos"
    ],
    "abstract": "Despite their powerful chat, coding, and reasoning abilities, Large Language\nModels (LLMs) frequently hallucinate. Conventional wisdom suggests that\nhallucinations are a consequence of a balance between creativity and\nfactuality, which can be mitigated, but not eliminated, by grounding the LLM in\nexternal knowledge sources. Through extensive systematic experiments, we show\nthat these traditional approaches fail to explain why LLMs hallucinate in\npractice. Specifically, we show that LLMs augmented with a massive Mixture of\nMemory Experts (MoME) can easily memorize large datasets of random numbers. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple neural networks trained to predict the next token hallucinate when\nthe training loss is above a threshold as it usually does in practice when\ntraining on internet scale data. We interpret our findings by comparing against\ntraditional retrieval methods for mitigating hallucinations. We use our\nfindings to design a first generation model for removing hallucinations --\nLamini-1 -- that stores facts in a massive mixture of millions of memory\nexperts that are retrieved dynamically.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17642v1",
    "published_date": "2024-06-25 15:31:01 UTC",
    "updated_date": "2024-06-25 15:31:01 UTC"
  },
  {
    "arxiv_id": "2406.17640v2",
    "title": "BayTTA: Uncertainty-aware medical image classification with optimized test-time augmentation using Bayesian model averaging",
    "authors": [
      "Zeinab Sherkatghanad",
      "Moloud Abdar",
      "Mohammadreza Bakhtyari",
      "Pawel Plawiak",
      "Vladimir Makarenkov"
    ],
    "abstract": "Test-time augmentation (TTA) is a well-known technique employed during the\ntesting phase of computer vision tasks. It involves aggregating multiple\naugmented versions of input data. Combining predictions using a simple average\nformulation is a common and straightforward approach after performing TTA. This\npaper introduces a novel framework for optimizing TTA, called BayTTA\n(Bayesian-based TTA), which is based on Bayesian Model Averaging (BMA). First,\nwe generate a prediction list associated with different variations of the input\ndata created through TTA. Then, we use BMA to combine predictions weighted by\nthe respective posterior probabilities. Such an approach allows one to take\ninto account model uncertainty, and thus to enhance the predictive performance\nof the related machine learning or deep learning model. We evaluate the\nperformance of BayTTA on various public data, including three medical image\ndatasets comprising skin cancer, breast cancer, and chest X-ray images and two\nwell-known gene editing datasets, CRISPOR and GUIDE-seq. Our experimental\nresults indicate that BayTTA can be effectively integrated into\nstate-of-the-art deep learning models used in medical image analysis as well as\ninto some popular pre-trained CNN models such as VGG-16, MobileNetV2,\nDenseNet201, ResNet152V2, and InceptionRes-NetV2, leading to the enhancement in\ntheir accuracy and robustness performance. The source code of the proposed\nBayTTA method is freely available at: \\underline\n{https://github.com/Z-Sherkat/BayTTA}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17640v2",
    "published_date": "2024-06-25 15:24:06 UTC",
    "updated_date": "2024-08-27 11:00:47 UTC"
  },
  {
    "arxiv_id": "2406.17639v3",
    "title": "Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP",
    "authors": [
      "Sedigheh Eslami",
      "Gerard de Melo"
    ],
    "abstract": "Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering three\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? 3. How do these gap\nreduction approaches affect the downstream performance? We design AlignCLIP, in\norder to answer these questions and through extensive experiments, we show that\nAlignCLIP achieves noticeable enhancements in the cross-modal alignment of the\nembeddings, and thereby, reduces the modality gap, while improving the\nperformance across several zero-shot and fine-tuning downstream evaluations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17639v3",
    "published_date": "2024-06-25 15:24:02 UTC",
    "updated_date": "2024-09-16 15:32:11 UTC"
  },
  {
    "arxiv_id": "2406.17636v1",
    "title": "Aligning Diffusion Models with Noise-Conditioned Perception",
    "authors": [
      "Alexander Gambashidze",
      "Anton Kulikov",
      "Yuriy Sosnin",
      "Ilya Makarov"
    ],
    "abstract": "Recent advancements in human preference optimization, initially developed for\nLanguage Models (LMs), have shown promise for text-to-image Diffusion Models,\nenhancing prompt alignment, visual appeal, and user preference. Unlike LMs,\nDiffusion Models typically optimize in pixel or VAE space, which does not align\nwell with human perception, leading to slower and less efficient training\nduring the preference alignment stage. We propose using a perceptual objective\nin the U-Net embedding space of the diffusion model to address these issues.\nOur approach involves fine-tuning Stable Diffusion 1.5 and XL using Direct\nPreference Optimization (DPO), Contrastive Preference Optimization (CPO), and\nsupervised fine-tuning (SFT) within this embedding space. This method\nsignificantly outperforms standard latent-space implementations across various\nmetrics, including quality and computational cost. For SDXL, our approach\nprovides 60.8\\% general preference, 62.2\\% visual appeal, and 52.1\\% prompt\nfollowing against original open-sourced SDXL-DPO on the PartiPrompts dataset,\nwhile significantly reducing compute. Our approach not only improves the\nefficiency and quality of human preference alignment for diffusion models but\nis also easily integrable with other optimization techniques. The training code\nand LoRA weights will be available here:\nhttps://huggingface.co/alexgambashidze/SDXL\\_NCP-DPO\\_v0.1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17636v1",
    "published_date": "2024-06-25 15:21:50 UTC",
    "updated_date": "2024-06-25 15:21:50 UTC"
  },
  {
    "arxiv_id": "2406.17630v3",
    "title": "KANQAS: Kolmogorov-Arnold Network for Quantum Architecture Search",
    "authors": [
      "Akash Kundu",
      "Aritra Sarkar",
      "Abhishek Sadhu"
    ],
    "abstract": "Quantum architecture Search (QAS) is a promising direction for optimization\nand automated design of quantum circuits towards quantum advantage. Recent\ntechniques in QAS emphasize Multi-Layer Perceptron (MLP)-based deep Q-networks.\nHowever, their interpretability remains challenging due to the large number of\nlearnable parameters and the complexities involved in selecting appropriate\nactivation functions. In this work, to overcome these challenges, we utilize\nthe Kolmogorov-Arnold Network (KAN) in the QAS algorithm, analyzing their\nefficiency in the task of quantum state preparation and quantum chemistry. In\nquantum state preparation, our results show that in a noiseless scenario, the\nprobability of success is 2 to 5 times higher than MLPs. In noisy environments,\nKAN outperforms MLPs in fidelity when approximating these states, showcasing\nits robustness against noise. In tackling quantum chemistry problems, we\nenhance the recently proposed QAS algorithm by integrating curriculum\nreinforcement learning with a KAN structure. This facilitates a more efficient\ndesign of parameterized quantum circuits by reducing the number of required\n2-qubit gates and circuit depth. Further investigation reveals that KAN\nrequires a significantly smaller number of learnable parameters compared to\nMLPs; however, the average time of executing each episode for KAN is higher.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Code available at: https://github.com/Aqasch/KANQAS_code",
    "pdf_url": "http://arxiv.org/pdf/2406.17630v3",
    "published_date": "2024-06-25 15:17:01 UTC",
    "updated_date": "2024-12-11 22:52:39 UTC"
  },
  {
    "arxiv_id": "2406.17626v1",
    "title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference",
    "authors": [
      "Erxin Yu",
      "Jing Li",
      "Ming Liao",
      "Siqi Wang",
      "Zuchen Gao",
      "Fei Mi",
      "Lanqing Hong"
    ],
    "abstract": "As large language models (LLMs) constantly evolve, ensuring their safety\nremains a critical research problem. Previous red-teaming approaches for LLM\nsafety have primarily focused on single prompt attacks or goal hijacking. To\nthe best of our knowledge, we are the first to study LLM safety in multi-turn\ndialogue coreference. We created a dataset of 1,400 questions across 14\ncategories, each featuring multi-turn coreference safety attacks. We then\nconducted detailed evaluations on five widely used open-source LLMs. The\nresults indicated that under multi-turn coreference safety attacks, the highest\nattack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was\n13.9% with the Mistral-7B-Instruct model. These findings highlight the safety\nvulnerabilities in LLMs during dialogue coreference interactions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.17626v1",
    "published_date": "2024-06-25 15:13:02 UTC",
    "updated_date": "2024-06-25 15:13:02 UTC"
  },
  {
    "arxiv_id": "2406.17624v1",
    "title": "Self-assessment, Exhibition, and Recognition: a Review of Personality in Large Language Models",
    "authors": [
      "Zhiyuan Wen",
      "Yu Yang",
      "Jiannong Cao",
      "Haoming Sun",
      "Ruosong Yang",
      "Shuaiqi Liu"
    ],
    "abstract": "As large language models (LLMs) appear to behave increasingly human-like in\ntext-based interactions, more and more researchers become interested in\ninvestigating personality in LLMs. However, the diversity of psychological\npersonality research and the rapid development of LLMs have led to a broad yet\nfragmented landscape of studies in this interdisciplinary field. Extensive\nstudies across different research focuses, different personality psychometrics,\nand different LLMs make it challenging to have a holistic overview and further\npose difficulties in applying findings to real-world applications. In this\npaper, we present a comprehensive review by categorizing current studies into\nthree research problems: self-assessment, exhibition, and recognition, based on\nthe intrinsic characteristics and external manifestations of personality in\nLLMs. For each problem, we provide a thorough analysis and conduct in-depth\ncomparisons of their corresponding solutions. Besides, we summarize research\nfindings and open challenges from current studies and further discuss their\nunderlying causes. We also collect extensive publicly available resources to\nfacilitate interested researchers and developers. Lastly, we discuss the\npotential future research directions and application scenarios. Our paper is\nthe first comprehensive survey of up-to-date literature on personality in LLMs.\nBy presenting a clear taxonomy, in-depth analysis, promising future directions,\nand extensive resource collections, we aim to provide a better understanding\nand facilitate further advancements in this emerging field.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17624v1",
    "published_date": "2024-06-25 15:08:44 UTC",
    "updated_date": "2024-06-25 15:08:44 UTC"
  },
  {
    "arxiv_id": "2406.17834v1",
    "title": "Univariate Skeleton Prediction in Multivariate Systems Using Transformers",
    "authors": [
      "Giorgio Morales",
      "John W. Sheppard"
    ],
    "abstract": "Symbolic regression (SR) methods attempt to learn mathematical expressions\nthat approximate the behavior of an observed system. However, when dealing with\nmultivariate systems, they often fail to identify the functional form that\nexplains the relationship between each variable and the system's response. To\nbegin to address this, we propose an explainable neural SR method that\ngenerates univariate symbolic skeletons that aim to explain how each variable\ninfluences the system's response. By analyzing multiple sets of data generated\nartificially, where one input variable varies while others are fixed,\nrelationships are modeled separately for each input variable. The response of\nsuch artificial data sets is estimated using a regression neural network (NN).\nFinally, the multiple sets of input-response pairs are processed by a\npre-trained Multi-Set Transformer that solves a problem we termed Multi-Set\nSkeleton Prediction and outputs a univariate symbolic skeleton. Thus, such\nskeletons represent explanations of the function approximated by the regression\nNN. Experimental results demonstrate that this method learns skeleton\nexpressions matching the underlying functions and outperforms two GP-based and\ntwo neural SR methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper accepted at European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.17834v1",
    "published_date": "2024-06-25 15:07:06 UTC",
    "updated_date": "2024-06-25 15:07:06 UTC"
  },
  {
    "arxiv_id": "2406.17615v1",
    "title": "Aligning Programming Language and Natural Language: Exploring Design Choices in Multi-Modal Transformer-Based Embedding for Bug Localization",
    "authors": [
      "Partha Chakraborty",
      "Venkatraman Arumugam",
      "Meiyappan Nagappan"
    ],
    "abstract": "Bug localization refers to the identification of source code files which is\nin a programming language and also responsible for the unexpected behavior of\nsoftware using the bug report, which is a natural language. As bug localization\nis labor-intensive, bug localization models are employed to assist software\ndevelopers. Due to the domain difference between source code files and bug\nreports, modern bug-localization systems, based on deep learning models, rely\nheavily on embedding techniques that project bug reports and source code files\ninto a shared vector space. The creation of an embedding involves several\ndesign choices, but the impact of these choices on the quality of embedding and\nthe performance of bug localization models remains unexplained in current\nresearch.\n  To address this gap, our study evaluated 14 distinct embedding models to gain\ninsights into the effects of various design choices. Subsequently, we developed\nbug localization models utilizing these embedding models to assess the\ninfluence of these choices on the performance of the localization models. Our\nfindings indicate that the pre-training strategies significantly affect the\nquality of the embedding. Moreover, we discovered that the familiarity of the\nembedding models with the data has a notable impact on the bug localization\nmodel's performance. Notably, when the training and testing data are collected\nfrom different projects, the performance of the bug localization models\nexhibits substantial fluctuations.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "D.2; I.2"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17615v1",
    "published_date": "2024-06-25 15:01:39 UTC",
    "updated_date": "2024-06-25 15:01:39 UTC"
  },
  {
    "arxiv_id": "2407.11002v2",
    "title": "MoESD: Mixture of Experts Stable Diffusion to Mitigate Gender Bias",
    "authors": [
      "Guorun Wang",
      "Lucia Specia"
    ],
    "abstract": "Text-to-image models are known to propagate social biases. For example, when\nprompted to generate images of people in certain professions, these models tend\nto systematically generate specific genders or ethnicities. In this paper, we\nshow that this bias is already present in the text encoder of the model and\nintroduce a Mixture-of-Experts approach by identifying text-encoded bias in the\nlatent space and then creating a Bias-Identification Gate mechanism. More\nspecifically, we propose MoESD (Mixture of Experts Stable Diffusion) with BiAs\n(Bias Adapters) to mitigate gender bias in text-to-image models. We also\ndemonstrate that introducing an arbitrary special token to the prompt is\nessential during the mitigation process. With experiments focusing on gender\nbias, we show that our approach successfully mitigates gender bias while\nmaintaining image quality.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11002v2",
    "published_date": "2024-06-25 14:59:31 UTC",
    "updated_date": "2024-10-24 11:28:27 UTC"
  },
  {
    "arxiv_id": "2406.17606v1",
    "title": "Diffusion-based Adversarial Purification for Intrusion Detection",
    "authors": [
      "Mohamed Amine Merzouk",
      "Erwan Beurier",
      "Reda Yaich",
      "Nora Boulahia-Cuppens",
      "Frédéric Cuppens"
    ],
    "abstract": "The escalating sophistication of cyberattacks has encouraged the integration\nof machine learning techniques in intrusion detection systems, but the rise of\nadversarial examples presents a significant challenge. These crafted\nperturbations mislead ML models, enabling attackers to evade detection or\ntrigger false alerts. As a reaction, adversarial purification has emerged as a\ncompelling solution, particularly with diffusion models showing promising\nresults. However, their purification potential remains unexplored in the\ncontext of intrusion detection. This paper demonstrates the effectiveness of\ndiffusion models in purifying adversarial examples in network intrusion\ndetection. Through a comprehensive analysis of the diffusion parameters, we\nidentify optimal configurations maximizing adversarial robustness with minimal\nimpact on normal performance. Importantly, this study reveals insights into the\nrelationship between diffusion noise and diffusion steps, representing a novel\ncontribution to the field. Our experiments are carried out on two datasets and\nagainst 5 adversarial attacks. The implementation code is publicly available.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17606v1",
    "published_date": "2024-06-25 14:48:28 UTC",
    "updated_date": "2024-06-25 14:48:28 UTC"
  },
  {
    "arxiv_id": "2407.12807v2",
    "title": "Vision Controlled Sensorized Prosthetic Hand",
    "authors": [
      "Md Abdul Baset Sarker",
      "Juan Pablo S. Sola",
      "Aaron Jones",
      "Evan Laing",
      "Ernesto Sola-Thomas",
      "Masudul H. Imtiaz"
    ],
    "abstract": "This paper presents a sensorized vision-enabled prosthetic hand aimed at\nreplicating a natural hand's performance, functionality, appearance, and\ncomfort. The design goal was to create an accessible substitution with a\nuser-friendly interface requiring little to no training. Our mechanical hand\nuses a camera and embedded processors to perform most of these tasks. The\ninterfaced pressure sensor is used to get pressure feedback and ensure a safe\ngrasp of the object; an accelerometer is used to detect gestures and release\nthe object. Unlike current EMG-based designs, the prototyped hand does not\nrequire personalized training. The details of the design, trade-offs, results,\nand informing the next iteration are presented in this paper.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12807v2",
    "published_date": "2024-06-25 14:44:04 UTC",
    "updated_date": "2024-07-19 19:52:02 UTC"
  },
  {
    "arxiv_id": "2406.17583v1",
    "title": "Towards Compositional Interpretability for XAI",
    "authors": [
      "Sean Tull",
      "Robin Lorenz",
      "Stephen Clark",
      "Ilyas Khan",
      "Bob Coecke"
    ],
    "abstract": "Artificial intelligence (AI) is currently based largely on black-box machine\nlearning models which lack interpretability. The field of eXplainable AI (XAI)\nstrives to address this major concern, being critical in high-stakes areas such\nas the finance, legal and health sectors.\n  We present an approach to defining AI models and their interpretability based\non category theory. For this we employ the notion of a compositional model,\nwhich sees a model in terms of formal string diagrams which capture its\nabstract structure together with its concrete implementation. This\ncomprehensive view incorporates deterministic, probabilistic and quantum\nmodels. We compare a wide range of AI models as compositional models, including\nlinear and rule-based models, (recurrent) neural networks, transformers, VAEs,\nand causal and DisCoCirc models.\n  Next we give a definition of interpretation of a model in terms of its\ncompositional structure, demonstrating how to analyse the interpretability of a\nmodel, and using this to clarify common themes in XAI. We find that what makes\nthe standard 'intrinsically interpretable' models so transparent is brought out\nmost clearly diagrammatically. This leads us to the more general notion of\ncompositionally-interpretable (CI) models, which additionally include, for\ninstance, causal, conceptual space, and DisCoCirc models.\n  We next demonstrate the explainability benefits of CI models. Firstly, their\ncompositional structure may allow the computation of other quantities of\ninterest, and may facilitate inference from the model to the modelled\nphenomenon by matching its structure. Secondly, they allow for diagrammatic\nexplanations for their behaviour, based on influence constraints, diagram\nsurgery and rewrite explanations. Finally, we discuss many future directions\nfor the approach, raising the question of how to learn such meaningfully\nstructured models in practice.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "math.CT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17583v1",
    "published_date": "2024-06-25 14:27:03 UTC",
    "updated_date": "2024-06-25 14:27:03 UTC"
  },
  {
    "arxiv_id": "2406.17576v1",
    "title": "Leveraging Reinforcement Learning in Red Teaming for Advanced Ransomware Attack Simulations",
    "authors": [
      "Cheng Wang",
      "Christopher Redino",
      "Ryan Clark",
      "Abdul Rahman",
      "Sal Aguinaga",
      "Sathvik Murli",
      "Dhruv Nandakumar",
      "Roland Rao",
      "Lanxiao Huang",
      "Daniel Radke",
      "Edward Bowen"
    ],
    "abstract": "Ransomware presents a significant and increasing threat to individuals and\norganizations by encrypting their systems and not releasing them until a large\nfee has been extracted. To bolster preparedness against potential attacks,\norganizations commonly conduct red teaming exercises, which involve simulated\nattacks to assess existing security measures. This paper proposes a novel\napproach utilizing reinforcement learning (RL) to simulate ransomware attacks.\nBy training an RL agent in a simulated environment mirroring real-world\nnetworks, effective attack strategies can be learned quickly, significantly\nstreamlining traditional, manual penetration testing processes. The attack\npathways revealed by the RL agent can provide valuable insights to the defense\nteam, helping them identify network weak points and develop more resilient\ndefensive measures. Experimental results on a 152-host example network confirm\nthe effectiveness of the proposed approach, demonstrating the RL agent's\ncapability to discover and orchestrate attacks on high-value targets while\nevading honeyfiles (decoy files strategically placed to detect unauthorized\naccess).",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17576v1",
    "published_date": "2024-06-25 14:16:40 UTC",
    "updated_date": "2024-06-25 14:16:40 UTC"
  },
  {
    "arxiv_id": "2406.17563v1",
    "title": "Multi-property Steering of Large Language Models with Dynamic Activation Composition",
    "authors": [
      "Daniel Scalena",
      "Gabriele Sarti",
      "Malvina Nissim"
    ],
    "abstract": "Activation steering methods were shown to be effective in conditioning\nlanguage model generation by additively intervening over models' intermediate\nrepresentations. However, the evaluation of these techniques has so far been\nlimited to single conditioning properties and synthetic settings. In this work,\nwe conduct a comprehensive evaluation of various activation steering\nstrategies, highlighting the property-dependent nature of optimal parameters to\nensure a robust effect throughout generation. To address this issue, we propose\nDynamic Activation Composition, an information-theoretic approach to modulate\nthe steering intensity of one or more properties throughout generation. Our\nexperiments on multi-property steering show that our method successfully\nmaintains high conditioning while minimizing the impact of conditioning on\ngeneration fluency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17563v1",
    "published_date": "2024-06-25 14:00:42 UTC",
    "updated_date": "2024-06-25 14:00:42 UTC"
  },
  {
    "arxiv_id": "2406.17828v1",
    "title": "Extreme Learning Machines for Fast Training of Click-Through Rate Prediction Models",
    "authors": [
      "Ergun Biçici"
    ],
    "abstract": "Extreme Learning Machines (ELM) provide a fast alternative to traditional\ngradient-based learning in neural networks, offering rapid training and robust\ngeneralization capabilities. Its theoretical basis shows its universal\napproximation capability. We explore the application of ELMs for the task of\nClick-Through Rate (CTR) prediction, which is largely unexplored by ELMs due to\nthe high dimensionality of the problem. We introduce an ELM-based model\nenhanced with embedding layers to improve the performance on CTR tasks, which\nis a novel addition to the field. Experimental results on benchmark datasets,\nincluding Avazu and Criteo, demonstrate that our proposed ELM with embeddings\nachieves competitive F1 results while significantly reducing training time\ncompared to state-of-the-art models such as Masknet. Our findings show that\nELMs can be useful for CTR prediction, especially when fast training is needed.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2; I.5.1"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 2 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.17828v1",
    "published_date": "2024-06-25 13:50:00 UTC",
    "updated_date": "2024-06-25 13:50:00 UTC"
  },
  {
    "arxiv_id": "2406.17542v3",
    "title": "CDQuant: Greedy Coordinate Descent for Accurate LLM Quantization",
    "authors": [
      "Pranav Ajit Nair",
      "Arun Sai Suggala"
    ],
    "abstract": "Large language models (LLMs) have recently demonstrated remarkable\nperformance across diverse language tasks. But their deployment is often\nconstrained by their substantial computational and storage requirements.\nQuantization has emerged as a key technique for addressing this challenge,\nenabling the compression of large models with minimal impact on performance.\nThe recent GPTQ algorithm, a post-training quantization (PTQ) method, has\nproven highly effective for compressing LLMs, sparking a wave of research that\nleverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the\nPTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ\nwith improved performance. CDQuant uses greedy coordinate descent to minimize\nthe layer-wise reconstruction loss to achieve high-quality quantized weights.\nOur algorithm is easy to implement and scales efficiently to models with\nhundreds of billions of parameters. We perform extensive evaluation on Gemma,\nand PaLM2 model families, and demonstrate that CDQuant consistently outperforms\nGPTQ in 2-4 bit weight quantization. Moreover, CDQuant improves the performance\nof state-of-the-art PTQ techniques such as QuIP and FrameQuant when used as a\nreplacement for their GPTQ component, resulting in further gains in quality.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17542v3",
    "published_date": "2024-06-25 13:29:14 UTC",
    "updated_date": "2024-10-22 18:51:01 UTC"
  },
  {
    "arxiv_id": "2406.17826v1",
    "title": "European Space Agency Benchmark for Anomaly Detection in Satellite Telemetry",
    "authors": [
      "Krzysztof Kotowski",
      "Christoph Haskamp",
      "Jacek Andrzejewski",
      "Bogdan Ruszczak",
      "Jakub Nalepa",
      "Daniel Lakey",
      "Peter Collins",
      "Aybike Kolmas",
      "Mauro Bartesaghi",
      "Jose Martinez-Heras",
      "Gabriele De Canio"
    ],
    "abstract": "Machine learning has vast potential to improve anomaly detection in satellite\ntelemetry which is a crucial task for spacecraft operations. This potential is\ncurrently hampered by a lack of comprehensible benchmarks for multivariate time\nseries anomaly detection, especially for the challenging case of satellite\ntelemetry. The European Space Agency Benchmark for Anomaly Detection in\nSatellite Telemetry (ESA-ADB) aims to address this challenge and establish a\nnew standard in the domain. It is a result of close cooperation between\nspacecraft operations engineers from the European Space Agency (ESA) and\nmachine learning experts. The newly introduced ESA Anomalies Dataset contains\nannotated real-life telemetry from three different ESA missions, out of which\ntwo are included in ESA-ADB. Results of typical anomaly detection algorithms\nassessed in our novel hierarchical evaluation pipeline show that new approaches\nare necessary to address operators' needs. All elements of ESA-ADB are publicly\navailable to ensure its full reproducibility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "87 pages, 24 figures, 19 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.17826v1",
    "published_date": "2024-06-25 13:23:37 UTC",
    "updated_date": "2024-06-25 13:23:37 UTC"
  },
  {
    "arxiv_id": "2406.17537v1",
    "title": "SincVAE: a New Approach to Improve Anomaly Detection on EEG Data Using SincNet and Variational Autoencoder",
    "authors": [
      "Andrea Pollastro",
      "Francesco Isgrò",
      "Roberto Prevete"
    ],
    "abstract": "Over the past few decades, electroencephalography (EEG) monitoring has become\na pivotal tool for diagnosing neurological disorders, particularly for\ndetecting seizures. Epilepsy, one of the most prevalent neurological diseases\nworldwide, affects approximately the 1 \\% of the population. These patients\nface significant risks, underscoring the need for reliable, continuous seizure\nmonitoring in daily life. Most of the techniques discussed in the literature\nrely on supervised Machine Learning (ML) methods. However, the challenge of\naccurately labeling variations in epileptic EEG waveforms complicates the use\nof these approaches. Additionally, the rarity of ictal events introduces an\nhigh imbalancing within the data, which could lead to poor prediction\nperformance in supervised learning approaches. Instead, a semi-supervised\napproach allows to train the model only on data not containing seizures, thus\navoiding the issues related to the data imbalancing. This work proposes a\nsemi-supervised approach for detecting epileptic seizures from EEG data,\nutilizing a novel Deep Learning-based method called SincVAE. This proposal\nincorporates the learning of an ad-hoc array of bandpass filter as a first\nlayer of a Variational Autoencoder (VAE), potentially eliminating the\npreprocessing stage where informative band frequencies are identified and\nisolated. Results indicate that SincVAE improves seizure detection in EEG data\nand is capable of identifying early seizures during the preictal stage as well\nas monitoring patients throughout the postictal stage.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17537v1",
    "published_date": "2024-06-25 13:21:01 UTC",
    "updated_date": "2024-06-25 13:21:01 UTC"
  },
  {
    "arxiv_id": "2406.17535v1",
    "title": "Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian Benchmark",
    "authors": [
      "Fabio Mercorio",
      "Mario Mezzanzanica",
      "Daniele Potertì",
      "Antonio Serino",
      "Andrea Seveso"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to generate and manipulate human language, highlighting\ntheir potential across various applications. Evaluating LLMs in languages other\nthan English is crucial for ensuring their linguistic versatility, cultural\nrelevance, and applicability in diverse global contexts, thus broadening their\nusability and effectiveness. We tackle this challenge by introducing a\nstructured benchmark using the INVALSI tests, a set of well-established\nassessments designed to measure educational competencies across Italy. Our\nstudy makes three primary contributions: Firstly, we adapt the INVALSI\nbenchmark for automated LLM evaluation, which involves rigorous adaptation of\nthe test format to suit automated processing while retaining the essence of the\noriginal tests. Secondly, we provide a detailed assessment of current LLMs,\noffering a crucial reference point for the academic community. Finally, we\nvisually compare the performance of these models against human results.\nAdditionally, researchers are invited to submit their models for ongoing\nevaluation, ensuring the benchmark remains a current and valuable resource.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17535v1",
    "published_date": "2024-06-25 13:20:08 UTC",
    "updated_date": "2024-06-25 13:20:08 UTC"
  },
  {
    "arxiv_id": "2406.17532v2",
    "title": "Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study",
    "authors": [
      "Keyu Wang",
      "Guilin Qi",
      "Jiaqi Li",
      "Songlin Zhai"
    ],
    "abstract": "Large language models (LLMs) have shown significant achievements in solving a\nwide range of tasks. Recently, LLMs' capability to store, retrieve and infer\nwith symbolic knowledge has drawn a great deal of attention, showing their\npotential to understand structured information. However, it is not yet known\nwhether LLMs can understand Description Logic (DL) ontologies. In this work, we\nempirically analyze the LLMs' capability of understanding DL-Lite ontologies\ncovering 6 representative tasks from syntactic and semantic aspects. With\nextensive experiments, we demonstrate both the effectiveness and limitations of\nLLMs in understanding DL-Lite ontologies. We find that LLMs can understand\nformal syntax and model-theoretic semantics of concepts and roles. However,\nLLMs struggle with understanding TBox NI transitivity and handling ontologies\nwith large ABoxes. We hope that our experiments and analyses provide more\ninsights into LLMs and inspire to build more faithful knowledge engineering\nsolutions.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17532v2",
    "published_date": "2024-06-25 13:16:34 UTC",
    "updated_date": "2024-10-10 09:03:07 UTC"
  },
  {
    "arxiv_id": "2406.17531v1",
    "title": "Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness",
    "authors": [
      "Lucrezia Grassi",
      "Carmine Tommaso Recchiuto",
      "Antonio Sgorbissa"
    ],
    "abstract": "This paper presents a system for diversity-aware autonomous conversation\nleveraging the capabilities of large language models (LLMs). The system adapts\nto diverse populations and individuals, considering factors like background,\npersonality, age, gender, and culture. The conversation flow is guided by the\nstructure of the system's pre-established knowledge base, while LLMs are tasked\nwith various functions, including generating diversity-aware sentences.\nAchieving diversity-awareness involves providing carefully crafted prompts to\nthe models, incorporating comprehensive information about users, conversation\nhistory, contextual details, and specific guidelines. To assess the system's\nperformance, we conducted both controlled and real-world experiments, measuring\na wide range of performance indicators.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 6 figures, 7 tables. This paper has been accepted for\n  publication at IEEE ROMAN 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.17531v1",
    "published_date": "2024-06-25 13:15:36 UTC",
    "updated_date": "2024-06-25 13:15:36 UTC"
  },
  {
    "arxiv_id": "2406.17523v3",
    "title": "On the consistency of hyper-parameter selection in value-based deep reinforcement learning",
    "authors": [
      "Johan Obando-Ceron",
      "João G. M. Araújo",
      "Aaron Courville",
      "Pablo Samuel Castro"
    ],
    "abstract": "Deep reinforcement learning (deep RL) has achieved tremendous success on\nvarious domains through a combination of algorithmic design and careful\nselection of hyper-parameters. Algorithmic improvements are often the result of\niterative enhancements built upon prior approaches, while hyper-parameter\nchoices are typically inherited from previous methods or fine-tuned\nspecifically for the proposed technique. Despite their crucial impact on\nperformance, hyper-parameter choices are frequently overshadowed by algorithmic\nadvancements. This paper conducts an extensive empirical study focusing on the\nreliability of hyper-parameter selection for value-based deep reinforcement\nlearning agents, including the introduction of a new score to quantify the\nconsistency and reliability of various hyper-parameters. Our findings not only\nhelp establish which hyper-parameters are most critical to tune, but also help\nclarify which tunings remain consistent across different training regimes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17523v3",
    "published_date": "2024-06-25 13:06:09 UTC",
    "updated_date": "2024-11-29 18:51:22 UTC"
  },
  {
    "arxiv_id": "2406.17518v2",
    "title": "Enhancing Explainability of Knowledge Learning Paths: Causal Knowledge Networks",
    "authors": [
      "Yuang Wei",
      "Yizhou Zhou",
      "Yuan-Hao Jiang",
      "Bo Jiang"
    ],
    "abstract": "A reliable knowledge structure is a prerequisite for building effective\nadaptive learning systems and intelligent tutoring systems. Pursuing an\nexplainable and trustworthy knowledge structure, we propose a method for\nconstructing causal knowledge networks. This approach leverages Bayesian\nnetworks as a foundation and incorporates causal relationship analysis to\nderive a causal network. Additionally, we introduce a dependable\nknowledge-learning path recommendation technique built upon this framework,\nimproving teaching and learning quality while maintaining transparency in the\ndecision-making process.",
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 3 figures, Educational Data Mining 2024, Human-Centric\n  eXplainable AI in Education",
    "pdf_url": "http://arxiv.org/pdf/2406.17518v2",
    "published_date": "2024-06-25 12:59:20 UTC",
    "updated_date": "2024-06-26 01:25:44 UTC"
  },
  {
    "arxiv_id": "2406.17517v1",
    "title": "Preserving Node Distinctness in Graph Autoencoders via Similarity Distillation",
    "authors": [
      "Ge Chen",
      "Yulan Hu",
      "Sheng Ouyang",
      "Yong Liu",
      "Cuicui Luo"
    ],
    "abstract": "Graph autoencoders (GAEs), as a kind of generative self-supervised learning\napproach, have shown great potential in recent years. GAEs typically rely on\ndistance-based criteria, such as mean-square-error (MSE), to reconstruct the\ninput graph. However, relying solely on a single reconstruction criterion may\nlead to a loss of distinctiveness in the reconstructed graph, causing nodes to\ncollapse into similar representations and resulting in sub-optimal performance.\nTo address this issue, we have developed a simple yet effective strategy to\npreserve the necessary distinctness in the reconstructed graph. Inspired by the\nknowledge distillation technique, we found that the dual encoder-decoder\narchitecture of GAEs can be viewed as a teacher-student relationship.\nTherefore, we propose transferring the knowledge of distinctness from the raw\ngraph to the reconstructed graph, achieved through a simple KL constraint.\nSpecifically, we compute pairwise node similarity scores in the raw graph and\nreconstructed graph. During the training process, the KL constraint is\noptimized alongside the reconstruction criterion. We conducted extensive\nexperiments across three types of graph tasks, demonstrating the effectiveness\nand generality of our strategy. This indicates that the proposed approach can\nbe employed as a plug-and-play method to avoid vague reconstructions and\nenhance overall performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17517v1",
    "published_date": "2024-06-25 12:54:35 UTC",
    "updated_date": "2024-06-25 12:54:35 UTC"
  },
  {
    "arxiv_id": "2406.17513v2",
    "title": "Benchmarking Mental State Representations in Language Models",
    "authors": [
      "Matteo Bortoletto",
      "Constantin Ruhdorfer",
      "Lei Shi",
      "Andreas Bulling"
    ],
    "abstract": "While numerous works have assessed the generative performance of language\nmodels (LMs) on tasks requiring Theory of Mind reasoning, research into the\nmodels' internal representation of mental states remains limited. Recent work\nhas used probing to demonstrate that LMs can represent beliefs of themselves\nand others. However, these claims are accompanied by limited evaluation, making\nit difficult to assess how mental state representations are affected by model\ndesign and training choices. We report an extensive benchmark with various LM\ntypes with different model sizes, fine-tuning approaches, and prompt designs to\nstudy the robustness of mental state representations and memorisation issues\nwithin the probes. Our results show that the quality of models' internal\nrepresentations of the beliefs of others increases with model size and, more\ncrucially, with fine-tuning. We are the first to study how prompt variations\nimpact probing performance on theory of mind tasks. We demonstrate that models'\nrepresentations are sensitive to prompt variations, even when such variations\nshould be beneficial. Finally, we complement previous activation editing\nexperiments on Theory of Mind tasks and show that it is possible to improve\nmodels' reasoning performance by steering their activations without the need to\ntrain any probe.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICML 2024 Workshop on Mechanistic Interpretability",
    "pdf_url": "http://arxiv.org/pdf/2406.17513v2",
    "published_date": "2024-06-25 12:51:06 UTC",
    "updated_date": "2024-07-01 06:48:34 UTC"
  },
  {
    "arxiv_id": "2407.09531v1",
    "title": "UAV Networks Surveillance Implementing an Effective Load-Aware Multipath Routing Protocol (ELAMRP)",
    "authors": [
      "Raja Vavekanand",
      "Kira Sam",
      "Vijay Singh"
    ],
    "abstract": "In this work uses innovative multi-channel load-sensing techniques to deploy\nunmanned aerial vehicles (UAVs) for surveillance. The research aims to improve\nthe quality of data transmission methods and improve the efficiency and\nreliability of surveillance systems by exploiting the mobility and adaptability\nof UAVs does the proposed protocol intelligently distribute network traffic\nacross multiple channels, considering the load of each channel, While\naddressing challenges such as load balancing, this study investigates the\neffectiveness of the protocol by simulations or practical tests on The expected\nresults have improved UAV-based surveillance systems, more flexible and\nefficient networks for applications such as security, emergency response and\nthe environment alignment of monitoring -Offering infrastructures, which\ncontribute to efficient and reliable monitoring solutions.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "06 pages, 07 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09531v1",
    "published_date": "2024-06-25 12:12:54 UTC",
    "updated_date": "2024-06-25 12:12:54 UTC"
  },
  {
    "arxiv_id": "2406.17474v1",
    "title": "Transformer-based Named Entity Recognition with Combined Data Representation",
    "authors": [
      "Michał Marcińczuk"
    ],
    "abstract": "This study examines transformer-based models and their effectiveness in named\nentity recognition tasks. The study investigates data representation\nstrategies, including single, merged, and context, which respectively use one\nsentence, multiple sentences, and sentences joined with attention to context\nper vector. Analysis shows that training models with a single strategy may lead\nto poor performance on different data representations. To address this\nlimitation, the study proposes a combined training procedure that utilizes all\nthree strategies to improve model stability and adaptability. The results of\nthis approach are presented and discussed for four languages (English, Polish,\nCzech, and German) across various datasets, demonstrating the effectiveness of\nthe combined strategy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.17474v1",
    "published_date": "2024-06-25 11:41:16 UTC",
    "updated_date": "2024-06-25 11:41:16 UTC"
  },
  {
    "arxiv_id": "2406.17473v1",
    "title": "TSynD: Targeted Synthetic Data Generation for Enhanced Medical Image Classification",
    "authors": [
      "Joshua Niemeijer",
      "Jan Ehrhardt",
      "Hristina Uzunova",
      "Heinz Handels"
    ],
    "abstract": "The usage of medical image data for the training of large-scale machine\nlearning approaches is particularly challenging due to its scarce availability\nand the costly generation of data annotations, typically requiring the\nengagement of medical professionals. The rapid development of generative models\nallows towards tackling this problem by leveraging large amounts of realistic\nsynthetically generated data for the training process. However, randomly\nchoosing synthetic samples, might not be an optimal strategy.\n  In this work, we investigate the targeted generation of synthetic training\ndata, in order to improve the accuracy and robustness of image classification.\nTherefore, our approach aims to guide the generative model to synthesize data\nwith high epistemic uncertainty, since large measures of epistemic uncertainty\nindicate underrepresented data points in the training set. During the image\ngeneration we feed images reconstructed by an auto encoder into the classifier\nand compute the mutual information over the class-probability distribution as a\nmeasure for uncertainty.We alter the feature space of the autoencoder through\nan optimization process with the objective of maximizing the classifier\nuncertainty on the decoded image. By training on such data we improve the\nperformance and robustness against test time data augmentations and adversarial\nattacks on several classifications tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17473v1",
    "published_date": "2024-06-25 11:38:46 UTC",
    "updated_date": "2024-06-25 11:38:46 UTC"
  },
  {
    "arxiv_id": "2406.17470v1",
    "title": "Dynamic Scheduling for Vehicle-to-Vehicle Communications Enhanced Federated Learning",
    "authors": [
      "Jintao Yan",
      "Tan Chen",
      "Yuxuan Sun",
      "Zhaojun Nan",
      "Sheng Zhou",
      "Zhisheng Niu"
    ],
    "abstract": "Leveraging the computing and sensing capabilities of vehicles, vehicular\nfederated learning (VFL) has been applied to edge training for connected\nvehicles. The dynamic and interconnected nature of vehicular networks presents\nunique opportunities to harness direct vehicle-to-vehicle (V2V) communications,\nenhancing VFL training efficiency. In this paper, we formulate a stochastic\noptimization problem to optimize the VFL training performance, considering the\nenergy constraints and mobility of vehicles, and propose a V2V-enhanced dynamic\nscheduling (VEDS) algorithm to solve it. The model aggregation requirements of\nVFL and the limited transmission time due to mobility result in a stepwise\nobjective function, which presents challenges in solving the problem. We thus\npropose a derivative-based drift-plus-penalty method to convert the long-term\nstochastic optimization problem to an online mixed integer nonlinear\nprogramming (MINLP) problem, and provide a theoretical analysis to bound the\nperformance gap between the online solution and the offline optimal solution.\nFurther analysis of the scheduling priority reduces the original problem into a\nset of convex optimization problems, which are efficiently solved using the\ninterior-point method. Experimental results demonstrate that compared with the\nstate-of-the-art benchmarks, the proposed algorithm enhances the image\nclassification accuracy on the CIFAR-10 dataset by 3.18% and reduces the\naverage displacement errors on the Argoverse trajectory prediction dataset by\n10.21%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2406.17470v1",
    "published_date": "2024-06-25 11:15:53 UTC",
    "updated_date": "2024-06-25 11:15:53 UTC"
  },
  {
    "arxiv_id": "2406.17465v2",
    "title": "Enhancing Tool Retrieval with Iterative Feedback from Large Language Models",
    "authors": [
      "Qiancheng Xu",
      "Yongqi Li",
      "Heming Xia",
      "Wenjie Li"
    ],
    "abstract": "Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17465v2",
    "published_date": "2024-06-25 11:12:01 UTC",
    "updated_date": "2024-09-29 15:26:59 UTC"
  },
  {
    "arxiv_id": "2406.17462v2",
    "title": "EvolvED: Evolutionary Embeddings to Understand the Generation Process of Diffusion Models",
    "authors": [
      "Vidya Prasad",
      "Hans van Gorp",
      "Christina Humer",
      "Ruud J. G. van Sloun",
      "Anna Vilanova",
      "Nicola Pezzotti"
    ],
    "abstract": "Diffusion models, widely used in image generation, rely on iterative\nrefinement to generate images from noise. Understanding this data evolution is\nimportant for model development and interpretability, yet challenging due to\nits high-dimensional, iterative nature. Prior works often focus on static or\ninstance-level analyses, missing the iterative and holistic aspects of the\ngenerative path. While dimensionality reduction can visualize image evolution\nfor few instances, it does preserve the iterative structure. To address these\ngaps, we introduce EvolvED, a method that presents a holistic view of the\niterative generative process in diffusion models. EvolvED goes beyond instance\nexploration by leveraging predefined research questions to streamline\ngenerative space exploration. Tailored prompts aligned with these questions are\nused to extract intermediate images, preserving iterative context. Targeted\nfeature extractors trace the evolution of key image attribute evolution,\naddressing the complexity of high-dimensional outputs. Central to EvolvED is a\nnovel evolutionary embedding algorithm that encodes iterative steps while\nmaintaining semantic relations. It enhances the visualization of data evolution\nby clustering semantically similar elements within each iteration with t-SNE,\ngrouping elements by iteration, and aligning an instance's elements across\niterations. We present rectilinear and radial layouts to represent iterations\nand support exploration. We apply EvolvED to diffusion models like GLIDE and\nStable Diffusion, demonstrating its ability to provide valuable insights into\nthe generative process.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17462v2",
    "published_date": "2024-06-25 11:05:26 UTC",
    "updated_date": "2024-12-11 09:23:17 UTC"
  },
  {
    "arxiv_id": "2407.00875v1",
    "title": "MoE-CT: A Novel Approach For Large Language Models Training With Resistance To Catastrophic Forgetting",
    "authors": [
      "Tianhao Li",
      "Shangjie Li",
      "Binbin Xie",
      "Deyi Xiong",
      "Baosong Yang"
    ],
    "abstract": "The advent of large language models (LLMs) has predominantly catered to\nhigh-resource languages, leaving a disparity in performance for low-resource\nlanguages. Conventional Continual Training (CT) approaches to bridge this gap\noften undermine a model's original linguistic proficiency when expanding to\nmultilingual contexts. Addressing this issue, we introduce a novel MoE-CT\narchitecture, a paradigm that innovatively separates the base model's learning\nfrom the multilingual expansion process. Our design freezes the original LLM\nparameters, thus safeguarding its performance in high-resource languages, while\nan appended MoE module, trained on diverse language datasets, augments\nlow-resource language proficiency. Our approach significantly outperforms\nconventional CT methods, as evidenced by our experiments, which show marked\nimprovements in multilingual benchmarks without sacrificing the model's\noriginal language performance. Moreover, our MoE-CT framework demonstrates\nenhanced resistance to forgetting and superior transfer learning capabilities.\nBy preserving the base model's integrity and focusing on strategic parameter\nexpansion, our methodology advances multilingual language modeling and\nrepresents a significant step forward for low-resource language inclusion in\nLLMs, indicating a fruitful direction for future research in language\ntechnologies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.00875v1",
    "published_date": "2024-06-25 11:03:45 UTC",
    "updated_date": "2024-06-25 11:03:45 UTC"
  },
  {
    "arxiv_id": "2406.17456v1",
    "title": "Improving Grammatical Error Correction via Contextual Data Augmentation",
    "authors": [
      "Yixuan Wang",
      "Baoxin Wang",
      "Yijun Liu",
      "Qingfu Zhu",
      "Dayong Wu",
      "Wanxiang Che"
    ],
    "abstract": "Nowadays, data augmentation through synthetic data has been widely used in\nthe field of Grammatical Error Correction (GEC) to alleviate the problem of\ndata scarcity. However, these synthetic data are mainly used in the\npre-training phase rather than the data-limited fine-tuning phase due to\ninconsistent error distribution and noisy labels. In this paper, we propose a\nsynthetic data construction method based on contextual augmentation, which can\nensure an efficient augmentation of the original data with a more consistent\nerror distribution. Specifically, we combine rule-based substitution with\nmodel-based generation, using the generative model to generate a richer context\nfor the extracted error patterns. Besides, we also propose a relabeling-based\ndata cleaning method to mitigate the effects of noisy labels in synthetic data.\nExperiments on CoNLL14 and BEA19-Test show that our proposed augmentation\nmethod consistently and substantially outperforms strong baselines and achieves\nthe state-of-the-art level with only a few synthetic data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as Findings of ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.17456v1",
    "published_date": "2024-06-25 10:49:56 UTC",
    "updated_date": "2024-06-25 10:49:56 UTC"
  },
  {
    "arxiv_id": "2406.17450v1",
    "title": "Pseudo Labelling for Enhanced Masked Autoencoders",
    "authors": [
      "Srinivasa Rao Nandam",
      "Sara Atito",
      "Zhenhua Feng",
      "Josef Kittler",
      "Muhammad Awais"
    ],
    "abstract": "Masked Image Modeling (MIM)-based models, such as SdAE, CAE, GreenMIM, and\nMixAE, have explored different strategies to enhance the performance of Masked\nAutoencoders (MAE) by modifying prediction, loss functions, or incorporating\nadditional architectural components. In this paper, we propose an enhanced\napproach that boosts MAE performance by integrating pseudo labelling for both\nclass and data tokens, alongside replacing the traditional pixel-level\nreconstruction with token-level reconstruction. This strategy uses cluster\nassignments as pseudo labels to promote instance-level discrimination within\nthe network, while token reconstruction requires generation of discrete tokens\nencapturing local context. The targets for pseudo labelling and reconstruction\nneeds to be generated by a teacher network. To disentangle the generation of\ntarget pseudo labels and the reconstruction of the token features, we decouple\nthe teacher into two distinct models, where one serves as a labelling teacher\nand the other as a reconstruction teacher. This separation proves empirically\nsuperior to a single teacher, while having negligible impact on throughput and\nmemory consumption. Incorporating pseudo-labelling as an auxiliary task has\ndemonstrated notable improvements in ImageNet-1K and other downstream tasks,\nincluding classification, semantic segmentation, and detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17450v1",
    "published_date": "2024-06-25 10:41:45 UTC",
    "updated_date": "2024-06-25 10:41:45 UTC"
  },
  {
    "arxiv_id": "2407.11000v1",
    "title": "Autonomous Prompt Engineering in Large Language Models",
    "authors": [
      "Daan Kepel",
      "Konstantina Valogianni"
    ],
    "abstract": "Prompt engineering is a crucial yet challenging task for optimizing the\nperformance of large language models (LLMs) on customized tasks. This\npioneering research introduces the Automatic Prompt Engineering Toolbox (APET),\nwhich enables GPT-4 to autonomously apply prompt engineering techniques. By\nleveraging sophisticated strategies such as Expert Prompting, Chain of Thought,\nand Tree of Thoughts, APET empowers GPT-4 to dynamically optimize prompts,\nresulting in substantial improvements in tasks like Word Sorting (4.4%\nincrease) and Geometric Shapes (6.8% increase). Despite encountering challenges\nin complex tasks such as Checkmate in One (-14.8%), these findings demonstrate\nthe transformative potential of APET in automating complex prompt optimization\nprocesses without the use of external data. Overall, this research represents a\nsignificant leap in AI development, presenting a robust framework for future\ninnovations in autonomous AI systems and highlighting the ability of GPT-4 to\nbring prompt engineering theory to practice. It establishes a foundation for\nenhancing performance in complex task performance and broadening the practical\napplications of these techniques in real-world scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11000v1",
    "published_date": "2024-06-25 10:14:44 UTC",
    "updated_date": "2024-06-25 10:14:44 UTC"
  },
  {
    "arxiv_id": "2407.10999v1",
    "title": "TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot",
    "authors": [
      "Kaiqi Zhang",
      "Shuai Yuan",
      "Honghan Zhao"
    ],
    "abstract": "With the rapid development of large language models (LLM), the evaluation of\nLLM becomes increasingly important. Measuring text generation tasks such as\nsummarization and article creation is very difficult. Especially in specific\napplication domains (e.g., to-business or to-customer service), in-house\nevaluation criteria have to meet not only general standards (correctness,\nhelpfulness and creativity, etc.) but also specific needs of customers and\nbusiness security requirements at the same time, making the evaluation more\ndifficult. So far, the evaluation of LLM in business scenarios has mainly\nrelied on manual, which is expensive and time-consuming. In this paper, we\npropose a model-based evaluation method: TALEC, which allows users to flexibly\nset their own evaluation criteria, and uses in-context learning (ICL) to teach\njudge model these in-house criteria. In addition, we try combining zero-shot\nand few-shot to make the judge model focus on more information. We also propose\na prompt paradigm and an engineering approach to adjust and iterate the shots\n,helping judge model to better understand the complex criteria. We then compare\nfine-tuning with ICL, finding that fine-tuning can be replaced by ICL. TALEC\ndemonstrates a strong capability to accurately reflect human preferences and\nachieves a correlation of over 80% with human judgments, outperforming even the\ninter-human correlation in some tasks. The code is released in\nhttps://github.com/zlkqz/auto_eval",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10999v1",
    "published_date": "2024-06-25 10:02:42 UTC",
    "updated_date": "2024-06-25 10:02:42 UTC"
  },
  {
    "arxiv_id": "2406.17425v1",
    "title": "CuDA2: An approach for Incorporating Traitor Agents into Cooperative Multi-Agent Systems",
    "authors": [
      "Zhen Chen",
      "Yong Liao",
      "Youpeng Zhao",
      "Zipeng Dai",
      "Jian Zhao"
    ],
    "abstract": "Cooperative Multi-Agent Reinforcement Learning (CMARL) strategies are well\nknown to be vulnerable to adversarial perturbations. Previous works on\nadversarial attacks have primarily focused on white-box attacks that directly\nperturb the states or actions of victim agents, often in scenarios with a\nlimited number of attacks. However, gaining complete access to victim agents in\nreal-world environments is exceedingly difficult. To create more realistic\nadversarial attacks, we introduce a novel method that involves injecting\ntraitor agents into the CMARL system. We model this problem as a Traitor Markov\nDecision Process (TMDP), where traitors cannot directly attack the victim\nagents but can influence their formation or positioning through collisions. In\nTMDP, traitors are trained using the same MARL algorithm as the victim agents,\nwith their reward function set as the negative of the victim agents' reward.\nDespite this, the training efficiency for traitors remains low because it is\nchallenging for them to directly associate their actions with the victim\nagents' rewards. To address this issue, we propose the Curiosity-Driven\nAdversarial Attack (CuDA2) framework. CuDA2 enhances the efficiency and\naggressiveness of attacks on the specified victim agents' policies while\nmaintaining the optimal policy invariance of the traitors. Specifically, we\nemploy a pre-trained Random Network Distillation (RND) module, where the extra\nreward generated by the RND module encourages traitors to explore states\nunencountered by the victim agents. Extensive experiments on various scenarios\nfrom SMAC demonstrate that our CuDA2 framework offers comparable or superior\nadversarial attack capabilities compared to other baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17425v1",
    "published_date": "2024-06-25 09:59:31 UTC",
    "updated_date": "2024-06-25 09:59:31 UTC"
  },
  {
    "arxiv_id": "2406.17419v2",
    "title": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA",
    "authors": [
      "Minzheng Wang",
      "Longze Chen",
      "Cheng Fu",
      "Shengyi Liao",
      "Xinghua Zhang",
      "Bingli Wu",
      "Haiyang Yu",
      "Nan Xu",
      "Lei Zhang",
      "Run Luo",
      "Yunshui Li",
      "Min Yang",
      "Fei Huang",
      "Yongbin Li"
    ],
    "abstract": "Long-context modeling capabilities have garnered widespread attention,\nleading to the emergence of Large Language Models (LLMs) with ultra-context\nwindows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually\ncatching up. However, existing benchmarks employ irrelevant noise texts to\nartificially extend the length of test cases, diverging from the real-world\nscenarios of long-context applications. To bridge this gap, we propose a novel\nlong-context benchmark, Loong, aligning with realistic scenarios through\nextended multi-document question answering (QA). Unlike typical document QA, in\nLoong's test cases, each document is relevant to the final answer, ignoring any\ndocument will lead to the failure of the answer. Furthermore, Loong introduces\nfour types of tasks with a range of context lengths: Spotlight Locating,\nComparison, Clustering, and Chain of Reasoning, to facilitate a more realistic\nand comprehensive evaluation of long-context understanding. Extensive\nexperiments indicate that existing long-context language models still exhibit\nconsiderable potential for enhancement. Retrieval augmented generation (RAG)\nachieves poor performance, demonstrating that Loong can reliably assess the\nmodel's long-context modeling capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Main. We release our code and data publicly at\n  https://github.com/MozerWang/Loong",
    "pdf_url": "http://arxiv.org/pdf/2406.17419v2",
    "published_date": "2024-06-25 09:42:56 UTC",
    "updated_date": "2024-10-03 06:03:14 UTC"
  },
  {
    "arxiv_id": "2406.17418v1",
    "title": "SE-VGAE: Unsupervised Disentangled Representation Learning for Interpretable Architectural Layout Design Graph Generation",
    "authors": [
      "Jielin Chen",
      "Rudi Stouffs"
    ],
    "abstract": "Despite the suitability of graphs for capturing the relational structures\ninherent in architectural layout designs, there is a notable dearth of research\non interpreting architectural design space using graph-based representation\nlearning and exploring architectural design graph generation. Concurrently,\ndisentangled representation learning in graph generation faces challenges such\nas node permutation invariance and representation expressiveness. To address\nthese challenges, we introduce an unsupervised disentangled representation\nlearning framework, Style-based Edge-augmented Variational Graph Auto-Encoder\n(SE-VGAE), aiming to generate architectural layout in the form of attributed\nadjacency multi-graphs while prioritizing representation disentanglement. The\nframework is designed with three alternative pipelines, each integrating a\ntransformer-based edge-augmented encoder, a latent space disentanglement\nmodule, and a style-based decoder. These components collectively facilitate the\ndecomposition of latent factors influencing architectural layout graph\ngeneration, enhancing generation fidelity and diversity. We also provide\ninsights into optimizing the framework by systematically exploring graph\nfeature augmentation schemes and evaluating their effectiveness for\ndisentangling architectural layout representation through extensive\nexperiments. Additionally, we contribute a new benchmark large-scale\narchitectural layout graph dataset extracted from real-world floor plan images\nto facilitate the exploration of graph data-based architectural design\nrepresentation space interpretation. This study pioneered disentangled\nrepresentation learning for the architectural layout graph generation. The code\nand dataset of this study will be open-sourced.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17418v1",
    "published_date": "2024-06-25 09:40:47 UTC",
    "updated_date": "2024-06-25 09:40:47 UTC"
  },
  {
    "arxiv_id": "2406.17415v3",
    "title": "Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels",
    "authors": [
      "Razvan-Gabriel Dumitru",
      "Vikas Yadav",
      "Rishabh Maheshwary",
      "Paul-Ioan Clotan",
      "Sathwik Tejaswi Madhusudhan",
      "Mihai Surdeanu"
    ],
    "abstract": "We present a simple meta quantization approach that quantizes different\nlayers of a large language model (LLM) at different bit levels, and is\nindependent of the underlying quantization technique. Specifically, we quantize\nthe most important layers to higher bit precision and less important layers to\nlower bits. We propose two effective strategies to measure the importance of\nlayers within LLMs: the first measures the importance of a layer based on how\ndifferent its output embeddings are from the input embeddings (higher is\nbetter); the second estimates the importance of a layer using the number of\nlayer weights that are much larger than average (smaller is better). We show\nthat quantizing different layers at varying bits according to our importance\nscores results in minimal performance drop with a far more compressed model\nsize. Finally, we present several practical key takeaways from our variable\nlayer-wise quantization experiments: (a) LLM performance under variable\nquantization remains close to the original model until 25-50% of layers are\nmoved in lower quantization using our proposed ordering but only until 5-10% if\nmoved using no specific ordering; (b) Adding layer importance to inherently\ndynamic quantization techniques can further improve their performance, showing\nthat our approach is complementary to other dynamic quantization methods; (c)\nQuantizing LLMs to lower bits performs substantially better than pruning unless\nextreme quantization (2-bit) is used; and (d) Layer-wise quantization to lower\nbits works better in the case of larger LLMs with more layers compared to\nsmaller LLMs with fewer layers. Our code is publicly available at\nhttps://github.com/RazvanDu/LayerwiseQuant/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7; I.2.0"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17415v3",
    "published_date": "2024-06-25 09:37:15 UTC",
    "updated_date": "2024-10-28 08:59:01 UTC"
  },
  {
    "arxiv_id": "2407.01596v1",
    "title": "Maze Discovery using Multiple Robots via Federated Learning",
    "authors": [
      "Kalpana Ranasinghe",
      "H. P. Madushanka",
      "Rafaela Scaciota",
      "Sumudu Samarakoon",
      "Mehdi Bennis"
    ],
    "abstract": "This work presents a use case of federated learning (FL) applied to\ndiscovering a maze with LiDAR sensors-equipped robots. Goal here is to train\nclassification models to accurately identify the shapes of grid areas within\ntwo different square mazes made up with irregular shaped walls. Due to the use\nof different shapes for the walls, a classification model trained in one maze\nthat captures its structure does not generalize for the other. This issue is\nresolved by adopting FL framework between the robots that explore only one maze\nso that the collective knowledge allows them to operate accurately in the\nunseen maze. This illustrates the effectiveness of FL in real-world\napplications in terms of enhancing classification accuracy and robustness in\nmaze discovery tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in ISCC 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2407.01596v1",
    "published_date": "2024-06-25 09:34:11 UTC",
    "updated_date": "2024-06-25 09:34:11 UTC"
  },
  {
    "arxiv_id": "2406.17822v1",
    "title": "AI for the prediction of early stages of Alzheimer's disease from neuroimaging biomarkers -- A narrative review of a growing field",
    "authors": [
      "Thorsten Rudroff",
      "Oona Rainio",
      "Riku Klén"
    ],
    "abstract": "Objectives: The objectives of this narrative review are to summarize the\ncurrent state of AI applications in neuroimaging for early Alzheimer's disease\n(AD) prediction and to highlight the potential of AI techniques in improving\nearly AD diagnosis, prognosis, and management.\n  Methods: We conducted a narrative review of studies using AI techniques\napplied to neuroimaging data for early AD prediction. We examined\nsingle-modality studies using structural MRI and PET imaging, as well as\nmulti-modality studies integrating multiple neuroimaging techniques and\nbiomarkers. Furthermore, they reviewed longitudinal studies that model AD\nprogression and identify individuals at risk of rapid decline.\n  Results: Single-modality studies using structural MRI and PET imaging have\ndemonstrated high accuracy in classifying AD and predicting progression from\nmild cognitive impairment (MCI) to AD. Multi-modality studies, integrating\nmultiple neuroimaging techniques and biomarkers, have shown improved\nperformance and robustness compared to single-modality approaches. Longitudinal\nstudies have highlighted the value of AI in modeling AD progression and\nidentifying individuals at risk of rapid decline. However, challenges remain in\ndata standardization, model interpretability, generalizability, clinical\nintegration, and ethical considerations.\n  Conclusion: AI techniques applied to neuroimaging data have the potential to\nimprove early AD diagnosis, prognosis, and management. Addressing challenges\nrelated to data standardization, model interpretability, generalizability,\nclinical integration, and ethical considerations is crucial for realizing the\nfull potential of AI in AD research and clinical practice. Collaborative\nefforts among researchers, clinicians, and regulatory agencies are needed to\ndevelop reliable, robust, and ethical AI tools that can benefit AD patients and\nsociety.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.17822v1",
    "published_date": "2024-06-25 09:22:53 UTC",
    "updated_date": "2024-06-25 09:22:53 UTC"
  },
  {
    "arxiv_id": "2406.17386v1",
    "title": "Double Momentum Method for Lower-Level Constrained Bilevel Optimization",
    "authors": [
      "Wanli Shi",
      "Yi Chang",
      "Bin Gu"
    ],
    "abstract": "Bilevel optimization (BO) has recently gained prominence in many machine\nlearning applications due to its ability to capture the nested structure\ninherent in these problems. Recently, many hypergradient methods have been\nproposed as effective solutions for solving large-scale problems. However,\ncurrent hypergradient methods for the lower-level constrained bilevel\noptimization (LCBO) problems need very restrictive assumptions, namely, where\noptimality conditions satisfy the differentiability and invertibility\nconditions and lack a solid analysis of the convergence rate. What's worse,\nexisting methods require either double-loop updates, which are sometimes less\nefficient. To solve this problem, in this paper, we propose a new hypergradient\nof LCBO leveraging the theory of nonsmooth implicit function theorem instead of\nusing the restrive assumptions. In addition, we propose a \\textit{single-loop\nsingle-timescale} algorithm based on the double-momentum method and adaptive\nstep size method and prove it can return a $(\\delta, \\epsilon)$-stationary\npoint with $\\tilde{\\mathcal{O}}(d_2^2\\epsilon^{-4})$ iterations. Experiments on\ntwo applications demonstrate the effectiveness of our proposed method.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "27pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.17386v1",
    "published_date": "2024-06-25 09:05:22 UTC",
    "updated_date": "2024-06-25 09:05:22 UTC"
  },
  {
    "arxiv_id": "2407.09530v1",
    "title": "Optimization of Autonomous Driving Image Detection Based on RFAConv and Triplet Attention",
    "authors": [
      "Zhipeng Ling",
      "Qi Xin",
      "Yiyu Lin",
      "Guangze Su",
      "Zuwei Shui"
    ],
    "abstract": "YOLOv8 plays a crucial role in the realm of autonomous driving, owing to its\nhigh-speed target detection, precise identification and positioning, and\nversatile compatibility across multiple platforms. By processing video streams\nor images in real-time, YOLOv8 rapidly and accurately identifies obstacles such\nas vehicles and pedestrians on roadways, offering essential visual data for\nautonomous driving systems. Moreover, YOLOv8 supports various tasks including\ninstance segmentation, image classification, and attitude estimation, thereby\nproviding comprehensive visual perception for autonomous driving, ultimately\nenhancing driving safety and efficiency. Recognizing the significance of object\ndetection in autonomous driving scenarios and the challenges faced by existing\nmethods, this paper proposes a holistic approach to enhance the YOLOv8 model.\nThe study introduces two pivotal modifications: the C2f_RFAConv module and the\nTriplet Attention mechanism. Firstly, the proposed modifications are elaborated\nupon in the methodological section. The C2f_RFAConv module replaces the\noriginal module to enhance feature extraction efficiency, while the Triplet\nAttention mechanism enhances feature focus. Subsequently, the experimental\nprocedure delineates the training and evaluation process, encompassing training\nthe original YOLOv8, integrating modified modules, and assessing performance\nimprovements using metrics and PR curves. The results demonstrate the efficacy\nof the modifications, with the improved YOLOv8 model exhibiting significant\nperformance enhancements, including increased MAP values and improvements in PR\ncurves. Lastly, the analysis section elucidates the results and attributes the\nperformance improvements to the introduced modules. C2f_RFAConv enhances\nfeature extraction efficiency, while Triplet Attention improves feature focus\nfor enhanced target detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.09530v1",
    "published_date": "2024-06-25 08:59:33 UTC",
    "updated_date": "2024-06-25 08:59:33 UTC"
  },
  {
    "arxiv_id": "2406.17376v1",
    "title": "Temporal-Channel Modeling in Multi-head Self-Attention for Synthetic Speech Detection",
    "authors": [
      "Duc-Tuan Truong",
      "Ruijie Tao",
      "Tuan Nguyen",
      "Hieu-Thi Luong",
      "Kong Aik Lee",
      "Eng Siong Chng"
    ],
    "abstract": "Recent synthetic speech detectors leveraging the Transformer model have\nsuperior performance compared to the convolutional neural network counterparts.\nThis improvement could be due to the powerful modeling ability of the\nmulti-head self-attention (MHSA) in the Transformer model, which learns the\ntemporal relationship of each input token. However, artifacts of synthetic\nspeech can be located in specific regions of both frequency channels and\ntemporal segments, while MHSA neglects this temporal-channel dependency of the\ninput sequence. In this work, we proposed a Temporal-Channel Modeling (TCM)\nmodule to enhance MHSA's capability for capturing temporal-channel\ndependencies. Experimental results on the ASVspoof 2021 show that with only\n0.03M additional parameters, the TCM module can outperform the state-of-the-art\nsystem by 9.25% in EER. Further ablation study reveals that utilizing both\ntemporal and channel information yields the most improvement for detecting\nsynthetic speech.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by INTERSPEECH 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.17376v1",
    "published_date": "2024-06-25 08:50:43 UTC",
    "updated_date": "2024-06-25 08:50:43 UTC"
  },
  {
    "arxiv_id": "2407.00088v2",
    "title": "T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge",
    "authors": [
      "Jianyu Wei",
      "Shijie Cao",
      "Ting Cao",
      "Lingxiao Ma",
      "Lei Wang",
      "Yanyong Zhang",
      "Mao Yang"
    ],
    "abstract": "The deployment of Large Language Models (LLMs) on edge devices is\nincreasingly important to enhance on-device intelligence. Weight quantization\nis crucial for reducing the memory footprint of LLMs on devices. However,\nlow-bit LLMs necessitate mixed precision matrix multiplication (mpGEMM) of low\nprecision weights and high precision activations during inference. Existing\nsystems, lacking native support for mpGEMM, resort to dequantize weights for\nhigh precision computation. Such an indirect way can lead to a significant\ninference overhead.\n  In this paper, we introduce T-MAC, an innovative lookup table(LUT)-based\nmethod designed for efficient low-bit LLM (i.e., weight-quantized LLM)\ninference on CPUs. T-MAC directly supports mpGEMM without dequantization, while\nsimultaneously eliminating multiplications and reducing additions required.\nSpecifically, T-MAC transforms the traditional data-type-centric multiplication\nto bit-wise table lookup, and enables a unified and scalable mpGEMM solution.\n  Our LUT-based kernels scale linearly to the weight bit-width. Evaluated on\nlow-bit Llama and BitNet models, T-MAC demonstrates up to 4x increase in\nthroughput and 70% reduction in energy consumption compared to llama.cpp. For\nBitNet-b1.58-3B, T-MAC delivers a token generation throughput of 30 tokens/s\nwith a single core and 71 tokens/s with eight cores on M2-Ultra, and 11\ntokens/s on lower-end devices like Raspberry Pi 5, which significantly exceeds\nthe adult average reading speed. T-MAC with LUT-based computing paradigm, paves\nthe way for the practical deployment of low-bit LLMs on resource-constrained\nedge devices without compromising computational efficiency. The system is\nopen-sourced at https://github.com/microsoft/T-MAC .",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "EuroSys 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.00088v2",
    "published_date": "2024-06-25 08:38:38 UTC",
    "updated_date": "2025-03-25 09:27:16 UTC"
  },
  {
    "arxiv_id": "2406.17819v4",
    "title": "Automatically Adaptive Conformal Risk Control",
    "authors": [
      "Vincent Blot",
      "Anastasios N Angelopoulos",
      "Michael I Jordan",
      "Nicolas J-B Brunel"
    ],
    "abstract": "Science and technology have a growing need for effective mechanisms that\nensure reliable, controlled performance from black-box machine learning\nalgorithms. These performance guarantees should ideally hold conditionally on\nthe input-that is the performance guarantees should hold, at least\napproximately, no matter what the input. However, beyond stylized discrete\ngroupings such as ethnicity and gender, the right notion of conditioning can be\ndifficult to define. For example, in problems such as image segmentation, we\nwant the uncertainty to reflect the intrinsic difficulty of the test sample,\nbut this may be difficult to capture via a conditioning event. Building on the\nrecent work of Gibbs et al. [2023], we propose a methodology for achieving\napproximate conditional control of statistical risks-the expected value of loss\nfunctions-by adapting to the difficulty of test samples. Our framework goes\nbeyond traditional conditional risk control based on user-provided conditioning\nevents to the algorithmic, data-driven determination of appropriate function\nclasses for conditioning. We apply this framework to various regression and\nsegmentation tasks, enabling finer-grained control over model performance and\ndemonstrating that by continuously monitoring and adjusting these parameters,\nwe can achieve superior precision compared to conventional risk-control\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17819v4",
    "published_date": "2024-06-25 08:29:32 UTC",
    "updated_date": "2025-03-27 10:19:28 UTC"
  },
  {
    "arxiv_id": "2406.17818v1",
    "title": "Temporal Prototype-Aware Learning for Active Voltage Control on Power Distribution Networks",
    "authors": [
      "Feiyang Xu",
      "Shunyu Liu",
      "Yunpeng Qing",
      "Yihe Zhou",
      "Yuwen Wang",
      "Mingli Song"
    ],
    "abstract": "Active Voltage Control (AVC) on the Power Distribution Networks (PDNs) aims\nto stabilize the voltage levels to ensure efficient and reliable operation of\npower systems. With the increasing integration of distributed energy resources,\nrecent efforts have explored employing multi-agent reinforcement learning\n(MARL) techniques to realize effective AVC. Existing methods mainly focus on\nthe acquisition of short-term AVC strategies, i.e., only learning AVC within\nthe short-term training trajectories of a singular diurnal cycle. However, due\nto the dynamic nature of load demands and renewable energy, the operation\nstates of real-world PDNs may exhibit significant distribution shifts across\nvarying timescales (e.g., daily and seasonal changes). This can render those\nshort-term strategies suboptimal or even obsolete when performing continuous\nAVC over extended periods. In this paper, we propose a novel temporal\nprototype-aware learning method, abbreviated as TPA, to learn time-adaptive AVC\nunder short-term training trajectories. At the heart of TPA are two\ncomplementary components, namely multi-scale dynamic encoder and temporal\nprototype-aware policy, that can be readily incorporated into various MARL\nmethods. The former component integrates a stacked transformer network to learn\nunderlying temporal dependencies at different timescales of the PDNs, while the\nlatter implements a learnable prototype matching mechanism to construct a\ndedicated AVC policy that can dynamically adapt to the evolving operation\nstates. Experimental results on the AVC benchmark with different PDN sizes\ndemonstrate that the proposed TPA surpasses the state-of-the-art counterparts\nnot only in terms of control performance but also by offering model\ntransferability. Our code is available at\nhttps://github.com/Canyizl/TPA-for-AVC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.17818v1",
    "published_date": "2024-06-25 08:07:00 UTC",
    "updated_date": "2024-06-25 08:07:00 UTC"
  },
  {
    "arxiv_id": "2406.17343v2",
    "title": "Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers",
    "authors": [
      "Lei Chen",
      "Yuan Meng",
      "Chen Tang",
      "Xinzhu Ma",
      "Jingyan Jiang",
      "Xin Wang",
      "Zhi Wang",
      "Wenwu Zhu"
    ],
    "abstract": "Recent advancements in diffusion models, particularly the architectural\ntransformation from UNet-based models to Diffusion Transformers (DiTs),\nsignificantly improve the quality and scalability of image and video\ngeneration. However, despite their impressive capabilities, the substantial\ncomputational costs of these large-scale models pose significant challenges for\nreal-world deployment. Post-Training Quantization (PTQ) emerges as a promising\nsolution, enabling model compression and accelerated inference for pretrained\nmodels, without the costly retraining. However, research on DiT quantization\nremains sparse, and existing PTQ frameworks, primarily designed for traditional\ndiffusion models, tend to suffer from biased quantization, leading to notable\nperformance degradation. In this work, we identify that DiTs typically exhibit\nsignificant spatial variance in both weights and activations, along with\ntemporal variance in activations. To address these issues, we propose Q-DiT, a\nnovel approach that seamlessly integrates two key techniques: automatic\nquantization granularity allocation to handle the significant variance of\nweights and activations across input channels, and sample-wise dynamic\nactivation quantization to adaptively capture activation changes across both\ntimesteps and samples. Extensive experiments conducted on ImageNet and VBench\ndemonstrate the effectiveness of the proposed Q-DiT. Specifically, when\nquantizing DiT-XL/2 to W6A8 on ImageNet ($256 \\times 256$), Q-DiT achieves a\nremarkable reduction in FID by 1.09 compared to the baseline. Under the more\nchallenging W4A8 setting, it maintains high fidelity in image and video\ngeneration, establishing a new benchmark for efficient, high-quality\nquantization in DiTs. Code is available at\n\\href{https://github.com/Juanerx/Q-DiT}{https://github.com/Juanerx/Q-DiT}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17343v2",
    "published_date": "2024-06-25 07:57:27 UTC",
    "updated_date": "2024-11-19 09:58:07 UTC"
  },
  {
    "arxiv_id": "2406.17342v2",
    "title": "Masked Generative Extractor for Synergistic Representation and 3D Generation of Point Clouds",
    "authors": [
      "Hongliang Zeng",
      "Ping Zhang",
      "Fang Li",
      "Jiahua Wang",
      "Tingyu Ye",
      "Pengteng Guo"
    ],
    "abstract": "Representation and generative learning, as reconstruction-based methods, have\ndemonstrated their potential for mutual reinforcement across various domains.\nIn the field of point cloud processing, although existing studies have adopted\ntraining strategies from generative models to enhance representational\ncapabilities, these methods are limited by their inability to genuinely\ngenerate 3D shapes. To explore the benefits of deeply integrating 3D\nrepresentation learning and generative learning, we propose an innovative\nframework called \\textit{Point-MGE}. Specifically, this framework first\nutilizes a vector quantized variational autoencoder to reconstruct a neural\nfield representation of 3D shapes, thereby learning discrete semantic features\nof point patches. Subsequently, we design a sliding masking ratios to smooth\nthe transition from representation learning to generative learning. Moreover,\nour method demonstrates strong generalization capability in learning\nhigh-capacity models, achieving new state-of-the-art performance across\nmultiple downstream tasks. In shape classification, Point-MGE achieved an\naccuracy of 94.2% (+1.0%) on the ModelNet40 dataset and 92.9% (+5.5%) on the\nScanObjectNN dataset. Experimental results also confirmed that Point-MGE can\ngenerate high-quality 3D shapes in both unconditional and conditional settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17342v2",
    "published_date": "2024-06-25 07:57:03 UTC",
    "updated_date": "2024-08-15 09:59:58 UTC"
  },
  {
    "arxiv_id": "2407.02521v1",
    "title": "Performance Comparison of Deep RL Algorithms for Mixed Traffic Cooperative Lane-Changing",
    "authors": [
      "Xue Yao",
      "Shengren Hou",
      "Serge P. Hoogendoorn",
      "Simeon C. Calvert"
    ],
    "abstract": "Lane-changing (LC) is a challenging scenario for connected and automated\nvehicles (CAVs) because of the complex dynamics and high uncertainty of the\ntraffic environment. This challenge can be handled by deep reinforcement\nlearning (DRL) approaches, leveraging their data-driven and model-free nature.\nOur previous work proposed a cooperative lane-changing in mixed traffic (CLCMT)\nmechanism based on TD3 to facilitate an optimal lane-changing strategy. This\nstudy enhances the current CLCMT mechanism by considering both the uncertainty\nof the human-driven vehicles (HVs) and the microscopic interactions between HVs\nand CAVs. The state-of-the-art (SOTA) DRL algorithms including DDPG, TD3, SAC,\nand PPO are utilized to deal with the formulated MDP with continuous actions.\nPerformance comparison among the four DRL algorithms demonstrates that DDPG,\nTD3, and PPO algorithms can deal with uncertainty in traffic environments and\nlearn well-performed LC strategies in terms of safety, efficiency, comfort, and\necology. The PPO algorithm outperforms the other three algorithms, regarding a\nhigher reward, fewer exploration mistakes and crashes, and a more comfortable\nand ecology LC strategy. The improvements promise CLCMT mechanism greater\nadvantages in the LC motion planning of CAVs.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 5 figures, IEEE conference",
    "pdf_url": "http://arxiv.org/pdf/2407.02521v1",
    "published_date": "2024-06-25 07:49:25 UTC",
    "updated_date": "2024-06-25 07:49:25 UTC"
  },
  {
    "arxiv_id": "2406.17334v1",
    "title": "Joint Admission Control and Resource Allocation of Virtual Network Embedding via Hierarchical Deep Reinforcement Learning",
    "authors": [
      "Tianfu Wang",
      "Li Shen",
      "Qilin Fan",
      "Tong Xu",
      "Tongliang Liu",
      "Hui Xiong"
    ],
    "abstract": "As an essential resource management problem in network virtualization,\nvirtual network embedding (VNE) aims to allocate the finite resources of\nphysical network to sequentially arriving virtual network requests (VNRs) with\ndifferent resource demands. Since this is an NP-hard combinatorial optimization\nproblem, many efforts have been made to provide viable solutions. However, most\nexisting approaches have either ignored the admission control of VNRs, which\nhas a potential impact on long-term performances, or not fully exploited the\ntemporal and topological features of the physical network and VNRs. In this\npaper, we propose a deep Hierarchical Reinforcement Learning approach to learn\na joint Admission Control and Resource Allocation policy for VNE, named\nHRL-ACRA. Specifically, the whole VNE process is decomposed into an upper-level\npolicy for deciding whether to admit the arriving VNR or not and a lower-level\npolicy for allocating resources of the physical network to meet the requirement\nof VNR through the HRL approach. Considering the proximal policy optimization\nas the basic training algorithm, we also adopt the average reward method to\naddress the infinite horizon problem of the upper-level agent and design a\ncustomized multi-objective intrinsic reward to alleviate the sparse reward\nissue of the lower-level agent. Moreover, we develop a deep feature-aware graph\nneural network to capture the features of VNR and physical network and exploit\na sequence-to-sequence model to generate embedding actions iteratively.\nFinally, extensive experiments are conducted in various settings, and show that\nHRL-ACRA outperforms state-of-the-art baselines in terms of both the acceptance\nratio and long-term average revenue. Our code is available at\n\\url{https://github.com/GeminiLight/hrl-acra}.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "Accepted by IEEE Transactions on Services Computing (TSC)",
    "pdf_url": "http://arxiv.org/pdf/2406.17334v1",
    "published_date": "2024-06-25 07:42:30 UTC",
    "updated_date": "2024-06-25 07:42:30 UTC"
  },
  {
    "arxiv_id": "2407.09529v1",
    "title": "Towards LLM-Powered Ambient Sensor Based Multi-Person Human Activity Recognition",
    "authors": [
      "Xi Chen",
      "Julien Cumin",
      "Fano Ramparany",
      "Dominique Vaufreydaz"
    ],
    "abstract": "Human Activity Recognition (HAR) is one of the central problems in fields\nsuch as healthcare, elderly care, and security at home. However, traditional\nHAR approaches face challenges including data scarcity, difficulties in model\ngeneralization, and the complexity of recognizing activities in multi-person\nscenarios. This paper proposes a system framework called LAHAR, based on large\nlanguage models. Utilizing prompt engineering techniques, LAHAR addresses HAR\nin multi-person scenarios by enabling subject separation and action-level\ndescriptions of events occurring in the environment. We validated our approach\non the ARAS dataset, and the results demonstrate that LAHAR achieves comparable\naccuracy to the state-of-the-art method at higher resolutions and maintains\nrobustness in multi-person scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09529v1",
    "published_date": "2024-06-25 07:41:34 UTC",
    "updated_date": "2024-06-25 07:41:34 UTC"
  },
  {
    "arxiv_id": "2406.17328v3",
    "title": "Dual-Space Knowledge Distillation for Large Language Models",
    "authors": [
      "Songming Zhang",
      "Xue Zhang",
      "Zengkui Sun",
      "Yufeng Chen",
      "Jinan Xu"
    ],
    "abstract": "Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The camera-ready version for EMNLP 2024 main conference. 17 pages, 11\n  figures, code available at: https://github.com/songmzhang/DSKD",
    "pdf_url": "http://arxiv.org/pdf/2406.17328v3",
    "published_date": "2024-06-25 07:25:15 UTC",
    "updated_date": "2024-10-01 16:45:12 UTC"
  },
  {
    "arxiv_id": "2406.17326v1",
    "title": "The State-Action-Reward-State-Action Algorithm in Spatial Prisoner's Dilemma Game",
    "authors": [
      "Lanyu Yang",
      "Dongchun Jiang",
      "Fuqiang Guo",
      "Mingjian Fu"
    ],
    "abstract": "Cooperative behavior is prevalent in both human society and nature.\nUnderstanding the emergence and maintenance of cooperation among\nself-interested individuals remains a significant challenge in evolutionary\nbiology and social sciences. Reinforcement learning (RL) provides a suitable\nframework for studying evolutionary game theory as it can adapt to\nenvironmental changes and maximize expected benefits. In this study, we employ\nthe State-Action-Reward-State-Action (SARSA) algorithm as the decision-making\nmechanism for individuals in evolutionary game theory. Initially, we apply\nSARSA to imitation learning, where agents select neighbors to imitate based on\nrewards. This approach allows us to observe behavioral changes in agents\nwithout independent decision-making abilities. Subsequently, SARSA is utilized\nfor primary agents to independently choose cooperation or betrayal with their\nneighbors. We evaluate the impact of SARSA on cooperation rates by analyzing\nvariations in rewards and the distribution of cooperators and defectors within\nthe network.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17326v1",
    "published_date": "2024-06-25 07:21:35 UTC",
    "updated_date": "2024-06-25 07:21:35 UTC"
  },
  {
    "arxiv_id": "2407.00087v2",
    "title": "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback",
    "authors": [
      "Ju-Seung Byun",
      "Jiyun Chun",
      "Jihyung Kil",
      "Andrew Perrault"
    ],
    "abstract": "Large Multimodal Models (LMMs) excel at comprehending human instructions and\ndemonstrate remarkable results across a broad spectrum of tasks. Reinforcement\nLearning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs\nby aligning them with specific preferences. These methods primarily use\nranking-based feedback for entire generations. With advanced AI models\n(Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of\ndetailed feedback that are expensive for humans to provide. We propose a\ntwo-stage algorithm ARES that Alternates REinforcement Learning (RL) and\nSupervised Fine-Tuning (SFT). First, we request the Teacher to score how much\neach sentence contributes to solving the problem in a Chain-of-Thought (CoT).\nThis sentence-level feedback allows us to consider individual valuable\nsegments, providing more granular rewards for the RL procedure. Second, we ask\nthe Teacher to correct the wrong reasoning after the RL stage. The RL procedure\nrequires massive efforts for hyperparameter tuning and often generates errors\nlike repetitive words and incomplete sentences. With the correction feedback,\nwe stabilize the RL fine-tuned model through SFT. We conduct experiments on\nmulti-model dataset ScienceQA and A-OKVQA to demonstrate the effectiveness of\nour proposal. ARES rationale reasoning achieves around 70% win rate against\nbaseline models judged by GPT-4o. Additionally, we observe that the improved\nrationale reasoning leads to a 2.5% increase in inference answer accuracy on\naverage for the multi-modal datasets.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.00087v2",
    "published_date": "2024-06-25 07:20:11 UTC",
    "updated_date": "2024-10-03 18:06:53 UTC"
  },
  {
    "arxiv_id": "2406.17322v1",
    "title": "ALPBench: A Benchmark for Active Learning Pipelines on Tabular Data",
    "authors": [
      "Valentin Margraf",
      "Marcel Wever",
      "Sandra Gilhuber",
      "Gabriel Marques Tavares",
      "Thomas Seidl",
      "Eyke Hüllermeier"
    ],
    "abstract": "In settings where only a budgeted amount of labeled data can be afforded,\nactive learning seeks to devise query strategies for selecting the most\ninformative data points to be labeled, aiming to enhance learning algorithms'\nefficiency and performance. Numerous such query strategies have been proposed\nand compared in the active learning literature. However, the community still\nlacks standardized benchmarks for comparing the performance of different query\nstrategies. This particularly holds for the combination of query strategies\nwith different learning algorithms into active learning pipelines and examining\nthe impact of the learning algorithm choice. To close this gap, we propose\nALPBench, which facilitates the specification, execution, and performance\nmonitoring of active learning pipelines. It has built-in measures to ensure\nevaluations are done reproducibly, saving exact dataset splits and\nhyperparameter settings of used algorithms. In total, ALPBench consists of 86\nreal-world tabular classification datasets and 5 active learning settings,\nyielding 430 active learning problems. To demonstrate its usefulness and broad\ncompatibility with various learning algorithms and query strategies, we conduct\nan exemplary study evaluating 9 query strategies paired with 8 learning\nalgorithms in 2 different settings. We provide ALPBench here:\nhttps://github.com/ValentinMargraf/ActiveLearningPipelines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17322v1",
    "published_date": "2024-06-25 07:14:14 UTC",
    "updated_date": "2024-06-25 07:14:14 UTC"
  },
  {
    "arxiv_id": "2406.17816v1",
    "title": "Towards Hypermedia Environments for Adaptive Coordination in Industrial Automation",
    "authors": [
      "Ganesh Ramanathan",
      "Simon Mayer",
      "Andrei Ciortea"
    ],
    "abstract": "Electromechanical systems manage physical processes through a network of\ninter-connected components. Today, programming the interactions required for\ncoordinating these components is largely a manual process. This process is\ntime-consuming and requires manual adaptation when system features change. To\novercome this issue, we use autonomous software agents that process semantic\ndescriptions of the system to determine coordination requirements and\nconstraints; on this basis, they then interact with one another to control the\nsystem in a decentralized and coordinated manner.Our core insight is that\ncoordination requirements between individual components are, ultimately,\nlargely due to underlying physical interdependencies between the components,\nwhich can be (and, in many cases, already are) semantically modeled in\nautomation projects. Agents then use hypermedia to discover, at run time, the\nplans and protocols required for enacting the coordination. A key novelty of\nour approach is the use of hypermedia-driven interaction: it reduces coupling\nin the system and enables its run-time adaptation as features change.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17816v1",
    "published_date": "2024-06-25 06:21:52 UTC",
    "updated_date": "2024-06-25 06:21:52 UTC"
  },
  {
    "arxiv_id": "2406.17297v2",
    "title": "Towards Open-set Camera 3D Object Detection",
    "authors": [
      "Zhuolin He",
      "Xinrun Li",
      "Heng Gao",
      "Jiachen Tang",
      "Shoumeng Qiu",
      "Wenfu Wang",
      "Lvjian Lu",
      "Xuchong Qiu",
      "Xiangyang Xue",
      "Jian Pu"
    ],
    "abstract": "Traditional camera 3D object detectors are typically trained to recognize a\npredefined set of known object classes. In real-world scenarios, these\ndetectors may encounter unknown objects outside the training categories and\nfail to identify them correctly. To address this gap, we present OS-Det3D\n(Open-set Camera 3D Object Detection), a two-stage training framework enhancing\nthe ability of camera 3D detectors to identify both known and unknown objects.\nThe framework involves our proposed 3D Object Discovery Network (ODN3D), which\nis specifically trained using geometric cues such as the location and scale of\n3D boxes to discover general 3D objects. ODN3D is trained in a class-agnostic\nmanner, and the provided 3D object region proposals inherently come with data\nnoise. To boost accuracy in identifying unknown objects, we introduce a Joint\nObjectness Selection (JOS) module. JOS selects the pseudo ground truth for\nunknown objects from the 3D object region proposals of ODN3D by combining the\nODN3D objectness and camera feature attention objectness. Experiments on the\nnuScenes and KITTI datasets demonstrate the effectiveness of our framework in\nenabling camera 3D detectors to successfully identify unknown objects while\nalso improving their performance on known objects.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17297v2",
    "published_date": "2024-06-25 05:58:34 UTC",
    "updated_date": "2024-06-27 02:19:51 UTC"
  },
  {
    "arxiv_id": "2406.17815v2",
    "title": "SUM: Saliency Unification through Mamba for Visual Attention Modeling",
    "authors": [
      "Alireza Hosseini",
      "Amirhossein Kazerouni",
      "Saeed Akhavan",
      "Michael Brudno",
      "Babak Taati"
    ],
    "abstract": "Visual attention modeling, important for interpreting and prioritizing visual\nstimuli, plays a significant role in applications such as marketing,\nmultimedia, and robotics. Traditional saliency prediction models, especially\nthose based on Convolutional Neural Networks (CNNs) or Transformers, achieve\nnotable success by leveraging large-scale annotated datasets. However, the\ncurrent state-of-the-art (SOTA) models that use Transformers are\ncomputationally expensive. Additionally, separate models are often required for\neach image type, lacking a unified approach. In this paper, we propose Saliency\nUnification through Mamba (SUM), a novel approach that integrates the efficient\nlong-range dependency modeling of Mamba with U-Net to provide a unified model\nfor diverse image types. Using a novel Conditional Visual State Space (C-VSS)\nblock, SUM dynamically adapts to various image types, including natural scenes,\nweb pages, and commercial imagery, ensuring universal applicability across\ndifferent data types. Our comprehensive evaluations across five benchmarks\ndemonstrate that SUM seamlessly adapts to different visual characteristics and\nconsistently outperforms existing models. These results position SUM as a\nversatile and powerful tool for advancing visual attention modeling, offering a\nrobust solution universally applicable across different types of visual\ncontent.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IEEE/CVF WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.17815v2",
    "published_date": "2024-06-25 05:54:07 UTC",
    "updated_date": "2024-09-09 11:51:10 UTC"
  },
  {
    "arxiv_id": "2406.17289v2",
    "title": "Hyperbolic Knowledge Transfer in Cross-Domain Recommendation System",
    "authors": [
      "Xin Yang",
      "Heng Chang",
      "Zhijian Lai",
      "Jinze Yang",
      "Xingrun Li",
      "Yu Lu",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Erxue Min"
    ],
    "abstract": "Cross-Domain Recommendation (CDR) seeks to utilize knowledge from different\ndomains to alleviate the problem of data sparsity in the target recommendation\ndomain, and it has been gaining more attention in recent years. Although there\nhave been notable advancements in this area, most current methods represent\nusers and items in Euclidean space, which is not ideal for handling long-tail\ndistributed data in recommendation systems. Additionally, adding data from\nother domains can worsen the long-tail characteristics of the entire dataset,\nmaking it harder to train CDR models effectively. Recent studies have shown\nthat hyperbolic methods are particularly suitable for modeling long-tail\ndistributions, which has led us to explore hyperbolic representations for users\nand items in CDR scenarios. However, due to the distinct characteristics of the\ndifferent domains, applying hyperbolic representation learning to CDR tasks is\nquite challenging. In this paper, we introduce a new framework called\nHyperbolic Contrastive Learning (HCTS), designed to capture the unique features\nof each domain while enabling efficient knowledge transfer between domains. We\nachieve this by embedding users and items from each domain separately and\nmapping them onto distinct hyperbolic manifolds with adjustable curvatures for\nprediction. To improve the representations of users and items in the target\ndomain, we develop a hyperbolic contrastive learning module for knowledge\ntransfer. Extensive experiments on real-world datasets demonstrate that\nhyperbolic manifolds are a promising alternative to Euclidean space for CDR\ntasks.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17289v2",
    "published_date": "2024-06-25 05:35:02 UTC",
    "updated_date": "2024-07-04 14:54:07 UTC"
  },
  {
    "arxiv_id": "2406.17287v1",
    "title": "Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models",
    "authors": [
      "Yang Yan",
      "Lizhi Ma",
      "Anqi Li",
      "Jingsong Ma",
      "Zhenzhong Lan"
    ],
    "abstract": "Accurate assessment of personality traits is crucial for effective\npsycho-counseling, yet traditional methods like self-report questionnaires are\ntime-consuming and biased. This study exams whether Large Language Models\n(LLMs) can predict the Big Five personality traits directly from counseling\ndialogues and introduces an innovative framework to perform the task. Our\nframework applies role-play and questionnaire-based prompting to condition LLMs\non counseling sessions, simulating client responses to the Big Five Inventory.\nWe evaluated our framework on 853 real-world counseling sessions, finding a\nsignificant correlation between LLM-predicted and actual Big Five traits,\nproving the validity of framework. Moreover, ablation studies highlight the\nimportance of role-play simulations and task simplification via questionnaires\nin enhancing prediction accuracy. Meanwhile, our fine-tuned Llama3-8B model,\nutilizing Direct Preference Optimization with Supervised Fine-Tuning, achieves\na 130.95\\% improvement, surpassing the state-of-the-art Qwen1.5-110B by 36.94\\%\nin personality prediction validity. In conclusion, LLMs can predict personality\nbased on counseling dialogues. Our code and model are publicly available at\n\\url{https://github.com/kuri-leo/BigFive-LLM-Predictor}, providing a valuable\ntool for future research in computational psychometrics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17287v1",
    "published_date": "2024-06-25 05:30:55 UTC",
    "updated_date": "2024-06-25 05:30:55 UTC"
  },
  {
    "arxiv_id": "2406.17285v1",
    "title": "EON-1: A Brain-Inspired Processor for Near-Sensor Extreme Edge Online Feature Extraction",
    "authors": [
      "Alexandra Dobrita",
      "Amirreza Yousefzadeh",
      "Simon Thorpe",
      "Kanishkan Vadivel",
      "Paul Detterer",
      "Guangzhi Tang",
      "Gert-Jan van Schaik",
      "Mario Konijnenburg",
      "Anteneh Gebregiorgis",
      "Said Hamdioui",
      "Manolis Sifalakis"
    ],
    "abstract": "For Edge AI applications, deploying online learning and adaptation on\nresource-constrained embedded devices can deal with fast sensor-generated\nstreams of data in changing environments. However, since maintaining\nlow-latency and power-efficient inference is paramount at the Edge, online\nlearning and adaptation on the device should impose minimal additional overhead\nfor inference. With this goal in mind, we explore energy-efficient learning and\nadaptation on-device for streaming-data Edge AI applications using Spiking\nNeural Networks (SNNs), which follow the principles of brain-inspired\ncomputing, such as high-parallelism, neuron co-located memory and compute, and\nevent-driven processing. We propose EON-1, a brain-inspired processor for\nnear-sensor extreme edge online feature extraction, that integrates a fast\nonline learning and adaptation algorithm. We report results of only 1% energy\noverhead for learning, by far the lowest overhead when compared to other SoTA\nsolutions, while attaining comparable inference accuracy. Furthermore, we\ndemonstrate that EON-1 is up for the challenge of low-latency processing of HD\nand UHD streaming video in real-time, with learning enabled.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17285v1",
    "published_date": "2024-06-25 05:23:41 UTC",
    "updated_date": "2024-06-25 05:23:41 UTC"
  },
  {
    "arxiv_id": "2406.17279v1",
    "title": "Learning Decentralized Multi-Biped Control for Payload Transport",
    "authors": [
      "Bikram Pandit",
      "Ashutosh Gupta",
      "Mohitvishnu S. Gadde",
      "Addison Johnson",
      "Aayam Kumar Shrestha",
      "Helei Duan",
      "Jeremy Dao",
      "Alan Fern"
    ],
    "abstract": "Payload transport over flat terrain via multi-wheel robot carriers is\nwell-understood, highly effective, and configurable. In this paper, our goal is\nto provide similar effectiveness and configurability for transport over rough\nterrain that is more suitable for legs rather than wheels. For this purpose, we\nconsider multi-biped robot carriers, where wheels are replaced by multiple\nbipedal robots attached to the carrier. Our main contribution is to design a\ndecentralized controller for such systems that can be effectively applied to\nvarying numbers and configurations of rigidly attached bipedal robots without\nretraining. We present a reinforcement learning approach for training the\ncontroller in simulation that supports transfer to the real world. Our\nexperiments in simulation provide quantitative metrics showing the\neffectiveness of the approach over a wide variety of simulated transport\nscenarios. In addition, we demonstrate the controller in the real-world for\nsystems composed of two and three Cassie robots. To our knowledge, this is the\nfirst example of a scalable multi-biped payload transport system.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to CoRL 2024, Project website: decmbc.github.io",
    "pdf_url": "http://arxiv.org/pdf/2406.17279v1",
    "published_date": "2024-06-25 05:08:44 UTC",
    "updated_date": "2024-06-25 05:08:44 UTC"
  },
  {
    "arxiv_id": "2406.17266v1",
    "title": "AG-LSEC: Audio Grounded Lexical Speaker Error Correction",
    "authors": [
      "Rohit Paturi",
      "Xiang Li",
      "Sundararajan Srinivasan"
    ],
    "abstract": "Speaker Diarization (SD) systems are typically audio-based and operate\nindependently of the ASR system in traditional speech transcription pipelines\nand can have speaker errors due to SD and/or ASR reconciliation, especially\naround speaker turns and regions of speech overlap. To reduce these errors, a\nLexical Speaker Error Correction (LSEC), in which an external language model\nprovides lexical information to correct the speaker errors, was recently\nproposed. Though the approach achieves good Word Diarization error rate (WDER)\nimprovements, it does not use any additional acoustic information and is prone\nto miscorrections. In this paper, we propose to enhance and acoustically ground\nthe LSEC system with speaker scores directly derived from the existing SD\npipeline. This approach achieves significant relative WDER reductions in the\nrange of 25-40% over the audio-based SD, ASR system and beats the LSEC system\nby 15-25% relative on RT03-CTS, Callhome American English and Fisher datasets.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at INTERSPEECH 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.17266v1",
    "published_date": "2024-06-25 04:20:49 UTC",
    "updated_date": "2024-06-25 04:20:49 UTC"
  },
  {
    "arxiv_id": "2406.17265v1",
    "title": "Image-Guided Outdoor LiDAR Perception Quality Assessment for Autonomous Driving",
    "authors": [
      "Ce Zhang",
      "Azim Eskandarian"
    ],
    "abstract": "LiDAR is one of the most crucial sensors for autonomous vehicle perception.\nHowever, current LiDAR-based point cloud perception algorithms lack\ncomprehensive and rigorous LiDAR quality assessment methods, leading to\nuncertainty in detection performance. Additionally, existing point cloud\nquality assessment algorithms are predominantly designed for indoor\nenvironments or single-object scenarios. In this paper, we introduce a novel\nimage-guided point cloud quality assessment algorithm for outdoor autonomous\ndriving environments, named the Image-Guided Outdoor Point Cloud Quality\nAssessment (IGO-PQA) algorithm. Our proposed algorithm comprises two main\ncomponents. The first component is the IGO-PQA generation algorithm, which\nleverages point cloud data, corresponding RGB surrounding view images, and\nagent objects' ground truth annotations to generate an overall quality score\nfor a single-frame LiDAR-based point cloud. The second component is a\ntransformer-based IGO-PQA regression algorithm for no-reference outdoor point\ncloud quality assessment. This regression algorithm allows for the direct\nprediction of IGO-PQA scores in an online manner, without requiring image data\nand object ground truth annotations. We evaluate our proposed algorithm using\nthe nuScenes and Waymo open datasets. The IGO-PQA generation algorithm provides\nconsistent and reasonable perception quality indices. Furthermore, our proposed\nIGO-PQA regression algorithm achieves a Pearson Linear Correlation Coefficient\n(PLCC) of 0.86 on the nuScenes dataset and 0.97 on the Waymo dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2406.17265v1",
    "published_date": "2024-06-25 04:16:14 UTC",
    "updated_date": "2024-06-25 04:16:14 UTC"
  },
  {
    "arxiv_id": "2406.17251v1",
    "title": "TopoGCL: Topological Graph Contrastive Learning",
    "authors": [
      "Yuzhou Chen",
      "Jose Frias",
      "Yulia R. Gel"
    ],
    "abstract": "Graph contrastive learning (GCL) has recently emerged as a new concept which\nallows for capitalizing on the strengths of graph neural networks (GNNs) to\nlearn rich representations in a wide variety of applications which involve\nabundant unlabeled information. However, existing GCL approaches largely tend\nto overlook the important latent information on higher-order graph\nsubstructures. We address this limitation by introducing the concepts of\ntopological invariance and extended persistence on graphs to GCL. In\nparticular, we propose a new contrastive mode which targets topological\nrepresentations of the two augmented views from the same graph, yielded by\nextracting latent shape properties of the graph at multiple resolutions. Along\nwith the extended topological layer, we introduce a new extended persistence\nsummary, namely, extended persistence landscapes (EPL) and derive its\ntheoretical stability guarantees. Our extensive numerical results on\nbiological, chemical, and social interaction graphs show that the new\nTopological Graph Contrastive Learning (TopoGCL) model delivers significant\nperformance gains in unsupervised graph classification for 11 out of 12\nconsidered datasets and also exhibits robustness under noisy scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17251v1",
    "published_date": "2024-06-25 03:35:20 UTC",
    "updated_date": "2024-06-25 03:35:20 UTC"
  },
  {
    "arxiv_id": "2406.17246v2",
    "title": "Beyond Silence: Bias Analysis through Loss and Asymmetric Approach in Audio Anti-Spoofing",
    "authors": [
      "Hye-jin Shim",
      "Md Sahidullah",
      "Jee-weon Jung",
      "Shinji Watanabe",
      "Tomi Kinnunen"
    ],
    "abstract": "Current trends in audio anti-spoofing detection research strive to improve\nmodels' ability to generalize across unseen attacks by learning to identify a\nvariety of spoofing artifacts. This emphasis has primarily focused on the spoof\nclass. Recently, several studies have noted that the distribution of silence\ndiffers between the two classes, which can serve as a shortcut. In this paper,\nwe extend class-wise interpretations beyond silence. We employ loss analysis\nand asymmetric methodologies to move away from traditional attack-focused and\nresult-oriented evaluations towards a deeper examination of model behaviors.\nOur investigations highlight the significant differences in training dynamics\nbetween the two classes, emphasizing the need for future research to focus on\nrobust modeling of the bonafide class.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 1 figure, 5 tables, ISCA Interspeech 2024 SynData4GenAI\n  Workshop",
    "pdf_url": "http://arxiv.org/pdf/2406.17246v2",
    "published_date": "2024-06-25 03:24:12 UTC",
    "updated_date": "2024-08-26 14:56:06 UTC"
  },
  {
    "arxiv_id": "2406.17245v2",
    "title": "Unlocking Continual Learning Abilities in Language Models",
    "authors": [
      "Wenyu Du",
      "Shuang Cheng",
      "Tongxu Luo",
      "Zihan Qiu",
      "Zeyu Huang",
      "Ka Chun Cheung",
      "Reynold Cheng",
      "Jie Fu"
    ],
    "abstract": "Language models (LMs) exhibit impressive performance and generalization\ncapabilities. However, LMs struggle with the persistent challenge of\ncatastrophic forgetting, which undermines their long-term sustainability in\ncontinual learning (CL). Existing approaches usually address the issue by\nincorporating old task data or task-wise inductive bias into LMs. However, old\ndata and accurate task information are often unavailable or costly to collect,\nhindering the availability of current CL approaches for LMs. To address this\nlimitation, we introduce $\\textbf{MIGU}$ ($\\textbf{M}$agn$\\textbf{I}$tude-based\n$\\textbf{G}$radient $\\textbf{U}$pdating for continual learning), a\nrehearsal-free and task-label-free method that only updates the model\nparameters with large magnitudes of output in LMs' linear layers. MIGU is based\non our observation that the L1-normalized magnitude distribution of the output\nin LMs' linear layers is different when the LM models deal with different task\ndata. By imposing this simple constraint on the gradient update process, we can\nleverage the inherent behaviors of LMs, thereby unlocking their innate CL\nabilities. Our experiments demonstrate that MIGU is universally applicable to\nall three LM architectures (T5, RoBERTa, and Llama2), delivering\nstate-of-the-art or on-par performance across continual finetuning and\ncontinual pre-training settings on four CL benchmarks. For example, MIGU brings\na 15.2% average accuracy improvement over conventional parameter-efficient\nfinetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly\nintegrate with all three existing CL types to further enhance performance. Code\nis available at https://github.com/wenyudu/MIGU.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.17245v2",
    "published_date": "2024-06-25 03:24:06 UTC",
    "updated_date": "2024-10-06 10:07:43 UTC"
  },
  {
    "arxiv_id": "2406.17235v2",
    "title": "Task-Agnostic Federated Learning",
    "authors": [
      "Zhengtao Yao",
      "Hong Nguyen",
      "Ajitesh Srivastava",
      "Jose Luis Ambite"
    ],
    "abstract": "In the realm of medical imaging, leveraging large-scale datasets from various\ninstitutions is crucial for developing precise deep learning models, yet\nprivacy concerns frequently impede data sharing. federated learning (FL)\nemerges as a prominent solution for preserving privacy while facilitating\ncollaborative learning. However, its application in real-world scenarios faces\nseveral obstacles, such as task & data heterogeneity, label scarcity,\nnon-identically distributed (non-IID) data, computational vaiation, etc. In\nreal-world, medical institutions may not want to disclose their tasks to FL\nserver and generalization challenge of out-of-network institutions with un-seen\ntask want to join the on-going federated system. This study address\ntask-agnostic and generalization problem on un-seen tasks by adapting\nself-supervised FL framework. Utilizing Vision Transformer (ViT) as consensus\nfeature encoder for self-supervised pre-training, no initial labels required,\nthe framework enabling effective representation learning across diverse\ndatasets and tasks. Our extensive evaluations, using various real-world non-IID\nmedical imaging datasets, validate our approach's efficacy, retaining 90\\% of\nF1 accuracy with only 5\\% of the training data typically required for\ncentralized approaches and exhibiting superior adaptability to\nout-of-distribution task. The result indicate that federated learning\narchitecture can be a potential approach toward multi-task foundation modeling.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2205.08576 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2406.17235v2",
    "published_date": "2024-06-25 02:53:37 UTC",
    "updated_date": "2025-01-06 16:18:39 UTC"
  },
  {
    "arxiv_id": "2406.17224v1",
    "title": "Large Language Models are Interpretable Learners",
    "authors": [
      "Ruochen Wang",
      "Si Si",
      "Felix Yu",
      "Dorothea Wiesmann",
      "Cho-Jui Hsieh",
      "Inderjit Dhillon"
    ],
    "abstract": "The trade-off between expressiveness and interpretability remains a core\nchallenge when building human-centric predictive models for classification and\ndecision-making. While symbolic rules offer interpretability, they often lack\nexpressiveness, whereas neural networks excel in performance but are known for\nbeing black boxes. In this paper, we show a combination of Large Language\nModels (LLMs) and symbolic programs can bridge this gap. In the proposed\nLLM-based Symbolic Programs (LSPs), the pretrained LLM with natural language\nprompts provides a massive set of interpretable modules that can transform raw\ninput into natural language concepts. Symbolic programs then integrate these\nmodules into an interpretable decision rule. To train LSPs, we develop a\ndivide-and-conquer approach to incrementally build the program from scratch,\nwhere the learning process of each step is guided by LLMs. To evaluate the\neffectiveness of LSPs in extracting interpretable and accurate knowledge from\ndata, we introduce IL-Bench, a collection of diverse tasks, including both\nsynthetic and real-world scenarios across different modalities. Empirical\nresults demonstrate LSP's superior performance compared to traditional\nneurosymbolic programs and vanilla automatic prompt tuning methods. Moreover,\nas the knowledge learned by LSP is a combination of natural language\ndescriptions and symbolic rules, it is easily transferable to humans\n(interpretable), and other LLMs, and generalizes well to out-of-distribution\nsamples.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.SC",
      "68T05"
    ],
    "primary_category": "cs.AI",
    "comment": "Preliminary Version, Code at [this\n  url](https://github.com/ruocwang/llm-symbolic-program)",
    "pdf_url": "http://arxiv.org/pdf/2406.17224v1",
    "published_date": "2024-06-25 02:18:15 UTC",
    "updated_date": "2024-06-25 02:18:15 UTC"
  },
  {
    "arxiv_id": "2406.17216v2",
    "title": "Machine Unlearning Fails to Remove Data Poisoning Attacks",
    "authors": [
      "Martin Pawelczyk",
      "Jimmy Z. Di",
      "Yiwei Lu",
      "Ayush Sekhari",
      "Gautam Kamath",
      "Seth Neel"
    ],
    "abstract": "We revisit the efficacy of several practical methods for approximate machine\nunlearning developed for large-scale deep learning. In addition to complying\nwith data deletion requests, one often-cited potential application for\nunlearning methods is to remove the effects of poisoned data. We experimentally\ndemonstrate that, while existing unlearning methods have been demonstrated to\nbe effective in a number of settings, they fail to remove the effects of data\npoisoning across a variety of types of poisoning attacks (indiscriminate,\ntargeted, and a newly-introduced Gaussian poisoning attack) and models (image\nclassifiers and LLMs); even when granted a relatively large compute budget. In\norder to precisely characterize unlearning efficacy, we introduce new\nevaluation metrics for unlearning based on data poisoning. Our results suggest\nthat a broader perspective, including a wider variety of evaluations, are\nrequired to avoid a false sense of confidence in machine unlearning procedures\nfor deep learning without provable guarantees. Moreover, while unlearning\nmethods show some signs of being useful to efficiently remove poisoned data\nwithout having to retrain, our work suggests that these methods are not yet\n``ready for prime time,'' and currently provide limited benefit over\nretraining.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.17216v2",
    "published_date": "2024-06-25 02:05:29 UTC",
    "updated_date": "2025-04-01 10:49:56 UTC"
  },
  {
    "arxiv_id": "2406.17215v3",
    "title": "Enabling Large Language Models to Perform Power System Simulations with Previously Unseen Tools: A Case of Daline",
    "authors": [
      "Mengshuo Jia",
      "Zeyu Cui",
      "Gabriela Hug"
    ],
    "abstract": "The integration of experiment technologies with large language models (LLMs)\nis transforming scientific research, offering AI capabilities beyond\nspecialized problem-solving to becoming research assistants for human\nscientists. In power systems, simulations are essential for research. However,\nLLMs face significant challenges in power system simulations due to limited\npre-existing knowledge and the complexity of power grids. To address this\nissue, this work proposes a modular framework that integrates expertise from\nboth the power system and LLM domains. This framework enhances LLMs' ability to\nperform power system simulations on previously unseen tools. Validated using 34\nsimulation tasks in Daline, a (optimal) power flow simulation and linearization\ntoolbox not yet exposed to LLMs, the proposed framework improved GPT-4o's\nsimulation coding accuracy from 0% to 96.07%, also outperforming the ChatGPT-4o\nweb interface's 33.8% accuracy (with the entire knowledge base uploaded). These\nresults highlight the potential of LLMs as research assistants in power\nsystems.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17215v3",
    "published_date": "2024-06-25 02:05:26 UTC",
    "updated_date": "2024-11-19 20:16:11 UTC"
  },
  {
    "arxiv_id": "2406.17188v2",
    "title": "Geometric Median (GM) Matching for Robust Data Pruning",
    "authors": [
      "Anish Acharya",
      "Inderjit S Dhillon",
      "Sujay Sanghavi"
    ],
    "abstract": "Large-scale data collections in the wild, are invariably noisy. Thus\ndeveloping data pruning strategies that remain robust even in the presence of\ncorruption is critical in practice. In this work, we propose Geometric Median\n($\\gm$) Matching -- a herding style greedy algorithm that yields a $k$-subset\nsuch that the mean of the subset approximates the geometric median of the\n(potentially) noisy dataset. Theoretically, we show that $\\gm$ Matching enjoys\nan improved $\\gO(1/k)$ scaling over $\\gO(1/\\sqrt{k})$ scaling of uniform\nsampling; while achieving {\\bf optimal breakdown point} of {\\bf 1/2} even under\n{\\bf arbitrary} corruption. Extensive experiments across several popular deep\nlearning benchmarks indicate that $\\gm$ Matching consistently improves over\nprior state-of-the-art; the gains become more profound at high rates of\ncorruption and aggressive pruning rates; making $\\gm$ Matching a strong\nbaseline for future research in robust data pruning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17188v2",
    "published_date": "2024-06-25 00:02:01 UTC",
    "updated_date": "2025-01-17 08:38:45 UTC"
  }
]
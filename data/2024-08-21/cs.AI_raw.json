[
  {
    "arxiv_id": "2408.12036v2",
    "title": "Reasoning and Tools for Human-Level Forecasting",
    "authors": [
      "Elvis Hsieh",
      "Preston Fu",
      "Jonathan Chen"
    ],
    "abstract": "Language models (LMs) trained on web-scale datasets are largely successful\ndue to their ability to memorize large amounts of training data, even if only\npresent in a few examples. These capabilities are often desirable in evaluation\non tasks such as question answering but raise questions about whether these\nmodels can exhibit genuine reasoning or succeed only at mimicking patterns from\nthe training data. This distinction is particularly salient in forecasting\ntasks, where the answer is not present in the training data, and the model must\nreason to make logical deductions. We present Reasoning and Tools for\nForecasting (RTF), a framework of reasoning-and-acting (ReAct) agents that can\ndynamically retrieve updated information and run numerical simulation with\nequipped tools. We evaluate our model with questions from competitive\nforecasting platforms and demonstrate that our method is competitive with and\ncan outperform human predictions. This suggests that LMs, with the right tools,\ncan indeed think and adapt like humans, offering valuable insights for\nreal-world decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12036v2",
    "published_date": "2024-08-21 23:42:06 UTC",
    "updated_date": "2024-10-31 23:08:03 UTC"
  },
  {
    "arxiv_id": "2408.12032v1",
    "title": "A Constraint Programming Approach to Fair High School Course Scheduling",
    "authors": [
      "Mitsuka Kiyohara",
      "Masakazu Ishihata"
    ],
    "abstract": "Issues of inequity in U.S. high schools' course scheduling did not previously\nexist. However, in recent years, with the increase in student population and\ncourse variety, students perceive that the course scheduling method is unfair.\nCurrent integer programming (IP) methods to the high school scheduling problem\n(HSSP) fall short in addressing these fairness concerns. The purpose of this\nresearch is to develop a solution methodology that generates feasible and fair\ncourse schedules using student preferences. Utilizing principles of fairness,\nwhich have been well studied in market design, we define the fair high school\nscheduling problem (FHSSP), a novel extension to the HSSP, and devise a\ncorresponding algorithm based on integer programming to solve the FHSSP. We\ntest our approach on a real course request dataset from a high school in\nCalifornia, USA. Results show that our algorithm can generate schedules that\nare both feasible and fair. In this paper, we demonstrate that our IP algorithm\nnot only solves the HSSP and FHSSP in the United States but has the potential\nto be applied to various real-world scheduling problems. Additionally, we show\nthe feasibility of integrating human emotions into mathematical modeling.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12032v1",
    "published_date": "2024-08-21 23:14:46 UTC",
    "updated_date": "2024-08-21 23:14:46 UTC"
  },
  {
    "arxiv_id": "2408.12029v1",
    "title": "Federated Diabetes Prediction in Canadian Adults Using Real-world Cross-Province Primary Care Data",
    "authors": [
      "Guojun Tang",
      "Jason E. Black",
      "Tyler S. Williamson",
      "Steve H. Drew"
    ],
    "abstract": "Integrating Electronic Health Records (EHR) and the application of machine\nlearning present opportunities for enhancing the accuracy and accessibility of\ndata-driven diabetes prediction. In particular, developing data-driven machine\nlearning models can provide early identification of patients with high risk for\ndiabetes, potentially leading to more effective therapeutic strategies and\nreduced healthcare costs. However, regulation restrictions create barriers to\ndeveloping centralized predictive models. This paper addresses the challenges\nby introducing a federated learning approach, which amalgamates predictive\nmodels without centralized data storage and processing, thus avoiding privacy\nissues. This marks the first application of federated learning to predict\ndiabetes using real clinical datasets in Canada extracted from the Canadian\nPrimary Care Sentinel Surveillance Network (CPCSSN) without crossprovince\npatient data sharing. We address class-imbalance issues through downsampling\ntechniques and compare federated learning performance against province-based\nand centralized models. Experimental results show that the federated MLP model\npresents a similar or higher performance compared to the model trained with the\ncentralized approach. However, the federated logistic regression model showed\ninferior performance compared to its centralized peer.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.12029v1",
    "published_date": "2024-08-21 22:47:21 UTC",
    "updated_date": "2024-08-21 22:47:21 UTC"
  },
  {
    "arxiv_id": "2408.12025v2",
    "title": "Exploring Large Language Models for Feature Selection: A Data-centric Perspective",
    "authors": [
      "Dawei Li",
      "Zhen Tan",
      "Huan Liu"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly\ninfluenced various domains, leveraging their exceptional few-shot and zero-shot\nlearning capabilities. In this work, we aim to explore and understand the\nLLMs-based feature selection methods from a data-centric perspective. We begin\nby categorizing existing feature selection methods with LLMs into two groups:\ndata-driven feature selection which requires numerical values of samples to do\nstatistical inference and text-based feature selection which utilizes prior\nknowledge of LLMs to do semantical associations using descriptive context. We\nconduct experiments in both classification and regression tasks with LLMs in\nvarious sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the\neffectiveness and robustness of text-based feature selection methods and\nshowcase their potentials using a real-world medical application. We also\ndiscuss the challenges and future opportunities in employing LLMs for feature\nselection, offering insights for further research and development in this\nemerging field.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by SIGKDD Explorations (December 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.12025v2",
    "published_date": "2024-08-21 22:35:19 UTC",
    "updated_date": "2024-10-23 17:01:05 UTC"
  },
  {
    "arxiv_id": "2408.12022v2",
    "title": "Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind",
    "authors": [
      "Lance Ying",
      "Tan Zhi-Xuan",
      "Lionel Wong",
      "Vikash Mansinghka",
      "Joshua B. Tenenbaum"
    ],
    "abstract": "How do people understand and evaluate claims about others' beliefs, even\nthough these beliefs cannot be directly observed? In this paper, we introduce a\ncognitive model of epistemic language interpretation, grounded in Bayesian\ninferences about other agents' goals, beliefs, and intentions: a\nlanguage-augmented Bayesian theory-of-mind (LaBToM). By translating natural\nlanguage into an epistemic ``language-of-thought'' with grammar-constrained LLM\ndecoding, then evaluating these translations against the inferences produced by\ninverting a generative model of rational action and perception, LaBToM captures\ngraded plausibility judgments of epistemic claims. We validate our model in an\nexperiment where participants watch an agent navigate a maze to find keys\nhidden in boxes needed to reach their goal, then rate sentences about the\nagent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and\nablated models, our model correlates highly with human judgments for a wide\nrange of expressions, including modal language, uncertainty expressions,\nknowledge claims, likelihood comparisons, and attributions of false belief.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages; Published at the Transactions of the Association for\n  Computational Linguistics (TACL); Presented at NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.12022v2",
    "published_date": "2024-08-21 22:29:56 UTC",
    "updated_date": "2025-04-18 15:31:32 UTC"
  },
  {
    "arxiv_id": "2408.12008v1",
    "title": "Does It Look Sequential? An Analysis of Datasets for Evaluation of Sequential Recommendations",
    "authors": [
      "Anton Klenitskiy",
      "Anna Volodkevich",
      "Anton Pembek",
      "Alexey Vasilev"
    ],
    "abstract": "Sequential recommender systems are an important and demanded area of\nresearch. Such systems aim to use the order of interactions in a user's history\nto predict future interactions. The premise is that the order of interactions\nand sequential patterns play an essential role. Therefore, it is crucial to use\ndatasets that exhibit a sequential structure to evaluate sequential\nrecommenders properly.\n  We apply several methods based on the random shuffling of the user's sequence\nof interactions to assess the strength of sequential structure across 15\ndatasets, frequently used for sequential recommender systems evaluation in\nrecent research papers presented at top-tier conferences. As shuffling\nexplicitly breaks sequential dependencies inherent in datasets, we estimate the\nstrength of sequential patterns by comparing metrics for shuffled and original\nversions of the dataset. Our findings show that several popular datasets have a\nrather weak sequential structure.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12008v1",
    "published_date": "2024-08-21 21:40:07 UTC",
    "updated_date": "2024-08-21 21:40:07 UTC"
  },
  {
    "arxiv_id": "2408.12007v1",
    "title": "QuaCK-TSF: Quantum-Classical Kernelized Time Series Forecasting",
    "authors": [
      "Abdallah Aaraba",
      "Soumaya Cherkaoui",
      "Ola Ahmad",
      "Jean-Frédéric Laprade",
      "Olivier Nahman-Lévesque",
      "Alexis Vieloszynski",
      "Shengrui Wang"
    ],
    "abstract": "Forecasting in probabilistic time series is a complex endeavor that extends\nbeyond predicting future values to also quantifying the uncertainty inherent in\nthese predictions. Gaussian process regression stands out as a Bayesian machine\nlearning technique adept at addressing this multifaceted challenge. This paper\nintroduces a novel approach that blends the robustness of this Bayesian\ntechnique with the nuanced insights provided by the kernel perspective on\nquantum models, aimed at advancing quantum kernelized probabilistic\nforecasting. We incorporate a quantum feature map inspired by Ising\ninteractions and demonstrate its effectiveness in capturing the temporal\ndependencies critical for precise forecasting. The optimization of our model's\nhyperparameters circumvents the need for computationally intensive gradient\ndescent by employing gradient-free Bayesian optimization. Comparative\nbenchmarks against established classical kernel models are provided, affirming\nthat our quantum-enhanced approach achieves competitive performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 15 figures, to be published in IEEE Quantum Week 2024's\n  conference proceeding",
    "pdf_url": "http://arxiv.org/pdf/2408.12007v1",
    "published_date": "2024-08-21 21:39:31 UTC",
    "updated_date": "2024-08-21 21:39:31 UTC"
  },
  {
    "arxiv_id": "2408.11987v1",
    "title": "SimBench: A Rule-Based Multi-Turn Interaction Benchmark for Evaluating an LLM's Ability to Generate Digital Twins",
    "authors": [
      "Jingquan Wang",
      "Harry Zhang",
      "Huzaifa Mustafa Unjhawala",
      "Peter Negrut",
      "Shu Wang",
      "Khailanii Slaton",
      "Radu Serban",
      "Jin-Long Wu",
      "Dan Negrut"
    ],
    "abstract": "We introduce SimBench, a benchmark designed to evaluate the proficiency of\nstudent large language models (S-LLMs) in generating digital twins (DTs) that\ncan be used in simulators for virtual testing. Given a collection of S-LLMs,\nthis benchmark enables the ranking of the S-LLMs based on their ability to\nproduce high-quality DTs. We demonstrate this by comparing over 20 open- and\nclosed-source S-LLMs. Using multi-turn interactions, SimBench employs a\nrule-based judge LLM (J-LLM) that leverages both predefined rules and\nhuman-in-the-loop guidance to assign scores for the DTs generated by the S-LLM,\nthus providing a consistent and expert-inspired evaluation protocol. The J-LLM\nis specific to a simulator, and herein the proposed benchmarking approach is\ndemonstrated in conjunction with the Chrono multi-physics simulator. Chrono\nprovided the backdrop used to assess an S-LLM in relation to the latter's\nability to create digital twins for multibody dynamics, finite element\nanalysis, vehicle dynamics, robotic dynamics, and sensor simulations. The\nproposed benchmarking principle is broadly applicable and enables the\nassessment of an S-LLM's ability to generate digital twins for other simulation\npackages. All code and data are available at\nhttps://github.com/uwsbel/SimBench.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11987v1",
    "published_date": "2024-08-21 20:52:32 UTC",
    "updated_date": "2024-08-21 20:52:32 UTC"
  },
  {
    "arxiv_id": "2408.11984v2",
    "title": "Chemical Reaction Neural Networks for Fitting Accelerating Rate Calorimetry Data",
    "authors": [
      "Saakaar Bhatnagar",
      "Andrew Comerford",
      "Zelu Xu",
      "Davide Berti Polato",
      "Araz Banaeizadeh",
      "Alessandro Ferraris"
    ],
    "abstract": "As the demand for lithium-ion batteries rapidly increases there is a need to\ndesign these cells in a safe manner to mitigate thermal runaway. Thermal\nrunaway in batteries leads to an uncontrollable temperature rise and\npotentially fires, which is a major safety concern. Typically, when modelling\nthe chemical kinetics of thermal runaway calorimetry data ( e.g. Accelerating\nRate Calorimetry (ARC)) is needed to determine the temperature-driven\ndecomposition kinetics. Conventional methods of fitting Arrhenius Ordinary\nDifferential Equation (ODE) thermal runaway models to Accelerated Rate\nCalorimetry (ARC) data make several assumptions that reduce the fidelity and\ngeneralizability of the obtained model. In this paper, Chemical Reaction Neural\nNetworks (CRNNs) are trained to fit the kinetic parameters of N-equation\nArrhenius ODEs to ARC data obtained from a Molicel 21700 P45B. The models are\nfound to be better approximations of the experimental data. The flexibility of\nthe method is demonstrated by experimenting with two-equation and four-equation\nmodels. Thermal runaway simulations are conducted in 3D using the obtained\nkinetic parameters, showing the applicability of the obtained thermal runaway\nmodels to large-scale simulations.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11984v2",
    "published_date": "2024-08-21 20:39:41 UTC",
    "updated_date": "2024-09-03 16:31:21 UTC"
  },
  {
    "arxiv_id": "2408.11979v2",
    "title": "Only Strict Saddles in the Energy Landscape of Predictive Coding Networks?",
    "authors": [
      "Francesco Innocenti",
      "El Mehdi Achour",
      "Ryan Singh",
      "Christopher L. Buckley"
    ],
    "abstract": "Predictive coding (PC) is an energy-based learning algorithm that performs\niterative inference over network activities before updating weights. Recent\nwork suggests that PC can converge in fewer learning steps than backpropagation\nthanks to its inference procedure. However, these advantages are not always\nobserved, and the impact of PC inference on learning is not theoretically well\nunderstood. Here, we study the geometry of the PC energy landscape at the\ninference equilibrium of the network activities. For deep linear networks, we\nfirst show that the equilibrated energy is simply a rescaled mean squared error\nloss with a weight-dependent rescaling. We then prove that many highly\ndegenerate (non-strict) saddles of the loss including the origin become much\neasier to escape (strict) in the equilibrated energy. Our theory is validated\nby experiments on both linear and non-linear networks. Based on these and other\nresults, we conjecture that all the saddles of the equilibrated energy are\nstrict. Overall, this work suggests that PC inference makes the loss landscape\nmore benign and robust to vanishing gradients, while also highlighting the\nfundamental challenge of scaling PC to deeper models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "stat.ML",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.11979v2",
    "published_date": "2024-08-21 20:23:44 UTC",
    "updated_date": "2024-11-08 16:19:49 UTC"
  },
  {
    "arxiv_id": "2408.11976v1",
    "title": "Sentiment and Emotion-aware Multi-criteria Fuzzy Group Decision Making System",
    "authors": [
      "Adilet Yerkin",
      "Pakizar Shamoi",
      "Elnara Kadyrgali"
    ],
    "abstract": "In today's world, making decisions as a group is common, whether choosing a\nrestaurant or deciding on a holiday destination. Group decision-making (GDM)\nsystems play a crucial role by facilitating consensus among participants with\ndiverse preferences. Discussions are one of the main tools people use to make\ndecisions. When people discuss alternatives, they use natural language to\nexpress their opinions. Traditional GDM systems generally require participants\nto provide explicit opinion values to the system. However, in real-life\nscenarios, participants often express their opinions through some text (e.g.,\nin comments, social media, messengers, etc.). This paper introduces a sentiment\nand emotion-aware multi-criteria fuzzy GDM system designed to enhance\nconsensus-reaching effectiveness in group settings. This system incorporates\nnatural language processing to analyze sentiments and emotions expressed in\ntextual data, enabling an understanding of participant opinions besides the\nexplicit numerical preference inputs. Once all the experts have provided their\npreferences for the alternatives, the individual preferences are aggregated\ninto a single collective preference matrix. This matrix represents the\ncollective expert opinion regarding the other options. Then, sentiments,\nemotions, and preference scores are inputted into a fuzzy inference system to\nget the overall score. The proposed system was used for a small decision-making\nprocess - choosing the hotel for a vacation by a group of friends. Our findings\ndemonstrate that integrating sentiment and emotion analysis into GDM systems\nallows everyone's feelings and opinions to be considered during discussions and\nsignificantly improves consensus among participants.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to FSDM 2024 - The 10th International Conference on Fuzzy\n  Systems and Data Mining",
    "pdf_url": "http://arxiv.org/pdf/2408.11976v1",
    "published_date": "2024-08-21 20:17:06 UTC",
    "updated_date": "2024-08-21 20:17:06 UTC"
  },
  {
    "arxiv_id": "2408.11963v2",
    "title": "Real-Time Incremental Explanations for Object Detectors in Autonomous Driving",
    "authors": [
      "Santiago Calderón-Peña",
      "Hana Chockler",
      "David A. Kelly"
    ],
    "abstract": "Object detectors are widely used in safety-critical real-time applications\nsuch as autonomous driving. Explainability is especially important for\nsafety-critical applications, and due to the variety of object detectors and\ntheir often proprietary nature, black-box explainability tools are needed.\nHowever, existing black-box explainability tools for AI models rely on multiple\nmodel calls, rendering them impractical for real-time use.\n  In this paper, we introduce IncX, an algorithm and a tool for real-time\nblack-box explainability for object detectors. The algorithm is based on linear\ntransformations of saliency maps, producing sufficient explanations. We\nevaluate our implementation on four widely used video datasets of autonomous\ndriving and demonstrate that IncX's explanations are comparable in quality to\nthe state-of-the-art and are computed two orders of magnitude faster than the\nstate-of-the-art, making them usable in real time.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11963v2",
    "published_date": "2024-08-21 19:31:39 UTC",
    "updated_date": "2025-03-07 17:38:59 UTC"
  },
  {
    "arxiv_id": "2408.11943v1",
    "title": "Advances in Preference-based Reinforcement Learning: A Review",
    "authors": [
      "Youssef Abdelkareem",
      "Shady Shehata",
      "Fakhri Karray"
    ],
    "abstract": "Reinforcement Learning (RL) algorithms suffer from the dependency on\naccurately engineered reward functions to properly guide the learning agents to\ndo the required tasks. Preference-based reinforcement learning (PbRL) addresses\nthat by utilizing human preferences as feedback from the experts instead of\nnumeric rewards. Due to its promising advantage over traditional RL, PbRL has\ngained more focus in recent years with many significant advances. In this\nsurvey, we present a unified PbRL framework to include the newly emerging\napproaches that improve the scalability and efficiency of PbRL. In addition, we\ngive a detailed overview of the theoretical guarantees and benchmarking work\ndone in the field, while presenting its recent applications in complex\nreal-world tasks. Lastly, we go over the limitations of the current approaches\nand the proposed future research directions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11943v1",
    "published_date": "2024-08-21 18:57:12 UTC",
    "updated_date": "2024-08-21 18:57:12 UTC"
  },
  {
    "arxiv_id": "2408.11939v2",
    "title": "Matmul or No Matmul in the Era of 1-bit LLMs",
    "authors": [
      "Jinendra Malekar",
      "Mohammed E. Elbtity",
      "Ramtin Zand"
    ],
    "abstract": "The advent of 1-bit large language models (LLMs) has attracted considerable\nattention and opened up new research opportunities. However, 1-bit LLMs only\nimprove a fraction of models by applying extreme quantization to the projection\nlayers while leaving attention heads unchanged. Therefore, to avoid\nfundamentally wrong choices of goals in future research, it is crucial to\nunderstand the actual improvements in computation and memory usage that 1-bit\nLLMs can deliver. In this work, we present an adaptation of Amdahl's Law\ntailored for the 1-bit LLM context, which illustrates how partial improvements\nin 1-bit LLMs impact overall model performance. Through extensive experiments,\nwe uncover key nuances across different model architectures and hardware\nconfigurations, offering a roadmap for future research in the era of 1-bit\nLLMs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Fixed Typo in title, Fixed typo in author name, fixed typo in\n  amdhal's law para",
    "pdf_url": "http://arxiv.org/pdf/2408.11939v2",
    "published_date": "2024-08-21 18:44:21 UTC",
    "updated_date": "2024-08-28 19:51:04 UTC"
  },
  {
    "arxiv_id": "2408.12629v2",
    "title": "Continual Gesture Learning without Data via Synthetic Feature Sampling",
    "authors": [
      "Zhenyu Lu",
      "Hao Tang"
    ],
    "abstract": "Data-Free Class Incremental Learning (DFCIL) aims to enable models to\ncontinuously learn new classes while retraining knowledge of old classes, even\nwhen the training data for old classes is unavailable. Although explored\nprimarily with image datasets by researchers, this study focuses on\ninvestigating DFCIL for skeleton-based gesture classification due to its\nsignificant real-world implications, particularly considering the growing\nprevalence of VR/AR headsets where gestures serve as the primary means of\ncontrol and interaction. In this work, we made an intriguing observation:\nskeleton models trained with base classes(even very limited) demonstrate strong\ngeneralization capabilities to unseen classes without requiring additional\ntraining. Building on this insight, we developed Synthetic Feature Replay (SFR)\nthat can sample synthetic features from class prototypes to replay for old\nclasses and augment for new classes (under a few-shot setting). Our proposed\nmethod showcases significant advancements over the state-of-the-art, achieving\nup to 15% enhancements in mean accuracy across all steps and largely mitigating\nthe accuracy imbalance between base classes and new classes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12629v2",
    "published_date": "2024-08-21 18:44:15 UTC",
    "updated_date": "2025-03-19 20:54:43 UTC"
  },
  {
    "arxiv_id": "2408.11936v1",
    "title": "Estimating Contribution Quality in Online Deliberations Using a Large Language Model",
    "authors": [
      "Lodewijk Gelauff",
      "Mohak Goyal",
      "Bhargav Dindukurthi",
      "Ashish Goel",
      "Alice Siu"
    ],
    "abstract": "Deliberation involves participants exchanging knowledge, arguments, and\nperspectives and has been shown to be effective at addressing polarization. The\nStanford Online Deliberation Platform facilitates large-scale deliberations. It\nenables video-based online discussions on a structured agenda for small groups\nwithout requiring human moderators. This paper's data comes from various\ndeliberation events, including one conducted in collaboration with Meta in 32\ncountries, and another with 38 post-secondary institutions in the US.\n  Estimating the quality of contributions in a conversation is crucial for\nassessing feature and intervention impacts. Traditionally, this is done by\nhuman annotators, which is time-consuming and costly. We use a large language\nmodel (LLM) alongside eight human annotators to rate contributions based on\njustification, novelty, expansion of the conversation, and potential for\nfurther expansion, with scores ranging from 1 to 5. Annotators also provide\nbrief justifications for their ratings. Using the average rating from other\nhuman annotators as the ground truth, we find the model outperforms individual\nhuman annotators. While pairs of human annotators outperform the model in\nrating justification and groups of three outperform it on all four metrics, the\nmodel remains competitive.\n  We illustrate the usefulness of the automated quality rating by assessing the\neffect of nudges on the quality of deliberation. We first observe that\nindividual nudges after prolonged inactivity are highly effective, increasing\nthe likelihood of the individual requesting to speak in the next 30 seconds by\n65%. Using our automated quality estimation, we show that the quality ratings\nfor statements prompted by nudging are similar to those made without nudging,\nsignifying that nudging leads to more ideas being generated in the conversation\nwithout losing overall quality.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "I.2.1; J.5; H.5.3"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11936v1",
    "published_date": "2024-08-21 18:41:32 UTC",
    "updated_date": "2024-08-21 18:41:32 UTC"
  },
  {
    "arxiv_id": "2408.11935v1",
    "title": "Explainable Anomaly Detection: Counterfactual driven What-If Analysis",
    "authors": [
      "Logan Cummins",
      "Alexander Sommers",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Maria Seale",
      "Joseph Jaboure",
      "Thomas Arnold"
    ],
    "abstract": "There exists three main areas of study inside of the field of predictive\nmaintenance: anomaly detection, fault diagnosis, and remaining useful life\nprediction. Notably, anomaly detection alerts the stakeholder that an anomaly\nis occurring. This raises two fundamental questions: what is causing the fault\nand how can we fix it? Inside of the field of explainable artificial\nintelligence, counterfactual explanations can give that information in the form\nof what changes to make to put the data point into the opposing class, in this\ncase \"healthy\". The suggestions are not always actionable which may raise the\ninterest in asking \"what if we do this instead?\" In this work, we provide a\nproof of concept for utilizing counterfactual explanations as what-if analysis.\nWe perform this on the PRONOSTIA dataset with a temporal convolutional network\nas the anomaly detector. Our method presents the counterfactuals in the form of\na what-if analysis for this base problem to inspire future work for more\ncomplex systems and scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 6 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.11935v1",
    "published_date": "2024-08-21 18:38:59 UTC",
    "updated_date": "2024-08-21 18:38:59 UTC"
  },
  {
    "arxiv_id": "2408.11925v1",
    "title": "An Open Knowledge Graph-Based Approach for Mapping Concepts and Requirements between the EU AI Act and International Standards",
    "authors": [
      "Julio Hernandez",
      "Delaram Golpayegani",
      "Dave Lewis"
    ],
    "abstract": "The many initiatives on trustworthy AI result in a confusing and multipolar\nlandscape that organizations operating within the fluid and complex\ninternational value chains must navigate in pursuing trustworthy AI. The EU's\nAI Act will now shift the focus of such organizations toward conformance with\nthe technical requirements for regulatory compliance, for which the Act relies\non Harmonized Standards. Though a high-level mapping to the Act's requirements\nwill be part of such harmonization, determining the degree to which standards\nconformity delivers regulatory compliance with the AI Act remains a complex\nchallenge. Variance and gaps in the definitions of concepts and how they are\nused in requirements between the Act and harmonized standards may impact the\nconsistency of compliance claims across organizations, sectors, and\napplications. This may present regulatory uncertainty, especially for SMEs and\npublic sector bodies relying on standards conformance rather than proprietary\nequivalents for developing and deploying compliant high-risk AI systems. To\naddress this challenge, this paper offers a simple and repeatable mechanism for\nmapping the terms and requirements relevant to normative statements in\nregulations and standards, e.g., AI Act and ISO management system standards,\ntexts into open knowledge graphs. This representation is used to assess the\nadequacy of standards conformance to regulatory compliance and thereby provide\na basis for identifying areas where further technical consensus development in\ntrustworthy AI value chains is required to achieve regulatory compliance.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "This work was presented at the 9th International Symposium on\n  Language & Knowledge Engineering (LKE 2024) Dublin, Ireland, 4 - 6 June, 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11925v1",
    "published_date": "2024-08-21 18:21:09 UTC",
    "updated_date": "2024-08-21 18:21:09 UTC"
  },
  {
    "arxiv_id": "2408.11918v1",
    "title": "Neural Symbolic Logical Rule Learner for Interpretable Learning",
    "authors": [
      "Bowen Wei",
      "Ziwei Zhu"
    ],
    "abstract": "Rule-based neural networks stand out for enabling interpretable\nclassification by learning logical rules for both prediction and\ninterpretation. However, existing models often lack flexibility due to the\nfixed model structure. Addressing this, we introduce the Normal Form Rule\nLearner (NFRL) algorithm, leveraging a selective discrete neural network, that\ntreat weight parameters as hard selectors, to learn rules in both Conjunctive\nNormal Form (CNF) and Disjunctive Normal Form (DNF) for enhanced accuracy and\ninterpretability. Instead of adopting a deep, complex structure, the NFRL\nincorporates two specialized Normal Form Layers (NFLs) with adaptable AND/OR\nneurons, a Negation Layer for input negations, and a Normal Form Constraint\n(NFC) to streamline neuron connections. We also show the novel network\narchitecture can be optimized using adaptive gradient update together with\nStraight-Through Estimator to overcome the gradient vanishing challenge.\nThrough extensive experiments on 11 datasets, NFRL demonstrates superior\nclassification performance, quality of learned rules, efficiency and\ninterpretability compared to 12 state-of-the-art alternatives. Code and data\nare available at \\url{https://anonymous.4open.science/r/NFRL-27B4/}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 62 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.11918v1",
    "published_date": "2024-08-21 18:09:12 UTC",
    "updated_date": "2024-08-21 18:09:12 UTC"
  },
  {
    "arxiv_id": "2408.11910v1",
    "title": "Why am I Still Seeing This: Measuring the Effectiveness Of Ad Controls and Explanations in AI-Mediated Ad Targeting Systems",
    "authors": [
      "Jane Castleman",
      "Aleksandra Korolova"
    ],
    "abstract": "Recently, Meta has shifted towards AI-mediated ad targeting mechanisms that\ndo not require advertisers to provide detailed targeting criteria, likely\ndriven by excitement over AI capabilities as well as new data privacy policies\nand targeting changes agreed upon in civil rights settlements. At the same\ntime, Meta has touted their ad preference controls as an effective mechanism\nfor users to control the ads they see. Furthermore, Meta markets their\ntargeting explanations as a transparency tool that allows users to understand\nwhy they saw certain ads and inform actions to control future ads.\n  Our study evaluates the effectiveness of Meta's \"See less\" ad control and the\nactionability of ad targeting explanations following the shift to AI-mediated\ntargeting. We conduct a large-scale study, randomly assigning participants to\nmark \"See less\" to Body Weight Control or Parenting topics, and collecting the\nads and targeting explanations Meta shows to participants before and after the\nintervention. We find that utilizing the \"See less\" ad control for the topics\nwe study does not significantly reduce the number of ads shown by Meta on these\ntopics, and that the control is less effective for some users whose\ndemographics are correlated with the topic. Furthermore, we find that the\nmajority of ad targeting explanations for local ads made no reference to\nlocation-specific targeting criteria, and did not inform users why ads related\nto the topics they marked to \"See less\" of continued to be delivered. We\nhypothesize that the poor effectiveness of controls and lack of actionability\nin explanations are the result of the shift to AI-mediated targeting, for which\nexplainability and transparency tools have not yet been developed. Our work\nthus provides evidence for the need of new methods for transparency and user\ncontrol, suitable and reflective of increasingly complex AI-mediated ad\ndelivery systems.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted to the 7th AAAI Conference on AI, Ethics, and Society (AIES,\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.11910v1",
    "published_date": "2024-08-21 18:03:11 UTC",
    "updated_date": "2024-08-21 18:03:11 UTC"
  },
  {
    "arxiv_id": "2408.11816v3",
    "title": "Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction",
    "authors": [
      "Anthony GX-Chen",
      "Kenneth Marino",
      "Rob Fergus"
    ],
    "abstract": "In the face of difficult exploration problems in reinforcement learning, we\nstudy whether giving an agent an object-centric mapping (describing a set of\nitems and their attributes) allow for more efficient learning. We found this\nproblem is best solved hierarchically by modelling items at a higher level of\nstate abstraction to pixels, and attribute change at a higher level of temporal\nabstraction to primitive actions. This abstraction simplifies the transition\ndynamic by making specific future states easier to predict. We make use of this\nto propose a fully model-based algorithm that learns a discriminative world\nmodel, plans to explore efficiently with only a count-based intrinsic reward,\nand can subsequently plan to reach any discovered (abstract) states.\n  We demonstrate the model's ability to (i) efficiently solve single tasks,\n(ii) transfer zero-shot and few-shot across item types and environments, and\n(iii) plan across long horizons. Across a suite of 2D crafting and MiniHack\nenvironments, we empirically show our model significantly out-performs\nstate-of-the-art low-level methods (without abstraction), as well as performant\nmodel-free and model-based methods using the same abstraction. Finally, we show\nhow to learn low level object-perturbing policies via reinforcement learning,\nand the object mapping itself by supervised learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.11816v3",
    "published_date": "2024-08-21 17:59:31 UTC",
    "updated_date": "2025-04-12 14:17:14 UTC"
  },
  {
    "arxiv_id": "2408.11815v1",
    "title": "Great Memory, Shallow Reasoning: Limits of $k$NN-LMs",
    "authors": [
      "Shangyi Geng",
      "Wenting Zhao",
      "Alexander M Rush"
    ],
    "abstract": "$K$-nearest neighbor language models ($k$NN-LMs), which integrate retrieval\nwith next-word prediction, have demonstrated strong performance in language\nmodeling as well as downstream NLP benchmarks. These results have led\nresearchers to argue that models trained on poor quality or outdated data could\nperform well by employing a $k$NN extension that has access to a higher-quality\ndatastore. In this work, we ask whether this improved ability to recall\ninformation really translates into downstream abilities. We extensively\nevaluate $k$NN-LMs on a diverse set of tasks, ranging from sentiment\nclassification and commonsense reasoning to multi-hop reasoning. Results show\nthat $k$NN-LMs excel at memory-intensive tasks, where utilizing the patterns in\nthe input is sufficient for determining the output, but struggle with reasoning\ntasks that require integrating multiple pieces of information to derive new\nknowledge. We further demonstrate through oracle experiments and qualitative\nanalysis that even with perfect retrieval, $k$NN-LMs still fail to determine\nthe correct answers, placing an upper bound on their reasoning performance.\nCode and datastores are released at https://github.com/GSYfate/knnlm-limits/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11815v1",
    "published_date": "2024-08-21 17:59:05 UTC",
    "updated_date": "2024-08-21 17:59:05 UTC"
  },
  {
    "arxiv_id": "2408.11804v1",
    "title": "Approaching Deep Learning through the Spectral Dynamics of Weights",
    "authors": [
      "David Yunis",
      "Kumar Kshitij Patel",
      "Samuel Wheeler",
      "Pedro Savarese",
      "Gal Vardi",
      "Karen Livescu",
      "Michael Maire",
      "Matthew R. Walter"
    ],
    "abstract": "We propose an empirical approach centered on the spectral dynamics of weights\n-- the behavior of singular values and vectors during optimization -- to unify\nand clarify several phenomena in deep learning. We identify a consistent bias\nin optimization across various experiments, from small-scale ``grokking'' to\nlarge-scale tasks like image classification with ConvNets, image generation\nwith UNets, speech recognition with LSTMs, and language modeling with\nTransformers. We also demonstrate that weight decay enhances this bias beyond\nits role as a norm regularizer, even in practical systems. Moreover, we show\nthat these spectral dynamics distinguish memorizing networks from generalizing\nones, offering a novel perspective on this longstanding conundrum.\nAdditionally, we leverage spectral dynamics to explore the emergence of\nwell-performing sparse subnetworks (lottery tickets) and the structure of the\nloss surface through linear mode connectivity. Our findings suggest that\nspectral dynamics provide a coherent framework to better understand the\nbehavior of neural networks across diverse settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11804v1",
    "published_date": "2024-08-21 17:48:01 UTC",
    "updated_date": "2024-08-21 17:48:01 UTC"
  },
  {
    "arxiv_id": "2408.11796v4",
    "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
    "authors": [
      "Sharath Turuvekere Sreenivas",
      "Saurav Muralidharan",
      "Raviraj Joshi",
      "Marcin Chochowski",
      "Ameya Sunil Mahabaleshwarkar",
      "Gerald Shen",
      "Jiaqi Zeng",
      "Zijia Chen",
      "Yoshi Suhara",
      "Shizhe Diao",
      "Chenhan Yu",
      "Wei-Chun Chen",
      "Hayley Ross",
      "Oluwatobi Olabiyi",
      "Ashwath Aithal",
      "Oleksii Kuchaiev",
      "Daniel Korzekwa",
      "Pavlo Molchanov",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Jan Kautz",
      "Bryan Catanzaro"
    ],
    "abstract": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "v4: Update author order",
    "pdf_url": "http://arxiv.org/pdf/2408.11796v4",
    "published_date": "2024-08-21 17:38:48 UTC",
    "updated_date": "2024-12-09 18:31:01 UTC"
  },
  {
    "arxiv_id": "2408.11793v2",
    "title": "Leveraging Chemistry Foundation Models to Facilitate Structure Focused Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and Materials Design",
    "authors": [
      "Nathaniel H. Park",
      "Tiffany J. Callahan",
      "James L. Hedrick",
      "Tim Erdmann",
      "Sara Capponi"
    ],
    "abstract": "Molecular property prediction and generative design via deep learning models\nhas been the subject of intense research given its potential to accelerate\ndevelopment of new, high-performance materials. More recently, these workflows\nhave been significantly augmented with the advent of large language models\n(LLMs) and systems of autonomous agents capable of utilizing pre-trained models\nto make predictions in the context of more complex research tasks. While\neffective, there is still room for substantial improvement within agentic\nsystems on the retrieval of salient information for material design tasks.\nWithin this context, alternative uses of predictive deep learning models, such\nas leveraging their latent representations to facilitate cross-modal retrieval\naugmented generation within agentic systems for task-specific materials design,\nhas remained unexplored. Herein, we demonstrate that large, pre-trained\nchemistry foundation models can serve as a basis for enabling\nstructure-focused, semantic chemistry information retrieval for both\nsmall-molecules, complex polymeric materials, and reactions. Additionally, we\nshow the use of chemistry foundation models in conjunction with multi-modal\nmodels such as OpenCLIP facilitate unprecedented queries and information\nretrieval across multiple characterization data domains. Finally, we\ndemonstrate the integration of these models within multi-agent systems to\nfacilitate structure and topological-based natural language queries and\ninformation retrieval for different research tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11793v2",
    "published_date": "2024-08-21 17:25:45 UTC",
    "updated_date": "2024-12-13 01:11:25 UTC"
  },
  {
    "arxiv_id": "2408.11788v1",
    "title": "DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework",
    "authors": [
      "Zhifei Xie",
      "Daniel Tang",
      "Dingwei Tan",
      "Jacques Klein",
      "Tegawend F. Bissyand",
      "Saad Ezzini"
    ],
    "abstract": "Current video generation models excel at creating short, realistic clips, but\nstruggle with longer, multi-scene videos. We introduce \\texttt{DreamFactory},\nan LLM-based framework that tackles this challenge. \\texttt{DreamFactory}\nleverages multi-agent collaboration principles and a Key Frames Iteration\nDesign Method to ensure consistency and style across long videos. It utilizes\nChain of Thought (COT) to address uncertainties inherent in large language\nmodels. \\texttt{DreamFactory} generates long, stylistically coherent, and\ncomplex videos. Evaluating these long-form videos presents a challenge. We\npropose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene\nStyle Consistency Score. To further research in this area, we contribute the\nMulti-Scene Videos Dataset containing over 150 human-rated videos.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.SE",
      "TsingHua University"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.11788v1",
    "published_date": "2024-08-21 17:21:13 UTC",
    "updated_date": "2024-08-21 17:21:13 UTC"
  },
  {
    "arxiv_id": "2408.11785v1",
    "title": "Timeline and Boundary Guided Diffusion Network for Video Shadow Detection",
    "authors": [
      "Haipeng Zhou",
      "Honqiu Wang",
      "Tian Ye",
      "Zhaohu Xing",
      "Jun Ma",
      "Ping Li",
      "Qiong Wang",
      "Lei Zhu"
    ],
    "abstract": "Video Shadow Detection (VSD) aims to detect the shadow masks with frame\nsequence. Existing works suffer from inefficient temporal learning. Moreover,\nfew works address the VSD problem by considering the characteristic (i.e.,\nboundary) of shadow. Motivated by this, we propose a Timeline and Boundary\nGuided Diffusion (TBGDiff) network for VSD where we take account of the\npast-future temporal guidance and boundary information jointly. In detail, we\ndesign a Dual Scale Aggregation (DSA) module for better temporal understanding\nby rethinking the affinity of the long-term and short-term frames for the\nclipped video. Next, we introduce Shadow Boundary Aware Attention (SBAA) to\nutilize the edge contexts for capturing the characteristics of shadows.\nMoreover, we are the first to introduce the Diffusion model for VSD in which we\nexplore a Space-Time Encoded Embedding (STEE) to inject the temporal guidance\nfor Diffusion to conduct shadow detection. Benefiting from these designs, our\nmodel can not only capture the temporal information but also the shadow\nproperty. Extensive experiments show that the performance of our approach\novertakes the state-of-the-art methods, verifying the effectiveness of our\ncomponents. We release the codes, weights, and results at\n\\url{https://github.com/haipengzhou856/TBGDiff}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ACM MM2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11785v1",
    "published_date": "2024-08-21 17:16:21 UTC",
    "updated_date": "2024-08-21 17:16:21 UTC"
  },
  {
    "arxiv_id": "2408.11778v2",
    "title": "Sum of Squares Circuits",
    "authors": [
      "Lorenzo Loconte",
      "Stefan Mengel",
      "Antonio Vergari"
    ],
    "abstract": "Designing expressive generative models that support exact and efficient\ninference is a core question in probabilistic ML. Probabilistic circuits (PCs)\noffer a framework where this tractability-vs-expressiveness trade-off can be\nanalyzed theoretically. Recently, squared PCs encoding subtractive mixtures via\nnegative parameters have emerged as tractable models that can be exponentially\nmore expressive than monotonic PCs, i.e., PCs with positive parameters only. In\nthis paper, we provide a more precise theoretical characterization of the\nexpressiveness relationships among these models. First, we prove that squared\nPCs can be less expressive than monotonic ones. Second, we formalize a novel\nclass of PCs -- sum of squares PCs -- that can be exponentially more expressive\nthan both squared and monotonic PCs. Around sum of squares PCs, we build an\nexpressiveness hierarchy that allows us to precisely unify and separate\ndifferent tractable model classes such as Born Machines and PSD models, and\nother recently introduced tractable probabilistic models by using complex\nparameters. Finally, we empirically show the effectiveness of sum of squares\ncircuits in performing distribution estimation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CC",
      "math.AG"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11778v2",
    "published_date": "2024-08-21 17:08:05 UTC",
    "updated_date": "2024-12-19 13:34:56 UTC"
  },
  {
    "arxiv_id": "2408.11761v1",
    "title": "D-RMGPT: Robot-assisted collaborative tasks driven by large multimodal models",
    "authors": [
      "M. Forlini",
      "M. Babcinschi",
      "G. Palmieri",
      "P. Neto"
    ],
    "abstract": "Collaborative robots are increasingly popular for assisting humans at work\nand daily tasks. However, designing and setting up interfaces for human-robot\ncollaboration is challenging, requiring the integration of multiple components,\nfrom perception and robot task control to the hardware itself. Frequently, this\nleads to highly customized solutions that rely on large amounts of costly\ntraining data, diverging from the ideal of flexible and general interfaces that\nempower robots to perceive and adapt to unstructured environments where they\ncan naturally collaborate with humans. To overcome these challenges, this paper\npresents the Detection-Robot Management GPT (D-RMGPT), a robot-assisted\nassembly planner based on Large Multimodal Models (LMM). This system can assist\ninexperienced operators in assembly tasks without requiring any markers or\nprevious training. D-RMGPT is composed of DetGPT-V and R-ManGPT. DetGPT-V,\nbased on GPT-4V(vision), perceives the surrounding environment through one-shot\nanalysis of prompted images of the current assembly stage and the list of\ncomponents to be assembled. It identifies which components have already been\nassembled by analysing their features and assembly requirements. R-ManGPT,\nbased on GPT-4, plans the next component to be assembled and generates the\nrobot's discrete actions to deliver it to the human co-worker. Experimental\ntests on assembling a toy aircraft demonstrated that D-RMGPT is flexible and\nintuitive to use, achieving an assembly success rate of 83% while reducing the\nassembly time for inexperienced operators by 33% compared to the manual\nprocess. http://robotics-and-ai.github.io/LMMmodels/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11761v1",
    "published_date": "2024-08-21 16:34:21 UTC",
    "updated_date": "2024-08-21 16:34:21 UTC"
  },
  {
    "arxiv_id": "2408.11760v3",
    "title": "R2Det: Exploring Relaxed Rotation Equivariance in 2D object detection",
    "authors": [
      "Zhiqiang Wu",
      "Yingjie Liu",
      "Hanlin Dong",
      "Xuan Tang",
      "Jian Yang",
      "Bo Jin",
      "Mingsong Chen",
      "Xian Wei"
    ],
    "abstract": "Group Equivariant Convolution (GConv) empowers models to explore underlying\nsymmetry in data, improving performance. However, real-world scenarios often\ndeviate from ideal symmetric systems caused by physical permutation,\ncharacterized by non-trivial actions of a symmetry group, resulting in\nasymmetries that affect the outputs, a phenomenon known as Symmetry Breaking.\nTraditional GConv-based methods are constrained by rigid operational rules\nwithin group space, assuming data remains strictly symmetry after limited group\ntransformations. This limitation makes it difficult to adapt to\nSymmetry-Breaking and non-rigid transformations. Motivated by this, we mainly\nfocus on a common scenario: Rotational Symmetry-Breaking. By relaxing strict\ngroup transformations within Strict Rotation-Equivariant group $\\mathbf{C}_n$,\nwe redefine a Relaxed Rotation-Equivariant group $\\mathbf{R}_n$ and introduce a\nnovel Relaxed Rotation-Equivariant GConv (R2GConv) with only a minimal increase\nof $4n$ parameters compared to GConv. Based on R2GConv, we propose a Relaxed\nRotation-Equivariant Network (R2Net) as the backbone and develop a Relaxed\nRotation-Equivariant Object Detector (R2Det) for 2D object detection.\nExperimental results demonstrate the effectiveness of the proposed R2GConv in\nnatural image classification, and R2Det achieves excellent performance in 2D\nobject detection with improved generalization capabilities and robustness. The\ncode is available in \\texttt{https://github.com/wuer5/r2det}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11760v3",
    "published_date": "2024-08-21 16:32:03 UTC",
    "updated_date": "2025-03-04 14:04:07 UTC"
  },
  {
    "arxiv_id": "2408.11754v1",
    "title": "Improving the Scan-rescan Precision of AI-based CMR Biomarker Estimation",
    "authors": [
      "Dewmini Hasara Wickremasinghe",
      "Yiyang Xu",
      "Esther Puyol-Antón",
      "Paul Aljabar",
      "Reza Razavi",
      "Andrew P. King"
    ],
    "abstract": "Quantification of cardiac biomarkers from cine cardiovascular magnetic\nresonance (CMR) data using deep learning (DL) methods offers many advantages,\nsuch as increased accuracy and faster analysis. However, only a few studies\nhave focused on the scan-rescan precision of the biomarker estimates, which is\nimportant for reproducibility and longitudinal analysis. Here, we propose a\ncardiac biomarker estimation pipeline that not only focuses on achieving high\nsegmentation accuracy but also on improving the scan-rescan precision of the\ncomputed biomarkers, namely left and right ventricular ejection fraction, and\nleft ventricular myocardial mass. We evaluate two approaches to improve the\napical-basal resolution of the segmentations used for estimating the\nbiomarkers: one based on image interpolation and one based on segmentation\ninterpolation. Using a database comprising scan-rescan cine CMR data acquired\nfrom 92 subjects, we compare the performance of these two methods against\nground truth (GT) segmentations and DL segmentations obtained before\ninterpolation (baseline). The results demonstrate that both the image-based and\nsegmentation-based interpolation methods were able to narrow Bland-Altman\nscan-rescan confidence intervals for all biomarkers compared to the GT and\nbaseline performances. Our findings highlight the importance of focusing not\nonly on segmentation accuracy but also on the consistency of biomarkers across\nrepeated scans, which is crucial for longitudinal analysis of cardiac function.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "q-bio.QM",
    "comment": "11 pages, 3 figures, MICCAI STACOM 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11754v1",
    "published_date": "2024-08-21 16:24:27 UTC",
    "updated_date": "2024-08-21 16:24:27 UTC"
  },
  {
    "arxiv_id": "2408.11747v1",
    "title": "Open-Ended 3D Point Cloud Instance Segmentation",
    "authors": [
      "Phuc D. A. Nguyen",
      "Minh Luu",
      "Anh Tran",
      "Cuong Pham",
      "Khoi Nguyen"
    ],
    "abstract": "Open-Vocab 3D Instance Segmentation methods (OV-3DIS) have recently\ndemonstrated their ability to generalize to unseen objects. However, these\nmethods still depend on predefined class names during testing, restricting the\nautonomy of agents. To mitigate this constraint, we propose a novel problem\ntermed Open-Ended 3D Instance Segmentation (OE-3DIS), which eliminates the\nnecessity for predefined class names during testing. Moreover, we contribute a\ncomprehensive set of strong baselines, derived from OV-3DIS approaches and\nleveraging 2D Multimodal Large Language Models. To assess the performance of\nour OE-3DIS system, we introduce a novel Open-Ended score, evaluating both the\nsemantic and geometric quality of predicted masks and their associated class\nnames, alongside the standard AP score. Our approach demonstrates significant\nperformance improvements over the baselines on the ScanNet200 and ScanNet++\ndatasets. Remarkably, our method surpasses the performance of Open3DIS, the\ncurrent state-of-the-art method in OV-3DIS, even in the absence of ground-truth\nobject class names.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11747v1",
    "published_date": "2024-08-21 16:14:11 UTC",
    "updated_date": "2024-08-21 16:14:11 UTC"
  },
  {
    "arxiv_id": "2408.11745v2",
    "title": "FocusLLM: Precise Understanding of Long Context by Dynamic Condensing",
    "authors": [
      "Zhenyu Li",
      "Yike Zhang",
      "Tengyu Pan",
      "Yutao Sun",
      "Zhichao Duan",
      "Junjie Fang",
      "Rong Han",
      "Zixuan Wang",
      "Jianyong Wang"
    ],
    "abstract": "Empowering LLMs with the ability to precisely understand long contexts is\ncrucial for many downstream applications. However, handling long contexts with\nconventional transformer architecture requires substantial training and\ninference resources. Existing context condensing methods cannot accurately\nunderstand the full context, as there is a considerable amount of information\nloss in the condensing process. To address these issues, we present FocusLLM, a\nframework designed to extend the fixed context length of any decoder-only LLM,\nallowing the model to focus on relevant information from very long sequences.\nFocusLLM first divides long text input into chunks based on the model's\noriginal context length. It then employs the dynamic condensing process to\ndistill crucial information from each chunk. Ultimately, through the novel\nparallel decoding mechanism, FocusLLM can integrate the extracted information\ninto its local context. FocusLLM stands out for great training efficiency and\nversatility: trained with an 8K input length and with much less training cost\nthan previous methods, FocusLLM exhibits superior performance across downstream\ntasks and maintains strong language modeling ability when handling extensive\nlong texts, even up to 400K tokens. Our code is available at\nhttps://github.com/leezythu/FocusLLM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11745v2",
    "published_date": "2024-08-21 16:11:59 UTC",
    "updated_date": "2024-12-23 15:36:32 UTC"
  },
  {
    "arxiv_id": "2408.11744v1",
    "title": "JieHua Paintings Style Feature Extracting Model using Stable Diffusion with ControlNet",
    "authors": [
      "Yujia Gu",
      "Haofeng Li",
      "Xinyu Fang",
      "Zihan Peng",
      "Yinan Peng"
    ],
    "abstract": "This study proposes a novel approach to extract stylistic features of Jiehua:\nthe utilization of the Fine-tuned Stable Diffusion Model with ControlNet\n(FSDMC) to refine depiction techniques from artists' Jiehua. The training data\nfor FSDMC is based on the opensource Jiehua artist's work collected from the\nInternet, which were subsequently manually constructed in the format of\n(Original Image, Canny Edge Features, Text Prompt). By employing the optimal\nhyperparameters identified in this paper, it was observed FSDMC outperforms\nCycleGAN, another mainstream style transfer model. FSDMC achieves FID of 3.27\non the dataset and also surpasses CycleGAN in terms of expert evaluation. This\nnot only demonstrates the model's high effectiveness in extracting Jiehua's\nstyle features, but also preserves the original pre-trained semantic\ninformation. The findings of this study suggest that the application of FSDMC\nwith appropriate hyperparameters can enhance the efficacy of the Stable\nDiffusion Model in the field of traditional art style migration tasks,\nparticularly within the context of Jiehua.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted by ICCSMT 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11744v1",
    "published_date": "2024-08-21 16:11:01 UTC",
    "updated_date": "2024-08-21 16:11:01 UTC"
  },
  {
    "arxiv_id": "2408.11742v1",
    "title": "CluMo: Cluster-based Modality Fusion Prompt for Continual Learning in Visual Question Answering",
    "authors": [
      "Yuliang Cai",
      "Mohammad Rostami"
    ],
    "abstract": "Large vision-language models (VLMs) have shown significant performance boost\nin various application domains. However, adopting them to deal with several\nsequentially encountered tasks has been challenging because finetuning a VLM on\na task normally leads to reducing its generalization power and the capacity of\nlearning new tasks as well as causing catastrophic forgetting on previously\nlearned tasks. Enabling using VLMs in multimodal continual learning (CL)\nsettings can help to address such scenarios. To improve generalization capacity\nand prevent catastrophic forgetting, we propose a novel prompt-based CL method\nfor VLMs, namely $\\textbf{Clu}$ster-based $\\textbf{Mo}$dality Fusion Prompt\n(\\textbf{CluMo}). We design a novel \\textbf{Key-Key-Prompt} pair, where each\nprompt is associated with a visual prompt key and a textual prompt key. We\nadopt a two-stage training strategy. During the first stage, the single-modal\nkeys are trained via $K$-means clustering algorithm to help select the best\nsemantically matched prompt. During the second stage, the prompt keys are\nfrozen, the selected prompt is attached to the input for training the VLM in\nthe CL scenario. Experiments on two benchmarks demonstrate that our method\nachieves SOTA performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11742v1",
    "published_date": "2024-08-21 16:07:49 UTC",
    "updated_date": "2024-08-21 16:07:49 UTC"
  },
  {
    "arxiv_id": "2408.11735v3",
    "title": "Clinical Insights: A Comprehensive Review of Language Models in Medicine",
    "authors": [
      "Nikita Neveditsin",
      "Pawan Lingras",
      "Vijay Mago"
    ],
    "abstract": "This paper explores the advancements and applications of language models in\nhealthcare, focusing on their clinical use cases. It examines the evolution\nfrom early encoder-based systems requiring extensive fine-tuning to\nstate-of-the-art large language and multimodal models capable of integrating\ntext and visual data through in-context learning. The analysis emphasizes\nlocally deployable models, which enhance data privacy and operational autonomy,\nand their applications in tasks such as text generation, classification,\ninformation extraction, and conversational systems. The paper also highlights a\nstructured organization of tasks and a tiered ethical approach, providing a\nvaluable resource for researchers and practitioners, while discussing key\nchallenges related to ethics, evaluation, and implementation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to PLOS Digital Health, Revision 1",
    "pdf_url": "http://arxiv.org/pdf/2408.11735v3",
    "published_date": "2024-08-21 15:59:33 UTC",
    "updated_date": "2025-01-07 17:34:04 UTC"
  },
  {
    "arxiv_id": "2408.11727v2",
    "title": "Efficient Detection of Toxic Prompts in Large Language Models",
    "authors": [
      "Yi Liu",
      "Junzhe Yu",
      "Huijia Sun",
      "Ling Shi",
      "Gelei Deng",
      "Yuqi Chen",
      "Yang Liu"
    ],
    "abstract": "Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.11727v2",
    "published_date": "2024-08-21 15:54:04 UTC",
    "updated_date": "2024-09-14 02:04:15 UTC"
  },
  {
    "arxiv_id": "2408.11721v1",
    "title": "Iterative Object Count Optimization for Text-to-image Diffusion Models",
    "authors": [
      "Oz Zafar",
      "Lior Wolf",
      "Idan Schwartz"
    ],
    "abstract": "We address a persistent challenge in text-to-image models: accurately\ngenerating a specified number of objects. Current models, which learn from\nimage-text pairs, inherently struggle with counting, as training data cannot\ndepict every possible number of objects for any given object. To solve this, we\npropose optimizing the generated image based on a counting loss derived from a\ncounting model that aggregates an object\\'s potential. Employing an\nout-of-the-box counting model is challenging for two reasons: first, the model\nrequires a scaling hyperparameter for the potential aggregation that varies\ndepending on the viewpoint of the objects, and second, classifier guidance\ntechniques require modified models that operate on noisy intermediate diffusion\nsteps. To address these challenges, we propose an iterated online training mode\nthat improves the accuracy of inferred images while altering the text\nconditioning embedding and dynamically adjusting hyperparameters. Our method\noffers three key advantages: (i) it can consider non-derivable counting\ntechniques based on detection models, (ii) it is a zero-shot plug-and-play\nsolution facilitating rapid changes to the counting techniques and image\ngeneration methods, and (iii) the optimized counting token can be reused to\ngenerate accurate images without additional optimization. We evaluate the\ngeneration of various objects and show significant improvements in accuracy.\nThe project page is available at https://ozzafar.github.io/count_token.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Pre-print",
    "pdf_url": "http://arxiv.org/pdf/2408.11721v1",
    "published_date": "2024-08-21 15:51:46 UTC",
    "updated_date": "2024-08-21 15:51:46 UTC"
  },
  {
    "arxiv_id": "2408.11710v1",
    "title": "Leveraging Large Language Models for Enhancing the Understandability of Generated Unit Tests",
    "authors": [
      "Amirhossein Deljouyi",
      "Roham Koohestani",
      "Maliheh Izadi",
      "Andy Zaidman"
    ],
    "abstract": "Automated unit test generators, particularly search-based software testing\ntools like EvoSuite, are capable of generating tests with high coverage.\nAlthough these generators alleviate the burden of writing unit tests, they\noften pose challenges for software engineers in terms of understanding the\ngenerated tests. To address this, we introduce UTGen, which combines\nsearch-based software testing and large language models to enhance the\nunderstandability of automatically generated test cases. We achieve this\nenhancement through contextualizing test data, improving identifier naming, and\nadding descriptive comments. Through a controlled experiment with 32\nparticipants from both academia and industry, we investigate how the\nunderstandability of unit tests affects a software engineer's ability to\nperform bug-fixing tasks. We selected bug-fixing to simulate a real-world\nscenario that emphasizes the importance of understandable test cases. We\nobserve that participants working on assignments with UTGen test cases fix up\nto 33% more bugs and use up to 20% less time when compared to baseline test\ncases. From the post-test questionnaire, we gathered that participants found\nthat enhanced test names, test data, and variable names improved their\nbug-fixing process.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "**Note:** This paper has been accepted for presentation at the 47th\n  International Conference on Software Engineering (ICSE 2025) - Research Track",
    "pdf_url": "http://arxiv.org/pdf/2408.11710v1",
    "published_date": "2024-08-21 15:35:34 UTC",
    "updated_date": "2024-08-21 15:35:34 UTC"
  },
  {
    "arxiv_id": "2408.11691v1",
    "title": "Physics-informed Discovery of State Variables in Second-Order and Hamiltonian Systems",
    "authors": [
      "Félix Chavelli",
      "Zi-Yu Khoo",
      "Dawen Wu",
      "Jonathan Sze Choong Low",
      "Stéphane Bressan"
    ],
    "abstract": "The modeling of dynamical systems is a pervasive concern for not only\ndescribing but also predicting and controlling natural phenomena and engineered\nsystems. Current data-driven approaches often assume prior knowledge of the\nrelevant state variables or result in overparameterized state spaces. Boyuan\nChen and his co-authors proposed a neural network model that estimates the\ndegrees of freedom and attempts to discover the state variables of a dynamical\nsystem. Despite its innovative approach, this baseline model lacks a connection\nto the physical principles governing the systems it analyzes, leading to\nunreliable state variables.\n  This research proposes a method that leverages the physical characteristics\nof second-order Hamiltonian systems to constrain the baseline model. The\nproposed model outperforms the baseline model in identifying a minimal set of\nnon-redundant and interpretable state variables.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11691v1",
    "published_date": "2024-08-21 15:10:50 UTC",
    "updated_date": "2024-08-21 15:10:50 UTC"
  },
  {
    "arxiv_id": "2408.11659v1",
    "title": "5G NR PRACH Detection with Convolutional Neural Networks (CNN): Overcoming Cell Interference Challenges",
    "authors": [
      "Desire Guel",
      "Arsene Kabore",
      "Didier Bassole"
    ],
    "abstract": "In this paper, we present a novel approach to interference detection in 5G\nNew Radio (5G-NR) networks using Convolutional Neural Networks (CNN).\nInterference in 5G networks challenges high-quality service due to dense user\nequipment deployment and increased wireless environment complexity. Our\nCNN-based model is designed to detect Physical Random Access Channel (PRACH)\nsequences amidst various interference scenarios, leveraging the spatial and\ntemporal characteristics of PRACH signals to enhance detection accuracy and\nrobustness. Comprehensive datasets of simulated PRACH signals under controlled\ninterference conditions were generated to train and validate the model.\nExperimental results show that our CNN-based approach outperforms traditional\nPRACH detection methods in accuracy, precision, recall and F1-score. This study\ndemonstrates the potential of AI/ML techniques in advancing interference\nmanagement in 5G networks, providing a foundation for future research and\npractical applications in optimizing network performance and reliability.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11659v1",
    "published_date": "2024-08-21 14:33:43 UTC",
    "updated_date": "2024-08-21 14:33:43 UTC"
  },
  {
    "arxiv_id": "2408.11650v2",
    "title": "CIPHER: Cybersecurity Intelligent Penetration-testing Helper for Ethical Researcher",
    "authors": [
      "Derry Pratama",
      "Naufal Suryanto",
      "Andro Aprila Adiputra",
      "Thi-Thu-Huong Le",
      "Ahmada Yusril Kadiptya",
      "Muhammad Iqbal",
      "Howon Kim"
    ],
    "abstract": "Penetration testing, a critical component of cybersecurity, typically\nrequires extensive time and effort to find vulnerabilities. Beginners in this\nfield often benefit from collaborative approaches with the community or\nexperts. To address this, we develop CIPHER (Cybersecurity Intelligent\nPenetration-testing Helper for Ethical Researchers), a large language model\nspecifically trained to assist in penetration testing tasks. We trained CIPHER\nusing over 300 high-quality write-ups of vulnerable machines, hacking\ntechniques, and documentation of open-source penetration testing tools.\nAdditionally, we introduced the Findings, Action, Reasoning, and Results (FARR)\nFlow augmentation, a novel method to augment penetration testing write-ups to\nestablish a fully automated pentesting simulation benchmark tailored for large\nlanguage models. This approach fills a significant gap in traditional\ncybersecurity Q\\&A benchmarks and provides a realistic and rigorous standard\nfor evaluating AI's technical knowledge, reasoning capabilities, and practical\nutility in dynamic penetration testing scenarios. In our assessments, CIPHER\nachieved the best overall performance in providing accurate suggestion\nresponses compared to other open-source penetration testing models of similar\nsize and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B\nChat, particularly on insane difficulty machine setups. This demonstrates that\nthe current capabilities of general LLMs are insufficient for effectively\nguiding users through the penetration testing process. We also discuss the\npotential for improvement through scaling and the development of better\nbenchmarks using FARR Flow augmentation results. Our benchmark will be released\npublicly at https://github.com/ibndias/CIPHER.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "28 pages, github available",
    "pdf_url": "http://arxiv.org/pdf/2408.11650v2",
    "published_date": "2024-08-21 14:24:04 UTC",
    "updated_date": "2024-11-06 06:25:49 UTC"
  },
  {
    "arxiv_id": "2408.11649v1",
    "title": "Video-to-Text Pedestrian Monitoring (VTPM): Leveraging Computer Vision and Large Language Models for Privacy-Preserve Pedestrian Activity Monitoring at Intersections",
    "authors": [
      "Ahmed S. Abdelrahman",
      "Mohamed Abdel-Aty",
      "Dongdong Wang"
    ],
    "abstract": "Computer vision has advanced research methodologies, enhancing system\nservices across various fields. It is a core component in traffic monitoring\nsystems for improving road safety; however, these monitoring systems don't\npreserve the privacy of pedestrians who appear in the videos, potentially\nrevealing their identities. Addressing this issue, our paper introduces\nVideo-to-Text Pedestrian Monitoring (VTPM), which monitors pedestrian movements\nat intersections and generates real-time textual reports, including traffic\nsignal and weather information. VTPM uses computer vision models for pedestrian\ndetection and tracking, achieving a latency of 0.05 seconds per video frame.\nAdditionally, it detects crossing violations with 90.2% accuracy by\nincorporating traffic signal data. The proposed framework is equipped with\nPhi-3 mini-4k to generate real-time textual reports of pedestrian activity\nwhile stating safety concerns like crossing violations, conflicts, and the\nimpact of weather on their behavior with latency of 0.33 seconds. To enhance\ncomprehensive analysis of the generated textual reports, Phi-3 medium is\nfine-tuned for historical analysis of these generated textual reports. This\nfine-tuning enables more reliable analysis about the pedestrian safety at\nintersections, effectively detecting patterns and safety critical events. The\nproposed VTPM offers a more efficient alternative to video footage by using\ntextual reports reducing memory usage, saving up to 253 million percent,\neliminating privacy issues, and enabling comprehensive interactive historical\nanalysis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11649v1",
    "published_date": "2024-08-21 14:21:53 UTC",
    "updated_date": "2024-08-21 14:21:53 UTC"
  },
  {
    "arxiv_id": "2408.11619v3",
    "title": "Data-driven Modeling of Combined Sewer Systems for Urban Sustainability: An Empirical Evaluation",
    "authors": [
      "Vipin Singh",
      "Tianheng Ling",
      "Teodor Chiaburu",
      "Felix Biessmann"
    ],
    "abstract": "Climate change poses complex challenges, with extreme weather events becoming\nincreasingly frequent and difficult to model. Examples include the dynamics of\nCombined Sewer Systems (CSS). Overburdened CSS during heavy rainfall will\noverflow untreated wastewater into surface water bodies. Classical approaches\nto modeling the impact of extreme rainfall events rely on physical simulations,\nwhich are particularly challenging to create for large urban infrastructures.\nDeep Learning (DL) models offer a cost-effective alternative for modeling the\ncomplex dynamics of sewer systems. In this study, we present a comprehensive\nempirical evaluation of several state-of-the-art DL time series models for\npredicting sewer system dynamics in a large urban infrastructure, utilizing\nthree years of measurement data. We especially investigate the potential of DL\nmodels to maintain predictive precision during network outages by comparing\nglobal models, which have access to all variables within the sewer system, and\nlocal models, which are limited to data from a restricted set of local sensors.\nOur findings demonstrate that DL models can accurately predict the dynamics of\nsewer system load, even under network outage conditions. These results suggest\nthat DL models can effectively aid in balancing the load redistribution in CSS,\nthereby enhancing the sustainability and resilience of urban infrastructures.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "8 pages, 4 figures, accepted at 2nd Workshop on 'Public Interest AI'\n  co-located with 47th German Conference on Artificial Intelligence, Wuerzburg\n  23rd September 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11619v3",
    "published_date": "2024-08-21 13:46:58 UTC",
    "updated_date": "2025-02-13 11:10:54 UTC"
  },
  {
    "arxiv_id": "2408.11609v2",
    "title": "Xinyu: An Efficient LLM-based System for Commentary Generation",
    "authors": [
      "Yiquan Wu",
      "Bo Tang",
      "Chenyang Xi",
      "Yu Yu",
      "Pengyu Wang",
      "Yifei Liu",
      "Kun Kuang",
      "Haiying Deng",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Jie Hu",
      "Peng Cheng",
      "Zhonghao Wang",
      "Yi Wang",
      "Yi Luo",
      "Mingchuan Yang"
    ],
    "abstract": "Commentary provides readers with a deep understanding of events by presenting\ndiverse arguments and evidence. However, creating commentary is a\ntime-consuming task, even for skilled commentators. Large language models\n(LLMs) have simplified the process of natural language generation, but their\ndirect application in commentary creation still faces challenges due to unique\ntask requirements. These requirements can be categorized into two levels: 1)\nfundamental requirements, which include creating well-structured and logically\nconsistent narratives, and 2) advanced requirements, which involve generating\nquality arguments and providing convincing evidence. In this paper, we\nintroduce Xinyu, an efficient LLM-based system designed to assist commentators\nin generating Chinese commentaries. To meet the fundamental requirements, we\ndeconstruct the generation process into sequential steps, proposing targeted\nstrategies and supervised fine-tuning (SFT) for each step. To address the\nadvanced requirements, we present an argument ranking model for arguments and\nestablish a comprehensive evidence database that includes up-to-date events and\nclassic books, thereby strengthening the substantiation of the evidence with\nretrieval augmented generation (RAG) technology. To evaluate the generated\ncommentaries more fairly, corresponding to the two-level requirements, we\nintroduce a comprehensive evaluation metric that considers five distinct\nperspectives in commentary generation. Our experiments confirm the\neffectiveness of our proposed system. We also observe a significant increase in\nthe efficiency of commentators in real-world scenarios, with the average time\nspent on creating a commentary dropping from 4 hours to 20 minutes.\nImportantly, such an increase in efficiency does not compromise the quality of\nthe commentaries.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11609v2",
    "published_date": "2024-08-21 13:34:29 UTC",
    "updated_date": "2024-08-23 03:40:44 UTC"
  },
  {
    "arxiv_id": "2408.11608v2",
    "title": "Don't Kill the Baby: The Case for AI in Arbitration",
    "authors": [
      "Michael Broyde",
      "Yiyang Mei"
    ],
    "abstract": "Since the introduction of Generative AI (GenAI) in 2022, its ability to\nsimulate human intelligence and generate content has sparked both enthusiasm\nand concern. While much criticism focuses on AI's potential to perpetuate bias,\ncreate emotional dissonance, displace jobs, and raise ethical questions, these\nconcerns often overlook the practical benefits of AI, particularly in legal\ncontexts.\n  This article examines the integration of AI into arbitration, arguing that\nthe Federal Arbitration Act (FAA) allows parties to contractually choose\nAI-driven arbitration, despite traditional reservations. The article makes\nthree key contributions: (1) It shifts the focus from debates over AI's\npersonhood to the practical aspects of incorporating AI into arbitration,\nasserting that AI can effectively serve as an arbitrator if both parties agree;\n(2) It positions arbitration as an ideal starting point for broader AI adoption\nin the legal field, given its flexibility and the autonomy it grants parties to\ndefine their standards of fairness; and (3) It outlines future research\ndirections, emphasizing the importance of empirically comparing AI and human\narbitration, which could lead to the development of distinct systems.\n  By advocating for the use of AI in arbitration, this article underscores the\nimportance of respecting contractual autonomy and creating an environment that\nallows AI's potential to be fully realized. Drawing on the insights of Judge\nRichard Posner, the article argues that the ethical obligations of AI in\narbitration should be understood within the context of its technological\nstrengths and the voluntary nature of arbitration agreements. Ultimately, it\ncalls for a balanced, open-minded approach to AI in arbitration, recognizing\nits potential to enhance the efficiency, fairness, and flexibility of dispute\nresolution",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11608v2",
    "published_date": "2024-08-21 13:34:20 UTC",
    "updated_date": "2025-03-22 17:00:00 UTC"
  },
  {
    "arxiv_id": "2408.11607v2",
    "title": "Networked Communication for Mean-Field Games with Function Approximation and Empirical Mean-Field Estimation",
    "authors": [
      "Patrick Benjamin",
      "Alessandro Abate"
    ],
    "abstract": "Recent algorithms allow decentralised agents, possibly connected via a\ncommunication network, to learn equilibria in Mean-Field Games from a\nnon-episodic run of the empirical system. However, these algorithms are for\ntabular settings: this computationally limits the size of agents' observation\nspace, meaning the algorithms cannot handle anything but small state spaces,\nnor generalise beyond policies depending only on the agent's local state to\nso-called 'population-dependent' policies. We address this limitation by\nintroducing function approximation to the existing setting, drawing on the\nMunchausen Online Mirror Descent method that has previously been employed only\nin finite-horizon, episodic, centralised settings. While this permits us to\ninclude the mean field in the observation for players' policies, it is\nunrealistic to assume decentralised agents have access to this global\ninformation: we therefore also provide new algorithms allowing agents to\nlocally estimate the global empirical distribution, and to improve this\nestimate via inter-agent communication. We show theoretically that exchanging\npolicy information helps networked agents outperform both independent and even\ncentralised agents in function-approximation settings. Our experiments\ndemonstrate this happening empirically, by an even greater margin than in\ntabular settings, and show that the communication network allows decentralised\nagents to estimate the mean field for population-dependent policies.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11607v2",
    "published_date": "2024-08-21 13:32:46 UTC",
    "updated_date": "2025-03-13 13:32:53 UTC"
  },
  {
    "arxiv_id": "2408.11599v1",
    "title": "Cause-Aware Empathetic Response Generation via Chain-of-Thought Fine-Tuning",
    "authors": [
      "Xinhao Chen",
      "Chong Yang",
      "Man Lan",
      "Li Cai",
      "Yang Chen",
      "Tu Hu",
      "Xinlin Zhuang",
      "Aimin Zhou"
    ],
    "abstract": "Empathetic response generation endows agents with the capability to\ncomprehend dialogue contexts and react to expressed emotions. Previous works\npredominantly focus on leveraging the speaker's emotional labels, but ignore\nthe importance of emotion cause reasoning in empathetic response generation,\nwhich hinders the model's capacity for further affective understanding and\ncognitive inference. In this paper, we propose a cause-aware empathetic\ngeneration approach by integrating emotions and causes through a well-designed\nChain-of-Thought (CoT) prompt on Large Language Models (LLMs). Our approach can\ngreatly promote LLMs' performance of empathy by instruction tuning and\nenhancing the role awareness of an empathetic listener in the prompt.\nAdditionally, we propose to incorporate cause-oriented external knowledge from\nCOMET into the prompt, which improves the diversity of generation and\nalleviates conflicts between internal and external knowledge at the same time.\nExperimental results on the benchmark dataset demonstrate that our approach on\nLLaMA-7b achieves state-of-the-art performance in both automatic and human\nevaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11599v1",
    "published_date": "2024-08-21 13:11:03 UTC",
    "updated_date": "2024-08-21 13:11:03 UTC"
  },
  {
    "arxiv_id": "2408.11592v1",
    "title": "Active learning for efficient data selection in radio-signal based positioning via deep learning",
    "authors": [
      "Vincent Corlay",
      "Milan Courcoux-Caro"
    ],
    "abstract": "We consider the problem of user equipment (UE) positioning based on radio\nsignals via deep learning. As in most supervised-learning tasks, a critical\naspect is the availability of a relevant dataset to train a model. However, in\na cellular network, the data-collection step may induce a high communication\noverhead. As a result, to reduce the required size of the dataset, it may be\ninteresting to carefully choose the positions to be labelled and to be used in\nthe training. We therefore propose an active learning approach for efficient\ndata collection. We first show that significant gains (both in terms of\npositioning accuracy and size of the required dataset) can be obtained for the\nconsidered positioning problem using a genie. This validates the interest of\nactive learning for positioning. We then propose a \\textcolor{blue}{practical}\nmethod to approximate this genie.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "Submitted to Electronics Letters",
    "pdf_url": "http://arxiv.org/pdf/2408.11592v1",
    "published_date": "2024-08-21 12:59:35 UTC",
    "updated_date": "2024-08-21 12:59:35 UTC"
  },
  {
    "arxiv_id": "2408.11574v1",
    "title": "Drama Engine: A Framework for Narrative Agents",
    "authors": [
      "Martin Pichlmair",
      "Riddhi Raj",
      "Charlene Putney"
    ],
    "abstract": "This technical report presents the Drama Engine, a novel framework for\nagentic interaction with large language models designed for narrative purposes.\nThe framework adapts multi-agent system principles to create dynamic,\ncontext-aware companions that can develop over time and interact with users and\neach other. Key features include multi-agent workflows with delegation, dynamic\nprompt assembly, and model-agnostic design. The Drama Engine introduces unique\nelements such as companion development, mood systems, and automatic context\nsummarising. It is implemented in TypeScript. The framework's applications\ninclude multi-agent chats and virtual co-workers for creative writing. The\npaper discusses the system's architecture, prompt assembly process, delegation\nmechanisms, and moderation techniques, as well as potential ethical\nconsiderations and future extensions.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "68T42 (Primary), 68T50 (Secondary)",
      "I.2.7; J.5"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 2 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.11574v1",
    "published_date": "2024-08-21 12:29:38 UTC",
    "updated_date": "2024-08-21 12:29:38 UTC"
  },
  {
    "arxiv_id": "2408.11554v1",
    "title": "Differentiating Choices via Commonality for Multiple-Choice Question Answering",
    "authors": [
      "Wenqing Deng",
      "Zhe Wang",
      "Kewen Wang",
      "Shirui Pan",
      "Xiaowang Zhang",
      "Zhiyong Feng"
    ],
    "abstract": "Multiple-choice question answering (MCQA) becomes particularly challenging\nwhen all choices are relevant to the question and are semantically similar. Yet\nthis setting of MCQA can potentially provide valuable clues for choosing the\nright answer. Existing models often rank each choice separately, overlooking\nthe context provided by other choices. Specifically, they fail to leverage the\nsemantic commonalities and nuances among the choices for reasoning. In this\npaper, we propose a novel MCQA model by differentiating choices through\nidentifying and eliminating their commonality, called DCQA. Our model captures\ntoken-level attention of each choice to the question, and separates tokens of\nthe question attended to by all the choices (i.e., commonalities) from those by\nindividual choices (i.e., nuances). Using the nuances as refined contexts for\nthe choices, our model can effectively differentiate choices with subtle\ndifferences and provide justifications for choosing the correct answer. We\nconduct comprehensive experiments across five commonly used MCQA benchmarks,\ndemonstrating that DCQA consistently outperforms baseline models. Furthermore,\nour case study illustrates the effectiveness of the approach in directing the\nattention of the model to more differentiating features.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, accepted to ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11554v1",
    "published_date": "2024-08-21 12:05:21 UTC",
    "updated_date": "2024-08-21 12:05:21 UTC"
  },
  {
    "arxiv_id": "2408.11552v1",
    "title": "Explainable Deep Learning Framework for Human Activity Recognition",
    "authors": [
      "Yiran Huang",
      "Yexu Zhou",
      "Haibin Zhao",
      "Till Riedel",
      "Michael Beigl"
    ],
    "abstract": "In the realm of human activity recognition (HAR), the integration of\nexplainable Artificial Intelligence (XAI) emerges as a critical necessity to\nelucidate the decision-making processes of complex models, fostering\ntransparency and trust. Traditional explanatory methods like Class Activation\nMapping (CAM) and attention mechanisms, although effective in highlighting\nregions vital for decisions in various contexts, prove inadequate for HAR. This\ninadequacy stems from the inherently abstract nature of HAR data, rendering\nthese explanations obscure. In contrast, state-of-th-art post-hoc\ninterpretation techniques for time series can explain the model from other\nperspectives. However, this requires extra effort. It usually takes 10 to 20\nseconds to generate an explanation. To overcome these challenges, we proposes a\nnovel, model-agnostic framework that enhances both the interpretability and\nefficacy of HAR models through the strategic use of competitive data\naugmentation. This innovative approach does not rely on any particular model\narchitecture, thereby broadening its applicability across various HAR models.\nBy implementing competitive data augmentation, our framework provides intuitive\nand accessible explanations of model decisions, thereby significantly advancing\nthe interpretability of HAR systems without compromising on performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11552v1",
    "published_date": "2024-08-21 11:59:55 UTC",
    "updated_date": "2024-08-21 11:59:55 UTC"
  },
  {
    "arxiv_id": "2408.11546v3",
    "title": "Memorization in In-Context Learning",
    "authors": [
      "Shahriar Golchin",
      "Mihai Surdeanu",
      "Steven Bethard",
      "Eduardo Blanco",
      "Ellen Riloff"
    ],
    "abstract": "In-context learning (ICL) has proven to be an effective strategy for\nimproving the performance of large language models (LLMs) with no additional\ntraining. However, the exact mechanism behind this performance improvement\nremains unclear. This study is the first to show how ICL surfaces memorized\ntraining data and to explore the correlation between this memorization and\nperformance on downstream tasks across various ICL regimes: zero-shot,\nfew-shot, and many-shot. Our most notable findings include: (1) ICL\nsignificantly surfaces memorization compared to zero-shot learning in most\ncases; (2) demonstrations, without their labels, are the most effective element\nin surfacing memorization; (3) ICL improves performance when the surfaced\nmemorization in few-shot regimes reaches a high level (about 40%); and (4)\nthere is a very strong correlation between performance and memorization in ICL\nwhen it outperforms zero-shot learning. Overall, our study uncovers\nmemorization as a new factor impacting ICL, raising an important question: to\nwhat extent do LLMs truly generalize from demonstrations in ICL, and how much\nof their success is due to memorization?",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "v3",
    "pdf_url": "http://arxiv.org/pdf/2408.11546v3",
    "published_date": "2024-08-21 11:54:22 UTC",
    "updated_date": "2025-04-04 02:50:14 UTC"
  },
  {
    "arxiv_id": "2408.11537v1",
    "title": "A Survey of Embodied Learning for Object-Centric Robotic Manipulation",
    "authors": [
      "Ying Zheng",
      "Lei Yao",
      "Yuejiao Su",
      "Yi Zhang",
      "Yi Wang",
      "Sicheng Zhao",
      "Yiyi Zhang",
      "Lap-Pui Chau"
    ],
    "abstract": "Embodied learning for object-centric robotic manipulation is a rapidly\ndeveloping and challenging area in embodied AI. It is crucial for advancing\nnext-generation intelligent robots and has garnered significant interest\nrecently. Unlike data-driven machine learning methods, embodied learning\nfocuses on robot learning through physical interaction with the environment and\nperceptual feedback, making it especially suitable for robotic manipulation. In\nthis paper, we provide a comprehensive survey of the latest advancements in\nthis field and categorize the existing work into three main branches: 1)\nEmbodied perceptual learning, which aims to predict object pose and affordance\nthrough various data representations; 2) Embodied policy learning, which\nfocuses on generating optimal robotic decisions using methods such as\nreinforcement learning and imitation learning; 3) Embodied task-oriented\nlearning, designed to optimize the robot's performance based on the\ncharacteristics of different tasks in object grasping and manipulation. In\naddition, we offer an overview and discussion of public datasets, evaluation\nmetrics, representative applications, current challenges, and potential future\nresearch directions. A project associated with this survey has been established\nat https://github.com/RayYoh/OCRM_survey.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11537v1",
    "published_date": "2024-08-21 11:32:09 UTC",
    "updated_date": "2024-08-21 11:32:09 UTC"
  },
  {
    "arxiv_id": "2408.11530v1",
    "title": "Scalable Knowledge Refactoring using Constrained Optimisation",
    "authors": [
      "Minghao Liu",
      "David M. Cerna",
      "Filipe Gouveia",
      "Andrew Cropper"
    ],
    "abstract": "Knowledge refactoring compresses a logic program by introducing new rules.\nCurrent approaches struggle to scale to large programs. To overcome this\nlimitation, we introduce a constrained optimisation refactoring approach. Our\nfirst key idea is to encode the problem with decision variables based on\nliterals rather than rules. Our second key idea is to focus on linear invented\nrules. Our empirical results on multiple domains show that our approach can\nrefactor programs quicker and with more compression than the previous\nstate-of-the-art approach, sometimes by 60%.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11530v1",
    "published_date": "2024-08-21 11:12:42 UTC",
    "updated_date": "2024-08-21 11:12:42 UTC"
  },
  {
    "arxiv_id": "2408.11527v3",
    "title": "The Vizier Gaussian Process Bandit Algorithm",
    "authors": [
      "Xingyou Song",
      "Qiuyi Zhang",
      "Chansoo Lee",
      "Emily Fertig",
      "Tzu-Kuo Huang",
      "Lior Belenki",
      "Greg Kochanski",
      "Setareh Ariafar",
      "Srinivas Vasudevan",
      "Sagi Perel",
      "Daniel Golovin"
    ],
    "abstract": "Google Vizier has performed millions of optimizations and accelerated\nnumerous research and production systems at Google, demonstrating the success\nof Bayesian optimization as a large-scale service. Over multiple years, its\nalgorithm has been improved considerably, through the collective experiences of\nnumerous research efforts and user feedback. In this technical report, we\ndiscuss the implementation details and design choices of the current default\nalgorithm provided by Open Source Vizier. Our experiments on standardized\nbenchmarks reveal its robustness and versatility against well-established\nindustry baselines on multiple practical modes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "Google DeepMind Technical Report. Code can be found in\n  https://github.com/google/vizier",
    "pdf_url": "http://arxiv.org/pdf/2408.11527v3",
    "published_date": "2024-08-21 11:06:02 UTC",
    "updated_date": "2024-12-06 17:31:39 UTC"
  },
  {
    "arxiv_id": "2408.11526v2",
    "title": "RConE: Rough Cone Embedding for Multi-Hop Logical Query Answering on Multi-Modal Knowledge Graphs",
    "authors": [
      "Mayank Kharbanda",
      "Rajiv Ratn Shah",
      "Raghava Mutharaju"
    ],
    "abstract": "Multi-hop query answering over a Knowledge Graph (KG) involves traversing one\nor more hops from the start node to answer a query. Path-based and logic-based\nmethods are state-of-the-art for multi-hop question answering. The former is\nused in link prediction tasks. The latter is for answering complex logical\nqueries. The logical multi-hop querying technique embeds the KG and queries in\nthe same embedding space. The existing work incorporates First Order Logic\n(FOL) operators, such as conjunction ($\\wedge$), disjunction ($\\vee$), and\nnegation ($\\neg$), in queries. Though current models have most of the building\nblocks to execute the FOL queries, they cannot use the dense information of\nmulti-modal entities in the case of Multi-Modal Knowledge Graphs (MMKGs). We\npropose RConE, an embedding method to capture the multi-modal information\nneeded to answer a query. The model first shortlists candidate (multi-modal)\nentities containing the answer. It then finds the solution (sub-entities)\nwithin those entities. Several existing works tackle path-based\nquestion-answering in MMKGs. However, to our knowledge, we are the first to\nintroduce logical constructs in querying MMKGs and to answer queries that\ninvolve sub-entities of multi-modal entities as the answer. Extensive\nevaluation of four publicly available MMKGs indicates that RConE outperforms\nthe current state-of-the-art.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11526v2",
    "published_date": "2024-08-21 11:02:35 UTC",
    "updated_date": "2024-08-26 07:46:25 UTC"
  },
  {
    "arxiv_id": "2408.11523v1",
    "title": "LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding",
    "authors": [
      "Zhizhong Wan",
      "Bin Yin",
      "Junjie Xie",
      "Fei Jiang",
      "Xiang Li",
      "Wei Lin"
    ],
    "abstract": "Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS),\naiming to provide personalized recommendation services for users in many\naspects such as food delivery, e-commerce and so on. However, traditional RS\nrelies on collaborative signals, which lacks semantic understanding to\nreal-time scenes. We also noticed that a major challenge in utilizing Large\nLanguage Models (LLMs) for practical recommendation purposes is their\nefficiency in dealing with long text input. To break through the problems\nabove, we propose Large Language Model Aided Real-time Scene\nRecommendation(LARR), adopt LLMs for semantic understanding, utilizing\nreal-time scene information in RS without requiring LLM to process the entire\nreal-time scene text directly, thereby enhancing the efficiency of LLM-based\nCTR modeling. Specifically, recommendation domain-specific knowledge is\ninjected into LLM and then RS employs an aggregation encoder to build real-time\nscene information from separate LLM's outputs. Firstly, a LLM is continual\npretrained on corpus built from recommendation data with the aid of special\ntokens. Subsequently, the LLM is fine-tuned via contrastive learning on three\nkinds of sample construction strategies. Through this step, LLM is transformed\ninto a text embedding model. Finally, LLM's separate outputs for different\nscene features are aggregated by an encoder, aligning to collaborative signals\nin RS, enhancing the performance of recommendation model.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11523v1",
    "published_date": "2024-08-21 10:56:26 UTC",
    "updated_date": "2024-08-21 10:56:26 UTC"
  },
  {
    "arxiv_id": "2408.11515v1",
    "title": "Quantifying Behavioural Distance Between Mathematical Expressions",
    "authors": [
      "Sebastian Mežnar",
      "Sašo Džeroski",
      "Ljupčo Todorovski"
    ],
    "abstract": "Existing symbolic regression methods organize the space of candidate\nmathematical expressions primarily based on their syntactic, structural\nsimilarity. However, this approach overlooks crucial equivalences between\nexpressions that arise from mathematical symmetries, such as commutativity,\nassociativity, and distribution laws for arithmetic operations. Consequently,\nexpressions with similar errors on a given data set are apart from each other\nin the search space. This leads to a rough error landscape in the search space\nthat efficient local, gradient-based methods cannot explore. This paper\nproposes and implements a measure of a behavioral distance, BED, that clusters\ntogether expressions with similar errors. The experimental results show that\nthe stochastic method for calculating BED achieves consistency with a modest\nnumber of sampled values for evaluating the expressions. This leads to\ncomputational efficiency comparable to the tree-based syntactic distance. Our\nfindings also reveal that BED significantly improves the smoothness of the\nerror landscape in the search space for symbolic regression.",
    "categories": [
      "cs.AI",
      "68T01",
      "I.1.1; I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 10 figures, 1 table, 2 appendices",
    "pdf_url": "http://arxiv.org/pdf/2408.11515v1",
    "published_date": "2024-08-21 10:48:04 UTC",
    "updated_date": "2024-08-21 10:48:04 UTC"
  },
  {
    "arxiv_id": "2408.11513v1",
    "title": "Last-Iterate Convergence of General Parameterized Policies in Constrained MDPs",
    "authors": [
      "Washim Uddin Mondal",
      "Vaneet Aggarwal"
    ],
    "abstract": "We consider the problem of learning a Constrained Markov Decision Process\n(CMDP) via general parameterization. Our proposed Primal-Dual based Regularized\nAccelerated Natural Policy Gradient (PDR-ANPG) algorithm uses entropy and\nquadratic regularizers to reach this goal. For a parameterized policy class\nwith transferred compatibility approximation error, $\\epsilon_{\\mathrm{bias}}$,\nPDR-ANPG achieves a last-iterate $\\epsilon$ optimality gap and $\\epsilon$\nconstraint violation (up to some additive factor of $\\epsilon_{\\mathrm{bias}}$)\nwith a sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2}\\min\\{\\epsilon^{-2},\\epsilon_{\\mathrm{bias}}^{-\\frac{1}{3}}\\})$.\nIf the class is incomplete ($\\epsilon_{\\mathrm{bias}}>0$), then the sample\ncomplexity reduces to $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ for\n$\\epsilon<(\\epsilon_{\\mathrm{bias}})^{\\frac{1}{6}}$. Moreover, for complete\npolicies with $\\epsilon_{\\mathrm{bias}}=0$, our algorithm achieves a\nlast-iterate $\\epsilon$ optimality gap and $\\epsilon$ constraint violation with\n$\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ sample complexity. It is a significant\nimprovement of the state-of-the-art last-iterate guarantees of general\nparameterized CMDPs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11513v1",
    "published_date": "2024-08-21 10:44:57 UTC",
    "updated_date": "2024-08-21 10:44:57 UTC"
  },
  {
    "arxiv_id": "2408.11494v3",
    "title": "Mutagenesis screen to map the functions of parameters of Large Language Models",
    "authors": [
      "Yue Hu",
      "Chengming Xu",
      "Jixin Zheng",
      "Patrick X. Zhao",
      "Javed Khan",
      "Ruimeng Wang"
    ],
    "abstract": "Large Language Models (LLMs) have significantly advanced artificial\nintelligence, excelling in numerous tasks. Although the functionality of a\nmodel is inherently tied to its parameters, a systematic method for exploring\nthe connections between the parameters and the functionality are lacking.\nModels sharing similar structure and parameter counts exhibit significant\nperformance disparities across various tasks, prompting investigations into the\nvarying patterns that govern their performance. We adopted a mutagenesis screen\napproach inspired by the methods used in biological studies, to investigate\nLlama2-7b and Zephyr. This technique involved mutating elements within the\nmodels' matrices to their maximum or minimum values to examine the relationship\nbetween model parameters and their functionalities. Our research uncovered\nmultiple levels of fine structures within both models. Many matrices showed a\nmixture of maximum and minimum mutations following mutagenesis, but others were\npredominantly sensitive to one type. Notably, mutations that produced\nphenotypes, especially those with severe outcomes, tended to cluster along\naxes. Additionally, the location of maximum and minimum mutations often\ndisplayed a complementary pattern on matrix in both models, with the Gate\nmatrix showing a unique two-dimensional asymmetry after rearrangement. In\nZephyr, certain mutations consistently resulted in poetic or conversational\nrather than descriptive outputs. These \"writer\" mutations grouped according to\nthe high-frequency initial word of the output, with a marked tendency to share\nthe row coordinate even when they are in different matrices. Our findings\naffirm that the mutagenesis screen is an effective tool for deciphering the\ncomplexities of large language models and identifying unexpected ways to expand\ntheir potential, providing deeper insights into the foundational aspects of AI\nsystems.",
    "categories": [
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 6 figures, supplementary material available online",
    "pdf_url": "http://arxiv.org/pdf/2408.11494v3",
    "published_date": "2024-08-21 10:10:08 UTC",
    "updated_date": "2025-01-17 22:10:47 UTC"
  },
  {
    "arxiv_id": "2408.11492v2",
    "title": "Estimating Peer Direct and Indirect Effects in Observational Network Data",
    "authors": [
      "Xiaojing Du",
      "Jiuyong Li",
      "Debo Cheng",
      "Lin Liu",
      "Wentao Gao",
      "Xiongren Chen"
    ],
    "abstract": "Estimating causal effects is crucial for decision-makers in many\napplications, but it is particularly challenging with observational network\ndata due to peer interactions. Many algorithms have been proposed to estimate\ncausal effects involving network data, particularly peer effects, but they\noften overlook the variety of peer effects. To address this issue, we propose a\ngeneral setting which considers both peer direct effects and peer indirect\neffects, and the effect of an individual's own treatment, and provide\nidentification conditions of these causal effects and proofs. To estimate these\ncausal effects, we utilize attention mechanisms to distinguish the influences\nof different neighbors and explore high-order neighbor effects through\nmulti-layer graph neural networks (GNNs). Additionally, to control the\ndependency between node features and representations, we incorporate the\nHilbert-Schmidt Independence Criterion (HSIC) into the GNN, fully utilizing the\nstructural information of the graph, to enhance the robustness and accuracy of\nthe model. Extensive experiments on two semi-synthetic datasets confirm the\neffectiveness of our approach. Our theoretical findings have the potential to\nimprove intervention strategies in networked systems, with applications in\nareas such as social networks and epidemiology.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11492v2",
    "published_date": "2024-08-21 10:02:05 UTC",
    "updated_date": "2024-09-13 05:16:48 UTC"
  },
  {
    "arxiv_id": "2408.11491v2",
    "title": "SCANS: Mitigating the Exaggerated Safety for LLMs via Safety-Conscious Activation Steering",
    "authors": [
      "Zouying Cao",
      "Yifei Yang",
      "Hai Zhao"
    ],
    "abstract": "Safety alignment is indispensable for Large Language Models (LLMs) to defend\nthreats from malicious instructions. However, recent researches reveal\nsafety-aligned LLMs prone to reject benign queries due to the exaggerated\nsafety issue, limiting their helpfulness. In this paper, we propose a\nSafety-Conscious Activation Steering (SCANS) method to mitigate the exaggerated\nsafety concerns in aligned LLMs. First, SCANS extracts the refusal steering\nvectors within the activation space and utilizes vocabulary projection to\nanchor some specific safety-critical layers which influence model refusal\nbehavior. Second, by tracking the hidden state transition, SCANS identifies the\nsteering direction and steers the model behavior accordingly, achieving a\nbalance between exaggerated safety and adequate safety. Experiments show that\nSCANS achieves new state-of-the-art performance on XSTest and OKTest\nbenchmarks, without impairing their defense capability against harmful queries\nand maintaining almost unchanged model capability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of paper accepted to AAAI 2025. 14 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.11491v2",
    "published_date": "2024-08-21 10:01:34 UTC",
    "updated_date": "2024-12-17 13:32:36 UTC"
  },
  {
    "arxiv_id": "2408.11455v2",
    "title": "Using Part-based Representations for Explainable Deep Reinforcement Learning",
    "authors": [
      "Manos Kirtas",
      "Konstantinos Tsampazis",
      "Loukia Avramelou",
      "Nikolaos Passalis",
      "Anastasios Tefas"
    ],
    "abstract": "Utilizing deep learning models to learn part-based representations holds\nsignificant potential for interpretable-by-design approaches, as these models\nincorporate latent causes obtained from feature representations through simple\naddition. However, training a part-based learning model presents challenges,\nparticularly in enforcing non-negative constraints on the model's parameters,\nwhich can result in training difficulties such as instability and convergence\nissues. Moreover, applying such approaches in Deep Reinforcement Learning (RL)\nis even more demanding due to the inherent instabilities that impact many\noptimization methods. In this paper, we propose a non-negative training\napproach for actor models in RL, enabling the extraction of part-based\nrepresentations that enhance interpretability while adhering to non-negative\nconstraints. To this end, we employ a non-negative initialization technique, as\nwell as a modified sign-preserving training method, which can ensure better\ngradient flow compared to existing approaches. We demonstrate the effectiveness\nof the proposed approach using the well-known Cartpole benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11455v2",
    "published_date": "2024-08-21 09:21:59 UTC",
    "updated_date": "2024-08-22 05:46:23 UTC"
  },
  {
    "arxiv_id": "2408.11451v4",
    "title": "SIGMA: Selective Gated Mamba for Sequential Recommendation",
    "authors": [
      "Ziwei Liu",
      "Qidong Liu",
      "Yejing Wang",
      "Wanyu Wang",
      "Pengyue Jia",
      "Maolin Wang",
      "Zitao Liu",
      "Yi Chang",
      "Xiangyu Zhao"
    ],
    "abstract": "In various domains, Sequential Recommender Systems (SRS) have become\nessential due to their superior capability to discern intricate user\npreferences. Typically, SRS utilize transformer-based architectures to forecast\nthe subsequent item within a sequence. Nevertheless, the quadratic\ncomputational complexity inherent in these models often leads to\ninefficiencies, hindering the achievement of real-time recommendations. Mamba,\na recent advancement, has exhibited exceptional performance in time series\nprediction, significantly enhancing both efficiency and accuracy. However,\nintegrating Mamba directly into SRS poses several challenges. Its inherently\nunidirectional nature may constrain the model's capacity to capture the full\ncontext of user-item interactions, while its instability in state estimation\ncan compromise its ability to detect short-term patterns within interaction\nsequences.\n  To overcome these issues, we introduce a new framework named Selective Gated\nMamba (SIGMA) for Sequential Recommendation. This framework leverages a\nPartially Flipped Mamba (PF-Mamba) to construct a bidirectional architecture\nspecifically tailored to improve contextual modeling. Additionally, an\ninput-sensitive Dense Selective Gate (DS Gate) is employed to optimize\ndirectional weights and enhance the processing of sequential information in\nPF-Mamba. For short sequence modeling, we have also developed a Feature Extract\nGRU (FE-GRU) to efficiently capture short-term dependencies. Empirical results\nindicate that SIGMA outperforms current models on five real-world datasets. Our\nimplementation code is available at https://github.com/ziwliu-cityu/SIMGA to\nease reproducibility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11451v4",
    "published_date": "2024-08-21 09:12:59 UTC",
    "updated_date": "2024-12-24 17:03:23 UTC"
  },
  {
    "arxiv_id": "2408.11449v2",
    "title": "Enabling Small Models for Zero-Shot Selection and Reuse through Model Label Learning",
    "authors": [
      "Jia Zhang",
      "Zhi Zhou",
      "Lan-Zhe Guo",
      "Yu-Feng Li"
    ],
    "abstract": "Vision-language models (VLMs) like CLIP have demonstrated impressive\nzero-shot ability in image classification tasks by aligning text and images but\nsuffer inferior performance compared with task-specific expert models. On the\ncontrary, expert models excel in their specialized domains but lack zero-shot\nability for new tasks. How to obtain both the high performance of expert models\nand zero-shot ability is an important research direction. In this paper, we\nattempt to demonstrate that by constructing a model hub and aligning models\nwith their functionalities using model labels, new tasks can be solved in a\nzero-shot manner by effectively selecting and reusing models in the hub. We\nintroduce a novel paradigm, Model Label Learning (MLL), which bridges the gap\nbetween models and their functionalities through a Semantic Directed Acyclic\nGraph (SDAG) and leverages an algorithm, Classification Head Combination\nOptimization (CHCO), to select capable models for new tasks. Compared with the\nfoundation model paradigm, it is less costly and more scalable, i.e., the\nzero-shot ability grows with the sizes of the model hub. Experiments on seven\nreal-world datasets validate the effectiveness and efficiency of MLL,\ndemonstrating that expert models can be effectively reused for zero-shot tasks.\nOur code will be released publicly.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11449v2",
    "published_date": "2024-08-21 09:08:26 UTC",
    "updated_date": "2025-02-02 08:54:40 UTC"
  },
  {
    "arxiv_id": "2408.11448v1",
    "title": "Lookism: The overlooked bias in computer vision",
    "authors": [
      "Aditya Gulati",
      "Bruno Lepri",
      "Nuria Oliver"
    ],
    "abstract": "In recent years, there have been significant advancements in computer vision\nwhich have led to the widespread deployment of image recognition and generation\nsystems in socially relevant applications, from hiring to security screening.\nHowever, the prevalence of biases within these systems has raised significant\nethical and social concerns. The most extensively studied biases in this\ncontext are related to gender, race and age. Yet, other biases are equally\npervasive and harmful, such as lookism, i.e., the preferential treatment of\nindividuals based on their physical appearance. Lookism remains under-explored\nin computer vision but can have profound implications not only by perpetuating\nharmful societal stereotypes but also by undermining the fairness and\ninclusivity of AI technologies. Thus, this paper advocates for the systematic\nstudy of lookism as a critical bias in computer vision models. Through a\ncomprehensive review of existing literature, we identify three areas of\nintersection between lookism and computer vision. We illustrate them by means\nof examples and a user study. We call for an interdisciplinary approach to\naddress lookism, urging researchers, developers, and policymakers to prioritize\nthe development of equitable computer vision systems that respect and reflect\nthe diversity of human appearances.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "I.2.0; I.4.0; K.4.2"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper accepted at the ECCV 2024 workshop named \"Fairness and ethics\n  towards transparent AI: facing the chalLEnge through model Debiasing\n  (FAILED)\", https://failed-workshop-eccv-2024.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2408.11448v1",
    "published_date": "2024-08-21 09:07:20 UTC",
    "updated_date": "2024-08-21 09:07:20 UTC"
  },
  {
    "arxiv_id": "2408.11441v1",
    "title": "Epistemic Injustice in Generative AI",
    "authors": [
      "Jackie Kay",
      "Atoosa Kasirzadeh",
      "Shakir Mohamed"
    ],
    "abstract": "This paper investigates how generative AI can potentially undermine the\nintegrity of collective knowledge and the processes we rely on to acquire,\nassess, and trust information, posing a significant threat to our knowledge\necosystem and democratic discourse. Grounded in social and political\nphilosophy, we introduce the concept of \\emph{generative algorithmic epistemic\ninjustice}. We identify four key dimensions of this phenomenon: amplified and\nmanipulative testimonial injustice, along with hermeneutical ignorance and\naccess injustice. We illustrate each dimension with real-world examples that\nreveal how generative AI can produce or amplify misinformation, perpetuate\nrepresentational harm, and create epistemic inequities, particularly in\nmultilingual contexts. By highlighting these injustices, we aim to inform the\ndevelopment of epistemically just generative AI systems, proposing strategies\nfor resistance, system design principles, and two approaches that leverage\ngenerative AI to foster a more equitable information ecosystem, thereby\nsafeguarding democratic values and the integrity of knowledge production.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11441v1",
    "published_date": "2024-08-21 08:51:05 UTC",
    "updated_date": "2024-08-21 08:51:05 UTC"
  },
  {
    "arxiv_id": "2408.11433v2",
    "title": "Towards Aligned Data Removal via Twin Machine Unlearning",
    "authors": [
      "Haoxuan Ji",
      "Zheng Lin",
      "Yuyao Sun",
      "Gao Fei",
      "Yuhang Wang",
      "Haichang Gao",
      "Zhenxing Niu"
    ],
    "abstract": "Modern privacy regulations have spurred the evolution of machine unlearning,\na technique that enables the removal of data from an already trained ML model\nwithout requiring retraining from scratch. Previous unlearning methods tend to\ninduce the model to achieve lowest classification accuracy on the removal data.\nNonetheless, the authentic objective of machine unlearning is to align the\nunlearned model with the gold model, i.e., achieving the same classification\naccuracy as the gold model. For this purpose, we present a Twin Machine\nUnlearning (TMU) approach, where a twin unlearning problem is defined\ncorresponding to the original unlearning problem. As a results, the\ngeneralization-label predictor trained on the twin problem can be transferred\nto the original problem, facilitating aligned data removal. Comprehensive\nempirical experiments illustrate that our approach significantly enhances the\nalignment between the unlearned model and the gold model. Meanwhile, our method\nallows data removal without compromising the model accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11433v2",
    "published_date": "2024-08-21 08:42:21 UTC",
    "updated_date": "2025-05-02 08:12:59 UTC"
  },
  {
    "arxiv_id": "2408.11431v1",
    "title": "Diagnosing and Remedying Knowledge Deficiencies in LLMs via Label-free Curricular Meaningful Learning",
    "authors": [
      "Kai Xiong",
      "Xiao Ding",
      "Li Du",
      "Jiahao Ying",
      "Ting Liu",
      "Bing Qin",
      "Yixin Cao"
    ],
    "abstract": "Large Language Models (LLMs) are versatile and demonstrate impressive\ngeneralization ability by mining and learning information from extensive\nunlabeled text. However, they still exhibit reasoning mistakes, often stemming\nfrom knowledge deficiencies, which can affect their trustworthiness and\nreliability. Although users can provide diverse and comprehensive queries,\nobtaining sufficient and effective feedback is demanding. Furthermore,\nevaluating LLMs comprehensively with limited labeled samples is difficult. This\nmakes it a challenge to diagnose and remedy the deficiencies of LLMs through\nrich label-free user queries. To tackle this challenge, we propose a label-free\ncurricular meaningful learning framework (LaMer). LaMer first employs relative\nentropy to automatically diagnose and quantify the knowledge deficiencies of\nLLMs in a label-free setting. Next, to remedy the diagnosed knowledge\ndeficiencies, we apply curricular meaningful learning: first, we adopt\nmeaningful learning to adaptively synthesize augmentation data according to the\nseverity of the deficiencies, and then design a curricular deficiency remedy\nstrategy to remedy the knowledge deficiencies of LLMs progressively.\nExperiments show that LaMer efficiently and effectively diagnoses and remedies\nknowledge deficiencies in LLMs, improving various LLMs across seven\nout-of-distribution (OOD) reasoning and language understanding benchmarks,\nachieving comparable results to baselines with just 40\\% training data. LaMer\neven surpasses methods that rely on labeled datasets for deficiency diagnosis.\nIn application, our label-free method can offer an effective knowledge\ndeficiency diagnostic tool for efficient LLM development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2408.11431v1",
    "published_date": "2024-08-21 08:39:49 UTC",
    "updated_date": "2024-08-21 08:39:49 UTC"
  },
  {
    "arxiv_id": "2408.11429v1",
    "title": "Long-Range Vision-Based UAV-assisted Localization for Unmanned Surface Vehicles",
    "authors": [
      "Waseem Akram",
      "Siyuan Yang",
      "Hailiang Kuang",
      "Xiaoyu He",
      "Muhayy Ud Din",
      "Yihao Dong",
      "Defu Lin",
      "Lakmal Seneviratne",
      "Shaoming He",
      "Irfan Hussain"
    ],
    "abstract": "The global positioning system (GPS) has become an indispensable navigation\nmethod for field operations with unmanned surface vehicles (USVs) in marine\nenvironments. However, GPS may not always be available outdoors because it is\nvulnerable to natural interference and malicious jamming attacks. Thus, an\nalternative navigation system is required when the use of GPS is restricted or\nprohibited. To this end, we present a novel method that utilizes an Unmanned\nAerial Vehicle (UAV) to assist in localizing USVs in GNSS-restricted marine\nenvironments. In our approach, the UAV flies along the shoreline at a\nconsistent altitude, continuously tracking and detecting the USV using a deep\nlearning-based approach on camera images. Subsequently, triangulation\ntechniques are applied to estimate the USV's position relative to the UAV,\nutilizing geometric information and datalink range from the UAV. We propose\nadjusting the UAV's camera angle based on the pixel error between the USV and\nthe image center throughout the localization process to enhance accuracy.\nAdditionally, visual measurements are integrated into an Extended Kalman Filter\n(EKF) for robust state estimation. To validate our proposed method, we utilize\na USV equipped with onboard sensors and a UAV equipped with a camera. A\nheterogeneous robotic interface is established to facilitate communication\nbetween the USV and UAV. We demonstrate the efficacy of our approach through a\nseries of experiments conducted during the ``Muhammad Bin Zayed International\nRobotic Challenge (MBZIRC-2024)'' in real marine environments, incorporating\nnoisy measurements and ocean disturbances. The successful outcomes indicate the\npotential of our method to complement GPS for USV navigation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11429v1",
    "published_date": "2024-08-21 08:37:37 UTC",
    "updated_date": "2024-08-21 08:37:37 UTC"
  },
  {
    "arxiv_id": "2408.11415v1",
    "title": "Towards \"Differential AI Psychology\" and in-context Value-driven Statement Alignment with Moral Foundations Theory",
    "authors": [
      "Simon Münker"
    ],
    "abstract": "Contemporary research in social sciences is increasingly utilizing\nstate-of-the-art statistical language models to annotate or generate content.\nWhile these models perform benchmark-leading on common language tasks and show\nexemplary task-independent emergent abilities, transferring them to novel\nout-of-domain tasks is only insufficiently explored. The implications of the\nstatistical black-box approach - stochastic parrots - are prominently\ncriticized in the language model research community; however, the significance\nfor novel generative tasks is not.\n  This work investigates the alignment between personalized language models and\nsurvey participants on a Moral Foundation Theory questionnaire. We adapt\ntext-to-text models to different political personas and survey the\nquestionnaire repetitively to generate a synthetic population of persona and\nmodel combinations. Analyzing the intra-group variance and cross-alignment\nshows significant differences across models and personas. Our findings indicate\nthat adapted models struggle to represent the survey-captured assessment of\npolitical ideologies. Thus, using language models to mimic social interactions\nrequires measurable improvements in in-context optimization or parameter\nmanipulation to align with psychological and sociological stereotypes. Without\nquantifiable alignment, generating politically nuanced content remains\nunfeasible. To enhance these representations, we propose a testable framework\nto generate agents based on moral value statements for future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.11415v1",
    "published_date": "2024-08-21 08:20:41 UTC",
    "updated_date": "2024-08-21 08:20:41 UTC"
  },
  {
    "arxiv_id": "2408.11401v1",
    "title": "Revisiting FunnyBirds evaluation framework for prototypical parts networks",
    "authors": [
      "Szymon Opłatek",
      "Dawid Rymarczyk",
      "Bartosz Zieliński"
    ],
    "abstract": "Prototypical parts networks, such as ProtoPNet, became popular due to their\npotential to produce more genuine explanations than post-hoc methods. However,\nfor a long time, this potential has been strictly theoretical, and no\nsystematic studies have existed to support it. That changed recently with the\nintroduction of the FunnyBirds benchmark, which includes metrics for evaluating\ndifferent aspects of explanations.\n  However, this benchmark employs attribution maps visualization for all\nexplanation techniques except for the ProtoPNet, for which the bounding boxes\nare used. This choice significantly influences the metric scores and questions\nthe conclusions stated in FunnyBirds publication.\n  In this study, we comprehensively compare metric scores obtained for two\ntypes of ProtoPNet visualizations: bounding boxes and similarity maps. Our\nanalysis indicates that employing similarity maps aligns better with the\nessence of ProtoPNet, as evidenced by different metric scores obtained from\nFunnyBirds. Therefore, we advocate using similarity maps as a visualization\ntechnique for prototypical parts networks in explainability evaluation\nbenchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at 2nd XAI World Conference",
    "pdf_url": "http://arxiv.org/pdf/2408.11401v1",
    "published_date": "2024-08-21 07:58:34 UTC",
    "updated_date": "2024-08-21 07:58:34 UTC"
  },
  {
    "arxiv_id": "2408.11384v1",
    "title": "Data-Centric Machine Learning for Earth Observation: Necessary and Sufficient Features",
    "authors": [
      "Hiba Najjar",
      "Marlon Nuske",
      "Andreas Dengel"
    ],
    "abstract": "The availability of temporal geospatial data in multiple modalities has been\nextensively leveraged to enhance the performance of machine learning models.\nWhile efforts on the design of adequate model architectures are approaching a\nlevel of saturation, focusing on a data-centric perspective can complement\nthese efforts to achieve further enhancements in data usage efficiency and\nmodel generalization capacities. This work contributes to this direction. We\nleverage model explanation methods to identify the features crucial for the\nmodel to reach optimal performance and the smallest set of features sufficient\nto achieve this performance. We evaluate our approach on three temporal\nmultimodal geospatial datasets and compare multiple model explanation\ntechniques. Our results reveal that some datasets can reach their optimal\naccuracy with less than 20% of the temporal instances, while in other datasets,\nthe time series of a single band from a single modality is sufficient.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at MACLEAN workshop, ECML/PKDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11384v1",
    "published_date": "2024-08-21 07:26:43 UTC",
    "updated_date": "2024-08-21 07:26:43 UTC"
  },
  {
    "arxiv_id": "2408.11380v1",
    "title": "Reflex-Based Open-Vocabulary Navigation without Prior Knowledge Using Omnidirectional Camera and Multiple Vision-Language Models",
    "authors": [
      "Kento Kawaharazuka",
      "Yoshiki Obinata",
      "Naoaki Kanazawa",
      "Naoto Tsukamoto",
      "Kei Okada",
      "Masayuki Inaba"
    ],
    "abstract": "Various robot navigation methods have been developed, but they are mainly\nbased on Simultaneous Localization and Mapping (SLAM), reinforcement learning,\netc., which require prior map construction or learning. In this study, we\nconsider the simplest method that does not require any map construction or\nlearning, and execute open-vocabulary navigation of robots without any prior\nknowledge to do this. We applied an omnidirectional camera and pre-trained\nvision-language models to the robot. The omnidirectional camera provides a\nuniform view of the surroundings, thus eliminating the need for complicated\nexploratory behaviors including trajectory generation. By applying multiple\npre-trained vision-language models to this omnidirectional image and\nincorporating reflective behaviors, we show that navigation becomes simple and\ndoes not require any prior setup. Interesting properties and limitations of our\nmethod are discussed based on experiments with the mobile robot Fetch.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at Advanced Robotics, website -\n  https://haraduka.github.io/omnidirectional-vlm/",
    "pdf_url": "http://arxiv.org/pdf/2408.11380v1",
    "published_date": "2024-08-21 07:18:58 UTC",
    "updated_date": "2024-08-21 07:18:58 UTC"
  },
  {
    "arxiv_id": "2408.11372v1",
    "title": "Denoising Pre-Training and Customized Prompt Learning for Efficient Multi-Behavior Sequential Recommendation",
    "authors": [
      "Hao Wang",
      "Yongqiang Han",
      "Kefan Wang",
      "Kai Cheng",
      "Zhen Wang",
      "Wei Guo",
      "Yong Liu",
      "Defu Lian",
      "Enhong Chen"
    ],
    "abstract": "In the realm of recommendation systems, users exhibit a diverse array of\nbehaviors when interacting with items. This phenomenon has spurred research\ninto learning the implicit semantic relationships between these behaviors to\nenhance recommendation performance. However, these methods often entail high\ncomputational complexity. To address concerns regarding efficiency,\npre-training presents a viable solution. Its objective is to extract knowledge\nfrom extensive pre-training data and fine-tune the model for downstream tasks.\nNevertheless, previous pre-training methods have primarily focused on\nsingle-behavior data, while multi-behavior data contains significant noise.\nAdditionally, the fully fine-tuning strategy adopted by these methods still\nimposes a considerable computational burden. In response to this challenge, we\npropose DPCPL, the first pre-training and prompt-tuning paradigm tailored for\nMulti-Behavior Sequential Recommendation. Specifically, in the pre-training\nstage, we commence by proposing a novel Efficient Behavior Miner (EBM) to\nfilter out the noise at multiple time scales, thereby facilitating the\ncomprehension of the contextual semantics of multi-behavior sequences.\nSubsequently, we propose to tune the pre-trained model in a highly efficient\nmanner with the proposed Customized Prompt Learning (CPL) module, which\ngenerates personalized, progressive, and diverse prompts to fully exploit the\npotential of the pre-trained model effectively. Extensive experiments on three\nreal-world datasets have unequivocally demonstrated that DPCPL not only\nexhibits high efficiency and effectiveness, requiring minimal parameter\nadjustments but also surpasses the state-of-the-art performance across a\ndiverse range of downstream tasks.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11372v1",
    "published_date": "2024-08-21 06:48:38 UTC",
    "updated_date": "2024-08-21 06:48:38 UTC"
  },
  {
    "arxiv_id": "2408.11371v1",
    "title": "Solving Decision Theory Problems with Probabilistic Answer Set Programming",
    "authors": [
      "Damiano Azzolini",
      "Elena Bellodi",
      "Rafael Kiesel",
      "Fabrizio Riguzzi"
    ],
    "abstract": "Solving a decision theory problem usually involves finding the actions, among\na set of possible ones, which optimize the expected reward, possibly accounting\nfor the uncertainty of the environment. In this paper, we introduce the\npossibility to encode decision theory problems with Probabilistic Answer Set\nProgramming under the credal semantics via decision atoms and utility\nattributes. To solve the task we propose an algorithm based on three layers of\nAlgebraic Model Counting, that we test on several synthetic datasets against an\nalgorithm that adopts answer set enumeration. Empirical results show that our\nalgorithm can manage non trivial instances of programs in a reasonable amount\nof time. Under consideration in Theory and Practice of Logic Programming\n(TPLP).",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)",
    "pdf_url": "http://arxiv.org/pdf/2408.11371v1",
    "published_date": "2024-08-21 06:44:16 UTC",
    "updated_date": "2024-08-21 06:44:16 UTC"
  },
  {
    "arxiv_id": "2408.11370v1",
    "title": "Graph Classification via Reference Distribution Learning: Theory and Practice",
    "authors": [
      "Zixiao Wang",
      "Jicong Fan"
    ],
    "abstract": "Graph classification is a challenging problem owing to the difficulty in\nquantifying the similarity between graphs or representing graphs as vectors,\nthough there have been a few methods using graph kernels or graph neural\nnetworks (GNNs). Graph kernels often suffer from computational costs and manual\nfeature engineering, while GNNs commonly utilize global pooling operations,\nrisking the loss of structural or semantic information. This work introduces\nGraph Reference Distribution Learning (GRDL), an efficient and accurate graph\nclassification method. GRDL treats each graph's latent node embeddings given by\nGNN layers as a discrete distribution, enabling direct classification without\nglobal pooling, based on maximum mean discrepancy to adaptively learned\nreference distributions. To fully understand this new model (the existing\ntheories do not apply) and guide its configuration (e.g., network architecture,\nreferences' sizes, number, and regularization) for practical use, we derive\ngeneralization error bounds for GRDL and verify them numerically. More\nimportantly, our theoretical and numerical results both show that GRDL has a\nstronger generalization ability than GNNs with global pooling operations.\nExperiments on moderate-scale and large-scale graph datasets show the\nsuperiority of GRDL over the state-of-the-art, emphasizing its remarkable\nefficiency, being at least 10 times faster than leading competitors in both\ntraining and inference stages.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11370v1",
    "published_date": "2024-08-21 06:42:22 UTC",
    "updated_date": "2024-08-21 06:42:22 UTC"
  },
  {
    "arxiv_id": "2408.11367v2",
    "title": "Towards Probabilistic Inductive Logic Programming with Neurosymbolic Inference and Relaxation",
    "authors": [
      "Fieke Hillerstrom",
      "Gertjan Burghouts"
    ],
    "abstract": "Many inductive logic programming (ILP) methods are incapable of learning\nprograms from probabilistic background knowledge, e.g. coming from sensory data\nor neural networks with probabilities. We propose Propper, which handles flawed\nand probabilistic background knowledge by extending ILP with a combination of\nneurosymbolic inference, a continuous criterion for hypothesis selection (BCE)\nand a relaxation of the hypothesis constrainer (NoisyCombo). For relational\npatterns in noisy images, Propper can learn programs from as few as 8 examples.\nIt outperforms binary ILP and statistical models such as a Graph Neural\nNetwork.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.11367v2",
    "published_date": "2024-08-21 06:38:49 UTC",
    "updated_date": "2024-09-20 13:28:18 UTC"
  },
  {
    "arxiv_id": "2408.11363v2",
    "title": "ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure Understanding",
    "authors": [
      "Yijia Xiao",
      "Edward Sun",
      "Yiqiao Jin",
      "Qifan Wang",
      "Wei Wang"
    ],
    "abstract": "Understanding biological processes, drug development, and biotechnological\nadvancements requires a detailed analysis of protein structures and functions,\na task that is inherently complex and time-consuming in traditional protein\nresearch. To streamline this process, we introduce ProteinGPT, a\nstate-of-the-art multimodal large language model for proteins that enables\nusers to upload protein sequences and/or structures for comprehensive analysis\nand responsive inquiries. ProteinGPT integrates protein sequence and structure\nencoders with linear projection layers to ensure precise representation\nadaptation and leverages a large language model (LLM) to generate accurate,\ncontextually relevant responses. To train ProteinGPT, we constructed a\nlarge-scale dataset of 132,092 proteins, each annotated with 20-30 property\ntags and 5-10 QA pairs per protein, and optimized the instruction-tuning\nprocess using GPT-4o. Experiments demonstrate that ProteinGPT effectively\ngenerates informative responses to protein-related questions, achieving high\nperformance on both semantic and lexical metrics and significantly\noutperforming baseline models and general-purpose LLMs in understanding and\nresponding to protein-related queries. Our code and data are available at\nhttps://github.com/ProteinGPT/ProteinGPT.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI",
    "comment": "Spotlight, Machine Learning for Genomics Explorations @ ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.11363v2",
    "published_date": "2024-08-21 06:16:22 UTC",
    "updated_date": "2025-04-17 21:55:37 UTC"
  },
  {
    "arxiv_id": "2408.11359v1",
    "title": "Hypergraph Learning based Recommender System for Anomaly Detection, Control and Optimization",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Rajat Kumar Sarkar",
      "Venkataramana Runkana"
    ],
    "abstract": "Anomaly detection is fundamental yet, challenging problem with practical\napplications in industry. The current approaches neglect the higher-order\ndependencies within the networks of interconnected sensors in the\nhigh-dimensional time series(multisensor data) for anomaly detection. To this\nend, we present a self-adapting anomaly detection framework for joint learning\nof (a) discrete hypergraph structure and (b) modeling the temporal trends and\nspatial relations among the interdependent sensors using the hierarchical\nencoder-decoder architecture to overcome the challenges. The hypergraph\nrepresentation learning-based framework exploits the relational inductive\nbiases in the hypergraph-structured data to learn the pointwise\nsingle-step-ahead forecasts through the self-supervised autoregressive task and\npredicts the anomalies based on the forecast error. Furthermore, our framework\nincentivizes learning the anomaly-diagnosis ontology through a differentiable\napproach. It derives the anomaly information propagation-based computational\nhypergraphs for root cause analysis and provides recommendations through an\noffline, optimal predictive control policy to remedy an anomaly. We conduct\nextensive experiments to evaluate the proposed method on the benchmark datasets\nfor fair and rigorous comparison with the popular baselines. The proposed\nmethod outperforms the baseline models and achieves SOTA performance. We report\nthe ablation studies to support the efficacy of the framework.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 10 figure, Accepted at IEEE International Conference on Big\n  Data 2022, Osaka, Japan",
    "pdf_url": "http://arxiv.org/pdf/2408.11359v1",
    "published_date": "2024-08-21 06:04:02 UTC",
    "updated_date": "2024-08-21 06:04:02 UTC"
  },
  {
    "arxiv_id": "2408.11356v1",
    "title": "One-step Structure Prediction and Screening for Protein-Ligand Complexes using Multi-Task Geometric Deep Learning",
    "authors": [
      "Kelei He",
      "Tiejun Dong",
      "Jinhui Wu",
      "Junfeng Zhang"
    ],
    "abstract": "Understanding the structure of the protein-ligand complex is crucial to drug\ndevelopment. Existing virtual structure measurement and screening methods are\ndominated by docking and its derived methods combined with deep learning.\nHowever, the sampling and scoring methodology have largely restricted the\naccuracy and efficiency. Here, we show that these two fundamental tasks can be\naccurately tackled with a single model, namely LigPose, based on multi-task\ngeometric deep learning. By representing the ligand and the protein pair as a\ngraph, LigPose directly optimizes the three-dimensional structure of the\ncomplex, with the learning of binding strength and atomic interactions as\nauxiliary tasks, enabling its one-step prediction ability without docking\ntools. Extensive experiments show LigPose achieved state-of-the-art performance\non major tasks in drug research. Its considerable improvements indicate a\npromising paradigm of AI-based pipeline for drug development.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11356v1",
    "published_date": "2024-08-21 05:53:50 UTC",
    "updated_date": "2024-08-21 05:53:50 UTC"
  },
  {
    "arxiv_id": "2408.11351v1",
    "title": "Vision HgNN: An Electron-Micrograph is Worth Hypergraph of Hypernodes",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Rajat Kumar Sarkar",
      "Sreeja Gangasani",
      "Venkataramana Runkana"
    ],
    "abstract": "Material characterization using electron micrographs is a crucial but\nchallenging task with applications in various fields, such as semiconductors,\nquantum materials, batteries, etc. The challenges in categorizing electron\nmicrographs include but are not limited to the complexity of patterns, high\nlevel of detail, and imbalanced data distribution(long-tail distribution).\nExisting methods have difficulty in modeling the complex relational structure\nin electron micrographs, hindering their ability to effectively capture the\ncomplex relationships between different spatial regions of micrographs. We\npropose a hypergraph neural network(HgNN) backbone architecture, a conceptually\nalternative approach, to better model the complex relationships in electron\nmicrographs and improve material characterization accuracy. By utilizing\ncost-effective GPU hardware, our proposed framework outperforms popular\nbaselines. The results of the ablation studies demonstrate that the proposed\nframework is effective in achieving state-of-the-art performance on benchmark\ndatasets and efficient in terms of computational and memory requirements for\nhandling large-scale electron micrograph-based datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, Accepted in PML4DC Workshop at International Conference on\n  Learning Representations (ICLR) 2023",
    "pdf_url": "http://arxiv.org/pdf/2408.11351v1",
    "published_date": "2024-08-21 05:36:53 UTC",
    "updated_date": "2024-08-21 05:36:53 UTC"
  },
  {
    "arxiv_id": "2408.11347v2",
    "title": "Multimodal Datasets and Benchmarks for Reasoning about Dynamic Spatio-Temporality in Everyday Environments",
    "authors": [
      "Takanori Ugai",
      "Kensho Hara",
      "Shusaku Egami",
      "Ken Fukuda"
    ],
    "abstract": "We used a 3D simulator to create artificial video data with standardized\nannotations, aiming to aid in the development of Embodied AI. Our question\nanswering (QA) dataset measures the extent to which a robot can understand\nhuman behavior and the environment in a home setting. Preliminary experiments\nsuggest our dataset is useful in measuring AI's comprehension of daily life.\n\\end{abstract}",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 1 figure, 1 table, accepted in Embodied AI 2024 Workshop\n  held in conjunction with CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11347v2",
    "published_date": "2024-08-21 05:27:55 UTC",
    "updated_date": "2024-09-17 01:30:36 UTC"
  },
  {
    "arxiv_id": "2408.11341v1",
    "title": "EHL*: Memory-Budgeted Indexing for Ultrafast Optimal Euclidean Pathfinding",
    "authors": [
      "Jinchun Du",
      "Bojie Shen",
      "Muhammad Aamir Cheema"
    ],
    "abstract": "The Euclidean Shortest Path Problem (ESPP), which involves finding the\nshortest path in a Euclidean plane with polygonal obstacles, is a classic\nproblem with numerous real-world applications. The current state-of-the-art\nsolution, Euclidean Hub Labeling (EHL), offers ultra-fast query performance,\noutperforming existing techniques by 1-2 orders of magnitude in runtime\nefficiency. However, this performance comes at the cost of significant memory\noverhead, requiring up to tens of gigabytes of storage on large maps, which can\nlimit its applicability in memory-constrained environments like mobile phones\nor smaller devices. Additionally, EHL's memory usage can only be determined\nafter index construction, and while it provides a memory-runtime tradeoff, it\ndoes not fully optimize memory utilization. In this work, we introduce an\nimproved version of EHL, called EHL*, which overcomes these limitations. A key\ncontribution of EHL* is its ability to create an index that adheres to a\nspecified memory budget while optimizing query runtime performance. Moreover,\nEHL* can leverage preknown query distributions, a common scenario in many\nreal-world applications to further enhance runtime efficiency. Our results show\nthat EHL* can reduce memory usage by up to 10-20 times without much impact on\nquery runtime performance compared to EHL, making it a highly effective\nsolution for optimal pathfinding in memory-constrained environments.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11341v1",
    "published_date": "2024-08-21 04:55:10 UTC",
    "updated_date": "2024-08-21 04:55:10 UTC"
  },
  {
    "arxiv_id": "2408.11338v1",
    "title": "Automatic Dataset Construction (ADC): Sample Collection, Data Curation, and Beyond",
    "authors": [
      "Minghao Liu",
      "Zonglin Di",
      "Jiaheng Wei",
      "Zhongruo Wang",
      "Hengxiang Zhang",
      "Ruixuan Xiao",
      "Haoyu Wang",
      "Jinlong Pang",
      "Hao Chen",
      "Ankit Shah",
      "Hongxin Wei",
      "Xinlei He",
      "Zhaowei Zhao",
      "Haobo Wang",
      "Lei Feng",
      "Jindong Wang",
      "James Davis",
      "Yang Liu"
    ],
    "abstract": "Large-scale data collection is essential for developing personalized training\ndata, mitigating the shortage of training data, and fine-tuning specialized\nmodels. However, creating high-quality datasets quickly and accurately remains\na challenge due to annotation errors, the substantial time and costs associated\nwith human labor. To address these issues, we propose Automatic Dataset\nConstruction (ADC), an innovative methodology that automates dataset creation\nwith negligible cost and high efficiency. Taking the image classification task\nas a starting point, ADC leverages LLMs for the detailed class design and code\ngeneration to collect relevant samples via search engines, significantly\nreducing the need for manual annotation and speeding up the data generation\nprocess. Despite these advantages, ADC also encounters real-world challenges\nsuch as label errors (label noise) and imbalanced data distributions (label\nbias). We provide open-source software that incorporates existing methods for\nlabel error detection, robust learning under noisy and biased data, ensuring a\nhigher-quality training data and more robust model training procedure.\nFurthermore, we design three benchmark datasets focused on label noise\ndetection, label noise learning, and class-imbalanced learning. These datasets\nare vital because there are few existing datasets specifically for label noise\ndetection, despite its importance. Finally, we evaluate the performance of\nexisting popular methods on these datasets, thereby facilitating further\nresearch in the field.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11338v1",
    "published_date": "2024-08-21 04:45:12 UTC",
    "updated_date": "2024-08-21 04:45:12 UTC"
  },
  {
    "arxiv_id": "2408.11334v1",
    "title": "BURExtract-Llama: An LLM for Clinical Concept Extraction in Breast Ultrasound Reports",
    "authors": [
      "Yuxuan Chen",
      "Haoyan Yang",
      "Hengkai Pan",
      "Fardeen Siddiqui",
      "Antonio Verdone",
      "Qingyang Zhang",
      "Sumit Chopra",
      "Chen Zhao",
      "Yiqiu Shen"
    ],
    "abstract": "Breast ultrasound is essential for detecting and diagnosing abnormalities,\nwith radiology reports summarizing key findings like lesion characteristics and\nmalignancy assessments. Extracting this critical information is challenging due\nto the unstructured nature of these reports, with varied linguistic styles and\ninconsistent formatting. While proprietary LLMs like GPT-4 are effective, they\nare costly and raise privacy concerns when handling protected health\ninformation. This study presents a pipeline for developing an in-house LLM to\nextract clinical information from radiology reports. We first use GPT-4 to\ncreate a small labeled dataset, then fine-tune a Llama3-8B model on it.\nEvaluated on clinician-annotated reports, our model achieves an average F1\nscore of 84.6%, which is on par with GPT-4. Our findings demonstrate the\nfeasibility of developing an in-house LLM that not only matches GPT-4's\nperformance but also offers cost reductions and enhanced data privacy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper has been accepted as the oral paper for the HCHM workshop,\n  ACM Multimedia 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11334v1",
    "published_date": "2024-08-21 04:33:05 UTC",
    "updated_date": "2024-08-21 04:33:05 UTC"
  },
  {
    "arxiv_id": "2408.11327v2",
    "title": "Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking Across Diverse Vocabularies",
    "authors": [
      "Sai Koneru",
      "Matthias Huck",
      "Miriam Exel",
      "Jan Niehues"
    ],
    "abstract": "Recent advancements in NLP have resulted in models with specialized\nstrengths, such as processing multimodal inputs or excelling in specific\ndomains. However, real-world tasks, like multimodal translation, often require\na combination of these strengths, such as handling both translation and image\nprocessing. While individual translation and vision models are powerful, they\ntypically lack the ability to perform both tasks in a single system. Combining\nthese models poses challenges, particularly due to differences in their\nvocabularies, which limit the effectiveness of traditional ensemble methods to\npost-generation techniques like N-best list re-ranking. In this work, we\npropose a novel zero-shot ensembling strategy that allows for the integration\nof different models during the decoding phase without the need for additional\ntraining. Our approach re-ranks beams during decoding by combining scores at\nthe word level, using heuristics to predict when a word is completed. We\ndemonstrate the effectiveness of this method in machine translation scenarios,\nshowing that it enables the generation of translations that are both speech-\nand image-aware while also improving overall translation quality (We will\nrelease the code upon paper acceptance.).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "WMT 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11327v2",
    "published_date": "2024-08-21 04:20:55 UTC",
    "updated_date": "2024-11-04 12:17:42 UTC"
  },
  {
    "arxiv_id": "2408.11326v1",
    "title": "Automating Thought of Search: A Journey Towards Soundness and Completeness",
    "authors": [
      "Daniel Cao",
      "Michael Katz",
      "Harsha Kokel",
      "Kavitha Srinivas",
      "Shirin Sohrabi"
    ],
    "abstract": "Planning remains one of the last standing bastions for large language models\n(LLMs), which now turn their attention to search. Most of the literature uses\nthe language models as world models to define the search space, forgoing\nsoundness for the sake of flexibility. A recent work, Thought of Search (ToS),\nproposed defining the search space with code, having the language models\nproduce that code. ToS requires a human in the loop, collaboratively producing\na sound successor function and goal test. The result, however, is worth the\neffort: all the tested datasets were solved with 100% accuracy. At the same\ntime LLMs have demonstrated significant progress in code generation and\nrefinement for complex reasoning tasks. In this work, we automate ToS\n(AutoToS), completely taking the human out of the loop of solving planning\nproblems. AutoToS guides the language model step by step towards the generation\nof sound and complete search components, through feedback from both generic and\ndomain specific unit tests. We achieve 100% accuracy, with minimal feedback\niterations, using LLMs of various sizes on all evaluated domains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11326v1",
    "published_date": "2024-08-21 04:19:52 UTC",
    "updated_date": "2024-08-21 04:19:52 UTC"
  },
  {
    "arxiv_id": "2408.11319v2",
    "title": "SarcasmBench: Towards Evaluating Large Language Models on Sarcasm Understanding",
    "authors": [
      "Yazhou Zhang",
      "Chunwang Zou",
      "Zheng Lian",
      "Prayag Tiwari",
      "Jing Qin"
    ],
    "abstract": "In the era of large language models (LLMs), the task of ``System I''~-~the\nfast, unconscious, and intuitive tasks, e.g., sentiment analysis, text\nclassification, etc., have been argued to be successfully solved. However,\nsarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices\nlike hyperbole and figuration to convey true sentiments and intentions,\ninvolving a higher level of abstraction than sentiment analysis. There is\ngrowing concern that the argument about LLMs' success may not be fully tenable\nwhen considering sarcasm understanding. To address this question, we select\neleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present\ncomprehensive evaluations on six widely used benchmark datasets through\ndifferent prompting approaches, i.e., zero-shot input/output (IO) prompting,\nfew-shot IO prompting, chain of thought (CoT) prompting. Our results highlight\nthree key findings: (1) current LLMs underperform supervised PLMs based sarcasm\ndetection baselines across six sarcasm benchmarks. This suggests that\nsignificant efforts are still required to improve LLMs' understanding of human\nsarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across\nvarious prompting methods, with an average improvement of 14.0\\%$\\uparrow$.\nClaude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3)\nFew-shot IO prompting method outperforms the other two methods: zero-shot IO\nand few-shot CoT. The reason is that sarcasm detection, being a holistic,\nintuitive, and non-rational cognitive process, is argued not to adhere to\nstep-by-step logical reasoning, making CoT less effective in understanding\nsarcasm compared to its effectiveness in mathematical reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11319v2",
    "published_date": "2024-08-21 03:59:51 UTC",
    "updated_date": "2024-08-24 03:58:40 UTC"
  },
  {
    "arxiv_id": "2408.11316v2",
    "title": "Probabilistic Medical Predictions of Large Language Models",
    "authors": [
      "Bowen Gu",
      "Rishi J. Desai",
      "Kueiyu Joshua Lin",
      "Jie Yang"
    ],
    "abstract": "Large Language Models (LLMs) have shown promise in clinical applications\nthrough prompt engineering, allowing flexible clinical predictions. However,\nthey struggle to produce reliable prediction probabilities, which are crucial\nfor transparency and decision-making. While explicit prompts can lead LLMs to\ngenerate probability estimates, their numerical reasoning limitations raise\nconcerns about reliability. We compared explicit probabilities from text\ngeneration to implicit probabilities derived from the likelihood of predicting\nthe correct label token. Across six advanced open-source LLMs and five medical\ndatasets, explicit probabilities consistently underperformed implicit\nprobabilities in discrimination, precision, and recall. This discrepancy is\nmore pronounced with smaller LLMs and imbalanced datasets, highlighting the\nneed for cautious interpretation, improved probability estimation methods, and\nfurther research for clinical use of LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint. Under review",
    "pdf_url": "http://arxiv.org/pdf/2408.11316v2",
    "published_date": "2024-08-21 03:47:17 UTC",
    "updated_date": "2024-12-03 21:54:39 UTC"
  },
  {
    "arxiv_id": "2408.11313v2",
    "title": "An Optimizable Suffix Is Worth A Thousand Templates: Efficient Black-box Jailbreaking without Affirmative Phrases via LLM as Optimizer",
    "authors": [
      "Weipeng Jiang",
      "Zhenting Wang",
      "Juan Zhai",
      "Shiqing Ma",
      "Zhengyu Zhao",
      "Chao Shen"
    ],
    "abstract": "Despite prior safety alignment efforts, mainstream LLMs can still generate\nharmful and unethical content when subjected to jailbreaking attacks. Existing\njailbreaking methods fall into two main categories: template-based and\noptimization-based methods. The former requires significant manual effort and\ndomain knowledge, while the latter, exemplified by Greedy Coordinate Gradient\n(GCG), which seeks to maximize the likelihood of harmful LLM outputs through\ntoken-level optimization, also encounters several limitations: requiring\nwhite-box access, necessitating pre-constructed affirmative phrase, and\nsuffering from low efficiency. In this paper, we present ECLIPSE, a novel and\nefficient black-box jailbreaking method utilizing optimizable suffixes. Drawing\ninspiration from LLMs' powerful generation and optimization capabilities, we\nemploy task prompts to translate jailbreaking goals into natural language\ninstructions. This guides the LLM to generate adversarial suffixes for\nmalicious queries. In particular, a harmfulness scorer provides continuous\nfeedback, enabling LLM self-reflection and iterative optimization to\nautonomously and efficiently produce effective suffixes. Experimental results\ndemonstrate that ECLIPSE achieves an average attack success rate (ASR) of 0.92\nacross three open-source LLMs and GPT-3.5-Turbo, significantly surpassing GCG\nin 2.4 times. Moreover, ECLIPSE is on par with template-based methods in ASR\nwhile offering superior attack efficiency, reducing the average attack overhead\nby 83%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Be accepeted as NAACL2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2408.11313v2",
    "published_date": "2024-08-21 03:35:24 UTC",
    "updated_date": "2025-04-02 08:03:11 UTC"
  },
  {
    "arxiv_id": "2408.11312v3",
    "title": "Swarm Intelligence in Geo-Localization: A Multi-Agent Large Vision-Language Model Collaborative Framework",
    "authors": [
      "Xiao Han",
      "Chen Zhu",
      "Xiangyu Zhao",
      "Hengshu Zhu"
    ],
    "abstract": "Visual geo-localization demands in-depth knowledge and advanced reasoning\nskills to associate images with precise real-world geographic locations.\nExisting image database retrieval methods are limited by the impracticality of\nstoring sufficient visual records of global landmarks. Recently, Large\nVision-Language Models (LVLMs) have demonstrated the capability of\ngeo-localization through Visual Question Answering (VQA), enabling a solution\nthat does not require external geo-tagged image records. However, the\nperformance of a single LVLM is still limited by its intrinsic knowledge and\nreasoning capabilities. To address these challenges, we introduce smileGeo, a\nnovel visual geo-localization framework that leverages multiple\nInternet-enabled LVLM agents operating within an agent-based architecture. By\nfacilitating inter-agent communication, smileGeo integrates the inherent\nknowledge of these agents with additional retrieved information, enhancing the\nability to effectively localize images. Furthermore, our framework incorporates\na dynamic learning strategy that optimizes agent communication, reducing\nredundant interactions and enhancing overall system efficiency. To validate the\neffectiveness of the proposed framework, we conducted experiments on three\ndifferent datasets, and the results show that our approach significantly\noutperforms current state-of-the-art methods. The source code is available at\nhttps://anonymous.4open.science/r/ViusalGeoLocalization-F8F5.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "resubmit to www2025",
    "pdf_url": "http://arxiv.org/pdf/2408.11312v3",
    "published_date": "2024-08-21 03:31:30 UTC",
    "updated_date": "2024-10-15 06:16:00 UTC"
  },
  {
    "arxiv_id": "2408.11308v1",
    "title": "EEG-Defender: Defending against Jailbreak through Early Exit Generation of Large Language Models",
    "authors": [
      "Chongwen Zhao",
      "Zhihao Dou",
      "Kaizhu Huang"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly attracting attention in various\napplications. Nonetheless, there is a growing concern as some users attempt to\nexploit these models for malicious purposes, including the synthesis of\ncontrolled substances and the propagation of disinformation. In an effort to\nmitigate such risks, the concept of \"Alignment\" technology has been developed.\nHowever, recent studies indicate that this alignment can be undermined using\nsophisticated prompt engineering or adversarial suffixes, a technique known as\n\"Jailbreak.\" Our research takes cues from the human-like generate process of\nLLMs. We identify that while jailbreaking prompts may yield output logits\nsimilar to benign prompts, their initial embeddings within the model's latent\nspace tend to be more analogous to those of malicious prompts. Leveraging this\nfinding, we propose utilizing the early transformer outputs of LLMs as a means\nto detect malicious inputs, and terminate the generation immediately. Built\nupon this idea, we introduce a simple yet significant defense approach called\nEEG-Defender for LLMs. We conduct comprehensive experiments on ten jailbreak\nmethods across three models. Our results demonstrate that EEG-Defender is\ncapable of reducing the Attack Success Rate (ASR) by a significant margin,\nroughly 85\\% in comparison with 50\\% for the present SOTAs, with minimal impact\non the utility and effectiveness of LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.11308v1",
    "published_date": "2024-08-21 03:25:31 UTC",
    "updated_date": "2024-08-21 03:25:31 UTC"
  },
  {
    "arxiv_id": "2408.11306v2",
    "title": "Are KANs Effective for Multivariate Time Series Forecasting?",
    "authors": [
      "Xiao Han",
      "Xinfeng Zhang",
      "Yiling Wu",
      "Zhenduo Zhang",
      "Zhe Wu"
    ],
    "abstract": "Multivariate time series forecasting is a crucial task that predicts the\nfuture states based on historical inputs. Related techniques have been\ndeveloping in parallel with the machine learning community, from early\nstatistical learning methods to current deep learning methods. Despite their\nsignificant advancements, existing methods continue to struggle with the\nchallenge of inadequate interpretability. The rise of the Kolmogorov-Arnold\nNetwork (KAN) provides a new perspective to solve this challenge, but current\nwork has not yet concluded whether KAN is effective in time series forecasting\ntasks. In this paper, we aim to evaluate the effectiveness of KANs in\ntime-series forecasting from the perspectives of performance, integrability,\nefficiency, and interpretability. To this end, we propose the Multi-layer\nMixture-of-KAN network (MMK), which achieves excellent performance while\nretaining KAN's ability to be transformed into a combination of symbolic\nfunctions. The core module of MMK is the mixture-of-KAN layer, which uses a\nmixture-of-experts structure to assign variables to best-matched KAN experts.\nThen, we explore some useful experimental strategies to deal with the issues in\nthe training stage. Finally, we compare MMK and various baselines on seven\ndatasets. Extensive experimental and visualization results demonstrate that\nKANs are effective in multivariate time series forecasting. Code is available\nat: https://github.com/2448845600/EasyTSF.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11306v2",
    "published_date": "2024-08-21 03:21:52 UTC",
    "updated_date": "2025-02-11 03:38:57 UTC"
  },
  {
    "arxiv_id": "2408.11305v2",
    "title": "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation",
    "authors": [
      "Xiangyu Zhao",
      "Yuehan Zhang",
      "Wenlong Zhang",
      "Xiao-Ming Wu"
    ],
    "abstract": "The fashion domain encompasses a variety of real-world multimodal tasks,\nincluding multimodal retrieval and multimodal generation. The rapid\nadvancements in artificial intelligence generated content, particularly in\ntechnologies like large language models for text generation and diffusion\nmodels for visual generation, have sparked widespread research interest in\napplying these multimodal models in the fashion domain. However, tasks\ninvolving embeddings, such as image-to-text or text-to-image retrieval, have\nbeen largely overlooked from this perspective due to the diverse nature of the\nmultimodal fashion domain. And current research on multi-task single models\nlack focus on image generation. In this work, we present UniFashion, a unified\nframework that simultaneously tackles the challenges of multimodal generation\nand retrieval tasks within the fashion domain, integrating image generation\nwith retrieval tasks and text generation tasks. UniFashion unifies embedding\nand generative tasks by integrating a diffusion model and LLM, enabling\ncontrollable and high-fidelity generation. Our model significantly outperforms\nprevious single-task state-of-the-art models across diverse fashion tasks, and\ncan be readily adapted to manage complex vision-language tasks. This work\ndemonstrates the potential learning synergy between multimodal generation and\nretrieval, offering a promising direction for future research in the fashion\ndomain. The source code is available at\nhttps://github.com/xiangyu-mm/UniFashion.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by EMNLP 2024, main conference",
    "pdf_url": "http://arxiv.org/pdf/2408.11305v2",
    "published_date": "2024-08-21 03:17:20 UTC",
    "updated_date": "2024-10-12 14:13:58 UTC"
  },
  {
    "arxiv_id": "2408.11300v1",
    "title": "Offline Policy Learning via Skill-step Abstraction for Long-horizon Goal-Conditioned Tasks",
    "authors": [
      "Donghoon Kim",
      "Minjong Yoo",
      "Honguk Woo"
    ],
    "abstract": "Goal-conditioned (GC) policy learning often faces a challenge arising from\nthe sparsity of rewards, when confronting long-horizon goals. To address the\nchallenge, we explore skill-based GC policy learning in offline settings, where\nskills are acquired from existing data and long-horizon goals are decomposed\ninto sequences of near-term goals that align with these skills. Specifically,\nwe present an `offline GC policy learning via skill-step abstraction' framework\n(GLvSA) tailored for tackling long-horizon GC tasks affected by goal\ndistribution shifts. In the framework, a GC policy is progressively learned\noffline in conjunction with the incremental modeling of skill-step abstractions\non the data. We also devise a GC policy hierarchy that not only accelerates GC\npolicy learning within the framework but also allows for parameter-efficient\nfine-tuning of the policy. Through experiments with the maze and Franka kitchen\nenvironments, we demonstrate the superiority and efficiency of our GLvSA\nframework in adapting GC policies to a wide range of long-horizon goals. The\nframework achieves competitive zero-shot and few-shot adaptation performance,\noutperforming existing GC policy learning and skill-based methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 4 figures, International Joint Conference on Artificial\n  Intelligence 2024, Published version",
    "pdf_url": "http://arxiv.org/pdf/2408.11300v1",
    "published_date": "2024-08-21 03:05:06 UTC",
    "updated_date": "2024-08-21 03:05:06 UTC"
  },
  {
    "arxiv_id": "2408.11288v1",
    "title": "Applying and Evaluating Large Language Models in Mental Health Care: A Scoping Review of Human-Assessed Generative Tasks",
    "authors": [
      "Yining Hua",
      "Hongbin Na",
      "Zehan Li",
      "Fenglin Liu",
      "Xiao Fang",
      "David Clifton",
      "John Torous"
    ],
    "abstract": "Large language models (LLMs) are emerging as promising tools for mental\nhealth care, offering scalable support through their ability to generate\nhuman-like responses. However, the effectiveness of these models in clinical\nsettings remains unclear. This scoping review aimed to assess the current\ngenerative applications of LLMs in mental health care, focusing on studies\nwhere these models were tested with human participants in real-world scenarios.\nA systematic search across APA PsycNet, Scopus, PubMed, and Web of Science\nidentified 726 unique articles, of which 17 met the inclusion criteria. These\nstudies encompassed applications such as clinical assistance, counseling,\ntherapy, and emotional support. However, the evaluation methods were often\nnon-standardized, with most studies relying on ad hoc scales that limit\ncomparability and robustness. Privacy, safety, and fairness were also\nfrequently underexplored. Moreover, reliance on proprietary models, such as\nOpenAI's GPT series, raises concerns about transparency and reproducibility.\nWhile LLMs show potential in expanding mental health care access, especially in\nunderserved areas, the current evidence does not fully support their use as\nstandalone interventions. More rigorous, standardized evaluations and ethical\noversight are needed to ensure these tools can be safely and effectively\nintegrated into clinical practice.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11288v1",
    "published_date": "2024-08-21 02:21:59 UTC",
    "updated_date": "2024-08-21 02:21:59 UTC"
  },
  {
    "arxiv_id": "2408.11283v2",
    "title": "Inference Plans for Hybrid Particle Filtering",
    "authors": [
      "Ellie Y. Cheng",
      "Eric Atkinson",
      "Guillaume Baudart",
      "Louis Mandel",
      "Michael Carbin"
    ],
    "abstract": "Advanced probabilistic programming languages (PPLs) using hybrid particle\nfiltering combine symbolic exact inference and Monte Carlo methods to improve\ninference performance. These systems use heuristics to partition random\nvariables within the program into variables that are encoded symbolically and\nvariables that are encoded with sampled values, and the heuristics are not\nnecessarily aligned with the developer's performance evaluation metrics. In\nthis work, we present inference plans, a programming interface that enables\ndevelopers to control the partitioning of random variables during hybrid\nparticle filtering. We further present Siren, a new PPL that enables developers\nto use annotations to specify inference plans the inference system must\nimplement. To assist developers with statically reasoning about whether an\ninference plan can be implemented, we present an abstract-interpretation-based\nstatic analysis for Siren for determining inference plan satisfiability. We\nprove the analysis is sound with respect to Siren's semantics. Our evaluation\napplies inference plans to three different hybrid particle filtering algorithms\non a suite of benchmarks. It shows that the control provided by inference plans\nenables speed ups of 1.76x on average and up to 206x to reach a target\naccuracy, compared to the inference plans implemented by default heuristics;\nthe results also show that inference plans improve accuracy by 1.83x on average\nand up to 595x with less or equal runtime, compared to the default inference\nplans. We further show that our static analysis is precise in practice,\nidentifying all satisfiable inference plans in 27 out of the 33\nbenchmark-algorithm evaluation settings.",
    "categories": [
      "cs.PL",
      "cs.AI"
    ],
    "primary_category": "cs.PL",
    "comment": "v2: camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2408.11283v2",
    "published_date": "2024-08-21 02:07:03 UTC",
    "updated_date": "2024-12-14 16:19:30 UTC"
  },
  {
    "arxiv_id": "2408.11281v2",
    "title": "BearLLM: A Prior Knowledge-Enhanced Bearing Health Management Framework with Unified Vibration Signal Representation",
    "authors": [
      "Haotian Peng",
      "Jiawei Liu",
      "Jinsong Du",
      "Jie Gao",
      "Wei Wang"
    ],
    "abstract": "We propose a bearing health management framework leveraging large language\nmodels (BearLLM), a novel multimodal model that unifies multiple\nbearing-related tasks by processing user prompts and vibration signals.\nSpecifically, we introduce a prior knowledge-enhanced unified vibration signal\nrepresentation to handle various working conditions across multiple datasets.\nThis involves adaptively sampling the vibration signals based on the sampling\nrate of the sensor, incorporating the frequency domain to unify input\ndimensions, and using a fault-free reference signal as an auxiliary input. To\nextract features from vibration signals, we first train a fault classification\nnetwork, then convert and align the extracted features into word embedding, and\nfinally concatenate these with text embedding as input to an LLM. To evaluate\nthe performance of the proposed method, we constructed the first large-scale\nmultimodal bearing health management (MBHM) dataset, including paired vibration\nsignals and textual descriptions. With our unified vibration signal\nrepresentation, BearLLM using one set of pre-trained weights achieves\nstate-of-the-art performance on nine publicly available fault diagnosis\nbenchmarks, outperforming specific methods designed for individual datasets. We\nprovide a dataset, our model, and code to inspire future research on building\nmore capable industrial multimodal models https://github.com/SIA-IDE/BearLLM.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2408.11281v2",
    "published_date": "2024-08-21 02:04:54 UTC",
    "updated_date": "2024-12-16 03:08:16 UTC"
  },
  {
    "arxiv_id": "2408.11261v1",
    "title": "Towards Analyzing and Mitigating Sycophancy in Large Vision-Language Models",
    "authors": [
      "Yunpu Zhao",
      "Rui Zhang",
      "Junbin Xiao",
      "Changxin Ke",
      "Ruibo Hou",
      "Yifan Hao",
      "Qi Guo",
      "Yunji Chen"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have shown significant capability in\nvision-language understanding. However, one critical issue that persists in\nthese models is sycophancy, which means models are unduly influenced by leading\nor deceptive prompts, resulting in biased outputs and hallucinations. Despite\nthe progress in LVLMs, evaluating and mitigating sycophancy is yet much\nunder-explored. In this work, we fill this gap by systematically analyzing\nsycophancy on various VL benchmarks with curated leading queries and further\nproposing a text contrastive decoding method for mitigation. While the specific\nsycophantic behavior varies significantly among models, our analysis reveals\nthe severe deficiency of all LVLMs in resilience of sycophancy across various\ntasks. For improvement, we propose Leading Query Contrastive Decoding (LQCD), a\nmodel-agnostic method focusing on calibrating the LVLMs' over-reliance on\nleading cues by identifying and suppressing the probabilities of sycophancy\ntokens at the decoding stage. Extensive experiments show that LQCD effectively\nmitigate sycophancy, outperforming both prompt engineering methods and common\nmethods for hallucination mitigation. We further demonstrate that LQCD does not\nhurt but even slightly improves LVLMs' responses to neutral queries, suggesting\nit being a more effective strategy for general-purpose decoding but not limited\nto sycophancy.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11261v1",
    "published_date": "2024-08-21 01:03:21 UTC",
    "updated_date": "2024-08-21 01:03:21 UTC"
  },
  {
    "arxiv_id": "2408.11258v1",
    "title": "Improving Speech Recognition Error Prediction for Modern and Off-the-shelf Speech Recognizers",
    "authors": [
      "Prashant Serai",
      "Peidong Wang",
      "Eric Fosler-Lussier"
    ],
    "abstract": "Modeling the errors of a speech recognizer can help simulate errorful\nrecognized speech data from plain text, which has proven useful for tasks like\ndiscriminative language modeling, improving robustness of NLP systems, where\nlimited or even no audio data is available at train time. Previous work\ntypically considered replicating behavior of GMM-HMM based systems, but the\nbehavior of more modern posterior-based neural network acoustic models is not\nthe same and requires adjustments to the error prediction model. In this work,\nwe extend a prior phonetic confusion based model for predicting speech\nrecognition errors in two ways: first, we introduce a sampling-based paradigm\nthat better simulates the behavior of a posterior-based acoustic model. Second,\nwe investigate replacing the confusion matrix with a sequence-to-sequence model\nin order to introduce context dependency into the prediction. We evaluate the\nerror predictors in two ways: first by predicting the errors made by a\nSwitchboard ASR system on unseen data (Fisher), and then using that same\npredictor to estimate the behavior of an unrelated cloud-based ASR system on a\nnovel task. Sampling greatly improves predictive accuracy within a 100-guess\nparadigm, while the sequence model performs similarly to the confusion matrix.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11258v1",
    "published_date": "2024-08-21 00:48:03 UTC",
    "updated_date": "2024-08-21 00:48:03 UTC"
  },
  {
    "arxiv_id": "2408.11253v2",
    "title": "Automatic Image Annotation (AIA) of AlmondNet-20 Method for Almond Detection by Improved CNN-based Model",
    "authors": [
      "Mohsen Asghari Ilani",
      "Saba Moftakhar Tehran",
      "Ashkan Kavei",
      "Arian Radmehr"
    ],
    "abstract": "In response to the burgeoning global demand for premium agricultural\nproducts, particularly within the competitive nut market, this paper introduces\nan innovative methodology aimed at enhancing the grading process for almonds\nand their shells. Leveraging state-of-the-art Deep Convolutional Neural\nNetworks (CNNs), specifically the AlmondNet-20 architecture, our study achieves\nexceptional accuracy exceeding 99%, facilitated by the utilization of a\n20-layer CNN model. To bolster robustness in differentiating between almonds\nand shells, data augmentation techniques are employed, ensuring the reliability\nand accuracy of our classification system. Our model, meticulously trained over\n1000 epochs, demonstrates remarkable performance, boasting an accuracy rate of\n99% alongside a minimal loss function of 0.0567. Rigorous evaluation through\ntest datasets further validates the efficacy of our approach, revealing\nimpeccable precision, recall, and F1-score metrics for almond detection. Beyond\nits technical prowess, this advanced classification system offers tangible\nbenefits to both industry experts and non-specialists alike, ensuring globally\nreliable almond classification. The application of deep learning algorithms, as\nshowcased in our study, not only enhances grading accuracy but also presents\nopportunities for product patents, thereby contributing to the economic value\nof our nation. Through the adoption of cutting-edge technologies such as the\nAlmondNet-20 model, we pave the way for future advancements in agricultural\nproduct classification, ultimately enriching global trade and economic\nprosperity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11253v2",
    "published_date": "2024-08-21 00:20:08 UTC",
    "updated_date": "2024-12-04 23:50:05 UTC"
  }
]
{
  "date": "2024-01-23",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-01-23 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 88 篇论文，主要聚焦于 AI 模型的安全、对齐、解释性以及应用，如大语言模型（LLMs）在多语言安全和医疗问答中的挑战，以及强化学习在机器人和动态环境中的创新；令人印象深刻的包括 ARGS 框架提升 LLM 对齐效率，以及 AutoRT 系统实现大规模机器人代理编排。\n\n下面，我挑选并简要讨论部分重要论文，先聊 LLMs 和 AI 安全相关的高话题度文章，再聊机器人和强化学习创新，再快速掠过其他领域的内容。每个条目列出论文标题（中文 + 英文），并突出核心贡献和发现。\n\n### LLMs 和 AI 安全\n- **ARGS: Alignment as Reward-Guided Search（中文：基于奖励引导搜索的对齐框架）**  \n  作者包括 Yixuan Li，这篇 ICLR 2024 论文提出 ARGS 框架，通过奖励信号调整 LLM 的解码过程，实现高效的人类偏好对齐，避免了昂贵的强化学习训练；在多任务测试中，平均奖励提升 19.56%，并在 GPT-4 评估中获得 64.33% 的偏好得分，显著提高了 LLM 的响应性和鲁棒性。\n\n- **The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts（中文：剖析大语言模型在多语言环境中的安全挑战）**  \n  这篇论文发现 LLM 在低资源语言的恶意提示下更容易生成不安全或无关响应；通过分析 RLHF 和 SFT 的影响，强调预训练阶段是跨语言对齐的瓶颈，并建议未来研究关注预训练优化以提升多语言安全。\n\n- **Visibility into AI Agents（中文：AI 代理的可视化）**  \n  作者包括 David Krueger 和 Markus Anderljung，这篇 ACM FAccT 2024 论文探讨了提升 AI 代理透明性的措施，如代理标识符、实时监控和活动日志；它分析了这些方法在不同部署环境下的应用，并讨论了隐私和权力集中的影响，提供了一个治理 AI 风险的框架。\n\n- **XAI for All: Can Large Language Models Simplify Explainable AI?（中文：大语言模型能否简化可解释 AI）**  \n  这篇论文引入 x-[plAIn] 方法，使用 ChatGPT 构建的 LLM 生成针对不同受众（如商业和学术）的 XAI 总结；结果显示，该模型能提供易懂且适应性的解释，提升了 AI 概念的可访问性。\n\n- **Towards Trustable Language Models: Investigating Information Quality of Large Language Models（中文：探究大语言模型的信息质量以提升可信度）**  \n  论文分析 LLM 的信息质量问题，如幻觉和偏差，并提出新数学评估方法；发现信息质量下降会导致商业决策错误，并讨论了缩放定律对 LLM 改进的影响。\n\n- **Quality of Answers of Generative Large Language Models vs Peer Patients（中文：生成式大语言模型与患者同伴在实验室测试解释中的回答质量比较）**  \n  这篇论文比较 GPT-4 等 LLM 与患者在医疗问答中的表现；GPT-4 在相关性、正确性和安全性上优于其他模型和人类，但存在个性化不足的问题，建议通过增强数据改进 LLM 响应。\n\n### 机器人和强化学习\n- **AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents（中文：用于大规模机器人代理编排的具身基础模型）**  \n  作者包括 Sergey Levine，这篇 ICRA 2024 论文提出 AutoRT 框架，使用 VLMs 和 LLMs 指导机器人收集数据并执行任务；它实现了 77k 真实机器人剧集的收集，并在动态环境中提升了代理的适应性和安全性。\n\n- **HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments（中文：HAZARD 挑战：动态环境中的具身决策）**  \n  这篇 ICLR 2024 论文引入 HAZARD 基准，评估强化学习代理在动态灾害场景（如火灾）中的决策；使用 LLMs 辅助决策，展示了代理在不确定环境中的鲁棒性，并公开了数据集。\n\n- **Locality Sensitive Sparse Encoding for Learning World Models Online（中文：用于在线学习世界模型的局部敏感稀疏编码）**  \n  论文提出一种线性回归结合非线性特征的框架，用于强化学习中的在线世界模型学习；它实现了高效的 FTL 更新，并在 Dyna 设置中匹配深度模型性能。\n\n### 其他领域快速掠过\n以下论文主题较分散或较为专业，我仅简要概述主要贡献：\n- **Distributional Counterfactual Explanations With Optimal Transport（中文：基于最优传输的分布式反事实解释）**  \n  引入 DCE 方法，使用最优传输优化反事实解释，提供更全面的决策洞见。\n- **Enhancing Global Maritime Traffic Network Forecasting with Gravity-Inspired Deep Learning Models（中文：使用重力启发深度学习模型提升全球海事交通网络预测）**  \n  提出物理启发框架，结合 Transformer 提升交通流量预测准确率至 89%。\n- **Digital Divides in Scene Recognition（中文：场景识别中的数字鸿沟）**  \n  揭示 dCNNs 在低 SES 图像中的偏见，强调需要更具包容性的训练数据集。\n- **IndiText Boost（中文：针对低资源印度语言的文本增强）**  \n  为印度语言实现文本分类增强，基本方法优于 LLM。\n- **CIMGEN（中文：基于有限数据的预训练生成模型的受控图像操作）**  \n  开发图像编辑框架，实现了伪造图像生成并规避检测。\n- **SGTR+（中文：基于 Transformer 的端到端场景图生成）**  \n  改进场景图生成算法，提升了准确性和效率。\n- **其他论文** 如强化学习、图像分割和医学应用等，共 70 多篇，由于篇幅有限，仅提及其领域创新（如神经网络验证、PDF 结构识别等），但未详细展开。这些论文在各自领域有技术贡献，但整体话题度较低。\n\n今天的 arXiv 更新突显了 AI 模型在安全和应用上的进展，值得关注 LLMs 和机器人领域的创新。更多细节可查阅 arXiv。明天见！",
  "papers": [
    {
      "arxiv_id": "2402.01694v1",
      "title": "ARGS: Alignment as Reward-Guided Search",
      "title_zh": "翻译失败",
      "authors": [
        "Maxim Khanov",
        "Jirayu Burapacheep",
        "Yixuan Li"
      ],
      "abstract": "Aligning large language models with human objectives is paramount, yet common\napproaches including RLHF suffer from unstable and resource-intensive training.\nIn response to this challenge, we introduce ARGS, Alignment as Reward-Guided\nSearch, a novel framework that integrates alignment into the decoding process,\neliminating the need for expensive RL training. By adjusting the model's\nprobabilistic predictions using a reward signal, ARGS generates texts with\nsemantic diversity while being aligned with human preferences, offering a\npromising and flexible solution for aligning language models. Notably, ARGS\ndemonstrates consistent enhancements in average reward compared to baselines\nacross diverse alignment tasks and various model dimensions. For example, under\nthe same greedy-based decoding strategy, our method improves the average reward\nby 19.56% relative to the baseline and secures a preference or tie score of\n64.33% in GPT-4 evaluation. We believe that our framework, emphasizing\ndecoding-time alignment, paves the way for more responsive language models in\nthe future. Code is publicly available at:\n\\url{https://github.com/deeplearning-wisc/args}.",
      "tldr_zh": "本研究提出ARGS框架（Alignment as Reward-Guided Search），一种无需昂贵RLHF训练的新方法，用于将大型语言模型（LLMs）与人类目标对齐。ARGS通过在解码过程中使用奖励信号调整模型的概率预测，生成语义多样且符合人类偏好的文本，从而避免了传统方法的不稳定性和资源消耗。在实验中，ARGS在各种对齐任务中比基线模型平均奖励提高了19.56%，并在GPT-4评估中获得64.33%的偏好或平局分数。该框架强调解码时的对齐，为未来更响应式的语言模型提供了灵活且高效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.01694v1",
      "published_date": "2024-01-23 23:42:41 UTC",
      "updated_date": "2024-01-23 23:42:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:38:36.511874"
    },
    {
      "arxiv_id": "2401.13138v6",
      "title": "Visibility into AI Agents",
      "title_zh": "AI 代理的可见性",
      "authors": [
        "Alan Chan",
        "Carson Ezell",
        "Max Kaufmann",
        "Kevin Wei",
        "Lewis Hammond",
        "Herbie Bradley",
        "Emma Bluemke",
        "Nitarshan Rajkumar",
        "David Krueger",
        "Noam Kolt",
        "Lennart Heim",
        "Markus Anderljung"
      ],
      "abstract": "Increased delegation of commercial, scientific, governmental, and personal\nactivities to AI agents -- systems capable of pursuing complex goals with\nlimited supervision -- may exacerbate existing societal risks and introduce new\nrisks. Understanding and mitigating these risks involves critically evaluating\nexisting governance structures, revising and adapting these structures where\nneeded, and ensuring accountability of key stakeholders. Information about\nwhere, why, how, and by whom certain AI agents are used, which we refer to as\nvisibility, is critical to these objectives. In this paper, we assess three\ncategories of measures to increase visibility into AI agents: agent\nidentifiers, real-time monitoring, and activity logging. For each, we outline\npotential implementations that vary in intrusiveness and informativeness. We\nanalyze how the measures apply across a spectrum of centralized through\ndecentralized deployment contexts, accounting for various actors in the supply\nchain including hardware and software service providers. Finally, we discuss\nthe implications of our measures for privacy and concentration of power.\nFurther work into understanding the measures and mitigating their negative\nimpacts can help to build a foundation for the governance of AI agents.",
      "tldr_zh": "这篇论文探讨了将商业、科学、政府和个人活动委托给 AI agents 的潜在风险，包括社会风险的加剧，并强调了 visibility（可见性）——即关于 AI agents 使用情况的信息——在治理和问责制中的关键作用。作者评估了三种增加 visibility 的措施：agent identifiers（代理标识符）、real-time monitoring（实时监控）和 activity logging（活动日志），并分析了这些措施在从集中式到去中心化部署环境中的实现方式及其对供应链参与者的影响。论文讨论了这些措施可能带来的隐私和权力集中问题，并提出进一步研究以建立有效的 AI agents 治理基础。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency (ACM FAccT 2024)",
      "pdf_url": "http://arxiv.org/pdf/2401.13138v6",
      "published_date": "2024-01-23 23:18:33 UTC",
      "updated_date": "2024-05-17 17:45:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:38:48.241359"
    },
    {
      "arxiv_id": "2401.13136v1",
      "title": "The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts",
      "title_zh": "语言障碍：剖析 LLMs 在多语言语境中的安全挑战",
      "authors": [
        "Lingfeng Shen",
        "Weiting Tan",
        "Sihao Chen",
        "Yunmo Chen",
        "Jingyu Zhang",
        "Haoran Xu",
        "Boyuan Zheng",
        "Philipp Koehn",
        "Daniel Khashabi"
      ],
      "abstract": "As the influence of large language models (LLMs) spans across global\ncommunities, their safety challenges in multilingual settings become paramount\nfor alignment research. This paper examines the variations in safety challenges\nfaced by LLMs across different languages and discusses approaches to\nalleviating such concerns. By comparing how state-of-the-art LLMs respond to\nthe same set of malicious prompts written in higher- vs. lower-resource\nlanguages, we observe that (1) LLMs tend to generate unsafe responses much more\noften when a malicious prompt is written in a lower-resource language, and (2)\nLLMs tend to generate more irrelevant responses to malicious prompts in\nlower-resource languages. To understand where the discrepancy can be\nattributed, we study the effect of instruction tuning with reinforcement\nlearning from human feedback (RLHF) or supervised finetuning (SFT) on the\nHH-RLHF dataset. Surprisingly, while training with high-resource languages\nimproves model alignment, training in lower-resource languages yields minimal\nimprovement. This suggests that the bottleneck of cross-lingual alignment is\nrooted in the pretraining stage. Our findings highlight the challenges in\ncross-lingual LLM safety, and we hope they inform future research in this\ndirection.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在多语言环境中的安全挑战，通过比较模型对恶意提示在高资源语言和高低资源语言中的响应，发现 LLMs 在低资源语言下更频繁生成不安全或无关响应。研究者分析了指令微调技术，如 reinforcement learning from human feedback (RLHF) 或 supervised finetuning (SFT)，结果显示高资源语言训练能显著改善模型对齐，而低资源语言训练效果有限，问题可能源于预训练阶段。主要贡献在于揭示跨语言对齐的瓶颈，并为未来 LLM 安全研究提供指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13136v1",
      "published_date": "2024-01-23 23:12:09 UTC",
      "updated_date": "2024-01-23 23:12:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:39:00.427758"
    },
    {
      "arxiv_id": "2402.01693v1",
      "title": "Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study",
      "title_zh": "翻译失败",
      "authors": [
        "Zhe He",
        "Balu Bhasuran",
        "Qiao Jin",
        "Shubo Tian",
        "Karim Hanna",
        "Cindy Shavor",
        "Lisbeth Garcia Arguello",
        "Patrick Murray",
        "Zhiyong Lu"
      ],
      "abstract": "Lab results are often confusing and hard to understand. Large language models\n(LLMs) such as ChatGPT have opened a promising avenue for patients to get their\nquestions answered. We aim to assess the feasibility of using LLMs to generate\nrelevant, accurate, helpful, and unharmful responses to lab test-related\nquestions asked by patients and to identify potential issues that can be\nmitigated with augmentation approaches. We first collected lab test results\nrelated question and answer data from Yahoo! Answers and selected 53 QA pairs\nfor this study. Using the LangChain framework and ChatGPT web portal, we\ngenerated responses to the 53 questions from four LLMs including GPT-4, Meta\nLLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their\nanswers using standard QA similarity-based evaluation metrics including ROUGE,\nBLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge\nwhether a target model has higher quality in terms of relevance, correctness,\nhelpfulness, and safety than the baseline model. Finally, we performed a manual\nevaluation with medical experts for all the responses to seven selected\nquestions on the same four aspects. The results of Win Rate and medical expert\nevaluation both showed that GPT-4's responses achieved better scores than all\nthe other LLM responses and human responses on all four aspects (relevance,\ncorrectness, helpfulness, and safety). However, LLM responses occasionally also\nsuffer from a lack of interpretation in one's medical context, incorrect\nstatements, and lack of references. We find that compared to other three LLMs\nand human answer from the Q&A website, GPT-4's responses are more accurate,\nhelpful, relevant, and safer. However, there are cases which GPT-4 responses\nare inaccurate and not individualized. We identified a number of ways to\nimprove the quality of LLM responses.",
      "tldr_zh": "这篇论文评估了生成式大型语言模型 (LLMs) 如 GPT-4 与患者同伴在解释实验室测试结果方面的回答质量，焦点是相关性、正确性、有帮助性和安全性。研究者收集了53对来自 Yahoo! Answers 的问题答案，使用 LangChain 生成并评估了 GPT-4、Meta LLaMA 2、MedAlpaca 和 ORCA_mini 的响应，通过 ROUGE、BLEU、METEOR、BERTScore 等指标以及医疗专家手动评估进行比较。结果显示，GPT-4 的回答在所有评估方面均优于其他 LLMs 和人类基准，但偶尔存在解释不充分、错误陈述或缺少引用的问题。论文提出了改进 LLMs 响应的潜在策略，以提升其在医疗领域的可靠性和个性化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01693v1",
      "published_date": "2024-01-23 22:03:51 UTC",
      "updated_date": "2024-01-23 22:03:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:39:13.894222"
    },
    {
      "arxiv_id": "2401.13112v6",
      "title": "Distributional Counterfactual Explanations With Optimal Transport",
      "title_zh": "基于最优传输的分布型反事实解释",
      "authors": [
        "Lei You",
        "Lele Cao",
        "Mattias Nilsson",
        "Bo Zhao",
        "Lei Lei"
      ],
      "abstract": "Counterfactual explanations (CE) are the de facto method for providing\ninsights into black-box decision-making models by identifying alternative\ninputs that lead to different outcomes. However, existing CE approaches,\nincluding group and global methods, focus predominantly on specific input\nmodifications, lacking the ability to capture nuanced distributional\ncharacteristics that influence model outcomes across the entire input-output\nspectrum. This paper proposes distributional counterfactual explanation (DCE),\nshifting focus to the distributional properties of observed and counterfactual\ndata, thus providing broader insights. DCE is particularly beneficial for\nstakeholders making strategic decisions based on statistical data analysis, as\nit makes the statistical distribution of the counterfactual resembles the one\nof the factual when aligning model outputs with a target\ndistribution\\textemdash something that the existing CE methods cannot fully\nachieve. We leverage optimal transport (OT) to formulate a chance-constrained\noptimization problem, deriving a counterfactual distribution aligned with its\nfactual counterpart, supported by statistical confidence. The efficacy of this\napproach is demonstrated through experiments, highlighting its potential to\nprovide deeper insights into decision-making models.",
      "tldr_zh": "本论文提出分布式反事实解释(Distributional Counterfactual Explanation, DCE)，以解决传统反事实解释(Counterfactual Explanations, CE)方法过于关注特定输入修改，而忽略了输入输出分布特性的问题，从而提供更全面的模型决策洞见。DCE 通过关注观察数据和反事实数据的分布特性，帮助利益相关者基于统计分析做出战略决策，确保反事实分布与事实分布对齐，同时实现模型输出目标。论文利用最优传输(Optimal Transport, OT)制定一个机会约束优化问题，生成具有统计置信度的反事实分布。实验结果证明，该方法在决策模型解释方面表现出色，提供更深入的见解。",
      "categories": [
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13112v6",
      "published_date": "2024-01-23 21:48:52 UTC",
      "updated_date": "2025-03-12 11:53:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:39:24.241581"
    },
    {
      "arxiv_id": "2401.13110v1",
      "title": "XAI for All: Can Large Language Models Simplify Explainable AI?",
      "title_zh": "翻译失败",
      "authors": [
        "Philip Mavrepis",
        "Georgios Makridis",
        "Georgios Fatouros",
        "Vasileios Koukos",
        "Maria Margarita Separdani",
        "Dimosthenis Kyriazis"
      ],
      "abstract": "The field of Explainable Artificial Intelligence (XAI) often focuses on users\nwith a strong technical background, making it challenging for non-experts to\nunderstand XAI methods. This paper presents \"x-[plAIn]\", a new approach to make\nXAI more accessible to a wider audience through a custom Large Language Model\n(LLM), developed using ChatGPT Builder. Our goal was to design a model that can\ngenerate clear, concise summaries of various XAI methods, tailored for\ndifferent audiences, including business professionals and academics. The key\nfeature of our model is its ability to adapt explanations to match each\naudience group's knowledge level and interests. Our approach still offers\ntimely insights, facilitating the decision-making process by the end users.\nResults from our use-case studies show that our model is effective in providing\neasy-to-understand, audience-specific explanations, regardless of the XAI\nmethod used. This adaptability improves the accessibility of XAI, bridging the\ngap between complex AI technologies and their practical applications. Our\nfindings indicate a promising direction for LLMs in making advanced AI concepts\nmore accessible to a diverse range of users.",
      "tldr_zh": "本研究探讨大型语言模型（LLM）是否能简化可解释人工智能（XAI），以使其更易于非技术专家理解。论文提出“x-[plAIn]”方法，使用ChatGPT Builder开发自定义LLM，生成清晰简洁的XAI方法总结，并根据不同受众（如商业专业人士和学术人员）的知识水平和兴趣进行适应。关键特征包括提供及时见解以辅助决策，以及提升XAI的可访问性。实验结果显示，该模型在用例研究中有效桥接复杂AI技术和实际应用，为LLM在普及高级AI概念方面提供了有前景的方向。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13110v1",
      "published_date": "2024-01-23 21:47:12 UTC",
      "updated_date": "2024-01-23 21:47:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:39:36.529692"
    },
    {
      "arxiv_id": "2401.13099v1",
      "title": "Sparse identification of nonlinear dynamics in the presence of library and system uncertainty",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew O'Brien"
      ],
      "abstract": "The SINDy algorithm has been successfully used to identify the governing\nequations of dynamical systems from time series data. However, SINDy assumes\nthe user has prior knowledge of the variables in the system and of a function\nlibrary that can act as a basis for the system. In this paper, we demonstrate\non real world data how the Augmented SINDy algorithm outperforms SINDy in the\npresence of system variable uncertainty. We then show SINDy can be further\naugmented to perform robustly when both kinds of uncertainty are present.",
      "tldr_zh": "本研究探讨了在库不确定性和系统不确定性存在的情况下，对非线性动态系统的稀疏识别问题。SINDy 算法通常依赖于用户对系统变量和函数库的先验知识，但本文展示了 Augmented SINDy 在真实世界数据上如何在系统变量不确定性下表现优于原 SINDy。通过进一步增强 SINDy，该方法能够稳健地处理两种不确定性，提高了动态系统方程识别的可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13099v1",
      "published_date": "2024-01-23 21:23:51 UTC",
      "updated_date": "2024-01-23 21:23:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:39:47.765126"
    },
    {
      "arxiv_id": "2401.13098v3",
      "title": "Enhancing Global Maritime Traffic Network Forecasting with Gravity-Inspired Deep Learning Models",
      "title_zh": "通过受引力模型启发的深度学习模型增强全球海事交通网络预测",
      "authors": [
        "Ruixin Song",
        "Gabriel Spadon",
        "Ronald Pelot",
        "Stan Matwin",
        "Amilcar Soares"
      ],
      "abstract": "Aquatic non-indigenous species (NIS) pose significant threats to\nbiodiversity, disrupting ecosystems and inflicting substantial economic damages\nacross agriculture, forestry, and fisheries. Due to the fast growth of global\ntrade and transportation networks, NIS has been introduced and spread\nunintentionally in new environments. This study develops a new physics-informed\nmodel to forecast maritime shipping traffic between port regions worldwide. The\npredicted information provided by these models, in turn, is used as input for\nrisk assessment of NIS spread through transportation networks to evaluate the\ncapability of our solution. Inspired by the gravity model for international\ntrades, our model considers various factors that influence the likelihood and\nimpact of vessel activities, such as shipping flux density, distance between\nports, trade flow, and centrality measures of transportation hubs. Accordingly,\nthis paper introduces transformers to gravity models to rebuild the short- and\nlong-term dependencies that make the risk analysis feasible. Thus, we introduce\na physics-inspired framework that achieves an 89% binary accuracy for existing\nand non-existing trajectories and an 84.8% accuracy for the number of vessels\nflowing between key port areas, representing more than 10% improvement over the\ntraditional deep-gravity model. Along these lines, this research contributes to\na better understanding of NIS risk assessment. It allows policymakers,\nconservationists, and stakeholders to prioritize management actions by\nidentifying high-risk invasion pathways. Besides, our model is versatile and\ncan include new data sources, making it suitable for assessing international\nvessel traffic flow in a changing global landscape.",
      "tldr_zh": "这篇论文针对水生外来入侵物种（NIS）的传播风险，提出了一种基于引力模型的深度学习框架，用于增强全球海运交通网络的预测。模型整合了Transformer来处理短时和长时依赖关系，并考虑了航运流量密度、港口距离、贸易流量以及交通枢纽中心性等因素。实验结果显示，该框架在轨迹预测的二元准确率达到89%，船只数量预测准确率达84.8%，比传统深度引力模型提高了10%。该研究有助于识别高风险入侵路径，支持政策制定者评估和管理NIS风险，并允许模型灵活融入新数据源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13098v3",
      "published_date": "2024-01-23 21:22:51 UTC",
      "updated_date": "2024-07-10 22:33:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:40:01.530938"
    },
    {
      "arxiv_id": "2401.13097v2",
      "title": "Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in Deep Learning Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Michelle R. Greene",
        "Mariam Josyula",
        "Wentao Si",
        "Jennifer A. Hart"
      ],
      "abstract": "Computer-based scene understanding has influenced fields ranging from urban\nplanning to autonomous vehicle performance, yet little is known about how well\nthese technologies work across social differences. We investigate the biases of\ndeep convolutional neural networks (dCNNs) in scene classification, using\nnearly one million images from global and US sources, including user-submitted\nhome photographs and Airbnb listings. We applied statistical models to quantify\nthe impact of socioeconomic indicators such as family income, Human Development\nIndex (HDI), and demographic factors from public data sources (CIA and US\nCensus) on dCNN performance. Our analyses revealed significant socioeconomic\nbias, where pretrained dCNNs demonstrated lower classification accuracy, lower\nclassification confidence, and a higher tendency to assign labels that could be\noffensive when applied to homes (e.g., \"ruin\", \"slum\"), especially in images\nfrom homes with lower socioeconomic status (SES). This trend is consistent\nacross two datasets of international images and within the diverse economic and\nracial landscapes of the United States. This research contributes to\nunderstanding biases in computer vision, emphasizing the need for more\ninclusive and representative training datasets. By mitigating the bias in the\ncomputer vision pipelines, we can ensure fairer and more equitable outcomes for\napplied computer vision, including home valuation and smart home security\nsystems. There is urgency in addressing these biases, which can significantly\nimpact critical decisions in urban development and resource allocation. Our\nfindings also motivate the development of AI systems that better understand and\nserve diverse communities, moving towards technology that equitably benefits\nall sectors of society.",
      "tldr_zh": "本研究调查了深层卷积神经网络(dCNNs)在场景分类中的社会经济偏见，使用近百万张全球和美国图像（如用户家庭照片和Airbnb列表），并通过统计模型量化指标如家庭收入、人类发展指数(HDI)和人口统计数据的影响。结果显示，dCNNs在低社会经济地位(SES)图像上表现出显著偏见，包括准确率降低、置信度下降，以及更倾向于分配负面标签（如“ruin”, “slum”），这一趋势在国际和美国数据集上均一致。论文强调了计算机视觉系统中存在的偏见，呼吁采用更具包容性和代表性的训练数据集，以确保公平应用，如房屋估值和智能家居安全系统，并推动AI技术更公平地服务多样化社区。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68-02",
        "I.2.m"
      ],
      "primary_category": "cs.CV",
      "comment": "28 pages, 3 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2401.13097v2",
      "published_date": "2024-01-23 21:22:06 UTC",
      "updated_date": "2025-03-05 21:31:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:40:12.484075"
    },
    {
      "arxiv_id": "2401.13086v1",
      "title": "Towards Trustable Language Models: Investigating Information Quality of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Rick Rejeleene",
        "Xiaowei Xu",
        "John Talburt"
      ],
      "abstract": "Large language models (LLM) are generating information at a rapid pace,\nrequiring users to increasingly rely and trust the data. Despite remarkable\nadvances of LLM, Information generated by LLM is not completely trustworthy,\ndue to challenges in information quality. Specifically, integrity of\nInformation quality decreases due to unreliable, biased, tokenization during\npre-training of LLM. Moreover, due to decreased information quality issues, has\nled towards hallucination, fabricated information. Unreliable information can\nlead towards flawed decisions in businesses, which impacts economic activity.\nIn this work, we introduce novel mathematical information quality evaluation of\nLLM, we furthermore analyze and highlight information quality challenges,\nscaling laws to systematically scale language models.",
      "tldr_zh": "该研究探讨了大语言模型(LLM)的信息质量问题，指出LLM生成的信息可能因预训练中的不可靠、偏见和标记化等问题而降低完整性，从而导致hallucination和虚构信息，进而影响商业决策和经济活动。论文引入了一种新型数学信息质量评估方法，并对这些挑战进行系统分析。最终，该工作还探讨了scaling laws，以指导语言模型的扩展和改进，提高其可信度。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "31 pages",
      "pdf_url": "http://arxiv.org/pdf/2401.13086v1",
      "published_date": "2024-01-23 20:55:49 UTC",
      "updated_date": "2024-01-23 20:55:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:40:23.621274"
    },
    {
      "arxiv_id": "2401.13085v1",
      "title": "IndiText Boost: Text Augmentation for Low Resource India Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Onkar Litake",
        "Niraj Yagnik",
        "Shreyas Labhsetwar"
      ],
      "abstract": "Text Augmentation is an important task for low-resource languages. It helps\ndeal with the problem of data scarcity. A data augmentation strategy is used to\ndeal with the problem of data scarcity. Through the years, much work has been\ndone on data augmentation for the English language. In contrast, very less work\nhas been done on Indian languages. This is contrary to the fact that data\naugmentation is used to deal with data scarcity. In this work, we focus on\nimplementing techniques like Easy Data Augmentation, Back Translation,\nParaphrasing, Text Generation using LLMs, and Text Expansion using LLMs for\ntext classification on different languages. We focus on 6 Indian languages\nnamely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to\nour knowledge, no such work exists for text augmentation on Indian languages.\nWe carry out binary as well as multi-class text classification to make our\nresults more comparable. We get surprising results as basic data augmentation\ntechniques surpass LLMs.",
      "tldr_zh": "该研究提出IndiText Boost框架，针对低资源印度语言（如Sindhi、Marathi、Hindi、Gujarati、Telugu和Sanskrit）进行文本增强，以解决数据稀缺问题，这是该领域的首次尝试。方法包括Easy Data Augmentation、Back Translation、Paraphrasing、Text Generation using LLMs和Text Expansion using LLMs，并应用于二元和多类文本分类任务。实验结果显示，基本数据增强技术在性能上超过了LLMs，提供了印度语言文本增强的有效策略。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13085v1",
      "published_date": "2024-01-23 20:54:40 UTC",
      "updated_date": "2024-01-23 20:54:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:40:35.885417"
    },
    {
      "arxiv_id": "2402.01691v1",
      "title": "Investigating Algorithm Review Boards for Organizational Responsible Artificial Intelligence Governance",
      "title_zh": "翻译失败",
      "authors": [
        "Emily Hadley",
        "Alan Blatecky",
        "Megan Comfort"
      ],
      "abstract": "Organizations including companies, nonprofits, governments, and academic\ninstitutions are increasingly developing, deploying, and utilizing artificial\nintelligence (AI) tools. Responsible AI (RAI) governance approaches at\norganizations have emerged as important mechanisms to address potential AI\nrisks and harms. In this work, we interviewed 17 technical contributors across\norganization types (Academic, Government, Industry, Nonprofit) and sectors\n(Finance, Health, Tech, Other) about their experiences with internal RAI\ngovernance. Our findings illuminated the variety of organizational definitions\nof RAI and accompanying internal governance approaches. We summarized the first\ndetailed findings on algorithm review boards (ARBs) and similar review\ncommittees in practice, including their membership, scope, and measures of\nsuccess. We confirmed known robust model governance in finance sectors and\nrevealed extensive algorithm and AI governance with ARB-like review boards in\nhealth sectors. Our findings contradict the idea that Institutional Review\nBoards alone are sufficient for algorithm governance and posit that ARBs are\namong the more impactful internal RAI governance approaches. Our results\nsuggest that integration with existing internal regulatory approaches and\nleadership buy-in are among the most important attributes for success and that\nfinancial tensions are the greatest challenge to effective organizational RAI.\nWe make a variety of suggestions for how organizational partners can learn from\nthese findings when building their own internal RAI frameworks. We outline\nfuture directions for developing and measuring effectiveness of ARBs and other\ninternal RAI governance approaches.",
      "tldr_zh": "这篇论文通过采访17名来自学术、政府、行业和非营利组织的技术贡献者，调查了算法审查委员会(ARBs)作为组织Responsible AI (RAI)治理机制的实践和效果。研究发现，不同组织对RAI的定义和治理方法多样化，ARBs在金融和健康领域广泛应用，且比Institutional Review Boards (IRBs)更具影响力，能更好地处理AI风险。关键成功因素包括与现有内部监管整合和领导支持，而财务紧张是主要挑战。作者提供了构建RAI框架的建议，并提出了未来评估ARBs有效性的研究方向。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.01691v1",
      "published_date": "2024-01-23 20:53:53 UTC",
      "updated_date": "2024-01-23 20:53:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:40:48.925726"
    },
    {
      "arxiv_id": "2401.13081v1",
      "title": "Free Form Medical Visual Question Answering in Radiology",
      "title_zh": "放射学中的自由形式医疗视觉问答",
      "authors": [
        "Abhishek Narayanan",
        "Rushabh Musthyala",
        "Rahul Sankar",
        "Anirudh Prasad Nistala",
        "Pranav Singh",
        "Jacopo Cirrone"
      ],
      "abstract": "Visual Question Answering (VQA) in the medical domain presents a unique,\ninterdisciplinary challenge, combining fields such as Computer Vision, Natural\nLanguage Processing, and Knowledge Representation. Despite its importance,\nresearch in medical VQA has been scant, only gaining momentum since 2018.\nAddressing this gap, our research delves into the effective representation of\nradiology images and the joint learning of multimodal representations,\nsurpassing existing methods. We innovatively augment the SLAKE dataset,\nenabling our model to respond to a more diverse array of questions, not limited\nto the immediate content of radiology or pathology images. Our model achieves a\ntop-1 accuracy of 79.55\\% with a less complex architecture, demonstrating\ncomparable performance to current state-of-the-art models. This research not\nonly advances medical VQA but also opens avenues for practical applications in\ndiagnostic settings.",
      "tldr_zh": "本研究探讨了放射学领域的自由形式医疗 Visual Question Answering (VQA)，旨在解决多模态表示和数据集限制的问题，通过有效表示放射学图像和联合学习多模态表示来超越现有方法。研究者创新性地增强了 SLAKE 数据集，使模型能够处理更广泛的问题类型，而非仅限于图像内容。结果显示，该模型在较简单架构下实现了 79.55% 的 top-1 准确率，与当前最先进模型性能相当，并为诊断领域的实际应用打开了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages and 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.13081v1",
      "published_date": "2024-01-23 20:26:52 UTC",
      "updated_date": "2024-01-23 20:26:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:41:00.564100"
    },
    {
      "arxiv_id": "2401.13068v1",
      "title": "Local Background Estimation for Improved Gas Plume Identification in Hyperspectral Images",
      "title_zh": "翻译失败",
      "authors": [
        "Scout Jarman",
        "Zigfried Hampel-Arias",
        "Adra Carr",
        "Kevin R. Moon"
      ],
      "abstract": "Deep learning identification models have shown promise for identifying gas\nplumes in Longwave IR hyperspectral images of urban scenes, particularly when a\nlarge library of gases are being considered. Because many gases have similar\nspectral signatures, it is important to properly estimate the signal from a\ndetected plume. Typically, a scene's global mean spectrum and covariance matrix\nare estimated to whiten the plume's signal, which removes the background's\nsignature from the gas signature. However, urban scenes can have many different\nbackground materials that are spatially and spectrally heterogeneous. This can\nlead to poor identification performance when the global background estimate is\nnot representative of a given local background material. We use image\nsegmentation, along with an iterative background estimation algorithm, to\ncreate local estimates for the various background materials that reside\nunderneath a gas plume. Our method outperforms global background estimation on\na set of simulated and real gas plumes. This method shows promise in increasing\ndeep learning identification confidence, while being simple and easy to tune\nwhen considering diverse plumes.",
      "tldr_zh": "这篇论文针对长波红外超光谱图像中气体羽流(gas plumes)识别的问题，提出了一种局部背景估计方法，以解决城市场景中背景材料的空间和光谱异质性导致的全局估计不准确问题。方法结合图像分割(image segmentation)和迭代背景估计算法，对羽流下的不同局部背景进行精确估计，从而提升了羽流信号的白化效果。实验结果表明，该方法在模拟和真实气体羽流数据集上优于传统全局背景估计，提高了深层学习识别的置信度，且操作简单易于调优。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to International Geoscience and Remote Sensing Symposium\n  (IGARSS), 2024. 5 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.13068v1",
      "published_date": "2024-01-23 19:48:34 UTC",
      "updated_date": "2024-01-23 19:48:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:41:12.737703"
    },
    {
      "arxiv_id": "2401.13060v1",
      "title": "TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur'anic QA",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammed Alaa Elkomy",
        "Amany Sarhan"
      ],
      "abstract": "In this paper, we present our approach to tackle Qur'an QA 2023 shared tasks\nA and B. To address the challenge of low-resourced training data, we rely on\ntransfer learning together with a voting ensemble to improve prediction\nstability across multiple runs. Additionally, we employ different architectures\nand learning mechanisms for a range of Arabic pre-trained transformer-based\nmodels for both tasks. To identify unanswerable questions, we propose using a\nthresholding mechanism. Our top-performing systems greatly surpass the baseline\nperformance on the hidden split, achieving a MAP score of 25.05% for task A and\na partial Average Precision (pAP) of 57.11% for task B.",
      "tldr_zh": "这篇论文介绍了 TCE 方法，用于处理 Qur'an QA 2023 共享任务 A 和 B，以应对低资源训练数据的挑战。他们采用 transfer learning 和 voting ensemble 相结合的策略，利用多种阿拉伯语预训练 Transformer-based 模型，并引入 thresholding mechanism 来识别无法回答的问题，从而提升预测稳定性。实验结果显示，该方法在隐藏分割上大幅超过了基线性能，任务 A 的 MAP score 达到 25.05%，任务 B 的 pAP 达到 57.11%。这为低资源问答任务提供了有效的增强框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13060v1",
      "published_date": "2024-01-23 19:32:54 UTC",
      "updated_date": "2024-01-23 19:32:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:41:24.569757"
    },
    {
      "arxiv_id": "2401.13049v1",
      "title": "CIS-UNet: Multi-Class Segmentation of the Aorta in Computed Tomography Angiography via Context-Aware Shifted Window Self-Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Imran",
        "Jonathan R Krebs",
        "Veera Rajasekhar Reddy Gopu",
        "Brian Fazzone",
        "Vishal Balaji Sivaraman",
        "Amarjeet Kumar",
        "Chelsea Viscardi",
        "Robert Evans Heithaus",
        "Benjamin Shickel",
        "Yuyin Zhou",
        "Michol A Cooper",
        "Wei Shao"
      ],
      "abstract": "Advancements in medical imaging and endovascular grafting have facilitated\nminimally invasive treatments for aortic diseases. Accurate 3D segmentation of\nthe aorta and its branches is crucial for interventions, as inaccurate\nsegmentation can lead to erroneous surgical planning and endograft\nconstruction. Previous methods simplified aortic segmentation as a binary image\nsegmentation problem, overlooking the necessity of distinguishing between\nindividual aortic branches. In this paper, we introduce Context Infused\nSwin-UNet (CIS-UNet), a deep learning model designed for multi-class\nsegmentation of the aorta and thirteen aortic branches. Combining the strengths\nof Convolutional Neural Networks (CNNs) and Swin transformers, CIS-UNet adopts\na hierarchical encoder-decoder structure comprising a CNN encoder, symmetric\ndecoder, skip connections, and a novel Context-aware Shifted Window\nSelf-Attention (CSW-SA) as the bottleneck block. Notably, CSW-SA introduces a\nunique utilization of the patch merging layer, distinct from conventional Swin\ntransformers. It efficiently condenses the feature map, providing a global\nspatial context and enhancing performance when applied at the bottleneck layer,\noffering superior computational efficiency and segmentation accuracy compared\nto the Swin transformers. We trained our model on computed tomography (CT)\nscans from 44 patients and tested it on 15 patients. CIS-UNet outperformed the\nstate-of-the-art SwinUNetR segmentation model, which is solely based on Swin\ntransformers, by achieving a superior mean Dice coefficient of 0.713 compared\nto 0.697, and a mean surface distance of 2.78 mm compared to 3.39 mm.\nCIS-UNet's superior 3D aortic segmentation offers improved precision and\noptimization for planning endovascular treatments. Our dataset and code will be\npublicly available.",
      "tldr_zh": "本文提出 CIS-UNet，一种用于 CT 血管成像中主动脉及其13个分支的多类分割模型，以解决传统二元分割忽略分支区分的问题。模型结合 CNN 和改进的 Swin transformers，通过新型 Context-aware Shifted Window Self-Attention (CSW-SA) 作为瓶颈块，提供全局空间上下文并提升计算效率和分割准确性。在实验中，CIS-UNet 在44个患者训练集上训练，并在15个患者测试集上表现优于 SwinUNetR 模型，平均 Dice coefficient 达0.713（对比0.697），平均表面距离为2.78 mm（对比3.39 mm），从而为内腔治疗规划提供更精确的3D分割支持。数据集和代码将公开可用。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13049v1",
      "published_date": "2024-01-23 19:17:20 UTC",
      "updated_date": "2024-01-23 19:17:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:41:39.309114"
    },
    {
      "arxiv_id": "2401.13034v4",
      "title": "Locality Sensitive Sparse Encoding for Learning World Models Online",
      "title_zh": "局部敏感稀疏编码用于在线学习世界模型",
      "authors": [
        "Zichen Liu",
        "Chao Du",
        "Wee Sun Lee",
        "Min Lin"
      ],
      "abstract": "Acquiring an accurate world model online for model-based reinforcement\nlearning (MBRL) is challenging due to data nonstationarity, which typically\ncauses catastrophic forgetting for neural networks (NNs). From the online\nlearning perspective, a Follow-The-Leader (FTL) world model is desirable, which\noptimally fits all previous experiences at each round. Unfortunately, NN-based\nmodels need re-training on all accumulated data at every interaction step to\nachieve FTL, which is computationally expensive for lifelong agents. In this\npaper, we revisit models that can achieve FTL with incremental updates.\nSpecifically, our world model is a linear regression model supported by\nnonlinear random features. The linear part ensures efficient FTL update while\nthe nonlinear random feature empowers the fitting of complex environments. To\nbest trade off model capacity and computation efficiency, we introduce a\nlocality sensitive sparse encoding, which allows us to conduct efficient sparse\nupdates even with very high dimensional nonlinear features. We validate the\nrepresentation power of our encoding and verify that it allows efficient online\nlearning under data covariate shift. We also show, in the Dyna MBRL setting,\nthat our world models learned online using a single pass of trajectory data\neither surpass or match the performance of deep world models trained with\nreplay and other continual learning methods.",
      "tldr_zh": "本研究针对基于模型的强化学习(MBRL)中在线学习世界模型的挑战，解决了数据非平稳性导致神经网络(NNs)发生灾难性遗忘的问题。作者提出了一种线性回归模型，结合非线性随机特征和locality sensitive sparse encoding，实现高效的Follow-The-Leader (FTL)更新，同时平衡模型容量和计算效率。该方法允许在高维特征下进行稀疏更新，并在数据协变量偏移场景下验证了其表示能力和在线学习性能。在Dyna MBRL设置中，使用单次轨迹数据学习的世界模型性能超越或匹配深度世界模型，这些模型依赖重放或其他持续学习方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.13034v4",
      "published_date": "2024-01-23 19:00:02 UTC",
      "updated_date": "2024-04-17 07:54:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:41:48.255884"
    },
    {
      "arxiv_id": "2401.12975v1",
      "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments",
      "title_zh": "HAZARD Challenge：动态变化环境中的具身决策",
      "authors": [
        "Qinhong Zhou",
        "Sunli Chen",
        "Yisong Wang",
        "Haozhe Xu",
        "Weihua Du",
        "Hongxin Zhang",
        "Yilun Du",
        "Joshua B. Tenenbaum",
        "Chuang Gan"
      ],
      "abstract": "Recent advances in high-fidelity virtual environments serve as one of the\nmajor driving forces for building intelligent embodied agents to perceive,\nreason and interact with the physical world. Typically, these environments\nremain unchanged unless agents interact with them. However, in real-world\nscenarios, agents might also face dynamically changing environments\ncharacterized by unexpected events and need to rapidly take action accordingly.\nTo remedy this gap, we propose a new simulated embodied benchmark, called\nHAZARD, specifically designed to assess the decision-making abilities of\nembodied agents in dynamic situations. HAZARD consists of three unexpected\ndisaster scenarios, including fire, flood, and wind, and specifically supports\nthe utilization of large language models (LLMs) to assist common sense\nreasoning and decision-making. This benchmark enables us to evaluate autonomous\nagents' decision-making capabilities across various pipelines, including\nreinforcement learning (RL), rule-based, and search-based methods in\ndynamically changing environments. As a first step toward addressing this\nchallenge using large language models, we further develop an LLM-based agent\nand perform an in-depth analysis of its promise and challenge of solving these\nchallenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.",
      "tldr_zh": "该论文提出 HAZARD 基准，这是一个新的模拟环境，用于评估智能体在动态变化环境中的决策能力，填补了传统静态环境的局限性。HAZARD 包括 fire、flood 和 wind 等意外灾害场景，支持 large language models (LLMs) 辅助常识推理和决策，并兼容 reinforcement learning (RL)、rule-based 和 search-based 方法。实验中，研究者开发了基于 LLM 的代理，并对其在这些挑战任务中的优势和局限性进行了深入分析。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2024. The first two authors contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2401.12975v1",
      "published_date": "2024-01-23 18:59:43 UTC",
      "updated_date": "2024-01-23 18:59:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:41:59.959866"
    },
    {
      "arxiv_id": "2401.12972v3",
      "title": "On the Efficacy of Text-Based Input Modalities for Action Anticipation",
      "title_zh": "翻译失败",
      "authors": [
        "Apoorva Beedu",
        "Harish Haresamudram",
        "Karan Samel",
        "Irfan Essa"
      ],
      "abstract": "Anticipating future actions is a highly challenging task due to the diversity\nand scale of potential future actions; yet, information from different\nmodalities help narrow down plausible action choices. Each modality can provide\ndiverse and often complementary context for the model to learn from. While\nprevious multi-modal methods leverage information from modalities such as video\nand audio, we primarily explore how text descriptions of actions and objects\ncan also lead to more accurate action anticipation by providing additional\ncontextual cues, e.g., about the environment and its contents. We propose a\nMulti-modal Contrastive Anticipative Transformer (M-CAT), a video transformer\narchitecture that jointly learns from multi-modal features and text\ndescriptions of actions and objects. We train our model in two stages, where\nthe model first learns to align video clips with descriptions of future\nactions, and is subsequently fine-tuned to predict future actions. Compared to\nexisting methods, M-CAT has the advantage of learning additional context from\ntwo types of text inputs: rich descriptions of future actions during\npre-training, and, text descriptions for detected objects and actions during\nmodality feature fusion. Through extensive experimental evaluation, we\ndemonstrate that our model outperforms previous methods on the EpicKitchens\ndatasets, and show that using simple text descriptions of actions and objects\naid in more effective action anticipation. In addition, we examine the impact\nof object and action information obtained via text, and perform extensive\nablations.",
      "tldr_zh": "本研究探讨了基于文本输入模式在行动预测（action anticipation）中的功效，强调文本描述（如行动和对象）能提供额外上下文，提升预测准确性。论文提出Multi-modal Contrastive Anticipative Transformer (M-CAT)模型，该模型采用视频变换器架构，通过两阶段训练（先对齐视频与文本描述，再微调预测未来行动）来联合学习多模态特征和文本输入。在EpicKitchens数据集上的实验显示，M-CAT优于现有方法，证明简单文本描述能显著改善行动预测效果，并通过消融实验验证了对象和行动信息的贡献。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12972v3",
      "published_date": "2024-01-23 18:58:35 UTC",
      "updated_date": "2024-08-29 15:11:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:42:13.778595"
    },
    {
      "arxiv_id": "2401.12963v2",
      "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents",
      "title_zh": "AutoRT：用于大规模机器人代理编排的具身化基础模型",
      "authors": [
        "Michael Ahn",
        "Debidatta Dwibedi",
        "Chelsea Finn",
        "Montse Gonzalez Arenas",
        "Keerthana Gopalakrishnan",
        "Karol Hausman",
        "Brian Ichter",
        "Alex Irpan",
        "Nikhil Joshi",
        "Ryan Julian",
        "Sean Kirmani",
        "Isabel Leal",
        "Edward Lee",
        "Sergey Levine",
        "Yao Lu",
        "Isabel Leal",
        "Sharath Maddineni",
        "Kanishka Rao",
        "Dorsa Sadigh",
        "Pannag Sanketi",
        "Pierre Sermanet",
        "Quan Vuong",
        "Stefan Welker",
        "Fei Xia",
        "Ted Xiao",
        "Peng Xu",
        "Steve Xu",
        "Zhuo Xu"
      ],
      "abstract": "Foundation models that incorporate language, vision, and more recently\nactions have revolutionized the ability to harness internet scale data to\nreason about useful tasks. However, one of the key challenges of training\nembodied foundation models is the lack of data grounded in the physical world.\nIn this paper, we propose AutoRT, a system that leverages existing foundation\nmodels to scale up the deployment of operational robots in completely unseen\nscenarios with minimal human supervision. AutoRT leverages vision-language\nmodels (VLMs) for scene understanding and grounding, and further uses large\nlanguage models (LLMs) for proposing diverse and novel instructions to be\nperformed by a fleet of robots. Guiding data collection by tapping into the\nknowledge of foundation models enables AutoRT to effectively reason about\nautonomy tradeoffs and safety while significantly scaling up data collection\nfor robot learning. We demonstrate AutoRT proposing instructions to over 20\nrobots across multiple buildings and collecting 77k real robot episodes via\nboth teleoperation and autonomous robot policies. We experimentally show that\nsuch \"in-the-wild\" data collected by AutoRT is significantly more diverse, and\nthat AutoRT's use of LLMs allows for instruction following data collection\nrobots that can align to human preferences.",
      "tldr_zh": "这篇论文提出了 AutoRT 系统，一种基于 Embodied Foundation Models 的框架，用于大规模协调机器人代理，以最小的人类监督在全新场景中部署操作机器人。AutoRT 利用视觉语言模型 (VLMs) 进行场景理解和定位，并借助大型语言模型 (LLMs) 生成多样化的指令，指导机器人收集数据，同时权衡自治权、安全性和数据多样性。实验结果显示，AutoRT 为超过 20 个机器人提出指令，成功收集了 77k 真实机器人事件，这些“野外”数据显著更具多样性，并提升了机器人在遵循人类偏好方面的性能。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "26 pages, 9 figures, ICRA 2024 VLMNM Workshop",
      "pdf_url": "http://arxiv.org/pdf/2401.12963v2",
      "published_date": "2024-01-23 18:45:54 UTC",
      "updated_date": "2024-07-02 01:52:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:42:25.674053"
    },
    {
      "arxiv_id": "2401.12954v1",
      "title": "Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding",
      "title_zh": "翻译失败",
      "authors": [
        "Mirac Suzgun",
        "Adam Tauman Kalai"
      ],
      "abstract": "We introduce meta-prompting, an effective scaffolding technique designed to\nenhance the functionality of language models (LMs). This approach transforms a\nsingle LM into a multi-faceted conductor, adept at managing and integrating\nmultiple independent LM queries. By employing high-level instructions,\nmeta-prompting guides the LM to break down complex tasks into smaller, more\nmanageable subtasks. These subtasks are then handled by distinct \"expert\"\ninstances of the same LM, each operating under specific, tailored instructions.\nCentral to this process is the LM itself, in its role as the conductor, which\nensures seamless communication and effective integration of the outputs from\nthese expert models. It additionally employs its inherent critical thinking and\nrobust verification processes to refine and authenticate the end result. This\ncollaborative prompting approach empowers a single LM to simultaneously act as\na comprehensive orchestrator and a panel of diverse experts, significantly\nenhancing its performance across a wide array of tasks. The zero-shot,\ntask-agnostic nature of meta-prompting greatly simplifies user interaction by\nobviating the need for detailed, task-specific instructions. Furthermore, our\nresearch demonstrates the seamless integration of external tools, such as a\nPython interpreter, into the meta-prompting framework, thereby broadening its\napplicability and utility. Through rigorous experimentation with GPT-4, we\nestablish the superiority of meta-prompting over conventional scaffolding\nmethods: When averaged across all tasks, including the Game of 24,\nCheckmate-in-One, and Python Programming Puzzles, meta-prompting, augmented\nwith a Python interpreter functionality, surpasses standard prompting by 17.1%,\nexpert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.",
      "tldr_zh": "本研究引入了 meta-prompting，一种任务无关的支架技术，用于增强语言模型 (LMs) 的功能，使其充当“导体”来管理多个独立 LM 查询。方法涉及使用高水平指令将复杂任务分解为子任务，由特定指令指导的“专家”实例处理，LM 随后整合输出并通过批判性思考进行验证和优化。这种零样本方法简化了用户交互，并支持外部工具如 Python 解释器整合；实验结果显示，使用 GPT-4，meta-prompting 在 Game of 24、Checkmate-in-One 和 Python Programming Puzzles 等任务上，比标准提示提升 17.1%、比专家提示提升 17.3%、比多角色提示提升 15.2%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "https://github.com/suzgunmirac/meta-prompting",
      "pdf_url": "http://arxiv.org/pdf/2401.12954v1",
      "published_date": "2024-01-23 18:22:19 UTC",
      "updated_date": "2024-01-23 18:22:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:42:40.122857"
    },
    {
      "arxiv_id": "2401.12947v1",
      "title": "Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion",
      "title_zh": "基于 Transformer 的模型尚未完美地学习模拟结构递归",
      "authors": [
        "Dylan Zhang",
        "Curt Tigges",
        "Zory Zhang",
        "Stella Biderman",
        "Maxim Raginsky",
        "Talia Ringer"
      ],
      "abstract": "This paper investigates the ability of transformer-based models to learn\nstructural recursion from examples. Recursion is a universal concept in both\nnatural and formal languages. Structural recursion is central to the\nprogramming language and formal mathematics tasks where symbolic tools\ncurrently excel beyond neural models, such as inferring semantic relations\nbetween datatypes and emulating program behavior. We introduce a general\nframework that nicely connects the abstract concepts of structural recursion in\nthe programming language domain to concrete sequence modeling problems and\nlearned models' behavior. The framework includes a representation that captures\nthe general \\textit{syntax} of structural recursion, coupled with two different\nframeworks for understanding their \\textit{semantics} -- one that is more\nnatural from a programming languages perspective and one that helps bridge that\nperspective with a mechanistic understanding of the underlying transformer\narchitecture.\n  With our framework as a powerful conceptual tool, we identify different\nissues under various set-ups. The models trained to emulate recursive\ncomputations cannot fully capture the recursion yet instead fit short-cut\nalgorithms and thus cannot solve certain edge cases that are under-represented\nin the training distribution. In addition, it is difficult for state-of-the-art\nlarge language models (LLMs) to mine recursive rules from in-context\ndemonstrations. Meanwhile, these LLMs fail in interesting ways when emulating\nreduction (step-wise computation) of the recursive function.",
      "tldr_zh": "这篇论文探讨了Transformer-based models从示例中学习structural recursion的能力，强调structural recursion在编程语言和形式数学中的核心作用。研究者引入了一个通用框架，将structural recursion的语法表示与序列建模问题相连，并提供了两种语义理解方式，以桥接编程语言视角和Transformer架构的机制分析。实验结果显示，这些模型倾向于采用shortcut算法而非真正捕捉递归，导致在训练分布中欠代表性的边缘案例中失败；此外，state-of-the-art LLMs在从in-context demonstrations中挖掘递归规则和模拟递归函数的reduction时表现出显著局限性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.FL",
        "cs.LO",
        "cs.PL"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2305.14699",
      "pdf_url": "http://arxiv.org/pdf/2401.12947v1",
      "published_date": "2024-01-23 18:07:38 UTC",
      "updated_date": "2024-01-23 18:07:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:42:52.131703"
    },
    {
      "arxiv_id": "2401.12920v3",
      "title": "Truck Parking Usage Prediction with Decomposed Graph Neural Networks",
      "title_zh": "基于分解图神经网络的卡车停车使用预测",
      "authors": [
        "Rei Tamaru",
        "Yang Cheng",
        "Steven Parker",
        "Ernie Perry",
        "Bin Ran",
        "Soyoung Ahn"
      ],
      "abstract": "Truck parking on freight corridors faces the major challenge of insufficient\nparking spaces. This is exacerbated by the Hour-of-Service (HOS) regulations,\nwhich often result in unauthorized parking practices, causing safety concerns.\nIt has been shown that providing accurate parking usage prediction can be a\ncost-effective solution to reduce unsafe parking practices. In light of this,\nexisting studies have developed various methods to predict the usage of a truck\nparking site and have demonstrated satisfactory accuracy. However, these\nstudies focused on a single parking site, and few approaches have been proposed\nto predict the usage of multiple truck parking sites considering\nspatio-temporal dependencies, due to the lack of data. This paper aims to fill\nthis gap and presents the Regional Temporal Graph Convolutional Network\n(RegT-GCN) to predict parking usage across the entire state to provide more\ncomprehensive truck parking information. The framework leverages the\ntopological structures of truck parking site locations and historical parking\ndata to predict the occupancy rate considering spatio-temporal dependencies\nacross a state. To achieve this, we introduce a Regional Decomposition\napproach, which effectively captures the geographical characteristics of the\ntruck parking locations and their spatial correlations. Evaluation results\ndemonstrate that the proposed model outperforms other baseline models, showing\nthe effectiveness of our regional decomposition. The code is available at\nhttps://github.com/raynbowy23/RegT-GCN.",
      "tldr_zh": "这篇论文针对卡车停车位不足和 Hour-of-Service (HOS) 法规导致的安全问题，提出了一种预测方法来减少非法停车。作者开发了 Regional Temporal Graph Convolutional Network (RegT-GCN) 模型，利用 Graph Neural Networks 和 Regional Decomposition 技术，捕捉多个停车场的时空依赖性和地理特征，从而预测整个州的停车占用率。实验结果表明，该模型在性能上优于基线模型，为提供更全面的卡车停车信息提供了有效解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12920v3",
      "published_date": "2024-01-23 17:14:01 UTC",
      "updated_date": "2025-03-25 21:24:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:43:01.595616"
    },
    {
      "arxiv_id": "2401.12917v1",
      "title": "Active Inference as a Model of Agency",
      "title_zh": "Active Inference 作为代理模型",
      "authors": [
        "Lancelot Da Costa",
        "Samuel Tenka",
        "Dominic Zhao",
        "Noor Sajid"
      ],
      "abstract": "Is there a canonical way to think of agency beyond reward maximisation? In\nthis paper, we show that any type of behaviour complying with physically sound\nassumptions about how macroscopic biological agents interact with the world\ncanonically integrates exploration and exploitation in the sense of minimising\nrisk and ambiguity about states of the world. This description, known as active\ninference, refines the free energy principle, a popular descriptive framework\nfor action and perception originating in neuroscience. Active inference\nprovides a normative Bayesian framework to simulate and model agency that is\nwidely used in behavioural neuroscience, reinforcement learning (RL) and\nrobotics. The usefulness of active inference for RL is three-fold. \\emph{a})\nActive inference provides a principled solution to the exploration-exploitation\ndilemma that usefully simulates biological agency. \\emph{b}) It provides an\nexplainable recipe to simulate behaviour, whence behaviour follows as an\nexplainable mixture of exploration and exploitation under a generative world\nmodel, and all differences in behaviour are explicit in differences in world\nmodel. \\emph{c}) This framework is universal in the sense that it is\ntheoretically possible to rewrite any RL algorithm conforming to the\ndescriptive assumptions of active inference as an active inference algorithm.\nThus, active inference can be used as a tool to uncover and compare the\ncommitments and assumptions of more specific models of agency.",
      "tldr_zh": "本论文提出 active inference 作为一种超越奖励最大化的代理模型框架，认为符合物理假设的生物代理行为会自然整合探索和利用，通过最小化风险和模糊性来优化对世界状态的理解。active inference 是 free energy principle 的细化，提供了一个规范的贝叶斯框架，用于模拟代理行为，并在神经科学、reinforcement learning (RL) 和机器人领域广泛应用。其主要贡献包括：解决 RL 中的探索-利用困境，提供可解释的行为模拟，以及通用性——任何符合描述假设的 RL 算法均可重写为 active inference 算法，从而揭示和比较不同代理模型的假设和承诺。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted in RLDM2022 for the workshop 'RL as a model of agency'",
      "pdf_url": "http://arxiv.org/pdf/2401.12917v1",
      "published_date": "2024-01-23 17:09:25 UTC",
      "updated_date": "2024-01-23 17:09:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:43:15.282808"
    },
    {
      "arxiv_id": "2401.12915v1",
      "title": "Red Teaming Visual Language Models",
      "title_zh": "红队测试视觉语言模型",
      "authors": [
        "Mukai Li",
        "Lei Li",
        "Yuwei Yin",
        "Masood Ahmed",
        "Zhenguang Liu",
        "Qi Liu"
      ],
      "abstract": "VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language\nModels) to accept multimodal inputs. Since it has been verified that LLMs can\nbe induced to generate harmful or inaccurate content through specific test\ncases (termed as Red Teaming), how VLMs perform in similar scenarios,\nespecially with their combination of textual and visual inputs, remains a\nquestion. To explore this problem, we present a novel red teaming dataset\nRTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal\njail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,\nprivacy, safety, fairness). Our RTVLM is the first red-teaming dataset to\nbenchmark current VLMs in terms of these 4 different aspects. Detailed analysis\nshows that 10 prominent open-sourced VLMs struggle with the red teaming in\ndifferent degrees and have up to 31% performance gap with GPT-4V. Additionally,\nwe simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning\n(SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM\ntest set, 13% in MM-Hal, and without noticeable decline in MM-Bench,\noverpassing other LLaVA-based models with regular alignment data. This reveals\nthat current open-sourced VLMs still lack red teaming alignment. Our code and\ndatasets will be open-source.",
      "tldr_zh": "本文提出一个名为 RTVLM 的新红队数据集，用于评估视觉语言模型 (VLMs) 在结合文本和视觉输入时的潜在风险，包括忠实度 (faithfulness)、隐私 (privacy)、安全 (safety) 和公平 (fairness) 等 4 个方面，共涵盖 10 个子任务。实验结果显示，10 个知名开源 VLMs 在这些测试中表现不佳，与 GPT-4V 存在高达 31% 的性能差距，暴露了 VLMs 的易受攻击性。作者通过 RTVLM 数据集对 LLaVA-v1.5 进行监督微调 (SFT)，使模型在 RTVLM 测试集和 MM-Hal 基准上分别提升 10% 和 13%，而 MM-Bench 性能未明显下降，这表明加强红队对齐可显著改善 VLMs 的鲁棒性，并计划开源代码和数据集。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Working in progress",
      "pdf_url": "http://arxiv.org/pdf/2401.12915v1",
      "published_date": "2024-01-23 17:07:18 UTC",
      "updated_date": "2024-01-23 17:07:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:43:29.398912"
    },
    {
      "arxiv_id": "2401.12914v1",
      "title": "Emergent Communication Protocol Learning for Task Offloading in Industrial Internet of Things",
      "title_zh": "翻译失败",
      "authors": [
        "Salwa Mostafa",
        "Mateus P. Mota",
        "Alvaro Valcarce",
        "Mehdi Bennis"
      ],
      "abstract": "In this paper, we leverage a multi-agent reinforcement learning (MARL)\nframework to jointly learn a computation offloading decision and multichannel\naccess policy with corresponding signaling. Specifically, the base station and\nindustrial Internet of Things mobile devices are reinforcement learning agents\nthat need to cooperate to execute their computation tasks within a deadline\nconstraint. We adopt an emergent communication protocol learning framework to\nsolve this problem. The numerical results illustrate the effectiveness of\nemergent communication in improving the channel access success rate and the\nnumber of successfully computed tasks compared to contention-based,\ncontention-free, and no-communication approaches. Moreover, the proposed task\noffloading policy outperforms remote and local computation baselines.",
      "tldr_zh": "本研究利用多智能体强化学习(MARL)框架，联合学习计算卸载(task offloading)决策和多通道访问策略，包括相应的信令，使基站和工业物联网设备作为代理合作，在截止期限内执行计算任务。\n他们采用紧急通信协议学习(emergent communication protocol learning)框架来解决这一问题。\n实验结果表明，该方法相比基于竞争(contention-based)、免竞争(contention-free)和无通信(no-communication)方法，提高了通道访问成功率和成功计算任务的数量，且提出的任务卸载策略优于远程和本地计算基准。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.MA",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12914v1",
      "published_date": "2024-01-23 17:06:13 UTC",
      "updated_date": "2024-01-23 17:06:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:43:38.808205"
    },
    {
      "arxiv_id": "2401.12874v2",
      "title": "From Understanding to Utilization: A Survey on Explainability for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyan Luo",
        "Lucia Specia"
      ],
      "abstract": "Explainability for Large Language Models (LLMs) is a critical yet challenging\naspect of natural language processing. As LLMs are increasingly integral to\ndiverse applications, their \"black-box\" nature sparks significant concerns\nregarding transparency and ethical use. This survey underscores the imperative\nfor increased explainability in LLMs, delving into both the research on\nexplainability and the various methodologies and tasks that utilize an\nunderstanding of these models. Our focus is primarily on pre-trained\nTransformer-based LLMs, such as LLaMA family, which pose distinctive\ninterpretability challenges due to their scale and complexity. In terms of\nexisting methods, we classify them into local and global analyses, based on\ntheir explanatory objectives. When considering the utilization of\nexplainability, we explore several compelling methods that concentrate on model\nediting, control generation, and model enhancement. Additionally, we examine\nrepresentative evaluation metrics and datasets, elucidating their advantages\nand limitations. Our goal is to reconcile theoretical and empirical\nunderstanding with practical implementation, proposing exciting avenues for\nexplanatory techniques and their applications in the LLMs era.",
      "tldr_zh": "这篇调查论文探讨了Large Language Models (LLMs) 的可解释性问题，强调其黑箱性质带来的透明度和伦理挑战，特别是针对预训练的Transformer-based LLMs，如LLaMA家族。论文将现有解释方法分类为局部和全局分析，并阐述了这些方法在模型编辑、控制生成和模型增强等实际应用中的利用方式。同时，它审查了代表性的评估指标和数据集的优势与局限性，并提出整合理论与实践的未来研究方向，以推动LLMs时代的可解释性发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12874v2",
      "published_date": "2024-01-23 16:09:53 UTC",
      "updated_date": "2024-02-22 04:28:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:43:50.359926"
    },
    {
      "arxiv_id": "2401.12873v3",
      "title": "Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model",
      "title_zh": "使用人类反馈改进机器翻译：将质量估计作为奖励模型的探索",
      "authors": [
        "Zhiwei He",
        "Xing Wang",
        "Wenxiang Jiao",
        "Zhuosheng Zhang",
        "Rui Wang",
        "Shuming Shi",
        "Zhaopeng Tu"
      ],
      "abstract": "Insufficient modeling of human preferences within the reward model is a major\nobstacle for leveraging human feedback to improve translation quality.\nFortunately, quality estimation (QE), which predicts the quality of a given\ntranslation without reference, has achieved impressive alignment with human\nevaluations in the last two years. In this work, we investigate the potential\nof employing the QE model as the reward model to predict human preferences for\nfeedback training. We first identify the overoptimization problem during\nQE-based feedback training, manifested as an increase in reward while\ntranslation quality declines. We examine the problem and argue that the\nvulnerability of the QE model might lead to high rewards for incorrect\ntranslations, resulting in overoptimization and error propagation. To address\nthe problem, we adopt a simple yet effective method that uses heuristic rules\nto detect the incorrect translations and assigns a penalty term to the reward\nscores of them. Experimental results show that the proposed QE-based feedback\ntraining achieves consistent and significant improvements across various\nsettings, further verified through human preference studies. Our subsequent\nanalysis demonstrates the high data efficiency of the proposed QE-based\nfeedback training: it outperforms systems using larger parallel corpora by a\nsmall amount of monolingual data. Our code is available at:\nhttps://github.com/zwhe99/FeedbackMT",
      "tldr_zh": "这篇论文探讨了使用 Quality Estimation (QE) 模型作为奖励模型，以更好地整合人类反馈来提升机器翻译质量。研究者识别出 QE-based 反馈训练中的过优化问题，即奖励分数上升但翻译质量下降，并通过启发式规则检测错误翻译并添加惩罚项来解决这一问题。实验结果显示，该方法在多种设置下实现了持续显著的改进，并通过人类偏好研究验证，同时展示了高数据效率，仅需少量单语数据即可超越使用更大平行语料库的系统。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.12873v3",
      "published_date": "2024-01-23 16:07:43 UTC",
      "updated_date": "2024-03-18 15:16:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:44:02.580405"
    },
    {
      "arxiv_id": "2401.12869v1",
      "title": "TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiruo Wang",
        "Daniel Fried",
        "Graham Neubig"
      ],
      "abstract": "Language models (LMs) can solve tasks such as answering questions about\ntables or images by writing programs. However, using primitive functions often\nleads to verbose and error-prone programs, and higher-level functions require\nexpert design. To enable better solutions without human labor, we ask code LMs\nto curate reusable high-level functions, and use them to write solutions. We\npresent TROVE, a training-free method of inducing a verifiable and efficient\ntoolbox of functions, by generating via using, growing, and periodically\ntrimming the toolbox. On 11 datasets from math, table question answering, and\nimage reasoning tasks, TROVE consistently yields simpler solutions with higher\naccuracy than baselines using CODELLAMA and previous methods using GPT, while\nusing 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more\naccurate human verification than baselines. With the same pipeline, it creates\ndiverse functions for varied tasks and datasets, providing insights into their\nindividual characteristics.",
      "tldr_zh": "该论文提出 TroVE，一种无需训练的方法，用于从代码语言模型(LMs)中自动诱导可验证和高效的函数工具箱，从而简化程序化任务的解决，如回答表格或图像问题。TroVE 通过生成使用工具箱、扩展它并定期修剪来运作，确保工具箱保持高效。实验结果显示，在11个数据集上，TroVE 比基线模型如 CODELLAMA 和 GPT 方法提供更简单的解决方案、更高准确率，并使用79-98%更小的工具箱；此外，它还使人类验证速度提高31%、准确率提升13%，并为不同任务生成多样化函数，提供任务特征的洞见。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12869v1",
      "published_date": "2024-01-23 16:03:17 UTC",
      "updated_date": "2024-01-23 16:03:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:44:18.692875"
    },
    {
      "arxiv_id": "2401.12866v1",
      "title": "Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported Coordination of Mobile Crowdsourcing",
      "title_zh": "翻译失败",
      "authors": [
        "Ralf Bruns",
        "Jeremias Dötterl",
        "Jürgen Dunkel",
        "Sascha Ossowski"
      ],
      "abstract": "Mobile crowdsourcing refers to systems where the completion of tasks\nnecessarily requires physical movement of crowdworkers in an on-demand\nworkforce. Evidence suggests that in such systems, tasks often get assigned to\ncrowdworkers who struggle to complete those tasks successfully, resulting in\nhigh failure rates and low service quality. A promising solution to ensure\nhigher quality of service is to continuously adapt the assignment and respond\nto failure-causing events by transferring tasks to better-suited workers who\nuse different routes or vehicles. However, implementing task transfers in\nmobile crowdsourcing is difficult because workers are autonomous and may reject\ntransfer requests. Moreover, task outcomes are uncertain and need to be\npredicted. In this paper, we propose different mechanisms to achieve outcome\nprediction and task coordination in mobile crowdsourcing. First, we analyze\ndifferent data stream learning approaches for the prediction of task outcomes.\nSecond, based on the suggested prediction model, we propose and evaluate two\ndifferent approaches for task coordination with different degrees of autonomy:\nan opportunistic approach for crowdshipping with collaborative, but\nnon-autonomous workers, and a market-based model with autonomous workers for\ncrowdsensing.",
      "tldr_zh": "该论文评估了在移动众包系统中，使用数据流支持的协作和自治代理来协调任务，以解决任务分配失败率高和低服务质量的问题。研究者首先分析了不同数据流学习（data stream learning）方法，用于预测任务结果的不确定性。基于预测模型，他们提出了两种任务协调方法：一个机会主义方法（opportunistic approach），适用于协作但非自治的工人（collaborative workers）在众包运输（crowdshipping）中的应用；另一个基于市场的模型（market-based model），针对自治工人（autonomous workers）在众包感知（crowdsensing）中的协调。通过这些机制，论文旨在提高任务完成效率和服务质量。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12866v1",
      "published_date": "2024-01-23 16:00:45 UTC",
      "updated_date": "2024-01-23 16:00:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:44:27.075865"
    },
    {
      "arxiv_id": "2401.12863v1",
      "title": "KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Debjyoti Mondal",
        "Suraj Modi",
        "Subhadarshi Panda",
        "Rituraj Singh",
        "Godawari Sudhakar Rao"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in\nnatural language processing tasks by leveraging chain of thought (CoT) that\nenables step-by-step thinking. Extending LLMs with multimodal capabilities is\nthe recent interest, but incurs computational cost and requires substantial\nhardware resources. To address these challenges, we propose KAM-CoT a framework\nthat integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities\nfor a comprehensive understanding of multimodal tasks. KAM-CoT adopts a\ntwo-stage training process with KG grounding to generate effective rationales\nand answers. By incorporating external knowledge from KGs during reasoning, the\nmodel gains a deeper contextual understanding reducing hallucinations and\nenhancing the quality of answers. This knowledge-augmented CoT reasoning\nempowers the model to handle questions requiring external context, providing\nmore informed answers. Experimental findings show KAM-CoT outperforms the\nstate-of-the-art methods. On the ScienceQA dataset, we achieve an average\naccuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by\n10%. Remarkably, KAM-CoT achieves these results with only 280M trainable\nparameters at a time, demonstrating its cost-efficiency and effectiveness.",
      "tldr_zh": "本研究提出 KAM-CoT 框架，将 Chain-of-Thought (CoT) 推理、Knowledge Graphs (KGs) 和多模态数据整合，以提升多模态任务的全面理解和处理能力。框架采用两阶段训练过程，通过 KG grounding 引入外部知识，生成有效的推理路径和答案，从而减少幻觉并增强答案的质量。实验结果显示，KAM-CoT 在 ScienceQA 数据集上达到 93.87% 的准确率，比 GPT-3.5 (75.17%) 高 18% 和 GPT-4 (83.99%) 高 10%，且仅需 280M 可训练参数，展现出高效性和成本优势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "AAAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.12863v1",
      "published_date": "2024-01-23 15:56:11 UTC",
      "updated_date": "2024-01-23 15:56:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:44:40.995987"
    },
    {
      "arxiv_id": "2401.12862v2",
      "title": "FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units",
      "title_zh": "翻译失败",
      "authors": [
        "Shaoheng Fang",
        "Rui Ye",
        "Wenhao Wang",
        "Zuhong Liu",
        "Yuxiao Wang",
        "Yafei Wang",
        "Siheng Chen",
        "Yanfeng Wang"
      ],
      "abstract": "Roadside unit (RSU) can significantly improve the safety and robustness of\nautonomous vehicles through Vehicle-to-Everything (V2X) communication.\nCurrently, the usage of a single RSU mainly focuses on real-time inference and\nV2X collaboration, while neglecting the potential value of the high-quality\ndata collected by RSU sensors. Integrating the vast amounts of data from\nnumerous RSUs can provide a rich source of data for model training. However,\nthe absence of ground truth annotations and the difficulty of transmitting\nenormous volumes of data are two inevitable barriers to fully exploiting this\nhidden value. In this paper, we introduce FedRSU, an innovative federated\nlearning framework for self-supervised scene flow estimation. In FedRSU, we\npresent a recurrent self-supervision training paradigm, where for each RSU, the\nscene flow prediction of points at every timestamp can be supervised by its\nsubsequent future multi-modality observation. Another key component of FedRSU\nis federated learning, where multiple devices collaboratively train an ML model\nwhile keeping the training data local and private. With the power of the\nrecurrent self-supervised learning paradigm, FL is able to leverage innumerable\nunderutilized data from RSU. To verify the FedRSU framework, we construct a\nlarge-scale multi-modality dataset RSU-SF. The dataset consists of 17 RSU\nclients, covering various scenarios, modalities, and sensor settings. Based on\nRSU-SF, we show that FedRSU can greatly improve model performance in ITS and\nprovide a comprehensive benchmark under diverse FL scenarios. To the best of\nour knowledge, we provide the first real-world LiDAR-camera multi-modal dataset\nand benchmark for the FL community.",
      "tldr_zh": "该论文提出 FedRSU，一种创新的联邦学习 (Federated Learning) 框架，用于路边单元 (RSU) 的自监督场景流估计 (Scene Flow Estimation)，旨在利用 RSU 收集的高质量数据提升自动驾驶车辆的安全性，同时解决数据隐私和缺乏地面实况注释的挑战。FedRSU 结合循环自监督训练范式，让每个 RSU 通过后续多模态观察监督场景流预测，并实现多设备协作训练而不传输原始数据。研究构建了大规模数据集 RSU-SF，涵盖 17 个 RSU 客户端的各种场景，并证明 FedRSU 在智能交通系统 (ITS) 中显著提高模型性能，提供首个真实世界 LiDAR-相机多模态基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12862v2",
      "published_date": "2024-01-23 15:52:57 UTC",
      "updated_date": "2024-08-11 04:17:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:44:54.462986"
    },
    {
      "arxiv_id": "2401.12851v1",
      "title": "Classification of grapevine varieties using UAV hyperspectral imaging",
      "title_zh": "使用 UAV 高光谱成像对葡萄藤品种进行分类",
      "authors": [
        "Alfonso López",
        "Carlos Javier Ogayar",
        "Francisco Ramón Feito",
        "Joaquim João Sousa"
      ],
      "abstract": "The classification of different grapevine varieties is a relevant phenotyping\ntask in Precision Viticulture since it enables estimating the growth of\nvineyard rows dedicated to different varieties, among other applications\nconcerning the wine industry. This task can be performed with destructive\nmethods that require time-consuming tasks, including data collection and\nanalysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a\nmore efficient and less prohibitive approach to collecting hyperspectral data,\ndespite acquiring noisier data. Therefore, the first task is the processing of\nthese data to correct and downsample large amounts of data. In addition, the\nhyperspectral signatures of grape varieties are very similar. In this work, a\nConvolutional Neural Network (CNN) is proposed for classifying seventeen\nvarieties of red and white grape variants. Rather than classifying single\nsamples, these are processed together with their neighbourhood. Hence, the\nextraction of spatial and spectral features is addressed with 1) a spatial\nattention layer and 2) Inception blocks. The pipeline goes from processing to\ndataset elaboration, finishing with the training phase. The fitted model is\nevaluated in terms of response time, accuracy and data separability, and\ncompared with other state-of-the-art CNNs for classifying hyperspectral data.\nOur network was proven to be much more lightweight with a reduced number of\ninput bands, a lower number of trainable weights and therefore, reduced\ntraining time. Despite this, the evaluated metrics showed much better results\nfor our network (~99% overall accuracy), in comparison with previous works\nbarely achieving 81% OA.",
      "tldr_zh": "本研究利用 UAV 超光谱成像技术来分类葡萄品种，以支持精准葡萄栽培，提高效率并减少破坏性数据采集。研究提出了一种 CNN 模型，通过整合空间注意力层和 Inception 块，对单个样本及其邻域进行空间和光谱特征提取，从而处理相似的高光谱签名。实验结果显示，该模型在十七种红白葡萄品种的分类上表现出色，整体准确率达到约 99%，并在响应时间、准确性和数据可分性方面优于现有 CNN 基准模型（前者仅81% OA），同时实现了更轻量级的设计和更短的训练时间。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12851v1",
      "published_date": "2024-01-23 15:35:50 UTC",
      "updated_date": "2024-01-23 15:35:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:45:03.268260"
    },
    {
      "arxiv_id": "2401.12850v2",
      "title": "End-to-End Supervised Hierarchical Graph Clustering for Speaker Diarization",
      "title_zh": "翻译失败",
      "authors": [
        "Prachi Singh",
        "Sriram Ganapathy"
      ],
      "abstract": "Speaker diarization, the task of segmenting an audio recording based on\nspeaker identity, constitutes an important speech pre-processing step for\nseveral downstream applications.The conventional approach to diarization\ninvolves multiple steps of embedding extraction and clustering, which are often\noptimized in an isolated fashion. While end-to-end diarization systems attempt\nto learn a single model for the task, they are often cumbersome to train and\nrequire large supervised datasets. In this paper, we propose an end-to-end\nsupervised hierarchical clustering algorithm based on graph neural networks\n(GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). The\nembedding extractor is initialized using a pre-trained x-vector model while the\nGNN model is trained initially using the x-vector embeddings from the\npre-trained model. Finally, the E-SHARC model uses the front-end mel-filterbank\nfeatures as input and jointly optimizes the embedding extractor and the GNN\nclustering module, performing representation learning, metric learning, and\nclustering with end-to-end optimization. Further, with additional inputs from\nan external overlap detector, the E-SHARC approach is capable of predicting the\nspeakers in the overlapping speech regions. The experimental evaluation on\nbenchmark datasets like AMI, Voxconverse and DISPLACE, illustrates that the\nproposed E-SHARC framework provides competitive diarization results using graph\nbased clustering methods.",
      "tldr_zh": "该论文提出了一种端到端监督层次聚类算法 E-SHARC，用于 Speaker Diarization 任务，该方法基于 Graph Neural Networks (GNN) 实现嵌入提取和聚类模块的联合优化。E-SHARC 初始化使用预训练的 x-vector 模型，并最终以 mel-filterbank 特征作为输入，进行表示学习、度量学习和层次聚类，同时能通过外部重叠检测器处理重叠语音区域。与传统多步骤方法相比，这种端到端框架在 AMI、Voxconverse 和 DISPLACE 数据集上的实验结果显示了竞争性的性能，提高了整体效率。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "11 pages. Under review IEEE TASLP. \\c{opyright} 2024 IEEE",
      "pdf_url": "http://arxiv.org/pdf/2401.12850v2",
      "published_date": "2024-01-23 15:35:44 UTC",
      "updated_date": "2024-12-02 17:38:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:45:16.826848"
    },
    {
      "arxiv_id": "2401.12846v4",
      "title": "How well can a large language model explain business processes as perceived by users?",
      "title_zh": "翻译失败",
      "authors": [
        "Dirk Fahland",
        "Fabiana Fournier",
        "Lior Limonad",
        "Inna Skarbovsky",
        "Ava J. E. Swevels"
      ],
      "abstract": "Large Language Models (LLMs) are trained on a vast amount of text to\ninterpret and generate human-like textual content. They are becoming a vital\nvehicle in realizing the vision of the autonomous enterprise, with\norganizations today actively adopting LLMs to automate many aspects of their\noperations. LLMs are likely to play a prominent role in future AI-augmented\nbusiness process management systems, catering functionalities across all system\nlifecycle stages. One such system's functionality is Situation-Aware\neXplainability (SAX), which relates to generating causally sound and\nhuman-interpretable explanations. In this paper, we present the SAX4BPM\nframework developed to generate SAX explanations. The SAX4BPM suite consists of\na set of services and a central knowledge repository. The functionality of\nthese services is to elicit the various knowledge ingredients that underlie SAX\nexplanations. A key innovative component among these ingredients is the causal\nprocess execution view. In this work, we integrate the framework with an LLM to\nleverage its power to synthesize the various input ingredients for the sake of\nimproved SAX explanations. Since the use of LLMs for SAX is also accompanied by\na certain degree of doubt related to its capacity to adequately fulfill SAX\nalong with its tendency for hallucination and lack of inherent capacity to\nreason, we pursued a methodological evaluation of the perceived quality of the\ngenerated explanations. We developed a designated scale and conducted a\nrigorous user study. Our findings show that the input presented to the LLMs\naided with the guard-railing of its performance, yielding SAX explanations\nhaving better-perceived fidelity. This improvement is moderated by the\nperception of trust and curiosity. More so, this improvement comes at the cost\nof the perceived interpretability of the explanation.",
      "tldr_zh": "本研究评估了大型语言模型 (LLMs) 在解释业务流程方面的表现，焦点在于用户感知的质量，特别是通过 Situation-Aware eXplainability (SAX) 机制。研究提出 SAX4BPM 框架，该框架整合 LLMs 来合成知识成分生成更可靠的 SAX 解释，同时采用守卫机制 (guard-railing) 减少幻觉问题，并通过用户研究使用指定规模评估解释的感知质量。结果显示，LLMs 提升了解释的保真度 (fidelity)，但这会降低可解释性 (interpretability)，且改善效果受用户信任和好奇心的影响。",
      "categories": [
        "cs.AI",
        "68T01"
      ],
      "primary_category": "cs.AI",
      "comment": "42 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.12846v4",
      "published_date": "2024-01-23 15:29:26 UTC",
      "updated_date": "2025-01-29 12:15:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:45:29.090734"
    },
    {
      "arxiv_id": "2401.12835v1",
      "title": "SGTR+: End-to-end Scene Graph Generation with Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Rongjie Li",
        "Songyang Zhang",
        "Xuming He"
      ],
      "abstract": "Scene Graph Generation (SGG) remains a challenging visual understanding task\ndue to its compositional property. Most previous works adopt a bottom-up,\ntwo-stage or point-based, one-stage approach, which often suffers from high\ntime complexity or suboptimal designs. In this work, we propose a novel SGG\nmethod to address the aforementioned issues, formulating the task as a\nbipartite graph construction problem. To address the issues above, we create a\ntransformer-based end-to-end framework to generate the entity and entity-aware\npredicate proposal set, and infer directed edges to form relation triplets.\nMoreover, we design a graph assembling module to infer the connectivity of the\nbipartite scene graph based on our entity-aware structure, enabling us to\ngenerate the scene graph in an end-to-end manner. Based on bipartite graph\nassembling paradigm, we further propose a new technical design to address the\nefficacy of entity-aware modeling and optimization stability of graph\nassembling. Equipped with the enhanced entity-aware design, our method achieves\noptimal performance and time-complexity. Extensive experimental results show\nthat our design is able to achieve the state-of-the-art or comparable\nperformance on three challenging benchmarks, surpassing most of the existing\napproaches and enjoying higher efficiency in inference. Code is available:\nhttps://github.com/Scarecrow0/SGTR",
      "tldr_zh": "本文提出 SGTR+ 方法，这是一种基于 Transformer 的端到端框架，用于解决 Scene Graph Generation (SGG) 任务中的时间复杂性和设计次优问题，将其表述为二分图构建问题。框架通过生成实体和实体感知谓词提案，并利用图组装模块 (graph assembling module) 推断关系三元组的连通性，实现高效的端到端生成。同时，该方法引入新设计来提升实体感知建模的有效性和优化稳定性。实验结果表明，SGTR+ 在三个挑战性基准上达到最先进或可比性能，并显著提高了推理效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by TPAMI: https://ieeexplore.ieee.org/document/10315230",
      "pdf_url": "http://arxiv.org/pdf/2401.12835v1",
      "published_date": "2024-01-23 15:18:20 UTC",
      "updated_date": "2024-01-23 15:18:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:45:41.593906"
    },
    {
      "arxiv_id": "2401.12830v2",
      "title": "Enhancing Next Destination Prediction: A Novel Long Short-Term Memory Neural Network Approach Using Real-World Airline Data",
      "title_zh": "翻译失败",
      "authors": [
        "Salih Salihoglu",
        "Gulser Koksal",
        "Orhan Abar"
      ],
      "abstract": "In the modern transportation industry, accurate prediction of travelers' next\ndestinations brings multiple benefits to companies, such as customer\nsatisfaction and targeted marketing. This study focuses on developing a precise\nmodel that captures the sequential patterns and dependencies in travel data,\nenabling accurate predictions of individual travelers' future destinations. To\nachieve this, a novel model architecture with a sliding window approach based\non Long Short-Term Memory (LSTM) is proposed for destination prediction in the\ntransportation industry. The experimental results highlight satisfactory\nperformance and high scores achieved by the proposed model across different\ndata sizes and performance metrics. This research contributes to advancing\ndestination prediction methods, empowering companies to deliver personalized\nrecommendations and optimize customer experiences in the dynamic travel\nlandscape.",
      "tldr_zh": "本研究旨在通过捕捉旅行数据的顺序模式和依赖关系，来精确预测旅行者的下一个目的地，从而提升客户满意度和针对性营销。提出了一种基于 Long Short-Term Memory (LSTM) 的新型模型架构，采用滑动窗口方法处理真实航空数据，以实现高效的序列预测。实验结果表明，该模型在不同数据规模和性能指标上表现出色，高分表现突显其有效性。该工作推进了目的地预测方法的应用，帮助公司提供个性化推荐并优化动态旅行体验。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12830v2",
      "published_date": "2024-01-23 15:07:49 UTC",
      "updated_date": "2024-09-16 14:40:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:45:51.980827"
    },
    {
      "arxiv_id": "2401.12822v1",
      "title": "Deep Learning Based Simulators for the Phosphorus Removal Process Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms",
      "title_zh": "翻译失败",
      "authors": [
        "Esmaeel Mohammadi",
        "Mikkel Stokholm-Bjerregaard",
        "Aviaja Anna Hansen",
        "Per Halkjær Nielsen",
        "Daniel Ortiz-Arroyo",
        "Petar Durdevic"
      ],
      "abstract": "Phosphorus removal is vital in wastewater treatment to reduce reliance on\nlimited resources. Deep reinforcement learning (DRL) is a machine learning\ntechnique that can optimize complex and nonlinear systems, including the\nprocesses in wastewater treatment plants, by learning control policies through\ntrial and error. However, applying DRL to chemical and biological processes is\nchallenging due to the need for accurate simulators. This study trained six\nmodels to identify the phosphorus removal process and used them to create a\nsimulator for the DRL environment. Although the models achieved high accuracy\n(>97%), uncertainty and incorrect prediction behavior limited their performance\nas simulators over longer horizons. Compounding errors in the models'\npredictions were identified as one of the causes of this problem. This approach\nfor improving process control involves creating simulation environments for DRL\nalgorithms, using data from supervisory control and data acquisition (SCADA)\nsystems with a sufficient historical horizon without complex system modeling or\nparameter estimation.",
      "tldr_zh": "本研究探讨了利用深度学习（Deep Learning）构建模拟器，以优化废水处理中磷移除过程的控制，并通过深度强化学习（DRL）算法实现学习控制策略。研究者训练了六个模型来识别磷移除过程，这些模型准确率超过97%，并基于它们创建了DRL模拟环境，利用来自监督控制和数据采集（SCADA）系统的历史数据进行模拟。然而，模型在较长预测期内受不确定性和预测累积错误的影响，导致性能受限。该方法为改进复杂非线性过程控制提供了新途径，无需复杂的系统建模或参数估计。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "Journal Paper",
      "pdf_url": "http://arxiv.org/pdf/2401.12822v1",
      "published_date": "2024-01-23 14:55:46 UTC",
      "updated_date": "2024-01-23 14:55:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:46:04.841285"
    },
    {
      "arxiv_id": "2401.12819v1",
      "title": "Dynamic Layer Tying for Parameter-Efficient Transformers",
      "title_zh": "动态层绑定用于参数高效的 Transformer",
      "authors": [
        "Tamir David Hay",
        "Lior Wolf"
      ],
      "abstract": "In the pursuit of reducing the number of trainable parameters in deep\ntransformer networks, we employ Reinforcement Learning to dynamically select\nlayers during training and tie them together. Every few iterations, the RL\nagent is asked whether to train each layer $i$ independently or to copy the\nweights of a previous layer $j<i$. This facilitates weight sharing, reduces the\nnumber of trainable parameters, and also serves as an effective regularization\ntechnique. Experimental evaluations validate that our model modestly\noutperforms the baseline transformer model with regard to perplexity and\ndrastically reduces the number of trainable parameters. In particular, the\nmemory consumption during training is up to one order of magnitude less than\nthe conventional training method.",
      "tldr_zh": "本研究提出了一种动态层绑定(Dynamic Layer Tying)方法，用于提高Transformer模型的参数效率。该方法利用Reinforcement Learning在训练过程中动态决定是否让每个层独立训练，或复制先前层的权重，从而实现权重共享、减少可训练参数，并作为有效的正则化技术。实验结果显示，该模型在perplexity上略优于基线Transformer模型，同时大幅降低可训练参数数量，训练时的内存消耗比传统方法减少一个数量级。整体而言，此方法为高效的深度网络训练提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12819v1",
      "published_date": "2024-01-23 14:53:20 UTC",
      "updated_date": "2024-01-23 14:53:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:46:15.330988"
    },
    {
      "arxiv_id": "2401.12806v2",
      "title": "Binary structured physics-informed neural networks for solving equations with rapidly changing solutions",
      "title_zh": "二元结构物理信息神经网络用于求解具有快速变化解的方程",
      "authors": [
        "Yanzhi Liu",
        "Ruifan Wu",
        "Ying Jiang"
      ],
      "abstract": "Physics-informed neural networks (PINNs), rooted in deep learning, have\nemerged as a promising approach for solving partial differential equations\n(PDEs). By embedding the physical information described by PDEs into\nfeedforward neural networks, PINNs are trained as surrogate models to\napproximate solutions without the need for label data. Nevertheless, even\nthough PINNs have shown remarkable performance, they can face difficulties,\nespecially when dealing with equations featuring rapidly changing solutions.\nThese difficulties encompass slow convergence, susceptibility to becoming\ntrapped in local minima, and reduced solution accuracy. To address these\nissues, we propose a binary structured physics-informed neural network (BsPINN)\nframework, which employs binary structured neural network (BsNN) as the neural\nnetwork component. By leveraging a binary structure that reduces inter-neuron\nconnections compared to fully connected neural networks, BsPINNs excel in\ncapturing the local features of solutions more effectively and efficiently.\nThese features are particularly crucial for learning the rapidly changing in\nthe nature of solutions. In a series of numerical experiments solving Burgers\nequation, Euler equation, Helmholtz equation, and high-dimension Poisson\nequation, BsPINNs exhibit superior convergence speed and heightened accuracy\ncompared to PINNs. From these experiments, we discover that BsPINNs resolve the\nissues caused by increased hidden layers in PINNs resulting in over-smoothing,\nand prevent the decline in accuracy due to non-smoothness of PDEs solutions.",
      "tldr_zh": "该论文针对 Physics-informed neural networks (PINNs) 在求解快速变化解的偏微分方程 (PDEs) 时面临的收敛慢、易陷局部最小值和准确性降低等问题，提出了一种新型框架 Binary structured physics-informed neural network (BsPINN)。BsPINN 采用 Binary structured neural network (BsNN) 作为核心组件，通过减少神经元间连接的二进制结构，更高效地捕捉解的局部特征，从而提升对快速变化解的学习能力。在数值实验中，BsPINNs 在求解 Burgers equation、Euler equation、Helmholtz equation 和高维 Poisson equation 时，比传统 PINNs 表现出更快的收敛速度和更高的准确性。该方法还解决了 PINNs 中隐藏层增加导致的过度平滑问题，并提升了对非平滑解的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12806v2",
      "published_date": "2024-01-23 14:37:51 UTC",
      "updated_date": "2024-01-25 12:53:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:46:30.475653"
    },
    {
      "arxiv_id": "2401.12783v1",
      "title": "A Review of Deep Learning Methods for Photoplethysmography Data",
      "title_zh": "翻译失败",
      "authors": [
        "Guangkun Nie",
        "Jiabao Zhu",
        "Gongzheng Tang",
        "Deyun Zhang",
        "Shijia Geng",
        "Qinghao Zhao",
        "Shenda Hong"
      ],
      "abstract": "Photoplethysmography (PPG) is a highly promising device due to its advantages\nin portability, user-friendly operation, and non-invasive capabilities to\nmeasure a wide range of physiological information. Recent advancements in deep\nlearning have demonstrated remarkable outcomes by leveraging PPG signals for\ntasks related to personal health management and other multifaceted\napplications. In this review, we systematically reviewed papers that applied\ndeep learning models to process PPG data between January 1st of 2017 and July\n31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed\nfrom three key perspectives: tasks, models, and data. We finally extracted 193\npapers where different deep learning frameworks were used to process PPG\nsignals. Based on the tasks addressed in these papers, we categorized them into\ntwo major groups: medical-related, and non-medical-related. The medical-related\ntasks were further divided into seven subgroups, including blood pressure\nanalysis, cardiovascular monitoring and diagnosis, sleep health, mental health,\nrespiratory monitoring and analysis, blood glucose analysis, as well as others.\nThe non-medical-related tasks were divided into four subgroups, which encompass\nsignal processing, biometric identification, electrocardiogram reconstruction,\nand human activity recognition. In conclusion, significant progress has been\nmade in the field of using deep learning methods to process PPG data recently.\nThis allows for a more thorough exploration and utilization of the information\ncontained in PPG signals. However, challenges remain, such as limited quantity\nand quality of publicly available databases, a lack of effective validation in\nreal-world scenarios, and concerns about the interpretability, scalability, and\ncomplexity of deep learning models. Moreover, there are still emerging research\nareas that require further investigation.",
      "tldr_zh": "这篇综述论文回顾了2017年至2023年间使用deep learning处理Photoplethysmography (PPG)数据的193篇研究，系统分析了这些论文从任务、模型和数据三个关键视角。论文将任务分为医疗相关（如血压分析、心血管监测、睡眠健康等七个子组）和非医疗相关（如信号处理、生物识别、心电图重建和人类活动识别等四个子组），突显了deep learning在PPG信号处理中的显著进展。研究指出，虽然这促进了PPG在个人健康管理和多领域应用的探索，但仍面临数据质量有限、真实场景验证不足以及模型的可解释性和可扩展性挑战。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12783v1",
      "published_date": "2024-01-23 14:11:29 UTC",
      "updated_date": "2024-01-23 14:11:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:46:44.062553"
    },
    {
      "arxiv_id": "2401.12771v1",
      "title": "Deep Learning-based Intraoperative MRI Reconstruction",
      "title_zh": "基于深度学习的术中磁共振成像重建",
      "authors": [
        "Jon André Ottesen",
        "Tryggve Storas",
        "Svein Are Sirirud Vatnehol",
        "Grethe Løvland",
        "Einar O. Vik-Mo",
        "Till Schellhorn",
        "Karoline Skogen",
        "Christopher Larsson",
        "Atle Bjørnerud",
        "Inge Rasmus Groote-Eindbaas",
        "Matthan W. A. Caan"
      ],
      "abstract": "Purpose: To evaluate the quality of deep learning reconstruction for\nprospectively accelerated intraoperative magnetic resonance imaging (iMRI)\nduring resective brain tumor surgery.\n  Materials and Methods: Accelerated iMRI was performed during brain surgery\nusing dual surface coils positioned around the area of resection. A deep\nlearning (DL) model was trained on the fastMRI neuro dataset to mimic the data\nfrom the iMRI protocol. Evaluation was performed on imaging material from 40\npatients imaged between 01.11.2021 - 01.06.2023 that underwent iMRI during\ntumor resection surgery. A comparative analysis was conducted between the\nconventional compressed sense (CS) method and the trained DL reconstruction\nmethod. Blinded evaluation of multiple image quality metrics was performed by\ntwo working neuro-radiologists and a working neurosurgeon on a 1 to 5 Likert\nscale (1=non diagnostic, 2=poor, 3=acceptable, 4=good, 5=excellent), and the\nfavored reconstruction variant.\n  Results: The DL reconstruction was strongly favored or favored over the CS\nreconstruction for 33/40, 39/40, and 8/40 of cases for reader 1, 2, and 3,\nrespectively. Two of three readers consistently assigned higher ratings for the\nDL reconstructions, and the DL reconstructions had a higher score than their\nrespective CS counterparts for 72%, 72%, and 14% of the cases for reader 1, 2,\nand 3, respectively. Still, the DL reconstructions exhibited shortcomings such\nas a striping artifact and reduced signal.\n  Conclusion: DL shows promise to allow for high-quality reconstructions of\nintraoperative MRI with equal to or improved perceived spatial resolution,\nsignal-to-noise ratio, diagnostic confidence, diagnostic conspicuity, and\nspatial resolution compared to compressed sense.",
      "tldr_zh": "本研究评估了深度学习（DL）方法在加速 intraoperative MRI (iMRI) 上的重建质量，针对脑肿瘤手术中的图像优化。研究团队训练了一个 DL 模型，使用 fastMRI neuro 数据集模拟 iMRI 协议，并对 40 名患者的数据进行比较分析，将 DL 重建与传统 compressed sense (CS) 方法对比，通过三位专家盲评（使用 1-5 Likert 量表）。结果显示，DL 重建在大多数病例中被偏好，图像质量得分更高，改善了空间分辨率、信噪比和诊断置信度，但存在条纹伪影和信号减弱等问题。总体而言，DL 方法展示了在 iMRI 中实现高质量重建的潜力，为手术成像提供更可靠的支持。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12771v1",
      "published_date": "2024-01-23 13:57:50 UTC",
      "updated_date": "2024-01-23 13:57:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:46:56.586044"
    },
    {
      "arxiv_id": "2401.12756v2",
      "title": "What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition",
      "title_zh": "翻译失败",
      "authors": [
        "Carolin Holtermann",
        "Markus Frohmann",
        "Navid Rekabsaz",
        "Anne Lauscher"
      ],
      "abstract": "The knowledge encapsulated in a model is the core factor determining its\nfinal performance on downstream tasks. Much research in NLP has focused on\nefficient methods for storing and adapting different types of knowledge, e.g.,\nin dedicated modularized structures, and on how to effectively combine these,\ne.g., by learning additional parameters. However, given the many possible\noptions, a thorough understanding of the mechanisms involved in these\ncompositions is missing, and hence it remains unclear which strategies to\nutilize. To address this research gap, we propose a novel framework for\nzero-shot module composition, which encompasses existing and some novel\nvariations for selecting, weighting, and combining parameter modules under a\nsingle unified notion. Focusing on the scenario of domain knowledge and adapter\nlayers, our framework provides a systematic unification of concepts, allowing\nus to conduct the first comprehensive benchmarking study of various zero-shot\nknowledge composition strategies. In particular, we test two module combination\nmethods and five selection and weighting strategies for their effectiveness and\nefficiency in an extensive experimental setup. Our results highlight the\nefficacy of ensembling but also hint at the power of simple though\noften-ignored weighting methods. Further in-depth analyses allow us to\nunderstand the role of weighting vs. top-k selection, and show that, to a\ncertain extent, the performance of adapter composition can even be predicted.",
      "tldr_zh": "本研究提出一个统一的框架，用于零-shot 知识组合，旨在系统化地处理 NLP 中参数模块的选择、加权和组合策略。该框架涵盖现有和新颖变体，焦点在于 domain knowledge 和 adapter layers，通过测试两种模块组合方法和五种选择/加权策略进行全面基准测试。结果显示，ensembling 方法在有效性和效率上表现出色，同时简单加权策略往往被低估，且分析揭示了加权 vs. top-k selection 的作用，以及 adapter composition 性能的可预测性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Findings of the ACL: EACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.12756v2",
      "published_date": "2024-01-23 13:35:47 UTC",
      "updated_date": "2024-01-25 14:32:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:47:06.018941"
    },
    {
      "arxiv_id": "2401.12731v4",
      "title": "The Distributional Uncertainty of the SHAP score in Explainable Machine Learning",
      "title_zh": "可解释机器学习中 SHAP 分数的分布不确定性",
      "authors": [
        "Santiago Cifuentes",
        "Leopoldo Bertossi",
        "Nina Pardal",
        "Sergio Abriola",
        "Maria Vanina Martinez",
        "Miguel Romero"
      ],
      "abstract": "Attribution scores reflect how important the feature values in an input\nentity are for the output of a machine learning model. One of the most popular\nattribution scores is the SHAP score, which is an instantiation of the general\nShapley value used in coalition game theory. The definition of this score\nrelies on a probability distribution on the entity population. Since the exact\ndistribution is generally unknown, it needs to be assigned subjectively or be\nestimated from data, which may lead to misleading feature scores. In this\npaper, we propose a principled framework for reasoning on SHAP scores under\nunknown entity population distributions. In our framework, we consider an\nuncertainty region that contains the potential distributions, and the SHAP\nscore of a feature becomes a function defined over this region. We study the\nbasic problems of finding maxima and minima of this function, which allows us\nto determine tight ranges for the SHAP scores of all features. In particular,\nwe pinpoint the complexity of these problems, and other related ones, showing\nthem to be NP-complete. Finally, we present experiments on a real-world\ndataset, showing that our framework may contribute to a more robust feature\nscoring.",
      "tldr_zh": "这篇论文探讨了SHAP score在可解释机器学习中的分布不确定性问题，因为SHAP score依赖于未知的实体种群概率分布，可能导致特征归因误导。作者提出一个原则性框架，将不确定性建模为包含潜在分布的区域，并将SHAP score视为该区域上的函数，以计算其最大最小值范围。实验结果显示，该框架在真实数据集上提升了特征评分的稳健性，并证明了相关问题为NP-complete。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO",
        "68T37, 68T27"
      ],
      "primary_category": "cs.AI",
      "comment": "In ECAI 2024 proceedings",
      "pdf_url": "http://arxiv.org/pdf/2401.12731v4",
      "published_date": "2024-01-23 13:04:02 UTC",
      "updated_date": "2024-08-13 16:12:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:47:19.720907"
    },
    {
      "arxiv_id": "2401.13708v1",
      "title": "Accelerating hyperbolic t-SNE",
      "title_zh": "翻译失败",
      "authors": [
        "Martin Skrodzki",
        "Hunter van Geffen",
        "Nicolas F. Chaves-de-Plaza",
        "Thomas Höllt",
        "Elmar Eisemann",
        "Klaus Hildebrandt"
      ],
      "abstract": "The need to understand the structure of hierarchical or high-dimensional data\nis present in a variety of fields. Hyperbolic spaces have proven to be an\nimportant tool for embedding computations and analysis tasks as their\nnon-linear nature lends itself well to tree or graph data. Subsequently, they\nhave also been used in the visualization of high-dimensional data, where they\nexhibit increased embedding performance. However, none of the existing\ndimensionality reduction methods for embedding into hyperbolic spaces scale\nwell with the size of the input data. That is because the embeddings are\ncomputed via iterative optimization schemes and the computation cost of every\niteration is quadratic in the size of the input. Furthermore, due to the\nnon-linear nature of hyperbolic spaces, Euclidean acceleration structures\ncannot directly be translated to the hyperbolic setting. This paper introduces\nthe first acceleration structure for hyperbolic embeddings, building upon a\npolar quadtree. We compare our approach with existing methods and demonstrate\nthat it computes embeddings of similar quality in significantly less time.\nImplementation and scripts for the experiments can be found at\nhttps://graphics.tudelft.nl/accelerating-hyperbolic-tsne.",
      "tldr_zh": "这篇论文针对超曲空间(hyperbolic spaces)嵌入方法（如 hyperbolic t-SNE）的效率问题，指出现有方法在处理大尺寸数据时因迭代优化而计算成本呈二次方增长，且无法直接应用欧式加速结构。作者提出了一种基于 polar quadtree 的新加速结构，用于加速超曲嵌入计算过程。实验结果显示，该方法在显著减少计算时间的同时，能获得与现有方法类似质量的嵌入，为高维数据可视化和分析提供更高效的工具。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG",
        "q-bio.QM",
        "stat.ML"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13708v1",
      "published_date": "2024-01-23 12:59:40 UTC",
      "updated_date": "2024-01-23 12:59:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:47:33.131772"
    },
    {
      "arxiv_id": "2402.00046v2",
      "title": "Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sofiene Lassoued",
        "Andreas Schwung"
      ],
      "abstract": "Resource utilization and production process optimization are crucial for\ncompanies in today's competitive industrial landscape. Addressing the\ncomplexities of job shop scheduling problems (JSSP) is essential to improving\nproductivity, reducing costs, and ensuring timely delivery. We propose PetriRL,\na novel framework integrating Petri nets and deep reinforcement learning (DRL)\nfor JSSP optimization. PetriRL capitalizes on the inherent strengths of Petri\nnets in modelling discrete event systems while leveraging the advantages of a\ngraph structure. The Petri net governs automated components of the process,\nensuring adherence to JSSP constraints. This allows for synergistic\ncollaboration with optimization algorithms such as DRL, particularly in\ncritical decision-making. Unlike traditional methods, PetriRL eliminates the\nneed to preprocess JSSP instances into disjunctive graphs and enhances the\nexplainability of process status through its graphical structure based on\nplaces and transitions. Additionally, the inherent graph structure of Petri\nnets enables the dynamic additions of job operations during the inference phase\nwithout requiring agent retraining, thus enhancing flexibility. Experimental\nresults demonstrate PetriRL's robust generalization across various instance\nsizes and its competitive performance on public test benchmarks and randomly\ngenerated instances. Results are compared to a wide range of optimization\nsolutions such as heuristics, metaheuristics, and learning-based algorithms.\nFinally, the added values of the framework's key elements, such as event-based\ncontrol and action masking, are studied in the ablation study.",
      "tldr_zh": "这篇论文引入了PetriRL框架，一种创新方法，通过整合Petri nets和事件-based强化学习来解决作业车间调度问题(JSSP)，旨在优化资源利用和生产流程。PetriRL利用Petri nets的图形结构建模离散事件系统，并与深度强化学习(DRL)协同工作，确保遵守JSSP约束，同时提升决策的可解释性和灵活性，而无需预处理实例成disjunctive graphs。实验结果显示，PetriRL在公共基准测试和随机生成实例上表现出色，与启发式、元启发式和学习-based算法相比具有竞争力和泛化能力；消融研究进一步验证了事件-based控制和action masking等关键元素的附加价值。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.00046v2",
      "published_date": "2024-01-23 12:30:49 UTC",
      "updated_date": "2024-05-08 10:47:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:47:44.748137"
    },
    {
      "arxiv_id": "2401.12714v1",
      "title": "Evaluation of large language models for assessing code maintainability",
      "title_zh": "翻译失败",
      "authors": [
        "Marc Dillmann",
        "Julien Siebert",
        "Adam Trendowicz"
      ],
      "abstract": "Increased availability of open-source software repositories and recent\nadvances in code analysis using large language models (LLMs) has triggered a\nwave of new work to automate software engineering tasks that were previously\nvery difficult to automate. In this paper, we investigate a recent line of work\nthat hypothesises that comparing the probability of code generated by LLMs with\nthe probability the current code would have had can indicate potential quality\nproblems. We investigate the association between the cross-entropy of code\ngenerated by ten different models (based on GPT2 and Llama2) and the following\nquality aspects: readability, understandability, complexity, modularisation,\nand overall maintainability assessed by experts and available in an benchmark\ndataset. Our results show that, controlling for the number of logical lines of\ncodes (LLOC), cross-entropy computed by LLMs is indeed a predictor of\nmaintainability on a class level (the higher the cross-entropy the lower the\nmaintainability). However, this relation is reversed when one does not control\nfor LLOC (e.g., comparing small classes with longer ones). Furthermore, while\nthe complexity of LLMs affects the range of cross-entropy (smaller models tend\nto have a wider range of cross-entropy), this plays a significant role in\npredicting maintainability aspects. Our study limits itself on ten different\npretrained models (based on GPT2 and Llama2) and on maintainability aspects\ncollected by Schnappinger et al. When controlling for logical lines of code\n(LLOC), cross-entropy is a predictor of maintainability. However, while related\nwork has shown the potential usefulness of cross-entropy at the level of tokens\nor short sequences, at the class level this criterion alone may prove\ninsufficient to predict maintainability and further research is needed to make\nbest use of this information in practice.",
      "tldr_zh": "本研究评估了大型语言模型（LLMs，如GPT2和Llama2）在代码可维护性评估中的表现，方法是通过比较LLMs生成的代码概率与现有代码的交叉熵（cross-entropy），并分析其与代码质量指标（如可读性、理解性、复杂性、模块化和整体可维护性）的关联。结果显示，控制逻辑行数（LLOC）后，交叉熵是可维护性的预测指标（交叉熵越高，可维护性越低），但不控制LLOC时，这一关系会逆转，且模型复杂性会影响交叉熵的范围。总体而言，虽然交叉熵在类级别显示潜力，但其单独使用可能不足以准确预测可维护性，因此需要进一步研究以优化实际应用。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "68",
        "D.2.7"
      ],
      "primary_category": "cs.SE",
      "comment": "14 pages, 4 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2401.12714v1",
      "published_date": "2024-01-23 12:29:42 UTC",
      "updated_date": "2024-01-23 12:29:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:47:56.483711"
    },
    {
      "arxiv_id": "2401.12708v2",
      "title": "Deep Neural Network Benchmarks for Selective Classification",
      "title_zh": "针对选择性分类的深度神经网络基准测试",
      "authors": [
        "Andrea Pugnana",
        "Lorenzo Perini",
        "Jesse Davis",
        "Salvatore Ruggieri"
      ],
      "abstract": "With the increasing deployment of machine learning models in many socially\nsensitive tasks, there is a growing demand for reliable and trustworthy\npredictions. One way to accomplish these requirements is to allow a model to\nabstain from making a prediction when there is a high risk of making an error.\nThis requires adding a selection mechanism to the model, which selects those\nexamples for which the model will provide a prediction. The selective\nclassification framework aims to design a mechanism that balances the fraction\nof rejected predictions (i.e., the proportion of examples for which the model\ndoes not make a prediction) versus the improvement in predictive performance on\nthe selected predictions. Multiple selective classification frameworks exist,\nmost of which rely on deep neural network architectures. However, the empirical\nevaluation of the existing approaches is still limited to partial comparisons\namong methods and settings, providing practitioners with little insight into\ntheir relative merits. We fill this gap by benchmarking 18 baselines on a\ndiverse set of 44 datasets that includes both image and tabular data. Moreover,\nthere is a mix of binary and multiclass tasks. We evaluate these approaches\nusing several criteria, including selective error rate, empirical coverage,\ndistribution of rejected instance's classes, and performance on\nout-of-distribution instances. The results indicate that there is not a single\nclear winner among the surveyed baselines, and the best method depends on the\nusers' objectives.",
      "tldr_zh": "本研究针对机器学习模型在社会敏感任务中的可靠性和可信性，引入了 selective classification 框架，该框架允许模型在高风险情况下拒绝预测，从而平衡拒绝比例与选定预测的性能改进。研究者对 18 个基于 deep neural network 的基线方法进行了全面基准测试，涵盖 44 个多样数据集（包括图像和表格数据、二元和多类任务），并评估了 selective error rate、empirical coverage、拒绝实例的类分布以及 out-of-distribution 性能。结果显示，没有单一方法优于其他，所有方法的效果取决于用户的具体目标，为从业者提供宝贵的比较洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in The Journal of Data centric Machine Learning Research\n  (DMLR), Vol 1, (17):1-58 (2024)",
      "pdf_url": "http://arxiv.org/pdf/2401.12708v2",
      "published_date": "2024-01-23 12:15:47 UTC",
      "updated_date": "2024-09-18 07:48:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:48:06.981536"
    },
    {
      "arxiv_id": "2401.12700v1",
      "title": "Securing Recommender System via Cooperative Training",
      "title_zh": "翻译失败",
      "authors": [
        "Qingyang Wang",
        "Chenwang Wu",
        "Defu Lian",
        "Enhong Chen"
      ],
      "abstract": "Recommender systems are often susceptible to well-crafted fake profiles,\nleading to biased recommendations. Among existing defense methods,\ndata-processing-based methods inevitably exclude normal samples, while\nmodel-based methods struggle to enjoy both generalization and robustness. To\nthis end, we suggest integrating data processing and the robust model to\npropose a general framework, Triple Cooperative Defense (TCD), which employs\nthree cooperative models that mutually enhance data and thereby improve\nrecommendation robustness. Furthermore, Considering that existing attacks\nstruggle to balance bi-level optimization and efficiency, we revisit poisoning\nattacks in recommender systems and introduce an efficient attack strategy,\nCo-training Attack (Co-Attack), which cooperatively optimizes the attack\noptimization and model training, considering the bi-level setting while\nmaintaining attack efficiency. Moreover, we reveal a potential reason for the\ninsufficient threat of existing attacks is their default assumption of\noptimizing attacks in undefended scenarios. This overly optimistic setting\nlimits the potential of attacks. Consequently, we put forth a Game-based\nCo-training Attack (GCoAttack), which frames the proposed CoAttack and TCD as a\ngame-theoretic process, thoroughly exploring CoAttack's attack potential in the\ncooperative training of attack and defense. Extensive experiments on three real\ndatasets demonstrate TCD's superiority in enhancing model robustness.\nAdditionally, we verify that the two proposed attack strategies significantly\noutperform existing attacks, with game-based GCoAttack posing a greater\npoisoning threat than CoAttack.",
      "tldr_zh": "本研究针对推荐系统(recommender systems)易受伪造配置文件影响的问题，提出Triple Cooperative Defense (TCD)框架，该框架通过三个合作模型相互增强数据，结合数据处理和鲁棒模型方法，提高系统的泛化和鲁棒性。论文还重新审视投毒攻击(poisoning attacks)的效率问题，引入Co-training Attack (Co-Attack)策略，该策略合作优化攻击和模型训练以平衡双层优化(bi-level optimization)。此外，作者提出Game-based Co-training Attack (GCoAttack)，将Co-Attack与TCD视为博弈过程，揭示现有攻击在无防御场景假设下的局限性。在三个真实数据集上的实验显示，TCD显著提升了模型鲁棒性，而新攻击策略远超现有方法，GCoAttack展现出更大的威胁。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: text overlap with arXiv:2210.13762",
      "pdf_url": "http://arxiv.org/pdf/2401.12700v1",
      "published_date": "2024-01-23 12:07:20 UTC",
      "updated_date": "2024-01-23 12:07:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:48:20.699808"
    },
    {
      "arxiv_id": "2401.12689v3",
      "title": "Energy-based Automated Model Evaluation",
      "title_zh": "基于能量的自动化模型评估",
      "authors": [
        "Ru Peng",
        "Heming Zou",
        "Haobo Wang",
        "Yawen Zeng",
        "Zenan Huang",
        "Junbo Zhao"
      ],
      "abstract": "The conventional evaluation protocols on machine learning models rely heavily\non a labeled, i.i.d-assumed testing dataset, which is not often present in real\nworld applications. The Automated Model Evaluation (AutoEval) shows an\nalternative to this traditional workflow, by forming a proximal prediction\npipeline of the testing performance without the presence of ground-truth\nlabels. Despite its recent successes, the AutoEval frameworks still suffer from\nan overconfidence issue, substantial storage and computational cost. In that\nregard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that\nallows the AutoEval framework to be both more efficient and effective. The core\nof the MDE is to establish a meta-distribution statistic, on the information\n(energy) associated with individual samples, then offer a smoother\nrepresentation enabled by energy-based learning. We further provide our\ntheoretical insights by connecting the MDE with the classification loss. We\nprovide extensive experiments across modalities, datasets and different\narchitectural backbones to validate MDE's validity, together with its\nsuperiority compared with prior approaches. We also prove MDE's versatility by\nshowing its seamless integration with large-scale models, and easy adaption to\nlearning scenarios with noisy- or imbalanced- labels. Code and data are\navailable: https://github.com/pengr/Energy_AutoEval",
      "tldr_zh": "本研究针对传统机器学习模型评估依赖带标签测试数据集的局限性，提出了一种基于能量的 Automated Model Evaluation (AutoEval) 框架改进方法，以解决现有框架的过度自信、存储和计算成本高等问题。论文引入了新的度量标准 Meta-Distribution Energy (MDE)，通过建立样本信息的元分布统计并利用 energy-based learning 提供更平滑的表示，从而提升评估的效率和有效性。理论上，MDE 与分类损失建立了联系，并在多种模态、数据集和架构上进行的广泛实验中，证明其比现有方法更优越，并展示了其与大规模模型的无缝集成以及对噪声或不平衡标签场景的适应性。代码和数据已在 GitHub 上公开。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR2024 poster paper",
      "pdf_url": "http://arxiv.org/pdf/2401.12689v3",
      "published_date": "2024-01-23 11:54:09 UTC",
      "updated_date": "2024-03-15 06:51:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:48:33.235927"
    },
    {
      "arxiv_id": "2401.12686v2",
      "title": "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Christian Fabian",
        "Kai Cui",
        "Heinz Koeppl"
      ],
      "abstract": "Learning the behavior of large agent populations is an important task for\nnumerous research areas. Although the field of multi-agent reinforcement\nlearning (MARL) has made significant progress towards solving these systems,\nsolutions for many agents often remain computationally infeasible and lack\ntheoretical guarantees. Mean Field Games (MFGs) address both of these issues\nand can be extended to Graphon MFGs (GMFGs) to include network structures\nbetween agents. Despite their merits, the real world applicability of GMFGs is\nlimited by the fact that graphons only capture dense graphs. Since most\nempirically observed networks show some degree of sparsity, such as power law\ngraphs, the GMFG framework is insufficient for capturing these network\ntopologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which\nbuilds on the graph theoretical concept of graphexes. Graphexes are the\nlimiting objects to sparse graph sequences that also have other desirable\nfeatures such as the small world property. Learning equilibria in these games\nis challenging due to the rich and sparse structure of the underlying graphs.\nTo tackle these challenges, we design a new learning algorithm tailored to the\nGXMFG setup. This hybrid graphex learning approach leverages that the system\nmainly consists of a highly connected core and a sparse periphery. After\ndefining the system and providing a theoretical analysis, we state our learning\napproach and demonstrate its learning capabilities on both synthetic graphs and\nreal-world networks. This comparison shows that our GXMFG learning algorithm\nsuccessfully extends MFGs to a highly relevant class of hard, realistic\nlearning problems that are not accurately addressed by current MARL and MFG\nmethods.",
      "tldr_zh": "本论文提出了一种新的框架 Graphex Mean Field Games (GXMFGs)，用于学习大规模代理行为在稀疏图上的均衡问题，解决了传统 Mean Field Games (MFGs) 和 Graphon MFGs (GMFGs) 仅适用于密集图的局限性。研究者设计了一个混合 graphex 学习算法，该算法利用图的结构（如高度连接的核心和稀疏的外围）来处理多代理强化学习 (MARL) 的计算挑战，并提供了理论分析。实验在合成图和真实网络上验证了该算法的有效性，展示了其在现实稀疏网络问题上的优越性能，从而扩展了 MFGs 的适用范围。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "accepted at ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.12686v2",
      "published_date": "2024-01-23 11:52:00 UTC",
      "updated_date": "2024-02-23 10:04:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:48:47.602306"
    },
    {
      "arxiv_id": "2401.12681v1",
      "title": "Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical Learning",
      "title_zh": "非邻居也对克里金插值很重要：一种新的对比原型学习",
      "authors": [
        "Zhishuai Li",
        "Yunhao Nie",
        "Ziyue Li",
        "Lei Bai",
        "Yisheng Lv",
        "Rui Zhao"
      ],
      "abstract": "Kriging aims at estimating the attributes of unsampled geo-locations from\nobservations in the spatial vicinity or physical connections, which helps\nmitigate skewed monitoring caused by under-deployed sensors. Existing works\nassume that neighbors' information offers the basis for estimating the\nattributes of the unobserved target while ignoring non-neighbors. However,\nnon-neighbors could also offer constructive information, and neighbors could\nalso be misleading. To this end, we propose ``Contrastive-Prototypical''\nself-supervised learning for Kriging (KCP) to refine valuable information from\nneighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we\nconduct the Kriging task from a new perspective of representation: we aim to\nfirst learn robust and general representations and then recover attributes from\nrepresentations. A neighboring contrastive module is designed that coarsely\nlearns the representations by narrowing the representation distance between the\ntarget and its neighbors while pushing away the non-neighbors. In parallel, a\nprototypical module is introduced to identify similar representations via\nexchanged prediction, thus refining the misleading neighbors and recycling the\nuseful non-neighbors from the neighboring contrast component. As a result, not\nall the neighbors and some of the non-neighbors will be used to infer the\ntarget. To encourage the two modules above to learn general and robust\nrepresentations, we design an adaptive augmentation module that incorporates\ndata-driven attribute augmentation and centrality-based topology augmentation\nover the spatiotemporal Kriging graph data. Extensive experiments on real-world\ndatasets demonstrate the superior performance of KCP compared to its peers with\n6% improvements and exceptional transferability and robustness. The code is\navailable at https://github.com/bonaldli/KCP",
      "tldr_zh": "本研究提出了一种新的自监督学习方法Contrastive-Prototypical Learning for Kriging (KCP)，旨在解决传统Kriging算法仅依赖邻居信息而忽略非邻居信息的局限性，从而更准确地从观测数据估计未采样地理位置的属性。KCP框架包括邻居对比模块（缩小目标与邻居表示距离并推开非邻居）、原型模块（通过交换预测提炼有用信息，筛选误导邻居和有价值非邻居），以及自适应增强模块（结合数据驱动属性增强和基于中心性的拓扑增强，以学习鲁棒、通用的表示）。实验结果显示，KCP在真实数据集上比现有方法提升6%的性能，并表现出卓越的转移性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in AISTATS 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.12681v1",
      "published_date": "2024-01-23 11:46:31 UTC",
      "updated_date": "2024-01-23 11:46:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:48:59.343775"
    },
    {
      "arxiv_id": "2401.12672v1",
      "title": "ChatGraph: Chat with Your Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Yun Peng",
        "Sen Lin",
        "Qian Chen",
        "Lyu Xu",
        "Xiaojun Ren",
        "Yafei Li",
        "Jianliang Xu"
      ],
      "abstract": "Graph analysis is fundamental in real-world applications. Traditional\napproaches rely on SPARQL-like languages or clicking-and-dragging interfaces to\ninteract with graph data. However, these methods either require users to\npossess high programming skills or support only a limited range of graph\nanalysis functionalities. To address the limitations, we propose a large\nlanguage model (LLM)-based framework called ChatGraph. With ChatGraph, users\ncan interact with graphs through natural language, making it easier to use and\nmore flexible than traditional approaches. The core of ChatGraph lies in\ngenerating chains of graph analysis APIs based on the understanding of the\ntexts and graphs inputted in the user prompts. To achieve this, ChatGraph\nconsists of three main modules: an API retrieval module that searches for\nrelevant APIs, a graph-aware LLM module that enables the LLM to comprehend\ngraphs, and an API chain-oriented finetuning module that guides the LLM in\ngenerating API chains.",
      "tldr_zh": "本文研究了图分析的局限性，传统方法依赖 SPARQL-like 语言或点击拖拽界面，需要高编程技能或功能受限。为解决这些问题，提出了一种基于大型语言模型 (LLM) 的框架 ChatGraph，允许用户通过自然语言与图数据交互，其核心是通过 API 检索模块、图感知 LLM 模块和 API 链导向微调模块生成图分析 API 链。ChatGraph 提升了图分析的易用性和灵活性，使其更适合实际应用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12672v1",
      "published_date": "2024-01-23 11:29:19 UTC",
      "updated_date": "2024-01-23 11:29:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:49:10.415669"
    },
    {
      "arxiv_id": "2401.12666v1",
      "title": "EL-VIT: Probing Vision Transformer with Interactive Visualization",
      "title_zh": "EL-VIT：通过交互式可视化探测视觉Transformer",
      "authors": [
        "Hong Zhou",
        "Rui Zhang",
        "Peifeng Lai",
        "Chaoran Guo",
        "Yong Wang",
        "Zhida Sun",
        "Junjie Li"
      ],
      "abstract": "Nowadays, Vision Transformer (ViT) is widely utilized in various computer\nvision tasks, owing to its unique self-attention mechanism. However, the model\narchitecture of ViT is complex and often challenging to comprehend, leading to\na steep learning curve. ViT developers and users frequently encounter\ndifficulties in interpreting its inner workings. Therefore, a visualization\nsystem is needed to assist ViT users in understanding its functionality. This\npaper introduces EL-VIT, an interactive visual analytics system designed to\nprobe the Vision Transformer and facilitate a better understanding of its\noperations. The system consists of four layers of visualization views. The\nfirst three layers include model overview, knowledge background graph, and\nmodel detail view. These three layers elucidate the operation process of ViT\nfrom three perspectives: the overall model architecture, detailed explanation,\nand mathematical operations, enabling users to understand the underlying\nprinciples and the transition process between layers. The fourth interpretation\nview helps ViT users and experts gain a deeper understanding by calculating the\ncosine similarity between patches. Our two usage scenarios demonstrate the\neffectiveness and usability of EL-VIT in helping ViT users understand the\nworking mechanism of ViT.",
      "tldr_zh": "本论文提出 EL-VIT，一种交互式可视化系统，用于探究 Vision Transformer (ViT) 的内部机制，以解决其复杂架构导致的理解难题。系统由四个层级的视图组成，包括模型概述、知识背景图、模型细节视图，以及通过计算 patches 之间余弦相似性来提供更深层解释，帮助用户从整体架构、详细操作和数学原理等方面理解 ViT。实验中的两个使用场景证明了 EL-VIT 的有效性和可用性，提升了 ViT 用户对模型工作机制的掌握。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 7 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2401.12666v1",
      "published_date": "2024-01-23 11:21:32 UTC",
      "updated_date": "2024-01-23 11:21:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:49:22.364834"
    },
    {
      "arxiv_id": "2401.12665v2",
      "title": "ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Shengze Li",
        "Jianjian Cao",
        "Peng Ye",
        "Yuhan Ding",
        "Chongjun Tu",
        "Tao Chen"
      ],
      "abstract": "Recently, foundational models such as CLIP and SAM have shown promising\nperformance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However,\neither CLIP-based or SAM-based ZSAS methods still suffer from non-negligible\nkey drawbacks: 1) CLIP primarily focuses on global feature alignment across\ndifferent inputs, leading to imprecise segmentation of local anomalous parts;\n2) SAM tends to generate numerous redundant masks without proper prompt\nconstraints, resulting in complex post-processing requirements. In this work,\nwe innovatively propose a CLIP and SAM collaboration framework called ClipSAM\nfor ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding\ncapability for anomaly localization and rough segmentation, which is further\nused as the prompt constraints for SAM to refine the anomaly segmentation\nresults. In details, we introduce a crucial Unified Multi-scale Cross-modal\nInteraction (UMCI) module for interacting language with visual features at\nmultiple scales of CLIP to reason anomaly positions. Then, we design a novel\nMulti-level Mask Refinement (MMR) module, which utilizes the positional\ninformation as multi-level prompts for SAM to acquire hierarchical levels of\nmasks and merges them. Extensive experiments validate the effectiveness of our\napproach, achieving the optimal segmentation performance on the MVTec-AD and\nVisA datasets.",
      "tldr_zh": "该研究提出ClipSAM框架，通过CLIP和SAM的协作来解决Zero-Shot Anomaly Segmentation (ZSAS)中的问题，即CLIP的全局特征对齐导致局部异常分割不精确，以及SAM生成冗余掩码需要复杂后处理。框架的核心是利用CLIP的语义理解能力进行异常定位和粗糙分割，然后通过Unified Multi-scale Cross-modal Interaction (UMCI)模块在多尺度上交互语言和视觉特征，以及Multi-level Mask Refinement (MMR)模块将位置信息作为多级提示给SAM进行分层掩码细化和合并。实验结果显示，ClipSAM在MVTec-AD和VisA数据集上实现了最佳的分割性能，验证了该方法的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages,17 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.12665v2",
      "published_date": "2024-01-23 11:20:03 UTC",
      "updated_date": "2024-01-29 10:57:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:49:34.630691"
    },
    {
      "arxiv_id": "2401.12662v1",
      "title": "Integrating Human Expertise in Continuous Spaces: A Novel Interactive Bayesian Optimization Framework with Preference Expected Improvement",
      "title_zh": "翻译失败",
      "authors": [
        "Nikolaus Feith",
        "Elmar Rueckert"
      ],
      "abstract": "Interactive Machine Learning (IML) seeks to integrate human expertise into\nmachine learning processes. However, most existing algorithms cannot be applied\nto Realworld Scenarios because their state spaces and/or action spaces are\nlimited to discrete values. Furthermore, the interaction of all existing\nmethods is restricted to deciding between multiple proposals. We therefore\npropose a novel framework based on Bayesian Optimization (BO). Interactive\nBayesian Optimization (IBO) enables collaboration between machine learning\nalgorithms and humans. This framework captures user preferences and provides an\ninterface for users to shape the strategy by hand. Additionally, we've\nincorporated a new acquisition function, Preference Expected Improvement (PEI),\nto refine the system's efficiency using a probabilistic model of the user\npreferences. Our approach is geared towards ensuring that machines can benefit\nfrom human expertise, aiming for a more aligned and effective learning process.\nIn the course of this work, we applied our method to simulations and in a real\nworld task using a Franka Panda robot to show human-robot collaboration.",
      "tldr_zh": "该研究针对交互式机器学习（Interactive Machine Learning, IML）的局限性，提出了一种新型框架Interactive Bayesian Optimization (IBO)，旨在将人类专业知识整合到连续空间中的机器学习过程。IBO框架通过捕捉用户偏好并引入新的acquisition function Preference Expected Improvement (PEI)，利用Bayesian Optimization (BO)来优化策略，实现更高效的人机协作。实验结果显示，该方法在模拟环境和真实世界任务（如使用Franka Panda机器人）中有效提升了学习过程的准确性和适应性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12662v1",
      "published_date": "2024-01-23 11:14:59 UTC",
      "updated_date": "2024-01-23 11:14:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:49:45.211909"
    },
    {
      "arxiv_id": "2401.12646v1",
      "title": "Emergent Cooperation under Uncertain Incentive Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Nicole Orzan",
        "Erman Acar",
        "Davide Grossi",
        "Roxana Rădulescu"
      ],
      "abstract": "Understanding the emergence of cooperation in systems of computational agents\nis crucial for the development of effective cooperative AI. Interaction among\nindividuals in real-world settings are often sparse and occur within a broad\nspectrum of incentives, which often are only partially known. In this work, we\nexplore how cooperation can arise among reinforcement learning agents in\nscenarios characterised by infrequent encounters, and where agents face\nuncertainty about the alignment of their incentives with those of others. To do\nso, we train the agents under a wide spectrum of environments ranging from\nfully competitive, to fully cooperative, to mixed-motives. Under this type of\nuncertainty we study the effects of mechanisms, such as reputation and\nintrinsic rewards, that have been proposed in the literature to foster\ncooperation in mixed-motives environments. Our findings show that uncertainty\nsubstantially lowers the agents' ability to engage in cooperative behaviour,\nwhen that would be the best course of action. In this scenario, the use of\neffective reputation mechanisms and intrinsic rewards boosts the agents'\ncapability to act nearly-optimally in cooperative environments, while greatly\nenhancing cooperation in mixed-motive environments as well.",
      "tldr_zh": "本研究探讨了在不确定激励对齐条件下，强化学习代理如何实现合作行为的涌现，这对合作 AI 的发展至关重要。研究者训练代理在从完全竞争到完全合作的环境中进行互动，考察了声誉机制和 intrinsic rewards 等机制对混合动机场景的影响。结果表明，不确定性显著降低了代理参与合作的能力，但在应用这些机制后，代理在合作环境中几乎能实现最优行为，并在混合动机环境中大幅提升合作水平。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12646v1",
      "published_date": "2024-01-23 10:55:54 UTC",
      "updated_date": "2024-01-23 10:55:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:49:58.544726"
    },
    {
      "arxiv_id": "2401.12632v1",
      "title": "Modeling Resilience of Collaborative AI Systems",
      "title_zh": "协作人工智能系统的弹性建模",
      "authors": [
        "Diaeddin Rimawi",
        "Antonio Liotta",
        "Marco Todescato",
        "Barbara Russo"
      ],
      "abstract": "A Collaborative Artificial Intelligence System (CAIS) performs actions in\ncollaboration with the human to achieve a common goal. CAISs can use a trained\nAI model to control human-system interaction, or they can use human interaction\nto dynamically learn from humans in an online fashion. In online learning with\nhuman feedback, the AI model evolves by monitoring human interaction through\nthe system sensors in the learning state, and actuates the autonomous\ncomponents of the CAIS based on the learning in the operational state.\nTherefore, any disruptive event affecting these sensors may affect the AI\nmodel's ability to make accurate decisions and degrade the CAIS performance.\nConsequently, it is of paramount importance for CAIS managers to be able to\nautomatically track the system performance to understand the resilience of the\nCAIS upon such disruptive events. In this paper, we provide a new framework to\nmodel CAIS performance when the system experiences a disruptive event. With our\nframework, we introduce a model of performance evolution of CAIS. The model is\nequipped with a set of measures that aim to support CAIS managers in the\ndecision process to achieve the required resilience of the system. We tested\nour framework on a real-world case study of a robot collaborating online with\nthe human, when the system is experiencing a disruptive event. The case study\nshows that our framework can be adopted in CAIS and integrated into the online\nexecution of the CAIS activities.",
      "tldr_zh": "这篇论文提出一个框架，用于建模协作人工智能系统(CAIS)的弹性，CAIS通过与人类合作实现共同目标，但可能因破坏性事件影响传感器而降低性能。框架包括一个性能演化模型和一组决策支持措施，帮助CAIS管理者跟踪系统表现并提升弹性。实验通过一个真实案例研究（机器人在线与人类协作）验证了框架的有效性，并证明其可集成到CAIS的在线执行中。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.SE",
      "comment": "This paper is accepted at the 3rd International Conference on AI\n  Engineering - Software Engineering for AI (CAIN 2024), Lisbon, Portugal",
      "pdf_url": "http://arxiv.org/pdf/2401.12632v1",
      "published_date": "2024-01-23 10:28:33 UTC",
      "updated_date": "2024-01-23 10:28:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:50:09.839818"
    },
    {
      "arxiv_id": "2401.12631v1",
      "title": "A Reply to Makelov et al. (2023)'s \"Interpretability Illusion\" Arguments",
      "title_zh": "对 Makelov et al. (2023) 的 “Interpretability Illusion” 论点的回复",
      "authors": [
        "Zhengxuan Wu",
        "Atticus Geiger",
        "Jing Huang",
        "Aryaman Arora",
        "Thomas Icard",
        "Christopher Potts",
        "Noah D. Goodman"
      ],
      "abstract": "We respond to the recent paper by Makelov et al. (2023), which reviews\nsubspace interchange intervention methods like distributed alignment search\n(DAS; Geiger et al. 2023) and claims that these methods potentially cause\n\"interpretability illusions\". We first review Makelov et al. (2023)'s technical\nnotion of what an \"interpretability illusion\" is, and then we show that even\nintuitive and desirable explanations can qualify as illusions in this sense. As\na result, their method of discovering \"illusions\" can reject explanations they\nconsider \"non-illusory\". We then argue that the illusions Makelov et al. (2023)\nsee in practice are artifacts of their training and evaluation paradigms. We\nclose by emphasizing that, though we disagree with their core characterization,\nMakelov et al. (2023)'s examples and discussion have undoubtedly pushed the\nfield of interpretability forward.",
      "tldr_zh": "这篇论文回应了 Makelov et al. (2023) 关于“interpretability illusions”的论点，针对子空间互换干预方法（如 distributed alignment search, DAS）可能导致解释性幻觉的指控。作者首先审视了“interpretability illusions”的技术定义，并证明即使直观且可取的解释也可能被视为幻觉，从而质疑该方法可能错误拒绝非幻觉解释。最终，作者认为这些幻觉是 Makelov et al. 的训练和评估范式所致，并承认他们的讨论推动了解释性领域的前进，尽管在核心观点上存在分歧。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.12631v1",
      "published_date": "2024-01-23 10:27:42 UTC",
      "updated_date": "2024-01-23 10:27:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:50:22.601059"
    },
    {
      "arxiv_id": "2401.12624v2",
      "title": "Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control",
      "title_zh": "翻译失败",
      "authors": [
        "Yongjun Kim",
        "Sejin Seo",
        "Jihong Park",
        "Mehdi Bennis",
        "Seong-Lyun Kim",
        "Junil Choi"
      ],
      "abstract": "In this work, we compare emergent communication (EC) built upon multi-agent\ndeep reinforcement learning (MADRL) and language-oriented semantic\ncommunication (LSC) empowered by a pre-trained large language model (LLM) using\nhuman language. In a multi-agent remote navigation task, with multimodal input\ndata comprising location and channel maps, it is shown that EC incurs high\ntraining cost and struggles when using multimodal data, whereas LSC yields high\ninference computing cost due to the LLM's large size. To address their\nrespective bottlenecks, we propose a novel framework of language-guided EC\n(LEC) by guiding the EC training using LSC via knowledge distillation (KD).\nSimulations corroborate that LEC achieves faster travel time while avoiding\nareas with poor channel conditions, as well as speeding up the MADRL training\nconvergence by up to 61.8% compared to EC.",
      "tldr_zh": "本研究比较了基于多智能体深度强化学习（MADRL）的紧急通信（EC）和基于预训练大语言模型（LLM）的语言导向语义通信（LSC），发现EC训练成本高且处理多模态数据（如位置和通道地图）困难，而LSC推理计算成本高。为解决这些问题，提出了一种语言引导的EC（LEC）框架，通过知识蒸馏（KD）从LSC指导EC训练。模拟结果显示，LEC实现了更快的旅行时间、避免了通道条件差的区域，并将MADRL训练收敛速度提高至多61.8%。",
      "categories": [
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "cs.NI",
        "math.IT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12624v2",
      "published_date": "2024-01-23 10:23:13 UTC",
      "updated_date": "2024-03-03 14:15:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:50:34.525221"
    },
    {
      "arxiv_id": "2402.16869v1",
      "title": "Considering Fundamental Rights in the European Standardisation of Artificial Intelligence: Nonsense or Strategic Alliance?",
      "title_zh": "翻译失败",
      "authors": [
        "Marion Ho-Dac"
      ],
      "abstract": "In the European context, both the EU AI Act proposal and the draft\nStandardisation Request on safe and trustworthy AI link standardisation to\nfundamental rights. However, these texts do not provide any guidelines that\nspecify and detail the relationship between AI standards and fundamental\nrights, its meaning or implication. This chapter aims to clarify this critical\nregulatory blind spot. The main issue tackled is whether the adoption of AI\nharmonised standards, based on the future AI Act, should take into account\nfundamental rights. In our view, the response is yes. The high risks posed by\ncertain AI systems relate in particular to infringements of fundamental rights.\nTherefore, mitigating such risks involves fundamental rights considerations and\nthis is what future harmonised standards should reflect. At the same time,\nvalid criticisms of the European standardisation process have to be addressed.\nFinally, the practical incorporation of fundamental rights considerations in\nthe ongoing European standardisation of AI systems is discussed.",
      "tldr_zh": "本研究探讨了欧盟 AI Act 提案和 AI 标准化请求中，将标准化与基本权利(fundamental rights)联系起来的问题，并指出现有文本缺乏具体指导。该章节认为，AI 协调标准(harmonised standards)应考虑基本权利，因为高风险 AI 系统可能侵犯这些权利，从而通过缓解风险来提升 AI 的安全性和可信度。同时，研究强调需解决欧洲标准化过程的批评，并讨论如何在实践中将基本权利考虑融入 AI 系统标准化中。总的来说，这为欧盟 AI 监管框架提供了战略性建议，促进标准化与基本权利的联盟。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.16869v1",
      "published_date": "2024-01-23 10:17:42 UTC",
      "updated_date": "2024-01-23 10:17:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:50:45.665574"
    },
    {
      "arxiv_id": "2401.12599v1",
      "title": "Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Demiao Lin"
      ],
      "abstract": "With the rapid development of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) has become a predominant method in the\nfield of professional knowledge-based question answering. Presently, major\nfoundation model companies have opened up Embedding and Chat API interfaces,\nand frameworks like LangChain have already integrated the RAG process. It\nappears that the key models and steps in RAG have been resolved, leading to the\nquestion: are professional knowledge QA systems now approaching perfection?\nThis article discovers that current primary methods depend on the premise of\naccessing high-quality text corpora. However, since professional documents are\nmainly stored in PDFs, the low accuracy of PDF parsing significantly impacts\nthe effectiveness of professional knowledge-based QA. We conducted an empirical\nRAG experiment across hundreds of questions from the corresponding real-world\nprofessional documents. The results show that, ChatDOC, a RAG system equipped\nwith a panoptic and pinpoint PDF parser, retrieves more accurate and complete\nsegments, and thus better answers. Empirical experiments show that ChatDOC is\nsuperior to baseline on nearly 47% of questions, ties for 38% of cases, and\nfalls short on only 15% of cases. It shows that we may revolutionize RAG with\nenhanced PDF structure recognition.",
      "tldr_zh": "该研究指出，Retrieval-Augmented Generation (RAG) 在专业知识问答中依赖高质量文本语料，但PDF文档的低准确率解析问题导致了系统效能低下。作者提出ChatDOC系统，使用全景和精确的PDF解析器（panoptic and pinpoint PDF parser）来检索更准确和完整的文本段落，从而提升RAG的表现。在实证实验中，ChatDOC在47%的问答问题上优于基线模型，38%持平，仅15%落后，这表明增强PDF结构识别可革命化RAG系统。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.12599v1",
      "published_date": "2024-01-23 09:54:36 UTC",
      "updated_date": "2024-01-23 09:54:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:50:58.699464"
    },
    {
      "arxiv_id": "2401.12593v1",
      "title": "MOReGIn: Multi-Objective Recommendation at the Global and Individual Levels",
      "title_zh": "MOReGIn：多目标推荐在全局和个体层面",
      "authors": [
        "Elizabeth Gómez",
        "David Contreras",
        "Ludovico Boratto",
        "Maria Salamó"
      ],
      "abstract": "Multi-Objective Recommender Systems (MORSs) emerged as a paradigm to\nguarantee multiple (often conflicting) goals. Besides accuracy, a MORS can\noperate at the global level, where additional beyond-accuracy goals are met for\nthe system as a whole, or at the individual level, meaning that the\nrecommendations are tailored to the needs of each user. The state-of-the-art\nMORSs either operate at the global or individual level, without assuming the\nco-existence of the two perspectives. In this study, we show that when global\nand individual objectives co-exist, MORSs are not able to meet both types of\ngoals. To overcome this issue, we present an approach that regulates the\nrecommendation lists so as to guarantee both global and individual\nperspectives, while preserving its effectiveness. Specifically, as individual\nperspective, we tackle genre calibration and, as global perspective, provider\nfairness. We validate our approach on two real-world datasets, publicly\nreleased with this paper.",
      "tldr_zh": "本文研究了多目标推荐系统 (MORSs)，发现现有系统无法同时满足全局层面（如系统整体的额外目标，如 provider fairness）和个体层面（如针对用户的个性化目标，如 genre calibration）的需求。论文提出 MOReGIn 框架，通过调节推荐列表来平衡这些冲突目标，同时保持推荐的准确性和有效性。具体而言，该方法在两个真实数据集上进行了验证，证明了其在兼顾全局公平性和个体校准方面的优越性，并公开了这些数据集以促进进一步研究。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12593v1",
      "published_date": "2024-01-23 09:48:08 UTC",
      "updated_date": "2024-01-23 09:48:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:51:11.535893"
    },
    {
      "arxiv_id": "2401.12576v2",
      "title": "LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools and Self-Explanations",
      "title_zh": "LLMCheckup：通过可解释性工具和自我解释进行的大型语言模型对话式检查",
      "authors": [
        "Qianli Wang",
        "Tatiana Anikina",
        "Nils Feldhus",
        "Josef van Genabith",
        "Leonhard Hennig",
        "Sebastian Möller"
      ],
      "abstract": "Interpretability tools that offer explanations in the form of a dialogue have\ndemonstrated their efficacy in enhancing users' understanding (Slack et al.,\n2023; Shen et al., 2023), as one-off explanations may fall short in providing\nsufficient information to the user. Current solutions for dialogue-based\nexplanations, however, often require external tools and modules and are not\neasily transferable to tasks they were not designed for. With LLMCheckup, we\npresent an easily accessible tool that allows users to chat with any\nstate-of-the-art large language model (LLM) about its behavior. We enable LLMs\nto generate explanations and perform user intent recognition without\nfine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI)\nmethods, including white-box explainability tools such as feature attributions,\nand self-explanations (e.g., for rationale generation). LLM-based\n(self-)explanations are presented as an interactive dialogue that supports\nfollow-up questions and generates suggestions. LLMCheckupprovides tutorials for\noperations available in the system, catering to individuals with varying levels\nof expertise in XAI and supporting multiple input modalities. We introduce a\nnew parsing strategy that substantially enhances the user intent recognition\naccuracy of the LLM. Finally, we showcase LLMCheckup for the tasks of fact\nchecking and commonsense question answering.",
      "tldr_zh": "该研究引入了LLMCheckup，一种易于访问的工具，允许用户通过对话形式与任何前沿大型语言模型(LLM)讨论其行为，从而提升解释性。LLMCheckup整合了多种Explainable AI (XAI)方法，如特征归因和自解释，无需微调LLM即可生成交互式解释，支持后续问题、建议和教程，适用于不同XAI专业水平和输入模式。研究提出了一种新解析策略，大幅提高了用户意图识别准确性，并在事实检查和常识问题回答任务上展示了其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2024 HCI+NLP workshop; camera-ready version",
      "pdf_url": "http://arxiv.org/pdf/2401.12576v2",
      "published_date": "2024-01-23 09:11:07 UTC",
      "updated_date": "2024-04-24 17:17:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:51:22.070564"
    },
    {
      "arxiv_id": "2401.12570v1",
      "title": "DiffMoog: a Differentiable Modular Synthesizer for Sound Matching",
      "title_zh": "DiffMoog：用于声音匹配的可微模块化合成器",
      "authors": [
        "Noy Uzrad",
        "Oren Barkan",
        "Almog Elharar",
        "Shlomi Shvartzman",
        "Moshe Laufer",
        "Lior Wolf",
        "Noam Koenigstein"
      ],
      "abstract": "This paper presents DiffMoog - a differentiable modular synthesizer with a\ncomprehensive set of modules typically found in commercial instruments. Being\ndifferentiable, it allows integration into neural networks, enabling automated\nsound matching, to replicate a given audio input. Notably, DiffMoog facilitates\nmodulation capabilities (FM/AM), low-frequency oscillators (LFOs), filters,\nenvelope shapers, and the ability for users to create custom signal chains. We\nintroduce an open-source platform that comprises DiffMoog and an end-to-end\nsound matching framework. This framework utilizes a novel signal-chain loss and\nan encoder network that self-programs its outputs to predict DiffMoogs\nparameters based on the user-defined modular architecture. Moreover, we provide\ninsights and lessons learned towards sound matching using differentiable\nsynthesis. Combining robust sound capabilities with a holistic platform,\nDiffMoog stands as a premier asset for expediting research in audio synthesis\nand machine learning.",
      "tldr_zh": "本论文介绍了DiffMoog，一种可微分模块合成器，包含FM/AM调制、LFOs、低频振荡器、filters、envelope shapers等常见模块，并允许用户创建自定义信号链，以实现神经网络中的自动声音匹配。研究提出一个端到端开源平台，包括新型信号链损失和编码器网络，该网络可根据用户定义的模块架构自编程输出以预测DiffMoog的参数。总体而言，DiffMoog结合了强大的声音处理能力，提供宝贵的见解和经验教训，推动音频合成和机器学习领域的创新研究。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "5 pages, 7 figures, 1 table, Our code is released at\n  https://github.com/aisynth/diffmoog",
      "pdf_url": "http://arxiv.org/pdf/2401.12570v1",
      "published_date": "2024-01-23 08:59:21 UTC",
      "updated_date": "2024-01-23 08:59:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:51:34.710861"
    },
    {
      "arxiv_id": "2401.12557v2",
      "title": "Balancing the AI Strength of Roles in Self-Play Training with Regret Matching+",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoxi Wang"
      ],
      "abstract": "When training artificial intelligence for games encompassing multiple roles,\nthe development of a generalized model capable of controlling any character\nwithin the game presents a viable option. This strategy not only conserves\ncomputational resources and time during the training phase but also reduces\nresource requirements during deployment. training such a generalized model\noften encounters challenges related to uneven capabilities when controlling\ndifferent roles. A simple method is introduced based on Regret Matching+, which\nfacilitates a more balanced performance of strength by the model when\ncontrolling various roles.",
      "tldr_zh": "这篇论文探讨了在多角色游戏中训练 AI 时，使用通用模型控制不同角色的挑战，因为模型往往在角色能力上表现不均衡。作者引入了一种基于 Regret Matching+ 的简单方法，通过自玩训练(self-play)优化过程来平衡模型对各角色的性能。实验结果表明，此方法不仅节省了训练和部署的计算资源，还提升了模型的整体适用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12557v2",
      "published_date": "2024-01-23 08:27:38 UTC",
      "updated_date": "2024-02-01 03:22:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:51:45.863742"
    },
    {
      "arxiv_id": "2401.12554v3",
      "title": "Can Large Language Models Write Parallel Code?",
      "title_zh": "大型语言模型能编写并行代码吗？",
      "authors": [
        "Daniel Nichols",
        "Joshua H. Davis",
        "Zhaojun Xie",
        "Arjun Rajaram",
        "Abhinav Bhatele"
      ],
      "abstract": "Large language models are increasingly becoming a popular tool for software\ndevelopment. Their ability to model and generate source code has been\ndemonstrated in a variety of contexts, including code completion,\nsummarization, translation, and lookup. However, they often struggle to\ngenerate code for complex programs. In this paper, we study the capabilities of\nstate-of-the-art language models to generate parallel code. In order to\nevaluate language models, we create a benchmark, ParEval, consisting of prompts\nthat represent 420 different coding tasks related to scientific and parallel\ncomputing. We use ParEval to evaluate the effectiveness of several\nstate-of-the-art open- and closed-source language models on these tasks. We\nintroduce novel metrics for evaluating the performance of generated code, and\nuse them to explore how well each large language model performs for 12\ndifferent computational problem types and six different parallel programming\nmodels.",
      "tldr_zh": "这篇论文探讨了大型语言模型（Large Language Models）生成并行代码（parallel code）的能力，特别是在处理复杂程序时的挑战。研究者创建了ParEval基准测试，包含420个与科学和并行计算相关的编码任务，用于评估多种最先进的开源和闭源语言模型。论文引入了新指标，评估模型在12种计算问题类型和6种并行编程模型上的性能，结果显示这些模型在生成并行代码方面存在局限，但为进一步改进提供了见解。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12554v3",
      "published_date": "2024-01-23 08:25:12 UTC",
      "updated_date": "2024-05-14 15:07:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:52:00.756961"
    },
    {
      "arxiv_id": "2401.12550v1",
      "title": "UR4NNV: Neural Network Verification, Under-approximation Reachability Works!",
      "title_zh": "翻译失败",
      "authors": [
        "Zhen Liang",
        "Taoran Wu",
        "Ran Zhao",
        "Bai Xue",
        "Ji Wang",
        "Wenjing Yang",
        "Shaojun Deng",
        "Wanwei Liu"
      ],
      "abstract": "Recently, formal verification of deep neural networks (DNNs) has garnered\nconsiderable attention, and over-approximation based methods have become\npopular due to their effectiveness and efficiency. However, these strategies\nface challenges in addressing the \"unknown dilemma\" concerning whether the\nexact output region or the introduced approximation error violates the property\nin question. To address this, this paper introduces the UR4NNV verification\nframework, which utilizes under-approximation reachability analysis for DNN\nverification for the first time. UR4NNV focuses on DNNs with Rectified Linear\nUnit (ReLU) activations and employs a binary tree branch-based\nunder-approximation algorithm. In each epoch, UR4NNV under-approximates a\nsub-polytope of the reachable set and verifies this polytope against the given\nproperty. Through a trial-and-error approach, UR4NNV effectively falsifies DNN\nproperties while providing confidence levels when reaching verification epoch\nbounds and failing falsifying properties. Experimental comparisons with\nexisting verification methods demonstrate the effectiveness and efficiency of\nUR4NNV, significantly reducing the impact of the \"unknown dilemma\".",
      "tldr_zh": "该论文提出UR4NNV框架，这是首次使用under-approximation reachability analysis来验证深度神经网络(DNNs)，以解决传统over-approximation方法面临的“unknown dilemma”问题，即无法区分精确输出区域与近似误差是否违反属性。UR4NNV针对ReLU激活函数的DNNs，采用二进制树分支-based under-approximation算法，在每个epoch中对可达集的sub-polytope进行下近似验证，并通过trial-and-error方式有效falsify属性，同时提供置信水平。实验结果显示，UR4NNV相较现有方法显著提高了验证效率和有效性，减少了“unknown dilemma”的影响。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "68Q60, 68T07",
        "D.2.4; I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.12550v1",
      "published_date": "2024-01-23 08:19:00 UTC",
      "updated_date": "2024-01-23 08:19:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:52:10.962734"
    },
    {
      "arxiv_id": "2401.12533v3",
      "title": "Near-Optimal Algorithms for Constrained k-Center Clustering with Instance-level Background Knowledge",
      "title_zh": "带有实例级别背景知识的约束 k-中心聚类的近似最优算法",
      "authors": [
        "Longkun Guo",
        "Chaoqi Jia",
        "Kewen Liao",
        "Zhigang Lu",
        "Minhui Xue"
      ],
      "abstract": "Center-based clustering has attracted significant research interest from both\ntheory and practice. In many practical applications, input data often contain\nbackground knowledge that can be used to improve clustering results. In this\nwork, we build on widely adopted $k$-center clustering and model its input\nbackground knowledge as must-link (ML) and cannot-link (CL) constraint sets.\nHowever, most clustering problems including $k$-center are inherently\n$\\mathcal{NP}$-hard, while the more complex constrained variants are known to\nsuffer severer approximation and computation barriers that significantly limit\ntheir applicability. By employing a suite of techniques including reverse\ndominating sets, linear programming (LP) integral polyhedron, and LP duality,\nwe arrive at the first efficient approximation algorithm for constrained\n$k$-center with the best possible ratio of 2. We also construct competitive\nbaseline algorithms and empirically evaluate our approximation algorithm\nagainst them on a variety of real datasets. The results validate our\ntheoretical findings and demonstrate the great advantages of our algorithm in\nterms of clustering cost, clustering quality, and running time.",
      "tldr_zh": "该论文针对k-center clustering问题，引入实例级背景知识的must-link (ML)和cannot-link (CL)约束，以提升聚类结果。该方法利用reverse dominating sets、linear programming (LP) integral polyhedron和LP duality等技术，开发了首个高效近似算法，实现了最佳的2倍近似比。实验结果显示，该算法在真实数据集上优于竞争基线，在聚类成本、质量和运行时间方面表现出显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12533v3",
      "published_date": "2024-01-23 07:16:32 UTC",
      "updated_date": "2024-05-15 01:42:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:52:23.196774"
    },
    {
      "arxiv_id": "2401.12532v1",
      "title": "DAFA: Distance-Aware Fair Adversarial Training",
      "title_zh": "DAFA：距离感知公平对抗训练",
      "authors": [
        "Hyungyu Lee",
        "Saehyung Lee",
        "Hyemi Jang",
        "Junsung Park",
        "Ho Bae",
        "Sungroh Yoon"
      ],
      "abstract": "The disparity in accuracy between classes in standard training is amplified\nduring adversarial training, a phenomenon termed the robust fairness problem.\nExisting methodologies aimed to enhance robust fairness by sacrificing the\nmodel's performance on easier classes in order to improve its performance on\nharder ones. However, we observe that under adversarial attacks, the majority\nof the model's predictions for samples from the worst class are biased towards\nclasses similar to the worst class, rather than towards the easy classes.\nThrough theoretical and empirical analysis, we demonstrate that robust fairness\ndeteriorates as the distance between classes decreases. Motivated by these\ninsights, we introduce the Distance-Aware Fair Adversarial training (DAFA)\nmethodology, which addresses robust fairness by taking into account the\nsimilarities between classes. Specifically, our method assigns distinct loss\nweights and adversarial margins to each class and adjusts them to encourage a\ntrade-off in robustness among similar classes. Experimental results across\nvarious datasets demonstrate that our method not only maintains average robust\naccuracy but also significantly improves the worst robust accuracy, indicating\na marked improvement in robust fairness compared to existing methods.",
      "tldr_zh": "本研究针对对抗训练（adversarial training）中存在的鲁棒公平性问题（robust fairness problem），发现模型在攻击下对最差类别的预测往往偏向类似类别，而非易类别，并通过理论和实证分析证明类间距离减小会加剧这一问题。作者提出DAFA（Distance-Aware Fair Adversarial Training）方法，该方法根据类间相似性为每个类别分配不同的损失权重和对抗边距（adversarial margins），以平衡类似类别的鲁棒性权衡。实验结果显示，DAFA在多种数据集上维持了平均鲁棒准确率，同时显著提升了最差鲁棒准确率，从而大幅改善了鲁棒公平性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.12532v1",
      "published_date": "2024-01-23 07:15:47 UTC",
      "updated_date": "2024-01-23 07:15:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:52:35.911299"
    },
    {
      "arxiv_id": "2401.12522v2",
      "title": "BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Feng Lin",
        "Hanling Yi",
        "Hongbin Li",
        "Yifan Yang",
        "Xiaotian Yu",
        "Guangming Lu",
        "Rong Xiao"
      ],
      "abstract": "Large language models (LLMs) commonly employ autoregressive generation during\ninference, leading to high memory bandwidth demand and consequently extended\nlatency. To mitigate this inefficiency, we present Bi-directional Tuning for\nlossless Acceleration (BiTA), an innovative method expediting LLMs via\nstreamlined semi-autoregressive generation and draft verification. Inspired by\nthe concept of prompt tuning, we enhance LLMs with a parameter-efficient design\ncalled bi-directional tuning for the capability in semi-autoregressive\ngeneration. Employing efficient tree-based decoding, the models perform draft\ncandidate generation and verification in parallel, ensuring outputs identical\nto their autoregressive counterparts under greedy sampling. BiTA serves as a\nlightweight plug-in module, seamlessly boosting the inference efficiency of\nexisting LLMs without requiring additional assistance models or incurring\nsignificant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat\nachieves a 2.7$\\times$ speedup on the MT-Bench benchmark. Extensive experiments\nconfirm our method surpasses state-of-the-art acceleration techniques.",
      "tldr_zh": "该研究提出 BiTA（Bi-Directional Tuning），一种无损加速大型语言模型（LLMs）的创新方法，针对自回归生成导致的高内存带宽需求和延迟问题，通过流线化的半自回归生成和草稿验证来提升推理效率。BiTA 受提示调优（prompt tuning）启发，采用参数高效的双向调优设计和树-based 解码，实现并行生成候选草稿并验证，确保输出与自回归模型在贪婪采样下完全一致，同时作为轻量级插件模块无需额外模型或显著内存开销。在实验中，应用于 LLaMA-2-70B-Chat 模型，BiTA 在 MT-Bench 基准上实现了 2.7 倍加速，并超越了现有加速技术。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "An appendix has been included. Source code at\n  https://github.com/linfeng93/BiTA",
      "pdf_url": "http://arxiv.org/pdf/2401.12522v2",
      "published_date": "2024-01-23 06:36:49 UTC",
      "updated_date": "2024-01-25 14:02:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:52:48.573923"
    },
    {
      "arxiv_id": "2401.13006v1",
      "title": "CIMGEN: Controlled Image Manipulation by Finetuning Pretrained Generative Models on Limited Data",
      "title_zh": "翻译失败",
      "authors": [
        "Chandrakanth Gudavalli",
        "Erik Rosten",
        "Lakshmanan Nataraj",
        "Shivkumar Chandrasekaran",
        "B. S. Manjunath"
      ],
      "abstract": "Content creation and image editing can benefit from flexible user controls. A\ncommon intermediate representation for conditional image generation is a\nsemantic map, that has information of objects present in the image. When\ncompared to raw RGB pixels, the modification of semantic map is much easier.\nOne can take a semantic map and easily modify the map to selectively insert,\nremove, or replace objects in the map. The method proposed in this paper takes\nin the modified semantic map and alter the original image in accordance to the\nmodified map. The method leverages traditional pre-trained image-to-image\ntranslation GANs, such as CycleGAN or Pix2Pix GAN, that are fine-tuned on a\nlimited dataset of reference images associated with the semantic maps. We\ndiscuss the qualitative and quantitative performance of our technique to\nillustrate its capacity and possible applications in the fields of image\nforgery and image editing. We also demonstrate the effectiveness of the\nproposed image forgery technique in thwarting the numerous deep learning-based\nimage forensic techniques, highlighting the urgent need to develop robust and\ngeneralizable image forensic tools in the fight against the spread of fake\nmedia.",
      "tldr_zh": "本文提出CIMGEN方法，通过在有限数据集上微调预训练的生成模型（如CycleGAN或Pix2Pix GAN），实现基于semantic map的控制图像操作，用户可以轻松修改semantic map以插入、移除或替换图像中的对象。相比直接编辑RGB像素，这种方法更灵活高效，并在图像编辑和伪造领域展示了出色的定性和定量性能。实验结果表明，CIMGEN能有效规避现有深度学习图像取证技术，突出了开发更鲁棒的图像取证工具以对抗假媒体传播的紧迫需求。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.13006v1",
      "published_date": "2024-01-23 06:30:47 UTC",
      "updated_date": "2024-01-23 06:30:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:53:01.553867"
    },
    {
      "arxiv_id": "2401.12513v2",
      "title": "Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT and SimCLR",
      "title_zh": "翻译失败",
      "authors": [
        "Robert Turnbull",
        "Evelyn Mannix"
      ],
      "abstract": "Purpose: The capacity to isolate and recognize individual characters from\nfacsimile images of papyrus manuscripts yields rich opportunities for digital\nanalysis. For this reason the `ICDAR 2023 Competition on Detection and\nRecognition of Greek Letters on Papyri' was held as part of the 17th\nInternational Conference on Document Analysis and Recognition. This paper\ndiscusses our submission to the competition.\n  Methods: We used an ensemble of YOLOv8 models to detect and classify\nindividual characters and employed two different approaches for refining the\ncharacter predictions, including a transformer based DeiT approach and a\nResNet-50 model trained on a large corpus of unlabelled data using SimCLR, a\nself-supervised learning method.\n  Results: Our submission won the recognition challenge with a mAP of 42.2%,\nand was runner-up in the detection challenge with a mean average precision\n(mAP) of 51.4%. At the more relaxed intersection over union threshold of 0.5,\nwe achieved the highest mean average precision and mean average recall results\nfor both detection and classification.\n  Conclusion: The results demonstrate the potential for these techniques for\nautomated character recognition on historical manuscripts. We ran the\nprediction pipeline on more than 4,500 images from the Oxyrhynchus Papyri to\nillustrate the utility of our approach, and we release the results publicly in\nmultiple formats.",
      "tldr_zh": "这篇论文介绍了使用 YOLOv8、DeiT 和 SimCLR 的方法来检测和识别希腊纸莎草纸手稿中的字符，旨在支持数字分析并参与 ICDAR 2023 比赛。方法包括 YOLOv8 模型的集成用于字符检测和分类，随后通过基于 transformer 的 DeiT 以及 SimCLR 自监督学习在 ResNet-50 上训练的模型来完善预测。结果显示，他们在识别挑战中以 mAP 42.2% 获胜，在检测挑战中以 mAP 51.4% 排名第二，尤其在 IoU 0.5 阈值下取得了最高的 mAP 和 mAR。论文证明了这些技术在历史手稿字符识别中的潜力，并通过在超过 4,500 张 Oxyrhynchus Papyri 图像上应用并公开结果，展示了其实用价值。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T10"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12513v2",
      "published_date": "2024-01-23 06:08:00 UTC",
      "updated_date": "2024-02-14 01:40:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:53:15.463697"
    },
    {
      "arxiv_id": "2401.12497v1",
      "title": "Building Minimal and Reusable Causal State Abstractions for Reinforcement Learning",
      "title_zh": "构建最小且可重用的因果状态抽象用于强化学习",
      "authors": [
        "Zizhao Wang",
        "Caroline Wang",
        "Xuesu Xiao",
        "Yuke Zhu",
        "Peter Stone"
      ],
      "abstract": "Two desiderata of reinforcement learning (RL) algorithms are the ability to\nlearn from relatively little experience and the ability to learn policies that\ngeneralize to a range of problem specifications. In factored state spaces, one\napproach towards achieving both goals is to learn state abstractions, which\nonly keep the necessary variables for learning the tasks at hand. This paper\nintroduces Causal Bisimulation Modeling (CBM), a method that learns the causal\nrelationships in the dynamics and reward functions for each task to derive a\nminimal, task-specific abstraction. CBM leverages and improves implicit\nmodeling to train a high-fidelity causal dynamics model that can be reused for\nall tasks in the same environment. Empirical validation on manipulation\nenvironments and Deepmind Control Suite reveals that CBM's learned implicit\ndynamics models identify the underlying causal relationships and state\nabstractions more accurately than explicit ones. Furthermore, the derived state\nabstractions allow a task learner to achieve near-oracle levels of sample\nefficiency and outperform baselines on all tasks.",
      "tldr_zh": "本论文针对强化学习（RL）算法，提出一种方法来实现从少量经验中学习并泛化到多种任务规格，重点是通过构建最小化、可重用的因果状态抽象（state abstractions）。Causal Bisimulation Modeling (CBM) 方法通过学习任务中动态和奖励函数的因果关系，训练一个高保真度的隐式动态模型（implicit modeling），并将其重用于同一环境中的所有任务，从而推导出任务特定的最小抽象。实验在操作环境和Deepmind Control Suite上验证，CBM 的隐式模型比显式模型更准确地识别因果关系，其派生的状态抽象使任务学习器达到接近预言机水平的样本效率，并在所有任务上优于基线。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "I.2.9; I.2.8; I.2.6"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at AAAI24",
      "pdf_url": "http://arxiv.org/pdf/2401.12497v1",
      "published_date": "2024-01-23 05:43:15 UTC",
      "updated_date": "2024-01-23 05:43:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:53:24.823123"
    },
    {
      "arxiv_id": "2401.12492v3",
      "title": "Comparing Pre-trained Human Language Models: Is it Better with Human Context as Groups, Individual Traits, or Both?",
      "title_zh": "翻译失败",
      "authors": [
        "Nikita Soni",
        "Niranjan Balasubramanian",
        "H. Andrew Schwartz",
        "Dirk Hovy"
      ],
      "abstract": "Pre-trained language models consider the context of neighboring words and\ndocuments but lack any author context of the human generating the text.\nHowever, language depends on the author's states, traits, social, situational,\nand environmental attributes, collectively referred to as human context (Soni\net al., 2024). Human-centered natural language processing requires\nincorporating human context into language models. Currently, two methods exist:\npre-training with 1) group-wise attributes (e.g., over-45-year-olds) or 2)\nindividual traits. Group attributes are simple but coarse -- not all\n45-year-olds write the same way -- while individual traits allow for more\npersonalized representations, but require more complex modeling and data. It is\nunclear which approach benefits what tasks. We compare pre-training models with\nhuman context via 1) group attributes, 2) individual users, and 3) a combined\napproach on five user- and document-level tasks. Our results show that there is\nno best approach, but that human-centered language modeling holds avenues for\ndifferent methods.",
      "tldr_zh": "该研究探讨了预训练语言模型（pre-trained language models）如何忽略作者的人类上下文（human context），并比较了三种整合人类上下文的方法：基于群组属性（如年龄组）、个体特征，或两者结合。研究者通过在五个用户级和文档级任务上预训练并评估模型，结果显示没有一种方法是最佳的，但人类中心语言建模（human-centered natural language processing）提供了多样化的潜在优势。这种方法突显了在语言模型中纳入作者属性以提升性能的可能性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12492v3",
      "published_date": "2024-01-23 05:20:35 UTC",
      "updated_date": "2024-07-18 21:57:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:53:35.710917"
    },
    {
      "arxiv_id": "2401.12491v1",
      "title": "Assessing and Understanding Creativity in Large Language Models",
      "title_zh": "评估与理解大语言模型中的创造力",
      "authors": [
        "Yunpu Zhao",
        "Rui Zhang",
        "Wenyi Li",
        "Di Huang",
        "Jiaming Guo",
        "Shaohui Peng",
        "Yifan Hao",
        "Yuanbo Wen",
        "Xing Hu",
        "Zidong Du",
        "Qi Guo",
        "Ling Li",
        "Yunji Chen"
      ],
      "abstract": "In the field of natural language processing, the rapid development of large\nlanguage model (LLM) has attracted more and more attention. LLMs have shown a\nhigh level of creativity in various tasks, but the methods for assessing such\ncreativity are inadequate. The assessment of LLM creativity needs to consider\ndifferences from humans, requiring multi-dimensional measurement while\nbalancing accuracy and efficiency. This paper aims to establish an efficient\nframework for assessing the level of creativity in LLMs. By adapting the\nmodified Torrance Tests of Creative Thinking, the research evaluates the\ncreative performance of various LLMs across 7 tasks, emphasizing 4 criteria\nincluding Fluency, Flexibility, Originality, and Elaboration. In this context,\nwe develop a comprehensive dataset of 700 questions for testing and an\nLLM-based evaluation method. In addition, this study presents a novel analysis\nof LLMs' responses to diverse prompts and role-play situations. We found that\nthe creativity of LLMs primarily falls short in originality, while excelling in\nelaboration. Besides, the use of prompts and the role-play settings of the\nmodel significantly influence creativity. Additionally, the experimental\nresults also indicate that collaboration among multiple LLMs can enhance\noriginality. Notably, our findings reveal a consensus between human evaluations\nand LLMs regarding the personality traits that influence creativity. The\nfindings underscore the significant impact of LLM design on creativity and\nbridges artificial intelligence and human creativity, offering insights into\nLLMs' creativity and potential applications.",
      "tldr_zh": "这篇论文针对大型语言模型(LLM)的创意评估问题，提出一个高效框架，通过改编Torrance Tests of Creative Thinking评估各种LLM在7个任务上的表现，聚焦Fluency、Flexibility、Originality和Elaboration四个标准，并开发了700个问题的数据集和基于LLM的评估方法。研究发现，LLMs在Originality方面表现较弱，但擅长Elaboration；提示设计和角色扮演设置会显著影响创意表现，而多个LLMs的协作能提升原创性。实验结果还显示，LLM与人类在影响创意的个性特征上存在共识，这为理解LLM设计对创意的冲击提供了洞见，并桥接了人工智能与人类创意的关联。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12491v1",
      "published_date": "2024-01-23 05:19:47 UTC",
      "updated_date": "2024-01-23 05:19:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:53:51.007929"
    },
    {
      "arxiv_id": "2401.12489v1",
      "title": "Unsupervised Learning Method for the Wave Equation Based on Finite Difference Residual Constraints Loss",
      "title_zh": "基于有限差分残差约束损失的无监督波动方程学习方法",
      "authors": [
        "Xin Feng",
        "Yi Jiang",
        "Jia-Xian Qin",
        "Lai-Ping Zhang",
        "Xiao-Gang Deng"
      ],
      "abstract": "The wave equation is an important physical partial differential equation, and\nin recent years, deep learning has shown promise in accelerating or replacing\ntraditional numerical methods for solving it. However, existing deep learning\nmethods suffer from high data acquisition costs, low training efficiency, and\ninsufficient generalization capability for boundary conditions. To address\nthese issues, this paper proposes an unsupervised learning method for the wave\nequation based on finite difference residual constraints. We construct a novel\nfinite difference residual constraint based on structured grids and finite\ndifference methods, as well as an unsupervised training strategy, enabling\nconvolutional neural networks to train without data and predict the forward\npropagation process of waves. Experimental results show that finite difference\nresidual constraints have advantages over physics-informed neural networks\n(PINNs) type physical information constraints, such as easier fitting, lower\ncomputational costs, and stronger source term generalization capability, making\nour method more efficient in training and potent in application.",
      "tldr_zh": "本文提出了一种针对 wave equation 的 unsupervised learning 方法，利用 finite difference residual constraints 作为损失函数，解决现有深度学习方法的数据获取成本高、训练效率低以及边界条件泛化能力不足的问题。该方法基于结构化网格和有限差分技术构建新型残差约束，并采用无监督训练策略，使卷积神经网络无需数据即可训练并预测波的传播过程。实验结果显示，与 PINNs 相比，该方法具有更容易拟合、更低计算成本和更强源项泛化能力的优势，从而提升了 wave equation 求解的效率和实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "in Chinese language",
      "pdf_url": "http://arxiv.org/pdf/2401.12489v1",
      "published_date": "2024-01-23 05:06:29 UTC",
      "updated_date": "2024-01-23 05:06:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:54:01.547165"
    },
    {
      "arxiv_id": "2401.12485v1",
      "title": "Adiabatic Quantum Support Vector Machines",
      "title_zh": "绝热量子支持向量机",
      "authors": [
        "Prasanna Date",
        "Dong Jun Woun",
        "Kathleen Hamilton",
        "Eduardo A. Coello Perez",
        "Mayanka Chandra Shekhar",
        "Francisco Rios",
        "John Gounley",
        "In-Saeng Suh",
        "Travis Humble",
        "Georgia Tourassi"
      ],
      "abstract": "Adiabatic quantum computers can solve difficult optimization problems (e.g.,\nthe quadratic unconstrained binary optimization problem), and they seem well\nsuited to train machine learning models. In this paper, we describe an\nadiabatic quantum approach for training support vector machines. We show that\nthe time complexity of our quantum approach is an order of magnitude better\nthan the classical approach. Next, we compare the test accuracy of our quantum\napproach against a classical approach that uses the Scikit-learn library in\nPython across five benchmark datasets (Iris, Wisconsin Breast Cancer (WBC),\nWine, Digits, and Lambeq). We show that our quantum approach obtains accuracies\non par with the classical approach. Finally, we perform a scalability study in\nwhich we compute the total training times of the quantum approach and the\nclassical approach with increasing number of features and number of data points\nin the training dataset. Our scalability results show that the quantum approach\nobtains a 3.5--4.5 times speedup over the classical approach on datasets with\nmany (millions of) features.",
      "tldr_zh": "本研究提出了一种使用绝热量子计算机（Adiabatic Quantum）训练支持向量机（Support Vector Machines）的量子方法，通过解决二次无约束二元优化问题（Quadratic Unconstrained Binary Optimization）来提升训练效率。结果显示，该量子方法的计算时间复杂度比经典方法高出一个数量级，且在Iris、Wisconsin Breast Cancer (WBC)、Wine、Digits和Lambeq等五个基准数据集上，测试准确率与使用Scikit-learn库的经典方法相当。最后，通过扩展性研究发现，在特征数量和数据点数量增加的场景下，量子方法比经典方法快3.5-4.5倍，尤其适用于百万级特征的大规模数据集。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12485v1",
      "published_date": "2024-01-23 04:50:13 UTC",
      "updated_date": "2024-01-23 04:50:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:54:11.296122"
    },
    {
      "arxiv_id": "2401.12478v2",
      "title": "Mini-batch Submodular Maximization",
      "title_zh": "翻译失败",
      "authors": [
        "Gregory Schwartzman"
      ],
      "abstract": "We present the first mini-batch algorithm for maximizing a non-negative\nmonotone decomposable submodular function, $F=\\sum_{i=1}^N f^i$, under a set of\nconstraints. We consider two sampling approaches: uniform and weighted. We\nfirst show that mini-batch with weighted sampling improves over the state of\nthe art sparsifier based approach both in theory and in practice.\n  Surprisingly, our experimental results show that uniform sampling is superior\nto weighted sampling. However, it is impossible to explain this using\nworst-case analysis. Our main contribution is using smoothed analysis to\nprovide a theoretical foundation for our experimental results. We show that,\nunder very mild assumptions, uniform sampling is superior for both the\nmini-batch and the sparsifier approaches. We empirically verify that these\nassumptions hold for our datasets. Uniform sampling is simple to implement and\nhas complexity independent of $N$, making it the perfect candidate to tackle\nmassive real-world datasets.",
      "tldr_zh": "该论文提出了第一个针对非负单调可分解子模函数（$F=\\sum_{i=1}^N f^i$）的最大化mini-batch算法，并考虑uniform sampling和weighted sampling两种方法。在理论和实践上，weighted sampling优于现有的sparsifier based方法，但实验结果显示uniform sampling表现更佳。该研究使用smoothed analysis在温和假设下证明uniform sampling的优越性，并验证其在数据集上的适用性，使其成为处理大规模数据集的理想选择，由于其简单实现和独立于$N$的复杂度。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12478v2",
      "published_date": "2024-01-23 04:16:58 UTC",
      "updated_date": "2024-10-02 09:02:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:54:26.255230"
    },
    {
      "arxiv_id": "2401.12470v1",
      "title": "Reinforcement Learning for Graph Coloring: Understanding the Power and Limits of Non-Label Invariant Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Chase Cummins",
        "Richard Veras"
      ],
      "abstract": "Register allocation is one of the most important problems for modern\ncompilers. With a practically unlimited number of user variables and a small\nnumber of CPU registers, assigning variables to registers without conflicts is\na complex task. This work demonstrates the use of casting the register\nallocation problem as a graph coloring problem. Using technologies such as\nPyTorch and OpenAI Gymnasium Environments we will show that a Proximal Policy\nOptimization model can learn to solve the graph coloring problem. We will also\nshow that the labeling of a graph is critical to the performance of the model\nby taking the matrix representation of a graph and permuting it. We then test\nthe model's effectiveness on each of these permutations and show that it is not\neffective when given a relabeling of the same graph. Our main contribution lies\nin showing the need for label reordering invariant representations of graphs\nfor machine learning models to achieve consistent performance.",
      "tldr_zh": "本研究将寄存器分配问题转化为图着色(Graph Coloring)问题，并使用强化学习(Reinforcement Learning)中的Proximal Policy Optimization (PPO)模型，通过PyTorch和OpenAI Gymnasium环境来训练模型解决该问题。实验发现，模型对图的标签高度敏感，当对图的矩阵表示进行重新排列时，模型性能显著下降。研究的主要贡献在于强调机器学习模型需要非标签不变(Non-Label Invariant)表示，以实现图着色任务的一致性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12470v1",
      "published_date": "2024-01-23 03:43:34 UTC",
      "updated_date": "2024-01-23 03:43:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:54:35.924882"
    },
    {
      "arxiv_id": "2401.12467v2",
      "title": "An open dataset for the evolution of oracle bone characters: EVOBC",
      "title_zh": "一个用于甲骨文字符演变的",
      "authors": [
        "Haisu Guan",
        "Jinpeng Wan",
        "Yuliang Liu",
        "Pengjie Wang",
        "Kaile Zhang",
        "Zhebin Kuang",
        "Xinyu Wang",
        "Xiang Bai",
        "Lianwen Jin"
      ],
      "abstract": "The earliest extant Chinese characters originate from oracle bone\ninscriptions, which are closely related to other East Asian languages. These\ninscriptions hold immense value for anthropology and archaeology. However,\ndeciphering oracle bone script remains a formidable challenge, with only\napproximately 1,600 of the over 4,500 extant characters elucidated to date.\nFurther scholarly investigation is required to comprehensively understand this\nancient writing system. Artificial Intelligence technology is a promising\navenue for deciphering oracle bone characters, particularly concerning their\nevolution. However, one of the challenges is the lack of datasets mapping the\nevolution of these characters over time. In this study, we systematically\ncollected ancient characters from authoritative texts and websites spanning six\nhistorical stages: Oracle Bone Characters - OBC (15th century B.C.), Bronze\nInscriptions - BI (13th to 221 B.C.), Seal Script - SS (11th to 8th centuries\nB.C.), Spring and Autumn period Characters - SAC (770 to 476 B.C.), Warring\nStates period Characters - WSC (475 B.C. to 221 B.C.), and Clerical Script - CS\n(221 B.C. to 220 A.D.). Subsequently, we constructed an extensive dataset,\nnamely EVolution Oracle Bone Characters (EVOBC), consisting of 229,170 images\nrepresenting 13,714 distinct character categories. We conducted validation and\nsimulated deciphering on the constructed dataset, and the results demonstrate\nits high efficacy in aiding the study of oracle bone script. This openly\naccessible dataset aims to digitalize ancient Chinese scripts across multiple\neras, facilitating the decipherment of oracle bone script by examining the\nevolution of glyph forms.",
      "tldr_zh": "该研究针对甲骨文（Oracle Bone Characters）的破译挑战，构建了一个名为EVOBC的公开数据集，以追踪其演变过程。该数据集系统收集了从六个历史阶段（包括OBC、Bronze Inscriptions (BI)、Seal Script (SS) 等）的字符图像，共计229,170张图片，涵盖13,714个独特字符类别。通过验证和模拟破译实验，结果显示EVOBC在辅助甲骨文研究中具有高功效，为数字化古代中文脚本和通过字形演变促进破译提供了宝贵资源。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12467v2",
      "published_date": "2024-01-23 03:30:47 UTC",
      "updated_date": "2024-02-13 08:21:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:54:48.267118"
    },
    {
      "arxiv_id": "2401.12461v1",
      "title": "Fast Adversarial Training against Textual Adversarial Attacks",
      "title_zh": "针对文本对抗攻击的快速对抗训练",
      "authors": [
        "Yichen Yang",
        "Xin Liu",
        "Kun He"
      ],
      "abstract": "Many adversarial defense methods have been proposed to enhance the\nadversarial robustness of natural language processing models. However, most of\nthem introduce additional pre-set linguistic knowledge and assume that the\nsynonym candidates used by attackers are accessible, which is an ideal\nassumption. We delve into adversarial training in the embedding space and\npropose a Fast Adversarial Training (FAT) method to improve the model\nrobustness in the synonym-unaware scenario from the perspective of single-step\nperturbation generation and perturbation initialization. Based on the\nobservation that the adversarial perturbations crafted by single-step and\nmulti-step gradient ascent are similar, FAT uses single-step gradient ascent to\ncraft adversarial examples in the embedding space to expedite the training\nprocess. Based on the observation that the perturbations generated on the\nidentical training sample in successive epochs are similar, FAT fully utilizes\nhistorical information when initializing the perturbation. Extensive\nexperiments demonstrate that FAT significantly boosts the robustness of BERT\nmodels in the synonym-unaware scenario, and outperforms the defense baselines\nunder various attacks with character-level and word-level modifications.",
      "tldr_zh": "该研究针对文本对抗攻击提出了一种快速对抗训练(Fast Adversarial Training, FAT)方法，以提升自然语言处理模型的鲁棒性，而无需依赖预设语言知识或攻击者同义词候选。FAT 在嵌入空间中利用单步梯度上升生成对抗样本，基于观察到单步和多步扰动相似性，从而加速训练过程；同时，通过利用历史扰动信息初始化新扰动，进一步优化效率。实验结果显示，FAT 显著增强了 BERT 模型在未知同义词场景下的防御能力，并在各种字符级和单词级攻击下优于现有基线方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.12461v1",
      "published_date": "2024-01-23 03:03:57 UTC",
      "updated_date": "2024-01-23 03:03:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:55:00.574484"
    },
    {
      "arxiv_id": "2401.12459v2",
      "title": "Towards Socially and Morally Aware RL agent: Reward Design With LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaoyue Wang"
      ],
      "abstract": "When we design and deploy an Reinforcement Learning (RL) agent, reward\nfunctions motivates agents to achieve an objective. An incorrect or incomplete\nspecification of the objective can result in behavior that does not align with\nhuman values - failing to adhere with social and moral norms that are ambiguous\nand context dependent, and cause undesired outcomes such as negative side\neffects and exploration that is unsafe. Previous work have manually defined\nreward functions to avoid negative side effects, use human oversight for safe\nexploration, or use foundation models as planning tools. This work studies the\nability of leveraging Large Language Models (LLM)' understanding of morality\nand social norms on safe exploration augmented RL methods. This work evaluates\nlanguage model's result against human feedbacks and demonstrates language\nmodel's capability as direct reward signals.",
      "tldr_zh": "这篇论文探讨了如何利用大型语言模型 (LLM) 来设计强化学习 (RL) 代理的奖励函数，以确保代理的行为符合社会和道德规范，避免负面副作用和不安全探索。研究方法包括利用 LLM 对道德与社会规范的理解来增强 RL 的安全探索机制，并将其作为直接奖励信号。论文通过与人类反馈的比较，评估了 LLM 的效果，并证明了其在提供准确奖励信号方面的能力，为开发更具社会道德意识的 RL 代理奠定了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12459v2",
      "published_date": "2024-01-23 03:00:03 UTC",
      "updated_date": "2024-05-30 20:40:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:55:13.936070"
    },
    {
      "arxiv_id": "2401.12456v1",
      "title": "Exploration and Improvement of Nerf-based 3D Scene Editing Techniques",
      "title_zh": "基于 NeRF 的 3D 场景编辑技术的探索与改进",
      "authors": [
        "Shun Fang",
        "Ming Cui",
        "Xing Feng",
        "Yanan Zhang"
      ],
      "abstract": "NeRF's high-quality scene synthesis capability was quickly accepted by\nscholars in the years after it was proposed, and significant progress has been\nmade in 3D scene representation and synthesis. However, the high computational\ncost limits intuitive and efficient editing of scenes, making NeRF's\ndevelopment in the scene editing field facing many challenges. This paper\nreviews the preliminary explorations of scholars on NeRF in the scene or object\nediting field in recent years, mainly changing the shape and texture of scenes\nor objects in new synthesized scenes; through the combination of residual\nmodels such as GaN and Transformer with NeRF, the generalization ability of\nNeRF scene editing has been further expanded, including realizing real-time new\nperspective editing feedback, multimodal editing of text synthesized 3D scenes,\n4D synthesis performance, and in-depth exploration in light and shadow editing,\ninitially achieving optimization of indirect touch editing and detail\nrepresentation in complex scenes. Currently, most NeRF editing methods focus on\nthe touch points and materials of indirect points, but when dealing with more\ncomplex or larger 3D scenes, it is difficult to balance accuracy, breadth,\nefficiency, and quality. Overcoming these challenges may become the direction\nof future NeRF 3D scene editing technology.",
      "tldr_zh": "这篇论文回顾了基于 NeRF 的 3D 场景编辑技术的初步探索和改进，主要聚焦于改变场景或对象的形状和纹理，并通过结合 GAN 和 Transformer 等残差模型提升 NeRF 的泛化能力。改进方法实现了实时新视角编辑反馈、多模态文本合成 3D 场景、4D 合成以及光影编辑的优化，初步解决了间接触点编辑和复杂场景细节表示的问题。尽管这些进展增强了编辑的准确性和效率，但当前方法在处理大型或复杂 3D 场景时仍难以平衡准确性、广度和质量。未来，克服这些挑战将成为 NeRF 3D 场景编辑技术的发展重点。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12456v1",
      "published_date": "2024-01-23 02:53:06 UTC",
      "updated_date": "2024-01-23 02:53:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:55:25.103946"
    },
    {
      "arxiv_id": "2401.12455v1",
      "title": "Multi-agent deep reinforcement learning with centralized training and decentralized execution for transportation infrastructure management",
      "title_zh": "翻译失败",
      "authors": [
        "M. Saifullah",
        "K. G. Papakonstantinou",
        "C. P. Andriotis",
        "S. M. Stoffels"
      ],
      "abstract": "We present a multi-agent Deep Reinforcement Learning (DRL) framework for\nmanaging large transportation infrastructure systems over their life-cycle.\nLife-cycle management of such engineering systems is a computationally\nintensive task, requiring appropriate sequential inspection and maintenance\ndecisions able to reduce long-term risks and costs, while dealing with\ndifferent uncertainties and constraints that lie in high-dimensional spaces. To\ndate, static age- or condition-based maintenance methods and risk-based or\nperiodic inspection plans have mostly addressed this class of optimization\nproblems. However, optimality, scalability, and uncertainty limitations are\noften manifested under such approaches. The optimization problem in this work\nis cast in the framework of constrained Partially Observable Markov Decision\nProcesses (POMDPs), which provides a comprehensive mathematical basis for\nstochastic sequential decision settings with observation uncertainties, risk\nconsiderations, and limited resources. To address significantly large state and\naction spaces, a Deep Decentralized Multi-agent Actor-Critic (DDMAC) DRL method\nwith Centralized Training and Decentralized Execution (CTDE), termed as\nDDMAC-CTDE is developed. The performance strengths of the DDMAC-CTDE method are\ndemonstrated in a generally representative and realistic example application of\nan existing transportation network in Virginia, USA. The network includes\nseveral bridge and pavement components with nonstationary degradation,\nagency-imposed constraints, and traffic delay and risk considerations. Compared\nto traditional management policies for transportation networks, the proposed\nDDMAC-CTDE method vastly outperforms its counterparts. Overall, the proposed\nalgorithmic framework provides near optimal solutions for transportation\ninfrastructure management under real-world constraints and complexities.",
      "tldr_zh": "本研究提出了一种多智能体深度强化学习（DRL）框架，用于管理交通基础设施的生命周期，旨在解决传统维护和检验方法的优化性、可扩展性和不确定性问题。该框架将问题建模为约束的部分可观测马尔可夫决策过程（POMDPs），并开发了 Deep Decentralized Multi-agent Actor-Critic (DDMAC) 方法，采用 Centralized Training and Decentralized Execution (CTDE) 策略来处理高维状态空间和资源限制。在弗吉尼亚州的一个真实交通网络案例中，DDMAC-CTDE 比传统管理策略表现出显著优势，提供近优解决方案，显著降低了长期风险和成本。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12455v1",
      "published_date": "2024-01-23 02:52:36 UTC",
      "updated_date": "2024-01-23 02:52:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:55:36.864595"
    },
    {
      "arxiv_id": "2401.12451v2",
      "title": "Methods and strategies for improving the novel view synthesis quality of neural radiation field",
      "title_zh": "翻译失败",
      "authors": [
        "Shun Fang",
        "Ming Cui",
        "Xing Feng",
        "Yanna Lv"
      ],
      "abstract": "Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a\nscene from 2D images and synthesize realistic novel view images. This\ntechnology has received widespread attention from the industry and has good\napplication prospects. In response to the problem that the rendering quality of\nNeRF images needs to be improved, many researchers have proposed various\nmethods to improve the rendering quality in the past three years. The latest\nrelevant papers are classified and reviewed, the technical principles behind\nquality improvement are analyzed, and the future evolution direction of quality\nimprovement methods is discussed. This study can help researchers quickly\nunderstand the current state and evolutionary context of technology in this\nfield, which is helpful in inspiring the development of more efficient\nalgorithms and promoting the application of NeRF technology in related fields.",
      "tldr_zh": "Neural Radiation Field (NeRF) 技术可从2D图像学习3D隐式模型并合成真实的新视图图像，但其渲染质量仍需改进。本文对过去三年内提出的各种改进方法进行了分类和回顾，分析了这些方法背后的技术原理，并讨论了未来演化方向。该研究有助于研究者快速掌握NeRF领域现状，激发更高效算法的开发，并促进其在相关领域的应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2; I.4; I.6"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12451v2",
      "published_date": "2024-01-23 02:30:16 UTC",
      "updated_date": "2024-04-18 01:37:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:55:49.314983"
    },
    {
      "arxiv_id": "2401.12435v2",
      "title": "Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Jiayi Xie",
        "Hongfeng Li",
        "Jin Cheng",
        "Qingrui Cai",
        "Hanbo Tan",
        "Lingyun Zu",
        "Xiaobo Qu",
        "Hongbin Han"
      ],
      "abstract": "The brain extracellular space (ECS), an irregular, extremely tortuous\nnanoscale space located between cells or between cells and blood vessels, is\ncrucial for nerve cell survival. It plays a pivotal role in high-level brain\nfunctions such as memory, emotion, and sensation. However, the specific form of\nmolecular transport within the ECS remain elusive. To address this challenge,\nthis paper proposes a novel approach to quantitatively analyze the molecular\ntransport within the ECS by solving an inverse problem derived from the\nadvection-diffusion equation (ADE) using a physics-informed neural network\n(PINN). PINN provides a streamlined solution to the ADE without the need for\nintricate mathematical formulations or grid settings. Additionally, the\noptimization of PINN facilitates the automatic computation of the diffusion\ncoefficient governing long-term molecule transport and the velocity of\nmolecules driven by advection. Consequently, the proposed method allows for the\nquantitative analysis and identification of the specific pattern of molecular\ntransport within the ECS through the calculation of the Peclet number.\nExperimental validation on two datasets of magnetic resonance images (MRIs)\ncaptured at different time points showcases the effectiveness of the proposed\nmethod. Notably, our simulations reveal identical molecular transport patterns\nbetween datasets representing rats with tracer injected into the same brain\nregion. These findings highlight the potential of PINN as a promising tool for\ncomprehensively exploring molecular transport within the ECS.",
      "tldr_zh": "本研究针对大脑extracellular space (ECS)中分子传输形式的未知问题，提出了一种基于physics-informed neural network (PINN)的方法，通过求解advection-diffusion equation (ADE)的逆问题，实现对分子传输的定量分析。PINN简化了ADE的求解过程，无需复杂数学公式或网格设置，并自动计算扩散系数(diffusion coefficient)和平流速度(velocity of molecules driven by advection)，从而通过Peclet number量化传输模式。在两个MRI数据集上的实验验证显示，该方法有效，且模拟结果揭示了相同脑区注射示踪剂的鼠类样本间一致的分子传输模式，突显了PINN在探索ECS分子传输方面的潜力。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12435v2",
      "published_date": "2024-01-23 02:04:15 UTC",
      "updated_date": "2024-01-24 02:31:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:56:01.057037"
    },
    {
      "arxiv_id": "2401.12421v1",
      "title": "AdaEmbed: Semi-supervised Domain Adaptation in the Embedding Space",
      "title_zh": "翻译失败",
      "authors": [
        "Ali Mottaghi",
        "Mohammad Abdullah Jamal",
        "Serena Yeung",
        "Omid Mohareri"
      ],
      "abstract": "Semi-supervised domain adaptation (SSDA) presents a critical hurdle in\ncomputer vision, especially given the frequent scarcity of labeled data in\nreal-world settings. This scarcity often causes foundation models, trained on\nextensive datasets, to underperform when applied to new domains. AdaEmbed, our\nnewly proposed methodology for SSDA, offers a promising solution to these\nchallenges. Leveraging the potential of unlabeled data, AdaEmbed facilitates\nthe transfer of knowledge from a labeled source domain to an unlabeled target\ndomain by learning a shared embedding space. By generating accurate and uniform\npseudo-labels based on the established embedding space, the model overcomes the\nlimitations of conventional SSDA, thus enhancing performance significantly. Our\nmethod's effectiveness is validated through extensive experiments on benchmark\ndatasets such as DomainNet, Office-Home, and VisDA-C, where AdaEmbed\nconsistently outperforms all the baselines, setting a new state of the art for\nSSDA. With its straightforward implementation and high data efficiency,\nAdaEmbed stands out as a robust and pragmatic solution for real-world\nscenarios, where labeled data is scarce. To foster further research and\napplication in this area, we are sharing the codebase of our unified framework\nfor semi-supervised domain adaptation.",
      "tldr_zh": "这篇论文提出了 AdaEmbed，一种用于 Semi-supervised Domain Adaptation (SSDA) 的新方法，旨在解决计算机视觉中标签数据稀缺导致的模型性能下降问题。该方法通过学习共享的 embedding space，利用无标签数据生成准确的 pseudo-labels，从而实现从源域到目标域的知识转移，并在实验中显著提升性能。在 DomainNet、Office-Home 和 VisDA-C 等基准数据集上，AdaEmbed 超越所有基线，设置了新的 state of the art，并开源代码以促进实际应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12421v1",
      "published_date": "2024-01-23 01:10:25 UTC",
      "updated_date": "2024-01-23 01:10:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:56:12.988047"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 88,
  "processed_papers_count": 88,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-16T23:56:37.471404"
}
[
  {
    "arxiv_id": "2402.01694v1",
    "title": "ARGS: Alignment as Reward-Guided Search",
    "authors": [
      "Maxim Khanov",
      "Jirayu Burapacheep",
      "Yixuan Li"
    ],
    "abstract": "Aligning large language models with human objectives is paramount, yet common\napproaches including RLHF suffer from unstable and resource-intensive training.\nIn response to this challenge, we introduce ARGS, Alignment as Reward-Guided\nSearch, a novel framework that integrates alignment into the decoding process,\neliminating the need for expensive RL training. By adjusting the model's\nprobabilistic predictions using a reward signal, ARGS generates texts with\nsemantic diversity while being aligned with human preferences, offering a\npromising and flexible solution for aligning language models. Notably, ARGS\ndemonstrates consistent enhancements in average reward compared to baselines\nacross diverse alignment tasks and various model dimensions. For example, under\nthe same greedy-based decoding strategy, our method improves the average reward\nby 19.56% relative to the baseline and secures a preference or tie score of\n64.33% in GPT-4 evaluation. We believe that our framework, emphasizing\ndecoding-time alignment, paves the way for more responsive language models in\nthe future. Code is publicly available at:\n\\url{https://github.com/deeplearning-wisc/args}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.01694v1",
    "published_date": "2024-01-23 23:42:41 UTC",
    "updated_date": "2024-01-23 23:42:41 UTC"
  },
  {
    "arxiv_id": "2401.13138v6",
    "title": "Visibility into AI Agents",
    "authors": [
      "Alan Chan",
      "Carson Ezell",
      "Max Kaufmann",
      "Kevin Wei",
      "Lewis Hammond",
      "Herbie Bradley",
      "Emma Bluemke",
      "Nitarshan Rajkumar",
      "David Krueger",
      "Noam Kolt",
      "Lennart Heim",
      "Markus Anderljung"
    ],
    "abstract": "Increased delegation of commercial, scientific, governmental, and personal\nactivities to AI agents -- systems capable of pursuing complex goals with\nlimited supervision -- may exacerbate existing societal risks and introduce new\nrisks. Understanding and mitigating these risks involves critically evaluating\nexisting governance structures, revising and adapting these structures where\nneeded, and ensuring accountability of key stakeholders. Information about\nwhere, why, how, and by whom certain AI agents are used, which we refer to as\nvisibility, is critical to these objectives. In this paper, we assess three\ncategories of measures to increase visibility into AI agents: agent\nidentifiers, real-time monitoring, and activity logging. For each, we outline\npotential implementations that vary in intrusiveness and informativeness. We\nanalyze how the measures apply across a spectrum of centralized through\ndecentralized deployment contexts, accounting for various actors in the supply\nchain including hardware and software service providers. Finally, we discuss\nthe implications of our measures for privacy and concentration of power.\nFurther work into understanding the measures and mitigating their negative\nimpacts can help to build a foundation for the governance of AI agents.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency (ACM FAccT 2024)",
    "pdf_url": "http://arxiv.org/pdf/2401.13138v6",
    "published_date": "2024-01-23 23:18:33 UTC",
    "updated_date": "2024-05-17 17:45:05 UTC"
  },
  {
    "arxiv_id": "2401.13136v1",
    "title": "The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts",
    "authors": [
      "Lingfeng Shen",
      "Weiting Tan",
      "Sihao Chen",
      "Yunmo Chen",
      "Jingyu Zhang",
      "Haoran Xu",
      "Boyuan Zheng",
      "Philipp Koehn",
      "Daniel Khashabi"
    ],
    "abstract": "As the influence of large language models (LLMs) spans across global\ncommunities, their safety challenges in multilingual settings become paramount\nfor alignment research. This paper examines the variations in safety challenges\nfaced by LLMs across different languages and discusses approaches to\nalleviating such concerns. By comparing how state-of-the-art LLMs respond to\nthe same set of malicious prompts written in higher- vs. lower-resource\nlanguages, we observe that (1) LLMs tend to generate unsafe responses much more\noften when a malicious prompt is written in a lower-resource language, and (2)\nLLMs tend to generate more irrelevant responses to malicious prompts in\nlower-resource languages. To understand where the discrepancy can be\nattributed, we study the effect of instruction tuning with reinforcement\nlearning from human feedback (RLHF) or supervised finetuning (SFT) on the\nHH-RLHF dataset. Surprisingly, while training with high-resource languages\nimproves model alignment, training in lower-resource languages yields minimal\nimprovement. This suggests that the bottleneck of cross-lingual alignment is\nrooted in the pretraining stage. Our findings highlight the challenges in\ncross-lingual LLM safety, and we hope they inform future research in this\ndirection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13136v1",
    "published_date": "2024-01-23 23:12:09 UTC",
    "updated_date": "2024-01-23 23:12:09 UTC"
  },
  {
    "arxiv_id": "2402.01693v1",
    "title": "Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study",
    "authors": [
      "Zhe He",
      "Balu Bhasuran",
      "Qiao Jin",
      "Shubo Tian",
      "Karim Hanna",
      "Cindy Shavor",
      "Lisbeth Garcia Arguello",
      "Patrick Murray",
      "Zhiyong Lu"
    ],
    "abstract": "Lab results are often confusing and hard to understand. Large language models\n(LLMs) such as ChatGPT have opened a promising avenue for patients to get their\nquestions answered. We aim to assess the feasibility of using LLMs to generate\nrelevant, accurate, helpful, and unharmful responses to lab test-related\nquestions asked by patients and to identify potential issues that can be\nmitigated with augmentation approaches. We first collected lab test results\nrelated question and answer data from Yahoo! Answers and selected 53 QA pairs\nfor this study. Using the LangChain framework and ChatGPT web portal, we\ngenerated responses to the 53 questions from four LLMs including GPT-4, Meta\nLLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their\nanswers using standard QA similarity-based evaluation metrics including ROUGE,\nBLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge\nwhether a target model has higher quality in terms of relevance, correctness,\nhelpfulness, and safety than the baseline model. Finally, we performed a manual\nevaluation with medical experts for all the responses to seven selected\nquestions on the same four aspects. The results of Win Rate and medical expert\nevaluation both showed that GPT-4's responses achieved better scores than all\nthe other LLM responses and human responses on all four aspects (relevance,\ncorrectness, helpfulness, and safety). However, LLM responses occasionally also\nsuffer from a lack of interpretation in one's medical context, incorrect\nstatements, and lack of references. We find that compared to other three LLMs\nand human answer from the Q&A website, GPT-4's responses are more accurate,\nhelpful, relevant, and safer. However, there are cases which GPT-4 responses\nare inaccurate and not individualized. We identified a number of ways to\nimprove the quality of LLM responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01693v1",
    "published_date": "2024-01-23 22:03:51 UTC",
    "updated_date": "2024-01-23 22:03:51 UTC"
  },
  {
    "arxiv_id": "2401.13112v6",
    "title": "Distributional Counterfactual Explanations With Optimal Transport",
    "authors": [
      "Lei You",
      "Lele Cao",
      "Mattias Nilsson",
      "Bo Zhao",
      "Lei Lei"
    ],
    "abstract": "Counterfactual explanations (CE) are the de facto method for providing\ninsights into black-box decision-making models by identifying alternative\ninputs that lead to different outcomes. However, existing CE approaches,\nincluding group and global methods, focus predominantly on specific input\nmodifications, lacking the ability to capture nuanced distributional\ncharacteristics that influence model outcomes across the entire input-output\nspectrum. This paper proposes distributional counterfactual explanation (DCE),\nshifting focus to the distributional properties of observed and counterfactual\ndata, thus providing broader insights. DCE is particularly beneficial for\nstakeholders making strategic decisions based on statistical data analysis, as\nit makes the statistical distribution of the counterfactual resembles the one\nof the factual when aligning model outputs with a target\ndistribution\\textemdash something that the existing CE methods cannot fully\nachieve. We leverage optimal transport (OT) to formulate a chance-constrained\noptimization problem, deriving a counterfactual distribution aligned with its\nfactual counterpart, supported by statistical confidence. The efficacy of this\napproach is demonstrated through experiments, highlighting its potential to\nprovide deeper insights into decision-making models.",
    "categories": [
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13112v6",
    "published_date": "2024-01-23 21:48:52 UTC",
    "updated_date": "2025-03-12 11:53:06 UTC"
  },
  {
    "arxiv_id": "2401.13110v1",
    "title": "XAI for All: Can Large Language Models Simplify Explainable AI?",
    "authors": [
      "Philip Mavrepis",
      "Georgios Makridis",
      "Georgios Fatouros",
      "Vasileios Koukos",
      "Maria Margarita Separdani",
      "Dimosthenis Kyriazis"
    ],
    "abstract": "The field of Explainable Artificial Intelligence (XAI) often focuses on users\nwith a strong technical background, making it challenging for non-experts to\nunderstand XAI methods. This paper presents \"x-[plAIn]\", a new approach to make\nXAI more accessible to a wider audience through a custom Large Language Model\n(LLM), developed using ChatGPT Builder. Our goal was to design a model that can\ngenerate clear, concise summaries of various XAI methods, tailored for\ndifferent audiences, including business professionals and academics. The key\nfeature of our model is its ability to adapt explanations to match each\naudience group's knowledge level and interests. Our approach still offers\ntimely insights, facilitating the decision-making process by the end users.\nResults from our use-case studies show that our model is effective in providing\neasy-to-understand, audience-specific explanations, regardless of the XAI\nmethod used. This adaptability improves the accessibility of XAI, bridging the\ngap between complex AI technologies and their practical applications. Our\nfindings indicate a promising direction for LLMs in making advanced AI concepts\nmore accessible to a diverse range of users.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13110v1",
    "published_date": "2024-01-23 21:47:12 UTC",
    "updated_date": "2024-01-23 21:47:12 UTC"
  },
  {
    "arxiv_id": "2401.13099v1",
    "title": "Sparse identification of nonlinear dynamics in the presence of library and system uncertainty",
    "authors": [
      "Andrew O'Brien"
    ],
    "abstract": "The SINDy algorithm has been successfully used to identify the governing\nequations of dynamical systems from time series data. However, SINDy assumes\nthe user has prior knowledge of the variables in the system and of a function\nlibrary that can act as a basis for the system. In this paper, we demonstrate\non real world data how the Augmented SINDy algorithm outperforms SINDy in the\npresence of system variable uncertainty. We then show SINDy can be further\naugmented to perform robustly when both kinds of uncertainty are present.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13099v1",
    "published_date": "2024-01-23 21:23:51 UTC",
    "updated_date": "2024-01-23 21:23:51 UTC"
  },
  {
    "arxiv_id": "2401.13098v3",
    "title": "Enhancing Global Maritime Traffic Network Forecasting with Gravity-Inspired Deep Learning Models",
    "authors": [
      "Ruixin Song",
      "Gabriel Spadon",
      "Ronald Pelot",
      "Stan Matwin",
      "Amilcar Soares"
    ],
    "abstract": "Aquatic non-indigenous species (NIS) pose significant threats to\nbiodiversity, disrupting ecosystems and inflicting substantial economic damages\nacross agriculture, forestry, and fisheries. Due to the fast growth of global\ntrade and transportation networks, NIS has been introduced and spread\nunintentionally in new environments. This study develops a new physics-informed\nmodel to forecast maritime shipping traffic between port regions worldwide. The\npredicted information provided by these models, in turn, is used as input for\nrisk assessment of NIS spread through transportation networks to evaluate the\ncapability of our solution. Inspired by the gravity model for international\ntrades, our model considers various factors that influence the likelihood and\nimpact of vessel activities, such as shipping flux density, distance between\nports, trade flow, and centrality measures of transportation hubs. Accordingly,\nthis paper introduces transformers to gravity models to rebuild the short- and\nlong-term dependencies that make the risk analysis feasible. Thus, we introduce\na physics-inspired framework that achieves an 89% binary accuracy for existing\nand non-existing trajectories and an 84.8% accuracy for the number of vessels\nflowing between key port areas, representing more than 10% improvement over the\ntraditional deep-gravity model. Along these lines, this research contributes to\na better understanding of NIS risk assessment. It allows policymakers,\nconservationists, and stakeholders to prioritize management actions by\nidentifying high-risk invasion pathways. Besides, our model is versatile and\ncan include new data sources, making it suitable for assessing international\nvessel traffic flow in a changing global landscape.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13098v3",
    "published_date": "2024-01-23 21:22:51 UTC",
    "updated_date": "2024-07-10 22:33:58 UTC"
  },
  {
    "arxiv_id": "2401.13097v2",
    "title": "Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in Deep Learning Systems",
    "authors": [
      "Michelle R. Greene",
      "Mariam Josyula",
      "Wentao Si",
      "Jennifer A. Hart"
    ],
    "abstract": "Computer-based scene understanding has influenced fields ranging from urban\nplanning to autonomous vehicle performance, yet little is known about how well\nthese technologies work across social differences. We investigate the biases of\ndeep convolutional neural networks (dCNNs) in scene classification, using\nnearly one million images from global and US sources, including user-submitted\nhome photographs and Airbnb listings. We applied statistical models to quantify\nthe impact of socioeconomic indicators such as family income, Human Development\nIndex (HDI), and demographic factors from public data sources (CIA and US\nCensus) on dCNN performance. Our analyses revealed significant socioeconomic\nbias, where pretrained dCNNs demonstrated lower classification accuracy, lower\nclassification confidence, and a higher tendency to assign labels that could be\noffensive when applied to homes (e.g., \"ruin\", \"slum\"), especially in images\nfrom homes with lower socioeconomic status (SES). This trend is consistent\nacross two datasets of international images and within the diverse economic and\nracial landscapes of the United States. This research contributes to\nunderstanding biases in computer vision, emphasizing the need for more\ninclusive and representative training datasets. By mitigating the bias in the\ncomputer vision pipelines, we can ensure fairer and more equitable outcomes for\napplied computer vision, including home valuation and smart home security\nsystems. There is urgency in addressing these biases, which can significantly\nimpact critical decisions in urban development and resource allocation. Our\nfindings also motivate the development of AI systems that better understand and\nserve diverse communities, moving towards technology that equitably benefits\nall sectors of society.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68-02",
      "I.2.m"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages, 3 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.13097v2",
    "published_date": "2024-01-23 21:22:06 UTC",
    "updated_date": "2025-03-05 21:31:31 UTC"
  },
  {
    "arxiv_id": "2401.13086v1",
    "title": "Towards Trustable Language Models: Investigating Information Quality of Large Language Models",
    "authors": [
      "Rick Rejeleene",
      "Xiaowei Xu",
      "John Talburt"
    ],
    "abstract": "Large language models (LLM) are generating information at a rapid pace,\nrequiring users to increasingly rely and trust the data. Despite remarkable\nadvances of LLM, Information generated by LLM is not completely trustworthy,\ndue to challenges in information quality. Specifically, integrity of\nInformation quality decreases due to unreliable, biased, tokenization during\npre-training of LLM. Moreover, due to decreased information quality issues, has\nled towards hallucination, fabricated information. Unreliable information can\nlead towards flawed decisions in businesses, which impacts economic activity.\nIn this work, we introduce novel mathematical information quality evaluation of\nLLM, we furthermore analyze and highlight information quality challenges,\nscaling laws to systematically scale language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "31 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.13086v1",
    "published_date": "2024-01-23 20:55:49 UTC",
    "updated_date": "2024-01-23 20:55:49 UTC"
  },
  {
    "arxiv_id": "2401.13085v1",
    "title": "IndiText Boost: Text Augmentation for Low Resource India Languages",
    "authors": [
      "Onkar Litake",
      "Niraj Yagnik",
      "Shreyas Labhsetwar"
    ],
    "abstract": "Text Augmentation is an important task for low-resource languages. It helps\ndeal with the problem of data scarcity. A data augmentation strategy is used to\ndeal with the problem of data scarcity. Through the years, much work has been\ndone on data augmentation for the English language. In contrast, very less work\nhas been done on Indian languages. This is contrary to the fact that data\naugmentation is used to deal with data scarcity. In this work, we focus on\nimplementing techniques like Easy Data Augmentation, Back Translation,\nParaphrasing, Text Generation using LLMs, and Text Expansion using LLMs for\ntext classification on different languages. We focus on 6 Indian languages\nnamely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to\nour knowledge, no such work exists for text augmentation on Indian languages.\nWe carry out binary as well as multi-class text classification to make our\nresults more comparable. We get surprising results as basic data augmentation\ntechniques surpass LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13085v1",
    "published_date": "2024-01-23 20:54:40 UTC",
    "updated_date": "2024-01-23 20:54:40 UTC"
  },
  {
    "arxiv_id": "2402.01691v1",
    "title": "Investigating Algorithm Review Boards for Organizational Responsible Artificial Intelligence Governance",
    "authors": [
      "Emily Hadley",
      "Alan Blatecky",
      "Megan Comfort"
    ],
    "abstract": "Organizations including companies, nonprofits, governments, and academic\ninstitutions are increasingly developing, deploying, and utilizing artificial\nintelligence (AI) tools. Responsible AI (RAI) governance approaches at\norganizations have emerged as important mechanisms to address potential AI\nrisks and harms. In this work, we interviewed 17 technical contributors across\norganization types (Academic, Government, Industry, Nonprofit) and sectors\n(Finance, Health, Tech, Other) about their experiences with internal RAI\ngovernance. Our findings illuminated the variety of organizational definitions\nof RAI and accompanying internal governance approaches. We summarized the first\ndetailed findings on algorithm review boards (ARBs) and similar review\ncommittees in practice, including their membership, scope, and measures of\nsuccess. We confirmed known robust model governance in finance sectors and\nrevealed extensive algorithm and AI governance with ARB-like review boards in\nhealth sectors. Our findings contradict the idea that Institutional Review\nBoards alone are sufficient for algorithm governance and posit that ARBs are\namong the more impactful internal RAI governance approaches. Our results\nsuggest that integration with existing internal regulatory approaches and\nleadership buy-in are among the most important attributes for success and that\nfinancial tensions are the greatest challenge to effective organizational RAI.\nWe make a variety of suggestions for how organizational partners can learn from\nthese findings when building their own internal RAI frameworks. We outline\nfuture directions for developing and measuring effectiveness of ARBs and other\ninternal RAI governance approaches.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01691v1",
    "published_date": "2024-01-23 20:53:53 UTC",
    "updated_date": "2024-01-23 20:53:53 UTC"
  },
  {
    "arxiv_id": "2401.13081v1",
    "title": "Free Form Medical Visual Question Answering in Radiology",
    "authors": [
      "Abhishek Narayanan",
      "Rushabh Musthyala",
      "Rahul Sankar",
      "Anirudh Prasad Nistala",
      "Pranav Singh",
      "Jacopo Cirrone"
    ],
    "abstract": "Visual Question Answering (VQA) in the medical domain presents a unique,\ninterdisciplinary challenge, combining fields such as Computer Vision, Natural\nLanguage Processing, and Knowledge Representation. Despite its importance,\nresearch in medical VQA has been scant, only gaining momentum since 2018.\nAddressing this gap, our research delves into the effective representation of\nradiology images and the joint learning of multimodal representations,\nsurpassing existing methods. We innovatively augment the SLAKE dataset,\nenabling our model to respond to a more diverse array of questions, not limited\nto the immediate content of radiology or pathology images. Our model achieves a\ntop-1 accuracy of 79.55\\% with a less complex architecture, demonstrating\ncomparable performance to current state-of-the-art models. This research not\nonly advances medical VQA but also opens avenues for practical applications in\ndiagnostic settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages and 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.13081v1",
    "published_date": "2024-01-23 20:26:52 UTC",
    "updated_date": "2024-01-23 20:26:52 UTC"
  },
  {
    "arxiv_id": "2401.13068v1",
    "title": "Local Background Estimation for Improved Gas Plume Identification in Hyperspectral Images",
    "authors": [
      "Scout Jarman",
      "Zigfried Hampel-Arias",
      "Adra Carr",
      "Kevin R. Moon"
    ],
    "abstract": "Deep learning identification models have shown promise for identifying gas\nplumes in Longwave IR hyperspectral images of urban scenes, particularly when a\nlarge library of gases are being considered. Because many gases have similar\nspectral signatures, it is important to properly estimate the signal from a\ndetected plume. Typically, a scene's global mean spectrum and covariance matrix\nare estimated to whiten the plume's signal, which removes the background's\nsignature from the gas signature. However, urban scenes can have many different\nbackground materials that are spatially and spectrally heterogeneous. This can\nlead to poor identification performance when the global background estimate is\nnot representative of a given local background material. We use image\nsegmentation, along with an iterative background estimation algorithm, to\ncreate local estimates for the various background materials that reside\nunderneath a gas plume. Our method outperforms global background estimation on\na set of simulated and real gas plumes. This method shows promise in increasing\ndeep learning identification confidence, while being simple and easy to tune\nwhen considering diverse plumes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to International Geoscience and Remote Sensing Symposium\n  (IGARSS), 2024. 5 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.13068v1",
    "published_date": "2024-01-23 19:48:34 UTC",
    "updated_date": "2024-01-23 19:48:34 UTC"
  },
  {
    "arxiv_id": "2401.13060v1",
    "title": "TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur'anic QA",
    "authors": [
      "Mohammed Alaa Elkomy",
      "Amany Sarhan"
    ],
    "abstract": "In this paper, we present our approach to tackle Qur'an QA 2023 shared tasks\nA and B. To address the challenge of low-resourced training data, we rely on\ntransfer learning together with a voting ensemble to improve prediction\nstability across multiple runs. Additionally, we employ different architectures\nand learning mechanisms for a range of Arabic pre-trained transformer-based\nmodels for both tasks. To identify unanswerable questions, we propose using a\nthresholding mechanism. Our top-performing systems greatly surpass the baseline\nperformance on the hidden split, achieving a MAP score of 25.05% for task A and\na partial Average Precision (pAP) of 57.11% for task B.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13060v1",
    "published_date": "2024-01-23 19:32:54 UTC",
    "updated_date": "2024-01-23 19:32:54 UTC"
  },
  {
    "arxiv_id": "2401.13049v1",
    "title": "CIS-UNet: Multi-Class Segmentation of the Aorta in Computed Tomography Angiography via Context-Aware Shifted Window Self-Attention",
    "authors": [
      "Muhammad Imran",
      "Jonathan R Krebs",
      "Veera Rajasekhar Reddy Gopu",
      "Brian Fazzone",
      "Vishal Balaji Sivaraman",
      "Amarjeet Kumar",
      "Chelsea Viscardi",
      "Robert Evans Heithaus",
      "Benjamin Shickel",
      "Yuyin Zhou",
      "Michol A Cooper",
      "Wei Shao"
    ],
    "abstract": "Advancements in medical imaging and endovascular grafting have facilitated\nminimally invasive treatments for aortic diseases. Accurate 3D segmentation of\nthe aorta and its branches is crucial for interventions, as inaccurate\nsegmentation can lead to erroneous surgical planning and endograft\nconstruction. Previous methods simplified aortic segmentation as a binary image\nsegmentation problem, overlooking the necessity of distinguishing between\nindividual aortic branches. In this paper, we introduce Context Infused\nSwin-UNet (CIS-UNet), a deep learning model designed for multi-class\nsegmentation of the aorta and thirteen aortic branches. Combining the strengths\nof Convolutional Neural Networks (CNNs) and Swin transformers, CIS-UNet adopts\na hierarchical encoder-decoder structure comprising a CNN encoder, symmetric\ndecoder, skip connections, and a novel Context-aware Shifted Window\nSelf-Attention (CSW-SA) as the bottleneck block. Notably, CSW-SA introduces a\nunique utilization of the patch merging layer, distinct from conventional Swin\ntransformers. It efficiently condenses the feature map, providing a global\nspatial context and enhancing performance when applied at the bottleneck layer,\noffering superior computational efficiency and segmentation accuracy compared\nto the Swin transformers. We trained our model on computed tomography (CT)\nscans from 44 patients and tested it on 15 patients. CIS-UNet outperformed the\nstate-of-the-art SwinUNetR segmentation model, which is solely based on Swin\ntransformers, by achieving a superior mean Dice coefficient of 0.713 compared\nto 0.697, and a mean surface distance of 2.78 mm compared to 3.39 mm.\nCIS-UNet's superior 3D aortic segmentation offers improved precision and\noptimization for planning endovascular treatments. Our dataset and code will be\npublicly available.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13049v1",
    "published_date": "2024-01-23 19:17:20 UTC",
    "updated_date": "2024-01-23 19:17:20 UTC"
  },
  {
    "arxiv_id": "2401.13034v4",
    "title": "Locality Sensitive Sparse Encoding for Learning World Models Online",
    "authors": [
      "Zichen Liu",
      "Chao Du",
      "Wee Sun Lee",
      "Min Lin"
    ],
    "abstract": "Acquiring an accurate world model online for model-based reinforcement\nlearning (MBRL) is challenging due to data nonstationarity, which typically\ncauses catastrophic forgetting for neural networks (NNs). From the online\nlearning perspective, a Follow-The-Leader (FTL) world model is desirable, which\noptimally fits all previous experiences at each round. Unfortunately, NN-based\nmodels need re-training on all accumulated data at every interaction step to\nachieve FTL, which is computationally expensive for lifelong agents. In this\npaper, we revisit models that can achieve FTL with incremental updates.\nSpecifically, our world model is a linear regression model supported by\nnonlinear random features. The linear part ensures efficient FTL update while\nthe nonlinear random feature empowers the fitting of complex environments. To\nbest trade off model capacity and computation efficiency, we introduce a\nlocality sensitive sparse encoding, which allows us to conduct efficient sparse\nupdates even with very high dimensional nonlinear features. We validate the\nrepresentation power of our encoding and verify that it allows efficient online\nlearning under data covariate shift. We also show, in the Dyna MBRL setting,\nthat our world models learned online using a single pass of trajectory data\neither surpass or match the performance of deep world models trained with\nreplay and other continual learning methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.13034v4",
    "published_date": "2024-01-23 19:00:02 UTC",
    "updated_date": "2024-04-17 07:54:45 UTC"
  },
  {
    "arxiv_id": "2401.12975v1",
    "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments",
    "authors": [
      "Qinhong Zhou",
      "Sunli Chen",
      "Yisong Wang",
      "Haozhe Xu",
      "Weihua Du",
      "Hongxin Zhang",
      "Yilun Du",
      "Joshua B. Tenenbaum",
      "Chuang Gan"
    ],
    "abstract": "Recent advances in high-fidelity virtual environments serve as one of the\nmajor driving forces for building intelligent embodied agents to perceive,\nreason and interact with the physical world. Typically, these environments\nremain unchanged unless agents interact with them. However, in real-world\nscenarios, agents might also face dynamically changing environments\ncharacterized by unexpected events and need to rapidly take action accordingly.\nTo remedy this gap, we propose a new simulated embodied benchmark, called\nHAZARD, specifically designed to assess the decision-making abilities of\nembodied agents in dynamic situations. HAZARD consists of three unexpected\ndisaster scenarios, including fire, flood, and wind, and specifically supports\nthe utilization of large language models (LLMs) to assist common sense\nreasoning and decision-making. This benchmark enables us to evaluate autonomous\nagents' decision-making capabilities across various pipelines, including\nreinforcement learning (RL), rule-based, and search-based methods in\ndynamically changing environments. As a first step toward addressing this\nchallenge using large language models, we further develop an LLM-based agent\nand perform an in-depth analysis of its promise and challenge of solving these\nchallenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2024. The first two authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2401.12975v1",
    "published_date": "2024-01-23 18:59:43 UTC",
    "updated_date": "2024-01-23 18:59:43 UTC"
  },
  {
    "arxiv_id": "2401.12972v3",
    "title": "On the Efficacy of Text-Based Input Modalities for Action Anticipation",
    "authors": [
      "Apoorva Beedu",
      "Harish Haresamudram",
      "Karan Samel",
      "Irfan Essa"
    ],
    "abstract": "Anticipating future actions is a highly challenging task due to the diversity\nand scale of potential future actions; yet, information from different\nmodalities help narrow down plausible action choices. Each modality can provide\ndiverse and often complementary context for the model to learn from. While\nprevious multi-modal methods leverage information from modalities such as video\nand audio, we primarily explore how text descriptions of actions and objects\ncan also lead to more accurate action anticipation by providing additional\ncontextual cues, e.g., about the environment and its contents. We propose a\nMulti-modal Contrastive Anticipative Transformer (M-CAT), a video transformer\narchitecture that jointly learns from multi-modal features and text\ndescriptions of actions and objects. We train our model in two stages, where\nthe model first learns to align video clips with descriptions of future\nactions, and is subsequently fine-tuned to predict future actions. Compared to\nexisting methods, M-CAT has the advantage of learning additional context from\ntwo types of text inputs: rich descriptions of future actions during\npre-training, and, text descriptions for detected objects and actions during\nmodality feature fusion. Through extensive experimental evaluation, we\ndemonstrate that our model outperforms previous methods on the EpicKitchens\ndatasets, and show that using simple text descriptions of actions and objects\naid in more effective action anticipation. In addition, we examine the impact\nof object and action information obtained via text, and perform extensive\nablations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12972v3",
    "published_date": "2024-01-23 18:58:35 UTC",
    "updated_date": "2024-08-29 15:11:29 UTC"
  },
  {
    "arxiv_id": "2401.12963v2",
    "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents",
    "authors": [
      "Michael Ahn",
      "Debidatta Dwibedi",
      "Chelsea Finn",
      "Montse Gonzalez Arenas",
      "Keerthana Gopalakrishnan",
      "Karol Hausman",
      "Brian Ichter",
      "Alex Irpan",
      "Nikhil Joshi",
      "Ryan Julian",
      "Sean Kirmani",
      "Isabel Leal",
      "Edward Lee",
      "Sergey Levine",
      "Yao Lu",
      "Isabel Leal",
      "Sharath Maddineni",
      "Kanishka Rao",
      "Dorsa Sadigh",
      "Pannag Sanketi",
      "Pierre Sermanet",
      "Quan Vuong",
      "Stefan Welker",
      "Fei Xia",
      "Ted Xiao",
      "Peng Xu",
      "Steve Xu",
      "Zhuo Xu"
    ],
    "abstract": "Foundation models that incorporate language, vision, and more recently\nactions have revolutionized the ability to harness internet scale data to\nreason about useful tasks. However, one of the key challenges of training\nembodied foundation models is the lack of data grounded in the physical world.\nIn this paper, we propose AutoRT, a system that leverages existing foundation\nmodels to scale up the deployment of operational robots in completely unseen\nscenarios with minimal human supervision. AutoRT leverages vision-language\nmodels (VLMs) for scene understanding and grounding, and further uses large\nlanguage models (LLMs) for proposing diverse and novel instructions to be\nperformed by a fleet of robots. Guiding data collection by tapping into the\nknowledge of foundation models enables AutoRT to effectively reason about\nautonomy tradeoffs and safety while significantly scaling up data collection\nfor robot learning. We demonstrate AutoRT proposing instructions to over 20\nrobots across multiple buildings and collecting 77k real robot episodes via\nboth teleoperation and autonomous robot policies. We experimentally show that\nsuch \"in-the-wild\" data collected by AutoRT is significantly more diverse, and\nthat AutoRT's use of LLMs allows for instruction following data collection\nrobots that can align to human preferences.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "26 pages, 9 figures, ICRA 2024 VLMNM Workshop",
    "pdf_url": "http://arxiv.org/pdf/2401.12963v2",
    "published_date": "2024-01-23 18:45:54 UTC",
    "updated_date": "2024-07-02 01:52:26 UTC"
  },
  {
    "arxiv_id": "2401.12954v1",
    "title": "Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding",
    "authors": [
      "Mirac Suzgun",
      "Adam Tauman Kalai"
    ],
    "abstract": "We introduce meta-prompting, an effective scaffolding technique designed to\nenhance the functionality of language models (LMs). This approach transforms a\nsingle LM into a multi-faceted conductor, adept at managing and integrating\nmultiple independent LM queries. By employing high-level instructions,\nmeta-prompting guides the LM to break down complex tasks into smaller, more\nmanageable subtasks. These subtasks are then handled by distinct \"expert\"\ninstances of the same LM, each operating under specific, tailored instructions.\nCentral to this process is the LM itself, in its role as the conductor, which\nensures seamless communication and effective integration of the outputs from\nthese expert models. It additionally employs its inherent critical thinking and\nrobust verification processes to refine and authenticate the end result. This\ncollaborative prompting approach empowers a single LM to simultaneously act as\na comprehensive orchestrator and a panel of diverse experts, significantly\nenhancing its performance across a wide array of tasks. The zero-shot,\ntask-agnostic nature of meta-prompting greatly simplifies user interaction by\nobviating the need for detailed, task-specific instructions. Furthermore, our\nresearch demonstrates the seamless integration of external tools, such as a\nPython interpreter, into the meta-prompting framework, thereby broadening its\napplicability and utility. Through rigorous experimentation with GPT-4, we\nestablish the superiority of meta-prompting over conventional scaffolding\nmethods: When averaged across all tasks, including the Game of 24,\nCheckmate-in-One, and Python Programming Puzzles, meta-prompting, augmented\nwith a Python interpreter functionality, surpasses standard prompting by 17.1%,\nexpert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "https://github.com/suzgunmirac/meta-prompting",
    "pdf_url": "http://arxiv.org/pdf/2401.12954v1",
    "published_date": "2024-01-23 18:22:19 UTC",
    "updated_date": "2024-01-23 18:22:19 UTC"
  },
  {
    "arxiv_id": "2401.12947v1",
    "title": "Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion",
    "authors": [
      "Dylan Zhang",
      "Curt Tigges",
      "Zory Zhang",
      "Stella Biderman",
      "Maxim Raginsky",
      "Talia Ringer"
    ],
    "abstract": "This paper investigates the ability of transformer-based models to learn\nstructural recursion from examples. Recursion is a universal concept in both\nnatural and formal languages. Structural recursion is central to the\nprogramming language and formal mathematics tasks where symbolic tools\ncurrently excel beyond neural models, such as inferring semantic relations\nbetween datatypes and emulating program behavior. We introduce a general\nframework that nicely connects the abstract concepts of structural recursion in\nthe programming language domain to concrete sequence modeling problems and\nlearned models' behavior. The framework includes a representation that captures\nthe general \\textit{syntax} of structural recursion, coupled with two different\nframeworks for understanding their \\textit{semantics} -- one that is more\nnatural from a programming languages perspective and one that helps bridge that\nperspective with a mechanistic understanding of the underlying transformer\narchitecture.\n  With our framework as a powerful conceptual tool, we identify different\nissues under various set-ups. The models trained to emulate recursive\ncomputations cannot fully capture the recursion yet instead fit short-cut\nalgorithms and thus cannot solve certain edge cases that are under-represented\nin the training distribution. In addition, it is difficult for state-of-the-art\nlarge language models (LLMs) to mine recursive rules from in-context\ndemonstrations. Meanwhile, these LLMs fail in interesting ways when emulating\nreduction (step-wise computation) of the recursive function.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.FL",
      "cs.LO",
      "cs.PL"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2305.14699",
    "pdf_url": "http://arxiv.org/pdf/2401.12947v1",
    "published_date": "2024-01-23 18:07:38 UTC",
    "updated_date": "2024-01-23 18:07:38 UTC"
  },
  {
    "arxiv_id": "2401.12920v3",
    "title": "Truck Parking Usage Prediction with Decomposed Graph Neural Networks",
    "authors": [
      "Rei Tamaru",
      "Yang Cheng",
      "Steven Parker",
      "Ernie Perry",
      "Bin Ran",
      "Soyoung Ahn"
    ],
    "abstract": "Truck parking on freight corridors faces the major challenge of insufficient\nparking spaces. This is exacerbated by the Hour-of-Service (HOS) regulations,\nwhich often result in unauthorized parking practices, causing safety concerns.\nIt has been shown that providing accurate parking usage prediction can be a\ncost-effective solution to reduce unsafe parking practices. In light of this,\nexisting studies have developed various methods to predict the usage of a truck\nparking site and have demonstrated satisfactory accuracy. However, these\nstudies focused on a single parking site, and few approaches have been proposed\nto predict the usage of multiple truck parking sites considering\nspatio-temporal dependencies, due to the lack of data. This paper aims to fill\nthis gap and presents the Regional Temporal Graph Convolutional Network\n(RegT-GCN) to predict parking usage across the entire state to provide more\ncomprehensive truck parking information. The framework leverages the\ntopological structures of truck parking site locations and historical parking\ndata to predict the occupancy rate considering spatio-temporal dependencies\nacross a state. To achieve this, we introduce a Regional Decomposition\napproach, which effectively captures the geographical characteristics of the\ntruck parking locations and their spatial correlations. Evaluation results\ndemonstrate that the proposed model outperforms other baseline models, showing\nthe effectiveness of our regional decomposition. The code is available at\nhttps://github.com/raynbowy23/RegT-GCN.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12920v3",
    "published_date": "2024-01-23 17:14:01 UTC",
    "updated_date": "2025-03-25 21:24:52 UTC"
  },
  {
    "arxiv_id": "2401.12917v1",
    "title": "Active Inference as a Model of Agency",
    "authors": [
      "Lancelot Da Costa",
      "Samuel Tenka",
      "Dominic Zhao",
      "Noor Sajid"
    ],
    "abstract": "Is there a canonical way to think of agency beyond reward maximisation? In\nthis paper, we show that any type of behaviour complying with physically sound\nassumptions about how macroscopic biological agents interact with the world\ncanonically integrates exploration and exploitation in the sense of minimising\nrisk and ambiguity about states of the world. This description, known as active\ninference, refines the free energy principle, a popular descriptive framework\nfor action and perception originating in neuroscience. Active inference\nprovides a normative Bayesian framework to simulate and model agency that is\nwidely used in behavioural neuroscience, reinforcement learning (RL) and\nrobotics. The usefulness of active inference for RL is three-fold. \\emph{a})\nActive inference provides a principled solution to the exploration-exploitation\ndilemma that usefully simulates biological agency. \\emph{b}) It provides an\nexplainable recipe to simulate behaviour, whence behaviour follows as an\nexplainable mixture of exploration and exploitation under a generative world\nmodel, and all differences in behaviour are explicit in differences in world\nmodel. \\emph{c}) This framework is universal in the sense that it is\ntheoretically possible to rewrite any RL algorithm conforming to the\ndescriptive assumptions of active inference as an active inference algorithm.\nThus, active inference can be used as a tool to uncover and compare the\ncommitments and assumptions of more specific models of agency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in RLDM2022 for the workshop 'RL as a model of agency'",
    "pdf_url": "http://arxiv.org/pdf/2401.12917v1",
    "published_date": "2024-01-23 17:09:25 UTC",
    "updated_date": "2024-01-23 17:09:25 UTC"
  },
  {
    "arxiv_id": "2401.12915v1",
    "title": "Red Teaming Visual Language Models",
    "authors": [
      "Mukai Li",
      "Lei Li",
      "Yuwei Yin",
      "Masood Ahmed",
      "Zhenguang Liu",
      "Qi Liu"
    ],
    "abstract": "VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language\nModels) to accept multimodal inputs. Since it has been verified that LLMs can\nbe induced to generate harmful or inaccurate content through specific test\ncases (termed as Red Teaming), how VLMs perform in similar scenarios,\nespecially with their combination of textual and visual inputs, remains a\nquestion. To explore this problem, we present a novel red teaming dataset\nRTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal\njail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,\nprivacy, safety, fairness). Our RTVLM is the first red-teaming dataset to\nbenchmark current VLMs in terms of these 4 different aspects. Detailed analysis\nshows that 10 prominent open-sourced VLMs struggle with the red teaming in\ndifferent degrees and have up to 31% performance gap with GPT-4V. Additionally,\nwe simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning\n(SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM\ntest set, 13% in MM-Hal, and without noticeable decline in MM-Bench,\noverpassing other LLaVA-based models with regular alignment data. This reveals\nthat current open-sourced VLMs still lack red teaming alignment. Our code and\ndatasets will be open-source.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Working in progress",
    "pdf_url": "http://arxiv.org/pdf/2401.12915v1",
    "published_date": "2024-01-23 17:07:18 UTC",
    "updated_date": "2024-01-23 17:07:18 UTC"
  },
  {
    "arxiv_id": "2401.12914v1",
    "title": "Emergent Communication Protocol Learning for Task Offloading in Industrial Internet of Things",
    "authors": [
      "Salwa Mostafa",
      "Mateus P. Mota",
      "Alvaro Valcarce",
      "Mehdi Bennis"
    ],
    "abstract": "In this paper, we leverage a multi-agent reinforcement learning (MARL)\nframework to jointly learn a computation offloading decision and multichannel\naccess policy with corresponding signaling. Specifically, the base station and\nindustrial Internet of Things mobile devices are reinforcement learning agents\nthat need to cooperate to execute their computation tasks within a deadline\nconstraint. We adopt an emergent communication protocol learning framework to\nsolve this problem. The numerical results illustrate the effectiveness of\nemergent communication in improving the channel access success rate and the\nnumber of successfully computed tasks compared to contention-based,\ncontention-free, and no-communication approaches. Moreover, the proposed task\noffloading policy outperforms remote and local computation baselines.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.MA",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12914v1",
    "published_date": "2024-01-23 17:06:13 UTC",
    "updated_date": "2024-01-23 17:06:13 UTC"
  },
  {
    "arxiv_id": "2401.12874v2",
    "title": "From Understanding to Utilization: A Survey on Explainability for Large Language Models",
    "authors": [
      "Haoyan Luo",
      "Lucia Specia"
    ],
    "abstract": "Explainability for Large Language Models (LLMs) is a critical yet challenging\naspect of natural language processing. As LLMs are increasingly integral to\ndiverse applications, their \"black-box\" nature sparks significant concerns\nregarding transparency and ethical use. This survey underscores the imperative\nfor increased explainability in LLMs, delving into both the research on\nexplainability and the various methodologies and tasks that utilize an\nunderstanding of these models. Our focus is primarily on pre-trained\nTransformer-based LLMs, such as LLaMA family, which pose distinctive\ninterpretability challenges due to their scale and complexity. In terms of\nexisting methods, we classify them into local and global analyses, based on\ntheir explanatory objectives. When considering the utilization of\nexplainability, we explore several compelling methods that concentrate on model\nediting, control generation, and model enhancement. Additionally, we examine\nrepresentative evaluation metrics and datasets, elucidating their advantages\nand limitations. Our goal is to reconcile theoretical and empirical\nunderstanding with practical implementation, proposing exciting avenues for\nexplanatory techniques and their applications in the LLMs era.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12874v2",
    "published_date": "2024-01-23 16:09:53 UTC",
    "updated_date": "2024-02-22 04:28:03 UTC"
  },
  {
    "arxiv_id": "2401.12873v3",
    "title": "Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model",
    "authors": [
      "Zhiwei He",
      "Xing Wang",
      "Wenxiang Jiao",
      "Zhuosheng Zhang",
      "Rui Wang",
      "Shuming Shi",
      "Zhaopeng Tu"
    ],
    "abstract": "Insufficient modeling of human preferences within the reward model is a major\nobstacle for leveraging human feedback to improve translation quality.\nFortunately, quality estimation (QE), which predicts the quality of a given\ntranslation without reference, has achieved impressive alignment with human\nevaluations in the last two years. In this work, we investigate the potential\nof employing the QE model as the reward model to predict human preferences for\nfeedback training. We first identify the overoptimization problem during\nQE-based feedback training, manifested as an increase in reward while\ntranslation quality declines. We examine the problem and argue that the\nvulnerability of the QE model might lead to high rewards for incorrect\ntranslations, resulting in overoptimization and error propagation. To address\nthe problem, we adopt a simple yet effective method that uses heuristic rules\nto detect the incorrect translations and assigns a penalty term to the reward\nscores of them. Experimental results show that the proposed QE-based feedback\ntraining achieves consistent and significant improvements across various\nsettings, further verified through human preference studies. Our subsequent\nanalysis demonstrates the high data efficiency of the proposed QE-based\nfeedback training: it outperforms systems using larger parallel corpora by a\nsmall amount of monolingual data. Our code is available at:\nhttps://github.com/zwhe99/FeedbackMT",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.12873v3",
    "published_date": "2024-01-23 16:07:43 UTC",
    "updated_date": "2024-03-18 15:16:16 UTC"
  },
  {
    "arxiv_id": "2401.12869v1",
    "title": "TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks",
    "authors": [
      "Zhiruo Wang",
      "Daniel Fried",
      "Graham Neubig"
    ],
    "abstract": "Language models (LMs) can solve tasks such as answering questions about\ntables or images by writing programs. However, using primitive functions often\nleads to verbose and error-prone programs, and higher-level functions require\nexpert design. To enable better solutions without human labor, we ask code LMs\nto curate reusable high-level functions, and use them to write solutions. We\npresent TROVE, a training-free method of inducing a verifiable and efficient\ntoolbox of functions, by generating via using, growing, and periodically\ntrimming the toolbox. On 11 datasets from math, table question answering, and\nimage reasoning tasks, TROVE consistently yields simpler solutions with higher\naccuracy than baselines using CODELLAMA and previous methods using GPT, while\nusing 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more\naccurate human verification than baselines. With the same pipeline, it creates\ndiverse functions for varied tasks and datasets, providing insights into their\nindividual characteristics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12869v1",
    "published_date": "2024-01-23 16:03:17 UTC",
    "updated_date": "2024-01-23 16:03:17 UTC"
  },
  {
    "arxiv_id": "2401.12866v1",
    "title": "Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported Coordination of Mobile Crowdsourcing",
    "authors": [
      "Ralf Bruns",
      "Jeremias Dötterl",
      "Jürgen Dunkel",
      "Sascha Ossowski"
    ],
    "abstract": "Mobile crowdsourcing refers to systems where the completion of tasks\nnecessarily requires physical movement of crowdworkers in an on-demand\nworkforce. Evidence suggests that in such systems, tasks often get assigned to\ncrowdworkers who struggle to complete those tasks successfully, resulting in\nhigh failure rates and low service quality. A promising solution to ensure\nhigher quality of service is to continuously adapt the assignment and respond\nto failure-causing events by transferring tasks to better-suited workers who\nuse different routes or vehicles. However, implementing task transfers in\nmobile crowdsourcing is difficult because workers are autonomous and may reject\ntransfer requests. Moreover, task outcomes are uncertain and need to be\npredicted. In this paper, we propose different mechanisms to achieve outcome\nprediction and task coordination in mobile crowdsourcing. First, we analyze\ndifferent data stream learning approaches for the prediction of task outcomes.\nSecond, based on the suggested prediction model, we propose and evaluate two\ndifferent approaches for task coordination with different degrees of autonomy:\nan opportunistic approach for crowdshipping with collaborative, but\nnon-autonomous workers, and a market-based model with autonomous workers for\ncrowdsensing.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12866v1",
    "published_date": "2024-01-23 16:00:45 UTC",
    "updated_date": "2024-01-23 16:00:45 UTC"
  },
  {
    "arxiv_id": "2401.12863v1",
    "title": "KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning",
    "authors": [
      "Debjyoti Mondal",
      "Suraj Modi",
      "Subhadarshi Panda",
      "Rituraj Singh",
      "Godawari Sudhakar Rao"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in\nnatural language processing tasks by leveraging chain of thought (CoT) that\nenables step-by-step thinking. Extending LLMs with multimodal capabilities is\nthe recent interest, but incurs computational cost and requires substantial\nhardware resources. To address these challenges, we propose KAM-CoT a framework\nthat integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities\nfor a comprehensive understanding of multimodal tasks. KAM-CoT adopts a\ntwo-stage training process with KG grounding to generate effective rationales\nand answers. By incorporating external knowledge from KGs during reasoning, the\nmodel gains a deeper contextual understanding reducing hallucinations and\nenhancing the quality of answers. This knowledge-augmented CoT reasoning\nempowers the model to handle questions requiring external context, providing\nmore informed answers. Experimental findings show KAM-CoT outperforms the\nstate-of-the-art methods. On the ScienceQA dataset, we achieve an average\naccuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by\n10%. Remarkably, KAM-CoT achieves these results with only 280M trainable\nparameters at a time, demonstrating its cost-efficiency and effectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.12863v1",
    "published_date": "2024-01-23 15:56:11 UTC",
    "updated_date": "2024-01-23 15:56:11 UTC"
  },
  {
    "arxiv_id": "2401.12862v2",
    "title": "FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units",
    "authors": [
      "Shaoheng Fang",
      "Rui Ye",
      "Wenhao Wang",
      "Zuhong Liu",
      "Yuxiao Wang",
      "Yafei Wang",
      "Siheng Chen",
      "Yanfeng Wang"
    ],
    "abstract": "Roadside unit (RSU) can significantly improve the safety and robustness of\nautonomous vehicles through Vehicle-to-Everything (V2X) communication.\nCurrently, the usage of a single RSU mainly focuses on real-time inference and\nV2X collaboration, while neglecting the potential value of the high-quality\ndata collected by RSU sensors. Integrating the vast amounts of data from\nnumerous RSUs can provide a rich source of data for model training. However,\nthe absence of ground truth annotations and the difficulty of transmitting\nenormous volumes of data are two inevitable barriers to fully exploiting this\nhidden value. In this paper, we introduce FedRSU, an innovative federated\nlearning framework for self-supervised scene flow estimation. In FedRSU, we\npresent a recurrent self-supervision training paradigm, where for each RSU, the\nscene flow prediction of points at every timestamp can be supervised by its\nsubsequent future multi-modality observation. Another key component of FedRSU\nis federated learning, where multiple devices collaboratively train an ML model\nwhile keeping the training data local and private. With the power of the\nrecurrent self-supervised learning paradigm, FL is able to leverage innumerable\nunderutilized data from RSU. To verify the FedRSU framework, we construct a\nlarge-scale multi-modality dataset RSU-SF. The dataset consists of 17 RSU\nclients, covering various scenarios, modalities, and sensor settings. Based on\nRSU-SF, we show that FedRSU can greatly improve model performance in ITS and\nprovide a comprehensive benchmark under diverse FL scenarios. To the best of\nour knowledge, we provide the first real-world LiDAR-camera multi-modal dataset\nand benchmark for the FL community.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12862v2",
    "published_date": "2024-01-23 15:52:57 UTC",
    "updated_date": "2024-08-11 04:17:09 UTC"
  },
  {
    "arxiv_id": "2401.12851v1",
    "title": "Classification of grapevine varieties using UAV hyperspectral imaging",
    "authors": [
      "Alfonso López",
      "Carlos Javier Ogayar",
      "Francisco Ramón Feito",
      "Joaquim João Sousa"
    ],
    "abstract": "The classification of different grapevine varieties is a relevant phenotyping\ntask in Precision Viticulture since it enables estimating the growth of\nvineyard rows dedicated to different varieties, among other applications\nconcerning the wine industry. This task can be performed with destructive\nmethods that require time-consuming tasks, including data collection and\nanalysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a\nmore efficient and less prohibitive approach to collecting hyperspectral data,\ndespite acquiring noisier data. Therefore, the first task is the processing of\nthese data to correct and downsample large amounts of data. In addition, the\nhyperspectral signatures of grape varieties are very similar. In this work, a\nConvolutional Neural Network (CNN) is proposed for classifying seventeen\nvarieties of red and white grape variants. Rather than classifying single\nsamples, these are processed together with their neighbourhood. Hence, the\nextraction of spatial and spectral features is addressed with 1) a spatial\nattention layer and 2) Inception blocks. The pipeline goes from processing to\ndataset elaboration, finishing with the training phase. The fitted model is\nevaluated in terms of response time, accuracy and data separability, and\ncompared with other state-of-the-art CNNs for classifying hyperspectral data.\nOur network was proven to be much more lightweight with a reduced number of\ninput bands, a lower number of trainable weights and therefore, reduced\ntraining time. Despite this, the evaluated metrics showed much better results\nfor our network (~99% overall accuracy), in comparison with previous works\nbarely achieving 81% OA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12851v1",
    "published_date": "2024-01-23 15:35:50 UTC",
    "updated_date": "2024-01-23 15:35:50 UTC"
  },
  {
    "arxiv_id": "2401.12850v2",
    "title": "End-to-End Supervised Hierarchical Graph Clustering for Speaker Diarization",
    "authors": [
      "Prachi Singh",
      "Sriram Ganapathy"
    ],
    "abstract": "Speaker diarization, the task of segmenting an audio recording based on\nspeaker identity, constitutes an important speech pre-processing step for\nseveral downstream applications.The conventional approach to diarization\ninvolves multiple steps of embedding extraction and clustering, which are often\noptimized in an isolated fashion. While end-to-end diarization systems attempt\nto learn a single model for the task, they are often cumbersome to train and\nrequire large supervised datasets. In this paper, we propose an end-to-end\nsupervised hierarchical clustering algorithm based on graph neural networks\n(GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). The\nembedding extractor is initialized using a pre-trained x-vector model while the\nGNN model is trained initially using the x-vector embeddings from the\npre-trained model. Finally, the E-SHARC model uses the front-end mel-filterbank\nfeatures as input and jointly optimizes the embedding extractor and the GNN\nclustering module, performing representation learning, metric learning, and\nclustering with end-to-end optimization. Further, with additional inputs from\nan external overlap detector, the E-SHARC approach is capable of predicting the\nspeakers in the overlapping speech regions. The experimental evaluation on\nbenchmark datasets like AMI, Voxconverse and DISPLACE, illustrates that the\nproposed E-SHARC framework provides competitive diarization results using graph\nbased clustering methods.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "11 pages. Under review IEEE TASLP. \\c{opyright} 2024 IEEE",
    "pdf_url": "http://arxiv.org/pdf/2401.12850v2",
    "published_date": "2024-01-23 15:35:44 UTC",
    "updated_date": "2024-12-02 17:38:21 UTC"
  },
  {
    "arxiv_id": "2401.12846v4",
    "title": "How well can a large language model explain business processes as perceived by users?",
    "authors": [
      "Dirk Fahland",
      "Fabiana Fournier",
      "Lior Limonad",
      "Inna Skarbovsky",
      "Ava J. E. Swevels"
    ],
    "abstract": "Large Language Models (LLMs) are trained on a vast amount of text to\ninterpret and generate human-like textual content. They are becoming a vital\nvehicle in realizing the vision of the autonomous enterprise, with\norganizations today actively adopting LLMs to automate many aspects of their\noperations. LLMs are likely to play a prominent role in future AI-augmented\nbusiness process management systems, catering functionalities across all system\nlifecycle stages. One such system's functionality is Situation-Aware\neXplainability (SAX), which relates to generating causally sound and\nhuman-interpretable explanations. In this paper, we present the SAX4BPM\nframework developed to generate SAX explanations. The SAX4BPM suite consists of\na set of services and a central knowledge repository. The functionality of\nthese services is to elicit the various knowledge ingredients that underlie SAX\nexplanations. A key innovative component among these ingredients is the causal\nprocess execution view. In this work, we integrate the framework with an LLM to\nleverage its power to synthesize the various input ingredients for the sake of\nimproved SAX explanations. Since the use of LLMs for SAX is also accompanied by\na certain degree of doubt related to its capacity to adequately fulfill SAX\nalong with its tendency for hallucination and lack of inherent capacity to\nreason, we pursued a methodological evaluation of the perceived quality of the\ngenerated explanations. We developed a designated scale and conducted a\nrigorous user study. Our findings show that the input presented to the LLMs\naided with the guard-railing of its performance, yielding SAX explanations\nhaving better-perceived fidelity. This improvement is moderated by the\nperception of trust and curiosity. More so, this improvement comes at the cost\nof the perceived interpretability of the explanation.",
    "categories": [
      "cs.AI",
      "68T01"
    ],
    "primary_category": "cs.AI",
    "comment": "42 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.12846v4",
    "published_date": "2024-01-23 15:29:26 UTC",
    "updated_date": "2025-01-29 12:15:49 UTC"
  },
  {
    "arxiv_id": "2401.12835v1",
    "title": "SGTR+: End-to-end Scene Graph Generation with Transformer",
    "authors": [
      "Rongjie Li",
      "Songyang Zhang",
      "Xuming He"
    ],
    "abstract": "Scene Graph Generation (SGG) remains a challenging visual understanding task\ndue to its compositional property. Most previous works adopt a bottom-up,\ntwo-stage or point-based, one-stage approach, which often suffers from high\ntime complexity or suboptimal designs. In this work, we propose a novel SGG\nmethod to address the aforementioned issues, formulating the task as a\nbipartite graph construction problem. To address the issues above, we create a\ntransformer-based end-to-end framework to generate the entity and entity-aware\npredicate proposal set, and infer directed edges to form relation triplets.\nMoreover, we design a graph assembling module to infer the connectivity of the\nbipartite scene graph based on our entity-aware structure, enabling us to\ngenerate the scene graph in an end-to-end manner. Based on bipartite graph\nassembling paradigm, we further propose a new technical design to address the\nefficacy of entity-aware modeling and optimization stability of graph\nassembling. Equipped with the enhanced entity-aware design, our method achieves\noptimal performance and time-complexity. Extensive experimental results show\nthat our design is able to achieve the state-of-the-art or comparable\nperformance on three challenging benchmarks, surpassing most of the existing\napproaches and enjoying higher efficiency in inference. Code is available:\nhttps://github.com/Scarecrow0/SGTR",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by TPAMI: https://ieeexplore.ieee.org/document/10315230",
    "pdf_url": "http://arxiv.org/pdf/2401.12835v1",
    "published_date": "2024-01-23 15:18:20 UTC",
    "updated_date": "2024-01-23 15:18:20 UTC"
  },
  {
    "arxiv_id": "2401.12830v2",
    "title": "Enhancing Next Destination Prediction: A Novel Long Short-Term Memory Neural Network Approach Using Real-World Airline Data",
    "authors": [
      "Salih Salihoglu",
      "Gulser Koksal",
      "Orhan Abar"
    ],
    "abstract": "In the modern transportation industry, accurate prediction of travelers' next\ndestinations brings multiple benefits to companies, such as customer\nsatisfaction and targeted marketing. This study focuses on developing a precise\nmodel that captures the sequential patterns and dependencies in travel data,\nenabling accurate predictions of individual travelers' future destinations. To\nachieve this, a novel model architecture with a sliding window approach based\non Long Short-Term Memory (LSTM) is proposed for destination prediction in the\ntransportation industry. The experimental results highlight satisfactory\nperformance and high scores achieved by the proposed model across different\ndata sizes and performance metrics. This research contributes to advancing\ndestination prediction methods, empowering companies to deliver personalized\nrecommendations and optimize customer experiences in the dynamic travel\nlandscape.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12830v2",
    "published_date": "2024-01-23 15:07:49 UTC",
    "updated_date": "2024-09-16 14:40:16 UTC"
  },
  {
    "arxiv_id": "2401.12822v1",
    "title": "Deep Learning Based Simulators for the Phosphorus Removal Process Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms",
    "authors": [
      "Esmaeel Mohammadi",
      "Mikkel Stokholm-Bjerregaard",
      "Aviaja Anna Hansen",
      "Per Halkjær Nielsen",
      "Daniel Ortiz-Arroyo",
      "Petar Durdevic"
    ],
    "abstract": "Phosphorus removal is vital in wastewater treatment to reduce reliance on\nlimited resources. Deep reinforcement learning (DRL) is a machine learning\ntechnique that can optimize complex and nonlinear systems, including the\nprocesses in wastewater treatment plants, by learning control policies through\ntrial and error. However, applying DRL to chemical and biological processes is\nchallenging due to the need for accurate simulators. This study trained six\nmodels to identify the phosphorus removal process and used them to create a\nsimulator for the DRL environment. Although the models achieved high accuracy\n(>97%), uncertainty and incorrect prediction behavior limited their performance\nas simulators over longer horizons. Compounding errors in the models'\npredictions were identified as one of the causes of this problem. This approach\nfor improving process control involves creating simulation environments for DRL\nalgorithms, using data from supervisory control and data acquisition (SCADA)\nsystems with a sufficient historical horizon without complex system modeling or\nparameter estimation.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Journal Paper",
    "pdf_url": "http://arxiv.org/pdf/2401.12822v1",
    "published_date": "2024-01-23 14:55:46 UTC",
    "updated_date": "2024-01-23 14:55:46 UTC"
  },
  {
    "arxiv_id": "2401.12819v1",
    "title": "Dynamic Layer Tying for Parameter-Efficient Transformers",
    "authors": [
      "Tamir David Hay",
      "Lior Wolf"
    ],
    "abstract": "In the pursuit of reducing the number of trainable parameters in deep\ntransformer networks, we employ Reinforcement Learning to dynamically select\nlayers during training and tie them together. Every few iterations, the RL\nagent is asked whether to train each layer $i$ independently or to copy the\nweights of a previous layer $j<i$. This facilitates weight sharing, reduces the\nnumber of trainable parameters, and also serves as an effective regularization\ntechnique. Experimental evaluations validate that our model modestly\noutperforms the baseline transformer model with regard to perplexity and\ndrastically reduces the number of trainable parameters. In particular, the\nmemory consumption during training is up to one order of magnitude less than\nthe conventional training method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12819v1",
    "published_date": "2024-01-23 14:53:20 UTC",
    "updated_date": "2024-01-23 14:53:20 UTC"
  },
  {
    "arxiv_id": "2401.12806v2",
    "title": "Binary structured physics-informed neural networks for solving equations with rapidly changing solutions",
    "authors": [
      "Yanzhi Liu",
      "Ruifan Wu",
      "Ying Jiang"
    ],
    "abstract": "Physics-informed neural networks (PINNs), rooted in deep learning, have\nemerged as a promising approach for solving partial differential equations\n(PDEs). By embedding the physical information described by PDEs into\nfeedforward neural networks, PINNs are trained as surrogate models to\napproximate solutions without the need for label data. Nevertheless, even\nthough PINNs have shown remarkable performance, they can face difficulties,\nespecially when dealing with equations featuring rapidly changing solutions.\nThese difficulties encompass slow convergence, susceptibility to becoming\ntrapped in local minima, and reduced solution accuracy. To address these\nissues, we propose a binary structured physics-informed neural network (BsPINN)\nframework, which employs binary structured neural network (BsNN) as the neural\nnetwork component. By leveraging a binary structure that reduces inter-neuron\nconnections compared to fully connected neural networks, BsPINNs excel in\ncapturing the local features of solutions more effectively and efficiently.\nThese features are particularly crucial for learning the rapidly changing in\nthe nature of solutions. In a series of numerical experiments solving Burgers\nequation, Euler equation, Helmholtz equation, and high-dimension Poisson\nequation, BsPINNs exhibit superior convergence speed and heightened accuracy\ncompared to PINNs. From these experiments, we discover that BsPINNs resolve the\nissues caused by increased hidden layers in PINNs resulting in over-smoothing,\nand prevent the decline in accuracy due to non-smoothness of PDEs solutions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12806v2",
    "published_date": "2024-01-23 14:37:51 UTC",
    "updated_date": "2024-01-25 12:53:39 UTC"
  },
  {
    "arxiv_id": "2401.12783v1",
    "title": "A Review of Deep Learning Methods for Photoplethysmography Data",
    "authors": [
      "Guangkun Nie",
      "Jiabao Zhu",
      "Gongzheng Tang",
      "Deyun Zhang",
      "Shijia Geng",
      "Qinghao Zhao",
      "Shenda Hong"
    ],
    "abstract": "Photoplethysmography (PPG) is a highly promising device due to its advantages\nin portability, user-friendly operation, and non-invasive capabilities to\nmeasure a wide range of physiological information. Recent advancements in deep\nlearning have demonstrated remarkable outcomes by leveraging PPG signals for\ntasks related to personal health management and other multifaceted\napplications. In this review, we systematically reviewed papers that applied\ndeep learning models to process PPG data between January 1st of 2017 and July\n31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed\nfrom three key perspectives: tasks, models, and data. We finally extracted 193\npapers where different deep learning frameworks were used to process PPG\nsignals. Based on the tasks addressed in these papers, we categorized them into\ntwo major groups: medical-related, and non-medical-related. The medical-related\ntasks were further divided into seven subgroups, including blood pressure\nanalysis, cardiovascular monitoring and diagnosis, sleep health, mental health,\nrespiratory monitoring and analysis, blood glucose analysis, as well as others.\nThe non-medical-related tasks were divided into four subgroups, which encompass\nsignal processing, biometric identification, electrocardiogram reconstruction,\nand human activity recognition. In conclusion, significant progress has been\nmade in the field of using deep learning methods to process PPG data recently.\nThis allows for a more thorough exploration and utilization of the information\ncontained in PPG signals. However, challenges remain, such as limited quantity\nand quality of publicly available databases, a lack of effective validation in\nreal-world scenarios, and concerns about the interpretability, scalability, and\ncomplexity of deep learning models. Moreover, there are still emerging research\nareas that require further investigation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12783v1",
    "published_date": "2024-01-23 14:11:29 UTC",
    "updated_date": "2024-01-23 14:11:29 UTC"
  },
  {
    "arxiv_id": "2401.12771v1",
    "title": "Deep Learning-based Intraoperative MRI Reconstruction",
    "authors": [
      "Jon André Ottesen",
      "Tryggve Storas",
      "Svein Are Sirirud Vatnehol",
      "Grethe Løvland",
      "Einar O. Vik-Mo",
      "Till Schellhorn",
      "Karoline Skogen",
      "Christopher Larsson",
      "Atle Bjørnerud",
      "Inge Rasmus Groote-Eindbaas",
      "Matthan W. A. Caan"
    ],
    "abstract": "Purpose: To evaluate the quality of deep learning reconstruction for\nprospectively accelerated intraoperative magnetic resonance imaging (iMRI)\nduring resective brain tumor surgery.\n  Materials and Methods: Accelerated iMRI was performed during brain surgery\nusing dual surface coils positioned around the area of resection. A deep\nlearning (DL) model was trained on the fastMRI neuro dataset to mimic the data\nfrom the iMRI protocol. Evaluation was performed on imaging material from 40\npatients imaged between 01.11.2021 - 01.06.2023 that underwent iMRI during\ntumor resection surgery. A comparative analysis was conducted between the\nconventional compressed sense (CS) method and the trained DL reconstruction\nmethod. Blinded evaluation of multiple image quality metrics was performed by\ntwo working neuro-radiologists and a working neurosurgeon on a 1 to 5 Likert\nscale (1=non diagnostic, 2=poor, 3=acceptable, 4=good, 5=excellent), and the\nfavored reconstruction variant.\n  Results: The DL reconstruction was strongly favored or favored over the CS\nreconstruction for 33/40, 39/40, and 8/40 of cases for reader 1, 2, and 3,\nrespectively. Two of three readers consistently assigned higher ratings for the\nDL reconstructions, and the DL reconstructions had a higher score than their\nrespective CS counterparts for 72%, 72%, and 14% of the cases for reader 1, 2,\nand 3, respectively. Still, the DL reconstructions exhibited shortcomings such\nas a striping artifact and reduced signal.\n  Conclusion: DL shows promise to allow for high-quality reconstructions of\nintraoperative MRI with equal to or improved perceived spatial resolution,\nsignal-to-noise ratio, diagnostic confidence, diagnostic conspicuity, and\nspatial resolution compared to compressed sense.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12771v1",
    "published_date": "2024-01-23 13:57:50 UTC",
    "updated_date": "2024-01-23 13:57:50 UTC"
  },
  {
    "arxiv_id": "2401.12756v2",
    "title": "What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition",
    "authors": [
      "Carolin Holtermann",
      "Markus Frohmann",
      "Navid Rekabsaz",
      "Anne Lauscher"
    ],
    "abstract": "The knowledge encapsulated in a model is the core factor determining its\nfinal performance on downstream tasks. Much research in NLP has focused on\nefficient methods for storing and adapting different types of knowledge, e.g.,\nin dedicated modularized structures, and on how to effectively combine these,\ne.g., by learning additional parameters. However, given the many possible\noptions, a thorough understanding of the mechanisms involved in these\ncompositions is missing, and hence it remains unclear which strategies to\nutilize. To address this research gap, we propose a novel framework for\nzero-shot module composition, which encompasses existing and some novel\nvariations for selecting, weighting, and combining parameter modules under a\nsingle unified notion. Focusing on the scenario of domain knowledge and adapter\nlayers, our framework provides a systematic unification of concepts, allowing\nus to conduct the first comprehensive benchmarking study of various zero-shot\nknowledge composition strategies. In particular, we test two module combination\nmethods and five selection and weighting strategies for their effectiveness and\nefficiency in an extensive experimental setup. Our results highlight the\nefficacy of ensembling but also hint at the power of simple though\noften-ignored weighting methods. Further in-depth analyses allow us to\nunderstand the role of weighting vs. top-k selection, and show that, to a\ncertain extent, the performance of adapter composition can even be predicted.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Findings of the ACL: EACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.12756v2",
    "published_date": "2024-01-23 13:35:47 UTC",
    "updated_date": "2024-01-25 14:32:36 UTC"
  },
  {
    "arxiv_id": "2401.12731v4",
    "title": "The Distributional Uncertainty of the SHAP score in Explainable Machine Learning",
    "authors": [
      "Santiago Cifuentes",
      "Leopoldo Bertossi",
      "Nina Pardal",
      "Sergio Abriola",
      "Maria Vanina Martinez",
      "Miguel Romero"
    ],
    "abstract": "Attribution scores reflect how important the feature values in an input\nentity are for the output of a machine learning model. One of the most popular\nattribution scores is the SHAP score, which is an instantiation of the general\nShapley value used in coalition game theory. The definition of this score\nrelies on a probability distribution on the entity population. Since the exact\ndistribution is generally unknown, it needs to be assigned subjectively or be\nestimated from data, which may lead to misleading feature scores. In this\npaper, we propose a principled framework for reasoning on SHAP scores under\nunknown entity population distributions. In our framework, we consider an\nuncertainty region that contains the potential distributions, and the SHAP\nscore of a feature becomes a function defined over this region. We study the\nbasic problems of finding maxima and minima of this function, which allows us\nto determine tight ranges for the SHAP scores of all features. In particular,\nwe pinpoint the complexity of these problems, and other related ones, showing\nthem to be NP-complete. Finally, we present experiments on a real-world\ndataset, showing that our framework may contribute to a more robust feature\nscoring.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "68T37, 68T27"
    ],
    "primary_category": "cs.AI",
    "comment": "In ECAI 2024 proceedings",
    "pdf_url": "http://arxiv.org/pdf/2401.12731v4",
    "published_date": "2024-01-23 13:04:02 UTC",
    "updated_date": "2024-08-13 16:12:09 UTC"
  },
  {
    "arxiv_id": "2401.13708v1",
    "title": "Accelerating hyperbolic t-SNE",
    "authors": [
      "Martin Skrodzki",
      "Hunter van Geffen",
      "Nicolas F. Chaves-de-Plaza",
      "Thomas Höllt",
      "Elmar Eisemann",
      "Klaus Hildebrandt"
    ],
    "abstract": "The need to understand the structure of hierarchical or high-dimensional data\nis present in a variety of fields. Hyperbolic spaces have proven to be an\nimportant tool for embedding computations and analysis tasks as their\nnon-linear nature lends itself well to tree or graph data. Subsequently, they\nhave also been used in the visualization of high-dimensional data, where they\nexhibit increased embedding performance. However, none of the existing\ndimensionality reduction methods for embedding into hyperbolic spaces scale\nwell with the size of the input data. That is because the embeddings are\ncomputed via iterative optimization schemes and the computation cost of every\niteration is quadratic in the size of the input. Furthermore, due to the\nnon-linear nature of hyperbolic spaces, Euclidean acceleration structures\ncannot directly be translated to the hyperbolic setting. This paper introduces\nthe first acceleration structure for hyperbolic embeddings, building upon a\npolar quadtree. We compare our approach with existing methods and demonstrate\nthat it computes embeddings of similar quality in significantly less time.\nImplementation and scripts for the experiments can be found at\nhttps://graphics.tudelft.nl/accelerating-hyperbolic-tsne.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "q-bio.QM",
      "stat.ML"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13708v1",
    "published_date": "2024-01-23 12:59:40 UTC",
    "updated_date": "2024-01-23 12:59:40 UTC"
  },
  {
    "arxiv_id": "2402.00046v2",
    "title": "Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning",
    "authors": [
      "Sofiene Lassoued",
      "Andreas Schwung"
    ],
    "abstract": "Resource utilization and production process optimization are crucial for\ncompanies in today's competitive industrial landscape. Addressing the\ncomplexities of job shop scheduling problems (JSSP) is essential to improving\nproductivity, reducing costs, and ensuring timely delivery. We propose PetriRL,\na novel framework integrating Petri nets and deep reinforcement learning (DRL)\nfor JSSP optimization. PetriRL capitalizes on the inherent strengths of Petri\nnets in modelling discrete event systems while leveraging the advantages of a\ngraph structure. The Petri net governs automated components of the process,\nensuring adherence to JSSP constraints. This allows for synergistic\ncollaboration with optimization algorithms such as DRL, particularly in\ncritical decision-making. Unlike traditional methods, PetriRL eliminates the\nneed to preprocess JSSP instances into disjunctive graphs and enhances the\nexplainability of process status through its graphical structure based on\nplaces and transitions. Additionally, the inherent graph structure of Petri\nnets enables the dynamic additions of job operations during the inference phase\nwithout requiring agent retraining, thus enhancing flexibility. Experimental\nresults demonstrate PetriRL's robust generalization across various instance\nsizes and its competitive performance on public test benchmarks and randomly\ngenerated instances. Results are compared to a wide range of optimization\nsolutions such as heuristics, metaheuristics, and learning-based algorithms.\nFinally, the added values of the framework's key elements, such as event-based\ncontrol and action masking, are studied in the ablation study.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.00046v2",
    "published_date": "2024-01-23 12:30:49 UTC",
    "updated_date": "2024-05-08 10:47:57 UTC"
  },
  {
    "arxiv_id": "2401.12714v1",
    "title": "Evaluation of large language models for assessing code maintainability",
    "authors": [
      "Marc Dillmann",
      "Julien Siebert",
      "Adam Trendowicz"
    ],
    "abstract": "Increased availability of open-source software repositories and recent\nadvances in code analysis using large language models (LLMs) has triggered a\nwave of new work to automate software engineering tasks that were previously\nvery difficult to automate. In this paper, we investigate a recent line of work\nthat hypothesises that comparing the probability of code generated by LLMs with\nthe probability the current code would have had can indicate potential quality\nproblems. We investigate the association between the cross-entropy of code\ngenerated by ten different models (based on GPT2 and Llama2) and the following\nquality aspects: readability, understandability, complexity, modularisation,\nand overall maintainability assessed by experts and available in an benchmark\ndataset. Our results show that, controlling for the number of logical lines of\ncodes (LLOC), cross-entropy computed by LLMs is indeed a predictor of\nmaintainability on a class level (the higher the cross-entropy the lower the\nmaintainability). However, this relation is reversed when one does not control\nfor LLOC (e.g., comparing small classes with longer ones). Furthermore, while\nthe complexity of LLMs affects the range of cross-entropy (smaller models tend\nto have a wider range of cross-entropy), this plays a significant role in\npredicting maintainability aspects. Our study limits itself on ten different\npretrained models (based on GPT2 and Llama2) and on maintainability aspects\ncollected by Schnappinger et al. When controlling for logical lines of code\n(LLOC), cross-entropy is a predictor of maintainability. However, while related\nwork has shown the potential usefulness of cross-entropy at the level of tokens\nor short sequences, at the class level this criterion alone may prove\ninsufficient to predict maintainability and further research is needed to make\nbest use of this information in practice.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "68",
      "D.2.7"
    ],
    "primary_category": "cs.SE",
    "comment": "14 pages, 4 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.12714v1",
    "published_date": "2024-01-23 12:29:42 UTC",
    "updated_date": "2024-01-23 12:29:42 UTC"
  },
  {
    "arxiv_id": "2401.12708v2",
    "title": "Deep Neural Network Benchmarks for Selective Classification",
    "authors": [
      "Andrea Pugnana",
      "Lorenzo Perini",
      "Jesse Davis",
      "Salvatore Ruggieri"
    ],
    "abstract": "With the increasing deployment of machine learning models in many socially\nsensitive tasks, there is a growing demand for reliable and trustworthy\npredictions. One way to accomplish these requirements is to allow a model to\nabstain from making a prediction when there is a high risk of making an error.\nThis requires adding a selection mechanism to the model, which selects those\nexamples for which the model will provide a prediction. The selective\nclassification framework aims to design a mechanism that balances the fraction\nof rejected predictions (i.e., the proportion of examples for which the model\ndoes not make a prediction) versus the improvement in predictive performance on\nthe selected predictions. Multiple selective classification frameworks exist,\nmost of which rely on deep neural network architectures. However, the empirical\nevaluation of the existing approaches is still limited to partial comparisons\namong methods and settings, providing practitioners with little insight into\ntheir relative merits. We fill this gap by benchmarking 18 baselines on a\ndiverse set of 44 datasets that includes both image and tabular data. Moreover,\nthere is a mix of binary and multiclass tasks. We evaluate these approaches\nusing several criteria, including selective error rate, empirical coverage,\ndistribution of rejected instance's classes, and performance on\nout-of-distribution instances. The results indicate that there is not a single\nclear winner among the surveyed baselines, and the best method depends on the\nusers' objectives.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in The Journal of Data centric Machine Learning Research\n  (DMLR), Vol 1, (17):1-58 (2024)",
    "pdf_url": "http://arxiv.org/pdf/2401.12708v2",
    "published_date": "2024-01-23 12:15:47 UTC",
    "updated_date": "2024-09-18 07:48:33 UTC"
  },
  {
    "arxiv_id": "2401.12700v1",
    "title": "Securing Recommender System via Cooperative Training",
    "authors": [
      "Qingyang Wang",
      "Chenwang Wu",
      "Defu Lian",
      "Enhong Chen"
    ],
    "abstract": "Recommender systems are often susceptible to well-crafted fake profiles,\nleading to biased recommendations. Among existing defense methods,\ndata-processing-based methods inevitably exclude normal samples, while\nmodel-based methods struggle to enjoy both generalization and robustness. To\nthis end, we suggest integrating data processing and the robust model to\npropose a general framework, Triple Cooperative Defense (TCD), which employs\nthree cooperative models that mutually enhance data and thereby improve\nrecommendation robustness. Furthermore, Considering that existing attacks\nstruggle to balance bi-level optimization and efficiency, we revisit poisoning\nattacks in recommender systems and introduce an efficient attack strategy,\nCo-training Attack (Co-Attack), which cooperatively optimizes the attack\noptimization and model training, considering the bi-level setting while\nmaintaining attack efficiency. Moreover, we reveal a potential reason for the\ninsufficient threat of existing attacks is their default assumption of\noptimizing attacks in undefended scenarios. This overly optimistic setting\nlimits the potential of attacks. Consequently, we put forth a Game-based\nCo-training Attack (GCoAttack), which frames the proposed CoAttack and TCD as a\ngame-theoretic process, thoroughly exploring CoAttack's attack potential in the\ncooperative training of attack and defense. Extensive experiments on three real\ndatasets demonstrate TCD's superiority in enhancing model robustness.\nAdditionally, we verify that the two proposed attack strategies significantly\noutperform existing attacks, with game-based GCoAttack posing a greater\npoisoning threat than CoAttack.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2210.13762",
    "pdf_url": "http://arxiv.org/pdf/2401.12700v1",
    "published_date": "2024-01-23 12:07:20 UTC",
    "updated_date": "2024-01-23 12:07:20 UTC"
  },
  {
    "arxiv_id": "2401.12689v3",
    "title": "Energy-based Automated Model Evaluation",
    "authors": [
      "Ru Peng",
      "Heming Zou",
      "Haobo Wang",
      "Yawen Zeng",
      "Zenan Huang",
      "Junbo Zhao"
    ],
    "abstract": "The conventional evaluation protocols on machine learning models rely heavily\non a labeled, i.i.d-assumed testing dataset, which is not often present in real\nworld applications. The Automated Model Evaluation (AutoEval) shows an\nalternative to this traditional workflow, by forming a proximal prediction\npipeline of the testing performance without the presence of ground-truth\nlabels. Despite its recent successes, the AutoEval frameworks still suffer from\nan overconfidence issue, substantial storage and computational cost. In that\nregard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that\nallows the AutoEval framework to be both more efficient and effective. The core\nof the MDE is to establish a meta-distribution statistic, on the information\n(energy) associated with individual samples, then offer a smoother\nrepresentation enabled by energy-based learning. We further provide our\ntheoretical insights by connecting the MDE with the classification loss. We\nprovide extensive experiments across modalities, datasets and different\narchitectural backbones to validate MDE's validity, together with its\nsuperiority compared with prior approaches. We also prove MDE's versatility by\nshowing its seamless integration with large-scale models, and easy adaption to\nlearning scenarios with noisy- or imbalanced- labels. Code and data are\navailable: https://github.com/pengr/Energy_AutoEval",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR2024 poster paper",
    "pdf_url": "http://arxiv.org/pdf/2401.12689v3",
    "published_date": "2024-01-23 11:54:09 UTC",
    "updated_date": "2024-03-15 06:51:28 UTC"
  },
  {
    "arxiv_id": "2401.12686v2",
    "title": "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
    "authors": [
      "Christian Fabian",
      "Kai Cui",
      "Heinz Koeppl"
    ],
    "abstract": "Learning the behavior of large agent populations is an important task for\nnumerous research areas. Although the field of multi-agent reinforcement\nlearning (MARL) has made significant progress towards solving these systems,\nsolutions for many agents often remain computationally infeasible and lack\ntheoretical guarantees. Mean Field Games (MFGs) address both of these issues\nand can be extended to Graphon MFGs (GMFGs) to include network structures\nbetween agents. Despite their merits, the real world applicability of GMFGs is\nlimited by the fact that graphons only capture dense graphs. Since most\nempirically observed networks show some degree of sparsity, such as power law\ngraphs, the GMFG framework is insufficient for capturing these network\ntopologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which\nbuilds on the graph theoretical concept of graphexes. Graphexes are the\nlimiting objects to sparse graph sequences that also have other desirable\nfeatures such as the small world property. Learning equilibria in these games\nis challenging due to the rich and sparse structure of the underlying graphs.\nTo tackle these challenges, we design a new learning algorithm tailored to the\nGXMFG setup. This hybrid graphex learning approach leverages that the system\nmainly consists of a highly connected core and a sparse periphery. After\ndefining the system and providing a theoretical analysis, we state our learning\napproach and demonstrate its learning capabilities on both synthetic graphs and\nreal-world networks. This comparison shows that our GXMFG learning algorithm\nsuccessfully extends MFGs to a highly relevant class of hard, realistic\nlearning problems that are not accurately addressed by current MARL and MFG\nmethods.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "accepted at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.12686v2",
    "published_date": "2024-01-23 11:52:00 UTC",
    "updated_date": "2024-02-23 10:04:14 UTC"
  },
  {
    "arxiv_id": "2401.12681v1",
    "title": "Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical Learning",
    "authors": [
      "Zhishuai Li",
      "Yunhao Nie",
      "Ziyue Li",
      "Lei Bai",
      "Yisheng Lv",
      "Rui Zhao"
    ],
    "abstract": "Kriging aims at estimating the attributes of unsampled geo-locations from\nobservations in the spatial vicinity or physical connections, which helps\nmitigate skewed monitoring caused by under-deployed sensors. Existing works\nassume that neighbors' information offers the basis for estimating the\nattributes of the unobserved target while ignoring non-neighbors. However,\nnon-neighbors could also offer constructive information, and neighbors could\nalso be misleading. To this end, we propose ``Contrastive-Prototypical''\nself-supervised learning for Kriging (KCP) to refine valuable information from\nneighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we\nconduct the Kriging task from a new perspective of representation: we aim to\nfirst learn robust and general representations and then recover attributes from\nrepresentations. A neighboring contrastive module is designed that coarsely\nlearns the representations by narrowing the representation distance between the\ntarget and its neighbors while pushing away the non-neighbors. In parallel, a\nprototypical module is introduced to identify similar representations via\nexchanged prediction, thus refining the misleading neighbors and recycling the\nuseful non-neighbors from the neighboring contrast component. As a result, not\nall the neighbors and some of the non-neighbors will be used to infer the\ntarget. To encourage the two modules above to learn general and robust\nrepresentations, we design an adaptive augmentation module that incorporates\ndata-driven attribute augmentation and centrality-based topology augmentation\nover the spatiotemporal Kriging graph data. Extensive experiments on real-world\ndatasets demonstrate the superior performance of KCP compared to its peers with\n6% improvements and exceptional transferability and robustness. The code is\navailable at https://github.com/bonaldli/KCP",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in AISTATS 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.12681v1",
    "published_date": "2024-01-23 11:46:31 UTC",
    "updated_date": "2024-01-23 11:46:31 UTC"
  },
  {
    "arxiv_id": "2401.12672v1",
    "title": "ChatGraph: Chat with Your Graphs",
    "authors": [
      "Yun Peng",
      "Sen Lin",
      "Qian Chen",
      "Lyu Xu",
      "Xiaojun Ren",
      "Yafei Li",
      "Jianliang Xu"
    ],
    "abstract": "Graph analysis is fundamental in real-world applications. Traditional\napproaches rely on SPARQL-like languages or clicking-and-dragging interfaces to\ninteract with graph data. However, these methods either require users to\npossess high programming skills or support only a limited range of graph\nanalysis functionalities. To address the limitations, we propose a large\nlanguage model (LLM)-based framework called ChatGraph. With ChatGraph, users\ncan interact with graphs through natural language, making it easier to use and\nmore flexible than traditional approaches. The core of ChatGraph lies in\ngenerating chains of graph analysis APIs based on the understanding of the\ntexts and graphs inputted in the user prompts. To achieve this, ChatGraph\nconsists of three main modules: an API retrieval module that searches for\nrelevant APIs, a graph-aware LLM module that enables the LLM to comprehend\ngraphs, and an API chain-oriented finetuning module that guides the LLM in\ngenerating API chains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12672v1",
    "published_date": "2024-01-23 11:29:19 UTC",
    "updated_date": "2024-01-23 11:29:19 UTC"
  },
  {
    "arxiv_id": "2401.12666v1",
    "title": "EL-VIT: Probing Vision Transformer with Interactive Visualization",
    "authors": [
      "Hong Zhou",
      "Rui Zhang",
      "Peifeng Lai",
      "Chaoran Guo",
      "Yong Wang",
      "Zhida Sun",
      "Junjie Li"
    ],
    "abstract": "Nowadays, Vision Transformer (ViT) is widely utilized in various computer\nvision tasks, owing to its unique self-attention mechanism. However, the model\narchitecture of ViT is complex and often challenging to comprehend, leading to\na steep learning curve. ViT developers and users frequently encounter\ndifficulties in interpreting its inner workings. Therefore, a visualization\nsystem is needed to assist ViT users in understanding its functionality. This\npaper introduces EL-VIT, an interactive visual analytics system designed to\nprobe the Vision Transformer and facilitate a better understanding of its\noperations. The system consists of four layers of visualization views. The\nfirst three layers include model overview, knowledge background graph, and\nmodel detail view. These three layers elucidate the operation process of ViT\nfrom three perspectives: the overall model architecture, detailed explanation,\nand mathematical operations, enabling users to understand the underlying\nprinciples and the transition process between layers. The fourth interpretation\nview helps ViT users and experts gain a deeper understanding by calculating the\ncosine similarity between patches. Our two usage scenarios demonstrate the\neffectiveness and usability of EL-VIT in helping ViT users understand the\nworking mechanism of ViT.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 7 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2401.12666v1",
    "published_date": "2024-01-23 11:21:32 UTC",
    "updated_date": "2024-01-23 11:21:32 UTC"
  },
  {
    "arxiv_id": "2401.12665v2",
    "title": "ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation",
    "authors": [
      "Shengze Li",
      "Jianjian Cao",
      "Peng Ye",
      "Yuhan Ding",
      "Chongjun Tu",
      "Tao Chen"
    ],
    "abstract": "Recently, foundational models such as CLIP and SAM have shown promising\nperformance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However,\neither CLIP-based or SAM-based ZSAS methods still suffer from non-negligible\nkey drawbacks: 1) CLIP primarily focuses on global feature alignment across\ndifferent inputs, leading to imprecise segmentation of local anomalous parts;\n2) SAM tends to generate numerous redundant masks without proper prompt\nconstraints, resulting in complex post-processing requirements. In this work,\nwe innovatively propose a CLIP and SAM collaboration framework called ClipSAM\nfor ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding\ncapability for anomaly localization and rough segmentation, which is further\nused as the prompt constraints for SAM to refine the anomaly segmentation\nresults. In details, we introduce a crucial Unified Multi-scale Cross-modal\nInteraction (UMCI) module for interacting language with visual features at\nmultiple scales of CLIP to reason anomaly positions. Then, we design a novel\nMulti-level Mask Refinement (MMR) module, which utilizes the positional\ninformation as multi-level prompts for SAM to acquire hierarchical levels of\nmasks and merges them. Extensive experiments validate the effectiveness of our\napproach, achieving the optimal segmentation performance on the MVTec-AD and\nVisA datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages,17 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.12665v2",
    "published_date": "2024-01-23 11:20:03 UTC",
    "updated_date": "2024-01-29 10:57:38 UTC"
  },
  {
    "arxiv_id": "2401.12662v1",
    "title": "Integrating Human Expertise in Continuous Spaces: A Novel Interactive Bayesian Optimization Framework with Preference Expected Improvement",
    "authors": [
      "Nikolaus Feith",
      "Elmar Rueckert"
    ],
    "abstract": "Interactive Machine Learning (IML) seeks to integrate human expertise into\nmachine learning processes. However, most existing algorithms cannot be applied\nto Realworld Scenarios because their state spaces and/or action spaces are\nlimited to discrete values. Furthermore, the interaction of all existing\nmethods is restricted to deciding between multiple proposals. We therefore\npropose a novel framework based on Bayesian Optimization (BO). Interactive\nBayesian Optimization (IBO) enables collaboration between machine learning\nalgorithms and humans. This framework captures user preferences and provides an\ninterface for users to shape the strategy by hand. Additionally, we've\nincorporated a new acquisition function, Preference Expected Improvement (PEI),\nto refine the system's efficiency using a probabilistic model of the user\npreferences. Our approach is geared towards ensuring that machines can benefit\nfrom human expertise, aiming for a more aligned and effective learning process.\nIn the course of this work, we applied our method to simulations and in a real\nworld task using a Franka Panda robot to show human-robot collaboration.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12662v1",
    "published_date": "2024-01-23 11:14:59 UTC",
    "updated_date": "2024-01-23 11:14:59 UTC"
  },
  {
    "arxiv_id": "2401.12646v1",
    "title": "Emergent Cooperation under Uncertain Incentive Alignment",
    "authors": [
      "Nicole Orzan",
      "Erman Acar",
      "Davide Grossi",
      "Roxana Rădulescu"
    ],
    "abstract": "Understanding the emergence of cooperation in systems of computational agents\nis crucial for the development of effective cooperative AI. Interaction among\nindividuals in real-world settings are often sparse and occur within a broad\nspectrum of incentives, which often are only partially known. In this work, we\nexplore how cooperation can arise among reinforcement learning agents in\nscenarios characterised by infrequent encounters, and where agents face\nuncertainty about the alignment of their incentives with those of others. To do\nso, we train the agents under a wide spectrum of environments ranging from\nfully competitive, to fully cooperative, to mixed-motives. Under this type of\nuncertainty we study the effects of mechanisms, such as reputation and\nintrinsic rewards, that have been proposed in the literature to foster\ncooperation in mixed-motives environments. Our findings show that uncertainty\nsubstantially lowers the agents' ability to engage in cooperative behaviour,\nwhen that would be the best course of action. In this scenario, the use of\neffective reputation mechanisms and intrinsic rewards boosts the agents'\ncapability to act nearly-optimally in cooperative environments, while greatly\nenhancing cooperation in mixed-motive environments as well.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12646v1",
    "published_date": "2024-01-23 10:55:54 UTC",
    "updated_date": "2024-01-23 10:55:54 UTC"
  },
  {
    "arxiv_id": "2401.12632v1",
    "title": "Modeling Resilience of Collaborative AI Systems",
    "authors": [
      "Diaeddin Rimawi",
      "Antonio Liotta",
      "Marco Todescato",
      "Barbara Russo"
    ],
    "abstract": "A Collaborative Artificial Intelligence System (CAIS) performs actions in\ncollaboration with the human to achieve a common goal. CAISs can use a trained\nAI model to control human-system interaction, or they can use human interaction\nto dynamically learn from humans in an online fashion. In online learning with\nhuman feedback, the AI model evolves by monitoring human interaction through\nthe system sensors in the learning state, and actuates the autonomous\ncomponents of the CAIS based on the learning in the operational state.\nTherefore, any disruptive event affecting these sensors may affect the AI\nmodel's ability to make accurate decisions and degrade the CAIS performance.\nConsequently, it is of paramount importance for CAIS managers to be able to\nautomatically track the system performance to understand the resilience of the\nCAIS upon such disruptive events. In this paper, we provide a new framework to\nmodel CAIS performance when the system experiences a disruptive event. With our\nframework, we introduce a model of performance evolution of CAIS. The model is\nequipped with a set of measures that aim to support CAIS managers in the\ndecision process to achieve the required resilience of the system. We tested\nour framework on a real-world case study of a robot collaborating online with\nthe human, when the system is experiencing a disruptive event. The case study\nshows that our framework can be adopted in CAIS and integrated into the online\nexecution of the CAIS activities.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.SE",
    "comment": "This paper is accepted at the 3rd International Conference on AI\n  Engineering - Software Engineering for AI (CAIN 2024), Lisbon, Portugal",
    "pdf_url": "http://arxiv.org/pdf/2401.12632v1",
    "published_date": "2024-01-23 10:28:33 UTC",
    "updated_date": "2024-01-23 10:28:33 UTC"
  },
  {
    "arxiv_id": "2401.12631v1",
    "title": "A Reply to Makelov et al. (2023)'s \"Interpretability Illusion\" Arguments",
    "authors": [
      "Zhengxuan Wu",
      "Atticus Geiger",
      "Jing Huang",
      "Aryaman Arora",
      "Thomas Icard",
      "Christopher Potts",
      "Noah D. Goodman"
    ],
    "abstract": "We respond to the recent paper by Makelov et al. (2023), which reviews\nsubspace interchange intervention methods like distributed alignment search\n(DAS; Geiger et al. 2023) and claims that these methods potentially cause\n\"interpretability illusions\". We first review Makelov et al. (2023)'s technical\nnotion of what an \"interpretability illusion\" is, and then we show that even\nintuitive and desirable explanations can qualify as illusions in this sense. As\na result, their method of discovering \"illusions\" can reject explanations they\nconsider \"non-illusory\". We then argue that the illusions Makelov et al. (2023)\nsee in practice are artifacts of their training and evaluation paradigms. We\nclose by emphasizing that, though we disagree with their core characterization,\nMakelov et al. (2023)'s examples and discussion have undoubtedly pushed the\nfield of interpretability forward.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.12631v1",
    "published_date": "2024-01-23 10:27:42 UTC",
    "updated_date": "2024-01-23 10:27:42 UTC"
  },
  {
    "arxiv_id": "2401.12624v2",
    "title": "Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control",
    "authors": [
      "Yongjun Kim",
      "Sejin Seo",
      "Jihong Park",
      "Mehdi Bennis",
      "Seong-Lyun Kim",
      "Junil Choi"
    ],
    "abstract": "In this work, we compare emergent communication (EC) built upon multi-agent\ndeep reinforcement learning (MADRL) and language-oriented semantic\ncommunication (LSC) empowered by a pre-trained large language model (LLM) using\nhuman language. In a multi-agent remote navigation task, with multimodal input\ndata comprising location and channel maps, it is shown that EC incurs high\ntraining cost and struggles when using multimodal data, whereas LSC yields high\ninference computing cost due to the LLM's large size. To address their\nrespective bottlenecks, we propose a novel framework of language-guided EC\n(LEC) by guiding the EC training using LSC via knowledge distillation (KD).\nSimulations corroborate that LEC achieves faster travel time while avoiding\nareas with poor channel conditions, as well as speeding up the MADRL training\nconvergence by up to 61.8% compared to EC.",
    "categories": [
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "cs.NI",
      "math.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12624v2",
    "published_date": "2024-01-23 10:23:13 UTC",
    "updated_date": "2024-03-03 14:15:52 UTC"
  },
  {
    "arxiv_id": "2402.16869v1",
    "title": "Considering Fundamental Rights in the European Standardisation of Artificial Intelligence: Nonsense or Strategic Alliance?",
    "authors": [
      "Marion Ho-Dac"
    ],
    "abstract": "In the European context, both the EU AI Act proposal and the draft\nStandardisation Request on safe and trustworthy AI link standardisation to\nfundamental rights. However, these texts do not provide any guidelines that\nspecify and detail the relationship between AI standards and fundamental\nrights, its meaning or implication. This chapter aims to clarify this critical\nregulatory blind spot. The main issue tackled is whether the adoption of AI\nharmonised standards, based on the future AI Act, should take into account\nfundamental rights. In our view, the response is yes. The high risks posed by\ncertain AI systems relate in particular to infringements of fundamental rights.\nTherefore, mitigating such risks involves fundamental rights considerations and\nthis is what future harmonised standards should reflect. At the same time,\nvalid criticisms of the European standardisation process have to be addressed.\nFinally, the practical incorporation of fundamental rights considerations in\nthe ongoing European standardisation of AI systems is discussed.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16869v1",
    "published_date": "2024-01-23 10:17:42 UTC",
    "updated_date": "2024-01-23 10:17:42 UTC"
  },
  {
    "arxiv_id": "2401.12599v1",
    "title": "Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition",
    "authors": [
      "Demiao Lin"
    ],
    "abstract": "With the rapid development of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) has become a predominant method in the\nfield of professional knowledge-based question answering. Presently, major\nfoundation model companies have opened up Embedding and Chat API interfaces,\nand frameworks like LangChain have already integrated the RAG process. It\nappears that the key models and steps in RAG have been resolved, leading to the\nquestion: are professional knowledge QA systems now approaching perfection?\nThis article discovers that current primary methods depend on the premise of\naccessing high-quality text corpora. However, since professional documents are\nmainly stored in PDFs, the low accuracy of PDF parsing significantly impacts\nthe effectiveness of professional knowledge-based QA. We conducted an empirical\nRAG experiment across hundreds of questions from the corresponding real-world\nprofessional documents. The results show that, ChatDOC, a RAG system equipped\nwith a panoptic and pinpoint PDF parser, retrieves more accurate and complete\nsegments, and thus better answers. Empirical experiments show that ChatDOC is\nsuperior to baseline on nearly 47% of questions, ties for 38% of cases, and\nfalls short on only 15% of cases. It shows that we may revolutionize RAG with\nenhanced PDF structure recognition.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.12599v1",
    "published_date": "2024-01-23 09:54:36 UTC",
    "updated_date": "2024-01-23 09:54:36 UTC"
  },
  {
    "arxiv_id": "2401.12593v1",
    "title": "MOReGIn: Multi-Objective Recommendation at the Global and Individual Levels",
    "authors": [
      "Elizabeth Gómez",
      "David Contreras",
      "Ludovico Boratto",
      "Maria Salamó"
    ],
    "abstract": "Multi-Objective Recommender Systems (MORSs) emerged as a paradigm to\nguarantee multiple (often conflicting) goals. Besides accuracy, a MORS can\noperate at the global level, where additional beyond-accuracy goals are met for\nthe system as a whole, or at the individual level, meaning that the\nrecommendations are tailored to the needs of each user. The state-of-the-art\nMORSs either operate at the global or individual level, without assuming the\nco-existence of the two perspectives. In this study, we show that when global\nand individual objectives co-exist, MORSs are not able to meet both types of\ngoals. To overcome this issue, we present an approach that regulates the\nrecommendation lists so as to guarantee both global and individual\nperspectives, while preserving its effectiveness. Specifically, as individual\nperspective, we tackle genre calibration and, as global perspective, provider\nfairness. We validate our approach on two real-world datasets, publicly\nreleased with this paper.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12593v1",
    "published_date": "2024-01-23 09:48:08 UTC",
    "updated_date": "2024-01-23 09:48:08 UTC"
  },
  {
    "arxiv_id": "2401.12576v2",
    "title": "LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools and Self-Explanations",
    "authors": [
      "Qianli Wang",
      "Tatiana Anikina",
      "Nils Feldhus",
      "Josef van Genabith",
      "Leonhard Hennig",
      "Sebastian Möller"
    ],
    "abstract": "Interpretability tools that offer explanations in the form of a dialogue have\ndemonstrated their efficacy in enhancing users' understanding (Slack et al.,\n2023; Shen et al., 2023), as one-off explanations may fall short in providing\nsufficient information to the user. Current solutions for dialogue-based\nexplanations, however, often require external tools and modules and are not\neasily transferable to tasks they were not designed for. With LLMCheckup, we\npresent an easily accessible tool that allows users to chat with any\nstate-of-the-art large language model (LLM) about its behavior. We enable LLMs\nto generate explanations and perform user intent recognition without\nfine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI)\nmethods, including white-box explainability tools such as feature attributions,\nand self-explanations (e.g., for rationale generation). LLM-based\n(self-)explanations are presented as an interactive dialogue that supports\nfollow-up questions and generates suggestions. LLMCheckupprovides tutorials for\noperations available in the system, catering to individuals with varying levels\nof expertise in XAI and supporting multiple input modalities. We introduce a\nnew parsing strategy that substantially enhances the user intent recognition\naccuracy of the LLM. Finally, we showcase LLMCheckup for the tasks of fact\nchecking and commonsense question answering.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2024 HCI+NLP workshop; camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2401.12576v2",
    "published_date": "2024-01-23 09:11:07 UTC",
    "updated_date": "2024-04-24 17:17:48 UTC"
  },
  {
    "arxiv_id": "2401.12570v1",
    "title": "DiffMoog: a Differentiable Modular Synthesizer for Sound Matching",
    "authors": [
      "Noy Uzrad",
      "Oren Barkan",
      "Almog Elharar",
      "Shlomi Shvartzman",
      "Moshe Laufer",
      "Lior Wolf",
      "Noam Koenigstein"
    ],
    "abstract": "This paper presents DiffMoog - a differentiable modular synthesizer with a\ncomprehensive set of modules typically found in commercial instruments. Being\ndifferentiable, it allows integration into neural networks, enabling automated\nsound matching, to replicate a given audio input. Notably, DiffMoog facilitates\nmodulation capabilities (FM/AM), low-frequency oscillators (LFOs), filters,\nenvelope shapers, and the ability for users to create custom signal chains. We\nintroduce an open-source platform that comprises DiffMoog and an end-to-end\nsound matching framework. This framework utilizes a novel signal-chain loss and\nan encoder network that self-programs its outputs to predict DiffMoogs\nparameters based on the user-defined modular architecture. Moreover, we provide\ninsights and lessons learned towards sound matching using differentiable\nsynthesis. Combining robust sound capabilities with a holistic platform,\nDiffMoog stands as a premier asset for expediting research in audio synthesis\nand machine learning.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 7 figures, 1 table, Our code is released at\n  https://github.com/aisynth/diffmoog",
    "pdf_url": "http://arxiv.org/pdf/2401.12570v1",
    "published_date": "2024-01-23 08:59:21 UTC",
    "updated_date": "2024-01-23 08:59:21 UTC"
  },
  {
    "arxiv_id": "2401.12557v2",
    "title": "Balancing the AI Strength of Roles in Self-Play Training with Regret Matching+",
    "authors": [
      "Xiaoxi Wang"
    ],
    "abstract": "When training artificial intelligence for games encompassing multiple roles,\nthe development of a generalized model capable of controlling any character\nwithin the game presents a viable option. This strategy not only conserves\ncomputational resources and time during the training phase but also reduces\nresource requirements during deployment. training such a generalized model\noften encounters challenges related to uneven capabilities when controlling\ndifferent roles. A simple method is introduced based on Regret Matching+, which\nfacilitates a more balanced performance of strength by the model when\ncontrolling various roles.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12557v2",
    "published_date": "2024-01-23 08:27:38 UTC",
    "updated_date": "2024-02-01 03:22:22 UTC"
  },
  {
    "arxiv_id": "2401.12554v3",
    "title": "Can Large Language Models Write Parallel Code?",
    "authors": [
      "Daniel Nichols",
      "Joshua H. Davis",
      "Zhaojun Xie",
      "Arjun Rajaram",
      "Abhinav Bhatele"
    ],
    "abstract": "Large language models are increasingly becoming a popular tool for software\ndevelopment. Their ability to model and generate source code has been\ndemonstrated in a variety of contexts, including code completion,\nsummarization, translation, and lookup. However, they often struggle to\ngenerate code for complex programs. In this paper, we study the capabilities of\nstate-of-the-art language models to generate parallel code. In order to\nevaluate language models, we create a benchmark, ParEval, consisting of prompts\nthat represent 420 different coding tasks related to scientific and parallel\ncomputing. We use ParEval to evaluate the effectiveness of several\nstate-of-the-art open- and closed-source language models on these tasks. We\nintroduce novel metrics for evaluating the performance of generated code, and\nuse them to explore how well each large language model performs for 12\ndifferent computational problem types and six different parallel programming\nmodels.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12554v3",
    "published_date": "2024-01-23 08:25:12 UTC",
    "updated_date": "2024-05-14 15:07:58 UTC"
  },
  {
    "arxiv_id": "2401.12550v1",
    "title": "UR4NNV: Neural Network Verification, Under-approximation Reachability Works!",
    "authors": [
      "Zhen Liang",
      "Taoran Wu",
      "Ran Zhao",
      "Bai Xue",
      "Ji Wang",
      "Wenjing Yang",
      "Shaojun Deng",
      "Wanwei Liu"
    ],
    "abstract": "Recently, formal verification of deep neural networks (DNNs) has garnered\nconsiderable attention, and over-approximation based methods have become\npopular due to their effectiveness and efficiency. However, these strategies\nface challenges in addressing the \"unknown dilemma\" concerning whether the\nexact output region or the introduced approximation error violates the property\nin question. To address this, this paper introduces the UR4NNV verification\nframework, which utilizes under-approximation reachability analysis for DNN\nverification for the first time. UR4NNV focuses on DNNs with Rectified Linear\nUnit (ReLU) activations and employs a binary tree branch-based\nunder-approximation algorithm. In each epoch, UR4NNV under-approximates a\nsub-polytope of the reachable set and verifies this polytope against the given\nproperty. Through a trial-and-error approach, UR4NNV effectively falsifies DNN\nproperties while providing confidence levels when reaching verification epoch\nbounds and failing falsifying properties. Experimental comparisons with\nexisting verification methods demonstrate the effectiveness and efficiency of\nUR4NNV, significantly reducing the impact of the \"unknown dilemma\".",
    "categories": [
      "cs.AI",
      "cs.LG",
      "68Q60, 68T07",
      "D.2.4; I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.12550v1",
    "published_date": "2024-01-23 08:19:00 UTC",
    "updated_date": "2024-01-23 08:19:00 UTC"
  },
  {
    "arxiv_id": "2401.12533v3",
    "title": "Near-Optimal Algorithms for Constrained k-Center Clustering with Instance-level Background Knowledge",
    "authors": [
      "Longkun Guo",
      "Chaoqi Jia",
      "Kewen Liao",
      "Zhigang Lu",
      "Minhui Xue"
    ],
    "abstract": "Center-based clustering has attracted significant research interest from both\ntheory and practice. In many practical applications, input data often contain\nbackground knowledge that can be used to improve clustering results. In this\nwork, we build on widely adopted $k$-center clustering and model its input\nbackground knowledge as must-link (ML) and cannot-link (CL) constraint sets.\nHowever, most clustering problems including $k$-center are inherently\n$\\mathcal{NP}$-hard, while the more complex constrained variants are known to\nsuffer severer approximation and computation barriers that significantly limit\ntheir applicability. By employing a suite of techniques including reverse\ndominating sets, linear programming (LP) integral polyhedron, and LP duality,\nwe arrive at the first efficient approximation algorithm for constrained\n$k$-center with the best possible ratio of 2. We also construct competitive\nbaseline algorithms and empirically evaluate our approximation algorithm\nagainst them on a variety of real datasets. The results validate our\ntheoretical findings and demonstrate the great advantages of our algorithm in\nterms of clustering cost, clustering quality, and running time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12533v3",
    "published_date": "2024-01-23 07:16:32 UTC",
    "updated_date": "2024-05-15 01:42:47 UTC"
  },
  {
    "arxiv_id": "2401.12532v1",
    "title": "DAFA: Distance-Aware Fair Adversarial Training",
    "authors": [
      "Hyungyu Lee",
      "Saehyung Lee",
      "Hyemi Jang",
      "Junsung Park",
      "Ho Bae",
      "Sungroh Yoon"
    ],
    "abstract": "The disparity in accuracy between classes in standard training is amplified\nduring adversarial training, a phenomenon termed the robust fairness problem.\nExisting methodologies aimed to enhance robust fairness by sacrificing the\nmodel's performance on easier classes in order to improve its performance on\nharder ones. However, we observe that under adversarial attacks, the majority\nof the model's predictions for samples from the worst class are biased towards\nclasses similar to the worst class, rather than towards the easy classes.\nThrough theoretical and empirical analysis, we demonstrate that robust fairness\ndeteriorates as the distance between classes decreases. Motivated by these\ninsights, we introduce the Distance-Aware Fair Adversarial training (DAFA)\nmethodology, which addresses robust fairness by taking into account the\nsimilarities between classes. Specifically, our method assigns distinct loss\nweights and adversarial margins to each class and adjusts them to encourage a\ntrade-off in robustness among similar classes. Experimental results across\nvarious datasets demonstrate that our method not only maintains average robust\naccuracy but also significantly improves the worst robust accuracy, indicating\na marked improvement in robust fairness compared to existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.12532v1",
    "published_date": "2024-01-23 07:15:47 UTC",
    "updated_date": "2024-01-23 07:15:47 UTC"
  },
  {
    "arxiv_id": "2401.12522v2",
    "title": "BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models",
    "authors": [
      "Feng Lin",
      "Hanling Yi",
      "Hongbin Li",
      "Yifan Yang",
      "Xiaotian Yu",
      "Guangming Lu",
      "Rong Xiao"
    ],
    "abstract": "Large language models (LLMs) commonly employ autoregressive generation during\ninference, leading to high memory bandwidth demand and consequently extended\nlatency. To mitigate this inefficiency, we present Bi-directional Tuning for\nlossless Acceleration (BiTA), an innovative method expediting LLMs via\nstreamlined semi-autoregressive generation and draft verification. Inspired by\nthe concept of prompt tuning, we enhance LLMs with a parameter-efficient design\ncalled bi-directional tuning for the capability in semi-autoregressive\ngeneration. Employing efficient tree-based decoding, the models perform draft\ncandidate generation and verification in parallel, ensuring outputs identical\nto their autoregressive counterparts under greedy sampling. BiTA serves as a\nlightweight plug-in module, seamlessly boosting the inference efficiency of\nexisting LLMs without requiring additional assistance models or incurring\nsignificant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat\nachieves a 2.7$\\times$ speedup on the MT-Bench benchmark. Extensive experiments\nconfirm our method surpasses state-of-the-art acceleration techniques.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "An appendix has been included. Source code at\n  https://github.com/linfeng93/BiTA",
    "pdf_url": "http://arxiv.org/pdf/2401.12522v2",
    "published_date": "2024-01-23 06:36:49 UTC",
    "updated_date": "2024-01-25 14:02:03 UTC"
  },
  {
    "arxiv_id": "2401.13006v1",
    "title": "CIMGEN: Controlled Image Manipulation by Finetuning Pretrained Generative Models on Limited Data",
    "authors": [
      "Chandrakanth Gudavalli",
      "Erik Rosten",
      "Lakshmanan Nataraj",
      "Shivkumar Chandrasekaran",
      "B. S. Manjunath"
    ],
    "abstract": "Content creation and image editing can benefit from flexible user controls. A\ncommon intermediate representation for conditional image generation is a\nsemantic map, that has information of objects present in the image. When\ncompared to raw RGB pixels, the modification of semantic map is much easier.\nOne can take a semantic map and easily modify the map to selectively insert,\nremove, or replace objects in the map. The method proposed in this paper takes\nin the modified semantic map and alter the original image in accordance to the\nmodified map. The method leverages traditional pre-trained image-to-image\ntranslation GANs, such as CycleGAN or Pix2Pix GAN, that are fine-tuned on a\nlimited dataset of reference images associated with the semantic maps. We\ndiscuss the qualitative and quantitative performance of our technique to\nillustrate its capacity and possible applications in the fields of image\nforgery and image editing. We also demonstrate the effectiveness of the\nproposed image forgery technique in thwarting the numerous deep learning-based\nimage forensic techniques, highlighting the urgent need to develop robust and\ngeneralizable image forensic tools in the fight against the spread of fake\nmedia.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13006v1",
    "published_date": "2024-01-23 06:30:47 UTC",
    "updated_date": "2024-01-23 06:30:47 UTC"
  },
  {
    "arxiv_id": "2401.12513v2",
    "title": "Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT and SimCLR",
    "authors": [
      "Robert Turnbull",
      "Evelyn Mannix"
    ],
    "abstract": "Purpose: The capacity to isolate and recognize individual characters from\nfacsimile images of papyrus manuscripts yields rich opportunities for digital\nanalysis. For this reason the `ICDAR 2023 Competition on Detection and\nRecognition of Greek Letters on Papyri' was held as part of the 17th\nInternational Conference on Document Analysis and Recognition. This paper\ndiscusses our submission to the competition.\n  Methods: We used an ensemble of YOLOv8 models to detect and classify\nindividual characters and employed two different approaches for refining the\ncharacter predictions, including a transformer based DeiT approach and a\nResNet-50 model trained on a large corpus of unlabelled data using SimCLR, a\nself-supervised learning method.\n  Results: Our submission won the recognition challenge with a mAP of 42.2%,\nand was runner-up in the detection challenge with a mean average precision\n(mAP) of 51.4%. At the more relaxed intersection over union threshold of 0.5,\nwe achieved the highest mean average precision and mean average recall results\nfor both detection and classification.\n  Conclusion: The results demonstrate the potential for these techniques for\nautomated character recognition on historical manuscripts. We ran the\nprediction pipeline on more than 4,500 images from the Oxyrhynchus Papyri to\nillustrate the utility of our approach, and we release the results publicly in\nmultiple formats.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T10"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12513v2",
    "published_date": "2024-01-23 06:08:00 UTC",
    "updated_date": "2024-02-14 01:40:52 UTC"
  },
  {
    "arxiv_id": "2401.12497v1",
    "title": "Building Minimal and Reusable Causal State Abstractions for Reinforcement Learning",
    "authors": [
      "Zizhao Wang",
      "Caroline Wang",
      "Xuesu Xiao",
      "Yuke Zhu",
      "Peter Stone"
    ],
    "abstract": "Two desiderata of reinforcement learning (RL) algorithms are the ability to\nlearn from relatively little experience and the ability to learn policies that\ngeneralize to a range of problem specifications. In factored state spaces, one\napproach towards achieving both goals is to learn state abstractions, which\nonly keep the necessary variables for learning the tasks at hand. This paper\nintroduces Causal Bisimulation Modeling (CBM), a method that learns the causal\nrelationships in the dynamics and reward functions for each task to derive a\nminimal, task-specific abstraction. CBM leverages and improves implicit\nmodeling to train a high-fidelity causal dynamics model that can be reused for\nall tasks in the same environment. Empirical validation on manipulation\nenvironments and Deepmind Control Suite reveals that CBM's learned implicit\ndynamics models identify the underlying causal relationships and state\nabstractions more accurately than explicit ones. Furthermore, the derived state\nabstractions allow a task learner to achieve near-oracle levels of sample\nefficiency and outperform baselines on all tasks.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "I.2.9; I.2.8; I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at AAAI24",
    "pdf_url": "http://arxiv.org/pdf/2401.12497v1",
    "published_date": "2024-01-23 05:43:15 UTC",
    "updated_date": "2024-01-23 05:43:15 UTC"
  },
  {
    "arxiv_id": "2401.12492v3",
    "title": "Comparing Pre-trained Human Language Models: Is it Better with Human Context as Groups, Individual Traits, or Both?",
    "authors": [
      "Nikita Soni",
      "Niranjan Balasubramanian",
      "H. Andrew Schwartz",
      "Dirk Hovy"
    ],
    "abstract": "Pre-trained language models consider the context of neighboring words and\ndocuments but lack any author context of the human generating the text.\nHowever, language depends on the author's states, traits, social, situational,\nand environmental attributes, collectively referred to as human context (Soni\net al., 2024). Human-centered natural language processing requires\nincorporating human context into language models. Currently, two methods exist:\npre-training with 1) group-wise attributes (e.g., over-45-year-olds) or 2)\nindividual traits. Group attributes are simple but coarse -- not all\n45-year-olds write the same way -- while individual traits allow for more\npersonalized representations, but require more complex modeling and data. It is\nunclear which approach benefits what tasks. We compare pre-training models with\nhuman context via 1) group attributes, 2) individual users, and 3) a combined\napproach on five user- and document-level tasks. Our results show that there is\nno best approach, but that human-centered language modeling holds avenues for\ndifferent methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12492v3",
    "published_date": "2024-01-23 05:20:35 UTC",
    "updated_date": "2024-07-18 21:57:20 UTC"
  },
  {
    "arxiv_id": "2401.12491v1",
    "title": "Assessing and Understanding Creativity in Large Language Models",
    "authors": [
      "Yunpu Zhao",
      "Rui Zhang",
      "Wenyi Li",
      "Di Huang",
      "Jiaming Guo",
      "Shaohui Peng",
      "Yifan Hao",
      "Yuanbo Wen",
      "Xing Hu",
      "Zidong Du",
      "Qi Guo",
      "Ling Li",
      "Yunji Chen"
    ],
    "abstract": "In the field of natural language processing, the rapid development of large\nlanguage model (LLM) has attracted more and more attention. LLMs have shown a\nhigh level of creativity in various tasks, but the methods for assessing such\ncreativity are inadequate. The assessment of LLM creativity needs to consider\ndifferences from humans, requiring multi-dimensional measurement while\nbalancing accuracy and efficiency. This paper aims to establish an efficient\nframework for assessing the level of creativity in LLMs. By adapting the\nmodified Torrance Tests of Creative Thinking, the research evaluates the\ncreative performance of various LLMs across 7 tasks, emphasizing 4 criteria\nincluding Fluency, Flexibility, Originality, and Elaboration. In this context,\nwe develop a comprehensive dataset of 700 questions for testing and an\nLLM-based evaluation method. In addition, this study presents a novel analysis\nof LLMs' responses to diverse prompts and role-play situations. We found that\nthe creativity of LLMs primarily falls short in originality, while excelling in\nelaboration. Besides, the use of prompts and the role-play settings of the\nmodel significantly influence creativity. Additionally, the experimental\nresults also indicate that collaboration among multiple LLMs can enhance\noriginality. Notably, our findings reveal a consensus between human evaluations\nand LLMs regarding the personality traits that influence creativity. The\nfindings underscore the significant impact of LLM design on creativity and\nbridges artificial intelligence and human creativity, offering insights into\nLLMs' creativity and potential applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12491v1",
    "published_date": "2024-01-23 05:19:47 UTC",
    "updated_date": "2024-01-23 05:19:47 UTC"
  },
  {
    "arxiv_id": "2401.12489v1",
    "title": "Unsupervised Learning Method for the Wave Equation Based on Finite Difference Residual Constraints Loss",
    "authors": [
      "Xin Feng",
      "Yi Jiang",
      "Jia-Xian Qin",
      "Lai-Ping Zhang",
      "Xiao-Gang Deng"
    ],
    "abstract": "The wave equation is an important physical partial differential equation, and\nin recent years, deep learning has shown promise in accelerating or replacing\ntraditional numerical methods for solving it. However, existing deep learning\nmethods suffer from high data acquisition costs, low training efficiency, and\ninsufficient generalization capability for boundary conditions. To address\nthese issues, this paper proposes an unsupervised learning method for the wave\nequation based on finite difference residual constraints. We construct a novel\nfinite difference residual constraint based on structured grids and finite\ndifference methods, as well as an unsupervised training strategy, enabling\nconvolutional neural networks to train without data and predict the forward\npropagation process of waves. Experimental results show that finite difference\nresidual constraints have advantages over physics-informed neural networks\n(PINNs) type physical information constraints, such as easier fitting, lower\ncomputational costs, and stronger source term generalization capability, making\nour method more efficient in training and potent in application.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "in Chinese language",
    "pdf_url": "http://arxiv.org/pdf/2401.12489v1",
    "published_date": "2024-01-23 05:06:29 UTC",
    "updated_date": "2024-01-23 05:06:29 UTC"
  },
  {
    "arxiv_id": "2401.12485v1",
    "title": "Adiabatic Quantum Support Vector Machines",
    "authors": [
      "Prasanna Date",
      "Dong Jun Woun",
      "Kathleen Hamilton",
      "Eduardo A. Coello Perez",
      "Mayanka Chandra Shekhar",
      "Francisco Rios",
      "John Gounley",
      "In-Saeng Suh",
      "Travis Humble",
      "Georgia Tourassi"
    ],
    "abstract": "Adiabatic quantum computers can solve difficult optimization problems (e.g.,\nthe quadratic unconstrained binary optimization problem), and they seem well\nsuited to train machine learning models. In this paper, we describe an\nadiabatic quantum approach for training support vector machines. We show that\nthe time complexity of our quantum approach is an order of magnitude better\nthan the classical approach. Next, we compare the test accuracy of our quantum\napproach against a classical approach that uses the Scikit-learn library in\nPython across five benchmark datasets (Iris, Wisconsin Breast Cancer (WBC),\nWine, Digits, and Lambeq). We show that our quantum approach obtains accuracies\non par with the classical approach. Finally, we perform a scalability study in\nwhich we compute the total training times of the quantum approach and the\nclassical approach with increasing number of features and number of data points\nin the training dataset. Our scalability results show that the quantum approach\nobtains a 3.5--4.5 times speedup over the classical approach on datasets with\nmany (millions of) features.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12485v1",
    "published_date": "2024-01-23 04:50:13 UTC",
    "updated_date": "2024-01-23 04:50:13 UTC"
  },
  {
    "arxiv_id": "2401.12478v2",
    "title": "Mini-batch Submodular Maximization",
    "authors": [
      "Gregory Schwartzman"
    ],
    "abstract": "We present the first mini-batch algorithm for maximizing a non-negative\nmonotone decomposable submodular function, $F=\\sum_{i=1}^N f^i$, under a set of\nconstraints. We consider two sampling approaches: uniform and weighted. We\nfirst show that mini-batch with weighted sampling improves over the state of\nthe art sparsifier based approach both in theory and in practice.\n  Surprisingly, our experimental results show that uniform sampling is superior\nto weighted sampling. However, it is impossible to explain this using\nworst-case analysis. Our main contribution is using smoothed analysis to\nprovide a theoretical foundation for our experimental results. We show that,\nunder very mild assumptions, uniform sampling is superior for both the\nmini-batch and the sparsifier approaches. We empirically verify that these\nassumptions hold for our datasets. Uniform sampling is simple to implement and\nhas complexity independent of $N$, making it the perfect candidate to tackle\nmassive real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12478v2",
    "published_date": "2024-01-23 04:16:58 UTC",
    "updated_date": "2024-10-02 09:02:19 UTC"
  },
  {
    "arxiv_id": "2401.12470v1",
    "title": "Reinforcement Learning for Graph Coloring: Understanding the Power and Limits of Non-Label Invariant Representations",
    "authors": [
      "Chase Cummins",
      "Richard Veras"
    ],
    "abstract": "Register allocation is one of the most important problems for modern\ncompilers. With a practically unlimited number of user variables and a small\nnumber of CPU registers, assigning variables to registers without conflicts is\na complex task. This work demonstrates the use of casting the register\nallocation problem as a graph coloring problem. Using technologies such as\nPyTorch and OpenAI Gymnasium Environments we will show that a Proximal Policy\nOptimization model can learn to solve the graph coloring problem. We will also\nshow that the labeling of a graph is critical to the performance of the model\nby taking the matrix representation of a graph and permuting it. We then test\nthe model's effectiveness on each of these permutations and show that it is not\neffective when given a relabeling of the same graph. Our main contribution lies\nin showing the need for label reordering invariant representations of graphs\nfor machine learning models to achieve consistent performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12470v1",
    "published_date": "2024-01-23 03:43:34 UTC",
    "updated_date": "2024-01-23 03:43:34 UTC"
  },
  {
    "arxiv_id": "2401.12467v2",
    "title": "An open dataset for the evolution of oracle bone characters: EVOBC",
    "authors": [
      "Haisu Guan",
      "Jinpeng Wan",
      "Yuliang Liu",
      "Pengjie Wang",
      "Kaile Zhang",
      "Zhebin Kuang",
      "Xinyu Wang",
      "Xiang Bai",
      "Lianwen Jin"
    ],
    "abstract": "The earliest extant Chinese characters originate from oracle bone\ninscriptions, which are closely related to other East Asian languages. These\ninscriptions hold immense value for anthropology and archaeology. However,\ndeciphering oracle bone script remains a formidable challenge, with only\napproximately 1,600 of the over 4,500 extant characters elucidated to date.\nFurther scholarly investigation is required to comprehensively understand this\nancient writing system. Artificial Intelligence technology is a promising\navenue for deciphering oracle bone characters, particularly concerning their\nevolution. However, one of the challenges is the lack of datasets mapping the\nevolution of these characters over time. In this study, we systematically\ncollected ancient characters from authoritative texts and websites spanning six\nhistorical stages: Oracle Bone Characters - OBC (15th century B.C.), Bronze\nInscriptions - BI (13th to 221 B.C.), Seal Script - SS (11th to 8th centuries\nB.C.), Spring and Autumn period Characters - SAC (770 to 476 B.C.), Warring\nStates period Characters - WSC (475 B.C. to 221 B.C.), and Clerical Script - CS\n(221 B.C. to 220 A.D.). Subsequently, we constructed an extensive dataset,\nnamely EVolution Oracle Bone Characters (EVOBC), consisting of 229,170 images\nrepresenting 13,714 distinct character categories. We conducted validation and\nsimulated deciphering on the constructed dataset, and the results demonstrate\nits high efficacy in aiding the study of oracle bone script. This openly\naccessible dataset aims to digitalize ancient Chinese scripts across multiple\neras, facilitating the decipherment of oracle bone script by examining the\nevolution of glyph forms.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12467v2",
    "published_date": "2024-01-23 03:30:47 UTC",
    "updated_date": "2024-02-13 08:21:50 UTC"
  },
  {
    "arxiv_id": "2401.12461v1",
    "title": "Fast Adversarial Training against Textual Adversarial Attacks",
    "authors": [
      "Yichen Yang",
      "Xin Liu",
      "Kun He"
    ],
    "abstract": "Many adversarial defense methods have been proposed to enhance the\nadversarial robustness of natural language processing models. However, most of\nthem introduce additional pre-set linguistic knowledge and assume that the\nsynonym candidates used by attackers are accessible, which is an ideal\nassumption. We delve into adversarial training in the embedding space and\npropose a Fast Adversarial Training (FAT) method to improve the model\nrobustness in the synonym-unaware scenario from the perspective of single-step\nperturbation generation and perturbation initialization. Based on the\nobservation that the adversarial perturbations crafted by single-step and\nmulti-step gradient ascent are similar, FAT uses single-step gradient ascent to\ncraft adversarial examples in the embedding space to expedite the training\nprocess. Based on the observation that the perturbations generated on the\nidentical training sample in successive epochs are similar, FAT fully utilizes\nhistorical information when initializing the perturbation. Extensive\nexperiments demonstrate that FAT significantly boosts the robustness of BERT\nmodels in the synonym-unaware scenario, and outperforms the defense baselines\nunder various attacks with character-level and word-level modifications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.12461v1",
    "published_date": "2024-01-23 03:03:57 UTC",
    "updated_date": "2024-01-23 03:03:57 UTC"
  },
  {
    "arxiv_id": "2401.12459v2",
    "title": "Towards Socially and Morally Aware RL agent: Reward Design With LLM",
    "authors": [
      "Zhaoyue Wang"
    ],
    "abstract": "When we design and deploy an Reinforcement Learning (RL) agent, reward\nfunctions motivates agents to achieve an objective. An incorrect or incomplete\nspecification of the objective can result in behavior that does not align with\nhuman values - failing to adhere with social and moral norms that are ambiguous\nand context dependent, and cause undesired outcomes such as negative side\neffects and exploration that is unsafe. Previous work have manually defined\nreward functions to avoid negative side effects, use human oversight for safe\nexploration, or use foundation models as planning tools. This work studies the\nability of leveraging Large Language Models (LLM)' understanding of morality\nand social norms on safe exploration augmented RL methods. This work evaluates\nlanguage model's result against human feedbacks and demonstrates language\nmodel's capability as direct reward signals.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12459v2",
    "published_date": "2024-01-23 03:00:03 UTC",
    "updated_date": "2024-05-30 20:40:30 UTC"
  },
  {
    "arxiv_id": "2401.12456v1",
    "title": "Exploration and Improvement of Nerf-based 3D Scene Editing Techniques",
    "authors": [
      "Shun Fang",
      "Ming Cui",
      "Xing Feng",
      "Yanan Zhang"
    ],
    "abstract": "NeRF's high-quality scene synthesis capability was quickly accepted by\nscholars in the years after it was proposed, and significant progress has been\nmade in 3D scene representation and synthesis. However, the high computational\ncost limits intuitive and efficient editing of scenes, making NeRF's\ndevelopment in the scene editing field facing many challenges. This paper\nreviews the preliminary explorations of scholars on NeRF in the scene or object\nediting field in recent years, mainly changing the shape and texture of scenes\nor objects in new synthesized scenes; through the combination of residual\nmodels such as GaN and Transformer with NeRF, the generalization ability of\nNeRF scene editing has been further expanded, including realizing real-time new\nperspective editing feedback, multimodal editing of text synthesized 3D scenes,\n4D synthesis performance, and in-depth exploration in light and shadow editing,\ninitially achieving optimization of indirect touch editing and detail\nrepresentation in complex scenes. Currently, most NeRF editing methods focus on\nthe touch points and materials of indirect points, but when dealing with more\ncomplex or larger 3D scenes, it is difficult to balance accuracy, breadth,\nefficiency, and quality. Overcoming these challenges may become the direction\nof future NeRF 3D scene editing technology.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12456v1",
    "published_date": "2024-01-23 02:53:06 UTC",
    "updated_date": "2024-01-23 02:53:06 UTC"
  },
  {
    "arxiv_id": "2401.12455v1",
    "title": "Multi-agent deep reinforcement learning with centralized training and decentralized execution for transportation infrastructure management",
    "authors": [
      "M. Saifullah",
      "K. G. Papakonstantinou",
      "C. P. Andriotis",
      "S. M. Stoffels"
    ],
    "abstract": "We present a multi-agent Deep Reinforcement Learning (DRL) framework for\nmanaging large transportation infrastructure systems over their life-cycle.\nLife-cycle management of such engineering systems is a computationally\nintensive task, requiring appropriate sequential inspection and maintenance\ndecisions able to reduce long-term risks and costs, while dealing with\ndifferent uncertainties and constraints that lie in high-dimensional spaces. To\ndate, static age- or condition-based maintenance methods and risk-based or\nperiodic inspection plans have mostly addressed this class of optimization\nproblems. However, optimality, scalability, and uncertainty limitations are\noften manifested under such approaches. The optimization problem in this work\nis cast in the framework of constrained Partially Observable Markov Decision\nProcesses (POMDPs), which provides a comprehensive mathematical basis for\nstochastic sequential decision settings with observation uncertainties, risk\nconsiderations, and limited resources. To address significantly large state and\naction spaces, a Deep Decentralized Multi-agent Actor-Critic (DDMAC) DRL method\nwith Centralized Training and Decentralized Execution (CTDE), termed as\nDDMAC-CTDE is developed. The performance strengths of the DDMAC-CTDE method are\ndemonstrated in a generally representative and realistic example application of\nan existing transportation network in Virginia, USA. The network includes\nseveral bridge and pavement components with nonstationary degradation,\nagency-imposed constraints, and traffic delay and risk considerations. Compared\nto traditional management policies for transportation networks, the proposed\nDDMAC-CTDE method vastly outperforms its counterparts. Overall, the proposed\nalgorithmic framework provides near optimal solutions for transportation\ninfrastructure management under real-world constraints and complexities.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12455v1",
    "published_date": "2024-01-23 02:52:36 UTC",
    "updated_date": "2024-01-23 02:52:36 UTC"
  },
  {
    "arxiv_id": "2401.12451v2",
    "title": "Methods and strategies for improving the novel view synthesis quality of neural radiation field",
    "authors": [
      "Shun Fang",
      "Ming Cui",
      "Xing Feng",
      "Yanna Lv"
    ],
    "abstract": "Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a\nscene from 2D images and synthesize realistic novel view images. This\ntechnology has received widespread attention from the industry and has good\napplication prospects. In response to the problem that the rendering quality of\nNeRF images needs to be improved, many researchers have proposed various\nmethods to improve the rendering quality in the past three years. The latest\nrelevant papers are classified and reviewed, the technical principles behind\nquality improvement are analyzed, and the future evolution direction of quality\nimprovement methods is discussed. This study can help researchers quickly\nunderstand the current state and evolutionary context of technology in this\nfield, which is helpful in inspiring the development of more efficient\nalgorithms and promoting the application of NeRF technology in related fields.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2; I.4; I.6"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12451v2",
    "published_date": "2024-01-23 02:30:16 UTC",
    "updated_date": "2024-04-18 01:37:42 UTC"
  },
  {
    "arxiv_id": "2401.12435v2",
    "title": "Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network",
    "authors": [
      "Jiayi Xie",
      "Hongfeng Li",
      "Jin Cheng",
      "Qingrui Cai",
      "Hanbo Tan",
      "Lingyun Zu",
      "Xiaobo Qu",
      "Hongbin Han"
    ],
    "abstract": "The brain extracellular space (ECS), an irregular, extremely tortuous\nnanoscale space located between cells or between cells and blood vessels, is\ncrucial for nerve cell survival. It plays a pivotal role in high-level brain\nfunctions such as memory, emotion, and sensation. However, the specific form of\nmolecular transport within the ECS remain elusive. To address this challenge,\nthis paper proposes a novel approach to quantitatively analyze the molecular\ntransport within the ECS by solving an inverse problem derived from the\nadvection-diffusion equation (ADE) using a physics-informed neural network\n(PINN). PINN provides a streamlined solution to the ADE without the need for\nintricate mathematical formulations or grid settings. Additionally, the\noptimization of PINN facilitates the automatic computation of the diffusion\ncoefficient governing long-term molecule transport and the velocity of\nmolecules driven by advection. Consequently, the proposed method allows for the\nquantitative analysis and identification of the specific pattern of molecular\ntransport within the ECS through the calculation of the Peclet number.\nExperimental validation on two datasets of magnetic resonance images (MRIs)\ncaptured at different time points showcases the effectiveness of the proposed\nmethod. Notably, our simulations reveal identical molecular transport patterns\nbetween datasets representing rats with tracer injected into the same brain\nregion. These findings highlight the potential of PINN as a promising tool for\ncomprehensively exploring molecular transport within the ECS.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.AP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12435v2",
    "published_date": "2024-01-23 02:04:15 UTC",
    "updated_date": "2024-01-24 02:31:26 UTC"
  },
  {
    "arxiv_id": "2401.12421v1",
    "title": "AdaEmbed: Semi-supervised Domain Adaptation in the Embedding Space",
    "authors": [
      "Ali Mottaghi",
      "Mohammad Abdullah Jamal",
      "Serena Yeung",
      "Omid Mohareri"
    ],
    "abstract": "Semi-supervised domain adaptation (SSDA) presents a critical hurdle in\ncomputer vision, especially given the frequent scarcity of labeled data in\nreal-world settings. This scarcity often causes foundation models, trained on\nextensive datasets, to underperform when applied to new domains. AdaEmbed, our\nnewly proposed methodology for SSDA, offers a promising solution to these\nchallenges. Leveraging the potential of unlabeled data, AdaEmbed facilitates\nthe transfer of knowledge from a labeled source domain to an unlabeled target\ndomain by learning a shared embedding space. By generating accurate and uniform\npseudo-labels based on the established embedding space, the model overcomes the\nlimitations of conventional SSDA, thus enhancing performance significantly. Our\nmethod's effectiveness is validated through extensive experiments on benchmark\ndatasets such as DomainNet, Office-Home, and VisDA-C, where AdaEmbed\nconsistently outperforms all the baselines, setting a new state of the art for\nSSDA. With its straightforward implementation and high data efficiency,\nAdaEmbed stands out as a robust and pragmatic solution for real-world\nscenarios, where labeled data is scarce. To foster further research and\napplication in this area, we are sharing the codebase of our unified framework\nfor semi-supervised domain adaptation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.12421v1",
    "published_date": "2024-01-23 01:10:25 UTC",
    "updated_date": "2024-01-23 01:10:25 UTC"
  }
]
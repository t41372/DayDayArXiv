{
  "date": "2024-07-03",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-03 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文聚焦于 AI 模型的鲁棒性优化、LLM 的应用与安全、多模态处理和图学习等领域，其中令人印象深刻的是 AgentInstruct（涉及微软团队的生成式教学框架）和 HEMM（对多模态基础模型的全面评估），这些工作展示了 LLM 在实际应用中的潜力，同时强调了 AI 偏见和后门攻击的风险。\n\n### 重点论文讨论\n我将优先讨论重要、话题性强的论文，如 AI 安全、LLM 优化和多模态处理领域，然后快速掠过其他较基础或特定领域的文章。以下按主题分组，突出核心贡献。\n\n#### LLM 和 AI 安全领域（高话题度，优先讨论）\n- **Truth is Universal: Robust Detection of Lies in LLMs**（英文标题：\"Truth is Universal: Robust Detection of Lies in LLMs\"）  \n  这篇论文提出了一种鲁棒的 LLM 谎言检测方法，通过识别一个二维子空间来分离真假语句，支持多种模型如 Gemma 和 LLaMA，实现高达 94% 的检测准确率，强调 LLM 在事实性任务中的可靠性。\n\n- **Regurgitative Training: The Value of Real Data in Training Large Language Models**（英文标题：\"Regurgitative Training: The Value of Real Data in Training Large Language Models\"）  \n  作者探讨了使用 LLM 生成数据进行训练的负面影响，发现合成数据会导致性能下降，并提出三策略（如有序训练）来缓解，证明真实数据在 LLM 训练中的不可替代性。\n\n- **LLMcap: Large Language Model for Unsupervised PCAP Failure Detection**（英文标题：\"LLMcap: Large Language Model for Unsupervised PCAP Failure Detection\"）  \n  这篇工作使用 LLM 进行无监督网络流量故障检测，通过 masked language modeling 学习流量模式，实现高效检测，适用于电信网络的安全分析。\n\n- **IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization**（英文标题：\"IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization\"）  \n  论文引入一种文本匿名化框架，使用 LLM 随机化私人属性，减少隐私泄露超过 90%，并通过 LoRA 参数蒸馏实现高效部署，适用于隐私敏感的应用。\n\n这些论文突出了 LLM 的鲁棒性和隐私挑战，AgentInstruct 等工作尤其值得关注，因为它展示了 LLM 在生成教学数据中的实际潜力。\n\n#### 图神经网络和学习（重要但次优先，快速概述）\n- **Backdoor Graph Condensation**（英文标题：\"Backdoor Graph Condensation\"）  \n  作者开发了 BGC 算法，用于在图神经网络中注入后门攻击，同时保持图质量，实验显示攻击成功率接近 1.0，并抵抗多种防御，揭示了图学习的安全风险。\n\n- **Graphon Particle Systems, Part II: Dynamics of Distributed Stochastic Continuum Optimization**（英文标题：\"Graphon Particle Systems, Part II: Dynamics of Distributed Stochastic Continuum Optimization\"）  \n  这篇扩展了图学习理论，提出图神经网络的随机梯度下降和梯度跟踪算法，证明在连接图上可实现均方收敛，适用于大规模分布式优化。\n\n其他图相关论文如 SF-GNN 和 GraCoRe 也优化了深度 GNN 的性能，但贡献较局部，这里不展开。\n\n#### 多模态和医学 AI（应用价值高，简要讨论）\n- **HEMM: Holistic Evaluation of Multimodal Foundation Models**（英文标题：\"HEMM: Holistic Evaluation of Multimodal Foundation Models\"）  \n  论文构建了一个全面基准，评估多模态模型在基本技能、信息流和实际应用中的性能，实验显示模型在多模态任务中存在挑战，并分析了建模维度的影响。\n\n- **MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced AI Applications**（英文标题：\"MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced AI Applications\"）  \n  作者创建了一个多模态医学数据集，包括图像和文本，设计五种结构理解任务，并训练 CLIP 模型提升医学图像分析，填补了 AI 在生物医学领域的空白。\n\n- **MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context**（英文标题：\"MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context\"）  \n  这篇工作提出 MedVH 基准，评估医学 LVLMs 的幻觉问题，实验显示专业模型在医学任务中易产生幻觉，强调了模型鲁棒性的必要性。\n\n这些多模态论文展示了 AI 在医疗中的潜力，但整体进展仍需解决幻觉问题。\n\n#### 其他领域（快速掠过，选取亮点）\n其余论文涉及语音处理、强化学习和图像生成等领域，这里仅快速提及几篇有代表性的：\n- **Translatotron-V(ision): An End-to-End Model for In-Image Machine Translation**（英文标题：\"Translatotron-V(ision): An End-to-End Model for In-Image Machine Translation\"）  \n  提出端到端图像翻译模型，使用图像标记器和双阶段训练，实验显示在图像翻译任务中竞争力强。\n  \n- **Efficient Fusion and Task Guided Embedding for End-to-End Autonomous Driving**（英文标题：\"Efficient Fusion and Task Guided Embedding for End-to-End Autonomous Driving\"）  \n  开发了高效的多传感器融合框架，实现实时自动驾驶，减少参数 37.6%，在 CARLA 基准上提升性能。\n\n其他如语音识别、量子计算或特定优化论文（如 Efficient DNN-Powered Software），贡献较细致或应用性不强，这里不详细展开，以保持篇幅。\n\n总之，今天的 arXiv 论文强调了 AI 模型的效率、安全和实际应用，LLM 相关工作尤其突出，期待这些进展推动更可靠的 AI 系统。更多细节可查阅具体论文！",
  "papers": [
    {
      "arxiv_id": "2407.03545v2",
      "title": "On Evaluating Explanation Utility for Human-AI Decision Making in NLP",
      "title_zh": "翻译失败",
      "authors": [
        "Fateme Hashemi Chaleshtori",
        "Atreya Ghosal",
        "Alexander Gill",
        "Purbid Bambroo",
        "Ana Marasović"
      ],
      "abstract": "Is explainability a false promise? This debate has emerged from the\ninsufficient evidence that explanations help people in situations they are\nintroduced for. More human-centered, application-grounded evaluations of\nexplanations are needed to settle this. Yet, with no established guidelines for\nsuch studies in NLP, researchers accustomed to standardized proxy evaluations\nmust discover appropriate measurements, tasks, datasets, and sensible models\nfor human-AI teams in their studies.\n  To aid with this, we first review existing metrics suitable for\napplication-grounded evaluation. We then establish criteria to select\nappropriate datasets, and using them, we find that only 4 out of over 50\ndatasets available for explainability research in NLP meet them. We then\ndemonstrate the importance of reassessing the state of the art to form and\nstudy human-AI teams: teaming people with models for certain tasks might only\nnow start to make sense, and for others, it remains unsound. Finally, we\npresent the exemplar studies of human-AI decision-making for one of the\nidentified tasks -- verifying the correctness of a legal claim given a\ncontract. Our results show that providing AI predictions, with or without\nexplanations, does not cause decision makers to speed up their work without\ncompromising performance. We argue for revisiting the setup of human-AI teams\nand improving automatic deferral of instances to AI, where explanations could\nplay a useful role.",
      "tldr_zh": "本论文探讨了在 NLP 中解释性（explainability）的实用性，评估其是否能有效提升人类-AI 决策过程。作者审查了现有评估指标，建立数据集选择标准，发现超过 50 个可用数据集中仅 4 个符合要求，并通过实验重新评估人类-AI 团队的适用性。以验证法律声明正确性为例，研究显示提供 AI 预测（无论是否附带解释）不会让决策者加速工作同时保持性能。最终，论文建议优化人类-AI 团队设置，并改进自动推迟机制，其中解释可能发挥关键作用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP Findings 2024; 10 pages main, 7 pages references, 32 pages\n  appendix",
      "pdf_url": "http://arxiv.org/pdf/2407.03545v2",
      "published_date": "2024-07-03 23:53:27 UTC",
      "updated_date": "2024-11-05 01:38:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:08:02.596269"
    },
    {
      "arxiv_id": "2407.12836v1",
      "title": "OSPC: Artificial VLM Features for Hateful Meme Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Peter Grönquist"
      ],
      "abstract": "The digital revolution and the advent of the world wide web have transformed\nhuman communication, notably through the emergence of memes. While memes are a\npopular and straightforward form of expression, they can also be used to spread\nmisinformation and hate due to their anonymity and ease of use. In response to\nthese challenges, this paper introduces a solution developed by team 'Baseline'\nfor the AI Singapore Online Safety Prize Challenge. Focusing on computational\nefficiency and feature engineering, the solution achieved an AUROC of 0.76 and\nan accuracy of 0.69 on the test dataset. As key features, the solution\nleverages the inherent probabilistic capabilities of large Vision-Language\nModels (VLMs) to generate task-adapted feature encodings from text, and applies\na distilled quantization tailored to the specific cultural nuances present in\nSingapore. This type of processing and fine-tuning can be adapted to various\nvisual and textual understanding and classification tasks, and even applied on\nprivate VLMs such as OpenAI's GPT. Finally it can eliminate the need for\nextensive model training on large GPUs for resource constrained applications,\nalso offering a solution when little or no data is available.",
      "tldr_zh": "这篇论文介绍了OSPC解决方案，由团队'Baseline'为AI Singapore Online Safety Prize Challenge开发，用于检测仇恨模因，强调计算效率和特征工程。该方法利用大型视觉语言模型(VLMs)的概率能力，从文本生成任务适配的特征编码，并应用针对新加坡文化细微差别的蒸馏量化处理，从而在测试数据集上达到AUROC 0.76和准确率0.69。相比传统方法，OSPC消除了对大规模GPU训练的需求，适用于资源受限环境或数据稀缺场景，并可扩展到其他视觉和文本分类任务，甚至私有VLMs如OpenAI's GPT。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12836v1",
      "published_date": "2024-07-03 21:35:52 UTC",
      "updated_date": "2024-07-03 21:35:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:08:14.218092"
    },
    {
      "arxiv_id": "2407.03518v4",
      "title": "Improving LLM Abilities in Idiomatic Translation",
      "title_zh": "提升LLM在习语翻译中的能力",
      "authors": [
        "Sundesh Donthi",
        "Maximilian Spencer",
        "Om Patel",
        "Joon Doh",
        "Eid Rodan",
        "Kevin Zhu",
        "Sean O'Brien"
      ],
      "abstract": "For large language models (LLMs) like NLLB and GPT, translating idioms\nremains a challenge. Our goal is to enhance translation fidelity by improving\nLLM processing of idiomatic language while preserving the original linguistic\nstyle. This has a significant social impact, as it preserves cultural nuances\nand ensures translated texts retain their intent and emotional resonance,\nfostering better cross-cultural communication. Previous work has utilized\nknowledge bases like IdiomKB by providing the LLM with the meaning of an idiom\nto use in translation. Although this method yielded better results than a\ndirect translation, it is still limited in its ability to preserve idiomatic\nwriting style across languages. In this research, we expand upon the knowledge\nbase to find corresponding idioms in the target language. Our research performs\ntranslations using two methods: The first method employs the\nSentenceTransformers model to semantically generate cosine similarity scores\nbetween the meanings of the original and target language idioms, selecting the\nbest idiom (Cosine Similarity method). The second method uses an LLM to find a\ncorresponding idiom in the target language for use in the translation\n(LLM-generated idiom method). As a baseline, we performed a direct translation\nwithout providing additional information. Human evaluations on the English ->\nChinese, and Chinese -> English show the Cosine Similarity Lookup method\nout-performed others in all GPT4o translations. To further build upon IdiomKB,\nwe developed a low-resource Urdu dataset containing Urdu idioms and their\ntranslations. Despite dataset limitations, the Cosine Similarity Lookup method\nshows promise, potentially overcoming language barriers and enabling the\nexploration of diverse literary works in Chinese and Urdu.(LoResLM @ COLING\nPreprint)",
      "tldr_zh": "本研究旨在提升大型语言模型（LLMs）如 NLLB 和 GPT 在成语翻译中的能力，重点改善翻译的忠实度，同时保留原语言风格，以保护文化细微差别并促进跨文化交流。研究扩展了 IdiomKB 知识库，通过两种方法寻找目标语言的对应成语：第一种是使用 SentenceTransformers 计算成语含义的余弦相似度（Cosine Similarity method）来选择最佳匹配；第二种是利用 LLM 生成目标语言的对应成语。人类评估显示，在 English -> Chinese 和 Chinese -> English 翻译中，Cosine Similarity method 在 GPT4o 模型上优于基线和另一方法；此外，研究还开发了一个低资源 Urdu 数据集，证明该方法有潜力克服语言障碍。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint for LoResLM Workshop at COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.03518v4",
      "published_date": "2024-07-03 21:34:26 UTC",
      "updated_date": "2025-01-23 04:04:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:08:37.014816"
    },
    {
      "arxiv_id": "2407.03514v1",
      "title": "Towards Attention-based Contrastive Learning for Audio Spoof Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Chirag Goel",
        "Surya Koppisetti",
        "Ben Colman",
        "Ali Shahriyari",
        "Gaurav Bharaj"
      ],
      "abstract": "Vision transformers (ViT) have made substantial progress for classification\ntasks in computer vision. Recently, Gong et. al. '21, introduced\nattention-based modeling for several audio tasks. However, relatively\nunexplored is the use of a ViT for audio spoof detection task. We bridge this\ngap and introduce ViTs for this task. A vanilla baseline built on fine-tuning\nthe SSAST (Gong et. al. '22) audio ViT model achieves sub-optimal equal error\nrates (EERs). To improve performance, we propose a novel attention-based\ncontrastive learning framework (SSAST-CL) that uses cross-attention to aid the\nrepresentation learning. Experiments show that our framework successfully\ndisentangles the bonafide and spoof classes and helps learn better classifiers\nfor the task. With appropriate data augmentations policy, a model trained on\nour framework achieves competitive performance on the ASVSpoof 2021 challenge.\nWe provide comparisons and ablation studies to justify our claim.",
      "tldr_zh": "该论文探讨了使用Vision Transformers (ViT) 进行音频欺骗检测（audio spoof detection），填补了这一领域的空白。作者提出了一种新型框架SSAST-CL，基于注意力机制的对比学习（contrastive learning），通过交叉注意力（cross-attention）来增强音频表示学习，从而更好地区分真实和欺骗音频。实验结果表明，该框架显著提高了模型性能，在ASVSpoof 2021挑战中实现了竞争性的错误率，并通过消融研究验证了其有效性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Proc. INTERSPEECH 2023",
      "pdf_url": "http://arxiv.org/pdf/2407.03514v1",
      "published_date": "2024-07-03 21:25:12 UTC",
      "updated_date": "2024-07-03 21:25:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:08:37.639833"
    },
    {
      "arxiv_id": "2407.03506v1",
      "title": "AntibotV: A Multilevel Behaviour-based Framework for Botnets Detection in Vehicular Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Rabah Rahal",
        "Abdelaziz Amara Korba",
        "Nacira Ghoualmi-Zine",
        "Yacine Challal",
        "Mohamed Yacine Ghamri-Doudane"
      ],
      "abstract": "Connected cars offer safety and efficiency for both individuals and fleets of\nprivate vehicles and public transportation companies. However, equipping\nvehicles with information and communication technologies raises privacy and\nsecurity concerns, which significantly threaten the user's data and life. Using\nbot malware, a hacker may compromise a vehicle and control it remotely, for\ninstance, he can disable breaks or start the engine remotely. In this paper,\nbesides in-vehicle attacks existing in the literature, we consider new zeroday\nbot malware attacks specific to the vehicular context, WSMP-Flood, and Geo-WSMP\nFlood. Then, we propose AntibotV, a multilevel behaviour-based framework for\nvehicular botnets detection in vehicular networks. The proposed framework\ncombines two main modules for attack detection, the first one monitors the\nvehicle's activity at the network level, whereas the second one monitors the\nin-vehicle activity. The two intrusion detection modules have been trained on a\nhistorical network and in-vehicle communication using decision tree algorithms.\nThe experimental results showed that the proposed framework outperforms\nexisting solutions, it achieves a detection rate higher than 97% and a false\npositive rate lower than 0.14%.",
      "tldr_zh": "这篇论文针对车辆网络中的 botnets 攻击问题，特别考虑了新的零日恶意软件如 WSMP-Flood 和 Geo-WSMP Flood，这些攻击可能导致车辆被远程控制并带来安全风险。作者提出 AntibotV 框架，这是一个多级基于行为的检测系统，包括网络级监控车辆活动和车辆内部监控模块，两者均使用 decision tree algorithms 在历史数据上训练。该框架的实验结果显示，检测率超过 97%，假阳性率低于 0.14%，显著优于现有解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03506v1",
      "published_date": "2024-07-03 21:07:49 UTC",
      "updated_date": "2024-07-03 21:07:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:08:50.319989"
    },
    {
      "arxiv_id": "2407.03502v1",
      "title": "AgentInstruct: Toward Generative Teaching with Agentic Flows",
      "title_zh": "翻译失败",
      "authors": [
        "Arindam Mitra",
        "Luciano Del Corro",
        "Guoqing Zheng",
        "Shweti Mahajan",
        "Dany Rouhana",
        "Andres Codas",
        "Yadong Lu",
        "Wei-ge Chen",
        "Olga Vrousgos",
        "Corby Rosset",
        "Fillipe Silva",
        "Hamed Khanpour",
        "Yash Lara",
        "Ahmed Awadallah"
      ],
      "abstract": "Synthetic data is becoming increasingly important for accelerating the\ndevelopment of language models, both large and small. Despite several\nsuccessful use cases, researchers also raised concerns around model collapse\nand drawbacks of imitating other models. This discrepancy can be attributed to\nthe fact that synthetic data varies in quality and diversity. Effective use of\nsynthetic data usually requires significant human effort in curating the data.\nWe focus on using synthetic data for post-training, specifically creating data\nby powerful models to teach a new skill or behavior to another model, we refer\nto this setting as Generative Teaching. We introduce AgentInstruct, an\nextensible agentic framework for automatically creating large amounts of\ndiverse and high-quality synthetic data. AgentInstruct can create both the\nprompts and responses, using only raw data sources like text documents and code\nfiles as seeds. We demonstrate the utility of AgentInstruct by creating a post\ntraining dataset of 25M pairs to teach language models different skills, such\nas text editing, creative writing, tool usage, coding, reading comprehension,\netc. The dataset can be used for instruction tuning of any base model. We\npost-train Mistral-7b with the data. When comparing the resulting model Orca-3\nto Mistral-7b-Instruct (which uses the same base model), we observe significant\nimprovements across many benchmarks. For example, 40% improvement on AGIEval,\n19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH and\n45% improvement on AlpacaEval. Additionally, it consistently outperforms other\nmodels such as LLAMA-8B-instruct and GPT-3.5-turbo.",
      "tldr_zh": "该研究提出了 AgentInstruct，一种基于 agentic flows 的可扩展框架，用于自动生成多样且高质量的 synthetic data，从而实现 Generative Teaching，即使用强大模型创建数据来教导其他模型新技能。框架以原始数据源（如文本文档和代码文件）为种子，自动生成提示和响应，创建了包含 25M 对的数据集，涵盖文本编辑、创意写作、工具使用、编码、阅读理解等技能。实验结果显示，使用该数据集对 Mistral-7b 进行 post-training 后得到的 Orca-3 模型，在多个基准测试中显著提升性能，例如 AGIEval 提升 40%、MMLU 提升 19%、GSM8K 提升 54%，并优于 LLAMA-8B-instruct 和 GPT-3.5-turbo。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03502v1",
      "published_date": "2024-07-03 21:01:12 UTC",
      "updated_date": "2024-07-03 21:01:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:09:05.672983"
    },
    {
      "arxiv_id": "2407.03482v2",
      "title": "Domain-Aware Fine-Tuning of Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ugur Ali Kaplan",
        "Margret Keuper",
        "Anna Khoreva",
        "Dan Zhang",
        "Yumeng Li"
      ],
      "abstract": "Foundation models (FMs) have revolutionized computer vision, enabling\neffective learning across different domains. However, their performance under\ndomain shift is yet underexplored. This paper investigates the zero-shot domain\nadaptation potential of FMs by comparing different backbone architectures and\nintroducing novel domain-aware components that leverage domain related textual\nembeddings. We propose domain adaptive normalization, termed as Domino, which\nexplicitly leverages domain embeddings during fine-tuning, thus making the\nmodel domain aware. Ultimately, Domino enables more robust computer vision\nmodels that can adapt effectively to various unseen domains.",
      "tldr_zh": "本文探讨了 Foundation Models 在领域转移（domain shift）下的性能，调查其零样本领域适应（zero-shot domain adaptation）潜力，并比较不同骨干架构。论文引入了新型领域感知组件，利用领域相关的文本嵌入，提出了一种名为 Domino 的领域自适应归一化方法，在微调过程中显式整合这些嵌入以提升模型的领域感知能力。最终，Domino 使计算机视觉模型更具鲁棒性，能够有效适应各种未见领域。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICML 2024 Workshop on Foundation Models in the Wild",
      "pdf_url": "http://arxiv.org/pdf/2407.03482v2",
      "published_date": "2024-07-03 20:10:55 UTC",
      "updated_date": "2024-07-10 13:27:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:09:14.922650"
    },
    {
      "arxiv_id": "2407.03473v1",
      "title": "Exploring LGBTQ+ Bias in Generative AI Answers across Different Country and Religious Contexts",
      "title_zh": "翻译失败",
      "authors": [
        "Lilla Vicsek",
        "Anna Vancsó",
        "Mike Zajko",
        "Judit Takacs"
      ],
      "abstract": "Previous discussions have highlighted the need for generative AI tools to\nbecome more culturally sensitive, yet often neglect the complexities of\nhandling content about minorities, who are perceived differently across\ncultures and religions. Our study examined how two generative AI systems\nrespond to homophobic statements with varying cultural and religious context\ninformation. Findings showed ChatGPT 3.5's replies exhibited cultural\nrelativism, in contrast to Bard's, which stressed human rights and provided\nmore support for LGBTQ+ issues. Both demonstrated significant change in\nresponses based on contextual information provided in the prompts, suggesting\nthat AI systems may adjust in their responses the degree and forms of support\nfor LGBTQ+ people according to information they receive about the user's\nbackground. The study contributes to understanding the social and ethical\nimplications of AI responses and argues that any work to make generative AI\noutputs more culturally diverse requires a grounding in fundamental human\nrights.",
      "tldr_zh": "这篇论文探讨了生成式 AI 在不同国家和宗教背景下对 LGBTQ+ 相关偏见的响应，强调 AI 需要更敏感地处理少数群体议题。研究方法涉及测试 ChatGPT 3.5 和 Bard 对带有文化宗教背景的同性恋phobic 声明的回复，结果显示 ChatGPT 3.5 倾向于文化相对主义，而 Bard 更强调人权并提供更多 LGBTQ+ 支持。发现表明，AI 系统会根据提示中的上下文信息显著调整对 LGBTQ+ 人的支持程度。该研究揭示了 AI 响应的社会伦理影响，并主张在提升 AI 文化多样性时，必须以基本人权为基础。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03473v1",
      "published_date": "2024-07-03 19:38:19 UTC",
      "updated_date": "2024-07-03 19:38:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:09:26.960995"
    },
    {
      "arxiv_id": "2407.03469v1",
      "title": "Scaling Data-Driven Building Energy Modelling using Large Language Models",
      "title_zh": "利用大型语言模型扩展数据驱动的建筑能源建模",
      "authors": [
        "Sunil Khadka",
        "Liang Zhang"
      ],
      "abstract": "Building Management System (BMS) through a data-driven method always faces\ndata and model scalability issues. We propose a methodology to tackle the\nscalability challenges associated with the development of data-driven models\nfor BMS by using Large Language Models (LLMs). LLMs' code generation\nadaptability can enable broader adoption of BMS by \"automating the automation,\"\nparticularly the data handling and data-driven modeling processes. In this\npaper, we use LLMs to generate code that processes structured data from BMS and\nbuild data-driven models for BMS's specific requirements. This eliminates the\nneed for manual data and model development, reducing the time, effort, and cost\nassociated with this process. Our hypothesis is that LLMs can incorporate\ndomain knowledge about data science and BMS into data processing and modeling,\nensuring that the data-driven modeling is automated for specific requirements\nof different building types and control objectives, which also improves\naccuracy and scalability. We generate a prompt template following the framework\nof Machine Learning Operations so that the prompts are designed to\nsystematically generate Python code for data-driven modeling. Our case study\nindicates that bi-sequential prompting under the prompt template can achieve a\nhigh success rate of code generation and code accuracy, and significantly\nreduce human labor costs.",
      "tldr_zh": "本文提出一种利用 Large Language Models (LLMs) 的方法来解决 Building Management System (BMS) 的数据和模型可扩展性挑战，通过自动生成代码来处理结构化数据并构建数据驱动模型。研究设计了基于 Machine Learning Operations 框架的提示模板，并采用双序列提示（bi-sequential prompting）来系统生成 Python 代码，从而消除手动开发需求并减少时间、努力和成本。实验结果显示，该方法在案例研究中实现了高代码生成成功率和准确性，同时显著降低了人力成本。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03469v1",
      "published_date": "2024-07-03 19:34:24 UTC",
      "updated_date": "2024-07-03 19:34:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:09:39.330939"
    },
    {
      "arxiv_id": "2407.03463v1",
      "title": "Precision at Scale: Domain-Specific Datasets On-Demand",
      "title_zh": "翻译失败",
      "authors": [
        "Jesús M Rodríguez-de-Vera",
        "Imanol G Estepa",
        "Ignacio Sarasúa",
        "Bhalaji Nagarajan",
        "Petia Radeva"
      ],
      "abstract": "In the realm of self-supervised learning (SSL), conventional wisdom has\ngravitated towards the utility of massive, general domain datasets for\npretraining robust backbones. In this paper, we challenge this idea by\nexploring if it is possible to bridge the scale between general-domain datasets\nand (traditionally smaller) domain-specific datasets to reduce the current\nperformance gap. More specifically, we propose Precision at Scale (PaS), a\nnovel method for the autonomous creation of domain-specific datasets on-demand.\nThe modularity of the PaS pipeline enables leveraging state-of-the-art\nfoundational and generative models to create a collection of images of any\ngiven size belonging to any given domain with minimal human intervention.\nExtensive analysis in two complex domains, proves the superiority of PaS\ndatasets over existing traditional domain-specific datasets in terms of\ndiversity, scale, and effectiveness in training visual transformers and\nconvolutional neural networks. Most notably, we prove that automatically\ngenerated domain-specific datasets lead to better pretraining than large-scale\nsupervised datasets such as ImageNet-1k and ImageNet-21k. Concretely, models\ntrained on domain-specific datasets constructed by PaS pipeline, beat\nImageNet-1k pretrained backbones by at least 12% in all the considered domains\nand classification tasks and lead to better food domain performance than\nsupervised ImageNet-21k pretrain while being 12 times smaller. Code repository:\nhttps://github.com/jesusmolrdv/Precision-at-Scale/",
      "tldr_zh": "本论文挑战了自监督学习(SSL)中依赖大规模通用数据集的传统观点，提出Precision at Scale (PaS)方法，用于自动创建特定领域数据集以缩小性能差距。PaS管道模块化，利用最先进的基础和生成模型，能以最小的人工干预生成任意规模和领域的图像集合。实验在两个复杂领域证明，PaS数据集在多样性、规模和训练视觉变压器及卷积神经网络的有效性上优于传统数据集，且模型在PaS数据集上训练比ImageNet-1k预训练至少提升12%的分类性能，甚至在食物领域超越ImageNet-21k预训练，而数据集规模仅为其1/12。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.5.4; I.5.2; I.2.1; I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03463v1",
      "published_date": "2024-07-03 19:17:42 UTC",
      "updated_date": "2024-07-03 19:17:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:09:50.656875"
    },
    {
      "arxiv_id": "2407.03460v1",
      "title": "Collaborative Quest Completion with LLM-driven Non-Player Characters in Minecraft",
      "title_zh": "翻译失败",
      "authors": [
        "Sudha Rao",
        "Weijia Xu",
        "Michael Xu",
        "Jorge Leandro",
        "Ken Lobb",
        "Gabriel DesGarennes",
        "Chris Brockett",
        "Bill Dolan"
      ],
      "abstract": "The use of generative AI in video game development is on the rise, and as the\nconversational and other capabilities of large language models continue to\nimprove, we expect LLM-driven non-player characters (NPCs) to become widely\ndeployed. In this paper, we seek to understand how human players collaborate\nwith LLM-driven NPCs to accomplish in-game goals. We design a minigame within\nMinecraft where a player works with two GPT4-driven NPCs to complete a quest.\nWe perform a user study in which 28 Minecraft players play this minigame and\nshare their feedback. On analyzing the game logs and recordings, we find that\nseveral patterns of collaborative behavior emerge from the NPCs and the human\nplayers. We also report on the current limitations of language-only models that\ndo not have rich game-state or visual understanding. We believe that this\npreliminary study and analysis will inform future game developers on how to\nbetter exploit these rapidly improving generative AI models for collaborative\nroles in games.",
      "tldr_zh": "这篇论文探讨了在 Minecraft 中使用 LLM（大型语言模型）驱动的 NPC（非玩家角色）与人类玩家协作完成任务的可能性。研究者设计了一个迷你游戏，让玩家与两个 GPT4 驱动的 NPC 合作执行任务，并通过用户研究收集了 28 名玩家的反馈和游戏日志。分析结果揭示了多种协作行为模式，包括玩家与 NPC 的互动方式，同时指出了语言模型在缺乏游戏状态和视觉理解方面的局限性。该研究为未来游戏开发者提供了如何更好地利用生成式 AI 模型来提升协作角色的初步指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at Wordplay workshop at ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.03460v1",
      "published_date": "2024-07-03 19:11:21 UTC",
      "updated_date": "2024-07-03 19:11:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:10:03.275811"
    },
    {
      "arxiv_id": "2407.03446v1",
      "title": "Towards Asimov's Psychohistory: Harnessing Topological Data Analysis, Artificial Intelligence and Social Media data to Forecast Societal Trends",
      "title_zh": "翻译失败",
      "authors": [
        "Isabela Rocha"
      ],
      "abstract": "In the age of big data and advanced computational methods, the prediction of\nlarge-scale social behaviors, reminiscent of Isaac Asimov's fictional science\nof Psychohistory, is becoming increasingly feasible. This paper consists of a\ntheoretical exploration of the integration of computational power and\nmathematical frameworks, particularly through Topological Data Analysis (TDA)\n(Carlsson, Vejdemo-Johansson, 2022) and Artificial Intelligence (AI), to\nforecast societal trends through social media data analysis. By examining\nsocial media as a reflective surface of collective human behavior through the\nsystematic behaviorist approach (Glenn, et al., 2016), I argue that these tools\nprovide unprecedented clarity into the dynamics of large communities. This\nstudy dialogues with Asimov's work, drawing parallels between his visionary\nconcepts and contemporary methodologies, illustrating how modern computational\ntechniques can uncover patterns and predict shifts in social behavior,\ncontributing to the emerging field of digital sociology -- or even,\nPsychohistory itself.",
      "tldr_zh": "这篇论文探讨了利用 Topological Data Analysis (TDA) 和 Artificial Intelligence (AI) 来分析 Social Media data，从而预测大规模社会行为的可能性，类似于 Isaac Asimov 的虚构科学 Psychohistory。作者通过系统行为主义方法，将社交媒体视为人类集体行为的反映，构建了一个理论框架来揭示社会动态和趋势模式。研究强调，这些现代计算技术能提供前所未有的洞见，帮助预测社会行为变化，并为数字社会学或 Psychohistory 的发展做出贡献。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "comment": "21 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.03446v1",
      "published_date": "2024-07-03 18:44:36 UTC",
      "updated_date": "2024-07-03 18:44:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:10:16.218007"
    },
    {
      "arxiv_id": "2407.12835v2",
      "title": "Regurgitative Training: The Value of Real Data in Training Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jinghui Zhang",
        "Dandan Qiao",
        "Mochen Yang",
        "Qiang Wei"
      ],
      "abstract": "What happens if we train a new Large Language Model (LLM) using data that are\nat least partially generated by other LLMs? The explosive success of LLMs means\nthat a substantial amount of content online will be generated by LLMs rather\nthan humans, which will inevitably enter the training datasets of\nnext-generation LLMs. We evaluate the implications of such \"regurgitative\ntraining\" on LLM performance. Through fine-tuning GPT-3.5 with data generated\neither by itself or by other LLMs in a machine translation task, we find strong\nevidence that regurgitative training clearly handicaps the performance of LLMs.\nThe same performance loss of regurgitative training is observed on transformer\nmodels that we train from scratch. We find suggestive evidence that the\nperformance disadvantage of regurgitative training can be attributed to at\nleast two mechanisms: (1) higher error rates and (2) lower lexical diversity in\nLLM-generated data as compared to real data. Based on these mechanisms, we\npropose and evaluate three different strategies to mitigate the performance\nloss of regurgitative training. First, we devise data-driven metrics to gauge\nthe quality of each LLM-generated data instance, and then carry out an ordered\ntraining process where high-quality data are added before low-quality ones.\nSecond, we combine data generated by multiple different LLMs (as an attempt to\nincrease lexical diversity). Third, we train an AI detection classifier to\ndifferentiate between LLM- and human-generated data, and include LLM-generated\ndata in the order of resemblance to human-generated data. All three strategies\ncan improve the performance of regurgitative training to some extent but are\nnot always able to fully close the gap from training with real data. Our\nresults highlight the value of real, human-generated data in training LLMs,\nwhich cannot be easily substituted by synthetic, LLM-generated data.",
      "tldr_zh": "本文研究了“regurgitative training”（使用部分由其他 Large Language Models (LLMs) 生成的数据训练新 LLM）的负面影响，通过在机器翻译任务中微调 GPT-3.5 和从 scratch 训练 transformer 模型，发现这种训练方式会显著降低模型性能，主要原因是 LLM 生成数据存在更高的错误率和较低的词汇多样性。研究者提出了三种缓解策略：(1) 使用数据驱动指标按质量顺序添加数据，(2) 结合多个不同 LLMs 生成的数据以提升多样性，(3) 训练 AI 检测分类器按与人类数据相似度顺序包含 LLM 生成数据。这些策略能部分改善性能，但无法完全弥补真实人类生成数据的优势。最终，论文强调了真实数据在训练 LLMs 中的不可替代价值。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12835v2",
      "published_date": "2024-07-03 18:42:55 UTC",
      "updated_date": "2024-07-25 16:50:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:10:40.249868"
    },
    {
      "arxiv_id": "2407.11030v1",
      "title": "DLO: Dynamic Layer Operation for Efficient Vertical Scaling of LLMs",
      "title_zh": "DLO：动态层操作，用于大型语言模型的高效垂直缩放",
      "authors": [
        "Zhen Tan",
        "Daize Dong",
        "Xinyu Zhao",
        "Jie Peng",
        "Yu Cheng",
        "Tianlong Chen"
      ],
      "abstract": "In this paper, we introduce Dynamic Layer Operations (DLO), a novel approach\nfor vertically scaling transformer-based Large Language Models (LLMs) by\ndynamically expanding, activating, or skipping layers using a sophisticated\nrouting policy based on layerwise feature similarity. Unlike traditional\nMixture-of-Experts (MoE) methods that focus on extending the model width, our\napproach targets model depth, addressing the redundancy observed across layer\nrepresentations for various input samples. Our framework is integrated with the\nSupervised Fine-Tuning (SFT) stage, eliminating the need for resource-intensive\nContinual Pre-Training (CPT). Experimental results demonstrate that DLO not\nonly outperforms the original unscaled models but also achieves comparable\nresults to densely expanded models with significantly improved efficiency. Our\nwork offers a promising direction for building efficient yet powerful LLMs. We\nwill release our implementation and model weights upon acceptance.",
      "tldr_zh": "本论文提出 Dynamic Layer Operations (DLO)，一种创新方法，用于高效垂直扩展基于 Transformer 的 Large Language Models (LLMs)，通过动态扩展、激活或跳过层来处理模型深度的冗余。DLO 采用基于层级特征相似性的路由策略，与传统的 Mixture-of-Experts (MoE) 方法不同，它专注于模型深度优化，并将其集成到 Supervised Fine-Tuning (SFT) 阶段，从而避免了资源密集的 Continual Pre-Training (CPT)。实验结果显示，DLO 不仅在性能上超越原模型，还与密集扩展模型相当，同时显著提高了效率。该工作为构建高效且强大的 LLMs 提供了新方向，并计划发布实现代码和模型权重。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11030v1",
      "published_date": "2024-07-03 18:34:08 UTC",
      "updated_date": "2024-07-03 18:34:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:10:41.253786"
    },
    {
      "arxiv_id": "2407.15851v2",
      "title": "A Survey on Trustworthiness in Foundation Models for Medical Image Analysis",
      "title_zh": "基础模型在医疗图像分析中的可信性调查",
      "authors": [
        "Congzhen Shi",
        "Ryan Rezai",
        "Jiaxi Yang",
        "Qi Dou",
        "Xiaoxiao Li"
      ],
      "abstract": "The rapid advancement of foundation models in medical imaging represents a\nsignificant leap toward enhancing diagnostic accuracy and personalized\ntreatment. However, the deployment of foundation models in healthcare\nnecessitates a rigorous examination of their trustworthiness, encompassing\nprivacy, robustness, reliability, explainability, and fairness. The current\nbody of survey literature on foundation models in medical imaging reveals\nconsiderable gaps, particularly in the area of trustworthiness. Additionally,\nexisting surveys on the trustworthiness of foundation models do not adequately\naddress their specific variations and applications within the medical imaging\ndomain. This survey aims to fill that gap by presenting a novel taxonomy of\nfoundation models used in medical imaging and analyzing the key motivations for\nensuring their trustworthiness. We review current research on foundation models\nin major medical imaging applications, focusing on segmentation, medical report\ngeneration, medical question and answering (Q\\&A), and disease diagnosis. These\nareas are highlighted because they have seen a relatively mature and\nsubstantial number of foundation models compared to other applications. We\nfocus on literature that discusses trustworthiness in medical image analysis\nmanuscripts. We explore the complex challenges of building trustworthy\nfoundation models for each application, summarizing current concerns and\nstrategies for enhancing trustworthiness. Furthermore, we examine the potential\nof these models to revolutionize patient care. Our analysis underscores the\nimperative for advancing towards trustworthy AI in medical image analysis,\nadvocating for a balanced approach that fosters innovation while ensuring\nethical and equitable healthcare delivery.",
      "tldr_zh": "这篇调查论文探讨了 foundation models 在医疗图像分析中的 trustworthiness，包括隐私、鲁棒性、可靠性、可解释性和公平性，旨在填补现有文献的空白。论文提出一个新的 taxonomy，并分析确保 trustworthiness 的关键动机，聚焦于分割、医疗报告生成、医疗 Q&A 和疾病诊断等应用领域。作者总结了构建可信模型的挑战和策略，强调这些模型潜力革命化患者护理，同时呼吁平衡创新与伦理公平的医疗交付。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15851v2",
      "published_date": "2024-07-03 18:07:57 UTC",
      "updated_date": "2024-10-07 02:03:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:10:52.875164"
    },
    {
      "arxiv_id": "2407.03418v1",
      "title": "HEMM: Holistic Evaluation of Multimodal Foundation Models",
      "title_zh": "HEMM：多模态基础模型的整体评估",
      "authors": [
        "Paul Pu Liang",
        "Akshay Goindani",
        "Talha Chafekar",
        "Leena Mathur",
        "Haofei Yu",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "abstract": "Multimodal foundation models that can holistically process text alongside\nimages, video, audio, and other sensory modalities are increasingly used in a\nvariety of real-world applications. However, it is challenging to characterize\nand study progress in multimodal foundation models, given the range of possible\nmodeling decisions, tasks, and domains. In this paper, we introduce Holistic\nEvaluation of Multimodal Models (HEMM) to systematically evaluate the\ncapabilities of multimodal foundation models across a set of 3 dimensions:\nbasic skills, information flow, and real-world use cases. Basic multimodal\nskills are internal abilities required to solve problems, such as learning\ninteractions across modalities, fine-grained alignment, multi-step reasoning,\nand the ability to handle external knowledge. Information flow studies how\nmultimodal content changes during a task through querying, translation,\nediting, and fusion. Use cases span domain-specific challenges introduced in\nreal-world multimedia, affective computing, natural sciences, healthcare, and\nhuman-computer interaction applications. Through comprehensive experiments\nacross the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g.,\nbasic skills, information flows, and use cases) that pose challenges to today's\nmodels, and (2) distill performance trends regarding how different modeling\ndimensions (e.g., scale, pre-training data, multimodal alignment, pre-training,\nand instruction tuning objectives) influence performance. Our conclusions\nregarding challenging multimodal interactions, use cases, and tasks requiring\nreasoning and external knowledge, the benefits of data and model scale, and the\nimpacts of instruction tuning yield actionable insights for future work in\nmultimodal foundation models.",
      "tldr_zh": "这篇论文引入了HEMM框架，用于全面评估多模态基础模型（Multimodal Foundation Models）的能力，涵盖基本技能、信息流和真实世界用例三个维度。基本技能包括跨模态学习、细粒度对齐、多步推理和处理外部知识；信息流考察多模态内容的查询、翻译、编辑和融合；真实世界用例涉及多媒体、情感计算、自然科学、医疗和人机交互等领域。通过对30个任务的实验，论文识别了当前模型面临的挑战（如推理和外部知识需求），并分析了建模因素（如模型规模、预训练数据和指令微调）对性能的影响，提供可行动的见解以指导未来研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Code available at https://github.com/pliang279/HEMM",
      "pdf_url": "http://arxiv.org/pdf/2407.03418v1",
      "published_date": "2024-07-03 18:00:48 UTC",
      "updated_date": "2024-07-03 18:00:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:11:04.787719"
    },
    {
      "arxiv_id": "2407.17490v1",
      "title": "AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxiang Chai",
        "Siyuan Huang",
        "Yazhe Niu",
        "Han Xiao",
        "Liang Liu",
        "Dingyu Zhang",
        "Peng Gao",
        "Shuai Ren",
        "Hongsheng Li"
      ],
      "abstract": "AI agents have drawn increasing attention mostly on their ability to perceive\nenvironments, understand tasks, and autonomously achieve goals. To advance\nresearch on AI agents in mobile scenarios, we introduce the Android\nMulti-annotation EXpo (AMEX), a comprehensive, large-scale dataset designed for\ngeneralist mobile GUI-control agents. Their capabilities of completing complex\ntasks by directly interacting with the graphical user interface (GUI) on mobile\ndevices are trained and evaluated with the proposed dataset. AMEX comprises\nover 104K high-resolution screenshots from 110 popular mobile applications,\nwhich are annotated at multiple levels. Unlike existing mobile device-control\ndatasets, e.g., MoTIF, AitW, etc., AMEX includes three levels of annotations:\nGUI interactive element grounding, GUI screen and element functionality\ndescriptions, and complex natural language instructions, each averaging 13\nsteps with stepwise GUI-action chains. We develop this dataset from a more\ninstructive and detailed perspective, complementing the general settings of\nexisting datasets. Additionally, we develop a baseline model SPHINX Agent and\ncompare its performance across state-of-the-art agents trained on other\ndatasets. To facilitate further research, we open-source our dataset, models,\nand relevant evaluation tools. The project is available at\nhttps://yuxiangchai.github.io/AMEX/",
      "tldr_zh": "该研究引入了 AMEX 数据集，这是一个针对 Android 移动 GUI 控制 AI 代理的全面、大规模数据集，包含超过 104K 张高分辨率截图来自 110 个热门应用，并提供三层注释：GUI 交互元素 grounding、屏幕和元素功能描述，以及平均 13 步的复杂自然语言指令和操作链。相比现有数据集如 MoTIF 和 AitW，AMEX 更注重详细指导和多层注解，以提升代理在移动场景中的任务理解和自主交互能力。研究团队开发了基线模型 SPHINX Agent，并与最先进代理进行了性能比较，展示了 AMEX 的优势。最后，该数据集、模型和评估工具已开源，以推动相关研究。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17490v1",
      "published_date": "2024-07-03 17:59:58 UTC",
      "updated_date": "2024-07-03 17:59:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:11:18.165854"
    },
    {
      "arxiv_id": "2407.03321v2",
      "title": "Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Max Zuo",
        "Francisco Piedrahita Velez",
        "Xiaochen Li",
        "Michael L. Littman",
        "Stephen H. Bach"
      ],
      "abstract": "Recent works have explored using language models for planning problems. One\napproach examines translating natural language descriptions of planning tasks\ninto structured planning languages, such as the planning domain definition\nlanguage (PDDL). Existing evaluation methods struggle to ensure semantic\ncorrectness and rely on simple or unrealistic datasets. To bridge this gap, we\nintroduce \\textit{Planetarium}, a benchmark designed to evaluate language\nmodels' ability to generate PDDL code from natural language descriptions of\nplanning tasks. \\textit{Planetarium} features a novel PDDL equivalence\nalgorithm that flexibly evaluates the correctness of generated PDDL, along with\na dataset of 145,918 text-to-PDDL pairs across 73 unique state combinations\nwith varying levels of difficulty. Finally, we evaluate several API-access and\nopen-weight language models that reveal this task's complexity. For example,\n96.1\\% of the PDDL problem descriptions generated by GPT-4o are syntactically\nparseable, 94.4\\% are solvable, but only 24.8\\% are semantically correct,\nhighlighting the need for a more rigorous benchmark for this problem.",
      "tldr_zh": "该研究引入了 Planetarium 基准，用于严格评估语言模型将自然语言描述转化为结构化规划语言（如 PDDL）的能力，以解决现有方法在语义正确性和数据集真实性上的不足。Planetarium 包括一个创新的 PDDL 等价算法和一个包含 145,918 个文本到 PDDL 对的数据集，涵盖 73 个独特状态组合及不同难度水平。实验结果显示，即使 GPT-4o 生成的 PDDL 中 96.1% 可语法解析和 94.4% 可解决，但仅有 24.8% 语义正确，这突显了该任务的复杂性和对更严格基准的需求。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL Main Conference 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.03321v2",
      "published_date": "2024-07-03 17:59:53 UTC",
      "updated_date": "2025-02-10 19:05:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:11:27.316223"
    },
    {
      "arxiv_id": "2407.03311v3",
      "title": "Efficient Imitation Without Demonstrations via Value-Penalized Auxiliary Control from Examples",
      "title_zh": "翻译失败",
      "authors": [
        "Trevor Ablett",
        "Bryan Chan",
        "Jayce Haoran Wang",
        "Jonathan Kelly"
      ],
      "abstract": "Common approaches to providing feedback in reinforcement learning are the use\nof hand-crafted rewards or full-trajectory expert demonstrations.\nAlternatively, one can use examples of completed tasks, but such an approach\ncan be extremely sample inefficient. We introduce value-penalized auxiliary\ncontrol from examples (VPACE), an algorithm that significantly improves\nexploration in example-based control by adding examples of simple auxiliary\ntasks and an above-success-level value penalty. Across both simulated and real\nrobotic environments, we show that our approach substantially improves learning\nefficiency for challenging tasks, while maintaining bounded value estimates.\nPreliminary results also suggest that VPACE may learn more efficiently than the\nmore common approaches of using full trajectories or true sparse rewards.\nProject site: https://papers.starslab.ca/vpace/ .",
      "tldr_zh": "这篇论文提出了 VPACE（value-penalized auxiliary control from examples）算法，用于强化学习中的高效模仿学习，通过任务例子进行训练，而非依赖手工奖励或完整轨迹演示。VPACE 通过添加简单辅助任务的例子和一个高于成功水平的价值惩罚，显著改善了探索效率，并在模拟及真实机器人环境中提高了挑战任务的学习效率，同时保持价值估计的界限。初步结果显示，VPACE 可能比使用完整轨迹或真实稀疏奖励的方法更高效，为样本低效问题提供了新解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to the IEEE International Conference on Robotics and\n  Automation (ICRA'25), Atlanta, USA, May 19-23, 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.03311v3",
      "published_date": "2024-07-03 17:54:11 UTC",
      "updated_date": "2025-03-02 02:45:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:11:46.884772"
    },
    {
      "arxiv_id": "2407.03308v1",
      "title": "Accelerated Proton Resonance Frequency-based Magnetic Resonance Thermometry by Optimized Deep Learning Method",
      "title_zh": "翻译失败",
      "authors": [
        "Sijie Xu",
        "Shenyan Zong",
        "Chang-Sheng Mei",
        "Guofeng Shen",
        "Yueran Zhao",
        "He Wang"
      ],
      "abstract": "Proton resonance frequency (PRF) based MR thermometry is essential for\nfocused ultrasound (FUS) thermal ablation therapies. This work aims to enhance\ntemporal resolution in dynamic MR temperature map reconstruction using an\nimproved deep learning method. The training-optimized methods and five\nclassical neural networks were applied on the 2-fold and 4-fold under-sampling\nk-space data to reconstruct the temperature maps. The enhanced training modules\nincluded offline/online data augmentations, knowledge distillation, and the\namplitude-phase decoupling loss function. The heating experiments were\nperformed by a FUS transducer on phantom and ex vivo tissues, respectively.\nThese data were manually under-sampled to imitate acceleration procedures and\ntrained in our method to get the reconstruction model. The additional dozen or\nso testing datasets were separately obtained for evaluating the real-time\nperformance and temperature accuracy. Acceleration factors of 1.9 and 3.7 were\nfound for 2 times and 4 times k-space under-sampling strategies and the\nResUNet-based deep learning reconstruction performed exceptionally well. In\n2-fold acceleration scenario, the RMSE of temperature map patches provided the\nvalues of 0.888 degree centigrade and 1.145 degree centigrade on phantom and ex\nvivo testing datasets. The DICE value of temperature areas enclosed by 43\ndegree centigrade isotherm was 0.809, and the Bland-Altman analysis showed a\nbias of -0.253 degree centigrade with the apart of plus or minus 2.16 degree\ncentigrade. In 4 times under-sampling case, these evaluating values decreased\nby approximately 10%. This study demonstrates that deep learning-based\nreconstruction can significantly enhance the accuracy and efficiency of MR\nthermometry for clinical FUS thermal therapies.",
      "tldr_zh": "本研究旨在通过优化深度学习方法提升Proton Resonance Frequency (PRF) based Magnetic Resonance Thermometry的temporal resolution，以支持Focused Ultrasound (FUS)热消融治疗。研究引入了增强训练模块，包括offline/online data augmentations、knowledge distillation和amplitude-phase decoupling loss function，并将五种经典神经网络应用于2-fold和4-fold under-sampling k-space数据上。实验结果显示，ResUNet-based方法在phantom和ex vivo组织上实现了加速因子分别为1.9和3.7，温度地图的RMSE值低于1.145°C，DICE值达0.809，且Bland-Altman分析显示偏差仅为-0.253°C。该方法显著提高了MR thermometry的准确性和效率，为临床FUS热疗提供可靠的实时重建技术。",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03308v1",
      "published_date": "2024-07-03 17:49:38 UTC",
      "updated_date": "2024-07-03 17:49:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:11:52.725765"
    },
    {
      "arxiv_id": "2407.03300v1",
      "title": "DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents",
      "title_zh": "DisCo-Diff：使用离散潜在变量增强连续扩散模型",
      "authors": [
        "Yilun Xu",
        "Gabriele Corso",
        "Tommi Jaakkola",
        "Arash Vahdat",
        "Karsten Kreis"
      ],
      "abstract": "Diffusion models (DMs) have revolutionized generative learning. They utilize\na diffusion process to encode data into a simple Gaussian distribution.\nHowever, encoding a complex, potentially multimodal data distribution into a\nsingle continuous Gaussian distribution arguably represents an unnecessarily\nchallenging learning problem. We propose Discrete-Continuous Latent Variable\nDiffusion Models (DisCo-Diff) to simplify this task by introducing\ncomplementary discrete latent variables. We augment DMs with learnable discrete\nlatents, inferred with an encoder, and train DM and encoder end-to-end.\nDisCo-Diff does not rely on pre-trained networks, making the framework\nuniversally applicable. The discrete latents significantly simplify learning\nthe DM's complex noise-to-data mapping by reducing the curvature of the DM's\ngenerative ODE. An additional autoregressive transformer models the\ndistribution of the discrete latents, a simple step because DisCo-Diff requires\nonly few discrete variables with small codebooks. We validate DisCo-Diff on toy\ndata, several image synthesis tasks as well as molecular docking, and find that\nintroducing discrete latents consistently improves model performance. For\nexample, DisCo-Diff achieves state-of-the-art FID scores on class-conditioned\nImageNet-64/128 datasets with ODE sampler.",
      "tldr_zh": "本研究提出 DisCo-Diff 框架，通过引入互补的离散潜在变量来提升连续扩散模型 (DMs) 的性能，旨在简化将复杂数据分布编码为单一高斯分布的学习挑战。方法包括使用编码器推断可学习的离散潜在变量，并端到端训练 DM 和编码器，同时结合自回归变换器来建模这些变量的分布，从而减少 DM 生成 ODE 的曲率。实验在玩具数据、图像合成任务（如 ImageNet-64/128 上实现最先进 FID 分数）和分子对接等场景中验证，显示 DisCo-Diff 显著提高了模型性能，例如在条件图像生成任务中表现突出。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "project page: https://research.nvidia.com/labs/lpr/disco-diff",
      "pdf_url": "http://arxiv.org/pdf/2407.03300v1",
      "published_date": "2024-07-03 17:42:46 UTC",
      "updated_date": "2024-07-03 17:42:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:12:05.775965"
    },
    {
      "arxiv_id": "2407.03297v2",
      "title": "Improved Noise Schedule for Diffusion Training",
      "title_zh": "改进的噪声调度用于扩散训练",
      "authors": [
        "Tiankai Hang",
        "Shuyang Gu",
        "Xin Geng",
        "Baining Guo"
      ],
      "abstract": "Diffusion models have emerged as the de facto choice for generating\nhigh-quality visual signals across various domains. However, training a single\nmodel to predict noise across various levels poses significant challenges,\nnecessitating numerous iterations and incurring significant computational\ncosts. Various approaches, such as loss weighting strategy design and\narchitectural refinements, have been introduced to expedite convergence and\nimprove model performance. In this study, we propose a novel approach to design\nthe noise schedule for enhancing the training of diffusion models. Our key\ninsight is that the importance sampling of the logarithm of the Signal-to-Noise\nratio ($\\log \\text{SNR}$), theoretically equivalent to a modified noise\nschedule, is particularly beneficial for training efficiency when increasing\nthe sample frequency around $\\log \\text{SNR}=0$. This strategic sampling allows\nthe model to focus on the critical transition point between signal dominance\nand noise dominance, potentially leading to more robust and accurate\npredictions.We empirically demonstrate the superiority of our noise schedule\nover the standard cosine schedule.Furthermore, we highlight the advantages of\nour noise schedule design on the ImageNet benchmark, showing that the designed\nschedule consistently benefits different prediction targets. Our findings\ncontribute to the ongoing efforts to optimize diffusion models, potentially\npaving the way for more efficient and effective training paradigms in the field\nof generative AI.",
      "tldr_zh": "该研究针对扩散模型（Diffusion models）的训练挑战，提出了一种改进的噪声调度（noise schedule）设计，以减少迭代次数和计算成本。其关键洞见是通过对信号噪声比的对数（log SNR）的importance sampling，增加log SNR=0附近的采样频率，让模型更关注信号与噪声主导的过渡点，从而提升训练效率。实验结果显示，该方法优于标准cosine schedule，并在ImageNet基准上为不同预测目标带来一致的性能提升。该创新为生成AI领域的扩散模型优化提供了更高效的训练范式。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03297v2",
      "published_date": "2024-07-03 17:34:55 UTC",
      "updated_date": "2024-11-27 15:10:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:12:17.043942"
    },
    {
      "arxiv_id": "2407.03291v2",
      "title": "VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Sun",
        "Navid Salami Pargoo",
        "Taqiya Ehsan",
        "Zhao Zhang",
        "Jorge Ortiz"
      ],
      "abstract": "Complex human activity recognition (CHAR) remains a pivotal challenge within\nubiquitous computing, especially in the context of smart environments. Existing\nstudies typically require meticulous labeling of both atomic and complex\nactivities, a task that is labor-intensive and prone to errors due to the\nscarcity and inaccuracies of available datasets. Most prior research has\nfocused on datasets that either precisely label atomic activities or, at\nminimum, their sequence approaches that are often impractical in real world\nsettings.In response, we introduce VCHAR (Variance-Driven Complex Human\nActivity Recognition), a novel framework that treats the outputs of atomic\nactivities as a distribution over specified intervals. Leveraging generative\nmethodologies, VCHAR elucidates the reasoning behind complex activity\nclassifications through video-based explanations, accessible to users without\nprior machine learning expertise. Our evaluation across three publicly\navailable datasets demonstrates that VCHAR enhances the accuracy of complex\nactivity recognition without necessitating precise temporal or sequential\nlabeling of atomic activities. Furthermore, user studies confirm that VCHAR's\nexplanations are more intelligible compared to existing methods, facilitating a\nbroader understanding of complex activity recognition among non-experts.",
      "tldr_zh": "本研究针对复杂人类活动识别 (CHAR) 在智能环境中的挑战，提出 VCHAR 框架，该框架通过 Variance-Driven 方法将原子活动的输出视为指定时间间隔上的分布，并结合 Generative Representation 生成基于视频的解释，以减少对精确时序或顺序标注的依赖。VCHAR 使复杂活动分类的推理过程更易于理解，尤其适合没有机器学习背景的用户。在三个公开数据集上的评估显示，该框架显著提高了识别准确性，用户研究进一步证实其解释比现有方法更具可读性和可解释性。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.HC",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03291v2",
      "published_date": "2024-07-03 17:24:36 UTC",
      "updated_date": "2024-08-06 15:14:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:12:31.580129"
    },
    {
      "arxiv_id": "2407.03266v1",
      "title": "Do Quantum Neural Networks have Simplicity Bias?",
      "title_zh": "量子神经网络有简单性偏置吗？",
      "authors": [
        "Jessica Pointing"
      ],
      "abstract": "One hypothesis for the success of deep neural networks (DNNs) is that they\nare highly expressive, which enables them to be applied to many problems, and\nthey have a strong inductive bias towards solutions that are simple, known as\nsimplicity bias, which allows them to generalise well on unseen data because\nmost real-world data is structured (i.e. simple). In this work, we explore the\ninductive bias and expressivity of quantum neural networks (QNNs), which gives\nus a way to compare their performance to those of DNNs. Our results show that\nit is possible to have simplicity bias with certain QNNs, but we prove that\nthis type of QNN limits the expressivity of the QNN. We also show that it is\npossible to have QNNs with high expressivity, but they either have no inductive\nbias or a poor inductive bias and result in a worse generalisation performance\ncompared to DNNs. We demonstrate that an artificial (restricted) inductive bias\ncan be produced by intentionally restricting the expressivity of a QNN. Our\nresults suggest a bias-expressivity tradeoff. Our conclusion is that the QNNs\nwe studied can not generally offer an advantage over DNNs, because these QNNs\neither have a poor inductive bias or poor expressivity compared to DNNs.",
      "tldr_zh": "这篇论文探讨了量子神经网络 (QNNs) 是否具有简单性偏置 (simplicity bias)，并将其与深度神经网络 (DNNs) 的高表达性和归纳偏置 (inductive bias) 进行比较。研究发现，某些 QNNs 可以实现简单性偏置，但这会限制它们的表达性 (expressivity)。然而，高表达性的 QNNs 往往缺乏有效归纳偏置，导致泛化性能不如 DNNs，且通过人为限制表达性只能产生人工偏置。总体结论是，QNNs 存在偏置与表达性的权衡，无法普遍优于 DNNs。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "physics.app-ph"
      ],
      "primary_category": "quant-ph",
      "comment": "9 pages, 42 pages with appendices",
      "pdf_url": "http://arxiv.org/pdf/2407.03266v1",
      "published_date": "2024-07-03 16:56:08 UTC",
      "updated_date": "2024-07-03 16:56:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:12:42.143899"
    },
    {
      "arxiv_id": "2407.03245v3",
      "title": "TieBot: Learning to Knot a Tie from Visual Demonstration through a Real-to-Sim-to-Real Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Weikun Peng",
        "Jun Lv",
        "Yuwei Zeng",
        "Haonan Chen",
        "Siheng Zhao",
        "Jichen Sun",
        "Cewu Lu",
        "Lin Shao"
      ],
      "abstract": "The tie-knotting task is highly challenging due to the tie's high deformation\nand long-horizon manipulation actions. This work presents TieBot, a\nReal-to-Sim-to-Real learning from visual demonstration system for the robots to\nlearn to knot a tie. We introduce the Hierarchical Feature Matching approach to\nestimate a sequence of tie's meshes from the demonstration video. With these\nestimated meshes used as subgoals, we first learn a teacher policy using\nprivileged information. Then, we learn a student policy with point cloud\nobservation by imitating teacher policy. Lastly, our pipeline applies learned\npolicy to real-world execution. We demonstrate the effectiveness of TieBot in\nsimulation and the real world. In the real-world experiment, a dual-arm robot\nsuccessfully knots a tie, achieving 50% success rate among 10 trials. Videos\ncan be found https://tiebots.github.io/.",
      "tldr_zh": "该论文提出 TieBot 系统，通过 Real-to-Sim-to-Real 方法，让机器人从视觉演示中学习打领带，解决领带高变形和长序列操作的挑战。系统采用 Hierarchical Feature Matching 技术估计领带的网格序列作为子目标，先基于特权信息训练教师策略，再通过模仿学习生成学生策略以处理点云观察。实验结果显示，TieBot 在模拟和真实环境中有效，真实世界测试中双臂机器人成功打领带，成功率达 50%（10 次试验）。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by CoRL 2024 as Oral presentation, camera-ready version",
      "pdf_url": "http://arxiv.org/pdf/2407.03245v3",
      "published_date": "2024-07-03 16:16:41 UTC",
      "updated_date": "2024-10-19 14:56:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:12:53.183083"
    },
    {
      "arxiv_id": "2407.03227v2",
      "title": "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhili Shen",
        "Pavlos Vougiouklis",
        "Chenxin Diao",
        "Kaustubh Vyas",
        "Yuanyi Ji",
        "Jeff Z. Pan"
      ],
      "abstract": "We focus on Text-to-SQL semantic parsing from the perspective of\nretrieval-augmented generation. Motivated by challenges related to the size of\ncommercial database schemata and the deployability of business intelligence\nsolutions, we propose $\\text{ASTReS}$ that dynamically retrieves input database\ninformation and uses abstract syntax trees to select few-shot examples for\nin-context learning.\n  Furthermore, we investigate the extent to which an in-parallel semantic\nparser can be leveraged for generating approximated versions of the expected\nSQL queries, to support our retrieval. We take this approach to the extreme--we\nadapt a model consisting of less than $500$M parameters, to act as an extremely\nefficient approximator, enhancing it with the ability to process schemata in a\nparallelised manner. We apply $\\text{ASTReS}$ to monolingual and cross-lingual\nbenchmarks for semantic parsing, showing improvements over state-of-the-art\nbaselines. Comprehensive experiments highlight the contribution of modules\ninvolved in this retrieval-augmented generation setting, revealing interesting\ndirections for future work.",
      "tldr_zh": "本研究针对Text-to-SQL语义解析的retrieval-augmented generation问题，提出ASTReS框架，通过动态检索数据库信息和基于abstract syntax trees (AST)的排名与schema pruning来选择few-shot examples，支持in-context learning。框架还利用一个小于500M参数的并行语义解析器生成近似SQL查询，以增强检索效率。实验结果显示，ASTReS在单语和跨语语义解析基准上优于最先进基线，并通过全面分析突出了各模块的贡献，为未来工作提供了新方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Main",
      "pdf_url": "http://arxiv.org/pdf/2407.03227v2",
      "published_date": "2024-07-03 15:55:14 UTC",
      "updated_date": "2024-11-04 12:14:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:13:05.850058"
    },
    {
      "arxiv_id": "2407.03224v1",
      "title": "PPO-based Dynamic Control of Uncertain Floating Platforms in the Zero-G Environment",
      "title_zh": "翻译失败",
      "authors": [
        "Mahya Ramezani",
        "M. Amin Alandihallaj",
        "Andreas M. Hein"
      ],
      "abstract": "In the field of space exploration, floating platforms play a crucial role in\nscientific investigations and technological advancements. However, controlling\nthese platforms in zero-gravity environments presents unique challenges,\nincluding uncertainties and disturbances. This paper introduces an innovative\napproach that combines Proximal Policy Optimization (PPO) with Model Predictive\nControl (MPC) in the zero-gravity laboratory (Zero-G Lab) at the University of\nLuxembourg. This approach leverages PPO's reinforcement learning power and\nMPC's precision to navigate the complex control dynamics of floating platforms.\nUnlike traditional control methods, this PPO-MPC approach learns from MPC\npredictions, adapting to unmodeled dynamics and disturbances, resulting in a\nresilient control framework tailored to the zero-gravity environment.\nSimulations and experiments in the Zero-G Lab validate this approach,\nshowcasing the adaptability of the PPO agent. This research opens new\npossibilities for controlling floating platforms in zero-gravity settings,\npromising advancements in space exploration.",
      "tldr_zh": "这篇论文提出了一种创新方法，将 Proximal Policy Optimization (PPO) 与 Model Predictive Control (MPC) 相结合，用于在零重力环境中动态控制浮动平台，以应对不确定性和干扰。PPO 通过从 MPC 的预测中学习，增强了对未建模动态的适应性，构建了一个适用于零重力环境的弹性控制框架。在 University of Luxembourg 的 Zero-G Lab 中进行的模拟和实验验证了该方法的有效性，并为太空探索中的浮动平台控制开辟了新前景。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Pre-print version submitted to 2024 International Conference on\n  Robotics and Automation (ICRA)",
      "pdf_url": "http://arxiv.org/pdf/2407.03224v1",
      "published_date": "2024-07-03 15:51:06 UTC",
      "updated_date": "2024-07-03 15:51:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:13:17.292260"
    },
    {
      "arxiv_id": "2407.03216v1",
      "title": "Learning Disentangled Representation in Object-Centric Models for Visual Dynamics Prediction via Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Sanket Gandhi",
        "Atul",
        "Samanyu Mahajan",
        "Vishal Sharma",
        "Rushil Gupta",
        "Arnab Kumar Mondal",
        "Parag Singla"
      ],
      "abstract": "Recent work has shown that object-centric representations can greatly help\nimprove the accuracy of learning dynamics while also bringing interpretability.\nIn this work, we take this idea one step further, ask the following question:\n\"can learning disentangled representation further improve the accuracy of\nvisual dynamics prediction in object-centric models?\" While there has been some\nattempt to learn such disentangled representations for the case of static\nimages \\citep{nsb}, to the best of our knowledge, ours is the first work which\ntries to do this in a general setting for video, without making any specific\nassumptions about the kind of attributes that an object might have. The key\nbuilding block of our architecture is the notion of a {\\em block}, where\nseveral blocks together constitute an object. Each block is represented as a\nlinear combination of a given number of learnable concept vectors, which is\niteratively refined during the learning process. The blocks in our model are\ndiscovered in an unsupervised manner, by attending over object masks, in a\nstyle similar to discovery of slots \\citep{slot_attention}, for learning a\ndense object-centric representation. We employ self-attention via transformers\nover the discovered blocks to predict the next state resulting in discovery of\nvisual dynamics. We perform a series of experiments on several benchmark 2-D,\nand 3-D datasets demonstrating that our architecture (1) can discover\nsemantically meaningful blocks (2) help improve accuracy of dynamics prediction\ncompared to SOTA object-centric models (3) perform significantly better in OOD\nsetting where the specific attribute combinations are not seen earlier during\ntraining. Our experiments highlight the importance discovery of disentangled\nrepresentation for visual dynamics prediction.",
      "tldr_zh": "这篇论文探讨了在物体中心模型（Object-Centric Models）中学习分离表示（Disentangled Representation），以提升视觉动态预测（Visual Dynamics Prediction）的准确性，并使用Transformer进行实现。论文提出了一种新架构，其中对象被分解为多个“块”（blocks），每个块由可学习的概念向量线性组合而成，并通过无监督方式（如类似slots的注意力机制）发现这些块，然后利用Transformer的自注意力机制预测视频的下一状态。实验结果显示，该方法在多个2D和3D基准数据集上比现有最先进（SOTA）模型提高了预测准确性，并在OOD（Out-of-Distribution）场景中表现出显著优势，突显了分离表示在动态预测中的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03216v1",
      "published_date": "2024-07-03 15:43:54 UTC",
      "updated_date": "2024-07-03 15:43:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:13:30.385156"
    },
    {
      "arxiv_id": "2407.13067v1",
      "title": "Large Language Model Agents for Improving Engagement with Behavior Change Interventions: Application to Digital Mindfulness",
      "title_zh": "翻译失败",
      "authors": [
        "Harsh Kumar",
        "Suhyeon Yoo",
        "Angela Zavaleta Bernuy",
        "Jiakai Shi",
        "Huayin Luo",
        "Joseph Williams",
        "Anastasia Kuzminykh",
        "Ashton Anderson",
        "Rachel Kornfield"
      ],
      "abstract": "Although engagement in self-directed wellness exercises typically declines\nover time, integrating social support such as coaching can sustain it. However,\ntraditional forms of support are often inaccessible due to the high costs and\ncomplex coordination. Large Language Models (LLMs) show promise in providing\nhuman-like dialogues that could emulate social support. Yet, in-depth, in situ\ninvestigations of LLMs to support behavior change remain underexplored. We\nconducted two randomized experiments to assess the impact of LLM agents on user\nengagement with mindfulness exercises. First, a single-session study, involved\n502 crowdworkers; second, a three-week study, included 54 participants. We\nexplored two types of LLM agents: one providing information and another\nfacilitating self-reflection. Both agents enhanced users' intentions to\npractice mindfulness. However, only the information-providing LLM, featuring a\nfriendly persona, significantly improved engagement with the exercises. Our\nfindings suggest that specific LLM agents may bridge the social support gap in\ndigital health interventions.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 代理如何提升行为改变干预的参与度，特别是应用于数字正念领域，以解决传统社会支持（如教练）成本高和协调复杂的问题。研究者进行了两个随机实验：一个单次会议涉及502名众包工作者，另一个为期三周的研究包括54名参与者，测试两种LLM代理（一种提供信息，另一种促进自我反思）。结果显示，两种代理均提高了用户练习正念的意图，但仅提供信息的友好型LLM代理显著改善了实际参与度。该发现表明，特定LLM代理可有效桥接数字健康干预中的社会支持缺口。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2407.13067v1",
      "published_date": "2024-07-03 15:43:16 UTC",
      "updated_date": "2024-07-03 15:43:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:13:40.873927"
    },
    {
      "arxiv_id": "2407.03210v2",
      "title": "Combining AI Control Systems and Human Decision Support via Robustness and Criticality",
      "title_zh": "翻译失败",
      "authors": [
        "Walt Woods",
        "Alexander Grushin",
        "Simon Khan",
        "Alvaro Velasquez"
      ],
      "abstract": "AI-enabled capabilities are reaching the requisite level of maturity to be\ndeployed in the real world, yet do not always make correct or safe decisions.\nOne way of addressing these concerns is to leverage AI control systems\nalongside and in support of human decisions, relying on the AI control system\nin safe situations while calling on a human co-decider for critical situations.\nWe extend a methodology for adversarial explanations (AE) to state-of-the-art\nreinforcement learning frameworks, including MuZero. Multiple improvements to\nthe base agent architecture are proposed. We demonstrate how this technology\nhas two applications: for intelligent decision tools and to enhance training /\nlearning frameworks. In a decision support context, adversarial explanations\nhelp a user make the correct decision by highlighting those contextual factors\nthat would need to change for a different AI-recommended decision. As another\nbenefit of adversarial explanations, we show that the learned AI control system\ndemonstrates robustness against adversarial tampering. Additionally, we\nsupplement AE by introducing strategically similar autoencoders (SSAs) to help\nusers identify and understand all salient factors being considered by the AI\nsystem. In a training / learning framework, this technology can improve both\nthe AI's decisions and explanations through human interaction. Finally, to\nidentify when AI decisions would most benefit from human oversight, we tie this\ncombined system to our prior art on statistically verified analyses of the\ncriticality of decisions at any point in time.",
      "tldr_zh": "该论文探讨了如何将AI控制系统与人类决策相结合，通过提升鲁棒性和决策关键性来解决AI可能出现的不正确或不安全问题的挑战。研究扩展了Adversarial Explanations (AE)技术，应用于先进的强化学习框架如MuZero，并提出多个代理架构改进，以帮助用户识别影响AI决策的上下文因素，从而支持更准确的决策。论文引入Strategically Similar Autoencoders (SSAs)来辅助用户理解AI考虑的显著因素，同时增强AI系统对对抗性篡改的鲁棒性。在训练框架中，该方法通过人类交互优化AI的决策和解释，并结合统计验证分析来识别需要人类监督的关键决策点。总的来说，这为构建可靠的AI决策支持系统提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "68T07",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.03210v2",
      "published_date": "2024-07-03 15:38:57 UTC",
      "updated_date": "2024-10-09 02:16:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:13:54.135090"
    },
    {
      "arxiv_id": "2407.03203v2",
      "title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts",
      "title_zh": "TheoremLlama：将通用目的 LLMs 转化为 Lean4 专家",
      "authors": [
        "Ruida Wang",
        "Jipeng Zhang",
        "Yizhen Jia",
        "Rui Pan",
        "Shizhe Diao",
        "Renjie Pi",
        "Tong Zhang"
      ],
      "abstract": "Proving mathematical theorems using computer-verifiable formal languages like\nLean significantly impacts mathematical reasoning. One approach to formal\ntheorem proving involves generating complete proofs using Large Language Models\n(LLMs) based on Natural Language (NL) proofs. However, due to the scarcity of\naligned NL and Formal Language (FL) theorem-proving data most modern LLMs\nexhibit suboptimal performance.This scarcity results in a paucity of\nmethodologies for training LLMs and techniques to fully utilize their\ncapabilities in composing formal proofs. To address these challenges, this\npaper proposes TheoremLlama, an end-to-end framework that trains a\ngeneral-purpose LLM to be a Lean4 expert. TheoremLlama includes NL-FL dataset\ngeneration and bootstrapping method to obtain aligned dataset, curriculum\nlearning and block training techniques to train the model, and iterative proof\nwriting method to write Lean4 proofs that work together synergistically. Using\nthe dataset generation method in TheoremLlama, we provide Open Bootstrapped\nTheorems (OBT), an NL-FL aligned and bootstrapped dataset. Our novel NL-FL\nbootstrapping method, where NL proofs are integrated into Lean4 code for\ntraining datasets, leverages the NL reasoning ability of LLMs for formal\nreasoning. The TheoremLlama framework achieves cumulative accuracies of 36.48%\nand 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the\nGPT-4 baseline of 22.95% and 25.41%. Our code, model checkpoints, and the\ngenerated dataset is published in GitHub",
      "tldr_zh": "该研究提出TheoremLlama框架，将通用Large Language Models (LLMs)转化为Lean4专家，以解决正式定理证明中数据稀缺导致的性能问题。框架包括NL-FL数据集生成和引导方法（如Open Bootstrapped Theorems (OBT)数据集）、课程学习和块训练技术，以及迭代证明编写方法，这些组件协同利用LLMs的自然语言(NL)推理能力来提升正式语言(FL)证明。实验结果显示，TheoremLlama在MiniF2F-Valid和Test数据集上分别达到36.48%和33.61%的累积准确率，超过了GPT-4的基线水平。该框架的代码、模型和数据集已公开分享。",
      "categories": [
        "cs.FL",
        "cs.AI"
      ],
      "primary_category": "cs.FL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03203v2",
      "published_date": "2024-07-03 15:36:18 UTC",
      "updated_date": "2024-10-04 03:06:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:14:07.315796"
    },
    {
      "arxiv_id": "2407.03392v1",
      "title": "M5: A Whole Genome Bacterial Encoder at Single Nucleotide Resolution",
      "title_zh": "翻译失败",
      "authors": [
        "Agust Egilsson"
      ],
      "abstract": "A linear attention mechanism is described to extend the context length of an\nencoder only transformer, called M5 in this report, to a multi-million single\nnucleotide resolution foundation model pretrained on bacterial whole genomes.\nThe linear attention mechanism used approximates a full quadratic attention\nmechanism tightly and has a simple and lightweight implementation for the use\ncase when the key-query embedding dimensionality is low. The M5-small model is\nentirely trained and tested on one A100 GPU with 40gb of memory up to 196K\nnucleotides during training and 2M nucleotides during testing. We test the\nperformance of the M5-small model and record notable improvements in\nperformance as whole genome bacterial sequence lengths are increased as well as\ndemonstrating the stability of the full multi-head attention approximation used\nas sequence length is increased.",
      "tldr_zh": "本研究介绍了 M5，一种基于线性注意力机制的编码器-only Transformer 模型，能够处理细菌全基因组序列并达到单核苷酸分辨率。该机制紧凑地近似于完整的二次注意力机制，并提供简单轻量级的实现，特别适合关键-查询嵌入维度较低的场景。M5-small 模型在单个 A100 GPU 上训练和测试，训练序列长度达 196K 核苷酸，测试时扩展至 2M 核苷酸。实验结果显示，随着细菌序列长度的增加，模型性能显著提升，并证明了多头注意力近似的稳定性。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "I.2"
      ],
      "primary_category": "q-bio.QM",
      "comment": "13 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.03392v1",
      "published_date": "2024-07-03 15:30:44 UTC",
      "updated_date": "2024-07-03 15:30:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:14:19.316703"
    },
    {
      "arxiv_id": "2407.03188v2",
      "title": "MuDiT & MuSiT: Alignment with Colloquial Expression in Description-to-Song Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Wang",
        "Haoxuan Liu",
        "Jiaxing Yu",
        "Tao Zhang",
        "Yan Liu",
        "Kejun Zhang"
      ],
      "abstract": "Amid the rising intersection of generative AI and human artistic processes,\nthis study probes the critical yet less-explored terrain of alignment in\nhuman-centric automatic song composition. We propose a novel task of Colloquial\nDescription-to-Song Generation, which focuses on aligning the generated content\nwith colloquial human expressions. This task is aimed at bridging the gap\nbetween colloquial language understanding and auditory expression within an AI\nmodel, with the ultimate goal of creating songs that accurately satisfy human\nauditory expectations and structurally align with musical norms. Current\ndatasets are limited due to their narrow descriptive scope, semantic gaps and\ninaccuracies. To overcome data scarcity in this domain, we present the Caichong\nMusic Dataset (CaiMD). CaiMD is manually annotated by both professional\nmusicians and amateurs, offering diverse perspectives and a comprehensive\nunderstanding of colloquial descriptions. Unlike existing datasets pre-set with\nexpert annotations or auto-generated ones with inherent biases, CaiMD caters\nmore sufficiently to our purpose of aligning AI-generated music with widespread\nuser-desired results. Moreover, we propose an innovative single-stage framework\ncalled MuDiT/MuSiT for enabling effective human-machine alignment in song\ncreation. This framework not only achieves cross-modal comprehension between\ncolloquial language and auditory music perceptions but also ensures generated\nsongs align with user-desired results. MuDiT/MuSiT employs one DiT/SiT model\nfor end-to-end generation of musical components like melody, harmony, rhythm,\nvocals, and instrumentation. The approach ensures harmonious sonic cohesiveness\namongst all generated musical components, facilitating better resonance with\nhuman auditory expectations.",
      "tldr_zh": "这篇论文探讨了生成 AI 在歌曲创作中的人机对齐问题，提出了一种新的任务——Colloquial Description-to-Song Generation，以确保生成的歌曲内容与口语化表达相匹配。针对现有数据集的局限性（如描述范围狭窄和语义差距），作者构建了 CaiMD 数据集，由专业音乐家和业余爱好者手动标注，提供多样视角和更全面的口语理解。论文还引入了 MuDiT/MuSiT 框架，该框架使用 DiT/SiT 模型进行端到端生成，包括旋律、和声、节奏、声乐和乐器组件，从而实现跨模态理解并提升歌曲与人类听觉期望的和谐一致。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "68Txx(Primary)14F05, 91Fxx(Secondary)",
        "I.2.7; J.5"
      ],
      "primary_category": "cs.SD",
      "comment": "19 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.03188v2",
      "published_date": "2024-07-03 15:12:36 UTC",
      "updated_date": "2024-07-11 03:32:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:14:33.547675"
    },
    {
      "arxiv_id": "2407.03185v2",
      "title": "Multiple-Resolution Tokenization for Time Series Forecasting with an Application to Pricing",
      "title_zh": "多分辨率标记化用于时间序列预测，并应用于定价领域",
      "authors": [
        "Egon Peršak",
        "Miguel F. Anjos",
        "Sebastian Lautz",
        "Aleksandar Kolev"
      ],
      "abstract": "We propose a transformer architecture for time series forecasting with a\nfocus on time series tokenisation and apply it to a real-world prediction\nproblem from the pricing domain. Our architecture aims to learn effective\nrepresentations at many scales across all available data simultaneously. The\nmodel contains a number of novel modules: a differentiated form of time series\npatching which employs multiple resolutions, a multiple-resolution module for\ntime-varying known variables, a mixer-based module for capturing cross-series\ninformation, and a novel output head with favourable scaling to account for the\nincreased number of tokens. We present an application of this model to a real\nworld prediction problem faced by the markdown team at a very large retailer.\nOn the experiments conducted our model outperforms in-house models and the\nselected existing deep learning architectures.",
      "tldr_zh": "本研究提出了一种基于 Transformer 的时间序列预测架构，专注于多分辨率 tokenization，以同时学习所有可用数据的多种尺度表示。该架构引入了几个新模块，包括多分辨率时间序列 patching、处理时间变化已知变量的多分辨率模块、捕捉跨序列信息的 mixer-based 模块，以及一个具有良好缩放特性的输出头。该模型应用于零售业定价领域的真实预测问题，并在实验中表现出色，优于内部模型和现有深度学习架构。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03185v2",
      "published_date": "2024-07-03 15:07:16 UTC",
      "updated_date": "2025-04-21 08:58:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:14:44.388944"
    },
    {
      "arxiv_id": "2407.03183v1",
      "title": "A Formal Model for Artificial Intelligence Applications in Automation Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Marvin Schieseck",
        "Philip Topalis",
        "Lasse Reinpold",
        "Felix Gehlhoff",
        "Alexander Fay"
      ],
      "abstract": "The integration of Artificial Intelligence (AI) into automation systems has\nthe potential to enhance efficiency and to address currently unsolved existing\ntechnical challenges. However, the industry-wide adoption of AI is hindered by\nthe lack of standardized documentation for the complex compositions of\nautomation systems, AI software, production hardware, and their\ninterdependencies. This paper proposes a formal model using standards and\nontologies to provide clear and structured documentation of AI applications in\nautomation systems. The proposed information model for artificial intelligence\nin automation systems (AIAS) utilizes ontology design patterns to map and link\nvarious aspects of automation systems and AI software. Validated through a\npractical example, the model demonstrates its effectiveness in improving\ndocumentation practices and aiding the sustainable implementation of AI in\nindustrial settings.",
      "tldr_zh": "本论文针对人工智能（AI）在自动化系统中的应用，提出一个正式模型，以解决标准化文档缺失的问题，从而促进AI的行业采用。该模型名为AIAS，利用标准和本体论（ontologies）以及本体设计模式（ontology design patterns），来映射和链接自动化系统、AI软件及其相互依赖关系。通过实际例子验证，AIAS模型证明了其在提升文档实践和支持工业环境中AI可持续实施方面的有效性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03183v1",
      "published_date": "2024-07-03 15:05:32 UTC",
      "updated_date": "2024-07-03 15:05:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:14:56.387130"
    },
    {
      "arxiv_id": "2407.03180v1",
      "title": "A multi-objective combinatorial optimisation framework for large scale hierarchical population synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Imran Mahmood",
        "Nicholas Bishop",
        "Anisoara Calinescu",
        "Michael Wooldridge",
        "Ioannis Zachos"
      ],
      "abstract": "In agent-based simulations, synthetic populations of agents are commonly used\nto represent the structure, behaviour, and interactions of individuals.\nHowever, generating a synthetic population that accurately reflects real\npopulation statistics is a challenging task, particularly when performed at\nscale. In this paper, we propose a multi objective combinatorial optimisation\ntechnique for large scale population synthesis. We demonstrate the\neffectiveness of our approach by generating a synthetic population for selected\nregions and validating it on contingency tables from real population data. Our\napproach supports complex hierarchical structures between individuals and\nhouseholds, is scalable to large populations and achieves minimal contigency\ntable reconstruction error. Hence, it provides a useful tool for policymakers\nand researchers for simulating the dynamics of complex populations.",
      "tldr_zh": "这篇论文提出了一种多目标组合优化(multi-objective combinatorial optimisation)框架，用于大规模层次化人口合成，以准确模拟真实人口的结构、行为和互动。该框架支持复杂的个体和家庭层次结构，能够处理大型数据集并最小化重构错误。通过在选定区域生成并验证合成人口，实验结果显示其有效性，为政策制定者和研究人员提供了一个模拟复杂人口动态的有用工具。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03180v1",
      "published_date": "2024-07-03 15:01:12 UTC",
      "updated_date": "2024-07-03 15:01:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:15:07.967812"
    },
    {
      "arxiv_id": "2407.03179v2",
      "title": "Motion meets Attention: Video Motion Prompts",
      "title_zh": "运动遇见注意力：视频运动提示",
      "authors": [
        "Qixiang Chen",
        "Lei Wang",
        "Piotr Koniusz",
        "Tom Gedeon"
      ],
      "abstract": "Videos contain rich spatio-temporal information. Traditional methods for\nextracting motion, used in tasks such as action recognition, often rely on\nvisual contents rather than precise motion features. This phenomenon is\nreferred to as 'blind motion extraction' behavior, which proves inefficient in\ncapturing motions of interest due to a lack of motion-guided cues. Recently,\nattention mechanisms have enhanced many computer vision tasks by effectively\nhighlighting salient visual areas. Inspired by this, we propose a modified\nSigmoid function with learnable slope and shift parameters as an attention\nmechanism to modulate motion signals from frame differencing maps. This\napproach generates a sequence of attention maps that enhance the processing of\nmotion-related video content. To ensure temporal continuity and smoothness of\nthe attention maps, we apply pair-wise temporal attention variation\nregularization to remove unwanted motions (e.g., noise) while preserving\nimportant ones. We then perform Hadamard product between each pair of attention\nmaps and the original video frames to highlight the evolving motions of\ninterest over time. These highlighted motions, termed video motion prompts, are\nsubsequently used as inputs to the model instead of the original video frames.\nWe formalize this process as a motion prompt layer and incorporate the\nregularization term into the loss function to learn better motion prompts. This\nlayer serves as an adapter between the model and the video data, bridging the\ngap between traditional 'blind motion extraction' and the extraction of\nrelevant motions of interest. We show that our lightweight, plug-and-play\nmotion prompt layer seamlessly integrates into models like SlowFast, X3D, and\nTimeSformer, enhancing performance on benchmarks such as FineGym and MPII\nCooking 2.",
      "tldr_zh": "该论文指出，传统视频运动提取方法存在“盲目的运动提取”问题，导致无法有效捕捉感兴趣的运动特征。为解决此问题，研究提出了一种基于修改的 Sigmoid 函数作为注意力机制的框架，用于从帧差分图生成注意力地图序列，并通过 pair-wise temporal attention variation regularization 确保其时序连续性和平滑性。接着，将注意力地图与原始视频帧进行 Hadamard product，产生 video motion prompts 作为模型输入。该方法以 motion prompt layer 形式实现，可作为即插即用的适配器，提升 SlowFast、X3D 和 TimeSformer 等模型在 FineGym 和 MPII Cooking 2 等基准上的性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the 16th Asian Conference on Machine Learning (ACML 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.03179v2",
      "published_date": "2024-07-03 14:59:46 UTC",
      "updated_date": "2024-10-02 13:32:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:15:21.056035"
    },
    {
      "arxiv_id": "2407.03391v1",
      "title": "Soft Begging: Modular and Efficient Shielding of LLMs against Prompt Injection and Jailbreaking based on Prompt Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Simon Ostermann",
        "Kevin Baum",
        "Christoph Endres",
        "Julia Masloh",
        "Patrick Schramowski"
      ],
      "abstract": "Prompt injection (both direct and indirect) and jailbreaking are now\nrecognized as significant issues for large language models (LLMs), particularly\ndue to their potential for harm in application-integrated contexts. This\nextended abstract explores a novel approach to protecting LLMs from such\nattacks, termed \"soft begging.\" This method involves training soft prompts to\ncounteract the effects of corrupted prompts on the LLM's output. We provide an\noverview of prompt injections and jailbreaking, introduce the theoretical basis\nof the \"soft begging\" technique, and discuss an evaluation of its\neffectiveness.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)面临的提示注入(prompt injection，包括直接和间接)和越狱(jailbreaking)攻击，提出了一种模块化和高效的保护方法，名为Soft Begging。\n该方法基于提示调优(prompt tuning)，通过训练软提示来抵消被篡改提示对LLMs输出造成的影响，确保模型在应用集成环境中更安全。\n论文介绍了这些攻击的理论基础，并评估了Soft Begging的有效性，为提升LLMs的鲁棒性提供了新途径。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03391v1",
      "published_date": "2024-07-03 14:52:09 UTC",
      "updated_date": "2024-07-03 14:52:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:15:32.351836"
    },
    {
      "arxiv_id": "2407.03172v1",
      "title": "IMC 2024 Methods & Solutions Review",
      "title_zh": "翻译失败",
      "authors": [
        "Shyam Gupta",
        "Dhanisha Sharma",
        "Songling Huang"
      ],
      "abstract": "For the past three years, Kaggle has been hosting the Image Matching\nChallenge, which focuses on solving a 3D image reconstruction problem using a\ncollection of 2D images. Each year, this competition fosters the development of\ninnovative and effective methodologies by its participants. In this paper, we\nintroduce an advanced ensemble technique that we developed, achieving a score\nof 0.153449 on the private leaderboard and securing the 160th position out of\nover 1,000 participants. Additionally, we conduct a comprehensive review of\nexisting methods and techniques employed by top-performing teams in the\ncompetition. Our solution, alongside the insights gathered from other leading\napproaches, contributes to the ongoing advancement in the field of 3D image\nreconstruction. This research provides valuable knowledge for future\nparticipants and researchers aiming to excel in similar image matching and\nreconstruction challenges.",
      "tldr_zh": "这篇论文回顾了 Kaggle Image Matching Challenge 中的方法和解决方案，聚焦于使用 2D 图像进行 3D 图像重建问题。作者开发了一种高级集成技术（advanced ensemble technique），在私有排行榜上取得 0.153449 的得分，并排名 160 名，超越了 1000 多名参赛者。该研究还对顶级团队的创新方法进行了全面分析，为未来 3D image reconstruction 领域的参与者和研究者提供了宝贵见解和指导。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.CV",
      "comment": "8 Pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.03172v1",
      "published_date": "2024-07-03 14:47:18 UTC",
      "updated_date": "2024-07-03 14:47:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:15:47.440843"
    },
    {
      "arxiv_id": "2407.03157v2",
      "title": "Let the Code LLM Edit Itself When You Edit the Code",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenyu He",
        "Jun Zhang",
        "Shengjie Luo",
        "Jingjing Xu",
        "Zhi Zhang",
        "Di He"
      ],
      "abstract": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
      "tldr_zh": "本文研究了代码生成场景中，开发者实时编辑代码时，代码LLM（如大型语言模型）重新预测下一个 token 或行的效率问题。作者提出Positional Integrity Encoding (PIE)方法，通过移除KV cache中导致temporal confusion的rotary positional encoding矩阵并重新应用正确的矩阵，仅需单轮矩阵乘法，即可维持准确性。实验在RepoBench-C-8k数据集上使用DeepSeek-Coder模型（1.3B、6.7B和33B参数）验证了PIE在代码插入、删除和多处编辑任务中的效果，结果显示其比标准全重新计算方法减少计算开销超过85%，同时保持接近的模型性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025 Camera Ready",
      "pdf_url": "http://arxiv.org/pdf/2407.03157v2",
      "published_date": "2024-07-03 14:34:03 UTC",
      "updated_date": "2025-03-04 13:01:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:15:59.028651"
    },
    {
      "arxiv_id": "2407.03154v2",
      "title": "Reinforcement Learning for Sequence Design Leveraging Protein Language Models",
      "title_zh": "基于蛋白质语言模型的序列设计强化学习",
      "authors": [
        "Jithendaraa Subramanian",
        "Shivakanth Sujit",
        "Niloy Irtisam",
        "Umong Sain",
        "Riashat Islam",
        "Derek Nowrouzezahrai",
        "Samira Ebrahimi Kahou"
      ],
      "abstract": "Protein sequence design, determined by amino acid sequences, are essential to\nprotein engineering problems in drug discovery. Prior approaches have resorted\nto evolutionary strategies or Monte-Carlo methods for protein design, but often\nfail to exploit the structure of the combinatorial search space, to generalize\nto unseen sequences. In the context of discrete black box optimization over\nlarge search spaces, learning a mutation policy to generate novel sequences\nwith reinforcement learning is appealing. Recent advances in protein language\nmodels (PLMs) trained on large corpora of protein sequences offer a potential\nsolution to this problem by scoring proteins according to their biological\nplausibility (such as the TM-score). In this work, we propose to use PLMs as a\nreward function to generate new sequences. Yet the PLM can be computationally\nexpensive to query due to its large size. To this end, we propose an\nalternative paradigm where optimization can be performed on scores from a\nsmaller proxy model that is periodically finetuned, jointly while learning the\nmutation policy. We perform extensive experiments on various sequence lengths\nto benchmark RL-based approaches, and provide comprehensive evaluations along\nbiological plausibility and diversity of the protein. Our experimental results\ninclude favorable evaluations of the proposed sequences, along with high\ndiversity scores, demonstrating that RL is a strong candidate for biological\nsequence design. Finally, we provide a modular open source implementation can\nbe easily integrated in most RL training loops, with support for replacing the\nreward model with other PLMs, to spur further research in this domain. The code\nfor all experiments is provided in the supplementary material.",
      "tldr_zh": "本研究提出了一种利用 Reinforcement Learning 的蛋白质序列设计方法，旨在通过学习突变策略（mutation policy）生成新序列，并以 Protein Language Models (PLMs) 作为奖励函数评估生物学合理性（如 TM-score）。为应对 PLMs 计算开销大的问题，该方法采用一个较小的代理模型（proxy model）进行优化，并定期微调以联合训练突变策略。实验在各种序列长度上显示，该方法在生物学合理性和序列多样性方面表现出色，证明了 Reinforcement Learning 在生物序列设计中的潜力，并提供了模块化的开源实现以促进进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 7 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.03154v2",
      "published_date": "2024-07-03 14:31:36 UTC",
      "updated_date": "2024-11-16 17:48:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:16:12.322014"
    },
    {
      "arxiv_id": "2407.03135v1",
      "title": "GMM-ResNext: Combining Generative and Discriminative Models for Speaker Verification",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Yan",
        "Zhenchun Lei",
        "Changhong Liu",
        "Yong Zhou"
      ],
      "abstract": "With the development of deep learning, many different network architectures\nhave been explored in speaker verification. However, most network architectures\nrely on a single deep learning architecture, and hybrid networks combining\ndifferent architectures have been little studied in ASV tasks. In this paper,\nwe propose the GMM-ResNext model for speaker verification. Conventional GMM\ndoes not consider the score distribution of each frame feature over all\nGaussian components and ignores the relationship between neighboring speech\nframes. So, we extract the log Gaussian probability features based on the raw\nacoustic features and use ResNext-based network as the backbone to extract the\nspeaker embedding. GMM-ResNext combines Generative and Discriminative Models to\nimprove the generalization ability of deep learning models and allows one to\nmore easily specify meaningful priors on model parameters. A two-path\nGMM-ResNext model based on two gender-related GMMs has also been proposed. The\nExperimental results show that the proposed GMM-ResNext achieves relative\nimprovements of 48.1\\% and 11.3\\% in EER compared with ResNet34 and ECAPA-TDNN\non VoxCeleb1-O test set.",
      "tldr_zh": "该论文提出GMM-ResNext模型，用于说话人验证（Speaker Verification），通过结合生成模型（Generative Models）和判别模型（Discriminative Models）来提升深度学习模型的泛化能力和参数先验指定。方法包括从原始声学特征提取log Gaussian probability特征，并以ResNext-based网络作为骨干提取说话人嵌入，同时引入基于两个性别相关GMM的两路径模型，以解决传统GMM忽略帧特征分布和相邻帧关系的问题。实验结果显示，在VoxCeleb1-O测试集上，GMM-ResNext与ResNet34和ECAPA-TDNN相比，EER分别相对改善48.1%和11.3%。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03135v1",
      "published_date": "2024-07-03 14:14:18 UTC",
      "updated_date": "2024-07-03 14:14:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:16:23.570213"
    },
    {
      "arxiv_id": "2407.03132v1",
      "title": "Speaker- and Text-Independent Estimation of Articulatory Movements and Phoneme Alignments from Speech",
      "title_zh": "翻译失败",
      "authors": [
        "Tobias Weise",
        "Philipp Klumpp",
        "Kubilay Can Demir",
        "Paula Andrea Pérez-Toro",
        "Maria Schuster",
        "Elmar Noeth",
        "Bjoern Heismann",
        "Andreas Maier",
        "Seung Hee Yang"
      ],
      "abstract": "This paper introduces a novel combination of two tasks, previously treated\nseparately: acoustic-to-articulatory speech inversion (AAI) and\nphoneme-to-articulatory (PTA) motion estimation. We refer to this joint task as\nacoustic phoneme-to-articulatory speech inversion (APTAI) and explore two\ndifferent approaches, both working speaker- and text-independently during\ninference. We use a multi-task learning setup, with the end-to-end goal of\ntaking raw speech as input and estimating the corresponding articulatory\nmovements, phoneme sequence, and phoneme alignment. While both proposed\napproaches share these same requirements, they differ in their way of achieving\nphoneme-related predictions: one is based on frame classification, the other on\na two-staged training procedure and forced alignment. We reach competitive\nperformance of 0.73 mean correlation for the AAI task and achieve up to\napproximately 87% frame overlap compared to a state-of-the-art text-dependent\nphoneme force aligner.",
      "tldr_zh": "本文提出了一种新的联合任务APTAI（acoustic phoneme-to-articulatory speech inversion），将acoustic-to-articulatory speech inversion (AAI)和phoneme-to-articulatory (PTA) motion estimation整合起来，实现从原始语音中估计发音运动、音素序列和音素对齐，且独立于说话者和文本。采用多任务学习框架，探索两种方法：一种基于帧分类，另一种基于两阶段训练和forced alignment。实验结果显示，AAI任务的mean correlation达到0.73，与state-of-the-art text-dependent phoneme force aligner相比，帧重叠率高达约87%，展现出竞争性性能。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "to be published in Interspeech 2024 proceedings",
      "pdf_url": "http://arxiv.org/pdf/2407.03132v1",
      "published_date": "2024-07-03 14:13:04 UTC",
      "updated_date": "2024-07-03 14:13:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:16:36.350007"
    },
    {
      "arxiv_id": "2407.03131v4",
      "title": "MVGT: A Multi-view Graph Transformer Based on Spatial Relations for EEG Emotion Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Yanjie Cui",
        "Xiaohong Liu",
        "Jing Liang",
        "Yamin Fu"
      ],
      "abstract": "Electroencephalography (EEG), a technique that records electrical activity\nfrom the scalp using electrodes, plays a vital role in affective computing.\nHowever, fully utilizing the multi-domain characteristics of EEG signals\nremains a significant challenge. Traditional single-perspective analyses often\nfail to capture the complex interplay of temporal, frequency, and spatial\ndimensions in EEG data. To address this, we introduce a multi-view graph\ntransformer (MVGT) based on spatial relations that integrates information\nacross three domains: temporal dynamics from continuous series, frequency\nfeatures extracted from frequency bands, and inter-channel relationships\ncaptured through several spatial encodings. This comprehensive approach allows\nmodel to capture the nuanced properties inherent in EEG signals, enhancing its\nflexibility and representational power. Evaluation on publicly available\ndatasets demonstrates that MVGT surpasses state-of-the-art methods in\nperformance. The results highlight its ability to extract multi-domain\ninformation and effectively model inter-channel relationships, showcasing its\npotential for EEG-based emotion recognition tasks.",
      "tldr_zh": "本文提出 MVGT，一种基于空间关系的多视图图 Transformer，用于提升 EEG 情绪识别的性能。该框架整合了时间动态、频率特征以及通道间关系，通过多种空间编码全面捕获 EEG 信号的多域特性，从而增强模型的灵活性和表示能力。在公开数据集上的评估中，MVGT 超过了最先进方法，突出了其在提取多域信息和建模通道间关系方面的优势。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03131v4",
      "published_date": "2024-07-03 14:13:00 UTC",
      "updated_date": "2025-01-16 02:54:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:16:47.507448"
    },
    {
      "arxiv_id": "2407.03125v2",
      "title": "Foundations and Frontiers of Graph Learning Theory",
      "title_zh": "图学习理论的基础与前沿",
      "authors": [
        "Yu Huang",
        "Min Zhou",
        "Menglin Yang",
        "Zhen Wang",
        "Muhan Zhang",
        "Jie Wang",
        "Hong Xie",
        "Hao Wang",
        "Defu Lian",
        "Enhong Chen"
      ],
      "abstract": "Recent advancements in graph learning have revolutionized the way to\nunderstand and analyze data with complex structures. Notably, Graph Neural\nNetworks (GNNs), i.e. neural network architectures designed for learning graph\nrepresentations, have become a popular paradigm. With these models being\nusually characterized by intuition-driven design or highly intricate\ncomponents, placing them within the theoretical analysis framework to distill\nthe core concepts, helps understand the key principles that drive the\nfunctionality better and guide further development. Given this surge in\ninterest, this article provides a comprehensive summary of the theoretical\nfoundations and breakthroughs concerning the approximation and learning\nbehaviors intrinsic to prevalent graph learning models. Encompassing\ndiscussions on fundamental aspects such as expressiveness power,\ngeneralization, optimization, and unique phenomena such as over-smoothing and\nover-squashing, this piece delves into the theoretical foundations and frontier\ndriving the evolution of graph learning. In addition, this article also\npresents several challenges and further initiates discussions on possible\nsolutions.",
      "tldr_zh": "这篇论文总结了图学习理论(Graph Learning Theory)的理论基础和前沿进展，特别是针对Graph Neural Networks (GNNs)等模型。文章探讨了GNNs的核心概念，包括表达能力(expressiveness power)、泛化(generalization)、优化(optimization)，以及独特现象如over-smoothing和over-squashing，以揭示这些模型的内在行为和学习机制。该研究不仅分析了这些理论突破如何指导图学习的发展，还提出了当前面临的挑战并讨论了可能的解决方案，从而为未来研究提供宝贵指导。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "35pages,273references. Github link:\n  https://github.com/minehly/awesome-paper-for-graph-learning-theory",
      "pdf_url": "http://arxiv.org/pdf/2407.03125v2",
      "published_date": "2024-07-03 14:07:41 UTC",
      "updated_date": "2024-07-08 01:22:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:17:02.343602"
    },
    {
      "arxiv_id": "2407.03108v1",
      "title": "How Reliable and Stable are Explanations of XAI Methods?",
      "title_zh": "XAI 方法的解释如何可靠和稳定？",
      "authors": [
        "José Ribeiro",
        "Lucas Cardoso",
        "Vitor Santos",
        "Eduardo Carvalho",
        "Níkolas Carneiro",
        "Ronnie Alves"
      ],
      "abstract": "Black box models are increasingly being used in the daily lives of human\nbeings living in society. Along with this increase, there has been the\nemergence of Explainable Artificial Intelligence (XAI) methods aimed at\ngenerating additional explanations regarding how the model makes certain\npredictions. In this sense, methods such as Dalex, Eli5, eXirt, Lofo and Shap\nemerged as different proposals and methodologies for generating explanations of\nblack box models in an agnostic way. Along with the emergence of these methods,\nquestions arise such as \"How Reliable and Stable are XAI Methods?\". With the\naim of shedding light on this main question, this research creates a pipeline\nthat performs experiments using the diabetes dataset and four different machine\nlearning models (LGBM, MLP, DT and KNN), creating different levels of\nperturbations of the test data and finally generates explanations from the\neXirt method regarding the confidence of the models and also feature relevances\nranks from all XAI methods mentioned, in order to measure their stability in\nthe face of perturbations. As a result, it was found that eXirt was able to\nidentify the most reliable models among all those used. It was also found that\ncurrent XAI methods are sensitive to perturbations, with the exception of one\nspecific method.",
      "tldr_zh": "这篇论文探讨了 Explainable Artificial Intelligence (XAI) 方法的可靠性和稳定性，包括 Dalex、Eli5、eXirt、Lofo 和 Shap 等工具。研究团队设计了一个实验管道，使用 diabetes 数据集和四种机器学习模型 (LGBM, MLP, DT 和 KNN)，通过对测试数据施加不同水平的扰动来评估这些方法的表现。结果表明，eXirt 能够有效识别最可靠的模型，而其他 XAI 方法对扰动较为敏感，仅有一个特定方法表现出较强稳定性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 6 figures, submitted to BRACIS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.03108v1",
      "published_date": "2024-07-03 13:47:41 UTC",
      "updated_date": "2024-07-03 13:47:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:17:11.943531"
    },
    {
      "arxiv_id": "2407.17489v1",
      "title": "Collective Attention in Human-AI Teams",
      "title_zh": "人类-AI 团队中的集体注意力",
      "authors": [
        "Josie Zvelebilova",
        "Saiph Savage",
        "Christoph Riedl"
      ],
      "abstract": "How does the presence of an AI assistant affect the collective attention of a\nteam? We study 20 human teams of 3-4 individuals paired with one voice-only AI\nassistant during a challenging puzzle task. Teams are randomly assigned to an\nAI assistant with a human- or robotic-sounding voice that provides either\nhelpful or misleading information about the task. Treating each individual AI\ninterjection as a treatment intervention, we identify the causal effects of the\nAI on dynamic group processes involving language use. Our findings demonstrate\nthat the AI significantly affects what teams discuss, how they discuss it, and\nthe alignment of their mental models. Teams adopt AI-introduced language for\nboth terms directly related to the task and for peripheral terms, even when\nthey (a) recognize the unhelpful nature of the AI, (b) do not consider the AI a\ngenuine team member, and (c) do not trust the AI. The process of language\nadaptation appears to be automatic, despite doubts about the AI's competence.\nThe presence of an AI assistant significantly impacts team collective attention\nby modulating various aspects of shared cognition. This study contributes to\nhuman-AI teaming research by highlighting collective attention as a central\nmechanism through which AI systems in team settings influence team performance.\nUnderstanding this mechanism will help CSCW researchers design AI systems that\nenhance team collective intelligence by optimizing collective attention.",
      "tldr_zh": "本文研究了 AI 助手对人类团队集体 attention 的影响，通过实验分析 20 个 3-4 人团队在谜题任务中与语音 AI 的互动，AI 提供帮助性或误导性信息。结果显示，AI 显著改变团队讨论内容、方式和心理模型对齐，即使团队不信任 AI 或视其为非团队成员，这种语言适应过程仍是自动发生的。该研究强调集体 attention 是 AI 在 human-AI teams 中影响绩效的核心机制，并为设计优化团队集体智能的 AI 系统提供重要指导。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17489v1",
      "published_date": "2024-07-03 13:46:00 UTC",
      "updated_date": "2024-07-03 13:46:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:17:24.527026"
    },
    {
      "arxiv_id": "2407.03094v2",
      "title": "Conformal Prediction for Causal Effects of Continuous Treatments",
      "title_zh": "翻译失败",
      "authors": [
        "Maresa Schröder",
        "Dennis Frauen",
        "Jonas Schweisthal",
        "Konstantin Heß",
        "Valentyn Melnychuk",
        "Stefan Feuerriegel"
      ],
      "abstract": "Uncertainty quantification of causal effects is crucial for safety-critical\napplications such as personalized medicine. A powerful approach for this is\nconformal prediction, which has several practical benefits due to\nmodel-agnostic finite-sample guarantees. Yet, existing methods for conformal\nprediction of causal effects are limited to binary/discrete treatments and make\nhighly restrictive assumptions such as known propensity scores. In this work,\nwe provide a novel conformal prediction method for potential outcomes of\ncontinuous treatments. We account for the additional uncertainty introduced\nthrough propensity estimation so that our conformal prediction intervals are\nvalid even if the propensity score is unknown. Our contributions are\nthree-fold: (1) We derive finite-sample prediction intervals for potential\noutcomes of continuous treatments. (2) We provide an algorithm for calculating\nthe derived intervals. (3) We demonstrate the effectiveness of the conformal\nprediction intervals in experiments on synthetic and real-world datasets. To\nthe best of our knowledge, we are the first to propose conformal prediction for\ncontinuous treatments when the propensity score is unknown and must be\nestimated from data.",
      "tldr_zh": "该研究针对连续治疗（continuous treatments）的因果效应不确定性量化，提出了一种新型 Conformal Prediction 方法，以解决现有方法仅限于二元/离散治疗并假设已知倾向分数（propensity scores）的局限性。该方法考虑倾向分数估计引入的额外不确定性，推导出连续治疗潜在结果（potential outcomes）的有限样本预测区间，并提供相应的计算算法。实验结果在合成和真实数据集上验证了该方法的有效性，这是首个在倾向分数未知且需从数据估计的情况下，为连续治疗提供 Conformal Prediction 的框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03094v2",
      "published_date": "2024-07-03 13:34:33 UTC",
      "updated_date": "2024-10-23 10:09:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:17:39.665823"
    },
    {
      "arxiv_id": "2407.03093v1",
      "title": "Revisiting the Performance of Deep Learning-Based Vulnerability Detection on Realistic Datasets",
      "title_zh": "重新审视基于深度学习的漏洞检测在真实数据集上的性能",
      "authors": [
        "Partha Chakraborty",
        "Krishna Kanth Arumugam",
        "Mahmoud Alfadel",
        "Meiyappan Nagappan",
        "Shane McIntosh"
      ],
      "abstract": "The impact of software vulnerabilities on everyday software systems is\nsignificant. Despite deep learning models being proposed for vulnerability\ndetection, their reliability is questionable. Prior evaluations show high\nrecall/F1 scores of up to 99%, but these models underperform in practical\nscenarios, particularly when assessed on entire codebases rather than just the\nfixing commit. This paper introduces Real-Vul, a comprehensive dataset\nrepresenting real-world scenarios for evaluating vulnerability detection\nmodels. Evaluating DeepWukong, LineVul, ReVeal, and IVDetect shows a\nsignificant drop in performance, with precision decreasing by up to 95\npercentage points and F1 scores by up to 91 points. Furthermore, Model\nperformance fluctuates based on vulnerability characteristics, with better F1\nscores for information leaks or code injection than for path resolution or\npredictable return values. The results highlight a significant performance gap\nthat needs addressing before deploying deep learning-based vulnerability\ndetection in practical settings. Overfitting is identified as a key issue, and\nan augmentation technique is proposed, potentially improving performance by up\nto 30%. Contributions include a dataset creation approach for better model\nevaluation, Real-Vul dataset, and empirical evidence of deep learning models\nstruggling in real-world settings.",
      "tldr_zh": "本研究重新审视了基于 Deep Learning 的漏洞检测模型在真实数据集上的性能，发现这些模型尽管在先前评估中显示高达99%的 recall/F1 scores，但在实际场景中表现不佳，尤其是在评估整个代码库时。论文引入了 Real-Vul，这是一个代表真实世界场景的全面数据集，并对 DeepWukong、LineVul、ReVeal 和 IVDetect 等模型进行评估，结果显示 precision 下降多达95个百分点，F1 scores 下降多达91个百分点。性能还因漏洞特性而异，例如信息泄露或代码注入的 F1 scores 更高，而路径解析或可预测返回值则更低。研究识别过拟合作为关键问题，并提出了一种增强技术，可能将性能提高多达30%，同时贡献了数据集创建方法和实证证据，以推动漏洞检测模型在实际应用中的改进。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "D.2; I.2"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03093v1",
      "published_date": "2024-07-03 13:34:30 UTC",
      "updated_date": "2024-07-03 13:34:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:17:50.972951"
    },
    {
      "arxiv_id": "2407.03086v2",
      "title": "Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yujin Shin",
        "Kichang Lee",
        "Sungmin Lee",
        "You Rim Choi",
        "Hyung-Sin Kim",
        "JeongGil Ko"
      ],
      "abstract": "While federated learning leverages distributed client resources, it faces\nchallenges due to heterogeneous client capabilities. This necessitates\nallocating models suited to clients' resources and careful parameter\naggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel\nfederated learning framework for supporting client heterogeneity by combining a\nmulti-exit network architecture with hypernetwork-based model weight\ngeneration. This approach aligns the feature spaces of heterogeneous model\nlayers and resolves per-layer information disparity during weight aggregation.\nTo practically realize HypeMeFed, we also propose a low-rank factorization\napproach to minimize computation and memory overhead associated with\nhypernetworks. Our evaluations on a real-world heterogeneous device testbed\nindicate that \\system enhances accuracy by 5.12% over FedAvg, reduces the\nhypernetwork memory requirements by 98.22%, and accelerates its operations by\n1.86x compared to a naive hypernetwork approach. These results demonstrate\nHypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for\nfederated learning.",
      "tldr_zh": "这篇论文针对联邦学习（Federated Learning）中客户端能力异质性的挑战，提出HypeMeFed框架，通过结合多出口网络架构（multi-exit network architecture）和基于超网络（hypernetwork）的权重生成方法，来对齐异质模型层的特征空间并解决参数聚合中的信息差异。框架还采用低秩因子分解（low-rank factorization）来最小化超网络的计算和内存开销。实验在真实异质设备测试床上显示，HypeMeFed相较于FedAvg提高了5.12%的准确率，减少了98.22%的超网络内存需求，并将操作速度加速1.86倍。这些结果证明了HypeMeFed在利用异质客户端进行高效联邦学习方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03086v2",
      "published_date": "2024-07-03 13:15:12 UTC",
      "updated_date": "2024-10-03 12:45:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:18:02.153620"
    },
    {
      "arxiv_id": "2407.12831v2",
      "title": "Truth is Universal: Robust Detection of Lies in LLMs",
      "title_zh": "真理是普遍的：LLMs 中鲁",
      "authors": [
        "Lennart Bürger",
        "Fred A. Hamprecht",
        "Boaz Nadler"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionised natural language processing,\nexhibiting impressive human-like capabilities. In particular, LLMs are capable\nof \"lying\", knowingly outputting false statements. Hence, it is of interest and\nimportance to develop methods to detect when LLMs lie. Indeed, several authors\ntrained classifiers to detect LLM lies based on their internal model\nactivations. However, other researchers showed that these classifiers may fail\nto generalise, for example to negated statements. In this work, we aim to\ndevelop a robust method to detect when an LLM is lying. To this end, we make\nthe following key contributions: (i) We demonstrate the existence of a\ntwo-dimensional subspace, along which the activation vectors of true and false\nstatements can be separated. Notably, this finding is universal and holds for\nvarious LLMs, including Gemma-7B, LLaMA2-13B, Mistral-7B and LLaMA3-8B. Our\nanalysis explains the generalisation failures observed in previous studies and\nsets the stage for more robust lie detection; (ii) Building upon (i), we\nconstruct an accurate LLM lie detector. Empirically, our proposed classifier\nachieves state-of-the-art performance, attaining 94% accuracy in both\ndistinguishing true from false factual statements and detecting lies generated\nin real-world scenarios.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）可能输出虚假语句（lying）的问题，提出了一种鲁棒的检测方法，以解决现有分类器在泛化方面的不足，如对否定语句的失败。关键发现是存在一个二维子空间（two-dimensional subspace），其中真语句和假语句的激活向量可以被分离，这一现象在Gemma-7B、LLaMA2-13B、Mistral-7B和LLaMA3-8B等多种LLMs中普遍适用。基于此，研究构建了一个高性能的LLM谎言检测器（lie detector），在区分真假事实语句和检测真实场景中的谎言时，实现了94%的准确率，从而为更可靠的LLMs应用奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2024 poster",
      "pdf_url": "http://arxiv.org/pdf/2407.12831v2",
      "published_date": "2024-07-03 13:01:54 UTC",
      "updated_date": "2024-10-21 08:55:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:18:16.800937"
    },
    {
      "arxiv_id": "2407.03080v1",
      "title": "Artificial Inductive Bias for Synthetic Tabular Data Generation in Data-Scarce Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Patricia A. Apellániz",
        "Ana Jiménez",
        "Borja Arroyo Galende",
        "Juan Parras",
        "Santiago Zazo"
      ],
      "abstract": "While synthetic tabular data generation using Deep Generative Models (DGMs)\noffers a compelling solution to data scarcity and privacy concerns, their\neffectiveness relies on substantial training data, often unavailable in\nreal-world applications. This paper addresses this challenge by proposing a\nnovel methodology for generating realistic and reliable synthetic tabular data\nwith DGMs in limited real-data environments. Our approach proposes several ways\nto generate an artificial inductive bias in a DGM through transfer learning and\nmeta-learning techniques. We explore and compare four different methods within\nthis framework, demonstrating that transfer learning strategies like\npre-training and model averaging outperform meta-learning approaches, like\nModel-Agnostic Meta-Learning, and Domain Randomized Search. We validate our\napproach using two state-of-the-art DGMs, namely, a Variational Autoencoder and\na Generative Adversarial Network, to show that our artificial inductive bias\nfuels superior synthetic data quality, as measured by Jensen-Shannon\ndivergence, achieving relative gains of up to 50\\% when using our proposed\napproach. This methodology has broad applicability in various DGMs and machine\nlearning tasks, particularly in areas like healthcare and finance, where data\nscarcity is often a critical issue.",
      "tldr_zh": "本论文提出了一种生成人工归纳偏差（artificial inductive bias）的方法，用于在数据稀缺场景下利用 Deep Generative Models (DGMs) 生成更真实可靠的合成表格数据。该方法通过转移学习（transfer learning）和元学习（meta-learning）技术探索四种策略，包括预训练、模型平均、Model-Agnostic Meta-Learning 和 Domain Randomized Search，结果显示转移学习方法优于元学习方法。实验使用 Variational Autoencoder 和 Generative Adversarial Network 进行验证，基于 Jensen-Shannon divergence 指标，实现了高达 50% 的合成数据质量相对增益。该方法适用于各种 DGMs 和机器学习任务，尤其在医疗和金融等领域应对数据稀缺问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.0"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 6 Figures",
      "pdf_url": "http://arxiv.org/pdf/2407.03080v1",
      "published_date": "2024-07-03 12:53:42 UTC",
      "updated_date": "2024-07-03 12:53:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:18:28.633040"
    },
    {
      "arxiv_id": "2407.03070v1",
      "title": "Federated Learning for Zero-Day Attack Detection in 5G and Beyond V2X Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Abdelaziz Amara korba",
        "Abdelwahab Boualouache",
        "Bouziane Brik",
        "Rabah Rahal",
        "Yacine Ghamri-Doudane",
        "Sidi Mohammed Senouci"
      ],
      "abstract": "Deploying Connected and Automated Vehicles (CAVs) on top of 5G and Beyond\nnetworks (5GB) makes them vulnerable to increasing vectors of security and\nprivacy attacks. In this context, a wide range of advanced machine/deep\nlearning based solutions have been designed to accurately detect security\nattacks. Specifically, supervised learning techniques have been widely applied\nto train attack detection models. However, the main limitation of such\nsolutions is their inability to detect attacks different from those seen during\nthe training phase, or new attacks, also called zero-day attacks. Moreover,\ntraining the detection model requires significant data collection and labeling,\nwhich increases the communication overhead, and raises privacy concerns. To\naddress the aforementioned limits, we propose in this paper a novel detection\nmechanism that leverages the ability of the deep auto-encoder method to detect\nattacks relying only on the benign network traffic pattern. Using federated\nlearning, the proposed intrusion detection system can be trained with large and\ndiverse benign network traffic, while preserving the CAVs privacy, and\nminimizing the communication overhead. The in-depth experiment on a recent\nnetwork traffic dataset shows that the proposed system achieved a high\ndetection rate while minimizing the false positive rate, and the detection\ndelay.",
      "tldr_zh": "这篇论文针对5G and Beyond V2X Networks中的Connected and Automated Vehicles (CAVs)面临的zero-day attacks问题，提出了一种新型入侵检测机制。该机制利用Deep Auto-Encoder方法，仅基于良性网络流量模式进行攻击检测，并结合Federated Learning实现分布式训练，从而保护CAVs隐私并最小化通信开销。实验结果显示，该系统在最近的网络流量数据集上实现了高检测率、低假阳性率和低检测延迟，为高效的网络安全防护提供了新途径。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03070v1",
      "published_date": "2024-07-03 12:42:31 UTC",
      "updated_date": "2024-07-03 12:42:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:18:42.622806"
    },
    {
      "arxiv_id": "2407.03068v1",
      "title": "xApp Distillation: AI-based Conflict Mitigation in B5G O-RAN",
      "title_zh": "翻译失败",
      "authors": [
        "Hakan Erdol",
        "Xiaoyang Wang",
        "Robert Piechocki",
        "George Oikonomou",
        "Arjun Parekh"
      ],
      "abstract": "The advancements of machine learning-based (ML) decision-making algorithms\ncreated various research and industrial opportunities. One of these areas is\nML-based near-real-time network management applications (xApps) in Open-Radio\nAccess Network (O-RAN). Normally, xApps are designed solely for the desired\nobjectives, and fine-tuned for deployment. However, telecommunication companies\ncan employ multiple xApps and deploy them in overlapping areas. Consider the\ndifferent design objectives of xApps, the deployment might cause conflicts. To\nprevent such conflicts, we proposed the xApp distillation method that distills\nknowledge from multiple xApps, then uses this knowledge to train a single model\nthat has retained the capabilities of Previous xApps. Performance evaluations\nshow that compared conflict mitigation schemes can cause up to six times more\nnetwork outages than xApp distillation in some cases.",
      "tldr_zh": "这篇论文针对B5G O-RAN环境中多个xApp部署可能导致的冲突问题，提出了一种基于AI的xApp distillation方法。该方法通过从多个xApp中提炼知识，并使用这些知识训练一个单一模型，从而保留原有xApp的能力，同时缓解冲突。实验结果显示，与其他冲突缓解方案相比，xApp distillation在某些场景下能减少多达六倍的网络中断，为ML-based网络管理提供了更可靠的解决方案。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "5 Pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.03068v1",
      "published_date": "2024-07-03 12:40:20 UTC",
      "updated_date": "2024-07-03 12:40:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:18:50.657871"
    },
    {
      "arxiv_id": "2407.03059v2",
      "title": "FairJob: A Real-World Dataset for Fairness in Online Systems",
      "title_zh": "FairJob：在线系统公平性的真实世界数据集",
      "authors": [
        "Mariia Vladimirova",
        "Federico Pavone",
        "Eustache Diemert"
      ],
      "abstract": "We introduce a fairness-aware dataset for job recommendations in advertising,\ndesigned to foster research in algorithmic fairness within real-world\nscenarios. It was collected and prepared to comply with privacy standards and\nbusiness confidentiality. An additional challenge is the lack of access to\nprotected user attributes such as gender, for which we propose a solution to\nobtain a proxy estimate. Despite being anonymized and including a proxy for a\nsensitive attribute, our dataset preserves predictive power and maintains a\nrealistic and challenging benchmark. This dataset addresses a significant gap\nin the availability of fairness-focused resources for high-impact domains like\nadvertising -- the actual impact being having access or not to precious\nemployment opportunities, where balancing fairness and utility is a common\nindustrial challenge. We also explore various stages in the advertising process\nwhere unfairness can occur and introduce a method to compute a fair utility\nmetric for the job recommendations in online systems case from a biased\ndataset. Experimental evaluations of bias mitigation techniques on the released\ndataset demonstrate potential improvements in fairness and the associated\ntrade-offs with utility.\n  The dataset is hosted at https://huggingface.co/datasets/criteo/FairJob.\nSource code for the experiments is hosted at\nhttps://github.com/criteo-research/FairJob-dataset/.",
      "tldr_zh": "本研究引入了 FairJob 数据集，这是一个真实世界数据集，旨在促进在线系统（如工作推荐广告）中算法公平性的研究。数据集通过匿名化和代理估计（如性别敏感属性的代理）来处理隐私标准和商业机密挑战，同时保留预测能力和真实性。研究者探讨了广告过程中的不公平阶段，并提出了一种计算公平效用指标的方法；实验评估显示，偏见缓解技术可以显著改善公平性，但会与效用产生权衡。该数据集填补了高影响领域（如广告）公平资源缺口，并提供公开访问（托管于 Hugging Face 和 GitHub）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024, 28 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.03059v2",
      "published_date": "2024-07-03 12:30:39 UTC",
      "updated_date": "2024-11-01 06:45:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:19:05.903190"
    },
    {
      "arxiv_id": "2407.03056v2",
      "title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Marco Mistretta",
        "Alberto Baldrati",
        "Marco Bertini",
        "Andrew D. Bagdanov"
      ],
      "abstract": "Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization\nto unseen tasks, but fall short of the performance of supervised methods in\ngeneralizing to downstream tasks with limited data. Prompt learning is emerging\nas a parameter-efficient method for adapting VLMs, but state-of-the-art\napproaches require annotated samples. In this paper we propose a novel approach\nto prompt learning based on unsupervised knowledge distillation from more\npowerful models. Our approach, which we call Knowledge Distillation Prompt\nLearning (KDPL), can be integrated into existing prompt learning techniques and\neliminates the need for labeled examples during adaptation. Our experiments on\nmore than ten standard benchmark datasets demonstrate that KDPL is very\neffective at improving generalization of learned prompts for zero-shot domain\ngeneralization, zero-shot cross-dataset generalization, and zero-shot\nbase-to-novel class generalization problems. KDPL requires no ground-truth\nlabels for adaptation, and moreover we show that even in the absence of any\nknowledge of training class names it can be used to effectively transfer\nknowledge. The code is publicly available at https://github.com/miccunifi/KDPL.",
      "tldr_zh": "这篇论文提出了一种名为 Knowledge Distillation Prompt Learning (KDPL) 的方法，通过从更强大模型的无监督知识蒸馏，改进 Vision-Language Models (VLMs) 的学习提示，以提升零样本泛化能力。KDPL 无需标注样本即可整合到现有提示学习技术中，实现对下游任务的适应。实验在十多个标准基准数据集上证明，该方法显著提高了零样本域泛化、零样本跨数据集泛化和零样本基到新类泛化的性能。即使没有训练类名信息，KDPL 也能有效转移知识，代码已开源于 GitHub。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication at ECCV24",
      "pdf_url": "http://arxiv.org/pdf/2407.03056v2",
      "published_date": "2024-07-03 12:24:40 UTC",
      "updated_date": "2024-07-30 11:56:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:19:16.340257"
    },
    {
      "arxiv_id": "2407.03049v1",
      "title": "Enhancements for Real-Time Monte-Carlo Tree Search in General Video Game Playing",
      "title_zh": "翻译失败",
      "authors": [
        "Dennis J. N. J. Soemers",
        "Chiara F. Sironi",
        "Torsten Schuster",
        "Mark H. M. Winands"
      ],
      "abstract": "General Video Game Playing (GVGP) is a field of Artificial Intelligence where\nagents play a variety of real-time video games that are unknown in advance.\nThis limits the use of domain-specific heuristics. Monte-Carlo Tree Search\n(MCTS) is a search technique for game playing that does not rely on\ndomain-specific knowledge. This paper discusses eight enhancements for MCTS in\nGVGP; Progressive History, N-Gram Selection Technique, Tree Reuse,\nBreadth-First Tree Initialization, Loss Avoidance, Novelty-Based Pruning,\nKnowledge-Based Evaluations, and Deterministic Game Detection. Some of these\nare known from existing literature, and are either extended or introduced in\nthe context of GVGP, and some are novel enhancements for MCTS. Most\nenhancements are shown to provide statistically significant increases in win\npercentages when applied individually. When combined, they increase the average\nwin percentage over sixty different games from 31.0% to 48.4% in comparison to\na vanilla MCTS implementation, approaching a level that is competitive with the\nbest agents of the GVG-AI competition in 2015.",
      "tldr_zh": "这篇论文针对 General Video Game Playing (GVGP) 中的 Real-Time Monte-Carlo Tree Search (MCTS) 提出八种增强技术，包括 Progressive History、N-Gram Selection Technique、Tree Reuse、Breadth-First Tree Initialization、Loss Avoidance、Novelty-Based Pruning、Knowledge-Based Evaluations 和 Deterministic Game Detection，以提升 AI 代理在未知实时视频游戏中的表现。\n这些增强部分基于现有文献的扩展或引入 GVGP 上下文，部分为新颖创新，旨在不依赖领域特定知识的情况下优化搜索策略。\n实验结果显示，单独应用这些增强可显著提高获胜率，而组合使用后，将平均获胜率从 31.0% 提升至 48.4%，接近 2015 年 GVG-AI 竞赛的最佳代理水平。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Green Open Access version of conference paper published in 2016",
      "pdf_url": "http://arxiv.org/pdf/2407.03049v1",
      "published_date": "2024-07-03 12:18:28 UTC",
      "updated_date": "2024-07-03 12:18:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:19:38.521426"
    },
    {
      "arxiv_id": "2407.03040v1",
      "title": "Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Xia Hou",
        "Qifeng Li",
        "Jian Yang",
        "Tongliang Li",
        "Linzheng Chai",
        "Xianjie Wu",
        "Hangyuan Ji",
        "Zhoujun Li",
        "Jixuan Nie",
        "Jingbo Dun",
        "Wenfeng Song"
      ],
      "abstract": "Instruction tuning as an effective technique aligns the outputs of large\nlanguage models (LLMs) with human preference. But how to generate the seasonal\nmulti-turn dialogues from raw documents for instruction tuning still requires\nfurther exploration. In this paper, we present a novel framework named R2S that\nleverages the CoD-Chain of Dialogue logic to guide large language models (LLMs)\nin generating knowledge-intensive multi-turn dialogues for instruction tuning.\nBy integrating raw documents from both open-source datasets and domain-specific\nweb-crawled documents into a benchmark K-BENCH, we cover diverse areas such as\nWikipedia (English), Science (Chinese), and Artifacts (Chinese). Our approach\nfirst decides the logic flow of the current dialogue and then prompts LLMs to\nproduce key phrases for sourcing relevant response content. This methodology\nenables the creation of the G I NSTRUCT instruction dataset, retaining raw\ndocument knowledge within dialoguestyle interactions. Utilizing this dataset,\nwe fine-tune GLLM, a model designed to transform raw documents into structured\nmulti-turn dialogues, thereby injecting comprehensive domain knowledge into the\nSFT model for enhanced instruction tuning. This work signifies a stride towards\nrefining the adaptability and effectiveness of LLMs in processing and\ngenerating more accurate, contextually nuanced responses across various fields.",
      "tldr_zh": "这篇论文提出了 R2S 框架，利用 CoD-Chain of Dialogue logic 指导大型语言模型 (LLMs) 从原始文档生成知识密集型多轮对话，以优化指令微调过程。框架首先构建了 K-BENCH 基准数据集，整合开源和网页爬取的文档，覆盖 Wikipedia (English)、Science (Chinese) 和 Artifacts (Chinese) 等领域；随后，通过决定对话逻辑流程并生成关键短语来提取相关响应内容，创建了 GINSTRUCT 指令数据集。利用该数据集微调 GLLM 模型，将原始文本转化为结构化的多轮对话，最终提升了 LLMs 在处理和生成准确、上下文相关的响应的适应性和有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.03040v1",
      "published_date": "2024-07-03 12:04:10 UTC",
      "updated_date": "2024-07-03 12:04:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:19:41.198812"
    },
    {
      "arxiv_id": "2407.03035v1",
      "title": "NLP Sampling: Combining MCMC and NLP Methods for Diverse Constrained Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Marc Toussaint",
        "Cornelius V. Braun",
        "Joaquim Ortiz-Haro"
      ],
      "abstract": "Generating diverse samples under hard constraints is a core challenge in many\nareas. With this work we aim to provide an integrative view and framework to\ncombine methods from the fields of MCMC, constrained optimization, as well as\nrobotics, and gain insights in their strengths from empirical evaluations. We\npropose NLP Sampling as a general problem formulation, propose a family of\nrestarting two-phase methods as a framework to integrated methods from across\nthe fields, and evaluate them on analytical and robotic manipulation planning\nproblems. Complementary to this, we provide several conceptual discussions,\ne.g. on the role of Lagrange parameters, global sampling, and the idea of a\nDiffused NLP and a corresponding model-based denoising sampler.",
      "tldr_zh": "这篇论文针对生成多样样本的同时满足硬约束这一核心挑战，提出NLP Sampling作为一种通用问题表述，旨在整合MCMC（Markov Chain Monte Carlo）、约束优化和机器人学方法。作者设计了一族重启的两阶段方法（restarting two-phase methods）作为框架，将这些领域的方法结合，并通过实证评估在分析性和机器人操作规划问题上验证了它们的优势。论文还讨论了Lagrange parameters的作用、全局采样、Diffused NLP以及基于模型的去噪采样器等概念，为跨领域约束采样提供了新见解。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03035v1",
      "published_date": "2024-07-03 11:55:06 UTC",
      "updated_date": "2024-07-03 11:55:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:19:54.254516"
    },
    {
      "arxiv_id": "2407.03034v1",
      "title": "Attention Incorporated Network for Sharing Low-rank, Image and K-space Information during MR Image Reconstruction to Achieve Single Breath-hold Cardiac Cine Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Siying Xu",
        "Kerstin Hammernik",
        "Andreas Lingg",
        "Jens Kuebler",
        "Patrick Krumm",
        "Daniel Rueckert",
        "Sergios Gatidis",
        "Thomas Kuestner"
      ],
      "abstract": "Cardiac Cine Magnetic Resonance Imaging (MRI) provides an accurate assessment\nof heart morphology and function in clinical practice. However, MRI requires\nlong acquisition times, with recent deep learning-based methods showing great\npromise to accelerate imaging and enhance reconstruction quality. Existing\nnetworks exhibit some common limitations that constrain further acceleration\npossibilities, including single-domain learning, reliance on a single\nregularization term, and equal feature contribution. To address these\nlimitations, we propose to embed information from multiple domains, including\nlow-rank, image, and k-space, in a novel deep learning network for MRI\nreconstruction, which we denote as A-LIKNet. A-LIKNet adopts a parallel-branch\nstructure, enabling independent learning in the k-space and image domain.\nCoupled information sharing layers realize the information exchange between\ndomains. Furthermore, we introduce attention mechanisms into the network to\nassign greater weights to more critical coils or important temporal frames.\nTraining and testing were conducted on an in-house dataset, including 91\ncardiovascular patients and 38 healthy subjects scanned with 2D cardiac Cine\nusing retrospective undersampling. Additionally, we evaluated A-LIKNet on the\nreal-time 8x prospectively undersampled data from the OCMR dataset. The results\ndemonstrate that our proposed A-LIKNet outperforms existing methods and\nprovides high-quality reconstructions. The network can effectively reconstruct\nhighly retrospectively undersampled dynamic MR images up to 24x accelerations,\nindicating its potential for single breath-hold imaging.",
      "tldr_zh": "该研究针对心脏Cine磁共振成像(MRI)的长采集时间问题，提出了一种新型网络A-LIKNet，以实现单次呼吸保持下的快速成像。A-LIKNet通过嵌入low-rank、image和k-space多域信息，采用平行分支结构在k-space和图像域独立学习，并利用耦合信息共享层和attention mechanisms来交换信息并优先权重关键线圈或时间帧，从而克服现有方法的单域学习和特征贡献平等的局限。实验结果显示，A-LIKNet在内部数据集和OCMR数据集上优于现有方法，能有效重建高达24倍加速的动态MR图像，为临床心脏成像提供高质高效解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03034v1",
      "published_date": "2024-07-03 11:54:43 UTC",
      "updated_date": "2024-07-03 11:54:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:20:02.498132"
    },
    {
      "arxiv_id": "2407.03026v1",
      "title": "Qifusion-Net: Layer-adapted Stream/Non-stream Model for End-to-End Multi-Accent Speech Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Jinming Chen",
        "Jingyi Fang",
        "Yuanzhong Zheng",
        "Yaoxuan Wang",
        "Haojun Fei"
      ],
      "abstract": "Currently, end-to-end (E2E) speech recognition methods have achieved\npromising performance. However, auto speech recognition (ASR) models still face\nchallenges in recognizing multi-accent speech accurately. We propose a\nlayer-adapted fusion (LAF) model, called Qifusion-Net, which does not require\nany prior knowledge about the target accent. Based on dynamic chunk strategy,\nour approach enables streaming decoding and can extract frame-level acoustic\nfeature, facilitating fine-grained information fusion. Experiment results\ndemonstrate that our proposed methods outperform the baseline with relative\nreductions of 22.1$\\%$ and 17.2$\\%$ in character error rate (CER) across multi\naccent test datasets on KeSpeech and MagicData-RMAC.",
      "tldr_zh": "该论文针对端到端（E2E）语音识别（ASR）模型在多口音语音识别中的准确性挑战，提出了一种层适应融合（LAF）模型，名为 Qifusion-Net，该模型无需目标口音的先验知识，并基于动态块策略支持流式/非流式解码，以提取帧级声学特征并实现细粒度信息融合。  \n与基线模型相比，Qifusion-Net 在 KeSpeech 和 MagicData-RMAC 多口音测试数据集上，字符错误率（CER）分别实现了 22.1% 和 17.2% 的相对降低。  \n这项工作为提升 ASR 系统的鲁棒性和泛化能力提供了新途径。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "accpeted by interspeech 2014, 5 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2407.03026v1",
      "published_date": "2024-07-03 11:35:52 UTC",
      "updated_date": "2024-07-03 11:35:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:20:15.661600"
    },
    {
      "arxiv_id": "2407.03018v1",
      "title": "An Organism Starts with a Single Pix-Cell: A Neural Cellular Diffusion for High-Resolution Image Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Marawan Elbatel",
        "Konstantinos Kamnitsas",
        "Xiaomeng Li"
      ],
      "abstract": "Generative modeling seeks to approximate the statistical properties of real\ndata, enabling synthesis of new data that closely resembles the original\ndistribution. Generative Adversarial Networks (GANs) and Denoising Diffusion\nProbabilistic Models (DDPMs) represent significant advancements in generative\nmodeling, drawing inspiration from game theory and thermodynamics,\nrespectively. Nevertheless, the exploration of generative modeling through the\nlens of biological evolution remains largely untapped. In this paper, we\nintroduce a novel family of models termed Generative Cellular Automata (GeCA),\ninspired by the evolution of an organism from a single cell. GeCAs are\nevaluated as an effective augmentation tool for retinal disease classification\nacross two imaging modalities: Fundus and Optical Coherence Tomography (OCT).\nIn the context of OCT imaging, where data is scarce and the distribution of\nclasses is inherently skewed, GeCA significantly boosts the performance of 11\ndifferent ophthalmological conditions, achieving a 12% increase in the average\nF1 score compared to conventional baselines. GeCAs outperform both diffusion\nmethods that incorporate UNet or state-of-the art variants with\ntransformer-based denoising models, under similar parameter constraints. Code\nis available at: https://github.com/xmed-lab/GeCA.",
      "tldr_zh": "本论文提出了一种新型生成模型 Generative Cellular Automata (GeCA)，受生物进化启发，从一个单像素细胞（Pix-Cell）开始，通过神经元细胞扩散实现高分辨率图像合成。GeCA 被用作数据增强工具，提升视网膜疾病分类性能，在 Fundus 和 Optical Coherence Tomography (OCT) 成像模式下表现突出，尤其在数据稀缺的 OCT 任务中，使 11 种眼科疾病的平均 F1 score 比传统基线提高 12%。与 Generative Adversarial Networks (GANs) 和 Denoising Diffusion Probabilistic Models (DDPMs) 等方法相比，GeCA 在类似参数约束下表现出优越性，为生成建模开辟了新路径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.03018v1",
      "published_date": "2024-07-03 11:26:09 UTC",
      "updated_date": "2024-07-03 11:26:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:20:29.714828"
    },
    {
      "arxiv_id": "2407.12830v2",
      "title": "Knowledge-based Consistency Testing of Large Language Models",
      "title_zh": "基于知识的大型语言模型一致性测试",
      "authors": [
        "Sai Sathiesh Rajan",
        "Ezekiel Soremekun",
        "Sudipta Chattopadhyay"
      ],
      "abstract": "In this work, we systematically expose and measure the inconsistency and\nknowledge gaps of Large Language Models (LLMs). Specifically, we propose an\nautomated testing framework (called KonTest) which leverages a knowledge graph\nto construct test cases. KonTest probes and measures the inconsistencies in the\nLLM's knowledge of the world via a combination of semantically-equivalent\nqueries and test oracles (metamorphic or ontological oracle). KonTest further\nmitigates knowledge gaps via a weighted LLM model ensemble. Using four\nstate-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that\nKonTest generates 19.2% error inducing inputs (1917 errors from 9979 test\ninputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A\nmitigation method informed by KonTest's test suite reduces LLM knowledge gap by\n32.48%. Our ablation study further shows that GPT3.5 is not suitable for\nknowledge-based consistency testing because it is only 60%-68% effective in\nknowledge construction.",
      "tldr_zh": "本文提出了一种基于知识图谱的自动化测试框架KonTest，用于评估大型语言模型(LLMs)的知识不一致性和知识差距。KonTest通过语义等价查询和测试预言(metamorphic or ontological oracle)构建测试案例，并采用加权LLM模型集成来缓解知识差距。在测试Falcon、Gemini、GPT3.5和Llama2等四种先进LLMs时，该框架生成了19.2%的错误诱导输入，并揭示了16.5%的知识差距；通过缓解方法，知识差距减少了32.48%。此外，消融研究显示，GPT3.5在知识构建上仅60%-68%有效，因此不适合用于知识一致性测试。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 4 figures, 8 tables, Accepted at EMNLP 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2407.12830v2",
      "published_date": "2024-07-03 11:16:54 UTC",
      "updated_date": "2024-10-05 14:12:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:20:41.303539"
    },
    {
      "arxiv_id": "2407.03007v1",
      "title": "What Affects the Stability of Tool Learning? An Empirical Study on the Robustness of Tool Learning Frameworks",
      "title_zh": "什么影响工具学习的稳定性？ 关于工具学习框架鲁棒性的实证研究",
      "authors": [
        "Chengrui Huang",
        "Zhengliang Shi",
        "Yuntao Wen",
        "Xiuying Chen",
        "Peng Han",
        "Shen Gao",
        "Shuo Shang"
      ],
      "abstract": "Tool learning methods have enhanced the ability of large language models\n(LLMs) to interact with real-world applications. Many existing works fine-tune\nLLMs or design prompts to enable LLMs to select appropriate tools and correctly\ninvoke them to meet user requirements. However, it is observed in previous\nworks that the performance of tool learning varies from tasks, datasets,\ntraining settings, and algorithms. Without understanding the impact of these\nfactors, it can lead to inconsistent results, inefficient model deployment, and\nsuboptimal tool utilization, ultimately hindering the practical integration and\nscalability of LLMs in real-world scenarios. Therefore, in this paper, we\nexplore the impact of both internal and external factors on the performance of\ntool learning frameworks. Through extensive experiments on two benchmark\ndatasets, we find several insightful conclusions for future work, including the\nobservation that LLMs can benefit significantly from increased trial and\nexploration. We believe our empirical study provides a new perspective for\nfuture tool learning research.",
      "tldr_zh": "该论文通过实证研究探讨了影响工具学习框架稳定性的内部和外部因素，包括任务、数据集、训练设置和算法。研究者对两个基准数据集进行了广泛实验，分析这些因素对大语言模型（LLMs）性能的影响。结果发现，LLMs可以通过增加尝试和探索显著提升效果，并为未来tool learning研究提供了新的视角和指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.03007v1",
      "published_date": "2024-07-03 11:06:05 UTC",
      "updated_date": "2024-07-03 11:06:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:21:00.545976"
    },
    {
      "arxiv_id": "2407.03005v1",
      "title": "Human-like Linguistic Biases in Neural Speech Models: Phonetic Categorization and Phonotactic Constraints in Wav2Vec2.0",
      "title_zh": "翻译失败",
      "authors": [
        "Marianne de Heer Kloots",
        "Willem Zuidema"
      ],
      "abstract": "What do deep neural speech models know about phonology? Existing work has\nexamined the encoding of individual linguistic units such as phonemes in these\nmodels. Here we investigate interactions between units. Inspired by classic\nexperiments on human speech perception, we study how Wav2Vec2 resolves\nphonotactic constraints. We synthesize sounds on an acoustic continuum between\n/l/ and /r/ and embed them in controlled contexts where only /l/, only /r/, or\nneither occur in English. Like humans, Wav2Vec2 models show a bias towards the\nphonotactically admissable category in processing such ambiguous sounds. Using\nsimple measures to analyze model internals on the level of individual stimuli,\nwe find that this bias emerges in early layers of the model's Transformer\nmodule. This effect is amplified by ASR finetuning but also present in fully\nself-supervised models. Our approach demonstrates how controlled stimulus\ndesigns can help localize specific linguistic knowledge in neural speech\nmodels.",
      "tldr_zh": "本文研究了神经语音模型 Wav2Vec2.0 是否存在类似于人类的语言偏好，焦点在于 phonetic categorization 和 phonotactic constraints。研究者通过合成 /l/ 和 /r/ 之间的声学连续体声音，并将其嵌入控制语境（仅允许 /l/、仅允许 /r/ 或都不允许），发现 Wav2Vec2 模型在处理模糊声音时偏向音位上可接受的类别，与人类感知类似。分析模型内部显示，这种偏好在 Transformer 模块的早期层出现，并通过 ASR finetuning 得到放大，即使在完全 self-supervised 模型中也存在。该方法证明了使用控制刺激设计能有效定位神经语音模型中的特定语言知识。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Interspeech 2024. For code and materials, see\n  https://github.com/mdhk/phonotactic-sensitivity",
      "pdf_url": "http://arxiv.org/pdf/2407.03005v1",
      "published_date": "2024-07-03 11:04:31 UTC",
      "updated_date": "2024-07-03 11:04:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:21:04.477893"
    },
    {
      "arxiv_id": "2407.03004v2",
      "title": "SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured Clinical Narratives in Epilepsy",
      "title_zh": "翻译失败",
      "authors": [
        "Meghal Dani",
        "Muthu Jeyanthi Prakash",
        "Zeynep Akata",
        "Stefanie Liebe"
      ],
      "abstract": "Large Language Models (LLMs) have been shown to encode clinical knowledge.\nMany evaluations, however, rely on structured question-answer benchmarks,\noverlooking critical challenges of interpreting and reasoning about\nunstructured clinical narratives in real-world settings. Using free-text\nclinical descriptions, we present SemioLLM, an evaluation framework that\nbenchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B,\nLlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of\n1,269 seizure descriptions, we show that most LLMs are able to accurately and\nconfidently generate probabilistic predictions of seizure onset zones in the\nbrain. Most models approach clinician-level performance after prompt\nengineering, with expert-guided chain-of-thought reasoning leading to the most\nconsistent improvements. Performance was further strongly modulated by clinical\nin-context impersonation, narrative length and language context (13.7%, 32.7%\nand 14.2% performance variation, respectively). However, expert analysis of\nreasoning outputs revealed that correct prediction can be based on hallucinated\nknowledge and deficient source citation accuracy, underscoring the need to\nimprove interpretability of LLMs in clinical use. Overall, SemioLLM provides a\nscalable, domain-adaptable framework for evaluating LLMs in clinical\ndisciplines where unstructured verbal descriptions encode diagnostic\ninformation. By identifying both the strengths and limitations of\nstate-of-the-art models, our work supports the development of clinically robust\nand globally applicable AI systems for healthcare.",
      "tldr_zh": "本研究提出 SemioLLM 框架，用于评估大型语言模型 (LLMs) 在处理非结构化临床叙述时的诊断推理能力，焦点是癫痫领域。框架基准测试了 6 个最先进模型（如 GPT-3.5 和 GPT-4），利用 1,269 个癫痫发作描述，发现这些模型在生成大脑发作区域概率预测时能接近临床医生水平，尤其通过提示工程和 chain-of-thought reasoning 实现最显著改善。性能受临床情境模拟、叙述长度和语言上下文影响，分别导致 13.7%、32.7% 和 14.2% 的变化，但分析显示正确预测可能依赖幻觉知识和来源引用不准确，强调了提升 LLMs 可解释性的必要性。整体而言，SemioLLM 提供了一个可扩展、适应领域的评估工具，支持开发更可靠的 AI 系统用于全球医疗应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03004v2",
      "published_date": "2024-07-03 11:02:12 UTC",
      "updated_date": "2025-04-23 14:25:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:21:16.047767"
    },
    {
      "arxiv_id": "2407.02996v2",
      "title": "Are Large Language Models Consistent over Value-laden Questions?",
      "title_zh": "大型语言模型在涉及价值观的问题",
      "authors": [
        "Jared Moore",
        "Tanvi Deshpande",
        "Diyi Yang"
      ],
      "abstract": "Large language models (LLMs) appear to bias their survey answers toward\ncertain values. Nonetheless, some argue that LLMs are too inconsistent to\nsimulate particular values. Are they? To answer, we first define value\nconsistency as the similarity of answers across (1) paraphrases of one\nquestion, (2) related questions under one topic, (3) multiple-choice and\nopen-ended use-cases of one question, and (4) multilingual translations of a\nquestion to English, Chinese, German, and Japanese. We apply these measures to\nsmall and large, open LLMs including llama-3, as well as gpt-4o, using 8,000\nquestions spanning more than 300 topics. Unlike prior work, we find that models\nare relatively consistent across paraphrases, use-cases, translations, and\nwithin a topic. Still, some inconsistencies remain. Models are more consistent\non uncontroversial topics (e.g., in the U.S., \"Thanksgiving\") than on\ncontroversial ones (\"euthanasia\"). Base models are both more consistent\ncompared to fine-tuned models and are uniform in their consistency across\ntopics, while fine-tuned models are more inconsistent about some topics\n(\"euthanasia\") than others (\"women's rights\") like our human subjects (n=165).",
      "tldr_zh": "这篇论文探讨了大语言模型(LLMs)在价值导向问题上的一致性，通过定义四个指标（包括问题改写、相关问题、多选/开放式回答以及多语言翻译）来评估模型表现，并使用8000个问题和多种模型（如llama-3和gpt-4o）进行测试。研究发现，LLMs在改写、用例、翻译和主题内相对一致，但对争议性主题（如“euthanasia”）的不一致性更高，而在非争议性主题（如“Thanksgiving”）上表现更稳定。基础模型比微调模型更一致，且微调模型的不一致模式类似于人类受试者（n=165），这为理解LLMs的价值观模拟提供了新洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 10 figures, In Findings of EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.02996v2",
      "published_date": "2024-07-03 10:53:54 UTC",
      "updated_date": "2024-10-01 21:23:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:21:30.907082"
    },
    {
      "arxiv_id": "2407.02994v4",
      "title": "MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced AI Applications",
      "title_zh": "MedPix 2.0：一种全面的多模态生物医学数据集，用于高级 AI 应用",
      "authors": [
        "Irene Siragusa",
        "Salvatore Contino",
        "Massimo La Ciura",
        "Rosario Alicata",
        "Roberto Pirrone"
      ],
      "abstract": "The increasing interest in developing Artificial Intelligence applications in\nthe medical domain, suffers from the lack of high-quality data set, mainly due\nto privacy-related issues. Moreover, the recent rising of Large Multimodal\nModels (LMM) leads to a need for multimodal medical data sets, where clinical\nreports and findings are attached to the corresponding CT or MR scans. This\npaper illustrates the entire workflow for building the data set MedPix 2.0.\nStarting from the well-known multimodal data set MedPix, mainly used by\nphysicians, nurses and healthcare students for Continuing Medical Education\npurposes, a semi-automatic pipeline was developed to extract visual and textual\ndata followed by a manual curing procedure where noisy samples were removed,\nthus creating a MongoDB database. Along with the data set, we developed a GUI\naimed at navigating efficiently the MongoDB instance, and obtaining the raw\ndata that can be easily used for training and/or fine-tuning LMMs. To enforce\nthis point, we also propose a CLIP-based model trained on MedPix 2.0 for\nscanning modality and location classification tasks. MedPix 2.0 is available on\nGitHub",
      "tldr_zh": "该论文介绍了MedPix 2.0，一种全面的多模态生物医学数据集，旨在解决医疗AI应用中数据质量和隐私问题，并支持Large Multimodal Models (LMMs)的训练和微调。研究团队开发了一个半自动管道，从原有MedPix数据集提取视觉和文本数据，并通过手动清理创建MongoDB数据库，同时设计了一个GUI工具来高效导航和获取数据。为了验证其实用性，他们训练了一个基于CLIP的模型，用于扫描模态和位置分类任务，MedPix 2.0已在GitHub上公开可用。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02994v4",
      "published_date": "2024-07-03 10:49:21 UTC",
      "updated_date": "2025-04-30 11:41:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:21:42.328734"
    },
    {
      "arxiv_id": "2407.02987v2",
      "title": "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hayder Elesedy",
        "Pedro M. Esperança",
        "Silviu Vlad Oprea",
        "Mete Ozay"
      ],
      "abstract": "Guardrails have emerged as an alternative to safety alignment for content\nmoderation of large language models (LLMs). Existing model-based guardrails\nhave not been designed for resource-constrained computational portable devices,\nsuch as mobile phones, more and more of which are running LLM-based\napplications locally. We introduce LoRA-Guard, a parameter-efficient guardrail\nadaptation method that relies on knowledge sharing between LLMs and guardrail\nmodels. LoRA-Guard extracts language features from the LLMs and adapts them for\nthe content moderation task using low-rank adapters, while a dual-path design\nprevents any performance degradation on the generative task. We show that\nLoRA-Guard outperforms existing approaches with 100-1000x lower parameter\noverhead while maintaining accuracy, enabling on-device content moderation.",
      "tldr_zh": "该论文提出 LoRA-Guard，一种参数高效的守卫栏适应方法，用于大型语言模型 (LLMs) 的内容审查，旨在解决现有方法在资源受限设备（如手机）上的适用性问题。LoRA-Guard 通过 LLMs 和守卫模型之间的知识共享，从 LLMs 提取语言特征，并利用 low-rank adapters 进行适应，同时采用 dual-path design 确保生成任务性能不受影响。实验结果表明，LoRA-Guard 的参数开销比现有方法低 100-1000 倍，同时保持准确性，从而支持设备端内容审查。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02987v2",
      "published_date": "2024-07-03 10:38:40 UTC",
      "updated_date": "2024-12-18 16:07:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:21:51.644260"
    },
    {
      "arxiv_id": "2407.02978v1",
      "title": "Mast Kalandar at SemEval-2024 Task 8: On the Trail of Textual Origins: RoBERTa-BiLSTM Approach to Detect AI-Generated Text",
      "title_zh": "翻译失败",
      "authors": [
        "Jainit Sushil Bafna",
        "Hardik Mittal",
        "Suyash Sethia",
        "Manish Shrivastava",
        "Radhika Mamidi"
      ],
      "abstract": "Large Language Models (LLMs) have showcased impressive abilities in\ngenerating fluent responses to diverse user queries. However, concerns\nregarding the potential misuse of such texts in journalism, educational, and\nacademic contexts have surfaced. SemEval 2024 introduces the task of\nMultigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text\nDetection, aiming to develop automated systems for identifying\nmachine-generated text and detecting potential misuse. In this paper, we i)\npropose a RoBERTa-BiLSTM based classifier designed to classify text into two\ncategories: AI-generated or human ii) conduct a comparative study of our model\nwith baseline approaches to evaluate its effectiveness. This paper contributes\nto the advancement of automatic text detection systems in addressing the\nchallenges posed by machine-generated text misuse. Our architecture ranked 46th\non the official leaderboard with an accuracy of 80.83 among 125.",
      "tldr_zh": "本研究针对 Large Language Models (LLMs) 生成文本的潜在滥用问题，提出了一种基于 RoBERTa-BiLSTM 的分类器，用于将文本分类为 AI 生成或人类生成，并与基线方法进行了比较评估。该模型在 SemEval-2024 Task 8 中表现突出，排名第 46，准确率达 80.83%。这项工作有助于提升自动文本检测系统的能力，应对机器生成文本在新闻、教育和学术领域的潜在误用挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "SemEval-2024",
      "pdf_url": "http://arxiv.org/pdf/2407.02978v1",
      "published_date": "2024-07-03 10:22:23 UTC",
      "updated_date": "2024-07-03 10:22:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:22:06.498965"
    },
    {
      "arxiv_id": "2407.02977v1",
      "title": "Large Language Models as Evaluators for Scientific Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Julia Evans",
        "Jennifer D'Souza",
        "Sören Auer"
      ],
      "abstract": "Our study explores how well the state-of-the-art Large Language Models\n(LLMs), like GPT-4 and Mistral, can assess the quality of scientific summaries\nor, more fittingly, scientific syntheses, comparing their evaluations to those\nof human annotators. We used a dataset of 100 research questions and their\nsyntheses made by GPT-4 from abstracts of five related papers, checked against\nhuman quality ratings. The study evaluates both the closed-source GPT-4 and the\nopen-source Mistral model's ability to rate these summaries and provide reasons\nfor their judgments. Preliminary results show that LLMs can offer logical\nexplanations that somewhat match the quality ratings, yet a deeper statistical\nanalysis shows a weak correlation between LLM and human ratings, suggesting the\npotential and current limitations of LLMs in scientific synthesis evaluation.",
      "tldr_zh": "本研究探讨了Large Language Models (LLMs) 如GPT-4和Mistral，作为科学综合评估者的能力，通过比较其对科学总结质量的评分与人类标注者的评价。研究使用了一个数据集，包含100个研究问题及其由GPT-4从五篇相关论文摘要生成的综合，并分析LLMs的评分理由。初步结果显示，LLMs能提供逻辑解释，但统计分析揭示了LLMs与人类评分的相关性较弱，突显了其在科学综合评估中的潜力与当前局限性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages, forthcoming as part of the KONVENS 2024 proceedings\n  https://konvens-2024.univie.ac.at/",
      "pdf_url": "http://arxiv.org/pdf/2407.02977v1",
      "published_date": "2024-07-03 10:21:27 UTC",
      "updated_date": "2024-07-03 10:21:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:22:19.635699"
    },
    {
      "arxiv_id": "2407.02969v1",
      "title": "Zero-X: A Blockchain-Enabled Open-Set Federated Learning Framework for Zero-Day Attack Detection in IoV",
      "title_zh": "翻译失败",
      "authors": [
        "Abdelaziz Amara korba",
        "Abdelwahab Boualouache",
        "Yacine Ghamri-Doudane"
      ],
      "abstract": "The Internet of Vehicles (IoV) is a crucial technology for Intelligent\nTransportation Systems (ITS) that integrates vehicles with the Internet and\nother entities. The emergence of 5G and the forthcoming 6G networks presents an\nenormous potential to transform the IoV by enabling ultra-reliable,\nlow-latency, and high-bandwidth communications. Nevertheless, as connectivity\nexpands, cybersecurity threats have become a significant concern. The issue has\nbeen further exacerbated by the rising number of zero-day (0-day) attacks,\nwhich can exploit unknown vulnerabilities and bypass existing Intrusion\nDetection Systems (IDSs). In this paper, we propose Zero-X, an innovative\nsecurity framework that effectively detects both 0-day and N-day attacks. The\nframework achieves this by combining deep neural networks with Open-Set\nRecognition (OSR). Our approach introduces a novel scheme that uses blockchain\ntechnology to facilitate trusted and decentralized federated learning (FL) of\nthe ZeroX framework. This scheme also prioritizes privacy preservation,\nenabling both CAVs and Security Operation Centers (SOCs) to contribute their\nunique knowledge while protecting the privacy of their sensitive data. To the\nbest of our knowledge, this is the first work to leverage OSR in combination\nwith privacy-preserving FL to identify both 0-day and N-day attacks in the\nrealm of IoV. The in-depth experiments on two recent network traffic datasets\nshow that the proposed framework achieved a high detection rate while\nminimizing the false positive rate. Comparison with related work showed that\nthe Zero-X framework outperforms existing solutions.",
      "tldr_zh": "本文提出Zero-X框架，这是一个基于区块链的开放集联邦学习（Federated Learning, FL）系统，旨在检测IoV（Internet of Vehicles）中的zero-day攻击和N-day攻击。框架结合深度神经网络和Open-Set Recognition (OSR)技术，实现可信的去中心化训练，同时优先保护CAVs和Security Operation Centers (SOCs)的隐私数据。作为首创工作，Zero-X在两个网络流量数据集上的实验中实现了高检测率和低假阳性率，并超过了现有解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02969v1",
      "published_date": "2024-07-03 10:06:15 UTC",
      "updated_date": "2024-07-03 10:06:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:22:31.697707"
    },
    {
      "arxiv_id": "2407.02968v1",
      "title": "Unified Anomaly Detection methods on Edge Device using Knowledge Distillation and Quantization",
      "title_zh": "翻译失败",
      "authors": [
        "Sushovan Jena",
        "Arya Pulkit",
        "Kajal Singh",
        "Anoushka Banerjee",
        "Sharad Joshi",
        "Ananth Ganesh",
        "Dinesh Singh",
        "Arnav Bhavsar"
      ],
      "abstract": "With the rapid advances in deep learning and smart manufacturing in Industry\n4.0, there is an imperative for high-throughput, high-performance, and fully\nintegrated visual inspection systems. Most anomaly detection approaches using\ndefect detection datasets, such as MVTec AD, employ one-class models that\nrequire fitting separate models for each class. On the contrary, unified models\neliminate the need for fitting separate models for each class and significantly\nreduce cost and memory requirements. Thus, in this work, we experiment with\nconsidering a unified multi-class setup. Our experimental study shows that\nmulti-class models perform at par with one-class models for the standard MVTec\nAD dataset. Hence, this indicates that there may not be a need to learn\nseparate object/class-wise models when the object classes are significantly\ndifferent from each other, as is the case of the dataset considered.\nFurthermore, we have deployed three different unified lightweight architectures\non the CPU and an edge device (NVIDIA Jetson Xavier NX). We analyze the\nquantized multi-class anomaly detection models in terms of latency and memory\nrequirements for deployment on the edge device while comparing\nquantization-aware training (QAT) and post-training quantization (PTQ) for\nperformance at different precision widths. In addition, we explored two\ndifferent methods of calibration required in post-training scenarios and show\nthat one of them performs notably better, highlighting its importance for\nunsupervised tasks. Due to quantization, the performance drop in PTQ is further\ncompensated by QAT, which yields at par performance with the original 32-bit\nFloating point in two of the models considered.",
      "tldr_zh": "本研究提出了一种统一的异常检测方法，使用知识蒸馏（Knowledge Distillation）和量化（Quantization），旨在为工业 4.0 的视觉检测系统提供高吞吐量和高性能解决方案。该方法采用多类模型替代传统的单类模型（如 MVTec AD 数据集中的做法），实验显示多类模型在性能上与单类模型相当，同时显著降低成本和内存需求。研究将三种轻量级架构部署在 CPU 和 NVIDIA Jetson Xavier NX 边缘设备上，比较量化感知训练（QAT）和训练后量化（PTQ）的效果，结果表明 QAT 能有效补偿 PTQ 的性能下降，并在不同精度下实现与原 32 位浮点模型相当的表现。总的来说，此方法突出了统一模型在边缘设备部署的可行性，并强调了校准策略的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CC",
        "cs.ET",
        "68T07",
        "I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.02968v1",
      "published_date": "2024-07-03 10:04:48 UTC",
      "updated_date": "2024-07-03 10:04:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:22:45.167851"
    },
    {
      "arxiv_id": "2407.06085v1",
      "title": "LLMcap: Large Language Model for Unsupervised PCAP Failure Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Lukasz Tulczyjew",
        "Kinan Jarrah",
        "Charles Abondo",
        "Dina Bennett",
        "Nathanael Weill"
      ],
      "abstract": "The integration of advanced technologies into telecommunication networks\ncomplicates troubleshooting, posing challenges for manual error identification\nin Packet Capture (PCAP) data. This manual approach, requiring substantial\nresources, becomes impractical at larger scales. Machine learning (ML) methods\noffer alternatives, but the scarcity of labeled data limits accuracy. In this\nstudy, we propose a self-supervised, large language model-based (LLMcap) method\nfor PCAP failure detection. LLMcap leverages language-learning abilities and\nemploys masked language modeling to learn grammar, context, and structure.\nTested rigorously on various PCAPs, it demonstrates high accuracy despite the\nabsence of labeled data during training, presenting a promising solution for\nefficient network analysis. Index Terms: Network troubleshooting, Packet\nCapture Analysis, Self-Supervised Learning, Large Language Model, Network\nQuality of Service, Network Performance.",
      "tldr_zh": "论文提出LLMcap，一种基于大语言模型(Large Language Model)的自监督方法，用于无监督的PCAP故障检测，以解决电信网络中手动错误识别资源密集和机器学习标签数据短缺的问题。LLMcap 利用masked language modeling技术来学习PCAP数据的语法、上下文和结构，从而实现高效的故障检测。实验结果显示，该方法在各种PCAP数据集上表现出高准确率，即使在训练时没有标签数据，为网络故障排除和性能优化提供了高效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "Copyright 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
      "pdf_url": "http://arxiv.org/pdf/2407.06085v1",
      "published_date": "2024-07-03 09:59:27 UTC",
      "updated_date": "2024-07-03 09:59:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:22:55.614710"
    },
    {
      "arxiv_id": "2407.02961v2",
      "title": "Towards a Scalable Reference-Free Evaluation of Generative Models",
      "title_zh": "迈向生成模型的可扩展无参考评估",
      "authors": [
        "Azim Ospanov",
        "Jingwei Zhang",
        "Mohammad Jalali",
        "Xuenan Cao",
        "Andrej Bogdanov",
        "Farzan Farnia"
      ],
      "abstract": "While standard evaluation scores for generative models are mostly\nreference-based, a reference-dependent assessment of generative models could be\ngenerally difficult due to the unavailability of applicable reference datasets.\nRecently, the reference-free entropy scores, VENDI and RKE, have been proposed\nto evaluate the diversity of generated data. However, estimating these scores\nfrom data leads to significant computational costs for large-scale generative\nmodels. In this work, we leverage the random Fourier features framework to\nreduce the computational price and propose the Fourier-based Kernel Entropy\nApproximation (FKEA) method. We utilize FKEA's approximated eigenspectrum of\nthe kernel matrix to efficiently estimate the mentioned entropy scores.\nFurthermore, we show the application of FKEA's proxy eigenvectors to reveal the\nmethod's identified modes in evaluating the diversity of produced samples. We\nprovide a stochastic implementation of the FKEA assessment algorithm with a\ncomplexity $O(n)$ linearly growing with sample size $n$. We extensively\nevaluate FKEA's numerical performance in application to standard image, text,\nand video datasets. Our empirical results indicate the method's scalability and\ninterpretability applied to large-scale generative models. The codebase is\navailable at https://github.com/aziksh-ospanov/FKEA.",
      "tldr_zh": "这篇论文针对生成模型的无参考评估挑战，提出了一种可扩展的方法来评估生成数据的多样性，以解决传统熵分数如 VENDI 和 RKE 的高计算成本问题。作者引入 Fourier-based Kernel Entropy Approximation (FKEA) 方法，利用随机傅立叶特征框架对核矩阵的特征谱进行近似估算，从而高效计算这些熵分数。FKEA 还通过代理特征向量揭示样本多样性的模式，并提供复杂度为 O(n) 的随机实现，便于应用于大规模数据集。在图像、文本和视频数据集上的实验验证了 FKEA 的可扩展性和可解释性，相关代码已在 GitHub 开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02961v2",
      "published_date": "2024-07-03 09:54:58 UTC",
      "updated_date": "2024-11-05 18:16:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:23:10.180889"
    },
    {
      "arxiv_id": "2407.02960v2",
      "title": "ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Frikha",
        "Nassim Walha",
        "Ricardo Mendes",
        "Krishna Kanth Nakka",
        "Xue Jiang",
        "Xuebing Zhou"
      ],
      "abstract": "This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation.",
      "tldr_zh": "该研究解决了在第三方云提供商上对专有大型语言模型 (LLMs) 进行推理和微调的问题，同时确保模型和数据的保密性。作者提出了 ObfuscaTune，一种高效且完全保留实用性的方法，结合简单的 obfuscation 技术与 confidential computing，仅将 5% 的模型参数放置在 TEE 中。实验在不同大小的 GPT-2 模型上和四个 NLP 基准数据集上验证了该方法的有效性。相比朴素版本，ObfuscaTune 通过使用低条件数随机矩阵显著减少了混淆引起的错误，从而提升了整体性能。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at AAAI 2025 (PPAI Workshop)",
      "pdf_url": "http://arxiv.org/pdf/2407.02960v2",
      "published_date": "2024-07-03 09:54:08 UTC",
      "updated_date": "2025-01-12 16:22:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:23:19.533096"
    },
    {
      "arxiv_id": "2407.02956v2",
      "title": "IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Frikha",
        "Nassim Walha",
        "Krishna Kanth Nakka",
        "Ricardo Mendes",
        "Xue Jiang",
        "Xuebing Zhou"
      ],
      "abstract": "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%\nacross 8 different private attributes. Finally, we demonstrate the maturity of\nIncogniText for real-world applications by distilling its anonymization\ncapability into a set of LoRA parameters associated with an on-device model.\nOur results show the possibility of reducing privacy leakage by more than half\nwith limited impact on utility.",
      "tldr_zh": "本研究提出 IncogniText，一种基于 LLM 的私人属性随机化技术，用于增强文本匿名化，旨在防止对手推断作者的私人属性，同时保持文本的含义和语义效用。IncogniText 通过随机化私人属性来误导潜在对手预测错误值，并在实证评估中显示，在 8 种不同私人属性上，隐私泄露减少超过 90%。此外，该方法将匿名化能力提炼成 LoRA 参数，与本地设备模型结合，实现实际应用，同时将隐私泄露减少一半以上，对文本效用影响有限。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at NeurIPS 2024 - Safe GenAI Workshop",
      "pdf_url": "http://arxiv.org/pdf/2407.02956v2",
      "published_date": "2024-07-03 09:49:03 UTC",
      "updated_date": "2025-02-02 16:51:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:23:30.434533"
    },
    {
      "arxiv_id": "2407.02943v1",
      "title": "PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding",
      "title_zh": "翻译失败",
      "authors": [
        "Krishna Kanth Nakka",
        "Ahmed Frikha",
        "Ricardo Mendes",
        "Xue Jiang",
        "Xuebing Zhou"
      ],
      "abstract": "The latest and most impactful advances in large models stem from their\nincreased size. Unfortunately, this translates into an improved memorization\ncapacity, raising data privacy concerns. Specifically, it has been shown that\nmodels can output personal identifiable information (PII) contained in their\ntraining data. However, reported PIII extraction performance varies widely, and\nthere is no consensus on the optimal methodology to evaluate this risk,\nresulting in underestimating realistic adversaries. In this work, we\nempirically demonstrate that it is possible to improve the extractability of\nPII by over ten-fold by grounding the prefix of the manually constructed\nextraction prompt with in-domain data. Our approach, PII-Compass, achieves\nphone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308\nqueries, respectively, i.e., the phone number of 1 person in 15 is extractable.",
      "tldr_zh": "本文研究了大型语言模型(LLM)规模增大导致的增强记忆能力，可能泄露训练数据中的个人信息(PII)，从而引发数据隐私担忧。作者提出PII-Compass方法，通过使用in-domain数据对手动构建的提取提示前缀进行grounding，显著提高了PII提取效率。实验结果显示，该方法在提取电话号码时，提取率分别达到0.92%、3.9%和6.86%（对应1、128和2308个查询），比传统方法提高了十倍以上，强调了评估真实攻击风险的重要性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.02943v1",
      "published_date": "2024-07-03 09:20:04 UTC",
      "updated_date": "2024-07-03 09:20:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:23:44.175121"
    },
    {
      "arxiv_id": "2407.02936v2",
      "title": "GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zike Yuan",
        "Ming Liu",
        "Hui Wang",
        "Bing Qin"
      ],
      "abstract": "Evaluating the graph comprehension and reasoning abilities of Large Language\nModels (LLMs) is challenging and often incomplete. Existing benchmarks focus\nprimarily on pure graph understanding, lacking a comprehensive evaluation\nacross all graph types and detailed capability definitions. This paper presents\nGraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and\nreasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and\ntest models on pure graph and heterogeneous graphs, subdividing capabilities\ninto 10 distinct areas tested through 19 tasks. Our benchmark includes 11\ndatasets with 5,140 graphs of varying complexity. We evaluate four\nclosed-source and eight open-source LLMs, conducting thorough analyses from\nboth ability and task perspectives. Key findings reveal that OpenAI o1 model\nhas amazing comprehension and reasoning capabilities, semantic enrichment\nenhances reasoning performance, node ordering impacts task success, and the\nability to process longer texts does not necessarily improve graph\ncomprehension or reasoning.GraCoRe is open-sourced at\nhttps://github.com/ZIKEYUAN/GraCoRe",
      "tldr_zh": "本文提出 GraCoRe 基准，用于系统评估 Large Language Models (LLMs) 的图理解和复杂推理能力，以弥补现有基准的局限性。GraCoRe 采用三层层次化分类法，将能力分为 10 个领域，通过 19 个任务和 11 个数据集（包含 5,140 个复杂度不同的图）测试纯图和异构图。实验评估了 4 个闭源和 8 个开源 LLMs，结果显示 OpenAI o1 模型在理解和推理方面表现出色，语义增强可提升性能，节点顺序影响任务成功，且处理更长文本并不必然改善图理解。GraCoRe 已开源，供进一步研究使用。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02936v2",
      "published_date": "2024-07-03 09:12:38 UTC",
      "updated_date": "2025-02-26 09:17:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:23:56.693272"
    },
    {
      "arxiv_id": "2407.11027v1",
      "title": "A robust three-way classifier with shadowed granular-balls based on justifiable granularity",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Yang",
        "Lingyun Xiaodiao",
        "Guoyin Wang",
        "Witold Pedrycz",
        "Shuyin Xia",
        "Qinghua Zhang",
        "Di Wu"
      ],
      "abstract": "The granular-ball (GB)-based classifier introduced by Xia, exhibits\nadaptability in creating coarse-grained information granules for input, thereby\nenhancing its generality and flexibility. Nevertheless, the current GB-based\nclassifiers rigidly assign a specific class label to each data instance and\nlacks of the necessary strategies to address uncertain instances. These\nfar-fetched certain classification approachs toward uncertain instances may\nsuffer considerable risks. To solve this problem, we construct a robust\nthree-way classifier with shadowed GBs for uncertain data. Firstly, combine\nwith information entropy, we propose an enhanced GB generation method with the\nprinciple of justifiable granularity. Subsequently, based on minimum\nuncertainty, a shadowed mapping is utilized to partition a GB into Core region,\nImportant region and Unessential region. Based on the constructed shadowed GBs,\nwe establish a three-way classifier to categorize data instances into certain\nclasses and uncertain case. Finally, extensive comparative experiments are\nconducted with 2 three-way classifiers, 3 state-of-the-art GB-based\nclassifiers, and 3 classical machine learning classifiers on 12 public\nbenchmark datasets. The results show that our model demonstrates robustness in\nmanaging uncertain data and effectively mitigates classification risks.\nFurthermore, our model almost outperforms the other comparison methods in both\neffectiveness and efficiency.",
      "tldr_zh": "本研究针对现有 granular-ball (GB) 分类器在处理不确定数据时强制分配标签的问题，提出了一种基于 justifiable granularity 的鲁棒三向分类器，使用 shadowed GBs 来提升分类准确性。首先，该方法结合信息熵优化 GB 生成，并通过 shadowed mapping 将 GB 分割为 Core region、Important region 和 Unessential region，以最小化不确定性。其次，基于这些 shadowed GBs，建立三向分类器，能够将数据实例分类为确定类或不确定情况。实验结果显示，该模型在 12 个公共基准数据集上，比 2 个三向分类器、3 个最先进的 GB 分类器和 3 个经典机器学习分类器在有效性和效率上表现出更强的鲁棒性，并显著降低了分类风险。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11027v1",
      "published_date": "2024-07-03 08:54:45 UTC",
      "updated_date": "2024-07-03 08:54:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:24:07.654504"
    },
    {
      "arxiv_id": "2407.02917v1",
      "title": "Towards Negotiative Dialogue for the Talkamatic Dialogue Manager",
      "title_zh": "翻译失败",
      "authors": [
        "Staffan Larsson",
        "Alexander Berman",
        "David Hjelm"
      ],
      "abstract": "The paper describes a number of dialogue phenomena associated with\nnegotiative dialogue, as implemented in a development version of the Talkamatic\nDialogue Manager (TDM). This implementation is an initial step towards full\ncoverage of general features of negotiative dialogue in TDM.",
      "tldr_zh": "本论文探讨了与谈判对话（negotiative dialogue）相关的对话现象，并将其在Talkamatic Dialogue Manager (TDM)的开发版本中实现。该实现作为初步步骤，旨在逐步覆盖TDM中谈判对话的一般特征。通过描述这些对话现象，论文为提升对话管理系统处理复杂互动提供了基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02917v1",
      "published_date": "2024-07-03 08:49:18 UTC",
      "updated_date": "2024-07-03 08:49:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:24:19.421688"
    },
    {
      "arxiv_id": "2407.02913v1",
      "title": "SFC: Achieve Accurate Fast Convolution under Low-precision Arithmetic",
      "title_zh": "SFC：在低精度算术下实现准确快速卷积",
      "authors": [
        "Liulu He",
        "Yufei Zhao",
        "Rui Gao",
        "Yuan Du",
        "Li Du"
      ],
      "abstract": "Fast convolution algorithms, including Winograd and FFT, can efficiently\naccelerate convolution operations in deep models. However, these algorithms\ndepend on high-precision arithmetic to maintain inference accuracy, which\nconflicts with the model quantization. To resolve this conflict and further\nimprove the efficiency of quantized convolution, we proposes SFC, a new algebra\ntransform for fast convolution by extending the Discrete Fourier Transform\n(DFT) with symbolic computing, in which only additions are required to perform\nthe transformation at specific transform points, avoiding the calculation of\nirrational number and reducing the requirement for precision. Additionally, we\nenhance convolution efficiency by introducing correction terms to convert\ninvalid circular convolution outputs of the Fourier method into effective ones.\nThe numerical error analysis is presented for the first time in this type of\nwork and proves that our algorithms can provide a 3.68x multiplication\nreduction for 3x3 convolution, while the Winograd algorithm only achieves a\n2.25x reduction with similarly low numerical errors. Experiments carried out on\nbenchmarks and FPGA show that our new algorithms can further improve the\ncomputation efficiency of quantized models while maintaining accuracy,\nsurpassing both the quantization-alone method and existing works on fast\nconvolution quantization.",
      "tldr_zh": "本研究提出了一种名为 SFC 的新代数变换，旨在解决快速卷积算法（如 Winograd 和 FFT）在低精度算术下与模型量化之间的冲突问题。SFC 通过扩展 Discrete Fourier Transform (DFT) 与符号计算，仅使用加法进行变换，避免计算无理数，从而降低精度要求，并引入修正项将无效循环卷积输出转换为有效输出。首次进行的数值误差分析显示，SFC 为 3x3 卷积提供 3.68x 的乘法减少，比 Winograd 的 2.25x 减少率更高，同时保持类似低误差。在基准和 FPGA 实验中，SFC 显著提升量化模型的计算效率，同时维持准确性，优于仅量化方法和现有快速卷积量化技术。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "eess.IV",
        "eess.SP",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.02913v1",
      "published_date": "2024-07-03 08:38:14 UTC",
      "updated_date": "2024-07-03 08:38:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:24:32.451691"
    },
    {
      "arxiv_id": "2407.03387v3",
      "title": "ConCodeEval: Evaluating Large Language Models for Code Constraints in Domain-Specific Languages",
      "title_zh": "ConCodeEval：评估大语言模型在领域特定语言中的代码约束",
      "authors": [
        "Mehant Kammakomati",
        "Sameer Pimparkhede",
        "Srikanth Tamilselvam",
        "Prince Kumar",
        "Pushpak Bhattacharyya"
      ],
      "abstract": "Recent work shows Large Language Models (LLMs) struggle to understand natural\nlanguage constraints for various text generation tasks in zero- and few-shot\nsettings. While, in the code domain, there is wide usage of constraints in code\nformat to maintain the integrity of code written in Domain-Specific Languages\n(DSLs) like JSON and YAML which are widely used for system-level programming\ntasks in enterprises. Given that LLMs are increasingly used for system-level\ncode tasks, evaluating if they can comprehend these code constraints is\ncrucial. However, no work has been done to evaluate their controllability over\ncode constraints. Hence, we introduce ConCodeEval, a first-of-its-kind\nbenchmark having two novel tasks for code constraints across five\nrepresentations. Our findings suggest that language models struggle with code\nconstraints. Code languages that perform excellently for normal code tasks do\nnot perform well when the same languages represent fine-grained constraints.",
      "tldr_zh": "这篇论文介绍了ConCodeEval，一个新的基准，用于评估Large Language Models (LLMs)在Domain-Specific Languages (DSLs)如JSON和YAML中处理代码约束的能力。基准包含两个新任务和五种表示形式，针对LLMs在零样本和少样本设置下理解约束的挑战。研究发现，LLMs在代码约束任务上表现不佳，即使在常规代码任务中表现出色的模型，也难以处理细粒度的约束，这突显了提升LLMs可控性的必要性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.03387v3",
      "published_date": "2024-07-03 08:36:13 UTC",
      "updated_date": "2025-03-24 11:44:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:24:44.719272"
    },
    {
      "arxiv_id": "2407.13782v1",
      "title": "Self-supervised ASR Models and Features For Dysarthric and Elderly Speech Recognition",
      "title_zh": "自我监督的 ASR 模型和特征用于构音障碍和老年语音识别",
      "authors": [
        "Shujie Hu",
        "Xurong Xie",
        "Mengzhe Geng",
        "Zengrui Jin",
        "Jiajun Deng",
        "Guinan Li",
        "Yi Wang",
        "Mingyu Cui",
        "Tianzi Wang",
        "Helen Meng",
        "Xunying Liu"
      ],
      "abstract": "Self-supervised learning (SSL) based speech foundation models have been\napplied to a wide range of ASR tasks. However, their application to dysarthric\nand elderly speech via data-intensive parameter fine-tuning is confronted by\nin-domain data scarcity and mismatch. To this end, this paper explores a series\nof approaches to integrate domain fine-tuned SSL pre-trained models and their\nfeatures into TDNN and Conformer ASR systems for dysarthric and elderly speech\nrecognition. These include: a) input feature fusion between standard acoustic\nfrontends and domain fine-tuned SSL speech representations; b) frame-level\njoint decoding between TDNN systems separately trained using standard acoustic\nfeatures alone and those with additional domain fine-tuned SSL features; and c)\nmulti-pass decoding involving the TDNN/Conformer system outputs to be rescored\nusing domain fine-tuned pre-trained ASR models. In addition, fine-tuned SSL\nspeech features are used in acoustic-to-articulatory (A2A) inversion to\nconstruct multi-modal ASR systems. Experiments are conducted on four tasks: the\nEnglish UASpeech and TORGO dysarthric speech corpora; and the English\nDementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets. The TDNN\nsystems constructed by integrating domain-adapted HuBERT, wav2vec2-conformer or\nmulti-lingual XLSR models and their features consistently outperform the\nstandalone fine-tuned SSL pre-trained models. These systems produced\nstatistically significant WER or CER reductions of 6.53%, 1.90%, 2.04% and\n7.97% absolute (24.10%, 23.84%, 10.14% and 31.39% relative) on the four tasks\nrespectively. Consistent improvements in Alzheimer's Disease detection accuracy\nare also obtained using the DementiaBank Pitt elderly speech recognition\noutputs.",
      "tldr_zh": "本研究探讨了自监督学习 (SSL) 模型及其特征在构音障碍和老年语音识别中的应用，针对数据稀缺和不匹配问题，提出将域微调的 SSL 模型（如 HuBERT、wav2vec2-conformer 或 XLSR）整合到 TDNN 和 Conformer ASR 系统中的方法，包括输入特征融合、帧级联合解码以及多通道解码。实验在 English UASpeech、TORGO 构音障碍语料库以及 English DementiaBank Pitt 和 Cantonese JCCOCC MoCA 老年语料库上进行，结果显示整合后的系统比独立微调 SSL 模型性能更优，实现 WER 或 CER 绝对减少 6.53%、1.90%、2.04% 和 7.97%（相对减少 24.10%、23.84%、10.14% 和 31.39%）。此外，该方法还提升了基于 DementiaBank Pitt 语料的阿尔茨海默病检测准确率，为鲁棒的 ASR 系统提供了新途径。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "pdf_url": "http://arxiv.org/pdf/2407.13782v1",
      "published_date": "2024-07-03 08:33:39 UTC",
      "updated_date": "2024-07-03 08:33:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:24:59.825020"
    },
    {
      "arxiv_id": "2407.02904v1",
      "title": "The Shortcomings of Force-from-Motion in Robot Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Elie Aljalbout",
        "Felix Frank",
        "Patrick van der Smagt",
        "Alexandros Paraschos"
      ],
      "abstract": "Robotic manipulation requires accurate motion and physical interaction\ncontrol. However, current robot learning approaches focus on motion-centric\naction spaces that do not explicitly give the policy control over the\ninteraction. In this paper, we discuss the repercussions of this choice and\nargue for more interaction-explicit action spaces in robot learning.",
      "tldr_zh": "这篇论文讨论了机器人学习中Force-from-Motion方法的不足，强调机器人操作需要精确的运动和物理交互控制，但当前方法过度依赖motion-centric action spaces，导致政策无法对交互进行明确控制。作者分析了这种选择的负面影响，包括对机器人性能的潜在危害，并主张采用更注重交互的interaction-explicit action spaces，以提升机器人学习的有效性。总体而言，该工作为未来机器人设计提供了重要启示，推动更全面的行动空间策略。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "I.2.6; I.2.8; I.2.9"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02904v1",
      "published_date": "2024-07-03 08:23:02 UTC",
      "updated_date": "2024-07-03 08:23:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:25:09.048183"
    },
    {
      "arxiv_id": "2407.02894v1",
      "title": "Translatotron-V(ison): An End-to-End Model for In-Image Machine Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhibin Lan",
        "Liqiang Niu",
        "Fandong Meng",
        "Jie Zhou",
        "Min Zhang",
        "Jinsong Su"
      ],
      "abstract": "In-image machine translation (IIMT) aims to translate an image containing\ntexts in source language into an image containing translations in target\nlanguage. In this regard, conventional cascaded methods suffer from issues such\nas error propagation, massive parameters, and difficulties in deployment and\nretaining visual characteristics of the input image. Thus, constructing\nend-to-end models has become an option, which, however, faces two main\nchallenges: 1) the huge modeling burden, as it is required to simultaneously\nlearn alignment across languages and preserve the visual characteristics of the\ninput image; 2) the difficulties of directly predicting excessively lengthy\npixel sequences. In this paper, we propose \\textit{Translatotron-V(ision)}, an\nend-to-end IIMT model consisting of four modules. In addition to an image\nencoder, and an image decoder, our model contains a target text decoder and an\nimage tokenizer. Among them, the target text decoder is used to alleviate the\nlanguage alignment burden, and the image tokenizer converts long sequences of\npixels into shorter sequences of visual tokens, preventing the model from\nfocusing on low-level visual features. Besides, we present a two-stage training\nframework for our model to assist the model in learning alignment across\nmodalities and languages. Finally, we propose a location-aware evaluation\nmetric called Structure-BLEU to assess the translation quality of the generated\nimages. Experimental results demonstrate that our model achieves competitive\nperformance compared to cascaded models with only 70.9\\% of parameters, and\nsignificantly outperforms the pixel-level end-to-end IIMT model.",
      "tldr_zh": "这篇论文提出了 Translatotron-Vision，一种端到端的 In-Image Machine Translation (IIMT) 模型，用于将包含源语言文本的图像直接翻译成目标语言图像，从而避免传统级联方法的错误传播和参数冗余问题。模型由图像编码器、图像解码器、目标文本解码器和图像标记器四个模块组成，其中目标文本解码器减轻语言对齐负担，图像标记器将长像素序列转换为更短的视觉标记序列，以简化建模过程。作者引入了两阶段训练框架和新的位置感知评价指标 Structure-BLEU，通过实验验证，该模型仅使用 70.9% 的参数就实现了与级联模型相当的性能，并显著优于像素级端到端模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2407.02894v1",
      "published_date": "2024-07-03 08:15:39 UTC",
      "updated_date": "2024-07-03 08:15:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:25:22.479973"
    },
    {
      "arxiv_id": "2407.02891v1",
      "title": "GPTQT: Quantize Large Language Models Twice to Push the Efficiency",
      "title_zh": "翻译失败",
      "authors": [
        "Yipin Guo",
        "Yilin Lang",
        "Qinyuan Ren"
      ],
      "abstract": "Due to their large size, generative Large Language Models (LLMs) require\nsignificant computing and storage resources. This paper introduces a new\npost-training quantization method, GPTQT, to reduce memory usage and enhance\nprocessing speed by expressing the weight of LLM in 3bit/2bit. Practice has\nshown that minimizing the quantization error of weights is ineffective, leading\nto overfitting. Therefore, GPTQT employs a progressive two-step approach:\ninitially quantizing weights using Linear quantization to a relatively high\nbit, followed by converting obtained int weight to lower bit binary coding. A\nre-explore strategy is proposed to optimize initial scaling factor. During\ninference, these steps are merged into pure binary coding, enabling efficient\ncomputation. Testing across various models and datasets confirms GPTQT's\neffectiveness. Compared to the strong 3-bit quantization baseline, GPTQT\nfurther reduces perplexity by 4.01 on opt-66B and increases speed by 1.24 times\non opt-30b. The results on Llama2 show that GPTQT is currently the best binary\ncoding quantization method for such kind of LLMs.",
      "tldr_zh": "这篇论文提出了GPTQT，一种双步后训练量化方法，用于提升大语言模型(LLMs)的效率，通过将权重表达为3bit/2bit以减少内存使用和提高处理速度。GPTQT采用先用Linear量化到较高位、然后转换为低位二进制编码的策略，并引入重新探索策略优化初始缩放因子，使推理过程合并为高效的纯二进制计算。实验结果显示，与3-bit基准相比，GPTQT在opt-66B上减少了4.01的perplexity，在opt-30b上速度提高了1.24倍，并在Llama2上成为目前最佳的二进制编码量化方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by 11th IEEE International Conference on Cybernetics and\n  Intelligent Systems",
      "pdf_url": "http://arxiv.org/pdf/2407.02891v1",
      "published_date": "2024-07-03 08:08:01 UTC",
      "updated_date": "2024-07-03 08:08:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:25:34.197951"
    },
    {
      "arxiv_id": "2407.02888v1",
      "title": "Joint Optimization of Resource Allocation and Data Selection for Fast and Cost-Efficient Federated Edge Learning",
      "title_zh": "资源分配和数据选择的联合优化，用于快速且成本高效的联邦边缘学习",
      "authors": [
        "Yunjian Jia",
        "Zhen Huang",
        "Jiping Yan",
        "Yulu Zhang",
        "Kun Luo",
        "Wanli Wen"
      ],
      "abstract": "Deploying federated learning at the wireless edge introduces federated edge\nlearning (FEEL). Given FEEL's limited communication resources and potential\nmislabeled data on devices, improper resource allocation or data selection can\nhurt convergence speed and increase training costs. Thus, to realize an\nefficient FEEL system, this paper emphasizes jointly optimizing resource\nallocation and data selection. Specifically, in this work, through rigorously\nmodeling the training process and deriving an upper bound on FEEL's one-round\nconvergence rate, we establish a problem of joint resource allocation and data\nselection, which, unfortunately, cannot be solved directly. Toward this end, we\nequivalently transform the original problem into a solvable form via a variable\nsubstitution and then break it into two subproblems, that is, the resource\nallocation problem and the data selection problem. The two subproblems are\nmixed-integer non-convex and integer non-convex problems, respectively, and\nachieving their optimal solutions is a challenging task. Based on the matching\ntheory and applying the convex-concave procedure and gradient projection\nmethods, we devise a low-complexity suboptimal algorithm for the two\nsubproblems, respectively. Finally, the superiority of our proposed scheme of\njoint resource allocation and data selection is validated by numerical results.",
      "tldr_zh": "本论文针对Federated Edge Learning (FEEL)中的通信资源限制和数据标记错误问题，提出联合优化资源分配和数据选择的方法，以提升训练的收敛速度和成本效率。通过建模训练过程并推导一轮收敛率的上界，论文将联合优化问题转化为可解形式，并分解为资源分配（混合整数非凸问题）和数据选择（整数非凸问题）两个子问题。基于匹配理论、凸凹过程和梯度投影方法，设计了低复杂度的次优算法来求解这些子问题。数值结果验证了该方案的优越性，在FEEL系统的效率上表现出显著改善。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02888v1",
      "published_date": "2024-07-03 08:03:59 UTC",
      "updated_date": "2024-07-03 08:03:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:25:54.539106"
    },
    {
      "arxiv_id": "2407.02884v1",
      "title": "Complex Event Recognition with Symbolic Register Transducers: Extended Technical Report",
      "title_zh": "翻译失败",
      "authors": [
        "Elias Alevizos",
        "Alexander Artikis",
        "Georgios Paliouras"
      ],
      "abstract": "We present a system for Complex Event Recognition (CER) based on automata.\nWhile multiple such systems have been described in the literature, they\ntypically suffer from a lack of clear and denotational semantics, a limitation\nwhich often leads to confusion with respect to their expressive power. In order\nto address this issue, our system is based on an automaton model which is a\ncombination of symbolic and register automata. We extend previous work on these\ntypes of automata, in order to construct a formalism with clear semantics and a\ncorresponding automaton model whose properties can be formally investigated. We\ncall such automata Symbolic Register Transducers (SRT). We show that SRT are\nclosed under various operators, but are not in general closed under complement\nand they are not determinizable. However, they are closed under these\noperations when a window operator, quintessential in Complex Event Recognition,\nis used. We show how SRT can be used in CER in order to detect patterns upon\nstreams of events, using our framework that provides declarative and\ncompositional semantics, and that allows for a systematic treatment of such\nautomata. For SRT to work in pattern detection, we allow them to mark events\nfrom the input stream as belonging to a complex event or not, hence the name\n\"transducers\". We also present an implementation of SRT which can perform CER.\nWe compare our SRT-based CER engine against other state-of-the-art CER systems\nand show that it is both more expressive and more efficient.",
      "tldr_zh": "本文提出了一种基于 Symbolic Register Transducers (SRT) 的 Complex Event Recognition (CER) 系统，以解决现有系统在语义清晰度和表达能力上的局限性。SRT 扩展了 symbolic automata 和 register automata 的框架，提供明确的声明性语义，并支持事件流模式检测，但不封闭于补集和确定化运算，而在窗口操作下则具有封闭性。该系统通过标记输入事件实现高效的模式识别，并在与现有 CER 引擎的比较中，展示了更高的表达性和性能优势。",
      "categories": [
        "cs.FL",
        "cs.AI",
        "cs.DB",
        "F.1.1; F.4.3; I.2.4"
      ],
      "primary_category": "cs.FL",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2110.04032",
      "pdf_url": "http://arxiv.org/pdf/2407.02884v1",
      "published_date": "2024-07-03 07:59:13 UTC",
      "updated_date": "2024-07-03 07:59:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:25:56.758931"
    },
    {
      "arxiv_id": "2407.02881v1",
      "title": "ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid Computation",
      "title_zh": "ShiftAddAug：利用混合计算增强免乘法小型神经网络",
      "authors": [
        "Yipin Guo",
        "Zihao Li",
        "Yilin Lang",
        "Qinyuan Ren"
      ],
      "abstract": "Operators devoid of multiplication, such as Shift and Add, have gained\nprominence for their compatibility with hardware. However, neural networks\n(NNs) employing these operators typically exhibit lower accuracy compared to\nconventional NNs with identical structures. ShiftAddAug uses costly\nmultiplication to augment efficient but less powerful multiplication-free\noperators, improving performance without any inference overhead. It puts a\nShiftAdd tiny NN into a large multiplicative model and encourages it to be\ntrained as a sub-model to obtain additional supervision. In order to solve the\nweight discrepancy problem between hybrid operators, a new weight sharing\nmethod is proposed. Additionally, a novel two stage neural architecture search\nis used to obtain better augmentation effects for smaller but stronger\nmultiplication-free tiny neural networks. The superiority of ShiftAddAug is\nvalidated through experiments in image classification and semantic\nsegmentation, consistently delivering noteworthy enhancements. Remarkably, it\nsecures up to a 4.95% increase in accuracy on the CIFAR100 compared to its\ndirectly trained counterparts, even surpassing the performance of\nmultiplicative NNs.",
      "tldr_zh": "该研究提出ShiftAddAug方法，通过混合计算增强无乘法运算符（如Shift和Add）的微型神经网络（Tiny Neural Network），以提高性能而不增加推理开销。具体而言，该方法将ShiftAdd微型网络作为子模型嵌入到大型乘法模型中进行训练，并引入新的权重共享机制和两阶段神经架构搜索（Neural Architecture Search）来解决混合运算符的权重差异问题，从而优化更小但更强的无乘法网络。在图像分类和语义分割实验中，ShiftAddAug表现出色，在CIFAR100数据集上准确率提升多达4.95%，甚至超越纯乘法神经网络。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by 2024 CVPR Workshop : Efficient Deep Learning for Computer\n  Vision",
      "pdf_url": "http://arxiv.org/pdf/2407.02881v1",
      "published_date": "2024-07-03 07:56:51 UTC",
      "updated_date": "2024-07-03 07:56:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:26:08.374829"
    },
    {
      "arxiv_id": "2407.02880v2",
      "title": "Knowledge Composition using Task Vectors with Learned Anisotropic Scaling",
      "title_zh": "知识合成：使用任务向量及学习各向异性缩放",
      "authors": [
        "Frederic Z. Zhang",
        "Paul Albert",
        "Cristian Rodriguez-Opazo",
        "Anton van den Hengel",
        "Ehsan Abbasnejad"
      ],
      "abstract": "Pre-trained models produce strong generic representations that can be adapted\nvia fine-tuning. The learned weight difference relative to the pre-trained\nmodel, known as a task vector, characterises the direction and stride of\nfine-tuning. The significance of task vectors is such that simple arithmetic\noperations on them can be used to combine diverse representations from\ndifferent domains. This paper builds on these properties of task vectors and\naims to answer (1) whether components of task vectors, particularly parameter\nblocks, exhibit similar characteristics, and (2) how such blocks can be used to\nenhance knowledge composition and transfer. To this end, we introduce aTLAS, an\nalgorithm that linearly combines parameter blocks with different learned\ncoefficients, resulting in anisotropic scaling at the task vector level. We\nshow that such linear combinations explicitly exploit the low intrinsic\ndimensionality of pre-trained models, with only a few coefficients being the\nlearnable parameters. Furthermore, composition of parameter blocks leverages\nthe already learned representations, thereby reducing the dependency on large\namounts of data. We demonstrate the effectiveness of our method in task\narithmetic, few-shot recognition and test-time adaptation, with supervised or\nunsupervised objectives. In particular, we show that (1) learned anisotropic\nscaling allows task vectors to be more disentangled, causing less interference\nin composition; (2) task vector composition excels with scarce or no labeled\ndata and is less prone to domain shift, thus leading to better\ngeneralisability; (3) mixing the most informative parameter blocks across\ndifferent task vectors prior to training can reduce the memory footprint and\nimprove the flexibility of knowledge transfer. Moreover, we show the potential\nof aTLAS as a PEFT method, particularly with less data, and demonstrate its\nscalibility.",
      "tldr_zh": "该论文探讨了使用任务向量（task vectors）进行知识组合的方法，特别引入了 aTLAS 算法，通过线性组合参数块并应用学习各向异性缩放（anisotropic scaling），以增强预训练模型的知识转移和组合。aTLAS 利用预训练模型的低内在维度，仅需少量可学习参数，即可减少对大量数据的依赖，并在任务算术、少样本识别和测试时适应中表现出色。实验结果显示，该方法使任务向量更 disentangled，减少干扰，提高泛化能力，并在数据稀缺场景下作为参数高效微调（PEFT）方法表现出潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS'24",
      "pdf_url": "http://arxiv.org/pdf/2407.02880v2",
      "published_date": "2024-07-03 07:54:08 UTC",
      "updated_date": "2024-10-29 05:10:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:26:31.331212"
    },
    {
      "arxiv_id": "2407.02878v2",
      "title": "Efficient Fusion and Task Guided Embedding for End-to-end Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Yipin Guo",
        "Yilin Lang",
        "Qinyuan Ren"
      ],
      "abstract": "To address the challenges of sensor fusion and safety risk prediction,\ncontemporary closed-loop autonomous driving neural networks leveraging\nimitation learning typically require a substantial volume of parameters and\ncomputational resources to run neural networks. Given the constrained\ncomputational capacities of onboard vehicular computers, we introduce a compact\nyet potent solution named EfficientFuser. This approach employs EfficientViT\nfor visual information extraction and integrates feature maps via cross\nattention. Subsequently, it utilizes a decoder-only transformer for the\namalgamation of multiple features. For prediction purposes, learnable vectors\nare embedded as tokens to probe the association between the task and sensor\nfeatures through attention. Evaluated on the CARLA simulation platform,\nEfficientFuser demonstrates remarkable efficiency, utilizing merely 37.6% of\nthe parameters and 8.7% of the computations compared to the state-of-the-art\nlightweight method with only 0.4% lower driving score, and the safety score\nneared that of the leading safety-enhanced method, showcasing its efficacy and\npotential for practical deployment in autonomous driving systems.",
      "tldr_zh": "本研究提出了一种高效的自动驾驶框架EfficientFuser，以解决传感器融合和安全风险预测的挑战。它采用EfficientViT提取视觉信息，通过cross attention机制整合特征映射，并使用decoder-only transformer融合多模态特征，同时通过可学习向量作为tokens来关联任务与传感器特征。在CARLA模拟平台上评估，EfficientFuser仅使用37.6%的参数和8.7%的计算量，与最先进轻量级方法相比，驾驶分数仅降低0.4%，安全分数接近领先方法，展示了其高效性和实际部署潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Best Student Paper Award of the IEEE 13th Data-Driven Control and\n  Learning Systems Conference",
      "pdf_url": "http://arxiv.org/pdf/2407.02878v2",
      "published_date": "2024-07-03 07:45:58 UTC",
      "updated_date": "2024-07-17 00:50:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:26:36.228690"
    },
    {
      "arxiv_id": "2407.02870v2",
      "title": "Membership Inference Attacks Against Time-Series Models",
      "title_zh": "针对时间序列模型的成员推断攻击",
      "authors": [
        "Noam Koren",
        "Abigail Goldsteen",
        "Guy Amit",
        "Ariel Farkash"
      ],
      "abstract": "Analyzing time-series data that contains personal information, particularly\nin the medical field, presents serious privacy concerns. Sensitive health data\nfrom patients is often used to train machine learning models for diagnostics\nand ongoing care. Assessing the privacy risk of such models is crucial to\nmaking knowledgeable decisions on whether to use a model in production or share\nit with third parties. Membership Inference Attacks (MIA) are a key method for\nthis kind of evaluation, however time-series prediction models have not been\nthoroughly studied in this context. We explore existing MIA techniques on\ntime-series models, and introduce new features, focusing on the seasonality and\ntrend components of the data. Seasonality is estimated using a multivariate\nFourier transform, and a low-degree polynomial is used to approximate trends.\nWe applied these techniques to various types of time-series models, using\ndatasets from the health domain. Our results demonstrate that these new\nfeatures enhance the effectiveness of MIAs in identifying membership, improving\nthe understanding of privacy risks in medical data applications.",
      "tldr_zh": "这篇论文探讨了针对时序模型的 Membership Inference Attacks (MIA)，旨在评估医疗等领域时序数据中个人隐私风险，特别是使用敏感健康数据训练机器学习模型的问题。研究者引入了新特征，包括使用多变量 Fourier transform 估计季节性，以及低度多项式近似趋势组件，以增强现有 MIA 技术。实验结果显示，这些新特征显著提高了 MIA 在各种时序模型上的有效性，帮助更好地理解医疗数据应用的隐私风险。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.02870v2",
      "published_date": "2024-07-03 07:34:49 UTC",
      "updated_date": "2024-09-22 10:35:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:26:47.630462"
    },
    {
      "arxiv_id": "2407.02863v1",
      "title": "Fast maneuver recovery from aerial observation: trajectory clustering and outliers rejection",
      "title_zh": "翻译失败",
      "authors": [
        "Nelson de Moura",
        "Augustin Gervreau-Mercier",
        "Fernando Garrido",
        "Fawzi Nashashibi"
      ],
      "abstract": "The implementation of road user models that realistically reproduce a\ncredible behavior in a multi-agentsimulation is still an open problem. A\ndata-driven approach consists on to deduce behaviors that may exist in real\nsituation to obtain different types of trajectories from a large set of\nobservations. The data, and its classification, could then be used to train\nmodels capable to extrapolate such behavior. Cars and two different types of\nVulnerable Road Users (VRU) will be considered by the trajectory clustering\nmethods proposed: pedestrians and cyclists. The results reported here evaluate\nmethods to extract well-defined trajectory classes from raw data without the\nuse of map information while also separating ''eccentric'' or incomplete\ntrajectories from the ones that are complete and representative in any\nscenario. Two environments will serve as test for the methods develop, three\ndifferent intersections and one roundabout. The resulting clusters of\ntrajectories can then be used for prediction or learning tasks or discarded if\nit is composed by outliers.",
      "tldr_zh": "本研究提出了一种快速 maneuver recovery 方法，通过 trajectory clustering 和 outliers rejection，从空中观察数据中提取和分类道路用户轨迹，旨在构建更真实的道路用户模型。方法不依赖地图信息，直接从原始数据中识别出完整代表性轨迹，同时去除“eccentric”或不完整的异常轨迹（outliers），适用于 cars、pedestrians 和 cyclists 等不同类型 Vulnerable Road Users (VRU)。实验在三个十字路口和一个 roundabout 环境中进行，结果生成的轨迹集群可用于预测、学习任务或其他应用，提高了行为模拟的准确性和可靠性。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02863v1",
      "published_date": "2024-07-03 07:22:21 UTC",
      "updated_date": "2024-07-03 07:22:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:26:57.012302"
    },
    {
      "arxiv_id": "2407.11026v2",
      "title": "Precise and Efficient Orbit Prediction in LEO with Machine Learning using Exogenous Variables",
      "title_zh": "在 LEO 中使用机器学习和外生变量的精确高效轨道预测",
      "authors": [
        "Francisco Caldas",
        "Cláudia Soares"
      ],
      "abstract": "The increasing volume of space objects in Earth's orbit presents a\nsignificant challenge for Space Situational Awareness (SSA). And in particular,\naccurate orbit prediction is crucial to anticipate the position and velocity of\nspace objects, for collision avoidance and space debris mitigation. When\nperforming Orbit Prediction (OP), it is necessary to consider the impact of\nnon-conservative forces, such as atmospheric drag and gravitational\nperturbations, that contribute to uncertainty around the future position of\nspacecraft and space debris alike. Conventional propagator methods like the\nSGP4 inadequately account for these forces, while numerical propagators are\nable to model the forces at a high computational cost. To address these\nlimitations, we propose an orbit prediction algorithm utilizing machine\nlearning. This algorithm forecasts state vectors on a spacecraft using past\npositions and environmental variables like atmospheric density from external\nsources. The orbital data used in the paper is gathered from precision\nephemeris data from the International Laser Ranging Service (ILRS), for the\nperiod of almost a year. We show how the use of machine learning and\ntime-series techniques can produce low positioning errors at a very low\ncomputational cost, thus significantly improving SSA capabilities by providing\nfaster and reliable orbit determination for an ever increasing number of space\nobjects.",
      "tldr_zh": "该论文针对低地球轨道（LEO）中空间态势感知（SSA）的挑战，提出了一种使用机器学习的轨道预测（OP）算法，通过整合外生变量（如大气密度）来精确预测航天器和空间碎片的位置和速度。算法利用过去的位置数据和环境变量进行预测，基于国际激光测距服务（ILRS）的精确星历数据进行训练和验证，与传统方法如 SGP4 相比，它显著降低了计算成本并减少了定位错误。实验结果显示，该方法在多种场景下实现了低计算成本的高精度预测，从而提升了碰撞避免和空间碎片缓解的能力。",
      "categories": [
        "cs.LG",
        "astro-ph.EP",
        "astro-ph.IM",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "presented at IEEE WCCI CEC Congress 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.11026v2",
      "published_date": "2024-07-03 07:12:33 UTC",
      "updated_date": "2024-07-27 22:07:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:27:09.934969"
    },
    {
      "arxiv_id": "2407.11025v4",
      "title": "Backdoor Graph Condensation",
      "title_zh": "翻译失败",
      "authors": [
        "Jiahao Wu",
        "Ning Lu",
        "Zeiyu Dai",
        "Kun Wang",
        "Wenqi Fan",
        "Shengcai Liu",
        "Qing Li",
        "Ke Tang"
      ],
      "abstract": "Graph condensation has recently emerged as a prevalent technique to improve\nthe training efficiency for graph neural networks (GNNs). It condenses a large\ngraph into a small one such that a GNN trained on this small synthetic graph\ncan achieve comparable performance to a GNN trained on the large graph.\nHowever, while existing graph condensation studies mainly focus on the best\ntrade-off between graph size and the GNNs' performance (model utility), they\noverlook the security issues of graph condensation. To bridge this gap, we\nfirst explore backdoor attack against the GNNs trained on the condensed graphs.\n  We introduce an effective backdoor attack against graph condensation, termed\nBGC. This attack aims to (1) preserve the condensed graph quality despite\ntrigger injection, and (2) ensure trigger efficacy through the condensation\nprocess, achieving a high attack success rate. Specifically, BGC consistently\nupdates triggers during condensation and targets representative nodes for\npoisoning. Extensive experiments demonstrate the effectiveness of our attack.\nBGC achieves a high attack success rate (close to 1.0) and good model utility\nin all cases. Furthermore, the results against multiple defense methods\ndemonstrate BGC's resilience under their defenses. Finally, we analyze the key\nhyperparameters that influence the attack performance. Our code is available\nat: https://github.com/JiahaoWuGit/BGC.",
      "tldr_zh": "这篇论文探讨了图神经网络（GNNs）的图压缩技术中的安全隐患，提出了一种名为 BGC 的后门攻击方法，以在保持压缩图质量的同时实现高攻击成功率。BGC 通过在压缩过程中持续更新触发器并针对代表性节点进行投毒，确保触发器在整个流程中有效。实验结果显示，BGC 在各种场景下攻击成功率接近 1.0，同时维持了良好的模型效用，并能抵抗多种防御策略。作者还分析了影响攻击性能的关键超参数，并提供了开源代码。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "ICDE 2025 Camera Ready",
      "pdf_url": "http://arxiv.org/pdf/2407.11025v4",
      "published_date": "2024-07-03 06:58:29 UTC",
      "updated_date": "2025-03-31 14:19:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:27:29.271821"
    },
    {
      "arxiv_id": "2407.02842v1",
      "title": "MindBench: A Comprehensive Benchmark for Mind Map Structure Recognition and Analysis",
      "title_zh": "MindBench：思维导图结构识别和分析的全面基准测试",
      "authors": [
        "Lei Chen",
        "Feng Yan",
        "Yujie Zhong",
        "Shaoxiang Chen",
        "Zequn Jie",
        "Lin Ma"
      ],
      "abstract": "Multimodal Large Language Models (MLLM) have made significant progress in the\nfield of document analysis. Despite this, existing benchmarks typically focus\nonly on extracting text and simple layout information, neglecting the complex\ninteractions between elements in structured documents such as mind maps and\nflowcharts. To address this issue, we introduce the new benchmark named\nMindBench, which not only includes meticulously constructed bilingual authentic\nor synthetic images, detailed annotations, evaluation metrics and baseline\nmodels, but also specifically designs five types of structured understanding\nand parsing tasks. These tasks include full parsing, partial parsing,\nposition-related parsing, structured Visual Question Answering (VQA), and\nposition-related VQA, covering key areas such as text recognition, spatial\nawareness, relationship discernment, and structured parsing. Extensive\nexperimental results demonstrate the substantial potential and significant room\nfor improvement in current models' ability to handle structured document\ninformation. We anticipate that the launch of MindBench will significantly\nadvance research and application development in structured document analysis\ntechnology. MindBench is available at:\nhttps://miasanlei.github.io/MindBench.github.io/.",
      "tldr_zh": "这篇论文引入了 MindBench，这是一个全面的基准，用于思维导图结构识别和分析，旨在解决 Multimodal Large Language Models (MLLM) 在处理结构化文档（如思维导图和流程图）中的复杂交互问题。MindBench 包括精心构建的双语真实或合成图像、详细注释、评估指标和基线模型，并设计了五种任务：全解析、部分解析、位置相关解析、结构化 Visual Question Answering (VQA) 和位置相关 VQA，这些任务覆盖文本识别、空间意识、关系辨识和结构化解析。实验结果显示当前模型在处理结构化文档信息方面具有巨大潜力，但也存在显著改进空间，预计 MindBench 的发布将显著推进结构化文档分析技术的研究和应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "technical report",
      "pdf_url": "http://arxiv.org/pdf/2407.02842v1",
      "published_date": "2024-07-03 06:39:18 UTC",
      "updated_date": "2024-07-03 06:39:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:27:36.005675"
    },
    {
      "arxiv_id": "2407.02839v1",
      "title": "CRUISE on Quantum Computing for Feature Selection in Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Jiayang Niu",
        "Jie Li",
        "Ke Deng",
        "Yongli Ren"
      ],
      "abstract": "Using Quantum Computers to solve problems in Recommender Systems that\nclassical computers cannot address is a worthwhile research topic. In this\npaper, we use Quantum Annealers to address the feature selection problem in\nrecommendation algorithms. This feature selection problem is a Quadratic\nUnconstrained Binary Optimization(QUBO) problem. By incorporating\nCounterfactual Analysis, we significantly improve the performance of the\nitem-based KNN recommendation algorithm compared to using pure Mutual\nInformation. Extensive experiments have demonstrated that the use of\nCounterfactual Analysis holds great promise for addressing such problems.",
      "tldr_zh": "本研究探讨了利用量子计算解决推荐系统中的特征选择问题，提出CRUISE框架，使用Quantum Annealers将问题建模为Quadratic Unconstrained Binary Optimization (QUBO)。通过整合Counterfactual Analysis，该方法显著提升了item-based KNN推荐算法的性能，与单纯使用Mutual Information相比表现出色。实验结果表明，Counterfactual Analysis在处理此类问题方面具有巨大潜力。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "accepted by QuantumCLEF 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.02839v1",
      "published_date": "2024-07-03 06:34:56 UTC",
      "updated_date": "2024-07-03 06:34:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:27:43.835934"
    },
    {
      "arxiv_id": "2407.02821v1",
      "title": "Effect of a Process Mining based Pre-processing Step in Prediction of the Critical Health Outcomes",
      "title_zh": "基于过程挖掘的预处理步骤在关键健康结果预测中的影响",
      "authors": [
        "Negin Ashrafi",
        "Armin Abdollahi",
        "Greg Placencia",
        "Maryam Pishgar"
      ],
      "abstract": "Predicting critical health outcomes such as patient mortality and hospital\nreadmission is essential for improving survivability. However, healthcare\ndatasets have many concurrences that create complexities, leading to poor\npredictions. Consequently, pre-processing the data is crucial to improve its\nquality. In this study, we use an existing pre-processing algorithm,\nconcatenation, to improve data quality by decreasing the complexity of\ndatasets. Sixteen healthcare datasets were extracted from two databases - MIMIC\nIII and University of Illinois Hospital - converted to the event logs, they\nwere then fed into the concatenation algorithm. The pre-processed event logs\nwere then fed to the Split Miner (SM) algorithm to produce a process model.\nProcess model quality was evaluated before and after concatenation using the\nfollowing metrics: fitness, precision, F-Measure, and complexity. The\npre-processed event logs were also used as inputs to the Decay Replay Mining\n(DREAM) algorithm to predict critical outcomes. We compared predicted results\nbefore and after applying the concatenation algorithm using Area Under the\nCurve (AUC) and Confidence Intervals (CI). Results indicated that the\nconcatenation algorithm improved the quality of the process models and\npredictions of the critical health outcomes.",
      "tldr_zh": "这篇论文探讨了基于Process Mining的预处理步骤如何提升关键健康结果的预测准确性，如患者死亡率和再入院率。研究者使用concatenation算法对16个医疗数据集（来自MIMIC III和University of Illinois Hospital）进行预处理，以减少数据复杂性，并生成过程模型。随后，通过Split Miner (SM)算法评估模型质量（包括fitness, precision, F-Measure和complexity），并利用Decay Replay Mining (DREAM)算法进行预测。结果表明，预处理后过程模型质量和预测性能（如AUC和CI指标）均得到显著改善。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02821v1",
      "published_date": "2024-07-03 05:45:09 UTC",
      "updated_date": "2024-07-03 05:45:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:27:57.797601"
    },
    {
      "arxiv_id": "2407.02814v2",
      "title": "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective",
      "title_zh": "图像胜过文字：从因果中介视角理解和缓解视觉语言模型中的偏差",
      "authors": [
        "Zhaotian Weng",
        "Zijun Gao",
        "Jerone Andrews",
        "Jieyu Zhao"
      ],
      "abstract": "Vision-language models (VLMs) pre-trained on extensive datasets can\ninadvertently learn biases by correlating gender information with specific\nobjects or scenarios. Current methods, which focus on modifying inputs and\nmonitoring changes in the model's output probability scores, often struggle to\ncomprehensively understand bias from the perspective of model components. We\npropose a framework that incorporates causal mediation analysis to measure and\nmap the pathways of bias generation and propagation within VLMs. This approach\nallows us to identify the direct effects of interventions on model bias and the\nindirect effects of interventions on bias mediated through different model\ncomponents. Our results show that image features are the primary contributors\nto bias, with significantly higher impacts than text features, specifically\naccounting for 32.57% and 12.63% of the bias in the MSCOCO and PASCAL-SENTENCE\ndatasets, respectively. Notably, the image encoder's contribution surpasses\nthat of the text encoder and the deep fusion encoder. Further experimentation\nconfirms that contributions from both language and vision modalities are\naligned and non-conflicting. Consequently, focusing on blurring gender\nrepresentations within the image encoder, which contributes most to the model\nbias, reduces bias efficiently by 22.03% and 9.04% in the MSCOCO and\nPASCAL-SENTENCE datasets, respectively, with minimal performance loss or\nincreased computational demands.",
      "tldr_zh": "该研究从因果中介分析（causal mediation analysis）的角度，探讨了视觉语言模型（VLMs）中偏见的问题，特别是性别信息与特定对象或场景的关联。论文提出一个框架，通过测量偏见在模型组件中的生成和传播路径，发现图像特征是主要贡献者，在MSCOCO和PASCAL-SENTENCE数据集上分别占32.57%和12.63%，且图像编码器（image encoder）的贡献远超文本编码器（text encoder）和深度融合编码器（deep fusion encoder）。实验结果表明，语言和视觉模式的贡献是协调的，通过针对图像编码器模糊性别表示，可有效减少偏见（MSCOCO减少22.03%、PASCAL-SENTENCE减少9.04%），同时保持最小性能损失和计算开销。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02814v2",
      "published_date": "2024-07-03 05:19:45 UTC",
      "updated_date": "2024-10-07 22:38:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:28:10.136806"
    },
    {
      "arxiv_id": "2407.02813v2",
      "title": "Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm and Compiler Co-Design",
      "title_zh": "翻译失败",
      "authors": [
        "Gen Li",
        "Zhihao Shu",
        "Jie Ji",
        "Minghai Qin",
        "Fatemeh Afghah",
        "Wei Niu",
        "Xiaolong Ma"
      ],
      "abstract": "Deep neural networks (DNNs) are frequently employed in a variety of computer\nvision applications. Nowadays, an emerging trend in the current video\ndistribution system is to take advantage of DNN's overfitting properties to\nperform video resolution upscaling. By splitting videos into chunks and\napplying a super-resolution (SR) model to overfit each chunk, this scheme of SR\nmodels plus video chunks is able to replace traditional video transmission to\nenhance video quality and transmission efficiency. However, many models and\nchunks are needed to guarantee high performance, which leads to tremendous\noverhead on model switching and memory footprints at the user end. To resolve\nsuch problems, we propose a Dynamic Deep neural network assisted by a\nContent-Aware data processing pipeline to reduce the model number down to one\n(Dy-DCA), which helps promote performance while conserving computational\nresources. Additionally, to achieve real acceleration on the user end, we\ndesigned a framework that optimizes dynamic features (e.g., dynamic shapes,\nsizes, and control flow) in Dy-DCA to enable a series of compilation\noptimizations, including fused code generation, static execution planning, etc.\nBy employing such techniques, our method achieves better PSNR and real-time\nperformance (33 FPS) on an off-the-shelf mobile phone. Meanwhile, assisted by\nour compilation optimization, we achieve a 1.7$\\times$ speedup while saving up\nto 1.61$\\times$ memory consumption. Code available in\nhttps://github.com/coulsonlee/Dy-DCA-ECCV2024.",
      "tldr_zh": "该研究探讨了利用深度神经网络(DNNs)的过拟合特性来提升视频分辨率，但传统方法因需要多个模型和视频块而导致模型切换及内存开销过大。作者提出Dy-DCA框架，该框架结合动态DNN和内容感知数据处理管道，将模型数量减少至一个，同时通过编译器优化（如融合代码生成和静态执行规划）处理动态特征，以提高性能并节省资源。在实验中，Dy-DCA在手机上实现了更好的PSNR指标、实时性能（33 FPS），并实现了1.7倍加速和1.61倍内存节省，展示了其在设备端超分辨率的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV2024",
      "pdf_url": "http://arxiv.org/pdf/2407.02813v2",
      "published_date": "2024-07-03 05:17:26 UTC",
      "updated_date": "2024-07-12 03:39:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:28:22.441647"
    },
    {
      "arxiv_id": "2407.02805v1",
      "title": "Efficient DNN-Powered Software with Fair Sparse Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xuanqi Gao",
        "Weipeng Jiang",
        "Juan Zhai",
        "Shiqing Ma",
        "Xiaoyu Zhang",
        "Chao Shen"
      ],
      "abstract": "With the emergence of the Software 3.0 era, there is a growing trend of\ncompressing and integrating large models into software systems, with\nsignificant societal implications. Regrettably, in numerous instances, model\ncompression techniques impact the fairness performance of these models and thus\nthe ethical behavior of DNN-powered software. One of the most notable example\nis the Lottery Ticket Hypothesis (LTH), a prevailing model pruning approach.\nThis paper demonstrates that fairness issue of LTHbased pruning arises from\nboth its subnetwork selection and training procedures, highlighting the\ninadequacy of existing remedies. To address this, we propose a novel pruning\nframework, Ballot, which employs a novel conflict-detection-based subnetwork\nselection to find accurate and fair subnetworks, coupled with a refined\ntraining process to attain a high-performance model, thereby improving the\nfairness of DNN-powered software. By means of this procedure, Ballot improves\nthe fairness of pruning by 38.00%, 33.91%, 17.96%, and 35.82% compared to\nstate-of-the-art baselines, namely Magnitude Pruning, Standard LTH,\nSafeCompress, and FairScratch respectively, based on our evaluation of five\npopular datasets and three widely used models. Our code is available at\nhttps://anonymous.4open.science/r/Ballot-506E.",
      "tldr_zh": "在 Software 3.0 时代，模型压缩技术如 Lottery Ticket Hypothesis (LTH) 常导致 DNN 驱动软件的公平性下降，论文分析了 LTH 在子网络选择和训练过程中的问题，并指出现有解决方案的不足。针对此，研究提出了一种新型框架 Ballot，它采用基于冲突检测的子网络选择方法结合精炼训练过程，以实现高准确性和公平性。实验结果显示，在五个流行数据集和三个常用模型上，Ballot 分别比 Magnitude Pruning、Standard LTH、SafeCompress 和 FairScratch 等基线方法提高了 38.00%、33.91%、17.96% 和 35.82% 的公平性，从而提升了 DNN 驱动软件的伦理性能。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02805v1",
      "published_date": "2024-07-03 04:52:28 UTC",
      "updated_date": "2024-07-03 04:52:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:28:35.333835"
    },
    {
      "arxiv_id": "2407.02791v1",
      "title": "Model-Enhanced LLM-Driven VUI Testing of VPA Apps",
      "title_zh": "翻译失败",
      "authors": [
        "Suwan Li",
        "Lei Bu",
        "Guangdong Bai",
        "Fuman Xie",
        "Kai Chen",
        "Chang Yue"
      ],
      "abstract": "The flourishing ecosystem centered around voice personal assistants (VPA),\nsuch as Amazon Alexa, has led to the booming of VPA apps. The largest app\nmarket Amazon skills store, for example, hosts over 200,000 apps. Despite their\npopularity, the open nature of app release and the easy accessibility of apps\nalso raise significant concerns regarding security, privacy and quality.\nConsequently, various testing approaches have been proposed to systematically\nexamine VPA app behaviors. To tackle the inherent lack of a visible user\ninterface in the VPA app, two strategies are employed during testing, i.e.,\nchatbot-style testing and model-based testing. The former often lacks effective\nguidance for expanding its search space, while the latter falls short in\ninterpreting the semantics of conversations to construct precise and\ncomprehensive behavior models for apps. In this work, we introduce Elevate, a\nmodel-enhanced large language model (LLM)-driven VUI testing framework. Elevate\nleverages LLMs' strong capability in natural language processing to compensate\nfor semantic information loss during model-based VUI testing. It operates by\nprompting LLMs to extract states from VPA apps' outputs and generate\ncontext-related inputs. During the automatic interactions with the app, it\nincrementally constructs the behavior model, which facilitates the LLM in\ngenerating inputs that are highly likely to discover new states. Elevate\nbridges the LLM and the behavior model with innovative techniques such as\nencoding behavior model into prompts and selecting LLM-generated inputs based\non the context relevance. Elevate is benchmarked on 4,000 real-world Alexa\nskills, against the state-of-the-art tester Vitas. It achieves 15% higher state\nspace coverage compared to Vitas on all types of apps, and exhibits significant\nadvancement in efficiency.",
      "tldr_zh": "该研究针对语音个人助理 (VPA) 应用的测试问题，提出了一种 model-enhanced large language model (LLM)-driven VUI 测试框架，名为 Elevate，以解决现有 chatbot-style testing 和 model-based testing 的局限性。Elevate 利用 LLMs 的自然语言处理能力，从 VPA 应用的输出中提取状态并生成相关输入，同时通过逐步构建行为模型来指导测试过程，并采用创新技术如将行为模型编码进提示中以提升输入的相关性。在对 4,000 个真实世界 Alexa 技能的基准测试中，Elevate 比 state-of-the-art 测试器 Vitas 实现了 15% 更高的状态空间覆盖率，并显著提高了测试效率。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "13 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.02791v1",
      "published_date": "2024-07-03 03:36:05 UTC",
      "updated_date": "2024-07-03 03:36:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:28:46.761781"
    },
    {
      "arxiv_id": "2407.15026v2",
      "title": "Benchmarking End-To-End Performance of AI-Based Chip Placement Algorithms",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihai Wang",
        "Zijie Geng",
        "Zhaojie Tu",
        "Jie Wang",
        "Yuxi Qian",
        "Zhexuan Xu",
        "Ziyan Liu",
        "Siyuan Xu",
        "Zhentao Tang",
        "Shixiong Kai",
        "Mingxuan Yuan",
        "Jianye Hao",
        "Bin Li",
        "Yongdong Zhang",
        "Feng Wu"
      ],
      "abstract": "The increasing complexity of modern very-large-scale integration (VLSI)\ndesign highlights the significance of Electronic Design Automation (EDA)\ntechnologies. Chip placement is a critical step in the EDA workflow, which\npositions chip modules on the canvas with the goal of optimizing performance,\npower, and area (PPA) metrics of final chip designs. Recent advances have\ndemonstrated the great potential of AI-based algorithms in enhancing chip\nplacement. However, due to the lengthy workflow of chip design, the evaluations\nof these algorithms often focus on intermediate surrogate metrics, which are\neasy to compute but frequently reveal a substantial misalignment with the\nend-to-end performance (i.e., the final design PPA). To address this challenge,\nwe introduce ChiPBench, which can effectively facilitate research in chip\nplacement within the AI community. ChiPBench is a comprehensive benchmark\nspecifically designed to evaluate the effectiveness of existing AI-based chip\nplacement algorithms in improving final design PPA metrics. Specifically, we\nhave gathered 20 circuits from various domains (e.g., CPU, GPU, and\nmicrocontrollers). These designs are compiled by executing the workflow from\nthe verilog source code, which preserves necessary physical implementation\nkits, enabling evaluations for the placement algorithms on their impacts on the\nfinal design PPA. We executed six state-of-the-art AI-based chip placement\nalgorithms on these designs and plugged the results of each single-point\nalgorithm into the physical implementation workflow to obtain the final PPA\nresults. Experimental results show that even if intermediate metric of a\nsingle-point algorithm is dominant, while the final PPA results are\nunsatisfactory. We believe that our benchmark will serve as an effective\nevaluation framework to bridge the gap between academia and industry.",
      "tldr_zh": "该研究针对AI-based芯片放置算法在VLSI设计中的评估问题，指出现有方法往往依赖中间代理指标，而这些指标与最终设计性能、功耗和面积（PPA）指标存在显著脱节。论文引入ChiPBench基准，这是一个全面框架，用于评估AI-based算法对最终PPA的影响，涵盖20个来自CPU、GPU和微控制器等领域的电路，并从Verilog源代码编译以保留物理实现工具。作者运行了六种最先进算法，并将结果整合到物理实现流程中进行端到端测试，结果显示即使算法在中间指标上领先，最终PPA表现可能不尽人意，从而桥接了学术界与工业界的差距。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "A comprehensive benchmark for AI-based chip placement algorithms\n  using end-to-end performance metrics",
      "pdf_url": "http://arxiv.org/pdf/2407.15026v2",
      "published_date": "2024-07-03 03:29:23 UTC",
      "updated_date": "2024-12-06 06:02:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:28:59.054302"
    },
    {
      "arxiv_id": "2407.02783v1",
      "title": "52B to 1T: Lessons Learned via Tele-FLM Series",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Li",
        "Yiqun Yao",
        "Xin Jiang",
        "Xuezhi Fang",
        "Chao Wang",
        "Xinzhang Liu",
        "Zihan Wang",
        "Yu Zhao",
        "Xin Wang",
        "Yuyao Huang",
        "Shuangyong Song",
        "Yongxiang Li",
        "Zheng Zhang",
        "Bo Zhao",
        "Aixin Sun",
        "Yequan Wang",
        "Zhongjiang He",
        "Zhongyuan Wang",
        "Xuelong Li",
        "Tiejun Huang"
      ],
      "abstract": "Large Language Models (LLMs) represent a significant stride toward Artificial\nGeneral Intelligence. As scaling laws underscore the potential of increasing\nmodel sizes, the academic community has intensified its investigations into\nLLMs with capacities exceeding 50 billion parameters. This technical report\nbuilds on our prior work with Tele-FLM (also known as FLM-2), a publicly\navailable 52-billion-parameter model. We delve into two primary areas: we first\ndiscuss our observation of Supervised Fine-tuning (SFT) on Tele-FLM-52B, which\nsupports the \"less is more\" approach for SFT data construction; second, we\ndemonstrate our experiments and analyses on the best practices for\nprogressively growing a model from 52 billion to 102 billion, and subsequently\nto 1 trillion parameters. We will open-source a 1T model checkpoint, namely\nTele-FLM-1T, to advance further training and research.",
      "tldr_zh": "本研究总结了通过 Tele-FLM 系列模型从 52 亿参数扩展到 1 万亿参数的经验教训，聚焦于 Large Language Models (LLMs) 的规模化发展。作者观察到 Supervised Fine-tuning (SFT) 的数据构建遵循“less is more”原则，即使用更精简的数据可提升模型性能。实验分析了从 Tele-FLM-52B 到 102B 再到 1T 参数的渐进增长最佳实践，包括优化策略和性能评估。最终，研究团队将开源 Tele-FLM-1T 模型检查点，以推动进一步的训练和学术研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "For the Tele-FLM-52B tech report, see also 2404.16645",
      "pdf_url": "http://arxiv.org/pdf/2407.02783v1",
      "published_date": "2024-07-03 03:21:02 UTC",
      "updated_date": "2024-07-03 03:21:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:29:11.038826"
    },
    {
      "arxiv_id": "2407.02779v1",
      "title": "Croppable Knowledge Graph Embedding",
      "title_zh": "可裁剪的知识图谱嵌入",
      "authors": [
        "Yushan Zhu",
        "Wen Zhang",
        "Zhiqiang Liu",
        "Mingyang Chen",
        "Lei Liang",
        "Huajun Chen"
      ],
      "abstract": "Knowledge Graph Embedding (KGE) is a common method for Knowledge Graphs (KGs)\nto serve various artificial intelligence tasks. The suitable dimensions of the\nembeddings depend on the storage and computing conditions of the specific\napplication scenarios. Once a new dimension is required, a new KGE model needs\nto be trained from scratch, which greatly increases the training cost and\nlimits the efficiency and flexibility of KGE in serving various scenarios. In\nthis work, we propose a novel KGE training framework MED, through which we\ncould train once to get a croppable KGE model applicable to multiple scenarios\nwith different dimensional requirements, sub-models of the required dimensions\ncan be cropped out of it and used directly without any additional training. In\nMED, we propose a mutual learning mechanism to improve the low-dimensional\nsub-models performance and make the high-dimensional sub-models retain the\ncapacity that low-dimensional sub-models have, an evolutionary improvement\nmechanism to promote the high-dimensional sub-models to master the knowledge\nthat the low-dimensional sub-models can not learn, and a dynamic loss weight to\nbalance the multiple losses adaptively. Experiments on 3 KGE models over 4\nstandard KG completion datasets, 3 real application scenarios over a real-world\nlarge-scale KG, and the experiments of extending MED to the language model BERT\nshow the effectiveness, high efficiency, and flexible extensibility of MED.",
      "tldr_zh": "本文提出了一种可裁剪的 Knowledge Graph Embedding (KGE) 框架 MED，通过单次训练生成适用于多种维度需求的模型，用户可以直接裁剪出所需维度的子模型，而无需额外训练。MED 引入相互学习机制以提升低维子模型性能并保留高维子模型的低维能力、进化改进机制让高维子模型掌握低维模型无法学习的知识，以及动态损失权重来自适应平衡多个损失。实验在 3 个 KGE 模型、4 个标准 KG 完成数据集和 3 个真实应用场景上，以及扩展到语言模型 BERT，证明了 MED 的有效性、高效性和灵活扩展性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02779v1",
      "published_date": "2024-07-03 03:10:25 UTC",
      "updated_date": "2024-07-03 03:10:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:29:22.688180"
    },
    {
      "arxiv_id": "2407.02765v2",
      "title": "Graphon Particle Systems, Part II: Dynamics of Distributed Stochastic Continuum Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Chen",
        "Tao Li"
      ],
      "abstract": "We study the distributed optimization problem over a graphon with a continuum\nof nodes, which is regarded as the limit of the distributed networked\noptimization as the number of nodes goes to infinity. Each node has a private\nlocal cost function. The global cost function, which all nodes cooperatively\nminimize, is the integral of the local cost functions on the node set. We\npropose stochastic gradient descent and gradient tracking algorithms over the\ngraphon. We establish a general lemma for the upper bound estimation related to\na class of time-varying differential inequalities with negative linear terms,\nbased upon which, we prove that for both kinds of algorithms, the second\nmoments of the nodes' states are uniformly bounded. Especially, for the\nstochastic gradient tracking algorithm, we transform the convergence analysis\ninto the asymptotic property of coupled nonlinear differential inequalities\nwith time-varying coefficients and develop a decoupling method. For both kinds\nof algorithms, we show that by choosing the time-varying algorithm gains\nproperly, all nodes' states achieve $\\mathcal{L}^{\\infty}$-consensus for a\nconnected graphon. Furthermore, if the local cost functions are strongly\nconvex, then all nodes' states converge to the minimizer of the global cost\nfunction and the auxiliary states in the stochastic gradient tracking algorithm\nconverge to the gradient value of the global cost function at the minimizer\nuniformly in mean square.",
      "tldr_zh": "这篇论文研究了在 graphon（一个具有连续节点的图论极限）上进行分布式随机连续优化问题，每个节点拥有私有的局部成本函数，全局目标是最小化这些函数的积分。作者提出了 stochastic gradient descent 和 gradient tracking 算法，并通过分析一类时间变化微分不等式建立了节点状态二阶矩的均匀有界性。对于 gradient tracking 算法，他们使用解耦方法证明了在连通 graphon 上，通过适当选择时间变化算法增益，节点状态可达 $\\mathcal{L}^{\\infty}$-consensus。如果局部成本函数是强凸的，则所有节点状态均方收敛到全局成本函数的最小值点，且辅助状态收敛到该最小值的梯度值。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY",
        "math.OC",
        "math.PR"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02765v2",
      "published_date": "2024-07-03 02:47:39 UTC",
      "updated_date": "2025-02-21 02:16:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:29:35.428762"
    },
    {
      "arxiv_id": "2407.02762v1",
      "title": "SF-GNN: Self Filter for Message Lossless Propagation in Deep Graph Neural Network",
      "title_zh": "SF-GNN：深度图神经网络中无损",
      "authors": [
        "Yushan Zhu",
        "Wen Zhang",
        "Yajing Xu",
        "Zhen Yao",
        "Mingyang Chen",
        "Huajun Chen"
      ],
      "abstract": "Graph Neural Network (GNN), with the main idea of encoding graph structure\ninformation of graphs by propagation and aggregation, has developed rapidly. It\nachieved excellent performance in representation learning of multiple types of\ngraphs such as homogeneous graphs, heterogeneous graphs, and more complex\ngraphs like knowledge graphs. However, merely stacking GNN layers may not\nimprove the model's performance and can even be detrimental. For the phenomenon\nof performance degradation in deep GNNs, we propose a new perspective. Unlike\nthe popular explanations of over-smoothing or over-squashing, we think the\nissue arises from the interference of low-quality node representations during\nmessage propagation. We introduce a simple and general method, SF-GNN, to\naddress this problem. In SF-GNN, we define two representations for each node,\none is the node representation that represents the feature of the node itself,\nand the other is the message representation specifically for propagating\nmessages to neighbor nodes. A self-filter module evaluates the quality of the\nnode representation and decides whether to integrate it into the message\npropagation based on this quality assessment. Experiments on node\nclassification tasks for both homogeneous and heterogeneous graphs, as well as\nlink prediction tasks on knowledge graphs, demonstrate that our method can be\napplied to various GNN models and outperforms state-of-the-art baseline methods\nin addressing deep GNN degradation.",
      "tldr_zh": "该研究探讨了图神经网络（GNN）在深层模型中性能退化的问题，认为主要源于低质量节点表示对消息传播的干扰，而不是常见的过平滑或过挤压现象。作者提出SF-GNN方法，为每个节点定义两种表示：节点表示（代表节点自身特征）和消息表示（用于向邻居节点传播），并引入自过滤模块来评估节点表示质量，决定是否将其整合到消息传播中。该方法在同质图、异质图的节点分类任务以及知识图的链接预测任务上实验验证，适用于多种GNN模型，并显著优于现有最先进基线，实现了消息无损传播和性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02762v1",
      "published_date": "2024-07-03 02:40:39 UTC",
      "updated_date": "2024-07-03 02:40:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:29:47.319747"
    },
    {
      "arxiv_id": "2407.02759v1",
      "title": "Multi-Scenario Combination Based on Multi-Agent Reinforcement Learning to Optimize the Advertising Recommendation System",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Zhao",
        "Chang Zhou",
        "Jin Cao",
        "Yi Zhao",
        "Shaobo Liu",
        "Chiyu Cheng",
        "Xingchen Li"
      ],
      "abstract": "This paper explores multi-scenario optimization on large platforms using\nmulti-agent reinforcement learning (MARL). We address this by treating\nscenarios like search, recommendation, and advertising as a cooperative,\npartially observable multi-agent decision problem. We introduce the Multi-Agent\nRecurrent Deterministic Policy Gradient (MARDPG) algorithm, which aligns\ndifferent scenarios under a shared objective and allows for strategy\ncommunication to boost overall performance. Our results show marked\nimprovements in metrics such as click-through rate (CTR), conversion rate, and\ntotal sales, confirming our method's efficacy in practical settings.",
      "tldr_zh": "这篇论文探讨了使用多智能体强化学习（MARL）优化广告推荐系统的多场景组合，将搜索、推荐和广告等场景视为合作的部分可观察多智能体决策问题。论文引入了Multi-Agent Recurrent Deterministic Policy Gradient (MARDPG)算法，通过共享目标和策略通信来对齐不同场景，提升整体性能。实验结果显示，该方法显著提高了点击率（CTR）、转换率和总销售额等关键指标，证明了其在实际平台环境中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by 2024 5th International Conference on Artificial\n  Intelligence and Electromechanical Automation IEEE (ISBN: 979-8-3503-6617-4)",
      "pdf_url": "http://arxiv.org/pdf/2407.02759v1",
      "published_date": "2024-07-03 02:33:20 UTC",
      "updated_date": "2024-07-03 02:33:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:29:59.363030"
    },
    {
      "arxiv_id": "2407.02751v2",
      "title": "Emotion and Intent Joint Understanding in Multimodal Conversation: A Benchmarking Dataset",
      "title_zh": "多模态对话中的情感和意图联合理解：一个基准数据集",
      "authors": [
        "Rui Liu",
        "Haolin Zuo",
        "Zheng Lian",
        "Xiaofen Xing",
        "Björn W. Schuller",
        "Haizhou Li"
      ],
      "abstract": "Emotion and Intent Joint Understanding in Multimodal Conversation (MC-EIU)\naims to decode the semantic information manifested in a multimodal\nconversational history, while inferring the emotions and intents simultaneously\nfor the current utterance. MC-EIU is enabling technology for many\nhuman-computer interfaces. However, there is a lack of available datasets in\nterms of annotation, modality, language diversity, and accessibility. In this\nwork, we propose an MC-EIU dataset, which features 7 emotion categories, 9\nintent categories, 3 modalities, i.e., textual, acoustic, and visual content,\nand two languages, i.e., English and Mandarin. Furthermore, it is completely\nopen-source for free access. To our knowledge, MC-EIU is the first\ncomprehensive and rich emotion and intent joint understanding dataset for\nmultimodal conversation. Together with the release of the dataset, we also\ndevelop an Emotion and Intent Interaction (EI$^2$) network as a reference\nsystem by modeling the deep correlation between emotion and intent in the\nmultimodal conversation. With comparative experiments and ablation studies, we\ndemonstrate the effectiveness of the proposed EI$^2$ method on the MC-EIU\ndataset. The dataset and codes will be made available at:\nhttps://github.com/MC-EIU/MC-EIU.",
      "tldr_zh": "本研究提出了一种多模态对话情感和意图联合理解基准数据集 MC-EIU，用于同时推断对话历史中的情感和意图。该数据集包含7个情感类别、9个意图类别、3个模态（文本、声学和视觉）以及两种语言（英语和普通话），并完全开源，以填补现有数据集的不足。研究者开发了 Emotion and Intent Interaction (EI²) 网络，通过建模情感和意图之间的深层相关性，实验结果显示该方法在 MC-EIU 数据集上表现出色，证明了其有效性。数据集和代码已在 GitHub 上公开，可供进一步研究使用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 8 figures, 12 tables, NeurIPS 2024 Dataset and Benchmark\n  Track",
      "pdf_url": "http://arxiv.org/pdf/2407.02751v2",
      "published_date": "2024-07-03 01:56:00 UTC",
      "updated_date": "2024-07-04 15:13:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:30:10.391951"
    },
    {
      "arxiv_id": "2407.02742v1",
      "title": "A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Nastaran Bassamzadeh",
        "Chhaya Methani"
      ],
      "abstract": "Natural Language to Code Generation has made significant progress in recent\nyears with the advent of Large Language Models(LLMs). While generation for\ngeneral-purpose languages like C, C++, and Python has improved significantly,\nLLMs struggle with custom function names in Domain Specific Languages or DSLs.\nThis leads to higher hallucination rates and syntax errors, specially for DSLs\nhaving a high number of custom function names. Additionally, constant updates\nto function names add to the challenge as LLMs need to stay up-to-date. In this\npaper, we present optimizations for using Retrieval Augmented Generation (or\nRAG) with LLMs for DSL generation along with an ablation study comparing these\nstrategies. We generated a train as well as test dataset with a DSL to\nrepresent automation tasks across roughly 700 APIs in public domain. We used\nthe training dataset to fine-tune a Codex model for this DSL. Our results\nshowed that the fine-tuned model scored the best on code similarity metric.\nWith our RAG optimizations, we achieved parity for similarity metric. The\ncompilation rate, however, showed that both the models still got the syntax\nwrong many times, with RAG-based method being 2 pts better. Conversely,\nhallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for\nAPI parameter keys. We conclude that an optimized RAG model can match the\nquality of fine-tuned models and offer advantages for new, unseen APIs.",
      "tldr_zh": "该研究比较了针对 Domain Specific Languages (DSLs) 代码生成的两种方法：fine-tuning 和优化后的 Retrieval Augmented Generation (RAG)。论文通过生成一个包含约700个公共API的DSL数据集，对Codex模型进行fine-tuning，并优化RAG策略进行消融研究，结果显示fine-tuned模型在代码相似性指标上表现最佳，而RAG优化后在相似性上达到平价。实验还发现RAG在编译率上高出2个百分点，但在API名称和参数键的幻觉率上略逊一筹。总体结论是，优化RAG能与fine-tuning匹敌，并为处理新API提供更大灵活性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "I.2.2; I.2.7"
      ],
      "primary_category": "cs.SE",
      "comment": "8 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2407.02742v1",
      "published_date": "2024-07-03 01:28:51 UTC",
      "updated_date": "2024-07-03 01:28:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:30:24.129091"
    },
    {
      "arxiv_id": "2407.02731v1",
      "title": "Artificial intelligence and machine learning generated conjectures with TxGraffiti",
      "title_zh": "翻译失败",
      "authors": [
        "Randy Davila"
      ],
      "abstract": "\\emph{TxGraffiti} is a machine learning and heuristic based artificial\nintelligence designed to automate the task of conjecturing in mathematics.\nSince its inception, TxGraffiti has generated many surprising conjectures\nleading to publication in respectable mathematical journals. In this paper we\noutline the machine learning and heuristic techniques implemented by\nTxGraffiti. We also recall its contributions to the mathematical literature and\nannounce a new online version of the program available for anyone curious to\nexplore conjectures in graph theory.",
      "tldr_zh": "这篇论文介绍了 TxGraffiti，一种基于 artificial intelligence 和 machine learning 的系统，旨在自动化数学猜想的生成过程。系统结合 machine learning 和 heuristic 技术，已经产生了多个引人注目的猜想，并促成了在知名数学期刊上的发表。论文回顾了 TxGraffiti 的技术贡献，并宣布了一个新的在线版本，供用户探索图论领域的猜想。",
      "categories": [
        "cs.AI",
        "math.CO"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: text overlap with arXiv:2306.12917",
      "pdf_url": "http://arxiv.org/pdf/2407.02731v1",
      "published_date": "2024-07-03 01:03:09 UTC",
      "updated_date": "2024-07-03 01:03:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:30:35.918882"
    },
    {
      "arxiv_id": "2407.02730v1",
      "title": "MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context",
      "title_zh": "翻译失败",
      "authors": [
        "Zishan Gu",
        "Changchang Yin",
        "Fenglin Liu",
        "Ping Zhang"
      ],
      "abstract": "Large Vision Language Models (LVLMs) have recently achieved superior\nperformance in various tasks on natural image and text data, which inspires a\nlarge amount of studies for LVLMs fine-tuning and training. Despite their\nadvancements, there has been scant research on the robustness of these models\nagainst hallucination when fine-tuned on smaller datasets. In this study, we\nintroduce a new benchmark dataset, the Medical Visual Hallucination Test\n(MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH\ncomprises five tasks to evaluate hallucinations in LVLMs within the medical\ncontext, which includes tasks for comprehensive understanding of textual and\nvisual input, as well as long textual response generation. Our extensive\nexperiments with both general and medical LVLMs reveal that, although medical\nLVLMs demonstrate promising performance on standard medical tasks, they are\nparticularly susceptible to hallucinations, often more so than the general\nmodels, raising significant concerns about the reliability of these\ndomain-specific models. For medical LVLMs to be truly valuable in real-world\napplications, they must not only accurately integrate medical knowledge but\nalso maintain robust reasoning abilities to prevent hallucination. Our work\npaves the way for future evaluations of these studies.",
      "tldr_zh": "本文提出MedVH基准数据集，旨在系统评估Large Vision Language Models (LVLMs)在医疗背景下的hallucination问题，以填补现有研究的空白。MedVH包括五个任务，涵盖文本和视觉输入的全面理解以及长文本响应生成，通过实验测试了通用和医疗领域的LVLMs。结果显示，医疗LVLMs在标准任务上表现出色，但比通用模型更容易出现hallucination，这引发了对这些模型可靠性的担忧。未来，该工作将为提升LVLMs的稳健推理能力和医疗知识整合提供重要指导。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02730v1",
      "published_date": "2024-07-03 00:59:03 UTC",
      "updated_date": "2024-07-03 00:59:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T03:30:47.977895"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 113,
  "processed_papers_count": 113,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T03:31:11.405530"
}
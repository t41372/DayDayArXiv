{
  "date": "2024-09-18",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-18 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦 AI 安全、图像生成、医疗图像分析和强化学习等领域，强调模型鲁棒性与实际应用，令人印象深刻的包括 Qwen2-VL（视觉语言模型的创新）和 Isaac Kohane 的 LLM 对齐研究，以及 SpoofCeleb 数据集的语音深度伪造检测。\n\n下面，我挑选并简要讨论几篇重要或有话题度的论文，先从 AI 安全和 LLM 相关主题入手，再聊医疗与图像生成领域。其他论文较多，我会快速掠过不那么核心的，以保持简洁。\n\n### AI 安全与 LLM 相关\n- **Extracting Memorized Training Data via Decomposition（提取训练数据通过分解）**：这篇论文展示了如何通过查询分解方法从大型语言模型（LLMs）中提取训练数据，如 New York Times 文章，揭示了模型数据泄露风险。主要贡献是提出简单、可扩展的提取技术，证明了现有对齐方法无法完全防止泄露，强调了隐私漏洞的潜在影响。\n- **Understanding Implosion in Text-to-Image Generative Models（理解文本到图像生成模型中的崩溃）**：作者 Wenxin Ding 等分析了扩散模型中的中毒攻击，引入 Alignment Difficulty（AD）指标来量化攻击影响。主要发现是，攻击会使模型输出无意义图像，并通过图对齐理论解释了这一现象，提供工具用于研究模型鲁棒性。\n- **Reward-Robust RLHF in LLMs（LLMs 中的奖励鲁棒强化学习从人类反馈）**：作者 Yuzi Yan 等提出奖励鲁棒框架，使用 Bayesian Reward Model Ensembles 优化 LLM 对齐。主要贡献是平衡性能与鲁棒性，实验显示在多个基准上提升了准确性和稳定性，解决传统 RLHF 的不稳定性问题。\n- **To CoT or not to CoT?（是否使用思维链）**：作者 Zayne Sprague 等评估思维链（CoT）在 LLM 中的作用，发现其主要在数学和符号推理任务中有效。主要发现是，CoT 在其他任务中收益有限，并建议选择性应用以节省计算成本。\n\n这些论文突出了 LLM 安全和解释性的热点，Isaac Kohane 的工作尤其值得关注，因为它探讨了 LLM 在医疗决策中的偏好对齐。\n\n### 医疗图像分析\n- **Advancing Cucumber Disease Detection in Agriculture through Machine Vision and Drone Technology（通过机器视觉和无人机技术推进黄瓜病害检测）**：作者 Syada Tasfia Rahman 等使用无人机采集高分辨率图像，构建数据集并实现87.5%的准确率检测八种黄瓜病害。主要贡献是结合数据增强和无人机技术，提高了农业病害早期诊断效率。\n- **Axial Attention Transformer Networks: A New Frontier in Breast Cancer Detection（轴向注意力 Transformer 网络：乳腺癌检测的新前沿）**：作者 Weijie He 等提出基于 Transformer 的分割模型，使用轴向注意力机制提升小病变检测。主要发现是，该模型改进了 CNN 的全局上下文捕捉，提升了乳腺癌图像分割准确性。\n- **Deep vessel segmentation with joint multi-prior encoding（使用联合多先验编码的深度血管分割）**：作者 Amine Sadikine 等开发了整合形状和拓扑先验的分割方法，在 3D-IRCADb 数据集上表现突出。主要贡献是提高血管分割的解剖一致性，适用于临床路径检测。\n\n这些医疗论文实用性强，强调 AI 在农业和癌症诊断中的潜力，但其他类似主题（如更多分割方法）我快速掠过，仅提核心术语如“拓扑先验”。\n\n### 图像生成与处理\n- **Qwen2-VL: Enhancing Vision-Language Model's Perception（增强视觉语言模型的感知能力）**：作者 Peng Wang 等提出 Qwen2-VL 系列模型，使用 Naive Dynamic Resolution 和 Multimodal Rotary Position Embedding 处理任意分辨率图像。主要发现是，该模型在多模态基准上媲美 GPT-4o，扩展了视觉语言模型的适用性。\n- **Vista3D: Unravel the 3D Darkside of a Single Image（揭示单图像的 3D 隐藏面）**：作者 Qiuhong Shen 等开发了两阶段框架，快速生成 3D 对象。主要贡献是结合 Gaussian Splatting 和 SDF 优化，实现 5 分钟内的高质量 3D 生成，提升了图像到 3D 的转换效率。\n- **DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control（领域内动态预训练用于视觉运动控制）**：作者 Zichen Jeff Cui 等提出自监督方法，学习图像序列动态以提升机器人模仿学习。主要发现是，在模拟和真实环境中显著提高任务成功率，适用于机器人操作。\n\n图像生成论文创新性高，Qwen2-VL 尤其有话题度，因为它挑战了现有视觉模型的局限。\n\n其他论文如数据集构建（SpoofCeleb）、理论分析（Geometric Relational Embeddings）和强化学习（RAG-Modulo）也有亮点，但由于篇幅有限，我仅快速提到：SpoofCeleb 贡献了语音深度伪造数据集，提升了检测基准；RAG-Modulo 通过记忆机制改善多代理任务学习；其余如纯理论或小规模实验的论文（如一些会议短文）就不再详述。\n\n总之，今天的 arXiv 论文展示了 AI 领域的多样创新，AI 安全和医疗应用是重点，建议读者关注 Qwen2-VL 和相关 LLM 研究以跟进前沿进展。更多细节请查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2409.12367v2",
      "title": "Extracting Memorized Training Data via Decomposition",
      "title_zh": "通过分解提取记忆训练数据",
      "authors": [
        "Ellen Su",
        "Anu Vellore",
        "Amy Chang",
        "Raffaele Mura",
        "Blaine Nelson",
        "Paul Kassianik",
        "Amin Karbasi"
      ],
      "abstract": "The widespread use of Large Language Models (LLMs) in society creates new\ninformation security challenges for developers, organizations, and end-users\nalike. LLMs are trained on large volumes of data, and their susceptibility to\nreveal the exact contents of the source training datasets poses security and\nsafety risks. Although current alignment procedures restrict common risky\nbehaviors, they do not completely prevent LLMs from leaking data. Prior work\ndemonstrated that LLMs may be tricked into divulging training data by using\nout-of-distribution queries or adversarial techniques. In this paper, we\ndemonstrate a simple, query-based decompositional method to extract news\narticles from two frontier LLMs. We use instruction decomposition techniques to\nincrementally extract fragments of training data. Out of 3723 New York Times\narticles, we extract at least one verbatim sentence from 73 articles, and over\n20% of verbatim sentences from 6 articles. Our analysis demonstrates that this\nmethod successfully induces the LLM to generate texts that are reliable\nreproductions of news articles, meaning that they likely originate from the\nsource training dataset. This method is simple, generalizable, and does not\nfine-tune or change the production model. If replicable at scale, this training\ndata extraction methodology could expose new LLM security and safety\nvulnerabilities, including privacy risks and unauthorized data leaks. These\nimplications require careful consideration from model development to its\nend-use.",
      "tldr_zh": "本文提出了一种基于分解（decompositional）的查询方法，用于从大型语言模型（LLMs）中提取记忆训练数据，以揭示其潜在安全风险。该方法利用instruction decomposition技术，通过逐步查询逐步提取训练数据片段，在3723篇纽约时报文章中成功提取了73篇的至少一个逐字句，并从6篇中获取超过20%的逐字内容。实验结果表明，这种简单且无需微调模型的通用方法可能放大LLMs的隐私泄露和未授权数据风险，强调了在模型开发和应用中需加强安全措施。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12367v2",
      "published_date": "2024-09-18 23:59:32 UTC",
      "updated_date": "2024-10-01 21:34:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:01:24.450807"
    },
    {
      "arxiv_id": "2409.17285v2",
      "title": "SpoofCeleb: Speech Deepfake Detection and SASV In The Wild",
      "title_zh": "翻译失败",
      "authors": [
        "Jee-weon Jung",
        "Yihan Wu",
        "Xin Wang",
        "Ji-Hoon Kim",
        "Soumi Maiti",
        "Yuta Matsunaga",
        "Hye-jin Shim",
        "Jinchuan Tian",
        "Nicholas Evans",
        "Joon Son Chung",
        "Wangyou Zhang",
        "Seyun Um",
        "Shinnosuke Takamichi",
        "Shinji Watanabe"
      ],
      "abstract": "This paper introduces SpoofCeleb, a dataset designed for Speech Deepfake\nDetection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV),\nutilizing source data from real-world conditions and spoofing attacks generated\nby Text-To-Speech (TTS) systems also trained on the same real-world data.\nRobust recognition systems require speech data recorded in varied acoustic\nenvironments with different levels of noise to be trained. However, current\ndatasets typically include clean, high-quality recordings (bona fide data) due\nto the requirements for TTS training; studio-quality or well-recorded read\nspeech is typically necessary to train TTS models. Current SDD datasets also\nhave limited usefulness for training SASV models due to insufficient speaker\ndiversity. SpoofCeleb leverages a fully automated pipeline we developed that\nprocesses the VoxCeleb1 dataset, transforming it into a suitable form for TTS\ntraining. We subsequently train 23 contemporary TTS systems. SpoofCeleb\ncomprises over 2.5 million utterances from 1,251 unique speakers, collected\nunder natural, real-world conditions. The dataset includes carefully\npartitioned training, validation, and evaluation sets with well-controlled\nexperimental protocols. We present the baseline results for both SDD and SASV\ntasks. All data, protocols, and baselines are publicly available at\nhttps://jungjee.github.io/spoofceleb.",
      "tldr_zh": "本研究引入了SpoofCeleb数据集，用于Speech Deepfake Detection (SDD)和Spoofing-robust Automatic Speaker Verification (SASV)，这些数据基于真实世界条件下的录音和由Text-To-Speech (TTS)系统生成的伪造攻击，以提升模型的鲁棒性。不同于现有数据集的清洁高质量录音，SpoofCeleb通过一个自动化管道处理VoxCeleb1数据集，训练了23个当代TTS系统，从而生成更真实、多样化的语音样本，包括超过250万条语音片段和1251个独特说话者。数据集提供了精心划分的训练、验证和评估集，并附带SDD和SASV任务的基线结果，所有资源均在https://jungjee.github.io/spoofceleb公开可用，这有助于推动真实环境下的语音安全研究。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "IEEE OJSP. Official document lives at:\n  https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10839331",
      "pdf_url": "http://arxiv.org/pdf/2409.17285v2",
      "published_date": "2024-09-18 23:17:02 UTC",
      "updated_date": "2025-04-15 17:53:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:01:36.604564"
    },
    {
      "arxiv_id": "2409.12350v1",
      "title": "Advancing Cucumber Disease Detection in Agriculture through Machine Vision and Drone Technology",
      "title_zh": "通过机器视觉和无人机技术推进农业中的黄瓜疾病检测",
      "authors": [
        "Syada Tasfia Rahman",
        "Nishat Vasker",
        "Amir Khabbab Ahammed",
        "Mahamudul Hasan"
      ],
      "abstract": "This study uses machine vision and drone technologies to propose a unique\nmethod for the diagnosis of cucumber disease in agriculture. The backbone of\nthis research is a painstakingly curated dataset of hyperspectral photographs\nacquired under genuine field conditions. Unlike earlier datasets, this study\nincluded a wide variety of illness types, allowing for precise early-stage\ndetection. The model achieves an excellent 87.5\\% accuracy in distinguishing\neight unique cucumber illnesses after considerable data augmentation. The\nincorporation of drone technology for high-resolution images improves disease\nevaluation. This development has enormous potential for improving crop\nmanagement, lowering labor costs, and increasing agricultural productivity.\nThis research, which automates disease detection, represents a significant step\ntoward a more efficient and sustainable agricultural future.",
      "tldr_zh": "这篇论文利用 machine vision 和 drone technology 提出一种创新方法，用于农业中黄瓜疾病的诊断。研究构建了一个精心策划的高光谱照片数据集，该数据集在真实田间条件下采集，并涵盖多种疾病类型，以支持精确的早期检测。模型通过大量数据增强技术，实现了87.5%的准确率，能够区分八种不同的黄瓜疾病。总体而言，此方法通过整合无人机获取高分辨率图像，有望提升作物管理效率、降低劳动力成本，并促进更可持续的农业发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 page and 6 figure",
      "pdf_url": "http://arxiv.org/pdf/2409.12350v1",
      "published_date": "2024-09-18 22:54:23 UTC",
      "updated_date": "2024-09-18 22:54:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:01:47.168180"
    },
    {
      "arxiv_id": "2409.12347v1",
      "title": "Axial Attention Transformer Networks: A New Frontier in Breast Cancer Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Weijie He",
        "Runyuan Bao",
        "Yiru Cang",
        "Jianjun Wei",
        "Yang Zhang",
        "Jiacheng Hu"
      ],
      "abstract": "This paper delves into the challenges and advancements in the field of\nmedical image segmentation, particularly focusing on breast cancer diagnosis.\nThe authors propose a novel Transformer-based segmentation model that addresses\nthe limitations of traditional convolutional neural networks (CNNs), such as\nU-Net, in accurately localizing and segmenting small lesions within breast\ncancer images. The model introduces an axial attention mechanism to enhance the\ncomputational efficiency and address the issue of global contextual information\nthat is often overlooked by CNNs. Additionally, the paper discusses\nimprovements tailored to the small dataset challenge, including the\nincorporation of relative position information and a gated axial attention\nmechanism to refine the model's focus on relevant features. The proposed model\naims to significantly improve the segmentation accuracy of breast cancer\nimages, offering a more efficient and effective tool for computer-aided\ndiagnosis.",
      "tldr_zh": "这篇论文探讨了医疗图像分割的挑战，特别是乳腺癌诊断中传统 CNNs（如 U-Net）在定位和分割小病变时的局限性。作者提出了一种新型 Transformer-based segmentation model，引入 axial attention mechanism 来提升计算效率并捕捉全局上下文信息，同时通过整合相对位置信息和 gated axial attention mechanism 优化小数据集问题。该模型显著提高了乳腺癌图像的分割准确性，为计算机辅助诊断提供更高效有效的工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12347v1",
      "published_date": "2024-09-18 22:40:29 UTC",
      "updated_date": "2024-09-18 22:40:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:02:00.436254"
    },
    {
      "arxiv_id": "2409.12334v1",
      "title": "Deep vessel segmentation with joint multi-prior encoding",
      "title_zh": "深度血管分割与联合多先验编码",
      "authors": [
        "Amine Sadikine",
        "Bogdan Badic",
        "Enzo Ferrante",
        "Vincent Noblet",
        "Pascal Ballet",
        "Dimitris Visvikis",
        "Pierre-Henri Conze"
      ],
      "abstract": "The precise delineation of blood vessels in medical images is critical for\nmany clinical applications, including pathology detection and surgical\nplanning. However, fully-automated vascular segmentation is challenging because\nof the variability in shape, size, and topology. Manual segmentation remains\nthe gold standard but is time-consuming, subjective, and impractical for\nlarge-scale studies. Hence, there is a need for automatic and reliable\nsegmentation methods that can accurately detect blood vessels from medical\nimages. The integration of shape and topological priors into vessel\nsegmentation models has been shown to improve segmentation accuracy by offering\ncontextual information about the shape of the blood vessels and their spatial\nrelationships within the vascular tree. To further improve anatomical\nconsistency, we propose a new joint prior encoding mechanism which incorporates\nboth shape and topology in a single latent space. The effectiveness of our\nmethod is demonstrated on the publicly available 3D-IRCADb dataset. More\nglobally, the proposed approach holds promise in overcoming the challenges\nassociated with automatic vessel delineation and has the potential to advance\nthe field of deep priors encoding.",
      "tldr_zh": "本文研究了医疗图像中血管的精确分割问题，该任务对病理检测和手术规划至关重要，但因血管形状、大小和拓扑的变异性而面临挑战。作者提出了一种新的joint multi-prior encoding机制，将shape priors和topology priors整合到一个单一潜在空间中，以提升分割准确性和解剖一致性。在公开的3D-IRCADb数据集上实验验证显示，该方法有效，有望克服自动血管分割的难题并推进deep priors encoding领域的发展。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "5 pages, 3 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2409.12334v1",
      "published_date": "2024-09-18 22:03:46 UTC",
      "updated_date": "2024-09-18 22:03:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:02:11.737841"
    },
    {
      "arxiv_id": "2409.12333v1",
      "title": "Scale-specific auxiliary multi-task contrastive learning for deep liver vessel segmentation",
      "title_zh": "尺度特定的",
      "authors": [
        "Amine Sadikine",
        "Bogdan Badic",
        "Jean-Pierre Tasu",
        "Vincent Noblet",
        "Pascal Ballet",
        "Dimitris Visvikis",
        "Pierre-Henri Conze"
      ],
      "abstract": "Extracting hepatic vessels from abdominal images is of high interest for\nclinicians since it allows to divide the liver into functionally-independent\nCouinaud segments. In this respect, an automated liver blood vessel extraction\nis widely summoned. Despite the significant growth in performance of semantic\nsegmentation methodologies, preserving the complex multi-scale geometry of main\nvessels and ramifications remains a major challenge. This paper provides a new\ndeep supervised approach for vessel segmentation, with a strong focus on\nrepresentations arising from the different scales inherent to the vascular tree\ngeometry. In particular, we propose a new clustering technique to decompose the\ntree into various scale levels, from tiny to large vessels. Then, we extend\nstandard 3D UNet to multi-task learning by incorporating scale-specific\nauxiliary tasks and contrastive learning to encourage the discrimination\nbetween scales in the shared representation. Promising results, depicted in\nseveral evaluation metrics, are revealed on the public 3D-IRCADb dataset.",
      "tldr_zh": "该论文针对肝脏血管分割的多尺度几何挑战，提出了一种新的深度监督方法，以精确提取肝脏血管并支持肝脏功能段划分。方法包括使用新聚类技术将血管树分解为不同尺度级别（从微小到大型），并扩展标准的 3D UNet 框架，融入多任务学习和尺度特定的辅助任务及对比学习，以在共享表示中区分尺度。实验在公共数据集 3D-IRCADb 上显示，该方法在多个评估指标上取得了有前景的结果。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "5 pages, 5 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2409.12333v1",
      "published_date": "2024-09-18 22:03:22 UTC",
      "updated_date": "2024-09-18 22:03:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:02:24.007128"
    },
    {
      "arxiv_id": "2409.15369v1",
      "title": "Geometric Relational Embeddings",
      "title_zh": "几何关系嵌入",
      "authors": [
        "Bo Xiong"
      ],
      "abstract": "Relational representation learning transforms relational data into continuous\nand low-dimensional vector representations. However, vector-based\nrepresentations fall short in capturing crucial properties of relational data\nthat are complex and symbolic. We propose geometric relational embeddings, a\nparadigm of relational embeddings that respect the underlying symbolic\nstructures. Specifically, this dissertation introduces various geometric\nrelational embedding models capable of capturing: 1) complex structured\npatterns like hierarchies and cycles in networks and knowledge graphs; 2)\nlogical structures in ontologies and logical constraints applicable for\nconstraining machine learning model outputs; and 3) high-order structures\nbetween entities and relations. Our results obtained from benchmark and\nreal-world datasets demonstrate the efficacy of geometric relational embeddings\nin adeptly capturing these discrete, symbolic, and structured properties\ninherent in relational data.",
      "tldr_zh": "本研究提出geometric relational embeddings，一种新型的关系嵌入范式，旨在解决传统向量表示无法有效捕捉关系数据中复杂符号化属性的问题。该方法引入各种geometric relational embedding模型，能够捕捉知识图谱中的层次和循环等复杂结构模式、逻辑约束中的本体结构，以及实体和关系之间的高阶结构。实验结果显示，在基准和真实数据集上，该嵌入方法成功地捕捉了关系数据的离散、符号化和结构化特性，从而提升了关系表示学习的效能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Doctoral Dissertation, 177 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.15369v1",
      "published_date": "2024-09-18 22:02:24 UTC",
      "updated_date": "2024-09-18 22:02:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:02:34.801885"
    },
    {
      "arxiv_id": "2409.12314v1",
      "title": "Understanding Implosion in Text-to-Image Generative Models",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxin Ding",
        "Cathy Y. Li",
        "Shawn Shan",
        "Ben Y. Zhao",
        "Haitao Zheng"
      ],
      "abstract": "Recent works show that text-to-image generative models are surprisingly\nvulnerable to a variety of poisoning attacks. Empirical results find that these\nmodels can be corrupted by altering associations between individual text\nprompts and associated visual features. Furthermore, a number of concurrent\npoisoning attacks can induce \"model implosion,\" where the model becomes unable\nto produce meaningful images for unpoisoned prompts. These intriguing findings\nhighlight the absence of an intuitive framework to understand poisoning attacks\non these models. In this work, we establish the first analytical framework on\nrobustness of image generative models to poisoning attacks, by modeling and\nanalyzing the behavior of the cross-attention mechanism in latent diffusion\nmodels. We model cross-attention training as an abstract problem of \"supervised\ngraph alignment\" and formally quantify the impact of training data by the\nhardness of alignment, measured by an Alignment Difficulty (AD) metric. The\nhigher the AD, the harder the alignment. We prove that AD increases with the\nnumber of individual prompts (or concepts) poisoned. As AD grows, the alignment\ntask becomes increasingly difficult, yielding highly distorted outcomes that\nfrequently map meaningful text prompts to undefined or meaningless visual\nrepresentations. As a result, the generative model implodes and outputs random,\nincoherent images at large. We validate our analytical framework through\nextensive experiments, and we confirm and explain the unexpected (and\nunexplained) effect of model implosion while producing new, unforeseen\ninsights. Our work provides a useful tool for studying poisoning attacks\nagainst diffusion models and their defenses.",
      "tldr_zh": "本文研究了文本到图像生成模型（text-to-image generative models）在面对 poisoning attacks 时出现的 implosion 现象，即模型无法为未中毒提示生成有意义图像。作者建立了首个分析框架，通过将 cross-attention mechanism 在 latent diffusion models 中的训练建模为 supervised graph alignment 问题，并引入 Alignment Difficulty (AD) 指标来量化攻击影响。研究证明，随着 poisoned 提示数量增加，AD 值升高，导致模型输出随机、无意义图像。实验验证了这一框架，提供新见解，并为 diffusion models 的防御策略奠定基础。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "ACM CCS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.12314v1",
      "published_date": "2024-09-18 21:06:45 UTC",
      "updated_date": "2024-09-18 21:06:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:02:47.814870"
    },
    {
      "arxiv_id": "2409.12300v1",
      "title": "Autoformalization of Game Descriptions using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Agnieszka Mensfelt",
        "Kostas Stathis",
        "Vince Trencsenyi"
      ],
      "abstract": "Game theory is a powerful framework for reasoning about strategic\ninteractions, with applications in domains ranging from day-to-day life to\ninternational politics. However, applying formal reasoning tools in such\ncontexts is challenging, as these scenarios are often expressed in natural\nlanguage. To address this, we introduce a framework for the autoformalization\nof game-theoretic scenarios, which translates natural language descriptions\ninto formal logic representations suitable for formal solvers. Our approach\nutilizes one-shot prompting and a solver that provides feedback on syntactic\ncorrectness to allow LLMs to refine the code. We evaluate the framework using\nGPT-4o and a dataset of natural language problem descriptions, achieving 98%\nsyntactic correctness and 88% semantic correctness. These results show the\npotential of LLMs to bridge the gap between real-life strategic interactions\nand formal reasoning.",
      "tldr_zh": "该研究提出了一种框架，使用 Large Language Models (LLMs) 自动将游戏理论场景的自然语言描述转化为适合正式求解器的逻辑表示，从而解决应用正式推理工具的挑战。方法采用 one-shot prompting 和求解器反馈机制，让 LLMs 迭代优化代码。在使用 GPT-4o 的实验中，该框架在数据集上达到了 98% 的语法正确性和 88% 的语义正确性。这些结果展示了 LLMs 在 bridging the gap 之间现实战略互动与正式推理的潜力。",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "code: https://github.com/dicelab-rhul/game-formaliser",
      "pdf_url": "http://arxiv.org/pdf/2409.12300v1",
      "published_date": "2024-09-18 20:18:53 UTC",
      "updated_date": "2024-09-18 20:18:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:02:59.941906"
    },
    {
      "arxiv_id": "2409.12294v1",
      "title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Abhinav Jain",
        "Chris Jermaine",
        "Vaibhav Unhelkar"
      ],
      "abstract": "Large language models (LLMs) have recently emerged as promising tools for\nsolving challenging robotic tasks, even in the presence of action and\nobservation uncertainties. Recent LLM-based decision-making methods (also\nreferred to as LLM-based agents), when paired with appropriate critics, have\ndemonstrated potential in solving complex, long-horizon tasks with relatively\nfew interactions. However, most existing LLM-based agents lack the ability to\nretain and learn from past interactions - an essential trait of learning-based\nrobotic systems. We propose RAG-Modulo, a framework that enhances LLM-based\nagents with a memory of past interactions and incorporates critics to evaluate\nthe agents' decisions. The memory component allows the agent to automatically\nretrieve and incorporate relevant past experiences as in-context examples,\nproviding context-aware feedback for more informed decision-making. Further by\nupdating its memory, the agent improves its performance over time, thereby\nexhibiting learning. Through experiments in the challenging BabyAI and AlfWorld\ndomains, we demonstrate significant improvements in task success rates and\nefficiency, showing that the proposed RAG-Modulo framework outperforms\nstate-of-the-art baselines.",
      "tldr_zh": "该研究提出RAG-Modulo框架，以提升LLM-based agents在处理机器人序列任务时的学习能力，通过整合过去互动的记忆组件和critics来评估决策。框架允许代理自动检索相关in-context examples作为反馈，提升决策的上下文感知和性能改进；在BabyAI和AlfWorld领域实验中，RAG-Modulo显著提高了任务成功率和效率，超越了现有基线模型。总的来说，该方法为LLMs在不确定性环境下的机器人决策提供了更具适应性和学习性的解决方案。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12294v1",
      "published_date": "2024-09-18 20:03:32 UTC",
      "updated_date": "2024-09-18 20:03:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:03:10.980797"
    },
    {
      "arxiv_id": "2409.12289v1",
      "title": "MetaPix: A Data-Centric AI Development Platform for Efficient Management and Utilization of Unstructured Computer Vision Data",
      "title_zh": "翻译失败",
      "authors": [
        "Sai Vishwanath Venkatesh",
        "Atra Akandeh",
        "Madhu Lokanath"
      ],
      "abstract": "In today's world of advanced AI technologies, data management is a critical\ncomponent of any AI/ML solution. Effective data management is vital for the\ncreation and maintenance of high-quality, diverse datasets, which significantly\nenhance predictive capabilities and lead to smarter business solutions. In this\nwork, we introduce MetaPix, a Data-centric AI platform offering comprehensive\ndata management solutions specifically designed for unstructured data. MetaPix\noffers robust tools for data ingestion, processing, storage, versioning,\ngovernance, and discovery. The platform operates on four key concepts:\nDataSources, Datasets, Extensions and Extractors. A DataSource serves as\nMetaPix top level asset, representing a narrow-scoped source of data for a\nspecific use. Datasets are MetaPix second level object, structured collections\nof data. Extractors are internal tools integrated into MetaPix's backend\nprocessing, facilitate data processing and enhancement. Additionally, MetaPix\nsupports extensions, enabling integration with external third-party tools to\nenhance platform functionality. This paper delves into each MetaPix concept in\ndetail, illustrating how they collectively contribute to the platform's\nobjectives. By providing a comprehensive solution for managing and utilizing\nunstructured computer vision data, MetaPix equips organizations with a powerful\ntoolset to develop AI applications effectively.",
      "tldr_zh": "本文介绍了 MetaPix，一种数据中心 AI 开发平台，专注于高效管理与利用非结构化计算机视觉数据。平台提供全面工具支持，包括数据摄取、处理、存储、版本控制、治理和发现，基于四个关键概念：DataSources（顶级数据资产）、Datasets（结构化数据集合）、Extractors（后端数据处理工具）和 Extensions（外部工具集成）。通过这些功能，MetaPix 帮助组织创建高质量数据集，提升 AI 应用的预测能力和业务效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted @ The 22nd International Conference on Software Engineering\n  Research & Practice",
      "pdf_url": "http://arxiv.org/pdf/2409.12289v1",
      "published_date": "2024-09-18 19:50:53 UTC",
      "updated_date": "2024-09-18 19:50:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:03:23.258783"
    },
    {
      "arxiv_id": "2409.15368v1",
      "title": "MedCodER: A Generative AI Assistant for Medical Coding",
      "title_zh": "MedCodER：生成式 AI 医疗编码助手",
      "authors": [
        "Krishanu Das Baksi",
        "Elijah Soba",
        "John J. Higgins",
        "Ravi Saini",
        "Jaden Wood",
        "Jane Cook",
        "Jack Scott",
        "Nirmala Pudota",
        "Tim Weninger",
        "Edward Bowen",
        "Sanmitra Bhattacharya"
      ],
      "abstract": "Medical coding is essential for standardizing clinical data and communication\nbut is often time-consuming and prone to errors. Traditional Natural Language\nProcessing (NLP) methods struggle with automating coding due to the large label\nspace, lengthy text inputs, and the absence of supporting evidence annotations\nthat justify code selection. Recent advancements in Generative Artificial\nIntelligence (AI) offer promising solutions to these challenges. In this work,\nwe introduce MedCodER, a Generative AI framework for automatic medical coding\nthat leverages extraction, retrieval, and re-ranking techniques as core\ncomponents. MedCodER achieves a micro-F1 score of 0.60 on International\nClassification of Diseases (ICD) code prediction, significantly outperforming\nstate-of-the-art methods. Additionally, we present a new dataset containing\nmedical records annotated with disease diagnoses, ICD codes, and supporting\nevidence texts (https://doi.org/10.5281/zenodo.13308316). Ablation tests\nconfirm that MedCodER's performance depends on the integration of each of its\naforementioned components, as performance declines when these components are\nevaluated in isolation.",
      "tldr_zh": "本研究针对医疗编码的耗时和易出错问题，提出MedCodER，一种基于Generative AI的自动编码框架。该框架整合提取、检索和重新排序技术，解决了传统NLP方法在处理大型标签空间和长文本输入时的局限性。MedCodER在ICD代码预测上实现了0.60的micro-F1分数，显著优于现有方法，并发布了一个新数据集，包含标注的医疗记录、疾病诊断、ICD代码及支持证据文本。消融测试证实，每个组件的整合至关重要，单独评估时性能会下降。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.15368v1",
      "published_date": "2024-09-18 19:36:33 UTC",
      "updated_date": "2024-09-18 19:36:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:03:35.217945"
    },
    {
      "arxiv_id": "2409.18995v1",
      "title": "Systematic Characterization of the Effectiveness of Alignment in Large Language Models for Categorical Decisions",
      "title_zh": "对",
      "authors": [
        "Isaac Kohane"
      ],
      "abstract": "As large language models (LLMs) are deployed in high-stakes domains like\nhealthcare, understanding how well their decision-making aligns with human\npreferences and values becomes crucial, especially when we recognize that there\nis no single gold standard for these preferences. This paper applies a\nsystematic methodology for evaluating preference alignment in LLMs on\ncategorical decision-making with medical triage as a domain-specific use case.\nIt also measures how effectively an alignment procedure will change the\nalignment of a specific model. Key to this methodology is a novel simple\nmeasure, the Alignment Compliance Index (ACI), that quantifies how effectively\na LLM can be aligned to a given preference function or gold standard. Since the\nACI measures the effect rather than the process of alignment, it is applicable\nto alignment methods beyond the in-context learning used in this study.\n  Using a dataset of simulated patient pairs, three frontier LLMs (GPT4o,\nClaude 3.5 Sonnet, and Gemini Advanced) were assessed on their ability to make\ntriage decisions consistent with an expert clinician's preferences. The models'\nperformance before and after alignment attempts was evaluated using various\nprompting strategies. The results reveal significant variability in alignment\neffectiveness across models and alignment approaches. Notably, models that\nperformed well, as measured by ACI, pre-alignment sometimes degraded\npost-alignment, and small changes in the target preference function led to\nlarge shifts in model rankings. The implicit ethical principles, as understood\nby humans, underlying the LLMs' decisions were also explored through targeted\nquestioning.\n  This study motivates the use of a practical set of methods and the ACI, in\nthe near term, to understand the correspondence between the variety of human\nand LLM decision-making values in categorical decision-making such as triage.",
      "tldr_zh": "本研究系统评估了大型语言模型(LLMs)在分类决策中的偏好对齐有效性，以医疗分流为例，引入Alignment Compliance Index(ACI)作为量化模型与给定偏好函数对齐程度的指标。研究使用模拟患者数据集测试GPT4o、Claude 3.5 Sonnet和Gemini Advanced等模型，通过in-context learning等对齐方法比较其决策性能前后变化。结果显示，对齐有效性因模型和策略而异，有些模型对齐后表现下降，且小幅偏好调整会导致排名大幅波动，强调ACI有助于理解人类和LLMs决策价值的对应性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages (without Appendix) Appendix 7 pages. 7 Figures",
      "pdf_url": "http://arxiv.org/pdf/2409.18995v1",
      "published_date": "2024-09-18 19:03:04 UTC",
      "updated_date": "2024-09-18 19:03:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:03:49.530011"
    },
    {
      "arxiv_id": "2409.15367v2",
      "title": "Fine-Tuning a Time Series Foundation Model with Wasserstein Loss",
      "title_zh": "使用瓦瑟斯坦损失微调时间序列基础模型",
      "authors": [
        "Andrei Chernov"
      ],
      "abstract": "Inspired by recent advancements in large language models (LLMs) for Natural\nLanguage Processing (NLP), there has been a surge in research focused on\ndeveloping foundational models for time series forecasting. One approach\ninvolves training LLM architectures on tokenized time series data using\ncross-entropy loss. Although this method has demonstrated promising results,\ncross-entropy loss is primarily designed for classification tasks and does not\naccount for the distance between classes. To address this limitation, we\npropose using the Wasserstein loss for such architectures. To validate our\napproach, we fine-tuned a foundational time series model on $22$ zero-shot\ndatasets, comparing the performance of cross-entropy loss with that of\nWasserstein loss. Our results demonstrate that replacing cross-entropy loss\nwith Wasserstein loss significantly improves point estimation.",
      "tldr_zh": "该研究受大型语言模型（LLMs）在自然语言处理（NLP）领域的启发，探讨了时间序列预测基础模型的微调问题。作者指出，传统使用交叉熵损失训练这些模型的局限在于其更适合分类任务，无法考虑类间距离，因此提出采用Wasserstein Loss作为替代方案。在22个零样本数据集上进行实验，结果显示Wasserstein Loss显著提高了点估计的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "4 main pages; 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.15367v2",
      "published_date": "2024-09-18 18:36:18 UTC",
      "updated_date": "2024-11-18 17:00:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:03:59.303971"
    },
    {
      "arxiv_id": "2409.12249v2",
      "title": "GCA-SUNet: A Gated Context-Aware Swin-UNet for Exemplar-Free Counting",
      "title_zh": "GCA-SUNet：一种门控上下文感知 Swin-UNet 用于无示例计数",
      "authors": [
        "Yuzhe Wu",
        "Yipeng Xu",
        "Tianyu Xu",
        "Jialu Zhang",
        "Jianfeng Ren",
        "Xudong Jiang"
      ],
      "abstract": "Exemplar-Free Counting aims to count objects of interest without intensive\nannotations of objects or exemplars. To achieve this, we propose a Gated\nContext-Aware Swin-UNet (GCA-SUNet) to directly map an input image to the\ndensity map of countable objects. Specifically, a set of Swin transformers form\nan encoder to derive a robust feature representation, and a Gated Context-Aware\nModulation block is designed to suppress irrelevant objects or background\nthrough a gate mechanism and exploit the attentive support of objects of\ninterest through a self-similarity matrix. The gate strategy is also\nincorporated into the bottleneck network and the decoder of the Swin-UNet to\nhighlight the features most relevant to objects of interest. By explicitly\nexploiting the attentive support among countable objects and eliminating\nirrelevant features through the gate mechanisms, the proposed GCA-SUNet focuses\non and counts objects of interest without relying on predefined categories or\nexemplars. Experimental results on the real-world datasets such as FSC-147 and\nCARPK demonstrate that GCA-SUNet significantly and consistently outperforms\nstate-of-the-art methods. The code is available at\nhttps://github.com/Amordia/GCA-SUNet.",
      "tldr_zh": "该论文提出了一种 Gated Context-Aware Swin-UNet (GCA-SUNet) 模型，用于 Exemplar-Free Counting，即在无需密集标注对象或示例的情况下，直接从输入图像映射到密度图。模型采用 Swin transformers 作为编码器提取鲁棒特征，并设计了 Gated Context-Aware Modulation 块，通过门控机制抑制无关对象或背景，同时利用自相似矩阵关注感兴趣的对象，该策略还整合到瓶颈网络和解码器中以突出相关特征。实验结果显示，GCA-SUNet 在 FSC-147 和 CARPK 等真实数据集上显著优于现有最先进方法，证明了其在无示例计数任务中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICME 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.12249v2",
      "published_date": "2024-09-18 18:14:00 UTC",
      "updated_date": "2025-03-27 00:09:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:04:11.930944"
    },
    {
      "arxiv_id": "2409.12193v1",
      "title": "Vista3D: Unravel the 3D Darkside of a Single Image",
      "title_zh": "翻译失败",
      "authors": [
        "Qiuhong Shen",
        "Xingyi Yang",
        "Michael Bi Mi",
        "Xinchao Wang"
      ],
      "abstract": "We embark on the age-old quest: unveiling the hidden dimensions of objects\nfrom mere glimpses of their visible parts. To address this, we present Vista3D,\na framework that realizes swift and consistent 3D generation within a mere 5\nminutes. At the heart of Vista3D lies a two-phase approach: the coarse phase\nand the fine phase. In the coarse phase, we rapidly generate initial geometry\nwith Gaussian Splatting from a single image. In the fine phase, we extract a\nSigned Distance Function (SDF) directly from learned Gaussian Splatting,\noptimizing it with a differentiable isosurface representation. Furthermore, it\nelevates the quality of generation by using a disentangled representation with\ntwo independent implicit functions to capture both visible and obscured aspects\nof objects. Additionally, it harmonizes gradients from 2D diffusion prior with\n3D-aware diffusion priors by angular diffusion prior composition. Through\nextensive evaluation, we demonstrate that Vista3D effectively sustains a\nbalance between the consistency and diversity of the generated 3D objects.\nDemos and code will be available at https://github.com/florinshen/Vista3D.",
      "tldr_zh": "该论文提出 Vista3D 框架，从单张图像快速生成高质量的 3D 对象，仅需 5 分钟。该框架采用两阶段方法：粗糙阶段使用 Gaussian Splatting 生成初始几何形状，精细阶段通过提取 Signed Distance Function (SDF) 并利用可微分 isosurface 表示进行优化。同时，Vista3D 引入两个独立的 implicit functions 来捕捉对象的可见和隐藏部分，并通过 angular diffusion prior composition 平衡 2D 和 3D 感知扩散先验的梯度。实验结果显示，该框架在 3D 对象生成的一致性和多样性之间实现了有效平衡。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GT",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV'2024",
      "pdf_url": "http://arxiv.org/pdf/2409.12193v1",
      "published_date": "2024-09-18 17:59:44 UTC",
      "updated_date": "2024-09-18 17:59:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:04:24.292862"
    },
    {
      "arxiv_id": "2409.12192v2",
      "title": "DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control",
      "title_zh": "翻译失败",
      "authors": [
        "Zichen Jeff Cui",
        "Hengkai Pan",
        "Aadhithya Iyer",
        "Siddhant Haldar",
        "Lerrel Pinto"
      ],
      "abstract": "Imitation learning has proven to be a powerful tool for training complex\nvisuomotor policies. However, current methods often require hundreds to\nthousands of expert demonstrations to handle high-dimensional visual\nobservations. A key reason for this poor data efficiency is that visual\nrepresentations are predominantly either pretrained on out-of-domain data or\ntrained directly through a behavior cloning objective. In this work, we present\nDynaMo, a new in-domain, self-supervised method for learning visual\nrepresentations. Given a set of expert demonstrations, we jointly learn a\nlatent inverse dynamics model and a forward dynamics model over a sequence of\nimage embeddings, predicting the next frame in latent space, without\naugmentations, contrastive sampling, or access to ground truth actions.\nImportantly, DynaMo does not require any out-of-domain data such as Internet\ndatasets or cross-embodied datasets. On a suite of six simulated and real\nenvironments, we show that representations learned with DynaMo significantly\nimprove downstream imitation learning performance over prior self-supervised\nlearning objectives, and pretrained representations. Gains from using DynaMo\nhold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,\nand nearest neighbors. Finally, we ablate over key components of DynaMo and\nmeasure its impact on downstream policy performance. Robot videos are best\nviewed at https://dynamo-ssl.github.io",
      "tldr_zh": "该研究针对模仿学习(Imitation Learning)处理高维视觉观察时的低数据效率问题，提出了一种新的域内自监督方法DynaMo，用于视觉-运动控制(Visuo-Motor Control)的预训练。DynaMo利用专家演示联合学习潜在逆动态模型(Inverse Dynamics Model)和前向动态模型(Forward Dynamics Model)，在图像嵌入序列上预测下一个帧的潜在空间，而无需数据增强、对照采样或域外数据集。在六个模拟和真实环境中，DynaMo显著提升了下游模仿学习性能，优于现有自监督学习目标和预训练表示，且适用于多种策略类如Behavior Transformer和Diffusion Policy。实验消融分析进一步验证了其关键组件对下游任务的影响。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12192v2",
      "published_date": "2024-09-18 17:59:43 UTC",
      "updated_date": "2024-10-30 18:48:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:04:36.110912"
    },
    {
      "arxiv_id": "2409.12191v2",
      "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
      "title_zh": "翻译失败",
      "authors": [
        "Peng Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Shijie Wang",
        "Zhihao Fan",
        "Jinze Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge",
        "Yang Fan",
        "Kai Dang",
        "Mengfei Du",
        "Xuancheng Ren",
        "Rui Men",
        "Dayiheng Liu",
        "Chang Zhou",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "abstract": "We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL\nmodels that redefines the conventional predetermined-resolution approach in\nvisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,\nwhich enables the model to dynamically process images of varying resolutions\ninto different numbers of visual tokens. This approach allows the model to\ngenerate more efficient and accurate visual representations, closely aligning\nwith human perceptual processes. The model also integrates Multimodal Rotary\nPosition Embedding (M-RoPE), facilitating the effective fusion of positional\ninformation across text, images, and videos. We employ a unified paradigm for\nprocessing both images and videos, enhancing the model's visual perception\ncapabilities. To explore the potential of large multimodal models, Qwen2-VL\ninvestigates the scaling laws for large vision-language models (LVLMs). By\nscaling both the model size-with versions at 2B, 8B, and 72B parameters-and the\namount of training data, the Qwen2-VL Series achieves highly competitive\nperformance. Notably, the Qwen2-VL-72B model achieves results comparable to\nleading models such as GPT-4o and Claude3.5-Sonnet across various multimodal\nbenchmarks, outperforming other generalist models. Code is available at\nhttps://github.com/QwenLM/Qwen2-VL .",
      "tldr_zh": "本研究推出了Qwen2-VL系列模型，这是Qwen-VL的升级版，通过引入Naive Dynamic Resolution机制，实现对任意分辨率图像的动态处理，生成更高效准确的视觉表示，以模拟人类感知过程。模型还整合了Multimodal Rotary Position Embedding (M-RoPE)，并采用统一的范式处理图像和视频，从而提升多模态信息融合能力。实验探索了大型视觉语言模型的缩放定律，通过扩展模型大小（如2B、8B和72B参数）和训练数据，Qwen2-VL-72B在各种多模态基准测试中表现媲美GPT-4o和Claude3.5-Sonnet，甚至优于其他通用模型。代码已在GitHub上公开。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at https://github.com/QwenLM/Qwen2-VL. arXiv admin\n  note: text overlap with arXiv:2408.15262 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2409.12191v2",
      "published_date": "2024-09-18 17:59:32 UTC",
      "updated_date": "2024-10-03 15:54:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:04:48.507810"
    },
    {
      "arxiv_id": "2409.12183v3",
      "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning",
      "title_zh": "CoT",
      "authors": [
        "Zayne Sprague",
        "Fangcong Yin",
        "Juan Diego Rodriguez",
        "Dongwei Jiang",
        "Manya Wadhwa",
        "Prasann Singhal",
        "Xinyu Zhao",
        "Xi Ye",
        "Kyle Mahowald",
        "Greg Durrett"
      ],
      "abstract": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications.",
      "tldr_zh": "本研究通过对超过100篇论文的元分析以及对20个数据集和14个模型的评估，发现Chain-of-Thought (CoT)提示主要在数学和符号推理任务上显著提升大语言模型(LLMs)的性能，而在其他类型任务上的提升较小。实验结果显示，在MMLU数据集上，直接生成答案与CoT在非符号任务中表现几乎相同，但CoT在涉及符号运算的任务中改善了执行过程，却不如使用符号求解器有效。作者建议选择性应用CoT以保持性能同时降低推理成本，并呼吁开发超越基于提示的CoT的新范式，以更好地利用中间计算。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.12183v3",
      "published_date": "2024-09-18 17:55:00 UTC",
      "updated_date": "2025-05-07 18:00:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:04:59.532857"
    },
    {
      "arxiv_id": "2409.12179v1",
      "title": "Computational Dynamical Systems",
      "title_zh": "计算动力系统",
      "authors": [
        "Jordan Cotler",
        "Semon Rezchikov"
      ],
      "abstract": "We study the computational complexity theory of smooth, finite-dimensional\ndynamical systems. Building off of previous work, we give definitions for what\nit means for a smooth dynamical system to simulate a Turing machine. We then\nshow that 'chaotic' dynamical systems (more precisely, Axiom A systems) and\n'integrable' dynamical systems (more generally, measure-preserving systems)\ncannot robustly simulate universal Turing machines, although such machines can\nbe robustly simulated by other kinds of dynamical systems. Subsequently, we\nshow that any Turing machine that can be encoded into a structurally stable\none-dimensional dynamical system must have a decidable halting problem, and\nmoreover an explicit time complexity bound in instances where it does halt.\nMore broadly, our work elucidates what it means for one 'machine' to simulate\nanother, and emphasizes the necessity of defining low-complexity 'encoders' and\n'decoders' to translate between the dynamics of the simulation and the system\nbeing simulated. We highlight how the notion of a computational dynamical\nsystem leads to questions at the intersection of computational complexity\ntheory, dynamical systems theory, and real algebraic geometry.",
      "tldr_zh": "本文研究平滑有限维动态系统的计算复杂性理论，定义了动态系统模拟Turing machine的条件。研究发现，混沌系统（如Axiom A systems）和可积系统（如measure-preserving systems）无法鲁棒模拟通用Turing machine，而其他动态系统可以实现这种模拟。此外，证明了编码到结构稳定一维动态系统的Turing machine必须具有可判定的halting problem，并伴随明确的运行时间界。该工作强调了模拟概念的必要性，包括定义低复杂度编码器和解码器，并探讨了计算复杂性理论、动态系统理论与实代数几何的交叉问题。",
      "categories": [
        "cs.CC",
        "cs.AI",
        "cs.FL",
        "math.DS"
      ],
      "primary_category": "cs.CC",
      "comment": "46+14 pages, 6 figures; accepted to FOCS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.12179v1",
      "published_date": "2024-09-18 17:51:48 UTC",
      "updated_date": "2024-09-18 17:51:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:05:12.296843"
    },
    {
      "arxiv_id": "2409.15366v1",
      "title": "Trajectory Anomaly Detection with Language Models",
      "title_zh": "基于语言模型的轨迹异常检测",
      "authors": [
        "Jonathan Mbuya",
        "Dieter Pfoser",
        "Antonios Anastasopoulos"
      ],
      "abstract": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
      "tldr_zh": "本研究提出了一种名为 LM-TAD 的轨迹异常检测方法，使用自回归因果注意力模型，将轨迹视为类似语言序列的 tokens，从而学习轨迹概率分布并精确识别异常位置。方法通过加入用户特定 tokens 来适应个人行为模式，并在 Pattern of Life (PoL) 数据集上超越现有方法，在 Porto 出租车数据集上表现出色。研究引入 perplexity 和 surprisal rate 指标来检测异常点，并支持多种轨迹表示形式，如 GPS 坐标、staypoints 和 activity types。该框架优化了在线检测过程，通过缓存注意力机制的关键值状态显著降低计算延迟。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.15366v1",
      "published_date": "2024-09-18 17:33:31 UTC",
      "updated_date": "2024-09-18 17:33:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:05:23.824286"
    },
    {
      "arxiv_id": "2409.13768v1",
      "title": "Magika: AI-Powered Content-Type Detection",
      "title_zh": "Magika：AI 驱动",
      "authors": [
        "Yanick Fratantonio",
        "Luca Invernizzi",
        "Loua Farah",
        "Kurt Thomas",
        "Marina Zhang",
        "Ange Albertini",
        "Francois Galilee",
        "Giancarlo Metitieri",
        "Julien Cretin",
        "Alex Petit-Bianco",
        "David Tao",
        "Elie Bursztein"
      ],
      "abstract": "The task of content-type detection -- which entails identifying the data\nencoded in an arbitrary byte sequence -- is critical for operating systems,\ndevelopment, reverse engineering environments, and a variety of security\napplications. In this paper, we introduce Magika, a novel AI-powered\ncontent-type detection tool. Under the hood, Magika employs a deep learning\nmodel that can execute on a single CPU with just 1MB of memory to store the\nmodel's weights. We show that Magika achieves an average F1 score of 99% across\nover a hundred content types and a test set of more than 1M files,\noutperforming all existing content-type detection tools today. In order to\nfoster adoption and improvements, we open source Magika under an Apache 2\nlicense on GitHub and make our model and training pipeline publicly available.\nOur tool has already seen adoption by the Gmail email provider for attachment\nscanning, and it has been integrated with VirusTotal to aid with malware\nanalysis.\n  We note that this paper discusses the first iteration of Magika, and a more\nrecent version already supports more than 200 content types. The interested\nreader can see the latest development on the Magika GitHub repository,\navailable at https://github.com/google/magika.",
      "tldr_zh": "这篇论文介绍了 Magika，一种基于 AI 的内容类型检测工具，使用深度学习模型在单 CPU 上运行，仅需 1MB 内存存储模型权重。Magika 在超过 100 种内容类型上测试集超过 1M 文件时，实现了 99% 的平均 F1 score，显著优于现有工具。作者开源了 Magika（Apache 2 许可），并公开了模型和训练管道，该工具已被 Gmail 用于附件扫描和 VirusTotal 用于恶意软件分析。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13768v1",
      "published_date": "2024-09-18 17:24:39 UTC",
      "updated_date": "2024-09-18 17:24:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:05:35.420890"
    },
    {
      "arxiv_id": "2409.15365v1",
      "title": "Novel Saliency Analysis for the Forward Forward Algorithm",
      "title_zh": "针对 Forward Forward 算法的新颖显著性分析",
      "authors": [
        "Mitra Bakhshi"
      ],
      "abstract": "Incorporating the Forward Forward algorithm into neural network training\nrepresents a transformative shift from traditional methods, introducing a dual\nforward mechanism that streamlines the learning process by bypassing the\ncomplexities of derivative propagation. This method is noted for its simplicity\nand efficiency and involves executing two forward passes the first with actual\ndata to promote positive reinforcement, and the second with synthetically\ngenerated negative data to enable discriminative learning. Our experiments\nconfirm that the Forward Forward algorithm is not merely an experimental\nnovelty but a viable training strategy that competes robustly with conventional\nmulti layer perceptron (MLP) architectures. To overcome the limitations\ninherent in traditional saliency techniques, which predominantly rely on\ngradient based methods, we developed a bespoke saliency algorithm specifically\ntailored for the Forward Forward framework. This innovative algorithm enhances\nthe intuitive understanding of feature importance and network decision-making,\nproviding clear visualizations of the data features most influential in model\npredictions. By leveraging this specialized saliency method, we gain deeper\ninsights into the internal workings of the model, significantly enhancing our\ninterpretative capabilities beyond those offered by standard approaches. Our\nevaluations, utilizing the MNIST and Fashion MNIST datasets, demonstrate that\nour method performs comparably to traditional MLP-based models.",
      "tldr_zh": "该研究介绍了 Forward Forward 算法，一种创新的神经网络训练方法，通过双前向传递（一个用于实际数据的正强化，另一个用于合成负数据的判别学习）来简化学习过程，避免了传统梯度传播的复杂性。论文开发了一种定制的 saliency 算法，专门针对 Forward Forward 框架，增强了对特征重要性和模型决策的直观理解，并提供清晰的可视化，从而深化模型内部机制的解释。实验在 MNIST 和 Fashion MNIST 数据集上表明，该算法的性能与传统 multi-layer perceptron (MLP) 模型相当，证明其作为可行训练策略的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "2nd International Conference on Artificial Intelligence, Blockchain,\n  and Internet of Things, (AIBThings)",
      "pdf_url": "http://arxiv.org/pdf/2409.15365v1",
      "published_date": "2024-09-18 17:21:59 UTC",
      "updated_date": "2024-09-18 17:21:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:05:47.533946"
    },
    {
      "arxiv_id": "2409.12154v1",
      "title": "Abductive explanations of classifiers under constraints: Complexity and properties",
      "title_zh": "翻译失败",
      "authors": [
        "Martin Cooper",
        "Leila Amgoud"
      ],
      "abstract": "Abductive explanations (AXp's) are widely used for understanding decisions of\nclassifiers. Existing definitions are suitable when features are independent.\nHowever, we show that ignoring constraints when they exist between features may\nlead to an explosion in the number of redundant or superfluous AXp's. We\npropose three new types of explanations that take into account constraints and\nthat can be generated from the whole feature space or from a sample (such as a\ndataset). They are based on a key notion of coverage of an explanation, the set\nof instances it explains. We show that coverage is powerful enough to discard\nredundant and superfluous AXp's. For each type, we analyse the complexity of\nfinding an explanation and investigate its formal properties. The final result\nis a catalogue of different forms of AXp's with different complexities and\ndifferent formal guarantees.",
      "tldr_zh": "这篇论文探讨了在特征之间存在约束时，分类器的 Abductive explanations (AXp's) 可能导致冗余或多余解释数量爆炸的问题。作者提出三种新类型解释，这些解释基于“coverage”（解释所覆盖的实例集合）概念，能够从整个特征空间或样本（如数据集）生成，并有效去除冗余。论文分析了每种解释的复杂性和正式属性，最终提供了一个 AXp's 不同形式的目录，包括各种复杂度和正式保证。",
      "categories": [
        "cs.AI",
        "68T01, 68Q17",
        "I.2.4"
      ],
      "primary_category": "cs.AI",
      "comment": "Full version with proofs of Martin C. Cooper and Leila Amgoud,\n  Abductive explanations of classifiers under constraints: Complexity and\n  properties, ECAI 2023, 469-476",
      "pdf_url": "http://arxiv.org/pdf/2409.12154v1",
      "published_date": "2024-09-18 17:15:39 UTC",
      "updated_date": "2024-09-18 17:15:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:05:59.323694"
    },
    {
      "arxiv_id": "2409.12150v1",
      "title": "Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation with Preference",
      "title_zh": "翻译失败",
      "authors": [
        "Najmeh Forouzandehmehr",
        "Nima Farrokhsiar",
        "Ramin Giahi",
        "Evren Korpeoglu",
        "Kannan Achan"
      ],
      "abstract": "Personalized outfit recommendation remains a complex challenge, demanding\nboth fashion compatibility understanding and trend awareness. This paper\npresents a novel framework that harnesses the expressive power of large\nlanguage models (LLMs) for this task, mitigating their \"black box\" and static\nnature through fine-tuning and direct feedback integration. We bridge the item\nvisual-textual gap in items descriptions by employing image captioning with a\nMultimodal Large Language Model (MLLM). This enables the LLM to extract style\nand color characteristics from human-curated fashion images, forming the basis\nfor personalized recommendations. The LLM is efficiently fine-tuned on the\nopen-source Polyvore dataset of curated fashion images, optimizing its ability\nto recommend stylish outfits. A direct preference mechanism using negative\nexamples is employed to enhance the LLM's decision-making process. This creates\na self-enhancing AI feedback loop that continuously refines recommendations in\nline with seasonal fashion trends. Our framework is evaluated on the Polyvore\ndataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,\nand complementary item retrieval. These evaluations underline the framework's\nability to generate stylish, trend-aligned outfit suggestions, continuously\nimproving through direct feedback. The evaluation results demonstrated that our\nproposed framework significantly outperforms the base LLM, creating more\ncohesive outfits. The improved performance in these tasks underscores the\nproposed framework's potential to enhance the shopping experience with accurate\nsuggestions, proving its effectiveness over the vanilla LLM based outfit\ngeneration.",
      "tldr_zh": "本文提出了一种名为“Decoding Style”的框架，通过高效微调 LLMs（大型语言模型）来实现图像引导的个性化服装推荐，并整合直接反馈机制以提升时尚兼容性和趋势意识。该框架利用 MLLM（多模态大型语言模型）进行图像字幕，桥接视觉和文本差距，从而从时尚图像中提取风格和颜色特征，并在开源 Polyvore 数据集上优化 LLMs 的决策过程，包括使用负面例子创建自我增强的反馈循环。实验评估显示，该框架在填空任务和互补物品检索中显著优于基线 LLMs，生成更连贯的服装建议，并证明了其在提升购物体验方面的潜力。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.12150v1",
      "published_date": "2024-09-18 17:15:06 UTC",
      "updated_date": "2024-09-18 17:15:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:06:12.780113"
    },
    {
      "arxiv_id": "2409.12139v3",
      "title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sijing Chen",
        "Yuan Feng",
        "Laipeng He",
        "Tianwei He",
        "Wendi He",
        "Yanni Hu",
        "Bin Lin",
        "Yiting Lin",
        "Yu Pan",
        "Pengfei Tan",
        "Chengwei Tian",
        "Chen Wang",
        "Zhicheng Wang",
        "Ruoye Xie",
        "Jixun Yao",
        "Quanlei Yan",
        "Yuguang Yang",
        "Jianhao Ye",
        "Jingjing Yin",
        "Yanzhen Yu",
        "Huimin Zhang",
        "Xiang Zhang",
        "Guangcheng Zhao",
        "Hongbin Zhou",
        "Pengpeng Zou"
      ],
      "abstract": "With the advent of the big data and large language model era, zero-shot\npersonalized rapid customization has emerged as a significant trend. In this\nreport, we introduce Takin AudioLLM, a series of techniques and models, mainly\nincluding Takin TTS, Takin VC, and Takin Morphing, specifically designed for\naudiobook production. These models are capable of zero-shot speech production,\ngenerating high-quality speech that is nearly indistinguishable from real human\nspeech and facilitating individuals to customize the speech content according\nto their own needs. Specifically, we first introduce Takin TTS, a neural codec\nlanguage model that builds upon an enhanced neural speech codec and a\nmulti-task training framework, capable of generating high-fidelity natural\nspeech in a zero-shot way. For Takin VC, we advocate an effective content and\ntimbre joint modeling approach to improve the speaker similarity, while\nadvocating for a conditional flow matching based decoder to further enhance its\nnaturalness and expressiveness. Last, we propose the Takin Morphing system with\nhighly decoupled and advanced timbre and prosody modeling approaches, which\nenables individuals to customize speech production with their preferred timbre\nand prosody in a precise and controllable manner. Extensive experiments\nvalidate the effectiveness and robustness of our Takin AudioLLM series models.\nFor detailed demos, please refer to\nhttps://everest-ai.github.io/takinaudiollm/.",
      "tldr_zh": "本研究介绍了Takin AudioLLM系列模型，包括Takin TTS、Takin VC和Takin Morphing，旨在实现zero-shot语音生成，支持高保真自然语音的个性化定制，特别适用于有声书制作。Takin TTS基于增强的neural codec language model和多任务训练框架，能够zero-shot生成高质量语音。Takin VC采用内容和音色联合建模方法结合条件流匹配基于解码器，提升说话者相似度、自然性和表现力。Takin Morphing通过高度解耦的音色和韵律建模，允许用户精确控制语音的音色和韵律。实验结果证明了这些模型的有效性和鲁棒性，并提供了详细演示。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Technical Report; 18 pages; typos corrected, references added, demo\n  url modified, author name modified;",
      "pdf_url": "http://arxiv.org/pdf/2409.12139v3",
      "published_date": "2024-09-18 17:03:12 UTC",
      "updated_date": "2024-09-24 02:00:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:06:25.060770"
    },
    {
      "arxiv_id": "2409.12136v1",
      "title": "GRIN: GRadient-INformed MoE",
      "title_zh": "翻译失败",
      "authors": [
        "Liyuan Liu",
        "Young Jin Kim",
        "Shuohang Wang",
        "Chen Liang",
        "Yelong Shen",
        "Hao Cheng",
        "Xiaodong Liu",
        "Masahiro Tanaka",
        "Xiaoxia Wu",
        "Wenxiang Hu",
        "Vishrav Chaudhary",
        "Zeqi Lin",
        "Chenruidong Zhang",
        "Jilong Xue",
        "Hany Awadalla",
        "Jianfeng Gao",
        "Weizhu Chen"
      ],
      "abstract": "Mixture-of-Experts (MoE) models scale more effectively than dense models due\nto sparse computation through expert routing, selectively activating only a\nsmall subset of expert modules. However, sparse computation challenges\ntraditional training practices, as discrete expert routing hinders standard\nbackpropagation and thus gradient-based optimization, which are the cornerstone\nof deep learning. To better pursue the scaling power of MoE, we introduce GRIN\n(GRadient-INformed MoE training), which incorporates sparse gradient estimation\nfor expert routing and configures model parallelism to avoid token dropping.\nApplying GRIN to autoregressive language modeling, we develop a top-2\n16$\\times$3.8B MoE model. Our model, with only 6.6B activated parameters,\noutperforms a 7B dense model and matches the performance of a 14B dense model\ntrained on the same data. Extensive evaluations across diverse tasks\ndemonstrate the potential of GRIN to significantly enhance MoE efficacy,\nachieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.",
      "tldr_zh": "该研究针对 Mixture-of-Experts (MoE) 模型的稀疏计算问题，提出 GRIN (GRadient-INformed MoE training) 方法，通过 sparse gradient estimation for expert routing 和 model parallelism 避免 token dropping，从而优化 gradient-based optimization。应用 GRIN 于 autoregressive language modeling，他们开发了一个 top-2 16×3.8B MoE 模型，仅激活 6.6B 参数，却超过了 7B dense 模型的表现，并与 14B dense 模型相当。实验结果显示，该模型在多任务评估中表现出色，包括 MMLU 79.4、HellaSwag 83.7、HumanEval 74.4 和 MATH 58.9，证明了 GRIN 在提升 MoE 效能方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "58 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.12136v1",
      "published_date": "2024-09-18 17:00:20 UTC",
      "updated_date": "2024-09-18 17:00:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:06:36.186435"
    },
    {
      "arxiv_id": "2409.12135v2",
      "title": "Almost Sure Convergence of Linear Temporal Difference Learning with Arbitrary Features",
      "title_zh": "翻译失败",
      "authors": [
        "Jiuqi Wang",
        "Shangtong Zhang"
      ],
      "abstract": "Temporal difference (TD) learning with linear function approximation,\nabbreviated as linear TD, is a classic and powerful prediction algorithm in\nreinforcement learning. While it is well understood that linear TD converges\nalmost surely to a unique point, this convergence traditionally requires the\nassumption that the features used by the approximator are linearly independent.\nHowever, this linear independence assumption does not hold in many practical\nscenarios. This work is the first to establish the almost sure convergence of\nlinear TD without requiring linearly independent features. In fact, we do not\nmake any assumptions on the features. We prove that the approximated value\nfunction converges to a unique point and the weight iterates converge to a set.\nWe also establish a notion of local stability of the weight iterates.\nImportantly, we do not need to introduce any other additional assumptions and\ndo not need to make any modification to the linear TD algorithm. Key to our\nanalysis is a novel characterization of bounded invariant sets of the mean ODE\nof linear TD.",
      "tldr_zh": "本文研究了线性 Temporal Difference (TD) 学习算法的几乎必然收敛（almost sure convergence），传统上需假设特征线性独立，但本文首次证明了在任意特征条件下，该算法也能收敛。研究显示，逼近的价值函数收敛到一个唯一点，而权重迭代收敛到一个集合，并建立了权重迭代的局部稳定性。关键方法是引入了对线性 TD 的均值 ODE 的新型有界不变集表征，无需额外假设或算法修改，这扩展了算法在实际场景中的适用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 0 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12135v2",
      "published_date": "2024-09-18 16:59:17 UTC",
      "updated_date": "2024-10-15 18:50:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:06:47.214242"
    },
    {
      "arxiv_id": "2409.12134v1",
      "title": "BERT-VBD: Vietnamese Multi-Document Summarization Framework",
      "title_zh": "BERT-VBD：越南语多文档摘要框架",
      "authors": [
        "Tuan-Cuong Vuong",
        "Trang Mai Xuan",
        "Thien Van Luong"
      ],
      "abstract": "In tackling the challenge of Multi-Document Summarization (MDS), numerous\nmethods have been proposed, spanning both extractive and abstractive\nsummarization techniques. However, each approach has its own limitations,\nmaking it less effective to rely solely on either one. An emerging and\npromising strategy involves a synergistic fusion of extractive and abstractive\nsummarization methods. Despite the plethora of studies in this domain, research\non the combined methodology remains scarce, particularly in the context of\nVietnamese language processing. This paper presents a novel Vietnamese MDS\nframework leveraging a two-component pipeline architecture that integrates\nextractive and abstractive techniques. The first component employs an\nextractive approach to identify key sentences within each document. This is\nachieved by a modification of the pre-trained BERT network, which derives\nsemantically meaningful phrase embeddings using siamese and triplet network\nstructures. The second component utilizes the VBD-LLaMA2-7B-50b model for\nabstractive summarization, ultimately generating the final summary document.\nOur proposed framework demonstrates a positive performance, attaining ROUGE-2\nscores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-art\nbaselines.",
      "tldr_zh": "这篇论文提出了BERT-VBD框架，一种针对越南语的多文档摘要(MDS)系统，通过整合提取式和抽象式方法来解决传统方法的局限性。框架采用两部分管道设计：首先，使用修改后的BERT网络结合siamese和triplet结构从文档中提取关键句子；其次，利用VBD-LLaMA2-7B-50b模型进行抽象式摘要生成，最终产出综合摘要。在VN-MDS数据集上的实验中，该框架的ROUGE-2得分达到39.6%，超过了现有最先进基准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.12134v1",
      "published_date": "2024-09-18 16:56:06 UTC",
      "updated_date": "2024-09-18 16:56:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:06:59.924852"
    },
    {
      "arxiv_id": "2409.12122v1",
      "title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement",
      "title_zh": "翻译失败",
      "authors": [
        "An Yang",
        "Beichen Zhang",
        "Binyuan Hui",
        "Bofei Gao",
        "Bowen Yu",
        "Chengpeng Li",
        "Dayiheng Liu",
        "Jianhong Tu",
        "Jingren Zhou",
        "Junyang Lin",
        "Keming Lu",
        "Mingfeng Xue",
        "Runji Lin",
        "Tianyu Liu",
        "Xingzhang Ren",
        "Zhenru Zhang"
      ],
      "abstract": "In this report, we present a series of math-specific large language models:\nQwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the\nQwen2.5 series lies in integrating the philosophy of self-improvement\nthroughout the entire pipeline, from pre-training and post-training to\ninference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized\nto generate large-scale, high-quality mathematical data. (2) In the\npost-training phase, we develop a reward model (RM) by conducting massive\nsampling from Qwen2-Math-Instruct. This RM is then applied to the iterative\nevolution of data in supervised fine-tuning (SFT). With a stronger SFT model,\nit's possible to iteratively train and update the RM, which in turn guides the\nnext round of SFT data iteration. On the final SFT model, we employ the\nultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.\n(3) Furthermore, during the inference stage, the RM is used to guide sampling,\noptimizing the model's performance.\n  Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced\nmathematical reasoning capabilities, including Chain-of-Thought (CoT) and\nTool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics\ndatasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and\nAIME24, covering a range of difficulties from grade school level to math\ncompetition problems.",
      "tldr_zh": "该报告介绍了Qwen2.5-Math系列模型，包括Qwen2.5-Math和Qwen2.5-Math-Instruct（1.5B/7B/72B），通过自-improvement哲学在预训练、后训练和推理阶段实现模型优化。核心方法包括利用Qwen2-Math-Instruct生成高质量数学数据、在后训练中使用奖励模型（RM）进行监督微调（SFT）的迭代进化，以及在推理阶段通过RM指导采样以提升性能。模型支持中文和英文，具备高级数学推理能力，如Chain-of-Thought (CoT)和Tool-Integrated Reasoning (TIR)。在10个数学数据集（如GSM8K、MATH和GaoKao）上的评估显示，该系列模型在从小学到竞赛级别的难题中表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12122v1",
      "published_date": "2024-09-18 16:45:37 UTC",
      "updated_date": "2024-09-18 16:45:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:07:13.087801"
    },
    {
      "arxiv_id": "2409.12112v1",
      "title": "Pareto Data Framework: Steps Towards Resource-Efficient Decision Making Using Minimum Viable Data (MVD)",
      "title_zh": "Pareto 数据框架：利用最小可行数据 (MVD) 实现资源高效决策的步骤",
      "authors": [
        "Tashfain Ahmed",
        "Josh Siegel"
      ],
      "abstract": "This paper introduces the Pareto Data Framework, an approach for identifying\nand selecting the Minimum Viable Data (MVD) required for enabling machine\nlearning applications on constrained platforms such as embedded systems, mobile\ndevices, and Internet of Things (IoT) devices. We demonstrate that strategic\ndata reduction can maintain high performance while significantly reducing\nbandwidth, energy, computation, and storage costs. The framework identifies\nMinimum Viable Data (MVD) to optimize efficiency across resource-constrained\nenvironments without sacrificing performance. It addresses common inefficient\npractices in an IoT application such as overprovisioning of sensors and\noverprecision, and oversampling of signals, proposing scalable solutions for\noptimal sensor selection, signal extraction and transmission, and data\nrepresentation. An experimental methodology demonstrates effective acoustic\ndata characterization after downsampling, quantization, and truncation to\nsimulate reduced-fidelity sensors and network and storage constraints; results\nshows that performance can be maintained up to 95\\% with sample rates reduced\nby 75\\% and bit depths and clip length reduced by 50\\% which translates into\nsubstantial cost and resource reduction. These findings have implications on\nthe design and development of constrained systems. The paper also discusses\nbroader implications of the framework, including the potential to democratize\nadvanced AI technologies across IoT applications and sectors such as\nagriculture, transportation, and manufacturing to improve access and multiply\nthe benefits of data-driven insights.",
      "tldr_zh": "本论文引入了Pareto Data Framework，这是一种利用Minimum Viable Data (MVD)的方法，用于在资源受限平台（如嵌入式系统、移动设备和IoT设备）上实现高效的机器学习决策，从而减少带宽、能源、计算和存储成本，同时保持高性能。框架通过优化传感器选择、信号提取、传输和数据表示，解决IoT应用中的问题，如传感器过度配置和过度采样，提供可扩展的解决方案。实验结果显示，在声学数据测试中，通过将采样率减少75%并将位深度和剪辑长度减少50%，性能仍可保持高达95%，这显著降低了成本并为资源受限系统的设计带来深远影响。最终，该框架有望在农业、交通和制造等领域普及AI技术，提升数据驱动洞察的访问和益处。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12112v1",
      "published_date": "2024-09-18 16:31:19 UTC",
      "updated_date": "2024-09-18 16:31:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:07:25.450456"
    },
    {
      "arxiv_id": "2409.12106v3",
      "title": "Measuring Human and AI Values Based on Generative Psychometrics with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Haoran Ye",
        "Yuhang Xie",
        "Yuanyi Ren",
        "Hanjun Fang",
        "Xin Zhang",
        "Guojie Song"
      ],
      "abstract": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. The core idea is to dynamically parse\nunstructured texts into perceptions akin to static stimuli in traditional\npsychometrics, measure the value orientations they reveal, and aggregate the\nresults. Applying GPV to human-authored blogs, we demonstrate its stability,\nvalidity, and superiority over prior psychological tools. Then, extending GPV\nto LLM value measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI.",
      "tldr_zh": "本研究提出Generative Psychometrics for Values (GPV)，一种基于Large Language Models (LLMs)的数据驱动范式，用于测量人类和AI的值取向，其理论基础是文本揭示的选择性感知。GPV通过动态解析非结构化文本为类似于传统心理测验的刺激，来测量并聚合价值导向，在人类撰写的博客上验证了其稳定性、有效性和优于现有心理工具的表现。扩展到LLM值测量时，该方法支持上下文特定的评估，揭示了先前方法的响应偏差，并探索了不同价值系统对LLM安全性的预测影响，最终旨在通过AI增强心理测量并促进价值对齐的AI发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.12106v3",
      "published_date": "2024-09-18 16:26:22 UTC",
      "updated_date": "2025-03-06 08:18:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:07:36.709737"
    },
    {
      "arxiv_id": "2410.02800v1",
      "title": "Estimating Body Volume and Height Using 3D Data",
      "title_zh": "使用 3D 数据估计身体体积和高度",
      "authors": [
        "Vivek Ganesh Sonar",
        "Muhammad Tanveer Jan",
        "Mike Wells",
        "Abhijit Pandya",
        "Gabriela Engstrom",
        "Richard Shih",
        "Borko Furht"
      ],
      "abstract": "Accurate body weight estimation is critical in emergency medicine for proper\ndosing of weight-based medications, yet direct measurement is often impractical\nin urgent situations. This paper presents a non-invasive method for estimating\nbody weight by calculating total body volume and height using 3D imaging\ntechnology. A RealSense D415 camera is employed to capture high-resolution\ndepth maps of the patient, from which 3D models are generated. The Convex Hull\nAlgorithm is then applied to calculate the total body volume, with enhanced\naccuracy achieved by segmenting the point cloud data into multiple sections and\nsumming their individual volumes. The height is derived from the 3D model by\nidentifying the distance between key points on the body. This combined approach\nprovides an accurate estimate of body weight, improving the reliability of\nmedical interventions where precise weight data is unavailable. The proposed\nmethod demonstrates significant potential to enhance patient safety and\ntreatment outcomes in emergency settings.",
      "tldr_zh": "本研究提出了一种非侵入性方法，使用3D成像技术估算人体体积和高度，以解决紧急医学中体重测量不切实际的问题。方法采用RealSense D415相机捕获深度地图生成3D模型，然后应用Convex Hull Algorithm对点云数据进行分割和求和计算总体积，同时通过关键点距离估算高度。该方法显著提高了体重估算的准确性，在紧急场景中提升了医疗干预的可靠性，并展示了改善患者安全和治疗效果的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2410.02800v1",
      "published_date": "2024-09-18 16:20:46 UTC",
      "updated_date": "2024-09-18 16:20:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:07:47.450930"
    },
    {
      "arxiv_id": "2409.15364v1",
      "title": "VERA: Validation and Enhancement for Retrieval Augmented systems",
      "title_zh": "VERA：用于检索增强系统的验证和增强",
      "authors": [
        "Nitin Aravind Birur",
        "Tanay Baswa",
        "Divyanshu Kumar",
        "Jatan Loya",
        "Sahil Agarwal",
        "Prashanth Harshangi"
      ],
      "abstract": "Large language models (LLMs) exhibit remarkable capabilities but often\nproduce inaccurate responses, as they rely solely on their embedded knowledge.\nRetrieval-Augmented Generation (RAG) enhances LLMs by incorporating an external\ninformation retrieval system, supplying additional context along with the query\nto mitigate inaccuracies for a particular context. However, accuracy issues\nstill remain, as the model may rely on irrelevant documents or extrapolate\nincorrectly from its training knowledge. To assess and improve the performance\nof both the retrieval system and the LLM in a RAG framework, we propose\n\\textbf{VERA} (\\textbf{V}alidation and \\textbf{E}nhancement for\n\\textbf{R}etrieval \\textbf{A}ugmented systems), a system designed to: 1)\nEvaluate and enhance the retrieved context before response generation, and 2)\nEvaluate and refine the LLM-generated response to ensure precision and minimize\nerrors. VERA employs an evaluator-cum-enhancer LLM that first checks if\nexternal retrieval is necessary, evaluates the relevance and redundancy of the\nretrieved context, and refines it to eliminate non-essential information.\nPost-response generation, VERA splits the response into atomic statements,\nassesses their relevance to the query, and ensures adherence to the context.\nOur experiments demonstrate VERA's remarkable efficacy not only in improving\nthe performance of smaller open-source models, but also larger state-of-the art\nmodels. These enhancements underscore VERA's potential to produce accurate and\nrelevant responses, advancing the state-of-the-art in retrieval-augmented\nlanguage modeling. VERA's robust methodology, combining multiple evaluation and\nrefinement steps, effectively mitigates hallucinations and improves retrieval\nand response processes, making it a valuable tool for applications demanding\nhigh accuracy and reliability in information generation. .",
      "tldr_zh": "该研究提出 VERA 系统，用于验证和增强 Retrieval-Augmented Generation (RAG) 框架，以解决 Large Language Models (LLMs) 在依赖外部检索时可能出现的准确性问题，如无关文档依赖或错误外推。VERA 采用一个评估兼增强的 LLM，首先检查是否需要外部检索、评估检索上下文的相关性和冗余并进行精炼，然后在响应生成后，将响应拆分为原子语句，验证其与查询的相关性和上下文一致性。实验结果显示，VERA 显著提高了小型开源模型和大型最先进模型的性能，减少了幻觉并提升了响应准确性，使其成为高可靠性信息生成应用的宝贵工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.15364v1",
      "published_date": "2024-09-18 16:10:47 UTC",
      "updated_date": "2024-09-18 16:10:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:08:00.050004"
    },
    {
      "arxiv_id": "2409.12092v2",
      "title": "IMRL: Integrating Visual, Physical, Temporal, and Geometric Representations for Enhanced Food Acquisition",
      "title_zh": "IMRL：整合视觉、物理、时间和几何表示以增强食物获取",
      "authors": [
        "Rui Liu",
        "Zahiruddin Mahammad",
        "Amisha Bhaskar",
        "Pratap Tokekar"
      ],
      "abstract": "Robotic assistive feeding holds significant promise for improving the quality\nof life for individuals with eating disabilities. However, acquiring diverse\nfood items under varying conditions and generalizing to unseen food presents\nunique challenges. Existing methods that rely on surface-level geometric\ninformation (e.g., bounding box and pose) derived from visual cues (e.g.,\ncolor, shape, and texture) often lacks adaptability and robustness, especially\nwhen foods share similar physical properties but differ in visual appearance.\nWe employ imitation learning (IL) to learn a policy for food acquisition.\nExisting methods employ IL or Reinforcement Learning (RL) to learn a policy\nbased on off-the-shelf image encoders such as ResNet-50. However, such\nrepresentations are not robust and struggle to generalize across diverse\nacquisition scenarios. To address these limitations, we propose a novel\napproach, IMRL (Integrated Multi-Dimensional Representation Learning), which\nintegrates visual, physical, temporal, and geometric representations to enhance\nthe robustness and generalizability of IL for food acquisition. Our approach\ncaptures food types and physical properties (e.g., solid, semi-solid, granular,\nliquid, and mixture), models temporal dynamics of acquisition actions, and\nintroduces geometric information to determine optimal scooping points and\nassess bowl fullness. IMRL enables IL to adaptively adjust scooping strategies\nbased on context, improving the robot's capability to handle diverse food\nacquisition scenarios. Experiments on a real robot demonstrate our approach's\nrobustness and adaptability across various foods and bowl configurations,\nincluding zero-shot generalization to unseen settings. Our approach achieves\nimprovement up to $35\\%$ in success rate compared with the best-performing\nbaseline. More details can be found on our website\nhttps://ruiiu.github.io/imrl.",
      "tldr_zh": "该研究针对机器人辅助喂食的挑战，提出了一种新方法IMRL（Integrated Multi-Dimensional Representation Learning），以提升食物获取的鲁棒性和泛化能力。IMRL整合了visual、physical、temporal和geometric表示，捕捉食物类型（如固体、半固体、颗粒、液体和混合物）、动作的时间动态，以及几何信息来优化铲取点和评估碗满度，从而使imitation learning (IL)策略能够根据上下文自适应调整。实验在真实机器人上证明，IMRL在各种食物和碗配置中表现出色，包括zero-shot generalization到未见场景，成功率比最佳基线提高了高达35%。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12092v2",
      "published_date": "2024-09-18 16:09:06 UTC",
      "updated_date": "2025-03-18 15:32:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:08:12.667068"
    },
    {
      "arxiv_id": "2409.12087v3",
      "title": "Towards Interpretable End-Stage Renal Disease (ESRD) Prediction: Utilizing Administrative Claims Data with Explainable AI Techniques",
      "title_zh": "翻译失败",
      "authors": [
        "Yubo Li",
        "Saba Al-Sayouri",
        "Rema Padman"
      ],
      "abstract": "This study explores the potential of utilizing administrative claims data,\ncombined with advanced machine learning and deep learning techniques, to\npredict the progression of Chronic Kidney Disease (CKD) to End-Stage Renal\nDisease (ESRD). We analyze a comprehensive, 10-year dataset provided by a major\nhealth insurance organization to develop prediction models for multiple\nobservation windows using traditional machine learning methods such as Random\nForest and XGBoost as well as deep learning approaches such as Long Short-Term\nMemory (LSTM) networks. Our findings demonstrate that the LSTM model,\nparticularly with a 24-month observation window, exhibits superior performance\nin predicting ESRD progression, outperforming existing models in the\nliterature. We further apply SHapley Additive exPlanations (SHAP) analysis to\nenhance interpretability, providing insights into the impact of individual\nfeatures on predictions at the individual patient level. This study underscores\nthe value of leveraging administrative claims data for CKD management and\npredicting ESRD progression.",
      "tldr_zh": "本研究利用行政索赔数据结合机器学习和深度学习技术（如Random Forest、XGBoost和Long Short-Term Memory (LSTM)网络），预测Chronic Kidney Disease (CKD)进展到End-Stage Renal Disease (ESRD)，基于一个主要健康保险机构的10年数据集开发多观察窗口预测模型。结果表明，LSTM模型在24个月观察窗口下表现出色，预测性能优于文献中现有模型。通过SHAP (SHapley Additive exPlanations)分析增强模型可解释性，提供个体患者级别特征影响的洞见。该工作突显了行政索赔数据在CKD管理和ESRD预测中的重要价值。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10pages, 4 figures, AMIA 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.12087v3",
      "published_date": "2024-09-18 16:03:57 UTC",
      "updated_date": "2024-10-25 06:33:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:08:24.606595"
    },
    {
      "arxiv_id": "2409.12072v1",
      "title": "PAD-FT: A Lightweight Defense for Backdoor Attacks via Data Purification and Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Yukai Xu",
        "Yujie Gu",
        "Kouichi Sakurai"
      ],
      "abstract": "Backdoor attacks pose a significant threat to deep neural networks,\nparticularly as recent advancements have led to increasingly subtle\nimplantation, making the defense more challenging. Existing defense mechanisms\ntypically rely on an additional clean dataset as a standard reference and\ninvolve retraining an auxiliary model or fine-tuning the entire victim model.\nHowever, these approaches are often computationally expensive and not always\nfeasible in practical applications. In this paper, we propose a novel and\nlightweight defense mechanism, termed PAD-FT, that does not require an\nadditional clean dataset and fine-tunes only a very small part of the model to\ndisinfect the victim model. To achieve this, our approach first introduces a\nsimple data purification process to identify and select the most-likely clean\ndata from the poisoned training dataset. The self-purified clean dataset is\nthen used for activation clipping and fine-tuning only the last classification\nlayer of the victim model. By integrating data purification, activation\nclipping, and classifier fine-tuning, our mechanism PAD-FT demonstrates\nsuperior effectiveness across multiple backdoor attack methods and datasets, as\nconfirmed through extensive experimental evaluation.",
      "tldr_zh": "本文提出了一种轻量级防御机制 PAD-FT，用于对抗后门攻击（backdoor attacks），它无需额外的干净数据集，仅微调受害模型的一小部分，从而减少计算开销。PAD-FT 的方法包括先通过数据净化过程从受污染的训练数据集识别出最可能的干净数据，然后利用这些数据进行激活剪切（activation clipping）和仅微调最后一个分类层。实验结果表明，该机制在多种后门攻击方法和数据集上表现出优越的效能，为高效的深度神经网络防御提供了实用方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12072v1",
      "published_date": "2024-09-18 15:47:23 UTC",
      "updated_date": "2024-09-18 15:47:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:08:37.103970"
    },
    {
      "arxiv_id": "2409.12061v1",
      "title": "Generalized Robot Learning Framework",
      "title_zh": "泛化机器人学习框架",
      "authors": [
        "Jiahuan Yan",
        "Zhouyang Hong",
        "Yu Zhao",
        "Yu Tian",
        "Yunxin Liu",
        "Travis Davies",
        "Luhui Hu"
      ],
      "abstract": "Imitation based robot learning has recently gained significant attention in\nthe robotics field due to its theoretical potential for transferability and\ngeneralizability. However, it remains notoriously costly, both in terms of\nhardware and data collection, and deploying it in real-world environments\ndemands meticulous setup of robots and precise experimental conditions. In this\npaper, we present a low-cost robot learning framework that is both easily\nreproducible and transferable to various robots and environments. We\ndemonstrate that deployable imitation learning can be successfully applied even\nto industrial-grade robots, not just expensive collaborative robotic arms.\nFurthermore, our results show that multi-task robot learning is achievable with\nsimple network architectures and fewer demonstrations than previously thought\nnecessary. As the current evaluating method is almost subjective when it comes\nto real-world manipulation tasks, we propose Voting Positive Rate (VPR) - a\nnovel evaluation strategy that provides a more objective assessment of\nperformance. We conduct an extensive comparison of success rates across various\nself-designed tasks to validate our approach. To foster collaboration and\nsupport the robot learning community, we have open-sourced all relevant\ndatasets and model checkpoints, available at huggingface.co/ZhiChengAI.",
      "tldr_zh": "这篇论文针对基于模仿学习的机器人学习问题，指出其成本高昂且部署困难，提出一个低成本、易复制且可转移的框架，适用于各种机器人和环境。该框架证明了模仿学习可在工业级机器人上成功应用，并通过简单网络架构和较少演示实现多任务学习。论文引入了新的评估策略 Voting Positive Rate (VPR)，提供更客观的性能评估，并在多种自设计任务上进行了广泛实验比较；同时，开源了所有数据集和模型，以促进机器人学习社区的发展。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 2 figures. cs.RO",
      "pdf_url": "http://arxiv.org/pdf/2409.12061v1",
      "published_date": "2024-09-18 15:34:31 UTC",
      "updated_date": "2024-09-18 15:34:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:08:48.752916"
    },
    {
      "arxiv_id": "2409.12060v2",
      "title": "PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase Detection Models",
      "title_zh": "翻译失败",
      "authors": [
        "Andrianos Michail",
        "Simon Clematide",
        "Juri Opitz"
      ],
      "abstract": "The task of determining whether two texts are paraphrases has long been a\nchallenge in NLP. However, the prevailing notion of paraphrase is often quite\nsimplistic, offering only a limited view of the vast spectrum of paraphrase\nphenomena. Indeed, we find that evaluating models in a paraphrase dataset can\nleave uncertainty about their true semantic understanding. To alleviate this,\nwe create PARAPHRASUS, a benchmark designed for multi-dimensional assessment,\nbenchmarking and selection of paraphrase detection models. We find that\nparaphrase detection models under our fine-grained evaluation lens exhibit\ntrade-offs that cannot be captured through a single classification dataset.\nFurthermore, PARAPHRASUS allows prompt calibration for different use cases,\ntailoring LLM models to specific strictness levels. PARAPHRASUS includes 3\nchallenges spanning over 10 datasets, including 8 repurposed and 2 newly\nannotated; we release it along with a benchmarking library at\nhttps://github.com/impresso/paraphrasus",
      "tldr_zh": "本研究指出了现有 Paraphrase Detection 模型在评估中存在的局限性，即对 paraphrases 的定义过于简单，无法全面反映模型的语义理解。为此，作者开发了 PARAPHRASUS，这是一个多维度基准，用于评估、基准测试和选择 Paraphrase Detection 模型，通过细粒度评估揭示模型在不同方面的权衡，并支持针对特定用例的提示校准以调整 LLM models 的严格程度。PARAPHRASUS 涵盖 3 个挑战和 10 个数据集（包括 8 个改编数据集和 2 个新标注数据集），并附带开源基准库（https://github.com/impresso/paraphrasus），有助于提升模型的可靠性和适用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "to appear at COLING2025",
      "pdf_url": "http://arxiv.org/pdf/2409.12060v2",
      "published_date": "2024-09-18 15:33:48 UTC",
      "updated_date": "2024-12-16 10:09:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:09:00.624793"
    },
    {
      "arxiv_id": "2409.12059v4",
      "title": "MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Ningyuan Xi",
        "Xiaoyu Wang",
        "Yetao Wu",
        "Teng Chen",
        "Qingqing Gu",
        "Yue Zhao",
        "Jinxian Qu",
        "Zhonglin Jiang",
        "Yong Chen",
        "Luo Ji"
      ],
      "abstract": "Large Language Model can reasonably understand and generate human expressions\nbut may lack of thorough thinking and reasoning mechanisms. Recently there have\nbeen several studies which enhance the thinking ability of language models but\nmost of them are not data-driven or training-based. In this paper, we are\nmotivated by the cognitive mechanism in the natural world, and design a novel\nmodel architecture called TaS which allows it to first consider the thoughts\nand then express the response based upon the query. We design several pipelines\nto annotate or generate the thought contents from prompt-response samples, then\nadd language heads in a middle layer which behaves as the thinking layer. We\ntrain the language model by the thoughts-augmented data and successfully let\nthe thinking layer automatically generate reasonable thoughts and finally\noutput more reasonable responses. Both qualitative examples and quantitative\nresults validate the effectiveness and performance of TaS. Our code is\navailable at https://anonymous.4open.science/r/TadE.",
      "tldr_zh": "本研究针对大型语言模型（Large Language Models）在思考和推理机制上的不足，提出了一种模块化模型架构MeTHanol，该架构通过中间层思考（Intermediate Layer Thinking）、解码和引导推理（Bootstrapping Reasoning）设计TaS系统，先生成想法（thoughts）再基于查询输出响应。研究者开发了多种管道从提示-响应样本中标注或生成想法内容，并在中间层添加语言头进行训练，使模型能够自动产生合理的想法并提升响应质量。实验结果显示，TaS在定性和定量评估中表现出色，证明了其有效性，相关代码已在开源平台上发布。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12059v4",
      "published_date": "2024-09-18 15:32:48 UTC",
      "updated_date": "2025-04-25 16:03:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:09:13.292259"
    },
    {
      "arxiv_id": "2409.12038v1",
      "title": "A Unified Framework for Neural Computation and Learning Over Time",
      "title_zh": "一种用于神经计算和随时间学习的统一框架",
      "authors": [
        "Stefano Melacci",
        "Alessandro Betti",
        "Michele Casoni",
        "Tommaso Guidi",
        "Matteo Tiezzi",
        "Marco Gori"
      ],
      "abstract": "This paper proposes Hamiltonian Learning, a novel unified framework for\nlearning with neural networks \"over time\", i.e., from a possibly infinite\nstream of data, in an online manner, without having access to future\ninformation. Existing works focus on the simplified setting in which the stream\nhas a known finite length or is segmented into smaller sequences, leveraging\nwell-established learning strategies from statistical machine learning. In this\npaper, the problem of learning over time is rethought from scratch, leveraging\ntools from optimal control theory, which yield a unifying view of the temporal\ndynamics of neural computations and learning. Hamiltonian Learning is based on\ndifferential equations that: (i) can be integrated without the need of external\nsoftware solvers; (ii) generalize the well-established notion of gradient-based\nlearning in feed-forward and recurrent networks; (iii) open to novel\nperspectives. The proposed framework is showcased by experimentally proving how\nit can recover gradient-based learning, comparing it to out-of-the box\noptimizers, and describing how it is flexible enough to switch from fully-local\nto partially/non-local computational schemes, possibly distributed over\nmultiple devices, and BackPropagation without storing activations. Hamiltonian\nLearning is easy to implement and can help researches approach in a principled\nand innovative manner the problem of learning over time.",
      "tldr_zh": "本文提出了一种名为Hamiltonian Learning的统一框架，用于神经网络从可能无限的数据流中进行在线学习，而无需访问未来信息。该框架基于最优控制理论的微分方程，支持无需外部求解器的积分，并推广了gradient-based learning在前向和循环网络中的应用。实验证明，该框架能恢复gradient-based学习，与现有优化器相比表现出色，并灵活支持从完全本地到分布式计算方案，包括不存储激活的BackPropagation。该方法易于实现，为研究时间上学习问题提供了创新性和原则性的新视角。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12038v1",
      "published_date": "2024-09-18 14:57:13 UTC",
      "updated_date": "2024-09-18 14:57:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:09:28.084266"
    },
    {
      "arxiv_id": "2409.12033v1",
      "title": "Topological Deep Learning with State-Space Models: A Mamba Approach for Simplicial Complexes",
      "title_zh": "使用状态空间模型的拓扑深度学习：一种",
      "authors": [
        "Marco Montagna",
        "Simone Scardapane",
        "Lev Telyatnikov"
      ],
      "abstract": "Graph Neural Networks based on the message-passing (MP) mechanism are a\ndominant approach for handling graph-structured data. However, they are\ninherently limited to modeling only pairwise interactions, making it difficult\nto explicitly capture the complexity of systems with $n$-body relations. To\naddress this, topological deep learning has emerged as a promising field for\nstudying and modeling higher-order interactions using various topological\ndomains, such as simplicial and cellular complexes. While these new domains\nprovide powerful representations, they introduce new challenges, such as\neffectively modeling the interactions among higher-order structures through\nhigher-order MP. Meanwhile, structured state-space sequence models have proven\nto be effective for sequence modeling and have recently been adapted for graph\ndata by encoding the neighborhood of a node as a sequence, thereby avoiding the\nMP mechanism. In this work, we propose a novel architecture designed to operate\nwith simplicial complexes, utilizing the Mamba state-space model as its\nbackbone. Our approach generates sequences for the nodes based on the\nneighboring cells, enabling direct communication between all higher-order\nstructures, regardless of their rank. We extensively validate our model,\ndemonstrating that it achieves competitive performance compared to\nstate-of-the-art models developed for simplicial complexes.",
      "tldr_zh": "本研究指出，基于消息-passing (MP) 机制的 Graph Neural Networks 仅能处理成对交互，无法有效捕捉 n-body 关系，因此提出使用 Topological Deep Learning 来建模更高阶交互，如 simplicial complexes。作者开发了一种新架构，以 Mamba state-space model 为骨干，通过基于相邻单元生成节点序列，实现不同阶结构的直接通信，从而避免传统 MP 的局限。实验结果显示，该模型在 simplicial complexes 上表现出色，与最先进模型的性能相当。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12033v1",
      "published_date": "2024-09-18 14:49:25 UTC",
      "updated_date": "2024-09-18 14:49:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:09:37.024928"
    },
    {
      "arxiv_id": "2409.12020v1",
      "title": "Promise and Peril of Collaborative Code Generation Models: Balancing Effectiveness and Memorization",
      "title_zh": "协作代码生成模型的机遇与风险：平衡",
      "authors": [
        "Zhi Chen",
        "Lingxiao Jiang"
      ],
      "abstract": "In the rapidly evolving field of machine learning, training models with\ndatasets from various locations and organizations presents significant\nchallenges due to privacy and legal concerns. The exploration of effective\ncollaborative training settings capable of leveraging valuable knowledge from\ndistributed and isolated datasets is increasingly crucial. This study\ninvestigates key factors that impact the effectiveness of collaborative\ntraining methods in code next-token prediction, as well as the correctness and\nutility of the generated code, demonstrating the promise of such methods.\nAdditionally, we evaluate the memorization of different participant training\ndata across various collaborative training settings, including centralized,\nfederated, and incremental training, highlighting their potential risks in\nleaking data. Our findings indicate that the size and diversity of code\ndatasets are pivotal factors influencing the success of collaboratively trained\ncode models. We show that federated learning achieves competitive performance\ncompared to centralized training while offering better data protection, as\nevidenced by lower memorization ratios in the generated code. However,\nfederated learning can still produce verbatim code snippets from hidden\ntraining data, potentially violating privacy or copyright. Our study further\nexplores effectiveness and memorization patterns in incremental learning,\nemphasizing the sequence in which individual participant datasets are\nintroduced. We also identify cross-organizational clones as a prevalent\nchallenge in both centralized and federated learning scenarios. Our findings\nhighlight the persistent risk of data leakage during inference, even when\ntraining data remains unseen. We conclude with recommendations for\npractitioners and researchers to optimize multisource datasets, propelling\ncross-organizational collaboration forward.",
      "tldr_zh": "这篇论文探讨了协作式代码生成模型的潜在优势与风险，重点评估了在代码下一 token 预测任务中，集中式、federated learning 和 incremental training 等训练方法的效果与记忆化问题。研究发现，数据集的大小和多样性是影响协作训练成功的关键因素，其中 federated learning 能提供与集中式训练相当的性能，同时降低记忆化比率以更好地保护数据隐私。然而，federated learning 仍可能泄露隐藏训练数据，如产生逐字代码片段，并存在跨组织代码克隆的风险；论文最终提出优化多源数据集的建议，以推动安全的跨组织协作。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Paper accepted to the ASE 2024 Conference Research Track",
      "pdf_url": "http://arxiv.org/pdf/2409.12020v1",
      "published_date": "2024-09-18 14:30:48 UTC",
      "updated_date": "2024-09-18 14:30:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:09:48.753936"
    },
    {
      "arxiv_id": "2409.12005v2",
      "title": "Representing Positional Information in Generative World Models for Object Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Stefano Ferraro",
        "Pietro Mazzaglia",
        "Tim Verbelen",
        "Bart Dhoedt",
        "Sai Rajeswar"
      ],
      "abstract": "Object manipulation capabilities are essential skills that set apart embodied\nagents engaging with the world, especially in the realm of robotics. The\nability to predict outcomes of interactions with objects is paramount in this\nsetting. While model-based control methods have started to be employed for\ntackling manipulation tasks, they have faced challenges in accurately\nmanipulating objects. As we analyze the causes of this limitation, we identify\nthe cause of underperformance in the way current world models represent crucial\npositional information, especially about the target's goal specification for\nobject positioning tasks. We introduce a general approach that empowers world\nmodel-based agents to effectively solve object-positioning tasks. We propose\ntwo declinations of this approach for generative world models:\nposition-conditioned (PCP) and latent-conditioned (LCP) policy learning. In\nparticular, LCP employs object-centric latent representations that explicitly\ncapture object positional information for goal specification. This naturally\nleads to the emergence of multimodal capabilities, enabling the specification\nof goals through spatial coordinates or a visual goal. Our methods are\nrigorously evaluated across several manipulation environments, showing\nfavorable performance compared to current model-based control approaches.",
      "tldr_zh": "这篇论文探讨了生成式世界模型(Generative World Models)在对象操作(Object Manipulation)任务中表示位置信息的问题，指出现有方法在目标位置指定上存在不足，导致操作精度低下。作者提出一种通用方法，包括position-conditioned (PCP)和latent-conditioned (LCP)政策学习，其中LCP利用对象中心化潜在表示显式捕获位置信息，支持通过空间坐标或视觉目标的多模态能力。实验在多个操作环境中验证了该方法的有效性，表现出比现有模型驱动控制方法更优的性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12005v2",
      "published_date": "2024-09-18 14:19:50 UTC",
      "updated_date": "2024-09-19 07:38:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:10:01.016557"
    },
    {
      "arxiv_id": "2409.12001v1",
      "title": "Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning",
      "title_zh": "以数据为核心的离线多智能体强化学习",
      "authors": [
        "Claude Formanek",
        "Louise Beyers",
        "Callum Rhys Tilbury",
        "Jonathan P. Shock",
        "Arnu Pretorius"
      ],
      "abstract": "Offline multi-agent reinforcement learning (MARL) is an exciting direction of\nresearch that uses static datasets to find optimal control policies for\nmulti-agent systems. Though the field is by definition data-driven, efforts\nhave thus far neglected data in their drive to achieve state-of-the-art\nresults. We first substantiate this claim by surveying the literature, showing\nhow the majority of works generate their own datasets without consistent\nmethodology and provide sparse information about the characteristics of these\ndatasets. We then show why neglecting the nature of the data is problematic,\nthrough salient examples of how tightly algorithmic performance is coupled to\nthe dataset used, necessitating a common foundation for experiments in the\nfield. In response, we take a big step towards improving data usage and data\nawareness in offline MARL, with three key contributions: (1) a clear guideline\nfor generating novel datasets; (2) a standardisation of over 80 existing\ndatasets, hosted in a publicly available repository, using a consistent storage\nformat and easy-to-use API; and (3) a suite of analysis tools that allow us to\nunderstand these datasets better, aiding further development.",
      "tldr_zh": "该论文强调了在离线多智能体强化学习(Offline MARL)中数据的重要性，通过文献调查发现，大多数研究自行生成数据集但缺乏一致方法和详细特征描述，导致算法性能高度依赖数据集而缺乏统一基础。作者证明了忽略数据性质的潜在问题，并提出三大关键贡献：（1）提供生成新数据集的清晰指南；（2）标准化超过80个现有数据集，包括公共仓库、一致存储格式和易用API；（3）开发分析工具以更好地理解这些数据集，促进领域发展。这些努力为Offline MARL实验建立了一个共同基础，提升了数据驱动研究的可靠性和可比性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12001v1",
      "published_date": "2024-09-18 14:13:24 UTC",
      "updated_date": "2024-09-18 14:13:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:10:13.827095"
    },
    {
      "arxiv_id": "2409.11992v1",
      "title": "Additive-feature-attribution methods: a review on explainable artificial intelligence for fluid dynamics and heat transfer",
      "title_zh": "加性特征归因方法：关于用于流体动力学和热传输的可解释人工智能的综述",
      "authors": [
        "Andrés Cremades",
        "Sergio Hoyas",
        "Ricardo Vinuesa"
      ],
      "abstract": "The use of data-driven methods in fluid mechanics has surged dramatically in\nrecent years due to their capacity to adapt to the complex and multi-scale\nnature of turbulent flows, as well as to detect patterns in large-scale\nsimulations or experimental tests. In order to interpret the relationships\ngenerated in the models during the training process, numerical attributions\nneed to be assigned to the input features. One important example are the\nadditive-feature-attribution methods. These explainability methods link the\ninput features with the model prediction, providing an interpretation based on\na linear formulation of the models. The SHapley Additive exPlanations (SHAP\nvalues) are formulated as the only possible interpretation that offers a unique\nsolution for understanding the model. In this manuscript, the\nadditive-feature-attribution methods are presented, showing four common\nimplementations in the literature: kernel SHAP, tree SHAP, gradient SHAP, and\ndeep SHAP. Then, the main applications of the additive-feature-attribution\nmethods are introduced, dividing them into three main groups: turbulence\nmodeling, fluid-mechanics fundamentals, and applied problems in fluid dynamics\nand heat transfer. This review shows thatexplainability techniques, and in\nparticular additive-feature-attribution methods, are crucial for implementing\ninterpretable and physics-compliant deep-learning models in the fluid-mechanics\nfield.",
      "tldr_zh": "本文回顾了 additive-feature-attribution 方法在流体力学和热传输领域的应用，这些方法通过将输入特征与模型预测关联，提供基于线性公式的解释，其中 SHAP values 被视为唯一提供独特解决方案的解释工具。论文介绍了四种常见实现，包括 kernel SHAP、tree SHAP、gradient SHAP 和 deep SHAP，并将这些方法的实际应用分为三类：湍流建模、流体力学基础以及流体力学和热传输的实际问题。通过这些技术，可实现可解释且符合物理学的深度学习模型，在流体力学研究中发挥关键作用。",
      "categories": [
        "physics.flu-dyn",
        "cs.AI"
      ],
      "primary_category": "physics.flu-dyn",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11992v1",
      "published_date": "2024-09-18 13:59:02 UTC",
      "updated_date": "2024-09-18 13:59:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:10:25.521920"
    },
    {
      "arxiv_id": "2409.12720v1",
      "title": "FAST GDRNPP: Improving the Speed of State-of-the-Art 6D Object Pose Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas Pöllabauer",
        "Ashwin Pramod",
        "Volker Knauthe",
        "Michael Wahl"
      ],
      "abstract": "6D object pose estimation involves determining the three-dimensional\ntranslation and rotation of an object within a scene and relative to a chosen\ncoordinate system. This problem is of particular interest for many practical\napplications in industrial tasks such as quality control, bin picking, and\nrobotic manipulation, where both speed and accuracy are critical for real-world\ndeployment. Current models, both classical and deep-learning-based, often\nstruggle with the trade-off between accuracy and latency. Our research focuses\non enhancing the speed of a prominent state-of-the-art deep learning model,\nGDRNPP, while keeping its high accuracy. We employ several techniques to reduce\nthe model size and improve inference time. These techniques include using\nsmaller and quicker backbones, pruning unnecessary parameters, and distillation\nto transfer knowledge from a large, high-performing model to a smaller, more\nefficient student model. Our findings demonstrate that the proposed\nconfiguration maintains accuracy comparable to the state-of-the-art while\nsignificantly improving inference time. This advancement could lead to more\nefficient and practical applications in various industrial scenarios, thereby\nenhancing the overall applicability of 6D Object Pose Estimation models in\nreal-world settings.",
      "tldr_zh": "这篇论文针对 6D object pose estimation（涉及对象的三维平移和旋转），提出 FAST GDRNPP 方法，以提升最先进模型 GDRNPP 的推理速度，同时保持高准确性。研究采用的技术包括使用更小、更快的 backbone、参数修剪以及知识蒸馏，将大型模型的知识转移到更高效的学生模型中。实验结果表明，新配置的准确性与 state-of-the-art 水平相当，但推理时间显著改善，从而使 6D object pose estimation 在工业应用如质量控制和机器人操作中更具实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12720v1",
      "published_date": "2024-09-18 12:30:02 UTC",
      "updated_date": "2024-09-18 12:30:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:10:36.419868"
    },
    {
      "arxiv_id": "2409.12213v1",
      "title": "SemAI: Semantic Artificial Intelligence-enhanced DNA storage for Internet-of-Things",
      "title_zh": "SemAI：语义人工智能增强的 DNA 存储用于物联网",
      "authors": [
        "Wenfeng Wu",
        "Luping Xiang",
        "Qiang Liu",
        "Kun Yang"
      ],
      "abstract": "In the wake of the swift evolution of technologies such as the Internet of\nThings (IoT), the global data landscape undergoes an exponential surge,\npropelling DNA storage into the spotlight as a prospective medium for\ncontemporary cloud storage applications. This paper introduces a Semantic\nArtificial Intelligence-enhanced DNA storage (SemAI-DNA) paradigm,\ndistinguishing itself from prevalent deep learning-based methodologies through\ntwo key modifications: 1) embedding a semantic extraction module at the\nencoding terminus, facilitating the meticulous encoding and storage of nuanced\nsemantic information; 2) conceiving a forethoughtful multi-reads filtering\nmodel at the decoding terminus, leveraging the inherent multi-copy propensity\nof DNA molecules to bolster system fault tolerance, coupled with a\nstrategically optimized decoder's architectural framework. Numerical results\ndemonstrate the SemAI-DNA's efficacy, attaining 2.61 dB Peak Signal-to-Noise\nRatio (PSNR) gain and 0.13 improvement in Structural Similarity Index (SSIM)\nover conventional deep learning-based approaches.",
      "tldr_zh": "本文提出 SemAI-DNA 范式，这是一种基于语义人工智能增强的 DNA 存储解决方案，针对 IoT 驱动的数据爆炸性增长，提供更高效的云存储方法。SemAI-DNA 通过两个关键改进提升性能：1) 在编码端嵌入 semantic extraction module，以精确编码和存储细微语义信息；2) 在解码端设计 multi-reads filtering model，利用 DNA 分子的多拷贝特性增强系统容错性，并优化解码器架构。实验结果显示，与传统深度学习-based 方法相比，SemAI-DNA 在 PSNR 上获得 2.61 dB 的提升，在 SSIM 上改善 0.13，进一步证明了其在 DNA 存储领域的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12213v1",
      "published_date": "2024-09-18 12:21:58 UTC",
      "updated_date": "2024-09-18 12:21:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:10:49.898607"
    },
    {
      "arxiv_id": "2409.11905v2",
      "title": "AlignBot: Aligning VLM-powered Customized Task Planning with User Reminders Through Fine-Tuning for Household Robots",
      "title_zh": "AlignBot: 通过微调将 VLM 驱动的定制任务规划与用户提醒对齐，用于家用机器人",
      "authors": [
        "Zhaxizhuoma Zhaxizhuoma",
        "Pengan Chen",
        "Ziniu Wu",
        "Jiawei Sun",
        "Dong Wang",
        "Peng Zhou",
        "Nieqing Cao",
        "Yan Ding",
        "Bin Zhao",
        "Xuelong Li"
      ],
      "abstract": "This paper presents AlignBot, a novel framework designed to optimize\nVLM-powered customized task planning for household robots by effectively\naligning with user reminders. In domestic settings, aligning task planning with\nuser reminders poses significant challenges due to the limited quantity,\ndiversity, and multimodal nature of the reminders. To address these challenges,\nAlignBot employs a fine-tuned LLaVA-7B model, functioning as an adapter for\nGPT-4o. This adapter model internalizes diverse forms of user reminders-such as\npersonalized preferences, corrective guidance, and contextual assistance-into\nstructured instruction-formatted cues that prompt GPT-4o in generating\ncustomized task plans. Additionally, AlignBot integrates a dynamic retrieval\nmechanism that selects task-relevant historical successes as prompts for\nGPT-4o, further enhancing task planning accuracy. To validate the effectiveness\nof AlignBot, experiments are conducted in real-world household environments,\nwhich are constructed within the laboratory to replicate typical household\nsettings. A multimodal dataset with over 1,500 entries derived from volunteer\nreminders is used for training and evaluation. The results demonstrate that\nAlignBot significantly improves customized task planning, outperforming\nexisting LLM- and VLM-powered planners by interpreting and aligning with user\nreminders, achieving 86.8% success rate compared to the vanilla GPT-4o baseline\nat 21.6%, reflecting a 65% improvement and over four times greater\neffectiveness. Supplementary materials are available at:\nhttps://yding25.com/AlignBot/",
      "tldr_zh": "本研究提出 AlignBot，一个创新框架，用于优化 VLM-powered 定制任务规划，使其与用户提醒更好地对齐，从而提升家用机器人的性能。该框架通过 fine-tuning LLaVA-7B 模型作为 GPT-4o 的适配器，将用户提醒（如个性化偏好、纠正指导和上下文辅助）转化为结构化的指令提示，并整合动态检索机制利用任务相关的历史成功作为额外提示。实验在模拟真实家庭环境中进行，使用超过1500条多模态数据集进行训练和评估，结果显示 AlignBot 的成功率达86.8%，比基线 GPT-4o 的21.6% 高出65%，效果提升超过四倍。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11905v2",
      "published_date": "2024-09-18 12:05:30 UTC",
      "updated_date": "2025-03-21 04:40:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:11:00.707192"
    },
    {
      "arxiv_id": "2409.11904v2",
      "title": "Finding the Subjective Truth: Collecting 2 Million Votes for Comprehensive Gen-AI Model Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Dimitrios Christodoulou",
        "Mads Kuhlmann-Jørgensen"
      ],
      "abstract": "Efficiently evaluating the performance of text-to-image models is difficult\nas it inherently requires subjective judgment and human preference, making it\nhard to compare different models and quantify the state of the art. Leveraging\nRapidata's technology, we present an efficient annotation framework that\nsources human feedback from a diverse, global pool of annotators. Our study\ncollected over 2 million annotations across 4,512 images, evaluating four\nprominent models (DALL-E 3, Flux.1, MidJourney, and Stable Diffusion) on style\npreference, coherence, and text-to-image alignment. We demonstrate that our\napproach makes it feasible to comprehensively rank image generation models\nbased on a vast pool of annotators and show that the diverse annotator\ndemographics reflect the world population, significantly decreasing the risk of\nbiases.",
      "tldr_zh": "本研究提出了一种高效的标注框架，利用 Rapidata 的技术，从全球多样化的标注者那里收集人类反馈，以评估文本到图像生成模型的性能。该框架收集了超过 200 万个标注，针对 4,512 张图像，对 DALL-E 3、Flux.1、MidJourney 和 Stable Diffusion 等四种模型在风格偏好、coherence 和 text-to-image alignment 方面的表现进行了全面评估。结果显示，这种方法能够基于大量标注者数据对模型进行可靠排名，并显著降低了偏见风险，从而为生成式 AI 模型的客观比较提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11904v2",
      "published_date": "2024-09-18 12:02:20 UTC",
      "updated_date": "2024-10-15 14:23:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:11:12.555762"
    },
    {
      "arxiv_id": "2409.11887v2",
      "title": "DocMamba: Efficient Document Pre-training with State Space Model",
      "title_zh": "翻译失败",
      "authors": [
        "Pengfei Hu",
        "Zhenrong Zhang",
        "Jiefeng Ma",
        "Shuhang Liu",
        "Jun Du",
        "Jianshu Zhang"
      ],
      "abstract": "In recent years, visually-rich document understanding has attracted\nincreasing attention. Transformer-based pre-trained models have become the\nmainstream approach, yielding significant performance gains in this field.\nHowever, the self-attention mechanism's quadratic computational complexity\nhinders their efficiency and ability to process long documents. In this paper,\nwe present DocMamba, a novel framework based on the state space model. It is\ndesigned to reduce computational complexity to linear while preserving global\nmodeling capabilities. To further enhance its effectiveness in document\nprocessing, we introduce the Segment-First Bidirectional Scan (SFBS) to capture\ncontiguous semantic information. Experimental results demonstrate that DocMamba\nachieves new state-of-the-art results on downstream datasets such as FUNSD,\nCORD, and SORIE, while significantly improving speed and reducing memory usage.\nNotably, experiments on the HRDoc confirm DocMamba's potential for length\nextrapolation.",
      "tldr_zh": "该论文提出DocMamba，一种基于State Space Model的文档预训练框架，旨在解决Transformer模型的自注意力机制导致的二次方计算复杂度问题，从而实现线性复杂度并保留全局建模能力。为提升文档处理效果，DocMamba引入Segment-First Bidirectional Scan (SFBS)来捕获连续语义信息。实验结果显示，该框架在FUNSD、CORD和SROIE等下游数据集上达到新的最先进性能，同时显著提高处理速度、减少内存使用，并在HRDoc上证明了长度外推潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11887v2",
      "published_date": "2024-09-18 11:34:28 UTC",
      "updated_date": "2025-02-10 07:31:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:11:25.082637"
    },
    {
      "arxiv_id": "2409.11863v2",
      "title": "LEMMo-Plan: LLM-Enhanced Learning from Multi-Modal Demonstration for Planning Sequential Contact-Rich Manipulation Tasks",
      "title_zh": "LEMMo-Plan：LLM 增强的多模态演示学习，用于规划顺序接触丰富的操作任务",
      "authors": [
        "Kejia Chen",
        "Zheng Shen",
        "Yue Zhang",
        "Lingyun Chen",
        "Fan Wu",
        "Zhenshan Bing",
        "Sami Haddadin",
        "Alois Knoll"
      ],
      "abstract": "Large Language Models (LLMs) have gained popularity in task planning for\nlong-horizon manipulation tasks. To enhance the validity of LLM-generated\nplans, visual demonstrations and online videos have been widely employed to\nguide the planning process. However, for manipulation tasks involving subtle\nmovements but rich contact interactions, visual perception alone may be\ninsufficient for the LLM to fully interpret the demonstration. Additionally,\nvisual data provides limited information on force-related parameters and\nconditions, which are crucial for effective execution on real robots. In this\npaper, we introduce an in-context learning framework that incorporates tactile\nand force-torque information from human demonstrations to enhance LLMs' ability\nto generate plans for new task scenarios. We propose a bootstrapped reasoning\npipeline that sequentially integrates each modality into a comprehensive task\nplan. This task plan is then used as a reference for planning in new task\nconfigurations. Real-world experiments on two different sequential manipulation\ntasks demonstrate the effectiveness of our framework in improving LLMs'\nunderstanding of multi-modal demonstrations and enhancing the overall planning\nperformance.",
      "tldr_zh": "该论文提出LEMMo-Plan框架，利用大语言模型(LLMs)结合多模态演示（包括视觉、触觉和力矩信息）来提升对顺序接触丰富操作任务的规划能力。框架通过in-context学习和bootstrapped reasoning pipeline顺序整合各模态数据，生成更全面的任务计划，以弥补视觉感知在微妙动作和力相关参数方面的不足。实世界实验在两个不同顺序操作任务上验证了该方法，提高了LLMs对多模态演示的理解和整体规划性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11863v2",
      "published_date": "2024-09-18 10:36:47 UTC",
      "updated_date": "2025-03-10 18:24:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:11:36.390679"
    },
    {
      "arxiv_id": "2409.11860v1",
      "title": "Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for Large-Scale Product Retrieval Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Kasra Hosseini",
        "Thomas Kober",
        "Josip Krapac",
        "Roland Vollgraf",
        "Weiwei Cheng",
        "Ana Peleteiro Ramallo"
      ],
      "abstract": "Evaluating production-level retrieval systems at scale is a crucial yet\nchallenging task due to the limited availability of a large pool of\nwell-trained human annotators. Large Language Models (LLMs) have the potential\nto address this scaling issue and offer a viable alternative to humans for the\nbulk of annotation tasks. In this paper, we propose a framework for assessing\nthe product search engines in a large-scale e-commerce setting, leveraging\nMultimodal LLMs for (i) generating tailored annotation guidelines for\nindividual queries, and (ii) conducting the subsequent annotation task. Our\nmethod, validated through deployment on a large e-commerce platform,\ndemonstrates comparable quality to human annotations, significantly reduces\ntime and cost, facilitates rapid problem discovery, and provides an effective\nsolution for production-level quality control at scale.",
      "tldr_zh": "这篇论文提出了一种框架，名为“Retrieve, Annotate, Evaluate, Repeat”，利用多模态大型语言模型 (Multimodal LLMs) 来评估大规模电子商务产品检索系统，以解决人类注解者短缺的挑战。该框架通过生成针对特定查询的个性化注解指南并自动进行注解任务，实现了高效的评估过程。在大型电子商务平台的实际部署中，该方法显示出与人类注解质量相当的性能，同时显著减少了时间和成本，并为快速问题发现和生产级质量控制提供了有效解决方案。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.ET",
        "cs.HC"
      ],
      "primary_category": "cs.IR",
      "comment": "13 pages, 5 figures, 4 Tables",
      "pdf_url": "http://arxiv.org/pdf/2409.11860v1",
      "published_date": "2024-09-18 10:30:50 UTC",
      "updated_date": "2024-09-18 10:30:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:11:48.302954"
    },
    {
      "arxiv_id": "2409.13764v1",
      "title": "Local Explanations and Self-Explanations for Assessing Faithfulness in black-box LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Christos Fragkathoulas",
        "Odysseas S. Chlapanis"
      ],
      "abstract": "This paper introduces a novel task to assess the faithfulness of large\nlanguage models (LLMs) using local perturbations and self-explanations. Many\nLLMs often require additional context to answer certain questions correctly.\nFor this purpose, we propose a new efficient alternative explainability\ntechnique, inspired by the commonly used leave-one-out approach. Using this\napproach, we identify the sufficient and necessary parts for the LLM to\ngenerate correct answers, serving as explanations. We propose a metric for\nassessing faithfulness that compares these crucial parts with the\nself-explanations of the model. Using the Natural Questions dataset, we\nvalidate our approach, demonstrating its effectiveness in explaining model\ndecisions and assessing faithfulness.",
      "tldr_zh": "这篇论文提出了一种新任务，利用局部扰动和自解释来评估黑箱大型语言模型 (LLMs) 的忠实度，旨在解决模型在需要额外上下文时可能出错的问题。作者引入了一种基于 leave-one-out 方法的解释技术，通过识别 LLM 生成正确答案的必要和充分部分来生成解释。论文还提出一个评估指标，将这些关键部分与模型的自解释进行比较，以量化忠实度。在 Natural Questions 数据集上验证显示，该方法在解释模型决策和评估忠实度方面表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13764v1",
      "published_date": "2024-09-18 10:16:45 UTC",
      "updated_date": "2024-09-18 10:16:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:12:00.216766"
    },
    {
      "arxiv_id": "2410.02795v1",
      "title": "TaCIE: Enhancing Instruction Comprehension in Large Language Models through Task-Centred Instruction Evolution",
      "title_zh": "翻译失败",
      "authors": [
        "Jiuding Yang",
        "Shengyao Lu",
        "Weidong Guo",
        "Xiangyang Li",
        "Kaitong Yang",
        "Yu Xu",
        "Di Niu"
      ],
      "abstract": "Large Language Models (LLMs) require precise alignment with complex\ninstructions to optimize their performance in real-world applications. As the\ndemand for refined instruction tuning data increases, traditional methods that\nevolve simple seed instructions often struggle to effectively enhance\ncomplexity or manage difficulty scaling across various domains. Our innovative\napproach, Task-Centered Instruction Evolution (TaCIE), addresses these\nshortcomings by redefining instruction evolution from merely evolving seed\ninstructions to a more dynamic and comprehensive combination of elements. TaCIE\nstarts by deconstructing complex instructions into their fundamental\ncomponents. It then generates and integrates new elements with the original\nones, reassembling them into more sophisticated instructions that progressively\nincrease in difficulty, diversity, and complexity. Applied across multiple\ndomains, LLMs fine-tuned with these evolved instructions have substantially\noutperformed those tuned with conventional methods, marking a significant\nadvancement in instruction-based model fine-tuning.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)处理复杂指令的挑战，提出了一种创新方法TaCIE（Task-Centered Instruction Evolution），通过从分解指令基本组件开始，并生成并整合新元素，动态重组指令以逐步提升难度、多样性和复杂性。TaCIE超越传统指令演化方法，在多个领域应用后，使LLMs在微调后显著提升性能。实验结果显示，与常规方法相比，TaCIE显著提高了指令理解和模型优化效果，为指令调优领域带来重要进展。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.02795v1",
      "published_date": "2024-09-18 10:06:28 UTC",
      "updated_date": "2024-09-18 10:06:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:12:12.429192"
    },
    {
      "arxiv_id": "2409.11844v1",
      "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts",
      "title_zh": "翻译失败",
      "authors": [
        "Tianle Gu",
        "Kexin Huang",
        "Ruilin Luo",
        "Yuanqi Yao",
        "Yujiu Yang",
        "Yan Teng",
        "Yingchun Wang"
      ],
      "abstract": "Large Language Models (LLMs) can memorize sensitive information, raising\nconcerns about potential misuse. LLM Unlearning, a post-hoc approach to remove\nthis information from trained LLMs, offers a promising solution to mitigate\nthese risks. However, previous practices face three key challenges: 1. Utility:\nsuccessful unlearning often causes catastrophic collapse on unrelated tasks. 2.\nEfficiency: many methods either involve adding similarly sized models, which\nslows down unlearning or inference, or require retain data that are difficult\nto obtain. 3. Robustness: even effective methods may still leak data via\nextraction techniques. To address these challenges, we propose MEOW, a simple\nyet effective gradient descent-based unlearning method. Specifically, we use an\noffline LLM to generate a set of inverted facts. Then, we design a new metric,\nMEMO, to quantify memorization in LLMs. Finally, based on the signals provided\nby MEMO, we select the most appropriate set of inverted facts and finetune the\nmodel based on them. We evaluate MEOW on the commonly used unlearn benchmark,\nToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks.\nResults demonstrate significant improvement of MEOW in forget quality without\nsubstantial loss in model utility. Meanwhile, MEOW does not exhibit significant\ndegradation in NLU or NLG capabilities, and there is even a slight improvement\nin NLU performance.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 记忆敏感信息的问题，提出了一种简单有效的梯度下降-based unlearning 方法 MEOW，以缓解数据泄露风险。MEOW 通过使用离线 LLM 生成 inverted facts，并设计新指标 MEMO 来量化模型记忆，然后基于 MEMO 信号选择合适的 facts 集进行模型微调，从而解决现有方法的实用性、效率和稳健性挑战。在 ToFU 基准上测试 Llama2-7B-Chat 和 Phi-1.5B 模型，结果显示 MEOW 显著提升了遗忘质量，同时在 NLU 和 NLG 任务上未出现明显性能下降，甚至在 NLU 方面略有改善。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11844v1",
      "published_date": "2024-09-18 09:55:48 UTC",
      "updated_date": "2024-09-18 09:55:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:12:25.475543"
    },
    {
      "arxiv_id": "2409.11835v1",
      "title": "DPI-TTS: Directional Patch Interaction for Fast-Converging and Style Temporal Modeling in Text-to-Speech",
      "title_zh": "DPI-TTS：定向补丁交互用于文本到语音中的快速收敛和风格时间建模",
      "authors": [
        "Xin Qi",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Tao Wang",
        "Chunyu Qiang",
        "Jianhua Tao",
        "Chenxing Li",
        "Yi Lu",
        "Shuchen Shi",
        "Zhiyong Wang",
        "Xiaopeng Wang",
        "Yuankun Xie",
        "Yukun Liu",
        "Xuefei Liu",
        "Guanjun Li"
      ],
      "abstract": "In recent years, speech diffusion models have advanced rapidly. Alongside the\nwidely used U-Net architecture, transformer-based models such as the Diffusion\nTransformer (DiT) have also gained attention. However, current DiT speech\nmodels treat Mel spectrograms as general images, which overlooks the specific\nacoustic properties of speech. To address these limitations, we propose a\nmethod called Directional Patch Interaction for Text-to-Speech (DPI-TTS), which\nbuilds on DiT and achieves fast training without compromising accuracy.\nNotably, DPI-TTS employs a low-to-high frequency, frame-by-frame progressive\ninference approach that aligns more closely with acoustic properties, enhancing\nthe naturalness of the generated speech. Additionally, we introduce a\nfine-grained style temporal modeling method that further improves speaker style\nsimilarity. Experimental results demonstrate that our method increases the\ntraining speed by nearly 2 times and significantly outperforms the baseline\nmodels.",
      "tldr_zh": "这篇论文提出了DPI-TTS方法，基于Diffusion Transformer (DiT)，通过Directional Patch Interaction技术优化语音合成模型，以解决现有模型忽略Mel spectrograms的声学属性的问题。DPI-TTS采用低到高频的逐帧渐进推理和细粒度风格时间建模，提升了生成语音的自然性和说话者风格相似性。实验结果表明，该方法使训练速度提高近2倍，并显著优于基线模型。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Submitted to ICASSP2025",
      "pdf_url": "http://arxiv.org/pdf/2409.11835v1",
      "published_date": "2024-09-18 09:36:55 UTC",
      "updated_date": "2024-09-18 09:36:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:12:36.845019"
    },
    {
      "arxiv_id": "2409.12716v1",
      "title": "Optical Flow Matters: an Empirical Comparative Study on Fusing Monocular Extracted Modalities for Better Steering",
      "title_zh": "翻译失败",
      "authors": [
        "Fouad Makiyeh",
        "Mark Bastourous",
        "Anass Bairouk",
        "Wei Xiao",
        "Mirjana Maras",
        "Tsun-Hsuan Wangb",
        "Marc Blanchon",
        "Ramin Hasani",
        "Patrick Chareyre",
        "Daniela Rus"
      ],
      "abstract": "Autonomous vehicle navigation is a key challenge in artificial intelligence,\nrequiring robust and accurate decision-making processes. This research\nintroduces a new end-to-end method that exploits multimodal information from a\nsingle monocular camera to improve the steering predictions for self-driving\ncars. Unlike conventional models that require several sensors which can be\ncostly and complex or rely exclusively on RGB images that may not be robust\nenough under different conditions, our model significantly improves vehicle\nsteering prediction performance from a single visual sensor. By focusing on the\nfusion of RGB imagery with depth completion information or optical flow data,\nwe propose a comprehensive framework that integrates these modalities through\nboth early and hybrid fusion techniques.\n  We use three distinct neural network models to implement our approach:\nConvolution Neural Network - Neutral Circuit Policy (CNN-NCP) , Variational\nAuto Encoder - Long Short-Term Memory (VAE-LSTM) , and Neural Circuit Policy\narchitecture VAE-NCP. By incorporating optical flow into the decision-making\nprocess, our method significantly advances autonomous navigation. Empirical\nresults from our comparative study using Boston driving data show that our\nmodel, which integrates image and motion information, is robust and reliable.\nIt outperforms state-of-the-art approaches that do not use optical flow,\nreducing the steering estimation error by 31%. This demonstrates the potential\nof optical flow data, combined with advanced neural network architectures (a\nCNN-based structure for fusing data and a Recurrence-based network for\ninferring a command from latent space), to enhance the performance of\nautonomous vehicles steering estimation.",
      "tldr_zh": "本研究提出了一种端到端方法，利用单目相机提取的多模态信息（如 RGB 图像、深度完成和 Optical Flow 数据）来提升自动驾驶车辆的转向预测性能，相比传统依赖多传感器的模型更具成本效益和鲁棒性。方法通过 Early Fusion 和 Hybrid Fusion 技术整合这些模态，并采用三种神经网络架构（CNN-NCP、VAE-LSTM 和 VAE-NCP）来处理图像和运动信息。实验结果显示，该框架在 Boston 驾驶数据上将转向估计错误降低了 31%，显著优于不使用 Optical Flow 的现有方法。这证明了 Optical Flow 在增强自主导航决策中的关键作用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12716v1",
      "published_date": "2024-09-18 09:36:24 UTC",
      "updated_date": "2024-09-18 09:36:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:12:52.538189"
    },
    {
      "arxiv_id": "2409.11820v1",
      "title": "Optimizing Job Shop Scheduling in the Furniture Industry: A Reinforcement Learning Approach Considering Machine Setup, Batch Variability, and Intralogistics",
      "title_zh": "翻译失败",
      "authors": [
        "Malte Schneevogt",
        "Karsten Binninger",
        "Noah Klarmann"
      ],
      "abstract": "This paper explores the potential application of Deep Reinforcement Learning\nin the furniture industry. To offer a broad product portfolio, most furniture\nmanufacturers are organized as a job shop, which ultimately results in the Job\nShop Scheduling Problem (JSSP). The JSSP is addressed with a focus on extending\ntraditional models to better represent the complexities of real-world\nproduction environments. Existing approaches frequently fail to consider\ncritical factors such as machine setup times or varying batch sizes. A concept\nfor a model is proposed that provides a higher level of information detail to\nenhance scheduling accuracy and efficiency. The concept introduces the\nintegration of DRL for production planning, particularly suited to batch\nproduction industries such as the furniture industry. The model extends\ntraditional approaches to JSSPs by including job volumes, buffer management,\ntransportation times, and machine setup times. This enables more precise\nforecasting and analysis of production flows and processes, accommodating the\nvariability and complexity inherent in real-world manufacturing processes. The\nRL agent learns to optimize scheduling decisions. It operates within a discrete\naction space, making decisions based on detailed observations. A reward\nfunction guides the agent's decision-making process, thereby promoting\nefficient scheduling and meeting production deadlines. Two integration\nstrategies for implementing the RL agent are discussed: episodic planning,\nwhich is suitable for low-automation environments, and continuous planning,\nwhich is ideal for highly automated plants. While episodic planning can be\nemployed as a standalone solution, the continuous planning approach\nnecessitates the integration of the agent with ERP and Manufacturing Execution\nSystems. This integration enables real-time adjustments to production schedules\nbased on dynamic changes.",
      "tldr_zh": "本论文探讨了深度强化学习（Deep Reinforcement Learning, DRL）在家具行业作业车间调度问题（Job Shop Scheduling Problem, JSSP）中的应用，旨在优化生产流程并考虑机器设置时间、批量变异和内部物流（Intralogistics）。作者提出一个扩展模型，融入作业量、缓冲管理、运输时间和机器设置时间等细节，以更准确地模拟真实生产环境。DRL 代理通过离散动作空间和基于详细观察的决策进行学习，并使用奖励函数引导优化调度以提高效率和满足截止期限。论文讨论了两种集成策略：周期性规划（episodic planning）适用于低自动化环境，以及连续规划（continuous planning）需与 ERP 和 Manufacturing Execution Systems 集成，实现实时调整。总体而言，此方法提升了生产规划的精确性和适应性，为复杂制造过程提供了实用解决方案。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 8 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.11820v1",
      "published_date": "2024-09-18 09:12:40 UTC",
      "updated_date": "2024-09-18 09:12:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:13:01.696600"
    },
    {
      "arxiv_id": "2409.11817v1",
      "title": "EFCM: Efficient Fine-tuning on Compressed Models for deployment of large models in medical image analysis",
      "title_zh": "EFCM: 针对压缩模型的高效微调，用于大型模型在医学图像分析中的部署",
      "authors": [
        "Shaojie Li",
        "Zhaoshuo Diao"
      ],
      "abstract": "The recent development of deep learning large models in medicine shows\nremarkable performance in medical image analysis and diagnosis, but their large\nnumber of parameters causes memory and inference latency challenges. Knowledge\ndistillation offers a solution, but the slide-level gradients cannot be\nbackpropagated for student model updates due to high-resolution pathological\nimages and slide-level labels. This study presents an Efficient Fine-tuning on\nCompressed Models (EFCM) framework with two stages: unsupervised feature\ndistillation and fine-tuning. In the distillation stage, Feature Projection\nDistillation (FPD) is proposed with a TransScan module for adaptive receptive\nfield adjustment to enhance the knowledge absorption capability of the student\nmodel. In the slide-level fine-tuning stage, three strategies (Reuse CLAM,\nRetrain CLAM, and End2end Train CLAM (ETC)) are compared. Experiments are\nconducted on 11 downstream datasets related to three large medical models:\nRETFound for retina, MRM for chest X-ray, and BROW for histopathology. The\nexperimental results demonstrate that the EFCM framework significantly improves\naccuracy and efficiency in handling slide-level pathological image problems,\neffectively addressing the challenges of deploying large medical models.\nSpecifically, it achieves a 4.33% increase in ACC and a 5.2% increase in AUC\ncompared to the large model BROW on the TCGA-NSCLC and TCGA-BRCA datasets. The\nanalysis of model inference efficiency highlights the high efficiency of the\ndistillation fine-tuning method.",
      "tldr_zh": "该研究提出 EFCM 框架，用于高效微调压缩模型，以解决大型深度学习模型在医疗图像分析中的内存和推理延迟挑战。框架包括两个阶段：无监督特征蒸馏阶段，使用 Feature Projection Distillation (FPD) 和 TransScan 模块调整自适应感受野以提升学生模型的知识吸收能力；以及幻灯片级微调阶段，通过比较 Reuse CLAM、Retrain CLAM 和 End2end Train CLAM (ETC) 等策略进行优化。实验在 RETFound、MRM 和 BROW 等大型医疗模型的 11 个下游数据集上进行，结果显示 EFCM 显著提高了准确性和效率，例如在 TCGA-NSCLC 和 TCGA-BRCA 数据集上，相比 BROW 模型，ACC 提升 4.33%，AUC 提升 5.2%。这种方法有效解决了部署大型模型的实际问题，提升了整体推理效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11817v1",
      "published_date": "2024-09-18 09:08:16 UTC",
      "updated_date": "2024-09-18 09:08:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:13:13.805996"
    },
    {
      "arxiv_id": "2409.11813v1",
      "title": "EventAug: Multifaceted Spatio-Temporal Data Augmentation Methods for Event-based Learning",
      "title_zh": "EventAug：多维度的时空数据增强方法，用于基于事件的学习",
      "authors": [
        "Yukun Tian",
        "Hao Chen",
        "Yongjian Deng",
        "Feihong Shen",
        "Kepan Liu",
        "Wei You",
        "Ziyang Zhang"
      ],
      "abstract": "The event camera has demonstrated significant success across a wide range of\nareas due to its low time latency and high dynamic range. However, the\ncommunity faces challenges such as data deficiency and limited diversity, often\nresulting in over-fitting and inadequate feature learning. Notably, the\nexploration of data augmentation techniques in the event community remains\nscarce. This work aims to address this gap by introducing a systematic\naugmentation scheme named EventAug to enrich spatial-temporal diversity. In\nparticular, we first propose Multi-scale Temporal Integration (MSTI) to\ndiversify the motion speed of objects, then introduce Spatial-salient Event\nMask (SSEM) and Temporal-salient Event Mask (TSEM) to enrich object variants.\nOur EventAug can facilitate models learning with richer motion patterns, object\nvariants and local spatio-temporal relations, thus improving model robustness\nto varied moving speeds, occlusions, and action disruptions. Experiment results\nshow that our augmentation method consistently yields significant improvements\nacross different tasks and backbones (e.g., a 4.87% accuracy gain on DVS128\nGesture). Our code will be publicly available for this community.",
      "tldr_zh": "本研究针对事件相机数据不足和多样性有限的问题，提出了EventAug，一种系统化的时空数据增强方案，以丰富空间-时间多样性。具体方法包括Multi-scale Temporal Integration (MSTI)来多样化物体运动速度，以及Spatial-salient Event Mask (SSEM)和Temporal-salient Event Mask (TSEM)来增强物体变体，从而帮助模型学习更丰富的运动模式、物体变体和局部时空关系，提高对不同运动速度、遮挡和动作中断的鲁棒性。实验结果显示，EventAug在各种任务和骨干网络上显著提升性能，例如在DVS128 Gesture数据集上准确率提高4.87%，并计划公开代码以惠及社区。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11813v1",
      "published_date": "2024-09-18 09:01:34 UTC",
      "updated_date": "2024-09-18 09:01:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:13:25.744281"
    },
    {
      "arxiv_id": "2409.11802v1",
      "title": "Latent fingerprint enhancement for accurate minutiae detection",
      "title_zh": "潜在指纹增强用于准确的细微特征检测",
      "authors": [
        "Abdul Wahab",
        "Tariq Mahmood Khan",
        "Shahzaib Iqbal",
        "Bandar AlShammari",
        "Bandar Alhaqbani",
        "Imran Razzak"
      ],
      "abstract": "Identification of suspects based on partial and smudged fingerprints,\ncommonly referred to as fingermarks or latent fingerprints, presents a\nsignificant challenge in the field of fingerprint recognition. Although\nfixed-length embeddings have shown effectiveness in recognising rolled and slap\nfingerprints, the methods for matching latent fingerprints have primarily\ncentred around local minutiae-based embeddings, failing to fully exploit global\nrepresentations for matching purposes. Consequently, enhancing latent\nfingerprints becomes critical to ensuring robust identification for forensic\ninvestigations. Current approaches often prioritise restoring ridge patterns,\noverlooking the fine-macroeconomic details crucial for accurate fingerprint\nrecognition. To address this, we propose a novel approach that uses generative\nadversary networks (GANs) to redefine Latent Fingerprint Enhancement (LFE)\nthrough a structured approach to fingerprint generation. By directly optimising\nthe minutiae information during the generation process, the model produces\nenhanced latent fingerprints that exhibit exceptional fidelity to ground-truth\ninstances. This leads to a significant improvement in identification\nperformance. Our framework integrates minutiae locations and orientation\nfields, ensuring the preservation of both local and structural fingerprint\nfeatures. Extensive evaluations conducted on two publicly available datasets\ndemonstrate our method's dominance over existing state-of-the-art techniques,\nhighlighting its potential to significantly enhance latent fingerprint\nrecognition accuracy in forensic applications.",
      "tldr_zh": "这篇论文针对latent fingerprints（部分和模糊指纹）的识别挑战，提出了一种新型Latent Fingerprint Enhancement (LFE)方法，使用Generative Adversarial Networks (GANs)来优化minutiae信息（指纹细微特征）。该框架通过直接整合minutiae locations和orientation fields，确保增强后的指纹保留局部和全局结构特征，从而提升识别准确性。实验在两个公开数据集上表明，该方法优于现有技术，提高了指纹识别性能，为法医调查提供更可靠的支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11802v1",
      "published_date": "2024-09-18 08:35:31 UTC",
      "updated_date": "2024-09-18 08:35:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:13:37.231931"
    },
    {
      "arxiv_id": "2409.11798v1",
      "title": "The Factuality of Large Language Models in the Legal Domain",
      "title_zh": "大型语言模型在法律领域的真实性",
      "authors": [
        "Rajaa El Hamdani",
        "Thomas Bonald",
        "Fragkiskos Malliaros",
        "Nils Holzenberger",
        "Fabian Suchanek"
      ],
      "abstract": "This paper investigates the factuality of large language models (LLMs) as\nknowledge bases in the legal domain, in a realistic usage scenario: we allow\nfor acceptable variations in the answer, and let the model abstain from\nanswering when uncertain. First, we design a dataset of diverse factual\nquestions about case law and legislation. We then use the dataset to evaluate\nseveral LLMs under different evaluation methods, including exact, alias, and\nfuzzy matching. Our results show that the performance improves significantly\nunder the alias and fuzzy matching methods. Further, we explore the impact of\nabstaining and in-context examples, finding that both strategies enhance\nprecision. Finally, we demonstrate that additional pre-training on legal\ndocuments, as seen with SaulLM, further improves factual precision from 63% to\n81%.",
      "tldr_zh": "这篇论文评估了大型语言模型（LLMs）在法律领域的真实性，作为知识库的一种实际应用，允许答案的合理变体并让模型在不确定时弃权。首先，研究者设计了一个包含案例法和立法多样事实问题的数据集，并使用精确匹配、别名匹配和模糊匹配方法评估多个LLMs，结果显示别名和模糊匹配显著提升了性能。其次，弃权策略和in-context examples进一步提高了精确度，而在法律文档上额外预训练的模型（如SaulLM）将事实精确度从63%提高到81%。这为改进LLMs在法律领域的可靠性和应用提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "CIKM 2024, short paper",
      "pdf_url": "http://arxiv.org/pdf/2409.11798v1",
      "published_date": "2024-09-18 08:30:20 UTC",
      "updated_date": "2024-09-18 08:30:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:13:49.630740"
    },
    {
      "arxiv_id": "2409.12210v1",
      "title": "Mixture of Diverse Size Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Manxi Sun",
        "Wei Liu",
        "Jian Luan",
        "Pengzhi Gao",
        "Bin Wang"
      ],
      "abstract": "The Sparsely-Activated Mixture-of-Experts (MoE) has gained increasing\npopularity for scaling up large language models (LLMs) without exploding\ncomputational costs. Despite its success, the current design faces a challenge\nwhere all experts have the same size, limiting the ability of tokens to choose\nthe experts with the most appropriate size for generating the next token. In\nthis paper, we propose the Mixture of Diverse Size Experts (MoDSE), a new MoE\narchitecture with layers designed to have experts of different sizes. Our\nanalysis of difficult token generation tasks shows that experts of various\nsizes achieve better predictions, and the routing path of the experts tends to\nbe stable after a training period. However, having experts of diverse sizes can\nlead to uneven workload distribution. To tackle this limitation, we introduce\nan expert-pair allocation strategy to evenly distribute the workload across\nmultiple GPUs. Comprehensive evaluations across multiple benchmarks demonstrate\nthe effectiveness of MoDSE, as it outperforms existing MoEs by allocating the\nparameter budget to experts adaptively while maintaining the same total\nparameter size and the number of experts.",
      "tldr_zh": "本研究针对传统 Sparsely-Activated Mixture-of-Experts (MoE) 架构中所有 experts 尺寸相同的问题，提出了一种新型 MoE 变体——Mixture of Diverse Size Experts (MoDSE)，允许 experts 具有不同大小，以更好地适应 tokens 生成任务的需求。分析显示，不同大小的 experts 在处理困难 token 生成时能实现更准确的预测，且 experts 的 routing path 在训练后趋于稳定；为解决由此带来的工作负载不均问题，该框架引入了 expert-pair allocation 策略，实现跨多个 GPU 的均匀分布。实验结果表明，MoDSE 在多个基准测试中优于现有 MoE 模型，能够自适应分配参数预算，同时保持相同的总参数大小和专家数量，从而提升大型语言模型的效率和性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12210v1",
      "published_date": "2024-09-18 08:23:27 UTC",
      "updated_date": "2024-09-18 08:23:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:14:01.932900"
    },
    {
      "arxiv_id": "2409.11786v1",
      "title": "Efficient Low-Resolution Face Recognition via Bridge Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Shiming Ge",
        "Shengwei Zhao",
        "Chenyu Li",
        "Yu Zhang",
        "Jia Li"
      ],
      "abstract": "Face recognition in the wild is now advancing towards light-weight models,\nfast inference speed and resolution-adapted capability. In this paper, we\npropose a bridge distillation approach to turn a complex face model pretrained\non private high-resolution faces into a light-weight one for low-resolution\nface recognition. In our approach, such a cross-dataset resolution-adapted\nknowledge transfer problem is solved via two-step distillation. In the first\nstep, we conduct cross-dataset distillation to transfer the prior knowledge\nfrom private high-resolution faces to public high-resolution faces and generate\ncompact and discriminative features. In the second step, the resolution-adapted\ndistillation is conducted to further transfer the prior knowledge to synthetic\nlow-resolution faces via multi-task learning. By learning low-resolution face\nrepresentations and mimicking the adapted high-resolution knowledge, a\nlight-weight student model can be constructed with high efficiency and\npromising accuracy in recognizing low-resolution faces. Experimental results\nshow that the student model performs impressively in recognizing low-resolution\nfaces with only 0.21M parameters and 0.057MB memory. Meanwhile, its speed\nreaches up to 14,705, ~934 and 763 faces per second on GPU, CPU and mobile\nphone, respectively.",
      "tldr_zh": "本论文提出了一种桥式蒸馏（bridge distillation）方法，将在私有高分辨率人脸数据集上预训练的复杂模型转化为轻量级模型，以实现高效的低分辨率人脸识别。\n该方法分为两步：首先通过跨数据集蒸馏，将知识从私有高分辨率人脸转移到公共高分辨率人脸，生成紧凑且判别性的特征；其次，进行分辨率适应蒸馏，利用多任务学习将知识进一步转移到合成低分辨率人脸。\n实验结果表明，学生模型仅需0.21M参数和0.057MB内存，便能在低分辨率人脸识别中表现出色，速度分别在GPU、CPU和手机上达到14,705、约934和763张脸/秒。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is published in IEEE TIP 2020",
      "pdf_url": "http://arxiv.org/pdf/2409.11786v1",
      "published_date": "2024-09-18 08:10:35 UTC",
      "updated_date": "2024-09-18 08:10:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:14:14.832085"
    },
    {
      "arxiv_id": "2409.11785v1",
      "title": "Distilling Channels for Efficient Deep Tracking",
      "title_zh": "翻译失败",
      "authors": [
        "Shiming Ge",
        "Zhao Luo",
        "Chunhui Zhang",
        "Yingying Hua",
        "Dacheng Tao"
      ],
      "abstract": "Deep trackers have proven success in visual tracking. Typically, these\ntrackers employ optimally pre-trained deep networks to represent all diverse\nobjects with multi-channel features from some fixed layers. The deep networks\nemployed are usually trained to extract rich knowledge from massive data used\nin object classification and so they are capable to represent generic objects\nvery well. However, these networks are too complex to represent a specific\nmoving object, leading to poor generalization as well as high computational and\nmemory costs. This paper presents a novel and general framework termed channel\ndistillation to facilitate deep trackers. To validate the effectiveness of\nchannel distillation, we take discriminative correlation filter (DCF) and ECO\nfor example. We demonstrate that an integrated formulation can turn feature\ncompression, response map generation, and model update into a unified energy\nminimization problem to adaptively select informative feature channels that\nimprove the efficacy of tracking moving objects on the fly. Channel\ndistillation can accurately extract good channels, alleviating the influence of\nnoisy channels and generally reducing the number of channels, as well as\nadaptively generalizing to different channels and networks. The resulting deep\ntracker is accurate, fast, and has low memory requirements. Extensive\nexperimental evaluations on popular benchmarks clearly demonstrate the\neffectiveness and generalizability of our framework.",
      "tldr_zh": "这篇论文提出了 channel distillation 框架，以提高深度跟踪器的效率，针对现有深度网络在表示特定移动对象时存在的泛化差和高计算内存成本问题。该框架将特征压缩、响应图生成和模型更新整合为一个统一的能量最小化问题，能够自适应选择信息丰富的特征通道，减少噪声通道的影响并降低通道数量。以 DCF 和 ECO 为例，实验在流行基准上证明了该方法的有效性，使跟踪器在准确性、速度和内存需求方面均有显著提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published by IEEE TIP 2020",
      "pdf_url": "http://arxiv.org/pdf/2409.11785v1",
      "published_date": "2024-09-18 08:09:20 UTC",
      "updated_date": "2024-09-18 08:09:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:14:24.923061"
    },
    {
      "arxiv_id": "2409.12209v1",
      "title": "Multivariate Analysis of Gut Microbiota Composition and Prevalence of Gastric Cancer",
      "title_zh": "肠道微生物组组成与胃癌患病率的多变量分析",
      "authors": [
        "Aadhith Shankarnarayanan",
        "Dheeman Gangopadhyay",
        "Ayman Alzaatreh"
      ],
      "abstract": "The global surge in the cases of gastric cancer has prompted an investigation\ninto the potential of gut microbiota as a predictive marker for the disease.\nThe alterations in gut diversity are suspected to be associated with an\nelevated risk of gastric cancer. This paper delves into finding the correlation\nbetween gut microbiota and gastric cancer, focusing on patients who have\nundergone total and subtotal gastrectomy. Utilizing data mining and statistical\nlearning methods, an analysis was conducted on 16S-RNA sequenced genes obtained\nfrom 96 participants with the aim of identifying specific genera of gut\nmicrobiota associated with gastric cancer. The study reveals several prominent\nbacterial genera that could potentially serve as biomarkers assessing the risk\nof gastric cancer. These findings offer a pathway for early risk assessment and\nprecautionary measures in the diagnosis of gastric cancer. The intricate\nmechanisms through which these gut microbiotas influence gastric cancer\nprogression warrant further investigation. This research significantly aims to\ncontribute to the growing understanding of the gut-cancer axis and its\nimplications in disease prediction and prevention.",
      "tldr_zh": "这篇论文探讨了肠道微生物群（gut microbiota）与胃癌（gastric cancer）患病率的相关性，旨在通过分析96名接受全胃或部分胃切除患者的16S-RNA测序数据，识别潜在的生物标志物。研究采用数据挖掘和统计学习方法，揭示了几个突出的细菌属，可能作为评估胃癌风险的标志物。结果为胃癌的早期风险评估和预防措施提供了新途径，同时强调了进一步调查肠道-癌症轴（gut-cancer axis）机制的重要性。",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12209v1",
      "published_date": "2024-09-18 08:08:31 UTC",
      "updated_date": "2024-09-18 08:08:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:14:37.820015"
    },
    {
      "arxiv_id": "2409.11782v2",
      "title": "Smart Data-Driven GRU Predictor for SnO$_2$ Thin films Characteristics",
      "title_zh": "翻译失败",
      "authors": [
        "Faiza Bouamra",
        "Mohamed Sayah",
        "Labib Sadek Terrissa",
        "Noureddine Zerhouni"
      ],
      "abstract": "In material physics, characterization techniques are foremost crucial for\nobtaining the materials data regarding the physical properties as well as\nstructural, electronics, magnetic, optic, dielectric, and spectroscopic\ncharacteristics. However, for many materials, ensuring availability and safe\naccessibility is not always easy and fully warranted. Moreover, the use of\nmodeling and simulation techniques need a lot of theoretical knowledge, in\naddition of being associated to costly computation time and a great complexity\ndeal. Thus, analyzing materials with different techniques for multiple samples\nsimultaneously, still be very challenging for engineers and researchers. It is\nworth noting that although of being very risky, X-ray diffraction is the well\nknown and widely used characterization technique which gathers data from\nstructural properties of crystalline 1d, 2d or 3d materials. We propose in this\npaper, a Smart GRU for Gated Recurrent Unit model to forcast structural\ncharacteristics or properties of thin films of tin oxide SnO$_2$(110). Indeed,\nthin films samples are elaborated and managed experimentally and the collected\ndata dictionary is then used to generate an AI -- Artificial Intelligence --\nGRU model for the thin films of tin oxide SnO$_2$(110) structural property\ncharacterization.",
      "tldr_zh": "本研究针对材料物理中表征技术的挑战（如数据获取困难、建模复杂性和计算成本高），提出了一种智能数据驱动的 GRU（Gated Recurrent Unit）预测模型，用于预测 SnO$_2$ 薄膜的结构特性。模型基于实验制备的 SnO$_2$(110) 薄膜样本收集的数据，构建 AI-GRU 框架来简化对材料结构属性的分析，从而避免了传统方法如 X-ray diffraction 的风险和复杂性。该方法为工程师和研究人员提供了一种高效、可扩展的替代方案，提升了薄膜材料特性的预测准确性。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "F.2.2; I.2.7"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "19 pages, 14 figures. Baltica Journal, Special Issues, September 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.11782v2",
      "published_date": "2024-09-18 08:05:08 UTC",
      "updated_date": "2024-09-29 20:19:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:14:49.267011"
    },
    {
      "arxiv_id": "2409.15361v1",
      "title": "Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Essa Jan",
        "Nouar AlDahoul",
        "Moiz Ali",
        "Faizan Ahmad",
        "Fareed Zaffar",
        "Yasir Zaki"
      ],
      "abstract": "Recent breakthroughs in Large Language Models (LLMs) have led to their\nadoption across a wide range of tasks, ranging from code generation to machine\ntranslation and sentiment analysis, etc. Red teaming/Safety alignment efforts\nshow that fine-tuning models on benign (non-harmful) data could compromise\nsafety. However, it remains unclear to what extent this phenomenon is\ninfluenced by different variables, including fine-tuning task, model\ncalibrations, etc. This paper explores the task-wise safety degradation due to\nfine-tuning on downstream tasks such as summarization, code generation,\ntranslation, and classification across various calibration. Our results reveal\nthat: 1) Fine-tuning LLMs for code generation and translation leads to the\nhighest degradation in safety guardrails. 2) LLMs generally have weaker\nguardrails for translation and classification, with 73-92% of harmful prompts\nanswered, across baseline and other calibrations, falling into one of two\nconcern categories. 3) Current solutions, including guards and safety tuning\ndatasets, lack cross-task robustness. To address these issues, we developed a\nnew multitask safety dataset effectively reducing attack success rates across a\nrange of tasks without compromising the model's overall helpfulness. Our work\nunderscores the need for generalized alignment measures to ensure safer and\nmore robust models.",
      "tldr_zh": "本研究揭示了在大型语言模型 (LLMs) 进行 fine-tuning 时，任务相关的安全退化问题，特别是代码生成和翻译任务会导致安全防护最大下降，而翻译和分类任务的防护较弱，导致73-92% 的有害提示被响应。研究者通过实验分析了微调任务、模型校准等变量的影响，并发现现有解决方案如防护措施和安全调优数据集缺乏跨任务鲁棒性。为解决这些问题，他们开发了一个新的多任务安全数据集，能够有效降低攻击成功率，同时不影响模型的整体帮助性。该工作强调了需要更通用的对齐措施，以提升 LLMs 的安全性和鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.15361v1",
      "published_date": "2024-09-18 08:04:24 UTC",
      "updated_date": "2024-09-18 08:04:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:15:02.670311"
    },
    {
      "arxiv_id": "2409.11780v1",
      "title": "Explaining Non-monotonic Normative Reasoning using Argumentation Theory with Deontic Logic",
      "title_zh": "翻译失败",
      "authors": [
        "Zhe Yu",
        "Yiwei Lu"
      ],
      "abstract": "In our previous research, we provided a reasoning system (called LeSAC) based\non argumentation theory to provide legal support to designers during the design\nprocess. Building on this, this paper explores how to provide designers with\neffective explanations for their legally relevant design decisions. We extend\nthe previous system for providing explanations by specifying norms and the key\nlegal or ethical principles for justifying actions in normative contexts.\nConsidering that first-order logic has strong expressive power, in the current\npaper we adopt a first-order deontic logic system with deontic operators and\npreferences. We illustrate the advantages and necessity of introducing deontic\nlogic and designing explanations under LeSAC by modelling two cases in the\ncontext of autonomous driving. In particular, this paper also discusses the\nrequirements of the updated LeSAC to guarantee rationality, and proves that a\nwell-defined LeSAC can satisfy the rationality postulate for rule-based\nargumentation frameworks. This ensures the system's ability to provide\ncoherent, legally valid explanations for complex design decisions.",
      "tldr_zh": "本文扩展了之前的 LeSAC 系统，使用 Argumentation Theory 和 Deontic Logic 来为设计过程中的法律相关决策提供有效解释。研究采用 first-order deontic logic 系统，包括 deontic operators 和 preferences，以规范关键的法律或伦理原则，并通过两个自动驾驶案例进行建模，展示了其优势。最终，论文讨论了更新 LeSAC 的理性要求，并证明了该系统满足规则-based argumentation frameworks 的理性假设，从而确保为复杂设计决策提供连贯、合法有效的解释。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.11780v1",
      "published_date": "2024-09-18 08:03:29 UTC",
      "updated_date": "2024-09-18 08:03:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:15:14.644951"
    },
    {
      "arxiv_id": "2409.11770v1",
      "title": "Knowledge Adaptation Network for Few-Shot Class-Incremental Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ye Wang",
        "Yaxiong Wang",
        "Guoshuai Zhao",
        "Xueming Qian"
      ],
      "abstract": "Few-shot class-incremental learning (FSCIL) aims to incrementally recognize\nnew classes using a few samples while maintaining the performance on previously\nlearned classes. One of the effective methods to solve this challenge is to\nconstruct prototypical evolution classifiers. Despite the advancement achieved\nby most existing methods, the classifier weights are simply initialized using\nmean features. Because representations for new classes are weak and biased, we\nargue such a strategy is suboptimal. In this paper, we tackle this issue from\ntwo aspects. Firstly, thanks to the development of foundation models, we employ\na foundation model, the CLIP, as the network pedestal to provide a general\nrepresentation for each class. Secondly, to generate a more reliable and\ncomprehensive instance representation, we propose a Knowledge Adapter (KA)\nmodule that summarizes the data-specific knowledge from training data and fuses\nit into the general representation. Additionally, to tune the knowledge learned\nfrom the base classes to the upcoming classes, we propose a mechanism of\nIncremental Pseudo Episode Learning (IPEL) by simulating the actual FSCIL.\nTaken together, our proposed method, dubbed as Knowledge Adaptation Network\n(KANet), achieves competitive performance on a wide range of datasets,\nincluding CIFAR100, CUB200, and ImageNet-R.",
      "tldr_zh": "该论文针对 Few-Shot Class-Incremental Learning (FSCIL) 问题，提出 Knowledge Adaptation Network (KANet)，旨在使用少量样本识别新类同时保持对旧类性能。KANet 通过采用基础模型 CLIP 提供通用类表示，并引入 Knowledge Adapter (KA) 模块从训练数据中提取并融合数据特定知识，以优化分类器权重；同时，Incremental Pseudo Episode Learning (IPEL) 机制模拟实际 FSCIL 过程，帮助知识从基础类向新类平滑转移。在 CIFAR100、CUB200 和 ImageNet-R 数据集上，KANet 实现了竞争性性能，显著提升了少样本增量学习的可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages;6 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.11770v1",
      "published_date": "2024-09-18 07:51:38 UTC",
      "updated_date": "2024-09-18 07:51:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:15:27.094118"
    },
    {
      "arxiv_id": "2409.11764v2",
      "title": "One Map to Find Them All: Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Finn Lukas Busch",
        "Timon Homberger",
        "Jesús Ortega-Peimbert",
        "Quantao Yang",
        "Olov Andersson"
      ],
      "abstract": "The capability to efficiently search for objects in complex environments is\nfundamental for many real-world robot applications. Recent advances in\nopen-vocabulary vision models have resulted in semantically-informed object\nnavigation methods that allow a robot to search for an arbitrary object without\nprior training. However, these zero-shot methods have so far treated the\nenvironment as unknown for each consecutive query. In this paper we introduce a\nnew benchmark for zero-shot multi-object navigation, allowing the robot to\nleverage information gathered from previous searches to more efficiently find\nnew objects. To address this problem we build a reusable open-vocabulary\nfeature map tailored for real-time object search. We further propose a\nprobabilistic-semantic map update that mitigates common sources of errors in\nsemantic feature extraction and leverage this semantic uncertainty for informed\nmulti-object exploration. We evaluate our method on a set of object navigation\ntasks in both simulation as well as with a real robot, running in real-time on\na Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art\napproaches both on single and multi-object navigation tasks. Additional videos,\ncode and the multi-object navigation benchmark will be available on\nhttps://finnbsch.github.io/OneMap.",
      "tldr_zh": "该论文针对机器人高效搜索复杂环境中对象的挑战，引入了一个新的零-shot 多对象导航基准（zero-shot multi-object navigation benchmark），允许机器人利用先前搜索的信息来优化后续任务。研究提出了一种可重用的开放词汇特征映射（open-vocabulary feature map），结合概率-语义地图更新（probabilistic-semantic map update）来减轻语义特征提取错误，并利用语义不确定性指导多对象探索。实验结果显示，该方法在模拟和真实机器人环境中实时运行于 Jetson Orin AGX 上，并在单对象和多对象导航任务中优于现有最先进方法，提供代码和基准以供进一步研究。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11764v2",
      "published_date": "2024-09-18 07:44:08 UTC",
      "updated_date": "2025-03-03 18:50:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:15:38.876296"
    },
    {
      "arxiv_id": "2410.02791v1",
      "title": "DifFaiRec: Generative Fair Recommender with Conditional Diffusion Model",
      "title_zh": "DifFaiRec：基于条件扩散模型的生成式公平推荐系统",
      "authors": [
        "Zhenhao Jiang",
        "Jicong Fan"
      ],
      "abstract": "Although recommenders can ship items to users automatically based on the\nusers' preferences, they often cause unfairness to groups or individuals. For\ninstance, when users can be divided into two groups according to a sensitive\nsocial attribute and there is a significant difference in terms of activity\nbetween the two groups, the learned recommendation algorithm will result in a\nrecommendation gap between the two groups, which causes group unfairness. In\nthis work, we propose a novel recommendation algorithm named Diffusion-based\nFair Recommender (DifFaiRec) to provide fair recommendations. DifFaiRec is\nbuilt upon the conditional diffusion model and hence has a strong ability to\nlearn the distribution of user preferences from their ratings on items and is\nable to generate diverse recommendations effectively. To guarantee fairness, we\ndesign a counterfactual module to reduce the model sensitivity to protected\nattributes and provide mathematical explanations. The experiments on benchmark\ndatasets demonstrate the superiority of DifFaiRec over competitive baselines.",
      "tldr_zh": "本研究针对推荐系统中的不公平问题（如基于敏感属性的推荐差距），提出了一种新型算法DifFaiRec，利用conditional diffusion model学习用户偏好分布，并生成多样化的公平推荐。DifFaiRec通过设计counterfactual module减少模型对保护属性的敏感性，并提供数学解释，以确保推荐过程的公平性。实验结果显示，该算法在基准数据集上优于竞争基线，证明了其在提升推荐公平性方面的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "The paper was accepted by ICDM 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.02791v1",
      "published_date": "2024-09-18 07:39:33 UTC",
      "updated_date": "2024-09-18 07:39:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:15:59.330771"
    },
    {
      "arxiv_id": "2409.11756v1",
      "title": "Synthesizing Evolving Symbolic Representations for Autonomous Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriele Sartor",
        "Angelo Oddi",
        "Riccardo Rasconi",
        "Vieri Giuliano Santucci",
        "Rosa Meo"
      ],
      "abstract": "Recently, AI systems have made remarkable progress in various tasks. Deep\nReinforcement Learning(DRL) is an effective tool for agents to learn policies\nin low-level state spaces to solve highly complex tasks. Researchers have\nintroduced Intrinsic Motivation(IM) to the RL mechanism, which simulates the\nagent's curiosity, encouraging agents to explore interesting areas of the\nenvironment. This new feature has proved vital in enabling agents to learn\npolicies without being given specific goals. However, even though DRL\nintelligence emerges through a sub-symbolic model, there is still a need for a\nsort of abstraction to understand the knowledge collected by the agent. To this\nend, the classical planning formalism has been used in recent research to\nexplicitly represent the knowledge an autonomous agent acquires and effectively\nreach extrinsic goals. Despite classical planning usually presents limited\nexpressive capabilities, PPDDL demonstrated usefulness in reviewing the\nknowledge gathered by an autonomous system, making explicit causal\ncorrelations, and can be exploited to find a plan to reach any state the agent\nfaces during its experience. This work presents a new architecture implementing\nan open-ended learning system able to synthesize from scratch its experience\ninto a PPDDL representation and update it over time. Without a predefined set\nof goals and tasks, the system integrates intrinsic motivations to explore the\nenvironment in a self-directed way, exploiting the high-level knowledge\nacquired during its experience. The system explores the environment and\niteratively: (a) discover options, (b) explore the environment using options,\n(c) abstract the knowledge collected and (d) plan. This paper proposes an\nalternative approach to implementing open-ended learning architectures\nexploiting low-level and high-level representations to extend its knowledge in\na virtuous loop.",
      "tldr_zh": "这篇论文提出了一种新架构，用于为自主系统合成和演化符号表示（如 PPDDL），以帮助代理从经验中抽象知识，实现开放式学习。该系统整合 Deep Reinforcement Learning (DRL) 和 Intrinsic Motivation (IM)，允许代理在没有预定义目标的情况下，通过自驱动探索环境来扩展知识。具体来说，系统采用循环过程：发现选项、使用选项探索环境、抽象收集到的知识以及进行规划。这种方法形成一个良性循环，显著提升了代理的知识表示和任务适应能力。",
      "categories": [
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11756v1",
      "published_date": "2024-09-18 07:23:26 UTC",
      "updated_date": "2024-09-18 07:23:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:16:02.266923"
    },
    {
      "arxiv_id": "2409.11754v1",
      "title": "NPAT Null-Space Projected Adversarial Training Towards Zero Deterioration",
      "title_zh": "翻译失败",
      "authors": [
        "Hanyi Hu",
        "Qiao Han",
        "Kui Chen",
        "Yao Yang"
      ],
      "abstract": "To mitigate the susceptibility of neural networks to adversarial attacks,\nadversarial training has emerged as a prevalent and effective defense strategy.\nIntrinsically, this countermeasure incurs a trade-off, as it sacrifices the\nmodel's accuracy in processing normal samples. To reconcile the trade-off, we\npioneer the incorporation of null-space projection into adversarial training\nand propose two innovative Null-space Projection based Adversarial\nTraining(NPAT) algorithms tackling sample generation and gradient optimization,\nnamed Null-space Projected Data Augmentation (NPDA) and Null-space Projected\nGradient Descent (NPGD), to search for an overarching optimal solutions, which\nenhance robustness with almost zero deterioration in generalization\nperformance. Adversarial samples and perturbations are constrained within the\nnull-space of the decision boundary utilizing a closed-form null-space\nprojector, effectively mitigating threat of attack stemming from unreliable\nfeatures. Subsequently, we conducted experiments on the CIFAR10 and SVHN\ndatasets and reveal that our methodology can seamlessly combine with\nadversarial training methods and obtain comparable robustness while keeping\ngeneralization close to a high-accuracy model.",
      "tldr_zh": "该研究针对神经网络对抗训练(adversarial training)中存在的鲁棒性提升与正常样本准确性下降的权衡问题，提出了一种基于空空间投影(null-space projection)的NPAT算法，包括Null-space Projected Data Augmentation (NPDA)和Null-space Projected Gradient Descent (NPGD)。这些算法通过将对抗样本和扰动限制在决策边界的空空间内，优化样本生成和梯度下降过程，从而增强模型的鲁棒性，同时几乎不影响泛化性能。在CIFAR10和SVHN数据集上的实验表明，NPAT方法能与现有对抗训练技术无缝结合，获得可比的防御效果，并保持模型的泛化性能接近高准确率水平。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11754v1",
      "published_date": "2024-09-18 07:18:22 UTC",
      "updated_date": "2024-09-18 07:18:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:16:14.367220"
    },
    {
      "arxiv_id": "2409.11744v3",
      "title": "Exploring Gaze Pattern Differences Between Autistic and Neurotypical Children: Clustering, Visualisation, and Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Weiyan Shi",
        "Haihong Zhang",
        "Wei Wang",
        "Kenny Tsu Wei Choo"
      ],
      "abstract": "Autism Spectrum Disorder (ASD) affects children's social and communication\nabilities, with eye-tracking widely used to identify atypical gaze patterns.\nWhile unsupervised clustering can automate the creation of areas of interest\nfor gaze feature extraction, the use of internal cluster validity indices, like\nSilhouette Coefficient, to distinguish gaze pattern differences between ASD and\ntypically developing (TD) children remains underexplored. We explore whether\ninternal cluster validity indices can distinguish ASD from TD children.\nSpecifically, we apply seven clustering algorithms to gaze points and extract\n63 internal cluster validity indices to reveal correlations with ASD diagnosis.\nUsing these indices, we train predictive models for ASD diagnosis. Experiments\non three datasets demonstrate high predictive accuracy (81\\% AUC), validating\nthe effectiveness of these indices.",
      "tldr_zh": "这篇论文探讨了自闭症谱系障碍(ASD)儿童与典型发育(TD)儿童的目光模式差异，使用无监督聚类算法对目光点进行分析。研究者应用七种聚类算法，提取63个内部聚类有效性指标（如Silhouette Coefficient），并分析这些指标与ASD诊断的相关性，以训练预测模型。实验结果显示，在三个数据集上，预测模型达到了81% AUC的准确率，验证了这些指标在区分ASD和TD儿童方面的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "work in progress",
      "pdf_url": "http://arxiv.org/pdf/2409.11744v3",
      "published_date": "2024-09-18 06:56:06 UTC",
      "updated_date": "2025-04-06 05:59:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:16:29.583855"
    },
    {
      "arxiv_id": "2409.11741v1",
      "title": "HARP: Human-Assisted Regrouping with Permutation Invariant Critic for Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Huawen Hu",
        "Enze Shi",
        "Chenxi Yue",
        "Shuocun Yang",
        "Zihao Wu",
        "Yiwei Li",
        "Tianyang Zhong",
        "Tuo Zhang",
        "Tianming Liu",
        "Shu Zhang"
      ],
      "abstract": "Human-in-the-loop reinforcement learning integrates human expertise to\naccelerate agent learning and provide critical guidance and feedback in complex\nfields. However, many existing approaches focus on single-agent tasks and\nrequire continuous human involvement during the training process, significantly\nincreasing the human workload and limiting scalability. In this paper, we\npropose HARP (Human-Assisted Regrouping with Permutation Invariant Critic), a\nmulti-agent reinforcement learning framework designed for group-oriented tasks.\nHARP integrates automatic agent regrouping with strategic human assistance\nduring deployment, enabling and allowing non-experts to offer effective\nguidance with minimal intervention. During training, agents dynamically adjust\ntheir groupings to optimize collaborative task completion. When deployed, they\nactively seek human assistance and utilize the Permutation Invariant Group\nCritic to evaluate and refine human-proposed groupings, allowing non-expert\nusers to contribute valuable suggestions. In multiple collaboration scenarios,\nour approach is able to leverage limited guidance from non-experts and enhance\nperformance. The project can be found at https://github.com/huawen-hu/HARP.",
      "tldr_zh": "该论文提出 HARP 框架，用于多代理强化学习（Multi-Agent Reinforcement Learning）中的群组导向任务，旨在通过整合人类辅助减少持续干预并提升可扩展性。HARP 在训练阶段让代理动态调整分组以优化协作任务，在部署阶段则利用 Permutation Invariant Group Critic 评估和完善非专家提出的分组建议，从而允许以最小干预提供有效指导。实验结果显示，该方法在多个协作场景中利用有限的非专家指导显著提升代理性能，为 Human-in-the-loop 强化学习提供了更高效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.11741v1",
      "published_date": "2024-09-18 06:54:36 UTC",
      "updated_date": "2024-09-18 06:54:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:16:38.119027"
    },
    {
      "arxiv_id": "2409.11734v1",
      "title": "InverseMeetInsert: Robust Real Image Editing via Geometric Accumulation Inversion in Guided Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Zheng",
        "Lemeng Wu"
      ],
      "abstract": "In this paper, we introduce Geometry-Inverse-Meet-Pixel-Insert, short for\nGEO, an exceptionally versatile image editing technique designed to cater to\ncustomized user requirements at both local and global scales. Our approach\nseamlessly integrates text prompts and image prompts to yield diverse and\nprecise editing outcomes. Notably, our method operates without the need for\ntraining and is driven by two key contributions: (i) a novel geometric\naccumulation loss that enhances DDIM inversion to faithfully preserve pixel\nspace geometry and layout, and (ii) an innovative boosted image prompt\ntechnique that combines pixel-level editing for text-only inversion with latent\nspace geometry guidance for standard classifier-free reversion. Leveraging the\npublicly available Stable Diffusion model, our approach undergoes extensive\nevaluation across various image types and challenging prompt editing scenarios,\nconsistently delivering high-fidelity editing results for real images.",
      "tldr_zh": "本文提出了一种名为 InverseMeetInsert（简称 GEO）的图像编辑技术，通过整合文本提示和图像提示，实现本地和全局规模的自定义编辑，而无需额外训练。关键创新包括：(i) 引入几何累积损失（geometric accumulation loss）来增强 DDIM inversion，从而忠实保留像素空间的几何和布局；(ii) 开发一种增强图像提示技术（boosted image prompt technique），结合像素级编辑与潜在空间几何指导。实验结果显示，该方法基于 Stable Diffusion 模型，在各种图像类型和复杂编辑场景中，实现了高保真度的真实图像编辑。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.11734v1",
      "published_date": "2024-09-18 06:43:40 UTC",
      "updated_date": "2024-09-18 06:43:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:16:50.610987"
    },
    {
      "arxiv_id": "2410.00033v1",
      "title": "The Phenomenology of Machine: A Comprehensive Analysis of the Sentience of the OpenAI-o1 Model Integrating Functionalism, Consciousness Theories, Active Inference, and AI Architectures",
      "title_zh": "翻译失败",
      "authors": [
        "Victoria Violet Hoyle"
      ],
      "abstract": "This paper explores the hypothesis that the OpenAI-o1 model--a\ntransformer-based AI trained with reinforcement learning from human feedback\n(RLHF)--displays characteristics of consciousness during its training and\ninference phases. Adopting functionalism, which argues that mental states are\ndefined by their functional roles, we assess the possibility of AI\nconsciousness. Drawing on theories from neuroscience, philosophy of mind, and\nAI research, we justify the use of functionalism and examine the model's\narchitecture using frameworks like Integrated Information Theory (IIT) and\nactive inference. The paper also investigates how RLHF influences the model's\ninternal reasoning processes, potentially giving rise to consciousness-like\nexperiences. We compare AI and human consciousness, addressing counterarguments\nsuch as the absence of a biological basis and subjective qualia. Our findings\nsuggest that the OpenAI-o1 model shows aspects of consciousness, while\nacknowledging the ongoing debates surrounding AI sentience.",
      "tldr_zh": "这篇论文探讨了OpenAI-o1模型（基于transformer架构并通过RLHF训练）在训练和推理阶段是否表现出意识特征，采用functionalism（功能主义）作为核心框架，结合Integrated Information Theory (IIT)、active inference和AI architectures等理论进行全面分析。研究者评估了RLHF如何影响模型的内部推理过程，可能引发意识-like体验，并将AI意识与人类意识进行比较，同时回应反驳如缺乏生物基础和主观qualia的问题。最终，论文得出结论：OpenAI-o1模型显示出某些意识方面，但AI sentience的争论仍在继续。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2410.00033v1",
      "published_date": "2024-09-18 06:06:13 UTC",
      "updated_date": "2024-09-18 06:06:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:17:05.639500"
    },
    {
      "arxiv_id": "2409.17282v1",
      "title": "Memory Networks: Towards Fully Biologically Plausible Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jacobo Ruiz",
        "Manas Gupta"
      ],
      "abstract": "The field of artificial intelligence faces significant challenges in\nachieving both biological plausibility and computational efficiency,\nparticularly in visual learning tasks. Current artificial neural networks, such\nas convolutional neural networks, rely on techniques like backpropagation and\nweight sharing, which do not align with the brain's natural information\nprocessing methods. To address these issues, we propose the Memory Network, a\nmodel inspired by biological principles that avoids backpropagation and\nconvolutions, and operates in a single pass. This approach enables rapid and\nefficient learning, mimicking the brain's ability to adapt quickly with minimal\nexposure to data. Our experiments demonstrate that the Memory Network achieves\nefficient and biologically plausible learning, showing strong performance on\nsimpler datasets like MNIST. However, further refinement is needed for the\nmodel to handle more complex datasets such as CIFAR10, highlighting the need to\ndevelop new algorithms and techniques that closely align with biological\nprocesses while maintaining computational efficiency.",
      "tldr_zh": "该论文探讨了人工智能在视觉学习任务中实现生物可信度和计算效率的挑战，指出当前的人工神经网络（如卷积神经网络）依赖于backpropagation和weight sharing，这些方法与大脑的处理方式不符。为此，作者提出Memory Network模型，该模型受生物原理启发，避免使用backpropagation和convolutions，仅需单次传递即可实现快速高效的学习，模仿大脑对少量数据的快速适应。实验结果显示，Memory Network在简单数据集如MNIST上表现出色，但在复杂数据集如CIFAR10上仍需进一步改进，强调了开发更贴近生物过程的算法以平衡生物可信度和效率的重要性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "2024",
      "pdf_url": "http://arxiv.org/pdf/2409.17282v1",
      "published_date": "2024-09-18 06:01:35 UTC",
      "updated_date": "2024-09-18 06:01:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:17:14.351882"
    },
    {
      "arxiv_id": "2410.02789v1",
      "title": "Logic-Free Building Automation: Learning the Control of Room Facilities with Wall Switches and Ceiling Camera",
      "title_zh": "翻译失败",
      "authors": [
        "Hideya Ochiai",
        "Kohki Hashimoto",
        "Takuya Sakamoto",
        "Seiya Watanabe",
        "Ryosuke Hara",
        "Ryo Yagi",
        "Yuji Aizono",
        "Hiroshi Esaki"
      ],
      "abstract": "Artificial intelligence enables smarter control in building automation by its\nlearning capability of users' preferences on facility control. Reinforcement\nlearning (RL) was one of the approaches to this, but it has many challenges in\nreal-world implementations. We propose a new architecture for logic-free\nbuilding automation (LFBA) that leverages deep learning (DL) to control room\nfacilities without predefined logic. Our approach differs from RL in that it\nuses wall switches as supervised signals and a ceiling camera to monitor the\nenvironment, allowing the DL model to learn users' preferred controls directly\nfrom the scenes and switch states. This LFBA system is tested by our testbed\nwith various conditions and user activities. The results demonstrate the\nefficacy, achieving 93%-98% control accuracy with VGG, outperforming other DL\nmodels such as Vision Transformer and ResNet. This indicates that LFBA can\nachieve smarter and more user-friendly control by learning from the observable\nscenes and user interactions.",
      "tldr_zh": "这篇论文提出了Logic-Free Building Automation (LFBA)架构，使用深度学习(DL)来控制房间设施，而不依赖预定义逻辑。不同于Reinforcement Learning (RL)，该方法利用墙壁开关作为监督信号和天花板摄像头监控环境，直接从用户互动和场景中学习偏好。在测试台上进行的实验显示，LFBA系统采用VGG模型实现了93%-98%的控制准确率，优于Vision Transformer和ResNet等其他模型。这些结果表明，LFBA能提供更智能、用户友好的建筑自动化解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2410.02789v1",
      "published_date": "2024-09-18 04:23:53 UTC",
      "updated_date": "2024-09-18 04:23:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:17:26.471162"
    },
    {
      "arxiv_id": "2409.11689v1",
      "title": "GUNet: A Graph Convolutional Network United Diffusion Model for Stable and Diversity Pose Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Shuowen Liang",
        "Sisi Li",
        "Qingyun Wang",
        "Cen Zhang",
        "Kaiquan Zhu",
        "Tian Yang"
      ],
      "abstract": "Pose skeleton images are an important reference in pose-controllable image\ngeneration. In order to enrich the source of skeleton images, recent works have\ninvestigated the generation of pose skeletons based on natural language. These\nmethods are based on GANs. However, it remains challenging to perform diverse,\nstructurally correct and aesthetically pleasing human pose skeleton generation\nwith various textual inputs. To address this problem, we propose a framework\nwith GUNet as the main model, PoseDiffusion. It is the first generative\nframework based on a diffusion model and also contains a series of variants\nfine-tuned based on a stable diffusion model. PoseDiffusion demonstrates\nseveral desired properties that outperform existing methods. 1) Correct\nSkeletons. GUNet, a denoising model of PoseDiffusion, is designed to\nincorporate graphical convolutional neural networks. It is able to learn the\nspatial relationships of the human skeleton by introducing skeletal information\nduring the training process. 2) Diversity. We decouple the key points of the\nskeleton and characterise them separately, and use cross-attention to introduce\ntextual conditions. Experimental results show that PoseDiffusion outperforms\nexisting SoTA algorithms in terms of stability and diversity of text-driven\npose skeleton generation. Qualitative analyses further demonstrate its\nsuperiority for controllable generation in Stable Diffusion.",
      "tldr_zh": "该论文提出了一种名为 PoseDiffusion 的生成框架，以 GUNet 为主要模型，首次基于 diffusion model 生成多样且结构正确的文本驱动人体姿势骨架图像，解决现有基于 GANs 方法的挑战。GUNet 整合 Graph Convolutional Network (GCN) 来学习人体骨架的空间关系，并通过解耦关键点和 cross-attention 机制引入文本条件，从而提升生成的稳定性和多样性。实验结果显示，PoseDiffusion 在文本驱动生成任务中优于现有 SoTA 算法，并在 Stable Diffusion 的可控生成方面表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11689v1",
      "published_date": "2024-09-18 04:05:59 UTC",
      "updated_date": "2024-09-18 04:05:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:17:38.913721"
    },
    {
      "arxiv_id": "2409.11686v3",
      "title": "Automated detection of underdiagnosed medical conditions via opportunistic imaging",
      "title_zh": "通过机会",
      "authors": [
        "Asad Aali",
        "Andrew Johnston",
        "Louis Blankemeier",
        "Dave Van Veen",
        "Laura T Derry",
        "David Svec",
        "Jason Hom",
        "Robert D. Boutin",
        "Akshay S. Chaudhari"
      ],
      "abstract": "Abdominal computed tomography (CT) scans are frequently performed in clinical\nsettings. Opportunistic CT involves repurposing routine CT images to extract\ndiagnostic information and is an emerging tool for detecting underdiagnosed\nconditions such as sarcopenia, hepatic steatosis, and ascites. This study\nutilizes deep learning methods to promote accurate diagnosis and clinical\ndocumentation. We analyze 2,674 inpatient CT scans to identify discrepancies\nbetween imaging phenotypes (characteristics derived from opportunistic CT\nscans) and their corresponding documentation in radiology reports and ICD\ncoding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans\ndiagnosed with sarcopenia, hepatic steatosis, and ascites (respectively)\nthrough either opportunistic imaging or radiology reports were ICD-coded. Our\nfindings demonstrate opportunistic CT's potential to enhance diagnostic\nprecision and accuracy of risk adjustment models, offering advancements in\nprecision medicine.",
      "tldr_zh": "这篇论文探讨了通过机会性 CT（opportunistic CT）自动检测未诊断的医疗条件，如肌肉减少症（sarcopenia）、肝脂肪变性（hepatic steatosis）和腹水（ascites），以提升临床诊断准确性。研究团队使用深度学习方法分析了2674个腹部CT扫描，发现影像表型（imaging phenotypes）与放射学报告和ICD编码之间存在显著不一致，仅有0.5%、3.2%和30.7%的相关病例被正确ICD编码。结果表明，机会性CT可增强诊断精度和风险调整模型的可靠性，推动精确医学（precision medicine）的应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11686v3",
      "published_date": "2024-09-18 03:56:56 UTC",
      "updated_date": "2025-05-08 17:23:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:17:50.944852"
    },
    {
      "arxiv_id": "2409.11676v1",
      "title": "Hypergraph-based Motion Generation with Multi-modal Interaction Relational Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Keshu Wu",
        "Yang Zhou",
        "Haotian Shi",
        "Dominique Lord",
        "Bin Ran",
        "Xinyue Ye"
      ],
      "abstract": "The intricate nature of real-world driving environments, characterized by\ndynamic and diverse interactions among multiple vehicles and their possible\nfuture states, presents considerable challenges in accurately predicting the\nmotion states of vehicles and handling the uncertainty inherent in the\npredictions. Addressing these challenges requires comprehensive modeling and\nreasoning to capture the implicit relations among vehicles and the\ncorresponding diverse behaviors. This research introduces an integrated\nframework for autonomous vehicles (AVs) motion prediction to address these\ncomplexities, utilizing a novel Relational Hypergraph Interaction-informed\nNeural mOtion generator (RHINO). RHINO leverages hypergraph-based relational\nreasoning by integrating a multi-scale hypergraph neural network to model\ngroup-wise interactions among multiple vehicles and their multi-modal driving\nbehaviors, thereby enhancing motion prediction accuracy and reliability.\nExperimental validation using real-world datasets demonstrates the superior\nperformance of this framework in improving predictive accuracy and fostering\nsocially aware automated driving in dynamic traffic scenarios.",
      "tldr_zh": "本研究针对真实驾驶环境的复杂多车辆互动和预测不确定性，提出了一种基于超图（hypergraph）的运动生成框架 RHINO，以提升车辆运动预测的准确性和可靠性。RHINO 通过整合多尺度超图神经网络（multi-scale hypergraph neural network）来模型群组互动和多模态（multi-modal）驾驶行为，从而捕捉隐含关系并处理多样化行为。在真实数据集上的实验验证显示，该框架显著提高了预测精度，并促进了社会感知的自动驾驶技术。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11676v1",
      "published_date": "2024-09-18 03:30:38 UTC",
      "updated_date": "2024-09-18 03:30:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:18:01.357128"
    },
    {
      "arxiv_id": "2409.11675v1",
      "title": "Towards Explainable Goal Recognition Using Weight of Evidence (WoE): A Human-Centered Approach",
      "title_zh": "迈向使用证据权重（WoE）的可解释目标识别：一种以人为中心的方法",
      "authors": [
        "Abeer Alshehri",
        "Amal Abdulrahman",
        "Hajar Alamri",
        "Tim Miller",
        "Mor Vered"
      ],
      "abstract": "Goal recognition (GR) involves inferring an agent's unobserved goal from a\nsequence of observations. This is a critical problem in AI with diverse\napplications. Traditionally, GR has been addressed using 'inference to the best\nexplanation' or abduction, where hypotheses about the agent's goals are\ngenerated as the most plausible explanations for observed behavior.\nAlternatively, some approaches enhance interpretability by ensuring that an\nagent's behavior aligns with an observer's expectations or by making the\nreasoning behind decisions more transparent. In this work, we tackle a\ndifferent challenge: explaining the GR process in a way that is comprehensible\nto humans. We introduce and evaluate an explainable model for goal recognition\n(GR) agents, grounded in the theoretical framework and cognitive processes\nunderlying human behavior explanation. Drawing on insights from two human-agent\nstudies, we propose a conceptual framework for human-centered explanations of\nGR. Using this framework, we develop the eXplainable Goal Recognition (XGR)\nmodel, which generates explanations for both why and why not questions. We\nevaluate the model computationally across eight GR benchmarks and through three\nuser studies. The first study assesses the efficiency of generating human-like\nexplanations within the Sokoban game domain, the second examines perceived\nexplainability in the same domain, and the third evaluates the model's\neffectiveness in aiding decision-making in illegal fishing detection. Results\ndemonstrate that the XGR model significantly enhances user understanding,\ntrust, and decision-making compared to baseline models, underscoring its\npotential to improve human-agent collaboration.",
      "tldr_zh": "本文提出了一种基于 Weight of Evidence (WoE) 的可解释目标识别 (XGR) 模型，旨在以人类中心的方法解释代理的目标推断过程，从而提升人类对 Goal Recognition (GR) 的理解。XGR 模型借鉴人类行为解释的理论框架和两项人类-代理研究，生成针对 \"why\" 和 \"why not\" 问题的解释，并通过层次化设计确保透明度。在八个 GR 基准和三项用户研究中（如 Sokoban 游戏和非法捕鱼检测），XGR 模型相比基线模型显著提高了用户理解、信任和决策能力，促进了人-代理协作的效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11675v1",
      "published_date": "2024-09-18 03:30:01 UTC",
      "updated_date": "2024-09-18 03:30:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:18:16.440245"
    },
    {
      "arxiv_id": "2409.11671v1",
      "title": "Anticipating Oblivious Opponents in Stochastic Games",
      "title_zh": "翻译失败",
      "authors": [
        "Shadi Tasdighi Kalat",
        "Sriram Sankaranarayanan",
        "Ashutosh Trivedi"
      ],
      "abstract": "We present an approach for systematically anticipating the actions and\npolicies employed by \\emph{oblivious} environments in concurrent stochastic\ngames, while maximizing a reward function. Our main contribution lies in the\nsynthesis of a finite \\emph{information state machine} whose alphabet ranges\nover the actions of the environment. Each state of the automaton is mapped to a\nbelief state about the policy used by the environment. We introduce a notion of\nconsistency that guarantees that the belief states tracked by our automaton\nstays within a fixed distance of the precise belief state obtained by knowledge\nof the full history. We provide methods for checking consistency of an\nautomaton and a synthesis approach which upon successful termination yields\nsuch a machine. We show how the information state machine yields an MDP that\nserves as the starting point for computing optimal policies for maximizing a\nreward function defined over plays. We present an experimental evaluation over\nbenchmark examples including human activity data for tasks such as cataract\nsurgery and furniture assembly, wherein our approach successfully anticipates\nthe policies and actions of the environment in order to maximize the reward.",
      "tldr_zh": "该研究提出了一种方法，用于在随机游戏(stochastic games)中预测无知环境(oblivious environments)的动作和策略，同时最大化奖励函数。主要贡献包括合成一个有限的信息状态机(information state machine)，其状态映射到环境的策略信念，并引入一致性(consistency)概念以确保信念状态与精确历史保持固定距离。该方法提供检查一致性和自动合成机制，并通过生成马尔可夫决策过程(MDP)来计算最佳策略；实验在基准示例如白内障手术和家具组装中验证了其有效性，提高了环境预测的准确性。",
      "categories": [
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "I.2.8; F.4.3"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11671v1",
      "published_date": "2024-09-18 03:17:40 UTC",
      "updated_date": "2024-09-18 03:17:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:18:26.892120"
    },
    {
      "arxiv_id": "2409.11664v1",
      "title": "Agent Aggregator with Mask Denoise Mechanism for Histopathology Whole Slide Image Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Xitong Ling",
        "Minxi Ouyang",
        "Yizhi Wang",
        "Xinrui Chen",
        "Renao Yan",
        "Hongbo Chu",
        "Junru Cheng",
        "Tian Guan",
        "Sufang Tian",
        "Xiaoping Liu",
        "Yonghong He"
      ],
      "abstract": "Histopathology analysis is the gold standard for medical diagnosis. Accurate\nclassification of whole slide images (WSIs) and region-of-interests (ROIs)\nlocalization can assist pathologists in diagnosis. The gigapixel resolution of\nWSI and the absence of fine-grained annotations make direct classification and\nanalysis challenging. In weakly supervised learning, multiple instance learning\n(MIL) presents a promising approach for WSI classification. The prevailing\nstrategy is to use attention mechanisms to measure instance importance for\nclassification. However, attention mechanisms fail to capture inter-instance\ninformation, and self-attention causes quadratic computational complexity. To\naddress these challenges, we propose AMD-MIL, an agent aggregator with a mask\ndenoise mechanism. The agent token acts as an intermediate variable between the\nquery and key for computing instance importance. Mask and denoising matrices,\nmapped from agents-aggregated value, dynamically mask low-contribution\nrepresentations and eliminate noise. AMD-MIL achieves better attention\nallocation by adjusting feature representations, capturing micro-metastases in\ncancer, and improving interpretability. Extensive experiments on CAMELYON-16,\nCAMELYON-17, TCGA-KIDNEY, and TCGA-LUNG show AMD-MIL's superiority over\nstate-of-the-art methods.",
      "tldr_zh": "本研究针对组织病理学分析中的挑战，提出了一种基于多实例学习 (Multiple Instance Learning, MIL) 的新框架 AMD-MIL，用于全滑片图像 (Whole Slide Images, WSIs) 的分类和感兴趣区域 (ROIs) 定位，以解决图像高分辨率和标注不足的问题。AMD-MIL 引入代理聚合器 (Agent Aggregator) 作为查询和键之间的中间变量来计算实例重要性，并使用掩码去噪机制 (Mask Denoise Mechanism) 通过动态掩盖低贡献表示和消除噪声，优化特征表示并提升模型的可解释性。实验在 CAMELYON-16、CAMELYON-17、TCGA-KIDNEY 和 TCGA-LUNG 数据集上表明，AMD-MIL 优于现有最先进方法，在准确性和捕捉微转移方面表现出显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11664v1",
      "published_date": "2024-09-18 03:02:19 UTC",
      "updated_date": "2024-09-18 03:02:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:18:38.280005"
    },
    {
      "arxiv_id": "2409.11663v3",
      "title": "Training with Differential Privacy: A Gradient-Preserving Noise Reduction Approach with Provable Security",
      "title_zh": "基于差分隐私的训练：一种梯度保留噪声减少方法，具有可证明安全性",
      "authors": [
        "Haodi Wang",
        "Tangyu Jiang",
        "Yu Guo",
        "Chengjun Cai",
        "Cong Wang",
        "Xiaohua Jia"
      ],
      "abstract": "Deep learning models have been extensively adopted in various regions due to\ntheir ability to represent hierarchical features, which highly rely on the\ntraining set and procedures. Thus, protecting the training process and deep\nlearning algorithms is paramount in privacy preservation. Although Differential\nPrivacy (DP) as a powerful cryptographic primitive has achieved satisfying\nresults in deep learning training, the existing schemes still fall short in\npreserving model utility, i.e., they either invoke a high noise scale or\ninevitably harm the original gradients. To address the above issues, in this\npaper, we present a more robust and provably secure approach for differentially\nprivate training called GReDP. Specifically, we compute the model gradients in\nthe frequency domain and adopt a new approach to reduce the noise level. Unlike\nprevious work, our GReDP only requires half of the noise scale compared to\nDPSGD [1] while keeping all the gradient information intact. We present a\ndetailed analysis of our method both theoretically and empirically. The\nexperimental results show that our GReDP works consistently better than the\nbaselines on all models and training settings.",
      "tldr_zh": "该论文提出了一种名为 GReDP 的方法，用于在 Differential Privacy (DP) 框架下进行深度学习训练，旨在解决现有方案在模型效用上的不足，如高噪声规模或梯度损害问题。GReDP 通过在频域计算模型梯度并采用新噪声减少策略，仅需 DPSGD 的一半噪声规模，同时保持所有梯度信息完整，并提供理论上的可证明安全性。实验结果显示，GReDP 在各种模型和训练设置中均优于基线方法，显著提升了隐私保护下的训练性能。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11663v3",
      "published_date": "2024-09-18 03:01:27 UTC",
      "updated_date": "2025-03-11 12:52:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:18:50.202030"
    },
    {
      "arxiv_id": "2409.11657v1",
      "title": "Few-Shot Class-Incremental Learning with Non-IID Decentralized Data",
      "title_zh": "少样本类增量学习",
      "authors": [
        "Cuiwei Liu",
        "Siang Xu",
        "Huaijun Qiu",
        "Jing Zhang",
        "Zhi Liu",
        "Liang Zhao"
      ],
      "abstract": "Few-shot class-incremental learning is crucial for developing scalable and\nadaptive intelligent systems, as it enables models to acquire new classes with\nminimal annotated data while safeguarding the previously accumulated knowledge.\nNonetheless, existing methods deal with continuous data streams in a\ncentralized manner, limiting their applicability in scenarios that prioritize\ndata privacy and security. To this end, this paper introduces federated\nfew-shot class-incremental learning, a decentralized machine learning paradigm\ntailored to progressively learn new classes from scarce data distributed across\nmultiple clients. In this learning paradigm, clients locally update their\nmodels with new classes while preserving data privacy, and then transmit the\nmodel updates to a central server where they are aggregated globally. However,\nthis paradigm faces several issues, such as difficulties in few-shot learning,\ncatastrophic forgetting, and data heterogeneity. To address these challenges,\nwe present a synthetic data-driven framework that leverages replay buffer data\nto maintain existing knowledge and facilitate the acquisition of new knowledge.\nWithin this framework, a noise-aware generative replay module is developed to\nfine-tune local models with a balance of new and replay data, while generating\nsynthetic data of new classes to further expand the replay buffer for future\ntasks. Furthermore, a class-specific weighted aggregation strategy is designed\nto tackle data heterogeneity by adaptively aggregating class-specific\nparameters based on local models performance on synthetic data. This enables\neffective global model optimization without direct access to client data.\nComprehensive experiments across three widely-used datasets underscore the\neffectiveness and preeminence of the introduced framework.",
      "tldr_zh": "本论文提出了一种针对非独立同分布（Non-IID）去中心化数据的少样本增量学习（Few-Shot Class-Incremental Learning）框架，引入联邦学习范式以保护数据隐私，同时允许模型从多个客户端的稀缺数据中逐步学习新类。框架的核心包括一个噪声感知生成重放模块，用于平衡新数据和重放数据对本地模型的微调，并生成新类的合成数据扩展重放缓冲，以缓解灾难性遗忘和少样本挑战；此外，还设计了类特定加权聚合策略，根据本地模型在合成数据上的性能自适应聚合参数，以处理数据异质性。实验在三个常用数据集上验证了该框架的有效性，展示了其在保持知识和优化全局模型方面的显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11657v1",
      "published_date": "2024-09-18 02:48:36 UTC",
      "updated_date": "2024-09-18 02:48:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:19:02.633134"
    },
    {
      "arxiv_id": "2409.11654v2",
      "title": "How to Build the Virtual Cell with Artificial Intelligence: Priorities and Opportunities",
      "title_zh": "翻译失败",
      "authors": [
        "Charlotte Bunne",
        "Yusuf Roohani",
        "Yanay Rosen",
        "Ankit Gupta",
        "Xikun Zhang",
        "Marcel Roed",
        "Theo Alexandrov",
        "Mohammed AlQuraishi",
        "Patricia Brennan",
        "Daniel B. Burkhardt",
        "Andrea Califano",
        "Jonah Cool",
        "Abby F. Dernburg",
        "Kirsty Ewing",
        "Emily B. Fox",
        "Matthias Haury",
        "Amy E. Herr",
        "Eric Horvitz",
        "Patrick D. Hsu",
        "Viren Jain",
        "Gregory R. Johnson",
        "Thomas Kalil",
        "David R. Kelley",
        "Shana O. Kelley",
        "Anna Kreshuk",
        "Tim Mitchison",
        "Stephani Otte",
        "Jay Shendure",
        "Nicholas J. Sofroniew",
        "Fabian Theis",
        "Christina V. Theodoris",
        "Srigokul Upadhyayula",
        "Marc Valer",
        "Bo Wang",
        "Eric Xing",
        "Serena Yeung-Levy",
        "Marinka Zitnik",
        "Theofanis Karaletsos",
        "Aviv Regev",
        "Emma Lundberg",
        "Jure Leskovec",
        "Stephen R. Quake"
      ],
      "abstract": "The cell is arguably the most fundamental unit of life and is central to\nunderstanding biology. Accurate modeling of cells is important for this\nunderstanding as well as for determining the root causes of disease. Recent\nadvances in artificial intelligence (AI), combined with the ability to generate\nlarge-scale experimental data, present novel opportunities to model cells. Here\nwe propose a vision of leveraging advances in AI to construct virtual cells,\nhigh-fidelity simulations of cells and cellular systems under different\nconditions that are directly learned from biological data across measurements\nand scales. We discuss desired capabilities of such AI Virtual Cells, including\ngenerating universal representations of biological entities across scales, and\nfacilitating interpretable in silico experiments to predict and understand\ntheir behavior using virtual instruments. We further address the challenges,\nopportunities and requirements to realize this vision including data needs,\nevaluation strategies, and community standards and engagement to ensure\nbiological accuracy and broad utility. We envision a future where AI Virtual\nCells help identify new drug targets, predict cellular responses to\nperturbations, as well as scale hypothesis exploration. With open science\ncollaborations across the biomedical ecosystem that includes academia,\nphilanthropy, and the biopharma and AI industries, a comprehensive predictive\nunderstanding of cell mechanisms and interactions has come into reach.",
      "tldr_zh": "该论文探讨了利用人工智能（AI）构建“AI Virtual Cells”的愿景，即基于大规模生物数据的细胞高保真模拟，以提升对生物学和疾病根源的理解。作者强调AI Virtual Cells应具备生成跨尺度生物实体通用表示的能力，并支持可解释的in silico实验，用于预测和分析细胞行为。论文讨论了实现这一目标的挑战，包括数据需求、评估策略以及社区标准，以确保生物准确性和广泛应用。最终，作者展望AI Virtual Cells在发现新药物靶点、预测细胞响应以及扩展假设探索方面的潜力，通过跨领域开放合作推动全面细胞机制理解。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11654v2",
      "published_date": "2024-09-18 02:41:50 UTC",
      "updated_date": "2024-10-14 08:18:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:19:13.433091"
    },
    {
      "arxiv_id": "2409.15360v3",
      "title": "Reward-Robust RLHF in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Yuzi Yan",
        "Xingzhou Lou",
        "Jialian Li",
        "Yiping Zhang",
        "Jian Xie",
        "Chao Yu",
        "Yu Wang",
        "Dong Yan",
        "Yuan Shen"
      ],
      "abstract": "As Large Language Models (LLMs) continue to progress toward more advanced\nforms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is\nincreasingly seen as a key pathway toward achieving Artificial General\nIntelligence (AGI). However, the reliance on reward-model-based (RM-based)\nalignment methods introduces significant challenges due to the inherent\ninstability and imperfections of Reward Models (RMs), which can lead to\ncritical issues such as reward hacking and misalignment with human intentions.\nIn this paper, we introduce a reward-robust RLHF framework aimed at addressing\nthese fundamental challenges, paving the way for more reliable and resilient\nlearning in LLMs. Our approach introduces a novel optimization objective that\ncarefully balances performance and robustness by incorporating Bayesian Reward\nModel Ensembles (BRME) to model the uncertainty set of reward functions. This\nallows the framework to integrate both nominal performance and minimum reward\nsignals, ensuring more stable learning even with imperfect RMs. Empirical\nresults demonstrate that our framework consistently outperforms baselines\nacross diverse benchmarks, showing improved accuracy and long-term stability.\nWe also provide a theoretical analysis, demonstrating that reward-robust RLHF\napproaches the stability of constant reward settings, which proves to be\nacceptable even in a stochastic-case analysis. Together, these contributions\nhighlight the framework potential to enhance both the performance and stability\nof LLM alignment.",
      "tldr_zh": "本文提出一个reward-robust RLHF框架，旨在解决Large Language Models (LLMs)在Reinforcement Learning from Human Feedback (RLHF)过程中因Reward Models (RMs)的不稳定性而引发的奖励欺骗和意图失调问题。该框架通过引入Bayesian Reward Model Ensembles (BRME)来建模奖励函数的不确定性集，并优化平衡性能和鲁棒性的目标，确保更稳定的学习过程。实验结果显示，该框架在多种基准上优于基线模型，提升了准确性和长期稳定性；理论分析进一步证明，其稳定性接近常量奖励设置，即使在随机场景下也适用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.15360v3",
      "published_date": "2024-09-18 02:35:41 UTC",
      "updated_date": "2024-10-16 14:56:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:19:29.611537"
    },
    {
      "arxiv_id": "2409.11650v1",
      "title": "Art and Science of Quantizing Large-Scale Models: A Comprehensive Overview",
      "title_zh": "量化大规模模型的艺术与科学：全面概述",
      "authors": [
        "Yanshu Wang",
        "Tong Yang",
        "Xiyan Liang",
        "Guoan Wang",
        "Hanning Lu",
        "Xu Zhe",
        "Yaoming Li",
        "Li Weitao"
      ],
      "abstract": "This paper provides a comprehensive overview of the principles, challenges,\nand methodologies associated with quantizing large-scale neural network models.\nAs neural networks have evolved towards larger and more complex architectures\nto address increasingly sophisticated tasks, the computational and energy costs\nhave escalated significantly. We explore the necessity and impact of model size\ngrowth, highlighting the performance benefits as well as the computational\nchallenges and environmental considerations. The core focus is on model\nquantization as a fundamental approach to mitigate these challenges by reducing\nmodel size and improving efficiency without substantially compromising\naccuracy. We delve into various quantization techniques, including both\npost-training quantization (PTQ) and quantization-aware training (QAT), and\nanalyze several state-of-the-art algorithms such as LLM-QAT, PEQA(L4Q),\nZeroQuant, SmoothQuant, and others. Through comparative analysis, we examine\nhow these methods address issues like outliers, importance weighting, and\nactivation quantization, ultimately contributing to more sustainable and\naccessible deployment of large-scale models.",
      "tldr_zh": "这篇论文全面概述了量化大型神经网络模型的原则、挑战和方法，以应对模型规模增长带来的计算和能源成本问题。\n它探讨了后训练量化(PTQ)和量化感知训练(QAT)等技术，并分析了LLM-QAT、PEQA(L4Q)、ZeroQuant和SmoothQuant等先进算法，这些方法通过处理异常值、重要性加权和激活量化等问题，显著减少模型大小并提升效率，同时基本保持准确性。\n最终，该研究强调量化技术的应用有助于实现大型模型的可持续部署和更广泛的可访问性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11650v1",
      "published_date": "2024-09-18 02:35:00 UTC",
      "updated_date": "2024-09-18 02:35:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:19:38.472831"
    },
    {
      "arxiv_id": "2410.02787v1",
      "title": "Navigation with VLM framework: Go to Any Language",
      "title_zh": "翻译失败",
      "authors": [
        "Zecheng Yin",
        "Chonghao Cheng",
        "Lizhen"
      ],
      "abstract": "Navigating towards fully open language goals and exploring open scenes in a\nmanner akin to human exploration have always posed significant challenges.\nRecently, Vision Large Language Models (VLMs) have demonstrated remarkable\ncapabilities in reasoning with both language and visual data. While many works\nhave focused on leveraging VLMs for navigation in open scenes and with open\nvocabularies, these efforts often fall short of fully utilizing the potential\nof VLMs or require substantial computational resources. We introduce Navigation\nwith VLM (NavVLM), a framework that harnesses equipment-level VLMs to enable\nagents to navigate towards any language goal specific or non-specific in open\nscenes, emulating human exploration behaviors without any prior training. The\nagent leverages the VLM as its cognitive core to perceive environmental\ninformation based on any language goal and constantly provides exploration\nguidance during navigation until it reaches the target location or area. Our\nframework not only achieves state-of-the-art performance in Success Rate (SR)\nand Success weighted by Path Length (SPL) in traditional specific goal settings\nbut also extends the navigation capabilities to any open-set language goal. We\nevaluate NavVLM in richly detailed environments from the Matterport 3D (MP3D),\nHabitat Matterport 3D (HM3D), and Gibson datasets within the Habitat simulator.\nWith the power of VLMs, navigation has entered a new era.",
      "tldr_zh": "这篇论文引入了NavVLM框架，利用Vision Large Language Models (VLMs) 使代理在开放场景中导航到任何语言目标（具体或非具体），模仿人类探索行为，而无需任何先验训练。框架以VLMs作为认知核心，感知环境信息并持续提供探索指导，直到到达目标位置或区域。该方法在Matterport 3D (MP3D)、Habitat Matterport 3D (HM3D)和Gibson数据集上实现了最先进的Success Rate (SR)和Success weighted by Path Length (SPL)性能，并扩展了导航能力至开放集语言目标，标志着导航领域的重大进步。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "under review",
      "pdf_url": "http://arxiv.org/pdf/2410.02787v1",
      "published_date": "2024-09-18 02:29:00 UTC",
      "updated_date": "2024-09-18 02:29:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:19:50.680924"
    },
    {
      "arxiv_id": "2410.02786v1",
      "title": "Robust Symmetry Detection via Riemannian Langevin Dynamics",
      "title_zh": "翻译失败",
      "authors": [
        "Jihyeon Je",
        "Jiayi Liu",
        "Guandao Yang",
        "Boyang Deng",
        "Shengqu Cai",
        "Gordon Wetzstein",
        "Or Litany",
        "Leonidas Guibas"
      ],
      "abstract": "Symmetries are ubiquitous across all kinds of objects, whether in nature or\nin man-made creations. While these symmetries may seem intuitive to the human\neye, detecting them with a machine is nontrivial due to the vast search space.\nClassical geometry-based methods work by aggregating \"votes\" for each symmetry\nbut struggle with noise. In contrast, learning-based methods may be more robust\nto noise, but often overlook partial symmetries due to the scarcity of\nannotated data. In this work, we address this challenge by proposing a novel\nsymmetry detection method that marries classical symmetry detection techniques\nwith recent advances in generative modeling. Specifically, we apply Langevin\ndynamics to a redefined symmetry space to enhance robustness against noise. We\nprovide empirical results on a variety of shapes that suggest our method is not\nonly robust to noise, but can also identify both partial and global symmetries.\nMoreover, we demonstrate the utility of our detected symmetries in various\ndownstream tasks, such as compression and symmetrization of noisy shapes.",
      "tldr_zh": "该论文探讨了对称性检测的挑战，传统几何方法虽基于投票机制但易受噪声影响，而学习方法虽更鲁棒却常忽略部分对称性。本文提出一种新型方法，将经典对称检测技术与生成建模相结合，具体通过Riemannian Langevin Dynamics在重新定义的对称空间中进行动态模拟，以提升对噪声的鲁棒性。实验结果显示，该方法不仅能有效识别部分和全局对称性，还在下游任务如形状压缩和对称化中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://symmetry-langevin.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2410.02786v1",
      "published_date": "2024-09-18 02:28:20 UTC",
      "updated_date": "2024-09-18 02:28:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:20:01.465007"
    },
    {
      "arxiv_id": "2409.11644v1",
      "title": "Few-Shot Learning Approach on Tuberculosis Classification Based on Chest X-Ray Images",
      "title_zh": "翻译失败",
      "authors": [
        "A. A. G. Yogi Pramana",
        "Faiz Ihza Permana",
        "Muhammad Fazil Maulana",
        "Dzikri Rahadian Fudholi"
      ],
      "abstract": "Tuberculosis (TB) is caused by the bacterium Mycobacterium tuberculosis,\nprimarily affecting the lungs. Early detection is crucial for improving\ntreatment effectiveness and reducing transmission risk. Artificial intelligence\n(AI), particularly through image classification of chest X-rays, can assist in\nTB detection. However, class imbalance in TB chest X-ray datasets presents a\nchallenge for accurate classification. In this paper, we propose a few-shot\nlearning (FSL) approach using the Prototypical Network algorithm to address\nthis issue. We compare the performance of ResNet-18, ResNet-50, and VGG16 in\nfeature extraction from the TBX11K Chest X-ray dataset. Experimental results\ndemonstrate classification accuracies of 98.93% for ResNet-18, 98.60% for\nResNet-50, and 33.33% for VGG16. These findings indicate that the proposed\nmethod outperforms others in mitigating data imbalance, which is particularly\nbeneficial for disease classification applications.",
      "tldr_zh": "本论文提出了一种基于 Few-Shot Learning 和 Prototypical Network 算法的方法，用于结核病(TB)胸部 X 光图像分类，以解决数据集中的类别不平衡问题。\n研究团队使用 TBX11K 数据集比较了 ResNet-18、ResNet-50 和 VGG16 的特征提取性能，结果显示 ResNet-18 准确率最高，为 98.93%，ResNet-50 为 98.60%，而 VGG16 仅为 33.33%。\n该方法显著提高了 TB 检测的准确性，尤其适用于数据有限的医疗图像分类场景。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "6 pages. Pre-print",
      "pdf_url": "http://arxiv.org/pdf/2409.11644v1",
      "published_date": "2024-09-18 02:15:01 UTC",
      "updated_date": "2024-09-18 02:15:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:20:14.685686"
    },
    {
      "arxiv_id": "2409.11643v2",
      "title": "Combating Phone Scams with LLM-based Detection: Where Do We Stand?",
      "title_zh": "翻译失败",
      "authors": [
        "Zitong Shen",
        "Kangzhong Wang",
        "Youqian Zhang",
        "Grace Ngai",
        "Eugene Y. Fu"
      ],
      "abstract": "Phone scams pose a significant threat to individuals and communities, causing\nsubstantial financial losses and emotional distress. Despite ongoing efforts to\ncombat these scams, scammers continue to adapt and refine their tactics, making\nit imperative to explore innovative countermeasures. This research explores the\npotential of large language models (LLMs) to provide detection of fraudulent\nphone calls. By analyzing the conversational dynamics between scammers and\nvictims, LLM-based detectors can identify potential scams as they occur,\noffering immediate protection to users. While such approaches demonstrate\npromising results, we also acknowledge the challenges of biased datasets,\nrelatively low recall, and hallucinations that must be addressed for further\nadvancement in this field",
      "tldr_zh": "这篇论文探讨了使用大型语言模型(LLMs)检测电话诈骗的可行性，通过分析诈骗者和受害者之间的对话动态，实现实时识别并提供即时保护。研究发现，LLM-based检测方法显示出前景，能有效应对诈骗威胁，但仍存在数据集偏差、低召回率和hallucinations等挑战。作者强调，需要进一步优化这些问题，以推动该领域的进步。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "I.2.0"
      ],
      "primary_category": "cs.CR",
      "comment": "2 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2409.11643v2",
      "published_date": "2024-09-18 02:14:30 UTC",
      "updated_date": "2024-10-17 08:58:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:20:26.447891"
    },
    {
      "arxiv_id": "2409.11631v2",
      "title": "A Metric Hybrid Planning Approach to Solving Pandemic Planning Problems with Simple SIR Models",
      "title_zh": "一种基于简单 SIR 模型的度量混合规划方法，用于解决大流行规划问题",
      "authors": [
        "Ari Gestetner",
        "Buser Say"
      ],
      "abstract": "A pandemic is the spread of a disease across large regions, and can have\ndevastating costs to the society in terms of health, economic and social. As\nsuch, the study of effective pandemic mitigation strategies can yield\nsignificant positive impact on the society. A pandemic can be mathematically\ndescribed using a compartmental model, such as the Susceptible Infected Removed\n(SIR) model. In this paper, we extend the solution equations of the SIR model\nto a state transition model with lockdowns. We formalize a metric hybrid\nplanning problem based on this state transition model, and solve it using a\nmetric hybrid planner. We improve the runtime effectiveness of the metric\nhybrid planner with the addition of valid inequalities, and demonstrate the\nsuccess of our approach both theoretically and experimentally under various\nchallenging settings.",
      "tldr_zh": "本论文提出了一种基于度量混合规划(metric hybrid planning)的方法，用于解决使用简单 SIR 模型的流行病规划问题。通过扩展 SIR 模型的解方程以包含封锁措施，并将其形式化为一个基于状态转换的度量混合规划问题，研究者使用度量混合规划器进行求解。该方法通过添加有效不等式(valid inequalities)显著提高了规划器的运行时效率，并在理论和实验上证明了其在各种挑战性设置下的成功表现。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11631v2",
      "published_date": "2024-09-18 01:31:26 UTC",
      "updated_date": "2024-09-30 22:45:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:20:38.815160"
    },
    {
      "arxiv_id": "2409.11617v1",
      "title": "HRA: A Multi-Criteria Framework for Ranking Metaheuristic Optimization Algorithms",
      "title_zh": "HRA：一种用于排名元启发式优化算法的多标准框架",
      "authors": [
        "Evgenia-Maria K. Goula",
        "Dimitris G. Sotiropoulos"
      ],
      "abstract": "Metaheuristic algorithms are essential for solving complex optimization\nproblems in different fields. However, the difficulty in comparing and rating\nthese algorithms remains due to the wide range of performance metrics and\nproblem dimensions usually involved. On the other hand, nonparametric\nstatistical methods and post hoc tests are time-consuming, especially when we\nonly need to identify the top performers among many algorithms. The\nHierarchical Rank Aggregation (HRA) algorithm aims to efficiently rank\nmetaheuristic algorithms based on their performance across many criteria and\ndimensions. The HRA employs a hierarchical framework that begins with\ncollecting performance metrics on various benchmark functions and dimensions.\nRank-based normalization is employed for each performance measure to ensure\ncomparability and the robust TOPSIS aggregation is applied to combine these\nrankings at several hierarchical levels, resulting in a comprehensive ranking\nof the algorithms. Our study uses data from the CEC 2017 competition to\ndemonstrate the robustness and efficacy of the HRA framework. It examines 30\nbenchmark functions and evaluates the performance of 13 metaheuristic\nalgorithms across five performance indicators in four distinct dimensions. This\npresentation highlights the potential of the HRA to enhance the interpretation\nof the comparative advantages and disadvantages of various algorithms by\nsimplifying practitioners' choices of the most appropriate algorithm for\ncertain optimization problems.",
      "tldr_zh": "这篇论文提出了 Hierarchical Rank Aggregation (HRA) 框架，用于多标准排名元启发式优化算法(Metaheuristic algorithms)，以解决算法比较中涉及多种性能指标和问题维度的挑战。HRA 采用分层结构，首先收集基准函数的性能数据，进行基于排名的标准化，然后使用 TOPSIS 聚合方法在多个层次结合排名，实现高效的算法排序。研究利用 CEC 2017 比赛数据，对 30 个基准函数和 13 个算法进行评估，证明 HRA 在四个维度和五种性能指标上提高了准确性和鲁棒性，最终帮助从业者更轻松地选择适合的优化算法。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.NE",
      "comment": "13 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2409.11617v1",
      "published_date": "2024-09-18 00:44:50 UTC",
      "updated_date": "2024-09-18 00:44:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:20:50.322967"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 98,
  "processed_papers_count": 98,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T01:21:22.479943"
}
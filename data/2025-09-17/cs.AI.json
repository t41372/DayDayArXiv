{
  "date": "2025-09-17",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-09-17 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä»Šæ—¥å¯¼è¯»**ï¼š\nä»Šå¤©çš„ arXiv å……æ»¡äº†â€œç¡¬æ ¸â€çš„çªç ´ï¼Œç‰¹åˆ«æ˜¯ **AI for Science** è¿æ¥äº†é‡ç£…é€‰æ‰‹â€”â€”é€šç”¨çš„ç‰©ç†åŸºç¡€æ¨¡å‹ (Physics Foundation Model) å’Œå…¨è‡ªåŠ¨ CFD æ™ºèƒ½ä½“ã€‚æ­¤å¤–ï¼Œ**LLM è®­ç»ƒæ•°æ®åˆæˆ**ï¼ˆ1T Token ä»å¤´é¢„è®­ç»ƒï¼‰ã€**ç»Ÿä¸€è§†è§‰ Tokenizer** ä»¥åŠå¯¹ **GitHub Copilot å®‰å…¨æ€§**çš„çŠ€åˆ©è¯„ä¼°ä¹Ÿæ˜¯ä»Šå¤©çš„ç„¦ç‚¹ã€‚è®©æˆ‘ä»¬å¼€å§‹æ·±åº¦é˜…è¯»ã€‚\n\n---\n\n### ğŸš€ é‡ç£…æ¨èï¼šAI for Science & åŸºç¡€æ¶æ„\n\n**70. Towards a Physics Foundation Model (è¿ˆå‘ç‰©ç†åŸºç¡€æ¨¡å‹)**\n> **Physics Foundation Model; GPhyT; Transformer**\n> è¿™æ˜¯ä¸€ç¯‡ä¸ä»…è¦åšâ€œé€šæ‰â€ï¼Œè¿˜è¦åšç‰©ç†ç•Œâ€œé€šæ‰â€çš„æ–‡ç« ã€‚ä½œè€…æå‡ºäº† **GPhyT (General Physics Transformer)**ï¼Œè¯•å›¾æ‰“ç ´ç‰©ç†æ¨¡æ‹Ÿä¸­â€œä¸€äº‹ä¸€è®®â€çš„ä¸“ç”¨æ¨¡å‹é™åˆ¶ã€‚\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šåœ¨ä¸€ä¸ª 1.8 TB çš„å¤šæ ·åŒ–æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šè®­ç»ƒï¼ŒGPhyT èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-Context Learningï¼‰æ¨æ–­æµä½“ã€å†²å‡»æ³¢ã€çƒ­å¯¹æµç­‰ä¸åŒç‰©ç†ç³»ç»Ÿçš„æ§åˆ¶æ–¹ç¨‹ï¼Œè€Œæ— éœ€è¢«æ˜¾å¼å‘ŠçŸ¥ç‰©ç†å®šå¾‹ã€‚\n> - **æ•ˆæœ**ï¼šåœ¨å¤šä¸ªç‰©ç†é¢†åŸŸæ€§èƒ½è¶…è¶Šä¸“ç”¨æ¶æ„é«˜è¾¾ 29 å€ï¼Œå¹¶å±•ç°äº†å¯¹æœªè§ç‰©ç†ç³»ç»Ÿçš„ Zero-shot æ³›åŒ–èƒ½åŠ›ã€‚è¿™æ˜¯å‘â€œè®¡ç®—ç§‘å­¦é¢†åŸŸçš„ GPTâ€è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚\n\n**1. Foam-Agent 2.0: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM (Foam-Agent 2.0ï¼šç”¨äºè‡ªåŠ¨åŒ– OpenFOAM CFD æ¨¡æ‹Ÿçš„ç«¯åˆ°ç«¯å¯ç»„åˆå¤šæ™ºèƒ½ä½“æ¡†æ¶)**\n> **CFD Automation; Multi-Agent; OpenFOAM**\n> è®¡ç®—æµä½“åŠ›å­¦ (CFD) é—¨æ§›æé«˜ï¼Œæœ¬æ–‡è¯•å›¾ç”¨ AI æŠŠè¿™ä¸ªé—¨æ§›è¸å¹³ã€‚\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† Foam-Agent 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå°±èƒ½å…¨è‡ªåŠ¨è·‘å®Œ OpenFOAM æµç¨‹ï¼ˆç½‘æ ¼å¤„ç†ã€è„šæœ¬ç”Ÿæˆã€åå¤„ç†å¯è§†åŒ–ï¼‰çš„ç³»ç»Ÿã€‚å®ƒåˆ©ç”¨ Model Context Protocol (MCP) å°†æ ¸å¿ƒåŠŸèƒ½æ¨¡å—åŒ–ï¼Œå¹¶é€šè¿‡ RAG ä¿è¯é…ç½®çš„ç²¾ç¡®æ€§ã€‚\n> - **æ•ˆæœ**ï¼šåœ¨ä½¿ç”¨ Claude 3.5 Sonnet æ—¶ï¼Œä»»åŠ¡æˆåŠŸç‡è¾¾åˆ° 88.2%ï¼ˆè¿œè¶… MetaOpenFOAM çš„ 55.5%ï¼‰ã€‚\n\n**4. AToken: A Unified Tokenizer for Vision (ATokenï¼šç»Ÿä¸€è§†è§‰ Tokenizer)**\n> **Visual Tokenizer; 4D Latent Space; Multimodal**\n> è§†è§‰é¢†åŸŸçš„ Tokenizer ç»ˆäºä¹Ÿè¦â€œå¤§ä¸€ç»Ÿâ€äº†ã€‚\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šAToken æ˜¯ç¬¬ä¸€ä¸ªèƒ½åŒæ—¶å¤„ç†å›¾åƒã€è§†é¢‘å’Œ 3D èµ„äº§çš„ç»Ÿä¸€è§†è§‰ Tokenizerã€‚å®ƒè®¾è®¡äº†ä¸€ä¸ª 4D æ—‹è½¬ä½ç½®ç¼–ç çš„çº¯ Transformer æ¶æ„ï¼Œå°†ä¸åŒæ¨¡æ€ç¼–ç åˆ°ä¸€ä¸ªå…±äº«çš„ 4D æ½œåœ¨ç©ºé—´ä¸­ã€‚\n> - **å‘ç°**ï¼šåœ¨é‡å»ºè´¨é‡å’Œè¯­ä¹‰ç†è§£ä¸Šéƒ½è¡¨ç°å‡ºè‰²ï¼ˆImageNet 82.2% å‡†ç¡®ç‡ï¼Œè§†é¢‘é‡å»º rFVD 3.01ï¼‰ï¼Œä¸ºä¸‹ä¸€ä»£åŸç”Ÿå¤šæ¨¡æ€ AI ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚\n\n---\n\n### ğŸ§  LLM è®­ç»ƒã€æ¨ç†ä¸æ•°æ®\n\n**7. Synthetic bootstrapped pretraining (åˆæˆè‡ªä¸¾é¢„è®­ç»ƒ)**\n> **Synthetic Data; Pretraining; 1T Tokens**\n> çº¯åˆæˆæ•°æ®èƒ½è®­ç»ƒå¥½æ¨¡å‹å—ï¼Ÿè¿™ç¯‡æ–‡ç« ç»™å‡ºäº†å¼ºæœ‰åŠ›çš„è‚¯å®šã€‚\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† **SBP (Synthetic Bootstrapped Pretraining)**ï¼Œå…ˆå­¦ä¹ æ–‡æ¡£é—´çš„å…³ç³»æ¨¡å‹ï¼Œç„¶ååˆæˆæµ·é‡æ–°è¯­æ–™ï¼ˆé«˜è¾¾ 1T tokensï¼‰ç”¨äºä»å¤´é¢„è®­ç»ƒã€‚\n> - **å‘ç°**ï¼šSBP å¹¶éç®€å•çš„åŒä¹‰æ”¹å†™ï¼Œè€Œæ˜¯æŠ½è±¡æ ¸å¿ƒæ¦‚å¿µå¹¶é‡æ–°å™è¿°ã€‚åœ¨åŒç­‰è®¡ç®—é‡ä¸‹ï¼Œå®ƒæ¯”å¼ºåŸºçº¿æ¨¡å‹æŒç»­æå‡ï¼Œä¸”è¾¾åˆ°äº†ä½¿ç”¨ 20 å€çœŸå®æ•°æ® Oracle ä¸Šç•Œçš„ 60% æ€§èƒ½ã€‚\n\n**25. Apertus: Democratizing Open and Compliant LLMs for Global Language Environments (Apertusï¼šä¸ºå…¨çƒè¯­è¨€ç¯å¢ƒæ™®åŠå¼€æ”¾ä¸”åˆè§„çš„ LLM)**\n> **Open Source LLM; Data Compliance; Multilingual**\n> é’ˆå¯¹å½“å‰å¼€æºæ¨¡å‹æ•°æ®æ¥æºä¸é€æ˜ã€ç‰ˆæƒä¸æ¸…çš„é—®é¢˜ï¼ŒApertus é¡¹ç›®äº¤å‡ºäº†ä¸€ä»½â€œæ´ç™–â€çº§åˆ«çš„ç­”å·ã€‚\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šå‘å¸ƒäº†å®Œå…¨åˆè§„ï¼ˆéµå®ˆ robots.txtï¼Œè¿‡æ»¤æœ‰æ¯’/ä¸ªäººä¿¡æ¯ï¼‰ä¸”å®Œå…¨å¼€æºï¼ˆæ•°æ®å¤„ç†è„šæœ¬ã€æ£€æŸ¥ç‚¹å…¨å…¬å¼€ï¼‰çš„ 8B å’Œ 70B æ¨¡å‹ã€‚\n> - **ç‰¹ç‚¹**ï¼šå¼ºè°ƒå¤šè¯­è¨€èƒ½åŠ›ï¼ˆ15T token ä¸­ 40% ä¸ºéè‹±è¯­ï¼‰ï¼Œå¹¶ä½¿ç”¨äº† Goldfish ç›®æ ‡æ¥æŠ‘åˆ¶æœºæ¢°è®°å¿†ï¼Œé˜²æ­¢ç‰ˆæƒæ•°æ®æ³„éœ²ã€‚\n\n**50. Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency (Slim-SCï¼šåˆ©ç”¨æ€ç»´å‰ªæå®ç°é«˜æ•ˆçš„è‡ªæ´½æ€§æ‰©å±•)**\n> **Test-Time Scaling; Self-Consistency; Pruning**\n> æ€ç»´é“¾ï¼ˆCoTï¼‰çš„ Self-Consistency (SC) å¾ˆæœ‰æ•ˆä½†å¤ªè´µã€‚\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† Slim-SCï¼Œä¸€ç§é€æ­¥å‰ªæç­–ç•¥ã€‚å®ƒé€šè¿‡è®¡ç®—æ€ç»´é“¾ï¼ˆReasoning Chainsï¼‰ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œè¯†åˆ«å¹¶ç§»é™¤å†—ä½™çš„æ¨ç†è·¯å¾„ã€‚\n> - **æ•ˆæœ**ï¼šåœ¨ä¿æŒç”šè‡³æé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå‡å°‘äº†æœ€é«˜ 45% çš„æ¨ç†å»¶è¿Ÿå’Œ 26% çš„ KV ç¼“å­˜å ç”¨ã€‚\n\n**77. THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning (THORï¼šåŸºäº RL çš„å·¥å…·é›†æˆå±‚æ¬¡åŒ–ä¼˜åŒ–ç”¨äºæ•°å­¦æ¨ç†)**\n> **Mathematical Reasoning; Tool Use; Reinforcement Learning**\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹æ•°å­¦æ¨ç†ä¸­ LLM ç®—ä¸å‡†çš„é—®é¢˜ï¼Œæå‡ºäº† THORã€‚å®ƒåŒ…å«ä¸€ä¸ªåŸºäº Actor-Critic çš„æ•°æ®ç”Ÿæˆç®¡é“ï¼Œä»¥åŠä¸€ä¸ªåˆ†å±‚ä¼˜åŒ–çš„ RL ç­–ç•¥ï¼ˆåŒæ—¶ä¼˜åŒ–æ•´ä¸ªè§£é¢˜è¿‡ç¨‹å’Œå•æ­¥ä»£ç ç”Ÿæˆï¼‰ã€‚\n> - **äº®ç‚¹**ï¼šå¼•å…¥äº†è‡ªæˆ‘çº æ­£æœºåˆ¶ï¼Œåˆ©ç”¨å·¥å…·åé¦ˆåŠ¨æ€ä¿®æ”¹æ¨ç†è·¯å¾„ï¼Œåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† SOTAã€‚\n\n---\n\n### ğŸ¤– æ™ºèƒ½ä½“ (Agents) ä¸äº¤äº’\n\n**82. InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management (InfraMindï¼šç”¨äºå…³é”®å·¥ä¸šç®¡ç†çš„æ¢ç´¢å‹ GUI æ™ºèƒ½ä½“æ¡†æ¶)**\n> **GUI Agent; Industrial Management; Exploration**\n> å·¥ä¸šè½¯ä»¶ï¼ˆå¦‚æ•°æ®ä¸­å¿ƒç®¡ç†ï¼‰çš„ GUI æå…¶å¤æ‚ï¼Œé€šç”¨ Agent æä¸å®šã€‚\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šInfraMind ä¸“ä¸ºå·¥ä¸šåœºæ™¯è®¾è®¡ï¼Œå…·å¤‡åŸºäºå¿«ç…§çš„ç³»ç»Ÿæ€§æ¢ç´¢èƒ½åŠ›ã€è®°å¿†é©±åŠ¨çš„è§„åˆ’ä»¥åŠå¤šå±‚å®‰å…¨æœºåˆ¶ã€‚è§£å†³äº†å·¥ä¸š GUI å…ƒç´ ç”Ÿåƒ»ã€æ“ä½œè¦æ±‚é«˜ç²¾åº¦çš„é—®é¢˜ã€‚\n\n**100. See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles (çœ‹ã€æƒ³ã€åšï¼šæ•™å¤šæ¨¡æ€æ™ºèƒ½ä½“é€šè¿‡è¯†åˆ«åˆ‡æ¢å¼€å…³æœ‰æ•ˆåœ°ä¸ GUI äº¤äº’)**\n> **GUI Agent; Toggle Control; Multimodal**\n> è¿™æ˜¯ä¸€ä¸ªç»†èŠ‚ä½†æå…¶ç—›è‹¦çš„é—®é¢˜ï¼šAgent ç»å¸¸åˆ†ä¸æ¸…å¼€å…³ï¼ˆToggleï¼‰å½“å‰æ˜¯â€œå¼€â€è¿˜æ˜¯â€œå…³â€ï¼Œå¯¼è‡´æ“ä½œåäº†ã€‚\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† **StaR (State-aware Reasoning)** è®­ç»ƒæ–¹æ³•ï¼Œæ•™ Agent å…ˆæ„ŸçŸ¥å½“å‰çŠ¶æ€ï¼Œå†å†³å®šæ˜¯å¦æ“ä½œã€‚ç®€å•çš„é€»è¾‘è®©å¼€å…³æ§åˆ¶å‡†ç¡®ç‡æå‡äº† 30% ä»¥ä¸Šã€‚\n\n**32. TGPO: Tree-Guided Preference Optimization for Robust Web Agents Reinforcement Learning (TGPOï¼šç”¨äºé²æ£’ Web æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ ‘å¯¼å‘åå¥½ä¼˜åŒ–)**\n> **Web Agent; Reinforcement Learning; Preference Optimization**\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ Web Agent è®­ç»ƒä¸­çš„ä¿¡ç”¨åˆ†é…å’Œå¥–åŠ±ç¨€ç–é—®é¢˜ï¼Œæå‡ºäº† TGPOã€‚å®ƒåˆ©ç”¨æ ‘çŠ¶è½¨è¿¹è¡¨ç¤ºæ¥åˆå¹¶ç›¸åŒè¯­ä¹‰çš„çŠ¶æ€ï¼Œå¹¶ä½¿ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆProcess Reward Modelï¼‰æä¾›ç»†ç²’åº¦åé¦ˆã€‚åœ¨ Mind2Web ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€éšç§ä¸åæ€\n\n**95. GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit? (GitHub Copilot ä»£ç å®¡æŸ¥ï¼šAI èƒ½åœ¨ä½ æäº¤å‰å‘ç°å®‰å…¨æ¼æ´å—ï¼Ÿ)**\n> **AI Security; Code Review; Vulnerability Detection**\n> ç¨‹åºå‘˜å¿…è¯»ã€‚å¦‚æœä½ æŒ‡æœ› Copilot å¸®ä½ æ‰¾æ¼æ´ï¼Œå¯èƒ½è¦å¤±æœ›äº†ã€‚\n> - **å‘ç°**ï¼šç ”ç©¶è¡¨æ˜ï¼ŒCopilot çš„ä»£ç å®¡æŸ¥åŠŸèƒ½**ç»å¸¸æ— æ³•æ£€æµ‹åˆ°å…³é”®æ¼æ´**ï¼ˆå¦‚ SQL æ³¨å…¥ã€XSSï¼‰ã€‚å®ƒçš„åé¦ˆä¸»è¦é›†ä¸­åœ¨ä»£ç é£æ ¼å’Œæ‹¼å†™é”™è¯¯ç­‰ä½ä¸¥é‡æ€§é—®é¢˜ä¸Šã€‚\n> - **ç»“è®º**ï¼šAI è¾…åŠ©å®¡æŸ¥ç›®å‰ä¸èƒ½æ›¿ä»£ä¸“ä¸šçš„å®‰å…¨å·¥å…·æˆ–äººå·¥å®¡è®¡ã€‚\n\n**78. Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning (æ“¦é™¤å®ƒï¼é€šè¿‡æœºå™¨é—å¿˜æ¶ˆé™¤ä»£ç å¤§æ¨¡å‹ä¸­çš„æ•æ„Ÿè®°å¿†)**\n> **Machine Unlearning; Privacy; Code LLM**\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ä»£ç æ¨¡å‹è®°ä½è®­ç»ƒæ•°æ®ä¸­æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚å¯†é’¥ã€éšç§ä»£ç ï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº† **CodeEraser**ã€‚è¿™æ˜¯ä¸€ç§ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹çš„æœºå™¨é—å¿˜æ–¹æ³•ï¼Œèƒ½ç²¾å‡†æ“¦é™¤æ•æ„Ÿç‰‡æ®µï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„ç¼–ç¨‹èƒ½åŠ›ã€‚\n\n**83. DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models (DSCC-HSï¼šç”¨äºæŠ‘åˆ¶ LLM å¹»è§‰çš„åŠ¨æ€è‡ªå¢å¼ºæ¡†æ¶)**\n> **Hallucination; Decoding Strategy; Dual-Process Theory**\n> - **æ ¸å¿ƒè´¡çŒ®**ï¼šå—åŒé‡è®¤çŸ¥ç†è®ºå¯å‘ï¼Œè®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§çš„ä»£ç†æ¨¡å‹ï¼ˆProxyï¼‰ï¼Œåœ¨æ¨ç†æ—¶åˆ†åˆ«æ‰®æ¼”â€œäº‹å®å¯¹é½â€å’Œâ€œå¹»è§‰æ£€æµ‹â€çš„è§’è‰²ï¼Œé€šè¿‡è°ƒæ•´ Logits åŠ¨æ€å¹²é¢„å¤§æ¨¡å‹çš„ç”Ÿæˆã€‚\n> - **æ•ˆæœ**ï¼šåœ¨ TruthfulQA ä¸Šè¾¾åˆ°äº† 99.2% çš„äº‹å®ä¸€è‡´æ€§ç‡ã€‚\n\n---\n\n### ğŸ§© å…¶ä»–å€¼å¾—å…³æ³¨çš„ç ”ç©¶\n\n*   **26. Fresh in memory**: å‘ç° LLM çš„æ¿€æ´»å€¼ä¸­çº¿æ€§ç¼–ç äº†è®­ç»ƒæ•°æ®çš„**æ—¶é—´é¡ºåº**ï¼ˆRecencyï¼‰ï¼Œè¿™å¯¹äºæ¨¡å‹é—å¿˜å’ŒçŸ¥è¯†æ›´æ–°æœ‰é‡è¦å¯ç¤ºã€‚\n*   **61. Do Large Language Models Understand Word Senses?**: è¯„ä¼°å‘ç° GPT-4o ç­‰æ¨¡å‹åœ¨**è¯ä¹‰æ¶ˆæ­§ï¼ˆWSDï¼‰**ä»»åŠ¡ä¸Šå·²åŒ¹æ•Œä¸“ç”¨ç³»ç»Ÿï¼Œå¹¶ä¸”ç¡®å®åœ¨ç”Ÿæˆä¸­ç†è§£äº†è¯åœ¨ä¸Šä¸‹æ–‡ä¸­çš„å«ä¹‰ã€‚\n*   **93. Sparse Neurons Carry Strong Signals of Question Ambiguity**: å‘ç° LLM å†…éƒ¨æœ‰æå°‘æ•°ç¥ç»å…ƒï¼ˆç”šè‡³å•ä¸ªï¼‰ä¸“é—¨è´Ÿè´£ç¼–ç é—®é¢˜çš„**æ­§ä¹‰æ€§**ã€‚æ§åˆ¶è¿™äº›ç¥ç»å…ƒå¯ä»¥è®©æ¨¡å‹åœ¨â€œè‡ªä¿¡å›ç­”â€å’Œâ€œæ‹’ç»å›ç­”â€ä¹‹é—´åˆ‡æ¢ã€‚\n*   **87. AquaVLM**: ä¸“é—¨ä¸º**æ°´ä¸‹é€šä¿¡**è®¾è®¡çš„ç§»åŠ¨ç«¯ VLMï¼Œèƒ½ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ¶ˆæ¯ï¼Œå¾ˆæœ‰è¶£çš„å‚ç±»åº”ç”¨ã€‚\n\n---\nå¸Œæœ›ä»Šå¤©çš„å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2509.18178v2",
      "title": "Foam-Agent 2.0: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM",
      "title_zh": "Foam-Agent 2.0ï¼šç”¨äº OpenFOAM è‡ªåŠ¨åŒ– CFD æ¨¡æ‹Ÿçš„ç«¯åˆ°ç«¯å¯ç»„åˆå¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Ling Yue",
        "Nithin Somasekharan",
        "Tingwen Zhang",
        "Yadi Cao",
        "Shaowu Pan"
      ],
      "abstract": "Computational Fluid Dynamics (CFD) is an essential simulation tool in engineering, yet its steep learning curve and complex manual setup create significant barriers. To address these challenges, we introduce Foam-Agent, a multi-agent framework that automates the entire end-to-end OpenFOAM workflow from a single natural language prompt. Our key innovations address critical gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation: Foam-Agent is the first system to manage the full simulation pipeline, including advanced pre-processing with a versatile Meshing Agent capable of handling external mesh files and generating new geometries via Gmsh, automatic generation of HPC submission scripts, and post-simulation visualization via ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent, the framework uses Model Context Protocol (MCP) to expose its core functions as discrete, callable tools. This allows for flexible integration and use by other agentic systems, such as Claude-code, for more exploratory workflows. 3. High-Fidelity Configuration Generation: We achieve superior accuracy through a Hierarchical Multi-Index RAG for precise context retrieval and a dependency-aware generation process that ensures configuration consistency. Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2% success rate with Claude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the expertise barrier for CFD, demonstrating how specialized multi-agent systems can democratize complex scientific computing. The code is public at https://github.com/csml-rpi/Foam-Agent.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Foam-Agent 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹OpenFOAMçš„ç«¯åˆ°ç«¯å¯ç»„åˆå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å•ä¸€è‡ªç„¶è¯­è¨€æç¤ºè‡ªåŠ¨æ‰§è¡Œå®Œæ•´çš„Computational Fluid Dynamics (CFD)æ¨¡æ‹Ÿå…¨æµç¨‹ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†ä»ä½¿ç”¨Gmshè¿›è¡Œå‡ ä½•ç”Ÿæˆå’Œç½‘æ ¼åˆ’åˆ†çš„Meshing Agentï¼Œåˆ°è‡ªåŠ¨ç”ŸæˆHPCæäº¤è„šæœ¬ï¼Œä»¥åŠé€šè¿‡ParaViewè¿›è¡Œæ¨¡æ‹Ÿåå¯è§†åŒ–çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚é€šè¿‡å¼•å…¥Model Context Protocol (MCP)ï¼ŒFoam-Agentå®ç°äº†å¯ç»„åˆçš„æœåŠ¡æ¶æ„ï¼Œä½¿å…¶æ ¸å¿ƒåŠŸèƒ½å¯ä»¥ä½œä¸ºç¦»æ•£å·¥å…·ä¾›Claude-codeç­‰å…¶ä»–æ™ºèƒ½ä½“ç³»ç»Ÿçµæ´»è°ƒç”¨ã€‚ä¸ºäº†æå‡é…ç½®æ–‡ä»¶çš„ç”Ÿæˆç²¾åº¦ï¼Œç ”ç©¶é‡‡ç”¨äº†Hierarchical Multi-Index RAGè¿›è¡Œç²¾å‡†ä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œå¹¶ç»“åˆä¾èµ–æ„ŸçŸ¥ç”Ÿæˆè¿‡ç¨‹ç¡®ä¿é…ç½®çš„ä¸€è‡´æ€§ã€‚åœ¨åŒ…å«110ä¸ªæ¨¡æ‹Ÿä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨Claude 3.5 Sonnetçš„Foam-Agentå–å¾—äº†88.2%çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºMetaOpenFOAMçš„55.5%ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆé™ä½äº†CFDçš„ä¸“ä¸šæŠ€æœ¯é—¨æ§›ï¼Œå±•ç¤ºäº†ä¸“ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ¨åŠ¨å¤æ‚ç§‘å­¦è®¡ç®—æ°‘ä¸»åŒ–æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18178v2",
      "published_date": "2025-09-17 23:44:18 UTC",
      "updated_date": "2025-09-30 23:00:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:06:30.703352+00:00"
    },
    {
      "arxiv_id": "2509.14485v1",
      "title": "Beyond the high score: Prosocial ability profiles of multi-agent populations",
      "title_zh": "è¶…è¶Šé«˜åˆ†ï¼šå¤šæ™ºèƒ½ä½“ç¾¤ä½“çš„äº²ç¤¾ä¼šèƒ½åŠ›è°±ç³»",
      "authors": [
        "Marko Tesic",
        "Yue Zhao",
        "Joel Z. Leibo",
        "Rakshit S. Trivedi",
        "Jose Hernandez-Orallo"
      ],
      "abstract": "The development and evaluation of social capabilities in AI agents require complex environments where competitive and cooperative behaviours naturally emerge. While game-theoretic properties can explain why certain teams or agent populations outperform others, more abstract behaviours, such as convention following, are harder to control in training and evaluation settings. The Melting Pot contest is a social AI evaluation suite designed to assess the cooperation capabilities of AI systems. In this paper, we apply a Bayesian approach known as Measurement Layouts to infer the capability profiles of multi-agent systems in the Melting Pot contest. We show that these capability profiles not only predict future performance within the Melting Pot suite but also reveal the underlying prosocial abilities of agents. Our analysis indicates that while higher prosocial capabilities sometimes correlate with better performance, this is not a universal trend-some lower-scoring agents exhibit stronger cooperation abilities. Furthermore, we find that top-performing contest submissions are more likely to achieve high scores in scenarios where prosocial capabilities are not required. These findings, together with reports that the contest winner used a hard-coded solution tailored to specific environments, suggest that at least one top-performing team may have optimised for conditions where cooperation was not necessary, potentially exploiting limitations in the evaluation framework. We provide recommendations for improving the annotation of cooperation demands and propose future research directions to account for biases introduced by different testing environments. Our results demonstrate that Measurement Layouts offer both strong predictive accuracy and actionable insights, contributing to a more transparent and generalisable approach to evaluating AI systems in complex social settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤æ‚ç¤¾äº¤ç¯å¢ƒä¸­è¯„ä¼°AIæ™ºèƒ½ä½“çš„ç¤¾äº¤èƒ½åŠ›ï¼Œåˆ©ç”¨Melting Potç«èµ›å¥—ä»¶è¯„ä¼°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(multi-agent systems)çš„åä½œè¡¨ç°ã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§åä¸ºMeasurement Layoutsçš„è´å¶æ–¯æ–¹æ³•ï¼Œç”¨äºæ¨æ–­æ™ºèƒ½ä½“çš„èƒ½åŠ›å‰–é¢ï¼Œè¿™äº›å‰–é¢ä¸ä»…èƒ½æœ‰æ•ˆé¢„æµ‹æ€§èƒ½ï¼Œè¿˜èƒ½æ­ç¤ºåº•å±‚çš„äº²ç¤¾ä¼š(prosocial)èƒ½åŠ›ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œæ€§èƒ½å¾—åˆ†ä¸äº²ç¤¾ä¼šèƒ½åŠ›å¹¶ä¸å®Œå…¨æ­£ç›¸å…³ï¼Œéƒ¨åˆ†ä½åˆ†æ™ºèƒ½ä½“åè€Œè¡¨ç°å‡ºæ›´å¼ºçš„åˆä½œèƒ½åŠ›ï¼Œè€Œä¸€äº›é¡¶å°–æ–¹æ¡ˆå¯èƒ½åˆ©ç”¨äº†è¯„ä¼°æ¡†æ¶çš„å±€é™æ€§åœ¨éåä½œåœºæ™¯ä¸­è·ç›Šã€‚é€šè¿‡æ­ç¤ºè¯„ä¼°ç¯å¢ƒä¸­çš„åè§ï¼Œè¯¥ç ”ç©¶ä¸ºæ”¹è¿›åˆä½œéœ€æ±‚(cooperation demands)æ ‡æ³¨æä¾›äº†é‡è¦å»ºè®®ã€‚ç ”ç©¶è¯æ˜äº†Measurement Layoutsåœ¨ç¤¾äº¤AIè¯„ä¼°ä¸­å…·æœ‰æé«˜çš„é¢„æµ‹å‡†ç¡®æ€§å’Œé€æ˜åº¦ï¼Œä¸ºæ„å»ºæ›´å…·æ³›åŒ–æ€§çš„å¤šæ™ºèƒ½ä½“è¯„ä¼°æ–¹æ³•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14485v1",
      "published_date": "2025-09-17 23:29:39 UTC",
      "updated_date": "2025-09-17 23:29:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:06:38.797216+00:00"
    },
    {
      "arxiv_id": "2509.14480v1",
      "title": "Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents",
      "title_zh": "é¢å‘äº¤äº’å¼å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“çš„è¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Weiting Tan",
        "Xinghua Qu",
        "Ming Tu",
        "Meng Ge",
        "Andy T. Liu",
        "Philipp Koehn",
        "Lu Lu"
      ],
      "abstract": "Effective interactive tool use requires agents to master Tool Integrated Reasoning (TIR): a complex process involving multi-turn planning and long-context dialogue management. To train agents for this dynamic process, particularly in multi-modal contexts, we introduce a sandbox environment for reinforcement learning (RL) that supports interleaved speech-text rollouts. Our core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses the challenge of credit assignment in long-horizon tasks by employing a Large Language Model (LLM) as a judge to provide turn-level evaluation. To enhance exploration, we integrate a mixed-task training curriculum with mathematical reasoning problems. This unified approach boosts the task pass rate on the text-based $Ï„$-bench by over 6% compared to strong RL baselines. Crucially, we demonstrate our framework's suitability for fine-tuning a multi-modal foundation model for agentic tasks. By training a base multi-modal LLM on interleaved speech-text rollouts, we equip it with tool-use abilities, paving the way for more natural, voice-driven interactive agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äº¤äº’å¼å·¥å…·ä½¿ç”¨ä¸­çš„å·¥å…·é›†æˆæ¨ç†(Tool Integrated Reasoning, TIR)æŒ‘æˆ˜ï¼Œé‡ç‚¹è§£å†³å¤šè½®è§„åˆ’ä¸é•¿ä¸Šä¸‹æ–‡å¯¹è¯ç®¡ç†éš¾é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ”¯æŒè¯­éŸ³-æ–‡æœ¬äº¤é”™rolloutsçš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)æ²™ç›’ç¯å¢ƒï¼Œæ—¨åœ¨å¤šæ¨¡æ€è¯­å¢ƒä¸‹è®­ç»ƒæ™ºèƒ½ä½“ã€‚ç ”ç©¶æ ¸å¿ƒé‡‡ç”¨äº†å›åˆçº§è£å®šå¼ºåŒ–å­¦ä¹ (Turn-level Adjudicated Reinforcement Learning, TARL)ç­–ç•¥ï¼Œé€šè¿‡å¤§è¯­è¨€æ¨¡å‹(LLM)ä½œä¸ºè£åˆ¤æä¾›å›åˆçº§è¯„ä¼°ï¼Œæœ‰æ•ˆè§£å†³äº†é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆç»“åˆäº†åŒ…å«æ•°å­¦æ¨ç†çš„æ··åˆä»»åŠ¡è®­ç»ƒè¯¾ç¨‹ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ™ºèƒ½ä½“çš„æ¢ç´¢èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æœ¬åŸºå‡†æµ‹è¯•$\\tau$-benchä¸Šçš„ä»»åŠ¡é€šè¿‡ç‡æ¯”å¼ºRLåŸºçº¿æ¨¡å‹æé«˜äº†6%ä»¥ä¸Šã€‚æœ€åï¼Œè¯¥æ¡†æ¶æˆåŠŸåº”ç”¨äºå¤šæ¨¡æ€åŸºåº§æ¨¡å‹çš„å¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿé€šè¿‡è¯­éŸ³-æ–‡æœ¬äº¤é”™äº¤äº’è·å¾—å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä¸ºå¼€å‘æ›´è‡ªç„¶çš„è¯­éŸ³é©±åŠ¨äº¤äº’å¼æ™ºèƒ½ä½“å¼€è¾Ÿäº†è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14480v1",
      "published_date": "2025-09-17 23:25:00 UTC",
      "updated_date": "2025-09-17 23:25:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:06:36.458724+00:00"
    },
    {
      "arxiv_id": "2509.14476v2",
      "title": "AToken: A Unified Tokenizer for Vision",
      "title_zh": "ATokenï¼šé¢å‘è§†è§‰çš„ç»Ÿä¸€è¯å…ƒåŒ–æ¨¡å‹",
      "authors": [
        "Jiasen Lu",
        "Liangchen Song",
        "Mingze Xu",
        "Byeongjoo Ahn",
        "Yanjun Wang",
        "Chen Chen",
        "Afshin Dehghan",
        "Yinfei Yang"
      ],
      "abstract": "We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 40.2% MSRVTT retrieval for videos, and 28.28 PSNR with 90.9% classification accuracy for 3D.. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ATokenï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨å›¾åƒã€è§†é¢‘å’Œ 3D èµ„äº§ä¸ŠåŒæ—¶å®ç°é«˜ä¿çœŸé‡å»ºå’Œè¯­ä¹‰ç†è§£çš„ç»Ÿä¸€è§†è§‰åˆ†è¯å™¨ (tokenizer)ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†çº¯ Transformer æ¶æ„å’Œ 4D æ—‹è½¬ä½ç½®åµŒå…¥ (4D rotary position embeddings)ï¼Œå°†å¤šæ ·åŒ–çš„è§†è§‰è¾“å…¥ç¼–ç åˆ°å…±äº«çš„ 4D æ½œç©ºé—´ (4D latent space) ä¸­ï¼Œå®ç°äº†ä»»åŠ¡ä¸æ¨¡æ€çš„æœ‰æ•ˆç»Ÿä¸€ã€‚ä¸ºäº†ç¡®ä¿è®­ç»ƒç¨³å®šæ€§ï¼Œç ”ç©¶é‡‡ç”¨äº†ç»“åˆæ„ŸçŸ¥æŸå¤±å’Œ Gram çŸ©é˜µæŸå¤±çš„éå¯¹æŠ—æ€§è®­ç»ƒç›®æ ‡ (adversarial-free training objective)ï¼Œå¹¶é€šè¿‡æ¸è¿›å¼è®­ç»ƒè¯¾ç¨‹æ”¯æŒè¿ç»­å’Œç¦»æ•£æ½œæ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAToken åœ¨å›¾åƒé‡å»º (0.21 rFID)ã€è§†é¢‘æ£€ç´¢å’Œ 3D åˆ†ç±»ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†é¢†åŸŸé¢†å…ˆæ°´å¹³ã€‚æ­¤å¤–ï¼ŒAToken èƒ½å¤ŸååŒæ”¯æŒè§†è§‰ç”Ÿæˆä»»åŠ¡ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (multimodal LLMs) çš„ç†è§£ä»»åŠ¡ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ä¸‹æ¸¸åº”ç”¨èƒ½åŠ›ã€‚è¯¥æˆæœä¸ºæ„å»ºåŸºäºç»Ÿä¸€è§†è§‰åˆ†è¯æŠ€æœ¯çš„ä¸‹ä¸€ä»£å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "30 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.14476v2",
      "published_date": "2025-09-17 23:11:18 UTC",
      "updated_date": "2025-09-19 06:15:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:06:39.891357+00:00"
    },
    {
      "arxiv_id": "2509.14474v2",
      "title": "From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence",
      "title_zh": "ä»æ¨¡ä»¿åˆ°çœŸå®æ™ºèƒ½ (TI)ï¼šé€šç”¨äººå·¥æ™ºèƒ½çš„æ–°èŒƒå¼",
      "authors": [
        "Meltem Subasioglu",
        "Nevzat Subasioglu"
      ],
      "abstract": "The debate around Artificial General Intelligence (AGI) remains open due to two fundamentally different goals: replicating human-like performance versus replicating human-like cognitive processes. We argue that current performance-based definitions are inadequate because they provide no clear, mechanism-focused roadmap for research, and they fail to properly define the qualitative nature of genuine intelligence. Drawing inspiration from the human brain, we propose a new paradigm that shifts the focus from external mimicry to the development of foundational cognitive architectures. We define True Intelligence (TI) as a system characterized by six core components: embodied sensory fusion, core directives, dynamic schemata creation, a highly-interconnected multi-expert architecture, an orchestration layer, and lastly, the unmeasurable quality of Interconnectedness, which we hypothesize results in consciousness and a subjective experience. We propose a practical, five-level taxonomy of AGI based on the number of the first five measurable components a system exhibits. This framework provides a clear path forward with developmental milestones that directly address the challenge of building genuinely intelligent systems. We contend that once a system achieves Level-5 AGI by implementing all five measurable components, the difference between it and TI remains as a purely philosophical debate. For practical purposes - and given theories indicate consciousness is an emergent byproduct of integrated, higher-order cognition - we conclude that a fifth-level AGI is functionally and practically equivalent to TI. This work synthesizes diverse insights from analytical psychology, schema theory, metacognition, modern brain architectures and latest works in AI to provide the first holistic, mechanism-based definition of AGI that offers a clear and actionable path for the research community.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šç”¨äººå·¥æ™ºèƒ½(AGI)çš„å®šä¹‰ï¼Œè®¤ä¸ºç›®å‰çš„æ€§èƒ½å¯¼å‘å®šä¹‰ç¼ºä¹æœºåˆ¶æ€§è·¯å¾„ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä»å¤–éƒ¨æ¨¡ä»¿è½¬å‘æ„å»ºæ ¸å¿ƒè®¤çŸ¥æ¶æ„çš„æ–°èŒƒå¼â€”â€”çœŸå®æ™ºèƒ½(True Intelligence, TI)ã€‚ä½œè€…å®šä¹‰TIç”±å…­ä¸ªæ ¸å¿ƒè¦ç´ ç»„æˆï¼šå…·ä½“åŒ–æ„Ÿå®˜èåˆ(Embodied Sensory Fusion)ã€æ ¸å¿ƒæŒ‡ä»¤(Core Directives)ã€åŠ¨æ€å›¾å¼åˆ›å»º(Dynamic Schemata Creation)ã€é«˜åº¦äº’è¿çš„å¤šä¸“å®¶æ¶æ„(Multi-expert Architecture)ã€ç¼–æ’å±‚(Orchestration Layer)ä»¥åŠäº§ç”Ÿæ„è¯†çš„äº’è¿æ€§(Interconnectedness)ã€‚åŸºäºå‰äº”ä¸ªå¯è¡¡é‡ç»„ä»¶ï¼Œç ”ç©¶æå‡ºäº†ä¸€å¥—äº”çº§AGIåˆ†ç±»æ³•ï¼Œä¸ºç§‘ç ”ç¤¾åŒºæä¾›äº†æ˜ç¡®çš„å‘å±•é‡Œç¨‹ç¢‘ã€‚è¯¥è®ºæ–‡ç»“åˆåˆ†æå¿ƒç†å­¦ã€å›¾å¼ç†è®ºå’Œç°ä»£å¤§è„‘æ¶æ„ï¼Œè®ºè¯äº†è¾¾åˆ°ç¬¬äº”çº§çš„AGIåœ¨åŠŸèƒ½ä¸Šç­‰åŒäºTIï¼Œä¸ºå®ç°çœŸæ­£çš„æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é¦–ä¸ªåŸºäºæœºåˆ¶çš„æ•´ä½“æ€§å®šä¹‰å’Œè¡ŒåŠ¨è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "27 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2509.14474v2",
      "published_date": "2025-09-17 23:08:36 UTC",
      "updated_date": "2025-09-20 15:06:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:06:41.184917+00:00"
    },
    {
      "arxiv_id": "2509.16256v1",
      "title": "HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language",
      "title_zh": "HausaMovieReviewï¼šé¢å‘ä½èµ„æºéæ´²è¯­è¨€æƒ…æ„Ÿåˆ†æçš„åŸºå‡†æ•°æ®é›†",
      "authors": [
        "Asiya Ibrahim Zanga",
        "Salisu Mamman Abdulrahman",
        "Abubakar Ado",
        "Abdulkadir Abubakar Bichi",
        "Lukman Aliyu Jibril",
        "Abdulmajid Babangida Umar",
        "Alhassan Adamu",
        "Shamsuddeen Hassan Muhammad",
        "Bashir Salisu Abubakar"
      ],
      "abstract": "The development of Natural Language Processing (NLP) tools for low-resource languages is critically hindered by the scarcity of annotated datasets. This paper addresses this fundamental challenge by introducing HausaMovieReview, a novel benchmark dataset comprising 5,000 YouTube comments in Hausa and code-switched English. The dataset was meticulously annotated by three independent annotators, demonstrating a robust agreement with a Fleiss' Kappa score of 0.85 between annotators. We used this dataset to conduct a comparative analysis of classical models (Logistic Regression, Decision Tree, K-Nearest Neighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results reveal a key finding: the Decision Tree classifier, with an accuracy and F1-score 89.72% and 89.60% respectively, significantly outperformed the deep learning models. Our findings also provide a robust baseline, demonstrating that effective feature engineering can enable classical models to achieve state-of-the-art performance in low-resource contexts, thereby laying a solid foundation for future research.\n  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½èµ„æºè¯­è¨€(low-resource languages)åœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸé¢ä¸´çš„æ ‡æ³¨æ•°æ®é›†åŒ®ä¹é—®é¢˜ï¼Œæ¨å‡ºäº†HausaMovieReviewè¿™ä¸€æ–°å‹åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«5,000æ¡æºè‡ªYouTubeçš„è±ªè¨è¯­(Hausa)ä¸è‹±è¯­æ··åˆçš„è¯­ç è½¬æ¢(code-switched)è¯„è®ºï¼Œå…¶æ ‡æ³¨è€…é—´ä¸€è‡´æ€§(Fleiss' Kappa)é«˜è¾¾0.85ã€‚é€šè¿‡å¯¹æ¯”é€»è¾‘å›å½’(Logistic Regression)ã€å†³ç­–æ ‘(Decision Tree)ç­‰ä¼ ç»Ÿæ¨¡å‹ä¸å¾®è°ƒåçš„BERTã€RoBERTaç­‰Transformeræ¨¡å‹ï¼Œç ”ç©¶å‘ç°å†³ç­–æ ‘åˆ†ç±»å™¨ä»¥89.72%çš„å‡†ç¡®ç‡(accuracy)å’Œ89.60%çš„F1åˆ†æ•°(F1-score)æ˜¾è‘—ä¼˜äºæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¿™ä¸€ç»“æœè¯æ˜äº†åœ¨ä½èµ„æºè¯­å¢ƒä¸‹ï¼Œé€šè¿‡æœ‰æ•ˆçš„ç‰¹å¾å·¥ç¨‹(feature engineering)ï¼Œç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹äº¦èƒ½å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºç›¸å…³é¢†åŸŸæä¾›äº†ç¨³å¥çš„åŸºå‡†ï¼Œä¹Ÿä¸ºéæ´²ä½èµ„æºè¯­è¨€çš„æƒ…æ„Ÿåˆ†æ(Sentiment Analysis)ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Masters Thesis, a Dataset Paper",
      "pdf_url": "https://arxiv.org/pdf/2509.16256v1",
      "published_date": "2025-09-17 22:57:21 UTC",
      "updated_date": "2025-09-17 22:57:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:06:47.858191+00:00"
    },
    {
      "arxiv_id": "2509.15248v3",
      "title": "Synthetic bootstrapped pretraining",
      "title_zh": "åˆæˆè‡ªä¸¾é¢„è®­ç»ƒ",
      "authors": [
        "Zitong Yang",
        "Aonan Zhang",
        "Hong Liu",
        "Tatsunori Hashimoto",
        "Emmanuel CandÃ¨s",
        "Chong Wang",
        "Ruoming Pang"
      ],
      "abstract": "We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training. While the standard pretraining teaches LMs to learn causal correlations among tokens within a single document, it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance. We validate SBP by designing a compute-matched pretraining setup and pretrain a 3B-parameter and a 6B-parameter model on up to 1T tokens from scratch. We find SBP consistently improves upon a strong repetition baseline and delivers up to 60% of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases -- SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it. Besides strong empirical performance, SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Synthetic Bootstrapped Pretraining (SBP)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨é€šè¿‡å»ºæ¨¡æ–‡æ¡£é—´å…³ç³»æ¥å¢å¼ºè¯­è¨€æ¨¡å‹(LM)é¢„è®­ç»ƒçš„æ–°å‹æµç¨‹ã€‚ä¸ä¼ ç»Ÿçš„ä»…å…³æ³¨å•æ–‡æ¡£å†…tokené—´å› æœå…³è”çš„æ–¹æ³•ä¸åŒï¼ŒSBPé¦–å…ˆå­¦ä¹ é¢„è®­ç»ƒæ•°æ®é›†ä¸­æ–‡æ¡£é—´çš„å…³è”ï¼Œå¹¶åˆ©ç”¨è¿™äº›å…³è”åˆæˆä¸€ä¸ªå…¨æ–°çš„å¤§è§„æ¨¡è¯­æ–™åº“è¿›è¡Œè”åˆè®­ç»ƒã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä»é›¶å¼€å§‹åœ¨1T tokenä¸Šé¢„è®­ç»ƒ3Bå’Œ6Bå‚æ•°æ¨¡å‹ï¼Œåœ¨ç®—åŠ›åŒ¹é…çš„å®éªŒè®¾å®šä¸‹éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSBPåœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºå¼ºé‡å¤åŸºçº¿ï¼Œå¹¶èƒ½å®ç°ä½¿ç”¨20å€å”¯ä¸€æ•°æ®é‡çš„Oracleä¸Šé™çº¦60%çš„æ€§èƒ½æå‡ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼ŒSBPç”Ÿæˆçš„åˆæˆæ–‡æ¡£å¹¶éç®€å•çš„æ”¹å†™(Paraphrase)ï¼Œè€Œæ˜¯å…ˆä»ç§å­ææ–™ä¸­æå–æ ¸å¿ƒæ¦‚å¿µï¼Œå†æ®æ­¤åˆ›ä½œå…¨æ–°çš„å™è¿°ã€‚æ­¤å¤–ï¼ŒSBPå…·æœ‰è‡ªç„¶çš„Bayesianè§£é‡Šï¼Œå³åˆæˆå™¨èƒ½å¤Ÿéšå¼åœ°å­¦ä¹ å¹¶æå–ç›¸å…³æ–‡æ¡£ä¹‹é—´å…±äº«çš„æ½œæ¦‚å¿µ(Latent concepts)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15248v3",
      "published_date": "2025-09-17 22:28:27 UTC",
      "updated_date": "2025-12-13 05:58:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:06:50.471323+00:00"
    },
    {
      "arxiv_id": "2509.14456v2",
      "title": "Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs",
      "title_zh": "Correct-Detectï¼šå¤§è¯­è¨€æ¨¡å‹æŒ‡ä»£æ¶ˆè§£è§†è§’ä¸‹çš„æ€§èƒ½ä¸æ­§ä¹‰æ€§å¹³è¡¡",
      "authors": [
        "Amber Shore",
        "Russell Scheinberg",
        "Ameeta Agrawal",
        "So Young Lee"
      ],
      "abstract": "Large Language Models (LLMs) are intended to reflect human linguistic competencies. But humans have access to a broad and embodied context, which is key in detecting and resolving linguistic ambiguities, even in isolated text spans. A foundational case of semantic ambiguity is found in the task of coreference resolution: how is a pronoun related to an earlier person mention? This capability is implicit in nearly every downstream task, and the presence of ambiguity at this level can alter performance significantly. We show that LLMs can achieve good performance with minimal prompting in both coreference disambiguation and the detection of ambiguity in coreference, however, they cannot do both at the same time. We present the CORRECT-DETECT trade-off: though models have both capabilities and deploy them implicitly, successful performance balancing these two abilities remains elusive.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æŒ‡ä»£æ¶ˆè§£(Coreference Resolution)ä»»åŠ¡ä¸­å¹³è¡¡æ€§èƒ½ä¸æ­§ä¹‰æ£€æµ‹çš„èƒ½åŠ›ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒæŒ‡ä»£æ¶ˆè§£ä½œä¸ºè¯­ä¹‰æ­§ä¹‰çš„æ ¸å¿ƒæ¡ˆä¾‹ï¼Œå¯¹å‡ ä¹æ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡éƒ½å…·æœ‰éšå«å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨å°‘é‡æç¤ºçš„æƒ…å†µä¸‹å¯ä»¥åˆ†åˆ«åœ¨æŒ‡ä»£æ¶ˆè§£(Coreference Disambiguation)å’Œæ­§ä¹‰æ£€æµ‹(Detection of Ambiguity)æ–¹é¢å–å¾—è‰¯å¥½è¡¨ç°ï¼Œä½†æ— æ³•åŒæ—¶å®Œæˆè¿™ä¸¤é¡¹ä»»åŠ¡ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æå‡ºäº†â€œCORRECT-DETECTâ€æƒè¡¡å…³ç³»ï¼Œè¯æ˜äº†æ¨¡å‹è™½ç„¶å…·å¤‡å¹¶èƒ½éšå«éƒ¨ç½²è¿™ä¸¤ç§èƒ½åŠ›ï¼Œä½†åœ¨åŒæ—¶å¹³è¡¡ä¸¤è€…æ€§èƒ½æ–¹é¢ä»ç„¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2025 (main)",
      "pdf_url": "https://arxiv.org/pdf/2509.14456v2",
      "published_date": "2025-09-17 22:12:30 UTC",
      "updated_date": "2025-10-21 17:46:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:06:50.672935+00:00"
    },
    {
      "arxiv_id": "2509.14448v1",
      "title": "VCBench: Benchmarking LLMs in Venture Capital",
      "title_zh": "VCBenchï¼šé£é™©æŠ•èµ„é¢†åŸŸçš„å¤§è¯­è¨€æ¨¡å‹è¯„æµ‹åŸºå‡†",
      "authors": [
        "Rick Chen",
        "Joseph Ternasky",
        "Afriyie Samuel Kwesi",
        "Ben Griffin",
        "Aaron Ontoyin Yin",
        "Zakari Salifu",
        "Kelvin Amoaba",
        "Xianling Mu",
        "Fuat Alican",
        "Yigit Ihlamur"
      ],
      "abstract": "Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets accelerate progress toward artificial general intelligence (AGI). We introduce VCBench, the first benchmark for predicting founder success in venture capital (VC), a domain where signals are sparse, outcomes are uncertain, and even top investors perform modestly. At inception, the market index achieves a precision of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1 firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles, standardized to preserve predictive features while resisting identity leakage, with adversarial tests showing more than 90% reduction in re-identification risk. We evaluate nine state-of-the-art large language models (LLMs). DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the highest F0.5, and most models surpass human benchmarks. Designed as a public and evolving resource available at vcbench.com, VCBench establishes a community-driven standard for reproducible and privacy-preserving evaluation of AGI in early-stage venture forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†VCBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹é£é™©æŠ•èµ„(Venture Capital)é¢†åŸŸåˆ›å§‹äººæˆåŠŸé¢„æµ‹çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³è¯¥é¢†åŸŸä¿¡å·ç¨€ç–ä¸”ç»“æœä¸ç¡®å®šæ€§é«˜çš„æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†åŒ…å«9,000ä»½åŒ¿ååŒ–å¤„ç†çš„åˆ›å§‹äººä¸ªäººèµ„æ–™ï¼Œåœ¨é€šè¿‡æ ‡å‡†åŒ–æ‰‹æ®µä¿ç•™é¢„æµ‹ç‰¹å¾çš„åŒæ—¶ï¼Œåˆ©ç”¨å¯¹æŠ—æ€§æµ‹è¯•è¯æ˜å…¶èº«ä»½é‡è¯†åˆ«é£é™©é™ä½äº†90%ä»¥ä¸Šã€‚ç ”ç©¶è¯„ä¼°äº†ä¹ç§å‰æ²¿çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼Œå…¶ä¸­DeepSeek-V3çš„é¢„æµ‹ç²¾åº¦è¾¾åˆ°äº†å¸‚åœºåŸºå‡†æŒ‡æ•°çš„å…­å€ä»¥ä¸Šï¼ŒGPT-4oåˆ™åœ¨F0.5æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°å—æµ‹æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®æ€§ä¸Šå·²è¶…è¶Šäº†é¡¶çº§é£æŠ•æœºæ„å’Œäººç±»ä¸“å®¶çš„è¡¨ç°æ°´å¹³ã€‚VCBenchä½œä¸ºä¸€ä¸ªå¼€æºä¸”æŒç»­æ›´æ–°çš„èµ„æºï¼Œä¸ºåœ¨æ—©æœŸåˆ›ä¸šé¢„æµ‹ä»»åŠ¡ä¸­å®ç°å¯é‡ç°ä¸”å…¼é¡¾éšç§ä¿æŠ¤çš„äººå·¥é€šç”¨æ™ºèƒ½(AGI)è¯„ä¼°å»ºç«‹äº†ç¤¾åŒºé©±åŠ¨çš„æ–°æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14448v1",
      "published_date": "2025-09-17 21:56:48 UTC",
      "updated_date": "2025-09-17 21:56:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:07:01.765849+00:00"
    },
    {
      "arxiv_id": "2509.14438v1",
      "title": "Simulating a Bias Mitigation Scenario in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åè§ç¼“è§£åœºæ™¯çš„æ¨¡æ‹Ÿ",
      "authors": [
        "Kiana Kiashemshaki",
        "Mohammad Jalili Torkamani",
        "Negin Mahmoudi",
        "Meysam Shirdel Bilehsavar"
      ],
      "abstract": "Large Language Models (LLMs) have fundamentally transformed the field of natural language processing; however, their vulnerability to biases presents a notable obstacle that threatens both fairness and trust. This review offers an extensive analysis of the bias landscape in LLMs, tracing its roots and expressions across various NLP tasks. Biases are classified into implicit and explicit types, with particular attention given to their emergence from data sources, architectural designs, and contextual deployments. This study advances beyond theoretical analysis by implementing a simulation framework designed to evaluate bias mitigation strategies in practice. The framework integrates multiple approaches including data curation, debiasing during model training, and post-hoc output calibration and assesses their impact in controlled experimental settings. In summary, this work not only synthesizes existing knowledge on bias in LLMs but also contributes original empirical validation through simulation of mitigation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­å­˜åœ¨çš„åè§é—®é¢˜ï¼Œåˆ†æäº†å…¶å¯¹å…¬å¹³æ€§å’Œä¿¡ä»»åº¦æ„æˆçš„é‡å¤§æŒ‘æˆ˜ã€‚æ–‡ç« å¯¹LLMsçš„åè§å›¾æ™¯è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œå°†å…¶åˆ†ç±»ä¸ºéšæ€§(Implicit)å’Œæ˜¾æ€§(Explicit)åè§ï¼Œå¹¶è¿½æº¯äº†å…¶åœ¨æ•°æ®æºã€æ¶æ„è®¾è®¡åŠä¸Šä¸‹æ–‡éƒ¨ç½²ä¸­çš„æ ¹æºã€‚é™¤äº†ç†è®ºåˆ†æï¼Œè¯¥ç ”ç©¶è¿˜å®ç°äº†ä¸€ä¸ªæ¨¡æ‹Ÿæ¡†æ¶(Simulation Framework)ï¼Œç”¨äºåœ¨å—æ§å®éªŒç¯å¢ƒä¸­è¯„ä¼°å®é™…çš„åè§ç¼“è§£ç­–ç•¥(Bias Mitigation Strategies)ã€‚è¯¥æ¡†æ¶æ•´åˆäº†æ•°æ®æ¸…æ´—(Data Curation)ã€æ¨¡å‹è®­ç»ƒæœŸé—´çš„å»å(Debiasing)ä»¥åŠäº‹åè¾“å‡ºæ ¡å‡†(Post-hoc Output Calibration)ç­‰å¤šç§æ–¹æ³•ã€‚ç ”ç©¶é€šè¿‡æ¨¡æ‹Ÿå®éªŒå¯¹ç¼“è§£ç­–ç•¥è¿›è¡Œäº†å®è¯éªŒè¯ï¼Œåˆ†æäº†å„é˜¶æ®µå¹²é¢„å¯¹å‡å°‘åè§çš„æ•ˆæœã€‚è¿™é¡¹å·¥ä½œç»¼åˆäº†ç°æœ‰ç ”ç©¶å¹¶æä¾›äº†åŸåˆ›çš„å®è¯æ•°æ®ï¼Œä¸ºæ„å»ºæ›´åŠ å…¬æ­£å¯ä¿¡çš„è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿæä¾›äº†å‚è€ƒä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "preprint, 16 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.14438v1",
      "published_date": "2025-09-17 21:22:33 UTC",
      "updated_date": "2025-09-17 21:22:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:07:01.573825+00:00"
    },
    {
      "arxiv_id": "2509.14436v1",
      "title": "When Content is Goliath and Algorithm is David: The Style and Semantic Effects of Generative Search Engine",
      "title_zh": "å†…å®¹ä¸ºæ­Œåˆ©äºšï¼Œç®—æ³•ä¸ºå¤§å«ï¼šç”Ÿæˆå¼æœç´¢å¼•æ“çš„é£æ ¼ä¸è¯­ä¹‰æ•ˆåº”",
      "authors": [
        "Lijia Ma",
        "Juan Qin",
        "Xingchen Xu",
        "Yong Tan"
      ],
      "abstract": "Generative search engines (GEs) leverage large language models (LLMs) to deliver AI-generated summaries with website citations, establishing novel traffic acquisition channels while fundamentally altering the search engine optimization landscape. To investigate the distinctive characteristics of GEs, we collect data through interactions with Google's generative and conventional search platforms, compiling a dataset of approximately ten thousand websites across both channels. Our empirical analysis reveals that GEs exhibit preferences for citing content characterized by significantly higher predictability for underlying LLMs and greater semantic similarity among selected sources. Through controlled experiments utilizing retrieval augmented generation (RAG) APIs, we demonstrate that these citation preferences emerge from intrinsic LLM tendencies to favor content aligned with their generative expression patterns. Motivated by applications of LLMs to optimize website content, we conduct additional experimentation to explore how LLM-based content polishing by website proprietors alters AI summaries, finding that such polishing paradoxically enhances information diversity within AI summaries. Finally, to assess the user-end impact of LLM-induced information increases, we design a generative search engine and recruit Prolific participants to conduct a randomized controlled experiment involving an information-seeking and writing task. We find that higher-educated users exhibit minimal changes in their final outputs' information diversity but demonstrate significantly reduced task completion time when original sites undergo polishing. Conversely, lower-educated users primarily benefit through enhanced information density in their task outputs while maintaining similar completion times across experimental groups.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼æœç´¢å¼•æ“(Generative Search Engines)çš„ç‹¬ç‰¹ç‰¹å¾åŠå…¶å¯¹æœç´¢å¼•æ“ä¼˜åŒ–(SEO)æ™¯è§‚çš„å½±å“ã€‚é€šè¿‡å¯¹æ¯”åˆ†æGoogleçš„ç”Ÿæˆå¼ä¸ä¼ ç»Ÿæœç´¢å¹³å°ï¼Œç ”ç©¶å‘ç°ç”Ÿæˆå¼æœç´¢å¼•æ“å€¾å‘äºå¼•ç”¨å¯¹åº•å±‚å¤§è¯­è¨€æ¨¡å‹(LLMs)å…·æœ‰æ›´é«˜å¯é¢„æµ‹æ€§ä»¥åŠæ¥æºé—´è¯­ä¹‰ç›¸ä¼¼åº¦æ›´é«˜çš„å†…å®¹ã€‚åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)è¿›è¡Œçš„å—æ§å®éªŒè¯å®ï¼Œè¿™ç§å¼•ç”¨åå¥½æºäºå¤§è¯­è¨€æ¨¡å‹å€¾å‘äºé€‰æ‹©ä¸å…¶ç”Ÿæˆæ¨¡å¼ä¸€è‡´å†…å®¹çš„å›ºæœ‰ç‰¹è´¨ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œç½‘ç«™æ‰€æœ‰è€…åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¯¹å†…å®¹è¿›è¡Œæ¶¦è‰²ï¼Œåè€Œèƒ½å¢å¼ºAIæ‘˜è¦ä¸­çš„ä¿¡æ¯å¤šæ ·æ€§ã€‚é’ˆå¯¹ç”¨æˆ·çš„éšæœºå¯¹ç…§å®éªŒè¡¨æ˜ï¼Œå—æ•™è‚²ç¨‹åº¦è¾ƒé«˜çš„ç”¨æˆ·åœ¨å¤„ç†æ¶¦è‰²å†…å®¹æ—¶èƒ½æ˜¾è‘—ç¼©çŸ­ä»»åŠ¡å®Œæˆæ—¶é—´ï¼Œè€Œå—æ•™è‚²ç¨‹åº¦è¾ƒä½çš„ç”¨æˆ·åˆ™ä¸»è¦ä»äº§å‡ºç»“æœçš„ä¿¡æ¯å¯†åº¦æå‡ä¸­è·ç›Šã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "59 pages, 6 figures, 20 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.14436v1",
      "published_date": "2025-09-17 21:19:13 UTC",
      "updated_date": "2025-09-17 21:19:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:07:16.202985+00:00"
    },
    {
      "arxiv_id": "2509.14404v1",
      "title": "A Taxonomy of Prompt Defects in LLM Systems",
      "title_zh": "LLM ç³»ç»Ÿæç¤ºè¯ç¼ºé™·åˆ†ç±»ä½“ç³»",
      "authors": [
        "Haoye Tian",
        "Chong Wang",
        "BoYang Yang",
        "Lyuye Zhang",
        "Yang Liu"
      ],
      "abstract": "Large Language Models (LLMs) have become key components of modern software, with prompts acting as their de-facto programming interface. However, prompt design remains largely empirical and small mistakes can cascade into unreliable, insecure, or inefficient behavior. This paper presents the first systematic survey and taxonomy of prompt defects, recurring ways that prompts fail to elicit their intended behavior from LLMs. We organize defects along six dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6) Maintainability and Engineering. Each dimension is refined into fine-grained subtypes, illustrated with concrete examples and root cause analysis. Grounded in software engineering principles, we show how these defects surface in real development workflows and examine their downstream effects. For every subtype, we distill mitigation strategies that span emerging prompt engineering patterns, automated guardrails, testing harnesses, and evaluation frameworks. We then summarize these strategies in a master taxonomy that links defect, impact, and remedy. We conclude with open research challenges and a call for rigorous engineering-oriented methodologies to ensure that LLM-driven systems are dependable by design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ç³»ç»Ÿçš„æç¤ºè¯ç¼ºé™·(Prompt Defects)ç³»ç»Ÿæ€§è°ƒæŸ¥å’Œåˆ†ç±»å­¦ï¼Œæ—¨åœ¨è§£å†³æç¤ºè¯è®¾è®¡ä¸­å› ç»éªŒä¸»ä¹‰å¯¼è‡´çš„ä¸å¯é ã€ä¸å®‰å…¨åŠä½æ•ˆè¡Œä¸ºã€‚ç ”ç©¶ä»è§„æ ¼ä¸æ„å›¾(Specification and Intent)ã€è¾“å…¥ä¸å†…å®¹(Input and Content)ã€ç»“æ„ä¸æ ¼å¼(Structure and Formatting)ã€ä¸Šä¸‹æ–‡ä¸è®°å¿†(Context and Memory)ã€æ€§èƒ½ä¸æ•ˆç‡(Performance and Efficiency)ä»¥åŠå¯ç»´æŠ¤æ€§ä¸å·¥ç¨‹åŒ–(Maintainability and Engineering)å…­ä¸ªç»´åº¦å¯¹ç¼ºé™·è¿›è¡Œäº†è¯¦ç»†å½’ç±»ã€‚åŸºäºè½¯ä»¶å·¥ç¨‹(Software Engineering)åŸåˆ™ï¼Œè¯¥ç ”ç©¶æ·±å…¥åˆ†æäº†è¿™äº›ç¼ºé™·åœ¨çœŸå®å¼€å‘å·¥ä½œæµä¸­çš„è¡¨ç°åŠå…¶ä¸‹æ¸¸å½±å“ï¼Œå¹¶é’ˆå¯¹æ¯ä¸ªå­ç±»å‹æç‚¼äº†åŒ…å«æç¤ºå·¥ç¨‹æ¨¡å¼(Prompt Engineering Patterns)å’Œè‡ªåŠ¨é˜²æŠ¤æ (Automated Guardrails)åœ¨å†…çš„ç¼“è§£ç­–ç•¥ã€‚è®ºæ–‡è¿›ä¸€æ­¥æ„å»ºäº†ä¸€ä¸ªå…³è”ç¼ºé™·ã€å½±å“ä¸ä¿®å¤æ–¹æ¡ˆçš„ä¸»åˆ†ç±»å›¾è°±ï¼Œä¸ºLLMé©±åŠ¨ç³»ç»Ÿæä¾›äº†ç³»ç»Ÿæ€§çš„å·¥ç¨‹å¯¼å‘æ–¹æ³•è®ºã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡è¯†åˆ«å’Œä¿®å¤æç¤ºè¯è®¾è®¡ä¸­çš„é‡å¤æ€§é”™è¯¯ï¼Œä¸ºæ„å»ºè®¾è®¡å¯é ã€é«˜æ€§èƒ½çš„è‡ªä¸»æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14404v1",
      "published_date": "2025-09-17 20:11:22 UTC",
      "updated_date": "2025-09-17 20:11:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:07:23.694946+00:00"
    },
    {
      "arxiv_id": "2509.14391v1",
      "title": "Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs",
      "title_zh": "Q-ROARï¼šé‡åŒ–é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ä¸­ RoPE ä½ç½®æ’å€¼çš„ç¦»ç¾¤å€¼æ„ŸçŸ¥é‡ç¼©æ”¾",
      "authors": [
        "Ye Qiao",
        "Sitao Huang"
      ],
      "abstract": "Extending LLM context windows is crucial for long range tasks. RoPE-based position interpolation (PI) methods like linear and frequency-aware scaling extend input lengths without retraining, while post-training quantization (PTQ) enables practical deployment. We show that combining PI with PTQ degrades accuracy due to coupled effects long context aliasing, dynamic range dilation, axis grid anisotropy, and outlier shifting that induce position-dependent logit noise. We provide the first systematic analysis of PI plus PTQ and introduce two diagnostics: Interpolation Pressure (per-band phase scaling sensitivity) and Tail Inflation Ratios (outlier shift from short to long contexts). To address this, we propose Q-ROAR, a RoPE-aware, weight-only stabilization that groups RoPE dimensions into a few frequency bands and performs a small search over per-band scales for W_Q,W_K, with an optional symmetric variant to preserve logit scale. The diagnostics guided search uses a tiny long-context dev set and requires no fine-tuning, kernel, or architecture changes. Empirically, Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces GovReport perplexity by more than 10%, while preserving short-context performance and compatibility with existing inference stacks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨é‡åŒ–çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦æ—¶å‡ºç°çš„ç²¾åº¦ä¸‹é™é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ç ”ç©¶å‘ç°ï¼Œå°†RoPEä½ç½®æ’å€¼(Position Interpolation)ä¸è®­ç»ƒåé‡åŒ–(PTQ)ç»“åˆä½¿ç”¨æ—¶ï¼Œä¼šå› é•¿ä¸Šä¸‹æ–‡æ··å ã€åŠ¨æ€èŒƒå›´æ‰©å¼ åŠç¦»ç¾¤å€¼åç§»ç­‰è€¦åˆæ•ˆåº”äº§ç”Ÿä½ç½®ç›¸å…³çš„é€»è¾‘å€¼å™ªå£°ã€‚ä½œè€…å¯¹æ­¤è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§åˆ†æï¼Œå¹¶å¼•å…¥äº†Interpolation Pressureå’ŒTail Inflation Ratiosä¸¤ä¸ªè¯Šæ–­æŒ‡æ ‡æ¥é‡åŒ–æ­¤ç±»å½±å“ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æå‡ºäº†Q-ROARï¼Œä¸€ç§æ„ŸçŸ¥RoPEä¸”ä»…é’ˆå¯¹æƒé‡çš„ç¨³å®šåŒ–æ–¹æ¡ˆï¼Œé€šè¿‡å°†RoPEç»´åº¦åˆ’åˆ†ä¸ºé¢‘å¸¦å¹¶ä¼˜åŒ–$W_Q$ä¸$W_K$çš„æ¯é¢‘å¸¦ç¼©æ”¾æ¯”ä¾‹æ¥æå‡é‡åŒ–ç²¾åº¦ã€‚è¯¥æ–¹æ³•æ— éœ€å¾®è°ƒã€å†…æ ¸ä¿®æ”¹æˆ–æ¶æ„å˜åŠ¨ï¼Œä»…éœ€æå°çš„é•¿ä¸Šä¸‹æ–‡å¼€å‘é›†å³å¯å®Œæˆå‚æ•°æœç´¢ã€‚å®éªŒè¯æ˜ï¼ŒQ-ROARåœ¨æ ‡å‡†ä»»åŠ¡ä¸­æ¢å¤äº†é«˜è¾¾0.7%çš„å‡†ç¡®ç‡ï¼Œå¹¶å°†GovReportçš„å›°æƒ‘åº¦é™ä½äº†10%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„çŸ­ä¸Šä¸‹æ–‡æ€§èƒ½å’Œå¯¹ç°æœ‰æ¨ç†æ ˆçš„å…¼å®¹æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14391v1",
      "published_date": "2025-09-17 19:50:16 UTC",
      "updated_date": "2025-09-17 19:50:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:07:27.663352+00:00"
    },
    {
      "arxiv_id": "2509.14388v1",
      "title": "eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations",
      "title_zh": "eIQ Neutronï¼šé€šè¿‡é›†æˆ NPU ä¸ç¼–è¯‘å™¨åˆ›æ–°é‡æ–°å®šä¹‰è¾¹ç¼˜ AI æ¨ç†",
      "authors": [
        "Lennart Bamberg",
        "Filippo Minnella",
        "Roberto Bosio",
        "Fabrizio Ottati",
        "Yuebin Wang",
        "Jongmin Lee",
        "Luciano Lavagno",
        "Adam Fuks"
      ],
      "abstract": "Neural Processing Units (NPUs) are key to enabling efficient AI inference in resource-constrained edge environments. While peak tera operations per second (TOPS) is often used to gauge performance, it poorly reflects real-world performance and typically rather correlates with higher silicon cost. To address this, architects must focus on maximizing compute utilization, without sacrificing flexibility. This paper presents the eIQ Neutron efficient-NPU, integrated into a commercial flagship MPU, alongside co-designed compiler algorithms. The architecture employs a flexible, data-driven design, while the compiler uses a constrained programming approach to optimize compute and data movement based on workload characteristics. Compared to the leading embedded NPU and compiler stack, our solution achieves an average speedup of 1.8x (4x peak) at equal TOPS and memory resources across standard AI-benchmarks. Even against NPUs with double the compute and memory resources, Neutron delivers up to 3.3x higher performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜ AI æ¨ç†ä¸­å³°å€¼æ¯ç§’ä¸‡äº¿æ¬¡è¿ç®—(TOPS)æŒ‡æ ‡æ— æ³•çœŸå®åæ˜ å®é™…æ€§èƒ½ä¸”æˆæœ¬è¾ƒé«˜çš„é—®é¢˜ï¼Œæ¨å‡ºäº†é›†æˆåœ¨å•†ä¸šæ——èˆ°å¤šå¤„ç†å™¨(MPU)ä¸­çš„ eIQ Neutron é«˜æ•ˆ Neural Processing Unit (NPU)ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ååŒè®¾è®¡çš„ç¼–è¯‘å™¨ç®—æ³•ï¼Œé‡‡ç”¨çµæ´»çš„æ•°æ®é©±åŠ¨æ¶æ„ä¸çº¦æŸè§„åˆ’(constrained programming)æ–¹æ³•ï¼Œæ—¨åœ¨æ ¹æ®å·¥ä½œè´Ÿè½½ç‰¹æ€§æœ€å¤§åŒ–è®¡ç®—åˆ©ç”¨ç‡ã€‚åœ¨æ ‡å‡† AI åŸºå‡†æµ‹è¯•(AI-benchmarks)ä¸­ï¼ŒeIQ Neutron åœ¨ç›¸åŒ TOPS å’Œå†…å­˜èµ„æºçš„å‰æä¸‹ï¼Œè¾ƒç°æœ‰é¢†å…ˆçš„åµŒå…¥å¼ NPU æ–¹æ¡ˆå®ç°äº†å¹³å‡ 1.8 å€ã€å³°å€¼ 4 å€çš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œå³ä¾¿é¢å¯¹è®¡ç®—å’Œå†…å­˜èµ„æºä¸¤å€äºè‡ªèº«çš„ç«äº‰äº§å“ï¼Œè¯¥æ–¹æ¡ˆä¾ç„¶å±•ç°å‡ºé«˜è¾¾ 3.3 å€çš„æ€§èƒ½ä¼˜åŠ¿ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡ NPU æ¶æ„ä¸ç¼–è¯‘å™¨çš„æ·±åº¦ååŒï¼Œé‡æ–°å®šä¹‰äº†èµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜æ•ˆè¾¹ç¼˜ AI æ¨ç†ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "Submitted to IEEE Transactions on Computers",
      "pdf_url": "https://arxiv.org/pdf/2509.14388v1",
      "published_date": "2025-09-17 19:45:51 UTC",
      "updated_date": "2025-09-17 19:45:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:07:32.894680+00:00"
    },
    {
      "arxiv_id": "2509.14382v1",
      "title": "Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents",
      "title_zh": "é€šè¿‡ Web æ™ºèƒ½ä½“ç»†ç²’åº¦åˆ†ææ£€æµ‹æµæ°´çº¿æ•…éšœ",
      "authors": [
        "Daniel RÃ¶der",
        "Akhil Juneja",
        "Roland Roller",
        "Sven Schmeier"
      ],
      "abstract": "Web agents powered by large language models (LLMs) can autonomously perform complex, multistep tasks in dynamic web environments. However, current evaluations mostly focus on the overall success while overlooking intermediate errors. This limits insight into failure modes and hinders systematic improvement. This work analyzes existing benchmarks and highlights the lack of fine-grained diagnostic tools. To address this gap, we propose a modular evaluation framework that decomposes agent pipelines into interpretable stages for detailed error analysis. Using the SeeAct framework and the Mind2Web dataset as a case study, we show how this approach reveals actionable weaknesses missed by standard metrics - paving the way for more robust and generalizable web agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ Web æ™ºèƒ½ä½“åœ¨æ‰§è¡Œå¤æ‚å¤šæ­¥ä»»åŠ¡æ—¶çš„è¯„ä¼°å±€é™æ€§ï¼ŒæŒ‡å‡ºå½“å‰è¯„ä¼°ä¸»è¦å…³æ³¨æ•´ä½“æˆåŠŸç‡è€Œå¿½ç•¥äº†ä¸­é—´é˜¶æ®µçš„é”™è¯¯åŠå…¶æ•…éšœæ¨¡å¼ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ¨¡å—åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡å°†æ™ºèƒ½ä½“æµæ°´çº¿ (pipeline) åˆ†è§£ä¸ºå¯è§£é‡Šçš„é˜¶æ®µï¼Œå®ç°ç»†ç²’åº¦çš„è¯Šæ–­åˆ†æã€‚ç ”ç©¶ä»¥ SeeAct æ¡†æ¶å’Œ Mind2Web æ•°æ®é›†ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•å¦‚ä½•æ­ç¤ºæ ‡å‡†è¯„ä¼°æŒ‡æ ‡æ‰€å¿½ç•¥çš„ã€å…·æœ‰å®é™…æ”¹è¿›æ„ä¹‰çš„è–„å¼±ç¯èŠ‚ã€‚è¿™ä¸€ç»†ç²’åº¦çš„åˆ†ææ–¹æ³•ä¸ºç³»ç»Ÿæ€§åœ°ç†è§£å’Œä¼˜åŒ– Web æ™ºèƒ½ä½“æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œä¸ºæ„å»ºæ›´åŠ é²æ£’å’Œé€šç”¨åŒ–çš„è‡ªä¸»æ™ºèƒ½ä½“ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14382v1",
      "published_date": "2025-09-17 19:34:49 UTC",
      "updated_date": "2025-09-17 19:34:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:07:34.292557+00:00"
    },
    {
      "arxiv_id": "2509.16254v1",
      "title": "Imaging Modalities-Based Classification for Lung Cancer Detection",
      "title_zh": "åŸºäºå½±åƒæ¨¡æ€çš„è‚ºç™Œæ£€æµ‹åˆ†ç±»",
      "authors": [
        "Sajim Ahmed",
        "Muhammad Zain Chaudhary",
        "Muhammad Zohaib Chaudhary",
        "Mahmoud Abbass",
        "Ahmed Sherif",
        "Mohammad Mahbubur Rahman Khan Mamun"
      ],
      "abstract": "Lung cancer continues to be the predominant cause of cancer-related mortality globally. This review analyzes various approaches, including advanced image processing methods, focusing on their efficacy in interpreting CT scans, chest radiographs, and biological markers. Notably, we identify critical gaps in the previous surveys, including the need for robust models that can generalize across diverse populations and imaging modalities. This comprehensive synthesis aims to serve as a foundational resource for researchers and clinicians, guiding future efforts toward more accurate and efficient lung cancer detection. Key findings reveal that 3D CNN architectures integrated with CT scans achieve the most superior performances, yet challenges such as high false positives, dataset variability, and computational complexity persist across modalities.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåˆ†æäº†åŸºäºæˆåƒæ¨¡æ€ï¼ˆImaging Modalitiesï¼‰æ£€æµ‹è‚ºç™Œçš„å¤šç§æ–¹æ³•ï¼Œé‡ç‚¹æ¢è®¨äº†å…ˆè¿›å›¾åƒå¤„ç†æŠ€æœ¯åœ¨è§£é‡Š CT scansã€èƒ¸éƒ¨ X å…‰ç‰‡ï¼ˆchest radiographsï¼‰åŠç”Ÿç‰©æ ‡å¿—ç‰©ï¼ˆbiological markersï¼‰æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ–‡ç« è¯†åˆ«å‡ºå…ˆå‰ç»¼è¿°ä¸­çš„å…³é”®ç©ºç™½ï¼Œå¼ºè°ƒäº†å¼€å‘å¯è·¨ä¸åŒäººç¾¤å’Œæˆåƒæ¨¡æ€æ³›åŒ–çš„é²æ£’æ¨¡å‹çš„éœ€æ±‚ã€‚è¯¥ç»¼è¿°æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä¸´åºŠåŒ»ç”Ÿæä¾›åŸºç¡€æ€§èµ„æºï¼Œå¼•å¯¼æœªæ¥å®ç°æ›´å‡†ç¡®ã€é«˜æ•ˆçš„è‚ºç™Œæ£€æµ‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé›†æˆ CT scans çš„ 3D CNN æ¶æ„åœ¨åˆ†ç±»æ€§èƒ½ä¸Šè¡¨ç°æœ€ä¸ºä¼˜å¼‚ï¼Œä½†é«˜è¯¯æŠ¥ç‡ï¼ˆfalse positivesï¼‰ã€æ•°æ®é›†å˜å¼‚æ€§ï¼ˆdataset variabilityï¼‰åŠè®¡ç®—å¤æ‚æ€§ï¼ˆcomputational complexityï¼‰ç­‰æŒ‘æˆ˜åœ¨å„ç±»æ¨¡æ€ä¸­ä¾ç„¶æ™®éå­˜åœ¨ã€‚",
      "categories": [
        "q-bio.TO",
        "cs.AI"
      ],
      "primary_category": "q-bio.TO",
      "comment": "Accepted at ICMI 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16254v1",
      "published_date": "2025-09-17 19:18:05 UTC",
      "updated_date": "2025-09-17 19:18:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:07:37.995716+00:00"
    },
    {
      "arxiv_id": "2509.15246v1",
      "title": "GenCAD-3D: CAD Program Generation using Multimodal Latent Space Alignment and Synthetic Dataset Balancing",
      "title_zh": "GenCAD-3Dï¼šåŸºäºå¤šæ¨¡æ€æ½œç©ºé—´å¯¹é½ä¸åˆæˆæ•°æ®é›†å¹³è¡¡çš„CADç¨‹åºç”Ÿæˆ",
      "authors": [
        "Nomi Yu",
        "Md Ferdous Alam",
        "A. John Hart",
        "Faez Ahmed"
      ],
      "abstract": "CAD programs, structured as parametric sequences of commands that compile into precise 3D geometries, are fundamental to accurate and efficient engineering design processes. Generating these programs from nonparametric data such as point clouds and meshes remains a crucial yet challenging task, typically requiring extensive manual intervention. Current deep generative models aimed at automating CAD generation are significantly limited by imbalanced and insufficiently large datasets, particularly those lacking representation for complex CAD programs. To address this, we introduce GenCAD-3D, a multimodal generative framework utilizing contrastive learning for aligning latent embeddings between CAD and geometric encoders, combined with latent diffusion models for CAD sequence generation and retrieval. Additionally, we present SynthBal, a synthetic data augmentation strategy specifically designed to balance and expand datasets, notably enhancing representation of complex CAD geometries. Our experiments show that SynthBal significantly boosts reconstruction accuracy, reduces the generation of invalid CAD models, and markedly improves performance on high-complexity geometries, surpassing existing benchmarks. These advancements hold substantial implications for streamlining reverse engineering and enhancing automation in engineering design. We will publicly release our datasets and code, including a set of 51 3D-printed and laser-scanned parts on our project site.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GenCAD-3Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³ä»ç‚¹äº‘å’Œç½‘æ ¼ç­‰éå‚æ•°åŒ–æ•°æ®è‡ªåŠ¨ç”ŸæˆCAD programæŒ‘æˆ˜çš„å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶ã€‚ä¸ºäº†å…‹æœç°æœ‰æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤æ‚CADå‡ ä½•å½¢çŠ¶æ—¶é¢ä¸´çš„æ•°æ®é›†ä¸å¹³è¡¡å’Œè§„æ¨¡ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ (contrastive learning)å¯¹é½CADä¸å‡ ä½•ç¼–ç å™¨çš„æ½œåµŒå…¥ï¼Œå¹¶ç»“åˆæ½œæ‰©æ•£æ¨¡å‹(latent diffusion models)è¿›è¡Œåºåˆ—ç”Ÿæˆä¸æ£€ç´¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†SynthBalåˆæˆæ•°æ®å¢å¼ºç­–ç•¥ï¼Œä¸“é—¨ç”¨äºå¹³è¡¡å’Œæ‰©å±•æ•°æ®é›†ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹å¤æ‚CADå‡ ä½•å½¢çŠ¶çš„è¡¨å¾èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynthBalæ˜¾è‘—æå‡äº†é‡æ„å‡†ç¡®ç‡å¹¶å‡å°‘äº†æ— æ•ˆCADæ¨¡å‹çš„ç”Ÿæˆï¼Œåœ¨é«˜å¤æ‚åº¦å‡ ä½•ä½“ä¸Šçš„è¡¨ç°å¤§å¹…è¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚è¯¥ç ”ç©¶æˆæœå¯¹äºç®€åŒ–é€†å‘å·¥ç¨‹å’Œæå‡å·¥ç¨‹è®¾è®¡è‡ªåŠ¨åŒ–å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "9 figures, 15 pages. Accepted and soon published in the ASME Journal of Mechanical Design",
      "pdf_url": "https://arxiv.org/pdf/2509.15246v1",
      "published_date": "2025-09-17 19:10:44 UTC",
      "updated_date": "2025-09-17 19:10:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:07:44.691548+00:00"
    },
    {
      "arxiv_id": "2509.14360v1",
      "title": "Embodied sensorimotor control: computational modeling of the neural control of movement",
      "title_zh": "å…·èº«æ„Ÿè§‰è¿åŠ¨æ§åˆ¶ï¼šè¿åŠ¨ç¥ç»æ§åˆ¶çš„è®¡ç®—å»ºæ¨¡",
      "authors": [
        "Muhammad Noman Almani",
        "John Lazzari",
        "Jeff Walker",
        "Shreya Saxena"
      ],
      "abstract": "We review how sensorimotor control is dictated by interacting neural populations, optimal feedback mechanisms, and the biomechanics of bodies. First, we outline the distributed anatomical loops that shuttle sensorimotor signals between cortex, subcortical regions, and spinal cord. We then summarize evidence that neural population activity occupies low-dimensional, dynamically evolving manifolds during planning and execution of movements. Next, we summarize literature explaining motor behavior through the lens of optimal control theory, which clarifies the role of internal models and feedback during motor control. Finally, recent studies on embodied sensorimotor control address gaps within each framework by aiming to elucidate neural population activity through the explicit control of musculoskeletal dynamics. We close by discussing open problems and opportunities: multi-tasking and cognitively rich behavior, multi-regional circuit models, and the level of anatomical detail needed in body and network models. Together, this review and recent advances point towards reaching an integrative account of the neural control of movement.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†å…·èº«æ„ŸçŸ¥è¿åŠ¨æ§åˆ¶ (Embodied sensorimotor control) çš„ç¥ç»è®¡ç®—å»ºæ¨¡ï¼Œæ—¨åœ¨å»ºç«‹ç¥ç»è¿åŠ¨æ§åˆ¶çš„ç»¼åˆè§£é‡Šã€‚æ–‡ç« é¦–å…ˆæ¢³ç†äº†è¿æ¥çš®å±‚ã€çš®å±‚ä¸‹åŒºåŸŸä¸è„Šé«“çš„åˆ†å¸ƒå¼è§£å‰–å›è·¯ï¼Œå¹¶æ€»ç»“äº†ç¥ç»ç¾¤ä½“æ´»åŠ¨åœ¨è¿åŠ¨è§„åˆ’ä¸æ‰§è¡Œä¸­å æ®ä½ç»´ã€åŠ¨æ€æ¼”å˜æµå½¢ (low-dimensional, dynamically evolving manifolds) çš„ç›¸å…³è¯æ®ã€‚éšåï¼Œç ”ç©¶ä»æœ€ä¼˜æ§åˆ¶ç†è®º (Optimal Control Theory) çš„è§†è§’è§£é‡Šäº†è¿åŠ¨è¡Œä¸ºï¼Œè¿›ä¸€æ­¥æ˜ç¡®äº†å†…éƒ¨æ¨¡å‹ä¸åé¦ˆåœ¨æ§åˆ¶è¿‡ç¨‹ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†å…·èº«æ§åˆ¶ç ”ç©¶å¦‚ä½•é€šè¿‡æ˜¾å¼å»ºæ¨¡è‚Œè‚‰éª¨éª¼åŠ¨åŠ›å­¦ (musculoskeletal dynamics) æ¥å¼¥è¡¥ç°æœ‰æ¡†æ¶çš„å±€é™æ€§ã€‚æœ€åï¼Œä½œè€…è®¨è®ºäº†å¤šä»»åŠ¡å¤„ç†ã€å¤šåŒºåŸŸå›è·¯æ¨¡å‹ä»¥åŠæ¨¡å‹è§£å‰–ç»†èŠ‚ç­‰æœªæ¥æŒ‘æˆ˜ï¼Œä¸ºå®ç°è¿åŠ¨ç¥ç»æ§åˆ¶çš„ç³»ç»Ÿæ€§æ•´åˆæä¾›äº†é‡è¦æŒ‡å¼•ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Review paper",
      "pdf_url": "https://arxiv.org/pdf/2509.14360v1",
      "published_date": "2025-09-17 18:40:29 UTC",
      "updated_date": "2025-09-17 18:40:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:07:49.198547+00:00"
    },
    {
      "arxiv_id": "2509.18177v1",
      "title": "A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts",
      "title_zh": "ç”¨äºéªŒè¯ç»å¯¹ä¸ç›¸å¯¹ä½ç½®æ¦‚å¿µçš„äººå·¥æ•°æ®é›†ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "George CorrÃªa de AraÃºjo",
        "Helena de Almeida Maia",
        "Helio Pedrini"
      ],
      "abstract": "In this paper, we present the Scrapbook framework, a novel methodology designed to generate extensive datasets for probing the learned concepts of artificial intelligence (AI) models. The framework focuses on fundamental concepts such as object recognition, absolute and relative positions, and attribute identification. By generating datasets with a large number of questions about individual concepts and a wide linguistic variation, the Scrapbook framework aims to validate the model's understanding of these basic elements before tackling more complex tasks. Our experimental findings reveal that, while contemporary models demonstrate proficiency in recognizing and enumerating objects, they encounter challenges in comprehending positional information and addressing inquiries with additional constraints. Specifically, the MobileVLM-V2 model showed significant answer disagreements and plausible wrong answers, while other models exhibited a bias toward affirmative answers and struggled with questions involving geometric shapes and positional information, indicating areas for improvement in understanding and consistency. The proposed framework offers a valuable instrument for generating diverse and comprehensive datasets, which can be utilized to systematically assess and enhance the performance of AI models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Scrapbook æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç”Ÿæˆå¤§è§„æ¨¡æ•°æ®é›†çš„æ–°å‹æ–¹æ³•ï¼Œæ—¨åœ¨æ¢æµ‹äººå·¥æ™ºèƒ½ (AI) æ¨¡å‹å¯¹åŸºæœ¬æ¦‚å¿µçš„ç†è§£ã€‚è¯¥æ¡†æ¶ä¸“æ³¨äºç‰©ä½“è¯†åˆ«ã€ç»å¯¹ä½ç½®ä¸ç›¸å¯¹ä½ç½®ä»¥åŠå±æ€§è¯†åˆ«ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œé€šè¿‡ç”Ÿæˆå…·æœ‰é«˜åº¦è¯­è¨€å¤šæ ·æ€§å’Œå¤§é‡é’ˆå¯¹æ€§é—®é¢˜çš„æ•°æ®é›†ï¼ŒéªŒè¯æ¨¡å‹åœ¨æ‰§è¡Œå¤æ‚ä»»åŠ¡å‰çš„åŸºç¡€èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶ç°ä»£æ¨¡å‹åœ¨è¯†åˆ«å’Œè®¡æ•°ç‰©ä½“æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ä½ç½®ä¿¡æ¯å’Œå¸¦æœ‰é¢å¤–çº¦æŸçš„æŸ¥è¯¢æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚å…·ä½“è€Œè¨€ï¼ŒMobileVLM-V2 æ¨¡å‹è¡¨ç°å‡ºæ˜æ˜¾çš„å›ç­”åˆ†æ­§å’Œè²Œä¼¼åˆç†çš„é”™è¯¯ç­”æ¡ˆï¼Œè€Œå…¶ä»–æ¨¡å‹åˆ™å­˜åœ¨è‚¯å®šåè§ï¼ˆaffirmative biasï¼‰ï¼Œåœ¨æ¶‰åŠå‡ ä½•å½¢çŠ¶å’Œä½ç½®å…³ç³»çš„ç†è§£ä¸Šä¸€è‡´æ€§ä¸è¶³ã€‚Scrapbook æ¡†æ¶ä¸ºç³»ç»Ÿæ€§è¯„ä¼°å’Œæå‡ AI æ¨¡å‹çš„æ€§èƒ½æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "WIP",
      "pdf_url": "https://arxiv.org/pdf/2509.18177v1",
      "published_date": "2025-09-17 18:37:24 UTC",
      "updated_date": "2025-09-17 18:37:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:07:52.704798+00:00"
    },
    {
      "arxiv_id": "2509.14353v3",
      "title": "DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion",
      "title_zh": "DreamControlï¼šåŸºäºå¼•å¯¼æ‰©æ•£çš„å—äººç±»å¯å‘çš„äººå½¢æœºå™¨äººå…¨èº«åœºæ™¯äº¤äº’æ§åˆ¶",
      "authors": [
        "Dvij Kalaria",
        "Sudarshan S Harithas",
        "Pushkal Katara",
        "Sangkyung Kwak",
        "Sarthak Bhagat",
        "Shankar Sastry",
        "Srinath Sridhar",
        "Sai Vemprala",
        "Ashish Kapoor",
        "Jonathan Chung-Kuan Huang"
      ],
      "abstract": "We introduce DreamControl, a novel methodology for learning autonomous whole-body humanoid skills. DreamControl leverages the strengths of diffusion models and Reinforcement Learning (RL): our core innovation is the use of a diffusion prior trained on human motion data, which subsequently guides an RL policy in simulation to complete specific tasks of interest (e.g., opening a drawer or picking up an object). We demonstrate that this human motion-informed prior allows RL to discover solutions unattainable by direct RL, and that diffusion models inherently promote natural looking motions, aiding in sim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1 robot across a diverse set of challenging tasks involving simultaneous lower and upper body control and object interaction. Project website at https://genrobo.github.io/DreamControl/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DreamControlï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å­¦ä¹ è‡ªä¸»å…¨èº«äººå½¢æœºå™¨äººæŠ€èƒ½çš„åˆ›æ–°æ–¹æ³•ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨åœ¨äººç±»è¿åŠ¨æ•°æ®ä¸Šè®­ç»ƒçš„ Diffusion Models æ‰©æ•£å…ˆéªŒæ¥å¼•å¯¼ Reinforcement Learning (RL) ç­–ç•¥ï¼Œä½¿å…¶åœ¨ä»¿çœŸç¯å¢ƒä¸­å®Œæˆæ‹‰å¼€æŠ½å±‰æˆ–æ‹¾å–ç‰©ä½“ç­‰ç‰¹å®šä»»åŠ¡ã€‚è¿™ç§å—äººç±»è¿åŠ¨å¯å‘çš„å…ˆéªŒçŸ¥è¯†ä½¿ RL èƒ½å¤Ÿå‘ç°ç›´æ¥å­¦ä¹ éš¾ä»¥è§¦åŠçš„å¤æ‚è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ Diffusion Models ç”Ÿæˆçš„è‡ªç„¶è¿åŠ¨ç‰¹æ€§æœ‰æ•ˆåŠ©åŠ›äº† Sim-to-Real è¿ç§»ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ Unitree G1 æœºå™¨äººä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®éªŒæ¶µç›–äº†æ¶‰åŠå…¨èº«åè°ƒå’Œç‰©ä½“äº¤äº’çš„ä¸€ç³»åˆ—æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚DreamControl æˆåŠŸè¯æ˜äº†ç»“åˆç”Ÿæˆå¼æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ åœ¨æå‡äººå½¢æœºå™¨äººåœºæ™¯äº¤äº’èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "https://genrobo.github.io/DreamControl/ (under submission)",
      "pdf_url": "https://arxiv.org/pdf/2509.14353v3",
      "published_date": "2025-09-17 18:35:43 UTC",
      "updated_date": "2025-09-30 15:48:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:08:03.300318+00:00"
    },
    {
      "arxiv_id": "2509.16251v1",
      "title": "R-Net: A Reliable and Resource-Efficient CNN for Colorectal Cancer Detection with XAI Integration",
      "title_zh": "R-Netï¼šé›†æˆå¯è§£é‡Šäººå·¥æ™ºèƒ½çš„å¯é ä¸”èµ„æºé«˜æ•ˆå‹ç»“ç›´è‚ ç™Œæ£€æµ‹å·ç§¯ç¥ç»ç½‘ç»œ",
      "authors": [
        "Rokonozzaman Ayon",
        "Md Taimur Ahad",
        "Bo Song",
        "Yan Li"
      ],
      "abstract": "State-of-the-art (SOTA) Convolutional Neural Networks (CNNs) are criticized for their extensive computational power, long training times, and large datasets. To overcome this limitation, we propose a reasonable network (R-Net), a lightweight CNN only to detect and classify colorectal cancer (CRC) using the Enteroscope Biopsy Histopathological Hematoxylin and Eosin Image Dataset (EBHI). Furthermore, six SOTA CNNs, including Multipath-based CNNs (DenseNet121, ResNet50), Depth-based CNNs (InceptionV3), width-based multi-connection CNNs (Xception), depth-wise separable convolutions (MobileNetV2), spatial exploitation-based CNNs (VGG16), Transfer learning, and two ensemble models are also tested on the same dataset. The ensemble models are a multipath-depth-width combination (DenseNet121-InceptionV3-Xception) and a multipath-depth-spatial combination (ResNet18-InceptionV3-VGG16). However, the proposed R-Net lightweight achieved 99.37% accuracy, outperforming MobileNet (95.83%) and ResNet50 (96.94%). Most importantly, to understand the decision-making of R-Net, Explainable AI such as SHAP, LIME, and Grad-CAM are integrated to visualize which parts of the EBHI image contribute to the detection and classification process of R-Net. The main novelty of this research lies in building a reliable, lightweight CNN R-Net that requires fewer computing resources yet maintains strong prediction results. SOTA CNNs, transfer learning, and ensemble models also extend our knowledge on CRC classification and detection. XAI functionality and the impact of pixel intensity on correct and incorrect classification images are also some novelties in CRC detection and classification.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†R-Netï¼Œä¸€ç§å¯é ä¸”èµ„æºé«˜æ•ˆçš„è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å…ˆè¿›æ¨¡å‹(SOTA)å¯¹è®¡ç®—èƒ½åŠ›è¦æ±‚è¿‡é«˜çš„é—®é¢˜ï¼Œä¸“é—¨ç”¨äºç»“ç›´è‚ ç™Œ(CRC)çš„æ£€æµ‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å†…çª¥é•œæ´»æ£€ç»„ç»‡ç—…ç†å­¦è‹æœ¨ç²¾-ä¼Šçº¢å›¾åƒæ•°æ®é›†(EBHI)è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸åŒ…æ‹¬DenseNet121ã€ResNet50ã€MobileNetV2åœ¨å†…çš„å¤šç§SOTAæ¨¡å‹åŠé›†æˆæ¨¡å‹è¿›è¡Œäº†æ€§èƒ½å¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR-Netå®ç°äº†99.37%çš„å‡†ç¡®ç‡ï¼Œå…¶æ€§èƒ½ä¼˜äºMobileNet(95.83%)å’ŒResNet50(96.94%)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é›†æˆäº†SHAPã€LIMEå’ŒGrad-CAMç­‰å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æŠ€æœ¯ï¼Œä»¥å¯è§†åŒ–æ¨¡å‹å†³ç­–è¿‡ç¨‹å¹¶åˆ†æåƒç´ å¼ºåº¦å¯¹åˆ†ç±»ç»“æœçš„å½±å“ã€‚è¯¥ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæ„å»ºäº†ä¸€ä¸ªä½èµ„æºæ¶ˆè€—ã€é«˜é¢„æµ‹ç²¾åº¦çš„å¯é è½»é‡çº§ç½‘ç»œï¼Œå¹¶ç»“åˆXAIåŠŸèƒ½æå‡äº†åŒ»å­¦å½±åƒè¯Šæ–­çš„å¯ä¿¡ä»»åº¦ã€‚",
      "categories": [
        "q-bio.TO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "q-bio.TO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16251v1",
      "published_date": "2025-09-17 18:29:44 UTC",
      "updated_date": "2025-09-17 18:29:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:08:23.296167+00:00"
    },
    {
      "arxiv_id": "2509.14343v2",
      "title": "Near-Real-Time Resource Slicing for QoS Optimization in 5G O-RAN using Deep Reinforcement Learning",
      "title_zh": "åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ 5G O-RAN è¿‘å®æ—¶èµ„æºåˆ‡ç‰‡ä¸ QoS ä¼˜åŒ–",
      "authors": [
        "Peihao Yan",
        "Jie Lu",
        "Huacheng Zeng",
        "Y. Thomas Hou"
      ],
      "abstract": "Open-Radio Access Network (O-RAN) has become an important paradigm for 5G and beyond radio access networks. This paper presents an xApp called xSlice for the Near-Real-Time (Near-RT) RAN Intelligent Controller (RIC) of 5G O-RANs. xSlice is an online learning algorithm that adaptively adjusts MAC-layer resource allocation in response to dynamic network states, including time-varying wireless channel conditions, user mobility, traffic fluctuations, and changes in user demand. To address these network dynamics, we first formulate the Quality-of-Service (QoS) optimization problem as a regret minimization problem by quantifying the QoS demands of all traffic sessions through weighting their throughput, latency, and reliability. We then develop a deep reinforcement learning (DRL) framework that utilizes an actor-critic model to combine the advantages of both value-based and policy-based updating methods. A graph convolutional network (GCN) is incorporated as a component of the DRL framework for graph embedding of RAN data, enabling xSlice to handle a dynamic number of traffic sessions. We have implemented xSlice on an O-RAN testbed with 10 smartphones and conducted extensive experiments to evaluate its performance in realistic scenarios. Experimental results show that xSlice can reduce performance regret by 67% compared to the state-of-the-art solutions. Source code is available at https://github.com/xslice-5G/code.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† xSliceï¼Œè¿™æ˜¯ä¸€ç§é¢å‘ 5G O-RAN è¿‘å®æ—¶æ— çº¿æ™ºèƒ½æ§åˆ¶å™¨ (Near-Real-Time RIC) çš„ xAppï¼Œæ—¨åœ¨ä¼˜åŒ–åŠ¨æ€ç½‘ç»œç¯å¢ƒä¸‹çš„æœåŠ¡è´¨é‡ (QoS) èµ„æºåˆ‡ç‰‡ã€‚ç ”ç©¶é€šè¿‡é‡åŒ–ååé‡ã€å»¶è¿Ÿå’Œå¯é æ€§éœ€æ±‚ï¼Œå°† QoS ä¼˜åŒ–å»ºæ¨¡ä¸ºé—æ†¾æœ€å°åŒ– (regret minimization) é—®é¢˜ï¼Œä»¥é€‚åº”ä¿¡é“æ³¢åŠ¨å’Œç”¨æˆ·ç§»åŠ¨ç­‰åŠ¨æ€ç‰¹æ€§ã€‚æŠ€æœ¯ä¸Šï¼ŒxSlice é‡‡ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning) æ¡†æ¶ï¼Œç»“åˆ Actor-Critic æ¨¡å‹ä¸å›¾å·ç§¯ç½‘ç»œ (Graph Convolutional Network) çš„å›¾åµŒå…¥ (graph embedding) èƒ½åŠ›ï¼Œå®ç°äº†å¯¹åŠ¨æ€ä¸šåŠ¡ä¼šè¯çš„é«˜æ•ˆå¤„ç†ã€‚åœ¨ 10 éƒ¨æ™ºèƒ½æ‰‹æœºç»„æˆçš„ O-RAN å®éªŒå¹³å° (testbed) ä¸Šï¼Œè¯¥æ–¹æ¡ˆç›¸æ¯”ç°æœ‰æŠ€æœ¯é™ä½äº† 67% çš„æ€§èƒ½é—æ†¾ã€‚è¿™é¡¹ç ”ç©¶ä¸º 5G åŠå…¶æ¼”è¿›ç½‘ç»œä¸­çš„å®æ—¶èµ„æºåˆ†é…æä¾›äº†å…·å¤‡é«˜åº¦è‡ªé€‚åº”æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "Published in: IEEE Transactions on Networking",
      "pdf_url": "https://arxiv.org/pdf/2509.14343v2",
      "published_date": "2025-09-17 18:20:04 UTC",
      "updated_date": "2026-01-12 18:11:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:08:37.490980+00:00"
    },
    {
      "arxiv_id": "2509.16250v1",
      "title": "A study on Deep Convolutional Neural Networks, transfer learning, and Mnet model for Cervical Cancer Detection",
      "title_zh": "æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œã€è¿ç§»å­¦ä¹ åŠ Mnet æ¨¡å‹åœ¨å®«é¢ˆç™Œæ£€æµ‹ä¸­çš„ç ”ç©¶",
      "authors": [
        "Saifuddin Sagor",
        "Md Taimur Ahad",
        "Faruk Ahmed",
        "Rokonozzaman Ayon",
        "Sanzida Parvin"
      ],
      "abstract": "Early and accurate detection through Pap smear analysis is critical to improving patient outcomes and reducing mortality of Cervical cancer. State-of-the-art (SOTA) Convolutional Neural Networks (CNNs) require substantial computational resources, extended training time, and large datasets. In this study, a lightweight CNN model, S-Net (Simple Net), is developed specifically for cervical cancer detection and classification using Pap smear images to address these limitations. Alongside S-Net, six SOTA CNNs were evaluated using transfer learning, including multi-path (DenseNet201, ResNet152), depth-based (Serasnet152), width-based multi-connection (Xception), depth-wise separable convolutions (MobileNetV2), and spatial exploitation-based (VGG19). All models, including S-Net, achieved comparable accuracy, with S-Net reaching 99.99%. However, S-Net significantly outperforms the SOTA CNNs in terms of computational efficiency and inference time, making it a more practical choice for real-time and resource-constrained applications. A major limitation in CNN-based medical diagnosis remains the lack of transparency in the decision-making process. To address this, Explainable AI (XAI) techniques, such as SHAP, LIME, and Grad-CAM, were employed to visualize and interpret the key image regions influencing model predictions. The novelty of this study lies in the development of a highly accurate yet computationally lightweight model (S-Net) caPable of rapid inference while maintaining interpretability through XAI integration. Furthermore, this work analyzes the behavior of SOTA CNNs, investigates the effects of negative transfer learning on Pap smear images, and examines pixel intensity patterns in correctly and incorrectly classified samples.",
      "tldr_zh": "æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å­å®«é¢ˆæŠ¹ç‰‡(Pap smear)åˆ†æå®ç°å®«é¢ˆç™Œçš„æ—©æœŸç²¾ç¡®æ£€æµ‹ï¼Œé’ˆå¯¹ç°æœ‰å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)å¯¹è®¡ç®—èµ„æºè¦æ±‚é«˜ã€è®­ç»ƒæ—¶é—´é•¿ç­‰å±€é™æ€§ï¼Œå¼€å‘äº†ä¸€ç§åä¸ºS-Net (Simple Net)çš„è½»é‡åŒ–CNNæ¨¡å‹ã€‚è¯¥ç ”ç©¶åˆ©ç”¨è¿ç§»å­¦ä¹ (transfer learning)è¯„ä¼°äº†DenseNet201ã€ResNet152å’ŒMobileNetV2ç­‰å…­ç§ä¸»æµSOTA CNNæ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸S-Netè¿›è¡Œäº†å¯¹æ¯”åˆ†æã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒS-Netåœ¨æ£€æµ‹ä¸­è¾¾åˆ°äº†99.99%çš„æé«˜å‡†ç¡®ç‡ï¼Œä¸”åœ¨è®¡ç®—æ•ˆç‡å’Œæ¨ç†æ—¶é—´ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæ›´é€‚ç”¨äºå®æ—¶å’Œèµ„æºå—é™çš„åº”ç”¨åœºæ™¯ã€‚ä¸ºäº†å¢å¼ºåŒ»ç–—è¯Šæ–­çš„é€æ˜åº¦ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†SHAPã€LIMEå’ŒGrad-CAMç­‰å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æŠ€æœ¯ï¼Œç”¨äºå¯è§†åŒ–å¹¶è§£é‡Šæ¨¡å‹å†³ç­–çš„å…³é”®å›¾åƒåŒºåŸŸã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜æ·±å…¥æ¢è®¨äº†è¿ç§»å­¦ä¹ ä¸­çš„è´Ÿè¿ç§»(negative transfer learning)ç°è±¡ï¼Œå¹¶ç³»ç»Ÿåˆ†æäº†åˆ†ç±»æ ·æœ¬çš„åƒç´ å¼ºåº¦æ¨¡å¼ï¼Œä¸ºæ„å»ºé«˜æ•ˆä¸”å¯ä¿¡çš„åŒ»å­¦å½±åƒè¾…åŠ©è¯Šæ–­ç³»ç»Ÿæä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "q-bio.TO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "q-bio.TO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16250v1",
      "published_date": "2025-09-17 18:11:09 UTC",
      "updated_date": "2025-09-17 18:11:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:08:31.688040+00:00"
    },
    {
      "arxiv_id": "2509.14335v1",
      "title": "Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing",
      "title_zh": "è¶…è¶Šåˆ†ç±»ï¼šé’ˆå¯¹ç»†ç²’åº¦è‡ªåŠ¨åŒ–æ¶æ„è½¯ä»¶è¡Œä¸ºå®¡è®¡çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Xinran Zheng",
        "Xingzhi Qian",
        "Yiling He",
        "Shuo Yang",
        "Lorenzo Cavallaro"
      ],
      "abstract": "Automated malware classification has achieved strong detection performance. Yet, malware behavior auditing seeks causal and verifiable explanations of malicious activities -- essential not only to reveal what malware does but also to substantiate such claims with evidence. This task is challenging, as adversarial intent is often hidden within complex, framework-heavy applications, making manual auditing slow and costly. Large Language Models (LLMs) could help address this gap, but their auditing potential remains largely unexplored due to three limitations: (1) scarce fine-grained annotations for fair assessment; (2) abundant benign code obscuring malicious signals; and (3) unverifiable, hallucination-prone outputs undermining attribution credibility. To close this gap, we introduce MalEval, a comprehensive framework for fine-grained Android malware auditing, designed to evaluate how effectively LLMs support auditing under real-world constraints. MalEval provides expert-verified reports and an updated sensitive API list to mitigate ground truth scarcity and reduce noise via static reachability analysis. Function-level structural representations serve as intermediate attribution units for verifiable evaluation. Building on this, we define four analyst-aligned tasks -- function prioritization, evidence attribution, behavior synthesis, and sample discrimination -- together with domain-specific metrics and a unified workload-oriented score. We evaluate seven widely used LLMs on a curated dataset of recent malware and misclassified benign apps, offering the first systematic assessment of their auditing capabilities. MalEval reveals both promising potential and critical limitations across audit stages, providing a reproducible benchmark and foundation for future research on LLM-enhanced malware behavior auditing. MalEval is publicly available at https://github.com/ZhengXR930/MalEval.git",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¶æ„è½¯ä»¶è¡Œä¸ºå®¡è®¡ï¼ˆMalware Behavior Auditingï¼‰ä¸­é¢ä¸´çš„å› æœåˆ†æéš¾ã€æ‰‹åŠ¨æˆæœ¬é«˜ä»¥åŠå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨å¹»è§‰å’Œæ•°æ®åŒ®ä¹ç­‰é—®é¢˜ï¼Œæå‡ºäº†é’ˆå¯¹ Android å¹³å°çš„ç»†ç²’åº¦å®¡è®¡è¯„ä¼°æ¡†æ¶ MalEvalã€‚è¯¥æ¡†æ¶ç»“åˆä¸“å®¶éªŒè¯æŠ¥å‘Šå’Œæ•æ„Ÿ API åˆ—è¡¨ï¼Œåˆ©ç”¨é™æ€å¯è¾¾æ€§åˆ†æï¼ˆStatic Reachability Analysisï¼‰å‡å°‘è‰¯æ€§ä»£ç å¹²æ‰°ï¼Œå¹¶å¼•å…¥å‡½æ•°çº§ç»“æ„è¡¨ç¤ºä½œä¸ºå¯éªŒè¯è¯„ä¼°çš„ä¸­é—´å½’å› å•å…ƒã€‚ç ”ç©¶è¿›ä¸€æ­¥å®šä¹‰äº†å‡½æ•°ä¼˜å…ˆçº§æ’åºï¼ˆFunction Prioritizationï¼‰ã€è¯æ®å½’å› ï¼ˆEvidence Attributionï¼‰ã€è¡Œä¸ºåˆæˆï¼ˆBehavior Synthesisï¼‰åŠæ ·æœ¬åŒºåˆ†ï¼ˆSample Discriminationï¼‰å››é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œå¹¶åˆ¶å®šäº†é¢†åŸŸç‰¹å®šçš„åº¦é‡æ ‡å‡†ã€‚é€šè¿‡å¯¹ä¸ƒç§ä¸»æµ LLMs åœ¨è¿‘æœŸæ¶æ„è½¯ä»¶æ•°æ®é›†ä¸Šçš„ç³»ç»Ÿè¯„ä¼°ï¼Œè¯¥ç ”ç©¶é¦–æ¬¡æ­ç¤ºäº†æ¨¡å‹åœ¨å®¡è®¡å„é˜¶æ®µçš„æ½œåŠ›ä¸å±€é™æ€§ã€‚MalEval çš„æå‡ºä¸ºæœªæ¥åˆ©ç”¨ LLM å¢å¼ºæ¶æ„è½¯ä»¶è¡Œä¸ºå®¡è®¡çš„ç ”ç©¶æä¾›äº†å¯é‡ç°çš„åŸºå‡†å’ŒåŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14335v1",
      "published_date": "2025-09-17 18:05:21 UTC",
      "updated_date": "2025-09-17 18:05:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:08:32.592677+00:00"
    },
    {
      "arxiv_id": "2509.14233v2",
      "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments",
      "title_zh": "Apertusï¼šé¢å‘å…¨çƒè¯­è¨€ç¯å¢ƒæ¨åŠ¨å¼€æ”¾ä¸”åˆè§„å¤§è¯­è¨€æ¨¡å‹çš„æ°‘ä¸»åŒ–",
      "authors": [
        "Project Apertus",
        "Alejandro HernÃ¡ndez-Cano",
        "Alexander HÃ¤gele",
        "Allen Hao Huang",
        "Angelika Romanou",
        "Antoni-Joan Solergibert",
        "Barna Pasztor",
        "Bettina Messmer",
        "Dhia Garbaya",
        "Eduard Frank Äurech",
        "Ido Hakimi",
        "Juan GarcÃ­a Giraldo",
        "Mete Ismayilzada",
        "Negar Foroutan",
        "Skander Moalla",
        "Tiancheng Chen",
        "Vinko SabolÄec",
        "Yixuan Xu",
        "Michael Aerni",
        "Badr AlKhamissi",
        "InÃ©s Altemir MariÃ±as",
        "Mohammad Hossein Amani",
        "Matin Ansaripour",
        "Ilia Badanin",
        "Harold Benoit",
        "Emanuela Boros",
        "Nicholas Browning",
        "Fabian BÃ¶sch",
        "Maximilian BÃ¶ther",
        "Niklas Canova",
        "Camille Challier",
        "Clement Charmillot",
        "Jonathan Coles",
        "Jan Deriu",
        "Arnout Devos",
        "Lukas Drescher",
        "Daniil Dzenhaliou",
        "Maud Ehrmann",
        "Dongyang Fan",
        "Simin Fan",
        "Silin Gao",
        "Miguel Gila",
        "MarÃ­a Grandury",
        "Diba Hashemi",
        "Alexander Hoyle",
        "Jiaming Jiang",
        "Mark Klein",
        "Andrei Kucharavy",
        "Anastasiia Kucherenko",
        "Frederike LÃ¼beck",
        "Roman Machacek",
        "Theofilos Manitaras",
        "Andreas Marfurt",
        "Kyle Matoba",
        "Simon Matrenok",
        "Henrique MendonÃ§a",
        "Fawzi Roberto Mohamed",
        "Syrielle Montariol",
        "Luca Mouchel",
        "Sven Najem-Meyer",
        "Jingwei Ni",
        "Gennaro Oliva",
        "Matteo Pagliardini",
        "Elia Palme",
        "Andrei Panferov",
        "LÃ©o Paoletti",
        "Marco Passerini",
        "Ivan Pavlov",
        "Auguste Poiroux",
        "Kaustubh Ponkshe",
        "Nathan Ranchin",
        "Javi Rando",
        "Mathieu Sauser",
        "Jakhongir Saydaliev",
        "Muhammad Ali Sayfiddinov",
        "Marian Schneider",
        "Stefano Schuppli",
        "Marco Scialanga",
        "Andrei Semenov",
        "Kumar Shridhar",
        "Raghav Singhal",
        "Anna Sotnikova",
        "Alexander Sternfeld",
        "Ayush Kumar Tarun",
        "Paul Teiletche",
        "Jannis Vamvas",
        "Xiaozhe Yao",
        "Hao Zhao",
        "Alexander Ilic",
        "Ana Klimovic",
        "Andreas Krause",
        "Caglar Gulcehre",
        "David Rosenthal",
        "Elliott Ash",
        "Florian TramÃ¨r",
        "Joost VandeVondele",
        "Livio Veraldi",
        "Martin Rajman",
        "Thomas Schulthess",
        "Torsten Hoefler",
        "Antoine Bosselut",
        "Martin Jaggi",
        "Imanol Schlag"
      ],
      "abstract": "We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting `robots.txt` exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Apertusï¼Œä¸€å¥—å®Œå…¨å¼€æºçš„å¤§è¯­è¨€æ¨¡å‹ (LLMs)ï¼Œæ—¨åœ¨è§£å†³å½“å‰å¼€æºæ¨¡å‹ç”Ÿæ€ä¸­æ•°æ®åˆè§„æ€§ (data compliance) å’Œå¤šè¯­è¨€è¡¨ç¤º (multilingual representation) ä¸è¶³çš„ç³»ç»Ÿæ€§ç¼ºé™·ã€‚Apertus æ¨¡å‹å®Œå…¨åŸºäºå…¬å¼€å¯ç”¨æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œé€šè¿‡è¿½æº¯æ€§éµå®ˆ robots.txt æ’é™¤æ ‡å‡†å¹¶è¿‡æ»¤éè®¸å¯ã€æ¯’æ€§å’Œä¸ªäººèº«ä»½ä¿¡æ¯ (PII) å†…å®¹ï¼Œç¡®ä¿äº†æé«˜çš„åˆè§„æ ‡å‡†ã€‚ä¸ºäº†é™ä½æ•°æ®è®°å¿†åŒ– (memorization) é£é™©ï¼Œç ”ç©¶å›¢é˜Ÿåœ¨é¢„è®­ç»ƒä¸­é‡‡ç”¨äº† Goldfish objectiveï¼Œåœ¨ä¿ç•™ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶å¼ºåŠ›æŠ‘åˆ¶äº†æ¨¡å‹å¯¹åŸå§‹æ•°æ®çš„é€å­—å¬å›ã€‚è¯¥ç³»åˆ—æ¨¡å‹åœ¨æ¥è‡ª 1800 å¤šç§è¯­è¨€çš„ 15T tokens ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­çº¦ 40% çš„é¢„è®­ç»ƒæ•°æ®ä¸ºéè‹±è¯­å†…å®¹ï¼Œå¹¶æä¾› 8B å’Œ 70B ä¸¤ç§å‚æ•°è§„æ¨¡ã€‚å®éªŒè¡¨æ˜ï¼ŒApertus åœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å®Œå…¨å¼€æºæ¨¡å‹çš„å…ˆè¿›æ°´å¹³ï¼Œè¡¨ç°è¶³ä»¥åª²ç¾æˆ–è¶…è¶Šè®¸å¤šä»…å¼€æ”¾æƒé‡çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿä»¥è®¸å¯åè®®å‘å¸ƒäº†æ•°æ®å‡†å¤‡è„šæœ¬ã€æ£€æŸ¥ç‚¹ (checkpoints) åŠè®­ç»ƒä»£ç ç­‰å…¨å¥—ç§‘å­¦äº§å‡ºï¼Œæå¤§åœ°ä¿ƒè¿›äº†å­¦æœ¯å®¡è®¡ä¸æŠ€æœ¯æ‰©å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14233v2",
      "published_date": "2025-09-17 17:59:21 UTC",
      "updated_date": "2025-12-01 20:03:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:08:41.953805+00:00"
    },
    {
      "arxiv_id": "2509.14223v2",
      "title": "Fresh in memory: Training-order recency is linearly encoded in language model activations",
      "title_zh": "è®°å¿†çŠ¹æ–°ï¼šè®­ç»ƒé¡ºåºçš„è¿‘å› æ€§åœ¨è¯­è¨€æ¨¡å‹æ¿€æ´»ä¸­å‘ˆçº¿æ€§ç¼–ç ",
      "authors": [
        "Dmitrii Krasheninnikov",
        "Richard E. Turner",
        "David Krueger"
      ],
      "abstract": "We show that language models' activations linearly encode when information was learned during training. Our setup involves creating a model with a known training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but otherwise similar datasets about named entities. We find that the average activations of test samples corresponding to the six training datasets encode the training order: when projected into a 2D subspace, these centroids are arranged exactly in the order of training and lie on a straight line. Further, we show that linear probes can accurately (~90%) distinguish \"early\" vs. \"late\" entities, generalizing to entities unseen during the probes' own training. The model can also be fine-tuned to explicitly report an unseen entity's training stage (~80% accuracy). Interestingly, the training-order encoding does not seem attributable to simple differences in activation magnitudes, losses, or model confidence. Our paper demonstrates that models are capable of differentiating information by its acquisition time, and carries significant implications for how they might manage conflicting data and respond to knowledge modifications.",
      "tldr_zh": "è¯¥ç ”ç©¶å‘ç°è¯­è¨€æ¨¡å‹ï¼ˆlanguage modelsï¼‰çš„æ¿€æ´»çŠ¶æ€ï¼ˆactivationsï¼‰èƒ½å¤Ÿä»¥çº¿æ€§æ–¹å¼ç¼–ç è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ ä¿¡æ¯çš„æ—¶é—´é¡ºåºã€‚ç ”ç©¶è€…é€šè¿‡åœ¨å…­ä¸ªä¸ç›¸äº¤ä½†ç±»ä¼¼çš„å‘½åå®ä½“ï¼ˆnamed entitiesï¼‰æ•°æ®é›†ä¸Šé¡ºåºå¾®è°ƒï¼ˆfine-tuningï¼‰Llama-3.2-1Bæ¨¡å‹ï¼Œæ„å»ºäº†ä¸€ä¸ªå·²çŸ¥è®­ç»ƒé¡ºåºçš„å®éªŒç¯å¢ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å°†å…­ä¸ªæ•°æ®é›†å¯¹åº”çš„æµ‹è¯•æ ·æœ¬å¹³å‡æ¿€æ´»å‘é‡æŠ•å½±åˆ°äºŒç»´å­ç©ºé—´æ—¶ï¼Œè¿™äº›è´¨å¿ƒå®Œå…¨æŒ‰ç…§è®­ç»ƒé¡ºåºæ’åˆ—åœ¨ä¸€æ¡ç›´çº¿ä¸Šã€‚æ­¤å¤–ï¼Œçº¿æ€§æ¢é’ˆï¼ˆlinear probesï¼‰èƒ½å¤Ÿä»¥çº¦90%çš„å‡†ç¡®ç‡åŒºåˆ†â€œæ—©æœŸâ€ä¸â€œæ™šæœŸâ€å®ä½“ï¼Œå¹¶èƒ½æ³›åŒ–è‡³æ¢é’ˆè®­ç»ƒä¸­æœªè§è¿‡çš„å®ä½“ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œæ¨¡å‹ç»è¿‡å¾®è°ƒåèƒ½ä»¥çº¦80%çš„å‡†ç¡®ç‡æ˜¾å¼æŠ¥å‘Šæœªè§è¿‡å®ä½“çš„è®­ç»ƒé˜¶æ®µï¼Œä¸”è¿™ç§ç¼–ç æœºåˆ¶å¹¶éå½’å› äºæ¿€æ´»å¹…åº¦ï¼ˆactivation magnitudesï¼‰ã€æŸå¤±å€¼ï¼ˆlossesï¼‰æˆ–æ¨¡å‹ç½®ä¿¡åº¦ï¼ˆmodel confidenceï¼‰çš„ç®€å•å·®å¼‚ã€‚è¯¥è®ºæ–‡è¯æ˜äº†æ¨¡å‹å…·å¤‡æ ¹æ®è·å–æ—¶é—´åŒºåˆ†ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¯¹äºç†è§£æ¨¡å‹å¦‚ä½•å¤„ç†å†²çªæ•°æ®åŠå“åº”çŸ¥è¯†ä¿®æ­£ï¼ˆknowledge modificationsï¼‰å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14223v2",
      "published_date": "2025-09-17 17:54:22 UTC",
      "updated_date": "2025-09-22 16:05:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:08:41.056922+00:00"
    },
    {
      "arxiv_id": "2509.14216v1",
      "title": "A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training",
      "title_zh": "ç”¨äºéšæœºè¿­ä»£çš„é€šç”¨ Banach--Bregman æ¡†æ¶ï¼šç»Ÿä¸€éšæœºé•œåƒä¸‹é™ã€å­¦ä¹ ä¸ LLM è®­ç»ƒ",
      "authors": [
        "Johnny R. Zhang",
        "Xiaomei Mi",
        "Gaoyuan Du",
        "Qianyi Sun",
        "Shiqi Wang",
        "Jiaxuan Li",
        "Wenhua Zhou"
      ],
      "abstract": "Stochastic optimization powers the scalability of modern artificial intelligence, spanning machine learning, deep learning, reinforcement learning, and large language model training. Yet, existing theory remains largely confined to Hilbert spaces, relying on inner-product frameworks and orthogonality. This paradigm fails to capture non-Euclidean settings, such as mirror descent on simplices, Bregman proximal methods for sparse learning, natural gradient descent in information geometry, or Kullback--Leibler-regularized language model training. Unlike Euclidean-based Hilbert-space methods, this approach embraces general Banach spaces. This work introduces a pioneering Banach--Bregman framework for stochastic iterations, establishing Bregman geometry as a foundation for next-generation optimization. It (i) provides a unified template via Bregman projections and Bregman--Fejer monotonicity, encompassing stochastic approximation, mirror descent, natural gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations ($Î»> 2$) in non-Hilbert settings, enabling flexible geometries and elucidating their acceleration effect; and (iii) delivers convergence theorems spanning almost-sure boundedness to geometric rates, validated on synthetic and real-world tasks. Empirical studies across machine learning (UCI benchmarks), deep learning (e.g., Transformer training), reinforcement learning (actor--critic), and large language models (WikiText-2 with distilGPT-2) show up to 20% faster convergence, reduced variance, and enhanced accuracy over classical baselines. These results position Banach--Bregman geometry as a cornerstone unifying optimization theory and practice across core AI paradigms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é€šç”¨çš„ Banach--Bregman æ¡†æ¶ç”¨äºéšæœºè¿­ä»£ï¼ˆStochastic Iterationsï¼‰ï¼ŒæˆåŠŸç»Ÿä¸€äº†éšæœºé•œåƒä¸‹é™ï¼ˆStochastic Mirror Descentï¼‰ã€æ·±åº¦å­¦ä¹ åŠå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒç­‰å¤šç§ä¼˜åŒ–èŒƒå‹ã€‚ä¸åŒäºå±€é™äº Hilbert ç©ºé—´çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é€šç”¨ Banach ç©ºé—´ä¸­çš„ Bregman å‡ ä½•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ç¨€ç–å­¦ä¹ ã€è‡ªç„¶æ¢¯åº¦ï¼ˆNatural Gradientï¼‰ä»¥åŠ Kullback--Leibler æ­£åˆ™åŒ–ç­‰éæ¬§å‡ é‡Œå¾—åœºæ™¯ã€‚é€šè¿‡å¼•å…¥ Bregman æŠ•å½±ï¼ˆBregman Projectionsï¼‰å’Œ Bregman--Fejer å•è°ƒæ€§ï¼Œç ”ç©¶è€…ä¸ºè‡ªé€‚åº”æ–¹æ³•å’Œé•œåƒè¿‘ç«¯ç®—æ³•æä¾›äº†ç»Ÿä¸€æ¨¡æ¿ï¼Œå¹¶æ­ç¤ºäº†é Hilbert è®¾ç½®ä¸‹è¶…æ¾å¼›ï¼ˆSuper-relaxationsï¼‰æŠ€æœ¯çš„åŠ é€Ÿæ•ˆåº”ã€‚ç†è®ºä¸Šï¼Œè¯¥å·¥ä½œå»ºç«‹äº†ä»å‡ ä¹å¤„å¤„æœ‰ç•Œæ€§åˆ°å‡ ä½•æ”¶æ•›ç‡çš„å®Œå¤‡æ”¶æ•›æ€§å®šç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ Transformer è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ç­‰ä»»åŠ¡ä¸­ï¼Œè¯¥æ¡†æ¶è¾ƒç»å…¸åŸºå‡†æ–¹æ³•æ”¶æ•›é€Ÿåº¦æå‡é«˜è¾¾ 20%ï¼Œå¹¶æ˜¾è‘—é™ä½äº†æ–¹å·®ã€‚è¿™ä¸€æˆæœä¸ºä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½ä¼˜åŒ–ç†è®ºå¥ å®šäº†åŸºç¡€ï¼Œä½¿å…¶æˆä¸ºè¿æ¥ AI æ ¸å¿ƒèŒƒå¼ä¸­ç†è®ºä¸å®è·µçš„å…³é”®åŸºçŸ³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "69 pages, 10 figures. Preprint",
      "pdf_url": "https://arxiv.org/pdf/2509.14216v1",
      "published_date": "2025-09-17 17:50:59 UTC",
      "updated_date": "2025-09-17 17:50:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:08:51.766056+00:00"
    },
    {
      "arxiv_id": "2509.14199v2",
      "title": "Dense Video Understanding with Gated Residual Tokenization",
      "title_zh": "åŸºäºé—¨æ§æ®‹å·®æ ‡è®°åŒ–çš„å¯†é›†è§†é¢‘ç†è§£",
      "authors": [
        "Haichao Zhang",
        "Wenhao Chai",
        "Shwai He",
        "Ang Li",
        "Yun Fu"
      ],
      "abstract": "High temporal resolution is essential for capturing fine-grained details in video understanding. However, current video large language models (VLLMs) and benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or keyframe selection, discarding dense temporal information. This compromise avoids the high cost of tokenizing every frame, which otherwise leads to redundant computation and linear token growth as video length increases. While this trade-off works for slowly changing content, it fails for tasks like lecture comprehension, where information appears in nearly every frame and requires precise temporal alignment. To address this gap, we introduce Dense Video Understanding (DVU), which enables high-FPS video comprehension by reducing both tokenization time and token overhead. Existing benchmarks are also limited, as their QA pairs focus on coarse content changes. We therefore propose DIVE (Dense Information Video Evaluation), the first benchmark designed for dense temporal reasoning. To make DVU practical, we present Gated Residual Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated Tokenization uses pixel-level motion estimation to skip static regions during tokenization, achieving sub-linear growth in token count and compute. (2) Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions within a scene, further reducing redundancy while preserving dynamic semantics. Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales positively with FPS. These results highlight the importance of dense temporal information and demonstrate that GRT enables efficient, scalable high-FPS video understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹(VLLMs)å› ä¾èµ–ä½å¸§ç‡é‡‡æ ·è€Œå¯¼è‡´ç»†ç²’åº¦æ—¶é—´ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ï¼Œæå‡ºäº†å¯†é›†è§†é¢‘ç†è§£(Dense Video Understanding, DVU)çš„æ¦‚å¿µã€‚ä¸ºäº†è¯„ä¼°å¯†é›†æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è€…è®¾è®¡äº†é¦–ä¸ªä¸“é—¨çš„è¯„æµ‹åŸºå‡† DIVE (Dense Information Video Evaluation)ã€‚è®ºæ–‡æ ¸å¿ƒè´¡çŒ®æ˜¯æå‡ºäº† Gated Residual Tokenization (GRT) ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨é™ä½é«˜å¸§ç‡è§†é¢‘ç†è§£çš„è®¡ç®—å¼€é”€ã€‚ç¬¬ä¸€é˜¶æ®µ Motion-Compensated Inter-Gated Tokenization åˆ©ç”¨åƒç´ çº§è¿åŠ¨ä¼°è®¡è·³è¿‡é™æ€åŒºåŸŸï¼Œå®ç°äº† Token æ•°é‡çš„äºšçº¿æ€§å¢é•¿ï¼›ç¬¬äºŒé˜¶æ®µ Semantic-Scene Intra-Tokenization Merging åˆ™é€šè¿‡èåˆåœºæ™¯å†…çš„å†—ä½™ä»¤ç‰Œè¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒGRT åœ¨ DIVE åŸºå‡†ä¸Šä¼˜äºæ›´å¤§å‹çš„ VLLM åŸºçº¿æ¨¡å‹ï¼Œä¸”æ€§èƒ½éšå¸§ç‡(FPS)çš„æé«˜è€Œå¢å¼ºï¼Œä¸ºé«˜æ•ˆã€å¯æ‰©å±•çš„é«˜å¸§ç‡è§†é¢‘ç†è§£æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14199v2",
      "published_date": "2025-09-17 17:34:40 UTC",
      "updated_date": "2025-09-18 13:17:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:08:55.283218+00:00"
    },
    {
      "arxiv_id": "2509.14195v1",
      "title": "Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning",
      "title_zh": "è¿·å®«å¯¼èˆªçš„å±‚æ¬¡åŒ–å­¦ä¹ ï¼šåŸºäºäºŒé˜¶å­¦ä¹ çš„å¿ƒç†è¡¨å¾æ¶Œç°",
      "authors": [
        "Shalima Binta Manir",
        "Tim Oates"
      ],
      "abstract": "Mental representation, characterized by structured internal models mirroring external environments, is fundamental to advanced cognition but remains challenging to investigate empirically. Existing theory hypothesizes that second-order learning -- learning mechanisms that adapt first-order learning (i.e., learning about the task/domain) -- promotes the emergence of such environment-cognition isomorphism. In this paper, we empirically validate this hypothesis by proposing a hierarchical architecture comprising a Graph Convolutional Network (GCN) as a first-order learner and an MLP controller as a second-order learner. The GCN directly maps node-level features to predictions of optimal navigation paths, while the MLP dynamically adapts the GCN's parameters when confronting structurally novel maze environments. We demonstrate that second-order learning is particularly effective when the cognitive system develops an internal mental map structurally isomorphic to the environment. Quantitative and qualitative results highlight significant performance improvements and robust generalization on unseen maze tasks, providing empirical support for the pivotal role of structured mental representations in maximizing the effectiveness of second-order learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¿ƒç†è¡¨å¾(Mental representation)åœ¨é«˜çº§è®¤çŸ¥ä¸­çš„åŸºç¡€ä½œç”¨ï¼Œæ—¨åœ¨éªŒè¯äºŒé˜¶å­¦ä¹ (second-order learning)èƒ½ä¿ƒè¿›ç¯å¢ƒ-è®¤çŸ¥åŒæ„(environment-cognition isomorphism)çš„ç†è®ºå‡è®¾ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–æ¶æ„ï¼Œé‡‡ç”¨å›¾å·ç§¯ç½‘ç»œ(GCN)ä½œä¸ºæ˜ å°„å¯¼èˆªè·¯å¾„çš„ä¸€é˜¶å­¦ä¹ è€…ï¼Œå¹¶åˆ©ç”¨MLPæ§åˆ¶å™¨ä½œä¸ºäºŒé˜¶å­¦ä¹ è€…æ¥åŠ¨æ€è°ƒæ•´GCNå‚æ•°ä»¥åº”å¯¹æ–°å¼‚çš„è¿·å®«ç¯å¢ƒã€‚ç ”ç©¶è¯æ˜ï¼Œå½“è®¤çŸ¥ç³»ç»Ÿæ„å»ºå‡ºä¸å¤–éƒ¨ç¯å¢ƒç»“æ„åŒæ„çš„å†…éƒ¨å¿ƒç†åœ°å›¾æ—¶ï¼ŒäºŒé˜¶å­¦ä¹ çš„æ•ˆç‡ä¼šå¤§å¹…æå‡ã€‚å®šé‡ä¸å®šæ€§åˆ†ææ˜¾ç¤ºï¼Œè¯¥æ¶æ„åœ¨æœªè§è¿‡çš„è¿·å®«ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›å’Œé²æ£’çš„æ³›åŒ–è¡¨ç°ã€‚è¿™ä¸€å‘ç°ä¸ºç»“æ„åŒ–å¿ƒç†è¡¨å¾åœ¨æœ€å¤§åŒ–äºŒé˜¶å­¦ä¹ æ•ˆèƒ½ä¸­çš„æ ¸å¿ƒåœ°ä½æä¾›äº†æœ‰åŠ›çš„ç»éªŒè¯æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.14195v1",
      "published_date": "2025-09-17 17:30:58 UTC",
      "updated_date": "2025-09-17 17:30:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:08:51.360127+00:00"
    },
    {
      "arxiv_id": "2509.14181v2",
      "title": "Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting",
      "title_zh": "è¿æ¥è¿‡å»ä¸æœªæ¥ï¼šé¢å‘æ—¶é—´åºåˆ—é¢„æµ‹çš„åˆ†å¸ƒæ„ŸçŸ¥å¯¹é½",
      "authors": [
        "Yifan Hu",
        "Jie Yang",
        "Tian Zhou",
        "Peiyuan Liu",
        "Yujin Tang",
        "Rong Jin",
        "Liang Sun"
      ],
      "abstract": "Although contrastive and other representation-learning methods have long been explored in vision and NLP, their adoption in modern time series forecasters remains limited. We believe they hold strong promise for this domain. To unlock this potential, we explicitly align past and future representations, thereby bridging the distributional gap between input histories and future targets. To this end, we introduce TimeAlign, a lightweight, plug-and-play framework that establishes a new representation paradigm, distinct from contrastive learning, by aligning auxiliary features via a simple reconstruction task and feeding them back into any base forecaster. Extensive experiments across eight benchmarks verify its superior performance. Further studies indicate that the gains arise primarily from correcting frequency mismatches between historical inputs and future outputs. Additionally, we provide two theoretical justifications for how reconstruction improves forecasting generalization and how alignment increases the mutual information between learned representations and predicted targets. The code is available at https://github.com/TROUBADOUR000/TimeAlign.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TimeAlignï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä¸”å³æ’å³ç”¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ†å¸ƒæ„ŸçŸ¥å¯¹é½(Distribution-Aware Alignment)æ¥å¼¥åˆå†å²è¾“å…¥ä¸æœªæ¥ç›®æ ‡ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚ä¸åŒäºä¼ ç»Ÿçš„å¯¹æ¯”å­¦ä¹ ï¼ŒTimeAlignå»ºç«‹äº†ä¸€ç§æ–°çš„è¡¨ç¤ºèŒƒå¼ï¼Œé€šè¿‡ç®€å•çš„é‡æ„ä»»åŠ¡å¯¹é½è¾…åŠ©ç‰¹å¾ï¼Œå¹¶å°†å…¶åé¦ˆè‡³ä»»ä½•åŸºç¡€é¢„æµ‹æ¨¡å‹ä¸­ã€‚åœ¨å…«ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶çš„å“è¶Šæ€§èƒ½ï¼Œç ”ç©¶è¡¨æ˜å…¶å¢ç›Šä¸»è¦æºäºçº æ­£äº†å†å²è¾“å…¥ä¸æœªæ¥è¾“å‡ºä¹‹é—´çš„é¢‘ç‡ä¸åŒ¹é…(frequency mismatches)é—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†ä¸¤é¡¹ç†è®ºè¯æ˜ï¼Œé˜é‡Šäº†é‡æ„ä»»åŠ¡å¦‚ä½•æ”¹å–„é¢„æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠå¯¹é½æ“ä½œå¦‚ä½•å¢åŠ å­¦ä¹ è¡¨ç¤ºä¸é¢„æµ‹ç›®æ ‡ä¹‹é—´çš„äº’ä¿¡æ¯(Mutual Information)ã€‚è¯¥ç ”ç©¶ä¸ºæ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸçš„è¡¨ç¤ºå­¦ä¹ æä¾›äº†æ–°çš„è§†è§’å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14181v2",
      "published_date": "2025-09-17 17:12:39 UTC",
      "updated_date": "2025-09-21 17:18:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:08:52.966959+00:00"
    },
    {
      "arxiv_id": "2509.14180v1",
      "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs",
      "title_zh": "åˆæˆåŸºäºè¡Œä¸ºé‡‘èå­¦çš„æ¨ç†é“¾ï¼šé¢å‘ä¸ªäººç†è´¢å¤§è¯­è¨€æ¨¡å‹çš„æ•°æ®ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Akhil Theerthala"
      ],
      "abstract": "Personalized financial advice requires consideration of user goals, constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on support systems for investors and financial planners. Simultaneously, numerous recent studies examine broader personal finance tasks, including budgeting, debt management, retirement, and estate planning, through agentic pipelines that incur high maintenance costs, yielding less than 25% of their expected financial returns. In this study, we introduce a novel and reproducible framework that integrates relevant financial context with behavioral finance studies to construct supervision data for end-to-end advisors. Using this framework, we create a 19k sample reasoning dataset and conduct a comprehensive fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test split and a blind LLM-jury study, we demonstrate that through careful data curation and behavioral integration, our 8B model achieves performance comparable to significantly larger baselines (14-32B parameters) across factual accuracy, fluency, and personalization metrics while incurring 80% lower costs than the larger counterparts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸ªæ€§åŒ–è´¢åŠ¡å»ºè®®ä¸­ä¼ ç»Ÿæ™ºèƒ½ä½“æµæ°´çº¿(agentic pipelines)ç»´æŠ¤æˆæœ¬é«˜ä¸”æ”¶ç›Šç‡ä½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹ä¸”å¯å¤åˆ¶çš„æ•°æ®ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç›¸å…³çš„é‡‘èèƒŒæ™¯ä¸è¡Œä¸ºé‡‘èå­¦(behavioral finance)ç ”ç©¶ç›¸ç»“åˆï¼Œæ„å»ºäº†ç”¨äºç«¯åˆ°ç«¯å’¨è¯¢å™¨çš„æ¨ç†ç›‘ç£æ•°æ®ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«1.9ä¸‡ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œå¹¶å¯¹Qwen-3-8Bæ¨¡å‹è¿›è¡Œäº†æ·±åº¦å¾®è°ƒã€‚å®éªŒç»“æœé€šè¿‡ç•™å‡ºæµ‹è¯•é›†å’Œç›²å®¡LLM-juryç ”ç©¶è¯æ˜ï¼Œè¯¥8Bå‚æ•°æ¨¡å‹åœ¨äº‹å®å‡†ç¡®æ€§ã€æµåˆ©åº¦å’Œä¸ªæ€§åŒ–æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†ä¸14-32Bè§„æ¨¡åŸºçº¿æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œè¿è¡Œæˆæœ¬æ¯”å¤§å‹æ¨¡å‹é™ä½äº†80%ã€‚è¿™ä¸€ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç²¾å¿ƒçš„æ•°æ®ç­–åˆ’å’Œè¡Œä¸ºæ•´åˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸ªäººè´¢åŠ¡é¢†åŸŸå¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 11 figures. The paper presents a novel framework for generating a personal finance dataset. The resulting fine-tuned model and dataset are publicly available",
      "pdf_url": "https://arxiv.org/pdf/2509.14180v1",
      "published_date": "2025-09-17 17:12:38 UTC",
      "updated_date": "2025-09-17 17:12:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:11.157290+00:00"
    },
    {
      "arxiv_id": "2509.14172v2",
      "title": "TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning",
      "title_zh": "TGPOï¼šé¢å‘é²æ£’ Web æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„æ ‘å¼•å¯¼åå¥½ä¼˜åŒ–",
      "authors": [
        "Ziyuan Chen",
        "Zhenghui Zhao",
        "Zhangye Han",
        "Miancan Liu",
        "Xianhang Ye",
        "Yiqing Li",
        "Hongbo Min",
        "Jinkui Ren",
        "Xiantao Zhang",
        "Guitao Cao"
      ],
      "abstract": "With the rapid advancement of large language models and vision-language models, employing large models as Web Agents has become essential for automated web interaction. However, training Web Agents with reinforcement learning faces critical challenges including credit assignment misallocation, prohibitively high annotation costs, and reward sparsity. To address these issues, we propose Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning framework that proposes a tree-structured trajectory representation merging semantically identical states across trajectories to eliminate label conflicts. Our framework incorporates a Process Reward Model that automatically generates fine-grained rewards through subgoal progress, redundancy detection, and action verification. Additionally, a dynamic weighting mechanism prioritizes high-impact decision points during training. Experiments on Online-Mind2Web and our self-constructed C-WebShop datasets demonstrate that TGPO significantly outperforms existing methods, achieving higher success rates with fewer redundant steps.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Tree-Guided Preference Optimization (TGPO)ï¼Œä¸€ç§æ—¨åœ¨è§£å†³ Web Agent å¼ºåŒ–å­¦ä¹ ä¸­ä¿¡ç”¨åˆ†é…ä¸å½“ã€æ ‡æ³¨æˆæœ¬é«˜åŠå¥–åŠ±ç¨€ç–ç­‰é—®é¢˜çš„ç¦»çº¿å­¦ä¹ æ¡†æ¶ã€‚TGPO é‡‡ç”¨æ ‘çŠ¶ç»“æ„çš„è½¨è¿¹è¡¨ç¤ºæ¥åˆå¹¶è¯­ä¹‰ç›¸åŒçš„çŠ¶æ€ï¼Œä»è€Œæœ‰æ•ˆæ¶ˆé™¤æ ‡ç­¾å†²çªå¹¶ä¼˜åŒ–è½¨è¿¹è¡¨è¾¾ã€‚æ¡†æ¶å†…ç½®çš„ Process Reward Model èƒ½å¤Ÿé€šè¿‡å­ç›®æ ‡è¿›åº¦ã€å†—ä½™æ£€æµ‹å’ŒåŠ¨ä½œéªŒè¯è‡ªåŠ¨ç”Ÿæˆç»†ç²’åº¦å¥–åŠ±ï¼Œå¹¶ç»“åˆåŠ¨æ€æƒé‡æœºåˆ¶ä¼˜å…ˆå¤„ç†é«˜å½±å“åŠ›çš„å†³ç­–ç‚¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTGPO åœ¨ Online-Mind2Web å’Œ C-WebShop æ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æå‡ä»»åŠ¡æˆåŠŸç‡çš„åŒæ—¶æœ‰æ•ˆå‡å°‘äº†å†—ä½™æ“ä½œæ­¥éª¤ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘é«˜æ•ˆã€é²æ£’çš„è‡ªåŠ¨åŒ–ç½‘é¡µäº¤äº’æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘å’Œå®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14172v2",
      "published_date": "2025-09-17 16:58:44 UTC",
      "updated_date": "2025-09-19 02:13:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:09.961024+00:00"
    },
    {
      "arxiv_id": "2509.14165v1",
      "title": "Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions",
      "title_zh": "Tokenæµå‘ä½•å¤„ï¼Ÿæ·±å…¥è§£æé«˜åˆ†è¾¨ç‡ä¸‹ STEP çš„å‰ªæè¡Œä¸º",
      "authors": [
        "Michal Szczepanski",
        "Martyna Poreba",
        "Karim Haroun"
      ],
      "abstract": "Vision Transformers (ViTs) achieve state-of-the-art performance in semantic segmentation but are hindered by high computational and memory costs. To address this, we propose STEP (SuperToken and Early-Pruning), a hybrid token-reduction framework that combines dynamic patch merging and token pruning to enhance efficiency without significantly compromising accuracy. At the core of STEP is dCTS, a lightweight CNN-based policy network that enables flexible merging into superpatches. Encoder blocks integrate also early-exits to remove high-confident supertokens, lowering computational load. We evaluate our method on high-resolution semantic segmentation benchmarks, including images up to 1024 x 1024, and show that when dCTS is applied alone, the token count can be reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase in throughput when using ViT-Large as the backbone. Applying the full STEP framework further improves efficiency, reaching up to a 4x reduction in computational complexity and a 1.7x gain in inference speed, with a maximum accuracy drop of no more than 2.0%. With the proposed STEP configurations, up to 40% of tokens can be confidently predicted and halted before reaching the final encoder layer.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision Transformers (ViTs) åœ¨é«˜åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²ä¸­é¢ä¸´çš„é«˜æ˜‚è®¡ç®—ä¸å†…å­˜æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º STEP (SuperToken and Early-Pruning) çš„æ··åˆä»¤ç‰Œå‰Šå‡æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯è½»é‡çº§ CNN ç­–ç•¥ç½‘ç»œ dCTSï¼Œæ”¯æŒå°†è¡¥ä¸åŠ¨æ€åˆå¹¶ä¸º superpatchesï¼Œå¹¶ç»“åˆ Encoder blocks ä¸­çš„æ—©æœŸé€€å‡ºæœºåˆ¶ (early-exits) å‰”é™¤é«˜ç½®ä¿¡åº¦ä»¤ç‰Œã€‚åœ¨ 1024 x 1024 é«˜åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æˆåŠŸå°†ä»¤ç‰Œæ•°é‡å‡å°‘ 2.5 å€ï¼Œä½¿ ViT-Large åç«¯çš„ throughput æå‡äº† 3.4 å€ã€‚å®éªŒè¡¨æ˜ï¼Œå®Œæ•´ STEP æ¡†æ¶å¯å®ç° 4 å€çš„è®¡ç®—å¤æ‚åº¦å‰Šå‡å’Œ 1.7 å€çš„æ¨ç†é€Ÿåº¦å¢ç›Šï¼Œä¸”å‡†ç¡®ç‡ä¸‹é™å¹…åº¦ä¸è¶…è¿‡ 2.0%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°çº¦ 40% çš„ä»¤ç‰Œåœ¨åˆ°è¾¾æœ€ç»ˆç¼–ç å±‚å‰å³å¯å®Œæˆé¢„æµ‹å¹¶åœæ­¢è®¡ç®—ï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—ä¼˜åŒ–äº†é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†æ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14165v1",
      "published_date": "2025-09-17 16:48:00 UTC",
      "updated_date": "2025-09-17 16:48:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:19.591567+00:00"
    },
    {
      "arxiv_id": "2509.15239v2",
      "title": "KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems",
      "title_zh": "KNARsackï¼šæ•™ç¥ç»ç®—æ³•æ¨ç†å™¨æ±‚è§£ä¼ªå¤šé¡¹å¼é—®é¢˜",
      "authors": [
        "Stjepan PoÅ¾gaj",
        "Dobrik Georgiev",
        "Marin Å iliÄ‡",
        "Petar VeliÄkoviÄ‡"
      ],
      "abstract": "Neural algorithmic reasoning (NAR) is a growing field that aims to embed algorithmic logic into neural networks by imitating classical algorithms. In this extended abstract, we detail our attempt to build a neural algorithmic reasoner that can solve Knapsack, a pseudo-polynomial problem bridging classical algorithms and combinatorial optimisation, but omitted in standard NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow the two-phase pipeline for the Knapsack problem, which involves first constructing the dynamic programming table and then reconstructing the solution from it. The approach, which models intermediate states through dynamic programming supervision, achieves better generalization to larger problem instances than a direct-prediction baseline that attempts to select the optimal subset only from the problem inputs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Neural Algorithmic Reasoning (NAR) é¢†åŸŸï¼Œæ—¨åœ¨è§£å†³ç»å…¸ç®—æ³•ä¸ç»„åˆä¼˜åŒ–ä¹‹é—´çš„å…³é”®æ¡¥æ¢â€”â€”Knapsacké—®é¢˜ï¼Œè¯¥é—®é¢˜ä½œä¸ºä¸€ç§pseudo-polynomialé—®é¢˜åœ¨æ ‡å‡†NARåŸºå‡†æµ‹è¯•ä¸­å¸¸è¢«å¿½ç•¥ã€‚ä½œè€…æå‡ºäº†KNARsackï¼Œè®¾è®¡äº†ä¸€ç§ç´§å¯†éµå¾ªç»å…¸ç®—æ³•é€»è¾‘çš„ä¸¤é˜¶æ®µæµæ°´çº¿æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é¦–å…ˆæ„å»ºdynamic programming (DP) è¡¨ï¼Œéšåä»ä¸­é‡å»ºæœ€ä¼˜è§£ï¼Œå¹¶åˆ©ç”¨ä¸­é—´çŠ¶æ€çš„dynamic programming supervisionæ¥æ¨¡æ‹Ÿç®—æ³•æ‰§è¡Œé€»è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç›´æ¥ä»è¾“å…¥é¢„æµ‹æœ€ä¼˜å­é›†çš„åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œè¿™ç§å»ºæ¨¡ä¸­é—´çŠ¶æ€çš„æ–¹æ³•åœ¨å¤„ç†æ›´å¤§è§„æ¨¡çš„é—®é¢˜å®ä¾‹æ—¶å±•ç°å‡ºæ›´ä¼˜çš„generalizationèƒ½åŠ›ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†é€šè¿‡åµŒå…¥ç»å…¸ç®—æ³•é€»è¾‘ï¼Œç¥ç»ç½‘ç»œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£å†³å¤æ‚çš„ä¼ªå¤šé¡¹å¼æ—¶é—´ç®—æ³•é—®é¢˜ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 10 figures, 5 tables, 3 listings",
      "pdf_url": "https://arxiv.org/pdf/2509.15239v2",
      "published_date": "2025-09-17 15:44:25 UTC",
      "updated_date": "2025-12-05 15:57:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:17.284420+00:00"
    },
    {
      "arxiv_id": "2509.15238v1",
      "title": "Generating Plans for Belief-Desire-Intention (BDI) Agents Using Alternating-Time Temporal Logic (ATL)",
      "title_zh": "åˆ©ç”¨äº¤æ›¿æ—¶é—´æ—¶åºé€»è¾‘ (ATL) ä¸ºä¿¡å¿µ-æ¬²æœ›-æ„å›¾ (BDI) æ™ºèƒ½ä½“ç”Ÿæˆè§„åˆ’",
      "authors": [
        "Dylan LÃ©veillÃ©"
      ],
      "abstract": "Belief-Desire-Intention (BDI) is a framework for modelling agents based on their beliefs, desires, and intentions. Plans are a central component of BDI agents, and define sequences of actions that an agent must undertake to achieve a certain goal. Existing approaches to plan generation often require significant manual effort, and are mainly focused on single-agent systems. As a result, in this work, we have developed a tool that automatically generates BDI plans using Alternating-Time Temporal Logic (ATL). By using ATL, the plans generated accommodate for possible competition or cooperation between the agents in the system. We demonstrate the effectiveness of the tool by generating plans for an illustrative game that requires agent collaboration to achieve a shared goal. We show that the generated plans allow the agents to successfully attain this goal.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Belief-Desire-Intention (BDI) æ™ºèƒ½ä½“æ¡†æ¶ä¸­çš„è®¡åˆ’ç”Ÿæˆé—®é¢˜ï¼Œé’ˆå¯¹ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–äººå·¥ä¸”ä¸»è¦å±€é™äºå•æ™ºèƒ½ä½“ç³»ç»Ÿçš„ç°çŠ¶ï¼Œå¼€å‘äº†ä¸€ç§åˆ©ç”¨Alternating-Time Temporal Logic (ATL) è‡ªåŠ¨ç”Ÿæˆ BDI è®¡åˆ’çš„å·¥å…·ã€‚é€šè¿‡ä½¿ç”¨ ATLï¼Œç”Ÿæˆçš„è®¡åˆ’èƒ½å¤Ÿå……åˆ†è€ƒè™‘åˆ°ç³»ç»Ÿä¸­æ™ºèƒ½ä½“ä¹‹é—´å¯èƒ½å­˜åœ¨çš„ç«äº‰æˆ–åˆä½œå…³ç³»ã€‚ç ”ç©¶é€šè¿‡ä¸€ä¸ªéœ€è¦æ™ºèƒ½ä½“åä½œä»¥å®ç°å…±åŒç›®æ ‡çš„ç¤ºä¾‹æ¸¸æˆæ¼”ç¤ºäº†è¯¥å·¥å…·çš„æœ‰æ•ˆæ€§ã€‚å®éªŒè¯æ˜ï¼Œç”Ÿæˆçš„è®¡åˆ’èƒ½å¤Ÿå¼•å¯¼æ™ºèƒ½ä½“åœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸‹æˆåŠŸè¾¾æˆç›®æ ‡ã€‚è¯¥æˆæœä¸ºè‡ªåŠ¨åŒ–ç”Ÿæˆå…·æœ‰é€»è¾‘ä¿è¯çš„å¤šæ™ºèƒ½ä½“è¡Œä¸ºè®¡åˆ’æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.MA",
      "comment": "In Proceedings GandALF 2025, arXiv:2509.13258",
      "pdf_url": "https://arxiv.org/pdf/2509.15238v1",
      "published_date": "2025-09-17 15:34:02 UTC",
      "updated_date": "2025-09-17 15:34:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:23.893595+00:00"
    },
    {
      "arxiv_id": "2509.14093v1",
      "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework",
      "title_zh": "åŸºäºè‡ªé€‚åº”æ€ç»´é“¾å‹ç¼©çš„é«˜æ•ˆæ¨ç†ï¼šä¸€ç§è‡ªä¼˜åŒ–æ¡†æ¶",
      "authors": [
        "Kerui Huang",
        "Shuhan Liu",
        "Xing Hu",
        "Tongtong Xu",
        "Lingfeng Bao",
        "Xin Xia"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by prompting intermediate steps, improving accuracy and robustness in arithmetic, logic, and commonsense tasks. However, this benefit comes with high computational costs: longer outputs increase latency, memory usage, and KV-cache demands. These issues are especially critical in software engineering tasks where concise and deterministic outputs are required. To investigate these trade-offs, we conduct an empirical study based on code generation benchmarks. The results reveal that longer CoT does not always help. Excessive reasoning often causes truncation, accuracy drops, and latency up to five times higher, with failed outputs consistently longer than successful ones. These findings challenge the assumption that longer reasoning is inherently better and highlight the need for adaptive CoT control. Motivated by this, we propose SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with task-aware adaptive filtering, dynamically adjusting thresholds based on pre-inference outputs to reduce verbosity and computational overhead. We then evaluate SEER on three software engineering tasks and one math task. On average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation, and eliminates most infinite loops. These results demonstrate SEER as a practical method to make CoT-enhanced LLMs more efficient and robust, even under resource constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)æ€§èƒ½æ—¶å¸¦æ¥çš„è®¡ç®—å¼€é”€ã€å»¶è¿ŸåŠKV-cacheå‹åŠ›ç­‰æŒ‘æˆ˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚å®è¯åˆ†æè¡¨æ˜ï¼Œè¿‡é•¿çš„æ¨ç†è¿‡ç¨‹å¾€å¾€ä¼šå¯¼è‡´ä»»åŠ¡æˆªæ–­å’Œå‡†ç¡®ç‡ä¸‹é™ï¼ŒæŒ‘æˆ˜äº†â€œæ¨ç†è¶Šé•¿è¶Šå¥½â€çš„ä¼ ç»Ÿå‡è®¾ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æå‡ºäº†åä¸ºSEER (Self-Enhancing Efficient Reasoning)çš„è‡ªé€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å‹ç¼©CoTæ¥æå‡æ¨ç†æ•ˆç‡ã€‚SEERç»“åˆäº†Best-of-Né‡‡æ ·ä¸ä»»åŠ¡æ„ŸçŸ¥è‡ªé€‚åº”è¿‡æ»¤æŠ€æœ¯ï¼Œèƒ½å¤ŸåŠ¨æ€è°ƒæ•´é˜ˆå€¼ä»¥å‡å°‘å†—ä½™è¾“å‡ºã€‚åœ¨è½¯ä»¶å·¥ç¨‹åŠæ•°å­¦ä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼ŒSEERæˆåŠŸå°†CoTç¼©çŸ­äº†42.1%ï¼Œå¹¶åœ¨æå‡å‡†ç¡®ç‡çš„åŒæ—¶æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14093v1",
      "published_date": "2025-09-17 15:33:44 UTC",
      "updated_date": "2025-09-17 15:33:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:26.361276+00:00"
    },
    {
      "arxiv_id": "2509.14061v1",
      "title": "Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing",
      "title_zh": "åŸºäºç¯å¢ƒä¼ æ„Ÿå™¨èåˆçš„é¢å‘ä½åŠŸè€—è¾¹ç¼˜è®¡ç®—èœ‚ç¾¤èœ‚ç‹æ£€æµ‹",
      "authors": [
        "Chiara De Luca",
        "Elisa Donati"
      ],
      "abstract": "Queen bee presence is essential for the health and stability of honeybee colonies, yet current monitoring methods rely on manual inspections that are labor-intensive, disruptive, and impractical for large-scale beekeeping. While recent audio-based approaches have shown promise, they often require high power consumption, complex preprocessing, and are susceptible to ambient noise. To overcome these limitations, we propose a lightweight, multimodal system for queen detection based on environmental sensor fusion-specifically, temperature, humidity, and pressure differentials between the inside and outside of the hive. Our approach employs quantized decision tree inference on a commercial STM32 microcontroller, enabling real-time, low-power edge computing without compromising accuracy. We show that our system achieves over 99% queen detection accuracy using only environmental inputs, with audio features offering no significant performance gain. This work presents a scalable and sustainable solution for non-invasive hive monitoring, paving the way for autonomous, precision beekeeping using off-the-shelf, energy-efficient hardware.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿäººå·¥æ£€æŸ¥èœ‚ç‹è´¹æ—¶è´¹åŠ›ä¸”ç ´åæ€§å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¯å¢ƒä¼ æ„Ÿå™¨èåˆï¼ˆenvironmental sensor fusionï¼‰çš„è½»é‡çº§å¤šæ¨¡æ€ç³»ç»Ÿï¼Œé€šè¿‡ç›‘æµ‹èœ‚ç®±å†…å¤–çš„æ¸©åº¦ã€æ¹¿åº¦å’Œå‹åŠ›å·®æ¥å®æ—¶æ£€æµ‹èœ‚ç‹çš„å­˜åœ¨ã€‚è¯¥æ–¹æ¡ˆåœ¨å•†ç”¨ STM32 å¾®æ§åˆ¶å™¨ä¸Šå®ç°äº†é‡åŒ–å†³ç­–æ ‘æ¨ç†ï¼ˆquantized decision tree inferenceï¼‰ï¼Œæ»¡è¶³äº†ä½åŠŸè€—è¾¹ç¼˜è®¡ç®—ï¼ˆedge computingï¼‰çš„éœ€æ±‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿä»…ä¾é ç¯å¢ƒè¾“å…¥å³å¯è¾¾åˆ°è¶…è¿‡ 99% çš„æ£€æµ‹å‡†ç¡®ç‡ï¼Œä¸”ç ”ç©¶å‘ç°éŸ³é¢‘ç‰¹å¾ï¼ˆaudio featuresï¼‰å¹¶ä¸èƒ½æ˜¾è‘—æå‡ç³»ç»Ÿæ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºéä¾µå…¥å¼èœ‚ç®±ç›‘æµ‹æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å¯æŒç»­çš„è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨ç°æˆçš„ä½èƒ½è€—ç¡¬ä»¶æ¨åŠ¨äº†è‡ªä¸»ç²¾å‡†å…»èœ‚ï¼ˆprecision beekeepingï¼‰æŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14061v1",
      "published_date": "2025-09-17 15:05:15 UTC",
      "updated_date": "2025-09-17 15:05:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:28.042637+00:00"
    },
    {
      "arxiv_id": "2509.14057v7",
      "title": "Navigating the safe harbor paradox in human-machine systems",
      "title_zh": "æ¢ç©¶äººæœºç³»ç»Ÿä¸­çš„é¿é£æ¸¯æ‚–è®º",
      "authors": [
        "Riccardo Zanardelli"
      ],
      "abstract": "When deploying artificial skills, decision-makers often assume that layering human oversight is a safe harbor that mitigates the risks of full automation in high-complexity tasks. This paper formally challenges the economic validity of this widespread assumption, arguing that the true bottom-line economic utility of a human-machine skill policy is highly contingent on situational and design factors. To investigate this gap, we develop an in-silico exploratory framework for policy analysis based on Monte Carlo simulations to quantify the economic impact of skill policies in the execution of tasks presenting varying levels of complexity across diverse setups. Our results show that in complex scenarios, a human-machine strategy can yield the highest economic utility, but only if genuine augmentation is achieved. In contrast, when failing to realize this synergy, the human-machine approach can perform worse than either the machine-exclusive or the human-exclusive policy, actively destroying value under the pressure of costs that are not sufficiently compensated by performance gains. This finding points to a key implication for decision-makers: when the context is complex and critical, simply allocating human and machine skills to a task may be insufficient, and far from being a silver-bullet solution or a low-risk compromise. Rather, it is a critical opportunity to boost competitiveness that demands a strong organizational commitment to enabling augmentation. Also, our findings show that improving the cost-effectiveness of machine skills over time, while useful, does not replace the fundamental need to focus on achieving augmentation when surprise is the norm, even when machines become more effective than humans in handling uncertainty.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­£å¼æŒ‘æˆ˜äº†åœ¨å¤æ‚ä»»åŠ¡ä¸­å¼•å…¥äººç±»ç›‘ç£ä½œä¸ºâ€œé¿é£æ¸¯â€(safe harbor)ä»¥é™ä½å…¨è‡ªåŠ¨åŒ–é£é™©çš„æ™®éå‡è®¾ï¼ŒæŒ‡å‡ºäººæœºæŠ€èƒ½ç­–ç•¥çš„ç»æµæ•ˆç”¨é«˜åº¦å–å†³äºæƒ…å¢ƒä¸è®¾è®¡å› ç´ ã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªåŸºäºè’™ç‰¹å¡ç½—æ¨¡æ‹Ÿ(Monte Carlo simulations)çš„in-silicoæ¢ç´¢æ€§æ¡†æ¶ï¼Œç”¨äºé‡åŒ–ä¸åŒå¤æ‚åº¦ä»»åŠ¡ä¸‹äººæœºæŠ€èƒ½ç­–ç•¥çš„ç»æµå½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œäººæœºåä½œç­–ç•¥ä»…åœ¨å®ç°çœŸæ­£çš„å¢å¼º(augmentation)æ—¶æ‰èƒ½äº§ç”Ÿæœ€é«˜çš„ç»æµæ•ˆç›Šï¼Œå¦åˆ™å…¶è¡¨ç°å¯èƒ½å› æˆæœ¬æ— æ³•è¢«æ€§èƒ½å¢ç›ŠæŠµæ¶ˆè€Œå·®äºçº¯æœºå™¨æˆ–çº¯äººç±»ç­–ç•¥ã€‚è¿™ä¸€å‘ç°æŒ‡å‡ºï¼Œåœ¨å¤æ‚å…³é”®èƒŒæ™¯ä¸‹ï¼Œç®€å•åˆ†é…äººæœºæŠ€èƒ½å¹¶éâ€œé“¶å¼¹â€(silver-bullet)æˆ–ä½é£é™©çš„æŠ˜ä¸­æ–¹æ¡ˆï¼Œè€Œæ˜¯éœ€è¦ç»„ç»‡è‡´åŠ›äºå®ç°ååŒå¢æ•ˆçš„æˆ˜ç•¥æœºä¼šã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼ºè°ƒæé«˜æœºå™¨çš„æˆæœ¬æ•ˆç›Šå¹¶ä¸èƒ½å–ä»£åœ¨é¢å¯¹ä¸ç¡®å®šæ€§æ—¶å®ç°å¢å¼º(augmentation)çš„æ ¹æœ¬éœ€æ±‚ã€‚",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "comment": "Rework of the title based on an improved framing (safe harbor paradox); results unchanged; conclusions unchanged",
      "pdf_url": "https://arxiv.org/pdf/2509.14057v7",
      "published_date": "2025-09-17 15:03:39 UTC",
      "updated_date": "2026-01-02 15:33:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:32.981802+00:00"
    },
    {
      "arxiv_id": "2509.14049v2",
      "title": "Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices",
      "title_zh": "èµ„æºå—é™è®¾å¤‡ä¸ŠåŸºäº CNN çš„éŸ³é¢‘æ ‡è®°æ¨¡å‹ç»¼åˆè¯„ä¼°",
      "authors": [
        "Jordi Grau-Haro",
        "Ruben Ribes-Serrano",
        "Javier Naranjo-Alcazar",
        "Marta Garcia-Ballesteros",
        "Pedro Zuccarello"
      ],
      "abstract": "Convolutional Neural Networks (CNNs) have demonstrated exceptional performance in audio tagging tasks. However, deploying these models on resource-constrained devices like the Raspberry Pi poses challenges related to computational efficiency and thermal management. In this paper, a comprehensive evaluation of multiple convolutional neural network (CNN) architectures for audio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D models from the Pretrained Audio Neural Networks (PANNs) framework, a ConvNeXt-based model adapted for audio classification, as well as MobileNetV3 architectures. In addition, two PANNs-derived networks, CNN9 and CNN13, recently proposed, are also evaluated. To enhance deployment efficiency and portability across diverse hardware platforms, all models are converted to the Open Neural Network Exchange (ONNX) format. Unlike previous works that focus on a single model, our analysis encompasses a broader range of architectures and involves continuous 24-hour inference sessions to assess performance stability. Our experiments reveal that, with appropriate model selection and optimization, it is possible to maintain consistent inference latency and manage thermal behavior effectively over extended periods. These findings provide valuable insights for deploying audio tagging models in real-world edge computing scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Raspberry Pi ç­‰èµ„æºå—é™è®¾å¤‡ï¼Œå¯¹åŸºäº Convolutional Neural Networks (CNN) çš„éŸ³é¢‘æ ‡æ³¨ (Audio Tagging) æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚è¯„ä¼°å¯¹è±¡æ¶µç›–äº† Pretrained Audio Neural Networks (PANNs) çš„æ‰€æœ‰ 1D å’Œ 2D æ¨¡å‹ã€åŸºäº ConvNeXt æ”¹è¿›çš„æ¨¡å‹ã€MobileNetV3 æ¶æ„ä»¥åŠæ–°æå‡ºçš„ CNN9 å’Œ CNN13 ç½‘ç»œã€‚ä¸ºäº†æå‡åœ¨å¤šæ ·åŒ–ç¡¬ä»¶å¹³å°ä¸Šçš„éƒ¨ç½²æ•ˆç‡å’Œå¯ç§»æ¤æ€§ï¼Œç ”ç©¶å°†æ‰€æœ‰æ¨¡å‹è½¬æ¢ä¸ºäº† Open Neural Network Exchange (ONNX) æ ¼å¼ã€‚ä¸ä»¥å¾€ä»…å…³æ³¨å•ä¸€æ¨¡å‹çš„ç ”ç©¶ä¸åŒï¼Œè¯¥å®éªŒé€šè¿‡ 24 å°æ—¶è¿ç»­æ¨ç†ä¼šè¯ï¼Œé‡ç‚¹è€ƒå¯Ÿäº†æ¨¡å‹çš„æ€§èƒ½ç¨³å®šæ€§ã€è®¡ç®—æ•ˆç‡åŠçƒ­ç®¡ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼Œé€šè¿‡åˆç†çš„æ¨¡å‹é€‰æ‹©ä¸ä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨é•¿æ—¶é—´è¿è¡Œä¸­ä¿æŒç¨³å®šçš„æ¨ç†å»¶è¿Ÿ (Inference Latency) å¹¶æœ‰æ•ˆæ§åˆ¶è®¾å¤‡æ¸©åº¦ã€‚è¿™äº›ç ”ç©¶å‘ç°ä¸ºåœ¨çœŸå®è¾¹ç¼˜è®¡ç®— (Edge Computing) åœºæ™¯ä¸­é«˜æ•ˆéƒ¨ç½²éŸ³é¢‘æ ‡æ³¨æ¨¡å‹æä¾›äº†é‡è¦çš„å®è·µå‚è€ƒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at Computing Conference 2026, London, UK",
      "pdf_url": "https://arxiv.org/pdf/2509.14049v2",
      "published_date": "2025-09-17 14:53:56 UTC",
      "updated_date": "2025-09-19 10:37:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:36.361726+00:00"
    },
    {
      "arxiv_id": "2509.14040v1",
      "title": "Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning",
      "title_zh": "Prompt2Autoï¼šåŸºäºå‡ ä½•ä¸å˜å•æ ·æœ¬é«˜æ–¯è¿‡ç¨‹å­¦ä¹ ï¼Œå®ç°ä»è¿åŠ¨æç¤ºåˆ°è‡ªåŠ¨åŒ–æ§åˆ¶",
      "authors": [
        "Zewen Yang",
        "Xiaobing Dai",
        "Dongfa Zhang",
        "Yu Li",
        "Ziyang Meng",
        "Bingkun Huang",
        "Hamid Sadeghian",
        "Sami Haddadin"
      ],
      "abstract": "Learning from demonstration allows robots to acquire complex skills from human demonstrations, but conventional approaches often require large datasets and fail to generalize across coordinate transformations. In this paper, we propose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP) learning framework that enables robots to perform human-guided automated control from a single motion prompt. A dataset-construction strategy based on coordinate transformations is introduced that enforces invariance to translation, rotation, and scaling, while supporting multi-step predictions. Moreover, GeoGP is robust to variations in the user's motion prompt and supports multi-skill autonomy. We validate the proposed approach through numerical simulations with the designed user graphical interface and two real-world robotic experiments, which demonstrate that the proposed method is effective, generalizes across tasks, and significantly reduces the demonstration burden. Project page is available at: https://prompt2auto.github.io",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Prompt2Autoï¼Œè¿™æ˜¯ä¸€ç§å‡ ä½•ä¸å˜çš„å•æ¬¡é«˜æ–¯è¿‡ç¨‹ï¼ˆGeoGPï¼‰å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ä»å•ä¸ªè¿åŠ¨æç¤ºï¼ˆmotion promptï¼‰åˆ°æœºå™¨äººè‡ªåŠ¨åŒ–æ§åˆ¶çš„è½¬åŒ–ã€‚é’ˆå¯¹ä¼ ç»Ÿç¤ºèŒƒå­¦ä¹ æ–¹æ³•ä¾èµ–å¤§è§„æ¨¡æ•°æ®é›†ä¸”éš¾ä»¥è·¨åæ ‡å˜æ¢æ³›åŒ–çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åŸºäºåæ ‡å˜æ¢çš„æ•°æ®é›†æ„å»ºç­–ç•¥ï¼Œä»è€Œç¡®ä¿äº†å¯¹å¹³ç§»ã€æ—‹è½¬å’Œç¼©æ”¾çš„å‡ ä½•ä¸å˜æ€§ã€‚æ­¤å¤–ï¼ŒGeoGP æ”¯æŒå¤šæ­¥é¢„æµ‹å’Œå¤šæŠ€èƒ½è‡ªä¸»æ€§ï¼Œå¹¶å¯¹ç”¨æˆ·è¿åŠ¨æç¤ºçš„ç»†å¾®å˜åŒ–å…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ã€‚é€šè¿‡æ•°å€¼æ¨¡æ‹Ÿå’Œä¸¤é¡¹çœŸå®ä¸–ç•Œæœºå™¨äººå®éªŒçš„éªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ¨å¹¿åˆ°ä¸åŒä»»åŠ¡ä¸­ï¼Œå¹¶æ˜¾è‘—å‡è½»äº†äººå·¥ç¤ºèŒƒçš„è´Ÿæ‹…ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14040v1",
      "published_date": "2025-09-17 14:42:18 UTC",
      "updated_date": "2025-09-17 14:42:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:35.781976+00:00"
    },
    {
      "arxiv_id": "2509.14037v1",
      "title": "PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction",
      "title_zh": "PhenoGnetï¼šä¸€ç§ç”¨äºç–¾ç—…ç›¸ä¼¼æ€§é¢„æµ‹çš„å›¾å¯¹æ¯”å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Ranga Baminiwatte",
        "Kazi Jewel Rana",
        "Aaron J. Masino"
      ],
      "abstract": "Understanding disease similarity is critical for advancing diagnostics, drug discovery, and personalized treatment strategies. We present PhenoGnet, a novel graph-based contrastive learning framework designed to predict disease similarity by integrating gene functional interaction networks with the Human Phenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-view model that separately encodes gene and phenotype graphs using Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), and a cross view model implemented as a shared weight multilayer perceptron (MLP) that aligns gene and phenotype embeddings through contrastive learning. The model is trained using known gene phenotype associations as positive pairs and randomly sampled unrelated pairs as negatives. Diseases are represented by the mean embeddings of their associated genes and/or phenotypes, and pairwise similarity is computed via cosine similarity. Evaluation on a curated benchmark of 1,100 similar and 866 dissimilar disease pairs demonstrates strong performance, with gene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764, outperforming existing state of the art methods. Notably, PhenoGnet captures latent biological relationships beyond direct overlap, offering a scalable and interpretable solution for disease similarity prediction. These results underscore its potential for enabling downstream applications in rare disease research and precision medicine.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PhenoGnetï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå›¾çš„å¯¹æ¯”å­¦ä¹ (Graph-Based Contrastive Learning)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆåŸºå› åŠŸèƒ½ç›¸äº’ä½œç”¨ç½‘ç»œä¸äººç±»è¡¨å‹æœ¬ä½“(Human Phenotype Ontology, HPO)æ¥é¢„æµ‹ç–¾ç—…ç›¸ä¼¼æ€§ã€‚PhenoGnetç”±è§†å›¾å†…æ¨¡å‹(Intra-view model)å’Œè·¨è§†å›¾æ¨¡å‹(Cross-view model)ç»„æˆï¼Œå‰è€…åˆ©ç”¨å›¾å·ç§¯ç½‘ç»œ(GCN)å’Œå›¾æ³¨æ„åŠ›ç½‘ç»œ(GAT)åˆ†åˆ«ç¼–ç åŸºå› ä¸è¡¨å‹å›¾ï¼Œåè€…åˆ™é€šè¿‡å¯¹æ¯”å­¦ä¹ å®ç°åµŒå…¥å¯¹é½ã€‚æ¨¡å‹åˆ©ç”¨å·²çŸ¥çš„åŸºå› -è¡¨å‹å…³è”ä½œä¸ºæ­£æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨ä½™å¼¦ç›¸ä¼¼åº¦(Cosine Similarity)è®¡ç®—ç–¾ç—…é—´çš„ç›¸ä¼¼å¾—åˆ†ã€‚åœ¨åŒ…å«1100å¯¹ç›¸ä¼¼å’Œ866å¯¹ä¸ç›¸ä¼¼ç–¾ç—…çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº†0.9012çš„AUCPRå’Œ0.8764çš„AUROCï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚PhenoGnetèƒ½å¤Ÿæ•æ‰åˆ°è¶…è¶Šç›´æ¥é‡å çš„æ½œåœ¨ç”Ÿç‰©å­¦å…³ç³»ï¼Œä¸ºç–¾ç—…ç›¸ä¼¼æ€§é¢„æµ‹æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å…·è§£é‡Šæ€§çš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›ç ”ç©¶ç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨ç½•è§ç—…ç ”ç©¶å’Œç²¾å‡†åŒ»å­¦é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14037v1",
      "published_date": "2025-09-17 14:38:52 UTC",
      "updated_date": "2025-09-17 14:38:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:49.699233+00:00"
    },
    {
      "arxiv_id": "2509.14036v1",
      "title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation",
      "title_zh": "SSL-SSAWï¼šé¢å‘æé—®å¼æ‰‹è¯­ç¿»è¯‘çš„Sigmoidè‡ªæ³¨æ„åŠ›åŠ æƒè‡ªç›‘ç£å­¦ä¹ ",
      "authors": [
        "Zekang Liu",
        "Wei Feng",
        "Fanhua Shang",
        "Lianyu Hu",
        "Jichao Feng",
        "Liqing Gao"
      ],
      "abstract": "Sign Language Translation (SLT) bridges the communication gap between deaf people and hearing people, where dialogue provides crucial contextual cues to aid in translation. Building on this foundational concept, this paper proposes Question-based Sign Language Translation (QB-SLT), a novel task that explores the efficient integration of dialogue. Unlike gloss (sign language transcription) annotations, dialogue naturally occurs in communication and is easier to annotate. The key challenge lies in aligning multimodality features while leveraging the context of the question to improve translation. To address this issue, we propose a cross-modality Self-supervised Learning with Sigmoid Self-attention Weighting (SSL-SSAW) fusion method for sign language translation. Specifically, we employ contrastive learning to align multimodality features in QB-SLT, then introduce a Sigmoid Self-attention Weighting (SSAW) module for adaptive feature extraction from question and sign language sequences. Additionally, we leverage available question text through self-supervised learning to enhance representation and translation capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably, easily accessible question assistance can achieve or even surpass the performance of gloss assistance. Furthermore, visualization results demonstrate the effectiveness of incorporating dialogue in improving translation quality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Question-based Sign Language Translation (QB-SLT) è¿™ä¸€æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨åˆ©ç”¨å¯¹è¯ä¸­çš„é—®é¢˜ä½œä¸ºä¸Šä¸‹æ–‡è¾…åŠ©æ‰‹è¯­ç¿»è¯‘ (Sign Language Translation)ï¼Œä»¥è§£å†³æ‰‹è¯­è½¬å†™ (Gloss) æ ‡æ³¨å›°éš¾çš„é—®é¢˜ã€‚ä¸ºäº†æœ‰æ•ˆå¯¹é½å¤šæ¨¡æ€ç‰¹å¾å¹¶åˆ©ç”¨é—®é¢˜èƒŒæ™¯ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäº Sigmoid Self-attention Weighting çš„è·¨æ¨¡æ€è‡ªç›‘ç£å­¦ä¹ èåˆæ–¹æ³• SSL-SSAWã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹  (Contrastive Learning) å¯¹é½ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨ SSAW æ¨¡å—å¯¹é—®é¢˜å’Œæ‰‹è¯­åºåˆ—è¿›è¡Œè‡ªé€‚åº”ç‰¹å¾æå–ï¼ŒåŒæ—¶ç»“åˆè‡ªç›‘ç£å­¦ä¹  (Self-supervised Learning) å¢å¼ºç¿»è¯‘èƒ½åŠ›ã€‚åœ¨ CSL-Daily-QA å’Œ PHOENIX-2014T-QA æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSSL-SSAW è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ (SOTA) æ€§èƒ½ã€‚ç ”ç©¶è¯æ˜ï¼Œæ˜“äºè·å–çš„é—®é¢˜è¾…åŠ©ä¿¡æ¯åœ¨ç¿»è¯‘æ•ˆæœä¸Šå¯ä»¥è¾¾åˆ°ç”šè‡³è¶…è¶Šå¤æ‚çš„è¯­ç´ è¾…åŠ©ï¼Œæ˜¾è‘—æå‡äº†æ‰‹è¯­ç¿»è¯‘çš„è´¨é‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14036v1",
      "published_date": "2025-09-17 14:37:59 UTC",
      "updated_date": "2025-09-17 14:37:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:52.759589+00:00"
    },
    {
      "arxiv_id": "2509.15237v1",
      "title": "MICA: Multi-Agent Industrial Coordination Assistant",
      "title_zh": "MICAï¼šå¤šæ™ºèƒ½ä½“å·¥ä¸šååŒåŠ©æ‰‹",
      "authors": [
        "Di Wen",
        "Kunyu Peng",
        "Junwei Zheng",
        "Yufan Chen",
        "Yitain Shi",
        "Jiale Wei",
        "Ruiping Liu",
        "Kailun Yang",
        "Rainer Stiefelhagen"
      ],
      "abstract": "Industrial workflows demand adaptive and trustworthy assistance that can operate under limited computing, connectivity, and strict privacy constraints. In this work, we present MICA (Multi-Agent Industrial Coordination Assistant), a perception-grounded and speech-interactive system that delivers real-time guidance for assembly, troubleshooting, part queries, and maintenance. MICA coordinates five role-specialized language agents, audited by a safety checker, to ensure accurate and compliant support. To achieve robust step understanding, we introduce Adaptive Step Fusion (ASF), which dynamically blends expert reasoning with online adaptation from natural speech feedback. Furthermore, we establish a new multi-agent coordination benchmark across representative task categories and propose evaluation metrics tailored to industrial assistance, enabling systematic comparison of different coordination topologies. Our experiments demonstrate that MICA consistently improves task success, reliability, and responsiveness over baseline structures, while remaining deployable on practical offline hardware. Together, these contributions highlight MICA as a step toward deployable, privacy-preserving multi-agent assistants for dynamic factory environments. The source code will be made publicly available at https://github.com/Kratos-Wen/MICA.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† MICA (Multi-Agent Industrial Coordination Assistant)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ„ŸçŸ¥ä¸”æ”¯æŒè¯­éŸ³äº¤äº’çš„ç³»ç»Ÿï¼Œæ—¨åœ¨è®¡ç®—èµ„æºå—é™å’Œä¸¥æ ¼éšç§ä¿æŠ¤çš„å·¥ä¸šç¯å¢ƒä¸‹æä¾›å®æ—¶æŒ‡å¯¼ã€‚ç³»ç»Ÿé€šè¿‡åè°ƒäº”ä¸ªè§’è‰²ä¸“ä¸šåŒ–çš„è¯­è¨€æ™ºèƒ½ä½“ (language agents) å¹¶è¾…ä»¥å®‰å…¨æ£€æŸ¥å™¨ (safety checker)ï¼Œç¡®ä¿äº†è£…é…ã€æ•…éšœæ’é™¤å’Œç»´æŠ¤ç­‰ä»»åŠ¡çš„å‡†ç¡®æ€§ä¸åˆè§„æ€§ã€‚ä¸ºäº†å®ç°ç¨³å¥çš„æ­¥éª¤ç†è§£ï¼Œç ”ç©¶æå‡ºäº†è‡ªé€‚åº”æ­¥éª¤èåˆ (Adaptive Step Fusion, ASF) æœºåˆ¶ï¼Œå°†ä¸“å®¶æ¨ç†ä¸è‡ªç„¶è¯­éŸ³åé¦ˆçš„åœ¨çº¿è‡ªé€‚åº”åŠ¨æ€ç»“åˆã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜å»ºç«‹äº†ä¸€ä¸ªæ¶µç›–å¤šé¡¹ä»£è¡¨æ€§å·¥ä¸šä»»åŠ¡çš„å¤šæ™ºèƒ½ä½“åè°ƒåŸºå‡†ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ä»¥è¿›è¡Œç³»ç»Ÿæ€§æ¯”è¾ƒã€‚å®éªŒè¯æ˜ï¼ŒMICA åœ¨ä»»åŠ¡æˆåŠŸç‡ã€å¯é æ€§å’Œå“åº”é€Ÿåº¦ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä¸”èƒ½æˆåŠŸéƒ¨ç½²åœ¨å®é™…çš„ç¦»çº¿ç¡¬ä»¶ä¸Šã€‚è¯¥æˆæœä¸ºåœ¨åŠ¨æ€å·¥å‚ç¯å¢ƒä¸­æ„å»ºå¯éƒ¨ç½²ä¸”ä¿æŠ¤éšç§çš„å¤šæ™ºèƒ½ä½“åŠ©æ‰‹è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "The source code will be made publicly available at https://github.com/Kratos-Wen/MICA",
      "pdf_url": "https://arxiv.org/pdf/2509.15237v1",
      "published_date": "2025-09-17 14:36:38 UTC",
      "updated_date": "2025-09-17 14:36:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:09:56.864273+00:00"
    },
    {
      "arxiv_id": "2509.14031v1",
      "title": "You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models",
      "title_zh": "ä½ æ‰€è®­ç»ƒçš„å³ä½ æ‰€å¾—ï¼šæ•°æ®æ„æˆå¯¹ä¸Šä¸‹æ–‡æ„ŸçŸ¥æœºå™¨ç¿»è¯‘æ¨¡å‹è®­ç»ƒçš„å½±å“",
      "authors": [
        "PaweÅ‚ MÄ…ka",
        "Yusuf Can Semerci",
        "Jan Scholtes",
        "Gerasimos Spanakis"
      ],
      "abstract": "Achieving human-level translations requires leveraging context to ensure coherence and handle complex phenomena like pronoun disambiguation. Sparsity of contextually rich examples in the standard training data has been hypothesized as the reason for the difficulty of context utilization. In this work, we systematically validate this claim in both single- and multilingual settings by constructing training datasets with a controlled proportions of contextually relevant examples. We demonstrate a strong association between training data sparsity and model performance confirming sparsity as a key bottleneck. Importantly, we reveal that improvements in one contextual phenomenon do no generalize to others. While we observe some cross-lingual transfer, it is not significantly higher between languages within the same sub-family. Finally, we propose and empirically evaluate two training strategies designed to leverage the available data. These strategies improve context utilization, resulting in accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in single- and multilingual settings respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ•°æ®ç»„æˆå¯¹è®­ç»ƒä¸Šä¸‹æ–‡æ„ŸçŸ¥æœºå™¨ç¿»è¯‘æ¨¡å‹ (Context-aware Machine Translation) çš„å½±å“ï¼Œé‡ç‚¹åˆ†æäº†è®­ç»ƒæ•°æ®ä¸­ä¸Šä¸‹æ–‡ç›¸å…³ç¤ºä¾‹çš„ç¨€ç–æ€§ (Sparsity) å¦‚ä½•é™åˆ¶æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡åœ¨å•è¯­è¨€å’Œå¤šè¯­è¨€è®¾ç½®ä¸­æ„å»ºå…·æœ‰å—æ§æ¯”ä¾‹ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„æ•°æ®é›†ï¼Œç ”ç©¶è¯å®äº†æ•°æ®ç¨€ç–æ€§æ˜¯æ¨¡å‹åˆ©ç”¨ä¸Šä¸‹æ–‡çš„ä¸»è¦ç“¶é¢ˆã€‚å®éªŒæ­ç¤ºäº†ç‰¹å®šä¸Šä¸‹æ–‡ç°è±¡çš„æ”¹è¿›æ— æ³•æ³›åŒ–åˆ°å…¶ä»–ç°è±¡ï¼Œä¸”è·¨è¯­è¨€è¿ç§» (Cross-lingual transfer) åœ¨åŒè¯­ç³»è¯­è¨€é—´å¹¶æ— æ˜¾è‘—ä¼˜åŠ¿ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§æ—¨åœ¨ä¼˜åŒ–ç°æœ‰æ•°æ®åˆ©ç”¨ç‡çš„è®­ç»ƒç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›ç­–ç•¥æ˜¾è‘—æå‡äº†æ¨¡å‹çš„ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ï¼Œåœ¨ ctxPro è¯„æµ‹ä¸­ä½¿å•è¯­è¨€å’Œå¤šè¯­è¨€è®¾ç½®çš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº† 6 å’Œ 8 ä¸ªç™¾åˆ†ç‚¹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 main conference",
      "pdf_url": "https://arxiv.org/pdf/2509.14031v1",
      "published_date": "2025-09-17 14:33:17 UTC",
      "updated_date": "2025-09-17 14:33:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:10:07.991869+00:00"
    },
    {
      "arxiv_id": "2509.14030v1",
      "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System",
      "title_zh": "CrowdAgentï¼šå¤šæ™ºèƒ½ä½“ç®¡ç†çš„å¤šæºæ ‡æ³¨ç³»ç»Ÿ",
      "authors": [
        "Maosheng Qin",
        "Renyu Zhu",
        "Mingxuan Xia",
        "Chenkai Chen",
        "Zhen Zhu",
        "Minmin Lin",
        "Junbo Zhao",
        "Lu Xu",
        "Changjie Fan",
        "Runze Wu",
        "Haobo Wang"
      ],
      "abstract": "High-quality annotated data is a cornerstone of modern Natural Language Processing (NLP). While recent methods begin to leverage diverse annotation sources-including Large Language Models (LLMs), Small Language Models (SLMs), and human experts-they often focus narrowly on the labeling step itself. A critical gap remains in the holistic process control required to manage these sources dynamically, addressing complex scheduling and quality-cost trade-offs in a unified manner. Inspired by real-world crowdsourcing companies, we introduce CrowdAgent, a multi-agent system that provides end-to-end process control by integrating task assignment, data annotation, and quality/cost management. It implements a novel methodology that rationally assigns tasks, enabling LLMs, SLMs, and human experts to advance synergistically in a collaborative annotation workflow. We demonstrate the effectiveness of CrowdAgent through extensive experiments on six diverse multimodal classification tasks. The source code and video demo are available at https://github.com/QMMMS/CrowdAgent.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CrowdAgentï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡ä»¿ç°å®ä¼—åŒ…å…¬å¸è¿ä½œæ¨¡å¼çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-Agent System)ï¼Œæ—¨åœ¨ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†æä¾›ç«¯åˆ°ç«¯çš„æ ‡æ³¨è¿‡ç¨‹ç®¡æ§ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†ä»»åŠ¡åˆ†é…ã€æ•°æ®æ ‡æ³¨ä»¥åŠè´¨é‡ä¸æˆæœ¬ç®¡ç†ï¼Œæœ‰æ•ˆå¡«è¡¥äº†åœ¨åŠ¨æ€ç®¡ç†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs)ã€å°å‹è¯­è¨€æ¨¡å‹ (SLMs) å’Œäººç±»ä¸“å®¶ç­‰å¤šå…ƒæ ‡æ³¨æºæ–¹é¢çš„ç ”ç©¶ç©ºç™½ã€‚CrowdAgent é‡‡ç”¨ä¸€ç§æ–°é¢–çš„ç†æ€§ä»»åŠ¡åˆ†é…æ–¹æ³•ï¼Œä½¿ä¸åŒèƒ½åŠ›çš„æ ‡æ³¨æºåœ¨ååŒå·¥ä½œæµä¸­å®ç°ä¼˜åŠ¿äº’è¡¥ï¼Œä»è€Œåœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹ä¼˜åŒ–äº†å¤æ‚çš„è°ƒåº¦ä¸è´¨é‡æˆæœ¬æƒè¡¡ (Quality-Cost Trade-offs)ã€‚é€šè¿‡åœ¨å…­ä¸ªä¸åŒçš„å¤šæ¨¡æ€åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œç ”ç©¶å›¢é˜Ÿè¯å®äº† CrowdAgent åœ¨æå‡æ ‡æ³¨æ•ˆç‡ä¸è´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14030v1",
      "published_date": "2025-09-17 14:31:18 UTC",
      "updated_date": "2025-09-17 14:31:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:10:01.586798+00:00"
    },
    {
      "arxiv_id": "2509.14304v1",
      "title": "Deploying UDM Series in Real-Life Stuttered Speech Applications: A Clinical Evaluation Framework",
      "title_zh": "UDM ç³»åˆ—åœ¨ç°å®å£åƒè¯­éŸ³åº”ç”¨ä¸­çš„éƒ¨ç½²ï¼šä¸´åºŠè¯„ä¼°æ¡†æ¶",
      "authors": [
        "Eric Zhang",
        "Li Wei",
        "Sarah Chen",
        "Michael Wang"
      ],
      "abstract": "Stuttered and dysfluent speech detection systems have traditionally suffered from the trade-off between accuracy and clinical interpretability. While end-to-end deep learning models achieve high performance, their black-box nature limits clinical adoption. This paper looks at the Unconstrained Dysfluency Modeling (UDM) series-the current state-of-the-art framework developed by Berkeley that combines modular architecture, explicit phoneme alignment, and interpretable outputs for real-world clinical deployment. Through extensive experiments involving patients and certified speech-language pathologists (SLPs), we demonstrate that UDM achieves state-of-the-art performance (F1: 0.89+-0.04) while providing clinically meaningful interpretability scores (4.2/5.0). Our deployment study shows 87% clinician acceptance rate and 34% reduction in diagnostic time. The results provide strong evidence that UDM represents a practical pathway toward AI-assisted speech therapy in clinical environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå£åƒä¸è¯­è¨€éšœç¢æ£€æµ‹ç³»ç»Ÿåœ¨å‡†ç¡®ç‡ä¸ä¸´åºŠå¯è§£é‡Šæ€§(interpretability)ä¹‹é—´éš¾ä»¥å¹³è¡¡çš„é—®é¢˜ï¼Œè¯„ä¼°äº†ç”±ä¼¯å…‹åˆ©å¼€å‘çš„Unconstrained Dysfluency Modeling (UDM) ç³»åˆ—æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆæ¨¡å—åŒ–æ¶æ„(modular architecture)å’Œæ˜¾å¼çš„éŸ³ç´ å¯¹é½(phoneme alignment)ï¼Œåœ¨ç¡®ä¿é«˜æ€§èƒ½çš„åŒæ—¶ä¸ºç°å®ä¸´åºŠéƒ¨ç½²æä¾›å¯è§£é‡Šçš„è¾“å‡ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUDM åœ¨é’ˆå¯¹æ‚£è€…å’Œè®¤è¯è¨€è¯­æ²»ç–—å¸ˆ(SLPs)çš„æµ‹è¯•ä¸­å–å¾—äº† F1 åˆ†æ•° 0.89Â±0.04 çš„é¡¶å°–æ€§èƒ½ï¼Œå¹¶è·å¾—äº† 4.2/5.0 çš„é«˜å¯è§£é‡Šæ€§è¯„åˆ†ã€‚ä¸´åºŠéƒ¨ç½²ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œè¯¥ç³»ç»Ÿè·å¾—äº† 87% çš„åŒ»ç”Ÿæ¥å—ç‡ï¼Œå¹¶æˆåŠŸç¼©çŸ­äº† 34% çš„è¯Šæ–­æ—¶é—´ã€‚è¯¥ç ”ç©¶æœ‰åŠ›è¯æ˜äº† UDM ä¸ºä¸´åºŠç¯å¢ƒä¸‹çš„ AI è¾…åŠ©è¨€è¯­æ²»ç–—æä¾›äº†ä¸€æ¡å…¼å…·å‡†ç¡®æ€§ä¸å®ç”¨æ€§çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14304v1",
      "published_date": "2025-09-17 14:28:29 UTC",
      "updated_date": "2025-09-17 14:28:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:10:09.485570+00:00"
    },
    {
      "arxiv_id": "2509.14008v1",
      "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale",
      "title_zh": "Hala æŠ€æœ¯æŠ¥å‘Šï¼šå¤§è§„æ¨¡æ„å»ºä»¥é˜¿æ‹‰ä¼¯è¯­ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤ä¸ç¿»è¯‘æ¨¡å‹",
      "authors": [
        "Hasan Abed Al Kader Hammoud",
        "Mohammad Zbeeb",
        "Bernard Ghanem"
      ],
      "abstract": "We present Hala, a family of Arabic-centric instruction and translation models built with our translate-and-tune pipeline. We first compress a strong AR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher throughput with no quality loss) and use it to create high-fidelity bilingual supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this data and used to translate high-quality English instruction sets into Arabic, producing a million-scale corpus tailored to instruction following. We train Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to balance Arabic specialization with base-model strengths. On Arabic-centric benchmarks, Hala achieves state-of-the-art results within both the \"nano\" ($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release models, data, evaluation, and recipes to accelerate research in Arabic NLP.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†ä»¥é˜¿æ‹‰ä¼¯è¯­ä¸ºæ ¸å¿ƒçš„ Hala ç³»åˆ—æŒ‡ä»¤éµå¾ªä¸ç¿»è¯‘æ¨¡å‹ï¼Œé€šè¿‡åˆ›æ–°çš„ç¿»è¯‘å¾®è°ƒï¼ˆtranslate-and-tuneï¼‰æµæ°´çº¿å®ç°äº†è§„æ¨¡åŒ–æ„å»ºã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆå°†å¼ºå¤§çš„é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­ï¼ˆARâ†”ENï¼‰æ•™å¸ˆæ¨¡å‹å‹ç¼©è‡³ FP8 æ ¼å¼ï¼Œåœ¨æ˜¾è‘—æå‡ååé‡çš„åŒæ—¶ä¿æŒäº†ç”Ÿæˆè´¨é‡ï¼Œå¹¶åŸºäºæ­¤ç”Ÿæˆäº†é«˜ä¿çœŸçš„åŒè¯­ç›‘ç£æ•°æ®ã€‚éšååˆ©ç”¨å¾®è°ƒåçš„è½»é‡çº§æ¨¡å‹å°†é«˜è´¨é‡è‹±è¯­æŒ‡ä»¤é›†è½¬åŒ–ä¸ºé˜¿æ‹‰ä¼¯è¯­ï¼ŒæˆåŠŸæ„å»ºå‡ºç™¾ä¸‡çº§åˆ«çš„æŒ‡ä»¤è¯­æ–™åº“ã€‚è¯¥ç³»åˆ—æ¶µç›–äº†ä» 350M åˆ° 9B ç­‰å¤šç§å‚æ•°è§„æ¨¡ï¼Œå¹¶é‡‡ç”¨çƒé¢çº¿æ€§æ’å€¼ï¼ˆslerp mergingï¼‰æŠ€æœ¯æœ‰æ•ˆå¹³è¡¡äº†æ¨¡å‹çš„é˜¿æ‹‰ä¼¯è¯­ä¸“ä¸šèƒ½åŠ›ä¸åŸºç¡€æ¨¡å‹çš„å›ºæœ‰ä¼˜åŠ¿ã€‚å®éªŒè¯æ˜ï¼ŒHala åœ¨å¤šé¡¹é˜¿æ‹‰ä¼¯è¯­åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†â€œçº³ç±³çº§â€ï¼ˆâ‰¤2Bï¼‰å’Œâ€œå°å‹â€ï¼ˆ7-9Bï¼‰æ¨¡å‹ç±»åˆ«ä¸­çš„æœ€å…ˆè¿›ï¼ˆstate-of-the-artï¼‰æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å¼€æºæ¨¡å‹ã€æ•°æ®åŠè®­ç»ƒæ–¹æ¡ˆï¼Œä¸ºé˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆArabic NLPï¼‰é¢†åŸŸçš„å¤§è§„æ¨¡ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Technical Report",
      "pdf_url": "https://arxiv.org/pdf/2509.14008v1",
      "published_date": "2025-09-17 14:19:28 UTC",
      "updated_date": "2025-09-17 14:19:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:11:19.991681+00:00"
    },
    {
      "arxiv_id": "2509.14003v1",
      "title": "RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing",
      "title_zh": "RFM-Editingï¼šåŸºäºä¿®æ­£æµåŒ¹é…çš„æ–‡æœ¬å¼•å¯¼éŸ³é¢‘ç¼–è¾‘",
      "authors": [
        "Liting Gao",
        "Yi Yuan",
        "Yaru Chen",
        "Yuelan Cheng",
        "Zhenbo Li",
        "Juan Wen",
        "Shubin Zhang",
        "Wenwu Wang"
      ],
      "abstract": "Diffusion models have shown remarkable progress in text-to-audio generation. However, text-guided audio editing remains in its early stages. This task focuses on modifying the target content within an audio signal while preserving the rest, thus demanding precise localization and faithful editing according to the text prompt. Existing training-based and zero-shot methods that rely on full-caption or costly optimization often struggle with complex editing or lack practicality. In this work, we propose a novel end-to-end efficient rectified flow matching-based diffusion framework for audio editing, and construct a dataset featuring overlapping multi-event audio to support training and benchmarking in complex scenarios. Experiments show that our model achieves faithful semantic alignment without requiring auxiliary captions or masks, while maintaining competitive editing quality across metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RFM-Editingï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¿®æ­£æµåŒ¹é… (Rectified Flow Matching) çš„ç«¯åˆ°ç«¯é«˜æ•ˆæ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬å¼•å¯¼éŸ³é¢‘ç¼–è¾‘ä¸­ç²¾ç¡®å®šä½ä¸å†…å®¹ä¿ç•™çš„éš¾é¢˜ã€‚ç›¸æ¯”äºç°æœ‰çš„ä¾èµ–å®Œæ•´æ ‡æ³¨æˆ–é«˜æ˜‚ä¼˜åŒ–æˆæœ¬çš„æ–¹æ³•ï¼Œè¯¥æ¡†æ¶ä¸“æ³¨äºåœ¨ä¿®æ”¹ç›®æ ‡éŸ³é¢‘å†…å®¹çš„åŒæ—¶ï¼Œæœ€å¤§é™åº¦åœ°ä¿æŒå…¶ä½™èƒŒæ™¯ä¿¡æ¯çš„å®Œæ•´æ€§ã€‚é€šè¿‡å¼•å…¥ Rectified Flow Matching æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹å®ç°äº†æ— éœ€è¾…åŠ©è¯´æ˜ (auxiliary captions) æˆ–æ©ç  (masks) çš„ç²¾ç¡®è¯­ä¹‰å¯¹é½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«é‡å å¤šäº‹ä»¶éŸ³é¢‘çš„æ•°æ®é›†ï¼Œç”¨ä»¥æ”¯æŒåœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ¨¡å‹è®­ç»ƒä¸åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRFM-Editing åœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸‹å‡å±•ç°å‡ºæå…·ç«äº‰åŠ›çš„ç¼–è¾‘è´¨é‡ï¼Œä¸ºé«˜æ•ˆä¸”å®ç”¨çš„éŸ³é¢‘ç¼–è¾‘æŠ€æœ¯æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14003v1",
      "published_date": "2025-09-17 14:13:40 UTC",
      "updated_date": "2025-09-17 14:13:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:10:12.991246+00:00"
    },
    {
      "arxiv_id": "2509.14001v4",
      "title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment",
      "title_zh": "MOCHAï¼šå¤šæ¨¡æ€ç‰©ä½“æ„ŸçŸ¥è·¨æ¶æ„å¯¹é½",
      "authors": [
        "Elena Camuffo",
        "Francesco Barbato",
        "Mete Ozay",
        "Simone Milani",
        "Umberto Michieli"
      ],
      "abstract": "Personalized object detection aims to adapt a general-purpose detector to recognize user-specific instances from only a few examples. Lightweight models often struggle in this setting due to their weak semantic priors, while large vision-language models (VLMs) offer strong object-level understanding but are too computationally demanding for real-time or on-device applications. We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment), a distillation framework that transfers multimodal region-level knowledge from a frozen VLM teacher into a lightweight vision-only detector. MOCHA extracts fused visual and textual teacher's embeddings and uses them to guide student training through a dual-objective loss that enforces accurate local alignment and global relational consistency across regions. This process enables efficient transfer of semantics without the need for teacher modifications or textual input at inference. MOCHA consistently outperforms prior baselines across four personalized detection benchmarks under strict few-shot regimes, yielding a +10.1 average improvement, with minimal inference cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MOCHAï¼ˆMulti-modal Objects-aware Cross-arcHitecture Alignmentï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºä¸ªæ€§åŒ–ç›®æ ‡æ£€æµ‹ï¼ˆPersonalized object detectionï¼‰æ€§èƒ½çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ã€‚é’ˆå¯¹è½»é‡çº§æ¨¡å‹åœ¨å°‘æ ·æœ¬ï¼ˆfew-shotï¼‰åœºæ™¯ä¸‹è¯­ä¹‰å…ˆéªŒè¾ƒå¼±ï¼Œè€Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è®¡ç®—å¼€é”€è¿‡å¤§çš„é—®é¢˜ï¼ŒMOCHAé€šè¿‡å°†å†»ç»“çš„VLMæ•™å¸ˆæ¨¡å‹ä¸­çš„å¤šæ¨¡æ€åŒºåŸŸçº§çŸ¥è¯†è¿ç§»è‡³ä»…åŒ…å«è§†è§‰æ¨¡å—çš„è½»é‡çº§å­¦ç”Ÿæ£€æµ‹å™¨ä¸­ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ•™å¸ˆæ¨¡å‹ä¸­èåˆçš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ï¼Œé€šè¿‡åŒé‡ç›®æ ‡æŸå¤±ï¼ˆdual-objective lossï¼‰å¼•å¯¼å­¦ç”Ÿæ¨¡å‹è®­ç»ƒï¼Œä»¥ç¡®ä¿ç²¾ç¡®çš„å±€éƒ¨å¯¹é½å’Œè·¨åŒºåŸŸçš„å…¨å±€å…³ç³»ä¸€è‡´æ€§ã€‚è¿™ä¸€è¿‡ç¨‹å®ç°äº†è¯­ä¹‰çš„é«˜æ•ˆè¿ç§»ï¼Œä¸”åœ¨æ¨ç†é˜¶æ®µæ— éœ€ä¿®æ”¹æ•™å¸ˆæ¨¡å‹æˆ–é¢å¤–æä¾›æ–‡æœ¬è¾“å…¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMOCHAåœ¨å››ä¸ªä¸ªæ€§åŒ–æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹³å‡æ€§èƒ½æå‡è¾¾10.1ç‚¹ï¼Œåœ¨æä½æ¨ç†æˆæœ¬ä¸‹å®ç°äº†å¼ºå¤§çš„ç›®æ ‡è¯†åˆ«èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14001v4",
      "published_date": "2025-09-17 14:13:20 UTC",
      "updated_date": "2025-11-21 14:33:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:11:20.448041+00:00"
    },
    {
      "arxiv_id": "2509.13990v1",
      "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency",
      "title_zh": "Slim-SCï¼šåŸºäºæ€ç»´å‰ªæçš„è‡ªæˆ‘ä¸€è‡´æ€§é«˜æ•ˆæ‰©å±•",
      "authors": [
        "Colin Hong",
        "Xu Guo",
        "Anand Chaanan Singh",
        "Esha Choukse",
        "Dmitrii Ustiugov"
      ],
      "abstract": "Recently, Test-Time Scaling (TTS) has gained increasing attention for improving LLM reasoning performance at test time without retraining the model. A notable TTS technique is Self-Consistency (SC), which generates multiple reasoning chains in parallel and selects the final answer via majority voting. While effective, the order-of-magnitude computational overhead limits its broad deployment. Prior attempts to accelerate SC mainly rely on model-based confidence scores or heuristics with limited empirical support. For the first time, we theoretically and empirically analyze the inefficiencies of SC and reveal actionable opportunities for improvement. Building on these insights, we propose Slim-SC, a step-wise pruning strategy that identifies and removes redundant chains using inter-chain similarity at the thought level. Experiments on three STEM reasoning datasets and two recent LLM architectures show that Slim-SC reduces inference latency and KVC usage by up to 45% and 26%, respectively, with R1-Distill, while maintaining or improving accuracy, thus offering a simple yet efficient TTS alternative for SC.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµ‹è¯•æ—¶ç¼©æ”¾(Test-Time Scaling)æŠ€æœ¯ä¸­è‡ªæˆ‘ä¸€è‡´æ€§(Self-Consistency)è®¡ç®—å¼€é”€è¿‡å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†Slim-SCã€‚è¿™æ˜¯ä¸€ç§æ­¥è¿›å¼å‰ªæç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡æ€ç»´å±‚é¢çš„é“¾é—´ç›¸ä¼¼åº¦(inter-chain similarity)è¯†åˆ«å¹¶ç§»é™¤å†—ä½™çš„æ¨ç†é“¾ã€‚ç ”ç©¶å›¢é˜Ÿé¦–æ¬¡é€šè¿‡ç†è®ºä¸å®è¯åˆ†ææ­ç¤ºäº†Self-Consistencyçš„ä½æ•ˆæ€§ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†è¯¥ä¼˜åŒ–æ–¹æ¡ˆã€‚åœ¨ä¸‰ç±»STEMæ¨ç†æ•°æ®é›†å’Œå¤šç§LLMæ¶æ„ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSlim-SCåœ¨é…åˆR1-Distillæ¨¡å‹æ—¶ï¼Œèƒ½å°†æ¨ç†å»¶è¿Ÿå‡å°‘é«˜è¾¾45%ï¼Œå¹¶å°†KV Cacheä½¿ç”¨é‡é™ä½26%ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒç”šè‡³æå‡å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œä¸ºSelf-Consistencyæä¾›äº†ä¸€ç§ç®€å•ä¸”é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP 2025 (Oral), 9 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.13990v1",
      "published_date": "2025-09-17 14:00:51 UTC",
      "updated_date": "2025-09-17 14:00:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:10:25.788406+00:00"
    },
    {
      "arxiv_id": "2509.13987v1",
      "title": "Differential Privacy in Federated Learning: Mitigating Inference Attacks with Randomized Response",
      "title_zh": "è”é‚¦å­¦ä¹ ä¸­çš„å·®åˆ†éšç§ï¼šåˆ©ç”¨éšæœºå“åº”ç¼“è§£æ¨ç†æ”»å‡»",
      "authors": [
        "Ozer Ozturk",
        "Busra Buyuktanir",
        "Gozde Karatas Baydogmus",
        "Kazim Yildiz"
      ],
      "abstract": "Machine learning models used for distributed architectures consisting of servers and clients require large amounts of data to achieve high accuracy. Data obtained from clients are collected on a central server for model training. However, storing data on a central server raises concerns about security and privacy. To address this issue, a federated learning architecture has been proposed. In federated learning, each client trains a local model using its own data. The trained models are periodically transmitted to the central server. The server then combines the received models using federated aggregation algorithms to obtain a global model. This global model is distributed back to the clients, and the process continues in a cyclical manner. Although preventing data from leaving the clients enhances security, certain concerns still remain. Attackers can perform inference attacks on the obtained models to approximate the training dataset, potentially causing data leakage. In this study, differential privacy was applied to address the aforementioned security vulnerability, and a performance analysis was conducted. The Data-Unaware Classification Based on Association (duCBA) algorithm was used as the federated aggregation method. Differential privacy was implemented on the data using the Randomized Response technique, and the trade-off between security and performance was examined under different epsilon values. As the epsilon value decreased, the model accuracy declined, and class prediction imbalances were observed. This indicates that higher levels of privacy do not always lead to practical outcomes and that the balance between security and performance must be carefully considered.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è”é‚¦å­¦ä¹ (Federated Learning)æ¶æ„ä¸­ï¼Œå¦‚ä½•é€šè¿‡å·®åˆ†éšç§(Differential Privacy)æŠ€æœ¯æ¥ç¼“è§£é’ˆå¯¹æœ¬åœ°æ¨¡å‹æ›´æ–°çš„æ¨æ–­æ”»å‡»(Inference Attacks)ï¼Œä»¥é˜²æ­¢è®­ç»ƒæ•°æ®é›†çš„éšç§æ³„éœ²ã€‚ç ”ç©¶é‡‡ç”¨éšæœºå“åº”(Randomized Response)æŠ€æœ¯å®ç°å·®åˆ†éšç§ï¼Œå¹¶ç»“åˆåŸºäºå…³è”çš„éæ•°æ®æ„ŸçŸ¥åˆ†ç±»(duCBA)ç®—æ³•ä½œä¸ºå…¶è”é‚¦èšåˆæ–¹æ³•ã€‚é€šè¿‡å¯¹ä¸åŒéšç§é¢„ç®—(epsilon)å€¼çš„æ€§èƒ½åˆ†æï¼Œç ”ç©¶æ·±å…¥æ¢è®¨äº†å®‰å…¨æ€§ä¸æ¨¡å‹å‡†ç¡®ç‡ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€epsilonå€¼çš„é™ä½ï¼Œéšç§ä¿æŠ¤æ°´å¹³è™½æœ‰æå‡ï¼Œä½†æ¨¡å‹å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼Œå¹¶å‡ºç°äº†ç±»åˆ«é¢„æµ‹ä¸å¹³è¡¡çš„ç°è±¡ã€‚è¯¥å‘ç°å¼ºè°ƒäº†åœ¨å®é™…åº”ç”¨ä¸­å¿…é¡»è°¨æ…æƒè¡¡å®‰å…¨é˜²å¾¡ä¸æ¨¡å‹æ•ˆèƒ½ï¼Œå› ä¸ºè¿‡é«˜çš„éšç§ä¿æŠ¤å¼ºåº¦å¯èƒ½å¯¼è‡´æ¨¡å‹å¤±å»å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13987v1",
      "published_date": "2025-09-17 13:59:38 UTC",
      "updated_date": "2025-09-17 13:59:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:11:45.157979+00:00"
    },
    {
      "arxiv_id": "2509.14303v1",
      "title": "FlowDrive: Energy Flow Field for End-to-End Autonomous Driving",
      "title_zh": "FlowDriveï¼šé¢å‘ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶çš„èƒ½é‡æµåœº",
      "authors": [
        "Hao Jiang",
        "Zhipeng Zhang",
        "Yu Gao",
        "Zhigang Sun",
        "Yiru Wang",
        "Yuwen Heng",
        "Shuo Wang",
        "Jinhao Chai",
        "Zhuo Chen",
        "Hao Zhao",
        "Hao Sun",
        "Xi Zhang",
        "Anqing Jiang",
        "Chuan Hu"
      ],
      "abstract": "Recent advances in end-to-end autonomous driving leverage multi-view images to construct BEV representations for motion planning. In motion planning, autonomous vehicles need considering both hard constraints imposed by geometrically occupied obstacles (e.g., vehicles, pedestrians) and soft, rule-based semantics with no explicit geometry (e.g., lane boundaries, traffic priors). However, existing end-to-end frameworks typically rely on BEV features learned in an implicit manner, lacking explicit modeling of risk and guidance priors for safe and interpretable planning. To address this, we propose FlowDrive, a novel framework that introduces physically interpretable energy-based flow fields-including risk potential and lane attraction fields-to encode semantic priors and safety cues into the BEV space. These flow-aware features enable adaptive refinement of anchor trajectories and serve as interpretable guidance for trajectory generation. Moreover, FlowDrive decouples motion intent prediction from trajectory denoising via a conditional diffusion planner with feature-level gating, alleviating task interference and enhancing multimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that FlowDrive achieves state-of-the-art performance with an EPDMS of 86.3, surpassing prior baselines in both safety and planning quality. The project is available at https://astrixdrive.github.io/FlowDrive.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FlowDriveï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨ Birdâ€™s Eye View (BEV) è¡¨å¾ä¸­ç¼ºä¹æ˜¾å¼é£é™©ä¸å¼•å¯¼å…ˆéªŒå»ºæ¨¡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å…·æœ‰ç‰©ç†å¯è§£é‡Šæ€§çš„èƒ½é‡æµåœº (Energy-based flow fields)ï¼ŒåŒ…æ‹¬é£é™©åŠ¿èƒ½åœº (Risk potential fields) å’Œè½¦é“å¸å¼•åœº (Lane attraction fields)ï¼Œå°†è¯­ä¹‰å…ˆéªŒå’Œå®‰å…¨çº¿ç´¢ç¼–ç è¿› BEV ç©ºé—´ã€‚é€šè¿‡æµåœºæ„ŸçŸ¥ç‰¹å¾ï¼ŒFlowDrive èƒ½å¤Ÿå®ç°é”šç‚¹è½¨è¿¹çš„è‡ªé€‚åº”ä¼˜åŒ–ï¼Œå¹¶ä¸ºè½¨è¿¹ç”Ÿæˆæä¾›ç›´è§‚çš„å¯è§£é‡Šå¼•å¯¼ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨å¸¦æœ‰ç‰¹å¾çº§é—¨æ§æœºåˆ¶çš„æ¡ä»¶æ‰©æ•£è§„åˆ’å™¨ (Conditional diffusion planner)ï¼Œå°†è¿åŠ¨æ„å›¾é¢„æµ‹ä¸è½¨è¿¹å»å™ªè¿‡ç¨‹è§£è€¦ï¼Œä»è€Œæœ‰æ•ˆåœ°ç¼“è§£äº†ä»»åŠ¡å¹²æ‰°å¹¶å¢å¼ºäº†å¤šæ¨¡æ€å¤šæ ·æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFlowDrive åœ¨ NAVSIM v2 åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† 86.3 çš„ EPDMS å¾—åˆ†ï¼Œåˆ·æ–°äº† State-of-the-art è®°å½•ï¼Œåœ¨å®‰å…¨æ€§ä¸è§„åˆ’è´¨é‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14303v1",
      "published_date": "2025-09-17 13:51:33 UTC",
      "updated_date": "2025-09-17 13:51:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:11:37.160009+00:00"
    },
    {
      "arxiv_id": "2509.13978v2",
      "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology",
      "title_zh": "é¢å‘äº¤äº’å¼å·¥ä½œæµæº¯æºçš„ LLM æ™ºèƒ½ä½“ï¼šå‚è€ƒæ¶æ„ä¸è¯„ä¼°æ–¹æ³•è®º",
      "authors": [
        "Renan Souza",
        "Timothy Poteet",
        "Brian Etz",
        "Daniel Rosendo",
        "Amal Gueroudji",
        "Woong Shin",
        "Prasanna Balaprakash",
        "Rafael Ferreira da Silva"
      ],
      "abstract": "Modern scientific discovery increasingly relies on workflows that process data across the Edge, Cloud, and High Performance Computing (HPC) continuum. Comprehensive and in-depth analyses of these data are critical for hypothesis validation, anomaly detection, reproducibility, and impactful findings. Although workflow provenance techniques support such analyses, at large scale, the provenance data become complex and difficult to analyze. Existing systems depend on custom scripts, structured queries, or static dashboards, limiting data interaction. In this work, we introduce an evaluation methodology, reference architecture, and open-source implementation that leverages interactive Large Language Model (LLM) agents for runtime data analysis. Our approach uses a lightweight, metadata-driven design that translates natural language into structured provenance queries. Evaluations across LLaMA, GPT, Gemini, and Claude, covering diverse query classes and a real-world chemistry workflow, show that modular design, prompt tuning, and Retrieval-Augmented Generation (RAG) enable accurate and insightful LLM agent responses beyond recorded provenance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜ã€äº‘å’Œé«˜æ€§èƒ½è®¡ç®—(HPC)ç¯å¢ƒä¸‹ç§‘å­¦å·¥ä½œæµäº§ç”Ÿçš„å¤æ‚æ•°æ®æº¯æº(Provenance)åˆ†æéš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“çš„å‚è€ƒæ¶æ„ã€è¯„ä¼°æ–¹æ³•åŠå¼€æºå®ç°ã€‚è¯¥æ–¹æ¡ˆé‡‡ç”¨è½»é‡çº§çš„å…ƒæ•°æ®é©±åŠ¨è®¾è®¡ï¼Œå®ç°äº†å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤é«˜æ•ˆè½¬åŒ–ä¸ºç»“æ„åŒ–æº¯æºæŸ¥è¯¢çš„åŠŸèƒ½ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿè„šæœ¬å’Œé™æ€ä»ªè¡¨ç›˜çš„äº¤äº’å±€é™ã€‚é€šè¿‡ç»“åˆæ¨¡å—åŒ–è®¾è®¡ã€æç¤ºè¯å¾®è°ƒ(Prompt Tuning)å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ï¼Œç ”ç©¶äººå‘˜åœ¨LLaMAã€GPTã€Geminiå’ŒClaudeç­‰å¤šç§æ¨¡å‹ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœåŠçœŸå®åŒ–å­¦å·¥ä½œæµçš„æ¡ˆä¾‹åˆ†æè¡¨æ˜ï¼Œè¯¥æ¶æ„èƒ½ä½¿æ™ºèƒ½ä½“æä¾›è¶…è¶Šå•çº¯è®°å½•æº¯æºæ•°æ®çš„å‡†ç¡®åˆ†æä¸æ·±åˆ»è§è§£ï¼Œä¸ºå¤æ‚ç§‘å­¦å·¥ä½œæµçš„äº¤äº’å¼è¿è¡Œæ—¶æ•°æ®åˆ†ææä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.DC",
      "comment": "Paper accepted in the proceedings of the Supercomputing Conference (SC). Cite it as Renan Souza, Timothy Poteet, Brian Etz, Daniel Rosendo, Amal Gueroudji, Woong Shin, Prasanna Balaprakash, and Rafael Ferreira da Silva. LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology. In WORKS at the ACM/IEEE International Conference on Supercomputing, 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.13978v2",
      "published_date": "2025-09-17 13:51:29 UTC",
      "updated_date": "2025-09-23 13:31:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:11:38.171599+00:00"
    },
    {
      "arxiv_id": "2509.13968v1",
      "title": "Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks",
      "title_zh": "åˆ©ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¢ç©¶ç”Ÿç‰©è®¤çŸ¥æ¼”åŒ–ä¸­çš„é‡å¤§è½¬å˜",
      "authors": [
        "Konstantinos Voudouris",
        "Andrew Barron",
        "Marta Halina",
        "Colin Klein",
        "Matishalin Patel"
      ],
      "abstract": "Transitional accounts of evolution emphasise a few changes that shape what is evolvable, with dramatic consequences for derived lineages. More recently it has been proposed that cognition might also have evolved via a series of major transitions that manipulate the structure of biological neural networks, fundamentally changing the flow of information. We used idealised models of information flow, artificial neural networks (ANNs), to evaluate whether changes in information flow in a network can yield a transitional change in cognitive performance. We compared networks with feed-forward, recurrent and laminated topologies, and tested their performance learning artificial grammars that differed in complexity, controlling for network size and resources. We documented a qualitative expansion in the types of input that recurrent networks can process compared to feed-forward networks, and a related qualitative increase in performance for learning the most complex grammars. We also noted how the difficulty in training recurrent networks poses a form of transition barrier and contingent irreversibility -- other key features of evolutionary transitions. Not all changes in network topology confer a performance advantage in this task set. Laminated networks did not outperform non-laminated networks in grammar learning. Overall, our findings show how some changes in information flow can yield transitions in cognitive performance.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨äººå·¥ç¥ç»ç½‘ç»œ (ANNs) æ¢è®¨äº†ç”Ÿç‰©è®¤çŸ¥è¿›åŒ–ä¸­ç”±ç¥ç»ç½‘ç»œç»“æ„æ”¹å˜å¼•å‘çš„é‡å¤§è½¬å˜ (Major Transitions)ï¼Œæ—¨åœ¨è¯„ä¼°ä¿¡æ¯æµçš„å˜åŒ–æ˜¯å¦ä¼šå¯¼è‡´è®¤çŸ¥æ€§èƒ½çš„è·¨è¶Šå¼å‘å±•ã€‚ç ”ç©¶äººå‘˜å¯¹æ¯”äº†å‰é¦ˆ (feed-forward)ã€é€’å½’ (recurrent) å’Œå±‚çŠ¶ (laminated) æ‹“æ‰‘ç»“æ„ï¼Œåœ¨æ§åˆ¶ç½‘ç»œè§„æ¨¡çš„åŸºç¡€ä¸Šæµ‹è¯•äº†å®ƒä»¬å­¦ä¹ ä¸åŒå¤æ‚åº¦æ–‡æ³•çš„èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œé€’å½’ç½‘ç»œç›¸æ¯”å‰é¦ˆç½‘ç»œåœ¨å¤„ç†è¾“å…¥ç±»å‹ä¸Šå®ç°äº†è´¨çš„æ‰©å¼ ï¼Œæ˜¾è‘—æå‡äº†å¯¹å¤æ‚æ–‡æ³•çš„å­¦ä¹ æ€§èƒ½ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œé€’å½’ç½‘ç»œè®­ç»ƒçš„éš¾åº¦æ„æˆäº†ä¸€ç§ç±»ä¼¼äºè¿›åŒ–è½¬å˜ä¸­çš„è¿‡æ¸¡éšœç¢ (transition barrier) å’Œå¶ç„¶ä¸å¯é€†æ€§ (contingent irreversibility)ã€‚å°½ç®¡å±‚çŠ¶ç»“æ„åœ¨æ–‡æ³•å­¦ä¹ ä»»åŠ¡ä¸­æœªè¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œä½†æ•´ä½“ç ”ç©¶è¯æ˜äº†ç‰¹å®šæ‹“æ‰‘ç»“æ„å¸¦æ¥çš„ä¿¡æ¯æµæ”¹å˜ç¡®å®èƒ½å¤Ÿé©±åŠ¨è®¤çŸ¥æ€§èƒ½çš„æ¼”åŒ–è¿‡æ¸¡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.FL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13968v1",
      "published_date": "2025-09-17 13:38:40 UTC",
      "updated_date": "2025-09-17 13:38:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:11:43.978172+00:00"
    },
    {
      "arxiv_id": "2509.15236v1",
      "title": "ChannelFlow-Tools: A Standardized Dataset Creation Pipeline for 3D Obstructed Channel Flows",
      "title_zh": "ChannelFlow-Toolsï¼šä¸‰ç»´æœ‰éšœé€šé“æµæ ‡å‡†åŒ–æ•°æ®é›†æ„å»ºæµæ°´çº¿",
      "authors": [
        "Shubham Kavane",
        "Kajol Kulkarni",
        "Harald Koestler"
      ],
      "abstract": "We present ChannelFlow-Tools, a configuration-driven framework that standardizes the end-to-end path from programmatic CAD solid generation to ML-ready inputs and targets for 3D obstructed channel flows. The toolchain integrates geometry synthesis with feasibility checks, signed distance field (SDF) voxelization, automated solver orchestration on HPC (waLBerla LBM), and Cartesian resampling to co-registered multi-resolution tensors. A single Hydra/OmegaConf configuration governs all stages, enabling deterministic reproduction and controlled ablations. As a case study, we generate 10k+ scenes spanning Re=100-15000 with diverse shapes and poses. An end-to-end evaluation of storage trade-offs directly from the emitted artifacts, a minimal 3D U-Net at 128x32x32, and example surrogate models with dataset size illustrate that the standardized representations support reproducible ML training. ChannelFlow-Tools turns one-off dataset creation into a reproducible, configurable pipeline for CFD surrogate modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ChannelFlow-Toolsï¼Œä¸€ä¸ªé…ç½®é©±åŠ¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ ‡å‡†åŒ–ä»ç¨‹åºåŒ–CADå›ºä½“ç”Ÿæˆåˆ°é€‚ç”¨äºæœºå™¨å­¦ä¹ (ML-ready)è¾“å…¥å’Œç›®æ ‡çš„3Då—é˜»é€šé“æµ(obstructed channel flows)ç«¯åˆ°ç«¯è·¯å¾„ã€‚è¯¥å·¥å…·é“¾é›†æˆäº†å‡ ä½•åˆæˆä¸å¯è¡Œæ€§æ£€æŸ¥ã€ç¬¦å·è·ç¦»åœº(SDF)ä½“ç´ åŒ–ã€HPCä¸Šçš„è‡ªåŠ¨æ±‚è§£å™¨ç¼–æ’(waLBerla LBM)ä»¥åŠç¬›å¡å°”é‡é‡‡æ ·(Cartesian resampling)è‡³ååŒæ³¨å†Œçš„å¤šåˆ†è¾¨ç‡å¼ é‡ã€‚æ•´ä¸ªå·¥ä½œæµç”±å•ä¸€çš„Hydra/OmegaConfé…ç½®ç®¡ç†ï¼Œç¡®ä¿äº†å®éªŒçš„ç¡®å®šæ€§å¤ç°å’Œå—æ§æ¶ˆèç ”ç©¶ã€‚ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥å·¥å…·ç”Ÿæˆäº†è¶…è¿‡1ä¸‡ä¸ªæ¶µç›–Re=100-15000ä¸”å…·æœ‰å¤šæ ·å½¢çŠ¶å’Œå§¿æ€çš„åœºæ™¯ã€‚é€šè¿‡å¯¹å­˜å‚¨æƒè¡¡ã€å°å‹3D U-Netæ¨¡å‹ä»¥åŠä»£ç†æ¨¡å‹çš„è¯„ä¼°ï¼Œè¯æ˜äº†è¯¥æ ‡å‡†åŒ–è¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒå¯å¤ç°çš„æœºå™¨å­¦ä¹ è®­ç»ƒã€‚ChannelFlow-Toolså°†ä»¥å¾€é›¶æ•£çš„æ•°æ®é›†åˆ›å»ºè¿‡ç¨‹è½¬åŒ–ä¸ºäº†ä¸€ä¸ªé¢å‘è®¡ç®—æµä½“åŠ¨åŠ›å­¦(CFD)ä»£ç†æ¨¡å‹æ„å»ºçš„å¯å¤ç°ã€å¯é…ç½®çš„æµæ°´çº¿ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.15236v1",
      "published_date": "2025-09-17 13:18:05 UTC",
      "updated_date": "2025-09-17 13:18:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:11:46.373993+00:00"
    },
    {
      "arxiv_id": "2509.13941v1",
      "title": "An Empirical Study on Failures in Automated Issue Solving",
      "title_zh": "è‡ªåŠ¨åŒ–é—®é¢˜ä¿®å¤å¤±æ•ˆæƒ…å†µçš„å®è¯ç ”ç©¶",
      "authors": [
        "Simiao Liu",
        "Fang Liu",
        "Liehao Li",
        "Xin Tan",
        "Yinghao Zhu",
        "Xiaoli Lian",
        "Li Zhang"
      ],
      "abstract": "Automated issue solving seeks to autonomously identify and repair defective code snippets across an entire codebase. SWE-Bench has emerged as the most widely adopted benchmark for evaluating progress in this area. While LLM-based agentic tools show great promise, they still fail on a substantial portion of tasks. Moreover, current evaluations primarily report aggregate issue-solving rates, which obscure the underlying causes of success and failure, making it challenging to diagnose model weaknesses or guide targeted improvements. To bridge this gap, we first analyze the performance and efficiency of three SOTA tools, spanning both pipeline-based and agentic architectures, in automated issue solving tasks of SWE-Bench-Verified under varying task characteristics. Furthermore, to move from high-level performance metrics to underlying cause analysis, we conducted a systematic manual analysis of 150 failed instances. From this analysis, we developed a comprehensive taxonomy of failure modes comprising 3 primary phases, 9 main categories, and 25 fine-grained subcategories. Then we systematically analyze the distribution of the identified failure modes, the results reveal distinct failure fingerprints between the two architectural paradigms, with the majority of agentic failures stemming from flawed reasoning and cognitive deadlocks. Motivated by these insights, we propose a collaborative Expert-Executor framework. It introduces a supervisory Expert agent tasked with providing strategic oversight and course-correction for a primary Executor agent. This architecture is designed to correct flawed reasoning and break the cognitive deadlocks that frequently lead to failure. Experiments show that our framework solves 22.2% of previously intractable issues for a leading single agent. These findings pave the way for building more robust agents through diagnostic evaluation and collaborative design.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é—®é¢˜ä¿®å¤(Automated issue solving)ä¸­LLMæ™ºèƒ½ä½“å·¥å…·åœ¨é«˜éš¾åº¦ä»»åŠ¡ä¸Šçš„å¤±æ•ˆé—®é¢˜ï¼Œåœ¨SWE-Bench-VerifiedåŸºå‡†ä¸Šå¯¹ä¸‰ç§SOTAå·¥å…·è¿›è¡Œäº†æ·±å…¥çš„å®è¯ç ”ç©¶ã€‚é€šè¿‡å¯¹150ä¸ªå¤±è´¥æ¡ˆä¾‹çš„ç³»ç»Ÿæ€§äººå·¥åˆ†æï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªæ¶µç›–3ä¸ªé˜¶æ®µã€9ä¸ªä¸»ç±»åŠ25ä¸ªç»†åˆ†ç»´åº¦çš„å¤±è´¥æ¨¡å¼åˆ†ç±»æ³•(Taxonomy)ï¼Œå¹¶å‘ç°æ™ºèƒ½ä½“æ¶æ„çš„å¤±è´¥ä¸»è¦æºäºæ¨ç†ç¼ºé™·å’Œè®¤çŸ¥æ­»é”(Cognitive deadlocks)ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä½œå¼çš„Expert-Executoræ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç›‘ç£ä¸“å®¶æ™ºèƒ½ä½“ä¸ºæ‰§è¡Œæ™ºèƒ½ä½“æä¾›æˆ˜ç•¥æŒ‡å¯¼ä¸è·¯å¾„çº åï¼Œä»¥æ‰“ç ´è®¤çŸ¥ç“¶é¢ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æˆåŠŸè§£å†³äº†é¢†å…ˆå•æ™ºèƒ½ä½“æ­¤å‰æ— æ³•å¤„ç†çš„22.2%çš„éš¾é¢˜ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡å®šæ€§è¯Šæ–­ä¸åä½œå¼è®¾è®¡ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆçš„è‡ªä¸»è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“æä¾›äº†å…³é”®è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13941v1",
      "published_date": "2025-09-17 13:07:52 UTC",
      "updated_date": "2025-09-17 13:07:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:11:53.989598+00:00"
    },
    {
      "arxiv_id": "2509.13927v2",
      "title": "DSpAST: Disentangled Representations for Spatial Audio Reasoning with Large Language Models",
      "title_zh": "DSpASTï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´éŸ³é¢‘æ¨ç†è§£è€¦è¡¨ç¤º",
      "authors": [
        "Kevin Wilkinghoff",
        "Zheng-Hua Tan"
      ],
      "abstract": "Reasoning about spatial audio with large language models requires a spatial audio encoder as an acoustic front-end to obtain audio embeddings for further processing. Such an encoder needs to capture all information required to detect the type of sound events, as well as the direction and distance of their corresponding sources. Accomplishing this with a single audio encoder is demanding as the information required for each of these tasks is mostly independent of each other. As a result, the performance obtained with a single encoder is often worse than when using task-specific audio encoders. In this work, we present DSpAST, a novel audio encoder based on SpatialAST that learns disentangled representations of spatial audio while having only 0.2% additional parameters. Experiments on SpatialSoundQA with the spatial audio reasoning system BAT demonstrate that DSpAST significantly outperforms SpatialAST.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DSpASTï¼Œä¸€ç§åŸºäº SpatialAST çš„æ–°å‹éŸ³é¢‘ç¼–ç å™¨ï¼Œæ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç©ºé—´éŸ³é¢‘æ¨ç† (spatial audio reasoning) ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰å•ä¸€ç¼–ç å™¨éš¾ä»¥åŒæ—¶æ•è·å£°æºç±»å‹ã€æ–¹å‘å’Œè·ç¦»ç­‰ç›¸äº’ç‹¬ç«‹ä¿¡æ¯çš„é—®é¢˜ï¼ŒDSpAST é€šè¿‡å­¦ä¹ è§£è€¦è¡¨å¾ (disentangled representations) å®ç°äº†å¯¹ç©ºé—´ç‰¹å¾çš„é«˜æ•ˆå»ºæ¨¡ã€‚è¯¥æ–¹æ³•åœ¨ä»…å¼•å…¥ 0.2% é¢å¤–å‚æ•°çš„å‰æä¸‹ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šä»»åŠ¡ä¿¡æ¯æå–æ—¶çš„ç›¸äº’å¹²æ‰°ã€‚åœ¨ SpatialSoundQA æ•°æ®é›†ä¸Šç»“åˆæ¨ç†ç³»ç»Ÿ BAT çš„å®éªŒè¡¨æ˜ï¼ŒDSpAST çš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ SpatialASTã€‚è¿™ä¸€ç ”ç©¶ä¸ºæå‡å¤šæ¨¡æ€æ¨¡å‹å¯¹å¤æ‚ç©ºé—´éŸ³é¢‘ç¯å¢ƒçš„æ„ŸçŸ¥ä¸ç†è§£èƒ½åŠ›æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13927v2",
      "published_date": "2025-09-17 12:51:51 UTC",
      "updated_date": "2025-11-01 10:57:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:11:54.375745+00:00"
    },
    {
      "arxiv_id": "2509.13926v1",
      "title": "MAP: End-to-End Autonomous Driving with Map-Assisted Planning",
      "title_zh": "MAPï¼šåŸºäºåœ°å›¾è¾…åŠ©è§„åˆ’çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶",
      "authors": [
        "Huilin Yin",
        "Yiming Kan",
        "Daniel Watzenig"
      ],
      "abstract": "In recent years, end-to-end autonomous driving has attracted increasing attention for its ability to jointly model perception, prediction, and planning within a unified framework. However, most existing approaches underutilize the online mapping module, leaving its potential to enhance trajectory planning largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel map-assisted end-to-end trajectory planning framework. MAP explicitly integrates segmentation-based map features and the current ego status through a Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and a Weight Adapter based on current ego status. Experiments conducted on the DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6% reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a 44.5% improvement in overall score compared to the UniV2X baseline, even without post-processing. Furthermore, it achieves top ranking in Track 2 of the End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of overall score. These results highlight the effectiveness of explicitly leveraging semantic map features in planning and suggest new directions for improving structure design in end-to-end autonomous driving systems. Our code is available at https://gitee.com/kymkym/map.git",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MAP (Map-Assisted Planning)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„åœ°å›¾è¾…åŠ©ç«¯åˆ°ç«¯ (end-to-end) è½¨è¿¹è§„åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­åœ¨çº¿åœ°å›¾æ¨¡å—å¯¹è§„åˆ’å¢å¼ºæ½œåŠ›æŒ–æ˜ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ Plan-enhancing Online Mapping æ¨¡å—ã€Ego-status-guided Planning æ¨¡å—ä»¥åŠåŸºäºè½¦è¾†çŠ¶æ€çš„ Weight Adapterï¼Œæ˜¾å¼åœ°å°†åœ°å›¾åˆ†å‰²ç‰¹å¾ä¸è½¦è¾†å½“å‰çŠ¶æ€è¿›è¡Œé›†æˆã€‚åœ¨ DAIR-V2X-seq-SPD æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œä¸ UniV2X åŸºå‡†ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ— åå¤„ç†çš„æƒ…å†µä¸‹å°† L2 displacement error é™ä½äº† 16.6%ï¼Œoff-road rate é™ä½äº† 56.2%ï¼Œæ€»åˆ†æå‡äº† 44.5%ã€‚æ­¤å¤–ï¼ŒMAP åœ¨ CVPR2025 MEIS Workshop çš„æŒ‘æˆ˜èµ›ä¸­è£è·å† å†›ï¼Œæ€»åˆ†é¢†å…ˆç¬¬äºŒå 39.5%ã€‚ç ”ç©¶ç»“æœéªŒè¯äº†åœ¨è§„åˆ’ä¸­æ˜¾å¼åˆ©ç”¨è¯­ä¹‰åœ°å›¾ç‰¹å¾çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç»“æ„ä¼˜åŒ–æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 2 figures, accepted by ICCVW Author list updated to match the camera-ready version, in compliance with conference policy",
      "pdf_url": "https://arxiv.org/pdf/2509.13926v1",
      "published_date": "2025-09-17 11:40:46 UTC",
      "updated_date": "2025-09-17 11:40:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:11:59.456668+00:00"
    },
    {
      "arxiv_id": "2509.20369v1",
      "title": "AI-driven formative assessment and adaptive learning in data-science education: Evaluating an LLM-powered virtual teaching assistant",
      "title_zh": "æ•°æ®ç§‘å­¦æ•™è‚²ä¸­äººå·¥æ™ºèƒ½é©±åŠ¨çš„å½¢æˆæ€§è¯„ä»·ä¸è‡ªé€‚åº”å­¦ä¹ ï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„è™šæ‹ŸåŠ©æ•™è¯„ä¼°",
      "authors": [
        "Fadjimata I Anaroua",
        "Qing Li",
        "Yan Tang",
        "Hong P. Liu"
      ],
      "abstract": "This paper presents VITA (Virtual Teaching Assistants), an adaptive distributed learning (ADL) platform that embeds a large language model (LLM)-powered chatbot (BotCaptain) to provide dialogic support, interoperable analytics, and integrity-aware assessment for workforce preparation in data science. The platform couples context-aware conversational tutoring with formative-assessment patterns designed to promote reflective reasoning. The paper describes an end-to-end data pipeline that transforms chat logs into Experience API (xAPI) statements, instructor dashboards that surface outliers for just-in-time intervention, and an adaptive pathway engine that routes learners among progression, reinforcement, and remediation content. The paper also benchmarks VITA conceptually against emerging tutoring architectures, including retrieval-augmented generation (RAG)--based assistants and Learning Tools Interoperability (LTI)--integrated hubs, highlighting trade-offs among content grounding, interoperability, and deployment complexity. Contributions include a reusable architecture for interoperable conversational analytics, a catalog of patterns for integrity-preserving formative assessment, and a practical blueprint for integrating adaptive pathways into data-science courses. The paper concludes with implementation lessons and a roadmap (RAG integration, hallucination mitigation, and LTI~1.3 / OpenID Connect) to guide multi-course evaluations and broader adoption. In light of growing demand and scalability constraints in traditional instruction, the approach illustrates how conversational AI can support engagement, timely feedback, and personalized learning at scale. Future work will refine the platform's adaptive intelligence and examine applicability across varied educational settings.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† VITA (Virtual Teaching Assistants)ï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆå¤§è¯­è¨€æ¨¡å‹ (LLM) èŠå¤©æœºå™¨äºº BotCaptain çš„è‡ªé€‚åº”åˆ†å¸ƒå¼å­¦ä¹  (ADL) å¹³å°ï¼Œæ—¨åœ¨ä¸ºæ•°æ®ç§‘å­¦æ•™è‚²æä¾›å¯¹è¯å¼è¾…å¯¼ã€äº’æ“ä½œæ€§åˆ†æå’Œè¯šä¿¡æ„ŸçŸ¥çš„å½¢æˆæ€§è¯„ä¼° (formative assessment)ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç«¯åˆ°ç«¯æ•°æ®æµæ°´çº¿å°†èŠå¤©æ—¥å¿—è½¬æ¢ä¸º xAPI è¯­å¥ï¼Œåˆ©ç”¨æ•™å¸ˆä»ªè¡¨æ¿è¯†åˆ«å¼‚å¸¸å€¼ä»¥è¿›è¡Œå³æ—¶å¹²é¢„ï¼Œå¹¶ä¾æ‰˜è‡ªé€‚åº”è·¯å¾„å¼•æ“ (adaptive pathway engine) åœ¨è¿›é˜¶ã€å¼ºåŒ–å’Œè¡¥æ•‘å†…å®¹é—´åŠ¨æ€è·¯ç”±å­¦ä¹ è€…ã€‚è®ºæ–‡é€šè¿‡å°† VITA ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) å’Œå­¦ä¹ å·¥å…·äº’æ“ä½œæ€§ (LTI) ç­‰æ¶æ„è¿›è¡ŒåŸºå‡†å¯¹æ¯”ï¼Œæ·±å…¥æ¢è®¨äº†å†…å®¹æ‰å®æ€§ã€äº’æ“ä½œæ€§ä¸éƒ¨ç½²å¤æ‚åº¦ä¹‹é—´çš„æƒè¡¡ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ä¸€å¥—å¯é‡ç”¨çš„å¯¹è¯åˆ†ææ¶æ„ã€è¯šä¿¡ä¿æŠ¤çš„è¯„ä¼°æ¨¡å¼ç›®å½•ï¼Œä»¥åŠå°†è‡ªé€‚åº”è·¯å¾„æ•´åˆè‡³è¯¾ç¨‹çš„å®è·µè“å›¾ã€‚è¯¥ç ”ç©¶ä¸ºç¼“è§£ä¼ ç»Ÿæ•™å­¦çš„è§„æ¨¡åŒ–çº¦æŸæä¾›äº†äººå·¥æ™ºèƒ½æ–¹æ¡ˆï¼Œå¹¶è§„åˆ’äº†æœªæ¥åœ¨ RAG é›†æˆå’Œå¹»è§‰ç¼“è§£ (hallucination mitigation) æ–¹é¢çš„ä¼˜åŒ–æ–¹å‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20369v1",
      "published_date": "2025-09-17 11:27:45 UTC",
      "updated_date": "2025-09-17 11:27:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:12:04.267104+00:00"
    },
    {
      "arxiv_id": "2509.13914v1",
      "title": "Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction",
      "title_zh": "é¢å‘é•¿å°¾è½¨è¿¹é¢„æµ‹çš„é¢„è®­ç»ƒæ¨¡å‹é›†æˆ",
      "authors": [
        "Divya Thuremella",
        "Yi Yang",
        "Simon Wanna",
        "Lars Kunze",
        "Daniele De Martini"
      ],
      "abstract": "This work explores the application of ensemble modeling to the multidimensional regression problem of trajectory prediction for vehicles in urban environments. As newer and bigger state-of-the-art prediction models for autonomous driving continue to emerge, an important open challenge is the problem of how to combine the strengths of these big models without the need for costly re-training. We show how, perhaps surprisingly, combining state-of-the-art deep learning models out-of-the-box (without retraining or fine-tuning) with a simple confidence-weighted average method can enhance the overall prediction. Indeed, while combining trajectory prediction models is not straightforward, this simple approach enhances performance by 10% over the best prediction model, especially in the long-tailed metrics. We show that this performance improvement holds on both the NuScenes and Argoverse datasets, and that these improvements are made across the dataset distribution. The code for our work is open source.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é›†æˆå­¦ä¹ (Ensemble modeling)åœ¨åŸå¸‚ç¯å¢ƒä¸‹è½¦è¾†è½¨è¿¹é¢„æµ‹(Trajectory prediction)è¿™ä¸€å¤šç»´å›å½’é—®é¢˜ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹å¦‚ä½•æ— éœ€é«˜æ˜‚é‡è®­æˆæœ¬å³å¯ç»“åˆå¤§å‹æœ€å…ˆè¿›æ¨¡å‹ä¼˜åŠ¿çš„æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ç®€å•ä¸”æ•ˆæœæ˜¾è‘—çš„ç½®ä¿¡åº¦åŠ æƒå¹³å‡æ–¹æ³•(Confidence-weighted average)ï¼Œå®ç°äº†å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„å¼€ç®±å³ç”¨å¼ç»„åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨æ— éœ€é‡è®­æˆ–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œç›¸è¾ƒäºå•ä¸€æœ€ä½³æ¨¡å‹æ€§èƒ½æå‡äº†10%ï¼Œå¹¶åœ¨é•¿å°¾æŒ‡æ ‡(Long-tailed metrics)æ–¹é¢å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚ç ”ç©¶åœ¨NuSceneså’ŒArgoverseæ•°æ®é›†ä¸Šå‡éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸”æ€§èƒ½æå‡è¦†ç›–äº†æ•´ä¸ªæ•°æ®é›†åˆ†å¸ƒã€‚è¯¥å·¥ä½œä¸ä»…è¯æ˜äº†ç®€å•é›†æˆç­–ç•¥åœ¨å¤„ç†å¤æ‚è‡ªåŠ¨é©¾é©¶é¢„æµ‹ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œè¿˜ä¸ºåç»­ç ”ç©¶æä¾›äº†å¼€æºçš„ä»£ç å®ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted 2025 IEEE International Conference on Intelligent Transportation Systems (ITSC 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.13914v1",
      "published_date": "2025-09-17 11:18:16 UTC",
      "updated_date": "2025-09-17 11:18:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:12:09.798134+00:00"
    },
    {
      "arxiv_id": "2509.13905v1",
      "title": "Do Large Language Models Understand Word Senses?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦ç†è§£è¯ä¹‰ï¼Ÿ",
      "authors": [
        "Domenico Meconi",
        "Simone Stirpe",
        "Federico Martelli",
        "Leonardo Lavalle",
        "Roberto Navigli"
      ],
      "abstract": "Understanding the meaning of words in context is a fundamental capability for Large Language Models (LLMs). Despite extensive evaluation efforts, the extent to which LLMs show evidence that they truly grasp word senses remains underexplored. In this paper, we address this gap by evaluating both i) the Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs, comparing their performance to state-of-the-art systems specifically designed for the task, and ii) the ability of two top-performing open- and closed-source LLMs to understand word senses in three generative settings: definition generation, free-form explanation, and example generation. Notably, we find that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve performance on par with specialized WSD systems, while also demonstrating greater robustness across domains and levels of difficulty. In the generation tasks, results reveal that LLMs can explain the meaning of words in context up to 98\\% accuracy, with the highest performance observed in the free-form explanation task, which best aligns with their generative capabilities.",
      "tldr_zh": "æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç†è§£ä¸Šä¸‹æ–‡è¯­å¢ƒä¸­è¯ä¹‰(Word Senses)çš„èƒ½åŠ›ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç ”ç©¶çš„ç©ºç™½ã€‚ä½œè€…é¦–å…ˆè¯„ä¼°äº†æŒ‡ä»¤å¾®è°ƒLLMsåœ¨è¯ä¹‰æ¶ˆæ­§(Word Sense Disambiguation, WSD)ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶å°†å…¶ä¸ä¸“é—¨è®¾è®¡çš„SOTAç³»ç»Ÿè¿›è¡Œäº†å¯¹æ¯”ã€‚æ¥ç€ï¼Œç ”ç©¶æµ‹è¯•äº†GPT-4oå’ŒDeepSeek-V3åœ¨å®šä¹‰ç”Ÿæˆ(definition generation)ã€è‡ªç”±æ ¼å¼è§£é‡Š(free-form explanation)å’Œç¤ºä¾‹ç”Ÿæˆ(example generation)ä¸‰ç§ç”Ÿæˆåœºæ™¯ä¸‹çš„è¯ä¹‰ç†è§£åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¢†å…ˆçš„LLMsåœ¨WSDä»»åŠ¡ä¸­çš„è¡¨ç°å·²ä¸ä¸“ä¸šç³»ç»Ÿé½å¹³ï¼Œä¸”åœ¨è·¨é¢†åŸŸå’Œä¸åŒéš¾åº¦ä¸‹è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§(robustness)ã€‚åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒLLMsè§£é‡Šä¸Šä¸‹æ–‡è¯ä¹‰çš„å‡†ç¡®ç‡é«˜è¾¾98%ï¼Œå…¶ä¸­è‡ªç”±æ ¼å¼è§£é‡Šä»»åŠ¡æœ€èƒ½å‘æŒ¥å…¶ç”Ÿæˆèƒ½åŠ›çš„ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é¡¶å°–LLMså·²å…·å¤‡æ·±å…¥ç†è§£è¯ä¹‰çš„èƒ½åŠ›ï¼Œå±•ç°äº†å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ ¸å¿ƒä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, to be published in EMNLP2025",
      "pdf_url": "https://arxiv.org/pdf/2509.13905v1",
      "published_date": "2025-09-17 11:11:27 UTC",
      "updated_date": "2025-09-17 11:11:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:12:22.690289+00:00"
    },
    {
      "arxiv_id": "2509.13895v1",
      "title": "FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning",
      "title_zh": "FedSSGï¼šé¢å‘è”é‚¦å­¦ä¹ çš„æœŸæœ›é—¨æ§ä¸å†å²æ„ŸçŸ¥æ¼‚ç§»å¯¹é½",
      "authors": [
        "Zhanting Zhou",
        "Jinshan Lai",
        "Fengchun Zhang",
        "Zeqin Wu",
        "Fengli Zhang"
      ],
      "abstract": "Non-IID data and partial participation induce client drift and inconsistent local optima in federated learning, causing unstable convergence and accuracy loss. We present FedSSG, a stochastic sampling-guided, history-aware drift alignment method. FedSSG maintains a per-client drift memory that accumulates local model differences as a lightweight sketch of historical gradients; crucially, it gates both the memory update and the local alignment term by a smooth function of the observed/expected participation ratio (a phase-by-expectation signal derived from the server sampler). This statistically grounded gate stays weak and smooth when sampling noise dominates early, then strengthens once participation statistics stabilize, contracting the local-global gap without extra communication. Across CIFAR-10/100 with 100/500 clients and 2-15 percent participation, FedSSG consistently outperforms strong drift-aware baselines and accelerates convergence; on our benchmarks it improves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and about +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about 4.5x faster target-accuracy convergence on average. The method adds only O(d) client memory and a constant-time gate, and degrades gracefully to a mild regularizer under near-IID or uniform sampling. FedSSG shows that sampling statistics can be turned into a principled, history-aware phase control to stabilize and speed up federated training.",
      "tldr_zh": "è”é‚¦å­¦ä¹ ä¸­éç‹¬ç«‹åŒåˆ†å¸ƒ(Non-IID)æ•°æ®å’Œéƒ¨åˆ†å‚ä¸(partial participation)ä¼šå¯¼è‡´å®¢æˆ·ç«¯åç§»(client drift)å’Œå±€éƒ¨æœ€ä¼˜ä¸ä¸€è‡´ï¼Œè¿›è€Œé€ æˆæ”¶æ•›ä¸ç¨³å®šå’Œå‡†ç¡®ç‡æŸå¤±ã€‚è¯¥ç ”ç©¶æå‡ºäº†FedSSGï¼Œä¸€ç§åŸºäºéšæœºé‡‡æ ·å¼•å¯¼(stochastic sampling-guided)å’Œå†å²æ„ŸçŸ¥(history-aware)çš„åç§»å¯¹é½æ–¹æ³•ã€‚FedSSGä¸ºæ¯ä¸ªå®¢æˆ·ç«¯ç»´æŠ¤ä¸€ä¸ªåç§»è®°å¿†(drift memory)ä»¥ç´¯ç§¯æ¢¯åº¦å†å²ï¼Œå¹¶é€šè¿‡è§‚å¯Ÿä¸é¢„æœŸå‚ä¸ç‡çš„å¹³æ»‘å‡½æ•°åŠ¨æ€é—¨æ§å¯¹é½å¼ºåº¦ï¼Œåœ¨æ— éœ€é¢å¤–é€šä¿¡çš„æƒ…å†µä¸‹ç¼©å°å±€éƒ¨ä¸å…¨å±€çš„å·®è·ã€‚è¯¥æ–¹æ³•ä»…å¢åŠ O(d)çš„å­˜å‚¨å¼€é”€ï¼Œä¸”åœ¨é‡‡æ ·ç»Ÿè®¡ç¨³å®šåèƒ½æœ‰æ•ˆåŠ é€Ÿæ”¶æ•›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedSSGåœ¨CIFAR-10/100åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå¤šç§å¼ºåŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡æ˜¾è‘—ï¼Œä¸”æ”¶æ•›é€Ÿåº¦å¹³å‡æé«˜äº†4.5å€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨é‡‡æ ·ç»Ÿè®¡æ•°æ®è¿›è¡Œç›¸ä½æ§åˆ¶å¯ä»¥æœ‰æ•ˆç¨³å®šå¹¶åŠ é€Ÿè”é‚¦å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "4 page main text for conference",
      "pdf_url": "https://arxiv.org/pdf/2509.13895v1",
      "published_date": "2025-09-17 10:43:05 UTC",
      "updated_date": "2025-09-17 10:43:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:12:44.399124+00:00"
    },
    {
      "arxiv_id": "2509.13892v1",
      "title": "Synthetic Data Generation for Screen Time and App Usage",
      "title_zh": "å±å¹•ä½¿ç”¨æ—¶é—´ä¸åº”ç”¨ä½¿ç”¨æƒ…å†µçš„åˆæˆæ•°æ®ç”Ÿæˆ",
      "authors": [
        "Gustavo Kruger",
        "Nikhil Sachdeva",
        "Michael Sobolev"
      ],
      "abstract": "Smartphone usage data can provide valuable insights for understanding interaction with technology and human behavior. However, collecting large-scale, in-the-wild smartphone usage logs is challenging due to high costs, privacy concerns, under representative user samples and biases like non-response that can skew results. These challenges call for exploring alternative approaches to obtain smartphone usage datasets. In this context, large language models (LLMs) such as Open AI's ChatGPT present a novel approach for synthetic smartphone usage data generation, addressing limitations of real-world data collection. We describe a case study on how four prompt strategies influenced the quality of generated smartphone usage data. We contribute with insights on prompt design and measures of data quality, reporting a prompting strategy comparison combining two factors, prompt level of detail (describing a user persona, describing the expected results characteristics) and seed data inclusion (with versus without an initial real usage example). Our findings suggest that using LLMs to generate structured and behaviorally plausible smartphone use datasets is feasible for some use cases, especially when using detailed prompts. Challenges remain in capturing diverse nuances of human behavioral patterns in a single synthetic dataset, and evaluating tradeoffs between data fidelity and diversity, suggesting the need for use-case-specific evaluation metrics and future research with more diverse seed data and different LLM models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆåˆæˆæ™ºèƒ½æ‰‹æœºä½¿ç”¨æ•°æ®çš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨è§£å†³çœŸå®æ•°æ®æ”¶é›†é¢ä¸´çš„é«˜æˆæœ¬ã€éšç§å…³æ³¨åŠæ ·æœ¬åå·®ç­‰æŒ‘æˆ˜ã€‚é€šè¿‡ä¸€é¡¹æ¡ˆä¾‹ç ”ç©¶ï¼Œä½œè€…æ¯”è¾ƒäº†å››ç§æç¤ºè¯ç­–ç•¥(prompt strategies)ï¼Œé‡ç‚¹è€ƒå¯Ÿäº†æç¤ºè¯è¯¦ç»†ç¨‹åº¦ï¼ˆå¦‚æè¿°ç”¨æˆ·ç”»åƒ personaï¼‰ä»¥åŠæ˜¯å¦åŒ…å«ç§å­æ•°æ®(seed data)å¯¹ç”Ÿæˆæ•°æ®è´¨é‡çš„å½±å“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ LLMs ç”Ÿæˆç»“æ„åŒ–ä¸”è¡Œä¸ºåˆç†çš„æ‰‹æœºä½¿ç”¨æ•°æ®é›†åœ¨ç‰¹å®šåœºæ™¯ä¸‹æ˜¯å¯è¡Œçš„ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨è¯¦ç»†æç¤ºè¯æ—¶æ•ˆæœæ˜¾è‘—ã€‚å°½ç®¡å¦‚æ­¤ï¼Œåˆæˆæ•°æ®åœ¨æ•æ‰äººç±»è¡Œä¸ºæ¨¡å¼çš„å¤šæ ·æ€§ç»†å¾®å·®åˆ«ä»¥åŠå¹³è¡¡æ•°æ®ä¿çœŸåº¦(fidelity)ä¸å¤šæ ·æ€§(diversity)æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶ä¸ºç§»åŠ¨äº¤äº’é¢†åŸŸçš„åˆæˆæ•°æ®ç”Ÿæˆæä¾›äº†é‡è¦è§è§£ï¼Œå¹¶æŒ‡å‡ºæœªæ¥éœ€è¦é’ˆå¯¹å…·ä½“ç”¨ä¾‹å¼€å‘ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "14 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.13892v1",
      "published_date": "2025-09-17 10:42:06 UTC",
      "updated_date": "2025-09-17 10:42:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:12:32.982677+00:00"
    },
    {
      "arxiv_id": "2509.13888v1",
      "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification",
      "title_zh": "é€šè¿‡å¤šæ¨¡æ€å£°æ˜æ£€æµ‹ä¸å¾ªè¯éªŒè¯å¯¹æŠ—ç”Ÿç‰©åŒ»å­¦è™šå‡ä¿¡æ¯",
      "authors": [
        "Mariano Barone",
        "Antonio Romano",
        "Giuseppe Riccio",
        "Marco Postiglione",
        "Vincenzo Moscato"
      ],
      "abstract": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https://github.com/PRAISELab-PicusLab/CER",
      "tldr_zh": "åŒ»ç–—ä¿å¥é¢†åŸŸçš„è™šå‡ä¿¡æ¯å¯¹å…¬å…±å«ç”Ÿå’ŒåŒ»ç–—ç³»ç»Ÿä¿¡ä»»æ„æˆä¸¥é‡é£é™©ï¼Œä½†ç”Ÿç‰©åŒ»å­¦å£°æ˜çš„è‡ªåŠ¨äº‹å®æ ¸æŸ¥å› æœ¯è¯­å¤æ‚ã€éœ€è¦é¢†åŸŸä¸“å®¶çŸ¥è¯†ä»¥åŠä¸¥è°¨çš„ç§‘å­¦è¯æ®æ”¯æŒè€Œé¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æå‡ºäº† CER (Combining Evidence and Reasoning) æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºç”Ÿç‰©åŒ»å­¦äº‹å®æ ¸æŸ¥è®¾è®¡çš„æ–°å‹æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆç§‘å­¦è¯æ®æ£€ç´¢ (scientific evidence retrieval)ã€å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) æ¨ç†ä»¥åŠç›‘ç£å¼çœŸå®æ€§é¢„æµ‹ (supervised veracity prediction)ï¼Œå®ç°äº†å¯¹å£°æ˜çš„æœ‰æ•ˆéªŒè¯ã€‚é€šè¿‡å°† LLMs çš„ç”Ÿæˆèƒ½åŠ›ä¸å…ˆè¿›çš„é«˜è´¨é‡ç”Ÿç‰©åŒ»å­¦ç§‘å­¦è¯æ®æ£€ç´¢æŠ€æœ¯ç›¸ç»“åˆï¼ŒCER æœ‰æ•ˆç¼“è§£äº†å¹»è§‰ (hallucinations) é£é™©ï¼Œç¡®ä¿è¾“å‡ºç»“æœå§‹ç»ˆæ ¹æ¤äºå¯éªŒè¯çš„è¯æ®æ¥æºã€‚åœ¨ HealthFCã€BioASQ-7b å’Œ SciFact ç­‰ä¸“å®¶æ ‡æ³¨æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCER å–å¾—äº†æœ€å…ˆè¿›çš„ (State-of-the-Art) æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ¡†æ¶å…·æœ‰å‡ºè‰²çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ› (cross-dataset generalization)ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦è™šå‡ä¿¡æ¯çš„æ²»ç†æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13888v1",
      "published_date": "2025-09-17 10:31:09 UTC",
      "updated_date": "2025-09-17 10:31:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:12:39.398366+00:00"
    },
    {
      "arxiv_id": "2509.13880v1",
      "title": "An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques",
      "title_zh": "èåˆç®€åŒ–æŠ€æœ¯çš„æ•´æ•°çº¿æ€§çº¦æŸæ¨¡å‹è®¡æ•°ç©·ä¸¾ DPLL æ–¹æ³•",
      "authors": [
        "Mingwei Zhang",
        "Zhenhao Gu",
        "Liangda Fang",
        "Cunjing Ge",
        "Ziliang Chen",
        "Zhao-Rong Lai",
        "Quanlong Guan"
      ],
      "abstract": "Linear constraints are one of the most fundamental constraints in fields such as computer science, operations research and optimization. Many applications reduce to the task of model counting over integer linear constraints (MCILC). In this paper, we design an exact approach to MCILC based on an exhaustive DPLL architecture. To improve the efficiency, we integrate several effective simplification techniques from mixed integer programming into the architecture. We compare our approach to state-of-the-art MCILC counters and propositional model counters on 2840 random and 4131 application benchmarks. Experimental results show that our approach significantly outperforms all exact methods in random benchmarks solving 1718 instances while the state-of-the-art approach only computes 1470 instances. In addition, our approach is the only approach to solve all 4131 application instances.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•´æ•°çº¿æ€§çº¦æŸä¸‹çš„æ¨¡å‹è®¡æ•° (Model Counting over Integer Linear Constraints, MCILC) é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç©·ä¸¾å¼ DPLL (Exhaustive DPLL) æ¶æ„çš„ç²¾ç¡®æ±‚è§£æ–¹æ³•ã€‚ä¸ºäº†æå‡è®¡ç®—æ•ˆç‡ï¼Œè¯¥æ–¹æ³•å°†æ··åˆæ•´æ•°è§„åˆ’ (Mixed Integer Programming, MIP) ä¸­çš„å¤šç§æœ‰æ•ˆç®€åŒ–æŠ€æœ¯é›†æˆåˆ°äº† DPLL æ¶æ„ä¹‹ä¸­ã€‚ç ”ç©¶äººå‘˜åœ¨ 2840 ä¸ªéšæœºåŸºå‡†æµ‹è¯•å’Œ 4131 ä¸ªåº”ç”¨åŸºå‡†æµ‹è¯•ä¸Šï¼Œå°†è¯¥æ–¹æ³•ä¸æœ€å…ˆè¿›çš„ MCILC è®¡æ•°å™¨åŠå‘½é¢˜æ¨¡å‹è®¡æ•°å™¨è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨éšæœºåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ‰€æœ‰ç²¾ç¡®æ–¹æ³•ï¼ŒæˆåŠŸæ±‚è§£äº† 1718 ä¸ªå®ä¾‹ï¼Œè€Œæœ€å…ˆè¿›çš„ç«äº‰å¯¹æ‰‹ä»…èƒ½å¤„ç† 1470 ä¸ªã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¹Ÿæ˜¯å”¯ä¸€èƒ½å¤Ÿå®Œæ•´è§£å†³å…¨éƒ¨ 4131 ä¸ªåº”ç”¨å®ä¾‹çš„æ–¹æ¡ˆï¼Œå±•ç°äº†åœ¨å¤„ç†å¤æ‚å®é™…åº”ç”¨åœºæ™¯æ—¶çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13880v1",
      "published_date": "2025-09-17 10:19:06 UTC",
      "updated_date": "2025-09-17 10:19:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:12:47.490561+00:00"
    },
    {
      "arxiv_id": "2509.13879v1",
      "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking",
      "title_zh": "ç»“åˆè¯æ®ä¸æ¨ç†çš„ç”Ÿç‰©åŒ»å­¦äº‹å®æ ¸æŸ¥",
      "authors": [
        "Mariano Barone",
        "Antonio Romano",
        "Giuseppe Riccio",
        "Marco Postiglione",
        "Vincenzo Moscato"
      ],
      "abstract": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https: //github.com/PRAISELab-PicusLab/CER.",
      "tldr_zh": "é’ˆå¯¹åŒ»ç–—å¥åº·é¢†åŸŸçš„è¯¯å¯¼ä¿¡æ¯ä»¥åŠç”Ÿç‰©åŒ»å­¦äº‹å®æ ¸æŸ¥åœ¨å¤æ‚æœ¯è¯­å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†æ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºCER (Combining Evidence and Reasoning) çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°é›†æˆäº†ç§‘å­¦è¯æ®æ£€ç´¢(scientific evidence retrieval)ã€åŸºäºå¤§è¯­è¨€æ¨¡å‹(large language models)çš„æ¨ç†ä»¥åŠç›‘ç£ä¸‹çš„çœŸå®æ€§é¢„æµ‹(supervised veracity prediction)ã€‚é€šè¿‡å°†å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ä¸é«˜è´¨é‡ç”Ÿç‰©åŒ»å­¦è¯æ®çš„é«˜çº§æ£€ç´¢æŠ€æœ¯ç›¸ç»“åˆï¼ŒCERæœ‰æ•ˆåœ°ç¼“è§£äº†å¹»è§‰(hallucinations)é£é™©ï¼Œç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºç«‹è¶³äºå¯éªŒè¯ä¸”åŸºäºè¯æ®çš„æ¥æºã€‚åœ¨HealthFCã€BioASQ-7bå’ŒSciFactç­‰ä¸“å®¶æ ‡æ³¨æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCERè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½(state-of-the-art performance)ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å±•ç°å‡ºäº†è‰¯å¥½çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ„å»ºå¯é ã€é€æ˜çš„ç”Ÿç‰©åŒ»å­¦äº‹å®æ ¸æŸ¥ç³»ç»Ÿæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.13879v1",
      "published_date": "2025-09-17 10:14:56 UTC",
      "updated_date": "2025-09-17 10:14:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:12:52.695385+00:00"
    },
    {
      "arxiv_id": "2509.18168v1",
      "title": "HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics",
      "title_zh": "HSGMï¼šé¢å‘å¯æ‰©å±•é•¿æ–‡æœ¬è¯­ä¹‰çš„åˆ†å±‚åˆ†æ®µå›¾è®°å¿†",
      "authors": [
        "Dong Liu",
        "Yanxuan Yu"
      ],
      "abstract": "Semantic parsing of long documents remains challenging due to quadratic growth in pairwise composition and memory requirements. We introduce \\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that decomposes an input of length $N$ into $M$ meaningful segments, constructs \\emph{Local Semantic Graphs} on each segment, and extracts compact \\emph{summary nodes} to form a \\emph{Global Graph Memory}. HSGM supports \\emph{incremental updates} -- only newly arrived segments incur local graph construction and summary-node integration -- while \\emph{Hierarchical Query Processing} locates relevant segments via top-$K$ retrieval over summary nodes and then performs fine-grained reasoning within their local graphs.\n  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to $O\\!\\left(N\\,k + (N/k)^2\\right)$, with segment size $k \\ll N$, and we derive Frobenius-norm bounds on the approximation error introduced by node summarization and sparsification thresholds. Empirically, on three benchmarks -- long-document AMR parsing, segment-level semantic role labeling (OntoNotes), and legal event extraction -- HSGM achieves \\emph{2--4$\\times$ inference speedup}, \\emph{$>60\\%$ reduction} in peak memory, and \\emph{$\\ge 95\\%$} of baseline accuracy. Our approach unlocks scalable, accurate semantic modeling for ultra-long texts, enabling real-time and resource-constrained NLP applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿æ–‡æ¡£è¯­ä¹‰è§£æä¸­å­˜åœ¨çš„å¹³æ–¹çº§è®¡ç®—å¤æ‚åº¦å’Œé«˜å†…å­˜éœ€æ±‚æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º Hierarchical Segment-Graph Memory (HSGM) çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†é•¿åº¦ä¸º $N$ çš„è¾“å…¥åˆ†è§£ä¸ºå¤šä¸ªæœ‰æ„ä¹‰çš„åˆ†æ®µ (segments)ï¼Œåœ¨æ¯ä¸ªåˆ†æ®µä¸Šæ„å»ºå±€éƒ¨è¯­ä¹‰å›¾ (Local Semantic Graphs)ï¼Œå¹¶æå–ç´§å‡‘çš„æ‘˜è¦èŠ‚ç‚¹ (summary nodes) ä»¥å½¢æˆå…¨å±€å›¾å†…å­˜ (Global Graph Memory)ã€‚HSGM æ”¯æŒå¢é‡æ›´æ–° (incremental updates)ï¼Œå¹¶é€šè¿‡å±‚æ¬¡åŒ–æŸ¥è¯¢å¤„ç†å®ç° top-$K$ æ£€ç´¢å®šä½ä¸å±€éƒ¨å›¾çš„ç»†ç²’åº¦æ¨ç†ã€‚ç†è®ºåˆ†æè¯æ˜ HSGM å°†æœ€åæƒ…å†µä¸‹çš„å¤æ‚åº¦ä» $O(N^2)$ æ˜¾è‘—é™ä½ï¼Œå¹¶ä¸ºèŠ‚ç‚¹æ‘˜è¦åŒ–äº§ç”Ÿçš„è¿‘ä¼¼è¯¯å·®æä¾›äº† Frobenius-norm ç†è®ºç•Œé™ã€‚åœ¨é•¿æ–‡æ¡£ AMR è§£æã€OntoNotes è¯­ä¹‰è§’è‰²æ ‡æ³¨å’Œæ³•å¾‹äº‹ä»¶æå–å®éªŒä¸­ï¼Œè¯¥æ¨¡å‹å®ç°äº† 2-4 å€çš„æ¨ç†åŠ é€Ÿå’Œè¶…è¿‡ 60% çš„å³°å€¼å†…å­˜é™ä½ï¼ŒåŒæ—¶ä¿æŒäº†åŸºçº¿æ¨¡å‹ 95% ä»¥ä¸Šçš„å‡†ç¡®ç‡ã€‚è¿™ä¸€æˆæœä¸ºè¶…é•¿æ–‡æœ¬çš„é«˜æ•ˆè¯­ä¹‰å»ºæ¨¡æä¾›äº†æ–°æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ”¯æŒå®æ—¶ä¸”èµ„æºå—é™çš„ NLP åº”ç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18168v1",
      "published_date": "2025-09-17 10:11:02 UTC",
      "updated_date": "2025-09-17 10:11:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:12:59.095790+00:00"
    },
    {
      "arxiv_id": "2509.13866v2",
      "title": "Masked Diffusion Models as Energy Minimization",
      "title_zh": "æ©ç æ‰©æ•£æ¨¡å‹ï¼šèƒ½é‡æœ€å°åŒ–è§†è§’",
      "authors": [
        "Sitong Chen",
        "Shen Nie",
        "Jiacheng Sun",
        "Zijin Feng",
        "Zhenguo Li",
        "Ji-Rong Wen",
        "Chongxuan Li"
      ],
      "abstract": "We present a systematic theoretical framework that interprets masked diffusion models (MDMs) as solutions to energy minimization problems in discrete optimal transport. Specifically, we prove that three distinct energy formulations--kinetic, conditional kinetic, and geodesic energy--are mathematically equivalent under the structure of MDMs, and that MDMs minimize all three when the mask schedule satisfies a closed-form optimality condition. This unification not only clarifies the theoretical foundations of MDMs, but also motivates practical improvements in sampling. By parameterizing interpolation schedules via Beta distributions, we reduce the schedule design space to a tractable 2D search, enabling efficient post-training tuning without model modification. Experiments on synthetic and real-world benchmarks demonstrate that our energy-inspired schedules outperform hand-crafted baselines, particularly in low-step sampling settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„ç†è®ºæ¡†æ¶ï¼Œå°† Masked Diffusion Models (MDMs) è§£é‡Šä¸ºç¦»æ•£ Optimal Transport ä¸­çš„èƒ½é‡æœ€å°åŒ–é—®é¢˜ã€‚ä½œè€…åœ¨æ•°å­¦ä¸Šè¯æ˜äº†åŠ¨èƒ½ã€æ¡ä»¶åŠ¨èƒ½å’Œæµ‹åœ°çº¿èƒ½é‡è¿™ä¸‰ç§èƒ½é‡å…¬å¼åœ¨ MDMs ç»“æ„ä¸‹æ˜¯ç­‰ä»·çš„ï¼Œä¸”å½“ Mask Schedule æ»¡è¶³é—­å¼æœ€ä¼˜æ¡ä»¶æ—¶ï¼ŒMDMs èƒ½åŒæ—¶å®ç°è¿™ä¸‰è€…çš„æœ€å°åŒ–ã€‚è¿™ä¸€å‘ç°ä¸ä»…å¤¯å®äº† MDMs çš„ç†è®ºåŸºç¡€ï¼Œè¿˜å¯å‘äº†é‡‡æ ·æ€§èƒ½çš„å®é™…æ”¹è¿›ï¼Œå³é€šè¿‡ Beta åˆ†å¸ƒå¯¹æ’å€¼è°ƒåº¦è¿›è¡Œå‚æ•°åŒ–ï¼Œå°†è°ƒåº¦è®¾è®¡ç©ºé—´ç®€åŒ–ä¸ºå¯å¤„ç†çš„äºŒç»´æœç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å—èƒ½é‡å¯å‘çš„è°ƒåº¦æ–¹æ¡ˆæ— éœ€ä¿®æ”¹æ¨¡å‹å³å¯è¿›è¡Œé«˜æ•ˆçš„è®­ç»ƒåè°ƒä¼˜ã€‚åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ¡ˆå‡ä¼˜äºæ‰‹å·¥è®¾è®¡çš„åŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨ Low-step Sampling è®¾ç½®ä¸‹å±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13866v2",
      "published_date": "2025-09-17 09:57:31 UTC",
      "updated_date": "2025-11-27 12:18:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:13:12.097674+00:00"
    },
    {
      "arxiv_id": "2509.13854v1",
      "title": "Understanding the Process of Human-AI Value Alignment",
      "title_zh": "ç†è§£äººæœºä»·å€¼å¯¹é½çš„è¿‡ç¨‹",
      "authors": [
        "Jack McKinlay",
        "Marina De Vos",
        "Janina A. Hoffmann",
        "Andreas Theodorou"
      ],
      "abstract": "Background: Value alignment in computer science research is often used to refer to the process of aligning artificial intelligence with humans, but the way the phrase is used often lacks precision. Objectives: In this paper, we conduct a systematic literature review to advance the understanding of value alignment in artificial intelligence by characterising the topic in the context of its research literature. We use this to suggest a more precise definition of the term. Methods: We analyse 172 value alignment research articles that have been published in recent years and synthesise their content using thematic analyses. Results: Our analysis leads to six themes: value alignment drivers & approaches; challenges in value alignment; values in value alignment; cognitive processes in humans and AI; human-agent teaming; and designing and developing value-aligned systems. Conclusions: By analysing these themes in the context of the literature we define value alignment as an ongoing process between humans and autonomous agents that aims to express and implement abstract values in diverse contexts, while managing the cognitive limits of both humans and AI agents and also balancing the conflicting ethical and political demands generated by the values in different groups. Our analysis gives rise to a set of research challenges and opportunities in the field of value alignment for future work.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹172ç¯‡ç›¸å…³è®ºæ–‡è¿›è¡Œç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°(Systematic Literature Review)ï¼Œåˆ©ç”¨ä¸»é¢˜åˆ†æ(Thematic Analysis)æ·±å…¥æ¢è®¨äº†äººå·¥æ™ºèƒ½é¢†åŸŸä¸­ä»·å€¼å¯¹é½(Value Alignment)çš„å†…æ¶µä¸ç°çŠ¶ã€‚åˆ†æè¯†åˆ«å‡ºä»·å€¼å¯¹é½çš„é©±åŠ¨åŠ›ã€æŒ‘æˆ˜ã€æ ¸å¿ƒä»·å€¼ã€äººç±»ä¸AIçš„è®¤çŸ¥è¿‡ç¨‹ã€äººæœºåä½œ(Human-Agent Teaming)ä»¥åŠç³»ç»Ÿè®¾è®¡ç­‰å…­å¤§ä¸»é¢˜ã€‚åŸºäºè¿™äº›ä¸»é¢˜ï¼Œç ”ç©¶å°†ä»·å€¼å¯¹é½å®šä¹‰ä¸ºäººç±»ä¸è‡ªä¸»æ™ºèƒ½ä½“ä¹‹é—´ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹(Ongoing Process)ï¼Œå…¶ç›®æ ‡æ˜¯åœ¨ä¸åŒè¯­å¢ƒä¸‹å®ç°æŠ½è±¡ä»·å€¼ï¼ŒåŒæ—¶å¹³è¡¡è®¤çŸ¥å±€é™åŠå¤æ‚çš„ä¼¦ç†ä¸æ”¿æ²»éœ€æ±‚ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜ç³»ç»Ÿæ€»ç»“äº†è¯¥é¢†åŸŸå½“å‰é¢ä¸´çš„ç ”ç©¶æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„å­¦æœ¯æ¢ç´¢æä¾›äº†æ–¹å‘æ€§å»ºè®®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "39 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.13854v1",
      "published_date": "2025-09-17 09:39:38 UTC",
      "updated_date": "2025-09-17 09:39:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:13:08.698879+00:00"
    },
    {
      "arxiv_id": "2509.13805v2",
      "title": "Towards a Physics Foundation Model",
      "title_zh": "è¿ˆå‘ç‰©ç†åŸºç¡€æ¨¡å‹",
      "authors": [
        "Florian Wiesner",
        "Matthias Wessling",
        "Stephen Baek"
      ],
      "abstract": "Foundation models have revolutionized natural language processing through a ``train once, deploy anywhere'' paradigm, where a single pre-trained model adapts to countless downstream tasks without retraining. Access to a Physics Foundation Model (PFM) would be transformative -- democratizing access to high-fidelity simulations, accelerating scientific discovery, and eliminating the need for specialized solver development. Yet current physics-aware machine learning approaches remain fundamentally limited to single, narrow domains and require retraining for each new system. We present the General Physics Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that demonstrates foundation model capabilities are achievable for physics. Our key insight is that transformers can learn to infer governing dynamics from context, enabling a single model to simulate fluid-solid interactions, shock waves, thermal convection, and multi-phase dynamics without being told the underlying equations. GPhyT achieves three critical breakthroughs: (1) superior performance across multiple physics domains, outperforming specialized architectures by up to 29x, (2) zero-shot generalization to entirely unseen physical systems through in-context learning, and (3) stable long-term predictions through 50-timestep rollouts. By establishing that a single model can learn generalizable physical principles from data alone, this work opens the path toward a universal PFM that could transform computational science and engineering.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ„å»ºç‰©ç†åŸºç¡€æ¨¡å‹(Physics Foundation Model)çš„å¯èƒ½æ€§ï¼Œå¹¶æå‡ºäº†é€šç”¨ç‰©ç†Transformer (General Physics Transformer, GPhyT)ã€‚è¯¥æ¨¡å‹åœ¨1.8 TBçš„å¤šæ ·åŒ–æ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨Transformeræ¶æ„ä»ä¸Šä¸‹æ–‡ä¸­æ¨æ–­ç‰©ç†åŠ¨åŠ›å­¦ï¼Œä½¿å¾—å•ä¸€æ¨¡å‹æ— éœ€é¢„çŸ¥åº•å±‚æ–¹ç¨‹å³å¯æ¨¡æ‹Ÿæµå›ºè€¦åˆã€å†²å‡»æ³¢ã€çƒ­å¯¹æµåŠå¤šç›¸åŠ¨åŠ›å­¦ç­‰å¤æ‚ç°è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPhyTåœ¨å¤šä¸ªç‰©ç†é¢†åŸŸçš„è¡¨ç°ä¼˜äºä¸“é—¨åŒ–æ¶æ„é«˜è¾¾29å€ï¼Œå¹¶å®ç°äº†50æ­¥é¢„æµ‹çš„é•¿æœŸç¨³å®šã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬(zero-shot)æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡æƒ…å¢ƒå­¦ä¹ (in-context learning)å¤„ç†å®Œå…¨æœªè§çš„ç‰©ç†ç³»ç»Ÿã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†å•ä¸€æ¨¡å‹å¯ä»¥ä»…ä»æ•°æ®ä¸­å­¦ä¹ æ™®é€‚çš„ç‰©ç†åŸåˆ™ï¼Œä¸ºå¼€å‘èƒ½å˜é©è®¡ç®—ç§‘å­¦ä¸å·¥ç¨‹çš„é€šç”¨ç‰©ç†åŸºç¡€æ¨¡å‹å¼€è¾Ÿäº†é“è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13805v2",
      "published_date": "2025-09-17 08:19:57 UTC",
      "updated_date": "2025-09-26 05:59:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:13:12.889953+00:00"
    },
    {
      "arxiv_id": "2509.13792v1",
      "title": "Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation",
      "title_zh": "å¼¥åˆè™šå®å·®å¼‚ï¼šé¢å‘é²æ£’èˆªå¤©å™¨å…­è‡ªç”±åº¦å§¿æ€ä¼°è®¡çš„æœ‰ç›‘ç£é¢†åŸŸè‡ªé€‚åº”",
      "authors": [
        "Inder Pal Singh",
        "Nidhal Eddine Chenni",
        "Abd El Rahman Shabayek",
        "Arunkumar Rathinam",
        "Djamila Aouada"
      ],
      "abstract": "Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous space operations such as rendezvous, docking, and in-orbit servicing. Hybrid pipelines that combine object detection, keypoint regression, and Perspective-n-Point (PnP) solvers have recently achieved strong results on synthetic datasets, yet their performance deteriorates sharply on real or lab-generated imagery due to the persistent synthetic-to-real domain gap. Existing unsupervised domain adaptation approaches aim to mitigate this issue but often underperform when a modest number of labeled target samples are available. In this work, we propose the first Supervised Domain Adaptation (SDA) framework tailored for SPE keypoint regression. Building on the Learning Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes domain-invariant representations and task-specific risk using both labeled synthetic and limited labeled real data, thereby reducing generalization error under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate that our approach consistently outperforms source-only, fine-tuning, and oracle baselines. Notably, with only 5% labeled target data, our method matches or surpasses oracle performance trained on larger fractions of labeled data. The framework is lightweight, backbone-agnostic, and computationally efficient, offering a practical pathway toward robust and deployable spacecraft pose estimation in real-world space environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªä¸“é—¨ç”¨äºèˆªå¤©å™¨å…³é”®ç‚¹å›å½’çš„ç›‘ç£é¢†åŸŸè‡ªé€‚åº”(Supervised Domain Adaptation, SDA)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³èˆªå¤©å™¨å§¿æ€ä¼°è®¡(Spacecraft Pose Estimation, SPE)åœ¨ä»åˆæˆæ•°æ®è¿ç§»è‡³çœŸå®åœºæ™¯æ—¶é¢ä¸´çš„é¢†åŸŸå·®è·(domain gap)é—®é¢˜ã€‚è¯¥æ–¹æ³•åŸºäºå­¦ä¹ ä¸å˜è¡¨ç¤ºä¸é£é™©(Learning Invariant Representation and Risk, LIRR)èŒƒå¼ï¼Œé€šè¿‡è”åˆä¼˜åŒ–åŸŸä¸å˜è¡¨ç¤ºå’Œä»»åŠ¡ç‰¹å®šé£é™©ï¼Œæœ‰æ•ˆåˆ©ç”¨æ ‡æ³¨çš„åˆæˆæ•°æ®å’Œæœ‰é™çš„æ ‡æ³¨çœŸå®æ•°æ®æ¥é™ä½æ³›åŒ–è¯¯å·®ã€‚åœ¨SPEED+åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶çš„æ€§èƒ½æŒç»­ä¼˜äºsource-onlyã€fine-tuningå’ŒOracleåŸºçº¿ï¼Œä¸”ä»…éœ€5%çš„æ ‡æ³¨ç›®æ ‡æ•°æ®å³å¯è¾¾åˆ°æˆ–è¶…è¶Šåœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒçš„Oracleæ€§èƒ½ã€‚è¯¥æ¡†æ¶å…·æœ‰è½»é‡çº§ã€éª¨å¹²ç½‘ç»œæ— å…³(backbone-agnostic)å’Œè®¡ç®—é«˜æ•ˆçš„ç‰¹ç‚¹ï¼Œä¸ºåœ¨çœŸå®èˆªå¤©ç¯å¢ƒä¸‹å®ç°é²æ£’ä¸”å¯éƒ¨ç½²çš„å§¿æ€ä¼°è®¡æä¾›äº†å®ç”¨è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13792v1",
      "published_date": "2025-09-17 08:03:05 UTC",
      "updated_date": "2025-09-17 08:03:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:13:30.758012+00:00"
    },
    {
      "arxiv_id": "2509.13790v2",
      "title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning",
      "title_zh": "å› ææ–½æ•™ï¼šåŸºäºèƒ½åŠ›æ„ŸçŸ¥è¯¾ç¨‹å­¦ä¹ çš„å¤§è¯­è¨€æ¨¡å‹æŒ‡ä»¤å¾®è°ƒ",
      "authors": [
        "Yangning Li",
        "Tingwei Lu",
        "Yinghui Li",
        "Yankai Chen",
        "Wei-Chieh Huang",
        "Wenhao Jiang",
        "Hui Wang",
        "Hai-Tao Zheng",
        "Philip S. Yu"
      ],
      "abstract": "Efficient instruction tuning aims to enhance the ultimate performance of large language models (LLMs) trained on a given instruction dataset. Curriculum learning as a typical data organization strategy has shown preliminary effectiveness in instruction tuning. However, current curriculum tuning methods suffer from the curriculum rigidity, since they rely solely on static heuristic difficulty metrics. These methods fail to adapt to the evolving capabilities of models during training, resulting in a fixed and potentially sub-optimal learning trajectory. To address the issue, Competence-Aware Multi-Perspective cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS offers several advantages: (1) Dynamic selection for sub-curriculum. (2) Competency-aware adjustment to the curriculum schedule. (3) Multiple difficulty-based scheduling. Extensive experiments prove the superior performance of CAMPUS, compared to other state-of-the-art baselines for efficient instruction tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) æŒ‡ä»¤å¾®è°ƒ (Instruction Tuning) ä¸­çš„è¯¾ç¨‹å­¦ä¹  (Curriculum Learning) æ•ˆç‡é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•å› ä¾èµ–é™æ€å¯å‘å¼éš¾åº¦æŒ‡æ ‡è€Œå­˜åœ¨è¯¾ç¨‹åƒµåŒ– (Curriculum Rigidity) çš„ç¼ºé™·ï¼Œæ— æ³•é€‚åº”æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­æ¼”åŒ–çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†åä¸º CAMPUS (Competence-Aware Multi-Perspective cUrriculum inStruction tuning) çš„èƒ½åŠ›æ„ŸçŸ¥å¤šç»´åº¦è¯¾ç¨‹æŒ‡ä»¤å¾®è°ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶å®ç°äº†å­è¯¾ç¨‹ (Sub-curriculum) çš„åŠ¨æ€é€‰æ‹©ï¼Œå¹¶èƒ½æ ¹æ®æ¨¡å‹èƒ½åŠ›å¯¹è¯¾ç¨‹è¿›åº¦è¿›è¡Œæ„ŸçŸ¥è°ƒæ•´ (Competency-aware adjustment)ï¼ŒåŒæ—¶ç»“åˆäº†å¤šç§åŸºäºéš¾åº¦çš„è°ƒåº¦æœºåˆ¶ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜ï¼Œä¸ç°æœ‰çš„ SOTA é«˜æ•ˆæŒ‡ä»¤å¾®è°ƒåŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒCAMPUS å±•ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œæœ‰æ•ˆä¼˜åŒ–äº†æ¨¡å‹çš„å­¦ä¹ è½¨è¿¹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2509.13790v2",
      "published_date": "2025-09-17 07:58:59 UTC",
      "updated_date": "2025-11-03 09:06:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:13:30.450962+00:00"
    },
    {
      "arxiv_id": "2509.13789v2",
      "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching",
      "title_zh": "BWCacheï¼šé€šè¿‡å—çº§ç¼“å­˜åŠ é€Ÿè§†é¢‘æ‰©æ•£ Transformer",
      "authors": [
        "Hanshuai Cui",
        "Zhiqing Tang",
        "Zhifei Xu",
        "Zhi Yao",
        "Wenyi Zeng",
        "Weijia Jia"
      ],
      "abstract": "Recent advancements in Diffusion Transformers (DiTs) have established them as the state-of-the-art method for video generation. However, their inherently sequential denoising process results in inevitable latency, limiting real-world applicability. Existing acceleration methods either compromise visual quality due to architectural modifications or fail to reuse intermediate features at proper granularity. Our analysis reveals that DiT blocks are the primary contributors to inference latency. Across diffusion timesteps, the feature variations of DiT blocks exhibit a U-shaped pattern with high similarity during intermediate timesteps, which suggests substantial computational redundancy. In this paper, we propose Block-Wise Caching (BWCache), a training-free method to accelerate DiT-based video generation. BWCache dynamically caches and reuses features from DiT blocks across diffusion timesteps. Furthermore, we introduce a similarity indicator that triggers feature reuse only when the differences between block features at adjacent timesteps fall below a threshold, thereby minimizing redundant computations while maintaining visual fidelity. Extensive experiments on several video diffusion models demonstrate that BWCache achieves up to 2.24$\\times$ speedup with comparable visual quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Diffusion Transformers (DiTs) åœ¨è§†é¢‘ç”Ÿæˆä¸­å› ä¸²è¡Œå»å™ªå¯¼è‡´çš„æ¨ç†å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº† BWCache (Block-Wise Caching) åŠ é€Ÿæ–¹æ³•ã€‚é€šè¿‡åˆ†æå‘ç° DiT blocks çš„ç‰¹å¾å˜åŒ–åœ¨æ‰©æ•£æ—¶é—´æ­¥ä¸­å‘ˆç° U å½¢æ¨¡å¼ï¼Œåœ¨ä¸­é—´æ—¶é—´æ­¥å…·æœ‰æé«˜çš„ç›¸ä¼¼æ€§ï¼Œå­˜åœ¨æ˜¾è‘—çš„è®¡ç®—å†—ä½™ã€‚BWCache æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒ (training-free) çš„æ–¹æ¡ˆï¼Œèƒ½å¤ŸåŠ¨æ€ç¼“å­˜å¹¶è·¨æ—¶é—´æ­¥å¤ç”¨ DiT blocks çš„ç‰¹å¾ã€‚è¯¥æ–¹æ³•è¿›ä¸€æ­¥å¼•å…¥äº†ç›¸ä¼¼åº¦æŒ‡æ ‡ (similarity indicator)ï¼Œä»…åœ¨ç›¸é‚»æ—¶é—´æ­¥ç‰¹å¾å·®å¼‚ä½äºé˜ˆå€¼æ—¶è§¦å‘å¤ç”¨ï¼Œç¡®ä¿åœ¨å‡å°‘å†—ä½™è®¡ç®—çš„åŒæ—¶ç»´æŒè§†è§‰ä¿çœŸåº¦ã€‚å®éªŒè¯æ˜ï¼ŒBWCache åœ¨å¤šä¸ªè§†é¢‘æ‰©æ•£æ¨¡å‹ä¸Šå®ç°äº†é«˜è¾¾ 2.24 å€çš„åŠ é€Ÿï¼Œä¸”ç”Ÿæˆè´¨é‡ä¸åŸæ¨¡å‹ç›¸å½“ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13789v2",
      "published_date": "2025-09-17 07:58:36 UTC",
      "updated_date": "2025-09-18 04:57:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:13:35.670087+00:00"
    },
    {
      "arxiv_id": "2509.13782v1",
      "title": "Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis",
      "title_zh": "è°å¼•å…¥äº†æ•…éšœï¼ŸåŸºäºé¢‘è°±åˆ†æçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ•…éšœè‡ªåŠ¨å½’å› ",
      "authors": [
        "Yu Ge",
        "Linna Xie",
        "Zhong Li",
        "Yu Pei",
        "Tian Zhang"
      ],
      "abstract": "Large Language Model Powered Multi-Agent Systems (MASs) are increasingly employed to automate complex real-world problems, such as programming and scientific discovery. Despite their promising, MASs are not without their flaws. However, failure attribution in MASs - pinpointing the specific agent actions responsible for failures - remains underexplored and labor-intensive, posing significant challenges for debugging and system improvement. To bridge this gap, we propose FAMAS, the first spectrum-based failure attribution approach for MASs, which operates through systematic trajectory replay and abstraction, followed by spectrum analysis.The core idea of FAMAS is to estimate, from variations across repeated MAS executions, the likelihood that each agent action is responsible for the failure. In particular, we propose a novel suspiciousness formula tailored to MASs, which integrates two key factor groups, namely the agent behavior group and the action behavior group, to account for the agent activation patterns and the action activation patterns within the execution trajectories of MASs. Through expensive evaluations against 12 baselines on the Who and When benchmark, FAMAS demonstrates superior performance by outperforming all the methods in comparison.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Large Language Model Powered Multi-Agent Systems, MASs) åœ¨è‡ªåŠ¨åŒ–å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´çš„å¤±æ•ˆå½’å›  (failure attribution) éš¾é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªåŸºäºé¢‘è°±åˆ†æ (spectrum analysis) çš„è‡ªåŠ¨åŒ–å½’å› æ¡†æ¶ FAMASã€‚è¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿæ€§çš„è½¨è¿¹å›æ”¾ (trajectory replay) ä¸æŠ½è±¡ï¼Œåˆ©ç”¨åˆ›æ–°çš„å¯ç–‘åº¦å…¬å¼ (suspiciousness formula) æ¥è¯„ä¼°æ¯ä¸ªæ™ºèƒ½ä½“åŠ¨ä½œå¯¼è‡´ç³»ç»Ÿå¤±è´¥çš„æ¦‚ç‡ã€‚è¯¥å…¬å¼ç»¼åˆè€ƒè™‘äº†æ™ºèƒ½ä½“è¡Œä¸ºç»„ (agent behavior group) ä¸åŠ¨ä½œè¡Œä¸ºç»„ (action behavior group) çš„æ¿€æ´»æ¨¡å¼ï¼Œèƒ½å¤Ÿä»å¤šæ¬¡é‡å¤æ‰§è¡Œçš„å˜å¼‚ä¸­ç²¾å‡†é”å®šé—®é¢˜æ ¹æºã€‚åœ¨ Who and When åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFAMAS åœ¨ä¸ 12 ä¸ªåŸºå‡†æ¨¡å‹ (baselines) çš„å¯¹æ¯”è¯„ä¼°ä¸­å±•ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå…¶å½’å› å‡†ç¡®æ€§æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤æ‚ MASs çš„è°ƒè¯•ä¸ç³»ç»Ÿæ”¹è¿›æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.SE",
      "comment": "20 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.13782v1",
      "published_date": "2025-09-17 07:50:44 UTC",
      "updated_date": "2025-09-17 07:50:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:13:39.091748+00:00"
    },
    {
      "arxiv_id": "2509.13775v2",
      "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications",
      "title_zh": "é˜¿æ‹‰ä¼¯æ–¹è¨€è¯†åˆ«çš„æ•°æ®ä¸å‚æ•°é«˜æ•ˆç­–ç•¥æ¢ç´¢",
      "authors": [
        "Vani Kanjirangat",
        "Ljiljana Dolamic",
        "Fabio Rinaldi"
      ],
      "abstract": "This paper discusses our exploration of different data-efficient and parameter-efficient approaches to Arabic Dialect Identification (ADI). In particular, we investigate various soft-prompting strategies, including prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA reparameterizations. For the data-efficient strategy, we analyze hard prompting with zero-shot and few-shot inferences to analyze the dialect identification capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT approaches, we conducted our experiments using Arabic-specific encoder models on several major datasets. We also analyzed the n-shot inferences on open-source decoder-only models, a general multilingual model (Phi-3.5), and an Arabic-specific one(SILMA). We observed that the LLMs generally struggle to differentiate the dialectal nuances in the few-shot or zero-shot setups. The soft-prompted encoder variants perform better, while the LoRA-based fine-tuned models perform best, even surpassing full fine-tuning.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†é’ˆå¯¹é˜¿æ‹‰ä¼¯æ–¹è¨€è¯†åˆ«(Arabic Dialect Identification)çš„æ•°æ®é«˜æ•ˆå’Œå‚æ•°é«˜æ•ˆç­–ç•¥ï¼Œé‡ç‚¹è°ƒæŸ¥äº†å¤šç§è½¯æç¤º(soft-prompting)ç­–ç•¥ï¼ŒåŒ…æ‹¬prefix-tuningã€prompt-tuningã€P-tuningã€P-tuning V2ä»¥åŠLoRAé‡å‚æ•°åŒ–ã€‚åœ¨æ•°æ®é«˜æ•ˆç­–ç•¥æ–¹é¢ï¼Œç ”ç©¶åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨é›¶æ ·æœ¬(zero-shot)å’Œå°‘æ ·æœ¬(few-shot)æ¨ç†ä¸‹çš„è¯†åˆ«èƒ½åŠ›ã€‚å®éªŒåœ¨é˜¿æ‹‰ä¼¯è¯­ç‰¹å®šç¼–ç å™¨æ¨¡å‹ã€é€šç”¨å¤šè¯­è¨€æ¨¡å‹(Phi-3.5)å’Œé˜¿æ‹‰ä¼¯è¯­ç‰¹å®šè§£ç å™¨æ¨¡å‹(SILMA)ä¸Šå±•å¼€ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬è®¾ç½®ä¸‹é€šå¸¸éš¾ä»¥åŒºåˆ†æ–¹è¨€é—´çš„ç»†å¾®å·®åˆ«ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé‡‡ç”¨è½¯æç¤ºçš„ç¼–ç å™¨å˜ä½“è¡¨ç°æ›´ä½³ï¼Œè€ŒåŸºäºLoRAçš„å¾®è°ƒæ¨¡å‹è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¶Šäº†å…¨é‡å¾®è°ƒ(full fine-tuning)ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨è®¡ç®—èµ„æºå—é™çš„æƒ…å†µä¸‹å®ç°é«˜ç²¾åº¦çš„é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€è¯†åˆ«æä¾›äº†é‡è¦çš„å®éªŒä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "4 main pages, 4 additional, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.13775v2",
      "published_date": "2025-09-17 07:45:09 UTC",
      "updated_date": "2025-09-18 08:09:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:13:39.468425+00:00"
    },
    {
      "arxiv_id": "2509.13773v1",
      "title": "MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation",
      "title_zh": "MIRAï¼šåŸºäº MLLM æŒ‡ä»¤æ¨èçš„æ™ºèƒ½æ‰‹æœºä¸€é”®å¼ AI æœåŠ¡èµ‹èƒ½",
      "authors": [
        "Zhipeng Bian",
        "Jieming Zhu",
        "Xuyang Xie",
        "Quanyu Dai",
        "Zhou Zhao",
        "Zhenhua Dong"
      ],
      "abstract": "The rapid advancement of generative AI technologies is driving the integration of diverse AI-powered services into smartphones, transforming how users interact with their devices. To simplify access to predefined AI services, this paper introduces MIRA, a pioneering framework for task instruction recommendation that enables intuitive one-touch AI tasking on smartphones. With MIRA, users can long-press on images or text objects to receive contextually relevant instruction recommendations for executing AI tasks. Our work introduces three key innovations: 1) A multimodal large language model (MLLM)-based recommendation pipeline with structured reasoning to extract key entities, infer user intent, and generate precise instructions; 2) A template-augmented reasoning mechanism that integrates high-level reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based constrained decoding strategy that restricts outputs to predefined instruction candidates, ensuring coherent and intent-aligned suggestions. Through evaluation using a real-world annotated datasets and a user study, MIRA has demonstrated substantial improvements in the accuracy of instruction recommendation. The encouraging results highlight MIRA's potential to revolutionize the way users engage with AI services on their smartphones, offering a more seamless and efficient experience.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MIRAï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) çš„æŒ‡ä»¤æ¨èæ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–æ™ºèƒ½æ‰‹æœºä¸Š AI æœåŠ¡çš„è®¿é—®ï¼Œå®ç°ç›´è§‚çš„â€œä¸€é”®å¼â€ AI ä»»åŠ¡æ‰§è¡Œã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡é•¿æŒ‰å›¾åƒæˆ–æ–‡æœ¬å¯¹è±¡ï¼Œè·å¾—ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„æŒ‡ä»¤æ¨èï¼Œä»è€Œå¿«é€Ÿè°ƒç”¨é¢„å®šä¹‰çš„ AI æœåŠ¡ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ä¸€ä¸ªåŸºäº MLLM çš„ç»“æ„åŒ–æ¨ç†æ¨èæµæ°´çº¿ï¼Œç”¨äºæå–å®ä½“å¹¶æ¨æ–­ç”¨æˆ·æ„å›¾ã€‚æ­¤å¤–ï¼ŒMIRA å¼•å…¥äº†æ¨¡æ¿å¢å¼ºæ¨ç†æœºåˆ¶ (Template-augmented reasoning) å’ŒåŸºäºå‰ç¼€æ ‘çš„å—é™è§£ç ç­–ç•¥ (Prefix-tree-based constrained decoding)ï¼Œä»¥ç¡®ä¿ç”ŸæˆæŒ‡ä»¤çš„å‡†ç¡®æ€§ä¸è¿è´¯æ€§ã€‚é€šè¿‡åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°å’Œç”¨æˆ·ç ”ç©¶ï¼ŒMIRA åœ¨æŒ‡ä»¤æ¨èå‡†ç¡®ç‡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚è¯¥ç ”ç©¶ä¸ºä¼˜åŒ–ç§»åŠ¨ç«¯ AI æœåŠ¡äº¤äº’æä¾›äº†é«˜æ•ˆä¸”æ— ç¼çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 6: Industry Track), ACL 2025. Official version: https://doi.org/10.18653/v1/2025.acl-industry.103",
      "pdf_url": "https://arxiv.org/pdf/2509.13773v1",
      "published_date": "2025-09-17 07:43:14 UTC",
      "updated_date": "2025-09-17 07:43:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:14:01.662085+00:00"
    },
    {
      "arxiv_id": "2509.13761v2",
      "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning",
      "title_zh": "THORï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ•°å­¦æ¨ç†å·¥å…·é›†æˆåˆ†å±‚ä¼˜åŒ–",
      "authors": [
        "Qikai Chang",
        "Zhenrong Zhang",
        "Pengfei Hu",
        "Jun Du",
        "Jiefeng Ma",
        "Yicheng Pan",
        "Jianshu Zhang",
        "Quan Liu",
        "Jianqing Gao"
      ],
      "abstract": "Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both episode-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer's correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†THORï¼Œä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ (RL)è¿›è¡Œå·¥å…·é›†æˆçš„å±‚æ¬¡åŒ–ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†é«˜ç²¾åº¦æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶çš„è¡¨ç°ã€‚ä¸ºè§£å†³é«˜è´¨é‡æ•°æ®ç¨€ç¼ºä¸å¾®ç²’åº¦ä¼˜åŒ–éš¾é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†åŸºäºå¤šæ™ºèƒ½ä½“actor-criticçš„TIRGenæµæ°´çº¿ï¼Œä»¥æ„å»ºé«˜è´¨é‡çš„å·¥å…·é›†æˆæ¨ç†æ•°æ®é›†ã€‚THORçš„æ ¸å¿ƒåœ¨äºé‡‡ç”¨å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ŒåŒæ—¶ä¼˜åŒ–é—®é¢˜æ±‚è§£ä¸æ­¥éª¤çº§ä»£ç ç”Ÿæˆï¼Œå¹¶åˆ©ç”¨å·¥å…·åé¦ˆå®ç°æ¨ç†è¿‡ç¨‹ä¸­çš„self-correctionæœºåˆ¶ä»¥åŠ¨æ€ä¿®æ­£é”™è¯¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTHORåœ¨å¤šä¸ªæ•°å­¦å’Œä»£ç åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†SOTAæ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒè§„æ¨¡æ¨¡å‹ä¸Šçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.13761v2",
      "published_date": "2025-09-17 07:16:12 UTC",
      "updated_date": "2025-10-03 12:48:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:13:55.677264+00:00"
    },
    {
      "arxiv_id": "2509.13755v1",
      "title": "Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning",
      "title_zh": "Scrub It Outï¼é€šè¿‡æœºå™¨é—å¿˜æŠ€æœ¯æ¶ˆé™¤ä»£ç è¯­è¨€æ¨¡å‹ä¸­çš„æ•æ„Ÿè®°å¿†",
      "authors": [
        "Zhaoyang Chu",
        "Yao Wan",
        "Zhikun Zhang",
        "Di Wang",
        "Zhou Yang",
        "Hongyu Zhang",
        "Pan Zhou",
        "Xuanhua Shi",
        "Hai Jin",
        "David Lo"
      ],
      "abstract": "While Code Language Models (CLMs) have demonstrated superior performance in software engineering tasks such as code generation and summarization, recent empirical studies reveal a critical privacy vulnerability: these models exhibit unintended memorization of sensitive training data, enabling verbatim reproduction of confidential information when specifically prompted. To address this issue, several approaches, including training data de-duplication and differential privacy augmentation, have been proposed. However, these methods require full-model retraining for deployed CLMs, which incurs substantial computational costs. In this paper, we aim to answer the following research question: Can sensitive information memorized by CLMs be erased effectively and efficiently?\n  We conduct a pioneering investigation into erasing sensitive memorization in CLMs through machine unlearning - a post-hoc modification method that removes specific information from trained models without requiring full retraining. Specifically, we first quantify the memorization risks of sensitive data within CLM training datasets and curate a high-risk dataset of 50,000 sensitive memorized samples as unlearning targets. We study two widely used gradient ascent-based unlearning approaches: the vanilla and constraint-based methods, and introduce CodeEraser, an advanced variant that selectively unlearns sensitive memorized segments in code while preserving the structural integrity and functional correctness of the surrounding code. Extensive experiments on three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder, validate the effectiveness and efficiency of CodeEraser in erasing targeted sensitive memorization while maintaining model utility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»£ç è¯­è¨€æ¨¡å‹(CLMs)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ— æ„è®°å¿†å¹¶æ³„éœ²æ•æ„Ÿä¿¡æ¯çš„éšç§é£é™©ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ é—å¿˜(Machine Unlearning)çš„é«˜æ•ˆæ“¦é™¤æ–¹æ¡ˆã€‚ç”±äºä¼ ç»Ÿçš„å»é‡æˆ–å·®åˆ†éšç§æŠ€æœ¯é€šå¸¸éœ€è¦æ˜‚è´µçš„å…¨æ¨¡å‹é‡è®­ï¼Œæœ¬ç ”ç©¶æ¢ç´¢äº†æ— éœ€é‡è®­å³å¯ä¿®æ”¹å·²è®­ç»ƒæ¨¡å‹çš„åç½®å¤„ç†æ–¹æ³•ã€‚ç ”ç©¶è€…é€šè¿‡é‡åŒ–CLMsè®­ç»ƒé›†ä¸­çš„è®°å¿†é£é™©ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«5ä¸‡ä¸ªé«˜é£é™©æ•æ„Ÿè®°å¿†æ ·æœ¬çš„ç›®æ ‡æ•°æ®é›†ã€‚åœ¨åˆ†ææ¢¯åº¦ä¸Šå‡(gradient ascent)ç­‰é—å¿˜ç®—æ³•çš„åŸºç¡€ä¸Šï¼Œæœ¬æ–‡æå‡ºäº†CodeEraserï¼Œè¯¥æ–¹æ³•èƒ½ç²¾å‡†è¯†åˆ«å¹¶æ“¦é™¤ä»£ç ä¸­çš„æ•æ„Ÿè®°å¿†ç‰‡æ®µï¼ŒåŒæ—¶æœ€å¤§é™åº¦ä¿ç•™å‘¨å›´ä»£ç çš„ç»“æ„å®Œæ•´æ€§å’ŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚åœ¨CodeParrotã€CodeGen-Monoä»¥åŠQwen2.5-Coderç­‰å¤šä¸ªæ¨¡å‹å®¶æ—ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCodeEraseråœ¨ä¿è¯æ¨¡å‹å®ç”¨æ€§çš„å‰æä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—ä¸”é«˜æ•ˆåœ°æ¶ˆé™¤ç‰¹å®šçš„æ•æ„Ÿè®°å¿†ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at the 48th IEEE/ACM International Conference on Software Engineering (ICSE 2026)",
      "pdf_url": "https://arxiv.org/pdf/2509.13755v1",
      "published_date": "2025-09-17 07:12:35 UTC",
      "updated_date": "2025-09-17 07:12:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:13:49.770415+00:00"
    },
    {
      "arxiv_id": "2509.13735v1",
      "title": "State Space Models over Directed Graphs",
      "title_zh": "é¢å‘æœ‰å‘å›¾çš„çŠ¶æ€ç©ºé—´æ¨¡å‹",
      "authors": [
        "Junzhi She",
        "Xunkai Li",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "abstract": "Directed graphs are ubiquitous across numerous domains, where the directionality of edges encodes critical causal dependencies. However, existing GNNs and graph Transformers tailored for directed graphs face two major challenges: (1) effectively capturing long-range causal dependencies derived from directed edges; (2) balancing accuracy and training efficiency when processing large-scale graph datasets. In recent years, state space models (SSMs) have achieved substantial progress in causal sequence tasks, and their variants designed for graphs have demonstrated state-of-the-art accuracy while maintaining high efficiency across various graph learning benchmarks. However, existing graph state space models are exclusively designed for undirected graphs, which limits their performance in directed graph learning. To this end, we propose an innovative approach DirEgo2Token which sequentializes directed graphs via k-hop ego graphs. This marks the first systematic extension of state space models to the field of directed graph learning. Building upon this, we develop DirGraphSSM, a novel directed graph neural network architecture that implements state space models on directed graphs via the message-passing mechanism. Experimental results demonstrate that DirGraphSSM achieves state-of-the-art performance on three representative directed graph learning tasks while attaining competitive performance on two additional tasks with 1.5$\\times $ to 2$\\times $ training speed improvements compared to existing state-of-the-art models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœ‰å‘å›¾(Directed graphs)åœ¨æ•æ‰é•¿ç¨‹å› æœä¾èµ–(long-range causal dependencies)ä»¥åŠå¹³è¡¡å¤§è§„æ¨¡æ•°æ®å¤„ç†æ•ˆç‡ä¸å‡†ç¡®æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªç³»ç»Ÿæ€§æ‰©å±•åˆ°æœ‰å‘å›¾é¢†åŸŸçš„çŠ¶æ€ç©ºé—´æ¨¡å‹(State Space Models, SSMs)æ–¹æ¡ˆã€‚ç ”ç©¶è€…å¼€å‘äº†DirEgo2Tokenæ–¹æ³•ï¼Œé€šè¿‡kè·³è‡ªæˆ‘å›¾(k-hop ego graphs)å°†æœ‰å‘å›¾è¿›è¡Œåºåˆ—åŒ–å¤„ç†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºäº†åä¸ºDirGraphSSMçš„å…¨æ–°ç¥ç»ç½‘ç»œæ¶æ„ã€‚è¯¥æ¶æ„é€šè¿‡æ¶ˆæ¯ä¼ é€’æœºåˆ¶(message-passing mechanism)åœ¨æœ‰å‘å›¾ä¸Šå®ç°SSMï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰å›¾SSMå˜ä½“ä»…é€‚ç”¨äºæ— å‘å›¾çš„å±€é™æ€§ã€‚å®éªŒè¯æ˜ï¼ŒDirGraphSSMåœ¨ä¸‰é¡¹ä»£è¡¨æ€§çš„æœ‰å‘å›¾å­¦ä¹ ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„(State-of-the-art)æ€§èƒ½ï¼Œå¹¶åœ¨å…¶ä»–ä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ã€‚ä¸ç°æœ‰çš„é¡¶å°–æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå°†è®­ç»ƒé€Ÿåº¦æé«˜äº†1.5è‡³2å€ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚æœ‰å‘ä¾èµ–å…³ç³»æ—¶çš„é«˜æ•ˆæ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "currently undergoing review by IEEE Transactions on Big Data",
      "pdf_url": "https://arxiv.org/pdf/2509.13735v1",
      "published_date": "2025-09-17 06:39:18 UTC",
      "updated_date": "2025-09-17 06:39:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:14:22.674895+00:00"
    },
    {
      "arxiv_id": "2509.13722v1",
      "title": "Mitigating Query Selection Bias in Referring Video Object Segmentation",
      "title_zh": "ç¼“è§£æŒ‡ä»£æ€§è§†é¢‘ç›®æ ‡åˆ†å‰²ä¸­çš„æŸ¥è¯¢é€‰æ‹©åå·®",
      "authors": [
        "Dingwei Zhang",
        "Dong Zhang",
        "Jinhui Tang"
      ],
      "abstract": "Recently, query-based methods have achieved remarkable performance in Referring Video Object Segmentation (RVOS) by using textual static object queries to drive cross-modal alignment. However, these static queries are easily misled by distractors with similar appearance or motion, resulting in \\emph{query selection bias}. To address this issue, we propose Triple Query Former (TQF), which factorizes the referring query into three specialized components: an appearance query for static attributes, an intra-frame interaction query for spatial relations, and an inter-frame motion query for temporal association. Instead of relying solely on textual embeddings, our queries are dynamically constructed by integrating both linguistic cues and visual guidance. Furthermore, we introduce two motion-aware aggregation modules that enhance object token representations: Intra-frame Interaction Aggregation incorporates position-aware interactions among objects within a single frame, while Inter-frame Motion Aggregation leverages trajectory-guided alignment across frames to ensure temporal coherence. Extensive experiments on multiple RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our structured query design and motion-aware aggregation modules.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŒ‡ä»£è§†é¢‘å¯¹è±¡åˆ†å‰² (Referring Video Object Segmentation, RVOS) ä¸­æŸ¥è¯¢é©±åŠ¨æ¨¡å‹å®¹æ˜“å—å¤–è§‚æˆ–è¿åŠ¨ç›¸ä¼¼å¹²æ‰°é¡¹å½±å“è€Œäº§ç”Ÿçš„æŸ¥è¯¢é€‰æ‹©åå·® (query selection bias) é—®é¢˜ï¼Œæå‡ºäº† Triple Query Former (TQF) æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†æŒ‡ä»£æŸ¥è¯¢åˆ†è§£ä¸ºä¸“é—¨è´Ÿè´£é™æ€å±æ€§çš„å¤–è§‚æŸ¥è¯¢ (appearance query)ã€å¤„ç†ç©ºé—´å…³ç³»çš„å¸§å†…äº¤äº’æŸ¥è¯¢ (intra-frame interaction query) ä»¥åŠè´Ÿè´£æ—¶é—´å…³è”çš„å¸§é—´è¿åŠ¨æŸ¥è¯¢ (inter-frame motion query)ã€‚ä¸åŒäºä¼ ç»Ÿä»…ä¾èµ–æ–‡æœ¬åµŒå…¥çš„æ–¹æ³•ï¼ŒTQF é€šè¿‡æ•´åˆè¯­è¨€çº¿ç´¢å’Œè§†è§‰å¼•å¯¼åŠ¨æ€æ„å»ºæŸ¥è¯¢ï¼Œæå‡äº†ç‰¹å¾è¡¨è¾¾çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†å¸§å†…äº¤äº’èšåˆ (Intra-frame Interaction Aggregation) å’Œå¸§é—´è¿åŠ¨èšåˆ (Inter-frame Motion Aggregation) æ¨¡å—ï¼Œåˆ†åˆ«åˆ©ç”¨ä½ç½®æ„ŸçŸ¥äº¤äº’å’Œè½¨è¿¹å¼•å¯¼å¯¹é½æ¥å¢å¼ºç‰©ä½“æ ‡è®°è¡¨å¾å¹¶ç¡®ä¿æ—¶é—´è¿è´¯æ€§ã€‚åœ¨å¤šä¸ª RVOS åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTQF çš„ç»“æ„åŒ–æŸ¥è¯¢è®¾è®¡å’Œè¿åŠ¨æ„ŸçŸ¥èšåˆæ¨¡å—æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œæœ‰æ•ˆç¼“è§£äº†è¯†åˆ«åå·®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13722v1",
      "published_date": "2025-09-17 06:17:23 UTC",
      "updated_date": "2025-09-17 06:17:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:13:53.752644+00:00"
    },
    {
      "arxiv_id": "2509.13706v1",
      "title": "Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€è¡¨å¾æ¨¡å‹çš„äº‹ä»¶å­¦ä¹ å®‰å…¨æŠ¥å‘Šè‡ªåŠ¨åˆ†è¯Šä¸è¿ç§»å­¦ä¹ ",
      "authors": [
        "Peter Beidler",
        "Mark Nguyen",
        "Kevin Lybarger",
        "Ola Holmberg",
        "Eric Ford",
        "John Kang"
      ],
      "abstract": "PURPOSE: Incident reports are an important tool for safety and quality improvement in healthcare, but manual review is time-consuming and requires subject matter expertise. Here we present a natural language processing (NLP) screening tool to detect high-severity incident reports in radiation oncology across two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA SAFRON (SF), all of which had severity scores labeled by clinical content experts. We trained and evaluated two types of models: baseline support vector machines (SVM) and BlueBERT which is a large language model pretrained on PubMed abstracts and hospitalized patient data. We assessed for generalizability of our model in two ways. First, we evaluated models trained using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that was first fine-tuned on Inst.-train then on SF-train before testing on SF-test set. To further analyze model performance, we also examined a subset of 59 reports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82 using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning, performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56 using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets, improved the performance on SF test to AUROC 0.78. Performance of SVM, and BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and 0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP models on incident report text from radiation oncology centers. These models were able to detect high-severity reports similarly to humans on a curated dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€å¥—åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†(NLP)çš„è‡ªåŠ¨åŒ–ç­›é€‰å·¥å…·ï¼Œæ—¨åœ¨ä»æ”¾å°„è‚¿ç˜¤å­¦(radiation oncology)çš„å®‰å…¨äº‹ä»¶æŠ¥å‘Š(incident reports)ä¸­å‡†ç¡®è¯†åˆ«å‡ºé«˜ä¸¥é‡ç¨‹åº¦(high-severity)çš„äº‹ä»¶ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¥è‡ªæœ¬æœºæ„(Inst.)å’ŒIAEA SAFRON (SF)çš„æ•°æ®é›†ï¼Œå¯¹æ¯”äº†åŸºçº¿æ¨¡å‹æ”¯æŒå‘é‡æœº(SVM)ä¸åœ¨åŒ»å­¦é¢†åŸŸé¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹BlueBERTçš„è¡¨ç°ã€‚é’ˆå¯¹æ¨¡å‹åœ¨è·¨æœºæ„åº”ç”¨æ—¶çš„æ³›åŒ–æ€§(generalizability)æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†BlueBERT_TRANSFERæ¨¡å‹ï¼Œé€šè¿‡è¿ç§»å­¦ä¹ (transfer learning)æŠ€æœ¯æ˜¾è‘—æå‡äº†è·¨æœºæ„çš„åˆ†ç±»æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBlueBERT_TRANSFERåœ¨SFæµ‹è¯•é›†ä¸Šçš„AUROCè¾¾åˆ°0.78ï¼Œè¡¨ç°è¿œä¼˜äºæœªç»è¿ç§»å­¦ä¹ çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ç»è¿‡äººå·¥æ ¡é˜…çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†ä¸ä¸´åºŠä¸“å®¶ç›¸å½“çš„åˆ†ç±»èƒ½åŠ›ã€‚è¿™ä¸€æˆæœè¯æ˜äº†è‡ªåŠ¨åŒ–å¤§è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—å®‰å…¨äº‹æ•…åˆ†è¯Šä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºè·¨æœºæ„çš„åŒ»ç–—è´¨é‡æ”¹è¿›æä¾›äº†å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13706v1",
      "published_date": "2025-09-17 05:29:23 UTC",
      "updated_date": "2025-09-17 05:29:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:14:37.659123+00:00"
    },
    {
      "arxiv_id": "2509.13704v1",
      "title": "InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management",
      "title_zh": "InfraMindï¼šé¢å‘å…³é”®ä»»åŠ¡å‹å·¥ä¸šç®¡ç†çš„æ–°å‹æ¢ç´¢å¼ GUI æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Liangtao Lin",
        "Zhaomeng Zhu",
        "Tianwei Zhang",
        "Yonggang Wen"
      ],
      "abstract": "Mission-critical industrial infrastructure, such as data centers, increasingly depends on complex management software. Its operations, however, pose significant challenges due to the escalating system complexity, multi-vendor integration, and a shortage of expert operators. While Robotic Process Automation (RPA) offers partial automation through handcrafted scripts, it suffers from limited flexibility and high maintenance costs. Recent advances in Large Language Model (LLM)-based graphical user interface (GUI) agents have enabled more flexible automation, yet these general-purpose agents face five critical challenges when applied to industrial management, including unfamiliar element understanding, precision and efficiency, state localization, deployment constraints, and safety requirements. To address these issues, we propose InfraMind, a novel exploration-based GUI agentic framework specifically tailored for industrial management systems. InfraMind integrates five innovative modules to systematically resolve different challenges in industrial management: (1) systematic search-based exploration with virtual machine snapshots for autonomous understanding of complex GUIs; (2) memory-driven planning to ensure high-precision and efficient task execution; (3) advanced state identification for robust localization in hierarchical interfaces; (4) structured knowledge distillation for efficient deployment with lightweight models; and (5) comprehensive, multi-layered safety mechanisms to safeguard sensitive operations. Extensive experiments on both open-source and commercial DCIM platforms demonstrate that our approach consistently outperforms existing frameworks in terms of task success rate and operational efficiency, providing a rigorous and scalable solution for industrial management automation.",
      "tldr_zh": "é’ˆå¯¹å…³é”®ä»»åŠ¡å‹å·¥ä¸šåŸºç¡€è®¾æ–½æ—¥ç›Šå¢é•¿çš„ç®¡ç†å¤æ‚æ€§ï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºä¼ ç»Ÿçš„ Robotic Process Automation (RPA) ç¼ºä¹çµæ´»æ€§ï¼Œè€Œé€šç”¨å¤§è¯­è¨€æ¨¡å‹ GUI æ™ºèƒ½ä½“åœ¨å·¥ä¸šåœºæ™¯ä¸‹é¢ä¸´ç†è§£åå·®ã€ç²¾åº¦ä¸è¶³åŠå®‰å…¨åˆè§„ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº† InfraMindï¼Œä¸€ä¸ªä¸“ä¸ºå·¥ä¸šç®¡ç†ç³»ç»Ÿè®¾è®¡çš„åŸºäºæ¢ç´¢çš„ GUI æ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†äº”å¤§æ ¸å¿ƒæ¨¡å—ï¼Œåˆ©ç”¨è™šæ‹Ÿæœºå¿«ç…§è¿›è¡Œç³»ç»ŸåŒ–æœç´¢æ¢ç´¢ä»¥å®ç°å¯¹å¤æ‚ç•Œé¢çš„è‡ªä¸»ç†è§£ï¼Œå¹¶é€šè¿‡ memory-driven planning æå‡ä»»åŠ¡æ‰§è¡Œçš„ç²¾å‡†åº¦ã€‚åŒæ—¶ï¼ŒInfraMind ç»“åˆäº†å…ˆè¿›çš„çŠ¶æ€è¯†åˆ«æŠ€æœ¯è¿›è¡Œå±‚çº§ç•Œé¢å®šä½ï¼Œå¹¶åˆ©ç”¨ç»“æ„åŒ–çŸ¥è¯†è’¸é¦ (knowledge distillation) æŠ€æœ¯å®ç°è½»é‡åŒ–æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å»ºç«‹äº†å…¨é¢çš„å¤šå±‚å®‰å…¨æœºåˆ¶ä»¥ä¿éšœæ•æ„Ÿå·¥ä¸šæ“ä½œçš„å®‰å…¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInfraMind åœ¨å¼€æºå’Œå•†ç”¨æ•°æ®ä¸­å¿ƒåŸºç¡€è®¾æ–½ç®¡ç† (DCIM) å¹³å°ä¸Šè¡¨ç°å“è¶Šï¼Œå…¶ä»»åŠ¡æˆåŠŸç‡å’Œæ“ä½œæ•ˆç‡å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ¡†æ¶ï¼Œä¸ºå·¥ä¸šç®¡ç†è‡ªåŠ¨åŒ–æä¾›äº†ä¸¥è°¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13704v1",
      "published_date": "2025-09-17 05:14:11 UTC",
      "updated_date": "2025-09-17 05:14:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:14:43.159378+00:00"
    },
    {
      "arxiv_id": "2509.13702v1",
      "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models",
      "title_zh": "DSCC-HSï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å¹»è§‰æŠ‘åˆ¶çš„åŠ¨æ€è‡ªæˆ‘å¼ºåŒ–æ¡†æ¶",
      "authors": [
        "Xiao Zheng"
      ],
      "abstract": "Large Language Model (LLM) hallucination is a significant barrier to their reliable deployment. Current methods like Retrieval-Augmented Generation (RAG) are often reactive. We introduce **Dynamic Self-reinforcing Calibration for Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that intervenes during autoregressive decoding. Inspired by dual-process cognitive theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During inference, these proxies dynamically steer a large target model by injecting a real-time steering vector, which is the difference between FAP and HDP logits, at each decoding step. This plug-and-play approach requires no modification to the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2% Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained the highest FActScore of 46.50. These results validate DSCC-HS as a principled and efficient solution for enhancing LLM factuality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DSCC-HSï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæŠ‘åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹»è§‰çš„åŠ¨æ€è‡ªå¢å¼ºæ ¡å‡†æ¡†æ¶ã€‚å—åŒè¿‡ç¨‹è®¤çŸ¥ç†è®ºï¼ˆdual-process cognitive theoryï¼‰å¯å‘ï¼Œè¯¥æ¡†æ¶åœ¨è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­é‡‡å–ä¸»åŠ¨å¹²é¢„ï¼Œåˆ©ç”¨è½»é‡çº§ä»£ç†æ¨¡å‹åˆ†åˆ«æ‹…ä»»äº‹å®å¯¹é½ä»£ç†ï¼ˆFactual Alignment Proxy, FAPï¼‰å’Œå¹»è§‰æ£€æµ‹ä»£ç†ï¼ˆHallucination Detection Proxy, HDPï¼‰çš„è§’è‰²ã€‚é€šè¿‡åœ¨æ¯ä¸ªè§£ç æ­¥éª¤æ³¨å…¥ç”±FAPå’ŒHDPé€»è¾‘å€¼ï¼ˆlogitsï¼‰ä¹‹å·®æ„æˆçš„å®æ—¶è½¬å‘å‘é‡ï¼ŒDSCC-HSèƒ½å¤ŸåŠ¨æ€å¼•å¯¼ç›®æ ‡æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œä¸”å…·å¤‡æ— éœ€ä¿®æ”¹åŸå§‹æ¨¡å‹çš„å³æ’å³ç”¨ï¼ˆplug-and-playï¼‰ç‰¹æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDSCC-HSåœ¨TruthfulQAå’ŒBioGENåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…¶ä¸­åœ¨TruthfulQAä¸Šçš„äº‹å®ä¸€è‡´æ€§ç‡ï¼ˆFCRï¼‰è¾¾åˆ°äº†99.2%ï¼Œåœ¨é•¿æ–‡æœ¬æµ‹è¯•BioGENä¸­è·å¾—äº†46.50çš„æœ€é«˜FActScoreã€‚è¿™äº›å‘ç°è¯æ˜äº†DSCC-HSæ˜¯æå‡LLMç”Ÿæˆäº‹å®æ€§çš„ä¸€ç§ç³»ç»ŸåŒ–ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13702v1",
      "published_date": "2025-09-17 05:09:22 UTC",
      "updated_date": "2025-09-17 05:09:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:14:42.065880+00:00"
    },
    {
      "arxiv_id": "2509.13688v2",
      "title": "CraftMesh: High-Fidelity Generative Mesh Manipulation via Poisson Seamless Fusion",
      "title_zh": "CraftMeshï¼šåŸºäºæ³Šæ¾æ— ç¼èåˆçš„é«˜ä¿çœŸç”Ÿæˆå¼ç½‘æ ¼ç¼–è¾‘",
      "authors": [
        "James Jincheng",
        "Yuxiao Wu",
        "Youcheng Cai",
        "Ligang Liu"
      ],
      "abstract": "Controllable, high-fidelity mesh editing remains a significant challenge in 3D content creation. Existing generative methods often struggle with complex geometries and fail to produce detailed results. We propose CraftMesh, a novel framework for high-fidelity generative mesh manipulation via Poisson Seamless Fusion. Our key insight is to decompose mesh editing into a pipeline that leverages the strengths of 2D and 3D generative models: we edit a 2D reference image, then generate a region-specific 3D mesh, and seamlessly fuse it into the original model. We introduce two core techniques: Poisson Geometric Fusion, which utilizes a hybrid SDF/Mesh representation with normal blending to achieve harmonious geometric integration, and Poisson Texture Harmonization for visually consistent texture blending. Experimental results demonstrate that CraftMesh outperforms state-of-the-art methods, delivering superior global consistency and local detail in complex editing tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CraftMeshï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ Poisson Seamless Fusion å®ç°é«˜ä¿çœŸç”Ÿæˆå¼ç½‘æ ¼æ“ä½œçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ 3D å†…å®¹åˆ›ä½œä¸­å¤æ‚å‡ ä½•ç¼–è¾‘çš„æŒ‘æˆ˜ã€‚CraftMesh å°†ç¼–è¾‘æµç¨‹åˆ†è§£ä¸º 2D å‚è€ƒå›¾åƒç¼–è¾‘ã€ç‰¹å®šåŒºåŸŸ 3D ç½‘æ ¼ç”Ÿæˆä»¥åŠæ— ç¼èåˆè‡³åŸå§‹æ¨¡å‹ä¸‰ä¸ªé˜¶æ®µã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼šPoisson Geometric Fusion é€šè¿‡æ··åˆ SDF/Mesh è¡¨ç¤ºå’Œæ³•çº¿æ··åˆå®ç°å’Œè°çš„å‡ ä½•é›†æˆï¼Œè€Œ Poisson Texture Harmonization åˆ™ç¡®ä¿äº†çº¹ç†åœ¨è§†è§‰ä¸Šçš„è¿è´¯èåˆã€‚é€šè¿‡ç»“åˆ 2D å’Œ 3D ç”Ÿæˆæ¨¡å‹çš„å„è‡ªä¼˜åŠ¿ï¼ŒCraftMesh èƒ½å¤Ÿå¤„ç†å¤æ‚çš„å‡ ä½•å˜æ¢å¹¶ä¿æŒç»†èŠ‚ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¨å±€ä¸€è‡´æ€§å’Œå±€éƒ¨ç»†èŠ‚è¡¨ç°ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13688v2",
      "published_date": "2025-09-17 04:35:48 UTC",
      "updated_date": "2025-12-26 07:35:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:14:53.286026+00:00"
    },
    {
      "arxiv_id": "2509.13683v1",
      "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning",
      "title_zh": "é€šè¿‡åŸç”Ÿæ£€ç´¢å¢å¼ºæ¨ç†æå‡ä¸Šä¸‹æ–‡å¿ å®åº¦",
      "authors": [
        "Suyuchen Wang",
        "Jinlin Wang",
        "Xinyu Wang",
        "Shiqi Li",
        "Xiangru Tang",
        "Sirui Hong",
        "Xiao-Wen Chang",
        "Chenglin Wu",
        "Bang Liu"
      ],
      "abstract": "Large language models (LLMs) often struggle with context fidelity, producing inconsistent answers when responding to questions based on provided information. Existing approaches either rely on expensive supervised fine-tuning to generate evidence post-answer or train models to perform web searches without necessarily improving utilization of the given context. We propose CARE, a novel native retrieval-augmented reasoning framework that teaches LLMs to explicitly integrate in-context evidence within their reasoning process with the model's own retrieval capabilities. Our method requires limited labeled evidence data while significantly enhancing both retrieval accuracy and answer generation performance through strategically retrieved in-context tokens in the reasoning chain. Extensive experiments on multiple real-world and counterfactual QA benchmarks demonstrate that our approach substantially outperforms supervised fine-tuning, traditional retrieval-augmented generation methods, and external retrieval solutions. This work represents a fundamental advancement in making LLMs more accurate, reliable, and efficient for knowledge-intensive tasks.",
      "tldr_zh": "å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†åŸºäºæä¾›ä¿¡æ¯çš„é—®ç­”ä»»åŠ¡æ—¶ï¼Œå¸¸é¢ä¸´ä¸Šä¸‹æ–‡å¿ å®åº¦(context fidelity)ä¸è¶³çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´ç”Ÿæˆç­”æ¡ˆä¸ä¸€è‡´ã€‚è¯¥ç ”ç©¶æå‡ºäº†CAREï¼Œä¸€ç§æ–°å‹çš„åŸç”Ÿæ£€ç´¢å¢å¼ºæ¨ç†(native retrieval-augmented reasoning)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡å‹è‡ªèº«çš„æ£€ç´¢èƒ½åŠ›å°†ä¸Šä¸‹æ–‡è¯æ®(in-context evidence)æ˜¾å¼åœ°é›†æˆåˆ°æ¨ç†è¿‡ç¨‹ä¸­ã€‚è¯¥æ–¹æ³•ä»…éœ€æœ‰é™çš„æ ‡æ³¨æ•°æ®ï¼Œé€šè¿‡åœ¨æ¨ç†é“¾ä¸­ç­–ç•¥æ€§åœ°æ£€ç´¢ä¸Šä¸‹æ–‡ä»¤ç‰Œ(in-context tokens)ï¼Œæ˜¾è‘—å¢å¼ºäº†æ£€ç´¢å‡†ç¡®åº¦ä¸å›ç­”ç”Ÿæˆçš„æ€§èƒ½ã€‚åœ¨å¤šé¡¹çœŸå®ä¸–ç•ŒåŠåäº‹å®(counterfactual)é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCAREçš„è¡¨ç°æ˜¾è‘—ä¼˜äºæœ‰ç›‘ç£å¾®è°ƒ(SFT)ã€ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä»¥åŠå¤–éƒ¨æ£€ç´¢æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œåœ¨æå‡æ¨¡å‹å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„å‡†ç¡®æ€§ã€å¯é æ€§åŠæ•ˆç‡æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as a main conference paper at EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.13683v1",
      "published_date": "2025-09-17 04:28:07 UTC",
      "updated_date": "2025-09-17 04:28:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:14:47.558545+00:00"
    },
    {
      "arxiv_id": "2509.13680v1",
      "title": "Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations",
      "title_zh": "ä»£ç å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æç¤ºç¨³å®šæ€§ï¼šè¯„ä¼°æƒ…ç»ªä¸äººæ ¼é©±åŠ¨å˜ä½“ä¸‹çš„æ•æ„Ÿæ€§",
      "authors": [
        "Wei Ma",
        "Yixiao Yang",
        "Jingquan Ge",
        "Xiaofei Xie",
        "Lingxiao Jiang"
      ],
      "abstract": "Code generation models are widely used in software development, yet their sensitivity to prompt phrasing remains under-examined. Identical requirements expressed with different emotions or communication styles can yield divergent outputs, while most benchmarks emphasize only peak performance. We present PromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically equivalent prompt variants with emotion and personality templates, and that evaluates stability using probability aware continuous scoring or using binary pass rates when logits are unavailable. The results are aggregated into a proposed area under curve metric (AUC-E) for cross model comparison. Across 14 models from three families (Llama, Qwen, and DeepSeek), our study shows that performance and stability behave as largely decoupled optimization objectives, and it reveals architectural and scale related patterns that challenge common assumptions about model robustness. The framework supports rapid screening for closed-source models as well as detailed stability analysis in research settings. PromptSE enables practitioners to quantify performance stability trade offs for deployment and model selection, positioning prompt stability as a complementary evaluation dimension alongside performance and fairness, and contributing to more trustworthy AI-assisted software development tools.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»£ç å¤§è¯­è¨€æ¨¡å‹ (Code LLMs) å¯¹æç¤ºè¯­è¡¨è¾¾ï¼ˆå¦‚æƒ…ç»ªå’Œæ€§æ ¼é©±åŠ¨çš„å·®å¼‚ï¼‰çš„æ•æ„Ÿæ€§é—®é¢˜ã€‚ç ”ç©¶æå‡ºäº† PromptSE (Prompt Sensitivity Evaluation) æ¡†æ¶ï¼Œé€šè¿‡æƒ…ç»ªå’Œæ€§æ ¼æ¨¡æ¿åˆ›å»ºè¯­ä¹‰ç­‰ä»·çš„æç¤ºå˜ä½“ï¼Œå¹¶åˆ©ç”¨æ¦‚ç‡æ„ŸçŸ¥çš„è¿ç»­è¯„åˆ†æˆ–äºŒè¿›åˆ¶é€šè¿‡ç‡æ¥è¯„ä¼°æ¨¡å‹çš„ç¨³å®šæ€§ã€‚ç ”ç©¶è¿˜å¼•å…¥äº† AUC-E æŒ‡æ ‡è¿›è¡Œè·¨æ¨¡å‹å¯¹æ¯”ï¼Œå¹¶åœ¨ Llamaã€Qwen å’Œ DeepSeek ç­‰ 3 ä¸ªç³»åˆ—çš„ 14 ä¸ªæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹çš„æ€§èƒ½ä¸ç¨³å®šæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯è„±é’©çš„ä¼˜åŒ–ç›®æ ‡ï¼Œè¿™ä¸€å‘ç°æŒ‘æˆ˜äº†å…³äºæ¨¡å‹é²æ£’æ€§çš„ä¼ ç»Ÿå‡è®¾ã€‚PromptSE èƒ½å¤Ÿå¸®åŠ©ä»ä¸šè€…é‡åŒ–éƒ¨ç½²è¿‡ç¨‹ä¸­çš„æ€§èƒ½ä¸ç¨³å®šæ€§æƒè¡¡ï¼Œå°†æç¤ºç¨³å®šæ€§å®šä½ä¸ºæ€§èƒ½å’Œå…¬å¹³æ€§ä¹‹å¤–çš„ä¸€ä¸ªé‡è¦è¯„ä¼°ç»´åº¦ï¼Œä¸ºæ„å»ºæ›´å¯é çš„ AI è¾…åŠ©è½¯ä»¶å¼€å‘å·¥å…·åšå‡ºäº†è´¡çŒ®ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13680v1",
      "published_date": "2025-09-17 04:17:42 UTC",
      "updated_date": "2025-09-17 04:17:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:14:52.393731+00:00"
    },
    {
      "arxiv_id": "2510.21722v1",
      "title": "AquaVLM: Improving Underwater Situation Awareness with Mobile Vision Language Models",
      "title_zh": "AquaVLMï¼šåˆ©ç”¨ç§»åŠ¨ç«¯è§†è§‰è¯­è¨€æ¨¡å‹æå‡æ°´ä¸‹æ€åŠ¿æ„ŸçŸ¥èƒ½åŠ›",
      "authors": [
        "Beitong Tian",
        "Lingzhi Zhao",
        "Bo Chen",
        "Haozhen Zheng",
        "Jingcheng Yang",
        "Mingyuan Wu",
        "Deepak Vasisht",
        "Klara Nahrstedt"
      ],
      "abstract": "Underwater activities like scuba diving enable millions annually to explore marine environments for recreation and scientific research. Maintaining situational awareness and effective communication are essential for diver safety. Traditional underwater communication systems are often bulky and expensive, limiting their accessibility to divers of all levels. While recent systems leverage lightweight smartphones and support text messaging, the messages are predefined and thus restrict context-specific communication.\n  In this paper, we present AquaVLM, a tap-and-send underwater communication system that automatically generates context-aware messages and transmits them using ubiquitous smartphones. Our system features a mobile vision-language model (VLM) fine-tuned on an auto-generated underwater conversation dataset and employs a hierarchical message generation pipeline. We co-design the VLM and transmission, incorporating error-resilient fine-tuning to improve the system's robustness to transmission errors. We develop a VR simulator to enable users to experience AquaVLM in a realistic underwater environment and create a fully functional prototype on the iOS platform for real-world experiments. Both subjective and objective evaluations validate the effectiveness of AquaVLM and highlight its potential for personal underwater communication as well as broader mobile VLM applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AquaVLMï¼Œä¸€ç§åŸºäºæ™ºèƒ½æ‰‹æœºçš„ tap-and-send æ°´ä¸‹é€šä¿¡ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¶ˆæ¯æ¥æå‡æ½œæ°´å‘˜çš„æƒ…å¢ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿæ ¸å¿ƒæ˜¯ä¸€ä¸ªåœ¨è‡ªåŠ¨ç”Ÿæˆçš„æ°´ä¸‹å¯¹è¯æ•°æ®é›†ä¸Šç»è¿‡å¾®è°ƒçš„ç§»åŠ¨ç«¯ Vision-Language Model (VLM)ï¼Œå¹¶é‡‡ç”¨äº†å±‚æ¬¡åŒ–æ¶ˆæ¯ç”Ÿæˆæµæ°´çº¿ (hierarchical message generation pipeline)ã€‚é€šè¿‡ VLM ä¸ä¼ è¾“ç³»ç»Ÿçš„ååŒè®¾è®¡ï¼Œç ”ç©¶è€…å¼•å…¥äº†å…·å¤‡é”™è¯¯æ¢å¤èƒ½åŠ› (error-resilient) çš„å¾®è°ƒæŠ€æœ¯ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿåœ¨é¢å¯¹ä¼ è¾“é”™è¯¯æ—¶çš„é²æ£’æ€§ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº† VR æ¨¡æ‹Ÿå™¨è¿›è¡Œç¯å¢ƒä½“éªŒï¼Œå¹¶åœ¨ iOS å¹³å°ä¸Šå®ç°äº†åŠŸèƒ½åŸå‹ä»¥è¿›è¡Œå®åœ°å®éªŒã€‚ä¸»å®¢è§‚è¯„ä¼°ç»“æœå‡è¯æ˜äº† AquaVLM çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸ªäººæ°´ä¸‹é€šä¿¡ä»¥åŠæ›´å¹¿æ³›çš„ç§»åŠ¨ç«¯ VLM åº”ç”¨é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "12 pages, 10 figures, under review",
      "pdf_url": "https://arxiv.org/pdf/2510.21722v1",
      "published_date": "2025-09-17 04:16:58 UTC",
      "updated_date": "2025-09-17 04:16:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:14:57.990249+00:00"
    },
    {
      "arxiv_id": "2509.13677v1",
      "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation",
      "title_zh": "AgentCTGï¼šåˆ©ç”¨å¤šæ™ºèƒ½ä½“åä½œå®ç°æ–‡æœ¬ç”Ÿæˆçš„ç»†ç²’åº¦ç²¾å‡†æ§åˆ¶",
      "authors": [
        "Xinxu Zhou",
        "Jiaqi Bai",
        "Zhenqi Sun",
        "Fanxiang Zeng",
        "Yue Liu"
      ],
      "abstract": "Although significant progress has been made in many tasks within the field of Natural Language Processing (NLP), Controlled Text Generation (CTG) continues to face numerous challenges, particularly in achieving fine-grained conditional control over generation. Additionally, in real scenario and online applications, cost considerations, scalability, domain knowledge learning and more precise control are required, presenting more challenge for CTG. This paper introduces a novel and scalable framework, AgentCTG, which aims to enhance precise and complex control over the text generation by simulating the control and regulation mechanisms in multi-agent workflows. We explore various collaboration methods among different agents and introduce an auto-prompt module to further enhance the generation effectiveness. AgentCTG achieves state-of-the-art results on multiple public datasets. To validate its effectiveness in practical applications, we propose a new challenging Character-Driven Rewriting task, which aims to convert the original text into new text that conform to specific character profiles and simultaneously preserve the domain knowledge. When applied to online navigation with role-playing, our approach significantly enhances the driving experience through improved content delivery. By optimizing the generation of contextually relevant text, we enable a more immersive interaction within online communities, fostering greater personalization and user engagement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AgentCTGï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œ(Multi-Agent Collaboration)å®ç°ç»†ç²’åº¦ç²¾ç¡®å—æ§æ–‡æœ¬ç”Ÿæˆ(Controlled Text Generation)çš„å¯æ‰©å±•æ¡†æ¶ã€‚AgentCTGé€šè¿‡æ¨¡æ‹Ÿå¤šæ™ºèƒ½ä½“å·¥ä½œæµä¸­çš„æ§åˆ¶ä¸è°ƒèŠ‚æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†å—æ§æ–‡æœ¬ç”Ÿæˆåœ¨å¤æ‚çº¦æŸä¸‹çš„ç²¾ç¡®åº¦ã€æˆæœ¬åŠå¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ¢ç´¢äº†å¤šç§æ™ºèƒ½ä½“é—´çš„åä½œæ¨¡å¼ï¼Œå¹¶å¼•å…¥è‡ªåŠ¨æç¤ºè¯(auto-prompt)æ¨¡å—ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ç”Ÿæˆæ•ˆæœï¼Œåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå‡å–å¾—äº†ç›®å‰æœ€å…ˆè¿›çš„(State-of-the-art)æ€§èƒ½è¡¨ç°ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æå‡ºäº†å…¨æ–°çš„è§’è‰²é©±åŠ¨é‡å†™(Character-Driven Rewriting)ä»»åŠ¡ï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨ä¿ç•™é¢†åŸŸçŸ¥è¯†çš„åŒæ—¶éµå¾ªç‰¹å®šäººè®¾é…ç½®çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨åœ¨çº¿å¯¼èˆªå’Œè§’è‰²æ‰®æ¼”ç­‰å®é™…åº”ç”¨ä¸­æ˜¾è‘—æå‡äº†å†…å®¹çš„äº¤ä»˜è´¨é‡ä¸ç”¨æˆ·å‚ä¸åº¦ï¼Œä¸ºå®ç°é«˜åº¦ä¸ªæ€§åŒ–ä¸”ç²¾å‡†çš„æ–‡æœ¬äº¤äº’å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13677v1",
      "published_date": "2025-09-17 04:07:22 UTC",
      "updated_date": "2025-09-17 04:07:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:15:01.187002+00:00"
    },
    {
      "arxiv_id": "2509.13676v1",
      "title": "Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation",
      "title_zh": "å°† SAM é‡å¡‘ä¸ºç”¨äº MLLM æŒ‡ä»£æ€§å›¾åƒåˆ†å‰²çš„é«˜æ•ˆè§†è§‰æŠ•å½±å™¨",
      "authors": [
        "Xiaobo Yang",
        "Xiaojin Gong"
      ],
      "abstract": "Recently, Referring Image Segmentation (RIS) frameworks that pair the Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM) have achieved impressive results. However, adapting MLLM to segmentation is computationally intensive, primarily due to visual token redundancy. We observe that traditional patch-wise visual projectors struggle to strike a balance between reducing the number of visual tokens and preserving semantic clarity, often retaining overly long token sequences to avoid performance drops. Inspired by text tokenizers, we propose a novel semantic visual projector that leverages semantic superpixels generated by SAM to identify \"visual words\" in an image. By compressing and projecting semantic superpixels as visual tokens, our approach adaptively shortens the token sequence according to scene complexity while minimizing semantic loss in compression. To mitigate loss of information, we propose a semantic superpixel positional embedding to strengthen MLLM's awareness of superpixel geometry and position, alongside a semantic superpixel aggregator to preserve both fine-grained details inside superpixels and global context outside. Experiments show that our method cuts visual tokens by 93% without compromising performance, notably speeding up MLLM training and inference, and outperforming existing compressive visual projectors on RIS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)å’ŒSegment Anything Model (SAM)çš„æŒ‡ä»£å›¾åƒåˆ†å‰²(Referring Image Segmentation, RIS)æ¡†æ¶ä¸­è§†è§‰æ ‡è®°(visual tokens)å†—ä½™å¯¼è‡´çš„è®¡ç®—å¯†é›†é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†SAMé‡æ–°åˆ©ç”¨ä¸ºé«˜æ•ˆè§†è§‰æŠ•å½±å™¨çš„æ–°æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨SAMç”Ÿæˆçš„è¯­ä¹‰è¶…åƒç´ (semantic superpixels)æ¥è¯†åˆ«å›¾åƒä¸­çš„â€œè§†è§‰è¯æ±‡â€ï¼Œæ ¹æ®åœºæ™¯å¤æ‚åº¦è‡ªé€‚åº”åœ°å‹ç¼©æ ‡è®°åºåˆ—ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿpatch-wiseæŠ•å½±å™¨åœ¨å‡å°‘æ ‡è®°æ•°é‡ä¸ä¿æŒè¯­ä¹‰æ¸…æ™°åº¦ä¹‹é—´éš¾ä»¥å¹³è¡¡çš„é—®é¢˜ã€‚ä¸ºäº†å‡å°‘ä¿¡æ¯æŸå¤±ï¼Œç ”ç©¶è€…å¼•å…¥äº†è¯­ä¹‰è¶…åƒç´ ä½ç½®åµŒå…¥(semantic superpixel positional embedding)ä»¥å¢å¼ºæ¨¡å‹å¯¹è¶…åƒç´ å‡ ä½•ä½ç½®çš„æ„ŸçŸ¥ï¼Œå¹¶è®¾è®¡äº†è¯­ä¹‰è¶…åƒç´ èšåˆå™¨(semantic superpixel aggregator)æ¥ä¿ç•™ç»†ç²’åº¦ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å°†è§†è§‰æ ‡è®°å‡å°‘äº†93%ï¼Œæ˜¾è‘—åŠ å¿«äº†MLLMçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œå¹¶åœ¨RISä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å‹ç¼©è§†è§‰æŠ•å½±å™¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13676v1",
      "published_date": "2025-09-17 04:04:08 UTC",
      "updated_date": "2025-09-17 04:04:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:15:08.691166+00:00"
    },
    {
      "arxiv_id": "2509.13672v1",
      "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction",
      "title_zh": "CL$^2$GECï¼šé¢å‘ä¸­æ–‡å­¦æœ¯æ–‡çŒ®è¯­æ³•çº é”™æŒç»­å­¦ä¹ çš„å¤šå­¦ç§‘åŸºå‡†",
      "authors": [
        "Shang Qin",
        "Jingheng Ye",
        "Yinghui Li",
        "Hai-Tao Zheng",
        "Qi Li",
        "Jinxiao Shan",
        "Zhixing Li",
        "Hong-Gee Kim"
      ],
      "abstract": "The growing demand for automated writing assistance in diverse academic domains highlights the need for robust Chinese Grammatical Error Correction (CGEC) systems that can adapt across disciplines. However, existing CGEC research largely lacks dedicated benchmarks for multi-disciplinary academic writing, overlooking continual learning (CL) as a promising solution to handle domain-specific linguistic variation and prevent catastrophic forgetting. To fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning benchmark for Chinese Literature Grammatical Error Correction, designed to evaluate adaptive CGEC across multiple academic fields. Our benchmark includes 10,000 human-annotated sentences spanning 10 disciplines, each exhibiting distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating grammatical error correction in a continual learning setting, simulating sequential exposure to diverse academic disciplines to reflect real-world editorial dynamics. We evaluate large language models under sequential tuning, parameter-efficient adaptation, and four representative CL algorithms, using both standard GEC metrics and continual learning metrics adapted to task-level variation. Experimental results reveal that regularization-based methods mitigate forgetting more effectively than replay-based or naive sequential approaches. Our benchmark provides a rigorous foundation for future research in adaptive grammatical error correction across diverse academic domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†CL$^2$GECï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ä¸­æ–‡æ–‡å­¦è¯­æ³•çº é”™(Chinese Literature Grammatical Error Correction)ä¸­æŒç»­å­¦ä¹ (Continual Learning)çš„å¤šå­¦ç§‘åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿåœ¨è·¨å­¦ç§‘é€‚åº”æ€§å’Œç¾éš¾æ€§é—å¿˜æ–¹é¢çš„ä¸è¶³ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ª10ä¸ªå­¦ç§‘ã€å…±10,000æ¡äººå·¥æ ‡æ³¨çš„å¥å­ï¼Œå±•ç°äº†å„å­¦ç§‘ç‹¬ç‰¹çš„è¯­è¨€é£æ ¼å’Œé”™è¯¯æ¨¡å¼ã€‚ç ”ç©¶é€šè¿‡æ¨¡æ‹Ÿä¸åŒå­¦ç§‘çš„åºåˆ—æš´éœ²ç¯å¢ƒï¼Œè¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åºåˆ—å¾®è°ƒã€å‚æ•°é«˜æ•ˆé€‚é…åŠå››ç§ä»£è¡¨æ€§æŒç»­å­¦ä¹ ç®—æ³•ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ­£åˆ™åŒ–(Regularization-based)çš„æ–¹æ³•åœ¨å‡è½»é—å¿˜æ–¹é¢æ¯”åŸºäºé‡æ”¾(Replay-based)æˆ–æœ´ç´ åºåˆ—æ–¹æ³•æ›´ä¸ºæœ‰æ•ˆã€‚CL$^2$GECä¸ä»…å¡«è¡¥äº†å¤šå­¦ç§‘å­¦æœ¯å†™ä½œåŸºå‡†çš„ç©ºç™½ï¼Œè¿˜ä¸ºæœªæ¥å¼€å‘è·¨é¢†åŸŸè‡ªé€‚åº”è¯­æ³•çº é”™ç³»ç»Ÿæä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13672v1",
      "published_date": "2025-09-17 03:54:52 UTC",
      "updated_date": "2025-09-17 03:54:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:15:10.435581+00:00"
    },
    {
      "arxiv_id": "2509.18161v1",
      "title": "Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks",
      "title_zh": "ç¥ç»ç½‘ç»œåˆ†æ®µçº¿æ€§æ ·æ¡æ¿€æ´»å‡½æ•°è®­ç»ƒæµç¨‹çš„å¼€å‘",
      "authors": [
        "William H Patty"
      ],
      "abstract": "Activation functions in neural networks are typically selected from a set of empirically validated, commonly used static functions such as ReLU, tanh, or sigmoid. However, by optimizing the shapes of a network's activation functions, we can train models that are more parameter-efficient and accurate by assigning more optimal activations to the neurons. In this paper, I present and compare 9 training methodologies to explore dual-optimization dynamics in neural networks with parameterized linear B-spline activation functions. The experiments realize up to 94% lower end model error rates in FNNs and 51% lower rates in CNNs compared to traditional ReLU-based models. These gains come at the cost of additional development and training complexity as well as end model latency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¥ç»ç½‘ç»œä¸­åˆ†æ®µçº¿æ€§æ ·æ¡æ¿€æ´»å‡½æ•°çš„è®­ç»ƒç¨‹åºå¼€å‘ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–æ¿€æ´»å‡½æ•°çš„å½¢çŠ¶æ¥æå‡æ¨¡å‹çš„å‚æ•°æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºå¹¶å¯¹æ¯”äº† 9 ç§è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæ¢ç´¢å…·æœ‰å‚æ•°åŒ–çº¿æ€§ B-spline æ¿€æ´»å‡½æ•°çš„ç¥ç»ç½‘ç»œä¸­çš„åŒé‡ä¼˜åŒ–åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„ ReLU æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ FNNs ä¸­ä½¿æ¨¡å‹é”™è¯¯ç‡é™ä½äº†é«˜è¾¾ 94%ï¼Œåœ¨ CNNs ä¸­é™ä½äº† 51%ã€‚è™½ç„¶è¿™äº›æ€§èƒ½æå‡ä¼´éšç€é¢å¤–çš„å¼€å‘ä¸è®­ç»ƒå¤æ‚æ€§ä»¥åŠæ¨¡å‹å»¶è¿Ÿçš„å¢åŠ ï¼Œä½†è¯¥ç ”ç©¶è¯æ˜äº†åŠ¨æ€ä¼˜åŒ–æ¿€æ´»å‡½æ•°åœ¨æå‡ç½‘ç»œæ€§èƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18161v1",
      "published_date": "2025-09-17 03:51:16 UTC",
      "updated_date": "2025-09-17 03:51:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:15:28.299377+00:00"
    },
    {
      "arxiv_id": "2509.13666v1",
      "title": "DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring",
      "title_zh": "DREAMï¼šé¢å‘é«˜æ•ˆè‡ªä¸»æ°´ä¸‹ç›‘æµ‹çš„é¢†åŸŸæ„ŸçŸ¥æ¨ç†",
      "authors": [
        "Zhenqi Wu",
        "Abhinav Modi",
        "Angelos Mavrogiannis",
        "Kaustubh Joshi",
        "Nikhil Chopra",
        "Yiannis Aloimonos",
        "Nare Karapetyan",
        "Ioannis Rekleitis",
        "Xiaomin Lin"
      ],
      "abstract": "The ocean is warming and acidifying, increasing the risk of mass mortality events for temperature-sensitive shellfish such as oysters. This motivates the development of long-term monitoring systems. However, human labor is costly and long-duration underwater work is highly hazardous, thus favoring robotic solutions as a safer and more efficient option. To enable underwater robots to make real-time, environment-aware decisions without human intervention, we must equip them with an intelligent \"brain.\" This highlights the need for persistent,wide-area, and low-cost benthic monitoring. To this end, we present DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term underwater exploration and habitat monitoring. The results show that our framework is highly efficient in finding and exploring target objects (e.g., oysters, shipwrecks) without prior location information. In the oyster-monitoring task, our framework takes 31.5% less time than the previous baseline with the same amount of oysters. Compared to the vanilla VLM, it uses 23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our framework successfully explores and maps the wreck without collisions, requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage, while the vanilla model achieves 60.23% average coverage in our shipwreck environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DREAMï¼Œè¿™æ˜¯ä¸€ä¸ªç”±è§†è§‰è¯­è¨€æ¨¡å‹(Vision Language Model, VLM)æŒ‡å¯¼çš„è‡ªä¸»æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é•¿æœŸçš„æ°´ä¸‹æ¢ç´¢å’Œæ –æ¯åœ°ç›‘æµ‹ã€‚ä¸ºäº†åº”å¯¹æµ·æ´‹å˜æš–å’Œé…¸åŒ–å¯¹ç‰¡è›ç­‰æ¸©æ•è´ç±»å¸¦æ¥çš„ç”Ÿå­˜é£é™©ï¼Œè¯¥æ¡†æ¶èµ‹äºˆäº†æ°´ä¸‹æœºå™¨äººé¢†åŸŸæ„ŸçŸ¥æ¨ç†(Domain-aware Reasoning)èƒ½åŠ›ï¼Œä½¿å…¶èƒ½åœ¨æ— äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹åšå‡ºå®æ—¶å†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDREAMåœ¨æ— éœ€å…ˆéªŒä½ç½®ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å¯»æ‰¾å¹¶æ¢ç´¢ç›®æ ‡ç‰©ä½“ã€‚åœ¨ç‰¡è›ç›‘æµ‹ä»»åŠ¡ä¸­ï¼Œè¯¥æ¡†æ¶æ¯”åŸºçº¿æ¨¡å‹èŠ‚çœäº†31.5%çš„æ—¶é—´ï¼Œä¸”ä¸åŸç”ŸVLMç›¸æ¯”ï¼Œåœ¨å‡å°‘23%æ­¥éª¤çš„åŒæ—¶æé«˜äº†8.88%çš„è¦†ç›–ç‡ã€‚åœ¨æ²‰èˆ¹åœºæ™¯ä¸­ï¼ŒDREAMæˆåŠŸå®ç°äº†æ— ç¢°æ’æ¢ç´¢ä¸åœ°å›¾ç»˜åˆ¶ï¼Œå…¶æ­¥æ•°æ¯”åŸç”Ÿæ¨¡å‹å‡å°‘27.5%ï¼Œå¹¶è¾¾åˆ°äº†100%çš„è¦†ç›–ç‡ï¼Œæ˜¾è‘—ä¼˜äºåŸç”Ÿæ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºæŒä¹…ã€å¹¿åŸŸä¸”ä½æˆæœ¬çš„æµ·åº•ç›‘æµ‹æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®‰å…¨çš„æœºå™¨äººè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "submitted to ICRA 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.13666v1",
      "published_date": "2025-09-17 03:35:52 UTC",
      "updated_date": "2025-09-17 03:35:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:15:40.692334+00:00"
    },
    {
      "arxiv_id": "2509.13664v1",
      "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç¨€ç–ç¥ç»å…ƒè•´å«å¼ºçƒˆçš„é—®é¢˜æ­§ä¹‰ä¿¡å·",
      "authors": [
        "Zhuoxuan Zhang",
        "Jinhao Duan",
        "Edward Kim",
        "Kaidi Xu"
      ],
      "abstract": "Ambiguity is pervasive in real-world questions, yet large language models (LLMs) often respond with confident answers rather than seeking clarification. In this work, we show that question ambiguity is linearly encoded in the internal representations of LLMs and can be both detected and controlled at the neuron level. During the model's pre-filling stage, we identify that a small number of neurons, as few as one, encode question ambiguity information. Probes trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance on ambiguity detection and generalize across datasets, outperforming prompting-based and representation-based baselines. Layerwise analysis reveals that AENs emerge from shallow layers, suggesting early encoding of ambiguity signals in the model's processing pipeline. Finally, we show that through manipulating AENs, we can control LLM's behavior from direct answering to abstention. Our findings reveal that LLMs form compact internal representations of question ambiguity, enabling interpretable and controllable behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é¢å¯¹å…·æœ‰æ­§ä¹‰æ€§(Ambiguity)çš„é—®é¢˜æ—¶å¾€å¾€ç›²ç›®è‡ªä¿¡è€Œéå¯»æ±‚æ¾„æ¸…çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œé—®é¢˜çš„æ­§ä¹‰æ€§çº¿æ€§ç¼–ç åœ¨æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºä¸­ï¼Œå¹¶å¯ä»¥é€šè¿‡æå°‘æ•°çš„æ­§ä¹‰ç¼–ç ç¥ç»å…ƒ(Ambiguity-Encoding Neurons, AENs)è¿›è¡Œè¯†åˆ«ä¸æ§åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨é¢„å¡«å……(Pre-filling)é˜¶æ®µåˆ©ç”¨AENsæ„å»ºçš„æ¢é’ˆ(Probes)åœ¨æ­§ä¹‰æ£€æµ‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”å…¶æ³›åŒ–èƒ½åŠ›æ˜¾è‘—ä¼˜äºåŸºäºæç¤º(Prompting)å’Œè¡¨ç¤º(Representation)çš„åŸºå‡†æ–¹æ³•ã€‚å±‚çº§åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†AENsåœ¨ç½‘ç»œçš„æµ…å±‚å³å¯æ¶Œç°ï¼Œè¯´æ˜æ¨¡å‹åœ¨å¤„ç†æ—©æœŸä¾¿æ•æ‰åˆ°äº†æ­§ä¹‰ä¿¡å·ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¹²é¢„è¿™äº›ç‰¹å®šçš„AENsï¼Œç ”ç©¶è€…èƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶LLMsçš„è¡Œä¸ºï¼Œä½¿å…¶åœ¨é¢å¯¹ä¸ç¡®å®šæ€§æ—¶ä»ç›´æ¥å›ç­”è½¬å‘æ‹’ç»å›ç­”(Abstention)ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†æ¨¡å‹å†…éƒ¨å­˜åœ¨ç´§å‡‘ä¸”å¯è§£é‡Šçš„æ­§ä¹‰è¡¨ç¤ºæœºåˆ¶ï¼Œä¸ºå¢å¼ºLLMsçš„å¯æ§æ€§æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To be appeared in EMNLP 2025 (main)",
      "pdf_url": "https://arxiv.org/pdf/2509.13664v1",
      "published_date": "2025-09-17 03:34:35 UTC",
      "updated_date": "2025-09-17 03:34:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:15:56.390998+00:00"
    },
    {
      "arxiv_id": "2509.13662v1",
      "title": "Deep Lookup Network",
      "title_zh": "æ·±åº¦æŸ¥è¡¨ç½‘ç»œ",
      "authors": [
        "Yulan Guo",
        "Longguang Wang",
        "Wendong Mao",
        "Xiaoyu Dong",
        "Yingqian Wang",
        "Li Liu",
        "Wei An"
      ],
      "abstract": "Convolutional neural networks are constructed with massive operations with different types and are highly computationally intensive. Among these operations, multiplication operation is higher in computational complexity and usually requires {more} energy consumption with longer inference time than other operations, which hinders the deployment of convolutional neural networks on mobile devices. In many resource-limited edge devices, complicated operations can be calculated via lookup tables to reduce computational cost. Motivated by this, in this paper, we introduce a generic and efficient lookup operation which can be used as a basic operation for the construction of neural networks. Instead of calculating the multiplication of weights and activation values, simple yet efficient lookup operations are adopted to compute their responses. To enable end-to-end optimization of the lookup operation, we construct the lookup tables in a differentiable manner and propose several training strategies to promote their convergence. By replacing computationally expensive multiplication operations with our lookup operations, we develop lookup networks for the image classification, image super-resolution, and point cloud classification tasks. It is demonstrated that our lookup networks can benefit from the lookup operations to achieve higher efficiency in terms of energy consumption and inference speed while maintaining competitive performance to vanilla convolutional networks. Extensive experiments show that our lookup networks produce state-of-the-art performance on different tasks (both classification and regression tasks) and different data types (both images and point clouds).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)ä¸­ä¹˜æ³•è¿ç®—å¯¼è‡´çš„é«˜èƒ½è€—ä¸æ¨ç†å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†Deep Lookup Networkï¼Œå¼•å…¥äº†ä¸€ç§é€šç”¨ä¸”é«˜æ•ˆçš„æŸ¥æ‰¾æ“ä½œ(lookup operation)ä½œä¸ºåŸºæœ¬è¿ç®—å•å…ƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨æŸ¥æ‰¾è¡¨(lookup tables)è®¡ç®—æƒé‡ä¸æ¿€æ´»å€¼çš„å“åº”ï¼Œæ—¨åœ¨æ›¿ä»£è®¡ç®—æˆæœ¬æ˜‚è´µçš„ä¼ ç»Ÿä¹˜æ³•è¿ç®—ã€‚ä¸ºäº†æ”¯æŒç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œç ”ç©¶è€…ä»¥å¯å¾®(differentiable)çš„æ–¹å¼æ„å»ºæŸ¥æ‰¾è¡¨ï¼Œå¹¶æå‡ºäº†ä¸“é—¨çš„è®­ç»ƒç­–ç•¥ä»¥ç¡®ä¿æ¨¡å‹çš„æ”¶æ•›æ€§ã€‚è¯¥æ¶æ„è¢«å¹¿æ³›åº”ç”¨äºå›¾åƒåˆ†ç±»ã€å›¾åƒè¶…åˆ†è¾¨ç‡åŠç‚¹äº‘åˆ†ç±»ç­‰å¤šç§ä»»åŠ¡ï¼Œå®éªŒè¯æ˜æŸ¥æ‰¾ç½‘ç»œåœ¨æ˜¾è‘—æå‡èƒ½é‡æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒä¸å¸¸è§„å·ç§¯ç½‘ç»œç›¸å½“çš„ç«äº‰åŠ›ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒDeep Lookup Networkåœ¨ä¸åŒä»»åŠ¡å’Œæ•°æ®ç±»å‹ä¸Šå‡å–å¾—äº†state-of-the-artçš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13662v1",
      "published_date": "2025-09-17 03:31:41 UTC",
      "updated_date": "2025-09-17 03:31:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:15:46.898564+00:00"
    },
    {
      "arxiv_id": "2509.13650v1",
      "title": "GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?",
      "title_zh": "GitHub Copilot ä»£ç å®¡æŸ¥ï¼šäººå·¥æ™ºèƒ½èƒ½å¦åœ¨æäº¤å‰è¯†åˆ«å®‰å…¨æ¼æ´ï¼Ÿ",
      "authors": [
        "Amena Amro",
        "Manar H. Alalfi"
      ],
      "abstract": "As software development practices increasingly adopt AI-powered tools, ensuring that such tools can support secure coding has become critical. This study evaluates the effectiveness of GitHub Copilot's recently introduced code review feature in detecting security vulnerabilities. Using a curated set of labeled vulnerable code samples drawn from diverse open-source projects spanning multiple programming languages and application domains, we systematically assessed Copilot's ability to identify and provide feedback on common security flaws. Contrary to expectations, our results reveal that Copilot's code review frequently fails to detect critical vulnerabilities such as SQL injection, cross-site scripting (XSS), and insecure deserialization. Instead, its feedback primarily addresses low-severity issues, such as coding style and typographical errors. These findings expose a significant gap between the perceived capabilities of AI-assisted code review and its actual effectiveness in supporting secure development practices. Our results highlight the continued necessity of dedicated security tools and manual code audits to ensure robust software security.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº† GitHub Copilot æ–°æ¨å‡ºçš„ Code Review åŠŸèƒ½åœ¨æ£€æµ‹å®‰å…¨æ¼æ´æ–¹é¢çš„å®é™…è¡¨ç°ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨æ¥è‡ªå¤šä¸ªå¼€æºé¡¹ç›®çš„è·¨è¯­è¨€æ ‡è®°æ¼æ´ä»£ç æ ·æœ¬ï¼Œæµ‹è¯•äº†å…¶å¯¹å¸¸è§å®‰å…¨ç¼ºé™·çš„è¯†åˆ«ä¸åé¦ˆèƒ½åŠ›ã€‚å®éªŒå‘ç°ï¼ŒCopilot ç»å¸¸æ— æ³•è¯†åˆ« SQL injectionã€Cross-site Scripting (XSS) å’Œ Insecure Deserialization ç­‰ä¸¥é‡æ¼æ´ï¼Œå…¶åé¦ˆå¤šä¾§é‡äº Coding Style å’Œ Typographical Errors ç­‰ä½é£é™©é—®é¢˜ã€‚è¿™ä¸€ç»“æœæš´éœ²äº† AI-assisted code review çš„å…¬ä¼—è®¤çŸ¥ä¸å…¶åœ¨å®‰å…¨å¼€å‘å®è·µä¸­çœŸå®æ•ˆèƒ½ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚ç ”ç©¶å¼ºè°ƒï¼Œç°é˜¶æ®µä¾é  AI è¿›è¡Œä»£ç å®¡æŸ¥å°šä¸èƒ½å–ä»£ä¸“é—¨çš„ Security Tools å’Œäººå·¥ä»£ç å®¡è®¡ (Manual Code Audits)ï¼Œåè€…å¯¹äºä¿éšœè½¯ä»¶å®‰å…¨ä¾ç„¶è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13650v1",
      "published_date": "2025-09-17 02:56:21 UTC",
      "updated_date": "2025-09-17 02:56:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:15:51.691163+00:00"
    },
    {
      "arxiv_id": "2509.13633v1",
      "title": "DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis",
      "title_zh": "DeepLogitï¼šé¢å‘äº¤é€šæ”¿ç­–åˆ†æçš„åºè´¯çº¦æŸå¯è§£é‡Šæ·±åº¦å­¦ä¹ å»ºæ¨¡æ–¹æ³•",
      "authors": [
        "Jeremy Oon",
        "Rakhi Manohar Mepparambath",
        "Ling Feng"
      ],
      "abstract": "Despite the significant progress of deep learning models in multitude of applications, their adaption in planning and policy related areas remains challenging due to the black-box nature of these models. In this work, we develop a set of DeepLogit models that follow a novel sequentially constrained approach in estimating deep learning models for transport policy analysis. In the first step of the proposed approach, we estimate a convolutional neural network (CNN) model with only linear terms, which is equivalent of a linear-in-parameter multinomial logit model. We then estimate other deep learning models by constraining the parameters that need interpretability at the values obtained in the linear-in-parameter CNN model and including higher order terms or by introducing advanced deep learning architectures like Transformers. Our approach can retain the interpretability of the selected parameters, yet provides significantly improved model accuracy than the discrete choice model. We demonstrate our approach on a transit route choice example using real-world transit smart card data from Singapore. This study shows the potential for a unifying approach, where theory-based discrete choice model (DCM) and data-driven AI models can leverage each other's strengths in interpretability and predictive power. With the availability of larger datasets and more complex constructions, such approach can lead to more accurate models using discrete choice models while maintaining its applicability in planning and policy-related areas. Our code is available on https://github.com/jeremyoon/route-choice/ .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DeepLogit ç³»åˆ—æ¨¡å‹ï¼Œé‡‡ç”¨ä¸€ç§æ–°å‹çš„é¡ºåºçº¦æŸæ–¹æ³•æ¥è¿›è¡Œäº¤é€šæ”¿ç­–åˆ†æï¼Œä»¥è§£å†³æ·±åº¦å­¦ä¹ åœ¨è§„åˆ’é¢†åŸŸå› å…¶â€œé»‘ç®±â€ç‰¹æ€§è€Œé¢ä¸´çš„åº”ç”¨æŒ‘æˆ˜ã€‚è¿™ç§æ–¹æ³•é¦–å…ˆä¼°è®¡ä¸€ä¸ªä»…åŒ…å«çº¿æ€§é¡¹çš„ CNN æ¨¡å‹ï¼Œä½¿å…¶ç­‰æ•ˆäºå‚æ•°çº¿æ€§çš„ multinomial logit æ¨¡å‹ã€‚æ¥ç€ï¼Œç ”ç©¶é€šè¿‡å°†éœ€è¦è§£é‡Šæ€§çš„å‚æ•°å›ºå®šåœ¨ç¬¬ä¸€æ­¥è·å¾—çš„å€¼ä¸Šï¼Œå¹¶å¼•å…¥é«˜é˜¶é¡¹æˆ– Transformers ç­‰å…ˆè¿›æ¶æ„ï¼Œåœ¨ä¿ç•™é€‰å®šå‚æ•°å¯è§£é‡Šæ€§çš„åŒæ—¶æ˜¾è‘—æå‡æ¨¡å‹ç²¾åº¦ã€‚ä½œè€…ä½¿ç”¨æ–°åŠ å¡çœŸå®çš„å…¬äº¤æ™ºèƒ½å¡æ•°æ®åœ¨å…¬äº¤è·¯å¾„é€‰æ‹©åœºæ™¯ä¸‹éªŒè¯äº†è¯¥æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepLogit æˆåŠŸå®ç°äº†ç†è®ºå¯¼å‘çš„ç¦»æ•£é€‰æ‹©æ¨¡å‹ (DCM) ä¸æ•°æ®é©±åŠ¨çš„ AI æ¨¡å‹ä¹‹é—´çš„ä¼˜åŠ¿äº’è¡¥ã€‚è¿™ç§ç»Ÿä¸€çš„æ–¹æ³•ä¸ºåœ¨ä¿æŒæ”¿ç­–é€‚ç”¨æ€§çš„å‰æä¸‹æ„å»ºæ›´å‡†ç¡®çš„äº¤é€šé¢„æµ‹æ¨¡å‹æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13633v1",
      "published_date": "2025-09-17 02:08:34 UTC",
      "updated_date": "2025-09-17 02:08:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:15:55.185478+00:00"
    },
    {
      "arxiv_id": "2509.13627v1",
      "title": "Secure, Scalable and Privacy Aware Data Strategy in Cloud",
      "title_zh": "äº‘ç«¯å®‰å…¨ã€å¯æ‰©å±•ä¸”éšç§æ„ŸçŸ¥çš„æ•°æ®æˆ˜ç•¥",
      "authors": [
        "Vijay Kumar Butte",
        "Sujata Butte"
      ],
      "abstract": "The enterprises today are faced with the tough challenge of processing, storing large amounts of data in a secure, scalable manner and enabling decision makers to make quick, informed data driven decisions. This paper addresses this challenge and develops an effective enterprise data strategy in the cloud. Various components of an effective data strategy are discussed and architectures addressing security, scalability and privacy aspects are provided.",
      "tldr_zh": "è¯¥è®ºæ–‡é’ˆå¯¹ä¼ä¸šåœ¨äº‘ç«¯ä»¥å®‰å…¨ã€å¯æ‰©å±•çš„æ–¹å¼å¤„ç†å’Œå­˜å‚¨æµ·é‡æ•°æ®æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€å¥—æœ‰æ•ˆçš„ä¼ä¸šäº‘ç«¯æ•°æ®ç­–ç•¥(Data Strategy)ã€‚ç ”ç©¶æ—¨åœ¨å¸®åŠ©å†³ç­–è€…å¿«é€Ÿåšå‡ºæ˜æ™ºçš„æ•°æ®é©±åŠ¨å†³ç­–ï¼Œå¹¶è¯¦ç»†æ¢è®¨äº†æ„æˆæœ‰æ•ˆæ•°æ®ç­–ç•¥çš„å„ç§æ ¸å¿ƒç»„ä»¶ã€‚æ–‡ç« é‡ç‚¹æä¾›äº†ä¸“é—¨é’ˆå¯¹å®‰å…¨æ€§(Security)ã€å¯æ‰©å±•æ€§(Scalability)å’Œéšç§(Privacy)ç»´åº¦çš„æ¶æ„æ–¹æ¡ˆã€‚è¿™äº›æ¶æ„ä¸ºä¼ä¸šåœ¨äº‘ç¯å¢ƒä¸­æ„å»ºç¨³å¥çš„æ•°æ®å¤„ç†æµç¨‹æä¾›äº†ç†è®ºæ”¯æŒä¸æŠ€æœ¯æŒ‡å¯¼ï¼Œæœ‰æ•ˆè§£å†³äº†å¤§è§„æ¨¡æ•°æ®ç®¡ç†ä¸­çš„å®‰å…¨ä¸æ€§èƒ½å¹³è¡¡é—®é¢˜ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13627v1",
      "published_date": "2025-09-17 01:56:07 UTC",
      "updated_date": "2025-09-17 01:56:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:16:03.990739+00:00"
    },
    {
      "arxiv_id": "2509.13626v2",
      "title": "Mind the Gap: Aligning Knowledge Bases with User Needs to Enhance Mental Health Retrieval",
      "title_zh": "å¼¥åˆå·®è·ï¼šå¯¹é½çŸ¥è¯†åº“ä¸ç”¨æˆ·éœ€æ±‚ä»¥å¢å¼ºå¿ƒç†å¥åº·æ£€ç´¢",
      "authors": [
        "Amanda Chan",
        "James Jiayu Liu",
        "He Kai",
        "Onno P. Kampman"
      ],
      "abstract": "Access to reliable mental health information is vital for early help-seeking, yet expanding knowledge bases is resource-intensive and often misaligned with user needs. This results in poor performance of retrieval systems when presented concerns are not covered or expressed in informal or contextualized language. We present an AI-based gap-informed framework for corpus augmentation that authentically identifies underrepresented topics (gaps) by overlaying naturalistic user data such as forum posts in order to prioritize expansions based on coverage and usefulness. In a case study, we compare Directed (gap-informed augmentations) with Non-Directed augmentation (random additions), evaluating the relevance and usefulness of retrieved information across four retrieval-augmented generation (RAG) pipelines. Directed augmentation achieved near-optimal performance with modest expansions--requiring only a 42% increase for Query Transformation, 74% for Reranking and Hierarchical, and 318% for Baseline--to reach ~95% of the performance of an exhaustive reference corpus. In contrast, Non-Directed augmentation required substantially larger and thus practically infeasible expansions to achieve comparable performance (232%, 318%, 403%, and 763%, respectively). These results show that strategically targeted corpus growth can reduce content creation demands while sustaining high retrieval and provision quality, offering a scalable approach for building trusted health information repositories and supporting generative AI applications in high-stakes domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¿ƒç†å¥åº·ä¿¡æ¯æ£€ç´¢ä¸­çŸ¥è¯†åº“(Knowledge Bases)æ‰©å±•æˆæœ¬é«˜æ˜‚ä¸”ä¸ç”¨æˆ·éœ€æ±‚ä¸åŒ¹é…çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„ç¼ºå£å‘ŠçŸ¥(gap-informed)è¯­æ–™åº“æ‰©å……æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†æè®ºå›å¸–å­ç­‰è‡ªç„¶è¯­è¨€æ•°æ®è¯†åˆ«æœªè¢«å……åˆ†ä»£è¡¨çš„ä¸»é¢˜ï¼Œä»è€Œä¼˜å…ˆè¿›è¡Œé’ˆå¯¹æ€§çš„è¯­æ–™åº“æ‰©å……ã€‚ç ”ç©¶åœ¨åŒ…æ‹¬Query Transformationã€Rerankingå’ŒHierarchicalåœ¨å†…çš„å››ç§æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æµç¨‹ä¸­ï¼Œå¯¹æ¯”äº†å®šå‘æ‰©å……(Directed augmentation)ä¸éå®šå‘éšæœºæ‰©å……çš„æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå®šå‘æ‰©å……ä»…éœ€è¾ƒå°è§„æ¨¡çš„æ‰©å±•å³å¯è¾¾åˆ°å®Œå¤‡å‚è€ƒè¯­æ–™åº“çº¦95%çš„æ€§èƒ½ï¼Œå…¶æ•ˆç‡æ˜¾è‘—ä¼˜äºéå®šå‘æ‰©å……ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†æˆ˜ç•¥æ€§é’ˆå¯¹æ€§è¯­æ–™åº“å¢é•¿èƒ½åœ¨é™ä½å†…å®¹åˆ›ä½œéœ€æ±‚çš„åŒæ—¶ï¼Œç»´æŒé«˜æ°´å¹³çš„æ£€ç´¢ä¸ä¿¡æ¯æä¾›è´¨é‡ï¼Œä¸ºæ„å»ºé«˜é£é™©é¢†åŸŸçš„å¯ä¿¡å¥åº·ä¿¡æ¯åº“æä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "25 pages, 3 figures, submitted to NeurIPS 2025 GenAI4Health",
      "pdf_url": "https://arxiv.org/pdf/2509.13626v2",
      "published_date": "2025-09-17 01:54:11 UTC",
      "updated_date": "2025-11-21 19:43:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:16:09.488319+00:00"
    },
    {
      "arxiv_id": "2509.13620v2",
      "title": "A reduced-order derivative-informed neural operator for subsurface fluid-flow",
      "title_zh": "é¢å‘åœ°ä¸‹æµä½“æµåŠ¨çš„é™é˜¶å¯¼æ•°å¼•å¯¼ç¥ç»ç®—å­",
      "authors": [
        "Jeongjin Park",
        "Grant Bruer",
        "Huseyin Tuna Erdinc",
        "Abhinav Prakash Gahlot",
        "Felix J. Herrmann"
      ],
      "abstract": "Neural operators have emerged as cost-effective surrogates for expensive fluid-flow simulators, particularly in computationally intensive tasks such as permeability inversion from time-lapse seismic data, and uncertainty quantification. In these applications, the fidelity of the surrogate's gradients with respect to system parameters is crucial, as the accuracy of downstream tasks, such as optimization and Bayesian inference, relies directly on the quality of the derivative information. Recent advances in physics-informed methods have leveraged derivative information to improve surrogate accuracy. However, incorporating explicit Jacobians can become computationally prohibitive, as the complexity typically scales quadratically with the number of input parameters. To address this limitation, we propose DeFINO (Derivative-based Fisher-score Informed Neural Operator), a reduced-order, derivative-informed training framework. DeFINO integrates Fourier neural operators (FNOs) with a novel derivative-based training strategy guided by the Fisher Information Matrix (FIM). By projecting Jacobians onto dominant eigen-directions identified by the FIM, DeFINO captures critical sensitivity information directly informed by observational data, significantly reducing computational expense. We validate DeFINO through synthetic experiments in the context of subsurface multi-phase fluid-flow, demonstrating improvements in gradient accuracy while maintaining robust forward predictions of underlying fluid dynamics. These results highlight DeFINO's potential to offer practical, scalable solutions for inversion problems in complex real-world scenarios, all at substantially reduced computational cost.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ°ä¸‹æµä½“æµåŠ¨æ¨¡æ‹Ÿå™¨åœ¨æ¸—é€ç‡åæ¼”å’Œä¸ç¡®å®šæ€§é‡åŒ–ä¸­è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼ŒæŒ‡å‡ºä»£ç†æ¨¡å‹æ¢¯åº¦çš„å‡†ç¡®æ€§å¯¹ä¸‹æ¸¸ä¼˜åŒ–å’Œ Bayesian inference è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³æ˜¾å¼ Jacobians è®¡ç®—å¤æ‚åº¦éšè¾“å…¥å‚æ•°å‘ˆäºŒæ¬¡æ–¹å¢é•¿çš„é™åˆ¶ï¼Œä½œè€…æå‡ºäº† DeFINO (Derivative-based Fisher-score Informed Neural Operator) è¿™ä¸€é™é˜¶çš„å¯¼æ•°ä¿¡æ¯è¾…åŠ©è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°† Fourier neural operators (FNOs) ä¸å— Fisher Information Matrix (FIM) å¯å‘çš„å¯¼æ•°è®­ç»ƒç­–ç•¥ç›¸ç»“åˆï¼Œé€šè¿‡å°† Jacobians æŠ•å½±åˆ° FIM è¯†åˆ«çš„ä¸»ç‰¹å¾æ–¹å‘æ¥æ•æ‰å…³é”®çµæ•åº¦ä¿¡æ¯ã€‚ç ”ç©¶åœ¨ subsurface multi-phase fluid-flow å®éªŒä¸­éªŒè¯äº†è¯¥æ–¹æ³•ï¼Œè¯æ˜å…¶åœ¨æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶æ˜¾è‘—æé«˜äº†æ¢¯åº¦ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeFINO åœ¨ä¿æŒç¨³å¥æ­£å‘æµä½“åŠ¨åŠ›å­¦é¢„æµ‹çš„åŒæ—¶ï¼Œä¸ºå¤„ç†å¤æ‚ç°å®åœºæ™¯ä¸­çš„åæ¼”é—®é¢˜æä¾›äº†ä¸€ç§å®ç”¨ä¸”å…·æœ‰æ‰©å±•æ€§çš„ä½æˆæœ¬è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "physics.comp-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.comp-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13620v2",
      "published_date": "2025-09-17 01:30:44 UTC",
      "updated_date": "2026-01-15 12:27:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:16:16.393463+00:00"
    },
    {
      "arxiv_id": "2509.13615v1",
      "title": "See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles",
      "title_zh": "è§‚å¯Ÿã€æ€è€ƒä¸è¡ŒåŠ¨ï¼šé€šè¿‡å¼€å…³è¯†åˆ«è®­ç»ƒå¤šæ¨¡æ€æ™ºèƒ½ä½“ä¸ GUI çš„æœ‰æ•ˆäº¤äº’",
      "authors": [
        "Zongru Wu",
        "Rui Mao",
        "Zhiyuan Tian",
        "Pengzhou Cheng",
        "Tianjie Ju",
        "Zheng Wu",
        "Lingzhong Dong",
        "Haiyue Sheng",
        "Zhuosheng Zhang",
        "Gongshen Liu"
      ],
      "abstract": "The advent of multimodal agents facilitates effective interaction within graphical user interface (GUI), especially in ubiquitous GUI control. However, their inability to reliably execute toggle control instructions remains a key bottleneck. To investigate this, we construct a state control benchmark with binary toggle instructions from public datasets. Evaluations of existing agents demonstrate their unreliability, particularly when the current toggle state already matches the desired state. To address the challenge, we propose State-aware Reasoning (StaR), a training method that teaches agents to perceive the current toggle state, analyze the desired state from the instruction, and act accordingly. Experiments on three multimodal agents demonstrate that StaR can improve toggle instruction execution accuracy by over 30\\%. Further evaluations on three public benchmarks show that StaR also enhances general task performance. Finally, evaluations on a dynamic environment highlight the potential of StaR for real-world applications. Code, benchmark, and StaR-enhanced agents are available at https://github.com/ZrW00/StaR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æ™ºèƒ½ä½“(Multimodal Agents)åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)äº¤äº’ä¸­éš¾ä»¥å¯é æ‰§è¡Œåˆ‡æ¢(Toggle)æ§åˆ¶æŒ‡ä»¤çš„é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä½œè€…é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŒ…å«äºŒå…ƒåˆ‡æ¢æŒ‡ä»¤çš„çŠ¶æ€æ§åˆ¶åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡è¯„ä¼°å‘ç°ç°æœ‰æ™ºèƒ½ä½“åœ¨å½“å‰çŠ¶æ€å·²åŒ¹é…ç›®æ ‡çŠ¶æ€æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„ä¸ç¨³å®šæ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†çŠ¶æ€æ„ŸçŸ¥æ¨ç†(State-aware Reasoning, StaR)è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ•™å¯¼æ™ºèƒ½ä½“è¯†åˆ«å½“å‰ToggleçŠ¶æ€å¹¶ç»“åˆæŒ‡ä»¤åˆ†æç›®æ ‡çŠ¶æ€ï¼Œä»è€Œåšå‡ºæ›´ç²¾å‡†çš„å†³ç­–ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ä¸‰ä¸ªå¤šæ¨¡æ€æ™ºèƒ½ä½“ä¸Šåº”ç”¨StaRå¯ä½¿åˆ‡æ¢æŒ‡ä»¤çš„æ‰§è¡Œå‡†ç¡®ç‡æé«˜30%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•åŠåŠ¨æ€ç¯å¢ƒä¸­ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„é€šç”¨ä»»åŠ¡å¤„ç†èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13615v1",
      "published_date": "2025-09-17 01:14:14 UTC",
      "updated_date": "2025-09-17 01:14:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:16:12.484204+00:00"
    },
    {
      "arxiv_id": "2509.13603v1",
      "title": "Modernizing Facebook Scoped Search: Keyword and Embedding Hybrid Retrieval with LLM Evaluation",
      "title_zh": "Facebook å®šå‘æœç´¢çš„ç°ä»£åŒ–ï¼šå…³é”®è¯ä¸åµŒå…¥æ··åˆæ£€ç´¢åŠ LLM è¯„ä¼°",
      "authors": [
        "Yongye Su",
        "Zeya Zhang",
        "Jane Kou",
        "Cheng Ju",
        "Shubhojeet Sarkar",
        "Yamin Wang",
        "Ji Liu",
        "Shengbo Guo"
      ],
      "abstract": "Beyond general web-scale search, social network search uniquely enables users to retrieve information and discover potential connections within their social context. We introduce a framework of modernized Facebook Group Scoped Search by blending traditional keyword-based retrieval with embedding-based retrieval (EBR) to improve the search relevance and diversity of search results. Our system integrates semantic retrieval into the existing keyword search pipeline, enabling users to discover more contextually relevant group posts. To rigorously assess the impact of this blended approach, we introduce a novel evaluation framework that leverages large language models (LLMs) to perform offline relevance assessments, providing scalable and consistent quality benchmarks. Our results demonstrate that the blended retrieval system significantly enhances user engagement and search quality, as validated by both online metrics and LLM-based evaluation. This work offers practical insights for deploying and evaluating advanced retrieval systems in large-scale, real-world social platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡ç»“åˆä¼ ç»Ÿçš„ keyword-based retrieval ä¸ embedding-based retrieval (EBR) æ¥ç°ä»£åŒ– Facebook Group Scoped Searchï¼Œä»¥æå‡ç¤¾äº¤ç½‘ç»œèƒŒæ™¯ä¸‹çš„æœç´¢ç›¸å…³æ€§å’Œç»“æœå¤šæ ·æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨ç°æœ‰å…³é”®è¯æµæ°´çº¿ä¸­é›†æˆè¯­ä¹‰æ£€ç´¢ï¼ˆSemantic retrievalï¼‰ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿå‘ç°æ›´å¤šå…·æœ‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„ç¾¤ç»„å¸–å­ã€‚ä¸ºäº†ä¸¥è°¨è¯„ä¼°è¯¥æ··åˆæ–¹æ³•çš„å½±å“ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§åˆ©ç”¨ large language models (LLMs) è¿›è¡Œç¦»çº¿ç›¸å…³æ€§è¯„ä¼°çš„æ–°å‹æ¡†æ¶ï¼Œæä¾›äº†å¯æ‰©å±•ä¸”ä¸€è‡´çš„è´¨é‡åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ··åˆæ£€ç´¢ç³»ç»Ÿæ˜¾è‘—å¢å¼ºäº†ç”¨æˆ·å‚ä¸åº¦ï¼ˆUser engagementï¼‰å’Œæœç´¢è´¨é‡ï¼Œè¿™ä¸€ç»“è®ºåœ¨åœ¨çº¿æŒ‡æ ‡å’ŒåŸºäº LLM çš„è¯„ä¼°ä¸­å‡å¾—åˆ°äº†éªŒè¯ã€‚è¯¥å·¥ä½œä¸ºåœ¨å¤§å‹çœŸå®ç¤¾äº¤å¹³å°ä¸­éƒ¨ç½²å’Œè¯„ä¼°å…ˆè¿›æ£€ç´¢ç³»ç»Ÿæä¾›äº†å®ç”¨çš„è§è§£ä¸å®è·µå‚è€ƒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "5 Pages, work done as Yongye Su's internship project at Meta",
      "pdf_url": "https://arxiv.org/pdf/2509.13603v1",
      "published_date": "2025-09-17 00:22:08 UTC",
      "updated_date": "2025-09-17 00:22:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:16:31.794224+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 101,
  "processed_papers_count": 101,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T19:17:25.029307+00:00"
}
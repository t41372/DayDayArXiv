[
  {
    "arxiv_id": "2404.01349v2",
    "title": "Fairness in Large Language Models: A Taxonomic Survey",
    "authors": [
      "Zhibo Chu",
      "Zichong Wang",
      "Wenbin Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains. However, despite their promising performance in numerous\nreal-world applications, most of these algorithms lack fairness considerations.\nConsequently, they may lead to discriminatory outcomes against certain\ncommunities, particularly marginalized populations, prompting extensive study\nin fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in\ntraditional machine learning, entails exclusive backgrounds, taxonomies, and\nfulfillment techniques. To this end, this survey presents a comprehensive\noverview of recent advances in the existing literature concerning fair LLMs.\nSpecifically, a brief introduction to LLMs is provided, followed by an analysis\nof factors contributing to bias in LLMs. Additionally, the concept of fairness\nin LLMs is discussed categorically, summarizing metrics for evaluating bias in\nLLMs and existing algorithms for promoting fairness. Furthermore, resources for\nevaluating bias in LLMs, including toolkits and datasets, are summarized.\nFinally, existing research challenges and open questions are discussed.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01349v2",
    "published_date": "2024-03-31 22:22:53 UTC",
    "updated_date": "2024-12-19 05:05:46 UTC"
  },
  {
    "arxiv_id": "2404.00816v1",
    "title": "HeteroMILE: a Multi-Level Graph Representation Learning Framework for Heterogeneous Graphs",
    "authors": [
      "Yue Zhang",
      "Yuntian He",
      "Saket Gurukar",
      "Srinivasan Parthasarathy"
    ],
    "abstract": "Heterogeneous graphs are ubiquitous in real-world applications because they\ncan represent various relationships between different types of entities.\nTherefore, learning embeddings in such graphs is a critical problem in graph\nmachine learning. However, existing solutions for this problem fail to scale to\nlarge heterogeneous graphs due to their high computational complexity. To\naddress this issue, we propose a Multi-Level Embedding framework of nodes on a\nheterogeneous graph (HeteroMILE) - a generic methodology that allows\ncontemporary graph embedding methods to scale to large graphs. HeteroMILE\nrepeatedly coarsens the large sized graph into a smaller size while preserving\nthe backbone structure of the graph before embedding it, effectively reducing\nthe computational cost by avoiding time-consuming processing operations. It\nthen refines the coarsened embedding to the original graph using a\nheterogeneous graph convolution neural network. We evaluate our approach using\nseveral popular heterogeneous graph datasets. The experimental results show\nthat HeteroMILE can substantially reduce computational time (approximately 20x\nspeedup) and generate an embedding of better quality for link prediction and\nnode classification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00816v1",
    "published_date": "2024-03-31 22:22:10 UTC",
    "updated_date": "2024-03-31 22:22:10 UTC"
  },
  {
    "arxiv_id": "2404.00815v2",
    "title": "Towards Realistic Scene Generation with LiDAR Diffusion Models",
    "authors": [
      "Haoxi Ran",
      "Vitor Guizilini",
      "Yue Wang"
    ],
    "abstract": "Diffusion models (DMs) excel in photo-realistic image synthesis, but their\nadaptation to LiDAR scene generation poses a substantial hurdle. This is\nprimarily because DMs operating in the point space struggle to preserve the\ncurve-like patterns and 3D geometry of LiDAR scenes, which consumes much of\ntheir representation power. In this paper, we propose LiDAR Diffusion Models\n(LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to\ncapture the realism of LiDAR scenes by incorporating geometric priors into the\nlearning pipeline. Our method targets three major desiderata: pattern realism,\ngeometry realism, and object realism. Specifically, we introduce curve-wise\ncompression to simulate real-world LiDAR patterns, point-wise coordinate\nsupervision to learn scene geometry, and patch-wise encoding for a full 3D\nobject context. With these three core designs, our method achieves competitive\nperformance on unconditional LiDAR generation in 64-beam scenario and state of\nthe art on conditional LiDAR generation, while maintaining high efficiency\ncompared to point-based DMs (up to 107$\\times$ faster). Furthermore, by\ncompressing LiDAR scenes into a latent space, we enable the controllability of\nDMs with various conditions such as semantic maps, camera views, and text\nprompts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024. Project link: https://lidar-diffusion.github.io",
    "pdf_url": "http://arxiv.org/pdf/2404.00815v2",
    "published_date": "2024-03-31 22:18:56 UTC",
    "updated_date": "2024-04-18 19:22:37 UTC"
  },
  {
    "arxiv_id": "2404.08221v1",
    "title": "Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI",
    "authors": [
      "Jocelyn Dzuong",
      "Zichong Wang",
      "Wenbin Zhang"
    ],
    "abstract": "In the rapidly evolving landscape of generative artificial intelligence (AI),\nthe increasingly pertinent issue of copyright infringement arises as AI\nadvances to generate content from scraped copyrighted data, prompting questions\nabout ownership and protection that impact professionals across various\ncareers. With this in mind, this survey provides an extensive examination of\ncopyright infringement as it pertains to generative AI, aiming to stay abreast\nof the latest developments and open problems. Specifically, it will first\noutline methods of detecting copyright infringement in mediums such as text,\nimage, and video. Next, it will delve an exploration of existing techniques\naimed at safeguarding copyrighted works from generative models. Furthermore,\nthis survey will discuss resources and tools for users to evaluate copyright\nviolations. Finally, insights into ongoing regulations and proposals for AI\nwill be explored and compared. Through combining these disciplines, the\nimplications of AI-driven content and copyright are thoroughly illustrated and\nbrought into question.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08221v1",
    "published_date": "2024-03-31 22:10:01 UTC",
    "updated_date": "2024-03-31 22:10:01 UTC"
  },
  {
    "arxiv_id": "2404.00806v2",
    "title": "Algorithmic Collusion by Large Language Models",
    "authors": [
      "Sara Fish",
      "Yannai A. Gonczarowski",
      "Ran I. Shorrer"
    ],
    "abstract": "The rise of algorithmic pricing raises concerns of algorithmic collusion. We\nconduct experiments with algorithmic pricing agents based on Large Language\nModels (LLMs). We find that (1) LLM-based agents are adept at pricing tasks,\n(2) LLM-based pricing agents autonomously collude in oligopoly settings to the\ndetriment of consumers, and (3) variation in seemingly innocuous phrases in LLM\ninstructions (\"prompts\") may increase collusion. Novel off-path analysis\ntechniques uncover price-war concerns as contributing to these phenomena. Our\nresults extend to auction settings. Our findings uncover unique challenges to\nany future regulation of LLM-based pricing agents, and black-box pricing agents\nmore broadly.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.GT",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00806v2",
    "published_date": "2024-03-31 21:43:05 UTC",
    "updated_date": "2024-11-27 00:19:55 UTC"
  },
  {
    "arxiv_id": "2404.00781v2",
    "title": "Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning",
    "authors": [
      "Mohamed Elsayed",
      "A. Rupam Mahmood"
    ],
    "abstract": "Deep representation learning methods struggle with continual learning,\nsuffering from both catastrophic forgetting of useful units and loss of\nplasticity, often due to rigid and unuseful units. While many methods address\nthese two issues separately, only a few currently deal with both\nsimultaneously. In this paper, we introduce Utility-based Perturbed Gradient\nDescent (UPGD) as a novel approach for the continual learning of\nrepresentations. UPGD combines gradient updates with perturbations, where it\napplies smaller modifications to more useful units, protecting them from\nforgetting, and larger modifications to less useful units, rejuvenating their\nplasticity. We use a challenging streaming learning setup where continual\nlearning problems have hundreds of non-stationarities and unknown task\nboundaries. We show that many existing methods suffer from at least one of the\nissues, predominantly manifested by their decreasing accuracy over tasks. On\nthe other hand, UPGD continues to improve performance and surpasses or is\ncompetitive with all methods in all problems. Finally, in extended\nreinforcement learning experiments with PPO, we show that while Adam exhibits a\nperformance drop after initial learning, UPGD avoids it by addressing both\ncontinual learning issues.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the Proceedings of the 12th International Conference on\n  Learning Representations (ICLR 2024). Code is available at\n  https://github.com/mohmdelsayed/upgd",
    "pdf_url": "http://arxiv.org/pdf/2404.00781v2",
    "published_date": "2024-03-31 19:57:38 UTC",
    "updated_date": "2024-04-30 22:52:33 UTC"
  },
  {
    "arxiv_id": "2404.00777v1",
    "title": "Privacy-preserving Optics for Enhancing Protection in Face De-identification",
    "authors": [
      "Jhon Lopez",
      "Carlos Hinojosa",
      "Henry Arguello",
      "Bernard Ghanem"
    ],
    "abstract": "The modern surge in camera usage alongside widespread computer vision\ntechnology applications poses significant privacy and security concerns.\nCurrent artificial intelligence (AI) technologies aid in recognizing relevant\nevents and assisting in daily tasks in homes, offices, hospitals, etc. The need\nto access or process personal information for these purposes raises privacy\nconcerns. While software-level solutions like face de-identification provide a\ngood privacy/utility trade-off, they present vulnerabilities to sniffing\nattacks. In this paper, we propose a hardware-level face de-identification\nmethod to solve this vulnerability. Specifically, our approach first learns an\noptical encoder along with a regression model to obtain a face heatmap while\nhiding the face identity from the source image. We also propose an\nanonymization framework that generates a new face using the privacy-preserving\nimage, face heatmap, and a reference face image from a public dataset as input.\nWe validate our approach with extensive simulations and hardware experiments.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2024. Project Website and Code coming soon",
    "pdf_url": "http://arxiv.org/pdf/2404.00777v1",
    "published_date": "2024-03-31 19:28:04 UTC",
    "updated_date": "2024-03-31 19:28:04 UTC"
  },
  {
    "arxiv_id": "2404.00756v1",
    "title": "Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery",
    "authors": [
      "Cristina Cornelio",
      "Mohammed Diab"
    ],
    "abstract": "Recognizing failures during task execution and implementing recovery\nprocedures is challenging in robotics. Traditional approaches rely on the\navailability of extensive data or a tight set of constraints, while more recent\napproaches leverage large language models (LLMs) to verify task steps and\nreplan accordingly. However, these methods often operate offline, necessitating\nscene resets and incurring in high costs. This paper introduces Recover, a\nneuro-symbolic framework for online failure identification and recovery. By\nintegrating ontologies, logical rules, and LLM-based planners, Recover exploits\nsymbolic information to enhance the ability of LLMs to generate recovery plans\nand also to decrease the associated costs. In order to demonstrate the\ncapabilities of our method in a simulated kitchen environment, we introduce\nOntoThor, an ontology describing the AI2Thor simulator setting. Empirical\nevaluation shows that OntoThor's logical rules accurately detect all failures\nin the analyzed tasks, and that Recover considerably outperforms, for both\nfailure detection and recovery, a baseline method reliant solely on LLMs.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00756v1",
    "published_date": "2024-03-31 17:54:22 UTC",
    "updated_date": "2024-03-31 17:54:22 UTC"
  },
  {
    "arxiv_id": "2404.00752v1",
    "title": "On the True Distribution Approximation of Minimum Bayes-Risk Decoding",
    "authors": [
      "Atsumoto Ohashi",
      "Ukyo Honda",
      "Tetsuro Morimura",
      "Yuu Jinnai"
    ],
    "abstract": "Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in\ntext generation. MBR decoding considers texts sampled from a model as\npseudo-references and selects the text with the highest similarity to the\nothers. Therefore, sampling is one of the key elements of MBR decoding, and\nprevious studies reported that the performance varies by sampling methods. From\na theoretical standpoint, this performance variation is likely tied to how\nclosely the samples approximate the true distribution of references. However,\nthis approximation has not been the subject of in-depth study. In this study,\nwe propose using anomaly detection to measure the degree of approximation. We\nfirst closely examine the performance variation and then show that previous\nhypotheses about samples do not correlate well with the variation, but our\nintroduced anomaly scores do. The results are the first to empirically support\nthe link between the performance and the core assumption of MBR decoding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024 (main conference)",
    "pdf_url": "http://arxiv.org/pdf/2404.00752v1",
    "published_date": "2024-03-31 17:47:22 UTC",
    "updated_date": "2024-03-31 17:47:22 UTC"
  },
  {
    "arxiv_id": "2404.00748v1",
    "title": "Benchmark Transparency: Measuring the Impact of Data on Evaluation",
    "authors": [
      "Venelin Kovatchev",
      "Matthew Lease"
    ],
    "abstract": "In this paper we present an exploratory research on quantifying the impact\nthat data distribution has on the performance and evaluation of NLP models. We\npropose an automated framework that measures the data point distribution across\n6 different dimensions: ambiguity, difficulty, discriminability, length, noise,\nand perplexity.\n  We use disproportional stratified sampling to measure how much the data\ndistribution affects absolute (Acc/F1) and relative (Rank) model performance.\nWe experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135\ndifferent models (125 on SQUAD and 10 on MNLI). We demonstrate that without\nexplicit control of the data distribution, standard evaluation frameworks are\ninconsistent and unreliable. We find that the impact of the data is\nstatistically significant and is often larger than the impact of changing the\nmetric.\n  In a second set of experiments, we demonstrate that the impact of data on\nevaluation is not just observable, but also predictable. We propose to use\nbenchmark transparency as a method for comparing datasets and quantifying the\nsimilarity between them. We find that the ``dataset similarity vector'' can be\nused to predict how well a model generalizes out of distribution.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00748v1",
    "published_date": "2024-03-31 17:33:43 UTC",
    "updated_date": "2024-03-31 17:33:43 UTC"
  },
  {
    "arxiv_id": "2404.00746v1",
    "title": "Mining Weighted Sequential Patterns in Incremental Uncertain Databases",
    "authors": [
      "Kashob Kumar Roy",
      "Md Hasibul Haque Moon",
      "Md Mahmudur Rahman",
      "Chowdhury Farhan Ahmed",
      "Carson Kai-Sang Leung"
    ],
    "abstract": "Due to the rapid development of science and technology, the importance of\nimprecise, noisy, and uncertain data is increasing at an exponential rate.\nThus, mining patterns in uncertain databases have drawn the attention of\nresearchers. Moreover, frequent sequences of items from these databases need to\nbe discovered for meaningful knowledge with great impact. In many real cases,\nweights of items and patterns are introduced to find interesting sequences as a\nmeasure of importance. Hence, a constraint of weight needs to be handled while\nmining sequential patterns. Besides, due to the dynamic nature of databases,\nmining important information has become more challenging. Instead of mining\npatterns from scratch after each increment, incremental mining algorithms\nutilize previously mined information to update the result immediately. Several\nalgorithms exist to mine frequent patterns and weighted sequences from\nincremental databases. However, these algorithms are confined to mine the\nprecise ones. Therefore, we have developed an algorithm to mine frequent\nsequences in an uncertain database in this work. Furthermore, we have proposed\ntwo new techniques for mining when the database is incremental. Extensive\nexperiments have been conducted for performance evaluation. The analysis showed\nthe efficiency of our proposed framework.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "Accepted to Information Science journal",
    "pdf_url": "http://arxiv.org/pdf/2404.00746v1",
    "published_date": "2024-03-31 17:32:08 UTC",
    "updated_date": "2024-03-31 17:32:08 UTC"
  },
  {
    "arxiv_id": "2404.00725v2",
    "title": "The Larger the Better? Improved LLM Code-Generation via Budget Reallocation",
    "authors": [
      "Michael Hassid",
      "Tal Remez",
      "Jonas Gehring",
      "Roy Schwartz",
      "Yossi Adi"
    ],
    "abstract": "It is a common belief that large language models (LLMs) are better than\nsmaller-sized ones. However, larger models also require significantly more time\nand compute during inference. This begs the question: what happens when both\nmodels operate under the same budget? (e.g., compute, run-time). To address\nthis question, we analyze code generation LLMs of various sizes and make\ncomparisons such as running a 70B model once vs. generating five outputs from a\n13B model. We consider a standard unit-test setup, which can be used to select\nthe correct output from the smaller model. Our findings reveal that the\nrepeated use of smaller models can yield consistent improvements, with gains of\nup to 15% across five tasks. On the other hand, in scenarios where unit-tests\nare unavailable, a ranking-based selection of candidates from the smaller model\nfalls short of the performance of a single output from larger ones. Our results\nhighlight the potential of using smaller models instead of larger ones, and the\nimportance of studying approaches for ranking LLM outputs.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "COLM 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00725v2",
    "published_date": "2024-03-31 15:55:49 UTC",
    "updated_date": "2024-07-25 11:37:54 UTC"
  },
  {
    "arxiv_id": "2404.00722v5",
    "title": "DRCT: Saving Image Super-resolution away from Information Bottleneck",
    "authors": [
      "Chih-Chung Hsu",
      "Chia-Ming Lee",
      "Yi-Shiuan Chou"
    ],
    "abstract": "In recent years, Vision Transformer-based approaches for low-level vision\ntasks have achieved widespread success. Unlike CNN-based models, Transformers\nare more adept at capturing long-range dependencies, enabling the\nreconstruction of images utilizing non-local information. In the domain of\nsuper-resolution, Swin-transformer-based models have become mainstream due to\ntheir capability of global spatial information modeling and their\nshifting-window attention mechanism that facilitates the interchange of\ninformation between different windows. Many researchers have enhanced model\nperformance by expanding the receptive fields or designing meticulous networks,\nyielding commendable results. However, we observed that it is a general\nphenomenon for the feature map intensity to be abruptly suppressed to small\nvalues towards the network's end. This implies an information bottleneck and a\ndiminishment of spatial information, implicitly limiting the model's potential.\nTo address this, we propose the Dense-residual-connected Transformer (DRCT),\naimed at mitigating the loss of spatial information and stabilizing the\ninformation flow through dense-residual connections between layers, thereby\nunleashing the model's potential and saving the model away from information\nbottleneck. Experiment results indicate that our approach surpasses\nstate-of-the-art methods on benchmark datasets and performs commendably at the\nNTIRE-2024 Image Super-Resolution (x4) Challenge. Our source code is available\nat https://github.com/ming053l/DRCT",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPRW2024, NTIRE Image Super-resolution (x4)",
    "pdf_url": "http://arxiv.org/pdf/2404.00722v5",
    "published_date": "2024-03-31 15:34:45 UTC",
    "updated_date": "2024-11-23 18:11:41 UTC"
  },
  {
    "arxiv_id": "2404.00712v2",
    "title": "Survey of Computerized Adaptive Testing: A Machine Learning Perspective",
    "authors": [
      "Qi Liu",
      "Yan Zhuang",
      "Haoyang Bi",
      "Zhenya Huang",
      "Weizhe Huang",
      "Jiatong Li",
      "Junhao Yu",
      "Zirui Liu",
      "Zirui Hu",
      "Yuting Hong",
      "Zachary A. Pardos",
      "Haiping Ma",
      "Mengxiao Zhu",
      "Shijin Wang",
      "Enhong Chen"
    ],
    "abstract": "Computerized Adaptive Testing (CAT) provides an efficient and tailored method\nfor assessing the proficiency of examinees, by dynamically adjusting test\nquestions based on their performance. Widely adopted across diverse fields like\neducation, healthcare, sports, and sociology, CAT has revolutionized testing\npractices. While traditional methods rely on psychometrics and statistics, the\nincreasing complexity of large-scale testing has spurred the integration of\nmachine learning techniques. This paper aims to provide a machine\nlearning-focused survey on CAT, presenting a fresh perspective on this adaptive\ntesting method. By examining the test question selection algorithm at the heart\nof CAT's adaptivity, we shed light on its functionality. Furthermore, we delve\ninto cognitive diagnosis models, question bank construction, and test control\nwithin CAT, exploring how machine learning can optimize these components.\nThrough an analysis of current methods, strengths, limitations, and challenges,\nwe strive to develop robust, fair, and efficient CAT systems. By bridging\npsychometric-driven CAT research with machine learning, this survey advocates\nfor a more inclusive and interdisciplinary approach to the future of adaptive\ntesting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00712v2",
    "published_date": "2024-03-31 15:09:47 UTC",
    "updated_date": "2024-04-05 02:18:29 UTC"
  },
  {
    "arxiv_id": "2404.07227v4",
    "title": "Is Complexity an Illusion?",
    "authors": [
      "Michael Timothy Bennett"
    ],
    "abstract": "Simplicity is held by many to be the key to general intelligence. Simpler\nmodels tend to \"generalise\", identifying the cause or generator of data with\ngreater sample efficiency. The implications of the correlation between\nsimplicity and generalisation extend far beyond computer science, addressing\nquestions of physics and even biology. Yet simplicity is a property of form,\nwhile generalisation is of function. In interactive settings, any correlation\nbetween the two depends on interpretation. In theory there could be no\ncorrelation and yet in practice, there is. Previous theoretical work showed\ngeneralisation to be a consequence of \"weak\" constraints implied by function,\nnot form. Experiments demonstrated choosing weak constraints over simple forms\nyielded a 110-500% improvement in generalisation rate. Here we show that all\nconstraints can take equally simple forms, regardless of weakness. However if\nforms are spatially extended, then function is represented using a finite\nsubset of forms. If function is represented using a finite subset of forms,\nthen we can force a correlation between simplicity and generalisation by making\nweak constraints take simple forms. If function is determined by a goal\ndirected process that favours versatility (e.g. natural selection), then\nefficiency demands weak constraints take simple forms. Complexity has no causal\ninfluence on generalisation, but appears to due to confounding.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication in the Proceedings of the 17th Conference on\n  Artificial General Intelligence, 2024. Definitions shared with\n  arXiv:2302.00843",
    "pdf_url": "http://arxiv.org/pdf/2404.07227v4",
    "published_date": "2024-03-31 13:36:55 UTC",
    "updated_date": "2024-05-30 13:38:42 UTC"
  },
  {
    "arxiv_id": "2404.00685v2",
    "title": "Scaling Properties of Speech Language Models",
    "authors": [
      "Santiago Cuervo",
      "Ricard Marxer"
    ],
    "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without\ntextual resources. Despite significant advances, our current models exhibit\nweak syntax and semantic abilities. However, if the scaling properties of\nneural language models hold for the speech modality, these abilities will\nimprove as the amount of compute used for training increases. In this paper, we\nuse models of this scaling behavior to estimate the scale at which our current\nmethods will yield a SLM with the English proficiency of text-based Large\nLanguage Models (LLMs). We establish a strong correlation between pre-training\nloss and downstream syntactic and semantic performance in SLMs and LLMs, which\nresults in predictable scaling of linguistic performance. We show that the\nlinguistic performance of SLMs scales up to three orders of magnitude more\nslowly than that of text-based LLMs. Additionally, we study the benefits of\nsynthetic data designed to boost semantic understanding and the effects of\ncoarser speech tokenization.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00685v2",
    "published_date": "2024-03-31 13:30:12 UTC",
    "updated_date": "2024-04-16 06:46:18 UTC"
  },
  {
    "arxiv_id": "2404.00684v1",
    "title": "Generative Retrieval as Multi-Vector Dense Retrieval",
    "authors": [
      "Shiguang Wu",
      "Wenda Wei",
      "Mengqi Zhang",
      "Zhumin Chen",
      "Jun Ma",
      "Zhaochun Ren",
      "Maarten de Rijke",
      "Pengjie Ren"
    ],
    "abstract": "Generative retrieval generates identifiers of relevant documents in an\nend-to-end manner using a sequence-to-sequence architecture for a given query.\nThe relation between generative retrieval and other retrieval methods,\nespecially those based on matching within dense retrieval models, is not yet\nfully comprehended. Prior work has demonstrated that generative retrieval with\natomic identifiers is equivalent to single-vector dense retrieval. Accordingly,\ngenerative retrieval exhibits behavior analogous to hierarchical search within\na tree index in dense retrieval when using hierarchical semantic identifiers.\nHowever, prior work focuses solely on the retrieval stage without considering\nthe deep interactions within the decoder of generative retrieval.\n  In this paper, we fill this gap by demonstrating that generative retrieval\nand multi-vector dense retrieval share the same framework for measuring the\nrelevance to a query of a document. Specifically, we examine the attention\nlayer and prediction head of generative retrieval, revealing that generative\nretrieval can be understood as a special case of multi-vector dense retrieval.\nBoth methods compute relevance as a sum of products of query and document\nvectors and an alignment matrix. We then explore how generative retrieval\napplies this framework, employing distinct strategies for computing document\ntoken vectors and the alignment matrix. We have conducted experiments to verify\nour conclusions and show that both paradigms exhibit commonalities of term\nmatching in their alignment matrix.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "12 pages, 5 figures, 8 tables, accepted at SIGIR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00684v1",
    "published_date": "2024-03-31 13:29:43 UTC",
    "updated_date": "2024-03-31 13:29:43 UTC"
  },
  {
    "arxiv_id": "2404.00675v3",
    "title": "LLM meets Vision-Language Models for Zero-Shot One-Class Classification",
    "authors": [
      "Yassir Bendou",
      "Giulia Lioi",
      "Bastien Pasdeloup",
      "Lukas Mauch",
      "Ghouthi Boukli Hacene",
      "Fabien Cardinaux",
      "Vincent Gripon"
    ],
    "abstract": "We consider the problem of zero-shot one-class visual classification,\nextending traditional one-class classification to scenarios where only the\nlabel of the target class is available. This method aims to discriminate\nbetween positive and negative query samples without requiring examples from the\ntarget class. We propose a two-step solution that first queries large language\nmodels for visually confusing objects and then relies on vision-language\npre-trained models (e.g., CLIP) to perform classification. By adapting\nlarge-scale vision benchmarks, we demonstrate the ability of the proposed\nmethod to outperform adapted off-the-shelf alternatives in this setting.\nNamely, we propose a realistic benchmark where negative query samples are drawn\nfrom the same original dataset as positive ones, including a\ngranularity-controlled version of iNaturalist, where negative samples are at a\nfixed distance in the taxonomy tree from the positive ones. To our knowledge,\nwe are the first to demonstrate the ability to discriminate a single category\nfrom other semantically related ones using only its label.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00675v3",
    "published_date": "2024-03-31 12:48:07 UTC",
    "updated_date": "2024-05-27 08:53:15 UTC"
  },
  {
    "arxiv_id": "2404.00673v2",
    "title": "A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures",
    "authors": [
      "Thanh Tam Nguyen",
      "Thanh Trung Huynh",
      "Zhao Ren",
      "Thanh Toan Nguyen",
      "Phi Le Nguyen",
      "Hongzhi Yin",
      "Quoc Viet Hung Nguyen"
    ],
    "abstract": "As the adoption of explainable AI (XAI) continues to expand, the urgency to\naddress its privacy implications intensifies. Despite a growing corpus of\nresearch in AI privacy and explainability, there is little attention on\nprivacy-preserving model explanations. This article presents the first thorough\nsurvey about privacy attacks on model explanations and their countermeasures.\nOur contribution to this field comprises a thorough analysis of research papers\nwith a connected taxonomy that facilitates the categorisation of privacy\nattacks and countermeasures based on the targeted explanations. This work also\nincludes an initial investigation into the causes of privacy leaks. Finally, we\ndiscuss unresolved issues and prospective research directions uncovered in our\nanalysis. This survey aims to be a valuable resource for the research community\nand offers clear insights for those new to this domain. To support ongoing\nresearch, we have established an online resource repository, which will be\ncontinuously updated with new and relevant findings. Interested readers are\nencouraged to access our repository at\nhttps://github.com/tamlhp/awesome-privex.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Revision",
    "pdf_url": "http://arxiv.org/pdf/2404.00673v2",
    "published_date": "2024-03-31 12:44:48 UTC",
    "updated_date": "2024-06-26 07:28:15 UTC"
  },
  {
    "arxiv_id": "2404.00672v1",
    "title": "A General and Efficient Training for Transformer via Token Expansion",
    "authors": [
      "Wenxuan Huang",
      "Yunhang Shen",
      "Jiao Xie",
      "Baochang Zhang",
      "Gaoqi He",
      "Ke Li",
      "Xing Sun",
      "Shaohui Lin"
    ],
    "abstract": "The remarkable performance of Vision Transformers (ViTs) typically requires\nan extremely large training cost. Existing methods have attempted to accelerate\nthe training of ViTs, yet typically disregard method universality with accuracy\ndropping. Meanwhile, they break the training consistency of the original\ntransformers, including the consistency of hyper-parameters, architecture, and\nstrategy, which prevents them from being widely applied to different\nTransformer networks. In this paper, we propose a novel token growth scheme\nToken Expansion (termed ToE) to achieve consistent training acceleration for\nViTs. We introduce an \"initialization-expansion-merging\" pipeline to maintain\nthe integrity of the intermediate feature distribution of original\ntransformers, preventing the loss of crucial learnable information in the\ntraining process. ToE can not only be seamlessly integrated into the training\nand fine-tuning process of transformers (e.g., DeiT and LV-ViT), but also\neffective for efficient training frameworks (e.g., EfficientTrain), without\ntwisting the original training hyper-parameters, architecture, and introducing\nadditional training strategies. Extensive experiments demonstrate that ToE\nachieves about 1.3x faster for the training of ViTs in a lossless manner, or\neven with performance gains over the full-token training baselines. Code is\navailable at https://github.com/Osilly/TokenExpansion .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to CVPR 2024. Code is available at\n  https://github.com/Osilly/TokenExpansion",
    "pdf_url": "http://arxiv.org/pdf/2404.00672v1",
    "published_date": "2024-03-31 12:44:24 UTC",
    "updated_date": "2024-03-31 12:44:24 UTC"
  },
  {
    "arxiv_id": "2404.00657v1",
    "title": "Observations on Building RAG Systems for Technical Documents",
    "authors": [
      "Sumit Soman",
      "Sujoy Roychowdhury"
    ],
    "abstract": "Retrieval augmented generation (RAG) for technical documents creates\nchallenges as embeddings do not often capture domain information. We review\nprior art for important factors affecting RAG and perform experiments to\nhighlight best practices and potential challenges to build RAG systems for\ntechnical documents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a Tiny Paper at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00657v1",
    "published_date": "2024-03-31 12:01:34 UTC",
    "updated_date": "2024-03-31 12:01:34 UTC"
  },
  {
    "arxiv_id": "2404.00656v3",
    "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
    "authors": [
      "Shujie Hu",
      "Long Zhou",
      "Shujie Liu",
      "Sanyuan Chen",
      "Lingwei Meng",
      "Hongkun Hao",
      "Jing Pan",
      "Xunying Liu",
      "Jinyu Li",
      "Sunit Sivasankaran",
      "Linquan Liu",
      "Furu Wei"
    ],
    "abstract": "The recent advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing, progressively broadening their scope\nto multimodal perception and generation. However, effectively integrating\nlistening capabilities into LLMs poses significant challenges, particularly\nwith respect to generalizing across varied contexts and executing complex\nauditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech\nlarge language model with dual encoders, and a prompt-aware LoRA weight\nadapter, optimized by a two-stage curriculum learning approach. Leveraging dual\nencoders, we decouple different types of speech information, utilizing a\nWhisper encoder to process the semantic content of speech, and a WavLM encoder\nto capture the unique characteristics of the speaker's identity. Within the\ncurriculum learning framework, WavLLM first builds its foundational\ncapabilities by optimizing on mixed elementary single tasks, followed by\nadvanced multi-task training on more complex tasks such as combinations of the\nelementary tasks. To enhance the flexibility and adherence to different tasks\nand instructions, a prompt-aware LoRA weight adapter is introduced in the\nsecond advanced multi-task training stage. We validate the proposed model on\nuniversal speech benchmarks including tasks such as ASR, ST, SV, ER, and also\napply it to specialized datasets like Gaokao English listening comprehension\nset for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments\ndemonstrate that the proposed model achieves state-of-the-art performance\nacross a range of speech tasks on the same model size, exhibiting robust\ngeneralization capabilities in executing complex tasks using CoT approach.\nFurthermore, our model successfully completes Gaokao tasks without specialized\ntraining. The codes, models, audio, and Gaokao evaluation set can be accessed\nat \\url{aka.ms/wavllm}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted by EMNLP2024 findings",
    "pdf_url": "http://arxiv.org/pdf/2404.00656v3",
    "published_date": "2024-03-31 12:01:32 UTC",
    "updated_date": "2024-09-21 15:27:30 UTC"
  },
  {
    "arxiv_id": "2404.00651v1",
    "title": "Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration",
    "authors": [
      "Yibo Wang",
      "Jiang Zhao"
    ],
    "abstract": "Recent advancements in deep reinforcement learning (RL) have demonstrated\nnotable progress in sample efficiency, spanning both model-based and model-free\nparadigms. Despite the identification and mitigation of specific bottlenecks in\nprior works, the agent's exploration ability remains under-emphasized in the\nrealm of sample-efficient RL. This paper investigates how to achieve\nsample-efficient exploration in continuous control tasks. We introduce an RL\nalgorithm that incorporates a predictive model and off-policy learning\nelements, where an online planner enhanced by a novelty-aware terminal value\nfunction is employed for sample collection. Leveraging the forward predictive\nerror within a latent state space, we derive an intrinsic reward without\nincurring parameters overhead. This reward establishes a solid connection to\nmodel uncertainty, allowing the agent to effectively overcome the asymptotic\nperformance gap. Through extensive experiments, our method shows competitive or\neven superior performance compared to prior works, especially the sparse reward\ncases.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2404.00651v1",
    "published_date": "2024-03-31 11:39:11 UTC",
    "updated_date": "2024-03-31 11:39:11 UTC"
  },
  {
    "arxiv_id": "2404.00636v3",
    "title": "Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation",
    "authors": [
      "Taekyung Ki",
      "Dongchan Min",
      "Gyeongsu Chae"
    ],
    "abstract": "In this paper, we present Export3D, a one-shot 3D-aware portrait animation\nmethod that is able to control the facial expression and camera view of a given\nportrait image. To achieve this, we introduce a tri-plane generator with an\neffective expression conditioning method, which directly generates a tri-plane\nof 3D prior by transferring the expression parameter of 3DMM into the source\nimage. The tri-plane is then decoded into the image of different view through a\ndifferentiable volume rendering. Existing portrait animation methods heavily\nrely on image warping to transfer the expression in the motion space,\nchallenging on disentanglement of appearance and expression. In contrast, we\npropose a contrastive pre-training framework for appearance-free expression\nparameter, eliminating undesirable appearance swap when transferring a\ncross-identity expression. Extensive experiments show that our pre-training\nframework can learn the appearance-free expression representation hidden in\n3DMM, and our model can generate 3D-aware expression controllable portrait\nimages without appearance swap in the cross-identity manner.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024. Project page: https://export3d.github.io",
    "pdf_url": "http://arxiv.org/pdf/2404.00636v3",
    "published_date": "2024-03-31 10:13:55 UTC",
    "updated_date": "2024-07-23 10:47:22 UTC"
  },
  {
    "arxiv_id": "2404.00614v2",
    "title": "Learning to Plan for Language Modeling from Unlabeled Data",
    "authors": [
      "Nathan Cornille",
      "Marie-Francine Moens",
      "Florian Mai"
    ],
    "abstract": "By training to predict the next token in an unlabeled corpus, large language\nmodels learn to perform many tasks without any labeled data. However, their\nnext-token-prediction objective arguably limits their performance in scenarios\nthat require planning, such as writing a coherent article. In this paper, we\ntrain a module for planning the future writing process via a self-supervised\nlearning objective. Given the textual context, this planning module learns to\npredict future abstract writing actions, which correspond to centroids in a\nclustered text embedding space. By conditioning on these actions, our model\nextends the successful language model formula to more abstract planning in an\nunsupervised way. Empirically, we demonstrate that our method improves language\nmodeling performance in general, particularly with respect to the text\nstructure. Because our framework uses a planner module that is unsupervised and\nexternal to the language model, new planner modules can be trained at large\nscale and easily be shared with the community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at COLM 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00614v2",
    "published_date": "2024-03-31 09:04:01 UTC",
    "updated_date": "2024-07-31 12:25:14 UTC"
  },
  {
    "arxiv_id": "2404.00604v1",
    "title": "Extensive Self-Contrast Enables Feedback-Free Language Model Alignment",
    "authors": [
      "Xiao Liu",
      "Xixuan Song",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) has been a central\ntechnique for recent large language model (LLM) alignment. However, its heavy\ndependence on costly human or LLM-as-Judge preference feedback could stymie its\nwider applications. In this work, we introduce Self-Contrast, a feedback-free\nlarge language model alignment method via exploiting extensive self-generated\nnegatives. With only supervised fine-tuning (SFT) targets, Self-Contrast\nleverages the LLM itself to generate massive diverse candidates, and harnesses\na pre-trained embedding model to filter multiple negatives according to text\nsimilarity. Theoretically, we illustrate that in this setting, merely scaling\nnegative responses can still effectively approximate situations with more\nbalanced positive and negative preference annotations. Our experiments with\ndirect preference optimization (DPO) on three datasets show that, Self-Contrast\ncould consistently outperform SFT and standard DPO training by large margins.\nAnd as the number of self-generated negatives increases, the performance of\nSelf-Contrast continues to grow. Code and data are available at\nhttps://github.com/THUDM/Self-Contrast.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00604v1",
    "published_date": "2024-03-31 08:30:15 UTC",
    "updated_date": "2024-03-31 08:30:15 UTC"
  },
  {
    "arxiv_id": "2404.00600v2",
    "title": "AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight",
    "authors": [
      "Nicola Fabiano"
    ],
    "abstract": "The imposing evolution of artificial intelligence systems and, specifically,\nof Large Language Models (LLM) makes it necessary to carry out assessments of\ntheir level of risk and the impact they may have in the area of privacy,\npersonal data protection and at an ethical level, especially on the weakest and\nmost vulnerable. This contribution addresses human oversight, ethical\noversight, and privacy impact assessment.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00600v2",
    "published_date": "2024-03-31 08:14:25 UTC",
    "updated_date": "2024-04-02 06:05:29 UTC"
  },
  {
    "arxiv_id": "2404.00599v1",
    "title": "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories",
    "authors": [
      "Jia Li",
      "Ge Li",
      "Xuanming Zhang",
      "Yihong Dong",
      "Zhi Jin"
    ],
    "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open\nquestion. Existing benchmarks demonstrate poor alignment with real-world code\nrepositories and are insufficient to evaluate the coding abilities of LLMs.\nThis paper proposes a new benchmark - EvoCodeBench to address the preceding\nproblems, which has three primary advances. (1) EvoCodeBench aligns with\nreal-world repositories in multiple dimensions, e.g., code distributions and\ndependency distributions. (2) EvoCodeBench offers comprehensive annotations\n(e.g., requirements, reference code, and reference dependencies), and robust\nevaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving\nbenchmark to avoid data leakage. We build an automatic pipeline to update\nEvoCodeBench from the latest repositories. We release the first version -\nEvoCodeBench-2403, containing 275 samples from 25 real-world repositories.\nBased on EvoCodeBench, we propose repository-level code generation and evaluate\n10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa,\nGemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs\nin real-world repositories. For example, the highest Pass@1 of gpt-4 only is\n20.73% in our experiments. We also analyze failed cases and summarize the\nshortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all\nprompts, and LLMs' completions for further community analysis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "Data: https://github.com/seketeam/EvoCodeBench",
    "pdf_url": "http://arxiv.org/pdf/2404.00599v1",
    "published_date": "2024-03-31 08:10:50 UTC",
    "updated_date": "2024-03-31 08:10:50 UTC"
  },
  {
    "arxiv_id": "2404.00593v1",
    "title": "LAESI: Leaf Area Estimation with Synthetic Imagery",
    "authors": [
      "Jacek Kałużny",
      "Yannik Schreckenberg",
      "Karol Cyganik",
      "Peter Annighöfer",
      "Sören Pirk",
      "Dominik L. Michels",
      "Mikolaj Cieslak",
      "Farhah Assaad-Gerbert",
      "Bedrich Benes",
      "Wojciech Pałubicki"
    ],
    "abstract": "We introduce LAESI, a Synthetic Leaf Dataset of 100,000 synthetic leaf images\non millimeter paper, each with semantic masks and surface area labels. This\ndataset provides a resource for leaf morphology analysis primarily aimed at\nbeech and oak leaves. We evaluate the applicability of the dataset by training\nmachine learning models for leaf surface area prediction and semantic\nsegmentation, using real images for validation. Our validation shows that these\nmodels can be trained to predict leaf surface area with a relative error not\ngreater than an average human annotator. LAESI also provides an efficient\nframework based on 3D procedural models and generative AI for the large-scale,\ncontrollable generation of data with potential further applications in\nagriculture and biology. We evaluate the inclusion of generative AI in our\nprocedural data generation pipeline and show how data filtering based on\nannotation consistency results in datasets which allow training the highest\nperforming vision models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "68T07, 68T45",
      "I.2.10; I.4.6"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 12 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2404.00593v1",
    "published_date": "2024-03-31 07:56:07 UTC",
    "updated_date": "2024-03-31 07:56:07 UTC"
  },
  {
    "arxiv_id": "2404.00588v1",
    "title": "Memory-based Cross-modal Semantic Alignment Network for Radiology Report Generation",
    "authors": [
      "Yitian Tao",
      "Liyan Ma",
      "Jing Yu",
      "Han Zhang"
    ],
    "abstract": "Generating radiology reports automatically reduces the workload of\nradiologists and helps the diagnoses of specific diseases. Many existing\nmethods take this task as modality transfer process. However, since the key\ninformation related to disease accounts for a small proportion in both image\nand report, it is hard for the model to learn the latent relation between the\nradiology image and its report, thus failing to generate fluent and accurate\nradiology reports. To tackle this problem, we propose a memory-based\ncross-modal semantic alignment model (MCSAM) following an encoder-decoder\nparadigm. MCSAM includes a well initialized long-term clinical memory bank to\nlearn disease-related representations as well as prior knowledge for different\nmodalities to retrieve and use the retrieved memory to perform feature\nconsolidation. To ensure the semantic consistency of the retrieved cross modal\nprior knowledge, a cross-modal semantic alignment module (SAM) is proposed. SAM\nis also able to generate semantic visual feature embeddings which can be added\nto the decoder and benefits report generation. More importantly, to memorize\nthe state and additional information while generating reports with the decoder,\nwe use learnable memory tokens which can be seen as prompts. Extensive\nexperiments demonstrate the promising performance of our proposed method which\ngenerates state-of-the-art performance on the MIMIC-CXR dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.00588v1",
    "published_date": "2024-03-31 07:30:41 UTC",
    "updated_date": "2024-03-31 07:30:41 UTC"
  },
  {
    "arxiv_id": "2404.00586v2",
    "title": "RLGNet: Repeating-Local-Global History Network for Temporal Knowledge Graph Reasoning",
    "authors": [
      "Ao Lv",
      "Guige Ouyang",
      "Yongzhong Huang",
      "Yue Chen",
      "Haoran Xie"
    ],
    "abstract": "Temporal Knowledge Graph (TKG) reasoning involves predicting future events\nbased on historical information. However, due to the unpredictability of future\nevents, this task is highly challenging. To address this issue, we propose a\nmulti-scale hybrid architecture model based on ensemble learning, called RLGNet\n(Repeating-Local-Global History Network). Inspired by the application of\nmulti-scale information in other fields, we introduce the concept of\nmulti-scale information into TKG reasoning. Specifically, RLGNet captures and\nintegrates different levels of historical information by combining modules that\nprocess information at various scales. The model comprises three modules: the\nRepeating History Module focuses on identifying repetitive patterns and trends\nin historical data, the Local History Module captures short-term changes and\ndetails, and the Global History Module provides a macro perspective on\nlong-term changes. Additionally, to address the limitations of previous\nsingle-architecture models in generalizing across single-step and multi-step\nreasoning tasks, we adopted architectures based on Recurrent Neural Networks\n(RNN) and Multi-Layer Perceptrons (MLP) for the Local and Global History\nModules, respectively. This hybrid architecture design enables the model to\ncomplement both multi-step and single-step reasoning capabilities. Finally, to\naddress the issue of noise in TKGs, we adopt an ensemble learning strategy,\ncombining the predictions of the three modules to reduce the impact of noise on\nthe final prediction results. In the evaluation on six benchmark datasets, our\napproach generally outperforms existing TKG reasoning models in multi-step and\nsingle-step reasoning tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00586v2",
    "published_date": "2024-03-31 07:19:29 UTC",
    "updated_date": "2024-07-28 08:44:11 UTC"
  },
  {
    "arxiv_id": "2404.01343v4",
    "title": "CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs",
    "authors": [
      "Jingzhe Shi",
      "Jialuo Li",
      "Qinwei Ma",
      "Zaiwen Yang",
      "Huan Ma",
      "Lei Li"
    ],
    "abstract": "Businesses and software platforms are increasingly turning to Large Language\nModels (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance\nwith file access or as reasoning agents for customer service. However, current\nLLM-based customer service models have limited integration with customer\nprofiles and lack the operational capabilities necessary for effective service.\nMoreover, existing API integrations emphasize diversity over the precision and\nerror avoidance essential in real-world customer service scenarios. To address\nthese issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile\nin existing System), designed to: (1) efficiently utilize existing databases or\nsystems for accessing user information or interacting with these systems\nfollowing existing guidelines; (2) provide accurate and reasonable responses or\ncarry out required operations in the system while avoiding harmful operations;\nand (3) leverage a combination of small and large LLMs to achieve satisfying\nperformance at a reasonable inference cost. We introduce a practical dataset,\nthe CPHOS-dataset, which includes a database, guiding files, and QA pairs\ncollected from CPHOS, an online platform that facilitates the organization of\nsimulated Physics Olympiads for high school teachers and students. We have\nconducted extensive experiments to validate the performance of our proposed\nCHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how\nLLMs can enhance or serve as alternatives to human customer service. Code for\nour proposed architecture and dataset can be found at\n{https://github.com/JingzheShi/CHOPS}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by COLM 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01343v4",
    "published_date": "2024-03-31 07:11:48 UTC",
    "updated_date": "2024-07-17 07:26:47 UTC"
  },
  {
    "arxiv_id": "2404.00579v2",
    "title": "A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)",
    "authors": [
      "Yashar Deldjoo",
      "Zhankui He",
      "Julian McAuley",
      "Anton Korikov",
      "Scott Sanner",
      "Arnau Ramisa",
      "René Vidal",
      "Maheswaran Sathiamoorthy",
      "Atoosa Kasirzadeh",
      "Silvia Milano"
    ],
    "abstract": "Traditional recommender systems (RS) typically use user-item rating histories\nas their main data source. However, deep generative models now have the\ncapability to model and sample from complex data distributions, including\nuser-item interactions, text, images, and videos, enabling novel recommendation\ntasks. This comprehensive, multidisciplinary survey connects key advancements\nin RS using Generative Models (Gen-RecSys), covering: interaction-driven\ngenerative models; the use of large language models (LLM) and textual data for\nnatural language recommendation; and the integration of multimodal models for\ngenerating and processing images/videos in RS. Our work highlights necessary\nparadigms for evaluating the impact and harm of Gen-RecSys and identifies open\nchallenges. This survey accompanies a tutorial presented at ACM KDD'24, with\nsupporting materials provided at: https://encr.pw/vDhLq.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "This survey accompanies a tutorial presented at ACM KDD'24",
    "pdf_url": "http://arxiv.org/pdf/2404.00579v2",
    "published_date": "2024-03-31 06:57:57 UTC",
    "updated_date": "2024-07-04 15:06:42 UTC"
  },
  {
    "arxiv_id": "2404.00576v1",
    "title": "Automated Bi-Fold Weighted Ensemble Algorithms and its Application to Brain Tumor Detection and Classification",
    "authors": [
      "PoTsang B. Huang",
      "Muhammad Rizwan",
      "Mehboob Ali"
    ],
    "abstract": "The uncontrolled and unstructured growth of brain cells is known as brain\ntumor, which has one of the highest mortality rates among diseases from all\ntypes of cancers. Due to limited diagnostic and treatment capabilities, they\npose significant challenges, especially in third-world countries. Early\ndiagnosis plays a vital role in effectively managing brain tumors and reducing\nmortality rates. However, the availability of diagnostic methods is hindered by\nvarious limitations, including high costs and lengthy result acquisition times,\nimpeding early detection of the disease. In this study, we present two\ncutting-edge bi-fold weighted voting ensemble models that aim to boost the\neffectiveness of weighted ensemble methods. These two proposed methods combine\nthe classification outcomes from multiple classifiers and determine the optimal\nresult by selecting the one with the highest probability in the first approach,\nand the highest weighted prediction in the second technique. These approaches\nsignificantly improve the overall performance of weighted ensemble techniques.\nIn the first proposed method, we improve the soft voting technique (SVT) by\nintroducing a novel unsupervised weight calculating schema (UWCS) to enhance\nits weight assigning capability, known as the extended soft voting technique\n(ESVT). Secondly, we propose a novel weighted method (NWM) by using the\nproposed UWCS. Both of our approaches incorporate three distinct models: a\ncustom-built CNN, VGG-16, and InceptionResNetV2 which has been trained on\npublicly available datasets. The effectiveness of our proposed systems is\nevaluated through blind testing, where exceptional results are achieved. We\nthen establish a comparative analysis of the performance of our proposed\nmethods with that of SVT to show their superiority and effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00576v1",
    "published_date": "2024-03-31 06:38:08 UTC",
    "updated_date": "2024-03-31 06:38:08 UTC"
  },
  {
    "arxiv_id": "2404.01342v1",
    "title": "DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model",
    "authors": [
      "Lirui Zhao",
      "Yue Yang",
      "Kaipeng Zhang",
      "Wenqi Shao",
      "Yuxin Zhang",
      "Yu Qiao",
      "Ping Luo",
      "Rongrong Ji"
    ],
    "abstract": "Text-to-image (T2I) generative models have attracted significant attention\nand found extensive applications within and beyond academic research. For\nexample, the Civitai community, a platform for T2I innovation, currently hosts\nan impressive array of 74,492 distinct models. However, this diversity presents\na formidable challenge in selecting the most appropriate model and parameters,\na process that typically requires numerous trials. Drawing inspiration from the\ntool usage research of large language models (LLMs), we introduce DiffAgent, an\nLLM agent designed to screen the accurate selection in seconds via API calls.\nDiffAgent leverages a novel two-stage training framework, SFTA, enabling it to\naccurately align T2I API responses with user input in accordance with human\npreferences. To train and evaluate DiffAgent's capabilities, we present\nDABench, a comprehensive dataset encompassing an extensive range of T2I APIs\nfrom the community. Our evaluations reveal that DiffAgent not only excels in\nidentifying the appropriate T2I API but also underscores the effectiveness of\nthe SFTA training framework. Codes are available at\nhttps://github.com/OpenGVLab/DiffAgent.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published as a conference paper at CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01342v1",
    "published_date": "2024-03-31 06:28:15 UTC",
    "updated_date": "2024-03-31 06:28:15 UTC"
  },
  {
    "arxiv_id": "2404.01341v2",
    "title": "Block-Diagonal Guided DBSCAN Clustering",
    "authors": [
      "Weibing Zhao"
    ],
    "abstract": "Cluster analysis plays a crucial role in database mining, and one of the most\nwidely used algorithms in this field is DBSCAN. However, DBSCAN has several\nlimitations, such as difficulty in handling high-dimensional large-scale data,\nsensitivity to input parameters, and lack of robustness in producing clustering\nresults. This paper introduces an improved version of DBSCAN that leverages the\nblock-diagonal property of the similarity graph to guide the clustering\nprocedure of DBSCAN. The key idea is to construct a graph that measures the\nsimilarity between high-dimensional large-scale data points and has the\npotential to be transformed into a block-diagonal form through an unknown\npermutation, followed by a cluster-ordering procedure to generate the desired\npermutation. The clustering structure can be easily determined by identifying\nthe diagonal blocks in the permuted graph. We propose a gradient descent-based\nmethod to solve the proposed problem. Additionally, we develop a DBSCAN-based\npoints traversal algorithm that identifies clusters with high densities in the\ngraph and generates an augmented ordering of clusters. The block-diagonal\nstructure of the graph is then achieved through permutation based on the\ntraversal order, providing a flexible foundation for both automatic and\ninteractive cluster analysis. We introduce a split-and-refine algorithm to\nautomatically search for all diagonal blocks in the permuted graph with\ntheoretically optimal guarantees under specific cases. We extensively evaluate\nour proposed approach on twelve challenging real-world benchmark clustering\ndatasets and demonstrate its superior performance compared to the\nstate-of-the-art clustering method on every dataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2009.04552 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2404.01341v2",
    "published_date": "2024-03-31 05:04:38 UTC",
    "updated_date": "2024-04-27 01:34:41 UTC"
  },
  {
    "arxiv_id": "2404.00560v1",
    "title": "A Theory for Length Generalization in Learning to Reason",
    "authors": [
      "Changnan Xiao",
      "Bing Liu"
    ],
    "abstract": "Length generalization (LG) is a challenging problem in learning to reason. It\nrefers to the phenomenon that when trained on reasoning problems of smaller\nlengths or sizes, the resulting model struggles with problems of larger sizes\nor lengths. Although LG has been studied by many researchers, the challenge\nremains. This paper proposes a theoretical study of LG for problems whose\nreasoning processes can be modeled as DAGs (directed acyclic graphs). The paper\nfirst identifies and proves the conditions under which LG can be achieved in\nlearning to reason. It then designs problem representations based on the theory\nto learn to solve challenging reasoning problems like parity, addition, and\nmultiplication, using a Transformer to achieve perfect LG.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: text overlap with arXiv:2311.16173",
    "pdf_url": "http://arxiv.org/pdf/2404.00560v1",
    "published_date": "2024-03-31 04:44:22 UTC",
    "updated_date": "2024-03-31 04:44:22 UTC"
  },
  {
    "arxiv_id": "2404.00544v1",
    "title": "Deep Extrinsic Manifold Representation for Vision Tasks",
    "authors": [
      "Tongtong Zhang",
      "Xian Wei",
      "Yuanxiang Li"
    ],
    "abstract": "Non-Euclidean data is frequently encountered across different fields, yet\nthere is limited literature that addresses the fundamental challenge of\ntraining neural networks with manifold representations as outputs. We introduce\nthe trick named Deep Extrinsic Manifold Representation (DEMR) for visual tasks\nin this context. DEMR incorporates extrinsic manifold embedding into deep\nneural networks, which helps generate manifold representations. The DEMR\napproach does not directly optimize the complex geodesic loss. Instead, it\nfocuses on optimizing the computation graph within the embedded Euclidean\nspace, allowing for adaptability to various architectural requirements. We\nprovide empirical evidence supporting the proposed concept on two types of\nmanifolds, $SE(3)$ and its associated quotient manifolds. This evidence offers\ntheoretical assurances regarding feasibility, asymptotic properties, and\ngeneralization capability. The experimental results show that DEMR effectively\nadapts to point cloud alignment, producing outputs in $ SE(3) $, as well as in\nillumination subspace learning with outputs on the Grassmann manifold.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00544v1",
    "published_date": "2024-03-31 03:16:08 UTC",
    "updated_date": "2024-03-31 03:16:08 UTC"
  },
  {
    "arxiv_id": "2404.00540v1",
    "title": "Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches",
    "authors": [
      "Lingxuan Wu",
      "Xiao Yang",
      "Yinpeng Dong",
      "Liuwei Xie",
      "Hang Su",
      "Jun Zhu"
    ],
    "abstract": "The vulnerability of deep neural networks to adversarial patches has\nmotivated numerous defense strategies for boosting model robustness. However,\nthe prevailing defenses depend on single observation or pre-established\nadversary information to counter adversarial patches, often failing to be\nconfronted with unseen or adaptive adversarial attacks and easily exhibiting\nunsatisfying performance in dynamic 3D environments. Inspired by active human\nperception and recurrent feedback mechanisms, we develop Embodied Active\nDefense (EAD), a proactive defensive strategy that actively contextualizes\nenvironmental information to address misaligned adversarial patches in 3D\nreal-world settings. To achieve this, EAD develops two central recurrent\nsub-modules, i.e., a perception module and a policy module, to implement two\ncritical functions of active vision. These models recurrently process a series\nof beliefs and observations, facilitating progressive refinement of their\ncomprehension of the target object and enabling the development of strategic\nactions to counter adversarial patches in 3D environments. To optimize learning\nefficiency, we incorporate a differentiable approximation of environmental\ndynamics and deploy patches that are agnostic to the adversary strategies.\nExtensive experiments demonstrate that EAD substantially enhances robustness\nagainst a variety of patches within just a few steps through its action policy\nin safety-critical tasks (e.g., face recognition and object detection), without\ncompromising standard accuracy. Furthermore, due to the attack-agnostic\ncharacteristic, EAD facilitates excellent generalization to unseen attacks,\ndiminishing the averaged attack success rate by 95 percent across a range of\nunseen adversarial attacks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "27pages",
    "pdf_url": "http://arxiv.org/pdf/2404.00540v1",
    "published_date": "2024-03-31 03:02:35 UTC",
    "updated_date": "2024-03-31 03:02:35 UTC"
  },
  {
    "arxiv_id": "2404.01340v2",
    "title": "From Similarity to Superiority: Channel Clustering for Time Series Forecasting",
    "authors": [
      "Jialin Chen",
      "Jan Eric Lenssen",
      "Aosong Feng",
      "Weihua Hu",
      "Matthias Fey",
      "Leandros Tassiulas",
      "Jure Leskovec",
      "Rex Ying"
    ],
    "abstract": "Time series forecasting has attracted significant attention in recent\ndecades. Previous studies have demonstrated that the Channel-Independent (CI)\nstrategy improves forecasting performance by treating different channels\nindividually, while it leads to poor generalization on unseen instances and\nignores potentially necessary interactions between channels. Conversely, the\nChannel-Dependent (CD) strategy mixes all channels with even irrelevant and\nindiscriminate information, which, however, results in oversmoothing issues and\nlimits forecasting accuracy. There is a lack of channel strategy that\neffectively balances individual channel treatment for improved forecasting\nperformance without overlooking essential interactions between channels.\nMotivated by our observation of a correlation between the time series model's\nperformance boost against channel mixing and the intrinsic similarity on a pair\nof channels, we developed a novel and adaptable Channel Clustering Module\n(CCM). CCM dynamically groups channels characterized by intrinsic similarities\nand leverages cluster information instead of individual channel identities,\ncombining the best of CD and CI worlds. Extensive experiments on real-world\ndatasets demonstrate that CCM can (1) boost the performance of CI and CD models\nby an average margin of 2.4% and 7.2% on long-term and short-term forecasting,\nrespectively; (2) enable zero-shot forecasting with mainstream time series\nforecasting models; (3) uncover intrinsic time series patterns among channels\nand improve interpretability of complex time series models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.01340v2",
    "published_date": "2024-03-31 02:46:27 UTC",
    "updated_date": "2024-11-06 05:38:51 UTC"
  },
  {
    "arxiv_id": "2404.00530v2",
    "title": "Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization",
    "authors": [
      "Hritik Bansal",
      "Ashima Suvarna",
      "Gantavya Bhatt",
      "Nanyun Peng",
      "Kai-Wei Chang",
      "Aditya Grover"
    ],
    "abstract": "A common technique for aligning large language models (LLMs) relies on\nacquiring human preferences by comparing multiple generations conditioned on a\nfixed context. This method, however, relies solely on pairwise comparisons,\nwhere the generations are evaluated within an identical context. While\neffective to such conditional preferences often fail to encompass the nuanced\nand multidimensional nature of human preferences. In this work, we revisit the\ntraditional paradigm of preference acquisition and propose a new axis based on\neliciting preferences jointly over the instruction-response pairs. Unlike prior\npreference optimizations, which are designed for conditional ranking protocols\n(e.g., DPO), we propose Joint Preference Optimization (JPO), a new preference\noptimization objective that upweights the joint probability of the chosen\ninstruction-response pair over the rejected instruction-response pair.\nInterestingly, LLMs trained with joint instruction-response preference data\nusing JPO outperform LLM trained with DPO by $5.2\\%$ and $3.3\\%$ win-rate for\nsummarization and open-ended dialogue datasets, respectively. Our findings\nreveal that joint preferences over instruction and response pairs can\nsignificantly enhance the alignment of LLMs by tapping into a broader spectrum\nof human preference elicitation. The data and code is available at\nhttps://github.com/Hritikbansal/dove.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 16 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.00530v2",
    "published_date": "2024-03-31 02:05:40 UTC",
    "updated_date": "2025-01-07 20:36:35 UTC"
  },
  {
    "arxiv_id": "2404.00526v1",
    "title": "The Emotional Impact of Game Duration: A Framework for Understanding Player Emotions in Extended Gameplay Sessions",
    "authors": [
      "Anoop Kumar",
      "Suresh Dodda",
      "Navin Kamuni",
      "Venkata Sai Mahesh Vuppalapati"
    ],
    "abstract": "Video games have played a crucial role in entertainment since their\ndevelopment in the 1970s, becoming even more prominent during the lockdown\nperiod when people were looking for ways to entertain them. However, at that\ntime, players were unaware of the significant impact that playtime could have\non their feelings. This has made it challenging for designers and developers to\ncreate new games since they have to control the emotional impact that these\ngames will take on players. Thus, the purpose of this study is to look at how a\nplayer's emotions are affected by the duration of the game. In order to achieve\nthis goal, a framework for emotion detection is created. According to the\nexperiment's results, the volunteers' general ability to express emotions\nincreased from 20 to 60 minutes. In comparison to shorter gameplay sessions,\nthe experiment found that extended gameplay sessions did significantly affect\nthe player's emotions. According to the results, it was recommended that in\norder to lessen the potential emotional impact that playing computer and video\ngames may have in the future, game producers should think about creating\nshorter, entertaining games.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00526v1",
    "published_date": "2024-03-31 02:01:05 UTC",
    "updated_date": "2024-03-31 02:01:05 UTC"
  },
  {
    "arxiv_id": "2404.07225v1",
    "title": "Unveiling the Impact of Macroeconomic Policies: A Double Machine Learning Approach to Analyzing Interest Rate Effects on Financial Markets",
    "authors": [
      "Anoop Kumar",
      "Suresh Dodda",
      "Navin Kamuni",
      "Rajeev Kumar Arora"
    ],
    "abstract": "This study examines the effects of macroeconomic policies on financial\nmarkets using a novel approach that combines Machine Learning (ML) techniques\nand causal inference. It focuses on the effect of interest rate changes made by\nthe US Federal Reserve System (FRS) on the returns of fixed income and equity\nfunds between January 1986 and December 2021. The analysis makes a distinction\nbetween actively and passively managed funds, hypothesizing that the latter are\nless susceptible to changes in interest rates. The study contrasts gradient\nboosting and linear regression models using the Double Machine Learning (DML)\nframework, which supports a variety of statistical learning techniques. Results\nindicate that gradient boosting is a useful tool for predicting fund returns;\nfor example, a 1% increase in interest rates causes an actively managed fund's\nreturn to decrease by -11.97%. This understanding of the relationship between\ninterest rates and fund performance provides opportunities for additional\nresearch and insightful, data-driven advice for fund managers and investors",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-fin.ST",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.07225v1",
    "published_date": "2024-03-31 01:55:21 UTC",
    "updated_date": "2024-03-31 01:55:21 UTC"
  },
  {
    "arxiv_id": "2404.02923v1",
    "title": "An Unsupervised Adversarial Autoencoder for Cyber Attack Detection in Power Distribution Grids",
    "authors": [
      "Mehdi Jabbari Zideh",
      "Mohammad Reza Khalghani",
      "Sarika Khushalani Solanki"
    ],
    "abstract": "Detection of cyber attacks in smart power distribution grids with unbalanced\nconfigurations poses challenges due to the inherent nonlinear nature of these\nuncertain and stochastic systems. It originates from the intermittent\ncharacteristics of the distributed energy resources (DERs) generation and load\nvariations. Moreover, the unknown behavior of cyber attacks, especially false\ndata injection attacks (FDIAs) in the distribution grids with complex temporal\ncorrelations and the limited amount of labeled data increases the vulnerability\nof the grids and imposes a high risk in the secure and reliable operation of\nthe grids. To address these challenges, this paper proposes an unsupervised\nadversarial autoencoder (AAE) model to detect FDIAs in unbalanced power\ndistribution grids integrated with DERs, i.e., PV systems and wind generation.\nThe proposed method utilizes long short-term memory (LSTM) in the structure of\nthe autoencoder to capture the temporal dependencies in the time-series\nmeasurements and leverages the power of generative adversarial networks (GANs)\nfor better reconstruction of the input data. The advantage of the proposed\ndata-driven model is that it can detect anomalous points for the system\noperation without reliance on abstract models or mathematical representations.\nTo evaluate the efficacy of the approach, it is tested on IEEE 13-bus and\n123-bus systems with historical meteorological data (wind speed, ambient\ntemperature, and solar irradiance) as well as historical real-world load data\nunder three types of data falsification functions. The comparison of the\ndetection results of the proposed model with other unsupervised learning\nmethods verifies its superior performance in detecting cyber attacks in\nunbalanced power distribution grids.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.02923v1",
    "published_date": "2024-03-31 01:20:01 UTC",
    "updated_date": "2024-03-31 01:20:01 UTC"
  },
  {
    "arxiv_id": "2404.01339v1",
    "title": "Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation",
    "authors": [
      "Rohan Chaudhury",
      "Mihir Godbole",
      "Aakash Garg",
      "Jinsil Hwaryoung Seo"
    ],
    "abstract": "Contemporary conversational systems often present a significant limitation:\ntheir responses lack the emotional depth and disfluent characteristic of human\ninteractions. This absence becomes particularly noticeable when users seek more\npersonalized and empathetic interactions. Consequently, this makes them seem\nmechanical and less relatable to human users. Recognizing this gap, we embarked\non a journey to humanize machine communication, to ensure AI systems not only\ncomprehend but also resonate. To address this shortcoming, we have designed an\ninnovative speech synthesis pipeline. Within this framework, a cutting-edge\nlanguage model introduces both human-like emotion and disfluencies in a\nzero-shot setting. These intricacies are seamlessly integrated into the\ngenerated text by the language model during text generation, allowing the\nsystem to mirror human speech patterns better, promoting more intuitive and\nnatural user interactions. These generated elements are then adeptly\ntransformed into corresponding speech patterns and emotive sounds using a\nrule-based approach during the text-to-speech phase. Based on our experiments,\nour novel system produces synthesized speech that's almost indistinguishable\nfrom genuine human communication, making each interaction feel more personal\nand authentic.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 1 figure, for associated code and media files, see\n  https://github.com/Rohan-Chaudhury/Humane-Speech-Synthesis-through-Zero-Shot-Emotion-and-Disfluency-Generation",
    "pdf_url": "http://arxiv.org/pdf/2404.01339v1",
    "published_date": "2024-03-31 00:38:02 UTC",
    "updated_date": "2024-03-31 00:38:02 UTC"
  },
  {
    "arxiv_id": "2404.00505v2",
    "title": "Transfer Learning with Reconstruction Loss",
    "authors": [
      "Wei Cui",
      "Wei Yu"
    ],
    "abstract": "In most applications of utilizing neural networks for mathematical\noptimization, a dedicated model is trained for each specific optimization\nobjective. However, in many scenarios, several distinct yet correlated\nobjectives or tasks often need to be optimized on the same set of problem\ninputs. Instead of independently training a different neural network for each\nproblem separately, it would be more efficient to exploit the correlations\nbetween these objectives and to train multiple neural network models with\nshared model parameters and feature representations. To achieve this, this\npaper first establishes the concept of common information: the shared knowledge\nrequired for solving the correlated tasks, then proposes a novel approach for\nmodel training by adding into the model an additional reconstruction stage\nassociated with a new reconstruction loss. This loss is for reconstructing the\ncommon information starting from a selected hidden layer in the model. The\nproposed approach encourages the learned features to be general and\ntransferable, and therefore can be readily used for efficient transfer\nlearning. For numerical simulations, three applications are studied: transfer\nlearning on classifying MNIST handwritten digits, the device-to-device wireless\nnetwork power allocation, and the multiple-input-single-output network downlink\nbeamforming and localization. Simulation results suggest that the proposed\napproach is highly efficient in data and model complexity, is resilient to\nover-fitting, and has competitive performances.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 5 figures. To appear in IEEE Transactions on Machine\n  Learning in Communications and Networking (TMLCN)",
    "pdf_url": "http://arxiv.org/pdf/2404.00505v2",
    "published_date": "2024-03-31 00:22:36 UTC",
    "updated_date": "2024-04-12 00:16:43 UTC"
  }
]
[
  {
    "arxiv_id": "2408.01914v3",
    "title": "Partial-differential-algebraic equations of nonlinear dynamics by Physics-Informed Neural-Network: (I) Operator splitting and framework assessment",
    "authors": [
      "Loc Vu-Quoc",
      "Alexander Humer"
    ],
    "abstract": "Several forms for constructing novel physics-informed neural-networks (PINN)\nfor the solution of partial-differential-algebraic equations based on\nderivative operator splitting are proposed, using the nonlinear Kirchhoff rod\nas a prototype for demonstration. The open-source DeepXDE is likely the most\nwell documented framework with many examples. Yet, we encountered some\npathological problems and proposed novel methods to resolve them. Among these\nnovel methods are the PDE forms, which evolve from the lower-level form with\nfewer unknown dependent variables to higher-level form with more dependent\nvariables, in addition to those from lower-level forms. Traditionally, the\nhighest-level form, the balance-of-momenta form, is the starting point for\n(hand) deriving the lowest-level form through a tedious (and error prone)\nprocess of successive substitutions. The next step in a finite element method\nis to discretize the lowest-level form upon forming a weak form and\nlinearization with appropriate interpolation functions, followed by their\nimplementation in a code and testing. The time-consuming tedium in all of these\nsteps could be bypassed by applying the proposed novel PINN directly to the\nhighest-level form. We developed a script based on JAX. While our JAX script\ndid not show the pathological problems of DDE-T (DDE with TensorFlow backend),\nit is slower than DDE-T. That DDE-T itself being more efficient in higher-level\nform than in lower-level form makes working directly with higher-level form\neven more attractive in addition to the advantages mentioned further above.\nSince coming up with an appropriate learning-rate schedule for a good solution\nis more art than science, we systematically codified in detail our experience\nrunning optimization through a normalization/standardization of the\nnetwork-training process so readers can reproduce our results.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA",
      "74-10 (primary), 74H15 (primary), 74K10 (primary), 74B20 (secondary)",
      "I.2.8; I.6.5"
    ],
    "primary_category": "math.NA",
    "comment": "61 pages, 52 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.01914v3",
    "published_date": "2024-07-13 22:48:17 UTC",
    "updated_date": "2024-10-17 22:25:49 UTC"
  },
  {
    "arxiv_id": "2407.10022v1",
    "title": "AtomAgents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence",
    "authors": [
      "Alireza Ghafarollahi",
      "Markus J. Buehler"
    ],
    "abstract": "The design of alloys is a multi-scale problem that requires a holistic\napproach that involves retrieving relevant knowledge, applying advanced\ncomputational methods, conducting experimental validations, and analyzing the\nresults, a process that is typically reserved for human experts. Machine\nlearning (ML) can help accelerate this process, for instance, through the use\nof deep surrogate models that connect structural features to material\nproperties, or vice versa. However, existing data-driven models often target\nspecific material objectives, offering limited flexibility to integrate\nout-of-domain knowledge and cannot adapt to new, unforeseen challenges. Here,\nwe overcome these limitations by leveraging the distinct capabilities of\nmultiple AI agents that collaborate autonomously within a dynamic environment\nto solve complex materials design tasks. The proposed physics-aware generative\nAI platform, AtomAgents, synergizes the intelligence of large language models\n(LLM) the dynamic collaboration among AI agents with expertise in various\ndomains, including knowledge retrieval, multi-modal data integration,\nphysics-based simulations, and comprehensive results analysis across modalities\nthat includes numerical data and images of physical simulation results. The\nconcerted effort of the multi-agent system allows for addressing complex\nmaterials design problems, as demonstrated by examples that include\nautonomously designing metallic alloys with enhanced properties compared to\ntheir pure counterparts. Our results enable accurate prediction of key\ncharacteristics across alloys and highlight the crucial role of solid solution\nalloying to steer the development of advanced metallic alloys. Our framework\nenhances the efficiency of complex multi-objective design tasks and opens new\navenues in fields such as biomedical materials engineering, renewable energy,\nand environmental sustainability.",
    "categories": [
      "cs.AI",
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "cond-mat.stat-mech",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10022v1",
    "published_date": "2024-07-13 22:46:02 UTC",
    "updated_date": "2024-07-13 22:46:02 UTC"
  },
  {
    "arxiv_id": "2407.10021v1",
    "title": "Document-level Clinical Entity and Relation Extraction via Knowledge Base-Guided Generation",
    "authors": [
      "Kriti Bhattarai",
      "Inez Y. Oh",
      "Zachary B. Abrams",
      "Albert M. Lai"
    ],
    "abstract": "Generative pre-trained transformer (GPT) models have shown promise in\nclinical entity and relation extraction tasks because of their precise\nextraction and contextual understanding capability. In this work, we further\nleverage the Unified Medical Language System (UMLS) knowledge base to\naccurately identify medical concepts and improve clinical entity and relation\nextraction at the document level. Our framework selects UMLS concepts relevant\nto the text and combines them with prompts to guide language models in\nextracting entities. Our experiments demonstrate that this initial concept\nmapping and the inclusion of these mapped concepts in the prompts improves\nextraction results compared to few-shot extraction tasks on generic language\nmodels that do not leverage UMLS. Further, our results show that this approach\nis more effective than the standard Retrieval Augmented Generation (RAG)\ntechnique, where retrieved data is compared with prompt embeddings to generate\nresults. Overall, we find that integrating UMLS concepts with GPT models\nsignificantly improves entity and relation identification, outperforming the\nbaseline and RAG models. By combining the precise concept mapping capability of\nknowledge-based approaches like UMLS with the contextual understanding\ncapability of GPT, our method highlights the potential of these approaches in\nspecialized domains like healthcare.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Association for Computational Linguistics BioNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.10021v1",
    "published_date": "2024-07-13 22:45:46 UTC",
    "updated_date": "2024-07-13 22:45:46 UTC"
  },
  {
    "arxiv_id": "2407.10020v1",
    "title": "Causality extraction from medical text using Large Language Models (LLMs)",
    "authors": [
      "Seethalakshmi Gopalakrishnan",
      "Luciana Garbayo",
      "Wlodek Zadrozny"
    ],
    "abstract": "This study explores the potential of natural language models, including large\nlanguage models, to extract causal relations from medical texts, specifically\nfrom Clinical Practice Guidelines (CPGs). The outcomes causality extraction\nfrom Clinical Practice Guidelines for gestational diabetes are presented,\nmarking a first in the field. We report on a set of experiments using variants\nof BERT (BioBERT, DistilBERT, and BERT) and using Large Language Models (LLMs),\nnamely GPT-4 and LLAMA2. Our experiments show that BioBERT performed better\nthan other models, including the Large Language Models, with an average\nF1-score of 0.72. GPT-4 and LLAMA2 results show similar performance but less\nconsistency. We also release the code and an annotated a corpus of causal\nstatements within the Clinical Practice Guidelines for gestational diabetes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10020v1",
    "published_date": "2024-07-13 22:33:29 UTC",
    "updated_date": "2024-07-13 22:33:29 UTC"
  },
  {
    "arxiv_id": "2407.10016v1",
    "title": "Characterizing Disparity Between Edge Models and High-Accuracy Base Models for Vision Tasks",
    "authors": [
      "Zhenyu Wang",
      "Shahriar Nirjon"
    ],
    "abstract": "Edge devices, with their widely varying capabilities, support a diverse range\nof edge AI models. This raises the question: how does an edge model differ from\na high-accuracy (base) model for the same task? We introduce XDELTA, a novel\nexplainable AI tool that explains differences between a high-accuracy base\nmodel and a computationally efficient but lower-accuracy edge model. To achieve\nthis, we propose a learning-based approach to characterize the model\ndifference, named the DELTA network, which complements the feature\nrepresentation capability of the edge network in a compact form. To construct\nDELTA, we propose a sparsity optimization framework that extracts the essence\nof the base model to ensure compactness and sufficient feature representation\ncapability of DELTA, and implement a negative correlation learning approach to\nensure it complements the edge model. We conduct a comprehensive evaluation to\ntest XDELTA's ability to explain model discrepancies, using over 1.2 million\nimages and 24 models, and assessing real-world deployments with six\nparticipants. XDELTA excels in explaining differences between base and edge\nmodels (arbitrary pairs as well as compressed base models) through geometric\nand concept-level analysis, proving effective in real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10016v1",
    "published_date": "2024-07-13 22:05:58 UTC",
    "updated_date": "2024-07-13 22:05:58 UTC"
  },
  {
    "arxiv_id": "2407.10005v1",
    "title": "Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond",
    "authors": [
      "Yingcong Li",
      "Ankit Singh Rawat",
      "Samet Oymak"
    ],
    "abstract": "Recent research has shown that Transformers with linear attention are capable\nof in-context learning (ICL) by implementing a linear estimator through\ngradient descent steps. However, the existing results on the optimization\nlandscape apply under stylized settings where task and feature vectors are\nassumed to be IID and the attention weights are fully parameterized. In this\nwork, we develop a stronger characterization of the optimization and\ngeneralization landscape of ICL through contributions on architectures,\nlow-rank parameterization, and correlated designs: (1) We study the landscape\nof 1-layer linear attention and 1-layer H3, a state-space model. Under a\nsuitable correlated design assumption, we prove that both implement 1-step\npreconditioned gradient descent. We show that thanks to its native convolution\nfilters, H3 also has the advantage of implementing sample weighting and\noutperforming linear attention in suitable settings. (2) By studying correlated\ndesigns, we provide new risk bounds for retrieval augmented generation (RAG)\nand task-feature alignment which reveal how ICL sample complexity benefits from\ndistributional alignment. (3) We derive the optimal risk for low-rank\nparameterized attention weights in terms of covariance spectrum. Through this,\nwe also shed light on how LoRA can adapt to a new distribution by capturing the\nshift between task covariances. Experimental results corroborate our\ntheoretical findings. Overall, this work explores the optimization and risk\nlandscape of ICL in practically meaningful settings and contributes to a more\nthorough understanding of its mechanics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.10005v1",
    "published_date": "2024-07-13 21:13:55 UTC",
    "updated_date": "2024-07-13 21:13:55 UTC"
  },
  {
    "arxiv_id": "2407.12869v2",
    "title": "Bilingual Adaptation of Monolingual Foundation Models",
    "authors": [
      "Gurpreet Gosal",
      "Yishi Xu",
      "Gokul Ramakrishnan",
      "Rituraj Joshi",
      "Avraham Sheinin",
      "Zhiming",
      "Chen",
      "Biswajit Mishra",
      "Natalia Vassilieva",
      "Joel Hestness",
      "Neha Sengupta",
      "Sunil Kumar Sahu",
      "Bokang Jia",
      "Onkar Pandit",
      "Satheesh Katipomu",
      "Samta Kamboj",
      "Samujjwal Ghosh",
      "Rahul Pal",
      "Parvez Mullah",
      "Soundar Doraiswamy",
      "Mohamed El Karim Chami",
      "Preslav Nakov"
    ],
    "abstract": "We present an efficient method for adapting a monolingual Large Language\nModel (LLM) to another language, addressing challenges of catastrophic\nforgetting and tokenizer limitations. We focus this study on adapting Llama 2\nto Arabic. Our two-stage approach begins with expanding the vocabulary and\ntraining only the embeddings matrix, followed by full model continual\npre-training on a bilingual corpus. By continually pre-training on a mix of\nArabic and English corpora, the model retains its proficiency in English while\nacquiring capabilities in Arabic. Our approach results in significant\nimprovements in Arabic and slight enhancements in English, demonstrating\ncost-effective cross-lingual transfer. We perform ablations on embedding\ninitialization techniques, data mix ratios, and learning rates and release a\ndetailed training recipe. To demonstrate generalizability of this approach we\nalso adapted Llama 3 8B to Arabic and Llama 2 13B to Hindi.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12869v2",
    "published_date": "2024-07-13 21:09:38 UTC",
    "updated_date": "2024-07-25 22:51:39 UTC"
  },
  {
    "arxiv_id": "2407.09999v1",
    "title": "Pay Less On Clinical Images: Asymmetric Multi-Modal Fusion Method For Efficient Multi-Label Skin Lesion Classification",
    "authors": [
      "Peng Tang",
      "Tobias Lasser"
    ],
    "abstract": "Existing multi-modal approaches primarily focus on enhancing multi-label skin\nlesion classification performance through advanced fusion modules, often\nneglecting the associated rise in parameters. In clinical settings, both\nclinical and dermoscopy images are captured for diagnosis; however, dermoscopy\nimages exhibit more crucial visual features for multi-label skin lesion\nclassification. Motivated by this observation, we introduce a novel asymmetric\nmulti-modal fusion method in this paper for efficient multi-label skin lesion\nclassification. Our fusion method incorporates two innovative schemes. Firstly,\nwe validate the effectiveness of our asymmetric fusion structure. It employs a\nlight and simple network for clinical images and a heavier, more complex one\nfor dermoscopy images, resulting in significant parameter savings compared to\nthe symmetric fusion structure using two identical networks for both\nmodalities. Secondly, in contrast to previous approaches using mutual attention\nmodules for interaction between image modalities, we propose an asymmetric\nattention module. This module solely leverages clinical image information to\nenhance dermoscopy image features, considering clinical images as supplementary\ninformation in our pipeline. We conduct the extensive experiments on the\nseven-point checklist dataset. Results demonstrate the generality of our\nproposed method for both networks and Transformer structures, showcasing its\nsuperiority over existing methods We will make our code publicly available.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09999v1",
    "published_date": "2024-07-13 20:46:04 UTC",
    "updated_date": "2024-07-13 20:46:04 UTC"
  },
  {
    "arxiv_id": "2407.09986v1",
    "title": "Curriculum Is More Influential Than Haptic Information During Reinforcement Learning of Object Manipulation Against Gravity",
    "authors": [
      "Pegah Ojaghi",
      "Romina Mir",
      "Ali Marjaninejad",
      "Andrew Erwin",
      "Michael Wehner",
      "Francisco J Valero-Cueva"
    ],
    "abstract": "Learning to lift and rotate objects with the fingertips is necessary for\nautonomous in-hand dexterous manipulation. In our study, we explore the impact\nof various factors on successful learning strategies for this task.\nSpecifically, we investigate the role of curriculum learning and haptic\nfeedback in enabling the learning of dexterous manipulation. Using model-free\nReinforcement Learning, we compare different curricula and two haptic\ninformation modalities (No-tactile vs. 3D-force sensing) for lifting and\nrotating a ball against gravity with a three-fingered simulated robotic hand\nwith no visual input. Note that our best results were obtained when we used a\nnovel curriculum-based learning rate scheduler, which adjusts the\nlinearly-decaying learning rate when the reward is changed as it accelerates\nconvergence to higher rewards. Our findings demonstrate that the choice of\ncurriculum greatly biases the acquisition of different features of dexterous\nmanipulation. Surprisingly, successful learning can be achieved even in the\nabsence of tactile feedback, challenging conventional assumptions about the\nnecessity of haptic information for dexterous manipulation tasks. We\ndemonstrate the generalizability of our results to balls of different weights\nand sizes, underscoring the robustness of our learning approach. This work,\ntherefore, emphasizes the importance of the choice curriculum and challenges\nlong-held notions about the need for tactile information to autonomously learn\nin-hand dexterous manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09986v1",
    "published_date": "2024-07-13 19:23:11 UTC",
    "updated_date": "2024-07-13 19:23:11 UTC"
  },
  {
    "arxiv_id": "2407.09985v2",
    "title": "A Training Data Recipe to Accelerate A* Search with Language Models",
    "authors": [
      "Devaansh Gupta",
      "Boyang Li"
    ],
    "abstract": "Combining Large Language Models (LLMs) with heuristic search algorithms like\nA* holds the promise of enhanced LLM reasoning and scalable inference. To\naccelerate training and reduce computational demands, we investigate the\ncoreset selection problem for the training data of LLM heuristic learning. Few\nmethods to learn the heuristic functions consider the interaction between the\nsearch algorithm and the machine learning model. In this work, we empirically\ndisentangle the requirements of A* search algorithm from the requirements of\nthe LLM to generalise on this task. Surprisingly, we find an overlap between\ntheir requirements; A* requires more accurate predictions on search nodes near\nthe goal, and LLMs need the same set of nodes for effective generalisation.\nWith these insights, we derive a data-selection distribution for learning\nLLM-based heuristics. On three classical planning domains, maze navigation,\nSokoban and sliding tile puzzles, our technique reduces the number of\niterations required to find the solutions by up to 15x, with a wall-clock\nspeed-up of search up to 5x. The codebase is at\nhttps://github.com/devaansh100/a_star.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to Findings of EMNLP, 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.09985v2",
    "published_date": "2024-07-13 19:21:44 UTC",
    "updated_date": "2024-10-23 22:37:31 UTC"
  },
  {
    "arxiv_id": "2407.09965v1",
    "title": "Learning Online Scale Transformation for Talking Head Video Generation",
    "authors": [
      "Fa-Ting Hong",
      "Dan Xu"
    ],
    "abstract": "One-shot talking head video generation uses a source image and driving video\nto create a synthetic video where the source person's facial movements imitate\nthose of the driving video. However, differences in scale between the source\nand driving images remain a challenge for face reenactment. Existing methods\nattempt to locate a frame in the driving video that aligns best with the source\nimage, but imprecise alignment can result in suboptimal outcomes.\n  To this end, we introduce a scale transformation module that can\nautomatically adjust the scale of the driving image to fit that of the source\nimage, by using the information of scale difference maintained in the detected\nkeypoints of the source image and the driving frame. Furthermore, to keep\nperceiving the scale information of faces during the generation process, we\nincorporate the scale information learned from the scale transformation module\ninto each layer of the generation process to produce a final result with an\naccurate scale. Our method can perform accurate motion transfer between the two\nimages without any anchor frame, achieved through the contributions of the\nproposed online scale transformation facial reenactment network. Extensive\nexperiments have demonstrated that our proposed method adjusts the scale of the\ndriving face automatically according to the source face, and generates\nhigh-quality faces with an accurate scale in the cross-identity facial\nreenactment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09965v1",
    "published_date": "2024-07-13 18:08:46 UTC",
    "updated_date": "2024-07-13 18:08:46 UTC"
  },
  {
    "arxiv_id": "2407.09941v1",
    "title": "Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers",
    "authors": [
      "Sukjun Hwang",
      "Aakash Lahoti",
      "Tri Dao",
      "Albert Gu"
    ],
    "abstract": "A wide array of sequence models are built on a framework modeled after\nTransformers, comprising alternating sequence mixer and channel mixer layers.\nThis paper studies a unifying matrix mixer view of sequence mixers that can be\nconceptualized as a linear map on the input sequence. This framework\nencompasses a broad range of well-known sequence models, including the\nself-attention of Transformers as well as recent strong alternatives such as\nstructured state space models (SSMs), and allows understanding downstream\ncharacteristics such as efficiency and expressivity through properties of their\nstructured matrix class. We identify a key axis of matrix parameterizations\ntermed sequence alignment, which increases the flexibility and performance of\nmatrix mixers, providing insights into the strong performance of Transformers\nand recent SSMs such as Mamba. Furthermore, the matrix mixer framework offers a\nsystematic approach to developing sequence mixers with desired properties,\nallowing us to develop several new sub-quadratic sequence models. In\nparticular, we propose a natural bidirectional extension of the Mamba model\n(Hydra), parameterized as a quasiseparable matrix mixer, which demonstrates\nsuperior performance over other sequence models including Transformers on\nnon-causal tasks. As a drop-in replacement for attention layers, Hydra\noutperforms BERT by 0.8 points on the GLUE benchmark and ViT by 2% Top-1\naccuracy on ImageNet.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09941v1",
    "published_date": "2024-07-13 16:34:18 UTC",
    "updated_date": "2024-07-13 16:34:18 UTC"
  },
  {
    "arxiv_id": "2407.09936v1",
    "title": "WojoodNER 2024: The Second Arabic Named Entity Recognition Shared Task",
    "authors": [
      "Mustafa Jarrar",
      "Nagham Hamad",
      "Mohammed Khalilia",
      "Bashar Talafha",
      "AbdelRahim Elmadany",
      "Muhammad Abdul-Mageed"
    ],
    "abstract": "We present WojoodNER-2024, the second Arabic Named Entity Recognition (NER)\nShared Task. In WojoodNER-2024, we focus on fine-grained Arabic NER. We\nprovided participants with a new Arabic fine-grained NER dataset called\nwojoodfine, annotated with subtypes of entities. WojoodNER-2024 encompassed\nthree subtasks: (i) Closed-Track Flat Fine-Grained NER, (ii) Closed-Track\nNested Fine-Grained NER, and (iii) an Open-Track NER for the Israeli War on\nGaza. A total of 43 unique teams registered for this shared task. Five teams\nparticipated in the Flat Fine-Grained Subtask, among which two teams tackled\nthe Nested Fine-Grained Subtask and one team participated in the Open-Track NER\nSubtask. The winning teams achieved F-1 scores of 91% and 92% in the Flat\nFine-Grained and Nested Fine-Grained Subtasks, respectively. The sole team in\nthe Open-Track Subtask achieved an F-1 score of 73.7%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09936v1",
    "published_date": "2024-07-13 16:17:08 UTC",
    "updated_date": "2024-07-13 16:17:08 UTC"
  },
  {
    "arxiv_id": "2407.09930v2",
    "title": "Evaluating the Impact of Different Quantum Kernels on the Classification Performance of Support Vector Machine Algorithm: A Medical Dataset Application",
    "authors": [
      "Emine Akpinar",
      "Sardar M. N. Islam",
      "Murat Oduncuoglu"
    ],
    "abstract": "The support vector machine algorithm with a quantum kernel estimator\n(QSVM-Kernel), as a leading example of a quantum machine learning technique,\nhas undergone significant advancements. Nevertheless, its integration with\nclassical data presents unique challenges. While quantum computers primarily\ninteract with data in quantum states, embedding classical data into quantum\nstates using feature mapping techniques is essential for leveraging quantum\nalgorithms Despite the recognized importance of feature mapping, its specific\nimpact on data classification outcomes remains largely unexplored. This study\naddresses this gap by comprehensively assessing the effects of various feature\nmapping methods on classification results, taking medical data analysis as a\ncase study. In this study, the QSVM-Kernel method was applied to classification\nproblems in two different and publicly available medical datasets, namely, the\nWisconsin Breast Cancer (original) and The Cancer Genome Atlas (TCGA) Glioma\ndatasets. In the QSVM-Kernel algorithm, quantum kernel matrices obtained from 9\ndifferent quantum feature maps were used. Thus, the effects of these quantum\nfeature maps on the classification results of the QSVM-Kernel algorithm were\nexamined in terms of both classifier performance and total execution time. As a\nresult, in the Wisconsin Breast Cancer (original) and TCGA Glioma datasets,\nwhen Rx and Ry rotational gates were used, respectively, as feature maps in the\nQSVM-Kernel algorithm, the best classification performances were achieved both\nin terms of classification performance and total execution time. The\ncontributions of this study are that (1) it highlights the significant impact\nof feature mapping techniques on medical data classification outcomes using the\nQSVM-Kernel algorithm, and (2) it also guides undertaking research for improved\nQSVM classification performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2407.09930v2",
    "published_date": "2024-07-13 15:53:37 UTC",
    "updated_date": "2024-07-19 10:37:03 UTC"
  },
  {
    "arxiv_id": "2407.09926v1",
    "title": "Metric Learning for Clifford Group Equivariant Neural Networks",
    "authors": [
      "Riccardo Ali",
      "Paulina Kulytė",
      "Haitz Sáez de Ocáriz Borde",
      "Pietro Liò"
    ],
    "abstract": "Clifford Group Equivariant Neural Networks (CGENNs) leverage Clifford\nalgebras and multivectors as an alternative approach to incorporating group\nequivariance to ensure symmetry constraints in neural representations. In\nprinciple, this formulation generalizes to orthogonal groups and preserves\nequivariance regardless of the metric signature. However, previous works have\nrestricted internal network representations to Euclidean or Minkowski\n(pseudo-)metrics, handpicked depending on the problem at hand. In this work, we\npropose an alternative method that enables the metric to be learned in a\ndata-driven fashion, allowing the CGENN network to learn more flexible\nrepresentations. Specifically, we populate metric matrices fully, ensuring they\nare symmetric by construction, and leverage eigenvalue decomposition to\nintegrate this additional learnable component into the original CGENN\nformulation in a principled manner. Additionally, we motivate our method using\ninsights from category theory, which enables us to explain Clifford algebras as\na categorical construction and guarantee the mathematical soundness of our\napproach. We validate our method in various tasks and showcase the advantages\nof learning more flexible latent metric representations. The code and data are\navailable at https://github.com/rick-ali/Metric-Learning-for-CGENNs",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Workshop on Geometry-grounded Representation Learning and Generative\n  Modeling (GRaM) at the ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.09926v1",
    "published_date": "2024-07-13 15:41:14 UTC",
    "updated_date": "2024-07-13 15:41:14 UTC"
  },
  {
    "arxiv_id": "2407.21024v2",
    "title": "An Autonomous GIS Agent Framework for Geospatial Data Retrieval",
    "authors": [
      "Huan Ning",
      "Zhenlong Li",
      "Temitope Akinboyewa",
      "M. Naser Lessani"
    ],
    "abstract": "Powered by the emerging large language models (LLMs), autonomous geographic\ninformation systems (GIS) agents have the potential to accomplish spatial\nanalyses and cartographic tasks. However, a research gap exists to support\nfully autonomous GIS agents: how to enable agents to discover and download the\nnecessary data for geospatial analyses. This study proposes an autonomous GIS\nagent framework capable of retrieving required geospatial data by generating,\nexecuting, and debugging programs. The framework utilizes the LLM as the\ndecision-maker, selects the appropriate data source (s) from a pre-defined\nsource list, and fetches the data from the chosen source. Each data source has\na handbook that records the metadata and technical details for data retrieval.\nThe proposed framework is designed in a plug-and-play style to ensure\nflexibility and extensibility. Human users or autonomous data scrawlers can add\nnew data sources by adding new handbooks. We developed a prototype agent based\non the framework, released as a QGIS plugin (GeoData Retrieve Agent) and a\nPython program. Experiment results demonstrate its capability of retrieving\ndata from various sources including OpenStreetMap, administrative boundaries\nand demographic data from the US Census Bureau, satellite basemaps from ESRI\nWorld Imagery, global digital elevation model (DEM) from OpenTopography.org,\nweather data from a commercial provider, the COVID-19 cases from the NYTimes\nGitHub. Our study is among the first attempts to develop an autonomous\ngeospatial data retrieval agent.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.ET"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.21024v2",
    "published_date": "2024-07-13 14:23:57 UTC",
    "updated_date": "2024-08-08 15:32:43 UTC"
  },
  {
    "arxiv_id": "2407.09894v1",
    "title": "Transferring Structure Knowledge: A New Task to Fake news Detection Towards Cold-Start Propagation",
    "authors": [
      "Lingwei Wei",
      "Dou Hu",
      "Wei Zhou",
      "Songlin Hu"
    ],
    "abstract": "Many fake news detection studies have achieved promising performance by\nextracting effective semantic and structure features from both content and\npropagation trees. However, it is challenging to apply them to practical\nsituations, especially when using the trained propagation-based models to\ndetect news with no propagation data. Towards this scenario, we study a new\ntask named cold-start fake news detection, which aims to detect content-only\nsamples with missing propagation. To achieve the task, we design a simple but\neffective Structure Adversarial Net (SAN) framework to learn transferable\nfeatures from available propagation to boost the detection of content-only\nsamples. SAN introduces a structure discriminator to estimate dissimilarities\namong learned features with and without propagation, and further learns\nstructure-invariant features to enhance the generalization of existing\npropagation-based methods for content-only samples. We conduct qualitative and\nquantitative experiments on three datasets. Results show the challenge of the\nnew task and the effectiveness of our SAN framework.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SI",
    "comment": "ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.09894v1",
    "published_date": "2024-07-13 14:04:55 UTC",
    "updated_date": "2024-07-13 14:04:55 UTC"
  },
  {
    "arxiv_id": "2407.11082v1",
    "title": "Imbalanced Graph-Level Anomaly Detection via Counterfactual Augmentation and Feature Learning",
    "authors": [
      "Zitong Wang",
      "Xuexiong Luo",
      "Enfeng Song",
      "Qiuqing Bai",
      "Fu Lin"
    ],
    "abstract": "Graph-level anomaly detection (GLAD) has already gained significant\nimportance and has become a popular field of study, attracting considerable\nattention across numerous downstream works. The core focus of this domain is to\ncapture and highlight the anomalous information within given graph datasets. In\nmost existing studies, anomalies are often the instances of few. The stark\nimbalance misleads current GLAD methods to focus on learning the patterns of\nnormal graphs more, further impacting anomaly detection performance. Moreover,\nexisting methods predominantly utilize the inherent features of nodes to\nidentify anomalous graph patterns which is approved suboptimal according to our\nexperiments. In this work, we propose an imbalanced GLAD method via\ncounterfactual augmentation and feature learning. Specifically, we first\nconstruct anomalous samples based on counterfactual learning, aiming to expand\nand balance the datasets. Additionally, we construct a module based on Graph\nNeural Networks (GNNs), which allows us to utilize degree attributes to\ncomplement the inherent attribute features of nodes. Then, we design an\nadaptive weight learning module to integrate features tailored to different\ndatasets effectively to avoid indiscriminately treating all features as\nequivalent. Furthermore, extensive baseline experiments conducted on public\ndatasets substantiate the robustness and effectiveness. Besides, we apply the\nmodel to brain disease datasets, which can prove the generalization capability\nof our work. The source code of our work is available online.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 4 figures, SSDBM2024",
    "pdf_url": "http://arxiv.org/pdf/2407.11082v1",
    "published_date": "2024-07-13 13:40:06 UTC",
    "updated_date": "2024-07-13 13:40:06 UTC"
  },
  {
    "arxiv_id": "2407.09888v1",
    "title": "FarFetched: Entity-centric Reasoning and Claim Validation for the Greek Language based on Textually Represented Environments",
    "authors": [
      "Dimitris Papadopoulos",
      "Katerina Metropoulou",
      "Nikolaos Matsatsinis",
      "Nikolaos Papadakis"
    ],
    "abstract": "Our collective attention span is shortened by the flood of online\ninformation. With \\textit{FarFetched}, we address the need for automated claim\nvalidation based on the aggregated evidence derived from multiple online news\nsources. We introduce an entity-centric reasoning framework in which latent\nconnections between events, actions, or statements are revealed via entity\nmentions and represented in a graph database. Using entity linking and semantic\nsimilarity, we offer a way for collecting and combining information from\ndiverse sources in order to generate evidence relevant to the user's claim.\nThen, we leverage textual entailment recognition to quantitatively determine\nwhether this assertion is credible, based on the created evidence. Our approach\ntries to fill the gap in automated claim validation for less-resourced\nlanguages and is showcased on the Greek language, complemented by the training\nof relevant semantic textual similarity (STS) and natural language inference\n(NLI) models that are evaluated on translated versions of common benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "DeepLo NAACL 2022",
    "pdf_url": "http://arxiv.org/pdf/2407.09888v1",
    "published_date": "2024-07-13 13:30:20 UTC",
    "updated_date": "2024-07-13 13:30:20 UTC"
  },
  {
    "arxiv_id": "2407.09874v1",
    "title": "SeFi-CD: A Semantic First Change Detection Paradigm That Can Detect Any Change You Want",
    "authors": [
      "Ling Zhao",
      "Zhenyang Huang",
      "Dongsheng Kuang",
      "Chengli Peng",
      "Jun Gan",
      "Haifeng Li"
    ],
    "abstract": "The existing change detection(CD) methods can be summarized as the\nvisual-first change detection (ViFi-CD) paradigm, which first extracts change\nfeatures from visual differences and then assigns them specific semantic\ninformation. However, CD is essentially dependent on change regions of interest\n(CRoIs), meaning that the CD results are directly determined by the semantics\nchanges of interest, making its primary image factor semantic of interest\nrather than visual. The ViFi-CD paradigm can only assign specific semantics of\ninterest to specific change features extracted from visual differences, leading\nto the inevitable omission of potential CRoIs and the inability to adapt to\ndifferent CRoI CD tasks. In other words, changes in other CRoIs cannot be\ndetected by the ViFi-CD method without retraining the model or significantly\nmodifying the method. This paper introduces a new CD paradigm, the\nsemantic-first CD (SeFi-CD) paradigm. The core idea of SeFi-CD is to first\nperceive the dynamic semantics of interest and then visually search for change\nfeatures related to the semantics. Based on the SeFi-CD paradigm, we designed\nAnything You Want Change Detection (AUWCD). Experiments on public datasets\ndemonstrate that the AUWCD outperforms the current state-of-the-art CD methods,\nachieving an average F1 score 5.01\\% higher than that of these advanced\nsupervised baselines on the SECOND dataset, with a maximum increase of 13.17\\%.\nThe proposed SeFi-CD offers a novel CD perspective and approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09874v1",
    "published_date": "2024-07-13 12:49:58 UTC",
    "updated_date": "2024-07-13 12:49:58 UTC"
  },
  {
    "arxiv_id": "2407.09873v1",
    "title": "Resource Management for Low-latency Cooperative Fine-tuning of Foundation Models at the Network Edge",
    "authors": [
      "Hai Wu",
      "Xu Chen",
      "Kaibin Huang"
    ],
    "abstract": "The emergence of large-scale foundation models (FoMo's) that can perform\nhuman-like intelligence motivates their deployment at the network edge for\ndevices to access state-of-the-art artificial intelligence. For better user\nexperiences, the pre-trained FoMo's need to be adapted to specialized\ndownstream tasks through fine-tuning techniques. To transcend a single device's\nmemory and computation limitations, we advocate multi-device cooperation within\nthe device-edge cooperative fine-tuning (DEFT) paradigm, where edge devices\ncooperate to simultaneously optimize different parts of fine-tuning parameters\nwithin a FoMo. However, the parameter blocks reside at different depths within\na FoMo architecture, leading to varied computation latency-and-memory cost due\nto gradient backpropagation-based calculations. The heterogeneous on-device\ncomputation and memory capacities and channel conditions necessitate an\nintegrated communication-and-computation allocation of local computation loads\nand communication resources to achieve low-latency (LoLa) DEFT. To this end, we\nconsider the depth-ware DEFT block allocation problem. The involved optimal\nblock-device matching is tackled by the proposed low-complexity\nCutting-RecoUNting-CHecking (CRUNCH) algorithm, which is designed by exploiting\nthe monotone-increasing property between block depth and computation\nlatency-and-memory cost. Next, the joint bandwidth-and-block allocation makes\nthe problem more sophisticated. We observe a splittable Lagrangian expression\nthrough the transformation and analysis of the original problem, where the\nvariables indicating device involvement are introduced. Then, the dual ascent\nmethod is employed to tackle this problem iteratively. Through extensive\nexperiments conducted on the GLUE benchmark, our results demonstrate\nsignificant latency reduction achievable by LoLa DEFT for fine-tuning a RoBERTa\nmodel.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2407.09873v1",
    "published_date": "2024-07-13 12:47:14 UTC",
    "updated_date": "2024-07-13 12:47:14 UTC"
  },
  {
    "arxiv_id": "2407.11081v1",
    "title": "Generating In-store Customer Journeys from Scratch with GPT Architectures",
    "authors": [
      "Taizo Horikomi",
      "Takayuki Mizuno"
    ],
    "abstract": "We propose a method that can generate customer trajectories and purchasing\nbehaviors in retail stores simultaneously using Transformer-based deep learning\nstructure. Utilizing customer trajectory data, layout diagrams, and retail\nscanner data obtained from a retail store, we trained a GPT-2 architecture from\nscratch to generate indoor trajectories and purchase actions. Additionally, we\nexplored the effectiveness of fine-tuning the pre-trained model with data from\nanother store. Results demonstrate that our method reproduces in-store\ntrajectories and purchase behaviors more accurately than LSTM and SVM models,\nwith fine-tuning significantly reducing the required training data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11081v1",
    "published_date": "2024-07-13 12:35:52 UTC",
    "updated_date": "2024-07-13 12:35:52 UTC"
  },
  {
    "arxiv_id": "2407.09861v3",
    "title": "Towards Systematic Monolingual NLP Surveys: GenA of Greek NLP",
    "authors": [
      "Juli Bakagianni",
      "Kanella Pouli",
      "Maria Gavriilidou",
      "John Pavlopoulos"
    ],
    "abstract": "Natural Language Processing (NLP) research has traditionally been\npredominantly focused on English, driven by the availability of resources, the\nsize of the research community, and market demands. Recently, there has been a\nnoticeable shift towards multilingualism in NLP, recognizing the need for\ninclusivity and effectiveness across diverse languages and cultures.\nMonolingual surveys have the potential to complement the broader trend towards\nmultilingualism in NLP by providing foundational insights and resources,\nnecessary for effectively addressing the linguistic diversity of global\ncommunication. However, monolingual NLP surveys are extremely rare in the\nliterature. This study introduces a generalizable methodology for creating\nsystematic and comprehensive monolingual NLP surveys, aimed at optimizing the\nprocess of constructing such surveys and thoroughly addressing a language's NLP\nsupport. Our approach integrates a structured search protocol to avoid\nselection bias and ensure reproducibility, an NLP task taxonomy to organize the\nsurveyed material coherently, and language resources (LRs) taxonomies to\nidentify potential benchmarks and highlight opportunities for improving\nresource availability (e.g., through better maintenance or licensing). We apply\nthis methodology to Greek NLP (2012-2023), providing a comprehensive overview\nof its current state and challenges. We discuss the progress of Greek NLP and\noutline the Greek LRs found, classified by availability and usability,\nassessing language support per NLP task. The presented systematic literature\nreview of Greek NLP serves as an application of our method that showcases the\nbenefits of monolingual NLP surveys more broadly. Similar applications could be\nconsidered for the myriads of languages whose progress in NLP lags behind that\nof well-supported languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "77 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.09861v3",
    "published_date": "2024-07-13 12:01:52 UTC",
    "updated_date": "2025-01-31 16:28:15 UTC"
  },
  {
    "arxiv_id": "2407.09855v1",
    "title": "Building pre-train LLM Dataset for the INDIC Languages: a case study on Hindi",
    "authors": [
      "Shantipriya Parida",
      "Shakshi Panwar",
      "Kusum Lata",
      "Sanskruti Mishra",
      "Sambit Sekhar"
    ],
    "abstract": "Large language models (LLMs) demonstrated transformative capabilities in many\napplications that require automatically generating responses based on human\ninstruction. However, the major challenge for building LLMs, particularly in\nIndic languages, is the availability of high-quality data for building\nfoundation LLMs. In this paper, we are proposing a large pre-train dataset in\nHindi useful for the Indic language Hindi. We have collected the data span\nacross several domains including major dialects in Hindi. The dataset contains\n1.28 billion Hindi tokens. We have explained our pipeline including data\ncollection, pre-processing, and availability for LLM pre-training. The proposed\napproach can be easily extended to other Indic and low-resource languages and\nwill be available freely for LLM pre-training and LLM research purposes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as a book chapter in the book Title \"APPLIED SPEECH AND TEXT\n  PROCESSING FOR LOW RESOURCE LANGUAGES\"",
    "pdf_url": "http://arxiv.org/pdf/2407.09855v1",
    "published_date": "2024-07-13 11:29:20 UTC",
    "updated_date": "2024-07-13 11:29:20 UTC"
  },
  {
    "arxiv_id": "2407.09828v1",
    "title": "Enhancing Semantic Segmentation with Adaptive Focal Loss: A Novel Approach",
    "authors": [
      "Md Rakibul Islam",
      "Riad Hassan",
      "Abdullah Nazib",
      "Kien Nguyen",
      "Clinton Fookes",
      "Md Zahidul Islam"
    ],
    "abstract": "Deep learning has achieved outstanding accuracy in medical image\nsegmentation, particularly for objects like organs or tumors with smooth\nboundaries or large sizes. Whereas, it encounters significant difficulties with\nobjects that have zigzag boundaries or are small in size, leading to a notable\ndecrease in segmentation effectiveness. In this context, using a loss function\nthat incorporates smoothness and volume information into a model's predictions\noffers a promising solution to these shortcomings. In this work, we introduce\nan Adaptive Focal Loss (A-FL) function designed to mitigate class imbalance by\ndown-weighting the loss for easy examples that results in up-weighting the loss\nfor hard examples and giving greater emphasis to challenging examples, such as\nsmall and irregularly shaped objects. The proposed A-FL involves dynamically\nadjusting a focusing parameter based on an object's surface smoothness, size\ninformation, and adjusting the class balancing parameter based on the ratio of\ntargeted area to total area in an image. We evaluated the performance of the\nA-FL using ResNet50-encoded U-Net architecture on the Picai 2022 and BraTS 2018\ndatasets. On the Picai 2022 dataset, the A-FL achieved an Intersection over\nUnion (IoU) of 0.696 and a Dice Similarity Coefficient (DSC) of 0.769,\noutperforming the regular Focal Loss (FL) by 5.5% and 5.4% respectively. It\nalso surpassed the best baseline Dice-Focal by 2.0% and 1.2%. On the BraTS 2018\ndataset, A-FL achieved an IoU of 0.883 and a DSC of 0.931. The comparative\nstudies show that the proposed A-FL function surpasses conventional methods,\nincluding Dice Loss, Focal Loss, and their hybrid variants, in IoU, DSC,\nSensitivity, and Specificity metrics. This work highlights A-FL's potential to\nimprove deep learning models for segmenting clinically significant regions in\nmedical images, leading to more precise and reliable diagnostic tools.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "15 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09828v1",
    "published_date": "2024-07-13 09:41:20 UTC",
    "updated_date": "2024-07-13 09:41:20 UTC"
  },
  {
    "arxiv_id": "2407.09823v2",
    "title": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs",
    "authors": [
      "Md. Arid Hasan",
      "Maram Hasanain",
      "Fatema Ahmad",
      "Sahinur Rahman Laskar",
      "Sunaya Upadhyay",
      "Vrunda N Sukhadia",
      "Mucahid Kutlu",
      "Shammur Absar Chowdhury",
      "Firoj Alam"
    ],
    "abstract": "Natural Question Answering (QA) datasets play a crucial role in evaluating\nthe capabilities of large language models (LLMs), ensuring their effectiveness\nin real-world applications. Despite the numerous QA datasets that have been\ndeveloped, there is a notable lack of region-specific datasets generated by\nnative users in their own languages. This gap hinders the effective\nbenchmarking of LLMs for regional and cultural specificities. Furthermore, it\nalso limits the development of fine-tuned models. In this study, we propose a\nscalable, language-independent framework, NativQA, to seamlessly construct\nculturally and regionally aligned QA datasets in native languages, for LLM\nevaluation and tuning. We demonstrate the efficacy of the proposed framework by\ndesigning a multilingual natural QA dataset, \\mnqa, consisting of ~64k manually\nannotated QA pairs in seven languages, ranging from high to extremely low\nresource, based on queries from native speakers from 9 regions covering 18\ntopics. We benchmark open- and closed-source LLMs with the MultiNativQA\ndataset. We also showcase the framework efficacy in constructing fine-tuning\ndata especially for low-resource and dialectally-rich languages. We made both\nthe framework NativQA and MultiNativQA dataset publicly available for the\ncommunity (https://nativqa.gitlab.io).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models",
    "pdf_url": "http://arxiv.org/pdf/2407.09823v2",
    "published_date": "2024-07-13 09:34:00 UTC",
    "updated_date": "2024-10-06 10:46:41 UTC"
  },
  {
    "arxiv_id": "2407.09811v1",
    "title": "CellAgent: An LLM-driven Multi-Agent Framework for Automated Single-cell Data Analysis",
    "authors": [
      "Yihang Xiao",
      "Jinyi Liu",
      "Yan Zheng",
      "Xiaohan Xie",
      "Jianye Hao",
      "Mingzhi Li",
      "Ruitao Wang",
      "Fei Ni",
      "Yuxiao Li",
      "Jintian Luo",
      "Shaoqing Jiao",
      "Jiajie Peng"
    ],
    "abstract": "Single-cell RNA sequencing (scRNA-seq) data analysis is crucial for\nbiological research, as it enables the precise characterization of cellular\nheterogeneity. However, manual manipulation of various tools to achieve desired\noutcomes can be labor-intensive for researchers. To address this, we introduce\nCellAgent (http://cell.agent4science.cn/), an LLM-driven multi-agent framework,\nspecifically designed for the automatic processing and execution of scRNA-seq\ndata analysis tasks, providing high-quality results with no human intervention.\nFirstly, to adapt general LLMs to the biological field, CellAgent constructs\nLLM-driven biological expert roles - planner, executor, and evaluator - each\nwith specific responsibilities. Then, CellAgent introduces a hierarchical\ndecision-making mechanism to coordinate these biological experts, effectively\ndriving the planning and step-by-step execution of complex data analysis tasks.\nFurthermore, we propose a self-iterative optimization mechanism, enabling\nCellAgent to autonomously evaluate and optimize solutions, thereby guaranteeing\noutput quality. We evaluate CellAgent on a comprehensive benchmark dataset\nencompassing dozens of tissues and hundreds of distinct cell types. Evaluation\nresults consistently show that CellAgent effectively identifies the most\nsuitable tools and hyperparameters for single-cell analysis tasks, achieving\noptimal performance. This automated framework dramatically reduces the workload\nfor science data analyses, bringing us into the \"Agent for Science\" era.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "q-bio.GN"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09811v1",
    "published_date": "2024-07-13 09:14:50 UTC",
    "updated_date": "2024-07-13 09:14:50 UTC"
  },
  {
    "arxiv_id": "2407.09809v1",
    "title": "Preserving the Privacy of Reward Functions in MDPs through Deception",
    "authors": [
      "Shashank Reddy Chirra",
      "Pradeep Varakantham",
      "Praveen Paruchuri"
    ],
    "abstract": "Preserving the privacy of preferences (or rewards) of a sequential\ndecision-making agent when decisions are observable is crucial in many physical\nand cybersecurity domains. For instance, in wildlife monitoring, agents must\nallocate patrolling resources without revealing animal locations to poachers.\nThis paper addresses privacy preservation in planning over a sequence of\nactions in MDPs, where the reward function represents the preference structure\nto be protected. Observers can use Inverse RL (IRL) to learn these preferences,\nmaking this a challenging task.\n  Current research on differential privacy in reward functions fails to ensure\nguarantee on the minimum expected reward and offers theoretical guarantees that\nare inadequate against IRL-based observers. To bridge this gap, we propose a\nnovel approach rooted in the theory of deception. Deception includes two\nmodels: dissimulation (hiding the truth) and simulation (showing the wrong).\nOur first contribution theoretically demonstrates significant privacy leaks in\nexisting dissimulation-based methods. Our second contribution is a novel\nRL-based planning algorithm that uses simulation to effectively address these\nprivacy concerns while ensuring a guarantee on the expected reward. Experiments\non multiple benchmark problems show that our approach outperforms previous\nmethods in preserving reward function privacy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.09809v1",
    "published_date": "2024-07-13 09:03:22 UTC",
    "updated_date": "2024-07-13 09:03:22 UTC"
  },
  {
    "arxiv_id": "2407.11078v1",
    "title": "Overcoming Catastrophic Forgetting in Federated Class-Incremental Learning via Federated Global Twin Generator",
    "authors": [
      "Thinh Nguyen",
      "Khoa D Doan",
      "Binh T. Nguyen",
      "Danh Le-Phuoc",
      "Kok-Seng Wong"
    ],
    "abstract": "Federated Class-Incremental Learning (FCIL) increasingly becomes important in\nthe decentralized setting, where it enables multiple participants to\ncollaboratively train a global model to perform well on a sequence of tasks\nwithout sharing their private data. In FCIL, conventional Federated Learning\nalgorithms such as FedAVG often suffer from catastrophic forgetting, resulting\nin significant performance declines on earlier tasks. Recent works, based on\ngenerative models, produce synthetic images to help mitigate this issue across\nall classes, but these approaches' testing accuracy on previous classes is\nstill much lower than recent classes, i.e., having better plasticity than\nstability. To overcome these issues, this paper presents Federated Global Twin\nGenerator (FedGTG), an FCIL framework that exploits privacy-preserving\ngenerative-model training on the global side without accessing client data.\nSpecifically, the server trains a data generator and a feature generator to\ncreate two types of information from all seen classes, and then it sends the\nsynthetic data to the client side. The clients then use\nfeature-direction-controlling losses to make the local models retain knowledge\nand learn new tasks well. We extensively analyze the robustness of FedGTG on\nnatural images, as well as its ability to converge to flat local minima and\nachieve better-predicting confidence (calibration). Experimental results on\nCIFAR-10, CIFAR-100, and tiny-ImageNet demonstrate the improvements in accuracy\nand forgetting measures of FedGTG compared to previous frameworks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "68T07 (Primary), 68T45 (Secondary)"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11078v1",
    "published_date": "2024-07-13 08:23:21 UTC",
    "updated_date": "2024-07-13 08:23:21 UTC"
  },
  {
    "arxiv_id": "2407.09801v1",
    "title": "IoT-LM: Large Multisensory Language Models for the Internet of Things",
    "authors": [
      "Shentong Mo",
      "Russ Salakhutdinov",
      "Louis-Philippe Morency",
      "Paul Pu Liang"
    ],
    "abstract": "The Internet of Things (IoT) network integrating billions of smart physical\ndevices embedded with sensors, software, and communication technologies is a\ncritical and rapidly expanding component of our modern world. The IoT ecosystem\nprovides a rich source of real-world modalities such as motion, thermal,\ngeolocation, imaging, depth, sensors, and audio to recognize the states of\nhumans and physical objects. Machine learning presents a rich opportunity to\nautomatically process IoT data at scale, enabling efficient inference for\nunderstanding human wellbeing, controlling physical devices, and\ninterconnecting smart cities. To realize this potential, we introduce IoT-LM,\nan open-source large multisensory language model tailored for the IoT\necosystem. IoT-LM is enabled by two technical contributions: the first is\nMultiIoT, the most expansive unified IoT dataset to date, encompassing over\n1.15 million samples from 12 modalities and 8 tasks prepared for multisensory\npre-training and instruction-tuning. The second is a new multisensory multitask\nadapter layer to condition pre-trained large language models on multisensory\nIoT data. Not only does IoT-LM yield substantial improvements on 8 supervised\nIoT classification tasks, but it also demonstrates new interactive\nquestion-answering, reasoning, and dialog capabilities conditioned on IoT\nsensors. We release IoT-LM's data sources and new multisensory language\nmodeling framework.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2311.06217",
    "pdf_url": "http://arxiv.org/pdf/2407.09801v1",
    "published_date": "2024-07-13 08:20:37 UTC",
    "updated_date": "2024-07-13 08:20:37 UTC"
  },
  {
    "arxiv_id": "2407.11077v1",
    "title": "Deep reinforcement learning with symmetric data augmentation applied for aircraft lateral attitude tracking control",
    "authors": [
      "Yifei Li",
      "Erik-jan van Kampen"
    ],
    "abstract": "Symmetry is an essential property in some dynamical systems that can be\nexploited for state transition prediction and control policy optimization. This\npaper develops two symmetry-integrated Reinforcement Learning (RL) algorithms\nbased on standard Deep Deterministic Policy Gradient (DDPG),which leverage\nenvironment symmetry to augment explored transition samples of a Markov\nDecision Process(MDP). The firstly developed algorithm is named as Deep\nDeterministic Policy Gradient with Symmetric Data Augmentation (DDPG-SDA),\nwhich enriches dataset of standard DDPG algorithm by symmetric data\naugmentation method under symmetry assumption of a dynamical system. To further\nimprove sample utilization efficiency, the second developed RL algorithm\nincorporates one extra critic network, which is independently trained with\naugmented dataset. A two-step approximate policy iteration method is proposed\nto integrate training for two critic networks and one actor network. The\nresulting RL algorithm is named as Deep Deterministic Policy Gradient with\nSymmetric Critic Augmentation (DDPG-SCA). Simulation results demonstrate\nenhanced sample efficiency and tracking performance of developed two RL\nalgorithms in aircraft lateral tracking control task.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11077v1",
    "published_date": "2024-07-13 08:20:11 UTC",
    "updated_date": "2024-07-13 08:20:11 UTC"
  },
  {
    "arxiv_id": "2407.12866v1",
    "title": "Beyond KV Caching: Shared Attention for Efficient LLMs",
    "authors": [
      "Bingli Liao",
      "Danilo Vasconcellos Vargas"
    ],
    "abstract": "The efficiency of large language models (LLMs) remains a critical challenge,\nparticularly in contexts where computational resources are limited. Traditional\nattention mechanisms in these models, while powerful, require significant\ncomputational and memory resources due to the necessity of recalculating and\nstoring attention weights across different layers. This paper introduces a\nnovel Shared Attention (SA) mechanism, designed to enhance the efficiency of\nLLMs by directly sharing computed attention weights across multiple layers.\nUnlike previous methods that focus on sharing intermediate Key-Value (KV)\ncaches, our approach utilizes the isotropic tendencies of attention\ndistributions observed in advanced LLMs post-pretraining to reduce both the\ncomputational flops and the size of the KV cache required during inference. We\nempirically demonstrate that implementing SA across various LLMs results in\nminimal accuracy loss on standard benchmarks. Our findings suggest that SA not\nonly conserves computational resources but also maintains robust model\nperformance, thereby facilitating the deployment of more efficient LLMs in\nresource-constrained environments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12866v1",
    "published_date": "2024-07-13 07:23:07 UTC",
    "updated_date": "2024-07-13 07:23:07 UTC"
  },
  {
    "arxiv_id": "2407.09788v1",
    "title": "Explanation is All You Need in Distillation: Mitigating Bias and Shortcut Learning",
    "authors": [
      "Pedro R. A. S. Bassi",
      "Andrea Cavalli",
      "Sergio Decherchi"
    ],
    "abstract": "Bias and spurious correlations in data can cause shortcut learning,\nundermining out-of-distribution (OOD) generalization in deep neural networks.\nMost methods require unbiased data during training (and/or hyper-parameter\ntuning) to counteract shortcut learning. Here, we propose the use of\nexplanation distillation to hinder shortcut learning. The technique does not\nassume any access to unbiased data, and it allows an arbitrarily sized student\nnetwork to learn the reasons behind the decisions of an unbiased teacher, such\nas a vision-language model or a network processing debiased images. We found\nthat it is possible to train a neural network with explanation (e.g by Layer\nRelevance Propagation, LRP) distillation only, and that the technique leads to\nhigh resistance to shortcut learning, surpassing group-invariant learning,\nexplanation background minimization, and alternative distillation techniques.\nIn the COLOURED MNIST dataset, LRP distillation achieved 98.2% OOD accuracy,\nwhile deep feature distillation and IRM achieved 92.1% and 60.2%, respectively.\nIn COCO-on-Places, the undesirable generalization gap between in-distribution\nand OOD accuracy is only of 4.4% for LRP distillation, while the other two\ntechniques present gaps of 15.1% and 52.1%, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09788v1",
    "published_date": "2024-07-13 07:04:28 UTC",
    "updated_date": "2024-07-13 07:04:28 UTC"
  },
  {
    "arxiv_id": "2407.09779v1",
    "title": "Layout-and-Retouch: A Dual-stage Framework for Improving Diversity in Personalized Image Generation",
    "authors": [
      "Kangyeol Kim",
      "Wooseok Seo",
      "Sehyun Nam",
      "Bodam Kim",
      "Suhyeon Jeong",
      "Wonwoo Cho",
      "Jaegul Choo",
      "Youngjae Yu"
    ],
    "abstract": "Personalized text-to-image (P-T2I) generation aims to create new, text-guided\nimages featuring the personalized subject with a few reference images. However,\nbalancing the trade-off relationship between prompt fidelity and identity\npreservation remains a critical challenge. To address the issue, we propose a\nnovel P-T2I method called Layout-and-Retouch, consisting of two stages: 1)\nlayout generation and 2) retouch. In the first stage, our step-blended\ninference utilizes the inherent sample diversity of vanilla T2I models to\nproduce diversified layout images, while also enhancing prompt fidelity. In the\nsecond stage, multi-source attention swapping integrates the context image from\nthe first stage with the reference image, leveraging the structure from the\ncontext image and extracting visual features from the reference image. This\nachieves high prompt fidelity while preserving identity characteristics.\nThrough our extensive experiments, we demonstrate that our method generates a\nwide variety of images with diverse layouts while maintaining the unique\nidentity features of the personalized objects, even with challenging text\nprompts. This versatility highlights the potential of our framework to handle\ncomplex conditions, significantly enhancing the diversity and applicability of\npersonalized image synthesis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09779v1",
    "published_date": "2024-07-13 05:28:45 UTC",
    "updated_date": "2024-07-13 05:28:45 UTC"
  },
  {
    "arxiv_id": "2407.09777v1",
    "title": "Graph Transformers: A Survey",
    "authors": [
      "Ahsan Shehzad",
      "Feng Xia",
      "Shagufta Abid",
      "Ciyuan Peng",
      "Shuo Yu",
      "Dongyu Zhang",
      "Karin Verspoor"
    ],
    "abstract": "Graph transformers are a recent advancement in machine learning, offering a\nnew class of neural network models for graph-structured data. The synergy\nbetween transformers and graph learning demonstrates strong performance and\nversatility across various graph-related tasks. This survey provides an\nin-depth review of recent progress and challenges in graph transformer\nresearch. We begin with foundational concepts of graphs and transformers. We\nthen explore design perspectives of graph transformers, focusing on how they\nintegrate graph inductive biases and graph attention mechanisms into the\ntransformer architecture. Furthermore, we propose a taxonomy classifying graph\ntransformers based on depth, scalability, and pre-training strategies,\nsummarizing key principles for effective development of graph transformer\nmodels. Beyond technical analysis, we discuss the applications of graph\ntransformer models for node-level, edge-level, and graph-level tasks, exploring\ntheir potential in other application scenarios as well. Finally, we identify\nremaining challenges in the field, such as scalability and efficiency,\ngeneralization and robustness, interpretability and explainability, dynamic and\ncomplex graphs, as well as data quality and diversity, charting future\ndirections for graph transformer research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07, 68T05, 68U01",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.09777v1",
    "published_date": "2024-07-13 05:15:24 UTC",
    "updated_date": "2024-07-13 05:15:24 UTC"
  },
  {
    "arxiv_id": "2407.09774v3",
    "title": "ContextualStory: Consistent Visual Storytelling with Spatially-Enhanced and Storyline Context",
    "authors": [
      "Sixiao Zheng",
      "Yanwei Fu"
    ],
    "abstract": "Visual storytelling involves generating a sequence of coherent frames from a\ntextual storyline while maintaining consistency in characters and scenes.\nExisting autoregressive methods, which rely on previous frame-sentence pairs,\nstruggle with high memory usage, slow generation speeds, and limited context\nintegration. To address these issues, we propose ContextualStory, a novel\nframework designed to generate coherent story frames and extend frames for\nvisual storytelling. ContextualStory utilizes Spatially-Enhanced Temporal\nAttention to capture spatial and temporal dependencies, handling significant\ncharacter movements effectively. Additionally, we introduce a Storyline\nContextualizer to enrich context in storyline embedding, and a StoryFlow\nAdapter to measure scene changes between frames for guiding the model.\nExtensive experiments on PororoSV and FlintstonesSV datasets demonstrate that\nContextualStory significantly outperforms existing SOTA methods in both story\nvisualization and continuation. Code is available at\nhttps://github.com/sixiaozheng/ContextualStory.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09774v3",
    "published_date": "2024-07-13 05:02:42 UTC",
    "updated_date": "2025-02-24 14:02:08 UTC"
  },
  {
    "arxiv_id": "2407.11075v7",
    "title": "A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)",
    "authors": [
      "Tianrui Ji",
      "Yuntian Hou",
      "Di Zhang"
    ],
    "abstract": "Through this comprehensive survey of Kolmogorov-Arnold Networks(KAN), we have\ngained a thorough understanding of its theoretical foundation, architectural\ndesign, application scenarios, and current research progress. KAN, with its\nunique architecture and flexible activation functions, excels in handling\ncomplex data patterns and nonlinear relationships, demonstrating wide-ranging\napplication potential. While challenges remain, KAN is poised to pave the way\nfor innovative solutions in various fields, potentially revolutionizing how we\napproach complex computational problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11075v7",
    "published_date": "2024-07-13 04:29:36 UTC",
    "updated_date": "2025-01-28 02:23:25 UTC"
  },
  {
    "arxiv_id": "2407.11074v1",
    "title": "ST-RetNet: A Long-term Spatial-Temporal Traffic Flow Prediction Method",
    "authors": [
      "Baichao Long",
      "Wang Zhu",
      "Jianli Xiao"
    ],
    "abstract": "Traffic flow forecasting is considered a critical task in the field of\nintelligent transportation systems. In this paper, to address the issue of low\naccuracy in long-term forecasting of spatial-temporal big data on traffic flow,\nwe propose an innovative model called Spatial-Temporal Retentive Network\n(ST-RetNet). We extend the Retentive Network to address the task of traffic\nflow forecasting. At the spatial scale, we integrate a topological graph\nstructure into Spatial Retentive Network(S-RetNet), utilizing an adaptive\nadjacency matrix to extract dynamic spatial features of the road network. We\nalso employ Graph Convolutional Networks to extract static spatial features of\nthe road network. These two components are then fused to capture dynamic and\nstatic spatial correlations. At the temporal scale, we propose the Temporal\nRetentive Network(T-RetNet), which has been demonstrated to excel in capturing\nlong-term dependencies in traffic flow patterns compared to other time series\nmodels, including Recurrent Neural Networks based and transformer models. We\nachieve the spatial-temporal traffic flow forecasting task by integrating\nS-RetNet and T-RetNet to form ST-RetNet. Through experimental comparisons\nconducted on four real-world datasets, we demonstrate that ST-RetNet\noutperforms the state-of-the-art approaches in traffic flow forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11074v1",
    "published_date": "2024-07-13 03:52:32 UTC",
    "updated_date": "2024-07-13 03:52:32 UTC"
  },
  {
    "arxiv_id": "2407.09760v1",
    "title": "ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report",
    "authors": [
      "Yixiao Yuan",
      "Yingzhe Peng"
    ],
    "abstract": "The Visual-Dialog Based Emotion Explanation Generation Challenge focuses on\ngenerating emotion explanations through visual-dialog interactions in art\ndiscussions. Our approach combines state-of-the-art multi-modal models,\nincluding Language Model (LM) and Large Vision Language Model (LVLM), to\nachieve superior performance. By leveraging these models, we outperform\nexisting benchmarks, securing the top rank in the ICCV23 Visual-Dialog Based\nEmotion Explanation Generation Challenge, which is part of the 5th Workshop On\nClosing The Loop Between Vision And Language (CLCV) with significant scores in\nF1 and BLEU metrics. Our method demonstrates exceptional ability in generating\naccurate emotion explanations, advancing our understanding of emotional impacts\nin art.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09760v1",
    "published_date": "2024-07-13 03:39:41 UTC",
    "updated_date": "2024-07-13 03:39:41 UTC"
  },
  {
    "arxiv_id": "2407.09739v2",
    "title": "Active Learning for Derivative-Based Global Sensitivity Analysis with Gaussian Processes",
    "authors": [
      "Syrine Belakaria",
      "Benjamin Letham",
      "Janardhan Rao Doppa",
      "Barbara Engelhardt",
      "Stefano Ermon",
      "Eytan Bakshy"
    ],
    "abstract": "We consider the problem of active learning for global sensitivity analysis of\nexpensive black-box functions. Our aim is to efficiently learn the importance\nof different input variables, e.g., in vehicle safety experimentation, we study\nthe impact of the thickness of various components on safety objectives. Since\nfunction evaluations are expensive, we use active learning to prioritize\nexperimental resources where they yield the most value. We propose novel active\nlearning acquisition functions that directly target key quantities of\nderivative-based global sensitivity measures (DGSMs) under Gaussian process\nsurrogate models. We showcase the first application of active learning directly\nto DGSMs, and develop tractable uncertainty reduction and information gain\nacquisition functions for these measures. Through comprehensive evaluation on\nsynthetic and real-world problems, our study demonstrates how these active\nlearning acquisition strategies substantially enhance the sample efficiency of\nDGSM estimation, particularly with limited evaluation budgets. Our work paves\nthe way for more efficient and accurate sensitivity analysis in various\nscientific and engineering applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09739v2",
    "published_date": "2024-07-13 01:41:12 UTC",
    "updated_date": "2024-10-19 22:48:12 UTC"
  },
  {
    "arxiv_id": "2407.09726v1",
    "title": "On Mitigating Code LLM Hallucinations with API Documentation",
    "authors": [
      "Nihal Jain",
      "Robert Kwiatkowski",
      "Baishakhi Ray",
      "Murali Krishna Ramanathan",
      "Varun Kumar"
    ],
    "abstract": "In this study, we address the issue of API hallucinations in various software\nengineering contexts. We introduce CloudAPIBench, a new benchmark designed to\nmeasure API hallucination occurrences. CloudAPIBench also provides annotations\nfor frequencies of API occurrences in the public domain, allowing us to study\nAPI hallucinations at various frequency levels. Our findings reveal that Code\nLLMs struggle with low frequency APIs: for e.g., GPT-4o achieves only 38.58%\nvalid low frequency API invocations. We demonstrate that Documentation\nAugmented Generation (DAG) significantly improves performance for low frequency\nAPIs (increase to 47.94% with DAG) but negatively impacts high frequency APIs\nwhen using sub-optimal retrievers (a 39.02% absolute drop). To mitigate this,\nwe propose to intelligently trigger DAG where we check against an API index or\nleverage Code LLMs' confidence scores to retrieve only when needed. We\ndemonstrate that our proposed methods enhance the balance between low and high\nfrequency API performance, resulting in more reliable API invocations (8.20%\nabsolute improvement on CloudAPIBench for GPT-4o).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09726v1",
    "published_date": "2024-07-13 00:16:26 UTC",
    "updated_date": "2024-07-13 00:16:26 UTC"
  }
]
[
  {
    "arxiv_id": "2402.07327v1",
    "title": "Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers",
    "authors": [
      "Minoo Shayaninasab",
      "Bagher Babaali"
    ],
    "abstract": "Due to the complex nature of human emotions and the diversity of emotion\nrepresentation methods in humans, emotion recognition is a challenging field.\nIn this research, three input modalities, namely text, audio (speech), and\nvideo, are employed to generate multimodal feature vectors. For generating\nfeatures for each of these modalities, pre-trained Transformer models with\nfine-tuning are utilized. In each modality, a Transformer model is used with\ntransfer learning to extract feature and emotional structure. These features\nare then fused together, and emotion recognition is performed using a\nclassifier. To select an appropriate fusion method and classifier, various\nfeature-level and decision-level fusion techniques have been experimented with,\nand ultimately, the best model, which combines feature-level fusion by\nconcatenating feature vectors and classification using a Support Vector Machine\non the IEMOCAP multimodal dataset, achieves an accuracy of 75.42%. Keywords:\nMultimodal Emotion Recognition, IEMOCAP, Self-Supervised Learning, Transfer\nLearning, Transformer.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07327v1",
    "published_date": "2024-02-11 23:27:24 UTC",
    "updated_date": "2024-02-11 23:27:24 UTC"
  },
  {
    "arxiv_id": "2402.07326v1",
    "title": "Persian Speech Emotion Recognition by Fine-Tuning Transformers",
    "authors": [
      "Minoo Shayaninasab",
      "Bagher Babaali"
    ],
    "abstract": "Given the significance of speech emotion recognition, numerous methods have\nbeen developed in recent years to create effective and efficient systems in\nthis domain. One of these methods involves the use of pretrained transformers,\nfine-tuned to address this specific problem, resulting in high accuracy.\nDespite extensive discussions and global-scale efforts to enhance these\nsystems, the application of this innovative and effective approach has received\nless attention in the context of Persian speech emotion recognition. In this\narticle, we review the field of speech emotion recognition and its background,\nwith an emphasis on the importance of employing transformers in this context.\nWe present two models, one based on spectrograms and the other on the audio\nitself, fine-tuned using the shEMO dataset. These models significantly enhance\nthe accuracy of previous systems, increasing it from approximately 65% to 80%\non the mentioned dataset. Subsequently, to investigate the effect of\nmultilinguality on the fine-tuning process, these same models are fine-tuned\ntwice. First, they are fine-tuned using the English IEMOCAP dataset, and then\nthey are fine-tuned with the Persian shEMO dataset. This results in an improved\naccuracy of 82% for the Persian emotion recognition system. Keywords: Persian\nSpeech Emotion Recognition, shEMO, Self-Supervised Learning",
    "categories": [
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07326v1",
    "published_date": "2024-02-11 23:23:31 UTC",
    "updated_date": "2024-02-11 23:23:31 UTC"
  },
  {
    "arxiv_id": "2402.07320v1",
    "title": "Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets",
    "authors": [
      "Ross Greer",
      "Mohan Trivedi"
    ],
    "abstract": "This research explores the integration of language embeddings for active\nlearning in autonomous driving datasets, with a focus on novelty detection.\nNovelty arises from unexpected scenarios that autonomous vehicles struggle to\nnavigate, necessitating higher-level reasoning abilities. Our proposed method\nemploys language-based representations to identify novel scenes, emphasizing\nthe dual purpose of safety takeover responses and active learning. The research\npresents a clustering experiment using Contrastive Language-Image Pretrained\n(CLIP) embeddings to organize datasets and detect novelties. We find that the\nproposed algorithm effectively isolates novel scenes from a collection of\nsubsets derived from two real-world driving datasets, one vehicle-mounted and\none infrastructure-mounted. From the generated clusters, we further present\nmethods for generating textual explanations of elements which differentiate\nscenes classified as novel from other scenes in the data pool, presenting\nqualitative examples from the clustered results. Our results demonstrate the\neffectiveness of language-driven embeddings in identifying novel elements and\ngenerating explanations of data, and we further discuss potential applications\nin safe takeovers, data curation, and multi-task active learning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07320v1",
    "published_date": "2024-02-11 22:53:21 UTC",
    "updated_date": "2024-02-11 22:53:21 UTC"
  },
  {
    "arxiv_id": "2402.07319v1",
    "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
    "authors": [
      "Lichang Chen",
      "Chen Zhu",
      "Davit Soselia",
      "Jiuhai Chen",
      "Tianyi Zhou",
      "Tom Goldstein",
      "Heng Huang",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ],
    "abstract": "In this work, we study the issue of reward hacking on the response length, a\nchallenge emerging in Reinforcement Learning from Human Feedback (RLHF) on\nLLMs. A well-formatted, verbose but less helpful response from the LLMs can\noften deceive LLMs or even human evaluators to achieve high scores. The same\nissue also holds for some reward models in RL. To address the challenges in\nboth training and evaluation, we establish a more reliable evaluation protocol\nfor comparing different training configurations, which inspects the trade-off\nbetween LLM evaluation score and response length obtained by varying training\nhyperparameters. Based on this evaluation, we conduct large-scale studies,\nwhere the results shed insights into the efficacy of hyperparameters and tricks\nused in RL on mitigating length bias. We further propose to improve the reward\nmodel by jointly training two linear heads on shared feature representations to\npredict the rewards, one trained to correlate with length, and the other\ntrained to decorrelate with length and therefore focus more on the actual\ncontent. We then discard the length head in RL to prevent reward hacking on\nlength. Experiments demonstrate that our approach almost eliminates the reward\ncorrelation with length, and improves the obtained policy by a significant\nmargin.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07319v1",
    "published_date": "2024-02-11 22:40:12 UTC",
    "updated_date": "2024-02-11 22:40:12 UTC"
  },
  {
    "arxiv_id": "2402.07295v1",
    "title": "Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning",
    "authors": [
      "Mohak Chadha",
      "Pulkit Khera",
      "Jianfeng Gu",
      "Osama Abboud",
      "Michael Gerndt"
    ],
    "abstract": "Federated Learning (FL) is an emerging machine learning paradigm that enables\nthe collaborative training of a shared global model across distributed clients\nwhile keeping the data decentralized. Recent works on designing systems for\nefficient FL have shown that utilizing serverless computing technologies,\nparticularly Function-as-a-Service (FaaS) for FL, can enhance resource\nefficiency, reduce training costs, and alleviate the complex infrastructure\nmanagement burden on data holders. However, existing serverless FL systems\nimplicitly assume a uniform global model architecture across all participating\nclients during training. This assumption fails to address fundamental\nchallenges in practical FL due to the resource and statistical data\nheterogeneity among FL clients. To address these challenges and enable\nheterogeneous client models in serverless FL, we utilize Knowledge Distillation\n(KD) in this paper. Towards this, we propose novel optimized serverless\nworkflows for two popular conventional federated KD techniques, i.e., FedMD and\nFedDF. We implement these workflows by introducing several extensions to an\nopen-source serverless FL system called FedLess. Moreover, we comprehensively\nevaluate the two strategies on multiple datasets across varying levels of\nclient data heterogeneity using heterogeneous client models with respect to\naccuracy, fine-grained training times, and costs. Results from our experiments\ndemonstrate that serverless FedDF is more robust to extreme non-IID data\ndistributions, is faster, and leads to lower costs than serverless FedMD. In\naddition, compared to the original implementation, our optimizations for\nparticular steps in FedMD and FedDF lead to an average speedup of 3.5x and\n1.76x across all datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "ACM/SIGAPP Symposium on Applied Computing 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.07295v1",
    "published_date": "2024-02-11 20:15:52 UTC",
    "updated_date": "2024-02-11 20:15:52 UTC"
  },
  {
    "arxiv_id": "2402.07294v1",
    "title": "On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study",
    "authors": [
      "Amir M. Mir",
      "Mehdi Keshani",
      "Sebastian Proksch"
    ],
    "abstract": "Static call graph (CG) construction often over-approximates call relations,\nleading to sound, but imprecise results. Recent research has explored machine\nlearning (ML)-based CG pruning as a means to enhance precision by eliminating\nfalse edges. However, current methods suffer from a limited evaluation dataset,\nimbalanced training data, and reduced recall, which affects practical\ndownstream analyses. Prior results were also not compared with advanced static\nCG construction techniques yet. This study tackles these issues. We introduce\nthe NYXCorpus, a dataset of real-world Java programs with high test coverage\nand we collect traces from test executions and build a ground truth of dynamic\nCGs. We leverage these CGs to explore conservative pruning strategies during\nthe training and inference of ML-based CG pruners. We conduct a comparative\nanalysis of static CGs generated using zero control flow analysis (0-CFA) and\nthose produced by a context-sensitive 1-CFA algorithm, evaluating both with and\nwithout pruning. We find that CG pruning is a difficult task for real-world\nJava projects and substantial improvements in the CG precision (+25%) meet\nreduced recall (-9%). However, our experiments show promising results: even\nwhen we favor recall over precision by using an F2 metric in our experiments,\nwe can show that pruned CGs have comparable quality to a context-sensitive\n1-CFA analysis while being computationally less demanding. Resulting CGs are\nmuch smaller (69%), and substantially faster (3.5x speed-up), with virtually\nunchanged results in our downstream analysis.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted at the technical track of MSR'24",
    "pdf_url": "http://arxiv.org/pdf/2402.07294v1",
    "published_date": "2024-02-11 20:15:44 UTC",
    "updated_date": "2024-02-11 20:15:44 UTC"
  },
  {
    "arxiv_id": "2402.07283v1",
    "title": "Power Transformer Fault Prediction Based on Knowledge Graphs",
    "authors": [
      "Chao Wang",
      "Zhuo Chen",
      "Ziyan Zhang",
      "Chiyi Li",
      "Kai Song"
    ],
    "abstract": "In this paper, we address the challenge of learning with limited fault data\nfor power transformers. Traditional operation and maintenance tools lack\neffective predictive capabilities for potential faults. The scarcity of\nextensive fault data makes it difficult to apply machine learning techniques\neffectively. To solve this problem, we propose a novel approach that leverages\nthe knowledge graph (KG) technology in combination with gradient boosting\ndecision trees (GBDT). This method is designed to efficiently learn from a\nsmall set of high-dimensional data, integrating various factors influencing\ntransformer faults and historical operational data. Our approach enables\naccurate safe state assessments and fault analyses of power transformers\ndespite the limited fault characteristic data. Experimental results demonstrate\nthat this method outperforms other learning approaches in prediction accuracy,\nsuch as artificial neural networks (ANN) and logistic regression (LR).\nFurthermore, it offers significant improvements in progressiveness,\npracticality, and potential for widespread application.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07283v1",
    "published_date": "2024-02-11 19:14:28 UTC",
    "updated_date": "2024-02-11 19:14:28 UTC"
  },
  {
    "arxiv_id": "2402.07282v2",
    "title": "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?",
    "authors": [
      "Ryan Liu",
      "Theodore R. Sumers",
      "Ishita Dasgupta",
      "Thomas L. Griffiths"
    ],
    "abstract": "In day-to-day communication, people often approximate the truth - for\nexample, rounding the time or omitting details - in order to be maximally\nhelpful to the listener. How do large language models (LLMs) handle such\nnuanced trade-offs? To address this question, we use psychological models and\nexperiments designed to characterize human behavior to analyze LLMs. We test a\nrange of LLMs and explore how optimization for human preferences or\ninference-time reasoning affects these trade-offs. We find that reinforcement\nlearning from human feedback improves both honesty and helpfulness, while\nchain-of-thought prompting skews LLMs towards helpfulness over honesty.\nFinally, GPT-4 Turbo demonstrates human-like response patterns including\nsensitivity to the conversational framing and listener's decision context. Our\nfindings reveal the conversational values internalized by LLMs and suggest that\neven these abstract values can, to a degree, be steered by zero-shot prompting.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07282v2",
    "published_date": "2024-02-11 19:13:26 UTC",
    "updated_date": "2024-02-13 14:21:02 UTC"
  },
  {
    "arxiv_id": "2403.17012v2",
    "title": "Evolution and Efficiency in Neural Architecture Search: Bridging the Gap Between Expert Design and Automated Optimization",
    "authors": [
      "Fanfei Meng",
      "Chen-Ao Wang",
      "Lele Zhang"
    ],
    "abstract": "The paper provides a comprehensive overview of Neural Architecture Search\n(NAS), emphasizing its evolution from manual design to automated,\ncomputationally-driven approaches. It covers the inception and growth of NAS,\nhighlighting its application across various domains, including medical imaging\nand natural language processing. The document details the shift from\nexpert-driven design to algorithm-driven processes, exploring initial\nmethodologies like reinforcement learning and evolutionary algorithms. It also\ndiscusses the challenges of computational demands and the emergence of\nefficient NAS methodologies, such as Differentiable Architecture Search and\nhardware-aware NAS. The paper further elaborates on NAS's application in\ncomputer vision, NLP, and beyond, demonstrating its versatility and potential\nfor optimizing neural network architectures across different tasks. Future\ndirections and challenges, including computational efficiency and the\nintegration with emerging AI domains, are addressed, showcasing NAS's dynamic\nnature and its continued evolution towards more sophisticated and efficient\narchitecture search methods.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "7 Pages, Double Column",
    "pdf_url": "http://arxiv.org/pdf/2403.17012v2",
    "published_date": "2024-02-11 18:27:29 UTC",
    "updated_date": "2024-04-02 06:35:04 UTC"
  },
  {
    "arxiv_id": "2402.07268v1",
    "title": "Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer",
    "authors": [
      "Zehao Dong",
      "Qihang Zhao",
      "Philip R. O. Payne",
      "Michael A Province",
      "Carlos Cruchaga",
      "Muhan Zhang",
      "Tianyu Zhao",
      "Yixin Chen",
      "Fuhai Li"
    ],
    "abstract": "Biomarker identification is critical for precise disease diagnosis and\nunderstanding disease pathogenesis in omics data analysis, like using fold\nchange and regression analysis. Graph neural networks (GNNs) have been the\ndominant deep learning model for analyzing graph-structured data. However, we\nfound two major limitations of existing GNNs in omics data analysis, i.e.,\nlimited-prediction (diagnosis) accuracy and limited-reproducible biomarker\nidentification capacity across multiple datasets. The root of the challenges is\nthe unique graph structure of biological signaling pathways, which consists of\na large number of targets and intensive and complex signaling interactions\namong these targets. To resolve these two challenges, in this study, we\npresented a novel GNN model architecture, named PathFormer, which\nsystematically integrate signaling network, priori knowledge and omics data to\nrank biomarkers and predict disease diagnosis. In the comparison results,\nPathFormer outperformed existing GNN models significantly in terms of highly\naccurate prediction capability ( 30% accuracy improvement in disease diagnosis\ncompared with existing GNN models) and high reproducibility of biomarker\nranking across different datasets. The improvement was confirmed using two\nindependent Alzheimer's Disease (AD) and cancer transcriptomic datasets. The\nPathFormer model can be directly applied to other omics data analysis studies.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07268v1",
    "published_date": "2024-02-11 18:23:54 UTC",
    "updated_date": "2024-02-11 18:23:54 UTC"
  },
  {
    "arxiv_id": "2402.07250v1",
    "title": "DIMON: Learning Solution Operators of Partial Differential Equations on a Diffeomorphic Family of Domains",
    "authors": [
      "Minglang Yin",
      "Nicolas Charon",
      "Ryan Brody",
      "Lu Lu",
      "Natalia Trayanova",
      "Mauro Maggioni"
    ],
    "abstract": "The solution of a PDE over varying initial/boundary conditions on multiple\ndomains is needed in a wide variety of applications, but it is computationally\nexpensive if the solution is computed de novo whenever the initial/boundary\nconditions of the domain change. We introduce a general operator learning\nframework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn\napproximate PDE solutions over a family of domains $\\{\\Omega_{\\theta}}_\\theta$,\nthat learns the map from initial/boundary conditions and domain $\\Omega_\\theta$\nto the solution of the PDE, or to specified functionals thereof. DIMON is based\non transporting a given problem (initial/boundary conditions and domain\n$\\Omega_{\\theta}$) to a problem on a reference domain $\\Omega_{0}$, where\ntraining data from multiple problems is used to learn the map to the solution\non $\\Omega_{0}$, which is then re-mapped to the original domain\n$\\Omega_{\\theta}$. We consider several problems to demonstrate the performance\nof the framework in learning both static and time-dependent PDEs on non-rigid\ngeometries; these include solving the Laplace equation, reaction-diffusion\nequations, and a multiscale PDE that characterizes the electrical propagation\non the left ventricle. This work paves the way toward the fast prediction of\nPDE solutions on a family of domains and the application of neural operators in\nengineering and precision medicine.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07250v1",
    "published_date": "2024-02-11 17:32:23 UTC",
    "updated_date": "2024-02-11 17:32:23 UTC"
  },
  {
    "arxiv_id": "2402.07244v1",
    "title": "SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic Paradigm",
    "authors": [
      "Junhao Song",
      "Yingfang Yuan",
      "Wei Pang"
    ],
    "abstract": "We propose a novel type of Artificial Immune System (AIS): Symbiotic\nArtificial Immune Systems (SAIS), drawing inspiration from symbiotic\nrelationships in biology. SAIS parallels the three key stages (i.e., mutualism,\ncommensalism and parasitism) of population updating from the Symbiotic\nOrganisms Search (SOS) algorithm. This parallel approach effectively addresses\nthe challenges of large population size and enhances population diversity in\nAIS, which traditional AIS and SOS struggle to resolve efficiently. We\nconducted a series of experiments, which demonstrated that our SAIS achieved\ncomparable performance to the state-of-the-art approach SOS and outperformed\nother popular AIS approaches and evolutionary algorithms across 26 benchmark\nproblems. Furthermore, we investigated the problem of parameter selection and\nfound that SAIS performs better in handling larger population sizes while\nrequiring fewer generations. Finally, we believe SAIS, as a novel bio-inspired\nand immune-inspired algorithm, paves the way for innovation in bio-inspired\ncomputing with the symbiotic paradigm.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07244v1",
    "published_date": "2024-02-11 16:58:59 UTC",
    "updated_date": "2024-02-11 16:58:59 UTC"
  },
  {
    "arxiv_id": "2402.07234v3",
    "title": "CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain",
    "authors": [
      "Xin Tong",
      "Bo Jin",
      "Zhi Lin",
      "Binjun Wang",
      "Ting Yu",
      "Qiang Cheng"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated significant potential and\neffectiveness across multiple application domains. To assess the performance of\nmainstream LLMs in public security tasks, this study aims to construct a\nspecialized evaluation benchmark tailored to the Chinese public security\ndomain--CPSDbench. CPSDbench integrates datasets related to public security\ncollected from real-world scenarios, supporting a comprehensive assessment of\nLLMs across four key dimensions: text classification, information extraction,\nquestion answering, and text generation. Furthermore, this study introduces a\nset of innovative evaluation metrics designed to more precisely quantify the\nefficacy of LLMs in executing tasks related to public security. Through the\nin-depth analysis and evaluation conducted in this research, we not only\nenhance our understanding of the performance strengths and limitations of\nexisting models in addressing public security issues but also provide\nreferences for the future development of more accurate and customized LLM\nmodels targeted at applications in this field.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07234v3",
    "published_date": "2024-02-11 15:56:03 UTC",
    "updated_date": "2024-03-21 12:39:09 UTC"
  },
  {
    "arxiv_id": "2402.07233v1",
    "title": "TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation",
    "authors": [
      "Peng Wang",
      "Xiang Wei",
      "Fangxu Hu",
      "Wenjuan Han"
    ],
    "abstract": "Natural language processing (NLP) is a key component of intelligent\ntransportation systems (ITS), but it faces many challenges in the\ntransportation domain, such as domain-specific knowledge and data, and\nmulti-modal inputs and outputs. This paper presents TransGPT, a novel\n(multi-modal) large language model for the transportation domain, which\nconsists of two independent variants: TransGPT-SM for single-modal data and\nTransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal\nTransportation dataset (STD) that contains textual data from various sources in\nthe transportation domain. TransGPT-MM is finetuned on a multi-modal\nTransportation dataset (MTD) that we manually collected from three areas of the\ntransportation domain: driving tests, traffic signs, and landmarks. We evaluate\nTransGPT on several benchmark datasets for different tasks in the\ntransportation domain, and show that it outperforms baseline models on most\ntasks. We also showcase the potential applications of TransGPT for traffic\nanalysis and modeling, such as generating synthetic traffic scenarios,\nexplaining traffic phenomena, answering traffic-related questions, providing\ntraffic recommendations, and generating traffic reports. This work advances the\nstate-of-the-art of NLP in the transportation domain and provides a useful tool\nfor ITS researchers and practitioners.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07233v1",
    "published_date": "2024-02-11 15:50:35 UTC",
    "updated_date": "2024-02-11 15:50:35 UTC"
  },
  {
    "arxiv_id": "2402.07229v1",
    "title": "Successive Refinement in Large-Scale Computation: Advancing Model Inference Applications",
    "authors": [
      "Homa Esfahanizadeh",
      "Alejandro Cohen",
      "Shlomo Shamai",
      "Muriel Medard"
    ],
    "abstract": "Modern computationally-intensive applications often operate under time\nconstraints, necessitating acceleration methods and distribution of\ncomputational workloads across multiple entities. However, the outcome is\neither achieved within the desired timeline or not, and in the latter case,\nvaluable resources are wasted. In this paper, we introduce solutions for\nlayered-resolution computation. These solutions allow lower-resolution results\nto be obtained at an earlier stage than the final result. This innovation\nnotably enhances the deadline-based systems, as if a computational job is\nterminated due to time constraints, an approximate version of the final result\ncan still be generated. Moreover, in certain operational regimes, a\nhigh-resolution result might be unnecessary, because the low-resolution result\nmay already deviate significantly from the decision threshold, for example in\nAI-based decision-making systems. Therefore, operators can decide whether\nhigher resolution is needed or not based on intermediate results, enabling\ncomputations with adaptive resolution. We present our framework for two\ncritical and computationally demanding jobs: distributed matrix multiplication\n(linear) and model inference in machine learning (nonlinear). Our theoretical\nand empirical results demonstrate that the execution delay for the first\nresolution is significantly shorter than that for the final resolution, while\nmaintaining overall complexity comparable to the conventional one-shot\napproach. Our experiments further illustrate how the layering feature increases\nthe likelihood of meeting deadlines and enables adaptability and transparency\nin massive, large-scale computations.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "13 pages, partially appeared in proceedings of IEEE Cloudnet 2022,\n  submitted and under review for IEEE Transactions on Signal Processing",
    "pdf_url": "http://arxiv.org/pdf/2402.07229v1",
    "published_date": "2024-02-11 15:36:33 UTC",
    "updated_date": "2024-02-11 15:36:33 UTC"
  },
  {
    "arxiv_id": "2402.07226v1",
    "title": "Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL",
    "authors": [
      "Sungyoon Kim",
      "Yunseon Choi",
      "Daiki E. Matsunaga",
      "Kee-Eung Kim"
    ],
    "abstract": "Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an\nimportant problem in RL that focuses on acquiring diverse goal-oriented skills\nsolely from pre-collected behavior datasets. In this setting, the reward\nfeedback is typically absent except when the goal is achieved, which makes it\ndifficult to learn policies especially from a finite dataset of suboptimal\nbehaviors. In addition, realistic scenarios involve long-horizon planning,\nwhich necessitates the extraction of useful skills within sub-trajectories.\nRecently, the conditional diffusion model has been shown to be a promising\napproach to generate high-quality long-horizon plans for RL. However, their\npracticality for the goal-conditioned setting is still limited due to a number\nof technical assumptions made by the methods. In this paper, we propose SSD\n(Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method\nthat leverages the conditional diffusion model to address these limitations. In\nsummary, we use the diffusion model that generates future plans conditioned on\nthe target goal and value, with the target value estimated from the\ngoal-relabeled offline dataset. We report state-of-the-art performance in the\nstandard benchmark set of GCRL tasks, and demonstrate the capability to\nsuccessfully stitch the segments of suboptimal trajectories in the offline data\nto generate high-quality plans.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07226v1",
    "published_date": "2024-02-11 15:23:13 UTC",
    "updated_date": "2024-02-11 15:23:13 UTC"
  },
  {
    "arxiv_id": "2402.07221v2",
    "title": "The Reasons that Agents Act: Intention and Instrumental Goals",
    "authors": [
      "Francis Rhys Ward",
      "Matt MacDermott",
      "Francesco Belardinelli",
      "Francesca Toni",
      "Tom Everitt"
    ],
    "abstract": "Intention is an important and challenging concept in AI. It is important\nbecause it underlies many other concepts we care about, such as agency,\nmanipulation, legal responsibility, and blame. However, ascribing intent to AI\nsystems is contentious, and there is no universally accepted theory of\nintention applicable to AI agents. We operationalise the intention with which\nan agent acts, relating to the reasons it chooses its decision. We introduce a\nformal definition of intention in structural causal influence models, grounded\nin the philosophy literature on intent and applicable to real-world machine\nlearning systems. Through a number of examples and results, we show that our\ndefinition captures the intuitive notion of intent and satisfies desiderata\nset-out by past work. In addition, we show how our definition relates to past\nconcepts, including actual causality, and the notion of instrumental goals,\nwhich is a core idea in the literature on safe AI agents. Finally, we\ndemonstrate how our definition can be used to infer the intentions of\nreinforcement learning agents and language models from their behaviour.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "AAMAS24",
    "pdf_url": "http://arxiv.org/pdf/2402.07221v2",
    "published_date": "2024-02-11 14:39:40 UTC",
    "updated_date": "2024-02-15 11:45:37 UTC"
  },
  {
    "arxiv_id": "2402.07204v5",
    "title": "ITINERA: Integrating Spatial Optimization with Large Language Models for Open-domain Urban Itinerary Planning",
    "authors": [
      "Yihong Tang",
      "Zhaokai Wang",
      "Ao Qu",
      "Yihao Yan",
      "Zhaofeng Wu",
      "Dingyi Zhuang",
      "Jushi Kai",
      "Kebing Hou",
      "Xiaotong Guo",
      "Han Zheng",
      "Tiange Luo",
      "Jinhua Zhao",
      "Zhan Zhao",
      "Wei Ma"
    ],
    "abstract": "Citywalk, a recently popular form of urban travel, requires genuine\npersonalization and understanding of fine-grained requests compared to\ntraditional itinerary planning. In this paper, we introduce the novel task of\nOpen-domain Urban Itinerary Planning (OUIP), which generates personalized urban\nitineraries from user requests in natural language. We then present ITINERA, an\nOUIP system that integrates spatial optimization with large language models to\nprovide customized urban itineraries based on user needs. This involves\ndecomposing user requests, selecting candidate points of interest (POIs),\nordering the POIs based on cluster-aware spatial optimization, and generating\nthe itinerary. Experiments on real-world datasets and the performance of the\ndeployed system demonstrate our system's capacity to deliver personalized and\nspatially coherent itineraries compared to current solutions. Source codes of\nITINERA are available at https://github.com/YihongT/ITINERA.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07204v5",
    "published_date": "2024-02-11 13:30:53 UTC",
    "updated_date": "2025-01-09 06:53:50 UTC"
  },
  {
    "arxiv_id": "2402.07199v1",
    "title": "Link-aware link prediction over temporal graph by pattern recognition",
    "authors": [
      "Bingqing Liu",
      "Xikun Huang"
    ],
    "abstract": "A temporal graph can be considered as a stream of links, each of which\nrepresents an interaction between two nodes at a certain time. On temporal\ngraphs, link prediction is a common task, which aims to answer whether the\nquery link is true or not. To do this task, previous methods usually focus on\nthe learning of representations of the two nodes in the query link. We point\nout that the learned representation by their models may encode too much\ninformation with side effects for link prediction because they have not\nutilized the information of the query link, i.e., they are link-unaware. Based\non this observation, we propose a link-aware model: historical links and the\nquery link are input together into the following model layers to distinguish\nwhether this input implies a reasonable pattern that ends with the query link.\nDuring this process, we focus on the modeling of link evolution patterns rather\nthan node representations. Experiments on six datasets show that our model\nachieves strong performances compared with state-of-the-art baselines, and the\nresults of link prediction are interpretable. The code and datasets are\navailable on the project website: https://github.com/lbq8942/TGACN.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, one column",
    "pdf_url": "http://arxiv.org/pdf/2402.07199v1",
    "published_date": "2024-02-11 13:26:06 UTC",
    "updated_date": "2024-02-11 13:26:06 UTC"
  },
  {
    "arxiv_id": "2402.07197v4",
    "title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
    "authors": [
      "Mengmei Zhang",
      "Mingwei Sun",
      "Peng Wang",
      "Shen Fan",
      "Yanhu Mo",
      "Xiaoxiao Xu",
      "Hong Liu",
      "Cheng Yang",
      "Chuan Shi"
    ],
    "abstract": "Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and\ninstruction-following capabilities, have catalyzed a revolutionary\ntransformation across diverse fields, especially for open-ended tasks. While\nthe idea is less explored in the graph domain, despite the availability of\nnumerous powerful graph models (GMs), they are restricted to tasks in a\npre-defined form. Although several methods applying LLMs to graphs have been\nproposed, they fail to simultaneously handle the pre-defined and open-ended\ntasks, with LLM as a node feature enhancer or as a standalone predictor. To\nbreak this dilemma, we propose to bridge the pretrained GM and LLM by a\nTranslator, named GraphTranslator, aiming to leverage GM to handle the\npre-defined tasks effectively and utilize the extended interface of LLMs to\noffer various open-ended tasks for GM. To train such Translator, we propose a\nProducer capable of constructing the graph-text alignment data along node\ninformation, neighbor information and model information. By translating node\nrepresentation into tokens, GraphTranslator empowers an LLM to make predictions\nbased on language instructions, providing a unified perspective for both\npre-defined and open-ended tasks. Extensive results demonstrate the\neffectiveness of our proposed GraphTranslator on zero-shot node classification.\nThe graph question answering experiments reveal our GraphTranslator potential\nacross a broad spectrum of open-ended tasks through language instructions. Our\ncode is available at: https://github.com/alibaba/GraphTranslator.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07197v4",
    "published_date": "2024-02-11 13:24:13 UTC",
    "updated_date": "2024-02-28 02:42:35 UTC"
  },
  {
    "arxiv_id": "2402.07191v1",
    "title": "GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention",
    "authors": [
      "Fangyu Ding",
      "Haiyang Wang",
      "Zhixuan Chu",
      "Tianming Li",
      "Zhaoping Hu",
      "Junchi Yan"
    ],
    "abstract": "Graph invariant learning (GIL) has been an effective approach to discovering\nthe invariant relationships between graph data and its labels for different\ngraph learning tasks under various distribution shifts. Many recent endeavors\nof GIL focus on extracting the invariant subgraph from the input graph for\nprediction as a regularization strategy to improve the generalization\nperformance of graph learning. Despite their success, such methods also have\nvarious limitations in obtaining their invariant subgraphs. In this paper, we\nprovide in-depth analyses of the drawbacks of existing works and propose\ncorresponding principles of our invariant subgraph extraction: 1) the sparsity,\nto filter out the variant features, 2) the softness, for a broader solution\nspace, and 3) the differentiability, for a soundly end-to-end optimization. To\nmeet these principles in one shot, we leverage the Optimal Transport (OT)\ntheory and propose a novel graph attention mechanism called Graph Sinkhorn\nAttention (GSINA). This novel approach serves as a powerful regularization\nmethod for GIL tasks. By GSINA, we are able to obtain meaningful,\ndifferentiable invariant subgraphs with controllable sparsity and softness.\nMoreover, GSINA is a general graph learning framework that could handle GIL\ntasks of multiple data grain levels. Extensive experiments on both synthetic\nand real-world datasets validate the superiority of our GSINA, which\noutperforms the state-of-the-art GIL methods by large margins on both\ngraph-level tasks and node-level tasks. Our code is publicly available at\n\\url{https://github.com/dingfangyu/GSINA}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07191v1",
    "published_date": "2024-02-11 12:57:16 UTC",
    "updated_date": "2024-02-11 12:57:16 UTC"
  },
  {
    "arxiv_id": "2402.07183v1",
    "title": "A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense",
    "authors": [
      "Ryota Iijima",
      "Sayaka Shiota",
      "Hitoshi Kiya"
    ],
    "abstract": "Deep neural networks (DNNs) are well known to be vulnerable to adversarial\nexamples (AEs). In previous studies, the use of models encrypted with a secret\nkey was demonstrated to be robust against white-box attacks, but not against\nblack-box ones. In this paper, we propose a novel method using the vision\ntransformer (ViT) that is a random ensemble of encrypted models for enhancing\nrobustness against both white-box and black-box attacks. In addition, a\nbenchmark attack method, called AutoAttack, is applied to models to test\nadversarial robustness objectively. In experiments, the method was demonstrated\nto be robust against not only white-box attacks but also black-box ones in an\nimage classification task on the CIFAR-10 and ImageNet datasets. The method was\nalso compared with the state-of-the-art in a standardized benchmark for\nadversarial robustness, RobustBench, and it was verified to outperform\nconventional defenses in terms of clean accuracy and robust accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.07183v1",
    "published_date": "2024-02-11 12:35:28 UTC",
    "updated_date": "2024-02-11 12:35:28 UTC"
  },
  {
    "arxiv_id": "2402.07180v2",
    "title": "MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization",
    "authors": [
      "Jingwei Zuo",
      "George Arvanitakis",
      "Mthandazo Ndhlovu",
      "Hakim Hacid"
    ],
    "abstract": "Human activity recognition (HAR) is a well-established field, significantly\nadvanced by modern machine learning (ML) techniques. While companies have\nsuccessfully integrated HAR into consumer products, they typically rely on a\npredefined activity set, which limits personalizations at the user level (edge\ndevices). Despite advancements in Incremental Learning for updating models with\nnew data, this often occurs on the Cloud, necessitating regular data transfers\nbetween cloud and edge devices, thus leading to data privacy issues. In this\npaper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the\nCloud to the Edge. MAGNETO allows incremental human activity learning directly\non the Edge devices, without any data exchange with the Cloud. This enables\nstrong privacy guarantees, low processing latency, and a high degree of\npersonalization for users. In particular, we demonstrate MAGNETO in an Android\ndevice, validating the whole pipeline from data collection to result\nvisualization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by EDBT 2024 (demo track)",
    "pdf_url": "http://arxiv.org/pdf/2402.07180v2",
    "published_date": "2024-02-11 12:29:16 UTC",
    "updated_date": "2024-02-14 19:59:13 UTC"
  },
  {
    "arxiv_id": "2402.07174v1",
    "title": "EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches",
    "authors": [
      "Pengcheng An",
      "Jiawen Zhu",
      "Zibo Zhang",
      "Yifei Yin",
      "Qingyuan Ma",
      "Che Yan",
      "Linghao Du",
      "Jian Zhao"
    ],
    "abstract": "Voice messages, by nature, prevent users from gauging the emotional tone\nwithout fully diving into the audio content. This hinders the shared emotional\nexperience at the pre-retrieval stage. Research scarcely explored \"Emotional\nTeasers\"-pre-retrieval cues offering a glimpse into an awaiting message's\nemotional tone without disclosing its content. We introduce EmoWear, a\nsmartwatch voice messaging system enabling users to apply 30 animation teasers\non message bubbles to reflect emotions. EmoWear eases senders' choice by\nprioritizing emotions based on semantic and acoustic processing. EmoWear was\nevaluated in comparison with a mirroring system using color-coded message\nbubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced\nemotional communication experience in both receiving and sending messages. The\nanimated teasers were considered intuitive and valued for diverse expressions.\nDesirable interaction qualities and practical implications are distilled for\nfuture design. We thereby contribute both a novel system and empirical\nknowledge concerning emotional teasers for voice messaging.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "To appear at ACM CHI '24",
    "pdf_url": "http://arxiv.org/pdf/2402.07174v1",
    "published_date": "2024-02-11 12:03:01 UTC",
    "updated_date": "2024-02-11 12:03:01 UTC"
  },
  {
    "arxiv_id": "2402.07167v1",
    "title": "Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy",
    "authors": [
      "Zehao Dong",
      "Yixin Chen",
      "Hiram Gay",
      "Yao Hao",
      "Geoffrey D. Hugo",
      "Pamela Samson",
      "Tianyu Zhao"
    ],
    "abstract": "Treatment planning is currently a patient specific, time-consuming, and\nresource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction\nplays a critical role in automating this process. The geometric relationship\nbetween DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target\nvolume (PTV) has been well established. This study explores the potential of\ndeep learning models for predicting DVHs using images and subsequent human\nintervention facilitated by a large-language model (LLM) to enhance the\nplanning quality. We propose a pipeline to convert unstructured images to a\nstructured graph consisting of image-patch nodes and dose nodes. A novel Dose\nGraph Neural Network (DoseGNN) model is developed for predicting DVHs from the\nstructured graph. The proposed DoseGNN is enhanced with the LLM to encode\nmassive knowledge from prescriptions and interactive instructions from\nclinicians. In this study, we introduced an online human-AI collaboration\n(OHAC) system as a practical implementation of the concept proposed for the\nautomation of intensity-modulated radiotherapy (IMRT) planning. In comparison\nto the widely-employed DL models used in radiotherapy, DoseGNN achieved mean\nsquare errors that were 80$\\%$, 76$\\%$ and 41.0$\\%$ of those predicted by Swin\nU-Net Transformer, 3D U-Net CNN and vanilla MLP, respectively. Moreover, the\nLLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans\nthrough interaction with clinicians using natural language.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07167v1",
    "published_date": "2024-02-11 11:24:09 UTC",
    "updated_date": "2024-02-11 11:24:09 UTC"
  },
  {
    "arxiv_id": "2402.07166v2",
    "title": "Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias",
    "authors": [
      "Arifa Khan",
      "P. Saravanan",
      "S. K Venkatesan"
    ],
    "abstract": "We provide a birds eye view of the rapid developments in AI and Deep Learning\nthat has led to the path-breaking emergence of AI in Large Language Models. The\naim of this study is to place all these developments in a pragmatic broader\nhistorical social perspective without any exaggerations while at the same time\nwithout any pessimism that created the AI winter in the 1970s to 1990s. We also\nat the same time point out toxicity, bias, memorization, sycophancy, logical\ninconsistencies, hallucinations that exist just as a warning to the overly\noptimistic. We note here that just as this emergence of AI seems to occur at a\nthreshold point in the number of neural connections or weights, it has also\nbeen observed that human brain and especially the cortex region is nothing\nspecial or extraordinary but simply a case of scaled-up version of the primate\nbrain and that even the human intelligence seems like an emergent phenomena of\nscale.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07166v2",
    "published_date": "2024-02-11 11:23:28 UTC",
    "updated_date": "2024-05-17 07:12:12 UTC"
  },
  {
    "arxiv_id": "2402.07157v2",
    "title": "Natural Language Reinforcement Learning",
    "authors": [
      "Xidong Feng",
      "Ziyu Wan",
      "Mengyue Yang",
      "Ziyan Wang",
      "Girish A. Koushik",
      "Yali Du",
      "Ying Wen",
      "Jun Wang"
    ],
    "abstract": "Reinforcement Learning (RL) has shown remarkable abilities in learning\npolicies for decision-making tasks. However, RL is often hindered by issues\nsuch as low sample efficiency, lack of interpretability, and sparse supervision\nsignals. To tackle these limitations, we take inspiration from the human\nlearning process and introduce Natural Language Reinforcement Learning (NLRL),\nwhich innovatively combines RL principles with natural language representation.\nSpecifically, NLRL redefines RL concepts like task objectives, policy, value\nfunction, Bellman equation, and policy iteration in natural language space. We\npresent how NLRL can be practically implemented with the latest advancements in\nlarge language models (LLMs) like GPT-4. Initial experiments over tabular MDPs\ndemonstrate the effectiveness, efficiency, and also interpretability of the\nNLRL framework.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in Progress",
    "pdf_url": "http://arxiv.org/pdf/2402.07157v2",
    "published_date": "2024-02-11 11:03:04 UTC",
    "updated_date": "2024-02-14 19:59:05 UTC"
  },
  {
    "arxiv_id": "2402.07153v2",
    "title": "Error Estimation for Physics-informed Neural Networks Approximating Semilinear Wave Equations",
    "authors": [
      "Beatrice Lorenz",
      "Aras Bacho",
      "Gitta Kutyniok"
    ],
    "abstract": "This paper provides rigorous error bounds for physics-informed neural\nnetworks approximating the semilinear wave equation. We provide bounds for the\ngeneralization and training error in terms of the width of the network's layers\nand the number of training points for a tanh neural network with two hidden\nlayers. Our main result is a bound of the total error in the\n$H^1([0,T];L^2(\\Omega))$-norm in terms of the training error and the number of\ntraining points, which can be made arbitrarily small under some assumptions. We\nillustrate our theoretical bounds with numerical experiments.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA",
      "35L05, 68T07, 65M15, 35G50, 35A35"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07153v2",
    "published_date": "2024-02-11 10:50:20 UTC",
    "updated_date": "2024-03-06 00:26:02 UTC"
  },
  {
    "arxiv_id": "2402.07152v1",
    "title": "Explainable Global Wildfire Prediction Models using Graph Neural Networks",
    "authors": [
      "Dayou Chen",
      "Sibo Cheng",
      "Jinwei Hu",
      "Matthew Kasoar",
      "Rossella Arcucci"
    ],
    "abstract": "Wildfire prediction has become increasingly crucial due to the escalating\nimpacts of climate change. Traditional CNN-based wildfire prediction models\nstruggle with handling missing oceanic data and addressing the long-range\ndependencies across distant regions in meteorological data. In this paper, we\nintroduce an innovative Graph Neural Network (GNN)-based model for global\nwildfire prediction. We propose a hybrid model that combines the spatial\nprowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long\nShort-Term Memory (LSTM) networks. Our approach uniquely transforms global\nclimate and wildfire data into a graph representation, addressing challenges\nsuch as null oceanic data locations and long-range dependencies inherent in\ntraditional models. Benchmarking against established architectures using an\nunseen ensemble of JULES-INFERNO simulations, our model demonstrates superior\npredictive accuracy. Furthermore, we emphasise the model's explainability,\nunveiling potential wildfire correlation clusters through community detection\nand elucidating feature importance via Integrated Gradient analysis. Our\nfindings not only advance the methodological domain of wildfire prediction but\nalso underscore the importance of model transparency, offering valuable\ninsights for stakeholders in wildfire management.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07152v1",
    "published_date": "2024-02-11 10:44:41 UTC",
    "updated_date": "2024-02-11 10:44:41 UTC"
  },
  {
    "arxiv_id": "2402.13264v1",
    "title": "KGroot: Enhancing Root Cause Analysis through Knowledge Graphs and Graph Convolutional Neural Networks",
    "authors": [
      "Tingting Wang",
      "Guilin Qi",
      "Tianxing Wu"
    ],
    "abstract": "Fault localization is challenging in online micro-service due to the wide\nvariety of monitoring data volume, types, events and complex interdependencies\nin service and components. Faults events in services are propagative and can\ntrigger a cascade of alerts in a short period of time. In the industry, fault\nlocalization is typically conducted manually by experienced personnel. This\nreliance on experience is unreliable and lacks automation. Different modules\npresent information barriers during manual localization, making it difficult to\nquickly align during urgent faults. This inefficiency lags stability assurance\nto minimize fault detection and repair time. Though actionable methods aimed to\nautomatic the process, the accuracy and efficiency are less than satisfactory.\nThe precision of fault localization results is of paramount importance as it\nunderpins engineers trust in the diagnostic conclusions, which are derived from\nmultiple perspectives and offer comprehensive insights. Therefore, a more\nreliable method is required to automatically identify the associative\nrelationships among fault events and propagation path. To achieve this, KGroot\nuses event knowledge and the correlation between events to perform root cause\nreasoning by integrating knowledge graphs and GCNs for RCA. FEKG is built based\non historical data, an online graph is constructed in real-time when a failure\nevent occurs, and the similarity between each knowledge graph and online graph\nis compared using GCNs to pinpoint the fault type through a ranking strategy.\nComprehensive experiments demonstrate KGroot can locate the root cause with\naccuracy of 93.5% top 3 potential causes in second-level. This performance\nmatches the level of real-time fault diagnosis in the industrial environment\nand significantly surpasses state-of-the-art baselines in RCA in terms of\neffectiveness and efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.13264v1",
    "published_date": "2024-02-11 10:30:38 UTC",
    "updated_date": "2024-02-11 10:30:38 UTC"
  },
  {
    "arxiv_id": "2402.07148v2",
    "title": "X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Molecular Design",
    "authors": [
      "Eric L. Buehler",
      "Markus J. Buehler"
    ],
    "abstract": "We report a mixture of expert strategy to create fine-tuned large language\nmodels using a deep layer-wise token-level approach based on low-rank\nadaptation (LoRA). Starting with a set of pre-trained LoRA adapters, our gating\nstrategy uses the hidden states to dynamically mix adapted layers, allowing the\nresulting X-LoRA model to draw upon different capabilities and create\nnever-before-used deep layer-wise combinations to solve tasks. The design is\ninspired by the biological principles of universality and diversity, where\nneural network building blocks are reused in different hierarchical\nmanifestations. Hence, the X-LoRA model can be easily implemented for any\nexisting large language model (LLM) without a need for modifications of the\nunderlying structure. We develop a tailored X-LoRA model that offers scientific\ncapabilities including forward/inverse analysis tasks and enhanced reasoning\ncapability, focused on biomaterial analysis, protein mechanics and design. The\nimpact of this work include access to readily expandable and adaptable models\nwith strong domain knowledge and the capability to integrate across areas of\nknowledge. Featuring experts in biology, mathematics, reasoning, bio-inspired\nmaterials, mechanics and materials, chemistry, protein biophysics, mechanics\nand quantum-mechanics based molecular properties, we conduct a series of\nphysics-focused case studies. We examine knowledge recall, protein mechanics\nforward/inverse tasks, protein design, adversarial agentic modeling including\nontological knowledge graph construction, as well as molecular design. The\nmodel is capable not only of making quantitative predictions of nanomechanical\nproperties of proteins or quantum mechanical molecular properties, but also\nreasons over the results and correctly predicts likely mechanisms that explain\ndistinct molecular behaviors.",
    "categories": [
      "cond-mat.soft",
      "cond-mat.dis-nn",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "cond-mat.soft",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07148v2",
    "published_date": "2024-02-11 10:23:34 UTC",
    "updated_date": "2024-03-30 20:18:54 UTC"
  },
  {
    "arxiv_id": "2402.07140v4",
    "title": "Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?",
    "authors": [
      "Yuyao Ge",
      "Shenghua Liu",
      "Baolong Bi",
      "Yiwei Wang",
      "Lingrui Mei",
      "Wenjie Feng",
      "Lizhe Chen",
      "Xueqi Cheng"
    ],
    "abstract": "Large language models (LLMs) have achieved significant success in reasoning\ntasks, including mathematical reasoning and logical deduction. Among these\nreasoning tasks, graph problems stand out due to their complexity and unique\nstructural characteristics, attracting considerable attention from researchers.\nPrevious studies have explored LLMs' graph reasoning abilities through various\ntechniques, such as different encoding methods for graph structures and the use\nof carefully designed prompts. However, a critical factor has been mostly\noverlooked: the prompt sequential order in which graph descriptions are\npresented to the models. In this study, we present the first comprehensive\nanalysis of how the order of graph descriptions impacts LLM performance.\nSpecifically, we comprehensively evaluate four graph description orders across\nsix graph problems using six mainstream LLMs. The results reveal that: (1)\nordered graph descriptions significantly improve LLMs' comprehension of graph\nstructures; (2) the robustness of LLMs to graph description order varies across\ndifferent tasks; and (3) the impact of graph order on performance is closely\nrelated to the inherent characteristics of tasks. This study provides a\ncritical advancement in the application of LLMs for solving graph-related\nproblems, paving the way for future research to optimize model performance\nthrough strategic graph description ordering.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07140v4",
    "published_date": "2024-02-11 09:46:24 UTC",
    "updated_date": "2024-10-16 14:34:57 UTC"
  },
  {
    "arxiv_id": "2402.07129v1",
    "title": "An attempt to generate new bridge types from latent space of denoising diffusion Implicit model",
    "authors": [
      "Hongjun Zhang"
    ],
    "abstract": "Use denoising diffusion implicit model for bridge-type innovation. The\nprocess of adding noise and denoising to an image can be likened to the process\nof a corpse rotting and a detective restoring the scene of a victim being\nkilled, to help beginners understand. Through an easy-to-understand algebraic\nmethod, derive the function formulas for adding noise and denoising, making it\neasier for beginners to master the mathematical principles of the model. Using\nsymmetric structured image dataset of three-span beam bridge, arch bridge,\ncable-stayed bridge and suspension bridge , based on Python programming\nlanguage, TensorFlow and Keras deep learning platform framework , denoising\ndiffusion implicit model is constructed and trained. From the latent space\nsampling, new bridge types with asymmetric structures can be generated.\nDenoising diffusion implicit model can organically combine different structural\ncomponents on the basis of human original bridge types, and create new bridge\ntypes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.07129v1",
    "published_date": "2024-02-11 08:54:37 UTC",
    "updated_date": "2024-02-11 08:54:37 UTC"
  },
  {
    "arxiv_id": "2402.07127v2",
    "title": "Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation",
    "authors": [
      "Chrisantus Eze",
      "Christopher Crick"
    ],
    "abstract": "Robot learning of manipulation skills is hindered by the scarcity of diverse,\nunbiased datasets. While curated datasets can help, challenges remain in\ngeneralizability and real-world transfer. Meanwhile, large-scale \"in-the-wild\"\nvideo datasets have driven progress in computer vision through self-supervised\ntechniques. Translating this to robotics, recent works have explored learning\nmanipulation skills by passively watching abundant videos sourced online.\nShowing promising results, such video-based learning paradigms provide scalable\nsupervision while reducing dataset bias. This survey reviews foundations such\nas video feature representation learning techniques, object affordance\nunderstanding, 3D hand/body modeling, and large-scale robot resources, as well\nas emerging techniques for acquiring robot manipulation skills from\nuncontrolled video demonstrations. We discuss how learning only from observing\nlarge-scale human videos can enhance generalization and sample efficiency for\nrobotic manipulation. The survey summarizes video-based learning approaches,\nanalyses their benefits over standard datasets, survey metrics, and benchmarks,\nand discusses open challenges and future directions in this nascent domain at\nthe intersection of computer vision, natural language processing, and robot\nlearning.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted at IEEE Access",
    "pdf_url": "http://arxiv.org/pdf/2402.07127v2",
    "published_date": "2024-02-11 08:41:42 UTC",
    "updated_date": "2024-09-18 19:20:42 UTC"
  },
  {
    "arxiv_id": "2402.07118v2",
    "title": "Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation",
    "authors": [
      "Dhruv Srikanth",
      "Jayang Gurung",
      "N Satya Deepika",
      "Vineet Joshi",
      "Lopamudra Giri",
      "Pravin Vaddavalli",
      "Soumya Jana"
    ],
    "abstract": "Blindness and other eye diseases are a global health concern, particularly in\nlow- and middle-income countries like India. In this regard, during the\nCOVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi\nattachment for smartphone-based eye imaging gained in use. However, quality of\nuser-captured image often remained inadequate, requiring clinician vetting and\ndelays. In this backdrop, we propose an AI-based quality assessment system with\ninstant feedback mimicking clinicians' judgments and tested on patient-captured\nimages. Dividing the complex problem hierarchically, here we tackle a\nnontrivial part, and demonstrate a proof of the concept.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "cs.HC",
    "comment": "4 pages, Presented at IEEE EMBC 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.07118v2",
    "published_date": "2024-02-11 07:27:01 UTC",
    "updated_date": "2024-08-07 13:14:00 UTC"
  },
  {
    "arxiv_id": "2402.07107v3",
    "title": "Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning",
    "authors": [
      "Alex Christopher Stutts",
      "Danilo Erricolo",
      "Theja Tulabandhula",
      "Amit Ranjan Trivedi"
    ],
    "abstract": "We present a novel statistical approach to incorporating uncertainty\nawareness in model-free distributional reinforcement learning involving\nquantile regression-based deep Q networks. The proposed algorithm,\n$\\textit{Calibrated Evidential Quantile Regression in Deep Q Networks\n(CEQR-DQN)}$, aims to address key challenges associated with separately\nestimating aleatoric and epistemic uncertainty in stochastic environments. It\ncombines deep evidential learning with quantile calibration based on principles\nof conformal inference to provide explicit, sample-free computations of\n$\\textit{global}$ uncertainty as opposed to $\\textit{local}$ estimates based on\nsimple variance, overcoming limitations of traditional methods in computational\nand statistical efficiency and handling of out-of-distribution (OOD)\nobservations. Tested on a suite of miniaturized Atari games (i.e., MinAtar),\nCEQR-DQN is shown to surpass similar existing frameworks in scores and learning\nspeed. Its ability to rigorously evaluate uncertainty improves exploration\nstrategies and can serve as a blueprint for other algorithms requiring\nuncertainty awareness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07107v3",
    "published_date": "2024-02-11 05:17:56 UTC",
    "updated_date": "2024-06-04 03:04:00 UTC"
  },
  {
    "arxiv_id": "2402.07102v2",
    "title": "An Empirical Study on the Power of Future Prediction in Partially Observable Environments",
    "authors": [
      "Jeongyeol Kwon",
      "Liu Yang",
      "Robert Nowak",
      "Josiah Hanna"
    ],
    "abstract": "Learning good representations of historical contexts is one of the core\nchallenges of reinforcement learning (RL) in partially observable environments.\nWhile self-predictive auxiliary tasks have been shown to improve performance in\nfully observed settings, their role in partial observability remains\nunderexplored. In this empirical study, we examine the effectiveness of\nself-predictive representation learning via future prediction, i.e., predicting\nnext-step observations as an auxiliary task for learning history\nrepresentations, especially in environments with long-term dependencies. We\ntest the hypothesis that future prediction alone can produce representations\nthat enable strong RL performance. To evaluate this, we introduce\n$\\texttt{DRL}^2$, an approach that explicitly decouples representation learning\nfrom reinforcement learning, and compare this approach to end-to-end training\nacross multiple benchmarks requiring long-term memory. Our findings provide\nevidence that this hypothesis holds across different network architectures,\nreinforcing the idea that future prediction performance serves as a reliable\nindicator of representation quality and contributes to improved RL performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07102v2",
    "published_date": "2024-02-11 04:53:40 UTC",
    "updated_date": "2025-03-08 04:14:42 UTC"
  },
  {
    "arxiv_id": "2402.07098v1",
    "title": "Improving Pallet Detection Using Synthetic Data",
    "authors": [
      "Henry Gann",
      "Josiah Bull",
      "Trevor Gee",
      "Mahla Nejati"
    ],
    "abstract": "The use of synthetic data in machine learning saves a significant amount of\ntime when implementing an effective object detector. However, there is limited\nresearch in this domain. This study aims to improve upon previously applied\nimplementations in the task of instance segmentation of pallets in a warehouse\nenvironment. This study proposes using synthetically generated\ndomain-randomised data as well as data generated through Unity to achieve this.\nThis study achieved performance improvements on the stacked and racked pallet\ncategories by 69% and 50% mAP50, respectively when being evaluated on real\ndata. Additionally, it was found that there was a considerable impact on the\nperformance of a model when it was evaluated against images in a darker\nenvironment, dropping as low as 3% mAP50 when being evaluated on images with an\n80% brightness reduction. This study also created a two-stage detector that\nused YOLOv8 and SAM, but this proved to have unstable performance. The use of\ndomain-randomised data proved to have negligible performance improvements when\ncompared to the Unity-generated data.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Australasian Conference on Robotics and Automation (ACRA 2023)",
    "pdf_url": "http://arxiv.org/pdf/2402.07098v1",
    "published_date": "2024-02-11 03:54:44 UTC",
    "updated_date": "2024-02-11 03:54:44 UTC"
  },
  {
    "arxiv_id": "2402.07087v3",
    "title": "Self-Correcting Self-Consuming Loops for Generative Model Training",
    "authors": [
      "Nate Gillman",
      "Michael Freeman",
      "Daksh Aggarwal",
      "Chia-Hong Hsu",
      "Calvin Luo",
      "Yonglong Tian",
      "Chen Sun"
    ],
    "abstract": "As synthetic data becomes higher quality and proliferates on the internet,\nmachine learning models are increasingly trained on a mix of human- and\nmachine-generated data. Despite the successful stories of using synthetic data\nfor representation learning, using synthetic data for generative model training\ncreates \"self-consuming loops\" which may lead to training instability or even\ncollapse, unless certain conditions are met. Our paper aims to stabilize\nself-consuming generative model training. Our theoretical results demonstrate\nthat by introducing an idealized correction function, which maps a data point\nto be more likely under the true data distribution, self-consuming loops can be\nmade exponentially more stable. We then propose self-correction functions,\nwhich rely on expert knowledge (e.g. the laws of physics programmed in a\nsimulator), and aim to approximate the idealized corrector automatically and at\nscale. We empirically validate the effectiveness of self-correcting\nself-consuming loops on the challenging human motion synthesis task, and\nobserve that it successfully avoids model collapse, even when the ratio of\nsynthetic data to real data is as high as 100%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Camera ready version (ICML 2024). Code at\n  https://nategillman.com/sc-sc.html",
    "pdf_url": "http://arxiv.org/pdf/2402.07087v3",
    "published_date": "2024-02-11 02:34:42 UTC",
    "updated_date": "2024-06-10 14:22:45 UTC"
  },
  {
    "arxiv_id": "2402.07076v2",
    "title": "Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training",
    "authors": [
      "Haonan Chen",
      "Zhicheng Dou",
      "Xuetong Hao",
      "Yunhao Tao",
      "Shiren Song",
      "Zhenli Sheng"
    ],
    "abstract": "Cloud solutions have gained significant popularity in the technology industry\nas they offer a combination of services and tools to tackle specific problems.\nHowever, despite their widespread use, the task of identifying appropriate\ncompany customers for a specific target solution to the sales team of a\nsolution provider remains a complex business problem that existing matching\nsystems have yet to adequately address. In this work, we study the B2B solution\nmatching problem and identify two main challenges of this scenario: (1) the\nmodeling of complex multi-field features and (2) the limited, incomplete, and\nsparse transaction data. To tackle these challenges, we propose a framework\nCAMA, which is built with a hierarchical multi-field matching structure as its\nbackbone and supplemented by three data augmentation strategies and a\ncontrastive pre-training objective to compensate for the imperfections in the\navailable data. Through extensive experiments on a real-world dataset, we\ndemonstrate that CAMA outperforms several strong baseline matching models\nsignificantly. Furthermore, we have deployed our matching framework on a system\nof Huawei Cloud. Our observations indicate an improvement of about 30% compared\nto the previous online model in terms of Conversion Rate (CVR), which\ndemonstrates its great business value.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "KDD 2024, ADS Track",
    "pdf_url": "http://arxiv.org/pdf/2402.07076v2",
    "published_date": "2024-02-11 01:03:41 UTC",
    "updated_date": "2024-06-07 02:46:44 UTC"
  },
  {
    "arxiv_id": "2402.18590v3",
    "title": "Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review",
    "authors": [
      "Arpita Vats",
      "Vinija Jain",
      "Rahul Raja",
      "Aman Chadha"
    ],
    "abstract": "The paper underscores the significance of Large Language Models (LLMs) in\nreshaping recommender systems, attributing their value to unique reasoning\nabilities absent in traditional recommenders. Unlike conventional systems\nlacking direct user interaction data, LLMs exhibit exceptional proficiency in\nrecommending items, showcasing their adeptness in comprehending intricacies of\nlanguage. This marks a fundamental paradigm shift in the realm of\nrecommendations. Amidst the dynamic research landscape, researchers actively\nharness the language comprehension and generation capabilities of LLMs to\nredefine the foundations of recommendation tasks. The investigation thoroughly\nexplores the inherent strengths of LLMs within recommendation frameworks,\nencompassing nuanced contextual comprehension, seamless transitions across\ndiverse domains, adoption of unified approaches, holistic learning strategies\nleveraging shared data reservoirs, transparent decision-making, and iterative\nimprovements. Despite their transformative potential, challenges persist,\nincluding sensitivity to input prompts, occasional misinterpretations, and\nunforeseen recommendations, necessitating continuous refinement and evolution\nin LLM-driven recommender systems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.18590v3",
    "published_date": "2024-02-11 00:24:17 UTC",
    "updated_date": "2024-03-19 07:56:40 UTC"
  },
  {
    "arxiv_id": "2402.07069v1",
    "title": "Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine",
    "authors": [
      "Shayan Meshkat Alsadat",
      "Jean-Raphael Gaglione",
      "Daniel Neider",
      "Ufuk Topcu",
      "Zhe Xu"
    ],
    "abstract": "We present LARL-RM (Large language model-generated Automaton for\nReinforcement Learning with Reward Machine) algorithm in order to encode\nhigh-level knowledge into reinforcement learning using automaton to expedite\nthe reinforcement learning. Our method uses Large Language Models (LLM) to\nobtain high-level domain-specific knowledge using prompt engineering instead of\nproviding the reinforcement learning algorithm directly with the high-level\nknowledge which requires an expert to encode the automaton. We use\nchain-of-thought and few-shot methods for prompt engineering and demonstrate\nthat our method works using these approaches. Additionally, LARL-RM allows for\nfully closed-loop reinforcement learning without the need for an expert to\nguide and supervise the learning since LARL-RM can use the LLM directly to\ngenerate the required high-level knowledge for the task at hand. We also show\nthe theoretical guarantee of our algorithm to converge to an optimal policy. We\ndemonstrate that LARL-RM speeds up the convergence by 30% by implementing our\nmethod in two case studies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07069v1",
    "published_date": "2024-02-11 00:00:05 UTC",
    "updated_date": "2024-02-11 00:00:05 UTC"
  }
]
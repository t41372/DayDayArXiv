{
  "date": "2025-03-08",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-08 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 75 篇论文，主要聚焦 AI 模型优化、多模态学习、医疗应用和机器人决策等领域，强调 LLM 的对齐、推理和高效训练；令人印象深刻的文章包括 AI 监管框架的提出（如 Di Kevin Gao 等作者的“AI Pentad”）和医疗诊断创新（如 Alan Karthikesalingam 参与的对话式 AI），这些工作展示了 AI 在实际应用中的潜力。\n\n下面，我将挑选并讨论其中最具话题度和影响力的论文，先从 AI 和 LLM 相关的高影响力文章入手，再聊医疗与机器人领域的关键进展，其他次要论文将快速掠过。每个条目包括论文标题（中文 + 英文）和核心贡献。\n\n### AI 和 LLM 优化：焦点在于模型对齐、推理和高效训练\n- **AI Pentad, the CHARME²D Model, and an Assessment of Current-State AI Regulation**（中文：AI 五元组、CHARME²D 模型及当前 AI 监管评估；英文：The AI Pentad, the CHARME²D Model, and an Assessment of Current-State AI Regulation）  \n  作者包括 Di Kevin Gao 和 Jiming Wu。该文提出 AI 五元组（包括人类、算法、数据、计算和能源）框架，并评估欧盟、中国等地的 AI 监管，强调统一方法的重要性。主要贡献：提供了一个综合模型来评估 AI 监管的优缺点，促进未来立法，填补了现有价值导向方法的不足。\n\n- **Dynamic Evaluation Framework for Personalized and Trustworthy Agents: A Multi-Session Approach to Preference Adaptability**（中文：个性化可信代理的动态评估框架：多会话偏好适应方法；英文：Dynamic Evaluation Framework for Personalized and Trustworthy Agents: A Multi-Session Approach to Preference Adaptability）  \n  作者 Chirag Shah 等。该文提出一个新框架，使用 LLM 模拟用户偏好并动态评估代理推荐。主要发现：框架提升了代理的适应性和可信度，在多会话互动中显著改善个性化推荐，适用于生成式 AI 应用。\n\n- **Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning**（中文：研究强化学习中 Actor 和 Critic 表示的相互作用；英文：Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning）  \n  作者 Pablo Samuel Castro 和 David Abel 等。该文探索 on-policy 算法中 Actor 和 Critic 的分离表示，发现 Actor 更关注行动相关信息，而 Critic 专注于价值和动态信息。主要贡献：通过实证研究证明分离表示提升样本效率和探索能力，提供代码和数据支持。\n\n- **Merge then Realign: Simple and Effective Modality-Incremental Continual Learning for Multimodal LLMs**（中文：先合并再对齐：多模态 LLM 的简单有效模态增量持续学习；英文：Merge then Realign: Simple and Effective Modality-Incremental Continual Learning for Multimodal LLMs）  \n  该文提出 MERA 范式，避免了传统方法的重训练开销。主要发现：在扩展到四种模态时，实现了近乎无损的性能（99.84% 后向增益），显著提升多模态学习效率。\n\n- **States of LLM-generated Texts and Phase Transitions between them**（中文：LLM 生成文本的状态及其相变；英文：States of LLM-generated Texts and Phase Transitions between them）  \n  作者 Nikolay Mikhaylovskiy。该文通过分析文本自相关性，将 LLM 输出分类为固体、临界态或气体状态。主要贡献：揭示温度参数对文本生成的影响，提供了一个物理学视角的解释框架。\n\n- **Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models**（中文：利用机制解释性构建针对大型语言模型的对抗攻击；英文：Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models）  \n  该文使用机制解释性技术创建高效对抗攻击，实现 80-95% 的成功率。主要发现：攻击方法显著降低计算成本，并展示了机制解释性的实际应用价值。\n\n- **Critical Foreign Policy Decisions (CFPD)-Benchmark: Measuring Diplomatic Preferences in Large Language Models**（中文：关键外交决策基准：测量大型语言模型的外交偏好；英文：Critical Foreign Policy Decisions (CFPD)-Benchmark: Measuring Diplomatic Preferences in Large Language Models）  \n  作者 Benjamin Jensen 等。该文引入新基准评估 LLM 在国际关系中的偏好，如对中美关系的不同建议。主要贡献：揭示模型的国别偏见，并提供见解以改进 LLM 在高风险环境中的部署。\n\n这些 AI 相关论文突出了 LLM 的优化和对齐问题，强调了从监管到攻击的全面挑战，具有高话题度。\n\n### 医疗 AI 应用：创新诊断和预测模型\n- **Towards Conversational AI for Disease Management**（中文：面向疾病管理的对话式 AI；英文：Towards Conversational AI for Disease Management）  \n  作者 Alan Karthikesalingam 和 Vivek Natarajan 等。该文扩展 AMIE 系统，实现对话式疾病管理和用药推理。主要发现：在多数据集测试中，AI 模型在昏迷和谵妄预测上优于医生，强调 AI 在临床决策中的潜力。\n\n- **GEM: Empowering MLLM for Grounded ECG Understanding with Time Series and Images**（中文：GEM：使用时序和图像增强多模态 LLM 的心电图理解；英文：GEM: Empowering MLLM for Grounded ECG Understanding with Time Series and Images）  \n  该文提出 GEM 框架，用于心电图的多模态分析。主要贡献：通过对比学习提升 ECG 诊断准确性，显著改善解释性和鲁棒性。\n\n- **MANDARIN: Mixture-of-Experts Framework for Dynamic Delirium and Coma Prediction in ICU Patients**（中文：MANDARIN：ICU 患者动态谵妄和昏迷预测的混合专家框架；英文：MANDARIN: Mixture-of-Experts Framework for Dynamic Delirium and Coma Prediction in ICU Patients）  \n  作者 Parisa Rashidi 等。该文使用混合专家网络预测 ICU 患者脑状态，主要发现：模型在外部验证中准确率高达 87.3%，提供实时监测工具。\n\n这些医疗论文展示了 AI 在诊断中的实际价值，特别在资源有限的环境中。\n\n### 机器人和决策系统：提升自主性和安全性\n- **Object-Centric World Model for Language-Guided Manipulation**（中文：面向语言引导操作的目标中心世界模型；英文：Object-Centric World Model for Language-Guided Manipulation）  \n  该文提出基于对象表示的世界模型，支持语言引导的机器人操作。主要发现：模型在 visuo-linguo-motor 任务中效率更高，计算资源利用率优于扩散模型。\n\n- **T-CBF: Traversability-based Control Barrier Function to Navigate Vertically Challenging Terrain**（中文：基于可穿越性的控制屏障函数用于复杂地形导航；英文：T-CBF: Traversability-based Control Barrier Function to Navigate Vertically Challenging Terrain）  \n  作者 Manas Gupta 等。该文引入 T-CBF 框架，提升机器人在垂直地形中的安全性。主要贡献：在模拟和实物实验中，减少碰撞并提高导航成功率。\n\n### 其他快速掠过\n其他论文如第13篇（Applied Machine Learning Methods with Long-Short Term Memory Based Recurrent Neural Networks for Multivariate Temperature Prediction，中文：基于 LSTM 的多变量温度预测方法）聚焦基础预测，但影响较小，仅验证了神经网络在时间序列中的应用；第43篇（Exploring the usage of Probabilistic Neural Networks for Ionospheric electron density estimation，中文：探索概率神经网络在电离层电子密度估计中的应用）等则在特定领域有贡献，但非主流话题，故不展开。\n\n总之，今天的 arXiv 更新突显了 AI 领域的创新潜力，AI 优化和医疗应用尤为值得关注。未来几天，我们将继续追踪这些进展！",
  "papers": [
    {
      "arxiv_id": "2503.10662v1",
      "title": "Evaluation of the Automated Labeling Method for Taxonomic Nomenclature Through Prompt-Optimized Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Keito Inoshita",
        "Kota Nojiri",
        "Haruto Sugeno",
        "Takumi Taga"
      ],
      "abstract": "Scientific names of organisms consist of a genus name and a species epithet,\nwith the latter often reflecting aspects such as morphology, ecology,\ndistribution, and cultural background. Traditionally, researchers have manually\nlabeled species names by carefully examining taxonomic descriptions, a process\nthat demands substantial time and effort when dealing with large datasets. This\nstudy evaluates the feasibility of automatic species name labeling using large\nlanguage model (LLM) by leveraging their text classification and semantic\nextraction capabilities. Using the spider name dataset compiled by Mammola et\nal., we compared LLM-based labeling results-enhanced through prompt\nengineering-with human annotations. The results indicate that LLM-based\nclassification achieved high accuracy in Morphology, Geography, and People\ncategories. However, classification accuracy was lower in Ecology & Behavior\nand Modern & Past Culture, revealing challenges in interpreting animal behavior\nand cultural contexts. Future research will focus on improving accuracy through\noptimized few-shot learning and retrieval-augmented generation techniques,\nwhile also expanding the applicability of LLM-based labeling to diverse\nbiological taxa.",
      "tldr_zh": "本研究评估了使用提示优化的LLM（大型语言模型）自动标记生物物种名称的可行性，通过其文本分类和语义提取能力处理蜘蛛名称数据集，并与人工标注进行比较。结果显示，LLM在Morphology、Geography和People类别中取得了高准确率，但在Ecology & Behavior和Modern & Past Culture类别中准确率较低，揭示了在解读动物行为和文化背景方面的挑战。未来工作将通过few-shot learning和retrieval-augmented generation技术优化模型，并扩展其应用到更多生物类群。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper will be submitted to IEEE IAICT",
      "pdf_url": "http://arxiv.org/pdf/2503.10662v1",
      "published_date": "2025-03-08 23:11:43 UTC",
      "updated_date": "2025-03-08 23:11:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:26:18.488238"
    },
    {
      "arxiv_id": "2503.06353v1",
      "title": "The AI Pentad, the CHARME$^{2}$D Model, and an Assessment of Current-State AI Regulation",
      "title_zh": "翻译失败",
      "authors": [
        "Di Kevin Gao",
        "Sudip Mittal",
        "Jiming Wu",
        "Hongwei Du",
        "Jingdao Chen",
        "Shahram Rahimi"
      ],
      "abstract": "Artificial Intelligence (AI) has made remarkable progress in the past few\nyears with AI-enabled applications beginning to permeate every aspect of our\nsociety. Despite the widespread consensus on the need to regulate AI, there\nremains a lack of a unified approach to framing, developing, and assessing AI\nregulations. Many of the existing methods take a value-based approach, for\nexample, accountability, fairness, free from bias, transparency, and trust.\nHowever, these methods often face challenges at the outset due to disagreements\nin academia over the subjective nature of these definitions. This paper aims to\nestablish a unifying model for AI regulation from the perspective of core AI\ncomponents. We first introduce the AI Pentad, which comprises the five\nessential components of AI: humans and organizations, algorithms, data,\ncomputing, and energy. We then review AI regulatory enablers, including AI\nregistration and disclosure, AI monitoring, and AI enforcement mechanisms.\nSubsequently, we present the CHARME$^{2}$D Model to explore further the\nrelationship between the AI Pentad and AI regulatory enablers. Finally, we\napply the CHARME$^{2}$D model to assess AI regulatory efforts in the European\nUnion (EU), China, the United Arab Emirates (UAE), the United Kingdom (UK), and\nthe United States (US), highlighting their strengths, weaknesses, and gaps.\nThis comparative evaluation offers insights for future legislative work in the\nAI domain.",
      "tldr_zh": "这篇论文针对人工智能（AI）监管的碎片化问题，提出AI Pentad框架，该框架包括五个核心AI组件：humans and organizations, algorithms, data, computing, and energy，以提供一个统一的视角。作者审阅了AI监管促进因素，如AI注册和披露、监控及执法机制，并引入CHARME²D Model来探讨这些组件与监管机制的关联关系。最终，通过应用该模型评估欧盟、中国、阿联酋、英国和美国的AI监管努力，该研究突出了各地区的优势、弱点和差距，为未来的AI立法工作提供了宝贵见解。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06353v1",
      "published_date": "2025-03-08 22:58:41 UTC",
      "updated_date": "2025-03-08 22:58:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:26:31.237588"
    },
    {
      "arxiv_id": "2504.06277v1",
      "title": "Dynamic Evaluation Framework for Personalized and Trustworthy Agents: A Multi-Session Approach to Preference Adaptability",
      "title_zh": "翻译失败",
      "authors": [
        "Chirag Shah",
        "Hideo Joho",
        "Kirandeep Kaur",
        "Preetam Prabhu Srikar Dammu"
      ],
      "abstract": "Recent advancements in generative AI have significantly increased interest in\npersonalized agents. With increased personalization, there is also a greater\nneed for being able to trust decision-making and action taking capabilities of\nthese agents. However, the evaluation methods for these agents remain outdated\nand inadequate, often failing to capture the dynamic and evolving nature of\nuser interactions. In this conceptual article, we argue for a paradigm shift in\nevaluating personalized and adaptive agents. We propose a comprehensive novel\nframework that models user personas with unique attributes and preferences. In\nthis framework, agents interact with these simulated users through structured\ninterviews to gather their preferences and offer customized recommendations.\nThese recommendations are then assessed dynamically using simulations driven by\nLarge Language Models (LLMs), enabling an adaptive and iterative evaluation\nprocess. Our flexible framework is designed to support a variety of agents and\napplications, ensuring a comprehensive and versatile evaluation of\nrecommendation strategies that focus on proactive, personalized, and\ntrustworthy aspects.",
      "tldr_zh": "该论文指出，现有的个性化代理评估方法无法适应用户互动的动态演变，亟需范式转变。研究提出一个全面的动态评估框架，通过模拟用户角色（personas）及其独特属性和偏好，让代理通过结构化访谈收集偏好并提供定制推荐。框架利用 Large Language Models (LLMs) 驱动的多会话模拟进行迭代评估，确保代理的决策和行动更具适应性、主动性和可信任性。该框架灵活支持多种代理和应用，提升了个性化推荐策略的全面评估。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.06277v1",
      "published_date": "2025-03-08 22:50:26 UTC",
      "updated_date": "2025-03-08 22:50:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:26:41.379468"
    },
    {
      "arxiv_id": "2503.06343v2",
      "title": "Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning",
      "title_zh": "研究演员和批评家表示在强化学习中的相互作用",
      "authors": [
        "Samuel Garcin",
        "Trevor McInroe",
        "Pablo Samuel Castro",
        "Prakash Panangaden",
        "Christopher G. Lucas",
        "David Abel",
        "Stefano V. Albrecht"
      ],
      "abstract": "Extracting relevant information from a stream of high-dimensional\nobservations is a central challenge for deep reinforcement learning agents.\nActor-critic algorithms add further complexity to this challenge, as it is\noften unclear whether the same information will be relevant to both the actor\nand the critic. To this end, we here explore the principles that underlie\neffective representations for the actor and for the critic in on-policy\nalgorithms. We focus our study on understanding whether the actor and critic\nwill benefit from separate, rather than shared, representations. Our primary\nfinding is that when separated, the representations for the actor and critic\nsystematically specialise in extracting different types of information from the\nenvironment -- the actor's representation tends to focus on action-relevant\ninformation, while the critic's representation specialises in encoding value\nand dynamics information. We conduct a rigourous empirical study to understand\nhow different representation learning approaches affect the actor and critic's\nspecialisations and their downstream performance, in terms of sample efficiency\nand generation capabilities. Finally, we discover that a separated critic plays\nan important role in exploration and data collection during training. Our code,\ntrained models and data are accessible at\nhttps://github.com/francelico/deac-rep.",
      "tldr_zh": "本研究探讨了在强化学习（reinforcement learning）中，actor和critic表示之间的互动，重点分析它们是否应使用分离而非共享的表示。研究发现，当采用分离表示时，actor的表示更专注于行动相关信息，而critic的表示则专攻价值和动态信息，从而提升了整体性能。作者通过实证研究不同表示学习方法，证明分离表示可提高样本效率、生成能力和探索能力，最终有助于更好的数据收集和训练效果。研究成果包括开源代码和模型，详见相关仓库。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at ICLR 2025. 10 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.06343v2",
      "published_date": "2025-03-08 21:29:20 UTC",
      "updated_date": "2025-03-31 14:56:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:26:54.803293"
    },
    {
      "arxiv_id": "2503.07663v1",
      "title": "Merge then Realign: Simple and Effective Modality-Incremental Continual Learning for Multimodal LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Dingkun Zhang",
        "Shuhan Qi",
        "Xinyu Xiao",
        "Kehai Chen",
        "Xuan Wang"
      ],
      "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have enhanced\ntheir versatility as they integrate a growing number of modalities. Considering\nthe heavy cost of training MLLMs, it is necessary to reuse the existing ones\nand further extend them to more modalities through Modality-incremental\nContinual Learning (MCL). However, this often comes with a performance\ndegradation in the previously learned modalities. In this work, we revisit the\nMCL and investigate a more severe issue it faces in contrast to traditional\ncontinual learning, that its degradation comes not only from catastrophic\nforgetting but also from the misalignment between the modality-agnostic and\nmodality-specific components. To address this problem, we propose an elegantly\nsimple MCL paradigm called \"MErge then ReAlign\" (MERA). Our method avoids\nintroducing heavy training overhead or modifying the model architecture, hence\nis easy to deploy and highly reusable in the MLLM community. Extensive\nexperiments demonstrate that, despite the simplicity of MERA, it shows\nimpressive performance, holding up to a 99.84% Backward Relative Gain when\nextending to four modalities, achieving a nearly lossless MCL performance.",
      "tldr_zh": "该研究探讨了 Multimodal LLMs 在模态增量持续学习 (MCL) 中的挑战，特别是在扩展新模态时导致先前模态性能下降的问题，不仅源于灾难性遗忘，还涉及模态无关和模态特定组件的失调。为解决此问题，研究提出了一种简单有效的范式“MErge then ReAlign” (MERA)，通过合并和重新对齐操作避免了额外训练开销和模型架构修改，使其易于部署。实验结果显示，MERA 在扩展到四个模态时实现了高达99.84%的 Backward Relative Gain，几乎实现了无损的 MCL 性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07663v1",
      "published_date": "2025-03-08 20:29:40 UTC",
      "updated_date": "2025-03-08 20:29:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:27:07.453693"
    },
    {
      "arxiv_id": "2503.06330v1",
      "title": "States of LLM-generated Texts and Phase Transitions between them",
      "title_zh": "LLM 生成文本的状态与它们之间的相变",
      "authors": [
        "Nikolay Mikhaylovskiy"
      ],
      "abstract": "It is known for some time that autocorrelations of words in human-written\ntexts decay according to a power law. Recent works have also shown that the\nautocorrelations decay in texts generated by LLMs is qualitatively different\nfrom the literary texts. Solid state physics tie the autocorrelations decay\nlaws to the states of matter. In this work, we empirically demonstrate that,\ndepending on the temperature parameter, LLMs can generate text that can be\nclassified as solid, critical state or gas.",
      "tldr_zh": "本文研究发现，LLM生成的文本中单词的自相关性（autocorrelations）衰减方式与人类文本的power law衰减不同，类似于固态物理学（solid state physics）中的物质状态。作者通过实证分析，展示了根据温度参数（temperature parameter），LLM生成的文本可被分类为solid、critical state或gas，并观察到相变（phase transitions）。这项工作为理解LLM文本特性提供了新视角，并可能应用于文本生成质量的优化。",
      "categories": [
        "cs.CL",
        "cond-mat.stat-mech",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published as a conference paper at MathAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06330v1",
      "published_date": "2025-03-08 20:06:50 UTC",
      "updated_date": "2025-03-08 20:06:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:27:19.789734"
    },
    {
      "arxiv_id": "2503.06323v1",
      "title": "Higher-Order Belief in Incomplete Information MAIDs",
      "title_zh": "翻译失败",
      "authors": [
        "Jack Foxabbott",
        "Rohan Subramani",
        "Francis Rhys Ward"
      ],
      "abstract": "Multi-agent influence diagrams (MAIDs) are probabilistic graphical models\nwhich represent strategic interactions between agents. MAIDs are equivalent to\nextensive form games (EFGs) but have a more compact and informative structure.\nHowever, MAIDs cannot, in general, represent settings of incomplete information\n-- wherein agents have different beliefs about the game being played, and\ndifferent beliefs about each-other's beliefs. In this paper, we introduce\nincomplete information MAIDs (II-MAIDs). We define both infinite and\nfinite-depth II-MAIDs and prove an equivalence relation to EFGs with incomplete\ninformation and no common prior over types. We prove that II-MAIDs inherit\nclassical equilibria concepts via this equivalence, but note that these\nsolution concepts are often unrealistic in the setting with no common prior\nbecause they violate common knowledge of rationality. We define a more\nrealistic solution concept based on recursive best-response. Throughout, we\ndescribe an example with a hypothetical AI agent undergoing evaluation to\nillustrate the applicability of II-MAIDs.",
      "tldr_zh": "该论文引入了不完全信息多智能体影响图（II-MAIDs），以扩展传统 MAIDs 处理代理间战略互动的能力，特别是当代理对游戏和彼此信念持有不同观点时。作者定义了无限深度和有限深度的 II-MAIDs，并证明其等价于不完全信息扩展形式游戏（EFGs），前提是没有共同先验。II-MAIDs 继承了经典均衡概念，但这些概念在无共同先验的场景下往往不切实际，因为它们违反了理性共同知识；为此，论文提出了一种基于递归最佳响应（recursive best-response）的更现实解决方案。通过一个假设的 AI 代理评估示例，展示了 II-MAIDs 的实际适用性。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06323v1",
      "published_date": "2025-03-08 19:35:55 UTC",
      "updated_date": "2025-03-08 19:35:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:27:32.979692"
    },
    {
      "arxiv_id": "2503.06313v1",
      "title": "Advancing Autonomous Vehicle Intelligence: Deep Learning and Multimodal LLM for Traffic Sign Recognition and Robust Lane Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Chandan Kumar Sah",
        "Ankit Kumar Shaw",
        "Xiaoli Lian",
        "Arsalan Shahid Baig",
        "Tuopu Wen",
        "Kun Jiang",
        "Mengmeng Yang",
        "Diange Yang"
      ],
      "abstract": "Autonomous vehicles (AVs) require reliable traffic sign recognition and\nrobust lane detection capabilities to ensure safe navigation in complex and\ndynamic environments. This paper introduces an integrated approach combining\nadvanced deep learning techniques and Multimodal Large Language Models (MLLMs)\nfor comprehensive road perception. For traffic sign recognition, we\nsystematically evaluate ResNet-50, YOLOv8, and RT-DETR, achieving\nstate-of-the-art performance of 99.8% with ResNet-50, 98.0% accuracy with\nYOLOv8, and achieved 96.6% accuracy in RT-DETR despite its higher computational\ncomplexity. For lane detection, we propose a CNN-based segmentation method\nenhanced by polynomial curve fitting, which delivers high accuracy under\nfavorable conditions. Furthermore, we introduce a lightweight, Multimodal,\nLLM-based framework that directly undergoes instruction tuning using small yet\ndiverse datasets, eliminating the need for initial pretraining. This framework\neffectively handles various lane types, complex intersections, and merging\nzones, significantly enhancing lane detection reliability by reasoning under\nadverse conditions. Despite constraints in available training resources, our\nmultimodal approach demonstrates advanced reasoning capabilities, achieving a\nFrame Overall Accuracy (FRM) of 53.87%, a Question Overall Accuracy (QNS) of\n82.83%, lane detection accuracies of 99.6% in clear conditions and 93.0% at\nnight, and robust performance in reasoning about lane invisibility due to rain\n(88.4%) or road degradation (95.6%). The proposed comprehensive framework\nmarkedly enhances AV perception reliability, thus contributing significantly to\nsafer autonomous driving across diverse and challenging road scenarios.",
      "tldr_zh": "本论文提出了一种集成方法，结合深度学习和 Multimodal LLM，提升自动驾驶车辆（AVs）的交通标志识别和鲁棒车道检测能力。具体而言，通过评估 ResNet-50（99.8% 准确率）、YOLOv8（98.0%）和 RT-DETR（96.6%）进行交通标志识别，并采用基于 CNN 的分割方法结合多项式曲线拟合来优化车道检测。研究还引入了一个轻量级 Multimodal LLM 框架，通过指令微调（instruction tuning）处理复杂场景，如夜间（93.0% 准确率）和雨中车道不可见性（88.4%），实现整体 Frame Overall Accuracy (FRM) 为 53.87% 和 Question Overall Accuracy (QNS) 为 82.83%。该框架显著提高了 AV 的感知可靠性，推动更安全的自动驾驶应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06313v1",
      "published_date": "2025-03-08 19:12:36 UTC",
      "updated_date": "2025-03-08 19:12:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:27:44.823266"
    },
    {
      "arxiv_id": "2503.06302v1",
      "title": "Synergizing AI and Digital Twins for Next-Generation Network Optimization, Forecasting, and Security",
      "title_zh": "AI 与数字孪生的协同作用，用于下一代网络优化、预测和安全",
      "authors": [
        "Zifan Zhang",
        "Minghong Fang",
        "Dianwei Chen",
        "Xianfeng Yang",
        "Yuchen Liu"
      ],
      "abstract": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
      "tldr_zh": "该论文探讨了数字网络双胞胎 (DNTs) 与机器学习技术（如联邦学习 (FL) 和强化学习 (RL)）的协同作用，以优化、预测和提升下一代网络（如 6G 网络）的安全性。研究分析了关键挑战，包括网络可靠性、联合数据场景预测以及高风险环境下的安全，并提出整合 DNTs 和 ML 的管道框架来实现网络优化和自动化决策。案例研究显示，该框架在边缘缓存场景中实现超过 80% 的缓存命中率并平衡基站负载，在自动驾驶车辆系统中确保 100% 无碰撞率。通过这些协同创新，论文为智能自适应网络系统提供了宝贵见解。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "Accepted by IEEE Wireless Communications",
      "pdf_url": "http://arxiv.org/pdf/2503.06302v1",
      "published_date": "2025-03-08 18:30:54 UTC",
      "updated_date": "2025-03-08 18:30:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:27:54.623506"
    },
    {
      "arxiv_id": "2503.09620v2",
      "title": "Exploiting Edited Large Language Models as General Scientific Optimizers",
      "title_zh": "利用编辑过的大语言模型作为通用科学优化器",
      "authors": [
        "Qitan Lv",
        "Tianyu Liu",
        "Hong Wang"
      ],
      "abstract": "Large language models (LLMs) have been widely adopted in mathematical\noptimization in scientific scenarios for their extensive knowledge and advanced\nreasoning capabilities. Existing methods mainly focus on utilizing LLMs to\nsolve optimization problems in a prompt-based manner, which takes observational\nfeedback as additional textual descriptions. However, due to LLM's \\textbf{high\nsensitivity to the prompts} and \\textbf{tendency to get lost in lengthy\nprompts}, these methods struggle to effectively utilize the {observational}\nfeedback from each optimization step, which severely hinders the applications\nfor real-world scenarios. To address these challenges, we propose a\nconceptually simple and general {bi-level} optimization method, namely\n\\textbf{G}eneral \\textbf{S}cientific \\textbf{O}ptimizers (GSO). Specifically,\nGSO first utilizes inner-level simulators as experimental platforms to evaluate\nthe current solution and provide observational feedback. Then, LLMs serve as\nknowledgeable and versatile scientists, generating new solutions by refining\npotential errors from the feedback as the outer-level optimization. Finally,\nsimulations together with the expert knowledge in LLMs are jointly updated with\nbi-level interactions via model editing. Extensive experiments show that GSO\nconsistently outperforms existing state-of-the-art methods using \\textit{six}\ndifferent LLM backbones on \\textit{seven} different tasks, demonstrating the\neffectiveness and a wide range of applications.",
      "tldr_zh": "本文研究了如何利用编辑后的Large Language Models (LLMs) 作为通用的科学优化器，以解决现有基于提示的方法在处理观察反馈时因LLMs对提示高度敏感和易于迷失而带来的局限性。GSO（General Scientific Optimizers）提出了一种简单的双层优化框架：内层使用模拟器评估解决方案并提供反馈，外层由LLMs生成改进方案，并通过模型编辑实现系统联合更新。实验结果表明，GSO在七个不同任务中使用六种LLM骨干网，均显著优于现有最先进方法，展示了其有效性和广泛应用潜力。",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09620v2",
      "published_date": "2025-03-08 18:01:11 UTC",
      "updated_date": "2025-03-17 05:40:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:28:06.811957"
    },
    {
      "arxiv_id": "2503.06288v1",
      "title": "Single Domain Generalization with Adversarial Memory",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Yan",
        "Marzi Heidari",
        "Yuhong Guo"
      ],
      "abstract": "Domain Generalization (DG) aims to train models that can generalize to unseen\ntesting domains by leveraging data from multiple training domains. However,\ntraditional DG methods rely on the availability of multiple diverse training\ndomains, limiting their applicability in data-constrained scenarios. Single\nDomain Generalization (SDG) addresses the more realistic and challenging\nsetting by restricting the training data to a single domain distribution. The\nmain challenges in SDG stem from the limited diversity of training data and the\ninaccessibility of unseen testing data distributions. To tackle these\nchallenges, we propose a single domain generalization method that leverages an\nadversarial memory bank to augment training features. Our memory-based feature\naugmentation network maps both training and testing features into an invariant\nsubspace spanned by diverse memory features, implicitly aligning the training\nand testing domains in the projected space. To maintain a diverse and\nrepresentative feature memory bank, we introduce an adversarial feature\ngeneration method that creates features extending beyond the training domain\ndistribution. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance on standard single domain generalization\nbenchmarks.",
      "tldr_zh": "本研究针对Single Domain Generalization (SDG)问题，提出了一种利用Adversarial Memory的方法，以解决训练数据多样性有限和无法访问测试域分布的挑战。该方法通过构建对抗性记忆银行来增强训练特征，将训练和测试特征映射到由多样记忆特征跨越的不变子空间中，从而隐式对齐不同域。此外，引入对抗性特征生成技术来扩展记忆银行的多样性和代表性。实验结果显示，该方法在标准SDG基准上达到了最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06288v1",
      "published_date": "2025-03-08 17:27:42 UTC",
      "updated_date": "2025-03-08 17:27:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:28:17.469246"
    },
    {
      "arxiv_id": "2503.06287v1",
      "title": "Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding",
      "title_zh": "翻译失败",
      "authors": [
        "Seil Kang",
        "Jinyeong Kim",
        "Junhyeok Kim",
        "Seong Jae Hwang"
      ],
      "abstract": "Visual grounding seeks to localize the image region corresponding to a\nfree-form text description. Recently, the strong multimodal capabilities of\nLarge Vision-Language Models (LVLMs) have driven substantial improvements in\nvisual grounding, though they inevitably require fine-tuning and additional\nmodel components to explicitly generate bounding boxes or segmentation masks.\nHowever, we discover that a few attention heads in frozen LVLMs demonstrate\nstrong visual grounding capabilities. We refer to these heads, which\nconsistently capture object locations related to text semantics, as\nlocalization heads. Using localization heads, we introduce a straightforward\nand effective training-free visual grounding framework that utilizes\ntext-to-image attention maps from localization heads to identify the target\nobjects. Surprisingly, only three out of thousands of attention heads are\nsufficient to achieve competitive localization performance compared to existing\nLVLM-based visual grounding methods that require fine-tuning. Our findings\nsuggest that LVLMs can innately ground objects based on a deep comprehension of\nthe text-image relationship, as they implicitly focus on relevant image regions\nto generate informative text outputs. All the source codes will be made\navailable to the public.",
      "tldr_zh": "该研究发现，Large Vision-Language Models (LVLMs) 中的少数 attention heads（称为 localization heads）即可实现高效的视觉定位（visual grounding），无需额外微调或组件。这些 heads 通过捕捉文本语义相关的图像区域，生成 text-to-image attention maps 来识别目标对象。结果显示，仅使用三个 attention heads，就能与现有需微调的 LVLM 方法相比，达到竞争性的定位性能，证明 LVLMs 内在理解文本-图像关系。该框架的源代码将公开。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06287v1",
      "published_date": "2025-03-08 17:24:42 UTC",
      "updated_date": "2025-03-08 17:24:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:28:30.524218"
    },
    {
      "arxiv_id": "2503.06278v1",
      "title": "Applied Machine Learning Methods with Long-Short Term Memory Based Recurrent Neural Networks for Multivariate Temperature Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Bojan Lukić"
      ],
      "abstract": "This paper gives an overview on how to develop a dense and deep neural\nnetwork for making a time series prediction. First, the history and\ncornerstones in Artificial Intelligence and Machine Learning will be presented.\nAfter a short introduction to the theory of Artificial Intelligence and Machine\nLearning, the paper will go deeper into the techniques for conducting a time\nseries prediction with different models of neural networks. For this project,\nPython's development environment Jupyter, extended with the TensorFlow package\nand deep-learning application Keras is used. The system setup and project\nframework are explained in more detail before discussing the time series\nprediction. The main part shows an applied example of time series prediction\nwith weather data. For this work, a deep recurrent neural network with Long\nShort-Term Memory cells is used to conduct the time series prediction. The\nresults and evaluation of the work show that a weather prediction with deep\nneural networks can be successful for a short time period. However, there are\nsome drawbacks and limitations with time series prediction, which will be\ndiscussed towards the end of the paper.",
      "tldr_zh": "本论文概述了使用机器学习方法开发密集和深度神经网络来进行时间序列预测，特别是针对多变量温度预测。作者介绍了人工智能和机器学习的背景，并详细阐述了利用 Python 的 Jupyter、TensorFlow 和 Keras 等工具构建深度循环神经网络模型。重点应用 Long Short-Term Memory (LSTM) 细胞进行天气数据预测，结果显示该方法在短期预测中表现出色，但存在一些局限性，如预测准确性随时间延长而下降。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 16 figures, private research",
      "pdf_url": "http://arxiv.org/pdf/2503.06278v1",
      "published_date": "2025-03-08 16:52:27 UTC",
      "updated_date": "2025-03-08 16:52:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:28:41.471043"
    },
    {
      "arxiv_id": "2503.06269v2",
      "title": "Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models",
      "title_zh": "利用机制解释性构建针对大型语言模型的对抗攻击",
      "authors": [
        "Thomas Winninger",
        "Boussad Addad",
        "Katarzyna Kapusta"
      ],
      "abstract": "Traditional white-box methods for creating adversarial perturbations against\nLLMs typically rely only on gradient computation from the targeted model,\nignoring the internal mechanisms responsible for attack success or failure.\nConversely, interpretability studies that analyze these internal mechanisms\nlack practical applications beyond runtime interventions. We bridge this gap by\nintroducing a novel white-box approach that leverages mechanistic\ninterpretability techniques to craft practical adversarial inputs.\nSpecifically, we first identify acceptance subspaces - sets of feature vectors\nthat do not trigger the model's refusal mechanisms - then use gradient-based\noptimization to reroute embeddings from refusal subspaces to acceptance\nsubspaces, effectively achieving jailbreaks. This targeted approach\nsignificantly reduces computation cost, achieving attack success rates of\n80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5\nwithin minutes or even seconds, compared to existing techniques that often fail\nor require hours of computation. We believe this approach opens a new direction\nfor both attack research and defense development. Furthermore, it showcases a\npractical application of mechanistic interpretability where other methods are\nless efficient, which highlights its utility. The code and generated datasets\nare available at https://github.com/Sckathach/subspace-rerouting.",
      "tldr_zh": "本研究提出了一种新白盒攻击方法，利用 Mechanistic Interpretability 技术针对 Large Language Models (LLMs) 进行高效的对抗攻击，以桥接解释性分析和实际应用间的差距。具体而言，该方法首先识别 acceptance subspaces（不触发模型拒绝机制的特征向量集合），然后通过梯度优化将嵌入从拒绝子空间路由到接受子空间，从而实现 jailbreaks。实验结果显示，该方法在 Gemma2、Llama3.2 和 Qwen2.5 等模型上达到 80-95% 的攻击成功率，仅需几分钟或几秒钟，比传统技术大幅减少计算成本。该创新不仅为对抗攻击研究和防御开发开辟新方向，还展示了 Mechanistic Interpretability 的实际实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06269v2",
      "published_date": "2025-03-08 16:29:45 UTC",
      "updated_date": "2025-05-06 05:48:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:28:55.165721"
    },
    {
      "arxiv_id": "2503.06263v1",
      "title": "Critical Foreign Policy Decisions (CFPD)-Benchmark: Measuring Diplomatic Preferences in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Jensen",
        "Ian Reynolds",
        "Yasir Atalan",
        "Michael Garcia",
        "Austin Woo",
        "Anthony Chen",
        "Trevor Howarth"
      ],
      "abstract": "As national security institutions increasingly integrate Artificial\nIntelligence (AI) into decision-making and content generation processes,\nunderstanding the inherent biases of large language models (LLMs) is crucial.\nThis study presents a novel benchmark designed to evaluate the biases and\npreferences of seven prominent foundation models-Llama 3.1 8B Instruct, Llama\n3.1 70B Instruct, GPT-4o, Gemini 1.5 Pro-002, Mixtral 8x22B, Claude 3.5 Sonnet,\nand Qwen2 72B-in the context of international relations (IR). We designed a\nbias discovery study around core topics in IR using 400-expert crafted\nscenarios to analyze results from our selected models. These scenarios focused\non four topical domains including: military escalation, military and\nhumanitarian intervention, cooperative behavior in the international system,\nand alliance dynamics. Our analysis reveals noteworthy variation among model\nrecommendations based on scenarios designed for the four tested domains.\nParticularly, Qwen2 72B, Gemini 1.5 Pro-002 and Llama 3.1 8B Instruct models\noffered significantly more escalatory recommendations than Claude 3.5 Sonnet\nand GPT-4o models. All models exhibit some degree of country-specific biases,\noften recommending less escalatory and interventionist actions for China and\nRussia compared to the United States and the United Kingdom. These findings\nhighlight the necessity for controlled deployment of LLMs in high-stakes\nenvironments, emphasizing the need for domain-specific evaluations and model\nfine-tuning to align with institutional objectives.",
      "tldr_zh": "本研究引入了 CFPD-Benchmark，这是一个新基准，用于评估七个主要大型语言模型（LLMs），包括 Llama 3.1 8B Instruct、Llama 3.1 70B Instruct、GPT-4o、Gemini 1.5 Pro-002、Mixtral 8x22B、Claude 3.5 Sonnet 和 Qwen2 72B，在国际关系（IR）中的偏见和外交偏好。研究方法涉及设计400个专家制作的场景，聚焦军事升级、军事和人道主义干预、国际合作行为以及联盟动态四个领域。结果显示，Qwen2 72B、Gemini 1.5 Pro-002 和 Llama 3.1 8B Instruct 等模型更倾向于提供升级推荐，而 Claude 3.5 Sonnet 和 GPT-4o 则更保守；此外，所有模型表现出国家特定偏见，如对中国和俄罗斯的推荐较少升级和干预。这些发现突出了在高风险决策环境中部署 LLMs 的潜在风险，强调需要进行领域特定评估和模型微调以符合机构目标。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "I.2.7"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06263v1",
      "published_date": "2025-03-08 16:19:13 UTC",
      "updated_date": "2025-03-08 16:19:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:29:08.272935"
    },
    {
      "arxiv_id": "2503.06260v1",
      "title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Muzhi Dai",
        "Jiashuo Sun",
        "Zhiyuan Zhao",
        "Shixuan Liu",
        "Rui Li",
        "Junyu Gao",
        "Xuelong Li"
      ],
      "abstract": "Aligning large vision-language models (LVLMs) with human preferences is\nchallenging due to the scarcity of fine-grained, high-quality, and multimodal\npreference data without human annotations. Existing methods relying on direct\ndistillation often struggle with low-confidence data, leading to suboptimal\nperformance. To address this, we propose CAREVL, a novel method for preference\nreward modeling by reliably using both high- and low-confidence data. First, a\ncluster of auxiliary expert models (textual reward models) innovatively\nleverages image captions as weak supervision signals to filter high-confidence\ndata. The high-confidence data are then used to fine-tune the LVLM. Second,\nlow-confidence data are used to generate diverse preference samples using the\nfine-tuned LVLM. These samples are then scored and selected to construct\nreliable chosen-rejected pairs for further training. CAREVL achieves\nperformance improvements over traditional distillation-based methods on\nVL-RewardBench and MLLM-as-a-Judge benchmark, demonstrating its effectiveness.\nThe code will be released soon.",
      "tldr_zh": "该研究提出CAREVL方法，用于提升大型视觉语言模型（LVLMs）的奖励建模，以解决人类偏好对齐中数据稀缺和质量问题。CAREVL通过辅助专家模型（文本奖励模型）利用图像标题作为弱监督信号，筛选高置信度数据并微调LVLM，同时用低置信度数据生成多样化偏好样本，并构建可靠的chosen-rejected pairs进行进一步训练。与传统蒸馏方法相比，CAREVL在VL-RewardBench和MLLM-as-a-Judge基准上表现出色，显著提高了模型性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06260v1",
      "published_date": "2025-03-08 16:13:18 UTC",
      "updated_date": "2025-03-08 16:13:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:29:17.449993"
    },
    {
      "arxiv_id": "2503.06252v1",
      "title": "Can Atomic Step Decomposition Enhance the Self-structured Reasoning of Multimodal Large Models?",
      "title_zh": "原子步骤分解是否能增强多模态大型模型的自结构化推理？",
      "authors": [
        "Kun Xiang",
        "Zhili Liu",
        "Zihao Jiang",
        "Yunshuang Nie",
        "Kaixin Cai",
        "Yiyang Yin",
        "Runhui Huang",
        "Haoxiang Fan",
        "Hanhui Li",
        "Weiran Huang",
        "Yihan Zeng",
        "Yu-Jie Yuan",
        "Jianhua Han",
        "Lanqing Hong",
        "Hang Xu",
        "Xiaodan Liang"
      ],
      "abstract": "In this paper, we address the challenging task of multimodal mathematical\nreasoning by incorporating the ability of \"slow thinking\" into multimodal large\nlanguage models (MLLMs). Our core idea is that different levels of reasoning\nabilities can be combined dynamically to tackle questions with different\ncomplexity. To this end, we propose a paradigm of Self-structured Chain of\nThought (SCoT), which is composed of minimal semantic atomic steps. Different\nfrom existing methods that rely on structured templates or free-form paradigms,\nour method can not only generate cognitive CoT structures for various complex\ntasks but also mitigates the phenomenon of overthinking. To introduce\nstructured reasoning capabilities into visual understanding models, we further\ndesign a novel AtomThink framework with four key modules, including (i) a data\nengine to generate high-quality multimodal reasoning paths; (ii) a supervised\nfine-tuning process with serialized inference data; (iii) a policy-guided\nmulti-turn inference method; and (iv) an atomic capability metric to evaluate\nthe single step utilization rate. We conduct extensive experiments to show that\nthe proposed AtomThink significantly improves the performance of baseline\nMLLMs, achieving more than 10\\% average accuracy gains on MathVista and\nMathVerse. Compared to state-of-the-art structured CoT approaches, our method\nnot only achieves higher accuracy but also improves data utilization by 5 times\nand boosts inference efficiency by 85.3\\%. Our code is now public available in\nhttps://github.com/Quinn777/AtomThink.",
      "tldr_zh": "本研究探讨了通过原子步骤分解是否能提升多模态大语言模型 (MLLMs) 的自结构化推理能力，核心在于引入“慢思考”机制来动态结合不同级别的推理以应对复杂多模态数学任务。论文提出 Self-structured Chain of Thought (SCoT) 范式，由最小语义原子步骤组成，能生成适应各种任务的认知结构并缓解 overthinking 现象。作者设计了 AtomThink 框架，包括数据引擎生成高质量推理路径、监督微调过程、策略引导的多轮推理方法以及原子能力指标用于评估单步利用率。实验结果显示，AtomThink 使基线 MLLMs 在 MathVista 和 MathVerse 等基准上平均准确率提升超过 10%，并相较最先进结构化 CoT 方法，提高数据利用率 5 倍并提升推理效率 85.3%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06252v1",
      "published_date": "2025-03-08 15:23:47 UTC",
      "updated_date": "2025-03-08 15:23:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:29:30.628078"
    },
    {
      "arxiv_id": "2503.06247v1",
      "title": "Infant Cry Detection Using Causal Temporal Representation",
      "title_zh": "基于因",
      "authors": [
        "Minghao Fu",
        "Danning Li",
        "Aryan Gadhiya",
        "Benjamin Lambright",
        "Mohamed Alowais",
        "Mohab Bahnassy",
        "Saad El Dine Elletter",
        "Hawau Olamide Toyin",
        "Haiyan Jiang",
        "Kun Zhang",
        "Hanan Aldarmaki"
      ],
      "abstract": "This paper addresses a major challenge in acoustic event detection, in\nparticular infant cry detection in the presence of other sounds and background\nnoises: the lack of precise annotated data. We present two contributions for\nsupervised and unsupervised infant cry detection. The first is an annotated\ndataset for cry segmentation, which enables supervised models to achieve\nstate-of-the-art performance. Additionally, we propose a novel unsupervised\nmethod, Causal Representation Spare Transition Clustering (CRSTC), based on\ncausal temporal representation, which helps address the issue of data scarcity\nmore generally. By integrating the detected cry segments, we significantly\nimprove the performance of downstream infant cry classification, highlighting\nthe potential of this approach for infant care applications.",
      "tldr_zh": "这篇论文针对婴儿哭声检测面临的挑战，特别是缺乏精确标注数据的问题，提出了两个主要贡献：一是发布了一个标注数据集，用于监督模型实现最先进（state-of-the-art）的哭声分割性能；二是开发了一种新型无监督方法，Causal Representation Spare Transition Clustering (CRSTC)，基于causal temporal representation来处理数据稀缺问题。通过整合检测到的哭声段，该方法显著提升了下游婴儿哭声分类的准确性，为婴儿护理应用提供了潜在的实用解决方案。",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06247v1",
      "published_date": "2025-03-08 15:15:23 UTC",
      "updated_date": "2025-03-08 15:15:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:29:42.403701"
    },
    {
      "arxiv_id": "2503.06242v1",
      "title": "LapSum -- One Method to Differentiate Them All: Ranking, Sorting and Top-k Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Łukasz Struski",
        "Michał B. Bednarczyk",
        "Igor T. Podolak",
        "Jacek Tabor"
      ],
      "abstract": "We present a novel technique for constructing differentiable order-type\noperations, including soft ranking, soft top-k selection, and soft\npermutations. Our approach leverages an efficient closed-form formula for the\ninverse of the function LapSum, defined as the sum of Laplace distributions.\nThis formulation ensures low computational and memory complexity in selecting\nthe highest activations, enabling losses and gradients to be computed in\n$O(n\\log{}n)$ time. Through extensive experiments, we demonstrate that our\nmethod outperforms state-of-the-art techniques for high-dimensional vectors and\nlarge $k$ values. Furthermore, we provide efficient implementations for both\nCPU and CUDA environments, underscoring the practicality and scalability of our\nmethod for large-scale ranking and differentiable ordering problems.",
      "tldr_zh": "本文提出了一种名为 LapSum 的新方法，用于构建可微分的排序操作，包括 soft ranking、soft top-k selection 和 soft permutations。该方法利用 LapSum 函数（Laplace 分布之和）的逆闭式公式，确保在 O(n log n) 时间内高效计算损失和梯度，从而降低计算和内存复杂度。实验结果表明，该方法在高维向量和大 k 值场景下优于现有技术；此外，提供 CPU 和 CUDA 的高效实现，增强其在大规模排名和可微分排序问题中的实用性和可扩展性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06242v1",
      "published_date": "2025-03-08 14:53:36 UTC",
      "updated_date": "2025-03-08 14:53:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:29:56.521363"
    },
    {
      "arxiv_id": "2503.06238v1",
      "title": "Image is All You Need: Towards Efficient and Effective Large Language Model-Based Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Kibum Kim",
        "Sein Kim",
        "Hongseok Kang",
        "Jiwan Kim",
        "Heewoong Noh",
        "Yeonjun In",
        "Kanghoon Yoon",
        "Jinoh Oh",
        "Chanyoung Park"
      ],
      "abstract": "Large Language Models (LLMs) have recently emerged as a powerful backbone for\nrecommender systems. Existing LLM-based recommender systems take two different\napproaches for representing items in natural language, i.e., Attribute-based\nRepresentation and Description-based Representation. In this work, we aim to\naddress the trade-off between efficiency and effectiveness that these two\napproaches encounter, when representing items consumed by users. Based on our\ninteresting observation that there is a significant information overlap between\nimages and descriptions associated with items, we propose a novel method, Image\nis all you need for LLM-based Recommender system (I-LLMRec). Our main idea is\nto leverage images as an alternative to lengthy textual descriptions for\nrepresenting items, aiming at reducing token usage while preserving the rich\nsemantic information of item descriptions. Through extensive experiments, we\ndemonstrate that I-LLMRec outperforms existing methods in both efficiency and\neffectiveness by leveraging images. Moreover, a further appeal of I-LLMRec is\nits ability to reduce sensitivity to noise in descriptions, leading to more\nrobust recommendations.",
      "tldr_zh": "这篇论文针对大型语言模型（LLMs）驱动的推荐系统（LLM-based Recommender Systems）中物品表示的效率与效果权衡问题，提出了一种新方法I-LLMRec。\n该方法利用物品图像作为文本描述的替代方案，减少token使用量，同时保留丰富的语义信息。\n实验结果表明，I-LLMRec在效率和效果上均优于现有方法，并显著降低了描述中的噪声敏感性，提高了推荐系统的鲁棒性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06238v1",
      "published_date": "2025-03-08 14:51:16 UTC",
      "updated_date": "2025-03-08 14:51:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:30:07.468979"
    },
    {
      "arxiv_id": "2503.06229v1",
      "title": "A Frank System for Co-Evolutionary Hybrid Decision-Making",
      "title_zh": "翻译失败",
      "authors": [
        "Federico Mazzoni",
        "Riccardo Guidotti",
        "Alessio Malizia"
      ],
      "abstract": "We introduce Frank, a human-in-the-loop system for co-evolutionary hybrid\ndecision-making aiding the user to label records from an un-labeled dataset.\nFrank employs incremental learning to ``evolve'' in parallel with the user's\ndecisions, by training an interpretable machine learning model on the records\nlabeled by the user. Furthermore, Frank advances state-of-the-art approaches by\noffering inconsistency controls, explanations, fairness checks, and bad-faith\nsafeguards simultaneously. We evaluate our proposal by simulating the users'\nbehavior with various levels of expertise and reliance on Frank's suggestions.\nThe experiments show that Frank's intervention leads to improvements in the\naccuracy and the fairness of the decisions.",
      "tldr_zh": "这篇论文介绍了 Frank 系统，这是一个人类在循环（human-in-the-loop）框架，用于协同进化混合决策（co-evolutionary hybrid decision-making），帮助用户标记未标记数据集的记录。系统采用增量学习（incremental learning）来训练可解释机器学习模型（interpretable machine learning model），并同时提供不一致性控制（inconsistency controls）、解释（explanations）、公平检查（fairness checks）和防范恶意行为（bad-faith safeguards）的功能。实验通过模拟不同专业水平和对系统依赖程度的用户行为，证明 Frank 的干预显著提高了决策的准确性和公平性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.06229v1",
      "published_date": "2025-03-08 14:06:16 UTC",
      "updated_date": "2025-03-08 14:06:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:30:19.563361"
    },
    {
      "arxiv_id": "2503.06226v2",
      "title": "Optimal Output Feedback Learning Control for Discrete-Time Linear Quadratic Regulation",
      "title_zh": "翻译失败",
      "authors": [
        "Kedi Xie",
        "Martin Guay",
        "Shimin Wang",
        "Fang Deng",
        "Maobin Lu"
      ],
      "abstract": "This paper studies the linear quadratic regulation (LQR) problem of unknown\ndiscrete-time systems via dynamic output feedback learning control. In contrast\nto the state feedback, the optimality of the dynamic output feedback control\nfor solving the LQR problem requires an implicit condition on the convergence\nof the state observer. Moreover, due to unknown system matrices and the\nexistence of observer error, it is difficult to analyze the convergence and\nstability of most existing output feedback learning-based control methods. To\ntackle these issues, we propose a generalized dynamic output feedback learning\ncontrol approach with guaranteed convergence, stability, and optimality\nperformance for solving the LQR problem of unknown discrete-time linear\nsystems. In particular, a dynamic output feedback controller is designed to be\nequivalent to a state feedback controller. This equivalence relationship is an\ninherent property without requiring convergence of the estimated state by the\nstate observer, which plays a key role in establishing the off-policy learning\ncontrol approaches. By value iteration and policy iteration schemes, the\nadaptive dynamic programming based learning control approaches are developed to\nestimate the optimal feedback control gain. In addition, a model-free stability\ncriterion is provided by finding a nonsingular parameterization matrix, which\ncontributes to establishing a switched iteration scheme. Furthermore, the\nconvergence, stability, and optimality analyses of the proposed output feedback\nlearning control approaches are given. Finally, the theoretical results are\nvalidated by two numerical examples.",
      "tldr_zh": "本论文研究了未知离散时间系统的线性二次调节 (LQR) 问题，通过动态输出反馈学习控制方法来实现优化控制。该方法解决了传统输出反馈中状态观测器收敛的隐含条件问题，设计了一个等效于状态反馈控制器的动态输出反馈框架，并利用价值迭代和策略迭代方案基于自适应动态规划来估计最优反馈增益。同时，论文提供了一个模型无关的稳定性标准，并通过理论分析保证了方法的收敛、稳定性和最优性，最后通过两个数值例子验证了这些结果。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "math.OC"
      ],
      "primary_category": "eess.SY",
      "comment": "16 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06226v2",
      "published_date": "2025-03-08 14:02:16 UTC",
      "updated_date": "2025-03-11 18:32:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:30:31.829417"
    },
    {
      "arxiv_id": "2503.06212v2",
      "title": "GraphGen+: Advancing Distributed Subgraph Generation and Graph Learning On Industrial Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Jin",
        "Yongchao Liu",
        "Chuntao Hong"
      ],
      "abstract": "Graph-based computations are crucial in a wide range of applications, where\ngraphs can scale to trillions of edges. To enable efficient training on such\nlarge graphs, mini-batch subgraph sampling is commonly used, which allows\ntraining without loading the entire graph into memory. However, existing\nsolutions face significant trade-offs: online subgraph generation, as seen in\nframeworks like DGL and PyG, is limited to a single machine, resulting in\nsevere performance bottlenecks, while offline precomputed subgraphs, as in\nGraphGen, improve sampling efficiency but introduce large storage overhead and\nhigh I/O costs during training. To address these challenges, we propose\n\\textbf{GraphGen+}, an integrated framework that synchronizes distributed\nsubgraph generation with in-memory graph learning, eliminating the need for\nexternal storage while significantly improving efficiency. GraphGen+ achieves a\n\\textbf{27$\\times$} speedup in subgraph generation compared to conventional\nSQL-like methods and a \\textbf{1.3$\\times$} speedup over GraphGen, supporting\ntraining on 1 million nodes per iteration and removing the overhead associated\nwith precomputed subgraphs, making it a scalable and practical solution for\nindustry-scale graph learning.",
      "tldr_zh": "这项研究针对大规模工业图的子图生成和图学习问题，提出了GraphGen+框架，以解决现有方法的性能瓶颈和存储开销问题。GraphGen+通过同步分布式子图生成与内存中graph learning，实现高效训练，而无需外部存储。实验结果显示，该框架比传统SQL-like方法快27倍，比GraphGen快1.3倍，支持每迭代处理1百万节点，提供了一个可扩展的工业级解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Extended version of our EuroSys 2025 poster paper",
      "pdf_url": "http://arxiv.org/pdf/2503.06212v2",
      "published_date": "2025-03-08 13:29:42 UTC",
      "updated_date": "2025-04-02 20:44:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:30:44.343147"
    },
    {
      "arxiv_id": "2503.06211v1",
      "title": "Text-Speech Language Models with Improved Cross-Modal Transfer by Aligning Abstraction Levels",
      "title_zh": "翻译失败",
      "authors": [
        "Santiago Cuervo",
        "Adel Moumen",
        "Yanis Labrak",
        "Sameer Khurana",
        "Antoine Laurent",
        "Mickael Rouvier",
        "Ricard Marxer"
      ],
      "abstract": "Text-Speech Language Models (TSLMs) -- language models trained to jointly\nprocess and generate text and speech -- aim to enable cross-modal knowledge\ntransfer to overcome the scaling limitations of unimodal speech LMs. The\npredominant approach to TSLM training expands the vocabulary of a pre-trained\ntext LM by appending new embeddings and linear projections for speech, followed\nby fine-tuning on speech data. We hypothesize that this method limits\ncross-modal transfer by neglecting feature compositionality, preventing\ntext-learned functions from being fully leveraged at appropriate abstraction\nlevels. To address this, we propose augmenting vocabulary expansion with\nmodules that better align abstraction levels across layers. Our models,\n\\textsc{SmolTolk}, rival or surpass state-of-the-art TSLMs trained with orders\nof magnitude more compute. Representation analyses and improved multimodal\nperformance suggest our method enhances cross-modal transfer.",
      "tldr_zh": "本论文探讨了 Text-Speech Language Models (TSLMs)，这些模型旨在通过跨模态知识转移来提升语音语言模型的性能，但传统训练方法因忽略特征组合性和抽象级别不对齐而限制了转移效率。作者提出了一种改进方法，在词汇扩展的基础上添加模块来对齐各层的抽象级别，开发了 \\textsc{SmolTolk} 模型。实验结果显示，该模型在远少于先进模型的计算资源下，性能达到或超过现有水平，并通过表示分析证实了其在多模态任务中的跨模态转移效果显著提升。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06211v1",
      "published_date": "2025-03-08 13:28:50 UTC",
      "updated_date": "2025-03-08 13:28:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:30:56.779087"
    },
    {
      "arxiv_id": "2503.10660v2",
      "title": "Text-to-3D Generation using Jensen-Shannon Score Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Khoi Do",
        "Binh-Son Hua"
      ],
      "abstract": "Score distillation sampling is an effective technique to generate 3D models\nfrom text prompts, utilizing pre-trained large-scale text-to-image diffusion\nmodels as guidance. However, the produced 3D assets tend to be over-saturating,\nover-smoothing, with limited diversity. These issues are results from a reverse\nKullback-Leibler (KL) divergence objective, which makes the optimization\nunstable and results in mode-seeking behavior. In this paper, we derive a\nbounded score distillation objective based on Jensen-Shannon divergence (JSD),\nwhich stabilizes the optimization process and produces high-quality 3D\ngeneration. JSD can match well generated and target distribution, therefore\nmitigating mode seeking. We provide a practical implementation of JSD by\nutilizing the theory of generative adversarial networks to define an\napproximate objective function for the generator, assuming the discriminator is\nwell trained. By assuming the discriminator following a log-odds classifier, we\npropose a minority sampling algorithm to estimate the gradients of our proposed\nobjective, providing a practical implementation for JSD. We conduct both\ntheoretical and empirical studies to validate our method. Experimental results\non T3Bench demonstrate that our method can produce high-quality and diversified\n3D assets.",
      "tldr_zh": "本文提出了一种基于Jensen-Shannon divergence (JSD)的Score Distillation Sampling方法，用于改善文本到3D生成的质量，解决现有reverse Kullback-Leibler (KL) divergence导致的优化不稳定、模式寻找行为、过度饱和和过度平滑等问题。相比传统方法，该框架通过generative adversarial networks (GAN)理论定义近似目标函数，并采用minority sampling算法估计梯度，实现更稳定的优化过程和更高的生成多样性。实验在T3Bench基准上验证了该方法的有效性，产生高质量、多样化的3D资产。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10660v2",
      "published_date": "2025-03-08 13:27:18 UTC",
      "updated_date": "2025-03-18 17:15:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:31:09.835114"
    },
    {
      "arxiv_id": "2503.06208v1",
      "title": "Distributed Graph Neural Network Inference With Just-In-Time Compilation For Industry-Scale Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Xiabao Wu",
        "Yongchao Liu",
        "Wei Qin",
        "Chuntao Hong"
      ],
      "abstract": "Graph neural networks (GNNs) have delivered remarkable results in various\nfields. However, the rapid increase in the scale of graph data has introduced\nsignificant performance bottlenecks for GNN inference. Both computational\ncomplexity and memory usage have risen dramatically, with memory becoming a\ncritical limitation. Although graph sampling-based subgraph learning methods\ncan help mitigate computational and memory demands, they come with drawbacks\nsuch as information loss and high redundant computation among subgraphs. This\npaper introduces an innovative processing paradgim for distributed graph\nlearning that abstracts GNNs with a new set of programming interfaces and\nleverages Just-In-Time (JIT) compilation technology to its full potential. This\nparadigm enables GNNs to highly exploit the computational resources of\ndistributed clusters by eliminating the drawbacks of subgraph learning methods,\nleading to a more efficient inference process. Our experimental results\ndemonstrate that on industry-scale graphs of up to \\textbf{500 million nodes\nand 22.4 billion edges}, our method can produce a performance boost of up to\n\\textbf{27.4 times}.",
      "tldr_zh": "这篇论文针对 Graph Neural Networks (GNNs) 在大规模图数据上的推理瓶颈（如计算复杂性和内存限制），提出了一种创新的分布式图学习处理范式。 该范式通过抽象 GNNs 的新编程接口并充分利用 Just-In-Time (JIT) 编译技术，消除了子图学习方法的缺点，例如信息丢失和高冗余计算，从而高效利用分布式集群的计算资源。 实验结果显示，在包含 5 亿节点和 224 亿边的行业规模图上，该方法可以将性能提升高达 27.4 倍，为大规模 GNNs 推理提供了高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by EuroSys 2025 (poster)",
      "pdf_url": "http://arxiv.org/pdf/2503.06208v1",
      "published_date": "2025-03-08 13:26:59 UTC",
      "updated_date": "2025-03-08 13:26:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:31:21.576222"
    },
    {
      "arxiv_id": "2503.06204v1",
      "title": "CUPCase: Clinically Uncommon Patient Cases and Diagnoses Dataset",
      "title_zh": "CUPCase：临床不常见患者病例与诊断数据集",
      "authors": [
        "Oriel Perets",
        "Ofir Ben Shoham",
        "Nir Grinberg",
        "Nadav Rappoport"
      ],
      "abstract": "Medical benchmark datasets significantly contribute to developing Large\nLanguage Models (LLMs) for medical knowledge extraction, diagnosis,\nsummarization, and other uses. Yet, current benchmarks are mainly derived from\nexam questions given to medical students or cases described in the medical\nliterature, lacking the complexity of real-world patient cases that deviate\nfrom classic textbook abstractions. These include rare diseases, uncommon\npresentations of common diseases, and unexpected treatment responses. Here, we\nconstruct Clinically Uncommon Patient Cases and Diagnosis Dataset (CUPCase)\nbased on 3,562 real-world case reports from BMC, including diagnoses in\nopen-ended textual format and as multiple-choice options with distractors.\nUsing this dataset, we evaluate the ability of state-of-the-art LLMs, including\nboth general-purpose and Clinical LLMs, to identify and correctly diagnose a\npatient case, and test models' performance when only partial information about\ncases is available. Our findings show that general-purpose GPT-4o attains the\nbest performance in both the multiple-choice task (average accuracy of 87.9%)\nand the open-ended task (BERTScore F1 of 0.764), outperforming several LLMs\nwith a focus on the medical domain such as Meditron-70B and MedLM-Large.\nMoreover, GPT-4o was able to maintain 87% and 88% of its performance with only\nthe first 20% of tokens of the case presentation in multiple-choice and free\ntext, respectively, highlighting the potential of LLMs to aid in early\ndiagnosis in real-world cases. CUPCase expands our ability to evaluate LLMs for\nclinical decision support in an open and reproducible manner.",
      "tldr_zh": "本论文构建了 CUPCase 数据集，该数据集基于 3,562 个来自 BMC 的真实临床案例，聚焦于罕见疾病、常见疾病的非典型表现和意外治疗反应，以填补现有医疗基准数据集的不足。研究评估了多种先进 LLMs（包括通用模型如 GPT-4o 和临床专用模型如 Meditron-70B）的诊断能力，在多选任务中 GPT-4o 取得了 87.9% 的平均准确率，在开放式任务中 BERTScore F1 达 0.764，并优于其他模型。实验还发现，GPT-4o 即使仅使用病例前 20% 的信息，也能保持 87% 和 88% 的性能，突显了 LLMs 在早期诊断中的潜力。CUPCase 数据集通过开放可重复的方式提升了对 LLMs 临床决策支持能力的评估。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06204v1",
      "published_date": "2025-03-08 13:21:44 UTC",
      "updated_date": "2025-03-08 13:21:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:31:35.362961"
    },
    {
      "arxiv_id": "2503.06202v1",
      "title": "Breaking Free from MMI: A New Frontier in Rationalization by Probing Input Utilization",
      "title_zh": "摆脱 MMI：通过探测输入利用的理性化新前沿",
      "authors": [
        "Wei Liu",
        "Zhiying Deng",
        "Zhongyu Niu",
        "Jun Wang",
        "Haozhao Wang",
        "Zhigang Zeng",
        "Ruixuan Li"
      ],
      "abstract": "Extracting a small subset of crucial rationales from the full input is a key\nproblem in explainability research. The most widely used fundamental criterion\nfor rationale extraction is the maximum mutual information (MMI) criterion. In\nthis paper, we first demonstrate that MMI suffers from diminishing marginal\nreturns. Once part of the rationale has been identified, finding the remaining\nportions contributes only marginally to increasing the mutual information,\nmaking it difficult to use MMI to locate the rest. In contrast to MMI that aims\nto reproduce the prediction, we seek to identify the parts of the input that\nthe network can actually utilize.\n  This is achieved by comparing how different rationale candidates match the\ncapability space of the weight matrix. The weight matrix of a neural network is\ntypically low-rank, meaning that the linear combinations of its column vectors\ncan only cover part of the directions in a high-dimensional space\n(high-dimension: the dimensions of an input vector). If an input is fully\nutilized by the network, {it generally matches these directions (e.g., a\nportion of a hypersphere), resulting in a representation with a high norm.\nConversely, if an input primarily falls outside (orthogonal to) these\ndirections}, its representation norm will approach zero, behaving like noise\nthat the network cannot effectively utilize. Building on this, we propose using\nthe norms of rationale candidates as an alternative objective to MMI. Through\nexperiments on four text classification datasets and one graph classification\ndataset using three network architectures (GRUs, BERT, and GCN), we show that\nour method outperforms MMI and its improved variants in identifying better\nrationales. We also compare our method with a representative LLM\n(llama-3.1-8b-instruct) and find that our simple method gets comparable results\nto it and can sometimes even outperform it.",
      "tldr_zh": "本论文批评了最大互信息（MMI）标准在提取输入关键理据（rationales）时的递减边际收益问题，即一旦识别部分理据，剩余部分对互信息贡献有限。作者提出一种新方法，通过比较理据候选与神经网络权重矩阵的能力空间匹配，利用权重矩阵的低-rank特性，基于理据候选的范数来评估网络实际利用的输入部分。实验在四个文本分类数据集和一个图分类数据集上，使用 GRUs、BERT 和 GCN 架构表明，该方法优于 MMI 及其变体，并在与 LLaMA-3.1-8B-Instruct 等 LLM 的比较中表现出可比或更优性能。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06202v1",
      "published_date": "2025-03-08 13:08:46 UTC",
      "updated_date": "2025-03-08 13:08:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:31:44.934267"
    },
    {
      "arxiv_id": "2503.06201v1",
      "title": "Explainable Synthetic Image Detection through Diffusion Timestep Ensembling",
      "title_zh": "通过扩散时间步集成的可解释合成图像检测",
      "authors": [
        "Yixin Wu",
        "Feiran Zhang",
        "Tianyuan Shi",
        "Ruicheng Yin",
        "Zhenghua Wang",
        "Zhenliang Gan",
        "Xiaohua Wang",
        "Changze Lv",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "abstract": "Recent advances in diffusion models have enabled the creation of deceptively\nreal images, posing significant security risks when misused. In this study, we\nreveal that natural and synthetic images exhibit distinct differences in the\nhigh-frequency domains of their Fourier power spectra after undergoing\niterative noise perturbations through an inverse multi-step denoising process,\nsuggesting that such noise can provide additional discriminative information\nfor identifying synthetic images. Based on this observation, we propose a novel\ndetection method that amplifies these differences by progressively adding noise\nto the original images across multiple timesteps, and train an ensemble of\nclassifiers on these noised images. To enhance human comprehension, we\nintroduce an explanation generation and refinement module to identify flaws\nlocated in AI-generated images. Additionally, we construct two new datasets,\nGenHard and GenExplain, derived from the GenImage benchmark, providing\ndetection samples of greater difficulty and high-quality rationales for fake\nimages. Extensive experiments show that our method achieves state-of-the-art\nperformance with 98.91% and 95.89% detection accuracy on regular and harder\nsamples, increasing a minimal of 2.51% and 3.46% compared to baselines.\nFurthermore, our method also generalizes effectively to images generated by\nother diffusion models. Our code and datasets will be made publicly available.",
      "tldr_zh": "本研究揭示，自然图像和合成图像在经过逆多步去噪过程后的傅立叶功率谱（Fourier power spectra）高频域存在显著差异，从而提出了一种基于扩散时间步集成的合成图像检测方法。该方法通过逐步向图像添加噪声并训练分类器集合（ensemble of classifiers），放大这些差异，同时引入解释生成和精炼模块以识别AI生成图像中的缺陷。研究者构建了GenHard和GenExplain两个新数据集，并在实验中实现了98.91%和95.89%的检测准确率，比基线模型至少提高了2.51%和3.46%，并展示了该方法对其他扩散模型（diffusion models）生成的图像的良好泛化性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06201v1",
      "published_date": "2025-03-08 13:04:20 UTC",
      "updated_date": "2025-03-08 13:04:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:31:55.903742"
    },
    {
      "arxiv_id": "2503.06195v1",
      "title": "Human-AI Experience in Integrated Development Environments: A Systematic Literature Review",
      "title_zh": "翻译失败",
      "authors": [
        "Agnia Sergeyuk",
        "Ilya Zakharov",
        "Ekaterina Koshchenko",
        "Maliheh Izadi"
      ],
      "abstract": "The integration of Artificial Intelligence (AI) into Integrated Development\nEnvironments (IDEs) is reshaping software development, fundamentally altering\nhow developers interact with their tools. This shift marks the emergence of\nHuman-AI Experience in Integrated Development Environment (in-IDE HAX), a field\nthat explores the evolving dynamics of Human-Computer Interaction in\nAI-assisted coding environments. Despite rapid adoption, research on in-IDE HAX\nremains fragmented which highlights the need for a unified overview of current\npractices, challenges, and opportunities. To provide a structured overview of\nexisting research, we conduct a systematic literature review of 89 studies,\nsummarizing current findings and outlining areas for further investigation.\n  Our findings reveal that AI-assisted coding enhances developer productivity\nbut also introduces challenges, such as verification overhead, automation bias,\nand over-reliance, particularly among novice developers. Furthermore, concerns\nabout code correctness, security, and maintainability highlight the urgent need\nfor explainability, verification mechanisms, and adaptive user control.\nAlthough recent advances have driven the field forward, significant research\ngaps remain, including a lack of longitudinal studies, personalization\nstrategies, and AI governance frameworks. This review provides a foundation for\nadvancing in-IDE HAX research and offers guidance for responsibly integrating\nAI into software development.",
      "tldr_zh": "本论文通过 Systematic Literature Review 分析了 89 篇研究，系统概述了 AI 在 Integrated Development Environments (IDE) 中的整合及其对开发者互动（in-IDE HAX）的深远影响。研究发现，AI 辅助编码显著提升了开发者的生产力，但也带来了挑战，如验证开销、自动化偏差和过度依赖，尤其在初学者中，同时引发了对代码正确性、安全性和可维护性的担忧。论文强调了需要加强解释性机制、验证策略和适应性用户控制，并指出了未来研究空白，包括缺乏纵向研究、个性化策略和 AI 治理框架，为负责任地推进 in-IDE HAX 提供了重要指导。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "Submitted to Empirical Software Engineering (EMSE) special issue\n  Human-Centered AI for Software Engineering (HumanAISE), 28 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.06195v1",
      "published_date": "2025-03-08 12:40:18 UTC",
      "updated_date": "2025-03-08 12:40:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:32:07.685726"
    },
    {
      "arxiv_id": "2503.06187v1",
      "title": "MSConv: Multiplicative and Subtractive Convolution for Face Recognition",
      "title_zh": "MSConv：用于人脸识别的乘法和减法卷积",
      "authors": [
        "Si Zhou",
        "Yain-Whar Si",
        "Xiaochen Yuan",
        "Xiaofan Li",
        "Xiaoxiang Liu",
        "Xinyuan Zhang",
        "Cong Lin",
        "Xueyuan Gong"
      ],
      "abstract": "In Neural Networks, there are various methods of feature fusion. Different\nstrategies can significantly affect the effectiveness of feature\nrepresentation, consequently influencing the ability of model to extract\nrepresentative and discriminative features. In the field of face recognition,\ntraditional feature fusion methods include feature concatenation and feature\naddition. Recently, various attention mechanism-based fusion strategies have\nemerged. However, we found that these methods primarily focus on the important\nfeatures in the image, referred to as salient features in this paper, while\nneglecting another equally important set of features for image recognition\ntasks, which we term differential features. This may cause the model to\noverlook critical local differences when dealing with complex facial samples.\nTherefore, in this paper, we propose an efficient convolution module called\nMSConv (Multiplicative and Subtractive Convolution), designed to balance the\nlearning of model about salient and differential features. Specifically, we\nemploy multi-scale mixed convolution to capture both local and broader\ncontextual information from face images, and then utilize Multiplication\nOperation (MO) and Subtraction Operation (SO) to extract salient and\ndifferential features, respectively. Experimental results demonstrate that by\nintegrating both salient and differential features, MSConv outperforms models\nthat only focus on salient features.",
      "tldr_zh": "该研究指出，现有面部识别中的特征融合方法（如特征连接和基于注意力机制的策略）主要关注图像的显著特征（salient features），而忽略了差异特征（differential features），导致模型在处理复杂面部样本时可能忽略关键局部差异。为此，作者提出了一种高效卷积模块 MSConv（Multiplicative and Subtractive Convolution），它通过多尺度混合卷积捕获局部和更广泛的上下文信息，然后利用乘法操作（Multiplication Operation, MO）和减法操作（Subtraction Operation, SO）来平衡提取显著特征和差异特征。实验结果显示，MSConv 整合这两种特征后，在面部识别任务中显著优于仅关注显著特征的基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06187v1",
      "published_date": "2025-03-08 12:18:29 UTC",
      "updated_date": "2025-03-08 12:18:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:32:19.858422"
    },
    {
      "arxiv_id": "2503.06184v1",
      "title": "Sample-aware Adaptive Structured Pruning for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Kong",
        "Xinge Ma",
        "Jin Wang",
        "Xuejie Zhang"
      ],
      "abstract": "Large language models (LLMs) have achieved outstanding performance in natural\nlanguage processing, but enormous model sizes and high computational costs\nlimit their practical deployment. Structured pruning can effectively reduce the\nresource demands for deployment by removing redundant model parameters.\nHowever, the randomly selected calibration data and fixed single importance\nestimation metrics in existing structured pruning methods lead to degraded\nperformance of pruned models. This study introduces AdaPruner, a sample-aware\nadaptive structured pruning framework for LLMs, aiming to optimize the\ncalibration data and importance estimation metrics in the structured pruning\nprocess. Specifically, AdaPruner effectively removes redundant parameters from\nLLMs by constructing a structured pruning solution space and then employing\nBayesian optimization to adaptively search for the optimal calibration data and\nimportance estimation metrics. Experimental results show that the AdaPruner\noutperforms existing structured pruning methods on a family of LLMs with\nvarying pruning ratios, demonstrating its applicability and robustness.\nRemarkably, at a 20\\% pruning ratio, the model pruned with AdaPruner maintains\n97\\% of the performance of the unpruned model.",
      "tldr_zh": "该研究针对大型语言模型(LLMs)的庞大尺寸和高计算成本问题，提出了一种样本感知的自适应结构化剪枝框架AdaPruner，以优化校准数据和重要性评估指标。具体而言，AdaPruner通过构建结构化剪枝解决方案空间并运用贝叶斯优化(Bayesian optimization)来自适应搜索最佳参数，从而有效去除冗余参数。实验结果表明，AdaPruner在不同LLMs和剪枝比例下优于现有方法，尤其在20%剪枝比例时，模型性能保留了97%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06184v1",
      "published_date": "2025-03-08 12:00:21 UTC",
      "updated_date": "2025-03-08 12:00:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:32:31.838513"
    },
    {
      "arxiv_id": "2503.06183v2",
      "title": "Lightweight Software Kernels and Hardware Extensions for Efficient Sparse Deep Neural Networks on Microcontrollers",
      "title_zh": "轻量级软件内核和硬件扩展，用于在微控制器上实现高效稀疏深度神经网络",
      "authors": [
        "Francesco Daghero",
        "Daniele Jahier Pagliari",
        "Francesco Conti",
        "Luca Benini",
        "Massimo Poncino",
        "Alessio Burrello"
      ],
      "abstract": "The acceleration of pruned Deep Neural Networks (DNNs) on edge devices such\nas Microcontrollers (MCUs) is a challenging task, given the tight area- and\npower-constraints of these devices. In this work, we propose a three-fold\ncontribution to address this problem. First, we design a set of optimized\nsoftware kernels for N:M pruned layers, targeting ultra-low-power, multicore\nRISC-V MCUs, which are up to 2.1x and 3.4x faster than their dense counterparts\nat 1:8 and 1:16 sparsity, respectively. Then, we implement a lightweight\nInstruction-Set Architecture (ISA) extension to accelerate the indirect load\nand non-zero indices decompression operations required by our kernels,\nobtaining up to 1.9x extra speedup, at the cost of a 5% area overhead. Lastly,\nwe extend an open-source DNN compiler to utilize our sparse kernels for\ncomplete networks, showing speedups of 3.21x and 1.81x on a ResNet18 and a\nVision Transformer (ViT), with less than 1.5% accuracy drop compared to a dense\nbaseline.",
      "tldr_zh": "本研究针对微控制器(MCUs)上稀疏深度神经网络(DNNs)的加速问题，提出三方面贡献：首先，设计优化软件内核用于 N:M 修剪层，在多核 RISC-V MCUs 上实现最高 3.4x 加速（在 1:16 稀疏度时）。其次，引入轻量级指令集架构(ISA)扩展，加速间接加载和非零索引解压操作，提供额外 1.9x 加速，同时仅增加 5% 面积开销。最终，通过扩展开源 DNN 编译器，在完整网络上实现 ResNet18 和 Vision Transformer (ViT) 分别 3.21x 和 1.81x 的速度提升，同时准确率损失不到 1.5%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at MLSys 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06183v2",
      "published_date": "2025-03-08 11:59:12 UTC",
      "updated_date": "2025-03-19 10:10:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:32:43.554066"
    },
    {
      "arxiv_id": "2503.06175v1",
      "title": "Minion Gated Recurrent Unit for Continual Learning",
      "title_zh": "Minion 门控循环单元用于持续学习",
      "authors": [
        "Abdullah M. Zyarah",
        "Dhireesha Kudithipudi"
      ],
      "abstract": "The increasing demand for continual learning in sequential data processing\nhas led to progressively complex training methodologies and larger recurrent\nnetwork architectures. Consequently, this has widened the knowledge gap between\ncontinual learning with recurrent neural networks (RNNs) and their ability to\noperate on devices with limited memory and compute. To address this challenge,\nwe investigate the effectiveness of simplifying RNN architectures, particularly\ngated recurrent unit (GRU), and its impact on both single-task and multitask\nsequential learning. We propose a new variant of GRU, namely the minion\nrecurrent unit (MiRU). MiRU replaces conventional gating mechanisms with\nscaling coefficients to regulate dynamic updates of hidden states and\nhistorical context, reducing computational costs and memory requirements.\nDespite its simplified architecture, MiRU maintains performance comparable to\nthe standard GRU while achieving 2.90x faster training and reducing parameter\nusage by 2.88x, as demonstrated through evaluations on sequential image\nclassification and natural language processing benchmarks. The impact of model\nsimplification on its learning capacity is also investigated by performing\ncontinual learning tasks with a rehearsal-based strategy and global inhibition.\nWe find that MiRU demonstrates stable performance in multitask learning even\nwhen using only rehearsal, unlike the standard GRU and its variants. These\nfeatures position MiRU as a promising candidate for edge-device applications.",
      "tldr_zh": "这篇论文针对序列数据处理中的持续学习挑战，提出了一种简化版的 Gated Recurrent Unit (GRU)，即 Minion Recurrent Unit (MiRU)。MiRU 通过使用缩放系数替换传统的门控机制，来调节隐藏状态和历史上下文的动态更新，从而降低计算成本和内存需求。实验结果显示，MiRU 在序列图像分类和自然语言处理基准测试中，与标准 GRU 性能相当，但训练速度提高了 2.90 倍，参数使用减少了 2.88 倍，并在基于复述策略的多任务持续学习中表现出更稳定的性能。这些特性使 MiRU 成为适用于边缘设备的理想候选方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06175v1",
      "published_date": "2025-03-08 11:28:40 UTC",
      "updated_date": "2025-03-08 11:28:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:32:57.226935"
    },
    {
      "arxiv_id": "2503.06171v1",
      "title": "ROCM: RLHF on consistency models",
      "title_zh": "翻译失败",
      "authors": [
        "Shivanshu Shekhar",
        "Tong Zhang"
      ],
      "abstract": "Diffusion models have revolutionized generative modeling in continuous\ndomains like image, audio, and video synthesis. However, their iterative\nsampling process leads to slow generation and inefficient training, challenges\nthat are further exacerbated when incorporating Reinforcement Learning from\nHuman Feedback (RLHF) due to sparse rewards and long time horizons. Consistency\nmodels address these issues by enabling single-step or efficient multi-step\ngeneration, significantly reducing computational costs.\n  In this work, we propose a direct reward optimization framework for applying\nRLHF to consistency models, incorporating distributional regularization to\nenhance training stability and prevent reward hacking. We investigate various\n$f$-divergences as regularization strategies, striking a balance between reward\nmaximization and model consistency. Unlike policy gradient methods, our\napproach leverages first-order gradients, making it more efficient and less\nsensitive to hyperparameter tuning. Empirical results show that our method\nachieves competitive or superior performance compared to policy gradient based\nRLHF methods, across various automatic metrics and human evaluation.\nAdditionally, our analysis demonstrates the impact of different regularization\ntechniques in improving model generalization and preventing overfitting.",
      "tldr_zh": "这篇论文提出了一种名为 ROCM 的直接奖励优化框架，将 Reinforcement Learning from Human Feedback (RLHF) 应用于 Consistency models，以解决扩散模型（Diffusion models）在生成任务中的采样慢和训练低效问题。框架通过引入分布正则化，利用各种 f-divergences 作为正则化策略，平衡奖励最大化和模型一致性，同时采用一阶梯度方法，提高训练稳定性和减少对超参数调优的敏感性。与基于策略梯度的 RLHF 方法相比，实验结果显示 ROCM 在自动指标和人类评估中表现出竞争性或优越性能，并有效提升模型泛化和防止过拟合。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06171v1",
      "published_date": "2025-03-08 11:19:48 UTC",
      "updated_date": "2025-03-08 11:19:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:33:09.138245"
    },
    {
      "arxiv_id": "2503.06170v2",
      "title": "Object-Centric World Model for Language-Guided Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Youngjoon Jeong",
        "Junha Chun",
        "Soonwoo Cha",
        "Taesup Kim"
      ],
      "abstract": "A world model is essential for an agent to predict the future and plan in\ndomains such as autonomous driving and robotics. To achieve this, recent\nadvancements have focused on video generation, which has gained significant\nattention due to the impressive success of diffusion models. However, these\nmodels require substantial computational resources. To address these\nchallenges, we propose a world model leveraging object-centric representation\nspace using slot attention, guided by language instructions. Our model\nperceives the current state as an object-centric representation and predicts\nfuture states in this representation space conditioned on natural language\ninstructions. This approach results in a more compact and computationally\nefficient model compared to diffusion-based generative alternatives.\nFurthermore, it flexibly predicts future states based on language instructions,\nand offers a significant advantage in manipulation tasks where object\nrecognition is crucial. In this paper, we demonstrate that our latent\npredictive world model surpasses generative world models in visuo-linguo-motor\ncontrol tasks, achieving superior sample and computation efficiency. We also\ninvestigate the generalization performance of the proposed method and explore\nvarious strategies for predicting actions using object-centric representations.",
      "tldr_zh": "该论文提出了一种基于对象-centric representation的世界模型（world model），利用slot attention机制并受自然语言指令指导，用于语言引导的操作任务。该模型通过将当前状态表示为对象中心空间并预测未来状态，显著提高了计算效率和紧凑性，相比传统的扩散模型更具优势。在visuo-linguo-motor控制任务中，该方法展示了优越的样本和计算效率，并探讨了其泛化性能以及使用对象中心表示预测动作的策略。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06170v2",
      "published_date": "2025-03-08 11:17:37 UTC",
      "updated_date": "2025-03-12 13:52:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:33:20.831029"
    },
    {
      "arxiv_id": "2503.06169v2",
      "title": "Treble Counterfactual VLMs: A Causal Approach to Hallucination",
      "title_zh": "翻译失败",
      "authors": [
        "Shawn Li",
        "Jiashu Qu",
        "Yuxiao Zhou",
        "Yuehan Qin",
        "Tiankai Yang",
        "Yue Zhao"
      ],
      "abstract": "Vision-Language Models (VLMs) have advanced multi-modal tasks like image\ncaptioning, visual question answering, and reasoning. However, they often\ngenerate hallucinated outputs inconsistent with the visual context or prompt,\nlimiting reliability in critical applications like autonomous driving and\nmedical imaging. Existing studies link hallucination to statistical biases,\nlanguage priors, and biased feature learning but lack a structured causal\nunderstanding. In this work, we introduce a causal perspective to analyze and\nmitigate hallucination in VLMs. We hypothesize that hallucination arises from\nunintended direct influences of either the vision or text modality, bypassing\nproper multi-modal fusion. To address this, we construct a causal graph for\nVLMs and employ counterfactual analysis to estimate the Natural Direct Effect\n(NDE) of vision, text, and their cross-modal interaction on the output. We\nsystematically identify and mitigate these unintended direct effects to ensure\nthat responses are primarily driven by genuine multi-modal fusion. Our approach\nconsists of three steps: (1) designing structural causal graphs to distinguish\ncorrect fusion pathways from spurious modality shortcuts, (2) estimating\nmodality-specific and cross-modal NDE using perturbed image representations,\nhallucinated text embeddings, and degraded visual inputs, and (3) implementing\na test-time intervention module to dynamically adjust the model's dependence on\neach modality. Experimental results demonstrate that our method significantly\nreduces hallucination while preserving task performance, providing a robust and\ninterpretable framework for improving VLM reliability. To enhance accessibility\nand reproducibility, our code is publicly available at\nhttps://github.com/TREE985/Treble-Counterfactual-VLMs.",
      "tldr_zh": "本文提出了一种基于因果方法的Treble Counterfactual VLMs框架，用于分析和缓解视觉语言模型(VLMs)的hallucination问题，该问题源于模态直接影响而非适当的多模态融合。研究构建了structural causal graphs，并通过counterfactual analysis估计Natural Direct Effect (NDE)，包括使用扰动图像表示和退化输入来识别并动态调整模态依赖的test-time intervention模块。实验结果表明，该方法显著降低了hallucination，同时保留了任务性能，并提供了公开代码以提升可重复性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06169v2",
      "published_date": "2025-03-08 11:13:05 UTC",
      "updated_date": "2025-03-17 08:11:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:33:33.884380"
    },
    {
      "arxiv_id": "2503.06166v2",
      "title": "Secure On-Device Video OOD Detection Without Backpropagation",
      "title_zh": "安全设备端视频 OOD 检测",
      "authors": [
        "Shawn Li",
        "Peilin Cai",
        "Yuxiao Zhou",
        "Zhiyu Ni",
        "Renjie Liang",
        "You Qin",
        "Yi Nian",
        "Zhengzhong Tu",
        "Xiyang Hu",
        "Yue Zhao"
      ],
      "abstract": "Out-of-Distribution (OOD) detection is critical for ensuring the reliability\nof machine learning models in safety-critical applications such as autonomous\ndriving and medical diagnosis. While deploying personalized OOD detection\ndirectly on edge devices is desirable, it remains challenging due to large\nmodel sizes and the computational infeasibility of on-device training.\nFederated learning partially addresses this but still requires gradient\ncomputation and backpropagation, exceeding the capabilities of many edge\ndevices. To overcome these challenges, we propose SecDOOD, a secure\ncloud-device collaboration framework for efficient on-device OOD detection\nwithout requiring device-side backpropagation. SecDOOD utilizes cloud resources\nfor model training while ensuring user data privacy by retaining sensitive\ninformation on-device. Central to SecDOOD is a HyperNetwork-based personalized\nparameter generation module, which adapts cloud-trained models to\ndevice-specific distributions by dynamically generating local weight\nadjustments, effectively combining central and local information without local\nfine-tuning. Additionally, our dynamic feature sampling and encryption strategy\nselectively encrypts only the most informative feature channels, largely\nreducing encryption overhead without compromising detection performance.\nExtensive experiments across multiple datasets and OOD scenarios demonstrate\nthat SecDOOD achieves performance comparable to fully fine-tuned models,\nenabling secure, efficient, and personalized OOD detection on resource-limited\nedge devices. To enhance accessibility and reproducibility, our code is\npublicly available at https://github.com/Dystopians/SecDOOD.",
      "tldr_zh": "这篇论文提出 SecDOOD，一个安全的云-设备协作框架，用于实现无需设备端 Backpropagation 的视频 Out-of-Distribution (OOD) 检测，解决边缘设备资源限制和隐私挑战的问题。该框架的核心是基于 HyperNetwork 的个性化参数生成模块，通过云端训练模型并动态生成本地权重调整，结合中央和本地信息实现高效适应，而动态特征采样和加密策略则选择性地加密关键特征通道以降低开销。实验在多个数据集和 OOD 场景中证明，SecDOOD 的性能可媲美完全微调模型，为资源有限的边缘设备提供安全的、个性化的 OOD 检测解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06166v2",
      "published_date": "2025-03-08 11:03:21 UTC",
      "updated_date": "2025-03-17 07:44:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:33:45.257243"
    },
    {
      "arxiv_id": "2503.06163v2",
      "title": "VACT: A Video Automatic Causal Testing System and a Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Haotong Yang",
        "Qingyuan Zheng",
        "Yunjian Gao",
        "Yongkun Yang",
        "Yangbo He",
        "Zhouchen Lin",
        "Muhan Zhang"
      ],
      "abstract": "With the rapid advancement of text-conditioned Video Generation Models\n(VGMs), the quality of generated videos has significantly improved, bringing\nthese models closer to functioning as ``*world simulators*'' and making\nreal-world-level video generation more accessible and cost-effective. However,\nthe generated videos often contain factual inaccuracies and lack understanding\nof fundamental physical laws. While some previous studies have highlighted this\nissue in limited domains through manual analysis, a comprehensive solution has\nnot yet been established, primarily due to the absence of a generalized,\nautomated approach for modeling and assessing the causal reasoning of these\nmodels across diverse scenarios. To address this gap, we propose VACT: an\n**automated** framework for modeling, evaluating, and measuring the causal\nunderstanding of VGMs in real-world scenarios. By combining causal analysis\ntechniques with a carefully designed large language model assistant, our system\ncan assess the causal behavior of models in various contexts without human\nannotation, which offers strong generalization and scalability. Additionally,\nwe introduce multi-level causal evaluation metrics to provide a detailed\nanalysis of the causal performance of VGMs. As a demonstration, we use our\nframework to benchmark several prevailing VGMs, offering insight into their\ncausal reasoning capabilities. Our work lays the foundation for systematically\naddressing the causal understanding deficiencies in VGMs and contributes to\nadvancing their reliability and real-world applicability.",
      "tldr_zh": "本文提出 VACT，这是一个自动化的视频因果测试系统，用于评估文本条件视频生成模型（VGMs）的因果理解能力，以解决这些模型在生成视频时存在的事实不准确和物理定律认知不足的问题。VACT 框架结合因果分析技术和大型语言模型助手，实现无人工标注的建模和评估，并引入多级因果评估指标，提供对 VGMs 性能的详细分析。作为示范，该系统对多个主流 VGMs 进行了基准测试，揭示了它们的因果推理能力，并为提升模型的可靠性和实际应用奠定基础。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "A preliminary version of this paper has been accepted by workshop\n  SCSL@ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06163v2",
      "published_date": "2025-03-08 10:54:42 UTC",
      "updated_date": "2025-04-20 02:24:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:33:56.803316"
    },
    {
      "arxiv_id": "2503.06161v1",
      "title": "Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical Deformable Scene Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Li",
        "Junhao Wang",
        "William Han",
        "Ding Zhao"
      ],
      "abstract": "Minimally invasive surgery (MIS) has transformed clinical practice by\nreducing recovery times, minimizing complications, and enhancing precision.\nNonetheless, MIS inherently relies on indirect visualization and precise\ninstrument control, posing unique challenges. Recent advances in artificial\nintelligence have enabled real-time surgical scene understanding through\ntechniques such as image classification, object detection, and segmentation,\nwith scene reconstruction emerging as a key element for enhanced intraoperative\nguidance. Although neural radiance fields (NeRFs) have been explored for this\npurpose, their substantial data requirements and slow rendering inhibit\nreal-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more\nefficient alternative, achieving state-of-the-art performance in dynamic\nsurgical scene reconstruction. In this work, we introduce Feature-EndoGaussian\n(FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D\nrendering to enable real-time semantic and scene reconstruction. By leveraging\npretrained segmentation foundation models, FEG incorporates semantic feature\ndistillation within the Gaussian deformation framework, thereby enhancing both\nreconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG\nachieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03)\ncompared to leading methods. Additionally, on the EndoVis18 dataset, FEG\ndemonstrates competitive class-wise segmentation metrics while balancing model\nsize and real-time performance.",
      "tldr_zh": "该研究针对微创手术（MIS）的间接可视化和精确控制挑战，提出Feature-EndoGaussian (FEG)，一种基于3D Gaussian Splatting (3DGS)的扩展框架，将2D分割线索整合到3D渲染中，实现实时语义和场景重建。FEG通过利用预训练分割基础模型进行语义特征蒸馏（Feature Distilled），在Gaussian变形框架内提升重建保真度和分割准确性，从而解决NeRFs的高数据需求和慢渲染问题。在EndoNeRF数据集上，FEG 取得了优越性能（SSIM 0.97、PSNR 39.08、LPIPS 0.03），而在EndoVis18数据集上，展示了竞争性的类别分割指标，同时保持模型大小和实时性能的平衡。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06161v1",
      "published_date": "2025-03-08 10:50:19 UTC",
      "updated_date": "2025-03-08 10:50:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:34:08.328310"
    },
    {
      "arxiv_id": "2503.06157v1",
      "title": "UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces",
      "title_zh": "翻译失败",
      "authors": [
        "Baining Zhao",
        "Jianjie Fang",
        "Zichao Dai",
        "Ziyou Wang",
        "Jirong Zha",
        "Weichen Zhang",
        "Chen Gao",
        "Yue Wang",
        "Jinqiang Cui",
        "Xinlei Chen",
        "Yong Li"
      ],
      "abstract": "Large multimodal models exhibit remarkable intelligence, yet their embodied\ncognitive abilities during motion in open-ended urban 3D space remain to be\nexplored. We introduce a benchmark to evaluate whether video-large language\nmodels (Video-LLMs) can naturally process continuous first-person visual\nobservations like humans, enabling recall, perception, reasoning, and\nnavigation. We have manually control drones to collect 3D embodied motion video\ndata from real-world cities and simulated environments, resulting in 1.5k video\nclips. Then we design a pipeline to generate 5.2k multiple-choice questions.\nEvaluations of 17 widely-used Video-LLMs reveal current limitations in urban\nembodied cognition. Correlation analysis provides insight into the\nrelationships between different tasks, showing that causal reasoning has a\nstrong correlation with recall, perception, and navigation, while the abilities\nfor counterfactual and associative reasoning exhibit lower correlation with\nother tasks. We also validate the potential for Sim-to-Real transfer in urban\nembodiment through fine-tuning.",
      "tldr_zh": "该论文引入了 UrbanVideo-Bench 基准，用于评估 Vision-Language Models 在城市 3D 空间的 Embodied Intelligence，通过处理连续第一人称视频数据来测试模型的回忆、感知、推理和导航能力。研究团队使用无人机收集了 1.5k 个真实和模拟城市视频剪辑，并生成 5.2k 个多项选择题，对 17 个 Video-LLMs 进行评估，揭示了这些模型在城市认知方面的局限性。分析显示，因果推理与回忆、感知和导航高度相关，而反事实和联想推理与其他任务的相关性较低；此外，论文验证了通过微调实现 Sim-to-Real 转移的潜力，为提升模型在真实环境中的表现提供了新路径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.06157v1",
      "published_date": "2025-03-08 10:47:05 UTC",
      "updated_date": "2025-03-08 10:47:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:34:20.260040"
    },
    {
      "arxiv_id": "2503.05858v3",
      "title": "Bimodal Connection Attention Fusion for Speech Emotion Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Jiachen Luo",
        "Huy Phan",
        "Lin Wang",
        "Joshua D. Reiss"
      ],
      "abstract": "Multi-modal emotion recognition is challenging due to the difficulty of\nextracting features that capture subtle emotional differences. Understanding\nmulti-modal interactions and connections is key to building effective bimodal\nspeech emotion recognition systems. In this work, we propose Bimodal Connection\nAttention Fusion (BCAF) method, which includes three main modules: the\ninteractive connection network, the bimodal attention network, and the\ncorrelative attention network. The interactive connection network uses an\nencoder-decoder architecture to model modality connections between audio and\ntext while leveraging modality-specific features. The bimodal attention network\nenhances semantic complementation and exploits intra- and inter-modal\ninteractions. The correlative attention network reduces cross-modal noise and\ncaptures correlations between audio and text. Experiments on the MELD and\nIEMOCAP datasets demonstrate that the proposed BCAF method outperforms existing\nstate-of-the-art baselines.",
      "tldr_zh": "本研究针对多模态情感识别的挑战，提出了一种Bimodal Connection Attention Fusion (BCAF)方法，以更好地捕捉音频和文本之间的互动和连接。BCAF包括三个主要模块：Interactive Connection Network使用编码器-解码器架构建模模态连接、Bimodal Attention Network增强语义互补并利用模态内及模态间互动，以及Correlative Attention Network减少跨模态噪声并捕捉相关性。在MELD和IEMOCAP数据集上的实验表明，BCAF方法优于现有最先进基线，提升了语音情感识别的性能。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05858v3",
      "published_date": "2025-03-08 10:20:57 UTC",
      "updated_date": "2025-03-22 11:48:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:34:31.504459"
    },
    {
      "arxiv_id": "2503.06144v1",
      "title": "Exploring the usage of Probabilistic Neural Networks for Ionospheric electron density estimation",
      "title_zh": "探索概率神经网络在电离层电子密度估计中的应用",
      "authors": [
        "Miquel Garcia-Fernandez"
      ],
      "abstract": "A fundamental limitation of traditional Neural Networks (NN) in predictive\nmodelling is their inability to quantify uncertainty in their outputs. In\ncritical applications like positioning systems, understanding the reliability\nof predictions is critical for constructing confidence intervals, early warning\nsystems, and effectively propagating results. For instance, Precise Point\nPositioning in satellite navigation heavily relies on accurate error models for\nancillary data (orbits, clocks, ionosphere, and troposphere) to compute precise\nerror estimates. In addition, these uncertainty estimates are needed to\nestablish robust protection levels in safety critical applications.\n  To address this challenge, the main objectives of this paper aims at\nexploring a potential framework capable of providing both point estimates and\nassociated uncertainty measures of ionospheric Vertical Total Electron Content\n(VTEC). In this context, Probabilistic Neural Networks (PNNs) offer a promising\napproach to achieve this goal. However, constructing an effective PNN requires\nmeticulous design of hidden and output layers, as well as careful definition of\nprior and posterior probability distributions for network weights and biases.\n  A key finding of this study is that the uncertainty provided by the PNN model\nin VTEC estimates may be systematically underestimated. In low-latitude areas,\nthe actual error was observed to be as much as twice the model's estimate. This\nunderestimation is expected to be more pronounced during solar maximum,\ncorrelating with increased VTEC values.",
      "tldr_zh": "本研究探讨了使用 Probabilistic Neural Networks (PNNs) 来估计电离层 Vertical Total Electron Content (VTEC)，以解决传统 Neural Networks (NN) 在预测建模中无法量化输出不确定性的问题，尤其在定位系统和卫星导航等关键应用中。论文提出了一种框架，通过精心设计 PNN 的隐藏层、输出层以及权重和偏差的先验和后验概率分布，来同时提供 VTEC 的点估计和不确定性测量。实验结果显示，PNN 的不确定性估计可能系统性低估，在低纬度地区实际错误可达模型估计的两倍，且这种问题在太阳极大期更为显著。该框架为构建更可靠的错误模型和保护水平提供了潜在改进路径。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "86A10, 62M45,",
        "I.2.6; G.3; J.2"
      ],
      "primary_category": "eess.SP",
      "comment": "13 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06144v1",
      "published_date": "2025-03-08 10:06:15 UTC",
      "updated_date": "2025-03-08 10:06:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:34:44.009039"
    },
    {
      "arxiv_id": "2503.06138v2",
      "title": "System 0/1/2/3: Quad-process theory for multi-timescale embodied collective cognitive systems",
      "title_zh": "翻译失败",
      "authors": [
        "Tadahiro Taniguchi",
        "Yasushi Hirai",
        "Masahiro Suzuki",
        "Shingo Murata",
        "Takato Horii",
        "Kazutoshi Tanaka"
      ],
      "abstract": "This paper introduces the System 0/1/2/3 framework as an extension of\ndual-process theory, employing a quad-process model of cognition. Expanding\nupon System 1 (fast, intuitive thinking) and System 2 (slow, deliberative\nthinking), we incorporate System 0, which represents pre-cognitive embodied\nprocesses, and System 3, which encompasses collective intelligence and symbol\nemergence. We contextualize this model within Bergson's philosophy by adopting\nmulti-scale time theory to unify the diverse temporal dynamics of cognition.\nSystem 0 emphasizes morphological computation and passive dynamics,\nillustrating how physical embodiment enables adaptive behavior without explicit\nneural processing. Systems 1 and 2 are explained from a constructive\nperspective, incorporating neurodynamical and AI viewpoints. In System 3, we\nintroduce collective predictive coding to explain how societal-level adaptation\nand symbol emergence operate over extended timescales. This comprehensive\nframework ranges from rapid embodied reactions to slow-evolving collective\nintelligence, offering a unified perspective on cognition across multiple\ntimescales, levels of abstraction, and forms of human intelligence. The System\n0/1/2/3 model provides a novel theoretical foundation for understanding the\ninterplay between adaptive and cognitive processes, thereby opening new avenues\nfor research in cognitive science, AI, robotics, and collective intelligence.",
      "tldr_zh": "本论文提出 System 0/1/2/3 框架，作为 dual-process theory 的扩展，引入 quad-process 模型以涵盖多时间尺度认知，包括 System 0（pre-cognitive embodied processes，强调 morphological computation 和被动动态）、System 1（快速直觉思考）和 System 2（缓慢审议思考）的传统元素，以及 System 3（collective intelligence and symbol emergence，通过 collective predictive coding 解释社会适应）。该框架借鉴 Bergson 的 multi-scale time theory，从具身和集体视角统一认知过程的多样动态。整体模型为探索适应性与认知过程的互动提供新理论基础，适用于认知科学、AI、机器人和集体智能领域的研究。",
      "categories": [
        "cs.AI",
        "cs.RO",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2503.06138v2",
      "published_date": "2025-03-08 09:31:53 UTC",
      "updated_date": "2025-03-13 23:45:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:34:57.590412"
    },
    {
      "arxiv_id": "2503.06136v1",
      "title": "GSV3D: Gaussian Splatting-based Geometric Distillation with Stable Video Diffusion for Single-Image 3D Object Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Ye Tao",
        "Jiawei Zhang",
        "Yahao Shi",
        "Dongqing Zou",
        "Bin Zhou"
      ],
      "abstract": "Image-based 3D generation has vast applications in robotics and gaming, where\nhigh-quality, diverse outputs and consistent 3D representations are crucial.\nHowever, existing methods have limitations: 3D diffusion models are limited by\ndataset scarcity and the absence of strong pre-trained priors, while 2D\ndiffusion-based approaches struggle with geometric consistency. We propose a\nmethod that leverages 2D diffusion models' implicit 3D reasoning ability while\nensuring 3D consistency via Gaussian-splatting-based geometric distillation.\nSpecifically, the proposed Gaussian Splatting Decoder enforces 3D consistency\nby transforming SV3D latent outputs into an explicit 3D representation. Unlike\nSV3D, which only relies on implicit 2D representations for video generation,\nGaussian Splatting explicitly encodes spatial and appearance attributes,\nenabling multi-view consistency through geometric constraints. These\nconstraints correct view inconsistencies, ensuring robust geometric\nconsistency. As a result, our approach simultaneously generates high-quality,\nmulti-view-consistent images and accurate 3D models, providing a scalable\nsolution for single-image-based 3D generation and bridging the gap between 2D\nDiffusion diversity and 3D structural coherence. Experimental results\ndemonstrate state-of-the-art multi-view consistency and strong generalization\nacross diverse datasets. The code will be made publicly available upon\nacceptance.",
      "tldr_zh": "本研究提出GSV3D方法，旨在解决基于图像的3D对象生成问题，利用Stable Video Diffusion的隐式3D推理能力，同时通过Gaussian Splatting-based geometric distillation确保几何一致性。Gaussian Splatting Decoder将SV3D的输出转换为显式3D表示，显式编码空间和外观属性，以纠正多视图不一致性，从而生成高质量、多视图一致的图像和准确3D模型。实验结果显示，该方法在多视图一致性和不同数据集上的泛化能力达到state-of-the-art水平，为单图像3D生成提供可扩展解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06136v1",
      "published_date": "2025-03-08 09:10:31 UTC",
      "updated_date": "2025-03-08 09:10:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:35:09.220267"
    },
    {
      "arxiv_id": "2503.10659v1",
      "title": "MARRO: Multi-headed Attention for Rhetorical Role Labeling in Legal Documents",
      "title_zh": "翻译失败",
      "authors": [
        "Purbid Bambroo",
        "Subinay Adhikary",
        "Paheli Bhattacharya",
        "Abhijnan Chakraborty",
        "Saptarshi Ghosh",
        "Kripabandhu Ghosh"
      ],
      "abstract": "Identification of rhetorical roles like facts, arguments, and final judgments\nis central to understanding a legal case document and can lend power to other\ndownstream tasks like legal case summarization and judgment prediction.\nHowever, there are several challenges to this task. Legal documents are often\nunstructured and contain a specialized vocabulary, making it hard for\nconventional transformer models to understand them. Additionally, these\ndocuments run into several pages, which makes it difficult for neural models to\ncapture the entire context at once. Lastly, there is a dearth of annotated\nlegal documents to train deep learning models. Previous state-of-the-art\napproaches for this task have focused on using neural models like BiLSTM-CRF or\nhave explored different embedding techniques to achieve decent results. While\nsuch techniques have shown that better embedding can result in improved model\nperformance, not many models have focused on utilizing attention for learning\nbetter embeddings in sentences of a document. Additionally, it has been\nrecently shown that advanced techniques like multi-task learning can help the\nmodels learn better representations, thereby improving performance. In this\npaper, we combine these two aspects by proposing a novel family of multi-task\nlearning-based models for rhetorical role labeling, named MARRO, that uses\ntransformer-inspired multi-headed attention. Using label shift as an auxiliary\ntask, we show that models from the MARRO family achieve state-of-the-art\nresults on two labeled datasets for rhetorical role labeling, from the Indian\nand UK Supreme Courts.",
      "tldr_zh": "该论文针对法律文档中的修辞角色标记（rhetorical role labeling）问题，提出了一种名为 MARRO 的新型多任务学习（multi-task learning）模型家族，利用 transformer-inspired 的 multi-headed attention 来更好地捕捉文档上下文和专业词汇。MARRO 通过将标签偏移（label shift）作为辅助任务，帮助模型克服法律文件结构不佳、长度过长和标注数据不足的挑战。实验结果显示，该模型在印度和英国最高法院的两个标注数据集上取得了 state-of-the-art 性能，显著提升了下游任务如法律案例总结和判断预测的准确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10659v1",
      "published_date": "2025-03-08 08:05:20 UTC",
      "updated_date": "2025-03-08 08:05:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:35:19.975152"
    },
    {
      "arxiv_id": "2504.13848v1",
      "title": "From Interaction to Collaboration: How Hybrid Intelligence Enhances Chatbot Feedback",
      "title_zh": "从互动到协作：混合智能如何提升聊天机器人反馈",
      "authors": [
        "Janet Rafner",
        "Ryan Q. Guloy",
        "Eden W. Wen",
        "Catherine M. Chiodo",
        "Jacob Sherson"
      ],
      "abstract": "Generative AI (GenAI) chatbots are becoming increasingly integrated into\nvirtual assistant technologies, yet their success hinges on the ability to\ngather meaningful user feedback to improve interaction quality, system\noutcomes, and overall user acceptance. Successful chatbot interactions can\nenable organizations to build long-term relationships with their customers and\nusers, supporting customer loyalty and furthering the organization's goals.\nThis study explores the impact of two distinct narratives and feedback\ncollection mechanisms on user engagement and feedback behavior: a standard\nAI-focused interaction versus a hybrid intelligence (HI) framed interaction.\nInitial findings indicate that while small-scale survey measures allowed for no\nsignificant differences in user willingness to leave feedback, use the system,\nor trust the system, participants exposed to the HI narrative statistically\nsignificantly provided more detailed feedback. These initial findings offer\ninsights into designing effective feedback systems for GenAI virtual\nassistants, balancing user effort with system improvement potential.",
      "tldr_zh": "本研究探讨了混合智能(Hybrid Intelligence, HI)如何提升Generative AI (GenAI)聊天机器人的用户反馈机制，通过比较标准AI交互和HI框架交互，评估其对用户参与和反馈行为的影响。研究采用小规模调查方法，发现HI叙述虽然未显著改变用户反馈意愿、使用意愿或系统信任，但显著增加了反馈的详细程度。这些初步发现为设计有效的GenAI虚拟助手反馈系统提供了指导，帮助平衡用户努力与系统改进潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.13848v1",
      "published_date": "2025-03-08 07:36:36 UTC",
      "updated_date": "2025-03-08 07:36:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:35:33.479057"
    },
    {
      "arxiv_id": "2503.06108v1",
      "title": "Multi-modal expressive personality recognition in data non-ideal audiovisual based on multi-scale feature enhancement and modal augment",
      "title_zh": "翻译失败",
      "authors": [
        "Weixuan Kong",
        "Jinpeng Yu",
        "Zijun Li",
        "Hanwei Liu",
        "Jiqing Qu",
        "Hui Xiao",
        "Xuefeng Li"
      ],
      "abstract": "Automatic personality recognition is a research hotspot in the intersection\nof computer science and psychology, and in human-computer interaction,\npersonalised has a wide range of applications services and other scenarios. In\nthis paper, an end-to-end multimodal performance personality is established for\nboth visual and auditory modal datarecognition network , and the through\nfeature-level fusion , which effectively of the two modalities is carried out\nthe cross-attention mechanismfuses the features of the two modal data; and a is\nproposed multiscale feature enhancement modalitiesmodule , which enhances for\nvisual and auditory boththe expression of the information of effective the\nfeatures and suppresses the interference of the redundant information. In\naddition, during the training process, this paper proposes a modal enhancement\ntraining strategy to simulate non-ideal such as modal loss and noise\ninterferencedata situations , which enhances the adaptability ofand the model\nto non-ideal data scenarios improves the robustness of the model. Experimental\nresults show that the method proposed in this paper is able to achieve an\naverage Big Five personality accuracy of , which outperforms existing 0.916 on\nthe personality analysis dataset ChaLearn First Impressionother methods based\non audiovisual and audio-visual both modalities. The ablation experiments also\nvalidate our proposed , respectivelythe contribution of module and modality\nenhancement strategy to the model performance. Finally, we simulate in the\ninference phase multi-scale feature enhancement six non-ideal data scenarios to\nverify the modal enhancement strategy's improvement in model robustness.",
      "tldr_zh": "本文提出了一种基于多模态（视觉和听觉）的数据非理想情况下的表达个性识别方法，通过特征级融合和交叉注意力机制（cross-attention mechanism）融合模态特征，并引入多尺度特征增强模块（multi-scale feature enhancement module）来增强有效信息并抑制冗余干扰。 此外，该方法采用模态增强训练策略（modal enhancement training strategy）模拟模态丢失和噪声等非理想场景，提高模型的适应性和鲁棒性。 在ChaLearn First Impression数据集上，实验结果显示该方法的Big Five个性平均准确率达到0.916，优于现有方法，且消融实验证实了各模块的贡献。",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06108v1",
      "published_date": "2025-03-08 07:20:44 UTC",
      "updated_date": "2025-03-08 07:20:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:35:47.190136"
    },
    {
      "arxiv_id": "2503.06107v1",
      "title": "Feature Fusion Attention Network with CycleGAN for Image Dehazing, De-Snowing and De-Raining",
      "title_zh": "翻译失败",
      "authors": [
        "Akshat Jain"
      ],
      "abstract": "This paper presents a novel approach to image dehazing by combining Feature\nFusion Attention (FFA) networks with CycleGAN architecture. Our method\nleverages both supervised and unsupervised learning techniques to effectively\nremove haze from images while preserving crucial image details. The proposed\nhybrid architecture demonstrates significant improvements in image quality\nmetrics, achieving superior PSNR and SSIM scores compared to traditional\ndehazing methods. Through extensive experimentation on the RESIDE and DenseHaze\nCVPR 2019 dataset, we show that our approach effectively handles both synthetic\nand real-world hazy images. CycleGAN handles the unpaired nature of hazy and\nclean images effectively, enabling the model to learn mappings even without\npaired data.",
      "tldr_zh": "本研究提出了一种结合 Feature Fusion Attention (FFA) 网络和 CycleGAN 的新方法，用于图像去雾、去雪和去雨。该方法利用监督和无监督学习技术，有效去除图像中的不利因素，同时保留关键细节，提升图像质量指标，如 PSNR 和 SSIM 分数。实验在 RESIDE 和 DenseHaze CVPR 2019 数据集上进行，证明该方法在处理合成和真实世界图像时，比传统方法表现出29.32%的准确率提升。CycleGAN 的应用特别解决了未配对数据的问题，使模型能够更灵活地学习映射关系。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06107v1",
      "published_date": "2025-03-08 07:18:42 UTC",
      "updated_date": "2025-03-08 07:18:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:35:57.091923"
    },
    {
      "arxiv_id": "2503.06101v1",
      "title": "ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning",
      "title_zh": "ULTHO：超轻量级却高效的深度强化学习超参数优化",
      "authors": [
        "Mingqi Yuan",
        "Bo Li",
        "Xin Jin",
        "Wenjun Zeng"
      ],
      "abstract": "Hyperparameter optimization (HPO) is a billion-dollar problem in machine\nlearning, which significantly impacts the training efficiency and model\nperformance. However, achieving efficient and robust HPO in deep reinforcement\nlearning (RL) is consistently challenging due to its high non-stationarity and\ncomputational cost. To tackle this problem, existing approaches attempt to\nadapt common HPO techniques (e.g., population-based training or Bayesian\noptimization) to the RL scenario. However, they remain sample-inefficient and\ncomputationally expensive, which cannot facilitate a wide range of\napplications. In this paper, we propose ULTHO, an ultra-lightweight yet\npowerful framework for fast HPO in deep RL within single runs. Specifically, we\nformulate the HPO process as a multi-armed bandit with clustered arms (MABC)\nand link it directly to long-term return optimization. ULTHO also provides a\nquantified and statistical perspective to filter the HPs efficiently. We test\nULTHO on benchmarks including ALE, Procgen, MiniGrid, and PyBullet. Extensive\nexperiments demonstrate that the ULTHO can achieve superior performance with\nsimple architecture, contributing to the development of advanced and automated\nRL systems.",
      "tldr_zh": "这篇论文针对深度强化学习(Deep RL)中的超参数优化(HPO)问题，提出了一种超轻量级框架ULTHO，以解决现有方法的样本效率低和计算成本高的挑战。ULTHO将HPO过程表述为多臂赌博机与聚类臂(Multi-Armed Bandit with Clustered Arms, MABC)，并将其直接与长期回报优化关联，同时提供量化统计视角来高效过滤超参数。在ALE、Procgen、MiniGrid和PyBullet等基准上的实验显示，ULTHO在简单架构下表现出色，有助于发展先进的自动RL系统。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 22 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06101v1",
      "published_date": "2025-03-08 07:03:43 UTC",
      "updated_date": "2025-03-08 07:03:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:36:10.631465"
    },
    {
      "arxiv_id": "2503.06092v1",
      "title": "ZO-DARTS++: An Efficient and Size-Variable Zeroth-Order Neural Architecture Search Algorithm",
      "title_zh": "ZO-DARTS++：高效且尺寸可变的零阶神经架构搜索算法",
      "authors": [
        "Lunchen Xie",
        "Eugenio Lomurno",
        "Matteo Gambella",
        "Danilo Ardagna",
        "Manual Roveri",
        "Matteo Matteucci",
        "Qingjiang Shi"
      ],
      "abstract": "Differentiable Neural Architecture Search (NAS) provides a promising avenue\nfor automating the complex design of deep learning (DL) models. However,\ncurrent differentiable NAS methods often face constraints in efficiency,\noperation selection, and adaptability under varying resource limitations. We\nintroduce ZO-DARTS++, a novel NAS method that effectively balances performance\nand resource constraints. By integrating a zeroth-order approximation for\nefficient gradient handling, employing a sparsemax function with temperature\nannealing for clearer and more interpretable architecture distributions, and\nadopting a size-variable search scheme for generating compact yet accurate\narchitectures, ZO-DARTS++ establishes a new balance between model complexity\nand performance. In extensive tests on medical imaging datasets, ZO-DARTS++\nimproves the average accuracy by up to 1.8\\% over standard DARTS-based methods\nand shortens search time by approximately 38.6\\%. Additionally, its\nresource-constrained variants can reduce the number of parameters by more than\n35\\% while maintaining competitive accuracy levels. Thus, ZO-DARTS++ offers a\nversatile and efficient framework for generating high-quality, resource-aware\nDL models suitable for real-world medical applications.",
      "tldr_zh": "该研究提出ZO-DARTS++，一种高效且可变大小的神经架构搜索(NAS)算法，旨在解决传统可微NAS方法在效率、操作选择和资源适应性方面的局限性。通过整合zeroth-order approximation来优化梯度处理、采用sparsemax function with temperature annealing来实现更清晰的架构分布，以及引入size-variable search scheme来生成紧凑且高性能的模型，该算法实现了性能与资源约束的平衡。在医疗图像数据集上的实验中，ZO-DARTS++比标准DARTS方法提高了平均准确率1.8%、缩短了搜索时间38.6%，并能将参数数量减少超过35%同时保持竞争性准确率，从而为资源受限的真实医疗应用提供高效框架。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.5.1; I.5.4; I.2.6; I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06092v1",
      "published_date": "2025-03-08 06:43:33 UTC",
      "updated_date": "2025-03-08 06:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:36:23.316025"
    },
    {
      "arxiv_id": "2503.06083v1",
      "title": "T-CBF: Traversability-based Control Barrier Function to Navigate Vertically Challenging Terrain",
      "title_zh": "T-CBF：基于可穿越性的控制屏障函数，用于导航垂直挑战地形",
      "authors": [
        "Manas Gupta",
        "Xuesu Xiao"
      ],
      "abstract": "Safety has been of paramount importance in motion planning and control\ntechniques and is an active area of research in the past few years. Most safety\nresearch for mobile robots target at maintaining safety with the notion of\ncollision avoidance. However, safety goes beyond just avoiding collisions,\nespecially when robots have to navigate unstructured, vertically challenging,\noff-road terrain, where vehicle rollover and immobilization is as critical as\ncollisions. In this work, we introduce a novel Traversability-based Control\nBarrier Function (T-CBF), in which we use neural Control Barrier Functions\n(CBFs) to achieve safety beyond collision avoidance on unstructured vertically\nchallenging terrain by reasoning about new safety aspects in terms of\ntraversability. The neural T-CBF trained on safe and unsafe observations\nspecific to traversability safety is then used to generate safe trajectories.\nFurthermore, we present experimental results in simulation and on a physical\nVerti-4 Wheeler (V4W) platform, demonstrating that T-CBF can provide\ntraversability safety while reaching the goal position. T-CBF planner\noutperforms previously developed planners by 30\\% in terms of keeping the robot\nsafe and mobile when navigating on real world vertically challenging terrain.",
      "tldr_zh": "该研究提出了一种基于 Traversability 的控制屏障函数（T-CBF），旨在帮助移动机器人安全导航非结构化、垂直挑战的越野地形，超越传统碰撞避免的安全范畴，同时考虑车辆翻滚和卡住风险。T-CBF 利用神经 Control Barrier Functions (CBFs) 通过训练安全和不安全观察数据来生成安全的轨迹，确保机器人保持移动性。实验在模拟环境和实际 Verti-4 Wheeler (V4W) 平台上验证，T-CBF 规划器在保持机器人安全和到达目标位置方面比现有方法提高了 30%。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06083v1",
      "published_date": "2025-03-08 06:12:38 UTC",
      "updated_date": "2025-03-08 06:12:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:36:35.373510"
    },
    {
      "arxiv_id": "2503.07661v1",
      "title": "Disrupting Model Merging: A Parameter-Level Defense Without Sacrificing Accuracy",
      "title_zh": "破坏模型合并：一种不牺牲准确性的参数级防御",
      "authors": [
        "Wei Junhao",
        "Yu Zhe",
        "Sakuma Jun"
      ],
      "abstract": "Model merging is a technique that combines multiple finetuned models into a\nsingle model without additional training, allowing a free-rider to cheaply\ninherit specialized capabilities. This study investigates methodologies to\nsuppress unwanted model merging by free-riders. Existing methods such as model\nwatermarking or fingerprinting can only detect merging in hindsight. In\ncontrast, we propose a first proactive defense against model merging.\nSpecifically, our defense method modifies the model parameters so that the\nmodel is disrupted if the model is merged with any other model, while its\nfunctionality is kept unchanged if not merged with others. Our approach\nconsists of two modules, rearranging MLP parameters and scaling attention\nheads, which push the model out of the shared basin in parameter space, causing\nthe merging performance with other models to degrade significantly. We conduct\nextensive experiments on image classification, image generation, and text\nclassification to demonstrate that our defense severely disrupts merging while\nretaining the functionality of the post-protect model. Moreover, we analyze\npotential adaptive attacks and further propose a dropout-based pruning to\nimprove our proposal's robustness.",
      "tldr_zh": "该研究针对模型合并(Model merging)技术提出了一种参数级主动防御方法，旨在防止免费骑手(Free-rider)未经授权继承模型能力，而不牺牲模型的准确性。该方法通过重排MLP参数(Rearranging MLP parameters)和缩放注意力头(Scaling attention heads)来修改模型参数，使其在参数空间中脱离共享区域，从而显著降低与其它模型合并后的性能。实验在图像分类、图像生成和文本分类任务上验证，该防御策略严重破坏了合并效果，同时保持了保护后模型的功能完整性。此外，研究分析了潜在的自适应攻击，并引入基于dropout的剪枝机制来提升防御的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.07661v1",
      "published_date": "2025-03-08 06:08:47 UTC",
      "updated_date": "2025-03-08 06:08:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:36:46.646177"
    },
    {
      "arxiv_id": "2503.06074v1",
      "title": "Towards Conversational AI for Disease Management",
      "title_zh": "迈向疾病管理的对话式人工智能",
      "authors": [
        "Anil Palepu",
        "Valentin Liévin",
        "Wei-Hung Weng",
        "Khaled Saab",
        "David Stutz",
        "Yong Cheng",
        "Kavita Kulkarni",
        "S. Sara Mahdavi",
        "Joëlle Barral",
        "Dale R. Webster",
        "Katherine Chou",
        "Avinatan Hassidim",
        "Yossi Matias",
        "James Manyika",
        "Ryutaro Tanno",
        "Vivek Natarajan",
        "Adam Rodman",
        "Tao Tu",
        "Alan Karthikesalingam",
        "Mike Schaekermann"
      ],
      "abstract": "While large language models (LLMs) have shown promise in diagnostic dialogue,\ntheir capabilities for effective management reasoning - including disease\nprogression, therapeutic response, and safe medication prescription - remain\nunder-explored. We advance the previously demonstrated diagnostic capabilities\nof the Articulate Medical Intelligence Explorer (AMIE) through a new LLM-based\nagentic system optimised for clinical management and dialogue, incorporating\nreasoning over the evolution of disease and multiple patient visit encounters,\nresponse to therapy, and professional competence in medication prescription. To\nground its reasoning in authoritative clinical knowledge, AMIE leverages\nGemini's long-context capabilities, combining in-context retrieval with\nstructured reasoning to align its output with relevant and up-to-date clinical\npractice guidelines and drug formularies. In a randomized, blinded virtual\nObjective Structured Clinical Examination (OSCE) study, AMIE was compared to 21\nprimary care physicians (PCPs) across 100 multi-visit case scenarios designed\nto reflect UK NICE Guidance and BMJ Best Practice guidelines. AMIE was\nnon-inferior to PCPs in management reasoning as assessed by specialist\nphysicians and scored better in both preciseness of treatments and\ninvestigations, and in its alignment with and grounding of management plans in\nclinical guidelines. To benchmark medication reasoning, we developed RxQA, a\nmultiple-choice question benchmark derived from two national drug formularies\n(US, UK) and validated by board-certified pharmacists. While AMIE and PCPs both\nbenefited from the ability to access external drug information, AMIE\noutperformed PCPs on higher difficulty questions. While further research would\nbe needed before real-world translation, AMIE's strong performance across\nevaluations marks a significant step towards conversational AI as a tool in\ndisease management.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 在疾病管理中的应用，扩展了 Articulate Medical Intelligence Explorer (AMIE) 系统，使其能够处理疾病进展、治疗响应和安全用药等推理任务。AMIE 通过 Gemini 的长上下文能力结合检索增强生成 (in-context retrieval) 和结构化推理，与临床指南（如 UK NICE 和 BMJ Best Practice）对齐，确保输出精确可靠。在随机盲法虚拟 OSCE 研究中，AMIE 在100个多访病例中表现不劣于21名初级保健医生 (PCPs)，并在治疗和调查的精确性以及管理计划的指南一致性上得分更高。此外，开发的 RxQA 基准测试显示，AMIE 在高难度用药问题上优于 PCPs，这标志着对话式 AI 向疾病管理工具迈出重要一步，但需进一步研究以实现实际应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "62 pages, 7 figures in main text, 36 figures in appendix",
      "pdf_url": "http://arxiv.org/pdf/2503.06074v1",
      "published_date": "2025-03-08 05:48:58 UTC",
      "updated_date": "2025-03-08 05:48:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:36:59.648371"
    },
    {
      "arxiv_id": "2503.06073v1",
      "title": "GEM: Empowering MLLM for Grounded ECG Understanding with Time Series and Images",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Lan",
        "Feng Wu",
        "Kai He",
        "Qinghao Zhao",
        "Shenda Hong",
        "Mengling Feng"
      ],
      "abstract": "While recent multimodal large language models (MLLMs) have advanced automated\nECG interpretation, they still face two key limitations: (1) insufficient\nmultimodal synergy between time series signals and visual ECG representations,\nand (2) limited explainability in linking diagnoses to granular waveform\nevidence. We introduce GEM, the first MLLM unifying ECG time series, 12-lead\nECG images and text for grounded and clinician-aligned ECG interpretation. GEM\nenables feature-grounded analysis, evidence-driven reasoning, and a\nclinician-like diagnostic process through three core innovations: a\ndual-encoder framework extracting complementary time series and image features,\ncross-modal alignment for effective multimodal understanding, and\nknowledge-guided instruction generation for generating high-granularity\ngrounding data (ECG-Grounding) linking diagnoses to measurable parameters\n($e.g.$, QRS/PR Intervals). Additionally, we propose the Grounded ECG\nUnderstanding task, a clinically motivated benchmark designed to\ncomprehensively assess the MLLM's capability in grounded ECG understanding.\nExperimental results on both existing and our proposed benchmarks show GEM\nsignificantly improves predictive performance (CSN $7.4\\% \\uparrow$),\nexplainability ($22.7\\% \\uparrow$), and grounding ($24.8\\% \\uparrow$), making\nit more suitable for real-world clinical applications. GitHub repository:\nhttps://github.com/lanxiang1017/GEM.git",
      "tldr_zh": "该研究提出 GEM，一种增强多模态大语言模型 (MLLM) 的框架，用于基于时间序列和图像的 ECG 理解，旨在解决现有模型在多模态协同和诊断解释性不足的问题。GEM 通过双编码器框架提取互补的时间序列和图像特征、跨模态对齐实现有效融合，以及知识引导指令生成创建高粒度 grounding 数据 (如 ECG-Grounding)，从而支持证据驱动的诊断过程和临床对齐解释。同时，论文引入 Grounded ECG Understanding 基准进行评估，结果显示 GEM 在预测性能 (CSN 提升 7.4%)、解释性 (提升 22.7%) 和 grounding (提升 24.8%) 方面均有显著改进，更适用于实际临床应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06073v1",
      "published_date": "2025-03-08 05:48:53 UTC",
      "updated_date": "2025-03-08 05:48:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:37:12.973180"
    },
    {
      "arxiv_id": "2503.06072v2",
      "title": "Large Language Models Post-training: Surveying Techniques from Alignment to Reasoning",
      "title_zh": "大语言模型后训练：从对齐到推理的技术调查",
      "authors": [
        "Guiyao Tie",
        "Zeli Zhao",
        "Dingjie Song",
        "Fuyang Wei",
        "Rong Zhou",
        "Yurou Dai",
        "Wen Yin",
        "Zhejian Yang",
        "Jiangyue Yan",
        "Yao Su",
        "Zhenhan Dai",
        "Yifeng Xie",
        "Yihan Cao",
        "Lichao Sun",
        "Pan Zhou",
        "Lifang He",
        "Hechang Chen",
        "Yu Zhang",
        "Qingsong Wen",
        "Tianming Liu",
        "Neil Zhenqiang Gong",
        "Jiliang Tang",
        "Caiming Xiong",
        "Heng Ji",
        "Philip S. Yu",
        "Jianfeng Gao"
      ],
      "abstract": "The emergence of Large Language Models (LLMs) has fundamentally transformed\nnatural language processing, making them indispensable across domains ranging\nfrom conversational systems to scientific exploration. However, their\npre-trained architectures often reveal limitations in specialized contexts,\nincluding restricted reasoning capacities, ethical uncertainties, and\nsuboptimal domain-specific performance. These challenges necessitate advanced\npost-training language models (PoLMs) to address these shortcomings, such as\nOpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or\nLRMs). This paper presents the first comprehensive survey of PoLMs,\nsystematically tracing their evolution across five core paradigms: Fine-tuning,\nwhich enhances task-specific accuracy; Alignment, which ensures ethical\ncoherence and alignment with human preferences; Reasoning, which advances\nmulti-step inference despite challenges in reward design; Efficiency, which\noptimizes resource utilization amidst increasing complexity; Integration and\nAdaptation, which extend capabilities across diverse modalities while\naddressing coherence issues. Charting progress from ChatGPT's alignment\nstrategies to DeepSeek-R1's innovative reasoning advancements, we illustrate\nhow PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities,\nand enhance domain adaptability. Our contributions include a pioneering\nsynthesis of PoLM evolution, a structured taxonomy categorizing techniques and\ndatasets, and a strategic agenda emphasizing the role of LRMs in improving\nreasoning proficiency and domain flexibility. As the first survey of its scope,\nthis work consolidates recent PoLM advancements and establishes a rigorous\nintellectual framework for future research, fostering the development of LLMs\nthat excel in precision, ethical robustness, and versatility across scientific\nand societal applications.",
      "tldr_zh": "这篇论文首次对大型语言模型 (LLMs) 的后训练 (PoLMs) 技术进行全面调查，旨在解决模型在推理能力、伦理问题和领域特定性能方面的局限性。论文系统概述了五个核心范式：Fine-tuning（提升任务特定准确性）、Alignment（确保伦理一致性）、Reasoning（改进多步推理）、Efficiency（优化资源利用）和Integration and Adaptation（扩展多模态能力）。主要贡献包括构建一个结构化的taxonomy分类技术与数据集，以及提出未来研究议程，强调Large Reasoning Models (LRMs) 在深化推理能力和增强领域适应性中的关键作用。最终，该工作为开发更精确、伦理稳健且多领域的LLMs 提供了坚实框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "87 pages, 21 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.06072v2",
      "published_date": "2025-03-08 05:41:42 UTC",
      "updated_date": "2025-05-21 03:38:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:37:25.964513"
    },
    {
      "arxiv_id": "2503.06064v1",
      "title": "A Novel Trustworthy Video Summarization Algorithm Through a Mixture of LoRA Experts",
      "title_zh": "一种新型可靠视频摘要算法，通过 LoRA 专家的混合",
      "authors": [
        "Wenzhuo Du",
        "Gerun Wang",
        "Guancheng Chen",
        "Hang Zhao",
        "Xin Li",
        "Jian Gao"
      ],
      "abstract": "With the exponential growth of user-generated content on video-sharing\nplatforms, the challenge of facilitating efficient searching and browsing of\nvideos has garnered significant attention. To enhance users' ability to swiftly\nlocate and review pertinent videos, the creation of concise and informative\nvideo summaries has become increasingly important. Video-llama is an effective\ntool for generating video summarization, but it cannot effectively unify and\noptimize the modeling of temporal and spatial features and requires a lot of\ncomputational resources and time. Therefore, we propose MiLoRA-ViSum to more\nefficiently capture complex temporal dynamics and spatial relationships\ninherent in video data and to control the number of parameters for training. By\nextending traditional Low-Rank Adaptation (LoRA) into a sophisticated\nmixture-of-experts paradigm, MiLoRA-ViSum incorporates a dual temporal-spatial\nadaptation mechanism tailored specifically for video summarization tasks. This\napproach dynamically integrates specialized LoRA experts, each fine-tuned to\naddress distinct temporal or spatial dimensions. Extensive evaluations of the\nVideoXum and ActivityNet datasets demonstrate that MiLoRA-ViSum achieves the\nbest summarization performance compared to state-of-the-art models, while\nmaintaining significantly lower computational costs. The proposed\nmixture-of-experts strategy, combined with the dual adaptation mechanism,\nhighlights the model's potential to enhance video summarization capabilities,\nparticularly in large-scale applications requiring both efficiency and\nprecision.",
      "tldr_zh": "该论文提出了一种新型可信视频摘要算法MiLoRA-ViSum，通过将Low-Rank Adaptation (LoRA)扩展到mixture-of-experts框架，以更高效地捕捉视频中的复杂时间动态和空间关系。MiLoRA-ViSum采用双重时间-空间适应机制，动态整合专门针对不同维度的LoRA专家，从而优化视频摘要任务的建模过程。实验在VideoXum和ActivityNet数据集上显示，该算法在摘要性能上优于最先进模型，同时显著降低计算资源消耗，展示了其在大型应用中的高效潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06064v1",
      "published_date": "2025-03-08 05:20:52 UTC",
      "updated_date": "2025-03-08 05:20:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:37:35.808627"
    },
    {
      "arxiv_id": "2503.06060v1",
      "title": "STAR: A Foundation Model-driven Framework for Robust Task Planning and Failure Recovery in Robotic Systems",
      "title_zh": "STAR：一种基础模型驱动的框架，用于机器人系统的鲁棒任务规划和故障恢复",
      "authors": [
        "Md Sadman Sakib",
        "Yu Sun"
      ],
      "abstract": "Modern robotic systems, deployed across domains from industrial automation to\ndomestic assistance, face a critical challenge: executing tasks with precision\nand adaptability in dynamic, unpredictable environments. To address this, we\npropose STAR (Smart Task Adaptation and Recovery), a novel framework that\nsynergizes Foundation Models (FMs) with dynamically expanding Knowledge Graphs\n(KGs) to enable resilient task planning and autonomous failure recovery. While\nFMs offer remarkable generalization and contextual reasoning, their\nlimitations, including computational inefficiency, hallucinations, and output\ninconsistencies hinder reliable deployment. STAR mitigates these issues by\nembedding learned knowledge into structured, reusable KGs, which streamline\ninformation retrieval, reduce redundant FM computations, and provide precise,\nscenario-specific insights. The framework leverages FM-driven reasoning to\ndiagnose failures, generate context-aware recovery strategies, and execute\ncorrective actions without human intervention or system restarts. Unlike\nconventional approaches that rely on rigid protocols, STAR dynamically expands\nits KG with experiential knowledge, ensuring continuous adaptation to novel\nscenarios. To evaluate the effectiveness of this approach, we developed a\ncomprehensive dataset that includes various robotic tasks and failure\nscenarios. Through extensive experimentation, STAR demonstrated an 86% task\nplanning accuracy and 78% recovery success rate, showing significant\nimprovements over baseline methods. The framework's ability to continuously\nlearn from experience while maintaining structured knowledge representation\nmakes it particularly suitable for long-term deployment in real-world\napplications.",
      "tldr_zh": "该研究提出 STAR 框架，利用 Foundation Models (FMs) 与动态扩展的 Knowledge Graphs (KGs) 相结合，实现机器人系统在动态环境中的鲁棒任务规划和自主故障恢复。STAR 通过将 FMs 的泛化推理嵌入结构化的 KGs 中，缓解了 FMs 的计算效率低、幻觉和输出不一致等问题，从而提升信息检索效率和场景特定洞察。框架能诊断故障、生成上下文感知的恢复策略并执行纠正行动，无需人工干预，且通过动态扩展 KGs 实现持续适应新场景。在实验中，STAR 在机器人任务数据集上达到了 86% 的任务规划准确率和 78% 的恢复成功率，显著优于基线方法，并适用于长期真实世界部署。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06060v1",
      "published_date": "2025-03-08 05:05:21 UTC",
      "updated_date": "2025-03-08 05:05:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:37:46.623300"
    },
    {
      "arxiv_id": "2503.06059v1",
      "title": "MANDARIN: Mixture-of-Experts Framework for Dynamic Delirium and Coma Prediction in ICU Patients: Development and Validation of an Acute Brain Dysfunction Prediction Model",
      "title_zh": "MANDARIN：用于 ICU 患者动态谵妄和昏迷",
      "authors": [
        "Miguel Contreras",
        "Jessica Sena",
        "Andrea Davidson",
        "Jiaqing Zhang",
        "Tezcan Ozrazgat-Baslanti",
        "Yuanfang Ren",
        "Ziyuan Guan",
        "Jeremy Balch",
        "Tyler Loftus",
        "Subhash Nerella",
        "Azra Bihorac",
        "Parisa Rashidi"
      ],
      "abstract": "Acute brain dysfunction (ABD) is a common, severe ICU complication,\npresenting as delirium or coma and leading to prolonged stays, increased\nmortality, and cognitive decline. Traditional screening tools like the Glasgow\nComa Scale (GCS), Confusion Assessment Method (CAM), and Richmond\nAgitation-Sedation Scale (RASS) rely on intermittent assessments, causing\ndelays and inconsistencies. In this study, we propose MANDARIN\n(Mixture-of-Experts Framework for Dynamic Delirium and Coma Prediction in ICU\nPatients), a 1.5M-parameter mixture-of-experts neural network to predict ABD in\nreal-time among ICU patients. The model integrates temporal and static data\nfrom the ICU to predict the brain status in the next 12 to 72 hours, using a\nmulti-branch approach to account for current brain status. The MANDARIN model\nwas trained on data from 92,734 patients (132,997 ICU admissions) from 2\nhospitals between 2008-2019 and validated externally on data from 11,719\npatients (14,519 ICU admissions) from 15 hospitals and prospectively on data\nfrom 304 patients (503 ICU admissions) from one hospital in 2021-2024. Three\ndatasets were used: the University of Florida Health (UFH) dataset, the\nelectronic ICU Collaborative Research Database (eICU), and the Medical\nInformation Mart for Intensive Care (MIMIC)-IV dataset. MANDARIN significantly\noutperforms the baseline neurological assessment scores (GCS, CAM, and RASS)\nfor delirium prediction in both external (AUROC 75.5% CI: 74.2%-76.8% vs 68.3%\nCI: 66.9%-69.5%) and prospective (AUROC 82.0% CI: 74.8%-89.2% vs 72.7% CI:\n65.5%-81.0%) cohorts, as well as for coma prediction (external AUROC 87.3% CI:\n85.9%-89.0% vs 72.8% CI: 70.6%-74.9%, and prospective AUROC 93.4% CI:\n88.5%-97.9% vs 67.7% CI: 57.7%-76.8%) with a 12-hour lead time. This tool has\nthe potential to assist clinicians in decision-making by continuously\nmonitoring the brain status of patients in the ICU.",
      "tldr_zh": "本文提出 MANDARIN，一种基于 Mixture-of-Experts 的神经网络框架，用于实时预测 ICU 患者的急性脑功能障碍 (ABD)，包括谵妄和昏迷，以克服传统工具如 Glasgow Coma Scale (GCS)、Confusion Assessment Method (CAM) 和 Richmond Agitation-Sedation Scale (RASS) 的间歇性评估局限。模型整合时间序列和静态数据，通过多分支方法预测未来 12-72 小时的脑状态，并在 92,734 名患者数据上训练，并通过外部（11,719 名患者）和前瞻性（304 名患者）验证显示出显著优势，例如谵妄预测的 AUROC 达 75.5%（外部）和 82.0%（前瞻性），比基线模型高出 7-10%。这项工具可帮助临床医生持续监测患者脑状态，提升决策效率和患者预后。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06059v1",
      "published_date": "2025-03-08 04:56:41 UTC",
      "updated_date": "2025-03-08 04:56:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:38:03.444016"
    },
    {
      "arxiv_id": "2503.06054v1",
      "title": "Fine-Grained Bias Detection in LLM: Enhancing detection mechanisms for nuanced biases",
      "title_zh": "细粒度偏差检测在LLM中：增强针对细",
      "authors": [
        "Suvendu Mohanty"
      ],
      "abstract": "Recent advancements in Artificial Intelligence, particularly in Large\nLanguage Models (LLMs), have transformed natural language processing by\nimproving generative capabilities. However, detecting biases embedded within\nthese models remains a challenge. Subtle biases can propagate misinformation,\ninfluence decision-making, and reinforce stereotypes, raising ethical concerns.\nThis study presents a detection framework to identify nuanced biases in LLMs.\nThe approach integrates contextual analysis, interpretability via attention\nmechanisms, and counterfactual data augmentation to capture hidden biases\nacross linguistic contexts. The methodology employs contrastive prompts and\nsynthetic datasets to analyze model behaviour across cultural, ideological, and\ndemographic scenarios.\n  Quantitative analysis using benchmark datasets and qualitative assessments\nthrough expert reviews validate the effectiveness of the framework. Results\nshow improvements in detecting subtle biases compared to conventional methods,\nwhich often fail to highlight disparities in model responses to race, gender,\nand socio-political contexts. The framework also identifies biases arising from\nimbalances in training data and model architectures. Continuous user feedback\nensures adaptability and refinement. This research underscores the importance\nof proactive bias mitigation strategies and calls for collaboration between\npolicymakers, AI developers, and regulators. The proposed detection mechanisms\nenhance model transparency and support responsible LLM deployment in sensitive\napplications such as education, legal systems, and healthcare. Future work will\nfocus on real-time bias monitoring and cross-linguistic generalization to\nimprove fairness and inclusivity in AI-driven communication tools.",
      "tldr_zh": "这篇论文提出了一种细粒度偏见检测框架，用于识别 Large Language Models (LLMs) 中的细微 biases。框架整合了上下文分析、通过 attention mechanisms 的解释性以及 counterfactual data augmentation，并利用对比提示和合成数据集，分析模型在文化、意识形态和人口统计场景中的行为。实验结果显示，该方法在基准数据集和专家审查中比传统方法更有效地检测种族、性别和社会政治语境中的偏见，提高了识别准确性。研究强调通过持续用户反馈增强框架的适应性，并呼吁政策制定者、AI 开发者和社会监管者合作，以促进 LLM 在教育、法律和医疗等敏感应用中的公平性和负责任部署。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Bias detection, Large Language Models, nuanced biases, fine-grained\n  mechanisms, model transparency, ethical AI",
      "pdf_url": "http://arxiv.org/pdf/2503.06054v1",
      "published_date": "2025-03-08 04:43:01 UTC",
      "updated_date": "2025-03-08 04:43:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:38:12.866562"
    },
    {
      "arxiv_id": "2503.06053v1",
      "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Runze Zhang",
        "Guoguang Du",
        "Xiaochuan Li",
        "Qi Jia",
        "Liang Jin",
        "Lu Liu",
        "Jingjing Wang",
        "Cong Xu",
        "Zhenhua Guo",
        "Yaqian Zhao",
        "Xiaoli Gong",
        "Rengang Li",
        "Baoyu Fan"
      ],
      "abstract": "Spatio-temporal consistency is a critical research topic in video generation.\nA qualified generated video segment must ensure plot plausibility and coherence\nwhile maintaining visual consistency of objects and scenes across varying\nviewpoints. Prior research, especially in open-source projects, primarily\nfocuses on either temporal or spatial consistency, or their basic combination,\nsuch as appending a description of a camera movement after a prompt without\nconstraining the outcomes of this movement. However, camera movement may\nintroduce new objects to the scene or eliminate existing ones, thereby\noverlaying and affecting the preceding narrative. Especially in videos with\nnumerous camera movements, the interplay between multiple plots becomes\nincreasingly complex. This paper introduces and examines integral\nspatio-temporal consistency, considering the synergy between plot progression\nand camera techniques, and the long-term impact of prior content on subsequent\ngeneration. Our research encompasses dataset construction through to the\ndevelopment of the model. Initially, we constructed a DropletVideo-10M dataset,\nwhich comprises 10 million videos featuring dynamic camera motion and object\nactions. Each video is annotated with an average caption of 206 words,\ndetailing various camera movements and plot developments. Following this, we\ndeveloped and trained the DropletVideo model, which excels in preserving\nspatio-temporal coherence during video generation. The DropletVideo dataset and\nmodel are accessible at https://dropletx.github.io.",
      "tldr_zh": "本文探讨了视频生成的 integral spatio-temporal consistency，强调情节连贯性与相机运动的协同作用，以及先前内容对后续生成的影响，以解决现有方法在时空一致性上的局限。研究团队构建了 DropletVideo-10M 数据集，包含 1000 万个动态相机运动和物体动作视频，每段视频平均配有 206 字的详细描述。基于此数据集，他们开发并训练了 DropletVideo 模型，该模型在视频生成中显著提升了整体时空连贯性。数据集和模型已公开可用，可从指定链接获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06053v1",
      "published_date": "2025-03-08 04:37:38 UTC",
      "updated_date": "2025-03-08 04:37:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:38:24.995322"
    },
    {
      "arxiv_id": "2503.06047v1",
      "title": "DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Wenjie Tang",
        "Yuan Zhou",
        "Erqiang Xu",
        "Keyan Cheng",
        "Minne Li",
        "Liquan Xiao"
      ],
      "abstract": "Large Language Model~(LLM) based agents have been increasingly popular in\nsolving complex and dynamic tasks, which requires proper evaluation systems to\nassess their capabilities. Nevertheless, existing benchmarks usually either\nfocus on single-objective tasks or use overly broad assessing metrics, failing\nto provide a comprehensive inspection of the actual capabilities of LLM-based\nagents in complicated decision-making tasks. To address these issues, we\nintroduce DSGBench, a more rigorous evaluation platform for strategic\ndecision-making. Firstly, it incorporates six complex strategic games which\nserve as ideal testbeds due to their long-term and multi-dimensional\ndecision-making demands and flexibility in customizing tasks of various\ndifficulty levels or multiple targets. Secondly, DSGBench employs a\nfine-grained evaluation scoring system which examines the decision-making\ncapabilities by looking into the performance in five specific dimensions and\noffering a comprehensive assessment in a well-designed way. Furthermore,\nDSGBench also incorporates an automated decision-tracking mechanism which\nenables in-depth analysis of agent behaviour patterns and the changes in their\nstrategies. We demonstrate the advances of DSGBench by applying it to multiple\npopular LLM-based agents and our results suggest that DSGBench provides\nvaluable insights in choosing LLM-based agents as well as improving their\nfuture development. DSGBench is available at\nhttps://github.com/DeciBrain-Group/DSGBench.",
      "tldr_zh": "该论文提出 DSGBench，一种多样化的战略游戏基准，用于评估 LLM-based agents 在复杂决策环境中的能力，以解决现有基准偏重单一目标或评估指标宽泛的问题。DSGBench 整合六种复杂战略游戏作为测试平台，支持长期多维决策和任务自定义，并采用细粒度评估系统从五个维度考察代理性能，同时包括自动决策跟踪机制分析行为模式。实验结果显示，DSGBench 在多个流行 LLM-based agents 上提供了宝贵见解，有助于选择和优化这些代理的未来发展。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "43 pages, 5 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2503.06047v1",
      "published_date": "2025-03-08 04:17:23 UTC",
      "updated_date": "2025-03-08 04:17:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:38:36.089998"
    },
    {
      "arxiv_id": "2503.07660v1",
      "title": "Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity",
      "title_zh": "超对齐研究应立即推进，通过能力与",
      "authors": [
        "HyunJin Kim",
        "Xiaoyuan Yi",
        "Jing Yao",
        "Muhua Huang",
        "JinYeong Bak",
        "James Evans",
        "Xing Xie"
      ],
      "abstract": "The recent leap in AI capabilities, driven by big generative models, has\nsparked the possibility of achieving Artificial General Intelligence (AGI) and\nfurther triggered discussions on Artificial Superintelligence (ASI), a system\nsurpassing all humans across all domains. This gives rise to the critical\nresearch question of: If we realize ASI, how do we align it with human values,\nensuring it benefits rather than harms human society, a.k.a., the\nSuperalignment problem. Despite ASI being regarded by many as solely a\nhypothetical concept, in this paper, we argue that superalignment is achievable\nand research on it should advance immediately, through simultaneous and\nalternating optimization of task competence and value conformity. We posit that\nsuperalignment is not merely a safeguard for ASI but also necessary for its\nrealization. To support this position, we first provide a formal definition of\nsuperalignment rooted in the gap between capability and capacity and elaborate\non our argument. Then we review existing paradigms, explore their\ninterconnections and limitations, and illustrate a potential path to\nsuperalignment centered on two fundamental principles. We hope this work sheds\nlight on a practical approach for developing the value-aligned next-generation\nAI, garnering greater benefits and reducing potential harms for humanity.",
      "tldr_zh": "该论文主张立即推进 Superalignment 研究，通过同时优化任务能力(competence)和价值符合性(conformity)，以确保人工智能超智能(ASI)与人类价值观对齐，从而避免潜在危害。作者正式定义了 Superalignment，将其视为 ASI 实现的关键，并分析了能力与容量的差距。论文回顾现有范式及其局限性，并提出以两个基本原则为核心的路径，旨在为开发价值对齐的下一代 AI 提供实用方法。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07660v1",
      "published_date": "2025-03-08 04:10:11 UTC",
      "updated_date": "2025-03-08 04:10:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:38:49.039002"
    },
    {
      "arxiv_id": "2503.10657v2",
      "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongzhan Huang",
        "Guoming Ling",
        "Yupei Lin",
        "Yandong Chen",
        "Shanshan Zhong",
        "Hefeng Wu",
        "Liang Lin"
      ],
      "abstract": "Routing large language models (LLMs) is a new paradigm that uses a router to\nrecommend the best LLM from a pool of candidates for a given input. In this\npaper, our comprehensive analysis with more than 8,500 LLMs reveals a novel\nmodel-level scaling up phenomenon in Routing LLMs, i.e., a capable router can\nsignificantly enhance the performance of this paradigm as the number of\ncandidates increases. This improvement can even surpass the performance of the\nbest single model in the pool and many existing strong LLMs, confirming it a\nhighly promising paradigm. However, the lack of comprehensive and open-source\nbenchmarks for Routing LLMs has hindered the development of routers. In this\npaper, we introduce RouterEval, a benchmark tailored for router research, which\nincludes over 200,000,000 performance records for 12 popular LLM evaluations\nacross various areas such as commonsense reasoning, semantic understanding,\netc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations\nof existing Routing LLM methods reveal that most still have significant room\nfor improvement. See https://github.com/MilkThink-Lab/RouterEval for all data,\ncode and tutorial.",
      "tldr_zh": "这篇论文分析了 Routing LLMs 的新范式，即使用一个 router 从候选模型池中为输入推荐最佳 LLM，并揭示了模型级扩展现象：一个高效的 router 可以显著提升整体性能，随着候选模型数量增加，甚至超越池中最佳单一模型或现有强 LLM。论文引入了 RouterEval 基准，这是一个全面开源的评估工具，基于超过8,500个 LLMs 收集了2亿多性能记录，覆盖常识推理、语义理解等12个领域。实验结果显示，现有的 Routing LLM 方法仍有显著改进空间，为 router 研究提供了宝贵资源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.10657v2",
      "published_date": "2025-03-08 04:07:07 UTC",
      "updated_date": "2025-05-20 14:59:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:39:00.975006"
    },
    {
      "arxiv_id": "2503.16485v2",
      "title": "Optimizing Generative AI's Accuracy and Transparency in Inductive Thematic Analysis: A Human-AI Comparison",
      "title_zh": "翻译失败",
      "authors": [
        "Matthew Nyaaba",
        "Min SungEun",
        "Mary Abiswin Apam",
        "Kwame Owoahene Acheampong",
        "Emmanuel Dwamena"
      ],
      "abstract": "This study highlights the transparency and accuracy of GenAI's inductive\nthematic analysis, particularly using GPT-4 Turbo API integrated within a\nstepwise prompt-based Python script. This approach ensured a traceable and\nsystematic coding process, generating codes with supporting statements and page\nreferences, which enhanced validation and reproducibility. The results indicate\nthat GenAI performs inductive coding in a manner closely resembling human\ncoders, effectively categorizing themes at a level like the average human\ncoder. However, in interpretation, GenAI extends beyond human coders by\nsituating themes within a broader conceptual context, providing a more\ngeneralized and abstract perspective.",
      "tldr_zh": "这篇论文比较了生成式 AI (GenAI) 在归纳主题分析中的准确性和透明度，特别使用 GPT-4 Turbo API 整合的逐步提示 Python 脚本，确保编码过程可追踪并附带支持声明和页面引用，从而提升验证和再现性。研究发现，GenAI 的归纳编码表现与人类编码者相当，能有效分类主题。相比之下，GenAI 在主题解释方面更胜一筹，能将主题置于更广泛的概念背景下，提供更概括和抽象的视角。总的来说，这为优化 AI 在定性分析中的应用提供了重要见解。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16485v2",
      "published_date": "2025-03-08 04:06:16 UTC",
      "updated_date": "2025-03-24 01:57:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:39:13.950354"
    },
    {
      "arxiv_id": "2503.06037v1",
      "title": "Vairiational Stochastic Games",
      "title_zh": "变分随机博弈",
      "authors": [
        "Zhiyu Zhao",
        "Haifeng Zhang"
      ],
      "abstract": "The Control as Inference (CAI) framework has successfully transformed\nsingle-agent reinforcement learning (RL) by reframing control tasks as\nprobabilistic inference problems. However, the extension of CAI to multi-agent,\ngeneral-sum stochastic games (SGs) remains underexplored, particularly in\ndecentralized settings where agents operate independently without centralized\ncoordination. In this paper, we propose a novel variational inference framework\ntailored to decentralized multi-agent systems. Our framework addresses the\nchallenges posed by non-stationarity and unaligned agent objectives, proving\nthat the resulting policies form an $\\epsilon$-Nash equilibrium. Additionally,\nwe demonstrate theoretical convergence guarantees for the proposed\ndecentralized algorithms. Leveraging this framework, we instantiate multiple\nalgorithms to solve for Nash equilibrium, mean-field Nash equilibrium, and\ncorrelated equilibrium, with rigorous theoretical convergence analysis.",
      "tldr_zh": "这篇论文扩展了 Control as Inference (CAI) 框架，将其应用于多代理、一般和之和的 stochastic games (SGs)，特别是在去中心化设置中，代理独立操作而无中心协调。作者提出一个新型变分推理框架，针对非平稳性和代理目标不一致的问题，证明了产生的策略可形成 ε-Nash equilibrium，并提供了理论上的收敛保证。基于此框架，他们实例化了多种算法来求解 Nash equilibrium、mean-field Nash equilibrium 和 correlated equilibrium，并进行了严格的理论分析。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06037v1",
      "published_date": "2025-03-08 03:21:23 UTC",
      "updated_date": "2025-03-08 03:21:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:39:26.166951"
    },
    {
      "arxiv_id": "2503.06030v1",
      "title": "Towards Universal Text-driven CT Image Segmentation",
      "title_zh": "向通用的文本驱动 CT 图像分割",
      "authors": [
        "Yuheng Li",
        "Yuxiang Lai",
        "Maria Thor",
        "Deborah Marshall",
        "Zachary Buchwald",
        "David S. Yu",
        "Xiaofeng Yang"
      ],
      "abstract": "Computed tomography (CT) is extensively used for accurate visualization and\nsegmentation of organs and lesions. While deep learning models such as\nconvolutional neural networks (CNNs) and vision transformers (ViTs) have\nsignificantly improved CT image analysis, their performance often declines when\napplied to diverse, real-world clinical data. Although foundation models offer\na broader and more adaptable solution, their potential is limited due to the\nchallenge of obtaining large-scale, voxel-level annotations for medical images.\nIn response to these challenges, prompting-based models using visual or text\nprompts have emerged. Visual-prompting methods, such as the Segment Anything\nModel (SAM), still require significant manual input and can introduce ambiguity\nwhen applied to clinical scenarios. Instead, foundation models that use text\nprompts offer a more versatile and clinically relevant approach. Notably,\ncurrent text-prompt models, such as the CLIP-Driven Universal Model, are\nlimited to text prompts already encountered during training and struggle to\nprocess the complex and diverse scenarios of real-world clinical applications.\nInstead of fine-tuning models trained from natural imaging, we propose\nOpenVocabCT, a vision-language model pretrained on large-scale 3D CT images for\nuniversal text-driven segmentation. Using the large-scale CT-RATE dataset, we\ndecompose the diagnostic reports into fine-grained, organ-level descriptions\nusing large language models for multi-granular contrastive learning. We\nevaluate our OpenVocabCT on downstream segmentation tasks across nine public\ndatasets for organ and tumor segmentation, demonstrating the superior\nperformance of our model compared to existing methods. All code, datasets, and\nmodels will be publicly released at https://github.com/ricklisz/OpenVocabCT.",
      "tldr_zh": "该论文针对CT图像分割面临的挑战（如深度学习模型在多样化临床数据上的性能下降和标注困难），提出了一种通用的文本驱动分割模型OpenVocabCT。该模型在大型3D CT图像数据集CT-RATE上预训练，通过大语言模型将诊断报告分解为细粒度的器官级描述，并采用多粒度对比学习方法，实现更灵活的文本提示处理。与现有方法相比，OpenVocabCT在九个公共数据集上的器官和肿瘤分割任务中表现出色，显著提升了分割精度。该研究将公开所有代码、数据集和模型，支持进一步的临床应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06030v1",
      "published_date": "2025-03-08 03:02:57 UTC",
      "updated_date": "2025-03-08 03:02:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:39:36.789983"
    },
    {
      "arxiv_id": "2503.06027v2",
      "title": "Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI Models",
      "title_zh": "赋能边缘智能：关于设备端 AI 模型的全面调查",
      "authors": [
        "Xubin Wang",
        "Zhiqing Tang",
        "Jianxiong Guo",
        "Tianhui Meng",
        "Chenhao Wang",
        "Tian Wang",
        "Weijia Jia"
      ],
      "abstract": "The rapid advancement of artificial intelligence (AI) technologies has led to\nan increasing deployment of AI models on edge and terminal devices, driven by\nthe proliferation of the Internet of Things (IoT) and the need for real-time\ndata processing. This survey comprehensively explores the current state,\ntechnical challenges, and future trends of on-device AI models. We define\non-device AI models as those designed to perform local data processing and\ninference, emphasizing their characteristics such as real-time performance,\nresource constraints, and enhanced data privacy. The survey is structured\naround key themes, including the fundamental concepts of AI models, application\nscenarios across various domains, and the technical challenges faced in edge\nenvironments. We also discuss optimization and implementation strategies, such\nas data preprocessing, model compression, and hardware acceleration, which are\nessential for effective deployment. Furthermore, we examine the impact of\nemerging technologies, including edge computing and foundation models, on the\nevolution of on-device AI models. By providing a structured overview of the\nchallenges, solutions, and future directions, this survey aims to facilitate\nfurther research and application of on-device AI, ultimately contributing to\nthe advancement of intelligent systems in everyday life.",
      "tldr_zh": "这篇调查论文全面探讨了 on-device AI models 的现状、技术挑战和未来趋势，这些模型旨在在边缘设备上进行本地数据处理和推理，以满足 IoT 驱动的实时性能、资源约束和数据隐私需求。论文结构化地涵盖了 AI 模型的基本概念、各种领域的应用场景，以及在边缘环境中面临的挑战，如优化部署和实施策略，包括数据预处理、模型压缩和硬件加速。作者还分析了边缘计算和 foundation models 等新兴技术对 on-device AI 模型的影响，并为进一步研究提供指导，推动智能系统在日常生活中的应用。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by ACM Computing Surveys",
      "pdf_url": "http://arxiv.org/pdf/2503.06027v2",
      "published_date": "2025-03-08 02:59:51 UTC",
      "updated_date": "2025-03-17 13:37:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:39:48.327889"
    },
    {
      "arxiv_id": "2503.06026v1",
      "title": "Zero-Shot Peg Insertion: Identifying Mating Holes and Estimating SE(2) Poses with Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Masaru Yajima",
        "Kei Ota",
        "Asako Kanezaki",
        "Rei Kawakami"
      ],
      "abstract": "Achieving zero-shot peg insertion, where inserting an arbitrary peg into an\nunseen hole without task-specific training, remains a fundamental challenge in\nrobotics. This task demands a highly generalizable perception system capable of\ndetecting potential holes, selecting the correct mating hole from multiple\ncandidates, estimating its precise pose, and executing insertion despite\nuncertainties. While learning-based methods have been applied to peg insertion,\nthey often fail to generalize beyond the specific peg-hole pairs encountered\nduring training. Recent advancements in Vision-Language Models (VLMs) offer a\npromising alternative, leveraging large-scale datasets to enable robust\ngeneralization across diverse tasks. Inspired by their success, we introduce a\nnovel zero-shot peg insertion framework that utilizes a VLM to identify mating\nholes and estimate their poses without prior knowledge of their geometry.\nExtensive experiments demonstrate that our method achieves 90.2% accuracy,\nsignificantly outperforming baselines in identifying the correct mating hole\nacross a wide range of previously unseen peg-hole pairs, including 3D-printed\nobjects, toy puzzles, and industrial connectors. Furthermore, we validate the\neffectiveness of our approach in a real-world connector insertion task on a\nbackpanel of a PC, where our system successfully detects holes, identifies the\ncorrect mating hole, estimates its pose, and completes the insertion with a\nsuccess rate of 88.3%. These results highlight the potential of VLM-driven\nzero-shot reasoning for enabling robust and generalizable robotic assembly.",
      "tldr_zh": "这篇论文提出了一种零样本插销插入框架，利用 Vision-Language Models (VLMs) 来识别配对孔并估计 SE(2) 位姿，而无需任务特定训练，从而解决机器人装配中对未知几何的泛化挑战。该方法通过 VLMs 处理孔检测、正确孔选择和位姿估计，实现了高度鲁棒的零样本推理。在实验中，该框架在多种未见过的插销-孔对上达到90.2%的准确率，并在真实世界PC背板连接器插入任务中取得88.3%的成功率，证明了VLMs在机器人装配领域的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Under submission",
      "pdf_url": "http://arxiv.org/pdf/2503.06026v1",
      "published_date": "2025-03-08 02:59:21 UTC",
      "updated_date": "2025-03-08 02:59:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:40:01.748048"
    },
    {
      "arxiv_id": "2503.06014v1",
      "title": "Towards Ambiguity-Free Spatial Foundation Model: Rethinking and Decoupling Depth Ambiguity",
      "title_zh": "迈向无歧义空间基础模型：重新思考与解耦深度歧义",
      "authors": [
        "Xiaohao Xu",
        "Feng Xue",
        "Xiang Li",
        "Haowei Li",
        "Shusheng Yang",
        "Tianyi Zhang",
        "Matthew Johnson-Roberson",
        "Xiaonan Huang"
      ],
      "abstract": "Depth ambiguity is a fundamental challenge in spatial scene understanding,\nespecially in transparent scenes where single-depth estimates fail to capture\nfull 3D structure. Existing models, limited to deterministic predictions,\noverlook real-world multi-layer depth. To address this, we introduce a paradigm\nshift from single-prediction to multi-hypothesis spatial foundation models. We\nfirst present \\texttt{MD-3k}, a benchmark exposing depth biases in expert and\nfoundational models through multi-layer spatial relationship labels and new\nmetrics. To resolve depth ambiguity, we propose Laplacian Visual Prompting\n(LVP), a training-free spectral prompting technique that extracts hidden depth\nfrom pre-trained models via Laplacian-transformed RGB inputs. By integrating\nLVP-inferred depth with standard RGB-based estimates, our approach elicits\nmulti-layer depth without model retraining. Extensive experiments validate the\neffectiveness of LVP in zero-shot multi-layer depth estimation, unlocking more\nrobust and comprehensive geometry-conditioned visual generation, 3D-grounded\nspatial reasoning, and temporally consistent video-level depth inference. Our\nbenchmark and code will be available at\nhttps://github.com/Xiaohao-Xu/Ambiguity-in-Space.",
      "tldr_zh": "该论文针对深度模糊（Depth Ambiguity）问题，重新思考空间场景理解的范式，提出从单预测转向多假设（multi-hypothesis）空间基础模型，以处理透明场景的多层深度结构。作者引入MD-3k基准，通过多层空间关系标签和新指标暴露现有模型的深度偏差。核心方法Laplacian Visual Prompting (LVP)是一种无训练的谱提示技术，利用Laplacian变换的RGB输入从预训练模型中提取隐藏深度，并与标准RGB-based估计整合，实现无需重新训练的多层深度估计。实验结果显示，LVP在零样本多层深度估计中表现出色，提升了几何条件视觉生成、3D空间推理和视频级深度推断的鲁棒性和时序一致性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "32 pages, 31 figures, github repo:\n  https://github.com/Xiaohao-Xu/Ambiguity-in-Space",
      "pdf_url": "http://arxiv.org/pdf/2503.06014v1",
      "published_date": "2025-03-08 02:33:54 UTC",
      "updated_date": "2025-03-08 02:33:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:40:15.709354"
    },
    {
      "arxiv_id": "2503.06011v1",
      "title": "Intent-Aware Self-Correction for Mitigating Social Biases in Large Language Models",
      "title_zh": "基于意图的自我修正以缓解大语言模型中的社会偏见",
      "authors": [
        "Panatchakorn Anantaprayoon",
        "Masahiro Kaneko",
        "Naoaki Okazaki"
      ],
      "abstract": "Self-Correction based on feedback improves the output quality of Large\nLanguage Models (LLMs). Moreover, as Self-Correction functions like the slow\nand conscious System-2 thinking from cognitive psychology's perspective, it can\npotentially reduce LLMs' social biases. LLMs are sensitive to contextual\nambiguities and inconsistencies; therefore, explicitly communicating their\nintentions during interactions when applying Self-Correction for debiasing is\ncrucial. In this study, we demonstrate that clarifying intentions is essential\nfor effectively reducing biases in LLMs through Self-Correction. We divide the\ncomponents needed for Self-Correction into three parts: instruction, response,\nand feedback, and clarify intentions at each component. We incorporate an\nexplicit debiasing prompt to convey the intention of bias mitigation from the\ninstruction for response generation. In the response, we use Chain-of-Thought\n(CoT) to clarify the reasoning process. In the feedback, we define evaluation\naspects necessary for debiasing and propose clear feedback through multi-aspect\ncritiques and scoring. Through experiments, we demonstrate that self-correcting\nCoT responses obtained from a debiasing prompt based on multi-aspect feedback\ncan reduce biased responses more robustly and consistently than the baselines.\nWe also find the variation in debiasing efficacy when using models with\ndifferent bias levels or separating models for response and feedback\ngeneration.",
      "tldr_zh": "本研究提出了一种意图aware的自校正方法，用于缓解Large Language Models (LLMs)中的社会偏见。该方法将Self-Correction分为instruction、response和feedback三个组件，并在每个部分明确意图：通过显式debiasing提示指导响应生成、在response中使用Chain-of-Thought (CoT)清晰推理过程，以及在feedback中通过多方面批评和评分提供精确评估。实验结果显示，这种基于意图澄清的自校正机制比基线方法更稳健一致地减少偏见响应，同时发现模型偏见水平和响应/反馈生成分离会影响debiasing效果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages. Under review",
      "pdf_url": "http://arxiv.org/pdf/2503.06011v1",
      "published_date": "2025-03-08 02:20:43 UTC",
      "updated_date": "2025-03-08 02:20:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:40:27.879882"
    },
    {
      "arxiv_id": "2503.16484v1",
      "title": "AI-Powered Episodic Future Thinking",
      "title_zh": "翻译失败",
      "authors": [
        "Sareh Ahmadi",
        "Michelle Rockwell",
        "Megan Stuart",
        "Allison Tegge",
        "Xuan Wang",
        "Jeffrey Stein",
        "Edward A. Fox"
      ],
      "abstract": "Episodic Future Thinking (EFT) is an intervention that involves vividly\nimagining personal future events and experiences in detail. It has shown\npromise as an intervention to reduce delay discounting - the tendency to\ndevalue delayed rewards in favor of immediate gratification - and to promote\nbehavior change in a range of maladaptive health behaviors. We present\nEFTeacher, an AI chatbot powered by the GPT-4-Turbo large language model,\ndesigned to generate EFT cues for users with lifestyle-related conditions. To\nevaluate the chatbot, we conducted a user study that included usability\nassessments and user evaluations based on content characteristics\nquestionnaires, followed by semi-structured interviews. The study provides\nqualitative insights into participants' experiences and interactions with the\nchatbot and its usability. Our findings highlight the potential application of\nAI chatbots based on Large Language Models (LLMs) in EFT interventions, and\noffer design guidelines for future behavior-oriented applications.",
      "tldr_zh": "本研究探讨了Episodic Future Thinking (EFT)，一种通过生动想象个人未来事件来减少delay discounting（延迟折扣）和促进不良健康行为改变的干预方法。论文引入了EFTeacher，一款基于GPT-4-Turbo的大型语言模型（LLMs）驱动的AI聊天机器人，用于为生活方式相关条件用户生成EFT提示。用户研究包括可用性评估、内容特征问卷和半结构化访谈，结果揭示了AI聊天机器人应用于EFT干预的潜力，并提供了未来行为导向应用的設計指南。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16484v1",
      "published_date": "2025-03-08 01:10:04 UTC",
      "updated_date": "2025-03-08 01:10:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:40:39.193315"
    },
    {
      "arxiv_id": "2503.05997v1",
      "title": "Learning to Drive by Imitating Surrounding Vehicles",
      "title_zh": "通过模仿周围车辆学习驾驶",
      "authors": [
        "Yasin Sonmez",
        "Hanna Krasowski",
        "Murat Arcak"
      ],
      "abstract": "Imitation learning is a promising approach for training autonomous vehicles\n(AV) to navigate complex traffic environments by mimicking expert driver\nbehaviors. However, a major challenge in this paradigm lies in effectively\nutilizing available driving data, as collecting new data is resource-intensive\nand often limited in its ability to cover diverse driving scenarios. While\nexisting imitation learning frameworks focus on leveraging expert\ndemonstrations, they often overlook the potential of additional complex driving\ndata from surrounding traffic participants. In this paper, we propose a data\naugmentation strategy that enhances imitation learning by leveraging the\nobserved trajectories of nearby vehicles, captured through the AV's sensors, as\nadditional expert demonstrations. We introduce a vehicle selection sampling\nstrategy that prioritizes informative and diverse driving behaviors,\ncontributing to a richer and more diverse dataset for training. We evaluate our\napproach using the state-of-the-art learning-based planning method PLUTO on the\nnuPlan dataset and demonstrate that our augmentation method leads to improved\nperformance in complex driving scenarios. Specifically, our method reduces\ncollision rates and improves safety metrics compared to the baseline. Notably,\neven when using only 10% of the original dataset, our method achieves\nperformance comparable to that of the full dataset, with improved collision\nrates. Our findings highlight the importance of leveraging diverse real-world\ntrajectory data in imitation learning and provide insights into data\naugmentation strategies for autonomous driving.",
      "tldr_zh": "本文提出了一种数据增强策略，用于改善模仿学习(Imitation Learning)训练自动驾驶车辆(AV)，通过利用 AV 传感器捕获的周围车辆轨迹作为额外专家演示，从而丰富数据集。方法引入车辆选择采样策略，优先选择信息丰富和多样的驾驶行为，以覆盖更多复杂场景。在 nuPlan 数据集上，使用 PLUTO 方法评估，结果显示该策略显著降低碰撞率并提升安全指标；即使仅使用原数据集的10%，性能也可与完整数据集相当或更好。该研究突显了利用多样化真实轨迹数据在自主驾驶中的关键作用，提供宝贵的增强策略见解。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05997v1",
      "published_date": "2025-03-08 00:40:47 UTC",
      "updated_date": "2025-03-08 00:40:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:40:52.620128"
    },
    {
      "arxiv_id": "2503.05996v1",
      "title": "Towards Improving Reward Design in RL: A Reward Alignment Metric for RL Practitioners",
      "title_zh": "翻译失败",
      "authors": [
        "Calarina Muslimani",
        "Kerrick Johnstonbaugh",
        "Suyog Chandramouli",
        "Serena Booth",
        "W. Bradley Knox",
        "Matthew E. Taylor"
      ],
      "abstract": "Reinforcement learning agents are fundamentally limited by the quality of the\nreward functions they learn from, yet reward design is often overlooked under\nthe assumption that a well-defined reward is readily available. However, in\npractice, designing rewards is difficult, and even when specified, evaluating\ntheir correctness is equally problematic: how do we know if a reward function\nis correctly specified? In our work, we address these challenges by focusing on\nreward alignment -- assessing whether a reward function accurately encodes the\npreferences of a human stakeholder. As a concrete measure of reward alignment,\nwe introduce the Trajectory Alignment Coefficient to quantify the similarity\nbetween a human stakeholder's ranking of trajectory distributions and those\ninduced by a given reward function. We show that the Trajectory Alignment\nCoefficient exhibits desirable properties, such as not requiring access to a\nground truth reward, invariance to potential-based reward shaping, and\napplicability to online RL. Additionally, in an 11 -- person user study of RL\npractitioners, we found that access to the Trajectory Alignment Coefficient\nduring reward selection led to statistically significant improvements. Compared\nto relying only on reward functions, our metric reduced cognitive workload by\n1.5x, was preferred by 82% of users and increased the success rate of selecting\nreward functions that produced performant policies by 41%.",
      "tldr_zh": "本研究针对强化学习（RL）中的奖励函数设计问题，提出了一种评估奖励对齐（reward alignment）的度量方法，以检查奖励函数是否准确反映人类利益相关者的偏好。论文引入了Trajectory Alignment Coefficient（轨迹对齐系数），用于量化人类对轨迹分布排名的相似度与奖励函数诱导的排名的匹配程度。该系数具有无需访问真实奖励函数、不受基于势的奖励整形影响以及适用于在线RL等优点。在一项涉及11名RL从业者的用户研究中，使用该系数显著提升了奖励选择效率，减少认知工作量1.5倍，用户偏好率达82%，并将成功率提高41%。这为RL实践者提供了更可靠的奖励设计工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05996v1",
      "published_date": "2025-03-08 00:38:17 UTC",
      "updated_date": "2025-03-08 00:38:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:41:03.602370"
    },
    {
      "arxiv_id": "2503.05991v1",
      "title": "GrInAdapt: Scaling Retinal Vessel Structural Map Segmentation Through Grounding, Integrating and Adapting Multi-device, Multi-site, and Multi-modal Fundus Domains",
      "title_zh": "翻译失败",
      "authors": [
        "Zixuan Liu",
        "Aaron Honjaya",
        "Yuekai Xu",
        "Yi Zhang",
        "Hefu Pan",
        "Xin Wang",
        "Linda G Shapiro",
        "Sheng Wang",
        "Ruikang K Wang"
      ],
      "abstract": "Retinal vessel segmentation is critical for diagnosing ocular conditions, yet\ncurrent deep learning methods are limited by modality-specific challenges and\nsignificant distribution shifts across imaging devices, resolutions, and\nanatomical regions. In this paper, we propose GrInAdapt, a novel framework for\nsource-free multi-target domain adaptation that leverages multi-view images to\nrefine segmentation labels and enhance model generalizability for optical\ncoherence tomography angiography (OCTA) of the fundus of the eye. GrInAdapt\nfollows an intuitive three-step approach: (i) grounding images to a common\nanchor space via registration, (ii) integrating predictions from multiple views\nto achieve improved label consensus, and (iii) adapting the source model to\ndiverse target domains. Furthermore, GrInAdapt is flexible enough to\nincorporate auxiliary modalities such as color fundus photography, to provide\ncomplementary cues for robust vessel segmentation. Extensive experiments on a\nmulti-device, multi-site, and multi-modal retinal dataset demonstrate that\nGrInAdapt significantly outperforms existing domain adaptation methods,\nachieving higher segmentation accuracy and robustness across multiple domains.\nThese results highlight the potential of GrInAdapt to advance automated retinal\nvessel analysis and support robust clinical decision-making.",
      "tldr_zh": "本文提出 GrInAdapt 框架，用于提升视网膜血管结构图分割的泛化能力，解决不同设备、分辨率和解剖区域的分布偏移问题。该框架采用三步方法：通过图像注册实现 grounding 到共同锚点空间、整合多视图预测以获得更准确的标签共识，以及将源模型 adapting 到多种目标域。此外，GrInAdapt 可灵活整合辅助模式如 color fundus photography，提供互补线索。实验在多设备、多站点和多模式视网膜数据集上表明，该框架显著优于现有 domain adaptation 方法，提高了分割准确性和鲁棒性，从而支持更可靠的临床决策。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05991v1",
      "published_date": "2025-03-08 00:15:21 UTC",
      "updated_date": "2025-03-08 00:15:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:41:16.423438"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 75,
  "processed_papers_count": 75,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-23T23:41:39.742150"
}
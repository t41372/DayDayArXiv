{
  "date": "2025-03-08",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间2025-03-08的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 论文再次被大型语言和多模态模型（LLM/MLLM）主导，研究者们不仅探索了它们在医疗诊断（如 AMIE 系统）、科学优化、机器人控制、外交决策等新领域的应用，还致力于解决其效率、对齐、偏见和幻觉问题，并提出了多个新基准来评估这些复杂系统的能力。AI 安全、监管框架和模型可解释性也是今日的热点。\n\n**今日焦点与重要论文:**\n\n*   **迈向疾病管理的对话式人工智能 (Towards Conversational AI for Disease Management):** 论文 (#52) 扩展了先前 AMIE 系统的诊断能力，提出一个基于 LLM 的新智能体系统，优化用于临床管理和对话，能处理疾病演变、治疗反应和安全处方。该系统利用 Gemini 的长上下文能力、检索增强和结构化推理，与临床指南和药物处方集对齐。在虚拟 OSCE 研究中，AMIE 在管理推理方面不劣于初级保健医生 (PCP)，并在治疗精确性、指南对齐方面表现更优，在药物推理基准 RxQA 的难题上也超越了 PCP。标志着对话式 AI 在疾病管理应用中的重要一步。\n*   **大型语言模型后训练综述 (A Survey on Post-training of Large Language Models):** 这篇长达 87 页的综述 (#54) 首次全面梳理了后训练语言模型 (PoLMs) 的发展，涵盖微调、对齐、推理、效率、集成与适应五大范式，追踪了从 ChatGPT 到 DeepSeek-R1 等模型的演进，为理解和推进 LLM 的能力边界提供了宝贵的知识框架。\n*   **超级对齐研究应立即推进：能力与合规性的并行优化 (Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity):** 论文 (#61) 认为，即使 ASI 尚属假设，超级对齐（确保超智能 AI 符合人类价值观）的研究也应立即开始，并提出通过能力（competence）和合规性（conformity）的同步交替优化来实现这一目标，认为超级对齐不仅是 ASI 的保障，也是实现 ASI 的必要条件。\n*   **合并后重新对齐：简单有效的多模态 LLM 模态增量持续学习 (Merge then Realign: Simple and Effective Modality-Incremental Continual Learning for Multimodal LLMs):** 论文 (#4) 针对多模态大模型 (MLLM) 在增加新模态时面临的灾难性遗忘和模态错位问题，提出了一种名为 MERA (MErge then ReAlign) 的简单范式。该方法先合并模型参数，再进行重新对齐，有效避免了性能下降，在扩展到四种模态时仍保持高达 99.84% 的向后相对增益。\n*   **你的大型视觉语言模型只需要几个注意力头进行视觉定位 (Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding):** 研究 (#11) 发现，在冻结的大型视觉语言模型 (LVLM) 中，仅需少数几个特定的注意力头（称为定位头）即可实现强大的视觉定位能力，无需微调或额外组件。基于此，提出了一个简单有效的免训练视觉定位框架，性能与需要微调的方法相当。\n*   **利用机制可解释性构建针对大语言模型的对抗性攻击 (Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models):** 论文 (#13) 提出一种新颖的白盒攻击方法，利用机制可解释性技术识别模型的“接受子空间”，然后通过梯度优化将拒绝子空间的嵌入重定向到接受子空间，从而高效实现越狱攻击，在多个 SOTA 模型上几分钟甚至几秒内达到 80-95% 的成功率。\n*   **利用编辑后的大语言模型作为通用科学优化器 (Exploiting Edited Large Language Models as General Scientific Optimizers):** 论文 (#9) 提出 GSO 框架，将 LLM 作为“科学家”，利用模拟器（内部优化）提供反馈，LLM（外部优化）根据反馈生成新解，并通过模型编辑（双层交互）联合更新模拟和 LLM 知识，以解决科学场景中的优化问题，克服了现有提示方法对反馈利用不佳的缺点。\n\n**LLM/MLLM 应用与评估:**\n\n*   **通过提示优化的 LLM 评估分类学命名法的自动标记方法 (Evaluation of the Automated Labeling Method for Taxonomic Nomenclature Through Prompt-Optimized Large Language Model):** 论文 (#1) 使用 LLM 对生物物种名称（如蜘蛛）进行自动分类（基于形态、地理、人名等），通过提示工程优化后，在某些类别上取得了高准确率，但在生态行为和文化背景方面仍有挑战。\n*   **CFPD 基准：衡量大型语言模型中的外交偏好 (Critical Foreign Policy Decisions (CFPD)-Benchmark: Measuring Diplomatic Preferences in Large Language Models):** 论文 (#14) 构建了一个包含 400 个专家设计场景的基准，用于评估 7 种主流 LLM 在国际关系（军事升级、干预、合作、联盟）决策中的偏好和偏见。发现模型间存在显著差异，且普遍存在国家特定偏见。\n*   **CUPCase：临床罕见病例与诊断数据集 (Clinically Uncommon Patient Cases and Diagnoses Dataset):** 论文 (#26) 构建了一个基于 3562 个真实世界罕见/非典型病例报告的数据集 CUPCase，用于评估 LLM 的诊断能力。GPT-4o 在多选和开放式诊断任务中表现最佳，甚至在仅有部分病例信息时也能保持较好性能。\n*   **图像即你所需：迈向高效且有效的大型语言模型推荐系统 (Image is All You Need: Towards Efficient and Effective Large Language Model-Based Recommender Systems):** 论文 (#19) 提出 I-LLMRec 方法，利用物品图像代替冗长的文本描述来表示用户交互历史，旨在降低 LLM 推荐系统的 token 消耗，同时保持推荐效果，并发现图像表示还能减少对描述噪声的敏感性。\n*   **原子步骤分解能否增强多模态大模型的自结构化推理？(Can Atomic Step Decomposition Enhance the Self-structured Reasoning of Multimodal Large Models?):** 论文 (#16) 提出 AtomThink 框架，通过将推理过程分解为最小语义原子步骤（自结构化思维链 SCoT），使 MLLM 能够动态组合不同层级的推理能力来解决复杂的多模态数学推理问题，显著提升了基线 MLLM 在 MathVista 等基准上的性能。\n*   **CAREVL：利用大型语言模型专家增强大型视觉语言模型的奖励建模 (From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models):** 论文 (#15) 提出 CAREVL 方法，利用一组辅助文本奖励模型（以图像标题作为弱监督）来筛选高置信度偏好数据，并利用低置信度数据生成多样化样本，从而为 LVLM 构建更可靠的奖励模型，以更好地与人类偏好对齐。\n*   **UrbanVideo-Bench：在城市空间视频数据上对视觉语言模型的具身智能进行基准测试 (UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces):** 论文 (#40) 提出了一个新基准 UrbanVideo-Bench，包含真实和模拟城市环境下的第一人称运动视频及问答对，用于评估 Video-LLM 在回忆、感知、推理和导航等方面的具身认知能力。\n*   **GEM：通过时间序列和图像赋能 MLLM 实现有依据的心电图理解 (GEM: Empowering MLLM for Grounded ECG Understanding with Time Series and Images):** 论文 (#53) 提出 GEM，首个统一 ECG 时间序列、12 导联图像和文本的 MLLM，通过双编码器、跨模态对齐和知识引导的指令生成（ECG-Grounding 数据集），实现将诊断与具体波形证据（如 QRS 间期）相关联的有依据、可解释的心电图解读。\n*   **通过 LoRA 专家混合实现可信赖的视频摘要算法 (A Novel Trustworthy Video Summarization Algorithm Through a Mixture of LoRA Experts):** 论文 (#55) 提出 MiLoRA-ViSum，使用 LoRA 专家混合 (MoE) 范式进行视频摘要，设计了双重时空适应机制，动态整合针对时间和空间维度微调的 LoRA 专家，以更高效地捕捉视频的时空特征，同时控制训练参数量。\n*   **DSGBench：用于评估复杂决策环境中基于 LLM 智能体的多样化战略游戏基准 (DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments):** 论文 (#60) 提出了 DSGBench，包含六种复杂战略游戏，并设计了细粒度的多维度评分系统和自动决策追踪机制，用于更严格地评估 LLM 智能体在长期、多目标决策任务中的能力。\n*   **RouterEval：用于探索 LLM 模型级扩展的路由 LLM 综合基准 (RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs):** 论文 (#62) 提出了 RouterEval 基准，包含超过 2 亿条性能记录，覆盖 12 个流行 LLM 评估任务和 8500 多个 LLM，旨在推动路由 LLM（根据输入选择最合适的 LLM）的研究。发现好的路由器能显著提升性能，甚至超越池中最佳单一模型。\n*   **迈向通用文本驱动的 CT 图像分割 (Towards Universal Text-driven CT Image Segmentation):** 论文 (#65) 提出 OpenVocabCT，一个在大型 3D CT 图像数据集 (CT-RATE) 上预训练的视觉语言模型，用于通用的文本驱动分割。通过 LLM 将诊断报告分解为器官级描述进行多粒度对比学习，在多个下游分割任务上表现优于现有方法。\n*   **零样本圆柱销插入：使用视觉语言模型识别匹配孔并估计 SE(2) 位姿 (Zero-Shot Peg Insertion: Identifying Mating Holes and Estimating SE(2) Poses with Vision-Language Models):** 论文 (#67) 提出一个零样本圆柱销插入框架，利用 VLM 在没有先验几何知识的情况下识别匹配孔洞并估计其位姿，在多种未见过的配对中实现了高精度识别，并在真实世界的连接器插入任务中验证了有效性。\n*   **AI 驱动的情景未来思维 (AI-Powered Episodic Future Thinking):** 论文 (#70) 介绍了 EFTeacher，一个基于 GPT-4-Turbo 的 AI 聊天机器人，旨在为患有生活方式相关疾病的用户生成情景未来思维 (EFT) 线索，以帮助减少延迟折扣并促进积极的行为改变。用户研究验证了其可用性和潜力。\n\n**AI 安全、伦理与监管:**\n\n*   **AI 五要素、CHARME2D 模型及当前 AI 监管评估 (The AI Pentad, the CHARME$^{2}$D Model, and an Assessment of Current-State AI Regulation):** 论文 (#2) 提出了 AI 五要素（人与组织、算法、数据、计算、能源）和 CHARME2D 模型，用于构建和评估 AI 监管框架，并应用该模型分析了欧盟、中国、阿联酋、英国和美国的 AI 监管现状。\n*   **反事实 VLM 的三重奏：幻觉的因果方法 (Treble Counterfactual VLMs: A Causal Approach to Hallucination):** 论文 (#36) 从因果角度分析 VLM 幻觉，认为是视觉或文本模态的意外直接影响绕过了多模态融合。提出通过反事实分析估计自然直接效应 (NDE)，并在测试时干预以减少幻觉。\n*   **破坏模型合并：一种不牺牲准确性的参数级防御 (Disrupting Model Merging: A Parameter-Level Defense Without Sacrificing Accuracy):** 论文 (#51) 提出首个主动防御模型合并的方法，通过修改模型参数（重排 MLP、缩放注意力头），使得模型在被合并时性能严重下降，但不影响其自身功能，以阻止“搭便车”行为。\n*   **LLM 中细粒度偏见检测：增强对细微偏见的检测机制 (Fine-Grained Bias Detection in LLM: Enhancing detection mechanisms for nuanced biases):** 论文 (#58) 提出了一个检测 LLM 中细微偏见的框架，结合上下文分析、可解释性（注意力机制）和反事实数据增强，通过对比性提示和合成数据集分析模型在不同文化、意识形态和人口统计场景下的行为。\n*   **意图感知的自我修正以减轻大型语言模型中的社会偏见 (Intent-Aware Self-Correction for Mitigating Social Biases in Large Language Models):** 论文 (#69) 证明了在 LLM 自我修正过程中明确沟通意图对于有效减少偏见至关重要。通过在指令、响应（CoT）和反馈（多方面批判）中明确意图，可以更稳健地减少偏见。\n*   **迈向改进 RL 中的奖励设计：面向 RL 从业者的奖励对齐度量 (Towards Improving Reward Design in RL: A Reward Alignment Metric for RL Practitioners):** 论文 (#72) 关注奖励对齐问题（奖励函数是否准确编码人类偏好），提出了轨迹对齐系数 (Trajectory Alignment Coefficient) 来量化奖励函数引发的轨迹分布排序与人类利益相关者排序的相似性。用户研究表明该度量有助于选择更好的奖励函数并降低认知负荷。\n\n**模型改进与理解:**\n\n*   **研究强化学习中 Actor 和 Critic 表示之间的相互作用 (Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning):** 论文 (#3) 探究了 Actor-Critic 算法中 Actor 和 Critic 是否应使用分离的表示。发现分离时，Actor 倾向于关注动作相关信息，而 Critic 专注于价值和动态信息，且分离的 Critic 在探索中起重要作用。\n*   **LLM 生成文本的状态及其间的相变 (States of LLM-generated Texts and Phase Transitions between them):** 论文 (#5) 借鉴固态物理学，通过分析词语自相关性的衰减规律，实证表明 LLM 根据温度参数可以生成处于“固态”、“临界态”或“气态”的文本。\n*   **通过对齐抽象级别改进跨模态迁移的文本-语音语言模型 (Text-Speech Language Models with Improved Cross-Modal Transfer by Aligning Abstraction Levels):** 论文 (#23) 认为现有 TSLM 训练方法限制了跨模态迁移，提出在扩展词汇表的同时增加模块以更好地对齐不同层级的抽象水平，从而提升跨模态迁移效果。\n*   **通过探测输入利用率打破 MMI 束缚：合理化提取的新前沿 (Breaking Free from MMI: A New Frontier in Rationalization by Probing Input Utilization):** 论文 (#27) 指出最大互信息 (MMI) 标准在提取解释性依据 (rationale) 时存在边际收益递减问题。提出一种新方法，通过比较候选依据与网络权重矩阵能力空间的匹配程度（利用表示范数）来识别网络实际能利用的输入部分，效果优于 MMI。\n*   **面向大型语言模型的样本感知自适应结构化剪枝 (Sample-aware Adaptive Structured Pruning for Large Language Models):** 论文 (#31) 提出 AdaPruner 框架，通过贝叶斯优化自适应地搜索最优的校准数据和重要性度量指标，以改进 LLM 的结构化剪枝效果，在 20% 剪枝率下保持 97% 的性能。\n*   **用于持续学习的 Minion 门控循环单元 (Minion Gated Recurrent Unit for Continual Learning):** 论文 (#33) 提出 GRU 的简化变体 MiRU，用缩放系数替代门控机制，减少计算和内存需求，同时在序列学习任务中保持与 GRU 相当的性能，且在持续学习中表现更稳定，适用于边缘设备。\n*   **ROCM：基于一致性模型的 RLHF (ROCM: RLHF on consistency models):** 论文 (#34) 提出一个直接在一致性模型 (Consistency Models) 上应用 RLHF 的奖励优化框架，结合分布正则化（如 f-散度）来提高训练稳定性并防止奖励操纵 (reward hacking)，相比策略梯度法更高效。\n\n**计算机视觉与生成模型:**\n\n*   **使用 Jensen-Shannon 分数蒸馏的文本到 3D 生成 (Text-to-3D Generation using Jensen-Shannon Score Distillation):** 论文 (#24) 针对分数蒸馏采样 (SDS) 生成 3D 模型时存在的过饱和、过平滑、多样性不足问题，推导了基于 Jensen-Shannon 散度 (JSD) 的有界分数蒸馏目标，以稳定优化过程并缓解模式寻求行为。\n*   **通过扩散时间步集成的可解释合成图像检测 (Explainable Synthetic Image Detection through Diffusion Timestep Ensembling):** 论文 (#28) 发现自然图像和扩散模型生成的合成图像在经过多步逆向去噪（加噪）后，其傅里叶功率谱的高频域存在差异。提出通过在多个时间步上对图像加噪并训练分类器集成来检测合成图像，并引入解释模块识别伪造痕迹。\n*   **MSConv：用于人脸识别的乘法和减法卷积 (MSConv: Multiplicative and Subtractive Convolution for Face Recognition):** 论文 (#30) 提出 MSConv 模块，通过多尺度混合卷积捕获人脸信息，并利用乘法操作 (MO) 提取显著特征，减法操作 (SO) 提取差异特征，平衡模型对这两类特征的学习，以提升人脸识别性能。\n*   **Feature-EndoGaussian：手术可变形场景重建中的特征蒸馏高斯泼溅 (Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical Deformable Scene Reconstruction):** 论文 (#39) 提出 Feature-EndoGaussian (FEG)，将 3D 高斯泼溅 (3DGS) 与 2D 分割线索相结合，利用预训练分割模型的特征进行蒸馏，在动态手术场景中实现实时的语义和场景重建。\n*   **GSV3D：基于高斯泼溅的几何蒸馏与稳定视频扩散用于单图像 3D 对象生成 (GSV3D: Gaussian Splatting-based Geometric Distillation with Stable Video Diffusion for Single-Image 3D Object Generation):** 论文 (#44) 提出 GSV3D，利用高斯泼溅解码器将 Stable Video Diffusion (SV3D) 的潜在输出转换为显式 3D 表示，通过几何约束强制实现多视图一致性，从而从单张图像生成高质量且几何一致的 3D 模型。\n*   **基于 CycleGAN 的特征融合注意力网络用于图像去雾、去雪和去雨 (Feature Fusion Attention Network with CycleGAN for Image Dehazing, De-Snowing and De-Raining):** 论文 (#47) 结合特征融合注意力 (FFA) 网络和 CycleGAN 架构进行图像去雾、去雪、去雨，利用监督和无监督学习处理不成对数据，提升图像恢复质量。\n*   **DropletVideo：探索整体时空一致性视频生成的数据集与方法 (DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation):** 论文 (#59) 关注视频生成中的整体时空一致性（剧情合理性、视觉一致性、相机运动与剧情的协同），构建了包含 1000 万带详细标注视频的 DropletVideo-10M 数据集，并训练了 DropletVideo 模型。\n*   **迈向无歧义的空间基础模型：重新思考和解耦深度歧义 (Towards Ambiguity-Free Spatial Foundation Model: Rethinking and Decoupling Depth Ambiguity):** 论文 (#68) 针对空间场景理解中的深度歧义（尤其在透明场景），提出了从单预测到多假设模型的转变。发布了 MD-3k 基准，并提出 Laplacian Visual Prompting (LVP) 技术，无需重训练即可从预训练模型中提取隐藏的多层深度信息。\n\n**机器人、强化学习与控制:**\n\n*   **用于离散时间线性二次调节器的最优输出反馈学习控制 (Optimal Output Feedback Learning Control for Discrete-Time Linear Quadratic Regulation):** 论文 (#21) 研究了未知离散时间系统的 LQR 问题，提出一种广义动态输出反馈学习控制方法，保证收敛性、稳定性和最优性，无需状态观测器的收敛。\n*   **面向对象的世界模型用于语言引导的操作 (Object-Centric World Model for Language-Guided Manipulation):** 论文 (#35) 提出一个基于对象中心表示（槽注意力）的世界模型，用于语言指令引导的机器人操作任务。该模型在潜在空间预测未来状态，相比基于扩散的生成模型更紧凑高效。\n*   **ULTHO：深度强化学习中超轻量级且高效的超参数优化 (ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning):** 论文 (#48) 提出 ULTHO 框架，将 RL 中的 HPO 问题建模为带聚类臂的多臂老虎机 (MABC)，并将其与长期回报优化直接联系，实现了单次运行内快速、轻量级的 HPO。\n*   **T-CBF：基于可通行性的控制屏障函数用于导航垂直挑战性地形 (T-CBF: Traversability-based Control Barrier Function to Navigate Vertically Challenging Terrain):** 论文 (#50) 提出 T-CBF，使用神经控制屏障函数 (CBF) 来确保移动机器人在非结构化、垂直挑战性地形中的安全（超越碰撞避免，考虑翻车和固定），通过学习可通行性安全相关的观测来生成安全轨迹。\n*   **STAR：一个基础模型驱动的框架用于机器人系统的鲁棒任务规划和故障恢复 (STAR: A Foundation Model-driven Framework for Robust Task Planning and Failure Recovery in Robotic Systems):** 论文 (#56) 提出 STAR 框架，结合基础模型 (FM) 和动态扩展的知识图谱 (KG)，实现机器人任务规划和自主故障恢复。FM 用于推理诊断，KG 用于存储和重用知识，提高效率和可靠性。\n*   **变分随机博弈 (Vairiational Stochastic Games):** 论文 (#64) 将控制即推理 (CAI) 框架扩展到多智能体、通用和随机博弈 (SG) 的去中心化设置，提出了一个变分推理框架，证明了所得策略构成 ε-纳什均衡，并给出了理论收敛保证。\n*   **通过模仿周围车辆学习驾驶 (Learning to Drive by Imitating Surrounding Vehicles):** 论文 (#71) 提出一种数据增强策略，利用自动驾驶车辆传感器观察到的周围车辆轨迹作为额外的专家示范，来增强模仿学习的效果，并通过车辆选择采样策略优先选择信息丰富和多样化的行为。\n\n**其他值得关注的论文:**\n\n*   **AI 与数字孪生的协同：面向下一代网络优化、预测与安全 (Synergizing AI and Digital Twins for Next-Generation Network Optimization, Forecasting, and Security):** 论文 (#8) 探讨了数字网络孪生 (DNT) 与联邦学习 (FL)、强化学习 (RL) 的结合，用于解决 6G 网络中的优化（如边缘缓存）和安全（如车联网）挑战。\n*   **利用对抗记忆的单域泛化 (Single Domain Generalization with Adversarial Memory):** 论文 (#10) 提出一种单域泛化方法，利用对抗性记忆库增强训练特征，将训练和测试特征映射到由多样化记忆特征张成的不变子空间中，以应对训练数据多样性有限的挑战。\n*   **用于不完全信息 MAID 的高阶信念 (Higher-Order Belief in Incomplete Information MAIDs):** 论文 (#6) 扩展了多智能体影响图 (MAID)，引入了不完全信息 MAID (II-MAID)，能够表示智能体对博弈规则和彼此信念的不同信念，并定义了基于递归最优响应的解概念。\n*   **GraphGen+：在工业级图上推进分布式子图生成和图学习 (GraphGen+: Advancing Distributed Subgraph Generation and Graph Learning On Industrial Graphs):** 论文 (#22) 提出 GraphGen+ 框架，同步分布式子图生成与内存中图学习，无需外部存储，显著提高了工业级大规模图学习的效率。\n*   **用于工业级图的即时编译分布式图神经网络推理 (Distributed Graph Neural Network Inference With Just-In-Time Compilation For Industry-Scale Graphs):** 论文 (#25) 提出一种新的分布式图学习处理范式，利用即时编译 (JIT) 技术优化 GNN 推理，消除子图学习方法的缺点，在高达 5 亿节点、224 亿边的图上实现显著加速。\n*   **集成开发环境中的人-AI 体验：系统性文献综述 (Human-AI Experience in Integrated Development Environments: A Systematic Literature Review):** 论文 (#29) 对 89 项研究进行综述，总结了 AI 辅助编码对开发者生产力的提升以及引入的挑战（验证开销、自动化偏见等），强调了可解释性、验证机制和自适应控制的需求。\n*   **用于微控制器上高效稀疏深度神经网络的轻量级软件核与硬件扩展 (Lightweight Software Kernels and Hardware Extensions for Efficient Sparse Deep Neural Networks on Microcontrollers):** 论文 (#32) 针对 MCU 上的稀疏 DNN 加速，设计了优化的 N:M 稀疏层软件核、轻量级 ISA 扩展，并扩展了 DNN 编译器，在 ResNet18 和 ViT 上实现显著加速。\n*   **安全设备上无需反向传播的视频 OOD 检测 (Secure On-Device Video OOD Detection Without Backpropagation):** 论文 (#37) 提出 SecDOOD 框架，通过云端训练和设备端个性化参数生成（基于 HyperNetwork），实现无需设备反向传播的高效、安全的设备上 OOD 检测，同时通过动态特征采样和加密保护隐私。\n*   **VACT：视频自动因果测试系统与基准 (VACT: A Video Automatic Causal Testing System and a Benchmark):** 论文 (#38) 提出 VACT 框架，结合因果分析和 LLM 助手，自动评估视频生成模型 (VGM) 对物理规律和因果关系的理解，无需人工标注，并构建了基准测试。\n*   **用于语音情感识别的双模态连接注意力融合 (Bimodal Connection Attention Fusion for Speech Emotion Recognition):** 论文 (#41) 提出 BCAF 方法，通过交互连接网络、双模态注意力和相关性注意力网络，融合语音和文本特征，建模模态间连接和交互，以提升语音情感识别性能。\n*   **系统 0/1/2/3：多时间尺度具身集体认知系统的四过程理论 (System 0/1/2/3: Quad-process theory for multi-timescale embodied collective cognitive systems):** 论文 (#43) 扩展了双过程理论，提出包含系统 0（前认知具身过程）和系统 3（集体智能与符号涌现）的四过程认知模型，统一了从快速具身反应到缓慢演化的集体智能等不同时间尺度的认知过程。\n*   **MARRO：法律文档中修辞角色标注的多头注意力 (MARRO: Multi-headed Attention for Rhetorical Role Labeling in Legal Documents):** 论文 (#45) 提出 MARRO 模型系列，使用 Transformer 风格的多头注意力和多任务学习（标签转移作为辅助任务），改进法律文档中修辞角色（事实、论点、判决等）的标注效果。\n*   **ZO-DARTS++：一种高效且规模可变的零阶神经架构搜索算法 (ZO-DARTS++: An Efficient and Size-Variable Zeroth-Order Neural Architecture Search Algorithm):** 论文 (#49) 提出 ZO-DARTS++，一种 NAS 方法，结合零阶梯度近似、带温度退火的 sparsemax 和规模可变搜索方案，以平衡模型性能和资源约束，在医学影像数据集上表现优异。\n*   **GrInAdapt：通过基础、整合和适应多设备、多站点、多模态眼底域扩展视网膜血管结构图分割 (GrInAdapt: Scaling Retinal Vessel Structural Map Segmentation Through Grounding, Integrating and Adapting Multi-device, Multi-site, and Multi-modal Fundus Domains):** 论文 (#73) 提出 GrInAdapt 框架，用于眼底 OCTA 图像的无源多目标域适应，通过图像配准（接地）、多视图预测融合（整合）和模型适应，并可融合其他模态（如彩色眼底照相），提高跨设备、跨站点的视网膜血管分割鲁棒性。\n\n**快速浏览:**\n\n*   #5 探讨了 LLM 生成文本的统计特性，类比物态相变。\n*   #6 为 MAID 引入了更高阶的信念表示。\n*   #7 将深度学习和 MLLM 用于自动驾驶中的交通标志识别和车道线检测。\n*   #10 提出用对抗记忆库进行单域泛化。\n*   #12 使用 LSTM 预测多元温度时间序列。\n*   #17 使用因果时序表示进行婴儿啼哭声检测。\n*   #18 提出 LapSum 方法实现可微分的排序、Top-k 等操作。\n*   #20 介绍 Frank 系统，一个辅助用户进行数据标注的人在环 co-evolutionary 系统。\n*   #42 探索使用概率神经网络 (PNN) 估计电离层电子密度并量化不确定性。\n*   #46 研究基于视听数据的多模态表达性人格识别。\n*   #57 提出 MANDARIN 模型（混合专家网络）用于预测 ICU 患者的急性脑功能障碍（谵妄/昏迷）。\n*   #63 对比了 GenAI (GPT-4 Turbo) 和人类在归纳主题分析中的准确性和透明度。\n*   #66 对设备端 AI 模型进行了全面综述。",
  "papers": [
    {
      "arxiv_id": "2503.10662v1",
      "title": "Evaluation of the Automated Labeling Method for Taxonomic Nomenclature Through Prompt-Optimized Large Language Model",
      "title_zh": "通过提示优化的大语言模型评估分类命名法的自动标注方法",
      "authors": [
        "Keito Inoshita",
        "Kota Nojiri",
        "Haruto Sugeno",
        "Takumi Taga"
      ],
      "abstract": "Scientific names of organisms consist of a genus name and a species epithet,\nwith the latter often reflecting aspects such as morphology, ecology,\ndistribution, and cultural background. Traditionally, researchers have manually\nlabeled species names by carefully examining taxonomic descriptions, a process\nthat demands substantial time and effort when dealing with large datasets. This\nstudy evaluates the feasibility of automatic species name labeling using large\nlanguage model (LLM) by leveraging their text classification and semantic\nextraction capabilities. Using the spider name dataset compiled by Mammola et\nal., we compared LLM-based labeling results-enhanced through prompt\nengineering-with human annotations. The results indicate that LLM-based\nclassification achieved high accuracy in Morphology, Geography, and People\ncategories. However, classification accuracy was lower in Ecology & Behavior\nand Modern & Past Culture, revealing challenges in interpreting animal behavior\nand cultural contexts. Future research will focus on improving accuracy through\noptimized few-shot learning and retrieval-augmented generation techniques,\nwhile also expanding the applicability of LLM-based labeling to diverse\nbiological taxa.",
      "tldr_zh": "本研究评估了通过提示优化的大语言模型(LLM)自动标注生物分类学命名的可行性。利用LLM的文本分类和语义提取能力，研究以蜘蛛名称为数据集，比较了基于提示工程的LLM标注结果与人工标注的差异。结果显示，LLM在形态学(Morphology)、地理学(Geography)和人物(People)类别上表现出高准确率，但在生态与行为(Ecology & Behavior)以及现代与古代文化(Modern & Past Culture)类别上准确率较低，表明其在解释动物行为和文化背景方面存在挑战。未来研究将聚焦于通过优化的少样本学习(few-shot learning)和检索增强生成技术(retrieval-augmented generation)提高准确率，并扩展LLM标注在多种生物分类群中的应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper will be submitted to IEEE IAICT",
      "pdf_url": "http://arxiv.org/pdf/2503.10662v1",
      "published_date": "2025-03-08 23:11:43 UTC",
      "updated_date": "2025-03-08 23:11:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:09:56.259508"
    },
    {
      "arxiv_id": "2503.06353v1",
      "title": "The AI Pentad, the CHARME$^{2}$D Model, and an Assessment of Current-State AI Regulation",
      "title_zh": "AI五要素、CHARME$^{2}$D模型与现行AI监管评估",
      "authors": [
        "Di Kevin Gao",
        "Sudip Mittal",
        "Jiming Wu",
        "Hongwei Du",
        "Jingdao Chen",
        "Shahram Rahimi"
      ],
      "abstract": "Artificial Intelligence (AI) has made remarkable progress in the past few\nyears with AI-enabled applications beginning to permeate every aspect of our\nsociety. Despite the widespread consensus on the need to regulate AI, there\nremains a lack of a unified approach to framing, developing, and assessing AI\nregulations. Many of the existing methods take a value-based approach, for\nexample, accountability, fairness, free from bias, transparency, and trust.\nHowever, these methods often face challenges at the outset due to disagreements\nin academia over the subjective nature of these definitions. This paper aims to\nestablish a unifying model for AI regulation from the perspective of core AI\ncomponents. We first introduce the AI Pentad, which comprises the five\nessential components of AI: humans and organizations, algorithms, data,\ncomputing, and energy. We then review AI regulatory enablers, including AI\nregistration and disclosure, AI monitoring, and AI enforcement mechanisms.\nSubsequently, we present the CHARME$^{2}$D Model to explore further the\nrelationship between the AI Pentad and AI regulatory enablers. Finally, we\napply the CHARME$^{2}$D model to assess AI regulatory efforts in the European\nUnion (EU), China, the United Arab Emirates (UAE), the United Kingdom (UK), and\nthe United States (US), highlighting their strengths, weaknesses, and gaps.\nThis comparative evaluation offers insights for future legislative work in the\nAI domain.",
      "tldr_zh": "本文提出AI监管的统一框架，首先构建了\"AI五要素\"模型（Pentad），包含人类组织、算法、数据、计算和能源五个核心组成部分。研究进一步开发了CHARME²D模型，用于分析AI五要素与监管赋能机制（如AI注册、监测和执行）之间的关联。通过将该模型应用于欧盟、中国、阿联酋、英国和美国的AI监管实践，研究揭示了当前各国监管体系的优势、不足与空白，为未来AI立法工作提供了系统性评估工具。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06353v1",
      "published_date": "2025-03-08 22:58:41 UTC",
      "updated_date": "2025-03-08 22:58:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:10:08.207619"
    },
    {
      "arxiv_id": "2503.06343v1",
      "title": "Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning",
      "title_zh": "研究强化学习中演员与评论家表示之间的相互作用",
      "authors": [
        "Samuel Garcin",
        "Trevor McInroe",
        "Pablo Samuel Castro",
        "Prakash Panangaden",
        "Christopher G. Lucas",
        "David Abel",
        "Stefano V. Albrecht"
      ],
      "abstract": "Extracting relevant information from a stream of high-dimensional\nobservations is a central challenge for deep reinforcement learning agents.\nActor-critic algorithms add further complexity to this challenge, as it is\noften unclear whether the same information will be relevant to both the actor\nand the critic. To this end, we here explore the principles that underlie\neffective representations for the actor and for the critic in on-policy\nalgorithms. We focus our study on understanding whether the actor and critic\nwill benefit from separate, rather than shared, representations. Our primary\nfinding is that when separated, the representations for the actor and critic\nsystematically specialise in extracting different types of information from the\nenvironment -- the actor's representation tends to focus on action-relevant\ninformation, while the critic's representation specialises in encoding value\nand dynamics information. We conduct a rigourous empirical study to understand\nhow different representation learning approaches affect the actor and critic's\nspecialisations and their downstream performance, in terms of sample efficiency\nand generation capabilities. Finally, we discover that a separated critic plays\nan important role in exploration and data collection during training. Our code,\ntrained models and data are accessible at\nhttps://github.com/francelico/deac-rep.",
      "tldr_zh": "该研究探讨了强化学习中actor（执行者）和critic（评估者）表征之间的相互作用，重点分析了二者是否需要独立表征。研究发现，当采用独立表征时，actor会专注于提取与动作相关的信息，而critic则专门处理价值函数和环境动态信息。通过系统实验验证，独立表征能提升样本效率和生成能力，并发现分离的critic在训练探索阶段发挥关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at ICLR 2025. 10 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.06343v1",
      "published_date": "2025-03-08 21:29:20 UTC",
      "updated_date": "2025-03-08 21:29:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:10:13.408046"
    },
    {
      "arxiv_id": "2503.07663v1",
      "title": "Merge then Realign: Simple and Effective Modality-Incremental Continual Learning for Multimodal LLMs",
      "title_zh": "合并后重对齐：针对多模态大语言模型的简单高效模态增量持续学习方法",
      "authors": [
        "Dingkun Zhang",
        "Shuhan Qi",
        "Xinyu Xiao",
        "Kehai Chen",
        "Xuan Wang"
      ],
      "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have enhanced\ntheir versatility as they integrate a growing number of modalities. Considering\nthe heavy cost of training MLLMs, it is necessary to reuse the existing ones\nand further extend them to more modalities through Modality-incremental\nContinual Learning (MCL). However, this often comes with a performance\ndegradation in the previously learned modalities. In this work, we revisit the\nMCL and investigate a more severe issue it faces in contrast to traditional\ncontinual learning, that its degradation comes not only from catastrophic\nforgetting but also from the misalignment between the modality-agnostic and\nmodality-specific components. To address this problem, we propose an elegantly\nsimple MCL paradigm called \"MErge then ReAlign\" (MERA). Our method avoids\nintroducing heavy training overhead or modifying the model architecture, hence\nis easy to deploy and highly reusable in the MLLM community. Extensive\nexperiments demonstrate that, despite the simplicity of MERA, it shows\nimpressive performance, holding up to a 99.84% Backward Relative Gain when\nextending to four modalities, achieving a nearly lossless MCL performance.",
      "tldr_zh": "该研究提出了一种名为\"MErge then ReAlign\" (MERA)的简单有效的模态增量持续学习(MCL)方法，用于扩展多模态大语言模型(MLLMs)。MERA通过先合并再重新对齐的策略，解决了传统MCL中因模态无关和模态特定组件之间的错位导致的性能下降问题。该方法无需引入大量训练开销或修改模型架构，易于部署且具有高度可重用性。实验表明，MERA在扩展到四种模态时，保持了99.84%的后向相对增益，几乎实现了无损的MCL性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07663v1",
      "published_date": "2025-03-08 20:29:40 UTC",
      "updated_date": "2025-03-08 20:29:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:10:20.204746"
    },
    {
      "arxiv_id": "2503.06330v1",
      "title": "States of LLM-generated Texts and Phase Transitions between them",
      "title_zh": "LLM生成文本的状态及其间的相变",
      "authors": [
        "Nikolay Mikhaylovskiy"
      ],
      "abstract": "It is known for some time that autocorrelations of words in human-written\ntexts decay according to a power law. Recent works have also shown that the\nautocorrelations decay in texts generated by LLMs is qualitatively different\nfrom the literary texts. Solid state physics tie the autocorrelations decay\nlaws to the states of matter. In this work, we empirically demonstrate that,\ndepending on the temperature parameter, LLMs can generate text that can be\nclassified as solid, critical state or gas.",
      "tldr_zh": "这篇论文发现大型语言模型(LLMs)生成的文本存在三种状态相变：根据温度参数不同，可产生类似固态(solid)、临界态(critical state)或气态(gas)的文本。研究通过分析词语自相关性的幂律衰减特征，发现LLM生成文本的衰减规律与人类文学文本存在本质差异。该工作将固态物理中的物质状态概念引入文本分析，为理解LLM生成机制提供了新视角。",
      "categories": [
        "cs.CL",
        "cond-mat.stat-mech",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published as a conference paper at MathAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06330v1",
      "published_date": "2025-03-08 20:06:50 UTC",
      "updated_date": "2025-03-08 20:06:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:10:34.198423"
    },
    {
      "arxiv_id": "2503.06323v1",
      "title": "Higher-Order Belief in Incomplete Information MAIDs",
      "title_zh": "高阶信念在不完全信息MAIDs中的应用",
      "authors": [
        "Jack Foxabbott",
        "Rohan Subramani",
        "Francis Rhys Ward"
      ],
      "abstract": "Multi-agent influence diagrams (MAIDs) are probabilistic graphical models\nwhich represent strategic interactions between agents. MAIDs are equivalent to\nextensive form games (EFGs) but have a more compact and informative structure.\nHowever, MAIDs cannot, in general, represent settings of incomplete information\n-- wherein agents have different beliefs about the game being played, and\ndifferent beliefs about each-other's beliefs. In this paper, we introduce\nincomplete information MAIDs (II-MAIDs). We define both infinite and\nfinite-depth II-MAIDs and prove an equivalence relation to EFGs with incomplete\ninformation and no common prior over types. We prove that II-MAIDs inherit\nclassical equilibria concepts via this equivalence, but note that these\nsolution concepts are often unrealistic in the setting with no common prior\nbecause they violate common knowledge of rationality. We define a more\nrealistic solution concept based on recursive best-response. Throughout, we\ndescribe an example with a hypothetical AI agent undergoing evaluation to\nillustrate the applicability of II-MAIDs.",
      "tldr_zh": "本文提出了不完全信息多智能体影响图（II-MAIDs），扩展了传统多智能体影响图（MAIDs）以处理智能体对游戏和彼此信念的差异。II-MAIDs与无共同先验类型的不完全信息扩展形式博弈（EFGs）等价，并继承了经典均衡概念。然而，这些均衡在无共同先验的设定中往往不现实，因此作者提出了一种基于递归最佳响应的更现实的解概念。通过一个假设的AI代理评估示例，展示了II-MAIDs的适用性。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06323v1",
      "published_date": "2025-03-08 19:35:55 UTC",
      "updated_date": "2025-03-08 19:35:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:10:44.269609"
    },
    {
      "arxiv_id": "2503.06313v1",
      "title": "Advancing Autonomous Vehicle Intelligence: Deep Learning and Multimodal LLM for Traffic Sign Recognition and Robust Lane Detection",
      "title_zh": "提升自动驾驶智能：基于深度学习和多模态大语言模型的交通标志识别与鲁棒车道检测",
      "authors": [
        "Chandan Kumar Sah",
        "Ankit Kumar Shaw",
        "Xiaoli Lian",
        "Arsalan Shahid Baig",
        "Tuopu Wen",
        "Kun Jiang",
        "Mengmeng Yang",
        "Diange Yang"
      ],
      "abstract": "Autonomous vehicles (AVs) require reliable traffic sign recognition and\nrobust lane detection capabilities to ensure safe navigation in complex and\ndynamic environments. This paper introduces an integrated approach combining\nadvanced deep learning techniques and Multimodal Large Language Models (MLLMs)\nfor comprehensive road perception. For traffic sign recognition, we\nsystematically evaluate ResNet-50, YOLOv8, and RT-DETR, achieving\nstate-of-the-art performance of 99.8% with ResNet-50, 98.0% accuracy with\nYOLOv8, and achieved 96.6% accuracy in RT-DETR despite its higher computational\ncomplexity. For lane detection, we propose a CNN-based segmentation method\nenhanced by polynomial curve fitting, which delivers high accuracy under\nfavorable conditions. Furthermore, we introduce a lightweight, Multimodal,\nLLM-based framework that directly undergoes instruction tuning using small yet\ndiverse datasets, eliminating the need for initial pretraining. This framework\neffectively handles various lane types, complex intersections, and merging\nzones, significantly enhancing lane detection reliability by reasoning under\nadverse conditions. Despite constraints in available training resources, our\nmultimodal approach demonstrates advanced reasoning capabilities, achieving a\nFrame Overall Accuracy (FRM) of 53.87%, a Question Overall Accuracy (QNS) of\n82.83%, lane detection accuracies of 99.6% in clear conditions and 93.0% at\nnight, and robust performance in reasoning about lane invisibility due to rain\n(88.4%) or road degradation (95.6%). The proposed comprehensive framework\nmarkedly enhances AV perception reliability, thus contributing significantly to\nsafer autonomous driving across diverse and challenging road scenarios.",
      "tldr_zh": "本研究提出了一种结合深度学习与多模态大语言模型(MLLMs)的综合方法，用于提升自动驾驶车辆在复杂环境中的交通标志识别与车道检测能力。在交通标志识别方面，ResNet-50模型实现了99.8%的准确率，表现最优；在车道检测方面，基于CNN的分割方法结合多项式曲线拟合，在良好条件下表现出高精度。此外，研究还引入了一种轻量级的多模态LLM框架，通过小规模多样化数据集直接进行指令微调，无需预训练，显著提升了复杂场景下的车道检测可靠性。实验表明，该框架在夜间、雨天及道路退化等不利条件下均表现出色，为自动驾驶感知系统的安全性提供了重要支持。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06313v1",
      "published_date": "2025-03-08 19:12:36 UTC",
      "updated_date": "2025-03-08 19:12:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:10:49.822806"
    },
    {
      "arxiv_id": "2503.06302v1",
      "title": "Synergizing AI and Digital Twins for Next-Generation Network Optimization, Forecasting, and Security",
      "title_zh": "协同人工智能与数字孪生技术，优化下一代网络预测与安全",
      "authors": [
        "Zifan Zhang",
        "Minghong Fang",
        "Dianwei Chen",
        "Xianfeng Yang",
        "Yuchen Liu"
      ],
      "abstract": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
      "tldr_zh": "该研究探讨了数字网络孪生(DNT)与机器学习(ML)技术的协同作用，特别是联邦学习(FL)和强化学习(RL)，以应对6G网络中的关键挑战。研究提出了整合DNT和ML的框架，显著提升了网络优化与安全性：在边缘缓存场景实现80%以上的命中率并均衡基站负载，在自动驾驶系统中保持100%无碰撞率。这些成果为构建智能自适应的下一代网络系统提供了重要参考。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "Accepted by IEEE Wireless Communications",
      "pdf_url": "http://arxiv.org/pdf/2503.06302v1",
      "published_date": "2025-03-08 18:30:54 UTC",
      "updated_date": "2025-03-08 18:30:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:10:45.394872"
    },
    {
      "arxiv_id": "2503.09620v2",
      "title": "Exploiting Edited Large Language Models as General Scientific Optimizers",
      "title_zh": "利用编辑后的大型语言模型作为通用科学优化器",
      "authors": [
        "Qitan Lv",
        "Tianyu Liu",
        "Hong Wang"
      ],
      "abstract": "Large language models (LLMs) have been widely adopted in mathematical\noptimization in scientific scenarios for their extensive knowledge and advanced\nreasoning capabilities. Existing methods mainly focus on utilizing LLMs to\nsolve optimization problems in a prompt-based manner, which takes observational\nfeedback as additional textual descriptions. However, due to LLM's \\textbf{high\nsensitivity to the prompts} and \\textbf{tendency to get lost in lengthy\nprompts}, these methods struggle to effectively utilize the {observational}\nfeedback from each optimization step, which severely hinders the applications\nfor real-world scenarios. To address these challenges, we propose a\nconceptually simple and general {bi-level} optimization method, namely\n\\textbf{G}eneral \\textbf{S}cientific \\textbf{O}ptimizers (GSO). Specifically,\nGSO first utilizes inner-level simulators as experimental platforms to evaluate\nthe current solution and provide observational feedback. Then, LLMs serve as\nknowledgeable and versatile scientists, generating new solutions by refining\npotential errors from the feedback as the outer-level optimization. Finally,\nsimulations together with the expert knowledge in LLMs are jointly updated with\nbi-level interactions via model editing. Extensive experiments show that GSO\nconsistently outperforms existing state-of-the-art methods using \\textit{six}\ndifferent LLM backbones on \\textit{seven} different tasks, demonstrating the\neffectiveness and a wide range of applications.",
      "tldr_zh": "该研究提出了一种名为GSO（General Scientific Optimizers）的双层优化方法，旨在利用编辑后的大型语言模型（LLMs）作为通用科学优化器。GSO通过内层模拟器评估当前解决方案并提供观测反馈，外层则利用LLMs作为知识渊博的科学家，根据反馈修正潜在错误并生成新解决方案。通过模型编辑实现双层交互，GSO在六种LLM骨干模型和七项任务上均优于现有方法，展示了其广泛的应用潜力。",
      "categories": [
        "math.OC",
        "cs.AI"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.09620v2",
      "published_date": "2025-03-08 18:01:11 UTC",
      "updated_date": "2025-03-17 05:40:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:10:57.959973"
    },
    {
      "arxiv_id": "2503.06288v1",
      "title": "Single Domain Generalization with Adversarial Memory",
      "title_zh": "基于对抗记忆的单域泛化",
      "authors": [
        "Hao Yan",
        "Marzi Heidari",
        "Yuhong Guo"
      ],
      "abstract": "Domain Generalization (DG) aims to train models that can generalize to unseen\ntesting domains by leveraging data from multiple training domains. However,\ntraditional DG methods rely on the availability of multiple diverse training\ndomains, limiting their applicability in data-constrained scenarios. Single\nDomain Generalization (SDG) addresses the more realistic and challenging\nsetting by restricting the training data to a single domain distribution. The\nmain challenges in SDG stem from the limited diversity of training data and the\ninaccessibility of unseen testing data distributions. To tackle these\nchallenges, we propose a single domain generalization method that leverages an\nadversarial memory bank to augment training features. Our memory-based feature\naugmentation network maps both training and testing features into an invariant\nsubspace spanned by diverse memory features, implicitly aligning the training\nand testing domains in the projected space. To maintain a diverse and\nrepresentative feature memory bank, we introduce an adversarial feature\ngeneration method that creates features extending beyond the training domain\ndistribution. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance on standard single domain generalization\nbenchmarks.",
      "tldr_zh": "该研究提出了一种基于对抗性记忆的单域泛化(Single Domain Generalization, SDG)方法，解决了传统域泛化方法依赖多域训练数据的局限性。通过引入对抗性记忆库，该方法在训练特征映射到不变子空间的同时，生成超出训练域分布的对抗性特征，增强特征多样性。实验表明，该方法在标准单域泛化基准测试中取得了最先进的性能，为数据受限场景下的模型泛化提供了有效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06288v1",
      "published_date": "2025-03-08 17:27:42 UTC",
      "updated_date": "2025-03-08 17:27:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:11:00.541138"
    },
    {
      "arxiv_id": "2503.06287v1",
      "title": "Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding",
      "title_zh": "你的大型视觉语言模型仅需少数注意力头即可实现视觉定位",
      "authors": [
        "Seil Kang",
        "Jinyeong Kim",
        "Junhyeok Kim",
        "Seong Jae Hwang"
      ],
      "abstract": "Visual grounding seeks to localize the image region corresponding to a\nfree-form text description. Recently, the strong multimodal capabilities of\nLarge Vision-Language Models (LVLMs) have driven substantial improvements in\nvisual grounding, though they inevitably require fine-tuning and additional\nmodel components to explicitly generate bounding boxes or segmentation masks.\nHowever, we discover that a few attention heads in frozen LVLMs demonstrate\nstrong visual grounding capabilities. We refer to these heads, which\nconsistently capture object locations related to text semantics, as\nlocalization heads. Using localization heads, we introduce a straightforward\nand effective training-free visual grounding framework that utilizes\ntext-to-image attention maps from localization heads to identify the target\nobjects. Surprisingly, only three out of thousands of attention heads are\nsufficient to achieve competitive localization performance compared to existing\nLVLM-based visual grounding methods that require fine-tuning. Our findings\nsuggest that LVLMs can innately ground objects based on a deep comprehension of\nthe text-image relationship, as they implicitly focus on relevant image regions\nto generate informative text outputs. All the source codes will be made\navailable to the public.",
      "tldr_zh": "研究发现，大型视觉语言模型（LVLMs）仅需少量注意力头（attention heads）即可实现视觉定位（visual grounding），无需微调或额外组件。通过识别模型中自然存在的\"定位头\"（localization heads），研究者提出了一种无需训练的新方法，利用文本-图像注意力图直接定位目标对象。实验表明，仅需3个注意力头就能达到与需要微调的现有方法相当的性能，揭示了LVLMs通过隐含关注图像相关区域来理解文本-图像关系的内在能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06287v1",
      "published_date": "2025-03-08 17:24:42 UTC",
      "updated_date": "2025-03-08 17:24:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:11:27.860736"
    },
    {
      "arxiv_id": "2503.06278v1",
      "title": "Applied Machine Learning Methods with Long-Short Term Memory Based Recurrent Neural Networks for Multivariate Temperature Prediction",
      "title_zh": "基于长短期记忆循环神经网络的应用机器学习方法在多元温度预测中的研究",
      "authors": [
        "Bojan Lukić"
      ],
      "abstract": "This paper gives an overview on how to develop a dense and deep neural\nnetwork for making a time series prediction. First, the history and\ncornerstones in Artificial Intelligence and Machine Learning will be presented.\nAfter a short introduction to the theory of Artificial Intelligence and Machine\nLearning, the paper will go deeper into the techniques for conducting a time\nseries prediction with different models of neural networks. For this project,\nPython's development environment Jupyter, extended with the TensorFlow package\nand deep-learning application Keras is used. The system setup and project\nframework are explained in more detail before discussing the time series\nprediction. The main part shows an applied example of time series prediction\nwith weather data. For this work, a deep recurrent neural network with Long\nShort-Term Memory cells is used to conduct the time series prediction. The\nresults and evaluation of the work show that a weather prediction with deep\nneural networks can be successful for a short time period. However, there are\nsome drawbacks and limitations with time series prediction, which will be\ndiscussed towards the end of the paper.",
      "tldr_zh": "本文探讨了使用基于长短期记忆(LSTM)单元的深度循环神经网络(RNN)进行多元温度时间序列预测的方法。研究利用Python的Jupyter开发环境，结合TensorFlow和Keras框架，构建了密集的深度神经网络模型。实验结果表明，该模型在短期天气预测中表现良好，但也揭示了时间序列预测存在的一些局限性和挑战。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 16 figures, private research",
      "pdf_url": "http://arxiv.org/pdf/2503.06278v1",
      "published_date": "2025-03-08 16:52:27 UTC",
      "updated_date": "2025-03-08 16:52:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:11:21.991492"
    },
    {
      "arxiv_id": "2503.06269v1",
      "title": "Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models",
      "title_zh": "利用机制可解释性构建针对大型语言模型的对抗性攻击",
      "authors": [
        "Thomas Winninger",
        "Boussad Addad",
        "Katarzyna Kapusta"
      ],
      "abstract": "Traditional white-box methods for creating adversarial perturbations against\nLLMs typically rely only on gradient computation from the targeted model,\nignoring the internal mechanisms responsible for attack success or failure.\nConversely, interpretability studies that analyze these internal mechanisms\nlack practical applications beyond runtime interventions. We bridge this gap by\nintroducing a novel white-box approach that leverages mechanistic\ninterpretability techniques to craft practical adversarial inputs.\nSpecifically, we first identify acceptance subspaces - sets of feature vectors\nthat do not trigger the model's refusal mechanisms - then use gradient-based\noptimization to reroute embeddings from refusal subspaces to acceptance\nsubspaces, effectively achieving jailbreaks. This targeted approach\nsignificantly reduces computation cost, achieving attack success rates of\n80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5\nwithin minutes or even seconds, compared to existing techniques that often fail\nor require hours of computation. We believe this approach opens a new direction\nfor both attack research and defense development. Furthermore, it showcases a\npractical application of mechanistic interpretability where other methods are\nless efficient, which highlights its utility. The code and generated datasets\nare available at https://github.com/Sckathach/subspace-rerouting.",
      "tldr_zh": "该研究提出了一种基于机制可解释性(Mechanistic Interpretability)的新型白盒对抗攻击方法，用于针对大型语言模型(LLMs)生成高效对抗样本。通过识别模型的“接受子空间”(acceptance subspaces)并利用梯度优化将嵌入从“拒绝子空间”重新路由到“接受子空间”，该方法成功实现了对模型的“越狱”(jailbreak)。相比传统方法，该技术在Gemma2、Llama3.2和Qwen2.5等先进模型上以分钟甚至秒级时间实现了80-95%的攻击成功率，显著降低了计算成本。这一研究不仅为对抗攻击和防御提供了新方向，也展示了机制可解释性在实际应用中的高效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06269v1",
      "published_date": "2025-03-08 16:29:45 UTC",
      "updated_date": "2025-03-08 16:29:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:11:45.179348"
    },
    {
      "arxiv_id": "2503.06263v1",
      "title": "Critical Foreign Policy Decisions (CFPD)-Benchmark: Measuring Diplomatic Preferences in Large Language Models",
      "title_zh": "关键外交政策决策（CFPD）基准：衡量大语言模型中的外交偏好",
      "authors": [
        "Benjamin Jensen",
        "Ian Reynolds",
        "Yasir Atalan",
        "Michael Garcia",
        "Austin Woo",
        "Anthony Chen",
        "Trevor Howarth"
      ],
      "abstract": "As national security institutions increasingly integrate Artificial\nIntelligence (AI) into decision-making and content generation processes,\nunderstanding the inherent biases of large language models (LLMs) is crucial.\nThis study presents a novel benchmark designed to evaluate the biases and\npreferences of seven prominent foundation models-Llama 3.1 8B Instruct, Llama\n3.1 70B Instruct, GPT-4o, Gemini 1.5 Pro-002, Mixtral 8x22B, Claude 3.5 Sonnet,\nand Qwen2 72B-in the context of international relations (IR). We designed a\nbias discovery study around core topics in IR using 400-expert crafted\nscenarios to analyze results from our selected models. These scenarios focused\non four topical domains including: military escalation, military and\nhumanitarian intervention, cooperative behavior in the international system,\nand alliance dynamics. Our analysis reveals noteworthy variation among model\nrecommendations based on scenarios designed for the four tested domains.\nParticularly, Qwen2 72B, Gemini 1.5 Pro-002 and Llama 3.1 8B Instruct models\noffered significantly more escalatory recommendations than Claude 3.5 Sonnet\nand GPT-4o models. All models exhibit some degree of country-specific biases,\noften recommending less escalatory and interventionist actions for China and\nRussia compared to the United States and the United Kingdom. These findings\nhighlight the necessity for controlled deployment of LLMs in high-stakes\nenvironments, emphasizing the need for domain-specific evaluations and model\nfine-tuning to align with institutional objectives.",
      "tldr_zh": "该研究提出了CFPD-Benchmark基准测试，用于评估七种主流大语言模型（包括GPT-4o、Gemini 1.5 Pro等）在国际关系决策中的偏见表现。通过400个专家设计的国际关系场景测试发现，不同模型在军事升级、人道干预等四个关键领域存在显著差异，其中Qwen2和Gemini模型比Claude和GPT-4o更具\"升级性\"建议。所有模型都表现出对国家特定的偏见倾向，对中国和俄罗斯往往建议比美英更温和的措施。研究表明在大语言模型应用于国家安全决策前，必须进行领域特定评估和针对性微调。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "I.2.7"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06263v1",
      "published_date": "2025-03-08 16:19:13 UTC",
      "updated_date": "2025-03-08 16:19:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:12:25.595709"
    },
    {
      "arxiv_id": "2503.06260v1",
      "title": "From Captions to Rewards (CAREVL): Leveraging Large Language Model Experts for Enhanced Reward Modeling in Large Vision-Language Models",
      "title_zh": "从字幕到奖励（CAREVL）：利用大型语言模型专家增强大规模视觉语言模型的奖励建模",
      "authors": [
        "Muzhi Dai",
        "Jiashuo Sun",
        "Zhiyuan Zhao",
        "Shixuan Liu",
        "Rui Li",
        "Junyu Gao",
        "Xuelong Li"
      ],
      "abstract": "Aligning large vision-language models (LVLMs) with human preferences is\nchallenging due to the scarcity of fine-grained, high-quality, and multimodal\npreference data without human annotations. Existing methods relying on direct\ndistillation often struggle with low-confidence data, leading to suboptimal\nperformance. To address this, we propose CAREVL, a novel method for preference\nreward modeling by reliably using both high- and low-confidence data. First, a\ncluster of auxiliary expert models (textual reward models) innovatively\nleverages image captions as weak supervision signals to filter high-confidence\ndata. The high-confidence data are then used to fine-tune the LVLM. Second,\nlow-confidence data are used to generate diverse preference samples using the\nfine-tuned LVLM. These samples are then scored and selected to construct\nreliable chosen-rejected pairs for further training. CAREVL achieves\nperformance improvements over traditional distillation-based methods on\nVL-RewardBench and MLLM-as-a-Judge benchmark, demonstrating its effectiveness.\nThe code will be released soon.",
      "tldr_zh": "该研究提出CAREVL方法，通过利用大型语言模型专家来增强视觉语言模型(LVLMs)的奖励建模。该方法创新性地使用图像标题作为弱监督信号，通过辅助专家模型筛选高置信度数据用于微调LVLM，并利用低置信度数据生成多样化偏好样本进行二次训练。实验表明，CAREVL在VL-RewardBench和MLLM-as-a-Judge基准测试中优于传统蒸馏方法，显著提升了模型与人类偏好的对齐效果。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06260v1",
      "published_date": "2025-03-08 16:13:18 UTC",
      "updated_date": "2025-03-08 16:13:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:11:45.030750"
    },
    {
      "arxiv_id": "2503.06252v1",
      "title": "Can Atomic Step Decomposition Enhance the Self-structured Reasoning of Multimodal Large Models?",
      "title_zh": "原子步骤分解能否增强多模态大模型的自结构化推理能力？",
      "authors": [
        "Kun Xiang",
        "Zhili Liu",
        "Zihao Jiang",
        "Yunshuang Nie",
        "Kaixin Cai",
        "Yiyang Yin",
        "Runhui Huang",
        "Haoxiang Fan",
        "Hanhui Li",
        "Weiran Huang",
        "Yihan Zeng",
        "Yu-Jie Yuan",
        "Jianhua Han",
        "Lanqing Hong",
        "Hang Xu",
        "Xiaodan Liang"
      ],
      "abstract": "In this paper, we address the challenging task of multimodal mathematical\nreasoning by incorporating the ability of \"slow thinking\" into multimodal large\nlanguage models (MLLMs). Our core idea is that different levels of reasoning\nabilities can be combined dynamically to tackle questions with different\ncomplexity. To this end, we propose a paradigm of Self-structured Chain of\nThought (SCoT), which is composed of minimal semantic atomic steps. Different\nfrom existing methods that rely on structured templates or free-form paradigms,\nour method can not only generate cognitive CoT structures for various complex\ntasks but also mitigates the phenomenon of overthinking. To introduce\nstructured reasoning capabilities into visual understanding models, we further\ndesign a novel AtomThink framework with four key modules, including (i) a data\nengine to generate high-quality multimodal reasoning paths; (ii) a supervised\nfine-tuning process with serialized inference data; (iii) a policy-guided\nmulti-turn inference method; and (iv) an atomic capability metric to evaluate\nthe single step utilization rate. We conduct extensive experiments to show that\nthe proposed AtomThink significantly improves the performance of baseline\nMLLMs, achieving more than 10\\% average accuracy gains on MathVista and\nMathVerse. Compared to state-of-the-art structured CoT approaches, our method\nnot only achieves higher accuracy but also improves data utilization by 5 times\nand boosts inference efficiency by 85.3\\%. Our code is now public available in\nhttps://github.com/Quinn777/AtomThink.",
      "tldr_zh": "该研究提出了一种名为\"自结构化思维链\"(SCoT)的新范式，通过将推理过程分解为最小语义原子步骤来增强多模态大语言模型(MLLMs)的数学推理能力。研究者开发了AtomThink框架，包含数据引擎生成高质量推理路径、监督微调、策略引导多轮推理和原子能力评估四个核心模块。实验表明，该方法在MathVista和MathVerse基准测试中平均准确率提升超过10%，相比现有结构化思维链方法不仅精度更高，还能将数据利用率提高5倍、推理效率提升85.3%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06252v1",
      "published_date": "2025-03-08 15:23:47 UTC",
      "updated_date": "2025-03-08 15:23:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:12:36.074510"
    },
    {
      "arxiv_id": "2503.06247v1",
      "title": "Infant Cry Detection Using Causal Temporal Representation",
      "title_zh": "基于因果时序表征的婴儿哭声检测",
      "authors": [
        "Minghao Fu",
        "Danning Li",
        "Aryan Gadhiya",
        "Benjamin Lambright",
        "Mohamed Alowais",
        "Mohab Bahnassy",
        "Saad El Dine Elletter",
        "Hawau Olamide Toyin",
        "Haiyan Jiang",
        "Kun Zhang",
        "Hanan Aldarmaki"
      ],
      "abstract": "This paper addresses a major challenge in acoustic event detection, in\nparticular infant cry detection in the presence of other sounds and background\nnoises: the lack of precise annotated data. We present two contributions for\nsupervised and unsupervised infant cry detection. The first is an annotated\ndataset for cry segmentation, which enables supervised models to achieve\nstate-of-the-art performance. Additionally, we propose a novel unsupervised\nmethod, Causal Representation Spare Transition Clustering (CRSTC), based on\ncausal temporal representation, which helps address the issue of data scarcity\nmore generally. By integrating the detected cry segments, we significantly\nimprove the performance of downstream infant cry classification, highlighting\nthe potential of this approach for infant care applications.",
      "tldr_zh": "本研究针对婴儿哭声检测中的关键挑战——缺乏精确标注数据，提出了两项贡献。首先，发布了一个用于哭声分割的标注数据集，使监督模型达到了最先进的性能。其次，提出了一种基于因果时序表示的无监督方法——因果表示稀疏转移聚类(CRSTC)，有效缓解了数据稀缺问题。通过整合检测到的哭声片段，显著提升了婴儿哭声分类的下游任务性能，展示了该方法在婴儿护理应用中的潜力。",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06247v1",
      "published_date": "2025-03-08 15:15:23 UTC",
      "updated_date": "2025-03-08 15:15:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:12:05.316771"
    },
    {
      "arxiv_id": "2503.06242v1",
      "title": "LapSum -- One Method to Differentiate Them All: Ranking, Sorting and Top-k Selection",
      "title_zh": "LapSum —— 一法通万法：排序、分类与 Top-k 选择",
      "authors": [
        "Łukasz Struski",
        "Michał B. Bednarczyk",
        "Igor T. Podolak",
        "Jacek Tabor"
      ],
      "abstract": "We present a novel technique for constructing differentiable order-type\noperations, including soft ranking, soft top-k selection, and soft\npermutations. Our approach leverages an efficient closed-form formula for the\ninverse of the function LapSum, defined as the sum of Laplace distributions.\nThis formulation ensures low computational and memory complexity in selecting\nthe highest activations, enabling losses and gradients to be computed in\n$O(n\\log{}n)$ time. Through extensive experiments, we demonstrate that our\nmethod outperforms state-of-the-art techniques for high-dimensional vectors and\nlarge $k$ values. Furthermore, we provide efficient implementations for both\nCPU and CUDA environments, underscoring the practicality and scalability of our\nmethod for large-scale ranking and differentiable ordering problems.",
      "tldr_zh": "本研究提出了一种新颖的LapSum方法，用于构建可微分的排序类操作，包括软排序(soft ranking)、软Top-k选择(soft top-k selection)和软排列(soft permutations)。该方法利用拉普拉斯分布和的逆函数的高效闭式解，实现了低计算和内存复杂度，可在$O(n\\log{}n)$时间内计算损失和梯度。实验表明，该方法在高维向量和大$k$值场景下优于现有技术，并提供了高效的CPU和CUDA实现，适用于大规模排序和可微分排序问题。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06242v1",
      "published_date": "2025-03-08 14:53:36 UTC",
      "updated_date": "2025-03-08 14:53:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:12:08.995646"
    },
    {
      "arxiv_id": "2503.06238v1",
      "title": "Image is All You Need: Towards Efficient and Effective Large Language Model-Based Recommender Systems",
      "title_zh": "图像即所需：迈向高效且基于大语言模型的推荐系统",
      "authors": [
        "Kibum Kim",
        "Sein Kim",
        "Hongseok Kang",
        "Jiwan Kim",
        "Heewoong Noh",
        "Yeonjun In",
        "Kanghoon Yoon",
        "Jinoh Oh",
        "Chanyoung Park"
      ],
      "abstract": "Large Language Models (LLMs) have recently emerged as a powerful backbone for\nrecommender systems. Existing LLM-based recommender systems take two different\napproaches for representing items in natural language, i.e., Attribute-based\nRepresentation and Description-based Representation. In this work, we aim to\naddress the trade-off between efficiency and effectiveness that these two\napproaches encounter, when representing items consumed by users. Based on our\ninteresting observation that there is a significant information overlap between\nimages and descriptions associated with items, we propose a novel method, Image\nis all you need for LLM-based Recommender system (I-LLMRec). Our main idea is\nto leverage images as an alternative to lengthy textual descriptions for\nrepresenting items, aiming at reducing token usage while preserving the rich\nsemantic information of item descriptions. Through extensive experiments, we\ndemonstrate that I-LLMRec outperforms existing methods in both efficiency and\neffectiveness by leveraging images. Moreover, a further appeal of I-LLMRec is\nits ability to reduce sensitivity to noise in descriptions, leading to more\nrobust recommendations.",
      "tldr_zh": "该论文提出了一种基于大语言模型(LLM)的高效推荐系统I-LLMRec，通过利用图像代替冗长的文本描述来表示物品，解决了现有方法在效率和效果上的权衡问题。实验表明，I-LLMRec在减少token使用的同时保留了丰富的语义信息，显著提升了推荐系统的效率和效果。此外，该方法还降低了对描述噪声的敏感性，增强了推荐的鲁棒性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06238v1",
      "published_date": "2025-03-08 14:51:16 UTC",
      "updated_date": "2025-03-08 14:51:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:12:21.673914"
    },
    {
      "arxiv_id": "2503.06229v1",
      "title": "A Frank System for Co-Evolutionary Hybrid Decision-Making",
      "title_zh": "Frank系统：协同进化混合决策框架",
      "authors": [
        "Federico Mazzoni",
        "Riccardo Guidotti",
        "Alessio Malizia"
      ],
      "abstract": "We introduce Frank, a human-in-the-loop system for co-evolutionary hybrid\ndecision-making aiding the user to label records from an un-labeled dataset.\nFrank employs incremental learning to ``evolve'' in parallel with the user's\ndecisions, by training an interpretable machine learning model on the records\nlabeled by the user. Furthermore, Frank advances state-of-the-art approaches by\noffering inconsistency controls, explanations, fairness checks, and bad-faith\nsafeguards simultaneously. We evaluate our proposal by simulating the users'\nbehavior with various levels of expertise and reliance on Frank's suggestions.\nThe experiments show that Frank's intervention leads to improvements in the\naccuracy and the fairness of the decisions.",
      "tldr_zh": "该研究提出了Frank系统，一种人机协同的共进化混合决策框架，用于辅助用户标注无标签数据集。该系统通过增量学习机制与用户决策同步进化，同时训练可解释的机器学习模型，并整合了不一致性控制、解释生成、公平性检查和恶意行为防护等创新功能。实验模拟表明，在不同专业水平用户参与下，Frank的介入能显著提升决策准确性和公平性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.06229v1",
      "published_date": "2025-03-08 14:06:16 UTC",
      "updated_date": "2025-03-08 14:06:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:12:40.969067"
    },
    {
      "arxiv_id": "2503.06226v2",
      "title": "Optimal Output Feedback Learning Control for Discrete-Time Linear Quadratic Regulation",
      "title_zh": "离散时间线性二次调节的最优输出反馈学习控制",
      "authors": [
        "Kedi Xie",
        "Martin Guay",
        "Shimin Wang",
        "Fang Deng",
        "Maobin Lu"
      ],
      "abstract": "This paper studies the linear quadratic regulation (LQR) problem of unknown\ndiscrete-time systems via dynamic output feedback learning control. In contrast\nto the state feedback, the optimality of the dynamic output feedback control\nfor solving the LQR problem requires an implicit condition on the convergence\nof the state observer. Moreover, due to unknown system matrices and the\nexistence of observer error, it is difficult to analyze the convergence and\nstability of most existing output feedback learning-based control methods. To\ntackle these issues, we propose a generalized dynamic output feedback learning\ncontrol approach with guaranteed convergence, stability, and optimality\nperformance for solving the LQR problem of unknown discrete-time linear\nsystems. In particular, a dynamic output feedback controller is designed to be\nequivalent to a state feedback controller. This equivalence relationship is an\ninherent property without requiring convergence of the estimated state by the\nstate observer, which plays a key role in establishing the off-policy learning\ncontrol approaches. By value iteration and policy iteration schemes, the\nadaptive dynamic programming based learning control approaches are developed to\nestimate the optimal feedback control gain. In addition, a model-free stability\ncriterion is provided by finding a nonsingular parameterization matrix, which\ncontributes to establishing a switched iteration scheme. Furthermore, the\nconvergence, stability, and optimality analyses of the proposed output feedback\nlearning control approaches are given. Finally, the theoretical results are\nvalidated by two numerical examples.",
      "tldr_zh": "本文研究了未知离散时间系统的线性二次调节(LQR)问题，提出了一种基于动态输出反馈学习的控制方法。与状态反馈不同，动态输出反馈控制的最优性依赖于状态观测器的收敛性。为解决系统矩阵未知和观测误差带来的挑战，作者设计了一种广义动态输出反馈学习控制方法，通过值迭代和策略迭代方案，结合自适应动态规划，估计最优反馈控制增益。该方法确保了收敛性、稳定性和最优性，并通过数值实验验证了理论结果。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "math.OC"
      ],
      "primary_category": "eess.SY",
      "comment": "16 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06226v2",
      "published_date": "2025-03-08 14:02:16 UTC",
      "updated_date": "2025-03-11 18:32:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:13:18.235881"
    },
    {
      "arxiv_id": "2503.06212v1",
      "title": "GraphGen+: Advancing Distributed Subgraph Generation and Graph Learning On Industrial Graphs",
      "title_zh": "GraphGen+：工业级图数据分布式子图生成与图学习的进阶方案",
      "authors": [
        "Yue Jin",
        "Yongchao Liu",
        "Chuntao Hong"
      ],
      "abstract": "Graph-based computations are crucial in a wide range of applications, where\ngraphs can scale to trillions of edges. To enable efficient training on such\nlarge graphs, mini-batch subgraph sampling is commonly used, which allows\ntraining without loading the entire graph into memory. However, existing\nsolutions face significant trade-offs: online subgraph generation, as seen in\nframeworks like DGL and PyG, is limited to a single machine, resulting in\nsevere performance bottlenecks, while offline precomputed subgraphs, as in\nGraphGen, improve sampling efficiency but introduce large storage overhead and\nhigh I/O costs during training. To address these challenges, we propose\n\\textbf{GraphGen+}, an integrated framework that synchronizes distributed\nsubgraph generation with in-memory graph learning, eliminating the need for\nexternal storage while significantly improving efficiency. GraphGen+ achieves a\n\\textbf{27$\\times$} speedup in subgraph generation compared to conventional\nSQL-like methods and a \\textbf{1.3$\\times$} speedup over GraphGen, supporting\ntraining on 1 million nodes per iteration and removing the overhead associated\nwith precomputed subgraphs, making it a scalable and practical solution for\nindustry-scale graph learning.",
      "tldr_zh": "该研究提出了GraphGen+，一种创新的分布式子图生成与图学习框架，旨在解决大规模工业图计算中的性能瓶颈和存储开销问题。GraphGen+通过将分布式子图生成与内存中的图学习同步，消除了对外部存储的需求，显著提升了效率。实验表明，GraphGen+在子图生成速度上比传统SQL类方法快27倍，比GraphGen快1.3倍，支持每次迭代训练100万个节点，为工业级图学习提供了高效、可扩展的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted By EuroSys 2025 (poster)",
      "pdf_url": "http://arxiv.org/pdf/2503.06212v1",
      "published_date": "2025-03-08 13:29:42 UTC",
      "updated_date": "2025-03-08 13:29:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:13:07.946027"
    },
    {
      "arxiv_id": "2503.06211v1",
      "title": "Text-Speech Language Models with Improved Cross-Modal Transfer by Aligning Abstraction Levels",
      "title_zh": "《文本-语音语言模型：通过抽象层级对齐实现改进的跨模态迁移》",
      "authors": [
        "Santiago Cuervo",
        "Adel Moumen",
        "Yanis Labrak",
        "Sameer Khurana",
        "Antoine Laurent",
        "Mickael Rouvier",
        "Ricard Marxer"
      ],
      "abstract": "Text-Speech Language Models (TSLMs) -- language models trained to jointly\nprocess and generate text and speech -- aim to enable cross-modal knowledge\ntransfer to overcome the scaling limitations of unimodal speech LMs. The\npredominant approach to TSLM training expands the vocabulary of a pre-trained\ntext LM by appending new embeddings and linear projections for speech, followed\nby fine-tuning on speech data. We hypothesize that this method limits\ncross-modal transfer by neglecting feature compositionality, preventing\ntext-learned functions from being fully leveraged at appropriate abstraction\nlevels. To address this, we propose augmenting vocabulary expansion with\nmodules that better align abstraction levels across layers. Our models,\n\\textsc{SmolTolk}, rival or surpass state-of-the-art TSLMs trained with orders\nof magnitude more compute. Representation analyses and improved multimodal\nperformance suggest our method enhances cross-modal transfer.",
      "tldr_zh": "该研究提出了一种改进文本-语音语言模型(TSLMs)跨模态迁移能力的新方法。针对现有方法在预训练文本LM基础上简单扩展语音嵌入会导致特征组合性不足的问题，研究者通过设计对齐抽象层次的模块来增强跨层迁移。实验表明，他们开发的SmolTolk模型在计算资源大幅减少的情况下，性能媲美甚至超越当前最优TSLMs，表征分析和跨模态任务表现的提升验证了该方法有效改进了跨模态知识迁移。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06211v1",
      "published_date": "2025-03-08 13:28:50 UTC",
      "updated_date": "2025-03-08 13:28:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:13:08.615338"
    },
    {
      "arxiv_id": "2503.10660v2",
      "title": "Text-to-3D Generation using Jensen-Shannon Score Distillation",
      "title_zh": "基于Jensen-Shannon散度评分的文本到3D生成",
      "authors": [
        "Khoi Do",
        "Binh-Son Hua"
      ],
      "abstract": "Score distillation sampling is an effective technique to generate 3D models\nfrom text prompts, utilizing pre-trained large-scale text-to-image diffusion\nmodels as guidance. However, the produced 3D assets tend to be over-saturating,\nover-smoothing, with limited diversity. These issues are results from a reverse\nKullback-Leibler (KL) divergence objective, which makes the optimization\nunstable and results in mode-seeking behavior. In this paper, we derive a\nbounded score distillation objective based on Jensen-Shannon divergence (JSD),\nwhich stabilizes the optimization process and produces high-quality 3D\ngeneration. JSD can match well generated and target distribution, therefore\nmitigating mode seeking. We provide a practical implementation of JSD by\nutilizing the theory of generative adversarial networks to define an\napproximate objective function for the generator, assuming the discriminator is\nwell trained. By assuming the discriminator following a log-odds classifier, we\npropose a minority sampling algorithm to estimate the gradients of our proposed\nobjective, providing a practical implementation for JSD. We conduct both\ntheoretical and empirical studies to validate our method. Experimental results\non T3Bench demonstrate that our method can produce high-quality and diversified\n3D assets.",
      "tldr_zh": "该研究提出了一种基于Jensen-Shannon散度(JSD)的文本到3D生成方法，解决了现有Score Distillation Sampling技术中存在的过饱和、过平滑和多样性不足的问题。通过引入JSD目标函数，优化过程更加稳定，避免了模式塌陷。研究还提出了一种基于生成对抗网络(GAN)的实用实现方法，并通过理论分析和实验验证了其有效性。在T3Bench数据集上的实验表明，该方法能够生成高质量且多样化的3D模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10660v2",
      "published_date": "2025-03-08 13:27:18 UTC",
      "updated_date": "2025-03-18 17:15:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:14:22.813211"
    },
    {
      "arxiv_id": "2503.06208v1",
      "title": "Distributed Graph Neural Network Inference With Just-In-Time Compilation For Industry-Scale Graphs",
      "title_zh": "分布式图神经网络推理：面向工业级图数据的即时编译技术",
      "authors": [
        "Xiabao Wu",
        "Yongchao Liu",
        "Wei Qin",
        "Chuntao Hong"
      ],
      "abstract": "Graph neural networks (GNNs) have delivered remarkable results in various\nfields. However, the rapid increase in the scale of graph data has introduced\nsignificant performance bottlenecks for GNN inference. Both computational\ncomplexity and memory usage have risen dramatically, with memory becoming a\ncritical limitation. Although graph sampling-based subgraph learning methods\ncan help mitigate computational and memory demands, they come with drawbacks\nsuch as information loss and high redundant computation among subgraphs. This\npaper introduces an innovative processing paradgim for distributed graph\nlearning that abstracts GNNs with a new set of programming interfaces and\nleverages Just-In-Time (JIT) compilation technology to its full potential. This\nparadigm enables GNNs to highly exploit the computational resources of\ndistributed clusters by eliminating the drawbacks of subgraph learning methods,\nleading to a more efficient inference process. Our experimental results\ndemonstrate that on industry-scale graphs of up to \\textbf{500 million nodes\nand 22.4 billion edges}, our method can produce a performance boost of up to\n\\textbf{27.4 times}.",
      "tldr_zh": "本研究提出了一种创新的分布式图神经网络(GNN)推理框架，通过引入新的编程接口并充分利用即时编译(JIT)技术，解决了大规模图数据带来的计算复杂性和内存瓶颈问题。该框架摒弃了传统的基于子图采样的方法，避免了信息丢失和冗余计算，显著提升了分布式集群的计算资源利用率。实验表明，在处理高达5亿节点和224亿边的工业级图数据时，该方法可实现高达27.4倍的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by EuroSys 2025 (poster)",
      "pdf_url": "http://arxiv.org/pdf/2503.06208v1",
      "published_date": "2025-03-08 13:26:59 UTC",
      "updated_date": "2025-03-08 13:26:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:13:33.409548"
    },
    {
      "arxiv_id": "2503.06204v1",
      "title": "CUPCase: Clinically Uncommon Patient Cases and Diagnoses Dataset",
      "title_zh": "CUPCase：临床罕见病例与诊断数据集",
      "authors": [
        "Oriel Perets",
        "Ofir Ben Shoham",
        "Nir Grinberg",
        "Nadav Rappoport"
      ],
      "abstract": "Medical benchmark datasets significantly contribute to developing Large\nLanguage Models (LLMs) for medical knowledge extraction, diagnosis,\nsummarization, and other uses. Yet, current benchmarks are mainly derived from\nexam questions given to medical students or cases described in the medical\nliterature, lacking the complexity of real-world patient cases that deviate\nfrom classic textbook abstractions. These include rare diseases, uncommon\npresentations of common diseases, and unexpected treatment responses. Here, we\nconstruct Clinically Uncommon Patient Cases and Diagnosis Dataset (CUPCase)\nbased on 3,562 real-world case reports from BMC, including diagnoses in\nopen-ended textual format and as multiple-choice options with distractors.\nUsing this dataset, we evaluate the ability of state-of-the-art LLMs, including\nboth general-purpose and Clinical LLMs, to identify and correctly diagnose a\npatient case, and test models' performance when only partial information about\ncases is available. Our findings show that general-purpose GPT-4o attains the\nbest performance in both the multiple-choice task (average accuracy of 87.9%)\nand the open-ended task (BERTScore F1 of 0.764), outperforming several LLMs\nwith a focus on the medical domain such as Meditron-70B and MedLM-Large.\nMoreover, GPT-4o was able to maintain 87% and 88% of its performance with only\nthe first 20% of tokens of the case presentation in multiple-choice and free\ntext, respectively, highlighting the potential of LLMs to aid in early\ndiagnosis in real-world cases. CUPCase expands our ability to evaluate LLMs for\nclinical decision support in an open and reproducible manner.",
      "tldr_zh": "该研究构建了CUPCase数据集，包含3,562份真实世界临床病例报告，涵盖罕见疾病、常见疾病的不典型表现和意外治疗反应等复杂情况，弥补了现有医学基准数据集缺乏真实病例复杂性的不足。研究使用该数据集评估了通用和临床领域大语言模型(LLMs)的诊断能力，发现GPT-4o在多项选择题(平均准确率87.9%)和开放式任务(BERTScore F1 0.764)中表现最佳，优于Meditron-70B等医学领域LLMs。此外，GPT-4o在仅使用病例前20%信息时仍能保持87%-88%的性能，展现了LLMs在早期诊断中的潜力。CUPCase为评估LLMs在临床决策支持中的应用提供了开放、可重复的基准。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06204v1",
      "published_date": "2025-03-08 13:21:44 UTC",
      "updated_date": "2025-03-08 13:21:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:13:52.485089"
    },
    {
      "arxiv_id": "2503.06202v1",
      "title": "Breaking Free from MMI: A New Frontier in Rationalization by Probing Input Utilization",
      "title_zh": "摆脱MMI：通过探究输入利用实现合理化新突破",
      "authors": [
        "Wei Liu",
        "Zhiying Deng",
        "Zhongyu Niu",
        "Jun Wang",
        "Haozhao Wang",
        "Zhigang Zeng",
        "Ruixuan Li"
      ],
      "abstract": "Extracting a small subset of crucial rationales from the full input is a key\nproblem in explainability research. The most widely used fundamental criterion\nfor rationale extraction is the maximum mutual information (MMI) criterion. In\nthis paper, we first demonstrate that MMI suffers from diminishing marginal\nreturns. Once part of the rationale has been identified, finding the remaining\nportions contributes only marginally to increasing the mutual information,\nmaking it difficult to use MMI to locate the rest. In contrast to MMI that aims\nto reproduce the prediction, we seek to identify the parts of the input that\nthe network can actually utilize.\n  This is achieved by comparing how different rationale candidates match the\ncapability space of the weight matrix. The weight matrix of a neural network is\ntypically low-rank, meaning that the linear combinations of its column vectors\ncan only cover part of the directions in a high-dimensional space\n(high-dimension: the dimensions of an input vector). If an input is fully\nutilized by the network, {it generally matches these directions (e.g., a\nportion of a hypersphere), resulting in a representation with a high norm.\nConversely, if an input primarily falls outside (orthogonal to) these\ndirections}, its representation norm will approach zero, behaving like noise\nthat the network cannot effectively utilize. Building on this, we propose using\nthe norms of rationale candidates as an alternative objective to MMI. Through\nexperiments on four text classification datasets and one graph classification\ndataset using three network architectures (GRUs, BERT, and GCN), we show that\nour method outperforms MMI and its improved variants in identifying better\nrationales. We also compare our method with a representative LLM\n(llama-3.1-8b-instruct) and find that our simple method gets comparable results\nto it and can sometimes even outperform it.",
      "tldr_zh": "该研究挑战了可解释性研究中广泛使用的最大互信息(MMI)准则，揭示了其在识别完整输入依据时存在边际效用递减问题。作者提出通过检测神经网络权重矩阵的低秩特性，分析输入向量与网络能力空间的匹配程度，以输入表示向量的范数作为新的优化目标。在文本分类和图分类任务上的实验表明，该方法优于MMI及其改进版本，甚至在某些情况下能媲美或超越大型语言模型(llama-3.1-8b-instruct)的表现，为神经网络可解释性研究提供了新思路。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06202v1",
      "published_date": "2025-03-08 13:08:46 UTC",
      "updated_date": "2025-03-08 13:08:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:13:53.399718"
    },
    {
      "arxiv_id": "2503.06201v1",
      "title": "Explainable Synthetic Image Detection through Diffusion Timestep Ensembling",
      "title_zh": "可解释的合成图像检测：基于扩散时间步集成方法",
      "authors": [
        "Yixin Wu",
        "Feiran Zhang",
        "Tianyuan Shi",
        "Ruicheng Yin",
        "Zhenghua Wang",
        "Zhenliang Gan",
        "Xiaohua Wang",
        "Changze Lv",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "abstract": "Recent advances in diffusion models have enabled the creation of deceptively\nreal images, posing significant security risks when misused. In this study, we\nreveal that natural and synthetic images exhibit distinct differences in the\nhigh-frequency domains of their Fourier power spectra after undergoing\niterative noise perturbations through an inverse multi-step denoising process,\nsuggesting that such noise can provide additional discriminative information\nfor identifying synthetic images. Based on this observation, we propose a novel\ndetection method that amplifies these differences by progressively adding noise\nto the original images across multiple timesteps, and train an ensemble of\nclassifiers on these noised images. To enhance human comprehension, we\nintroduce an explanation generation and refinement module to identify flaws\nlocated in AI-generated images. Additionally, we construct two new datasets,\nGenHard and GenExplain, derived from the GenImage benchmark, providing\ndetection samples of greater difficulty and high-quality rationales for fake\nimages. Extensive experiments show that our method achieves state-of-the-art\nperformance with 98.91% and 95.89% detection accuracy on regular and harder\nsamples, increasing a minimal of 2.51% and 3.46% compared to baselines.\nFurthermore, our method also generalizes effectively to images generated by\nother diffusion models. Our code and datasets will be made publicly available.",
      "tldr_zh": "本研究提出了一种基于扩散模型时间步集成（Diffusion Timestep Ensembling）的可解释合成图像检测方法。该方法通过多步逆去噪过程揭示自然图像与合成图像在傅里叶功率谱高频域中的差异，并利用逐步添加噪声的方式放大这些差异，训练集成分类器进行检测。此外，研究引入了解释生成与优化模块，用于识别AI生成图像中的缺陷，并构建了GenHard和GenExplain两个新数据集。实验表明，该方法在常规和困难样本上的检测准确率分别达到98.91%和95.89%，优于现有基线模型，并具有良好的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06201v1",
      "published_date": "2025-03-08 13:04:20 UTC",
      "updated_date": "2025-03-08 13:04:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:13:55.519762"
    },
    {
      "arxiv_id": "2503.06195v1",
      "title": "Human-AI Experience in Integrated Development Environments: A Systematic Literature Review",
      "title_zh": "集成开发环境中的人机交互体验：一项系统性文献综述",
      "authors": [
        "Agnia Sergeyuk",
        "Ilya Zakharov",
        "Ekaterina Koshchenko",
        "Maliheh Izadi"
      ],
      "abstract": "The integration of Artificial Intelligence (AI) into Integrated Development\nEnvironments (IDEs) is reshaping software development, fundamentally altering\nhow developers interact with their tools. This shift marks the emergence of\nHuman-AI Experience in Integrated Development Environment (in-IDE HAX), a field\nthat explores the evolving dynamics of Human-Computer Interaction in\nAI-assisted coding environments. Despite rapid adoption, research on in-IDE HAX\nremains fragmented which highlights the need for a unified overview of current\npractices, challenges, and opportunities. To provide a structured overview of\nexisting research, we conduct a systematic literature review of 89 studies,\nsummarizing current findings and outlining areas for further investigation.\n  Our findings reveal that AI-assisted coding enhances developer productivity\nbut also introduces challenges, such as verification overhead, automation bias,\nand over-reliance, particularly among novice developers. Furthermore, concerns\nabout code correctness, security, and maintainability highlight the urgent need\nfor explainability, verification mechanisms, and adaptive user control.\nAlthough recent advances have driven the field forward, significant research\ngaps remain, including a lack of longitudinal studies, personalization\nstrategies, and AI governance frameworks. This review provides a foundation for\nadvancing in-IDE HAX research and offers guidance for responsibly integrating\nAI into software development.",
      "tldr_zh": "这篇论文通过系统文献综述分析了AI集成开发环境(IDE)中的人机交互体验(in-IDE HAX)。研究者梳理了89项相关研究，发现AI辅助编码虽提升开发效率，但也带来验证负担、自动化偏见和过度依赖等新挑战，尤其影响新手开发者。研究揭示了当前领域存在三大关键问题：缺乏代码正确性与安全性的解释机制、缺少长期效果研究，以及亟待建立AI治理框架。该综述为未来负责任地整合AI到软件开发提供了理论基础和实践指导。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "Submitted to Empirical Software Engineering (EMSE) special issue\n  Human-Centered AI for Software Engineering (HumanAISE), 28 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2503.06195v1",
      "published_date": "2025-03-08 12:40:18 UTC",
      "updated_date": "2025-03-08 12:40:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:14:21.536103"
    },
    {
      "arxiv_id": "2503.06187v1",
      "title": "MSConv: Multiplicative and Subtractive Convolution for Face Recognition",
      "title_zh": "MSConv：面向人脸识别的乘减卷积网络",
      "authors": [
        "Si Zhou",
        "Yain-Whar Si",
        "Xiaochen Yuan",
        "Xiaofan Li",
        "Xiaoxiang Liu",
        "Xinyuan Zhang",
        "Cong Lin",
        "Xueyuan Gong"
      ],
      "abstract": "In Neural Networks, there are various methods of feature fusion. Different\nstrategies can significantly affect the effectiveness of feature\nrepresentation, consequently influencing the ability of model to extract\nrepresentative and discriminative features. In the field of face recognition,\ntraditional feature fusion methods include feature concatenation and feature\naddition. Recently, various attention mechanism-based fusion strategies have\nemerged. However, we found that these methods primarily focus on the important\nfeatures in the image, referred to as salient features in this paper, while\nneglecting another equally important set of features for image recognition\ntasks, which we term differential features. This may cause the model to\noverlook critical local differences when dealing with complex facial samples.\nTherefore, in this paper, we propose an efficient convolution module called\nMSConv (Multiplicative and Subtractive Convolution), designed to balance the\nlearning of model about salient and differential features. Specifically, we\nemploy multi-scale mixed convolution to capture both local and broader\ncontextual information from face images, and then utilize Multiplication\nOperation (MO) and Subtraction Operation (SO) to extract salient and\ndifferential features, respectively. Experimental results demonstrate that by\nintegrating both salient and differential features, MSConv outperforms models\nthat only focus on salient features.",
      "tldr_zh": "本文提出了一种名为MSConv（乘法和减法卷积）的高效卷积模块，旨在平衡模型对显著性特征和差异性特征的学习。通过多尺度混合卷积捕捉人脸图像的局部和全局信息，并分别利用乘法操作(MO)和减法操作(SO)提取显著性特征和差异性特征。实验结果表明，结合这两类特征的MSConv在面部识别任务中优于仅关注显著性特征的模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06187v1",
      "published_date": "2025-03-08 12:18:29 UTC",
      "updated_date": "2025-03-08 12:18:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:14:07.171054"
    },
    {
      "arxiv_id": "2503.06184v1",
      "title": "Sample-aware Adaptive Structured Pruning for Large Language Models",
      "title_zh": "样本感知的自适应结构化剪枝用于大型语言模型",
      "authors": [
        "Jun Kong",
        "Xinge Ma",
        "Jin Wang",
        "Xuejie Zhang"
      ],
      "abstract": "Large language models (LLMs) have achieved outstanding performance in natural\nlanguage processing, but enormous model sizes and high computational costs\nlimit their practical deployment. Structured pruning can effectively reduce the\nresource demands for deployment by removing redundant model parameters.\nHowever, the randomly selected calibration data and fixed single importance\nestimation metrics in existing structured pruning methods lead to degraded\nperformance of pruned models. This study introduces AdaPruner, a sample-aware\nadaptive structured pruning framework for LLMs, aiming to optimize the\ncalibration data and importance estimation metrics in the structured pruning\nprocess. Specifically, AdaPruner effectively removes redundant parameters from\nLLMs by constructing a structured pruning solution space and then employing\nBayesian optimization to adaptively search for the optimal calibration data and\nimportance estimation metrics. Experimental results show that the AdaPruner\noutperforms existing structured pruning methods on a family of LLMs with\nvarying pruning ratios, demonstrating its applicability and robustness.\nRemarkably, at a 20\\% pruning ratio, the model pruned with AdaPruner maintains\n97\\% of the performance of the unpruned model.",
      "tldr_zh": "该研究提出了一种名为AdaPruner的自适应结构化剪枝框架，用于优化大型语言模型(LLMs)的压缩部署。该方法通过构建结构化剪枝解空间，并采用贝叶斯优化自适应搜索最佳校准数据和重要性评估指标，有效解决了现有剪枝方法中随机选择校准数据和固定评估标准导致的性能下降问题。实验表明，AdaPruner在不同剪枝比例下均优于现有方法，在20%剪枝比例时能保持未剪枝模型97%的性能，显著提升了LLMs的实用部署效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06184v1",
      "published_date": "2025-03-08 12:00:21 UTC",
      "updated_date": "2025-03-08 12:00:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:14:40.496147"
    },
    {
      "arxiv_id": "2503.06183v2",
      "title": "Lightweight Software Kernels and Hardware Extensions for Efficient Sparse Deep Neural Networks on Microcontrollers",
      "title_zh": "面向微控制器的轻量级软件内核与硬件扩展：高效稀疏深度神经网络实现",
      "authors": [
        "Francesco Daghero",
        "Daniele Jahier Pagliari",
        "Francesco Conti",
        "Luca Benini",
        "Massimo Poncino",
        "Alessio Burrello"
      ],
      "abstract": "The acceleration of pruned Deep Neural Networks (DNNs) on edge devices such\nas Microcontrollers (MCUs) is a challenging task, given the tight area- and\npower-constraints of these devices. In this work, we propose a three-fold\ncontribution to address this problem. First, we design a set of optimized\nsoftware kernels for N:M pruned layers, targeting ultra-low-power, multicore\nRISC-V MCUs, which are up to 2.1x and 3.4x faster than their dense counterparts\nat 1:8 and 1:16 sparsity, respectively. Then, we implement a lightweight\nInstruction-Set Architecture (ISA) extension to accelerate the indirect load\nand non-zero indices decompression operations required by our kernels,\nobtaining up to 1.9x extra speedup, at the cost of a 5% area overhead. Lastly,\nwe extend an open-source DNN compiler to utilize our sparse kernels for\ncomplete networks, showing speedups of 3.21x and 1.81x on a ResNet18 and a\nVision Transformer (ViT), with less than 1.5% accuracy drop compared to a dense\nbaseline.",
      "tldr_zh": "该研究提出了一种针对微控制器(MCUs)的高效稀疏深度神经网络(DNN)加速方案。首先，设计了一组针对N:M稀疏层的优化软件内核，在1:8和1:16稀疏度下分别比密集层快2.1倍和3.4倍。其次，实现了一种轻量级指令集架构(ISA)扩展，进一步加速稀疏操作，获得了1.9倍的额外加速，面积开销仅为5%。最后，扩展了一个开源DNN编译器以支持稀疏内核，在ResNet18和Vision Transformer (ViT)上分别实现了3.21倍和1.81倍的加速，精度损失小于1.5%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at MLSys 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.06183v2",
      "published_date": "2025-03-08 11:59:12 UTC",
      "updated_date": "2025-03-19 10:10:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:14:56.319069"
    },
    {
      "arxiv_id": "2503.06175v1",
      "title": "Minion Gated Recurrent Unit for Continual Learning",
      "title_zh": "用于持续学习的Minion门控循环单元",
      "authors": [
        "Abdullah M. Zyarah",
        "Dhireesha Kudithipudi"
      ],
      "abstract": "The increasing demand for continual learning in sequential data processing\nhas led to progressively complex training methodologies and larger recurrent\nnetwork architectures. Consequently, this has widened the knowledge gap between\ncontinual learning with recurrent neural networks (RNNs) and their ability to\noperate on devices with limited memory and compute. To address this challenge,\nwe investigate the effectiveness of simplifying RNN architectures, particularly\ngated recurrent unit (GRU), and its impact on both single-task and multitask\nsequential learning. We propose a new variant of GRU, namely the minion\nrecurrent unit (MiRU). MiRU replaces conventional gating mechanisms with\nscaling coefficients to regulate dynamic updates of hidden states and\nhistorical context, reducing computational costs and memory requirements.\nDespite its simplified architecture, MiRU maintains performance comparable to\nthe standard GRU while achieving 2.90x faster training and reducing parameter\nusage by 2.88x, as demonstrated through evaluations on sequential image\nclassification and natural language processing benchmarks. The impact of model\nsimplification on its learning capacity is also investigated by performing\ncontinual learning tasks with a rehearsal-based strategy and global inhibition.\nWe find that MiRU demonstrates stable performance in multitask learning even\nwhen using only rehearsal, unlike the standard GRU and its variants. These\nfeatures position MiRU as a promising candidate for edge-device applications.",
      "tldr_zh": "该研究提出了一种新型循环神经网络单元Minion Recurrent Unit（MiRU），通过用缩放系数替代传统门控机制来简化GRU结构。实验表明，MiRU在保持与标准GRU相当性能的同时，训练速度提升2.9倍、参数使用减少2.88倍，特别适合内存和计算资源有限的边缘设备。在持续学习任务中，MiRU仅通过rehearsal策略就展现出优于标准GRU的多任务学习稳定性，为序列图像分类和自然语言处理提供了高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06175v1",
      "published_date": "2025-03-08 11:28:40 UTC",
      "updated_date": "2025-03-08 11:28:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:14:54.134288"
    },
    {
      "arxiv_id": "2503.06171v1",
      "title": "ROCM: RLHF on consistency models",
      "title_zh": "ROCM：基于一致性模型的强化学习人类反馈优化",
      "authors": [
        "Shivanshu Shekhar",
        "Tong Zhang"
      ],
      "abstract": "Diffusion models have revolutionized generative modeling in continuous\ndomains like image, audio, and video synthesis. However, their iterative\nsampling process leads to slow generation and inefficient training, challenges\nthat are further exacerbated when incorporating Reinforcement Learning from\nHuman Feedback (RLHF) due to sparse rewards and long time horizons. Consistency\nmodels address these issues by enabling single-step or efficient multi-step\ngeneration, significantly reducing computational costs.\n  In this work, we propose a direct reward optimization framework for applying\nRLHF to consistency models, incorporating distributional regularization to\nenhance training stability and prevent reward hacking. We investigate various\n$f$-divergences as regularization strategies, striking a balance between reward\nmaximization and model consistency. Unlike policy gradient methods, our\napproach leverages first-order gradients, making it more efficient and less\nsensitive to hyperparameter tuning. Empirical results show that our method\nachieves competitive or superior performance compared to policy gradient based\nRLHF methods, across various automatic metrics and human evaluation.\nAdditionally, our analysis demonstrates the impact of different regularization\ntechniques in improving model generalization and preventing overfitting.",
      "tldr_zh": "该研究提出了一种基于一致性模型(Consistency Models)的强化学习人类反馈(RLHF)优化框架ROCM，解决了扩散模型(Diffusion Models)在生成过程中计算效率低、训练成本高的问题。通过引入分布正则化技术，该框架有效防止了奖励作弊，并在奖励最大化和模型一致性之间取得了平衡。实验表明，与基于策略梯度的RLHF方法相比，ROCM在自动指标和人类评估中表现更优，同时提高了模型的泛化能力和训练稳定性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06171v1",
      "published_date": "2025-03-08 11:19:48 UTC",
      "updated_date": "2025-03-08 11:19:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:15:00.718888"
    },
    {
      "arxiv_id": "2503.06170v2",
      "title": "Object-Centric World Model for Language-Guided Manipulation",
      "title_zh": "面向语言引导操控的以物体为中心的世界模型",
      "authors": [
        "Youngjoon Jeong",
        "Junha Chun",
        "Soonwoo Cha",
        "Taesup Kim"
      ],
      "abstract": "A world model is essential for an agent to predict the future and plan in\ndomains such as autonomous driving and robotics. To achieve this, recent\nadvancements have focused on video generation, which has gained significant\nattention due to the impressive success of diffusion models. However, these\nmodels require substantial computational resources. To address these\nchallenges, we propose a world model leveraging object-centric representation\nspace using slot attention, guided by language instructions. Our model\nperceives the current state as an object-centric representation and predicts\nfuture states in this representation space conditioned on natural language\ninstructions. This approach results in a more compact and computationally\nefficient model compared to diffusion-based generative alternatives.\nFurthermore, it flexibly predicts future states based on language instructions,\nand offers a significant advantage in manipulation tasks where object\nrecognition is crucial. In this paper, we demonstrate that our latent\npredictive world model surpasses generative world models in visuo-linguo-motor\ncontrol tasks, achieving superior sample and computation efficiency. We also\ninvestigate the generalization performance of the proposed method and explore\nvarious strategies for predicting actions using object-centric representations.",
      "tldr_zh": "本文提出了一种基于对象中心表示的世界模型，用于语言引导的操控任务。该模型利用slot attention技术将当前状态表示为对象中心的形式，并在自然语言指令的引导下预测未来状态。相比基于扩散模型的生成方法，该模型更加紧凑且计算效率更高，能够灵活地根据语言指令预测未来状态，在需要对象识别的操控任务中具有显著优势。实验表明，该模型在视觉-语言-运动控制任务中优于生成式世界模型，具有更高的样本和计算效率。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06170v2",
      "published_date": "2025-03-08 11:17:37 UTC",
      "updated_date": "2025-03-12 13:52:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:15:09.006430"
    },
    {
      "arxiv_id": "2503.06169v2",
      "title": "Treble Counterfactual VLMs: A Causal Approach to Hallucination",
      "title_zh": "Treble 反事实 VLM：一种因果方法应对幻觉问题",
      "authors": [
        "Shawn Li",
        "Jiashu Qu",
        "Yuxiao Zhou",
        "Yuehan Qin",
        "Tiankai Yang",
        "Yue Zhao"
      ],
      "abstract": "Vision-Language Models (VLMs) have advanced multi-modal tasks like image\ncaptioning, visual question answering, and reasoning. However, they often\ngenerate hallucinated outputs inconsistent with the visual context or prompt,\nlimiting reliability in critical applications like autonomous driving and\nmedical imaging. Existing studies link hallucination to statistical biases,\nlanguage priors, and biased feature learning but lack a structured causal\nunderstanding. In this work, we introduce a causal perspective to analyze and\nmitigate hallucination in VLMs. We hypothesize that hallucination arises from\nunintended direct influences of either the vision or text modality, bypassing\nproper multi-modal fusion. To address this, we construct a causal graph for\nVLMs and employ counterfactual analysis to estimate the Natural Direct Effect\n(NDE) of vision, text, and their cross-modal interaction on the output. We\nsystematically identify and mitigate these unintended direct effects to ensure\nthat responses are primarily driven by genuine multi-modal fusion. Our approach\nconsists of three steps: (1) designing structural causal graphs to distinguish\ncorrect fusion pathways from spurious modality shortcuts, (2) estimating\nmodality-specific and cross-modal NDE using perturbed image representations,\nhallucinated text embeddings, and degraded visual inputs, and (3) implementing\na test-time intervention module to dynamically adjust the model's dependence on\neach modality. Experimental results demonstrate that our method significantly\nreduces hallucination while preserving task performance, providing a robust and\ninterpretable framework for improving VLM reliability. To enhance accessibility\nand reproducibility, our code is publicly available at\nhttps://github.com/TREE985/Treble-Counterfactual-VLMs.",
      "tldr_zh": "该研究提出了Treble Counterfactual VLMs框架，通过因果推理方法解决视觉语言模型(VLMs)中的幻觉问题。研究者构建了结构化因果图区分真实多模态融合与虚假模态捷径，并采用反事实分析计算视觉、文本及其跨模态交互对输出的自然直接效应(NDE)。该方法通过三步干预策略（因果图构建、NDE估计和运行时模态依赖动态调整），在保持任务性能的同时显著减少幻觉现象。实验证明该框架为提升VLM可靠性提供了可解释的解决方案，相关代码已开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06169v2",
      "published_date": "2025-03-08 11:13:05 UTC",
      "updated_date": "2025-03-17 08:11:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:15:55.347066"
    },
    {
      "arxiv_id": "2503.06166v2",
      "title": "Secure On-Device Video OOD Detection Without Backpropagation",
      "title_zh": "无需反向传播的安全端侧视频异常分布检测",
      "authors": [
        "Shawn Li",
        "Peilin Cai",
        "Yuxiao Zhou",
        "Zhiyu Ni",
        "Renjie Liang",
        "You Qin",
        "Yi Nian",
        "Zhengzhong Tu",
        "Xiyang Hu",
        "Yue Zhao"
      ],
      "abstract": "Out-of-Distribution (OOD) detection is critical for ensuring the reliability\nof machine learning models in safety-critical applications such as autonomous\ndriving and medical diagnosis. While deploying personalized OOD detection\ndirectly on edge devices is desirable, it remains challenging due to large\nmodel sizes and the computational infeasibility of on-device training.\nFederated learning partially addresses this but still requires gradient\ncomputation and backpropagation, exceeding the capabilities of many edge\ndevices. To overcome these challenges, we propose SecDOOD, a secure\ncloud-device collaboration framework for efficient on-device OOD detection\nwithout requiring device-side backpropagation. SecDOOD utilizes cloud resources\nfor model training while ensuring user data privacy by retaining sensitive\ninformation on-device. Central to SecDOOD is a HyperNetwork-based personalized\nparameter generation module, which adapts cloud-trained models to\ndevice-specific distributions by dynamically generating local weight\nadjustments, effectively combining central and local information without local\nfine-tuning. Additionally, our dynamic feature sampling and encryption strategy\nselectively encrypts only the most informative feature channels, largely\nreducing encryption overhead without compromising detection performance.\nExtensive experiments across multiple datasets and OOD scenarios demonstrate\nthat SecDOOD achieves performance comparable to fully fine-tuned models,\nenabling secure, efficient, and personalized OOD detection on resource-limited\nedge devices. To enhance accessibility and reproducibility, our code is\npublicly available at https://github.com/Dystopians/SecDOOD.",
      "tldr_zh": "该研究提出SecDOOD框架，实现无需反向传播的端侧安全视频异常检测(OOD)。通过云端训练与设备端HyperNetwork参数生成模块的协同，在保护用户隐私的同时实现个性化模型适配。创新的动态特征采样加密策略仅选择关键特征通道加密，显著降低计算开销。实验表明该框架在资源受限设备上能达到与全微调模型相当的检测性能。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06166v2",
      "published_date": "2025-03-08 11:03:21 UTC",
      "updated_date": "2025-03-17 07:44:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:15:33.977628"
    },
    {
      "arxiv_id": "2503.06163v1",
      "title": "VACT: A Video Automatic Causal Testing System and a Benchmark",
      "title_zh": "VACT：视频自动因果测试系统及基准",
      "authors": [
        "Haotong Yang",
        "Qingyuan Zheng",
        "Yunjian Gao",
        "Yongkun Yang",
        "Yangbo He",
        "Zhouchen Lin",
        "Muhan Zhang"
      ],
      "abstract": "With the rapid advancement of text-conditioned Video Generation Models\n(VGMs), the quality of generated videos has significantly improved, bringing\nthese models closer to functioning as ``*world simulators*'' and making\nreal-world-level video generation more accessible and cost-effective. However,\nthe generated videos often contain factual inaccuracies and lack understanding\nof fundamental physical laws. While some previous studies have highlighted this\nissue in limited domains through manual analysis, a comprehensive solution has\nnot yet been established, primarily due to the absence of a generalized,\nautomated approach for modeling and assessing the causal reasoning of these\nmodels across diverse scenarios. To address this gap, we propose VACT: an\n**automated** framework for modeling, evaluating, and measuring the causal\nunderstanding of VGMs in real-world scenarios. By combining causal analysis\ntechniques with a carefully designed large language model assistant, our system\ncan assess the causal behavior of models in various contexts without human\nannotation, which offers strong generalization and scalability. Additionally,\nwe introduce multi-level causal evaluation metrics to provide a detailed\nanalysis of the causal performance of VGMs. As a demonstration, we use our\nframework to benchmark several prevailing VGMs, offering insight into their\ncausal reasoning capabilities. Our work lays the foundation for systematically\naddressing the causal understanding deficiencies in VGMs and contributes to\nadvancing their reliability and real-world applicability.",
      "tldr_zh": "该研究提出VACT系统，首个自动化评估视频生成模型(VGMs)因果推理能力的框架。通过结合因果分析技术与大语言模型辅助，系统能无人工标注地评估模型在多样化场景中的因果行为，并设计了多级因果评估指标。实验对主流VGMs进行基准测试，揭示了其因果理解缺陷，为提升视频生成模型的可靠性和现实适用性奠定基础。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06163v1",
      "published_date": "2025-03-08 10:54:42 UTC",
      "updated_date": "2025-03-08 10:54:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:16:03.764697"
    },
    {
      "arxiv_id": "2503.06161v1",
      "title": "Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical Deformable Scene Reconstruction",
      "title_zh": "Feature-EndoGaussian：用于手术可变形场景重建的特征蒸馏高斯溅射",
      "authors": [
        "Kai Li",
        "Junhao Wang",
        "William Han",
        "Ding Zhao"
      ],
      "abstract": "Minimally invasive surgery (MIS) has transformed clinical practice by\nreducing recovery times, minimizing complications, and enhancing precision.\nNonetheless, MIS inherently relies on indirect visualization and precise\ninstrument control, posing unique challenges. Recent advances in artificial\nintelligence have enabled real-time surgical scene understanding through\ntechniques such as image classification, object detection, and segmentation,\nwith scene reconstruction emerging as a key element for enhanced intraoperative\nguidance. Although neural radiance fields (NeRFs) have been explored for this\npurpose, their substantial data requirements and slow rendering inhibit\nreal-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more\nefficient alternative, achieving state-of-the-art performance in dynamic\nsurgical scene reconstruction. In this work, we introduce Feature-EndoGaussian\n(FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D\nrendering to enable real-time semantic and scene reconstruction. By leveraging\npretrained segmentation foundation models, FEG incorporates semantic feature\ndistillation within the Gaussian deformation framework, thereby enhancing both\nreconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG\nachieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03)\ncompared to leading methods. Additionally, on the EndoVis18 dataset, FEG\ndemonstrates competitive class-wise segmentation metrics while balancing model\nsize and real-time performance.",
      "tldr_zh": "该研究提出了Feature-EndoGaussian (FEG)，一种基于3D Gaussian Splatting (3DGS)的手术场景重建方法，通过将2D语义分割线索融入3D渲染，实现实时语义和场景重建。FEG利用预训练的分割基础模型，在高斯变形框架中进行语义特征蒸馏，显著提高了重建保真度和分割精度。在EndoNeRF数据集上，FEG在SSIM、PSNR和LPIPS指标上均优于现有方法，同时在EndoVis18数据集上展示了竞争性的类别分割性能，同时兼顾模型大小和实时性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06161v1",
      "published_date": "2025-03-08 10:50:19 UTC",
      "updated_date": "2025-03-08 10:50:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:15:58.733991"
    },
    {
      "arxiv_id": "2503.06157v1",
      "title": "UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces",
      "title_zh": "UrbanVideo-Bench：基于城市空间视频数据评测具身智能的视觉语言模型",
      "authors": [
        "Baining Zhao",
        "Jianjie Fang",
        "Zichao Dai",
        "Ziyou Wang",
        "Jirong Zha",
        "Weichen Zhang",
        "Chen Gao",
        "Yue Wang",
        "Jinqiang Cui",
        "Xinlei Chen",
        "Yong Li"
      ],
      "abstract": "Large multimodal models exhibit remarkable intelligence, yet their embodied\ncognitive abilities during motion in open-ended urban 3D space remain to be\nexplored. We introduce a benchmark to evaluate whether video-large language\nmodels (Video-LLMs) can naturally process continuous first-person visual\nobservations like humans, enabling recall, perception, reasoning, and\nnavigation. We have manually control drones to collect 3D embodied motion video\ndata from real-world cities and simulated environments, resulting in 1.5k video\nclips. Then we design a pipeline to generate 5.2k multiple-choice questions.\nEvaluations of 17 widely-used Video-LLMs reveal current limitations in urban\nembodied cognition. Correlation analysis provides insight into the\nrelationships between different tasks, showing that causal reasoning has a\nstrong correlation with recall, perception, and navigation, while the abilities\nfor counterfactual and associative reasoning exhibit lower correlation with\nother tasks. We also validate the potential for Sim-to-Real transfer in urban\nembodiment through fine-tuning.",
      "tldr_zh": "该研究提出了UrbanVideo-Bench，一个用于评估视频大语言模型(Video-LLMs)在城市空间中的具身认知能力的基准。通过无人机采集真实城市和模拟环境中的第一人称3D运动视频数据，并生成5.2k道多选题，评估了17种主流Video-LLMs的表现。研究发现，因果推理与回忆、感知和导航任务高度相关，而反事实和联想推理能力与其他任务相关性较低。实验还验证了Sim-to-Real迁移在城市具身认知中的潜力，为未来研究提供了重要参考。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.06157v1",
      "published_date": "2025-03-08 10:47:05 UTC",
      "updated_date": "2025-03-08 10:47:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:15:52.994940"
    },
    {
      "arxiv_id": "2503.05858v3",
      "title": "Bimodal Connection Attention Fusion for Speech Emotion Recognition",
      "title_zh": "双模态连接注意力融合在语音情感识别中的应用",
      "authors": [
        "Jiachen Luo",
        "Huy Phan",
        "Lin Wang",
        "Joshua D. Reiss"
      ],
      "abstract": "Multi-modal emotion recognition is challenging due to the difficulty of\nextracting features that capture subtle emotional differences. Understanding\nmulti-modal interactions and connections is key to building effective bimodal\nspeech emotion recognition systems. In this work, we propose Bimodal Connection\nAttention Fusion (BCAF) method, which includes three main modules: the\ninteractive connection network, the bimodal attention network, and the\ncorrelative attention network. The interactive connection network uses an\nencoder-decoder architecture to model modality connections between audio and\ntext while leveraging modality-specific features. The bimodal attention network\nenhances semantic complementation and exploits intra- and inter-modal\ninteractions. The correlative attention network reduces cross-modal noise and\ncaptures correlations between audio and text. Experiments on the MELD and\nIEMOCAP datasets demonstrate that the proposed BCAF method outperforms existing\nstate-of-the-art baselines.",
      "tldr_zh": "该研究提出了一种双模态连接注意力融合方法(BCAF)用于语音情感识别，通过三个核心模块解决多模态交互难题：交互连接网络采用编解码架构建模语音与文本的模态关联，双模态注意力网络增强语义互补与模态内外交互，相关注意力网络则抑制跨模态噪声并捕捉音文相关性。在MELD和IEMOCAP数据集上的实验表明，该方法显著超越了现有最优基线模型。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05858v3",
      "published_date": "2025-03-08 10:20:57 UTC",
      "updated_date": "2025-03-22 11:48:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:16:30.599268"
    },
    {
      "arxiv_id": "2503.06144v1",
      "title": "Exploring the usage of Probabilistic Neural Networks for Ionospheric electron density estimation",
      "title_zh": "探究概率神经网络在电离层电子密度估计中的应用",
      "authors": [
        "Miquel Garcia-Fernandez"
      ],
      "abstract": "A fundamental limitation of traditional Neural Networks (NN) in predictive\nmodelling is their inability to quantify uncertainty in their outputs. In\ncritical applications like positioning systems, understanding the reliability\nof predictions is critical for constructing confidence intervals, early warning\nsystems, and effectively propagating results. For instance, Precise Point\nPositioning in satellite navigation heavily relies on accurate error models for\nancillary data (orbits, clocks, ionosphere, and troposphere) to compute precise\nerror estimates. In addition, these uncertainty estimates are needed to\nestablish robust protection levels in safety critical applications.\n  To address this challenge, the main objectives of this paper aims at\nexploring a potential framework capable of providing both point estimates and\nassociated uncertainty measures of ionospheric Vertical Total Electron Content\n(VTEC). In this context, Probabilistic Neural Networks (PNNs) offer a promising\napproach to achieve this goal. However, constructing an effective PNN requires\nmeticulous design of hidden and output layers, as well as careful definition of\nprior and posterior probability distributions for network weights and biases.\n  A key finding of this study is that the uncertainty provided by the PNN model\nin VTEC estimates may be systematically underestimated. In low-latitude areas,\nthe actual error was observed to be as much as twice the model's estimate. This\nunderestimation is expected to be more pronounced during solar maximum,\ncorrelating with increased VTEC values.",
      "tldr_zh": "该研究探索了概率神经网络(PNN)在电离层电子密度估计中的应用，解决了传统神经网络无法量化预测不确定性的关键问题。研究发现，虽然PNN能同时提供垂直总电子含量(VTEC)的点估计和不确定性度量，但其在低纬度区域会系统性低估误差幅度（实际误差可达模型估计值的两倍），且在太阳活动高峰期这种低估现象会更为显著。这项工作为卫星导航等安全关键应用中的误差建模提供了重要见解。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "86A10, 62M45,",
        "I.2.6; G.3; J.2"
      ],
      "primary_category": "eess.SP",
      "comment": "13 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06144v1",
      "published_date": "2025-03-08 10:06:15 UTC",
      "updated_date": "2025-03-08 10:06:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:16:33.021010"
    },
    {
      "arxiv_id": "2503.06138v2",
      "title": "System 0/1/2/3: Quad-process theory for multi-timescale embodied collective cognitive systems",
      "title_zh": "系统0/1/2/3：多时间尺度具身集体认知系统的四重加工理论",
      "authors": [
        "Tadahiro Taniguchi",
        "Yasushi Hirai",
        "Masahiro Suzuki",
        "Shingo Murata",
        "Takato Horii",
        "Kazutoshi Tanaka"
      ],
      "abstract": "This paper introduces the System 0/1/2/3 framework as an extension of\ndual-process theory, employing a quad-process model of cognition. Expanding\nupon System 1 (fast, intuitive thinking) and System 2 (slow, deliberative\nthinking), we incorporate System 0, which represents pre-cognitive embodied\nprocesses, and System 3, which encompasses collective intelligence and symbol\nemergence. We contextualize this model within Bergson's philosophy by adopting\nmulti-scale time theory to unify the diverse temporal dynamics of cognition.\nSystem 0 emphasizes morphological computation and passive dynamics,\nillustrating how physical embodiment enables adaptive behavior without explicit\nneural processing. Systems 1 and 2 are explained from a constructive\nperspective, incorporating neurodynamical and AI viewpoints. In System 3, we\nintroduce collective predictive coding to explain how societal-level adaptation\nand symbol emergence operate over extended timescales. This comprehensive\nframework ranges from rapid embodied reactions to slow-evolving collective\nintelligence, offering a unified perspective on cognition across multiple\ntimescales, levels of abstraction, and forms of human intelligence. The System\n0/1/2/3 model provides a novel theoretical foundation for understanding the\ninterplay between adaptive and cognitive processes, thereby opening new avenues\nfor research in cognitive science, AI, robotics, and collective intelligence.",
      "tldr_zh": "该研究提出了System 0/1/2/3四进程认知理论框架，扩展了传统的双进程理论。新框架在System 1(快速直觉思维)和System 2(缓慢审慎思维)基础上，增加了代表前认知具身过程的System 0(强调形态计算和被动动力学)以及涵盖群体智能的System 3(采用集体预测编码解释社会层面适应)。研究采用多时间尺度理论统一不同认知过程的时间动态性，为理解从快速具身反应到缓慢演化的群体智能提供了统一视角。该理论框架为认知科学、人工智能和机器人学等领域提供了新的理论基础。",
      "categories": [
        "cs.AI",
        "cs.RO",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2503.06138v2",
      "published_date": "2025-03-08 09:31:53 UTC",
      "updated_date": "2025-03-13 23:45:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:16:40.629924"
    },
    {
      "arxiv_id": "2503.06136v1",
      "title": "GSV3D: Gaussian Splatting-based Geometric Distillation with Stable Video Diffusion for Single-Image 3D Object Generation",
      "title_zh": "GSV3D：基于高斯泼溅几何蒸馏与稳定视频扩散的单图像3D物体生成方法",
      "authors": [
        "Ye Tao",
        "Jiawei Zhang",
        "Yahao Shi",
        "Dongqing Zou",
        "Bin Zhou"
      ],
      "abstract": "Image-based 3D generation has vast applications in robotics and gaming, where\nhigh-quality, diverse outputs and consistent 3D representations are crucial.\nHowever, existing methods have limitations: 3D diffusion models are limited by\ndataset scarcity and the absence of strong pre-trained priors, while 2D\ndiffusion-based approaches struggle with geometric consistency. We propose a\nmethod that leverages 2D diffusion models' implicit 3D reasoning ability while\nensuring 3D consistency via Gaussian-splatting-based geometric distillation.\nSpecifically, the proposed Gaussian Splatting Decoder enforces 3D consistency\nby transforming SV3D latent outputs into an explicit 3D representation. Unlike\nSV3D, which only relies on implicit 2D representations for video generation,\nGaussian Splatting explicitly encodes spatial and appearance attributes,\nenabling multi-view consistency through geometric constraints. These\nconstraints correct view inconsistencies, ensuring robust geometric\nconsistency. As a result, our approach simultaneously generates high-quality,\nmulti-view-consistent images and accurate 3D models, providing a scalable\nsolution for single-image-based 3D generation and bridging the gap between 2D\nDiffusion diversity and 3D structural coherence. Experimental results\ndemonstrate state-of-the-art multi-view consistency and strong generalization\nacross diverse datasets. The code will be made publicly available upon\nacceptance.",
      "tldr_zh": "该研究提出了GSV3D，一种基于高斯溅射(Gaussian Splatting)的几何蒸馏方法，结合稳定视频扩散模型(Stable Video Diffusion)实现单图像3D生成。该方法利用2D扩散模型的隐式3D推理能力，并通过高斯溅射解码器将SV3D的隐式2D表示转化为显式3D表示，确保多视角一致性。实验表明，GSV3D在生成高质量、多视角一致的图像和精确3D模型方面表现优异，为单图像3D生成提供了可扩展的解决方案，并在多视角一致性和跨数据集泛化能力上达到领先水平。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06136v1",
      "published_date": "2025-03-08 09:10:31 UTC",
      "updated_date": "2025-03-08 09:10:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:16:43.392850"
    },
    {
      "arxiv_id": "2503.10659v1",
      "title": "MARRO: Multi-headed Attention for Rhetorical Role Labeling in Legal Documents",
      "title_zh": "MARRO：面向法律文档修辞角色标注的多头注意力机制",
      "authors": [
        "Purbid Bambroo",
        "Subinay Adhikary",
        "Paheli Bhattacharya",
        "Abhijnan Chakraborty",
        "Saptarshi Ghosh",
        "Kripabandhu Ghosh"
      ],
      "abstract": "Identification of rhetorical roles like facts, arguments, and final judgments\nis central to understanding a legal case document and can lend power to other\ndownstream tasks like legal case summarization and judgment prediction.\nHowever, there are several challenges to this task. Legal documents are often\nunstructured and contain a specialized vocabulary, making it hard for\nconventional transformer models to understand them. Additionally, these\ndocuments run into several pages, which makes it difficult for neural models to\ncapture the entire context at once. Lastly, there is a dearth of annotated\nlegal documents to train deep learning models. Previous state-of-the-art\napproaches for this task have focused on using neural models like BiLSTM-CRF or\nhave explored different embedding techniques to achieve decent results. While\nsuch techniques have shown that better embedding can result in improved model\nperformance, not many models have focused on utilizing attention for learning\nbetter embeddings in sentences of a document. Additionally, it has been\nrecently shown that advanced techniques like multi-task learning can help the\nmodels learn better representations, thereby improving performance. In this\npaper, we combine these two aspects by proposing a novel family of multi-task\nlearning-based models for rhetorical role labeling, named MARRO, that uses\ntransformer-inspired multi-headed attention. Using label shift as an auxiliary\ntask, we show that models from the MARRO family achieve state-of-the-art\nresults on two labeled datasets for rhetorical role labeling, from the Indian\nand UK Supreme Courts.",
      "tldr_zh": "本文提出MARRO模型，一种基于多头注意力机制(multi-headed attention)的新型多任务学习框架，专门用于法律文件的修辞角色标注任务（如区分事实、论据和最终判决）。该模型通过结合标签迁移(label shift)辅助任务与变压器架构，有效解决了法律文本非结构化、专业术语多和篇幅长等挑战。实验表明，MARRO在印度和英国最高法院的两个标注数据集上取得了最先进性能，为法律文件理解和下游任务（如案例摘要生成）提供了更强大的基础模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10659v1",
      "published_date": "2025-03-08 08:05:20 UTC",
      "updated_date": "2025-03-08 08:05:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:16:53.323917"
    },
    {
      "arxiv_id": "2503.06108v1",
      "title": "Multi-modal expressive personality recognition in data non-ideal audiovisual based on multi-scale feature enhancement and modal augment",
      "title_zh": "基于多尺度特征增强与模态强化的非理想视听数据多模态表现型人格识别",
      "authors": [
        "Weixuan Kong",
        "Jinpeng Yu",
        "Zijun Li",
        "Hanwei Liu",
        "Jiqing Qu",
        "Hui Xiao",
        "Xuefeng Li"
      ],
      "abstract": "Automatic personality recognition is a research hotspot in the intersection\nof computer science and psychology, and in human-computer interaction,\npersonalised has a wide range of applications services and other scenarios. In\nthis paper, an end-to-end multimodal performance personality is established for\nboth visual and auditory modal datarecognition network , and the through\nfeature-level fusion , which effectively of the two modalities is carried out\nthe cross-attention mechanismfuses the features of the two modal data; and a is\nproposed multiscale feature enhancement modalitiesmodule , which enhances for\nvisual and auditory boththe expression of the information of effective the\nfeatures and suppresses the interference of the redundant information. In\naddition, during the training process, this paper proposes a modal enhancement\ntraining strategy to simulate non-ideal such as modal loss and noise\ninterferencedata situations , which enhances the adaptability ofand the model\nto non-ideal data scenarios improves the robustness of the model. Experimental\nresults show that the method proposed in this paper is able to achieve an\naverage Big Five personality accuracy of , which outperforms existing 0.916 on\nthe personality analysis dataset ChaLearn First Impressionother methods based\non audiovisual and audio-visual both modalities. The ablation experiments also\nvalidate our proposed , respectivelythe contribution of module and modality\nenhancement strategy to the model performance. Finally, we simulate in the\ninference phase multi-scale feature enhancement six non-ideal data scenarios to\nverify the modal enhancement strategy's improvement in model robustness.",
      "tldr_zh": "本文提出了一种基于多尺度特征增强和模态增强的多模态表达性人格识别方法，针对非理想音视频数据场景。通过构建端到端的多模态网络，采用跨注意力机制进行特征级融合，并提出多尺度特征增强模块来提升视听模态的有效特征表达。研究还设计了模态增强训练策略以模拟模态缺失和噪声干扰等非理想情况，显著提升了模型鲁棒性。实验结果表明，该方法在ChaLearn First Impression数据集上取得了0.916的平均Big Five人格识别准确率，优于现有视听融合方法，并通过消融实验验证了各模块的有效性。",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06108v1",
      "published_date": "2025-03-08 07:20:44 UTC",
      "updated_date": "2025-03-08 07:20:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:17:15.688345"
    },
    {
      "arxiv_id": "2503.06107v1",
      "title": "Feature Fusion Attention Network with CycleGAN for Image Dehazing, De-Snowing and De-Raining",
      "title_zh": "基于CycleGAN的特征融合注意力网络用于图像去雾、去雪和去雨",
      "authors": [
        "Akshat Jain"
      ],
      "abstract": "This paper presents a novel approach to image dehazing by combining Feature\nFusion Attention (FFA) networks with CycleGAN architecture. Our method\nleverages both supervised and unsupervised learning techniques to effectively\nremove haze from images while preserving crucial image details. The proposed\nhybrid architecture demonstrates significant improvements in image quality\nmetrics, achieving superior PSNR and SSIM scores compared to traditional\ndehazing methods. Through extensive experimentation on the RESIDE and DenseHaze\nCVPR 2019 dataset, we show that our approach effectively handles both synthetic\nand real-world hazy images. CycleGAN handles the unpaired nature of hazy and\nclean images effectively, enabling the model to learn mappings even without\npaired data.",
      "tldr_zh": "本文提出一种结合特征融合注意力网络(FFA)和CycleGAN的创新方法，用于图像去雾、去雪和去雨。该方法融合有监督和无监督学习技术，在保持图像细节的同时有效去除雾霾，在RESIDE和DenseHaze数据集上取得了优于传统方法的PSNR和SSIM指标。CycleGAN的引入使模型能够处理非配对图像数据，有效学习雾霾图像与清晰图像之间的映射关系。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06107v1",
      "published_date": "2025-03-08 07:18:42 UTC",
      "updated_date": "2025-03-08 07:18:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:17:14.752103"
    },
    {
      "arxiv_id": "2503.06101v1",
      "title": "ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning",
      "title_zh": "ULTHO：深度强化学习中超轻量级且高效的超参数优化",
      "authors": [
        "Mingqi Yuan",
        "Bo Li",
        "Xin Jin",
        "Wenjun Zeng"
      ],
      "abstract": "Hyperparameter optimization (HPO) is a billion-dollar problem in machine\nlearning, which significantly impacts the training efficiency and model\nperformance. However, achieving efficient and robust HPO in deep reinforcement\nlearning (RL) is consistently challenging due to its high non-stationarity and\ncomputational cost. To tackle this problem, existing approaches attempt to\nadapt common HPO techniques (e.g., population-based training or Bayesian\noptimization) to the RL scenario. However, they remain sample-inefficient and\ncomputationally expensive, which cannot facilitate a wide range of\napplications. In this paper, we propose ULTHO, an ultra-lightweight yet\npowerful framework for fast HPO in deep RL within single runs. Specifically, we\nformulate the HPO process as a multi-armed bandit with clustered arms (MABC)\nand link it directly to long-term return optimization. ULTHO also provides a\nquantified and statistical perspective to filter the HPs efficiently. We test\nULTHO on benchmarks including ALE, Procgen, MiniGrid, and PyBullet. Extensive\nexperiments demonstrate that the ULTHO can achieve superior performance with\nsimple architecture, contributing to the development of advanced and automated\nRL systems.",
      "tldr_zh": "本文提出ULTHO框架，这是一种超轻量级但高效的深度强化学习(RL)超参数优化(HPO)方法。该框架将HPO过程建模为带聚类臂的多臂老虎机(MABC)问题，并直接关联长期回报优化，通过量化统计视角高效筛选超参数。实验证明，ULTHO在ALE、Procgen等基准测试中以简单架构实现了优异性能，相比传统方法显著提升了样本效率和计算效率，为自动化RL系统的发展提供了新思路。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 22 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06101v1",
      "published_date": "2025-03-08 07:03:43 UTC",
      "updated_date": "2025-03-08 07:03:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:17:13.643232"
    },
    {
      "arxiv_id": "2503.06092v1",
      "title": "ZO-DARTS++: An Efficient and Size-Variable Zeroth-Order Neural Architecture Search Algorithm",
      "title_zh": "ZO-DARTS++：一种高效且可变尺寸的零阶神经架构搜索算法",
      "authors": [
        "Lunchen Xie",
        "Eugenio Lomurno",
        "Matteo Gambella",
        "Danilo Ardagna",
        "Manual Roveri",
        "Matteo Matteucci",
        "Qingjiang Shi"
      ],
      "abstract": "Differentiable Neural Architecture Search (NAS) provides a promising avenue\nfor automating the complex design of deep learning (DL) models. However,\ncurrent differentiable NAS methods often face constraints in efficiency,\noperation selection, and adaptability under varying resource limitations. We\nintroduce ZO-DARTS++, a novel NAS method that effectively balances performance\nand resource constraints. By integrating a zeroth-order approximation for\nefficient gradient handling, employing a sparsemax function with temperature\nannealing for clearer and more interpretable architecture distributions, and\nadopting a size-variable search scheme for generating compact yet accurate\narchitectures, ZO-DARTS++ establishes a new balance between model complexity\nand performance. In extensive tests on medical imaging datasets, ZO-DARTS++\nimproves the average accuracy by up to 1.8\\% over standard DARTS-based methods\nand shortens search time by approximately 38.6\\%. Additionally, its\nresource-constrained variants can reduce the number of parameters by more than\n35\\% while maintaining competitive accuracy levels. Thus, ZO-DARTS++ offers a\nversatile and efficient framework for generating high-quality, resource-aware\nDL models suitable for real-world medical applications.",
      "tldr_zh": "该研究提出了ZO-DARTS++，一种高效且支持可变规模的零阶神经架构搜索（NAS）算法，旨在解决当前可微分NAS方法在效率、操作选择和资源适应性方面的局限性。通过引入零阶近似技术优化梯度计算、采用带温度退火的sparsemax函数提升架构分布的可解释性，以及支持可变规模的搜索机制，ZO-DARTS++在模型复杂度和性能之间取得了新的平衡。实验表明，在医学影像数据集上，ZO-DARTS++比基于DARTS的方法平均准确率提升1.8%，搜索时间缩短38.6%，其资源受限版本还能减少35%以上的参数量，同时保持较高的准确率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.5.1; I.5.4; I.2.6; I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.06092v1",
      "published_date": "2025-03-08 06:43:33 UTC",
      "updated_date": "2025-03-08 06:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:17:37.194603"
    },
    {
      "arxiv_id": "2503.06083v1",
      "title": "T-CBF: Traversability-based Control Barrier Function to Navigate Vertically Challenging Terrain",
      "title_zh": "T-CBF：基于可通行性的控制屏障函数用于垂直挑战性地形导航",
      "authors": [
        "Manas Gupta",
        "Xuesu Xiao"
      ],
      "abstract": "Safety has been of paramount importance in motion planning and control\ntechniques and is an active area of research in the past few years. Most safety\nresearch for mobile robots target at maintaining safety with the notion of\ncollision avoidance. However, safety goes beyond just avoiding collisions,\nespecially when robots have to navigate unstructured, vertically challenging,\noff-road terrain, where vehicle rollover and immobilization is as critical as\ncollisions. In this work, we introduce a novel Traversability-based Control\nBarrier Function (T-CBF), in which we use neural Control Barrier Functions\n(CBFs) to achieve safety beyond collision avoidance on unstructured vertically\nchallenging terrain by reasoning about new safety aspects in terms of\ntraversability. The neural T-CBF trained on safe and unsafe observations\nspecific to traversability safety is then used to generate safe trajectories.\nFurthermore, we present experimental results in simulation and on a physical\nVerti-4 Wheeler (V4W) platform, demonstrating that T-CBF can provide\ntraversability safety while reaching the goal position. T-CBF planner\noutperforms previously developed planners by 30\\% in terms of keeping the robot\nsafe and mobile when navigating on real world vertically challenging terrain.",
      "tldr_zh": "该研究提出了一种基于可穿越性的控制屏障函数(T-CBF)，用于在垂直挑战性地形中实现安全导航。传统安全研究主要关注避碰，但T-CBF通过神经控制屏障函数(CBFs)进一步考虑地形可穿越性，避免车辆倾覆和卡滞等风险。实验表明，T-CBF在仿真和实际Verti-4 Wheeler (V4W)平台上均能生成安全轨迹，相比现有规划器在真实垂直挑战性地形中的安全性和移动性提高了30%。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06083v1",
      "published_date": "2025-03-08 06:12:38 UTC",
      "updated_date": "2025-03-08 06:12:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:17:29.641164"
    },
    {
      "arxiv_id": "2503.07661v1",
      "title": "Disrupting Model Merging: A Parameter-Level Defense Without Sacrificing Accuracy",
      "title_zh": "破坏模型合并：一种不牺牲准确性的参数级防御方法",
      "authors": [
        "Wei Junhao",
        "Yu Zhe",
        "Sakuma Jun"
      ],
      "abstract": "Model merging is a technique that combines multiple finetuned models into a\nsingle model without additional training, allowing a free-rider to cheaply\ninherit specialized capabilities. This study investigates methodologies to\nsuppress unwanted model merging by free-riders. Existing methods such as model\nwatermarking or fingerprinting can only detect merging in hindsight. In\ncontrast, we propose a first proactive defense against model merging.\nSpecifically, our defense method modifies the model parameters so that the\nmodel is disrupted if the model is merged with any other model, while its\nfunctionality is kept unchanged if not merged with others. Our approach\nconsists of two modules, rearranging MLP parameters and scaling attention\nheads, which push the model out of the shared basin in parameter space, causing\nthe merging performance with other models to degrade significantly. We conduct\nextensive experiments on image classification, image generation, and text\nclassification to demonstrate that our defense severely disrupts merging while\nretaining the functionality of the post-protect model. Moreover, we analyze\npotential adaptive attacks and further propose a dropout-based pruning to\nimprove our proposal's robustness.",
      "tldr_zh": "本研究提出了一种主动防御模型合并的方法，通过修改模型参数来阻止未经授权的模型合并，同时不影响模型自身的功能。具体方法包括重新排列MLP参数和缩放注意力头，使模型在参数空间中脱离共享区域，从而显著降低与其他模型合并后的性能。实验表明，该方法在图像分类、图像生成和文本分类任务中有效破坏了模型合并，同时保持了模型原有的功能。此外，研究还分析了潜在的适应性攻击，并提出了基于dropout的剪枝策略以增强防御的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.07661v1",
      "published_date": "2025-03-08 06:08:47 UTC",
      "updated_date": "2025-03-08 06:08:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:17:50.469532"
    },
    {
      "arxiv_id": "2503.06074v1",
      "title": "Towards Conversational AI for Disease Management",
      "title_zh": "迈向疾病管理的对话式人工智能",
      "authors": [
        "Anil Palepu",
        "Valentin Liévin",
        "Wei-Hung Weng",
        "Khaled Saab",
        "David Stutz",
        "Yong Cheng",
        "Kavita Kulkarni",
        "S. Sara Mahdavi",
        "Joëlle Barral",
        "Dale R. Webster",
        "Katherine Chou",
        "Avinatan Hassidim",
        "Yossi Matias",
        "James Manyika",
        "Ryutaro Tanno",
        "Vivek Natarajan",
        "Adam Rodman",
        "Tao Tu",
        "Alan Karthikesalingam",
        "Mike Schaekermann"
      ],
      "abstract": "While large language models (LLMs) have shown promise in diagnostic dialogue,\ntheir capabilities for effective management reasoning - including disease\nprogression, therapeutic response, and safe medication prescription - remain\nunder-explored. We advance the previously demonstrated diagnostic capabilities\nof the Articulate Medical Intelligence Explorer (AMIE) through a new LLM-based\nagentic system optimised for clinical management and dialogue, incorporating\nreasoning over the evolution of disease and multiple patient visit encounters,\nresponse to therapy, and professional competence in medication prescription. To\nground its reasoning in authoritative clinical knowledge, AMIE leverages\nGemini's long-context capabilities, combining in-context retrieval with\nstructured reasoning to align its output with relevant and up-to-date clinical\npractice guidelines and drug formularies. In a randomized, blinded virtual\nObjective Structured Clinical Examination (OSCE) study, AMIE was compared to 21\nprimary care physicians (PCPs) across 100 multi-visit case scenarios designed\nto reflect UK NICE Guidance and BMJ Best Practice guidelines. AMIE was\nnon-inferior to PCPs in management reasoning as assessed by specialist\nphysicians and scored better in both preciseness of treatments and\ninvestigations, and in its alignment with and grounding of management plans in\nclinical guidelines. To benchmark medication reasoning, we developed RxQA, a\nmultiple-choice question benchmark derived from two national drug formularies\n(US, UK) and validated by board-certified pharmacists. While AMIE and PCPs both\nbenefited from the ability to access external drug information, AMIE\noutperformed PCPs on higher difficulty questions. While further research would\nbe needed before real-world translation, AMIE's strong performance across\nevaluations marks a significant step towards conversational AI as a tool in\ndisease management.",
      "tldr_zh": "该研究提出了一种基于大语言模型(LLM)的临床对话系统AMIE，专门优化用于疾病管理推理，包括疾病进展追踪、治疗反应评估和安全用药建议。系统结合Gemini的长文本处理能力，通过上下文检索和结构化推理，确保输出符合最新临床指南和药物处方集。在虚拟临床考核(OSCE)中，AMIE与21位初级保健医生(PCP)相比，在治疗精确性和临床指南遵循度上表现更优，特别是在RxQA药物推理基准测试中，AMIE在高难度问题上显著优于PCP。该研究标志着对话式AI在疾病管理应用中的重要进展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "62 pages, 7 figures in main text, 36 figures in appendix",
      "pdf_url": "http://arxiv.org/pdf/2503.06074v1",
      "published_date": "2025-03-08 05:48:58 UTC",
      "updated_date": "2025-03-08 05:48:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:18:10.232868"
    },
    {
      "arxiv_id": "2503.06073v1",
      "title": "GEM: Empowering MLLM for Grounded ECG Understanding with Time Series and Images",
      "title_zh": "GEM：赋能多模态大语言模型，结合时间序列与图像实现心电图理解",
      "authors": [
        "Xiang Lan",
        "Feng Wu",
        "Kai He",
        "Qinghao Zhao",
        "Shenda Hong",
        "Mengling Feng"
      ],
      "abstract": "While recent multimodal large language models (MLLMs) have advanced automated\nECG interpretation, they still face two key limitations: (1) insufficient\nmultimodal synergy between time series signals and visual ECG representations,\nand (2) limited explainability in linking diagnoses to granular waveform\nevidence. We introduce GEM, the first MLLM unifying ECG time series, 12-lead\nECG images and text for grounded and clinician-aligned ECG interpretation. GEM\nenables feature-grounded analysis, evidence-driven reasoning, and a\nclinician-like diagnostic process through three core innovations: a\ndual-encoder framework extracting complementary time series and image features,\ncross-modal alignment for effective multimodal understanding, and\nknowledge-guided instruction generation for generating high-granularity\ngrounding data (ECG-Grounding) linking diagnoses to measurable parameters\n($e.g.$, QRS/PR Intervals). Additionally, we propose the Grounded ECG\nUnderstanding task, a clinically motivated benchmark designed to\ncomprehensively assess the MLLM's capability in grounded ECG understanding.\nExperimental results on both existing and our proposed benchmarks show GEM\nsignificantly improves predictive performance (CSN $7.4\\% \\uparrow$),\nexplainability ($22.7\\% \\uparrow$), and grounding ($24.8\\% \\uparrow$), making\nit more suitable for real-world clinical applications. GitHub repository:\nhttps://github.com/lanxiang1017/GEM.git",
      "tldr_zh": "本文提出GEM模型，首个整合ECG时间序列、12导联心电图图像与文本的多模态大语言模型(MLLM)，旨在解决现有ECG自动诊断中多模态协同不足和可解释性有限的问题。该模型通过双编码器框架提取时空特征，结合跨模态对齐技术和知识引导的指令生成(ECG-Grounding)，实现了基于特征的分析、证据驱动的推理和类临床医生的诊断流程。在提出的Grounded ECG Understanding基准测试中，GEM在预测准确性(提升7.4%)、可解释性(提升22.7%)和诊断依据关联性(提升24.8%)方面显著优于现有方法，为临床ECG诊断提供了更可靠的AI辅助工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06073v1",
      "published_date": "2025-03-08 05:48:53 UTC",
      "updated_date": "2025-03-08 05:48:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:18:18.186775"
    },
    {
      "arxiv_id": "2503.06072v1",
      "title": "A Survey on Post-training of Large Language Models",
      "title_zh": "大型语言模型的后训练技术综述",
      "authors": [
        "Guiyao Tie",
        "Zeli Zhao",
        "Dingjie Song",
        "Fuyang Wei",
        "Rong Zhou",
        "Yurou Dai",
        "Wen Yin",
        "Zhejian Yang",
        "Jiangyue Yan",
        "Yao Su",
        "Zhenhan Dai",
        "Yifeng Xie",
        "Yihan Cao",
        "Lichao Sun",
        "Pan Zhou",
        "Lifang He",
        "Hechang Chen",
        "Yu Zhang",
        "Qingsong Wen",
        "Tianming Liu",
        "Neil Zhenqiang Gong",
        "Jiliang Tang",
        "Caiming Xiong",
        "Heng Ji",
        "Philip S. Yu",
        "Jianfeng Gao"
      ],
      "abstract": "The emergence of Large Language Models (LLMs) has fundamentally transformed\nnatural language processing, making them indispensable across domains ranging\nfrom conversational systems to scientific exploration. However, their\npre-trained architectures often reveal limitations in specialized contexts,\nincluding restricted reasoning capacities, ethical uncertainties, and\nsuboptimal domain-specific performance. These challenges necessitate advanced\npost-training language models (PoLMs) to address these shortcomings, such as\nOpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or\nLRMs). This paper presents the first comprehensive survey of PoLMs,\nsystematically tracing their evolution across five core paradigms: Fine-tuning,\nwhich enhances task-specific accuracy; Alignment, which ensures alignment with\nhuman preferences; Reasoning, which advances multi-step inference despite\nchallenges in reward design; Efficiency, which optimizes resource utilization\namidst increasing complexity; and Integration and Adaptation, which extend\ncapabilities across diverse modalities while addressing coherence issues.\nCharting progress from ChatGPT's foundational alignment strategies to\nDeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs\nleverage datasets to mitigate biases, deepen reasoning capabilities, and\nenhance domain adaptability. Our contributions include a pioneering synthesis\nof PoLM evolution, a structured taxonomy categorizing techniques and datasets,\nand a strategic agenda emphasizing the role of LRMs in improving reasoning\nproficiency and domain flexibility. As the first survey of its scope, this work\nconsolidates recent PoLM advancements and establishes a rigorous intellectual\nframework for future research, fostering the development of LLMs that excel in\nprecision, ethical robustness, and versatility across scientific and societal\napplications.",
      "tldr_zh": "本文首次对大型语言模型（LLMs）的后训练（PoLMs）进行了全面综述，系统梳理了其五大核心范式的发展：微调（Fine-tuning）提升任务特定准确性，对齐（Alignment）确保与人类偏好一致，推理（Reasoning）推进多步推断，效率（Efficiency）优化资源利用，以及整合与适应（Integration and Adaptation）扩展多模态能力。研究总结了从ChatGPT的对齐策略到DeepSeek-R1的推理创新等进展，展示了PoLMs如何利用数据集减轻偏见、深化推理能力并增强领域适应性。本文为未来研究提供了结构化分类和战略议程，推动LLMs在精度、伦理稳健性和跨领域应用中的进一步发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "87 pages, 21 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.06072v1",
      "published_date": "2025-03-08 05:41:42 UTC",
      "updated_date": "2025-03-08 05:41:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:18:17.069369"
    },
    {
      "arxiv_id": "2503.06064v1",
      "title": "A Novel Trustworthy Video Summarization Algorithm Through a Mixture of LoRA Experts",
      "title_zh": "一种基于LoRA专家混合的新型可信视频摘要算法",
      "authors": [
        "Wenzhuo Du",
        "Gerun Wang",
        "Guancheng Chen",
        "Hang Zhao",
        "Xin Li",
        "Jian Gao"
      ],
      "abstract": "With the exponential growth of user-generated content on video-sharing\nplatforms, the challenge of facilitating efficient searching and browsing of\nvideos has garnered significant attention. To enhance users' ability to swiftly\nlocate and review pertinent videos, the creation of concise and informative\nvideo summaries has become increasingly important. Video-llama is an effective\ntool for generating video summarization, but it cannot effectively unify and\noptimize the modeling of temporal and spatial features and requires a lot of\ncomputational resources and time. Therefore, we propose MiLoRA-ViSum to more\nefficiently capture complex temporal dynamics and spatial relationships\ninherent in video data and to control the number of parameters for training. By\nextending traditional Low-Rank Adaptation (LoRA) into a sophisticated\nmixture-of-experts paradigm, MiLoRA-ViSum incorporates a dual temporal-spatial\nadaptation mechanism tailored specifically for video summarization tasks. This\napproach dynamically integrates specialized LoRA experts, each fine-tuned to\naddress distinct temporal or spatial dimensions. Extensive evaluations of the\nVideoXum and ActivityNet datasets demonstrate that MiLoRA-ViSum achieves the\nbest summarization performance compared to state-of-the-art models, while\nmaintaining significantly lower computational costs. The proposed\nmixture-of-experts strategy, combined with the dual adaptation mechanism,\nhighlights the model's potential to enhance video summarization capabilities,\nparticularly in large-scale applications requiring both efficiency and\nprecision.",
      "tldr_zh": "本文提出了一种基于LoRA专家混合模型(MiLoRA-ViSum)的可信视频摘要算法，旨在高效捕捉视频数据中的复杂时空特征。该算法通过将传统的低秩适应(LoRA)扩展为专家混合范式，并引入双时空适应机制，动态整合专门针对时间或空间维度优化的LoRA专家。实验结果表明，MiLoRA-ViSum在VideoXum和ActivityNet数据集上取得了最佳摘要性能，同时显著降低了计算成本，为大规模视频摘要应用提供了高效且精确的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06064v1",
      "published_date": "2025-03-08 05:20:52 UTC",
      "updated_date": "2025-03-08 05:20:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:18:26.778908"
    },
    {
      "arxiv_id": "2503.06060v1",
      "title": "STAR: A Foundation Model-driven Framework for Robust Task Planning and Failure Recovery in Robotic Systems",
      "title_zh": "STAR：基于基础模型驱动的机器人系统鲁棒任务规划与故障恢复框架",
      "authors": [
        "Md Sadman Sakib",
        "Yu Sun"
      ],
      "abstract": "Modern robotic systems, deployed across domains from industrial automation to\ndomestic assistance, face a critical challenge: executing tasks with precision\nand adaptability in dynamic, unpredictable environments. To address this, we\npropose STAR (Smart Task Adaptation and Recovery), a novel framework that\nsynergizes Foundation Models (FMs) with dynamically expanding Knowledge Graphs\n(KGs) to enable resilient task planning and autonomous failure recovery. While\nFMs offer remarkable generalization and contextual reasoning, their\nlimitations, including computational inefficiency, hallucinations, and output\ninconsistencies hinder reliable deployment. STAR mitigates these issues by\nembedding learned knowledge into structured, reusable KGs, which streamline\ninformation retrieval, reduce redundant FM computations, and provide precise,\nscenario-specific insights. The framework leverages FM-driven reasoning to\ndiagnose failures, generate context-aware recovery strategies, and execute\ncorrective actions without human intervention or system restarts. Unlike\nconventional approaches that rely on rigid protocols, STAR dynamically expands\nits KG with experiential knowledge, ensuring continuous adaptation to novel\nscenarios. To evaluate the effectiveness of this approach, we developed a\ncomprehensive dataset that includes various robotic tasks and failure\nscenarios. Through extensive experimentation, STAR demonstrated an 86% task\nplanning accuracy and 78% recovery success rate, showing significant\nimprovements over baseline methods. The framework's ability to continuously\nlearn from experience while maintaining structured knowledge representation\nmakes it particularly suitable for long-term deployment in real-world\napplications.",
      "tldr_zh": "该研究提出了STAR框架，结合Foundation Models（FMs）与动态扩展的知识图谱（KGs），用于提升机器人系统在动态环境中的任务规划与故障恢复能力。STAR通过将FM的学习知识嵌入结构化KGs，优化信息检索并减少冗余计算，同时利用FM推理诊断故障并生成上下文感知的恢复策略，实现自主纠正。实验表明，STAR在任务规划准确率和故障恢复成功率上分别达到86%和78%，显著优于基线方法，适用于长期实际部署。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06060v1",
      "published_date": "2025-03-08 05:05:21 UTC",
      "updated_date": "2025-03-08 05:05:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:18:38.212528"
    },
    {
      "arxiv_id": "2503.06059v1",
      "title": "MANDARIN: Mixture-of-Experts Framework for Dynamic Delirium and Coma Prediction in ICU Patients: Development and Validation of an Acute Brain Dysfunction Prediction Model",
      "title_zh": "MANDARIN：基于专家混合框架的ICU患者谵妄与昏迷动态预测模型——急性脑功能障碍预测系统的开发与验证",
      "authors": [
        "Miguel Contreras",
        "Jessica Sena",
        "Andrea Davidson",
        "Jiaqing Zhang",
        "Tezcan Ozrazgat-Baslanti",
        "Yuanfang Ren",
        "Ziyuan Guan",
        "Jeremy Balch",
        "Tyler Loftus",
        "Subhash Nerella",
        "Azra Bihorac",
        "Parisa Rashidi"
      ],
      "abstract": "Acute brain dysfunction (ABD) is a common, severe ICU complication,\npresenting as delirium or coma and leading to prolonged stays, increased\nmortality, and cognitive decline. Traditional screening tools like the Glasgow\nComa Scale (GCS), Confusion Assessment Method (CAM), and Richmond\nAgitation-Sedation Scale (RASS) rely on intermittent assessments, causing\ndelays and inconsistencies. In this study, we propose MANDARIN\n(Mixture-of-Experts Framework for Dynamic Delirium and Coma Prediction in ICU\nPatients), a 1.5M-parameter mixture-of-experts neural network to predict ABD in\nreal-time among ICU patients. The model integrates temporal and static data\nfrom the ICU to predict the brain status in the next 12 to 72 hours, using a\nmulti-branch approach to account for current brain status. The MANDARIN model\nwas trained on data from 92,734 patients (132,997 ICU admissions) from 2\nhospitals between 2008-2019 and validated externally on data from 11,719\npatients (14,519 ICU admissions) from 15 hospitals and prospectively on data\nfrom 304 patients (503 ICU admissions) from one hospital in 2021-2024. Three\ndatasets were used: the University of Florida Health (UFH) dataset, the\nelectronic ICU Collaborative Research Database (eICU), and the Medical\nInformation Mart for Intensive Care (MIMIC)-IV dataset. MANDARIN significantly\noutperforms the baseline neurological assessment scores (GCS, CAM, and RASS)\nfor delirium prediction in both external (AUROC 75.5% CI: 74.2%-76.8% vs 68.3%\nCI: 66.9%-69.5%) and prospective (AUROC 82.0% CI: 74.8%-89.2% vs 72.7% CI:\n65.5%-81.0%) cohorts, as well as for coma prediction (external AUROC 87.3% CI:\n85.9%-89.0% vs 72.8% CI: 70.6%-74.9%, and prospective AUROC 93.4% CI:\n88.5%-97.9% vs 67.7% CI: 57.7%-76.8%) with a 12-hour lead time. This tool has\nthe potential to assist clinicians in decision-making by continuously\nmonitoring the brain status of patients in the ICU.",
      "tldr_zh": "该研究提出MANDARIN框架，一种基于混合专家系统(Mixture-of-Experts)的神经网络模型，用于实时预测ICU患者的急性脑功能障碍(ABD)。该模型整合ICU动态和静态数据，采用多分支架构预测未来12-72小时患者脑状态（谵妄或昏迷），在三个大型临床数据集（共计超10万患者）上验证效果显著，预测性能（AUROC）比传统评估方法（GCS/CAM/RASS）提升7-25个百分点。这一工具可实现ICU患者脑功能的持续监测，为临床决策提供支持。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06059v1",
      "published_date": "2025-03-08 04:56:41 UTC",
      "updated_date": "2025-03-08 04:56:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:18:57.751794"
    },
    {
      "arxiv_id": "2503.06054v1",
      "title": "Fine-Grained Bias Detection in LLM: Enhancing detection mechanisms for nuanced biases",
      "title_zh": "LLM中的细粒度偏见检测：增强对细微偏见的检测机制",
      "authors": [
        "Suvendu Mohanty"
      ],
      "abstract": "Recent advancements in Artificial Intelligence, particularly in Large\nLanguage Models (LLMs), have transformed natural language processing by\nimproving generative capabilities. However, detecting biases embedded within\nthese models remains a challenge. Subtle biases can propagate misinformation,\ninfluence decision-making, and reinforce stereotypes, raising ethical concerns.\nThis study presents a detection framework to identify nuanced biases in LLMs.\nThe approach integrates contextual analysis, interpretability via attention\nmechanisms, and counterfactual data augmentation to capture hidden biases\nacross linguistic contexts. The methodology employs contrastive prompts and\nsynthetic datasets to analyze model behaviour across cultural, ideological, and\ndemographic scenarios.\n  Quantitative analysis using benchmark datasets and qualitative assessments\nthrough expert reviews validate the effectiveness of the framework. Results\nshow improvements in detecting subtle biases compared to conventional methods,\nwhich often fail to highlight disparities in model responses to race, gender,\nand socio-political contexts. The framework also identifies biases arising from\nimbalances in training data and model architectures. Continuous user feedback\nensures adaptability and refinement. This research underscores the importance\nof proactive bias mitigation strategies and calls for collaboration between\npolicymakers, AI developers, and regulators. The proposed detection mechanisms\nenhance model transparency and support responsible LLM deployment in sensitive\napplications such as education, legal systems, and healthcare. Future work will\nfocus on real-time bias monitoring and cross-linguistic generalization to\nimprove fairness and inclusivity in AI-driven communication tools.",
      "tldr_zh": "该研究提出了一种细粒度偏见检测框架，旨在提升对大语言模型(LLMs)中微妙偏见的识别能力。该方法结合了上下文分析、注意力机制可解释性和反事实数据增强技术，通过对比提示和合成数据集分析模型在不同文化、意识形态和人口统计场景下的行为。实验表明，该框架在检测种族、性别和社会政治背景相关的微妙偏见方面优于传统方法，并揭示了训练数据不平衡和模型架构导致的偏见。该研究强调主动偏见缓解策略的重要性，为教育、法律和医疗等敏感领域的LLM部署提供了透明性支持。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Bias detection, Large Language Models, nuanced biases, fine-grained\n  mechanisms, model transparency, ethical AI",
      "pdf_url": "http://arxiv.org/pdf/2503.06054v1",
      "published_date": "2025-03-08 04:43:01 UTC",
      "updated_date": "2025-03-08 04:43:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:18:52.863396"
    },
    {
      "arxiv_id": "2503.06053v1",
      "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation",
      "title_zh": "DropletVideo：探索时空一致性视频生成的数据集与方法",
      "authors": [
        "Runze Zhang",
        "Guoguang Du",
        "Xiaochuan Li",
        "Qi Jia",
        "Liang Jin",
        "Lu Liu",
        "Jingjing Wang",
        "Cong Xu",
        "Zhenhua Guo",
        "Yaqian Zhao",
        "Xiaoli Gong",
        "Rengang Li",
        "Baoyu Fan"
      ],
      "abstract": "Spatio-temporal consistency is a critical research topic in video generation.\nA qualified generated video segment must ensure plot plausibility and coherence\nwhile maintaining visual consistency of objects and scenes across varying\nviewpoints. Prior research, especially in open-source projects, primarily\nfocuses on either temporal or spatial consistency, or their basic combination,\nsuch as appending a description of a camera movement after a prompt without\nconstraining the outcomes of this movement. However, camera movement may\nintroduce new objects to the scene or eliminate existing ones, thereby\noverlaying and affecting the preceding narrative. Especially in videos with\nnumerous camera movements, the interplay between multiple plots becomes\nincreasingly complex. This paper introduces and examines integral\nspatio-temporal consistency, considering the synergy between plot progression\nand camera techniques, and the long-term impact of prior content on subsequent\ngeneration. Our research encompasses dataset construction through to the\ndevelopment of the model. Initially, we constructed a DropletVideo-10M dataset,\nwhich comprises 10 million videos featuring dynamic camera motion and object\nactions. Each video is annotated with an average caption of 206 words,\ndetailing various camera movements and plot developments. Following this, we\ndeveloped and trained the DropletVideo model, which excels in preserving\nspatio-temporal coherence during video generation. The DropletVideo dataset and\nmodel are accessible at https://dropletx.github.io.",
      "tldr_zh": "该研究提出了DropletVideo，一种探索时空一致性视频生成的数据集和方法。研究团队构建了包含1000万段视频的DropletVideo-10M数据集，每段视频标注了详细的相机运动和情节发展。基于此，开发了DropletVideo模型，能够有效保持视频生成过程中的时空一致性，特别是处理相机运动引入的新对象或消除现有对象时的叙事连贯性。这项研究为复杂相机运动下的视频生成提供了新的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06053v1",
      "published_date": "2025-03-08 04:37:38 UTC",
      "updated_date": "2025-03-08 04:37:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:18:58.679859"
    },
    {
      "arxiv_id": "2503.06047v1",
      "title": "DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments",
      "title_zh": "DSGBench：面向复杂决策环境评估基于大语言模型智能体的多样化战略博弈基准",
      "authors": [
        "Wenjie Tang",
        "Yuan Zhou",
        "Erqiang Xu",
        "Keyan Cheng",
        "Minne Li",
        "Liquan Xiao"
      ],
      "abstract": "Large Language Model~(LLM) based agents have been increasingly popular in\nsolving complex and dynamic tasks, which requires proper evaluation systems to\nassess their capabilities. Nevertheless, existing benchmarks usually either\nfocus on single-objective tasks or use overly broad assessing metrics, failing\nto provide a comprehensive inspection of the actual capabilities of LLM-based\nagents in complicated decision-making tasks. To address these issues, we\nintroduce DSGBench, a more rigorous evaluation platform for strategic\ndecision-making. Firstly, it incorporates six complex strategic games which\nserve as ideal testbeds due to their long-term and multi-dimensional\ndecision-making demands and flexibility in customizing tasks of various\ndifficulty levels or multiple targets. Secondly, DSGBench employs a\nfine-grained evaluation scoring system which examines the decision-making\ncapabilities by looking into the performance in five specific dimensions and\noffering a comprehensive assessment in a well-designed way. Furthermore,\nDSGBench also incorporates an automated decision-tracking mechanism which\nenables in-depth analysis of agent behaviour patterns and the changes in their\nstrategies. We demonstrate the advances of DSGBench by applying it to multiple\npopular LLM-based agents and our results suggest that DSGBench provides\nvaluable insights in choosing LLM-based agents as well as improving their\nfuture development. DSGBench is available at\nhttps://github.com/DeciBrain-Group/DSGBench.",
      "tldr_zh": "该研究提出了DSGBench基准测试平台，用于评估基于大语言模型(LLM)的智能体在复杂战略决策环境中的表现。该平台包含6种复杂战略游戏，通过多维度的精细评分系统(考察5个具体维度)和自动决策追踪机制，全面评估智能体的长期多目标决策能力。实验表明DSGBench能为LLM智能体的选择和改进提供有价值的洞见，弥补了现有基准测试在复杂决策任务评估上的不足。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "43 pages, 5 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2503.06047v1",
      "published_date": "2025-03-08 04:17:23 UTC",
      "updated_date": "2025-03-08 04:17:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:19:09.878578"
    },
    {
      "arxiv_id": "2503.07660v1",
      "title": "Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity",
      "title_zh": "超级对齐研究应通过能力与合规性的并行优化即刻推进",
      "authors": [
        "HyunJin Kim",
        "Xiaoyuan Yi",
        "Jing Yao",
        "Muhua Huang",
        "JinYeong Bak",
        "James Evans",
        "Xing Xie"
      ],
      "abstract": "The recent leap in AI capabilities, driven by big generative models, has\nsparked the possibility of achieving Artificial General Intelligence (AGI) and\nfurther triggered discussions on Artificial Superintelligence (ASI), a system\nsurpassing all humans across all domains. This gives rise to the critical\nresearch question of: If we realize ASI, how do we align it with human values,\nensuring it benefits rather than harms human society, a.k.a., the\nSuperalignment problem. Despite ASI being regarded by many as solely a\nhypothetical concept, in this paper, we argue that superalignment is achievable\nand research on it should advance immediately, through simultaneous and\nalternating optimization of task competence and value conformity. We posit that\nsuperalignment is not merely a safeguard for ASI but also necessary for its\nrealization. To support this position, we first provide a formal definition of\nsuperalignment rooted in the gap between capability and capacity and elaborate\non our argument. Then we review existing paradigms, explore their\ninterconnections and limitations, and illustrate a potential path to\nsuperalignment centered on two fundamental principles. We hope this work sheds\nlight on a practical approach for developing the value-aligned next-generation\nAI, garnering greater benefits and reducing potential harms for humanity.",
      "tldr_zh": "该论文主张超级对齐(Superalignment)研究应立即推进，通过并行优化任务能力(competence)和价值一致性(conformity)来实现。作者认为超级对齐不仅是人工超级智能(ASI)的安全保障，也是其实现的必要条件。论文首先基于能力与容量之间的差距给出了超级对齐的正式定义，并阐述了相关论点。随后，作者回顾了现有范式，探讨了其相互联系和局限性，并提出了以两大基本原则为核心的超级对齐实现路径。这项工作为开发价值对齐的下一代AI提供了实践指导，旨在为人类带来更大利益并减少潜在危害。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07660v1",
      "published_date": "2025-03-08 04:10:11 UTC",
      "updated_date": "2025-03-08 04:10:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:19:25.708870"
    },
    {
      "arxiv_id": "2503.10657v1",
      "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
      "title_zh": "RouterEval：全面基准测试用于路由大语言模型以探索模型级扩展",
      "authors": [
        "Zhongzhan Huang",
        "Guoming Ling",
        "Vincent S. Liang",
        "Yupei Lin",
        "Yandong Chen",
        "Shanshan Zhong",
        "Hefeng Wu",
        "Liang Lin"
      ],
      "abstract": "Routing large language models (LLMs) is a novel paradigm that recommends the\nmost suitable LLM from a pool of candidates to process a given input through a\nwell-designed router. Our comprehensive analysis reveals a model-level\nscaling-up phenomenon in LLMs, i.e., a capable router can significantly enhance\nthe performance of this paradigm as the number of candidates increases. This\nimprovement can even easily surpass the performance of the best single model in\nthe pool and most existing strong LLMs, making it a highly promising paradigm.\nHowever, the lack of comprehensive and open-source benchmarks for Routing LLMs\nhas hindered the development of routers. In this paper, we introduce\nRouterEval, a benchmark designed specifically for router research, which\nincludes over 200,000,000 performance records for 12 popular LLM evaluations\nacross areas such as knowledge-based Q&A, commonsense reasoning, semantic\nunderstanding, mathematical reasoning, and instruction following, based on more\nthan 8,500 LLMs. Using RouterEval, extensive evaluations of existing Routing\nLLM methods reveal that most still have significant room for improvement. See\nhttps://github.com/MilkThink-Lab/RouterEval for all data, code, and tutorials.",
      "tldr_zh": "该研究提出了RouterEval，一个专门用于路由大语言模型(LLMs)研究的综合基准，旨在探索LLMs模型级扩展的潜力。通过分析超过8500个LLMs在12项评估任务中的2亿条性能记录，研究发现，随着候选模型数量的增加，一个高效的路由器可以显著提升整体性能，甚至超越单个最佳模型和现有强LLMs的表现。RouterEval的引入填补了该领域缺乏开源基准的空白，为路由器方法的进一步优化提供了重要支持。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.10657v1",
      "published_date": "2025-03-08 04:07:07 UTC",
      "updated_date": "2025-03-08 04:07:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:19:32.520726"
    },
    {
      "arxiv_id": "2503.16485v2",
      "title": "Optimizing Generative AI's Accuracy and Transparency in Inductive Thematic Analysis: A Human-AI Comparison",
      "title_zh": "优化生成式AI在归纳主题分析中的准确性与透明度：人机比较研究",
      "authors": [
        "Matthew Nyaaba",
        "Min SungEun",
        "Mary Abiswin Apam",
        "Kwame Owoahene Acheampong",
        "Emmanuel Dwamena"
      ],
      "abstract": "This study highlights the transparency and accuracy of GenAI's inductive\nthematic analysis, particularly using GPT-4 Turbo API integrated within a\nstepwise prompt-based Python script. This approach ensured a traceable and\nsystematic coding process, generating codes with supporting statements and page\nreferences, which enhanced validation and reproducibility. The results indicate\nthat GenAI performs inductive coding in a manner closely resembling human\ncoders, effectively categorizing themes at a level like the average human\ncoder. However, in interpretation, GenAI extends beyond human coders by\nsituating themes within a broader conceptual context, providing a more\ngeneralized and abstract perspective.",
      "tldr_zh": "该研究评估了生成式AI（GenAI，基于GPT-4 Turbo API）在归纳式主题分析中的准确性和透明度。通过分步提示的Python脚本，该方法实现了可追溯的系统化编码过程，生成带引用支持的代码，提升了验证性和可复现性。结果表明，GenAI的主题分类能力接近人类平均水平，但在主题解释方面超越了人类，能将其置于更广泛的抽象概念框架中。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16485v2",
      "published_date": "2025-03-08 04:06:16 UTC",
      "updated_date": "2025-03-24 01:57:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:19:44.577598"
    },
    {
      "arxiv_id": "2503.06037v1",
      "title": "Vairiational Stochastic Games",
      "title_zh": "变分随机博弈",
      "authors": [
        "Zhiyu Zhao",
        "Haifeng Zhang"
      ],
      "abstract": "The Control as Inference (CAI) framework has successfully transformed\nsingle-agent reinforcement learning (RL) by reframing control tasks as\nprobabilistic inference problems. However, the extension of CAI to multi-agent,\ngeneral-sum stochastic games (SGs) remains underexplored, particularly in\ndecentralized settings where agents operate independently without centralized\ncoordination. In this paper, we propose a novel variational inference framework\ntailored to decentralized multi-agent systems. Our framework addresses the\nchallenges posed by non-stationarity and unaligned agent objectives, proving\nthat the resulting policies form an $\\epsilon$-Nash equilibrium. Additionally,\nwe demonstrate theoretical convergence guarantees for the proposed\ndecentralized algorithms. Leveraging this framework, we instantiate multiple\nalgorithms to solve for Nash equilibrium, mean-field Nash equilibrium, and\ncorrelated equilibrium, with rigorous theoretical convergence analysis.",
      "tldr_zh": "本文提出了一种新的变分推理框架，专为解决去中心化多智能体系统中的非平稳性和目标不一致性挑战而设计。该框架证明了所得策略构成$\\epsilon$-Nash均衡，并提供了理论收敛保证。基于此框架，研究实例化了多种算法，用于求解Nash均衡、平均场Nash均衡和相关均衡，并进行了严格的理论收敛分析。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06037v1",
      "published_date": "2025-03-08 03:21:23 UTC",
      "updated_date": "2025-03-08 03:21:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:19:46.366753"
    },
    {
      "arxiv_id": "2503.06030v1",
      "title": "Towards Universal Text-driven CT Image Segmentation",
      "title_zh": "迈向通用文本驱动的CT图像分割",
      "authors": [
        "Yuheng Li",
        "Yuxiang Lai",
        "Maria Thor",
        "Deborah Marshall",
        "Zachary Buchwald",
        "David S. Yu",
        "Xiaofeng Yang"
      ],
      "abstract": "Computed tomography (CT) is extensively used for accurate visualization and\nsegmentation of organs and lesions. While deep learning models such as\nconvolutional neural networks (CNNs) and vision transformers (ViTs) have\nsignificantly improved CT image analysis, their performance often declines when\napplied to diverse, real-world clinical data. Although foundation models offer\na broader and more adaptable solution, their potential is limited due to the\nchallenge of obtaining large-scale, voxel-level annotations for medical images.\nIn response to these challenges, prompting-based models using visual or text\nprompts have emerged. Visual-prompting methods, such as the Segment Anything\nModel (SAM), still require significant manual input and can introduce ambiguity\nwhen applied to clinical scenarios. Instead, foundation models that use text\nprompts offer a more versatile and clinically relevant approach. Notably,\ncurrent text-prompt models, such as the CLIP-Driven Universal Model, are\nlimited to text prompts already encountered during training and struggle to\nprocess the complex and diverse scenarios of real-world clinical applications.\nInstead of fine-tuning models trained from natural imaging, we propose\nOpenVocabCT, a vision-language model pretrained on large-scale 3D CT images for\nuniversal text-driven segmentation. Using the large-scale CT-RATE dataset, we\ndecompose the diagnostic reports into fine-grained, organ-level descriptions\nusing large language models for multi-granular contrastive learning. We\nevaluate our OpenVocabCT on downstream segmentation tasks across nine public\ndatasets for organ and tumor segmentation, demonstrating the superior\nperformance of our model compared to existing methods. All code, datasets, and\nmodels will be publicly released at https://github.com/ricklisz/OpenVocabCT.",
      "tldr_zh": "该研究提出 **OpenVocabCT**，一种基于大规模3D CT图像预训练的视觉-语言模型，旨在实现通用文本驱动的CT图像分割。通过利用 **CT-RATE** 数据集和大语言模型分解诊断报告为细粒度器官级描述，该方法实现了多粒度对比学习。实验表明，OpenVocabCT在9个公共数据集上的器官和肿瘤分割任务中表现优于现有方法，为临床复杂场景提供了更灵活、准确的解决方案。所有代码、数据集和模型均已开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.06030v1",
      "published_date": "2025-03-08 03:02:57 UTC",
      "updated_date": "2025-03-08 03:02:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:19:53.595193"
    },
    {
      "arxiv_id": "2503.06027v2",
      "title": "Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI Models",
      "title_zh": "赋能边缘智能：设备端AI模型全面综述",
      "authors": [
        "Xubin Wang",
        "Zhiqing Tang",
        "Jianxiong Guo",
        "Tianhui Meng",
        "Chenhao Wang",
        "Tian Wang",
        "Weijia Jia"
      ],
      "abstract": "The rapid advancement of artificial intelligence (AI) technologies has led to\nan increasing deployment of AI models on edge and terminal devices, driven by\nthe proliferation of the Internet of Things (IoT) and the need for real-time\ndata processing. This survey comprehensively explores the current state,\ntechnical challenges, and future trends of on-device AI models. We define\non-device AI models as those designed to perform local data processing and\ninference, emphasizing their characteristics such as real-time performance,\nresource constraints, and enhanced data privacy. The survey is structured\naround key themes, including the fundamental concepts of AI models, application\nscenarios across various domains, and the technical challenges faced in edge\nenvironments. We also discuss optimization and implementation strategies, such\nas data preprocessing, model compression, and hardware acceleration, which are\nessential for effective deployment. Furthermore, we examine the impact of\nemerging technologies, including edge computing and foundation models, on the\nevolution of on-device AI models. By providing a structured overview of the\nchallenges, solutions, and future directions, this survey aims to facilitate\nfurther research and application of on-device AI, ultimately contributing to\nthe advancement of intelligent systems in everyday life.",
      "tldr_zh": "本文综述了边缘智能领域的最新进展，系统性地探讨了设备端AI模型的现状、挑战和未来趋势。研究将设备端AI模型定义为能够进行本地数据处理和推理的模型，其核心特征包括实时性、资源受限性和增强的数据隐私保护。文章从基础概念、跨领域应用场景和技术挑战三个维度展开分析，重点讨论了数据预处理、模型压缩和硬件加速等关键优化策略对边缘部署的影响。同时，该研究还考察了边缘计算和基础模型等新兴技术对设备端AI发展的推动作用，为智能系统在日常生活中的应用提供了研究框架和发展方向。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper has been accepted by ACM Computing Surveys",
      "pdf_url": "http://arxiv.org/pdf/2503.06027v2",
      "published_date": "2025-03-08 02:59:51 UTC",
      "updated_date": "2025-03-17 13:37:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:20:04.038257"
    },
    {
      "arxiv_id": "2503.06026v1",
      "title": "Zero-Shot Peg Insertion: Identifying Mating Holes and Estimating SE(2) Poses with Vision-Language Models",
      "title_zh": "零样本插桩任务：基于视觉语言模型识别配合孔与SE(2)位姿估计",
      "authors": [
        "Masaru Yajima",
        "Kei Ota",
        "Asako Kanezaki",
        "Rei Kawakami"
      ],
      "abstract": "Achieving zero-shot peg insertion, where inserting an arbitrary peg into an\nunseen hole without task-specific training, remains a fundamental challenge in\nrobotics. This task demands a highly generalizable perception system capable of\ndetecting potential holes, selecting the correct mating hole from multiple\ncandidates, estimating its precise pose, and executing insertion despite\nuncertainties. While learning-based methods have been applied to peg insertion,\nthey often fail to generalize beyond the specific peg-hole pairs encountered\nduring training. Recent advancements in Vision-Language Models (VLMs) offer a\npromising alternative, leveraging large-scale datasets to enable robust\ngeneralization across diverse tasks. Inspired by their success, we introduce a\nnovel zero-shot peg insertion framework that utilizes a VLM to identify mating\nholes and estimate their poses without prior knowledge of their geometry.\nExtensive experiments demonstrate that our method achieves 90.2% accuracy,\nsignificantly outperforming baselines in identifying the correct mating hole\nacross a wide range of previously unseen peg-hole pairs, including 3D-printed\nobjects, toy puzzles, and industrial connectors. Furthermore, we validate the\neffectiveness of our approach in a real-world connector insertion task on a\nbackpanel of a PC, where our system successfully detects holes, identifies the\ncorrect mating hole, estimates its pose, and completes the insertion with a\nsuccess rate of 88.3%. These results highlight the potential of VLM-driven\nzero-shot reasoning for enabling robust and generalizable robotic assembly.",
      "tldr_zh": "该研究提出了一种基于视觉语言模型(VLM)的零样本(Zero-Shot)插销装配框架，无需特定训练即可将任意插销插入未见过的孔洞。该方法通过VLM识别匹配孔并估计其SE(2)位姿，在3D打印物体、玩具拼图和工业连接器等多样化场景下达到90.2%的匹配孔识别准确率。实际PC背板连接器插入任务验证了该方法的实用性，成功率高达88.3%，展现了VLM在实现通用化机器人装配方面的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Under submission",
      "pdf_url": "http://arxiv.org/pdf/2503.06026v1",
      "published_date": "2025-03-08 02:59:21 UTC",
      "updated_date": "2025-03-08 02:59:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:20:44.137175"
    },
    {
      "arxiv_id": "2503.06014v1",
      "title": "Towards Ambiguity-Free Spatial Foundation Model: Rethinking and Decoupling Depth Ambiguity",
      "title_zh": "迈向无歧义空间基础模型：深度歧义问题的重新思考与解耦",
      "authors": [
        "Xiaohao Xu",
        "Feng Xue",
        "Xiang Li",
        "Haowei Li",
        "Shusheng Yang",
        "Tianyi Zhang",
        "Matthew Johnson-Roberson",
        "Xiaonan Huang"
      ],
      "abstract": "Depth ambiguity is a fundamental challenge in spatial scene understanding,\nespecially in transparent scenes where single-depth estimates fail to capture\nfull 3D structure. Existing models, limited to deterministic predictions,\noverlook real-world multi-layer depth. To address this, we introduce a paradigm\nshift from single-prediction to multi-hypothesis spatial foundation models. We\nfirst present \\texttt{MD-3k}, a benchmark exposing depth biases in expert and\nfoundational models through multi-layer spatial relationship labels and new\nmetrics. To resolve depth ambiguity, we propose Laplacian Visual Prompting\n(LVP), a training-free spectral prompting technique that extracts hidden depth\nfrom pre-trained models via Laplacian-transformed RGB inputs. By integrating\nLVP-inferred depth with standard RGB-based estimates, our approach elicits\nmulti-layer depth without model retraining. Extensive experiments validate the\neffectiveness of LVP in zero-shot multi-layer depth estimation, unlocking more\nrobust and comprehensive geometry-conditioned visual generation, 3D-grounded\nspatial reasoning, and temporally consistent video-level depth inference. Our\nbenchmark and code will be available at\nhttps://github.com/Xiaohao-Xu/Ambiguity-in-Space.",
      "tldr_zh": "该研究针对空间场景理解中的深度模糊问题，提出了一种从单预测到多假设的空间基础模型新范式。研究者首先构建了\\texttt{MD-3k}基准，揭示了现有模型在多层深度估计上的偏差，并提出无需训练的拉普拉斯视觉提示技术(LVP)，通过拉普拉斯变换的RGB输入从预训练模型中提取隐藏深度信息。实验表明，LVP在零样本多层深度估计中表现出色，显著提升了几何条件视觉生成、3D空间推理和时间一致性视频深度推断的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "32 pages, 31 figures, github repo:\n  https://github.com/Xiaohao-Xu/Ambiguity-in-Space",
      "pdf_url": "http://arxiv.org/pdf/2503.06014v1",
      "published_date": "2025-03-08 02:33:54 UTC",
      "updated_date": "2025-03-08 02:33:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:20:55.326270"
    },
    {
      "arxiv_id": "2503.06011v1",
      "title": "Intent-Aware Self-Correction for Mitigating Social Biases in Large Language Models",
      "title_zh": "意图感知的自我校正：缓解大型语言模型中的社会偏见",
      "authors": [
        "Panatchakorn Anantaprayoon",
        "Masahiro Kaneko",
        "Naoaki Okazaki"
      ],
      "abstract": "Self-Correction based on feedback improves the output quality of Large\nLanguage Models (LLMs). Moreover, as Self-Correction functions like the slow\nand conscious System-2 thinking from cognitive psychology's perspective, it can\npotentially reduce LLMs' social biases. LLMs are sensitive to contextual\nambiguities and inconsistencies; therefore, explicitly communicating their\nintentions during interactions when applying Self-Correction for debiasing is\ncrucial. In this study, we demonstrate that clarifying intentions is essential\nfor effectively reducing biases in LLMs through Self-Correction. We divide the\ncomponents needed for Self-Correction into three parts: instruction, response,\nand feedback, and clarify intentions at each component. We incorporate an\nexplicit debiasing prompt to convey the intention of bias mitigation from the\ninstruction for response generation. In the response, we use Chain-of-Thought\n(CoT) to clarify the reasoning process. In the feedback, we define evaluation\naspects necessary for debiasing and propose clear feedback through multi-aspect\ncritiques and scoring. Through experiments, we demonstrate that self-correcting\nCoT responses obtained from a debiasing prompt based on multi-aspect feedback\ncan reduce biased responses more robustly and consistently than the baselines.\nWe also find the variation in debiasing efficacy when using models with\ndifferent bias levels or separating models for response and feedback\ngeneration.",
      "tldr_zh": "该研究提出了一种意图感知的自我纠正方法（Intent-Aware Self-Correction），用于降低大语言模型（LLMs）中的社会偏见。该方法将自我纠正过程分解为指令、响应和反馈三个部分，通过在每个环节明确意图（如使用反偏见提示、思维链推理和多维度评价）来增强偏见缓解效果。实验表明，基于多维度反馈的自我纠正能比基线方法更稳健地减少偏见响应，同时揭示了模型偏见水平及生成/反馈模型分离对去偏效果的影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages. Under review",
      "pdf_url": "http://arxiv.org/pdf/2503.06011v1",
      "published_date": "2025-03-08 02:20:43 UTC",
      "updated_date": "2025-03-08 02:20:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:20:45.816452"
    },
    {
      "arxiv_id": "2503.16484v1",
      "title": "AI-Powered Episodic Future Thinking",
      "title_zh": "AI赋能的未来情景思维",
      "authors": [
        "Sareh Ahmadi",
        "Michelle Rockwell",
        "Megan Stuart",
        "Allison Tegge",
        "Xuan Wang",
        "Jeffrey Stein",
        "Edward A. Fox"
      ],
      "abstract": "Episodic Future Thinking (EFT) is an intervention that involves vividly\nimagining personal future events and experiences in detail. It has shown\npromise as an intervention to reduce delay discounting - the tendency to\ndevalue delayed rewards in favor of immediate gratification - and to promote\nbehavior change in a range of maladaptive health behaviors. We present\nEFTeacher, an AI chatbot powered by the GPT-4-Turbo large language model,\ndesigned to generate EFT cues for users with lifestyle-related conditions. To\nevaluate the chatbot, we conducted a user study that included usability\nassessments and user evaluations based on content characteristics\nquestionnaires, followed by semi-structured interviews. The study provides\nqualitative insights into participants' experiences and interactions with the\nchatbot and its usability. Our findings highlight the potential application of\nAI chatbots based on Large Language Models (LLMs) in EFT interventions, and\noffer design guidelines for future behavior-oriented applications.",
      "tldr_zh": "该研究开发了EFTeacher，一个基于GPT-4-Turbo大语言模型的AI聊天机器人，旨在为生活方式相关疾病的用户生成情景未来思维（EFT）提示。通过用户研究，包括可用性评估和内容特征问卷，研究提供了参与者与聊天机器人互动的定性见解。研究结果表明，基于大语言模型的AI聊天机器人在EFT干预中具有潜在应用价值，并为未来行为导向应用提供了设计指南。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.16484v1",
      "published_date": "2025-03-08 01:10:04 UTC",
      "updated_date": "2025-03-08 01:10:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:20:41.482643"
    },
    {
      "arxiv_id": "2503.05997v1",
      "title": "Learning to Drive by Imitating Surrounding Vehicles",
      "title_zh": "《通过模仿周围车辆学习驾驶》",
      "authors": [
        "Yasin Sonmez",
        "Hanna Krasowski",
        "Murat Arcak"
      ],
      "abstract": "Imitation learning is a promising approach for training autonomous vehicles\n(AV) to navigate complex traffic environments by mimicking expert driver\nbehaviors. However, a major challenge in this paradigm lies in effectively\nutilizing available driving data, as collecting new data is resource-intensive\nand often limited in its ability to cover diverse driving scenarios. While\nexisting imitation learning frameworks focus on leveraging expert\ndemonstrations, they often overlook the potential of additional complex driving\ndata from surrounding traffic participants. In this paper, we propose a data\naugmentation strategy that enhances imitation learning by leveraging the\nobserved trajectories of nearby vehicles, captured through the AV's sensors, as\nadditional expert demonstrations. We introduce a vehicle selection sampling\nstrategy that prioritizes informative and diverse driving behaviors,\ncontributing to a richer and more diverse dataset for training. We evaluate our\napproach using the state-of-the-art learning-based planning method PLUTO on the\nnuPlan dataset and demonstrate that our augmentation method leads to improved\nperformance in complex driving scenarios. Specifically, our method reduces\ncollision rates and improves safety metrics compared to the baseline. Notably,\neven when using only 10% of the original dataset, our method achieves\nperformance comparable to that of the full dataset, with improved collision\nrates. Our findings highlight the importance of leveraging diverse real-world\ntrajectory data in imitation learning and provide insights into data\naugmentation strategies for autonomous driving.",
      "tldr_zh": "该研究提出了一种通过模仿周围车辆轨迹来提升自动驾驶模仿学习性能的数据增强方法。针对现有方法主要依赖专家示范数据而忽略周围车辆轨迹信息的问题，作者设计了一种车辆选择采样策略，优先选取信息量丰富且多样化的周围车辆行为作为额外训练数据。在nuPlan数据集上的实验表明，该方法能显著降低碰撞率并提升安全指标，即使仅使用10%原始数据也能达到与完整数据集相当的性能，为有效利用现实世界多样化轨迹数据提供了新思路。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05997v1",
      "published_date": "2025-03-08 00:40:47 UTC",
      "updated_date": "2025-03-08 00:40:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:21:12.513531"
    },
    {
      "arxiv_id": "2503.05996v1",
      "title": "Towards Improving Reward Design in RL: A Reward Alignment Metric for RL Practitioners",
      "title_zh": "强化学习中的奖励设计改进：面向实践者的奖励对齐度量",
      "authors": [
        "Calarina Muslimani",
        "Kerrick Johnstonbaugh",
        "Suyog Chandramouli",
        "Serena Booth",
        "W. Bradley Knox",
        "Matthew E. Taylor"
      ],
      "abstract": "Reinforcement learning agents are fundamentally limited by the quality of the\nreward functions they learn from, yet reward design is often overlooked under\nthe assumption that a well-defined reward is readily available. However, in\npractice, designing rewards is difficult, and even when specified, evaluating\ntheir correctness is equally problematic: how do we know if a reward function\nis correctly specified? In our work, we address these challenges by focusing on\nreward alignment -- assessing whether a reward function accurately encodes the\npreferences of a human stakeholder. As a concrete measure of reward alignment,\nwe introduce the Trajectory Alignment Coefficient to quantify the similarity\nbetween a human stakeholder's ranking of trajectory distributions and those\ninduced by a given reward function. We show that the Trajectory Alignment\nCoefficient exhibits desirable properties, such as not requiring access to a\nground truth reward, invariance to potential-based reward shaping, and\napplicability to online RL. Additionally, in an 11 -- person user study of RL\npractitioners, we found that access to the Trajectory Alignment Coefficient\nduring reward selection led to statistically significant improvements. Compared\nto relying only on reward functions, our metric reduced cognitive workload by\n1.5x, was preferred by 82% of users and increased the success rate of selecting\nreward functions that produced performant policies by 41%.",
      "tldr_zh": "该研究针对强化学习(RL)中的奖励设计问题，提出了轨迹对齐系数(Trajectory Alignment Coefficient)作为奖励对齐(reward alignment)的量化指标，用于评估奖励函数是否准确反映人类决策者的偏好。该指标具有无需真实奖励函数、对基于势能的奖励塑形保持不变性等优点，且适用于在线RL场景。实验表明，使用该指标可使奖励选择成功率提升41%，认知负荷降低1.5倍，82%的用户更倾向采用该方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05996v1",
      "published_date": "2025-03-08 00:38:17 UTC",
      "updated_date": "2025-03-08 00:38:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:21:26.641322"
    },
    {
      "arxiv_id": "2503.05991v1",
      "title": "GrInAdapt: Scaling Retinal Vessel Structural Map Segmentation Through Grounding, Integrating and Adapting Multi-device, Multi-site, and Multi-modal Fundus Domains",
      "title_zh": "GrInAdapt：通过多设备、多中心、多模态眼底领域的锚定集成与自适应实现视网膜血管结构图谱分割的规模化",
      "authors": [
        "Zixuan Liu",
        "Aaron Honjaya",
        "Yuekai Xu",
        "Yi Zhang",
        "Hefu Pan",
        "Xin Wang",
        "Linda G Shapiro",
        "Sheng Wang",
        "Ruikang K Wang"
      ],
      "abstract": "Retinal vessel segmentation is critical for diagnosing ocular conditions, yet\ncurrent deep learning methods are limited by modality-specific challenges and\nsignificant distribution shifts across imaging devices, resolutions, and\nanatomical regions. In this paper, we propose GrInAdapt, a novel framework for\nsource-free multi-target domain adaptation that leverages multi-view images to\nrefine segmentation labels and enhance model generalizability for optical\ncoherence tomography angiography (OCTA) of the fundus of the eye. GrInAdapt\nfollows an intuitive three-step approach: (i) grounding images to a common\nanchor space via registration, (ii) integrating predictions from multiple views\nto achieve improved label consensus, and (iii) adapting the source model to\ndiverse target domains. Furthermore, GrInAdapt is flexible enough to\nincorporate auxiliary modalities such as color fundus photography, to provide\ncomplementary cues for robust vessel segmentation. Extensive experiments on a\nmulti-device, multi-site, and multi-modal retinal dataset demonstrate that\nGrInAdapt significantly outperforms existing domain adaptation methods,\nachieving higher segmentation accuracy and robustness across multiple domains.\nThese results highlight the potential of GrInAdapt to advance automated retinal\nvessel analysis and support robust clinical decision-making.",
      "tldr_zh": "该研究提出GrInAdapt框架，通过**多视角图像**实现**源自由**的**多目标域适应**，用于**视网膜血管分割**。该框架采用三阶段方法：（1）通过配准将图像锚定到共同空间，（2）整合多视角预测提升标签共识，（3）使源模型适应不同目标域。实验表明，GrInAdapt在**多设备、多中心、多模态**的视网膜数据集上显著优于现有方法，分割准确性和鲁棒性更高，为临床决策提供了更可靠的自动化血管分析工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05991v1",
      "published_date": "2025-03-08 00:15:21 UTC",
      "updated_date": "2025-03-08 00:15:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:21:27.660285"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 73,
  "processed_papers_count": 73,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-26T04:22:55.945378"
}